# Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox

**Raymond Zhang**

Laboratoire des signaux et systemes

Universite Paris-Saclay, CNRS, CentraleSupelec,

91190, Gif-sur-Yvette, France.

Raymond.zhang@centralesupelec.fr

**Richard Combes**

Laboratoire des signaux et systemes

Universite Paris-Saclay, CNRS, CentraleSupelec,

91190, Gif-sur-Yvette, France.

Richard.combes@centralesupelec.fr

###### Abstract

We consider Thompson Sampling (TS) for linear combinatorial semi-bandits and subgaussian rewards. We propose the first known TS whose finite-time regret does not scale exponentially with the dimension of the problem. We further show the mismatched sampling paradox: A learner who knows the rewards distributions and samples from the correct posterior distribution can perform exponentially worse than a learner who does not know the rewards and simply samples from a well-chosen Gaussian posterior. The code used to generate the experiments is available at https://github.com/RaymZhang/CTS-Mismatched-Paradox

## 1 Introduction and Setting

We consider the linear combinatorial bandit problem with semi-bandit feedback: At time \(t[T]:=\{1,...,T\}\) a learner selects an action \(A(t)\) where the set of available actions \(\{0,1\}^{d}\) is a known combinatorial set, i.e., a set of binary vectors. Then the environment draws a random vector \(X(t)^{d}\), and the learner then observes \(Y(t)=A(t) X(t)\), where \(\) denotes the Hadamard (element-wise) product, and the learner obtains a reward of \(A(t)^{}X(t)\). We assume that the vectors \((X(t))_{t 1}\) are drawn i.i.d. from some distribution with expectation \((X(t))=^{}\) and that the entries of \(X(t)\) are independent. The vector \(^{}\) lies in some \(^{d}\) and is initially unknown to the learner. The learner wants to minimize the regret:

\[R(T,^{}):=T_{A}\{A^{}^{}\}- [_{t[T]}A(t)^{}^{}]=[ _{t[T]}_{A(t)}].\]

Where \(A^{}*{arg\,max}_{A}\{A^{}^{}\}\) is the optimal action and \(_{A}:={A^{}}^{}^{}-A^{}^{}\) is the reward gap between action \(A\) and optimal action \(A^{}\). The regret is the expected difference between the sum of rewards obtained by an oracle who knows \(^{}\) and always selects the optimal decision and that obtained by the learner. We assume that the optimal decision is unique. To state regret bounds, we use the following notation. We denote by \(_{}:=_{A:_{A}>0}_{A}\) the minimal reward gap and \(_{}:=_{A}_{A}\) the maximal reward gap, \((t):=_{A(t)}\) the reward gap of the action selected at time \(t\), \(m:=_{A}\|A\|_{1}\) the maximal size of an action.

The regret depends on the distributions of the random vector \(X(t)\), which generates the rewards, and we will assume throughout the paper that \(X(t)\) is \(^{2}\)-subgaussian so that for all \(^{d}\):

\[[(^{}X(t))] e^{^{}^{}+ _{}-2}{2}}.\]

This assumption holds in many scenarios of interest, for instance, when \(X(t)[a,b]^{d}\) with \(^{2}=(b-a)^{2}/4\), or when \(X(t)\) is normally distributed with a covariance matrix smaller than \(^{2}I_{d}\). We assume that \(\) is known, or at least upper-bounded.

One of the candidate algorithms for this problem is Thompson Sampling (TS), which is based on Bayesian inference. We consider a prior distribution \(_{0}\) over \(\), a likelihood function \(\) and \(_{t}\) the _posterior distribution_ of \(^{}\) at time \(t\) knowing the observations and the actions up to time \(t\):

\[_{t}()=_{i[d]}[A_{i}(s)(X_{i}(s), _{i})+(1-A_{i}(s))]_{0}()}{_{}_{s[t-1]}_{i[d]}[ A_{i}(s)(X_{i}(s),_{i})+(1-A_{i}(s))]_{0}()d}\]

The TS algorithm with prior \(_{0}\) and likelihood \(\) consists in sampling \((t)\) from distribution \(_{t}\), and select action \(A(t)_{a}A^{}(t)\). The random vector \((t)\), called a Thompson Sample, acts as a proxy for the unknown \(^{}\) and guides the exploration.

We will show how TS behaves with either Gaussian likelihood \((x,)=(2)^{-1/2}e^{-(x-)^{2}/2}\) or Bernoulli likelihood \((x,)=\{x=1\}+(1-)\{x=0\}\) in the next sections. If the likelihood \((X_{i}(s),_{i})\) is selected as the distribution of \(X_{i}(s)\) knowing \(_{i}\) then we say that TS is natural. If the likelihood \((X_{i}(s),_{i})\) is different from the distribution of \(X_{i}(s)\) knowing \(_{i}\), we will say that TS is mismatched. An example of mismatched TS would be to select \(\) as the Gaussian likelihood, although the actual distribution of \(X(t)\) knowing \(^{}\) is, say, Bernoulli or some other bounded distribution. Ultimately, the likelihood function is a choice left up to the learner to control how TS explores the suboptimal actions. Perhaps counterintuitively, mismatched TS can outperform natural TS, as we will demonstrate. The prior \(_{0}\) can be chosen in various ways, for instance, Jeffrey's non-informative prior. It can even be chosen as an improper prior, where \(_{}_{0}()d=+\), as long as the integral in the definition of \(_{t}\) is well-defined.

TS is usually computationally simple to implement as it requires a linear maximization over the action space \(\) at each step, which we assume can be done in polynomial time in \(m\) and \(d\). This fact explains the practical appeal of TS since whenever linear maximization over \(\) can be implemented efficiently; the algorithm has low computational complexity. Also, for some problem instances, it tends to perform well numerically.

## 2 Related Work and Contribution

Combinatorial bandits are a generalization of classical bandits studied in . Several asymptotically optimal algorithms are known for classical bandits, including the algorithm of , KL-UCB , DMED  and TS [14; 22]. Other algorithms include the celebrated UCB1 . Numerous algorithms for combinatorial semi-bandits have been proposed, many of which naturally extend algorithms for classical bandits to the combinatorial setting. CUCB [6; 16] is a natural extension of UCB1 to the combinatorial setting. ESCB [8; 10] is an improvement of CUCB, which leverages the independence of rewards between items. AESCB  is an approximate version of ESCB with roughly the same performance guarantees and reduced computational complexity. TS for combinatorial semi bandits was considered in [11; 15; 20; 21; 23] while TS for linear bandit was studied in [1; 3]. Also, combinatorial semi-bandits are a particular case of structured bandits, for which there exist asymptotically optimal algorithms such as OSSB . It was also shown in  that TS is asymptotically and finite time optimal for matroid-like action sets. The Bayesian regret of TS has been extensively studied, e.g., .

Three types of regret bounds exist in the literature: \(R(T,^{})\) is the problem-dependent regret, i.e., the regret of the learner on the particular problem instance defined by \(^{}\). The minimax regret is \(_{}R(T,^{})\) and the Bayesian regret \(_{^{}}[R(T,^{})]\) where \(^{}\) is drawn according to some prior distribution. We will state regret bounds as a function of the parameters \(T\), the time horizon; \(d\), the problem dimension; \(m\), the maximal size of an action; and \(_{}\), the minimal gap.

The paper  showed that the problem-dependent regret of CUCB is upper bounded by \(O(})\) and its minimax regret is upper bounded by \(O(dmT T}+dm)\).  showed that the problem dependent regret of ESCB is upper bounded by \(O(d( m)^{2} T}{_{min}}+}{_{ min}^{2}})\) and its minimax regret is upper bounded by \(O(T T}+dm)\).  showed that the problem-dependent regret of TS is upper bounded by

\[O(d( m)^{2}}{_{}} T+}{_ {}^{2}}+m(+1}{_{min}})^{2+4m}).\]

While this bound is almost optimal in the asymptotic regime where \(T\), it is exponentially suboptimal in the finite time regime since the last term of this expression scales exponentially with \(m\). Unless one assumes a particular type of action set as in , all the known generic regret upper bound for TS in the literature  feature an exponential dependency on \(m\).  further showed that the problem-dependent regret of TS for some simple combinatorial set can be lower bounded by an expression scaling exponentially in \(m\),

\[R(T,^{})}{4p_{_{}}}(1-(1-p_{ _{}})^{T-1}),p_{_{}}=\{-( -(}{m}+}))^{2}\}.\]

They further showed that Thompson Sampling is not minimax optimal for some combinatorial bandit problems. Therefore, the exponential term is not an artefact of the analysis of . It is also noted that the regret upper bounds for ESCB and CUCB do not feature this exponential dependency in \(m\), suggesting that those algorithms are better in the finite time regime than the versions of TS analyzed so far in the literature. This is unfortunate because TS usually has very low computational complexity, and having an algorithm with both low computational complexity and low regret in the finite time regime would be highly desirable.

**Our contribution : (i)** We propose a new variant of TS with a regret upper bounded by:

\[O(d m}{_{}} T+d^{2}m m }{_{}} T+P(m,d,},_{}, ))\]

where \(P\) is a polynomial in \(m\), \(d\), \(},_{}\). This polynomial term is a clear improvement over the bound of  in the finite time, high dimensional regime where \(T\) is relatively small and \(m\) is large. Indeed, the last term in this bound \(P(m,d,1/_{},_{},)\) will be much smaller than the last term in the bound of \(m((m^{2}+1)/_{})^{2+4m}\) which is exponential in \(m\). To design our variant, we add a slight exploration boost to TS, which vanishes as \(T\) but significantly impacts the algorithm behaviour when \(T\) is moderate and \(m\) is large. Also note that the improvement in the \( m\) term comes from a direct application of a result in .

**(ii)** We design new proof strategies to derive this upper bound, which are based on carefully bounding the sample path behaviour of TS. We believe those strategies are an essential contribution to the analysis of TS and enable us to show that with high probability, TS will sample the optimal action at least \((t^{})\) times with \(>0\). This number serves to control the transient behaviour of TS.

**(iii)** As a by-product, we show the mismatched sampling paradox of TS: in some cases, mismatched TS performs exponentially better than natural TS. For instance, in a problem where \(X(t)\) has Bernoulli distribution, a learner using a uniform (improper) prior and a Gaussian likelihood can perform exponentially better than a learner using a Beta prior (which includes Jeffreys' prior) and the Bernoulli likelihood. In essence, trying to exploit the learner's statistical knowledge about the model ends up harming them.

**(iv)** We confirm our theoretical predictions using numerical experiments, which clearly show that our variant of TS outperforms by several orders of magnitude the Beta-based versions studied in the literature whose regret scales exponentially in the ambient dimension.

## 3 Algorithms

In this section, we present three TS algorithms: B-CTS (Beta-Combinatorial Thompson Sampling), which is TS with a beta prior and a Bernoulli likelihood, G-CTS (Gaussian-Combinatorial Thompson Sampling) which is TS with a uniform (improper) prior and a Gaussian likelihood, and finally BG-CTS (Boosted Gaussian-Combinatorial Thompson Sampling), an algorithm we propose by introducing a carefully chosen exploration boost in G-CTS.

### Notation

We use the following notation to state algorithms. We define the statistics \(N(t):=_{s[t-1]}A(s)\), the vector containing the number of times each item has been selected, \(M_{A}(t):=_{s[t-1]}\{A(s)=A\}\) the number of times action \(A\) has been selected until \(t\), \(V(t):=D_{N(t)}^{-1}\) the diagonal matrix whose diagonal elements are \((1/N_{1}(t),...,1/N_{d}(t))\), \((t):=V(t)_{s[t-1]}X_{i}(s) A_{i}(s)\) the empirical average estimator for \(^{}\). We denote by \((t)=(A(s),A(s) X(s))_{s[t-1]}\) the history which contains all the information collected by the learner up to time \(t\), and which includes both the observations and the selected actions. For two vectors \(,\) in \((^{+})^{d}\) we denote by \((,)=_{i=1}^{d}(_{i},_{i})\) the distribution of a vector with independent entries, and where the \(i\)-th entry is \((_{i},_{i})\) distributed.

### B-Cts

B-CTS (see 1 in the algorithm format) considers the prior \(_{0}=((0),(0))\) where \((0),(0)\), are two vectors in \(^{d}\) chosen by the learner and the Bernoulli likelihood \((x,)=\{x=1\}+(1-)\{x=0\}\). If \((0)=(0)=(1,...,1)\), then the prior \(_{0}\) is uniform over \(^{d}\). If \((0)=(0)=(1/2,...,1/2)\), then the prior is Jeffreys' non-informative prior, which is proportional to the square root of the determinant of the Fisher information matrix. In B-CTS, the posterior distribution \(_{t}\) is also a Beta distribution so that the Beta-CTS selects the action:

\[A(t)_{A}A^{}(t)(t) ((t),(t))\]

where vectors \((t),(t)\) are defined as:

\[(t):=_{s[t-1]}X(s) A(s)+(0)(t):= _{s[t-1]}(1-X(s)) A(s)+(0).\]

### G-Cts

G-CTS considers the improper prior \(_{0}\) which is constant and equal to \(1/\) on all \(^{d}\) and Gaussian likelihood \((x,)=(2^{2})^{-1/2}e^{-(X-)^{2}/(2^{2})}\), where \(^{2}\) is the variance. Of course, since \(_{0}\) is improper, for \(_{t}\) to be well-defined, we require that enough samples have been collected so that \(N(t) 1\). This is easily achieved by selecting \(d\) actions \(A^{1},...,A^{d}\) that cover \(\) in the sense that \(_{i[d]}A^{i} 1\), and initializing the algorithm by sampling each of them once. In G-CTS, the posterior distribution \(_{t}\) is also a Gaussian distribution so that the G-CTS selects the action:

\[A(t)_{A}A^{}(t)(t) N ((t),^{2}V(t)).\]

### Bg-Cts

BG-CTS (see 2 in the algorithm format) is a modification of G-CTS that we propose and that selects the action

\[A(t)_{A}A^{}(t)(t) N ((t),2g(t)^{2}V(t))\]

\[g(t):=f(t):=(1+)( t+(m+2) t+ (1+))\]

and \(^{+}\) is an input parameter of the algorithm. BG-CTS behaves like G-CTS with a time-varying boost in its exploration denoted by \(g(t)\). This boost asymptotically behaves like a constant \(_{t}g(t)=1+\). This boost ensures a much better finite-time behaviour, especially in the moderate \(T\), large \(m\) regime, to avoid the exponentially large regret that can occur in TS. The form of \(g(t)\) is not arbitrary and is derived from the self-normalized concentration inequalities that control the large deviations of vector \((t)\). To make our analysis clearer, we will assume that there exists an exogenous process \((Z(t))_{t 1}\) of i.i.d. \((0,I_{d})\) vectors that serves as the random generator number for the Thompson samples with \((t)=(t)+V^{}(t)Z(t)\).

This decomposition is useful for separating the algorithm's randomness from the bandit environment's randomness. We notice that for all \(s t\), \(Z(s)\) and the history \((t)\) are independent. Furthermore, we call \(Z(s)\) the random part of the Thompson sample, \(A^{}(t)\) the Thompson sample of action

## 4 Main Result

We now state Theorem 1, our main result.

**Theorem 1**.: _For \(=1\), and \(^{2}\) subgaussian rewards, the regret of BG-CTS is upper bounded by:_

\[R(T,^{}) Cd m}{_{}} T+C^{} d^{2}m m}{_{}} T+P(m,d,},_{},)\] (1)

_with \(C,C^{}\) universal constants and \(P\) a polynomial in \(m,d,},_{},\)._

Theorem 1 states that the regret of BG-CTS is upper bounded by an expression with both the correct behaviour when \(T\) is large i.e., both this bound and that of  give the same upper bound on \(_{T})}{ T}\), but also a polynomial dependency in \(m,d,},_{},\). This result predicts that BG-CTS performs much better than other TS variants in the regime where the time horizon \(T\) is moderate and the decision size \(m\) is large.

A consequence of Theorem 1 combined with prior known results of  this is the mismatched sampling paradox for TS: a learner attempting to leverage his knowledge about the statistical model by using natural TS can perform exponentially worse than a learner willingly ignoring this knowledge and using mismatched TS by using an algorithm such as BG-CTS. Consider the example of  which features two disjoint actions of size \(m=d/2\) written \((1,...,1,0,...,0)\) and \((0,...,0,1,...,1)\) and Bernoulli rewards. Suppose the learner attempts to leverage that she knows the rewards are Bernoulli and that the parameter space is \(^{d}\). She will employ a uniform prior over \(^{d}\) and the Bernoulli likelihood. This means using B-CTS and getting a regret that scales exponentially with \(d\) as shown in . Using B-CTS with Jeffrey's prior does not help either. On the other hand, if the learner pretends she does not know the parameter space nor the rewards distribution and uses B-CTS, she gets a regret scaling only polynomially in \(d\). Furthermore, Bernoulli rewards are \(^{2}\) subgaussian with \(^{2} 1/4\) as stated above, so our regret upper bound for BG-CTS applies to this example.

At first glance, it seems outright absurd to use a prior whose support is the whole of \(^{d}\) instead of the actual parameter space \(^{d}\), and using a Gaussian likelihood, which is continuous when the rewards are binary, but this paradoxically gives exponentially better performance. This paradox leads us to believe one should be careful when using posterior sampling for regret minimization. While this is natural for Bayesian inference, things seem to be much more complex when solving bandit problems, which feature both inference and control/exploration.

## 5 Regret Analysis

In this section, we describe how to prove our main result. Due to space constraints, some proof elements are relegated to the appendix. In particular, to make this proof self-contained, we reproduce (without their proofs) the results from previous work that we use for our analysis. A reader can try to follow the proof with the help of the diagram in figure 2.

A fundamental idea of our analysis is to consider the event \(_{t}\) where both events occur :

\[ s[t], A:|A^{}(t)-A^{}^{ }| C_{1}A^{}V^{}(s)A,\]

\[\ |\{s[t]:A^{}{}^{}(s) A^{}{}^{}^{ }\}| C_{2}t^{}\]

Where \(C_{1}=+\), \(C_{2}=^{3}C_{3}}}\), \((C_{3})^{2}=1.238\), \(=3/4-(1/2)(C_{3})^{2} 0.131\).

When \(_{t}\) occurs, we say that we observe a clean run up to time \(t\). A clean run up to time \(t\) implies that the Thompson sample of any action \(A\) at any time \(s[t]\) cannot exceed the sum of its expected value and a bonus proportional to \(A^{}V^{}(t)A\), which can be interpreted as the confidence bonus used in the CUCB algorithm. A clean run also implies that there exist many instants at which the Thompson sample of the optimal action is at least as large as its expected reward \(A^{}{}^{}^{}\).

### Probability of observing a clean run

We first now show that most runs are clean, i.e., clean runs occur with high probability. Proposition 2 states that the probability of a non-clean run up to time \(t\) is much smaller than \(1/t\), and therefore non-clean runs cause little regret.

**Proposition 2**.: _For all \(t C_{5}\), we have \((_{t}) 1-4dt^{-2}-t^{-1}( t)^{-2}-e^{-C_{4}t^{ }}\) with \(C_{4}=C_{2}/8\) and \(C_{5}=23\)._

**Proof :** The proof is relatively technical, and involves decomposing \(_{t}\) according to the fluctuations of \((t)\) and \((t)\). We decompose the Thompson sample of the optimal action as follows:

\[A^{}(s)=A^{}^{}+[U^{}(s)+S^{}(s)] V(s)A^{}}\]

with

\[U^{}(s):=((s)-^{})}{V(s)A^{}}}S^{}(s):=V^{1/2}(s)Z(s)}{V(s)A^{}}}\]

which represent the deviation between the empirical mean and the expected reward, and the deviation of the Thompson sample from the expected value of the Thompson sample.

We introduce the following deviation events

\[_{t}:=\{_{s[t]}|V^{-}(s)(^{ }-(s))|_{}\} _{t}:=\{_{s[t]}|Z(s)|_{}\}\] \[_{t}:=\{|\{s[t]:S^{}(s) \}| C_{2}t^{}\} _{t}:=\{_{s[t]}U^{}(s)\}.\]

Each of those events can be interpreted as follows. \(_{t}\) means that the empirical mean of some item deviates from its expected value at least once, \(_{t}\) means that the randomization in the Thompson sample is abnormally large at least once, \(_{t}\) implies that the empirical mean of the optimal action deviates from its expected value at least once, and \(_{t}\) means that there exist too few instants at which \(S^{}(t)\) is reasonably large.

Assume that none of \(_{t}\), \(_{t}\), \(_{t}\), \(_{t}\) occur. For all \(A\) and all \(s[t]\):

\[|A^{}(s)-A^{}^{}||A^{}((s)-^{ })|+|A^{}((s)-(s))| C_{1}A ^{}V^{1/2}(s)A\]

since if \(_{t}\) does not occur:

\[|A^{}((s)-^{})|A^{}V^{1/2 }(s)A\]

and if \(_{t}\) does not occur and because \(g(t)<2(2m+1)<6m\) see lemma f. 7:

\[|A^{}((s)-(s))|A^{}V^{1/2 }(s)A\]

Furthermore if \(_{t}\) and \(_{t}\) do not occur, there exists at least \(C_{2}t^{}\) instants such that \(S^{}(s)\) and \(U^{}(s)-\), which implies \(A^{}(s) A^{}^{}\). This means that:

\[|\{s[t]:A^{}(t) A^{}^{}\}| C_{2}t ^{}\]

Therefore \(_{t}\) occurs, and we have a clean run. Hence \((_{t}) 1-(_{t})-( _{t})-(_{t})-(_{t})\). We now upper bound the probability of each event separately.

#### 5.1.1 Probability of \(_{t}\)

Using a union bound \((_{t})_{i[d]}(_{s[t]} (s)}(^{}_{i}(s)-(s)) ) 2dt^{-2}\). We used the concentration inequality first derived by  in their analysis of CUCB and recalled in lemma 7.

#### 5.1.2 Probability of \(_{t}\)

Using a union bound and a Chernoff bound for the Gaussian distribution (lemma 10), wheres \(Q\) is the tail function of the standard Gaussian distribution :

\[(_{t})_{i[d]}_{s[t]} (|Z_{i}(s)|) 2tdQ()  2td(-3 t)=2dt^{-2}\]

#### 5.1.3 Probability of \(_{t}\)

We have for \(t 2,(_{t}) t^{-1}( t)^{-2}\) from the concentration inequality derived by  in their analysis of ESCB and recalled in lemma 5.

#### 5.1.4 Probability of \(_{t}\)

In order to control the probability of \(_{t}\), consider the following counting process:

\[W(s)=_{u[s]}\{S^{}(u)\}\]

We wish to show that, with high probability, \(W(s) C_{2}t^{}\). One may readily check that \(W(s)\) is a sum of binary variables and that its conditional expected increment verifies:

\[p(s)=(W(s)-W(s-1)|(s))=(S^{}(s) |(s))=Q()\]

since, conditional to \((s)\), \(S^{}(s)\) has a gaussian distribution with mean \(0\) and variance \(2g(s)\). Let us lower bound of the sum of \(p\). By considering \(t C_{5}\) we have

\[_{s[t]}p(s)(t/2)p(t/2)=(t/2)Q()(t /2)Q(C_{3}) 2C_{2}t^{}\]

using the fact that \(p\) is increasing in \(s\), and the study of \(\) done in lemma f. 1, and lemma 10 on the asymptotic behaviour of the \(Q\) function.

We can now conclude by applying a multiplicative Azuma-Hoeffding style bound to \(W(s)\) presented in lemma 6 in the appendix. With \(C_{4}=C_{2}/8\) we have :

\[(_{t})(W(t) C_{2}t^{ })(W(t)(1/2)_{s[t]}p(s) ) e^{-_{u[t]}p(s)} e^{-C _{2}t^{}}=e^{-C_{4}t^{}}\]

#### 5.1.5 Putting everything together

Adding up the four previous bounds, for all \(t C_{5},(_{t}) 1-4dt^{-2}-t^{-1}( t)^{-2}-e ^{-C_{4}t^{}}\).

### Thompson sample for the optimal action on clean run

We have already established that clean runs occur with high probability, and now we concentrate on how the algorithm behaves on those runs. Proposition 3 further shows that the optimal action will be selected numerous times when a clean run occurs. In turn, the Thompson sample of the optimal action will be arbitrarily close to its expected reward. This argument is the cornerstone of our analysis (that we believe to be missing in the previous analysis of ) and will allow us to control the transient behaviour of the algorithm.

**Proposition 3**.: _For \(t P_{1}(m,d,},)\), if \(_{t}\) occurs, then we must have \(M_{A^{}}(t) C_{6}t^{}\) and \(A^{}{}^{}(t) A^{}{}^{}^{}-h(t)\). With \(P_{1}\) a polynomial in \(m,d,},\), \(C_{6}=C_{4}/2\), and \(h(t)=C_{1} mt^{}}}\). It is noted that \(_{t}h(t)=0\)_

**Proof:** Let us consider a clean run. We can count the number of times the optimal action was not chosen and the variance term of the action is greater than \(_{}\):

\[|\{C_{1}A^{}(s)V^{}( s)A(s)_{}\}| _{i[d]}|\{i A(s),C_{1}(s)}}}{m}\}|\] \[ d^{2}^{2}m^{3} t}{_{}^{2}}\]

And since we have a clean run: \(|\{s[t]:A^{}{}^{}(s) A^{}{}^{}^{} \}| C_{2}t^{}\). At those times, if the variance term of the action played is less than \(_{}\), then it means that the optimal action has been played due to the first condition of \(_{t}\). So we get :

\[M_{A^{}}(t)>C_{2}t^{}-d^{2}^{2}m^{3} t}{_{ }^{2}} C_{6}t^{}\]With \(C_{6}=C_{2}/2\) for \(t P_{1}(m,d,},):=()^{1 +}(1-)^{-1/}(^{2}} {C_{2}})^{1+1/}(m^{3}}{_{}^{2}} )^{1+1/}\) using lemma f. 8. Recall that under \(_{t}\) :

\[|A^{}(t)-A^{}^{}| C_{1} A^{}V^{1/2}(s)A.\]

Since \(M_{A^{}}(t) C_{6}t^{}\) has been selected at least \(C_{6}t^{}\) times up to time \(t\), we have \(A^{}V^{1/2}A^{}t^{}}}\) so we get the announced result :

\[|A^{}(t)-A^{}^{}| C_{1} mt^{}}}=h(t)\]

### Regret upper bound

We can now analyze the regret. Let us define some more events at time \(t\):

\[_{t} :=\{(t)>0\} _{t} :=\{A^{}(t)((t)-(t))>(t)A^{}(t)V(t)A(t)}\}\] \[_{t} :=\{A^{}(t)((t)-^{})> \} _{t} :=\{ i A(t),\;_{i}(t)-_{i}^{ }>}{4m}\}\]

with \((t):=2((||t)+(m+2)(1+d 2)( t)+ (1+e)).\)

The event \(_{t}\) means a suboptimal play. \(_{t}\) implies that the empirical mean of one of the items in the action selected at time \(t\) deviates from its expectation. \(_{t}\) means that the Thompson sample of the decision played is far from its true value, and finally, \(_{t}\) is for when the Thompson sample from the arm played is far from its empirical mean. The complete event system decomposed as follows.

#### 5.3.1 Regret due to \(}_{t}\)

Using proposition 2 we have that \((}_{t}) 4dt^{-2}+t^{-1}( t)^{-2}+e^{-C_{ 4}t^{}}\). Then we can use the fact that \(_{t^{}}}=}{6},_{t ^{}}}<4\). And furthermore, with lemma f. 9 we have that \(_{t^{}}e^{-C_{4}t^{}}<^{-1/}}{ }()\). Therefore, the regret caused by \(}_{t}\) is upper bounded by:

\[_{t[T]}[(t)\{}_{t} \}]<_{}_{t[T]}(}_{t})< _{}[d}{3}+^{-1/}}{} ()+4].\]

#### 5.3.2 Regret due to \(}_{t}_{t}\)

We use proposition 3 and we get that for \(t P_{1}(m,d,1/_{},),A^{}(t)>A^{} ^{}-h(t)\). Combining with event \(}_{t}\), we know that when \(h(t)<_{}/4\), the only action that can be played is the optimal one. We recall the formula of \(h(t)=C_{1} mt^{}}}\) And using lemma f. 8, this happens for \(t>P_{2}(m,},):=()^{1+2/ }(1-)^{-1/}(^{2}}{C_{6}} )^{1+1/}(m^{3}}{_{}^{2}})^{1 +1/}\) So the regret caused by this term is upper bounded by :

\[_{t[T]}[(t)\{}_{t} _{t}\}]<_{}\{P_{1}(m,d,1/ _{},),P_{2}(m,1/_{},)\}.\]

#### 5.3.3 Regret due to \(_{t}\)

This result comes from lemma 2 from  and is reproduced here in lemma 8. By setting \(=}{4}\), we have

\[_{t[T]}[(t)\{_{t}\}]< d_{}(^{2}}{_{}^{2}}).\]

#### 5.3.4 Regret due to \(_{t}\)

We show in lemma 9 in the appendix that \((_{t})<}\) and \(_{t[T]}[(t)\{_{t} \}]<_{}}{6}\).

3.5 Regret due to \(}}_{t}}_{t}}}_{t}\)

We use lemma 4 in this appendix, and we get with \(C=768,C^{}=2304 2\) that :

\[_{t\{T\}}[(t)\{}_{t} }_{t}}}_{t}\}]d m(T)}{_{}}\]

\(d m(T)}{_{}}<Cd m}{ _{}} T+C^{}d^{2}m m}{_{}}  T+1152md^{2} 2(1+e)}{_{}}\).

#### 5.3.6 Putting everything together

Finally, we can put everything together and obtain the regret upper bound found in 1 with the following polynomial constant term :

\[P(m,d,},_{},)= _{}[^{-1/}}{}()+4]+d_{}(^{2}}{_{ }^{2}}+}{3})\\ +_{}(P_{1}(m,d,}, )+P_{2}(m,},))+1152 md^{2} 2(1+e)}{_{}}.\]

The degree of this polynomial depends on \(1+1/<10\) with \(=0.131\). So the degrees of the polynomial in \(m,d,1/_{},,_{}\) are respectively \(30,10,20,20,1\).

## 6 Numerical experiments

In this section we perform numerical experiments with Beta CTS, BG-CTS and ESCB on a case where there are only two actions \(=\{A^{1},A^{2}\}\) of size \(m=d/2\) with \(A^{1}=(1,...,1,0,...,0)\) and \(A^{2}=(0,...,0,1,...,1)\). This action set exhibited exponential regret in . We set \(^{}=(0.7,...,0.7,0.9,...,0.9)\) with a Bernoulli distribution. The algorithm Beta CTS Uniform prior is initialized with the uniform distribution, while the Beta CTS Jeffreys is initialized with the Jeffreys prior on \(\), which puts more weight around the extremities of \(\), increasing exploration.

In the first experiment, we set a time horizon of \(T=2 10^{4}\), and each decision has \(m=50\) items. We run the experiment \(100\) times and plot the average regret over time and two empirical standard deviations in Figure 0(a). The regret is nearly linear for the Beta-based Thompson samplings, whereas the subgaussian Thompson sampling and ESCB showcase regret of magnitude much lower. We set a time horizon in the second experiment \(T=1 10^{4}\). For each decision size \(m\{5,25,45,65\}\), we run the experiments \(150\) times, and we plot the final regret as a function of \(m\) in Figure 0(b). In the Beta-based Thompson samplings, the final regret and its variance rapidly increase with \(m\). In comparison, BG-CTS and ESCB do not seem to be affected.

## 7 Conclusion

We proposed a Boosted variance Gaussian Thompson Sampling for linear combinatorial bandits (BG-CTS) and proved using novel strategies that its regret is bounded polynomially. This variant of TS far outperforms the classical TS by several orders of magnitude on a \(2\) decisions Bernoulli reward example.