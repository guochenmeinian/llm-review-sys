# Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts

Chaoqi Wang\({}^{1}\)   Ziyu Ye\({}^{1}\)   Zhe Feng\({}^{2}\)   Ashwinkumar Badanidiyuru\({}^{3}\)   Haifeng Xu\({}^{1}\)

University of Chicago\({}^{1}\)   Google Research\({}^{2}\)   Google\({}^{3}\)

{chaoqi, ziyuye, haifengxu}@uchicago.edu

{zhef, ashwinkumarbv}@google.com

###### Abstract

Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be of general interest. Extensive empirical tests on both synthetic and real-world datasets demonstrate the significant benefit of utilizing post-serving contexts as well as the superior performance of our algorithm over the state-of-the-art approaches.

## 1 Introduction

Contextual bandits represent a fundamental mathematical model that is employed across a variety of applications, such as personalized recommendations (Li et al., 2010; Wu et al., 2016) and online advertising (Schwartz et al., 2017; Nuara et al., 2018). In their conventional setup, at each round \(t\), a learner observes the context \(_{t}\), selects an arm \(a_{t}\), and subsequently, observes its associated reward \(r_{t,a_{t}}\). Despite being a basic and influential framework, it may not always capture the complexity of real-world scenarios (Wang et al., 2016; Yang et al., 2020). Specifically, the learner often observes valuable follow-up information pertinent to the payoff post arm selection (henceforth, the _post-serving context_). Standard contextual bandits framework that neglects such post-serving contexts may result in significantly suboptimal performance due to model misspecification.

Consider an algorithm designed to recommend educational resources to a user by utilizing the user's partially completed coursework, interests, and proficiency as pre-serving context (exemplified in platforms such as Coursera). After completing the recommendation, the system can refine the user's profile by incorporating many post-serving context features such as course completion status, how much time spent on different educational resources, performances, etc. This transition naturally delineates a mapping from user attributes (i.e., the pre-serving context) to user's learning experiences and outcomes (i.e., the post-serving context). It is not difficult to see that similar scenarios happen in many other recommender system applications. For instance, in e-commerce platforms (e.g., Amazon, Etsy or any retailing website), the system will first recommend products based on the user's profile information, purchasing pattern and browsing history, etc.; post recommendations, the system canthen update these information by integrating post-serving contexts such as this recent purchase behaviour and product reviews. Similarly, media content recommendation platforms like Youtube, Instagram and Tiktok, also observe many post-serving features (e.g., how long the user stayed) that can refine the system's estimation about users' interaction behavior as well as the rewards.

A common salient point in all the aforementioned scenarios is that the post-serving context are prevalent in many recommender systems; moreover, despite being unseen during the recommendation/serving phase, they can be estimated from the pre-serving context given enough past data. More formaly, we assume that there exists a learnable mapping \(^{}():^{d_{x}}^{d_{z}}\) that maps pre-serving feature \(^{d_{x}}\) to the expectation of the post-serving feature \(^{d_{z}}\), i.e., \([|]=^{}()\).

Unsurprisingly, and as we will also show, integrating the estimation of such post-serving features can significantly help to enhance the performance of contextual bandits. However, most of the existing contextual bandit algorithms, e.g., (Auer, 2002; Li et al., 2010; Chu et al., 2011; Agarwal et al., 2014; Tewari and Murphy, 2017), are not designed to accommodate the situations with post-serving contexts. We observe that directly applying these algorithms by ignoring post-serving contexts may lead to linear regret, whereas simple modification of these algorithms will also be sub-optimal. To address these shortcomings, this work introduces a novel algorithm, polLinUCB. Our algorithm leverages historical data to simultaneously estimate reward parameters and the functional mapping from the pre- to post-serving contexts so to optimize arm selection and achieves sublinear regret. En route to analyzing our algorithm, we also developed new technique tools that may be of independent interest.

Main Contributions.
* First, we introduce a new family of contextual linear bandit problems. In this framework, the decision-making process can effectively integrate post-serving contexts, premised on the assumption that the expectation of post-serving context as a function of the pre-serving context can be gradually learned from historical data. This new model allows us to develop more effective learning algorithms in many natural applications with post-serving contexts.
* Second, to study this new model, we developed a robustified and generalized version of the well-regarded elliptical potential lemma (EPL) in order to accommodate random noise in the post-serving contexts. While this generalized EPL is an instrumental tool in our algorithmic study, we believe it is also of independent interest due to the broad applicability of EPL in online learning.
* Third, building upon the generalized EPL, we design a new algorithm polLinUCB and prove that it enjoys a regret bound \(}(T^{1-}d_{u}^{}+d_{u})\), where \(T\) denotes the time horizon and \([0,1/2]\) is the learning speed of the pre- to post-context mapping function \(^{}()\), whereas \(d_{u}=d_{x}+d_{z}\) and \(K\) denote the parameter dimension and number of arms. When \(^{}()\) is easy to learn, e.g., \(=1/2\), the regret bound becomes \(}(}+d_{u})\) and is tight. For general functions \(^{}()\) that satisfy \( 1/2\), this regret bound degrades gracefully as the function becomes more difficult to learn, i.e., as \(\) decreases.
* Lastly, we empirically validate our proposed algorithm through thorough numerical experiments on both simulated benchmarks and real-world datasets. The results demonstrate that our algorithm surpasses existing state-of-the-art solutions. Furthermore, they highlight the tangible benefits of incorporating the functional relationship between pre- and post-serving contexts into the model, thereby affirming the effectiveness of our modeling.

## 2 Related Works

Contextual bandits. The literature on linear (contextual) bandits is extensive, with a rich body of works (Abe et al., 2003; Auer, 2002; Dani et al., 2008; Rusmevichientong and Tsitsiklis, 2010; Lu et al., 2010; Filippi et al., 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2017; Jun et al., 2017). One of the leading design paradigm is to employ upper confidence bounds as a means of balancing exploration and exploitation, leading to the attainment of minimax optimal regret bounds. The derivation of these regret bounds principally hinges on the utilization of confidence ellipsoids and the elliptical potential lemma. Almost all these works assume that the contextual information governing the payoff is fully observable. In contrast, our work focuses on scenarios where the context is not completely observable during arm selection, thereby presenting new challenges in addressing partially available information.

Contextual bandits with partial information.Contextual bandits with partial information has been relatively limited in the literature. Initial progress in this area was made by Wang et al. (2016), who studied settings with hidden contexts. In their setup there is some context (the post-serving context in our model) that can never be observed by the learner, whereas in our setup the learner can observe post-serving context but only after pulling the arm. Under the assumption that if the parameter initialization is extremely close to the true optimal parameter, then they develop a sub-linear regret algorithm. Our algorithm does not need such strong assumption on parameter initialization. Moreover, we show that their approach may perform poorly in our setup. Subsequent research by Qi et al. (2018); Yang et al. (2020); Park and Faradonbeh (2021); Yang and Ren (2021); Zhu and Kveton (2022) investigated scenarios with noisy or unobservable contexts. In these studies, the learning algorithm was designed to predict context information online through context history analysis, or selectively request context data from an external expert. Our work, on the other hand, introduces a novel problem setting that separates contexts into pre-serving and post-serving categories, enabling the exploration of a wide range of problems with varying learnability. Additionally, we also need to employ new techniques for analyzing our problem to get a near-optimal regret bound.

Generalizations of the elliptical potential lemma (EPL).The EPL, introduced in the seminal work ofLai and Wei (1982), is arguably a cornerstone in analyzing how fast stochastic uncertainty decreases with the observations of new sampled directions. Initially being employed in the analysis of stochastic linear regression, the EPL has since been extensively utilized in stochastic linear bandit problems (Auer, 2002; Dani et al., 2008; Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2019; Zhou et al., 2020; Wang et al., 2022). Researchers have also proposed various generalizations of the EPL to accommodate diverse assumptions and problems. For example, Carpentier et al. (2020) extended the EPL by allowing for the use of the \(_{t}^{-p}\)-norm, as opposed to the traditional \(_{t}^{-1}\)-norm. Meanwhile, Hamidi and Bayati (2022) investigated a generalized form of the \(1\|(_{t})\|_{_{t-1}^{-1}}^{2}\) term, which was inspired by the pursuit of variance reduction in non-Gaussian linear regression models. However, existing (generalized) EPLs are inadequate for the analysis of our new problem setup. Towards that end, we develop a new generalization of the EPL in this work to accommodate _noisy feature vectors_.

## 3 Linear Bandits with Post-Serving Contexts

Basic setup.We hereby delineate a basic setup of linear contextual bandits within the scope of the partial information setting, whereas multiple generalizations of our framework can be found in Section 6. This setting involves a finite and discrete action space, represented as \(=[K]\). Departing from the classic contextual bandit setup, the context in our model is bifurcated into two distinct components: the _pre-serving_ context, denoted as \(^{d_{x}}\), and the _post-serving_ context, signified as \(^{d_{z}}\). When it is clear from context, we sometimes refer to pre-serving context simply as _context_ as in classic setup, but always retain the post-serving context notion to emphasize its difference. We will denote \(_{t}=_{s=1}^{t}_{s}_{s}^{}+\) and \(_{t}=_{s=1}^{t}_{s}_{s}^{}+\). For the sake of brevity, we employ \(=(,)\) to symbolize the stacked vector of \(\) and \(\), with \(d_{u}=d_{x}+d_{z}\) and \(\|\|_{2} L_{u}\). The pre-serving context is available during arm selection, while the post-serving context is disclosed _post_ the arm selection. For each arm \(a\), the payoff, \(r_{a}(,)\), is delineated as follows:

\[r_{a}(,)=^{}_{a}^{}+^{}_{a}^{}+,\]

where \(_{a}^{}\) and \(_{a}^{}\) represent the parameters associated with the arm, unknown to the learner, whereas \(\) is a random noise sampled from an \(R_{}\)-sub-Gaussian distribution. We use \(\|\|_{p}\) to denote the \(p\)-norm of a vector \(\), and \(\|\|_{}^{}}\) is the matrix norm. For convenience, we assume \(\|_{a}^{}\|_{2} 1\) and \(\|_{a}^{}\|_{2} 1\) for all \(a\). Additionally, we posit that the norm of the pre-serving and post-serving contexts satisfies \(\|\|_{2} L_{x}\) and \(\|\|_{2} L_{z}\), respectively, and \(_{t[T]}_{a,b}_{a}^{}-_{b}^{},_{t} 1\) and \(_{t[T]}_{a,b}_{a}^{}-_{b}^{},_{t} 1\), same as in (Lattimore and Szepesvari, 2020).

### Problem Settings and Assumptions.

The learning process proceeds as follows at each time step \(t=1,2,,T\):

1. The learner observes the context \(_{t}\).
2. An arm \(a_{t}[K]\) is selected by the learner.

3. The learner observes the realized reward \(r_{t,a_{t}}\) and the post-serving context, \(_{t}\).

Without incorporating the post-serving context, one may incur linear regret as a result of model misspecification, as illustrated in the following observation. To see this, consider a setup with two arms, \(a_{1}\) and \(a_{2}\), and a context \(x\) drawn uniformly from the set \(\{-3,-1,1\}\) with \(^{}(x)=x^{2}\). The reward functions for the arms are noiseless and determined as \(r_{a_{1}}(x)=x+x^{2}/2\) and \(r_{a_{2}}(x)=-x-x^{2}/2\). It can be observed that \(r_{a_{1}}(x)>r_{a_{2}}(x)\) when \(x\{-3,1\}\) and \(r_{a_{1}}(x)<r_{a_{2}}(x)\) when \(x=-1\). Any linear bandit algorithm that solely dependent on the context \(x\) (ignoring \(^{}(x)\)) will inevitably suffer from linear regret, since it is impossible to have a linear function (i.e., \(r(x)= x\)) that satisfies the above two inequalities simultaneously.

**Observation 1**.: _There exists linear bandit environments in which any online algorithm without using post-serving context information will have \((T)\) regret._

Therefore, it is imperative that an effective learning algorithm must leverage the post-serving context, denoted as \(_{t}\). As one might anticipate, in the absence of any relationship between \(_{t}\) and \(_{t}\), it would be unfeasible to extrapolate any information regarding \(_{t}\) while deciding which arm to pull, a point at which only \(_{t}\) is known. Consequently, it is reasonable to hypothesize a correlation between \(_{t}\) and \(_{t}\). This relationship is codified in the subsequent learnability assumption.

Specifically, we make the following natural assumption -- there exists an algorithm that can learn the mean of the post-serving context \(_{t}\), conditioned on \(_{t}\). Our analysis will be general enough to accommodate different convergence rates of the learning algorithm, as one would naturally expect, the corresponding regret will degrade as this learning algorithm's convergence rate becomes worse. More specifically, we posit that, given the context \(_{t}\), the post-serving context \(_{t}\) is generated as1

\[_{t}=^{}(_{t})+_{t}, }^{}()=[|].\]

Here, \(_{t}\) is a zero-mean noise vector in \(^{d_{z}}\), and \(^{}:^{d_{x}}^{d_{z}}\) can be viewed as the post-serving context generating function, which is unknown to the learner. However, we assume \(^{}\) is learnable in the following sense.

**Assumption 1** (Generalized learnability of \(^{}\)).: _There exists an algorithm that, given \(t\) pairs of examples \(\{(_{s},_{s})\}_{s=1}^{t}\) with arbitrarily chosen \(_{s}\)'s, outputs an estimated function of \(^{}:^{d_{x}}^{d_{z}}\) such that for any \(^{d_{x}}\), the following holds with probability at least \(1-\),_

\[e_{t}^{}\|_{t}()-^{}( )\|_{2} C_{0}(\|\|_{_{t}^{-1}}^{2})^{ }(t/),\]

_where \((0,1/2]\) and \(C_{0}\) is some universal constant._

The aforementioned assumption encompasses a wide range of learning scenarios, each with different rates of convergence. Generally, the value of \(\) is directly proportional to the speed of learning; the larger the value of \(\), the quicker the learning rate. Later, we will demonstrate that the regret of our algorithm is proportional to \(O(T^{1-})\), exhibiting a graceful degradation as \(\) decreases. The ensuing proposition demonstrates that for linear functions, \(=1/2\). This represents the best learning rate that can be accommodated2. In this scenario, the regret of our algorithm is \(O()\), aligning with the situation devoid of post-serving contexts (Li et al., 2010; Abbasi-Yadkori et al., 2011).

**Observation 2**.: _Suppose \(()\) is a linear function, i.e., \(()=^{}\) for some \(^{d_{x} d_{z}}\), then \(e_{t}^{}=(\|\|_{_{t}^{-1}}(t/ ))\)._

This observation follows from the following inequalities \(\|_{t}()-^{}()\|=\|}_{t}^{}-^{}\|\|}_{t}-^{ }\|_{_{t}}\|\|_{_{t}^{-1}}=(\|\|_ {_{t}^{-1}}())\), where the last equation is due to the confidence ellipsoid bound (Abbasi-Yadkori et al., 2011). We refer curious readers to Appendix D.2 for a discussion on other more challenging \(()\) functions with possibly worse learning rates \(\).

### Warm-up: Why Natural Attempts May Be Inadequate?

Given the learnability assumption of \(^{}\), one natural idea for solving the above problem is to estimate \(^{}\), and then run the standard LinUCB algorithm to estimate \((_{a},_{a})\) together by treating \((_{t},_{t}(_{t}))\) as the true contexts. Indeed, this is the approach adopted by Wang et al. (2016) for addressing a similar problem of missing contexts \(_{t}\), except that they used a different unsurprised-learning-based approach to estimate the context \(_{t}\) due to not being able to observing any data about \(_{t}\). Given the estimation of \(\), their algorithm -- which we term it as _LinUCB_ (\(\)) -- iteratively carries out the steps below at each iteration \(t\) (see Algorithm 2 for additional details): 1) Estimation of the context-generating function \(_{t}()\) from historical data; 2) Solve of the following regularized least square problem for each arm \(a\), with regularization coefficient \( 0\):

\[_{t}(_{a},_{a})=_{s[t]:a_{s}=a}(r_{s,a}- _{t}^{}_{a}-_{s}(_{s})^{}_{a})^{2}+(\|_{a}\|_{2}^{2}+\|_{ a}\|_{2}^{2}),\] (1)

Under the assumption that the initialized parameters in their estimations are very close to the global optimum, Wang et al. (2016) were able to show the \(O()\) regret of this algorithm. However, it turns out that this algorithm will fail to yield an satisfying regret bound without their strong assumption on very close parameter initialization, because the errors arising from \(()\) will significantly enlarge the confidence set of \(}_{a}\) and \(}_{a}\).3 Thus after removing their initialization assumption, the best possible regret bound we can possibly achieve is of order \(}(T^{3/4})\), as illustrated in the subsequent proposition.

**Proposition 1** (Regret of LinUCB-(\(\))).: _The regret of LinUCB-(\(\)) in Algorithm 2 is upper bounded by \(}(T^{1-}d_{u}^{}+T^{1-/2}^{1+}})\) with probability at least \(1-\), by carefully setting the regularization coefficient \(=(L_{u}d_{u}^{}T^{1-})\) in Equation 1._

Since \([0,1/2]\), the best possible regret upper bound above is \(}(T^{3/4})\), which is considerably inferior to the sought-after regret bound of \(}()\). Such deficiency of LinUCB-(\(\)) is further observed in all our experiments in Section 7 as well. These motivate our following design of a new online learning algorithm to address the challenge of post-serving context, during which we also developed a new technical tool which may be of independent interest to the research community.

## 4 A Robustified and Generalized Elliptical Potential Lemma

It turns out that solving the learning problem above requires some novel designs; core to these novelties is a robustified and generalized version of the well-known elliptical potential lemma (EPL), which may be of independent interest. This widely used lemma states a fact about a sequence of vectors \(_{1},,_{T}^{d}\). Intuitively, it captures the rate of the sum of additional information contained in each \(_{t}\), relative to its predecessors \(_{1},,_{t-1}\). Formally,

**Lemma** (Original Elliptical Potential Lemma).: _Suppose (1) \(_{0}^{d d}\) is any positive definite matrix; (2) \(_{1},,_{T}^{d}\) is any sequence of vectors; and (3) \(_{t}=_{0}+_{s=1}^{}_{s}_{s}^{}\). Then the following inequality holds_

\[_{t=1}^{T}1\|_{t}\|_{_{t-1}^{-1}}^{2} 2(_{T}}{_{0}}),\]

_where \(a b=\{a,b\}\) is the \(\) among \(a,b\)._

To address our new contextual bandit setup with post-serving contexts, it turns out that we will need to robustify and generalize the above lemma to accommodate noises in \(_{t}\) vectors and slower learning rates. Specifically, we present the following variant of the EPL lemma.

**Lemma 1** (Generalized Elliptical Potential Lemma).: _Suppose (1) \(_{0}^{d d}\) is any positive definite matrix; (2) \(_{1},,_{T}^{d}\) is a sequence of vectors with bounded \(l_{2}\) norm \(_{t}\|_{t}\| L_{x}\); (3) \(_{1},,_{T}^{d}\) is a sequence of independent (not necessarily identical) bounded zero-mean noises satisfying \(_{t}\|_{t}\| L_{}\) and \([_{t}_{t}^{}]_{ }^{2}\) for any \(t\); and (4) \(}_{t}\) is defined as follows:_

\[}_{t}=_{0}+_{s=1}^{t}(_{s}+_{s})(_{s}+_{s})^{}^{d d}.\]

_Then, for any \(p\), the following inequality holds with probability at least \(1-\),_

\[_{t=1}^{T}(1\|_{t}\|_{}_{t- 1}^{-1}}^{2})^{p} 2^{p}T^{1-p}^{p}(_{T}}{ _{0}})+^{2}(L_{}+L_{x})^{2}}{ _{}^{4}}(^{2}(L_{}+L_{x})^ {2}}{_{}^{4}})\] (2)

Note that the second term is independent of time horizon \(T\) and only depends on the setup parameters. Generally, this can be treated as a constant. Before describing main proof idea of the lemma, we make a few remarks regarding Lemma 1 to highlight the significance of these generalizations.

1. The original Elliptical Potential Lemma (EPL) corresponds to the specific case of \(p=1\), while Lemma 1 is applicable for any \(p\). Notably, the \((1-p)\) rate in the \(T^{1-p}\) term of Inequality 2 is tight for _every_\(p\). In fact, this rate is tight even for \(_{t}=1, t\) and \(_{0}=1\) since, under these conditions, \(\|_{t}\|_{_{t-1}^{-1}}^{2}=1/t\) and, consequently, \(_{t=1}^{T}(1\|_{t}\|_{_{t-1}^{-1}}^{2})^{p}= _{t=1}^{T}t^{-p}\), yielding a rate of \(T^{1-p}\). This additional flexibility gained by allowing a general \(p\) (with the original EPL corresponding to \(p=1\)) helps us to accommodate slower convergence rates when learning the mean context from observed noisy contexts, as formalized in Assumption 1.
2. A crucial distinction between Lemma 1 and the original EPL lies in the definition of the noisy data matrix \(}_{t}\) in Equation 1, which permits noise. However, the measured context vector \(_{t}\) does _not_ have noise. This is beneficial in scenarios where a learner observes noisy contexts but seeks to establish an upper bound on the prediction error based on the underlying noise-free context or the mean context. Such situations are not rare in real applications; our problem of contextual bandits with post-serving contexts is precisely one of such case -- while choosing an arm, we can estimate the mean post-serving context conditioned on the observable pre-serving context but are only able to observe the noisy realization of post-serving contexts after acting.
3. Other generalized variants of the EPL have been recently proposed and found to be useful in different contexts. For instance, Carpentier et al. (2020) extends the EPL to allow for the \(_{t}^{-p}\)-norm, as opposed to the \(_{t}^{-1}\)-norm, while Hamidi and Bayati (2022) explores a generalized form of the \(1\|(_{t})\|_{_{t-1}^{-1}}^{2}\) term, which is motivated by variance reduction in non-Gaussian linear regression models. Nevertheless, to the best of our knowledge, our generalized version is novel and has not been identified in prior works.

Proof Sketche of Lemma 1.: The formal proof of this lemma is involved and deferred to Appendix B.1. At a high level, our proof follows procedure for proving the original EPL. However, to accommodate the noises in the data matrix, we have to introduce new matrix concentration tools to the original (primarily algebraic) proof, and also identify the right conditions for the argument to go through. A key lemma to our proof is a high probability bound regarding the constructed noisy data matrix \(}_{t}\) (Lemma 2 in Appendix B.1) that we derive based on Bernstein's Inequality for matrices under spectral norm (Tropp et al., 2015). We prove that, under mild assumptions on the noise, \(\|_{t}\|_{}_{t-1}^{-1}}^{2}\|_{t}\|_{_{t -1}^{-1}}^{2}\) with high probability for any \(t\). Next, we have to apply the union bound and this lemma to show that the above matrix inequality holds for _every_\(t 1\) with high probability. Unfortunately, this turns out to not be true because when \(t\) is very small (e.g., \(t=1\)), the above inequality cannot hold with high probability. Therefore, we have to use the union bound in a carefully tailored way by excluding all \(t\)'s that are smaller than a certain threshold (chosen optimally by solving certain inequalities) and handling these terms with small \(t\) separately (which is the reason of the second \(((1/))\) term in Inequality 2). Finally, we refine the analysis of sthe standard EPL by allowing the exponent \(p\) in\((1\|_{t}\|_{}_{t-1}^{-1}}^{2})^{p}\) and derive an upper bound on the sum \(_{t=1}^{T}(1\|_{t}\|_{}_{t-1}^{-1}}^{2})^{p}\) with high probability. These together yeilds a robustified and generalized version of EPL as in Lemma 1. 

## 5 No Regret Learning in Linear Bandits with Post-Serving Contexts

### The Main Algorithm

In the ensuing section, we introduce our algorithm, poLinUCB, designed to enhance linear contextual bandit learning through the incorporation of post-serving contexts and address the issue arose from the algorithm introduced in Section 3.2. The corresponding pseudo-code is delineated in Algorithm 1. Unlike the traditional LinUCB algorithm, which solely learns and sustains confidence sets for parameters (i.e., \(}_{a}\) and \(}_{a}\) for each \(a\)), our algorithm also _simultaneously_ manages the same for the post-serving context generating function, \(()\). Below, we expound on our methodology for parameter learning and confidence set construction.

**Parameter learning**. During each iteration \(t\), we fit the function \(_{t}()\) and the parameters \(\{}_{t,a}\}_{a}\) and \(\{}_{t,a}\}_{a}\). To fit \(_{t}()\), resort to the conventional empirical risk minimization (ERM) framework. As for \(\{}_{t,a}\}_{a}\) and \(\{}_{t,a}\}_{a}\), we solve the following least squared problem for each arm \(a\),

\[_{t}(_{a},_{a})=_{s[t]:a_{s}=a}(r_{s,a}- _{s}^{}_{a}-_{s}^{}_{a})^{2}+ (\|_{a}\|_{2}^{2}+\|_{a}\|_{2}^{2}).\] (3)

For convenience, we use \(\) and \(\) to denote \((,)\) and \((,)\) respectively. The closed-form solutions to \(}_{t,a}\) and \(}_{t,a}\) for each arm \(a\) are

\[}_{t,a}}_{t,a}\\ }_{t,a}=_{t,a}^{-1}_{t,a}_{t,a}=+_{s:a_{s}=a}^{t}_{s}_{s}^{} _{t,a}=_{s:a_{s}=a}^{t}r_{s,a}_{s}.\] (4)

**Confidence set construction.** At iteration \(t\), we construct the confidence set for \(_{t}(_{t})\) by

\[_{t}(_{t},_{t})\{ {z}^{d}:\|_{t}(_{t})-\|_{2}  e_{t}^{}\}.\] (5)

Similarly, we can construct the confidence set for the parameters \(}_{t,a}\) for each arm \(a\) by

\[_{t}(}_{t,a})\{ ^{d_{x}+d_{z}}:\|-}_{t,a}\|_{ _{t,a}}_{t,a}\},\] (6)

where \(_{t,a}=2+R_{q}((1+n_{t}(a)L_{u}^{2}/ )/)}\) and \(n_{t}(a)=_{s=1}^{t}[a_{s}=a]\). Additionally, we further define \(_{t}_{a}_{t,a}\). By the assumption 1 and Lemma 3, we have the followings hold with probability at least \(1-\) for each of the following events,

\[^{}(_{t})_{t}(_{t},_{t} )^{}_{t}(}_{t,a}).\] (7)

### Regret Analysis

In the forthcoming section, we establish the regret bound. Our proof is predicated upon the conventional proof of LinUCB (Li et al., 2010) in conjunction with our robust elliptical potential lemma. The pseudo-regret (Audibert et al., 2009) within this partial contextual bandit problem is defined as,

\[R_{T}=(T)=_{t=1}^{T}(r_{t,a_{t}^{}}-r_{t,a_{t}} ),\] (8)

in which we reload the notation of reward by ignoring the noise,

\[r_{t,a}=_{a}^{},_{t}+_{a} ^{},^{}(_{t}) a_{t}^{}= *{arg\,max}_{a}\,_{a}^{}, _{t}+_{a}^{},^{}(_{t}).\] (9)

It is crucial to note that our definition of the optimal action, \(a_{t}^{}\), in Eq. 9 depends on \(^{}(_{t})\) as opposed to \(_{t}\). This dependency ensures a more pragmatic benchmark, as otherwise, the noise present in \(\) would invariably lead to a linear regret, regardless of the algorithm implemented. In the ensuing section, we present our principal theoretical outcomes, which provide an upper bound on the regret of our poLinUCB algorithm.

**Theorem 1** (Regret of poLinUCB).: _The regret of poLinUCB in Algorithm 1 is upper bounded by \(}(T^{1-}d_{u}^{}+d_{u})\) with probability at least \(1-\), if \(T=((1/))\)._

The first term in the bound is implicated by learning the function \(^{}()\). Conversely, the second term resembles the one derived in conventional contextual linear bandits, with the exception that our dependency on \(d_{u}\) is linear. This linear dependency is a direct consequence of our generalized robust elliptical potential lemma. The proof is deferred in Appendix B.2.

## 6 Generalizations

So far we have focused on a basic linear bandit setup with post-serving features. Our results and analysis can be easily generalized to other variants of linear bandits, including those with feature mappings, and below we highlight some of these generalizations. They use similar proof ideas, up to some technical modifications; we thus defer all their formal proofs to Appendix B.3.

### Generalization to Action-Dependent Contexts

Our basic setup in Section 3 has a single context \(_{t}\) at any time step \(t\). This can be generalized to action-dependent contexts settings as studied in previous works (e.g., Li et al. (2010)). That is, during each iteration indexed by \(t\), the learning algorithm observes a context \(_{t,a}\) for each individual arm \(a\). Upon executing the action of pulling arm \(a_{t}\), the corresponding post-serving context \(_{t,a_{t}}\) is subsequently revealed. Notwithstanding, the post-serving context for all alternative arms remains unobserved. The entire procedure is the same as that of Section 3.

In extending this framework, we persist in our assumption that for each arm \(a\), there exists a specific function \(_{a}^{}():^{d_{x}}^{d_{z}}\) that generates the post-serving context \(\) upon receiving \(\) associated with arm \(a\). The primary deviation from our preliminary setup lies in the fact that we now require the function \(_{a}^{}()\) to be learned for each arm independently. The reward is generated as

\[r_{t,a_{t}}=_{a_{t}}^{},_{t,a_{t}}+ _{a_{t}}^{},_{t,a_{t}}+_{t}.\]

The following proposition shows our regret bound for this action-dependent context case. Its proof largely draws upon the proof idea of Theorem 1 and also relies on the generalized EPL Lemma 1.

**Proposition 2**.: _The regret of poLinUCB in Algorithm 1 for action-dependent contexts is upper bounded by \(}(T^{1-}d_{u}^{}+d_{u})\) with probability at least \(1-\) if \(T=((1/))\)._

The main difference with the bound in Theorem 1 is the additional \(\) appeared in the first term, which is caused by learning multiple \(_{a}^{}()\) functions with \(a\).

### Generalization to Linear Stochastic Bandits

Another variant of linear bandits is the _linear stochastic bandits_ setup (see, e.g., (Abbasi-Yadkori et al., 2011)). This model allows infinitely many arms, which consists of a decision set \(D_{t}^{d}\) at time \(t\), and the learner picks an action \(_{t} D_{t}\). This setup naturally generalizes to our problem with post-serving contexts. That is, at iteration \(t\), the learner selects an arm \(_{t} D_{t}\) first, receives reward \(r_{t,_{t}}\), and then observe the post-serving feature \(_{t}\) conditioned on \(_{t}\). Similarly, we assume the existence of a mapping \(^{}(_{t})=[_{t}|_{z}]\) that satisfies the Assumption 1. Consequently, the realized reward is generated as follows where \(^{},^{}\) are unknown parameters:

\[r_{t,_{t}}=_{t},^{}+_{t },^{}+_{t}.\]

Therefore, the learner needs to estimate the linear parameters \(}\) and \(}\), as well as the function \(()\). We obtain the following proposition for the this setup.

**Proposition 3**.: _The regret of poLinUCB in Algorithm 1 for the above setting is upper bounded by \(}(T^{1-}d_{u}^{}+d_{u})\) with probability at least \(1-\) if \(T=((1/))\)._

### Generalization to Linear Bandits with Feature Mappings

Finally, we briefly remark that while we have so far assumed that the arm parameters are directly linear in the context \(_{t},_{t}\), just like classic linear bandits our analysis can be easily generalized to accommodate feature mapping \(^{x}(_{t})\) and \(^{z}(_{t})=^{z}((_{t})+_{t})\). Specifically, if the reward generation process is \(r_{a}=_{a}^{},^{x}(_{t})+_{a}^{},^{z}(_{t})+_{t}\) instead, then we can simply view \(}_{t}=^{x}(_{t})\) and \(}_{t}=^{z}(_{t})\) as the new features, with \((_{t})=_{_{t}}[^{z}((_{t})+_{t})]\). By working with \(}_{t},}_{t},\), we shall obtain the same guarantees as Theorem 1.

## 7 Experiments

This section presents a comprehensive evaluation of our proposed poLinUCB algorithm on both synthetic and real-world data, demonstrating its effectiveness in incorporating follow-up information and outperforming the LinUCB(\(\)) variant. More empirical results can be found in Appendix D.1.

### Synthetic Data with Ground Truth Models

Evaluation Setup.We adopt three different synthetic environments that are representative of a range of mappings from the pre-serving context to the post-serving context: polynomial, periodicical and linear functions. The pre-serving contexts are sampled from a uniform noise in the range \([-10,10]^{d_{x}}\), and Gaussian noise is employed for both the post-serving contexts and the rewards. In each environment, the dimensions of the pre-serving context (\(d_{x}\)) and the post-serving context (\(d_{z}\)) are of \(100\) and \(5\), respectively with 10 arms (\(K\)). The evaluation spans \(T=1000\) or \(5000\) time steps, and each experiment is repeated with \(10\) different seeds. The cumulative regret for each policy in each environment is then calculated to provide a compararison.

Results and Discussion.Our experimental results, which are presented graphically in Figures 1, provide strong evidence of the superiority of our proposed poLinUCB algorithm. Across all setups, we

Figure 1: Cumulative Regret in three synthetic environments. Comparisons of different algorithms in terms of cumulative regret across the three synthetic environments. Our proposed poLinUCB (ours) consistently outperforms other strategies (except for LinUCB which has access to the post-serving context during arm selection), showcasing its effectiveness in utilizing post-serving contexts. The shaded area denotes the standard error computed using 10 different random seeds.

observe that the LinUCB (\(x\) and \(z\)) strategy, which has access to the post-serving context during arm selection, consistently delivers the best performance, thus serving as the upper bound for comparison. On the other hand, the Random policy, which does not exploit any environment information, performs the worst, serving as the lower bound. Our proposed poLinUCB (ours) outperforms all the other strategies, including the LinUCB (\(\)) variant, in all three setups, showcasing its effectiveness in adaptively handling various mappings from the pre-serving context to the post-serving context. Importantly, poLinUCB delivers significantly superior performance to LinUCB (\(x\) only), which operates solely based on the pre-serving context.

### Real World Data without Ground Truth

**Evaluation Setup.** The evaluation was conducted on a real-world dataset, MovieLens (Harper and Konstan, 2015), where the task is to recommend movies (arms) to a incoming user (context). Following Yao et al. (2023), we first map both movies and users to 32-dimensional real vectors using a neural network trained for predicting the rating. Initially, \(K=5\) movies were randomly sampled to serve as our arms and were held fixed throughout the experiment. The user feature vectors were divided into two parts serving as the pre-serving context (\(d_{x}=25\)) and the post-serving context (\(d_{z}=7\)). We fit the function \(()\) using a two-layer neural network with 64 hidden units and ReLU activation. The network was trained using the Adam optimizer with a learning rate of 1e-3. At each iteration, we randomly sampled a user from the dataset and exposed only the pre-serving context \(\) to our algorithm. The reward was computed as the dot product of the user's feature vector and the selected movie's feature vector and was revealed post the movie selection. The evaluation spanned \(T=500\) iterations and repeated with \(10\) seeds.

**Results and Discussion.** The experimental results, presented in Figure 2, demonstrate the effectiveness of our proposed algorithm. The overall pattern is similar to it observed in our synthetic experiments. Our proposed policy consistently outperforms the other strategies (except for LinUCB with both pre-serving and post-serving features). Significantly, our algorithm yields superior performance compared to policies operating solely on the pre-serving context, thereby demonstrating its effectiveness in leveraging the post-serving information.

## 8 Conclusions and Limitations

**Conlusions.** In this work, we have introduced a novel contextual bandit framework that incorporates post-serving contexts, thereby widening the range of complex real-world challenges it can address. By leveraging historical data, our proposed algorithm, poLinUCB, estimates the functional mapping from pre-serving to post-serving contexts, leading to improved online learning efficiency. For the purpose of theoretical analysis, the elliptical potential lemma has been expanded to manage noise within post-serving contexts, a development which may have wider applicability beyond this particular framework. Extensive empirical tests on synthetic and real-world datasets have demonstrated the significant benefits of utilizing post-serving contexts and the superior performance of our algorithm compared to state-of-the-art approaches.

**Limitations.** Our theoretical analysis hinges on a crucial assumption that the function \(^{*}()\) is learnable, which may not always be satisfied. This is particularly a concern when the post-serving contexts may hold additional information that cannot be deduced from the pre-serving context, irrespective of the amount of data collected. In such scenarios, the function mapping from the preserving context to the post-serving context may be much more difficult to learn, or even not learnable. Consequently, a linear regret may be inevitable due to model misspecification. However, from a practical point of view, our empirical findings from the real-world MovieLens dataset demonstrate that modeling the functional relationship between the pre-serving and post-serving contexts can still significantly enhance the learning efficiency. We hope our approaches can inform the design of practical algorithms that more effectively utilizes post-serving data in recommendations.

**Acknowledgement.** This work is supported by an NSF Award CCF-2303372, an Army Research Office Award W911NF-23-1-0030, and an Office of Naval Research Award N00014-23-1-2802.

Figure 2: Results on MovieLens.