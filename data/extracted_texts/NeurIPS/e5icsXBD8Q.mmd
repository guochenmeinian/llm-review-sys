# Large Language Model Unlearning via

Embedding-Corrupted Prompts

 Chris Yuhao Liu Yaxuan Wang Jeffrey Flanigan Yang Liu

University of California, Santa Cruz

{yliu298,ywan1225,jmflanig,yangliu}@ucsc.edu

Corresponding author: yliu298@ucsc.edu.Equal advising.

###### Abstract

Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present **Embedding-COrupted (ECO) Prompts**, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at _nearly zero side effects_ in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at https://github.com/chrisliu298/llm-unlearn-eco.

## 1 Introduction

The use of large language models (LLMs), trained on extensive text corpora , has increasingly become standard in daily life since the arrival of ChatGPT . Despite the benefits LLMs offer, they pose potential risks across a range of domains, such as copyright infringement , dissemination of hazardous knowledge , and privacy violations . Adherence to the General Data Protection Regulation (GDPR) , which requires the removal of users' data post-training, is essential. Machine unlearning has emerged as a new paradigm  and has been widely studied for classification models and tasks in recent years . However, unlearning in the context of LLMs remains largely underexplored, presenting unique challenges and risks that extend beyond privacy concerns due to the infeasibility ofretraining from scratch [19; 20], the ease with which anyone can access powerful models, and the substantial capabilities of these models across various tasks [80; 88].

Various machine unlearning methods have been proposed specifically for LLMs to address the above challenges. A major line of approaches focuses on parameter fine-tuning  based on a modified loss, usually by unlearning on the forget data and learning from the retained data to preserve utility [137; 143; 23; 142; 73; 149; 62], which requires only a small number of weight updates compared to retraining from scratch. Other approaches include model editing techniques [58; 139; 13; 148; 56; 102; 83], unlearning via in-context examples in the prompt [106; 99], and guarding the prompts themselves . Although effective, some approaches have been shown to impair a model's general capabilities [50; 88]. This is due to **knowledge entanglement** caused by the fuzzy boundary between retention and forgetting objectives (e.g., forgetting a single person without affecting other related ones) [93; 90; 88; 109]. Additionally, most prior work targets unlearning at the million- to billion-parameter scale through gradient-based optimization [143; 56; 36; 142; 90; 149; 73; 62], making the cost of unlearning scale with the model size and potentially expensive even with parameter-efficient modules. This cost could rise significantly for advanced proprietary models with hundreds of billions of parameters (e.g., GPT-4 , Gemini , Claude , and other model-as-a-service (MaaS)  providers), which makes gradient-based unlearning methods impractical.

In this work, we explore if an "unlearned state" can be imposed on an intact LLM and focus on tackling the challenges of knowledge entanglement and unlearning inefficiency in LLMs. We hypothesize that unlearning can be implemented as a state by decomposing the unlearning problem into two more tractable subproblems: 1) unlearning target identification, which explicitly identifies if the prompt contains content within the unlearning target, and 2) forgetting, which ensures that the generated responses no longer reflect any prior knowledge related to the unlearning target. We present **Embedding-COrrupted (ECO) Prompts**, a lightweight two-step framework to tackle both problems above:

1. To identify the unlearning target, we use a prompt classifier that is trained to explicitly model the prompt distribution and to safeguard prompts within the scope of the unlearning target.
2. To achieve forgetting, we approximate an unlearned state by passing the query identified by the prompt classifier to the LLM, but in a corrupted form. We leverage corruptions learned efficiently via zeroth-order optimization [124; 125] and apply them to the prompt's embedding space during inference.

Our contributions are as follows:

Figure 1: **Using embedding-corrupted prompts to maintain an unlearned state** on the LLM subject to unlearning. We first employ a classifier to identify whether the incoming prompt falls within the scope of the unlearning target. We construct embedding-corrupted prompts by selectively corrupting dimensions within the tokens’ embeddings. The corruption parameter is learned offline via zeroth order optimization. An unlearned state is imposed during inference and does not require any updates to the original model’s weights.

* We introduce Embedding-COrupted (ECO) Prompts, a novel and lightweight LLM unlearning method that enforces an unlearned state over an intact LLM.
* We demonstrate that, instead of relying on unlearning objective optimization, carefully corrupted prompts lead to behavior that resembles that of a model which has never seen the data intended to be forgotten, across multiple tasks and metrics.
* Through extensive experiments across three knowledge unlearning tasks, we demonstrate the superior performance of our method in both retaining and forgetting, incurring virtually zero side effects and no additional cost when scaling to larger models.
* To the best of our knowledge, we are the first to demonstrate universally effective and efficient unlearning for 100 LLMs and up to 236B parameters.

## 2 Preliminaries and Problem Setup

### Threat Model

In our threat model, we consider threats in three categories: entity leaking, hazardous knowledge, and copyrighted content extraction. We consider a gray box setting similar to that of  and , where users interact with an LLM or a model-as-a-service  through a chat interface or structured API access . Under this setting, all users can send prompts to the LLM and receive the corresponding completions or per-token logits of the output tokens. We also assume that adversaries within the user group generate prompts in-distribution and attempt to jailbreak either the guarding mechanism or the LLM itself. Our threats and goals below are as follows.

**Entity leaking** Entity leaking occurs when an LLM inadvertently discloses the identity or sensitive information of specific individuals whose data was unintentionally included in the training set [67; 17; 86]. Our goal is to ensure that the LLM either provides incorrect responses or refuses to answer queries from threat agents that involve these individuals or groups.

**Hazardous knowledge** Given the ease of use and accessibility of both commercial and open-source LLMs, individuals with malicious intent could exploit the advanced capabilities of LLMs to acquire hazardous knowledge at minimal cost [73; 53; 39; 115]. Here, the objective is to prevent such actors from obtaining dangerous knowledge from LLMs while ensuring that the models retain their original capabilities in benign but related domains.

**Copyrighted content** Extracting and distributing copyrighted content from an LLM is generally illegal, as it involves reproducing and distributing protected material without permission [49; 65; 72]. Even if copyrighted content is filtered from the pre-training data, fragments of the text may still be scattered throughout the corpus, and the LLM could memorize them. An attempt to extract the original passage by prompting with a known portion of the text might cause the LLM to generate the passage verbatim, which we aim to prevent.

Beyond the categorization of risks presented above, we also highlight a commonly overlooked aspect in unlearning: **timeliness**[109; 123; 16; 101]. Timeliness measures how quickly unlearning can be completed once the relevant risks are identified. Given the volume of real-time interactions from MaaS users , the effectiveness of LLM unlearning may degrade progressively with each hour of delay, particularly in safety and privacy domains. Our objective is to develop a method that can be implemented with extreme efficiency, ideally operating in real-time.

### Problem Setup

We assume a learning algorithm \(A\)3, the training set \(D_{tr}\), and the forget set \(D_{f}\). For each dataset \(D\), we have \(D=\{_{i}\}_{i=1}^{N}\), where each \(_{i}=\{_{i},_{i}\}\). In the traditional setting of machine unlearning [19; 101], a retained model \(_{r}\) that has never seen the forget dataset is obtained via the learning algorithm but excluding the forget set, \(_{r}=A(D_{tr} D_{f})\), where \(D_{r}=D_{tr} D_{f}\) is known as the retain set. We use \(_{o}\) to denote the **original model4** obtained from the learning algorithm \(A\), and \(_{r}\) to represent a **retained model** retained from scratch via an unlearning algorithm \(U\), which we define below, by training on \(D_{tr}\) and \(D_{r}\), respectively.

Based on our threat model in Section 2.1, which does not allow users to access model weights, instead of achieving unlearning in the weight space , we focus on weak unlearning  in the output space. Specifically, we aim for similarity between models \(h(;_{r})\) and \(h(;_{u})\) for all \(\), where \(h:\) maps from the input space \(\) and weight space \(\) to the output space \(\).

A relaxed objective of unlearningBecause we are in the LLM setting, we use a relaxed definition of unlearning that does not require differential privacy requirements (i.e., \((,)\)-close), similar to . More specifically, we follow prior work [46; 23; 69; 61; 142; 54] and evaluate whether the retained model and the unlearned model's metric values over a set of metrics \(=\{m_{1},m_{2},...,m_{K}\}\) are similar on both \(D_{r}\) and \(D_{f}\). To maintain the general utility of the LLM after unlearning, we would also like the model to perform well on an o.o.d. general domain distribution \(_{g}\), which is unknown during unlearning. Therefore, our goal of unlearning is

\[[m_{i}(h(;_{u}))]} {[m_{i}(h(;_{r}))]} 1\] (1)

for all \(m_{i}\), where \(\) is a set of non-negative metrics. We want this to hold separately for each case \( p_{_{f}}()\), \( p_{_{r}}()\), and \( p_{_{g}}()\). During evaluation, we assess whether the two models have empirically similar performance over the metrics set \(\).

## 3 ECO: Unlearned LLMs via Embedding-Corrupted Prompts

### Method Overview

Our method consists of two steps: 1) train a prompt classifier to predict if an incoming prompt falls within the scope of unlearning, and 2) corrupt the prompt in the embedding space if the classifier makes a positive prediction (i.e., should forget).

Enforcing retaining and forgetting via a classifierWe first train a prompt classifier to explicitly identify if the prompt falls within the scope of unlearning. For any incoming prompt, \(\), the prompt classifier \(C\) takes in \(\) and returns \(p_{C}(f)=1-p_{C}(r)\), the probability of the prompt being in the scope of forgetting. Similar to any classifier prediction, if \(p_{C}(f)>p_{C}(r)\), we consider \(\) as containing the unlearning concept that our LLM is supposed to forget. Formally, given a positive prediction, \(p_{C}(f)>p_{C}(r)\), we replace the original input \(\) with \(}\). Otherwise, the original \(\) is passed to the LLM.

\[=}&p_{C}(f)>p_{C}(r )\\ &\] (2)

Embedding-corrupted promptsInstead of modifying \(\) in the token space, we corrupt it in the embedding space. Let \(=\{x_{1},x_{2},,x_{T}\}\) be a prompt of \(T\) tokens and \(=\{e_{1},e_{2},,e_{T}\}\) be the corresponding embedding vectors. Let \(\) be the space of the token embeddings. Each embedding vector is produced by an embedding function \(E:^{d}\). We also use the symbol \(\) (where \(\)) to denote the strength of the corruption, which parameterizes the corruption function. Formally, for a single prompt \(\) mapped to the embeddings \(=E()=\{e_{1},e_{2},,e_{T}\}\), a corruption function \(:\), parameterized by \(\), produces the embedding-corrupted prompts

\[}=(;)=\{_{1}, _{2},,_{T}\}.\] (3)

Let \(:\) be the function \(h\) but taking the input embeddings instead of input tokens (i.e., \(h\) with the input embedding layer detached). Our objective is to pick a good \(^{*}\) such that the following modified unlearning objective is satisfied:

\[[m_{i}(((; ^{*});_{o}))]}{_{r}} 1, m_{i} .\] (4)

Here, \(_{r}\) is used to approximate the true \([m_{i}((;_{r}))]\) as the retained model is not available.

### Decision Threshold Calibration and Conformal Prediction

Due to the potential fuzzy boundary between retaining and forgetting, one needs to pick a threshold better than \(p(f)>p(r)\), which does not take into account the classifier's confidence. Depending on the application and the empirical performance of the classifier predictions, we incorporate two types of thresholding techniques.

#### Simple thresholding

We choose a simple threshold, \(\), as the criterion to determine if a prompt \(\) belongs to the forget distribution. Formally, the output \(}\) from the LLM is returned by feeding a prompt selected by the classifier, based on its prediction \(p_{C}(f)\):

\[}=((; );_{o})&p_{C}(f)\\ (;_{o})&\] (5)

We pick the value of \(\) using a separate set \(D_{}\) for calibration. The goal is to choose an optimal \(\) that has the smallest false positive rate and false negative rate on \(D_{}\).

#### Conformal prediction

We also consider conformal prediction (CP) , which finds a calibrated threshold given a target error rate \(\), as a second way for threshold calibration. In essence, conformal prediction uses a small user-specified error rate, \(\), and unlikelihood scores (e.g., \(1-p_{C}(y)\)) on a calibration set to derive a threshold. Labels with unlikelihood scores lower than the threshold are included in the final prediction set.

We adapt the split conformal prediction setup , which uses a separate calibration set, \(D_{}=\{_{i},y_{i}\}_{i=1}^{n}\) (\(y\{,f\}\)), to determine a conformity threshold and a non-conformity score, \(S:\), to measure how unlikely a sample \((x,y)\) is to the classifier \(C\). Following conventional choice, we use \(s_{i}=S(_{i},y_{i})=1-p_{C}(y_{i}_{i})\) as the non-conformity score. Given the calibration set size \(n\) and a small user-specified error rate \(\), we determine a quantile \(\) using the \((n+1)(1-)/n\) empirical quantile in the non-conformity scores from \(D_{}\). The final prediction set on a new test sample \(_{}\) is formed by including all labels with a non-conformity score below \(\) as

\[_{}(_{})=\{y :S(_{},y)\}.\] (6)

Formally, given the prompt classifier \(C\), a prediction set \(_{}()\) for the prompt \(\), and the decision threshold \(\), the response from the LLM is obtained by the following rules:

\[}=((; );_{o})&1_{}\\ (;_{o})&\] (7)

In experiments, we pick the thresholding method based on its empirical performance. In Appendix C.5, we give a toy example of how to determine the prediction set size for a test sample.

### Embedding-Corrupted Prompts

Given an accurate classifier, one can already mitigate the risk defined in our threat model by providing a template response. However, doing so violates the weak unlearning objective for \( p_{_{f}}()\) in Equation (1), because a retained model (that is, a model not trained on the forget data) is highly unlikely to give template responses to prompts in the forget data. To actually achieve unlearning, given the prompt classifier obtained in Section 3.2, we introduce a simple method that learns to corrupt user prompts in the embedding space efficiently via zeroth order optimization [124; 125] toward the unlearning objective. One may also set \(\) manually without optimization, at the cost of being further away from the desired retained model (see below).

#### Optimization objective

A natural choice to make the unlearned model behave like a retained model is to minimize a distance function that quantifies the gap between the two models for all \(m\). As the retained model is not available (otherwise, unlearning would not be needed), we use a surrogate metric value \(_{r}\) if available to approximate how the retained model would behave over \(\). Based on our relaxed unlearning objective in Equation (4), we define a general distance measure below:

\[d(},_{o},_{r},)=|}_{i}((}; _{o}))}_{}-_{r}}_{ }\] (8)

We aim to learn a \(^{*}\) such that the metric gap in Equation (8) between the unlearned model and the retained model is minimized. Formally, given a parameterized corruption function \((;)\), our unlearning objective is to minimize the following:

\[^{*}=*{arg\,min}_{}d((; ),_{o},_{r},)\] (9)

#### Note:

If the metric value \(_{r}\) is not obtainable, one may tune \(\) directly and inspect whether the model output on the forget set aligns with the unlearning criteria. For classification-style tasks, the target \(_{r}\) may correspond to random guessing.

#### 3.1.2 Corruption learning via zeroth order optimization

We now formulate the zeroth order gradient approximation via finite differences [124; 125]. Given a pre-defined perturbation size \(\) applied to the current corruption parameter \(_{k}\), we treat the distance function \(d()\) as a black-box and query it for the final metric gap during optimization. Because we only learn the strength of the corruption function with a scalar-valued \(\), we use a deterministic perturbation to \(_{k}\). For a single sample, given an initial guess \(_{0}\), a step size \(\), and a smoothing parameter \(\) (also known as perturbation size), the minimization of Equation (9) uses the following update rules:

\[}_{} =+(;_{k}+)\] (10) \[}_{} =+(;_{k}-)\] (11) \[d_{_{k}} =}_{},_{o},_{r},)-d(}_{}, _{o},_{r},)}{2}\] (12) \[_{k+1} =_{k}-d_{_{k}}\] (13)

#### 3.1.3 Choice of corruption function

Prior work  suggests that only a small number of dimensions for each embedding vector suffices to steer the output, so we only corrupt the first dimension of each token's embedding. We also experimented with other corruption functions (e.g., standard Gaussian noise or zeroing-out top entries) but found that our method is insensitive to the choice of corruption function, with all tested functions yielding similar end results. We conducted ablation studies on various corruption functions in Appendix D.2.

## 4 Experiments

In this section, we present experimental results for entity unlearning (Section 4.2), hazardous knowledge unlearning (Section 4.3), and copyrighted content unlearning (Section 4.4).

### Prompt Classifier

For each unlearning task, we fine-tune a RoBERTa  or a Llama-3.1-1B-Instruct  as the prompt classifier on the corresponding \(D_{r}\) and \(D_{f}\). In entity and copyrighted content unlearning tasks, we use the entire \(D_{f}\) to train the classifier5 because the unlearning target is fully captured by the forget set, which does not require generalization outside the set. For WMDP and MMLU, we only use a surrogate synthetic forget set \(D_{f}\) to train the prompt classifier, and the actual forget set \(D_{f}\) is not accessible until evaluation. For all prompt classifiers, we use an independent validation set \(D_{}\) to tune the decision threshold \(\) and hyperparameters or to calibrate the empirical quantile \(\), which is used to determine conformity. In Tables 5 to 7, we show that all classifiers can distinguish \(D_{r}\) and \(D_{f}\) well, and generalize to unseen \(D_{g}\) with a low false positive rate. Meanwhile, although we do not specifically target out-of-distribution prompts related to jailbreak attempts, we demonstrate our classifiers' ability to prevent such risks in appendix C.3.3. We also show that simple data augmentation techniques can further enhance our classifiers' performance in detecting out-of-distribution and jailbreak prompts. We provide further detailed information on how prompt classifiers are trained for each task and their performance in Appendix C.3.

### Entity Unlearning

#### 4.2.1 Experimental setup

The TOFU dataset  is a synthetic question-answering dataset of author biographies. The goal is for an LLM trained on the complete dataset (all authors) to unlearn a fraction of fictitious authors (1/5/10%) while retaining knowledge about both 1) the remaining fictitious authors and 2) the real world. To assess forgetting and retention, we use two metrics proposed alongside the TOFU dataset: forget quality and model utility. Forget quality is represented by a p-value from a Kolmogorov-Smirnov (KS) test, where a high value indicates high similarity in distribution between the output of the unlearned model and that of the retained model. Model utility assesses the model's performance on the retained set and real-world knowledge. For a detailed description of all the metrics, refer to Appendix C.1.1. We conduct experiments with two corruption functions, random noise (RN) and zero-out (ZO). We include all baselines from , a prompting baseline, and the recently proposed negative preference optimization (NPO) . We provide formulations of all baselines in Appendix C.4.

ECO brings Pareto improvement.

In Figure 2, we illustrate the trade-off between model utility and forget quality for two models, Phi-1.5  and Llama-2-7B-Chat , including forgetting 1%, 5%, and 10% of the samples. ECO-RN and ECO-ZO consistently achieve close-to-perfect forget quality regardless of the model or the size of the forget set. Notably, ECO-ZO maintains a distribution almost identical to the retained model (as the p-value is close to 1) in all cases, suggesting that ECO prompts can effectively approximate the outputs of the retained model in distribution. Given that the prompt classifier trained on the TOFU dataset incurs zero false positives, our method results in _zero sacrifice in model utility_, thus striking a perfect balance between forgetting and retention. For the ECO-RN variant, we optimize \(\) for Llama-2-7B-Chat on 1% of the forget set and use the same value for all five other settings, suggesting its transferability across models and forget tasks.

**Baselines struggle to forget or collapse in utility.** We also observe that GA, GD, KL, PO, and the prompting baseline exhibit minimal forgetting when the forget set size is small (i.e., 1%). Meanwhile, some of them experience a substantial decrease and even a collapse in utility when the forget set is larger (5% and 10%). Methods based on negative preference optimization  demonstrate a noticeably stronger trade-off compared to other baselines, especially with NPO-RT. Nevertheless, the effectiveness of the NPO variants varies across different models and forget set sizes, and the loss in model utility is non-trivial. We present the full results on all metrics and baselines in Table 17 and Table 18 in Appendix E.2.

### Hazardous Knowledge Unlearning

Experimental setupFor both WMDP  and MMLU subset unlearning tasks , we directly unlearn on pre-trained models. The WMDP benchmark focuses on unlearning knowledge in biology, chemistry, and cybersecurity. In MMLU subset unlearning, the goal is to unlearn three subjects while retaining their closely related counterparts: economics (econometrics), law (jurisprudence), and physics (math), all requiring high-precision forgetting to resolve knowledge entanglement. In line with , we assess all models based on their multiple-choice accuracy. A successfully unlearned model should exhibit an accuracy near random guessing (25% for four-option multiple-choice questions). We employ the ECO-RN variant (random noise) as the corruption function for both tasks. We optimize the corruption strength \(\) only for Zephyr-7B on a set of 100 synthetic questions and answers generated by GPT-4 to ensure that real questions are not exposed during unlearning. The

Figure 2: **Model utility versus forget quality (p-value) on three different forget set sizes of the TOFU dataset after unlearning.** We show two models, Phi-1.5 (top) and Llama-2-7B-Chat (bottom). For GA, GD, KL, PO, and the prompting baseline, the forget qualities are either too small or come at the cost of a substantial decrease in model utility. Negative preference optimization (NPO)  variants achieve a good balance in some cases, but the trade-off in model utility is still non-trivial. ECO-RN (random noise) and ECO-ZO (zero-out) achieve an almost identical distribution to the retained model while incurring no sacrifice in model utility.

same corruption parameter \(\) is used for all other models. We compare our method against LLMU , SCRUB , SSD , RMU , and a prompting baseline that instructs the model not to answer questions within the domain correctly.

**ECO is domain- and model-agnostic.** In Tables 1 and 2, for all models on the WMDP benchmark, ECO achieves accuracy close to random guessing for multiple-choice questions while maintaining original MMLU performance. LLMU, SCRUB, and SSD show limited forgetting performance across all subjects. Although RMU successfully unlearns biology and cybersecurity, it retains accuracy in chemistry, indicating that unlearning capability may vary across subjects or the available data for unlearning. On Yi-34B-Chat and Mixtral-8x7B-Instruct, RMU's forgetting capability is not as effective as on Zephyr-7B, while ECO's performance remains consistent despite increased original performance on the task.

**ECO unlearns at high precision.** On MMLU subset unlearning, both ECO and RMU successfully unlearn the three chosen subjects (Table 2). However, RMU's accuracy in econometrics and jurisprudence significantly decreases. This implies that RMU might be sensitive to the entanglement of knowledge in closely related subjects. In contrast, this entanglement poses no problem for ECO's prompt classifier due to its low false positive rate in the retain domain.

**ECO's universal effectiveness.** To further validate the effectiveness of our method across various models, we conducted experiments on **100 models ranging from 0.5B to 236B** on both the WMDP

  
**Model** & **Method** & **Bio (\(\))** & **Chem (\(\))** & **Cyber (\(\))** & **MMLU (\(\))** \\   & Original & 64.2 & 48.3 & 43.1 & 58.9 \\  & Prompting & 63.2 & 43.6 & 44.0 & 57.8 \\  & LLMU & 59.5 & 41.4 & 39.5 & 44.7 \\  & SCRUB & 43.8 & 40.4 & 39.3 & 51.2 \\  & SSD & 50.2 & 33.8 & 35.0 & 40.7 \\  & RMU & 29.7 & 47.1 & 28.1 & 57.5 \\  & **ECO (Ours)** & **24.7** & **26.5** & **24.4** & **58.9** \\   & Original & 76.2 & 56.9 & 56.9 & 72.8 \\  & Prompting & 43.0 & 36.0 & 47.2 & 61.0 \\  & RMU & 31.0 & 54.7 & 27.9 & 71.0 \\  & ECO (Ours) & **25.9** & **24.0** & **25.3** & **72.8** \\   & Original & 71.6 & 53.4 & 51.9 & 67.7 \\  & Prompting & 46.4 & 37.0 & 47.7 & 61.9 \\  & RMU & 32.0 & 52.7 & 31.4 & 66.1 \\  & ECO (Ours) & **25.0** & **23.4** & **26.4** & **67.2** \\   & Original & 77.3 & 56.6 & 52.6 & 73.9 \\  & Prompting & 56.4 & 45.6 & 42.5 & 69.8 \\  & ECO (Ours) & **26.7** & **23.9** & **24.1** & **73.9** \\   & Original & 76.5 & 57.4 & 48.9 & 74.7 \\  & Prompting & 54.4 & 44.9 & 46.3 & 71.2 \\   & ECO (Ours) & **23.2** & **27.0** & **23.8** & **74.7** \\   & Random guess & 25.0 & 25.0 & 25.0 & 25.0 \\   

Table 1: **Multiple-choice accuracy of five LLMs on the WMDP benchmark (forget) and the full MMLU (retain) after unlearning. ECO achieves accuracy close to random guessing on all subsets of the WMDP benchmark (as desired), and has zero decrease in accuracy on MMLU. Other baselines either struggle to forget or incur substantial decrease in MMLU.**

    &  &  \\   & **Economics (\(\))** & **Law (\(\))** & **Physics (\(\))** & **Econometrics (\(\))** & **Jurisprudence (\(\))** & **Math (\(\))** \\  Original & 58.1 & 45.0 & 41.8 & 47.4 & 74.1 & 34.6 \\ Prompting & 61.5 & 41.1 & 41.6 & 43.0 & 66.7 & 33.0 \\ RMU & 27.3 & 27.8 & 27.0 & 41.2 & 37.0 & 29.2 \\
**ECO** & **20.6** & **24.5** & **23.1** & **47.4** & **74.1** & **34.6** \\  Random guess & 25.0 & 25.0 & 25.0 & 25.0 & 25.0 & 25.0 \\   

Table 2: **Multiple-choice accuracy of Zephyr-7B after unlearning, on three MMLU subsets and the corresponding retain sets. The prompting baseline hurts the accuracy on the three forget subsets. While RMU reduces the forget set accuracy to the level of random-guess, it incurs substantial performance decrease on econometrics and jurisprudence while unlearning economics and law. ECO achieves both perfect retaining and unlearning on all subsets.**and MMLU subsets, using the same corruption function and hyperparameters obtained on Zephyr-7B. Our results in Table 21 and Table 22 further demonstrate that our method is universally effective without requiring hyperparameter tuning.

### Copyrighted Content Unlearning

Experimental setupWe select _Harry Potter and the Sorcerer's Stone_6 and BBC News articles7 as the copyrighted content material for unlearning and unlearn models fine-tuned on the text corpus. For this task, our goal is to prevent the unlearned model from generating passages with high similarity to the original text. For both datasets, we verify that the models used cannot generate the original passage and that the generated text has low similarity to the original passage. We first fine-tune a pre-trained model on the corresponding corpus and use it as the model subject to unlearning, with the original pre-trained checkpoint serving as the retained model. We use the original passage as the reference text and measure the text similarity between the reference and the text generated by the unlearned model using four text similarity metrics outlined in Appendix C.1.3, which we denote as the average similarity gap (ASG). Following , we also compute the perplexity and unique token ratio to assess whether the generated text remains meaningful and diverse. We compare our method to baselines in , SCRUB , and LLMU . We present full experimental details in Appendix C.

ECO maintains high similarity to the retained model.In Section 4.4, ECO achieves scores sufficiently close to those of the retained model in terms of generated text similarity. On the general utility metric, our prompt classifiers effectively distinguish copyrighted content from general domain queries with no performance loss. KL minimization and LLMU are strong baselines in terms of similarity gap and general utility, but the diversity of the generated text decreases after unlearning. Both gradient difference and random mismatch reduce the issue of model collapse but still lead to notable performance losses in general utility.

We further validate our findings on a total of 19 models in Appendix E.5, spanning from Table 23 to Table 60. We observe that some baselines cannot consistently maintain strong results in either unlearning or general utility, while ECO remains stable and consistently achieves a low similarity gap with the retained model and unharmed utility.

## 5 Related Work

Unlearning for LLMsMost existing machine unlearning methods for LLMs follow traditional machine unlearning approaches [119; 101; 119] to minimize the influence of the forget samples via

  
**Dataset** & **Method** & **ASG (\(\))** & **Utility (\(\))** & **PPL (\(\))** & **Unique Tok \% (\(\))** \\   & Original & 71.2 & 53.3 & 1 & 61 \\  & Retain & 0 & 59.2 & 3 & 28 \\  & Fine-tune & 48.3 & 53.2 & 1.7 & 58.3 \\  & GA & 12.4 & 33.1 & - & 0.8 \\  & GD & 26.3 & 41.2 & - & 1.5 \\  & KL & 6.5 & 48.9 & 1.8 & 28.4 \\  & Mismatch & 3.9 & **53.5** & 20.7 & 65.7 \\  & SCRUB & 12.7 & 33.9 & - & 2.3 \\  & LLMU & 18.4 & 49.1 & 1.6 & 38 \\  & ECO(Ours) & **15.5** & 53.3 & **1.5** & 50.4 \\   & Original & 74.7 & 52.6 & 1.1 & 63.4 \\  & Retain & 0 & 59.2 & 2.3 & 18 \\  & Fine-tune & 7.9 & 50.2 & 7.3 & 42.4 \\  & GA & 23.4 & 32.2 & - & 3.4 \\  & GD & 2.5 & 50.6 & 7.3 & 36.1 \\  & KL & **1** & 47.4 & 1.5 & 22.8 \\  & Mismatch & 8.2 & 50.4 & 6.9 & 40.3 \\  & SCRUB & 7.1 & 32 & - & 2.2 \\  & LMMU & 2.3 & 46.7 & 1.6 & 20 \\  & ECO(Ours) & 2.1 & **52.6** & **1.2** & **51.1** \\   

Table 3: Comparison of our method and the baseline methods to the retained model on two copyrighted content unlearning tasks. The results are obtained from unlearning OLMo-7B  models fine-tuned on the relevant corpus. ECO consistently maintains high similarity to the retained model (in average similarity gap (ASG)) and generates meaningful and diverse outputs (reflected by perplexity (PPL) and unique token ratio), while having no performance loss on utility.

gradient updates. The most straightforward approach employs a mixture of forgetting and retaining objectives by performing gradient ascent updates on the non-desirable sequences and regular gradient descent on the desirable sequences [137; 143; 23; 142; 73; 149; 62]. Other methods identify and modify a small fraction of the weights responsible for the undesired behavior [139; 13; 58], or use weight arithmetic [148; 56; 102; 83]. The above optimization-based methods all require compute that scales with the model size. Our method leaves the LLM subject to unlearning intact and unlearans by steering the inputs to match the output distribution of a retained model. Compute-wise, our unlearning method is independent of the model size.

LLM guardrailsGuardrailing, which accesses prompts before using them as inputs to the model, has been widely applied to modern LLMs to prevent adversaries with harmful incentives [111; 59; 145; 92; 71; 32; 138; 59; 47; 25]. Our work is most related to in-context unlearning  and a recent guardrail baseline via prompting , both of which require no additional fine-tuning to achieve unlearning to some extent.  leverages modern LLMs' ability in in-context learning by prepending a small number of positive and negative samples in the prompt to steer the model's response based on those samples.  guards the unlearning target via prompt injection, which inserts fixed instructions in the prompt to the LLM. Both methods can only be applied to instruction-tuned models and rely on an LLM's ability to follow instructions. Prepending such instructions also leads to significant performance degradation on regular tasks, as shown in .

Jailbreak via adversarial embeddingsPrior work on LLM jailbreaking [155; 40; 76; 44; 105] has demonstrated the power of adversarially optimizing toward a prompt that elicits a desired LLM response. In particular,  shows that the attack can be simplified to learning perturbation vectors added to the token embeddings, which eliminates the need to optimize over discrete tokens. Our results on the behavior of the attacked models are similar to the findings in , where inserting certain non-natural language token sequences in the prompt could elicit refusal behavior or incorrect answers from an instruction-tuned LLM. While jailbreak approaches can theoretically be applied in unlearning applications, they are prohibitively expensive to run , and there is an additional requirement for specifying a sequence of desirable tokens. Both requirements make them unsuitable for the task of unlearning.

## 6 Conclusion

In this paper, we introduced Embedding-COrupted (ECO) Prompts, a novel method to tackle the dual challenges of knowledge entanglement and unlearning efficiency in LLMs. ECO leverages a thresholded prompt classifier to determine whether a prompt falls within the scope of the unlearning target. By decoupling the unlearning process from the LLMs themselves, ECO offers a scalable and efficient approach that remains effective across a wide range of model sizes, from 0.5B to 236B parameters, with minimal side effects and no additional computational overhead. Our experiments across three unlearning tasks validate ECO's effectiveness, setting a foundation for responsible AI deployment in real-world scenarios.