# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

theory beyond the basic single-shot setting and into the realm of repeated contracts, applying a fresh perspective to contract design for repeated interactions.

Repeated contracts have been studied extensively in Economics. The literature explores many possible variations of how outcomes, actions, and contracts may evolve over time as the principal and agent interact (see, e.g.,  and references therein; for surveys, see , [14, Chapter 10], and [55, Chapter 8]). The main theme of this literature is that the incentive problem grows significantly in complexity with repetition. First, the agent's action set becomes extremely rich, and optimizing over it is highly non-trivial. Second, the optimal contract itself typically becomes excessively complex - "_too complex to be descriptive or prescriptive for incentive contracting in reality_" . Given this complexity, agents typically respond to repeated strategic interactions in practice in a manner consistent with no-regret learning  (see Appendix A for a more detailed overview).

Building on this, we revisit the classic question of optimal contract design in a repeated setting, this time considering a no-regret learning agent. Hence, the main question we address in this work, referring to the _optimal dynamic contract_, is:

_If the principal knows that the agent is a no-regret learner, what contract sequence should she offer?_

### Our Model and Contribution

**Optimizing against a no-regret learner.** We study the optimal dynamic contract in the following setting: A principal and agent interact over \(T\) time steps for some large \(T\). In each step \(t[T]\), the agent takes a costly action as recommended by the no-regret learning algorithm, and the principal pays the agent according to the current contract and the action's outcome. The contracts can be modified by the principal over time dynamically (and adaptively). A simple benchmark is achieved by _not_ modifying them, that is, simply repeating the optimal one-shot contract in each round. We refer to this as the _optimal static contract_. It is not hard to see that the principal's revenue in this case against a no-regret agent will essentially be the optimal static revenue (Observation I.1 in Appendix I).

Our main focus is on "mean-based" learning agents, who apply simple, natural and common learning algorithms, such as multiplicative weights , follow the perturbed leader , or EXP3 . Intuitively, mean-based algorithms consider the cumulative payoffs from each of the actions, and play actions which performed sub-optimally in the past with a low probability (see Section 2 for a precise definition, taken from ).

We also briefly consider more sophisticated agents who utilize _no-swap-regret_, rather than mean-based, learning (e.g., ). Against such agents, a crisp optimal strategy is immediate from previous work on general repeated games against learners : It is known that the best static solution is also the best dynamic one. In our context this means that no dynamic contract can achieve better than the optimal static contract (Observation I.2 in Appendix I). Since no-swap-regret learning also counts as a particular type of no-regret learning, this result also explains why focusing on mean-based learning is necessary for a separation result. Interestingly, we show that both players can be better off if the agent applies more naive, mean-based learning (although, as expected, there are also many cases where the agent winds up worse off due to this interaction). Due to space constraints, we defer a more extended discussion of related work to Appendix A and highlight here our contributions.

**Our contribution.** In this paper, we give a clean, tractable answer to our main question as follows. When the agent's choice among \(n\) actions can lead to success/failure of the project, the principal's optimal dynamic contract is surprisingly simple (especially compared to the optimal dynamic _auction_): offer the agent one carefully-designed contract for a certain fraction of the \(T\) rounds (both contract and fraction are poly-time computable), then switch to the zero contract (that is, pay the agent nothing) for the remaining rounds. In this setting, simple _linear_ (single-parameter) contracts are optimal, and this is actually the only property required for our result. Our analysis thus generalizes to any setting where the principal utilizes only linear contracts. Note that much of the previous literature on algorithmic contract design has focused on the canonical contract setting with success/failure outcomes and/or on the ubiquitous class of linear contracts (see, e.g., ).

Unlike the optimal dynamic _auction_, the optimal dynamic contract divides the welfare among the principal and agent. In fact, it can increase the utility of both players (and thus also the total welfare) in comparison to the optimal static contract. One interpretation of this result is the following. While a no-swap-regret algorithm is better for the agent against an adversarial player, an agent who commits to using a mean-based algorithm allows the principal more freedom to dynamically implement outcomes of _common interest_. A similar advantage of simple no-regret learning over no-swap-regret was noted in a different context by .

Our main result also generalizes to settings with a rich set of outcomes beyond success/failure, as long as the principal changes the contract dynamically by scaling it ("single-dimensional scaling"). However, we also show that absent this single-dimensional scaling restriction, there exist principal-agent instances where the optimal dynamic contract does not take this form.

Equivalently, with non-linear contracts it is possible for the principal to do strictly better than offering the same contract for several rounds and then switching to the zero contract.

As our second main result, we identify and address the following gap in the current literature on optimizing against no-regret learners: Implicit in all our positive results, as well as in all known results in this literature (see Appendix A), is the assumption that the optimizer knows the time horizon \(T\). We show that when there is (even limited) uncertainty about \(T\), the principal's ability to use dynamic contracts to guarantee more revenue than the optimal static contract diminishes. We achieve this by characterizing the optimal dynamic contract under uncertainty of \(T\), and showing that the principal's added value from being dynamic sharply degrades with an appropriate measure of uncertainty.

### Illustrative Example

To demonstrate our model and findings, we give an example where a simple dynamic contract yields higher revenue than the best static contract. The analysis requires some familiarity with the basics of contract settings, which appear in the first paragraphs of Section 2 for completeness. Consider the setting in Figure 1. There are three actions with costs \(c_{1},c_{2},c_{3}\) for the agent, leading, with the probabilities shown in the figure, to two outcomes--"failure" and "success"--with rewards \(0\) and \(1\) for the principal. Since there are two outcomes, w.l.o.g. we can consider linear contracts, which pay the agent \(\) for success (leaving the principal with a payoff of \(1-\)).

Under an optimal static linear contract, the agent must be indifferent either between actions \(1\) and \(2\) or between actions \(2\) and \(3\) (otherwise, the principal is overpaying for incentivizing an action). The indifference contracts are denoted by \(_{1,2}=1/3\) and \(_{2,3}=2/3\), respectively. These lead to the same expected utility for the principal, where the expectation is over the probability of success: \((1-_{1,2})=(1-_{2,3}) 1=\). That is, both \(_{1,2}\) and \(_{2,3}\) are optimal static contracts.

Now consider a principal interacting with a mean-based learning agent. The principal initially offers the contract \(=+\) for \(T/2\) time steps, with some small \(>0\). The agent follows his mean-based strategy and plays action \(3\) (in a \(1-o(T)\) fraction of the time with high probability), which yields a utility of roughly \( 1-c_{3}=\) per step for the agent and \(\) per step for the principal. Subsequently, the principal switches to the zero contract for the remaining time steps. From the perspective of the agent, at the time of the switch the cumulative utilities of actions \(2\) and \(3\) are roughly \(\) (compared to zero from action \(1\)). But in every step of the subsequent stage, the cumulative utility of action \(3\) is degraded by an amount of \(\) and the cumulative utility of action \(2\) is degraded by an amount of \(\). Thus the agent "falls" to action \(2\) and plays it until the last period \(T\). The overall utility for the agent is approximately zero, and for the principal \(+=T> T\). The principal thus improves her utility by a factor of \(\) compared to the optimal static contract.

Figure 2 shows a graphical representation of the above dynamic contract. It turns out that this simple "free-fall" contract (see Definition 2.1) is also an optimal dynamic strategy for the principal, and in fact, this is not a special property of the current example. In Section 3 we show that for any linear contract game, there exists an optimal strategy with this form: offer a fixed contract for a period of \( T\) steps and then switch to the zero contract. Moreover, we show that, surprisingly, this simple structure remains optimal also for general non-linear contracts, as long as their dynamics are characterized by a single scalar parameter. For details, see Appendix D.

Figure 1: A canonical contract setting in which a simple dynamic contract extracts higher expected revenue than the best static contract. The table entries show the outcome probabilities given the actions.

### Summary of Results and Roadmap

We initiate the study of repeated principal-agent problems with a learning agent; the key takeaways from our work are as follows:

**Theorem** (See Theorem 3.1 in Section 3.1).: _In success/failure settings, as well as in arbitrary contract settings where the principal restricts to linear contracts, the optimal dynamic contract against a mean-based agent is a free-fall contract. This optimal dynamic contract can be efficiently computed._

**Theorem** (See Theorem 3.2 in Section 3.2).: _Consider the space of repeated principal-agent problems; in a subset of this space of positive measure, both the principal and agent achieve unboundedly better expected utilities from the principal's optimal dynamic contract compared to the optimal static contract._

One takeaway from the previous theorem is that the principal and agent can _both_ benefit when the agent commits to mean-based rather than no-swap-regret learning, in stark contrast to auctions (where a buyer committing to a mean-based strategy is left with zero payoff, because the auctioneer can extract the full surplus). We generalize our findings on the optimality of free-fall contracts to any dynamic contract with _single-dimensional scaling_.

**Theorem** (See Theorem D.1 in Appendix D).: _In arbitrary contract settings, there is a free-fall contract that is optimal among dynamic contracts with single-dimensional scaling. A dynamic contract has single-dimensional scaling if it starts from an arbitrary contract \(_{ 0}^{m}\), and at every time step \(t[T]\) plays \(^{t}\) for some scalar \(^{t} 0\)._

In Section 3.3 and Appendix G, we demonstrate that in absence of single-dimensional scaling, there may not exist a free-fall contract that is optimal among dynamic contracts.

Finally, we investigate the impact of an unknown time horizon when optimizing against a no-regret learner. To the best of our knowledge, we are the first that explore this aspect. Of course, there are standard techniques to help a learning _agent_ can guarantee no-regret without knowing the time horizon (see e.g., (22, Section 2.3)). Typically, this agent manages to achieve with the same asymptotic (albeit with a larger constant factor) additive regret term when moving from the known to unknown time horizon setting. We show that the situation is different on the other side; uncertainty on the part of the _principal_ regarding the time horizon significantly degrades her ability to outperform the best static contract. Whereas the agent's uncertainty was affecting their \(o(T)\) factors, the principal's uncertainty degrades their \((T)\) factor; although some slight advantage is still always possible.

Figure 2: Two representations of the same dynamic contract, as applied to the contract setting described in Figure 1 and repeated for \(T\) steps. The dotted red curve on the left describes the _cumulative_ contract at time \(t\) as a function of \(t\), with both axes normalized by \(T\). The shaded areas represent the mean-based best-response regions for the agent. The lines \(_{1,2}\) and \(_{2,3}\) are the _indifference curves_ between these regions. The right diagram depicts the same dynamic contract as a function of the fraction of total time \(t/T\), where here the vertical axis represents the _average_ contract at time \(t\). Pictorially, after steadily building the agent’s incentives until time \(T/2\), the principal cuts payments. During the remaining time, the agent “free-falls” through action regions.

**Theorem** (See Theorems 4.2-4.3 in Section 4).: _For any contract problem and error-tolerance parameter \(>0\), there exists is some minimum time uncertainty \(\) so that for any minimum time horizon \(\), no randomized dynamic contract can guarantee the principal \(a\)\((1+)\) multiplicative advantage over the optimal static contract simultaneously for every time horizon \(T\) in the range \([,]\). Conversely, for any contract problem and time uncertainty \(>1\), there is some nonzero error-tolerance parameter \(>0\) such that for a sufficiently large time horizon minimum \(\), there is a randomized dynamic contract that can guarantee the principal \(a\)\((1+)\) multiplicative advantage over the optimal static contract simultaneously for every time horizon \(T\) in the range \([,]\)._

## 2 Model

We first present basic (non-repeated) contracts, and the class of linear contracts; a familiar reader may wish to skip to Sections 2.1-2.2 on repeated contract settings (discrete and continuous).

Single-shot contract setting.There are two players, a _principal_ and an _agent_. The agent has a finite set \([n]\) of \(n>1\)_actions_, among which it chooses an action \(a\) and incurs a corresponding _cost_\(c_{a} 0\) (in addition to \(a\) we will use \(i\) to index actions). W.l.o.g. the actions are sorted by cost (\(c_{1}<c_{2}<...<c_{n}\)) and the cost of the first (_null_) action is zero (\(c_{1}=0\)). There is a finite set \([m]\) of \(m>1\) possible _outcomes_, and every action \(a\) is associated with a probability distribution \(F_{a}^{m}\) over the outcomes. The null action leads with probability \(1\) to the first (_null_) outcome. Every outcome \(o\) is associated with a finite _reward_\(r_{o} 0\) for the principal (in addition to \(o\) we will use \(j\) to index rewards). We assume w.l.o.g. that \(r_{1} r_{2}... r_{m}\) and \(r_{1}=0\). We denote the expected reward by \(R_{a}=_{o F_{a}}[r_{o}]\) for action \(a\). As is standard we assume no dominated actions: \(i)\) if \(c_{a}<c_{a^{}}\) then \(R_{a}<R_{a^{}}\) and \(ii)\) for every action there exists a contract that uniquely incentivizes it. The contract setting (a.k.a. principal-agent problem) \((c,F,r)=(\{c_{a},F_{a}\}_{a=1}^{n},\{r_{o}\}_{o=1}^{m})\) is known to both players.

The game.The game in the basic (non-repeated) setting has the following steps:

1. The principal commits to a _contract_\(=(p_{j})_{j=1}^{m}\), \(p_{j} 0\), where \(p_{j} p_{}\) is the non-negative amount the principal will pay the agent if outcome \(j\) is realized.2 In particular, **p** can be the _zero contract_ in which \(p_{j}=0\) for all \(j\). 2. The agent selects an action \(a[n]\), unobservable to the principal, and incurs a cost \(c_{a}\).
3. An observable outcome \(o\) is realized according to distribution \(F_{a}\). The principal receives reward \(r_{o}\) and pays the agent \(p_{o}\).

The principal thus derives a _utility_ (_payoff_) of \(r_{o}-p_{o}\), and the agent of \(p_{o}-c_{a}\).

Expected utilities and optimality.In expectation over the outcomes, the utilities from contract **p** and action \(a\) are \(u_{P}(,a)=R_{a}-_{o F_{a}}[p_{o}]\) for the principal, and \(u_{A}(,a)=_{o F_{a}}[p_{o}]-c_{a}\) for the agent. Summing these up we get the expected welfare \(R_{a}-c_{a}\) from the agent's chosen action \(a\). For a given contract **p**, let \(()=_{a}u_{A}(,a)\) be the set of actions incentivized by this contract, i.e., maximizing the agent's expected utility (usually this will be a single element, but in the case of ties we include all actions in \((p)\)).3 The goal of the contract designer is to maximize the principal's expected utility, also known as _revenue_. Such a contract is referred to as _optimal_.

**Linear contracts.** In a linear contract with parameter \(\), the principal commits to paying the agent a fixed fraction (compression) \(\) of any obtained reward. Thus by choosing action \(a\), the agent gets expected utility \( R_{a}-c_{a}\), and the principal gets \((1-)R_{a}\). As \(\) is raised from \(0\) to \(1\), the agent's expected utility is affected less by the action cost, and the agent's incentives align more with the principal's and with social welfare. This intuition is formalized by , showing that as \(\) increases, the agent responds with actions that have increasing costs, increasing expected rewards, and increasing expected welfares.4 The _critical_\(\) at which the agent switches from action \(i-1\) to action \(i\) (for \(i>1\)) is denoted by \(_{i-1,i}=(c_{i}-c_{i-1})/(R_{i}-R_{i-1})\), and is also referred to as an _indifference point_ or _breakpoint_. For \(i=1\) we define \(_{0,1}=0\). Using this notation, for every linear contract \((_{i-1,i},_{i,i+1})\), the agent plays action \(i\). In the _linear contract setting_, the focus is on linear contracts and only such contracts are allowed.

### Repeated Contract Setting: Discrete Version

We study a repeated contract setting \((c,F,r,T)\), in which the above game \((c,F,r)\) is repeated for \(T\) discrete rounds between the same principal and agent. The number of rounds \(T\) is called the _time horizon_. The setting is known to both players,5 who update the contracts and actions in each round. The outcomes of the actions are drawn _independently_ per round (past outcomes affect future outcomes only through learning). Denote the contract, action, realized outcome and reward at time \(t[T]\) by \(^{t},a^{t},o^{t},r^{t}\), respectively. The agent's payoff at time \(t\) is \(p^{t}_{o^{t}}-c_{a^{t}}\). The sequence \((^{t})^{T}_{t=1}\) of contracts is called a _dynamic_ contract, and the \(T\) pairs \(\{(^{t},a^{t})\}^{T}_{t=1}\) form the _trajectory of play_. We define the following class:

**Definition 2.1**.: _A free-fall contract is a dynamic contract in which the principal offers a (single-shot) contract \(\) for the first \(T^{} T\) rounds, and then offers the zero contract for the remaining rounds._

Learning agent.The agent's approach to choosing an action is learning-based, by applying a no-regret algorithm (rather than based on myopic best-responding, as in the one-shot setting). Our analysis applies with _full feedback_ on the performance of each action, where the agent observes the expected payoffs of all actions (whether taken or not -- e.g. by observing someone else take that action), or with _bandit feedback_, where the agent observes only the achieved payoff of the action taken. A delicate issue is that, unlike the standard scenario of learning in games, the payments for each action are _stochastic_. Thus, the agent must not only learn which action to take, but also the expected payment from each action. When \(T\) is large enough, the extra learning has a vanishing impact, and does not affect the analysis of players' utilities and strategies.

Our main focus is on the prominent family of _mean-based_ algorithms. The idea behind mean-based algorithms is that they rarely pick an action whose current mean is significantly worse than the current best mean. There exist such algorithms with both full and bandit feedback that are mean-based and achieve no-regret. In our setting, let \(u^{t}_{i}\) be the expected utility the agent would achieve from taking action \(i\) at round \(t\), and let \(^{t}_{i}=_{t^{}=1}^{t-1}u^{t^{}}_{i}\) represent the cumulative utility achievable from action \(i\) up to time \(t\) given the principal's trajectory of play. Then:

**Definition 2.2** ().: _A learning algorithm is \((T)\)-mean-based if whenever \(^{t}_{i}<^{t}_{i^{}}-(T) T\), then the probability that the algorithm takes action \(i\) in round \(t\) is at most \((T)\). We say an algorithm is mean-based if it is \((T)\)-mean-based for some \((T)=o(1)\).6_

Optimal dynamic contract.The design goal in the repeated setting is to find an _optimal_ dynamic contract: a sequence \((^{t})^{T}_{t=1}\) that maximizes the total expected revenue against a learning agent (whether mean-based or no-swap-regret, where in either case we assume the worst-case such learning algorithm). In the linear contract setting, the sequence \((^{t})^{T}_{t=1}\) is composed of linear contracts. If it maximizes the total expected revenue among all linear contract sequences, we say it is the optimal dynamic _linear_ contract. We remark that it is without loss of generality to consider only linear contracts with7\( 1\).

Note that, as described here, the contract sequence is fixed by the principal at the beginning of the game. We refer to such a principal as _oblivious_. If the principal can choose \(^{t}\) as a function of the agent's previous actions, we say the principal is _adaptive_. Our positive results (showing the principal can guarantee at least some amount of utility) hold even for oblivious principals, and our negative results hold even for adaptive principals.

Optimal static contract.In a repeated setting, a _static contract_ is a sequence of contracts in which the same one-shot contract is played repeatedly. The repeated game with a static contract and a regret-minimizing agent is, in the limit \(T\), equivalent to the classic one-shot contract game with a best-responding agent (Observation I.1). A natural benchmark for dynamic contracts is thus the _optimal_ static contract, in which the optimal one-shot contract is played repeatedly.

### Repeated Contract Setting: Continuous Version

To simplify the technical analysis, we now present a continuous version of our repeated contract setting. For the remainder of the paper we will primarily work in the continuous-time model. We emphasize that the reduction to continuous time is for simplicity, and that the key ideas of our proofs are unrelated to it and can be applied to the discrete version of our setting as well.

Reduction to continuous timeIn , the authors consider the problem of strategizing against a mean-based learner in a repeated bi-matrix game, and show it reduces to designing dynamic strategies for a simplified continuous-time analogue (note that the choice of continuous-time analogue is tailored to mean-based learning - it is not intended to be a special case of a general discrete-continuous reduction against any learner). We pursue a similar reduction here, and show (in Theorem 2.4) how to reduce the problem of designing dynamic contracts in the discrete-time setting (Section 2.1), to a simpler problem in a continuous-time setting. We later extend the reduction to settings with an unknown time horizon (see Theorem H.3 in Section 4).

Trajectories of continuous playIn the continuous setting, rather than specifying the trajectory of play by a sequence of \(T\) contracts and responses, we instead specify it by a finite sequence \(\) of tuples \(\{(^{k},^{k},a^{k})\}_{k=1}^{K}\), each representing a "segment" of play where the principal plays a constant contract and the agent responds with a constant action. Here, each \(^{k}_{ 0}^{m}\) represents an arbitrary contract, each \(^{k}_{ 0}\) represents the (fractional) amount of time that the principal presents this contract to the agent, and each \(a^{k}[n]\) represents the action the agent takes during this time. In the linear contract setting, we use the notation \(^{k}\) instead of \(^{k}\). We sometimes refer to \(\) as a contract, by which we mean the dynamic contract composed of \(^{1},^{K}\) for segments of length \(^{1},,^{K}\).

To form what we call a _valid_ trajectory of play against a mean-based learner, the responses \(a^{k}\) of the agent must satisfy certain constraints. Let

\[^{k}=_{k^{}=1}^{k}^{k^{}};\;\;\;}^{k}=_{k^{}=1}^{k}(^{k^{}}^{k^{ }})/^{k}\]

be the total duration of the first \(k\) segments, and the average contract offered by the principal for the first \(k\) segments, respectively. Then each \(a^{k}\) (for \(k>1\)) must satisfy \(a^{k}(}^{k-1})\) and \(a^{k}(}^{k})\). In words, \(a^{k}\) must be a best-response to the historical average contract at both the beginning and end of segment \(k\) (and therefore also throughout segment \(k\)).

The following is a continuous analogue of Definition 2.1.

**Definition 2.3**.: _A free-fall trajectory \(\) is a game trajectory in which \(^{k}=\) for all \(k>1\)._

Optimal trajectoryThe expected utility of the principal along trajectory \(\) is given by

\[()=^{K}^{k}u_{P}(^{k},a^{k})}{ ^{K}}.\]

Let \(U^{}=_{}()\), where the \(\) runs over all valid trajectories of arbitrary finite length. We can think of \(U^{}\) as the maximum possible expected utility of the principal in the continuous setting game. The following theorem (a direct analogue of Theorem 9 in ) connects \(U^{}\) to what is achievable by the principal in our original discrete-time game.

**Theorem 2.4**.: _Fix any repeated principal-agent problem with \(T\) rounds, and let \(U^{}\) denote the optimal expected utility of a principal in the continuous analogue. Then:_

1. _For any_ \(>0\)_, there exists an oblivious strategy for the principal that gets at least_ \((U^{}-)T-o(T)\) _expected utility for the principal against an agent running any mean-based algorithm_ \(\)_._
2. _For any_ \(>0\)_, there exists a mean-based algorithm_ \(\) _such that no (even adaptive_8_) principal can get more than_ \((U^{}+)T+o(T)\) _expected utility against an agent running_ \(\)_._

The proof of Theorem 2.4 closely follows the proof in  and is deferred to Appendix H.

One important thing to note about Theorem 2.4 is that the first part is constructive. In fact, the discrete-time strategy for the principal corresponding to a trajectory \(\) is essentially the straightforward extrapolation, which plays each contract \(^{k}\) for \(}{^{}K}T\) rounds (although a slight perturbation is necessary to account for segments with a non-unique best-response). This means that when we show, in Theorem 3.1, that the utility-optimizing \(\) for \(U^{*}\) takes the form of a free-fall trajectory, we are simultaneously showing that a free-fall dynamic contract is asymptotically optimal in the original discrete-time setting.

Note that all the above definitions (and the reduction of Theorem 2.4) extend to the specific case where the learner is only allowed to use _linear contracts_. In this setting, we will write \(^{k}=_{k^{}=1}^{k}^{k^{}}^{k^{ }}/_{i=1}^{k}^{k^{}}\) in place of \(}^{k}\).

## 3 Linear Contracts

In this section we focus on the case where the principal restricts to using only linear contracts in every step of the interaction with the agent (one example is when there are \(m=2\) outcomes, such as success and failure; in this case, arbitrary contracts can be described as linear contracts). We begin, in Section 3.1, by showing that without loss of generality, optimal dynamic contracts take the form of free-fall contracts, and in Appendix D we generalize this result to a broader class of general contracts with single-dimensional scaling. Then, in Section 3.2, we analyze the implications of optimal free-fall contracts on the welfare and on the agent's utility. In particular, we show that dynamic contracts that are optimal for the principal can improve the utilities for both players compared to their utilities under the best static contract. Finally, in Section 3.3, we show that for unrestricted dynamic contracts, free-fall contracts may no longer be optimal.

### Free-Fall Contracts are Optimal Linear Contracts

The following theorem shows that free-fall contracts are optimal dynamic linear contracts.

**Theorem 3.1**.: _Let \(\) be any linear dynamic contract. Then, there exists a free-fall linear contract \(^{}\) where \((^{})()\), and which can be computed in time polynomial in the problem size._

The proof is deferred to Appendix B and hinges on applying a sequence of "rewriting" rules which allow us to gradually transform any given linear dynamic contract \(\) with a free-fall linear contract \(^{}\). At a high level, the crux of the proof is that any segment of the trajectory can be thought of as a combination of "stalling" at the current action and "falling" to the action below. Under linear contracts, the principal prefers to stall when a higher action is being induced. Grouping together all the stall at the highest action used exactly results in a free-fall contract.

### Implication to Welfare and Agent's Utility

In the example shown in Section 1.2, the free-fall dynamic manipulation that the principal made degraded the overall welfare, and all the added profits for the principal were at the expense of the agent. We demonstrate that this is not always the case; there are other scenarios where dynamic manipulations where the principal, optimizes her revenue, can actually be Pareto improvements over the best static contract, increasing the overall welfare.

**Example** (Welfare improvement).: _Consider the setting depicted in Figure 1 with a slight variation where the cost of action \(2\) is \(1/2+\), with \(<1/(2T)\). In this case, the best static contract incentivizes action \(1\) and yields a utility of \(1/3\) for the principal and zero for the agent. However, the best dynamic contract remains the same as in the previous analysis: it starts by incentivizing action \(2\) for a period of \(T\) steps and then transitions to action \(1\) by offering zero payments for the remaining time. This results in a utility of \(T\) for the principal and zero for the agent, thereby increasing welfare by a factor of \(5/4\) without altering the agent's utility._

Next, we show the existence of "win-win" scenarios where optimal dynamic contracts can enhance the payoffs for both the principal and the agent compared to the best static contract. The improvement in welfare can be substantial, reaching as much as \((n)\), essentially achieving full welfare. Specifically, we establish that the multiplicative gap between the utilities of the best static contract and those of the best dynamic contract can be \((n)\) for the principal's utility and \(((n))\) for the agent's utility.

**Theorem 3.2** (Win-win optimal dynamic contracts).: _There exist repeated contract settings where an optimal dynamic contract improves expected welfare by a \((n)\) multiplicative factor comparedto the best static contract, and where the agent's expected utility improves by a factor of \(((n))\). Moreover, these settings have a positive measure in the space of repeated contract games._

The idea of the proof is to look at games where the values for the principal when incentivizing each action are similar, but the actions differ significantly in terms of welfare. Then, by investing a small amount of additional payment in the early stages of the game (compared to the best static contract), the principal can incentivize the agent to substantially improve welfare, initially in the form of higher profits for the agent. This added welfare is then shared between the players during the free-fall stage of the dynamic.

The proof is deferred to Appendix C. One interesting point about the example presented in the proof is that if the agent had used a "smarter" learning algorithm that guaranties low swap regret, then the outcome of the best static contract would have been obtained (see Observation I.2 in the appendix, following the analysis of ). The agent in this case would have had lower utility. That is, using a better algorithm leads to a worse outcome! The explanation for this counter-intuitive result is that a mean-based regret-minimizing algorithm is only guaranteed to approach the set of Coarse Correlated Equilibria (CCE),9 whereas no-swap-regret dynamics must approach the set of Correlated Equilibria (CE, a subset of the set of CCE's). There are games in which some CCE distributions of play give higher utilities to the players than all CE distributions (see e.g., [36; 53] for examples in auctions, and  for a related example in general games). In other words - committing to use an algorithm with weaker worst-case guarantees yields better (non-worst-case) results.

### General Contracts and Free-Fall

Unlike in the linear contract setting and the single-dimensional scaling setting, free-fall contracts are _not_ optimal in the general contract setting. We provide an example outlining this in Appendix G. In fact it is an open question whether the the optimal dynamic contract is computable.

## 4 Unknown Time Horizon

Up until now, the principal has been able to take advantage of precisely knowing the time horizon. Notably, this assumption of knowledge of the time horizon underlies all prior theoretical results in the literature on optimization against learning algorithms. In this section, we explore what happens when the principal only approximately knows this parameter. We will consider the case where the principal knows that the time horizon \(T\) falls into some range \([,]\), and wants to guard against a worst-case choice of time horizon from that range. What are the trade-offs between the time uncertainty and how much additional principal utility we can get over the best static contract? To explore these concepts precisely, we introduce the following definition:

**Definition 4.1**.: _Suppose we have a principal-agent problem \((c,F,r)\). Let \(R_{*}\) be the single-round profit of the optimal static linear contract for this problem. We say that a pair \((,)\) is feasible with respect to \((c,F,r)\) if for all sufficiently large time horizons \(\), there exists a (potentially randomized) principal algorithm \(A\) such that the (expected) profit of \(A\) at any time \(t[,=[]]\) is at least \((1+)tR_{*}\) (and infeasible with respect to \((c,F,r)\) otherwise)._

The last part of our results is Theorem 4.2: for every principal-agent problem and any error-tolerance \(>0\), it is impossible to indefinitely maintain an \(\) advantage over the optimal static contract. To be more precise, when \(\) is \(((1/))\) we know that the instance has become \((,)\) infeasible (for some instances, this infeasibility transition may occur earlier). We argue this via a potential function; in order to stay a constant factor ahead of the optimal static contract, the principal must be constantly giving up potential. To complement this result, in Theorem 4.3, we show that all time ratios \( 1\) permit some advantage \(\) over the optimal static contract. We manage to achieve \(\) at least \((1/())\) for all problems where the optimal dynamic contract (with known time horizon) outperforms the optimal static contract. These ideas are captured in the theorems below:

**Theorem 4.2**.: _Suppose we have a principal-agent problem \((c,F,r)\). For every \(>0\), there exists a \(\) such that \((,)\) is infeasible with respect to \((c,F,r)\) for all \(\)._

**Theorem 4.3**.: _Suppose we have a principal-agent problem \((c,F,r)\). If there exists a \(\) such that for any \(>0\) and \(\), \((,)\) is infeasible, then there are no dynamic strategies that outperform the optimal static linear contract._

The proof of Theorem 4.2 is in Appendix E; Theorem 4.3, Appendix F. The key ideas are as follows. For Theorem 4.2, we construct a potential function which assigns a value to the current time-averaged linear contract, and show that any principal is forced to slowly sacrifice this potential as the possible time horizon gap grows. For Theorem 4.3, any dynamic strategy that outperforms the optimal static linear contract can also be made to either start or end at the optimal static linear contract. This allows us to pad such a strategy to last for a longer amount of time by adding a segment that just stalls at the optimal static linear contract. One technical issue we have to handle is that trajectories must be evaluated over the interval \([^{K},^{K}]\) instead of at a single time; this multidimensionality means we must now consider distributions of trajectories (see Appendix H).

## 5 Conclusion

In this paper, we provide a clean and tractable answer to our main question. When the agent's choice among \(n\) actions can lead to the success or failure of a project, the principal's optimal dynamic contract is surprisingly simple. Specifically, the principal should offer a carefully designed contract for a certain fraction of the \(T\) rounds (both the contract and the fraction are poly-time computable), then switch to a zero contract (i.e., pay the agent nothing) for the remaining rounds. Our main result also generalizes to settings with a rich set of outcomes beyond success/failure, as long as the principal changes the contract dynamically by scaling it ("single-dimensional scaling"). However, we show that without this single-dimensional scaling restriction, there exist principal-agent instances where the optimal dynamic contract does not take this form. In these cases, with non-linear contracts, the principal can do strictly better than offering the same contract for several rounds before switching to a zero contract.

As our second main result, we address a significant gap in the current literature on optimizing against no-regret learners: the assumption that the optimizer knows the time horizon \(T\). We show that when there is uncertainty about \(T\), even if limited, the principal's ability to use dynamic contracts to guarantee more revenue than the optimal static contract diminishes. We characterize the optimal dynamic contract under uncertainty of \(T\), demonstrating that the principal's added value from being dynamic sharply degrades with an appropriate measure of uncertainty.

**Open Problems.** The computational study of repeated contracts, particularly with learning agents, raises many open questions. These include determining the optimal dynamic contract when the principal is not restricted to one-dimensional dynamics, and the computational complexity of finding it. Additionally, it involves identifying the optimal dynamic contract against a learning agent with a hidden type, thereby unifying our contract model with the auction model of . Another intriguing area is understanding what the optimal dynamic contract would be against a team of multiple learning agents. Finally, it is crucial to explore the effects on welfare and utilities when there are two learning players, rather than a learner and an optimizer.