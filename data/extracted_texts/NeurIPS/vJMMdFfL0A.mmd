# The Benefits of Balance:

From Information Projections to Variance Reduction

 Lang Liu

These authors contributed equally to this work.

Ronak Mehta1

Soumik Pal

Zaid Harchaoui

University of Washington

###### Abstract

Data balancing across multiple modalities and sources appears in various forms in foundation models in machine learning and AI, _e.g._ in CLIP and DINO. We show that data balancing across modalities and sources actually offers an unsuspected benefit: variance reduction. We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators. Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint.

## 1 Introduction

Deep neural networks have shown remarkable success at learning task-specific representations of data when provided supervision from massive amounts of labeled training examples. Recent trends, however, have shifted toward task-agnostic, universal representations that may be easily fine-tuned or even have zero-shot capabilities out of the box. Supervised learning, _stricto sensu_, is too limited a framework for these billion-parameter, data-hungry models, and a question at the heart of modern machine learning is learning from unlabeled, partially labeled, or weakly labeled data.

This need has paved the way for the current generation of self-supervised learning (SSL) approaches that circumvent the need for large amounts of "strong" labels. In SSL, a model is trained on a generic pseudo-task suited for unlabeled data, such as relating image-caption pairs or augmentations of the same image. Despite modern foundation models such as DINO  and CLIP  being trained in this fashion, many aspects of SSL remain mysterious.

In particular, the training process of self-supervised models often transcends the rules of the standard empirical risk minimization (ERM) toolkit. ERM combines two well-understood techniques: minibatch sampling and gradient-based optimization using backpropagation. On the other hand, SSL adds clever, yet less-understood techniques to the training pipeline. To illustrate this, consider a minibatch of independent and identically distributed (i.i.d.) training examples \((X_{1},Y_{1}),,(X_{n},Y_{n}) P\), where \(P\) is a joint probability measure on sample spaces \(\) (e.g. feature-label or image-caption pairs) and let \(P_{n}=_{i=1}^{n}_{(X_{i},Y_{i})}\) be the empirical distribution. For a model parameterized by \(^{d}\) with loss function \(h_{}\), a stochastic learning algorithm involves computing the minibatch loss

\[_{P_{n}}[h_{}(X,Y)]=_{i=1}^{n}h_{ }(X_{i},Y_{i})\] (1)

and backpropagating through it to produce a minibatch stochastic gradient estimate. The algorithm then proceeds with the stochastic gradient descent (SGD) or a variant thereof (e.g., Adam, SGD with momentum, etc). Self-supervised methods often modify this recipe by _intervening_ on the optimization algorithm in a minibatch-specific way.

For example, SwaV (Caron et al., 2020) passes the minibatch examples through the model's encoder and clusters output vectors to generate pseudo-labels for a prediction task. In teacher-student architectures such as BYOL (Grill et al., 2020) and DINO (Caron et al., 2021), the minibatch is passed through two networks, where the "student" network is updated via backpropagation and the "teacher" network is updated by cloning the student's weights in regular intervals. In CLIP (Radford et al., 2021), a model optimizes the sum of two cross-entropy losses, where the predicted class probabilities on example \(i\) are generated by comparison to all other elements of the minibatch. While introducing such interventions into the procedure has clearly proven useful practically, it remains conceptually unclear what exactly is being optimized by the learning algorithm.

In this work, we aim to gain a better theoretical understanding of the objectives and algorithms underlying these empirically effective recipes. In particular, we want to shed a theoretical light on their precise benefits over traditional learning methods. We show that such recipes often enjoy an unsuspected benefit: reducing the variance of the empirical minibatch objective.

Concretely, we formalize the model updates described above as two phrases. Let \(Z_{1},,Z_{n}\) be a minibatch containing data points of arbitrary type (e.g. unlabeled images). In the first phase, this original data source is mapped (possibly using a model parameterized by \(\)) to another minibatch \((X_{1},Y_{1}),,(X_{n},Y_{n})\) of _derived_ pairs in \(\). For example, in SwaV, each \(Z_{i}\) is an image, and we derive \((X_{i},Y_{i})\) by setting \(X_{i}=Z_{i}\) and letting \(Y_{i}\) be the pseudo-label based on clustering the vector representations of the images. In CLIP, each \(Z_{i}\) is an image-caption pair, and we derive \((X_{i},Y_{i})\) by simply letting \(X_{i}\) be the image and \(Y_{i}\) be the caption. Note that \(Y_{i}\) is _not_ a label in the traditional sense in neither of these examples. In the second phase, we use the model to compute a probability distribution \(P_{n,}\) over \(\), and perform a stochastic gradient update for the objective

\[_{P_{n,}}[h_{}(X,Y)].\] (2)

This reduces to empirical risk minimization on the minibatch objective (1) when \(Z=(X,Y)\) (each data point is originally observed in \(\)) and \(P_{n,}=P_{n}\) (the empirical distribution of the data is used, regardless of the model). Beyond this setting, one specific example of \(P_{n,}\) has been applied across various families of self-supervised learning (as detailed in Sec. 2), which we refer to as _data balancing_ or simply _balancing_, the primary subject of this work.

For a probability measure \(Q\) on \(\), let \(Q_{X}\) and \(Q_{Y}\) be the respective marginals on \(\) and \(\) and let \(Q_{X|Y}\) and \(Q_{Y|X}\) denote the respective conditional distributions. Given fixed _target_ marginal distributions \(P_{X}\) on \(\) and \(P_{Y}\) on \(\), balancing refers to repeatedly applying the operations

\[R*{arg\,min}_{Q:Q_{X}=P_{X}}(Q\|R)  R*{arg\,min}_{Q:Q_{X}=P_{Y}} {KL}(Q\|R),\] (3)

in an alternating fashion. After enough iterations, the resulting probability measure approximately marginalizes to \(P_{X}\) and \(P_{Y}\) in each variable. When \(\) and \(\) are finite with \(||=m\) and \(||=l\), these operations reduce to rescaling the rows of an \((m l)\)-matrix by \(P_{X}/R_{X}\) or its columns by \(P_{Y}/R_{Y}\). This algorithm has a decades-old history and is known in other contexts as Sinkhorn-Knopp matrix scaling (Sinkhorn, 1967), iterative proportional or biproportional fitting (Johnston and Pattie, 1993), and raking-ratio estimation (Thompson, 2000). The marginals \(P_{X}\) and \(P_{Y}\) can represent auxiliary information or inductive bias from users, such as the desire for balanced clusters.

Returning to \(P_{n,}\) in (2), we show in Sec. 2 that both self-labeling and contrastive approaches in SSL implicitly define \(P_{n,}\) by the following steps: 1) constructing a method-specific "initial" measure \(P_{n}^{(0)}\) on \(\), then 2) applying \(k\) iterations of the operations (3) to generate a sequence \(P_{n}^{(0)},,P_{n}^{(k)}\), and finally, 3) setting \(P_{n,}:=P_{n}^{(k)}\). In other words, these methods embed a _learnable_ balancing operation in their objectives. A natural question to consider is: if the marginals one uses accurately represent the ones of the true probability measure \(P\) governing the data, are balanced quantities "better behaved" than their unbalanced counterparts? If so, in what way?

Inspired by this question, we fix the model parameter \(\) (thus dropping the subscript from the quantities above) and analyze the fluctuations of the unbalanced and balanced objectives. The formal problem statement is as follows. Let \(P_{n}^{(0)}=P_{n}\) and \(P_{n}^{(k)}\) denote the output of \(k 1\) iterations of data balancing (see Sec. 3 for the precise definition). Finally, letting \(h:\) be a fixed function of interest, we define the population parameter \(\) and its \(k\)-step _balanced estimator_\(_{n}^{(k)}\) by

\[:=_{P}[h(X,Y)]_{n}^{(k)} :=_{P_{n}^{(k)}}[h(X,Y)].\] (4)Our goal is to establish theoretical guarantees on the mean squared error (MSE) \(_{P}[(_{n}^{(k)}-)^{2}]\) of estimating \(\) using \(_{n}^{(k)}\), with an informative dependence on the sample size \(n\), number of iterations \(k\), target marginals \((P_{X},P_{Y})\), and test function \(h\). We are particularly interested in its comparison to the direct estimator based on the empirical measure \(_{n}^{(0)}=_{i=1}^{n}h(X_{i},Y_{i})\), as to quantify the effect of the auxiliary information \((P_{X},P_{Y})\). Our analysis uncovers two surprising facts. Firstly, while originally proposed for a different purpose, balancing reduces the variance of the empirical estimate. Secondly, while the balancing iterations are nonlinear operations on the input measure, the variance reduction can be precisely quantified using the spectral decay of two linear Markov operators: the conditional means given \(X\) and \(Y\), respectively.

Contributions.In Sec. 2, we detail the mathematical connection between balancing and the modern representation learning techniques mentioned above. In Sec. 3, we prove a new upper bound on the MSE of the balancing estimator \(_{n}^{(k)}\). The bound decomposes into an \(O(n^{-1})\) first-order variance term and an \(O(k^{6}n^{-3/2})\) second-order term. The first-order term is shown to have a strict improvement over the empirical measure baseline with a fine-grained dependence on the spectra of two particular Markov operators. The second-order term can be used to compute the asymptotic variance reduction for statistical efficiency comparisons. Our proof technique relies on a recursion decomposition for balancing-based estimators, which may be of independent interest. In Sec. 4, we illustrate how insights from our analysis can be practically applied to CLIP-type objectives and evaluation setups.

## 2 Data Balancing in Practice

To demonstrate a precise connection to (2), we describe how a collection of training examples \(Z_{1},,Z_{n}\) observed in an original data space \(\) (e.g. grayscale images) is mapped to a probability measure \(P_{n,}\). Using the framework introduced in Sec. 1, this amounts to specifying four components: 1) the map from the original data into the derived sample spaces \(\) and \(\), 2) the initial measure \(P_{n}^{(0)}\), 3) the function \(h\), and 4) the target marginals \((P_{X},P_{Y})\) for this measure to fit. From that point, the iterations of (3) produce \(P_{n}^{(1)},,P_{n}^{(k)}\), and we set \(P_{n,}:=P_{n}^{(k)}\). For ease of presentation, we hide the dependence of \(P_{n}^{(k)}=P_{n,}\) and \(h h_{}\) on the model parameter \(\). See Fig. 1 for examples of different choices of the sample spaces \(\) and \(\).

Example 1: Self-Supervised Clustering.Balancing is used in discriminative clustering and self-supervised clustering; see (Jones et al., 2022; Asano et al., 2020; Caron et al., 2020) for variations on this theme. We describe the swapped prediction task of Caron et al. (2020) for concreteness but emphasize that clustering of this form is used as an intermediate step (or as the task itself) in many SSL pseudo-tasks. At a high level, this approach involves passing elements of a minibatch through two encoders to generate vector representations. These representations are then clustered separately, and the features from one encoder predict the cluster label from the other encoders. Denote the encoders \(f_{_{s}}:^{r}\) and \(f_{_{s}}:^{r}\), colloquially known as the _student_ and _teacher_ networks, respectively. Here, we let \(\{Z_{i}\}_{i=1}^{n}\) be a minibatch of \(n\) images, with

\[=\{Z_{1},,Z_{n}\}=\{1, ,l\}\,,\]

where \(m=n\) and the elements of \(\) index learnable cluster representation vectors \(c_{1},,c_{l}^{r}\). Thus, we consider the overall parameter vector to be \(:=(_{s},_{t},c_{1},,c_{l})\). Given temperature hyperparameters \(,>0\), the initial measure and loss function are given by the expressions

\[P_{n}^{(0)}(x,y) e^{f_{_{s}}(x)^{}c_{y}/} h(x,y)=}(x)^{}c_{y}/}}{_{y^{ }=1}^{l}e^{f_{_{s}}(x)^{}c_{y^{}}/}}.\]

Directly optimizing \(_{x,y}P_{n}^{(0)}(x,y)h(x,y)\) without any constraints would lead to collapse, so it is balanced before optimization. The target marginals \(P_{X}\) and \(P_{Y}\) are given by the discrete uniform measures on \(\) and \(\). This formulation is often derived by solving an optimal transport problem with the Sinkhorn-Knopp algorithm to assign soft cluster labels, the iterative solution result from this procedure is precisely \(P_{n}^{(k)}\). The intuition behind the choice of uniform marginal \(P_{X}\) is that each data point has an equal amount of mass to be allotted, whereas \(P_{Y}\) captures that the cluster sizes are equal. The number of iterations \(k\) is selected based on optimization considerations.

Example 2: Contrastive Learning.Contrastive Language-Image Pre-Training (Radford et al., 2021), or CLIP, is an architecture with an image encoder and a text encoder that map to a joint embedding space. Trained using image-caption pairs, the loss promotes representations such that images and text that are paired in the minibatch are close, whereas those that are not paired are far. The latter aspect (promoting dissimilarity of unpaired images/text) is what prevents collapse in this framework. To our knowledge, our interpretation of the CLIP objective as an implicit data balancing procedure is novel. Under this interpretation, we demonstrate that the objective is in fact a nonlinear function of \(P_{n,}\), whereas its gradient will have a linear form similar to (2). In this case, each \(Z_{i}=(X_{i},Y_{i})\), where \(X_{i}\) is an image and \(Y_{i}\) is an associated caption. We have that

\[=\{X_{1},,X_{n}\}= \{Y_{1},,Y_{n}\},\]

so that \(m=n\). Consider an image encoder \(f_{_{I}}:^{r}\) and text encoder \(f_{_{T}}:^{r}\) with parameter vector \(=(_{I},_{T})\). The initial, unnormalized measure and the (in this case, vector-valued) function \(h\) are chosen based on these encoded representations:

\[P_{n}^{(})}(x,y) e^{f_{_{I}}(x)^{}f_{_{T }}(y)} h(x,y)=_{}(f_{_{I}}(x)^{}f_{ _{T}}(y)).\] (5)

While we usually interpret \(h\) as a loss function, we will show below that the CLIP loss depends nonlinearly on \(P_{n,}\), while the gradient has a linear dependence. If we believe, as in Example 1, that the target marginals \((P_{X},P_{Y})\) of the images and the text should be roughly uniform, we can apply the balancing iterations (3) with the target marginals being the uniform distributions over \(\) and \(\), respectively. Because there is no preference for starting the iterations with the \(\) or \(\) dimension first, we may consider both orderings. Let \(Q_{n}^{(})}\) be one iteration of balancing in the \(\) dimension and \(R_{n}^{(})}\) represent one such iteration in the \(\) dimension. Then the original CLIP objective \(L_{n}^{}\) can be recovered (up to an additive constant) as

\[L_{n}^{} :=-_{i=1}^{n}[^{(})}(X_{i},Y_{i})}{_{x}P_{n}^{(})}(x,Y_{i})}+ ^{(})}(X_{i},Y_{i})}{_{y}P_{n}^{(})}(X_{i},y)}]\] \[=-_{i=1}^{n}[ Q_{n}^{(})}(X _{i},Y_{i})+ R_{n}^{(})}(X_{i},Y_{i})]- n.\] (6)

The measure \(P_{n,}=Q_{n}^{(})}+R_{n}^{(})}\) is constructed in this case by averaging the outputs of one iteration of balancing under each modality. Taking the gradient of (6) with respect to \(\) (whose dependence is contained in \((Q_{n}^{(})},R_{n}^{(})})\)) recovers the expression for \(h\) in (5). The objective is often interpreted as an average of cross-entropy loss terms, each representing the prediction of one modality's original pair from the other. In our formulation, \(L_{n}^{}\) can also be viewed as an average negative log-likelihood under the \(Q_{n}^{(})}\) and \(R_{n}^{(})}\). It is also of interest to study the effect of using \(Q_{n}^{(})}\) and \(R_{n}^{(})}\) for \(k 0\) in general, as we show in Sec. 4.

Figure 1: **Data Balancing Examples:** Each panel shows a possible distribution \(Q\) on different choices of \((,)\). The orange histograms are the target marginal \(P_{Y}\). **Left:**\(Q(x,y)\) is the affinity of an image \(x\) for cluster \(y\). **Center:**\(Q(x,y)\) is the similarity of an image \(x\) to a text caption \(y\). **Right:**\(Q(x,y)\) is the proportion of substring matches between a text caption \(x\) and a keyword \(y\).

**Example 3: Metadata Curation.** Here, we consider balancing an entire training set, as opposed to a particular minibatch. At the billion-parameter scale, dataset design can be the primary factor that differentiates performance between foundation models (Fang et al., 2013; Xu et al., 2024; Gadre et al., 2023). One general approach used in both the original CLIP dataset (Radford et al., 2021) and an open-source replication (Xu et al., 2024) is metadata curation, wherein a text dataset (possibly captions for images) is synthesized using a list of keywords \(\{y_{1},,y_{l}\}\) so that

\[=\{Z_{1},,Z_{n}\},=\{y_{1}, ,y_{l}\},\]

meaning that \(m=n\). The keywords are matched to texts within \(\) via substring matching. While the approach of Xu et al. (2024) (dubbed MetaCLIP) pools all matched keywords on every text to measure the "distribution" of keywords, we consider a version in which each text \(Z_{i}\) can only be labeled with a single keyword \(y_{j}\). This allows for a true joint probability measure on \(\). The marginal distribution of observed keywords is initially long-tailed (see Fig. 4) (e.g., "the" will match many more texts than "xylophone"). In both Radford et al. (2021) and Xu et al. (2024), the data are resampled so that this distribution of keywords over matches is closer to uniformity, i.e. keywords with many matches have their associated texts downsampled during the dataset creation process. While the probability measure may not be computed explicitly (due to scale), this adjustment of the keyword distribution can be viewed as a single iteration of balancing (3) applied to the \(\) marginal. For tasks such as language modeling, we have

\[P_{n}^{}(x,y)=P_{n}(x,y) h(x,y)=_{ }(x),\] (7)

where \(_{}(x)\) denotes the loss of a model evaluated at a single text \(x\) (notice that the keyword is not used). We elucidate this connection by applying direct balancing on a subset of the ImageNet-Captions dataset in Sec. 4, observing the effect on downstream model performance.

Motivated by these scenarios, we address the statistical problem outlined in Sec. 1 by analyzing balancing-based estimators. We then return to examples mentioned above in Sec. 4, illustrating how the theoretical analysis can be translated to algorithmic variants.

## 3 Theoretical Analysis of Variance Reduction

We now present theoretical guarantees on the mean squared error (MSE) of the data-balanced estimator \(_{n}^{}\) and highlight relevant points in the proofs. For readers' convenience, a notation table (Tab. 1) is in Appx. \(\). We first give context on the main innovations of the analysis and then outline its high-level steps. These innovations include relating the nonlinear iterations of balancing over probability measures to linear operators on a vector space and using a singular value decomposition of these operators to quantify their effect after a finite number of iterations. Furthermore, by scaling the number of iterations appropriately, we can characterize the estimator using the limit of balancing iterations, which is an object of interest in applications including optimal transport.

**Preliminaries.** Recall the setting introduced in Sec. 1, in which we consider sample spaces \((,)\), along with true and unknown joint distribution \(P\) on \(\) with known marginals \((P_{X},P_{Y})\). For ease of presentation, we assume that \(||=||=m\), although the arguments do not rely on equal support sizes. We make the following assumption throughout, which is usually satisfied by the desired marginals \(P_{X}\) and \(P_{Y}\), such as in the uniform cases discussed in Sec. 2: the target marginals \(P_{X}(x)>0\) and \(P_{Y}(y)>0\) for all \(x\) and \(y\). We define \(P_{n}^{}=P_{n}\) as the empirical measure and for \(k 1\) construct

\[P_{n}^{}(x,y):=(x)}{P_{n}^{}(x)} P_{n}^{}(x,y)&\\ (y)}{P_{n}^{}(y)} P_{n}^{}(x, y)&.\] (8)

By direct computation, we see that the iterations in (8) are equivalent to applying (3) for \(k\) odd and even, respectively. See Fig. 2 (left) for a visualization of this procedure. The iterations are well-defined for all \(k\) under the event that \((P_{n,X})=(P_{X})\) and \((P_{n,Y})=(P_{Y})\), i.e., all observed row counts and column counts are non-empty.2To provide background, the scheme of alternating the operators (8) is often seen as an iterative algorithm to solve the problem

\[_{Q(P_{X},P_{Y})}(Q\|P_{n}^{{}_{(0)}}),\] (9)

where \((P_{X},P_{Y})\) denotes the set of probability measures on \(\) that marginalize to \(P_{X}\) and \(P_{Y}\) in each variable and \((\|)\) denotes the Kullback-Leibler divergence. The iterations (8) are based on the alternating minimization approach of solving

\[P_{n}^{{}_{(k)}}(x,y):=*{arg\,min}_{\{Q:Q_{X}=P_{X} \}}(Q\|P_{n}^{{}_{(k-1)}})&\\ *{arg\,min}_{\{Q:Q_{Y}=P_{Y}\}}(Q\|P_{n}^{{}_{( k-1)}})&,\]

which inspires the viewpoint of balancing as alternating _information projections_. As we show in Appx. C, the iterations of (8) can equivalently be defined using the KL, reverse KL, or \(^{2}\)-divergences. This viewpoint is relevant as previously, efforts have been made (e.g. in Bickel et al. (1991)) to analyze the variance reduction afforded by the solution to (9) directly. However, quantifying the variance reduction (in terms of properties of \(P\)) using this approach is challenging, as there is no closed-form expression for the solution of (9). A key mathematical outcome of our analysis is that the closed-form expressions of the projections (8) can be used to compute the reduction in mean squared error at each iteration. Thus, by letting \(k k(n)\) (scaled appropriately against \(n\)), we can determine the reduction for the solution of (9) for large \(n\). This is the subject of Thm. 1.

**From Information Projections to Orthogonal Projections.** First, we will show that the variance reduction resulting from each nonlinear iteration of (8) is associated with a linear operator applied to \(h\). Thus, instead of analyzing the alternating information projections over probability measures, we may use familiar tools to understand alternating orthogonal projections in a vector space. To define them, we first let \(^{2}(P)\) to be the set of functions \(h:\) satisfying \(_{P}[h^{2}(X,Y)]<\). Even though \(\) is finite, working within \(^{2}(P)\) will be analytically convenient. Let \(^{2}(P_{X})\) be the subspace of \(^{2}(P)\) containing functions that only depend on the first argument \(x\) and define \(^{2}(P_{Y})\) analogously. These are the solid-colored subspaces in Fig. 2 (right). Next, let \(_{X}:^{2}(P)^{2}(P_{X})\) and \(_{Y}:^{2}(P)^{2}(P_{Y})\) be defined as, for any \(h^{2}(P)\),

\[_{X}h=*{arg\,min}_{f^{2}(P_{X})}_{P} [(h(X,Y)-f(X))^{2}][_{X}h](x,y):=_ {P}[h(X,Y)|X](x)\]

The operator \(_{X}\) is an orthogonal projection onto \(^{2}(P_{X})\). The orthogonal projection operator \(_{Y}\) onto \(^{2}(P_{Y})\) is defined analogously. We may also define the conditional _debiasing_ operators \(_{X}=I-_{X}\) and \(_{Y}=I-_{Y}\), which each project onto the orthogonal complements of \(^{2}(P_{X})\) and

Figure 2: **Data Balancing.** Nonlinear and linear operators associated with each iteration of (8). **Left:** Visualization of the exact iterations of (8) in the space of probability measures. The blue set contains joint distributions with \(\)-marginal equal to \(P_{X}\), whereas the orange set contains joint distributions with \(\)-marginal equal to \(P_{Y}\). **Right:** Visualization of \(^{2}(P)\), the operators defining (11), and the singular values given in (13).

\(^{2}(P_{Y})\), visualized as subspaces with dotted border in Fig. 2 (right). To understand the importance of the conditional mean and debiasing operators, we give a recursive formula that forms the backbone of our analysis. Define \(_{k}=_{X}\) for \(k\) odd and \(_{k}=_{Y}\) for \(k\) even, and define \(_{k}\) similarly. Thus, by using the notation \(Q(h):=_{Q}[h(X,Y)]\), we have by linearity of expectation that

\[[P_{n}^{{(k)}}-P](h) =[P_{n}^{{(k)}}-P](_{k}h)+ ^{{(k)}}-P](_{k}h)}^{=0}\] \[=[P_{n}^{{(k-1)}}-P](_{k}h)+[P_{n}^{{(k)}}-P_{n}^{{ (k-1)}}](_{k}h)\] \[=^{{(0)}}-P](_{1} _{k}h)}_{}+^{k}[P_{n}^{ {(0)}}-P_{n}^{{(-1)}}](_{}_{k}h)}_{ }.\] (10)

To justify the first line, we discuss the case when \(k\) is odd. Notice that \(_{X}h\) is only a function of \(X\), so its expectation only depends on \(P_{X}\) that is equal to \(P_{n,X}^{{(k)}}\) (the \(\)-marginal of \(P_{n}^{{(k)}}\)) by (8). The last line follows by unrolling the previous step \(k-1\) times. This recursive expansion is proven formally in Prop. 15 in Appx. D. Given the expansion, the mean squared error can be computed by taking the expectation of squared (10). We show that the second moment of the first-order term in (10) is equal to \(_{k}^{2}/n\) where

\[_{0}^{2}:=(h)_{k}^{2}:= (_{1}_{k}h)k 1,\] (11)

and all other terms are \(O(k^{6}n^{-3/2})\). Thus, by exactly computing the constant in the dominating term, we may quantify the asymptotic variance reduction. Our first main result concerns the higher-order terms and shows that it is indeed dominated by the first-order term. Note that the empirical mean \(_{n}^{{(0)}}=_{i=1}^{n}h(X_{i},Y_{i})\) is unbiased, and so its MSE is equal to \(_{0}^{2}/n\). Define in addition

\[p_{}:=\{_{x}P_{X}(x),_{y}P_{Y}(y)\}\]

which measures the non-uniformity of the target marginals. We have that \(p_{}\) is positive because both \(P_{X}\) and \(P_{Y}\) are positive. We now state the first main result.

**Theorem 1**.: _For a sequence of data balancing estimators \((_{n}^{{(k)}})_{k 1}\) as defined in (4), there exists an absolute constant \(C>0\) and distribution dependent constant \(s[0,1)\) and such the following holds for \(_{}^{2}=_{0}^{2}-_{k}^{2}\): For \(n C[_{2}(2n/p_{})+m]/p_{}^{2}\) and \(k 1\), we have_

\[_{P}[(_{n}^{{(k)}}-)^{2} ]^{2}-_{}^{2}}{n}+O(}{n})+(}{n^{3/2}}).\] (12)

The quantities \(_{}^{2}\) and \(s\) are quantified toward the end of this section and are dependent on eigendecays of the conditional mean operators for each variable under \(P\). Furthermore, \(_{}^{2}>0\) except for the pathological case of \(_{X}h\) being a constant function. Showing Thm. 1 boils down to showing that the higher-order term in (10) is \(O(n^{-1})\) with high probability. Using the expression (8) and assuming that \( 1\) is odd, we see that

\[[P_{n}^{{()}}-P_{n}^{{(-1)}}](_{ }_{k}h)=_{x,y}[(x)}{P_{n,X}^{{(-1 )}}(x)}-1][_{}_{k}h](x,y)P_{n}^{{( -1)}}(x,y).\]

The first (blue) term in the product quantifies the disagreement between the \(\)-marginal of \(P_{n}^{{(-1)}}\) and the true marginal, which can be bounded in terms of \((P_{n,X}^{{(0)}}\|P_{X})\) and is shown to be \(O(n^{-1/2})\) with high probability via techniques from information theory. The second (orange) term can be unrolled recursively in a similar fashion to (10) itself, which will consequently be \(O(n^{-1/2})\) as well; this is the most technical part of the analysis (see Appx. D.3). Our analysis also yields a bound for the sensitivity of balancing to misspecified marginals; see Appx. D.5.

Given Thm. 1, a natural next step is to quantify the gap between \(_{0}^{2}\) and \(_{k}^{2}\), which requires finer-grained properties of \(_{X}\) and \(_{Y}\). Notably, we show that as \(k\), \(_{k}^{2}\) approaches a limiting value. Thus, via (12), by using \(k=o(n^{1/12})\) obtains asymptotic variance of the solution to (9). This contrasts with Albertus and Berthet (2019), in which the dependence of a quantity similar to (12) is exponential in \(k\), meaning that \(k=o((n))\) is required for convergence under this argument.

From Orthogonal Projections to Variance Reduction.We now clarify what is precisely meant by the "spectrum" of the conditional mean operators \(_{X}\) and \(_{Y}\). As proven using a _singular value decomposition_ (Prop. 3) in Appx. B.1, there exists a basis \(\{_{j}\}_{j=1}^{m}\) of \(^{2}(P_{X})\), a basis \(\{_{j}\}_{j=1}^{m}\) of \(^{2}(P_{Y})\), and real values \(\{s_{j}\}_{j=1}^{m}\), that satisfy

\[_{Y}_{j}=s_{j}_{j}_{X}_{j}=s_{j}_{j} j\{1,,m\}\,.\] (13)

Furthermore, \(_{1}=_{}\) and \(_{1}=_{}\) leading to the equality \( f,_{1}_{^{2}(P_{X})}=_{P_{X}}[f (X)]\). Finally, \(s_{1}=1\) and \(s_{j}\) is non-negative and non-increasing in \(j\). For a concrete example, consider \(m=2\), in which case \(P\) can be written as a matrix in \(^{2 2}\) and elements of \(^{2}(P_{X})\) and \(^{2}(P_{X})\) are vectors in \(^{2}\). Then, in the case of uniform marginals, we can verify directly that (13) can be satisfied by setting

\[_{1}=_{1}=1\\ 1,_{2}=_{2}=1\\ -1,P=1+s&1-s\\ 1-s&1+s\] (14)

for \(s=s_{2}\) (the second largest singular value). Thus, as \(s 1\), the distribution becomes "fully dependent" as \(Y\) and \(X\) are completely determined by one another. As \(s 0\), \(P\) approaches the product measure. Geometrically, because \(_{1}=_{1}\), we know that the angle \(a\) between the subspaces \(^{2}(P_{X})\) and \(^{2}(P_{Y})\) is given by the angle between \(_{2}\) and \(_{2}\). By computing their inner product in \(^{2}(P)\), we have that \(_{2},_{2}_{^{2}(P)}= P,_{2} _{2}^{}=s= a\). Thus, \(s=0\) indicates orthogonality of these subspaces, alluding to the independence of \(X\) and \(Y\) (see the right panel of Fig. 2).

Returning to \(m 2\), we consider the following as a sufficient condition for variance reduction: the operators \(_{X}\) and \(_{Y}\) have a positive spectral gap, i.e., \(s_{2}<s_{1}\). Note that this assumption is satisfied when \(P(x,y)>0\) for all \((x,y)\) by the Perron-Frobenius Theorem [12, Chapter 8]. Using the intuition from Fig. 2, this rules out pathological cases such as \(Y\) being a deterministic function of \(X\). Under the spectral gap condition, the singular values \(\{s_{j}\}_{j=2}^{m}\) that are strictly less than \(1\) will determine a geometric rate of decay in variance given in Cor. 2. The left and right singular functions \(_{j}:\) and \(_{j}:\) will define a useful coordinate system to represent projections of \(h\) when analyzing \(_{n}^{(h)}\).

Indeed, let \(=P(h)\) be the centered test function. Because \(_{X}^{2}(P_{X})\) and \(_{Y}^{2}(P_{Y})\), we may decompose this function on the two bases to write

\[_{X}=_{j=1}^{m}u_{j}_{j}_{Y}=_{j=1}^{m}v_{j}_{j}.\] (15)

Cor. 2 below relates the (normalized) variance \(_{k}^{2}\) of the first-order term to the one of the sample mean \(_{n}^{(0)}\). In fact, it shows that the variance reduction \(_{0}^{2}-_{k}^{2}\) decays geometrically to the quantity

\[_{}^{2}:=_{j=2}^{m}[u_{j}^{2}+-s_{j}u_{j })^{2}}{1-s_{j}^{2}}].\]

For simplicity, we only present the result for \(k\) even, i.e., \(_{2t}^{2}\).

**Corollary 2**.: _The variance reduction achieved by \(t+1\) iterations of the \(_{Y}_{X}\) operator can be quantified as_

\[_{0}^{2}-_{2(t+1)}^{2}=_{}^{2}-_{j=2}^{m} {s_{j}^{2}(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}s_{j}^{4t}=_{j=2}^{m}[ u_{j}^{2}+(1-s_{j}^{4t+2})-s_{j}u_{j})^{2}}{1-s_{j}^{2}}].\]

Intuitively, the operators \(_{X}\) and \(_{Y}\) are the main sources of the variance reduction via orthogonality. Since \(_{1}=_{}\), we can see that the reduction will always be strictly positive as long as \(_{X}\) is not a constant function. Finally, using \(s:=s_{2} s_{j}\) for \(j 2\) gives the second term in Thm. 1.

## 4 Numerical Illustrations

We illustrate how data balancing manifests in the motivating examples mentioned in Sec. 2 with experiments with CLIP-type models. We focus here on zero-shot image classification tasks. Details on these experiments, and additional ones including linear probing and zero-shot retrieval, as well as an empirical investigation of the sensitivity to misspecified marginals, are all contained in Appx. E. Code to reproduce the data and experiments can be found at https://github.com/ronakdm/balancing.

Model, Datasets, and Evaluation.Throughout, we consider training variants of CLIP models (see Sec. 2), which require a dataset of image-caption pairs. For the training set, we use the ImageNet-Captions dataset (Fang et al., 2013), which pairs images from ImageNet (Deng et al., 2009) that were taken from Flickr with their original captions. In the notation of Sec. 2, the model is specified by selecting an image encoder \(f_{_{I}}\) and a text encoder \(f_{_{T}}\). In all cases, we use a fixed image/text encoder as a base vector representation and compose it with a trainable feed-forward neural network, i.e., \(f_{}=f_{^{}}^{} f^{}\). We fix the base image encoder as CLIP ViT-B/32 architecture pre-trained on LAION-2B (Schuhmann et al., 2022), and vary the base text encoder across embedding models of varying quality: GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), and CLIP-based encodings. When two CLIP encoders are used for the base image/text vector representation, they are taken from separate CLIP models (i.e. the base representations are not dependent). We evaluate models based on zero-shot classification performance using the standard CLIP inference procedure: for any image \(x\), a label \(c\{1,,C\}\) is predicted by associating to each \(c\) a natural language prompt \(y_{c}\), and predicting the scores \(s(x)=(s_{1}(x),,s_{C}(x))\), with

\[s_{c}(x)=}(x),f_{_{T}}(y_{c}) /}}{_{c^{}=1}^{C}e^{ f_{_{I}}(x),f_{_{T} }(y_{c^{}})/}}\] (16)

for a temperature \(>0\). Multiple prompting strategies can be used depending on the evaluation dataset, for which we average embeddings before applying (16). We use the public CLIP Benchmark repository, using the datasets CIFAR-10, CIFAR-100, STL-10, with their default caption sets.

Data Balancing Effects.Fig. 3 shows the zero-shot classification performance (in terms of average per-class recall) of variants depending on whether the _contrastive learning_ objective from Sec. 2 is used or not. One iteration of balancing already leads to improvement in terms of downstream performance. Multiple balancing iterations lead to further improvements. See Appx. E for more details on this experiment, and for analogous ones with linear probing and zero-shot retrieval.

Fig. 4 then shows how balancing can be used to adjust an entire pre-training set to given marginals based on metadata, as described in Sec. 2 in the _metadata curation_ example. After balancing, the target marginal has less than \(2\) orders of difference. In terms of downstream performance, data balancing leads to some improvement in the smaller batch regime (\(m=512\)) when curating the dataset. See Appx. E for more details on this experiment.

Figure 3: **Zero-Shot Classification Performance across Embeddings, Batch Sizes, and Objectives. The three vertical panels describe different choices of the text encoder \(f_{_{T}}\) which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Within each vertical panel, examples include batch sizes \(m=128\) and \(m=512\). Rows indicate various evaluation datasets from CIFAR-10, CIFAR-100, and STL-10. The \(y\)-axis of each plot indicates average per-class recall, whereas the \(x\)-axis indicates training iterations at the given batch size.**

Related WorkSelf-supervised learning has witnessed a surge of recent interest as datasets and computing hardware allow for larger, more capable models (see Balestriero et al. (2023) and references therein). While we highlight in this paper the connections between data balancing and contrastive learning (Radford et al., 2021), we acknowledge that data balancing can also be related to "self-distillation" approaches more broadly (Grill et al., 2020; Chen and He, 2021; Oquab et al., 2024).

Historical motivations for data balancing include census or survey data, in which \(P_{n}\) is a cross-tabulation of (a limited number of) paired observations and the target marginals were estimated from large amounts of unpaired observations (Deming and Stephan, 1940; Ireland and Kullback, 1968). This situation is not unlike the present day--yet at a different scale--in which the amount of unstructured single-modality data (such as images) still dwarfs the amount of high-quality multimodal data (Gadre et al., 2023). Bickel et al. (1991) proved classical asymptotic results on balancing estimators. Linear operators similar to the ones we use in Sec. 3 also appear in their analysis. More recently, Albertus and Berthet (2019) studied such estimators from an asymptotic empirical process viewpoint. Our theoretical results significantly improve on those from Albertus and Berthet (2019) primarily in the dependence of the number of iterations \(k\) on the sample size \(n\) to achieve convergence guarantees (from logarithmic to polynomial).

Matrix scaling is a popular algorithm for solving entropy-regularized optimal transport (EOT). We refer to Peyre and Cuturi (2019) for a survey. See also Courty et al. (2017); Shen et al. (2018); Peng et al. (2019) for interesting methods based on EOT in machine learning. Entropy-regularized optimal transport was one of the original inspirations for SSL techniques such as SwaV (see Sec. 2). While EOT is itself a deterministic optimization problem, a related statistical problem is the large-sample limits of EOT solutions when the marginal measures are estimated from data (Mena and Niles-Weed, 2019; Genevay et al., 2019; Klatt et al., 2020). We emphasize that, while this line of work shares the matrix scaling algorithm with our setting, the statistical problem is entirely distinct; in statistical EOT, the target marginal distributions are computed from observations of independent, unpaired data, and the initial measure can be computed from the cost function. In our setting, the data are dependent, forming the random initial measure \(P_{n}\), whereas \(P_{X}\) and \(P_{Y}\) are fixed auxiliary information.

## 5 Conclusion

We showed how several disparate techniques used towards the training of foundation models are instances of a data balancing algorithm, which has the unsuspected benefit of reducing the variance of learning objectives involving multiple sources of data. We proved a new non-asymptotic bound on the mean-squared error of balanced estimators as they adjust to the given marginals. We also highlight the key roles of conditional expectation operators in quantifying that variance reduction effect. Finally, we translated the marginal balancing interpretation of several training practices for foundation models into algorithmic variants that warrant further investigation. Exploring variants incorporating prior information on the data sources is also an interesting venue for future work.

Figure 4: **Balancing and Metadata Curation.** Depiction of balancing and metadata curation (Example 3 in Sec. 2) on ImageNet-Captions dataset, in which \(\) represents image-caption pairs and \(\) represents keywords. **Left:** Observed marginal \(P_{n,Y}\) (orange) and \(P_{Y}\) (blue), which are sorted by order of increasing probability. **Right:** Zero-shot evaluation of an embedding model trained using the standard CLIP loss original versus the balanced training set.

AcknowledgementsThe authors are grateful to G. Ilharco, M. Wortsman, K. Pillutla, L. Schmidt, and J. Wellner for fruitful discussions related to this work. This work was supported by NSF DMS-2023166, CCF-2019844, DMS-2134012, PIMS 20240827-PRN01, NIH, and IARPA 2022-22072200003. Part of this work was done while L. Liu was with the University of Washington, and while R. Mehta and Z. Harchaoui were visiting the Simons Institute for the Theory of Computing.

Broader ImpactWhile this paper is of a theoretical nature, the web-scale pre-training sets used to train foundation models can affect not only the biases of the models themselves but also the behavior of individuals who interact with them. In the case of representation learning, unrefined Internet data may lead to non-uniform performance among protected attributes such as gender, age, etc. For generative models, individuals of all ages may be influenced by harmful images or textual output. Studying the relationship between the balancing procedures considered in this paper and more holistic model evaluations presents a valuable direction for follow-up work.