# Score matching through the roof: linear, nonlinear, and latent variables causal discovery

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function \( p(X)\) of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.

## 1 Introduction

The inference of causal effects from observations holds the potential for great impact arguably in any domain of science, where it is crucial to be able to answer interventional and counterfactual queries from observational data [1; 2; 3]. Existing causal discovery methods can be categorized based on the information they can extract from the data , and the assumptions they rely on. Traditional causal discovery methods (e.g. PC, GES [5; 6]) are general in their applicability but limited to the inference of an equivalence class. Additional assumptions on the structural equations generating effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [7; 8; 9; 10]. As a consequence, existing methods for causal discovery require specialized and often untestable assumptions, preventing their application to real-world scenarios.

Further, the majority of existing approaches are hindered by the assumption that all relevant causes of the measured data are observed, which is necessary to interpret associations in the data as causal relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solutions relaxing this requirement face substantial limitations. The FCI algorithm  can only return an equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some direct causal effects in the presence of latent variables: RCD  relies on the linear non-Gaussian additive noise model, whereas CAM-UV  requires nonlinear additive mechanisms. Nevertheless, the strict conditions on the structural equations hold back their applicability to more general settings.

Our paper tackles these challenges and can be put in the context of a recent line of work that derives a connection between the score function \( p(X)\) and the causal graph underlying the data-generating process [14; 15; 16; 17; 18; 19]. The use of the score for causal discovery is practically appealing, as it yields advantages in terms of scalability to high dimensional graphs and guarantees of finite sample complexity bounds . Instead of imposing assumptions that ensure strong, though often impractical, theoretical guarantees, we organically demonstrate different levels of identifiability based on the strength of the modeling hypotheses, always relying on the score function to encode all the causal information in the data. Starting from results of Spantini et al.  and Lin , we show how constraints on the Jacobian of the score \(^{2} p(X)\) can be used as an alternative to conditional independence testing to identify the Markov equivalence class of causal models with hidden variables. Further, we prove that the score function identifies the causal direction of additive noise models, with minimal assumptions on the causal mechanisms. This extends the previous findings of Montagna et al. , limited by the assumption of nonlinearity of the causal effects, and Ghoshal and Honorio , limited to linear mechanisms. On these results, we build the main contributions of our work, enabling the identification of direct causal effects in hidden variables models.

**Our main contributions** are as follows: _(i)_ We present the necessary conditions for the identifiability of direct causal effects and the presence of hidden variables with the score in the case of latent variables models. _(ii)_ We propose AdaScore (Adaptive Score-based causal discovery), a flexible algorithm for causal discovery based on score matching estimation of \( p(X)\). Based on the user's belief about the plausibility of several modeling assumptions on the data, AdaScore can output a Markov equivalence class, a directed acyclic graph, or a mixed graph, accounting for the presence of unobserved variables. To the best of our knowledge, the broad class of causal models handled by our method is unmatched by other approaches in the literature.

## 2 Model definition and related works

In this section, we introduce the formalism of structural causal models (SCMs), separately for the the cases with and without hidden variables.

### Causal model with observed variables

Let \(X\) be a set of random variables in \(\) defined according to the set of structural equations

\[X_{i} f_{i}(X_{_{i}^{}},N_{i}),\ \  i=1,,k.\] (1)

\(N_{i}\) are mutually independent random variables with strictly positive density, known as _noise_ or error terms. The function \(f_{i}\) is the _causal mechanism_ mapping the set of _direct causes_\(X_{_{i}^{}}\) of \(X_{i}\) and the noise term \(N_{i}\), to \(X_{i}\)'s value. A structural causal model (SCM) is defined as the tuple \((X,N,,_{N})\), where \(=(f_{i})_{i=1}^{k}\) is the set of causal mechanisms, and \(_{N}\) is the joint distribution relative to the density \(p_{N}\) over the noise terms \(N^{k}\). We define the _causal graph_\(\) as a directed acyclic graph (DAG) with nodes \(X=\{X_{1},,X_{k}\}\), and the set of edges defined as \(\{X_{j} X_{i}:X_{j} X_{_{i}^{}}\}\), such that \(_{i}^{}\) are the indices of the parent nodes of \(X_{i}\) in the graph \(\). (In the remainder of the paper, we adopt the following notation: given a set of random variables \(Y=\{Y_{1},,Y_{n}\}\) and a set of indices \(Z\), then \(Y_{Z}=\{Y_{i}|i Z,Y_{i} Y\}\).)

Under this model, the probability density of \(X\) satisfies the _Markov factorization_ (e.g. Peters et al.  Proposition 6.31):

\[p(x)=_{i=1}^{k}p(x_{i}|x_{_{i}^{}}),\] (2)

where we adopt the convention of lowercase letters referring to realized random variables, and use \(p\) to denote the density of different random objects, when the distinction is clear from the argument. This factorization is equivalent to the _global Markov condition_ (e.g. Peters et al.  Proposition 6.22) that demands that for all \(\{X_{i},X_{j}\} X,X_{Z} X\{X_{i},X_{j}\}\), then

\[X_{i}\,\,\,\,X_ {j}|X_{Z} X_{i}\,\,X_{j}|X_{Z},\]

where \((\,\,\,|)\) denotes probabilistic conditional independence of \(X_{i},X_{j}\) given \(X_{Z}\), and \((\,\,\,\,|)\) is the notation for _d-separation_, a criterion of conditional independence defined on the graph \(\) (Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction \(X_{i}\,\,X_{j}|X_{Z} X_{i}\,\,\,X_{j}|X_{Z}\) hold, and we say that the density \(p\) is _faithful_ to the graph \(\)[2; 24] (hence the _faithfulness assumption_). Together with the global Markov condition, faithfulness implies an equivalence between the probabilistic and graphical notions of conditional independence:

\[X_{i}\,\,X_{j}|X_{Z} X_{i}\, {$\!\!\!$}\,\,X_{j}|X_{Z}.\] (3)

[MISSING_PAGE_FAIL:3]

[MISSING_PAGE_EMPTY:4]

Markov equivalence class is the most we can hope to achieve without further hypotheses. As we will see in the next section, the score function can also help leverage additional restrictive assumptions on the causal mechanisms of Equation (4) to identify direct causal effects.

## 4 A theory of identifiability from the score

In this section, we show that, under additional assumptions on the data-generating process, we can identify the direct causal relations that are not influenced by unobserved variables, as well as the presence of unobserved active paths (Definition 5) between nodes in the marginalized graph \(_{V}^{}\).

As a preliminary step before diving into causal discovery with latent variables, we show how the properties of the score function identify edges in directed acyclic graphs, that is in the absence of latent variables (when \(U=\) and \(=_{V}^{}\)). The goal of the next section is two-sided: first, it introduces the fundamental ideas connecting the score function to causal discovery that also apply to hidden variable models, second, it extends the existing theory of causal discovery with score matching to additive noise models with both linear and nonlinear mechanisms.

### Warm up: identifiability without latent confounders

In this section, we summarise and extend the theoretical findings presented in Montagna et al. , where the authors show how to derive constraints on the score function that identify the causal order of the DAG \(\) where all the variables in the set \(X\) are observed. Define the structural relations of (1) as:

\[X_{i} h_{i}(X_{_{i}^{}})+N_{i},i=1,,k,\] (7)

with three times continuously differentiable mechanisms \(h_{i}\), noise terms centered at zero, and strictly positive density \(p_{X}\). Given the Markov factorization of Equation (2), the components of the score function \( p(x)\) are:

\[_{X_{i}} p(x)&=_{ X_{i}} p(x_{i}|x_{_{i}^{}})+_{j_{i}^{ }}_{X_{i}} p(x_{j}|x_{_{j}^{}}) \\ &=_{N_{i}} p(n_{i})-_{j_{i}^{ }}_{X_{i}}h_{j}(x_{_{j}^{}}) _{N_{j}} p(n_{j}),\] (8)

where \(_{i}^{}\) denotes the set of children of node \(X_{i}\). We observe that if a node \(X_{s}\) is a _sink_, i.e. a node satisfying \(_{s}^{}=\), then the summation over the children vanishes, implying that:

\[_{X_{s}} p(x)=_{N_{s}} p(n_{s}).\] (9)

The key point is that the score component of a sink node is a function of its structural equation noise term, such that one could learn a consistent estimator of \(_{X_{s}} p_{X}\) from a set of observations of the noise term \(N_{s}\). Given that, in general, one has access to \(X\) samples rather than observations of the noise random variables, authors in Montagna et al.  show that \(N_{s}\) of a sink node can be consistently estimated from i.i.d. realizations of \(X\). For each node \(X_{1},,X_{k}\), we define the quantity:

\[R_{i} X_{i}-[X_{i}|X_{ X_{i}}],\] (10)

where \(X_{ X_{i}}\) are the random variables in the set \(X\{X_{i}\}\). \([X_{i}|X_{ X_{i}}]\) is the optimal least squares predictor of \(X_{i}\) from all the remaining nodes in the graph, and \(R_{i}\) is the regression residual. For a sink node \(X_{s}\), the residual satisfies:

\[R_{s}=N_{s},\] (11)

which can be seen by rewriting \([X_{s}|X_{ X_{s}}]=h_{s}(X_{_{s}^{}} )+[N_{s}|X_{_{s}^{}},X_{_{s}^{ }}]=h_{s}(X_{_{s}^{}})+[N_{s}]\), where \(X_{_{s}^{}}\) and \(X_{_{s}^{}}\) denotes the descendants and non-descendants of \(X_{s}\), respectively. Equations (9) and (11) together imply that the score \(_{N_{s}} p(N_{s})\) is a function of \(R_{s}\), such that it is possible to find a consistent approximator of the score of a sink from observations of \(R_{s}\).

**Proposition 2** (Generalization of Lemma 1 in Montagna et al. ).: _Let \(X\) be a set of random variables, generated by a restricted additive noise model (Definition 9) with structural equations (7), and let \(X_{j} X\). Consider \(r_{j}\) in the support of \(R_{j}\). Then:_

\[X_{j}[( [_{X_{j}} p(X) R_{j}=r_{j}]-_{X_{j}} p( X))^{2}]=0.\] (12)Our result generalizes Lemma 1 in Montagna et al. , as they assume \(X\) generated by an identifiable additive noise model with nonlinear mechanisms. Instead, we remove the nonlinearity assumption and make the weaker hypothesis of a _restricted_ additive noise model, which is provably identifiable , in the formal sense defined in the appendix (Definition 8). This result doesn't come as a surprise, given the previous findings of Ghoshal and Honorio  showing that the score infers linear non-Gaussian additive noise models: Proposition 2 provides a unifying and general theory for the identifiability of models with potentially mixed linear and nonlinear mechanisms.

Based on these insights, Montagna et al.  propose the NoGAM algorithm to exploit the condition in (12) for identifying the causal order of the graph: being \([_{X_{i}} p(X) R_{i}]\) the optimal least squares estimator of the score of node \(X_{i}\) from \(R_{i}\), a sink node is characterized as the \(*{argmin}_{i}[[_{X_{i}}  p(X) R_{i}]-_{X_{i}} p(X)]^{2}\), where in practice the residuals \(R_{i}\), the score components and the least squares estimators are replaced by their empirical counterparts. After a sink node is identified, it is removed from the graph and assigned a position in the order, and the procedure is iteratively repeated up to the source nodes. Being the score estimated by score matching techniques , we usually make reference to _score matching-based_ causal discovery.

In the next section, we show how we can generalize these results to identify direct causal effects between a pair of variables in the marginal MAG \(^{}_{V}\) when \(U\)

### Identifiability in the presence of latent confounders

We now introduce the last of our main theoretical results, that is: given a pair of nodes \(V_{i}\), \(V_{j}\) that are adjacent in the graph \(^{}_{V}\) with \(U\), we can use the score function to identify the presence of a direct causal effect between \(V_{i}\) and \(V_{j}\), or that of an active path that is influenced by unobserved variables. Given that the causal model of Equation (4) ensures identifiability only up to the equivalence class, we need additional restrictive assumptions. In particular, we enforce an additive noise model with respect to both the observed and unobserved noise variables. This corresponds to an additive noise model on the observed variables with the noise terms recentered by the latent causal effects.

**Assumption 1** (SCM assumptions).: _The set of structural equations of the observable variables specified in (4) is now defined as:_

\[V_{i} f_{i}(V_{^{}_{i}})+g_{i}(U^{i})+N_{i},  i=1,,d,\] (13)

_assuming the mechanisms \(f_{i}\) to be of class \(^{3}(^{|V_{^{}_{i}}|})\), and mutually independent noise terms with strictly positive density function. The \(N_{i}\)'s are assumed to be non-Gaussian when \(f_{i}\) is linear in some of its arguments._

Crucially, our hypothesis is weaker than those required by two state-of-the-art approaches, CAM-UV  and RCD : CAM-UV assumes a Causal Additive Model (CAM) with structural equations with nonlinear mechanisms in the form \(V_{i}_{k^{}_{i}}f_{ik}(V_{k})+_{U^{i }_{k}}g_{ik}(U^{i}_{k})+N_{i}\), and RCD requires an additive noise model with linear effects of both the latent and observed causes. Thus, our model encompasses and extends the nonlinear and linear settings of CAM-UV and RCD, such that the theory developed in the remainder of the section is valid for a broader class of causal models.

Our first step is rewriting the structural relations in (13) as:

\[& V_{i} f_{i}(V_{^{}_{i} })+_{i},\\ &_{i} g_{i}(U^{i})+N_{i}, i=1,,d, \] (14)

which provides an additive noise model in the form of (7). Next, we define the following regression residuals for any node \(V_{k}\) in the graph \(^{}_{V}\):

\[R_{k}(V_{Z}) V_{k}-[V_{k} V_{Z\{k\}}],\] (15)

where \(V_{Z\{k\}}\) denotes the set of random variables \(V_{Z}\{V_{k}\}\).

Given these definitions, we are ready to show how directed edges, and the presence of unobserved variables can be identified from the score of linear and nonlinear additive noise models.

[MISSING_PAGE_EMPTY:7]

that are not affected by latents (using hypothesis testing to find vanishing mean squared error in the score predictions from the residuals), in the spirit of the NoGAM algorithm. If there is such a sink, we search all its adjacent nodes via Proposition 1 (plus an optional pruning step for better accuracy, Appendix C.2), and orient the inferred edges towards the sink. Else, if no sink can be found, we pick a node in the graph and find its neighbors by Proposition 1, orienting its edges using the condition in Proposition 3 (score estimation by residuals under latent effects). This way, we get an algorithm that is polynomial in the best case (Appendix C.3). Details on AdaScore are provided in Appendix C, while a pseudo-code summary is provided in the Algorithm 1 box.

``` while nodes remain do if Proposition 3 finds a sink with all parents observed then  add edges from adjacent nodes to sink else  pick some remaining node \(V_{i} V\)  prune neighbourhood of \(V_{i}\) using Proposition 1  orient edges adjacent to \(V_{i}\) using Proposition 3 if\(V_{i}\) has outgoing directed edge to some \(V_{j} V\)then continue with \(V_{j}\) else  remove \(V_{i}\) form remaining nodes  prune remaining bidirected edges using Proposition 1 ```

**Algorithm 1** Simplified pseudo-code of AdaScore

## 5 Experiments

We use the \(^{2}\) Python library  to generate synthetic data with known ground truths, created as Erdos-Renyi sparse and dense graphs, respectively with probability of edge between pair of nodes equals \(0.3\) and \(0.5\). We sample the data according to linear and nonlinear mechanisms with additive noise, where the nonlinear functions are parametrized by a neural network with random weights, a common approach in the literature [18; 26; 27; 28; 29]. Noise terms are sampled from a uniform distribution in the \([-2,2]\) range. Hidden causal effects are obtained by randomly picking two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for further details on the data generation. As metric, we consider the structural Hamming distance (SHD) [30; 31], a simple count of the number of incorrect edges, where missing and wrongly directed edges count as one error. We fix the level of the hypothesis tests of AdaScore to \(0.05\), which is a common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV, RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we comment on the results on datasets of \(1000\) observations from _dense_ graphs, with and without latent variables. Additional experiments including those on sparse networks are presented in Appendix E. Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [18; 32].

Discussion.Our experimental results on models without latent variables of Figure 0(a) show that when causal relations are linear, AdaScore can recover the causal graph with accuracy that is comparable with all the other benchmarks, with the exception of DirectLiNGAM. On nonlinear data AdaScore presents better performance than CAM-UV, RCD, and DirectLiNGAM while being comparable to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample errors and in the fully observable setting, NoGAM and AdaScore are indeed the same algorithms. When inferring under latent causal effects, Figure 0(b), our method performs comparably to CAM-UV and RCD on graphs up to seven nodes while slightly degrading on nine nodes. Additionally, AdaScore outperforms NoGAM in this setting, as we would expect according to our theory. Overall, we observe that our method is robust to a variety of structural assumptions, with accuracy that is often comparable and sometimes better than competitors (as in nonlinear observable settings). We remark that although AdaScore does not clearly outperform the other baselines, its broad theoretical guarantees of identifiability are not matched by any available method in the literature; this makes it an appealing option for inference in realistic scenarios that are hard to investigate with synthetic data, where the structural assumptions of the causal model underlying the observations are unknown.

## 6 Conclusion

The existing literature on causal discovery shows a connection between score matching and structure learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the presence of unobserved variables. Additionally, while previous works posit the accent on finding the causal order through the score, we study its potential to identify the Markov equivalence class with a _constraint-based_ strategy that does not explicitly require tests of conditional independence, as well as to identify direct causal effects. Our theoretical insights result in AdaScore: unlike existing approaches for the estimation of causal directions, our algorithm provides theoretical guarantees for a broad class of identifiable models, namely linear and nonlinear, with additive noise, in the presence of latent variables. Even though AdaScore does not clearly outperform the existing baselines on our synthetic benchmark, its adaptivity to different structural hypotheses is a step towards causal discovery that is less reliant on prior assumptions, which are often untestable and thus hindering reliable inference in real-world problems. While we do not touch on the task of causal representation learning , where causal variables are learned from data, we believe this is a promising research direction in relation to our work due to the specific interplay between score-matching estimation and generative models.

    & CAM-UV & RCD & NoGAM & DirectLiNGAM & AdaScore \\  Linear additive noise model & ✗ & ✓ & ✗ & ✓ \\ Nonlinear additive noise model & ✗ & ✗ & ✓ & ✗ & ✓ \\ Nonlinear CAM & ✓ & ✗ & ✓ & ✗ & ✓ \\ Latent variables effects & ✓ & ✓ & ✗ & ✗ & ✓ \\  Output & Mixed & Mixed & DAG & DAG & Mixed \\   

Table 1: Experiments causal discovery algorithms. The content of the cells denotes whether the method supports (✓) or not (✗) the condition specified in the corresponding row.

Figure 1: Empirical results on dense graphs with different numbers of nodes, on fully observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better). We note that DirectLiNGAM is surprisingly robust to different structural assumptions, and AdaScore is generally comparable or better (as in nonlinear observable data) than the other benchmarks.