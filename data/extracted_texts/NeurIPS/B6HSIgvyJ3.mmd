# A Batch-to-Online Transformation under

Random-Order Model

 Jing Dong

The Chinese University of Hong Kong, Shenzhen

jingdong@link.cuhk.edu.cn &Yuichi Yoshida

National Institute of Informatics

yyoshida@nii.ac.jp

###### Abstract

We introduce a transformation framework that can be utilized to develop online algorithms with low \(\)-approximate regret in the random-order model from offline approximation algorithms. We first give a general reduction theorem that transforms an offline approximation algorithm with low average sensitivity to an online algorithm with low \(\)-approximate regret. We then demonstrate that offline approximation algorithms can be transformed into a low-sensitivity version using a coreset construction method. To showcase the versatility of our approach, we apply it to various problems, including online \((k,z)\)-clustering, online matrix approximation, and online regression, and successfully achieve polylogarithmic \(\)-approximate regret for each problem. Moreover, we show that in all three cases, our algorithm also enjoys low inconsistency, which may be desired in some online applications.

## 1 Introduction

In online learning literature, stochastic and adversarial settings are two of the most well-studied cases. Although the stochastic setting is not often satisfied in real applications, the performance and guarantees of online algorithms in the adversarial case are considerably compromised. This is particularly true for important online tasks such as \(k\)-means clustering, which gives a significantly worse guarantee than their offline or stochastic counterparts (Cohen-Addad et al., 2021). As a result, their practical applicability is greatly limited.

Recently, the random-order model has been introduced as a means of modeling learning scenarios that fall between the stochastic and adversarial settings (Garber et al., 2020; Sherman et al., 2021). In the random-order model, the adversary is permitted to choose the set of losses, with full knowledge of the learning algorithm, but has no influence over the order in which the losses are presented to the learner. Instead, the loss sequence is uniformly and randomly permuted. This effectively bridges the gap between the stochastic setting, where only the distribution of losses can be chosen by the setting, and the adversarial setting, where the adversary has complete control over the order of the losses presented to the learner.

In this work, we introduce a batch-to-online transformation framework designed specifically for the random-order model. Our framework facilitates the conversion of an offline approximation algorithm into an online learning algorithm with \(\)-approximate regret guarantees. Our primary technical tool is average sensitivity, which was initially proposed by Varma and Yoshida (2021) to describe the algorithm's average-case sensitivity against input perturbations. We demonstrate that any offline approximation algorithm with low average sensitivity will result in a transformed online counterpart that has low \(\)-approximate regret. To achieve small average sensitivity for offline algorithms, we leverage the idea of a coreset (Agarwal et al., 2005; Har-Peled and Mazumdar, 2004), which is a small but representative subset of a larger dataset that preserves important properties of the original data. We present a coreset construction method that attains low average sensitivity, and when combined with the approximation algorithm, yields an overall algorithm with low average sensitivity.

To showcase the practicality and versatility of our framework, we apply it to three popular online learning problems: online \((k,z)\)-clustering, online matrix approximation, and online regression. In all three cases, our approach yields a polylogarithmic \(\)-approximate regret. Furthermore, due to the low average sensitivity of our algorithms, they also enjoy low inconsistency, which is the cumulative number of times the solution changes. This additional property may prove useful in certain online settings. We note that this inconsistency has also been investigated in the classic online learning and multi-armed bandits literature (Agrawal et al., 1988; Cesa-Bianchi et al., 2013).

## 2 Related Works

Average sensitivityVarma and Yoshida (2021) first introduced the notion of average sensitivity and proposed algorithms with low average sensitivity on graph problems such as minimum spanning tree, minimum cut, and minimum vertex cover problems. Various other problems have then been analyzed for the average sensitivity, including dynamic programming problems (Kumabe and Yoshida, 2022), spectral clustering (Peng and Yoshida, 2020), Euclidean \(k\)-clustering (Yoshida and Ito, 2022), maximum matching problems (Yoshida and Zhou, 2021), and decision tree learning problem (Hara and Yoshida, 2023).

Online (consistent) \((k,z)\)-clusteringWhile \((k,z)\)-clustering, which includes \(k\)-means (\(z=2\)) and \(k\)-median (\(z=1\)) as its special cases, has been studied extensively from various perspectives such as combinatorial optimization and probabilistic modeling, it can be NP-hard to obtain the exact solution (Impagliazzo et al., 2001). Thus most theoretical works have been focused on designing approximation algorithms. In the online setting, Li et al. (2018) proposed a Bayesian adaptive online clustering algorithm that enjoys a minimal sublinear regret. However, the algorithm is allowed to output more than \(k\) clusters. Without such assumption, Cohen-Addad et al. (2021) proposed the first algorithm that attains \(\)-approximate regret of \(O(kn}(^{-1}dkn))\) for \(k\)-means clustering under adversarial setting.

On a separate vein, Lattanzi and Vassilvitskii (2017) proposed an online consistent \((k,z)\)-clustering algorithm that produces a \(2^{O(z)}\)-approximate solution for the data points obtained so far at each step while maintaining an inconsistency bound of \(O(k^{2}^{4}n)\). This implies that their algorithm only updates the output \(O(k^{2}^{4}n)\) many times. Then, Yoshida and Ito (2022) gave an online algorithm with approximation ratio \((1+)\) and inconsistency \((d,k,2^{z},^{-1}) n\) in the random-order model. We remark that the way how the losses are computed in Lattanzi and Vassilvitskii (2017); Yoshida and Ito (2022) is different from that of the online setting, which Cohen-Addad et al. (2021) and our paper considered.

Online convex optimization and online principle component analysis (PCA) under the random-order modelThe online random-order optimization was proposed in Garber et al. (2020), which established a bound of \(O( n)\) for smooth and strongly convex losses. This result is then improved by Sherman et al. (2021) while still requiring smooth and convex losses.

The techniques and results are then extended to online PCA with the random-order setting, for which a regret of \(O(^{-1})\) was established, where \(\) is an instance-dependent constant. This recovers the regret for online PCA in the stochastic setting (Warmuth and Kuzmin, 2008; Nie et al., 2016). We remark that PCA can be viewed as a special case of matrix approximation, in which the matrix being approximated is the covariance matrix of the data, and we discuss the more general problem of matrix approximation in this paper.

## 3 Preliminaries

For a positive integer \(n\), let \([n]\) denote the set \(\{1,2,,n\}\). For real values \(a,b\), \(a(1)b\) is a shorthand for \((1-)b a(1+)b\).

### Offline Learning

We consider a general class of learning problems. Let \(\) be the input space, \(\) be the parameter space, and \(:_{+}\) be a loss function. For simplicity, we assume the loss is bounded, i.e.,\((,x) 1\). Given a set of \(n\) data points \(X^{n}\), we are asked to learn a parameter \(\) that minimizes the objective value \((,X):=_{x X}(,x)\). We call this problem the _offline learning problem_.

When the exact minimization of the loss function \(\) is NP-hard or computationally demanding, one may only hope to obtain an approximate solution efficiently. Specifically, for \(>0\), we say a solution \(\) is _\(\)-approximate_ for \(X^{n}\) if \((,X)_{}(,X)\). The value \(\) is called the _approximation ratio_ of the solution. We say a (possibly randomized) algorithm \(\) is _\(\)-approximate_ if the expected approximation ratio of the output solution is at most \(\).

### Online Learning with Random-Order Model

In the _online learning problem_, instead of receiving all points at once, the data arrives sequentially throughout a time horizon \(n\). Specifically, the data point comes one by one, where \(x_{t}\) comes at time \(t[n]\). At the time \(t\), using the collected data points \(X_{t-1}:=(x_{1},,x_{t-1})\), we are asked to output a parameter \(_{t}\). Then we receive the data point \(x_{t}\) and incur a loss of \((_{t},x_{t})\). In this work, we consider the _random-order model_, in which the data points \(x_{1},,x_{n}\) may be chosen adversarially, but their ordering is randomly permuted before the algorithm runs.

To evaluate our performance, we use the notion of regret, which is the cumulative difference between our solution and the best solution in hindsight. In cases where obtaining the exact solution is hard, and one may only hope to obtain an approximate solution efficiently, we use the _\(\)-approximate regret_.

**Definition 3.1** (\(\)-approximate regret for the random-order model).: _Given a (randomized) algorithm \(\) that outputs a sequence of parameters \(_{1},,_{n}\) when given input \(x_{1},,x_{n}\). The \(\)-approximate regret of \(\) for the random-order model is defined as_

\[_{}(n):=}_{,\{x_{t}\}} [_{t=1}^{n}(_{t},x_{t})-(1+)_{}_{t=1}^{n}(,x_{t})]\,.\]

_where the randomness is over the internal randomness of \(\) and the ordering of data points. When \(=0\), we simply call it the regret._

In certain cases, online algorithms are required to maintain a good solution while minimizing _inconsistency_, which is quantified as the number of times the solution changes. This can be expressed formally as \((n)=_{,\{x_{t}\}}[_{t=1}^{n-1} \{_{t}_{t+1}\}]\), where \(\) is the indicator function.

### Average sensitivity

On a high level, the notion of average sensitivity describes the differences in the performance of a randomized algorithm with respect to input changes. This difference is captured by the total variation distance, which is defined below.

**Definition 3.2**.: _For a measurable space \((,)\) and probability measures \(P,Q\) defined on \((,)\). The total variation distance between \(P\) and \(Q\) is defined as \((P,Q):=_{A}|P(A)-Q(A)|\)._

Equipped with this, the average sensitivity of a randomized algorithm is formally defined as the average total variation distance between the algorithm's output on two training data sets that differ by deleting one point randomly. For a dataset \(X=(x_{1},,x_{n})^{n}\) and \(i[n]\), let \(X^{(i)}\) denote the set \((x_{1},,x_{i-1},x_{i+1},,x_{n})\) obtained by deleting the \(i\)-th data point. Then, the following definition gives a detailed description of the notion:

**Definition 3.3** (Average Sensitivity (Varma and Yoshida, 2021; Yoshida and Ito, 2022)).: _Let \(\) be a (randomized) algorithm that takes an input \(X^{n}\) and outputs \((X)\). For \(:_{+}_{+}\), we say that the average sensitivity of \(\) is at most \(\) if_

\[_{i=1}^{n}((X),(X^{(i)})) (n)\,,\]

_for any \(X^{n}\), where we identify \((X)\) and \((X^{(i)})\) with their distributions._Batch-to-Online Transformation in the Random-Order Model

In this section, we describe a general framework that can transform any offline \((1+)\)-approximate algorithm into an online algorithm with low \(\)-approximate regret. Our goal is to show the following.

**Theorem 4.1**.: _Let \(\) be a (randomized) \((1+)\)-approximate algorithm for the offline learning algorithm with average sensitivity \(:_{+}_{+}\). Then, there exists an online learning algorithm in the random-order model such that \(_{}(n)=O(_{t=1}^{n}(t)+1)\)._

Our method is described in Algorithm 1. Let \(\) be an approximation algorithm for the offline learning problem. Then, at each time step, based on the collected data \(X_{t-1}\), we simply output \(_{t}=(X_{t-1})\).

``` Input: Offline approximation algorithm \(\).
1for\(t=1,,n\)do
2 Obtain \(_{t}\) by running \(\) on \(X_{t-1}\).
3 Receive \(x_{t}\) and \((_{t},x_{t})\). ```

**Algorithm 1**General batch-to-online conversion

To show that Algorithm 1 achieves a low approximate regret when \(\) has a low average sensitivity, the following lemma is useful.

**Lemma 4.2**.: _Let \(\) be a (randomized) algorithm for the offline learning problem with average sensitivity \(:_{+}_{+}\). Then for any input \(X^{n}\), we have_

\[_{i=1}^{n}[((X^{(i)}),x_{i})]= {n}_{i=1}^{n}[((X),x_{i})](n)\,,\]

_where \(x=a b\) means \(a-b x a+b\)._

Proof of Theorem 4.1.: Consider Algorithm 1. For any \(t[n]\), we have

\[_{,\{x_{i}\}}(_{t+1},x_{t+ 1})-(_{t+1},X_{t})=_{,\{x_{ i}\}}_{i=1}^{t}((_{t+1},x_{t+1})-( _{t+1},x_{i}))\] (By Lemma 4.2) \[=_{,\{x_{i}\}}_{i=1}^ {t}((X_{t}),x_{t+1})-((X_{t}^{(i)}),x_{t +1})+(t)\] \[_{,\{x_{i}\}}_{i =1}^{t}((X_{t}),(X_{t}^{(i)}))+ (t) 2(t)\,,\]

where the last equality follows by replacing \(x_{i}\) with \(x_{t+1}\) in \(((X_{t}^{(i)}),x_{i})\) because they have the same distribution, and the last inequality is by the average sensitivity of the algorithm.

Rearranging the terms, we have

\[_{,\{x_{i}\}}[(_{t+1},x_{t+ 1})]_{,\{x_{i}\}},X_{t})}{t}+2(t)\ _{\{x_{i}\}}[_{t}}{t}]+2 (t)\,,\]

where \(_{t}:=_{}(,X_{t})\) is the optimal value with respect to \(X_{t}\), and the second inequality holds because the approximation ratio of \(_{t+1}\) is \(1+\) in expectation.

Taking summation over both sides, we have

\[_{,\{x_{i}\}}[_{t=1}^{n}(_{t},x_{t}) ]=_{,\{x_{i}\}}[(_{1},x_{1})]+ _{,\{x_{i}\}}[_{t=1}^{n-1}(_{t+1},x_{t +1})]\]\[ 1+*{}_{\{x_{i}\}}[_{t=1}^{n-1}_{t}}{t}]+2_{t=1}^{n-1}(t)\,.\]

Fix the ordering \(x_{1},,x_{n}\), and let \(c_{i}\) (\(i[t]\)) be the loss incurred by \(x_{i}\) in \(_{n}\). In particular, we have \(_{n}=_{i=1}^{n}c_{i}\). Note that \(c_{i}\)'s are random variables depending on the ordering of data points, but their sum, \(_{n}\), is deterministic. Then, we have \(_{t}_{i=1}^{t}c_{i}\) because \(_{t}\) minimizes the loss up to time \(t\), Hence, we have

\[*{}_{\{x_{i}\}}[_{t=1}^{n}_{t}}{t}]*{}_{\{x_{i}\}}[ _{t=1}^{n}^{t}c_{i}}{t}]=*{} _{\{x_{i}\}}[_{i=1}^{n}c_{i}_{t=1}^{n}]=_{i =1}^{n}*{}_{\{x_{i}\}}[c_{i}]_{t=i}^{n}\] \[=_{n}}{n}_{i=1}^{n}_{t=i}^{n} =_{n}}{n} n=_{n}\,.\]

Therefore, we have

\[*{}_{,\{x_{i}\}}[_{t=1}^{n}( _{t},x_{t})]-(1+)_{n}=O(_{t=1}^{n} (t)+1)\,.\]

## 5 Approximation Algorithm with Low Average Sensitivity via Coreset

To design approximation algorithms for the offline learning problem with low average sensitivity, we consider the following approach: We first construct a small subset of the input that well preserves objective functions, called a coreset, with small average sensitivity, and then apply any known approximation algorithm on the coreset. Coreset is formally defined as follows:

**Definition 5.1** (Har-Peled and Mazumdar (2004); Agarwal et al. (2005)).: _Let \(:_{+}\) be a loss function and let \(X^{n}\). For \(>0\), we say that a weighted set \((Y,w)\) with \(Y X\) and \(w:Y_{+}\) is an \(\)-coreset of \(X\) with respect to \(\) if for any \(\), we have \(_{y Y}w(y)(,y)(1)_{x X}(,x)\)._

Now, we consider a popular method for constructing coresets based on importance sampling and show that it enjoys a low average sensitivity. For a data \(x X\), its _sensitivity_\(_{X}(x)\)1 is its maximum contribution to the loss of the whole dataset, or more formally

\[_{X}(x)=_{}\,.\] (1)

``` Input: Loss function \(:_{+}\), dataset \(X^{n}\), \(m\), and \(>0\)
1For each \(x X\), compute \(_{X}(x)\) and set \(p(x)=_{X}(x)/_{x^{} X}_{X}(x^{})\).
2Let \(S\) be an empty set.
3for\(i=1,,m\)do
4 Sample \(x\) with probability \(p(x)\).
5 Sample \(\) from \([p(x),(1+/2)p(x)]\) uniformly at random.
6if\(w(x)\) is undefinedthen
7\(S S\{x\}\).
8\(w(x) 1/\).
9else
10\(w(x) w(x)+1/\).
11return\((S,w)\). ```

**Algorithm 2**Coreset Construction Based on Sensitivity Sampling

\(\) Loss function \(:_{+}\), dataset \(X^{n}\), \(m\), and \(>0\)

It is known that we can construct a coreset as follows: A data point \(x X\) is sampled with probability \(p(x):=_{X}(x)/_{x^{} X}_{X}(x^{})\), and then its weight in the output coreset is increased by \(1/\), where \(\) is a slight perturbation of \(p(x)\). This process is to be repeated for a fixed number of times, where the exact number depends on the approximation ratio of the coreset. See Algorithm 2 for details. We can bound its average sensitivity as follows:

**Lemma 5.2**.: _The average sensitivity of Algorithm 2 is \(O(^{-1}m/n)\)._

A general bound on the number of times we need to repeat the process, i.e., \(m\) in Algorithm 2, to obtain an \(\)-coreset is known (see, e.g., Theorem 5.5 of Braverman et al. (2016)). However, we do not discuss it here because better bounds are known for specific problems and we do not use the general bound in the subsequent sections.

## 6 Online \((k,z)\)-Clustering

In online applications, unlabelled data are abundant and their structure can be essential, and clustering serves as an important tool for analyzing them. In this section, as an application of our general batch-to-online transformation, we describe an online \((k,z)\)-clustering method that enjoys low regret.

### Problem setup

The online \((k,z)\)-clustering problem (Cohen-Addad et al., 2021) is an instance of the general online learning problem described in Section 3. We describe the problem as follows: Let \(k 1\) be an integer and \(z 1\) be a real value. Over a time horizon \(n\), at each time step \(t\), a data point \(x_{t}^{d}\) is given. Using the set of data points \(X_{t-1}=\{x_{1},,x_{t-1}\}\), we are asked to compute a set \(Z_{t}=\{z_{1},,z_{k}\}\) of \(k\) points in \(^{d}\) that minimize \((Z_{t},x_{t}):=_{j=1,,k}\|x_{t}-z_{j}\|_{2 }^{z}\), which is the \(z\)-th power of the Euclidean distance between \(x_{t}\) and the closest point in \(Z_{t}\). Note that \(Z_{t}\) plays the role of \(_{t}\) in the general online learning problem. The regret and \(\)-approximate regret are defined accordingly.

### Method and results

One important ingredient to our method is the coreset construction method proposed by Huang and Vishnoi (2020). The method provides a unified two-stage importance sampling framework, which allows for a coreset with a size that is dimension independent. Specifically, the method constructs an \(\)-coreset of size \((\{^{-2z-2}k,2^{2z}^{-4}k^{2}\})\) in \((ndk)\) time, where the \(\) hides polylogarithmic factors in \(n\) and \(k\). We remark that the importance of sampling steps in the framework is similar to the ones described in Section 5, which thus allows us to analyze its average sensitivity.

Algorithm 3 gives a brief description of our algorithm, while a detailed description is presented in the appendix. The algorithm adheres to the standard transformation approach, whereby an offline approximation algorithm is run on the coreset derived from the aggregated data.

``` Input: Offline algorithm \(\) for \((k,z)\)-clustering, approximation ratio \(1+\), \((0,1)\). \(^{}/3\). for\(t=1,,n\)do  Construct an \(^{}\)-coreset \(C_{t-1}=(S_{t-1},_{t-1})\) on \(X_{t-1}\).  Obtain a cluster set \(Z_{t}\) by running a PTAS \(\) with approximation ratio of \((1+^{})\) on \(C_{t-1}\). Receive \(x_{t}^{d}\) and \((Z_{t},x_{t})_{+}\). ```

**Algorithm 3**Online consistent \((k,z)\)-clustering

**Theorem 6.1**.: _For any \((0,1)\), Algorithm 3 gives a regret bound of_

\[_{}(n) O(((168z)^{10z}^{-5z-15}k^ {5}+^{-2z-2}k k ) n)\,.\]

_Moreover, there exists an algorithm that enjoys the same regret bound and an inconsistency bound of \((n)=O(((168z)^{10z}^{-5z-15}k^{5}( ^{-1}kn)+^{-2z-2}k k(^{-1}kn)) n)\) for \((k,z)\)-clustering._

**Remark 6.1**.: _When \(z=2\), previous results for the adversarial setting show an \(\)-approximate regret bound of \(O(kn}(^{-1}dkn))\)(Cohen-Addad et al., 2021). In comparison, although our regret is for the random-order model, our method and results accommodate a range of values for \(z\), and the regret bound is only polylogarithmically dependent on \(n\) and is independent of the dimension \(d\)._

## 7 Online Low-Rank Matrix Approximation

Low-rank matrix approximation serves as a fundamental tool in statistics and machine learning. The problem is to find a rank-\(k\) matrix that approximates an input matrix \(^{d n}\) as much as possible. In this section, we apply the transformation framework to the offline approximation algorithm to obtain a low regret online algorithm.

### Problem setup

Low-rank matrix approximationBy the singular value decomposition (SVD), a rank-\(r\) matrix \(^{d n}\) can be decomposed as \(=^{}\), where \(^{d r}\) and \(^{n r}\) are orthonormal matrices, \(^{r r}\) is a diagonal matrix with \(\)'s singular values on the diagonal. The best rank-\(k\) approximation of \(\) is given by

\[_{k}=_{k}_{k}_{k}^{}= *{argmin}_{^{d n}:() k}\|-\|_{F}\,\]

where \(\|\|_{F}\) denotes the Frobenius norm, \(_{k}^{k k}\) is a diagonal matrix with \(_{k}\)'s top \(k\) singular values on the diagonal, and \(_{k}^{d k}\) and \(_{k}^{n k}\) are orthonormal matrices obtained from \(\) and \(\), respectively, by gathering corresponding columns. The best rank-\(k\) approximation can also be found by projecting \(\) onto the span of its top \(k\) singular vectors, that is, \(_{k}=_{k}_{k}^{}\). Then, we can say an orthonormal matrix \(\) is an \(\)-approximate solution if

\[\|-^{}\|_{F}(1+ )\|-_{k}_{k}^{} \|_{F}\.\]

The matrix approximation problem serves as an important tool in data analytics and is closely related to numerous machine learning methods such as principal component analysis and least squares analysis. When dealing with streaming data, the online version of the matrix approximation problem becomes a vital tool for designing online versions of the machine learning algorithms mentioned above.

Online matrix approximationThrough a time horizon of \(n\), we receive a column of \(\), \(a_{t}^{d}\) at each time step \(t\). We are then asked to compute \(_{t}^{d k}\) that minimizes

\[(_{t},a_{t})=\|a_{t}-_{t}_{t}^{}a_ {t}\|_{F}\.\]

Without loss of generality, we will assume that the losses are bounded between \(\). We remark that similar assumptions are also made in Nie et al. (2016).

The online matrix approximation problem serves as a core component of online machine learning algorithms such as principle component analysis. These algorithms are important to a range of applications, such as online recommendation systems and online experimental design (Warmuth and Kuzmin, 2008; Nie et al., 2016).

### Method and results

In the context of low-rank matrix approximation, the coreset of a matrix is called the projection-cost preserving samples, defined as follows:

**Definition 7.1** (Rank-\(k\) Projection-Cost Preserving Sample Cohen et al. (2017)).: _For \(n^{}<n\), a subset of rescaled columns \(^{d n^{}}\) of \(^{d n}\) is a \((1+)\) projection-cost preserving sample if, for all rank-\(k\) orthogonal projection matrices \(^{d d}\), \((1-)\|-\|_{F}^{2}\|- \|_{F}^{2}(1+)\|-\|_{F}^{2}\)._

Such sketches that satisfy Definition 7.1 can be constructed via importance sampling-based routines, which are modifications of the "leverage scores". Specifically, for the \(i\)-th column \(a_{i}\) of matrix \(A\)the _ridge leverage score_ is defined as \(_{i}()=a_{i}^{}(^{}+-_{}\|_{2}^{2}}{k})^{}a_{i}\), where \(\) denotes the Moore-Penrose pseudoinverse of a matrix (Cohen et al., 2017).

Now, we introduce our online matrix approximation algorithm in Algorithm 4, which builds upon our transformation framework. It computes the approximation of the matrix from the sketch derived from the aggregated matrix using ridge leverage scores.

``` Input: Approximation parameters \((0,1)\).
1 Set \(=O(/n)\) and \(m=O(^{-2}k(^{-1}k))\).
2for\(t=1,,n\)do
3 Construct \(_{t-1}^{d(t-1)}\) by concatenating \(a_{1}, a_{t-1}\).
4 Let \(_{t-1}^{d m}\) be the zero matrix.
5for\(j=1,,m\)do
6 Sample the \(i\)-th column \(a_{i}^{d}\) of \(_{t-1}\) with probability \(p_{i}:=(_{t-1})}{_{j=1}^{t-1}_{j}(_ {t-1})}\).
7 Sample \(w\) uniformly from \([1/},(1+)/}]\).
8 Replace the \(j\)-th column of \(_{t-1}\) with \(w a_{i}\).
9 Set \(_{t}^{d k}\) to the top \(k\) left singular vectors of \(_{t}\)Receive \(a_{t}^{d}\) and \((_{t},a_{t})_{+}\). ```

**Algorithm 4**Online low rank matrix approximation

**Theorem 7.2**.: _For any \((0,1)\), Algorithm 4 has regret \(_{}(n)=O(^{-2}k n(^{-1}kn))\). Moreover, there exists an algorithm for online low-rank matrix approximation that enjoys the same regret bound and an inconsistency bound of \((n)=O(^{-2}k n(^{-1}kn))\)._

**Remark 7.1**.: _The online matrix approximation with the random-order setting has previously been investigated in the context of principle component analysis by Garber et al. (2020). They established a regret of \(O(^{-1})\), where \(\) is the smallest difference between two eigenvalues of \(_{t}_{t}^{}\). In contrast, our result gives a polylogarithmic result on \(\)-regret, which translate to an exact regret of \(O(+O(^{-2}k n(^{-1}kn) ))\), with \(\) being the minimum possible cumulative loss attained by the hindsight best approximate._

## 8 Online Regression

In the online regression problem, at each time step \(t[n]\), we are asked to output a vector \(x_{t}^{d}\), and then we receive vectors \(a_{t}^{d}\) and \(b_{t}\) that incurs the loss of \((x_{t},a_{t},b_{t})=\|a_{t}^{}x_{t}-b\|_{2}\). Without loss of generality, we assume that the losses are bounded between \(\). We note that similar assumptions are also made in (Cesa-Bianchi et al., 1996; Ouhamma et al., 2021).

With our general reduction framework, we show an \(\)-regret upper bound as follows.

**Theorem 8.1**.: _For any \((0,1)\), Algorithm 5 has regret \(_{}(n)=O(^{-2}d n(^{-1}dn))\). Moreover, there exists an algorithm for online regression that enjoys the same regret bound and an inconsistency bound of \((n)=O(^{-2}d n(^{-1}dn))\)._

**Remark 8.1**.: _In the stochastic setting, the online regression problem has been extensively investigated (Foster, 1991; Littlestone et al., 1995; Cesa-Bianchi et al., 1996; Ouhamma et al., 2021). Using online ridge regression or forward algorithms, the regret is shown to be \(O(d n)\). In the random-order model setting, Garber et al. (2020); Sherman et al. (2021) give \(O()\)-type regret when the matrix \(\) has a small condition number. In comparison, our result attains polylogarithmic \(\)-approximate regret, while maintaining no requirement on the loss function or the condition number. Our result can be translated to an exact regret of \(O(+O(^{-2}d n(^{-1}dn) ))\), with \(\) being the minimum possible cumulative loss attained by the hindsight best parameter._

### Method and results

Similar to the low-rank matrix approximation problem, we utilize the leverage score method to learn a subspace that preserves information regarding the regression. Specifically, we use the leverage score to learn a \(\)-subspace embedding, which is defined as follows.

**Definition 8.2** (\(\)-Subspace Embedding).: _A matrix \(^{m n}\) is said to be an \(\)-subspace embedding of \(^{n d}\) if for any vector \(x^{d}\), we have \((1-)\|x\|\|x\|(1+)\|x\|\)._

The subspace embedding serves the same functionality as coreset in the problem of online regression, it preserves the loss of information while enjoying a much lower dimension. In the online regression problem context, we define the leverage score as follows.

**Definition 8.3** (Leverage Score).: _Let \(=^{}\) be the singular value decomposition of \(^{n d}\). For \(i[n]\), the \(i\)-th leverage score of \(\), is defined as \(_{i}=\|_{i,:}\|_{2}^{2}\)._

With the leverage score, we propose Algorithm 5. The algorithm follows the general transformation framework, where the regression problem is solved at every step with the sketch derived from the aggregated matrix using leverage score. For notational convenience, we construct the sketch by appending rows instead of columns as we did in Section 7.

``` Input: Approximation parameters \((0,1)\)
1 Set \(=O(/n)\) and \(m=O(^{-2}d(^{-1}d))\).
2for\(t=1,,n\)do
3 Construct \(_{t-1}^{(t-1) d}\) by stacking \(a_{1}^{}, a_{t-1}^{}\).
4 Construct \(b^{t-1}\) by stacking \(b_{1},,b_{t-1}\).
5 Set \(^{t}^{m(t-1)}\) be the zero matrix.
6for\(j=1,,m\)do
7 Sample \(i[t-1]\) with probability \(p_{i}:=(_{t-1})}{_{j=1}^{t-1}_{j}(_ {t-1})}\).
8 Sample \(w\) uniformly from \([}},}}]\).
9 Replace the \(j\)-th row of \(^{t}\) with \(w e_{i}^{}\), where \(e_{i}^{t-1}\) is a one-hot vector with \(1\) on the \(i\)-th index.
10 Solve the regression problem \(x_{t}=_{x}\|^{t}_{t-1}x-^{t}b\|_{2}\), e.g., by an iterative method such as Newton's method.
11 Receive \(a_{t}^{d}\), \(b_{t}\), and loss \((x_{t},a_{t},b_{t})\). ```

**Algorithm 5**Online consistent regression

The subspace embedding result of Woodruff (2014) immediately shows the following:

**Theorem 8.4**.: _For any \(,(0,1)\), if \(m=O(^{-2}d(^{-1}d))\), then with probability \( 1-\), \(^{t}\) is an \(\)-subspace embedding for \(_{t-1}\) with \(O(^{-2}d(^{-1}d))\) columns._

To obtain Theorem 8.1, we first analyze the average sensitivity of the leverage score sampling. Then, with Theorem 8.4 and the general reduction Theorem 4.1, we obtain the regret bound.

## 9 Experiments

We here provide a preliminary empirical evaluation of our framework in the context of online \(k\)-means clustering, and online linear regression, with the result shown in Figure 1. Our experiments are conducted with various approximation ratios and experimental setups (\(=0.1,0.01,0.001\), with \(k=3\) or \(k=5\) clusters). We then compare the performance of the proposed algorithm to the hindsight optimal solution. For \(k\)-means clustering, we obtain the hindsight optimal solution by applying \(k\)-means++ to all the data. In the context of regression, we utilize the least square formula to compute the hindsight optimal solution. Our experimental results demonstrate that the proposed algorithm is highly effective, and its performance aligns with our theoretical findings.

## 10 Conclusion

In this paper, we proposed a batch-to-online transformation framework that designs consistent online approximation algorithms from offline approximation algorithms. Our framework transforms an offline approximation algorithm with low average sensitivity to an online algorithm with low approximate regret. We then show a general method that can transform any offline approximation algorithm into one with low sensitivity by using a stable coreset. To demonstrate the generality of our framework, we applied it to online \((k,z)\)-clustering, online matrix approximation, and online regression. Through the transformation result, we obtain polylogarithmic approximate regret for all of the problems mentioned.