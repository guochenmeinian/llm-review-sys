# Online Posterior Sampling with a Diffusion Prior

Branislav Kveton

Adobe Research

&Boris N. Oreshkin

Amazon

&Youngsuk Park

AWS AI Labs

&Aniket Deshmukh

AWS AI Labs

&Rui Song

Amazon

The work was done at AWS AI Labs.

###### Abstract

Posterior sampling in contextual bandits with a Gaussian prior can be implemented exactly or approximately using the Laplace approximation. The Gaussian prior is computationally efficient but it cannot describe complex distributions. In this work, we propose approximate posterior sampling algorithms for contextual bandits with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse diffusion process, which are obtained by the Laplace approximation. Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency. They are asymptotically consistent and perform well empirically on a variety of contextual bandit problems.

## 1 Introduction

A _multi-armed bandit_[27; 6; 30] is an online learning problem where an agent sequentially interacts with an environment over \(n\) rounds with the goal of maximizing its rewards. In each round, it takes an _action_ and receives its _stochastic reward_. The mean rewards of the actions are unknown _a priori_ and must be learned. This leads to the _exploration-exploitation dilemma: explore_ actions to learn about them or _exploit_ the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and adaptation are beneficial, such recommender systems [32; 54; 25; 35] and hyper-parameter optimization .

Contextual bandits [29; 32] with linear [13; 1] and _generalized linear models (GLMs)_[17; 33; 2; 26] have become popular due to the their flexibility and efficiency. The features in these models can be hand-crafted or learned from historic data , and the models can be also updated incrementally [1; 24]. While the original algorithms for linear and GLM bandits were based on _upper confidence bounds (UCBs)_[13; 1; 17], _Thompson sampling (TS)_ is more popular in practice [11; 3; 42; 44]. The key idea in TS is to explore by sampling from the posterior distribution of model parameter \(_{*}\). TS uses the prior knowledge about \(_{*}\) to speed up exploration [11; 40; 36; 9; 21; 20; 5]. When the prior is a multivariate Gaussian, the posterior of \(_{*}\) can be updated and sampled from efficiently . This prior has a limited expressive power, because it cannot even represent multimodal distributions. To address this, we study posterior sampling with a diffusion prior. The main benefit of such priors is that they can represent complex distributions and be learned from data.

We make the following contributions. First, we propose novel posterior sampling approximations for linear models and GLMs with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse process, which are estimated in a closed form. In linear models, each conditional is a product of two Gaussians, representing prior knowledge and diffused evidence (Theorem 2). In GLMs, each conditional is obtained by a Laplace approximation, which mixes prior knowledge and evidence (Theorem 4). Our approximations are motivated by posterior sampling with Gaussian priors, and inherit its simplicity and efficiency. Prior works (Section 7) sampled from the posterior using the likelihood score, and their approximations become unstable when the score is high. We combine the likelihood with conditional priors, in eachstage of the diffusion model, using the Laplace approximation. The resulting posterior concentrates at a single point and can be sampled from efficiently even if the likelihood score is high. We prove that this approximation is asymptotically consistent.

Our second contribution is in theory. We properly derive our posterior approximations (Theorems 2 and 4) and show their asymptotic consistency (Theorem 3). The key idea in the proof of Theorem 3 is that the conditional posteriors concentrate at a scaled unknown model parameter as the number of observations increases. While this claim is asymptotic, it is an expected property of a posterior distribution. Many prior works, such as Chung et al. , do not propose asymptotically consistent approximations. All of our main results rely on a novel approximation of clean samples by scaled diffused samples (Section 4.3). The most challenging part of the analysis is Theorem 3, where we analyze an asymptotic behavior of a chain of \(T\) dependent random vectors.

Our last contribution is an empirical evaluation on contextual bandits. We focus on bandits because the ability to represent all levels of uncertainty precisely is critical for exploration. Our experiments show that a score-based method fails to do so (Section 6.2). Note that our posterior approximations are general and not restricted to bandits.

## 2 Setting

We start with introducing our notation. Random variables are capitalized, except for Greek letters like \(\). We denote the marginal and conditional probabilities under probability measure \(p\) by \(p(X=x)\) and \(p(X=x Y=y)\), respectively. When the random variables are clear from context, we write \(p(x)\) and \(p(x y)\). We denote by \(X_{n:m}\) and \(x_{n:m}\) a collection of random variables and their values, respectively. For a positive integer \(n\), we define \([n]=\{1,,n\}\). The indicator function is \(\{\}\). The \(i\)-th entry of vector \(v\) is \(v_{i}\). If the vector is already indexed, such as \(v_{j}\), we write \(v_{j,i}\). We denote the maximum and minimum eigenvalues of matrix \(M^{d d}\) by \(_{1}(M)\) and \(_{d}(M)\), respectively.

The posterior sampling problem can be formalized as follows. Let \(_{*}\) be an unknown _model parameter_ and \(^{d}\) be the space of model parameters. Let \(h=\{(_{},y_{})\}_{[N]}\) be the _history_ of \(N\) noisy observations of \(_{*}\), where \(_{}^{d}\) is the feature vector for \(y_{}\). We assume that

\[y_{}=g(_{}^{}_{*})+_{}\,,\] (1)

where \(g:\) is the _mean function_ and \(_{}\) is an independent zero-mean \(^{2}\)-sub-Gaussian noise for \(>0\). Let \(p(h_{*})\) be the _likelihood_ of observations in history \(h\) under model parameter \(_{*}\) and \(p(_{*})\) be its _prior probability_. By Bayes' rule, the posterior distribution of \(_{*}\) given \(h\) is

\[p(_{*} h) p(h_{*})\,p(_{*})\,.\] (2)

We want to sample from \(p( h)\) efficiently when the prior distribution is represented by a diffusion model. As a stepping stone, we review existing posterior formulas for multivariate Gaussian priors. This motivates our solution for diffusion model priors.

### Linear Model

The posterior of \(_{*}\) in linear models can be derived as follows.

**Assumption 1**.: _Let \(g\) in (1) be an identity and \(_{}(0,^{2})\). Then the likelihood of \(h\) under model parameter \(_{*}\) is \(p(h_{*})[-_{=1}^{N}(y_{}-_{}^{} _{*})^{2}/(2^{2})]\)._

Let \(p(_{*})=(_{*};_{0},_{0})\) be the prior distribution of \(_{*}\), where \(_{0}^{d}\) and \(_{0}^{d d}\) are the prior mean and covariance, respectively. Then \(p(_{*} h)(_{*};,)\), where

\[=(_{0}^{-1}_{0}+^{-2}_{ =1}^{N}_{}y_{})\,,=(_{0}^{- 1}+^{-2}_{=1}^{N}_{}_{}^{})^{-1}\,,\]

are the posterior mean and covariance, respectively. In this work, we write them equivalently as

\[=(_{0}^{-1}_{0}+^{-1})\,,=(_{0}^{-1}+^{-1})^{-1}\,,\] (3)

where \(=^{-2}_{=1}^{N}_{}y_{}\) and \(^{-1}=^{-2}_{=1}^{N}_{}_{}^{}\) are the empirical mean and inverse of its covariance, respectively. Therefore, the posterior of \(_{*}\) is a product of two multivariate Gaussians: \((_{0},_{0})\) representing prior knowledge about \(_{*}\) and \((,)\) representing empirical evidence.

### Generalized Linear Model

_Generalized linear models (GLMs)_ extend linear models (Section 2.1) to non-linear monotone _mean functions_\(g\) in (1). For instance, in logistic regression, \(g(u)=1/(1+[-u])\) is a sigmoid. The likelihood of observations in GLMs has the following form .

**Assumption 2**.: _Let \(h=\{(_{},y_{})\}_{[N]}\) be a history of \(N\) observations under mean function \(g\) and the corresponding noise. Then \( p(h_{*})_{=1}^{N}y_{}_{}^{} _{*}-b(_{}^{}_{*})+c(y_{}),\) where \(c\) is a real function and \(b\) is a function whose derivative is the mean function, \(=g\)._

The posterior distribution of \(_{*}\) in GLMs does not have a closed form in general . Therefore, it is often approximated by the _Laplace approximation_. Let the prior distribution of the model parameter be \(p(_{*})=(_{*};_{0},_{0}),\) as in Section 2.1. Then the Laplace approximation is \((,)\), where \(\) is the _maximum a posteriori (MAP) estimate_ of \(_{*}\) and \(\) is the corresponding covariance. Note that the Laplace approximation can be applied to non-Gaussian priors.

The MAP estimate \(\) can be obtained by _iteratively reweighted least squares (IRLS)_, which we present in Algorithm 1. IRLS is a Newton-type algorithm that computes \(\) iteratively (lines 6 and 7). It converges to the optimal solution due to the strong convexity of the problem. The solution has a similar structure to (3). That is, \((,)\) is a product of two multivariate Gaussians, representing prior knowledge about \(_{*}\) and empirical evidence. The new quantities in GLMs are the derivative of the mean function \(\) and pseudo-observations \(z_{}\) (line 5), which play the role of observations \(y_{}\) in Section 2.1.

```
1:Input: Prior parameters \(_{0}\) and \(_{0}\), history of observations \(h=\{(_{},y_{})\}_{[N]}\)
2:Initialize \(^{d}\)
3:repeat
4:for stage \(=1,,N\)do
5:\(z_{}_{}^{}+(y_{}-g(_{}^{ }))/(_{}^{})\)
6:\((_{0}^{-1}+_{=1}^{N}(_{ }^{})_{}_{}^{})^{-1}\)
7:\((_{0}^{-1}_{0}+_{= 1}^{N}(_{}^{})_{}z_{})\)
8:until\(\) converges
9:Output: Posterior mean \(\) and covariance \(\) ```

**Algorithm 1**IRLS: Iteratively reweighted least squares.

### Towards Diffusion Model Priors

The assumption that \(p(_{*})=(_{*};_{0},_{0})\) is limiting, for instance because it precludes multimodal priors. We relax it by representing \(p(_{*})\) by a diffusion model, which we call a _diffusion model prior_. We propose efficient posterior sampling approximations for this prior, where the prior and empirical evidence are mixed similarly to (3) and IRLS. We review diffusion models next.

## 3 Diffusion Models

Diffusion models [46; 19] are generative models trained by diffusing samples from unknown and hard to represent distributions. They can be viewed in multiple ways . We adopt the probabilistic formulation and presentation of Ho et al. . A _diffusion model_ is a graphical model with \(T\) stages indexed by \(t[T]\). Each stage \(t\) is associated with a _latent variable_\(S_{t}^{d}\). A _sample_ from the model is represented by an _observed variable_\(S_{0}^{d}\). We visualize a diffusion model in Figure 1. In the _forward process_, a clean sample \(s_{0}\) is diffused through a sequence of variables \(S_{1},,S_{T}\). This process is used to learn the _reverse process_, where the clean sample \(s_{0}\) is generated through a sequence of variables \(S_{T},,S_{0}\). To sample \(s_{0}\) from the posterior (Section 4), we add a random variable \(H\) that represents partial information about \(s_{0}\). We introduce forward and reverse diffusionprocesses next. Learning of the reverse process is described in Appendix B. While this is a critical component of diffusion models, it is not necessary to introduce our posterior approximations.

**Forward process.** In the forward process, a clean sample \(s_{0}\) is diffused through a chain of latent variables \(S_{1}, S_{T}\) (Figure 1). We denote the probability measure under this process by \(q\) and define its joint probability distribution as

\[q(s_{1:T} s_{0})=_{t=1}^{T}q(s_{t} s_{t-1})\,, t[T ]:q(s_{t} s_{t-1})=(s_{t};}s_{t-1},_{t}I_ {d})\,,\] (4)

where \(q(s_{t} s_{t-1})\) is the conditional density of mapping a less diffused \(s_{t-1}\) to a more diffused \(s_{t}\). The diffusion rate is set by parameters \(_{t}(0,1)\) and \(_{t}=1-_{t}\). The forward process is sampled from as follows. First, a clean sample \(s_{0}\) is chosen. Then \(S_{t} q( s_{t-1})\) are sampled, from \(t=1\) to \(t=T\).

**Reverse process.** In the reverse process, a clean sample \(s_{0}\) is generated through a chain of variables \(S_{T},,S_{0}\) (Figure 1). We denote the probability measure under this process by \(p\) and define its joint probability distribution as

\[p(s_{0:T}) =p(s_{T})_{t=1}^{T}p(s_{t-1} s_{t})\,,\] (5) \[p(s_{T}) =(s_{T};_{d},I_{d})\,, t[T]:p(s_ {t-1} s_{t})=(s_{t-1};_{t}(s_{t}),_{t})\,,\]

where \(p(s_{t-1} s_{t})\) is the conditional density of mapping a more diffused \(s_{t}\) to a less diffused \(s_{t-1}\). The function \(_{t}\) predicts the mean of \(S_{t-1} s_{t}\) and is learned (Appendix B). As in Ho et al. , we keep the covariance fixed at \(_{t}=_{t}I_{d}\), where \(_{t}=_{t-1}}{1-_{t}}_{t}\) and \(_{t}=_{=1}^{t}_{}\). This is known as a _stable diffusion_. We make this assumption only to simplify exposition. All our derivations in Section 4 hold when \(_{t}\) is learned, for instance as in Bao et al. .

This process is called reverse because it is learned by reversing the forward process. The reverse process is sampled from as follows. First, an initial diffused sample \(S_{T} p\) is sampled. After that, \(S_{t-1} p( s_{t})\) are sampled, from \(t=T\) to \(t=1\).

## 4 Posterior Sampling

This section is organized as follows. In Section 4.1, we show how to sample from a chain of random variables conditioned on observations. In Sections 4.2 and 4.4, we specialize this to the observation models in Section 2.

### Chain Model Posterior

Let \(h=\{(_{t},y_{})\}_{[N]}\) denote a _history_ of \(N\) observations (Section 2) and \(H\) be the corresponding random variable. In this section, we assume that \(h\) is fixed. The Markovian structure of the reverse process (Figure 1) implies that the joint probability distribution conditioned on \(h\) factors as

\[p(s_{0:T} h)=p(s_{T} h)_{t=1}^{T}p(s_{t-1} s_{t},h)\,.\]

Therefore, \(p(s_{0:T} h)\) can be sampled from efficiently by first sampling from \(p(s_{T} h)\) and then from \(T\) conditional distributions \(p(s_{t-1} s_{t},h)\). We derive these next.

**Lemma 1**.: _Let \(p\) be a probability measure over the reverse process (Figure 1). Then_

\[p(s_{T} h) _{s_{0}}p(h s_{0})\,p(s_{0} s_{T})\, s_{0}\,p(s_{T})\,,\] \[ t[T]\{1\}:p(s_{t-1} s_{t},h) _{s_{0}}p(h s_{0})\,p(s_{0} s_{t-1})\, s_{0}\,p(s_{t-1} s_{t})\,,\] \[p(s_{0} s_{1},h)  p(h s_{0})\,p(s_{0} s_{1})\,.\]

Figure 1: Graphical models of the forward and reverse processes in the diffusion model. The variable \(H\) represents partial information about \(S_{0}\).

Proof.: The claim is proved in Appendix A.1. 

### Linear Model Posterior

Now we specialize Lemma 1 to the diffusion model prior (Section 3) and linear models (Section 2.1). The prior distribution is the reverse process in (5),

\[p(s_{T})=(s_{T};_{d},I_{d})\,, t[T]:p(s_{t-1 } s_{t})=(s_{t-1};_{t}(s_{t}),_{t})\,.\]

The term \(p(h s_{0})\) is the likelihood of observations in Assumption 1. The main challenge in using the lemma is that the conditional densities of clean samples \(p(s_{0} S_{T})\) and \(p(s_{0} s_{t})\) are complex . To get around this, we make an additional assumption, which is discussed in Section 4.3.

**Theorem 2**.: _Let \(p\) be a probability measure over the reverse process (Figure 1). Let \(\) and \(^{-1}\) be defined as in (3). Suppose that_

\[_{s_{0}}p(h s_{0})\,p(s_{0} s_{t})\,s_{0} p(h s _{t}/_{t}})\] (6)

_holds for all \(t[T]\). Then \(p(s_{T} h)(s_{T};_{T+1}(h),_{T+1}(h))\), where_

\[_{T+1}(h)=_{T+1}(h)(\,_{d}}_{ }+^{-1}/_{T }}}_{})\,,_{T+1}(h)=(}_{}+^{-1}/_{T}}_{})^{-1}\,.\] (7)

_For \(t[T]\), we have \(p(s_{t-1} s_{t},h)(s_{t-1};_{t}(s_{t},h), {}_{t}(h))\), where_

\[_{t}(s_{t},h)=_{t}(h)(^{-1}_{t} (s_{t})}_{}+^{-1}/_{t-1}}}_{})\,,_{t}(h)=(^{-1}}_{}+^{-1}/_{t -1}}_{})^{-1}\,.\] (8)

Proof.: The proof is in Appendix A.2. It has four steps. First, we fix stage \(t\) and apply approximation (6). Second, we rewrite the likelihood as in (3). Third, we reparameterize it as a function of \(s_{t}\). At the end, we combine the likelihood with the Gaussian prior using Lemma 6 in Appendix A.5. 

The algorithm that samples from the posterior distribution in Theorem 2 is presented in Algorithm 2. We call it _Laplace diffusion posterior sampling_ (LaplaceDPS) because its generalization to GLMs uses the Laplace approximation. LaplaceDPS samples from a chain of products of two distributions: one distribution represents the pre-trained diffusion model and does not depend on history \(h\), and the other represents the history \(h\). The sampling is implemented as follows. The initial variable \(S_{T}\) is sampled conditioned on \(h\) (line 2) from the distribution in (7). This distribution is a product of the \(h\)-independent prior \((_{d},I_{d})\) and the \(h\)-dependent distribution of the diffused evidence up to stage \(T\), \((_{T}},_{T})\). Then, for any \(t[T]\), \(S_{t-1}\) is sampled conditioned on \(s_{t}\) and evidence \(h\) (line 4) from the distribution in (8). This distribution is a product of the \(h\)-independent conditional prior \((_{t}(s_{t}),_{t})\), from the pre-trained model, and the \(h\)-dependent distribution of the diffused evidence up to stage \(t-1\), \((_{t-1}},_{t-1})\). The last variable \(S_{0}\) is the clean sample. When compared to Section 2, the prior and evidence are mixed conditionally in a \(T\)-stage chain. This increases the computational cost \(T\) times, as discussed in Section 8.

```
1:Input: Diffusion model parameters \((_{t},_{t})_{t[T]}\), history of observations \(h\)
2:Initial sample \(S_{T}(_{T+1}(h),_{T+1}(h))\)
3:for stage \(t=T,,1\)do
4:\(S_{t-1}(_{t}(S_{t},h),_{t}(h))\)
5:Output: Posterior sample \(S_{0}\) ```

**Algorithm 2**LaplaceDPS: Laplace posterior sampling with a diffusion model prior.

### Key Approximation in Theorem 2

Now we motivate our assumption in (6). Simply put, we assume that \(s_{0}=s_{t}/_{t}}\), where \(s_{0}\) is a clean sample and \(s_{t}\) is the corresponding diffused sample in stage \(t\). This is motivated by the forward process, which relates \(s_{t}\) and \(s_{0}\) as \(s_{t}=_{t}}s_{0}+_{t}}_{t}\), where \(_{t}(_{d},I_{d})\) is a standard Gaussian noise . After rearranging, we get \(s_{0}=(s_{t}-_{t}}_{t})/_{t}}\), and therefore \(s_{0}\) can be viewed as a random variable with mean \(s_{t}/_{t}}\). The consequence of (6) is that the likelihood becomes a function of \(s_{t}\), which yields a closed form when multiplied by the conditional prior, also a function of \(s_{t}\). Our approximation can be also viewed as the Tweedie's formula in Chung et al.  where the score component is neglected.

Our approximation has several notable properties. First, \(_{t})/_{t}} 0\) as \(t 1\). Therefore, it becomes more precise in later stages of the reverse process. Second, in the absence of evidence \(h\), the approximation vanishes, and all posterior distributions in Theorem 2 reduce to the priors in (5). Finally, as the number of observations increases, sampling from the posterior in Theorem 2 is asymptotically consistent.

**Theorem 3**.: _Fix \(_{*}^{d}\). Let \(((_{t},_{t})_{t [T]},h)\), where \(h=\{(_{},y_{})\}_{[N]}\) is a history of \(N\) observations. Suppose that \(_{d}(^{-1})\) as \(N\), where \(\) is defined in (3). Then \((_{N}\|-_{*}\|_{2}=0)=1\)._

Proof.: The proof is in Appendix A.3. The key idea is that the conditional posteriors in (7) and (8) concentrate at a scaled unknown model parameter \(_{*}\) as the number of observations increases, which we formalize as \(_{d}(^{-1})\). 

The bound in Theorem 3 can be interpreted as follows. The sampled parameter \(\) approaches the true unknown parameter \(_{*}\) as the number of observations \(N\) increases. To guarantee that the posterior shrinks uniformly in all directions, we assume that the number of observations in all directions grows linearly with \(N\). This is akin to assuming that \(_{d}(^{-1})=(N)\). This lower bound can be attained in linear models by getting observations according to the D-optimal design .

### GLM Posterior

The Laplace approximation in GLMs (Section 2.2) naturally generalizes the exact posterior distribution in linear models (Section 2.1). We generalize Theorem 2 to GLMs along the same lines.

**Theorem 4**.: _Let \(p\) be a probability measure over the reverse process (Figure 1). Suppose that (6) holds for all \(t[T]\). Then \(p(s_{T} h)(s_{T};_{T+1}(h),_{T+1}( h))\), where_

\[_{T+1}(h)=_{T}}_{T+1}\,,_{T+1}(h)=_{T}_{T+1}\,,_{T+1}, _{T+1}(_{d},I_{d}/_{ T},h)\,.\]

_For \(t[T]\), we have \(p(s_{t-1} s_{t},h)(s_{t-1};_{t}(s_{t},h),_{t}(h))\), where_

\[_{t}(s_{t},h)=_{t-1}}_{t}\,,_{t}(h)=_{t-1}_{t}\,,_{t}, {}_{t}(_{t}(s_{t})/_{t-1}}, _{t}/_{t-1},h)\,.\]

Proof.: The proof is in Appendix A.4. It has four steps. First, we fix stage \(t\) and apply approximation (6). Second, we reparameterize the prior, from a function of \(s_{t}\) to a function of \(s_{t}/_{t}}\). Third, we combine the likelihood with the prior using the Laplace approximation. Finally, we reparameterize the posterior, from a function of \(s_{t}/_{t}}\) to a function of \(s_{t}\). 

Similarly to Theorem 2, the distributions in Theorem 4 mix evidence with the diffusion model prior. However, this is done implicitly in IRLS. The posterior can be sampled from using LaplaceDPS, where the mean and covariances would be taken from Theorem 4. Note that Theorem 2 is a special case of Theorem 4 where the mean function \(g\) is an identity.

Application to Contextual Bandits

Now we apply our posterior sampling approximations (Section 4) to contextual bandits. A _contextual bandit_[29; 32] is a classic model for sequential decision making under uncertainty where the agent takes actions conditioned on context. We denote the _action set_ by \(\) and the _context set_ by \(\). The _mean reward_ for taking action \(a\) in context \(x\) is \(r(x,a;_{*})\), where \(r:\) is a _reward function_ and \(_{*}\) is a _model parameter_ (Section 2). The agent interacts with the bandit for \(n\) rounds indexed by \(k[n]\). In round \(k\), it observes a _context_\(x_{k}\), takes an _action_\(a_{k}\), and observes its _stochastic reward_\(y_{k}=r(x_{k},a_{k};_{*})+_{k}\) with independent noise \(_{k}\). We assume that the noise is zero-mean \(^{2}\)-sub-Gaussian for \(>0\). The objective of the agent is to maximize its cumulative reward in \(n\) rounds, or equivalently to minimize its cumulative regret. We define the \(n\)_-round regret_ as

\[R(n)=_{k=1}^{n}[r(x_{k},a_{k,*};_{*})-r(x_{k},a_{k}; _{*})]\,,\] (9)

where \(a_{k,*}=*{arg\,max}_{a}r(x_{k},a;_{*})\) is the optimal action in round \(k\).

Arguably the most popular method for solving contextual bandit problems is Thompson sampling [50; 11; 3]. The key idea in TS is to use the posterior distribution of \(_{*}\) to explore. This is done as follows. In round \(k\), the model parameter is drawn from the posterior in (2), \(_{k} p( h_{k})\), where \(h_{k}\) is the _history_ of all interactions up to round \(k\). After that, the agent takes the action with the highest mean reward under \(_{k}\). The pseudo-code of this algorithm is given in Algorithm 3.

A _linear bandit_ has a linear reward function \(r(x,a;_{*})=(x,a)^{}_{*}\), where \(:^{d}\) is a _feature extractor_. The feature extractor can be non-linear in \(x\) and \(a\). Therefore, linear bandits can be applied to non-linear functions of \(x\) and \(a\). The feature extractor can be either learned  or hand-crafted. We denote the feature vector of the action in round \(k\) by \(_{k}=(x_{k},a_{k})\). Therefore, the _history_ of interactions up to round \(k\) is \(h_{k}=\{(_{},y_{})\}_{[k-1]}\). When the prior distribution is a Gaussian, \(p(_{*})=(_{*};_{0},_{0})\), the posterior in round \(k\) is a Gaussian in (3) for \(h=h_{k}\). When the prior is a diffusion model, we propose sampling from the posterior using

\[_{k}((_{t},_{t})_{t[T]},h_{k})\,,\] (10)

where \(_{t}\) and \(_{t}\) in \(\) are computed as in Theorem 2. We call this algorithm DiffTS.

A _generalized linear bandit_[17; 24; 33; 26] is an extension of linear bandits to generalized linear models (Section 2.2). When \(p(_{*})=(_{*};_{0},_{0})\), the Laplace approximation to the posterior is a Gaussian (Section 2.2). When the prior is a diffusion model, we propose posterior sampling using (10), where \(_{t}\) and \(_{t}\) in \(\) are computed as in Theorem 4.

## 6 Experiments

We conduct three experiments: synthetic problems in \(2\) dimensions (Section 6.2 and Appendix C.1), a recommender system (Section 6.3), and a classification problem (Appendix C.2). In addition, we conduct an ablation study in Appendix C.3, where we vary the number of training samples for the diffusion prior and the number of diffusion stages \(T\).

### Experimental Setup

We have four baselines. Three baselines are variants of contextual Thompson sampling [11; 3]: with an uninformative Gaussian prior (TS), a learned Gaussian prior (TunedTS), and a learned Gaussian mixture prior (MixTS) . The last baseline is diffusion posterior sampling (DPS) of Chung et al. . We implement all TS baselines as described in Section 5. The uninformative prior is \((_{d},I_{d})\). MixTS is used only in linear bandit experiments because the logistic regression variant does not exist. The TS baselines are chosen to cover various levels of prior information. Our implementation of DPS is described in Appendix D. We also experimented with frequentist baselines, such as LinUCB and the \(\)-greedy policy. They performed worse than TS and thus we do not report them.

Each experiment is set up as follows. First, the prior distribution of \(_{*}\) is specified: it can be synthetic or estimated from real-world data. Second, we learn this distribution from \(10\,000\) samples from it. In DiffTS and DPS, we follow Appendix B. The number of stages is \(T=100\) and the diffusion factor is \(_{t}=0.97\). Since \(0.97^{100} 0.05\), most of the information in the training samples is diffused. The regressor in Appendix B is a \(2\)-layer neural network with ReLU activations. In TunedTS, we fit the mean and covariance using maximum likelihood estimation. In MixTS, we fit the Gaussian mixture using scikit-learn. All algorithms are evaluated on \(_{*}\) sampled from the true prior. The regret is computed as defined in (9). All error bars are standard errors of the estimates.

### Synthetic Experiment

The first experiment is on three synthetic problems. Each problem is a linear bandit (Section 5) with \(K=100\) actions in \(d=2\) dimensions. The reward noise is \(=1\). The feature vectors of actions are sampled uniformly at random from a unit ball. The prior distributions of \(_{*}\) are shown in Figure 2. The first is a mixture of two Gaussians and the last can be approximated well by a mixture of two Gaussians. We implement MixTS with two mixture components. Therefore, it can represent the first prior exactly and approximate the last one well.

Our results are reported in Figure 2. We observe two main trends. First, samples from the diffusion prior closely resemble those from the true prior. In such cases, DiffTS is expected to perform well and even outperforms MixTS, because it has a better representation of the prior. We observe this in all problems. Second, DPS diverges as the number of rounds increases. This is because DPS uses an approximation based on the likelihood score (Section 7), which is unstable when the score is high. This happens despite our best efforts to tune DPS (Appendix D). We report results on another three synthetic problems in Appendix C.1.

DiffTS should be \(T\) times more computationally costly than TS with a Gaussian prior (Section 4.2). We observe this empirically. As an example, the average cost of \(100\) runs of DiffTS on any problem in Figure 2 is \(12\) seconds. The average cost of TS is \(0.1\) seconds. The computation and accuracy can be traded off, and we investigate this in Appendix C.3. In the cross problem, we vary the number of diffusion stages from \(T=1\) to \(T=300\). We observe that the computational cost is linear in \(T\) and the regret drops quickly from \(26\) at \(T=1\) to \(15\) at \(T=50\).

### MovieLens Experiment

In the second experiment, we learn to recommend an item to randomly arriving users. The problem is simulated using the MovieLens 1M dataset , with one million ratings for \(3\,706\) movies from \(6\,040\) users. We subtract the mean rating from all ratings and complete the sparse rating matrix \(M\) by

Figure 2: Evaluation of DiffTS on three synthetic problems. The first row shows samples from the true (blue) and diffusion model (red) priors. The second row shows the regret of DiffTS and the baselines as a function of round \(n\).

alternating least squares  with rank \(d=5\). The learned factorization is \(M=UV^{}\). The \(i\)-th row of \(U\), denoted by \(U_{i}\), represents user \(i\). The \(j\)-th row of \(V\), denoted by \(V_{j}\), represents movie \(j\). We use movie embeddings \(V_{j}\) as model parameters and user embeddings \(U_{i}\) as features of the actions. The movies are items.

We experiment with both linear and logistic bandits. In both, an item is initially chosen randomly from \(V_{j}\) and \(K=10\) actions are chosen randomly from \(U_{i}\) in each round. In the linear bandit, the mean reward of item \(j\) for user \(i\) is \(U_{i}^{}V_{j}\). The reward noise is \(=0.75\), and we estimate it from data. In the logistic bandit, the mean reward is \(g(U_{i}^{}V_{j})\), where \(g\) is a sigmoid.

Our MovieLens results are reported in Figure 3 and we observe similar trends to Section 6.2. First, samples from the diffusion prior closely resemble those from the true prior (Figure 2(a)). Since the problem is higher dimensional, we visualize the overlap using UMAP . Second, DiffTS has a lower regret than all baselines, in both linear (Figure 2(b)) and logistic (Figure 2(c)) bandits. Finally, MixTS barely outperforms TunedTS. We observe this trend consistently in higher dimensions, and this motivated our work on online learning with more complex priors.

## 7 Related Work

We start with reviewing related works on bandits with diffusion models. Hsieh et al.  proposed Thompson sampling with a diffusion model prior for \(K\)-armed bandits. There are multiple technical differences from our work. First, the diffusion model in Hsieh et al.  is over scalars representing individual arms. Our model is over vectors representing model parameters, and thus can be applied to contextual bandits. Second, the approximations are different. In stage \(t\), Hsieh et al.  sample from two distributions: the conditional prior and the distribution of the diffused empirical mean up to stage \(t\). Then they take a weighted sum of the samples. We sample only once, from the posterior distribution that combines the conditional prior in stage \(t\) and likelihood. Therefore, the method of Hsieh et al.  can be viewed as a non-contextual variant of our method, where posterior sampling is done by weighting samples from the prior and empirical distributions. Finally, Hsieh et al.  do not analyze their approximation.

Aouali  proposed and analyzed contextual bandits with a linear diffusion model prior: \(_{t}(s_{t})\) in (5) is linear in \(s_{t}\) and \(q(s_{0})\) is a Gaussian. Therefore, this model is a linear Gaussian model and not a general diffusion model, as in our work.

The closest related work on posterior sampling in diffusion models is DPS of Chung et al. . The key idea in DPS is to sample from the posterior distribution using the likelihood score \( p(h)\), where \(p(h)\) is the likelihood (Assumptions 1 and 2). Note that \( p(h)\) grows linearly in \(N\) because the history \(h\) in \(p(h)\) involves \(N\) terms. Therefore, DPS becomes unstable as \(N\). We show it empirically in Section 6.2 and discuss the implementation of DPS in Appendix D, which was tuned to improve its stability.

Many other posterior sampling methods for diffusion models have been proposed recently: a sequential Monte Carlo approximation for the conditional reverse process , a variant of DPS with an uninformative prior , a pseudo-inverse approximation to the likelihood of evidence , and

Figure 3: Evaluation of DiffTS on the MovieLens dataset: (a) shows samples from the true (blue) and diffusion model (red) priors, (b) shows regret in the linear bandit, and (c) shows regret in the logistic bandit.

posterior sampling in latent diffusion models . All of these methods rely on the likelihood score \( p(h)\) and thus become unstable as the number of observations \(N\) increases. Our posterior approximations do not have this issue because they are based on the product of prior and evidence distributions (Theorems 2 and 4), and thus gradient-free. They work well across different levels of uncertainty (Section 6) and do not require tuning.

We note that posterior sampling is a special form of inference-time guidance in diffusion models. Other approaches are conditional pre-training , a constraint in the reverse process , refining the null-space content , solving an optimization problem that pushes the reverse process towards evidence , and aligning the reverse process with the prompt .

## 8 Conclusions

We propose posterior sampling approximations for diffusion models priors. These approximations are contextual, and can be implemented efficiently in linear models and GLMs. We analyze them and evaluate them empirically on contextual bandit problems. Our method has two main limitations.

**Computational cost.** The cost of posterior sampling in LaplaceDPS with \(T\) stages is about \(T\) times higher than that of posterior sampling with a Gaussian prior (Section 2). We validate it empirically in Section 6.2. We plot the sampling time as a function of \(T\) in Figure 5(c) (Appendix C.3).

**Learning cost and hyper-parameter tuning.** In all experiments, the number of diffusion stages is \(T=100\) and the diffusion rate is set such that most of the signal diffuses. The regressor is a \(2\)-layer neural network and we learn it from \(10\,000\) samples from the prior. These settings resulted in stable performance in all our experiments (Section 6). However, they clearly impact the performance. We plot the regret as a function of the number of training samples in Figure 5(a) and as a function of \(T\) in Figure 5(b). When \(T\) or the number of training samples is small, DiffTS performs very similarly to posterior sampling with a Gaussian prior. In summary, there is no benefit in these cases.

**Future work.** We develop novel posterior approximations rather than bounding their regret. This is because the existing approximations are unstable and may diverge in the online setting (Sections 6.2 and 7). We believe that a proper regret analysis of DiffTS is possible and would require bounding two errors. The first error arises because the reverse process does not reverse the forward process exactly (Appendix B). The second error arises because our posterior distributions are approximate (Section 4.3). One possibility is to start with prior works that already showed the utility of complex priors. For instance, Russo and Van Roy  proved a \(O()n})\) regret bound for a linear bandit, where \(\) is the maximum ratio of regret to information gain and \(H(A_{*})\) is the entropy of the distribution of the optimal action under the prior. This bound holds for any prior and says that a lower entropy \(H(A_{*})\), which corresponds to more informative priors, yields a lower regret.

We also believe that our ideas can be extended beyond GLMs. The key idea in Section 4.4 is to use the Laplace approximation of the likelihood. This approximation can be computed exactly in GLMs. More generally though, it is a good approximation whenever the likelihood can be approximated well by a single Gaussian distribution. By the central limit theorem, under appropriate assumptions, this is expected for any observation model when the number of observations is large.