# Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense

Kalpesh Krishna\({}^{}\)\({}^{}\) Yixiao Song\({}^{}\) Marzena Karpinska\({}^{}\)

John Wieting\({}^{}\)\({}^{}\) Mohit Iyyer\({}^{}\)\({}^{}\)

\({}^{}\)University of Massachusetts Amherst, \({}^{}\)Google, \({}^{}\)Google DeepMind

{kalpeshk,jwieting}@google.com

yixiaosong@umass.edu {mkarpinska,miyyer}@cs.umass.edu

Work done as a PhD student at UMass, and partially as a student researcher in Google Research.John Wieting and Mohit Iyyer contributed equally as advisors.

###### Abstract

The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to _paraphrases_ of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (diper) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using dipper to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, dipper drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.

To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on _retrieving_ semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.1

## 1 Introduction

Large language models (LLMs) such as ChatGPT (Schulman et al., 2022) exhibit an unprecedented ability to write coherent and relevant long-form text in response to user-specified prompts. These abilities have sparked fears of malicious applications such as automatically generating fake news articles or homework answers (Stokel-Walker, 2022). To defend against these use cases, several algorithms have recently been proposed to detect AI-generated text, including watermarking (Kirchenbauer et al., 2023), GPTZero (Tian, 2023), DetectGPT (Mitchell et al., 2023), and OpenAI's text classifier (OpenAI, 2023). However, it remains unclear how robust these algorithms are to _paraphrase attacks_, in which AI-generated text from an LLM is rewritten by another (smaller) model to convey approximately2 the same meaning but using different word choice and syntax.

In this paper, we first demonstrate the vulnerability of these existing detectors to paraphrase attacks (Section 3, 4). Such attacks require an _external_ paraphraseer model, since paraphrases generated by the base LLM are still susceptible to detection techniques such as watermarking. We train an 11B parameter paraphrase generation model called dipper (or **D**iscourse **P**araphraseer) to execute these attacks. dipper possesses two unique features that help its outputs evade AI-generated text detectors: (1) **Paraphrasing long-form text in context:** Most modern paraphrasers are exclusively trained on sentence-level data, ignoring discourse-level information. However, many critical use cases of LLMs involve generating long-form text as responses to detailed user-specified prompts. Thus, we train dipper to paraphrase paragraph-length texts, re-order content, and optionally leverage context such as user prompts; (2) **Controlling output diversity:** Another weakness of existing paraphrasers is that they lack an easy way to control output diversity. An attacker may want to apply just the minimum amount of paraphrasing to evade a detector. dipper provides users with two intuitive scalar control knobs at inference time (lexical diversity, content reordering) that are trained end-to-end.

We use dipper to attack several recently proposed AI-generated text detectors (see Figure 1 for an attack overview). Experiments on multiple tasks and LLMs (including GPT3.5-davinci-003) show that after paraphrasing with dipper, a substantial fraction of AI-generated texts are misclassified as human-written texts by all detectors. For example, DetectGPT [Mitchell et al., 2023] correctly detects 70.3% of AI-generated sequences from GPT2-XL, but after paraphrasing, its detection rate drops to only 4.6%3 despite minimal semantic modification. We confirm the validity of dipper's paraphrases through several automatic evaluations and a human evaluation of semantic similarity.

Given the vulnerability of AI-generated text detectors to paraphrasing, how can we defend against such attacks? In the second part of our paper (Section 5), we propose to use _retrieval_ methods to detect AI-generated text instead of relying on statistical properties of the text or watermarking. First, an LLM API provider stores every output generated by their model in a database. The API provider then offers a service in which a semantic representation of a candidate text is compared to representations of every generation stored in the database. The search focuses on the _semantics_ of the input and can leverage both standard IR methods such as BM-25 [Robertson et al., 1995] as well as semantic vector representations such as P-SP from Wieting et al. . Since paraphrasing does not modify the semantics of the input, this algorithm is robust to paraphrasing attacks. Specifically, we find that 97.3% of PG19 paraphrases and 80.4% of Wikipedia paraphrases are successfully detected in a large database of over 15M generations, at a 1.0% false positive rate. We extensively discuss the limitations and scalability of retrieval-based detection in Section 5.4.

In contrast to concurrent work that also uses paraphrasing to attack AI-generated text detectors [Sadasivan et al., 2023], our work offers more comprehensive attack experiments, a new and more powerful paraphraser, human evaluations of paraphrase quality, and finally a novel defense mechanism based on retrieval to combat such attacks. To spur future research in this area, we will release our dipper model, data, and a codebase for evaluating both existing detectors and our retrieval-based method.

Figure 1: An overview of paraphrasing attacks with dipper on watermarked text [Kirchenbauer et al., 2023a]. The original model generation (top) contains several “green” watermarked tokens that are counted by a detector to judge whether the text was AI-generated. After paraphrasing, several green tokens are replaced with approximately semantically-equivalent red tokens, thereby fooling the detector. Actual outputs from a watermarked version of GPT2-XL and our paraphraser dipper.

Background on detectors of AI-generated text

In this section, we provide a brief overview of existing algorithms for detecting AI-generated text detection (see Appendix E for a detailed version). We also contrast our work to Sadasivan et al. (2023), a concurrent effort which notes the efficacy of paraphrasing attacks but does not consider a retrieval-based defense in its pessimistic conclusion about the fate of AI-generated text detection.

A **watermark** is a modification to the generated text that can be detected post-hoc by an algorithm while remaining imperceptible to human readers. Effective watermarks are difficult to remove and have little effect on the quality of generated text. Prior work has watermarked natural language using syntax tree manipulations (Topkara et al., 2005; Meral et al., 2009), and this area has received renewed interest with the advent of LLMs (Abdelnabi and Fritz, 2021; Grinbaum and Adomaitis, 2022). Most recently, Kirchenbauer et al. (2023) proposed a simple algorithm that watermarks LLMs by slightly perturbing its probability distribution while generating text.

**Statistical outlier detection methods** detect AI-generated text based on its artifacts (See et al., 2019; Holtzman et al., 2020) instead of modifying the generative algorithm. Early methods detect statistical irregularities in entropy (Lavergne et al., 2008) and perplexity (Beresneva, 2016), while Gehrmann et al. (2019) introduced the GLTR visualizer to assist humans in detecting AI-generated text. The release of ChatGPT prompted the development of two new tools: closed-source GPTZero (Tian, 2023) and open-source DetectGPT (Mitchell et al., 2023). The latter uses the observation that AI-generated text tends to have significantly higher LLM likelihood than meaningful perturbations of it.

**Classifier methods** train models to distinguish human-written text from AI-generated text. Early efforts used classifiers to detect fake reviews (Hovy, 2016) and news (Zellers et al., 2019), while others examined classifier performance across domains (Bakhtin et al., 2019) and decoding strategies (Ippolito et al., 2020). Most recently, OpenAI fine-tuned a GPT model to perform this task and released it as a web interface (OpenAI, 2023). Their model uses generations from 34 LLMs, with the human-written text from Wikipedia, WebText, and their internal human demonstration data.

**Comparison to Sadasivan et al. (2023)**: In recent concurrent work, Sadasivan et al. (2023) also demonstrate the utility of paraphrasing attacks against AI-generated text detectors. Our experiments encompass more tasks, detection algorithms, and larger LMs like GPT3.5. Additionally, we propose a discourse-level paraphrase model (dipper) that is much more suited to long-form text than the off-the-shelf sentence-level paraphrasers used in their paper. More importantly, our retrieval-based defense _directly contradicts_ the "impossibility result" of Sadasivan et al. (2023) and its associated proof, which states that an optimal detector will perform at random as the quality of LLM-generated text approaches that of human-written text. The quality of generated text is irrelevant to our detector's accuracy because it relies only on a corpus search, and thus the proof is inapplicable. Other concurrent work (Chakraborty et al., 2023) has also shown the proof's invalidity in practical settings.

## 3 Building a controllable discourse paraphraser

Having outlined existing methods to detect AI-generated text, we now focus on a simple attack against all detection techniques: _paraphrasing_ the generated text. Intuitively, paraphrasing alters the statistical properties of AI-generated text, which can fool outlier detection or classifiers while also reducing the number of watermarked tokens (Figure 1). To evade such detectors, a paraphraser must be able to handle _context_ in the form of prompts or multi-sentence inputs. Its behavior should also be _controllable_ in order to make as many/few changes as needed to evade a given detector. In all cases, it should not appreciably change the input semantics. Finally, to evade watermarking, the paraphraser must be different from the watermarked model, as otherwise the paraphrases will also be watermarked. Below, we detail how we construct a paraphraser (dipper) with all these properties.4

**Constructing paraphrase data**: Our process involves fine-tuning a LLM on a parallel dataset of paragraph-level paraphrases, which we modify to model control, external context and content reordering. We leverage the Par3 dataset (Thai et al., 2022), which contains multiple translations of non-English novels into English aligned at a paragraph level, which we treat as paraphrases. More formally, let \(p\) and \(q\) be aligned paragraphs, where \(p_{1},p_{2}...p_{N}\) denote sentences of \(p\) and \(q_{1},q_{2},...q_{M}\)denote sentences of \(q\). Note that \(M\) may not be equal \(N\) when two translators disagree on when to merge and split sentences. We perform the following steps (overview in Figure 2):

1. [leftmargin=*]
2. **Align sentences** of \(p\) to sentences of \(q\) by using the semantic similarity scores from the paraphrase similarity metric in Wieting et al. (2019) to run the sequence alignment algorithm designed by Needleman and Wunsch (1970) which uses dynamic programming (metric details in Section 4.1).
3. **Choose a subset of sentences \(p_{i}...p_{j}\)** from the first paragraph. Let \(q_{i^{}}...q_{j^{}}\) be the corresponding alignment in the second paragraph. In Figure 2, \(i=2,j=3,i^{}=2,j^{}=4\).
4. **Re-order** the sentences \(q_{i^{}}...q_{j^{}}\) randomly, and compute the **diversity control codes** between \(p_{i}...p_{j}\) and shuffle\((q_{i^{}}...q_{j^{}})\). We shuffle the sentences to allow for the model to learn content re-ordering. We compute lexical diversity (\(L\)) using unigram token overlap (F1 score), and the order diversity (\(O\)) using the Kendall-Tau correlation of tokens of overlapping words between \(p_{i}...p_{j}\) and shuffle\((q_{i^{}}...q_{j^{}})\), also used in Krishna et al. (2020). These scores are normalized to values \(\{0,20,40,60,80,100\}\), where \(L=20\) roughly corresponds to a 20% lexical modification.
5. **Map** the shuffled \(q_{i^{}}...q_{j^{}}\) to \(p_{i}...p_{j}\), leveraging context from the rest of \(p\) and control codes using string concatenation. Let input \(=(q_{i^{}}...q_{j^{}})\). We map, \[=L,=O p_{1}...p_{i-1}</ p_{j+1}...p_{N} p_{i}...p_{j}\] where \(\) is string concatenation. During inference, we can paraphrase any sequence of sentences by marking it with \(<\)p\(>\) tags, assigning the control codes (\(L\), \(O\)) the desired diversity values.

Our final dataset contains 6.3M paraphrase pairs. We **fine-tune** a sequence-to-sequence Transformer (Vaswani et al., 2017) on this data, initialized with the pretrained 11B parameter T5-XXL checkpoint (Raffel et al., 2020). See F.1 for details.

## 4 Experiments attacking detection algorithms with dipper

In this section, we describe our experimental setup in Section 4.1-4.2 and present our results in Section 4.3. Overall, we find that dipper evades all detectors across three LLMs (including GPT3.5).

### Evaluation metrics

**Detection accuracy**: Our first metric measures how often the input text is correctly detected as AI-generated. Since detection rates are heavily dependent on the chosen detection threshold, the AUC-ROC metric is commonly used to measure detector performance (Mitchell et al., 2023), which considers the range of all possible thresholds. However, in this application, it is critical that the _false positive rate_ (FPR) is low; in other words, human-written text must almost never be classified as machine-generated (OpenAI, 2023; Kirchenbauer et al., 2023). Hence, we fix the FPR to 1% for all detection algorithms (although even 1% is likely too high in practice), and adjust the detection threshold accordingly while reporting detection accuracies. Additionally, we also plot ROC curves focusing on the 0-1% FPR region. Overall, we expect detection rates to plummet on paraphrased text.

Figure 2: The method used to train dipper on English translations of the French novel _The Nun_. We first align sentences between the two translations to create parallel data. Next, a subset of the alignments are chosen; in this example, we use (\(p_{2}\), \(q_{2}\)) and (\(p_{3}\), \(q_{3}q_{4}\)). We shuffle sentences, compute control codes, and fine-tune a T5-XXL LM to generate \(p_{2}p_{3}\) given \(q_{3}q_{4}q_{2}\) and the context \(p_{1}\) and \(p_{4}\).

**Semantic similarity (Sim)**: Detection accuracy is an insufficient evaluation of our attack's success. We also need to measure whether the original and paraphrased generations share approximately the same semantics. We measure semantic similarity using the state-of-the-art semantic similarity model P-SP from Wieting et al. (2022), an embedding averaging model trained on a large corpus of filtered paraphrase data (Wieting and Gimpel, 2018). P-SP is a well-calibrated metric that performs well on semantic calibration tests as well as plagiarism detection in STS benchmarks (Agirre et al., 2016). P-SP is also robust against topically similar non-paraphrases. We found that P-SP it scores just 0.09 on random pairs of paragraphs from the same book (topically similar paragraphs but not paraphrases) in the Par3 dataset (Thai et al., 2022). In contrast, the average P-SP score of actual human paraphrase pairs in Par3 is 0.76. We consider semantics to be approximately preserved if the P-SP score is greater than this average human paraphrase score of 0.76.

Besides semantic similarity, we conduct several automatic evaluations, ablation studies, and human evaluations of intrinsic paraphrase quality in Appendix C.

### Models, datasets & detection algorithms

**Base language models**: We conduct attacks on three language models of varying sizes that belong to different model families. We consider the GPT2-XL model (1.5B parameters) (Radford et al., 2019), the OPT-13B model (Zhang et al., 2022), and the text-davinci-003 variant from the GPT-3.5 family (Brown et al., 2020), which has 175B parameters and has additionally been instruction tuned using reinforcement learning from human feedback (Ouyang et al., 2022). For all LMs, we sample generations that are 300 tokens long before passing them through dipper for the attack experiments.5

**Natural language generation tasks**: We experiment with two long-form text generation tasks, since most malicious applications (e.g., fake article creation) are associated with long-form outputs. First, we consider _open-ended generation_, where an LM generates a continuation to a two-sentence prompt. To obtain our prompts, we sample 3K contiguous two-sentence chunks from the validation split of WikiText-103 (Merity et al., 2017) and use the next 300 tokens as the "human-written" continuation. Second, we evaluate _long-form question answering_(Fan et al., 2019), in which an LM answers a question with a 300-word answer (dataset details in Appendix F.2). For our main results, the human

   Metric & Sim &  \\  Detector & & Watermarks & DetectGPT & OpenAI & GPTZero & RankGen \\  GPT2-1.5B & - & 100.0 & 70.3 & 21.6 & 13.9 & **13.5** \\ + dipper 20L & 99.2 & 97.1 & 28.7 & 19.2 & 9.1 & 15.8 \\ + dipper 40L & 98.4 & 85.8 & 15.4 & 17.8 & 7.3 & 18.0 \\ + dipper 60L & 96.9 & 68.9 & 8.7 & **13.3** & 7.1 & 19.8 \\ + dipper 60L, 60O & 94.3 & **57.2** & **4.6** & 14.8 & **1.2** & 28.5 \\  OPT-13B & - & 99.9 & 14.3 & 11.3 & 8.7 & **3.2** \\ + dipper 20L & 99.1 & 96.2 & 3.3 & 11.8 & 5.4 & 5.2 \\ + dipper 40L & 98.6 & 84.8 & 1.2 & 11.6 & 3.8 & 6.6 \\ + dipper 60L & 97.1 & 63.7 & 0.8 & **9.1** & 6.3 & 9.3 \\ + dipper 60L, 60O & 94.6 & **52.8** & **0.3** & 10.0 & **1.0** & 13.5 \\  GPT-3.5-175B, davinci-003 & - & - & 26.5* & 30.0 & 7.1 & **1.2** \\ + dipper 20L & 97.6 & - & 12.5* & 20.6 & 4.3 & 1.7 \\ + dipper 40L & 96.7 & - & 8.0* & 22.4 & 4.8 & 2.0 \\ + dipper 60L & 94.2 & - & 7.0* & **15.6** & 6.1 & 3.9 \\ + dipper 60L, 60O & 88.4 & - & **4.5*** & **15.6** & **1.8** & 7.3 \\  Human Text & - & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\   

Table 1: Performance of detection algorithms (at 1% FPR) before and after dipper paraphrasing on **open-ended generation** using Wikipedia prompts (300 generated tokens). As the diversity (L,O) increases, detection rates decrease across algorithms, with nearly perfect semantic similarity (Sim). *GPT3.5 DetectGPT scores computed using 200 samples at 20% FPR as it scores 0% at a 1% FPR.

reference answers or continuations are only used to adjust detection thresholds of studied methods to maintain a 1% FPR.6 Note that we are not removing human-written text from our test set. Our metric is equivalent to having a test set with a 50-50 split between machine/human-written text for the same prompts, and observing the FPR\(=\)1% point in the ROC curve (also provided in Appendix H).

**Detection algorithms**: We attack five detectors:7 (1) watermarking [Kirchenbauer et al., 2023a]; (2) DetectGPT [Mitchell et al., 2023]; (3) GPTZero [Tian, 2023]; (4) OpenAI's text classifier [OpenAI, 2023a],8, and (5) RankGenXL-all [Krishna et al., 2022a].9 We use the default hyperparameters for each detector. We also respect their minimum length specifications, discarding instances where any of the AI-generated text, human-written text, or paraphrased text is shorter than the minimum length.

**Paraphrasing AI-generated text**: We pass the prompts for each task and AI-generated responses to those prompts through dipper. Specifically, we feed the model input of the form,

\[\\ <>\]

where \(L\) and \(O\) represent the paraphraser diversity control codes and the \(<\)p\(>\) and \(<\)/p\(>\) special tokens mark the boundaries of the text to be paraphrased. We use \(L=20,40,60\) and \(O=0,60\) in our main attack experiments. After paraphrasing, we ensure that the AI-generated sequence, paraphrased sequence, and human-written sequence have an equal number of words by truncating them to the length of the shortest among the three. To ensure higher semantic preservation, we iteratively paraphrase long sequences three sentences at a time, keeping already paraphrased text in the context of the generation. To show the effectiveness of our attack, we only **paraphrase each generation once**, rather than draw multiple samples until it evades detection.10

### Attacking AI-generated text detectors

We present our results in Table 1 and Figure 3. Overall we find that:

**Paraphrasing significantly lowers detection accuracy while preserving input semantics**. Across all LMs, detectors,11 and tasks, paraphrasing significantly lowers detection accuracy across all diversity control codes. For instance, paraphrasing GPT2-XL open-ended generations reduces watermark detection accuracy from 100% to 57.2%, and DetectGPT accuracy from 70.3% to just 4.6%. Trends are similar even for large LMs like GPT-3.5, for which paraphrasing reduces OpenAI's text classifier accuracy from 30.0% to 15.6%. Additionally, dipper preserves semantics effectively,

Figure 3: Detector performance (at 1% FPR) on **long-form QA** before/after paraphrasing. As diversity (L,O) increases, detection rates decrease with very high semantic preservation (Sim). WM: Watermark, D.GPT: DetectGPT, O.AI: OpenAI. *GPT3.5 D.GPT uses 100 samples at 20% FPR to show attack success, as it scores 0% at 1% FPR.

as 88%-99% paraphrases achieve a P-SP score higher than the median score of human-written paraphrases. High semantic preservation is supported by careful human evaluations in Appendix C.2. Overall, we find that watermarking is the most resilient detector to paraphrasing.

**Non-watermarking detectors are generally ineffective.** We observe that all detectors apart from watermarking struggle with text generated by larger models like OPT-13B and GPT-3.5, achieving detection accuracies \(<\) 50%. While DetectGPT is effective on the smaller GPT-XL model (74.9% on long-form QA), its accuracy drops to just 29.8% on OPT-13B. Furthermore, GPTZero and RankGen perform the worst among the five detectors on all tested LMs (Table 1), as they are only able to detect < 15% of non-paraphrased AI-generated text. Thus, we recommend against using these detectors.

**ROC plots confirm the trends at different false positive rates**. In Figure 4, we plot the detection accuracy (true positive rate) at different values of FPR between 0% and 1% for GPT2-XL. Overall, paraphrasing significantly drops detection rates across all FPR thresholds (more plots in Appendix H).

### Alternative paraphrasing attacks

**Paraphrasing multiple times:** Our presented attacks use just a single paraphrase generated by dipper to evade detection. A simple way to further improve the effectiveness of a paraphrase attack is to sample multiple times12 from dipper and choose a paraphrase that evades the detector while also preserving semantics. We do not perform this attack as it can only be done if an attacker has access to a detector, which may be a strong assumption (see Appendix A.2). That being said, using multiple paraphrase samples can make the attacks even more potent against publicly available detectors.

**Non-dipper paraphrasers:** A second alternative is to use non-dipper paraphrasers that operate at the sentence level. These models can be deployed for long-form text inputs by paraphrasing the inputs sentence by sentence, ignoring prompt context. While the concurrent work of Sadasivan et al. (2023) shows that this method can also evade detection, our ablations in Appendix C show that non-contextual versions of dipper have lower quality and are less compatible with the prompt as dipper paraphrasers. Moreover, most existing paraphrasers lack fine-grained diversity control and multi-sentence input support (survey in Appendix D.1), two desired properties from an attacker's point of view: attackers want to modify long multi-sentence responses _just enough_ to evade detection.

A more interesting option is to use an LLM like ChatGPT to perform few-shot contextual paraphrasing. While this method is likely to provide accurate paraphrases,13 they may be detectable by strategies like watermarking (whether using the same API as the original LLM or a different one). We thus expect a sophisticated adversary to use their own private paraphraser (like dipper) to evade detection.

## 5 Defense against paraphrase attacks using retrieval

In Section 4.3, we showed that paraphrasing is an effective attack against AI-generated text detectors. How can LLM API providers defend against these attacks? In this section, we propose _retrieval_ over previously-generated sequences as a defense against paraphrase attacks. At a high level (Figure 5), an API provider first stores every sequence generated by their LLM in a database. The API provider offers an interface that allows users to enter candidate AI-generated text as a query. The interface searches

Figure 4: ROC plots (0-1% FPR) for GPT2-XL using different detectors, before (solid lines) and after paraphrasing (dashed). Paraphrasing reduces detection rate across FPRs, and our detector _retrieval_ detects paraphrases best; full plots in Appendix H.

over the entire database of previously-generated text, trying to find a sequence that approximately matches the content of the input query. This search can be done using a semantic similarity scorer like P-SP (Wieting et al., 2022) or a retriever like BM25 (Robertson et al., 1995). Since paraphrasing approximately preserves input semantics, we expect such a defense to still be able to map paraphrased generations to their source. We formalize our detector in Section 5.1, and then conduct a controlled comparison with competing detectors in Section 5.2. We evaluate retrieval-based detection at scale using a large retrieval corpus of 15M generations in Section 5.3. In Appendix A we extensively discuss limitations of retrieval-based detection and share ideas for enabling further scaling.

### Formulating the retrieval defense

Let \(f_{}\) be an LLM API (e.g., GPT-3.5) that takes a prompt \(x\) as input and returns a continuation \(y\). Let \(f_{}\) be an encoder (e.g., TF-IDF, neural network) that embeds variable-length sequences into fixed-size vectors that represent the input semantics. Then, we do the following:

**Building the database**: Let \(x_{1},...,x_{N}\) be the set of prompts that have been fed as input to the API in the past with \(y_{i}=f_{}(x_{i})\) being the LLM output. Here \(N\) can potentially be very large for popular APIs (we study up to \(N=15\)M). We construct our database \(=[_{1},..._{N}]\) by encoding every LLM API output with our retrieval encoder, or \(_{i}=f_{}(y_{i})\). The database \(\) is dynamically updated and stored on the API side. It is inaccessible to clients except via the API described in the next step.

**Querying the database**: Let \(y^{}\) be a candidate text and \(^{}=f_{}(y^{})\) its encoded vector. Suppose a client wishes to know whether \(y^{}\) was generated by the API \(f_{}\). The API provider can check this by seeing whether the maximum similarity score of \(y^{}\) to an entry in the database exceeds some detection threshold \(T\) chosen by the API provider:

\[=>T,=_{i\{1,...N\}}^{}_{i}}{|^{}|\ |_{i}|}\]

We expect unperturbed machine-generated text to always get a score of 1.0, while paraphrasing the text may lower the detection score. Hence, lowering \(T\) will increase the detection rate of heavily-paraphrased text but also increase the false positive rate (i.e., human-written text that resembles sequences previously generated by the LLM API can be falsely flagged). Since \(N\) can be very large, the score can also be approximated using efficient nearest neighbor libraries like FAISS (Johnson et al., 2019). However, in this work we only compute exact inner products.

As the **retriever**\(f_{}\), we experiment with two choices: P-SP (Wieting et al., 2022) and BM25 (Robertson et al., 1995). We implement BM25 using the retriv library from Bassani (2022). In order to normalize and calibrate BM25 scores, we compute the F1-score unigram token overlap (Rajpurkar et al., 2016) between the candidate \(y^{}\) and the best retrieval \(y*\) to get a detection score in \(\).

### Controlled comparisons of retrieval with other AI-generated text detectors

First, we conduct a controlled comparison between the detection algorithms evaluated in Section 4.3 and our retrieval method on long-form question answering.14 We construct three kinds of databases,

1. 3K sequences generated by a specific LM for one of the tasks;

Figure 5: An illustration of AI-generated text detection with retrieval. Several users (including the attacker, shown as the purple emoji) feed prompts to the API which are collectively added to a private API-side database. Candidate queries are compared against this database using a retriever like BM25.

2. 9K sequences formed by concatenating the generations from all three LMs in this paper;
3. 46K sequences constructed by combining the 3K sequences from (1) with 43K LLM responses from ShareGPT-Vicuna.15 
We expect (2) to be a more difficult test for our method than (1), since the retriever needs to distinguish between multiple generations from different models given the same prompt. On the other hand, (3) denotes a more real-world setting with several diverse LLM-generated outputs from ShareGPT. Next, we perform retrieval over this corpus using different types of queries: the original AI-generated text, its dipper paraphrase, and human-written text (each query with at least 50 tokens).

Table 2 shows that **across all LMs, retrieval is a much more effective detector than baseline detectors**. On unperturbed AI-generated text, retrieval has a 100% detection accuracy due to exact match with the retrieval corpus. On paraphrased text, retrieval with BM25 is quite effective, detecting 97.8% of the highest-diversity paraphrases (L60, O60) on GPT2-XL, 97.3% on OPT-13B and 96.2% on GPT-3.5 in long-form question answering. This is significantly better than the next best alternative with competing detectors (55.8%, 51.4%, 38.1%). Even on our harder augmented databases, detection rates continue to be high: 95.2%, 94.4%, 96.0% for the 9K augmented database; 97.5%, 97.3%, 95.5% for the ShareGPT augmented database. Finally, we observe that BM25 is a more effective retriever than P-SP, scoring 95.2% vs 75.4% on the augmented setting in GPT2-XL. These trends are consistent across different FPR thresholds, as shown in Figure 4.

In Appendix G.2, we additionally observe promising preliminary results that show the effectiveness of retrieval against **text mixing attacks**(Kirchenbauer et al., 2023b).

### Is retrieval an effective detector with a large retrieval corpus?

In the previous section, we conducted experiments using the set of 9K sequences generated by all three models as the retrieval corpus. However, this is more of a toy experiment: in practice, a popular LLM API may serve millions of queries a day. As the corpus grows larger, the false positive rate (i.e., human-written text falsely detected as AI-generated) will grow. How well do retrieval-based detectors scale? To answer this question, we need access to a large corpus of AI-generated text. We utilize the training data used to train RankGen (Krishna et al., 2022a), which contains over 70M AI-generated sequences. We use the Project Gutenberg and Wikipedia splits of the training data, each of which contain 15M sequences generated by a T5-XXL model (Raffel et al., 2020) fine-tuned on the different documents in the same domain. We discard generations which are shorter than 50 tokens, and paraphrase a subset of 2K generations to evaluate retrieval.

**Retrieval is effective even with a corpus size of 15M generations.** In Figure 5(a), we plot the detection accuracy as a function of retrieval database size. Overall, we observe that detection

  &  &  &  \\   & Original & + 60L & + 60 L,O & Original & + 60 L,O & Original & + 60L & + 60 L,O \\  Watermark (2023a) & 100.0 & 71.1 & 55.8 & 100.0 & 65.5 & 51.4 & - & - & - \\ DetectGPT (2023) & 74.9 & 15.8 & 7.6 & 29.8 & 3.2 & 1.5 & 1.0 & 0.0 & 0.0 \\ OpenAI (2023a) & 59.2 & 31.3 & 32.7 & 33.5 & 21.6 & 21.6 & 40.5 & 40.1 & 38.1 \\   \\ SP & 100.0 & 95.6 & 87.7 & 100.0 & 94.8 & 85.3 & 100.0 & 94.2 & 85.1 \\ BM25 & 100.0 & 99.2 & 97.8 & 100.0 & 99.3 & 97.3 & 100.0 & 98.6 & 96.2 \\   \\ SP & 100.0 & 88.9 & 75.4 & 100.0 & 89.6 & 76.4 & 100.0 & 93.8 & 84.6 \\ BM25 & 100.0 & 98.3 & 95.2 & 100.0 & 98.5 & 94.4 & 100.0 & 98.5 & 96.0 \\   \\ SP & 100.0 & 94.0 & 84.8 & 100.0 & 94.2 & 84.7 & 100.0 & 94.1 & 84.9 \\ BM25 & 100.0 & 98.9 & 97.5 & 100.0 & 99.0 & 97.3 & 100.0 & 98.4 & 95.5 \\  

Table 2: A comparison of retrieval against other detectors on long-form QA (300 generated tokens). Our detector outperforms baselines (at 1% FPR) even with the most diverse paraphrases (+60L,O).

accuracy remains consistently high across different corpus sizes (varying from 1M generations to 15M generations). We observe slight drops in performance as the corpus size increases: just 1% (98.3 to 97.3) on Project Gutenberg (PG19) and 9.6% (90.0 to 80.4) on Wikipedia. Consistent with the results in Section 5.2, BM25 continues to outperform P-SP across different corpus sizes.

**Retrieval detection works best with 50 or more tokens of generated text**. Another important factor for our retrieval-based detector is the query length: shorter queries are likely to have more matches (many of them spurious) compared to longer ones. In Figure 5(b), we plot the detection accuracy of paraphrased sequences at various query lengths by truncating each sequence to its first \(X\) words before using it as a query for BM25. We use a retrieval corpus of 2M generations for this experiment. We observe that BM25 struggles to detect paraphrased text with a query length of 20 (less than 25% accuracy), but the detection rate rapidly increases and begins to plateau at 50 tokens.

### Scalability and limitations of retrieval-based detectors

In Appendix A we extensively discuss the scalability of retrieval (A.1), its limitations (A.2), ideas for improving retrieval-based detectors (A.3), and incentive structures for LLM providers to implement retrieval (A.4). In summary, we believe retrieval-based detection is a scalable approach: we estimate that if OpenAI implemented it with ChatGPT, they would need just 5TB of storage space per month (similar to modern portable hard disks). Furthermore, retrieval on ChatGPT scale takes 130 seconds per retrieval on a CPU-only Macbook Pro, which can certainly be further optimized. However, retrieval-based detection has some important limitations: (1) potential privacy risk of exposing _all_ LLM responses behind a binary classifier; (2) inability to use retrieval-based detection on open-source LLMs like LLAMA (Touvron et al., 2023); and (3) the need to implement and maintain retrieval infrastructure. We discuss mitigation strategies for the limitations in Appendix A.2.

## 6 Conclusion

We present dipper, a controllable paraphraser that can rewrite paragraphs in context. We use dipper to stress test current AI-generated text detectors, and we find that dipper paraphrases easily evade these detectors while preserving input semantics. As a defense, we propose a simple retrieval-based detector which searches through a corpus of previously-generated sequences from an LLM API for semantically-similar generations to a given query. We show that this defense significantly outperforms baselines on paraphrased text, and scales effectively. We discuss the limitations and ethical considerations of our work in Appendix A, B. We have additionally open sourced our models, code and data to enable future research.1 Since this paper's initial release, dipper has been extensively utilized in follow-up studies to measure the robustness of AI-generated text detection algorithms (Lu et al., 2023; Koike et al., 2023; Zhao et al., 2023; Yoo et al., 2023; Patil et al., 2023; Kirchenbauer et al., 2023; Kumarage et al., 2023; Liu et al., 2023), to name a few).

Figure 6: Detection rate using retrieval at 1% FPR w.r.t. corpus size (left) and query length (right).