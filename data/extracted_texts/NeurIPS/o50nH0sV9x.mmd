# Certifiably Robust Graph Contrastive Learning

Minhua Lin, Teng Xiao, Enyan Dai, Xiang Zhang, Suhang Wang

The Pennsylvania State University

{mf15681,tengxiao,emd5759,xzhang,szu494}@psu.edu

###### Abstract

Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, **RES** (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any GCL model. The source code of RES is available at https://github.com/ventrlc/RES-GCL.

## 1 Introduction

Graph structured data are ubiquitous in real-world applications such as social networks , finance systems , and molecular graphs . Graph Neural Networks (GNNs) have emerged as a popular approach to learn graph representations by adopting a message passing scheme [4; 5; 6], which updates a node's representation by aggregating information from its neighbors. Recently, graph contrastive learning has gained popularity as an unsupervised approach to learn node or graph representations [7; 8; 9]. The graph contrastive learning creates augmented views and minimizes the distances among positive pairs while maximizing the distances among negative pairs in the embedding space.

Despite the great success of GNNs, some existing works [10; 11; 12; 13] have shown that GNNs are vulnerable to adversarial attacks where the attackers can deliberately manipulate the graph structures and/or node features to degrade the model's performance. The representations learned by Graph Contrastive Learning (GCL) are also susceptible to such attacks, which can lead to poor performance in downstream tasks with a small amount of perturbations [14; 15; 16]. To defend against graph adversarial attacks, several empirical-based works have been conducted in robust GNNs for classification tasks [17; 18] and representation learning with contrastive learning [14; 15]. Nevertheless, with the advancement of new defense strategies, new attacks may be developed to invalidate the defense methods [19; 20; 21; 22], leading to an endless arms race. Therefore, it is important to develop a certifiably robust GNNs under contrastive learning, which can provide certificates to nodes/graphs that are robust to potential perturbations in considered space. There are several efforts in certifiable robustness of GNNs under supervised learning, which requires labels for the certifiable robustness analysis [23; 24; 25; 26]. For instance,  firstly analyze the certifiable robustness against the perturbation on node features. Some following works [24; 25; 26] further study the certified robustness of GNN under graph topology attacks. However, the certifiable robustness of GCL, which is in unsupervised setting, is still rarely explored.

Certifying the robustness of GCL is rather challenging. _First_, quantifying the robustness of GCL consistently and reliably is a difficult task. Existing GCL methods [27; 14; 15] often use empirical robustness metrics (e.g., robust accuracy) that rely on the knowledge of specific downstream tasks (e.g., task types, node/graph labels) for evaluating GCL robustness. However, these criteria may not be universally applicable across different downstream tasks. The lack of clarity in defining and assessing robustness within GCL further compounds the difficulty of developing certifiably robust GCL methods. _Second_, the absence of labels in GCL makes it challenging to study certifiable robustness. Existing works [23; 28; 29; 25; 24] on certifiable robustness of GNNs are designed for supervised or semi-supervised settings, which are not applicable to GCL. They typically try to provide certificates by analyzing the worst-case attack on labeled nodes. However, it is difficult to conduct such analyses due to the absence of labels in GCL. Additionally, some works [29; 24] that rely on randomized smoothing to obtain robustness certificates can not be directly applied to GCL as they may require injecting noise into the whole graph during data augmentation, introducing too many spurious edges and hurting downstream task performance, particularly for large and sparse graphs. _Third_, it is unclear whether certifiable robustness of GCL can be converted into robust performance in downstream tasks. The challenge arises due to the misalignment of objectives between GCL and downstream tasks. For instance, GCL usually focuses on discriminating between different instances, while some downstream tasks may also require understanding the relations among instances. This discrepancy engenders a divide between unsupervised GCL and its downstream tasks, making it difficult to prove the robust performance of the certified robust GNN encoder in downstream tasks.

In this work, we propose the first certifiably robust GCL framework, RES (Randomized Edgedrop Smoothing). **(i)** To address the ambiguity in evaluating and certifying robustness of GCL, we propose a unified criteria based on the semantic similarity between node or graph representations in the latent space. **(ii)** To address the label absence challenges and avoid adding too many noisy edges for certifying the robustness in GCL, we employ the robustness defined at (i) for unlabeled data and introduce a novel technique called randomized edgedrop smoothing, which can transform any base GNN encoder trained by GCL into a certifiably robust encoder. Therein, some randomized edgedrop noises are injected into the graphs by stochastically dropping observed edges with a certain probability, preventing the introduction of excessive noisy edges and yielding effective and efficient performance. **(iii)** We theoretically demonstrate that the representations learned by our robust encoder can achieve provably robust performance in downstream tasks. **(iv)** Moreover, we present an effective training method that enhances the robustness of GCL by incorporating randomized edgedrop noise. Our experimental results show that GRACE  with our RES can outperform the state-of-the-art baselines against several structural attacks and also achieve 43.96% certified accuracy in OGB-arxiv dataset when the attacker arbitrarily adds at most 10 edges.

Our **main contributions** are: **(i)** We study a novel problem of certifying the robustness of GCL to various perturbations. We introduce a unified definition of robustness in GCL and design a novel framework RES to certify the robustness; **(ii)** Theoretical analysis demonstrates that the representations learned by our robust encoder can achieve provably robust performance in downstream tasks; **(iii)** We design an effective training method for robust GCL by incorporating randomize edgedrop noise; and **(iv)** Extensive empirical evaluations show that our method can provide certifiable robustness for downstream tasks (i.e., node and graph classifications) on real-world datasets, and also enhance the robust performance of any GNN encoder through GCL.

## 2 Related Work

**Graph Contrastive Learning.** GCL has recently gained significant attention and shows promise for improving graph representations in scenarios where labeled data is scarce [31; 32; 9; 30; 27; 15; 33; 32; 34; 35]. Generally, GCL creates two views through data augmentation and contrasts representations between two views. Several recent works [16; 36] show that GCL is vulnerable to adversarial attacks. Despite very few empirical works [9; 15; 14] on robustness of GCL, there are no existing works studying the certified robustness of GCL. In contrast, we propose to provide robustness certificates for GCL by using randomized edgedrop smoothing. More details are shown in Appendix A.1. To the best of our knowledge, our method is the first work to study the certified robustness of GCL.

**Certifiable Robustness of GNNs.** Several recent studies investigate the certified robustness of GNNs in the (semi)-supervised setting[23; 28; 29; 26; 24; 25; 37]. Zunger et al.  are the first to explore certifiable robustness with respect to node feature perturbations. Subsequent works[28; 26; 29; 24; 38] extend the analysis to certifiable robustness under topological attacks. More details are shown in Appendix A.2. Our work is inherently different from them: (i) we provide an unified definition to evalute and certify the robustness of GCL; (ii) we theoretically provide the certified robustness for GCL in the absence of labeled data, and this certified robustness can provably sustained in downstream tasks; (iii) we design an effective training method to enhance the robustness of GCL.

## 3 Backgrounds and Preliminaries

### Graph Contrastive Learning

**Notations.** Let \(=(,,)\) denote a graph, where \(=\{v_{1},,v_{N}\}\) is the set of \(N\) nodes, \(\) is the set of edges, and \(=\{_{1},...,_{N}\}\) is the set of node attributes with \(_{i}\) being the node attribute of \(v_{i}\). \(^{N N}\) is the adjacency matrix of \(\), where \(_{ij}=1\) if nodes \(v_{i}\) and \(v_{j}\) are connected; otherwise \(_{ij}=0\). In this paper, we focus on unsupervised graph contrastive learning, where label information of nodes and graphs are unavailable during training for both node- and graph-level tasks. For the node-level task, given a graph \(\), the goal is to learn a GNN encoder \(h\) to produce representation \(_{v}\) for \(v\), which can be used to conduct prediction for node \(v\) in downstream tasks. For the graph-level task, given a set of graph \(=\{_{1},_{2},\}\), the goal is to learn the latent representation \(_{}\) for each graph \(\), which can be used to predict downstream label \(y_{}\) of \(\). For simplicity and clarity, in this paper, we uniformly denote a node or graph as a concatenation vector \(\). The representation of \(\) from encoder \(h\) is denoted as \(h()\). More details are shown in Appendix E.2.

Graph Contrastive Learning.Generally, GCL consists of three steps: (i) multiple views are generated for each instance through stochastic data augmentation. Positive pairs are defined as two views generated from the same instance; while negative pairs are sampled from different instances; (ii) these views are fed into a set of GNN encoders \(\{h_{1},h_{2},\}\), which may share weights; (iii) a contrastive loss is applied to minimize the distance between positive pairs and maximize the distance between negative pairs in latent space. To achieve the certified robustness of GCL to downstream tasks, we introduce _latent class_ to formalize the contrastive loss for GCL, which is inspired by [39; 40].

**Definition 1** (Latent Class).: We utilize the latent class to formalize semantic similarity of positive and negative pairs in GCL . Consider a GNN encoder \(h\) learnt by GCL. Let \(\) denote the set of all possible nodes/graphs in the input space. There exists a set of _latent class_, denoted as \(\), where each sample \(\) is associated with a latent class \(c\). Each class \(c\) is associated with a distribution \(_{c}\) over the latent space of samples belonging to class \(c\) under \(h\). The distribution on \(\) is denoted as \(\). Intuitively, \(_{c}()\) captures how relevant \(\) is to class \(c\) in the latent space under \(h\), similar to the meaning of class in supervised settings. The latent class is related to the specific downstream task. For instance, when the downstream task is a classification problem, the latent class can be interpreted as the specific class to which the instances belong.

Typically, GCL is based on the label-invariant augmentation intuition: the augmented operations preserve the nature of graphs and make the augmented positive views have consistent latent classes with the original ones . Let \(c^{+}\),\(c^{-}\) denote the positive and negative latent classes drawn from \(\), \(_{c^{+}}\) and \(_{c^{-}}\) are the distributions to sample positive and negative samples, respectively. For each positive pair \((,^{+})_{c^{+}}^{2}\) associated with \(n\) negative samples \(\{_{i}^{-}_{c^{-}}|i[n]\}\), the widely-used loss for GCL, which is also known as InfoNCE loss , can be written as follows:

\[_{GCL}=,c^{-}^{2}}{} ,^{+}_{c^{+}}^{2},_{i}^ {-}_{c^{-}}}{}[-()^{} h(^{+})}}{e^{h()^{}h(^{+})}+_{i=1}^{n}e^{h( )^{}h(_{i}^{-})}})],\] (1)

### Threat Model

**Attacker's Goal.** We consider an attacker conducts an evasion attack in GCL. Given a well-trained GNN encoder \(h\) on a clean graph \(\) via GCL, an attacker aims to degrade the performance of \(h\) in downstream tasks by injecting noise into the graph. For instance, an attacker may attempt to manipulate a social network by injecting fake edges, which could affect the performance of a well-trained \(h\) in tasks such as community detection, node classification, or link prediction. The noise can take different forms, including adding/deleting nodes/edges or augmenting node features.

**Attacker's Knowledge and Capability.** We assume that the attacker follows the grey-box setting to conduct an evasion attack. Specifically, the attacker has access to a benign GNN encoder \(h\) trained on a clean dataset through GCL, and the training data to train downstream classifier is available to the attacker. The model architecture and other related information of the GNN encoder are unknown to the attacker. During inference phase, the attacker is capable of injecting noises to the graph within a given budget to degrade the performance of target nodes/graphs in downstream tasks. In this paper, we focus on perturbations on the graph structure \(\), i.e, only structural noises such as adding new edges/nodes are injected into the graph as structure attack is more effective than feature attack [11; 24; 43]. We leave the extension to feature attack and defense of GCL as future work. We denote \(\{0,1\}^{N}\) as the structural noise to a node/graph \(\), where \(N\) denote the number of nodes on the graph \(\) or \(K\)-hop subgraph of node \(\) and \(_{i}=1\) indicates the addition of a noisy edge to \(\) in the \(i\)-th entry, and its L\({}_{0}\)-norm \(||||_{0}\) indicates the number of noisy edges. We then have the perturbed version of \(\) by the attacker, denoted as \(^{}=\).

### Problem Statement

Our objective is to develop a certifiably robust GCL. We aim to train a GNN encoder that exhibits provably benign behaviors in downstream tasks. The problem can be formulated as follows:

**Problem 1**.: _Given a GNN encoder \(h\), a node or graph \(\). Let \(^{}=\) denote the perturbed version of \(\) by the attacker, where the structural noise \(\) with \(\|\|_{0} k\) is injected into \(\). Suppose that \(c^{*}\) is the latent class of \(\) and \(f\) is the downstream classifier. Our goal is to develop a certifiably robust GCL for \(h\) such that for any \(c c^{*}\), the representation of \(^{}\) under \(h\) can satisfy the following requirement in downstream tasks:_

\[m(^{},c^{*};h)=_{c c^{*}}[(f(h(^{}))=c^{*})-(f(h(^{}))=c)]>0,\] (2)

_where \(m(^{},c^{*};h)\) is the worst-case margin of \(h(^{})\) between \(c^{*}\) and \(y\) in the downstream task for any \(c c^{*}\), and \((f(h(^{}))=c)\) denotes the probability of the representation \(h(^{})\) being classified into class \(c\) by \(f\) in the downstream task. Eq. (2) implies that \(f(h(^{}))=f(h())=c^{*}\) for any \(\) within the budget \(k\), i.e., \(h\) is a certifiably robust at \((,c^{*})\) when perturbing at most \(k\) edges._

## 4 Certifying Robustness for GCL

In this section, we present the details of our RES, which aims to provide certifiable robustness for GCL models. There are mainly three challenges: (i) how to define the certified robustness of GCL; (ii) how to derive the certified robustness; and (iii) how to transfer the certified robustness of GCL to downstream tasks. To address these challenges, we first define certified robustness for GCL in Sec. 4.1. We then propose the RES method to derive the certificates and theoretically guarantee certifiable robustness in Sec. 4.2. Finally, we show that the certifiably robust representations learned from our approach are still provably robust in downstream tasks in Sec. 4.3.

### Certified Robustness of GCL

To give the definition of certified robustness of GCL, we first give the conditions for successfully attacking the GNN encoder. The core idea behind it comes from supervised learning, where the objective is to judge whether the predictions of target nodes/graphs are altered under specific perturbations. Inspired by , we consider the following scenario of a successful attack against the GNN encoder.

Given a clean input \(\), with positive and negative samples \(^{+}\) and \(^{-}\) used in the learning process of GCL, respectively, we consider \(^{}=\), where \(\) represents structural noise on \(\) and \(^{-}\) is the attack target of \(\). The attacker's goal is to produce an adversarial example \(^{}\) that can deceive the model into classifying \(\) as similar to \(^{-}\). Formally, given a well-trained GNN encoder \(h\) via GCL, we say that \(h\) has been successfully attacked at \(\) if the cosine similarity \(s(h(^{}),h(^{+}))\) is less than \(s(h(^{}),h(^{-}))\). This indicates that \(^{}\) is more similar to \(^{-}\) than to \(^{+}\) in the latent space. Otherwise, we conclude that \(h\) has not been successfully attacked. This definition of successful attack provides the basis for the definition of certified robustness of GCL. The formal definition of certified robustness problem for GCL is then given as

**Definition 2** (Certified Robustness of GCL).: Given a well trained GNN encoder \(h\) via GCL. Let \(\) be a clean input. \(^{+}\) is the positive sample of \(\) and \(^{-}=\{_{1}^{-},,_{n}^{-}\}\) denotes all possible negativesamples of \(^{+}\), which are sampled as discussed in Sec. 3.1. Suppose that \(^{}\) is a perturbed sample obtained by adding structural noise \(\) to \(\), where \(||||_{0} k\), and \(s(,)\) is a cosine similarity function. Then, \(h\) is certifiably \(l_{0}^{k}\)-robust at \((,^{+})\) if the following inequality is satisfied:

\[s(h(^{}),h(^{+}))>_{^{-}^{-}}s(h(^{-}),h(^{})),\;:\|\|_{0}  k.\] (3)

Similar to the supervised certified robustness, this problem is to prove that the point \(^{}=\) under the perturbation within budget \(k\) is still the positive sample of \(^{+}\). However, unlike the supervised learning, where we can estimate the prediction distribution of \(\) by leveraging the labels of the training data, it is difficult to estimate such a distribution in GCL because the label information is unavailable. To address this issue, we first consider a space \(\) for a sample \(^{+}\), where each sample within it is the positive sample of \(^{+}\). Formally, given a sample \(^{+}\) with the latent class \(c^{+}\) from the probability distribution \(_{c}^{+}\), \((^{+})\) is defined as

\[(^{+}):=\{| c^{+}\}.\] (4)

GCL maximizes the agreement of the positive pair in the latent space and assume access to _similar_ data in the form of pairs \((,^{+})\) that comes from a distribution \(_{c^{+}}^{2}\) given the latent class \(c^{+}\). Thus, it is natural to connect the latent class and the similarity between representations. The probability that \(\) is the positive sample of \(^{+}\) can be given by the following theorem.

**Theorem 1**.: _Let \((^{+})\) be a space around \(^{+}\) as defined in Eq. (4). Given an input sample \(\) and a GNN encoder \(h\) learnt by GCL in Eq. (1), the probability of \(\) being the positive sample of \(^{+}\) is:_

\[((^{+});h)=-(),h(^{+}))}{a})^{},\] (5)

_where \(s(,)\) is the cosine similarity function. \(a\), \(>0\) are Weibull shape and scale parameters ._

The proof is in Appendix C.1. Theorem 1 manifests that we can derive the probability of \(\) being the positive sample of \(^{+}\) based on the cosine similarity under \(h\), providing us a feasible way to study the certified robustness problem for GCL without the label information.

### Certifying Robustness by the Proposed Randomized EdgeDrop Smoothing (RES)

With the definition of certifiable robustness of GCL, we introduce our proposed Randomized Edgedrop Smoothing (RES), which provides certifiable robustness for GCL. RES injects randomized edgedrop noise by randomly dropping each edge of the input sample with a certain probability. We will also demonstrate the theoretical basis for the certifiable robustness of GCL achieved by RES.

Firstly, we define an randomized edgedrop noise \(\) for \(\) that remove each edge of \(\) with the probability \(\). Formally, given a sample \(\), the probability distribution of \(\) for \(\) is given as:

\[(_{i}=0|_{i}=0)=1,\;(_{i}=0| _{i}=1)=,\;\;(_{i}=1|_{i} =1)=1-,\] (6)

where \(_{i}\) denotes the connection status of \(i\)-th entry of \(\). Given a well-trained GNN encoder \(h\) via GCL and a noise \(\) with the distribution in Eq. (6), the smoothed GNN encoder \(g\) is defined as:

\[g()=h(),\;\;\;\; (*{arg\,max}_{} }((}); h)).\] (7)

Here \(\) is the set of all nodes/graphs in the dataset. It implies \(g\) will return the representation of \(\) whose latent class is the same as that of the most probable instance that \(\) is its positive sample. Therefore, consider the scenario where some noisy edges are injected into a clean input \(\), resulting in a perturbed sample \(^{}\). With a smoothed GNN encoder \(g\) learnt through our method, if we set \(\) to a large value, it is highly probable that the noisy edges will be eliminated from \(^{}\). Consequently, by running multiple randomized edgedrop on \(\) and \(^{}\), a majority of them will possess identical structural vectors, that is, \(=^{}\), which implies \(\) and \(^{}\) will have the same latent class and the robust performance of \(^{}\) in downstream tasks. We can then derive an upper bound on the expected difference in vote count for each class between \(\) and \(^{}\). With this, we can theoretically demonstrate that \(g\) is certifiablely robust at \(\) against any perturbation within a specific attack budget. Specifically, let \(p_{^{+},h}()=(( ^{+});h)\) for simplicity of notation, \(p_{^{+},h}()\) denotes a lower bound on \(p_{^{+},h}()\) with \((1-)\) confidence and \(^{-}_{i},h}}()\) denotes a similar upper bound, the formal theorem is presented as follows:

**Theorem 2**.: _Let \(\) be a clean input and \(^{}=\) be its perturbed version, where \(||||_{0} k\). \(^{-}=\{^{-}_{1},,^{-}_{n}\}\) is the set of negative samples of \(\). If for all \(^{-}_{i}^{-}\):_

\[^{+},h}}()-_{^{- }_{i}^{-}}^{-}_{i},h}}( )>2,\] (8)_where \(=1-}{}^{k}\) and \(e=\|\|_{0}\) denotes the number of remaining edges of \(\) after injecting \(\). Then, with a confidence level of at least \(1-\), we have:_

\[p_{^{+},h}(^{})>_{_{i}^{ -}^{-}}p_{_{i}^{-}h}(^{}).\] (9)

The proof is in Appendix C.2. Theorem 2 theoretically guarantees that of \(^{}\) is still the positive sample of \(^{+}\) with \((1-)\) confidence if the worst-case margin of \(\) under \(h\) is larger than \(2\), which indicates that the certified robustness of \(h\) at \((,^{+})\) is holds in this case. Moreover, it also paves us a efficient way to compute the certified perturbation size in practical (See in Sec. 5.2).

### Transfer the Certified Robustness to Downstream Tasks

Though we have theoretically shown the capability of learning \(l_{0}^{k}\)-certifiably robust node/graph representations with RES, it is unclear whether the certified robustness of the smoothed GNN encoder can be preserved in downstream tasks. To address this concern, we propose a theorem that establishes a direct relationship between the certified robustness of GCL and that of downstream tasks.

**Theorem 3**.: _Given a GNN encoder \(h\) trained via GCL and an clean input \(\). \(^{+}\) and \(^{-}=\{_{1}^{-},,_{n}^{-}\}\) are the positive sample and the set of negative samples of \(\), respectively. Let \(c^{+}\) and \(c^{-}_{i}\) denote the latent classes of \(^{+}\) and \(^{-}_{i}\), respectively. Suppose \(f\) is the downstream classifier that classify a data point into one of the classes in \(\). Then, we have_

\[(f(h())=c^{+})>_{_{i}^{-}^{-}} (f(h())=c^{-}_{i})\] (10)

The proof is in Appendix C.3. By applying Theorem 3, we can prove given \(\)'s perturbed version \(^{}=\), where \(||||_{0} k\), if \(\) and \(^{}\) satisfy Eq. (8) and Eq. (9) in Theorem 2, we have

\[(f(h(^{}))=c^{+})>_{_{i }^{-}^{-}}(f(h(^{}))=c^ {-}_{i}),\ \ \ \|\|_{0} k,\] (11)

which implies provable \(l_{0}^{k}\)-certified robustness retention of \(h\) at \((,c^{+})\) in downstream tasks.

## 5 Practical Algorithms

In this section, we first propose a simple yet effective GCL method to train the GNN encoder \(h\) robustly. Then we introduce the practical algorithms for the prediction and robustness certification.

### Training Robust Base Encoder

Theorem 2 holds regardless how the base GNN encoder \(h\) is trained. However, introducing randomized edgedrop noise solely to test samples during the inference phase could potentially compromise performance in downstream tasks, which further negatively impact the certified robustness performance based on Eq. 8. In order to make \(g\) can classify and certify \((,c^{+})\) correctly and robustly, inspire by , we propose to train the base GNN encoder \(h\) via GCL with randomized edgedrop noise. Our key idea is to inject randomized edgedrop noise to one augmented view and maximize the probability in Eq. 5 by using GCL to maximize the agreement between two views. Fig. 1 gives an illustration of general process of training the base GNN encoder via our RES. Given an input graph \(\), two contrasting views \(_{i}\) and \(_{j}\) are generated from \(\) through stochastic data augmentations. Specifically, two views generated from the same instance are usually considered as a positive pair, while two views constructed from different instances are considered as a negative pair. After that, we inject the randomized edgedrop noise \(\) to one of the two views \(_{i}\) and obtain \(_{i}{}^{}\). Finally, two GNN encoders are used to generate embeddings of the views and a contrastive loss is applied to maximize the agreement between \(_{i}{}^{}\) and \(_{j}\). The training algorithm is summarized in Appendix D.

Figure 1: General Framework of training GNN encoder via RES.

### Prediction & Certification

Following [46; 24], we then present the algorithm to use the smoothed GNN encoder \(g()\) in the downstream tasks and derive the robustness certificates through Monte Carlo algorithms.

Prediction on Downstream Tasks.We draw \(\) samples of \(h()\), corrupted by randomized edgedrop noises. Then we obtain their predictions via the downstream classifier. If \(c_{A}\) is the class which has the largest frequency \(_{c_{A}}\) among the \(\) predictions, \(c_{A}\) is returned as the final prediction.

Compute Robustness Certificates.One of robustness certificates is the certified perturbation size, which is the maximum attack budget that do not change the prediction of an instance no matter what perturbations within the budget are used. To derive the certified perturbation size of an instance \(\), we need to estimate the lower bound \(^{+},h}}()\) and upper bound \(^{-}_{i},h}}()\)in Eq. 8. Since it is challenging to directly estimate them in GCL, inspired by Theorem 3, if the prediction of \(h()\) in downstream tasks is correct, \(\) will also be the positive sample of \(\) under \(h\), which indicates the connection between \(^{+},h}}()\) and the probability of correctly classified in the downstream tasks. Formally, given a label set \(\) for the downstream task, let \(}\) denotes the lower bound of the probability that \(h()\) is correctly classified as \(c_{A}\) in the downstream task, \(}\) is the upper bound of the probability that \(h()\) is classified as \(c\) for \(c\{c_{A}\}\). we have the following probability bound by a confidence level at least \(1-\):

\[^{+},h}}()=}= B(|};_{c_{A}},-_{c_{A}}+1),^{-}_{i},h}}()=}=( _{c c_{A}}\,},1-})\] (12)

where \(}=B(1-|};_{c}+1,-_{c}),  c c_{A}\), and \(B(q;u,w)\) is the \(q\)-th quantile of a beta distribution with shape parameter \(u\) and \(w\). After that, we calculate \(\) based on Theorem 2 and the maximum \(k\) that satisfies Eq. 8 is the certified perturbation size.

## 6 Experiments

In this section, we conduct experiments to answer the following research questions: (**Q1**) How robust is RES under various adversarial attacks? (**Q2**) Can RES effectively provide certifiable robustness for various GCL methods? (**Q3**) How does RES contribute to the robustness in GCL?

### Experimental Setup

Datasets.We conduct experiments on 4 public benchmark datasets for node classification, i.e., Cora, Pubmed , Coauthor-Physics  and OGB-arxiv , and 3 widely used dataset for graph classification i.e., MUTAG, PROTEINS  and OGB-molhiv . We use public splits for Cora and Pubmed, and for five other datasets, we perform a 10/10/80 random split for training, validation, and testing, respectively. The details and splits of these datasets are summarized in Appendix G.1.

Attack Methods.To demonstrate the robustness of RES to various structural noises, we evaluate RES on 4 types of structural attacks in an evasion setting, i.e., Random attack, Nettack , PRBCD , CLGA  for both node and graph classification. In our evaluation, we first train a GNN encoder on a clean dataset using GCL, and then subject RES to attacks during the inference phase by employing perturbed graphs in downstream tasks. Especially, CLGA is an poisoning attack methods for GCL. To align it with our evasion setting, we directly employ the poisoned graph generated by CLGA in downstream tasks for evaluation. The details of these attacks are given in Appendix G.2.

Compared Methods.We employ 4 state-of-the-art GCL methods, i.e., GRACE , BGRL , DGI  and GraphCL , as baselines. More specifically, GRACE, BGRL and DGI are for node classification, GraphCL and BGRL are for graph classification. We apply our RES on them to train the smoothed GNN encoders. Recall one of our goal is to validate that our method can enhance the robustness of GCL against structural noises, we also consider two robust GCL methods (i.e., Ariel  and GCL-Jaccard ) and two unsupervised methods ( i.e., Node2Vec  and GAE ) as the baselines. All hyperparameters of the baselines are tuned based on the validation set to make fair comparisons. The detailed descriptions of the baselines are given in Appendix G.3

Evaluation Protocol.In this paper, we conduct experiments on both transductive node classification and inductive graph classification tasks. For each experiment, a 2-layer GCN is employed as the backbone GNN encoder. We adopt the common used linear evaluation scheme , where each model is firstly trained via GCL and then the resulting embeddings are used to train and test a\(l_{2}\)-regularized logistic regression classifier. The certified accuracy and robust accuracy on test nodes are used to evaluate the robustness performance. Specifically, _certified accuracy_[46; 24] denotes the fraction of correctly predicted test nodes/graphs whose certified perturbation size is no smaller than the given perturbation size. Each experiment is conduct 5 times and the average results are reported.

### Performance of Robustness

To answer **Q1**, we compare RES with the baselines on various noisy graphs. We also conduct experiments to demonstrate that RES can improve the robustness against different noisy levels, which can be found in Appendix I.2. We focus on node classification as the downstream task on four types of noisy graphs, i.e., raw graphs, random attack perturbed graphs, CLGA perturbed graphs and PRBCD perturbed graphs. The perturbation rate of noisy graphs is \(0.1\). The details of the noisy graphs are presented in Appendix G.2. GRACE is used as the target GCL method to train a GCN encoder. The smoothed version of GRACE is denoted as RES-GRACE. The results on Cora, Coauthor-Physics and OGB-arxiv are given in Table 1. From the table, we observe: **(i)** When no attack is applied on the clean graph, our RES-GRACE achieve state-of-the-art performance, especially on large-scale datasets, which indicates RES is beneficial to learn good representation by injecting random edgedrop noise to the graph. **(ii)** The structural noises degrade the performances of all baselines. However, its impact to RES-GRACE is negligible. RES-GRACE outperforms all baselines including two robust GCL methods, which indicates RES could eliminate the effects of the noisy edges. More results on graph classification are shown in Appendix I.1.

### Performance of Certificates

To answer **Q2**, we use certified accuracy as the metric to evaluate the performance of robustness certificates. We choose GCN as the GNN encoder and employ our method on GRACE  and GraphCL  for node classification and graph classification, respectively. We select the overall test nodes as the target nodes and set \(=200\), \((1-)=99.9\%\) to compute the certified accuracy. The results for various \(\) are shown in Fig. 2, where the x-axis denotes the given perturbation size. From the figures, we can observe that \(\) controls the tradeoff between robustness and model utility. When \(\) is larger, the certified accuracy on clean graph is larger, but it drops more quickly as the perturbation size increases. Especially, when \(=0.999\), the certified accuracy is nearly independent of the perturbation size. Our analysis reveals that when \(\) is low, more structural information is retained on the graph, which benefits the performance of RES under no attack, but also results in more noisy edges being retained on the noisy graph, leading to lower certified accuracy. Conversely, when \(\) is large, less structural information is retained on the graph, which may affect the performance of RES

   Dataset & Graph & Node2Vec & GAE & GCL-Jac. & Ariel & GRACE & RES-GRACE \\   & Raw & 67.6\(\)1.0 & 76.8\(\)0.9 & 76.1\(\)2.0 & **79.8\(\)0.6** & 77.1\(\)1.6 & 79.7\(\)1.0 \\  & Random & 57.7\(\)0.7 & 74.2\(\)0.9 & 74.1\(\)2.0 & 76.3\(\)0.6 & 74.5\(\)2.1 & **79.1\(\)1.2** \\  & CLGA & 64.7\(\)0.7 & 72.7\(\)1.3 & 73.7\(\)1.2 & 76.6\(\)0.4 & 74.9\(\)2.0 & **78.2\(\)1.0** \\  & PRBCD & 63.3\(\)3.2 & 75.6\(\)2.1 & 75.3\(\)1.2 & 75.6\(\)0.2 & 75.8\(\)2.5 & **78.5\(\)1.7** \\   & Raw & 66.4\(\)2.0 & 78.4\(\)0.4 & 78.2\(\)2.7 & 78.0\(\)1.1 & 79.5\(\)2.9 & **79.5\(\)1.2** \\  & Random & 56.8\(\)1.4 & 71.8\(\)1.0 & 75.8\(\)2.4 & 75.9\(\)2.0 & 75.0\(\)1.0 & **78.2\(\)0.9** \\  & CLGA & 61.9\(\)1.2 & 77.6\(\)0.5 & 76.2\(\)2.8 & 77.5\(\)1.1 & 76.0\(\)2.5 & **78.3\(\)1.1** \\  & PRBCD & 55.9\(\)1.5 & 74.8\(\)2.5 & 73.1\(\)2.0 & 73.5\(\)1.6 & 73.2\(\)2.3 & **78.8\(\)1.7** \\   & Raw & 92.9\(\)0.1 & 95.2\(\)0.1 & 94.1\(\)0.3 & **95.3\(\)0.3** & 94.0\(\)0.4 & 94.7\(\)0.2 \\  & Random & 85.7\(\)0.3 & 93.7\(\)0.1 & 93.4\(\)0.3 & 93.8\(\)0.1 & 92.6\(\)0.5 & **94.2\(\)0.3** \\  & PRBCD & 81.8\(\)0.6 & 91.1\(\)0.7 & 91.6\(\)0.2 & 91.0\(\)0.9 & 89.2\(\)0.6 & **94.1\(\)0.2** \\   & Raw & 64.6\(\)0.1 & 61.5\(\)0.5 & 64.7\(\)0.2 & 64.7\(\)0.3 & 65.1\(\)0.5 & **65.2\(\)0.1** \\  & Random & 52.4\(\)0.1 & 57.4\(\)0.5 & 59.0\(\)0.1 & 59.4\(\)0.4 & 59.0\(\)0.2 & **60.0\(\)0.1** \\   & PRBCD & 56.5\(\)0.5 & 54.1\(\)0.5 & 56.5\(\)0.4 & 57.0\(\)0.9 & 55.7\(\)0.4 & **58.3\(\)0.4** \\   

Table 1: Robust accuracy results for node classification.

Figure 2: Certified accuracy of smoothed GCL

### Ablation Study

To answer **Q3**, we conduct several ablation studies to investigate the effects of our proposed RES method. More results of ablation studies are shown in Appendix J. We also investigate how hyperparameters \((1-)\) and \(\) affect the performance of our RES, which can be found in Appendix K.

Our first goal is to demonstrate RES is more effective in GCL compared with vanilla binary randomized smoothing in , We implement a variant of our model, FLIP, which replaces the random edgedrop noise with binary random noise , thereby flipping the connection status within the graph with the probability \(\). For our method, we set \(=0.9\), \(1-=99\%\), and \(=50\). For FLIP, to ensure a fair comparison, we set \(=99\%\) and \(=50\), and vary \(\) over \(\{0.1,0.2,,0.9\}\) and select the value which yields the best performance on the validation set of clean graphs. The target GCL method is selected as GRACE. We compare the robust accuracy of the two methods on the clean graph and noisy graph under Nettack with attack budget \(3\). The average robust accuracy and standard deviation on Cora and Pubmed are reported in Fig. 3 (a) and (b). We observe: **(i)** RES achieves better results on various graphs (i.e., clean and noisy graph) compared to FLIP and the base GCL method. **(ii)** FLIP performs much worse than the base GCL method and RES on the clean graph, suggesting that vanilla binary randomized smoothing is ineffective in GCL. That is because FLIP introduces excessive noisy edges, which in turn makes it challenging for GCL to learn accurate representations.

Second goal is to understand how RES contributes to the robustness of GCL. We implement several variants of our model by removing structural information in the training and testing phases, which are named RES\({}_{i;}\), where \(i\{0,1,2\}\) and \(j\{0,1\}\) denote the number of removed structures in the training and testing phases, respectively. For our method, we set \(=0.9\), \(1-=99\%\) and \(=50\). We compare the robust accuracy on clean and noisy graphs and use PRBCD to perturb 10% of the total number of edges in the graph (before attack) for noisy graphs. The overall test set was selected as the target nodes. The results on Cora and OGB-arxiv are shown in Fig. 3 (c) and (d). We observe: **(i)** Our method significantly outperforms the ablative methods on various graphs, corroborating the effectiveness of randomized edgedrop smoothing for GCL. **(ii)** Our method and RES\({}_{100}\) achieve better robust accuracy on various graphs compared to other ablative methods. This is because dropping edges in only one augmented view during training both alleviates over-fitting and increases the worst-case margin, which is helpful for robustness and model utility. **(iii)** RES\({}_{2;}\) and RES\({}_{i1}\) perform worse than other methods because of the absence of structural information in training and test phases, highlighting the importance of structural information in GCL. Moreover, the ablation studies on the effectiveness of our approach for training the GNN encoder are in Appendix J.1.

## 7 Conclusion

In this paper, we present the first work to study the certifiable robustness of GCL. We address the existing ambiguity in quantifying the robustness of GCL to perturbations by introducing a unified definition for robustness in GCL. Our proposed approach, Randomized Edgedrop Smoothing, injects randomized edgedrop noise into graphs to provide certified robustness for GCL on unlabeled data, while minimizing the introduction of spurious edges. Theoretical analysis establishes the provable robust performance of our encoder in downstream tasks. Additionally, we present an effective training method for robust GCL. Extensive empirical evaluations on various real-world datasets show that our method guarantees certifiable robustness and enhances the robustness of any GCL model.

Figure 3: Ablation Study Results: Comparisons (a) and (b) with FLIP, (c) and (d) with RES\({}_{i;}\) under no attack, but also results in fewer noisy edges remaining on the noisy graph, leading to higher certified accuracy for larger perturbation sizes.

Acknowledgements

This material is based upon work supported by, or in part by, the National Science Foundation (NSF) under grant number IIS-1909702, the Army Research Office (ARO) under grant number W911NF21-1-0198, and Department of Homeland Security (DNS) CINA under grant number E205949D. The findings in this paper do not necessarily reflect the view of the funding agencies.