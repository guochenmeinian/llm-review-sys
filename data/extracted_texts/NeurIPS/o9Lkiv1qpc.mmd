# Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model

Min Zhao\({}^{1,3}\), Hongzhou Zhu\({}^{1,3}\), Chendong Xiang\({}^{1,3}\), Kaiwen Zheng\({}^{1,3}\),

**Chongxuan Li\({}^{2}\)**, Jun Zhu\({}^{1,3,4}\) +

\({}^{1}\)Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University

\({}^{2}\) Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China

Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

\({}^{3}\)ShengShu, Beijing, China; \({}^{4}\)Pazhou Laboratory (Huangpu), Guangzhou, China

{gracezhao1997,xiangxyaw,zkwthu}@gmail.com; zhuhz22@mails.tsinghua.edu.cn; chongxuanli@ruc.edu.cn; dcszj@tsinghua.edu.cn

Equal contribution. \({}^{}\)Correspondence to: C. Li and J. Zhu.

###### Abstract

Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control.

The project page: https://cond-image-leak.github.io/.

## 1 Introduction

Image-to-video (I2V) generation aims to generate videos with dynamic and natural motion while maintaining the content of the given image. It allows users to guide video creation from the input image (and optional text), thus increasing controllability and flexibility in content creation. Like the remarkable progress in text-to-image (T2I) generation  and text-to-video (T2V) generation , diffusion models have also obtained promising results for I2V generation . However, such models are not fully understood.

In this paper, we observe that existing image-to-video diffusion models (I2V-DMs) tend to generate videos with less motion than expected (see Fig. 1). We attribute this to a previously overlooked issue called _conditional image leakage_ (see Sec. 3.1). Normally, the noisy input contains the motion information of the target video, and I2V-DMs should rely on it to predict motion, while the static conditional image provides content guidance. However, in practice, as the diffusion processprogresses--especially at large time steps, the noisy input becomes heavily corrupted, while the conditional image preserves extensive detail of the target video. This biases the model to over-rely on the conditional image and neglect the noisy input, leading to videos with reduced motion. To validate this, we corrupt the ground truth (GT) clean video and compute one-step clean video predictions at each time step. As shown in Fig. 2, the predicted clean videos exhibit markedly reduced motion than GT at large time steps, indicating leakage.

Based on the above analysis, we attempt to address the challenge from both inference and training aspects (see Sec. 3.2). First, we present a simple yet effective inference strategy that starts the video generation process from an earlier time step, thus avoiding the unreliable large-time steps of the I2V-DMs. To further enhance performance, we derive an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the true marginal distribution to bridge the training-inference gap. Second, to mitigate leakage during training, we propose a time-dependent noise distribution (TimeNoise) that increases noise levels at larger time steps, effectively disrupting the conditional image and reducing model dependency on it. This is achieved by employing a logit-normal distribution with a center that gradually shifts over time. Finally, our method achieves higher motion scores with reduced motion score error and ensures that the predicted clean video maintains motion dynamics comparable to the ground truth across all time steps, effectively mitigating conditional image leakage. Notably, our general strategies are adaptable to various I2V-DMs based on both VP-SDE [12; 63; 70] and VE-SDE  framework.

Empirically, we validate our methods on various I2V-DMs [9; 63; 12; 70] using our collected open-domain images (ImageBench) and the UCF101 dataset. We conduct a user study on ImageBench and report FVD , IS , and motion score  on UCF101. For motion-conditioned models [9; 70], we also report the motion score error between the generated video and the input motion score at different levels. Extensive experimental results demonstrate that our strategies outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control.

## 2 Background

Diffusion Models.Diffusion models gradually perturb the data \(_{0} q(_{0})\) via a forward diffusion process and reverse the process to recover it. The forward transitional kernel \(q_{t|0}(_{t}|_{0})\) is given by

\[_{t}=_{t}_{0}+_{t}, (,), t[0,T],\] (1)

where \(_{t}\) and \(_{t}\) are the noise schedule chosen to ensure that \(_{T}\) contains minimal information about \(_{0}\). Such forward diffusion processes can be viewed as stochastic differential equations (SDEs), among which two prevalent types are commonly used [48; 29]. One is the variance-preserving SDE (VP-SDE) [42; 24], where \(_{t}^{2}+_{t}^{2}=1\) with \(_{t} 0\) as \(t T\), ensuring \(p_{T}(_{T})=(_{T};,)\). The other is the variance exploding SDE (VE-SDE), where \(_{t} 1\) and \(_{T}\) is set to a large constant, resulting in \(p_{T}(_{T})(_{T};,_{T}^{2})\). Such models can be parameterized with a noise-prediction model \(_{}(_{t},t)\) (\(\)-prediction) ,and the parameters are learned by minimizing:

\[_{_{0} q(_{0}),(,),t(1,T)}[\|_{}(_{t},t)- \|_{2}^{2}],\] (2)

Figure 1: **The issue of existing I2V-DMs.** Regardless of input motion scores (Input MS), the output motion scores (Output MS) are consistently lower than expected. In contrast, our method yields output motion scores either higher or lower than Input MS with reduced error.

where the noisy input \(_{t} q_{t|0}(_{t}|_{0})\). Alternative parametrizations such as \(_{0}\)-prediction , \(\)-prediction , _F_-prediction  are also commonly applied. The \(\)-prediction and \(_{0}\)-prediction aims to predict the added noise or clean video from noisy input \(_{t}\). Starting from \(_{T} p_{T}(_{T})\), various samplers [47; 33; 34; 28] can be employed to generate data. More recent studies  further demonstrate that diffusion models can generate realistic images with controllable semantics given only a few labels.

Diffusion Models for Image-to-Video Generation.Given an image \(_{0}\) from the open domain, the goal of I2V is to generate a video \(X_{0}=\{_{0}^{i}\}_{i=1}^{N}\) with dynamic and natural motion while keeping alignment with the appearance of \(_{0}\). This task can be formulated as designing a conditional distribution \(p_{}(X_{0}|_{0})\), which is achieved by a conditional diffusion model minimizing:

\[_{X_{0},_{0},,t}[\|_{}(X_ {t},_{0},t)-\|_{2}^{2}],\] (3)

where \(X_{t} q_{t|0}(X_{t}|X_{0})\). Typically, \(_{0}\) is the first frame of \(X_{0}\) and DynamiCrafter  adopts a randomly selected frame from \(X_{0}\) as \(_{0}\). The key issue is to effectively integrate the conditional image \(_{0}\) into the diffusion model. Most methods use CLIP image embeddings  to maintain the semantic content of \(_{0}\). Notably, VideoCrafter1  and Dynamicrafter  employ the last layer's full patch visual tokens from the CLIP ViT, enriching the encoded information, and other approaches prefer the class token layer. Yet, solely depending on these embeddings, such as in VideoCrafter1 , compromises detail retention, resulting in degradation of the image alignment. To enhance detail representation, I2VGen-XL  combines the conditional image with the noisy initial frame, while VideoComposer  develops a STC-encoder for multiple conditions. Although superior to CLIP image embeddings, these strategies still fail to fully retain the conditional image content. To mitigate this, AnimateAnything , Dynamicrafter  and SVD  directly concatenate noisy video \(X_{t}\) with \(_{0}\), which injects detailed information to the model. Apart from the prior work mentioned before, we discuss other related work about diffusion models for image generation and video generation in the Appendix E.

## 3 Method

Although existing I2V-DMs discussed in Sec. 2 have achieved significant progress, such models are not fully understood. In this section, we first identify a critical yet previously overlooked issue in I2V-DMs: conditional image leakage (CIL) (see Sec. 3.1). We then address this issue from both inference and training aspects accordingly (see Sec. 3.2). Finally, we offer insights into existing I2V-DMs through the lens of CIL (see Sec. 3.3).

Figure 2: **Identifying conditional image leakage**. As time step progresses, the noisy input becomes heavily corrupted, whereas the conditional image retains considerable detail from GT. This biases the model to over-rely on the conditional image at large \(t\), resulting in videos with less motion than GT.

### Identifying Conditional Image Leakage in Image-to-video Diffusion Models

As shown in Fig. 1, we observe that, regardless of the input motion scores, the motion scores of generated videos from existing I2V-DMs [9; 70] are consistently lower than expected. This raises the question: why do these models always produce lower motion scores, rather than fluctuating above or below the expected values, as observed in our method?

To understand this, we need to consider the source of motion information in the generated videos, which comes from the noisy input \(X_{t}\). Ideally, I2V-DMs should rely primarily on \(X_{t}\) for motion, with the static conditional image \(_{0}\) providing content guidance. However, as shown in Fig. 2, at large time steps, the noisy input \(X_{t}\) becomes increasingly corrupted, while the conditional image \(_{0}\) retains significant information of the target video. This biases the model to over-rely on the conditional image and neglect the noisy inputs, leading to videos with reduced motion.

To validate this, we corrupt a ground truth (GT) clean video \(X_{0}\) via the forward transition kernel in Eq. (1) and use it as the noisy input to compute the one-step prediction \(_{t 0}\) at time \(t\):

\[_{t 0}=(X_{t}-_{t}_{}(X_{t},y,t) )/_{t}.\] (4)

Ideally, \(_{t 0}\) should predict the GT \(X_{0}\) from noisy input \(X_{t}\) and exhibit comparable motion dynamics. However, as shown in Fig. 2, as time progresses--particularly at large time steps, \(_{t 0}\) exhibits markedly reduced motion than GT, indicating the conditional image leakage. This results in videos with reduced motion starting from time \(T\). Notably, recent techniques that adjust the noise schedule towards higher noise levels [14; 27; 9; 32] may further exacerbate this issue (see Appendix C for details).

### Solving Conditional Image Leakage in Image-to-video Diffusion Models

Building upon the above analysis, this section presents general strategies to address the issue of conditional image leakage in both the inference and training aspects.

Figure 3: **Benefits of Analytic-Init.** (a) An early start time \(M\) enhances motion but a too-small \(M\) degrades visual quality due to the training-inference gap, which Analytic-Init helps to reduce. (b) Analytic-Init produces higher motion scores with lower errors, mitigating conditional image leakage.

**Inference strategy.** As discussed in Sec. 3.1, conditional image leakage easily occurs at large time steps. To this end, a straightforward solution is to start the generation process from an earlier time step \(M(0,T)\), thus avoiding the unreliable later stages of I2V-DMs. Let \(p_{M}(X_{M})\) denote the initial noise distribution at the start time \(M\). Initially, we set \(p_{M}(X_{M})=p_{T}(X_{T})\), i.e. \((,)\) in VP-SDE  or \((,_{T}^{2})\) in VE-SDE . As illustrated in Fig. 3 (a), this straightforward strategy markedly improves motion dynamics without sacrificing other performance. However, a smaller \(M\) value (e.g., \(M=0.8T\)) results in poor visual quality due to the training-inference discrepancy.

To mitigate this gap, we propose Analytic Noise Initialization (_Analytic-Init_) to refine the initial noise distribution \(p_{M}(X_{M})\) by minimizing the KL divergence between it and the true marginal distribution \(q_{M}(X_{M})\) of the forward diffusion process. Inspired by previous work , we demonstrate that when \(p_{M}(X_{M})\) is modeled as a normal distribution \((X_{M};_{p},_{p}^{2})\), the optimal mean \(_{p}^{*}\) and variance \(_{p}^{2*}\) have analytical solutions, as stated in Proposition 1.

**Proposition 1**.: _Given a normal distribution \(p_{M}(X_{M})=(X_{M};_{p},_{p}^{2})\) and \(q_{M}(X_{M})\) is the margin distribution of diffusion forward process at time \(M\), with the forward transition kernel \(q_{M|0}(X_{M}|X_{0})=(X_{M};_{M}X_{0},_{M}^{2})\), the minimization problem \(_{_{p},_{p}^{2}}D_{KL}(q_{M}(X_{M})\|p_{M}(X_{M}))\) yields the following optimal solution:_

\[_{p}^{*}=_{M}_{q(X_{0})}[X_{0}], _{p}^{2*}=_{M}^{2}^{d}[Var(X_{0}^{(j)})]}{d}+ _{M}^{2},\] (5)

Figure 4: **Visualization of TimeNoise and the impact of tuning its hyperparameters.** (a) The designed \(p_{t}(_{s})\) favors high noise levels at large \(t\), gradually shifting to lower noise levels as \(t\) decreases. This is achieved by (b) \((t)\) increasing monotonically with \(t\). Finally, (c) modifying \(a\) and \(_{m}\) enables a trade-off between dynamic motion and image alignment.

_where \(q(X_{0})\) denotes the data distribution, \(d\) denotes the dimension of the data, and \(X_{0}^{(j)}\) denotes the j-th component of \(X_{0}\)._

The proof of Proposition 1 is provided in Appendix A. Empirically, \(_{p}^{*}\) and \(_{p}^{2*}\) can be estimated using the method of moments . The steps for inference are outlined in Algorithm 1. As demonstrated by the qualitative results in Fig. 3 (a) and quantitative results in Tab. 8, Analytic-Init improves video quality by reducing the training-inference gap, especially for smaller \(M\). Finally, as shown in Fig. 3 (b) and Tab. 1, Analytic-Init produces higher motion scores with lower errors, allowing for more accurate motion control and reducing conditional image leakage.

Training strategy.In this section, we show how to address the issue of conditional image leakage during the training phase. As outlined in Sec. 3.1, the conditional image \(_{0}\) retains substantial details of the target video, causing I2V-DMs to rely heavily on it. To mitigate this, a natural approach is to perturb \(_{0}\) to relieve this dependency. Our first attempt is to introduce noise at a similar scale to that in \(X_{t}\), aiming to balance the model's challenge of predicting clean video from \(X_{t}\) or \(_{0}\), thereby lessening reliance on \(_{0}\). However, this strategy also makes it difficult to employ \(_{0}\), resulting in lower video quality.

To overcome this, we propose a noise distribution on \(_{0}\) that introduces substantial noise to prevent leakage while maintaining a cleaner \(_{0}\) to aid content generation. Given that \(X_{t}\) contains less information about \(X_{0}\) as time progresses, increasing the risk of leakage, we further develop a time-dependent noise distribution \(p_{t}(_{s})\) (TimeNoise). The key principle is to favor high noise levels at large time steps to sufficiently disrupt \(_{0}\), shifting towards lower noise levels as the time step decreases. To achieve this, we employ a logit-normal distribution  defined as below:

\[p_{t}(_{s};(t),_{m})=}{}(_{m}-_{s})}e^{-(}{_{m}}) -(t))^{2}}{2}},\] (6)

where \((}{_{m}})\) follows a normal distribution centered around \((t)\) with a standard deviation of 1. This noise distribution includes two hyperparameters: \(_{m}\), the maximum noise level, and \((t)\), the center of the distribution. As illustrated in Fig. 4 (a), we can adjust \((t)\) over time \(t\) to satisfy the previously mentioned design principle. Finally, the noisy conditional image \(_{s}\) at time \(t\) is obtained by \(_{s}=_{0}+_{s}\), where \(_{s} p_{t}(_{s}),(,)\). We also tried other adding noise choices but found them to be less effective (see Appendix D). During inference, we add a fixed noise level to the conditional image across all time steps, following CDM, as the model is trained with varying noise levels. Empirically, we find directly using the clean conditional image performs well and thus adopt it for simplicity. The full algorithm is shown in Algorithm 2.

Figure 5: **Benefits of TimeNoise. TimeNoise (a) generates \(_{t 0}\) that maintains motion dynamics comparable to the GT across all time steps, and (b) achieves higher motion scores with lower errors, effectively reducing conditional image leakage.**

Next, we conduct a systematic analysis of the two hyperparameters to investigate their impact on video generation. Firstly, \((t)\) is designed to increase monotonically with time, ranging from \((0)=-1\) to \((T)=1\). To formalize this, we define \((t)\) as a power function: \((t)=2t^{a}-1\), where \(a>0\). This formulation allows flexible tuning of \(a\) to control the monotonic behavior, where smaller values of \(a\) cause higher noise levels to be sampled at later time steps. As shown in Fig. 4 (b), (1) \(a=1\) corresponds to a linear increase; (2) \(a(0,1)\) represents a concave function, indicating a faster noise level increase; and (3) \(a>1\) corresponds to a convex function, indicating a slower increase in noise levels over time. As shown in Fig. 4 (c), higher noise levels (e.g., when \(a<1\)) lead to increased dynamic motion but reduced temporal consistency and image alignment. For the maximum noise level \(_{m}\), the only constraint is that it must be greater than 0. As shown in Fig. 4 (c), a higher \(_{m}\) enhances dynamic motion but decreases temporal consistency and image alignment, while a lower \(_{m}\) reduces motion. Additionally, we apply TimeNoise to the CLIP Image Embedding for both VideoCrafter1  and DynamiCrafter , as they use full patch visual tokens from CLIP, which contain substantial information about the conditional image, increasing the likelihood of leakage.

Finally, we replicate the experiments described in Sec. 3.1, with results presented in Fig. 5 and Tab. 1. These results demonstrate that TimeNoise achieves higher motion scores with reduced error and ensures that \(_{t 0}\) maintains motion dynamics comparable to the ground truth across all time steps, effectively mitigating conditional image leakage.

### Understanding Existing Work from Conditional Image Leakage

In this section, we analyze popular I2V-DMs  through the lens of CIL. Although these models do not explicitly address this issue, we believe their strategies mitigate it to some degree.

Firstly, some methods only use partial information from the conditional image, which can help reduce leakage. For example, VideoCrafter1  only utilizes CLIP Image Embedding, and I2VGEN-XL  adds \(_{0}\) to the first frame of the noisy input. However, as shown in Fig. 6, the videos generated by these methods do not fully capture the details of \(_{0}\). To address this, models like DynamiCrafter  directly incorporate \(_{0}\) into I2V-DMs, improving detail preservation but also increasing the risk of leakage. DynamiCrafter selectively refines spatial layers while preserving the pre-trained temporal layers, which contain motion priors and thus maintain motion dynamics to a certain extent. However, it does not inherently solve the leakage issue. Moreover, some methods introduce external signals , forcing the model to align with additional conditions, which reduces its dependence on \(_{0}\) and helps mitigate leakage. However, we argue that they do not address the core challenge of I2V-DMs, which should predict clean video primarily from noisy input to capture motion information. As illustrated in Fig. 6, the SVD often results in static objects with excessive camera movements to meet high motion score requirements. In contrast, our method generates videos with natural, vivid object movements. In summary, while the above methods mitigate

Figure 6: **Understanding exiting work from conditional image leakage.** I2VGen-XL  and VideoCrafter1  mitigates the leakage at the expense of image alignment. The SVD produces videos with camera movements while keeping objects relatively static to meet high motion scores, while ours generates videos that feature both natural and dynamic object movements.

CIL to some extent, our approach provides a more effective solution to the fundamental challenges in I2V-DMs, enabling more precise motion control and enhancing naturalness by focusing more on the noisy input.

## 4 Experiments

### Setup

Datasets.We use WebVid-2M  as the training dataset, with all videos resized and center-cropped to \(320 512\) at 16 frames and 3 fps. For evaluation, we use UCF101  and our ImageBench dataset, which includes diverse categories (e.g., nature, humans, animals, plants, food, vehicles) and complex elements like numerals, colors, and intricate scenes, similar to DrawBench . In total, 100 images are collected from various websites and T2I models such as SDXL  and UniDiffuser .

Evaluation Metrics.On UCF101, we report Frechet Video Distance (FVD), Inception Score (IS), and Motion Score (MS). For ImageBench, we conduct user studies with 10 subjects to perform pairwise comparisons of our methods against baselines, evaluating motion, temporal consistency, image alignment, and overall performance. For motion-conditioned methods, we also report the motion score error between the generated video and the input motion score at various levels. Motion scores are computed using flow maps following SVD , except for the PIA , where we follow its original algorithm and compute the \(L1\) distance. More details on the metric computations are provided in Appendix B.

Implementation Details.For Analytic-Init, we use 5000 samples from the Webvid-2M dataset to estimate \(p_{M}(X_{M})\) by default. We set \(M=0.96T,0.96T\) and \(_{M}=100\) for VideoCrafter1 , DynamiCrafter , and SVD  on UCF101. For ImageBench, we adjust \(M\) to \(0.92T,0.92T\) and \(_{M}=100\). For the TimeNoise, we set \(_{m}=25,100\), and \(100\) for VideoCrafter1, DynamiCrafter, and SVD, respectively. The function \((t)=2t^{5}-1\) is applied across all baselines. These baselines are fine-tuned for 20,000 iterations, using either the official code or replication of the official settings except for batch size. More detailed information can be found in the Appendix B.

Figure 7: **Qualitative results of TimeNoise and Analytic-Init applied to various I2V-DMs.** Ours significantly enhances video dynamism while maintaining image alignment and temporal consistency. VC. and DC. denote VideoCrafter1  and DynamiCrafter  respectively.

[MISSING_PAGE_FAIL:9]

## 5 Conclusions and Discussions

In this paper, we identify a common issue in I2V-DMs: conditional image leakage. We address this challenge from two aspects. First, we introduce an inference strategy that starts the generation process from an earlier time step to avoid the unreliable late-time steps of I2V-DMs. Second, we design a time-dependent noise distribution for the conditional image to mitigate conditional image leakage during training. We validate the effectiveness of these strategies across various I2V-DMs.

Limitations and broader impact.One limitation of this paper is the need to balance TimeNoise to prevent conditional image leakage while maintaining image integrity. While we demonstrate the effectiveness of our strategy on existing I2V-DMs, we do not provide a definitive choice for a scratch-trained model. We leave this in the future work. Furthermore, we must use the method responsibly to prevent any negative social impacts, such as the creation of misleading fake videos.