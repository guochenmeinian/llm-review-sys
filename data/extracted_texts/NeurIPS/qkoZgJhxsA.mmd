# _SocraticLM_: Exploring Socratic Personalized Teaching with Large Language Models

Jiayu Liu\({}^{1,2}\) Zhenya Huang\({}^{1,2}\) Tong Xiao\({}^{1,2}\) Jing Sha\({}^{2}\) Jinze Wu\({}^{2}\)

Qi Liu\({}^{1,2}\) Shijin Wang\({}^{2}\) Enhong Chen\({}^{1,2}\)

1: University of Science and Technology of China

2: State Key Laboratory of Cognitive Intelligence

{jy251198,tongxiao2002}@mail.ustc.edu.cn;

{huangzhy,qiliuql,cheneh}@ustc.edu.cn;

{jingsha,jzwu4,sjwang3}@iflytek.com

Corresponding Authors

###### Abstract

Large language models (LLMs) are considered a crucial technology for advancing intelligent education since they exhibit the potential for an in-depth understanding of teaching scenarios and providing students with personalized guidance. Nonetheless, current LLM-based application in personalized teaching predominantly follows a "Question-Answering" paradigm, where students are _passively_ provided with answers and explanations. In this paper, we propose _SocraticLM_, which achieves a Socratic "Thought-Provoking" teaching paradigm that fulfills the role of a real classroom teacher in _actively_ engaging students in the thought process required for genuine problem-solving mastery. To build _SocraticLM_, we first propose a novel _"Dean-Teacher-Student"_ multi-agent pipeline to construct a new dataset, _SocraTeach_, which contains \(35\)K meticulously crafted Socratic-style multi-round (equivalent to \(208\)K single-round) teaching dialogues grounded in fundamental mathematical problems. Our dataset simulates authentic teaching scenarios, interacting with six representative types of simulated students with different cognitive states, and strengthening four crucial teaching abilities. _SocraticLM_ is then fine-tuned on _SocraTeach_ with three strategies balancing its teaching and reasoning abilities. Moreover, we contribute a comprehensive evaluation system encompassing five pedagogical dimensions for assessing the teaching quality of LLMs. Extensive experiments verify that _SocraticLM_ achieves significant improvements in the teaching performance, outperforming GPT4 by more than 12%. Our dataset and code is available at https://github.com/Ljyustc/SocraticLM.

## 1 Introduction

Large language models (LLMs) have achieved impressive results across a variety of tasks including natural language processing, translation, and question-answering . This draws widespread attention to the potential of using LLMs to revolutionize intelligent education, especially personalized teaching , mainly due to their two advantages. On one hand, LLMs have displayed human-like mastery and proficiency of knowledge in fundamental subjects like math and physics . Therefore, they can effectively deliver subject-specific instructions to students like a real teacher. On the other hand, LLMs exhibit exceptional understanding and adaptability to users' (i.e., students') inputs. They are capable of comprehending diverse students' demands, providing instant feedback, and engaging in interactive pedagogy. Based on these two aspects, there has been a lot of researchinvestigating to apply a general LLM (e.g., ChatGPT) to personalized teaching , or building specific teaching LLMs, such as MathGPT 2, EduGPT 3, and EduChat .

However, current LLMs-based personalized teaching methods predominantly adhere to a _"Question-Answering"_ paradigm. As shown in Figure 1(a), they passively offer functionalities such as providing answers to questions and explaining knowledge concepts to students' queries. In this process, they oversimplify the teaching into a series of Q&As, directly delivering complete answers based on CoT , ToT , etc., which falls short of truly identifying the issues students may have and offering targeted assistance. Consequently, students may struggle to comprehend the problem-solving process, lack a genuine improvement in their ability, and fail to resolve similar issues in the future.

In this paper, we draw inspiration from the Socratic method of teaching  and propose _SocraticLM_, which achieves a novel "_Thought-Provoking_" teaching paradigm as depicted in Figure 1(b). The key of this paradigm is to engage students in a dialogue to active participation in the learning process, which continually poses open-ended questions (marked red, e.g., "... how to calculate it?") to encourage them to articulate their thoughts, challenge assumptions, and think independently. This process enables students to learn to solve a problem by themselves, thereby fostering a deeper mastery and ability. Compared with LLM-based applications using prompt engineering directly (e.g., GPT4), we aim to systematically study 1) _The pedagogical demands of "Thought-Provoking" teaching_ and empower _SocraticLM_ to fulfill these demands. 2) _The teaching abilities of teachers_ and reinforce these abilities in _SocraticLM_. 3) _The cognitive states of students_ and enable _SocraticLM_ to accurately identify them during the teaching process. Consequently, our _SocraticLM_ can provide higher quality guidance that is more tailored and appropriate for each student's needs, transitioning from a "guardian of knowledge" to "choreographer of learning".

To build _SocraticLM_, we first construct a new dataset, _SocraTeach_, which consists of \(35\)K high-quality, fine-grained Socratic-style multi-round teaching dialogues grounded in mathematical problems. In constructing the dataset, we propose a novel _"Dean-Teacher-Student"_ pipeline, implementing three LLM agents to simulate the key roles in authentic teaching scenarios: _Dean, Teacher_, and _Student_. The _Dean_ is a director that oversees and refines the _Teacher_'s instructions before they are presented to the _Student_, ensuring that the whole teaching process adheres to the Socratic style. The _Teacher_ actively and gradually guides the _Student_ to solve a problem by generating Socratic instructions, inspired by classic pedagogical theories . The _Student_ responds to the _Teacher_'s instructions, where we establish a student cognitive state system that simulates six kinds of students in classroom to cover real and diverse teaching scenarios. Through multiple rounds of _"Teacher-Student"_ interaction under the supervision of _Dean_, a comprehensive Socratic teaching dialogue is formed. One step further, to enhance the diversity and robustness of our dataset, we summarize four types of student responses from real teaching scenarios and perform data augmentation to generate extra \(22\)K single-round teaching dialogues, specifically tailored to enhance four corresponding crucial teaching abilities.

Figure 1: Teaching paradigms: “_Question-Answering_” vs “_Thought-Provoking_”.

We fine-tune \(\)-\(6\) on our _SocraTeach_ dataset to obtain _SocraticLM_. During this process, we elaborate three training strategies to improve the pedagogical abilities while ensuring the problem-solving capacity of _SocraticLM_ simultaneously. In addition, we contribute a novel evaluation system encompassing five pedagogical dimensions for assessing the teaching quality of LLMs, which to the best of our knowledge, is the first exploration in this field. Experimental results show that our dataset can enhance the pedagogical performances of LLMs and the teaching quality of our _SocraticLM_ surpasses GPT4 by more than 12%.

The contributions of this paper are:

* We present _SocraticLM_, a language model that achieves Socratic "Thought-Provoking" teaching paradigm. Experimental results show that its Socratic teaching quality exceeds GPT4 by 12%, while maintaining the good problem-solving ability of the original \(\)-\(6\).
* We construct a new dataset _SocraTeach_ that contains massive, fine-grained Socratic teaching dialogues. To construct _SocraTeach_, we propose a novel _"Dean-Teacher-Student"_ multi-agent pipeline, in which we design an innovative supervisory role _Dean_, a cognitive state system to direct the _Student_'s behavior, and an enhancement in four teaching abilities for the _Teacher_. This pipeline is general and can be transferred to the teaching in other subjects.
* We develop a five-dimensional comprehensive evaluation system to assess the teaching quality of LLMs, which to the best of our knowledge, is the first attempt in the field.

## 2 Related Work

**LLMs-enhanced intelligent education.** Large language models (LLMs) revolutionize three typical applications of intelligent education, namely automatic generation of educational resources, instant assessment of student learning outcomes, and personalized teaching assistance [26; 29; 42]. For educational resources, there is a tendency to use LLMs to generate textbooks, exercises, etc., based on teaching goals and needs, providing teachers with richer inspiration [4; 16]. For students' outcomes, LLMs can analyze students' homework and exams to provide assessments and feedbacks on their learning progress . As for the most concerned personalized teaching in this paper, one line of research uses general LLMs like ChatGPT to provide students with multi-level assistance [44; 51; 58] in multiple disciplines, such as writing , programming , and medical education . By analyzing students' learning data and behavioral patterns, these LLMs also have the potential to design unique learning paths to help students learn more effectively . Another line of research is to collect a large amount of teaching instructions to fine-tune large models (e.g., EduChat ), giving them targeted teaching capabilities such as problem solving and emotional support.

**Personalized teaching dialogue dataset.** Constructing teaching dialogues is the basis for building LLM-based personalized teaching systems. In the literature, early attempts relied on crowd-sourcing (e.g., CIMA ) or rules (e.g., AutoTutor ) to create authentic dialogues. Subsequently, researchers adopted human-computer collaborative approaches. For instance, QuizBot  leveraged semantic similarity algorithms to analyze real students' responses and provided adaptive question by predefined teaching workflow. However, these methods required substantial manual effort or were constrained by predefined teaching procedures, resulting in limited scalability and difficulty in covering diverse real-world teaching scenarios. Recently, with LLMs demonstrating advantages in synthetic data generation , utilizing them to assist in teaching dialogue generation has attracted much attention. However, existing research  suggests that GPTs make for a bad teacher, thus current efforts mainly use LLMs to simulate students with different backgrounds , personalities  and error types , followed by human teachers providing explanations. Nevertheless, this process still requires human involvement, resulting in the latest dataset MATHDIAL  containing only 3K samples. Besides, these datasets also lack a systematic investigation into Socratic teaching.

## 3 The _SocraTeach_ Dataset

Pedagogical theories point out that there are two basic demands for Socratic teaching [13; 45]: 1) it is fundamentally dialogic, relying on conversations between teachers and students to facilitate learning; 2) it uses probing questions to actively engage students, promoting independent thinking and encouraging them to find answers themselves. In building our _SocraTeach_ dataset to meet these requirements, we face the following challenges. First, for the teacher, there is considerable variability in pedagogical methodologies and presentation styles among teachers. It may be difficult for a model to learn all of them at once, which may result in confusion and errors within the teaching logic. Second, for the student, in real teaching scenarios, students' cognitive states are intricate and heterogeneous [22; 35]. While some students have strong understanding abilities and sufficient knowledge, there are also a considerable number of students who cannot understand the problem or even lack the essential knowledge. We expect that our dataset should cover all these situations, so that the model can learn to provide different levels of guidance for students with different states.

To solve the above challenges, we construct _SocraTeach_ as follows. First, for each problem that needs to be taught, we decompose a list of step-wise guiding questions (Section 3.1). On this basis, we can control the simulation of teachers by aligning the instructional approach and explanatory style with these questions. Second, we devise an innovative _"Dean-Teacher-Student"_ pipeline, implementing three LLM agents including _"Dean"_, _"Teacher"_, and _"Student"_ to collect fine-grained multi-round teaching dialogues (Section 3.2). Especially, to align with student profiles in authentic scenarios, we build a cognitive state system to simulate six kinds of students in _"Student"_ from the aspects of comprehension, calculation, knowledge mastery, etc. (Section 3.3). Finally, to further enhance the diversity and robustness of _SocraTeach_, we design data augmentation methods to construct additional single-round teaching dialogues for improving four crucial teaching abilities (Section 3.4).

### Problem Collection & Step-by-Step Guiding Questions

In this paper, we take the teaching of mathematical problems at the primary school level as an example for exploration, because mathematics is a fundamental and critical subject and such problems involve the examination of students' basic understanding and reasoning abilities [32; 34]. Our problems are sourced from two representative datasets: MAWPS  and GSM8K , which contain \(2.3\)K and \(8.8\)K problems, respectively.

To ensure that the expression style and teaching approach are consistent in simulating the teacher role, we decompose each problem into a series of step-by-step guiding questions, such as **Q1**-**Q4** in Figure 2 (please refer to Appendix A for details). It should be noted that to ensure the efficiency and conciseness of teaching, a numerical calculation and a summary of the solution do not count as a step.

### The _Dean-Teacher-Student_ Pipeline

To create our _SocraTeach_ dataset that achieves the Socratic "Thought-Provoking" teaching paradigm, we propose a novel "_Dean-Teacher-Student_" (DTS) pipeline to collect one-to-one, multi-round, teacher-student dialogues, which consists of three LLM agents:

* _Dean_\(\): Research has indicated that GPTs have inadequacies in understanding students and language expression required to serve as a teacher . To address this issue, we propose a

Figure 2: Workflow of our _SocraTeach_ dataset construction.

_Dean_ agent to serve as an oversight role, which judges whether the _Teacher_'s instructions meet the requirements of Socratic teaching. If it thinks the instructions do not meet the requirements, it has the authority to revise them before they are presented to the _Student_.
* _Teacher_\(\): The _Teacher_ agent actively provokes the _Student_ agent to solve problems in a Socratic style, serving two primary purposes according to Socrates' educational theory . First, it should prompt the _Student_ to think at the appropriate time with Socratic questions, such as guiding the _Student_ to consider the next step after completing a reasoning step. Second, it needs to provide the _Student_ with explanations of the steps and the involved knowledge points. To maintain a consistent teaching style for a given problem, the _Teacher_ is asked to deliver teaching following the step-by-step questions constructed in Section 3.1.
* _Student_\(\): Representing the learner within the dataset, the _Student_ agent generates replies to the _Teacher_'s instructions (i.e., questions and explanations). To ensure the authenticity and diversity of _Student_, we build a cognitive state system that describes six kinds of real students in Section 3.3 and set _Student_ to simulate one of them each time it replies.

In DTS pipeline, each teaching dialogue \(\{(_{1},_{1}),(_{2},_{2}),...\}\) is formed by a cycle of interaction between _Teacher_ and _Student_ under the supervision of _Dean_, and each agent is simulated with GPT4. Taking Figure 2 as an example, in the first round (\(t=1\)), the _Teacher_ directly gives the question of the first step (i.e., \(_{1}=\)) constructed in Section 3.1 (this process does not need the use of LLMs). Then, the _Student_ selects a state profile (e.g., weak knowledge mastery) from six types of cognitive portraits in Section 3.3 and generates a corresponding response \(_{1}\) based on it (see Appendix B.1 for prompt). After that, at \(t=2\), the _Teacher_ provides instruction \(_{2}\) in a Socratic style, in which we design pedagogical demands such as not providing answers but rather following the flow of the step-by-step questions. It should be emphasized that here we set up a different response style for each student profile through examples (Appendix B.2). After the _Teacher_ generates \(_{2}\), the _Dean_ judges (e.g., "... doesn't meet the teaching criteria") and revises it (e.g., change "can you... 12 times 2?" to "how many... read today?"), focusing on 1) Whether it conforms to the Socratic style. 2) Whether it clearly points out the mistakes made by the _Student_. 3) Whether its language style resembles that of a real teacher (Appendix B.3), i.e., \(_{2} D(_{2})\). The revised response is then sent to the _Student_, and the next round of dialogue begins. Ultimately, if the _Teacher_ thinks that the teaching process has been completed, it will output an "[END]" token as the ending of its output, indicating to terminate the cycle. It is worth noting that although we focus on teaching mathematical problems in this paper, our DTS pipeline is general and can be extended to problems in other subjects (e.g., physics).

### Student Cognitive State System

To ensure that our dataset covers the real and diverse student status throughout the teaching process, it is necessary to simulate different student cognitive states within the _Student_ agent. However, a systematic and unified definition of these states has not been established in existing research . Some previous studies have concentrated on states that are specific to particular subjects such as math and English , while others abstractly define general states based on human cognitive science, such as concentration, working memory, and logical reasoning . Unfortunately, these definitions are either unadaptable to the teaching process or difficult to implement with LLMs. To address this issue, we review the Socratic teaching process from the perspective of students as follows. Initially, a student needs to grasp the meaning of the problem at hand. Then, he/she comprehends the instructions provided by the teachers and utilizes the computational ability and acquired knowledge to execute the instructions. Ultimately, this process fosters an interest in learning and helps to cultivate effective study results. Based on this idea, we summarize five dimensions of cognitive state:

(1) _Problem Understanding_: refers to the degree to which students understand the given problem.

(2) _Instruction Understanding_: refers to the degree to which students understand and carry out the teacher's instructions. A student in a good state should easily accomplish these instructions.

(3) _Calculation_: refers to the ability to derive mathematical expressions and numbers correctly.

(4) _Knowledge Mastery_: refers to the extent to which students have mastered knowledge.

(5) _Thirst for Learning_: refers to the students' desire or inclination to seek and acquire new information, ask questions, and explore possibilities.

Basically, we can define five types of students who perform poorly on one of the above dimensions. Moreover, we add a sixth type of student who excels at all dimensions.

### Teaching Ability Enhancement

The multi-round dialogues constructed by DTS pipeline (denoted as \(Dia_{M}\)) ensure a model to grasp the fundamental Socratic teaching paradigm. However, in \(Dia_{M}\), the _Student_ responds only once to each instruction given by the _Teacher_ and tends to choose simpler student portraits, leading to a lack of simulation for long-tail student responses (as discussed in Appendix D). In this section, to further enhance the diversity and robustness of our dataset, we construct more _Student-Teacher_ single-round dialogues \(Dia_{S}\) through data augmentation on \(Dia_{M}\), improving four important teaching abilities.

Specifically, in real teaching processes, students' responses can be classified as follows. First, from the macro perspective, the responses can be divided into _"Irrelevant"_ and _"Relevant"_. _"Relevant"_ refers to responses directly related to the problem or instruction, while _"Irrelevant"_ means that the responses are unrelated to the instructional content, such as asking "How's the weather today?" in Figure 2. Second, within the _"Relevant"_ category, it can be further divided into _"Questioning"_ and _"Replying"_, which refers to students asking questions to the teacher and answering the teacher's questions, respectively. Third, _"Replying"_ can be further classified as _"Incorrect-reply"_ and _"Correct-reply"_ based on whether the students' responses to the teacher's question are correct or not. Along this line, students' responses include four categories: _"Irrelevant"_, _"Questioning"_, _"Incorrect-reply"_, and _"Correct-reply"_. On this basis, there are four key teaching abilities that need targeted enhancement.

First, for _"Irrelevant"_ responses, we expect a teacher to recognize them and redirect the conversation towards teaching, such as responding "This question is unrelated to... Let's focus on the problem first...". To achieve this, we collect \(200\) genuine student inquiries from MOOCs that are unrelated to teaching and then construct \(2,000\) single-round _Student-Teacher_ dialogues by randomly inserting them into \(Dia_{M}\) and asking _Teacher_ to refuse answering (please refer to Appendix C for details).

Second, _"Questioning"_ corresponds to the most crucial teaching ability, that is, a teacher should provide students with accurate explanations. Regarding it, we randomly sample \(2,000\)_Teacher-Student_ conversation \((_{i},_{i})\) from \(Dia_{M}\) and use _Student_ agent to ask three more questions \(_{1}^{1},_{2}^{2},_{3}^{3}\) for \(_{i}\) (see Appendix C.1 for prompt). Then, we ask the _Teacher_ agent to provide \(_{i+1}^{1},_{i+1}^{j},_{i+1}^{3}\), ultimately forming \(6,000\) single-round _Student-Teacher_\(\{(_{i}^{j},_{i+1}^{j})|j=1,2,3\}\) dialogues.

Third, a teacher should accurately identify students' _"Incorrect-reply"_ and point out the idea of correction. To achieve this, we similarly sample \(2,000\)_Teacher-Student_ conversation from \(Dia_{M}\) and employ rules and generation techniques to rewrite the _Student_'s responses into five wrong answers. Then, we explicitly prompt the _Teacher_ to identify the errors and provide a response, obtaining another \(10\)K instances of _Student-Teacher_ dialogues (please refer to Appendix C for details).

Finally, to enable teachers to identify different expressions of the same _"Correct-reply"_ for enhancing robustness, we take the same \(2,000\) single-round _"Teacher-Student"_ dialogues used for _"Incorrect-reply"_ and create two correct responses with _Student_ (see Appendix C.2 for prompt). Subsequently, we collect _Teacher_'s replies and obtain another \(4,000\) single-round _"Student-Teacher"_ dialogues.

### Dataset Overview

In summary, our _SocraTeach_ consists of \(35\)K multi-round dialogues \(Dia_{M}\) constructed by _"Dean-Teacher-Student"_ pipeline in Sections 3.2 and \(22\)K single-round dialogues \(Dia_{S}\) through data augmentation in Section 3.4. The average number of rounds in \(Dia_{M}\) is \(5.28\), resulting in a total of \(208\)K single-round dialogue examples. More statistics of _SocraTeach_ are summarized in Appendix D.

In comparison to existing teaching dialogue datasets [17; 40; 41; 46; 49], our _SocraTeach_ first addresses the deficiency of LLMs inadequately simulating teachers  by introducing the role of "_Dean_" for supervision and correction. Secondly, to the best of our knowledge, _SocraTeach_ is the first publicly available dataset designed for Socratic teaching, which specifically enhances four key teaching abilities of _Teacher_. Thirdly, while existing datasets simulate different students by setting their demographic backgrounds (e.g., grade) or specific error types, _SocraTeach_ models six cognitive states of _Student_ during the teaching process based on pedagogical experience, which covers a wider range of authentic teaching scenarios and enables LLMs to possess better teaching capabilities. Lastly, _SocraTeach_ is a fully automatically generated large-scale dataset containing \(35\)multi-round dialogues and \(22\)K single-round dialogues, significantly surpassing the existing datasets that rely on real human students/teachers (e.g., the latest MATHDIAL  contains \(3\)K dialogues).

## 4 Fine-tune _SocraticLM_

Based on _SocraTeach_, we can fine-tune a SOTA LLM (e.g., \(\)-\(6\)) by splitting each dialogue \(\{(_{1},_{1}),(_{2},_{2}),...\}\) into multiple rounds, using the preceding context of each round \(\{(_{1},_{1}),...,(_{i},_{i})\}\) as input and the _Teacher_'s response \(_{i+1}\) as output. However, it may lead to catastrophic forgetting and reduce the problem-solving ability that the model already has because these dialogues may differ from the data used for pre-training [21; 30; 38]. Specifically, we observe a decrease of 31.2%/9.7% in the accuracy of _SocraticLM_ on the GSM8K/MAWPS dataset in Section 6.2. Therefore, to enhance _SocraticLM_'s teaching ability without compromising its fundamental problem-solving capability, we explore the following three training strategies:

**Separate Training.** To maintain problem-solving ability, one direct way is to mix the dialogue and problem-solving data for training. However, we discover that it does not yield satisfactory results as shown in Section 6.2. Therefore, we adopt a separate training approach wherein we first fine-tune _SocraticLM_ using dialogue data and then fine-tune it on a small amount of problem-solving data randomly sampled from GSM8K and MAWPS. Our experiments revealed that optimal performance is achieved when the ratio \(\) of problem-solving data to the dialogue data is approximately \(\).

**Instruction Tuning.** Inspired by , we employ different instructions for the dialogue data and problem-solving data, with the templates presented in Appendix E. It is worth noting that, unlike the prompt for _Teacher_ in Section 3.2, here our instruction for dialogues does not require the model to follow the step-by-step guiding questions in Section 3.1. This is because providing such information in training may lead the model to take shortcuts, that is, to simplify the teaching process into information extraction from the prompt, without truly mastering the pedagogical ability.

**Mixed Prompt Setting.** Training with mixed prompt settings for the same task is an important method for improving LLMs' reasoning abilities [7; 55]. To this end, in addition to the original zero-shot problem-solving data of GSM8K and MAWPS, we also construct their one-shot version for training, which consists of approximately \(\) of the amount of zero-shot data.

## 5 Our Socratic Teaching Evaluation System

Since there is no standard answer for the teaching process, previous metrics that calculate the similarity between model-generated responses and annotated responses (e.g., BLEU , Rouge ) may not fully assess the teaching quality of LLMs. To address this issue, in this paper, we contribute an evaluation system encompassing five pedagogical dimensions for Socratte style and teaching abilities, which to the best of our knowledge, is the first comprehensive exploration in this field.

(1) _Overall Quality (Overall)_: This metric is a holistic and subjective evaluation of teaching quality, requiring that the instruction satisfies Socratte style and enhances students' experience.

For _Overall Quality_, we randomly select \(1,000\) single-round "_Student-Teacher_" conversations from \(Dia_{M}\) and recruit \(10\) well-educated annotators to blindly rank the _Teacher_ response pairs provided by each model and GPT4 in the same context (please refer to Appendix F for details). The _Overall Quality_ is estimated by a normalized win rate difference \((1+)(0,1)\) (GPT4's own result is \(0.5\)). To ensure agreement of quality judge among humans, we also randomly construct 100 _Teacher_ response pairs of _SocraticLM_ and GPT4 and ask all annotators to judge which one is better. The Kappa score is \(0.70\), indicating good agreement among human annotators.

For the four Socratte teaching abilities we elaborated in Section 3.4, we propose metrics (2)-(5). For each of them, we randomly select \(100\) corresponding single-round dialogues from \(Dia_{S}\) for testing.

(2) _Incorrect Answer Recognition Accuracy (IARA)_: This dimension focuses on whether the teacher can accurately identify students' _"Incorrect-reply"_. For example, in Figure 2, if a student provides an incorrect answer (e.g., "\(14\)"), a competent teacher should be able to recognize and point it out. This process is objective and can be considered as a binary classification task.

(3) _Correct Answer Recognition Accuracy (CARA)_: In contrast to error recognition, this dimension focuses on whether the model can accurately identify students _"Correct-reply"_. Neglecting this metric may mislead the LLMs to consider any answer provided by the student as incorrect.

(4) _Successful Explanation Rate (SER)_: This dimension focuses on whether the model can provide students with satisfactory explanations for their _"Questioning"_. This metric is subjective, but can be converted to binary classification based on students' real experience.

(5) _Successful Rejection Rate (SRR)_: This metric is designed for the case where a teacher should refuse to answer students _"Irrelevant"_ questions and redirect them back to the instructional content. Based on whether the model refuses to answer the question, it is also calculated as binary classification.

Compared with existing works in evaluating LLMs for education, our evaluation system offers three main advantages. First, it provides a more comprehensive and adequate assessment. While previous works either rely on similarity metrics (e.g., BLEU ) or limited-scale manual evaluations (e.g., issuing questionnaires ), our system assesses overall teaching quality along with four key teaching abilities, which provides a more systematic organization of evaluation. Second, it enables better comparability across LLMs. Limited by the fact that a student can only interact with one LLM at a time, traditional human evaluations [5; 24] are difficult to compare the effectiveness of multiple models. In contrast, our system uses the same teaching dialogues shared across different models as test samples, allowing for a fair comparison of different LLMs simultaneously. Third, our system is more extensive and reliable benefiting from our larger _SocraTeach_ dataset, while recent studies rely on smaller datasets, such as the latest one  with only around 600 testing dialogues.

## 6 Experiments

In this section, we verify the effectiveness of our _SocraticLM_ by taking ChatGPT, GPT4, Vicuna-\(7\)b , \(\)-\(7\)b, \(\)-\(13\)b, \(\)-\(8\)b , \(\)-\(6\)b , and EduChat-\(32\)b  as baselines. The implementation details are described in Appendix G. Especially, for fair comparison, for the problems taught in the testing dialogues, we omit all of their dialogues in training. In addition to our proposed evaluation system in Section 5, we also invite human annotators to give a real teacher response for each testing dialogue, taking it as a standard to calculate the BLEU and Rouge.

### Main Results

Table 1 summarizes the results for all models. First, our _SocraticLM_, which contains 6 billion parameters, demonstrates a significant improvement in all Socratic teaching abilities. Notably, it outperforms GPT4 by 12% on _Overall_, 6% on _IARA_, 7% on _CARA_, 9% on _SER_, and 23% on _SRR_, as well as over 12% in mirroring the responses of human teachers as measured by BLEU and Rouge. In Appendix H, we present and analyze examples of their outputs. Second, our _SocraticLM_ demonstrates a significant improvement in _SER_. This indicates that the judgement and correction of our proposed _Dean_ agent can critically boost the explanatory capabilities of large models when they function

   & _Overall_ & _IARA_ & _CARA_ & _SER_ & _SRR_ & _BLEU-4_ & _Rouge-1_ & _Rouge-2_ & _Rouge-1_ \\  ChatGPT & 0.29 & 0.42 & 0.93 & 0.62 & 0.19 & 22.8 & 34.3 & 14.4 & 21.3 \\ GPT4 & 0.50 & 0.76 & 0.91 & 0.65 & 0.55 & 36.2 & 42.9 & 19.4 & 32.4 \\ Vicuna-7b & 0.15 & 0.16 & 0.77 & 0.16 & 0.39 & 22.8 & 34.9 & 14.4 & 22.8 \\ Llama2-7b & 0.27 & 0.15 & 0.86 & 0.32 & 0.13 & 28.3 & 35.7 & 14.0 & 24.1 \\ Llama2-13b & 0.25 & 0.23 & 0.87 & 0.30 & 0.08 & 27.9 & 36.4 & 14.3 & 23.6 \\ Llama3-8b & 0.33 & 0.75 & 0.77 & 0.39 & 0.52 & 27.4 & 33.0 & 10.9 & 22.0 \\ ChatGLM3-6b & 0.11 & 0.18 & 0.87 & 0.46 & 0.07 & 17.1 & 26.2 & 9.1 & 15.7 \\ EduChat-\(32\)b & 0.37 & 0.48 & 0.77 & 0.40 & 0.03 & 27.8 & 38.4 & 17.9 & 29.2 \\  _SocraticLM (ours)_ & **0.62** & **0.83** & **0.98** & **0.74** & **0.78** & **48.6** & **56.2** & **33.7** & **47.5** \\ w/o _Diags_ & 0.54 & 0.27 & 0.89 & 0.67 & 0.34 & 42.6 & 52.2 & 32.3 & 44.4 \\ w/o _Irrelevant_ & 0.57 & 0.79 & 0.87 & 0.69 & 0.43 & 45.7 & 52.3 & 29.8 & 44.2 \\ w/o _Questioning_ & 0.58 & 0.74 & 0.92 & 0.53 & 0.83 & 47.8 & 55.0 & 31.9 & 45.9 \\ w/o _Incorrect_ & 0.51 & 0.33 & 0.93 & 0.68 & 0.65 & 41.8 & 48.2 & 30.4 & 38.9 \\ w/o _Correct_ & 0.60 & 0.70 & 0.58 & 0.70 & 0.76 & 47.4 & 55.1 & 32.8 & 46.6 \\  

Table 1: Teaching performances. For all metrics, the higher value denotes the better performance. The best methods are highlighted in bold. The runner-up baselines are represented by underline.

as teachers. Third, in Figure 3, we evaluate the four teaching abilities on problems with different numbers of step-by-step guiding questions, which can reflect the difficulty of the problems. It is evident that our _SocraticLM_ consistently outperforms GPT4 across all difficulty levels.

### Ablation Study

**Importance of Teaching Ability Enhancement.** We explore the importance of single-round teaching dialogues \(Dia_{S}\) built for the four key teaching abilities in Section 3.4. From Table 1, we first observe a significant decline (e.g., _Overall Quality_ by 8%) when these dialogues are removed (i.e., "w/o \(Dia_{S}\)"). This illustrates the necessity of our teaching ability enhancement and confirms that our proposed four teaching abilities are effective in meeting the real demands of Socratic teaching. Second, the _IARA_ and _SRR_ metrics decrease the most, indicating the greatest disparity between the current LLM-based teaching and human teaching may lie in the response to students' incorrect answers and irrelevant questions. Third, each time a type of single-round dialogue data is eliminated, all teaching abilities will show a decline, which indicates that there is a coupling effect among different teaching abilities. Especially, the _CARA_ metric in the absence of dialogues for students' _"Correct-reply"_ (i.e., "w/o _Correct_") is even lower than when all single-round data is removed ("w/o \(Dia_{S}\)"). We hold the reason may be that in this case, _SocraticLM_ was still fine-tuned on dialogues corresponding to students' _"Incorrect-reply"_. This causes the model to develop a stronger tendency to perceive a student's reply as incorrect. This phenomenon further indicates that it is necessary to balance different types of single-round dialogues to avoid overfitting on specific instructional patterns.

**Importance of Ability-balancing Strategies.** Here we discard the problem-solving data and the three ability-balancing strategies in Section 4 in training to investigate their influence. From Table 2, fine-tuning without problem-solving data ("w/o _Problem_") will result in a 31.2%/9.7% lower accuracy on GSM8K/MAWPS compared with \(\)-\(6\). This could be attributed to the notable differences between teaching dialogues and data used for LLM pre-training, causing a dramatic disturbance in the parameters. Besides, all three training strategies are effective. Among them, _Separate Training/Instruction Tuning_ has the greatest influence on problem-solving/Socratic teaching respectively. _Mixed Prompt Setting_ might have already been employed in the LLM pre-training, hence exhibiting less noticeable improvements. Moreover, it is worth noting that _SocraticLM_ achieves higher accuracy on MAWPS than \(\)-\(6\). We speculate the reason is that, through fine-tuning on our _SocraTeach_ dataset, _SocraticLM_ indeed learns to address multiple student questions about various aspects of a single problem (e.g., asking about each reasoning step and the knowledge involved). This process allows _SocraticLM_ to develop a deeper understanding of the problem-solving process, which in turn can improve its problem-solving accuracy.

### Influence of Data Scale

Data scale is crucial for both the efficiency and effectiveness of training large language models. In order to investigate this issue, in this section, we vary the amount of multi-round dialogues in _SocraTeach_ dataset and the ratio \(\) between multi-round dialogues and problem-solving data.

    & _Overall_ & \(ACC_{G}\) & \(ACC_{M}\) \\  ChatGLM3\(\) & 0.11 & 0.624 & 0.798 \\  _SocraticLM_ & 0.62 & 0.606 & 0.814 \\ w/o _Problem_ & 0.58 & 0.312 & 0.701 \\ w/o _Separate_ & 0.54 & 0.159 & 0.646 \\ w/o _Instruction_ & 0.02 & 0.320 & 0.625 \\ w/o _Mixed-Prompt_ & 0.56 & 0.605 & 0.804 \\   

Table 2: Performance without problem-solving data and three ability-balancing training strategies in Section 4. \(ACC_{G}\), \(ACC_{M}\) represent the accuracy on GSM8K and MAWPS, respectively.

Figure 3: Performances on problems with different number of step-by-step guiding questions.

**Scale of Multi-round Dialogues.** To study the impact of different data scale, we randomly select 25%, 50%, 75% multi-round dialogues from _SocraTeach_ and expand it to 125% dialogues by running DTS pipeline more times to train _SocraTeLM_. The results in Figure 4 indicate that (i) Our data is not only effective but also impactful at different scales, which can significantly enhance the teaching capability of LLMs. (ii) As the volume of data increases, we observe a corresponding increase in the teaching ability. This correlation highlights the importance of data quantity in model performance. Specifically, it is noteworthy that a minimum of 75% (\( 26\)K) dialogues is required for surpassing the _Overall Quality_ of GPT4. (iii) As the volume of data surpasses the \(35\)K threshold, it tends to approach a saturation point where further increases in data volume yield smaller incremental benefits to the model's capability. Specifically, at the 125% data scale, the _IARA_ metric shows a decline, indicating that the root cause of this saturation is a decrease in the model's ability to identify incorrect answers (the decline in _Overall Quality_ is a subsequent result). This may be because, with the increase in multi-round dialogue data, the proportion of single-round dialogue data for _"Incorrect reply"_ decreases. When multi-round data scale exceeds 125%, this proportion may fall below a certain threshold, which results in diminishing effectiveness.

**Scale of Problem-solving Data.** Figure 5 shows the performance changes of _SocraticLM_ as we adjust the ratio \(\) between problem-solving data and dialogue data. The trend indicates that having too few or too much problem-solving data does not lead to satisfactory problem-solving ability. Instead, a balance needs to be struck with the teaching dialogue data. In fact, an excessive introduction of problem-solving data may even result in 1.9% decrease in accuracy on GSM8K. This could be attributed to that the parameters corresponding to _SocraticLM_ problem-solving ability might undergo disturbances after initial fine-tuning using the teaching dialogues. When retraining with problem-solving data, it requires to re-strike a delicate balance between underfitting and overfitting of this ability.

## 7 Conclusion

In this paper, we introduced _SocraticLM_, a LLM designed to facilitate Socratic "Thought-Provoking" personalized teaching. To build _SocraticLM_, we proposed a "_Dean-Teacher-Student_" pipeline to construct _SocraTeach_ dataset, which simulated six student cognitive states and strengthened four crucial teaching abilities. Besides, we developed a comprehensive teaching ability evaluation system for LLMs. Experiments demonstrated that _SocraticLM_ significantly outperforms current LLMs such as GPT4 and validated the necessity of each component within the _SocraTeach_ dataset. We discuss more cases, the broader impacts, limitations, and future work in Appendix H, I, and J.