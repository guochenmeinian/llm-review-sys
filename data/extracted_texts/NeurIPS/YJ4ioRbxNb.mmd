# A benchmark of categorical encoders

for binary classification

Federico Matteucci

Karlsruhe Institute of Technology

vadim.arzamasov, klemens.boehm@kit.edu

Vadim Arzamasov

Karlsruhe Institute of Technology

vadim.arzamasov, klemens.boehm@kit.edu

Klemens Bohm

Karlsruhe Institute of Technology

vadim.arzamasov, klemens.boehm@kit.edu

###### Abstract

Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of **1.** encoders, **2.** experimental factors, and **3.** datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 48 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks. Our code is available at https://github.com/DrCohomology/EncoderBenchmarking.

## 1 Introduction

Learning from categorical data poses additional challenges compared to numerical data, due to a lack of inherent structure such as order, distance, or kernel. The conventional solution is to transform categorical attributes into a numerical form, i.e., _encode_ them, before feeding them to a downstream Machine Learning (ML) model. Various encoders have been proposed, followed by several benchmark studies. However, their combined results remain inconclusive, as we now describe.

Many factors impact the generalizability  of a benchmark of encoders, including: **1.** the compared encoders, **2.** the number of datasets, **3.** the quality metrics, **4.** the ML models used, and **5.** the tuning strategy. We also hypothesize that **6.** the _aggregation strategy_ used to summarize the results of multiple experiments may affect the conclusions of a study. Existing encoder benchmarks, reviewed in Section 2, only partially control for these factors. First, none of these studies uses more than 15 datasets of a given type (regression or classification). Second, despite these studies collectively covering a substantial number of encoders, they often focus on specific encoder families, resulting in comparison gaps between the best encoders. For instance, the best-performing encoders from  (Cross-Validated GLMM) and  (Mean-Target) have not been studied together yet. Third, the results of existing studies are often not comparable due to variations in the selected quality metrics. For instance,  measures quality with ROC AUC,  with average precision, and  with accuracy. Fourth, existing studies tune ML models in different ways, yielding incompatible evaluations. For instance,  do not tune, while  tune but do not specify if they tune the ML model on encoded data or if they tune the entire ML pipeline. Last, no benchmark study of categorical encoders explores the impact of aggregation strategies, which is substantial according to our experiments. For instance,  ranks the encoders by average ranking across all datasets, while  computes the median ranking with Kemeny-Young aggregation .

This study offers a taxonomy and a comprehensive experimental comparison of encoders for binary classification, taking into account the factors just mentioned. In particular, we consider: **1.** 32 encoder configurations, including all of the best-performing ones from the literature and three novel encoders; **2.** 50 datasets for binary classification; **3.** four quality metrics; **4.** five widely used ML models; **5.** three tuning strategies; **6.** 10 aggregation strategies gathered from existing categorical encoder benchmarks and from benchmarking methodology studies [27; 9]. This allows us to provide novel insights into the sensitivity of experimental results to experimental factors. In particular, we demonstrate how replicability  may not be ensured even for studies conducted on up to 25 datasets. For those combinations of experimental factors that show reproducible results, we isolate and recommended the best encoders.

Paper outline: Section 2 reviews existing works, Section 3 presents a taxonomy of encoder families, Section 4 describes the experimental setup, and Section 5 features the results.

## 2 Related work

**Benchmarks of encoders.** We focus on binary classification tasks, as they offer a wider range of compatible encoders; indeed, we could conduct a deeper replicability analysis while maintaining the computation feasible. Table 1 summarizes the related work. The other benchmarks often consider few datasets and either do not tune the ML model or do not describe the tuning procedure. This limits their applicability and generalizability. Additionally, there are substantial differences in the experimental settings across articles, including the encoders considered, quality metrics employed, and aggregation strategies used to interpret results. Hence, the comparability of these findings is limited. For instance,  recommends a data-constraining encoder,  both data-constraining and contrast encoders, [5; 4] similarity encoders,  an identifier encoder, and  a simple target encoder. Other benchmarks of encoders are , which focuses on regression tasks and faces similar issues, and [30; 14; 20], that use only a single dataset.

**Analysis of benchmarks.** When designing our benchmark, we adhered to the best practices discussed in the literature on benchmark design and analysis. In particular,  studies how choices of experimental factors impact the experimental results and advocates for benchmarks that consider a large variety of factors. Similarly,  suggests guidelines to mitigate the inconsistencies in the choices of data and evaluation metric. Finally,  proposes a methodology to account for variance in the design choices (randomization of sources of variation) and post-processing of the experimental results (significant and meaningful improvements).

    & Ours &  &  &  &  &  &  \\  \# Binary classification datasets & 50 & 10 & 5 & 3 & 2 & 2 & 6 \\  \# ML models & 5 & 5 & 1 & 4 & 2 & 1 & 5 \\   & Identifier & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\  & Frequency-based & ✓ & ✓ & & & & & ✓ \\  & Contrast & ✓ & & & & & & ✓ \\  & Similarity & ✓ & & ✓ & & ✓ & & \\  & Simple target & ✓ & ✓ & & ✓ & & & ✓ \\  & Binning & ✓ & ✓ & & & & ✓ & ✓ \\  & Smoothing & ✓ & ✓ & & & ✓ & & ✓ \\  & Data-constraining & ✓ & ✓ & & & & & ✓ \\   & Precision-recall based & ✓ & ✓ & ✓ & ✓ & ✓ & & \\ Quality metric & Balanced accuracy & ✓ & ✓ & & ✓ & & & \\  & Accuracy & ✓ & & & ✓ & ✓ & ✓ \\   & Full pipeline tuning & ✓ & & &? & ✓ & ✓ \\ Tuning strategy & Model tuning & ✓ & & ✓ & & ✓ & & ✓ \\  & No tuning & ✓ & ✓ & & & & ✓ \\   & Heuristic & ✓ & & & & ✓ & \\ Aggregation strategy & Friedman-Nemenyi & ✓ & & ✓ & & & ✓ & \\  & Kemeny-Young & ✓ & ✓ & & & & & \\   

Table 1: Related work on categorical encoders for binary classification.

## 3 Taxonomy of encoders

This section presents the essential terminology and discusses the considered encoders and their corresponding families. Appendix 7.1 provides formal and detailed descriptions of the encoders.

### Notation and terminology

Consider a tabular dataset with target \(y\) taking values in \(\{0,1\}\), and let \(\) be one of its attributes (columns). \(\) is _categorical_ if it represents qualitative properties and takes values in a finite domain \(_{A}\). Each \(_{A}\) is a _level_ of \(\). Categorical attributes do not support arithmetic operations like addition or multiplication, and their comparison is not based on arithmetic relations. An _encoder_\(E\) replaces a categorical attribute \(\) with a set of numerical attributes, \(E()\). We write \(E(_{A})\) to indicate the domain of \(E()\). Encoders may encode different levels in \(\) in the same way, or encode in different ways different occurrences in the dataset of the same level. Encoders are either _supervised_ or _unsupervised_: Supervised encoders require a target column, while unsupervised encoders solely rely on \(\). In what follows, \(\) always denotes the categorical attribute to be encoded.

### Unsupervised encoders

**Identifier encoders** assign a unique vector identifier to each level. The most recognized encoder is One-Hot (OH), the default encoder in most machine learning pipelines [11; 15]. One-Hot is both space-inefficient and ineffective [28; 4; 5]. Alternatives include Ordinal (Ord), which assigns a unique consecutive identifier to each level, and Binary (Bin), which splits the base-2 representation of Ord(\(\)) into its digits.

**Frequency-based encoders** replace levels with some function of their frequency in the dataset. We use Count, which relies on absolute frequencies .

**Contrast encoders** encode levels into (\(L-1\))-dimensional vectors so that the encodings of all levels sum up to \((0,,0)\). A constant intercept term, \(1\), is usually appended to the encoding of each level. Contrast encoders encode levels such that their coefficients represent the level's effect contrasted against a reference value. A common example is Sum, which contrasts against the target's average value.

**Similarity encoders** treat \(_{A}\) as strings and map them into a numeric space taking their similarity into account [5; 4]. These encoders are particularly useful for handling "dirty" categorical datasets that may contain typos and redundancies. One example is Min-Hash (MH), which decomposes each level into a set of \(n\)-grams, sequences of \(n\) consecutive letters, and encodes to preserve the Jaccard similarity of the decompositions.

### Supervised encoders

**Simple target encoders** encode levels with a function of the target. Prime examples are Mean-Target (MT) , which encodes with the conditional average \(y\) given \(\), and Weight of Evidence (WoE) , which encodes with the logit of MT(\(\)). As Mean-Target can lead to severe overfitting [28; 31], it may benefit from regularization. The following families of encoders are regularization for Mean-Target.

We propose **Binning encoders**, that regularize MT by partitioning either \(_{A}\) or MT(\(_{A}\)) into bins. Pre-Binned MT (PBMT) partitions \(_{A}\) to maximize the number of bins such that each bin's relative frequency exceeds a specified threshold, then encodes the binned attribute with MT. Discretized MT (DMT) partitions MT(\(_{A}\)) into intervals of equal length, then encodes each level with the lower bound of the interval in which its MT encoding falls.

**Smoothing encoders** blend MT(\(_{A}\)) with the overall average target. Notable examples are Mean-Estimate (ME) , which uses a weighted average of the two, and Generalized Linear Mixed Model encoder (GLMM) , which encodes with the coefficients of a generalized linear mixed model fitted on the data.

**Data-constraining encoders** regularize MT(\(\)) by restricting the amount of data used to encode each occurrence of a level in the dataset. CatBoost (CB)  first randomly permutes the dataset's rows, then maps each occurrence of a level \(\) to the average target of its previous occurrences. Cross-Validated MT (CVMT)  splits the dataset into folds of equal size, then encodes each fold with anMT trained on the other folds. We propose the BlowUp variant of CVMT, BUMT, which trains an MT on each fold and uses them to encode the whole dataset. Related variants are Cross-Validated GLMM (CVGLMM)  and its BlowUp version (BUGLMM).

## 4 Experimental design

As there is no intrinsic measure of an encoder's quality, we proxy the latter with the quality of an ML model trained on encoded data. This procedure is in line with the current literature on the topic, discussed in Section 2. Each experiment thus consists of the following steps. First, we fix a combination of factors: a dataset, an ML model, a quality metric, and a tuning strategy. Then, we partition the dataset using a 5-fold stratified cross-validation and pre-process the training folds by:

* imputing missing values with median for numerical and mode for categorical attributes;
* scaling the numerical attributes;
* encoding the categorical attributes.

If tuning is to be applied, we fine-tune the pipeline with nested cross-validation and output the average performance over the outer test folds. We used standard scikit-learn  procedures for scaling and missing values imputation.

We conducted experiments using Python 3.8 on an AMD EPYX 7551 machine with 32 cores and 128 GB RAM. We limit each evaluation to 100 minutes to handle the extensive workload. As described in Appendix 7.3.1, out of the 64000 cross-validated evaluations, 61812 finished on time without throwing errors. For the sensitivity, replicability, and encoder comparison analysis, we ignored the missing evaluations. We did so **1.** since there is no clearly superior imputation method, and **2.** to avoid introducing unnecessary variability in the analysis. Our preliminary experiments confirm that imputing the small number of missing evaluations does not significantly impact our analysis.

In what follows, we describe the datasets, ML models, quality metrics, and tuning strategies we use in our experiments. Then, we outline the different aggregation strategies. Appendix 7.2 provides further details about datasets and aggregation strategies.

### Encoders

We used the category_encoders1 implementations of Bin, CB, Count, Ord, OH, Sum, and WoE. We sourced MH from the authors' implementation [4; 5].2 We implemented DMT, GLMM, ME, MT, PBMT, CVMT, BUMT, CVGLMM, and BUGLMM. We also added a baseline encoder, Drop, which encodes every level with \(1\). For DMT, we experimented with the number of bins: \(\{2,5,10\}\), for ME, with the regularization strength: \(\{0.1,1,10\}\), for PBMT, with the minimum frequency: \(\{0.001,0.01,0.1\}\), and for cross-validated encoders, such as CVMT, with the number of folds: \(\{2,5,10\}\). We display hyperparameter values with subscripts, e.g., CV\({}_{2}\)MT.

### Datasets

We used binary classification datasets. This allows us to conduct in-depth analysis using the same ML models and quality metrics. Additionally, certain supervised encoders, e.g., WoE, are specifically designed for binary classification tasks. We chose 50 datasets with categorical attributes from OpenML , including the suitable ones from the related work.

### ML models

We experimented with diverse ML models that process data in different ways: decision trees (DT) and boosted trees (LGBM) exploit orderings, support vector machines (SVM) use kernels, k-nearest neighbors (k-NN) relies on distances, and logistic regression (LogReg) is a "pseudo-linear" model. The LGBM implementation we used is from the LightGBM module,3 while the other models' implementations are from scikit-learn. Table 2 compares our model choices with related work. Weexcluded neural models due to their inferior performance on tabular data  and the absence of a recommended architecture. We also did not use Naive Bayes due to its lack of popularity.

### Quality metrics and tuning strategies

We assessed an encoder's quality by evaluating an ML model trained on the encoded data. We use four quality metrics: balanced accuracy (BAcc), F1-score (F1), accuracy (Acc), and Area Under the ROC Curve (AUC). We compared three tuning strategies:

* _no tuning_;
* _model tuning_: the entire training set is pre-processed before tuning the model;
* _full tuning_: the entire pipeline is tuned on the training set, with each training fold of the nested cross-validation pre-processed independently.

We used Bayesian search from scikit-optimize4 for full tuning, and for model tuning grid search from scikit-learn. Table 4 summarizes the tuning search space for different ML models. To mitigate excessive runtime, we chose not to tune certain ML models and limited the dataset selection to the smallest 30 for full tuning, as Table 3 illustrates.

### Aggregating into a consensus ranking

A common practice for summarizing and interpreting the results of benchmark experiments is to aggregate them into a _consensus ranking_ of _alternatives_ (encoders in our case) [10; 27; 15]. To obtain a dataset-independent ranking of encoders, we aggregate the results across different datasets while keeping all other factors fixed. We now present well-known aggregation strategies used in benchmarks.

**Heuristics** rank alternatives based on an aggregate score. Common aggregation heuristics include mean rank (R-M) , median rank (R-Md), mean quality (Q-M), median quality (Q-Md), rescaled

    & Hyperparameter & Interval & Grid \\   & max\_depth & \([2,,5]\) & \(\{2,5,None\}\) \\  & n\_neighbors & \([2,,10]\) & \(\{2,5,10\}\) \\ LogReg & C & \([0.2,5]\) & \(\{0,1,10\}\) \\  & C & \([0.1,2]\) & \\  & gamma & \([0.1,100]\) & \\   

Table 4: Tuning search space.

    & & Ours &  &  &  &  &  &  \\   & Tree ensembles & ✓ & ✓ & ✓ & ✓ & ✓ & & ✓ \\  & Linear & ✓ & ✓ & ✓ & & & ✓ \\  & SVM & ✓ & ✓ & & ✓ & & ✓ \\ Model family & k-NN & ✓ & ✓ & & & & \\  & DT & ✓ & ✓ & & & ✓ & \\  & Neural & & & & ✓ & & ✓ \\  & Naive Bayes & & & & & & ✓ \\   

Table 2: ML models used in related studies.

    & Models & \# Datasets \\  No tuning & DT, k-NN, LogReg, SVM, LGBM & 50 \\ Model tuning & DT, k-NN, LogReg & 50 \\ Full tuning & DT, k-NN, LogReg, SVM & 30 \\   

Table 3: Factors for different tuning strategies.

mean quality [36; 15] (Q-RM), the number of times the alternative was ranked the best (R-B) or the worst (R-W) , the number of times the alternative's quality is better than the best quality multiplied by a threshold \( 1\) (Q-Th\({}_{}\)).

**Friedman-Nemenyi tests** (R-Nem\({}_{}\)). First, one ranks alternatives separately for each dataset and then applies a Friedman test to reject the hypothesis that all encoders have the same average rank. If the hypothesis is rejected, pairwise Nemenyi post-hoc tests are conducted to compare pairs of alternatives. Finally, one uses the results of these post-hoc tests to construct the consensus ranking. This aggregation strategy requires the user to choose a p-value.

**Kemeny-Young aggregation**[21; 47] (R-Kem) first ranks alternatives separately for each dataset. Then, it determines the consensus ranking that minimizes the sum of distances to the datasets' rankings. We adopt the approach described in , with a distance measure that accomodates ties and missing values in the rankings. We then formulate the optimization problem as a mixed integer linear problem and solve it using a GUROBI solver with academic license.5 Kemeny-Young aggregation is much slower than the other aggregation strategies, taking minutes for each aggregation.

## 5 Results

This section summarizes the main results of our study. Appendix 7.3 further discusses the missing evaluations, run time, replicability, the ranks of the encoders and studies the effect of tuning on pipeline quality.

### Sensitivity analysis

The relative performance of encoders, i.e., the ranking, can depend on the pick of ML model, quality metric, and tuning strategy. More, the choice of an aggregation strategy impacts the consensus ranking. To quantify the influence of these choices, we calculate the similarity between rankings using the Jaccard index \(J\) for the sets of best encoders and the Spearman correlation coefficient \(\). Intuitively, \(J\) measures if two experiments with different factor combinations agree on the best encoders, while \(\) takes the entire ranking into account. For both measures, values close to 1 indicate high agreement and low sensitivity. Conversely, values near 0 (or, for \(\), negative) suggest low consistency and high sensitivity.

#### 5.1.1 Sensitivity to experimental factors

We evaluate the sensitivity of encoder rankings on individual datasets with respect to an experimental factor (ML model, quality metric, or tuning strategy) by varying the factor of interest and keeping the other factors fixed, then calculating the similarity between pairs of rankings. After that, we average the result across all combinations of the other factors. Figures 0(a), 0(b), and 0(c) show the resulting values, with Spearman's \(\) in the upper triangle and Jaccard index \(J\) in the lower triangle. For example, Spearman's \(\) between encoder rankings for DT and SVM, averaged across all datasets, tuning strategies, and quality metrics, is 0.3.

Our findings highlight the high sensitivity of results to experimental factors, for both the full rankings and the best encoders. They also explain why results from other studies are so inconsistent, as choosing different values for any factor will lead to different results.

#### 5.1.2 Sensitivity to aggregation strategy

To evaluate the impact of the aggregation strategy on the consensus ranking, we apply the same procedure as above to consensus rankings instead of rankings on individual datasets. Figure 0(a) presents the results with the notation from Section 4.5. For example, Spearman's \(\) between consensus rankings obtained with Q-M and Q-Md averaged across all ML models, tuning strategies, and quality metrics is 0.8.

While some aggregation strategies show strong similarities, different strategies yield very different consensus rankings in general. This is particularly evident for Jaccard index \(J\), indicating the high sensitivity of the best encoders to the rank aggregation strategy.

### Replicability

Replicability is defined as the property of a benchmark to produce consistent results from different data . This definition does not, however, provide a quantifiable notion of replicability. To overcome this, we made the following modeling decisions. First, we fix a factor combination: ML model, quality metric, tuning strategy, and aggregation strategy. We excluded the R-Nem and R-Kem aggregation strategies due to their slower run time. Second, we model the result of a benchmark on a dataset sample \(S\) with the consensus ranking aggregated across \(S\). Third, we quantify replicability as the similarity between consensus rankings averaged over all factor combinations and 100 pairs of equal-sized disjoint sets of datasets. As discussed in Section 5.1, we measure the similarity with \(\) and \(J\) to capture the similarity between both the rankings and the best encoders. We refer to them as \(\)-replicability and \(J\)-replicability, respectively.

Figure 2 shows the outcome for different tuning strategies, conditional on the ML model and the size of the dataset samples. We have studied additional factors in Appendix 7.3.3. The shaded areas represent a bootstrapped 95% confidence interval. Our findings show an upward trend of \(\)-replicability as the size of the dataset samples increases. This observation confirms that, in general, considering a larger number of datasets yields more reliable experimental outcomes. It is, however, important to note that this pattern does not always hold for \(J\)-replicability. This suggests that, for some models, the best encoders might vary significantly even with a relatively large number of datasets. To conclude, the replicability of our results strongly depends on the ML model, with logistic regression exhibiting the highest replicability and decision trees the lowest.

Figure 1: Sensitivity as the average similarity between rankings, measured with \(\) (upper triangle) and \(J\) (lower triangle), computed between individual rankings for varying: (a) ML model, (b) quality metric, (c) tuning strategy, and between consensus rankings for varying (d) aggregation strategy.

### Comparing encoders

Based on the outcome of Section 5.2, we now examine the ranks of encoders limited to decision trees, logistic regression, and all ML models.

Figure 2(a) shows the rank of encoders from the experiments with decision trees across all datasets, quality metrics, and tuning strategies. One-Hot is the best-performing encoder; however, Nemenyi tests at a significance level of 0.05 fail to reject that the average rank of One-Hot is the same as that of the other encoders.

Figure 2(b) features the encoder ranks for logistic regression, where four encoders, namely One-Hot, Sum, Binary, and Weight of Evidence, consistently achieve higher ranks compared to the others. Nemenyi tests confirm that this difference in ranks is significant. These results are in line with the ones from Section 5.2, which indicate low replicability of the results for decision trees and higher replicability for logistic regression.

Figure 2(c) presents the ranks of encoders across all datasets, ML models, quality metrics, and tuning strategies. Similarly to logistic regression, One-Hot, Sum, Binary, and Weight of Evidence consistently achieve significantly higher average ranks compared to the other encoders, again confirmed by Nemenyi tests. We recommend these four encoders as the preferred choices in practical applications. This conclusion contradicts other studies reporting a suboptimal performance of One-Hot [5; 28].

Our findings also reveal that Drop performs significantly worse than all other encoders, i.e., encoding categorical attributes generally yields better results than dropping them.

### Comparing to related work

In this section, we compare our results with the findings of other studies. To do so, we select subsets of our results that mimic the experimental settings in related work. In , CV\({}_{5}\)GLMM outperformed every competitor for boosted trees and k-NN, while GLMM was recommended for SVMs. However, in our experiments, Sum outperformed GLMM for SVMs, One-Hot did better than CV\({}_{5}\)GLMM for boosted trees, and CV\({}_{10}\)GLMM was better than CV\({}_{5}\)GLMM for k-NN. Next, while in  similarity encoders are better than One-Hot for boosted trees, subsequent research reported no significant difference between Min-hash and One-Hot on medium-sized tabular datasets . Our findings are in line with this latter result, as we could not find a performance difference between the two encoders with a t-test with a significance level of 0.05. In , Sum is reported as the best encoder on the Adult dataset for boosted trees, while a Data-constraining encoder is reported as the worst. With the same setting, we did not find a significant performance difference for any encoder except for Drop, which

Figure 2: Replicability as the average similarity of consensus rankings from disjoint subsets of datasets.

performed the worst. On the Bank marketing dataset,  showed that One-Hot and Mean-Target outperformed Binary with logistic regression. In our experiments, Binary was slightly worse than One-Hot and Mean-Target. In , Dummy, an identifier encoder similar to One-Hot, was better than Mean-Target on the Tic-tac-toe dataset with boosted trees. We, instead, did not observe any significant difference between One-Hot and Mean-Target for these factors.

## 6 Limitations and conclusions

**Limitations.** First, we treated encoders as part of the pre-processing, but certain encoders can be an integral component of specific ML models. For instance, CatBoost is derived from the homonymous boosted trees algorithm, which re-encodes the data multiple times during training. Second, we applied a single encoder to all categorical attributes. Using different encoders based on the cardinality of the attribute may sometimes yield favorable results [28; 4]. However, the selection of the optimal encoder for each attribute requires either domain knowledge of the attribute or purpose-built tools, which falls outside the scope of our benchmark and is therefore left as future work. We also did not include neural networks, due to the absence of a recommended architecture and reported interior performance to tree-based models on tabular data .

**Conclusions.**

In this study, we conducted an extensive evaluation of encoder performance across various experimental factors, including ML models, quality metrics, and tuning strategies. Our results demonstrate a high sensitivity of encoder rankings to these factors, both for the full rankings and the best-performing encoders. This sensitivity explains the inconsistent results among related studies, as different choices in any of these factors can lead to different outcomes. We also assessed the impact of aggregation strategies on consensus rankings, revealing significant variations in rankings depending on the chosen strategy. This emphasizes the importance of carefully considering the aggregation method when post-processing and interpreting results. Regarding replicability, we defined and quantified it using \(\)-replicability and \(J\)-replicability. Our findings indicate that replicability is influenced by factors such as the ML model, with logistic regression exhibiting the highest replicability and decision trees

Figure 3: Ranks of encoders.

the lowest. Additionally, larger dataset samples tend to yield more reliable experimental outcomes, although this trend does not always hold for \(J\)-replicability. Based on our results, we recommend specific encoders for practical applications. For decision trees, Weight of Evidence performed the best, although statistical tests did not show a significant difference from other encoders. For logistic regression, Sum, One-Hot, Binary, and Weight of Evidence consistently achieved higher ranks, with statistically significant differences from other encoders. These findings contradict previous studies, highlighting the importance of considering a broad range of experimental factors. Finally, our comparative analysis with related work revealed discrepancies in encoder performance, suggesting that the breadth of our study may contribute to these differences. This emphasizes the need for caution when interpreting results from studies with more limited experimental settings. Overall, our study provides valuable insights into the sensitivity of encoder performance to experimental factors, as well as recommendations for practical encoder selection across different scenarios.