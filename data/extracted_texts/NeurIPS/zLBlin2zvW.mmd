# Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders

Senthooran Rajamanoharan Arthur Conmy\({}^{*}\)

Google DeepMind

&Lewis Smith

Google DeepMind

&Tom Lieberum

Google DeepMind

&Vikrant Varma\({}^{}\)

Google DeepMind

&Janos Kramar

Google DeepMind

&Rohin Shah

Google DeepMind

&Neel Nanda

Google DeepMind

&Nael Nanda

Google DeepMind

###### Abstract

Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as _shrinkage_ - systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.

## 1 Introduction

Mechanistic interpretability aims to explain how neural networks produce outputs in terms of the learned algorithms executed during a forward pass . Much work makes use of the fact that many concept representations appear to be linear , i.e. that they correspond to interpretable directions in activation space. However, finding the set of all interpretable directions is a highly non-trivial problem. Classic approaches, like interpreting neurons (i.e. directions in the standard basis) are insufficient, as many are polysemantic and tend to activate for a range of different seemingly unrelated concepts . Within the field, there has recently been much interest  in using sparse autoencoders (SAEs; ) as an unsupervised method for finding causally relevant, and ideally interpretable, directions in a language model's activations.

Although SAEs show promise in this regard , the L1 penalty used in the prevailing training method to encourage sparsity also introduces biases that harm the accuracy of SAE reconstructions, as the loss can be decreased by trading-off some reconstruction accuracy for lower L1. In this paper, we introduce a modification to the baseline SAE architecture - a _Gated SAE_ - along with an accompanying loss function, which partially overcomes these limitations. Our key insight is to use separate affine transformations for (a) determining which dictionary elements to use in a reconstruction and (b) estimating the coefficients of active elements, and to apply the sparsity penalty only to the former task. We share a subset of weights between these transformations to avoid significantlyincreasing the parameter count and inference-time compute requirements of a Gated SAE compared to a baseline SAE of equivalent width.2

We evaluate Gated SAEs on multiple models: a one layer GELU activation language model , Pythia-2.8B  and Gemma-7B , and on multiple sites within models: MLP layer outputs, attention layer outputs, and residual stream activations. Across these models and sites, we find Gated SAEs to be a Pareto improvement over baseline SAEs holding training compute fixed (Fig. 1): they yield sparser decompositions at any desired level of reconstruction fidelity. We also conduct further follow up ablations and investigations on a subset of these models and sites to better understand the differences between Gated SAEs and baseline SAEs.

Overall, the key contributions of this work are that we:

1. Introduce the Gated SAE, a modification to the standard SAE architecture that decouples detection of which features are present from estimating their magnitudes (Section 3.2);
2. Show that Gated SAEs Pareto improve the sparsity and reconstruction fidelity trade-off compared to baseline SAEs (Section 4.1);
3. Confirm that Gated SAEs overcome shrinkage while outperforming other methods that also address this problem (Section 5.2);
4. Provide evidence from a small double-blind study that Gated SAE features are comparably interpretable to baseline SAE features (Section 4.2).

## 2 Preliminaries

In this section we summarise the concepts and notation necessary to understand existing SAE architectures and training methods following Bricken et al. , which we call the _baseline SAE_. We define Gated SAEs in Section 3.2.

As motivated in Section 1, we wish to decompose a model's activation \(^{n}\) into a sparse, linear combination of feature directions:

\[_{0}+_{i=1}^{M}f_{i}()_{i},\] (1)

where \(_{i}\) are dictionary of \(M n\) latent unit-norm _feature directions_, and the sparse coefficients \(f_{i}() 0\) are the corresponding _feature activations_ for \(\).3 The right-hand side of Eq. (1) naturally has the structure of an autoencoder: an input activation \(\) is encoded into a (sparse) feature activations vector \(()^{M}\), which in turn is linearly decoded to reconstruct \(\).

Figure 1: Gated SAEs consistently offer improved reconstruction fidelity for a given level of sparsity compared to prevailing (baseline) approaches. These plots compare Gated SAEs to baseline SAEs at Layer 20 in Gemma-7B. Gated SAEs’ dictionaries are of size \(2^{17} 131\)k whereas baseline dictionaries are 50% larger, so that both types are trained with equal compute. This performance improvement holds in layers throughout GELU-1L, Pythia-2.8B and Gemma-7B (see Appendix E).

Baseline architectureUsing this correspondence, Bricken et al.  and subsequent works attempt to learn a suitable sparse decomposition by parameterizing a single-layer autoencoder \((,})\) defined by:

\[() :=(_{}(- _{})+_{})\] (2) \[}() :=_{}+_{}\] (3)

and training it using gradient descent to reconstruct samples \(\) from a large dataset \(\) of activations collected from a single site and layer of a trained language model, constraining the hidden representation \(\) to be sparse. Once the sparse autoencoder has been trained, we obtain a decomposition of the form of Eq. (1) by identifying the (suitably normalised) columns of the decoder weight matrix \(_{}^{M n}\) with the dictionary of feature directions \(}\), the decoder bias \(_{}^{n}\) with the centering term \(_{0}\), and the (suitably normalised) entries of the latent representation \(()^{M}\) with the feature activations \(f_{i}()\).

Baseline training methodologyTo train sparse autoencoders, Bricken et al.  use a loss function with two terms that respectively encourage faithful reconstruction and sparsity:4

(4)

Since it is possible to arbitrarily reduce the L1 sparsity loss term without affecting reconstructions or sparsity by simply scaling down encoder outputs and scaling up the norm of the decoder weights, it is important to constrain the norms of the columns of \(_{}\) during training. Following Bricken et al. , we constrain norms to one. See Appendix G for further details on SAE training.

Evaluating SAEsTwo metrics are primarily used to get a sense of SAE quality : _L0_, a measure of SAE sparsity, and _loss recovered_, a measure of SAE reconstruction fidelity. L0 measures the average number of features used by a SAE to reconstruct input activations. Loss recovered is a normalised measure of the increase induced in a LM's cross entropy loss when we replace its original activations with the corresponding SAE reconstructions during the model's forward pass. Both these metrics are formally defined in Appendix B, where we also discuss shortcomings of and alternatives to the loss recovered metric as it is defined in Bricken et al. . Since it is possible for SAEs to score well on these metrics and still fail to be useful for interpretability-related tasks , we perform manual analysis of SAE interpretability in Section 4.2.

## 3 Gated SAEs

### Motivation

The intention behind how SAEs are trained is to maximise reconstruction fidelity at a given level of sparsity, as measured by L0, although in practice we optimize a mixture of reconstruction fidelity and L1 regularization. This difference is a source of unwanted bias in the training of a sparse autoencoder: for any fixed level of sparsity, a trained SAE can achieve lower loss (as defined in Eq. (4)) by trading off a little reconstruction fidelity to perform better on the L1 sparsity penalty.

The clearest consequence of this bias is _shrinkage_. Holding the decoder \(}()\) fixed, the L1 penalty pushes feature activations \(()\) towards zero, while the reconstruction loss pushes \(()\) high enough to produce an accurate reconstruction. Thus, the optimal value falls somewhere in between, and as a result the SAE systematically underestimates the magnitude of feature activations, without necessarily providing any compensatory benefit for sparsity.5

How can we reduce the bias introduced by the L1 penalty? The output of the encoder \(()\) of a baseline SAE (Section 2) has two roles:

1. It _detects_ which features are active (according to whether the outputs are zero or strictly positive). For this role, the L1 penalty is necessary to ensure the decomposition is sparse.

2. It _estimates_ the magnitudes of active features. For this role, the L1 penalty is a source of unwanted bias.

If we could separate out these two functions of the SAE encoder, we could design a training loss that narrows down the scope of SAE parameters that are affected (and therefore to some extent biased) by the L1 sparsity penalty to precisely those parameters that are involved in feature detection, minimising its impact on parameters used in feature magnitude estimation.

### Gated SAEs

ArchitectureHow should we modify the baseline SAE encoder to achieve this separation of concerns? Our solution is to replace the single-layer ReLU encoder of a baseline SAE with a _gated_ ReLU encoder. Taking inspiration from Gated Linear Units , we define the gated encoder as

\[}():=[(_{}(-_{})+_{ }})>]}_{_{}()} (_{}(-_{ })+_{})}_{_{}()},\] (5)

where \([>]\) is the (pointwise) Heaviside step function and \(\) denotes elementwise multiplication. Here, \(_{}\) determines which features are deemed to be active, while \(_{}\) estimates feature activation magnitudes (which only matter for features that have been deemed to be active); \(_{}()\) are the \(_{}\) sub-layer's pre-activations, which are used in the gated SAE loss, defined below.

TrainingA naive guess at a loss function for training Gated SAEs would be to replace the sparsity penalty in Eq. (4) with the L1 norm of \(_{}()\). Unfortunately, due to the Heaviside step activation function in \(_{}\), no gradients would propagate to \(_{}\) and \(_{}\). To mitigate this, we instead apply the L1 norm to the positive parts of the preactivation, \((_{}())\). To ensure \(_{}\) aids reconstruction by detecting active features, we add an auxiliary task requiring that these same rectified preactivations can be used by the decoder to produce a good reconstruction:

\[_{}():=-}(}())\|_{2}^{2}}_{ _{}}+( _{}())\|_{1}}_{_{}}+-}_{}((_{}()))\|_{2}^{2}}_{_{}}\] (6)

where \(}_{}\) is a frozen copy of the decoder, \(}_{}():=_{}^{ }+_{}^{}\), to ensure that gradients from \(_{}\) do not propagate back to \(_{}\) or \(_{}\). This can be implemented by stop gradient operations rather than creating copies. See Appendix J for pseudo-code for the forward pass and loss function.

To calculate this loss (or its gradient), we have to run the decoder twice: once to perform the main reconstruction for \(_{}\) and once to perform the auxiliary reconstruction for \(_{}\). This leads to a 50% increase in the compute required to perform a training update step. However, the increase in overall training time is typically much less, as in our experience much of the training wall clock time goes to generating language model activations (if these are being generated on the fly) or disk I/O (if training on saved activations).

Figure 2: The Gated SAE architecture with weight sharing between the gating and magnitude paths, shown with an example input. See Appendix J for a pseudo-code implementation.

Parameter reduction through weight-tyingNaively, we appear to have doubled the number of parameters in the encoder, increasing the total number of parameters by 50% with respect to baseline SAEs. We mitigate this through weight sharing: we parameterize these layers so that the two layers share the same projection directions, but allow the norms of these directions as well as the layer biases to differ. Concretely, we define \(_{}\) in terms of \(_{}\) and an additional vector-valued rescaling parameter \(_{}^{M}\) as follows:

\[(_{})_{ij}:=((_{} ))_{i}(_{})_{ij}.\] (7)

See Fig. 2 for an illustration of the tied-weight Gated SAEs architecture. With this weight tying scheme, the Gated SAE has only \(2 M\) more parameters than a baseline SAE. In Section 5.1, we show that this weight tying scheme does not harm performance.

With tied weights, the gated encoder can be reinterpreted as a single-layer linear encoder with a non-standard and discontinuous "JumpReLU" activation function , \(_{}(z)\), illustrated in Fig. 12. To be precise, using the weight tying scheme of Eq. (7), \(}()\) can be re-expressed as \(}()=_{}(_{ {mag}}+_{})\), with the JumpReLU gap given by \(=_{}-e^{_{}} _{}\); see Appendix H for an explanation. We think this is a useful intuition for reasoning about how Gated SAEs reconstruct activations in practice.

## 4 Evaluating Gated SAEs

In this section we benchmark Gated SAEs against baseline SAEs across a large variety of models and at different sites. We show that they produce more faithful reconstructions at equal sparsity and that they resolve shrinkage. Through a double-blind manual interpretability study, we find that Gated SAEs produce features that are similarly interpretable to baseline SAE features.

### Benchmarking Gated SAEs

MethodologyWe trained a suite of Gated and baseline SAEs, a family of each type to reconstruct each of the following activations:

1. The MLP neuron activations in GELU-1L, which is the closest direct comparison to Bricken et al. ;
2. The MLP outputs, attention layer outputs (taken pre-\(W_{O}\)) and residual stream activations in 5 different layers throughout Pythia-2.8B and four different layers in the Gemma-7B base model.

For each model and reconstruction site, we trained multiple SAEs using different values of \(\) (and therefore L0), allowing us to compare the Pareto frontiers of L0 and loss recovered between Gated and baseline SAEs. We also use the _relative reconstruction bias_ metric, \(\), defined in Appendix C to measure shrinkage in our trained SAEs. This metric measures the relative bias in the norm of an SAE's reconstructions; unbiased SAEs obtain \(=1\), whereas SAEs affected by shrinkage (which causes reconstruction norms to be systematically too small) have \(<1\).

Since Gated SAEs require at most 1.5\(\) more compute to train than regular SAEs (Section 3.2) of the same width, we compare Gated SAEs to baseline SAEs that have a 50% larger dictionary (hidden dimension \(M\)) to ensure fair comparison in our evaluations.6

ResultsWe plot sparsity against reconstruction fidelity for SAEs with different values of \(\). Higher \(\) corresponds to increased sparsity and worse reconstruction, so as in Bricken et al.  we observe a Pareto frontier of possible trade-offs. We plot Pareto curves for GELU-1L in Fig. 2(a) and Pythia-2.8B and Gemma-7B in Appendix E. At all sites tested, Gated SAEs are a Pareto improvement over regular SAEs: they provide better reconstruction fidelity at any fixed level of sparsity.7 For some sites in Pythia-2.8B and Gemma-7B, loss recovered does not monotonically increase with L0; we attribute this to difficulties training SAEs (Appendix G.1.3). Finally, full tables of results for Pythia and Gemma can be found in Appendix K.

As shown in Fig. 2(b), Gated SAEs' reconstructions are unbiased, with \( 1\), whereas baseline SAEs exhibit shrinkage (\(<1\)), with the impact of shrinkage getting worse as the L1 coefficient \(\) increases (and L0 consequently decreases). Fig. 10 shows that this result generalizes to Pythia-2.8B.

### Interpretability

Although Gated SAEs provide more faithful reconstructions than baselines at equal sparsity, it does not necessarily follow that these reconstructions are better suited to downstream interpretability-related tasks. Currently, there is no consensus on how to systematically assess the degree to which a SAE's features are useful for downstream tasks, but a plausible proxy is to assess the extent to which these features are human interpretable . Therefore, to gain a more qualitative understanding of the differences between their learned features, we conduct a blinded human study in which we rate and compare the interpretability of randomly sampled Gated and baseline SAE features.

MethodologyWe study a variety of SAEs from different layers and sites. For Pythia-2.8B we had 5 raters, who each rated one feature from baseline and Gated SAEs trained on each (site, layer) pair from Fig. 8, for a total of 150 features. For Gamma-7B we had 7 raters; one rated 2 features each, and the rest 1 feature each, from baseline or Gated SAEs trained on each (site, layer) pair from Fig. 9, for a total of 192 features.

For each model, raters are shown the features in random order, without revealing which SAE, site, or layer they came from.8 To assess a feature, the rater decides whether there is an explanation of the feature's behavior, in particular for its highest activating examples. The rater then enters that explanation (if applicable) and selects whether the feature is interpretable ('Yes'), uninterpretable ('No') or maybe interpretable ('Maybe'). All raters are either authors of this paper or colleagues, who have prior experience interpreting SAE features. As an interface we use an open source SAE visualizer library ; representative screenshots of the dashboards produced by this library are shown in Fig. 14.

Results & analysisFig. 4 shows interpretability rating distributions by SAE type and LM, marginalising over layers, sites and raters.9 To compare the interpretability of baseline and Gated SAEs, we first pair our datapoints according to all covariates (model, layer, site, rater); this lets us

Figure 3: (a) Gated SAEs offer better reconstruction fidelity (as measured by loss recovered) at any given level of feature sparsity (as measured by L0); (b) Gated SAEs address shrinkage. These plots compare Gated and baseline SAEs trained on GELU-1L neuron activations; see Appendix E for comparisons on Pythia-2.8B and Gamma-7B.

control for all of them without making any parametric assumptions, and thus reduces variance in the comparison. We then measure the mean difference between baseline and Gated labels, where we count 'No' as 0, 'Maybe' as 1, and 'Yes' as 2, and compute a 90% BCa bootstrap confidence interval. Thus we find that the mean difference in label scores is \(0.13\) (90% CI \([0,0.26]\)) in favour of Gated SAEs, breaking down to mean difference CIs of \([-0.07,0.33]\) and \([-0.04,0.29]\) on just the Pythia-2.8B data and Gemma-7B data respectively. Since our central estimate for the mean difference in scores is positive, we also test the hypothesis that Gated SAEs may be _more_ interpretable than baseline SAEs. However, a one-sided Wilcoxon-Pratt signed-rank test on the paired scores does not reject the null hypothesis that they are equally interpretable (\(p=0.06\)). The contingency tables used for these results are shown in Fig. 13. The overall conclusion is that Gated SAE features are similarly interpretable to baseline SAE features, while also providing better reconstruction fidelity (at fixed sparsity), as shown in the previous section. We provide more analysis of how these break down by site and layer in Appendix I.

## 5 Why do Gated SAEs improve SAE training?

### Ablation study

In this section, we vary several parts of the Gated SAE training methodology to gain insight into which aspects of the training drive the observed improvement in performance. Gated SAEs differ from baseline SAEs in many respects, making it easy to incorrectly attribute the performance gains to spurious details without a careful ablation study. Fig. 5a shows Pareto frontiers for these variations; below we describe each variation in turn and discuss our interpretation of the results.

**Unfreeze decoder**: Here we unfreeze the decoder weights in \(_{}\) - i.e. allow this auxiliary task to update the decoder weights in addition to training \(_{}\)'s parameters. Although this (slightly) simplifies the loss, there is a reduction in performance, suggesting that it is beneficial to limit the impact of the L1 sparsity penalty to just those parameters in the SAE that need it - i.e. those used to detect which features are active.

**No \(}\)**: Here we remove the \(_{}\) scaling parameter in Eq. (7), effectively setting it to zero, further tying \(_{}\)'s and \(_{}\)'s parameters together. With this change, the two encoder sublayers' preactivations can at most differ by an elementwise shift.10 There is a slight drop in performance, suggesting \(_{}\) contributes somewhat to the improved performance of the Gated SAE.

**Untied encoders**: Here we check whether our choice to share the majority of parameters between the two encoders has meaningfully hurt performance, by training Gated SAEs with gating and ReLU encoder parameters completely untied. Despite the greater expressive power of an untied encoder, we see no improvement in performance - in fact a slight deterioration. This suggests our tying scheme (Eq. (7)) - where encoder directions are shared, but magnitudes and biases aren't - is effec

Figure 4: Proportions of SAE features rated as interpretable / uninterpretable / maybe interpretable by SAE type (Gated or baseline) and language model. Gated and baseline SAEs are similarly interpretable, with a mean difference (in favor of Gated SAEs) of 0.13 (95% CI \([0,0.26]\)) after aggregating ratings for both models.

tive at capturing the advantages of using a gated SAE while avoiding the 50% increase in parameter count and inference-time compute of using an untied SAE.

### Is it sufficient to just address shrinkage?

As explained in Section 3.1, SAEs trained with the baseline architecture and L1 loss systematically underestimate the magnitudes of latent features' activations (i.e. shrinkage). Gated SAEs, through modifications to their architecture and loss function, overcome these limitations.

It is natural to ask to what extent the performance improvement of Gated SAEs is solely attributable to addressing shrinkage. Although addressing shrinkage would - all else staying equal - improve reconstruction fidelity, it is not the only way to improve SAEs' performance: for example, Gated SAEs could also improve upon baseline SAEs by learning better encoder directions (for estimating when features are active and their magnitudes) or by learning better decoder directions (i.e. better dictionaries for reconstructing activations).

Here we try to answer this question by comparing Gated SAEs trained as described in Section 3.2 with an alternative (architecturally equivalent) approach that also addresses shrinkage, but in a way that uses frozen encoder and decoder directions from a baseline SAE of equal dictionary size.11 Any performance improvement over baseline SAEs obtained by this alternative approach (which we dub "baseline + rescale & shift") can only be due to better estimations of active feature magnitudes, since by construction an SAE parameterized by "baseline + rescale & shift" shares the same encoder and decoder directions as a baseline SAE.

As shown in Fig. 4(b), although resolving shrinkage only ("baseline + rescale & shift") does improvement baseline SAEs' performance a little, a significant gap remains with respect to the performance of Gated SAEs. This suggests that the benefit of the gated architecture and loss comes from learning better encoder and decoder directions, not just from overcoming shrinkage. In Appendix D we ex

Figure 5: (a) Our ablation study on GELU-1L MLP neuron activations indicates: (i) the importance of freezing the decoder in the auxiliary task \(_{}\) used to train \(_{}\)’s parameters; (ii) tying encoder weights according to Eq. (7) is slightly beneficial for performance (in addition to yielding a significant reduction in parameter count and inference compute); (iii) further simplifying the encoder weight tying scheme in Eq. (7) by removing \(_{}\) is mildly harmful to performance. (b) Evidence from GELU-1L that the performance improvement of gated SAEs does not solely arise from addressing shrinkage (systematic underestimation of latent feature activations): taking a frozen baseline SAE’s parameters and learning \(_{}\) and \(_{}\) parameters on top of them (green line) does successfully resolve shrinkage, by decoupling feature magnitude estimation from active feature detection; however, it explains only a small part of the performance increase of gated SAEs (red line) over baseline SAEs (blue line).

plore further how Gated and baseline SAEs' decoders differ by replacing their respective encoders with an optimization algorithm at inference time.

## 6 Related work

Mechanistic interpretabilityRecent work in mechanistic interpretability has found recurring components in small and large LMs , identified computational subgraphs that carry out specific tasks in small LMs (circuits; ) and reverse-engineered how toy tasks are carried out in small transformers . A central difficulty in this kind of work is choosing the right units of analysis. Sparse linear features have been identified as a promising candidate in prior work [52; 46]. The superposition hypothesis outlined by Elhage et al.  also provided a theoretical basis for this theory, sparking a new interest in using SAEs specifically to learn a feature basis [42; 8; 11; 21; 22; 4], as well as using SAEs directly for circuit analysis . Other work has drawn awareness to issues or drawbacks with SAE training for this purpose, some of which our paper mitigates. Wright and Sharkey  raised awareness of shrinkage and proposed addressing this via fine-tuning. Gated SAEs, as discussed, resolve shrinkage during training. [35; 47; 2; 36] have also proposed general SAE training methodology improvements, which are mostly orthogonal to the architectural changes discussed in this work. In parallel work, Taggart  finds early improvements using a Jump ReLU , but with a different loss function, and without addressing the problems of the L1 penalty.

Classical dictionary learningResearch into the general problem of sparse dictionary learning precedes transformers, and even deep learning. For example, sparse coding  studies how discrete and continuous representations can involve more representations than basis vectors, and sparse representations are also studied in neuroscience [48; 37]. One dictionary learning algorithm, k-SVD  also uses two stages to learn a dictionary like Gated SAEs. Although classical dictionary learning algorithms can be more powerful than SAEs (Appendix D), they are less suited for downstream uses like weights-based circuit analysis or attribution patching [44; 24], because they typically use an iterative algorithm to decompose activations, whereas SAEs make feature extraction explicit via the encoder. Bricken et al.  have also argued that classical algorithms may be 'too strong', in the sense they may learn features the LM itself could not access, whereas SAEs uses components similar to a LM's MLP layer to decompose activations.

## 7 Conclusion

In this work we introduced Gated SAEs which are a Pareto improvement in terms of reconstruction quality and sparsity compared to baseline SAEs (Section 4.1), and are comparably interpretable (Section 4.2). We showed via an ablation study that every key part of the Gated SAE methodology was necessary for strong performance (Section 5.1). This represents significant progress on improving Dictionary Learning on LMs - at many sites, Gated SAEs require half the L0 to achieve the same loss recovered (Fig. 8). This is likely to improve work that uses SAEs to steer language models , interpret circuits , or understand LM components across the full distribution .

**Limitations & future work**. Our benchmarking study focused on GELU-1L and models in the Pythia and Gamma families. It is therefore not certain that these results will generalise to other model families. On the other hand, the theoretical underpinnings of the Gated SAE architecture (Section 3) make no assumptions about LM architecture, suggesting Gated SAEs should be a Pareto improvement more generally. While we have confirmed that Gated SAE features are comparably interpretable to baseline SAE features, it does not necessarily follow that Gated SAE decompositions are equally useful for mechanistic interpretability. It is certainly possible that human interpretability of SAE features is only weakly correlated with either: (i) identification of the causally meaningful directions in a LM's activations; or (ii) usefulness on downstream tasks like circuit analysis or steering. A framework for scalably and objectively evaluating the usefulness of SAE decompositions (gated or otherwise) is still in its early stages  and further progress in this area would be highly valuable. It is plausible that some of the performance gap between Gated and baseline SAEs could be closed by inexpensive inference-time interventions that prune the many low activating features that tend to appear in baseline SAEs, mimicking Gated SAEs' thresholding mechanism. Finally, we would be most excited to see progress on using dictionary learning techniques to further inter pretability in general, such as to improve circuit finding  or steering  in language models, and hope that Gated SAEs can serve to accelerate such work.

## Author contributions

Senthooran Rajamanoharan developed the Gated SAE architecture and training methodology, inspired by discussions with Lewis Smith on the topic of shrinkage. Arthur Conmy and Senthooran Rajamanoharan performed the mainline experiments in Section 4 and Section 5 and led the writing of all sections of the paper. Tom Lieberum implemented the manual interpretability study of Section 4.2, which was designed and analysed by Janos Kramar. Tom Lieberum also created Fig. 2 and Lewis Smith contributed Appendix D. Our SAE codebase was designed by Vikrant Varma who implemented it with Tom Lieberum, and was scaled to Gemma by Arthur Conmy, with contributions from Senthooran Rajamanoharan and Lewis Smith. Janos Kramar built most of our underlying interpretability infrastructure. Rohin Shah and Neel Nanda edited the manuscript and provided leadership and advice throughout the project.