# Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration

Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration

 Hongming Zhang\({}^{1}\), Chenjun Xiao\({}^{2}\), Chao Gao\({}^{3}\), Han Wang\({}^{1}\), Bo Xu\({}^{4}\), Martin Muller\({}^{1}\)

\({}^{1}\)Department of Computing Science and Amii, University of Alberta

\({}^{2}\)CUHK-Shenzhen

\({}^{3}\)Edmonton Research Center, Huawei Canada

\({}^{4}\)Institute of Automation, Chinese Academy of Sciences

hongmin2@ualberta.ca

###### Abstract

Reinforcement learning (RL) algorithms are typically based on optimizing a Markov Decision Process (MDP) using the optimal Bellman equation. Recent studies have revealed that focusing the optimization of Bellman equations solely on in-sample actions tends to result in more stable optimization, especially in the presence of function approximation. Upon on these findings, in this paper, we propose an Empirical MDP Iteration (EMIT) framework. EMIT constructs a sequence of empirical MDPs using data from the growing replay memory. For each of these empirical MDPs, it learns an estimated Q-function denoted as \(\). The key strength is that by restricting the Bellman update to in-sample bootstrapping, each empirical MDP converges to a unique optimal \(\) function. Furthermore, gradually expanding from the empirical MDPs to the original MDP induces a monotonic policy improvement. Instead of creating entirely new algorithms, we demonstrate that EMIT can be seamlessly integrated with existing online RL algorithms, effectively acting as a regularizer for contemporary Q-learning methods. We show this by implementing EMIT for two representative RL algorithms, DQN and TD3. Experimental results on Atari and MuJoCo benchmarks show that EMIT significantly reduces estimation errors and substantially improves the performance of both algorithms.

## 1 Introduction

Reinforcement learning (RL) has achieved remarkable success across various domains, such as games [1; 2; 3; 4; 5], robotics [6; 7; 8] and industrial applications [9; 10], by modeling them as Markov Decision Processes (MDPs). Most RL methods store transitions in a replay memory  and estimate an action-value function from batches of that data. They apply the Bellman optimality equation as an iterative update:

\[Q(s,a) r(s,a)+_{a^{}}Q(s^{},a^{}).\] (1)

Such value iteration converges to the optimal action value \(Q^{*}\) given infinite state-action visitation and updates [12; 13; 14; 15]. An optimal policy is derived by taking an action with maximum \(Q\) at each state or approximated by a parameterized policy [16; 17; 18; 19]. In practice, data coverage in the replay memory is limited to a potentially small subset of the whole state-action space, especially for complex environments. Indeed, the Bellman update in Eq. (1) suffers from estimation errors due to the combination of _applying the max operator to out-of-sample actions_ and _bootstrapping from a function approximator_. In the examples in Fig. 0(a), the well-known _double Q-learning_ method  cannot eliminate the estimation error. Its effects are heavily task-specific: it overestimates on Breakout game but underestimates on HalfCheetah.

We argue that the estimation error is hard to correct for methods based on Eq. (1) that consider the entire MDP, because there are infinitely many suboptimal solutions consistent with the Bellman equation when the data is incomplete. Errors due to such incomplete data can backpropagate to transitions in the replay memory and impede the whole learning process. However, bootstrapping only from in-sample actions without querying the values of unseen actions greatly reduces the estimation error, even with function approximation. The function \(\) in Fig. 0(a) was learned in this way.

Motivated by these observations, we advocate for solving the _empirical MDP_\(}\) which only uses transitions in a replay memory \(\) collected from the environment.

\[(s,a) r+_{a^{}:(s^{},a^{}) }(s^{},a^{}).\] (2)

There is no out-of-sample bootstrapping in Eq. (2), and the solution in a finite state-action space is unique. If \(\) grows over time and gradually covers the entire MDP, then optimizing with Eq. (2) eventually leads to the solution to the original MDP. This observation instantly implies a new iterative learning procedure: alternate between _exploitation by solving an incumbent empirical MDP_, and _exploration for growing the empirical MDP with new data_. This Empirical MDP Iteration (EMIT) process is illustrated in Fig. 0(b). How to grow \(}\)? In principle, exploration should find missing strong actions that bring \(}\) closer to \(\). Following the well-known principle of _optimism in the face of uncertainty_, we argue that one option is to drive the exploration for growing \(}\) by the value difference between \(Q\) and \(\). EMIT can be applied to any RL algorithms that can learn both \(Q\) and \(\). We implement and evaluate two variants based on DQN  and TD3 , for discrete action spaces and continuous control tasks respectively. Our contributions are summarized as follows:

1. We provide a thorough analysis to show why the estimation error is hard to eliminate when using the Bellman update with incomplete data, and that bootstrapping from in-sample transitions greatly reduces this error, both with and without function approximation.
2. In the tabular case, we prove that the in-sample bootstrapping guarantees a unique optimal \(\) for \(}\). If the optimal trajectory for \(\) is included in \(}\), then the greedy policy derived from \(^{*}\) is also optimal for \(\) following the optimal trajectory. Further, monotonic improvement in learning \(\) is guaranteed with growing data coverage if \(\) is deterministic. These imply that \(\) can be a natural regularizer for \(Q\).
3. We develop a novel framework EMIT, which can be used to enhance existing RL algorithms by iteratively solving a current empirical MDP for stable finite-time performance, and can progressively approach a solution to the original MDP. EMIT can be combined with any method that learns a \(Q\) function. We add EMIT to DQN and TD3, and conduct extensive experiments on Atari  and MuJoCo  tasks. We show strong performance enhancements for both methods. Further analysis demonstrates that EMIT largely overcomes problems of estimation error.

## 2 Background

**Markov Decision Process (MDP).** Reinforcement learning (RL) [26; 27] is a paradigm of agent learning via interaction. It can be modeled as a Markov Decision Process (MDP), a 5-tuple \(=(,,R,P,)\). \(\) denotes the state space, \(\) denotes the action space, \(P(s^{}|s,a):\) is the environment dynamics, \(R(s,a):\) is the reward function which is bounded, \((0,1)\) is the discount factor. The goal of an RL agent is to learn an optimal policy \(\) that maximizes the expected discounted cumulative reward \(_{}[_{t=0}^{}^{t}r_{t}]\).

**Value-Based Methods.** Q-learning is a classic algorithm based on the Bellman optimality equation \(Q^{*}(s,a)=[r+_{a^{}}Q^{*}(s^{},a^{})]\). An optimal policy takes an action with maximum \(Q\) at each state. DQN  scales up from tabular \(Q\)-learning by using deep neural networks and experience replay . Actor-critic methods  such as SAC  and TD3  learn a parameterized policy, which is suitable for continuous action spaces. The policy is updated by gradient ascent of a \(Q\) network, \(|}_{s}Q_{}(s,a_{})\), \(a_{}_{}(|s)\), where \(\) is a batch of samples.

**In-Sample Bellman Update.** The in-sample constraint in Eq. (2) avoids bootstrapping from unseen actions when estimating target values. Several recent offline RL  methods use Eq. (2) to approximate an optimal action value function. For example, implicit \(Q\)-learning (IQL)  uses expectile regression to learn the optimal \(Q\) function without ever querying the values of unseen actions. Its learning goal is to minimize the expectile regression objective:

\[L()=_{(s,a,s^{},a^{})}[L_{2}^{ }(r(s,a)+ Q_{}(s^{},a^{})-Q_{}(s,a) )],\] (3)

where \(L_{2}^{}(u)=|-1(u<0)|u^{2}\) is a weighted mean squared error loss, and \(Q_{}\) is the target network. This asymmetric loss function defines the entire spectrum of methods between SARSA (\(=0.5\)) and \(Q\)-learning (\( 1\)). Eq. (2) can be approximated using Eq. (3) with \( 1\). Other methods like In-Sample Actor-Critic (InAC)  and Extreme Q-learning (XQL)  are also based on Eq. (2), and EMIT is straightforward to implement based on these offline methods.

## 3 Empirical MDP Iteration

A replay memory \(\) is maintained to store transitions \((s,a,r,s^{})\). In most cases, the original MDP \(\) is too large, and \(\) contains only a small subset of all transitions in \(\). Due to the incomplete data, infinitely many other MDPs can be consistent with \(\) on transitions in \(\). For ease of notation, we denote \((s,a)\) if \(\,(s,a,r,s^{})\) and \(s\) if \(\,(s,a,r,s^{})\) or \(\,(,a,r,s)\). A visit count \(N(s,a,s^{})\) is defined as the number of times \((s,a,s^{})\) appears in \(\). We define the empirical MDP \(}\) to be the lowest reward MDP that uses all data in \(\).

**Definition 3.1** (Empirical MDP).: Given a dataset \(\) from MDP \(\), the empirical MDP \(}:=(},},,,)\), has state space \(}=\{s|s\}\), and action space \(}=\{a|(s,a)\}\), with reward function \((s,a)=R(s,a)\) if \((s,a)\), and \((s,a)=-\) otherwise. \((s^{}|s,a)=)}{_{s^{}}N(s,a,s^ {})}\) is the empirical transition dynamics based on visit counts in \(\), and \((0,1)\) is the discount factor.

The empirical MDP \(}\) contains all information we can obtain from \(\). If a (state, action) pair is not in the data, its reward is set to \(-\) and its transition probabilities are zero.

We analyze two update rules in the tabular case with finite \(\), the _Bellman update_ and the _in-sample Bellman update_ given in Eqs. (1) and (2). Both update rules try to minimize the Bellman residual , which is a surrogate objective to minimize the action value error and approximate the optimal value. Eq. (1) takes the maximum over all actions, while Eq. (2) only uses in-sample actions for a state. Let \(Q^{*}\) and \(^{*}\) be the optimal action values for \(\) and \(}\) respectively. Then:

**Proposition 3.2**.: _If the data coverage \(\) is incomplete, then the Bellman update Eq. (1) neither guarantees convergence to the optimal value \(Q^{*}\) for the original MDP \(\) nor to the optimal value \(^{*}\) for the empirical MDP \(}\), even in the limit of infinite updates._

The proof is straightforward. Given that \(\) is incomplete, there exist transitions in \(\) that are not present in \(}\). These absent transitions can be assigned any initial values, and may bootstrap to in-sample transitions, thereby influencing the convergence process. This indicates that the Bellmanupdate is heavily influenced by the values of unseen state-action pairs. A simple example is provided in Appendix A Fig. 7. In contrast, the in-sample Bellman update is not affected by unseen transitions, and thus converges to a unique optimal value \(^{*}\) for \(}\).

**Proposition 3.3**.: _The in-sample Bellman update Eq. (2) uniquely converges to the optimal value \(^{*}\) for the empirical MDP \(}\) in the limit of infinitely many updates. Furthermore, if the optimal trajectory for \(\) is included in \(}\), then the greedy policy derived from \(^{*}\) is also optimal for \(\) following the optimal trajectory._

We defer the proof to Appendix A, and provide a toy example on CliffWalk  in Fig. 2 to illustrate the difference between the two update rules. CliffWalk is a simple navigation task. The goal is to reach the state G at the bottom right starting from the bottom left. The reward of reaching G is 1. Dropping into the cliff gives reward -1, and all other rewards are 0. We present two cases: (1) \(\) contains a sub-optimal trajectory and misses many state-action pairs (Fig. 1(a) top left), and (2) \(\) contains an optimal action at each state but misses some sub-optimal actions (Fig. 1(a) top right).

We parameterize \(Q\) and \(\) with neural networks and sample batches of data to update them with gradient descent following Eqs. (1) and (2). The learning curves in Fig. 1(a) shows value errors comparing with the optimal values. \((Q_{1},Q_{2})\) denotes the averaged absolute error on state-action pairs in \(\) between \(Q_{1}\) and \(Q_{2}\). \((,^{*})\) converges to 0, indicates that \(\) converges to \(^{*}\). However, \(Q\) neither converges to \(Q^{*}\) nor \(^{*}\). In particular in the second case, all optimal actions are included in \(\). \(\) converges to global optimum while \(Q\) still fails due to missing actions. This highlights the heavy influence of missing transitions on the Bellman update. Fig. 1(b) further presents the greedy policies derived from \(Q\) and \(\) after learning. Red arrows present actions that neither follow \( Q^{*}\) nor \(^{*}\). The blue shading indicates the action values. \(Q\) overestimates on most state-action pairs (top row of Fig. 1(b)). The policy derived from \(Q\) cannot learn the (current) optimal actions, while \(\) represents a best possible policy.

We solve each empirical MDP \(}_{i}\), defined by the current memory \(_{i}\), and add data to progressively approach the original MDP \(\). If \(\) is deterministic, a monotonic improvement of the learning is guaranteed by alternating between learning a currently best possible policy and collecting more data.

**Proposition 3.4**.: _Assume \(\) has deterministic transitions. If \(\{_{i}\}\) are datasets collected from \(\) with \(_{1}_{2}_{n}\) and corresponding empirical MDPs \(\{}_{i}\}\), then \(_{1}^{*}_{2}^{*}_{n}^{*}\)._

This result is intuitive. With more transitions in the replay memory, we take the maximum over a wider range of actions, leading to a better solution closer to \(Q^{*}\). If the empirical MDP contains all transitions, then \(}=\) and \(^{*}=Q^{*}\). The proof details are provided in Appendix A.

The in-sample Bellman update provides a learning path to improve a policy. The guarantee of the convergence to a unique optimal value on existing transitions is a key property, which makes the in-sample Bellman update a more robust choice for learning in the presence of missing transitions.

Figure 2: (a) illustrates two cases in the CliffWalk task: memory \(\) only contains a sub-optimal trajectory and misses many transitions (left); \(\) contains all optimal state-action pairs but misses some sub-optimal actions (right). The curves show estimation errors of \(Q\) and \(\) learned with Eqs. (1) and (2) compared with \(Q^{*}\) and \(^{*}\). (b) presents the greedy policies after learning. Red arrows indicate incorrect actions neither follow \( Q^{*}\) nor \(^{*}\). The blue shading signifies accurately estimated \(\) and overestimated \(Q\) values.

More importantly, if the globally optimal trajectory has been explored and included in the memory, then the in-sample Bellman update can learn an optimal policy catching up with the optimal trajectory even when some other transitions are missing. In contrast, the Bellman update is strongly affected by such missing transitions, and cannot guarantee to learn even from optimal trajectories.

### Enhancing Online RL Algorithms with EMIT

Two major questions in EMIT are how to learn \(\) from \(}\) and how to grow \(}\) using out-of-the-box strong reinforcement learning algorithms. To take advantage of an existing value-based learning algorithm, we utilize \(\) to modify \(Q\) learning in two places, based on the value difference. First, given a sampled batch of data, we modify the standard mean squared error (MSE) loss function of \(Q\) by adding a MSE loss of \(\) as a regularizer:

\[=MSE(Q_{},Q_{})+ MSE(Q_{},),\] (4)

Here, \(Q_{}=r(s,a)+_{a^{}}Q(s^{},a^{})\), and \(\) is a parameter controlling the level of regularization. Second, we add an exploration bonus to the algorithm's behavior policy. The rationale is that since we regularize \(Q\) to approximate \(\) on existing transitions in the replay memory, the values of \(Q(s,a)\) and \((s,a)\) should be closer at known than at unseen state-action pairs. Therefore, we provide more incentive for the agent to explore those dissimilar states. Define the absolute difference between \(Q\) and \(\) at \((s,a)\) when interacting with the environment by:

\[(s,a)=|Q(s,a)-(s,a)|.\] (5)

For algorithms in discrete action spaces such as DQN, we can add an exploration bonus \((s,a)\) to the greedy part of an \(\)-greedy policy: with probability \(\) it chooses an random action as usual, otherwise it selects an action that is greedy w.r.t \(Q(s,a)+(s,a)\):

\[=,&p=\\ _{a}(Q(s,a)+(s,a)),&p=1-\] (6)

Eq. (6) adds a targeted exploration mechanism to \(\)-greedy.

For algorithms in continuous action spaces such as TD3, where the policy is assumed to be a unimodal Gaussian distribution, we use \(\) as a state-action dependent standard deviation and sample actions according to:

\[a(s)+,((0, (s,a)),-c,c),\] (7)

The added noise is clipped to keep the target close to the original action .

Algorithm 1 shows pseudo-code for incorporating EMIT into a \(Q\) learning algorithm 1. Blue text highlights the components added or modified by EMIT. EMIT maintains \(\) and incorporates it into the algorithm's action selection \(()\) and \(Q\) update \(()\). Any method based on Eq. (2) can be used to learn \(\). In our experiments, we apply implicit \(Q\)-learning (IQL) . We learn \(Q\) using DQN  and TD3  for discrete and continuous environments respectively. Clearly, Algorithm 1 should be more appropriate be viewed as a new framework, rather than a fixed algorithm.

```
1:Alg.Initialize the replay memory \(\) and action value network \(Q_{}\)
2:EMIT.Initialize\(_{}\)
3:Alg.Initialize the environment \(s_{0} Env\)
4:for environment step \(t\) = 0 to \(T\)do
5:Alg.Select an action \(a_{t}=(s_{t},_{})\) as Eq. (6) or (7) \(\{\) guided exploration\(\}\)
6:Alg.Execute \(a_{t}\) in \(Env\) and get \(r_{t},s_{t+1}\)
7:Alg.Store transition \((s_{t},a_{t},r_{t},s_{t+1})\) in \(\)
8:Alg.Sample random minibatch of transitions \(\) from \(\)
9:EMIT.update(\(_{},\)) w.r.t MSE loss derived by Eq.(2) \(\{\)Learning of \(\}\)
10:Alg.update(\(Q_{},,_{}\)) as Eq.(4) (\(\) regularized learning for \(Q\))
11:endfor ```

**Algorithm 1** Empirical MDP Iteration for Enhancing a \(Q\) Learning Algorithm 1.

Experiments

In this section, we present experimental results to validate the effectiveness of our proposed method. We applied EMIT to both DQN  and TD3 , covering both discrete and continuous action spaces. Our results demonstrate a significant performance improvement when EMIT is applied to these current DRL methods. Ablation studies underscore the importance of each component of our method in contributing to this improvement. Further analysis provides insights into why EMIT works. Notably, the in-sample Bellman update Eq. (2) is capable of learning a superior policy compared to the Bellman update Eq. (1). Regularization of \(Q\) by \(\) yields a more accurate estimate, and the estimation error in \(Q\) decreases. Additionally, the observed decrease in policy churn  may represent another potential benefit of our method, worth further investigation.

### Setup

**Environments.** We evaluate EMIT on Atari  and MuJoCo  tasks based on OpenAI Gym interface . Atari provides a diverse set of video games, making it an ideal platform for evaluating the general competency of AI agents. The input for these games is image-based, and the dynamics are nearly deterministic. MuJoCo offers a set of continuous control tasks, all of which have a standardized structure and interpretable rewards. These tasks are designed to facilitate research and development in the field of robotics, where the need for fast and accurate simulation is paramount. The dynamics in MuJoCo are deterministic and feature high-dimensional state features. More details about these environments can be found in Appendix B.

**Baselines and Implementation Details.** We benchmark EMIT against several established algorithms. For Atari games, we compare our method with DQN , C51 , IQN , and Rainbow . Each run involves 10 million interaction steps. Performance is evaluated by executing 30 episodes after every 100k environmental steps. For MuJoCo tasks, our method is compared with SAC [18; 39], TD3 , XQL , TRPO , and PPO . Each run involves 2 million interaction steps. Performance is evaluated by executing 10 episodes after every 10k environmental steps. We use the same network as in the original papers for each algorithm. We search the learning rates for baselines among {1e-3,3e-4,1e-4} and report the best performance. Each experiment is run with 5 different random seeds. For our method, we set the learning rate the same as the backbone algorithm, and search for the best regularization parameter \(\{0.05,0.1,0.5\}\). All other common parameters are set as detailed in Appendix B Tables 1 and 2. Further details can be found in Appendices B.2 and B.3.

### Performance Enhancement of EMIT for DQN and TD3

To demonstrate the effectiveness of EMIT, we initially compare it with its backbone algorithms. Fig. 3 depicts the learning curves for Atari games (first row) and MuJoCo tasks (second row). The solid line represents the mean score, while the shaded area indicates the standard deviation. The \(}\)**online** result refers to an IQL agent run online with the learning rule Eq. (2). This agent attempts to solve the empirical MDP but lacks the exploration mechanism that augments the MDP. DQN and TD3 are agents that follow the learning rule Eq. (1). They have exploration mechanisms such as

Figure 3: Performance on Atari (first row) and MuJoCo (second row) tasks. The learning curves show mean scores and standard deviations over five runs. EMIT consistently boosts performance across diverse tasks.

\(\)-greedy and random noise. They differentiate from the EMIT counterparts in that they do not learn from the empirical MDP, hence they lack the regularization and exploration bonus derived from \(\). Our EMIT methods learn both \(Q\) and \(\). As depicted in Fig. 3, they achieve clear improvements on all tasks compared to other methods that solely learn \(Q\) or \(\). EMIT can effectively augment and boost these existing RL algorithms. It provides a principled framework for seamlessly integrating the strengths of traditional learning based on the \(Q\) function and the more recent advancements in in-sample learning for \(\). Additional results can be found in Appendix C Figs. 23 and 24.

### Comparison with Other Baselines

We extend our comparison of EMIT with other strong baselines. For a clearer presentation, we display normalized scores. For each task, we define \(R_{}\) and \(R_{}\) as the maximum and minimum cumulative reward across all methods, respectively. The score for each method is then normalized using the formula \((R-R_{})/(R_{}-R_{})\).

The overall performance on various tasks is presented in Fig. 4. EMIT achieves the best average performance across these tasks. Specifically, EMIT either outperforms or is on par with (within a 10% difference) the best baselines on 19 out of 28 tasks. Details are given in Appendix C.2 Fig. 17. Notably, on MuJoCo tasks, EMIT significantly surpasses other baselines. These results corroborate our intuition that Empirical MDP Iteration is a more effective approach than focusing on the original MDP from the outset. This strategy circumvents bootstrapping error, thereby enhancing sample efficiency. While the Double Q technique mitigates overestimation by employing the target network to evaluate actions, the target network may still overestimate or underestimate certain state-action pairs. The distributional perspective of value estimation fosters more stable and risk-aware behavior. However, estimation error persists due to the maximization bootstrapping from out-of-sample actions. In contrast, EMIT seeks to simplify the problem, addressing it incrementally and progressively approximating the original problem. This approach can provide accurate target values for each empirical MDP, leading to consistent improvement. In summary, EMIT reduces estimation error and provides accurate targets, thereby making the learning process more efficient. Detailed learning curves can be found in Appendix C.2.

### How the In-Sample Bellman Update Benefits the Learning

In Section 3, we analyze beneficial properties of the in-sample Bellman update. In this section, we provide comprehensive empirical evidence for them.

We first demonstrate that the in-sample Bellman update is well-suited for passive learning [42; 29], without the need for active interaction with the environment. Since the policy under which the data is collected is not a concern, this allows greater flexibility in designing exploration strategies. We execute DQN and TD3 (based on the Bellman update in Eq. (1)) on Breakout and HalfCheetah to learn a policy \(\). Concurrently, we learn a policy \(\) using another IQL agent with in-sample Bellman updates Eq. (2). Policy \(\) interacts with the environment and collects data into a replay memory. Both \(\) and \(\) learn from this same dataset. Fig. 4(a) illustrates the performance of \(\) and \(\). Interestingly, \(\) performs comparably, if not better than \(\). Despite \(\) being learned without taking actions in the environment, we observe no performance degradation attributable to passive learning. This is somewhat counterintuitive, as active learning is generally considered superior .

Figure 4: Average performance on 20 Atari games and 8 MuJoCo tasks, with each method’s score normalized between 0 and 1. EMIT achieves the best average performance across these tasks.

We conclude that the in-sample Bellman update can learn an effective policy from given data without the need for active interaction with the environment, and that the active policy \(\) does not fully utilize the collected data and fails to identify the optimal policy due to bootstrapping errors. This underscores that bootstrapping from out-of-sample transitions hampers sample-efficient learning. Similar results on additional environments can be found in Appendix C.3 Figs. 18 and 19.

Next, we demonstrate that the regularization of \(Q\) with \(\) significantly diminishes the estimation error in \(Q\). At each evaluation, we obtain the true discounted Monte Carlo return, denoted as \(Q_{}\), by executing the current policy for 30 episodes and calculating the actual discounted cumulative rewards, \(Q_{}(s_{t},a_{t})=_{t=t}^{T}^{t}r_{t}\). We then compute the average estimation difference between the learned \(Q\) and the ground truth \(Q_{}\),

\[V_{}(Q,Q_{})=|}_{(s,a) }(Q(s,a)-Q_{}(s,a)),\] (8)

Here, \(\) represents the set of evaluation trajectories, and \(||\) denotes the total number of state-action pairs in \(\). We calculate \(V_{}\) for our method and compare it with DQN and TD3 on Breakout and HalfCheetah1. As shown in Fig. 4(b), our method maintains the difference near zero, indicating that the estimation is remarkably close to the true value, with no significant overestimation or underestimation. Similar results for other environments can be found in Appendix C.3.

Another potential factor contributing to the performance improvement is the reduction of policy churn, the rapid change of the greedy policy in value-based reinforcement learning, primarily induced by high-variance updates in deep learning. Much of this policy change could be unnecessary, particularly as learning converges . Measuring policy change in a continuous action space is not straightforward. Therefore, we assess the change on Breakout and Pong, which have discrete action spaces. The results are shown in Fig. 5(a), with additional results in Fig. 22 in Appendix C.4. Our findings consistently indicate that our method reduces policy change compared to DQN. However, policy churn is not entirely detrimental, as it can serve as a potentially beneficial form of implicit exploration. The optimal frequency of change that could enhance learning remains unclear and deserves further investigation.

### Effect of Loss Regularization and Exploration Bonus

Our algorithm design comprises two key components: the regularization term, determined by the parameter \(\), and the exploration term, designed based on the value difference \((s,a)\). The impact of each component on the Breakout and HalfCheetah environments is further shown in Figure 5(b). The label **w/o reg term** signifies the absence of regularization for the function \(Q\), i.e., \(=0\). **w/o explore term** indicates the use of existing exploration methods, such as \(\)-greedy or random noise, without the addition of our exploration mechanism. **w/o both** refers to the backbone methods devoid of two enhancements. **full method** incorporates both the regularization and exploration terms. Figure 5(b) reveals that both components independently contribute to learning. We see a more significant performance decline without the regularization term. This suggests that, by adhering to the paradigm of empirical MDP iteration, the current exploration mechanism can already yield substantial improvements. The introduction of an advanced exploration mechanism can further enhance performance. Similar results for more environments can be found in Appendix C.5.

Figure 5: (a) We run DQN and TD3 on Breakout and HalfCheetah, and concurrently learn action value \(\) with the collected data. The curves show the performance of policies \(\) and \(\), derived from \(Q\) and \(\) respectively. Remarkably, \(\), learned without active environment interaction, matches or even surpasses \(\)’s performance. (b) EMIT helps reduce the error in \(Q\) and learns almost accurate value estimation during the learning process.

## 5 Related Work

Accurate value estimation is crucial for RL algorithms. The use of erroneous bootstrapping targets can degrade the \(Q\) function, resulting in suboptimal performance [44; 45; 46; 47]. Overestimation can occur when bootstrapping error is combined with maximizing over action values . Double DQN  mitigates this overestimation by computing the maximum from two \(Q\) functions. Maxmin Q-learning  learns multiple Q functions to strike a balance between overestimation and underestimation. It requires meticulous hyper-parameter fine-tuning, which can be computationally intensive. CEER  constructs a graph to merge similar states and computes an additional conservative \(Q\) estimation akin to solving Eq. (2). However, CEER is effective only when the constructed graph is dense. C51  and IQN  are distributional RL methods that are generally more stable and risk-aware than DQN. Nonetheless, estimation error remains due to the maximization bootstrapping from out-of-sample actions. Rainbow  integrates six enhancements to the DQN algorithm for improved empirical performance: double Q-learning , prioritized replay , dueling networks , multi-step learning , distributional RL , and noisy nets .

Offline reinforcement learning  tries to fully use existing data without additional online data collection. They aim to extract best possible policy from the existing dataset. While earlier methods focused on constraining the distance between the learned policy and the behavior policy to avoid distributional shift caused by taking actions outside of the behavior distribution [54; 55; 56], recent studies found that selecting actions within the support of the dataset during training, similar to Eq. (2), is more effective [57; 30; 58; 31]. Implicit \(Q\)-Learning (IQL)  estimates the value of the best available action at a given state with expectile regression, without ever directly querying the \(Q\) function for unseen actions. In-Sample Actor-Critic (InAC)  approximates an in-sample softmax using only actions in the dataset. Extreme Q-learning (XQL)  models the maximal value using Extreme Value Theory (EVT) and avoids computing Q-values using out-of-sample actions. In EMIT, we provide a framework for easy integration of any of these methods to solve Eq. (2).

## 6 Discussion and Limitations

We study the application of the Bellman equation as a learning objective in scenarios with incomplete data. Bootstrapping from in-sample transitions with Eq. (2) theoretically leads to a unique solution and significantly reduces the estimation error in practice, even with function approximation. We introduce a novel learning paradigm, termed Empirical MDP Iteration (EMIT). Unlike previous methods that solely focus on solving the entire original MDP, we propose a regularization approach for learning by solving a series of empirical MDPs using only the transitions present in the data. EMIT provides an iterative learning pathway that uniquely solves each empirical MDP and incrementally approaches the original MDP through new data collection. We instantiate EMIT with the \(Q\)-learning algorithm DQN and the actor-critic algorithm TD3. Results demonstrate a substantial improvement in performance in applications of both video games and continuous control tasks. A shortcoming is that since the policy derived from \(\) lacks an exploration mechanism, we learn two \(Q\) functions in our algorithm design and the wall-clock time would be double. One future work could be designing a learning process that is directly based on in-sample Bellman update to avoid out-of-sample bootstrapping and also taking the exploration mechanism into consideration.

Figure 6: (a) EMIT helps reduce unnecessary policy change comparing with DQN, potentially contributing to the enhanced performance. (b) Both the regularization and the exploration mechanism benefit the online learning. The regularization term exerts a greater impact than the exploration mechanism.