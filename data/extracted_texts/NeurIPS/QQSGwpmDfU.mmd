# Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification

Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification

 Jiangming Shi\({}^{1,3}\)1, Xiangbo Yin\({}^{2}\)1, Yachao Zhang\({}^{2}\), Zhizhong Zhang\({}^{4,5}\)

\({}^{1}\)Institute of Artificial Intelligence, Xiamen University

\({}^{2}\)School of Informatics, Xiamen University

\({}^{3}\)Shanghai Innovation Institute

\({}^{4}\)East China Normal University

\({}^{5}\)Shanghai Key Laboratory of Computer Software Evaluating and Testing

jiangming.shi@outlook.com yxie@cs.ecnu.edu.cn yyqu@xmu.edu.cn

###### Abstract

Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified persons in infrared images to visible images without annotations, and vice versa. USVI-ReID is a challenging yet underexplored task. Most existing methods address the USVI-ReID through cluster-based contrastive learning, which simply employs the cluster center to represent an individual. However, the cluster center primarily focuses on commonality, overlooking divergence and variety. To address the problem, we propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes for USVI-ReID. In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center. We reveal that the inclusion of the hard prototype in contrastive loss helps to emphasize divergence. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. The dynamic prototype is used to encourage variety. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards divergence and variety, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.

## 1 Introduction

Visible-infrared person re-identification (VI-ReID) aims at matching the same person captured in one modality with their counterparts in another modality [1; 2; 3]. It has recently gained attention in computer vision applications like video surveillance  and image retrieval [5; 6; 7]. With the development of deep learning [8; 9; 10; 11], VI-ReID has achieved remarkable advancements [12; 13; 14]. However, the development of existing VI-ReID methods is still limited due to the requirement for expensive-annotated training data [15; 16]. To mitigate the problem of annotating large-scale cross-modality data, some semi-supervised VI-ReID methods [17; 18; 19] are proposed to learn modality-invariant and identity-related discriminative representations by utilizing both labeled and unlabeled data. For this purpose, OTLA  proposed an optimal transport label assignment mechanism to assign pseudo-labels for unlabeled infrared images while ignoring how to calibrate noise pseudo-labels. DPIS  integrates two pseudo-labels generated by distinct models into a hybrid pseudo-labelfor unlabeled infrared data, but it makes the training process more complex. Although these methods have gained promising performances, they still rely on a certain number of manual-labeled data.

Several USVI-ReID methods [20; 21; 22; 23] have proposed to tackle the issues of expensive visible-infrared annotation through contrastive learning. These methods create two modality-specific memories, one for visible features and the other for infrared features. During training, these methods consider the memory center as a prototype and minimize the contrastive loss across the features of query images and prototype. Then, these methods aggregate the corresponding prototypes based on similarity. However, the centroid prototype only stores the commonality of each person, neglecting the divergence [24; 25; 26], which causes the pseudo-labels generated by the cluster to be unreliable. Just like a normal distribution, to better reflect the data distribution of a dataset, we need not only the mean but also the variance.

In this paper, we argue that an important aspect of contrastive learning for USVI-ReID, i.e. the design of the prototype, has so far been neglected, and propose progressive contrastive learning with hard and dynamic prototype (PCLHD) method for the USVI-ReID. Firstly, we design a Hard Prototype Contrastive Learning (HPCL) to mine divergent yet meaningful information. In contrast to traditional contrastive learning methods, we choose the hard samples to serve as the hard prototype. In other words, the hard prototype is the one that is farthest from the memory center. The hard prototype encompasses distinctive information. Furthermore, we introduce the concept of Dynamic Prototype Contrastive Learning (DPCL), we randomly select samples from each cluster to serve as the dynamic prototype. DPCL effectively accounts for the intrinsic variety within clusters, enhancing the model's adaptability to varying data distributions. Early clustering results are unreliable, and utilizing hard and dynamic prototype at this stage may lead to cluster degradation. Therefore, we introduce progressive contrastive learning to gradually focus on divergence and variety.

The main contributions are summarized as follows:

* We propose a progressive contrastive learning with hard and dynamic prototype method for the USVI-ReID. We reconsider the design of prototypes in contrastive learning to ensure that the model stably captures commonality, divergence, and variety.
* We propose Hard Prototype Contrastive Learning for mining divergent yet significant information, and Dynamic Prototype Contrastive Learning for preserving the intrinsic variety in sample features.
* Experiments on SYSU-MM01 and RegDB datasets demonstrate the superiority of our method compared to existing USVI-ReID methods, and PCLHD generates higher-quality pseudo-labels than other methods.

## 2 Related Work

### Supervised Visible-Infrared Person ReID

Visible-infrared person re-identification (VI-ReID) has drawn much attention in recent years [27; 28; 29; 30; 31; 32]. Many VI-ReID methods focused on mitigating huge semantic gaps across modalities have made advanced progress, which can be classified into two primary classes based on their different aligning ways: image-level alignment and feature-level alignment. The image-level alignment methods focus on reducing cross-modality gaps by modality translation. Some GAN-based methods [33; 34] are proposed to perform style transformation for aligning cross-modality images. However, the generated images unavoidably contain noise. Therefore, X-modality  and its promotions [36; 37] align cross-modality images by introducing a middle modality. Mainstream feature-level alignment methods [38; 39; 40] focus on minimizing cross-modality gaps by finding a modality-shared feature space. However, the advanced performances of the above methods build on large-scale human-labeled cross-modality data, which are quite time-consuming and expensive, thus hindering the fast application of these methods in real-scenes.

### Unsupervised Single-Modality Person ReID

The existing unsupervised single-modality person ReID methods can be roughly divided into two classes: Unsupervised domain adaption (UDA) methods, which try to leverage the knowledge transferred from labeled source domain to improve performance [41; 42; 43; 44], and fully unsupervised methods (USL), which directly train a USL-ReID model on the unlabeled target domain [21; 45]. Compared with the UDA methods, the USL methods are more challenging. Recently, cluster-contrast learning  has achieved impressive performance by performing contrastive learning at the cluster level. However, cluster-contrast with a uni-proxy can be biased and confusing, which fails to accurately describe the information of a cluster. To this end, the methods [47; 48] proposed maintaining multi-proxies for a cluster to adaptively capture different information within the cluster. The above methods are mainly proposed to solve the single-modality ReID task, but they are limited to solving the USL-VI-ReID task due to large cross-modality gaps.

### Unsupervised Visible-Infrared Person ReID

Unsupervised visible-infrared person ReID (USVI-ReID) has attracted much attention due to the advantage of not relying on any data annotation. Some UDA methods [49; 17] use a well-annotated labeled source domain for pre-training to solve the USVI-ReID task. Some fully unsupervised methods [23; 22] adopt contrastive learning to boost performance, which mainly follow a two-step loop paradigm: generating pseudo-labels using the DBSCAN algorithm  to create memory banks with clustering centers and establishing cross-modality correspondences based on these memory banks. However, pseudo-labels are often inaccurate and rigid, CCLNet  leverages the text information from CLIP to afford greater semantic monitoring insights to compensate for the rigidity of pseudo-labels. Moreover, reliable cross-modality correspondences are vital to USVI-ReID, thus PGM  proposes a progressive graph matching framework to establish more reliable cross-modality correspondences. However, cluster centers mainly present common information while lacking distinctive information, which results in ambiguous cross-modality correspondences when meeting hard samples [52; 53].

## 3 Method

### Problem Formulation and Overview

Given a USVI-ReID dataset \(D=\{V,R\}\), where \(V=\{V_{i}\}_{i=1}^{N_{v}}\) represents the visible images and \(R=\{R_{j}\}_{j=1}^{N_{r}}\) denotes the infrared images. \(V_{i}\) and \(R_{j}\) represent the set of images corresponding to the \(i\)-th and \(j\)-th class. \(N_{v}\) and \(N_{r}\) denote the number of visible and infrared clusters, respectively. In the USVI-ReID task, the purpose is to train a deep neural network to obtain modality-invariant and identity-related features for matching pedestrian images with the same identity.

We propose a Progressive Contrastive Learning with Hard and Dynamic Prototype (PCLHD) method for USVI-ReID, which mainly contains online encoder, momentum encoder, and progressive contrastive learning strategy with centroid prototype, hard prototype, and dynamic prototype, as shown in Fig. 1. The online encoder is a standard network, updated through back-propagation. The momentum

Figure 1: Framework of our PCLHD. The framework consists of two stages: the first stage employs contrastive learning with centroid prototypes to learn well-discriminative representation, and the second stage introduces contrastive learning with hard and dynamic prototypes to further focus on divergence and variety.

encoder mirrors the structure of the online encoder, updated through the weights of the online encoder. The clustering is used to generate pseudo labels for creating cluster-aware memory, and we employ DBSCAN for clustering. PCLHD primarily focuses on representation learning, and we use PGM  to aggregate cross-modality memory.

### Centroid Prototype Contrastive Learning

Following the USVI-ReID methods [22; 54], we use centroid prototype contrastive learning to optimize the online encoder in the first state, which includes memory initialization and optimization.

**Memory Initialization.** Let \(_{0}\) be the online encoder that transforms the input image to an embedding vector. At the beginning of each training epoch, all image features are clustered by DBSCAN  and then each cluster's representations are stored in visible memory \(M_{RGB}=\){\(cm_{1}^{v}\), \(cm_{2}^{v}\), \(\), \(cm_{N_{v}}^{v}\)} and infrared memory \(M_{IR}\)={\(cm_{1}^{r}\), \(cm_{2}^{r}\), \(\), \(cm_{N_{r}}^{r}\)}, as follows:

\[cm_{i}^{v}=|}_{ V_{i}}_{0}(),\] (1)

\[cm_{j}^{r}=|}_{ R_{j}}_{0}(),\] (2)

where \(||\) denotes the number of instances belonging to specific cluster.

**Optimization.** During training, we update the two modality-specific memories by a momentum updating strategy . We treat the memory center as a centroid prototype and optimize the feature extractor \(_{0}\) using contrastive learning with the centroid prototype, computed as:

\[_{CPCL}^{v}=}_{i N_{v}}|}_ { V_{i}}(_{0}() cm _{i}^{v}/)}{_{j N_{r}}(_{0}( ) cm_{j}^{v}/)},\] (3)

\[_{CPCL}^{r}=}_{i N_{r}}|} _{ R_{i}}(_{0}() cm _{i}^{r}/)}{_{j N_{r}}(_{0}() cm_{j}^{r}/)},\] (4)

\[_{CPCL}=_{CPCL}^{v}+_{CPCL}^{r},\] (5)

where \(cm_{i}^{v(r)}\) is the positive centroid prototype, denoting a query and the prototype shares the same identity. The \(\) is a temperature hyper-parameter.

### Hard Prototype Contrastive Learning

To ensure that the prototype effectively captures divergence within a identity, we devise a novel hard prototype for contrastive learning, which is referred to as Hard Prototype Contrastive Learning. HPCL is designed to provide a comprehensive understanding of personal characteristics, which benefits its handling of hard samples . We use the online encoder \(_{0}\) to extract feature representations, and select \(k\) samples that are farthest from the memory center as the hard prototype:

\[hm_{i}^{v}=*{arg\,max}_{ V_{i}}\|_ {0}()-cm_{i}^{v}\|,\] (6)

\[hm_{j}^{r}=*{arg\,max}_{ R_{j}}\|_ {0}()-cm_{j}^{r}\|.\] (7)

**Theorem 1**.: The information entropy of hard sample prototypes is greater than the information entropy of centroid prototypes, thereby preserving greater divergence within the hard memory.

Given a set of features \(\{f_{1},f_{2},,f_{N_{c}}\}\) for class \(c\). The entropy \(H(cm_{c})\) can be approximated by the entropy of the distribution of the sample means. Considering that \(cm_{c}\) is a convex combination of the sample features \(f_{i}\), we have:\[H(cm_{c})=H(}_{i=1}^{N_{c}}f_{i}).\] (8)

By the convexity of entropy, we have:

\[H(}_{i=1}^{N_{c}}f_{i})}_{i= 1}^{N_{c}}H(f_{i}).\] (9)

This inequality implies that the entropy of the centroid prototype is generally lower due to the averaging effect, which reduces the divergence among the samples, leading to lower entropy. Given that \(hm_{c}\) is the sample with the maximum individual entropy among the set \(\{f_{1},f_{2},,f_{N_{c}}\}\), it follows that:

\[H(hm_{c})}_{i=1}^{N_{c}}H(f_{i}) H(cm_{c}).\] (10)

Then, we construct contrastive loss with the hard prototype to minimize the distance between the query and the positive hard prototype while maximizing their discrepancy to all other cluster hard prototypes, as follows:

\[_{HPCL}^{v}=}_{i N_{v}}|} _{ V_{i}}() hm_{i}^{v}/ )}{_{j N_{v}}(_{0}() hm_{ j}^{v}/)},\] (11)

\[_{HPCL}^{r}=}_{i N_{r}}|} _{ R_{i}}() hm_{i}^{r }/)}{_{j N_{r}}(_{0}() hm _{j}^{r}/)},\] (12)

\[_{HPCL}=_{HPCL}^{v}+_{HPCL}^{r},\] (13)

where \(hm_{i}^{v(r)}\) is the positive hard prototype representation and the \(\) is a temperature hyper-parameter.

Finally, we update the two modality-specific memories with a momentum-updating strategy:

\[hm_{i,t}^{v}= hm_{i,t-1}^{v}+(1-)_{0}(),  V_{i}\] (14)

\[hm_{i,t}^{r}= hm_{i,t-1}^{r}+(1-)_{0}(),  R_{i}\] (15)

where \(\) is a momentum coefficient that controls the update speed of the memories. \(t\) and \(t-1\) refer to the current and last iteration, respectively.

The hard prototype contrastive learning has two main advantages: For intra-class feature learning, it ensures that the learning process does not just focus on the shared characteristics within a cluster but also considers the diverse elements, which are often more informative. For inter-class feature learning, it is also beneficial for increasing the distances between different persons. In contrast, centroid prototypes tend to average features, lacking diversity, which can affect the network's ability to extract discriminative features.

### Dynamic Prototype Contrastive Learning

Inspired by MoCo  and DPM , we design dynamic prototype contrastive learning in order to preserve the intrinsic variety in sample features. DPCL comprises an online encoder \(_{0}\) and a momentum encoder \(_{m}\). The momentum encoder mirrors the structure of the online encoder, which is updated by the accumulated weights of the online encoder:

\[_{m}^{t}=_{m}^{t-1}+(1-)_{0}^{t},\] (16)

where \(\) is a momentum coefficient that controls the update speed of the momentum encoder. \(t\) and \(t-1\) refer to the current and last iteration, respectively. The momentum encoder \(_{m}\) is updated by the moving averaged weights, which are resistant to sudden fluctuations or noisy updates .

We use the momentum encoder \(_{m}\) to extract feature representation and store them in visible memory \(DM_{RGB}\)={\(dm_{1}^{v}\), \(dm_{2}^{v}\), \(\), \(dm_{N_{y}}^{v}\)} and infrared memory \(DM_{IR}\)={\(dm_{1}^{r}\), \(dm_{2}^{r}\), \(\), \(dm_{N_{y}}^{r}\)}. We randomly select \(M\) visible/infrared samples from each cluster, denoted as \(X_{i}^{v}\) and \(X_{j}^{r}\).as follows:

\[F_{i}^{v} =_{m}(X_{i}^{v}),\] (17) \[F_{j}^{r} =_{m}(X_{j}^{r}).\] (18)

We select visible dynamic prototype \(dm_{i}^{v}\) from \(DM_{RGB}\). In the same cluster, we select the sample farthest from the query image as the prototype. In different clusters, we choose the sample closest to the query image as the prototype:

\[dm_{i}^{v}=*{arg\,max}_{ f_{i}^{v} F_{i} ^{v}}\|_{m}(_{j})-f_{i}^{v}\|&y_{j}=y_{i}\\ *{arg\,min}_{ f_{i}^{v} F_{i}^{v}}\|_{m}(_{j})-f_{i}^{v}\|&y_{j} y_{i},\] (19)

where \(y_{q}\) and \(y_{i}\) represent the pseudo label of the query image and the dynamic prototype, respectively. \(\|\|\) denotes Euclidean norm. We obtain infrared prototype \(dm_{j}^{r}\) through the same method.

The overall optimization goal of DPCL is as follows:

\[_{DPCL}^{v}=}_{i N_{v}}|} _{ V_{i}}(_{m}() dm_{i} ^{v}/)}{_{j N_{v}}(_{m}( ) dm_{j}^{v}/)},\] (20)

\[_{DPCL}^{r}=}_{i N_{r}}|} _{r R_{i}}(_{m}() dm_{i}^{r}/ )}{_{j N_{r}}(_{m}()  dm_{j}^{r}/)},\] (21)

\[_{DPCL}=_{DPCL}^{v}+_{DPCL}^{r},\] (22)

where \(dm_{i}^{v(r)}\) is the positive dynamic prototype representation, i.e., the query image and dynamic prototype have the same identity.

DPCL promotes a flexible and adaptable learning process, aiming to minimize discrepancies between samples and their respective dynamic prototypes, rather than rigidly aligning query images with a fixed prototype.

### Progressive Contrastive Learning

In the initial training phases, representations are generally of lower quality. Introducing hard samples at this period could be counterproductive, potentially leading the model optimization in an incorrect direction right from the start [47; 57]. To address this issue, we introduce the Progressive Contrastive Learning, which forms the overall loss function:

\[_{PCLHD}=\{_{CPCL},& E_{}\\ _{HPCL}+(1-)_{DPCL},&.\] (23)

where \(\) is the loss weight, \(E_{}\) is a hyper-parameter.

## 4 Experiment

We conduct extensive experiments to validate the superiority of our proposed method. First, we provide the detailed experiment setting, which contains datasets, evaluation protocols, and implementation details. Then, we compare our method with many state-of-the-art VI-ReID methods and conduct ablation studies. In addition, to better illustrate our method, we also exhibit further analysis. If not specified, we conduct analysis experiments on SYSU-MM01 in the all-search mode.

### Experiment Setting

**Dataset.** We evaluate our method on two common benchmarks in VI-ReID: **SYSU-MM01** and **RegDB**. SYSU-MM01 is a large-scale public benchmark for the VI-ReID task, which contains 491 identities captured by four RGB cameras and two IR cameras in both outdoor and indoor environments. In this dataset, 22,258 RGB images and 11,909 IR images with 395 identities are collected for training. In the inference stage, the query set consists of 3,803 IR images with 96 identities and the galley set contains 301 randomly selected RGB images. RegDB is collected by an RGB camera and an IR camera, which contains 4,120 RGB images and 4,120 IR images with 412 identities. To be specific, the dataset is randomly divided into two non-overlapping sets: one set is used for training and the other is for testing.

**Evaluation Protocols.** The experiment follows the standard evaluation settings in VI-ReID, i.e., Cumulative Matching Characteristics (CMC)  and mean Average Precision (mAP).

**Implementation Details.** We adopt the feature extractor in AGW , which is initialized with ImageNet-pretrained weights to extract 2048-dimensional features. During the training stage, the

   &  &  \\   &  &  &  &  \\  Type & Method & Venue & Rank-1 & mAP & Rank-1 & mAP & Rank-1 & mAP & Rank-1 & mAP \\   & DDAG  & ECCV’20 & 54.8 & 53.0 & 61.0 & 68.0 & 69.4 & 63.5 & 68.1 & 61.8 \\  & AGW  & TPAMI’21 & 47.5 & 47.7 & 54.2 & 63.0 & 70.1 & 66.4 & 70.5 & 65.9 \\  & NFS  & CVPR’21 & 56.9 & 55.5 & 62.8 & 69.8 & 80.5 & 72.1 & 78.0 & 69.8 \\  & LbA  & ICCV’21 & 55.4 & 54.1 & 58.5 & 66.3 & 74.2 & 67.6 & 72.4 & 65.5 \\  & CAJ  & ICCV’21 & 69.9 & 66.9 & 76.3 & 80.4 & 85.0 & 79.1 & 84.8 & 77.8 \\  & MPANet  & CVPR’21 & 70.6 & 68.2 & 76.7 & 81.0 & 83.7 & 80.9 & 82.8 & 80.7 \\  & DART  & CVPR’22 & 68.7 & 66.3 & 72.5 & 78.2 & 83.6 & 75.7 & 82.0 & 73.8 \\  & FMCNet  & CVPR’22 & 66.3 & 62.5 & 68.2 & 74.1 & 89.1 & 84.4 & 88.4 & 83.9 \\  & MDI  & AAAI’22 & 60.3 & 59.4 & 64.9 & 70.1 & 87.5 & 84.9 & 84.3 & 81.4 \\  & LUPI  & ECCV’22 & 71.1 & 67.6 & 82.4 & 82.7 & 88.0 & 82.7 & 86.8 & 81.3 \\  & DEEN  & CVPR’23 & 74.7 & 71.8 & 80.3 & 83.3 & 91.1 & 85.1 & 89.5 & 83.4 \\  & SGLELE  & CVPR’23 & 77.1 & 72.3 & 82.1 & 83.0 & 92.2 & 86.6 & 91.1 & 85.2 \\  & PartMix  & CVPR’23 & 77.8 & 74.6 & 81.5 & 84.4 & 85.7 & 82.3 & 84.9 & 82.5 \\  & CAL  & ICCV’23 & 74.7 & 71.7 & 79.7 & 83.7 & 94.5 & 88.7 & 93.6 & 87.6 \\  & MUN  & ICCV’23 & 76.2 & 73.8 & 79.4 & 82.1 & 95.2 & 87.2 & 91.9 & 85.0 \\  & SAAI  & ICCV’23 & 75.9 & 77.0 & 83.2 & 88.0 & 91.1 & 91.5 & 92.1 & 92.0 \\  & FDNM  & arXiv’24 & 77.8 & 75.1 & 87.3 & 89.1 & 95.5 & 90.0 & 94.0 & 88.7 \\  & LCNL  & IJCV’24 & 70.2 & 68.0 & 76.2 & 80.3 & 85.6 & 78.7 & 84.0 & 76.9 \\   & OITA  & ECCV’22 & 48.2 & 43.9 & 47.4 & 56.8 & 49.9 & 41.8 & 49.6 & 42.8 \\  & TAA  & TIP’23 & 48.8 & 42.3 & 50.1 & 56.0 & 62.2 & 56.0 & 63.8 & 56.5 \\  & DPIs  & ICCV’23 & 58.4 & 55.6 & 63.0 & 70.0 & 62.3 & 53.2 & 61.5 & 52.7 \\   & HPH  & TIP’21 & 30.2 & 29.4 & - & - & 23.8 & 18.9 & - & - \\  & OITA  & ECCV’22 & 29.9 & 27.1 & 29.8 & 38.8 & 32.9 & 29.7 & 32.1 & 28.6 \\  & ADCA  & MM’22 & 45.5 & 42.7 & 50.6 & 59.1 & 67.2 & 64.1 & 68.5 & 63.8 \\  & NIGL  & MM’23 & 50.4 & 47.4 & 53.5 & 61.7 & 85.6 & 76.7 & 82.9 & 75.0 \\  & MSCM  & MM’23 & 53.1 & 48.2 & 55.2 & 62.0 & 83.8 & 77.9 & 82.8 & 76.7 \\  & CCLNet  & MM’23 & 54.0 & 50.2 & 56.7 & 65.1 & 69.9 & 65.5 & 70.2 & 66.7 \\  & PGM  & CVPR’23 & 57.3 & 51.8 & 56.2 & 62.7 & 69.5 & 65.4 & 69.9 & 65.2 \\  & GUR’*  & ICCV’23 & 61.0 & 57.0 & 64.2 & 69.5 & 73.9 & 70.2 & 75.0 & 69.9 \\  & MMM  & ECCV’24 & 61.6 & 57.9 & 64.4 & 70.4 & 89.7 & 80.5 & 85.8 & 77.0 \\   & **PCLHD** & **Ours** & **64.4** & **58.7** & **69.5** & **74.4** & **84.3** & **80.7** & **82.7** & **78.4** \\  & **PCHD+MMM** & **Enhanced** & **65.9** & **61.8** & **70.3** & **74.9** & **89.6** & **83.7** & **87.0** & **80.9** \\  

Table 1: Comparisons with state-of-the-art methods on SYSU-MM01 and RegDB, including SVI-ReID, SSVI-ReID and USVI-ReID methods. All methods are measured by Rank-1 (%) and mAP (%). GUR* denotes the results without camera information.

   &  &  &  \\  Index & Baseline & HPCL & DPCL & PCL & Rank-1 & mAP & Rank-1 & mAP \\ 
1 & ✓ & & & & 56.3 & 51.7 & 60.5 & 66.2 \\
2 & ✓ & & ✓ & & 59.1 & 54.4 & 63.6 & 68.8 \\
3 & ✓ & ✓ & & & 62.1 & 56.8 & 65.2 & 69.8 \\
4 & ✓ & ✓ & ✓ & & 63.7 & 57.8 & 67.0 & 72.6 \\
5 & ✓ & ✓ & ✓ & ✓ & & 64.4 & 58.7 & 69.5 & 74.4 \\  

Table 2: Ablation studies on SYSU-MM01 in all search mode and indoor search mode. “Baseline” means the model trained following PGM . Rank-R accuracy(%) and mAP(%) are reported.

input images are resized to 288\(\)144. We follow augmentations in CAJ  for data augmentation. In one batch, we randomly sample 16 pseudo identities, and each pseudo identity samples 16 instances. We set \(M\) to be 16 for computational convenience. The number of epochs is 100, in which the first 50 epochs are trained by contrastive loss with the centroid prototype. For the last 50 epochs, we train the model by contrastive loss with both the hard and dynamic prototypes. \(E_{CPCL}\) is 50. At the beginning of each epoch, we utilize the DBSCAN  algorithm to generate pseudo labels. During the inference stage, we use the momentum encoder \(_{m}\) to extract features and take the features of the global average pooling layer to calculate cosine similarity for retrieval. The momentum value \(\) and \(\) is set to 0.1 and 0.999, respectively. The temperature hyper-parameter \(\) is set to 0.05 and the weighting hyper-parameter \(\) in Eq.(23) is 0.5.

### Results and Analysis

To comprehensively evaluate our method, we compare our method with 18 supervised VI-ReID methods, 3 semi-supervised VI-ReID methods, and 9 unsupervised VI-ReID methods. The comparison results on the SYSU-MM01 and RegDB are reported in Tab. 1.

**Comparison with USVI-ReID Methods.** As shown in Tab. 1, our method achieves superior performance compared with state-of-the-art USVI-ReID methods. MMM  is proposed to establish reliable cross-modality correspondences and is also the current best-performing method. Our method with MMM can achieve 65.9% in Rank-1 and 61.8% in mAP, which surpasses that of MMM by a large margin of 4.3% and 3.9%. Notably, our method even without MMM gains the best performance with 64.4% in Rank-1 and 58.7% in mAP. Although existing USVI-ReID methods mentioned in Tab. 1 have made great progress in the USVI-ReID task, the neglects of divergence and variety hinder their further improvement. They overlook divergence and variety, which often constitutes hard samples. Thus, we propose progressive contrastive learning with hard and dynamic prototypes to mine hard samples, which can guide the model to learn more robust and discriminative features.

**Comparison with SSVI-ReID Methods.** There are three SSVI-ReID methods proposed to alleviate the problem of labeling cost by using a part of annotations. Remarkably, our method achieves superior performance without any annotations, outperforming all existing SSVI-ReID methods that utilize partial annotations. Moreover, the results suggest that our method can significantly reduce the dependency on manual annotations.

**Comparison with SVI-ReID Methods.** Surprisingly, our method without annotation outperforms several SVI-ReID methods, e.g., DDAG , AGW , NFS , LbA . This shows the immense competitiveness of PCLHD compared to SSVI-ReID methods that rely on complete data annotations. The superior performance of PCLHD mainly benefits from the hard prototype and dynamic prototype contrastive learning. Additionally, we have to acknowledge that a significant disparity still exists between PCLHD and the state-of-the-art fully-supervised results.

### Ablation Study

We conduct ablation studies on the SYSU-MM01 dataset in both all-search and indoor-search modes to show the effectiveness of each component in our method. The results are shown in Tab. 2.

**Baseline Settings.** We use PGM  as our baseline. Although PGM has achieved a promising performance on the USVI-ReID task, the neglect of hard samples hinders its further improvement.

**Effectiveness of HPCL.** The HPCL is proposed to mine divergence. As shown in Tab. 2, When adding the HPCL on Baseline, the performance improves a large margin of 5.8% in Rank-1 and 5.1% in mAP, respectively. It shows that divergence can be effectively mined using hard prototype contrast learning, facilitating the model to learn more discriminative features.

**Effectiveness of DPCL.** The DPCL is proposed to mine variety. The results show that Rank-1 accuracy can be improved by 2.8% in Rank-1 and 2.7% in mAP when adding the DPCL on Baseline, which confirms that contrastive learning with dynamic prototype can learn variety.

**Effectiveness of PCL.** PCL is introduced to smoothly shift the model's attention from commonality to divergence and variety. The results show that Rank-1 accuracy can be improved by about 1% in Rank-1 and mAP compared to adding simultaneously the HPCL and DPCL on the Baseline. This confirms that progressive contrastive learning plays a valuable role in assisting HPCL and DPCL.

Surprisingly, contrastive learning with both hard and dynamic prototypes significantly exceeds the baseline by a large margin of 8.1% in Rank-1 and 7.0% in mAP. The HPCL and DPCL can complement each other to learn divergence and variety, which effectively guides the network to learn more robust and discriminative features.

### Further Analysis

**Hyper-parameter analysis.** Hyper-parameter \(\) is a weighting parameter to trade-off \(L_{HPCL}\) and \(L_{DPCL}\). Fig. 2 (a) presents the results under different values of \(\). We can observe that when \(\) is small, i.e., \(L_{DPCL}\) contributes more to the model, the performance degrades. However, when \(\) is large, i.e., \(L_{HPCL}\) contributes heavily to the model, the model both achieves superior performance. Note that when \(=1\), i.e., the proposed method is trained without DPCL, the performance drops significantly. \(\) is finally set to 0.5 and our method achieves the best performance of 64.4% in Rank-1. Moreover, we also analyze the effect of the number of hard samples at hard prototype. As shown in Fig. 2 (b), we vary the \(k\) from 1 to 3 and keep the other hyper-parameters fixed, which shows that PCLHD achieves the best performance when \(k=1\). Hard samples are distributed in multiple directions, so multiple hard samples cannot be represented by a single prototype. This is why using more hard samples as prototypes leads to a decline in overall performance

**The ARI metric.** Following MMM , we utilize the Adjusted Rand Index (ARI) metric for clustering evaluation. The larger the ARI value, the higher the clustering quality. In Fig. 2 (c), "RGB" and "IR" denote the ARI values of visible and infrared clusterings, which can measure the quality of visible and infrared pseudo-labels. "ALL" means the ARI values of overall clusterings, which can evaluate the reliability of cross-modality correspondences. PCLHD surpasses other methods significantly on all of the mentioned ARI values, which demonstrates PCLHD can effectively mine divergence and variety to improve clustering quality.

**Visualization.** As shown in Fig. 3, we visualize the t-SNE map of 10 randomly chosen identities from SYSU-MM01. Compared to the baseline, the distribution of the same identity from the same modality is more compact and the distance of the same identity from different modalities is closer together. Moreover, some hard samples in the baseline are incorrectly clustered, while these hard samples are well clustered in our PCLHD, which shows the effectiveness of the proposed PCLHD.

Figure 3: The t-SNE visualization of 10 randomly selected identities. Different color indicates different IDs. Circle means visible features and the pentagram means infrared features.

Figure 2: (a) The effect of hyper-parameter \(\) with different values. (b) The effect of hyper-parameter \(k\) with different values. (c) Comparisons with ARI values of different methods.

Conclusion and Limitation

In this paper, we propose a novel method for USVI-ReID called Progressive Contrastive Learning with hard and dynamic prototype (PCLHD), which learns commonality, divergence and variety. To be specific, we design Hard Prototype Contrastive Learning to mine divergent yet significant information and Dynamic Prototype Contrastive Learning to preserve intrinsic variety features. Furthermore, we introduce a progressive learning strategy to incorporate both HPCL and DPCL into the model. Extensive experiments demonstrate that PCLHD outperforms state-of-the-art USVI-ReID methods.

This work relies on DBSCAN to generate pseudo-labels. However, for extremely large-scale datasets, DBSCAN's performance may be limited, which could affect the overall effectiveness of our approach. To address the limitation, we plan to explore hierarchical clustering in future research to better handle large-scale datasets.

## Broader Impacts

This work was developed using publicly available datasets and aims to enhance the capabilities of VI-ReID, which plays a vital role in scenarios where traditional ReID systems fail, such as in low-light or nighttime conditions. VI-ReID offers significant benefits in improving security and surveillance by enabling more reliable identification across varying environmental conditions. Importantly, this work raises no ethical, safety, or environmental concerns, and no harm was inflicted on living beings during the research. However, we acknowledge the risk of misuse, particularly privacy invasion if used to track individuals in public spaces without appropriate regulation. While VI-ReID does not directly identify specific individuals, its unauthorized deployment could still result in significant privacy violations. Therefore, public surveillance systems using VI-ReID should be controlled by authorized entities, ensuring proper regulatory frameworks and adherence to ethical standards.