# Data-Efficient Learning with Neural Programs

Alaia Solko-Breslin, Seewon Choi, Ziyang Li, Neelay Velingker,

**Rajeev Alur, Mayur Naik, Eric Wong**

University of Pennsylvania

{alaia,seewon,liby99,neelay,alur,mhnaik,exwong}@seas.upenn.edu

###### Abstract

Many computational tasks can be naturally expressed as a composition of a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites "neural programs" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite. When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as GPT-4 and also consider benchmarks from the neurosymbolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner. 1

## 1 Introduction

Many computational tasks cannot be solved by neural perception alone but can be naturally expressed as a composition of a neural model \(M_{}\) followed by a program \(P\) written in a traditional programming language or an API call to a large language model (LLM). We call such composites "neural programs" and study the problem of learning neural programs in an end-to-end manner with a focus on data and sample efficiency. One problem that is naturally expressed as a neural program is scene recognition , where \(M_{}\) classifies objects in an image and \(P\) prompts GPT-4 to identify the room type given these objects (Fig. 1).

Neurosymbolic learning  is one instance of neural program learning in which \(P\) takes the form of a logic program. DeepProbLog (DPL)  and Scallop  are frameworks that extend ProbLog and Datalog, respectively, to ensure that the symbolic component \(P\) is differentiable. This differentiability requirement is what facilitates learning in many neurosymbolic learning frameworks. There are also abductive learning frameworks that do not explicitly differentiate programs. Instead, they require that the symbolic component expose a method for abducing the function's inputs for a given output, often using Prolog for the symbolic component as a result [6; 23]. While logic programming languages are expressive enough for these frameworks to solve tasks such as sorting , visual question answering , and path planning , they offer restricted features and a narrow range of libraries, making them incompatible with calls to arbitrary APIs or to modern LLMs.

Learning neural programs when \(P\) is not expressed as a logic program is a difficult problem because gradients across black-box programs cannot be computed explicitly. One possible solution is to use REINFORCE  to sample symbols from distributions predicted by \(M_{}\) and compute the expectedreward using the output label. However, REINFORCE is not sample-efficient as it produces a weak learning signal, especially when applied to programs with a large number of inputs. There are other REINFORCE-based methods that can be applied to the neural program learning setting, namely IndeCateR  and Neural Attention for Symbolic Reasoning (NASR) . However, IndeCateR struggles with sample efficiency despite providing lower variance than REINFORCE, and NASR performs poorly when intermediate labels are unavailable for pretraining. Another possible solution is Approximate Neurosymbolic Inference (A-NeSI) , which trains a neural network to estimate the gradient of \(P\), but learning the surrogate neural network becomes more difficult as the complexity of \(P\) increases. Moreover, the additional neural models in the learning framework in A-NeSI results in data inefficiency.

In this paper, we propose an algorithm for learning neural programs, based on reinforcement learning, which is compatible with arbitrary programs. Our approach, called ISED (Infer-Sample-Estimate-Descend), yields a framework that expands the applicability of neural program learning frameworks by providing a data- and sample-efficient method of training neural models with randomly initialized weights. ISED uses outputs of \(M_{}\) as a probability distribution over inputs of \(P\) and samples representative symbols \(u\) from this distribution. ISED then computes outputs \(v\) of \(P\) corresponding to these symbols. The resulting symbol-output pairs can be viewed as a symbolic program consisting of clauses of the form if symbol\(=u\)then output\(=v\) summarizing \(P\). The final step is to estimate the gradient across this symbolic summary, inspired by ideas from the neurosymbolic learning literature, to propagate loss across the composite model.

Our evaluation considers 16 neural program benchmark tasks. Our results show that ISED outperforms purely neural networks and CLIP  on neural program tasks involving GPT-4 calls. Additionally, ISED outperforms neurosymbolic methods on 9 of the 14 benchmarks tasks that can be encoded in logic programming languages. ISED is also the top performer on 8 out of the 16 benchmark tasks when compared to REINFORCE-based and black-box gradient estimation baselines. Furthermore, we show that ISED is more data- and sample-efficient than baseline methods.

In summary, the main contributions of this paper are as follows: 1) we introduce neural programs as a generalization of neurosymbolic programs, 2) we introduce new tasks involving neural programs that use Python and calls to GPT-4 called neuroPython and neuroGPT programs, respectively, 3) we present ISED, a general algorithm for data- and sample-efficient learning with neural programs, and 4) we conduct a thorough evaluation using existing techniques against a diverse set of benchmarks.

## 2 Neural Programs

**Problem Statement.** In the neural program learning setting, we attempt to optimize model parameters \(M_{}\) which are being supervised by a fixed program \(P\). Specifically, we are given a training dataset \(\) of length \(N\) containing input-output pairs, i.e., \(=\{(x_{1},y_{1}),(x_{N},y_{N})\}\). Each \(x_{i}\) represents unstructured data (e.g., image data) whose corresponding structured data (intermediate labels) are not given. Each \(y_{i}\) is the result of applying \(P\) to the structured data corresponding to \(x_{i}\). Given a loss function \(\), we want to minimize the loss of \((P(M_{}(x_{i})),y_{i})\) for each \((x_{i},y_{i})\) pair in order to optimize \(\). Loss minimization is straightforward when there is some mechanism for automatically differentiating programs, but we focus on the setting of optimizing \(\) without assuming the differentiability of \(P\). We now introduce three motivating applications that can be framed in this

Figure 1: Neural program decomposition for scene recognition.

setting, namely classifying images of leaves , scene recognition , and hand-written formula evaluation (HWF) .

**Leaf Classification.** We consider a real-world example that deals with the problem of classifying leaf images. Traditional neural methods predict the species directly, without explicit notion of leaf features such as margin, shape, and texture, resulting in solutions that are data-inefficient, inaccurate, and harder to understand.

We instead present a neural programming solution, making use of leaf classification decision trees . These decision trees allow identifying plant species based on the visible characteristics of their leaves. Here, the neural model takes a leaf image and predicts its shape, margin, and texture. The program can then be written in two ways: one implementation involves encoding the decision tree in Python; another involves constructing a prompt using the predicted leaf features and calling GPT-4 (see Fig. 2). The latter is possible because ISED allows the use of black-box programs, so programs can also use state-of-the-art foundation models such as GPT-4 for computation.

**Scene Recognition.** The goal of this task is to classify images according to their room types. The model receives an image from a scene dataset  and predicts among the 9 different room types: bedroom, bathroom, dining room, living room, kitchen, lab, office, house lobby, and basement.

The traditional neural solution trains a convolutional neural network that directly predicts the room type. On the other hand, the neural program solution decomposes the task into detecting objects in the scene and identifying the room type based on those objects. We use an off-the-shelf object detection model YOLOv8  and finetune it with a custom convolutional neural network to output labels related to scene recognition. We then make a GPT-4 call to predict the most likely room type given the list of detected objects.

**Hand-written Formula.** In this task, a model is given a list of hand-written symbols containing digits (0-9) and operators (\(+\), \(-\), \(\), and \(\)) . The dataset contains length 1-7 formulas free of syntax or divide-by-zero errors. The model is trained with supervision on the evaluated floating-point result without the label of each symbol. Since inputs are combinatorial and results are rational numbers, end-to-end neural methods struggle with accuracy. Meanwhile, neurosymbolic methods for this task either use specialized algorithms  or handcrafted differentiable programs .

With ISED, the program can be written in just a few lines of Python. It takes in a list of characters representing symbols, and simply invokes the Python eval function on the joined expression string. The hwf evaluation function can be used just like any other PyTorch  module since ISED internally performs sampling and probability estimation to estimate the gradient.

## 3 Learning Neural Programs

In this section, we present the intuition behind ISED and the values it approximates. Next, we introduce the programming interface for ISED, which lays the groundwork for presenting the algorithm. We then formally describe the steps of ISED.

Figure 2: Illustration of our inference pipeline for the leaf classification task. leaf_id can be written with a decision tree (top program) or with a call to GPT-4 (bottom program).

### ISED Overview

Assuming \(P\) is a black-box, we can collect symbol-output samples \((u,v)\) from \(P\). Such collection of samples can be viewed as a _summary_ logic program consisting of rules of the form if\(r=u\) then\(y=v\). For instance, in the task of adding two digits \(r_{1}\) and \(r_{2}\), one rule of the logic program would be \(r_{1}=1 r_{2}=2 y=3\). Techniques from neurosymbolic literature via exact or approximate weighted model counting (WMC)  can then be used for computing the gradient across such a summary of \(P\). However, having the complete summary of all combinations of symbols is not feasible for a black-box \(P\). ISED samples symbols from the probability distribution predicted by the neural network \(M_{}\), evaluates \(P\) on each sample, and takes the gradient across this partial summary of \(P\). This is a good approximation of the complete summary since it is likely to contain symbols with high probability, which contribute the most in exact computation.

This approach differs from REINFORCE in how it differentiates through this summary of \(P\). REINFORCE rewards sampled symbols that resulted in the correct output through optimizing the log probability of each symbol, weighted by reward values. This weighted-sum style estimation provides a weaker learning signal compared to WMC used by ISED, making learning harder for REINFORCE as the number of inputs to \(P\) increases. See Appendix A for further details.

### Preliminaries and Programming Interface

ISED allows programmers to write black-box programs that operate on diverse structured inputs and outputs. To allow such programs to interact with neural networks, we define an interface named _structural mapping_. This interface serves to 1) define the data-types of black-box programs' input and output, 2) marshall and un-marshall data between neural networks and logical black-box functions, and 3) define the loss. We define a _structural mapping_\(\) as either a discrete mapping (with \(\) being the set of all possible elements), a floating point, a permutation mapping with \(n\) possible elements, a tuple of mappings, or a list of up to \(n\) elements. We define \(\) inductively as follows:

\[::=()_{n} (_{1},,_{m})_{n}()\]

Using this, we may further define data-types such as \(^{k}_{j}=(\{j,,k\})\), \(=^{9}_{0}\), and \(=(\{,\})\). These types give ISED the flexibility learn neural programs with diverse types of inputs and outputs, e.g., \(_{n}\) input and output types for integer list sorting and \(_{9}(_{9}())\) for sudoku solving.

We also define a _black-box program_\(P\) as a function \((_{1},,_{m})_{o}\), where \(_{1},,_{m}\) are the input types and \(_{o}\) is the output type. For example, the structural input mapping for the hand-written formula task is \(_{7}((\{0,,9,+,-,,\}))\), and the structural output mapping is Float. The mappings suggest that the program takes a list of length up to 7 as input, where each element is a digit or an arithmetic operator, and returns a floating point number.

There are two interpretations of a structural mapping: the set interpretation SET(\(\)) represents a mapping with defined values, e.g., a digit with value 8; the tensor interpretation \(()\) represents a mapping where each value is associated with a probability distribution, e.g., a digit that is 1 with probability 0.6 and 7 with probability 0.4. We use the set interpretation to represent structured program inputs that can be passed to a black-box program and the tensor interpretation to represent probability distributions for unstructured data and program outputs. These two interpretations are defined for the different structural mappings in Table 1.

In order to represent the ground truth output as a distribution to be used in the loss computation, there needs to be a mechanism for transforming SET(\(\)) mappings into DIST(\(\)) mappings. For

  
**Mapping (\(\))** & **Set Interpretation (SET(\(\)))** & **Tensor Interpretation (DIST(\(\)))** \\  \(_{2}\) & \(\) & \(\{^{||},v_{i},i 1| |\}\) \\  Float & \(\) & n/a \\  \(_{n}\) & \(\{[1,...,n]\}\) & \(\{[v_{1}^{n},...,v_{n}^{n}] v_{i}^{n}^{n},v_{i,j},i  1 n\}\) \\  \((_{1},,_{m})\) & \(\{(a_{1},...,a_{m}) a_{i}(_{i})\}\) & \(\{(a_{1},...,a_{m}) a_{i}(_{i})\}\) \\  \(_{n}(^{})\) & \(\{[a_{1},...,a_{j}] j n,a_{i}(^{})\}\) & \(\{[a_{1},...,a_{j}] j n,a_{i}(^{})\}\) \\   

Table 1: Set and tensor interpretations of different structural mappings.

this purpose, we define a _vectorize_ function \(_{}:((),2^{})()\) for the different output mappings \(\) in Table 2. When considering a datapoint \((x,y)\) during training, ISED samples many symbols and obtains a list of outputs \(\). The vectorizer then takes the ground truth \(y\) and the outputs \(\) as input and returns the equivalent distribution interpretation of \(y\). While \(\) is not used by \(_{}\) in most cases, we include it as an argument so that Float output mappings can be discretized, which is necessary for vectorization. For example, if the inputs to the vectorizer for the hand-written formula task are \(y=2.0\) and \(=[1.0,3.5,2.0,8.0]\), then it would return \(\).

We also require a mechanism to aggregate the probabilities of sampled symbols that resulted in a particular output. With this aim, we define an _aggregate_ function \(_{}:((),())\) for different input mappings \(\) in Table 2. ISED aggregates probabilities either by taking their minimum or their product, and we denote both operations by \(\). The aggregator takes as input sampled symbols \(\) and neural predictions \(\) from which \(\) was sampled. It gathers values in \(\) at each index in \(\) and returns the result of \(\) applied to these values. For example, suppose we use min as the aggregator \(\) for the hand-written formula task. Then if \(\) takes \(=[1,+,1]\) and \(\) as inputs where \(=0.1\), \([+]=0.05\), and \(=0.1\), it would return \(0.05\).

### Algorithm

We now formally present the ISED algorithm. For a given task, there is a black-box program \(P\), taking \(m\) inputs, that operates on structured data. Let \(_{1},...,_{m}\) be the mappings for these inputs and \(_{o}\) the mapping for the program's output. We write \(P\) as a function from its input mappings to its output mapping: \(P:(_{1},...,_{m})_{o}\). For each unstructured input \(i\) to the program, there is a neural model \(M^{i}_{_{i}}:x_{i}(_{i})\). \(S\) is a sampling strategy (e.g., categorical sampling) that samples symbols using the outputs of a neural model, and \(k\) is the number of samples to take for each training example. There is also a loss function \(\) whose first and second arguments are the predicted and ground truth values respectively. We present the pseudocode of the algorithm in Algorithm 1 and describe its steps with the hand-written formula task:

**Infer.** The training pipeline starts with an example from the dataset, \((x,y)=\) (\(\), \(\), \(\), \(\)], 3.0), and uses a CNN to predict these images, as shown on lines 3-4. ISED initializes \(=M_{}(x)\).

**Sample.** ISED samples \(\) from \(\) for \(k\) iterations using sampling strategy \(S\). For each sample \(j\), the algorithm initializes \(_{j}\) to be the sampled symbols, as shown on lines 6-9. To continue our example, suppose ISED initializes \(_{j}=[7,+,2]\) for sample \(j\). The next step is to execute the program on \(_{j}\), as shown on line 10, which in this example means setting \(_{j}=P(_{j})=9.0\).

**Estimate.** In order to compute the prediction value to use in the loss function, ISED must consider each output \(y_{l}\) in the output mapping and accumulate the aggregated probabilities for all sampled symbols that resulted in the output \(y_{l}\). We specify \(\) as the min function, and \(\) as the max function in this example. Note that ISED requires that \(\) and \(\) represent either min and max or mult and add respectively. We refer to these two options as the min-max and add-mult semirings. We define an _accumulate_ function \(\) that takes as input an element of the output mapping \(y_{l}\), sampled outputs \(\), sampled symbols \(\), and predicted input distributions \(\). The accumulator performs the \(\) operation on aggregated probabilities for elements of \(\) that are equal to \(y_{l}\) and is defined as follows:

\[(y_{l},,,)=_{j=1}^{k}_{_{j}= y_{l}}_{_{o}}(_{j},_{j})\]

Continuing our example, suppose, among the samples, there are two symbolic combinations (\([7,+,2]\) and \([3,*,3]\)) that resulted in the output \(9.0\). Let us say that these sets of symbols had probabilities \([0.3,0.8,0.8]\) and \([0.1,0.1,0.1]\), respectively. Then the result of the probability aggregation for \(y_{l}=9.0\) would be \((9.0,,,)=(([0.3,0.8,0.8]), ([0.1,0.1,0.1]))=0.3\).

  
**Mapping (\(\))** & **Vectorizer (\(_{}(y,)\))** & **Aggregator (\(_{}(,)\))** \\  Discrete\({}_{n}\) & \(^{(y)}\) with \(\;\) & \([]\) \\  Float & \([_{y=_{l}}\;\;i[1,,()]]\) & \(\) \\  Permutation\({}_{n}\) & \([_{_{n}}(y|\;i[1,,n])]\) & \(_{i=1}^{n}_{_{n}}([],[])\) \\  Tuple\((_{1},,_{m})\) & \([_{_{f}}(y|\;i[1,,m])]\) & \(_{i=1}^{n}_{_{f}}([],[])\) \\  List ISED then sets \(=[(y_{l},,,)\) for \(y_{l}_{o}]\) in the case where \(_{o}\) is not Float. When \(_{o}\) is Float, as for hand-written formula, it only considers \(y_{l}\). Next, it performs \(L_{2}\) normalization over each element in \(\) and sets \(\) to this result. To initialize the ground truth vector, it sets \(w=(y,)\). ISED then initializes \(l=(,w)\) and computes \(}\) for each input \(i\). These steps are shown on lines 12-15. In our running example, since \(9.0\) is an incorrect output, the probability of the first symbol being equal to \(7\) (instead of the correct answer \(1\)) will be penalized while the probabilities for predicting other symbols are unchanged.

**Descend.** The last step is shown on line 16, where the algorithm optimizes \(_{i}\) for each input \(i\) based on \(}\) using a stochastic optimizer (e.g., Adam optimizer). This completes the training pipeline for one example, and the algorithm returns all final \(_{i}\) after iterating through the entire dataset.

```
0:\(P\) is the black-box program \((_{1},,_{m})_{o}\), \(M^{i}_{_{i}}\) the neural model \(x_{i}(_{i})\) for each \(_{i}\), \(S\) the sampling strategy, \(k\) the sample count, \(\) the loss function, and \(\) the dataset.
1:procedureTRAIN
2:for\(((x_{1}, x_{m}),y)\)do
3:for\(i 1 m\)do
4:\([i] M^{i}_{_{i}}(x_{i})\)\(\)Infer
5:endfor
6:for\(j 1 k\)do
7:for\(i 1 m\)do
8: Sample \(_{j}[i]\) from \([i]\) using \(S\)\(\)Sample
9:endfor
10:\(_{j} P(_{j})\)
11:endfor
12:\(([(y_{l},,,)y_{l}_{o}y_{l})])\)\(\)Estimate
13:\(w(y,)\)
14:\(l(,w)\)
15: Compute \(\) by performing back-propagation on \(l\)
16: Optimize \(\) based on \(\)\(\)Descend
17:endfor
18:endprocedure ```

**Algorithm 1** ISED training pipeline

## 4 Evaluation

In this section, we evaluate ISED and aim to answer the following research questions:

**RQ1:** How does ISED compare to state-of-the-art neurosymbolic, REINFORCE-based, and gradient estimation baselines in terms of accuracy?

**RQ2:** What is the sample efficiency of ISED when compared to REINFORCE-based algorithms?

**RQ3:** How data-efficient is ISED compared to neural gradient estimation methods?

### Benchmark Tasks: NeuroGPT, NeuroPython, and Neurosymbolic

We first introduce two new neural program learning benchmarks which both contain a program component that can make a call to GPT-4. We call such models neuroGPT programs.

**Leaf Classification.** In this task, we use a dataset, which we call LEAF-ID, containing leaf images of 11 different plant species , containing 330 training samples and 110 testing samples. We define custom Discrete types Margin, Shape, Texture. With this, we define Leaf-Traits = Tuple(Margin, Shape, Texture) and Leaf-Output to be the Discrete set of 11 plant species in the dataset. Neural program solutions either prompt GPT-4 (GPT leaf) or use a decision tree (DT leaf).

**Scene Recognition.** We use a dataset containing scene images from 9 different room types , consisting of 830 training examples and 92 testing examples. We define custom types Objects and Scenes to be Discrete set of 45 objects and 9 room types, respectively. We freeze the parametersof YOLOv8 and only optimize the custom neural network. The neural program solution prompts GPT-4 to classify the scene.

We also consider several tasks from the neurosymbolic literature, including hand-written formula (HWF) evaluation and Sudoku solving. While the solutions to many of these tasks are usually presented as a logic program in neurosymbolic learning frameworks, neural program solutions can take the form of Python programs. We call such models neuroPython programs.

**MNIST-R.** MNIST-R [13; 14] contains 11 tasks operating on inputs of images of handwritten digits from the MNIST dataset . This synthetic test suite includes tasks performing arithmetic (sum\({}_{2}\), sum\({}_{3}\), sum\({}_{4}\), mult\({}_{2}\), mod\({}_{2}\), add-mod-3, add-sub), comparison (less-than, equal), counting (count-3-or-4), and negation (not-3-or-4) over the digits depicted in the images. Each task dataset has a training set of 5K samples and a testing set of 500 samples.

**HWF.** The goal of the HWF task is to classify images of handwritten digits and arithmetic operators and evaluate the formula . The dataset contains 10K formulas of length 1-7, with 1K length 1 formulas, 1K length 3 formulas, 2K length 5 formulas, and 6K length 7 formulas.

**Visual Sudoku.** The goal of this task is to solve an incomplete 9x9 Sudoku, where the problem board is given as MNIST digits. We follow the experimental setting of NASR , including their pre-trained MNIST digit recognition models and sudoku solvers. We use the SatNet dataset consisting of 9K training samples and 500 test samples .

### Evaluation Setup and Baselines

All of our experiments were conducted on a machine with two 20-core Intel Xeon CPUs, one NVIDIA RTX 2080 Ti GPU, and 755 GB RAM. Unless otherwise noted, the sample count, i.e., the number of calls to the program \(P\) per training example, is fixed at 100 for all relevant methods. For additional details on experimental setup, see Appendix B. We apply a timeout of 10 seconds per testing sample, and report the average accuracy and 1-sigma standard deviation obtained from 10 randomized runs.

We pick as baselines neurosymbolic methods DeepProbLog (DPL)  and Scallop , A-NeSI  which performs neural approximation of the gradients, and sampling-based gradient approximation methods REINFORCE , IndeCateR , and NASR . IndeCateR achieves provably lower variance than REINFORCE by using a specialized sampling method (Appendix A), and NASR is a variant specialized for efficient finetuning by using a single sample and a custom reward function. We also use purely neural baselines and CLIP  for GPT leaf and scene. CLIP is a multimodal model that supports zero-shot image classification by simply providing names of the output categories.

### RQ1: Performance and Accuracy

To answer **RQ1**, we evaluate ISED's accuracy against those of the baselines. ISED matches, and in many cases surpasses, the accuracy of neurosymbolic and gradient estimation baselines. We highlight the results for sum\({}_{n}\) from MNIST-R and other benchmarks in Table 3. Tables 7-10 in Appendix C

    \\ 
**Method** & sum\({}_{2}\) & sum\({}_{3}\) & sum\({}_{4}\) & HWF & DT leaf & GPT leaf & scene & sudoku \\   DPL & \(95.14\) & \(93.80\) & TO & TO & \(39.70\) & N/A & N/A & TO \\ Scallop & \(91.18\) & \(91.86\) & \(80.10\) & \(96.65\) & \(81.13\) & N/A & N/A & TO \\  A-NeSI & \(\) & \(94.39\) & \(78.10\) & \(3.13\) & \(78.82\) & \(72.40\) & \(61.46\) & \(26.36\) \\  REINFORCE & \(74.46\) & \(19.40\) & \(13.84\) & \(88.27\) & \(40.24\) & \(53.84\) & \(12.17\) & \(79.08\) \\ IndeCateR & \(96.48\) & \(93.76\) & \(92.58\) & \(95.08\) & \(78.71\) & \(69.16\) & \(12.72\) & \(66.50\) \\ NASR & \(6.08\) & \(5.48\) & \(4.86\) & \(1.85\) & \(16.41\) & \(17.32\) & \(2.02\) & \(\) \\ ISED (ours) & \(80.34\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(80.32\) \\   

Table 3: Performance on selected benchmarks. “TO” means time-out, and “N/A” means the task could not be programmed in the framework. Methods are divided (from top to bottom) by neurosymbolic, black-box gradient estimation, and REINFORCE-based.

contain results for the remaining MNIST-R tasks, including standard deviations for all tasks. ISED is the top performer on 8 out of the 16 total tasks.

On the GPT leaf and scene tasks, ISED outperforms the purely neural baseline by \(3.82\%\) and \(31.42\%\) respectively, and zero-shot CLIP by \(59.80\%\) and \(17.50\%\). For many tasks, A-NeSI is the non-neurosymbolic method that comes closest to ISED, sometimes outperforming our method. However, A-NeSI achieves significantly lower performance than ISED on tasks involving complex programs, namely HWF and sudoku. This is likely due to the difficulty of training a neural model to estimate the output of \(P\) and its gradient when \(P\) is complex. ISED also outperforms REINFORCE on all but 3 tasks due to the REINFORCE learning signal being weaker for tasks where \(P\) involves multiple inputs. NASR outperforms ISED only on sudoku by \(2.46\%\) due to NASR being well-suited for fine-tuning as it restricts its algorithm to use a single sample. IndeCateR achieves similar performance compared to ISED on most tasks but achieves significantly lower accuracy on the scene classification task, which has a large input space with maximum 10 objects each with 47 possible values in each scene, demonstrating that IndeCateR is less sample-efficient than ISED. We elaborate more on this point in **RQ2**.

ISED outperforms the neurosymbolic methods on 9 out of 14 tasks that can be written in logic programming languages. Despite treating \(P\) as a black-box, ISED even outperforms Scallop on HWF by \(0.69\%\) and comes within \(1.16\%\) of NGS, a specialized neurosymbolic learning framework that uses abductive reasoning . Furthermore, DPL timed out on 4 tasks, and Scallop timed out on 1 (sudoku). These results demonstrate that even for tasks that can be written in a logic programming language, treating the program as a black-box can often yield optimal results.

### RQ2: Sample Efficiency

To answer **RQ2**, we evaluate the sample efficiency of ISED against REINFORCE, IndeCateR, and IndeCateR+ on adding MNIST digits. IndeCateR+  is a variant of IndeCateR with a sampling method and loss computation customized for higher dimensional setting such as the addition of 16 MNIST digits. We vary the size of the input and output space (sum\({}_{8}\), sum\({}_{12}\), sum\({}_{16}\)) of \(P\) as well as the sample count, and report the average accuracy and standard deviation obtained from 5 randomized runs (Tables 4, 11-13).

For a lower number of samples, ISED outperforms all other methods on the three tasks, outperforming IndeCateR by over \(80\%\) on sum\({}_{8}\) and sum\({}_{12}\). The experimental findings support the conceptual difference of REINFORCE-based methods providing a weak learning signal compared to ISED (Section 3.1). While ISED achieves accuracy similar to the top performer for sum\({}_{8}\) and sum\({}_{12}\) with a high sample count, it comes second on sum\({}_{16}\) with IndeCateR+ beating ISED by \(75.39\%\). This suggests our approach is limited in scaling to high-dimensional inputs to \(P\), and motivates exploring better sampling techniques, which is the core difference between IndeCateR and IndeCateR+.

### RQ3: Data Efficiency

We now examine how ISED compares to state-of-the-art baselines in terms of data efficiency. We compare ISED and A-NeSI in terms of training time and accuracy on sum\({}_{3}\) and sum\({}_{4}\). We choose these tasks for evaluation because A-NeSI has been shown to scale well to multi-digit addition tasks . Furthermore, these tasks come from the MNIST-R suite in which we use 5K training samples, which is less than what A-NeSI would have used in its evaluation (20K training samples for sum

    \\   & _{8}\)} & _{12}\)} & _{16}\)} \\
**Method** & \(k=80\) & \(k=800\) & \(k=120\) & \(k=1200\) & \(k=160\) & \(k=1600\) \\  REINFORCE & \(8.32\) & \(8.28\) & \(7.52\) & \(8.20\) & \(5.12\) & \(6.28\) \\ IndeCateR & \(5.36\) & \(\) & \(4.60\) & \(77.88\) & \(1.24\) & \(5.16\) \\ IndeCateR+ & \(10.20\) & \(88.60\) & \(6.84\) & \(\) & \(4.24\) & \(\) \\ ISED (Ours) & \(\) & \(87.72\) & \(\) & \(86.72\) & \(\) & \(8.13\) \\  

Table 4: Performance comparisons for sum\({}_{8}\), sum\({}_{12}\), and sum\({}_{16}\) with different sample counts \(k\).

and 15K for sum\({}_{4}\)). We plot the average test accuracy and standard deviation vs. training time (over 10 runs) in Figures 4 and 4, where each point represents the result of 1 epoch.

While ISED and A-NeSI learn at about the same rate for sum\({}_{3}\) after about 5 minutes of training, ISED learns at a much faster rate for the first 5 minutes, reaching an accuracy of \(88.22\%\) after just 2 epochs (Fig. 4). The difference between ISED and A-NeSI is more pronounced for sum\({}_{4}\), with ISED reaching an accuracy of \(94.10\%\) after just 10 epochs while A-NeSI reaches \(49.51\%\) accuracy at the end of its 23rd epoch (Fig. 4). These results demonstrate that with limited training data, ISED is able to learn more quickly than A-NeSI, even for simple tasks. This result is likely due to A-NeSI training 2 additional neural models in its learning pipeline compared to ISED, with A-NeSI training a prior as well as a model to estimate the program output and gradient.

## 5 Limitations and Future Work

The main limitation of ISED is the difficulty of scaling with the dimensionality of the space of inputs to the program \(P\). There are interesting future directions in adapting and expanding ISED for high dimensionality. Specifically, improvements to the sampling strategy could help adapt ISED to a complex space of inputs. Techniques can be borrowed from the field of Bayesian optimization where such large spaces have traditionally been studied. Furthermore, there is merit to systematically combining white-box and black-box methods. ISED is especially useful when logic programs fail to encode reasoning components. Therefore, we believe that ISED can be used as an underlying engine for a new neurosymbolic language that blends the accessibility of black-box with the performance of white-box methods.

## 6 Related Work

**Neurosymbolic programming frameworks.** These frameworks provide a general mechanism to define white-box neurosymbolic programs. DeepProbLog  and Scallop  abstract away gradient calculations behind a rule-based language. Others specialize in targeted applications, such as NeurASP  for answer set programming, or NeuralLog  for phrase alignment in NLP. ISED is similar in that it seeks to make classes of neurosymbolic programs easier to write and access; however, it diverges by offering an interface not bound by any specific domain or language syntax.

**RL and sampling-based neurosymbolic frameworks.** ISED incorporates concepts found in the RL algorithm REINFORCE  such as the sampling of actions according to the current policy distribution, similar to NASR , and IndeCateR . Other work has proposed a semantic loss function for neurosymbolic learning which measures how well neural network outputs match a given constraint . While this technique resembles ISED in that it samples symbols from their predicted distributions to derive the loss, it relies on symbolic knowledge in the form of a constraint in Boolean logic, whereas ISED allows the program component to be any black-box program.

**Specialized neurosymbolic methods.** The majority of the neurosymbolic learning literature pertains to point solutions for specific use cases [7; 25]. In the HWF example, NGS  and several of its variants leverage a hand-defined syntax defining the inherent structure within mathematical expressions. Similarly, DiffSort  leverages the symbolic properties of sorting to produce differentiable sorting networks. Other point solutions address broader problem setups, such as NS-CL  which provides a framework for visual question answering by learning symbolic representations in text and images. For reading comprehension, the NeRd  framework converts NL questions into executable programs over symbolic information extracted from text. ISED aligns with all of these point solutions by aiming to solve problems that have thus far required technically specific solutions in order to access the advantages of neurosymbolic learning, but it takes an opposite and easier approach by forgoing significant specializations and instead leverages existing solutions as black-boxes.

**Differentiable programming and non-differentiable optimization.** Longstanding libraries in deep learning have grown to great popularity for their ability to abstract away automatic differentiation behind easy-to-use interfaces. PyTorch  is able to do so by keeping track of a dynamic computational graph. Similarly, JAX  leverages functional programming to abstract automatic differentiation. ISED follows the style of these frameworks by offering an interface to abstract away gradient calculations for algorithms used in deep learning, but ISED improves upon them by allowing systematic compatibility of non-differentiable functions.

## 7 Conclusion

We proposed ISED, a data- and sample-efficient algorithm for learning neural programs. Unlike existing general neurosymbolic frameworks which require differentiable logic programs, ISED is compatible with Python programs and API calls to GPT, and it employs a sampling-based technique to learn neural model parameters using forward evaluation. We showed that for neuroGPT, neuroPython, and neurosymbolic benchmarks, ISED achieves better accuracy than end-to-end neural models and similar accuracy compared to neurosymbolic frameworks. ISED also often achieves superior accuracy on complex programs compared to REINFORCE-based and gradient estimation baselines. Furthermore, ISED learns in a more data- and sample-efficient manner compared to these baselines.

## 8 Acknowledgements

We thank the anonymous reviewers for useful feedback. This research was supported by ARPA-H grant D24AC00253-00, NSF award CCF 2313010, and by a gift from AWS AI to ASSET (Penn Engineering Center on Trustworthy AI).