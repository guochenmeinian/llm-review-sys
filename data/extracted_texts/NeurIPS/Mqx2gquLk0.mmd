# Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms

Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms

 Thanh Nguyen-Tang

Department of Computer Science

Johns Hopkins University

Baltimore, MD 21218

nguyent@cs.jhu.edu

&Raman Arora

Department of Computer Science

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

###### Abstract

We study learning in a dynamically evolving environment modeled as a Markov game between a learner and a strategic opponent that can adapt to the learner's strategies. While most existing works in Markov games focus on external regret as the learning objective, external regret becomes inadequate when the adversaries are adaptive. In this work, we focus on _policy regret_ - a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight. We show that if the opponent has unbounded memory or if it is non-stationary, then sample-efficient learning is not possible. For memory-bounded and stationary, we show that learning is still statistically hard if the set of feasible strategies for the learner is exponentially large. To guarantee learnability, we introduce a new notion of _consistent_ adaptive adversaries, wherein, the adversary responds similarly to similar strategies of the learner. We provide algorithms that achieve \(\) policy regret against memory-bounded, stationary, and consistent adversaries.

## 1 Introduction

Recent years have witnessed tremendous advances in reinforcement learning for various challenging domains in AI, from the game of Go (Silver et al., 2016, 2017, 2018), real-time strategy games such as StarCraft II (Vinyals et al., 2019) and Dota (Berner et al., 2019), autonomous driving (Shalev-Shwartz et al., 2016), to socially complex games such as hide-and-seek (Baker et al., 2019), capture-the-flag (Jaderberg et al., 2019), and highly tactical games such as poker game Texas hold' em (Moravcik et al., 2017; Brown and Sandholm, 2018). Notably, most challenging RL applications can be systematically framed as multi-agent reinforcement learning (MARL) wherein multiple strategic agents learn to act in a shared environment (Yang and Wang, 2020; Zhang et al., 2021).

Despite the empirical successes, the theoretical foundations of MARL are underdeveloped, especially in settings where the learner faces _adaptive_ opponents who can strategically adapt and react to the learner's policies. Consider for example the optimal taxation problem in the AI economist (Zheng et al., 2020), a game that simulates dynamic economies that involve multiple actors (e.g., the government and its citizens) who strategically contribute to the game dynamics. The government agent learns to set a tax rate that optimizes for the economic equality and productivity of its citizens, whereas the citizens who perhaps have their own interests, respond adaptively to tax policies of the government agent (e.g., relocating to states that offer generous tax rates). Such adaptive behavior of participating agents is a crucial component in other applications as well, e.g., mechanism design (Conitzer and Sandholm, 2002; Balcan et al., 2005), optimal auctions (Cole and Roughgarden, 2014; Dutting et al., 2019).

The question of learning against adaptive opponents has been mostly studied under the framework of external regret, wherein the agent is required to compete with the best fixed policy in hindsight (Liu et al., 2022). However, external regret is not adequate to study adaptive opponents as it does not take into account the counterfactual response of the opponents. This motivates us to study MARL using the framework of _policy regret_(Arora et al., 2012), a counterfactual notion that aims to compete with the return that would have been attained if the agent had followed the best fixed sequence of policy in hindsight. Even though policy regret is now a standard notion to study adaptive adversaries and has been extensively studied in online (bandit) learning (Merhav et al., 2002; Arora et al., 2012; Malik et al., 2022) and repeated games (Arora et al., 2018), it has not received much attention in a multiagent reinforcement learning setting. In this paper, we aim to fill in this gap. We consider two-player Markov games (MGs) (Shapley, 1953; Littman, 1994) as a model for MARL, wherein one agent (the learner) learns to act against an adaptive opponent. We provide a series of negative and positive results for policy regret minimization in Markov games, highlighting the fundamental limits of learning and showcasing key principles underpinning the design of efficient learning algorithms against adaptive adversaries.

Fundamental barriers.We first show that any learner must incur a linear policy regret against an adaptive opponent who can adapt and remember the learner's past policies (Theorem 1). When the opponent has a bounded memory span, any learner must require an exponential number of samples \(((SA)^{H}/^{2})\) to obtain an \(\)-suboptimal policy regret, even with the weakest form of memory wherein the opponent is oblivious (Theorem 2). When the memory-bounded opponent's response is stationary, i.e., the response function does not vary with episodes, learning is still statistically hard when the learner's policy set is exponentially large, as in this case the policy regret necessarily scales polynomially with the cardinality of the learner's policy set (Theorem 3).

Efficient algorithms.Motivated by these statistical hardness results, we consider a structural condition on the response of the opponents, which we refer to as consistent behavior, wherein the opponent responds similarly to similar sequences of policies (Definition 5). We propose two algorithms OPO-OILE (Algorithm 1) and APE-OVE (Algorithm 3) that obtain \(\) policy regret against \(m\)-memory bounded, stationary, and consistent adversaries, for \(m=1\) and \(m 1\), respectively.

* **For memory length \(m=1\)**: We show that OPO-OME obtains a policy regret upper bound of \(}(H^{3}S^{2}AB+SA^{2}BT})\), when the learner's policy set is the set of all deterministic Markov policies, where \(H\) is the episode length, \(S\) is the number of states, \(A\) and \(B\) are the numbers of actions for the learner and the opponent, respectively, and \(T\) is the number of episodes.
* **For general memory length \(m 1\)**: We show that APE-OVE obtains a policy regret upper bound of \(}((m-1)H^{2}SAB+SAB}(SAB(H+)+H^{2}) }})\), where \(d^{*}\) is an instance-dependent quantity that features the minimum positive visitation probability.

We provide a summary of our main results in Table 1.

## 2 Related work

Learning in Markov games.Learning problems in Markov games have been studied extensively in the MARL literature. Most existing works focus on learning Nash equilibria either with known dynamics or infinite data (Littman, 1994; Hu and Wellman, 2003; Hansen et al., 2013; Wei et al., 2020), or otherwise in a self-play setting wherein we control all the players (Wei et al., 2017; Bai et al., 2020; Bai and Jin, 2020; Xie et al., 2020; Liu et al., 2021), or in an online setting wherein we

 
**Opponent’s Adaptive Behavior** & **Policy Regret** \\   Unbounded memory & \((T)\) \\  \(m\)-memory bounded (\(m 0\)) & \((})\) \\  \(m\)-memory bounded + stationary (\(m 1\)) & \((\{T,A^{H}\})\) \\ 
1-memory bounded + stationary + consistent & \(}(H^{3}S^{2}AB+SA^{2}BT})\) \\  \(m\)-memory bounded + stationary + consistent & \(}((m-1)H^{2}SAB+SAB}(SAB(H+)+H^{2}) }})\) \\  

Table 1: Summary of main results for learning against adaptive adversaries. Learner’s policy set is all deterministic Markov policies. \(m=0\) + stationary corresponds to standard single-agent MDPs.

control one player to learn against other potentially adversarial players (Brafman and Tennenholtz, 2002; Wei et al., 2020; Tian et al., 2021; Jin et al., 2022). Other related work focuses on exploiting sub-optimal opponents via no-external regret learning (Liu et al., 2022) and studying Stackelberg equilibria in two-player general-sum turn-based MGs, wherein only one player is allowed to take actions in each state (Ramponi and Restelli, 2022).

Policy regret in online learning settings.Policy regret minimization has been studied mostly in online (bandit) learning problems. It was first studied in a full information setting (Merhav et al., 2002) and extended to the bandit setting and more powerful competitor classes using swap regret and \(\)-regret (Arora et al., 2012). A lower bound of \(T^{2/3}\) on policy regret in a bandit setting was provided by Dekel et al. (2014) and was later extended to action space with metric (Koren et al., 2017, 2017). A long line of works studies (complete) policy regret in "tallying" bandits, wherein an action's loss is a function of the number of the action's pulls in the previous \(m\) rounds (Heidari et al., 2016; Levine et al., 2017; Seznec et al., 2019; Lindner et al., 2021; Awasthi et al., 2022; Malik et al., 2022, 2023).

Beyond online (bandit) learning, policy regret has been studied in several more challenging settings. In Arora et al. (2018) authors study the notion of policy equilibrium in repeated games (Markov games with \(H=S=1\)) when agents follow no-policy regret algorithms. A more complete characterization of the learnability in online learning with dynamics, where the loss function additionally depends on time-evolving states, was given in Bhatia and Sridharan (2020). Finally, in Dinh et al. (2023), authors study policy regret in online MDP, where an adversary who follows a no-external regret algorithm generates the loss functions, which effectively alleviates policy regret minimization to the standard external regret minimization in online MDPs.

## 3 Problem setup

Markov games.In this paper, we use the framework of Markov Games to study an interactive multi-agent decision-making and learning environment (Shapley, 1953). Markov games extend Markov decision processes (MDPs) to multiplayer scenarios, where each agent's action affects not only the environment but also the subsequent state of the game and the actions of other agents. Formally, a standard two-player Markov Game (MG) is specified by a tuple \(M=(,,,H,P,r)\). Here, \(\) denotes the state space with cardinality \(||=S\), \(\) is the action space of the first player (called _learner_) with cardinality \(||=A\), \(\) is the action space of the second player (referred to as an _opponent_ or an _adversary_) with cardinality \(||=B\), \(H\) is the time horizon for each game. \(P=\{P_{1},,P_{H}\}\) are the transition kernels with each \(P_{h}:(S)\) specifying the probability of transitioning to the next state given the current state, learner's action, and adversary's action (\(()\)) denotes the set of all probability distributions over \(\)). Finally, \(r=\{r_{1},,r_{H}\}\) are the (expected) reward functions with each \(r_{h}:\). For simplicity, we assume the learner knows the reward function.1

Each episode begins in a fixed initial state \(s_{1}\). At step \(h[H]\), the learner observes the state \(s_{h}\) and picks her action \(a_{h}\) while the opponent/adversary picks an action \(b_{h}\). As a result, the learner observes \(b_{h}\), receives reward \(r_{h}(s_{h},a_{h},b_{h})\) and the environment transitions to \(s_{h+1} P_{h}(|s_{h},a_{h},b_{h})\). The episode terminates after \(H\) steps.

Policies and value functions.A learner's policy (also referred to as strategy) is any tuple \(=\{_{h}\}_{h[H]}\) where \(_{h}:()^{h-1} ()\). A policy \(=\{_{h}\}_{h[H]}\) is said be Markovian if for every \(h[H],_{h}:()\). Similarly, an adversary's policy is any tuple \(=\{_{h}\}_{h[H]}\) where \(_{h}:()^{h-1} ()\). \(\) is said to be Markovian if for every \(h\), \(_{h}:()\). For simplicity, we will focus only on Markov policies for both the learner and the adversary in this paper. Let \(\) (respectively, \(\)) be the set of all feasible policies of the learner (respectively, the adversary). The value of a policy tuple \((,)\) at step \(h\) in state \(s\), denoted by \(V_{h}^{,}(s)\) is the expected accumulated reward starting in state \(s\) from step \(h\), if the learner and the adversary follow \(\) and \(\) respectively, i.e., \(V_{h}^{,}(s):=_{,}[_{l=h}^{H}r_{l}(s_{l},a_{l},b_{l})| s_{h}=s]\), where the expectation is with respect to the trajectory \((s_{1},a_{1},b_{1},r_{1},,s_{H},a_{H},b_{H},r_{H})\) distributed according to \(P\), \(\), and \(\). We also denote the action-value function \(Q_{h}^{,}(s,a,b):=_{,}[_{l=h}^{H}r_{l}(s_{l},a_{l},b_{ l})|(s_{h},a_{h},b_{h})=(s,a,b)]\).

Given a \(V:\), we write \(P_{h}V(s,a,b):=_{s^{} P_{h}(|s,a,b)}[V(s^{})]\). For any \(u:()\), \(v()\), \(Q\), denote \(Q(s,u,v):=_{a u(|s),b v(|s)}[Q(s,a,b)]\) for any \(s\).

Adaptive adversaries.We allow the adversary to be _adaptive_, i.e., the adversary can choose their policy in episode \(t\) based on the learner's policies on episodes \(1,,t\). We assume that the adversary is deterministic and has unlimited computational power, i.e., the adversary can plan, in advance, using as much computation as needed, as to how they would react in each episode to any sequence of policies. Formally, the adversary defines in advance a sequence of deterministic functions \(\{f_{t}\}_{t^{*}}\), where \(f_{t}:^{t}\). The input to each response function \(f_{t}\) is an entire history of the learner's policies, including her policy in episode \(t\). Therefore, if the learner follows policies \(^{1},,^{t}\), the adversary responds with policy \(f_{t}(^{1},,^{t})\) in episode \(t\). Since the response function \(f_{t}\) depends on the learner's policy at round \(t\), our setup is essentially a principal-follower model, akin to Stackelberg games (Letchford et al., 2009; Blum et al., 2014) and mechanism design for learning agents (Braverman et al., 2019). In this context, the principal agent (mechanism designer or learner) publicly declares a strategy before committing to it, allowing the followers to subsequently choose their strategies based on their understanding of the principal's decisions.

We evaluate the learner's performance using the notion of **policy regret**(Merhav et al., 2002; Arora et al., 2012), which compares the return on the first \(T\) episodes to the return of the best fixed sequence of policy in hindsight. Formally, the learner's policy regret after \(T\) episodes is defined as

\[(T)=_{}_{t=1}^{T}V_{1}^{,f_{t}([]^{t}) }(s_{1})-V_{1}^{^{t},f_{t}(^{1},,^{t})}(s_{1}),f_{t}([]^{t}):=f_{t}(_{t }).\] (1)

Policy regret has been studied in online (bandit) learning (Merhav et al., 2002; Arora et al., 2012) and repeated games (Arora et al., 2018), yet, to the best of our knowledge, it has never been studied in Markov games. Policy regret differs from the more common definition of external regret defined as \(R(T)=_{}_{t=1}^{T}V_{1}^{,f_{t}(^{1},, ^{t})}(s_{1})-V_{1}^{^{t},f_{t}(^{1},,^{t})}(s_{1})\), which is used in (Liu et al., 2022). However, external regret is inadequate for measuring the learner's performance against an adaptive adversary. Indeed, when the adversary is adaptive, the quantity \(V_{1}^{,f_{t}(^{1},,^{t})}\) is hardly interpretable anymore - see (Arora et al., 2012) for a more detailed discussion.

As a warm-up, we show in the following example that, policy regret minimization generalizes the standard Nash equilibrium learning problem in zero-sum two-player Markov games.

**Example 3.1** (Nash equilibrium).: _Consider the adversary with the following behavior: for any Markov policy \(\) of the learner, the adversary ignores all the learner's past policies and respond only to the current policy \(\) with a Markov policy \(f()\) such that for all \((s,h)\), \(V_{h}^{,f()}(s)=_{}V_{h}^{,}(s)\), where the minimum is taken over all the possible Markov policies for the adversary. By Filar and Vrieze (2012), such an \(f()\) exists. In addtion, there also exists a Markov policy \(^{*}\) such that for all \((s,h)\), \(V_{h}^{^{*},f(^{*})}(s)=_{}V_{h}^{,f()}(s)=_{}_{ }V_{h}^{,}(s)\). The policies \((^{*},f(^{*}))\) is a Nash equilibrium (Nash, 1950) of the Markov game. For such an adversary, the policy regret becomes \((T)=_{t=1}^{T}V_{1}^{^{*},f(^{*})}(s_{1})-_{t=1}^{T}V_{1 }^{^{t},f(^{t})}(s_{1})\). This Nash equilibrium can be computed using, e.g., the Q-ol algorithm of (Tian et al., 2021) with \(\) (policy) regret.2_

Additional notation.We write \(f g\) to mean \(f=(g)\). We use \(c\) to represent an absolute constant that can have different values in different appearances.

## 4 Fundamental barriers for learning against adaptive adversaries

In this section, we show that achieving low policy regret in Markov games against an adaptive adversary is statistically hard when (i) the adversary has an unbounded memory (see Definition 1), or (ii) the adversary is non-stationary, or (iii) the learner's policy set is exponentially large (even if the adversary is memory-bounded and stationary).

To begin with, we show that any learner must incur a linear policy regret in the general setting.

**Theorem 1**.: _For any learner, there exists an adaptive adversary and a Markov game instance such that \((T)=(T)\)._

The construction in the proof of Theorem1, shown in AppendixA.1, takes advantage of the unbounded memory of the adversary, that can remember the policy the learner takes in the first episode. This motivates us to consider memory-bounded adversaries, a situation that is quite similar to the online bandit learning setting of Arora et al. (2012).

**Definition 1** (\(m\)-memory bounded adversaries).: _An adversary \(\{f_{t}\}_{t^{*}}\) is said to be \(m\)-memory bounded for some \(m 0\) if for every \(t\) and policy sequence \(^{1},,^{t}\), we have \(f_{t}(^{1},,^{t})=f_{t}(^{\{1,t-m+1\}},,^{t})\)._

Is it possible to efficiently learn against memory-bounded adversaries? Unlike online bandit learning, we show that learning in Markov games is statistically hard even when the adversary is memory-bounded, even for the weakest case of memory \(m=0\) and the adversary's policy set \(\) is small.

**Theorem 2**.: _For any learner and any \(L\) and \(S,A\), \(H\), there exists an oblivious adversary (i.e., \(m=0\)) with the policy space \(\) of cardinality at least \(L\), a Markov game (with \(SA+S\) states, \(A\) actions for the learner, \(B=2S\) actions for the adversary) such that \((T)=(})\)._

Theorem2 claims that competing even with an oblivious adversary that employs a small set of policies takes an exponential number of samples (e.g., set \(S=L=H\)). The construction of the lower bound follows the construction used to prove a lower bound for learning latent MDPs (Kwon et al., 2021) and a reduction of a given latent MDP into a Markov game (Liu et al., 2022); we give complete details in AppendixA.2. The proof of Theorem2 utilizes the fact that the sequence of response function an adversary utilizes can be completely arbitrary. It implies that we need to constrain the adversary further beyond being memory-bounded. A natural restriction we consider given the construction is to assume stationarity, i.e. consider adversaries whose response functions do not change over time.

**Definition 2** (Stationary adversaries).: _An \(m\)-memory bounded adversary is said to be stationary if there exists an \(f:^{m}\) such that for all \(t\) and \(^{1},,^{t}\), we have \(f_{t}(^{1},,^{t})=f(^{\{1,t-m+1\}},,^{t})\)._

The stationary behavior is sometimes also referred to as "g-restricted" in the online learning literature-see the related discussion of Malik et al. (2022). In the special case wherein the adversary is both stationary and oblivious (i.e., \(m=0\)), the Markov game reduces to the standard single-agent MDP (and the policy regret reduces to standard regret of the MDP) - this setting has been studied in (Zhang et al., 2023). We, therefore, only need to consider \(m 1\).

Connections to Stackelberg equilibrium in general-sum Markov games.While seemingly restrictive, policy regret minimization with \(m\)-memory bounded and stationary adversaries already subsumes the problem of learning Stackelberg equilibrium (Von Stackelberg, 2010) in general-sum Markov games (Ramponi and Restelli, 2022).3 In general-sum Markov games, the adversary ("follower") aims at maximizing his own reward function given any policy of the learner ("leader"). That is, the adversary is \(1\)-memory bounded, and the response function \(f:\) corresponds to a function that selects the best response policy to any given policy of the learner. The benchmark \(_{}V_{1}^{,f()}\) in policy regret then becomes the Stackelberg equilibrium.

Is sample-efficient learning possible against \(m\)-memory bounded and stationary adversaries? One can notice an immediate approach to learning against a \(1\)-memory bounded and stationary adversaries is to simply view the problem as a \(||\)-armed bandit problem and apply any state-of-the-art bandit algorithm (Audibert and Bubeck, 2009) to obtain \((T)=(H)\). However, scaling polynomially with the learner's policy class is not desirable when the class is exponentially large (e.g., when the learner's policy class is the set of all deterministic policies, then \(||=(A^{HS})\)). And in fact, we cannot avoid polynomial scaling with the cardinality of the learner's policy class in general.

**Theorem 3**.: _For any learner with policy class \(\), there exists a \(1\)-memory bounded and stationary adversary and a Markov game with \(B=(1)\) such that \((T)=(\{T,||\})\)._

Note that the lower bound applies to \(m=1\), and, therefore, to any \(m 1\). Proof in AppendixA.3.

Efficient algorithms for learning against adaptive adversaries

Thus far, we have shown that learning against an adaptive adversary in Markov games is statistically hard, even when the adversary is \(m\)-memory bounded and stationary. The reason that stationarity is not sufficient for efficient learning (which the lower bound in Theorem 3 exploits for the construction of a hard instance) comes from the unstructured response of the adversary in the worst case. Even if the learner plays _nearly_ identical sequence of policies differing only on a small number of states and steps, the adversary can essentially respond completely arbitrarily. In other words, knowing the policies that the adversary plays in response to the policies of the learner (i.e., observing the values of the response function \(f\) at specific inputs) reveals zero information about the function \(f\) on previously seen inputs. Thus, the learner is required to explore all the policies in \(\) to be able to identify an optimal policy. This motivates us to consider an additional structural assumption on how the adversary responds to the learner's policies. We assume that the adversary is consistent in response to two similar sequences of policies of the learner. In essence, given that the learner plays two sequences of policies that agree on certain states (\(s\)) and steps (\(h\)) - then, we assume that the opponent also responds with two sequences of policies that agree on the same states and steps. We refer to this behavior as _consistent_; a formal definition follows.

**Definition 3** (Consistent adversaries).: _An \(m\)-memory bounded and stationary adversary \(f\) is said to be consistent if, for any two sequences of learner's policies \(^{1},,^{m}\) and \(^{1},,^{m}\), and any \((s,h)[H]\), if \(^{i}_{h}(|s)=^{i}_{h}(|s), i[m]\), then \(f(^{1},,^{m})_{h}(|s)=f(^{1},,^{m})_{h}(|s)\). Otherwise, we say that the opponent's response \(f\) is arbitrary._

We argue that the definition above is natural if we are to consider opponents that are self-interested strategic agents, and not simply a malicious adversary. So, it would be in an opponent's interest to play in a somewhat consistent manner. Playing optimally after figuring out the learner's strategy would indeed require playing consistently. An opponent that plays completely arbitrary, while challenging to learn anything from, also does not improve their value function. Some remarks are in order.

**Remark 1** (\(\)-approximately consistent adversaries).: _Our algorithms and results for consistent adversaries easily extend to \(\)-approximately consistent adversaries for any fixed constant \( 0\). An adversary \(f\) is said to be \(\)-approximately consistent if, for any \(^{1},,^{m}\) and \(^{1},,^{m}\), and any \((s,h)[H]\), if \(^{i}_{h}(|s)=^{i}_{h}(|s), i[m]\), then \(_{a}|_{h},,^{m})_{h}(a|s) }{f(^{1},,^{m})_{h}(a|s)}|\). For simplicity, we stick with Definition 3 (i.e., \(=0\)) to best convey our algorithmic and theoretical ideas._

**Remark 2**.: _While our notion of consistent behaviors is quite natural, it might as well be that there is a more general notion of complexity for the opponent's response function classes that fully characterizes learnability in this setting. This likely requires the definition of appropriate norms in the input policy space \(^{m}\) and the output policy space \(\), and a certain notion of predictability for the opponent's response function classes (e.g., in the spirit of Eluder dimension (Russo and Van Roy, 2013)), so that the learner can accurately estimate the opponent's response function, without trying out all possible policies. This question goes beyond the scope of our current work and is left to a future investigation._

**Remark 3**.: _To permit learnability in terms of external regret in Markov games, Liu et al. (2022) consider a policy-revealed setting, wherein the opponent reveals his current strategy to the learner at the end of each episode. No external regret is possible because the benchmark in external regret evaluates the learner's comparator policy against the same policy that the opponent reveals. For policy regret, however, knowing the opponent's strategy at the end of the episode gives the learner no advantage in general, as the counterfactual benchmark requires evaluating the learner's policies against the policy sequence that the opponent would have reacted with. Indeed, our lower bound in Theorem 3 still applies to the policy-revealed setting._

For \(m\)-memory bounded, stationary and consistent adversaries, we present two algorithms, one for \(m=1\) and the other for general \(m 1\), with sublinear policy regret. We give special consideration to the case with \(m=1\) as it helps with the exposition of key algorithmic design principles rather simply. For simplicity, we focus on \(\) being the set of all deterministic policies (i.e., \(||=(A^{HS})\)). Our algorithms and upper bounds easily extend to any general \(\) with polynomial log-cardinality.

**Assumption 5.1**.: _The learner's policy class \(\) is the set of all deterministic policies._

A key component of our algorithms is using maximum likelihood estimation (MLE) (Geer, 2000) to estimate action distributions with which the opponent can respond. As is the convention in MLE analysis, we make a realizability assumption and use bracketing numbers to control the model class.

**Assumption 5.2**.: _For any policy \(\) that the adversary employs and for all \((h,s)[H]\), assume \(_{h}(|s) P_{}:=\{P_{}():\}\), where the set \(P_{}\) has \(\)-bracketing number \(_{}()\) w.r.t. \(l_{1}\) norm, defined as the minimum number of \(\)-brackets \([l,u]:=\{P_{} P_{}:l P_{} u\}\) with \(\|l-u\|_{1}\), that are needed to cover \(P_{}\)._

Intuitively, restricting the adversary to be consistent, allows the learner to predict the opponent's response from previous episodes to similar settings. The learner can collect the data from what the adversary responds to and learn his response function. Given the consistent behavior, for every \((h,s)[H]\), the number of action distributions \(_{h}(|s)\) that the adversary can respond with cannot exceed the number of possible action distributions \(_{h}(|s)\) that the learner can construct in state \(s\) at step \(h\). Given \(\) is the set of all deterministic policies, we only need to learn \(HSA\) action distributions that the adversary can respond at any state and step. We begin with the oblivious case of \(m=1\) and end up resolving the general case \(m 1\) after.

### Memory of length \(m=1\)

We first consider the memory length of \(m=1\) for stationary and consistent adversaries.

Algorithm.We propose OPO-OMLE (Algorithm 1), which represents Optimistic Policy Optimization with Optimistic Maximum Likelihood Estimation. OPO-OMLE is a variant of the optimistic value iteration algorithm of , wherein we build an upper confidence bound on the value function \(V_{1}^{,f()}\) for any policy \(\), using a bonus function and optimistic MLE . The upper confidence bound is based on two levels of optimism: a bonus term \(\) that is based on confidence intervals on the transition kernels \(P\) and the parameter version spaces \(\{_{hsa}\}\) of the adversary's response at each level \((h,s,a)\). The parameter version spaces construct a set of parameters that are close to the MLE solution, up to an error \(\), in terms of the log-likelihood in the observed actions taken by the adversary.

```
1:Input: Bonus function \(:\), and MLE confidence parameter \(\)
2:Initialize:\(_{hsa},D_{hsa},N_{h}(s,a,b) 0,N_{h}(s,a,b,s^{}) 0,(h,s,a,b,s^{}) \)
3:for episode \(t=1,,T\)do
4:\(^{t}*{arg\,max}_{}(N,\{D_{i}\},\{_{i}\},,)\) (Algorithm 2)
5: Play \(^{t}\) (the opponent responds with \(f(^{t})\)) to observe \((s^{t}_{1},a^{t}_{1},b^{t}_{1},r^{t}_{1},,s^{t}_{H},a^{t}_{H},b^{t}_{H},r ^{t}_{H})\)
6:\( h N_{h}(s^{t}_{h},a^{t}_{h},b^{t}_{h}) N_{h}(s^{t}_{h},a ^{t}_{h},b^{t}_{h})+1\), \(N_{h}(s^{t}_{h},a^{t}_{h},b^{t}_{h},s^{t}_{h+1}) N_{h}(s^{t}_{h},a^{t }_{h},b^{t}_{h},s^{t}_{h+1})+1\), \(D_{hs^{t}_{h}a^{t}_{h}} D_{hs^{t}_{h}a^{t}_{h}}\{b^{t}_{h}\}\), and \(_{hs^{t}_{h}a^{t}_{h}}\{_{hs^{t}_{h}a^{t}_{h}}: _{b D_{hs^{t}_{h}a^{t}_{h}}} P_{}(b)_{_{ h^{t}_{h}a^{t}_{h}}_{b D_{hs^{t}_{h}a^{t}_{h}}} P_{}(b)-\}\)
7:endfor
8:Output:\(\{^{t}\}_{t[T]}\) ```

**Algorithm 2** DOUBLY_OPTIMIISTIC_VALUE_ESTIMATE(\(N,\{D_{i}\},\{_{i}\},,\))

Theoretical guarantee.We now present a theoretical guarantee for OPO-OMLE.

**Theorem 4**.: _In Algorithm 1, choose \((t)=cH}\), where \(:=(SABHT/)\), and \(=c(_{}(1/T)HSAT/)\). With probability at least \(1-\), we have_

\[(T)=(H^{3}S^{2}AB T+H^{2}+H^{2}).\]Theorem 4 shows that OPO-OMLE achieves \(\)-policy regret bounds against \(1\)-memory bounded, stationary and consistent adversaries in Markov games. Notably, the policy regret depends only on the log-cardinality of the learner's policy class \(\) and the log-bracketing number of the set of action distributions with which the adversary responds to the learner. Since \(||=A^{HS}\), the bound translates into \((T)=}(H^{3}S^{2}AB+SA^{2}BT})\).

Finally, comparing the lower bound of \((\{SAT},HT\})\) for single-agent MDPs (Domingues et al., 2021), which applies to this setting, the dominating term in our upper bound (Theorem 4) is worse only by a factor of \(H\) - this is due to the need to learn the opponent's moves.4

### Memory of any fixed length \(m 1\)

We now consider the general case of stationary and consistent adversaries that have a memory of any fixed length \(m 1\). Note that we assume that the learner knows (an upper bound of) \(m\). Playing against a \(1\)-memory bounded adversary does not stop the learner from changing her policies often, as the adversary does not remember any policies that the learner has taken previously. However, a sublinear policy regret learner against \(m\)-memory bounded adversaries should switch her policies as less frequently as possible, and at most only sublinear time switches. The reason is that every policy switch will add a constant cost to policy regret, as the benchmark in the policy regret is with the best _fixed_ sequence of policy. This makes the regret minimizer OPO-OMLE unable to generalize from \(m=1\) to any fixed \(m\). Instead, we propose a low-switching algorithm, in which the learner learns to play _exploratory_ policies repeatedly over consecutive episodes so that the switching cost is reduced. Here, as in Jin et al. (2020), exploratory policies are those with good coverage over the state space from which uniform policy evaluation can be performed to identify near-optimal policies.

Algorithm.We propose APE-OVE (Algorithm 3), which represents Adaptive Policy Elimination by Optimistic Value Estimation. APE-OVE generalizes the adaptive policy elimination algorithm of (Qiao et al., 2022) for MDPs to Markov games with unknown opponents. The high-level idea of our algorithm is as follows. The learner maintains a version space \(^{k}\) of remaining high-quality policies after each epoch - which is a sequence of consecutive episodes with an appropriate length (epoch \(k\) has a length of \(HSAB(m-1+T_{k})\) in APE-OVE).

* **Layerwise exploration** (Line 5 of Algorithm 3): Within each epoch, the learner performs layerwise exploration (Algorithm 4), wherein we devise high-coverage sampling policies \(^{hhsab}\) that aim at exploring \((s,a,b)\) in step \(h\) and epoch \(k\), starting from the lowest layer \(h=1\) up to the highest layer \(h=H\). However, some states might not be visited frequently by any policy, thus taking a large amount of exploration. They, fortunately, do not significantly affect the value functions of any policy and thus can be identified (by storing in \(^{k}\)) and removed from exploration quickly (via the truncated transition kernel estimates \(\) obtained in Algorithm 5). Layerwise exploration requires value estimation uniformly over all policies. However, the learner does not know the adversary's response \(f\). To address this, we use optimistic value estimation via the optimistic MLE in the collected data of the adversary's moves (Algorithm 6).
* **Version space refinement** (Line 6 of Algorithm 3): After the layerwise exploration, we refine the version space of policies that the learner can choose from at the next epoch using the optimistic value estimation based on the empirical transition kernels \(^{k}\), the parameter version space \(^{k}\) and the set of infrequent transition samples \(^{k}\) given any reward function \(r\). The version space is designed in such a way that the expected value for the learner to play any policy \(\) from the version space is guaranteed to be no worse than \(}(1/})\) compared to the optimal, with high probability.

Note that we do not directly use the reward function \(r\) in the version space refinement. Instead, we use a truncated reward function \(r_{^{k}}\) that is zero for any \((h,s,a,b,s^{})\) in the infrequent transition set \(^{k}\). This truncated design is critical to our analysis and the subsequent guarantees, e.g., see Lemma B.10. For the truncated reward functions, the backup step in Algorithm 6 should be understood as: \(_{h}^{}(s,a,b)=_{s^{}_{h}^{k}(|s,a, b)}[r_{h}(s,a,b)1\{(h,s,a,b,s^{})^{k}\}+_{h+1}^{ }(s^{})],(s,a,b)\).

We now present a theoretical guarantee for APE-OVE. We bound policy regret in terms of an instance-dependent quantity, namely minimum positive visitation probability, defined as follows.

```
1:Input: number of episodes \(T\), reward function \(r\)
2:Parameters:\(:=(_{}(1/T)HSAT/)\), \(:=\{t:(m{-}1) t{+}t\}\), \(K=()\), and \(T_{k}:=^{1-}}, k[K]\)
3:Initialize:\(^{1}=,^{1}=\)
4:for epoch \(k=1,,K\)do
5:\(^{k},^{k},^{k}=\) LAYERWISE_EXPLORATION\((^{k},T_{k})\) (Algorithm 4)
6:\(^{k+1}:=\{^{k}:^{}(r_{^{k}},^{k}, ^{k})_{^{k}}^{}(r_{^{k}}, ^{k},^{k})-cH^{2}SABT_{k})}\}\)  where \(r_{^{k}}(s_{1},a_{1},b_{1},,s_{H},a_{H},b_{H}):=_{h[H] }1\{(h,s_{h},a_{h},b_{h},s_{h+1})^{k}\}r_{h}(s_{h},a_{h},b_{h})\) and \(^{}(r,P,):=\) OPTIMISTIC_VALUE_ESTIMATE\((,r,P,)\) is given in Algorithm 6
7:endfor ```

**Algorithm 4**LAYERWISE_EXPLORATION\((^{k},T_{k})\)

```
1:Input: Policy version space \(^{k}\), number of episodes \(T_{k}\)
2:Initialize:\(^{k}=\{^{k}_{h}\}_{h[H]}\) arbitrary transition kernels, \(^{k}=\), \(^{k}_{hsa}=,(h,s,a)\), \(=\), \(N^{k}_{h}(s,a,b,s^{})=0,(h,s,a,b,s^{})\), and for each \((h,s,a,b),1_{hsab}\) is the reward function \(r^{}\) such \(r^{}_{h^{}}(s^{},a^{},b^{})=1\{(h^{},s^{ },a^{},b^{})=(h,s,a,b)\}\)
3:for\(h=1,,H\)do
4:for\((s,a,b)\)do
5:\(^{khash}=}{}\) OPTIMISTIC_VALUE_ESTIMATE\((,1_{hsab},^{k},^{k})\) (Algorithm 6)
6: Play \(^{khash}\) for \(m-1\) episodes (and collect nothing)
7: Keep playing \(^{khash}\) for \(T_{k}\) episodes and add all the transitions only at step \(h\) to \(\)
8:endfor
9:\(N^{k}_{h}(s,a,b,s^{}) N^{k}_{h}(s,a,b,s^{})+1,(s, a,b,s^{})\) s.t. \((h,s,a,b,s^{})\)
10:\(^{k}_{hsa}=\{^{k}_{hsa}:_{b:(h,s,a,b)}P _{}(b)_{hsa}}{}_{b:(h,s,a,b )}P_{}(b)-\},(s,a) \)
11:\(^{k}^{k}\{(h,s,a,b,s^{}):N^{k}_{h}(h, s,a,b,s^{}) cH^{2}(SABHK/)\}\)
12:\(^{k}_{h}=\) TRANSITION_ESTIMATE\((h,N^{k}_{h},^{k},s^{l})\) (Algorithm 5)
13: Reset \(=\)
14:endfor
15:Output:\(^{k}=\{^{k}_{h}\}_{h[H]}\), \(^{k}\), \(^{k}\) ```

**Algorithm 5**LAYERWISE_EXPLORATION\((^{k},T_{k})\)

**Definition 4** (Minimum positive visitation probability).: _The quantity \(d^{*}:=_{h,s,a:d^{*}_{h}(s,a)>0}d^{*}_{h}(s,a)\) is said to be the minimum positive visitation probability, where \(d^{*}_{h}(s,a):=_{:d^{*}_{h}(r^{}(||^{m})(s,a)>0}d^{,f(||^{m})}_{h}(s,a)\)._

The minimum positive visitation probability - which has also been used recently to characterize instance-dependent bounds for PAC RL [Tirinzoni et al., 2023], is the minimal probability that any state-action pair can be visited at a time step, given they can be visited at all. This implies that during the exploration phase if we try a certain policy \(\) for \(N\) episodes and encounter \((s,a)\) at step \(h\) (in any episode), on average, \(\) would visit \((h,s,a)\) for \(Nd^{*}\) times out of \(N\) episodes. This, along with the assumption that the adversary is consistent enables us to estimate the adversary's response to any \((h,s,a)\) that is visited within an estimation error of order \(1/}\). Note that we do not need to take care of the adversary's response to any \((h,s,a)\) that is not visited as these tuples are deemed infrequent by any policy and thus have negligible impact on the value estimation.

**Theorem 5**.: _Playing APE-OVE against any \(m\)-memory bounded, stationary, and consistent adversaries in any Markov game for \(T\) episodes, with \(T=(\{AB(d^{*})^{2}}{S^{3}},(m-1)HSAB\})\), and_

\[T\{SAB(d^{*})^{2}^{4}(HSABK/)}{^{2}}, (d^{*})^{2}^{4}(HSABK/)}{(SAB)^{3}^{2}}, ^{2}(HSABK/)}{(AB)^{3}S^{5}}\},\]guarantees that with probability at least \(1-\),_

\[(T)=\!(\!\!(m-1)H^{2}SAB T+H^{3/2}(HSAB+H^{2}+S^{3/2}AB) }} T)\]

_where \(d^{*}\) is the minimum positive visitation probability and \(\) is as defined in Algorithm 3._

```
1:Input: reward function \(r\), policy \(\), transition kernel \(P\), parameter version space \(\)
2:Initialize: \(_{H+1}^{}()=0\)
3:for\(h=H,H-1,,1\)do
4:\(_{h}^{}(s,a,b)=r_{h}(s,a,b)+[P_{h}_{h+1}^{}](s,a,b),( s,a,b)\)
5:\(_{h}^{}(s)=_{_{h,s_{h}(s)}}_{h}^{}(s, _{h}(s),P_{}), s\)\(\)Optimistic MLE
6:endfor
7:Output: \(_{1}^{}(s_{1})\) ```

**Algorithm 5**\((h,N_{h},,s^{})\)

Theorem 5 asserts a \(\) policy regret bound against \(m\)-memory bounded, stationary, and consistent adversaries in Markov games. Notably, our bounds grow linearly with memory length \(m\). Compared to the bound in Theorem 4, given \(T\) is sufficiently large, the bound in Theorem 5 deals with the general memory length \(m\) at the cost of a worse dependence on all other factors \(H,S,A,B,d^{*}\). Dealing with \(\)-approximately consistent adversaries (see Remark 1) will incur an additional term \((T)\) to the policy regret.

## 6 Discussion

In this paper, we study learning in Markov games against adaptive adversaries and highlight the statistical hardness of learning in this setting. We identify a natural structural assumption on the response function of the adversary, wherein we provide two distinct algorithms that attain \(\) policy regret, one for the unit memory and the other for general memory length.

There are several notable gaps in our current understanding of policy regret in Markov games. First, we do not know if the dependence on the minimum positive visitation probability \(d^{*}\) when learning against \(m\)-memory bounded opponents is necessary. In other words, can we derive minimax bounds that hold for any problem instance, regardless of how small \(d^{*}\) is, for the case of general \(m\)? While it seems to us that such a dependence is necessary (as it seems difficult otherwise to learn the opponent's response while also learning high-return policies), yet we are unable to prove or reject this conjecture. Second, as we state in Remark 2, we do not currently know the necessary conditions on the opponent's response functions for learnability in this setting. This might as well require an alternate condition that generalizes our notion of consistent behaviors and fully characterizes the predictability of the opponent (in a similar way as the VC dimension characterizes learnability in statistical learning theory). Third, our theory currently views information, and not computation, as the main bottleneck and aims for policy regret minimization without worrying about computational complexity. As a result, some of the steps in our algorithms happen to be computationally inefficient. In particular, selecting a policy that maximizes the optimistic value function requires iterating over the learner's policy set, which is exponentially large. Can we hope for computationally efficient no-policy regret algorithms in Markov games? Fourth, our policy regret bounds scale with the cardinality of the state space and the action space, which could be large in many practical settings. Can we avoid such dependence by employing function approximation (e.g., neural networks)?