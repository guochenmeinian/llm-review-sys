# Shape Arithmetic Expressions

Krzysztof Kacprzyk

University of Cambridge

kk751@cam.ac.uk &Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

###### Abstract

Symbolic regression has excelled in uncovering equations from physics, chemistry, biology, and related disciplines. However, its effectiveness becomes less certain when applied to experimental data lacking inherent closed-form expressions. Empirically derived relationships, such as entire stress-strain curves, may defy concise closed-form representation, compelling us to explore more adaptive modeling approaches that balance flexibility with interpretability. In our pursuit, we turn to Generalized Additive Models (GAMs), a widely used class of models known for their versatility across various domains. Although GAMs can capture non-linear relationships between variables and targets, they cannot capture intricate feature interactions. In this work, we investigate both of these challenges and propose a novel class of models, Shape Arithmetic Expressions (SHAREs), that fuses GAM's flexible shape functions with the complex feature interactions found in mathematical expressions. SHAREs also provide a unifying framework for both of these approaches. We also design a set of rules for constructing SHAREs that guarantee transparency of the found expressions beyond the standard constraints based on the model's size.

## 1 Introduction

**Symbolic Regression.** Symbolic regression (SR) is an area of machine learning that aims to construct a model in the form of a _closed-form expression_. Such an expression is a combination of variables, arithmetic operations (\(+\), \(-\), \(\), \(\)), some well-known functions (trigonometric functions, exponential, etc), and numeric constants. For instance, \(3(x_{1}+x_{2}) e^{2x_{3}^{2}}\). Such equations, if concise, are interpretable and well-suited to mathematical analysis. These properties have led to applications of SR in many areas such as physics , medicine , material science , and biology . Symbolic regression is usually validated on synthetic datasets with closed-form ground truth equations . However, as we investigate in Section 2.1, closed-form functions are often inefficient in describing some relatively simple relationships producing overly long expressions. They are also not compatible with categorical variables. Further discussion on symbolic regression methods can be found in Appendix D.

**Generalized Additive Models.** Widely used transparent models are Generalized Additive Models (GAMs) . They model the relationship between the features \(x_{i}\) and the label \(y\) as

\[g(y)=f_{1}(x_{1})++f_{n}(x_{n}) \]

where \(g\) is called the _link functions_ and the \(f_{k}\)'s are called _shape functions_. These models allow arbitrary complex shape functions but exclude more complicated interactions between the variables. They are deemed transparent as each function \(f_{k}\) can be plotted, and thus the contribution of \(x_{k}\) can be understood. Extensions of GAMs that include pairwise interactions have also been proposed (GA\({}^{2}\)M) . In these settings, the model can contain 2D shape functions that can be visualized using a heatmap. The main disadvantage of such models is their inability to model more complicated interactions, for instance, \(}{x_{3}}\) (see Section 2). See Appendix D for more discussion about GAMs.

**Transparency of closed-form expressions.** A model is considered _transparent_ if by itself it is understandable--a human understands its function . The transparency of symbolic regression can be compromised if the found expressions become too complex to comprehend. In many scenarios, an arbitrary closed-form expression is unlikely to be considered transparent. Most of the current works limit the complexity of the expression by introducing a constraint based on, e.g., the number of terms , the depth of the expression tree , or the description length . Although these metrics often correlate with the difficulty of understanding a particular equation, size does not always reflect the equation's complexity as it does not focus on its semantics. Some recent works introduce a recursive definition of complexity that takes into account the type and the order of operations performed . Although they are a step in the right direction, they are not grounded in how the model will be analyzed, and thus, it is not clear if they capture how comprehensible the model is (further discussion in Appendix D).

**Contributions and outline.** In Section 2, we investigate the limitations of SR and GAMs. In Section 3, we introduce a novel class of models called **SH**ape **AR**ithmetic **E**xpressions (SHAREs) that combine GAM's flexible shape functions with the complex feature interactions found in closed-form expressions thus providing a unifying framework for both approaches. In Section 4, we introduce a new kind of transparency that goes beyond the standard constraints based on the model's size and apply it to SHAREs. We also investigate theoretical properties of transparent SHAREs. Finally, we demonstrate their effectiveness through experiments in Section 5.

## 2 Limitations of current approaches

### Symbolic regression struggles with non-closed-form expressions.

Symbolic regression excels in settings where the ground truth is a closed-form expression . However, its effectiveness becomes less certain when applied to scenarios with no underlying closed-form expressions. Some phenomena do not have a closed-form expression (e.g., non-linear pendulum), and many functions in physics are determined experimentally rather than derived from a theory and are not inherently closed-form (e.g., current-voltage curves, drag coefficient as a function of Reynolds number, phase transition curves). This is even more relevant in life sciences, where the complexity of the studied phenomena makes it more difficult to construct theoretical models. We claim symbolic regression struggles to find a compact expression for some relatively simple univariate functions.

**Example: stress-strain curves.** To illustrate our point, we try to fit a symbolic regression model to an experimentally obtained stress-strain curve. We use data of stress-strain curves in steady-state tension of aluminum 6061-T651 at different temperatures obtained by . Figure 1 (left panel) shows a sample of these curves. These functions are relatively simple as they can be divided into a few interpretable segments representing different behaviors of the material. We use a symbolic regression library PySR  to fit the stress-strain curve of aluminum at 300\({}^{}\)C. We fit the model and present some of the found expressions in Figure 1 (right panel). The size of a closed-form expression is defined as the number of terms in its representation. For instance, \((x+1)\) has complexity \(4\) as it contains four terms: \(\), \(+\), \(x\), and \(1\). We can see that small programs do not fit the data well. A good fit is achieved only by bigger expressions. However, such expressions are much less comprehensible, and thus, their utility is diminished.

Figure 1: Left panel: examples of stress-strain curves. Right panel: Some of the equations discovered by Symbolic regression when fitted to the stress-strain curve of aluminum at 300\({}^{}\)C.

[MISSING_PAGE_FAIL:3]

SHAREs. In general, however, \(\) is supposed to be a flexible family of functions that are fitted to the data and are meant to be understood visually.

**Why univariate functions?** We decided to only use univariate functions for two reasons: they are easy to understand, and they are sufficient. Firstly, they are easy to comprehend because they can always be plotted. While analyzing them, we have to keep track of only one variable, and we can characterize them using monotonicity. Univariate functions are also much easier to edit in case we want to fix the model. Secondly, the Kolmogorov-Arnold representation theorem  states that for any continuous function \(f:^{n}\), there exist univariate continuous functions \(g_{q}\),\(_{p,q}\) such that

\[f(x_{1},,x_{n})=_{q=0}^{2n}g_{q}(_{p=1}^{n}_{p,q}(x_{p})) \]

That means in principle, for expressive enough shape functions, SHAREs should be able to approximate any continuous function. However, SHAREs of that form would not necessarily be very transparent. We discuss the transparency of SHAREs in the next section.

## 4 Transparency

As explained in Section 1, the transparency of symbolic regression can be compromised if the found expressions become too complex to comprehend. In many scenarios, an arbitrary closed-form expression is unlikely to be considered transparent. To see that, it is enough to realize that any fully connected deep neural network with sigmoid activation functions is technically a closed-form expression. As SHAREs extend SR, they inherit the same problem. Current works introduce constraints that are not grounded in how the model will be analyzed; thus, it is unclear whether they capture how comprehensible the model is. That includes constraints based on model size [41; 13; 43] and even recent semantic constraints [46; 23] (further discussion on SR constraints in Appendix D). We take an alternative approach. We define transparency implicitly by proposing two general rules for building machine learning models in a transparency-preserving way, and we justify why they may be sufficient for achieving transparency in certain scenarios. These rules, in turn, allow us to define a subset of transparent SHAREs.

**Rule 1.** Let \(s\) be any univariate function. \(s(x_{i})\) is transparent, where \(x_{i}\) is any variable. If \(f\) is transparent then \(s f\) is also transparent.

**Rule 2.** Let \(b\) be a binary operation. If \(f\) and \(g\) are transparent and have disjoint sets of arguments then \(b(f,g)\) is also transparent.

Motivated by research on human understanding and problem solving [29; 39; 28; 40], we assume that in some scenarios _understanding a complex expression involves decomposing it into smaller expressions and understanding them and the interactions between them_. Thus the model can be understood from the ground up. This is in agreement with recent research in XAI that highlights _decomposability_ as a crucial factor for transparency, enabling more interpretable and explainable machine learning methods . The rules we propose offer a rigorous way to encode this criterion for some classes of machine learning models. Below, we justify why these rules may often be sufficient.

Let us start with Rule 1. Let \(s\) be any univariate function. Then \(s(x_{i})\) is transparent because we can visualize it and create a mental model of its behavior. Let us now consider a transparent function \(f\). As it is transparent, we should have a fairly good understanding of the properties of \(f\). For instance, what range of values it attains, or whether it is monotonic for some subsets of the data. As we can

Figure 2: Shape Arithmetic Expression represented as a tree.

visualize \(s\), it is reasonable to expect that we can infer these properties about \(s f\) as well. We can analyze \(s\) and \(f\) separately and then use that knowledge to analyze \(s f\).

Let us now justify Rule 2. Let \(b\) be a binary operation and let \(f\) and \(g\) be transparent functions with non-overlapping sets of arguments. As these functions are transparent, we can understand their various properties. As they do not have any common variables, they act independently. Thus, we can combine them using the binary operation \(b\) and directly use the previous analysis to understand the new model \(b(f,g)\). Thus, it is considered transparent. See Appendix D for a practical example.

Although Rule 2 seems like a strong constraint, many common closed-form equations used to describe natural phenomena can be put in a form that satisfies this rule. In particular, 82 out of 100 equations from Feynman Symbolic Regression Database  satisfy Rule 2. Thus, in many cases, the space of transparent models should be rich enough to find a good fit.

**Restricting the search space.** The current definition of SHAREs contains certain redundancies. For instance, it allows for a direct composition of two shape functions. This unnecessarily complicates the model as the composition of two shape functions is just another shape function (given that the class of shape functions is expressive enough). As any binary operation applied to a function and a constant can be interpreted as applying a linear function, we can remove the constants without losing the expressivity of SHAREs (given that the shape functions can model linear functions).

We can now use these two rules and the above observations to define transparent SHAREs.

**Definition 1**.: A transparent SHARE is a SHARE that satisfies the following criteria:

* Any binary operator is applied to two functions with disjoint sets of variables.
* The argument of a shape function cannot be an output of another shape function, i.e., \(s_{1}(s_{2}(x))\) is not allowed.
* It does not contain any numeric constants.

_Remark 2_.: By this definition, any GAM is a transparent SHARE. This is consistent with the fact that GAMs are generally considered transparent models .

Transparent SHAREs have some useful properties. For instance, there is no need to arbitrarily limit the size of the expression tree (as might be the case for many SR algorithms). The following proposition demonstrates some useful properties of SHAREs, including that the depth and the number of nodes of a transparent SHARE are naturally constrained.

**Proposition 1**.: _Let \(f:^{n}\) be a transparent SHARE. Then_

* _Each variable node appears at most once in the expression tree._
* _The number of binary operators is_ \(d-1\)_, where_ \(d\) _is the number of variable nodes (leaves)_
* _The depth of the expression tree of_ \(f\) _is at most_ \(2n\)_._
* _The number of nodes in the expression tree of_ \(f\) _is at most_ \(4n-2\)_._

Proof.: Appendix A. 

For comparison, the expression tree of a GAM has \(3n-1\) nodes. That demonstrates that transparent SHAREs are not only naturally constrained, but even the largest possible expressions are not significantly longer than the expression for a GAM, even though it can capture much more complicated interactions. The immediate corollary of this proposition is useful for the implementation.

**Corollary 1**.: _SHARE \(f\) satisfies Rule 2 iff each variable appears at most once in its expression tree._

## 5 SHAREs in Action

In this section, we perform a series of experiments to show how SHAREs work in action. First, we justify our claim that SHAREs extend GAMs (Section 5.1) and SR (Section 5.2). Finally, we show an example that cannot be fitted by GAM or by SR. For details about the experiments, see Appendix C.

**Implementation.** We use nested optimization to implement SHAREs. The outer loop employs a modified genetic programming algorithm (based on gplearn  used for symbolic regression), while the inner loop optimizes shape functions as neural networks via gradient descent. Although not a key contribution due to its limited scalability, this implementation demonstrates SHAREs' potential to outperform existing transparent methods and enhance interpretability, given more efficient optimization algorithms. We note that optimization of transparent models is usually harder than that of black boxes . For further implementation details, see Appendix B.

### SHAREs extend GAMs

As we discussed earlier, GAMs (without interactions) are examples of SHAREs. That means that, in a particular, if we have a dataset that can be modeled well by a GAM, SHAREs should also model it well. To verify this, we generate a semi-synthetic dataset inspired by the application of GAMs to survival analysis described in . In this work, GAMs are used to model the risk scores of patients taking part in a clinical trial for the treatment of node-positive breast cancer. We choose three of the covariates considered and assume that the risk score (log of hazard ratio) can be modeled as a GAM of age, body mass index (BMI), and the number of nodes examined. We recreate the shape functions to resemble the ones reproduced in the original paper. Then we choose the covariates uniformly from the prescribed ranges and calculate the risk scores.

We fit SHAREs to this dataset and show the results in Figure 3. Each row shows the best equation with the corresponding number of shape functions and the shape functions of the equation with three shape functions are shown on the right side of the figure.

We see that the equation in the fourth row achieves a high \(R^{2}\) score. It is also in the desired form. When we plot the shape functions in Figure 3 we see that they match the ground truth well (the vertical translation is caused by the fact that shape functions can always be translated vertically).

### SHAREs extend SR

**Torque equation.** Consider equation I.18.12 (Table 1) used to calculate torque, given by \(=rF()\). We sample 100 rows from the Feynman dataset corresponding to this expression and we run our algorithm. Each row of the table in Figure 4 shows the best equation with the corresponding number of shape functions. The right side of the figure shows the shape functions of the equations in the second and fourth rows.

The equation that is symbolically equivalent to the ground truth is in the second row, \(=rFs_{3}()\). It achieves a nearly perfect \(R^{2}\) score. By plotting \(s_{3}\), we can verify that it matches \(\) function well (Figure 4, central panel).

**What are the shape functions of the longer equations?** Consider the expression in row 4 from the table in Figure 4, \(=s_{1}(r)s_{2}(F)s_{3}()\). It might look complicated because it contains three shape functions. But, if we inspect \(s_{1}\) and \(s_{2}\) (Figure 4, right panel), we see that they are linear functions. We can extract the line equations and put them into the found SHARE to get a simple expression \((-0.2r-0.56)(0.34F+0.78)s_{3}()\).

Figure 4: Equations found by fitting SHAREs to a torque equation \(=rF()\). Each row in the table shows the best found equation with the corresponding number of shape functions (#s). Central panel: shape function from the second row compared to ground truth. Right panel: shape functions from the fourth row.

Figure 3: Results of fitting SHAREs to the risk score data. Each row in the table shows the best found equation with the corresponding number of shape functions (#s). On the right side, shape functions from the fourth row compared to the ground truth.

### SHAREs go beyond SR and GAMs

We consider the following problem. Given \(m\) grams of water (in a liquid or solid form) of temperature \(t_{0}\) (in \({}^{}C\)), what would be the temperature of this water (in a solid, liquid, or gaseous form) after heating it with energy \(E\) (in calories). We restrict the initial temperature to be from -100 \({}^{}\)C to 0 \({}^{}\)C. This is a relatively simple problem with only 3 variables but we will show that both GAMs and SR are not sufficient to properly (and compactly) model this relationship.

**GAMs.** First, we fit GAMs without interactions using EBM . The shape functions of EBM are presented in Figure 5 (right panel). The \(R^{2}\) score on the validation set is 0.758. We can also see that the two of the shape function are very jagged. They do not seem to fit our definition of a simple univariate function. This makes it difficult to gain insight into the studied phenomenon.

**GA\({}^{2}\)Ms.** Now, we fit GAMs with pairwise interactions , once again using the EBM algorithm. The shape functions of EBM are presented in Figure 8 in Appendix C.3. Although the \(R^{2}\) score has been improved to 0.875, EBM's transparency is reduced even further by adding pairwise interactions.

**Symbolic Regression.** We fit symbolic regression using the PySR library . We limit the complexity of the program to 40 and we present the results in the table in Figure 5. Only the most complex equations give us a performance comparable to a GAM with interactions: 0.867. The last equation from the table is shown below. We argue that its complexity hinders its transparency.

\[y=}{(x_{0})}-1.72e^{e^{(} {x_{1}})}}+80.1+56.3((}{x_{1}}+31.7 (}{x_{1}}+0.93)))\]

**SHAREs.** We finally fit SHAREs to the temperature data. The found expressions are shown in Figure 6. We immediately see a very good performance from all models apart from the one not using any shape functions at all. The scores are also much better than the scores achieved by GAMs (with or without interactions) and SR. Let us investigate the equation in the third row; the shape functions are presented in Figure 6 (right panel).

We note that the expression \(s_{1}((+s_{2}(t_{0})))\) has a better performance than GAMs and SR, a more compact symbolic representation than SR, and simpler shape functions than GAM. This exemplifies how, by combining the advantages of GAMs and SR, we can address their underlying limitations. Let us see how this particular SHARE can aid in understanding the phenomenon it fits. We first recognize that \(s_{1}\) is contingent on the energy-to-mass ratio, which is offset by a function of the initial temperature, \(t_{0}\). As shown in Figure 6's right panel, \(s_{2}\) appears linear, with an irregularity around \(-40\)\({}^{}\)C, which regularization may have eliminated. Replacing \(s_{2}\) with an equivalent linear function and adjusting the equation gives us: \(t=s_{1}(+0.507t_{0}+24.973)\)

Analyzing \(s_{1}\), we find that without energy input, \(+0.507t_{0}+24.973\) ranges from \(-26\) (\(t_{0}=-100\)) to \(25\) (\(t_{0}=0\)), aligning with the first linear part of the \(s_{1}\) curve. Increasing energy per mass initially raises the temperature linearly to 0 \({}^{}\)C, then plateaus, characteristic of an ice-water mixture. When all ice melts, the temperature rises linearly again to 100 \({}^{}\)C, remaining constant until all water evaporates, after which steam temperature again increases linearly.

The shape functions also provide quantitative insights. The slopes of \(s_{1}\)'s linear parts approximate the specific heat capacities of ice, water, and steam. The constant parts' widths estimate the heat of fusion and vaporization. We compare these estimates from \(s_{1}\) with the physical ground truth in Table 6 in Appendix C.3 (the same values were used for data generation).

Figure 5: Left panel: Equations found by SR when fitted to the temperature data. The last four equations do not fit in the table; they are reproduced in Appendix C.3. Right panel: shape functions from the GAM fitted to the temperature dataset.

## 6 Discussion

**Applications.** SHAREs can be beneficial in settings where transparent models are needed or preferred, such as risk prediction in healthcare and finance. They can also be useful in AI applications for scientific discovery (AI4Science). Currently, a lot of work in AI4Science is focused on developing better symbolic regression methods. We believe that for AI4Science to advance beyond the synthetic experiments based on simple physical equations, we need to add more flexibility to our models. Transparent SHAREs add this flexibility without compromising the comprehensibility.

**Limitations.** The current implementation of SHAREs is time-intensive and thus does not scale to bigger datasets. We are confident that further optimizations will enable wider adoption of this novel approach. We hope that future work will address the limitation of our implementation and will enhance the ability to fit SHAREs to even larger and more complex datasets.