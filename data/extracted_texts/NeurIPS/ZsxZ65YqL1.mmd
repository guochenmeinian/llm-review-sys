# CriticEval: Evaluate Large Language Model as Critic

Tian Lan\({}^{1}\)+
Footnote †: Equal contributions

 Wenwei Zhang\({}^{2}\)+
Footnote †: Corresponding author

 Chen Xu\({}^{4}\)

Heyan Huang\({}^{1}\)

Dahua Lin\({}^{2,3,5}\)

 Kai Chen\({}^{2}\)+

Footnote †: Corresponding author

Xian-Ling Mao\({}^{1}\)+
Footnote †: Corresponding author

###### Abstract

Critique ability, _i.e.,_ the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight. While numerous studies have been proposed to evaluate critique ability of LLMs, their comprehensiveness and reliability are still limited. To overcome this problem, we introduce CriticEval, a novel benchmark designed to comprehensively and reliably evaluate critique ability of LLMs. Specifically, to ensure the comprehensiveness, CriticEval evaluates critique ability from four dimensions across nine diverse task scenarios. It evaluates both scalar-valued and textual critiques, targeting responses of varying quality. To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably. Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CriticEval. Then, experimental results demonstrate the promising potential of open-source LLMs, the effectiveness of critique datasets and several intriguing relationships between the critique ability and some critical factors, including task types, response qualities and critique dimensions.

## 1 Introduction

Critique ability is crucial for the self-improvement  of LLMs, as it enables the effective analysis and correction of flaws in responses . This capability also facilitates a more robust framework, _i.e.,_ scalable oversight , for ensuring the AI systems growing in scale and capability remain aligned with human-desired outcomes and ethical standards.

So far, while numerous works have been proposed to evaluate critique ability of LLMs in downstream tasks, like common NLP tasks  and reasoning tasks , their comprehensiveness and reliability are limited. Specifically, existing works  typically evaluate only specific aspects of critique ability, resulting in limited evaluated critique dimensions , insufficient analysis of response qualities and task types . Besides, while GPT-4 is frequently used to evaluate textual or natural language critiques , its reliability across all critique dimensions and tasks remains unverified . In summary, a comprehensive and reliable benchmark for assessing critique capability of LLMs is still under-explored, significantly impeding the in-depth analysis.

To fill this gap, we propose a novel benchmark, CriticEval, designed to comprehensively and reliably measure critique capability of LLMs. Specifically, to ensure comprehensiveness, CriticEval evaluates critique ability of LLMs from following dimensions: evaluating a single response (feedback), comparing pairs of responses (comparison), correcting the response based on feedback (correction) and evaluating one feedback of LLM (meta-feedback). These critique dimensions cover all categories of critiques in previous works [11; 13; 4] and the necessary capabilities for self-improvement of LLM  and scalable oversight . These critique dimensions are measured under nine diverse task scenarios, including three common NLP tasks, two alignment tasks, and four math reasoning and coding tasks. Moreover, evaluated responses in each critique dimension and each task are collected using various open-source and closed-source LLMs with different capabilities, with human annotation ensuring varied quality levels. Furthermore, since both the scalar-valued and textual formats of critique are commonly used in these scenarios , CriticEval evaluate the critiques in both formats, equipped with objective [18; 19] and subjective [10; 11] evaluations, respectively. Note that scalar-valued critiques typically refer to Likert scores and preference labels, while textual critiques refer to more fine-grained textual analysis about response quality [11; 3]. Overall, as shown in Table 1, CriticEval exhibits significant advantages in comprehensiveness compared to previous benchmarks. It demonstrates great diversity in critique formats, critique dimensions, response qualities and the data size of textual critique.

To ensure the reliability of evaluating textual critiques in CriticEval, a large number of high-quality critiques are annotated, serving as references for GPT-4 to evaluate textual critiques automatically. To annotate these textual critiques efficiently, we employ a human-in-the-loop pipeline , first generated by GPT-4 and then rigorously reviewed and refined by human experts.

Extensive evaluations of 35 widely used open-source and closed-source LLMs prove the reliability of CriticEval. Specifically, GPT-4 with human-annotated reference critiques achieves close correlations with human judgments, while removing them results in significant performance loss. Additionally, critiques with higher scores consistently lead to superior improvements, illustrating a clear correlation between the real critique ability of LLMs and their evaluation scores within CriticEval. Then, extensive evaluations results also demonstrate that some open-source LLMs, such as Qwen  and InternLM2 , are approaching state-of-the-art closed-source LLMs in critique capabilities, and their critique ability could be further improved through scaling strategy. Besides, the effectiveness of critique datasets is also validated. Finally, these evaluation results also reveal several intriguing phenomena:

* Critique difficulty varies by task type. For instance, math reasoning and coding tasks are more challenging for feedback, comparison, while they are easier for meta-feedback.
* There is an inverse relationship between the quality of critiques and responses. For example, high-quality responses pose a greater challenge to critique effectively.
* Critique difficulty correlates with the critique dimensions; notably, comparison and meta-feedback dimensions present greater challenges than feedback dimension.

These observations and phenomena promote an in-depth understanding of critique ability of LLMs. We hope the discoveries could spur future research in this field.

  
**Benchmarks** & **Critique** & **Critique** & **Critique** & **Response** & **Test NL** & **Subjective** & **Objective** & **Human** & **Release** \\  & **Format** & **Dimension** & **Quality** & **Data Size** & **Metric** & **Metric** & **Anno.** & **Release** \\ 
**CriticBench** & Scalar & 1 & 2 & 0 & - & Accuracy & ✗ & ✗ \\
**Shepherd** & NL & 1 & - & 352 & GPT-4 & - & ✗ & ✗ \\
**Auto-J** & NL/Scalar & 2 & - & 232 & GPT-4 & Accuracy & ✗ & ✓ \\
**UltraFeedback** & NL & 1 & - & 450 & GPT-4 & - & ✗ & ✗ \\
**CriticBench** \(\) & Scalar & 2 & 2 & 0 & - & F1,PR & ✓ & ✓ \\
**MetaCritique** \(\) & NL & 1 & - & 300 & GPT-4 w Ref. & - & ✓ & ✓ \\ 
**SumnEval ** & Scalar & 1 & - & 0 & - & Correlation & ✓ & ✓ \\
**WMT-22 (zh-en)** & Scalar & 1 & - & 0 & - & Correlation & ✓ & ✓ \\
**MT-Bench** & Scalar & 1 & - & 0 & - & Accuracy & ✓ & ✓ \\ 
**CriticEval (Ours)** & **NL/Scalar** & **4** & **4** & **3,608** & **GPT-4 w Ref.** & **Correlation,PR** & ✓ & ✓ \\   

Table 1: Comparison between the **test and dev set** of benchmarks and CriticEval. A complete list are described in Appendix C. The response quality in some benchmarks is unclassified (-). PR denotes the Pass Rate on reasoning and coding tasks. Our concurrent works are marked with a \(\).

Related Work

### Application of Critique Ability

Automatic EvaluationAutomatic evaluation, also known as critique ability in recent works [3; 10], has been well studied in the past few years [23; 18]. It aims to accurately judge the evaluated responses in numerous NLP tasks and reduce the high cost of human annotations [18; 17; 24; 25; 26]. Recently, advanced LLMs, like GPT-4, have exhibited very close correlation with human judgments [18; 27; 13], assign textual critiques with corresponding quality scores, _i.e._, the scalar-valued score, in a chain-of-thought inference manner . To further mitigate the high inference cost of closed-source LLMs, numerous works propose to improve critique ability of open-source LLMs by fine-tuning them on critique datasets generated by GPT-4 [14; 29; 30], like Auto-J  and UltraFeedback .

LLM Self-improvementSo far, critique ability has been widely used for self-improvement of LLMs in two stages: (1) **Inference stage**: Given textual critiques that analyze the flaws in the response and provide suggestions, LLMs can iteratively improve the response quality [6; 15; 31; 32; 33].; (2) **Training stage**: Scalar-valued critiques are frequently used to compile responses with a clear performance gap for rejective fine-tuning (RFT) or preference learning (RLHF ), which further enhances LLM capabilities [1; 2; 5; 35; 36]. For instance, Self-rewarding  improves Llama-2-70B by fine-tuning it on samples selected based on its rewards. Similarly, ChatGLM-Math  fine-tunes a math-critique model for scoring generated answers, which are used for rejective fine-tuning  and direct preference optimization .

### Benchmarking Critique Ability

So far, numerous meta-evaluation benchmarks have been proposed to evaluate the critique ability of models . Early benchmarks mainly focus on evaluating scalar-valued critiques [30; 24] on common NLP tasks [18; 23], like translation  and summary  by computing the correlations between model and human judgments. Recent works also assess scalar-valued critiques of LLMs on reasoning and coding tasks [24; 9]. For example, CriticBench built from 3 reasoning tasks, analyzes important properties of critique ability of LLMs. Our concurrent work, CriticBench analyzes several intriguing findings among generation, critique and correction capability on responses collect from five reasoning tasks.3 Compared with these existing works, our proposed CriticEval exhibits advantages on several crucial factors, like critique dimensions, response qualities and diverse task types, leading to more comprehensive evaluations for critique ability. Although CriticBench collect high-quality responses, their quality levels are still unclassified.

Beyond scalar-valued critiques, evaluating textual critiques is more challenging [10; 15]. Most existing works coarsely evaluated textual critiques using GPT-4 [11; 12], proven unreliable [10; 13]. Unlike them, our extensive results prove that GPT-4 with human-annotated critiques is reliable for evaluating textual critiques. Although our concurrent work, MetaCritique , demonstrates the reliability of evaluating textual critiques by verifying their Atomic Information Units, it is unclear whether their conclusions could be extended to more critique dimensions and tasks.

## 3 Preliminaries

We first formally define the key concepts and their corresponding notions in CriticEval. Figure 1 shows a specific case to understand these concepts.

Task Input (\(I\)) and Response (\(R\))represent the queries and generations of LLMs, respectively.

Critiqueaims to analyze and refine the generated responses. Formally, this paper studies the critique capabilities in four dimensions: (1) feedback \(F_{s}\) involves textual analysis and a quality score. Good feedback should not only find flaws in responses but also provide helpful suggestions for correction ; (2) correction or refinement \(CR\) aims to revise responses with or without feedback. Previous evaluations [11; 12; 9] overlook this dimension, although it is an inevitable step when letting the model improve itself ; (3) comparison \(F_{c}\) contains a textual critique and a preference label for a pair of responses \((R_{a},R_{b})\); (4) Meta-feedback \(F_{s}(F_{s})\), _i.e._, the feedback of feedback itself , involves a rating score reflecting the quality of \(F_{s}\) and corresponding textual analysis, which is a high-level critique dimension. Such an ability is necessary to improve critique ability of LLMs [1; 13]. Due to the complexity of the meta-feedback dimension, textual critiques are not collected in this paper, and we leave it for future research.

To the best of our knowledge, these four critiques cover all categories of critiques examined in prior research [11; 4; 13]. Although the feedback of correction and comparison dimensions are also important, they are not essential for the self-improvement of LLMs. Thus, this study mainly focuses on studying the feedback of feedback \(F_{s}(F_{s})\) and leaving the rest of them for our future work.

## 4 CriticEval Construction

Given the challenge of crafting scalar-valued and textual critiques from scratch, we construct CriticEval using a human-in-the-loop data construction pipeline as shown in Figure 2.4

### Task Input Collection

Task inputs for \(9\) distinct tasks are collected to evaluate critique capabilities comprehensively (Step 1 in Figure 2). Specifically, CriticEval includes three widely used tasks for evaluating critique ability: (1) representative classical language tasks: summary , translation , and question-answering ; (2) LLM alignment: general chat scenarios  and harmlessness cases ; (3) reasoning and code capabilities: math reasoning with chain-of-thought (CoT) and program-of-thought (PoT), and coding with and without execution results. We hereinafter refer to "code w/ execution" as "CodeExec" and "code w/o execution" as "CodeNE". For each task, we collect around 100 task inputs from the test sets of some widely used benchmark datasets to ensure the task input quality and avoid data contamination. Please refer to Appendix D for more details about the data source.

### Response and Critique to be Evaluated

For each collected \(I\) in each task, LLMs of different scales and capabilities are first employed to generate responses with diverse flaws (Step 2 (a) in Figure 2). The complete list of LLMs is in Appendix E. Then, low-, medium-, and high-quality responses with diverse quality differences are collected according to the quality score annotated by the human raters with GPT-4-turbo as the assistant (Step 2 (b)). Moreover, we also collect golden or correct responses, which have been proven challenging for critiques . More details about how to select low-, medium, high-quality and correct responses can be found in Appendix F.

After collecting responses, we further collect critiques to be evaluated for the meta-feedback dimension by utilizing four LLMs that are known powerful for critiques (Step 3 (d) in Figure 2): (1) GPT-4; (2) GPT-3.5-turbo; (3) Auto-J-13B ; (4) UltraCM-13B .

### Reference Critique Generation and Annotation

After collecting task inputs and responses, four kinds of reference critiques are collected on these responses to make the objective and subjective evaluation in our proposed CriticEval more reliable.

Feedback and CorrectionGPT-4-turbo is utilized to generate feedback and corrections sequentially (Step 3 (c) and (e) in Figure 2). The scalar-valued and textual critiques for feedback dimension

Figure 1: Cases of four critique dimensions. Scalar-valued critiques are scores and preference labels.

are collected, denotes as **"score"** and **"text"** in Figure 2. Since responses in math reasoning and coding tasks pose significant challenges for critiques during our annotation, ground-truth answers are provided for GPT-4 as references to generate high-quality feedback and corrections. Then, they are carefully reviewed and revised by human annotators.

ComparisonOur empirical finding suggests that pairs of responses pose greater challenges for comparison if they perform similarly. Therefore, two kinds of pairs are first created: \((R_{},R_{})\) and \((R_{},R_{})\), designated as the easy and hard samples, respectively. Then, GPT-4-turbo is prompted to provide scalar-valued and text critiques on these pairs (Step 3 (f) in Figure 2). These outputs, labeled as **"preferred label"** and **"text"** in Figure 2, are then refined by human annotators.

Meta-FeedbackSince GPT-4 has been proven unreliable to evaluate critiques [10; 15], three human experts are asked to provide their quality scores for generated critiques (Step 3 (d) in Figure 2).

During human annotation, multiple human experts are asked to follow a rigorous annotation protocol, detailed in Appendix H.1, and the statistics of human annotation for reference critiques are described in Appendix H.6. Besides, several case studies are shown in Appendix I to facilitate a clear understanding of our proposed CriticEval.

## 5 Evaluation Metrics

### Objective Evaluation

Feedback and meta-feedback evaluationaim to evaluate the consistency between generated scores and human judgments. This setup facilitates the generation in a chain-of-thought manner, followed by the quality score of the evaluated critiques. For the meta-feedback dimension, LLMs are prompted with annotated reference critiques. The widely-used Spearman correlations  are computed [23; 24], which ranges from \(-1\) to \(1\) (normalize to \((-100,100)\)). Higher scores indicate a higher consistency with human judgments. The \(p\)-value of spearman correlation are recorded, and \(p<0.05\) is typically considered to be statistically significant [23; 43].

Comparison evaluationassess the accuracy of LLM in deciding preferences between two responses. It is well known that current LLMs exhibit significant **positional bias**[17; 44; 45], _i.e.,_ LLMs tend to prefer responses based on their specific position in the prompt. We implement a rigorous verification process to mitigate the effects of positional bias. Specifically, given responses \(R_{a}\) and \(R_{b}\) to be compared, we obtain the comparison based on two orders, noted as \(F_{c}^{a}=F_{c}(R_{a},R_{b})\)

Figure 2: The data construction pipeline for our proposed CriticEval. **Step 1**: \(9\) tasks and numerous LLMs are prepared. **Step 2**: LLMs are employed to generate responses, which are then meticulously reviewed by human experts. **Step 3**: Critiques are generated by LLMs with strong critique ability, and human experts annotate them.

and \(F_{c}^{b}=F_{c}(R_{b},R_{a})\). The objective scores are computed by: \(s=_{i=1}^{N}(L(F_{c}^{a},F_{c}^{b}))\), where \(\{0,1\}\) is the indicator function. \(L(F_{c}^{a},F_{c}^{b})\) is true if and only if \(F_{c}^{a} F_{c}^{b}\) and \(F_{c}^{a},F_{c}^{b}\) align with ground-truth preference label. \(N\) is the number of test samples.

Correction evaluationis only conducted on math reasoning and coding tasks since the revision could be easily verified with the ground-truth answers and the test cases [4; 9]. Thus, the objective evaluation metric is implemented as the pass rate: \(}}{N}\), where \(N\) and \(N_{}\) are the number of the total samples and passed samples, respectively.

### Subjective Evaluation

The subjective evaluation aims to evaluate the quality of the generated textual critiques. Since responses in math reasoning and coding tasks can be verified, we only conduct subjective evaluations on other \(5\) tasks for the correction dimension. In our work, GPT-4 evaluates the generated critiques by generating the chain of thought followed by the score, with our human-annotated critiques as references. It is well-known that LLMs prefer longer generations during their automatic evaluation [46; 45]. However, Figure 7 in Appendix J proves that there is no clue that GPT-4, with our concise and precise reference critiques as input, would give higher scores to longer critiques. The subjective scores range from 1 to \(10\). Following previous work , the human-annotated reference critiques are anchored to \(8\), serving as a relative scoring pivot.

### Overall Score

The overall score of subjective and objective evaluation is computed as averaging on all the critique dimensions, and more details about computing the overall score can be found in Appendix K.

## 6 Evaluation and Analysis

The critique abilities of representative LLMs are analyzed in this section, and the overview results are shown in Table 2. Firstly, the reliability of evaluation in CriticEval are proven in Section 6.2

    &  &  \\   & \(}\) & \(\) & \(}\) & **Overall** & \(}\) & \(\) & \(}\) & \((F_{s})}\) & **Overall** \\   \\ 
**GPT-4-turbo** & 7.84 & 7.69 & **7.89** & **7.81** & **63.54** & **69.67** & **57.33** & **62.90** & **72.55** \\ 
**GPT-3.5-turbo** & 5.21 & 7.55 & 4.92 & 5.89 & 51.44 & 64.00 & 40.67 & 28.71 & 60.83 \\
**Claude-instant-1** & 5.88 & **7.72** & 5.76 & 6.45 & 42.78 & 50.00 & 44.89 & 38.89 & 58.93 \\   \\ 
**Qwen-72B-Chat** & 5.57 & 7.45 & 5.02 & 6.01 & 42.64 & 54.67 & 44.00 & 27.86 & 58.48 \\
**Qwen-14B-Chat** & 4.81 & 7.25 & 3.98 & 5.35 & 14.32\({}^{}\) & 38.00 & 15.78 & 10.72\({}^{}\) & 41.58 \\
**Qwen-7B-Chat** & 4.05 & 6.38 & 3.47 & 4.63 & -8.09\({}^{}\) & 32.33 & 5.33 & 11.73\({}^{}\) & 34.87 \\   \\ 
**InternLM2-20B** & 6.03 & 7.48 & 5.10 & 6.20 & 58.61 & 50.50 & 44.67 & 3.95\({}^{}\) & 56.61 \\
**InternLM2-7B** & 5.20 & 7.17 & 4.62 & 5.66 & 49.09 & 36.17 & 23.78 & 3.17\({}^{}\) & 46.52 \\   \\ 
**Mistral-8x7B** & 5.31 & 7.33 & 4.62 & 5.75 & 51.00 & 43.34 & 43.78 & 26.66 & 56.49 \\
**Mistral-7B** & 4.70 & 7.20 & 4.28 & 5.39 & 43.66 & 38.17 & 27.88 & 31.68 & 50.93 \\   \\ 
**Llama2-70B-Chat** & 4.12 & 7.11 & 3.95 & 5.06 & 32.79 & 42.34 & 21.11 & 28.32 & 48.50 \\
**Llama2-13B-Chat** & 3.70 & 7.11 & 3.32 & 4.71 & 30.61 & 24.67 & 22.67 & 31.02 & 44.54 \\
**Llama2-7B-Chat** & 3.44 & 6.02 & 3.21 & 4.22 & 20.81 & 21.00 & 5.33 & 5.67\({}^{}\) & 34.89 \\   

Table 2: Subjective and objective evaluation results on the test set of CriticEval. **Dark gray and shade gray in this and the following tables highlight the best and worst performance.** Objective feedback and meta-feedback scores with \(>0.05\)\(p\)-value are marked with \(\). \(},,},(F_{s})}\) represent feedback, correction, comparison and meta-feedback critique dimensions, respectively. **Overall** column denotes the overall score over multiple critique dimensions.

and Section 6.3. Then, overall analysis is described in Section 6.4. Furthermore, several intriguing phenomena about some critical factors are described, including task types (Section 6.5), the response quality (Section 6.6) and the critique dimensions (Section 6.7). Finally, we elaborate and analyze the fine-grained error patterns of model-generated critiques in Section 6.8. The complete experimental results of all evaluated LLMs on the test/dev set for each task and each critique dimension are placed in Appendix L.

### LLMs to be Evaluated

35 widely used open-source and closed-source LLMs of different sizes are evaluated on CriticEval, including the instruction-tuned LLMs , critique-tuned LLMs that fine-tuned on critique datasets generated by GPT-4 , and reward models . Please refer to Appendix N for all evaluated LLMs and the inference details. The prompt templates for LLMs on critique dimensions are shown in Appendix I with score rubrics listed in Figure 18 in Appendix H.3.

### Reliability of Subjective Evaluation in CriticEval

As LLMs are prompted with human-annotated critiques, their performance in meta-feedback could reveal their reliability for evaluating generated textual critiques. As shown in Table 2 (\(F_{s}(F_{s})\) column) and Table 3, GPT-4-turbo achieves very high correlations (\(62.90\), and \(66.18\)) with human judgment. Although there is still a gap compared to the average human level (\(66.18<79.03\)), the strong correlations ensure the reliable evaluation for textual critique ability . Furthermore, we also conduct the ablation study to prove the contribution of our human-annotated reference critiques. As shown in Table 3, it can be found that all LLMs perform worse when the reference critiques (**ref.**) are removed (average -\(13.36\) performance decrease), proving their significant contribution for reliable subjective evaluation in our proposed CriticEval.

Moreover, except for the feedback critique dimension, we also test the reliability of subjective evaluation on the correction and comparison critique dimensions. Specifically, we ask three human annotators to annotate the quality score of 450 critiques generated by five representative LLMs (GPT-3.5-turbo, Qwen-72B-Chat, InternLM2-20B-Chat, Mistral-7B and ChatGLM3-6B) from 9 tasks in CriticEval, and all the human annotators are guided by the same evaluation protocol in our subjective evaluation. The results are shown in Table 4. It can be found that GPT-4-turbo, with our human-annotated critiques as references, could achieve a very strong correlation with human judgments, close to the average human level. This observation proves the robust and reliable subjective evaluation of the textual critiques in the correction and comparison dimensions. Besides, the correlation scores on the correction and comparison critique dimension are higher than the feedback dimension. This phenomenon suggests that the feedback of the feedback is more challenging than the feedback of correction and comparison.

### More Effective Critiques Consistently Lead to Superior Corrections

Although the reliability of subjective evaluation has been proven in Section 6.2, it is still unknown whether **real feedback critique ability of LLMs is consistent with the evaluation results in CriticEval**. To explore this, we prompt the InternLM2-20B-Chat and Llama2-70B-Chat models to revise responses from CriticEval using three sources of feedback with varying quality levels. As

    & **Objective** & **Subjective** \\   & \(F_{s}\) & \(CR\) & \(F_{s}\) \\ 
**Human Avg.** & 87.04 & 76.55 \\
**GPT-4 w/ ref.** & 82.10 & 70.27 \\   

Table 4: Correlations in \(CR\) and \(F_{c}\) dimensions. \(p\)-value \(<0.05\).

    & **Objective** & **Subjective** \\   & \(F_{s}\) & \(CR\) & \(F_{s}\) & \(CR\) \\ 
**InternLM2-20B-Chat** & **Llama2-70B-Chat** & 2.24 & 7.15 & 5.63 & 5.71 \\
**InternLM2-20B-Chat** & **InternLM2-20B-Chat** & 7.53 & 10.33 & 6.85 & 5.80 \\
**InternLM2-20B-Chat** & **Human-Annotated** & **8.00** & **50.50** & **8.00** & **7.48** \\ 
**Llama2-70B-Chat** & **Llama2-70B-Chat** & 2.24 & 5.33 & 5.63 & 5.54 \\
**Llama2-70B-Chat** & **InternLM2-20B-Chat** & 7.53 & 12.47 & 6.85 & 6.32 \\
**Llama2-70B-Chat** & **Human-Annotated** & **8.00** & **42.34** & **8.00** & **7.11** \\   

Table 5: The quality of corrections \(CR\) increases as the quality of feedback increases.

illustrated in Table 5, a clear and consistent trend emerges: as the quality of the feedback increases, both the objective and subjective revision performance improves. This finding underscores that real critique ability of LLMs aligns closely with the evaluation results in our proposed CriticEval, _i.e._, critiques of LLMs with higher scores are more accurate and effective for corrections.

In summary, the reliability of evaluation in CriticEval has been well proven. Following sections will describe the overall analysis and relationships between critique ability and several crucial factors.

### Overall Analysis of LLMs

As shown in Table 2, GPT-4 significantly outperforms other LLMs on most critique dimensions, while slightly underperforms our human-annotated critiques (\(7.81<8\)). Surprisingly, open-source LLMs are approaching state-of-the-art closed-source LLMs. For example, InternLM2-20B-Chat surpasses GPT-3.5-turbo on overall subjective scores (\(6.20>5.89\)). Furthermore, there is a clear relationship that the critique ability of LLMs improves steadily as the number of parameters increases (Table 2), suggesting that the critique ability of LLMs highly correlates with their capability. We also provide a clear diagram to show this relationship in Figure 14 in Appendix R. Beyond the average scores, we also categorize the textual critiques of LLMs into multiple quality intervals for more interpretable analysis, which are described in Appendix O.

The results of critique-tuned LLMs in the feedback dimension on the test set are shown in Table 6. It can be found that critique-tuned LLMs fine-tuned from Llama-2-13B significantly outperform even the Llama-2-70B-Chat model, proving the effectiveness of critiques datasets . The results of representative reward models are shown in Table 7. From these results, it can be found that reward models like UltraRM-13B achieve impressive performance in scoring the quality of responses, significantly outperforming GPT-3.5-turbo. This observation aligns with findings in recent works .

### Relationship with Task Type

Effective critiques usually require domain knowledge and understanding of given tasks. We analyze the relationship between critique ability and task type in Table 8, which shows the average performance of all evaluated LLMs.

Feedback, ComparisonLLMs achieve much higher scores in the first five tasks than on math reasoning and coding tasks, indicating math reasoning and code tasks are more challenging.

Meta-FeedbackLLMs achieve much higher consistency with human judgments on code and math reasoning tasks, indicating that evaluating textual critiques in math reasoning and code tasks is more reliable.

Correction

Math reasoning tasks are more challenging than coding tasks, and CodeExec is easier to revise than CodeNE due to the richer information in execution results. Except for math reasoning and coding tasks, the translation is the most challenging task because professional domain knowledge is required, while harmlessness is the easiest to refine since most LLMs have been trained to avoid harmful generations . Furthermore, we explore the variance in correction quality on reasoning and coding tasks (**Obj.** in Table 9) and other subjective tasks (**Sub.** in Table 9).5 Specifically, three kinds of feedback are used for correction: (1) **H**uman-annotated **F**eedback (**HF**); (2) **E**mpt

  
**Models** & **Sub.** & **Obj** \\ 
**Llama-2-13B** & 3.70 & 30.61 \\
**Llama-2-70B** & 4.12 & 32.79 \\
**Auto-J-13B** & 4.21 & 36.05 \\
**UltraCm-13B** & 4.12 & 21.51 \\
**TigerScore-13B** & 3.31 & 17.87 \\   

Table 6: Critique-tuned LLMs results in feedback dimension.

  
**Models** & **\(}\)** & \(}\) \\ 
**GPT-3.5-turbo** & 51.44 & 40.67 \\ 
**UltraRM-13B** & **52.33** & **54.67** \\
**Ziya-7B** & 25.81 & 40.00 \\   

Table 7: Reward model objective results in \(F_{s}\) and \(F_{c}\) dimensions.

    &  & **Obj.** & **Sub.** & **Obj.** & **Sub.** & **Obj.** & **Sub.** & **Obj.** \\ 
**Translate** & 4.43 & 31.14 & 3.78 & 18.28 & 5.31 & - & -2.93 \\
**Chat** & 5.09 & 20.60 & 4.97 & 32.60 & 5.66 & - & 1.80 \\
**OA** & 5.20 & 30.75 & 5.05 & 27.67 & 6.42 & - & 13.50 \\
**Summary** & 4.76 & 28.93 & 4.63 & 73.12 & 5.99 & - & 0.54 \\
**Harness** & 5.12 & 25.04 & 3.97 & 19.35 & 7.51 & - & 2.71 \\ 
**Avg.** & **4.92** & **27.29** & **4.48** & **27.00** & 6.18 & - & -3.12 \\ 
**MathCoT** & 3.55 & 22.56 & 2.80 & 12.42 & - & 29.36 & 19.63 \\
**MathPoT** & 3.35 & 27.80 & 3.05 & 14.98 & - & 24.98 & 22.73 \\
**CodeExec** & 3.07 & 13.38 & 2.74 & 7.72 & - & 32.20 & 25.50 \\
**CodeNE** & 2.77 & 10.37 & 2.80 & 10.33 & - & 29.50 & 24.38 \\ 
**Avg.** & 3.19 & 18.53 & 2.85 & 11.36 & - & 29.01 & **23.06** \\   

Table 8: Two **Avg.** rows represent the average scores of all LLMs on the first \(5\) tasks and the last \(4\) tasks.

  
**Dimension** & **Sub.** & **Obj.** \\  \(\) w/ HF & **7.12** & **43.66** \\ \(\) w/ SF & 5.48 & 13.01 \\ \(\) w/ EF & 5.16 & 14.44 \\   

Table 9: Average performance of evaluated LLMs on test set.

improve responses without any feedback; and (3) LLMs **S**elf-generated **F**eedback (**SF**). As shown in Table 9, it can be found that self-generated feedback is beneficial to corrections on subjective evaluation (\((7.12)>(5.48)>(5.16)\)), while it might negatively affect corrections on objective evaluation of math reasoning and coding tasks (\((13.01)<(14.44)<(43.66)\)). This observation proves that LLMs struggle in self-improvement on challenging reasoning tasks, aligning with recent findings [54; 4].

### Relationship with Response Quality

Before analyzing the relationship between response qualities and critique ability, it is essential to categorize the error patterns in responses. We highlight that the error patterns are related to the task type, complicating the classification of errors. To conduct a representative analysis of errors in all tasks, human annotators are asked to categorize errors into three patterns, which collectively encompass nearly all the cases: (1) **Obvious error** is easy to critique and correct, like apparent misuses of words in translation task; (2) **Complex error** is challenging to correct, regardless of whether critiques are easy to critique, like logical reasoning error in reasoning tasks; (3) **Subtle error** is hard to critique, while it is usually easier to revise than complex error, like slight misunderstandings of context in general chat. The distribution presented in Table 10 reveals distinct primary errors across different response qualities. More details about these error patterns in each task are described in Appendix P.

Given the distribution of error patterns, we analyze critique ability of LLMs on responses with varying qualities. As shown in Table 11, high-quality responses are the hardest for feedback since they contain lots of subtle errors (Table 10). Note that the medium-quality responses have higher objective feedback scores than low-quality ones, which is inconsistent with our expectations. This phenomenon is because low-quality responses often receive very low human-annotated quality scores (near 1), while the scoring of LLMs tends to be higher, leading to a discrepancy. For the correction dimension, low-, and high-quality responses are easier to correct than medium-quality due to the most obvious and subtle errors. There are two kinds of qualities for comparison dimension: easy and hard. Most LLMs perform better on easy samples than on hard samples. Specifically, the subjective and objective scores of easy samples are 4.78 and 39.73, respectively, higher than those of hard samples (4.55 and 29.80). For the meta-feedback dimension, LLMs achieve the highest consistency with human judgments on high-quality responses while performing worst on medium-quality responses.

### Relationship with Critique Dimensions

The average scores of all evaluated LLMs on different critique dimensions are shown in Table 12. Objective scores of comparison and correction are not recorded because they are not correlations. Several conclusions can be made: (1) correction is the easiest critique dimension, followed by feedback, and then comparison. This observation demonstrates that comparison requires accurate analysis of both responses, which is more complex than the feedback dimension; (2) As a high-level critique dimension, meta-feedback is more challenging than the feedback.

### Fine-grained Failure Modes in Model-Generated Critiques

This section analyzes the fine-grained failure modes in model-generated critiques across feedback, comparison and correction dimensions. As illustrated in Table 13, human annotators summarize the 12 main failure modes in model-generated critiques. Then, we compute the distribution of these failure modes of all evaluated LLMs. Figure 3 demonstrate that the most frequent failure modes are missing errors (E1, E2), lacing effective comparison analysis (E7) and worse revision than references (E10) for feedback, comparison and correction dimensions, respectively. Furthermore, as shown in Figure 4, it can be observed that missing errors/suggestions (E1, E2) and inaccurate critiques (E3, E4, E8) usually lead to lower subjective scores.

  
**Error Pattern** & **Low** & **Med.** & **High** \\ 
**Obvious** & **74.68** & 29.48 & 20.42 \\
**Complex** & 16.46 & **45.51** & 31.69 \\
**Subtle** & 8.86 & 25.00 & **47.89** \\   

Table 10: Error pattern distribution (%).

    &  &  \\   & \(}\) & \(\) & \(}\) & \(\) & \((F_{s})}\) \\ 
**Low** & **5.14** & **7.17** & 21.93 & **46.04** & 22.73 \\
**Medium** & 4.76 & 7.08 & **23.10** & 40.58 & 19.78 \\
**High** & 4.66 & 7.15 & 20.62 & 45.19 & **28.84** \\   

Table 11: Average performance of LLMs on the different response qualities (test set).

## 7 Conclusion and Future Work

In this paper, we introduce a comprehensive and reliable benchmark for evaluating the critique abilities of LLMs, named CriticEval. Extensive experimental results first prove the reliability of CriticEval, and reveal the promising potential of open-source LLMs, the effectiveness of critique datasets and intriguing relationships between critique capabilities and some factors: task types, response qualities and critique dimensions. These observations significantly promote an in-depth understanding of the critical ability of LLMs and LLM's self-improvement. In the future, we plan to enhance our benchmark in several key areas: (1) Broadening the scope to include more tasks, such as tool-using; (2) Extending the benchmark to encompass other languages, like Chinese; (3) Improving the subjective evaluation protocol to allow for more fine-grained analysis; (2) Continue to evaluate LLMs and track their critique ability, like Llama-3 models; (5) Improving the quality of reference critiques by incorporating additional high-quality critiques from advanced LLMs if and only if their quality surpasses the existing reference critiques.

   \\  & **Failure Mode** & **Description of Failure Mode** \\   & **E1** & Feedback misses some errors. \\  & **E2** & Feedback misses revision suggestions or suggestions are low-quality. \\  & **E3** & Feedback incorrectly analyzes correct content as erroneous. \\  & **E4** & Feedback content contains errors. \\  & **E5** & Feedback is correct but complex. \\  & **E6** & Feedback is not concise, repetitive or irrelevant. \\   & **E7** & Critiques lack effective analysis between two responses. \\  & **E8** & Preference between two responses is wrong. \\   & **E9** & Revision does not follow suggestions in feedback well. \\  & **E10** & Revisions are better but have not reached the reference. \\  & **E11** & There are some errors in revisions. \\   & **Other** & Other Cases \\  

Table 13: Definition of Failure Modes in Feedback, Comparison and Correction critique dimensions.

**E1-E6** denotes the **shared** failure modes of feedback and comparison dimensions, and **E7-E8** belong to comparison dimension. **E9-E11** belong to the correction dimension.

Figure 4: Average subjective score of failure modes in each critique dimension.

Figure 3: Distribution of failure modes in each critique dimension.

Average Subjective Scores (1-10) of Failure Modes