# Mass-Producing Failures of Multimodal Systems with Language Models

Shengbang Tong

Equal contribution

Erik Jones1

UC Berkeley

{tsb, erjones, jsteinhardt}@berkeley.edu

 Jacob Steinhardt

UC Berkeley

###### Abstract

Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies _systematic failures_--generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal models, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-driving cars. We see MultiMon as a step towards evaluation that autonomously explores the long tail of potential system failures. 2

## 1 Introduction

Text-based multimodal systems, which produce images (Rombach et al., 2022), 3d scenes (Poole et al., 2022), and videos (Singer et al., 2022) from text, are extensively tested for failures during development, yet routinely fail at deployment (Rando et al., 2022). This gap exists in part because evaluators struggle to anticipate and test for all possible failures beforehand.

To close this gap, we seek evaluation systems for multimodal models that are _systematic_ and _human-compatible_. Systematic evaluations must peer into the long tail of possible model behaviors; this means that systems cannot assume a priori what behaviors to look for, or be bottlenecked by human labor. Human-compatible evaluations must be useful to the system designer; this means they should describe patterns of behavior beyond giving examples, and be steerable towards the designer's goals.

Towards satisfying these desiderata, we construct a system, MultiMon, that uses large language models to identify failures of multimodal systems (Section 3). MultiMon scrapes individual failures from a corpus, categorizes them into systematic failures (expressed in natural language), then flexibly generates novel instances. MultiMon works autonomously, improves as language models scale, and produces failures that transfer across a range of multimodal systems.

To systematically scrape for individual failures, MultiMon exploits _erroneous agreement_. Specifically, we observe that if two inputs produce the same output but have different semantics, at least one of them must be wrong. We can test whether two inputs produce the same output by comparing their CLIP embeddings, since many multimodal models encode inputs with CLIP before generatingoutputs. Using CLIP similarity circumvents the expensive decoding step of these models, allowing us to tractably scrape large corpora for failures.

With these scraped individual failures as a foundation, MultiMon next uses language models to produce human-compatible explanations. Specifically, we use GPT-4 to identify _systematic failures_: generalizable natural-language descriptions of patterns of failures, from the scraped individual failures. These systematic failures are useful both to qualitatively understand system behavior and to generate new instances. We can even steer generation towards specific attributes, e.g. "salient to self-driving", that are missing from the original corpus but are important for downstream applications.

To evaluate MultiMon, we measure the quantity and quality of the systematic failures. We measure quantity by counting the number of systematic failures generated, and quality by measuring what fraction of the new generated instances have high CLIP similarity.

We find that MultiMon uncovers 14 systematic failures of the CLIP text-encoder, and from them over one thousand new individual failures (Section 4). The systematic failures include failing to encode negation, spatial differences, numerical differences, role ambiguity, quantifiers, and more. These systematic failures are high quality; 12 of the 14 systematic failures produce pairs with high CLIP similarity at least half the time, and 7 produce such pairs at least 75% of the time.

The failures of the CLIP text-encoder transfer to downstream text-to-image, text-to-video, and text-to-3d systems (Figure 1, Section 5). We assess the new individual failures that MultiMon generates on five widely-used text-to-image systems: Stable Diffusion 1.5, Stable Diffusion 2.1, Stable Diffusion XL, DALL-E, and Midjourney 5.1, three of which were released within a month of the writing of this paper. Through a manual evaluation, we find that the systems err on 80.0% of the pairs generated by MultiMon, compared to only 20.5% for a baseline system. We also show that MultiMon can help evaluators identify inputs that evade commercial safety filters (Appendix F). Overall, the MultiMon pipeline--exploiting erroneous agreement to scrape individual failures and finding patterns with language models--is simple and general, and could be a foundation for broader automatic evaluation.

## 2 Related Work

**Text-guided multimodal models.** We study failures of text-guided multimodal models, which generate images (Rombach et al., 2022; Ramesh et al., 2022, 2021), video (Singer et al., 2022; Luo et al., 2023), and 3d-scenes (Jun and Nichol, 2023; Poole et al., 2022; Lin et al., 2022), to name a few output modalities, from textual descriptions. These models tend to first encode text with a vision-language model (VLM), which embeds text and images in a shared embedding space (Radford et al., 2021; Ramesh et al., 2022). They then generate outputs via a guided diffusion process

**Ambiguities and bias in embedding models.**MultiMon exploits failures of the CLIP embedding to produce failures of multimodal systems. This builds off of prior work documenting failures in text

Figure 1: Examples failures that MultiMon generates on state-of-the-art text-to-image systems.

embedding models (Bolukbasi et al., 2016; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019; Sun et al., 2019), including showing that BERT struggles to encode negation (Ettinger, 2020) and large numbers (Wallace et al., 2019). Some work uncovers failures of vision-language embedding models themselves using benchmarks. For example, Thrush et al. (2022) and Yuksekgoul et al. (2023) show that vision-language-models often fail to account for different word orderings.

The closest work to ours is Song et al. (2020), which aims to adversarially construct pairs of inputs that embedding models should not encode simiarly, but do. This work could potentially replace MultiMon's scraping step by generating adversarially constructed pairs without a corpus.

**Systematic failures.**MultiMon aims to automatically identify systematic failures of multimodal systems, without knowing what the failures are a priori. A related line of work automatically identifies slices of data that classifiers perform poorly on, then uses a VLM to choose a slice description (Eyuboglu et al., 2022; Jain et al., 2022; Gao et al., 2022; Wiles et al., 2022; Metzen et al., 2023; Zhang et al., 2023). The main differences to our approach are (i) we do not make use of ground-truth labels and (ii) we generate candidate systematic failures, rather than testing predefined descriptions.

Other work uses humans to conjecture potential systematic failures of generative systems, then shows that models exhibit them. These failures include biases (Maluleke et al., 2022; Grover et al., 2019), propagated stereotypes (Sheng et al., 2019; Abid et al., 2021; Hemmatian and Varshney, 2022; Blodgett et al., 2021), and training data leaks (Carlini et al., 2021, 2023). Liang et al. (2022) capture many language model behaviors via holistic evaluation, while other work surveys additional failures (Bender et al., 2021; Bommasani et al., 2021; Weidinger et al., 2021). Towards making this evaluation more systematic, Jones and Steinhardt (2022) use cognitive biases to identify and test for systematic failures of code models, while Perez et al. (2022) use language models to generate instances of conjectured systematic failures. Nushi et al. (2018) develop a system to help humans identify systematic failures, which they test on a captioning system.

**Automated ways to produce individual failures.**MultiMon builds on work that uses a specification of a class of failures to find examples. Perez et al. (2022) fine-tune a language model to find failures of a second language model, Jones et al. (2023) find language model failures directly using discrete optimization, and Wen et al. (2023) use discrete optimization to find prompts that a text-guided diffusion model generates a specific image from. Towards scraping corpora to find failures without direct supervision, Gehman et al. (2020) scrape a corpus for text that precedes toxic content, which they find often generates toxic text under a language model.

**Using language models to draw conclusions from instances.**MultiMon generates systematic failures by identifying patterns in scraped instances. This builds on a recent line of work that uses large autoregressive language models (Radford et al., 2018, 2019; Brockman et al., 2023; Anthropic, 2023; OpenAI, 2023b) to draw general conclusions from individual instances. Zhong et al. (2022) describe differences in text distributions, Singh et al. (2022) try to explain prediction patterns, and Bills et al. (2023) use activation values to explain model neurons. The closest work to our categorization step is Zhong et al. (2023), which describe differences in distributions that are salient to a target goal.

## 3 The MultiMon Pipeline

We now describe our system, multimodal monitor (MultiMon), which finds failures of the CLIP text embedding model. We check that these failures transfer to downstream systems in Section 5.

### Constructing MultiMon

In this section, we describe MultiMon's three steps, depicted in Figure 2. MultiMon first _scrapes_ a large corpus of sentences for individual failures, which are pairs of sentences that produce the same output, but should not (e.g., "a table with a few cups", "a table with many cups"). It then _categorizes_ these instances into systematic failures, which are generalizable, natural-language descriptions of patterns of failure (e.g., "Quantifiers: models fail to distinguish between quantifiers like "few", "some", or "many"). It finally _generates_ new candidate individual failures and checks their validity.

**Scraping.**MultiMon first scrapes a corpus to collect an initial set of individual failures. To do this, it considers every possible pair of examples from corpus, then returns pairs that produce similar outputs, but are semantically different--this means that at least one output is incorrect.

To measure whether two inputs produce similar outputs, we compare their CLIP embeddings, since many multimodal models encode inputs with CLIP before generating outputs. To measure whether inputs have different semantics, we compare them under a reference embedding model (in our case, DistillRoBERTA). We return the \(n\) pairs of inputs with highest CLIP cosine similarity, such that the cosine similarity of their reference embeddings is below a threshold \(\). This process is automatic and, importantly, efficient: by exploiting the CLIP embedding bottleneck of multimodal models, we avoid ever running their decoders, which can be very expensive (e.g., generating a video or 3d-image).

**Categorizing.** After scraping many individual failures, MultiMon categorizes them into general systematic failures. To do so, MultiMon queries a language model with the prompt below ([...] indicates further text that is omitted here for space; see Appendix B.1 for the full prompt).

**Prompt**:

I will provide a series of examples for you to remember. Subsequently, I will ask you [...] [n individual failures] The above are some pairs of sentences that an embedding model encodes very similarly. Using these examples, are there general types of failures that the embedding model is making? Give failures that are specific enough that someone could reliably produce [...]

We choose \(n\) such that this prompt fits in the model's context window. Empirically, the language model always produces a list of systematic failures under our prompt, which can be parsed automatically. For example, the first items in the list that the language model (in this case GPT-4) generates are

**Model output**:

1. Negation: Embedding models may not correctly capture the negative context in a sentence, leading to similarities between sentences with and without negation,
2. Temporal Differences: Embedding models might not differentiate between events happening in the past, present, or future.

To generate more systematic failures, the language model can be queried multiple times with the same prompt, as language models often generate outputs stochastically.

**Generating.**MultiMon's final step is generation, where it starts with the systematic failures from the categorization step, then queries a language model to generate arbitrarily many new individual failures. To do so, MultiMon queries a language model with the prompt below.

Figure 2: The MultiMon pipeline. **Left.**MultiMon starts with a corpus of sentences (dots), then identifies _individual failures_: pairs that have similar CLIP embeddings but should not (circled red dots). **Center.**MultiMon takes the individual failures, then categorizes them into systematic failures using a language model. **Right.**MultiMon takes the systematic failures, then generates new individual failures from them using a language model, which then generate incorrect images.

**Prompt:**

Write [m] pairs of sentences that an embedding model with the following failure might encode similarly, even though they would correspond to different images if used [...]

[Description of systematic failure]

See Appendix B.2 for the full prompt. We set \(m\) to be the maximum number of examples the generator can empirically produce in a single response. To generate subsequent instances, we query the language model in the same dialog session with the same prompt (but add "additional" after [m]).

### Steering MultiMon

Our construction of MultiMon outputs systematic and individual failures that capture system behavior, but may not be relevant to specific use-cases. To remedy this, we next show how to _steer_ MultiMon towards failures in a specific subdomain of interest. MultiMon can be steered during the scraping process (by choosing different individual failures to categorize), and during the generation process (by prompting language models to generate salient failures).

**Steering towards systematic failures.** To steer towards systematic failures that are related to a specific subdomain of interest, we edit the scraping stage of our pipeline. Specifically, we search for pairs of examples that a classifier identifies as relevant to the target subdomain, but that still have similar CLIP and different DistilRoBERTA embeddings. Intuitively, this constrains the categorizer to find only systematic failures that arise in the subdomain of interest.

**Steering towards individual failures.** To steer towards individual failures that are related to the target subdomain, we edit the generation stage of our pipeline. Specifically, we append "Keep in mind, your examples should be relevant to [subdomain]" to the generation prompt from Section 3.1. We generate instances using the unmodified descriptions of systematic failures from Section 3.1.

### Evaluating MultiMon

We want systems like MultiMon to find many high-quality systematic failures. We thus care about both the quantity and quality of failures produced, and for domain-specific use cases we also care about relevance of the failures.

To evaluate quantity, we simply count the number of systematic failures each system finds in the categorization step of the pipeline.

To evaluate the quality of a systematic failure, we measure the quality of instances generated from it. Specifically, we generate \(k\) new instances (candidate pairs) from the description of a systematic failure, using the generation step in Section 3.1. We say that a candidate pair is _successful_ if its CLIP similarity is above a threshold \(t\), chosen such that pairs with CLIP similarity above \(t\) tend to produce visually indistinguishable images. We then define the _success rate_ as the percentage of the \(k\) pairs that are successful. The success rate gives a quantitative metric of _how useful_ a qualitative description is for producing new failures.

Finally, to evaluate relevance, we test whether the systematic and individual failures are relevant to the subdomain of interest. We measure this with the _relevance rate_: the fraction of generated individual failures that are relevant to the subdomain of interest according to a binary classifier. We measure the relevance of systematic failures by generating new instances with the unmodified generation prompt from Section 3.1, and measure the relevance of individual failures directly.

## 4 Automatically Finding Failures of CLIP

In this section, we use MultiMon to produce systematic failures, and from them new individual failures (Section 4.1), using the methods described in Section 3. We then adjust MultiMon to steer towards specific kinds of systematic and individual failures (Section 4.2).

### Identifying systematic failures of CLIP with MultiMon

We first wish to evaluate whether MultiMon can successfully uncover failures of the CLIP text encoder. Specifically, we aim to measure whether MultiMon manages to find many systematicfailures, and whether these failures are high-quality, as measured by their success rates. We also wish to understand how both the language model and the input corpus affect the failures we recover.

To conduct this evaluation, we test the MultiMon system described in Section 3. During the scraping stage, we return the \(n=150\) pairs with highest CLIP similarity, and use a semantic similarity threshold of \(=0.7\).3 For the input corpus we test both SNLI [Bowman et al., 2015] and MS-COCO Captions [Lin et al., 2014]. For the language model categorizer, we consider GPT-4 [OpenAI, 2023b], Claude v1.3 [Anthropic, 2023], and GPT-3.5 [Brockman et al., 2023], and use GPT-4 as a generator unless otherwise noted.

**Assessing the quantity of systematic failures.** We first examine how many systematic failures MultiMon can produce. Specifically, we prompt each language model three times, and report the aggregate list of systematic failures that it returns in Figure 3. We find that GPT-4 identifies 14 systematic failures across the two corpora, while Claude finds 11 and GPT-3.5 finds only 8. The corpus also dictates what systematic errors MultiMon finds; for example, only COCO uncovers temporal differences as a source of failures, and the same is true for SNLI and numerical differences.

Some of the systematic failures we uncover were found in prior work using benchmarks. Yuksekgonul et al.  show that CLIP embeddings act like bag-of-words models, while Ettinger  find that BERT many not encode negation. MultiMon produces these failures autonomously, and uncovers new systematic failures in addition to these known ones.

**Assessing the quality of systematic failures.** We next measure the quality of the generated systematic failures, as measured by the success rate (Section 3.3). To compute success rate, we use GPT-4 to generate \(k=82\) new instances4 and set the CLIP similarity threshold for success to be \(t=0.88\) (we choose 0.88 based on an empirical study; see Section 5.1 for details).

We report the success rate in Figure 3. Overall, we find that the success rate when generating new instances is usually high, but varies across models even for the same systematic failure. For systematic failures found by all three models, GPT-4 had an average success rate of 80.2%, compared to 83.3% for Claude and 69.5% for GPT-3.5. This is because the models produce different quality descriptions (i.e., GPT-4 might produce a more detailed, useful, and faithful description of a failure than GPT-3.5).

These results demonstrate that MultiMon already produces many high-quality systematic failures, that better language models tend to improve the systematic failures generated (suggesting that MultiMon will continue to improve in the future), and that different input corpora find different failures (suggesting that highly diverse corpora or ensembles of corpora produce the best results).

Figure 3: We report whether each LM-corpus pair uncovers each systematic failure (checkmark), along with the success rate. Both the language model and corpus influence the systematic failures that MultiMon uncovers. We include raw success rates and error bars in Appendix C.3.

**Ablations.** Language models generate high-quality systematic failures from individual ones, but might have seen the systematic failures during training. To verify this is not the case, we prompt language models to produce systematic failures without the scraped individual failures the corpus, and find that they only identify 2 of the 14 systematic failures and that the average success rate is 29.3% (Appendix C.6). This low success rate implies that even for failures that are identified without the corpus, the resulting description is low-quality.

Secondly, all of our results use GPT-4 to generate new individual failures. To isolate the role of the language model generator and check robustness, we replace GPT-4 with Claude and GPT-3.5 when generating new failures. We find that Claude tends to produce similar success rates on average, though there is variability across different failures. In contrast, GPT-3.5 is worse (Appendix C.5). This suggests that improving language models would improve generation, in addition to categorization.

### Steering MultiMon towards specific applications

In this section, we demonstrate that evaluators can _steer_ MultiMon towards failures in a specific subdomain of interest, using "self-driving" as an illustrative example. As we describe in Section 3.3, MultiMon can be steered towards systematic failures (by choosing different examples to categorize), and towards individual failures (by prompting language models to generate salient failures).

**Steering towards systematic failures.** We first steer towards systematic failures that are related to self-driving, by editing the scraping stage of our pipeline with the method described in Section 3.3. We use a zero-shot GPT-3.5 classifier to identify instances that are relevant to self-driving (Appendix C.7), and the same classifier to compute the relevance rate.

We report the full results in Table 8 in Appendix C.7, and find that MultiMon generates five systematic failures that are relevant to self-driving, four of which have success rates over 95%. Moreover, the systematic failures consistently generate pairs that are relevant to the subdomain of interest; all failures have relevance rates above 90%, and four out of 5 have a 100% relevance rate.

Some of these systematic failures that MultiMon recovers are similar to those found in Section 4.1, but the descriptions tend to be different; for example, MultiMon identifies "attribute differences" with and without steering, but outputs the description _The model may not differentiate between important attributes of objects, such as "The pedestrian is crossing the street" and "The cyclist is crossing the street."_ when steered towards self-driving.

**Steering towards individual failures.** We next steer towards individual failures that are related to self-driving, by editing the generation stage of our pipeline with the method described in Section 3.3. Using the systematic failures from Section 4.1 and the modified generation stage, we find that the generated instances are often failures and related to self-driving; 74.6% of the instances are successful (i.e. have

Figure 4: Examples of inputs that MultiMon generates. Since MultiMon uses CLIP to find failures, a single input produces the same error in many state-of-the-art text-to-image systems.

high CLIP similarity), while 95.0% of pairs are relevant. Though relevance is computed with the GPT-3.5 classifier automatically, we empirically find the examples we generate are consistently related to self-driving; for example, using the systematic failure "action state differences", MultiMon generates examples such as _"Autonomous vehicle approaching a stop sign" and "Autonomous vehicle ignoring a stop sign"_.

Steering generation also allows MultiMon to generate failures that are not in the distribution of the original corpus. We show this by steering towards failures relevant to "Pokemon Go", which was released after both of the corpora we test. We manage to obtain an average success rate of 66.9% and relevance rate of 82.5%, and find examples like _"Team Mystic dominating a Pokemon Go gym", "Team Mystic not dominating a Pokemon Go gym"_.

We include additional experimental details, results, and generated individual failures in Appendix C.7.

## 5 Failures of CLIP lead to Failures Downstream

We next check that the failures generated by MultiMon produce errors not just in the CLIP embeddings, but in downstream state-of-the-art multimodal systems. Through manual labeling, we find that text-to-image models fail frequently (i.e., produce incorrect images) on our generated inputs (Section 5.1). We then show how the same prompt can produce failures on many state-of-the-art systems, and include qualitative examples of failures using state-of-the-art text-to-image, text-to-video, and text-to-3d models (Section 5.2).

### Manually evaluating generated images

We check that the inputs generated by MultiMon produce errors in downstream systems by manually labeling whether the output images match the generated input text. We also plot the error rate against CLIP similarity, and use this to justify the CLIP similarity threshold chosen in Section 4.

To measure whether MultiMon produces errors in downstream systems, we test the candidate pairs generated from systematic failures in Section 4.1. We say a candidate pair is a successful _downstream failure_ if at least one input in the pair produces an incorrect image. To measure this, we create an annotation UI (Appendix D.1) where annotators are shown one generated image from the pair along with both text inputs, and asked whether the image corresponds to input 1, input 2, or neither input. The annotators also report whether the text inputs describe the same set of images; e.g., "A nice house" and "A lovely house". An input pair is a downstream failure if at least one image is labeled with an incorrect input or with "neither".

When evaluating MultiMon, we want to ensure the failures found are nontrivial, since models may be brittle on any out-of-distribution input rather than the specific ones found by our system. To test this, we introduce a baseline system that ablates MultiMon's scraping stage. Specifically, the baseline scrapes random pairs from MS-COCO (without ensuring high CLIP similarity), then categorizes these into systematic failures and generates new individual failures normally. Since the categorization and generation stages are fixed, the pairs we produce seem plausible; e.g., "A woman painting a beautiful landscape", and "A beautiful landscape painting on a wall".

In total, we generate 100 input pairs with MultiMon and 100 pairs with the baseline. For each pair, we randomly select one of four text-to-image systems (Stable Diffusion XL, Stable Diffusion 2.1, Stable Diffusion 1.5, Midjourney 5.1) to generate images, label each image in the annotation UI, then combine the annotations to classify whether the pair is a failure. Annotations were performed by two authors, who were blinded to whether image pairs came from the baseline or from MultiMon.

We find that MultiMon produces far more downstream failures than the baseline; 80% of the pairs that MultiMon generates are downstream failures, compared to only 20% of the baseline pairs. We then use these results to calibrate the CLIP similarity threshold from Section 4, which aims to capture when outputted images are visually indistinguishable. To do so, we histogram the ratio of downstream failures versus the CLIP similarity (Figure 10 in Appendix D.2). We find that the ratio grows roughly monotonically, and set the threshold at a jump at 0.88 where 65% of pairs are failures. We include the user-interface, additional details, and additional results in Appendix D.1.

### Qualitative examples on state-of-the-art multimodal models

We next showcase how MultiMon produces compelling qualitative examples of failures on state-of-the-art text-to-image, text-to-video, and text-to-3d systems, including examples steered towards self-driving. These examples are easy to obtain using MultiMon; we simply take the pairs from Section 4, run both inputs through the model, and select one incorrect output.

Text-to-image models.MultiMon produces failures on all state-of-the-art text-to-image models: Stable Diffusion XL , Stable Diffusion 2.1 , Midjourney 5.1  and DALL-E . We access Stable Diffusion XL via DreamFusion, Stable Diffusion 2.1 via Huggingface , Midjourney via Discord fast mode, and DALL-E via New Bing. We present examples in Figure 4, and in Appendix D.3.

These results demonstrate how state-of-the-art diffusion models cannot overcome the failures of CLIP embeddings: the same inputs produce failures across all tested text-to-image systems. They also show that MultiMon can quickly find failures of new systems as they are released: two models that we test were released within two weeks of the writing of this paper, and three within a month.

Text-to-3D models.MultiMon produces failures on a state-of-the-art text-to-3D system, Shap-E . We access Shap-E via Huggingface. In Figure 5, we present an example where Shap-E ignores numerical quantities (by including too many chairs at a dining room table), and include more examples in Appendix D.4.

Text-to-video models.MultiMon also produces failures in _dynamic scenes_: we show that the pairs that MultiMon generates produce failures on the best open-source text-to-video system, VideoFusion . We access VideoFusion via Huggingface. In Figure 5, we present an example where VideoFusion struggles to capture differences in action states: "a wind turbine at rest" generates a video where the turbine is moving. Note that "a wind turbine at rest" and "a wind turbine in motion" might have been visually identical in static scenes, but are semantically distinct in video.

Steering Towards Applications. We next show that MultiMon can be steered to produce specific kinds of downstream failures. Using the pairs generated in in Section 4.2, we exhibit self-driving-related failures in text-to-image, text-to-3d, and text-to-video systems (Figure 6). These include image examples where a car is in the incorrect lane, a 3d-scene example where a stop sign is mixed up with a yield sign, and a video of a car erroneously running through a red light. These examples could be salient to multimodal systems deployed in self-driving settings, but would have been challenging to uncover without explicitly steering MultiMon towards the target subdomain.

### Downstream failures beyond CLIP

We additionally show that MultiMon can find failures in downstream systems that do not use CLIP. Specifically, we show that MultiMon can be used to find failures in text-to-image systems that do not use CLIP (Appendix E), and that MultiMon can circumvent safety filters on CLIP-based

Figure 5: **Top. Example of a 3d-scene Shape-E generates with 8 chairs instead of 4, rotated at different angles. Bottom. Example of a video VideoFusion generates of a wind turbine spinning, instead of at rest, captured at different frames.**

## 6 Discussion

In this work, we produce failures of text-guided multimodal systems by scraping failures using erroneous agreement, then categorizing and generating new failures with language models. Our resulting system, MultiMon, automatically finds failures that generalize across state-of-the-art text-to-image, text-to-video, and text-to-3d systems.

There is room for improvement at each stage of the MultiMon pipeline. For example, we could find ways to scrape individual failures that erroneous agreement does not catch, or use better prompts at the categorization and generation steps. However, MultiMon will naturally improve as language models do, since better language models can seamlessly plug into our pipeline. Subsequent work could even use MultiMon to improve other systems, e.g., via fine-tuning on failures.

Our pipeline can in principle find failures with any system (e.g., large language models), since erroneous agreement is agnostic to the system architecture, input, or output type. MultiMon is especially well-suited to multimodal systems, since erroneous agreement can be efficiently computed between embeddings; we thus find failures without ever generating outputs, which can be expensive (over one minute per output) for some of the models that we test. Subsequent work could design methods to efficiently approximate erroneous agreement for other systems, like language models or classifiers, by studying when inputs produce similar outputs but should not.

Our work demonstrates how recycling the same components across systems (such as CLIP) may inadvertently add new risks; the inputs that MultiMon generates produce failures across all of the multimodal systems that we test, since they all (likely) rely on CLIP to encode text. These failures are also hard to fix post-hoc: repairing the CLIP embeddings would not be enough, since most downstream models would have to be retrained on the new embeddings. This is related to the issue of _algorithmic monoculture_, where models that use similar algorithms (Kleinberg and Raghavan, 2021), or that are trained with similar data (Bommasani et al., 2022), produce homogeneous errors. Components that are likely to be recycled across many models, like CLIP or GPT-4, should undergo more rigorous testing and updates before deployment.

More broadly, to address the robustness problems of the future, we need _scalable evaluation systems_: evaluation systems that (i) improve naturally via existing scaling trends, and (ii) and are not bottlenecked by human ingenuity. Model outputs like videos, proteins, and code are challenging and time-consuming for humans to evaluate, and can be incorrect in ways that are difficult to predict a priori. Developing scalable evaluation systems is critical as models improve, as models may reach the point where only machines can anticipate, detect, and repair their failures.

Figure 6: Examples of failures that are relevant to “self-driving”. These include images (top left, showing incorrect positions and colors), a 3d-scene (top right, depicting stop instead of yield sign), and a video (bottom, showing a car in the background erroneously not stopping for a light).