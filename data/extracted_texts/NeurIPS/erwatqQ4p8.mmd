# Mixture of Experts Meets

Prompt-Based Continual Learning

 Minh Le\({}^{3}\)  An Nguyen\({}^{2}\)  Huy Nguyen\({}^{1}\)  Trang Nguyen\({}^{3}\)

**Trang Pham\({}^{3}\)**

Equal contribution.

Linh Van Ngo\({}^{2}\)  Nhat Ho\({}^{1}\)

\({}^{1}\) The University of Texas at Austin

\({}^{2}\) Hanoi University of Science and Technology

\({}^{3}\) VinAI Research

###### Abstract

Exploiting the power of pre-trained models, prompt-based approaches stand out compared to other continual learning solutions in effectively preventing catastrophic forgetting, even with very few learnable parameters and without the need for a memory buffer. While existing prompt-based continual learning methods excel in leveraging prompts for state-of-the-art performance, they often lack a theoretical explanation for the effectiveness of prompting. This paper conducts a theoretical analysis to unravel how prompts bestow such advantages in continual learning, thus offering a new perspective on prompt design. We first show that the attention block of pre-trained models like Vision Transformers inherently encodes a special mixture of experts architecture, characterized by linear experts and quadratic gating score functions. This realization drives us to provide a novel view on prefix tuning, reframing it as the addition of new task-specific experts, thereby inspiring the design of a novel gating mechanism termed Non-linear Residual Gates (NoRGa). Through the incorporation of non-linear activation and residual connection, NoRGa enhances continual learning performance while preserving parameter efficiency. The effectiveness of NoRGa is substantiated both theoretically and empirically across diverse benchmarks and pretraining paradigms. Our code is publicly available at https://github.com/Minhchuyentoanchon/MoE_PromptCL.

## 1 Introduction

Humans possess a remarkable ability to learn continuously by integrating new skills and knowledge while retaining past experiences. However, current AI models often fail to retain this ability. Unlike humans, they often suffer from _catastrophic forgetting_, a phenomenon where they struggle to retain knowledge from previous tasks while learning new ones. Inspired by human learning, Continual Learning  is an ongoing field that aims to train a model across a sequence of tasks while mitigating this challenge. Traditional continual learning methods often rely on storing past data for fine-tuning, which can raise concerns about memory usage and privacy . To address these limitations, prompt-based approaches have emerged as a promising alternative within rehearsal-free continual learning. By attaching prompts - small sets of learnable parameters - to a frozen pre-trained model, these approaches enable efficient adaptation to new tasks with minimal modifications to the underlying model . The effectiveness of prompt-based methods has been demonstrated by several recent works achieving state-of-the-art performance on various continual learning benchmarks .

While prompt-based methods have demonstrably achieved impressive results, their emphasis largely lies on prompt utility, leaving a gap in our theoretical comprehension of their effectiveness. Thisabsence of a theoretical foundation hinders our ability to further refine and optimize these methods. In this work, we offer a new perspective by focusing on prefix tuning  and its connection to mixture of experts models [19; 15; 13; 11]. We demonstrate that self-attention blocks in Vision Transformers  implicitly encode a special mixture of experts architecture, revealing a surprising connection between these seemingly disparate concepts. Leveraging this connection, we propose that applying prefix tuning within pre-trained models can be interpreted as introducing new experts. The newly introduced experts collaborate with the pre-trained experts, facilitating efficient adaptation of the model to new tasks.

Drawing insights from this analysis, we observe that the original prefix tuning suffers from suboptimal sample efficiency, requiring a substantial amount of data for reasonable parameter estimation. To address this challenge, we propose a novel gating mechanism termed Non-linear Residual Gates (NoRGa). This architecture integrates non-linear activation functions and residual connections within the gating score functions. Our work focuses on improving within-task prediction accuracy, a key component of continual learning performance as identified in previous research [22; 49]. We posit that NoRGa can enhance this aspect, which contributes to improved overall continual learning performance while maintaining parameter efficiency. We further provide theoretical justification for this improvement, demonstrating how NoRGa accelerates parameter estimation rates.

**Our contributions** can be summarized as follows: (1) We reveal a novel connection between self-attention and a mixture of experts, providing a fresh perspective on prompt-based continual learning approaches; (2) Leveraging this insight, we propose _Non-linear Residual Gates (NoRGa)_, an innovative gating mechanism that enhances continual learning performance while maintaining parameter efficiency, and provide a theoretical justification for this improvement; (3) Extensive experiments across various continual learning benchmarks and pre-training settings demonstrate that our approach achieves state-of-the-art performance compared to existing methods.

**Notation.** For any \(n\), we denote \([n]\) as the set \(\{1,2,,n\}\). Next, for any set \(S\), we let \(|S|\) stand for its cardinality. For any vector \(u:=(u_{1},u_{2},,u_{d})^{d}\) and \(:=(_{1},_{2},,_{d})^{d}\), we let \(u^{}=u_{1}^{_{1}}u_{2}^{_{2}} u_{d}^{_{d}}\), \(|u|:=u_{1}+u_{2}++u_{d}\) and \(!:=_{1}!_{2}!_{d}!\), while \(\|u\|\) stands for its \(2\)-norm value. Lastly, for any two positive sequences \(\{a_{n}\}_{n 1}\) and \(\{b_{n}\}_{n 1}\), we write \(a_{n}=(b_{n})\) or \(a_{n} b_{n}\) if \(a_{n} Cb_{n}\) for all \(n,\) where \(C>0\) is some universal constant. The notation \(a_{n}=_{P}(b_{n})\) indicates that \(a_{n}/b_{n}\) is stochastically bounded.

## 2 Background and Related Works

We first provide background and related works on continual learning. Then, we define the attention mechanism, followed by discussions on prompt-based continual learning and mixture of experts.

**Continual Learning (CL)** addresses the challenge of training a model incrementally on a sequence of \(T\) tasks, denoted by \(=\{_{1},...,_{T}\}\). Each task's training data \(_{t}=\{(_{i}^{(t)},y_{i}^{(t)})\}_{i=1}^{N_{t}}\) contains pairs of input sample \(_{i}^{(t)}^{(t)}\), and corresponding label \(y_{i}^{(t)}^{(t)}\). Notably, the class labels are distinct for each task, _i.e._, \(^{(t)}^{(t^{})}=, t t^ {}\). Consider a neural network with a backbone function \(f_{}\) and an output layer \(h_{}\). The model predicts a label \(=h_{}(f_{}())=_{t=1}^{T} ^{(t)}\), where \(=_{t=1}^{T}^{(t)}\) is an unseen test sample from arbitrary tasks. Importantly, during training on a new task, the model can only access the current data, without access to data from previous tasks. Prior approaches often rely on storing past task samples for training on new tasks, raising concerns regarding storage and privacy [5; 6; 39; 51; 59].

Our work focuses on the class-incremental learning (CIL) setting, where task identities are not provided during inference, unlike in task-incremental learning (TIL) . A recent theory by  analyzes the CIL objective by decomposing the probability of a test sample \(\) of the \(j\)-th class in task \(t\) into two probabilities:

\[P(_{j}^{(t)}|)=P(_{j}^{(t)} |^{(t)},)P(^{(t)}|),\] (1)

where the first term involves within-task prediction (WTP) and the second term pertains to task-identity inference (TII). This equation highlights that by improving either the WTP performance or the TII, we can consequently improve the overall CIL performance, as shown in [22; 49].

**Attention Mechanism.** Within the Transformer architecture, the attention mechanism plays a crucial role. One prevalent variant is scaled dot-product attention, formally defined as follows:

**Definition 2.1** (Scaled Dot-Product Attention).: Let \(^{N d_{v}}\) be a _key_ matrix with \(N\) key vectors, and \(^{N d_{v}}\) be a _value_ matrix with \(N\) corresponding value vectors. Given a _query_ matrix \(^{M d_{k}}\), _Attention_ over \((,)\) is defined as

\[(,,)=( ^{}}{}})\] (2)

where the softmax function acts on the rows of matrix \(^{}^{M N}\).

Vision Transformer (ViT)  employs the same attention mechanism within multiple Multi-head Self-Attention (MSA) layers, which is formally defined as follows:

**Definition 2.2** (Multi-head Self-Attention Layer).: Let \(^{Q},^{K},^{V}\) denote the input query, key, and value matrix, respectively, where \(^{Q}=^{K}=^{V}=[_{1},...,_{N}]^{}^{N d}\), and \(N\) is the length of the input sequence. The output is expressed as

\[(^{Q},^{K},^{V}):=(_{1},...,_{m})W^{O}^{N d},\] (3)

\[_{i}:=(^{Q}W_{i}^{Q},^{K}W_{i}^{K},^ {V}W_{i}^{V}),\ i[m].\] (4)

where \(W^{O}^{md_{v} d},W_{i}^{Q}^{d d_{k}}\), \(W_{i}^{K}^{d d_{k}}\), and \(W_{i}^{V}^{d d_{v}}\) are projection matrices, and \(m\) is the number of heads in the MSA layer. In ViTs, they use \(d_{k}=d_{v}=d/m\).

**Prompt-based continual learning.** Prompt-based approaches have emerged as a promising alternative within rehearsal-free continual learning . In vision tasks, prompt-based methods often leverage a pre-trained ViT as a feature extractor \(f_{}\), with its parameters \(\) typically frozen. These methods enhance the model by introducing _prompts_, small sets of learnable parameters that influence the operations of the MSA layer . Prompts are strategically injected into the query, key, and value matrices to guide the ViT in learning new tasks. We denote the prompt parameters by \(^{L_{p} d}\), where \(L_{p}\) is the sequence length and \(d\) is the embedding dimension. Previous work  outlines two main prompt-based approaches: Prompt Tuning (ProT)  and Prefix Tuning (PreT) . While Prompt Tuning directly concatenates the same prompt parameter \(\) to the query, key, and value, prefix tuning divides \(\) into prefixes \(\{^{K},^{V}\}^{}{2} d}\) and appends it to the key and value vectors:

\[f_{}^{}(,^{Q},^{K},^{V}):= (^{Q},^{K}\\ ^{K},^{V}\\ ^{V})=(}_{1},...,}_{m})W^{O}\] (5)

Existing prompt-based methods in CL address catastrophic forgetting by creating new adaptive prompts for each new task. During testing, the model chooses suitable prompt combinations to handle unseen data from any encountered task . L2P  proposes a shared prompt pool for all tasks, utilizing a query-key mechanism for prompt selection. Instead of using the same prompt pool across tasks, DualPrompt  introduces G-Prompt and E-Prompt to capture task-agnostic and task-specific information, respectively. S-Prompt  focuses on learning task-specific prompts and employs a ProT strategy similar to L2P. CODA-Prompt  expands the prompt pool across tasks and performs a weighted summation of the prompt pool using attention factors. A recent work, HiDe-Prompt , achieves state-of-the-art performance by introducing a new hierarchical decomposition of CIL objectives and optimizing each component for better performance.

In this study, we focus on prefix tuning as our primary prompt-based methodology and follow the framework presented in HiDe-Prompt . During training, HiDe-Prompt co-optimizes task-specific prompts \(_{t}\) and model's output layer parameters \(\) for each new task \(t\) using the WTP objective. These prompts are stored within a prompt pool \(=\{_{1},...,_{T}\}\). At test time, a separate lightweight auxiliary output layer \(_{}:^{D}^{T}\), trained with the TII objective, takes the uninstructed representation \(f_{}(x)\) of a new data point \(\) as input to infer the task identity, guiding the selection of the most suitable prompt \(_{k}\) from the prompt pool \(\). Subsequently, the final prediction is given as \(=h_{}(f_{}(,_{k}))\). For further details, please refer to Appendix D.

**Mixture of experts (MoE)** extends classical mixture models with an adaptive gating mechanism . An MoE model consists of a group of \(N\) expert networks \(f_{i}:^{d}^{d_{v}}\), for all \(i[N]\), and a gate function \(G:^{d}^{N}\). Given an input \(^{d}\), MoE computes a weighted sum of expert outputs \(f_{i}()\) based on learned score function \(s_{i}:^{d}\) for each expert:

\[:=_{j=1}^{N}G()_{j} f_{j}():=_{j=1}^{N} ())}}{_{=1}^{N}())}} f _{j}(),\] (6)where \(G():=(s_{1}(),,s_{N}(h))\). Building on this concept, works by [10; 41] established the MoE layer as a fundamental building block to scale up model capacity efficiently. Please refer to Appendix C for a comprehensive discussion of related works.

## 3 Connection between Prefix Tuning and Mixture of Experts

We first explore the relationship between attention and mixture of experts in Section 3.1, followed by establishing the connection between prefix tuning and the mixture of experts in Section 3.2.

### Mixture of Experts Meets Attention

Following the notation established in Definition 2.2, let's consider the \(l\)-th head within the MSA layer. Let \(=[_{1}^{},,_{N}^{}]^{} ^{Nd}\), which is the concatenation of input sequence embeddings into a single one-dimensional vector. We define the matrix \(E_{i}^{d Nd}\) such that \(E_{i}:=_{i}\) for all \(i[N]\). Furthermore, we introduce an MoE architecture consisting of a group of \(N\) expert networks \(f_{j}:^{Nd}^{d_{v}}\), \(N\) gating functions \(G_{i}:^{Nd}^{N}\) with the score function for the \(j\)-th expert of the \(i\)-th gating \(s_{i,j}:^{Nd}\), where

\[f_{j}():={W_{l}^{V}}^{}E_{j}={W_{l}^{V}}^{}_{j},\;s _{i,j}():=^{}E_{i}^{}W_{l}^{Q}{W_{l}^{K}}^{}E_{ j}}{}}=_{i}^{}W_{l}^{Q}{W_{l}^{K}}^{}_{j}} {}}\]

for \(i\) and \(j[N]\). From equation (4), we can express the output of the \(l\)-th head as follows:

\[_{l}=(^{Q}{W_{l}^{Q}{W_{l} ^{K}}^{}^{K}}^{}}{}})^{V}{W_{l}^{V}}=[ _{l,1},,_{l,N}]^{}^{N d_{v}},\] (7) \[_{l,i}=_{j=1}^{N}_{i}^{ }W_{l}^{Q}{W_{l}^{K}}^{}_{j}}{}})}{_{k=1}^{ N}(_{i}^{}W_{l}^{Q}{W_{k}^{K}}^{}_{k}}{ }})}{W_{l}^{V}}^{}_{j}=_{j=1}^{N}())}{_{k=1}^{N}(s_{i,k}())}f_{j}(),\] (8)

for \(i[N]\). Expanding on equation (8), we can discern that each attention head within the MSA layer implicitly embodies a special mixture of experts architecture. This architecture encompasses \(N\) MoE models, each featuring its own quadratic gating function \(G_{i}\). However, instead of employing

Figure 1: An illustrative depiction of the relationship between self-attention and MoE. Each output vector of a head in the MSA layer can be viewed as the output of a MoE model. These MoE models share the same set of experts encoded in the value matrix. Each entry in the attention matrix corresponds to a score function within this architecture.

separate expert networks for each model, this architecture utilizes \(N\) shared linear expert networks \(f_{j}\) for \(j[N]\), significantly reducing the number of parameters. Notably, each expert network and its corresponding gating function process the entire input sequence directly, rather than individual embedding \(_{i}\) as in traditional MoE layers . This connection between self-attention and mixture of experts is depicted in Figure 1. In the subsequent section, we explore how prompt-based techniques can be viewed through this lens.

### Prefix Tuning via the Perspective of Mixture of Experts

Building on the connection between self-attention and mixture of experts, we propose that applying prefix tuning can be interpreted as the introduction of new experts to customize the pre-trained model for a specific task, as illustrated in Figure 2. Specifically, similar to Section 3.1, we consider the \(l\)-th head within the MSA layer. We denote \(^{K}=[^{K}_{1},,^{K}_{L}]^{}^{L d}\), \(^{V}=[^{V}_{1},,^{V}_{L}]^{}^{L d}\), where \(L=}{2}\). We define new _prefix_ experts \(f_{N+j}:^{Nd}^{d_{u}}\) along with their corresponding new score functions \(s_{i,N+j}:^{Nd}\) as follows:

\[f_{N+j}():={W_{l}^{V}}^{}^{V}_{j}, s_{i,N+j}():= ^{}E_{i}^{}W_{l}^{Q}{W_{l}^{K}}^{}^{K}_{j}}{ {d_{v}}}=_{i}^{}W_{l}^{Q}{W_{l}^{K}}^{}^{K}_{j}}{ }}\] (9)

for \(i[N]\) and \(j[L]\). Then from equation (5), the output of the \(l\)-th head can be expressed as:

\[}_{l}=(^{Q}W_{l}^ {Q},[^{K}\\ ^{K}]W_{l}^{K},[^{V}\\ ^{V}]W_{l}^{V})=[}_{l,1},, }_{l,N}]^{}^{N d_{v}},\] (10) \[}_{l,i}=_{j=1}^{N}())}{ _{k=1}^{N}(s_{i,k}())+_{k^{}=1}^{L}(s_{i,N+k^{ }}())}f_{j}()\] \[+_{j^{}=1}^{L}}())}{ _{k=1}^{N}(s_{i,k}())+_{k^{}=1}^{L}(s_{i,N+k^{ }}())}f_{N+j^{}}()\] (11)

It's worth noting that \(W_{l}^{Q}\), \(W_{l}^{K}\), and \(W_{l}^{V}\) remain fixed, with only \(^{K}\) and \(^{V}\) being learnable. By examining equation (8) and equation (11), we can interpret each head in a multi-head self-attention layer within a pre-trained model as a mixture of experts architecture with pre-trained experts \(f_{j}\) and gating score functions \(s_{i,j}\) for \(i\) and \(j[N]\). Prefix tuning extends this MoE by introducing \(L\) additional prefix experts \(f_{N+j^{}}\) defined by prefix vectors \(^{V}_{j^{}}\) and linear score functions \(s_{i,N+j^{}}\) for \(i[N]\) and \(j^{}[L]\). These new experts collaborate with the pre-trained ones within the MoE model, facilitating the model's adaptation to downstream tasks.

We argue that our introduction of a novel connection between self-attention, prefix tuning, and MoE offers a fresh perspective on the design of previous prompt-based continual learning methods. In the context of continual learning, the pre-trained experts serve as a knowledge base, while prefix tuning augments it with task-specific knowledge encoded in new experts. Moreover, we draw a parallel between the pre-trained experts and the G(eneral)-Prompt utilized in DualPrompt, which captures

Figure 2: Left: An illustrative depiction of prefix tuning as the introduction of new experts into pre-trained MoE models. Right: Visualization of NoRGa implementation, integrating non-linear activation and residual connections into the prefix tuning attention matrix.

task-agnostic information . Both are shared across tasks, making them useful for prediction, especially when task identification is incorrect. Notably, the new experts achieve their efficiency through simple linear gating functions and independence from the input, unlike the pre-trained experts. For simplicity, we call the MoE model (11) as _linear gating prefix MoE_.

**Statistical suboptimality.** The connection between prefix tuning and the MoE within the linear gating prefix MoE model (11) allows us to theoretically explore the statistical behavior of the prefix tuning. In Appendix A, by interpreting the linear gating prefix MoE as a regression problem with sample size \(n\), we demonstrate that the convergence rate for estimating the model parameters, e.g., prompts, can be as slow as \((1/^{}(n))\) where \(>0\) is some constant. This suggests that a huge amount of data is required to achieve reasonable parameter estimation in the linear gating prefix MoE model, which can be discouraging in practice. To address this statistical limitation, the next section introduces a novel non-linear residual gating score function to replace the linear gating function.

## 4 Non-linear Residual Gate Meets Prefix Tuning

As discussed earlier, prefix tuning introduces additional experts within the MoE framework, resulting in the linear gating prefix MoE model. However, as outlined in Appendix A, this approach suffers from suboptimal sample efficiency for parameter estimation. To overcome this and enhance overall CIL performance, we propose an innovative approach that significantly improves sample efficiency while promoting WTP performance in Section 4.1 and provide theoretical explanations in Section 4.2.

### NoRGa: Non-linear Residual Gate

We propose a simple yet effective modification to the linear gating prefix MoE model by incorporating non-linear activation and residual connection within the score functions of prefix experts as follows:

\[_{i,N+j}() :=^{}E_{i}^{}W_{l}^{Q}{W_{l}^{K}}^{}_{j}^{K}}{}}+(^{}E_{i}^{ }W_{l}^{Q}{W_{l}^{K}}^{}_{j}^{K}}{}})\] \[=s_{i,N+j}()+( s_{i,N+j}()), \;i[N],\;j[L],\] (12)

where \(,\) are learnable scalar factors, and \(\) is a non-linear activation function. The new score function in equation (12) consists of a linear and a non-linear component. We call the new prefix MoE model with score functions (12) as _non-linear residual gating prefix MoE_.

The use of a non-linear activation function here is motivated by the algebraic independence condition in Definition 4.2 to theoretically guarantee the optimal sample efficiency of expert and parameter estimations (cf. Theorem 4.3). It's worth noting that removing the linear component \(s_{i,N+j}()\) in the score function (12) could potentially lead to the vanishing gradient problem during training. To mitigate this challenge, we incorporate a residual connection  into the formulation. Our modification introduces minimal additional parameters (\(\) and \(\)) compared to the original score function, ensuring parameter efficiency. This is particularly crucial in continual learning scenarios where the number of parameters grows with each new task. For implementation, we define \(H_{l}=W_{l}^{Q}{W_{l}^{K}}^{}\). From equation (5), the attention matrix of the \(l\)-th head can then be written as:

\[A_{l}=^{Q}H_{l}[^{K}{}^{},\;^{K}{}^{}]}{}}=^{Q}H_{l}^{K}{}^{},\;^{Q}H_{l}^{K}{}^{ }]}{}}=[A_{l}^{},\;A_{l}^{}].\] (13)

Here, \(A_{l}^{}\) denotes the attention score matrix for the prompts, and \(A_{l}^{}\) represents the attention score matrix for the pre-trained experts. To implement NoRGa, we can directly modify the final attention matrix as follows:

\[_{l}=[_{l}^{},A_{l}^{}],\] (14)

\[_{l}^{}=A_{l}^{}+( A _{l}^{}).\] (15)

The implementation of NoRGa is illustrated in Figure 2. Despite its simplicity, our modification can significantly enhance sample efficiency and promote more reasonable parameter estimation, as demonstrated in our theoretical analysis in Section 4.2. Within the HiDe-Prompt framework, task-specific prompt parameters are trained using the WTP objective for each new task. Consequently, our modification leads to better parameter estimation, which directly contributes to improved WTP performance, ultimately improving overall continual learning efficacy. Importantly, NoRGa maintains the same parameter count as HiDe-Prompt, which is crucial in CL because of the memory constraint. Here, we evaluated \(\) with \(\), \(\), and \(\), finding \(\) to perform well in most cases.

[MISSING_PAGE_FAIL:7]

**Definition 4.2** (Algebraic independence).: We say that an expert function \(h(,)\) and an activation function \(()\) are algebraically independent if they are twice differentiable w.r.t their parameters, and if for any \(k 1\) and pair-wise distinct parameters \((_{11},_{1}),,(_{1k},_{k})\), the following set of functions in \(\) is linearly independent for almost every \(^{Nd}\):

\[^{}(1+^{}(_{1j}^{} ))^{||}+_{\{||=2\}}^{}(_{1j}^{})h}{^{}}(,_{j}):j [k_{*}],\\ ^{Nd},^{q}:0||+| | 2}.\]

Intuitively, the algebraic independence condition ensures that there will be no interactions among parameters of the expert function \(h(,)\) and the activation function \(()\). Technically, a key step in our argument is to decompose the regression discrepancy \(g_{_{n}}()-g_{G_{*}}()\) into a combination of linearly independent terms by applying Taylor expansions to the product of the softmax's numerator and the expert function, i.e., \((_{1}^{}+(_{1}^{}))h(,)\). Thus, the above condition guarantees that all the derivative terms in the Taylor expansion are linearly independent. To exemplify the algebraic independence condition, we consider the following simple examples of the expert functions \(h(,)\) and the activation \(()\) that are algebraically independent.

**Example.** When the expert function \(h(,)\) is formulated as a neural network \(h(,(a,b))=(a^{}+b)\) with the activation \(()\{(),(),z z^{p}\}\), where \((a,b)^{Nd}\), and the activation function \(()\) is one among the functions \((),(),()\), then they satisfy the algebraic independence condition in Definition 4.2.

Finally, we establish the rates for estimating parameters and experts in the non-linear residual gating prefix MoE model in Theorem 4.3. Prior to presenting the theorem statement, let us design a loss function among parameters based on a notion of Voronoi cells , which is a commonly employed approach for the convergence analysis of expert estimation in MoE models , yet tailored to the setting of this paper. In particular, the Voronoi loss used for our analysis is defined as

\[_{1}(G,G_{*}):=_{j^{}[L]:|_{j^ {}}|=1}_{i_{j^{}}}(_{0i})\| _{1ij^{}}\|^{2}+\|_{ij^{}}\|^{2}\\ +_{j^{}[L]:|_{j^{}}|=1}_{i _{j^{}}}(_{0i})\|_{1ij^{}} \|+\|_{ij^{}}\|+_{j^{}=1}^{L}_{i _{j^{}}}(_{0i})-(_{0j^{}}^{*}) ,\] (20)

where we denote \(_{1ij^{}}:=_{1i}-_{1j^{}}^{*}\) and \(_{ij^{}}:=_{i}-_{j^{}}^{*}\). Above, \(_{j^{}}_{j^{}}(G)\), for \(j^{}[L]\), is a Voronoi cell associated with the mixing measure \(G\) generated by the true component \(_{j}^{*}:=(_{1j^{}}^{*},_{j^{}}^{*})\), which is defined as follows:

\[_{j^{}}:=\{i\{1,2,,L^{}\}:\|_{i}-_ {j^{}}^{*}\|\|_{i}-_{}^{*}\|,\; j^{ }\},\] (21)

where we denote \(_{i}:=(_{1i},_{i})\) as a component of \(G\). Note that, the cardinality of each Voronoi cell \(_{j^{}}\) indicates the number of components \(_{i}\) of \(G\) approximating the true component \(_{j^{}}^{*}\) of \(G_{*}\). Additionally, since \(_{1}(G,G_{*})=0\) if and only if \(G G_{*}\), it follows that when \(_{1}(G,G_{*})\) becomes sufficiently small, the differences \(_{1ij^{}}\) and \(_{ij^{}}\) are also small. This observation indicates that, although \(_{1}(G,G_{*})\) is a proper metric as it is not symmetric, it is an appropriate loss function for measuring the discrepancy between the least square estimator \(_{n}\) and the true mixing measures \(G_{*}\).

**Theorem 4.3**.: _Assume that the expert function \(h(x,)\) and the activation \(()\) are algebraically independent, then we achieve the following lower bound for any \(G_{L^{}}()\):_

\[\|g_{G}-g_{G_{*}}\|_{L_{2}()}_{1}(G,G_{*}),\]

_which together with Theorem 4.1 indicates that \(_{1}(_{n},G_{*})=}_{P}(n^{-1/2})\)._

Proof of Theorem 4.3 is in Appendix B.2. A few comments on Theorem 4.3 are in order: (i) From the bound \(_{1}(_{n},G_{*})=}_{P}(n^{-1/2})\), we deduce that the estimation rates for the over-specified parameters \(_{1j^{}}^{*},_{1j^{}}^{*}\), where \(j^{}[L]:|_{j^{}}|>1\), are all of order \(_{P}()\). Since the expert \(h(,)\) is twice differentiable over a bounded domain, it is also a Lipschitz function. Thus, denote\(_{n}:=_{i=1}^{L_{n}}(_{0i})_{( ^{*}_{1i},^{*}_{i})}\), we achieve that

\[_{}|h(,^{*}_{i})-h(,^{*}_{j^{}})| \|^{n}_{i}-^{*}_{j^{}}\|_{P }().\] (22)

The above bound indicates that if the experts \(h(,^{*}_{j})\) are fitted by at least two other experts, then their estimation rates are also of order \(_{P}()\); (ii) For exactly-specified parameters \(^{*}_{1j^{}},^{*}_{j^{}}\), where \(j^{}[L]:|_{j^{}}|=1\), the rates for estimating them are faster than those of their over-specified counterparts, standing at order \(_{P}()\). By arguing similarly to equation (22), the experts \(h(,^{*}_{j^{}})\) also enjoy the faster estimation rate of order \(_{P}()\), which is parametric on the sample size \(n\); (iii) It follows from the above rates that we only need a polynomial number of data (roughly \(^{-4}\) where \(\) is the desired approximation error) to estimate the parameters and experts of the non-linear residual gating prefix MoE. By contrast, when using the linear gating, as being demonstrated in Appendix A, it requires an exponential number of data. This highlights the statistical benefits of using the non-linear residual gating MoE model over the linear gating prefix MoE model.

## 5 Experiments

**Datasets.** We evaluate various continual learning methods on widely used CIL benchmarks, including Split CIFAR-100  and Split ImageNet-R , consistent with prior work . We further explore the model's performance on fine-grained classification tasks with Split CUB-200  and large inter-task differences with 5-Datasets . Please refer to Appendix E for more details.

**Evaluation Metrics.** We utilize several established metrics described in . These include: final average accuracy (FA), which represents the average accuracy after the final task; cumulative average accuracy (CA), which refers to the historical average accuracy; and average forgetting measure (FM). We give more emphasis to FA and CA due to their comprehensiveness, as noted in .

**Baselines.** We compare our approach with several representative prompt-based approaches including L2P , DualPrompt , CODA-Prompt , S-Prompt , and HiDe-Prompt . Additionally,

    &  &  &  \\   & & **FA** (\(\)) &  &  &  &  &  \\   & L2P & \(83.06 0.17\) & \(88.27 0.71\) & \(5.61 0.32\) & \(67.53 0.44\) & \(71.98 0.52\) & \(5.84 0.38\) \\  & DualPrompt & \(87.30 0.27\) & \(91.23 0.65\) & \(3.87 0.43\) & \(70.93 0.08\) & \(75.67 0.52\) & \(5.47 0.19\) \\  & S-Prompt & \(87.57 0.42\) & \(91.38 0.69\) & \(3.63 0.41\) & \(69.88 0.51\) & \(74.25 0.55\) & \(4.73 0.47\) \\  & CODA-Prompt & \(86.94 0.63\) & \(91.57 0.75\) & \(4.04 0.18\) & \(70.03 0.47\) & \(74.26 0.24\) & \(5.17 0.22\) \\  & HiDe-Prompt & \(92.61 0.28\) & \(94.03 0.01\) & \(1.50 0.28\) & \(75.06 0.12\) & \(76.60 0.01\) & \(\) \\  & NoRa (Ours) & \( 0.13\) & \( 0.37\) & \( 0.27\) & \( 0.39\) & \( 0.07\) & \(4.59 0.07\) \\   & L2P & \(79.13 1.25\) & \(85.13 0.05\) & \(7.50 1.21\) & \(61.31 0.50\) & \(68.81 0.52\) & \(10.72 0.40\) \\  & DualPrompt & \(78.84 0.47\) & \(86.16 0.02\) & \(8.84 0.67\) & \(58.69 0.61\) & \(66.61 0.67\) & \(11.75 0.92\) \\  & S-Prompt & \(79.14 0.65\) & \(85.85 0.17\) & \(8.23 1.15\) & \(57.96 1.10\) & \(66.42 0.71\) & \(11.27 0.72\) \\  & CODA-Prompt & \(80.83 0.27\) & \(87.02 0.20\) & \(7.50 0.25\) & \(61.22 0.35\) & \(67.66 0.37\) & \(9.66 0.20\) \\  & HiDe-Prompt & \(93.02 0.15\) & \(94.56 0.05\) & \( 0.13\) & \(70.83 0.17\) & \(73.23 0.08\) & \( 0.23\) \\  & NoRa (Ours) & \( 0.15\) & \( 0.31\) & \(1.34 0.14\) & \( 0.26\) & \( 0.42\) & \(6.88 0.49\) \\   & L2P & \(75.51 0.88\) & \(82.53 1.10\) & \(6.80 1.70\) & \(59.43 0.28\) & \(66.683 0.92\) & \(11.13 1.25\) \\  & DualPrompt & \(76.21 1.00\) & \(83.54 1.23\) & \(8.99 1.81\) & \(60.41 0.76\) & \(66.87 0.41\) & \(9.21 0.43\) \\  & S-Prompt & \(76.60 0.61\) & \(82.89 0.89\) & \(8.60 1.36\) & \(59.56 0.60\) & \(66.60 0.13\) & \(8.83 0.81\) \\  & CODA-Prompt & \(79.11 1.02\) & \(86.21 0.49\) & \(7.69 1.57\) & \(66.56 0.68\) & \(73.14 0.57\) & \(7.22 0.38\) \\  & HiDe-Prompt & \(93.48 0.11\) & \(95.02 0.01\) & \(1.63 0.13\) & \(71.33 0.21\) & \(73.62 0.13\) & \(71.11 0.02\) \\  & NoRa (Ours) & \( 0.04\) & \( 0.35\) & \( 0.30\) & \( 0.20\) & \( 0.46\) & \( 0.39\) \\   & L2P & \(72.23 0.35\) & \(79.71 1.26\) & \(8.37 2.30\) & \(57.21 0.6we evaluate against state-of-the-art pre-trained model-based continual learning methods, including ADAM  and RanPAC . We further extend our evaluation by applying HiDe-Prompt with parameter-efficient fine-tuning techniques like LoRA  and Adapters . In line with , we utilize the checkpoints of ViT that use supervised pre-training of Imagenet-21K (denoted as Sup-21K), and some self-supervised pre-training such as iBOT-21K, iBOT-1K , DINO-1K , and MoCo-1K . For implementation details, see Appendix E.

**Main Results.** In Table 1, we evaluate several continual learning methods on Split CIFAR-100 and Split ImageNet-R using diverse pre-trained models. NoRGa achieves state-of-the-art FA and CA across all datasets and models, consistently outperforming HiDe-Prompt. On Sup-21K, NoRGa demonstrates impressive FA results on both CIFAR-100 and ImageNet-R. It also maintains the highest CA, with significant margins of 1.80% and 2.92% on CIFAR-100 and ImageNet-R, respectively, compared to HiDe-Prompt. These results highlight NoRGa's strong ability to retain knowledge and exhibit minimal forgetting, as evidenced by the low FM values on both datasets. NoRGa also surpasses HiDe-Prompt on self-supervised models, with FA improvements up to 1.95% and 3.66%. We further investigate two scenarios: fine-grained classification tasks and large inter-task differences through experiments on Split CUB-200 and 5-Datasets, respectively, as shown in Table 2. NoRGa maintains its lead, achieving FA gaps of 4.34% and 2.46% on Split CUB-200, and the highest FA on 5-Datasets. While gains in some metrics may be modest, NoRGa consistently outperforms HiDe-Prompt in either FA or CA, underscoring its robustness. For example, on Split ImageNet-R with Sup-21K weights, the FA improvement is small (75.06% vs. 75.40%), but the CA gains are substantial (76.60% vs. 79.52%), demonstrating the method's effectiveness and robustness.

**Ablation Study.** To assess the impact of non-linear activation functions on NoRGa's performance, we evaluated the model's behavior with different choices for the activation function \(\), including \(\), \(\), and \(\) in Table 3. The results show that NoRGa achieves state-of-the-art performance on both Split CIFAR-100 and Split CUB-200 datasets with all three activation functions. These findings suggest that NoRGa exhibits robustness to the choice of non-linear activation within a reasonable range. While all functions perform well, the \(\) activation function demonstrates generally strong performance across scenarios. Further results are provided in the Appendix.

## 6 Conclusion

This paper presents an initial exploration of self-attention and prefix-tuning through the lens of mixture of experts. We find that applying prefix tuning can be viewed as introducing new prefix experts to adapt the pre-trained model. However, limitations in sample efficiency exist. We address this by proposing NoRGa, a novel gating mechanism to enhance continual learning performance. Our results demonstrate NoRGa's effectiveness both theoretically and empirically. While the current implementation of the expert network prioritizes simplicity, future research directions could involve investigating more intricate architectures. Furthermore, the choice of activation functions in our work requires fine-tuning, which opens avenues for future research on adaptively learning activation.

    &  &  \\   & Sup-21K & iBOT-21K & Sup-21K & iBOT-21K \\  L2P & 75.46 & 46.60 & 81.84 & 82.25 \\ DualPrompt & 77.56 & 45.93 & 77.91 & 68.03 \\ S-Prompt & 77.13 & 44.22 & 86.06 & 77.20 \\ CODA-Prompt & 74.34 & 47.79 & 64.18 & 51.65 \\ HiDe-Prompt & 86.56 & 78.23 & 93.83 & 94.88 \\ NoRGa (Ours) & **90.90** & **80.69** & **94.16** & **94.92** \\   

Table 2: Final average accuracy (FA) on Split CUB-200 and 5-Datasets.

    &  &  \\   & Sup-21K & iBOT-21K & Sup-21K & iBOT-21K \\  HiDe-Prompt & 92.61 & 93.02 & 86.56 & 78.23 \\ NoRGa \(\) & 94.36 & **94.76** & 90.87 & **80.69** \\ NoRGa sigmoid & **94.48** & 94.69 & **90.90** & 80.18 \\ NoRGa \(\) & 94.05 & 94.63 & 90.74 & 80.54 \\   

Table 3: Ablation study of different activation functions, measured by final average accuracy (FA).