# Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning

Shenzhi Wang\({}^{1}\)1, Qisen Yang\({}^{1}\)1, Jiawei Gao\({}^{1}\), Matthieu Lin\({}^{2}\), Hao Chen\({}^{4}\), Liwei Wu\({}^{1}\)

**Ning Jia\({}^{3}\), Shiji Song\({}^{1}\), Gao Huang\({}^{1}\)2**

\({}^{1}\) Department of Automation, BNRist, Tsinghua University

\({}^{2}\) Department of Computer Science, BNRist, Tsinghua University

\({}^{3}\) Beijing Academy of Artificial Intelligence (BAAI) \({}^{4}\) Independent Researcher

Project Page: https://shenzhi-wang.github.io/NIPS_Fam020

###### Abstract

Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a _policy constraint_ on the _policy improvement_ objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a _universal model_ to train a family of policies with different improvement/constraint intensities, and a _balance model_ to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.

Figure 1: FamO2O trains a policy family from datasets and selects policies state-adaptively using online feedback. Easily integrated, FamO2O statistically enhances existing algorithms’ performance.

Introduction

Offline reinforcement learning (RL) provides a pragmatic methodology for acquiring policies utilizing pre-existing datasets, circumventing the need for direct environment interaction . Nonetheless, the attainable policy performance in offline RL is frequently constrained by the quality of the dataset . The offline-to-online RL paradigm addresses this limitation by refining the offline RL policy through fine-tuning in an online setting .

While online fine-tuning can indeed elevate policy performance, it also potentially exacerbates the issue of distributional shift , where policy behavior diverges from the dataset distribution. Such shifts typically ensue from drastic policy improvements and are further amplified by state distribution changes when transitioning from offline learning to online fine-tuning [11; 25; 9; 26]. Prior works have attempted to counter this by imposing policy constraints on the policy improvement objective to deter excessive exploration of uncharted policy space [25; 54; 35]. However, this conservative approach can inadvertently stifle policy improvement . In essence, offline-to-online RL necessitates an effective balance between _policy improvement_ and _policy constraint_ during policy optimization.

Regrettably, prior offline-to-online RL algorithms tend to adopt a monolithic approach towards this improvement-constraint trade-off, indiscriminately applying it to all data in a mini-batch [10; 66; 53] or the entire dataset [35; 24; 27; 30]. Given the inherent data quality variation across states (see Figure 2), we argue that this one-size-fits-all manner may fail to optimally exploit each sample. In fact, data yielding high trajectory returns should encourage more "conservative" policies, while data leading to poor returns should incite more "radical" policy improvement.

In this paper, we introduce a novel framework, Family Offline-to-Online RL (FamO2O), which can discern a state-adaptive improvement-constraint balance for each state. FamO2O's design is founded on two key insights delineated in Figure 1(a): (i) The collected dataset, abundant in environmental information, could facilitate the training of a diverse policy family ranging from conservative to radical, and (ii) feedback from online learning might assist in selecting an appropriate policy from this family for each state. As depicted in Figure 3, FamO2O incorporates a _universal model_ and a _balance model_. The universal model, conditioned on a _balance coefficient_, determines the degree of policy conservatism or radicalism, while the balance model learns a mapping from states to balance coefficients, aiding the universal model in tailoring its behavior to each specific state.

FamO2O represents, to the best of our knowledge, the first offline-to-online RL approach harnessing the power of state-adaptive improvement-constraint balances. Theoretically, we establish that in policy optimization, _point-wise KL constraints_ afford a superior performance upper bound compared to the _distributional KL constraints_ adopted in prior works [37; 35; 24]. Importantly, these state-adaptive balances become indispensable when addressing point-wise KL constraints, thereby underlining the necessity of incorporating such balances in offline-to-online RL. Experimental results, as summarized in Figure 1(b), reveal that FamO2O is a simple yet effective framework that statistically significantly improves various offline-to-online RL algorithms and achieves state-of-the-art performance.

## 2 Preliminaries

We introduce essential RL and offline-to-online RL concepts and notations here. For descriptive convenience and theoretical analysis, we use the advantaged-weight regression (AWR) algorithm framework [37; 35; 24], but **FamO2O isn't limited to the AWR framework**. We later demonstrate its integration with non-AWR algorithms in Section 5.3 and Appendix D.

Figure 2: A t-SNE **visualization of randomly selected states** from (a) HalfCheetah and (b) Hopper medium-expert datasets in D4RL . The color coding represents the return of the trajectory associated with each state. This visualization underscores **the significant variation in data quality across different states**.

### Reinforcement learning formulation

RL is typically expressed as a Markov decision process (MDP) , denoted as \((,,P,d_{0},R,)\). Here, \(\) and \(\) are the state1 and action space; \(P(_{t+1}|_{t},)\) is the environmental state transition probability; \(d_{0}(_{0})\) represents initial state distribution; \(R(_{t},_{t},_{t+1})\) is the reward function; and \((0,1]\) is the discount factor.

### Offline-to-online reinforcement learning

Offline-to-online RL is a training paradigm including two phases: (i) _offline pre-training_: pre-training a policy based on an offline dataset; (ii) _online fine-tuning_: fine-tuning the pre-trained policy by interacting with the environment. Note that in the offline pre-training phase, the policy cannot interact with the environment, but in the online fine-tuning phase, the policy has access to the offline dataset.

Similar to offline RL algorithms, offline-to-online RL algorithms' training objectives usually consist of two terms, either explicitly [35; 24] or implicitly [27; 28]: (i) _policy improvement_, which aims to optimize the policy according to current value functions; (ii) _policy constraint_, which keeps the policy around the distribution of the offline dataset or current replay buffer. Using the AWR algorithm framework [37; 35; 24], we demonstrate our method (also applicable to non-AWR algorithms) and define notations in Equation (1) for later use.

\[L_{}=_{(,)}_{}(, )-V()}_{})}^{}|)}_{}.\] (1)

\(L_{}\) is a maximization objective, while \(\) represents the collected dataset. Initially, during offline pre-training, \(\) starts with the pre-collected offline dataset \(_{}=\{(_{k},_{k},^{ }_{k},r_{k}) k=1,2,,N\}\). As we move to the online fine-tuning phase, online interaction samples are continuously incorporated into \(\)[35; 24]. The balance coefficient \(\) is a predefined hyperparameter moderating between the policy improvement and policy constraint terms, while the imitation weight sets the imitation intensity for the state-action pair \((s,a)\).

## 3 State-Adaptive Balance Coefficients

Our design of state-adaptive improvement-constraint balances is motivated by the observations that (i) the quality of the dataset's behavior, _i.e._, the trajectory returns, fluctuates greatly with different states, as shown in Figure 2; (ii) state-dependent balances are conducive to a higher performance upper bound. In this section, we will theoretically validate the latter point.

We first present the policy optimization problem with point-wise KL constraints in Definition 3.1, which is the focus of FamO2O:

**Definition 3.1** (Point-wise KL Constrained Optimization Problem).: We consider a policy optimization problem defined as follows:

\[_{} _{ d_{}(), (|)}[Q^{^{k}}(,)-V^{^{k}}( )]\] (2) s.t. \[D_{}((|)|_{}(|))_{},\] (3) \[_{}(|)\, =1,.\] (4)

Here, \(^{k}(k)\) denotes the policy at iteration \(k\), \(_{}\) signifies a behavior policy representing the action selection way in the collected dataset \(\), \(d_{_{}}()\) refers to the state distribution of \(_{}\), and \(_{}\) is a state-related constant. The optimal policy derived from Equations (2) to (4) is designated as \(^{k+1}\).

The optimization problem common in previous work [37; 35; 24] is shown in Definition 3.2:

**Definition 3.2** (Optimization problem with distributional KL constraints).: The definition of the policy optimization problem with distributional KL constraints is the same as Definition 3.1, except thatEquation (3) in Definition 3.1 is substituted by Equation (5), where \(\) is a constant:

\[_{}d_{_{}}()D_{}(( |)\|_{}(|))\,.\] (5)

_Remark 3.3_.: The update rule in Equation (1) is based on the optimization problem in Definition 3.2.

The point-wise constraints' superiority over distributional constraints is shown in Proposition 3.4:

**Proposition 3.4** (Advantage of point-wise KL constraints).: _Denote the optimal value in Definition 3.1 as \(J_{*}^{k}[\{_{},\}]\), the optimal value in Definition 3.2 as \(J_{*}^{k}[]\). These optimal values satisfy:_

\[ 0,\{_{}, \}, J_{*}^{k}[\{_{},\}] J_{*}^{k}[].\] (6)

Proof.: Please refer to Appendix C.1. 

Proposition 3.4 indicates that the optimal value under the point-wise KL constraints, given suitable point-wise constraints, is _no less than_ that under distributional KL constraints. This finding justifies our approach under point-wise constraints.

Proposition 3.5 shows the necessity of state-dependent balance coefficient design in solving the point-wise KL constraint optimization problem:

**Proposition 3.5** (State-dependent balance coefficient).: _Consider the optimization problem in Definition 3.1. Assume that the state space \(=[s_{},s_{}]^{l}\) (\(l\) is the state dimension), and the feasible space constrained by Equations (3) to (4) is not empty for every \(\). Then the optimal solution of \(^{k+1}\), denoted as \(_{*}^{k+1}\), satisfies that \(,\),_

\[_{*}^{k+1}(|)=(|) }{Z_{}}(_{}(Q^{^{k}}(,)-V^{^{k}}())),\] (7)

_where \(_{}\) is a state-dependent balance coefficient, and \(Z_{}\) is a normalization term. When utilizing a parameterized policy \(_{}\) to approximate the optimal policy \(_{*}^{k+1}\), the training objective can be formulated as:_

\[=*{arg\,max}_{}_{(,) }(_{}(Q^{^{k}}(,) -V^{^{k}}()))_{}(|).\] (8)

Proof.: The proof is deferred to Appendix C.2. 

In contrast to AWR  and AWAC , Proposition 3.5 highlights state-dependent (marked in blue) balance coefficients in Equations (7) to (8), as opposed to a pre-defined hyperparameter in Equation (1). This state-adaptiveness is due to Proposition 3.5 considering the finer-grained constraints in Definition 3.1. Together, Proposition 3.4 and Proposition 3.5 indicate state-adaptive balance coefficients contribute to a higher performance upper bound.

## 4 Family Offline-to-Online RL

Section 3 theoretically proves that finer-grained policy constraints enhance performance upper bounds, necessitating state-adaptive balance coefficients. Accordingly, we introduce FamO2O, a framework adaptively assigning a balance coefficient to each state, easily implemented over various offline-to-online algorithms like , hereafter called the "base algorithm".

Essentially, FamO2O trains a policy family with varying balance coefficients during offline pre-training. During online fine-tuning, FamO2O identifies the appropriate policy, corresponding to the suitable balance coefficient, for each state from this policy family. In this section, we present FamO2O using the AWR algorithm framework (Equation (1)). FamO2O's compatibility with non-AWR algorithms is discussed in Appendix D.

Figure 3: FamO2O’s **inference** process. For each state \(\), the balance model \(_{b}\) computes a state-adaptive balance coefficient \(_{}\). Based on \(\) and \(_{}\), the universal model \(_{u}\) outputs an action \(\).

As shown in Figure 3, FamO2O's policy consists of two components: a _universal model_\(_{u}\) and a _balance model_\(_{b}\). Denote the space of balance coefficients as \(\). For every state \(\), the balance model2\(_{b}:\) figures out a suitable balance coefficient \(_{}\); based on the state \(\) and state-related balance coefficient \(_{}\), the universal model \(_{u}:\) outputs an action. The balance coefficient \(_{}\) is to control the conservative/radical degree of the universal model \(_{u}\) in dealing with the state \(\).

### Learning universal model

We initially address training the universal model \(_{u}\), aimed at learning a policy family with varying balances between policy improvement and constraint. The formal optimization target of \(_{u}\) is:

\[_{u}^{k+1}=*{arg\,max}_{_{u}}_{(, )}(_{}(Q^{k}(, )-V^{k}()))_{u}(|,_{ }).\] (9)

\(Q^{k}\) and \(V^{k}\), detailed in Section 4.3, represent \(Q\) and \(V\) functions at iteration \(k\). Equation (9) echoes Equation (8), but the policy also takes a balance coefficient \(_{}\) as input (highlighted in blue). In the offline pre-training phase, \(_{}\) is randomly sampled from balance coefficient space \(\). This encourages \(_{u}\) to learn varied strategies. During online fine-tuning, \(_{}\) is set by balance model \(_{b}\) before input to universal model \(_{u}\), which prompts cooperation between \(_{u}\) and \(_{b}\).

### Learning balance model

Next, we outline how the balance model \(_{b}\) chooses an appropriate policy for each state from the policy family trained by the universal model \(_{u}\). As indicated in Section 4.2, every \(_{}\) corresponds to a unique policy. Consequently, to select the optimal policy, \(_{b}\) needs to determine the appropriate balance coefficient \(_{}\) for each state \(\). Given this rationale, the update rule for \(_{b}\) is:

\[_{b}^{k+1}=*{arg\,max}_{_{b}}_{(, )}Q^{k}(,^{k+1}( ,_{b}())}_{}.\] (10)

Here, \(_{u}^{k+1}\) represents the updated universal model in Equation (9). Intuitively, Equation (10) aims to find a \(_{b}\) that maximizes \(Q^{k}\) value by translating balance coefficients into actions with \(_{u}^{k+1}\). This design is grounded in the understanding that the \(Q\) value serves as an estimate of future return, which is our ultimate goal of striking a balance between policy improvement and constraint. Concerns may arise about \(Q^{k}\)'s extrapolation error in Equation (10) potentially misguiding \(_{b}\)'s update. Empirical evidence suggests this is less of an issue if we avoid extremely radical values in the balance coefficient space \(\). Following the update rule in Equation (10), \(_{b}\) effectively assigns balance coefficients to states, demonstrated in Section 6.1.

### Learning value functions

Furthermore, we explain the value functions update. As per Equations (9) to (10), a single set of \(Q\) and \(V\) functions evaluate both \(_{u}\) and \(_{b}\). This is due to \(_{b}:\) and \(_{u}:\) collectively forming a standard RL policy \(_{u}(,_{b}()):\). Hence, the value functions update mirrors that in the base algorithm, simply replacing the original policy with \(_{u}(,_{b}())\).

Finally, we offer a pseudo-code of FamO2O's training process in Appendix B.

## 5 Experimental Evaluation

In this section, we substantiate the efficacy of FamO2O through empirical validation. We commence by showcasing its state-of-the-art performance on the D4RL benchmark  with IQL  in Section 5.1. We then evidence its performance improvement's generalizability and statistical significance in Section 5.2. Moreover, Section 5.3 reveals FamO2O's compatibility with non-AWR-style algorithms like CQL , yielding significant performance enhancement. Lastly, we reserve detailed FamO2O analyses for Section 6 and Appendix F. For more information on implementation details, please refer to Appendix E.2.

**Datasets** Our method is validated on two D4RL  benchmarks: Locomotion and AntMaze. Locomotion includes diverse environment datasets collected by varying quality policies. We utilize IQL  settings, assessing algorithms on hopper, halfcheetah, and walker2d environment datasets, each with three quality levels. AntMaze tasks involve guiding an ant-like robot in mazes of three sizes (umaze, medium, large), each with two different goal location datasets. The evaluation environments are listed in Table 1's first column.

    &  &  &  \\   & Base & Ours & Base & Ours & Base & Ours \\  hopper-mr-v2 & 56.0 & 86.8 & 91.0 & 97.6 & 73.5 & 92.2 \\ hopper-m-v2 & 54.1 & 75.0 & 65.4 & 90.7 & 59.7 & 82.8 \\ hopper-me-v2 & 97.7 & 92.9 & 76.5 & 87.3 & 87.1 & 90.1 \\ halfcheetah-mr-v2 & 43.9 & 49.0 & 53.7 & 53.1 & 48.8 & 51.0 \\ halfcheetah-m-v2 & 44.8 & 47.6 & 52.5 & 59.2 & 48.7 & 53.4 \\ halfcheetah-me-v2 & 91.0 & 90.6 & 92.8 & 93.1 & 91.9 & 91.8 \\ walker2d-mr-v2 & 72.8 & 84.4 & 90.1 & 92.9 & 81.5 & 88.6 \\ walker2d-m-v2 & 79.0 & 80.0 & 83.8 & 85.5 & 81.4 & 82.8 \\ walker2d-me-v2 & 109.3 & 108.5 & 112.6 & 112.7 & 110.9 & 110.6 \\ 
**Iocomotion total** & 648.4 & 714.9 & 718.3 & 772.0 & 683.4 & 743.4 \\
**95\% CIs** & 640.5\(\)656.8 & 667.3\(\)761.4 & 702.5\(\)733.5 & 753.5\(\)788.5 & 674.6\(\)692.0 & 732.1\(\)754.2 \\  umaze-v0 & 64.0 & 96.9 & 96.5 & 96.7 & 80.4 & 96.8 \\ umaze-diverse-v0 & 60.4 & 90.5 & 37.8 & 70.8 & 66.2 & 80.6 \\ medium-diverse-v0 & 0.2 & 22.2 & 92.8 & 93.0 & 45.2 & 57.6 \\ medium-play-v0 & 0.0 & 34.2 & 91.5 & 93.0 & 45.2 & 63.6 \\ large-diverse-v0 & 0.0 & 0.0 & 57.5 & 64.2 & 24.7 & 32.1 \\ large-play-v0 & 0.0 & 0.0 & 52.5 & 60.7 & 21.4 & 30.3 \\ 
**antmaze total** & 124.7 & 243.7 & 428.7 & 478.3 & 283.1 & 361.1 \\
**95\% CIs** & 116.5\(\)132.6 & 226.2\(\)259.9 & 406.7\(\)452.7 & 456.7\(\)498.7 & 274.1\(\)291.1 & 347.2\(\)374.3 \\ 
**total** & 773.0 & 958.6 & 1146.9 & 1250.3 & 960.0 & 1104.5 \\
**95\% CIs** & 761.5\(\)784.6 & 936.8\(\)979.6 & 1119.5\(\)1175.1 & 1221.9\(\)1277.0 & 911.3\(\)1008.8 & 1063.5\(\)1145.4 \\   

* mr: medium-replay, m: medium, me: medium-expert.
* **FamO2O’s improvement over base algorithms .** For D4RL Locomotion, AntMaze , and overall, FamO2O shows significant and meaningful performance gains, meeting Neyman-Pearson criteria .

Table 1: **Enhanced performance achieved by FamO2O after online fine-tuning. We evaluate the D4RL normalized score  of standard base algorithms (including AWAC  and IQL , denoted as “Base”) in comparison to the base algorithms augmented with FamO2O (referred to as “Ours”). All results are assessed across 6 random seeds. The superior offline-to-online scores are highlighted in blue. **FamO2O consistently delivers statistically significant performance enhancements across different algorithms and task sets.**

Figure 4: **FamO2O’s improvement over base algorithms . For D4RL Locomotion, AntMaze , and overall, FamO2O shows significant and meaningful performance gains, meeting Neyman-Pearson criteria .**

Figure 5: **Comparisons between our FamO2O against various competitors on D4RL normalized scores . All methods are tested on D4RL Locomotion and AntMaze for 6 random seeds. FamO2O achieves state-of-the-art performance by a statistically significant margin among all the competitors in offline-to-online RL (_i.e._ IQL , Balanced Replay (BR) , CQL , AWAC , and TD3+BC ), online RL (_i.e._ SAC ), and behavior cloning (BC).**MetricsConsidering RL's inherent variability, we adopt robust evaluation methods per dialog . Besides conventional Medium and Mean, we integrate IQM and Optimality Gap metrics for broader assessment. We also employ lriable's probability of improvement metric for gauging the likelihood of our method outperforming others. We confirm our performance enhancement's statistical significance using 95% Confidence Intervals (CIs).

### Benchmark Comparison

FamO2O's state-of-the-art performance is demonstrated by implementing it over IQL  and comparing with baseline methods.

BaselinesWe benchmark FamO2O against: (i) **offline-to-online RL**, including IQL , Balanced Replay (BR) , CQL , AWAC , and TD3+BC . For IQL, BR, and AWAC, official implementations are used. In the case of CQL and TD3+BC, we implement online fine-tuning based on the author-provided offline pre-training codes, following the procedures in IQL and AWAC; (ii) **online RL method**, SAC , to highlight offline pre-training's efficacy; (iii) **behavior cloning (BC)**, which is implemented by maximizing the log-likelihood of the samples in the offline dataset. For SAC and BC, we utilize the implementations of CQL. Further details are in Appendix E.1.

ComparisonAs shown in Figure 5, FamO2O outperforms competitors across all metrics (IQM, Medium, Optimality Gap, and Mean). Specifically, for IQM, Optimality Gap, and Mean, FamO2O's 95% CIs don't overlap with the competitors'. Even for Medium, all baseline expectations fall below the lower limit of FamO2O's 95% CIs. The results underscore the significant edge of our state-adaptive policy constraint mechanism over competing methods.

### Analyzing FamO2O's Performance Enhancement

Though FamO2O demonstrated superior performance on the D4RL benchmark in Section 5.1, it's vital to discern the actual contributions of FamO2O from its base policy, IQL. Therefore, we address two key questions: (i) Does FamO2O consistently enhance other offline-to-online RL algorithms? (ii) Is the performance boost by FamO2O statistically significant given RL's inherent variability?

SetupWe apply FamO2O to AWAC and IQL. AWAC  is one of the most famous offline-to-online algorithms, and IQL  is a recently proposed method that achieves great performance on D4RL . We use the authors' codes, maintaining the same hyperparameters for a fair comparison. Further details are in Appendix E.2.

ComparisonTable 1 shows AWAC's and IQL's performances w/ and w/o FamO2O. FamO2O generally enhances performance by a statistically significant margin across most datasets, regardless of the base algorithm, highlighting its versatility. Even on complex datasets where AWAC barely succeeds, _e.g._, AntMaze medium-diverse and medium-play, FamO2O still achieves commendable performance. Pursuing riable's recommendation , we evaluated FamO2O's statistical significance by calculating average probabilities of improvement against base policies (Figure 4). In all three cases (Locomotion,AntMaze, and Overall), the lower CI bounds exceed 50%, denoting the statistical significance of FamO2O's improvement. Specifically, the upper CI on Locomotion surpasses 75%, demonstrating statistical meaning as per the Neyman-Pearson criterion.

### Versatility of FamO2O with Non-AWR Algorithms

To demonstrate FamO2O's versatility beyond AWR-based algorithms, we extended it to CQL  in addition to AWAC  and IQL . The implementation specifics are in Appendix D. As Figure 6 reveals, FamO2O significantly outperforms CQL. Even when compared to Balance Replay (BR) , an offline-to-online method designed specifically for CQL, FamO2O still shows statistically significant superior performance. These results highlight FamO2O's adaptability to non-AWR algorithms.

## 6 Discussion

In this section, we further provide some in-depth studies on FamO2O, including visualization (Section 6.1) and quantitative analyses (Sections 6.2 to 6.6). More analyses are deferred to Appendix F.

### Does FamO2O really have state-wise adaptivity?

Here, we design a simple maze environment to visualize the state-wise adaptivity of FamO2O. As shown in Figure 8(a), the agent starts at a random cell in the top row and is encouraged to reach the goal at the bottom right corner through two crossing points. During data collection, guidance is provided when the agent passes through the lower crossing point, but no guidance for the upper crossing point. To elaborate, the guidance refers to compelling the agent to adhere to the route and direction that yields the shortest path to the goal. Without it, the agent moves randomly. It can be observed in Figure 8(b) that the agent generally outputs lower balance coefficients for the states with high-quality samples (_i.e._, those derived from the agent's movement with guidance) while generating higher balance coefficients for the states with low-quality data (_i.e._, data gathered when the agent moves without guidance). This result shows FamO2O's state-wise adaptivity in choosing proper balance coefficients according to the data quality related to the current state.

### What is the effect of state-adaptive balances?

In this section, we explore the impact of state-adaptive improvement-constraint balance. In order to encompass data of varying quality levels, we assess FamO2O using the medium-replay datasets of D4RL Locomotion . Our analysis focuses on two metrics: _imitation weights_ and _action distances_.

_Imitation weights_ are defined in Equation (1), with larger (or smaller) values prompting the agent to align more closely (or less) with the replay buffer \(\)'s behavior. _Action distance_, delineated in Equation (11), quantifies the discrepancy in action selection between a policy \(\) and a trajectory \(\):

\[d_{}^{,}=_{(,)} \|^{}}{}\ (^{}|)-\|_{2}^{2}.\] (11)

Here, a lower (or higher) action distance \(d_{}^{,}\) signifies a greater (or lesser) behavioral similarity between the policy \(\) and the trajectory \(\).

We evaluate IQL with and without FamO2O ('Base' and 'FamO2O') regarding imitation weights and action distances, depicted in Figure 7. Figure 7(a) computes the average imitation weight difference (AIWD) per trajectory in the offline dataset. AIWD indicates the mean imitation weight difference between FamO2O and the base algorithm for each \((s,a)\) pair within a trajectory. Figure 7(b) likewise determines an average action distance difference per offline dataset trajectory.

Figure 8: **State-wise adaptivity visualization** in a simple maze environment. (a) Higher data quality at the crossing point in the 5th row compared to the 2nd row. (b) Colors denote different balance coefficient values at traversed cells during inference. FamO2O typically displays conservative (or radical) behavior at cells with high-quality (or low-quality) data.

Figure 7(a) reveals that FamO2O typically shows higher imitation weights than the base algorithm for high-return trajectories. Figure 7(b) indicates that FamO2O aligns more with high-quality trajectories and less with low-quality ones than the base algorithm. These results highlight the state-adaptive balance's role in promoting emulation of high-quality behavior and avoidance of low-quality behavior.

### State-adaptive balances _vs._ fixed balances?

To prove that our adaptive balance coefficients are better than traditional fixed balance coefficients, we compare FamO2O against the base algorithms with different balance coefficients as hyperparameters. As shown in Figure 9, on both AWAC  and IQL , our adaptive balance coefficients outperform all the fixed balance coefficients. Significantly, the 95% CIs of adaptive and fixed balance coefficients have no overlap. These comparison results indicate that our adaptive balance coefficient approach surpasses the fixed balance coefficient method by a statistically significant margin.

### Does FamO2O's efficacy rely on varied data qualities?

It's worth noting that our method's efficacy doesn't rely on varied data qualities. Table 1 clearly demonstrates that FamO2O surpasses the base algorithms in performance across all the medium datasets, namely hopper-medium-v2, halfcheetah-medium-v2, and walker2d-medium-v2, which all maintain a relatively consistent data quality. We claim that FamO2O can determine the suitable conservative/radical balance for each state in online scenarios based on the data quality in the collected dataset. If the dataset is diverse in quality, the balances will be diverse; if the quality is consistent, the balances will be correspondingly consistent. The above claim is supported by Figure 11, which indicates that (i) in datasets with more (or less) diverse data qualities, i.e., medium-expert (or medium), the balance coefficients are more (or less) diverse, with corresponding larger (or smaller) standard deviations; (ii) with higher (or lower) quality datasets, the balance coefficients are averagely lower (or higher), leading to a more conservative (or radical) policy.

### Does FamO2O mitigate the performance drop stemming from the distributional shift?

Despite FamO2O's primary objective not being direct distributional shift handling, its state-adaptive improvement-constraint balances prove beneficial in mitigating performance degradation during the offline pre-training to online fine-tuning transition, attributed to the distributional shift. Existing offline-to-online RL algorithms [24; 35] already incorporate mechanisms to counter distributional shift, hence significant performance drops are infrequent. Figure 10 illustrates the training curves of IQL and IQL+FamO2O across both offline pre-training (negative steps) and online fine-tuning (positive steps) on antimaze-umaze-diverse, on which IQL exhibits the most significant decline in performance when transitioning from offline pre-training to online fine-tuning. As evidenced, while IQL+FamO2O initially experiences a performance drop akin to IQL during online fine-tuning, it recovers rapidly and attains high performance, in stark contrast to IQL's sustained performance decline throughout the fine-tuning stage.

### Balance model _vs._ random selector?

To validate the effect of the balance model \(_{b}\) in choosing balance coefficients, we present a FamO2O variant, denoted as _random-FamO2O_, with the balance model replaced with a random balance coefficient selector. Other training settings keep the same for FamO2O and random-FamO2O. Table 2 shows the improvement percentages and probability of improvement of FamO2O against random-FamO2O. As we can observe, FamO2O outperforms random-FamO2O on almost all the datasets of D4RL Locomotion . Furthermore, the lower CI of the probability of improvement is much higher than 50%, and the upper CI exceeds 75%. This indicates that the effect of the balance model is not only statistically significant but also statistically meaningful as per the Neyman-Person statistical testing criterion.

## 7 Conclusion

This work underscores the significance of state-adaptive improvement-constraint balances in offline-to-online RL. We establish a theoretical framework demonstrating the advantages of these state-adaptive balance coefficients for enhancing policy performance. Leveraging this analysis, we put forth Family Offline-to-Online RL (FamO2O), a versatile framework that equips existing offline-to-online RL algorithms with the ability to discern appropriate balance coefficients for each state.

Our experimental results, garnered from a variety of offline-to-online RL algorithms, offer substantial evidence of FamO2O's ability to significantly improve performance, attaining leading scores on the D4RL benchmark. In addition, we shed light on FamO2O's adaptive computation of state-adaptive improvement-constraint balances and their consequential effects through comprehensive analyses. Ablation studies on the adaptive balances and balance model further corroborate FamO2O's efficacy.

The limitation of our work is that FamO2O has been evaluated on just a handful of representative offline-to-online RL algorithms, leaving a vast array of such algorithms unexamined. Additionally, FamO2O's utility is somewhat limited, as it is applicable exclusively to offline-to-online algorithms. In future work, we aim to expand the applicability of FamO2O by integrating it with a broader spectrum of offline-to-online algorithms. Additionally, we intend to investigate the feasibility of incorporating state-adaptive improvement-constraint balances in offline RL settings, where online feedback is intrinsically absent, or even applying our adaptive design to the LLM-as-agent domain [49; 50].