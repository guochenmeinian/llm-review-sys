# Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow

Sizhe Wei\({}^{1}\)\({}^{}\) Yuxi Wei\({}^{1}\) Yue Hu\({}^{1}\) Yifan Lu\({}^{1}\)

**Yiqi Zhong\({}^{2}\) Siheng Chen\({}^{1,3}\)\({}^{*}\) Ya Zhang\({}^{1,3}\)\({}^{*}\)**

\({}^{1}\) Cooperative Medianet Innovation Center, Shanghai Jiao Tong University

\({}^{2}\) University of Southern California \({}^{3}\) Shanghai AI Laboratory

\({}^{1}\) {sizhewei, wyx3590236732, 18671129361, yifan_lu}@sjtu.edu.cn

{sihengc, ya_zhang}@sjtu.edu.cn

\({}^{2}\) yiqizhon@usc.edu

Equal contribution. \({}^{*}\)Corresponding author.

###### Abstract

Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings. The code is available at https://github.com/MediaBrain-SJTU/CoBEVFlow.

## 1 Introduction

Multi-agent collaborative perception allows agents to exchange complementary perceptual information through communication. This can overcome inherent limitations of single-agent perception, such as occlusion and long-range issues. Recent studies have shown that collaborative perception can substantially boost the performance of perception systems  and has great potential for a wide range of real-world applications, such as multi-robot automation system , vehicle-to-everything-communication-aided autonomous driving and multi-UAVs (unmanned aerial vehicles) . As an emerging area, the study of collaborative perception has many challenges to be tackled, such as high-quality datasets , model-agnostic and task-agnostic formulation  and the robustness to pose errors  and adversarial attacks .

However, a vast majority of existing works do not seriously account for the harsh realities of real-world communication among agents, such as congestion, heavy computation, interruptions, and the lack of calibration. These factors introduce delays or misalignments that severely impact the reliability and quality of information exchange among agents. Some prior works have touched upon the issue of communication latency. For instance, V2VNet  and V2X-ViT  incorporated the delay time as an input for feature compensation. However, they only account for a single frame without leveraging historical frames, making them inadequate for high-speed scenarios (above 20m/s) or high-latency scenarios (above 0.3s) scenarios. Meanwhile, SyncNet  uses historical features to predict the complete feature map at the current timestamp . Nevertheless, this RNN-based method assumes equal time intervals for its input, causing failures when delays are irregular. Overall, previous works have not addressed the issues raised by common irregular time delays, rendering existing collaborative perception systems can never reach their full potential in real-world scenarios.

To fill the research gap, we specifically formulate a setting of _asynchronous_ collaborative perception; see Fig. 1 for a visual demonstration. Here asynchrony indicates that the time stamps of the collaboration messages from other agents are not aligned and the time interval of two consecutive messages from the same agent is irregular. Due to the universality and inevitability of temporal asynchrony in practical applications, handling this setting is critical to the further development of collaborative perception. To address this, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key idea is to align perceptual information from other agents by compensating relative motions. Specifically, CoBEVFlow uses historical frames to estimate a BEV flow map, which encodes the motion information in each grid cell. Using the BEV flow map, CoBEVFlow can reassign asynchronous perceptual features to appropriate spatial locations, which aligns perceptual features on the time dimension, mitigating the impact caused by asynchrony. The proposed CoBEVFlow has two major advantages: i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous timestamps without discretization; ii) CoBEVFlow better adheres to the essence of compensation, which is to move features to the designated spatiotemporal positions. Not like SyncNet that regenerates features, this motion-guided position adjustment fundamentally prevents introducing extra noises to the features.

When validating the effectiveness of CoBEVFlow, we noticed that there is no appropriate collaborative perception dataset that contains asynchronous samples. To facilitate research on asynchronous collaborative perception, we create IRregular V2V(IRV2V), the first synthetic asynchronous collaborative perception dataset with irregular time delays, simulating various real-world scenarios. Sec. 5 show the experiment results and analysis on both IRV2V and a real-world dataset DARI-V2X. Results show that CoBEVFlow consistently achieves the best compensation performance across various latencies. When the expected latency on the IRV2V dataset is set to 500ms, CoBEVFlow outperforms other methods by more than 18.9%. In the case of a 300ms latency with an additional 200ms disturbance, the decrease in AP@0.50 is only 0.25%.

## 2 Related work

### Collaborative Perception

Factors such as limited sensor fields of view and physical environmental occlusions can negatively impact perception tasks for individual agents[22; 23]. To address the aforementioned challenges, collaborative perception based on multi-agent systems has emerged as a promising solution [2; 3; 5; 1; 24; 25]. It can enhance the performance of perception tasks by leveraging the exchange of information among different agents within the same scenario. V2VNet uses multi-round message passing via graph neural networks to achieve better perception and prediction performance; DiscoNet adopts knowledge distillation to take advantage of both early and intermediate collaboration; V2X-ViT proposes a heterogeneous multi-agent attention module to aggregate information from heterogeneous

Figure 1: Illustration of asynchronous collaborative perception and the perception result w.o./w. CoBEVFlow. Red boxes are detection results and green boxes are the ground-truth.

agents; Where2comm introduces a spatial confidence map which achieves pragmatic compression and improves perception performance with limited communication bandwidth.

As unideal communication is an inevitable issue that negatively impacts the performance and application of collaborative perception, some methods have been studied for robust collaborative perception. V2VNet utilizes a convolutional neural network to learn how to compensate for communication delay by taking time information and relative pose as input; V2X-ViT designs a Delay-aware Positional Encoding module to learn the influence caused by latency, but these methods do not consider the historical temporal information for compensation. SyncNet uses historical multi-frame information and compensates for the current time by Conv-LSTM, but its compensation for the whole feature map leads noises to the feature channels, and RNN-based framework can not handle temporal irregular inputs. This work formulates asynchronous collaborative perception, which considers real-world communication asynchrony.

### Time-Series Forecasting

Time series analysis aims to extract temporal information of variables, giving rise to numerous downstream tasks. Time series forecasting leverages historical sequences to predict observations of variables in future time periods. Classical methods for time series forecasting include the Fourier transform, autoregressive models, and Kalman filters. In the era of deep learning, a plethora of RNN-based and attention-based methods have emerged to model sequence input and address time series prediction problems[30; 31; 32]. The issue of sampling irregularity, which may arise due to physical interference or device issues, calls for irregularity-robust time series analysis systems. Approaches such as mTAND, IP-Net, and DGM\({}^{2}\) employ imputation-based techniques to tackle irregular sampling, which estimate the observation or hidden embedding of series at regular timestamps and then apply the methods designed for regular time series analysis, while SeFT and Raindrop utilize operations that are insensitive to sampling intervals for processing irregularly sampled data. In this work, we consider each collaboration message as one irregular sample and use irregular time series forecasting to estimate the BEV flow map for asynchronous feature alignment.

## 3 Problem Formulation

Consider \(N\) agents in a scene, where each agent can send and receive collaboration messages from other agents, and store \(k\) historical frames of messages. For the \(n\)th agent, let \(_{n}^{t_{n}^{i}}\) and \(_{m}^{t_{n}^{i}}\) be the raw observation and the perception ground-truth at time current \(t_{n}^{i}\), respectively, where \(t_{n}^{i}\) is the \(i\)-th timestamp of agent \(n\), and \(_{m n}^{t_{n}^{i}}\) be the collaboration message sent from agent \(m\) at time \(t_{m}^{j}\). The key of the asynchronous setting is that the timestamp of each collaboration message \(t_{n}^{i}\) is a continuous value, those messages from other agents are not aligned, \(t_{m}^{i} t_{n}^{i}\), and the time interval between two consecutive timestamps \(t_{n}^{i-1}-t_{n}^{i}\) is irregular. Therefore, each agent has to encounter collaboration messages from other agents sent at arbitrary times. Then, the task of asynchronous collaborative perception is formulated as:

\[_{,} _{n=1}^{N}g(}_{n}^{t_{n}^{i}}, _{n}^{t_{n}^{i}})\] (1) \[\ }_{n}^{t_{n}^{i}}= _{}(_{n}^{t_{n}^{i}},\{_{m n }^{t_{n}^{i}},_{m n}^{t_{n-1}^{i-1}},,_{m n }^{t_{j-k+1}^{j}}\}_{m=1}^{N}),\]

where \(g(,)\) is the perception evaluation metric, \(}_{n}^{t_{n}^{i}}\) is the perception result of agent \(n\) at time \(t_{n}^{i}\), \(_{}()\) is the collaborative perception network with trainable parameters \(\), and \(t_{m}^{j-k+1}<t_{m}^{j-k+2}<<t_{m}^{j} t_{n}^{i}\). Note that: i) when the collaboration messages from other agents are all aligned and the time interval between two consecutive timestamps is regular; that is, \(t_{m}^{i}=t_{n}^{i}\) for all agent's pairs \(m,n\), and \(t_{n}^{i}-t_{n}^{i-1}\) is a constant for all agents \(n\), the task degenerates to standard well-synchronized collaborative perception; and ii) when the collaboration messages from other the agents are not aligned, yet the time interval between two consecutive timestamps is regular; that is, \(t_{m}^{i} t_{n}^{i}\) and \(t_{n}^{i}-t_{n}^{i-1}\) is a constant, the task degenerates to the setting in SyncNet .

Given such irregular asynchrony, the performances of collaborative perception systems would be significantly degraded since features from asynchronized timestamps differ from the actual current features, and using asynchronized features may contain erroneous information during the perception process. In the next section, we will introduce CoBEVFlow to address this critical issue.

CoBEVFlow: Asynchrony-Robust Collaborative Perception System

This section proposes an asynchrony-robust collaborative perception system, CoBEVFlow. Figure 2 overviews the overall scheme of CoBEVFlow. We introduce the overall framework of the CoBEVFlow system in Sec. 4.1. The details of three key modules of CoBEVFlow can be found in Sec. 4.2-4.4. Sec. 4.5 demonstrates the training details and loss functions of the whole system.

### Overall architecture

The problem of asynchrony results in the misplacements of moving objects in the collaboration messages. That is, the collaboration messages from multiple agents would record various positions for the same moving object. The proposed CoBEVFlow addresses this issue with two key ideas: i) we use a BEV flow map to capture the motion in a scene, enabling motion-guided reassigning asynchronous perceptual features to appropriate positions; and ii) we generate the region of interest(ROI) to make sure that the reassignment only happens to the areas that potentially contain objects. By following these two ideas, we eliminate direct modification of the features and keep the background feature unaltered, effectively avoiding unnecessary noise in the learned features.

Mathematically, let the \(n\)-th agent be the ego agent and \(_{n}^{t_{n}^{i}}\) be its raw observation at the \(i\)-th timestamp of agent \(n\), denoted as \(t_{n}^{i}\). The proposed asynchrony-robust collaborative perception system CoBEVFlow is formulated as follows:

\[_{n}^{t_{n}^{i}} =f_{}(_{n}^{t_{n}^{i}}),\] (2a) \[}_{n}^{t_{n}^{i}},_{n}^{t_{i}} =f_{}(_{n}^{t_{i}^{i}}),\] (2b) \[_{m}^{t_{m}^{j} t_{n}^{i}} =f_{}(t_{n}^{i},\{_{m}^{t_{n}^{j}}\}_ {q=j-k+1,j-k+2,,j}),\] (2c) \[}_{m}^{t_{n}^{i}} =f_{}(}_{m}^{t_{n}^{j}},_{m}^{t_{n}^{j} t_{n}^{i}}),\] (2d) \[}_{n}^{t_{n}^{i}} =f_{}(}_{n}^{t_{n}^{i}},\{}_{m}^{t_{n}^{i}}\}_{m_{n}}),\] (2e) \[}_{n}^{t_{n}^{i}} =f_{}(}_{n}^{t_{n}^{i}}),\] (2f)

where \(_{n}^{t_{n}^{i}}^{H W D}\) is the BEV perceptual feature map of agent \(n\) at timestamp \(t_{n}^{i}\) with \(H,W\) the size of BEV map and \(D\) the number of channels; \(_{n}^{t_{n}^{i}}\) is the set of region of interest (ROI); \(}_{n}^{t_{n}^{i}}^{H W D}\) is the sparse version of \(_{n}^{t_{n}^{i}}\), which only contains features inside \(_{n}^{t_{n}^{i}}\) and zero-padding outside; \(_{m}^{t_{m}^{j} t_{n}^{i}}^{H W 2}\) is the \(m\)-th agent's the BEV flow map that reflects each grid cell's movement from timestamp \(t_{m}^{j}\) to timestamp \(t_{n}^{i}\), \(\{_{m}^{t_{n}^{j}}\}_{q=j-k+1,j-k+2,,j}\) indicates historical \(k\) ROI sets sent by agent \(m\); \(}_{m}^{t_{n}^{i}}^{H W D}\) is the realigned feature map from the \(m\)-th agent's at timestamp \(t_{n}^{i}\) after motion compensation; \(}_{n}^{t_{n}^{i}}^{H W D}\) is the aggregated features from all of the agents; \(_{n}\) is the collaboration neighbors of the \(n\)-th agent; and \(}_{n}^{t_{n}^{i}}\) is the final output of the system.

Step 2a extracts BEV perceptual feature from observation data. Step 2b generates the ROIs for each feature map, enabling BEV flow generation in Step 2c. Now all the agents exchange their messages,

Figure 2: System overview. Message packing process prepares ROI and sparse features as the message for efficient communication and BEV flow map generation. Message fusion process generates and applies BEV flow map for compensation, and fuses the features at the current timestamp from all agents.

including \(}_{n}^{t_{i}^{i}}\) and the \(_{n}^{t_{i}^{i}}\). Step 2c generate the BEV flow map \(_{m}^{t_{i}^{j} t_{i}^{i}}\) by leveraging historical ROIs from the same agent. Step 2d gets the estimated feature map by applying the BEV flow map to reassign the asynchronized features. Step 2e aggregates the feature maps of all agents. Finally, Step 2f outputs the final perceptual results.

Note that i) Steps 2a-2b are done before communication. Steps 2c-2f are performed after receiving the message from others. During the communication process, both sparse perceptual features and the ROI set are sent to other agents, which is communication bandwidth friendly; and ii) CoBEVFlow adopts the feature representations in bird's eye view (BEV), where the feature maps of all agents are projected to the same global coordinate system, avoiding complex coordinate transformations and supporting easier cross-agent collaboration.

The proposed asynchronous collaborative perception system has three advantages: i) CoBEVFlow can deal with asynchronous collaboration messages sent at irregular, continuous timestamps without discretization; (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, causing additional noises; and (iii) CoBEVFlow promotes robustness to asynchrony by introducing minor communication cost (ROI set).

We now elaborate on the details of Steps 2b-2e in the following subsections.

### ROI generation

Given the perceptual feature map of an agent, Step 2b aims to generate a set of spatial regions of interest (ROIs) for the areas that possibly contain objects. Each ROI indicates one potential object's region in the scene. The intuition is that foreground objects are the only ones that move, while the background remains static. Therefore, using ROI can enable the subsequent BEV flow map to concentrate on critical regions and simplify the computation of the BEV flow map.

To implement, we use the structure of the object detection decoder to produce the ROIs. Given agent \(m\)'s perceptual feature map at timestamp \(t_{m}^{j}\), \(_{m}^{t_{m}^{j}}\), the corresponding detection result is obtained as:

\[_{m}^{t_{m}^{j}}=_{}(_{m}^{t_{m}^{j}} )^{H W 7},\] (3)

where \(_{}()\) is the ROI generation network with detection decoder structure, and each element \((_{m}^{t_{m}^{j}})_{h,w}=(c,x,y,h,w,,)\) represents one detected ROI with its class confidence, position, size, and orientation. We threshold the class confidence, apply non-max suppression, and obtain a set of detected boxes, whose occupied spaces form the set of ROIs, \(_{m}^{t_{m}^{j}}\).

Based on this ROI set, we can also get a binary mask \(^{H W}\), whose values inside ROIs are 1 and the others are 0. We then get the sparse feature map \(}_{m}^{t_{m}^{j}}=_{m}^{t_{m}^{j}}\), which only contains the features within ROIs. Then, agent \(m\) packs its sparse feature map \(}_{m}^{t_{m}^{j}}\) and the ROI set \(_{m}^{t_{m}^{j}}\) as the message and sends out for collaboration.

### BEV flow map generation

After receiving collaboration messages from other agents at various timestamps, Step 2c aims to generate the BEV flow map to correct feature misalignment due to asynchrony. The proposed BEV flow map encodes the motion vector of each spatial location. The main idea of obtaining this BEV flow is to associate correlated ROIs based on a sequence of messages sent by the same collaborator. In this step, each ROI is regarded as an instance that has its own attributes generated by Step 2b. After the ROI association, we are able to compute the motion vector and further estimate the positions where the corresponding object would appear at a certain timestamp. The generation of the BEV flow map includes two key steps: adjacent timestamp' ROI matching and BEV flow estimation.

**Adjacent frames' ROI matching.** The purpose of adjacent frames' ROI matching is to match the ROIs in two consecutive messages sent by the same agent. The matched ROIs are essentially the same instance perceived at different timestamps. This module contains three processes: cost matrix construction, greedy matching, and post-processing. We first construct a cost matrix \(^{o_{1} o_{2}}\), where \(o_{1}\) and \(o_{2}\) are the numbers of ROIs in the two frames to be matched. Each element \(_{p,q}\) is the matching cost between ROI \(p\) in the earlier frame and ROI \(q\) in the later frame. To determine the value of \(_{p,q}\), we define the vicinity of the front and rear directions as a feasible angle range for matching. We set \(_{p,q}=d_{p,q}\) when \(q\) is within the feasible angle range of \(p\), otherwise \(_{p,q}=+\), where \(d_{p,q}\) is the Euclidean distance between the center of ROI \(p\) and \(q\). We then use the greedy matching strategy to search the paired ROIs. For each row \(p\), we search the \(q\) with minimum \(_{p,q}\), and match \(p\), \(q\) as a pair. To avoid invalid matching, we further post-process matched pairs by removing those with excessively large values of \(C_{p,q}\). Through these processes, we can get the matched ROI pairs in adjacent frames. For a sequence of frames, we can track each ROI's multiple locations across frames.

Based on \(_{m,r}\), we now predict \(_{r}^{t_{n}^{i}}\), which is the location and orientation of the \(r\)-th ROI at the ego agent's current timestamp. Unlike the common motion estimation, we need to handle an irregularly sampled sequence. To enable the irregularity-compatible motion estimation method, the information of the timestamp should be taken into account. Here we propose to use traditional trigonometric functions  for timestamp encoding, by which we map the continuous-valued timestamp \(t\) into its corresponding time code \((t)\) through:

\[((t))_{2e}=(}),((t))_{2e+1}= (}),\] (4)

where \(e\) is the index of temporal encoding. The timestamp information now can be input into the estimation process along with the irregularly sampled sequence and make the estimation process capable of irregularity-compatible motion estimation. We implement the estimation process using multi-head attention(MHA). The query of MHA is the time code of the target timestamp \(t_{n}^{i}\), and the key and value both are the sum of the features of the irregularly sampled sequence and its corresponding time code set \(_{k}\):

\[}_{r}^{t_{n}^{i}}=((t_{n}^{i}), (_{m,r})+_{k},(_{m,r})+ _{k}),\] (5)

where \(_{r}^{t_{n}^{i}}\) is the estimation of ROI's location and orientation \(v_{r}^{t_{n}^{i}}\), \(()\) is the encoding function of the irregular historical series \(_{m}^{r}\), and \(()\) is the multi-head attention for temporal estimation.

With the estimated location and orientation of ROIs in the ego agent's current timestamp along with the ROIs' sizes predicted by Step 2b, we calculate the motion vector at each grid cell by an affine transformation of the associated ROI's motion, constituting the whole BEV flow map \(_{m}^{t_{m}^{j} t_{n}^{i}}^{H W 2}\). Note that the grid cells outside ROIs are zero-padded.

Compared to Syncnet  that uses RNNs to handle regular communication latency, the generated BEV flow map has two benefits: i) it handles irregular asynchrony via the attention-based estimation with appropriate time encoding; and ii) it facilitates the motion-guided feature warping, which avoids regenerating the entire feature channels.

### Feature warp and aggregation

The BEV flow map \(_{m}^{t_{m}^{j} t_{n}^{i}}\) is applied on the sparse feature map \(}_{m}^{t_{m}^{j}}\), which implements Step 2d. The features at each grid cell are moved to the estimated position based on \(_{m}^{t_{m}^{j} t_{n}^{i}}\). The warping

Figure 3: The process of the BEV flow estimation.

process is: \(}_{m}^{t_{i}^{i}}[h+_{m}^{t_{m}^{j} t_{n}^{i}} [h,w,0],w+_{m}^{t_{m}^{j} t_{n}^{i}}[h,w,1] ]=_{m}^{t_{n}^{j}}[h,w]\). After adjusting each non-ego agent's feature map, these estimated feature maps and ego feature map are aggregated together by an aggregation function \(f_{}()\), which implements Step 2e. The fusing function can be any common fusion operation. All our experiments adopt Multi-scale Max-fusion.

### Training details and loss function

To train the overall system, we supervise three tasks: ROI generation, flow estimation, and the final fusion detector. As mentioned before, the functionality of the ROI generator and final fusion detector share the same architecture but do not share the parameters. During the training process, the ROI generator and flow estimation module are trained separately, and later the final fusion detector is trained with the pre-trained two modules. Common loss functions in detection tasks: cross entropy loss and weighted smooth L1 loss are used for the classification and regression of ROI generation and final fusion detector, and MSE loss is used for flow estimation.

## 5 Experimental Results

We propose the first asynchronous collaborative perception dataset and conduct extensive experiments on both simulated and real-world scenarios. The task of the experiments on the two datasets is point-cloud-based object detection. The detection performance was evaluated using Average Precision (AP) at Intersection-over-Union (IoU) thresholds of 0.50 and 0.70.

### Datasets

**IRregular V2V(IRV2V).** To facilitate research on asynchrony for collaborative perception, we simulate the first collaborative perception dataset with different temporal asynchronies based on CARLA , named IRregular V2V(IRV2V). We set 100ms as ideal sampling time interval and simulate various asynchronies in real-world scenarios from two main aspects: i) considering that agents are unsynchronized with the unified global clock, we uniformly sample a time shift \(_{s}(-50,50)\)ms for each agent in the same scene, and ii) considering the trigger noise of the sensors, we uniformly sample a time turbulence \(_{d}(-10,10)\)ms for each sampling timestamp. The final asynchronous time interval between adjacent timestamps is the summation of the time shift and time turbulence. In experiments, we also sample the frame intervals to achieve large-scale and diverse asynchrony. Each scene includes multiple collaborative agents ranging from 2 to 5. Each agent is equipped with 4 cameras with a resolution of 600 \(\) 800 and a 32-channel LiDAR. The detection range is 281.6m \(\) 80m. It results in 34K images and 8.5K LiDAR sweeps. See more details in the Appendix.

**DAIR-V2X.** DAIR-V2X  is a real-world collaborative perception dataset. There is one ego agent and one roadside unit in each frame. All frames are captured from real scenarios at 10 Hz with 3D annotations. Lu et al.  complemented the missing annotations outside the camera view on the vehicle side to cover a 360-degree view detection. We adopt the complemented annotations and set the perceptual range to \(x[-100.8,+100.8]\), \(y[-40,+40]\).

### Quantitative evaluation

**Benchmark comparison.** The baseline methods include late fusion, DiscoNet, V2VNet, V2X-ViT and Where2comm. The red dashed line represents single-agent detection without

Figure 4: Comparison of the performance of CoBEVFlow and other baseline methods under the expectation of time interval from 0 to 500ms. CoBEVFlow outperforms all the baseline methods and shows great robustness under any level of asynchrony on both two datasets.

collaboration. We also consider the integration of SyncNet with Where2comm, which presents the SOTA method Where2comm with resistance to time delay. All methods use the same feature encoder based on PointPillars. To simulate temporal asynchrony, we sample the frame intervals of received messages with binomial distribution to get random irregular time intervals. Fig. 4 shows the detection performances (AP@IoU=0.50/0.70) of the proposed CoBEVFlow and the baseline methods under varying levels of temporal asynchrony on both IRV2V and DAIR-V2X, where the \(x\)-axis is the expectation of the time interval of delay of the latest received information and interval between adjacent frames and \(y\)-axis the detection performance. Note that, when the \(x\)-axis is at 0, it represents standard collaborative perception without any asynchrony. We see that i) the proposed CoBEVFlow achieves the best performance in both simulation and real-world datasets at all asynchronous settings. On the IRV2V dataset, CoBEVFlow outperforms the best methods by 23.3% and 35.3% in terms of AP@0.50 and AP@0.70, respectively, under a 300ms interval expectation. Similarly, under a 500ms interval expectation, we achieve 30.3% and 28.2% improvements, respectively. On the DAIR-V2X dataset, CoBEVFlow still performs best. ii) CoBEVFlow demonstrates remarkable robustness to asynchrony. As shown by the red line in the graph, CoBEVFlow exhibits a decrease of only 4.94% and 14.0% in AP@0.50 and AP@0.70, respectively, on the IRV2V dataset under different asynchrony. These results far exceed the performance of single-object detection, even under extreme asynchrony.

**Trade-off between detection results and communication cost.** CoBEVFlow allows agents to share only sparse perceptual features and the ROI set, which is communication bandwidth-friendly. Figure 5 compares the proposed CoBEVFlow with the previous methods in terms of the trade-off between detection performance (AP@0.50/0.70) and communication bandwidth under asynchrony. We adopt the same asynchrony settings mentioned before and choose 300ms as the expectation of the time interval. We see: i) CoBEVFlow consistently outperforms the state-of-the-art communication efficient solution, where2comm, as well as the other baselines in the setting of asynchrony; ii) as the communication volume increases, the performance of CoBEVFlow continues to improve steadily, while the performance of where2comm and where2comm+SyncNet fluctuates due to improper information transformation caused by asynchrony.

**Robustness to pose error.** We conduct experiments to validate the performance under the impact of both asynchrony and pose error. To simulate the pose error, we add Gaussian noise \((0,_{t})\) on \(x,y\) and \((0,_{r})\) on \(\) during the inference phase, where \(x,y,\) are 2D centers and yaw angle of accurate global poses. Our pose noise setting follows the Gaussian distribution with a mean of 0m, a standard deviation of 0m-0.5m, a mean of \(0^{}\) and a standard deviation of \(0^{}-0.5^{}\). This experiment is conducted under the expectation of time interval is 300ms to simulate the time asynchrony. We compare our CoBEVFlow and other baseline methods including V2X-ViT, Where2comm

   Dataset &  &  \\  Noise Level \(_{t}/_{r}(m/)\) & 0.0/0.0 & 0.1/0.1 & 0.2/0.2 & 0.3/0.3 & 0.4/0.4 & 0.0/0.0 & 0.1/0.1 & 0.2/0.2 & 0.3/0.3 & 0.4/0.4 \\  Model / Metric &  \\  V2X-ViT & 0.641 & 0.626 & 0.627 & 0.625 & 0.619 & 0.693 & 0.692 & 0.545 & 0.685 & 0.681 \\ Where2comm & 0.510 & 0.411 & 0.411 & 0.411 & 0.702 & 0.693 & 0.679 & 0.658 & 0.643 \\ Where2comm+SyncNet & 0.654 & 0.653 & 0.652 & 0.651 & 0.648 & 0.711 & 0.692 & 0.583 & 0.579 & 0.671 \\ CoBEVFlow (ours) & **0.831** & **0.820** & **0.815** & **0.802** & **0.781** & **0.738** & **0.743** & **0.732** & **0.723** & **0.703** \\  Model / Metric &  \\  V2X-ViT & 0.511 & 0.504 & 0.502 & 0.504 & 0.501 & 0.545 & 0.545 & 0.545 & 0.685 & 0.543 \\ Where2comm & 0.388 & 0.323 & 0.312 & 0.302 & 0.293 & 0.577 & 0.577 & 0.561 & 0.658 & 0.543 \\ Where2comm+SyncNet & 0.549 & 0.550 & 0.545 & 0.538 & 0.527 & 0.587 & 0.583 & 0.579 & 0.570 & **0.567** \\ CoBEVFlow (ours) & **0.757** & **0.730** & **0.686** & **0.628** & **0.570** & **0.599** & **0.593** & **0.579** & **0.571** & 0.560 \\   

Table 1: Detection performance on IRV2V and DAIR-V2X dataset with pose noises following Gaussian distribution in the testing phase.

Figure 5: Trade-off between detection performance (AP@0.50/0.70) and communication bandwidth under asynchrony (expected 300ms latency) on IRV2V dataset. CoBEVFlow outperforms even with a much smaller communication volume.

and SyncNet. Table 1 shows the results on IRV2V and DAIR-V2X dataset. We see that **CoBEVFlow still performs well even when both pose errors and time asynchrony appear**. CoBEVFlow consistently outperforms other methods across all noise settings on the IRV2V dataset. In the case of noise levels of 0.4/0.4, our approach achieves 0.133 and 0.043 improvement over SyncNet.

### Qualitative evaluation

**Visualization of detection results.** We illustrate the detection results of V2X-ViT, SyncNet, and CoBEVFlow at three asynchrony levels on the IRV2V dataset in figure 6 and the DAIR-V2X dataset in figure 7. The expectations of time intervals are 100, 300, and 500ms. The red box represents the detection result and the green box represents the ground truth. V2X-ViT shows significant deviations in collaborative perception under asynchrony, while SyncNet shows poor compensation due to introducing noise in feature regeneration and irregularity-incompatible design. The third row shows the results of CoBEVFlow, which achieves precise compensation and outstanding detections.

**Visualization of BEV flow map.** Figure 8 visualizes the feature map before/after compensation of CoBEVFlow in Plot(a)(b), the corresponding flow map in Plot(c), and matching, detection results

Figure 6: Visualization of detection results for V2X-ViT, SyncNet, and CoBEVFlow with the expectation of time intervals are 100, 300, and 500ms on IRV2V dataset. CoBEVFlow qualitatively outperforms the others under different asynchrony. Red and green boxes denote detection results and ground-truth respectively.

Figure 7: Visualization of detection results for Where2comm, V2X-ViT, SyncNet, and our CoBEVFlow with the expectation of time intervals are 100, 300, and 500ms on the DAIR-V2X dataset. Red and green boxes denote detection results and ground-truth respectively.

after compensation in Plot(d). The green boxes in Plot (d) are the ground truth, the blue boxes are the historical detections with the matched asynchronous ROIs and the red boxes are the compensated detections. We see that the BEV flow map can be precisely estimated and is beneficial for perceptual feature alignment. The compensated detection results are more accurate than the uncompensated ones.

### Ablation Study

We conduct ablation studies on the IRV2V dataset. Table 2 assesses the effectiveness of the proposed operations, including time encoding, the object(feature/detected box) to warp, and our ROI matcher. We see that: i) time encoding encodes the irregular continuous timestamp and makes the estimation more accurate; ii) Warping the features outperforms warping the boxes directly a lot, which means the operation for features shows superiority over the operation for the detection results; and iii) our ROI matcher shows more proper matching results than traditional Hungarian matching.

## 6 Conclusion and limitation

We formulate the asynchrony collaborative perception task, which considers various unideal factors that may cause communication latency or information misalignments during collaborative communication. We further propose CoBEVFlow, a novel asynchrony-robust collaborative perception framework. The core idea of CoBEVFlow is BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. Comprehensive experiments show that CoBEVFlow achieves outstanding performance under all settings and far superior robustness with asynchrony.

**Limitation and future work.** The current work focuses on addressing the asynchrony problem in collaborative perception. It is evident that effective prediction can compensate for the negative impact of temporal asynchrony in collaborative perception. Moreover, the generated flow can also be utilized not only for compensation but also for prediction. In the future, we expect more works on exploring the ROI-based flow generation design for collaborative perception and prediction tasks.

**Acknowledgment.** This research is supported by NSFC under Grant 62171276 and the Science and Technology Commission of Shanghai Municipal under Grant 21511100900, 22511106101, and 22DZ2229005.

    &  &  \\  TE & Warped by Flow & Matcher & 300ms & 500ms \\   & Box & Hungarian & 0.724 / 0.473 & 0.644 / 0.388 \\  & Box & Hungarian & 0.747 / 0.595 & 0.668 / 0.438 \\ \(\) & Box & Ours & 0.764 / 0.611 & 0.571 / 0.399 \\ \(\) & Feature & Hungarian & 0.779 / 0.690 & 0.739 / 0.614 \\ \(\) & Feature & Ours & **0.831 / 0.757** & **0.815 / 0.687** \\   

Table 2: Ablation Study on IRV2V dataset. Time encoding(TE), BEV flow on features, and the proposed matcher all improve the performance.

Figure 8: Visualization of compensation with CoBEVFlow on IRV2V dataset. In subfigure(d), green boxes are the objectsâ€™ ground truth locations, blue boxes are the detection results based on the historical asynchronous features and red boxes are the detection results after compensation. CoBEVFlow achieves precise matching and compensation with the BEV flow map and mitigates the negative impact of asynchrony to a great extent.