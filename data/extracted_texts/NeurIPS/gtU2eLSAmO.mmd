# Brain-JEPA: Brain Dynamics Foundation Model

with Gradient Positioning and Spatiotemporal Masking

 Zijian Dong, Ruilin Li, Yilei Wu, Thuan Tinh Nguyen, Joanna Su Xian Chong, Fang Ji, Nathanael Ren Jie Tong, Christopher Li Hsian Chen, Juan Helen Zhou

zijian.dong@u.nus.edu, {li.rl, helen.zhou}@nus.edu.sg

Equal contributionCorresponding author

###### Abstract

We introduce _Brain-JEPA_, a brain dynamics foundation model with the Joint-Embedding Predictive Architecture (JEPA). This pioneering model achieves state-of-the-art performance in demographic prediction, disease diagnosis/prognosis, and trait prediction through fine-tuning. Furthermore, it excels in off-the-shelf evaluations (_e.g._, linear probing) and demonstrates superior generalizability across different ethnic groups, surpassing the previous large model for brain activity significantly. Brain-JEPA incorporates two innovative techniques: **Brain Gradient Positioning** and **Spatiotemporal Masking**. Brain Gradient Positioning introduces a functional coordinate system for brain functional parcellation, enhancing the positional encoding of different Regions of Interest (ROIs). Spatiotemporal Masking, tailored to the unique characteristics of fMRI data, addresses the challenge of heterogeneous time-series patches. These methodologies enhance model performance and advance our understanding of the neural circuits underlying cognition. Overall, Brain-JEPA is paving the way to address pivotal questions of building brain functional coordinate system and masking brain activity at the AI-neuroscience interface, and setting a potentially new paradigm in brain activity analysis through downstream adaptation. _Code is available at: https://github.com/Eric-LRL/Brain-JEPA_.

## 1 Introduction

Understanding large-scale brain activity data is crucial for deciphering the complex mechanisms underlying cognitive processes and human behavior. Functional magnetic resonance imaging (fMRI) captures blood-oxygen-level dependent (BOLD) signals that reflect regional brain activity. It emerges as an indispensable tool in neuroscience for identifying the neural bases of cognitive processes [1; 2; 3]. Deep learning approaches have been developed for fMRI analysis, improving brain disease diagnosis and deepening insights into cognition and behavior [4; 5; 6; 7; 8; 9; 10; 11; 12; 13]. Despite notable advances, these task-specific models suffer from limited generalizability and adaptability to other tasks. In addition, they fail to leverage the vast amounts of unlabeled fMRI data available [14; 15].

Artificial intelligence (AI) is experiencing a paradigm shift from task-specific training to building foundation models that are trained on extensive data using self-supervision at scale . Unlike the models with singular functions, foundation models can be adapted to a diverse array of downstream tasks. Large language models such as GPT  and LLaMA  have shown significant potential in natural language processing, with expansive applications in healthcare, biomedicine, and beyond .

In the field of fMRI time series analysis, brain language model (brainLM) is one of the most representative foundation models . It is a masked autoencoder (MAE)  trained to reconstruct masked fMRI time series. However, as an indirect measure of neuronal activity, the BOLD signal has a relatively low signal-to-noise ratio (SNR), which is influenced by a mixture of factors and distorted by non-neuronal fluctuations . Filling every bit of the fMRI time series as in brainLM can hinder the model's ability to distinguish between noise and actual signals. This can result in either amplifying noise or missing critical subtle variations in brain activity. Unlike natural images, which have high information density with structures such as edges and colors, fMRI data has spatiotemporally sparser signals distributed across brain volumes without clear boundaries, making it difficult to accurately reconstruct signal of masked regions of interest (ROIs). Furthermore, it has been widely shown that masked pretraining in generative architectures such as MAE leads to suboptimal performance in off-the-shelf evaluations (_e.g._, linear probing) . Because of that, BrainLM requires computationally intensive end-to-end finetuning, with a three-layer MLP attached to the pretrained encoder, to achieve optimal performance. Furthermore, the absence of comparisons with state-of-the-art methods for downstream task performance and the focus only on Caucasian cohorts limit BrainLM's applicability in clinical settings.

Therefore, rather than focusing on the original brain activity time series, the inherent noise and sparse information density of fMRI lead us to explore the latent space of fMRI time series extracted from a strong encoder (_e.g._, Vision Transformer (ViT) ). It potentially offers a higher SNR after "compression", achieving a greater level of abstraction that captures subtle yet crucial patterns . Recently, Imaged-based Joint-Embedding Predictive Architecture (I-JEPA) has been proposed as a non-generative architecture for self-supervised learning from images . It predicts the representations of various target blocks rather than reconstructing the masked input like MAE during pretraining. By predicting representations in the latent space, I-JEPA enhances the semantic quality of learned representations and boosts scalability and efficiency.

Training a brain dynamics foundation model using a JEPA-like architecture might offer advantages over the MAE approach. However, the distinct spatiotemporal characteristics of fMRI data make direct application of the JEPA architecture suboptimal: 

Positional embeddings in transformer play a crucial role by incorporating information about the order or position of tokens in the input data (_e.g._, the order of different words in a sentence or the locations of pixels in an image) . However, there is no such natural "order" for different ROIs across the 3D brain volume in fMRI. BrainLM utilizes anatomical positions to label each ROI , yet it does not account for brain functional parcellation, where nearby anatomical ROIs might exhibit rather different brain activation patterns represented by a lack of local coherence in fMRI data . 

I-JEPA employs a random multi-block selection of context and target. However, unlike images, fMRI presents complex patterns across both spatial and temporal domains. Given the smaller sample size and sparser information density in fMRI datasets compared to datasets like ImageNet , learning in fMRI requires a stronger inductive bias. This would enhance the efficiency of training models by better capturing the underlying patterns specific to brain activity. Given the unique challenges presented by fMRI data, there is a pressing need to develop a functual coordinate system and a tailored masking strategy for large-scale pretraining on fMRI data.

_These neglected yet crucial questions of developing a functional coordinate system and a masking strategy for large-scale pretraining with fMRI data, lie at the intersection of AI and neuroscience, highlighting important interdisciplinary challenges._

To address these gaps, here we introduce _Brain-JEPA_, a brain dynamics foundation model with the Joint-Embedding Predictive Architecture (JEPA). Instead of reconstructing masked inputs during pretraining, Brain-JEPA predicts abstract representations of sampled targets from the observation. We propose two innovative techniques to enhance model performance and address key questions in AI for neuroscience: First, **Brain Gradient Positioning** provides a brain functional coordinate system for positional embedding of brain functional parcellation (Section 3.1). Second, **Spatiotemporal Masking** offers a tailored masking strategy for the heterogeneous time-series patches inherent in fMRI (Section 3.2). Moreover, in downstream experiments, our proposed Brain-JEPA achieves state-of-the-art results in demographic prediction, disease diagnosis/prognosis, and trait prediction through fine-tuning. It also excels in off-the-shelf evaluations (_e.g._, linear probing), and shows superior generalizability across different ethnic groups. Brain-JEPA enhances brain activity analysis and deepens our understanding of critical AI-neuroscience questions related to constructing functional coordinate systems and developing spatiotemporal masking strategies.

## 2 Related Work

**Task-specific Models for fMRI (state-of-the-art).** SVR and MLP have been used in fMRI analysis, utilizing Pearson correlation matrices derived from fMRI time series as input [8; 9]. Deep learning models have substantially advanced fMRI analysis in recent years. BrainNetCNN  introduces a convolutional neural network (CNN) with specialized convolutional filters tailored for brain network. BrainGNN  utilizes ROI-aware graph neural networks (GNNs) to effectively harness functional brain network information, incorporating a pooling operator to highlight key ROIs. More recently, Brain network transformer (BNT)  employs transformer encoders to generate embeddings for ROIs based on Pearson correlation matrices, alongside a readout layer designed to identify clusters within the brain. Swift  applies Swin Transformer architecture  to process brain functional data. As noted in Section 1, these task-specific models have limited generalizability and adaptability across different tasks, and fail to utilize extensive unlabeled fMRI data.

**The fMRI Foundation Model.** BrainLM  stands out as the first fMRI foundation model, employing MAE for self-supervised pretraining of fMRI data. In this approach, fMRI time series are treated as images and patchified. The training goal is to reconstruct the masked patches of the time series. As outlined in Section 1, BrainLM exhibits several limitations: 1) Direct reconstruction of masked input may not be suitable for inherently noisy data with low information density, such as fMRI. It complicates the differentiation between noise and signal, making it difficult to capture underlying patterns. 2) Generative architectures like MAE result in suboptimal performance in linear probing, a critical method for evaluating learned representations. 3) The absence of comparisons with state-of-the-art models and evaluations limited to Caucasian cohorts restricts its broader applicability. BrainMass , a concurrent work in large-scale self-supervised learning for neuroimaging, focuses on brain network analysis rather than brain dynamics, distinguishing it from our research.

Figure 1: **Brain-JEPA.** With a Vision Transformer (ViT) as the observation encoder \(f_{}\), Brain-JEPA employs a single observation block to predict the representations of target blocks. **(1)** The input fMRI data is initially segmented into patches for subsequent processing. **(2)** Through Spatiotemporal Masking, the input data—excluding the observation block—is divided into three distinct regions: Cross-ROI (\(\)), Cross-Time (\(\)), and Double-Cross (\(\)). The target blocks are sampled from different regions separately. **(3)** A narrower ViT, serving as the predictor \(g_{}\), takes the output \(_{x}\) from \(f_{}\). It predicts the representations of a target block \(}_{y}^{r}\) conditioned on positional embedding (brain gradient positioning for ROI locations and sine and cosine functions for temporal positioning). **(4)** These predicted representations align with those \(s_{y}^{r}\) from the target encoder \(f_{}\), whose parameters are incrementally updated through an Exponential Moving Average (EMA) of the observation encoder’s parameters.

Method

In this section, we outline the methodology of Brain-JEPA. Instead of reconstructing masked patches of fMRI time series, Brain-JEPA operates in the latent space, as depicted in Figure 1. With the observation block excluded, the input data is divided into three non-overlapping regions: Cross-Time (\(\)), Cross-ROI (\(\)), and Double-Cross (\(\)). This division forces the model to engage in forecasting time series, generalizing across unseen ROIs, and predicting time series for unseen ROIs. Section 3.1 details the **Brain Gradient Positioning** we proposed, which encodes the functional relationships among different ROIs, serving as a brain functional coordinate system in the brain's functional organization. In Section 3.2, we introduce **Spatiotemporal Masking**, which injects a strong inductive bias during the masking process, leading to faster convergence during pretraining and superior performance in downstream tasks.

### Brain Gradient Positioning

We propose Brain Gradient Positioning, which provides a brain functional coordinates system based on the functional connectivity gradient. Positional embeddings are crucial in transformer architectures, as they encode information about the positions of tokens in a sequence. These embeddings can be implemented using fixed sine and cosine functions across various frequencies  or through learnable embeddings that adapt during training . However, the integration of positional information into fMRI time series has long been neglected. FMRI data, incorporating complex spatiotemporal information, requires separate consideration of its temporal and spatial domains. The temporal domain, representing timesteps during scanning, is well-suited for conventional sine and cosine positional embeddings, as the time series in each ROI is sequentially ordered by time. However, this method is not appropriate for the spatial domain, where ROIs across brain volumes lack a simple, inherent order, making sine and cosine embeddings unsuitable for capturing spatial relationships. Anatomical locations of ROIs offer an alternative to sine and cosine functions  but fall short in capturing functional parcellation. Spatially adjacent ROIs can exhibit significantly different brain activation patterns, reflecting the inherent lack of local coherence in fMRI data .

The functional connectivity gradient is a continuous measure that captures the functional relations among different ROIs. Each attribute in the gradient represents an axis in the latent space of brain regions and networks. The relative distance between different ROIs indicates the similarity in their connectivity (_i.e._, shorter distance means higher similarity in connectivity). The concept of a spatial gradient as conceptualized by Mesulam in 1998 entailed a synaptic hierarchy that supports cognitive processes . Recent studies have built upon this concept, revealing that brain networks in adult humans and macaques exhibit linear distributions across different gradient axes . Using this methodology, it has been shown that these gradients reflect the functional changes related to age [33; 34; 35; 36; 37], cognition [37; 38] and brain diseases [35; 39; 40]. These gradients together provide a framework to assess the relationship between brain regions based on their relative positioning across different gradient axes.

Before deriving the gradients, we first calculate a non-negative affinity matrix \((i,j)\) (a graph Laplacian) as follows:

\[(i,j)=1-^{-1}(_{i}_{j}^{T}}{ _{i}_{j}})\] (1)

where \(_{i}\) and \(_{j}\) represents the features (functional connectivity) across the ROI \(i\) and \(j\), respectively. Gradients are then derived using diffusion map [41; 42], a nonlinear dimension reduction method used to identify the underlying manifold structure of the data. We can obtain the diffusion matrix \(_{}\) and the diffusion operator \(_{}\) from \(\) as follows:

Figure 2: **Brain gradient positioning. Brain cortical regions are situated in the top 3 gradient axes and colored based on their positions. These colors are then projected back into the brain surface.**

\[_{}\!=\!^{-1}_{},_{}\!=\!^{-}^{-}\] (2)

where \(\) is the degree matrix of \(\). Here \(\) is set to 0.5 to maintain the global relations between ROIs in the embedding space.

Finally, we can compute the eigenvectors and eigenvalues of \(_{}\), and stack the column vectors to formulate the diffusion map \(_{t}\!\!^{n m}\) (\(n\) ROIs in total, with \(m\) gradients for each) at time \(t\) and the gradient matrix \(\) with the same dimension:

\[_{t}\!=\![_{1}^{t}_{1},\!_{2}^{t}_{2},\!..., \!_{m}^{t}_{m}],\!=\![_{1},\!_{2},\!...,\!_{m}]\] (3)

where \(_{k}\) are the eigenvalues and \(_{k}\) are the corresponding eigenvectors (gradients) of the graph Laplacian. The parameter \(t\) represents the diffusion time, which controls the scale of the diffusion process. Here we estimated the eigenvalues \(_{k}\) at time \(t\) by dividing it by \(1-_{k}\) to enhance robustness against noisy eigenvalues.

In Brain-JEPA, we leverage \(\) as the spatial positioning of ROIs. Specifically, the gradient \(\!\!^{n m}\) is transformed into \(}\!\!^{n d/2}\) through a trainable linear layer, where \(d\) represents the embedding dimension of the ViT backbone. The predefined temporal positioning \(\!\!^{n d/2}\) is obtained using sine and cosine functions . The final positional embedding can then be formulated as \(\!=\![,}]\!\!^{n d}\). Figure 2 provides a visualization of the top 3 gradients in Euclidean space with each ROI color coded by their locations. As shown, the brain gradient positioning reflects functional network architecture, such as the **somatomotor**, default mode and visual networks, consistent with previous literature [32; 37].

### Spatiotemporal Masking

**Observation.** Brain-JEPA aims to predict representations of multiple target blocks based on the representation of a single observation block. For an input fMRI time series, the temporal signal for each parcel is divided into patches after shuffling ROIs, each containing \(p\) time points (dash boxes in Figure 1). The observation block \(\) is obtained by randomly sampling a block within the range \(\{_{R}^{}\), \(_{T}^{}\}\). \(_{R}^{}\) specifies the range ratio along the ROI dimension, and \(_{T}^{}\) pertains to the timestep patches (10 in total). Subsequently, \(\) is fed through the observation encoder \(f_{}\), generating a corresponding patch-level representation \(_{x}\):

\[_{x}\!=\!\{_{x_{j}}\}_{j_{x}}\] (4)

where \(_{x}\) represents the mask associated with the observation block \(\), \(_{x_{j}}\) is the representation of the \(j^{}\) patch.

**Targets.** Given a single observation, the model is trained to predict other parts of the fMRI within the latent space. Random sampling of targets like MAE  might allow the model to learn shortcuts (_e.g._, interpolation of time series) or rely heavily on simpler, more frequent patterns in the data, which could limit its generalizability. It is crucial to recognize that patches in fMRI vary spatially depending on the positions in their brain functional organization, and temporally regarding brain states and task conditions. The nonlinear relationship among brain networks further complicates the interactions between different brain patches.

As shown in Figure 1, we categorize the remaining parts (with the observation excluded) into three distinct and non-overlapping regions: Cross-ROI (\(\)), Cross-Time (\(\)), and Double-Cross (\(\)). For targets in the \(\) and \(\) regions, the model should generalize the observation across different ROIs spatially or timesteps temporally. For targets in the \(\) regions, which are the most challenging, the model should generalize to unseen ROIs at unencountered timesteps. We randomly sample \(K\) blocks from each of the three types of regions as targets, forcing the model to handle a variety of prediction tasks with a stronger inductive bias. We denote the mask corresponding of the region \(r\) (\(r\!\!\{\),\(\),\(\))} as \(_{y}^{r}\).

**Overlapped sampling.** It has been shown in  that a sufficiently large dynamic range of masking ratio could benefit pretraining. To effectively adjust the observation-to-input ratio during pretraining, we implement an overlapped sampling strategy that allows for a flexible, rather than fixed, ratio. When sampling the target block \(_{y}^{r}\) from region \(r\), for \(r\!=\!\) or \(\), we sample the target from the union of the observation mask and region \(r\) mask; while for \(r\!=\!\), we directly sample the target from the \(\) region mask. Formally, the overlapped sampling strategy is defined as:

\[_{y}^{}\!\!_{x}\!_{y}^{},_{y}^{}\!\!_{x}\!_{y}^{},_{y}^{ }\!\!_{y}^{}\] (5)Afterwards, part of the observation region might overlap with some \(\) and \(\) targets. We remove any ROIs in the observation that overlap with the \(\) targets. Additionally, we eliminate all timesteps for ROIs that show overlap with the \(\) targets. Refer to Table 6 for the block sizes.

**Training.** Given the output \(_{x}\) from the observation encoder \(f_{}\), the predictor \(g_{}\) is trained to predict the three kinds of targets \(^{r}_{y}\) conditioned on the positional embedding \(\) (Figure 1). The training loss \(\) is the average \(L_{2}\) distance between \(^{r}_{y}\) and its corresponding prediction:

\[=_{r}\|}^{r}_{y}-^{r}_{y} \|_{2}^{2},\,}^{r}_{y}=g_{}(_{x}|)\] (6)

## 4 Experiments

### Datasets

We leveraged the large-scale public dataset - UK Biobank (UKB) [44; 45] for the self-supervised pretraining of Brain-JEPA. It includes resting-state fMRI recordings with medical records from 40,162 participants aged 44 to 83. Multi-site recordings were acquired with the temporal resolution of 0.735s. We allocated 80% of this dataset for pretraining (of which we calculated the group-level gradients as well), with the 20% held-out for downstream evaluation (internal tasks of age and sex prediction).

We used three datasets for external evaluation: HCP-Aging, as a segment of the public Human Connectome Project (HCP) , includes resting-state fMRI from 656 healthy elderly participants. It was used to predict traits (Neuroticism and Flanker score) and demographics (age and sex). The Alzheimer's Disease Neuroimaging Initiative (ADNI)  was used for the early diagnosis and prognosis of neurodegenerative diseases, with fMRI from 189 participants for normal control (NC) _v.s._ mild cognitive impairment (MCI) classification, and 100 cognitively normal participants for amyloid positive v.s. negative classification. Moreover, to assess generalizability across different ethnic groups and real-world clinical applications, we included resting-state fMRI of Asian participants recruited by Memory, Ageing and Cognition Centre (MACC), with 539 participants for NC v.s. MCI classification. More details of the downstream tasks performed can be found in the Appendix A.

All fMRI data was parcellated into \(n=450\) ROIs, using Schaefer-400  for cortical regions and Tian-Scale III  for subcortical regions. Robust scaling was implemented by subtracting the median and dividing by the interquartile range, calculated across participants for each ROI . Our default input size is 160 timesteps for each of the 450 ROIs (_i.e._, 450x160). UKB and HCP-Aging used multi-band acquisition with a high temporal resolution (TR \(\!0.7\) seconds), while ADNI and MACC used single-band acquisition with a lower resolution (TR \(\!2\) seconds). To ensure consistency across datasets, we standardized the temporal resolution by downsampling the multi-band data using a temporal stride of 3, aligning the TR of all datasets to approximately 2 seconds. During the fine-tuning and linear probing stage, all the downstream datasets were divided into a 6:2:2 ratio for training, validation, and testing.

### Implementation details

For Brain-JEPA pretraining, we utilized ViT architectures for the observation encoder, target encoder, and predictor. We employed FlashAttention [50; 51] in our self-attention implementation to improve computational efficiency and reduce memory usage. Balancing the trade-off between data quantity and the model complexity, we experimented with ViT-Small (ViT-S) (22M), ViT-Base (ViT-B) (86M), and ViT-Large (ViT-L) (307M) for the observation encoder. For predictor, it is designed as a lightweight (narrow) ViT. Specifically, the predictor has the same architecture as the corresponding observation encoder, differing only in embedding dimension and depth. For the ViT-S and ViT-B observation encoders, the predictor has a depth of 6 and embedding dimensions of 192 and 384, respectively. The ViT-L observation encoder uses a predictor with a depth of 12 and an embedding dimension of 384. Brain-JEPA is pretrained without a [cls] token. For evaluation, we used the target encoder and average pooled its output to generate a global fMRI representation. The main results in Section 4.3, along with the analysis in Section 4.5, 4.6 and 4.7 were all based on ViT-B pre-trained for 300 epochs. Refer to Appendix B for optimization and masking details.

### Main results

Table 1, 2, and 3 compare Brain-JEPA with the existing deep learning models for fMRI analysis and foundation model BrainLM. We select the three deep learning baselines because they not only represent the previous state-of-the-art in fMRI analysis but also exemplify diverse model types: convolutional neural network (CNN)-based BrainNetCNN , graph neural network (GNN)-based BrainGNN , and transformer-based BNT . For a fair comparison, both Brain-JEPA and BrainLM utilized a ViT-B backbone and were fine-tuned for downstream tasks (Section 4.4 will discuss performance scaling with different model sizes, and Section 4.5 will examine linear probing comparisons between the two models). BrainLM utilized [cls] token for downstream evaluation.

The results show that Brain-JEPA achieves state-of-the-art performance in various downstream tasks on both the unseen data from the same pretrained cohort and other independent datasets. Brain-JEPA effectively captures fundamental demographic information such as age and sex, cognitive/personality variance (Neuroticism and Flanker), and disease-related patterns for neurodegenerative diseases. Notably, Brain-JEPA demonstrates superior performance in classifying NC/MCI in Asian ethnic groups -- one of the most challenging tasks for early diagnosis and prognosis of Alzheimer's Disease (AD) -- even though it was trained exclusively on the Caucasian cohort. Please refer to C.1 for additional results on more datasets and comparisons with more baselines.

    &  &  &  & } \\    & MSE \(\) & \(\) & ACC (\%) \(\) &  \\  BrainNetCNN  & 0.462 (0.017) & 0.611 (0.023) & 71.16 (0.08) & 72.23 (0.92) & 1.201 (.097) & 0.096 (.006) & 1.045 (.036) & 0.201 (.018) \\ BrainGNN  & 0.423 (0.015) & 0.672 (0.024) & 72.7 (0.54) & 74.09 (0.67) & 1.183 (0.096) & 0.098 (.007) & 0.982 (.043) & 0.309 (.062) \\ BNT  & 0.414 (0.035) & 0.731 (0.057) & 72.41 (0.09) & 73.68 (1.11) & 1.199 (.091) & 0.101 (.005) & 0.997 (.037) & 0.307 (.026) \\ BrainLM  & 0.331 (0.018) & 0.832 (0.028) & 74.39 (1.55) & 77.51 (1.13) & 0.942 (0.082) & 0.231 (.012) & **0.971 (.054)** & 0.318 (.048) \\  Brain-JEPA & **0.298 (0.017)** & **0.844 (0.030)** & **81.52 (1.03)** & **84.26 (0.82)** & **0.897 (.055)** & **0.307 (.006)** & 0.972 (.038) & **0.406 (.027)** \\   

Table 2: External tasks of demographics and trait prediction on HCP-Aging.

    &  &  &  & } \\    & MSE \(\) & \(\) & ACC (\%) \(\) & F1 (\%) \(\) & MSE \(\) & \(\) & MSE \(\) & \(\) \\  BrainNetCNN  & 0.462 (0.017) & 0.611 (0.023) & 71.16 (0.08) & 72.23 (0.92) & 1.201 (.097) & 0.096 (.006) & 1.045 (.036) & 0.201 (.018) \\ BrainGNN  & 0.423 (0.015) & 0.672 (0.024) & 72.7 (0.54) & 74.09 (0.67) & 1.183 (0.096) & 0.098 (.007) & 0.982 (.043) & 0.309 (.062) \\ BNT  & 0.414 (0.035) & 0.731 (0.057) & 72.41 (0.09) & 73.68 (1.11) & 1.199 (.091) & 0.101 (.005) & 0.997 (.037) & 0.307 (.026) \\ BrainLM  & 0.331 (0.018) & 0.832 (0.028) & 74.39 (1.55) & 77.51 (1.13) & 0.942 (0.082) & 0.231 (.012) & **0.971 (.054)** & 0.318 (.048) \\  Brain-JEPA & **0.298 (0.017)** & **0.844 (0.030)** & **81.52 (1.03)** & **84.26 (0.82)** & **0.897 (.055)** & **0.307 (.006)** & 0.972 (.038) & **0.406 (.027)** \\   

Table 3: External tasks of brain disease diagnosis and prognosis on ADNI and MACC.

### Performance scaling

Figure 3 presents the performance of Brain-JEPA across various model sizes, using ViT-S, ViT-B, and ViT-L as backbones. The results demonstrate that the larger model configuration consistently achieves better performance. Specifically, there is a clear trend of increasing accuracy/correlation with larger models, with Brain-JEPA using ViT-L consistently achieving the best performance. We also studies the scaling property with respect to dataset size, please refer to C.2 for additional results.

### Linear probing

BrainLM initially showcases its performance improvements through fine-tuning, complemented by an attached MLP . However, to effectively assess the representations learned during pretraining, off-the-shelf evaluations such as linear probing are essential. As depicted in Figure 4, Brain-JEPA consistently outperforms BrainLM in linear probing and exhibits a smaller performance decline from fine-tuning to linear probing. This highlights the robustness and higher level of abstraction in the representations learned by Brain-JEPA.

### Ablation study

We first compared Brain-JEPA with its ablated versions, employing sine and cosine functions  and anatomical locations  for ROI spatial positioning, as shown in Figure 5. Brain Gradient Positioning demonstrates superior performance over these two baseline methods. It indicates that Brain Gradient Positioning facilitates natural and accurate placement of brain functional parcellations, enhancing the learning of brain dynamics. Next, we assessed the effectiveness of our proposed Spatiotemporal Masking by comparing Brain-JEPA, pretrained over various numbers of epochs, to its ablated counterpart that utilizes standard multi-block sampling of targets . This comparison, illustrated in Figure 6, highlights that not only does our proposed masking technique yield superior

Figure 4: Fine-tuning _v.s._ linear probing.

Figure 5: Comparisons of spatial positional embedding (For the first task, refer to the left \(y\) axis for the Pearson’s Correlation, with the right \(y\) axis accuracy for the last two tasks).

Figure 3: Performance scaling of the model sizes.

performance, but it also introduces a stronger inductive bias leading to a more efficient pretraining. Notably, Brain-JEPA achieves or even surpasses the peak performance of the ablated version, which was pretrained for 300 epochs, with significantly fewer epochs--only 100, 200, and 50 respectively. For more ablation results regarding architectures and the number of gradient components, please refer to C.3.

### Interpretation

With the Schaefer functional atlas , the brain network is categorized into seven distinct sub-networks: the control network (CN), the default mode network (DMN), the dorsal attention network (DAN), the limbic network (LN), the salience ventral attention network (SAN), the somatomotor network (SMN), and the visual network (VN). To assess whether Brain-JEPA has captured the brain functional organization, we calculate the network-level attention for NC/MCI classification. For each ROI, we first average the self-attention across its 10 patches. Next, we average the values of the ROIs within each sub-network and normalize them to obtain the network-level attention distribution. As shown in Figure 7, we found consistent patterns across both Caucasian and Asian ethnic groups, with the model highlighting the critical roles of the DMN, CN, SAN, and LN in cognitive impairment, consistent with previous literature [52; 53; 54].

## 5 Conclusion

In this study, we developed Brain-JEPA, a brain dynamics foundation model based on the Joint-Embedding Predictive Architecture (JEPA). Brain-JEPA predicts abstract representations of sampled targets from observations during the pretraining stage. Utilizing Brain Gradient Positioning, Brain-JEPA encodes brain functional organization more naturally and accurately. With Spatiotemporal Masking, it effectively handles heterogeneous patches in fMRI time series. Brain-JEPA fosters generalizable and highly abstract representations of fMRI, achieving state-of-the-art performance across various tasks, including demographic prediction, trait prediction, and disease diagnosis and prognosis across different cohorts and ethnic groups. Our study provides new insights into applying large-scale self-supervised learning to brain activity modelling and contributes to addressing key questions in AI for neuroscience.

Figure 6: Comparisons of masking strategies.

Figure 7: Attention across different brain networks for NC/MCI classification.

Limitation and future work

We acknowledge several limitations in our study, which also serve as inspirations for future research: 1) Larger models: Due to limited computing resources, we have not tested larger models like ViT-H. We expect that larger models could further improve performance. 2) More diverse datasets: A more diverse brain recording dataset for pretraining, including different ethnicity cohorts collected from various sites, scanning protocols, behavioral tasks, and disease groups, could enhance the generalizability and robustness of the representations learned by the model. 3) Fine-grained interpretation: More thorough interpretation can be achieved through the attention mechanism, such as comparing cortical and subcortical regions, identifying salient ROIs and critical timesteps. This would enable more nuanced and complex spatiotemporal interpretations. 4) Multi-modal integration: Brain-JEPA sets a potential foundation for integrating multimodal brain activity data such as MEG and EEG or even brain structure data like T1-weighted MRI. The integration could enhance our understanding of brain structure, function, and their links to human behavior and mental disorders. Please refer to Appendix D for the broader impact of Brain-JEPA.

## 7 Acknowledgements

This study was supported by the Singapore National Medical Research Council (NMRC/OFLCG19May-0035, NMRC/CIRG/1485/2018, NMRC/CSA-SI/0007/2016, NMRC/MOH-00707-01, NMRC/CG/435 M009/2017-NUH/NUHS, CIRG21nov-0007 and HLCA23Feb-0004), RIE2020 AME Programmatic Fund from A*STAR, Singapore (No. A20G8b0102), Ministry of Education (MOE-T2EP40120-0007 & T2EP2-0223-0025, MOE-T2EP20220-0001), and Yong Loo Lin School of Medicine Research Core Funding, National University of Singapore, Singapore.