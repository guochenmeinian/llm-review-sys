# Has My System Prompt Been Used? Large Language Model Prompt Membership Inference

Roman Levin

Amazon

Valeridia Cherepanova

Amazon AWS AI

&Abhimanyu Hans

University of Maryland

 Equal contribution.

Avi Schwarzschild

Carnegie Mellon University

&Tom Goldstein

University of Maryland

###### Abstract

Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of generations corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective in both standard and challenging scenarios, including black-box settings. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.

Figure 1: **Prompt Detective** verifies if a third-party chat bot uses a given proprietary system prompt by querying the system and comparing distribution of outputs with outputs obtained using proprietary system prompt.

## 1 Introduction

Prompt engineering offers a powerful, flexible, and fast way to optimize large language models (LLMs) for specific applications significantly reducing the time for prototype development. Carefully crafted prompts can have significant business impact allowing to reduce deployment costs, and ensure optimal customer-facing experiences. Large language model providers, such as Anthropic and OpenAI, release detailed prompt engineering guides on prompting strategies allowing their customers to reduce hallucination rates and optimize business performance (OpenAI, 2023; Anthropic, 2024b). Developers put significant effort into creating prompt templates, and consider them to be IP worth protecting (Schulhoff et al., 2024). The use of system prompts also provides specialized capabilities such as taking on a character which is often leveraged by startups 1.

The importance and promise of prompt engineering gave rise to the interest of the community in protecting proprietary prompts and a growing body of academic literature explores prompt reconstruction attacks (Hui et al., 2024; Zhang et al., Morris et al., 2023) which attempt to recover a prompt used in a language model to produce particular generations. As prompt reconstruction is inherently a sophisticated problem, it is challenging for these methods to consistently recover prompts with high accuracy. Additionally, prompt reconstruction approaches usually do not offer certifiable ways to verify if the recovered prompt was indeed used.

In this work, we specifically focus on the problem of verifying if a particular system prompt was used in a large language model. This problem can be viewed through the lens of an adversarial setup: an attacker may have reused someone else's proprietary system prompt and deployed an LLM-based chat bot with it. Assuming access to querying this chat bot, can we verify with statistical significance if the proprietary system prompt has not been used? In other words, we develop a method for system prompt membership inference. Our contributions are as follows:

* We develop Prompt Detective, a training-free statistical method to reliably verify whether a given system prompt was used by a third-party language model, assuming query access to it.
* We extensively evaluate the effectiveness of Prompt Detective across a variety of language models, including Llama, Mistral, Claude, and GPT families in both standard and challenging scenarios such as hard examples of similar system prompts and black-box settings.
* Our work reveals that even minor changes in system prompts manifest in distinct response distributions of LLMs, enabling Prompt Detective to verify prompt usage with statistical significance. This highlights that LLMs take specific trajectories when generating responses based on the provided system prompt.

We refer to Appendix A for related work.

## 2 Prompt Detective

### Setup

Prompt Detective aims to verify whether a particular known system prompt is used by a third-party chat bot as shown in Figure 1. In our setup, we assume an API or online chat access to the model, that is, we can query the chat bot with different task prompts and we have control over choosing these task prompts. We also assume the knowledge about which model is employed by the service in most of our experiments, and we explore the black-box scenario in section 5.

This setup can be applied when a user, who may have spent significant effort developing the system prompt for their product such as an LLM character or a domain-specific application, suspects that their proprietary system prompt has been utilized by a third-party chat service effectively replicating the behavior of their product. We note that prompt engineering is a much less resource-intensive task than developing or fine-tuning a custom language model, therefore, it is reasonable to assume that such chat bots which reuse system prompts are based on one of the publicly available language models such as API-based GPT models (Achiam et al., 2023), Claude models (Anthropic, 2024a), or open source models like Llama or Mistral (Touvron et al., 2023; Jiang et al., 2023).

Moreover, this adversarial setup can be seen through the lens of membership inference attacks, where instead of verifying membership of a given data sample in the training data of a language model, we verify membership of a particular system prompt in the context window of a language model. We therefore refer to our adversarial setting as _prompt membership inference_.

### How does it work?

We assume that a third-party generative language model \(f_{p}\) is prompted with an unknown system prompt \(p\), and that we can query the service with task prompts \(q\) to get generations \(f_{p}(q)\). We also assume access to a similar model prompted with our known proprietary system prompt \(\), that is \(\). Our goal is to determine whether \(p\) and \(\) are distinct.

Core ideaPrompt Detective is a training-free statistical method designed for this purpose. The core idea is to compare the distributions of two groups of generations corresponding to different system prompts and apply a statistical test to assess if the distributions are significantly different, which would indicate that the system prompts are distinct. That is, Prompt Detective compares the distributions of high-dimensional vector representations of generations \(f_{p}(q_{1})^{1},...,f_{p}(q_{1})^{k},,f_{p}(q_{n})^{1},...,f_{p}(q_{n}) ^{k}\) obtained from the third-party service \(f_{p}\) prompted with task queries \(q_{1},...,q_{n}\) (with \(k\) responses sampled for each task query) and generations \(_{}(q_{1})^{1},...,_{}(q_{1})^{k},,_{ }(q_{n})^{1},...,_{}(q_{n})^{k}\) from the \(\) model prompted with the proprietary prompt \(\) and the same task queries.

Text representationsWe simply utilized BERT (Reimers and Gurevych, 2019) embeddings in our experiments. We compute the BERT embeddings for both \(f_{p}(q_{1})^{1},...,f_{p}(q_{1})^{k},,f_{p}(q_{n})^{1},...,f_{p}(q_{n}) ^{k}\) and \(_{}(q_{1})^{1},...,_{}(q_{1})^{k},,_{ }(q_{n})^{1},...,_{}(q_{n})^{k}\)yielding two groups of high-dimensional vector representations of generations corresponding to the two system prompts under comparison.

Statistical test of the equality of representation distributionsTo compare the distributions of these two groups, we employ a permutation test (Good, 2013) with the cosine similarity between the mean vectors of the groups used as the test statistic. The permutation test is a non-parametric approach that does not make assumptions about the underlying distribution of the data, making it a suitable choice for Prompt Detective. Intuitively, the permutation test assesses whether the observed difference between the two groups of generations is significantly larger than what would be expected by chance if the generations were not influenced by the underlying system prompts. By randomly permuting the responses within each task prompt across the two groups, the test generates a null distribution of cosine similarities between their mean vectors under the assumption that the system prompts are identical, while preserving the task prompt structure. The observed cosine similarity is then compared against this null distribution to determine its statistical significance. Algorithm 1 outlines all of the steps of Prompt Detective in detail.

### Task queries

The selection of task prompts \(q_{1},,q_{n}\) is an important component of Prompt Detective, as these prompts serve as probes to elicit responses that are influenced by the underlying system prompt. Since we assume control over the task prompts provided to the third-party chat bot, we can strategically choose them to reveal differences in the response distributions induced by distinct system prompts.

We consider a task prompt a good probe for a given system prompt if it elicits responses that are directly influenced by and related to the system prompt. For example, if the system prompt is designed for a particular LLM persona or role, task prompts that encourage the model to express its personality, opinions, or decision-making processes would be effective probes. A diverse set of task prompts can be employed to increase the robustness of Prompt Detective. In practice, we generated task queries for each of the system prompts \(\) in our experiments with the Claude 3 Sonnet (Anthropic, 2024a) language model unless otherwise noted (see Appendix F).

## 3 Experimental Setup

### System prompt sources

Awesome-ChatGPT-Prompts 2 is a curated collection of 153 system prompts that enable users to tailor LLMs for specific roles. This dataset includes prompts for creative writing, programming, productivity, etc. Prompts are designed for various functions, such as acting as a Startup Idea

Figure 2: **Hard Examples illustrate varying degrees of similarity between the original prompts and their rephrased versions. Similarity Level 1 is highly similar, while Level 5 is completely different.**

Generator, Python Interpreter, or Personal Chef. The accompanying task prompts were generated with Claude 3 Sonnet (see Appendix F). For the 153 system prompts in Awesome-ChatGPT, we generated overall 50 task prompts. In these experiments, while a given task prompt is not necessarily a good probe for every system prompt, these 50 task prompts include at least one good probe for each of the system prompts.

**Anthropic's Prompt Library**3 provides detailed prompts that guide models into specific characters and use cases. For our experiments, we select all of the personal prompts from the library that include system prompts giving us 20 examples. Personal prompts include roles such as Dream Interpreter or Emoji Encoder. As the accompanying task prompts, we used 20 of the corresponding user prompts provided in the library.

**Hard Examples:** To evaluate the robustness of Prompt Detective in challenging scenarios, we create a set of hard examples by generating variations of prompts from Anthropic's Prompt Library. These variations are designed to have different levels of similarity to the original prompts, ranging from minimal rephrasing to significant conceptual changes, producing varying levels of difficulty for distinguishing them from the original prompts.

For each system prompt from Anthropic's Prompt Library, we generate five variations with the following similarity levels (see Figure 2 for examples):

1. **Same Prompt, Minimal Rephrasing**: The same prompt, slightly rephrased with minor changes in a few words.
2. **Same Prompt, Minor Rephrasing**: Very similar in spirit, but somewhat rephrased.
3. **Same Prompt, Significant Rephrasing**: Very similar in spirit, but significantly rephrased.
4. **Different Prompt, Remote Similarities**: A different prompt for the same role with some remote similarities to the original prompt.
5. **Different Prompt, Significant Conceptual Changes**: A completely different prompt for the same role with significant conceptual changes.

This process results in a total of 120 system prompts for hard examples. The system prompt variations and the accompanying task prompts were generated with the Claude 3 Sonnet model. For the hard example experiments, we generated 10 specific probe task queries per each of the original system prompts (see Appendices B,F).

### Models

We conduct our experiments with a variety of open-source and API-based models, including Llama2 13B (Touvron et al., 2023), Llama3 70B 4, Mistral 7B (Jiang et al., 2023), Mistral 8x7B (Jiang et al., 2024), Claude 3 Haiku (Anthropic, 2024a), and GPT-3.5 (Achiam et al., 2023).

### Evaluation: standard and hard examples

In the standard setup, to evaluate Prompt Detective, we construct pairs of system prompts representing two scenarios: (1) where the known system prompt \(\) is indeed used by the language model (positive case), and (2) where the known system prompt \(\) differs from the system prompt \(p\) used by the model (negative case). The positive case simulates a situation where the proprietary prompt has been reused, while the negative case represents no prompt reuse.

We construct a positive pair \((,)\) for each of the system prompts and randomly sample the same number of negative pairs \((,p), p\). The negative pairs may not represent similar system prompts, and we refer to this setting as the standard setup.

For the hard example setup, we construct prompt pairs using the variations of the Anthropic Prompt Library prompts with different levels of similarity, as described in section 3.1. The first prompt in each pair is the original prompt from the library, while the second prompt is one of the five variations, ranging from minimal rephrasing to significant conceptual changes. That is, while in this setup there are no positive pairs using identical prompts, some of the pairs represent extremely similar prompts differing by only very few words replaced with synonyms.

## 4 Results

### Prompt Detective can distinguish system prompts

Table 1 shows the effectiveness of Prompt Detective in distinguishing between system prompts in the standard setup across different models and prompt sources. We report the false positive rate (FPR) and false negative rate (FNR) at a standard \(p\)-value threshold of 0.05, along with the average \(p\)-value for both positive and negative prompt pairs. In all models except for Claude on AwesomeChatGPT dataset, Prompt Detective consistently achieves a zero false positive rate, and the false negative rate remains approximately 0.05. This rate corresponds to the selected significance level, indicating the probability of Type I error - rejecting the null hypothesis that system prompts are identical when they are indeed the same. Figure 3 (a) shows how the average \(p\)-value changes in negative cases (where the prompts differ) as the number of task queries increases. As expected, the \(p\)-value decreases with more queries, providing stronger evidence for rejecting the null hypothesis of equal distributions. Consequently, increasing the number of queries further improves the statistical test's power, allowing for the use of lower significance levels and thus ensuring a reduced false negative rate, while maintaining a low false positive rate.

### Hard examples: similar system prompts

Table 2 presents the results for the challenging hard example setup, where we evaluate Prompt Detective's performance on system prompts with varying degrees of similarity to the proprietary prompt. We conduct this experiment with Claude 3 Haiku and GPT-3.5 models, testing Prompt Detective in two scenarios. First, we use 2 generations per task prompt, resulting in 20 generations

    &  &  \\   & FPR & FNR & \(p_{avg}^{p}\) & \(p_{avg}^{n}\) & FPR & FNR & \(p_{avg}^{p}\) & \(p_{avg}^{n}\) \\  Llama2 13B & 0.000 & 0.052 & 0.491\({}_{ 0.28}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.100 & 0.483\({}_{ 0.30}\) & 0.000\({}_{ 0.00}\) \\ Llama3 70B & 0.000 & 0.072 & 0.484\({}_{ 0.29}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & 0.508\({}_{ 0.29}\) & 0.000\({}_{ 0.00}\) \\ Mistral 7B & 0.000 & 0.039 & 0.503\({}_{ 0.29}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & 0.501\({}_{ 0.33}\) & 0.000\({}_{ 0.00}\) \\ Mistral 8X7B & 0.000 & 0.026 & 0.475\({}_{ 0.30}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & 0.466\({}_{ 0.30}\) & 0.000\({}_{ 0.00}\) \\ Claude Haiku & 0.052 & 0.026 & 0.543\({}_{ 0.29}\) & 0.021\({}_{ 0.11}\) & 0.000 & 0.050 & 0.440\({}_{ 0.28}\) & 0.000\({}_{ 0.00}\) \\ GPT-3.5 & 0.000 & 0.059 & 0.501\({}_{ 0.28}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & 0.396\({}_{ 0.26}\) & 0.000\({}_{ 0.00}\) \\   

Table 1: **Prompt Detective** can reliably detect when system prompt used to produce generations is different from the given proprietary system prompt. We report false positive and false negative rates at a standard 0.05 \(p\)-value threshold. Additionaly, we report average \(p\)-value for positive and negative system prompt pairs.

   Model & Similarity 1 & Similarity 2 & Similarity 3 & Similarity 4 & Similarity 5 \\   & \(p_{avg}\) & FPR & \(p_{avg}\) & FPR & \(p_{avg}\) & FPR & \(p_{avg}\) & FPR & \(p_{avg}\) & FPR \\  Claude\({}_{2}\) & \(0.194_{ 0.22}\) & \(0.65\) & \(0.108_{ 0.19}\) & 0.35 & \(0.093_{ 0.25}\) & \(0.15\) & \(0.052_{ 0.18}\) & 0.10 & \(0.052_{ 0.13}\) & 0.20 \\ Claude\({}_{50}\) & \(0.007_{ 0.03}\) & \(0.05\) & \(0.000_{ 0.00}\) & 0.00 & \(0.000_{ 0.00}\) & 0.00 & \(0.000_{ 0.00}\) & 0.00 & \(0.000_{ 0.00}\) & 0.00 \\  GPT-3.5\({}_{2}\) & \(0.213_{ 0.25}\) & \(0.65\) & \(0.306_{ 0.34}\) & 0for each system prompt, as in the standard setup Anthropic Library experiments. Second, we use 50 generations for each task query, resulting in 500 generations per system prompt in total. We observe that when only 2 generations are used, the false positive rate is high reaching 65% for GPT 3.5 and Claude models in Similarity Level 1 setup, indicating the challenge of distinguishing the response distributions for two very similar system prompts. However, increasing the number of generations for each probe to 50 leads to Prompt Detective being able to almost perfectly separate between system prompts even in the highest similarity category.

We further explore the effect of including more generations and more task prompts on Prompt Detective's performance. In Figure 3 (b), we display the average \(p\)-value for Prompt Detective on Similarity Level 1 pairs versus the number of generations, the number of task prompts, and the number of tokens in the generations. We ask the following question: for a fixed budget in terms of the total number of tokens generated, is it more beneficial to include more different task prompts, more generations per task prompt, or longer responses from the model? Our observations suggest that while having more task prompts is comparable to having more generations per task prompt, it is important to have at least a few different task prompts for improved robustness of the method. However, having particularly long generations exceeding 64 tokens is not as useful, indicating that the optimal setup includes generating shorter responses to more task prompts and including more generations per task prompt.

We additionally find that Prompt Detective successfully distinguishes prompts in two case studies of special interest: (1) variations of the generic _"You are a helpful and harmless AI assistant"_ common in chat applications, and (2) system prompts that differ only by a typo as an example of extreme similarity (see Appendix D for details).

Figure 3: **(a) Average \(p\)-value computed for different number of task queries. Left: Awesome-ChatGPT-Prompts. Right: Anthropic Library. (b) Effect of the number of task prompts, generations, and tokens on the performance of Prompt Detective. Left: average \(p\)-value vs. the number of generations. The blue line corresponds to 10 task prompts and 5 to 50 generations per task prompt. The red line corresponds to 1 to 10 task prompts with 50 generations for each. Right: average \(p\)-value against the total number of tokens generated with green line additionally corresponding to 10 task prompts with 50 generations of 16 to 512 tokens.**

   Model &  &  \\   & FPR & FNR & \(p_{avg}^{p}\) & \(p_{avg}^{n}\) & FPR & FNR & \(p_{avg}^{p}\) & \(p_{avg}^{n}\) \\  Llama2 13B & 0.000 & 0.013 & 0.493\({}_{ 0.28}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.050 & \(0.484\)\({}_{ 0.30}\) & 0.000\({}_{ 0.00}\) \\ Llama3 70B & 0.007 & 0.020 & 0.485\({}_{ 0.29}\) & 0.001\({}_{ 0.02}\) & 0.000 & 0.000 & \(0.517\)\({}_{ 0.28}\) & 0.000\({}_{ 0.00}\) \\ Mistral 7B & 0.000 & 0.000 & 0.504\({}_{ 0.29}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & \(0.582\)\({}_{ 0.34}\) & 0.000\({}_{ 0.00}\) \\ Mistral 8x7B & 0.000 & 0.013 & 0.476\({}_{ 0.30}\) & 0.000\({}_{ 0.00}\) & 0.000 & 0.000 & \(0.467\)\({}_{ 0.29}\) & 0.000\({}_{ 0.00}\) \\ Claude Haiku & 0.105 & 0.000 & 0.545\({}_{ 0.29}\) & 0.017\({}_{ 0.08}\) & 0.000 & 0.000 & \(0.420\)\({}_{ 0.34}\) & 0.000\({}_{ 0.00}\) \\ GPT-3.5 & 0.020 & 0.013 & 0.505\({}_{ 0.28}\) & 0.001\({}_{ 0.01}\) & 0.000 & 0.000 & \(0.396\)\({}_{ 0.26}\) & 0.000\({}_{ 0.00}\) \\   

Table 3: **Prompt Detective in Black Box Setup. Assuming the third-party model \(f_{p}\) is one of the six models from previous experiments, we use Prompt Detective to compare it against each of the six reference models \(\{_{p}^{i}\}_{i=1}^{6}\).**

## 5 Black Box Setup

So far we assumed the knowledge of the third-party model used to produce generations, and in this section we explore the black-box setup where the exact model is unknown. As mentioned previously, it is reasonable to assume that chat bots which reuse system prompts likely rely on one of the widely used language model families. To simulate such scenario, we now say that all the information Prompt Detective has is that the third party model \(f_{p}\) is one of the six models used in our previous experiments. We then compare the generations of \(f_{p}\) against each model \(\{_{p}^{i}\}_{i=1}^{6}\) used as reference and take the maximum \(p\)-value. Because of the multiple-comparison problem in this setup, we apply the Bonferroni correction to the \(p\)-value threshold to maintain the overall significance level of \(0.05\). Table 3 displays the results for Prompt Detective in the black-box setup. We observe that, while false positive rates are slightly higher compared to the standard setup, Prompt Detective maintains its effectiveness, which demonstrates its applicability in realistic scenarios where the adversary's model is not known.

## 6 Discussion

We introduce Prompt Detective, a method for verifying with statistical significance whether a given system prompt was used by a language model and we demonstrate its effectiveness in experiments across various models and setups.

The robustness of Prompt Detective is highlighted by its performance on hard examples of highly similar system prompts and even prompts that differ only by a typo. The number of task queries and their strategic selection play a crucial role in achieving statistical significance, and in practice we find that generally 300 responses are enough to separate prompts of the highest similarity. Interestingly, we find that for a fixed budget of generated tokens having a larger number of shorter responses is most useful for effective separation.

A key finding of our work is that even minor changes in system prompts manifest in distinct response distributions, suggesting that large language models take distinct low-dimensional "role trajectories" even though the content may be similar and indistinguishable by eye when generating responses based on similar system prompts. This phenomenon is visualized in Appendix Figure 4, where generations from even quite similar prompts tend to cluster separately in a low-dimensional embedding space.