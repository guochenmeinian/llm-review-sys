# Fast Rates in Stochastic Online Convex Optimization

by Exploiting the Curvature of Feasible Sets

 Taira Tsuchiya

The University of Tokyo and RIKEN

tsuchiya@mist.i.u-tokyo.ac.jp

&Shinji Ito

The University of Tokyo and RIKEN

shinji@mist.i.u-tokyo.ac.jp

###### Abstract

In this work, we explore online convex optimization (OCO) and introduce a new condition and analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions exceeds a certain threshold, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This study reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. In particular, we first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret bound of \(O( T)\) in stochastic environments. Here, \(>0\) is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. Additionally, the algorithm achieves an \(O()\) regret even in adversarial environments, in which FTL suffers an \((T)\) regret, and achieves an \(O( T+)\) regret in corrupted stochastic environments with corruption level \(C\). Furthermore, by extending our analysis, we establish a matching regret upper bound of \(OT^{}( T)^{}\) for \(q\)-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and \(_{p}\)-balls for \(p[2,)\). This bound bridges the gap between the \(O( T)\) bound for strongly convex sets (\(q=2\)) and the \(O()\) bound for non-curved sets (\(q\)).

## 1 Introduction

This paper considers online convex optimization (OCO), a framework in which a learner and an environment interact in a sequential manner. At the beginning, a convex body (or feasible set) \(K^{d}\) is given. At each round \(t=1,,T\), the learner selects a decision \(x_{t} K\) from the convex body \(K\) using information obtained up to round \(t-1\). Then, the environment determines a convex loss function \(f_{t} K\), and the learner suffers loss \(f_{t}(x_{t})\) and observes \( f_{t}(x_{t})^{d}\). The goal of the learner is to minimize the regret, which is the expectation of the difference between the cumulative loss of decisions \((x_{t})_{t=1}^{T}\) and that of a single optimal decision \(x_{}\) fixed in hindsight, that is, \(_{T}=_{t=1}^{T}(f_{t}(x_{t})-f_{t}(x_{}) )\) for \(x_{}=_{x K}_{t=1}^{T}f_{t}(x)\). OCO is called online linear optimization (OLO) when \((f_{t})_{t}\) are linear functions, _i.e.,_\(f_{t}()= g_{t},\) for some \(g_{t}^{d}\).

In OCO and OLO, the well-known online gradient descent (OGD) achieves an \(O()\) regret upper bound for Lipschitz continuous \(f_{t}\). In general, this upper bound cannot be improved and is known to match the \(()\) regret lower bound . However, this lower bound can be circumvented under certain conditions. The most typical way is to exploit the curvature of loss functions. It is knownthat OGD with a learning rate of \((1/t)\) and online Newton step (ONS) can achieve an \(O( T)\) and \(O( T)\) regret for \(\)-strongly-convex and \(\)-exp-concave loss functions, respectively .

Another way to circumvent the lower bound is to harness _the curvature of the feasible set \(K\)_. Existing studies proved that in OLO if the feasible set is curved and loss vectors \(g_{t}\) are biased towards a specific direction, the follow-the-leader (FTL) algorithm can achieve a logarithmic regret. In particular, Huang et al.  first proved that under the _growth condition_ that there exists \(L>0\) such that \(\|g_{1}++g_{t}\|_{2} tL\) for any \(t[T]=\{1,,T\}\), FTL achieves an \(O(}{ L} T)\) regret for \(\)-strongly convex \(K\) and \(G\)-Lipschitz loss functions. This bound matches their lower bound of \(( T)\). Molinaro  also proves that FTL can achieve a logarithmic regret under the different assumption on the loss vectors that \(g_{t} 0\) for all \(t[T]\), providing an intuitive and simple proof.

Their approach, however, has several remaining limitations. First, they only consider OLO. While the linearization technique allows us to solve OCO by OLO, this may prevent us from leveraging the curvature of loss functions. Second, their analysis requires the curvature over the entire boundary of the feasible set, which is a rather limited condition. Finally, some of their approach suffers an \((T)\) regret if the ideal conditions on loss vectors, such as the growth condition, are not satisfied. Note that we cannot know in advance whether such conditions are satisfied or not. Exceptions are the method based on the expert tracking algorithm in [9, Section 4], in which FTL is combined with follow-the-regularized-leader, and the work by Anderson and Leith , who investigated the online lazy gradient descent over the strongly convex sets.

To overcome these limitations, we consider using algorithms adaptive to the curvature of loss functions [21; 22; 24], also known as _universal online learning_. The original motivation of this line of

  Reference & Feasible set & Loss functions & Regret bound \\ 
, **This work** (Thm 8) & ellipsoid \(W_{}\) &  \(f_{t}()= h_{t}^{L},\) \\ in Sec- \\ tion 2.3.2 \\ (\(\)-strongly convex) \\  & 
 \( T\), \((x_{})\|_{2}} T\) \\ corrupted \\ \((x_{})\|_{2}}+ (x_{})\|_{2}}}\) \\ corrupted \\ \( T\) \\  \\
**This work** (Cor 12) & &  \(f_{t}()= h_{t}^{L},\) \\ in Thm 6 \\  & 
 \(O T\) \\  \\ 
**This work** (Thms 10, 14) &  \((,x_{},f^{})\)- \\ sphere-enc. \\  &  stochastic, \\ convex \\ corrupted, \\ convex \\  &  \(O}{\| f^{}(x_{})\|_{2}} T\) \\ \(O}{\|^{}(_{})\|_{2 }} T+}{\|^{}(_{ })\|_{2}} T}\) \\  \\  Huang et al.  &  \(\)-strongly \\ convex \\  &  adversarial, \\ linear \\ adversarial, \\ linear \\  & 
 \(O}{ L} T\) \\ \(O}{} T\) \\ \(O}{} T\) \\ \(O}{\| f^{}(x_{})\|_{}} T \) \\  \\
**This work** (Thm 15) & &  \(\)-strongly \\ convex \\  &  adversarial, \\ linear \\  &  \(O}{ L} T\) \\ \(O}{} T\) \\ \(O}{} T\) \\ \(O}{\| f^{}(x_{})\|_{}}  T\) \\  \\  Kerdreux et al.  &  \((,q)\)- \\ uniformly \\ convex \\  &  adversarial, \\ linear \\ stochastic, \\ convex \\  & 
 \(O}}{ L^{}}T^{ }\) \\ \(O}{\| f^{}(x_{})\|_{ }}^{}T^{}( T)^{}\) \\  \\
**This work** (Thm 15) & &  adversarial, \\ linear \\ stochastic, \\ convex \\  & 
 \(O}}{ L^{}}T^{ }\) \\ \(O}{\| f^{}(x_{})\|_{ }}^{}T^{}( T)^{}\) \\  \\  

Table 1: Comparison of our regret upper bounds with existing bounds. All bounds assume that loss functions are \(G\)-Lipschitz (except Lines 1â€“3) and \(x_{}\) is on the boundary of \(K\). The upper bounds that contain the variable \(L>0\) assume \(\|g_{1}++g_{t}\|_{2} tL\) for all \(t[T]\). We use \(f^{}=_{f}[f]\), \(C 0\) is the corruption level, and the \(\) notation ignores logarithmic factors. The \((,2)\)-uniformly convex set is \(\)-strongly convex. Theorem is abbreviated as as Thm, Corollary as Cor, and sphere-enclosed as sphere-enc. Note that regret bounds proven in this study can be simultaneously achieved by the same algorithm with identical parameters.

work is to automatically achieve a regret bound that depends on the true curvature level of loss functions, _e.g.,_ parameters of strong convexity or exp-concavity, without knowing them. The crux of their analysis is to derive a bound of \(_{t=1}^{T} f_{t}(x_{t}),x_{t}-x_{}=O^{T} x_{t}-x_{}_{2}^{2} T}\).

Contributions of this paperWe introduce a new condition for achieving fast rates in OCO. We first show that algorithms adaptive to the curvature of loss functions can exploit the curvature of feasible sets and overcome the three limitations mentioned earlier. We prove the following theorem:

**Theorem 1** (informal version of Theorems 10 and 13).: _Any algorithm with \(_{t=1}^{T} f_{t}(x_{t}),x_{t}-x_{}=Oc_{ }^{T} x_{t}-x_{}_{2}^{2} T} \) for some \(c_{}>0\) achieves \(_{T}=O}^{2}}{ f^{}( x_{})_{2}} T\) in stochastic environments, where \(f^{}=_{f_{t}}[f_{t}]\) and \(>0\) is the smallest radius of a sphere that includes \(x_{}\) and encloses \(K\). The same algorithm achieves \(_{T}=O( T+)\) in corrupted stochastic environments for corruption level \(C\) and \(_{T}=O()\) in adversarial environments._

This upper bound matches an existing lower bound (9, Theorem 9), specifically when considering the environment employed to construct their lower bound. This will be formally stated in Corollary 12.

The advantage of our approach over the existing approach is that it overcomes all three limitations of the existing approach mentioned earlier. That is, (i) in contrast to existing studies, it can work with OCO without the linearization, allowing us to simultaneously exploit the curvature of feasible sets and the curvature of loss functions (see Theorem 14). (ii) Even in worst cases, where the specific conditions on loss vectors, such as the growth condition, are not satisfied, an \(O()\) regret upper bound can be achieved. (iii) The local structure of \(K\) around optimal decision \(x_{}\) is sufficient for our approach to achieve the logarithmic regret. As a further advantage, our approach can achieve an \(O( T+)\) regret bound for corrupted stochastic environments with corruption level \(C 0\), which are intermediate environments between stochastic and adversarial environments (Theorem 13). We provide a regret lower bound that nearly matches this upper bound (Theorem 9).

Our approach can also be used to obtain fast rates on _uniformly convex_ feasible sets, a broader class that includes _strongly convex_ sets and \(_{p}\)-balls for \(p[2,)\). For \(q\)-uniformly convex \(K\), Kerdreux et al.  proves an regret bound of \(OT^{}\), which is smaller than \(O()\) only when \(q(2,3)\). We improve this bound by proving the following upper bound, which matches the lower bound in :

**Theorem 2** (informal version of Theorem 15).: _In online convex optimization with \(q\)-uniformly convex feasible set \(K\), the same algorithm as Theorem 1 achieves \(_{T}=OT^{}( T)^{}\)._

This becomes a fast rate for any \(q>2\) and is strictly better than the bound in . Our bound interpolates between the \(O( T)\) bound for strongly convex sets (when \(q=2\)) and the \(O()\) bound for non-curved feasible sets (when \(q\)). Table 1 summarizes the regret comparison.

## 2 Preliminaries

Let \(e_{i}\{0,1\}^{d}\) be the \(i\)-th standard basis of \(^{d}\), and \(\) be the all-one vector. For \(p[1,]\) and vector \(x\), let \( x_{p}\) be \(_{p}\)-norm. Let \(>0\) be a constant satisfying \( x_{2} x\) for any \(x^{d}\). For a norm \(\), we use \( x_{}=\{ x,y y 1\}\) to denote its dual norm. Let \(_{}(x,r)\) be a ball with radius \(r\) centered at \(x\) associated with \(\), _i.e.,_\(_{}(x,r)=\{z z-x r\}\). We use \((x,r)\) to denote the Euclidean ball with radius \(r\) centered at \(x\) and \(_{}\) to denote the unit ball. Let \((K)\) be the boundary of \(K\). A function \(f^{d}(-,]\) is convex if for all \(xf\), \(f(y) f(x)+ f(x),y-x\) for all \(y^{d}\).1 For \(>0\), \(f K(-,]\) is \(\)-strongly convex over \(Kf\) w.r.t. \(\) if for all \(x,y K\), \(f(y) f(x)+ f(x),y-x+ x-y^{2}\). For \(>0\), \(f K(-,]\) is \(\)-exp-concave if \((- f(x))\) is concave.

### Online convex optimization

We consider online convex optimization (OCO). In OCO, a convex body (or feasible set) \(K^{d}\) is given before the game starts. Let \(D=_{x,y K} x-y_{2}\) be the diameter of \(K\). At each round \(t[T],\) the learner selects a decision \(x_{t} K\) using information obtained up to round \(t-1,\) and a convex loss function \(f_{t} K\) is determined by the environment. The learner then suffers a loss \(f_{t}(x_{t})\) and observes \( f_{t}(x_{t})^{d}\). The goal of the learner is to minimize the regret, which is defined as \(_{T}=_{t=1}^{T}(f_{t}(x_{t})-f_{t}(x_{})) \) for the optimal decision \(x_{}=*{arg\,min}_{x K}_{t=1}^{T}f_{ t}(x)\). When loss functions are restricted solely to linear functions, that is, when \(f_{t}()= g_{t},\) for some \(g_{t}^{d}\), OCO is referred to as online linear optimization (OLO).

### Assumptions on loss functions

In this study, we assume that \(f_{t}\) is \(G\)-Lipschitz, _i.e.,_\(_{x K} f_{t}(x)_{2} G\). In the following, we list three assumptions on how a sequence of \(f_{1},,f_{T}\) is generated. In stochastic environments, \(f_{t}\) is sampled in an i.i.d. manner from a certain probability distribution \(\). The expectation of \(f_{t}\) is denoted as \(f^{}=_{f}[f]\). In adversarial environments, \(f_{t}\) is arbitrarily determined depending on the past history, and \(f_{t}\) may depend on \(x_{t}\). The corrupted stochastic environment is an intermediate setting between stochastic and adversarial environments. The motivation for considering this environment is that in real-world problems, a sequence of loss functions is neither stochastic nor (fully) adversarial. In this environment, at each round \(t[T],_{t}}\) is obtained according to a certain distribution \(}\), where the expectation of \(_{t}\) is defined by \(^{}=_{}}[]\). Then, possibly depending on \(_{t}\) and the past history, loss function \(f_{t}\) is determined by the environment so that \(_{t=1}^{T} f_{t}-_{t}_{}  C,\) where \(_{t=1}^{T} f_{t}-_{t}_{}\) is the corruption level. In this paper, we consider these three environments.

### Exploiting the curvature of feasible sets

We start by introducing the definition of strongly and uniformly convex sets. We then define a new notion of convex bodies, _sphere-enclosed set_, for which we can also achieve the fast rates of \(O( T)\). We finally discuss the existing lower bound when exploiting the curvature.

#### 2.3.1 Strong convexity and sphere-enclosedness

One common way to describe the curvature of a convex body is with the following strong convexity.

**Definition 3**.: A convex body \(K\) is _\(\)-strongly convex w.r.t. a norm \(\)_ if for any \(x,y K\) and any \(\), it holds that \( x+(1-)y+(1-) x-y^{2} _{} K\,\).

For example, \(_{p}\)-balls for \(p\) are \((p-1)/2\)-strongly convex w.r.t. \(_{p}\)[7, Theorem 2], and another various examples of strongly convex sets can be found in [6, Section 5]. A more general notion of the curvature is by the following uniform convexity:

**Definition 4**.: A convex body \(K\) is _\((,q)\)-uniformly convex w.r.t. a norm \(\) (or \(q\)-uniformly convex)_ if for any \(x,y K\) and any \(\), it holds that \( x+(1-)y+(1-) x-y^{q}_ {} K\,\).

For example, \(_{p}\)-balls for \(p 2\) are \((1/p,p)\)-uniformly convex w.r.t. \(_{p}\)[7, Theorem 2], and \(p\)-Schatten balls are \((1/p,p)\)-uniformly convex w.r.t. the Schatten norm \(_{S(p)}\) (See  and Appendix H for the connection between the uniform convexity of a normed space and the uniform convexity of sets.) Note that \((,2)\)-uniformly convex sets are \(\)-strongly convex.

In this paper, we introduce a new, different characterization of convex bodies.

**Definition 5** (sphere-enclosed sets).: Let \(K^{d}\) be a convex body, \(u(K)\), and \(f K\). Then, \(K\) is _\((,u,f)\)-sphere-enclosed_ (or simply sphere-enclosed facing \(u\)) if there exists a sphere of radius \(\) that has \(u\) on it, encloses \(K\), and the gradient of \(f\) at point \(u\) is directed towards the center of the sphere. That is, there exists a ball \((c,)\) with \(c^{d}\) and

Figure 1: Examples of sphere-enclosed sets.

\(>0\) satisfying (i) \(u((c,))\), (ii) \(K(c,)\), and (iii) there exists \(r_{0}>0\) such that \(u+r_{0} f(u)=c\).2

One might think that the sphere-enclosed condition is complicated but Condition (iii) in Definition 5 is only for the case when \(x_{}\) is at the corner of \(K\). Figure 1 shows examples of sphere-enclosed sets. The area enclosed by the solid black lines is the convex body \(K\). In the left figure, we can see that \(K\) is sphere-enclosed facing \(x\) (the red dotted line is the minimum sphere facing \(x\)), but \(K\) is not sphere-enclosed facing \(y\). In the right figure, we can see that \(K\) is sphere-enclosed facing \(z\) (the blue dotted line is the minimum sphere facing \(z\) for \(K\)). Note that the notion of sphere-enclosedness is a local property defined for each point of the boundary of convex bodies, in contrast to the definition of strong convexity. In the next section, we will see that we can achieve a logarithmic regret if \(K\) is sphere-enclosed facing at optimal decision \(x_{}\).

#### 2.3.2 Existing lower bound

Here, we discuss a lower bound when exploiting the curvature of feasible sets. For \((0,1)\), let \(W_{}=\{(x,y)^{2} x^{2}+y^{2}/^{2} 1\}\) be an ellipsoid with principal curvature \(\). From [9, Proposition 4], ellipsoid \(W_{}\) is \(\)-strongly convex w.r.t. \(\|\|_{2}\). The following lower bound provided in [9, Theorem 9] is for this \(W_{}\), which matches the upper bound in [9, Theorem 5].

**Theorem 6**.: _Consider online linear optimization. Let \(,L(0,1)\) and \(K=W_{}\). Then, for any algorithm, there exists a sequence of linear loss functions \(f_{1},,f_{T}\) satisfying \(f_{t}()= g_{t},\), \(g_{1},,g_{T}\{(1,-L),(-1,-L)\}\), and the growth condition that \(\|g_{1}++g_{t}\|_{2} tL\) for all \(t[T]\) such that \(_{T}} T-\) for \(=(^{^{2}L^{2}}}+ }{108})\)._

In their proof, they use the following sequence of linear functions \(f_{t}()= h_{t}^{L},\). Let \(P\) be a random variable following a Beta distribution, \((k,k)\), for some \(k>0\). For this \(P\), let \((X_{t})_{t=1}^{T}\) be i.i.d. random variables following a Bernoulli distribution with parameter \(P\). Then for \(L(0,1)\), let \(h_{t}^{L}=(2X_{t}-1,-L)\), which indeed satisfies the growth condition \(\|h_{1}^{L}++h_{t}^{L}\|_{2} tL\) for all \(t[T]\). This construction of loss functions will be exploited to prove lower bounds in Section 3, and we will provide a matching upper bound in Corollary 12.

### Universal online learning

Our algorithm is based on the results of universal online learning. In the literature, the following regret upper bound is the crux for being adaptivity to the curvature of loss functions:

**Lemma 7**.: _Consider online convex optimization. Then, there exists an (efficient) algorithm such that \(_{t=1}^{T} f_{t}(x_{t}),x_{t}-x_{}\) is bounded from above by the order of_

\[\!\{c_{}^{T}\!\|x_{t}-x_{}\|_{2}^{2} \, T\!+\!c_{}^{} T\,,\,c_{}^ {T} f_{t}(x_{t}),x_{t}-x_{}^{2}\, T \!+\!c_{}^{} T\,,\,GD}}}\},\] (1)

_where \(c_{},c_{}^{},c_{},c_{}^{ },c_{}>0\) are algorithm dependent variables provided in the following.3_

For example, upper bound (1) can be achieved by the MetaGrad algorithm with \(c_{}=G\), \(c_{}^{}=d\), \(c_{}=\), \(c_{}^{}=d\), and \(c_{}= T\) and the Maler algorithm with \(c_{}=G\), \(c_{}^{}=GD\), \(c_{}=\), \(c_{}^{}=GD+d\), and \(c_{}=1\)[24, Theorem 1]. We will see that our regret bounds depend on \(c_{},c_{}^{},c_{},c_{}^{ },c_{}>0\), and one can use any algorithm with bound (1).

## 3 Regret lower bounds

In this section, we construct lower bounds that align with the assumptions of our regret bounds. Considering a sequence of loss functions to construct the lower bound in Theorem 6, we can immediately obtain the following lower bound.

**Theorem 8**.: _Consider online linear optimization. Let \(,L(0,1)\) and \(K=W_{}\). Then, for any algorithm, there exists a stochastic sequence of loss functions \(f_{1},,f_{T}\) satisfying \(f_{t}()= g_{t},,\)\(g_{1},,g_{T}\{(1,-L),(-1,-L)\}\), and \(\| f^{}(x_{})\|_{2}=L\) such that \(_{T}}(x_{}) \|_{2}} T-,\) where \(\) is defined in Theorem 6._

Proof.: Consider the sequence of loss vectors \(h_{1}^{L},,h_{T}^{L}\) after Theorem 6 and let \(f_{t}()= h_{t}^{L},\) for all \(t[T]\). For this sequence of \((f_{t})_{t=1}^{T}\), it holds that \(\| f^{}(x_{t})\|_{2}=\|h_{t}^{L}\|_{2}=\|(0,-L)\|_{2}=L 0,\) which completes the proof. 

With this lower bound, we have the following lower bound for corrupted stochastic environments.

**Theorem 9**.: _Consider online linear optimization. Let \(,L(0,1)\) and \(K=W_{}\). Suppose that \(T C/( L)^{2}\) and \(C 1/( L)\). Then, for any algorithm, there exists a corrupted stochastic environment with corruption level at most \(C 0\) satisfying \(\| f^{}(x_{})\|_{2}=L\) such that_

\[_{T}}(x_{t})\|_{2}}+(x_{t})\|_{2}}} (x_{t})\|_{2}} )}-\,,\]

_where \(\) is defined in Theorem 6._

The assumption that \(T C/( L)^{2}\) makes some sense since the construction of this lower bound relies on Theorem 8, and if the assumption does not hold then the lower bound becomes vacuous.

Proof.: We will construct \((f_{t})_{t=1}^{T}\) in a corrupted stochastic environment, where \((f_{t})_{t}\) are generated so that \(_{t}()=_{t},\) with \(_{1},,_{T}\) following a distribution \(\) and \(f_{t}\) is a corrupted function of \(_{t}\).

We first note that we have \(T C/( L) 1/( L)^{2}\). Define \(>0\) such that \(=\). Note that since \(C 1/( L)\), we have \( L\), implying that \((0,1)\). We also define \(:= 1/()^{2}= C/( L) T\), which follows from \(=\) and \(T C/( L)\).

With these definitions, we then consider the following corrupted stochastic environments:

* For \(t\{1,,\}\), define \(_{t}\) by \(_{t}()=_{t},\) for \(_{t}=h_{t}^{L}\), where \(h_{t}^{L}\) is defined after Theorem 8, and define loss function \(f_{t}\) by \(f_{t}()= g_{t},\) with \(g_{t}=h_{t}^{L}\).
* For \(t\{+1,,T\}\), let \(_{t}()=f_{t}()= g_{t},\) with \(g_{t}=h_{t}^{L}\), where there is no corruption.

In fact, the corruption level of this environment is bounded by \(C\) since \(_{t=1}^{T}\|f_{t}-_{t}\|_{}=_{t= 1}^{}_{x K}_{t}-_{t},x |L-| C/( L )|L-| C,\) where in the first inequality we used the fact that the first elements of \(g_{t}\) and \(_{t}\) are the same and that \(K=W_{}\) and in the second inequality we used \(L>0\). This implies that the sequence of \((f_{t})_{t=1}^{T}\) is a corrupted stochastic environment with the corruption level at most \(C\).

Hence, from Theorem 8 with \((0,1)\), \(=\), and the definition of \(\), the regret is bounded from below as \(_{T}}}- }}\!(})-}}\!(})-\). Taking the average of the last two inequalities completes the proof. 

Note that our lower bounds are not for general sphere-enclosed feasible sets, and establishing a new lower bound is important future work.

## 4 Regret upper bounds

In this section, we provide regret upper bounds that nearly match the lower bounds in Section 3, by the universal online learning framework, whose regret is bounded as (1). Note that this section works with convex loss functions.
We provide logarithmic regret for stochastic environments. Define the ball \(B_{}^{K}^{d}\) for \(>0\) by

\[B_{}^{K}=x_{}+ f^{}(x_ {})\,,\,\| f^{}(x_{})\|_{2}\,.\]

By the definition, we have \(x_{}(B_{}^{K})\). See Figure 2.

**Remark 1**.: The ball \(B_{}^{K}\) is determined in the following manner. We will see in the following proof that the inequality \( f^{}(x_{}),x-x_{}\|x-x_{} \|_{2}^{2}\) that holds for some \(>0\) and any action \(x\) plays a key role in proving a logarithmic regret. This inequality is equivalent to \(\|x-x_{}+ f^{}(x_{})\|_{2} \| f^{}(x_{})\|_{2}\) and we define \(B_{}^{K}\) as the set of all \(x^{d}\) satisfying this inequality.

Using this \(B_{}^{K}\), we let \(_{}=\{ 0 K B_{}^{K}\}\). Then, we can prove the following theorem.

**Theorem 10**.: _Consider online convex optimization in stochastic environments, where the optimal decision is \(x_{}=*{arg\,min}_{x K}f^{}(x)\). Suppose that \(K\) is \((,x_{},f^{})\)-sphere-enclosed and that \( f^{}(x_{}) 0\). Then, any algorithm with the bound (1) achieves_

\[_{T}=O}^{2}}{_{}} T+c_{ }^{} T=O}^{2}\,}{ \| f^{}(x_{})\|_{2}} T+c_{}^{} T\,.\]

The assumption that \(K\) is sphere-enclosed around \(x_{}\) is satisfied for many typical feasible sets. It holds if the feasible set \(K\) is a ball, or a polytope with a mild condition on \( f^{}(x_{})\). Specifically, the condition \( f^{}(x_{})(-N_{K}(x_{}))\) is sufficient to ensure that the feasible \(K\) is sphere-enclosed around \(x_{}\), where \(-N_{K}(x_{})=\{g^{d} g,x-x_{} 0 \, x K\}\) is the _negative_ normal cone. This condition is mild since, from the optimality condition of \(x_{}\), we have \( f^{}(x_{})-N_{K}(x_{})\). Hence the undesirable case is restricted to \( f^{}(x_{})(-N_{K}(x_{}))\) (see Figure 3 for an example). One might think that the assumption that \(x^{*}\) is on the boundary of \(K\) is restrictive, but for example, when the loss functions are linear, the minimizer is indeed on the boundary of the feasible set. We will see in Corollary 12 that this upper bound matches the lower bound in Theorem 8 in the environment used to construct the lower bound.

Proof.: The regret is bounded from below by \(_{T}=_{t=1}^{T}(f^{}(x_{t})-f^{ }(x_{}))_{t=1}^{T} f^ {}(x_{}),x_{t}-x_{}_{t=1 }^{T}_{}\|x_{t}-x_{}\|_{2}^{2}\), where the first inequality follows from the convexity of \(f^{}\), and the last inequality follows from \(x_{t} K B_{_{}}^{K}\) and the definition of \(_{}\). By combining this inequality with inequality (1), the regret is bounded as \(_{T}_{t=1}^{T} f_{t}(x_{t}),x _{t}-x_{}=Oc_{}_{T} }{_{}} T}+c_{}^{} T\). Solving this inequation w.r.t. \(_{T}\), we get \(_{T}=O}^{2}}{_{}} T+c_{ }^{} T\). Observing that \(}\| f^{}(x_{})\|_{2}=\), which holds from the assumption that \(K\) is \((,x_{},f^{})\)-sphere-enclosed, we complete the proof. 

The advantages of the regret bound in Theorem 10 compared to the existing upper bounds are the following: (i) The logarithmic regret can be achieved as long as the boundary of \(K\) is curved around the optimal decision \(x_{}\) or \(x_{}\) is on corners (see Figure 1), while the existing analysis requires strong convexity over the entire feasible set \(K\). (ii) While the existing analysis only considers linear loss functions, our approach can handle convex loss functions and thus the curvature of loss functions (_e.g.,_ strong convexity or exp-concavity) can be simultaneously exploited (see Section 4.3). (iii) Even if the growth assumptions on loss vectors \(g_{1},,g_{T}\) are not satisfied, the \(O()\) regret upper bound can be achieved in adversarial environments, while the existing approach, FTL, can suffer \((T)\) regret.

Figure 3: An example of an undesirable direction of \( f^{}(x_{})\).

Figure 2: The region enclosed by the black solid line is a feasible set \(K\) and the red dotted line \(B_{_{}}^{K}\) is the smallest sphere enclosing \(K\) and facing \(x_{}\).

A limitation of the proposed approach is that it assumes stochastic environments. However, our approach at least guarantees an \(O()\) bound in the (fully) adversarial environments, where the growth assumption needed for FTL to achieve the fast rates is not satisfied, and as we will see in the following section, we can achieve the fast rates also in corrupted stochastic environments.

For the assumption on loss vectors, the existing studies consider the following assumptions on loss vectors \(g_{t}\): There exists \(L>0\) such that \(\|g_{1}++g_{t}\|_{2} tL\) for all \(t[T]\), or \(g_{t} 0\) for all \(t[T]\). These assumptions cannot be directly comparable with our assumption that \( f^{}(x_{}) 0\). Note that the assumption that \(_{x K} f^{}(x)>0\) is standard in the literature of offline optimization, when deriving the fast convergence rate, see  and discussion in  for details.

Extending the sphere-enclosed condition to an arbitrary norm is challenging because the sphere-enclosed condition leverages the fact that the enclosing ball is an Euclidean ball. However, we will see in Section 4.4 that fast rates for uniformly convex sets can be achieved for general norms (Theorem 15).

**Remark 2**.: The sphere-enclosed condition is similar to the Bernstein condition investigated by Koolen et al. . These conditions are different for general convex loss functions; however, when the loss functions are linear, the sphere-enclosed condition implies the Bernstein condition, allowing us to apply their analysis in this case. Thus, our research can also be viewed as identifying a new condition that satisfies the Bernstein condition. Still, our analysis is more general in the sense that we can deal with general convex loss functions. See Appendix I for a detailed comparison between the sphere-enclosed condition and the Bernstein condition.

Tightness of regret upper bound in Theorem 10In the remainder of this subsection, we investigate the tightness of the regret upper bound in Theorem 10. To see the tightness of our regret bound, we consider the case when \(K\) is an ellipsoid. The following proposition implies that the regret upper bound in Theorem 10 matches the lower bound in Theorem 8.

**Proposition 11**.: _For \((0,1)\), let \(W_{}\) be the ellipsoid defined in Section 2.3.2. Then, its minimum enclosing sphere \(S\) such that \((0,-) S\) is \(S=\{(x,y)^{2} x^{2}+[y-(1-^{2})/]^{2}=1/ ^{2}\}\)._

The proof of Proposition 11 is deferred to Appendix B. This result immediately implies the following:

**Corollary 12**.: _Let \(K\) be \(W_{}\) and \(x^{*}=(0,-)\). Under the same assumption as in Theorem 10, in the environment considered in the construction of the lower bound in Theorem 6, the algorithm in  achieves \(_{T}=O T+GD T\)._

This upper bound matches the lower bound in Theorem 8 up to the additive \(GD T\) factor.

Proof.: From Proposition 11 and the fact that Euclidean ball with radius \(r\) is \(1/r\) strongly convex w.r.t. \(_{2}\), we have \(=1/\). This proposition with Theorem 10 gives the desired bound. 

The upper bound in Theorem 10 is applicable when \(K\) is a polytope. We will see that our approach work also in the corrupted stochastic environment in Section 4.2, and to our knowledge, this is the first upper bound that achieves fast rates when the feasible set is a polytope in non-stochastic environments. A further discussion regarding the case when \(K\) is a polytope can be found in Appendix C.

### Regret bounds in corrupted stochastic environments

Another advantage of our approach is that it can achieve nearly optimal regret upper bounds even in corrupted stochastic environments. For \(>0\), we define ball \(_{}^{K}^{d}\) by \(_{}^{K}=_{}+^{}(_{})\,\,,\,^{}(_{}) _{2}\,,\) which is defined in the same manner as \(B_{}^{K}\). For this \(_{}^{K}\), we let \(_{}=\{ 0 K_{ }^{K}\}\). Then, we can prove the following regret upper bound.

**Theorem 13**.: _Consider online convex optimization in corrupted stochastic environments with corruption level at most \(C\), where \(_{}=_{x K}^{}(x)\). Suppose \(K\) is \((,_{},^{})\)-sphere enclosed and \(^{}(_{}) 0\). Then, any algorithm with the bound (1) achieves_

\[_{T}=O}^{2}}{_{}}  T+}^{2}}{_{}} T}+c_{ }^{} T=O}^{2}\,}{ ^{}(_{})}_{2} T+ }^{2}\,}{^{}( _{})}_{2} T}+c_{}^{} T\,.\]The proof of Theorem 13 can be found in Appendix D. One can see that this upper bound matches the lower bound in Theorem 9 up to logarithmic factors. Note that all upper bounds provided in this study can be extended following the same line as the proof of Theorem 13.

### Exploiting the curvature of loss functions and feasible set simultaneously

One of the advantages of directly solving OCO over reducing to OLO is that we can obtain upper bounds that can simultaneously exploit the curvature of feasible sets and loss functions:

**Theorem 14**.: _Suppose that the same assumption as in Theorem 10 holds. If \(f_{1},,f_{T}\) are \(\)-strongly convex w.r.t. a norm \(\), then \(_{T}=O}^{2}}{_{*}+/} T +c_{}^{} T\). If \(f_{1},,f_{T}\) are \(\)-exp-concave, then \(_{T}=O\{}^{2}}{_{*}},}^{2}}{^{}+_{*}/G^{2}}\} T+\{c_{}^{},c_{}^{}\} T\) for \(^{},}\)._

The proof can be found in Appendix E. Theorem 14 implies that one can simultaneously exploit the curvature of feasible sets and loss functions.

### Matching regret upper bound for uniformly convex sets

Here, we prove that a regret bound smaller than \(O()\) can be achieved when \(K\) is uniformly convex. This can be proven by a similar argument using the idea of exploiting the lower bound, as in the proof for sphere-enclosed sets. For uniformly convex feasible sets, we can prove the following theorem.

**Theorem 15**.: _Consider online convex optimization in stochastic environments, where the optimal decision is \(x_{}=*{arg\,min}_{x K}f^{}(x)\). Suppose that \(K\) is \((,q)\)-uniformly convex w.r.t. a norm \(\) for some \(q 2\) and that \( f^{}(x_{}) 0\). Then, any algorithm with bound (1) achieves_

\[_{T}=O})^{}}{(q  f^{}(x_{})_{})^{}}T^{}( T)^{}+c_{}^{} T\,.\]

_In particular, when \(K\) is \(\)-strongly convex w.r.t. \(\), \(_{T}=O}}{ f^{ }(x_{})_{}} T+c_{}^{} T\,.\)_

The dependence on \(T\) in this bound is \(OT^{}( T)^{}\), which becomes \(O( T)\) when \(q=2\) and \(()\) when \(q\), and thus interpolates between the bound over the strongly convex sets and non-curved feasible sets. This is strictly better than the \(OT^{}\) bound in ; their regret upper bound is better than \(O()\) only when \(q(2,3)\). Notably, our bound matches the existing lower bound of \(T^{}\) proven for a stochastic environment with \(d=2\)[2, Theorem C.1]. For example, when \(K\) is \(_{p}\)-ball, by \( x_{2} d^{-} x_{p}\) for any \(x^{d}\) and \(p>2\), the regret is bounded as \(_{T}=O})^{}^{}}{( f^{}(x_{})_{})^{}}T^{ }( T)^{}+c_{}^{} T \).

It is worth noting that the result of Theorem 15 corresponds to the result for sphere-enclosed sets when \(q=2\) and \(\) is the Euclidean norm. Additionally, Theorem 15 does not require the feasible set \(K\) to be globally "curved." In fact, the proof of Theorem 15 only uses the inequality \( f^{}(x_{}),x-x_{}  x-x_{}^{q} f^{}(x_{})_{}\) for all \(x K\), which describes the local curvature around the optimal solution \(x_{}\).

Before proving Theorem 15, we present the following lemma, which provides a characterization of uniformly convex sets. This directly follows from the definition in Definition 4:

**Lemma 16**.: _Suppose that a convex body \(K\) is \((,q)\)-uniformly convex w.r.t. a norm \(\) for \(>0\) and \(q 2\). Let \(y K\), \(g^{d}\), and \(y_{}*{arg\,min}_{y^{} K} g,y^{}\). Then, \(-g,y_{}-y y-y_{}^{q}  g_{}\)._

The proof can be found in [12, Lemma 2.1], and we include the proof in Appendix F for completeness.

Proof of Theorem 15.: From \(x_{}=*{arg\,min}_{x K}f^{}(x)\) and the first-order optimality condition, \( f^{}(x_{}),x-x_{} 0\) for all \(x K\), which implies that \(x_{}=*{arg\,min}_{x K} f^{}(x_{}),x\). Hence,by combining this with Lemma 16 and that \(K\) is \((,q)\)-uniformly convex w.r.t. a norm \(\|\|\), we have \( f^{}(x_{}),x-x_{}\|x-x_ {}\|^{q}\| f^{}(x_{})\|_{}\) for all \(x K\). Using this inequality,

\[_{T}[_{t=1}^{T} f^{ }(x_{}),x_{t}-x_{}]\| f ^{}(x_{})\|_{}\,[_{t=1}^{T}\|x-x_{}\|^ {q}]\] \[}\| f^{}(x_{})\|_{ }[_{t=1}^{T}\|x-x_{}\|_{2}^{q}]}\| f^{}(x_{})\|_{}\,T^{1-}( [_{t=1}^{T}\|x-x_{}\|_{2}^{2}])^{q/2},\] (2)

where the first inequality follows from the convexity of \(f^{}\), the second inequality by \(\|x\|_{2}\|x\|\) for any \(x^{d}\), and the last inequality by Jensen's inequality and the fact that \(x x^{q/2}\) is convex for \(q 2\). Combining (1) and (2), we can bound the regret as

\[=O\!(c_{}\![_{t=1}^{T} \|x_{t}-x_{}\|_{2}^{2}] T}+c_{}^{} T)- }\| f^{}(x_{})\|_{}\,T^{1-}\!(\![_{t=1}^{T}\|x-x_{}\|_{2}^{2}] )^{q/2}\] \[=O\!(})^{}}{(q \| f^{}(x_{})\|_{})^{}}T^{}( T)^{}+c_{}^{} T),\]

where in the last line we used the inequality \(a-bx^{q/2} a^{}(qb)^{}\) that holds for \(a,b,x>0\) and \(q 2\). This completes the proof. 

The above analysis can be extended to corrupted stochastic environments in a straightforward manner:

**Theorem 17**.: _Consider online convex optimization in corrupted stochastic environments with corruption level \(C\), where the optimal decision is \(_{}=_{ K}^{}(x)\). Suppose that \(K\) is \((,q)\)-uniformly convex w.r.t. a norm \(\|\|\) for some \(q 2\) and that \(^{}(_{}) 0\). Then, any algorithm with the bound (1) achieves_

\[_{T}=O\!(+C^{}\,^{}+c_{}^{} T)=})^{}}{(q\|^{}( _{})\|_{})^{}}T^{}( T )^{}\,.\]

_When \(K\) is \(\)-strongly convex w.r.t. \(\|\|\), \(_{T}=O\!(}}{\| f^{}( _{})\|_{}} T+}c_{}}{\|^{}(_{})\|_{ }} T}+c_{}^{} T).\)_

The proof can be found in Appendix G. When \(q=2\), the dependence on the corruption level \(C\) is the same as that in Theorem 13.

## 5 Conclusion

In this work, we introduce a new curvature condition for achieving fast rates in online convex optimization. Under this condition, we developed a new analysis to achieve fast rates by exploiting the curvature of feasible sets. In particular, by the algorithm adaptive to the curvature of loss functions, we proved an \(O( T)\) regret bound for \((,x_{},f^{})\)-sphere enclosed feasible sets. There are several advantages of our approach: it can exploit the curvature of loss functions, can achieve the \(O( T)\) regret bound only with local curvature properties, and can work robustly even in environments where loss vectors do not satisfy the ideal conditions. Notably, following a similar analysis, we proved the matching regret upper bound for uniformly convex feasible sets, which include strongly convex sets and \(_{p}\)-balls for \(p[2,)\) as special cases. This regret bound interpolates the \(O( T)\) regret over strongly convex sets and the \(O()\) regret over non-curved sets.