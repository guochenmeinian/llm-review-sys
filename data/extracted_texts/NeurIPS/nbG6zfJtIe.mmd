# Learning Curves for

Deep Structured Gaussian Feature Models

 Jacob A. Zavatone-Veth\({}^{1,2}\) and **Cengiz Pehlevan\({}^{3,2,4}\)**

\({}^{1}\)Department of Physics, \({}^{2}\)Center for Brain Science,

\({}^{3}\)John A. Paulson School of Engineering and Applied Sciences,

\({}^{4}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138, USA

jzavatoneveth@g.harvard.edu, cpehlevan@seas.harvard.edu

###### Abstract

In recent years, significant attention in deep learning theory has been devoted to analyzing when models that interpolate their training data can still generalize well to unseen examples. Many insights have been gained from studying models with multiple layers of Gaussian random features, for which one can compute precise generalization asymptotics. However, few works have considered the effect of weight anisotropy; most assume that the random features are generated using independent and identically distributed Gaussian weights, and allow only for structure in the input data. Here, we use the replica trick from statistical physics to derive learning curves for models with many layers of structured Gaussian features. We show that allowing correlations between the rows of the first layer of features can aid generalization, while structure in later layers is generally detrimental. Our results shed light on how weight structure affects generalization in a simple class of solvable models.

## 1 Introduction

Characterizing how data structure and model architecture affect generalization performance is among the foremost goals of deep learning theory [1; 2]. A fruitful line of inquiry has focused on the properties of a class of simplified models that are asymptotically solvable: neural networks in which only the readout layer is trained and other weights are random, which are known as random feature models (RFMs) [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21]. Though RFMs cannot capture the effects of representation learning on generalization in richly-trained neural networks [13; 22; 23], they have substantially advanced our understanding of how data structure and model architecture interact to give rise to a wide array of generalization phenomena observed in deep learning [1; 2; 3; 4; 5; 7; 8; 9; 10; 24; 25].

Of particular interest is the question of when models overfit benignly, that is, when they generalize well despite having been trained to perfectly interpolate their training data. Here, much intuition has been gained by studying minimum-norm kernel interpolation--that is, the ridgeless limit of kernel ridge regression--with RFM kernels, for which precise generalization asymptotics can be computed using tools from random matrix theory. These asymptotics lead to a precise picture of how the spectrum of the random feature kernel and the structure of the task interact to determine generalization. These analyses are facilitated by universality results, often termed Gaussian equivalence theorems, that state that the generalization error of a nonlinear RFM is asymptotically equal to that of a linear Gaussian model with an effective noise term resulting from nonlinearity [3; 7; 10; 25; 26]. In the past few years, Gaussian equivalence theorems for ever more general classes of RFMs have been established: within this year Schroder et al.  and Bosch et al.  have established Gaussian equivalence theoremsfor deep nonlinear RFMs with unstructured feature weights, while Cui et al.  have extended some of these results to the setting of deep Bayesian neural networks when the target is of the same architecture.

However, these analyses consider the effect only of correlations in the data, and do not address the possibility of correlations between the random weights. It is standard to assume that the elements of the weight matrices at each layer are independent and identically distributed Gaussian random variables, and to our knowledge all existing Gaussian equivalence theorems make use of this assumption [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 19; 20; 21]. As a result, how weight anisotropy affects generalization in deep RFMs--in particular, if it can affect the asymptotic scaling of generalization error with dataset size and network width [16; 28; 19]--remains unclear.

In this note, we take the first step towards filling that gap in our theoretical understanding of RFMs by computing the asymptotic generalization error of the simplest class of deep RFMs with anisotropic weight correlations: models with linear activations. Our primary contributions are as follows:

* Using the replica method from statistical mechanics , we compute the asymptotic generalization error of deep linear random feature models with weights drawn from general matrix Gaussian distributions. This computation is closely related to prior replica approches to product random matrix problems [30; 13].
* We show that, in the ridgeless limit, structure in the weights beyond the first layer is detrimental for generalization.
* We next consider the special case of power-law spectra in the weights and in the data, which was classically studied in kernel interpolation in the form of source-capacity conditions , and has recently attracted substantial interest in deep learning due to approximate power-law spectra present in real data [16; 19; 28; 32]. Using approximations for required spectral statistics derived in past works , we show that altering the power laws of the weight covariance spectra do not affect the scaling laws of generalization.
* We finally show how our results can be extended from the ridge regression estimator to the Bayesian Gibbs estimator, an object of classic study in the statistical physics of learning [33; 13; 34]. For sufficiently large prior variance, structure can be beneficial for generalization with this estimator.

Taken together, these results are consistent with the intuition that representation learning at only the first layer of a deep linear model is sufficient to recover a single teacher weight vector [35; 36; 13; 37].

## 2 Preliminaries

We consider depth-\(L\) linear RFMs with input \(^{n_{0}}\) and scalar output given by

\[g(;,)=}}()^{},\] (1)

where the feature matrix \(^{n_{0} n_{L}}\) is fixed and the vector \(^{n_{L}}\) is trainable. If \(L=0\), corresponding to standard linear regression, the feature matrix is simply the identity: \(=_{n_{0}}\). If \(L>0\), we take the feature matrix to be defined by a product of \(L\) factors \(_{}^{n_{-1} n_{}}\):

\[= n_{L}}}_{1}_{ L}.\] (2)

We draw the random feature matrices independently from matrix Gaussian distributions

\[_{}_{n_{-1} n_{}}(, _{},_{})\] (3)

for input covariance matrices \(_{}^{n_{-1} n_{-1}}\) and output covariance matrices \(_{}^{n_{} n_{}}\), such that \([(U_{})_{ij}(U_{^{}})_{i^{}j^{}}]=_ {^{}}(_{})_{ii^{}}(_{})_{jj^{}}\). Subject to the constraints of layer-wise independence and separability--which are required for the factors to be matrix-Gaussian distributed--this is the most general covariance structure one could consider. One might wish to relax this to include non-separable covariance tensors \([(U_{})_{ij}(U_{^{}})_{i^{}j^{}}]= _{^{}}(_{})_{ii^{},jj^{}}\), but this would spoil the matrix-Gaussianity of the factors, and to our knowledge does not appear to be addressable using standard methods [30; 38]. We generate training datasets according to a structured Gaussian covariate model, with \(p\) i.i.d. training examples \((_{},y_{})\) generated as

\[_{}_{}(,_{0}), y_{}=}}_{*}^{}_{}+ _{},\] (4)

where the teacher weight vector \(_{*}\) is fixed and the label noise follows

\[_{}_{}(0,^{2}).\] (5)

We collect the covariates into a matrix \(^{p n_{0}}\), and the targets into a vector \(^{p}\).

As in most works on RFMs [3; 4; 5; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 25], our focus is on the ridge regression estimator

\[=*{arg\,min}_{}L L=\|}}- \|^{2}+\|_{L+1}^{-1/2}\|_ {2}^{2},\] (6)

where the positive-definite matrix \(_{L+1}^{n_{L} n_{L}}\) controls the anisotropy of the norm and the ridge parameter \(>0\) sets the regularization strength. This minimization problem has the well-known closed form solution

\[}=}}(_{L+1} ^{-1}+}^{}^{} )^{-1}^{}^{}.\] (7)

As motivated in the Introduction, we are chiefly interested in the ridgeless limit \( 0\), in which the ridge regression solution gives the minimum \(_{2}\) norm interpolant of the training data. We measure performance of this estimator by the generalization error

\[_{p,n_{0},,n_{L}}=_{}(g(; },)-_{}[y()])^{2}= {1}{n_{0}}\|_{0}^{1/2}(}-_{*})\|^{2},\] (8)

which is a random variable with distribution induced by the training data and feature weights.

This leads us to a simple, but important observation: including structured input-input covariances is equivalent to transforming the feature-feature covariances. We state this formally as:

**Lemma 2.1**.: _Fix sets of matrices \(\{_{}\}_{=1}^{L+1}\) and \(\{_{}\}_{=0}^{L}\), and a target vector \(_{*}\). Let \(_{p,n_{0},,n_{L}}\) be the resulting generalization error as defined in (8). Let_

\[}_{} =_{n_{-1}} =1,,L+1,\] (9) \[}_{} =_{+1}^{1/2}_{} _{+1}^{1/2} =0,,L,\] (10) \[}_{*} =_{1}^{-1/2}_{*}.\] (11)

_Let \(_{p,n_{0},,n_{L}}\) be the generalization error for these transformed covariance matrices and target. Then, for any \(>0\), we have the equality in distribution \(_{p,n_{0},,n_{L}}}{{=}}_{p,n_{0},,n_{L}}\)._

Proof of Lemma 2.1.: As the features and data are Gaussian, we can write \(}{{=}}_{0}^{1/2} _{0}\) and \(_{}}{{=}}_{}^{1/2} _{}_{}^{1/2}\) for unstructured Gaussian matrices \((Z_{})_{ij}_{}(0,1)\). Substituting these representations the ridge regression solution (7) and the generalization error (8), the claim follows. 

Therefore, we may take \(_{}=_{n_{-1}}\) without loss of generality. Moreover, thanks to the rotation-invariance of the isotropic Gaussian factors \(_{}\), we may in fact take the remaining covariance matrices \(_{}\) to be diagonal without loss of generality, so long as we then express \(}_{*}\) in the basis of eigenvectors of \(_{0}\). An important qualitative takeaway of this result is that changing the covariance matrix of the inputs of the first layer \(_{1}\) is equivalent to modifying the data covariance matrix, which was in a simpler form observed in the shallow setting (\(L=1\)) by Pandey et al. .

## 3 Asymptotic learning curves

Having defined the setting of our problem, we can define our concrete objective and state our main results, deferring their interpretation to the following section. We consider the standard proportional asymptotic limit

\[p,n_{0},,n_{L}, n_{}/p _{}(0,),\] (12)which we will refer to as the thermodynamic limit. Our goal is to compute the limiting generalization error:

\[=_{p,n_{0},,n_{L}}_{ }}\|_{0}^{1/2}(-_{*})\|^{2},\] (13)

where \(_{}\) denotes expectation over all sources of quenched disorder in the problem, i.e., the training data and the random feature weights. In the thermodynamic limit, we expect the generalization error to concentrate, which is why we compute its average in (13) .

To have a well-defined thermodynamic limit, the covariances \(}_{}\) and the teacher \(}_{}\) must be in some sense sufficiently well-behaved. We consider the following conditions, which are the generalization to our setting of those assumed in previous work :

**Assumption 3.1**.: _We assume that we are given deterministic sequences of positive-definite matrices \(}_{}(n_{})\) and vectors \(}_{*}(n_{0})\) indexed by the system size, such that the limiting (weighted) spectral moment generating functions_

\[M_{}_{}}(z)=_{n_{}} }[}_{}(z_{n_ {}}-}_{})^{-1}](z)=_{n_ {0}}}}_{*}^{}}_{0}(z_{n_{0}}+}_{0})^{-1} }_{*}\] (14)

_are well-defined, for all \(=0,,L\)._

We can now state our results. As a preliminary step, we first give an expression for the generalization error for a fixed teacher \(}_{*}\) at finite ridge \(\). Then, we pass to the ridgeless limit, on which we focus for the remainder of the paper. At finite ridge, we have the following:

**Proposition 3.1**.: _Assume Assumption 3.1 holds. For \(>0\), let \(\) solve the self-consistent equation_

\[=_{=0}^{L}}M_{}_{}}^{-1}(-}).\] (15)

_In terms of \(\), let \(_{}()\) solve_

\[_{_{}}[_{ }}{_{}()+_{}}]=-M_{}_{}}(-_{}())=}\] (16)

_for \(=0,,L\), where \(_{_{}}[h(_{})]=_{n_{} }n_{}^{-1}_{j=1}^{n_{}}h(_{,j})\) denotes expectation of a function \(h\) with respect to the limiting spectral distribution of \(}_{}\), for \(_{,j}\) its eigenvalues at finite size, and let_

\[_{}()=-}{}_{}( )M_{}_{}}^{}(-_{}())=1- }{}_{_{}}[( {_{}}{_{}()+_{}})^{2 }].\] (17)

_Then, the learning curve (13) at finite ridge for a fixed target is given by_

\[1+_{=0}^{L}}{_{}} (1-)=_{=1}^{L}}{ _{}}_{0}(_{0})-^{2}}{_{0}} ^{}(_{0})+_{=0}^{L}}{_{} }^{2}.\] (18)

Proof of Proposition 3.1.: We defer the derivation of (18) to Appendix A. To compute the disorder average in (13), we express the minimization problem in (6) as the zero-temperature limit \(\) of an auxiliary Gibbs distribution \(p() e^{- L}\), and evaluate the average over the random data random feature weights using the non-rigorous replica method from the statistical mechanics of disordered systems . This computation is lengthy but standard, and is closely related to the approach used in our previous works on deep linear models . All of our results are obtained under a replica-symmetric _Ansatz_; as the ridge regression problem (6) is convex, we expect replica symmetry to be unbroken . 

From the self-consistent equation (15), we recognize that \(\) is is up to a sign the spectral moment generating function of the feature Gram matrix \(=^{}^{}/n_{0}\), which is a product-Wishart random matrix :

\[()=-M_{}(-).\] (19)

This dependence falls out of the replica computation of the generalization error using an auxiliary Gibbs distribution; we emphasize that one could take an alternative approach in which the generalization error is first expressed in terms of \(M_{}\)--as, for instance, in Gerace et al.  or Hastie et al.

--and then use results on the spectra of product-Wishart matrices to conclude the claimed result . This approach would potentially have the advantage of giving a fully rigorous proof, rather than one that depends on the replica trick. However, one would still then be faced with the task of solving the self-consistent equation for the spectral moment generating function, and therefore would end up in the same place insofar as quantitative predictions are concerned.

In principle, we could now directly proceed to study how weight structure affects (18) for some fixed ridge \(\). However, as long as there is structure in the weights and/or the data, the self-consistent equation (15) must generally be solved numerically . To allow us to make analytical progress, we therefore focus on the ridgeless limit \( 0\) for the remainder of the present paper, and leave careful analysis of the \(>0\) case to future work. This follows the path of most recent studies of models with linear random features, and also the fundamental interest in interpolating models . We therefore emphasize that we state Proposition 3.1 merely as a preliminary result.

Before giving our result for the generalization error in the ridgeless limit, we warn the reader of an impending, somewhat severe abuse of notation: in Proposition 3.2 and for the remainder of the paper, we will re-define \(_{}\) to be given by its value for the solution for \(\) appropriate in the regime of interest. Moreover, we will simply write \(\) for \(_{ 0}\).

**Proposition 3.2**.: _Assume Assumption 3.1 holds, and let \(_{}=\{_{1},,_{L}\}\). For \(=0,,L\), in the regime \(_{}>1\), let \(_{}\) be given by the unique non-negative solution to the implicit equation_

\[}=-M_{}_{}}(-_{})= _{_{}}[_{}}{_{ }+_{}}].\] (20)

_In terms of \(_{}\), let_

\[_{}=-_{}_{}M^{}_{}_{ }}(-_{})=1-_{}_{_{}}[ (_{}}{_{}+_{}}) ^{2}].\] (21)

_In the regime \(_{}<_{0}\), let \(_{}\) be the unique non-negative solution to the implicit equation_

\[}{_{0}}=-M_{}_{0}}(-_{ })=_{_{0}}[_{0}}{_ {}+_{0}}].\] (22)

_Then, the learning curve (13) for a fixed target in the ridgeless limit \( 0\) is given by_

\[=_{=1}^{L}}{_{}} _{0}(_{0})-^{2}}{_{0}}^{} (_{0})+_{=0}^{L}}{_{}} ^{2},&_{0},_{}>1\\ (_{})}{1-_{}}+} {1-_{}}^{2},&_{}<1,_{}<_{0}\\ }{1-_{0}}^{2},&_{0}<1,_{0}<_{}.\] (23)

Proof of Proposition 3.2.: We derive (23) as the zero-ridge limit of Proposition 3.1 in Appendix A. 

Before we analyze the effect of weight anisotropy in detail in Section 4, we note several simplifying special cases of Proposition 3.2 which recover the results of prior works. To facilitate this comparison, we provide a notational dictionary in Appendix D. The first important special case is

**Corollary 3.1**.: _If \(L=0\), we have_

\[=-^{2}}{_{0}}^{}(_{0} )+}{_{0}}^{2},&_{0}>1\\ }{1-_{0}}^{2},&_{0}<1.\] (24)

This recovers the known, rigorously proved result for linear ridgeless regression . For larger depths, an important simplifying case of Proposition 3.2 is that in which the data and features are unstructured, in which case the generalization error is given by

**Corollary 3.2**.: _If \(}_{}=_{n_{}}\) for \(=0,,L\), we have, for any target satisfying \(\|}_{*}\|^{2}=n_{0}\),_

\[=1+_{=1}^{L}} 1-}+_{=0}^{L}-1}^{2},&_{0},_{}>1\\ /_{0}}{1-_{}}+}{1- _{}}^{2},&_{}<1,_{}<_{0}\\ }{1-_{0}}^{2},&_{0}<1,_{0}<_{}.\] (25)Proof of Corollary 3.2.: We have \(M_{_{n_{}}}(z)=1/(z-1)\), hence \(_{}=_{}-1\), \(_{}=1-1/_{}\), and \(_{}=_{0}/_{}-1\). Finally, for any fixed teacher vector satisfying \(\|}_{*}\|^{2}=n_{0}\), we have \((z)=1/(z+1)\) if \(}_{0}=_{n_{0}}\). Substituting these results into (23), we obtain (25). 

This recovers the result obtained in our previous work , and in the single-layer case \(L=1\) recovers results obtained by Rocks and Mehta , and by Hastie et al.  (see Appendix D). In the slightly more general case of unstructured weights but structured features, we have

**Corollary 3.3**.: _If \(}_{}=_{n_{}}\) for \(=1,,L\), but \(}_{0}_{n_{0}}\), we have, for any target satisfying \(\|}_{*}\|^{2}=n_{0}\)._

\[=_{=1}^{L}-1} _{0}(_{0})-^{2}}{_{0}}^{}(_ {0})+}{_{0}}+_{=1}^{L}- 1}^{2},&_{0},_{}>1\\ (_{})}{1-_{}}+ }{1-_{}}^{2},&_{}<1,_{}<_{0}\\ }{1-_{0}}^{2},&_{0}<1,_{0}<_{}.\] (26)

Proof of Corollary 3.3.: (26) follows from substituting the results of Corollary 3.2 into (23). 

In the special case \(L=1\), this recovers the result obtained using rigorous methods in contemporaneous work by Bach , posted to the arXiv one day after the first version of our work . Here, as the data spectrum and target vector enter the generalization error in nearly the same way as in the case of linear regression, all of the intuitions developed in that case can be carried over .

Another useful simplification can be obtained by further averaging over isotropically-distributed teachers \(}_{*}(,_{n_{0}})\), which gives

**Corollary 3.4**.: _Let \(=_{}_{*}(, _{n_{0}})}[]\). Then, we have_

\[=1+_{=1}^{L}}{ _{}}}{_{0}}+_{=0}^{L} {1-_{}}{_{}}^{2},&_{0},_{}>1\\ _{}/_{0}}{1-_{}}+}{1-_{}}^{2},&_{}<1,_{}<_{0}\\ }{1-_{0}}^{2},&_{0}<1,_{0}<_{}.\] (27)

Proof of Corollary 3.4.: Observing that \(_{}_{*}}(z)=-M_{}_{0}}( -z)\), the claim follows from (23). 

In the special case of a single layer of unstructured feature weights (\(L=1\), \(}_{1}=_{n_{1}}\)), this recovers the result of recent work by Maloney et al. , who used a planar diagram method to the generalization error of single-hidden-layer linear RFMs with unstructured weights (see Appendix D).

Another important simplifying case of Proposition 3.2 is the limit in which the hidden layer widths are large, in which the generalization error of the deep RFM reduces to that of a shallow model, as given by Corollary 3.1. More precisely, we have a large-width expansion given by:

Figure 1: Phase diagram of generalization in deep linear RFMs. For simplicity, we consider a model with a single hidden layer (\(L=1\)); the picture for deeper models is identical if one considers the narrowest hidden layer . (a). Generalization error \(\) for unstructured data and features from (25) as a function of training data density \(1/_{0}\) and hidden layer width \(_{1}/_{0}\) in the absence of label noise (\(=0\); _left_) and in the presence of label noise (\(=0.5\); _right_). (b). As in (a), but for power law structured data and weights, with \(_{0}=_{1}=1\), and \(\) given by (31). See Appendix F for numerical methods.

**Corollary 3.5**.: _In the large-width regime \(_{1},,_{L} 1\), assuming that the weight spectra have finite moments, the generalization error (23) expands as_

\[=-^{2}}{_{0}}^{}(_{0})+}{_{0}}^{2}+_{=1}^{L}_{_{ }}[_{}^{2}]}{_{_{}}[_{ }]^{2}}}(_{0}(_{0})+^{2})+ (_{1}^{-2},,_{L}^{-2})\] (28)

_in the regime \(_{0}>1\); if \(_{0}<1\) the generalization error does not depend on the hidden layer widths so long as they are greater than 1._

Proof of Corollary 3.5.: See Appendix E. 

## 4 How does weight structure affect generalization?

The first salient feature of the learning curves given by Proposition 3.2 is that the addition of weight structure does not alter the phase diagram of generalization, which is illustrated in Figure 1. There are three qualitatively distinct phases present, depending on the data density and minimum layer width: the overparameterized regime \(_{0},_{}>1\), the bottlenecked regime \(_{}<1\), \(_{}<_{0}\), and the overdetermined regime \(_{0}<1\), \(_{0}<_{}\). This dependence on the narrowest hidden layer matches our previous work on models with unstructured weights 1, and can be observed in the solutions to the ridge regression problem for fixed data (Appendix C). As \(_{} 1\), \(_{} 0\) and \(_{} 0\), and the generalization error diverges. Similarly, the generalization error diverges as \(_{} 1\), or \(_{0} 1\) in the presence of label noise. However, there are not multiple descents in these deep linear models, consistent with the qualitative picture of the effect of nonlinearity given by previous works [9; 10].

The second salient feature of Proposition 3.2 is that the matrices \(}_{}\) enter the generalization error independently; there are no 'interaction' terms involving products of the correlation matrices for different layers. This decoupling is expected given that the features are Gaussian and independent across layers . Moreover, under the rescaling \(}_{}^{}=_{}}_{}\) for \(_{}>0\), we have \(_{}^{}=_{}_{}\) and \(_{}^{}=_{}\) (we show this explicitly in Appendix B). Therefore, (23) is sensitive only to the overall scale of \(}_{0}\), not to the scales of \(}_{1},,}_{L}\). This scale-invariance can be observed directly from the ridgeless limit of the ridge regression estimator (7).

We can gain intuition for the effect of having \(}_{}_{n_{}}\) for \( 1\) through the following argument:

**Lemma 4.1**.: _Under the conditions of Proposition 3.2, in the regime \(_{0},_{}>1\), we have_

\[_{=1}^{L}-1}_{0 }(_{0})-^{2}}{_{0}}^{}(_{0})+ }{_{0}}+_{=1}^{L}-1} ^{2}.\] (29)

_That is, the generalization error for a given \(}_{1},,}_{L}\) is bounded from below by the generalization error for \(}_{}=_{n_{}}\) for \(=1,,L\)._

Proof of Lemma 4.1.: In Appendix B, we show that \(_{} 1-1/_{}\) for any weight spectrum, which implies that \((1-_{})/_{} 1/(_{}-1)\). Substituting these bounds in to the general expression for the generalization error in this regime from (23), the claim follows. 

Therefore, having \(}_{}_{n_{}}\) for \(=1,,L\) cannot improve generalization in the \(_{0},_{}>1\) regime. This is consistent with the large-width expansion in Corollary 3.5, where we can apply Jensen's inequality to bound the weight-dependence of the correction as \(_{_{}}[_{}^{2}]/_{_{} }[_{}]^{2} 1\), with equality only when the weights are unstructured. In other regimes, \(}_{1},,}_{L}\) do not affect the generalization error. In contrast, a similar argument shows that anisotropy in \(}_{0}\) can be beneficial in the target-averaged case, at least in the absence of label noise. We formalize this as:

**Lemma 4.2**.: _Under the conditions of Corollary 3.4, in the absence of label noise (\(=0\)), we have_

\[1+_{=1}^{L} }{_{}}1-}[_{0}],&_{0},_{}>1\\ /_{0})}{1-_{}}[_ {0}],&_{}<1,_{}<_{0}\\ 0,&_{0}<1,_{0}<_{}.\] (30)

_That is, \(\) for a given \(}_{0}\) is bounded from above by the generalization error for a flat spectrum \(}_{0}=[_{0}]_{n_{0}}\)._Proof of Lemma 4.2.: In Appendix B, we show that \(_{0}(_{0}-1)[_{0}]\). As its defining equation (22) is of the same form as (20), the corresponding bound for \(_{}\) follows immediately: \(_{}(_{0}/_{}-1)[_{0}]\). Substituting these bounds into (27) with \(=0\), the claim follows. 

If \([_{0}]\) is not finite, then this bound is entirely vacuous: \(\). If we do not average over isotropically-distributed targets, then the effect of anisotropy in \(}_{0}\) is harder to analyze. Previous works have, however, analyzed the interaction of data structure with a fixed target in great detail for models with \(L=0\) or \(L=1\), showing that targets that align with the top eigenvectors of \(}_{0}\) are easier to learn [5; 16; 42; 44].

## 5 Power law spectra

We can gain further intuition for the effect of weight structure by considering an approximately solvable model for anisotropic spectra: power laws [16; 19; 28]. Power law data spectra have recently attracted considerable attention as a possible model for explaining the scaling laws of generalization observed in large language models [16; 19; 28; 32]. Maloney et al.  proposed a single-hidden-layer (\(L=1\)) linear RFM with power-law-structured data and unstructured weights as a model for neural scaling laws. Does introducing power law structure into the weights affect the scaling laws predicted by deep linear RFMs? We have the following result:

**Corollary 5.1**.: _At finite size, define each covariance matrix \(}_{}\) such that its \(j\)-th eigenvalue is \(_{,j}=_{}(n_{}/j)^{1+_{}}\) for some fixed scale factor \(_{}>0\) and exponent \(_{}>0\). Then, the limiting target-averaged generalization error is approximately_

\[(1+_{L}+_{=1}^{L}-1})(_{0})+_{0}+_{L}+_{=0}^ {L}-1}^{2},&_{0},_{}>1\\ /_{})}{1-_{}}+}{1 -_{}}^{2},&_{}<1,_{}<_{0}\\ }{1-_{0}}^{2},&_{0}<1,_{0}<_{},\] (31)

_where \(_{L}=_{=1}^{L}_{}\) and for \(z>1\) we have \((z)-M_{}_{0}}^{-1}(z)/z\) given by \((z)=_{0}\{k(z^{_{0}}-1)+[2+_{0}(1- k)](1-1/z)\}\) for \(k=[/(1+_{0})]^{-(1+_{0})}\)._

Proof of Corollary 5.1.: Using the dictionary of notation in Appendix D, we can plug the approximate solutions for \(_{}\) and \(_{}\) derived by Maloney et al.  into (27) to obtain (31). 

Therefore, the power law exponents \(_{1},,_{L}\) of the weight covariances beyond the first layer, which enter only through their sum \(_{L}\), do not affect the scaling laws of the generalization error with the dataset size and network widths. In particular, in the absence of label noise (\(=0\)) we can approximate the scaling of (31) in the regimes of large or small hidden layer width by

\[_{0}^{_{0}},&_{}>1, _{0} 1,\\ (_{0}/_{})^{_{0}},&_{}<1,_{0}/_{ } 1,\] (32)

which recovers the results found by Maloney et al.  for \(L=1\) with unstructured weights. This behavior, and the agreement of (31) with numerical experiments, is illustrated in Figure 2. Consistent with Lemma 4.1, generalization with power-law weight structure is never better than with unstructured weights, as can be seen by comparing (31) with (25).

## 6 Bayesian inference and the Gibbs estimator at large prior variance

Thus far, we have focused on ridge regression (6). Though this is the most commonly-considered estimator in studies of random feature models [3; 4; 5; 6; 7; 19; 20; 17; 18; 17; 19], one might ask whether our qualitative findings--in particular, that feature weight structure beyond the first layer is generally harmful for generalization--carry over to other estimators. Our approach to Proposition 3.2 is easily extensible to the setting of _zero-temperature Bayesian inference_, which has recently attracted substantial interest [13; 27; 34; 45; 46; 47], sparked by work from Li and Sompolinsky . In this case, we take seriously the Gibbs distribution \(p() e^{- L}\), which in the ridge regression case was simply a convenient tool, and interpret it as the Bayes posterior for a Gaussian likelihood of variance \(1/\) and a Gaussian prior with covariance \(_{L+1}/()\). It is in this context conventional to fix \(=1/\), such that the prior variance does not scale with \(\). We can then study the average of the generalization error (13) under this posterior in the zero-temperature limit \(\), which we refer to as the generalization error of the Gibbs estimator. We emphasize that this is not identical to the Bayesian minimum mean squared error (MMSE) estimator given by the posterior mean, which would coincide with the ridgeless estimator in the zero-temperature limit (see Appendix A).

For a deep RFM, this simply has the effect of adding a "thermal" variance term to the generalization error of the ridgeless estimator, which we describe in detail in Appendices A and C. We have:

**Proposition 6.1**.: _With the same setup as in Proposition 3.2, the generalization error of the Gibbs estimator for a RFM is_

\[_{}=_{}+_{ =0}^{L}}{_{}},&_{0},_{}>1\\ 0,&,\] (33)

_where \(_{}\) is given by Proposition 3.2, and \(_{}\) is defined as in (20)._

Proof of Proposition 6.1.: We derive (33) alongside Proposition 3.2 in Appendix A. 

The Gibbs estimator is sensitive to the scale of the random feature weight distributions through \(_{}\), while as noted above the ridgeless estimator is not sensitive to their overall scale. This direct dependence on \(_{}\) means that the simple argument of Lemma 4.1 cannot be applied. Indeed, in the limit of large prior variance, where the thermal variance term dominates, structure can improve the performance of the Gibbs estimator. We make this result precise in the following lemma:

**Lemma 6.1**.: _In the setting of Proposition 6.1, consider Bayesian RFMs with weight covariances scaled as \(_{}}_{}\) for \(=1,,L\). Then, in the non-trivial regime \(_{0},_{}>1\) where the thermal variance is non-vanishing, we have_

\[_{_{1},,_{L}}}}{_{ =1}^{L}_{}}=_{=0}^{L}}{_{} }}{_{0}}^{2}_{=1}^{L}1- },\] (34)

_where the scalars \(_{}\) are defined in terms of the un-scaled covariances \(}_{}\) as in (20) and \(^{2}_{=1}^{L}_{_{}}[ {}_{}]\). Therefore, in the limit of large prior variance, including structure in the weight priors is generically advantageous for generalization. If \(_{_{}}[_{}]\) is not finite, then the bound is vacuous._

Proof of Lemma 6.1.: The first part of (34) follows from (33) using the scaling properties of \(_{}\), while the bound follows from the bounds on \(_{}\) derived as part of Lemma 4.2. 

In contrast, weight structure is generally harmful for the Bayesian RFM in the limit of small prior variance, as its performance then coincides with the ridgeless RFM, as can be seen from the scaling

Figure 2: Generalization for power-law spectra. (a). Target-averaged generalization error \(\) as a function of training data density \(1/_{0}\) for shallow models (\(L=1\)) of varying hidden layer width \(_{1}/_{0}\) in the absence of label noise (\(=0\)). Here, the data and weight spectra have identical power law decay \(_{0}=_{1}=1\). (b). As in (a), but in the presence of label noise (\(=1/2\)). (c). As in (b), but for fixed hidden layer width \(_{1}/_{0}=4\), fixed data exponent \(_{0}=1\), and varying weight exponents \(_{1}\). In all cases, solid lines show the predictions of (31), while dots with error bars show the mean and standard error over 100 realizations of numerical experiments with \(n_{0}=1000\). See Appendix F for details of our numerical methods.

of \(_{}\). This example illustrates that there are cases in which, depending on the estimator used, weight structure in deeper layers can sometimes be helpful for generalization. However, whereas the ridgeless estimator is commonly used in practice, the Gibbs estimator is less standard, and the limit of large prior variance is certainly artificial. Therefore, we emphasize that we give this example to show that the behavior of the ridgeless estimator is not entirely general, not to show that weight structure can be helpful in practical settings.

## 7 Discussion

We have computed learning curves for models with many layers of structured Gaussian random features learning a linear target function, showing that structure beyond the first layer is generally detrimental for generalization. This result is consistent with the intuition that in deep linear models learning a single target direction it is sufficient to modify the representation only at the first layer [13; 36]. It will be interesting to investigate whether this intuition carries over to nonlinear networks learning complex tasks, particularly including multi-index targets [35; 48]. Moreover, we have considered only linear, Gaussian models. As mentioned in the Introduction, past works have established Gaussian equivalence theorems for nonlinear RFMs with unstructured Gaussian feature weights. It will be important to investigate the effect of feature weight structure on Gaussian equivalence in future work, and determine whether our qualitative results carry over to nonlinear RFMs in the proportional limit.

Though our results are obtained using the replica trick, and we do not address the possibility of replica symmetry breaking, they should be rigorously justifiable given the convexity of the ridge regression problem [29; 33; 41]. We note that the replica approach makes it straightforward to handle models of any finite depth . The relevant averages could of course be computed with alternative random matrix theory techniques, which could allow for a fully rigorous proof [5; 19; 20; 21]. Another more challenging setting to study with either the replica trick or rigorous techniques would be that in which one allows for correlations between weights in different layers. This setting could qualitatively capture aspects of feature learning in deep networks, which induces couplings across depth .

In closing, we note that RFMs with structured weights may also have relevance for biological neural networks. A recent study by Pandey et al.  considered RFMs with a single layer of random features (\(L=1\)) with correlated rows (\(_{1}_{n_{0}}\)). In several biologically-inspired settings, they showed that introducing this structure could improve generalization, consistent with our results. More broadly, biological neural networks are imbued with rich priors ; investigating what insights deep structured models can afford for neuroscience will be an interesting subject for further study.