# Return of Unconditional Generation:

A Self-supervised Representation Generation Method

 Tianhong Li   Dina Katabi   Kaiming He

MIT CSAIL

###### Abstract

Unconditional generation--the problem of modeling data distribution without relying on human-annotated labels--is a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data. In the literature, the generation quality of an unconditional method has been much worse than that of its conditional counterpart. This gap can be attributed to the lack of semantic information provided by labels. In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder. These representations can be used to condition the image generator. This framework, called Representation-Conditioned Generation (RCG), provides an effective solution to the unconditional generation problem without using labels. Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: _e.g._, it achieves a new state-of-the-art FID of 2.15 on ImageNet 256\(\)256, largely reducing the previous best of 5.91 by a relative 64%. Our unconditional results are situated in the same tier as the leading class-conditional ones. We hope these encouraging observations will attract the community's attention to the fundamental problem of unconditional generation. Code is available at https://github.com/LTH14/rcg.

## 1 Introduction

Generative models have been long developed as _unsupervised_ learning methods in the history, _e.g._, in the seminal works including GAN , VAE , and diffusion models . These fundamental methods focus on learning the probabilistic distributions of data, without relying on the availability of human annotations. This problem, often categorized as _unconditional generation_ in today's community, is in pursuit of utilizing the vast abundance of unannotated data to learn complex distributions.

However, unconditional image generation has been largely stagnant in comparison with its conditional counterpart. Recent research [18; 54; 10; 11; 24; 50] has demonstrated compelling image generation quality when conditioned on class labels or text descriptions provided by humans, but its quality degrades _significantly_ without these conditions. Closing the gap between unconditional and conditional generation is a challenging and scientifically valuable problem: it is a necessary step towards unleashing the power of large-scale unannotated data, which is a common goal in today's deep learning community.

We hypothesize that such a performance gap is because human-annotated conditioning introduces rich semantic information to simplify the generative process. In this work, we largely close this gap by taking inspiration from a closely related area--unsupervised/self-supervised learning.1 Weobserve that the _representations_ produced by a strong self-supervised encoder (_e.g._, [30; 12; 8; 14]) can also capture a lot of semantic attributes without human supervision, as reflected by their transfer learning performance in the literature. These self-supervised representations can serve as a form of conditioning without violating the unsupervised nature of unconditional generation, creating an opportunity to get rid of the heavy reliance on human-annotated labels.

Based on this observation, we propose to first unconditionally generate a self-supervised representation and then condition on this representation to generate the images. As a preprocessing step (Figure 0(a)), we use a pre-trained self-supervised encoder (_e.g._, MoCo ) to map the image distribution into the corresponding representation distribution. In this representation space, we train a representation generator without any conditioning (Figure 0(b)). As this space is low-dimensional and compact , learning the representation distribution is favorably feasible with unconditional generation. In practice, we implement it as a very lightweight diffusion model. Given this representation space, we train a second generator that is conditioned on these representations and produces images (Figure 0(c)). This image generator can conceptually be any image generation model. The overall framework, called _Representation-Conditioned Generation_ (RCG), provides a new paradigm for unconditional generation.2

RCG is conceptually simple, flexible, yet highly effective for unconditional generation. RCG greatly improves unconditional generation quality regardless of the specific choice of the image generator (Figure 2), reducing FID by 71%, 76%, 82%, and 51% for LDM-8, ADM, DiT-XL/2, and MAGE-L, respectively. This indicates that RCG largely reduces the reliance of current generative models on

Figure 1: **The Representation-Conditioned Generation (RCG) framework** for unconditional generation. RCG consists of three parts: (a) it uses a pre-trained self-supervised encoder to map the image distribution to a representation distribution; (b) it learns a representation generator that samples from a noise distribution and generates a representation subject to the representation distribution; (c) it learns an image generator (_e.g._, which can be ADM , DiT , or MAGE ) that maps a noise distribution to the image distribution conditioned on the representation distribution.

manual labels. On the challenging ImageNet \(256\)256 benchmark, RCG achieves an unprecedented 2.15 FID for unconditional generation. This performance not only largely outperforms previous unconditional methods, but more surprisingly, can catch up with the strong leading methods that are _conditional_ on class labels. We hope our method and encouraging results will rekindle the community's interest in the fundamental problem of unconditional generation.

## 2 Related Work

**Generative Models.** Generative models aim at accurately modeling data distribution to generate new data point that resembles the original data. One stream of generative models is built on top of generative adversarial networks (GANs) [27; 69; 37; 70; 7]. Another stream is based on a two-stage scheme [63; 53; 10; 67; 40; 41; 11]: first tokenize the image into a latent space and then apply maximum likelihood estimation and sampling in the token space. Diffusion models [33; 59; 18; 54; 52] have also achieved superior results on image synthesis.

The design of a generative model is mostly orthogonal to how it is conditioned. However, literature has shown that unconditional generation often significantly lags behind conditional generation under the same design[18; 41; 10], especially on complex data distributions.

**Unconditional Generation.** Unconditional generation is the fundamental problem in the realm of generative models. It aims to model the data distribution without relying on human annotations, highlighted by seminal works of GAN , VAE , and diffusion models . It has demonstrated impressive performance in modeling simple image distributions such as scenes or human faces [23; 10; 18; 54], and has also been successful in applications beyond images where human annotation is challenging or impossible, such as novel molecular design [66; 28; 26], medical image synthesis [71; 16; 47], and audio generation [48; 42; 25]. However, recent research in this domain has been limited, largely due to the notable gap between conditional and unconditional generation capabilities of recent generative models on complex data distributions [46; 18; 19; 41; 3; 61].

Prior efforts to narrow this gap mainly group images into clusters in the representation space and use the cluster indices as underlying class labels to provide conditioning [46; 43; 3; 35]. However, these methods assume that the dataset is clusterable, and the optimal number of clusters is close to the number of classes. Additionally, these methods fall short of generating diverse representations--they are unable to produce different representations within the same cluster or underlying class.

**Representations for Image Generation.** Prior works have explored the effectiveness of exploiting representations for image generation. DALL-E 2 , a _text-conditional_ image generation model, first converts text prompts into image embeddings, and then uses these embeddings as the conditions

Figure 2: **Unconditional Image Generation can be largely improved by our RCG framework. Regardless of the specific form of the image generator (LDM , ADM , DiT , or MAGE ), RCG massively improves the unconditional generation quality. Generation quality is measured by FID on ImageNet with a 256\(\)256 resolution. All comparisons between models without and with RCG are conducted under controlled conditions to ensure fairness. The technical details and more metrics are in Section 4.1.**

to generate images. In contrast, RCG for the first time demonstrates the possibility of directly generating image representations _from scratch_, a necessary step to enable conditioning on self-supervised representations in unconditional image generation. Another work, DiffAE , trains an image encoder in an end-to-end manner with a diffusion model as decoder, aiming to learn a meaningful and decodable image representation. However, its semantic representation ability is still limited (e.g., compared to self-supervised models like MoCo , DINO ), which largely hinders its performance in unconditional generation. Another relevant line of work is retrieval-augmented generative models [5; 4; 9], where images are generated based on representations extracted from existing images. Such semi-parametric methods heavily rely on ground-truth images to provide representations during generation, a requirement that is impractical in many generative applications.

## 3 Method

Directly modeling a complex high-dimensional image distribution is a challenging task. RCG decomposes it into two much simpler sub-tasks: first modeling the distribution of a compact low-dimensional representation, and then modeling the image distribution conditioned on this representation distribution. Figure 1 illustrates the idea. Next, we describe RCG and its extensions in detail.

### The RCG Framework

RCG comprises three key components: a pre-trained self-supervised image encoder, a representation generator, and an image generator. Each component's design is elaborated below:

**Distribution Mapping.** RCG employs an off-the-shelf image encoder to convert the image distribution to a representation distribution. This image encoder has been pre-trained using self-supervised contrastive learning methods, such as MoCo v3 , on ImageNet. This approach regularizes the representations on a hyper-sphere while achieving state-of-the-art performance in representation learning. The resulting distribution is characterized by two essential properties: it is simple enough to be modeled effectively by an _unconditional_ representation generator, and it is rich in high-level semantic content, which is crucial for guiding image generation. These attributes are vital for the effectiveness of the following two components.

Figure 3: **RCG’s training framework. The pre-trained self-supervised image encoder extracts representations from images and is fixed during training. To train the representation generator, we add standard Gaussian noise to the representations and ask the network to denoise them. To train the MAGE image generator, we add random masking to the tokenized image and ask the network to reconstruct the missing tokens conditioned on the representation extracted from the same image.**

**Representation Generation.** In this stage, we want to generate abstract, unstructured representations without conditions. To address this issue, we develop a diffusion model for unconditional representation generation, which we call a representation diffusion model (RDM). RDM employs a fully-connected network with multiple fully-connected residual blocks as its backbone (Figure 4). Each block consists of an input layer, a timestep embedding projection layer, and an output layer, where each layer consists of a LayerNorm , a SiLU , and a linear layer. Such an architecture is simply controlled by two parameters: the number of blocks, \(N\), and the hidden dimension, \(C\).

RDM follows DDIM  for training and inference. As shown in Figure 2(a), during training, image representation \(z_{0}\) is mixed with standard Gaussian noise variable \(\): \(z_{t}=}z_{0}+}\). The RDM backbone is then trained to denoise \(z_{t}\) back to \(z_{0}\). During inference, RDM generates representations from Gaussian noise following the DDIM sampling process . Since RDM operates on highly compacted representations, it brings marginal computation overheads for both training and generation (Appendix B.1), while providing substantial semantic information for the image generator, introduced next.

**Image Generation.** The image generator in RCG crafts images conditioned on self-supervised representations. Conceptually, such an image generator can be any modern conditional image generative model by substituting its original conditioning (e.g., class label or text) with representations. In Figure 2(b), we take MAGE , a parallel decoding generative model as an example. The image generator is trained to reconstruct the original image from a masked version of the image, conditioned on the representation of the same image. During inference, the image generator generates images from a fully masked image, conditioned on the representation generated by the representation generator.

We experiment with four representative generative models: ADM , LDM , and DiT  are diffusion-based frameworks, and MAGE  is a parallel decoding framework. Our experiments show that all four generative models achieve much better performance when conditioned on high-level self-supervised representations (Table 1).

### Extensions

Our RCG framework can easily be extended to support guidance even in the absence of labels, and to support conditional generation when desired. We introduce these extensions as follows.

**Enabling Guidance in Unconditional Generation.** In class-conditional generation, the presence of labels allows not only for class conditioning but can also provides additional "guidance" in the generative process. This mechanism is often implemented through classifier-free guidance in class-conditional generation methods [32; 54; 11; 50]. In RCG, the representation-conditioning behavior enables us to also benefit from such guidance, even in the absence of labels.

Specifically, RCG follows [32; 11] to incorporate guidance into its MAGE generator. During training, the MAGE generator is trained with a 10% probability of not being conditioned on image representations, analogous to  which has a 10% probability of not being conditioned on labels. For each inference step, the MAGE generator produces a representation-conditioned logit, \(l_{c}\), and

Figure 4: **Representation generator’s backbone architecture.** Each “Layer” consists of a LayerNorm layer , a SiLU layer , and a linear layer. The backbone consists of an input layer that projects the representation to hidden dimension \(C\), followed by \(N\) fully-connected (fc) blocks, and an output layer that projects the hidden latent back to the original representation dimension. The diffusion timestep is embedded and added to every fc block.

an unconditional logit, \(l_{u}\), for each masked token. The final logits, \(l_{g}\), are calculated by adjusting \(l_{c}\) away from \(l_{u}\) by the guidance scale, \(\): \(l_{g}=l_{c}+(l_{c}-l_{u})\). The MAGE generator then uses \(l_{g}\) to sample the remaining masked tokens. Additional implementation details of RCG's guidance are provided in Appendix A.

**Simple Extension to Class-conditional Generation.** RCG seamlessly enables conditional image generation by training a task-specific conditional RDM. Specifically, a class embedding is integrated into each fully-connected block of the RDM, in addition to the timestep embedding. This enables the generation of class-specific representations. The image generator then crafts the image conditioned on the generated representation. As shown in Table 3 and Appendix C, this simple modification allows users to specify the class of the generated image while keeping RCG's superior generative performance, all without the need to retrain the image generator.

## 4 Experiments

We evaluate RCG on the ImageNet 256\(\)256 dataset , which is a common benchmark for image generation and is especially challenging for unconditional generation. Unless otherwise specified, we do not use ImageNet labels in any of the experiments. We generate 50K images and report the Frechet Inception Distance (FID)  and Inception Score (IS)  as the standard metrics for assessing the fidelity and diversity of the generated images. Evaluations of precision and recall are included in Appendix B.1. Unless otherwise specified, we follow the evaluation suite provided by ADM . **All ablations and results on other datasets are included in Appendix B.1.**

### Observations

We extensively evaluate the performance of RCG with various image generators and compare it to the results of state-of-the-art unconditional and conditional image generation methods. Several intriguing properties are observed.

**RCG significantly improves the unconditional generation performance of current generative models.** In Table 1, we evaluate the proposed RCG framework using different image generators. The results demonstrate that conditioning on generated representations substantially improves the performance of all image generators in unconditional generation. Specifically, it reduces the FID for unconditional LDM-8, ADM, DiT-XL/2, MAGE-B, and MAGE-L by 71%, 76%, 82%, 54%, and 51%, respectively. We further show that such improvement is also universal across different datasets, as demonstrated by the results on CIFAR-10 and iNaturalist in Appendix B.1. These findings confirm that RCG markedly boosts the performance of current generative models in unconditional generation, significantly reducing their reliance on human-annotated labels.

Moreover, such outstanding performance can be achieved with lower training cost compared to current generative models. In Figure 5, we compare the training cost and unconditional generation FIDs

 Unconditional generation & FID\(\) & IS\(\) \\   & baseline & 39.13 & 22.8 \\  & **w/ RCG** & **11.30 \((-\)27.83)** & **101.9 \((+\)79.1)** \\   & baseline & 26.21 & 39.7 \\  & **w/ RCG** & **6.24 \((-\)19.97)** & **136.9 \((+\)97.2)** \\   & baseline & 27.32 & 35.9 \\  & **w/ RCG** & **4.89 \((-\)22.43)** & **143.2 \((+\)107.3)** \\   & baseline & 8.67 & 94.8 \\  & **w/ RCG** & **3.98 \((-\)4.69)** & **177.8 \((+\)83.0)** \\   & baseline & 7.04 & 123.5 \\  & **w/ RCG** & **3.44 \((-\)3.60)** & **186.9 \((+\)63.4)** \\ 

Table 1: **RCG significantly improves the unconditional generation performance of current generative models**, evaluated on ImageNet 256\(\)256. All numbers are reported under the unconditional generation setting.

of RCG and current generative models. RCG achieves a significantly lower FID with less training cost than current generative models. Specifically, MAGE-B with RCG achieves an unconditional generation FID of 4.87 in less than a day when trained on 64 V100 GPUs. This demonstrates that decomposing the complex tasks of unconditional generation into much simpler sub-tasks can significantly simplify the data modeling process.

**RCG largely improves the state-of-the-art in unconditional image generation.** In Table 2, we compare MAGE with RCG and previous state-of-the-art methods in unconditional image generation. As shown in Figure 8 and Table 2, RCG can generate images with both high fidelity and diversity, achieving an FID of 3.44 and an Inception Score of 186.9. These results are further enhanced with the guided version of RCG (RCG-G), which reaches an FID of 2.15 and an Inception Score of 253.4, significantly surpassing previous methods of unconditional image generation.

**RCG's unconditional generation performance rivals leading methods in class-conditional image generation.** In Table 3, we perform a system-level comparison between the _unconditional_ RCG and state-of-the-art _class-conditional_ image generation methods. MAGE-L with RCG is comparable to leading class-conditional methods, with and without guidance. These results demonstrate that RCG, for the first time, improves the performance of unconditional image generation on complex data distributions to the same level as that of state-of-the-art class-conditional generation methods, effectively bridging the historical gap between class-conditional and unconditional generation.

In Table 4, we further conduct an apple-to-apple comparison between the class-conditional versions of LDM-8, ADM, and DiT-XL/2 and their unconditional counterparts using RCG. Surprisingly, with RCG, these generative models consistently outperform their class-conditional versions by a noticeable margin. This demonstrates that the rich semantic information from the unconditionally generated representations can guide the generative process even more effectively than class labels.

 Unconditional generation & \#params & FID\(\) & IS\(\) \\  BigGAN  & \(\)70M & 38.61 & 24.7 \\ ADM  & 554M & 26.21 & 39.7 \\ MaskGIT  & 227M & 20.72 & 42.1 \\ RCDM\({}^{}\) & - & 19.0 & 51.9 \\ IC-GAN\({}^{}\) & \(\)75M & 15.6 & 59.0 \\ ADDP  & 176M & 8.9 & 95.3 \\ MAGE-B  & 176M & 8.67 & 94.8 \\ MAGE-L  & 439M & 7.04 & 123.5 \\ RDM-IN\({}^{}\) & 400M & 5.91 & 158.8 \\ 
**RCG** (MAGE-B) & 239M & 3.98 & 177.8 \\
**RCG** (MAGE-L) & 502M & 3.44 & 186.9 \\
**RCG-G** (MAGE-B) & 239M & 3.19 & 212.6 \\
**RCG-G** (MAGE-L) & 502M & **2.15 & 253.4** \\ 

Table 2: **RCG largely improves the state-of-the-art in unconditional image generation** on ImageNet 256\(\)256. All numbers are reported under the unconditional generation setting. Following common practice, we report the number of parameters used during generation. \(\) denotes semi-parametric methods which require ground-truth ImageNet images during generation.

Figure 5: **RCG achieves outstanding unconditional generation performance with less training cost.** All numbers are reported under the unconditional generation setting. The training cost is measured using a cluster of 64 V100 GPUs. Given that the MoCo v3 ViT encoder is pre-trained and not needed for generation, its training cost is excluded. Detailed computational cost is reported in Appendix B.1.

As shown in Table 3 and Appendix C, RCG also supports class-conditional generation with a simple extension. Our representation diffusion model can easily adapt to class-conditional representation generation, thereby enabling RCG to also adeptly perform class-conditional image generation. This result demonstrates the effectiveness of RCG in leveraging its superior unconditional generation performance to benefit downstream conditional generation tasks.

Importantly, such an adaptation does not require retraining the representation-conditioned image generator. For any new conditioning, only the lightweight representation generator needs to be retrained. This potentially enables pre-training of the self-supervised encoder and image generator on large-scale unlabeled datasets, and task-specific training of conditional representation generator on a small-scale labeled dataset. We believe that this property, similar to self-supervised learning, allows RCG to both benefit from large unlabeled datasets and adapt to various downstream generative tasks with minimal overheads. We leave the exploration on this direction to future work.

### Qualitative Insights

In this section, we showcase the visualization results of RCG, providing insights into its superior generative capabilities. Figure 8 illustrates RCG's unconditional image generation results on ImageNet 256\(\)256. The model is capable of generating both diverse and high-quality images without relying on human annotations. The high-level semantic diversity in RCG's generation stems from

 Methods &  &  \\    & w/ class labels & 17.41 & 72.9 \\  & **w/ RCG** & **11.30** & **101.9** \\   & w/ class labels & 10.94 & 101.0 \\  & **w/ RCG** & **6.24** & **136.9** \\   & w/ class labels & 9.62 & 121.5 \\  & **w/ RCG** & **4.89** & **143.2** \\  

Table 4: **Apple-to-apple comparison: RCG’s unconditional generation outperforms the class-conditional counterparts of current generative models, evaluated on ImageNet 256\(\)256. MAGE does not report its class-conditional generation performance. Class-conditional results are marked in gray.**

 Methods &  &  \\ Methods & \#params & FID\(\) & IS\(\) & FID\(\) & IS\(\) \\   _Class-conditional_ & & & & & \\ ADM  & 554M & 10.94 & 101.0 & 4.59 & 186.7 \\ LDM-4  & 400M & 10.56 & 103.5 & 3.60 & 247.7 \\ U-ViT-H/2-G  & 501M & - & - & 2.29 & 263.9 \\ DiT-XL/2  & 675M & 9.62 & 121.5 & 2.27 & 278.2 \\ DiffiT  & - & - & - & 1.73 & 276.5 \\ BigGAN-deep  & 160M & 6.95 & 198.2 & - & - \\ MaskGIT  & 227M & 6.18 & 182.1 & - & - \\ MDTV2-XL/2  & 676M & 5.06 & 155.6 & **1.58** & 314.7 \\ CDM  & - & 4.88 & 158.7 & - & - \\ MAGVIT-v2  & 307M & 3.65 & 200.5 & 1.78 & **319.4** \\ RIN  & 410M & 3.42 & 182.0 & - & - \\ VDM++  & 2B & **2.40** & **225.3** & 2.12 & 267.7 \\ RCG, conditional (MAGE-L) & 512M & 2.99 & 215.5 & 2.25 & 300.7 \\   & & & & \\
**RCG (MAGE-L)** & **502M** & **3.44** & **186.9** & **2.15** & **253.4** \\ 

Table 3: **System-level comparison: RCG’s unconditional generation performance rivals leading methods in class-conditional image generation on ImageNet 256\(\)256. Following common practice, we report the number of parameters used during generation. Class-conditional results are marked in gray.**its representation generator, which models the distribution of representations and samples them with varied semantics. By conditioning on these representations, the complex data distribution is broken down into simpler, representation-conditioned sub-distributions. This decomposition significantly simplifies the task for the image generator, leading to the production of high-quality images.

Besides high-quality generation, the image generator can also introduce significant low-level diversity in the generative process. Figure 6 illustrates RCG's ability to generate diverse images that semantically align with each other, given the same representation from the reference image. The images generated by RCG can capture the semantic essence of the reference images while differing in specific details. This result highlights RCG's capability to leverage semantic information in representations to guide the generative process, without compromising the diversity that is important in unconditional image generation.

Figure 7 further showcases RCG's semantic interpolation ability, demonstrating that the representation space is semantically smooth. By leveraging RCG's dependency on representations, we can semantically transition between two images by linearly interpolating their respective representations. The interpolated images remain realistic across varying interpolation rates, and their semantic contents smoothly transition from one image to another. For example, interpolating between an image of "Tibetan mastiff" and an image of "wool" could generate a novel image featuring a dog wearing a woolen sweater. This also highlights RCG's potential in manipulating image semantics within a low-dimensional representation space, offering new possibilities to control image generation.

Figure 6: RCG can generate images with diverse appearances but similar semantics from the same representation. We extract representations from reference images and, for each representation, generate a variety of images from different random seeds.

Figure 7: RCG’s results conditioned on interpolated representations from two images. The semantics of the generated images gradually transfer between the two images.

Discussion

Computer vision has entered a new era where learning from extensive, unlabeled datasets is becoming increasingly common. Despite this trend, the training of image generation models still mostly relies on labeled datasets, which could be attributed to the large performance gap between conditional and unconditional image generation. Our paper addresses this issue by exploring _Representation-Conditioned Generation_, which we propose as a nexus between conditional and unconditional image generation. We demonstrate that the long-standing performance gap can be effectively bridged by generating images conditioned on self-supervised representations and leveraging a representation generator to model and sample from this representation space. We believe this approach has the potential to liberate image generation from the constraints of human annotations, enabling it to fully harness the vast amounts of unlabeled data and even generalize to modalities that are beyond the scope of human annotation capabilities.

**Acknowledgements.** This work was supported by the GIST MIT Research Collaboration grant funded by GIST. Tianhong Li was also supported by the Mathworks Fellowship. We thank Huiwen Chang, Saining Xie, Zhuang Liu, Xinlei Chen, and Mike Rabbat for their discussion and feedback. We also thank Xinlei Chen for his support on MoCo v3.