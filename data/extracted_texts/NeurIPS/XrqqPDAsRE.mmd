# A Randomized Approach to Tight Privacy Accounting

Jiachen T. Wang

Princeton University

tianhaowang@princeton.edu

&Saeed Mahloujifar

Princeton University

sfar@princeton.edu

&Tong Wu

Princeton University

tongwu@princeton.edu

&Ruoxi Jia

Virginia Tech

ruoxijia@vt.edu

&Prateek Mittal

Princeton University

pmittal@princeton.edu

###### Abstract

Bounding privacy leakage over compositions, i.e., privacy accounting, is a key challenge in differential privacy (DP). However, the privacy parameter (\(\) or \(\)) is often easy to estimate but hard to bound. In this paper, we propose a new differential privacy paradigm called estimate-verify-release (EVR), which tackles the challenges of providing a strict upper bound for the privacy parameter in DP compositions by converting an _estimate_ of privacy parameter into a formal guarantee. The EVR paradigm first verifies whether the mechanism meets the _estimated_ privacy guarantee, and then releases the query output based on the verification result. The core component of the EVR is privacy verification. We develop a randomized privacy verifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based DP accountant that outperforms existing DP accounting techniques in terms of accuracy and efficiency. MC-based DP verifier and accountant is applicable to an important and commonly used class of DP algorithms, including the famous DP-SGD. An empirical evaluation shows the proposed EVR paradigm improves the utility-privacy tradeoff for privacy-preserving machine learning.

## 1 Introduction

The concern of privacy is a major obstacle to deploying machine learning (ML) applications. In response, ML algorithms with differential privacy (DP) guarantees have been proposed and developed. For privacy-preserving ML algorithms, DP mechanisms are often repeatedly applied to private training data. For instance, when training deep learning models using DP-SGD , it is often necessary to execute sub-sampled Gaussian mechanisms on the private training data thousands of times.

A major challenge in machine learning with differential privacy is _privacy accounting_, i.e., measuring the privacy loss of the composition of DP mechanisms. A privacy accountant takes a list of mechanisms, and returns the privacy parameter (\(\) and \(\)) for the composition of those mechanisms. Specifically, a privacy accountant is given a target \(\) and finds the _smallest achievable_\(\) such that the composed mechanism \(\) is \((,)\)-DP (we can also fix \(\) and find \(\)). We use \(_{}()\) to denote the smallest achievable \(\) given \(\), which is often referred to as _optimal privacy curve_ in the literature.

Training deep learning models with DP-SGD is essentially the _adaptive composition_ for thousands of sub-sampled Gaussian Mechanisms. Moment Accountant (MA) is a pioneer solution for privacy loss calculation in differentially private deep learning . However, MA does not provide the optimal \(_{}()\) in general . This motivates the development of more advanced privacy accounting techniques that outperforms MA. Two major lines of such works are based on Fast Fourier Transform (FFT) (e.g.,  and Central Limit Theorem (CLT) [7; 41]. Both techniques can provide an _estimate_as well as an upper bound for \(_{}()\) though bounding the worst-case estimation error. In practice, **only the upper bounds for \(_{}()\) can be used**, as differential privacy is a strict guarantee.

**Motivation: estimates can be more accurate than upper bounds.** The motivation for this paper stems from the limitations of current privacy accounting techniques in providing tight upper bounds for \(_{}()\). Despite outperforming MA, both FFT- and CLT-based methods can provide ineffective bounds in certain regimes . We demonstrate such limitations in Figure 1 using the composition of Gaussian mechanisms. For FFT-based technique , we can see that although it outperforms MA for most of the regimes, the upper bounds (blue dashed curve) are worse than that of MA when \(<10^{-10}\) due to computational limitations (as discussed in 's Appendix A; also see Remark 6 for a discussion of why the regime of \(<10^{-10}\) is important). The CLT-based techniques (e.g., ) also produce sub-optimal upper bounds (red dashed curve) for the entire range of \(\). This is primarily due to the small number of mechanisms used (\(k=1200\)), which does not meet the requirements for CLT bounds to converge (similar phenomenon observed in ). On the other hand, we can see that the _estimates_ of \(_{}()\) from both FFT and CLT-based techniques, which estimate the parameters rather than providing an upper bound, are in fact very close to the ground truth (the three curves overlapped in Figure 1). However, as we mentioned earlier, these accurate estimations cannot be used in practice, as we cannot prove that they do not underestimate \(_{}()\). The dilemma raises an important question: _can we develop new techniques that allow us to use privacy parameter estimates instead of strict upper bounds in privacy accounting?1_

This paper gives a positive answer to it. Our contributions are summarized as follows:

**Estimate-Verify-Release (EVR): a DP paradigm that converts privacy parameter estimate into a formal guarantee.** We develop a new DP paradigm called _Estimate-Verify-Release_, which augments a mechanism with a formal privacy guarantee based on its privacy parameter estimates. The basic idea of EVR is to first verify whether the mechanism satisfies the estimated DP guarantee, and release the mechanism's output if the verification is passed. The core component of the EVR paradigm is **privacy verification**. A DP verifier can be randomized and imperfect, suffering from both false positives (accept an underestimation) and false negatives (reject an overestimation). We show that EVR's privacy guarantee can be achieved when privacy verification has a low false negative rate.

**A Monte Carlo-based DP Verifier.** For an important and widely used class of DP algorithms including Subsampled Gaussian mechanism (the building block for DP-SGD), we develop a Monte

Figure 1: Results of estimating/bounding \(_{}()\) for the composition of 1200 Gaussian mechanisms with \(=70\). ‘-upp’ means upper bound and ‘-est’ means estimate. Curves of ‘Exact’, ‘FFT-est’, and ‘CLT-est’ are overlapped. The groundtruth curve (‘Exact’) for pure Gaussian mechanism can be computed analytically .

Figure 2: An overview of our EVR paradigm. EVR converts an estimated \((,)\) provided by a privacy accountant into a formal guarantee. Compared with the original mechanism, the EVR has an extra failure mode that does not output anything when the estimated \((,)\) is rejected. We show that the MC-based verifier we proposed can achieve negligible failure probability \((O())\) in Section 4.4.

Carlo (MC) based DP verifier for the EVR paradigm. We present various techniques that ensure the DP verifier has both a low false positive rate (for privacy guarantee) and a low false negative rate (for utility guarantee, i.e., making the EVR and the original mechanism as similar as possible).

**A Monte Carlo-based DP Accountant.** We further propose a new MC-based approach for DP accounting, which we call the _MC accountant_. It utilizes similar MC techniques as in privacy verification. We show that the MC accountant achieves several advantages over existing privacy accounting methods. In particular, we demonstrate that MC accountant is efficient for _online privacy accounting_, a realistic scenario for privacy practitioners where one wants to update the estimate on privacy guarantee whenever executing a new mechanism.

Figure 2 gives an overview of the proposed EVR paradigm as well as this paper's contributions.

## 2 Privacy Accounting: a Mean Estimation/Bounding Problem

In this section, we review relevant concepts and introduce privacy accounting as a mean estimation/bounding problem.

**Symbols and notations.** We use \(D,D^{}^{}\) to denote two datasets with an unspecified size over space \(\). We call two datasets \(D\) and \(D^{}\)_adjacent_ (denoted as \(D D^{}\)) if we can construct one by adding/removing one data point from the other. We use \(P,Q\) to denote random variables. We also overload the notation and denote \(P(),Q()\) the density function of \(P,Q\).

**Differential privacy and its equivalent characterizations.** Having established the notations, we can now proceed to formally define differential privacy.

**Definition 1** (Differential Privacy ).: _For \(, 0\), a randomized algorithm \(:^{}\) is \((,)\)-differentially private if for every pair of adjacent datasets \(D D^{}\) and for every subset of possible outputs \(E\), we have \(_{}[(D) E] e^{}_{}[ (D^{}) E]+\)._

One can alternatively define differential privacy in terms of the maximum possible divergence between the output distribution of any pair of \((D)\) and \((D^{})\).

**Lemma 2** ().: _A mechanism \(\) is \((,)\)-DP iff \(_{D D^{}}_{^{}}((D) \|(D^{}))\), where \(_{}(P\|Q):=_{o Q}[(-)_{+}]\) & \((a)_{+}:=(a,0)\)._

\(_{}\) is usually referred as _Hockey-Stick_ (HS) Divergence in the literature. For every mechanism \(\) and every \( 0\), there exists a smallest \(\) such that \(\) is \((,)\)-DP. Following the literature , we formalize such a \(\) as a function of \(\).

**Definition 3** (Optimal Privacy Curve).: _The optimal privacy curve of a mechanism \(\) is the function \(_{}:^{+}\) s.t. \(_{}():=_{D D^{}}_{ ^{}}((D)\|(D^{}))\)._

**Dominating Distribution Pair and Privacy Loss Random Variable (PRV).** It is computationally infeasible to find \(_{}()\) by computing \(_{^{}}((D)\|(D^{ }))\) for all pairs of adjacent dataset \(D\) and \(D^{}\). A mainstream strategy in the literature is to find a pair of distributions \((P,Q)\) that dominates all \(((D),(D^{}))\) in terms of the Hockey-Stick divergence. This results in the introduction of _dominating distribution pair_ and _privacy loss random variable_ (PRV).

**Definition 4** ().: _A pair of distributions \((P,Q)\) is a pair of dominating distributions for \(\) under adjacent relation \(\) if **for all \( 0\)**, \(_{D D^{}}_{}((D)\|(D^{ }))_{}(P\|Q)\). If equality is achieved **for all \( 0\)**, then we say \((P,Q)\) is a pair of tightly dominating distributions for \(\). Furthermore, we call \(Y:=(),o P\) the privacy loss random variable (PRV) of \(\) associated with dominating distribution pair \((P,Q)\)._

Zhu et al.  shows that all mechanisms have a pair of tightly dominating distributions. Hence, we can alternatively characterize the optimal privacy curve as \(_{}()=_{^{}}(P\|Q)\) for the tightly dominating pair \((P,Q)\), and we have \(_{}()_{^{}}(P\|Q)\) if \((P,Q)\) is a dominating pair that is not necessarily tight. The importance of the concept of PRV comes from the fact that we can write \(_{^{}}(P\|Q)\) as an expectation over it: \(_{^{}}(P\|Q)=_{Y}[(1-e^{ -Y})_{+}]\). Thus, one can bound \(_{}()\) by first identifying \(\)'s dominating pair distributions as well as the associated PRV \(Y\), and then computing this expectation. Such a formulation allows us to bound \(_{}()\) without enumerating over all adjacent \(D\) and \(D^{}\). For notation convenience, we denote \(_{Y}():=_{Y}[(1-e^{-Y})_{+ }].\) Clearly, \(_{}_{Y}\). If \((P,Q)\) is a tightly dominating pair for \(\), then \(_{}=_{Y}\).

**Privacy Accounting as a Mean Estimation/Bounding Problem.** Privacy accounting aims to estimate and bound the optimal privacy curve \(_{}()\) for adaptively composed mechanism \(=_{1}_{k}(D)\). The _adaptive composition_ of two mechanisms \(_{1}\) and \(_{2}\) is defined as \(_{1}_{2}(D):=(_{1}(D),_{2}(D,_{1}(D)))\), in which \(_{2}\) can access both the dataset and the output of \(_{1}\). Most of the practical privacy accounting techniques are based on the concept of PRV, centered on the following result.

**Lemma 5** ().: _Let \((P_{j},Q_{j})\) be a pair of tightly dominating distributions for mechanism \(_{j}\) for \(j\{1,,k\}\). Then \((P_{1} P_{k},Q_{1} Q_{k})\) is a pair of dominating distributions for \(=_{1}_{k}\), where \(\) denotes the product distribution. Furthermore, the associated privacy loss random variable is \(Y=_{i=1}^{k}Y_{i}\) where \(Y_{i}\) is the PRV associated with \((P_{i},Q_{i})\)._

Lemma 5 suggests that privacy accounting for DP composition can be cast into a _mean estimation/bounding problem_ where one aims to approximate or bound the expectation in (2) when \(Y=_{i=1}^{k}Y_{i}\). Note that while Lemma 5 does not guarantee a pair of tightly dominating distributions for the adaptive composition, it cannot be improved in general, as noted in . Hence, all the current privacy accounting techniques work on \(_{Y}\) instead of \(_{}\), as Lemma 5 is tight even for non-adaptive composition. Following the prior works, in this paper, we only consider the practical scenarios where Lemma 5 is tight for the simplicity of presentation. That is, we assume \(_{Y}=_{}\) unless otherwise specified.

Most of the existing privacy accounting techniques can be described as different techniques for such a mean estimation problem. **Example-1: FFT-based methods.** This line of works (e.g., ) discretizes the domain of each \(Y_{i}\) and use Fast Fourier Transform (FFT) to speed up the approximation of \(_{Y}()\). The upper bound is derived through the worst-case error bound for the approximation.

**Example-2: CLT-based methods.**[7; 41] use CLT to approximate the distribution of \(Y=_{i=1}^{k}Y_{i}\) as Gaussian distribution. They then use CLT's finite-sample approximation guarantee to derive the upper bound for \(_{Y}()\).

**Remark 6** (**The Importance of Privacy Accounting in Regime \(<10^{-10}\)).: _The regime where \(<10^{-10}\) is of significant importance for two reasons. **(I)**\(\) serves as an upper bound on the chance of severe privacy breaches, such as complete dataset exposure, necessitating a "cryptographically small" value, namely, \(<n^{-(1)}\)[14; 40]. **(2)** Even with the off-used yet questionable guideline of \( n^{-1}\) or \(n^{-1.1}\), datasets of modern scale, such as JFT-3B  or LATION-5B , already comprise billions of records, thus rendering small \(\) values crucial. While we acknowledge that it requires a lot of effort to achieve a good privacy-utility tradeoff even for the current choice of \( n^{-1}\), it is important to keep such a goal in mind._

## 3 Estimate-Verify-Release

As mentioned earlier, upper bounds for \(_{Y}()\) are the only valid options for privacy accounting techniques. However, as we have demonstrated in Figure 1, both FFT- and CLT-based methods can provide overly conservative upper bounds in certain regimes. On the other hand, their _estimates_ for \(_{Y}()\) can be very close to the ground truth even though there is no provable guarantee. Therefore, it is highly desirable to develop new techniques that enable the use of privacy parameter estimates instead of overly conservative upper bounds in privacy accounting.

We tackle the problem by introducing a new paradigm for constructing DP mechanisms, which we call Estimate-Verify-Release (EVR). The key component of the EVR is an object called DP verifier (Section 3.1). The full EVR paradigm is then presented in Section 3.2, where the DP verifier is utilized as a building block to guarantee privacy.

### Differential Privacy Verifier

We first formalize the concept of _differential privacy verifier_, the central element of the EVR paradigm. In informal terms, a DP verifier is an algorithm that attempts to verify whether a mechanism satisfies a specific level of differential privacy.

**Definition 7** (Differential Privacy Verifier).: _We say a differentially private verifier \(()\) is an algorithm that takes the description of a mechanism \(\) and proposed privacy parameter \((,^{})\) as input, and returns \((,,^{})\) if the algorithm believes \(\) is \((,^{})\)-DP (i.e., \(^{}_{Y}()\) where \(Y\) is the PRV of \(\)), and returns \(\) otherwise._

A differential privacy verifier can be imperfect, suffering from both false positives (FP) and false negatives (FN). Typically, FP rate is the likelihood for \(\) to accept \((,^{})\) when \(^{}<_{Y}()\). However, \(^{}\) is still a good estimate for \(_{Y}()\) by being a small (e.g., <10%) underestimate. To account for this, we introduce a smoothing factor, \((0,1]\), such that \(^{}\) is deemed "should be rejected" only when \(^{}_{Y}()\). A similar argument can be put forth for FN cases where we also introduce a smoothing factor \((0,1]\). This leads to relaxed notions for FP/FN rate:

**Definition 8**.: _We say a DPV's \(\)-relaxed false positive rate at \((,^{})\) is_

\[_{}(,^{};):=_{ :^{}<_{Y}()}\;_{ (,,^{ })=}\]

_We say a DPV's \(\)-relaxed false negative rate at \((,^{})\) is_

\[_{}(,^{};):=_{ :^{}>_{Y}()}\;_{ (,,^{ })=}\]

Privacy Verification with DP Accountant.For a composed mechanism \(=_{1}_{k}\), a DP verifier can be easily implemented using any existing privacy accounting techniques. That is, one can execute DP accountant to obtain an estimate or upper bound \((,)\) of the actual privacy parameter. If \(^{}<\), then the proposed privacy level is rejected as it is more private than what the DP accountant tells; otherwise, the test is passed. The input description of a mechanism \(\), in this case, can differ depending on the DP accounting method. For Moment Accountant , the input description is the upper bound of the moment-generating function (MGF) of the privacy loss random variable for each individual mechanism. For FFT and CLT-based methods, the input description is the cumulative distribution functions (CDF) of the dominating distribution pair of each individual \(_{i}\).

### EVR: Ensuring Estimated Privacy with DP Verifier

We now present the full paradigm of EVR. As suggested by the name, it contains three steps: **(1) Estimate:** A privacy parameter \((,^{})\) for \(\) is estimated, e.g., based on a privacy auditing or accounting technique. **(2) Verify:** A DP verifier \(\) is used for validating whether mechanism \(\) satisfies \((,^{})\)-DP guarantee. **(3) Release:** If DP verification test is passed, we can execute \(\) as usual; otherwise, the program is terminated immediately. For practical utility, this rejection probability needs to be small when \((,^{})\) is an accurate estimation. The procedure is summarized in Algorithm 1.

```
1:Input:\(\): mechanism. \(D\): dataset. \((,^{})\): an estimated privacy parameter for \(\).
2:if\((,,^{})\) outputs \(\)then Execute \((D)\).
3:else Print \(\). ```

**Algorithm 1** Estimate-Verify-Release (EVR) Framework

Given estimated privacy parameter \((,^{})\), we have the privacy guarantee for the EVR paradigm:

**Theorem 9**.: _Algorithm 1 is \((,^{}/)\)-DP for any \(>0\) if \(_{}(,^{};)^{ }/\)._

We defer the proof to Appendix B. The implication of this result is that, for any _estimate_ of the privacy parameter, one can safely use it as a DPV with a bounded false positive rate would enforce differential privacy. However, this is not enough: an overly conservative DPV that satisfies 0 FP rate but rejects everything would not be useful. When \(^{}\) is accurate, we hope the DPV can also achieve a small _false negative rate_ so that the output distributions of EVR and \(\) are indistinguishable. We discuss the instantiation of DPV in Section 4.

## 4 Monte Carlo Verifier of Differential Privacy

As we can see from Section 3.2, a DP verifier (\(\)) that achieves a small FP rate is the central element for the EVR framework. In the meanwhile, it is also important that DPV has a low FN rate inorder to maintain the good utility of the EVR when the privacy parameter estimate is accurate. In this section, we introduce an instantiation of DPV based on the Monte Carlo technique that achieves both a low FP and FN rate, assuming the PRV is known for each individual mechanism.

**Remark 10** (**Mechanisms where PRV can be derived**).: _PRV can be derived for many commonly used DP mechanisms such as the Laplace, Gaussian, and Subsampled Gaussian Mechanism . In particular, our DP verifier applies for DP-SGD, one of the most important application scenarios of privacy accounting. Moreover, the availability of PRV is also the assumption for most of the recently developed privacy accounting techniques (including FFT- and CLT-based methods). The extension beyond these commonly used mechanisms is an important future work in the field._

**Remark 11** (**Previous studies on the hardness of privacy verification**).: _Several studies  have shown that DP verification is an NP-hard problem. However, these works consider the setting where the input description of the DP mechanism is its corresponding randomized Boolean circuits. Some other works  show that DP verification is impossible, but this assertion is proved for the black-box setting where the verifier can only query the mechanism. Our work gets around this barrier by providing the description of the PRV of the mechanism as input to the verifier._

### dpv through an MC Estimator for \(_{Y}()\)

Recall that most of the recently proposed DP accountants are essentially different techniques for estimating the expectation

\[_{Y=_{i=1}^{k}Y_{i}}()=_{Y}[(1-e^{ -Y})_{+}]\]

where each \(Y_{i}\) is the privacy loss random variable \(Y_{i}=((t)}{Q_{i}(t)})\) for \(t P_{i}\), and \((P_{i},Q_{i})\) is a pair of dominating distribution for individual mechanism \(_{i}\). In the following text, we denote the product distribution \(:=P_{1} P_{k}\) and \(:=Q_{1} Q_{k}\). Recall from Lemma 5 that \((,)\) is a pair of dominating distributions for the composed mechanism \(\). For notation simplicity, we denote a vector \(:=(t^{(1)},,t^{(k)})\).

_Monte Carlo_ (MC) technique is arguably one of the most natural and widely used techniques for approximating expectations. Since \(_{Y}()\) is an expectation in terms of the PRV \(Y\), one can apply MC-based technique to estimate it. Given an MC estimator for \(_{Y}()\), we construct a DPV\((,,^{})\) as shown in Algorithm 2 (instantiated by the Simple MC estimator introduced in Section 4.2). Specifically, we first obtain an estimate \(\) from an MC estimator for \(_{Y}()\). The estimate \(^{}\) passes the test if \(<}}{}-\), and fails otherwise. The parameter \( 0\) here is an offset that allows us to conveniently controls the \(\)-relaxed false positive rate. We will discuss how to set \(\) in Section 4.4.

In the following contents, we first present two constructions of MC estimators for \(_{Y}()\) in Section 4.2. We then discuss the condition for which our MC-based DPV achieves a certain target FP rate in Section 4.3. Finally, we discuss the utility guarantee for the MC-based DPV in Section 4.4.

### Constructing MC Estimator for \(_{Y}()\)

In this section, we first present a simple MC estimator that applies to any mechanisms where we can derive and sample from the dominating distribution pairs. Given the importance of Poisson Subsampled Gaussian mechanism for privacy-preserving machine learning, we further design a more advanced and specialized MC estimator for it based on the importance sampling technique.

**Simple Monte Carlo Estimator.** One can easily sample from \(Y\) by sampling \(\) and output \((()}{()})\). Hence, a straightforward algorithm for estimating (2) is the Simple Monte Carlo (SMC) algorithm, which directly samples from the privacy random variable \(Y\). We formally define it here.

**Definition 12** (Simple Monte Carlo (SMC) Estimator).: _We denote \(}^{m}_{}()\) as the random variable of SMC estimator for \(_{Y}()\) with \(m\) samples, i.e., \(}^{m}_{}():=_{i=1}^{m} (1-e^{-y_{i}})_{+}\) for \(y_{1},,y_{m}\) i.i.d. sampled from \(Y\)._

**Importance Sampling Estimator for Poisson Subsampled Gaussian (Overview).** As \(_{Y}()\) is usually a tiny value (\(10^{-5}\) or even cryptographically small), it is likely that by naive sampling from \(Y\), almost all of the samples in \(\{(1-e^{-y_{i}})_{+}\}_{i=1}^{m}\) are just 0s! That is, the i.i.d. samples \(\{y_{i}\}_{i=1}^{m}\) from \(Y\) can rarely exceed \(\). To further improve the sample efficiency, one can potentially use more advanced MC techniques such as Importance Sampling or MCMC. However, these advanced tools usually require additional distributional information about \(Y\) and thus need to be developed case-by-case.

Poisson Subsampled Gaussian mechanism is the main workhorse behind the DP-SGD algorithm . Given its important role in privacy-preserving ML, we derive an advanced MC estimator for it based on the Importance Sampling technique. Importance Sampling (IS) is a classic method for rare event simulation . It samples from an alternative distribution instead of the distribution of the quantity of interest, and a weighting factor is then used for correcting the difference between the two distributions. The specific design of alternative distribution is complicated and notation-heavy, and we defer the technical details to Appendix C. At a high level, we construct the alternative sampling distribution based on the _exponential tilting_ technique and derive the optimal tilting parameter such that the corresponding IS estimator approximately achieves the smallest variance. Similar to Definition 12, we use \(}^{m}_{}\) to denote the random variable of importance sampling estimator with \(m\) samples.

### Bounding FP Rate

We now discuss the FP guarantee for the \(\) instantiated by \(}^{m}_{}\) and \(}^{m}_{}\) we developed in the last section. Since both estimators are unbiased, by Law of Large Number, both \(}^{m}_{}\) and \(}^{m}_{}\) converge to \(_{Y}()\) almost surely as \(m\), which leads a \(\) with perfect accuracy. Of course, \(m\) cannot go to \(\) in practice. In the following, we derive the required amount of samples \(m\) for ensuring that \(\)-relaxed false positive rate is smaller than \(^{}/\) for \(}^{m}_{}\) and \(}^{m}_{}\). We use \(}_{}\) (or \(}_{}\)) as an abbreviation for \(}^{1}_{}\) (or \(}^{1}_{}\)), the random variable for a single draw of sampling. We state the theorem for \(}^{m}_{}\), and the same result for \(}^{m}_{}\) can be obtained by simply replacing \(}_{}\) with \(}_{}\). We use \(_{}\) to denote the FP rate for \(\) implemented by SMC estimator.

**Theorem 13**.: _Suppose \([(}_{})^{2}]\). \(\) instantiated by \(}^{m}_{}\) has bounded \(\)-relaxed false positive rate \(_{}(,^{};)^{ }/\) with \(m}(/^{})\)._

The proof is based on Bennett's inequality and is deferred to Appendix D. This result suggests that, to improve the computational efficiency of MC-based \(\) (i.e., tighten the number of required samples), it is important to tightly bound \([(}_{})^{2}]\) (or \([(}_{,})^{2}]\)), the second moment of \(}_{}\) (or \(}_{}\)).

**Bounding the Second-Moment of MC Estimators (Overview).** For clarity, we defer the notation-heavy results and derivation of the upper bounds for \([(}_{})^{2}]\) and \([(}_{})^{2}]\) to Appendix E. Our high-level idea for bounding \([(}_{})^{2}]\) is through the RDP guarantee for the composed mechanism \(\). This is a natural idea since converting RDP to upper bounds for \(_{Y}()\) - the first moment of \(}_{}\) - is a well-studied problem [30; 9; 3]. Bounding \([(}_{})^{2}]\) is highly technically involved.

### Guaranteeing Utility

**Overall picture so far.** Given the proposed privacy parameter \((,^{})\), a tolerable degree of underestimation \(\), and an offset parameter \(\), one can now compute the number of samples \(m\) required for the MC-based \(\) such that \(\)-relaxed FP rate to be \(^{}/\) based on the results from Section 4.3 and Appendix E. We have not yet discussed the selection of the hyperparameter \(\). An appropriate \(\) is important for the utility of MC-based \(\). That is, when \(^{}\) is not too smaller than \(_{Y}()\), the probability of being rejected by \(\) should stay negligible. If we set \(\), the \(\) simply rejects everything, which achieves 0 FP rate (and with \(m=0\)) but is not useful at all!

Formally, the utility of a \(\) is quantified by the \(\)-relaxed false negative (FN) rate (Definition 8). While one may be able to bound the FN rate through concentration inequalities, a more convenientway is to pick an appropriate \(\) such that \(_{}\) is _approximately_ smaller than \(_{}\). After all, \(_{}\) already has to be a small value \(^{}/\) for privacy guarantee. The result is stated informally in the following (holds for both \(_{}\) and \(_{}\)), and the involved derivation is deferred to Appendix F.

**Theorem 14** (Informal).: _When \(=0.4(1/-1/)^{}\), then \(_{}(,^{};) _{}(,^{};)\)._

Therefore, by setting \(=0.4(1/-1/)^{}\), one can ensure that \(_{}(,^{};)\) is also (approximately) upper bounded by \((^{}/)\). Moreover, in Appendix, we empirically show that the FP rate is actually a very conservative bound for the FN rate. Both \(\) and \(\) are selected based on the tradeoff between privacy, utility, and efficiency.

The pseudocode of privacy verification for DP-SGD is summarized in Appendix G.

## 5 Monte Carlo Accountant of Differential Privacy

The Monte Carlo estimators \(_{}\) and \(_{}\) described in Section 4.2 are used for implementing DP verifiers. One may already realize that the same estimators can also be utilized to directly implement a DP accountant which _estimates_\(_{Y}()\). It is important to note that with the EVR paradigm, DP accountants are no longer required to derive a strict upper bound for \(_{Y}()\). We refer to the technique of estimating \(_{Y}()\) using the MC estimators as _Monte Carlo accountant_.

**Finding \(\) for a given \(\).** It is straightforward to implement MC accountant when we fix \(\) and compute for \(_{Y}()\). In practice, privacy practitioners often want to do the inverse: finding \(\) for a given \(\), which we denote as \(_{Y}()\). Similar to the existing privacy accounting methods, we use binary search to find \(_{Y}()\) (see Algorithm 3). Specifically, after generating PRV samples \(\{y_{i}\}_{i=1}^{m}\), we simply need to find the \(\) such that \(_{i=1}^{m}(1-e^{-y_{i}})_{+}=\). We do **not** need to generate new PRV samples for different \(\) we evaluate during the binary search; hence the additional binary search is computationally efficient.

**Number of Samples for MC Accountant.** Compared with the number of samples required for achieving the FP guarantee in Section 4.3, one may be able to use much fewer samples to obtain a decent estimate for \(_{Y}()\), as the sample complexity bound derived based on concentration inequality may be conservative. Many heuristics for guiding the number of samples in MC simulation have been developed (e.g., Wald confidence interval) and can be applied to the setting of MC accountants.

Compared with FFT-based and CLT-based methods, MC accountant exhibits the following strength:

**(1) Accurate \(_{Y}()\) estimation in all regimes.** As we mentioned earlier, the state-of-the-art FFT-based method  fails to provide meaningful bounds due to computational limitations when the true value of \(_{Y}()\) is small. In contrast, the simplicity of the MC accountant allows us to accurately estimate \(_{Y}()\) in all regimes.

**(2) Short clock runtime & Easy GPU acceleration.** MC-based techniques are well-suited for parallel computing and GPU acceleration due to their nature of repeated sampling. One can easily utilize PyTorch's CUDA functionality (e.g., torch.randn(size=(k,m)).cuda()*sigma+mu) to significantly boost the computational efficiency for sampling from common distributions such as Gaussian. In Appendix H, we show that when using one NVIDIA A100 GPU, the runtime time of sampling Gaussian mixture \((1-q)(0,^{2})+q(1,^{2})\) can be improved by \(10^{3}\) times compared with CPU-only scenario.

**(3) Efficient _online privacy accounting._** When training ML models with DP-SGD or its variants, a privacy practitioner usually wants to compute a running privacy leakage for _every_ training iteration, and pick the checkpoint with the best utility-privacy tradeoff. This involves estimating \(_{Y^{(i)}}()\) for every \(i=1,,k\), where \(Y^{(i)}:=_{j=1}^{i}Y_{j}\). We refer to such a scenario as _online privacy accounting_2. MC accountant is especially efficient for online privacy accounting. When estimating\(_{Y^{(i)}}()\), one can re-use the samples previously drawn from \(Y_{1},,Y_{i-1}\) that were used for estimating privacy loss at earlier iterations.

These advantages are justified empirically in Section 6 and Appendix H.

## 6 Numerical Experiments

In this section, we conduct numerical experiments to illustrate **(1)** EVR paradigm with MC verifiers enables a tighter privacy analysis, and **(2)** MC accountant achieves state-of-the-art performance in privacy parameter estimation.

### EVR vs Upper Bound

To illustrate the advantage of the EVR paradigm compared with directly using a strict upper bound for privacy parameters, we take the current state-of-the-art DP accountant, the FFT-based method from  as the example.

**EVR provides a tighter privacy guarantee.** Recall that in Figure 1, FFT-based method provides vacuous bound when the ground-truth \(_{Y}()<10^{-10}\). Under the same hyperparameter setting, Figure 3 (a) shows the privacy bound of the EVR paradigm where the \(^{}\) are FFT's estimates. We use the Importance Sampling estimator \(}_{}\) for DP verification. We experiment with different values of \(\). A higher value of \(\) leads to tighter privacy guarantee but longer runtime. For fair comparison, the EVR's output distribution needs to be almost indistinguishable from the original mechanism. We set \(=(1+)/2\) and set \(\) according to the heuristic from Theorem 14. This guarantees that, as long as the estimate of \(^{}\) from FFT is not a big underestimation (i.e., as long as \(^{}_{Y}()\)), the failure probability of the EVR paradigm is negligible (\(O(_{Y}())\)). The 'FFT-EVR' curve in Figure 3 (a) is essentially the 'FFT-est' curve in Figure 1 scaled up by \(1/\). As we can see, EVR provides a significantly better privacy analysis in the regime where the 'FFT-upp' is unmeaningful (\(<10^{-10}\)).

**EVR incurs little extra runtime.** In Figure 3 (b), we plot the runtime of the Importance Sampling verifier in Figure 3 (b) for different \( 0.9\). Note that for \(>0.9\), the privacy curves are indistinguishable from 'Exact' in Figure 3 (a). The runtime of EVR is determined by the number of samples required to achieve the target \(\)-relaxed FP rate from Theorem 13. Smaller \(\) leads to faster DP verification. As we can see, even when \(=0.99\), the runtime of DP verification in the EVR is \(<2\) minutes. This is attributable to the sample-efficient IS estimator and GPU acceleration.

**EVR provides better privacy-utility tradeoff for Privacy-preserving ML with minimal time consumption.** To further underscore the superiority of the EVR paradigm in practical applications, we illustrate the privacy-utility tradeoff curve when finetuning on CIFAR100 dataset with DP-SGD. As shown in Figure 4, the EVR paradigm provides a lower test error across all privacy budget \(\) compared with the traditional upper bound method. For instance, it achieves around 7% (relative) error reduction when \(=0.6\). The runtime time required for privacy verification is less than \(<10^{-10}\) seconds for all \(\), which is negligible compared to the training time. We provide additional experimental results in Appendix H.

### MC Accountant

We evaluate the MC Accountant proposed in Section 5. We focus on privacy accounting for the composition of Poisson Subsampled Gaussian mechanisms, the algorithm behind the famous DP-SGD algorithm . The mechanism is specified by the noise magnitude \(\) and subsampling rate \(q\).

Figure 3: Privacy analysis and runtime of the EVR paradigm. The settings are the same as Figure 1. For (a), when \(>0.9\), the curves are indistinguishable from ‘Exact’. For fair comparison, we set \(=(1+)/2\) and set \(\) according to Theorem 14, which ensures EVR’s failure probability of the order of \(\). For (b), the runtime is estimated on an NVIDIA A100-SXM4-80GB GPU.

**Settings.** We consider two practical scenarios of privacy accounting: **(1) Offline accounting** which aims at estimating \(_{Y^{(k)}}()\), and **(2) Online accounting** which aims at estimating \(_{Y^{(i)}}()\) for all \(i=1,,k\). For space constraint, we only show the results of online accounting here, and defer the results for offline accounting to Appendix H. **Metric: Relative Error.** To easily and fairly evaluate the performance of privacy parameter estimation, we compute the _almost exact_ (yet computationally expensive) privacy parameters as the ground-truth value. The ground-truth value allows us to compute the _relative error_ of an estimate of privacy leakage. That is, if the corresponding ground-truth of an estimate \(\) is \(\), then the relative error \(r_{}=|-|/\). **Implementation.** For MC accountant, we use the IS estimator described in Section 4.2. For baselines, in addition to the FFT-based and CLT-based method we mentioned earlier, we also examine AFA  and GDP accountant . For a fair comparison, we adjust the number of samples for MC accountant so that the runtime of MC accountant and FFT is comparable. Note that we compared with the privacy parameter _estimates_ instead of upper bounds from the baselines. Detailed settings for both MC accountant and the baselines are provided in Appendix H.

**Results for Online Accounting: MC accountant is both more accurate and efficient.** Figure 5 (a) shows the online accounting results for \((,,q)=(1.0,10^{-9},10^{-3})\). As we can see, MC accountant outperforms all of the baselines in estimating \(_{Y}()\). The sharp decrease in FFT at approximately 250 steps is due to the transition of FFT's estimates from underestimating before this point to overestimating after. Figure 5 (b) shows that MC accountant is around 5 times faster than FFT, the baseline with the best performance in (a). This showcases the MC accountant's efficiency and accuracy in online setting.

## 7 Conclusion & Limitations

This paper tackles the challenge of deriving provable privacy leakage upper bounds in privacy accounting. We present the estimate-verify-release (EVR) paradigm which enables the safe use of privacy parameter estimate. **Limitations.** Currently, our MC-based DP verifier and accountant require known and efficiently samplable dominating pairs and PRV for the individual mechanism. Fortunately, this applies to commonly used mechanisms such as Gaussian mechanism and DP-SGD. Generalizing MC-based DP verifier and accountant to other mechanisms is an interesting future work.

Figure 4: Utility-privacy tradeoff curve for fine-tuning ImageNet-pretrained BEiT  on CIFAR100 when \(=10^{-5}\). We follow the training procedure from .

Figure 5: Experiment for Composing Subsampled Gaussian Mechanisms in the Online Setting. (a) Compares the relative error in approximating \(k_{Y}()\). The error bar for MC accountant is the variance taken over 5 independent runs. Note that the y-axis is in the log scale. (b) Compares the cumulative runtime for online privacy accounting. We did not show AFA  as it does not terminate in 24 hours.