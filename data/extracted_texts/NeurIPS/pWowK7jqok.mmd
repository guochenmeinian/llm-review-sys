# E-Motion: Future Motion Simulation via Event Sequence Diffusion

Song Wu \({}^{1}\), Zhiyu Zhu \({}^{2}\), Junhui Hou \({}^{2}\), Guangming Shi \({}^{1}\), Jinjian Wu \({}^{1}\)

\({}^{1}\) Xidian University, \({}^{2}\) City University of Hong Kong

swu_666@stu.xidian.edu.cn,zhiyuzhu2-c@my.cityu.edu.hk

jh.hou@cityu.edu.hk,gmshi@xidian.edu.cn,jinjian.wu@mail.xidian.edu.cn

This work was supported in part by National Key Research and Development Program of China(2023YFA1008500), in part by NSFC Excellent Young Scientists Fund 62422118, and in part by Hong Kong Innovation and Technology Fund ITS/164/23. The first two authors contributed to this paper equally. Corresponding author: Jinjian Wu.

###### Abstract

Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems. The source code is publicly available at https://github.com/p4r4mount/E-Motion.

## 1 Introduction

Accurately capturing and interpreting dynamic scenes under fluctuating motion and illumination conditions remains an enduring challenge in computer vision . This challenge is particularly pronounced in real-world settings, where subtle variations can dramatically affect the perception and analysis of future motion . Traditional imaging modalities often struggle to capture these nuances, leading to a gap in accurately modeling and predicting motion flow in complex visual environments.

The rapid advancements in deep learning have catalyzed transformative developments in computer vision, particularly in the generative models . Video diffusion models , which stand at the forefront of these innovations, leverage stochastic diffusion processes to generate, restore, and accurately manipulate video content. These models, emblematic of the state-of-the-art in temporal data processing, offer refined capabilities for complex video-based tasks , underscoring the significant strides made in understanding and interpreting dynamic visual scenes.

Coming into the high temporal resolution field, event data stands as a revolutionary sensing approach [37; 12; 53] that significantly mitigates this gap and consistently captures even very subtle fluctuations. Such strong capacity comes from the unique sensing pattern of the event camera, which asynchronously measures the intensity variation in high temporal resolution. It allows for the precise detection of miniature illumination changes, providing a rich, granular record that traditional cameras simply cannot offer [67; 68; 7].

To enable the video diffusion model to completely and correctly learn concise motion information for the estimation of potential future movement, integrating it with event data to realize an event sequence generation model is a potential solution that may inject high-frequency motion information into the video diffusion model. This paper delves into the symbiosis of video diffusion models and high-temporal-resolution event data, exploring its potential to redefine motion forecasting in computer vision. We commence by delineating the landscape of video diffusion models and the mechanics of event-based sensing, elucidating their complementary strengths. Subsequently, we introduce a novel framework that amalgamates these technologies, aiming to augment the precision of motion flow predictions. Through comprehensive experimentation, we validate the effectiveness of our methodology, demonstrating its superior performance across diverse scenarios. Our findings illuminate the path forward, showcasing the integration of video diffusion models with event data as a robust, innovative solution for capturing and interpreting the complexities of dynamic scenes with unparalleled detail and accuracy.

In summary, the contribution of this paper lies in the following three parts:

* we make the first attempt to integrate event-sequences with a video diffusion model, resulting in an event-sequence diffusion model, which could potentially estimate future object motion, given by a certain event prompt;
* we propose to align the pre-trained event-sequence diffusion model with the real-world motion via a reinforcement learning process, which stabilizes the results generated by the diffusion model and makes them more closely resemble real motion.
* we integrate a test-time prompt augmentation method to make use of high temporal resolution event sequence prompt to enhance the generation performance.

## 2 Related Work

### Event-based Vision

Event-based vision represents a paradigm shift from traditional frame-based imaging, offering a dynamic and highly granular approach to capturing visual information. Unlike conventional cameras that record static frames at fixed intervals, the event-based sensor (developed by Lichtsteiner _et al._ and further elaborated by Posch _et al._) asynchronously records intensity changes per pixel and generates a signal termed an "event" whenever the intensity surpasses a threshold, thereby providing a continuous stream of data that reflects temporal changes with remarkable precision. This

Figure 1: Illustration that the exceptional temporal resolution afforded by event cameras, alongside their distinctive event-driven sensing paradigm, presents a significant opportunity for advancing the precision in predicting future motion trajectories.

method is particularly effective in environments with rapid motion or varying illumination, where traditional cameras suffer from motion blur and latency issues.

Recent advancements in this field have focused on leveraging the high temporal resolution of event data for various applications, including high-speed tracking [57; 30; 65], dynamic scene reconstruction [43; 23; 69], and optical flow estimation [44; 9; 22]. Works by Gallego _et al._ and Rebecq _et al._ have been instrumental in demonstrating the utility of event-based data in reconstructing high-speed phenomena and enhancing motion analysis, setting a solid foundation for our research.

### Multimodal Diffusion Models

Generative diffusion models, introduced by Sohl-Dickstein _et al._, represent a class of probabilistic generative models that simulate the gradual transformation of data from a complex distribution into a simpler, typically Gaussian distribution, and vice versa. [8; 25; 61; 4]. This process, characterized by a series of forward and reverse diffusion steps, has been applied successfully to a range of tasks, including image synthesis [20; 34], restoration [29; 21], and, more recently, temporal data manipulation [32; 15].

The application of diffusion models to video data, as explored by Ho _et al._ and extended by others, marks a significant advancement in the field, offering new pathways for the generation and manipulation of dynamic scenes. These models have shown exceptional promise in capturing the temporal continuity and complexity inherent in video data, providing a robust framework for tasks such as video prediction and temporal interpolation.

In recent years, the development of multimodal diffusion technology has advanced rapidly. Researchers are dedicated to applying the powerful generative capabilities of diffusion to different modalities with unique advantages, such as optical flow and depth. Saxena et al.  was the first to apply diffusion models to optical flow and depth estimation. For the characteristics of training data, they introduced infilling, step-rolling, and L1 loss during training to mitigate distribution shifts between training and inference. To address the lack of ground truth in datasets, they also used a large amount of synthetic data for self-supervised pretraining, enabling the diffusion model to acquire reliable knowledge. Chen et al.  utilized the motion information embedded in control signals such as edges and depth maps to achieve more precise control over the text-to-video (T2V) process. They used pixel residuals and optical flow to extract motion-prior information to ensure continuity in video generation. Additionally, they proposed a first-frame generator to integrate semantic information from text and images.

Despite the extensive body of research within the domain of temporal analysis, it is important to acknowledge that the majority of these studies focus primarily on the domain of RGB images and videos. As previously discussed, the superior temporal resolution offered by event data holds significant potential for enhancing the alignment process. Consequently, it is imperative to undertake a thorough investigation into the application and adaptation of existing pre-trained diffusion models to the realm of event data.

## 3 Preliminary

**Diffusion Models** are a class of generative models that simulate the gradual transformation of data from a complex, high-dimensional distribution to a simpler, typically Gaussian distribution through a process known as forward diffusion [48; 18; 46; 47]. Conversely, the reverse diffusion process aims to reconstruct the original data distribution from the simpler one. This mechanism is inspired by thermodynamic processes and has been increasingly applied in the field of deep learning for generating high-quality, diverse samples from complex distributions.

The mathematical foundation of diffusion models is rooted in stochastic differential equations (SDEs), which describe the forward and reverse diffusion processes. By score-based formulation , the forward process of diffusion model acts as

\[d=(,t)dt+g(t)d,\] (1)

where \(^{n}\) indicates the state of reconstructed signal, \(^{n}\) represents a standard Wiener process \((g(t)d(0,g(t)^{2}d))\), \((,t):^{n}^{n}\) and \(g():\) represent the drift and diffusion coefficients, respectively. Moreover, in the evaluating phase, the reverse (inference) process could be illustrated as iteratively performing the following ODE step:

\[d=[(,t)-g(t)^{2}_{}logp( )]dt,\] (2)

where \(_{}logp()\) is usually approximated by a learnable score model \(S_{}(,t)\). Based on the theoretical formulation of stable video diffusion , e.g., variance exploding (VE) diffusion process , the reverse process is acted as

\[_{t-1}=_{t}-_{t}-_{}(_ {t},t)}{_{t}}(_{t}-_{t-1}),\] (3)

where \(_{}()\) is one of parametrization method of \(S_{}()\), which estimate the clean image from noise latent \(_{t}(_{0},_{t}^{2})\).

## 4 Methods

The endeavor of video diffusion presents multifaceted challenges, notably the simultaneous generation of sequences that not only manifest dynamic, convincing motion but also maintain authentic textures. The utilization of event data, characterized by its intrinsic high temporal resolution, emerges as a strategic solution for the precise modeling of object motion. In contrast to traditional stable video diffusion models, which are typically initialized using a singular image or textual prompt, our proposed methodology, designated as the Event-Sequence Diffusion Network, capitalizes on a succinct sequence of events as its conditioning input. This novel approach is illustrated in Fig. 2.

As elaborated in Sec. 4.1, we delineate the comprehensive pre-training regimen that facilitates the learning of object motion through the prediction of subsequent events. Nevertheless, due to the intrinsic diversity-generating property of diffusion models, they commonly yield several distinct samples from a single input prompt. Although these variations may each seem feasible, there is no assurance that they align consistently with the actual dynamics of real-world motion. Thus, in Sec. 4.2, we augment the quality of generation by incorporating an alignment process. It employs reinforcement learning techniques to impose a structured regularization on the generative process of the diffusion models, thereby enhancing the fidelity and coherence of the produced sequences.

### Learning Motion Prior via Pretraining on Event Sequences

An initial formatting process is discussed to retain its high-temporal resolution properties effectively and to facilitate integration with pre-existing large-scale models. Furthermore, to mitigate the substantial computational demands associated with video diffusion models, we have implemented a prompt sampler designed to enhance the efficiency of information encapsulation derived from event

Figure 2: Inference workflow of the proposed method, where the left upper one indicates the random Gaussian noise, left lower one represents the prompted event sequence. We perform \(\) steps forward diffusion processing on the event prompt and substitute a portion of the diffusion input noise, followed by \(T-\) Steps of conventional denoising.

sequence diffusion frameworks. Subsequently, we will delineate these critical aspects in a detailed manner.

**Event Representation.** To leverage contemporary video diffusion models for event data generation, we adopt a strategy where both event information and corresponding images are concurrently inputted into the video diffusion framework, which then processes adaptively sampled outcomes. Specifically, as illustrated in Fig. 2, for a given event denoted as \(=\{h,w,p,t\}\), we consolidate the event data into a voxel grid representation , symbolized as \(}^{B H W}\), where B denotes the number of time bins. In order to utilize the rich pre-training information from RGB frames, we set \(B=3\). This approach ensures that the video diffusion model can effectively interpret and integrate the high-dimensional event data alongside conventional image inputs, facilitating a more comprehensive synthesis of dynamic visual content.

**Pretraining.** The training regimen for our proposed approach is similar with that of diffusion models [48; 24]. This methodology involves estimating the clean image from perturbed samples, as

\[_{pre}=_{t}\{(t)_{_{0}}_{_{t}|_{0}}[||x_{0}-_{}(_{t},t)||_{2}^{2}]\},\] (4)

where \((t)\) denotes a weighting function, parameterized as \(^{2}+_{data}^{2}}{(_{1}+_{data})^{2}}\). During training, we randomly select a set of intermediate states \(_{t}\) and apply regularization techniques to guide them towards the accurate estimation of the underlying noise component \(\). Moreover, to make the diffusion network adaptively capture the object motion with different time windows, in the training process we randomly augment the diffusion network with the voxel from different time ranges. (_Please refer to the Appendix Sec. A for more details_)

**High-temporal Resolution Guided Sampling.** Owing to the unique operational mechanism of event cameras, which detect changes in intensity rather than capturing static images, the clarity and definition of recorded objects can significantly diminish if the chosen temporal window is either too brief or excessively prolonged, especially when we only feed one prompt, like a standard stable video diffusion network. This characteristic often results in blurred or indistinct imagery when the temporal resolution does not align optimally with the scene's dynamics.

Inspired by the recent advancement of guided sampling , we propose to aggregate multiple high-temporal resolution event frames as a test-time prompt. Specifically, as illustrated in Fig. 2, during the inference process, we feed \(s>1\) frames instead of a single frame as prompt \(_{0}^{pm}^{s h w 3}\). For the step \(T\), both \(_{0}^{pm}\) and \(_{0}^{vc}\) are initialized with the random Gaussian noise. However, for the step \(t<\), we set \(_{t-1}^{pm}_{0}^{pm}+_{t}\), Subsequently, the noised event prompt \(_{t-1}^{pm}\) replaces the first \(s\) random noises in the noise tensor \(_{t}^{vc}\), providing motion priors for the denoising process. (_Please refer to the Appendix Algorithm 2 for more details_)

### Motion Alignment via Reinforcement Learning

Given the inherent challenges in precisely tailoring diffusion models to fit the entirety of the training data, particularly when considering the disparity between the model's size and the volume of the dataset, it is pragmatic to direct potential losses towards regions less perceptible to end-users or higher-level algorithms. Furthermore, the multi-step nature of the diffusion generation process renders the simultaneous training of the entire pipeline nearly unfeasible. To address this, we employ a strategy of reinforced optimization, conceptualizing the generation process as a Markov chain. This approach underscores the importance of guiding the diffusion process to yield results of superior quality, thereby optimizing the model's performance while accommodating its structural and computational complexities. Optimizing such a diffusion model starts with the following equations:

\[_{}\ =_{_{}()}( )d,\] (5)

where \(_{}()\) indicates the distribution of reconstructed samples under the model weight \(\). Although the diffusion model is a probabilistic model, its weights are generally deterministic.  The randomness generally comes from Gaussian sampling. Thus, we adopt the same measurement as  to utilize the Gaussian density function \((_{t}|_{t},)}e^{ -_{t}-(_{t},1))^{2}}{2^{2}}}\) to measure the \(_{}()\) of a given sample \(_{t}\), where \(()\) indicates the estimated clean latent without adding random noise. We then adopt the policy gradient descent method, i.e., PPO , to optimize the alignment process of these diffusion models. The policy gradient descent optimization process is formulated as follows:

\[_{}^{}()=-P_{ }}{P_{^{}}}(_{O})+_{ }(P_{^{}}|P_{}),\] (6)

where \((|)\) indicates the KL-divergence between two distributions. The training process is shown as Algorithm 1. Note that due to the huge GPU memory and time consumption of the video diffusion process, we distribute the data generation and network training on different GPUs. Please refer to Sec.5**Settings** for more details.

**Modeling of Reward.** To measure the quality of the reconstructed event frame, we utilize the _FVD_ and _SSIM_ as the reward \(()\) to guide the training process of reinforcement alignment. Moreover, to remove the bias of rewards, we randomly generate \(M\) samples given one prompt, forming a pair of samples.

## 5 Experiments

**Dataset.** In our study, we utilize two large-scale event datasets, i.e., VisEvent  and EventVOT dataset . The VisEvent dataset encompasses a wide variety of scenes, including 820 RGB-Event video pairs and 371,128 RGB frames. Moreover, it was captured by the DAVIS346 camera , with resolutions of \(346 260\) for RGB and events. It addresses diverse environmental conditions and includes 17 distinct attributes such as camera motion, low illumination, and background clutter, facilitating detailed performance analysis under various challenging scenarios.

The EventVOT dataset provides 1,141 high-definition videos with 569,359 frames, making it the largest dataset in this domain. It features a resolution of \(1280 720\), encompassing 19 diverse classes of target objects. Compared to earlier datasets such as VOT-DVS, TD-DVS, and Ulster from 2016, and more recent ones like FE108 and COESOT, EventVOT offers an unprecedented scale and variety,

Figure 3: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP  and TAU . The final row represents the results obtained by our method. The complete sequence is shown in Fig.9.

including a substantial number of videos and a wide array of environmental conditions, aimed at improving the development and evaluation of event-based visual tracking algorithms. The dataset is meticulously annotated and divided into training (841 videos), validation (18 videos), and testing (282 videos) subsets, ensuring a comprehensive framework for robust algorithm testing and benchmarking.

**Settings.** All experiments are conducted on machines with \(8\) GeForce RTX 3090 GPUs, Intel(R) Core(TM) i7-10700 CPU of 2.90GHz, and 64-GB RAM. In the pre-training stage, we employed the ADAM optimizer with the exponential decay rates \(_{1}\) = 0.9 and \(_{2}\) = 0.999. The total training process was 20000 iterations for both kinds of noise experiments. We initialized the learning rate as 1e-5. We set the batch size to 128 (with 8 gradient accumulation steps).

For the alignment process, due to the fact that it takes quite a lot of GPU memory to generate a reference trajectory during the training process, we have to achieve the trajectory generation and reinforcement alignment in a parallel and distributed manner. Specifically, we utilize \(4\) RTX3090s to train reinforcement learning alignment processes. Moreover, \(4\) RTX3090s is utilized to generate training trajectory data. The updating episode of the reinforcement learning process is set at 100 optimization steps. (_Please refer to the Appendix for the detailed illustration of_) We also employed the ADAM optimizer with the exponential decay rates \(_{1}\) = 0.9 and \(_{2}\) = 0.999. We initialized

   Methods & Modal & _FVD_\(\) & _MSE_\(\) & _SSIM_\(\) & _LPIPS_\(\) & _mIoU_\(\) & _aIoU_\(\) \\  PhyNet  & VID & 1602.84 & 0.0295 & 0.4299 & 0.6048 & 0.077 & 0.348 \\ SimVP  & VID & 1347.57 & 0.0216 & 0.6261 & 0.3051 & 0.264 & 0.520 \\ TAU  & VID & 1371.65 & 0.0240 & 0.6381 & 0.3026 & 0.264 & 0.529 \\ PredRNNv2  & VID & 1266.83 & 0.0176 & 0.5550 & 0.2407 & 0.275 & 0.540 \\ SVD  & VID & 1122.54 & 0.0246 & 0.6451 & 0.3299 & 0.233 & 0.506 \\  PredRNNv2  & EVT & 1339.05 & 0.0306 & 0.6598 & 0.3388 & 0.166 & 0.504 \\ SimVP  & EVT & 1242.25 & 0.0210 & 0.7961 & 0.3371 & 0.213 & **0.532** \\ TAU  & EVT & 1218.03 & 0.0231 & 0.7972 & 0.3354 & 0.228 & 0.514 \\ Ours & EVT & **1055.25** & **0.0170** & **0.7998** & **0.3123** & **0.302** & 0.522 \\   

Table 1: Quantitative comparison between SOTA methods, where SVD denotes the standard stable video diffusion network. “VID” represents the video and “EVT” indicates the event data. \(\) (resp.\(\)) represents the bigger (resp. lower) the better.

Figure 4: More visualization of our method’s prediction in various scenarios. The results of the complete sequence along with other methods are presented in Fig. 12 and Fig. 13.

the learning rate as \(2e^{-6}\) and set the batch size to 16(with 2 gradient accumulation steps) in all experiments.

**Comparison Methods.** To comprehensively evaluate the performance of the proposed method, we compared different methods, which could be briefly divided into two categories: image-based future frame prediction and event-based future motion prediction. For the image-based future frame prediction, we adopt the original stable video diffusion  with some popular time serious forecasting methods, e.g., PredRNN , SimVP , TAU . Moreover, for future event forecasting, we simply use stable video diffusion  as our baseline since other methods make it hard to achieve desirable performance on those datasets.

**Metrics.** In the evaluation of our proposed model, we employ a comprehensive suite of metrics designed to assess the quality. We incorporate the _Frechet Video Distance (FVD)_ to evaluate the temporal coherence and visual quality of generated video sequences against real video content. The _Mean Squared Error (MSE)_ serves as a fundamental metric, quantifying the average squared difference between the estimated values and the actual values. The _Structural Similarity Index Measure (SSIM)_ is employed to assess the visual impact of structural information, brightness, and contrast differences between the generated images and the ground truth. Lastly, the _Learned Perceptual Image Patch Similarity (LPIPS)_ metric is adopted to evaluate the perceptual similarity between the generated and real images. In addition, we apply _mIoU_ and _aloU_ as metrics for the object segmentation task to validate the feasibility of the generated events in downstream tasks.

### Quantitative and Qualitative Comparison Results

The experimental outcomes are illustrated in Table 1. It is evident that by applying regularization to event data, our methodology attains performance levels comparable to state-of-the-art (SOTA) methods. Notably, in metrics specific to the event domain, our approach outperforms the comparative

Figure 5: Visualization results on downstream tasks, where we show the tasks of tracking, segmentation, and flow estimation. (a) denotes the ceiling performance of settings (b) and (c).

[MISSING_PAGE_FAIL:9]

have experimentally validated that after changing those modules. The experimental results are shown in Table 3, where we feed features from different clip models (Event-trained or RGB-trained) to the SVD U-Net. Note that all CLIPs are fed with event voxels. Even further fine-tuning the SVD with plenty of data, the resulting diffusion model with Event-trained is still underperformed.

**Testing-time Prompt Augmentation.** As detailed in Sec. 4.1, we enhance the test-time prompt by incorporating multiple event frames of high temporal resolution. To thoroughly examine the impact of these hyperparameters and ascertain the efficacy of our test-time augmentation strategy, we compare various approaches employing different test-time prompts, as delineated in Table 6, titled "Testing-time'. From this comparative analysis, it is evident that there is a gradual improvement in the neural network's performance as the extent of prompt augmentation increases.

### Discussion

While the proposed method has exhibited preliminary capabilities, it is imperative to address its existing limitations. One significant challenge is the inherent nature of event data; while it boasts high temporal resolution, this type of data typically lacks texture, thereby impeding the effective capture of detailed semantic information. This limitation highlights the difficulty in accurately representing complex visual scenes solely based on event data. Furthermore, there is a pressing need for the development of a lossless representation technique that can fully preserve the unique high-temporal attributes of event data, ensuring no critical information is lost during processing. _Specific constrained scenarios are visualized and discussed in detail in Appendix Section D_.

To address these challenges and enhance the efficacy of the proposed method, future work should focus on several key areas. Firstly, advancing the method's ability to interpret texture would mark a significant improvement. This could involve integrating additional sensory inputs, e.g., RGB images, or employing more sophisticated data fusion techniques. Secondly, **the creation of a more comprehensive event sequence dataset is essential**. Such a dataset should encompass a wider variety of scenarios and conditions, thereby providing a robust platform for training and testing the improved models. By addressing these aspects, future research can pave the way for more accurate, reliable, and versatile event-based vision systems for object motion forcasting.

## 6 Conclusion

In this study, we have introduced the Event-Sequence Diffusion Network, a novel approach poised to redefine the landscape of video diffusion. By leveraging event-based data, characterized by its high temporal resolution, our methodology advances the frontiers of motion modeling, enabling the generation of video sequences that are not only rich in detail but also grounded in the realistic dynamics of object motion. Our approach stands in stark contrast to traditional video diffusion models that rely on single images or textual prompts for initialization. By employing sequences of events as the conditioning input, we ensure a more nuanced and temporally coherent synthesis of video content.

Future work will focus on refining the Event-Sequence Diffusion Network, exploring its applicability across a broader spectrum of computer vision tasks, and enhancing its efficiency for real-time applications. Moreover, we aim to delve deeper into the interplay between event-based data and diffusion models, seeking to unlock new potentials and applications in areas such as autonomous navigation, interactive gaming, and dynamic scene reconstruction.

   Method & **EP** & **MA** & _FVD_\(\) & _SSIM_\(\) & _LPIPS_\(\) & _mIoU_\(\) & _aloU_\(\) \\  A & \(\) & \(\) & 1378.92 & 0.78496 & **0.3076** & 0.252 & 0.505 \\ B & ✓ & \(\) & 1227.56 & 0.79077 & 0.3101 & 0.295 & **0.528** \\ C & \(\) & ✓ & 1119.71 & 0.79597 & 0.3246 & 0.277 & 0.516 \\ D & ✓ & ✓ & **1055.25** & **0.79981** & 0.3123 & **0.302** & 0.522 \\   

Table 4: Ablation Study of motion alignment and multi prompt. All models are tested with only feeding single event voxel frame. ’EP’ denotes denoising using the high temporal resolution event prompt, and ’MA’ denotes motion alignment based on reinforcement learning.