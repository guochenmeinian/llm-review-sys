# TrackIME: Enhanced Video Point Tracking via

Instance Motion Estimation

 Seong Hyeon Park\({}^{1}\)   Huiwon Jang\({}^{1}\)   Byungwoo Jeon\({}^{1}\)   Sukmin Yun\({}^{2}\)

Paul Hongsuck Seo\({}^{3}\)   Jinwoo Shin\({}^{1}\)

\({}^{1}\)KAIST  \({}^{2}\)Hanyang University ERICA  \({}^{3}\)Korea University

{seonghyp, huiwoen0516, imbw2024, jinwoos}@kaist.ac.kr

sukminyun@hanyang.ac.kr   phseo@korea.ac.kr

###### Abstract

Tracking points in video frames is essential for understanding video content. However, the task is fundamentally hindered by the computation demands for brute-force correspondence matching across the frames. As the current models down-sample the frame resolutions to mitigate this challenge, they fall short in accurately representing point trajectories due to information truncation. Instead, we address the challenge by pruning the search space for point tracking and let the model process only the important regions of the frames without down-sampling. Our first key idea is to identify the object instance and its trajectory over the frames, then prune the regions of the frame that do not contain the instance. Concretely, to estimate the instance's trajectory, we track a group of points on the instance and aggregate their motion trajectories. Furthermore, to deal with the occlusions in complex scenes, we propose to compensate for the occluded points while tracking. To this end, we introduce a unified framework that jointly performs point tracking and segmentation, providing synergistic effects between the two tasks. For example, the segmentation results enable a tracking model to avoid the occluded points referring to the instance mask, and conversely, the improved tracking results can help to produce more accurate segmentation masks. Our framework can be easily incorporated with various tracking models, and we demonstrate its efficacy for enhanced point tracking throughout extensive experiments. For example, on the recent TAP-Vid benchmark, our framework consistently improves all baselines, _e.g._, up to 13.5% improvement on the average Jaccard metric. The project url is https://trackime.github.io/.

## 1 Introduction

Obtaining accurate point trajectories over the video frames is crucial for understanding complex dynamics in video data, a necessity for advanced spatial-temporal tasks like action recognition , novel-view rendering , video frame prediction/interpolation , and video depth estimation . Recently, video point tracking task  has witnessed rapid progress, which aims to predict the trajectory and visibility1 of a given query point, proving long-term trajectories robust to partial occlusions of objects in real video scenes.

Despite their success, we find current point tracking models are fundamentally challenged by an excessive computation demand since the task requires brute-force comparisons over every spatial location in every frame in a given video. As a result, to meet the computation constraints, the modelsdown-sample their tracking resolutions, sacrificing detailed visual features, which eventually leads to sub-optimal tracking accuracy and triggers tracking failures on intricate object parts. In this regard, we pursue the direction of pruning the excessive search space for point tracking, so that models can avoid the down-sampling and focus only on important regions maintaining detailed visual features, _e.g._, the object instance masks the query point lies in.

In this paper, we introduce TrackIME: Enhanced Video Point Tracking via Instance Motion Estimation that focuses on the region occupied by the object instance that the queried point lies in and guides point tracking models to prune the video frames along the instance's motion trajectory. Here, to obtain the instance trajectory, we first produce the instance mask for a given query point by utilizing the recent segmentation foundation models, _e.g._, segment anything (SAM) , where these foundation models show strong generalization performance to different objects/scenes and we find resulting instance masks in quality are readily available. Then, given the instance mask, we sample a set of points and aggregate their tracking results as the estimate of the instance trajectory.2

Furthermore, to deal with the occlusions in complex video scenes, we propose a unified framework that jointly performs the point tracking and video segmentation, where it re-samples the occluded points by referring to the instance mask. We note that our framework provides synergistic effects for both tasks, _i.e._, the point tracking results assisted by the segmentation can conversely bolster the quality of segmentation. Consequently, although our primary focus is on the advances in point tracking, our method can also demonstrate improved segmentation results than the baselines (see Section 4.2 for details).

Through the experiments on the TAP-Vid point tracking benchmark , we demonstrate the effectiveness of TrackIME by incorporating it with different point tracking models such as TAPIR . For example, in the DAVIS scenes  evaluating the point tracking for dynamic objects, our method achieved up to 13.5% relative improvement (57.5 \(\) 65.3 with TAPIR) in terms of the average Jaccard (AJ) metric. Moreover, as our framework allows pruning non-instance regions for point tracking

Figure 1: **The workflow of TrackIME. Our framework enhances point tracking by pruning the search space, along the instance trajectory in video frames. To estimate the instance trajectory, our framework utilizes the point tracking results for a group of points (blue lines) on top of the object instance predicted by segmentation model (_e.g._, SAM ) and aggregate their individual trajectories.**

models, the efficacy of our method stands out even more when evaluated on more harsh standards, _e.g._, the 1-pixel error threshold, where the conventional metrics allow up to 16-pixel errors when judging the prediction to be correct.

## 2 Method

In this section, we describe the detailed procedure of our TrackIME framework and its application to video point tracking. Specifically, in Section 2.1, we describe the formulation for instance trajectory estimation, which is based on the video point tracking of the query points found by the foundation segmentation model .

Next, in Section 2.2, we present the detailed formulation of TrackIME given the instance trajectory, which prunes unimportant regions in the video frame and achieves boosted point tracking performances.

As for the data notations, we denote vectors with \(N\) elements as bold letters \(:=[_{1};_{2};...;_{N}]\), tensors with \(N\) arrays as bold capital letters \(}:=[}_{1};}_{2};...;} _{N}]\), where the subscripts represent the indexed scalars or arrays. Otherwise, every non-bold symbol is scalar. We also introduce the superscripts, _e.g._, \(^{()}\), when denoting there is special semantics for a data, such as the query point.

Finally, when making binary classifications based on probability (or normalized confidence) values, we use threshold 0.5; nevertheless, the values are hyperparameters and can be altered in practice.

### Instance Trajectory Estimation

In this section, we provide the definition of the instance trajectory and procedures to obtain it, such as sampling a group of points on the instance, trajectory aggregation, and the point re-sampling modules.

Video point tracking.Let \(}^{L H W 3}\) be the tensor of video frames, where \(L\) denotes the time duration and \((H W)\) denotes the image size, and let \(^{()}^{2}\) be the spatial coordinates of the query point. Typically, we consider the query in the initial frame hence we do not denote the time index of the query point for clarity. Given the video \(}\) and the query point \(^{()}\), we consider a point tracking model Tracker that predicts the query trajectory \(^{()}^{L 2}\) and the probability of being visible \(^{()}(0,1)^{L}\) over the entire set of frames,

\[(^{()},^{()}):=(^{( )},}).\] (1)

Here, one might utilize Equation (1) as the simplest representation of the instance motion trajectory. However, modeling the instance motion solely using the query point has critical shortcomings. For example, when the instance is partially occluded by other objects, the trajectory of the query point may no longer exist (see Section 4.1 for the ablation study).

To address this challenge, we propose to sample additional tracking points automatically. Specifically, our idea is to identify the instance mask of the query point so that extra query points can be added from the mask.

Sampling points on the instance.Let \(}_{0}(0,1)^{H W}\) denote the segmentation mask that represents the object instance associated with the query point \(^{()}\). Given this mask, we sample a group of points on the instance,

\[(^{()}):=\{^{(n_{0})},,^{(n_{S})}\},\] (2)

which we refer to it as the _semantic neighbors_ of \(^{()}\). We note that \(S\) is the number of sampled points, where the query point is also counted as its semantic neighbor, _i.e._, \(^{(n_{0})}^{()}\).

For each semantic neighbor point, we employ Tracker in Equation (1) to produce its trajectory and visibility, \((^{(n_{i})},^{(n_{i})}):=(^{( n_{i})},})\),3 and pass it to the trajectory aggregation module. Since the query point also participate in our tracking procedure, the total effective number of points would be \(S+1\). For example, we choose \(S+1=32\) in our main experiments discussed in Section 2.

Trajectory aggregation.We produce an instance motion trajectory by aggregating the tracking results of the semantic neighbors. Specifically, we consider the velocity, \(_{t}^{(n_{i})}:=_{t}^{(n_{i})}-_{t-1}^{(n_{i})}\), and calculate the weighted average:

\[}_{t}^{()}:=_{_{t}^{(n_{i}) } 0.5}_{t}^{(n_{i})}_{t}^{(n_{i}) }}{_{_{t}^{(n_{j})} 0.5}_{t}^{(n_{ j})}}.\] (3)

In Equation (3), we note that velocities are aggregated only if the points are classified visible (\(_{t}^{(n_{i})} 0.5\)), and the visibility acts as the aggregation weight. Finally, we accumulate the aggregated velocity starting from \(}_{0}^{()}:=^{()}\), to obtain the instance motion trajectory,

\[}_{t}^{()}:=}_{t-1}^{()}+ {}_{t}^{()}.\] (4)

Instance mask.In order to identify the instance mask, we employ the recent foundation segmentation model, _e.g._, Segment Anything Model (SAM) , and prompt the model with the query point \(^{()}\), to produce the pixel-wise confidence representing the object instance indicated by the query point. We denote this function as \(\),

\[}_{0}:=(^{()},}_{0}) (0,1)^{H W}.\] (5)

Given the mask \(}_{0}\), we employ a weighted sampling for the semantic neighbors. Specifically, we encode the sampling weights with the distance transform (DT)  to the mask's region with positive classifications,

\[}_{0}:=([}_{0} 0.5]).\] (6)

In this way, the points near the mask's contour are preferred, which we find efficiently represent the object instance because the contour is approximately linearly proportional to the mask's radius.

Point re-sampling for robustness to occlusion.Occlusions are common in real video frames, due to dynamic objects and the camera's motion. In the extreme case, Equation (3) can become degenerate when all semantic neighbors are invisible in future frames \(t>0\). Therefore, maintaining a sufficient number of visible tracking points is crucial, and we tackle this issue by re-sampling occluded points from the instance mask jointly predicted while point tracking.

In a nutshell, whenever we find a certain semantic neighbor point \(^{(n_{i})}\) becomes invisible at time \(t^{}\) and does not show up again (\(_{t}^{(n_{i})}<0.5\) for \(t t^{}\)), we query the segmentation model with the tracking results of other semantic neighbors to obtain a new mask to re-sample the occluded point:

\[}_{t^{}}^{(n_{j})}:=(_{t^{}}^{(n_ {j})},}_{t^{}})(0,1)^{H W}.\] (7)

However, they could also have been affected by occlusions (_e.g._, when \(_{t^{}}^{(n_{j})}\) is close to the threshold 0.5), or by the severe errors in the trajectory \(_{t}^{(n_{j})}\) due to sub-optimal tracking performance of Equation (1). Hence, predicting segmentation with these points in a naive way can lead to erroneous masks being produced.

To address this problem, our key idea is to aggregate the group of segmentation masks. Specifically, we collect individual masks by Equation (7), then apply a weighted average of the positive classifications,

\[}_{t^{}}:=_{_{t^{}}^{(n_{i })} 0.5}_{t^{}}^{(n_{i})}[}_{t^{}}^{(n_{i})}>0.5]}{_{_{t^{ }}^{(n_{j})} 0.5}_{t^{}}^{(n_{j})}}(0,1)^{H  W}.\] (8)

We find the mask produced by Equation (8) reflects the confidence of each segmentation mask, as well as the visibility of the associated point, and refer to it as the _mixture of segmentation distributions_, where the value in each index represents the segmentation probability of the queried object.

Based on the constructed mixture of segmentation distributions, we obtain the sampling weight in similar manner to Equation (6) as, \(_{t^{}}:=([\!\!\!\!\!\!\!\!\! _{t^{}} r])\), where the threshold \(r[0,1)\) is set much smaller than the standard \(0.5\).4 This is because we should allow the confident partial segmentation distributions, but ignore the unconfident noise segmentation distributions.

Finally, we re-sample the additional points with \(_{t^{}}\) as the sampling weight. They replace the occluded points for the instance trajectory estimation in subsequent frames \(t>t^{}\). We execute this procedure during the tracking, which ensures that a sufficient number of visible points participate in Equations (3) and (4). For example, we set it to be the same as the number of initial semantic neighbors \(S\).

### TrackIME: Enhanced Video Point Tracking via Instance Motion Estimation

In this section, we describe our enhanced point tracking, which prunes the search spaces in frames and produces more accurate tracking results.

**Search space pruning.** Given the instance trajectory in Equation (4), we now aim to utilize it for pruning the search space. Specifically, we prune unimportant non-instance regions, by sampling each frame around the \((H_{0} W_{0})\) regions centered at the aggregated trajectory,

\[^{()}:=(,}^{( )},H_{0},W_{0})^{L H_{0} W_{0} 3}.\] (9)

We note that the sizes \((H_{0} W_{0})\) are set to be close to the down-sampling resolution considered by a tracking model (_e.g._, (\(256 256\)) for TAPIR ) so that the information loss is minimized.

Given the frames with pruned search spaces, we execute Tracker again to produce the enhanced tracking outputs. Also, for convenience, we abstract the entire process of the instance trajectory estimation (Section 2.1), the pruning (Equation (9)) and the tracking into a function TrackerHD,

\[(}^{()},}^{( )})&:=(}^{()}, ,H_{0},W_{0})\\ &:=(}^{()},^{( )}).\] (10)

We note that the feature resolutions inside the tracking model are not modified, therefore the computational complexity does not increase.

**Progressive inference.** To achieve a further boost in the tracking performance, we can additionally use a progressive inference structure. Formally, we consider a collection of \(K\) different TrackerHD models equipped with different pruning sizes \((H_{k},W_{k})\):

\[[}^{()}_{1};...;, {{T}}^{()}_{K}][}^{()}_{1};...;,}^{( )}_{K}],\] (11)

where \(}^{()}_{k}^{L 2}\) and \(}^{()}_{k}(0,1)^{L}\) denotes the outputs of the \(k\)-th TrackerHD model.

This progressive structure can boost the tracking performance in two ways. The first is utilizing a past \(k\)-th TrackerHD as the tracking model that estimates the instance trajectory for the next (\(k+1\))-th TrackerHD. In this way, the pruning is guided by a more accurate trajectory estimate. The second is that these \(K\) tracking results can be aggregated to produce the final trajectory. Specifically, we aggregate based on the visibility, in a similar manner to Equations (3) and (8):

\[}^{()}:=_{k=1}^{K} }^{()}_{k}}^{()}_{k}} {_{l=1}^{K}}^{()}_{l}},\] (12)

where \(\) indicates the element-wise product. This aggregation allows processing multiple scales in visual features, which can enhance the generalization performance of vision models [15; 16]. We note that the visibility predictions are averaged over the \(K\) predictions.

## 3 Related Work

**Optical Flow.** Optical flow deals with the dense computation of instantaneous motion patterns between two given video frames. Starting with the pioneering work of applying neural networks for motion estimation [17; 18], the seminal works such as DCFlow , PWC-Net  and RAFT  introduced the concept of dense correspondence matching between pairs of image patches. Despite their success, the optical flow's inherent limitations incapable of modeling trajectories and occlusions triggered the recent progress in the point tracking methods.

**Point Tracking.** In essence, point tracking attempts to find the long-term point correspondences over the entire video frames, and model the occlusions and trajectories. The current models in this domain, such as PIPs , TAPNet , TAPIR , CoTracker , and OmniMotion  has led rapid progress, with advanced neural architectures [6; 7] or test-time optimizations . However, they are fundamentally hindered by the excessive search space for correspondence matching over the entire frames. Our focus is to address this issue by pruning the search space, where our method can be readily incorporated with these baselines.

**Instance Segmentation.** Recently, the important advancement within image segmentation has been the introduction of segment anything (SAM) . SAM is specifically designed to perform image segmentation by general point prompts and exhibits an impressive capacity for class-agnostic segmentation. Specifically, in the context of point tracking, SAM serves as a valuable resource by generating segmentation masks for the object instance indicated by the query point. We also note the line of zero-shot video segmentation [23; 24; 25; 26; 27; 28; 29; 30]. Specifically, the recent SAM-PT  focuses on bolstering video segmentation based on point tracking, which is fundamentally different from our work; our primary goal is obtaining better point tracking, while that for SAM-PT is for better segmentation. Nevertheless, our method provides synergistic effects for both tasks, and even outperforms SAM-PT for segmentation tasks (see Table 4).

## 4 Experiments

In this section, we demonstrate the effectiveness of the proposed TrackIME on point tracking tasks and the downstream video object segmentation.

In Section 4.1, we focus on the point tracking tasks. Specifically, we first experiment the efficacy of the instance motion trajectory estimation and our search space pruning technique for point tracking by measuring the performance in video scenes that capture dynamic objects.

Next, we verify the universality of our method to different point tracking models and find whether it can provide general performance improvements when incorporated into the five recent baselines, _e.g._, TAPNet , PIPS2 , CoTracker, OmniMotion and TAPIR .

In the ablation study, we validate the effect of each component, namely the trajectory aggregation, the search space pruning, and the progressive inference modules described in Section 2.

    &  &  \\  Method & J\({}_{1}\) & AJ & \(_{1}^{x}\) & \(_{}^{x}\) & OA & J\({}_{1}\) & AJ & \(_{1}^{x}\) & \(_{}^{x}\) & OA \\  TAPNet  & 20.7 & 51.6 & 30.1 & 63.8 & 79.8 & 25.3 & 56.5 & 36.3 & 68.2 & 82.6 \\ PIPS2  & 19.6 & 46.6 & 35.8 & 69.4 & 80.3 & 6.9 & 52.8 & 14.2 & 65.8 & 83.5 \\ TAPIR  & 23.0 & 57.5 & 34.3 & 70.5 & 84.4 & 28.1 & 62.8 & 41.0 & 75.1 & 87.7 \\ CoTracker  & 28.3 & 60.8 & 43.5 & 76.1 & 86.0 & 34.9 & 64.3 & 50.9 & 78.9 & **89.1** \\ OmniMotion  & 21.5 & 52.6 & 39.1 & 68.1 & 85.4 & 30.1 & 55.6 & 45.1 & 70.3 & 88.9 \\
**TrackIME** & **35.4** & **65.3** & **48.2** & **78.6** & **86.5** & **41.9** & **69.3** & **55.0** & **81.4** & 89.0 \\   

Table 1: **The evaluation of point tracking performance for dynamic objects.** We benchmark the quality of point tracking in DAVIS  videos with the point annotations provided by TAP-Net . We note that TrackIME is incorporated with TAPIR point tracker .

In Section 4.2, we verify the efficacy of the enhanced point tracking results by TrackIME in the downstream video object segmentation. Specifically, we compare the zero-shot video segmentation performances with the recent SAM-PT  baseline which utilizes the point trajectories as the inputs, as well as the conventional baselines that input the semantic classes [27; 28; 29].

Common implementation details.We note that TrackIME is mainly incorporated with TAPIR point tracker  (as it empirically performs best) unless specified otherwise, and we subject it to all experiments including the point tracking and other downstream tasks.

For the segmentation model, we utilize the Segment Anything (SAM)  to perform the point-queried segmentation function described in Equation (5).

To prepare video frames, we always adjust the resolutions of raw video data to 1080p (1080 pixels in the shorter frame edges), then apply further resizing functions required by individual baseline models. For example, we resize the 1080p frames to \(256 256\) for TAPIR  baseline, following the default setting provided by the official open-source repository. When experimenting TrackIME, we choose the hyperparameters for each baseline, _e.g._, progressive inference steps \(K=2\), and the pruning sizes \(H_{0}=W_{0}=960\) and \(H_{1}=W_{1}=384\) when incorporated with TAPIR .

Since TrackIME is a plug-in to all baselines, we reproduce all results in the same system configuration for fair comparisons. We note that such modification can induce minor perturbation in the numerical values due to library and hardware-dependent characteristics, _e.g._, different characteristics between JAX  and PyTorch  libraries, and the difference in the filtering algorithm used when re-sizing the video frames.5 We refer the readers to Appendix A for more implementation details.

### Point Tracking

Baselines.We compare our method to the recent baselines OmniMotion, CoTracker , TAPIR , PIPS2 , and TAPNet . We utilize the official checkpoints provided by the official project pages and reproduce all experimental results under our common experimental set-up, except for OmniMotion  which does not provide checkpoints. Instead, we reproduced the training of OmniMotion models to obtain the experimental results. We use \(S=31\) semantic neighbors to incorporate our framework with the baselines.

Datasets.We evaluate these models on three different datasets, DAVIS , Kinetics , and RGBStacking , each representing different characteristics. For example, DAVIS contains 30 videos

    &  &  &  \\ Method & AJ & \(^{x}_{}\) & AJ & \(^{x}_{}\) & AJ & \(^{x}_{}\) \\  TAPNet  & 51.6 & 63.8 & 56.5 & 79.0 & 49.3 & 60.7 \\ **+ TrackIME** & **57.9** & **72.4** & **66.9** & **80.0** & **51.0** & **63.6** \\  PIPS2  & 46.6 & 69.4 & 52.3 & 74.9 & - & - \\ **+ TrackIME** & **50.3** & **74.0** & **52.8** & **75.8** & - & - \\  CoTracker  & 60.8 & 76.1 & 64.1 & 78.0 & 47.7 & 63.7 \\ **+ TrackIME** & **64.5** & **79.2** & **68.2** & **82.1** & **48.1** & **63.8** \\  OmniMotion†  & 52.6 & 68.1 & 71.2 & 81.1 & 51.0 & 64.3 \\ **+ TrackIME** & **54.1** & **69.3** & **71.9** & **81.9** & **51.2** & **64.6** \\  TAPIR  & 57.5 & 70.5 & 66.3 & 80.6 & 50.2 & 62.3 \\ **+ TrackIME** & **65.3** & **78.6** & **66.6** & **81.8** & **51.4** & **65.8** \\   

Table 2: **Universality of TrackIME with different point tracking models. We incorporate recent point tracking model baselines [6; 7; 8; 9; 11] with our method, and benchmark its performance on DAVIS , RGBStacking , and Kinetics . †: the underlined results are obtained with subsets of RGBStacking and Kinetics datasets due to a large optimization cost for the OmniMotion .**specifically curated to evaluate the tracking performance under large variances in the appearance and motion of object entities. Its two variants, DAVIS-F (First) and DAVIS-S (Strided) differ in how the query points are given to the models: DAVIS-F queries the model only once in the first frame, while DAVIS-S queries the model in strides of five frames. Because DAVIS-F requires long-term tracking, it is generally a more difficult setting. Kinetics contains 1,144 web videos collected from YouTube that represent realistic noisy characteristics of the video in the wild, such as sudden scene changes. RGB Stacking is a synthetically rendered dataset representing 50 different moves by a robotic arm. For all datasets, we refer to the point tracking annotations provided by TAP-Vid  and utilize them as the ground truth for evaluation.

**Metric.** To measure the quality of point tracking, we consider point tracking accuracy considered following TAP-Vid , such as the \(\)-average accuracy (\(_{}^{x}\)) and the average Jaccard (AJ). The average metrics are based on the \(\)-n accuracy (\(_{n}^{x}\)) which indicates the proportion of correct trajectory sequence as judged by whether they are within the n-pixel error threshold around the ground truth. In addition, the Jaccard-n (J\({}_{n}\)) judges a trajectory sequence to be correct only if the visibility prediction is also correct. Given these definitions, the average metrics are calculated by averaging \(n\{1,2,4,8,16\}\). To evaluate the fine-grained tracking performance in a harsh error threshold, we also report \(\)-1 accuracy (\(_{1}^{x}\)) and Jaccard-1 (J\({}_{1}\)). For Table 1, we also discuss the occlusion accuracy (OA), the proportion of correct visibility sequence given the ground truth.

**Effectiveness on point tracking in dynamic objects.** We first present the point tracking scenarios with dynamic objects. Specifically, we experiment with the DAVIS video scenes , which is curated for evaluating instance motion estimation tasks. As shown in Table 1, we find our method achieves the best point tracking accuracy surpassing all baselines, _e.g._, up-to 7.4% relative improvements in average Jaccard, _i.e._, 60.8 AJ (CoTracker ) vs. 65.3 AJ (TrackIME) when evaluated with the DAVIS-F (denoted First Query in Table 1). We also measure the occlusion accuracies (OA) and find a relatively incremental improvement than other metrics. Intuitively, there is a trade-off between modeling the occlusions among different objects and the search space pruning for one instance, as the pruning removes information from other instances. Nevertheless, our method is beneficial for detecting occlusion in fine-grained object parts, and we recommend searching for optimal pruning parameters that fit a user's purpose. Finally, we discuss the efficacy of TrackIME under the harsh \(_{1}^{x}\) and \(J_{1}\) metrics, where the conventional metrics allows up to 16-pixel errors and takes the average when judging whether the prediction is correct. For example, the improvement can be even larger, _e.g._, up to relative 25.1%, _i.e._, 28.3 J\({}_{1}\) (CoTracker ) vs. 35.4 J\({}_{1}\) (TrackIME) when evaluated with DAVIS-F. We highlight these benefits of TrackIME allowed by pruning the search space.

**Universality to different point tracking models.** We validate the universality of our method when plugged into the state-of-the-art baselines by evaluating the average tracking accuracy (AJ and \(_{n}^{x}\)) of the vanilla models and the variants incorporated with our method in Table 2 on DAVIS (First) , RGBStacking , and Kinetics  datasets. As a result, we observe that our method can provide consistent and significant performance improvements in all the baselines, _e.g._, 13.6% relative improvements (_i.e._, 57.5 \(\) 65.3 AJ) in TAPIR  when evaluated on the DAVIS. Since the model variant incorporated with TAPIR demonstrates the best performance, we chose it as our main model and subjected it to other studies. We note that the experiments for OmniMotion  have been conducted in 16 subsets for RGBStacking and Kinetics, and \(K=1\) progressive inference, due to its

   Pruning & Aggregation & Progressive & J\({}_{1}\) & AJ & \(_{1}^{x}\) & \(_{}^{x}\) \\  ✗ & ✗ & ✗ & 23.0 & 57.5 & 34.3 & 70.5 \\ ✓ & ✗ & ✗ & 28.2 & 62.5 & 41.1 & 75.3 \\ ✓ & ✗ & ✓ & 28.3 & 62.6 & 41.2 & 75.6 \\ ✓ & ✓ & ✗ & 34.0 & 62.9 & 48.0 & 77.0 \\ ✓ & ✓ & ✓ & **35.4** & **65.3** & **48.2** & **78.6** \\   

Table 3: **Ablation study of the components in our model.** We ablate the effect of search space pruning (Pruning), trajectory aggregation (Aggregation), and the progressive inference (Progressive) modules for point tracking. We evaluate the tracking benchmark in DAVIS scenes .

heavy optimization costs, _e.g._, approximately 13 gpu-hours for processing one scene. We also note that PIPS2  in Kinetics  is unavailable, as its memory requirement for processing Kinetics exceeds our system's capacity.

**Ablation study.** We perform an ablation study to understand how each component affects the point trajectory accuracy in Table 3. Specifically, we consider the search space pruning, the trajectory aggregation, and progressive inference modules as the subjects for the ablation.

First of all, we reveal the pure efficacy of our pruning method, separate from the effect of segmentation prior. Notably, when the trajectory aggregation module is removed (the first 2 rows in Table 3), we observe the pruning solely based on the query point's trajectory provides the most significant effect (_e.g._, \(23.0 28.2\) in \(J_{1}\)). This validates our key motivation for pruning the search space, which provides superior results even if SAM  is not employed.

Next, we discuss the effect of employing SAM  by enabling the trajectory aggregation. As expected, aggregating the trajectories for a group of points found in the segmentation mask provides another comparable gain (_e.g._, \(28.2 34.0\) in \(J_{1}\)), which validates that the aggregation improves the quality of instance trajectory estimation.

It is worth noting that the progressive inference boosts the performance, (_e.g._, \(34.0 35.4\) in \(J_{1}\)) when combined with the trajectory aggregation, otherwise the gain is lesser (_e.g._, \(28.2 28.3\) in \(J_{1}\)). As the progressive inference refers to the estimated instance trajectory, the estimation quality is essential for this module.

We also note that further ablation study is available in Appendix D, _e.g._, the number of semantic neighbors, progressive inference steps, or the pruning sizes.

### Video Object Segmentation

In this section, we validate the efficacy of TrackIME by performing the zero-shot video segmentation. We also provide the visualization results for selected scenes from DAVIS  in Figure 2.

**Baselines.** We experiment with zero-shot video object segmentation to check the efficacy of TrackIME for improving segmentation. Specifically, we consider the class-guided baselines for unsupervised video segmentation tasks, _e.g._, EntitySeg . In addition, we consider the SAM-PT  baseline which also proposes to take point tracking for producing segmentation. To consider the equivalent experimental set-ups for SAM-PT  and TrackIME, we incorporate the models with

Figure 2: **Demonstration of the video instance segmentation results by our TrackIME framework.** Given the query points in the reference frame, our framework can produce the video instance segmentation masks at quality by performing the weighted aggregation of the mask associated each query point, based on the visibility values.

HQ-SAM  variant for the segmentation, 16 points from the initial frame's mask, and employ the iterative refinement technique  to produce the video segmentation results.

**Evaluation.** We evaluate our model on the DAVIS-2017  video segmentation. In particular, we use the validation and the test-dev sets for the zero-shot benchmark. Both sets contain 30 non-overlapping scenes with single or multiple objects.

To measure the quality of video instance segmentation, we consider the standard metrics in baselines: the mean Jaccard (J\({}_{m}\)); the mean F-measure (F\({}_{m}\)); and the average \(()_{m}\). Specifically, we follow the official implementation suite provided by the DAVIS challenge .

**Effectiveness on zero-shot video object segmentation.** In Table 4, we first confirm that the point tracking provides useful guidance for video segmentation, observing that both SAM-PT  and TrackIME demonstrates significant improvement over the conventional class-prompted baselines. More importantly, as our framework brings synergistic improvements for both point tracking and segmentation tasks, we find TrackIME achieves even larger improvement, _e.g._, 78.8 vs. 79.6 (\()_{m}\) in the validation set of DAVIS-2017 .

**Discussions.** As for the commentary on the efficacy of TrackIME, our key advantage is removing erroneous query points for segmentation caused by the tracking failure on intricate object parts, enabling even finer query points for segmentation, _e.g._, the accuracy in harsh 1-pixel thresholds in Table 1, which is possible due to the pruning structure in our framework to maintain the high-frequency information.

## 5 Conclusion

In this work, we introduce TrackIME, a novel approach for point tracking to overcome the fundamental challenge of computation demands in existing models. Specifically, we reduce the search space by identifying the instance trajectory and pruning the video frames along it. To obtain the instance trajectory, we aggregate the motion for a group of points on the segmentation masks. To this end, we propose a unified framework that jointly performs point tracking and segmentation, with the techniques to ensure robustness to occlusion in complex video scenes. TrackIME demonstrates consistent and significant impacts by bolstering existing point tracking baselines. The joint framework also reveals the synergistic effects, which also demonstrates the improvements in the video segmentation task. Overall, our work highlights the effectiveness of considering instance motion trajectory and jointly solving the tracking and segmentation, and we believe our work could inspire researchers to consider a new direction to further leverage it in the future.

    & &  &  \\  Method & Input & \(()_{m}\) & J\({}_{m}\) & F\({}_{m}\) & \(()_{m}\) & J\({}_{m}\) & F\({}_{m}\) \\  PDB  & class & 55.1 & 53.2 & 57.0 & 40.4 & 37.7 & 43.0 \\ RVOS  & class & 41.2 & 36.8 & 45.7 & 22.5 & 17.7 & 27.3 \\ AGS  & class & 57.5 & 55.5 & 59.5 & 45.6 & 42.1 & 49.0 \\ MAST  & class & 65.5 & 63.3 & 67.6 & - & - & - \\ Propose-Reduce  & class & 70.4 & 67.0 & 73.8 & - & - & - \\ UnOVSOT  & class & 67.9 & 66.4 & 69.3 & 58.0 & 54.0 & 62.0 \\ EntitySeg  & class & 73.4 & 70.4 & 76.4 & 62.1 & - & - \\  SAM-PT\(\) & points & 78.8 & 76.3 & 81.3 & 65.3 & 62.3 & 68.3 \\
**TrackIME\(\)** & points & **79.6** & **76.4** & **82.8** & **65.9** & **62.5** & **69.4** \\   

Table 4: **Zero-shot video object segmentation performance in DAVIS benchmark.** We consider two set of zero-shot baselines, those utilizing the set of classes  and the baseline utilizing a set of query points  in a similar manner to our TrackIME. \(\): we produced the results for TrackIME and SAM-PT  under the common set-up, such as the number of tracking points, segmentation function (HQ-SAM ), and the same mask formatting for the benchmark.