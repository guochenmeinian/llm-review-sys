# Scaling Riemannian Diffusion Models

Aaron Lou, Minkai Xu, Adam Farris, Stefano Ermon

Stanford University

{aaronlou, minkai, adfarris, ermon}@stanford.edu

###### Abstract

Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ansatze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on \(SU(n)\) lattices and contrastively learned embeddings on high dimensional hyperspheres.

## 1 Introduction

By learning to faithfully capture high-dimensional probability distributions, modern deep generative models have transformed countless fields such as computer vision  and natural language processing . However, these models are built primarily for geometrically simple data spaces, such as Euclidean space for images and discrete space for text. For many applications such as protein structure prediction , contrastive learning , and high energy physics , the support of the data distribution is instead a Riemannian manifold such as the sphere or torus. Here, naively applying a standard generative model on the ambient space results in poor performance as it doesn't properly incorporate the geometric inductive bias and can suffer from singularities .

As such, a longstanding goal within Geometric Deep Learning has been the development of principled, general, and scalable generative models on manifolds . One promising method is the Riemannian Diffusion Model , the natural generalization of standard Euclidean space score-based diffusion models . These learn to reverse a diffusion process on a manifold-in particular, the heat equation-through Riemannian score matching methods. While this approach is principled and general, it is not scalable. In particular, the additional geometric complexity renders the denoising score matching loss intractable. Because of this, previous work resorts to inaccurate approximations or sliced score matching , but these degrade performance and can't be scaled to high dimensions. We emphasize that this fundamental problem causes Riemannian Diffusion Models to fail for even trivial distributions on high dimensional manifolds, which limits their applicability to relatively simple low-dimensional examples.

In our work, we propose several improvements to Riemannian Diffusion Models to stabilize their performance and enable scaling to high dimensions. In particular, we reexamine the heat kernel , which is the core building block for the denoising score matching objective. To enable denoisingscore matching, one needs to be able to sample from and compute the gradient of the logarithm of the heat kernel efficiently. This can be done trivially in Euclidean space as the heat kernel is a Gaussian distribution, but doing this is effectively intractable for general manifolds. By restricting our analysis to Riemannian symmetric spaces [9; 29], which are a class of a manifold with a special structure, we can make substantial improvements. We leverage this additional structure to quickly and precisely compute heat kernel quantities, allowing us to scale up Riemannian Diffusion Models to high dimensional real-world tasks. Furthermore, since almost all manifolds that practitioners work with are (or are diffeomorphic to) Riemannian symmetric spaces, our improvements are generalizable and not task-specific. Concretely our contributions are:

* **We present a generalized strategy for numerically computing the heat kernel on Riemannian symmetric spaces in the context of denoising score matching.** In particular, we adapt known heat kernel techniques to our more specific problem, allowing us to quickly, accurately, and stably train with denoising score matching.
* **We show how to exactly sample from the heat kernel using our quick computations above.** In particular, we show how our exact heat kernel computation enables fast simulation free techniques on simple manifolds. Furthermore, we develop a sampling method based on the probability flow ODE and show that this can be quickly computed with standard ODE solvers on the maximal torus of the manifold.
* **We empirically demonstrate that our improved Riemannian Diffusion Models improve performance and scale to high dimensional real world tasks.** For example, we can faithfully learn Wilson action on \(4 4\)\(SU(3)\) lattices (\(128\) dimensions). Furthermore, when applied to contrastively learned hyperspherical embeddings (\(127\) dimensions), our method enables better model interpretability by recovering the collapsed projection head representations. To the best of our knowledge, this is the first example where differential equation-based manifold generative models have scaled to real world tasks with hundreds of dimensions.

## 2 Background

### Diffusion Models

Diffusion models on \(^{d}\) are defined through stochastic differential equations [24; 45; 48]. Given an initial data distribution \(p_{0}\) on \(^{d}\), samples \(_{0}^{d}\) are perturbed with a stochastic differential equation

\[_{t}=(_{t},t)t+g(t)_{t} \]

where \(\) and \(g\) are fixed drift and diffusion coefficients, respectively. The time varying distributions \(p_{t}\) (defined by \(_{t}\)) evolves according to the Fokker-Planck Equation

\[p_{t}()=-(p_{t}( )(,t))+}{2}_{x}p_{t}( ) \]

and approaches a limiting distribution \( p_{T}\), which is normally a simple distribution like a Gaussian \((0,_{T}^{2}I)\) through carefully chosen \(\) and \(g\). Our SDE has a corresponding reversed SDE

\[_{t}=((_{t},t)-g(t)^{2}_{x}  p_{t}(_{t}))t+g(t)}_{t} \]

which maps \(p_{T}\) back to \(p_{0}\). Motivated by this correspondence, diffusion models approximate the score function \(_{x} p_{t}()\) using a neural network \(_{}(,t)\). To do this, one minimizes the score matching loss , which is weighted by constants \(_{t}\):

\[_{t,_{t} p_{t}}_{t}_{ }(_{t},t)-_{x} p_{t}(_{t})^{2} \]

Since this loss is intractable due to the unknown \(_{x} p_{t}(_{t})\), we instead use an alternative form of the loss. One such loss is the implicit score matching loss:

\[_{t,_{t} p_{t}}_{t}[( _{})(_{t},t)+_{ }(_{t},t)^{2}] \]

which normally is estimated using sliced score matching/Hutchinson's trace estimator[26; 47]:

\[_{t,_{t} p_{t}}_{t}[^{}D_{x} _{}(_{t},t)+_{}(_{t},t)^{2}] \]where \(\) is drawn over some \(0\) mean and identity covariance distribution like the standard normal distribution or the Rademacher distribution. Unfortunately, the added variance from the \(\) normally renders this loss unworkable in high dimensions, so practitioners instead use the denoising score matching loss

\[_{t,_{0} p_{0},_{t} p_{t}(|_{0})}_{t}\|_{}(_{t},t)-_{x} p _{t}(_{t}|_{0})\|^{2} \]

where \(p_{t}(_{t}|_{0})\) is derived from the SDE in Equation 1 and is normally tractable. Once \(_{}(_{t},t)\) is learned, we can construct a generative model by first sampling \(_{T} p_{T}\) and solving the generative SDE from \(t=T\) to \(t=0\):

\[_{t}=((_{t},t)-g^{2}(t)_{ }(_{t},t))t+g(t)}_{t} \]

Furthermore, there exists a corresponding "probability flow ODE" 

\[_{t}=((_{t},t)-}{2} _{x} p_{t}(_{t}))t \]

that has the same evolution of \(p_{t}\) as the SDE in Equation 1. This can be approximated using our score network \(_{}\) to get a Neural ODE 

\[_{t}=((_{t},t)-}{2} _{}(_{t},t))t \]

which can be used to evaluate exact likelihoods of the data .

### Riemannian Diffusion Models

To generalize diffusion models to \(d\)-dimensional Riemannian manifolds \(\), which we assume to be compact, connected, and isometrically embedded in Euclidean space, one adapts the existing machinery to the geometrically more complex space . Riemannian manifolds are deeply analytic constructs, so Euclidean space operations like vector fields \(\), gradients \(\), and Brownian motion \(_{t}\) have natural analogues for \(\). This allows one to mostly port over the diffusion model machinery from Euclidean space. Here, we highlight some of the core differences.

**The forward SDE is the heat equation.** The particle dynamics typically follow a Brownian motion:

\[_{t}=g(t)_{t} \]

Unlike in the Euclidean case, \(p_{t}\) approaches the uniform distribution \(_{}\) as \(t\) (in practice, this convergence is fast, getting within numerical precision for \(t 5\)).

**The transition density has no closed form.** Despite the fact that we work with the most simple SDE, the transition kernel defined by the manifold heat equation \(p_{t}(x_{t}|x_{0})\) has no closed form. This transition kernel is known as the heat kernel, which satisfies Equation 2 with the additional condition that, as \(t 0\), the kernel approaches \(_{x_{0}}\). We will denote this by \(K_{}(x_{t}|x_{0},t)\), and we highlight that, when \(\) is \(^{d}\), this corresponds to a Gaussian and is easy to work with.

This has several major consequences which cause prior work to favor sliced score matching over denoising score matching. First, to sample a point \(x K_{}(|x_{0},t)\), one must simulate a Geodesic Random Walk with the Riemannian exponential map \(\):

\[x_{t+ t}=_{x}(z) z(0,I_{d}) T_ {x} \]

Additionally, to calculate \(K_{}(|x_{0},t)\) or \( K_{}(x|x_{0},t)\), one must use eigenfunctions \(f_{i}\) which satisfy \( f_{i}=-_{i}f_{i}\). These \(f_{i}\) form an orthonormal basis for all \(L^{2}\) function on \(\), allowing us to write the heat kernel with an infinite sum:

\[K_{}^{}(x|x_{0},t)=_{i=0}^{}e^{-_{i}t} f_{i}(x_{0})f_{i}(x) \]

Additionally, previous work has also explored the use of the Varadhan approximation for small values of \(t\) (which uses the Riemannian logarithmic map \(\)) :

\[K_{}(x|x_{0},t)(0,)(_{ }(x_{0},x))_{x} K_{}(x|x_{0},t) _{x}(x_{0}) \]Method

The key problem with applying denoising score matching in practice is that the heat kernel computation is too expensive and inaccurate. Notably, simulating a geodesic random walk is expensive since it requires many exponential maps. Furthermore, the eigenfunction expansion in Equation 13 requires more and more eigenfunctions (numbering in the tens of thousands) as \(t 0\). Worse still, these eigenfunctions formulas are not well-known for most manifolds, and, even when explicit formulas exist, they can be numerically unstable (like in the case of \(S^{n}\)). One possible way to alleviate this is to use Varadhan's approximation for small \(t\), but this is also unreliable except for very small \(t\).

To remedy this issue, we instead consider the case of Riemannian symmetric spaces . We emphasize that most manifold generative modeling applications already model on Riemannian Symmetric Spaces like the sphere, torus, or Lie Groups, so we do not lose applicability by restricting our attention here. Furthermore, for surfaces (which have appeared as generative modeling test tasks in the literature ), one can always define a generative model by mapping the data points to \(S^{2}\), learning a generative model there, and mapping back . We empathize that, outside of these two examples, we are unaware of any other manifolds which have been used for Riemannian generative modeling tasks. For this very reason, we will also focus primarily on the compact case, since the irreducible (base case) noncompact symmetric spaces are all diffeomorphic to Euclidean space, although we discuss the extensions of our method in Appendix A.2.

### Heat Kernels on Riemannian Symmetric Spaces

In this section, we will define Riemannian Symmetric Spaces and showcase their relationship with the heat kernel. We empathize that our exposition is neither rigorous nor fully defines all terms, although it provides a general intuition with which to build our calculations. We urge interested readers to consult a book  or monograph  for a full treatment of the subject.

**Definition 3.1**.: _A Riemannian Symmetric Space is a Riemannian manifold such that, for all points \(x\), there exists a local isometry s.t. \(s(x)=x\) and \(D_{x}s=-_{T_{x}}\)._

Intuitively, this condition means that, at each point, the manifold "looks the same" in all directions, which simplifies the analysis. While this is defined as a local condition, it also has a global consequence: all Riemannian symmetric spaces are quotients \(G/K\) for Lie Groups \(G\) and compact isotropic subgroups \(K\). Importantly, most Riemannian manifolds that are used in practice are symmetric:

**Examples**.: _Lie Groups \(G G/\{e\}\) (where \(\{e\}\) is the trivial Lie Group), the sphere \(S^{n} SO(n+1)/SO(n)\), and hyperbolic space \(H^{n} SO(n,1)/O(n)\) are all symmetric spaces._

On symmetric spaces, one can define a special structure called the maximal torus:

**Definition 3.2** (Maximal Torus).: _A maximal torus \(T\) is a compact, connected, and abelian subgroup of \(G\) that is not contained in any other such subgroup. For symmetric spaces \(G/K\), the maximal torus is defined by quotienting out the maximal torus of \(G\). Importantly, the maximal torus is isomorphic to a standard torus \(T^{n}(S^{1})^{n}\)._

Intuitively, the maximal torus forms the "most stable" part of the symmetric space, and it shows up as a natural object when performing analysis due to its isometry with the standard flat torus.

**Examples**.: _For the sphere \(S^{n}\), a maximal torus is any great circle. For the Lie group of unitary matrix \(U(n)\), the maximal torus is the set of all diagonal matrices \(\{(e^{i_{1}},,e^{i_{n}}):_{k}[0,2)\}\)._

To work with the maximal torus algebraically (as we shall do in our definition of the heat kernel), we instead analyze the root system.

**Definition 3.3** (Roots of a Maximal Torus).: _We define a root \(\) be a vector (really a covector) that corresponds with a character \(\) of \(T\) s.t._

\[( X)=(2 i( X)) \]

_We define the set \(R^{+}\) to be the set of all positive roots. The multiplicity \(m_{}\) of a root \(\) is the dimension of the eigenspace over the Lie algebra \(\)_

\[\{Z:[H,Z]= i(H)Z\; H\} \]

**Examples**.: _For all spheres dimension \(n\), there is only one root of multiplicity \(n-1\), which corresponds (roughly) to measuring the angle on the maximal torus/great circle._

Notably, the heat kernel is invariant on the maximal torus. This allows us to write out the formula for the heat kernel on the general manifold as a simplified formula on the maximal torus

**Proposition 3.4** (Heat Kernel Reduces on Maximal Torus).: _The Laplace-Beltrami operator on \(\) (the manifold generalization of the standard Laplacian) induces the "radial" Laplacian on \(T\):_

\[L_{r}=_{T}+_{ R^{+}}m_{}( h) \]

_where \(_{T}\) is the standard Laplacian on the torus. Here, \(h\) is the "flat" coordinate of \(x\) on the maximal torus (e.g. for sphere this is the angle between \(x\) and \(x_{0}\))._

One can readily see that the above formula allows one to reduce computations over \(\) to computations over \(T\), which greatly reduces dimensionality (\(n\) dimensions to \(1\) dimension for spheres and \(O(n^{2})\) dimensions to \(O(n)\) for Matrix Lie Groups).

### Improved Heat Kernel Estimation

We now use this maximal torus perspective to improve computations related to the heat kernel. In particular, we can greatly improve both the speed and fidelity of the numerical evaluation during our training process.

#### 3.2.1 Eigenfunction Expansion Restricted to the Maximal Torus

We note that the maximal torus relationship in Proposition 3.4 reduces the eigenfunction expansion in Equation 13 to an eigenfunction expansion of the induced Laplacian on the maximal torus. This has implicitly appeared in previous work when defining Riemannian Diffusion Models on \(S^{2}\) and \(SO(3)\)[4; 34], allowing one to rewrite the summation as, respectively

\[K^{EF}_{S^{2}}(x|x_{0},t)=_{l=0}^{}(2l+1)P_{l}((x,x_{0 }))e^{-l(l+1)t} \]

where \(P_{l}\) are the Legendre Polynomials and

\[K^{EF}_{SO(3)}(x|x_{0},t)=}_{l=0}^{}e^{-2l(l+1)t} $} \]

By making this relationship explicit, we can directly generalize this formulation to other symmetric spaces, e.g. \(SU(3)\)

\[K^{EF}_{SU(3)}(x|x_{0},t)=_{p,q=1}^{}(^{p,q}( ,)+^{q,p}(,))e^{-+q^{2}+p)}{6}} \]

where \(\) are the (real) irreducible representations and \(,\) are the induced angles between \(x\) and \(x_{0}\).

By reducing the dimensionality, this strategy generally makes the summation more tractable. Furthermore, for cases such as the hypersphere \(S^{n}\), the eigenfunctions are numerically unstable (rendering them useless for our purposes), while the maximal torus setup stable.

#### 3.2.2 Controlling Errors for Small Time Values

While the torus eigenfunction expansion greatly reduces the computational cost (particularly for higher dimensions), for small values of \(t\) the heat kernel scaling terms \(e^{-_{i}t}\) remain large as one increases \(i,_{i}\). As a result, one must evaluate many more terms (often in the thousands), and the result can be very numerically unstable even with double precision.

To address this problem, we examine several refined versions of the Varadhan approximation that use the fact that the manifold is symmetric. These approximations allow us to control the number of eigenfunctions required and, in some cases, completely obviate the need for them entirely.

#### The Schwinger-Dewitt Approximation

The Varadhan approximation is constructed by approximating the heat kernel with a Gaussian distribution (with respect to the Riemannian distance function). However, doing this does not account for the curvature of the manifold. Incorporating this premise, we get the Schwinger-Dewitt approximation :

\[K^{}_{}(x|x_{0},t)=_{x_{0}}(x)^{1/ 2}e^{-(x_{0},x)^{2}}{4t}+}}{(4 t)^{ /2}} \]

Here, \(_{x_{0}}(x)=(D_{x_{0}}_{x_{0}}(_{x_{0}}(x)))\) is the (unnormalized) change of volume term introduced by the exponential map, and \(R\) is the scalar curvature of the manifold. Generally, this is much more accurate than Varadhan's approximation as it better accounts for the curvature of the manifold through the \(\) term, retaining accuracy up to moderate time values.

\(\) appears to be a rather computationally demanding term since it requires evaluating the determinant of a Jacobian. Indeed, this naively takes \(O(^{3})\) evaluations which is completely inaccessible in higher dimensions. However, we again emphasize the fact that we are working with symmetric spaces: \(\) has a particularly simple formula defined by our flat coordinate \(h\) from above:

\[_{x_{0}}(x)=_{ R^{+}}()^{m_{}} \]

#### Sum Over Paths

The fact that we can derive a better approximation using a different power of \(\) points to deeper connections between the heat kernel and the "Gaussian" with respect to distance. We draw inspiration from several Euclidean case examples, such as the flat torus  or the unit interval . For those cases, the heat kernel is derived by summing a Gaussian over all possible paths connecting \(x_{0}\) and \(x\). While this formula does not exactly lift over to Riemannian symmetric spaces, there exists an analogue for Lie Groups :

\[K^{}_{}(x|x_{0},t)=}}{(4 t)^{ /2}}_{2 n}_{ R^{+}}e^{-}{4t}} \]

Here, \(\) is infinite tangent space grid that corresponds with loops of the torus (i.e. \(2 n\) circular intervals), and \(^{2}\) is a manifold specific constant. We note that the product over \(R^{+}\) is exactly the \(\) change in variables as given above since \(m_{}=1\), but we simply extend this to every other root.

Generally, this formula is rather powerful as it gives us an exact (albeit infinite) representation for the heat kernel. Compared to the eigenfunction expansion in Equation 13, the Sum Over Paths representation is accurate for small \(t\), which nicely complements the fact that the eigenfunction representation is accurate for large \(t\). This formula does generalize to split-rank Riemannian symmetric spaces like odd dimensional spheres (more details in Appendix A.1) and noncompact spaces (Appendix A.2).

#### 3.2.3 A Unified Heat Kernel Estimator

We unify these approximations into a single heat kernel estimator. Our computation method splits up the heat kernel evaluation based on the value of \(t\) and applies an eigenfunction summation or an improved small time approximation accordingly. This allows us to effectively control the errors at both the small and large time steps while significantly reducing the number of function evaluations for each. Our full algorithm is outlined in Algorithm 1.

```
Hyperparameters: Riemannian symmetric space \(\), number of eigenfunctions \(n_{e}\), time value cutoff \(\), (optional, depending on if \(\) is a Lie Group) number of paths \(n_{p}\) Input: source \(x_{0}\), time \(t\), query value \(x\)  Compute if\(t<\)then if\(\) is a Lie Group then return\(K_{}^{}(x|x_{0},t)\) truncated to \(|n|<n_{p}\) in the summation over \(\). else return\(K_{}^{}(x|x_{0},t)\).  end if else return\(K_{}^{}(x|x_{0},t)\) truncated to \(|n|<n_{e}\). end if remark\(_{x} K_{}\) can be computed with autodifferentiation.
```

**Algorithm 1**Heat Kernel Computation

We ablate the accuracy of our various heat kernel (score) approximations in Figure 1 for \(S^{2}\), \(S^{127}\), and \(SO(3)\). In general, we found that computing with standard eigenfunctions was too costly and too prone to numerical blowup, and Varadhan's approximation was simply too inaccurate. For some spaces like \(S^{127}\), a combination of preexisting methods (like is briefly explored in ) would not work since \(K_{}^{}\) NaNs out before Varadhan becomes accurate.

### Exact Heat Kernel Sampling

To train with the denoising score matching objective, one must produce heat kernel samples to monte carlo samples the loss. Typically, Riemannian Diffusion Models sample by discretizing a Brownian motion through a geodesic random walk . However, this can be slow as it requires taking many computationally expensive exponential map steps on \(\) and can drift off the manifold due to compounding numerical error . In this section, we discuss strategies to sample from the heat kernel quickly and exactly.

#### Cheap Rejection Sampling Methods

When our dimension is low enough, we can sample using rejection sampling using our fast heat kernel evaluator. The key detail is the prior distribution, which needs to have a closed form density, be easy to sample from, and must not deviate from the heat kernel too much. For large time steps \(t\) the natural prior distribution is uniform. Conversely, for small time steps, we instead use the wrapped Gaussian distribution, which can be sampled by passing a tangent space Gaussian through the exponential map

Figure 1: **We compare the various heat kernel estimators on a variety of manifolds. We plot the relative error compared with \(t\). Our improved small time asymptotics allow us to control the numerical error, while baseline Varadhan is insufficient. For \(S^{127}\), Varadhan will still produce an error of \(10\%\) when the eigenfunction expansion NaNs out, so we need to use Schwinger-Dewitt**

and has a density

\[p_{}(x|x_{0},t)=/2}}_{2  n}_{ R^{+}}()e^{-}{4t}} \]

In both cases, the ratio can be trivially bounded by examining their behavior numerically.

**Heat Kernel ODE Sampling**

As an alternative, we notice that we can apply the probability flow ODE to sample from the heat kernel exactly. In particular, we draw a sample \(x_{T}_{}\), where \(T\) is large enough s.t. \(K_{}(|x_{0},T)\) is close to the uniform distribution (within numerical precision). We then solve the ODE \(x_{T}=-_{x} K_{}(x_{s}|x_{0},s)\) from \(s=T\) to \(s=t\). As the sampling procedure follows the probability flow ODE, this is guaranteed to produce samples from \(K_{}(|x_{0},t)\).

This is solvable as a manifold ODE, as previous works have already developed adaptive manifold ODE solvers . Furthermore, by Proposition 3.4, we can restrict our vector field to the maximal torus and solve it there. Note that this allows us to use preexisting Euclidean space solvers since the torus is effectively Euclidean space. Lastly, we can scale the time schedule of the ODE (with a scheme like a variance-exploding schedule) to stabilize the numerical values.

## 4 Related Work

Our work exists in the established framework of differential equation-based Riemannian generative models. Early methods generalized Neural ODEs to manifolds[15; 37; 38], enabling training with maximal likelihood. More recent methods attempt to remove the simulation components[3; 42], but this results in unscalable or biased objectives. We instead work with diffusion models, which are based on scores and SDEs and do not have any of these issues. In particular, we aim to resolve the main gap that prevents Riemannian Diffusion Models from scaling to high dimensions.

Riemannian Flow Matching (RFM)  is a concurrent work that attempts to achieve similar goals (i.e. scaling to high dimensions) by generalizing flow matching  to Riemannian manifolds. The fundamental difficulty is that one must design smooth vector fields that flows from a base distribution (ie the uniform distribution) to a data point. RFM introduces several geodesic-based vector fields, but these break the smoothness assumption (see Figure 2), which we found to be detrimental for some of the densities we considered (e.g. Figure 3). However, similar to the Euclidean case, RFM with the diffusion path corresponds to score matching with the probability flow ODE, so our work provides the numerical computation necessary to construct such a flow.

## 5 Experiments

### Simple Test Tasks

We start by comparing our accurate denoising score matching objective with the inaccurate approximation scheme suggested by  based on \(50\) eigenfunctions. We test on the compiled Earth science

Figure 2: **We visualize the vector fields generated by the flow matching geodesic path and our score matching diffusion path.** These are done on \(S^{1}\). (a) The flow matching path has a discontinuity at the pole. (b) The marginal densities of the flow matching path are not smooth and transition sharply at the boundary. (c) Our score matching path has a smooth density and smooth vectors. (d) At the pole, our score matching path anneals to \(0\) to maintain continuity.

datasets from , detailing results are in Table 1. Generally, our accurate heat kernel results in a substantial improvement and matches sliced score matching. Note that we do not expect our method to outperform sliced score matching since these datasets are low dimensional and the objectives are equivalent (up to a small additional variance).

We also compare directly with RFM on a series of increasingly complex checkerboard datasets on the flat torus. These datasets have appeared in prior work to measure model quality [3; 5; 37]. As a result of the non-smooth vector field dynamics, we find that RFM degrades in performance as the checkerboard increases in complexity, and is unable to learn past a certain point. Our visualized results are given in Figure 3.

### Learning the Wilson Action on \(Su(3)\) Lattices.

We apply our method to learn densities on \(SU(3)^{4 4}\). We consider the data density defined by the Wilson Action :

\[p(K) e^{-S(K)} S(K)=-_{x,y 2^{2}_{4}}\; (K_{x,y}K_{x+1,y}K^{*}_{x+1,y+1}K^{*}_{x,y+1}) \]

For our experiments, we take \(=9\) as a standard hyperparameter and learn this distribution, achieving an effective sample size of \(0.62\). Generally, learning to sample from \(p(K)\) directly requires a separate objective that is not based on score matching[6; 50]. However, to showcase our score matching techniques, we instead learn the density through precomputed samples, and concurrent work has shown that this type of training can be coupled with diffusion guidance to improve variational inference methods . As such, our model has the potential to improve \(SU(n)\) lattice QCD samplers, although we leave this task for future work. We also note that our model can likely be further improved by building data symmetries into the score network .

### Contrastively Learned Hyperspherical Embeddings

Finally, we examine contrastively learning . Standard contrastive losses optimize embeddings of the data that lie on the hypersphere. Paradoxically, these embeddings are unsuitable for most

 Method (\(S^{2}\)) & Volcano & Earthquake & Flood & Fire \\  Sliced Score Matching & -4.92 ± 0.25 & -0.19 ± 0.07 & 0.45 ± 0.17 & -1.33 ± 0.06 \\  Denoising Score Matching (inaccurate) & -1.28 ± 0.28 & 0.13 ± 0.01 & 0.73 ± 0.04 & -0.60 ± 0.18 \\ Denoising Score Matching (accurate) & -4.69 ± 0.29 & -0.27 ± 0.05 & 0.44 ± 0.03 & -1.51 ± 0.13 \\ 

Table 1: **We measure the improvement of our improved heat kernel estimator on downstream climate science tasks and report negative log likelihood (\(\)). Without our accurate heat kernel estimator, the denoising score matching loss produces subsandard results.**

Figure 3: **We compare Riemannian score matching and flow matching on increasingly complex checkerboard patterns on the torus. Flow matching learns suboptimal distributions with noticeable artifacts (like blurriness and spurious peaks) for simpler distributions and fails outright for more complex checkerboards. Conversely, Riemannian score matching learns accurate densities.**

downstream tasks, so practitioners instead use the penultimate layer . This degrades interpretability, since the theoretical analyses in this field work on the hyperspherical embeddings .

We investigate this issue further in the context of out of distribution (OOD) detection. We use the pretrained embedding network from CIDER. Using the hyperspherical representation for OOD detection produces very subpar results. However, using likelihoods from our Riemannian Diffusion Model stabilizes performance and achieves comparable results with other penultimate feature-based methods (see Figure 2). We emphasize that the embedding network has been tuned to optimize the performance using the penultimate layer. Since our established theory exclusively focuses on the properties of the hyperspherical embedding, the fact that our Riemannian Diffusion Models can extract a comparable representation can lead to more principled improvements for future work.

## 6 Conclusion

We have introduced several practical improvements for Riemannian Diffusion Models that leverage the fact that most relevant manifolds are Riemannian symmetric spaces. Our improved capabilities allow us, for the first time, to scale differential equation manifold models to hundreds of dimensions, where we showcase applications in lattice QCD and constrastive learning. We hope that our improvements help open the door to the broader adoption of Riemannian generative modeling techniques.

## 7 Acknowledgements

This project was supported by NSF (#1651565), ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), and CZ Biohub. AL is supported by a NSF Graduate Research Fellowship and MX is supported by a Stanford Graduate Fellowship.