# Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation

Shiji Zhao\({}^{1}\), Ranjie Duan\({}^{2}\), Xizhe Wang\({}^{1}\), Xingxing Wei\({}^{1}\)

\({}^{1}\)Institute of Artificial Intelligence, Beihang University, Beijing, China

\({}^{2}\)Security Department, Alibaba Group, Hangzhou, China

{zhaoshiji123,xizhewang,xxwei}@buaa.edu.cn, ranjieduan@gmail.com

Corresponding Author.

###### Abstract

Adversarial Training (AT) has been widely proved to be an effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD) has demonstrated its superior performance in improving the robustness of small student models with the guidance of large teacher models. However, both AT and ARD encounter the robust fairness problem: these models exhibit strong robustness when facing part of classes (easy class), but weak robustness when facing others (hard class). In this paper, we give an in-depth analysis of the potential factors and argue that the smoothness degree of samples' soft labels for different classes (i.e., hard class or easy class) will affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Based on the above finding, we propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of Knowledge Distillation (KD). Specifically, ABSLD adaptively reduces the student's error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples' soft labels during the training process, and the smoothness degree of soft labels is controlled by assigning different temperatures in KD to different classes. Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT, ARD, and robust fairness methods in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness.

## 1 Introduction

Deep neural networks (DNNs) have achieved great success in various tasks, e.g., classification , detection , and segmentation . However, DNNs are vulnerable to adversarial attacks [30; 35; 33; 34], where adding small perturbations to the input examples will lead to misclassification. To enhance the robustness of DNNs, Adversarial Training (AT) [20; 41; 32; 14] is proposed and has been proven to be an effective method to defend against adversarial examples. To further improve the robustness, Adversarial Robustness Distillation (ARD)  as a variant of AT is proposed and aims to transfer the robustness of the large models into the small models based on Knowledge Distillation (KD), and further researches [44; 45; 43; 12; 42] show the excellent performance of ARD.

Although AT and ARD can remarkably improve the adversarial robustness, some researches [2; 31; 39; 19; 28; 38] demonstrate the robust fairness problem: these models perform strong robustness on part of classes (easy class) but show high vulnerability on others (hard class). This phenomenon will raise further attention to class-wise security. Specifically, an overall robust model appears to be relatively safe for model users, however, the robust model with poor robust fairness will lead to attackers targeting vulnerable classes of the model, which leads to significant security risks topotential applications. Different from simply improving the overall robustness, some methods are proposed to address the robust fairness problem in AT and ARD [39; 36; 38; 19; 28] (i.e., improving the worst-class robustness as much as possible without sacrificing too much overall robustness). However, the robust fairness problem still exists and requires further to be explored.

For that, we give an in-depth analysis of the potential factors to influence robust fairness in the optimization objective function. From the perspective of the training sample, the sample itself has a certain degree of biased behavior, which is mainly reflected in the different learning difficulties and various vulnerabilities to adversarial attacks. For this reason, previous works apply the re-weighting ideology to achieve robust fairness for different types of classes in the optimization process [39; 36; 38]. However, as another important factor in the optimization objective function, the role of the labels applied to guide the model's training is ignored. Labels can be divided into two types, including one-hot labels and soft labels, where the soft labels are widely studied [29; 11] and have been proven effective in improving the performance of DNNs. Inspired by this, we try to explore robust fairness from the perspective of samples' soft labels. Interestingly, we first find that the smoothness degree of soft labels for different classes (i.e., hard and easy class) can affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Intuitively speaking, sharper soft labels mean larger supervision intensity, while smoother soft labels mean smaller supervision intensity, so it is helpful to improve robust fairness by assigning sharp soft labels for hard classes and smooth soft labels for easy classes.

Based on the above finding, we further propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of knowledge distillation. ABSLD can adaptively adjust the smoothness degree of soft labels by re-temperature the teacher's soft labels for different classes, and each class has its own teacher's temperatures based on the student's error risk. For instance, when the student performs more error risk in some classes, ABSLD will compute sharp soft labels by assigning lower temperatures, and the student's learning intensity for these classes will relatively increase compared with other classes. After the optimization, the student's robust error risk gap between different classes will be reduced. The code can be found in https://github.com/zhaoshiji123/ABSLD.

Our contribution can be summarized as follows:

* We explore the labels' effects on the adversarial robust fairness of DNNs, which is different from the existing sample perspective. To the best of our knowledge, we are the first one to find that the smoothness degree of samples' soft labels for different types of classes can affect the robust fairness from both empirical observation and theoretical analysis.
* We propose the Anti-Bias Soft Label Distillation (ABSLD) to enhance the adversarial robust fairness within the framework of knowledge distillation. Specifically, we re-temperate the teacher's soft labels to adjust the class-wise smoothness degree and further reduce the student's error risk gap between different classes in the training process.

Figure 1: The comparison between the sample-based fair adversarial training and our label-based fair adversarial training. For the former ideology in (a), the trained model’s bias is avoided by re-weighting the sample’s importance according to the different contribution to fairness. For the latter ideology in (b), the trained model’s bias is avoided by re-temperature the smoothness degree of soft labels for different classes.

* We empirically verify the effectiveness of ABSLD. Extensive experiments on different datasets and models demonstrate that our ABSLD can outperform state-of-the-art methods against a variety of attacks in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness.

## 2 Related Work

### Adversarial Training

To defend against the adversarial examples, Adversarial Training (AT) [20; 41; 32; 13; 25] is regarded as an effective method to obtain robust models. AT can be formulated as a min-max optimization problem as follows:

\[_{}E_{(x,y)}[_{}(f(x+ ;),y)],\] (1)

where \(f(;)\) represents a deep neural network with weight \(\), \(D\) represents a data distribution with clean example \(x\) and the ground truth label \(y\). \(\) represents the optimization loss function, e.g. the cross-entropy loss. \(\) represents the adversarial perturbation, and \(\) represents a bound, which can be defined as \(=\{:||||\}\) with the maximum perturbation scale \(\). To further improve the performance, some variant methods of AT appear including regularization [21; 41; 32], using additional data [27; 22], and optimizing iteration process [15; 23]. Different from the above methods for improving the overall robustness, in this paper, we focus on solving the robust fairness problem.

### Adversarial Robustness Distillation

Knowledge Distillation  as a training method can effectively transfer the large model's knowledge into the small model's knowledge, which has been widely applied in different areas. To enhance the adversarial robustness of small DNNs, Goldblum et al.  first propose Adversarial Robustness Distillation (ARD) by applying the clean prediction distribution of strong robust teacher models to guide the adversarial training of student models. Zhu et al.  argue that the prediction of the teacher model is not so reliable, and composite with unreliable teacher guidance and student introspection during the training process. RSLAD  applies the teacher clean prediction distribution as the guidance to train both clean examples and adversarial examples. MTARD [43; 42] applies clean teacher and adversarial teacher to enhance both accuracy and robustness, respectively. AdaAD  adaptively searches for optimal match points by directly applying the teacher adversarial prediction distribution in the inner maximization. In this paper, we explore how to enhance robust fairness within the framework of knowledge distillation.

### Adversarial Robust Fairness

Some researchers address the robust fairness problem from different views [39; 18; 36; 38; 19; 37; 28] and improve the fairness without losing too much robustness. The most intuitive idea is to give different weights to the sample of different classes in the optimization process, and Xu et al.  propose Fair Robust Learning (FRL), which adjusts the loss weight and the adversarial margin based on the prediction accuracy of different classes. Ma et al.  finds the trade-off exists between robustness and fairness and propose Fairly Adversarial Training to mitigate this phenomenon by adding a regularization loss to control the variance of class-wise adversarial error risk. Sun et al.  propose Balance Adversarial Training (BAT) to achieve both source-class fairness (different difficulties in generating adversarial examples from each class) and target-class fairness (disparate target class tendencies when generating adversarial examples). Wu et al.  argue that the maximum entropy regularization for the model's prediction distribution can help to achieve robust fairness. Wei et al.  propose Class-wise Calibrated Fair Adversarial Training (CFA) to address fairness by dynamically customizing adversarial configurations for different classes and modifying the weight averaging operation. To enhance the ARD robust fairness, Yue et al.  propose Fair-ARD by re-weighting different classes based on the vulnerable degree. Different from these sample-based fairness methods, we try to solve this problem from the perspective of samples' labels, by adjusting the class-wise smoothness degree of samples' soft labels in the optimization process.

## 3 Robust Fairness via Smoothness Degree of Soft Labels

As an important part of the model optimization, label information used to guide the model plays an important role. The label can be divided into one-hot labels and soft labels, where one-hot labels only contain one class's information and soft labels can be considered as an effective way to alleviate over-fitting and improve the performance . Previous methods usually ignore the class-wise smoothness degree of soft labels, either applying the same smoothness degree , or uniformly changing the smoothness degree of soft labels for all the classes without deliberate adjustments . Different from previous methods, we are curious about _if we adjust the class-wise smoothness degree of soft labels, will it influence the class-wise robust fairness of the trained model?_ Intuitively speaking, different smoothness degree of soft labels denote different supervision intensity, which means that it is possible to achieve fairness by adjusting the smoothness degree of soft labels. Here we try to explore the relationship between class-wise smoothness degree of soft labels and the robust fairness from both empirical observation and theoretical analysis.

### Empirical Observation

Here, we focus on the impact of the class-wise smoothness degree of soft labels on adversarial training. First, we train the model with soft labels that have the same smoothness degree for all types of classes (smoothing coefficient2 is 0.2). Then we assign different smoothness degrees of soft labels for hard classes and easy classes: specifically, we manually use sharper soft labels for hard classes (smoothing coefficient is 0.05) and smoother soft labels for easy classes (smoothing coefficient is 0.35). We conduct the experiment based on the SAT  shown in Figure 2.

The result shows that the class-wise smoothness degree of soft labels has an impact on class-wise robust fairness. When we apply the sharper smoothness degree of soft labels for hard classes and the smoother smoothness degree of soft labels for easy classes, the class-wise robust fairness problem can be alleviated. More specifically, for the two worst classes (class 4, 5), the robust accuracy of ResNet-18 guided by the soft label distribution with the same smoothness degree is 24.2%, and 30.9%, and the robust accuracy of ResNet-18 guided by the soft label distribution with different smoothness degree is 30.2%, and 39.1%, which exists an obvious improvement for the class-wise robust fairness, and the average robust accuracy has a slight improvement (52.12% vs 52.36%). Similar performance can also be observed in MobileNet-v2. This phenomenon indicates that appropriately assigning class-wise smoothness degrees of soft labels can be beneficial to achieve robust fairness.

Figure 2: The class-wise and average robustness of DNNs guided by soft labels with the same smoothness degree (SSD) and different smoothness degree (DSD) for different classes, respectively. For the soft labels with different smoothness degrees, we use sharper soft labels for hard classes and use smoother soft labels for easy classes. We select two DNNs (ResNet-18 and MobileNet-v2) trained by SAT  on CIFAR-10. The robust accuracy is evaluated based on PGD. The checkpoint is selected based on the best checkpoint of the highest mean value of all-class average robustness and the worst class robustness following . We see that blue lines and red lines have similar average robustness, but the worst robustness of blue lines are remarkably improved compared with red lines.

### Theoretical Analysis

Here we try to theoretically analyze the impact of the smoothness degree of soft labels on class-wise fairness. Firstly, we want to analyze the model bias performance with the guidance of the soft label distribution with the same smoothness degree. Then we give Corollary 1 by extending the prediction distribution of binary linear classifier into the prediction distribution of DNNs based on the theoretical analysis in  and .

**Corollary 1**.: _A dataset \((x,y)\) contains \(2\) classes (hard class \(c_{+}\) and easy class \(c_{-}\)). Based on the label distribution \(y\), the soft label distribution with same smoothness degree \(P_{ 1}=\{p_{c_{+}}^{ 1},p_{c_{-}}^{ 1}\}\) can be generated and satisfies:_

\[1>p_{c_{-}}^{ 1}(x_{c_{-}})=p_{c_{+}}^{ 1}(x_{c_{+}})>0.5,\] (2)

_If a DNN model \(f\) is optimized by minimizing the average optimization error risk in \(\) with the guidance of the equal soft labels \(P_{ 1}=\{p_{c_{+}}^{ 1},p_{c_{-}}^{ 1}\}\), and obtain the relevant parameter \(_{ 1}\), where the optimization error risk is measured by Kullback-Leibler divergence loss (\(KL\)):_

\[f(x;_{ 1})=*{arg\,min}_{f}_{(x,y) }(KL(f(x;_{ 1});P_{ 1})),\] (3)

_then the error risks (the expectation that samples are wrongly predicted by the model) for classes \(c_{+}\) and \(c_{-}\) have a relationship as follows:_

\[R(f(x_{c_{+}};_{ 1}))>R(f(x_{c_{-}};_{ 1})),\] (4)

_where the error risks can be defined:_

\[R(f(x_{c_{+}};_{ 1}))=_{(x,y) }(CE(f(x_{c_{+}};_{ 1});y_{c_{+}})),\] \[R(f(x_{c_{-}};_{ 1}))=_{(x,y) }(CE(f(x_{c_{-}};_{ 1});y_{c_{-}})).\] (5)

Corollary 1 demonstrates that when optimizing hard and easy classes with equal intensity, the model will inevitably be biased, and this bias mainly comes from the characteristics of the sample itself and is not related to the optimization method. Based on the Corollary 1, we can further analyze the performance differences with the guidance of the different types of soft labels. Here we provide Theorem 1 about the relationship between class-wise smoothness degree of soft labels and fairness.

**Theorem 1**.: _Following the setting in Corollary 1, for a dataset \(\) containing \(2\) classes (\(c_{+}\) and \(c_{-}\)), two soft label distribution (\(P_{ 1}=\{p_{c_{+}}^{ 1},p_{c_{-}}^{ 1}\}\) and \(P_{ 2}=\{p_{c_{+}}^{ 2},p_{c_{-}}^{ 2}\}\)) exist, where \(P_{ 2}\) have a correct prediction distribution but have a limited different class-wise smoothness degree of soft labels (\(v_{1}>0\), \(v_{2}>0\)):_

\[1>p_{c_{+}}^{ 2}(x_{c_{+}})= p_{c_{+}}^{ 1}(x_{c_{+}})+v_{1}>p_{c_{+}}^{ 1}(x_{c_{+}})=\] \[p_{c_{-}}^{ 1}(x_{c_{-}})>p_{c_{-}}^{ 2}(x_{c_{-}})=p_{c_{-}}^{  1}(x_{c_{-}})-v_{2}>0.5,\] (6)

_then the model is trained with the guidance of the soft label distribution \(P_{ 1}\) and soft label distribution \(P_{ 2}\) and obtains the trained model parameters \(_{ 1}\) and \(_{ 2}\), respectively. If the model parameter \(_{ 2}\) still satisfies: \(R(f(x_{c_{+}};_{ 2}))>R(f(x_{c_{-}};_{ 2}))\), then the model's error risk for hard classes \(c_{+}\) and easy classes \(c_{-}\) has a relationship as follows:_

\[R(f(x_{c_{+}};_{ 1}))-R(f(x_{c_{-}};_{ 1}))>R(f(x_{c_{+} };_{ 2}))-R(f(x_{c_{-}};_{ 2})).\] (7)

The proof of Theorem 1 can be found in Appendix A.1. Based on Theorem 1, the class-wise smoothness degree of soft labels theoretically has an impact on class-wise robust fairness. If the soft label distribution with different smoothness degree \(P_{ 2}\) is applied to guide the model training, where the sharper smoothness degree of soft labels for hard classes and smoother smoothness degree of soft labels for easy classes, the model will appear smaller error risk gap between easy and hard class compared with the soft label distribution with same smoothness degree \(P_{ 1}\), which demonstrates better robust fairness. The Theorem 1 theoretically demonstrates that if we appropriately adjust the class-wise smoothness degree of soft labels, the model can achieve class-wise robust fairness.

Anti-Bias Soft Label Distillation

### Overall Framework

Based on the above finding, adjusting the class-wise smoothness degree of soft labels can be regarded as a potential way to obtain robust fairness. Then we consider introducing this ideology into Knowledge Distillation (KD), which has been proven to be an effective method to improve the robustness of small models [44; 45; 43; 12; 42]. Since the core idea of KD is to use the teacher's soft labels to guide the student's optimization process, we can adjust the class-wise smoothness degree of soft labels and obtain the student with both strong robustness and fairness.

Here we propose the Anti-Bias Soft Label Distillation (ABSLD) to obtain a student with adversarial robust fairness. We formulate the optimization objective function for ABSLD as follows:

\[*{arg\,min}_{f_{s}}_{(x,y)}(_{absld}(,x;f_{s},f_{t}^{{}^{}})),\] (8)

\[s.t.\ R(f_{s}(_{k}))=_{i=1}^{C}R(f_{s}(_{i})),\] (9)

where \(x_{i}\) and \(_{i}\) are the clean examples and adversarial examples of the \(i\)-th class, \(f_{s}\) denotes the student model, \(f_{t}^{{}^{}}\) denotes the teacher model with Anti-Bias Soft Labels, C is the total number of classes, \(_{absld}\) is the loss function, and \(R(f_{s}(_{k}))\) denotes the robust error risk of \(k\)-th class in student model \(f_{s}\). Here we apply the Cross-Entropy loss \(CE(f_{s}(_{k}),y)\) as the evaluation criterion of the optimization error risk \(R(f_{s}(_{k}))\) following .

### Re-temperate Teacher's Soft Labels

In order to adjust the class-wise smoothness degree of soft labels in ARD, two options exist: one is to use student feedback to update the teacher parameter in the process of optimizing students, but this option requires retraining the teacher model, which may bring the pretty optimization difficulty and computational overhead; the other is to directly adjust the smoothness degree of soft labels for different classes. Inspired by , we apply the temperature as a means of directly controlling the smoothness degree of soft labels during the training process. Here, we provide Theorem 2 about the relationship between the teacher's temperature and the student's class-wise error risk gap.

**Theorem 2**.: _If the teacher \(f_{t}^{{}^{}}\) has a correct prediction distribution, the teacher temperature \(_{c+}^{t}\) of hard class \(c+\) is positively correlated with the error risk gap for student \(f_{s}\), and the teacher temperature \(_{c-}^{t}\) of easy class \(c-\) is negatively correlated with the error risk gap for student \(f_{s}\)._

The proof of Theorem 2 can be found in Appendix A.2. In particular, just as the conclusion in : The teacher has a more correct prediction distribution than the student even in the worst classes, which means Theorem 2 holds in most cases. Theorem 2 demonstrates that the different temperatures can influence the student robust fairness: when the student's error risk of \(k\)-th class is larger than the average error risk, we think that this type of class is relatively hard compared with others, then the teacher temperature for \(k\)-th class will reduce and the smoothness degree of soft labels will be sharper, and the optimization gap between teacher distribution and student distribution in \(k\)-th class will corresponding increase, leading to stronger learning intensity for \(k\)-th class and final reduce the student's class-wise optimization error risk gap.

To achieve the above optimization goal, we adjust the teacher's \(k\)-th class temperature \(_{k}^{t}\) for the guidance of adversarial examples as follows:

\[_{k}^{t}=_{k}^{t}-(_ {k}))-_{i=1}^{C}R(f_{s}(_{i}))}{max(|R(f_{s}(_ {k}))-_{i=1}^{C}R(f_{s}(_{i}))|)},\] (10)

where \(\) is the learning rate, \(max(.)\) denotes taking the maximum value, and \(|.|\) represents taking the absolute value, \(max(|.|)\) is applied for regularization to maintain the stability of optimization. The update operation in Eq.(10) can change the teacher temperature \(_{k}^{t}\) based on the gap between the student's \(k\)-th class error risk \(R(f_{s}(_{k}))\) and the average error risk \(_{i=1}^{C}R(f_{s}(_{i}))\).

Meanwhile, according to , both clean and adversarial examples exist the fairness problems and can affect each other, so it is necessary to achieve fairness for both types of data. Since clean and adversarial examples of the same classes may have different error risks during the training process, it is unreasonable to use the same set of class temperatures to adjust both clean and adversarial examples. Here we simultaneously optimize the student's clean error risk \(R(f_{s}(x_{k}))\) and the student's robust error risk \(R(f_{s}(_{k}))\), in other words, we apply two different sets of teacher temperatures: \(_{k}^{t}\) and \(_{k}^{t}\), for the adjustment of the teacher's soft labels for clean and adversarial examples, respectively.

Then we extend the Anti-Bias Soft Label Distillation based on  and the loss function \(_{absld}\) in Eq.(8) can be formulated as follows:

\[_{absld}(,x;f_{s},f_{t}^{{}^{}})= _{i=1}^{C}KL(f_{s}(_{i};^{s}),f_{t}^{{}^{}}(x_{i}; _{i}^{t}))+(1-)_{i=1}^{C}KL(f_{s}(x_{i};^{ s}),f_{t}^{{}^{}}(x_{i};_{i}^{t})),\] (11)

where \(KL\) represents Kullback-Leibler divergence loss, \(\) is the trade-off parameter between accuracy and robustness, \(f(x;)\) denotes model \(f\) predicts the output probability of \(x\) with temperature \(\) in the final softmax layer. It should be mentioned that the teacher is frozen and we apply the teacher's predicted soft labels \(f_{t}^{{}^{}}(x_{k};_{k}^{t})\) for \(k\)-th class to generate adversarial examples \(_{k}\) as follows:

\[_{k}=_{k}-x_{k}||}{argmax}KL(f_{s} (_{k};^{s}),f_{t}^{{}^{}}(x_{k};_{k}^{t})),\] (12)

and the complete process can be viewed in Algorithm 1.

```
0: the train dataset \(\), Student \(f_{s}\) with random initial weight \(_{s}\) and temperature \(_{s}\), pretrained robust teacher \(f_{t}\), the initial temperature \(_{y}^{t}\) and \(_{y}^{t}\) for the teacher's soft labels for clean examples \(x\) and adversarial examples \(\), where \(y=\{1,,C\}\), the max training epochs \(max\)-\(epoch\).
1:for\(0\) to \(max\)-\(epoch\)do
2:for\(k\) in \(y=\{1,,C\}\)do
3:\(R(f_{s}(_{k}))=0,\ R(f_{s}(x_{k}))=0.\)// Initialize the clean and robust error risk for each class.
4:endfor
5:for\(Every\ minibatch(x,y)\ in\ \)do
6:\(=^{-}||}{argmax}KL(f_{s}(;^{s}),f_{t}^{{}^{}}(x;_{y}^{t})).\)// Get adversarial examples with teacher's soft labels.
7:\(_{s}=_{s}-_{}_{absld}(,x ;f_{s},f_{t}^{{}^{}}).\)// Update student weight \(_{s}\) with teacher's soft labels.
8:\(R(f_{s}(_{y}))=R(f_{s}(_{y}))+CE(f_{s}(),y).\)// Calculate robust error risk for each class.
9:\(R(f_{s}(x_{y}))=R(f_{s}(x_{y}))+CE(f_{s}(x),y).\)// Calculate clean error risk for each class.
10:endfor
11:for\(k\) in \(y=\{1,,C\}\)do
12: Update \(_{k}^{t}\) and \(_{k}^{t}\) based on Eq.(10). // Re-temperate teacher's soft labels for \(\) and \(x\).
13:endfor
14:endfor ```

**Algorithm 1** Overview of ABSLD

## 5 Experiments

### Experimental Settings

We conduct our experiments on three datasets: CIFAR-10 , CIFAR-100, and Tiny-ImageNet . The results about CIFAR-100 and Tiny-ImageNet are in Appendix A.4 and A.5, respectively.

**Baselines.** We consider the standard training method and eight state-of-the-art methods as comparison methods: AT **methods**: SAT , and TRADES ; **ARD methods**: RSLAD , and AdaAD ; **Robust Fairness methods**: FRL , BAT , CFA , and Fair-ARD .

**Student and Teacher Networks.** For the student model, here we consider two networks for CIFAR-10 and CIFAR-100 including ResNet-18  and MobileNet-v2 . For the teacher model, we follow the setting in , and we select WiderResNet-34-10  trained by  for CIFAR-10 and WiderResNet-70-16 trained by  for CIFAR-100. The teacher's performance is in Appendix A.7.

**Training Setting.** For ABSLD, we train the model using the Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.1, a momentum of 0.9, and a weight decay of 2e-4. The learning rate \(\) of temperature is initially set as 0.1. For CIFAR-10 and CIFAR-100, we set the training epochs to 300. The learning rate is divided by 10 at the 215-th, 260-th, and 285-th epochs; We set the batch size to 128 for both CIFAR-10 and CIFAR-100 following . For the inner maximization, we use a 10-step PGD with a random start size of 0.001 and a step size of 2/255, and the perturbation is bounded to the \(L_{}\) norm \(\) = 8/255. The more training setting can be found in Appendix A.3.

**Metrics.** We apply two metrics to evaluate the robust fairness: Normalized Standard Deviation (NSD3)  and the worst-class robustness . **NSD can reflect robust fairness while also considering the average robustness.** The smaller standard deviation means better fairness, and the larger average means better robustness, so the smaller NSD means better comprehensive performance in terms of fairness and robustness. **The worst-class robustness** is easy to understand, and a larger value means better fairness. For CIFAR-10, we directly report the worst class robust accuracy; For CIFAR-100 and Tiny-ImageNet, due to the poor performance of the worst class robustness and only 100 images (CIFAR-100) or 50 images (Tiny-ImageNet) for each class in the test set, we follow the operation in CFA  and report the worst 10% class robust accuracy. Besides, we also report the **average robustness** as a reference. The attack setting for evaluation can be found in Appendix A.3.

### Robust Fairness Performance

The performances of ResNet-18 and MobileNet-v2 trained by our ABSLD and other baseline methods under the various attacks are shown in Table 1, Table 2 for CIFAR-10. The results demonstrate that ABSLD achieves the state-of-the-art worst-class robustness on CIFAR-10. For ResNet-18 on CIFAR-10, ABSLD improves the worst class robustness by 2.0%, 3.2%, 2.4%, and 2.9% compared with the best baseline method against the FGSM, PGD, CW\({}_{}\), and AA. Moreover, ABSLD shows relevant superiority on MobileNet-v2 compared with other methods.

Moreover, ABSLD can also show the best comprehensive performance of fairness and robustness (NSD) on CIFAR-10. For ResNet-18 on CIFAR-10, ABSLD reduces the NSD by 0.028, 0.032, 0.017, and 0.024 compared with the best baseline method against the FGSM, PGD, CW\({}_{}\), and AA. The result indicates that although the trade-off between robustness and fairness still exists as  say, we obtain the highest robust fairness while sacrificing the least average robustness.

Meanwhile, we visualize the class-wise robustness in Figure 3, and the result shows that the robustness of harder classes (class 3, 4, 5, 6) have different levels of improvement, which demonstrates that our method is beneficial to the overall robust fairness but not only to the worst class. Moreover, combined

   &  &  &  & _{}\)} &  \\   & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD \\  Natural & 94.57 & 86.00 & 0.035 & 18.66 & 9.00 & 0.436 & 0 & – & – & 0 & – & 0 & – & 0 & – & 0 & – \\  SAT & 84.03 & 63.00 & 0.118 & 56.70 & 26.70 & 0.283 & 49.34 & 21.00 & 0.332 & 48.99 & 19.90 & 3.624 & 46.180 & 3.85 \\ TRADE & 81.45 & 67.06 & 0.113 & 56.66 & 36.66 & 0.267 & 51.78 & 30.40 & 0.301 & 49.15 & 27.10 & 0.341 & 48.17 & 25.090 & 0.350 \\ RSLA & 82.94 & 66.30 & 0.122 & 59.51 & 34.70 & 2.044 & 54.00 & 28.50 & 0.276 & **25.51** & 27.00 & 2.96 & **41.25** & 25.50 & 0.304 \\ AdaD & 84.73 & 68.10 & 0.114 & 59.70 & 34.80 & 0.246 & 53.32 & 29.30 & -0.285 & 52.30 & -26.00 & 0.312 & -50.91 & 24.70 & -0.322 \\ FRL & 82.25 & 64.06 & 0.114 & 55.03 & 37.10 & 0.230 & 49.05 & 51.70 & 20.44 & 47.88 & 36.04 & 26.66 & -45.42 & 28.16 & -0.280 \\ BAT & 86.72 & 72.30 & 0.092 & **69.97** & 33.80 & 0.255 & 49.60 & 22.70 & 0.325 & 47.49 & 19.50 & 0.354 & 48.18 & 27.00 & 0.341 \\ CF & 78.64 & 63.06 & 0.123 & 57.95 & 36.80 & 0.231 & 54.42 & 33.30 & 0.258 & 50.91 & 27.50 & 0.288 & 50.37 & 26.70 & 0.296 \\ Fair-ARD & 83.81 & 69.40 & 0.102 & 88.41 & 38.80 & 0.251 & 59.90 & 29.90 & 2.966 & 28.30 & 0.312 & 47.97 & 25.10 & 0.338 \\
**ABSLD** & 83.04 & 68.10 & 0.103 & 59.83 & **40.50** & **0.202** & **54.50** & **36.50** & **0.216** & 51.77 & **32.80** & **0.290** & 50.25 & **31.00** & **0.256** \\  

Table 1: Result in average robustness(%) (Avg.\(\)), worst robustness(%) (Worst\(\)), and normalized standard deviation (NSD\(\)) on CIFAR-10 of ResNet-18.

   &  &  &  & _{}\)} &  \\   & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD & Avg. &  & NSD \\  Natural & 94.57 & 86.00 & 0.035 & 18.69 & 9.00 & 0.436 & 0 & – & – & – & 0 & – & – & 0 & – & – & 0 & – & – \\ SAT & 84.03 & 63.90 & 0.118 & 56.70 & 26.70 & 0.283 & 49.34 & 21.00 & 0.332 & 48.99 & 19.90 & 3.624 & 46.180 & 3.85 \\ TRADE & 81.45 & 67.06 & 0.113 & 56.66 & 36.66 & 0.

with Figure 2, we can find that the trend of class-wise bias is similar in different training strategies, indicating that the bias may be sourced from the dataset itself, which further confirms Corollary 1.

In particular, we compare ABSLD with Fair-ARD , which is an adaptive re-weighting method on ARD. From the results, ABSLD has better robust fairness performance, which means that our proposed re-temperature method has superiority compared to the re-weighting method.

### Ablation Study

To certify the effectiveness of our method, we perform ablation experiments on every component of ABSLD. First, based on the baseline method , we re-temperate the teacher's soft labels for the adversarial examples but do not re-temperate the teacher's soft labels for the clean examples (Baseline+ABSLD\({}_{adv}\)); then we re-temperate the teacher's soft labels for both the adversarial examples and clean examples (ABSLD). The results are shown in Figure 4. The results demonstrate the effectiveness of our ABSLD, and pursuing fairness for clean examples can also help robust fairness for adversarial examples as claimed in .

Meanwhile, to verify the optimization effect of our method, we visualize the standard deviation of class-wise optimization error risk in the training process (the optimization error risk is normalized by dividing the mean), which can reflect the optimization gap between different classes. We visualize the standard deviation of both the clean and adversarial optimization error risk, and the results are shown in Figure 5 and Figure 6. We can notice that the standard deviation of the baseline increases as the training epoch increases, which demonstrates that the baseline pays more attention to reducing the error risk of easy class, but the error risk of hard class is ignored, eventually leading to robust unfairness. On the contrary, our ABSLD can remarkably reduce the standard deviation of student's class-wise optimization error risk, which demonstrates the effectiveness of our method.

Figure 4: Ablation study for Baseline, Figure 5: Standard deviation Figure 6: Standard deviation Baseline+ABSLD\({}_{adv}\), and ABSLD. of class-wise clean optimiza- of class-wise adversarial optimization error risk.

Figure 3: The class-wise robustness (PGD) of models guided by RSLAD and ABSLD on CIFAR-10. We can see that the harder classes’ robustness (class 3, 4, 5, 6) of ABSLD (blue lines) have different levels of improvement compared with RSLAD (red lines).

Conclusion

In this paper, we comprehensively explored the potential factors that influence robust fairness in the model optimization process. We first found that the smoothness degrees of soft labels for different classes can be applied to eliminate the robust fairness based on empirical observation and theoretical analysis. Then we proposed Anti-Bias Soft Label Distillation (ABSLD) to address the robust fairness problem by adjusting the class-wise smoothness degree of soft labels. We adjusted the teacher's soft labels by assigning different temperatures to different classes based on the performance of student's class-wise error risk. A series of experiments proved that ABSLD was superior to state-of-the-art methods in the comprehensive metric (NSD) of robustness and fairness.