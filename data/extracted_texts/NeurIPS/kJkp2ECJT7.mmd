# Towards Flexible Visual Relationship Segmentation

Fangrui Zhu\({}^{1}\) Jianwei Yang\({}^{2}\) Huaizu Jiang\({}^{1}\)

\({}^{1}\)Northeastern University \({}^{2}\)Microsoft Research

https://neu-vi.github.io/FleVRS

###### Abstract

Visual relationship understanding has been studied separately in human-object interaction (HOI) detection, scene graph generation (SGG), and referring relationships (RR) tasks. Given the complexity and interconnectedness of these tasks, it is crucial to have a flexible framework that can effectively address these tasks in a cohesive manner. In this work, we propose FleVRS, a single model that seamlessly integrates the above three aspects in standard and promptable visual relationship segmentation, and further possesses the capability for open-vocabulary segmentation to adapt to novel scenarios. FleVRS leverages the synergy between text and image modalities, to ground various types of relationships from images and use textual features from vision-language models to visual conceptual understanding. Empirical validation across various datasets demonstrates that our framework outperforms existing models in standard, promptable, and open-vocabulary tasks, _e.g._, **+1.9**\(mAP\) on HICO-DET, **+11.4**\(Acc\) on VRD, **+4.7**\(mAP\) on unseen HICO-DET. Our FleVRS represents a significant step towards a more intuitive, comprehensive, and scalable understanding of visual relationships.

## 1 Introduction

An image is not merely a collection of objects. Understanding the visual relationships between different entities at pixel-level through segmentation is a fundamental task in computer vision, which has broad applications in autonomous driving , behavior analysis , navigation , _etc_. Furthermore, segmenting relational objects extends beyond mere detection, playing a

Figure 1: FleVRS is a single model trained to support **standard, promptable** and **open-vocabulary** fine-grained visual relationship segmentation (**<subject** mask, relationship categories, object mask>**). It can take images only or images with structured prompts as inputs, and segment all existing relationships or the ones subject to the text prompts.

crucial role in improving visual understanding and providing a more comprehensive abstraction on the visual contents and interactions among them.

Ideally, a visual relationship segmentation (VRS) model should demonstrate flexibility across three key dimensions. 1) **Capability of segmenting various types of relationships**, including both human-centric and generic ones. These relationships are defined as triplets in the form of <subject, predicate, object>. Human-object interaction (HOI) detection , which we adapt into HOI segmentation in our work, exemplifies this capability, such as <person, ride, horse> in Fig. 1. Panoptic scene graph generation (SGG) , captures generic spatial or semantic relationships among pairs of objects in an image, _e.g._, bench on pavement in Fig. 1. A unified model that can handle these tasks concurrently is essential, as it eliminates the need for separate designs and modifications for each specific task. 2) **Grounding of relational subject-object pairs with different prompts**. Given various textual prompts, the model should output the desired entities and relationships, facilitating a more natural and intuitive user interface. For instance, it should be able to detect just the person in an image or all possible interactions between a person and a horse, as illustrated in Fig. 1. 3) **Open-vocabulary recognition of visual relationships**. In realistic open-world applications, the model should generalize to new scenarios without requiring annotations for new concepts not seen during training. This capability includes detecting novel objects, relationships, and their combinations.

Existing models in visual relationship segmentation (VRS) have targeted aspects of the desired capabilities but fall short of providing a comprehensive solution, as detailed in Tab. 1. Models have typically focused on tasks like human-object interaction (HOI) detection  and panoptic SGG . Although models such as  have attempted to unify VRS under a single framework, they need additional pretraining on HOI datasets (Tab. 1) and lack features such as promptable segmentation, which allows for dynamic entity and relationship generation based on textual prompts, as well as capabilities for open-vocabulary promptable segmentation. Efforts to detect instances referred to by textual prompts have been made , but these models fail to capture all desired entities or relationships comprehensively and struggle with classifying multi-label interactions between the pairs, limiting their effectiveness in complex scenarios. Although recent vision-language grounding models like  and multimodal large language models such as  exhibit enhanced capabilities in grounding instances specified by free-form text and show strong generalization over novel concepts, they still do not generate the required pairs in the format of segmentation masks. Furthermore, these models require significant computational resources and additional vision models for precise localizations. For open-vocabulary VRS, existing works  leverage textual embeddings to transfer knowledge. However, models  fall short in grounding diverse prompts, while  is exclusively designed for HOI detection, not generic VRS.

To address the limitations in existing models, we introduce FleVRS, a flexible one-stage framework capable of performing standard, promptable, and open-vocabulary visual relationship segmentation _simultaneously_. Our approach integrates human-centric (HOI segmentation) and generic VRS (Panoptic SGG) by adopting SAM  to unify different types of annotations into segmentation masks and using a query-based Transformer architecture that outputs triplets in the format <subject, predicate, object>. The model enhances its interactive capabilities by accepting textual prompts as inputs. These prompts are converted into textual queries that assist the decoder in accurately identifying and localizing objects within the relationships. Additionally, we unify the labels from different datasets into a shared textual space, transforming classification into a process of matching with a set of textual features. Leveraging textual features from the CLIP model , we enable the effective matching of visual features with textual knowledge of novel concepts. This design

    &  &  &  &  \\  & HOI & SGG & & & \\  RLIPv2  & ✓ & ✓ & ✗ & ✓ & Two \\ UniVRD  & ✓ & ✓ & ✗ & ✗ & Two \\ SSAS  & ✗ & ✓ & ✗ & ✗ & One \\ GEN-VLKFT  & ✓ & ✗ & ✗ & ✓ & One \\  FleVRS (Ours) & ✓ & ✓ & ✓ & ✓ & One \\   

Table 1: **Comparisons with previous representative methods in three aspects of model capabilities**. To the best of our knowledge, our FleVRS is the first one-stage model capable of performing standard, promptable, and open-vocabulary visual relationship segmentation all at once.

inherently supports open-vocabulary and promptable relationship segmentation without pre-defining the number of object or predicate categories, facilitating dynamic and extensive adaptability.

Our FlevRS proposes a unified framework that integrates standard, promptable, and open-vocabulary VRS tasks into a single system, as detailed in Tab. 1, providing greater flexibility compared to existing methods. It employs a mask-based approach to effectively manage various VRS tasks, enabling adaptation to different types of annotations, including HOI detection and panoptic SGG. Our architecture incorporates dynamic prompt handling, which supports both prompt-based and open-vocabulary settings, allowing our model to combine promptable queries with open-vocabulary capabilities to ground novel relational objects.

We evaluate our FlevRS on standard, promptable, and open-vocabulary VRS tasks, _i.e._, HOI segmentation [4; 18] and panoptic SGG . Crucially, we demonstrate competitive performance from three perspectives - standard (**40.5**_vs._**39.1**\(mAP\) on HICO-DET ), promptable (**56.8**_vs._**33.5**sIoU on VRD ), and open-vocabulary (**31.7**_vs._**25.6**\(mAP\) for "unseen object" on HICO-DET ) visual relationship segmentation.

In summary, our main contributions are as follows: 1) We introduce a flexible one-stage framework capable of segmenting both human-centric and generic visual relationships across various datasets. 2) We present a promptable visual relationship learning framework that effectively utilizes diverse textual prompts to ground relationships. 3) We demonstrate competitive performance in both standard close-set and open-vocabulary scenarios, showcasing the model's strong generalization capabilities.

## 2 Related Work

**Visual Relationship Detection** (VRD) is split into two lines of works, including human-object interaction (HOI) detection [4; 18] and panoptic scene graph generation (SGG) [37; 85]. They are defined as detecting triplets in the form of <subject, predicate, object> triplet, where subject or object includes object box and category. HOI detection aims to detect human-centric visual relationships, while PSG focuses on generic object pairs' relationships. Previous works [1; 13; 16; 32; 35; 41; 47; 57; 81; 89; 95; 96; 97; 100; 104; 105; 106] usually train specialist models on a single data source and tackle them separately. Departing from this traditional bifurcation, UniVRD  initiated the development of a unified model for VRD, with subsequent efforts like [91; 92] advancing relational understanding through large-scale language-image pre-training. Unlike the two-stage approach of , which performs object detection before decoding visual relationships, our method employs a one-stage design that decodes objects and their relationships simultaneously. Crucially, our model extends beyond standard VRD capabilities to support promptable and open-vocabulary visual relationship segmentation, enhancing detailed scene comprehension.

**Referring relationship and visual grounding.** The most relevant work to ours is referring visual relationship introduced in , where the model detects the subject and object depicting the structured relationship <subject, predicate, object>. One-stage [38; 75], two-stage [63; 107] and three-stage  methods are proposed to localize the two entities' boxes iteratively based on the given structured prompt <subject, predicate, object>. Unlike these methods, our approach allows for more _flexible_ textual prompts without requiring the complete specification of the triplet. As shown in Fig. 1, our model can handle queries that include a single item (e.g., predicate) or a combination of two (e.g., predicate and object). Additionally, our method is capable of performing standard and open-vocabulary VRS. Visual grounding represents another related area, where models output bounding boxes [7; 8; 20; 29; 40; 52; 83] or object masks [9; 17; 44; 45; 51; 56; 79; 87; 99] in response to textual inputs. This process requires reasoning over the entities mentioned in the text to identify the corresponding objects in the visual space. However, our task fundamentally differs from this.

Figure 2: **Examples of converting HOI detection boxes to masks. We filter out low-quality masks during training by computing IoU between the mask and box.**

In our FleVRS, the promptable VRS task goes beyond mere identification; it involves outputting segmentation masks for both subject and object pairs along with categorizing their relationships. This capability is essential for understanding and interpreting complex relational dynamics.

**Vision and language models** Recent advancements in large-scale pre-trained vision-language models (VLM) [39; 64; 68; 69; 82; 94] and multimodal large language models (MLLM) [2; 5; 76; 78; 88] have demonstrated impressive performance and generalization capabilities across a variety of vision and multimodal tasks [36; 109; 110]. However, these models primarily focus on entity-level generalization, with open-vocabulary VRS receiving less attention. While recent efforts in zero-shot HOI detection [47; 61; 80; 92] often utilize CLIP  for category classification, their open-vocabulary capabilities lack the flexibility needed for prompt-driven input. Although current VLMs and MLLMs are adept at grounding novel concepts from text, they require significant computational resources and additional visual models, and cannot directly generate comprehensive segmentation masks for subject and object pairs. In contrast, our FleVRS provides a lightweight solution that effectively supports various types of open-vocabulary VRS, enabling category classification and the integration of novel concepts from prompts.

## 3 Method

### Overview

**Standard VRS**. Given an image **I**, the goal of _standard_ visual relationship segmentation (VRS) is to detect all the visual relationships of interest, either human-centric (_i.e._, HOI detection) or the generic ones (SGG), in terms of triplets in the form of <subject, predicate, object> (masks and object categories of subject and object, and the predicate category). The subject is always human in HOI detection, whereas it can be any type of object in SGG (may or may not be human). We consider the panoptic setting  of SGG, where a model needs to generate a more comprehensive scene graph representation based on panoptic segmentation rather than rigid bounding boxes, providing a clear and precise grounding of objects. To produce fine-grained masks, we convert existing bounding box annotations from HOI detection datasets [4; 18] into segmentation masks using the foundation model SAM , as illustrated in Fig. 2. We employ a filtering approach based on Intersection over Union (IoU) to filter out inaccurate masks. Details are in Appendix.

**Promptable VRS.** Our FleVRS optionally accepts textual prompts as inputs, enabling users to specify visual relationships for promptable VRS. It accommodates three types of structured text prompts: a single element (e.g., <?, predicate,?>), any two elements (e.g., <subject, predicate,?>, <subject,?, object>), or all three elements. Consequently, the model outputs only the triplets that match the specified elements in the prompt, as depicted in the right column of Fig. 1. Without textual prompts, it functions as a standard VRS model, exhaustively generating all possible triplets, illustrated in the left part of Fig.1.

Figure 3: **Overview of FleVRS. In standard VRS, without textual queries, the latent queries perform self- and cross-attention within the relationship decoder to output a triplet for each query. For promptable VRS, the decoder additionally incorporates textual queries \(}\), concatenated with \(}\). This setup similarly predicts triplets, each based on \(}\) outputs aligned with features from the optional textual prompt \(}\).**

**Open-vocabulary VRS.** In practice, it's essential for a VRS model to adapt to new concepts, including new categories of entities (_i.e._, subject and object), predicates, and their various combinations. Expanding these concept vocabularies to encompass a wider range is particularly challenging due to the vast potential combinations and the long-tail distribution of these categories. Thus, our goal is to equip the model to operate in an open-vocabulary setting, where it can effectively handle these diversities. It it important to note that the above three capabilities are complementary; for example, the text prompts in promptable VRS can include novel object or predicate categories.

To this end, we propose integrating the above three aspects into a _single unified_ framework. Since these settings are complementary, a general-purpose model should be capable of performing various combinations of these three functions. Additionally, their inherent similarities make it more intuitive to consolidate them within a flexible, unified approach.

### Model Architecture

Inspired by the success of Transformer-based segmentation models [6; 109], we design a dual-query system for our VRS model, illustrated in Fig. 3. Latent queries, a set of learned embeddings, generate triplets (which may be empty) to formulate output masks and relationship categories. For promptable VRS, textual queries derived from input prompts are incorporated. We employ an image encoder and a pixel decoder to extract visual features, coupled with a relationship decoder that processes <subject, object> pairs and their interrelations. For open-vocabulary VRS, our approach shifts from traditional classification to a matching strategy that aligns visual and textual features for both object and predicate categories, enhancing the model's adaptability to new concepts. Each component of this architecture is elaborated further below.

**Image Encoder.** Specifically, given the image \(^{H W 3}\), it is first fed into the image encoder \(}\) to obtain multi-scale low-resolution features \(=\{_{s}^{C_{}}\}\), where the stride of the feature map \(s\{4,8,16,32\}\), and \(C_{}\) is the number of channels.

**Pixel Decoder.** A Transformer-based pixel decoder \(}\) is used to upsample \(\) and gradually generate high-resolution per-pixel embeddings \(\). \(\) is then passed to the relationship decoder \(}\) to compute cross-attention with query features.

**Textual Encoder.** When a text prompt is provided for promptable VRS, we use the textual encoder \(}\) to encode it into a set of textual queries \(}^{N_{t} C_{a}}\), where \(N_{t}\) is the number of tokens in the textual queries, and \(C_{g}\) denotes the channel number of query features. In practice, we use the textual encoder from CLIP  as \(}\). The format of the text prompt can be a single item (_e.g._ "\(<\)_p\(>\)predicate\(<\)/p\(>\)_"), two of them ("\(<\)\(s\)\(>\)_subject\(<\)/s\(>\)\(<\)\(p\)\(>\)predicate\(<\)/p\(>\)"), or all three of them, where "_predicate_" and "_subject_" denote category names of predicate and subject, respectively. "\(<\)\(s\)\(>\)", "\(<\)\(o\)\(>\)", "\(<\)\(p\)\(>\)" are used as separate tokens between subject, predicate and object in the text prompt. We could use natural language as the textual prompt instead of using a structured format. However, collecting the textual VRD data is not trivial, and we leave it as an extension of our model in future work.

**Relationship Decoder.** The relationship decoder \(}\), based on a Transformer decoder design, processes pixel decoder outputs \(\) and latent queries \(}\) to generate all possible triplets for standard VRS. Inside, masked attention  utilizes masks from earlier layers for foreground information. Each \(}\) output feeds into five parallel heads: two mask heads for subject and object masks (\(M_{s},M_{o}\)), two class heads for their categories (\(C_{s},C_{o}\)), and another class head for relationship prediction (\(C_{p}\)). During training, Hungarian matching aligns predicted triplets with ground truth. For standard VRS inference, triplets above a confidence threshold are considered final predictions. For promptable VRS, \(}\) transforms text prompts into textual queries \(}\) that are concatenated with \(}\) and input into \(}\). This process, which uses self- and cross-attention mechanisms, generates <subject, predicate, object> triplets, similar to standard VRS. An additional matching loss during training ensures the model predicts triplets as specified by the text prompt. During inference, we calculate similarity scores between the textual query feature (last token's feature of \(}\)) and the latent query outputs. We then select entities and relationships specified in the textual prompt from the top \(k\) triplets for the final outputs.

**Matching with textual features.** To enable open-vocabulary VRS, our FleVRS uses the CLIP textual encoder  to match visual features with candidate textual features for object and predicatecategories. We convert these categories into textual features using prompt templates, such as "_A photo of [predicate-ing]_" for HOI segmentation and "_A photo of something [predicate-ing] (something)_" for panoptic SGG.1 The model computes matching scores between predicted class embeddings and these textual features, allowing classification beyond the fixed vocabulary of the training set and facilitating open-vocabulary VRS. Textual prompts are similarly encoded, and their features are used to calculate similarity scores for promptable VRS inference.

### Loss functions

We use Hungarian matching during training to find the matched triplets with ground truth ones. For standard VRS, we compute focal losses \(^{s}_{b}\), \(^{o}_{b}\) and dice losses \(^{s}_{d}\), \(^{o}_{d}\) on subject and object mask predictions, cross-entropy losses \(^{s}_{c}\), \(^{o}_{c}\), \(^{p}_{c}\) on subject, object, and predicate category classifications, which can be written as

\[= _{b}_{i\{s,o\}}^{i}_{b}+_{d} _{j\{s,o\}}^{j}_{d}+_{k\{s,o,p\}}^{k}_{c} ^{k}_{c},\] (1)

where \(_{b}\), \(_{d}\), and \(_{c}\) are hyper-parameters for adjusting the weights of each loss. \(^{s}_{c}\), \(^{o}_{c}\), \(^{p}_{c}\) are different classification loss weights for subject, object, and predicate. For promptable VRS, we adopt an additional matching loss \(_{g}\) between the matched triplet class embedding and the textual query feature (the last token feature of \(}\)), which is in the form of cross-entropy loss. The final training loss is written as

\[= _{b}_{i\{s,o\}}^{i}_{b}+_{d} _{j\{s,o\}}^{i}_{d}+_{k\{s,o,p\}}^{k}_{c} ^{k}_{c}+_{g}_{g},\] (2)

where \(_{g}\) controls the weight of \(_{g}\). \(_{c}\) depends on the text prompt. For example, given \(<\)subject, predicate\(>\), there will not have \(^{s}_{c}\) and \(^{p}_{c}\) terms in Eq. (2), with subject and predicate categories being given. See the appendix for the concrete values of loss weights.

## 4 Experiments

### Experimental Settings

**Datasets** For HOI segmentation, we utilize two public benchmarks: HICO-DET  and V-COCO . To fit Our FleURS, we use SAM  to transform box annotations into masks and apply Non-Maximum Suppression (NMS) to remove overlapping masks with an IoU threshold greater than 0.1. We omit no_interaction annotations from HICO-DET due to incomplete annotation, leaving 44,329 images (35,801 training, 8,528 testing) with 520 HOI classes from 80 objects and 116 actions.2 V-COCO is built from COCO , comprising 10,396 images (5,400 training, 4,964 testing), featuring 80 objects and 29 actions, and includes 263 HOI classes. Both datasets align with COCO's object categories. For panoptic SGG, we use the PSG dataset , sourced from COCO and VG  intersections, containing 48,749 images (46,572 training, 2,177 testing) with 133 objects and 56 predicates.

**Data Structure for open-vocabulary HOI segmentation** Following prior studies , we evaluate HICO-DET under three scenarios: (1) Unseen Composition (UC), where some HOI classes are absent despite all object and verb categories being present; (2) Unseen Object (UO), where certain object classes and their corresponding HOI triplets are excluded from training; and (3) Unseen Verb (UV), where specific verb classes and their associated triplets are similarly omitted. In UC, the Rare First (RF-UC) approach targets tail HOI classes, while Non-rare First (NF-UC) focuses on head categories. Originally, UC included 120/480/600 categories for unseen/seen/full sets, which reduces to 115/405/520 after removing no_interaction annotations. For UO, we select 12 unseen objects from 80, resulting in 88/432 unseen/seen HOI categories.

**Evaluation Metric** For standard HOI segmentation, we convert the predicted masks to bounding boxes to compare with current methods, and follow the setting in  to use the mean Average Precision (mAP) for evaluation. We also turn the outputs of other methods into masks and report mask \(mAP\) for thorough comparison. An HOI triplet prediction is a true positive if (1) both predicted human and object bounding boxes/masks have IoU larger than 0.5 _w.r.t._ GT boxes/masks; (2) Both the predicted object and verb categories are correct. For HICO-DET, we evaluate the three different category sets: all 520 HOI categories (Full), 112 HOI categories (less than 10 training instances) (Rare), and the other 408 HOI categories (Non-Rare). For VCOCO, we report the role mAPs in two scenarios: (1) S1: 29 actions including 4 body motions; (2) S2: 25 actions without the no-object HOI categories. For standard panoptic SGG, following , we use \(R@K\) and \(mR@K\) metrics, which calculate the triplet recall and mean recall for every predicate category, given the top K triplets from the model. A successful recall requires both subject and object to have mask-based IoU larger than 0.5 compared to their GT masks, with the correct predicate classification in the triplet.

**Implementation Details** Following , we use 100 latent queries and 9 decoder layers in the relationship decoder. We adopt Focal-T/L  for the Image Encoder and DaViT-B/L for the pixel decoder. We use the textual encoder from CLIP to encode input text prompt and subject, object, and predicate categories. During training, we set the input image to be \(640 640\), with batch size of 64. We optimize our network with AdamW  with a weight decay of \(10^{-4}\). We train all models for 30 epochs with an initial learning rate of \(10^{-4}\) decreased by 10 times at the 20th epoch. To improve training efficiency, we initialize Our FleURS using the pre-trained weights from . For all experiments, the parameters of the textual encoder are frozen except its logit scales. The loss weights \(_{b}\), \(_{d}\), \(_{c}\) and \(_{grd}\) (superscript omitted) are set to 1,1,2, and 2. More details are in the appendix.

### Standard VRS

We evaluate our method on three benchmarks, _i.e._ HICO-DET , VCOCO  for HOI segmentation, and PSG  for the panoptic SGG.

**HOI segmentation** Since Our FleURS leverages mask supervision, either converting mask results into bounding boxes or transforming bounding boxes from previous methods' output into masks does not facilitate a completely equitable comparison. For the utmost fairness in comparison, we report both box \(mAP\) and mask \(mAP\) from the above ways. As shown in Table 2, Our FleURS shows superior

Figure 4: **Qualitative results of promptable VRS on HICO-DET  test set. We show visualizations of subject and object masks and relationship category outputs, given three types of text prompts. In (c), we show the predicted predicates in bold characters. Unseen objects and predicates are denoted in red characters.**

performance over current single-stage methods in terms of box and mask \(mAP\) on HICO-DET. We also achieve competitive performance on VCOCO , as shown in Table 3. The advantages of Our FleVRS come from: (1) one-stage Transformer-based design with fine-grained training supervision for VRS. With subject and object masks, the model has more accurate supervision, compared with box annotations that contain redundancy . (2) good language-visual alignment with the large-scale pretrained model . Our FleVRS achieves competitive results without additional training on large-scale detection datasets . Among one-stage HOI methods, our approach is simpler and able to tackle different datasets without modifications to the structure.

**Panoptic SGG** From Table 4, Our FleVRS can achieve competitive results in terms of \(R@50\) and \(R@100\) without elaborated designs for PSG, compared with most of previous work. Our FleVRS is not superior to HiLo , which is mainly due to the long-tail distribution of the dataset and the

   Model & Backbone &  \\   & & box/mask mAP\({}_{}\) & box/mask mAP\({}_{}\) \\   \\  SCG  & ResNet-50 & 31.3 / 31.3 & 24.7 / 25.0 & 33.3 / 35.5 \\ UPT  & ResNet-101 & 32.6 / 34.9 & 28.6 / 29.4 & 33.8 / 36.1 \\ STIP  & ResNet-50 & 32.2 / 30.8 & 28.2 / 28.6 & 33.4 / 32.5 \\ ViPLO  & ViT-B & 37.2 / 39.1 & 35.5 / 37.8 & 37.8 / 39.7 \\  \\  UniVRD  & ViT-L & 37.4 / - & 28.9 / - & 39.9 / - \\ PyCv  & Swin-L & 44.3 / - & 44.6 / - & 44.2 / - \\ RLIPv2  & Swin-L & 45.1 / 48.6 & 45.6 / 44.3 & 43.2 / 49.8 \\   \\  HOTR  & ResNet-50 & 25.1 / 26.5 & 17.3 / 18.5 & 27.4 / 29.0 \\ QPIC  & ResNet-101 & 29.9 / 30.5 & 23.0 / 23.1 & 31.7 / 33.1 \\ CDN  & ResNet-101 & 32.1 / 33.9 & 27.2 / 28.9 & 33.5 / 36.0 \\ RLIP (VG+COCO) & ResNet-50 & 32.8 / 34.4 & 26.9 / 27.7 & 34.6 / 36.5 \\ GEN-VLKT  & ResNet-101 & 35.0 / 35.6 & 31.2 / 32.6 & 36.1 / 37.8 \\ ERNet  & EfficientNetV2-XL & 35.9 / - & 30.1 / - & 38.3 / - \\ MUREN  & ResNet-50 & 32.9 / 35.4 & 28.7 / 30.1 & 34.1 / 37.6 \\
**Ours** & Focal-L & **38.1 / 40.5** & **33.0 / 34.9** & **39.5 / 42.4** \\   

Table 2: **Quantitative results on the HICO-DET test set. We report both box and mask \(mAP\) under the _Default_ setting  containing the _Full_ (F), _Rare_ (R), and _Non-Rare_ (N) sets. no_interaction class is removed in mask mAP. The best score is highlighted in bold, and the second-best score is underscored. ’-’ means the model did not release weights and we cannot get the mask \(mAP\). Due to space limit, we show the complete table with more models in the appendix.**

   Model & Backbone & \(_{}^{}\) & \(_{}^{}\) \\   \\ VSGNet  & ResNet-152 & 51.8 / - & 57.0 / - \\ ACP  & ResNet-152 & 53.2 / - & - / - \\ IDN  & ResNet-50 & 53.3 / - & 60.3 / - \\ STIP  & ResNet-50 & **66.0 / 66.2** & **70.7 / 70.5** \\  \\  UniVRD  & ViT-L & 65.1 / - & 66.3 / - \\ PvIC  & Swin-L & 64.1 / - & 70.2 / - \\ RLIPv2  & Swin-L & 72.1 / 71.7 & 74.1 / 73.5 \\   \\  HOTR  & ResNet-50 & 55.2 / 55.0 & 64.4 / 64.1 \\ DIRV  & EfficientDet-d3 & 56.1 / - & - / - \\ CDN  & ResNet-101 & 63.9 / 61.3 & 65.8 / 63.2 \\ RLTP  & ResNet-50 & 61.9 / 61.3 & 64.2 / 64.0 \\ GEN-VLKT  & ResNet-101 & 63.6 / 61.8 & 65.9 / 64.0 \\ ERNet  & EfficientNetV2-XL & 64.2 / - & - / - \\
**Ours** & Focal-L & 65.2 / **66.5** & 66.5 / 67.9 \\   

Table 3: **Quantitative results on V-COCO. We report both box and mask \(mAP\).The best score is highlighted in bold, and the second-best score is underscored. ’-’ means the model did not release weights and we cannot get the mask \(mAP\). Due to space limit, we show the complete table with more models in the appendix.**limitation of using CLIP to encode abstract relationships (_e.g._, entering, exiting). The model tends to predict high-frequency relationships and is hard to understand and predict low-frequency ones.

**Ablation study.** We ablate Our FleVRS by testing different encoding strategies for relationships via the textual encoder in 7. Specifically, we compare encoding object and predicate categories as <person, predicate, object> triplets or separately, associating the results with either triplet cross-entropy (CE) loss or disentangled CE loss. Results reveal that while HICO-DET benefits from the disentangled CE loss, allowing better generalization to novel concepts, VCOCO performs better with triplet CE loss due to the challenge of distinguishing verbs without corresponding objects in various contexts (_e.g._, differentiating "eat" in "a person eating an apple" _vs_ "a person eating"). Further experiments with various backbones demonstrate performance enhancements with larger models. Additionally, incorporating a box head for supervision alongside mask supervision enhances performance, which is attributed to the masked attention mechanism inspired by . Exploring the potential synergies of training across multiple datasets, we find that while unified training improves VCOCO's performance due to its smaller size, HICO-DET and PSG show limited gains. This disparity is likely due to the different predicate categories used in PSG compared to HICO-DET and VCOCO.

**Comparison with previous works.** UniVRD uses a two-stage approach, where the model first detects independent objects and then decodes relationships between them, retrieving boxes from the initial detection stage. In contrast, our method employs a one-stage approach, where each query directly corresponds to a <subject, object, predicate> triplet. This transition improves time efficiency from O(MxN) to O(K), where M is the number of subject boxes, N is the number of object boxes, and K is the number of interactive pairs. Our approach also provides greater flexibility by learning a unified representation that encompasses object detection, subject-object association, and relationship classification in a single model.

In terms of training data, we use much fewer training data (x50 less, without using VG  and Objects365 ) and Our FleVRS with the Focal-L  backbone is much smaller than UniVRD  (164M vs 640M) with LiT(ViT-H/14), we achieve comparable results(37.4 vs 38.1 on HICO-DET). While our method does not match RLIPv2  in performance, this is due to different design philosophies and goals. RLIPv2 is a two-stage approach optimized for large-scale pretraining and relies on separately trained detectors. Our FleVRS, however, is not designed for pretraining and does not include a separately trained detector. Our focus is on enhancing the flexibility of the VRS model without _directly_ training on extensive curated data(x50 more, VG and Objects365). Thus, the differences in performance are attributed to the scale and design objectives. We further discuss the FLOPs and the number parameters of the backbone compared to previous works in the Appendix.

### Promptable VRS

We evaluate the ability of promptable VRS on the VRD dataset , to compare with . As in Table 5, Our FleVRS can locate entities given flexible text query inputs and performs better localizing

   Method & Backbone & R/mR@20 & R/mR@50 & R/mR@100 \\    \\ IMP  & VGG-16 & 17.9 / 7.35 & 19.5 / 7.88 & 20.1 / 8.02 \\ MOTIFS  & VGG-16 & 20.9 / 9.60 & 22.5 / 10.1 & 23.1 / 10.3 \\ VCTree  & VGG-16 & 21.7 / 9.68 & 23.3 / 10.2 & 23.7 / 10.3 \\ GPSNet  & VGG-16 & 18.4 / 6.52 & 20.0 / 6.97 & 20.6 / 7.17 \\   \\ PSGTR  & ResNet-101 & **28.2** / **15.4** & **32.1** / **20.3** & **35.3** / **21.5** \\ PSGFormer  & ResNet-101 & 18.0 / 14.2 & 20.1 / 18.3 & 21.0 / 19.8 \\   \\ HiLo  & Swin-L & 40.6 / 29.7 & 48.7 / 37.6 & 51.4 / 40.9 \\ 
**Ours** & Focal-L & 27.0 / 15.4 & 31.0 / 18.3 & 31.7 / 18.8 \\   

Table 4: **Quantitative results on PSG. The best score is highlighted in bold, and the second-best score is underscored.**

    & No subject & No object & Only predicate \\  & S-IoU & O-IoU & S-IoU & O-IoU \\   \\ VRD  & 0.208 & 0.008 & 0.024 & 0.026 \\ SSAS  & 0.335 & 0.363 & 0.334 & 0.365 \\ 
**Ours** & **0.568** & **0.364** & **0.556** & **0.366** \\   

Table 5: Comparison of promptable VRD results with the baseline on VRD dataset .

subjects and objects. Our FlevRS gets particular better results on localizing subjects (0.568 _vs_ 0.335, 0.556 _vs_ 0.334), which is mainly because there are fewer categories in subjects compared with objects and lots of subjects are humans, making it easier to segment subjects. We further evaluate our promptable VRS approach on HICO-DET and PSG, as they contain rich relationship labels. Since there are no previous baselines, we show qualitative results in Fig. 4. We visualize the subject and object masks with the highest matching score for each example. We can see that the model is able to localize subject and object masks and predict their relationships given the structured textual prompt. We further perform postprocessing way to search triplets from standard VRS output, which serves as another baseline to show the effectiveness of our method. Please refer to section E in the appendix for results and discussions of fair comparison.

**Difference with standard REC tasks** The referring expression comprehension (REC) tasks on benchmarks like RefCOCO , RefCOCO+ , and RefCOCOg  are designed to detect objects based on free-form textual phrases, such as "a ball and a cat" or "Two pandas lie on a climber." In contrast, the promptable VRS task in our work focuses on detecting subject-object pairs within a structured prompt format, such as <7, sit_on, bench> or <person,?, horse>, as illustrated in Fig. 1 of the main paper. Our FlevRS is designed to encode and compute similarity scores for each of these elements separately. Our primary focus is on relational object segmentation based on a single structured query, which differs significantly from the objectives of REC benchmarks.

### Open-vocabulary VRS

We conduct open-vocabulary experiments following the defined zero-shot HOI detection setting [23; 25; 26; 47] on HICO-DET. As shown in Table 6, Our FlevRS surpass previous single-dataset methods across all settings, with its open-vocabulary capabilities stemming from the knowledge transferred from CLIP . GEN-VLKT  also leverages CLIP to facilitate open-vocabulary capabilities by encoding <person, predicate, object> as a triplet and using it for HOI category classification. In contrast, our approach separates the encoding of predicate and object, enhancing the model's generalization ability over novel concepts.

## 5 Conclusion

In this work, we present a novel approach for visual relationship segmentation that integrates the three critical aspects of a flexible VRS model: standard VRS, promptable querying, and open-vocabulary capabilities. Our FlevRS demonstrates the ability to not only support HOI segmentation and panoptic SGG but also to do so in response to various textual prompts and across a spectrum of previously unseen objects and interactions. By harnessing the synergistic potential of textual and visual features, our model delivers promising experimental results on existing benchmark datasets. We hope our work can serve as a solid stepping stone for pursuing more flexible visual relationship segmentation models.

  Method & Unseen & Seen & Full \\  _Rareer Unseen Composition_ & & & \\ VCL  & 10.06 & 24.28 & 21.43 \\ ATL  & 9.18 & 24.67 & 21.57 \\ FCL  & 13.16 & 24.23 & 22.01 \\ GEN-VLKT  & 21.36 & 32.91 & 30.56 \\
**Ours** & **26.06** & **39.61** & **36.60** \\  _Non-rate Fast Unseen Composition_ & & & \\ VCL  & 16.22 & 18.52 & 18.06 \\ ATL  & 18.25 & 18.78 & 18.67 \\ FCL  & 18.66 & 19.55 & 19.37 \\ GEN-VLKT  & 25.05 & 23.38 & 23.71 \\
**Ours** & **26.62** & **31.70** & **30.17** \\  _Unseen Object_ & & & \\ FCL  & 0.00 & 13.71 & 11.43 \\ ATL  & 5.05 & 14.69 & 13.08 \\ GEN-VLKT  & 10.51 & 28.92 & 25.63 \\
**Ours** & **14.48** & **35.28** & **31.71** \\  _Unseen Verib_ & & & \\ GEN-VLKT  & 20.96 & 30.23 & 28.74 \\
**Ours** & **21.50** & **35.63** & **33.09** \\  

Table 6: Results of open-vocabulary HOI detection on HICO-DET.

    & HICO-DET & VCOCO & PSG \\  & mask mAP\({}_{}\) & mask AP\({}_{}^{}\) & R/mR@20 \\  _Different losses_ & & & \\ Disentangled CE loss & **40.5** & 62.1 & **27.0 / 15.4** \\ Triple CE loss & 36.8 & **66.5** & 25.5 / 14.6 \\ Disentangled CE loss + Triplet CE loss & 39.0 & 64.5 & 26.5 / 14.8 \\  _Important visual backbones_ & & & \\ Focal Tray & 34.2 & 59.8 & 25.8 / 15.0 \\ Focal Large & **40.5** & **66.5** & **27.0 / 15.4** \\  _Different design choices_ & & & \\ Box head only & 33.0 & 62.0 & - \\ Mask head only & 40.5 & 66.5 & 27.0 / 15.4 \\ Mask and box head & **41.2** & **67.0** & - \\  _Different training datasets_ & & & \\ Single source & **40.5** & 66.5 & 27.0 \\ HD-DET+VCOCO & **40.3** & **66.9** & - \\ HICO-DET+VCOCO+PSG & 40.0 & 66.4 & **27.6** \\   

Table 7: Ablations of different loss types, backbones, design choices and training sets. We adopt the Focal-L backbone by default.