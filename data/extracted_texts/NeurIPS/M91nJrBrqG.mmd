# Is Your HD Map Constructor Reliable

under Sensor Corruptions?

 Xiaoshuai Hao\({}^{1}\) Mengchuan Wei\({}^{1}\) Yifan Yang\({}^{1}\) Haimei Zhao\({}^{2}\)

**Hui Zhang\({}^{1}\) Yi Zhou\({}^{1}\) Qiang Wang\({}^{1}\) Weiming Li\({}^{1}\) Lingdong Kong\({}^{3,}\) Jing Zhang\({}^{2,}\)**

\({}^{1}\)Samsung R&D Institute China-Beijing

\({}^{2}\)The University of Sydney \({}^{3}\)National University of Singapore

{xshuai.hao,mc.wei,yifan.yang,hui123.zhang,yi0813.zhou}@samsung.com

{qiang.w,weiming.li}@samsung.com hzha7798@uni.sydney.edu.au

lingdong@comp.nus.edu.sg jing.zhang1@sydney.edu.au

###### Abstract

Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, _e.g._, adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of \(29\) types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across \(31\) HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.

+
Footnote †: leftmargin=*] \({}^{1}\)Samsung R&D Institute China–Beijing

https://mapbench.github.io

## 1 Introduction

HD maps are fundamental components in autonomous driving systems, providing centimeter-level details of traffic rules, vectorized topology, and navigation information . These maps enable the ego-vehicle to accurately locate itself on the road and anticipate upcoming features . HD map constructors formulate this task as predicting a collection of vectorized static map elements in bird's eye view (BEV), _e.g._, pedestrian crossings, lane dividers, road boundaries, _etc._.

Existing HD map construction methods can be categorized based on the input sensor modality: camera-only  LiDAR-only , and camera-LiDAR fusion  models. Each sensor poses distinct functionalities: cameras capture semantic-rich information from images, while LiDAR provides explicit geometric information from point clouds . Generally, camera-based methods outperform LiDAR-only methods, and fusion-based methods yield the most satisfactory results . However, current model designs and performance evaluations are based on ideal driving conditions, _e.g._, clear daytime weather and fully functional sensors .

In real-world driving scenarios, adverse conditions, such as bad weather, motion distortions, and sensor malfunctions (frame loss, sensor crashes, incomplete echoes, _etc._) are unavoidable . It remains unclear how existing HD map construction methods perform under such challenging yet safety-critical conditions, highlighting the need for a thorough out-of-domain robustness evaluation.

To address this gap, we introduce MapBench, the first comprehensive benchmark aimed at evaluating the reliability of HD map construction methods against natural corruptions that occur in real-world environments. We thoroughly assess the model's robustness under corruptions by investigating three popular configurations: camera-only, LiDAR-only, and camera-LiDAR fusion models. Our evaluation encompasses \(8\) types of camera corruptions, \(8\) types of LiDAR corruptions, and \(13\) types of camera-LiDAR corruption combinations, as depicted in Fig. 2 and Fig. 4. We define three severity levels for each corruption type and devise appropriate metrics for quantitative robustness comparisons.

Utilizing MapBench, we perform extensive experiments on a total of \(31\) state-of-the-art HD map construction methods. The results, as shown in Fig. 1, reveal significant discrepancies in model performance across "clean" and corrupted datasets. Key findings from these evaluations include:

1) Among all camera/LiDAR corruptions, Snow corruption significantly degrades model performance; it covers the road, rendering map elements unrecognizable and posing a major threat to autonomous driving. Besides, sensor failure corruptions (_e.g._, Frame Lost and Incomplete Echo) are also challenging for all models, demonstrating the serious threats of sensor failures on HD map models.

2) While Camera-LiDAR fusion methods have shown promising performance by incorporating information from both modalities , existing methods often assume access to complete sensor information, leading to poor robustness and potential collapse when sensors are corrupted or missing.

Through extensive benchmark studies, we further unveil crucial factors for enhancing the reliability of HD map constructors against sensor corruption. The key contributions of this work are three-fold:

* We introduce MapBench, making the first attempt to comprehensively benchmark and evaluate the robustness of HD map construction models against various sensor corruptions.
* We extensively benchmark a total of \(31\) state-of-the-art HD map constructors and their variants under three configurations: camera-only, LiDAR-only, and camera-LiDAR fusion. This involves studying their robustness to \(8\) types of camera corruptions, \(8\) types of LiDAR corruptions, and \(13\) types of camera-LiDAR corruption combinations for each configuration.
* We identify effective strategies for enhancing robustness, including innovative approaches that leverage advanced data augmentation and architectural techniques. Our findings reveal strategies that significantly improve performance and robustness, underscoring the importance of tailored solutions to address specific challenges in HD map construction.

Figure 1: Radar charts of state-of-the-art HD map constructors under the Camera and LiDAR sensor corruptions. We report the mAP scores of different map construction methods under each corruption type across severity levels. **Camera Corruptions:**#1 Clean, #2 Frame Lost, #3 Camera Crash, #4 Low-Light, #5 Bright, #6 Color Quant, #7 Snow, #8 Fog, and #9 Motion Blur. **LiDAR Corruptions:**#1 Clean, #2 Wet Ground, #3 Snow, #4 Motion Blur, #5 Incomplete Echo, #6 Fog, #7 Crosstalk, #8 Cross-Sensor, and #9 Beam Missing. The radius of each chart is normalized based on the Clean score. The larger the area coverage, the better the overall robustness.

Related Work

**HD Map Construction.** The construction of HD maps is a critical yet extensively researched area. Based on the input sensor modality, existing literature can be categorized into camera-only [41; 74; 11; 50; 42; 73; 77; 15], LiDAR-only [35; 43], and camera-LiDAR fusion [41; 42; 77] models. _Camera-based methods_[35; 43; 41; 74; 11; 50; 42; 73] have increasingly employed the BEV representation as an ideal feature space for multi-view perception due to its ability to mitigate scale ambiguity and occlusion challenges. Techniques such as LSS , Deformable Attention , and GKT  have been proposed to project perspective view (PV) features into the BEV space by leveraging geometric priors. However, these methods lack explicit depth information. Consequently, they have come to rely on higher resolution images or larger backbones to achieve enhanced accuracy [45; 44; 58; 39; 69; 65]. _LiDAR-based methods_[35; 35; 43; 42; 41; 38] benefit from the accurate 3D geometric information provided by LiDAR inputs but face challenges related to data sparsity and sensing noise.

**Multi-Sensor HD Map Construction.**_Camera-LiDAR fusion-based methods_ can be roughly divided into three categories: point-level fusion [54; 53; 66; 8; 70], feature-level fusion [71; 1; 6; 7; 40], and BEV-level fusion [41; 42; 77; 17]. Recently, camera-LiDAR feature fusion in the unified BEV space has gained attention. BEV-level fusion integrates features from camera and LiDAR sensors within the same BEV space, combining complementary modalities to achieve superior performance over uni-modal approaches. Despite the progress in HD map construction methods, their resilience to real-world challenges such as adverse weather and sensor failures remains unclear, raising safety concerns [28; 30]. In this work, we make the first attempt to explore the robustness of existing HD map construction methods under sensor corruptions that occur in real-world environments.

**Robustness against Sensor Corruptions.** Assessing the robustness of driving perception models under sensor corruptions has emerged as a crucial research area [16; 12; 78; 28; 62; 30]. Recently, the corruption robustness of BEV perception tasks has been extensively studied. RoboDepth [28; 51] establishes a robustness benchmark for monocular depth estimation under corruptions, while RoboBEV [62; 63] introduces a comprehensive benchmark for evaluating the robustness of four BEV perception tasks, including 3D object detection [39; 46], semantic segmentation [75; 76], depth estimation , and semantic occupancy prediction [60; 24]. However, RoboBEV's analyses of multi-modal fusion model robustness only consider complete sensor failure, overlooking other common sensor corruptions and their combinations. Dong _et al._ systematically design \(27\) types of common corruptions for 3D object detection in both LiDAR and camera sensors. Meanwhile, Robo3D  benchmarks the robustness of 3D detectors and segmentors against LiDAR corruptions.

**Comparison with Existing Works.** This work differs from prior literature in _three_ key aspects. Firstly, we focus on the vectorized HD map construction task, distinct from other BEV perception tasks [12; 28; 62]. Secondly, we introduce new sensor corruption types that closely mimic real-world scenarios. Specifically, we design \(13\) new multi-sensor corruption types to benchmark camera-LiDAR fusion models comprehensively, surpassing the scope of complete sensor failure analysis in RoboBEV . Thirdly, we explore distinct data augmentation techniques that are applied to LiDAR point clouds and RGB images to analyze their impact on enhancing corruption robustness. To the best of our knowledge, MapBench serves as the first study to comprehensively benchmark and evaluate the reliability of HD map construction methods again single- and multi-modal sensor corruptions.

## 3MapBench: Benchmarking HD Map Construction Robustness

In this work, we investigate three popular configurations, _i.e._, Camera-only, LiDAR-only, and Camera-LiDAR fusion-based HD map construction tasks, and study their robustness to various sensor corruptions. As illustrated in Fig. 2, the camera/LiDAR corruptions are grouped into exterior environments, interior sensors, and sensor failure types, covering the majority of real-world cases.

Following the protocol established in , we consider _three_ corruption severity levels, _i.e._, Easy, Moderate, and Hard, for each type of corruption. Additionally, regarding multi-sensor corruptions, we use camera/LiDAR sensor failure types to perturb camera and LiDAR sensor inputs separately or concurrently. MapBench is constructed by corrupting the _val_ set of nuScenes . We chose nuScenes since it has been widely utilized among almost all recent HD map construction works.

### Sensor Corruptions

**Camera Sensor Corruptions.** To probe the Camera-only model robustness, we employ \(8\) real-world camera sensor corruptions from , ranging from three perspectives: exterior environments, interior sensors, and sensor failures. Specifically, the exterior environments include various lighting and weather conditions such as Bright, Low-Light, Fog, and Snow. The camera inputs might also be corrupted by interior factors caused by sensors, such as Motion Blur and Color Quantization. Lastly, we consider the sensor failure cases where cameras crash or certain frames are dropped due to physical problems, leading to Camera Crash and Frame Lost, respectively. Due to page limits, the detailed definitions and visualizations of these corruptions are provided in Sec. A in the Appendix.

**LiDAR Sensor Corruptions.** To explore the LiDAR-only model robustness, we resort to \(8\) LiDAR sensor corruptions in , which are scenarios that have a high likelihood of occurring in real-world deployments. These corruptions also range from exterior, interior, and sensor failure cases. The exterior environments encompass Fog, Wet Ground, and Snow, which cause back-scattering, attenuation, and reflection of the LiDAR pulses. Besides, the LiDAR inputs might be corrupted by bumpy surfaces, dust, or insects, which often lead to disturbances and cause Motion Blur and Beam Missing. Lastly, we consider the cases of LiDAR internal sensor failures, such as Crosstalk, possible Incomplete Echo, and Cross-Sensor scenarios. Kindly refer to Sec. A for more details.

**Multi-Sensor Corruptions.** To explore the Camera-LiDAR fusion model robustness, we design \(13\) types of camera-LiDAR corruption combinations that perturb both camera and LiDAR input separately or concurrently, using the aforementioned sensor failure types. These multi-sensor corruptions are grouped into camera-only corruptions, LiDAR-only corruptions, and their combinations, covering the majority of real-world scenarios. Specifically, we design \(3\) camera-only corruptions by utilizing the "clean" LiDAR point data and three camera failure cases such as Unavailable Camera (all pixel values are set to _zero_ for all RGB images), Camera Crash, and Frame Lost. Moreover, we design \(4\) LiDAR-only corruptions by utilizing the "clean" camera data and the corrupted LiDAR data as the input. This includes complete LiDAR failure (since no model can work when all points are absent, we approximate this scenario by only retaining a single point as input), Incomplete Echo, Crosstalk, and Cross-Sensor. Note that our implementations of complete LiDAR failure are close to the real-world situation. Lastly, we design \(6\) camera-LiDAR corruption combinations that perturb both sensor inputs concurrently, using the previously mentioned image/LiDAR sensor failure types. Due to page limits, more detailed definitions of multi-sensor corruption are placed in Sec. B.

### Evaluation Metrics

Inspired by [20; 62; 27], we define two robustness evaluation metrics based on mAP (mean Average Precision), a commonly-used accuracy indicator for vectorized HD map construction.

Figure 2: Definitions of the Camera and LiDAR sensor corruptions in MapBench. Our benchmark encompasses a total of 16 corruption types for HD map construction, which can be categorized into exterior, interior, and sensor failure scenarios. Besides, we define 13 multi-sensor corruptions by combining the camera and LiDAR sensor failure types. Kindly refer to our Appendix for more details.

**Corruption Error (CE).** We define CE as the primary metric in comparing models' robustness. It measures the relative robustness of candidate models compared to a baseline. Given a total of \(N\) distinct corruption types, the CE and mCE (mean Corruption Error) scores are calculated as follows:

\[_{i}=^{3}(1-_{i,l})}{_{l=1}^{3}(1- _{i,l}^{})}\,=_{i=1}^{N}_{i}\,\] (1)

where \(i\) denotes the corruption type and \(l\) is the severity level. \(_{i,l}^{}\) is the baseline's accuracy score.

**Resilience Rate (RR).** We define RR as the relative robustness indicator for measuring how much accuracy a model can retain when evaluated on the corruption sets, which are calculated as follows:

\[_{i}=^{3}_{i,l}}{3^{ }}\,=_{i=1}^{N}_{i}\,\] (2)

where \(^{}\) denotes the mAP score of a candidate model on the "clean" evaluation set.

## 4 Experimental Analysis

### Benchmark Configuration

**Candidate Models.** Our MapBench encompasses a total of \(31\) HD map constructors and their variants, _i.e._, HDMapNet , VectorMapNet , PivotNet , BeMapNet , MapTR , MapTRv2 , StreamMapNet  and HIMap . The code of some other HD map methods [36; 64; 22; 74; 67; 4; 25; 65; 72; 55] are not open source, thus will not be considered in this work.

**Model Configurations.** We report the basic information of different models in Tab. 1, including input modality, BEV encoder, backbone, training epoch, and their performance on the official nuScenes

 
**Method** & **Venue** & **Modal** & **BEV Encoder** & **Backbone** & **Epoch** & \(_{5}\)\(\) & \(_{5}\)\(\) & \(_{5}\)\(\) & \(\)\(\) & \(\)\(\) & \(\)\(\) \\   HDMapNet  & ICAR2-2 & C & NVT & Eff-B0 & 30 & 14.4 & 21.7 & 33.0 & 23.0 & 43.3 & 187.8 \\ VectorMapNet  & ICAR2-23 & C & IPM & F80 & 110 & 36.1 & 47.3 & 39.3 & 40.9 & 40.6 & 148.5 \\ PivotNet  & ICCV-23 & C & PersFormer & R50 & 30 & 53.8 & 58.5 & 59.6 & 57.4 & 45.2 & 96.3 \\ BeMapNet  & CVPR-23 & C & IPM-PE & R50 & 30 & 57.7 & 62.3 & 59.4 & 59.8 & 50.3 & 78.5 \\ MapTRv2  & ICAR2-23 & C & GKT & R50 & 24 & 46.3 & 51.5 & 53.1 & 50.3 & 49.3 & 100.0 \\ MapTRv2  & arXiv2-23 & C & BEVPool & R50 & 24 & 59.8 & 62.4 & 62.4 & 61.5 & 51.4 & 72.6 \\ StreamMapNet  & WACV-24 & C & BEVFormer & R50 & 30 & 61.7 & 66.3 & 62.1 & 63.4 & 54.4 & 64.8 \\ HIMap  & CVPR-24 & C & BEVFormer & R50 & 24 & 62.2 & 66.5 & 67.9 & 65.5 & 56.6 & 56.9 \\  VectorMapNet  & ICAR2-23 & L & - & PP & 110 & 25.7 & 37.6 & 38.6 & 34.0 & 63.4 & 94.9 \\ MapTRv2  & ICAR2-23 & L & - & SEC & 24 & 48.5 & 53.7 & 64.7 & 53.6 & 55.1 & 100.0 \\ MapTRv2  & arXiv2-23 & L & - & SEC & 24 & 56.6 & 58.1 & 69.8 & 61.5 & 57.2 & 74.6 \\ HIMap  & CVPR-24 & L & - & SEC & 24 & 54.8 & 64.7 & 73.5 & 64.3 & 59.2 & 73.1 \\  MapTRv  & ICAR2-23 & C \& L & GKT & R50 \& SEC & 24 & 55.9 & 62.3 & 69.3 & 62.5 & 57.1 & 100.0 \\ HIMap  & CVPR-24 & C \& L & BEVFormer & R50 \& SEC & 24 & 71.0 & **72.4** & **79.4** & **74.3** & 41.7 & 110.6 \\  

Table 1: **Benchmarking HD map constructors. Methods are split into groups based on \({}^{1}\)input modality, \({}^{2}\)BEV encoder, \({}^{3}\)backbone, and \({}^{4}\)training epochs. “L” and “C” represent LiDAR and camera, respectively. “Effi-B0”, “R50”, “PP”, and “SEC” are short for EfficientNet-B0 , ResNet50 , PointPillars , and SECOND . AP denotes performance on the clean nuScenes _val_ set. The subscripts \(b\), \(p\), and \(d\). are short for the _boundary_, _pedestrian crossing_, and _divider_, respectively.

Figure 3: The correlations of accuracy (mAP) and robustness (mCE / mRR) for the Camera (a) and (b) and LiDAR (c) and (d) models. The size of the circle represents the number of model parameters.

[MISSING_PAGE_FAIL:6]

2) Most models exhibit negligible performance drops under Incomplete Echo. This corruption type primarily affects data from vehicles or objects with dark colors , whereas the HD map construction task concerns more on static map elements. Besides, although VectorMapNet  achieves the best mRR metric, it is not less accurate in terms of mAP compared to HIMap .

### Camera-LiDAR Fusion Benchmarking Results

To systematically evaluate the reliability of camera-LiDAR fusion-based methods, we design \(13\) types of multi-sensor corruptions that perturb camera and LiDAR inputs separately or concurrently. The results are presented in Fig. 4. Our findings indicate that the camera-LiDAR fusion model exhibits varying degrees of performance declines on different corruption combinations. The experimental results reveal several interesting findings, and we provide detailed analyses as follows:

1) In scenarios where Camera data is missing, the mAP of MapTR  and HIMap  dropped by \(40.0\%\) and \(68.9\%\), respectively, posing a significant threat to safe perceptions. Besides, Frame Lost causes a worse effect than Camera Crash in the performance of sensor fusion-based methods. These observations verify that camera sensor failures significantly threaten HD map fusion models.

2) In scenarios where LiDAR data is missing, the mAP of MapTR  and HIMap  dropped by \(42.1\%\) and \(41.5\%\), respectively, showing the indispensability of the LiDAR sensor. Moreover, the LiDAR Crosstalk and Cross-Sensor corruptions affect the performance of camera-LiDAR fusion the most. In contrast, the LiDAR Incomplete Echo corruption does not show a substantial impact on model performance, which is consistent with the observation under LiDAR-only configurations.

3) The results of Camera-LiDAR combined corruptions lead to worse performance than its both single-modality counterparts, highlighting the significant threats posed by both camera and LiDAR sensor failures to HD map construction tasks. Moreover, regardless of the type of LiDAR corruption combined, Frame Lost has a more significant impact on the fusion model performance than Camera Crash, underscoring the importance of multi-view inputs from the camera sensor. Among the three types of LiDAR corruptions, Cross-Sensor corruption affects the fusion model performance the most. This pattern remains consistent even when combined with various types of camera corruptions, illustrating the substantial threat posed by cross-configuration or cross-device LiDAR data input. We provide some qualitative examples of HD map construction under various camera-LiDAR corruption combinations in Fig. 5, which shows the performance decline under various corruptions.

Figure 5: Qualitative assessment of camera-LiDAR fusion-based HD map construction under the Camera and LiDAR combined sensor corruptions. Kindly refer to Sec. F for additional examples.

It is worth noting that although the performance of HIMap  is better than that of MapTR  under "clean" conditions, its robustness under corruption is relatively poorer. These observations necessitate further research focused on enhancing the robustness of camera-LiDAR fusion methods, especially when one sensory modality is absent or both the camera and LiDAR are corrupted.

## 5 Observation & Discussion

In this section, we analyze and discuss the impact of different model configurations and techniques that affect the robustness of HD map constructors, including different backbone networks, BEV encoders, temporal information, training epochs, data augmentation enhancement, and so on.

**Backbones.** We first comprehensively investigate the impact of backbone networks, with results presented in Tab. 4. Specifically, we use three different backbones in PivotNet  and BeMapNet , respectively. The results demonstrate that Swin Transformer  significantly retains model robustness. As an example, compared with ResNet-50 , the Swin Transformer  backbone improves the mCE of PivotNet  and BeMapNet  with \(22.2\%\) and \(24.1\%\) absolute gains, respectively. The results demonstrate that larger pretrained models tend to help enhance the robustness of feature extraction under out-of-domain data, which is in line with the observation drawn in [19; 2; 13; 10; 57].

**Different BEV Encoders.** We study several popular 2D-to-BEV transformation methods and show the results in Tab. 2. Specifically, we adopt the BEVFormer , BEVPool , and GKT  for the camera-only MapTR  model. The results demonstrate that MapTR  is compatible with various 2D-to-BEV methods and achieves stable robustness performance. Moreover, the mRR results of BEVPool  are inferior to those of BEVFormer  and GKT , verifying the effectiveness of transformer-based BEV encoders on improving HD map model robustness. GKT  achieves the best mCE, which is possibly due to the integration of both geometry and view transformer methods.

**Temporal Information.** We investigate the impact of utilizing temporal cues on the robustness of HD map models and show the results in Tab. 3. We examine two variants of StreamMapNet : one with and one without the temporal fusion module. The results demonstrate that the temporal fusion module can significantly enhance the robustness. The mAP results here differ from those in Tab. 1 since StreamMapNet  was retrained following the default settings of a new train/validation split, whereas the results in Tab. 1 were obtained using the old train/validation split. It can be observed that the model with temporal cues achieves \(8.4\%\) and \(14.1\%\) absolute gains on the mRR and mCE metrics,

  
**Method** & **Back** & **AP\({}_{}\)** & **AP\({}_{}\)** & **AP\({}_{}\)** & **mAP** & **mRR** & **mCE** \\   PivotNet & R50 & 53.8 & 58.8 & 59.6 & 57.4 & 45.2 & 100.0 \\ PivotNet & HIR-20 & 53.9 & 59.7 & 61.0 & 58.2 & 49.9 & 87.4 \\ PivotNet & Swinat & **58.7** & **63.8** & **64.9** & **62.5** & **50.8** & **77.8** \\ BeMapNet & R50 & 57.7 & 62.3 & 59.4 & 59.8 & 50.3 & 100.0 \\ BeMapNet & GIR-20 & 56.0 & 62.2 & 50.0 & 50.1 & 53.9 & 94.0 \\ BeMapNet & SwinT & **61.3** & **64.4** & **61.6** & **62.5** & **57.9** & **75.9** \\   

Table 4: Ablation on the use of backbone nets.

Figure 6: The mAP metrics of state-of-the-art HD map constructors under each of the three severity levels (Esay, Moderate, and Hard) in different Camera and LiDAR sensor corruption scenarios.

  
**Method** & **Epoch** & **AP\({}_{}\)** & **AP\({}_{}\)** & **AP\({}_{}\)** & **mAP** & **mRR** & **mCE** \\  MapTR \(\) & 24 & 46.3 & 51.5 & 53.1 & 50.3 & 49.3 & 100.0 \\  MapTR \(\) & 110 & **56.2** & **59.8** & **60.1** & **58.7** & **49.3** & **80.9** \\ PivotNet \(\) & 30 & 58.7 & 63.8 & 64.9 & 62.5 & 50.8 & 100.0 \\ PivotNet \(\) & 110 & **62.6** & **68.0** & **69.7** & **66.8** & **49.9** & **90.2** \\ BeMapNet \(\) & 30 & 61.3 & 64.4 & 61.6 & 62.5 & 57.9 & 100.0 \\ BeMapNet \(\) & 110 & **64.6** & **68.9** & **67.5** & **67.0** & **56.7** & **89.2** \\   

Table 5: Ablation on different training epochs.

respectively. This verifies that temporal fusion can provide additional complementary information under sensor corruptions, thereby enhancing robustness against different sensor corruptions.

**Training Epochs.** In this setting, we study three HD map models (MapTR , PivotNet , and BeMapNet ) trained with different numbers of epochs, with results shown in Tab. 5. It can be observed that training for more epochs significantly improves both performance on the "clean" set and robustness to corruptions. For example, utilizing a longer training schedule enhances robustness in mCE metrics: MapTR  (+\(19.1\%\)), PivotNet  (+\(9.8\%\)), and BeMapNet  (+\(10.8\%\)). Notably, the performance of these models on the "clean" set also improves as the training schedule lengthens, suggesting that extended training allows the model to better learn the inherent patterns from the dataset, thereby achieving better generalization performance on corrupted data .

**Data Augmentations to Boost Corruption Robustness.** We investigate the effect of various data augmentation techniques on the robustness of HD map models. As multi-modal data augmentation remains an open issue, in this work, we focus on investigating the effects of image and LiDAR data augmentation techniques. Specifically, we study three distinct image data augmentation methods, _i.e._ Rotate , Flip , and PhotoMetric , and three distinct LiDAR-based data augmentation methods, _i.e._ Dropout , RTS-LiDAR (Rotate-Translate-Scale for LiDAR) , and PolarMix .

1) For Camera-based data augmentations, we choose MapTR-R50  as the baseline and show results in Tab. 6. It can be observed that image augmentation methods moderately improve model performance on the "clean" set. However, they do not consistently enhance model robustness. For example, PhotoMetric  improves the robustness metrics, mRR and mCE, by \(8.2\%\) and \(15.5\%\), respectively, whereas Rotate  and Flip  weaken the robustness. This discrepancy likely arises from the fact that PhotoMetric  functions similarly to corruption augmentation for certain types, such as Bright and Low-Light, differing from other augmentation methods.

2) For LiDAR-based data augmentations, we choose the MapTR-LiDAR  model due to its superior robustness among all LiDAR-only models. The results of different LiDAR augmentations are presented in Tab. 7. We observe that all LiDAR augmentations significantly improve the model performance on the "clean" set. In particular, PolarMix  achieves a \(3.0\%\) absolute performance gain. Moreover, all LiDAR augmentation techniques are effective in enhancing the model robustness, reducing the absolute mCE values by \(1.1\%\) for Dropout , \(6.0\%\) for RTS-LiDAR , and \(6.5\%\) for PolarMix , respectively. These results demonstrate the effectiveness of LiDAR augmentation methods in improving the corruption robustness of LiDAR-only HD map construction methods.

## 6 Conclusion

In this work, we conducted the first study of benchmarking and analyzing the reliability of HD map construction methods under sensor corruptions that occur in real-world driving environments. Our results reveal key factors that coped closely with the out-of-domain robustness, highlighting crucial aspects in retaining satisfactory accuracy. We hope our comprehensive benchmarks, in-depth analyses, and insightful findings could help better understand the robustness of HD map construction tasks and offer useful insights into designing more reliable HD map constructors in future studies.

**Potential Limitation.** While our benchmark encompasses an abundant number of sensor corruption types, it is hard to cover the entirety of out-of-distribution contexts in real-world applications due to their unpredictable complexity. Furthermore, our experiments confirm the efficacy of standard data augmentation techniques in enhancing robustness, offering promising results. Nonetheless, further explorations into more advanced methods and network designs are warranted for future research.

   & \(}_{p}\) & \(}_{d}\) & \(}_{b}\) & \(}\) & \(}\) & \(}\) \\   None & \(45.6\) & \(50.1\) & \(52.3\) & \(49.3\) & \(41.1\) & \(100.0\) \\  Rotate  & \(44.6\) & \(50.5\) & \(54.0\) & \(49.7\) & \(38.1\) & \(105.1\) \\ Flip  & \(44.7\) & \(53.0\) & \(53.4\) & \(50.4\) & \(38.7\) & \(102.5\) \\ PhotoMetric  & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 6: Efficacy of Camera-based data augmentation techniques on HD map model robustness.

   & \(}_{p}\) & \(}_{d}\) & \(}_{b}\) & \(}\) & \(}\) & \(}\) \\   None & \(26.6\) & \(31.7\) & \(41.8\) & \(33.4\) & \(55.1\) & \(100.0\) \\  Dropout [\(\)] & \(28.4\) & \(31.0\) & \(42.5\) & \(33.9\) & \(56.9\) & \(98.9\) \\ RTS-LiDAR [\(\)] & \(28.3\) & \(32.7\) & \(44.1\) & \(35.0\) & \(57.0\) & \(94.0\) \\
**PolarMix [\(\)]** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 7: Efficacy of LiDAR-based data augmentation techniques on HD map model robustness.