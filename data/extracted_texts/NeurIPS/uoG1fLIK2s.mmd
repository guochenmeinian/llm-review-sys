# Sample-efficient Multi-objective Molecular Optimization with GFlowNets

Yiheng Zhu\({}^{1}\), Jialu Wu\({}^{2}\), Chaowen Hu\({}^{3}\), Jiahuan Yan\({}^{1}\),

Chang-Yu Hsieh\({}^{2}\), Tingjun Hou\({}^{2}\), Jian Wu\({}^{4,5,6}\)

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)College of Pharmaceutical Sciences, Zhejiang University

\({}^{3}\)Polytechnic Institute, Zhejiang University

\({}^{4}\)Second Affiliated Hospital School of Medicine, Zhejiang University

\({}^{5}\)School of Public Health, Zhejiang University

\({}^{6}\)Institute of Wenzhou, Zhejiang University

{zhuyiheng2020, jialuwu, ChaowenHu, jyansir, kimhsheh, tingjunhou, wujian2000}@zju.edu.cn

Equal contribution.Corresponding authors.

###### Abstract

Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as a black-box optimization problem over the _discrete_ chemical space. In practice, multiple conflicting objectives and costly evaluations (e.g., wet-lab experiments) make the _diversity_ of candidates paramount. Computational methods have achieved initial success but still struggle with considering diversity in both objective and search space. To fill this gap, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. We further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed up learning for HN-GFN. We empirically illustrate that HN-GFN has adequate capacity to generalize over preferences. Moreover, experiments in various real-world MOBO settings demonstrate that our framework predominantly outperforms existing methods in terms of candidate quality and sample efficiency. The code is available at https://github.com/violet-sto/HN-GFN.

## 1 Introduction

Designing novel molecular structures with desired properties, also referred to as molecular optimization, is a crucial task with great application potential in scientific fields ranging from drug discovery to material design. Molecular optimization can be naturally formulated as a black-box optimization problem over the _discrete_ chemical space, which is combinatorially large . Recent years have witnessed the trend to leverage computational methods, such as deep generative models  and combinatorial optimization algorithms , to facilitate optimization. However, the applicability of most prior approaches in real-world scenarios is hindered by two practical constraints: (i) Chemists commonly seek to optimize multiple properties of interest simultaneously . For example, in addition to effectively inhibiting a disease-associated target, an ideal drug is desired to be easily synthesizable and non-toxic. Unfortunately, as objectives often conflict, in most cases, there is no singleoptimal solution, but rather a set of Pareto optimal solutions defined with various trade-offs [18; 45]. (ii) Realistic oracles (e.g., wet-lab experiments and high-fidelity simulations) require substantial costs to synthesize and evaluate molecules . Hence, the number of oracle evaluations is notoriously limited. In such scenarios, the diversity of candidates is a critical consideration.

Bayesian optimization (BO) [34; 54] provides a sample-efficient framework for globally optimizing expensive black-box functions. The core idea is to construct a cheap-to-evaluate _surrogate model_ approximating the true objective function (also known as the _oracle_) on the observed dataset and optimize an _acquisition function_ (built upon the surrogate model) to obtain informative candidates with high utility for the next round of evaluations. Due to the common large-batch and low-round settings in biochemical experiments , batch BO is prioritized to shorten the entire cycle of optimization . MOBO also received broad attention and achieved promising performance in continuous problems by effectively optimizing differentiable acquisition functions [12; 13]. Nevertheless, it is less prominent in discrete problems, where no gradients can be leveraged to navigate the search space.

Although existing discrete molecular optimization methods can be adopted as the _acquisition function optimizer_ to alleviate this issue, they can hardly consider diversity in both search and objective space: 1) many neglect the diversity of proposed candidates in search space [30; 33]; 2) many multi-objective methods simply rely on a predefined scalarization function (e.g., mean) to turn the multi-objective optimization (MOO) problem into a single-objective one [61; 20]. As the surrogate model cannot exactly reproduce the oracle's full behaviors and the optimal trade-off is unclear before optimization (even with domain knowledge), it is desired to not only propose candidates that bring additional information about the search space but also explore more potential trade-offs of interest. To achieve this goal, we explore how to extend the recently proposed generative flow networks (GFlowNets) [6; 7], a class of generative models that aim to learn a stochastic policy for sequentially constructing objects with a probability proportional to a reward function, to facilitate multi-objective molecular optimization. Compared with traditional combinatorial optimization methods, GFlowNets possess merit in generating diverse and high-reward objects, which has been verified in single-objective problems [6; 28].

In this work, we present a MOBO algorithm based on GFlowNets for sample-efficient multi-objective molecular optimization. We propose a hypernetwork-based GFlowNet (HN-GFN) as the acquisition function optimizer within MOBO to sample a diverse batch of candidates from an approximate Pareto front. Instead of defining a fixed reward function as usual in past work , we train a unified GFlowNet on the distribution of reward functions (_random scalarizations_ parameterized by _preference vectors_) and control the policy using a single preference-conditioned hypernetwork. While sampling candidates, HN-GFN explores various trade-offs between competing objectives flexibly by varying the input preference vector. Inspired by hindsight experience replay , we further introduce a hindsight-like off-policy strategy to share high-performing molecules among different preferences

Figure 1: MOBO loop for molecular optimization using a surrogate model \(\) for uncertainty estimation and HN-GFN for acquisition function optimization. In each round, the policy \(_{}\) is trained with reward function \(R_{}\), where \(\) is sampled from \(()\) per iteration. A new batch of candidates is sampled from the approximate Pareto front according to target preference vectors \(_{}\).

and speed up learning for HN-GFN. As detailed in our reported experiments, we first empirically verify that HN-GFN is capable of generalizing over preference vectors, then apply the proposed framework to real-world scenarios and demonstrate its effectiveness and efficiency over existing methods. Our key contributions are summarized below:

* We introduce a GFlowNet-based MOBO algorithm to facilitate real-world molecular optimization. We propose HN-GFN, a conditional variant of GFlowNet that can efficiently sample candidates from an approximate Pareto front.
* We introduce a hindsight-like off-policy strategy to speed up learning in HN-GFN.
* Experiments verify that our MOBO algorithm based on HN-GFN can find a high-quality Pareto front more efficiently compared to state-of-the-art baselines.

## 2 Related Work

Molecular optimization.Molecular optimization has been approached with a wide variety of computational methods, which can be mainly grouped into three categories: 1) **Latent space optimization (LSO)** methods [24; 31; 58; 66] perform the optimization over the low-dimensional continuous latent space learned by generative models such as variational autoencoders (VAEs) . These methods require the latent representations to be discriminative, but the training of the generative model is decoupled from the optimization objectives, imposing challenges for optimization . Instead of navigating the latent space, combinatorial optimization methods search for the desired molecular structures directly in the explicit discrete space with 2) **evolutionary algorithms** and 3) **reinforcement learning (RL)** guiding the search. However, most prior methods only focus on a single property, from non-biological properties such as drug-likeliness (QED)  and synthetic accessibility (SA) , to biological properties that measure the binding energy to a protein target . Multi-objective molecular optimization has recently received wide attention [33; 61; 20]. For example, MARS  employs Markov chain Monte Carlo (MCMC) sampling to find novel molecules satisfying several properties. However, most approaches require a notoriously large number of oracle calls to evaluate molecules on the fly [33; 61]. In contrast, we tackle this problem in a sample-efficient manner.

GFlowNet.GFlowNets [6; 7] aim to sample composite objects proportionally to a reward function, instead of maximizing it as usual in RL . GFlowNets are related to the MCMC methods due to the same objective, while amortizing the high cost of sampling (mixing between modes) over training a generative model. GFlowNets have made impressive progress in various applications, such as biological sequence design , discrete probabilistic modeling , and Bayesian structure learning . While the concept of conditional GFlowNet was originally discussed in Bengio et al. , we are the first to study and instantiate this concept for MOO, in parallel with Jain et al. . Compared with them, we delicately design the conditioning mechanism and propose a hindsight-like off-policy strategy that is rarely studied for MOO (in both the RL and GFlowNet literature). Appendix D provides a detailed comparison.

Bayesian optimization for discrete spaces.The application of BO in discrete domains has proliferated in recent years. It is much more challenging to construct surrogate models and optimize acquisition functions in discrete spaces, compared to continuous spaces. One common approach is to convert discrete space into continuous space with generative models [24; 16; 44]. Besides, one can directly define a Gaussian Process (GP) with discrete kernels [46; 53] and solve the acquisition function optimization problem with evolutionary algorithms [35; 57].

Multi-objective Bayesian optimization.BO has been widely used in MOO problems for efficiently optimizing multiple competing expensive black-box functions. Most popular approaches are based on hypervolume improvement , random scalarizations [37; 49], and entropy search [27; 5]. While there have been several approaches that take into account parallel evaluations [11; 38] and diversity , they are limited to continuous domains.

Background

### Problem formulation

We address the problem of searching over a discrete chemical space \(\) to find molecular graphs \(x\) that maximize a _vector-valued objective_\(f(x)=f_{1}(x),f_{2}(x),,f_{M}(x):^{M}\), where \(f_{m}\) is a black-box function (also known as the oracle) evaluating a certain property of molecules. Practically, realistic oracles are extremely expensive to evaluate with either high-fidelity simulations or wet-lab experiments. We thus propose to perform optimization in as few oracle evaluations as possible, since the sample efficiency is paramount in such a scenario.

There is typically no single optimal solution to the MOO problem, as different objectives may contradict each other. Consequently, the goal is to recover the _Pareto front_ - the set of _Pareto optimal_ solutions which cannot be improved in any one objective without deteriorating another [18; 45]. In the context of maximization, a solution \(f(x)\) is said to _Pareto dominates_ another solution \(f(x^{})\) iff \(f_{m}(x) f_{m}(x^{})\  m=1,,M\) and \( m^{}\) such that \(f_{m^{}}(x)>f_{m^{}}(x^{})\), and we denote \(f(x) f(x^{})\). A solution \(f(x^{*})\) is _Pareto optimal_ if not Pareto dominated by any solution. The Pareto front can be written as \(^{*}=\{f(x^{*}):\{f(x):f(x) f(x^{*})\ \}=\}\).

The quality of a finite approximate Pareto front \(\) is commonly measured by _hypervolume_ - the M-dim Lebesgue measure \(_{M}\) of the space dominated by \(\) and bounded from below by a given reference point \(^{M}\): \(HV(,)=_{M}(_{i=1}^{||}[,y_{i}])\), where \([,y_{i}]\) denotes the hyper-rectangle bounded by \(\) and \(y_{i}=f(x_{i})\).

### Batch Bayesian optimization

Bayesian optimization (BO)  provides a model-based iterative framework for sample-efficient black-box optimization. Given an observed dataset \(\), BO relies on a Bayesian _surrogate model_\(\) to estimate a posterior distribution over the true oracle evaluations. Equipped with the surrogate model, an _acquisition function_\(a:\) is induced to assign the utility values to candidate objects for deciding which to be evaluated next on the oracle. Compared with the costly oracle, the cheap-to-evaluate acquisition function can be efficiently optimized. We consider the scenario where the oracle is given an evaluation budget of \(N\) rounds with fixed batches of size \(b\).

To be precise, we have access to a random initial dataset \(_{0}=\{(x_{i}^{0},y_{i}^{0})\}_{i=1}^{n}\), where \(y_{i}^{0}=f(x_{i}^{0})\) is a true oracle evaluation. In each round \(i\{1,,N\}\), the acquisition function is maximized to yield a batch of candidates \(_{i}=\{x_{j}^{i}\}_{j=1}^{b}\) to be evaluated in parallel on the oracle \(y_{j}^{i}=f(x_{j}^{i})\). The observed dataset \(_{i-1}\) is then augmented for the next round: \(_{i}=_{i-1}\{(x_{j}^{i},y_{j}^{i})\}_{j=1}^{b}\).

## 4 Method

In this section, we present the proposed MOBO algorithm based on hypernetwork-based GFlowNet (HN-GFN), shown in Figure 1. Due to the space limitation, we present the detailed algorithm in Appendix A. Our key idea is to extend GFlowNets as the acquisition function optimizer for MOBO, with the objective of sampling a diverse batch of candidates from the approximate Pareto front. To begin, we introduce GFlowNets in the context of molecule design, then describe how GFlowNet can be biased by a preference-conditioned hypernetwork to sample molecules according to various trade-offs between objectives. Next, we propose a hindsight-like off-policy strategy to speed up learning in HN-GFN. Finally, we introduce the surrogate model and acquisition function.

### Preliminaries

GFlowNets  seek to learn a stochastic policy \(\) for sequentially constructing discrete objects \(x\) with a probability \((x) R(x)\), where \(\) is a compositional space and \(R:_{ 0}\) is a non-negative reward function. The generation process of object \(x\) can be represented by a sequence of discrete _actions_\(a\) that incrementally modify a partially constructed object, which is denoted as _state_\(s\). Let the generation process begin at a special initial state \(s_{0}\) and terminate with a special action indicating that the object is complete (\(s=x\)), the construction of an object \(x\) can be defined as a complete trajectory \(=(s_{0} s_{1} s_{n}=x)\).

Following fragment-based molecule design [6; 61], the molecular graphs are generated by sequentially attaching a fragment, which is chosen from a predefined vocabulary of building blocks, to an atom of the partially constructed molecules. The maximum trajectory length is 8, with the number of actions varying between 100 and 2000 depending on the state, making \([]\) up to \(10^{16}\). There are multiple action sequences leading to the same state, and no fragment removal actions, the space of possible action sequences can thus be denoted by a directed acyclic graph (DAG) \(=(,)\), where the edges in \(\) are transitions \(s s^{}\) from one state to another. To learn the aforementioned desired policy, Bengio et al.  propose to see the DAG structure as a _flow network_.

Markovian flows.Bengio et al.  first define a _trajectory flow_\(F:_{ 0}\) on the set of all complete trajectories \(\) to measure the unnormalized density. The _edge flow_ and _state flow_ can then be defined as \(F(s s^{})=_{s s^{}}F()\) and \(F(s)=_{s}F()\), respectively. The trajectory flow \(F\) determines a probability measure \(P()=F()}\). If flow \(F\) is _Markovian_, the forward transition probabilities \(P_{F}\) can be computed as \(P_{F}(s^{}|s)=)}{F(s)}\).

Flow matching.A flow is _consistent_ if the following _flow consistency equation_ is satisfied \( s\):

\[F(s)=_{s^{} Pa_{}(s)}F(s^{} s)=R(s)+ _{s^{}:s Pa_{}(s^{})}F(s s ^{})\] (1)

where \(Pa_{}(s)\) is a set of parents of \(s\) in \(\). As proved in Bengio et al. , if the flow consistency equation is satisfied with \(R(s)=0\) for non-terminal state \(s\) and \(F(x)=R(x) 0\) for terminal state \(x\), a policy \(\) defined by the forward transition probability \((s^{}|s)=P_{F}(s^{}|s)\) samples object \(x\) with a probability \((x) R(x)\). GFlowNets propose to approximate the edge flow \(F(s s^{})\) using a neural network \(F_{}(s,s^{})\) with enough capacity, such that the flow consistency equation is respected at convergence. To achieve this, Bengio et al.  define a temporal difference-like  learning objective, called flow-matching (FM):

\[_{}(s,R)=( Pa_{ }(s)}F_{}(s^{},s)}{R(s)+_{s^{}:s Pa_{}(s^{})}F_{}(s,s^{})})^{2}\] (2)

One can use any exploratory policy \(\) with full support to sample training trajectories and obtain the consistent flow \(F_{}(s,s^{})\) by minimizing the FM objective . Consequently, a policy defined by this approximate flow \(_{}(s^{}|s)=P_{F_{}}(s^{}|s)=(s  s^{})}{F_{}(s)}\) can also sample objects \(x\) with a probability \(_{}\)(x) proportionally to reward \(R(x)\). Practically, the training trajectories are sampled from a mixture between the current policy \(P_{F_{}}\) and a uniform distribution over allowed actions . We adopt the FM objective in this work because the alternative trajectory balance  was also examined but gave a worse performance in early experiments. Note that more advanced objectives such as subtrajectory balance  can be employed in future work.

### Hypernetwork-based GFlowNets

Our proposed HN-GFN aims at sampling a diverse batch of candidates from the approximate Pareto front with a unified model. A common approach to MOO is to decompose it into a set of scalar optimization problems with scalarization functions and apply standard single-objective optimization methods to gradually approximate the Pareto front [37; 64]. Here we consider the weighted sum (WS): \(s_{}(x)=_{i}_{i}f^{i}(x)\) and Tchebycheff : \(s_{}(x)=_{i}_{i}f^{i}(x)\), where \(=(_{i},,_{M})\) is a preference vector that defines the trade-off between the competing properties.

To support parallel evaluations, one can simultaneously obtain candidates according to different scalarizations. Practically, this approach hardly scales efficiently with the number of objectives for discrete problems. Taking GFlowNet as an example, we need to train multiple GFlowNets separately for each choice of the reward function \(R_{}(x)=s_{}(x)\) to cover the objective space:

\[_{}^{*}=*{arg\,min}_{}_{s }_{}(s,R_{})\] (3)

Our key motivation is to design a unified GFlowNet to sample candidates according to different reward functions, even ones not seen during training. Instead of defining the reward function with a fixed preference vector \(\), we propose to train a preference-conditioned GFlowNet on the distribution of reward functions \(R_{}\), where \(\) is sampled from a simplex \(S_{M}=\{^{m}|_{i}_{i}=1,_{i} 0\}\):

\[^{*}=*{arg\,min}_{}_{ S_{M}} _{s}_{}(s,R_{})\] (4)

Remarks.Assuming an infinite model capacity, it is easy to prove that the proposed optimization scheme (Equation (4)) is as powerful as the original one (Equation (3)), since the solutions to both loss functions coincide . Nevertheless, the assumption of infinite capacity is extremely strict and hardly holds, so how to design the conditioning mechanism in practice becomes crucial.

#### 4.2.1 Hypernetwork-based conditioning mechanism

We propose to condition the GFlowNets on the preference vectors via hypernetworks . Hypernetworks are networks that generate the weights of a target network based on inputs. In vanilla GFlowNets, the flow predictor \(F_{}\) is parameterized with the message passing neural network (MPNN)  over the graph of molecular fragments, with two prediction heads approximating \(F(s,s^{})\) and \(F(s)\) based on the node and graph representations, respectively. These two heads are parameterized with multi-layer perceptrons (MLPs). For brevity, we write \(=(_{},_{})\).

One can view the training of HN-GFN as learning an agent to carry out multiple policies that correspond to different goals (reward functions \(R_{}\)) defined in the same environment (state space \(\) and action space \(\)). We thus propose only to condition the weights of prediction heads \(_{}\) with hypernetworks, while sharing the weights of MPNN \(_{}\), leading to more generalizable state representations. More precisely, the hypernetwork \(h(;)\) takes the preference vector \(\) as inputs and outputs the weights \(_{}=h(;)\) of prediction heads. The parameters \(\) of the hypernetwork are optimized like normal parameters. Following Navon et al. , we parametrize \(h\) using an MLP with multiple heads, each generating weights for different layers of the target network.

#### 4.2.2 As the acquisition function optimizer

Training.At each iteration, we first sample a preference vector \(\) from a Dirichlet distribution \(()\). Then the HN-GFN is trained with the reward function \(R_{}(x)=a((s_{}(x)),(s_{}(x));)\), where \(\) and \(\) are the posterior mean and standard deviation estimated by \(\). In principle, we retrain HN-GFN after every round. We also tried only initializing the parameters of the hypernetwork and found it brings similar performance and is more efficient.

Sampling.At each round \(i\), we use the trained HN-GFN to sample a diverse batch of \(b\) candidates. Let \(^{i}\) be the set of \(k\) target preference vectors \(^{i}_{}\). We sample \(\) molecules for each \(^{i}_{}^{i}\) and evaluate them on the oracle in parallel. We simply construct \(^{i}\) by sampling from \(()\), but it is worth noting that this prior distribution can also be defined adaptively based on the trade-off of interest. As the number of objectives increases, we choose a larger \(k\) to cover the objective space.

### Hindsight-like off-policy strategy

Resorting to the conditioning mechanism, HN-GFN can learn a family of policies to achieve various goals, i.e., one can treat sampling high-reward molecules for a particular preference vector as a separate goal. As verified empirically in Jain et al. , since the FM objective is _off-policy_ and _offline_, we can use offline trajectories to train the target policy for better exploration, so long as the assumption of full support holds. Our key insight is that each policy can learn from the valuable experience (high-reward molecules) of other similar policies.

Inspired by hindsight experience replay  in RL, we propose to share high-performing molecules among policies by re-examining them with different preference vectors. As there are infinite possible preference vectors, we focus on target preference vectors \(^{i}\), which are based on to sample candidates, and build a replay buffer for each \(^{i}_{}^{i}\). After sampling some trajectories during training, we store in the replay buffers the complete objects \(x\) with the corresponding reward \(R_{^{i}_{}}(x)\).

To be specific, at each iteration, we first sample a preference vector from a mixture between \(()\) and a uniform distribution over \(^{i}\): \((1-)()+\). If \(^{i}\) is chosen, we construct half of the training batch with offline trajectories from the corresponding replay buffer of moleculesencountered with the highest rewards. Otherwise, we incorporate offline trajectories from the current observed dataset \(_{i}\) instead to ensure that HN-GFN samples correctly in the vicinity of the observed Pareto set. Our strategy allows for flexible trade-offs between generalization and specialization. As we vary \(\) from 0 to 1, the training distribution of preference vectors moves from \(()\) to \(^{i}\). Exclusively training the HN-GFN with the finite target preference vectors \(^{i}\) can lead to poor generalization. In practice, although we only sample candidates based on \(^{i}\), we argue that it is vital to keep the generalization to leverage the trained HN-GFN to explore various preference vectors adaptively. The detailed algorithm can be found in Appendix A.

### Surrogate model and acquisition function

While GPs are well-established in continuous spaces, they scale poorly with the number of observations and do not perform well in discrete spaces . There has been significant work in efficiently training non-Bayesian neural networks to estimate epistemic uncertainty [21; 39]. We benchmark widely used approaches (in Appendix C.3), and use a flexible and effective one: evidential deep learning . As for the acquisition function, we use Upper Confidence Bound  to incorporate the uncertainty. To be precise, the objectives are modeled with a single multi-task MPNN, and the acquisition function is applied to the scalarization.

## 5 Experiments

We first verify that HN-GFN has adequate capacity to generalize over preference vectors in a single-round synthetic scenario. Next, we evaluate the effectiveness of the proposed MOBO algorithm based on HN-GFN in multi-round practical scenarios, which are more in line with real-world molecular optimization. Besides, we conduct several ablation studies to empirically justify the design choices.

### Single-round synthetic scenario

Here, our goal is to demonstrate that we can leverage the HN-GFN to sample molecules with preference-conditioned property distributions. The HN-GFN is used as a stand-alone optimizer outside of MOBO to directly optimize the scalarizations of oracle scores. As the oracle cannot be called as many times as necessary practically, we refer to this scenario as a synthetic scenario. To better visualize the trend of the property distribution of the sampled molecules as a function of the preference weight, we only consider two objectives: inhibition scores against glycogen synthase kinase-3 beta (GNK3\(\)) and c-Jun N-terminal kinase-3 (JNK3) [40; 33].

Compared methods.We compare HN-GFN against the following methods. Preference-specific GFlowNets (**PS-GFN**) is a set of vanilla unconditional GFlowNets, each trained separately for a specific preference vector. Note that PS-GFN is treated as "_gold standard_" rather than a baseline, as it is trained and evaluated using the same preference vector. **Concat-GFN** and **FiLM-GFN** are two alternative conditional variations of GFlowNet based on the concatenation and FiLM , respectively. **MOEA/D** and **NSGA-III** are two multi-objective evolutionary algorithms that also incorporate preference information. We perform evolutionary algorithms over the 32-dim latent space learned by HierVAE , which gives better optimization performance than JT-VAE .

Metrics.All the above methods are evaluated over the same set of 5 evenly spaced preference vectors. For each GFlowNet-based method, we sample 1000 molecules per preference vector as solutions. We compare the aforementioned methods on the following metrics: **Hypervolume indicator (HV)** measures the volume of the space dominated by the Pareto front of the solutions and bounded from below by the preference point \((0,0)\). **Diversity (Div)** is the average pairwise Tanimoto distance over Morgan fingerprints. **Correlation (Cor)** is the Spearman's rank correlation coefficient between the probability of sampling molecules from an external test set under the GFlowNet and their respective rewards in the logarithmic domain . See more details in Appendix B.1.2. In a nutshell, HV and Div measure the quality of the solutions, while Cor measures how well the trained model is aligned with the given preference vector. Each experiment is repeated with 3 random seeds.

Experimental results.As shown in Table 1, HN-GFN outperforms the baselines and achieves competitive performance to PS-GFN (_gold standard_) on all the metrics. Compared to GFlowNet-basedmethods, evolutionary algorithms (MOEA/D and NSGA-III) fail to find high-scoring molecules, especially the MOEA/D. HN-GFN outperforms Concat-GFN and FiLM-GFN, and is the only conditional variant that can match the performance of PS-GFN, implying the superiority of the well-designed hypernetwork-based conditioning mechanism. In Figure 2 (left), we visualize the empirical property (JNK3) distribution of the molecules sampled by HN-GFN and PS-GFN conditioned on the set of evaluation preference vectors. We observe that the distributions are similar and the trends are consistent: the larger the preference weight, the higher the average score. The comparable performance and consistent sampling distributions illustrate that HN-GFN has adequate capacity to generalize over preference vectors. Since the runtime and storage space of PS-GFN scale linearly with the number of preference vectors, our unified HN-GFN provides a significantly efficient way to explore various trade-offs between objectives.

### Multi-objective Bayesian optimization

Next, we evaluate the effectiveness of HN-GFN as an acquisition function optimizer within MOBO in practical scenarios. We consider the following objective combinations of varying sizes:

* GNK3\(\)+JNK3: Jointly inhibiting Alzheimer-related targets GNK3\(\) and JNK3.
* GNK3\(\)+JNK3+QED+SA: Jointly inhibiting GNK3\(\) and JNK3 while being drug-like and easy-to-synthesize.

We rescale the SA score (initially between 10 and 1) so that all the above properties have a range of  and higher is better. For both combinations, we consider starting with \(|_{0}|=200\) random molecules and further querying the oracle \(N=8\) rounds with batch size \(b=100\). Each experiment is repeated with 3 random seeds.

Baselines.We compare HN-GFN with three representative LSO methods (\(q\)**ParEGO**, \(q\)**EHVI**, and **LaMOO**), as well as a variety of state-of-the-art combinatorial optimization

   Method & HV (\(\)) & Div (\(\)) & Cor (\(\)) \\  MOEA/D & \(0.182 0.045\) & n/a & n/a \\ NSGA-III & \(0.364 0.041\) & n/a & n/a \\ PS-GFN & \(0.545 0.055\) & \(0.786 0.013\) & \(0.653 0.003\) \\  Concat-GFN & \(0.534 0.069\) & \(0.786 0.004\) & \(0.646 0.008\) \\ FiLM-GFN & \(0.431 0.045\) & \(0.795 0.014\) & \(0.633 0.009\) \\  HN-GFN & \(\) & \(\) & \(\) \\   

Table 1: Evaluation of different methods on the synthetic scenario.

Figure 2: **Left**: The distribution of Top-100 JNK3 scores. **Right**: The progression of the average Top-20 rewards over the course of training of the HN-GFN in optimizing GSK3\(\) and JNK3.

methods: **Graph GA** is a genetic algorithm, **MARS** is a MCMC sampling approach, and **P-MOCO** is a multi-objective RL method. We provide more details in Appendix B.2.

Experimental results.Figure 3 illustrates that HN-GFN achieves significantly superior performance (HV) to baselines, especially when trained with the proposed hindsight-like off-policy strategy. HN-GFN outperforms the best baselines P-MOCO and Graph GA of the two objective combinations by a large margin (0.67 vs. 0.50 and 0.42 vs. 0.34), respectively. Besides, HN-GFN is more sample-efficient, matching the performance of baselines with only half the number of oracle evaluations. All combinatorial optimization methods result in more performance gains compared to the LSO methods, implying that it is promising to optimize directly over the discrete space. In Table 2, we report the Div (computed among the batch of 100 candidates and averaged over rounds). For a fair comparison, we omit the LSO methods as they only support 160 rounds with batch size 5 due to memory constraints. Compared with Graph GA and P-MOCO, which sometimes propose quite similar candidates, the superior optimization performance of HN-GFN can be attributed to the ability to sample a diverse batch of candidates. Another interesting observation, in the more challenging setting (GNK3\(\)+JNK3+QED+SA), is that MARS generates diverse candidates via MCMC sampling but fails to find a high-quality Pareto front, indicating that HN-GFN can find high-reward modes better than MARS. The computational costs are discussed in Appendix B.4.

### Ablations

Effect of the hindsight-like off-policy strategy.In the first round of MOBO, for each \(_{}\) we sample 100 molecules every 500 training steps and compute the average Top-20 reward over \(\). In Figure 2 (right), we find that the hindsight-like off-policy strategy significantly boosts average rewards, demonstrating that sharing high-performing molecules among policies effectively speeds up the training of HN-GFN. On the other hand, further increasing \(\) leads to slight improvement. Hence, we choose \(=0.2\) for the desired trade-off between generalization and specialization.

    &  \\  & GSK3\(+\) & GSK3\(+++\) \\  Graph GA & 0.347 \(\) 0.059 & 0.562 \(\) 0.031 \\ MARS & 0.653 \(\) 0.072 & **0.754 \(\) 0.027** \\ P-MOCO & 0.646 \(\) 0.008 & 0.350 \(\) 0.130 \\  HN-GFN & **0.810 \(\) 0.003** & 0.744 \(\) 0.008 \\ HN-GFN w/ hindsight & 0.793 \(\) 0.007 & 0.738 \(\) 0.009 \\   

Table 2: Diversity for different methods in MOBO scenarios.

Figure 3: Optimization performance (hypervolume) over MOBO loops.

Effect of \(()\).Here we consider the more challenging GNK3\(\)+JNK3+QED+SA combination, where the difficulty of optimization varies widely for various properties. Table 3 shows that the distribution skewed toward harder properties results in better optimization performance. In our early experiments, we found that if the distribution of \(^{i}\) is fixed, HN-GFN is quite robust to changes in \(\).

Effect of scalarization functions \(s_{}\).In Table 3, we observe that WS leads to a better Pareto front than Tchebycheff. Although the simple WS is generally inappropriate when a non-convex Pareto front is encountered , we find that it is effective when optimized with HN-GFN, which can sample diverse high-reward candidates and may reach the non-convex part of the Pareto front. In addition, we conjecture that the non-smooth reward landscapes induced by Tchebycheff are harder to optimize.

## 6 Conclusion

We have introduced a MOBO algorithm for sample-efficient multi-objective molecular optimization. This algorithm leverages a hypernetwork-based GFlowNet (HN-GFN) to sample a diverse batch of candidates from the approximate Pareto front. In addition, we present a hindsight-like off-policy strategy to improve optimization performance. Our algorithm outperforms existing approaches in synthetic and practical scenarios. **Future work** includes extending this algorithm to other discrete optimization problems such as biological sequence design and neural architecture search. **One limitation** of the proposed HN-GFN is the higher computational costs than other training-free optimization methods. However, the costs resulting from model training are negligible compared to the costs of evaluating the candidates in real-world applications. We argue that the higher quality of the candidates is much more essential than lower costs.