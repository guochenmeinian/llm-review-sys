# RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark

Federico Berto\({}^{*}\)\({}^{1}\), Chuanbo Hua\({}^{*}\)\({}^{1}\), Junyoung Park\({}^{*}\)\({}^{1,2}\), Minsu Kim\({}^{1}\),

**Hyeonah Kim\({}^{1}\), Jiwoo Son\({}^{1}\), Haeyeon Kim\({}^{1}\), Joungho Kim\({}^{1}\), Jinkyoo Park\({}^{1,2}\)**

\({}^{1}\) Korea Advanced Institute of Science and Technology (KAIST)

\({}^{2}\) OMELET

Equal contribution authorsSupervised learning approaches also offer notable improvements; However, their use is restricted due to the requirements of (near) optimal solutions during training.

###### Abstract

We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and RL algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse CO tasks. We also systematically benchmark zero-shot generalization, sample efficiency, and adaptability to changes in data distributions of various models. Our experiments show that some recent SOTA methods fall behind their predecessors when evaluated using these metrics, suggesting the necessity for a more balanced view of the performance of neural CO (NCO) solvers. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the NCO community to compare with existing methods through a standardized interface that decouples the science from software engineering. We make our library publicly available at https://github.com/kaist-silab/rl4co.

## 1 Introduction

Combinatorial optimization (CO) is a mathematical optimization area that encompasses a wide variety of important practical problems, such as routing problems and hardware design, whose solution space typically grows exponentially to the size of the problem (also often referred to as NP-hardness). As a result, CO problems can take considerable expertise to craft solvers and raw computational power to solve. Neural Combinatorial Optimization (NCO) [7; 44; 56] provides breakthroughs in CO by leveraging recent advances in deep learning, especially by automating the design of solvers and considerably improving the efficiency in providing solutions. While conventional operations research (OR) approaches [17; 23; 69] have achieved significant progress in CO, they encounter limitations when addressing new CO tasks, as they necessitate extensive expertise. In contrast, NCO trained with reinforcement learning (RL) overcomes the limitations of OR-based approaches (i.e., manual designs) by harnessing RL's ability to learn in the absence of optimal solutions.2 NCO presents possibilities as a general problem-solving approach in CO, handling challenging problems with minimal dependent (or even independent) of problem-specific knowledge [6; 38; 40; 36; 24; 5; 4; 2].

Among CO tasks, the routing problems, such as Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP), serve as one of the central test suites for the capabilities of NCO due to the extensive NCO research on that types of problems [49; 38; 40; 36] and also, the applicability of at-hand comparison of highly dedicated heuristic solvers investigated over several decades of study by the OR community [17; 23]. Recent advances [20; 42; 30] of NCO achieve comparable or superior performance to state-of-the-art solvers on these benchmarks, implying the potential of NCO to revolutionize the laborious manual design of CO solvers [69; 63].

However, despite the successes and popularity of RL for CO, the NCO community still lacks unified implementations of NCO solvers for easily benchmarking different NCO solvers. Similar to the other ML research, in NCO research, a unified open-source software would serve as a cornerstone for progress, bolstering reproducibility, and ensuring findings can be reliably validated by peers. This would provide a flexible and extensive RL for CO foundation and a unified library can thus bridge the gap between innovative ideas and practical applications, enabling convenient training and testing of different solvers under new settings, and decoupling science from engineering. In practice, this would also serve to expand the NCO area and make it accessible to researchers and practitioners.

Another problem that NCO research faces is the absence of standardized evaluation metrics that, especially account for the practical usage of CO solvers. Although most NCO solvers are customarily assessed based on their performance within training distributions [38; 40; 36], ideally, they should solve CO problems from out-of-training-distribution well. However, such out-of-distribution evaluation is overlooked in the literature. Furthermore, unlike the other ML research that already has shown the importance of the volume of training data, in NCO, the evaluation of the methods with the controls on the number of training samples is not usually discussed (e.g., state-of-the-art methods can underperform than the other methods). This also hinders the use of NCO in the real world, where the evaluation of solutions becomes expensive (e.g., evaluation of solutions involves the physical dispatching of goods in logistic systems or physical design problems) [14; 35; 2].

Contributions.In this work, we introduce RL4CO, a new reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO is first and foremost a library of several environments, baselines and boilerplate from the literature implemented in a _modular_, _flexible_, and _unified_ way with what we found are the best software practices and libraries, including TorchRL , PyTorch Lightning , TensorDict  and Hydra . Through thoroughly tested unified implementations, we conduct several experiments to explore best practices in RL for CO and benchmark our baselines. We demonstrate that existing state-of-the-art methods may perform poorly on different evaluation metrics and sometimes even underperform their predecessors. We also introduce a new Pareto-optimal, simple-yet-effective sampling scheme based on greedy rollouts from random symmetric augmentations. Additionally, we incorporate real-world tasks, specifically hardware design, to highlight the importance of sample efficiency in scenarios where objective evaluation is black-box and expensive, further validating that the functionally decoupled implementation of RL4CO enhances accessibility for achieving better performance in a variety of tasks.

## 2 Preliminaries

The solution space of CO problems generally grows exponentially to their size. Such solution space of CO hinders the learning of NCO solvers that generate the solution in a single shot3. As a way to mitigate such difficulties, the _constructive_ (e.g., [49; 70; 38; 40; 36]) methods generate solutions one step at a time in an autoregressive fashion, akin to language models [13; 68; 50]. In RL4CO we focus primarily on benchmarking autoregressive approaches for the above reasons.

Solving Combinatorial Optimization with Autoregressive Sequence GenerationAutoregressive (or _constructive_) methods assume the autoregressive solution construction schemes, which decide the next "action" based on the current (partial) solution, and repeat this until the solver generates the complete solution (e.g., in TSP, the next action is deciding on a city to visit). Formally speaking,

\[(|)_{t=1}^{T-1}(a_{t}|a_{t-1},...a_{1},),\] (1)

where \(=(a_{1},...,a_{T})\), \(T\) is the solution construction steps, is a feasible (and potentially optimal) solution to CO problems, \(\) is the problem description of CO, \(\) is a (stochastic) solver that maps \(\) to a solution \(\). For example, for a 2D TSP with \(N\) cities, \(=\{(x_{i},y_{i})\}_{i=1}^{N}\), where \((x_{i},y_{i})\) is the coordinates of \(i\)th city \(v_{i}\), a solution \(a=(v_{1},v_{2},...v_{N})\).

Training NCO Solvers via Reinforcement LearningThe solver \(_{}\) parameterized with the parameters \(\) can be trained with supervised learning (SL) or RL schemes. In this work, we focus on RL-based solvers as they can be trained without relying on the optimal (or high-quality) solutions Under the RL formalism, the training problem of NCOs becomes as follows:

\[^{*}=}_{ P( )}_{_{}(|)}R(, {x}),\] (2)

where \(P()\) is problem distribution, \(R(,)\) is reward (i.e., the negative cost) of \(\) given \(\).

To solve Eq.2 via gradient-based optimization method, calculating the gradient of the objective function w.r.t. \(\) is required. However, due to the discrete nature of the CO, the computation of the gradient is not straightforward and often requires certain levels of approximation. Even though few researchers show breakthroughs for solving Eq.2 with gradient-based optimization, they are restricted to some relatively simpler cases of CO problems [58; 60; 72]. Instead, it is common to rely on RL-formalism to solve Eq.2. In theory, value-based methods  and policy gradient methods [38; 40; 36; 53], and also actor-critic methods [52; 75] are applicable to solve Eq.2. However, in practice, it is shown that the policy gradient methods (e.g., REINFORCE  with proper baselines), generally outperform the value-based methods  in NCO.

General Structure of Autoregressive PoliciesThe autoregressive NCO solver (i.e., policy) _encodes_ the given problem \(\) and auto-regressively _decodes_ the solution. This can be seen as a processing input problem with the encoder and planning (i.e., computing a complete solution) with the decoder. To maximize the solution-finding speed, a common design of the decoder is to fuse the RL environment (e.g., TSP solution construction schemes that update the partial solutions and constraints of CO as well) into the decoder. This aspect of NCO policy is distinctive from the other RL tasks, which maintains the environment separately from the policy. As a result, most competitive autoregressive NCO solver implementations show significant coupling with network architecture and targeting CO problems. This can hinder the reusability of NCO solver implementation to the new types of CO problems. Furthermore, this design choice introduces difficulties for the fairer comparison among the trained solvers, especially related to the effect of encoder/decoder architectures and training/evaluation data usage on the solver's solution qualities.

## 3 Rl4co

In this paper, we present RL4CO, an extensive reinforcement learning (RL) for Combinatorial Optimization (CO) benchmark. RL4CO aims to provide a _modular_, _flexible_, and _unified_ code base that addresses the challenges of autoregressive policy training/evaluation for NCO (discussed in Section2) and performs extensive benchmarking capabilities on various settings.

### Unified and Modular Implementation

As shown in Fig.3.1, RL4CO decouples the major components of the autoregressive NCO solvers and its training routine while prioritizing reusability. We consider the five major components, which are explained in the following paragraphs.

PolicyThis module is responsible for constructing solutions for CO problems autoregressively. Our initial investigation into various autoregressive NCO solvers, such as AM, POMO, Sym-NCO, across CO problems like Traveling TSP, Capacitated Vehicle Routing Problem (CVRP), Orienteering Problem (OP), Prize-collecting TSP (PCTSP), among others, has revealed a common structural pattern. The policy network \(_{}\) follows an architecture that combines an encoder \(f_{}\) and a decoder \(g_{}\) as follows:

\[_{}(|) g_{}(f_{}())\] (3)

Upon analyzing encoder-decoder architectures, we have identified components that hinder the encapsulation of the policy from the environment. To achieve greater modularity, RL4CO modularizes such components in the form of _embeddings_: InitEmbedding, ContextEmbedding and DynamicEmbedding 4.

The encoder's primary task is to encode input \(\) into a hidden embedding \(\). The structure of \(f_{}\) comprises two trainable modules: the InitEmbedding and encoder blocks. The InitEmbedding module typically transforms problem features into the latent space and problem-specific compared to the encoder blocks, which often involve plain multi-head attention (MHA):

\[=f_{}()(())\] (4)

The decoder autoregressively constructs the solution based on the encoder output \(\). Solution decoding involves iterative steps until a complete solution is constructed:

\[q_{t} =(,a_{t-1:0}),\] (5) \[_{t} =(q_{t},W_{k}^{g},W_{v}^{g}),\] (6) \[(a_{t}) =(_{t} W_{v},M_{t}),\] (7)

where the ContextEmbedding is tailored to the specific problem environment, \(q_{t}\) and \(_{t}\) represent the query and attended query (also referred to as glimpse in Mnih et al. ) at the \(t\)-th decoding step, \(W_{k}^{g}\), \(W_{v}^{g}\) and \(W_{v}\) are trainable linear projections computing keys and values from \(\), and \(M_{t}\) denotes the action mask, which is provided by the environment to ensure solution feasibility. It is noteworthy that we also modularize the DynamicEmbedding, which dynamically updates the keys and values of MHA and Softmax during decoding. This approach is often used in dynamic routing settings, such as split delivery VRP. For the details, please refer to Appendix A.4.

From Eqs.4 and 5, it is evident that creating embeddings demands problem-specific handling, often trigger coherence between the policy and CO problems. In RL4CO, we offer pre-coded environment embeddings investigated from NCO literature [35; 38; 41] and, more importantly, allow a drop-in replacement of pre-coded embedding modules to user-defined embedding modules to attain higher modularity. Furthermore, we accommodate various decoding schemes (which will be further discussed in SS 4) proposed from milestone papers [38; 40; 36] into a unified decoder implementation so that those schemes can be applied to the different model, such as applying greedy multi-starts to the Attention Model from Kool et al. .

EnvironmentThis module fully specifies the problem, updates the problem construction steps based on the input action and provides the result of updates (e.g., action masks) to the policy

Figure 3.1: An overview of RL4CO. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.

module. When implementing the environment, we focus on parallel execution of rollouts (i.e., problem-solving) while maintaining _statelessness_ in updating every step of solution decoding. These features are essential for ensuring the reproducibility of NCO and supporting "look-back" decoding schemes such as Monte-Carlo Tree Search. Our environment designs and implementations are flexible enough to accommodate various types of NCO solvers that generate a single action \(a_{t}\) at each decision-making step [3; 33; 52; 53; 75]. Additionally, our framework is extensible beyond routing problems. We investigate the use of RL4CO for electrical design automation in Appendix B.

Our environment implementation is based on TorchRL, an open-source RL library for PyTorch, which aims at high modularity and good runtime performance, especially on GPUs. This design choice makes the Environment implementation standalone, even outside of RL4CO, and consistently empowered by a community-supporting library - TorchRL. Moreover, we employ TensorDicts to move around data which allows for further flexibility.

RL AlgorithmThis module defines the routine that takes the Policy, Environment, and problem instances and computes the gradients of the policy (and possibly the critic for actor-critic methods). We intentionally decouple the routines for gradient computations and parameter updates to support modern training practices, which will be explained in the next paragraph.

TrainerTraining a single NCO model is typically computationally demanding, especially since most CO problems are NP-hard. Therefore, implementing a modernized training routine becomes crucial. To this end, we implement the Trainer using Lightning, which seamlessly supports features of modern training pipelines, including logging, checkpoint management, automatic mixed-precision training, various hardware acceleration supports (e.g., CPU, GPU, TPU, and Apple Silicon), multi-GPU support, and even multi-machine expansion. We have found that using mixed-precision training significantly decreases training time without sacrificing NCO solver quality and enables us to leverage recent routines such as FlashAttention [16; 15].

Configuration ManagementOptionally, but usefully, we adopt Hydra, an open-source Python framework that enables hierarchical config management. It promotes modularity, scalability, and reproducibility, making it easier to manage complex configurations and experiments with different settings and maintain consistency across different environments.

### Availability and Future Support

RL4CO can be installed through PyPI5and we adhere to continuous integration, deployment, and testing to ensure reproducibility and accessibility.6

Our goal is to provide long-term support for RL4CO. It is actively maintained and will continue to update to accommodate new features and contributions from the community. Ultimately, our aim is to make RL4CO the to-go library in the RL for CO research area that provides encompassing, accessible, and extensive boilerplate code.

## 4 Benchmark Experiments

Our focus is to benchmark the NCO solvers under controlled settings, aiming to compare all benchmarked methods as closely as possible in terms of network architectures and the number of training samples consumed.

TL; DRHere is a summary of the benchmark results.

* AM , with minor encoder modifications and trained with a sufficient number of samples, can at times outperform or closely match state-of-the-art (SOTA) methods such as POMO and Sym-NCO for TSP and CVRP with 20 and 50 nodes. (See SS 4.1)
* The choice of decoding schemes has a significant impact on the solution quality of NCO solvers. We introduce a simple-yet-effective decoding scheme based on greedy augmentations that significantly enhances the solution quality of the trained solver. (See SS 4.1)
* We find that in-distribution performance trends do not always match with out-of-distribution ones when testing with different problem sizes. (See SS 4.2)
* When the number of samples is limited, the ranking of baseline methods can significantly change. Actor-critic methods can be a good choice in data-constrained applications. (See SS 4.3)
* We find that in-distribution results may not easily determine the downstream performance of pre-trained models when search methods are used, and models that perform worse in-distribution may perform better during adaptation. (See SS 4.4)

Benchmarked SolversWe evaluate the following NCO solvers:

    &  &  &  &  \\   & Cost \(\) & Gap & Time & Cost \(\) & Gap & Time & Cost \(\) & Gap & Time & Cost \(\) & Gap & Time \\  _Gurobi\({}^{}\)_ & 3.84 & \(-\) & 7s & 5.70 & \(-\) & 2m & 6.10 & \(-\) & \(-\) & \(-\) & \(-\) & \(-\) \\  _Concode_ & 3.84 & 0.00\(\%\) & 1m & 5.70 & 0.00\(\%\) & 2m & & & & N/A & & & \\ _HGS_ & & & & N/A & & & 6.13 & 0.00\(\%\) & 4h & 10.37 & 0.00\(\%\) & 10h \\ _LKH3_ & 3.84 & 0.00\(\%\) & 15s & 5.70 & 0.00\(\%\) & (\(<\)Sm) & 6.14 & 0.00\(\%\) & 5h & 10.38 & 0.00\(\%\) & 12h \\   \\  AM-critic & 3.86 & 0.64\(\%\) & (\(<\)1s) & 5.83 & 2.22\(\%\) & (\(<\)1s) & 6.46 & 5.00\(\%\) & (\(<\)1s) & 11.16 & 7.09\(\%\) & (\(<\)1s) \\ AM & 3.84 & 0.19\(\%\) & (\(<\)1s) & 5.78 & 1.41\(\%\) & (\(<\)1s) & 6.39 & 3.92\(\%\) & (\(<\)1s) & 10.95 & 5.30\(\%\) & (\(<\)1s) \\ POMO & 3.84 & 0.18\(\%\) & (\(<\)1s) & 5.75 & 0.89\(\%\) & (\(<\)1s) & 6.33 & 3.00\(\%\) & (\(<\)1s) & 10.80 & 3.99\(\%\) & (1s) \\ Sym-NCO & 3.84 & 0.05\(\%\) & (\(<\)1s) & 5.72 & 0.47\(\%\) & (\(<\)1s) & 6.30 & 2.58\(\%\) & (\(<\)1s) & 10.87 & 4.61\(\%\) & (1s) \\ AM-XL & 3.84 & 0.07\(\%\) & (\(<\)1s) & 5.73 & 0.54\(\%\) & (\(<\)1s) & 6.31 & 2.81\(\%\) & (\(<\)1s) & 10.84 & 4.31\(\%\) & (1s) \\   \\  AM-critic & 3.84 & 0.15\(\%\) & 20s & 5.74 & 0.72\(\%\) & 40s & 6.26 & 2.08\(\%\) & 24s & 10.70 & 3.07\(\%\) & 1m24s \\ AM & 3.84 & 0.04\(\%\) & 20s & 5.72 & 0.40\(\%\) & 40s & 6.24 & 1.78\(\%\) & 24s & 10.60 & 2.22\(\%\) & 1m2* AM  employs the multi-head attention (MHA) encoder and single-head attention decoder trained using REINFORCE and the rollout baseline.
* AM-Critic evaluates the baseline using the learned critic.
* POMO  is an extension of AM that employs the shared baseline instead of the rollout baseline.
* Sym-NCO  introduces a symmetric baseline to train the AM instead of the rollout baseline.
* AM-XL is AM that adopts POMO-style MHA encoder, using six MHA layers and InstanceNorm instead of BatchNorm. We train AM-XL on the same number of samples as POMO.

For all benchmarked solvers, we schedule the learning rate with MultiStepLinear, which seems to have a non-negligible effect on the performances of NCO solvers - for instance, compared to the original AM implementation and with the same hyperparameters, we can consistently improve performance, i.e. greedy one-shot evaluation on TSP50 from \(5.80\) to \(5.78\) and on CVRP50 from \(10.98\) to \(10.95\). In addition to the NCO solvers, we compare them to SOTA classical solvers that specialize in solving specific types of CO problems.

Decoding SchemesThe solution quality of NCO solvers often shows large variations in performances to the different decoding schemes, even though using the same NCO solvers. Regarding that, we evaluate the trained solvers using five schemes:

* Greedy selects the highest probabilities at each decoding step.
* Sampling concurrently samples \(N\) solutions using a trained stochastic policy.
* Multistart Greedy, inspired by POMO, decodes from the first given nodes and considers the best results from \(N\) cases starting at \(N\) different cities. For example, in TSP with \(N\) nodes, a single problem involves starting from \(N\) different cities.
* Augmentation selects the best greedy solutions from randomly augmented problems (e.g., random rotation and flipping) during evaluation.
* Multistart Greedy + Augmentation combines the Multistart Greedy with Augmentation.

We emphasize that our work introduces the new greedy Symmetric Augmentation during evaluation, a simple-yet-effective scheme. POMO utilized the 'x8 augmentation' through the dihedral group of order 8. However, we found that generalized symmetric augmentations - even without multistarts - as in Kim et al.  can perform better than other decoding schemes. For a visual explanation of the decoding scheme, please refer to Fig. 4.1.

### In-distribution Benchmark

We first measure the performances of NCO solvers on the datasets on which they are trained on. The results are summarized in Table 4.1. We first observe that, counter to the commonly known trends that AM < POMO < Sym-NCO, the trend can change to the selection of decoding schemes. Especially when the solver decodes the solutions with Augmentation or Greedy Multistart + Augmentation, the performance differences among the benchmarked solvers on TSP20/50, CVRP20/50 become insignificant. That implies we can improve the solution qualities by increasing the computational budget. These observations lead us to the requirements for an in-depth investigation of the sampling methods and their efficiency.

Figure 4.1: Decoding schemes of the autoregressive NCO solvers evaluated in this paper.

More Sampling, which Decoding Scheme?Based on our previous findings, we anticipate that by investing more computational resources (i.e., increasing the number of samples), the trained NCO solver can discover improved solutions. In this investigation, we examine the performance gains achieved with varying numbers of samples on the TSP50 dataset. As shown in Fig. 4.2, all solvers demonstrate that the Augmentation decoding scheme achieves the Pareto front with limited samples and, notably, generally outperforms other decoding schemes. We observed a similar tendency in CVRP50 (see Fig. 4.3). Additional results on OP and PCTSP are available in Appendix E.

### Out-of-distribution Benchmark

In this section, we evaluate the out-of-distribution performance of the NCO solvers by measuring the optimality gap compared to the best-known tractable solver. The evaluation results are visualized in SS 4.2. Contrary to the in-distribution results, we find that NCO solvers with sophisticated baselines (i.e., POMO and Sym-NCO) tend to exhibit worse generalization when the problem size changes, either for solving smaller or larger instances. This can be seen as an indication of "overfitting" to the training sizes. On the other hand, the variant of AM shows relatively better generalization results overall. We also evaluate the solvers in two canonical public benchmark instances (TSPLib and CVRPLib) in Appendix F, which exhibit both variations in the number of nodes as well as their distributions and find a similar trend.

### Sample Efficiency Benchmark

We evaluate the NCO solvers based on the number of training samples (i.e., the number of reward evaluations). As shown in Fig. 4.5, we found that actor-critic methods (e.g., AM trained with PPO detailed in Appendix D.7 or AM Critic) can exhibit efficacy in scenarios with limited training samples, as demonstrated by the TSP50/100 results in Fig. 4.5. This observation suggests that NCO solvers with control over the number of samples may exhibit a different trend from the commonly recognized trends. In the extension of this viewpoint, we conducted additional benchmarks in a different problem domain: electrical design automation (EDA) where reward evaluation is resource

Figure 4.3: Pareto front of decoding schemes vs. number of samples on CVRP50

Figure 4.2: Pareto front of decoding schemes vs. number of samples on TSP50

intensive, due to the necessity of electrical simulations. Therefore, sample efficiency becomes even more critical. For more details, please refer to Appendix B.

### Search Methods Benchmark

One viable and prominent approach of NCO that mitigates distributional shift (e.g., the size of problems) is the (post) search methods which involve training (a part of) a pre-trained NCO solver to adapt to CO instances of interest.

Benchmarked Search MethodsWe evaluate the following search methods:

* Active Search (AS) from Bello et al.  finetunes a pre-trained model on the searched instances by adapting all the policy parameters.
* Efficient Active Search (EAS) from Hottung et al.  finetunes a subset of parameters (i.e., embeddings or new layers) and adds an imitation learning loss to improve convergence.

Figure 4.4: Out-of-distribution generalization results.

Figure 4.5: Validation cost over the number of training samples (i.e., number of reward evaluations).

ResultsWe extend RL4CO and apply AS and EAS to POMO and Sym-NCO pre-trained on TSP and CVRP with \(50\) nodes from SS 4.1 to solve larger instances having \(N\) nodes. As shown in Table 2, solvers with search methods improve the solution quality. However, POMO generally shows better improvements over Sym-NCO. This may again imply the "overfitting" of sophisticated baselines that can perform better in-training but eventually worse in downstream tasks.

## 5 Discussion

### Future Directions in RL4CO

The utilization of symmetries in learning, such as by POMO and Sym-NCO, has its limitations in sample efficiency and generalizability, but recent studies like Kim et al.  offer promising results by exploring symmetries without reward simulation. There is also a trend toward few-shot learning, where models adapt rapidly to tasks and scales; yet, the transition from tasks like TSP to CVRP still requires investigation [43; 65]. Meanwhile, as AM's neural architecture poses scalability issues, leveraging architectures such as Hyena  that scale sub-quadratically might be key. Furthermore, the emergence of foundation models akin to LLMs, with a focus on encoding continuous features and applying environment-specific constraints, can reshape the landscape of NCO [68; 50]. Efficient finetuning methods could also be pivotal for optimizing performance under constraints [26; 67].

### Limitations

We identify some limitations with our current benchmark. In terms of benchmarking, we majorly focus on training the solvers on relatively smaller sizes, due to our limited computational budgets. Another limitation is the main focus on routing problems, even if RL4CO can be easily extended for handling different classes of CO problems, such as scheduling problems. Moreover, we did not benchmark shifts in data distributions for the time being (except for the real-world instances of TSPLib and CVRLib), which could lead to new insights. In future works, we plan to implement new CO problems that stretch beyond the routing and tackle even larger instances, also owing to the capability of RL4CO library.

### Conclusion

This paper introduces RL4CO, a _modular_, _flexible_, and _unified_ software library for Reinforcement Learning (RL) for Combinatorial Optimization (CO). Our benchmark library aims at filling the gap in a unified implementation for the NCO area by utilizing several best practices with the goal provide researchers and practitioners with a flexible starting point for NCO research. With RL4CO, we rigorously benchmarked various NCO solvers in the measures of in-distribution, out-of-distribution, sample-efficiency, and search methods performances. Our findings show that a comparison of NCO solvers across different metrics and tasks is fundamental, as state-of-the-art approaches may in fact perform worse than predecessors under these metrics. We hope that our benchmark library will inspire NCO researchers to explore new avenues and drive advancements in this field.

   &  &  &  \\   & & &  &  &  &  \\  & & 200 & 500 & 1000 & 200 & 500 & 1000 & 200 & 500 & 1000 & 200 & 500 & 1000 \\  _Classic_ & Cost & 10.17 & 16.54 & 23.13 & 10.72 & 16.54 & 23.13 & 27.95 & 63.45 & 120.47 & 27.95 & 63.45 & 120.47 \\   & Cost & 13.15 & 29.96 & 58.01 & 13.30 & 29.42 & 56.47 & 29.16 & 92.30 & 141.76 & 32.75 & 86.82 & 190.69 \\  & Gap[\%] & 29.30 & 81.14 & 150.80 & 24.07 & 77.87 & 144.14 & 4.33 & 45.47 & 17.67 & 17.17 & 36.83 & 58.29 \\  & Time[s] & 2.52 & 11.87 & 96.30 & 2.70 & 13.19 & 140.91 & 1.94 & 15.03 & 250.71 & 2.93 & 15.86 & 150.69 \\   & Cost & 11.16 & 20.03 & OOM & 11.92 & 22.41 & OOM & 28.12 & 63.98 & OOM & 28.51 & 66.49 & OOM \\  & Gap[\%] & 4.13 & 21.12 & OOM & 11.21 & 35.48 & OOM & 0.60 & 0.83 & OOM & 2.00 & 4.79 & OOM \\  & Time[s] & 7504 & 10070 & OOM & 7917 & 10020 & OOM & 8860 & 21305 & OOM & 9679 & 24087 & OOM \\   & Cost & 11.10 & 20.94 & 35.36 & 11.65 & 22.80 & 38.37 & 28.10 & 64.74 & 125.54 & 29.25 & 70.15 & 140.97 \\  & Gap[\%] & 3.55 & 26.64 & 52.89 & 8.68 & 37.86 & 67.63 & 0.52 & 2.04 & 4.21 & 4.66 & 10.57 & 17.02 \\   & Time[s] & 348 & 1562 & 1361 & 376 & 1589 & 14532 & 432 & 1972 & 20650 & 460 & 2051 & 17640 \\   

Table 2: Search Methods Benchmark results of models pre-trained on 50 nodes. We apply the search methods with default parameters from the literature. _Classic_ refers to Concorde  for TSP and LKH3  for CVRP. OOM denotes “Out of Memory”, which occurred with AS on large-scale instances.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? See??? 3. Did you discuss any potential negative societal impacts of your work? Our work involves optimization problems, such as routing problems, with no clear negative societal impact. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Our 2. Did you include complete proofs of all theoretical results?
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? We focus on the reproducibility of the results. As a part of such efforts, we share all the details of code, data, and instructions for reproducing the results in a form of a configuration file in our code repository. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? As similar to the previous question, we leave and share all training details as a configuration file. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We note that, as common practice in the field, we did not report multiple runs for the main tables as algorithms can take more than one day each to train. However, for experiments limited in the number of samples, such as for the sample efficiency experiments and the mDPP benchmarking, we reported multiple runs with different random seeds, where we demonstrated the robustness of different runs to random seeds. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? See SectionD.1
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We based our implementation of baseline models on the original code - although with several modifications - and included proper citations and credits to the authors, as well as references to existing software packages. 2. Did you mention the license of the assets? See SectionA.2
5. Did you include any new assets either in the supplemental material or as a URL? Aside from the software library link, we included automatic download to the PDN data for the mDPP benchmarking with the link available in the library.

* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [NA] Our library is based on local data generation. The data we use (PDN board, TSPLib, CVRPLib) is publicly available online and open source.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA] We do not include any offensive content; information is personally identifiable but thanks to the single-blind review process.
* If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]