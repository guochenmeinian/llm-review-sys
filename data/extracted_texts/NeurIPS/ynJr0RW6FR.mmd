# ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting

Yiqun Mei Jiacong Xu Vishal M. Patel

Johns Hopkins University

{ymei7,jxu155, vpatel36}@jhu.edu

Equal contribution

###### Abstract

Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses _discrete_ Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGS can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation.

## 1 Introduction

Stylizing a 3D scene based on a 2D artwork is an active research area in both computer vision and graphics . One important direction of stylization aims to precisely stylize the scene appearance based on a 2D content-aligned reference image drawn by users . Such problem has numerous applications in digital art, film production and virtual reality. In the classical graphics pipeline, completing this task requires experienced 3D artists to manually create a UV texture map as input to the shader, a tedious process requiring professional knowledge, significant time, and effort.

Over the past decades, tremendous progress has been made in automatic scene stylization by leveraging view synthesis methods. While early attempts  suffer from geometry errors of point clouds or meshes, more recent methods  rely on radiance field (NeRF) , a powerful implicit 3D representation, to deliver high-quality renditions that are perceptually similar to the reference image. A typical stylization workflow starts from a pretrained NeRF model of the target scene, followed by an appearance optimization phase to match the given style. The density function is always fixed to maintain the scene geometry . Despite their promising results, NeRF-based approaches consume high training and rendering costs in order to obtain satisfactory results. Although some recent efforts make fast training possible , the improvement in efficiency often comes at the price of degraded visual quality. Meanwhile, real-time rendering at inference time still remains challenging.

Recently, 3D Gaussian Splatting (3DGS)  has become an emerging choice for representing 3D scenes. 3DGS creates millions of colored Gaussians with learnable attributes to jointly represent the target scene geometry and appearance. Importantly, it adopts splatting-based rasterization  to replace the time-consuming volume rendering of NeRF models, providing remarkably faster rendering speed while maintaining comparable visual quality. However, as it uses _discrete_ 3D Gaussians to represent the reconstructed scene, optimizing their appearance with a fixed geometry layout (as NeRF-based methods do) is often inadequate to capture the _continuous_ texture variance in the reference image. This "appearance-geometry entanglement" makes applying 3DGS to applications that require novel appearance, _i.e_. stylization, challenging. For 3DGS, how to properly control and edit the appearance without distorting the original geometry remains under-explored.

In this paper, we present a novel reference-based scene stylization method using 3DGS, dubbed ReGS, to enable real-time stylized view synthesis with high-fidelity textures well-aligned with the given reference. Similar to previous methods, our approach starts with a pretrained 3D Gaussian model of the target scene. The core enabler of ReGS is a novel texture-guided control procedure that makes high-fidelity appearance editing with ease. In particular, we adaptively adjust the local arrangement of responsible Gaussians in the appearance underfitting regions to a state that the desired textures specified in the reference image can be faithfully expressed. The control process is designed to **(1)** automatically identify target local Gaussians using texture clues, and **(2)** structurally distribute tiny Gaussians for fast detail infilling while **(3)** sticking to the original scene structure via a depth-based regularization. With these novel designs, ReGS is able to learn consistent 3D appearance that accurately follows the given reference image.

Following , we train ReGS on a set of pseudo-stylized images for view consistency, which are synthetic multi-view data created using extracted scene depth, alongside with a template-based matching loss to ensure style spread to the occluded regions. By combining these techniques with the proposed texture-guided control, ReGS is capable of producing visually appealing stylization results that attain both geometric and perceptual consistency. Through extensive experiments, we demonstrate that ReGS achieves state-of-the-art visual quality compared to existing stylization methods while enabling real-time view synthesis by embracing the fast rendering speed of Gaussian Splatting.

## 2 Related Work

### 3D Scene Representation

**Neural Radiance Field.** Reconstructing 3D scene from multi-view collections is a long-standing problem in computer vision. Early approaches adopting explicit mesh [22; 23; 24; 25] or voxel [26;

Figure 1: Given a pretrained 3DGS model of the target scene and its paired style reference, ReGS enables real-time stylized view synthesis (at 134 FPS) with high-fidelity texture well-aligned with the reference. In contrast, only optimizing the appearance of 3DGS (denoted as Naive 3DGS), as previous methods [8; 9; 3; 10; 6] do, fails to capture many texture details in the reference. We tackle the challenges in high-fidelity appearance editing with a texture-guided control mechanism that is significantly more effective than the default density control  in addressing texture underfitting. Side-by-side comparisons with default density control can be found in Figure 5.

27, 28] based representations often suffer from geometry error and lack of appearance details . Recent methods  adopt learnable radiance fields  to capture 3D scene implicitly and outperform previous techniques by a large margin. However, NeRF models require millions of network queries for a single rendition that can be extremely time and resource-consuming. To reduce the training time, advanced methods adopt explicit/hybrid representations including voxel grid , octree , planes  and hash grid , and successfully reduce the training time from days to minutes. Nevertheless, the rendering speed at inference time is still limited by their volumetric nature, which requires dense sampling along a ray to generate a single pixel.

**3D Gaussian Splatting**. Recently, 3D Gaussian Splatting (3DGS)  achieves real-time novel view synthesis based on a differentiable rasterizer  that efficiently projects millions of 3D Gaussians to a 2D canvas. Given its high efficiency, 3DGS becomes a promising solution to enable real-time vision applications, such as human avatar , 3D object and immersive scene creation , relighting , surface or mesh reconstruction , 3D segmentation , and SLAM . Motivated by its high efficiency, our work explores 3DGS to enable real-time stylized view navigation.

### 2D Stylization

**Arbitrary Style Transfer.** Our method is related to the general 2D stylization , which transfers the style from an artwork to a target image while maintaining the original content structure. In the pioneering work, Gatys _et al_.  introduce an iterative scheme that progressively reduces the difference between the Gram statistics of generated image and style image features, yet lengthy optimization is required per style. To improve efficiency, later methods  focus on arbitrary image/video stylization by transferring the content image to target style spaces in a zero-shot manner. For example, Huang _et al_.  introduce AdaIN, which achieves real-time stylization by matching content features with the mean and standard deviation of style features. Linear style transfer  instead predicts a linear transformation matrix based on both content and style pairs. For video stylization, it is crucial to maintain temporal coherence of the stylized frames. Techniques , such as flow-based wrapping , global SSIM constraint , and inter-frame feature similarity , are proposed to ensure the consistency.

**Optimization-based Style Transfer.** While arbitrary style transfer is desirable in terms of flexibility, they often fall short of reproducing small stylistic patterns and lack high-frequency details . Optimization-based Stylization  is still the primary choice to ensure visual quality. For instance, a coarse-to-fine strategy is proposed by Liao _et al_.  to compute the nearest-neighbor field and build a semantically meaningful mapping between input and style images for visual attribute transfer. Kolkin _et al_.  reach state-of-the-art stylization quality by replacing the content features with the nearest style feature. To enable better controllability, example-based methods  perform wrapping or stylizing based on the aligned correspondences between the style reference and content images. However, their 2D alignment is generally unsuitable for 3D scenes due to occlusions, leading to flickering effects .

### 3D Stylization

3D scene stylization extends artistic works beyond the 2D canvas . To stylizing a 3D scene, both image exemplar  and text instructions  have been explored as style guidance. This work focuses on image-exemplar-based methods. Early works  typically back-project image colors as 3D point cloud for processing, and project stylized point features back to 2D for view synthesis. Yet, using point cloud often fails to represent complicated geometry and produces artifacts for complex scenes .

Benefiting from NeRF, methods stylizing radiance fields  have shown visually compelling and geometry-consistent results than previously possible. Similar to image stylization, several works  deal with arbitrary or multiple style transfer using various techniques such as 2D-3D mutual learning , deferred style transformation , and hypernetwork . While a universal stylizer might be desirable, these methods can only transfer the overall color tone and lack detailed style patterns, _i.e_. brushstrokes. Per-style optimization is still required for better visual quality. Among these methods , ARF  shows state-of-the-art stylization capability by progressively matching the generated features with the closest style feature via nearest neighbor search. Besides NeRF-based approaches, concurrent works [93; 94; 95; 96] also explore 3DGS  as scene representation. However, these methods are designed for transferring styles from an arbitrary reference and lack controllability over generated results. To this end, Ref-NPR  introduces a reference-based scheme that controls stylized appearance based on a content-aligned reference image. Our work also focuses on this setting.

## 3 Method

An overview of ReGS is shown in Figure 2. ReGS takes a pretrained 3DGS model (Figure 2 (a)) of the target scene as well as a content-aligned reference image as inputs. It outputs a stylized 3DGS model (Figure 2 (b)) that bakes the texture of the reference image into the scene and enables real-time stylized views synthesis (Figure 2 (d)).

As 3DGS represents a scene as discrete Gaussians, simply optimizing its appearance often cannot capture the continuous texture details in the reference image. We introduce a texture-guided control mechanism to progressively address this challenge (Sec. 3.2). To ensure no geometry distortion happens during optimization, we propose a geometry regularization using scene depth (Sec. 3.3). We then introduce two techniques to encourage perceptual-consistent renditions (Sec. 3.4). Finally, we describe our training objectives in Sec. 3.5.

### Preliminary: 3D Gaussian Splatting

Before introducing our method, we first provide a brief review of 3D Gaussian Splatting . 3DGS represents the scene explicitly by a collection of learnable Gaussians. Each 3D Gaussian is attributed by a positional vector \(^{3}\) and a 3D covariance matrix \(^{3 3}\). Its influence on a space point \(\) is proportional to a Gaussian distribution:

\[G()=e^{-(-)^{}^{-1}(-)}.\] (1)

By definition, the covariance matrix should be positive semi-definite. This is achieved by decomposing \(\) into a scaling matrix \(\) and a quaternion \(\)_i.e._\(=^{}^{}\). Each Gaussian also stores an opacity value \(_{i}\) and a view-dependent color represented by Spherical Harmonic (SH) coefficients.

The rendering procedure is implemented as splatting-based rasterization  which projects Gaussians to a 2D canvas. The projected 2D splats are then sorted based on the depth to the camera. After

Figure 2: **An overview of ReGS. (a) The proposed method starts with a pretrained content 3DGS of the target scene, and (b) outputs a stylized 3DGS that follows the reference. (c) We propose Texture-Guided Gaussian Control that can progressively resolve texture underfitting by automatically locating responsible Gaussians and adjusting local geometry layout for fitting high-frequency textures. (d) Once training is done, our method enables real-time stylized scene navigation.**

sorting, the final color for each pixel is computed through \(\)-blending:

\[C=_{i=1}^{n}c_{i}_{i}^{}_{j=1}^{i-1}(1-_{j}^{}),\] (2)

where \(c_{i}\) is a view-dependent color of the \(i\)-th Gaussian computed from SH. \(_{i}^{}\) is the multiplication result of the learned opacity \(_{i}\) and evaluated value of the projected 2D Gaussian.

During optimization, heuristic controls are employed to adaptively manage the density of Gaussians to better represent the scene. Specifically, it densifies Gaussians with large positional gradients to capture missing geometry and prunes Gaussians with small opacity to improve compactness.

### Texture-Guided Gaussian Control

As a discrete scene representation, the geometry layout and arrangement of Gaussians essentially limit the range of appearance it can express. For example, as shown in Figure 2 (c), appearance underfitting happens frequently at the area where local granularity of Gaussians is greater than the variance of the texture, _e.g._ a smooth colored surface in the original scene is painted with rich details in the reference view. ReGS addresses such challenges via a novel texture-guided control that splits these responsible local Gaussians into a denser set suitable for high-frequency texture. Specifically, the proposed mechanism automatically identifies responsible Gaussians using texture clues and structurally replaces them with a denser set of tiny Gaussians to compensate for the missing details. We describe important designs of the proposed algorithm below.

Texture Guidance.The control algorithm is directly guided by texture clues. Specifically, we accumulate color gradients of all Gaussians over iterations and select Gaussians with larger gradient magnitude than a threshold for densification. We found that a larger color gradient corresponds to Gaussians that have large texture errors while the optimization struggles to find the correct colors to reduce the loss. This control scheme shares a similar spirit with the original control scheme in 3DGS, where they leverage positional gradients to locate Gaussians responsible for missing geometric features. But in stylization, scene geometry is already well-reconstructed through pretraining, and therefore, the positional gradient is no longer informative. As demonstrated in Figure 5, our color-based control scheme is more sensitive for pinpointing Gaussians with missing textures than the positional-based solution. In practical implementation, we increase density based on the gradient statistics of every 100 iterations.

Structured Densification.Traditional mesh subdivision  cuts large faces into more sub-faces to express surface details. Sharing a similar spirit, we structurally split each responsible Gaussians into a structured denser set to better represent texture details. Intuitively, after densification, newly added Gaussians need to approximate the original space coverage to avoid inducing large geometry errors, and they should be sufficiently small and form a dense set to capture appearance variance. Based on these considerations, we propose a structured densification scheme that adds tiny Gaussians into the most representative locations surrounding their parent Gaussian. Specifically, we use nine tiny Gaussians to replace a parent Gaussian. Eight of them correspond to eight separate octants divided by the equatorial plane and perpendicular meridian planes of the original ellipsoid. And the rest is placed at the original center. We reduce their size by shrinking the scales with a factor of 8 and copy remaining parameters from their parent Gaussian. We empirically found this setup can roughly maintain a space coverage that approximates the original geometry. As optimization continues, the densified Gaussians are progressively updated to infill missing textures.

Figure 3: Examples of (a) rendered depth maps using Eq.3 and (b) synthesized stylized pseudo views.

### Depth-based Geometry Regularization

While our control mechanism progressively improves texture details, it is essential to ensure the original scene geometry is preserved after optimization. We resort to the scene depth as an additional regularization to penalize geometry changes. Examples of rendered depth are shown in Figure 3 (a). Formally, we derive the scene depth via a \(\)-blending-based equation:

\[d=_{i=1}^{n}d_{i}_{i}^{}_{j=1}^{i-1}(1-_{j}^{}),\] (3)

where the \(d_{i}\) is the z-buffer associated with the \(i\)th Gaussian and \({_{i}}^{}\) is the same evaluated opacity in Eq. 2. \(d_{i}\) is computed by projecting the 3D location \(\) to the camera space.

The depth regularization is defined as the \(L_{1}\) distance between a depth image \(D_{i}\) rendered from original scene model m and a depth image \(_{i}\) rendered from the stylized model \(}\) using the same camera pose \(_{i}\)_i_. \(_{depth}=\|_{i}-D_{i}\|_{1}\).

### View-Consistent Stylization

For stylization, it is necessary to ensure the stylized appearance is consistent across different viewpoints and inpaints the occluded areas. Following , we adopt two strategies to address them.

**Stylized Pseudo View Supervision.** An image with paired depth contains sufficient information to re-render from nearby viewpoints . This allows us to create a set of stylized pseudo views for obtaining additional supervision from the reference image. Our synthesis approach is very similar to classic depth-based 3D warping [98; 99]. Specifically, we back-project the reference image \(S_{R}\) to the world space using the depth image \(D_{R}\) and its camera pose \(_{R}\). Then, we re-project these 3D points back to a new viewpoint \(_{i}\). The resulting 2D image \(S_{i}\) is used as an additional style supervision. Examples of the created pseudo views are shown in Figure 3 (b). It is important to make sure supervision only happens on meaningful pixels, \(i\)._e_. they are projections of 3D points that are visible from the current viewpoint \(_{i}\). Therefore, we conduct a visibility check by comparing the depth between the 2D projections of the 3D points and the depth image \(D_{i}\) from the current viewpoint \(_{i}\). This results in a visibility mask \(M_{i}\). Given the pseudo views and visibility masks, one can define a pseudo view supervision loss as

\[_{view}=\|_{0}}\|M_{i}_{i}-M_{i}S_{i}\| _{1},\] (4)

where \(\|.\|_{0}\) is the \(_{0}\)-norm that counts the number of valid pixels and \(_{i}\) is renderings of the stylized model \(}\).

**Template Correspondence Matching (TCM) Loss.** To ensure stylized appearance spreads to the occluded areas, we adopt the same TCM loss proposed in . We briefly describe it here and refer readers to  for more details. TCM regularizes the difference of semantic correspondences before and after stylization. Given the style reference \(S_{R}\), its corresponding view \(I_{R}\), and a scene image \(I_{i}\) rendered from a camera pose \(_{i}\), it constructs a guidance feature \(F_{G}\) by \(F_{G_{i}}^{(x,y)}=F_{S_{R}}^{(x^{*},y^{*})}\) where

\[(x^{*},y^{*})=*{argmin}_{x^{},y^{}}(F_{ I_{i}}^{(x,y)},F_{I_{R}}^{(x^{},y^{})}).\] (5)

Here, \(F_{S_{R}}\), \(F_{I_{R}}\), \(F_{I_{i}}\) denote deep semantic features of image \(S_{R}\), \(I_{R}\), and \(I_{i}\) extracted by an ImageNet pretrained VGG . The superscript \((x,y)\) denotes the \(xy\) coordinates on the feature map. **dist** denotes the cosine distance. After obtaining the guidance feature, TCM loss is defined as a cosine distance loss:

\[_{TCM}=(F_{_{i}},F_{G_{i}}),\] (6)

where \(F_{_{i}}\) is the extracted VGG features of the generated stylized view \(_{i}\).

### Training Objectives

Besides aforementioned depth loss \(_{depth}\), pseudo view supervision loss \(_{view}\) and TCM loss \(_{TCM}\), ReGS further optimizes a reconstruction loss \(_{rec}\) and a coarse color-matching loss \(_{color}\). The reconstruction loss is defined as the \(L_{1}\) distance between the reference \(S_{R}\) and the correspondingstylized output \(_{R}\) to enforce appearance baking. The color-matching loss is defined as

\[_{color}=\|_{i}^{(x,y)}-S_{R}^{(x^{*},y^{*})}\|_{2}^{2},\] (7)

where \(S^{(x,y)}\) denotes the average color of a patch associated with feature-level index \((x,y)\). Feature-level index is computed using Eq. 9. This loss is directly adapted from  to encourage overall color matching in the occluded area. The overall loss for ReGS can be expressed as

\[=_{rec}_{rec}+_{depth}_{depth }+_{view}_{view}+_{tcm}_{TCM}+_{ color}_{color}\] (8)

where \(_{(.)}\) denotes the balancing parameter.

### Implementation and Training Details

ReGS uses 3D Gaussians  as the scene representation and is built upon their official codebase. We follow the default parameter settings to obtain the pretrained 3D Gaussian model of the photo-realistic scene. For stylization, as we do not expect view-dependent effects, we discard the higher order SH and only render diffuse color in the stylization phase. Therefore, content images used in \(_{TCM}\) and \(_{color}\) are the results of this diffuse model.

For texture-guided control, we start accumulating gradients after a warm-up of 100 iterations and then perform the densification operation based on the color gradient statistics of every 100 iterations. The control process stops when it reaches half of the total iterations. The gradient threshold is empirically set to \(1e-5\) at the beginning, and we linearly reduce it to \(5e-6\) to allow for refining tiny details in the later training stage. Following [10; 8], we use the ImageNet pretrained VGG16  as the feature extractor and use the features produced by _relu_3_ and _relu_4_ in \(_{TCM}\). For balancing parameters we set \(_{rec}=_{tcm}=1\), \(_{depth}=10\), \(_{view}=2\), and \(_{color}=15\), which are determined by a simple grid search on Blender  scenes. At each iteration, we always sample two views: the reference view and a random view. We train our model for 3000 iterations. The proposed method is implemented using PyTorch and trained on one A5000 GPU.

## 4 Experiments

In this section, we demonstrate the stylization quality and our designs through extensive experiments. More experiment results and ablations can be found in the supplemental file and accompanied video.

### Datasets

The only available reference-based stylization dataset is provided by . The dataset contains 12 selected scenes from Blender , LLFF , and Tanks and Temples . Each scene is paired with a content-aligned reference image.

### Ablation Study

We conduct controlled experiments to analyze the effectiveness of each design choice in ReGS. Results are illustrated in Figures 4 & 5. As illustrated in Figure 4, replacing any components of ReGS will harm the stylization quality. For example, Figure 4 (a) shows that optimizing only the

Figure 4: **Ablation study on different components of ReGS. (a) Optimizing only the appearance of a 3DGS model cannot reproduce texture details. (b) Removing depth regularization causes Gaussians to float out from the surface and distort the origin geometry. (c) Without pseudo-view supervision, results lack view consistency. (d) Our full model produces the best results that faithfully respect the texture in the reference.**

appearance with fixed geometry arrangement like previous methods [8; 6; 3; 9; 10] do fails to recover the texture details. As shown in Figure 4 (b), after removing depth regularization, Gaussians float out from the surface and distort the original scene geometry. Similarly, discarding the pseudo view supervision will induce view-inconsistency as highlighted in the inset (Figure 4 (c)). The full model overcomes these issues and produces more visually appealing results that follow the given reference.

**Effectiveness of Texture-Guided Control.** The core enabler of ReGS is the proposed texture-guided control mechanism that makes high-fidelity appearance editing with ease. Here, we demonstrate its effectiveness by comparing it with the default positional-gradient-guided density control  in addressing texture underfitting. Specifically, we conduct controlled experiments by setting a series of limits on the total number of Gaussians that can grow throughout optimization. Results are reported in Figure 5. One can see that by growing a very small amount of Gaussians (0.05M), the proposed texture-guided method can quickly infill most of the missing details. With more Gaussians added, it can further faithfully reproduce the given texture. In contrast, even with a large amount of new Gaussians (0.25M) created, the default method can barely capture high-frequency texture details. This is mainly because positional gradient is not sensitive to texture errors. As such, it fails to grow Gaussians in the regions with texture underfitting. And further moving these incorrectly placed Gaussians to the correct place for texture infilling is challenging. These results demonstrate our method is indeed more favorable for addressing appearance underfitting. Study on individual component (_i.e_. structured densification and color-gradient guidance) can be found in the supplement.

### Compare with State-of-the-art Methods

To evaluate stylization performance, we compare our method with three state-of-the-art baselines: ARF , SNeRF , and Ref-NPR . ARF  and SNeRF  are general stylization methods that conduct style transfer without considering content correspondence. Ref-NPR  is a reference-based stylization method similar to ours that aims to precisely edit the 3D appearance based on the reference. All baselines are NeRF-based approaches built upon Plenoxels .

**Qualitative Evaluation.** We report qualitative results in Figure 6. As shown, ARF  and SNeRF  cannot generate semantic-consistent results with respect to the reference image as they ignore content correspondence. In contrast, Ref-NPR  produces more controllable results but yields artifacts. In some challenging cases (_e.g_. last row in Figure 6), it also fails to achieve semantic consistent

  Metric & ARF  & SNeRF  & Ref-NPR  & ReGS (Ours) \\  Ref-LPIPS\(\) & 0.394 & 0.405 & 0.339 & **0.202** \\ Robustness\(\) & 26.34 & 26.03 & 28.11 & **31.27** \\ Speed (fps) & 16.5 & 16.3 & 16.4 & **91.4** \\  

Table 1: Quantitative comparison of different stylization methods.

Figure 5: **Effectiveness of Texture-Guided Control.** We conduct controlled experiments by limiting the number of newly densified Gaussians throughout optimization. The pretrained model contains 0.3M Gaussians. The proposed texture-guided control can more faithfully reproduce the target texture details with a small number of Gaussians added (0.05M). The default strategy struggles to capture high-frequency details, even with a large number of Gaussians added (0.25M).

stylization (_i.e_. green tree in the reference image is colored as white). In contrast, our method achieves better results that reproduce the desired texture, including challenging high-frequency ones.

**Quantitative Evaluation.** We present quantitative results in Table 1. Results are averaged over all scenes. We follow the protocol from  and report Ref-LPIPS and Robustness. Ref-LPIPS computes LPIPS  score between the reference image and the 10 nearest test views. To calculate robustness, we first (1) train a stylized base model \(m_{b}\) and use it to render a set of stylized views as new references; (2) then we use these references to train another set of stylized models and (3) compute PSNR results between images produced by them and \(m_{b}\) (using the same camera path). To measure run-time efficiency, we also report run-time FPS on a single A5000 GPU. As shown in Table 1, our method achieves the best results in terms of both quality and efficiency. Notably, our method enables real-time stylized view synthesis at 91 FPS.

Figure 6: **Visual comparisons with state-of-the-art methods.** Paired reference and content view are shown on the left. Our method produces visual-compelling results that precisely follow the texture of the given reference, including the challenging high-frequency details such as the leaf in the second example. Baseline methods  either lack semantic consistency or produce artifacts.

## 5 Application: Appearance Editing

ReGS naturally enables high-fidelity appearance editing. As shown in Figure 7, given a pretrained 3DGS model and its rendering, our method allows users to make freehand edits on the image (_e.g_. "computer" on the plate) and robustly bake the edits back into the 3D scene with view-consistency. Unlike NeRFs , such task cannot be robustly handled by just optimizing the appearance of 3DGS (denoted as Naive Gaussian), especially when edits happen on a smooth surface where the granularity of Gaussians is greater than the texture variance. Benefiting from texture-guided control, our method can effectively locate these large Gaussians and replace them with a denser set for better appearance editing.

## 6 Conclusion

In this work, we introduce ReGS, which adapts Gaussian Splatting for reference-based controllable scene stylization. ReGS adopts a novel texture-guided control mechanism to make high-fidelity appearance editing with ease. This is achieved by adaptively replacing responsible Gaussians with a denser set to express the desired appearance details. The control process is guided by texture clues for appearance editing while preserving original scene geometry through a depth-based regularization. We demonstrate the state-of-the-art scene stylization quality and effective designs of ReGS through extensive experiments. Benefiting from the high efficiency of 3DGS, our method naturally enables real-time stylized view synthesis. Discussions of limitations can be found in the supplemental file.

## 7 Acknowledgment

This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 140D0423C0076. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.