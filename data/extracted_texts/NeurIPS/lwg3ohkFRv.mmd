# CARE: Modeling Interacting Dynamics Under Temporal Environmental Variation

Xiao Luo\({}^{1}\), Haixin Wang\({}^{2}\), Zijie Huang\({}^{1}\), Huiyu Jiang\({}^{3}\),

**Abhijeet Sadashiv Gangan\({}^{1}\), Song Jiang\({}^{1}\), Yizhou Sun\({}^{1}\)**

\({}^{1}\)University of California, Los Angeles, \({}^{2}\)Peking University,

\({}^{3}\)University of California, Santa Barbara

{xiaoluo,yzsun}@cs.ucla.edu, wang.hx@stu.pku.edu.cn

###### Abstract

Modeling interacting dynamical systems, such as fluid dynamics and intermolecular interactions, is a fundamental research problem for understanding and simulating complex real-world systems. Many of these systems can be naturally represented by dynamic graphs, and graph neural network-based approaches have been proposed and shown promising performance. However, most of these approaches assume the underlying dynamics does not change over time, which is unfortunately untrue. For example, a molecular dynamics can be affected by the environment temperature over the time. In this paper, we take an attempt to provide a probabilistic view for _time-varying_ dynamics and propose a model Context-attended Graph ODE (CARE) for modeling time-varying interacting dynamical systems. In our CARE, we explicitly use a context variable to model time-varying environment and construct an encoder to initialize the context variable from historical trajectories. Furthermore, we employ a neural ODE model to depict the dynamic evolution of the context variable inferred from system states. This context variable is incorporated into a coupled ODE to simultaneously drive the evolution of systems. Comprehensive experiments on four datasets demonstrate the effectiveness of our proposed CARE compared with several state-of-the-art approaches.

## 1 Introduction

Modeling interacting dynamical systems is a fundamental machine learning problem  with a wide range of applications, including social network analysis  and particle-based physical simulations . Geometric graphs  are utilized to formalize these interactions between objects. For example, in particle-based physical systems, edges are constructed based on the geographical distance between particles, which represents the transfer of energy.

In the literature, numerous data-driven approaches have been proposed for understanding interacting dynamical systems . Among them, graph neural networks  (GNNs) are widely utilized to predict trajectories at the next timestamp due to their strong capacity to capture interactions in graph-structured data. In particular, each object is considered as a graph node, and edges represent interactions between neighboring objects. Given the observations and their corresponding graph structure, these methods forecast states in the next timestamp using the message passing mechanism. This process involves aggregating information from the neighbors of each node to update its representation in an iterative fashion, effectively capturing the dynamics of the system.

Although impressive progress has been achieved on GNN-based approaches, capturing long-term dependency in interacting dynamical systems is still a practical but underexplored problem. Existing next-step predictors  can send the predictions back to generate rollout trajectories, which could suffer from serious error accumulation for long-term predictions. More importantly, systemenvironments and relational structures could be changeable  (e.g., unsteady flow [11; 16]), which implies the potential temporal distribution variation during the evolution. In particular, in physical systems, there are various potential factors which can influence the trajectories extensively. For example, high temperatures  or pressure  could speed up molecular movement. Their continuous variation would make understanding interacting dynamic systems more challenging. First, temporal environmental variation would indicate different data distributions over the time , which requires the model equipped with superior generalization capability. In contrast, existing methods typically focus on in-distribution trajectories [7; 1; 58; 3; 21; 66], which would perform worse when it comes to out-of-distribution data. Second, recent out-of-distribution generalization methods [50; 39; 57; 64; 44] usually focus on vision and text with discrete shift across different domains. However, our scenarios would face the continuous distribution variation, which is difficult to capture in interacting dynamical systems.

In this paper, we propose a novel method named Context-attended Graph ODE (CARE) to capture interacting system dynamics. The core of our CARE approach is to characterize the temporal environmental variation by introducing the context variable. In particular, we propose a probability model to depict the interaction between the context variable and trajectories. Based on the probabilistic decomposition, we divide each training sequence into two parts for initializing embeddings and predictions. Here, we first construct a temporal graph and then leverage an attention-based encoder to generate node representations and the context representation from spatial and temporal signals simultaneously. More importantly, we introduce coupled ODEs to model the dynamic evolution of node representations and the context variable. On the one hand, we adopt a graph-based ODE system enhanced with context information to drive the evolution. On the other hand, the context information can also be updated using summarized system states and the current context. We also provide a theoretical analysis that indicates, at least locally, the future system trajectory and context information are predictable based on their historical values. Finally, we introduce efficient dynamical graph updating and robust learning strategies to enhance the generalization capability and efficiency of the framework, respectively. Extensive experiments on various dynamical systems demonstrate the superiority of our proposed CARE compared with state-of-the-art approaches.

To summarize, in this paper we make the following contributions: (1) _Problem Formalization_. We formalize the problem of temporal environmental variation in interacting dynamics modeling. (2) _Novel Methodology_. We analyze the problem under a probabilistic framework, and propose a novel approach CARE, which incorporates the continuous context variations and system states into a coupled ODE system. (3) _Extensive Experiments_. Extensive experiments conducted on four datasets validate the superiority of our CARE. The performance gain of our proposed CARE over the best baseline is up to 36.35%.

## 2 Related Work

**Interacting Dynamics Modeling.** Deep learning approaches have been extensively used in recent years to model interacting systems across various fields [9; 53; 29; 20], including molecular dynamics and computational fluid dynamics. Early efforts focus on incorporating convolutional neural networks to learn from interacting regular grids . To address more generalized scenarios, graph neural network (GNN) methods have been developed [42; 46; 45], leveraging message mechanisms to extract complex spatial signals. However, these methods often fail to account for environmental fluctuations, which hinders their ability to make reliable long-term predictions. In contrast, our CARE adopts a context-attended ODE architecture to explicitly represent both the observations and the underlying environment, enabling the generation of accurate future trajectories.

**Neural Ordinary Differential Equations (ODEs).** Drawing inspiration from the approximation of ResNet , neural ODEs equip neural networks with a continuous-depth architecture by parameterizing the derivatives of hidden states. Several attempts have been made to increase the expressiveness of neural ODEs , including adding regularization  and designing high-order ODEs . Neural ODEs have also been incorporated into GNNs, producing continuous message passing layers to avoid oversmoothing  and increase the interpretability of predictions . In this study, we employ a graph ODE architecture to capture the continuous nature of interacting system dynamics, relieving potential error accumulation caused by discrete prediction models.

**Out-of-distribution (OOD) Generalization.** OOD generation aims to make models more effective when the training and testing distributions diverge [50; 39; 57; 63]. This problem has drawn considerable attention in several areas, including text and vision . One effective solution is to learn domain-invariant representations in the hidden space [31; 55; 30], which has been achieved under the guidance of invariant learning theory [6; 32]. Additionally, uncertainty modeling , causal learning [54; 14], and model selection [38; 56] are employed to improve the performance. Interacting systems inherently exhibit dynamic distribution variation caused by environmental changes, an aspect that remains underexplored in the literature. To address this, our paper proposes a novel approach named CARE to model context information from the perspective of a probabilistic model.

## 3 Background

### Problem Definition

In a multi-agent dynamical system, the state at time \(t\) is represented by \(G^{t}=(V,E^{t},^{t})\), where each node in \(V\) corresponds to an object, \(E^{t}\) denotes the current edge set, and \(^{t}\) signifies the node attribute matrix. Specifically, the state vector for each \(i V\) is given by \(_{i}^{t}=[_{i}^{t},_{i}^{t},_{i}]\), with \(_{i}^{t}^{3}\) and \(_{i}^{t}^{3}\) representing the position and velocity, respectively, and \(_{i}\) representing static attributes. We are given the sequence \(\{G^{0},G^{1},,G^{t}\}\) and aim to learn a model that produces the target dynamic states \(^{s}(s>t)\) (e.g., velocity), which are part of \(^{s}\) at the corresponding time. The temporal environmental variation would result in data distribution changes during the evolution of interacting systems. If we utilize \(C^{0:t}\) to indicate the dynamical environment factor till timestamp \(t\), we have data from variable distribution, i.e., \((G^{0:t},^{s}) P(G^{0:t},^{s}|C^{0:t})\). Thus, we must take these changes into account for accurate trajectory predictions.

### GNNs for Modeling Dynamical Systems

Graph neural networks (GNNs) are extensively employed in dynamical system modeling to investigate the interactive relationships between objects [42; 46; 45]. These methods typically use the current states to predict the states of nodes at the next timestamp. Specifically, omitting the time notation, GNNs first initialize node representations and edge representations using encoders:

\[_{i}^{(0)}=f^{v}(_{i}),_{ij}^{(0)}=f^{e} (_{i},_{j}),\] (1)

where \(f^{v}()\) and \(f^{e}()\) are two encoders for node and edge representations, respectively. Then, they utilize two propagation modules to update these representations at the \(l\)-th layer, i.e., \(_{i}^{(l)}\) and \(_{ij}^{(l)}\) following the message passing mechanism:

\[_{ij}^{(l+1)}=^{e}(_{i}^{(l)},_{j}^{(l)},_{ ij}^{(l)}),_{i}^{(l+1)}=^{v}(_{i}^{(l)},_{j _{i}}_{ij}^{(l+1)}),\] (2)

Figure 1: An overview of the proposed CARE. To begin, we construct a temporal graph and utilize an encoder to initialize both the context variable and node representations from historical trajectories. Then, a coupled ODE model simulates the evolution of both nodes and context. Finally, CARE feeds node representations into decoders, which output the predicted trajectories at any timestamp.

where \(_{i}\) collects the neighbours of node \(i\). \(^{e}()\) and \(^{v}()\) are two functions for representation updating. Finally, they generate the target velocity vectors at the next timestamp using a decoder.

## 4 Methodology

In this paper, we propose a novel method called CARE for modeling interacting dynamics under temporal environmental variation. We start by formalizing a probabilistic model to understand the relationships between trajectories and contexts. Based on this foundation, we construct a spatio-temporal encoder to initialize the representations of nodes and contexts. Then, to simultaneously model their evolution, a coupled graph ODE is introduced where node representations are evolved with the guidance of contexts and the states of the context variable are inferred from current trajectories. Additionally, we introduce a regularization term and dynamic graph updating strategies to enhance our framework. An illustration of our CARE can be found in Figure 1.

### Probabilistic Model for System Dynamics under Temporal Distribution Drift

In this work, to tackle the challenge brought by temporal distribution drift, we inject a context variable \(^{t}\) in our dynamic system modeling, which indicates the environment state at timestamp \(t\). For example, the context variable could indicate flow speed, density and viscosity in fluid dynamics.

Here, we make two basic assumptions in our probabilistic model.

**Assumption 4.1**.: _(Independence-I) The context variable is independent of the sequences before the last observed timestamp, i.e., \(P(^{t}|^{t-k},G^{0:t})=P(^{t}|^{t-k},G^{t-k:t})\), where \(t-k\) is the last observed timestamp._

**Assumption 4.2**.: _(Independence-II) Given the current states and contexts, the future trajectories are independent of the previous trajectories and contexts, i.e., \(P(^{t-k:t+l}|G^{0:t-k},^{0:t-k})=P(^{t-k:t-k+l}|G^{t-k},^{t-k})\) where \(l\) is the length of the prediction._

Then, we can have the following lemma:

**Lemma 4.1**.: _With Assumptions 4.1 and 4.2, we have:_

\[&(^{t} G^{0:t-1})= (^{t}^{t-1},G^{t-1})\\ &(^{t-1}^{t-k},G^{t-k:t-1}) (^{t-k} G^{0:t-k})d^{t-1}d^{t -k}.\] (3)

The proof of Lemma 4.2 can be found in Appendix. From Lemma 4.1, we decompose the probability \((^{t} G^{0:t-1})\) into three terms. Specifically, the last term necessitates encoding context information based on the historical trajectory \(G^{0:t-k}\). The second term aims to update the context vector according to the recent trajectory \(G^{t-k:t-1}\). The first term suggests using the context variable in conjunction with the current states to make predictions for the next timestamp. Besides making a single next-time prediction, our model can also predict trajectories (\(^{t-k},^{t-k+1},,^{t}\)) by modifying Eqn. 14.

Consequently, we divide each training sequence into two parts, namely \([0,t-k]\) and \((t-k,t]\) as in [19; 18]. The first part is used to encode contexts and nodes in the system, while the second part serves for updating and prediction purposes.

### Context Acquirement from Spatio-temporal Signals

In this part, our goal is to acquire knowledge from the historical trajectory, i.e., \(\{G^{0},,G^{t-k}\}\) to encode contexts and nodes for initialization. To be specific, we first construct a temporal graph to capture spatial and temporal signals simultaneously. Subsequently, we employ the attention mechanism to update temporal node representations, which will be used to initialize the context representation and node representations for \(\{G^{t-k},,G^{T}\}\).

To begin, we construct a temporal graph containing two types of edges, i.e., spatial and temporal edges. Spatial edges are built when the distance between two nodes at the same timestamp is less than a threshold while temporal edges are between every two consecutive observations for each node. Specifically, in the constructed temporal graph \(G^{H}\), there are \(N(t-k+1)\) nodesin total. The adjacent matrix \(\) contains both spatial and temporal edges as follows:

\[(i^{s},j^{s^{}})=\{exp(-d^{s}(i,j))&s=s^{ },exp(-d^{s}(i,j))<,\\ 1&i=j,s^{}=s+1,\\ 0&,.\] (4)

where \(d^{s}(i,j)\) denotes the distance between particles \(i\) and \(j\) at timestamp \(s\) and \(\) is the predefined threshold. Then, we utilize an attention-based GNN to extract spatio-temporal relationships into node representations. Here, we first compute the interaction scores between each node in \(G^{H}\) with its neighboring nodes, and then aggregate their embeddings at the previous layer. Let \(d\) represent the hidden dimension, the interaction score between \(i^{s}\) and \(j^{s^{}}\) at layer \(l\) is:

\[w^{(l)}(i^{s},j^{s^{}})=(i^{s},j^{s^{}})(_{query}_{i}^{s,(l)})(_{key}_{j}^{s^{},(l)}),\] (5)

where \(_{query}^{d d}\) and \(_{key}^{d d}\) are two matrices to map temporal node representations into different spaces. \(\) computes the cosine similarity between two vectors. With the interaction scores, we can compute temporal node representations at the next layer:

\[_{i}^{s,(l+1)}=_{i}^{s,(l)}+(_{j^{s^{}} (i^{s})}w^{(l)}(i^{s},j^{s^{}})_{value}_{j}^{s^{ },(l)}),\] (6)

where \(_{value}\) is a weight matrix, \((i^{s})\) collects all the neighbours of \(i^{s}\) and \(()\) is an activation function. After stacking \(L\) layers, we add temporal encoding and then summarize all these temporal node representations to initialize node representations for the upcoming ODE module:

\[_{i}^{s}=_{i}^{s,(L)}+(s),_{i}^{t-k}=_{s=0}^{t-k}(_{sum}_{i}^{s}),\] (7)

where \((s)[2i]=(})\), \((s)[2i+1]=(})\) and \(_{sum}\) denotes a projection matrix. The initial context variable \(^{t-k}\) is driven by summarizing all node representations:

\[_{i}^{t}=tanh((_{i^{} V}_{i^{}}^{t -k})_{context})_{i}^{t-k},^{t-k}=_{i V} _{i}^{t}_{i}^{t-k},\] (8)

where \(_{context}\) is a learnable matrix and \(_{i}^{t}\) calculates the attention score for each node.

### Context-attended Graph ODE

In this module, to model continuous evolution, we incorporate an ODE system into our approach. The precondition requires assuming that both the context variable and node representations are continuous to fit neural ODE models, which inherently holds for common dynamical systems in practice. We then introduce coupled ODEs to model the dynamic evolution of node representations and the context variable. Specifically, the context variable can be inferred during the evolution of node representations, which in turn drives the evolution of the system. We first introduce the assumption:

**Assumption 4.3**.: _(Continuous) We assume that both context variable \(^{s}\) and node representations \(_{i}^{s}\) are continuously differentiable with respect to \(s\)._

Then, to utilize the context variable and the current state for making future predictions, we introduce a graph ODE model. Let \(}^{s}\) denote the adjacency matrix at timestamp \(s\) with self-loop, we have:

\[_{i}^{s}}{ds}=([_{1}^{s},,_{N}^{s},^{ s}])=(_{j^{s}(i)}}_{ij}^{s}}{_{i}^{s}_{j}^{s}}}_{j}^{s}_{1}+^{s}_ {2}),\] (9)

where \(^{s}(i)\) denotes the neighbours of node \(i\) at timestamp \(s\) and \(_{i}^{s}\) represents the degree of node \(i\) according to \(}^{s}\). The first term in Eqn. 9 aggregates information from its instant neighbors and the second term captures information from the current context information.

The next question is how to model the evolution of \(^{s}\). Notice that we have:

\[(^{t}^{t-k},G^{t-k:t })= P(^{t}|^{t- t},G^{t- t:t})\\ P(^{t-k+ t}|^{t-k},G^{t-k:t-k+ t})d^{t-k + t} d^{t- t},\] (10)where \( t\) denotes a small time interval. With Assumption 4.3, we can simplify \(P(^{t-k+ t}|^{t-k},G^{t-k:t-k+ t})\) into \(P(^{t-k+ t}|^{t-k},^{t-k},d^{t-k})\) where \(^{t-k}\) denotes the node embedding matrix at timestamp \(t-k\) and \(d^{t-k}\) is its differentiation. On this basis, we introduce another ODE to update the context variable as:

\[^{s}}{ds}=^{c}((\{^{s}_{i}\}_{i V}), (\{^{s}_{i}}{ds}\}_{i V}),^{s}]),\] (11)

where \(^{c}\) is an MLP with the concatenated input and \(()\) is an operator to summarize node representations such as averaging and sum. Compared to previous methods, the key to our CARE is to take into account the mutual impact between the environment and the trajectories, and model their evolution simultaneously by coupling Eqn. 9 and Eqn. 11. We also provide a theoretical analysis of the uniqueness of the solution to our system. To simplify the analysis, we set \(()\) to summation and rewrite Eqn. 11 with learnable matrices \(_{3}\), \(_{4}\) and \(_{5}\) as:

\[^{s}}{ds}=(_{i=1}^{N}(^{s}_{i}_{3}+ ^{s}_{i}}{ds}_{4})+^{s}_{5}).\] (12)

Then, we introduce the following assumption:

**Assumption 4.4**.: _All time-dependent coefficients in Eqn. 9, i.e. \(^{t}_{ij},^{t}_{i}\) are continuous with respect to \(t\) and bounded by a constant \(C>0\). All parameters in the weight matrix are also bounded by a constant \(W>0\)._

With Assumption 4.4, we can deduce the following lemma:

**Lemma 4.2**.: _Given the initial state \((t_{0},^{t_{0}}_{1},,^{t_{0}}_{N},^{t_{0}})\), we claim that there exists \(>0\), s.t. the ODE system 9 and 12 has a unique solution in the interval \([t_{0}-,t_{0}+]\)._

The proof of Lemma 4.2 can be found in Appendix. Our theoretical analysis indicates that at least locally, the future system trajectory and context information are predictable based on their historical values , which is also an important property for dynamical system modeling [5; 28].

### Decoder and Optimization

**Decoder.** We introduce an MLP \(^{d}()\) to predict both the position and velocity vectors using corresponding node representations, i.e., \([}^{s}_{i},}^{s}_{i}]=^{d}(^{s}_{i})\).

**Dynamic Graph Updating.** We can estimate the instant distance between nodes using the encoder and then construct the graphs, which could suffer from a large computational burden. To improve the efficiency of graph construction during ODE propagation, we not only update the graph structure every \( s\), and but also introduce a graph updating strategy that calculates the distance between first-order and second-order neighbors in the last graph. By doing so, we can delete edges between first-order neighbors and add edges between second-order neighbors, reducing quadratic complexity to linear complexity in sparse graphs. We will also validate this empirically.

**Learning Objective.** Given the ground truth, we first minimize the mean squared error (MSE) of the predicted trajectory. Moreover, we require both node and context representations to be robust to noise attacks to improve the robustness of the ODE system. The overall objective is written as:

\[=_{s=t-k}^{t}||}^{s}-^{s}||+(||}^{s}-^{s}||+||}^{s}-^{s}||),\] (13)

where \(}^{s}\) denotes the predictions from the encoder and \(\) is a parameter set to \(0.1\) to balance two losses. \(}^{s}\) and \(}^{s}\) denote the perturbed representations under noise attack to the input.

## 5 Experiments

We evaluate our proposed CARE on both particle-based and mesh-based physical systems. To ensure the accuracy of our results, we use a rigorous data split strategy, where first 80\(\%\) of the samples are reserved for training purposes and the remaining 10\(\%\) are set aside for testing and validating,separately. During training, we split each trajectory sample into two parts, i.e., a conditional part and a prediction part. We initialize node representations and the context representation based on the first part and utilize the second part to supervise the model. The size of the two parts is represented as conditional length and prediction length, respectively. Our approach is compared with various baselines for interacting systems modeling, i.e., LSTM , STGCN , GNS , MeshGraphNet , TIE  and CG-ODE .

### Performance on Particle-based Physical Simulations

**Datasets.** We evaluate our proposed CARE on two particle-based simulation datasets with temporal environmental variations, i.e., _Lennard-Jones Potential_ and _3-body Stillinger-Weber Potential_. _Lennard-Jones Potential_ is popular in modeling electronically neutral atoms or molecules. _3-body Stillinger-Weber Potential_ provides more complex relationships in atom systems The temperature in two particle-based systems is continuously changed along with the time to model the environmental variations. The objective is to predict the future velocity values in all directions, i.e., \(v_{x}\), \(v_{y}\) and \(v_{z}\). More details can be found in Appendix.

**Performance Comparison.** We evaluate the performance in terms of RMSE with different prediction lengths. Table 1 show the compared results on these two datasets. We can observe that our proposed CARE outperforms all the baselines on two datasets. In particular, compared with TIE, CARE accomplishes an error reduction of 24.03% and 36.35% on two datasets, respectively. The remarkable performance can be attributed to two factors: (1) Introduction of the context variable. Our CARE infers the context states during the evolution of the system, which can help the model understand environmental variations. (2) Introduction of robust learning. We add noise attack to both system and context states, which improves the model generalization to potential distribution changes.

   Prediction Length &  &  &  &  \\  Variable & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) \\   \\  LSTM & 3.95 & 3.92 & 3.68 & 9.12 & 9.21 & 9.15 & 10.84 & 10.87 & 10.76 & 14.82 & 14.94 & 14.67 \\ GNS & 3.28 & 3.75 & 3.39 & 7.97 & 8.05 & 7.68 & 10.09 & 10.15 & 10.13 & 13.65 & 13.62 & 13.59 \\ STGCN & 2.91 & 3.08 & 2.95 & 5.06 & 5.17 & 5.11 & 6.89 & 6.90 & 6.93 & 9.31 & 9.32 & 9.44 \\ MeshGraphNet & 2.89 & 3.13 & 2.94 & 5.29 & 5.53 & 5.28 & 7.03 & 7.09 & 7.11 & 9.12 & 9.21 & 9.24 \\ CG-ODE & 1.79 & 2.05 & 1.71 & 3.47 & 3.92 & 3.38 & 5.46 & 5.99 & 5.36 & 9.03 & 9.26 & 8.92 \\ TIE & 1.62 & 1.98 & 1.47 & 3.25 & 3.90 & 3.15 & 5.24 & 5.82 & 5.17 & 8.24 & 8.34 & 8.47 \\ Ours & **0.76** & **0.89** & **1.01** & **2.94** & **3.16** & **2.85** & **5.01** & **4.69** & **4.71** & **5.75** & **5.91** & **5.82** \\   \\  LSTM & 17.11 & 17.14 & 17.18 & 23.64 & 23.69 & 23.60 & 25.46 & 25.42 & 25.48 & 28.44 & 28.45 & 28.44 \\ GNS & 15.39 & 15.27 & 15.33 & 22.14 & 22.19 & 22.17 & 25.29 & 25.36 & 25.31 & 27.18 & 27.15 & 27.14 \\ STGCN & 12.33 & 12.31 & 12.35 & 17.94 & 17.96 & 17.91 & 20.08 & 20.14 & 20.13 & 23.49 & 23.51 & 23.52 \\ MeshGraphNet & 12.16 & 12.10 & 12.13 & 18.33 & 18.38 & 18.34 & 20.65 & 20.62 & 20.71 & 23.62 & 23.54 & 23.61 \\ CG-ODE & 9.78 & 9.74 & 9.75 & 12.11 & 12.05 & 12.14 & 15.55 & 15.58 & 15.50 & 16.17 & 16.24 & 16.22 \\ TIE & 10.18 & 10.26 & 10.19 & 14.75 & 14.70 & 14.73 & 18.42 & 18.45 & 18.41 & 20.92 & 21.04 & 21.36 \\ Ours & **4.21** & **4.29** & **4.18** & **9.74** & **9.79** & **9.71** & **13.65** & **13.71** & **13.57** & **15.30** & **15.39** & **15.35** \\   

Table 1: The RMSE (\( 10^{-2}\)) results of the compared methods with the prediction lengths \(1\), \(5\), \(10\) and \(20\). \(v_{x}\), \(v_{y}\) and \(v_{z}\) represent the velocity in the direction of each coordinate axis.

Figure 2: Visualization of _Lennard-Jones Potential_ with multiple timestamps. We render the 3D positions of each particle according to the historical positions and predicted velocities.

**Visualization.** Figure 2 visualizes the prediction of positions in comparison to the ground truth on _Lennard-Jones Potential_. Here, we sample six timestamps in every trajectory to validate the performance of both short-term and long-term predictions. From the qualitative results, we can observe that in the first three frames, the particle motion is not strenuous due to low temperature in the system. Surprisingly, our proposed CARE can always make faithful physical simulations close to the ground truth even though the system environment is highly variable.

### Performance on Mesh-based Physical Simulations

**Datasets.** We employ two popular mesh-based simulation datasets, i.e., _CylinderFlow_, and _Airfoil_. _CylinderFlow_ consists of simulation data from modeling an incompressible flow governed by the Navier-Stokes equations. Notably, the initial flow velocity of the incoming water flow to the cylinder varies cyclically over time, meaning the Reynolds number of the flow field also changes periodically. _Airfoil_ is generated in a similar manner through simulations of a compressible flow, wherein the inlet velocity over the wing varies cyclically over time. We aim to forecast the velocity values \(v_{x}\) and \(v_{y}\), as well as the pressure \(p\). More details can be found in Appendix.

**Performance Comparison.** The performance with respect to different variables is recorded in Table 2. From the results, we can observe that the average performance of the proposed CARE is over the best baseline by 12.99% and 22.78% on two datasets, respectively. Note that unsteady flow [11; 16] is a crucial problem in recent fluid dynamics, our proposed CARE can benefit abundant complex mesh-based simulations under environmental variations.

**Visualization.** Moreover, we show the qualitative results of the best baseline and our CARE in comparison to the ground truth. From the results, we can observe that our CARE can capture more accurate signals in unsteady fluid dynamics. In particular, in the last two frames with complicated

   Prediction Length &  &  &  &  \\  Variable & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) \\   \\  LSTM & 3.35 & 29.4 & 12.5 & 7.06 & 44.8 & 17.8 & 9.47 & 49.5 & 19.9 & 14.3 & 73.6 & 42.3 \\ GNS & 3.12 & 28.8 & 11.9 & 7.18 & 44.3 & 17.3 & 9.01 & 49.6 & 19.2 & 13.5 & 73.2 & 41.6 \\ STGCN & 2.68 & 26.7 & 11.0 & 5.47 & 42.1 & 16.9 & 6.72 & 45.6 & 18.4 & 9.15 & 68.7 & 40.0 \\ MeshGraphNet & 1.75 & 22.4 & 10.6 & 4.09 & 39.7 & 15.7 & 5.38 & 44.5 & 17.2 & 7.92 & 64.3 & 37.7 \\ CG-ODE & 1.05 & 20.4 & 8.51 & 3.44 & 36.8 & 13.6 & 4.15 & 38.5 & 17.1 & 5.14 & 61.2 & 32.3 \\ TIE & 1.22 & 20.8 & 8.94 & 3.75 & 35.2 & 13.0 & 4.62 & 40.6 & 16.0 & 5.87 & 59.5 & 32.1 \\ Ours & **0.87** & **19.1** & **7.21** & **3.02** & **32.9** & **11.8** & **3.95** & **37.8** & **13.9** & **4.97** & **55.8** & **29.4** \\   \\  LSTM & 7.49 & 7.73 & 1.92 & 8.86 & 9.02 & 3.78 & 10.8 & 11.0 & 4.71 & 14.9 & 15.7 & 4.96 \\ GNS & 6.95 & 7.14 & 1.69 & 8.20 & 8.34 & 3.34 & 10.2 & 10.5 & 3.98 & 14.2 & 14.1 & 4.11 \\ STGCN & 6.24 & 5.35 & 1.07 & 6.57 & 6.51 & 2.33 & 7.88 & 8.01 & 3.16 & 11.6 & 11.8 & 3.17 \\ MeshGraphNet & 4.72 & 4.68 & 0.50 & 5.89 & 5.74 & 1.23 & 6.32 & 6.48 & 1.85 & 9.03 & 9.12 & 2.08 \\ CG-ODE & 4.26 & 4.32 & 0.35 & 4.78 & 4.70 & 0.46 & 5.81 & 5.66 & 1.04 & 7.39 & 7.85 & 1.69 \\ TIE & 4.17 & 4.39 & 0.33 & 4.99 & 4.86 & 0.51 & 5.75 & 5.62 & 0.95 & 7.25 & 7.63 & 1.44 \\ Ours & **3.51** & **4.11** & **0.19** & **3.86** & **3.75** & **0.34** & **4.16** & **4.12** & **0.45** & **6.74** & **6.82** & **0.81** \\   

Table 2: The RMSE results of the compared methods over different prediction lengths \(1\), \(10\), \(20\) and \(50\). \(v_{x}\), \(v_{y}\) and \(p\) represent the velocity in different directions and the pressure field, respectively.

Figure 3: Visualization of the CylinderFlow Dataset with multiple timestamps. We render the velocity in the \(x\)-axis in the fluid field of our CARE and the ground truth.

structures, our CARE still can generate superior simulations in both scenarios under potential environmental variation while the baseline fails, which shows the superiority of our proposed CARE.

### Further Analysis

**Ablation Study.** To analyze the effectiveness of different components in our CARE, we introduce two different variants: (1) CARE V1, which removes the context variable in Eqn. 9; (2) CARE V2, which removes the robust learning term in Eqn. 13. The compared performance is recorded in Figure 3 and we have two observations. First, our full model outperforms CARE V1, which indicates the incorporation of the context variable would benefit interacting system modeling under temporal environmental variation. Second, without the robust learning term, the performance would get worse, implying that improving the robustness can also benefit tackling the distribution changes.

**Parameter Sensitivity.** We begin by analyzing the performance with respect to different condition lengths and prediction lengths. Here the condition length and prediction length vary from {10,15,20,25,30}, {20,50}, respectively. From the results in Figure 4 (a) and (b), we can observe that our proposed CARE can always achieve superior performance compared with CG-ODE. Moreover, we can observe that a longer condition length would benefit the performance in most cases due to more provided information. It can also be seen that a smaller interval for graph updating would improve the performance before saturation from Figure 4 (c).

**Efficiency.** To show the efficiency of our proposed dynamic graph updating, we propose a model variant named CARE E, which calculates all pairwise distances to update graph structure during the evolution. The computational cost is recorded in Figure 4 (d) and we can observe that our strategy can reduce a large number of computational costs, which validates the complexity analysis before.

## 6 Conclusion

This paper studies the problem of modeling interacting dynamics under temporal environmental variation and we propose a probabilistic framework to depict the dynamical system. Then, a novel approach named CARE is proposed. CARE first constructs an encoder to initialize the context variable indicating the environment and then utilizes a coupled ODE system, which combines both the context variable and node representation to drive the evolution of the system. Finally, we introduce both efficient dynamical graph updating and robust learning strategies to enhance our framework. Extensive experiments on four datasets validate the superiority of our CARE.

**Broader Impacts and Limitations.** This work presents an effective learning-based model for interacting dynamical systems under temporal environmental variation, which can benefit complex physical simulations such as unsteady flow. Moreover, our study provides a new perspective on

Figure 4: (a), (b) The performance with respect to different condition and prediction lengths on CylinderFlow and Airfoil. (c) The sensitivity of interval on Lennard-Jones Potential (LJP) and 3-body Stillinger-Weber Potential (SWP) datasets. (d) The comparison of running time for our dynamic graph updating and full pairwise calculation on two particle-based datasets.

   Datasets &  &  &  &  \\  Variable & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) \\  CARE V1 & 6.98 & 7.12 & 7.06 & 18.2 & 18.3 & 18.3 & 6.13 & 60.4 & 32.2 & 7.13 & 7.21 & 1.43 \\ CARE V2 & 6.03 & 6.35 & 6.30 & 16.8 & 16.5 & 16.6 & 5.21 & 57.2 & 29.8 & 6.94 & 6.99 & 1.15 \\ Ours & **5.75** & **5.91** & **5.82** & **15.3** & **15.4** & **15.4** & **4.97** & **55.8** & **29.4** & **6.74** & **6.82** & **0.81** \\   

Table 3: Ablation study on four datasets.

modeling environmental variations for fluid dynamics and intermolecular interactions. One potential limitation is that our CARE cannot directly fit more physical scenarios requiring abundant external knowledge. In future work, we would extend our CARE to more complicated applications such as rigid dynamics.