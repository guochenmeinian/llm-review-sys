# Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks

Chiraag Kaushik

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30308

ckaushik7@gatech.edu

Justin Romberg

Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta, GA 30308

jrom@ece.gatech.edu

Vidya Muthukumar

Electrical and Computer Engineering,

Industrial & Systems Engineering

Atlanta, GA 30308

vmuthukumar8@gatech.edu

###### Abstract

The classical _iteratively reweighted least-squares_ (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and \(_{p}\)-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a "batched" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.

## 1 Introduction

Many high-dimensional machine learning and signal processing tasks rely on solving optimization problems with regularizers that explicitly enforce certain structure on the learned parameters. The traditional formulation for such tasks involves a regularized empirical risk minimization (ERM) problem of the form

\[_{^{d}}L()+ R(),\] (1)

where \(L()\) is a loss function that encourages fidelity to the observed training data and \(R()\) encodes desirable structural properties. In many important applications, it is desirable to obtain a sparsity-seeking solution; in such cases, the regularizer is typically non-smooth, as in the LASSO, group LASSO, and nuclear norm regularizers. As an alternative approach to this non-smooth optimization, several recent works have proposed the "Hadamard over-parameterization" of \(\) into the entry-wise product of two factors \(\). While the resulting minimization problem is non-convex, thisparameterization, coupled with a _smooth_ regularizer, has been shown to achieve competitive empirical performance (in terms of numerical stability, robustness, and convergence rate) when compared to traditional sparse recovery algorithms . For example, rather than solving the convex, but non-smooth LASSO (where \(L\) is the squared loss and \(R\) is the \(_{1}\) norm), the Hadamard reparameterization yields the following non-convex and smooth formulation:

\[_{,^{d}}L()+( \|\|_{2}^{2}+\|\|_{2}^{2}).\] (2)

In the case where the regression function is linear in \(\), solving (2) is equivalent to learning a function of the form

\[f_{,}()=,= (),,\]

which can be thought of as a one hidden layer neural network with linear activation function and inner weight matrix \(()\). In this context, this _linear diagonal neural network (LDNN)_ architecture has also been studied as an illustrative case study to improve our understanding of how neural networks perform iterative _"feature learning"_ to leverage low-dimensional structure in high-dimensional settings . We note here that, in the linear model case, feature learning is equivalent to learning which subset of the input's coordinates are relevant for the true predictor (i.e., feature selection).

One way to understand the connection between classical sparse recovery algorithms and the Hadamard product/LDNN form in (2) is to consider the change of variable \(v_{i}}\) and \(u_{i}}{}}\). This yields the following optimization problem, which is jointly convex in \(\) and \(\):

\[_{^{d}}_{^{d}_{+}}L( {})+_{i=1}^{d}^{2}}{ _{i}}+_{i}.\] (3)

After solving the minimum over \(\) explicitly, the second term becomes exactly \(\|\|_{1}\), and we recover the Lasso objective. This is a special case of the so-called "eta-trick" , which can be used to write many common sparsity-inducing penalties as the minimization of a quadratic functional of \(\).

A variety of algorithms for learning Hadamard product parameterizations have recently been studied, including alternating minimization , bi-level optimization , and joint gradient descent on \((,)\). The connection to the \((,)\) optimization in (3) can also be leveraged to construct algorithms based on classical sparse recovery techniques. In particular, alternating minimization over \(\) and \(\) in (3) yields the popular _iteratively reweighted least-squares (IRLS)_ algorithm . Translating these updates to the equivalent updates on \((,)\), we obtain an iterative least-squares algorithm for LDNNs, which alternately sets \(^{(t+1)}=^{(t)}^{(t)}|}\) and performs a weighted least squares update on \(\). This particular form of reparameterized IRLS was generalized in  to a larger family of iterative least-squares algorithms under the name of _linear recursive feature machines (lin-RFM)_.

While several methods for learning Hadamard/LDNN parameterizations have been introduced in the literature, there remain many open questions about how they each perform and how they compare. Theoretical analyses of these algorithms typically assume fixed, possibly worst-case training data, and aim to characterize the properties of the fixed-points  or give convergence guarantees to second-order stationary points . However, these worst-case analyses do not readily yield guarantees on the estimation error, which is the principal metric of interest. Indeed, many works have shown that studying the _average-case_, or typical, behavior of non-convex optimization algorithms can allow for estimation guarantees that are more precise and reflective of practice .

In this paper, we provide a precise analysis of a general family of iterative algorithms for learning LDNNs that take the form

\[^{(t+1)} =_{^{d}}^{( t)}-}^{(t)}(^{(t)})_{2}^{2}+ \|\|_{2}^{2}\] \[^{(t+1)} =(^{(t+1)},^{(t)}),\]

for some reweighting function \(\) and batches of training data \((^{(t)},^{(t)})\). As we show in Section 2, this formulation encompasses multiple existing algorithms, including reparameterized IRLS, lin-RFM, and alternating minimization over \(\) and \(\). We consider the common scenario where training is performed with batches of data and characterize the _exact_ distribution of the parameters after each iteration in the high-dimensional limit \((n,d)\). This allows us to address questions such as * How do different algorithm choices compare (in terms of convergence and signal recovery) in the high-dimensional regime?
* How many iterations does it take common algorithms to find statistically favorable solutions?
* What is the effect of _model architecture_ in LDNNs? Does leveraging group structure provably improve sample complexity when the ground-truth signal is group-sparse?

Contributions: We define a general class of algorithms which learns LDNNs by alternately performing least-squares and reweighting steps in a sample-split/batched setting, and we show the following.

**(1)** Under mild assumptions on the target signal, initialization, and reweighting function, we provide an exact characterization of the distribution of the entries of the parameters at each iteration in the limit as \(n,d\) approach infinity (Theorem 1).

**(2)** We show that this asymptotic result aligns well with numerical simulations and allows for accurate prediction of the test error at each iteration. This enables rigorous comparison between different algorithms and demonstrates that, with appropriate reweighting schemes, a statistically favorable solution can be obtained in only a handful of iterations.

**(3)** Lastly, we extend our asymptotic framework to a setting of _structured sparsity_, where \(^{*}\) has group-sparse structure (Theorem 2). Our results show that using a grouped Hadamard parameterization (i.e., tying together groups of weights in the LDNN) effectively learns such signals, with performance scaling with the number of non-zero groups, rather than the total sparsity level.

### Related work

**IRLS and the \(\)-trick:** The reformulation of non-smooth regularizers in terms of quadratic variational forms (the "\(\)-trick") has been studied in various early works in computer vision and robust statistics [12; 4]. Further analysis and examples of sparsity-promoting norms are provided in [20; 2], and  provides a characterization of when a regularizer admits a variational form of this type. The resultant optimization algorithm is iteratively-reweighted least-squares (IRLS), a popular technique for compressive sensing and sparse recovery [11; 9]. These works also consider IRLS algorithms corresponding to \(_{p}\)-norm regularization for \(0<p<1\); in this case, the minimization is no longer convex, but  shows that such methods can find sparse solutions with fast local convergence rate. The family of algorithms we consider includes a reparameterized version of each of these IRLS algorithms, but unlike these prior works, we consider a batched setting and the high-dimensional asymptotic regime. Moreover, our results apply to other algorithms which may not be easily expressed as resulting from the \(\)-trick.

Hadamard parameterization and linear diagonal networks: The reparameterization of \(\) into the product of factors \(\) has been considered in a variety of recent works. The authors of [31; 35] show that early-stopped joint gradient descent over the two factors can lead to optimal sample complexity for sparse linear regression. The equivalence of this parameterization to LDNNs has also led to a surge of interest in the _implicit bias_ of gradient descent/flow on this parameterization, i.e., a characterization of which solution gradient descent will reach without explicit regularization (corresponding to \(=0\)). These works typically consider gradient flow run until completion and characterize the solution as a minimizer of a certain sparsity-inducing functional that depends on the initialization [34; 10; 23].

The connection between the LASSO (as well as some non-convex \(_{q}\) penalties) and the Hadamard parameterization was studied in , where alternating minimization over the two factors is used instead of first-order methods. More recently,  extends these observations by making explicit the connection to the \(\)-trick and showing that saddle points are strict (escapable). These insights lead to global convergence guarantees and a smooth bi-level optimization scheme [25; 26] for non-smooth structured optimization problems that was shown to perform competitively with state-of-the-art solvers. The non-convex landscape of such formulations is further explored in , where it is shown that for a large class of parameterizations (including grouped, deep, and fractional Hadamard products), the non-convex problem has no spurious local minima. Motivated by the type of feature learning observed in neural networks, the authors of  propose lin-RFM, which updates one of the parameters via weighted least-squares while iteratively updating the other parameter via a reweighting scheme based on the average gradient outer product of the learned function. The authors characterize properties of the fixed-points and show that, for certain reweighting schemes, lin-RFM is equivalent to a reparameterization of IRLS. The family of algorithms we consider is similar, consisting of a weighted least-squares step and a reweighting step; however, it is more general and doesn't require the reweighting function to have the particular form required by lin-RFM. Moreover, our asymptotic characterization of the iterates allows for a precise understanding of how the test error evolves. On the other hand, our analysis relies on batching/sample-splitting of training data while all of the above works reuse the entire batch of training data at each iteration.

We make particular note here of the few works which explicitly consider a "grouped" Hadamard parameterization, which we consider in Section 4. This corresponds to a LDNN with groups of tied weights in the hidden layer. Early stopped gradient flow/descent for this type of architecture was shown in  to achieve sample-complexity scaling with the number of non-zero groups (rather than the overall sparsity). The non-convex landscape for this grouped architecture is studied in  and . Our results complement these works by studying group-reweighted least-squares algorithms (rather than gradient methods) for learning functions of this form.

**Precise characterization of higher-order non-convex optimization problems:** On a technical level, our work provides a precise deterministic characterization of a family of higher-order optimization algorithms. In this sense, our results are of a similar flavor to , where Gaussian comparison inequalities are used to obtain a precise characterization of non-convex optimization problems. However, since the Hadamard parameterization is a re-parameterization of the actual estimator of interest (\(\)), the results of  are not directly applicable. While our results are asymptotic and do not provide finite-sample guarantees, we provide a _distributional_ characterization of \(\) after each reweighting step, which allows us to characterize the behavior of more complicated functions of the iterates. Precise characterizations of alternating minimization and lin-prox methods for rank-1 matrix sensing are studied in the works [6; 19]. While these works obtain non-asymptotic guarantees, the estimation model and resulting optimization objective are quite different, with each unknown parameter interacting with independent sensing vectors (rather than a single sensing vector interacting with the product of the two parameters).

## 2 Background and formulation

**Notation:** The ones vector of dimension \(d\) is denoted as \(_{d}\). We denote the element-wise multiplication (Hadamard product) of two vectors \(\) and \(\) as \(\). Element-wise division of two vectors is denoted as \(}{}\). We say a function \(f^{p}\) is _pseudo-Lipschitz_ of order \(2\) if, for all \(\), \(^{p}\),

\[|f()-f()| C(1+\|\|_{2}+\|\|_{2})\|-\|_ {2}\]

for some constant \(C>0\). The set of such functions is denoted by \((2)\).

Convergence in probability of a sequence of random variables \(X_{d}\) to a random variable \(X\) is denoted by \(X_{d}}{{}}X\). Convergence in Wasserstein-2 distance of a sequence of probability distributions \(_{d}\) to a limiting distribution \(\) is denoted as \(_{d}_{2}}}{{}}\), and this fact is equivalent to the statement \(_{X_{d}}g(X)_{X}g(X)\) for all \(g(2)\). If the \(_{d}\) are _random_ probability measures, we say that \(_{d}_{2}}}{{}}\) if the same convergence holds in probability, i.e., \(_{X_{d}}g(X)}{{}}_{X }g(X)\) for all \(g(2)\). The empirical distribution of a vector \(^{d}\) is defined as \(_{i=1}^{d}(z_{i})\), where \((z_{i})\) is the Dirac delta distribution centered at \(z_{i}\).

**Formulation:** We consider a batched noisy linear model where, at each time \(t=0,1,,T\), a user has access to an independent batch of data \((^{(t)},^{(t)})^{n d}^{n}\) satisfying

\[^{(t)}=}^{(t)}^{*}+^{(t)}.\]

Above, \(^{*}^{d}\) is an unknown signal, \(^{(t)}\) has i.i.d. standard Gaussian entries, and \(^{(t)}(,^{2}_{n})\) is i.i.d. noise in the measurements. Given an initial weight vector \(^{(0)}^{d}\), we are interested in the behavior of iterative algorithms of the form

\[^{(t+1)}&=_{ ^{d}}^{(t)}-}^{(t)}( ^{(t)})_{2}^{2}+\|\|_{2}^{2} \\ ^{(t+1)}&=(^{(t+1)},^{(t)}), \] (4)where \(\) is a "reweighting" function that acts entry-wise on \((^{(t)},^{(t)})\) and \(>0\) is a hyperparameter governing the strength of the regularization. We we will study the behavior of the iterates \(^{(t)},^{(t)}\) in the high-dimensional limit where \(n\) and \(d\) both approach infinity with fixed ratio \(=\). Since our primary interest is to reveal the feature learning capabilities of such algorithms when \(^{*}\) is a high-dimensional signal with low-dimensional structure, we will typically focus on the regime where \(>T\), where \(T\) is the number of total iterations. This ensures that the total number of observed samples \(nT\) is smaller than the ambient dimension \(d\).

Before proceeding to our main results, we note that this formulation encompasses a wide variety of classical and modern algorithms (summarized in Table 1):

* **Alternating minimization:** One perspective on this algorithm is to consider it as a way to perform alternating minimization on the non-convex loss function \[L(,)=-}( )_{2}^{2}+\|\|_{2}^{2}+\|\|_{2}^{2}.\] Using the fact that the loss function is symmetric in \(\) and \(\), choosing \((u,v)=u\) recovers the mini-batched alternating minimization algorithm for this loss. In other words, \(\) simply switches the two parameters \(\) and \(\).
* **IRLS algorithms for sparse recovery:** As shown in , classical IRLS reweighting schemes used for sparse recovery and compressed sensing [11; 21] can be reparameterized in the form of (4) \((u,v)=(u^{2}v^{2}+)^{}\), where different choices of \(\) correspond to different \(_{p}\) penalties. In particular, the choice \(p=2-4\) corresponds to the IRLS-p algorithm of .
* **Lin-RFM :** Generalizing the reparameterized IRLS update, the authors of  propose the choice \((u,v)=(u^{2}v^{2})\) for some continuous function \(^{+}\). Here, the quantity \(u^{2}v^{2}\) arises from the average outer product of the gradient of the learned regression function, which was shown empirically in  to correlate with the features learned in the weight matrices of various common neural network architectures.

Our goal is to understand statistical properties of the iterates for different choices of \(\), and in particular how the test error evolves from iteration to iteration. In the following section, we develop an asymptotic characterization of the iterates that can be used to gain insight into these questions for a large class of reweighting functions and problem settings.

## 3 A precise characterization of the iterates

In this section, we provide a precise characterization of the iterates of the algorithm in Equation 4 with i.i.d. Gaussian covariates. First, we introduce and discuss the two main assumptions needed for our main result. The first assumption is concerned with the distribution of the initialization \(_{0}\) and the target signal \(^{*}\):

**Assumption 1**.: _The empirical distribution of the entries of \(^{(0)}\) and \(^{*}\) converges in \(_{2}\) distance to some joint distribution \(_{0}\), i.e., \(_{i=1}^{d}(v_{i}^{(0)},_{i}^{*})_{2}}{}_{0}\). Additionally, \(v_{i}^{(0)} 0\) for all \(i\) and \(^{*}\) has bounded entries almost surely._

Here, the requirement of empirical distribution convergence is easily satisfied by common choices of \(^{(0)}\), including the ones vector and i.i.d. Gaussian entries. For a typical sparse regression setup, we might, for example, consider the \(_{0}\) induced by choosing \(^{(0)}=_{d}\) and letting \(^{*}\) have i.i.d. entries

   Algorithm & Reweighting function \\  Alternating minimization (AM)  & \((u,v)=u\) \\ Reparameterized IRLS [9; 11; 28] & \((u,v)=(u^{2}v^{2}+)^{}\) \\ Linear recursive feature machines (lin-RFM)  & \((u,v)=(u^{2}v^{2})\) \\   

Table 1: Some algorithms taking the form (4)that equal \(0\) with certain probability. The requirement that \(^{*}\) has bounded entries appears to be an artifact of the proof, and is used only in the proof of one technical lemma. In our simulations, we find that our asymptotic predictions often remain accurate when \(^{*}\) has entries from distributions which are not bounded almost surely (e.g., Gaussian entries).

Secondly, we define the set of reweighting functions \(\) for which our result will apply.

**Assumption 2**.: _The reweighting function \(\) satisfies the following:_

1. _If_ \(U,V\) _are random variables such that_ \(U,V 0\) _with probability 1, then_ \((U,V) 0\) _with probability 1._
2. \(\) _is continuous and bounded or_ \(^{2}\) _is pseudo-Lipschitz of order 2._

This family allows us to consider many of the choices of \(\) discussed in the previous section, including \((u,v)=u\) (AM on linear diagonal networks), \((u,v)=\) (lin-RFM and IRLS), \((u,v)=(u^{2}v^{2})\) for bounded \(\) (lin-RFM). We note that this does _not_ include some choices of \(\) which apply more "aggressive" weighting, such as \((u,v)=|uv|\). Nevertheless, we can apply our theoretical predictions for these choices of \(\) after passing the weights through a bounded activation (such as a sigmoid). In Appendix D, we show that our predictions often still show excellent agreement with empirical simulation even when the boundedness assumption is violated.

Our results are stated in terms of the following iteration, for \(t 0\):

\[&_{t+1},_{t+1}=_{ 0}_{  0}\{}{}+(1-)-^{2}+ }_{(V,)_{t}}\![+^{2}}{ V^{2}+}]\}\\ & Q_{t+1}=V(+_{t+1}G_{t}) }{_{t+1}V^{2}+_{t+1}},\\ &_{t+1}=((Q_{t+1},V),),\] (5)

where \(G_{t}}{}(0,1)\). In words, given a probability distribution \(_{t}\) over \(\), \(_{t+1}\) and \(_{t+1}\) are scalars computed as the unique1 solutions to a _deterministic_ optimization problem (this can be solved easily by studying the optimality conditions, as shown in Appendix C). Then, \(Q_{t+1}\) is defined as a random variable that is a function of \((V,)_{t}\) and \(G_{t}(0,1)\). Lastly, \(_{t+1}\) is defined as the joint distribution of \((Q_{t+1},V)\) and \(\).

Given this iteration, we obtain the following result, which is proved in Appendix A:

**Theorem 1**.: _Suppose Assumptions 1 and 2 are satisfied. Then, for any \(t 0\) and any function \(g^{3}\) such that \(g(2)\) or \(g\) is bounded and continuous, we have_

\[_{i=1}^{d}g(u_{i}^{(t+1)},v_{i}^{(t)},_{i}^{*})[g(Q_{t+1},V,)],\]

_where the expectation is over the independent random variables \((V,)_{t}\) and \(G_{t}(0,1)\)._

The limit in this theorem should be interpreted as being the limit as \(n,d\) with their ratio \(=\) held as a constant. Applying the above theorem for each \(t 0\), we can get precise asymptotic predictions for a wide variety of test functions of the iterates. One example of particular interest is the test error, which we measure as the normalized \(_{1}\) distance between \(^{(t+1)}^{(t)}\) and \(^{*}\), corresponding to \(g(u,v,)=|uv-|\) (we provide a proof that this is PL(2) in Proposition 1 in Appendix B). We note that the limiting expectation can be computed via simple Monte Carlo simulation of a _scalar_ random variable.

From a technical standpoint, our result is obtained by applying the Convex Gaussian Min-Max Theorem (CGMT)  to the weighted least-squares objective function in (4). Previous works have obtained a similar distributional characterization for the solution to least-squares with anisotropic covariates (where the "weights" \(\) are the square root of the eigenvalues of the data covariance) . However, while  assume that the eigenvalues are uniformly bounded by constants, this is not a reasonable assumption in our setting, since many common choices of \(\) are not bounded and hence \(^{(t)}\) is not necessarily bounded uniformly for \(t>1\). A second key difference is that we need to obtain a distributional characterization which can be applied recursively for all \(t 0\). The analysis in  assumes convergence of the initialization in \(_{4}\) distance and proves convergence of the estimator in \(_{3}\) distance. However, to apply the result recursively in our setting, if assume that the empirical distribution of \((^{(0)},^{*})\) converges in \(_{k}\) distance, then we need to show that after one iteration, the iterates also converge in \(_{k}\) distance (and not in any weaker sense).

To overcome these differences, we use a different technique to show distributional convergence of the iterates. Similar to the approach in , we apply the CGMT to a _perturbed_ optimization problem, which ultimately allows us to show convergence of test functions of the solutions to the unperturbed problem. While this approach necessitates the additional assumption that \(^{*}\) has bounded entries and we obtain results for a slightly smaller family of test functions \(g\) (note, for example, that the squared loss \(g(u,v,)=(uv-)^{2}\) is not \((2)\)), we obtain a distributional convergence result that can be applied to a _sequence_ of recursively defined least-squares problems which define the trajectory of an algorithm, rather than to a single optimization problem. Moreover, our simulations in Appendix D suggest that the predictions of Theorem 1 still often apply without these additional assumptions, including in the case of the squared loss, indicating that these additional assumptions could potentially be weakened with a more complicated analysis.

### Application to sparse linear regression

In this subsection, we apply Theorem 1 to a sparse recovery setting and compare the asymptotic predictions to numerical simulations on high-dimensional Gaussian data. First, we consider a setting where \(n=250,d=2000\), and \(^{*}\) has \((0.01)\) entries (so the expected sparsity level is \([s]=20\)). We run Algorithm 4 with initialization \(^{(0)}=_{d}\) for four different choices of reweighting function and display the test error at each iteration (median over 100 trials) in Figure 1. For each choice of reweighting function \(\), we choose the regularization parameter \(\) that minimizes the asymptotic test loss achieved within 8 iterations, and we plot the corresponding trajectory. As shown in the figure, the numerical simulations show excellent alignment with the asymptotic predictions even for this moderate choice of \(n\) and \(d\).

The asymptotic predictions show that this family of algorithms can find solutions with low test error within only a few iterations. Our results also reveal fine-grained differences in the convergence

Figure 1: Theoretical predictions and simulations of the test error \(\|-^{*}\|_{1}\) (log scale, pluses denote the median over 100 trials and the shaded region indicates the interquartile range) for two different noise levels, where \(n=250,d=2000\), and \(^{*}\) has \((0.01)\) entries. Here, \(=|uv|^{}\) corresponds to the classical IRLS weighting from , \(=|uv|\) is a version of lin-RFM, \(=u\) corresponds to AM, and \(= u^{2}\) is a new reweighting scheme we introduce. We note that the \(\) which depend only on \(\) can lead to oscillatory behavior in the test risk.

behavior of the different algorithms. For instance, more aggressive weightings \(=|uv|\) and \(= u^{2}\) seem to find better solutions after several iterations. Interestingly, the weighting functions which depend only on \(u\) (like alternating minimization) sometimes display a non-monotonic, oscillatory decay of the test loss, particularly in the low-noise regime. However, we do see a steady decrease in test error after every _pair_ of iterations (e.g., in AM, after both parameters have been updated). Finally, we note that our framework allows for analysis of new algorithms for training LDNNs. In particular, to our knowledge, weighting functions of the form \((u^{2})\) have not been previously considered for this task, but our results indicate that this small modification to AM is competitive with many existing algorithms in this setting.

## 4 Grouped IRLS and the benefits of structured feature learning

In many scenarios, the unknown signal \(^{*}\) is known to possess additional structure that can be leveraged during training. One commonly studied example of this is _structured sparsity_, or group sparsity, where \(^{*}\) has many blocks which are zero. In this section, we generalize the results of Theorem 1 to the case where the reweighting function respects this additional structure in the signal, i.e., \(\) acts on blocks of \(\), rather than on individual coordinates.

Concretely, we consider the following modification to our formulation. Let \(b 1\) be a constant and write \(^{d}\) as a product space over \(M=\) factors: \(^{b}^{b}\). Then, \(^{*},^{(t)},^{(t)}^{d}\) can all be represented as \(M\) stacked blocks, each in \(^{b}\). Under the same linear measurement model, we now let \(:^{b}^{b}^{b}\) act on each of the _factors_ of \((^{(t)},^{*})\), and consider the same Algorithm 4.

Here, the case \(b=1\) recovers the results of the previous section, but the case \(b>1\) allows us to study the interplay between signal structure and reweighting scheme in a more fine-grained way. For example, suppose \(^{*}\) is known to be group-sparse, meaning that many of the factors \(\{^{*}_{i}\}_{i=1}^{M}\) are zero. In this case, it might make sense for \(\) to return a vector of the form

\[(^{(t)}_{i},^{(t)}_{i})=_{i}_{b},\]

for some \(_{i}\) that is chosen as a function \(^{(t)}_{i}\) and \(^{(t)}_{i}\). This corresponds to a reweighting scheme which acts on blocks, rather than individual entries. Another way to motivate this "grouped reweighting" is to leverage the connection to the \(\)-trick, as in (3). In particular, the group Lasso problem can be written in the following variational form :

\[_{^{d}}_{^{M}_{+}}L( )+_{i=1}^{M}_{i} \|_{2}^{2}}{_{i}}+_{i},\] (6)

where the closed-form solution to the \(\) minimization yields the classical group norm regularizer \(_{i=1}^{M}\|_{i}\|_{2}\). Here, reparameterizing as \(_{i}}\) and \(_{i}_{i}}{}}\) gives rise naturally to the grouped Hadamard parameterization, with \(_{i}=_{i}_{b}\).

From the perspective of linear diagonal networks, such an approach is equivalent to "tying" together the weights of the hidden layer that correspond to each block. Rather than studying gradient descent/flow for this parameterization (as in [16; 17]), we consider an optimization approach that relies on alternate updates of \(^{(t)}\) and \(^{(t)}\).

We make the same technical assumptions as in Assumptions 1 and 2, with the natural modifications to accommodate \(b 1\):

1. \(_{0}\) is the limit of the empirical distribution of factors of \((^{(0)},^{*})\) and hence is a distribution over \(^{b}^{b}\).
2. Each factor \(^{*}_{i}^{b}\) for \(i=1,,M\) has bounded \(_{2}\)-norm almost surely.
3. \(\) is bounded and continuous or each of its coordinate projections satisfies \(^{2}_{j}(2)\) for \(j=1,,b\).

A straightforward extension of Theorem 1 yields the following generalization for the grouped algorithm, where \(,,_{t},_{t+1}^{b}\) are now vector-valued random variables: For \(t 0\), let

\[&_{t+1},_{t+1}=_{ 0}_{  0}\{}{}+(1-)-^{2} +}_{(,)_{t}}\![ _{j=1}^{b}^{2}+^{2}}{ V_{j}^{2}+ }]\}\\ &_{t+1}=(+_{t+1} _{t})}{_{t+1}^{ 2}+_{t+1}_{b}},\\ &_{t+1}=((_{t+1},),). \] (7)

Here, \(_{t}}{}(,_{b})\). Then, we have the following result, which is proved in Appendix A.

**Theorem 2**.: _[Generalization of Theorem 1 for \(b 1\)] Under the assumptions above, for any \(t 0\) and any function \(g(^{b})^{3}\) such that \(g(2)\) or \(g\) is bounded and continuous, we have_

\[_{i=1}^{M}g(_{i}^{(t+1)},_{i}^{(t)},_{i }^{*})[g(_{t+1},,)].\]

Given a reweighting function \(\), Theorem 2 characterizes the distribution of the factors (blocks) of the iterates. Hence, by choosing \(g(,,)=|-|\), we can predict the exact limiting test error for this family of algorithms.

Computing these theoretical predictions reveals that choosing \(\) in a group-aware way can lead to significant performance improvements compared to coordinate-wise reweighting. In Figure 2, we fix \(=0.1,n=500,d=4000\), and set the overall expected sparsity level of \(^{*}\) as in Figure 1. We compare the performance of Algorithm 4 for a "group-blind" (\(_{gb}\)) and "group-aware" (\(_{ga}\)) choice of reweighting function:

* \(_{gb}(,)=||\) -- note this is identical to one of the reweightings considered in Section 3.1.
* \(_{}(,)=(_{j=1}^{b}|u_{j}v_{ j}|)_{b}\)

The theoretical predictions align with simulations and show a notable improvement in performance when using the group-aware scheme with \(b>1\). Moreover, as the group size \(b\) increases, the performance of \(_{gb}\) remains approximately the same, indicating that it is not able to take adapt to the

Figure 2: Group-blind (\(_{gb}\)) vs. group-aware (\(_{ga}\)) reweighting when \(^{*}\) has group-sparse structure. We set \(n=500,d=4000,=0.1\), and \(_{i}^{*}}{}(0.01)_{b}\). For each curve, \(\) is set to minimize the asymptotic test error achieved. Simulation results are the median/IQR over 100 trials. Left: Comparison of the test error trajectory (log scale) for a fixed block size \(b=8\). Right: \(_{1}\) test error after \(T=4\) iterations, for varying group sizes.

group-structure. By contrast, using \(_{ga}\) leads to a consistent improvement in test error as \(b\) gets larger. Hence, the test error when using the group-aware scheme scales with the size/number of groups, rather than the overall sparsity level.

## 5 Conclusion

In this paper, we derived a precise asymptotic characterization of the iterates of a family of algorithms for learning high-dimensional linear models with linear diagonal networks. We used these predictions to obtain fine-grained predictions of the test error at each iteration for various existing algorithms for this task, and we showed that our framework can also be used as a test bed for new variations on these algorithms that take a similar form. Lastly, we demonstrated the advantage of embedding more structure into the model by using together groups of weights when the ground-truth has structured sparsity. Several interesting open questions about these types of algorithms remain. While our simulations align very well with the predicted asymptotic trajectory, it would be interesting to obtain finite-sample guarantees that hold even for batch sizes that are much smaller than \(d\) (as in the "mini-batch" case studied by ). Moreover, seeing as our analysis depends crucially on the independence the covariates at every iteration, developing precise predictions of the trajectory in the non-batched setting remains an interesting direction for future work.