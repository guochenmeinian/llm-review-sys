# Learning Variational Temporal Abstraction Embeddings in Option-Induced MDPs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The option framework in hierarchical reinforcement learning has notably advanced the automatic discovery of temporally-extended actions from long-horizon tasks. However, existing methods often struggle with ineffective exploration and unstable updates when learning action and option policies simultaneously. Addressing these challenges, we introduce the Variational Markovian Option Critic (VMOC), an off-policy algorithm with provable convergence that employs variational inference to stabilize updates. VMOC naturally integrates maximum entropy as intrinsic rewards to promote the exploration of diverse and effective options. Furthermore, we adopt low-cost option embeddings instead of traditional, computationally expensive option triples, enhancing scalability and expressiveness. Extensive experiments in challenging Mujoco environments validate VMOC's superior performance over existing on-policy and off-policy methods, demonstrating its effectiveness in learning coherent and diverse option sets suitable for complex tasks.

## 1 Introduction

Recent advancements in deep reinforcement learning (DRL) have demonstrated significant successes across a variety of complex domains, such as mastering the human level of atari  and Go  games. These achievements underscore the potential of combining reinforcement learning (RL) with powerful function approximators like neural networks  to tackle intricate tasks that require nuanced control over extended periods. Despite these breakthroughs, Deep RL still faces substantial challenges, such as insufficient exploration in dynamic environments [18; 13; 42], inefficient learning associated with temporally extended actions [6; 9] and long horizon tasks [30; 4], and vast amounts of samples required for training proficient behaviors [16; 40; 15].

One promising area for addressing these challenges is the utilization of hierarchical reinforcement learning (HRL) [11; 2; 12], a diverse set of strategies that decompose complex tasks into simpler, hierarchical structures for more manageable learning. Among these strategies, the option framework , developed on the Semi-Markov Decision Process (SMDP), is particularly effective at segmenting non-stationary task stages into temporally-extended actions known as options. Options are typically learned through a maximum likelihood approach that aims to maximize the expected rewards across trajectories. In this framework, options act as temporally abstracted actions executed over variable time steps, controlled by a master policy that decides when each option should execute and terminate. This structuring not only simplifies the management of complex environments but also enables the systematic discovery and execution of temporal abstractions over long-horizon tasks [24; 23].

However, the underlying SMDP framework is frequently undermined by three key challenges: 1) Insufficient exploration and degradation [20; 37; 23]. As options are unevenly updated using conventional maximum likelihood methods [4; 10; 45; 25; 26], the policy is quickly saturated with early rewarding observations. This typically results in focusing on only low-entropy options that leadto local optima rewards, causing a single option to either dominate the entire policy or switch every timestep. Such premature convergence limits option diversity significantly. 2) Sample Inefficiency. The semi-Markovian nature inherently leads to sample inefficiency [47; 29]: each policy update at the master level extends over multiple time steps, thus consuming a considerable volume of experience samples with relatively low informational gain. This inefficiency is further exacerbated by the prevalence of on-policy option learning algorithms [4; 52], which require new samples to be collected simultaneously from both high-level master policies and low-level action policies at each gradient step, and thus sample expensive. 3) Computationally expensive. Options are conventionally defined as triples  with intra-option policies and termination functions, often modeled using neural networks which are expensive to optimize. These challenges collectively limit the broader adoption and effectiveness of the option framework in real-world scenarios, particularly in complex continuous environments where scalability and stability are critical [14; 34; 26].

To address these challenges, we introduce the Variational Markovian Option Critic (VMOC), a novel off-policy algorithm that integrates the variational inference framework on option-induced MDPs . We first formulate the optimal option-induced SMDP trajectory as a probabilistic inference problem, presenting a theoretical convergence proof of the variational distribution under the soft policy iteration framework . Similar to prior variational methods , policy entropy terms naturally arise as intrinsic rewards during the inference procedure. As a result, VMOC not only seeks high-reward options but also maximizes entropy across the space, promoting extensive exploration and maintaining high diversity. We implements this inference procedure as an off-policy soft actor critic  algorithm, which allows reusing samples from replay buffer and enhances sample efficiency. Furthermore, to address the computational inefficiencies associated with conventional option triples, we follow  and employ low-cost option embeddings rather than complex neural network models. This not only simplifies the training process but also enhances the expressiveness of the model by allowing the agent to capture a more diverse set of environmental dynamics.

Our contributions can be summarized as follows:

* We propose a variational inference approach within the maximum entropy framework to enhance diverse and robust exploration of options.
* We implement an off-policy algorithm that improves sample efficiency.
* We introduce option embeddings into latent variable policies and enhance expressiveness and computational cost-effectiveness of option representations.
* We conduct extensive experiments in OpenAI Gym Mujoco  environments, demonstrating that VMOC significantly outperforms other option-based variants in terms of exploration capabilities, sample efficiency, and computational efficiency.

## 2 Preliminary

### Control as Structured Variational Inference

Conventionally, the control as inference framework [19; 31; 19; 53] is derived using the maximum entropy objective. In this section, we present an alternative derivation from the perspective of structured variational inference. We demonstrate that this approach provides a more concise and intuitive pathway to the same theoretical results, where the maximum entropy principle naturally emerges through the direct application of variational inference techniques.

Traditional control methods focus on directly maximizing rewards, often resulting in suboptimal tradeoffs between exploration and exploitation. By reinterpreting the control problem as a probabilistic inference problem, the control as inference framework incorporates both the reward structure and environmental uncertainty into decision-making, providing a more robust and flexible approach to policy optimization. In this framework, optimality is represented by a binary random variable \(\{0,1\}\)1. The probability of optimality given a state-action pair \((,)\) is denoted as \(P(=1,)=(r(,))\), which is an exponential function of the conventional reward function \(r(,)\) that measures the desirability of an action in a specific state. Focusing on \(=1\) captures the occurrence of optimal events. For simplicity, we will use \(\) instead of \(=1\) in the following text to avoid cluttered notations. The joint distribution over trajectories \(=(_{1},_{1},,_{T},_{T})\) given optimality is expressed as:

\[P(|_{1:T}) P(,_{1:T})=P(_{1}) _{t=1}^{T-1}P(_{t+1}|_{t},_{t})P(_{t}|_{t},_{t})\]

where \(P(_{1})\) is the initial state distribution, \(P(_{t+1}|_{t},_{t})\) is the dynamics model. As explained in [19; 31], direct optimization of \(P(_{1:T})\) can result in an optimistic policy that assumes a degree of control over the dynamics. One way to correct this risk-seeking behavior  is through structured variational inference. In our case, the goal is to approximate the optimal trajectory \(P()\) with the variational distribution:

\[q()=P(_{1})_{t=1}^{T-1}P(_{t+1}_{t },_{t})q(_{t}_{t})\]

where the initial distribution \(P(_{1})\) and transition distribution \(P(_{t+1}_{t},_{t})\) is set to be the true environment dynamics from \(P()\). The only variational term is the variational policy \(q(_{t}_{t})\), which is used to approximate the optimal policy \(P(_{t}_{t},_{1:T})\). Under this setting, the environment dynamics will be canceled out from the optimization objective between \(P()\) and \(q()\), thus explicitly disallowing the agent to influence its dynamics and correcting the risk-seeking behavior.

With the variational distribution at hand, the conventional maximum entropy framework can be recovered through a direct application of standard structural variational inference :

\[ P(_{1:T}) =(q(),P(,_{1:T}))+D_{}(q () P(|_{1:T}))\] \[=_{ q()}[_{t}^{T}r(_{t},_{t})+(q(|_{t}))]}_{}+D_{}(q(_{t}|_{t}) P( _{t}|_{t},_{1:T}))\]

where \((q,P)=_{q}[]\) is the Evidence Lower Bound (ELBO) . The maximum entropy objective arises naturally as the environment dynamics in \(P(,)\) and \(q()\) cancel out. Under this formulation, the soft policy iteration theorem  has an elegant Expectation-Maximization (EM) algorithm  interpretation: the E-step corresponds to the policy evaluation of the maximum entropy objective \((q^{[k]},P)\); while the M-step corresponds to the policy improvement of the \(D_{}\) term \(q^{[k+1]}=*{arg\,max}_{q}D_{}(q^{[k]}() P ())\). Thus, soft policy iteration is an exact inference if both EM steps can be performed exactly.

**Theorem 1** (Convergence Theorem for Soft Policy Iteration).: _Let \(\) be the latent variable and \(\) be the observed variable. Define the variational distribution \(q()\) and the log-likelihood \( P()\). Let \(M:q^{[k]} q^{[k+1]}\) represent the mapping defined by the EM steps inference update, so that \(q^{[k+1]}=M(q^{[k]})\). The likelihood function increases at each iteration of the variational inference algorithm until convergence conditions are satisfied._

Proof.: See A.1. 

### The Option Framework

In conventional SMDP-based Option Framework , an option is a triple \((_{o},_{o},_{o})\), where \(\) denotes the option set; \(o=\{1,2,,K\}\) is a positive integer index which denotes the \(o\)-th triple where \(K\) is the number of options; \(_{o}\) is an initiation set indicating where the option can be initiated; \(_{o}=P_{o}(|):\) is the action policy of the \(o\)th option; \(_{o}=P_{o}(=1|):\) where \(\{0,1\}\) is a _termination function_. For clarity, we use \(P_{o}(=1|)\) instead of \(_{o}\) which is widely used in previous option literatures (e.g., Sutton et al. , Bacon et al. ). A _master policy_\((|)=P(|)\) where \(\) is used to sample which option will be executed. Therefore, the dynamics (stochastic process) of the option framework is written as:

\[P()=P(_{0},_{0}) _{t=1}^{}P(_{t}|_{t-1},_{ t-1})P_{o_{t}}(_{t}|_{t})\] \[[P_{o_{t-1}}(_{t}=0|_{t})_{_{t}=o_{t-1}}+P_{o_{t-1}}(_{t}=1|_{t})P(_{t}| _{t})],\] (1)where \(=\{_{0},_{0},_{0},_{1},_{1}, _{1},\}\) denotes the trajectory of the option framework. \(\) is an indicator function and is only true when \(_{t}=o_{t-1}\) (notice that \(o_{t-1}\) is the realization at \(_{t-1}\)). Therefore, under this formulation the option framework is defined as a Semi-Markov process since the dependency on an activated option \(o\) can cross a variable amount of time . Due to the nature of SMDP assumption, conventional option framework is unstable and computationally expensive to optimize. Li et al. [34; 35] proposed the Hidden Temporal Markovian Decision Process (HiT-MDP):

\[P()=P(_{0},_{0})_{t=1}^{}P(_{t}| _{t-1},_{t-1})P(_{t}|_{t},_{ t})P(_{t}|_{t},_{t-1})\] (2)

and theoretically proved that the option-induced HiT-MDP is homomorphically equivalent to the conventional SMDP-based option framework. Following RL conventions, we use \(^{A}=P(_{t}|_{t},_{t})\) to denote the action policy and \(^{O}=P(_{t}|_{t},_{t-1})\) to denote the option policy respectively. In HiT-MDPs, options can be viewed as latent variables with a temporal structure \(P(_{t}|_{t},_{t-1})\), enabling options to be represented as dense latent embeddings rather than traditional option triples. They demonstrated that learning options as embeddings on HiT-MDPs offers significant advantages in performance, scalability, and stability by reducing variance. However, their work only derived an on-policy policy gradient algorithm for learning options on HiT-MDPs. In this work, we extend their approach to an off-policy algorithm under the variational inference framework, enhancing exploration and sample efficiency.

## 3 Methodology

In this section, we introduce the Variational Markovian Option Critic (VMOC) algorithm by extending the variational policy iteration (Theorem 1) to the option framework. In Section 3.1, we reformulate the optimal option trajectory and the variational distribution as probabilistic graphical models (PGMs), propose the corresponding variational objective, and present a provable exact inference procedure for these objectives in tabular settings. Section 3.2 extends this result by introducing VMOC, a practical off-policy option learning algorithm that uses neural networks as function approximators and proves the convergence of VMOC under approximate inference settings. Our approach differs from previous works [19; 33; 34] by leveraging structured variational inference directly, providing a more concise pathway to both theoretical results and practical algorithms.

### PGM Formulations of The Option Framework

Formulating complex problems as probabilistic graphical models (PGMs) offers a consistent and flexible framework for deriving principled objectives, analyzing convergence, and devising practical algorithms. In this section, we first formulate the optimal trajectory of the conventional SMDP-based option framework (Eq. 1) as a PGM. We then use the HiT-MDPs as the variational distribution to approximate this optimal trajectory. With these PGMs, we can straightforwardly derive the variational objective, where maximum entropy terms arise naturally. This approach allows us to develop a stable algorithm for learning diversified options and preventing degeneracy. Specifically, we follow [31; 28]

by introducing the concept of "Optimality"  into the conventional SMDP-based option framework (Equation equation 1). This allows us to define the probability of an option trajectory being optimal

Figure 1: PGMs of the option framework.

as a probabilistic graphical model (PGM), as illustrated in Figure 1 (a):

\[P(,^{A}_{1:T},^{O}_{1:T}) =P(_{0},_{0})_{t=1}^{T}P(_{t+1}| _{t},_{t})P(^{A}_{t}=1|_{t},_{t})P(^{O}_{t}=1|_{t},_{t},_{t}, _{t-1})P(_{t})P(_{t})\] \[_{0})_{t=1}^{T}P(_ {t+1}|_{t},_{t})}_{}^{T}P(^{A}_{t}=1|_{t},_{t})P( ^{O}_{t}=1|_{t},_{t},_{t}, _{t-1})}_{},\] (3)

where \(\{0,1\}\) are observable binary "optimal random variables" , \(=\{_{0},_{0},_{0},_{1}\}\) denotes the trajectory of the option framework. The agent is _optimal_ at time step \(t\) when \(P(^{A}_{t}=1|_{t},_{t})\) and \(P(^{O}_{t}=1|_{t},_{t},_{t},_{t-1})\). We will use \(\) instead of \(=1\) in the following text to avoid cluttered notations. To simplify the derivation, priors \(P()\) and \(P()\) can be assumed to be uniform distributions without loss of generality . Note that Eq. 3 shares the same environment dynamics with Eq. 1 and Eq. 2. With the optimal random variables \(^{O}\) and \(^{A}\), the likelihood of a state-action \(\{_{t},_{t}\}\) pair that is optimal is defined as:

\[P(^{A}_{t}|_{t},_{t})=(r(_{t}, _{t})),\] (4)

as this specific design facilitates recovering the value function at the latter structural variational inference stage. Based on the same motivation, the likelihood of an option-state-action \(\{_{t},_{t},_{t},_{t-1}\}\) pair that is optimal is defined as,

\[P(^{O}_{t}|_{t},_{t},_{t}, _{t-1})=(f(_{t},_{t},_{t},_{t-1})),\] (5)

where \(f()\) is an arbitrary non-positive function which measures the preferable of selecting an option given state-action pair \([_{t},_{t}]\) and the previous executed option \(_{t-1}\). In this work, we choose \(f\) to be the mutual-information \(f=I[_{t}|_{t},_{t},_{t-1}]\) as a fact that when the uniform prior assumption of \(P()\) is relaxed the optimization introduces a mutual-information as a regularizer .

As explained in Section 2.1, direct optimization of Eq. 3 results in optimistic policies that assumes a degree of control over the dynamics. We correct this risk-seeking behavior  through approximating the optimal trajectory \(P()\) with the variational distribution:

\[q()=P(_{0},_{0})_{t=1}^{T-1}P(_{t+1}| _{t},_{t})q(_{t}|_{t},_{t}) q(_{t}|_{t},_{t-1})\] (6)

where the initial distribution \(P(_{0},_{0})\) and transition distribution \(P(_{t+1}_{t},_{t})\) is set to be the true environment dynamics from \(P()\). The variational distribution turns out to be the HiT-MDP, where the action policy \(q(_{t}_{t})\) and the option policy \(q(_{t}|_{t},_{t-1})\) are used to approximate the optimal policy \(P(_{t}|_{t},_{t},^{A}_{1:T})\) and \(P(_{t}|_{t},_{t-1},^{O}_{1:T})\). The Evidence Lower Bound (ELBO)  of the log-likelihood optimal trajectory (Eq. 3) can be derived as (see A.3):

\[(q(),P(,^{A}_{1:T},^{O}_ {1:T})) =_{q()}[ P(,^{A}_{1:T},^{O}_ {1:T})- q()]\] \[=_{q()}[r(_{t},_{t})+f()  q(_{t}|_{t},_{t})- q(_{ t}|_{t},_{t-1})]\] \[=_{q()}[r(_{t},_{t})+f( )+[^{A}]+[^{O}]]\] (7)

where line 2 is substituting Eq. 3 and Eq. 6 into the ELBO. As a result, the maximum entropy objective naturally arises in Eq. 7. Optimizing the ELBO not only seeks high-reward options but also maximizes entropy across the space, promoting extensive exploration and maintaining high diversity.

Given the ELBO, we now define soft value functions of the option framework following the Bellman Backup Functions along the trajectory \(q()\) as follow:

\[Q^{soft}_{O}[_{t},_{t}] =f()+_{_{t}^{A}}[Q^{soft}_{A }[_{t},_{t},_{t}]]+H[^{A}],\] (8) \[Q^{soft}_{A}[_{t},_{t},_{t}] =r(s,a)+_{_{t+1} P(_{t+1}| _{t},_{t})}[_{_{t+1}^{O}} [Q^{soft}_{O}[_{t+1},_{t+1}]]+H[^{O}]]\] (9)

Assuming policies \(^{A},^{O}\) where \(\) is an arbitrary feasible set, under a tabular setting where the inference on \(\) can be done exactly, we have the following theorem holds:

**Theorem 2** (Soft Option Policy Iteration Theorem).: _Repeated optimizing \(\) and \(D_{}\) defined in Eq. 10 from any \(^{A}_{0},^{O}_{0}\) converges to optimal policies \(^{A*},^{O*}\) such that \(Q^{soft*}_{O}[_{t},_{t}] Q^{soft}_{O}[_{t}, _{t}]\) and \(Q^{soft*}_{A}[_{t},_{t},_{t}] Q^{soft}_{A}[ _{t},_{t},_{t}]\), for all \(^{A}_{0},^{O}_{0}\) and \((_{t},_{t},_{t}) \), assuming under tabular settings where \(||<,\ ||<,\ ||<\)._

Proof.: See Appendix A.2. 

Theorem 2 guarantees finding the optimal solution only when the inference can be done exactly under tabular settings. However, real-world applications often involve large continuous domains and employ neural networks as function approximators. In these cases, inference procedures can only be done approximately. This necessitate a practical approximation algorithm which we present below.

### Variational Markovian Option Critic Algorithm

Formulating complex problems as probabilistic graphical models (PGMs) allowing us to leverage established methods from PGM literature to address the associated inference and learning challenges in real-world applications. To this end, we utilizes the structured variational inference treatment for optimizing the log-likelihood of optimal trajectory and prove its convergence under approximate inference settings. Specifically, using the variational distribution \(q()\) (Eq. 6) as an approximator, the ELBO can be derived as (see Appendix A.3):

\[(q(),P(,^{A}_{1:T},^{O}_{1:T}))=-D_{ }(q()||P(|^{A}_{1:T},^{O}_{1:T}))+  P(^{A}_{1:T},^{O}_{1:T})\] (10)

where \(D_{}\) is the KL-divergence between the trajectory following variational policies \(q()\) and optimal policies \(P(|^{A}_{1:T},^{O}_{1:T})\). Under the structural variational inference  perspective, convergence to the optimal policy can be achieved by optimizing the ELBO with respect to the the variational policy repeatedly:

**Theorem 3** (Convergence Theorem for Variational Markovian Option Policy Iteration).: _Let \(\) be the latent variable and \(^{A},^{O}\) be the ground-truth optimality variables. Define the variational distribution \(q()\) and the true log-likelihood of optimality \( P(^{A},^{O})\). iterates according to the update rule \(q^{k+1}=*{arg\,max}_{q}(q(),P(,^{A }_{1:T},^{O}_{1:T}))\) converges to the maximum value bounded by the true log-likelihood of optimality._

Proof.: See Appendix A.4. 

We further implements a practical algorithm, the Variational Markovian Option Policy Iteration. Let \(\) be the latent variable and \(^{A},^{O}\) be the ground-truth optimality variables. Define the variational distribution \(q()\) and the true log-likelihood of optimality \( P(^{A},^{O})\). iterates according to the update rule \(q^{k+1}=*{arg\,max}_{q}(q(),P(,^{A }_{1:T},^{O}_{1:T}))\) converges to the maximum value bounded by the true log-likelihood of optimality.

Proof.: See Appendix A.4. 

We further implements a practical algorithm, the Variational Markovian Option Critic (VMOC) algorithm, which is suitable for complex continuous domains. Specifically, we employ parameterized neural networks as function approximators for both the Q-functions (\(Q^{soft}_{^{A}}\), \(Q^{soft}_{^{A}}\)) and the policies (\(_{^{A}}\), \(_{^{O}}\)). Instead of running evaluation and improvement to full convergence using Theorem 2, we can optimize the variational distribution by taking stochastic gradient descent following Theorem 3 with respect to the ELBO (Eq. 7) directly. Share the same motivation with Haarnoja et al.  of reducing the variance during the optimization procedure, we derive an option critic framework by optimizing the maximum entropy objectives between the action Eq. 9 and the option Eq. 8 alternatively. The Bellman residual for the action critic is:

\[J_{Q^{A}}(^{A}_{i})=_{(_{t},_{t}, _{t},_{t+1}) D}_{i=1,2}Q_{^{A} _{i}}(_{t},_{t},_{t})-\] \[r(_{t},_{t})+_{_{t+ 1}^{O}}[Q^{soft}_{O}[_{t+1},_{t+1}]]+ ^{O}H[^{O}]^{2}\]

where \(^{O}\) is the temperature hyper-parameter and the expectation over option random variable \(_{_{t+1}^{O}}\) can be evaluated exactly since \(^{O}\) is a discrete distribution. The Bellman residual for the option critic is:

\[J_{Q^{O}}(^{O}_{i})=_{(_{t},_{t}) D} _{i=1,2}Q^{O}_{^{O}_{i}}(_{t},_{t} )-\] \[f()+_{_{t}^{A}}[Q^{ soft}_{A}[_{t},_{t},_{t}]-^{A} q( _{t}|_{t},_{t})]^{2}\]\(^{A}\) is the temperature hyper-parameter. Unlike \(_{_{t+1^{O}}}\) can be trivially evaluated, evaluating \(_{_{t}^{A}}\) is typically intractable. Therefore, in implementation we use \(_{t}\) sampled from the replay buffer to estimate the expectation over \(^{A}\).

Following Theorem3, the policy gradients can be derived by directly taking gradient with respect to the ELBOs defined for the action Eq.9 and the option Eq.8 policies respectively. The action policy objective is given by:

\[J_{^{A}}(^{A})=-_{(_{t},_{t}) D} [_{i=1,2}Q_{_{i}^{A}}(_{t},_{t},}_{t})-^{A} q(}_{t}|_{t}, _{t})],\ }_{t} q(|_{t},_{t})\]

where in practice the action policy is often sampled by using the re-parameterization trick introduced in . The option objective is given by:

\[J_{^{O}}(^{O})=-_{(_{t},_{t-1}) D }[_{i=1,2}Q_{_{i}^{O}}(_{t},_{t})+^{O }[^{O}]]\]

The variational distribution \(q()\) defined in Eq.6 allows us to learn options as embeddings [34; 35] with a learnable embedding matrix \(^{}\). Under this setting, the embedding matrix \(\) can be absorbed into the parameter vector \(^{O}\). This integration into VMOC ensures that options are represented as embeddings without any additional complications, thereby enhancing the expressiveness and scalability of the model.

The temperature hyper-parameters can also be adjusted by minimizing the following objective:

\[J(^{A})=-_{}_{t}^{A}}[^{A }(^{A}(}_{t}_{t},_{t})+ })]\]

for the action policy temperature \(^{A}\), where \(}\) is a target entropy. Similarly, the option policy temperature \(^{O}\) can be adjusted by:

\[J(^{O})=-_{_{t}^{O}}[^{O}( ^{O}(_{t}_{t},_{t-1})+})]\]

where \(}\) is also a target entropy for the option policy. In both cases, the temperatures \(^{A}\) and \(^{O}\) are updated using gradient descent, ensuring that the entropy regularization terms dynamically adapt to maintain a desired level of exploration. This approach aligns with the methodology proposed in SAC . By adjusting the temperature parameters, the VMOC algorithm ensures a balanced trade-off between exploration and exploitation, which is crucial for achieving optimal performance in complex continuous control tasks. We summarize the VMOC algorithm in AppendixB.

## 4 Experiments

In this section, we design experiments on the challenging single task OpenAI Gym MuJoCo  environments (10 environments) to test Variational Markovian Option Critic (VMOC)'s performance over other option variants and non-option baselines.

For VMOC in all environments, we fix the temperature rate for both \(^{O}\) and \(^{A}\) to \(0.05\); we add an exploration noise \((=0,=0.2)\) during exploration. For all baselines, we follow DAC 's open source implementations and compare our algorithm with six baselines, five of which are option variants, _i.e._, MOPG , DAC+PPO, AHP+PPO , IOPG , PPOC , OC  and PPO . All baselines' parameters used by DAC remain unchanged over 1 million environment steps to converge. Figures are plotted following DAC's style: curves are averaged over 10 independent runs and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations. All experiments are run on an Intel(r) Core(tm) i9-9900X CPU @ 3.50GHz with a single thread and process. Our implementation details are summarized in AppendixC. For a fair comparison, we follow option literature conventions and use four options in all implementations. Our code is available in supplemental materials.

## 5 Experiments

We evaluate the performance of VMOC against six option-based baselines (MOPG , DAC+PPO , AHP+PPO , IOPG , PPOC , and OC ) as well as the hierarchy-freePPO algorithm . Previous studies [27; 45; 20; 52] have suggested that option-based algorithms do not exhibit significant advantages over hierarchy-free algorithms in single-task environments. Nonetheless, our results demonstrate that VMOC significantly outperforms all baselines in terms of episodic return, convergence speed, step variance, and variance across 10 runs, as illustrated in Figure 2. The only exception is the relatively simple InvertedDoublePendulum environment, which

Notably, VMOC exhibits superior performance on the Humanoid-v2 and HumanoidStandup-v2 environments. These environments are characterized by a large state space (\(^{376}\)) and action space (\(^{17}\)), whereas other environments typically have state dimensions less than 20 and action dimensions less than 5. The enhanced performance of VMOC in these environments can be attributed to its maximum entropy capability: in large state-action spaces, the agent must maximize rewards while exploring a diverse set of state-action pairs. Maximum likelihood methods tend to quickly saturate with early rewarding observations, leading to the selection of low-entropy options that converge to local optima.

A particularly relevant comparison is with the Markovian Option Policy Gradient (MOPG) , as both VMOC and MOPG are developed based on HiT-MDPs and employ option embeddings. Despite being derived under the maximum entropy framework, MOPG utilizes an on-policy gradient descent approach. Our experimental results show that VMOC's performance surpasses that of MOPG, highlighting the limitations of on-policy methods, which suffer from shortsighted rollout lengths and quickly saturate to early high-reward observations. In contrast, VMOC's variational off-policy approach effectively utilizes the maximum entropy framework by ensuring better exploration and stability across the learning process. Additionally, the off-policy nature of VMOC allows it to reuse samples from a replay buffer, enhancing sample efficiency and promoting greater diversity in the learned policies. This capability leads to more robust learning, as the algorithm can leverage a broader range of experiences to improve policy optimization.

## 6 Related Work

The VMOC incorporates three key ingredients: the option framework, a structural variational inference based off-policy algorithm and latent variable policies. We review prior works that draw

Figure 2: Experiments on Mujoco Environments. Curves are averaged over 10 independent runs with different random seeds and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations.

on some of these ideas in this section. The options framework  offers a promising approach for discovering and reusing temporal abstractions, with options representing temporally abstract skills. Conventional option frameworks , typically developed under the maximum likelihood (MLE) framework with few constraints on options behavior, often suffer from the option degradation problem . This problem occurs when options quickly saturate with early rewarding observations, causing a single option to dominate the entire policy, or when options switch every timestep, maximizing policy at the expense of skill reuse across tasks. On-policy option learning algorithms  aim to maximize expected return by adjusting policy parameters to increase the likelihood of high-reward option trajectories, which often leads to focusing on low-entropy options. Several techniques  have been proposed to enhance on-policy algorithms with entropy-like extrinsic rewards as regularizers, but these often result in biased optimal trajectories. In contrast, the maximum entropy term in VMOC arises naturally within the variational framework and provably converges to the optimal trajectory.

Although several off-policy option learning algorithms have been proposed , these typically focus on improving sample efficiency by leveraging the control as inference framework. Recent works  aim to enhance sample efficiency by inferring and marginalizing over options, allowing all options to be learned simultaneously. Wulfmeier et al.  propose off-policy learning of all options across every experience in hindsight, further boosting sample efficiency. However, these approaches generally lack constraints on options behavior. A closely related work  also derives a variational approach under the option framework; however, it is based on probabilistic graphical model that we believe are incorrect, potentially leading to convergence issues. Additionally, our algorithm enables learning options as latent embeddings, a feature not present in their approach.

Recently, several studies have extended the maximum entropy reinforcement learning framework to discover skills by incorporating additional latent variables. One class of methods  maintains latent variables constant over the duration of an episode, providing a time-correlated exploration signal. Other works  focus on discovering multi-level action abstractions that are suitable for repurposing by promoting skill distinguishability, but they do not incorporate temporal abstractions. Studies such as  aim to discover temporally abstract skills essential for exploration, but they predefine their temporal resolution. In contrast, VMOC learns temporal abstractions as embeddings in an end-to-end data-driven approach with minimal prior knowledge encoded in the framework.

## 7 Conclusion

In this paper, we have introduced the Variational Markovian Option Critic (VMOC), a novel off-policy algorithm designed to address the challenges of ineffective exploration, sample inefficiency, and computational complexity inherent in the conventional option framework for hierarchical reinforcement learning. By integrating a variational inference framework, VMOC leverages maximum entropy as intrinsic rewards to promote the discovery of diverse and effective options. Additionally, by employing low-cost option embeddings instead of traditional, computationally expensive option triples, VMOC enhances both scalability and expressiveness. Extensive experiments in challenging Mujoco environments demonstrate that VMOC significantly outperforms existing on-policy and off-policy option variants, validating its effectiveness in learning coherent and diverse option sets suitable for complex tasks. This work advances the field of hierarchical reinforcement learning by providing a robust, scalable, and efficient method for learning temporally extended actions.

## 8 Limitations

Due to limited computing resources, we did not conduct an ablation study of VMOC. Additionally, the temperature parameter was fixed in our experiments, whereas an automatically tuned parameter could potentially enhance performance (see SAC ). While our baselines focus on option variants, a thorough comparison to other off-policy algorithms is also worth investigating. It is particularly important to explore whether VMOC exhibits performance improvements in scalability when the number of option embeddings is significantly increased. These investigations are left for future work.