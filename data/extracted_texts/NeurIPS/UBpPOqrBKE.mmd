# Federated Graph Learning for Cross-Domain Recommendation

Ziqi Yang\({}^{1,2}\), Zhaopeng Peng\({}^{1,2}\), Zihui Wang\({}^{1,2}\), Jianzhong Qi\({}^{3}\), Chaochao Chen\({}^{4}\),

Weike Pan\({}^{5}\), Chenglu Wen\({}^{1,2}\), Cheng Wang\({}^{1,2}\), Xiaoliang Fan\({}^{1,2}\)

\({}^{1}\)Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University, China

\({}^{2}\)Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University, China

\({}^{3}\)School of Computing and Information Systems, The University of Melbourne, Australia

\({}^{4}\)College of Computer Science and Technology, Zhejiang University Hangzhou, China

\({}^{5}\)College of Computer Science and Software Engineering, Shenzhen University Shenzhen, China

{yangziqi,pengzhaopeng,wangziwei}@stu.xmu.edu.cn

{clwen,cwang,fanxiaoliang}@xmu.edu.cn

jianzhong.qi@unimelb.edu.au, zjuccc@zju.edu.cn, panweike@szu.edu.cn

The corresponding author.

###### Abstract

Cross-domain recommendation (CDR) offers a promising solution to the data sparsity problem by enabling knowledge transfer between source and target domains. However, many recent CDR models overlook crucial issues such as privacy as well as the risk of negative transfer (which negatively impact model performance), especially in multi-domain settings. To address these challenges, we propose FedGCDR, a novel federated graph learning framework that securely and effectively leverages positive knowledge from multiple source domains. First, we design a positive knowledge transfer module that ensures privacy during inter-domain knowledge transmission. This module employs differential privacy-based knowledge extraction combined with a feature mapping mechanism, transforming source domain embeddings from federated graph attention networks into reliable domain knowledge. Second, we design a knowledge activation module to filter out potential harmful or conflicting knowledge from source domains, addressing the issues of negative transfer. This module enhances target domain training by expanding the graph of the target domain to generate reliable domain attentions and fine-tunes the target model for improved negative knowledge filtering and more accurate predictions. We conduct extensive experiments on 16 popular domains of the Amazon dataset, demonstrating that FedGCDR significantly outperforms state-of-the-art methods. We open source the code at https://github.com/LafinfHana/FedGCDR.

## 1 Introduction

Cross-domain recommendation (CDR) has emerged as an effective solution for mitigating data sparsity in recommender systems [1; 2; 3; 4; 5]. CDR operates by integrating auxiliary information from source domains, thereby enhancing recommendation relevance in the target domain. Recently, to address data privacy constraints, many privacy-preserving CDR frameworks have been proposed [6; 7; 8; 9], which achieve strong performance under the assumptions of **data sparsity and a dual-domain model (i.e., typically involving a single source domain and a single target domain)**.

In this paper, we focus on a more generic scenario of **Broader-Source Cross-Domain Recommendation (BS-CDR)**, which integrates knowledge from more than two source domains whilepreserving privacy. Given the diverse nature of user preferences, it is essential to gain a more holistic understanding of user interests by incorporating user behaviors from diversified domains [10; 11]. For example, in Figure 0(a), a user who enjoys certain types of books might also enjoy movies, toys, and games in similar genres. However, incorporating more domains while preserving privacy poses challenges to counteract negative transfer (NT), which is a phenomenon of transferring knowledge from a source domain that negatively impacts the performance of the recommender model in the target domain . Suppose that the Books domain in Figure 0(a) is the target domain. The Clothing domain causes NT because of the domain discrepancy. While the Music domain is supposed to transfer positive knowledge, it might also lead to NT because of lossy privacy-preserving techniques applied to broader source domains. As a result, the influx of negative knowledge accumulated from the source domains will poison the model performance of the target domain in BS-CDR scenarios.

To mitigate the NT issue, attention mechanisms have been widely leveraged, either in an explicit (e.g., determine domain attentions by predefined domain features [13; 14]) or implicit manner (e.g., employ hyper-parameters [7; 15]). Several other studies [16; 17; 18], ensure positive transfer by passing only domain-shared features. However, existing methods cannot be directly applied to BS-CDR due to two major challenges. **First**, inadequate privacy preservation (**CH1**). Both intra-domain and inter-domain privacy must be carefully considered in BS-CDR. As depicted in Figure 0(a), BS-CDR relies on extensive knowledge transfer, risking simultaneous privacy leakages across broader source domains (inter-domain privacy) [9; 19; 20; 21]. Additionally, concerns over centralized data storage may prevent users from sharing sensitive rating data (intra-domain privacy). **Second**, accumulative negative transfer (**CH2**). Adjusting attention-related hyper-parameters for a large number of source domains in BS-CDR scenarios is extremely difficult, as well as predefined or domain-shared features cannot accommodate complex domain diversities. In addition, the use of various lossy privacy-preserving techniques can further degrade the quality of the transferred knowledge, complicating the achievement of positive transfer. Consequently, the impact of NT can inevitably intensify with an increasing number of source domains  and the performance of CDR models can decline to levels lower than those of single-domain models, as shown in Figure 0(b).

To address the challenges of privacy (CH1) and NT (CH2) in BS-CDR, we propose Federated Graph learning for Cross-Domain Recommendation (FedGCDR). It follows a **horizontal-vertical-horizontal pipeline** and consists of **two key modules**. First, the positive knowledge transfer module aims to safeguard inter-domain privacy and mitigate potential NT before transfer. This module adopts differential privacy (DP)  with a theoretical guarantee and aligns the feature spaces to facilitate positive knowledge transfer. Second, the positive knowledge activation module is engaged to further alleviate NT. Specifically, it expands the local graph of the target domain by incorporating virtual social links, enabling the generation of domain attentions. Additionally, it

Figure 1: (a) In order to obtain accurate recommendations in the Books domain, we aim to exploit user preferences (i.e., knowledge of external domains should be fully utilized, e.g. Movie, Toys, and Games domains). However, with the influence of lossy privacy-preserving techniques, the results of the transfer could be negative (e.g., the Music domain with low-quality data). (b) There is a diminishing marginal effect on the growth rate of the model performance with pure positive knowledge, while NT accumulates with an increasing number of source domains. Consequently, the performance of existing methods declines and is worse than that of a single domain model.

performs target model fine-tuning to optimize the broader-source CDR. Extensive experiments on 16 popular domains of the Amazon benchmarks demonstrate that FedGCDR outperforms all baseline methods in terms of recommendation accuracy.

Our contributions are summarized as follows:

* We introduce FedGCDR, a novel federated graph learning framework for CDR that provides high-quality BS-CDR recommendations while safeguarding both user privacy and domain confidentiality;
* We propose two key model, i.e., the positive knowledge transfer module and the positive knowledge activation module. The first transfer module ensure privacy and positive knowledge flows via privacy-preserving knowledge extraction and feature mapping. The second activation module filter harmful information via graph expansion, target domain training and target model fine-tuning;
* We conduct extensive experiments on 16 domains of the Amazon datasets that confirm the effectiveness of FedGCDR in terms of recommendation accuracy.

## 2 Related work

### Cross-domain recommendation

CDR utilizes auxiliary information from external domains to alleviate the data sparsity problem and effectively improve recommendation quality. Li et al.  enrich domain knowledge by transferring user-item rating patterns from source domains to target domains. Man et al.  and Elkahky et al.  augment entities' embeddings in the target domain by employing a linear or multilayer perceptron (MLP)-based nonlinear mapping function across domains. Liu et al.  address the review-based non-overlapped recommendation problem by attribution alignment. Zhao et al.  improve the recommendation quality of multi-sparse-domains by mining domain-invariant preferences. Liu et al.  achieve knowledge transfer without overlapping users by mining joint preferences. Chen et al.  and Liu et al.  avoid intermediate result privacy leakage during cross-domain knowledge transfer by employing DP. In these works, the NT problem is often ignored because most of them assume a carefully selected dual-domain scenarios or limited multi-domain scenarios where NT is not evident. We aim to solve the NT problem in complex BS-CDR scenarios.

### Federated recommendation

Recently, federated learning (FL) [28; 29; 30; 31; 32] has been widely adopted to tackle the privacy issue in recommender system. Chai et al.  adopt FL to classic matrix factorization algorithm and utilize homomorphic encryption to avoid the potential threat of privacy disclosure. Later, Wu et al.  explore the application of federated graph neural networks (GNN) models to improve the recommendation quality and ensure user privacy. To utilize sensitive social information, Liu et al.  adopt local differential privacy (LDP) and negative sampling. More recent studies use vertical federated learning (VFL) to protect company's privacy in recommender system. Mai et al.  utilize random projection and ternary quantization to ensure privacy preservation in VFL. In CDR, Chen et al.  design a dual-target VFL CDR model with orthogonal mapping matrix and LDP for organizations' privacy preservation. Liu et al.  design a graph convolutional networks (GCN)-based federated framework to learn user preference distributions for more accurate recommendations. To ensure user privacy in CDR, Liu et al.  utilize a VAE-based federated model to mine user preference with data stored locally. Wu et al.  design a personal module and a transfer module to provide personalized recommendation while preserving user privacy. These existing works, especially federated CDR frameworks, consider only one type of privacy (intra- or inter-domain). We aim to provide both intra-domain and inter-domain privacy.

## 3 Methodology

### Problem definition

We consider \(M\) (\(M\)?3) domains participating in the CDR process. The domains are divided into M-1 source domains \(^{S_{1}},^{S_{2}},...,^{S_{M-1}}\) and one target domain \(^{T}\). Each domain is assigned a domain server to conduct intra-domain model training. \(\) is the user set across all the domains, \(=_{1}_{2}... _{M}\), where \(_{i}\) denotes the user set of domain \(i\). We assume that users partially overlap between domains. Each user is treated as an individual client. User space refers to the virtual space in the user's device containing domain models distributed from each domain server. Meanwhile, \(_{i}\) is the item set of domain \(i\). Let \(^{i}^{|_{i}||_{i}|}\) be the observed rating matrix of the \(i\)-th domain. We consider top-K recommendation, i.e., we learn a function to estimate the scores of unobserved entries in the rating matrix, which are later used for item ranking and recommendations. Our goal is to achieve highly accurate recommendations in the target domain.

### Framework of FedGCDR

#### 3.2.1 Overview

The overall framework of FedGCDR is shown in Figure 2. FedGCDR follows a Horizontal-Vertical-Horizontal (HVH) pipeline, and its two horizontal FL stages ensure the intra-domain privacy. Our two key modules focus on the vertical stage and the second horizontal stage: (1) The positive knowledge transfer module preserves the inter-domain privacy by DP and alleviates NT by feature mapping. (2) The positive knowledge activation module filters out potential harmful or conflicting knowledge from the source domains. Specifically, we expand the local graph of the target domain by virtual social links, such that the target domain graph attention network (GAT) model could generate reliable domain attention based on the expanded graph. After target domain GAT model training, we further mitigate NT by adopting a fine-tuning stage.

Horizontal-Vertical-Horizontal pipelineThe HVH pipeline contains three stages with switching federated settings. The first horizontal stage refers to the source domain training in which source domain servers individually interact with its domain users (clients). The private rating information is stored within each client, while the clients exchange model and gradients to train a domain-specific global model. The next two stages correspond to our two key modules (vertical positive knowledge transfer module and horizontal positive knowledge activation module), which we will cover in detail in the following subsections. It's important to note that the vertical positive knowledge transfer

Figure 2: An overview of FedGCDR. It consists of two key modules and follows a HVH pipeline: (1) Source Domain Training (Horizontal FL): 1 Each source domain maintains its graph attention network (GAT)-based federated model. (2) Positive Knowledge Transfer Module (Vertical FL): 2 Source domain embeddings are extracted from GAT layers and perturbed with Gaussian noise. 3 The multilayer perceptron aligns the feature space of source domain embeddings and target domain embeddings. (3) Positive Knowledge Activation Module (Horizontal FL): 4 Local graph is expanded with source domain embeddings. 5 Enhanced federated training of the target domain is achieved through the expanded graph. 6 The target domain maintains its GAT-based federated model. 7 The target domain freezes the GAT layer and fine tunes the model.

module is completely computed in each client's user space (their personal devices), thus reducing communication overheads. This is because the needed source domain knowledge can be extracted from local source models on each client which are distributed during the first horizontal source domain training stage.

Following the HVH pipeline, we achieve: (1) Privacy enhancement. The two horizontal stages can provide intra-domain privacy preservation, while we further ensure inter-domain privacy by applying DP to the vertical stage. In the mean time, servers are not involved in the knowledge transfer process (i.e., the positive knowledge transfer module), making them unaware of user interactions in other source domains. (2) Communication efficiency. Cross-domain knowledge transfer does not require additional communication overhead.

Intra-domain GAT-based federated modelWe adopt a GAT-based [37; 38; 39] federated framework as the underlying model for our intra-domain recommender system. The horizontal paradigm avoids centralized storage of user ratings to ensure intra-domain privacy (**CH1**). In the initial step, each user and item is offered an ID embedding of size \(d\), denoted by \(_{u}^{0}\), \(_{v}^{0}^{d}\) respectively. The embedding is passed through \(L\) message propagation layers [40; 41; 42]. For the \(l\)-th layer:

\[_{u}^{l+1}=(^{l}(a_{uu}^{l}_{u}^{l}+_{ v N_{u}}a_{uv}^{l}_{v}^{l})),\] (1)

where \(N_{u}\) is the neighbor set of \(u\), \(^{l}\) is a learnable weight matrix, \(a_{uu}^{l}\) and \(a_{uv}^{l}\) are the importance coefficients computed by the attention mechanism:

\[a_{uv}^{l}=_{u}^{l}||_{v}^{l}))}{_{v^{} N_{u} u}exp(LeakyReLU([ _{u}^{l}||_{v^{}}^{l}])},\] (2)

where \(\) is the weight vector. Inspired by LightGCN , we discard feature transformation and nonlinear activation for better model efficiency and learning effectiveness:

\[e_{u}^{l+1}=a_{uu}^{l}_{u}^{l}+_{v N_{u}}a_{uv}^{l}_{v}^{l},\] (3)

\[a_{uv}=_{u}^{l}||_{v}^{l}))}{_{v^{ } N_{u} u}exp((_{u}^{l}||_{v^{ }}^{l})}.\] (4)

In each source domain, the domain server and corresponding users collaboratively train a GAT-based federated model. The training process follows the horizontal federated learning (HFL) paradigm in which only the model and gradients are exchanged considering intra-domain privacy. We will not detail the horizontal federation model (e.g., further privacy guarantee and more high-order information) as it is a well established FL model and not our novel contribution. This model can be replaced by other GAT-based FL models [34; 44] as well.

#### 3.2.2 Positive knowledge transfer module

After the source domain training, we obtain a series of source models in individual client's user space. Our positive knowledge transfer module then prepares positive knowledge to be transferred from each source domains \(^{S}\) to the target domain \(^{T}\), while protecting inter-domain privacy (**CH1**). Specifically, suppose an individual user (client) \(u\) and a source domain \(^{S_{i}}\), we transfer the user \(u\)'s embedding matrix \(_{S_{i}}^{L d}\). Take the row \(l\) of the matrix (i.e., \(_{S_{i}}^{l}\)) as an example, it is the user \(u\)'s embedding output by the \(l\)-th message propagation layer (\(e_{u}^{l}\)). In an ideal scenario (i.e., we transfer totally positive knowledge without taking inter-domain privacy into account) , embedding matrices from different source domains can be directly used to enhance target domain local training in client \(u\). By utilizing the source domain embeddings, \(u\)'s final target domain embedding \(_{T}^{l}\) of layer \(l\) is:

\[_{T}^{l}=f_{T}(_{T}^{l},_{S_{1}}^{l},..., _{S_{M-1}}^{l}), l[1,L]\] (5)

where \(f_{T}()\) is the function that the target domain aggregates the knowledge of the source domains and we will give its final expression in Subsection 3.2.3. In this process, the transfer of knowledge between domains takes place entirely in the user \(u\)'s local space. Such a fully localized mode of knowledge transfer avoids additional communication overhead and potential privacy issues . However, this direct embeddings transfer does not meet the privacy and NT constraints in BS-CDR scenarios.

Privacy-preserving knowledge extractionIn existing CDR frameworks, the user or item embedding was shared as knowledge [9; 15; 6], which neglects inter-domain privacy. In a GNN-based approach, such direct transfers are subject to privacy attacks. Each message propagation layer can be viewed as a function with user and item embeddings as input. An attacker can easily obtain the user's private rating matrix based on these embeddings. We apply DP to the source domain embeddings \(_{S_{i}}\)[22; 45] to safeguard inter-domain privacy.

theorem 1.: _By introducing Gaussian noise into the source domain embeddings, the reconstructed data from the ideal attack deviates from the actual data, therefore preventing a perfect reconstruction._

In FedGCDR, we incorporate the Gaussian mechanism with the source domain embeddings \(_{S_{i}}\) to obtain \(}_{S_{i}}\) for knowledge transfer. Detailed privacy analysis is included in Appendix A.

Feature mappingUser features could represent personal preferences and are influenced by domain features. The discrepancy of domains leads to the heterogeneity of feature space between domains which means that source domain embeddings cannot be utilized directly by the target domain. Man et al.  show that there exists an underlying mapping relationship between the latent user matrix of different domains, which can be captured by a mapping function. In order to alleviate NT, we adopt a series of MLP to explore mapping functions for each source domain. Adding Gaussian noise and feature mapping, Equation (5) becomes:

\[_{T}^{l}=f_{T}(_{T}^{l},MLP_{1}(}_{S_{1}}^ {l}),...,MLP_{M-1}(}_{S_{M-1}}^{l})).\] (6)

To learn more effective mapping function, we adopt a mapping loss term:

\[l_{m}=_{i=1}^{M-1}_{l=1}^{L}||_{T}^{l}-MLP_{i}(}_{S_{i}}^{l})||^{2},\] (7)

#### 3.2.3 Positive knowledge activation module

After the aforementioned operations, the target domain obtains a list of source domain matrices \(}_{S_{1}},}_{S_{2}},...,}_{S_{M-1}}\). The rows of the matrices represent \(MLP_{i}(}_{S_{i}}^{l})\). It is worth noting that for source domains where a user has no rating, \(}_{S_{i}}\) is a Gaussian noise matrix and our motivation is: (1) no rating may also suggest a preference; (2) this is beneficial for enhancing the model's capability to filter noise and identify NT. With knowledge from the source domains, the purpose of the positive knowledge activation module is to alleviate NT following the knowledge transfer. (**CH2**). Although we have aligned the feature space in the previous module, the Gaussian noise that has been fed to the target domain with source domain embedding matrices leads to potential NT. How to utilize the transferred knowledge remains a great challenge.

Graph expansion and target domain trainingTo alleviate NT, common approaches are to generate domain attention by predefined domain features [13; 46; 47] or to control the transfer ratio of source domains by Writefull . These methods are only applicable to a limited number of domains and have excessive human intervention. In FedGCDR, we take an attention-based approach. First, we expand \(u\)'s (Mary's) local graph of the target domain as shown in Figure 3.

Figure 3: Illustration of target domain graph expansion. The virtual users are constructed with the source domain embeddings from the Movie domain and the Music domain. The attentions generated by social links to the virtual user can be regarded as the domain attentions.

For the source domain embedding matrices \(}_{S_{1}},}_{S_{2}},...,}_{S_{M-1}}\), we represent them as \(M-1\) virtual users. Since the virtual users constructed from source domain embeddings represent the same individual \(u\), they share correlated preferences, with their features (i.e., embeddings) characterizing \(u\)'s preferences. Inspired by social recommendation [48; 49; 50], we consider that there is an implicit social relationship between virtual users and the actual user \(u\), because of the correlation in their preferences. Then, we build virtual social links between them to expand the original target domain graph. Second, by incorporating this expanded graph into target domain training, the GAT model generates corresponding attention coefficients for the virtual users, which can be interpreted as domain-specific attentions. Leveraging the domain attention coefficients, the target domain can focus on domains that transfer positive knowledge and we can finally give \(f_{T}()\):

\[f_{T}(x_{T}^{l},MLP_{1}(}_{S_{1}}^{l}),...,MLP_{M-1}(}_{S_{M-1}}^{l}))=a_{uu}^{l}_{T}^{l}+_{v N_{u}}a_{ uv}^{l}_{v}^{l}+_{i=1}^{M-1}a_{i}^{l}MLP_{i}(}_{S_{i}}^{l }),\] (8)

where \(a_{i}^{l}\) is the domain attention of source domain \(i\) generated by the \(l\)-th layer. Beside, we introduce a social regularization term to strengthen the virtual social links:

\[l_{s}=_{l=1}^{L}_{T}^{l}-^{M-1}Sim( _{T}^{l},}_{S_{i}}^{l})}_{S_{i} }^{l}}{_{i=1}^{M-1}Sim(_{T}^{l},}_{S_{i}}^{l})} ^{2},\] (9)

the function \(Sim()\) calculates the cosine similarity .

Through the graph expansion, we achieve: (1) dynamic domain attentions that focus on positive source domain knowledge to alleviate NT; (2) attention generation by GAT, eliminating the need for additional interventions such as hyper-parameter tuning or feature engineering.

For top-\(k\) recommendation, we adopt a widely-used inner product model to estimate the value of target domain rating \(_{uv}^{T}\), which is the interaction probability between a pair of user \(u\) and item \(v\):

\[}_{uv}^{T}=Sigmoid(_{T}^{u}_{T}^{v}),\] (10)

where \(_{T}^{u}\) and \(_{T}^{v}\) are the final user and item embeddings output by GAT. Our objective function consists of three terms as follows:

\[L_{GAT}=BCELoss(}_{uv}^{T},_{uv}^{T})+l_{m}+l_{s},\] (11)

where \(\) and \(\) are Writefull, and \(BCELoss()\) is the binary cross-entropy loss . The target domain federated GAT training with the expanded graph following the HFL paradigm.

Target model fine-tuningAfter target domain training with the expanded graph, the target domain GAT model assimilates knowledge from the source domains. However, NT may still be unavoidable, potentially leading to the accumulation of negative knowledge in the target domain. An example of this is the Gaussian noise matrices transferred from source domains where the user has no interactions. On the basis of this consideration, we adopt an additional fine-tuning stage: First, we freeze the message propagation layers of GAT to isolate the influence of source domains preventing negative information from permeating through the transfer process. Second, we directly train the well-informed embeddings generated by the target domain GAT. Adapting the learned external knowledge through these steps enables more accurate prediction of ratings in the target domain. In this process, we use the loss of prediction in Equation (11) as the object function:

\[L_{ft}=BCELoss(}_{uv}^{T},_{uv}^{T}).\] (12)

We provide a computational analysis and a communication analysis of FedGCDR in Appendix B.

## 4 Experiments

### Experimental setup

DatasetsWe study the effectiveness of FedGCDR with 16 popular domains of a real-world dataset Amazon . To study the impact of the number of domains on model performance, we divide these 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

out mapping. **FedGCDR-M** replaces the attention graph expansion with the average sum of source domain embeddings and omits the fine-tuning stage. We experiment with Books and CDs as target domains on the Amazon-16 dataset. The experimental results are shown in Figure 5. We make the following observations: (1) The two variants perform differently on different target domains. On the Books domain, **FedGCDR-T** performs better than **FedGCDR-M**, which indicates that for domains with higher data quality, preventing the transfer of negative knowledge from other domains is more important than mapping this knowledge better (in other words, the quality of external information holds greater significance than its quantity), and the Positive Knowledge Activation module meets the requirements of such domains. On the CDs domain, **FedGCDR-M** performs better than **FedGCDR-T**, which indicates that for domains that are deficient in information, mapping knowledge correctly is more important than preventing inter-domain negative knowledge (in other words, the quantity of external information holds greater significance than its quality), and the Positive Knowledge Transfer module meets these requirements. (2) Compared to **FedGCDR**, the absence of either module can cause a significant drop in performance. This indicates that in cross-domain recommendation, we should not only focus on transferring positive knowledge, but also control the spread of negative knowledge to the target domain, especially when a large number of domains.

### Dual-domain scenario

According to the experimental results shown in Table 4, our **FedGCDR** achieved the best experimental metrics in both knowledge transfer directions. This shows that our approach is also suitable for dual-domain scenarios where users full-overlap and have only a single source domain and a single target domain.

We provide experimental results on privacy budget in Appendix D.2.

## 5 Limitations

Our experiments were conducted on 16 domains of the Amazon dataset. While this extensive dataset covers broader source domains, relying on a single dataset may limit the generalizability of our model to data from other sources. Our approach uses overlapping users as a cross-domain bridge. Indeed, there are no widely-recognized cross-domain recommendation datasets with more than three domains, aside from the Amazon dataset. Despite this limitation, we believe that the improvements in privacy preservation and model performance demonstrated by FedGCDR underscore its superiority.

## 6 Conclusion

We proposed FedGCDR, a federated graph learning framework designed for BS-CDR. FedGCDR addresses the critical challenge of privacy preservation and negative transfer by employing a positive knowledge transfer module and a positive knowledge activation module. Our method achieves best recommendation quality results on 16 domains of the Amazon dataset. In the future, we aim to extend FedGCDR to improve the recommendation performance of both the target and the source domains.

## 7 Acknowledgment

The research was supported by Natural Science Foundation of China (62272403).

    &  &  \\  & HR@10 & NDCG@10 & HR@10 & NDCG@10 \\  Single Domain & 0.2713 & 0.1429 & 0.2594 & 0.1524 \\  EMCDR & 0.2816 & 0.1409 & 0.2596 & 0.1540 \\ PriCDR & 0.2903 & 0.1446 & 0.2662 & 0.1583 \\ FedCT & 0.2384 & 0.1239 & 0.2570 & 0.1551 \\ FedCDR & 0.2566 & 0.1376 & 0.2657 & 0.1554 \\  FedGCDR-DP & 0.3076 & 0.1552 & 0.2749 & 0.1602 \\ FedGCDR & **0.3323** & **0.1838** & **0.2958** & **0.1797** \\   

Table 4: Dual-domain CDR performance.