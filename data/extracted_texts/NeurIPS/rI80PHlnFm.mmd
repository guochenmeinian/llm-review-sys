# Model-based inference of synaptic plasticity rules

Yash Mehta\({}^{1,2}\)

Danil Tyulmankov\({}^{3,4}\)

Adithya E. Rajagopalan\({}^{1,5}\)

Glenn C. Turner\({}^{1}\)

James E. Fitzgerald\({}^{1,6}\)

Jain Funke\({}^{1}\)

Joint senior authors \({}^{1}\)Janelia Research Campus, Howard Hughes Medical Institute

\({}^{2}\)Department of Cognitive Science, Johns Hopkins University

\({}^{3}\)Center for Theoretical Neuroscience, Columbia University

\({}^{4}\)Viterbi School of Engineering, University of Southern California

\({}^{5}\)Center for Neural Science, New York University

\({}^{6}\)Department of Neurobiology, Northwestern University

###### Abstract

Inferring the synaptic plasticity rules that govern learning in the brain is a key challenge in neuroscience. We present a novel computational method to infer these rules from experimental data, applicable to both neural and behavioral data. Our approach approximates plasticity rules using a parameterized function, employing either truncated Taylor series for theoretical interpretability or multilayer perceptrons. These plasticity parameters are optimized via gradient descent over entire trajectories to align closely with observed neural activity or behavioral learning dynamics. This method can uncover complex rules that induce long nonlinear time dependencies, particularly involving factors like postsynaptic activity and current synaptic weights. We validate our approach through simulations, successfully recovering established rules such as Oja's, as well as more intricate plasticity rules with reward-modulated terms. We assess the robustness of our technique to noise and apply it to behavioral data from _Drosophila_ in a probabilistic reward-learning experiment. Notably, our findings reveal an active forgetting component in reward learning in flies, improving predictive accuracy over previous models. This modeling framework offers a promising new avenue for elucidating the computational principles of synaptic plasticity and learning in the brain.

## 1 Introduction

Synaptic plasticity, the ability of synapses to change their strength, is a key neural mechanism underlying learning and memory in the brain. These synaptic updates are driven by neuronal activity, and they in turn modify the dynamics of neural circuits. Advances in neuroscience have enabled the recording of neuronal activity on an unprecedented scale (Steinmetz et al., 2018; Vanwalleghem et al., 2018; Zhang et al., 2023), and connectome data for various organisms is becoming increasingly available (Winding et al., 2023; Takemura et al., 2023; Hildebrand et al., 2017; Scheffer et al., 2020). However, the inaccessibility of direct large-scale recordings of synaptic dynamics leaves the identification of biological learning rules an open challenge. Existing neuroscience literature (Citri & Malenka, 2008; Morrison et al., 2008) suggests that synaptic changes are functions of local variables such as presynaptic activity, postsynaptic activity, and current synaptic weight, as well as a global reward signal. Uncovering the specific form of this function in different brain circuits promises profound biological insights and holds practical significance for developing more biologically plausible learning algorithms for AI, particularly with neuromorphic implementations (Zenke & Neftci, 2021).

In this paper, we introduce a gradient-based method for inferring synaptic plasticity rules. Our method optimizes parameterized plasticity rules to align with either neural and behavioral data, thereby elucidating the mechanisms governing synaptic changes in biological systems. We utilize interpretable models of plasticity, allowing direct comparisons with existing biological theories and addressing specific questions, such as the role of weight decay in synaptic plasticity or postsynaptic dependence. We validate our approach for recovering plasticity rules using synthetic neural activity or behavior2. Finally, applying our model to behavioral data from fruit flies, we uncover an active forgetting mechanism in the neural circuitry underlying decision making. This readily adaptable modeling framework offers new opportunities for exploring the core mechanisms behind learning and memory processes in a variety of experimental paradigms.

## 2 Method overview

Our goal is to infer the synaptic plasticity function by examining neural activity or behavioral trajectories from an animal learning about its environment. Specifically, we aim to find a function that prescribes changes in synaptic weights based on relevant biological variables. For simplicity, we consider a model with plasticity localized to a single layer of a neural network:

\[^{(t)}=(W^{(t)}^{(t)}),\] (1)

where the vector \(^{(t)}\) represents the input to the plastic layer (Figure 1, "stimulus") and \(^{(t)}\) is the resulting postsynaptic neuron activity at time \(t\). The synaptic weight matrix \(W^{(t)}\) is updated at each time step based on a parameterized biologically plausible plasticity function \(g_{}\). The change in synaptic weight between neurons \(i\) and \(j\) is given by

\[ w^{(t)}_{ij}=g_{}(x^{(t)}_{j},y^{(t)}_{i},w^{(t)}_{ij},r^{( t)}),\] (2)

where \(\) are the (trainable) parameters of the function, \(x^{(t)}_{j}\) is the presynaptic neural activity, \(y^{(t)}_{i}\) the postsynaptic activity, \(w^{(t)}_{ij}\) is the current synaptic weight between neurons \(i\) and \(j\), and \(r^{(t)}\) is a global reward signal that influences all synaptic connections. However, it may be the case that we do not have direct access to the neuronal firing rates \(^{(t)}\). We therefore further define a (fixed) readout function \(f\) that determines the observable variables \(^{(t)}\) of the network, given by

\[^{(t)}=f(^{(t)}).\] (3)

Figure 1: Schematic overview of the proposed method. Animal-derived time-series data (yellow) and a plasticity-regulated _in silico_ model (blue) generate trajectories \(^{(t)}\) and \(^{(t)}\). A loss function quantifies trajectory mismatch to produce a gradient, enabling the inference of the synaptic plasticity rule \(g_{}\).

In the context of neural activity fitting, the readout is a subset of \(^{(t)}\), whereas for behavioral models the readout aggregates \(^{(t)}\) to yield the probability of a specific action. We introduce our specific choices for readout functions in the following sections.

We use stochastic gradient descent (Kingma & Ba, 2014) to optimize the parameters \(\) of the plasticity rule \(g_{}\). At each iteration, we use the model (Equation 1-3) to generate a length-\(T\) trajectory \(^{(1)},,^{(T)}\) (Figure 1, blue traces), driven by input stimuli \(^{(1)},,^{(T)}\) (Figure 1, black box). We then use backpropagation through time to compute the gradient of the loss (Figure 1, purple) between the model trajectory and the corresponding experimental observations \(^{(1)},,^{(T)}\) (Figure 1, orange) generated using the same input stimulus:

\[()=_{t=1}^{T}(^{(t)},^{(t)}),\] (4)

where the choice of \(\) depends on the particular modeling scenario, specified in the following sections. In practice, Equation 4 may also be summed over multiple trajectories to generate a mini-batch.

## 3 Inferring a plasticity rule from neural activity

To validate our approach on neural activity, we generate synthetic neural trajectories of observed outputs \(^{(t)}\) from a single-layer feedforward network that undergoes synaptic plasticity according to the well-known Oja's rule (Oja, 1982). At each timestep, the weight updates depend on pre- and post-synaptic neuronal activity, as well as the strength of the synapse itself (Figure 2A, top),

\[ w_{ij}=x_{j}y_{i}-y_{i}^{2}w_{ij},\] (5)

Figure 2: Recovery of Oja’s plasticity rule from simulated neural activity. (A) Schematic of the models used to simulate neural activity and infer plasticity. (B) Mean-squared difference between ground-truth and model synaptic weight trajectories over time (horizontal axis) over the course of training epochs (vertical axis). (C) The evolution of \(\) during training. Coefficients \(_{110}\) and \(_{021}\), corresponding to Oja’s rule values \((1,-1)\), are highlighted in orange. (D) \(R^{2}\) scores over weights, under varying noise and sparsity conditions in neural data. (E, F) Boxplots of distributions, across 50 seeds, corresponding to the first column (E) and row (F) in (D). (G) The evolution of learning rule coefficients over the course of training showing inaccurate \(\) recovery under high noise and sparsity conditions.

where we omit the time index \(t\) for brevity (see subsection A.3 for details). To infer the plasticity rule, we use a model network with an architecture identical to the ground-truth network (Figure 2A, bottom). While the exact circuit architecture may not always be known in biological data (particularly when studying mammalian brains), this assumption was made to show that our approach accurately infers plasticity rules in a scenario where generative and predictive circuit architectures were matched. Increasingly available connectomic information in biological systems can be used to design sufficiently accurate model architectures for our approach to work, as we will show later on in this paper (Bentley et al., 2016; Hildebrand et al., 2017; Scheffer et al., 2020). However, it is to be noted that connectomes can miss important information relevant to plasticity and may cause mismatches between model and generative architectures that could lead the model to incorrect outcomes (Liang and Brinkman, 2024). This should be kept in mind when interpreting the plasticity rules estimated by our approach.

Following previous work (Confavreux et al., 2020), we parameterize the model's plasticity function with a truncated Taylor series,

\[g_{}^{}=_{,,=0}^{2}_{ }x_{i}^{}y_{j}^{}w_{ij}^{},\] (6)

where the coefficients \(_{}\) are learned. Note that Oja's rule can be represented within this family of plasticity rules by setting \(_{110}=1\), \(_{021}=-1\), and all others to zero. Finally, we compute the loss as the mean squared error (MSE) between the neural trajectories produced by the ground truth network and the model:

\[_{}(^{(t)},^{(t)})=||^{(t)}-^{(t)}||^{2},\] (7)

where we let \(^{(t)}=^{(t)}\), assuming all neurons in the circuit are recorded (but see next section for analysis of sparse recordings).

### Recovering Oja's rule

Despite the fact that the model is optimized using _neuronal_ trajectories, the error of the _synaptic_ weight trajectories decreases over the course of training (Figure 2B), indicating that the model successfully learns to approximate the ground-truth plasticity rule. More explicitly examining the coefficients \(_{}\) over the course of training illustrates the recovery of Oja's rule as \(_{110}\) and \(_{021}\) approach \(1\) and \(-1\), respectively, and all others go to zero (Figure 2C).

To evaluate the robustness of our method, we assess how both noise and sparsity affect the model's performance (Figure 2D). We first consider the case where all neurons in the circuit are recorded, and we vary the degree of additive Gaussian noise in the recorded neurons. We find that the model's performance (\(R^{2}\) score between the ground-truth and model synaptic weight trajectories calculated on a separate held-out test set) decreases with increasing noise variance (Figure 2E). To simulate varying sparsity levels, we consider the readout \(^{(t)}=f(^{(t)})=(y_{k_{1}}^{(t)},,y_{k_{n}}^{(t)})\) to be the activity of a random subset \(n\) of all \(N\) postsynaptic neurons \(^{(t)}\), and we use the corresponding subset \(}^{(t)}=(o_{k_{1}}^{(t)},,o_{k_{n}}^{(t)})\) of recorded neurons from the ground-truth network in Equation 7 to optimize the plasticity rule. Our model maintains a high level of accuracy even when data is available from only 50% of the neurons (Figure 2F). This resilience to sparsity and noise is beneficial given that experimental neural recordings often suffer from these issues. However, we note that the model struggles to learn a sparse set of parameters for the plasticity rule when faced with both high recording sparsity and noise. The evolution of the plasticity parameters during training in this case is illustrated in Figure 2G. Together, these results show that our approach can accurately infer learning rules from neural trajectories in a wide array of recording conditions.

## 4 Inferring plasticity rules from behavior

Our approach can also be applied to behavioral data. This is particularly important because behavioral experiments are more widely available and easier to conduct than those that directly measure neural activity. We first validate the method on simulated behavior, mimicking decision-making experiments in which animals are presented with a series of stimuli that they choose to accept or reject. The animals' choices result in rewards and subsequent synaptic changes at behaviorally relevant synapses.

For this proof-of-principle, our ground-truth network architecture (Figure 3A, top) is inspired by recent studies that have successfully mapped observed behaviors to plasticity rules in the mushroom body (MB), the learning and memory center of the fruit fly _Drosophila melanogaster_(Li et al., 2020; Modi et al., 2020; Davis, 2023). Our neural network's three layer structure mimics the MB's neural architecture (see subsection A.4 for details). The readout \(^{(i)}\) is a series of binary decisions to either "accept" or "reject" the presented stimulus based on the average activity of the output layer. A probabilistic binary reward \(\{0,1\}\) is provided based on the choice. The reward signal is common to all synapses, which could be interpreted as a global neuromodulatory signal like dopamine. This reward leads to changes in the plastic weights of the network, determined by the underlying synaptic plasticity rule.

Plasticity occurs exclusively between the input and output layers. We simulate a covariance-based learning rule (Loewenstein and Seung, 2006) known from previous experiments (Rajagopalan et al., 2023). The change in synaptic weight \( w_{ij}\) is determined by the presynaptic input \(x_{j}\), and a global reward signal \(r\). This reward signal is the deviation of the actual reward \(\) from its expected value \([]\), which we calculate as a moving average over the last 10 trials. We neglect hypothetical dependencies on \(y_{i}\) because they are known to not impact reward learning in the fly mushroom body (although see also Table 1 and Appendix Table 3 for experiments with alternative plasticity rules):

\[ w_{ij}=x_{j}r=x_{j}(-[]).\] (8)

We model a plastic layer of neural connections that gives rise to learned behavior. The synaptic weights of the model are initialized randomly, reflecting the fact that the initial synaptic configurations are usually unknown _a priori_ in real-world biological systems. We consider a plasticity function parameterized through either a truncated Taylor series or a multilayer perceptron (MLP):

\[g_{}^{}=_{,,,=0}^{2}_{ }x_{j}^{}y_{i}^{}w_{ij}^{}r^{}  g_{}^{}=_{}(x_{j},y_{i}, w_{ij},r),\] (9)

Figure 3: Recovery of a reward-based plasticity rule from simulated behavior. (A) Schematic of the models used to stimulate behavior and infer plasticity rules. (B) The evolution of the weight of a single synapse, trained with \(g_{}^{}\) and \(g_{}^{}\), compared against weight from a known reward-based update rule. (C) \(R^{2}\) distributions on the weights across 10 seeds, corresponding to varied weight initializations and stimulus encodings. (D) The evolution of \(\) during training, with \(_{110}\), corresponding to ground truth rule (value = 1), highlighted in red. (E) Distribution of final inferred \(\) values across seeds, showing accurate identification of the relevant term from the ground truth learning rule. (F) The goodness of fit between ground truth behavior and model predictions plotted as the percent deviance explained.

where the ground truth reward \(r\) is used as the reward value in the weight update function. We use Binary Cross-Entropy (BCE) as the loss function, which is proportional to the model's negative log-likelihood function, to quantify the difference between the observed decisions and the model's predicted probabilities of accepting a stimulus.

\[_{}(^{(t)},^{(t)})=-^{(t)}(^{(t)})-(1 -^{(t)})(1-^{(t)}).\] (10)

Crucially, the training data only consists of these binary decisions (accept or reject), without direct access to the underlying synaptic weights or neural activity.

### Recovering reward-based plasticity from behavior

Figure 3B presents the weight dynamics of three networks: the ground-truth synaptic update mechanism, as well as those fitted with an MLP or a Taylor series. Both the ground-truth network and our model use an architecture with 2 input neurons, 10 neurons in the hidden layer, and 1 output neuron with a trajectory length of 240 time steps (see Appendix subsection A.4). Our evaluation metrics - high \(R^{2}\) values for both synaptic weights and neural activity - affirm the robustness of our models in capturing the observed data (Figure 3C). The method accurately discerns the plasticity coefficient of the ground truth rule (Figure 3D,E), albeit with a reduced magnitude. The model also does a good job at explaining the observed behavior (Figure 3F), where we use the percent deviance explained (see Appendix subsection A.4) as the performance metric.

We also consider alternative plasticity rules in the ground-truth network. Table 1 summarizes the recoverability of various reward-based plasticity rules for both MLP and Taylor series frameworks, with results averaged over 3 random seeds. Note that solely reward-based rules (without \([]\) or \(w\)) are strictly potentiating, as they lack the capacity for bidirectional plasticity. This unidirectional potentiation ultimately results in the saturation of the sigmoidal non-linearity. Therefore, it is possible to simultaneously observe high \(R^{2}\) values for neural activities with low \(R^{2}\) values for weight trajectories.

We further investigate the scalability of our method by varying the length of the observed trajectory and the number of neurons in the hidden layer (see Table 2). The model's goodness-of-fit generally improved with longer simulations, likely due to more data points for inferring the plasticity rule. However, \(R^{2}\) values for activity and weights peaked before declining, suggesting potential overfitting on very long trajectories. Model performance remained consistent when scaling to larger hidden layers, assuming the same plasticity rule is shared by all synapses.

## 5 Application: inferring plasticity in the fruit fly

In extending our results to biological data, we explore its applicability to the decision-making behavior in _Drosophila melanogaster_ that inspired our simulated behavior results. Recent research (Rajagopalan et al., 2023) employed logistic regression to infer learning rules governing synaptic plasticity in the mushroom body, identifying a rule that incorporates the difference between received and expected reward information when modulating synaptic plasticity. However, logistic

   \)} &  &  \\   & \(R^{2}\) & \(R^{2}\) & \% & \(R^{2}\) & \(R^{2}\) & \% \\  & Weights & Activity & Deviance & Weights & Activity & Deviance \\  \(x_{j}r\) & 0.85 & 0.96 & 64.76 & 0.78 & 0.94 & 61.91 \\ \(x_{j}r^{2}-0.05y_{i}\) & 0.97 & 0.97 & 34.55 & 0.96 & 0.97 & 34.36 \\ \(x_{j}r-0.05w_{ij}\) & 0.87 & 0.91 & 57.01 & 0.70 & 0.86 & 51.80 \\ \(x_{j}r^{2}-0.05x_{j}w_{ij}r\) & 0.85 & 0.96 & 53.04 & 0.78 & 0.92 & 51.30 \\ \(x_{j}y_{i}w_{ij}r-0.05r\) & 0.27 & 0.34 & 79.92 & 0.41 & 0.51 & 84.22 \\   

Table 1: Assessment of various reward-based plasticity rules: \(R^{2}\) scores for weight and individual neural activity trajectories, and the percentage of deviance explained for behavior. Refer to Appendix Table 3 for a comprehensive list of simulated plasticity rules.

regression cannot be used to infer plasticity rules that incorporate recurrent temporal dependencies, such as those that depend on current synaptic weights. Our method offers a more general approach. Specifically, we apply our model to behavioral data obtained from flies engaged in a two-alternative choice task, as outlined in Figure 4A. This allows us to investigate two key questions concerning the influence of synaptic weight on the plasticity rules governing the mushroom body.

### Experimental setup and details

In the experimental setup, individual flies are placed in a symmetrical Y-arena where they are presented with a choice between two odor cues. Each trial starts with the fly in an arm filled with clean air (Fig. 4A, left). The remaining two arms are randomly filled with two different odors, and the fly was free to navigate between the three arms. When the fly enters the'reward zone' at the end of an odorized arm, a choice was considered to have been made (Fig. 4A, right). Rewards are then dispensed probabilistically, based on the odor chosen. For model fitting, we use data from 18 flies, each subjected to a protocol that mirrors the trial and block structures in the simulated experiments presented previously. Over time, flies consistently showed a preference for the odor associated with a higher probability of reward, and this preference adapted to changes in the relative value of the options (Fig. 4B; example fly (Rajagopalan et al., 2023)).

### Plasticity in the fruit fly includes a synaptic weight decay

Existing behavioral studies in fruit flies have shown that these insects can forget learned associations between stimuli and rewards over time (Shuai et al., 2015; Aso and Rubin, 2016; Berry et al., 2018; Gkanias et al., 2022). One prevailing hypothesis attributes this forgetting to homeostatic adjustments in synaptic strength within the mushroom body (Davis and Zhong, 2017; Davis, 2023). However, earlier statistical approaches aimed at estimating the underlying synaptic plasticity rule present in the mushroom body were unable to account for recurrent dependencies such as synapse strength (Rajagopalan et al., 2023). Here we explore two types of plasticity rules: one based solely on reward and presynaptic activity, and another that incorporates a term dependent on current synaptic weight - \(w_{ij}\) (\(_{001}\)). Both rule types allocate significant positive weights to a term representing the product of presynaptic activity and reward (Fig. 4C, gray). Our results indicate that the model with a weight-dependent term offers a better fit to observed fly behavior (Wilcoxon signed-rank test: \(p=5 10^{-5}\); Fig. 4D), whereas the model without it matched the performance reported in Rajagopalan et al. (2023). Intriguingly, our analysis additionally reveals that the inferred learning rule assigns a negative value to the weight-dependent term (Fig. 4C). This finding also held in a validation experiment that assessed how well the inferred plasticity rule generalizes to unseen biological data (Appendix A.6). This negative sign aligns with the hypothesis that a weight-dependent decay mechanism operates at these synapses. The relative-magnitude of this decay term compared to the positive learning-related terms suggests that forgetting happens over a slightly longer-time scale than learning, in agreement with observed time-scales of forgetting in behavioral experiments Shuai et al. (2015); Davis and Zhong (2017).

    &  &  \\   & 30 & 60 & 120 & 240 & 480 & 960 & 1920 & 10 & 50 & 100 & 500 & 1000 \\  \(R^{2}\) Weights & 0.79 & 0.74 & 0.70 & 0.78 & 0.72 & 0.53 & 0.64 & 0.78 & 0.75 & 0.79 & 0.79 & 0.79 \\ \(R^{2}\) Activity & 0.92 & 0.91 & 0.91 & 0.94 & 0.95 & 0.87 & 0.92 & 0.94 & 0.94 & 0.95 & 0.95 & 0.95 \\ \% Deviance & 39.52 & 40.14 & 48.06 & 61.91 & 76.90 & 73.78 & 79.66 & 61.91 & 62.29 & 62.25 & 62.27 & 62.26 \\   

Table 2: Scalability analysis with respect to trajectory length (with a hidden layer size of 10) and hidden layer size (with a trajectory length of 240), assuming the ground-truth learning rule of \( w_{ij}=x_{j}r=x_{j}(-[])\) and using the Taylor series parameterization. Results are averaged over three runs with different random seeds.

### Incorporating reward expectation provides better fit than reward alone

Rajagopalan and colleagues used reward expectations (defined as the average reward received over the last approximately 'n' trials - see Appendix A.4.2) to generate bidirectional synaptic plasticity. Our discovery of a negative weight-dependent component in the plasticity rule provides an alternate mechanism for bidirectional plasticity, raising the question of whether the neural circuit really needs to calculate reward expectation. Could a plasticity rule incorporating the product of presynaptic activity and absolute reward combine with a weight-dependent homeostatic term to approximate a

Figure 4: Inferring principles of plasticity in the fruit fly. (A) Schematic of the experimental setup used to study two-alternative choice behavior in flies. _Left_ Design of arena showing odor entry ports and location of the reward zones. _Right_ Description of the trial structure, showing two example trials. (B) The behavior of an example fly in the task. _Top_ Schematics indicate the reward baiting probabilities for each odor in the three blocks. _Bottom_ Individual odor choices are denoted by rasters, tall rasters - rewarded choices, short rasters - unrewarded choices. Curves show 10-trial averaged choice (red) and reward (black) ratios, and horizontal lines the corresponding averages over the 80-trial blocks. (C) Final inferred \(\) value distribution across 18 flies, comparing models with and without a \(w_{ij}\) term and the method from Rajagopalan et al. (2023). Plasticity rule terms are as follows: bias - (\(_{000}\)), \(w_{ij}\) - (\(_{001}\)), \(x_{j}\) - (\(_{100}\)), \(r\) - (\(_{110}\)), \(x_{j}\)\(r\) - (\(_{110}\)) (D) _Left_ Goodness of fit between fly behavior and model predictions plotted as the percent deviance explained (n = 18 flies). _Right_ Change in the percent deviance explained calculated by subtracting percent deviance explained of model without a \(w_{ij}\) (\(_{001}\)) term from that of a model with a \(w_{ij}\) (\(_{001}\)) term. (E,F) Same as (C,D), except comparing models that do or don’t incorporated reward expectation. Since these models include weight dependence, they cannot be fit using Rajagopalan et al. (2023)’s method.

plasticity rule that involves reward expectation? To answer this, we contrast two models: one using only the absolute reward and another using reward adjusted by its expectation, both complemented by weight-dependent terms. Our analyses show that adding a weight-dependent term enhances the predictive power of both models (Fig 4E,F). However, the model that also factors in reward expectations provides a superior fit for the majority of flies in the data set (Wilcoxon signed-rank test: \(p=0.067\); Fig 4F). These compelling preliminary findings reaffirm the utility of reward expectations for fly learning, and larger behavioral datasets could increase the statistical significance of the trend. Overall, our model-based inference approach, when applied to fly choice behavior, suggests that synaptic plasticity rules in the mushroom body of fruit flies are more intricate than previously understood. These insights could potentially inspire further experimental work to confirm the roles of weight-dependent homeostatic plasticity and reward expectation in shaping learning rules.

## 6 Related work

Recent work has begun to address the question of understanding computational principles governing synaptic plasticity by developing data-driven frameworks to infer underlying plasticity rules from neural recordings. Lim et al. (2015) infer plasticity rules, as well as the neuronal transfer function, from firing rate distributions before and after repeated viewing of stimuli in a familiarity task. The authors make assumptions about the distribution of firing rates, as well as a first-order functional form of the learning rule. Chen et al. (2023) elaborate on this approach, fitting a plasticity rule by either a Gaussian process or Taylor expansion, either directly to the synaptic weight updates or indirectly through neural activity over the course of learning. Both approaches consider only the difference in synaptic weights before and after learning. In contrast, our approach fits neural firing rate _trajectories_ over the course of learning and can be adapted to fit any parameterized plasticity rule.

Other work infers learning rules based on behavior instead. Ashwood et al. (2020) uses a Bayesian framework to fit parameters of learning rules in a rodent decision-making task. The authors explicitly optimize the weight trajectory in addition to parameters of the learning rules, requiring an approximation to the posterior of the weights. Our approach directly optimizes the match between the model and either neural or behavioral data, as defined by a pre-determined loss function. Interestingly, despite this indirect optimization, we see matching in the weight trajectories as well. Rajagopalan et al. (2023) fit plasticity rules in the same fly decision-making task we consider here. They assumed that the learning rule depended only on presynaptic activity and reward, which recasts the problem as logistic regression and permits easy optimization. Our approach allows us to account for arbitrary dependencies, such as on postsynaptic activities and synaptic weight values, and we thereby identify a weight decay term that leads to active forgetting.

Ramesh et al. (2023) also consider optimization of plasticity rules based on neural trajectories. Unlike our approach which uses an explicit loss function, the authors use a generative adversarial (GAN) approach to construct a generator network, endowed with a plasticity rule, to produce neural trajectories that are indistinguishable by a discriminator network from ground-truth trajectories. Although, in principle, this approach can account for arbitrary and unknown noise distributions, it comes at a cost of high compute resources, a need for large amounts of data, and potential training instability - all well-known limitations of GANs. In practice, it is common to make an assumption about the noise model through an appropriately defined loss function (e.g. Gaussian noise for the MSE loss we use here). Importantly, the authors note a degeneracy of plasticity rules - different rules leading to similar neural dynamics. We see similar results, although we interpret this as "sloppiness" (Gutenkunst et al., 2007) - overparameterized models being underconstrained by the data (e.g. functions \(x\) and \(x^{2}\) are indistinguishable if the only values of \(x\) which are sampled are \(0\) and \(1\)). We hypothesize that in the infinite data limit the fitted plasticity rules would, in fact, be unique.

Previous work has also considered inferring plasticity rules directly from spiking data (Stevenson and Koerding, 2011; Robinson et al., 2016; Linderman et al., 2014; Wei and Stevenson, 2021) or selecting families of plausible rules in spiking neural network models (Confavreux et al., 2024). Due to the gradient-based nature of our optimization technique, our proposed approach can account for such data by converting spike trains to a rate-based representation by smoothing. Alternatively, black-box optimization techniques such as evolutionary algorithms can be used to circumvent the need for computing gradients, allowing non-differentiable plasticity rules like spike-timing dependent plasticity to be used as model candidates.

Alternatively, meta-learning techniques (Thrun and Pratt, 2012) can be used to discover synaptic plasticity rules optimized for specific computational tasks (Tylmanskov et al., 2022; Najarro and Risi, 2020; Confavreux et al., 2020; Bengio et al., 1990). The plasticity rules are represented as parameterized functions of pre- and post-synaptic activity and optimized through gradient descent or evolutionary algorithms to produce a desired network output. However, the task may not be well-defined in biological scenarios, and the network's computation may not be known _a priori_. Our method obviates the need for specifying the task, directly inferring plasticity rules from recorded neural activity or behavioral trajectories.

Finally, Nayebi et al. (2020) do not fit parameters of a learning rule at all, but use a classifier to distinguish among four classes of learning rules based on various statistics (e.g. mean, variance) of network observables (e.g. activities, weights). Similarly, Portes et al. (2022) propose a metric for distinguishing between supervised and reinforcement learning algorithms based on changes in neural activity flow fields in a recurrent neural network.

## 7 Limitations and future work

Despite its strengths, our model has several limitations that offer avenues for future research. One such limitation is the lack of complex temporal dependencies in synaptic plasticity, neglecting biological phenomena like metaplasticity (Abraham, 2008). Extending our model to account for such temporal dynamics would increase its biological fidelity. Another issue is the model's "sloppiness" in the solution space; it can fail to identify a unique, sparse solution even with extensive data. As neural recording technologies like Neuropixels (Steinmetz et al., 2021, 2018) and whole-brain imaging (Vanwalleghem et al., 2018) become more advanced, and connectome data for various organisms become increasingly available (Bentley et al., 2016; Hildebrand et al., 2017; Scheffer et al., 2020; Winding et al., 2023), there are exciting opportunities for validating and refining our approach. Incorporating these high-resolution, large-scale datasets into our model is a crucial next step. In particular, future work could focus on scaling our approach to work with large-scale neural recordings and connectomics, offering insights into the spatial organization of plasticity mechanisms. Such refinements will be important when applying our approach to larger and more densely connected brains, such as mammalian ones. Additional considerations for future research include the challenges posed by unknown initial synaptic weights, the potential necessity for exact connectome information, and the adequacy of available behavioral data for model fitting.