# EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations

Jia Li \(^{1,2}\), Ge Li\({}^{1,2}\), Xuanming Zhang\({}^{3}\), Yunfei Zhao\({}^{1,2}\), Yihong Dong\({}^{1,2}\), Zhi Jin\({}^{1,2}\)

Binhua Li\({}^{4}\), Fei Huang\({}^{4}\), Yongbin Li\({}^{4}\)

\({}^{1}\)Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education

\({}^{2}\)School of Computer Science, Peking University, Beijing, China

\({}^{3}\)Bytedance, \({}^{4}\)Alibaba Group

lijia@stu.pku.edu.cn, lige@pku.edu.cn, shuide.lyb@alibaba-inc.com

Corresponding authors

###### Abstract

How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, _i.e.,_ data leakage and lack of domain-specific evaluation. The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.

To address these two limitations, we propose a new benchmark - **EvoCodeBench**, which has the following advances: **Evolving data.** EvoCodeBench will be dynamically updated every period (_e.g.,_ 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories. **A domain taxonomy and domain labels.** Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. EvoCodeBench provides a broad platform for domain-specific evaluations. **Domain-specific evaluations.** Besides the Pass@\(k\), we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs. Besides, EvoCodeBench is collected by a rigorous pipeline and aligns with real-world repositories in multiple aspects (_e.g.,_ code distributions). We evaluate 8 popular LLMs (_e.g.,_ gpt-4, DeepSeek Coder, StarCoder 2) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, **the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%.** Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, **gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis1.

## 1 Introduction

Large Language Models (LLMs) have shown impressive abilities in code generation [14; 15; 18]. As more and more LLMs emerge, a reliable code generation benchmark is crucial to evaluating and selecting superior LLMs. Many benchmarks have been proposed, such as HumanEval , ClassEval, and DevEval . Researchers spend lots of effort to annotate test data manually and construct these benchmarks. For example, ClassEval and DevEval cost 500 and 674 person-hours, respectively.

Although promising, existing benchmarks have two limitations.

**Data leakage (aka data contamination).** It means that test data is included in the training data. The trained models perform much better on leaked benchmarks than on real-world tasks. Because the training data of LLMs contains almost all open-source code repositories, existing benchmarks probably have data leakages . Researchers have to spend more effort to construct new benchmarks.

**Lack of domain-specific evaluation.** Programming is highly domain-specific. Developers typically focus on specific domains (_e.g.,_ database). Compared to comprehensive coding abilities, developers are more concerned about the performance of LLMs in specific domains. However, existing benchmarks lack domain labels or fall into narrow domains. Besides, they ignore domain-specific evaluations and analyses. Thus, the performance of LLMs across domains is still unclear.

**To alleviate the above limitations, we propose a new code generation benchmark named EvoCodeBench.** EvoCodeBench has three novelties.

**Evolving data.** To avoid data leakages, EvoCodeBench is an evolving benchmark and will be dynamically updated every period (_e.g.,_ 6 months). Specifically, we build an automatic collection pipeline, which constructs new versions of EvoCodeBench from the latest repositories. More details about the pipeline are in Section 2.3.

**A domain taxonomy and domain labels.** PyPI  is a popular open-source community containing code repositories from various domains. Based on the statistics of repositories on PyPI, we design a programming domain taxonomy covering 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. In the future, we will refine the taxonomy (_e.g.,_ adding emerging domains) and provide a broad platform for domain-specific evaluations.

**Domain-specific evaluations.** Besides the Pass@\(k\), we propose the Domain-Specific Improvement (DSI), which reflects the position of an LLM in specific domains. Based on the DSI, we define the comfort domains (_e.g.,_ DSI > 10%) and strange domains (_e.g.,_ DSI < -10%) of LLMs. These metrics allow practitioners to effectively select superior LLMs in specific domains. Model trainers can also discover which domains LLMs are weak to facilitate future iterations.

Besides the above advances, EvoCodeBench has an advantage in data quality.

**EvoCodeBench** is collected from high-quality open-source repositories. More importantly, EvoCodeBench aligns with real-world repositories in multiple aspects, _e.g.,_ code distributions and dependency distributions. This ensures that the performance of LLMs on EvoCodeBench reflects their abilities in real-world development scenarios.

**EvoCodeBench** offers comprehensive annotations, _e.g.,_ natural language requirements, original repositories, reference code, reference dependencies, domain labels, and test cases. EvoCodeBench computes Pass@\(k\) and Recall@\(k\) to measure the correctness of generated programs in functionality and dependencies.

In this paper, we release the first version - EvoCodeBench-2403, which consists of 275 samples from 25 real-world repositories. Based on EvoCodeBench-2403, we evaluate 8 popular LLMs (_i.e.,_ gpt-4 , gpt-3.5 , DeepSeek Coder , StarCoder 2 , CodeLAMa ). Based on extensive experiments, we obtain the following insights.

**EvoCodeBench** significantly alleviates the data leakage and decreases the potential leak rate from 41.47% to 2.18%.

**EvoCodeBench** provides a reliable evaluation for repo-level code generation. We analyze these LLMs' failed cases and summarize future directions, _e.g.,_ long context modeling

**We evaluate LLMs in different domains and discover their comfort domains and strange domains. For example, gpt-4 performs best in most domains but performs worse than others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs.

## 2 EvoCodeBench

In this section, we first show an overview of EvoCodeBench and then describe its tasks and evaluation metrics. Then, we present the first version - EvoCodeBench-2403 and its statistics. Finally, we describe the automatic pipeline for constructing EvoCodeBench.

### Overview

Figure 1 shows a sample in EvoCodeBench. Each sample consists of seven components.

**Function Signature:** The signature of the target code. **Requirement:** An English description detailing the functionality of the target code. **Repository:** The current repository contains hundreds of code files. **Reference Code:** A developer-written implementation of the target code. This code may invoke dependencies defined in the current repository. **Reference Dependency:** The dependencies invoked in the reference code include intra-class, intra-file, and cross-file dependencies. **Domain Label:** The domain of the target code. **Test Cases:** Test cases are used to check the functional correctness of the code.

### Task and Metrics

EvoCodeBench evaluates LLMs in **repo-level code generation**. This task simulates the developers' coding process in a working repository. Given a requirement and a repository, LLMs are tasked to generate the code for the repository. Following previous work , EvoCodeBench contains two evaluation metrics, _i.e.,_ Pass@\(k\) and Recall@\(k\).

**Pass@\(k\) (Functional Correctness).** Following previous studies , we assess the functional correctness of programs by executing test cases and compute the unbiased Pass@\(k\). Specifically, we generate \(n k\) programs per requirement, count the number of correct programs \(c n\) that pass test cases, and calculate the Pass@\(k\):

\[k:=}{}[1-n-c\\ k)}{(n\\ k)}]\] (1)

**Recall@\(k\) (Recall of Reference Dependency).** We expect that generated programs can invoke relevant dependencies defined in contexts. Following previous work , we report Recall@\(k\), which computes the recall of reference dependencies in generated programs.

Figure 1: An overview of EvoCodeBench. Each sample consists of seven components.

Specifically, LLMs generate \(k\) programs per requirement. The dependencies invoked by the \(i\)-th generated program are denoted as \(_{i}\). We compare \(_{i}\) with reference dependencies \(\) and compute the Recall@\(k\):

\[k:=*{}_{}[_{i [1,k]}_{i}|}{||}]\] (2)

where \(||\) means the number of elements of a set.

### Benchmark Collection Pipeline

We build an automatic pipeline for collecting EvoCodeBench from the latest repositories. The pipeline consists of four stages as follows.

**Stage I: Repository selection and function scraping.** We crawl high-quality repositories from GitHub, satisfying the following criteria: open-source Python repositories with permissive licenses, created within the last six months, non-fork and non-malicious projects, more than 50 stars, and having explicit unit tests. Then, we extract candidate functions from repositories and exclude trivial functions (_e.g.,_ empty or initialization functions).

**Stage II: Execution-based filtering.** For each candidate function, we extract test cases invoking it from current repositories. We use pip  to install execution environments and leverage Pytest  to run test cases. Candidate functions without executable test cases are excluded.

**Stage III: Automatic annotations.** We leverage a static analysis-based parser  to extract each candidate function's signature, function body (_i.e.,_ reference code), and invoked dependencies (_i.e.,_ reference dependencies). Because manually writing requirements is laborious, we use LLMs to generate requirements. Specifically, we craft a one-shot prompt, which teaches LLMs to write requirements in a specific format (_i.e.,_ functional descriptions and input-output parameters). The prompt template is in Appendix E.2.

Next, we annotate each sample's domain label. To standardize the domains, we manually design a domain taxonomy. Specifically, we collect the statistics (_e.g.,_ stars and domains) of repositories in a popular software community - PyPI . Based on the statistics, we determine the top 10 domains with the most high-star repositories and construct the taxonomy. The 10 domains cover most of the repositories on PyPI and are shown in Table 1. In the future, we will continuously refine the taxonomy (_e.g.,_ adding emerging domains). Finally, we make a prompt and leverage LLMs to automatically annotate domain labels based on candidate functions and our taxonomy. Functions that do not satisfy any of the domains in our taxonomy are excluded. The prompt template is in Appendix E.2.

In Section 4, we conduct a human evaluation to assess auto-generated annotations. The results show that auto-generated annotations are comparable to human-written ones in most cases (_i.e.,_ requirement: 96.7% samples and domain label: 98.5% samples).

**Stage IV: Benchmark Construction.** Finally, we randomly select several candidate functions to construct EvoCodeBench. Following the related work , we strive to make EvoCodeBench satisfy the following goals: consistent with the code distribution observed in 500 real-world repositories, close to the average number of dependencies in 500 real-world repositories, including as many samples as possible. We have anonymized all personal information in the benchmark.

### EvoCodeBench-2403

Through the above pipeline, we collect and release the first version - EvoCodeBench-2403. The statistics of EvoCodeBench-2403 are shown in Table 2. We discuss its features as follows.

**Latest repositories to avoid data leakage.** Considering that most code LLM's [19; 10] training data is up to September 2023, existing benchmarks might have been leaked. For example, all repositories in CoderEval were created before September 2023. In contrast, the 25 repositories in EvoCodeBench-2403 were created between October 2023 and March 2024 and are not included in the training data. The details of 25 repositories is in Appendix D.2.

   Domain & Count \\  Scientific Engineering & 120 \\ Software Development & 50 \\ Multimedia & 32 \\ Database & 18 \\ System & 17 \\ Internet & 15 \\ Text Processing & 12 \\ Communications & 8 \\ Utilities & 2 \\ Security & 1 \\   

Table 1: The domain distribution of EvoCodeBench-2403.

**C Diverse domains.** EvoCodeBench-2403 covers all programming domains in our taxonomy. The domain distribution of EvoCodeBench-2403 is shown in Table 1. It provides a broad platform to evaluate and analyze the performance of LLMs across domains. Because that EvoCodeBench-2403 is our first version, the domain distribution may be unbalanced. In the future, we will collect new samples from the latest repositories and expand the data sizes in different domains.

**High data quality.** EvoCodeBench-2403 is collected by a rigorous pipeline and contains high-quality test data. First, as shown in Table 2, EvoCodeBench-2403 aligns with real-world repositories in multiple aspects. For example, the code distribution of EvoCodeBench-2403 is consistent with that of 500 real-world repositories2. Second, EvoCodeBench-2403 provides comprehensive annotations (_e.g._, requirements, reference code, reference dependency, and the original repository). These annotations offer a broad arena to explore repo-level code generation. Third, each sample in EvoCodeBench-2403 is equipped with an average of 6 test cases rigorously validated through human reviews. In comparison, each sample in a popular benchmark - MBPP  has three test cases on average.

## 3 Experiments

### Studied LLMs

We select 8 popular LLMs and evaluate them in EvoCodeBench. They cover general LLMs (_i.e.,_ gpt-4-turbo-1106  and gpt-3.5-turbo-1106 ) and Code LLMs (_i.e.,_ StarCoder 2-{15B, 7B} , DeepSeek Coder-{33B, 6.7B} , and CodeLLMaMa-{13B, 7B} ). We use official interfaces or implementations to reproduce these LLMs. We run these LLMs on 4 NVIDIA A100-40GB GPUs.

### Data Leakage Detection

As stated in Section 2, an advantage of EvoCodeBench is to alleviate data leakage significantly. To validate this point, we use the latest data leakage detection approach - CDD  to check EvoCodeBench-2403. CDD can detect whether LLMs have been trained on specific benchmarks and their variants. The detection results are shown in Table 3. Compared to a mainstream benchmark - HumanEval , the leakage rate of EvoCodeBench-2403 drops significantly to less than 3%. Besides, since different repositories typically contain similar programs (_e.g.,_ logging functions), it is almost impossible to achieve a 0% leakage rate.

Thus, we think that EvoCodeBench-2403 is leakage-free and can provide trustworthy evaluations in repo-level code generation.

   Benchmark &  &  &  \\  & \#Repo. & \#Sample & SA & Non-SA & \#Type & \#Avg. & Annotation \\  CoNaLa  & – & 500 & 100\% & 0\% & 0 & 0 & NL, Code \\ HumanEval  & – & 164 & 100\% & 0\% & 0 & 0 & NL, Code \\ MBPP  & – & 974 & 100\% & 0\% & 0 & 0 & NL, Code \\ APPS  & – & 5,000 & 100\% & 0\% & 0 & 0 & NL, Code \\ PandasEval  & – & 101 & 100\% & 0\% & 0 & 0 & NL, Code \\ NumpyEval  & – & 101 & 100\% & 0\% & 0 & 0 & NL, Code \\ AirBench  & – & 175 & 100\% & 0\% & 0 & 0 & NL, Code \\ ClassEval  & – & 100 & 100\% & 0\% & 0 & 0 & NL, Code, Depend. Name \\  Concode  & – & 2,000 & 20\% & 80\% & 1 & 1.23 & NL, Code \\ CoderEval  & 43 & 230 & 36\% & 64\% & 3 & 1.73 & NL, Code, Depend. Name \\ DevEval  & 117 & 1,874 & 27\% & 73\% & 3 & 3.41 & NL, Code, Depend. Repo \\ 
**EvoCodeBench-2403** & **25** & **275** & **27\%** & **73\%** & **3** & **3.46** & **NL, Code, Depend \\  & & & & & & & & Repo, Domain \\  
500 Real Repositories & 500 & 1M+ & 27\% & 73\% & 3 & 3.22 & – \\   

Table 2: The comparison between existing benchmarks and EvoCodeBench-2403. SA and Depend are the abbreviations of “standalone” and “dependency”, respectively.

   Benchmark & LLMs & Leak Ratio (\%) \(\) \\  HumanEval & gpt-3.5 & **41.47** \\   & gpt-4 & 2.18 \\  & gpt-3.5 & 1.75 \\  & DeepSeek Coder-33B & 1.88 \\ EvoCodeBench-2403 & DeepSeek Coder-7B & 1.82 \\  & StarCoder 2-15B & 1.45 \\  & StarCoder 2-7B & 1.09 \\  & CodeLLMa-13B & 0.82 \\  & CodeLLMa-7B & 0.73 \\   

Table 3: The results of data leakage detection.

### Performance in Repo-level Code Generation

**Experimental Settings.** Repo-level code generation takes a requirement and a repository as inputs. Typically, a repository is very long and surpasses the context windows of existing LLMs. Following previous work [16; 5], we extract parts of code contexts from the repository as inputs and design the following experimental settings. **Without context.** We ignore contexts and directly generate the code based on requirements and signatures. **Local File (Completion).** The local file denotes the code file where the reference code is in. This setting simulates the scenario where developers continue to write code at the end of a file. Besides the requirements and signatures, LLMs can access code contexts above the reference code in the local file. **Local File (Infilling).** This setting simulates the scenario where developers infill code in the middle of a file. Besides requirements and signatures, LLMs can see the code contexts above and below the reference code in the local file.

We use Pass@\(k\) and Recall@\(k\) (see Section 2.2) to assess generated programs. In this paper, \(k\). When \(k=1\), we use the greedy search and generate a single program per requirement. When \(k>1\), we use the nucleus sampling with a temperature 0.4 and sample 20 programs per requirement. We set the top-\(p\) to 0.95 and the max generation length to 500. Because EvoCodeBench is an evolving benchmark, this paper evaluates LLMs upon EvoCodeBench-2403. Note that the Pass@\(k\) and Recall@\(k\) between different versions of EvoCodeBench are not comparable.

**Results.** The Pass@\(k\) and Recall@\(k\) of studied LLMs are shown in Table 4.

**Compared to previous benchmarks, these LLMs' performance in EvoCodeBench-2403 drops dramatically.** For example, the highest Pass@1 scores of gpt-4 on the latest repo-level benchmark  is 53.04. In contrast, gpt-4 only achieves 20.73 on Pass@1 upon EvoCodeBench-2403. The decreases demonstrate that EvoCodeBench is more challenging, and previous benchmarks might have been leaked.

**LLMs benefit from more code contexts in repo-level code generation.** As shown in Table 4, after introducing the contexts, the Pass@\(k\) and Recall@\(k\) of LLMs obviously increase. For example, the Pass@1 of gpt-4 is improved by 104% and 152% in two settings, respectively. Similar phenomena occur in previous studies . We attribute the improvements to the domain knowledge contained in contexts. Figure 2 shows a uniquely successful case in the Local File (Completion) setting. Without context, gpt-4 fabricated a non-existent field as cache directories, generating the incorrect code. In

   LLMs & Size & Pass@1 & Pass@3 & Pass@5 & Pass@10 & Recall@1 & Recall@3 & Recall@5 & Recall@10 \\    \\  gpt-4 & N/A & **20.73** & **23.03** & **24.11** & **25.34** & 68.24 & 70.63 & 72.05 & 73.52 \\ gpt-3.5 & N/A & 17.82 & 21.78 & 23.06 & 24.46 & 61.94 & 68.13 & 69.69 & 70.85 \\ DeepSeek Coder & 33B & 19.64 & 22.78 & 24.29 & 26.01 & **71.46** & **79.93** & **82.11** & **86.25** \\ DeepSeek Coder & 6.7B & 17.82 & 21.02 & 22.40 & 23.97 & 69.58 & 74.04 & 78.00 & 83.22 \\ StarCoder 2 & 15B & 15.27 & 17.54 & 18.63 & 20.09 & 50.90 & 53.29 & 55.89 & 61.76 \\ StarCoder 2 & 7B & 14.91 & 17.29 & 18.63 & 19.86 & 56.35 & 60.59 & 63.74 & 74.20 \\    \\  gpt-4 & N/A & **17.45** & **19.65** & **20.80** & **22.41** & 63.49 & 68.67 & 70.00 & 72.07 \\ gpt-3.5 & N/A & 15.64 & 17.29 & 18.21 & 19.36 & 61.44 & 66.25 & 66.82 & 69.89 \\ DeepSeek Coder & 33B & 14.18 & 17.57 & 18.66 & 19.95 & 66.90 & **72.83** & 74.40 & 80.02 \\ DeepSeek Coder & 6.7B & 13.45 & 17.10 & 18.81 & 21.07 & 65.76 & 72.32 & 75.61 & 78.45 \\ StarCoder 2 & 15B & 13.82 & 15.44 & 17.84 & 19.59 & **68.55** & 71.37 & 74.76 & 77.70 \\ StarCoder 2 & 7B & 13.45 & 15.15 & 16.18 & 17.65 & 62.93 & 69.85 & 73.54 & 78.40 \\ CodeLAMa & 13B & 12.73 & 15.78 & 16.86 & 18.19 & 63.34 & 71.26 & **76.43** & 80.11 \\ CodeLAMa & 7B & 12.73 & 15.33 & 16.00 & 16.93 & 63.33 & 69.79 & 71.91 & 76.50 \\    \\  gpt-4 & N/A & **7.27** & **10.05** & **10.70** & 11.49 & 21.58 & 23.93 & 25.69 & 26.23 \\ gpt-3.5 & N/A & 6.55 & 7.85 & 8.28 & 8.73 & 21.66 & 24.31 & 24.77 & 25.40 \\ DeepSeek Coder & 33B & 6.91 & 8.92 & 9.79 & 11.03 & **27.67** & **32.73** & 34.92 & 37.76 \\ DeepSeek Coder & 6.7B & 5.82 & 8.56 & 9.67 & 11.26 & 25.89 & 32.06 & **35.59** & **38.33** \\ StarCoder 2 & 15B & 6.18 & 8.77 & 9.95 & **11.53** & 24.03 & 29.86 & 33.62 & 36.91 \\ StarCoder 2 & 7B & 5.82 & 6.72 & 7.43 & 8.62 & 27.39 & 32.60 & 34.88 & 36.81 \\ CodeLAMa & 13B & 5.45 & 7.38 & 8.37 & 9.95 & 25.52 & 31.28 & 33.66 & 36.36 \\ CodeLAMa & 7B & 5.45 & 6.94 & 7.75 & 9.03 & 26.97 & 31.17 & 34.08 & 36.82 \\   

Table 4: Pass@\(k\) and Recall@\(k\) of LLMs on EvoCodeBench-2403. Bold and underlined data indicate top-1 and top-2 results, respectively.

fact, two functions for returning the cache directories are available in the local file. After introducing the local file, gpt-4 successfully invokes relevant functions and generates the correct code.

Retrieval-Augmented Generation (RAG).RAG enhances generative models with retrieved information and has achieved promising results in code generation [17; 18]. We apply RAG to repo-level code generation and consider the current repository a retrieval corpus. Because most programs in repositories are not equipped with documentation, we retrieve top-\(k\) (_i.e.,_\(k=5\) in this paper) functions with similar names to the target function. Specifically, we split names into tokens based on underscore or camelcase formatting and then match the tokens of names. Finally, we use similar functions as contexts in prompts. The results are shown in Table 5. The performance of both LLMs is improved after introducing similar functions. We attribute the improvements to relevant algorithms and dependencies in similar functions. It inspired researchers to explore more advanced RAG techniques to improve repo-level code generation.

Error Analyses.The Pass@\(k\) of existing LLMs in repo-level code generation is still low. To determine LLMs' shortcomings, we manually analyze 50 error cases of gpt-4 in the Local File (Infilling) setting. We found the most cases (29 cases) are caused by implementation logic errors. 20 cases failed since the necessary contexts were missing, _e.g.,_ APIs defined in other files. Besides, one case failed because of the vague requirement. It shows that existing LLMs' reasoning and coding abilities need to be improved. Meanwhile, how to utilize more contexts is necessary to explore.

### Performance in Different Domains

We divide EvoCodeBench into multiple subsets according to the domain labels and then calculate the Pass@1 of LLMs in different domains. The results are shown in Table 6. We ignore three domains with less than 10 samples and leave them for future work.

EvoCodeBench shows superior LLMs in specific domains.The Pass@1 scores in overall benchmarks demonstrate the comprehensive coding abilities of LLMs. Because developers typically focus

    &  &  & DeepSeek &  &  \\  & & & 33B & 6.7B & 15B & 7B & 13B & 7B \\  Database & **38.89** & **38.89** & 33.33 & 33.33 & **38.89** & 33.33 & 33.33 & 33.33 \\ System & **35.29** & **35.29** & **35.29** & **35.29** & 29.41 & 29.41 & 23.53 & **35.29** \\ Software Development & **12.00** & **12.00** & 8.00 & **12.00** & 10.00 & 6.00 & 8.00 & 8.00 \\ Internet & 20.00 & **26.67** & **26.67** & **26.67** & 20.00 & **26.67** & **26.67** & **26.67** \\ Scientific Engineering & **11.67** & 10.00 & 10.00 & 6.67 & 8.33 & 9.17 & 7.50 & 8.33 \\ Multimedia & **25.00** & 18.75 & 15.63 & 15.63 & 18.75 & 18.75 & 18.75 & 12.50 \\ Text Processing & **16.67** & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\  All Domains & 17.45 & 15.64 & 14.18 & 13.45 & 13.82 & 13.45 & 12.73 & 12.73 \\   

Table 6: The Pass@1 of studied LLMs in different domains upon EvoCodeBench-2403.

    & Setting & Pass@1 & Recall@1 \\   & Without Context & 8.31 & 21.08 \\  & Similar Functions & 12.29 & 45.14 \\   & Without Context & 6.64 & 21.16 \\  & Similar Functions & 11.62 & 41.93 \\   

Table 5: Performance of RAG.

Figure 2: A uniquely successful case in Local File (Completion) setting.

on specific programming domains, they are more concerned about the performance of LLMs in specific domains. Imagine we are developers focused on internet-related programming tasks. Based on the overall Pass@1, we would think StarCoder 2-7B is stronger than DeepSeek Coder-6.7B, _i.e.,_ 13.82 \(>\) 13.45. However, according to Table 6, DeepSeek Coder-6.7B performs better than StarCoder 2-7B in the Internet domain. This result can help us to select more suitable models.

**EvoCodeBench uncovers the comfort domains and strange domains of specific LLMs.** For ease of observation, we compute the Domain-Specific Improvement (DSI) of LLMs in different domains. The DSI refers to the average relative improvement of Pass@1 of an LLM in a domain compared to other LLMs. Suppose we evaluate \(N\) LLMs on a specific domain, and their Pass@1 scores are represented as \(\). Then, the DSI (%) of \(i\)-th LLM in this domain is computed as:

\[DSI_{i}=_{j}-P_{j}}{P_{i}}*100(i j)\] (3)

The larger the DSI, the better an LLM is at that domain. If an LLM's DSI in a domain exceeds a threshold \(\), we consider it a comfort domain. If an LLM's DSI in a domain is less than \(-\), it is considered a strange domain. \(\) is a hyper-parameter and is set to 10% in this paper. Practitioners can further tune this parameter.

Table 7 shows the DSIs of studied LLMs across domains. The comfort domains and strange domains are marked in bleu and red, respectively. We can see that gpt-4 has the most comfort domains. Especially in the Text Processing domain, among all LLMs, only gpt-4 successfully solves some programming tasks. However, gpt-4 performs worse than others in the Internet domain. Besides, we discover that StarCoder 2-15B unexpectedly performs well in the Database domain and even is comparable to gpt-4. The potential reason for comfort and strange domains is that the pre-training data mix of LLMs is different. For example, gpt-4's training data contains fewer repositories in the Internet domain, resulting in weak performance. These findings can help model trainers analyze the shortcomings of existing LLMs and build more powerful code LLMs.

## 4 Discussion

**Evaluation of auto-generated annotations.** We leverage an LLM (_i.e.,_ gpt-4 in this paper) to annotate natural language requirements and domain labels for functions. To assess the quality of auto-generated annotations, We hire five developers to write requirements and domain labels for EvoCodeBench-2403. Then, we hire another five developers to compare annotations from gpt-4 and developers. All of these developers have at least 3 years of Python development experience. All developers are paid according to the relevant policies3 (_i.e.,_ $7.5 per hour). The details of human evaluation are in Appendix E.3.

The evaluation results are shown in Table 8. The Cohen's Kappa coefficient between all evaluators is 0.9. The _The_ means both requirements are satisfying. We can see that gpt-4 produces high-quality annotations comparable to human-written annotations in most cases (_e.g.,_ requirement: 96.7% = (30+236)/275, domain labels: 98.5% = (3+268)/275). We also inspect failed cases of gpt-4 and summarize two main reasons. First, gpt-4 may miss some

    &  &  &  &  &  \\  & & & & 33B & 6.7B & 15B & 7B & 13B & 7B \\  Database & 10.21 & 10.21 & -7.14 & -7.14 & -7.14 & 10.21 & -7.15 & -7.15 \\ System & 9.51 & 9.52 & 9.51 & 9.51 & -11.42 & -11.42 & -42.84 & 9.52 \\ Software Development & 23.81 & 23.81 & 21.743 & 23.81 & -66.67 & 5.71 & -21.43 & -21.43 \\ Internet & 22.859 & 7.15 & 7.15 & 7.15 & 7.15 & -28.59 & -7.15 & -7.15 \\ Scientific Engineering & 26.55 & 11.90 & 11.90 & -39.22 & 2.63 & -8.63 & -22.23 & -8.63 \\ Multimedia & 32.14 & 4.75 & -17.11 & -17.11 & 4.75 & 4.75 & 4.75 & -50.01 \\ Text Processing & 100.00 & -100 & -100 & -100 & -100 & -100 & -100 & -100 \\   

Table 7: The Domain-Specific Improvements (%) of LLMs in different domains. The comfort domains and strange domains are marked in bleu and red, respectively.

details (_e.g.,_ hyper-parameters) that are necessary for requirements. Second, gpt-4 may be mistaken by specific APIs and output inaccurate domain labels. In the future, we will explore new techniques to solve this problem, _e.g.,_ controllable text generation . Besides, gpt-4 shows advantages in costs. As shown in Table 8, gpt-4 costs less time and money to annotate requirements. Thus, it is a feasible and efficient approach for us to use gpt-4 to annotate requirements for EvoCodeBench.

**Limitations.** There are two main limitations in EvoCodeBench. First, EvoCodeBench is a monolingual (_i.e.,_ Python) benchmark and ignores other programming languages (_e.g.,_ Java, C++). Because building repo-level benchmarks faces many language-specific challenges (_e.g.,_ how to install execution environments, how to run test cases), we chose to start with Python, a mainstream programming language in existing benchmarks [28; 3; 7; 29]. We plan to support other programming languages in the future gradually. Second, the size of EvoCodeBench is currently smaller than some existing benchmarks. The reason is that EvoCodeBench-2403 only collects samples from recent repositories (_i.e.,_ Oct. 2023 - Mar. 2024). In the future, we will continue to collect new samples from the latest repositories and expand the scale of EvoCodeBench.

## 5 Related Work

**Code Generation Benchmarks.** Nowadays, prevalent code generation benchmarks can be divided into two groups: \(\) Snippet-level benchmarks [3; 1; 11; 7]. They comprise human-written or competitive programming problems, which ask LLMs to generate standalone code snippets. \(\) Repro-level benchmarks [29; 16]. They ask LLMs to generate new programs based on requirements and contexts from current repositories. Compared to snippet-level benchmarks, repo-level benchmarks are more challenging and closer to real-world software development scenarios.

This paper proposes a new benchmark - EvoCodeBench, to alleviate two limitations of previous benchmarks (_i.e.,_ data leakage and lack of domain-specific evaluations). We notice that some recent benchmarks focus on similar limitations. We further clarify the differences between EvoCodeBench and existing benchmarks.

**Data leakage.** LiveCodeBench  collects the latest competitive programming problems. EvoEval  leverages LLMs to mutate HumanEval and obtain new benchmarks. They are both snippet-level benchmarks, while EvoCodeBench is a more practical repo-level benchmark. The collection pipelines in LiveCodeBench and EvoEval can not be applied to repo-level benchmarks, which involve many new challenges (_e.g.,_ repository selection, test construction, and requirement annotation). We fill this knowledge gap by building a new collection pipeline and release EvoCodeBench.

**Domain-specific evaluations.** Existing benchmarks typically fall into narrow domains (_e.g.,_ PandasEval ) or lack domain labels (_e.g.,_ DevEval ). ClassEval  contains 100 standalone programming tasks from seven domains. However, these domains are manually designed based on human experiences and may ignore important domains (_e.g.,_ Internet and Multimedia). Besides, ClassEval ignores repo-level benchmarks and may be leaked in the future. In contrast, we consider the statistics of a mainstream open-source community and identify the top 10 popular programming domains. Besides, EvoCodeBench is free of data leakage and continually expands domains.

## 6 Conclusion and Future Work

We introduce EvoCodeBench, an evolving code generation benchmark. EvoCodeBench is designed to address two limitations (_i.e.,_ data leakage and lack of domain-specific evaluations). EvoCodeBench is an evolving benchmark and will be dynamically updated every period (_e.g.,_ six months), to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, which contains 275 samples. Besides, we design a programming domain taxonomy consisting of ten popular domains and annotate samples with domain labels. We conduct extensive experiments on EvoCodeBench and reveal the actual abilities of LLMs in real-world repositories. We also evaluate LLMs in different domains and discover their comfort and strange domains. These insights can help practitioners evaluate LLMs comprehensively.

In the future, we will continuously release new versions of EvoCodeBench and extend EvoCodeBench into other programming languages (_e.g.,_ Java and C++).