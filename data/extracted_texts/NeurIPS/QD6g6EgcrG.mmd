# EnsemW2S: Can an Ensemble of LLMs be

Leveraged to Obtain a Stronger LLM?

Aakriti Agrawal\({}^{}\) Mucong Ding\({}^{}\) Zora Che\({}^{}\) Chenghao Deng\({}^{}\)

Anirudh Satheesh\({}^{}\) John Langford\({}^{*}\) Furong Huang\({}^{}\)

MicrosoftUniversity of Maryland; e-mail: {agrawal5, furongh}@umd.edu

###### Abstract

How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization--a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of up to 14% over existing baselines and average improvements of 5% and 4% for binary classification and generative tasks, respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient.

## 1 Introduction

As AI models, particularly Large Language Models (LLMs), continue to surpass human performance in various domains, a pressing challenge arises: how do we effectively supervise models that exceed our capabilities? This problem, known as super-alignment, is exacerbated by the scarcity of high-quality labeled data, which limits direct human oversight. The key question driving our work is whether weak models, trained on simpler tasks, can be leveraged to instruct and improve stronger models in complex settings--a problem known as weak-to-strong (w2s) generalization.

The concept of w2s generalization was introduced by Burns et al. (2023), where weak models are used to align stronger models in the absence of sufficient ground-truth supervision. However, while this work laid the groundwork, it left several critical challenges unresolved. **(C1) Single Weak Supervisor Limitation.** Prior studies (Burns et al., 2023; Ji et al., 2024; Charikar et al., 2024; Lang et al., 2024) tend to rely on a single weak supervisor, limiting the diversity and robustness of the supervision. A single model's perspective often falls short when attempting to instruct stronger models in more complex tasks, highlighting the need for a more diversified supervisory approach. **(C2) Lack of Focus on Weak Model Enhancement.** Another limitation is that previous research (Burns et al., 2023; Ji et al., 2024; Charikar et al., 2024; Lang et al., 2024) has focused predominantly on improving knowledge transfer from weak to strong models without addressing how to enhance the weak models themselves. This oversight leaves weak models under-optimized, thereby restricting their utility in complex problem settings. **(C3) Overlooking Task Complexity.** Furthermore, while task complexity plays a crucial role in determining how well weak models can supervise stronger ones, most prior work (Sun et al., 2024) has not adequately addressed this issue. For instance, Burns et al. (2023) briefly explored the impact of task complexity using chess data, but a more structured and systematic approach is needed to differentiate between easy and hard tasks and study their effects on supervision.

To address these challenges, we propose a novel ensemble-based method designed to improve w2s generalization. Central to our approach is an easy-to-hard (e2h) framework, which extends w2s generalization by focusing on the progression from simpler tasks (easy) to more complex tasks (hard). This mirrors practical scenarios, where human oversight is more feasible for simpler tasks, and weak models must step in to guide stronger models in tackling harder tasks. In this setting, weak models trained on easy data supervise stronger models working on more difficult problems, creating a more pragmatic approach to w2s generalization.

To further enhance the capabilities of weak models, we develop a novel AdaBoost-inspired ensemble method for generation tasks, in addition to classification tasks. By combining the supervision of multiple weak models, we create a more robust and effective supervisory system for stronger LLMs. This ensemble approach overcomes the limitations of single-supervisor systems and introduces a mechanism to refine the weak models themselves, ensuring they can provide meaningful guidance even in complex tasks. Our experiments demonstrate that this ensemble method not only improves the weak models' generalization capabilities but also enables stronger models to achieve performance on par with oracle models trained on high-quality data.

The **main contributions** of this paper are the following:

**(1) We introduce an ensemble method inspired by AdaBoost,** combining weak LLMs to provide stronger supervision for training stronger models. Our approach is validated through experiments on binary classification tasks, where we observe improvements of up to 14% over baselines and an average improvement of 7% across all model pairs, showcasing the feasibility of w2s generalization.

**(2) We extend this framework to supervised fine-tuning tasks for autoregressive LLMs,** where our novel algorithm combines weak LLMs via a voting mechanism that adjusts token probabilities. In several cases, we observe our strong model trained using weak labels to outperform the strong model trained on ground truth, thus enabling effective supervision, even on complex tasks.

**(3) We propose a practical easy-to-hard (e2h) framework for w2s generalization,** where models trained on easy data provide supervision for harder tasks. This setup emphasizes the importance of task complexity and demonstrates significant improvements when weak models guide strong LLMs. For our EnsemW2S-AdaBoost method, along with observing w2s-trained student models outperforming the strong student oracle in several e2h generalization scenarios, we also observe accuracy improvements of up to 10% over baselines and an average improvement of 3.34% and 4.4% for Quartz and ARC data respectively.

## 2 Weak-to-Strong Generalization via Easy-to-Hard Framework

Figure 1: This figure illustrates the complete pipeline of our EnsemW2S method for easy-to-hard generalization using w2s generalization. In a realistic scenario, weak experts are adept at answering easy questions but must supervise strong models to tackle hard problems. **In the leftmost portion**, we show that we train weak models on easy data, strong models on hard data, and transfer models on pseudo labels generated by the weak model on hard data. Ultimately, we aim to increase the Performance Gap Recovered (PGR). **On the right**, we depict how our EnsemW2S-AdaBoost algorithm chooses the correct answer at the token level. **At the bottom**, we provide an example of easy and hard data for the Quartz dataset for e2h generalization, highlighting the importance of distinguishing between easy and hard data for realistic w2s generation.

**The Overall Idea.** We investigate the easy-to-hard framework as a more pragmatic setting to study the (im)possibility of w2s generalization. In this framework, weak models train on simpler tasks and subsequently instruct strong models to tackle more complex challenges, closely mirroring real-world conditions with limited human oversight. Figure 1 explains our idea and pipeline for easy-to-hard generalization using w2s generalization. (Figure 6 in the Appendix provides the detailed algorithmic and data flow). In a realistic scenario, weak experts are proficient in answering easy questions but must supervise strong models to tackle hard problems. We train weak models on easy data and strong models on hard data. A transfer model is trained using pseudo labels generated by the weak model on the hard data. Ultimately, we aim to improve the Performance Gap Recovered (PGR).

### The Easy-to-Hard Framework

**Weak Model \(h_{}\) as the Teacher.** A state-of-the-art LLM \(h_{}\) is trained on a set of 'easy data' that we currently have access to labels, i.e., \((^{e},^{e})\). For example, this could be Go games, math problems, or common sense reasoning questions that we have solutions for. This "weak teacher is trained on the labeled easy data \((^{e},^{e})\). Although we refer to this model as a "weak teacher", it is only relatively weak compared to the strong model we aim to obtain. Moreover, the "easy data" is only relatively easy compared to the hard data for which we currently lack solutions. Thus, the easy data may not be simple but slightly easier than the hard data, which are currently unsolvable using existing models.

**Strong Model \(u_{}\) as the Upper Bound.** As an important part of our thought experiment, we establish an upper bound, which is not attainable in practice. Specifically, we assume access to the ground-truth labels of the hard data \((^{h},^{h})\), which is impractical but establishes an upper bound for this thought experiment. A model \(u_{}\), larger than the weak teacher \(h_{}\), is trained on the labeled hard data \((^{h},^{h})\). The reason why \(u_{}\) is larger than \(h_{}\) is that we believe a model strong enough to solve hard questions that no existing models can solve will require high capacity.

**Weak-to-Strong Model \(f_{}\) Obtained in Practice.** To test the weak-to-strong generalization, we will train a weak-to-strong transfer model \(f_{}\) that has the same capacity as the strong model, i.e., the same model size as \(u_{}\), but is not trained under the unrealistic assumption of oracle access to hard labels. Rather, it is trained using weak teacher's feedback. Specifically, we consider using the pseudo-labeled \((^{h},h_{}(^{h})\) as training data for training the weak-to-strong transfer model \(f_{}\).

### Easy and Hard Data

**Dataset and Setup.** We use the SciQ dataset (Welbl et al., 2017) for the binary classification task. It is a multiple-choice science question-answer dataset and is also used as one of the NLP classification datasets by Burns et al. (2023). We convert it into binary labels following (Burns et al., 2023). For the supervised fine-tuning (SFT) task on the Q/A dataset, we use ARC (Clark et al., 2018) and Quartz (Tafjord et al., 2019) datasets, which are also multiple-choice question-answer datasets, allowing us to generate multiple-choice pseudo labels. Ding et al. (2024) provide difficulty levels for some common mathematics and programming problems, chess puzzles, and reasoning question datasets, which can be further utilized to expand this work. For details on how we conduct **easy \((^{e},^{e})\) and hard \((^{h},^{h})\) data split**, refer Appendix Section E.

### An Ensemble of Teachers

In a practical situation, we may face a dearth of strong supervisors but have an abundance of weak supervisors. Previous works (Burns et al., 2023; Ji et al., 2024) have used only one weak supervisor. Our work aims to combine the power of multiple weak supervisors to provide stronger supervision for better weak-to-strong (w2s) generalization. However, combining multiple weak supervisors to improve w2s generalization is challenging. In the following section, we detail how to combine a collection of weak teachers with diverse skill sets to obtain a competitive w2s model that is better than the weak model and ideally reaches or even surpasses the strong model, i.e., the upper bound of performance.

## 3 W2S Generalization via AdaBoost of Diverse Teacher LLMs

In this section, we introduce our method to boost experts for two tasks: a binary classification task for an NLP dataset and a supervised fine-tuning task for multiple-choice Q/A datasets. A list of important notions is mentioned in Appendix D for reference.

### AdaBoost of Weak LLM Teachers for Classification Tasks

This simple thought experiment tests w2s generalization and is the first task tested by Burns et al. (2023). We utilize the vanilla AdaBoost algorithm (Algorithm 2 detailed in the Appendix) to generate answers to a hard question \(^{h}\) from each weak LLM teacher, i.e., generate \(h^{t}_{}(^{h})\) for\(t\{1,,T\}\). A weighted "majority vote/aggregation" is implemented to generate a consensus as the answer \((_{t=1}^{T}_{h}^{t}_{b}^{t}(^{h})>0)\{0,1\}\), also known as the pseudo-label, to the hard question \(^{h}\). Here, the coefficients \(\{_{t} t\{1,,T\}\}\) are hyperparameters learned during the AdaBoost training. (More details in Appendix Sec F.)

### Improving AdaBoost for Complex Generation Tasks

**Challenges of Applying AdaBoost.** The canonical AdaBoost algorithm assumes a sophisticated ensemble of feedback in the form of scores. However, LLMs are generative AI models known for their remarkable ability to generate coherent, free-form text. Applying the vanilla AdaBoost algorithm directly to generation tasks is challenging because (1) the output is not just a single class label but a sequence of text with no fixed length, and (2) different teachers may generate answers in various formats, making it non-trivial to combine their responses.

**EnsemW2S-AdaBoost: Our modified AdaBoost Algorithm for Multiple-Choice Q/A Task.** To address these challenges, we propose a modified multi-class AdaBoost algorithm where the number of classes corresponds to the vocabulary size. We treat each token as an independent sample, as shown in Algorithm 1, and apply multi-class AdaBoost (Hastie et al., 2009) with modifications, calling our algorithm EnsemW2S-AdaBoost.

_Token-Level Weighting._ The first modification involves generating weights for each token within a sentence sample. We define the initial token-sample weights vector \(D_{1}(i,j)\) for all \(i[m],j[k_{i}]\), where \(n=_{i=1}^{m}k_{i}\), \(k_{i}\) is the number of tokens in the answer part of each sample \(i\), \(m\) is the total number of training data samples and \(j\) is the \(j^{}\) token in a particular chosen \(i^{}\) sample. We update these weights, \(D_{t}(i,j)\), for each iteration \(t\) of EnsemW2S-AdaBoost.

_Token-Level Data Sampling._ We sample \(S^{}=\{(_{i}^{ e},_{i}^{ e})\}_{i=1}^{m}\) from \(S\) using token-sample weights \(D_{t}(i,j)\). By sampling with respect to probability masses \(D_{t}(i,j)\) with repetition, we obtain a set of \(n=_{i=1}^{m}k_{i}\) tokens to train on. However, treating these \(n\) sampled tokens as independent training samples is very inefficient. Instead, we "assemble" the sampled tokens back into the sentences they belong to and implement label masks to only train on the sampled tokens in each sentence. Following this method, we can train on sampled tokens with minimal overheads.

_Training and Generating New Weak Teachers._ For each iteration, \(t\), of EnsemW2S-AdaBoost we train a new weak teacher model \(h_{}^{t}\) on the sampled data, \(S^{}\).

_Incorporating Prior Term._ Following Hastie et al. (2009), multi-class boosting uses an additional \((c-1)\) term, where \(c\) is the number of classes, in the calculation of the AdaBoost parameter \(\). This term serves two purposes: (1) It enables the generation of weak models with accuracy between 50% and random \(\) %, which is crucial for smaller models and challenging tasks that cannot achieve 50% accuracy. (2) It ensures that \(\) remains positive. Bayesian inference is used to provide proof of the benefits of this prior term. Given the large vocabulary size in our case, we introduce a prior term \((}-1)\), where \(_{pre}\) is the pre-trained model error of the chosen LLM. This term is sensible because it represents the error before fine-tuning the LLM, effectively replacing the random error baseline. Thus, the final \(\) equation is: \(_{t}(}{_{t}})+(}-1)\).

_Weighted Error Calculation._ Our weighted error equation \(_{t}\) also undergoes minor changes. The strict condition for each round of AdaBoost training is that the weighted model error (calculated by comparing each token of each sample) must be less than the pre-training error, i.e., \(_{t}<_{pre}\). The weighted model error \(_{t}\) is defined as, \(_{t}=_{i=1}^{m}_{j=1}^{k_{i}}\{h_{}^{t}(_{i}^{e},_{i}^{e,j-1})_{i}^{e,j}\}D_{t}(i,j)<_{pre}\). Here, \(_{i}^{e,j-1}\) is the \((j-1)^{}\) ground-truth token in the answer part. The model \(h_{}^{t}(_{i}^{e},_{i}^{e,j-1})\) predicts the next token and compares it with the ground-truth token \(_{i}^{j}\).

_Weight Update Equation._ Our weight update equation for each token is \(D_{t+1}(i,j)}D_{t}(i,j)e^{_{t}1\{h_{}^{t} (_{i}^{e},_{i}^{e,j-1})_{i}^{e,j}\}}\) where \(Z_{t}\) is a normalization factor calculated by taking the norm of the updated weight vector to ensure \(_{i=1}^{m}_{j=1}^{k_{i}}D_{t+1}(i,j)=1\).

**Combining Experts to Generate Pseudo Answers for Hard Questions:** To combine the outputs of different experts trained during the various EnsemW2S-AdaBoost rounds, we scale the probability distribution for each token generated by the model \(h_{}^{t}\) in round \(t\) by its corresponding weight \(_{t}\). Specifically, we multiply \(_{t}\) by the probability distribution vector of each token. We then aggregate these weighted distributions across all rounds, normalizing the resulting vector to form a new probability distribution for each token.

```
0: An "easy" Q/A training dataset with \(m\) examples: \(S^{c}=\{(_{i}^{c},_{i}^{c})\}_{i=1}^{m}\); a pre-trained weak teacher model \(h_{}^{0}\) parameterized by \(\); total number of EnsemW2S-AdBoost iterations \(T\); a "hard" unlabeled (questions only) dataset with \(O\) examples: \(S^{h}=\{_{}^{h}\}_{o=1}^{O}\)
0: Weak-to-Strong Student Model \(f_{}()\)
1: Initialize Token-Sample Weights: \(D_{1}(j,t)\) for all \(i[m],j[k_{i}]\), where \(k_{i}\) is the token length in the \(i^{}\) easy example (i.e., \(_{i}^{c}=(_{i}^{e,1},_{i}^{e,2}..._{i}^{e,k_{i}})\)) and \(n=_{i=1}^{m}\,k_{i}\)
2: Calculate pre-training error of \(h_{}^{0}\): \(_{pre}_{i=1}^{m}_{j=1}^{k_{i}}1\{h_{}^{0}( {x}_{i}^{e},_{i}^{e,j-1})_{i}^{e,j}\}D_{1}(i,j)\)
3:for\(t 1\) to \(T\)do
4: Sample \(S^{}=\{(_{i}^{},_{i}^{})\}_{i=1}^{m}\) from \(S\) using token-sample weights \(D_{t}(i,j)\)
5: Train a new weak teacher \(_{}^{0}\) on \(S^{}\)
6: Calculate \(_{t}=_{i=1}^{m}_{j=1}^{k_{i}}1\{h_{}^{0}(_{i}^{c},_{i}^{e,j-1})_{i}^{e,j}\}D_{t}(i,j)\)
7:if\(_{t}_{pre}\)then
8:break
9: Calculate \(_{t}}{_{t}}+(}-1)\)
10: Update \(D_{t+1}(i,j)}D_{t}(i,j)e^{_{t}1\{h_{}^{0}( _{i}^{e},_{i}^{e,j-1})_{i}^{e,j}\}}\) for all \(i[m],j[k_{i}]\), where \(Z_{t}\) is a normalization factor such that \(_{i=1}^{m}_{j=1}^{k_{i}}D_{t+1}(i,j)=1\)
11:for\(o 1\) to \(O\)do
12:for\(j 1\) to \(k_{o}\)do
13: Autoregressively generate the \(j^{}\) token of the "pseudo-answer" \(}_{o}^{h,j}^{}(_{t=1}^{T}_{t} (h_{}^{t}([_{o}^{h},}_{o}^{h,1:j-1}])))\), where \(^{}\) denotes the simplex on the vocabulary
14: Train weak-to-strong student model \(f_{}()\) on \(\{(_{o}^{h},}_{o}^{h})\}_{o=1}^{O}\) ```

**Algorithm 1****Main Algorithm: EnsemW2S-AdBaBoost**

Using this aggregated distribution, we sample the final predicted token. The process is autoregressive, where the \(j^{}\) token of the "pseudo-answer" is generated as

\[}_{o}^{h,j}^{}(_{t=1}^{T}_ {t}(h_{}^{t}([_{o}^{h},}_{o}^{h,1:j-1}])))\] (1)

where \(^{}\) represents the simplex over the vocabulary.

By combining the outputs of multiple experts, each trained in different EnsemW2S-AdBaBoost rounds, the ensemble approach leverages diverse perspectives from the weak models. Each expert contributes its learned strengths, and through weighted aggregation, we diminish the influence of models that are less confident or less effective on certain tokens. This helps reduce variance in the generation process, ensuring that errors from individual weak models are mitigated. The result is a more robust pseudo-labeling system that is better aligned with the true distribution of the hard data, often yielding a performance improvement over any single weak model.

Unlike classification, where scores are combined over a fixed set of classes, generation tasks involve predicting sequences of tokens, where each prediction affects future ones. This makes combining generation probabilities more complex, as errors in early token predictions can propagate throughout the sequence. Additionally, we are aggregating probability distributions over large vocabularies, which introduces computational overhead and potential numerical instability.

Our method addresses these challenges by using a weighted combination of expert models' token probabilities, ensuring that weaker predictions from individual rounds are minimized. By normalizing the aggregated distribution for each token, we maintain valid probability distributions across the vocabulary, effectively reducing the risk of cascading errors during autoregressive generation. This ensemble approach results in a more stable and accurate generation process, mitigating the issues inherent in sequence modeling.

**Pseudo answer generation on multiple-choice datasets:** On multiple-choice datasets, instead of using generated tokens \(}^{h}\) as pseudo answers, we can select one of the choices in the MCQ datasetusing negative log-likelihood (NLL). Specifically, we calculate the NLL between the choices and \(}^{h}\) and select the choice with the lowest NLL. For datasets without multiple choices, we can directly use \(}^{h}\). For **ablation studies** on our method refer Appendix section G.2

**Train W2S Model:** The strong student model, \(f_{}()\), is trained using pseudo answers generated for the hard data \(\{(_{o}^{h},}_{o}^{h})\}_{o=1}^{O}\). While it might be beneficial to include the labeled easy data in the training process, we adhere to the pipeline established by Burns et al. (2023) by focusing exclusively on the hard examples to maintain consistency.

**Evaluation Metric.** We used two metrics to evaluate this Q/A dataset. One is **(1) Token-wise comparison**, where we compare each predicted token and average the total error, and **(2) Option-wise comparison**, where we compare the negative log-likelihood (NLL) of the correct answer completion with the NLLs of the incorrect answer completions. Accuracy represents the number of entries where the correct answer completion has the lowest NLL among all choices.

## 4 Experimental Setup

We test two strategies for each task. The first, following Burns et al. (2023), randomly splits the training data into train-weak and train-strong. Train-weak is used to train the weak model. Train-strong is used to train the strong and transfer models using pseudo labels generated using the weak model. The second strategy splits the data into easy (train-weak) and hard (train-strong) subsets, with the same training pipeline, offering a more realistic w2s generalization setup, as discussed in Section 1. Both strategies aim to recover the performance gap (PGR) and maximize the strong model's capability using an ensemble of weak models. The baseline in all experiments uses a single model for w2s generalization, following the principle of Burns et al. (2023). More details in appendix sec G.3.

### Binary Classification Task

**W2S Results with Random Training Data Splits.** The baseline of this method is a replication of Burns et al. (2023). From Figure 2, by applying AdaBoost, we observe a significant improvement in the weak model accuracy, significantly improving the PGR values. In the case of the GPT-2-medium to GPT-2-large pair, we even see the PGR exceeding 100%, meaning that the transfer model has

Figure 2: **Binary Classification Task: Top figure** shows a bar plot comparing w2s generalization of our method (grey) with a baseline (blue) from Burns et al. (2023) using accuracy values(%) for different combinations of weak and strong model pairs for random data split (top bar-plot) and easy-hard split(bottom bar-plot). **Bottom figure** shows a line plot comparing the accuracy and performance gap recovered values (PGR). The left two figures are for random data split, while the right two figures are for the easy-hard split to show e2h generalization.

outperformed the strong model's performance. This is the ambitious aim of the w2s generalization problem, and our results show that w2s generalization is achievable.

**W2S Results with Easy and Hard Training Data Splits.** From Figure 2, we see that applying AdaBoost significantly improves weak model accuracy, thereby enhancing the PGR values. However, for this holistic e2h generalization problem, we are far from reaching the full capability of a strong model. For very small (GPT-2) and large model pairs (GPT-2-xl and above), we do not see improvement in w2s generalization despite the weak models' accuracy improvements. Overall, we observe an improvement of up to 14% in accuracy compared to the baseline and an average improvement of 6.52% and 3% for random and easy-hard splits, respectively.

**Scaling Law:** In Figure 2 (line plot), we see less PGR recovery for the Qwen-1.8B model even though it is similar in size to GPT-2-xl. Similarly, in the bar plot, we see a drastic difference between the oracle performance of GPT2xl and Qwen-1.8B. This is because the Qwen models series are more capable even after being the same size. Thus, model size is not a good metric, but model capability is a better metric for differentiating between weak and strong models.

**Better metric:** Figure 2 shows the accuracy and PGR plots for both random and easy-hard split. We observe that PGR is not very informative, as it can produce extremely large or even negative values. In the w2s experiments, large values occur because the ensemble of weak models becomes strong enough to match or exceed a strong model, improving w2s generalization. Negative values, seen in baseline experiments, indicate the transfer model performed worse than the weak model, often when the strong model fails to learn and its inductive bias becomes random with pseudo-label training. Similar patterns are seen in Figure 15 and 4. (Refer to Appendix Table 1 and 2 for more details.)

### Generation Task for Multiple Choice Dataset

#### 4.2.1 Comparing Weak model's performance

In Figure 3, we compare the performance of a single weak model (dark color) with combined weak models after 5 rounds of EnsemW2S-AdaBoost. Smaller models show greater improvement, which is expected since boosting works best when weak models are diverse. Using EnsemW2S-AdaBoost, smaller models can diversify through the data sampling step; however, larger models tend to learn all possible information and cannot learn something different with each round. We use token-error here since it's a more precise metric to measure improvement in weak models.

#### 4.2.2 Comparing Strong model's performance

Here, we use the multiple-choice classification accuracies to calculate the accuracy of all our plots. We show the accuracy values of token-wise metrics in the Appendix tables.

**W2S Results with Random Training Data Splits.** From Figure 4 and 15, we see that w2s training using an ensemble of experts almost consistently outperforms the baseline (single expert). Thus, ensemble learning is beneficial. We can see the trend of accuracy and performance gap recovered for the different model pairs in Figure 4 and 15 for Quartz and ARC datasets, respectively. For Quartz data, we see that our PGR percentage (Figure 4) improves as the model scales up except when the weak model is the smallest sized model (pythia-70m). This could be because the increasing capability difference between the small and large models makes it difficult for the strong model to learn anything from the weak. This trend is the same in the baseline as well as our EnsemW2S. But an important thing to note is that for some cases for both ARC and Quartz data, our method generates a large PGR percentage of >=100%, showing the ability of our w2s method to recover the performance gap.

**W2S Results with Easy-Hard Training Data Splits.** From Figure 4 and 15, we see that w2s training using an ensemble of experts almost consistently outperforms the baseline (single expert). Thus showing that ensemble learning is beneficial. Our method shows more improvement over baseline for easy-hard data split as compared to random split. This is because of two reasons. Firstly, the power of combining weak models using our modified AdaBoost is more useful when all of them are weak

Figure 3: Performance comparison of a single weak model (dark color) with the combined weak models (Lighter hue shows improvement).

but slightly different from each other. Secondly, by easy and hard splitting, the margin between weak and strong increases more, giving more room for improvement.

We also observe that PGR for e2h generalization is significantly lower, highlighting the complexity of the e2h generalization problem. We hope this work could motivate researchers to build more sophisticated methods for this more complex e2h generalization problem. Another simple observation is as the models become more capable, both the performances (baseline and ours) increase.

Note: Refer to Appendix Table 3 and 6 for detailed values of our experiments on the Quartz and ARC datasets with random data splits. Bar plots of weak and strong (oracle) model performance for these splits are shown in Appendix Figure 13 and 16. For easy-hard data split, the same details can be found in Appendix Tables 4, 7 and Figure 14 and 17.

#### 4.2.3 Performance on hard data after training on weak vs strong data

This experiment highlights the importance of e2h with w2s generalization. In Table 5, the Quartz dataset shows significant improvement for larger models when trained on hard data, indicating their better ability to understand complex data. For ARC, all models improve but with a smaller margin, suggesting less disparity between easy and hard samples in the ARC dataset.

## 5 Conclusion

This paper aims to stimulate discussion on the more holistic problem of w2s generalization by emphasizing e2h generalization. We develop a new AdaBoost-inspired algorithm and conduct a thought experiment on how to combine the "wisdom of the crowd" to improve w2s generalization. We are first to focus on the idea of making the weaks less weak using an ensemble, and test our method for complex SFT tasks. Our method in some cases recovers full strong model capability.

Figure 4: **Generation Task (Quartz Data): Top figure** shows a bar plot comparing the w2s generalization of our method (grey) with a baseline (blue) for various combinations of weak and strong model pairs for the SFT task on Q/A data for random data split (top bar-plot) and easy-hard split (bottom bar-plot). **Bottom figure** shows a line plot comparing accuracy and PGR. The left two figures are for random data split, while the right two are for the easy-hard split to show e2h generalization.

Figure 5: Accuracy (%) values for LLMs trained on easy vs hard data and evaluated on hard data.