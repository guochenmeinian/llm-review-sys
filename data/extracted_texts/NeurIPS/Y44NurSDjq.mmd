# Quantum Bayesian Optimization

Zhongxiang Dai*\({}^{1}\), Gregory Kang Ruey Lau*\({}^{1,2}\), Arun Verma\({}^{1}\), Yao Shu\({}^{1}\), Bryan Kian Hsiang Low\({}^{1}\), Patrick Jaillet\({}^{3}\)

\({}^{1}\)Department of Computer Science, National University of Singapore

\({}^{2}\)CNRS@CREATE, 1 Create Way, #08-01 Create Tower, Singapore 138602

\({}^{3}\)Department of Electrical Engineering and Computer Science, MIT

dzx@nus.edu.sg, greglau@comp.nus.edu.sg, arun@comp.nus.edu.sg, shuyao95@u.nus.edu, lowkh@comp.nus.edu.sg, jaillet@mit.edu

* Equal contribution.

###### Abstract

_Kernelized bandits_, also known as _Bayesian optimization_ (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their _cumulative regret_ which are sub-linear in the number \(T\) of iterations, and a regret lower bound of \(()\) has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on _quantum bandits_ have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the _quantum-Gaussian process-upper confidence bound_ (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB _is the first BO algorithm able to achieve a regret upper bound of \(( T)\)_, which is significantly smaller than its regret lower bound of \(()\) in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment _using a real quantum computer_, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.

## 1 Introduction

_Kernelized bandits_, also named _Bayesian optimization_ (BO) [10; 11; 16; 19; 32], has been an immensely popular method for various applications involving the optimization of complicated black-box reward functions. For example, BO has been extensively used to optimize the hyperparameters of machine learning (ML) models [12; 14; 22], the parameters of computationally expensive simulators , etc. In every iteration \(t=1,,T\), BO chooses an arm/input \(x_{t}\) and then queries the reward function \(f\) for a noisy observation \(y_{t}=f(x_{t})+_{t}\) where \(_{t}\) is a sub-Gaussian noise. In addition to its impressive practical performance, BO is also equipped with solid theoretical guarantees. A number of commonly adopted BO algorithms have been theoretically shown to enjoy sub-linear upper bounds on their _cumulative regret_[9; 33], which ensures that they are guaranteed to be able to find the global optimum of the reward function as \(T\) (i.e., the total number of function queries) increases. On the other hand, a lower bound of \(()\) (ignoring additional log factors) on the cumulative regret of BO has been shown , which represents the fundamental limit of any BO algorithm (in the classical setting). In other words, no classical BO algorithm can achieve a cumulative regret smaller than \(()\). This naturally begs the question: _can we leverage more advanced technology to go beyondthe classical setting and hence break this fundamental limit of \(()\)?_ In this work, we give an affirmative answer by showing that this can be achieved with the aid of _quantum computing_.

Quantum bandits, which incorporates quantum computing into bandit algorithms, has been studied by a number of recent works . Notably, the recent work of  has introduced quantum variants of classical algorithms for multi-armed bandits (MAB) and stochastic linear bandits (SLB). In the setting of quantum bandits adopted by , every query to the reward function \(f\) at the arm \(x_{t}\) (in the classical setting) is replaced by a chance to access a _quantum oracle_, which encodes the reward distribution for the arm \(x_{t}\). For every selected arm \(x_{t}\),  has proposed to adopt the _quantum Monte Carlo_ (QMC) algorithm  as a subroutine to obtain an accurate estimation of \(f(x_{t})\) in an efficient way, i.e., using a small number of queries to the quantum oracle (Lemma 1). As a result,  has shown that the regrets of the quantum algorithms for both MAB and SLB are significantly improved compared with their classical counterparts and are smaller than their classical lower bounds. However, both MAB and SLB fall short when optimizing complex real-world functions (e.g., optimizing the hyperparameters of ML models). This is because MAB is unable to exploit the correlations among the arms to model the reward function, and the assumption of a linear reward function adopted by SLB is usually too restrictive in practice. Therefore, designing a quantum bandit algorithm capable of optimizing sophisticated non-linear reward functions, which is a critical step towards practically useful quantum bandit algorithms, is still an open problem. In this work, we resolve this open problem by proposing the first algorithm for quantum BO.

Similar to , in our quantum BO problem setting, queries to the reward function in the classical setting are replaced by access to a quantum oracle encoding the reward distribution. As discussed in , a quantum oracle is available when the learning environment is implemented by a quantum algorithm, which makes the setting of our quantum BO fairly general. For example, our quantum BO algorithm may be used to optimize the hyperparameters of quantum ML algorithms , such as quantum support vector machines (SVMs) , quantum neural networks (NNs) , among others. It may also be used to optimize the parameters of simulators implemented on a quantum computer, or for applications involving quantum systems where the data produced is inherently quantum. Moreover, our quantum BO algorithm could also be applied to classical data and algorithms, because as discussed in , any classical computer program can be converted to a quantum circuit, allowing it to serve as a quantum oracle in our quantum BO algorithm.

In this work, we introduce the first quantum BO algorithm: _quantum-Gaussian process-upper confidence bound_ (Q-GP-UCB). For every selected arm \(x_{s}\), our Q-GP-UCB algorithm (Algo. 1) adopts the QMC subroutine (Lemma 1) to efficiently estimate the corresponding reward function value \(f(x_{s})\) to satisfy a target estimation error \(_{s}\). Every arm \(x_{s}\) is selected based on our _weighted_ GP posterior distribution, in which every previously selected arm \(x_{s}\) is given a weight of \(1/_{s}^{2}\) which is inversely related to its estimation error. The theoretical analysis of our Q-GP-UCB is faced with non-trivial technical challenges, and our contributions can be summarized as follows:

* We analyze the growth rate of the _weighted information gain_ (Sec. 5.1) which arises due to the use of our weighted GP regression (Sec. 4.1), and show its connection with the standard maximum information gain commonly used in the analysis of BO/kernelized bandits . Our analysis here may be of broader independent interest for future works adopting weighted GP regression.
* We derive a tight confidence ellipsoid which gives a guarantee on the concentration of the reward function around the weighted GP posterior mean (Sec. 5.3), and discuss the intuition behind our proof which corresponds to an interesting interpretation about our Q-GP-UCB algorithm. A particularly crucial and novel step in our analysis relies on the recognition to apply the concentration inequality for 1-sub-Gaussian noise (Sec. 5.3).
* We prove that our Q-GP-UCB achieves a regret upper bound of \(( T)\) for the commonly used _squared exponential_ (SE) kernel (Secs. 5.4 and 5.5), which is considerably smaller than the classical regret lower bound of \(()\) and hence represents a significant quantum speedup.
* By using a similar proof technique to the one adopted for our tight confidence ellipsoid (Sec. 5.3), we improve the confidence ellipsoid for the quantum linear UCB (Q-LinUCB) algorithm . This improved analysis leads to _a tighter regret upper bound for Q-LinUCB_, which matches the regret of our Q-GP-UCB with the linear kernel (Sec. 5.6).
* We use simulations implemented based on the Qiskit package to verify the empirical improvement of our Q-GP-UCB over the classical GP-UCB and over Q-LinUCB, through a synthetic experiment and an experiment on _automated machine learning_ (AutoML) (Sec. 6). Notably, we have also performed an experiment _using a real quantum computer_, in which our Q-GP-UCB still achieves a consistent performance advantage (Fig. 4, App. J)

## 2 Related Work

A number of recent works have studied bandit algorithms in the quantum setting. The works of  and  have focused on the problem of pure exploration in quantum bandits.  has studied a different problem setting where the learner aims to maximize some property of an unknown quantum state (i.e., the rewards) by sequentially selecting the observables (i.e., actions). The recent work of  has introduced algorithms for quantum multi-armed bandits (MAB) and stochastic linear bandits (SLB), and has shown that by incorporating the QMC subroutine into MAB and SLB, tight regret upper bounds can be achieved which are better than the classical lower bounds. More recently, the works of  and  have followed similar approaches to introduce quantum bandit algorithms for, respectively, stochastic convex bandits and bandits with heavy-tailed reward distributions. In addition to quantum bandits, some recent works have introduced quantum reinforcement learning (RL) algorithms.  has assumed access to a generative model of the environment and proved that their quantum RL algorithm achieves significantly better sample complexity over their classical counterparts. More recently, [17; 43] have removed the requirement for a generative model in quantum RL and achieved quantum speedup in terms of the regret. More comprehensive reviews of the related works on quantum RL can be found in recent surveys (e.g., ). The problem setting of our quantum BO (Sec. 3.2) is the same as many of these above-mentioned previous works on quantum bandits [38; 41] and quantum RL [17; 39; 43]. However, to the best of our knowledge, none of the existing works on quantum bandits (and quantum RL) is able to handle non-linear reward functions, which we resolve in this work. Over the years, extensive efforts [7; 24; 30; 36] have been made to design BO/kernelized bandit algorithms whose regret upper bounds are small enough to (nearly) match the classical regret lower bound of \(()\) (ignoring additional log factors). Our work provides an alternative route by proving that with the aid of quantum computing, it is possible to achieve a tight regret upper bound which is significantly smaller than the classical regret lower bound.

## 3 Problem Setting and Background

### Classical Kernelized Bandits

A BO/kernelized bandit algorithm aims to maximize a reward function \(f:\) where \(^{d}\), i.e., it aims to find \(x^{*}*{arg\,max}_{x}f(x)\). Consistent with the previous works on BO/kernelized bandits [9; 37], we assume that \(f\) lies in the _reproducing kernel Hilbert space_ (RKHS) associated with a kernel \(k\): \(f H_{k}()\). That is, we assume that \(_{H_{k}} B\) for \(B>0\), in which \(_{H_{k}}\) denotes the RKHS norm induced by the kernel \(k\). Note that unlike previous works on linear bandits , our assumption here allows \(f\) to be non-linear w.r.t. the input \(x\). In this work, we mainly focus on the widely used _squared exponential_ (SE) kernel: \(k(x,x^{})(-}_{2}^{2}/(2l^{2}))\) in which \(l\) is the _length scale_. We also extend our results to the Matern kernel in Sec. 5.7. Without loss of generality, we assume that \(k(,) 1\). In the following, we use \([t]\) to denote \(\{1,,t\}\) for simplicity.

In every iteration \(t[T]\) of BO, an input \(x_{t}\) is selected, after which a corresponding noisy observation \(y_{t}=f(x_{t})+_{t}\) is collected where \(_{t}\) is a sub-Gaussian noise (e.g., bounded noise or Gaussian noise). The inputs \(x_{t}\)'s are sequentially selected by maximizing an _acquisition function_, which is calculated based on the _Gaussian process_ (GP) posterior distribution. Specifically, after \(t\) iterations, we use the current history \(_{t}(\{x_{1},y_{1}\},,(x_{t},y_{t})\}\) to calculate the GP posterior predictive distribution at \(x\): \((_{t}(x),_{t}^{2}(x))\) where

\[_{t}(x) k_{t}^{}(x)(K_{t}+ I)^{-1}Y_{t},_{ t}^{2}(x) k(x,x)-k_{t}^{}(x)(K_{t}+ I)^{-1}k_{t}(x),\] (1)

in which \(k_{t}(x)[k(x_{},x)]_{[t]}^{}\) and \(Y_{t}[y_{x_{}}]_{[t]}^{}\) are column vectors, \(K_{t}[k(x_{},x_{^{}})]_{,^{}[t]}\) is the \(t t\) covariance matrix, and \(>1\) is a regularization parameter. Based on (1), a commonly used acquisition function is GP-UCB , which selects the next query by: \(x_{t+1}=*{arg\,max}_{x}_{t}(x)+_{t+1}_{ t}(x)\) where \(_{t+1}\) is a parameter carefully chosen to balance exploration and exploitation. The performance of a BO/kernelized bandit algorithm is usually theoretically analyzed by deriving an upper bound on its cumulative regret: \(R_{T}=_{t=1}^{T}[f(x^{*})-f(x_{t})]\), and a tighter regret upper bound is an indicator of a better theoretical convergence.

### Quantum Bandits/BO

A _quantum state_\(|x\) in Hilbert space \(^{n}\) can be expressed as a superposition of \(n\) basis states (e.g. qubits) via a vector \(=[x_{1},,x_{n}]^{}\), with \(|x_{i}|^{2}\) representing the probability of being in the \(i^{th}\) basis state. Given two quantum states \(|x^{n}\) and \(|y^{m}\), we use \(|x|y=[x_{1}y_{1},,x_{n}y_{m}]^{}\) to denote their tensor product. A quantum algorithm typically works by applying unitary operators to quantum states, and may have access to input data encoded in unitary operators called _quantum oracles_ (examples have been discussed in Sec. 1). We defer a more detailed introduction to quantum computing to related works dedicated to these topics (e.g., ).

Our quantum BO setting follows that of the quantum bandits works in [38; 41]. In every iteration of quantum BO, after an input \(x\) is selected, instead of observing a noisy reward as in classical BO/bandits (Sec. 3.1), we get a chance to access a quantum unitary oracle \(_{x}\) and its inverse that encode the noisy reward distribution. Specifically, let \(P_{x}\) denote the reward distribution, \(_{x}\) denote the finite sample space of \(P_{x}\), and \(y_{x}:_{x}\) denote the random reward associated with input \(x\). Then, \(_{x}\) is formally defined as:

\[_{x}:|0_{_{x}}( )}||y_{x}().\] (2)

_Quantum mean estimation_, which aims to estimate the mean of an unknown distribution with better sample efficiency than classical algorithms, has been a widely studied problem [4; 21]. Consistent with the works of [38; 41], we will make use of the following _quantum Monte Carlo_ (QMC) algorithm:

**Lemma 1** (Quantum Monte Carlo (QMC) ).: _Let \(y_{x}:_{x}\) denote a random variable, \(_{x}\) is equipped with probability measure \(P_{x}\), and the quantum unitary oracle \(_{x}\) encodes \(P_{x}\) and \(y_{x}\)._

* _Bounded Noise__: If the noisy output observation satisfies_ \(y_{x}\)_, then there exists a constant_ \(C_{1}>1\) _and a QMC algorithm_ \((_{x},,)\) _which returns an estimate_ \(_{x}\) _of_ \([y_{x}]\) _such that_ \((|_{x}-[y_{x}]|>)\)_,_ _using at most_ \(}{}(1/)\) _queries to_ \(_{x}\) _and its inverse._
* _Noise with Bounded Variance__: If the variance of_ \(y_{x}\) _is_ \(^{2}\)_, then for_ \(<4\)_, there exists a constant_ \(C_{2}>1\) _and a QMC algorithm_ \((_{x},,)\) _which returns an estimate_ \(_{x}\) _s.t._ \((|_{x}-[y_{x}]|>)\)_, using at most_ \(}{}_{2}^{3/2}( )_{2}(_{2})\) _queries to_ \(_{x}\) _and its inverse._

The _quantum query complexity_ of a quantum algorithm is usually measured by _the number of queries to the quantum oracle_ [3; 38]. So, the sample complexities from Lemma 1 can be compared with that of classical Monte Carlo (MC) estimation. In the classical setting, it can be easily shown using concentration inequalities that MC estimation requires \(}(1/^{2})\) samples to reach a target mean estimation error of \(\). Therefore, the QMC algorithm (Lemma 1), which only needs \(}(1/)\) samples, achieves a quadratic reduction in the required number of samples. This dramatic improvement is crucial for the quantum speedup in terms of regrets achieved by our algorithm (Sec. 5.4).

During our Q-GP-UCB algorithm, after every input \(x_{s}\) is selected, we will apply the QMC algorithm from Lemma 1 with the quantum oracle \(_{x_{s}}\) to obtain an estimation \(y_{s}\) of its reward \(f(x_{s})\) (line 6 of Algo. 1). Of note, the above-mentioned equivalence between a query to the quantum oracle and a sample for classical MC mean estimation implies that _a query to the quantum oracle \(_{x}\) in quantum BO/bandits is equivalent to the pulling of an arm \(x\) in classical BO/bandits_. Therefore, the cumulative regret \(R_{T}\) of our Q-GP-UCB algorithm is defined as the regret incurred after \(T\) queries to the quantum oracle, which makes it amenable to comparisons with the regrets of classical algorithms.

## 4 Quantum Bayesian Optimization

In this section, we first introduce weighted GP regression (Sec. 4.1) which will be used by our Q-GP-UCB algorithm to calculate the acquisition function for input selection, and then describe our Q-GP-UCB algorithm (Sec. 4.2) in detail.

### Weighted GP Posterior Distribution

In contrast to standard GP-UCB  which uses the standard GP posterior distribution (1) to calculate the acquisition function, our Q-GP-UCB makes use of a _weighted_ GP posterior distribution. That is, we assign a weight to every previous observation. Specifically, after stage \(s\) of our Q-GP-UCB (i.e., given \(_{s}\{(x_{},y_{})\}_{[s]}\)), we define the weight matrix \(W_{s}(1/_{1}^{2},,1/_{s}^{2})\). \(W_{s}\) isan \(s s\) diagonal matrix, in which the \(^{}\) diagonal element represents the weight \(1/_{}^{2}\) given to the \(^{}\) observation \((x_{},y_{})\). We will set \(_{}=_{-1}(x_{})/\) (Sec. 4.2), i.e., \(_{}\) is calculated using the weighted GP posterior standard deviation (3) (conditioned on the first \(-1\) observations) at \(x_{}\).

Define \(K_{s}[k(x_{},x_{^{}})]_{,^{}[s]}\) which is the \(s s\)-dimensional covariance matrix given the first \(s\) observations, and define \(_{s} W_{s}^{1/2}K_{s}W_{s}^{1/2}\) which is the _weighted_ covariance matrix. Similarly, define \(k_{s}(x)[k(x,x_{})]_{=[s]}^{}\) and \(_{s}(x) W_{s}^{1/2}k_{s}(x)\). Denote the collection of output observations by \(Y_{s}[y_{}]_{[s]}^{}\), and define \(_{s} W_{s}^{1/2}Y_{s}\). With these definitions, given \(_{s}\), our weighted GP posterior distribution at an input \(x\) is a Gaussian distribution: \((_{s}(x),_{s}^{2}(x))\), in which

\[_{s}(x)_{s}^{}(x)(_{s }+ I)^{-1}_{s},_{s}^{2}(x)  k(x,x)-_{s}^{}(x)(_{s}+ I)^{- 1}_{s}(x).\] (3)

Note that the GP posterior mean \(_{s}\) above is equivalently the solution to the following weighted kernel ridge regression problem: \(_{s}=_{f H_{k}(X)}_{=1}^{s}^{2}}(y_{}-f(x_{}))^{2}+\|f\|_{H_{k}}^{2}\). We give a more detailed analysis of the weighted GP posterior (3) in App. A. Weighted GP regression has also been adopted by previous works on BO such as . However, our choice of the weights, algorithmic design and theoretical analyses all require significantly novel treatments.

### Q-GP-UCB Algorithm

```
1:for stage \(s=1,2,\)do
2:\(x_{s}=_{x}_{s-1}(x)+_{s}_{s-1}(x)\) (3).
3:\(_{s}=_{s-1}(x_{s})/\).
4:
4:
5:
6:
7: Update the weighted GP posterior (3) using \((x_{s},y_{s})\). ```

**Algorithm 1** Q-GP-UCB

Our Q-GP-UCB algorithm is presented in Algo. 1. Q-GP-UCB proceeds in stages. In stage \(s\), we first select the next input \(x_{s}\) to query by maximizing the GP-UCB acquisition function calculated using the weighted GP posterior distribution (3) (line 2 of Algo. 1). Here \(_{s} B+_{s-1}+1+(2/))}\) (more details in Sec. 5.3), in which \((0,2/e]\) and \(_{s-1}((I+_{s-1}))\) is the _weighted information gain_ (more details in Sec. 5.1). Next, we calculate \(_{s}=_{s-1}(x_{s})/\) (line 3) and \(N_{_{s}}\) (line 4), in which \(N_{_{s}}\) depends on the type of noise and the value of \(_{s}\). Here \(\) is an upper bound on the total number \(m\) of stages which we will analyze in Sec. 5.2. Subsequently, unless the algorithm is terminated (line 5), we run the QMC algorithm (Lemma 1) to estimate \(f(x_{s})\) by querying the quantum oracle of \(x_{s}\) for \(N_{_{s}}\) rounds (line 6). The QMC procedure returns an estimate \(y_{s}\) of the reward function value \(f(x_{s})\), for which the estimation error is guaranteed to be bounded: \(|y_{s}-f(x_{s})|_{s}\) with probability of at least \(1-/(2)\). Lastly, we update the weighted GP posterior (3) using the newly collected input-output pair \((x_{s},y_{s})\), as well as its weight \(1/_{s}^{2}\) (line 7).

Of note, the value of \(_{s}\) is used for both **(a)** calculating the number \(N_{_{s}}\) of queries to the quantum oracle for \(x_{s}\), and **(b)** computing the weight \(1/_{s}^{2}\) assigned to \((x_{s},y_{s})\) in the weighted GP regression (3) in the subsequent iterations. Regarding **(a)**, our designs of \(_{s}\) and \(N_{_{s}}=}(1/_{s})\) have an interesting interpretation in terms of the exploration-exploitation trade-off: In the initial stages, the weighted GP posterior standard deviation \(_{s-1}(x_{s})\) and hence \(_{s}\) are usually large, which leads to a small number \(N_{_{s}}\) of queries for every \(x_{s}\) and hence allows our Q-GP-UCB to favor the _exploration_ of more unique inputs; in later stages, \(_{s-1}(x_{s})\) and \(_{s}\) usually become smaller, which results in large \(N_{_{s}}\)'s and hence causes our Q-GP-UCB to prefer the _exploitation_ of a small number of unique inputs. Regarding **(b)**, assigning a larger weight \(1/_{s}^{2}\) to an input \(x_{s}\) with a smaller \(_{s}\) is reasonable, because a smaller \(_{s}\) indicates a smaller estimation error for \(y_{s}\) as we explained above, which makes the observation \(y_{s}\) more accurate and reliable for calculating the weighted GP regression (3).

Note that we have modified the original GP-UCB by querying every selected input \(x_{s}\) multiple times, in order to make it amenable to the integration of the QMC subroutine. The recent work of  has also adapted BO to evaluate a small number of unique inputs while querying each of them multiple times. It has been shown  that the resulting BO algorithm preserves both the theoretical guarantee (i.e., the regret upper bound) and empirical performance of the original BO, while significantly reducing the computational cost. So, the findings from  can serve as justifications for our modification to GP-UCB. Also note that despite this similarity,  still focuses on the traditional setting (i.e., their regret upper bound is \(R_{T}=(_{T})\)) and their regret analyses are entirely different from ours.

## 5 Theoretical Analysis

Throughout our analysis, we condition on the event that \(|f(x_{s})-y_{s}|_{s}, s=1,,m\). This event holds with probability of at least \(1-/2\), because we set the error probability in QMC (Lemma 1) to \(/(2)\) in which \( m\) (see Theorem 2 for details). For simplicity, we mainly focus on the scenario of bounded noise (i.e., the observations are bounded within \(\), the first case of Lemma 1), because the theoretical analyses and insights about the other scenario of noise with bounded variance (i.e., the second case of Lemma 1) are almost identical (more details in Sec. 5.5).

### Weighted Information Gain

To begin with, the following lemma (proof in App. B) gives an upper bound on the sum of the weights in all \(m\) stages, which will be useful for upper-bounding the weighted information gain \(_{m}\).

**Lemma 2**.: _Choose \((0,2/e]\), then we have that \(_{s=1}^{m}1/_{s}^{2} T^{2}\)._

Denote by \(_{T}\) the maximum information gain from any set of \(T\) inputs: \(_{T}_{\{x_{1},,x_{T}\} X} (I+K_{T})\), which is commonly used in the analysis of BO/kernelized bandit algorithms [9; 33]. Denote by \(_{m}\) the _weighted information gain_ from all \(m\) selected inputs in all stages: \(_{m}(I+ _{m})\). Note that in the definition of \(_{m}\), we do not take the maximum over all sets of \(m\) inputs, so \(_{m}\) still depends on the selected inputs \(_{m}\{x_{1},,x_{m}\}\). Also note that \(\{_{1},,_{m}\}\) are known constants conditioned on \(_{m}\). This is because the weighted GP posterior standard deviation \(_{s-1}\) (3) does not depend on the output observations, so every \(_{s}=_{s-1}(x_{s})/\) only depends on \(\{x_{1},,x_{s}\}\). The next theorem gives an upper bound on \(_{m}\).

**Theorem 1**.: _(a) Given \(_{m}\{x_{1},,x_{m}\}\), the growth rate of \(_{m}\) is the same as that of \(_{_{s=1}^{m}1/_{s}^{2}}\). (b) \(_{m}_{T^{2}}\)._

We give the complete statement of result _(a)_ in Theorem 8 (App. C), which presents the concrete growth rate. As stated by result _(a)_, to obtain an upper bound on \(_{m}\), we simply need to firstly use the upper bound on \(_{T}\) from previous works , and then replace the \(T\) in this upper bound by \(_{s=1}^{m}1/_{s}^{2}\). Based on this, the result _(b)_ has further upper-bounded \(_{s=1}^{m}1/_{s}^{2}\) by \(T^{2}\) through Lemma 2. Note that because \(_{s}=_{s-1}(x_{s})/ 1\), we have that \(1/_{s}^{2} 1\) and hence \(_{s=1}^{m}1/_{s}^{2} m\). Therefore, our weighted information gain \(_{m}\) has a looser upper bound than \(_{m}\). This intuitively implies that our weighted covariance matrix \(_{m}\) is expected to _contain more information than the original \(K_{m}\)_, whose implication will be discussed again in Sec. 5.2. The proof of Theorem 1 (App. C) has followed the analysis of the information gain from . Specifically, we have leveraged a finite-dimensional projection of the infinite-dimensional RKHS feature space, and hence upper-bounded the information gain in terms of the tail mass of the eigenvalues of the kernel \(k\). The proof requires carefully tracking the impact of the weights \(1/_{s}^{2}\) throughout the analysis. Importantly, our Theorem 1 and its analysis may be _of wider independent interest_ for future works which also adopt weighted GP regression (Sec. 4.1).

The growth rate of \(_{T}\) has been characterized for commonly used kernels . Based on these, the next corollary provides the growth rates of \(_{m}\) for the linear and squared exponential (SE) kernels.

**Corollary 1**.: _For the linear and squared exponential (SE) kernels, we have that_

\[_{m} =(d_{s=1}^{m}1/_{s}^{2})=(d  T)\] \[_{m} =(^{d+1}_{s=1}^{m}1/_{s}^{2})= (( T)^{d+1})\]

### Upper Bound on The Total Number \(m\) of Stages

The next theorem gives an upper bound on the total number of stages of our Q-GP-UCB algorithm.

**Theorem 2**.: _For the linear kernel, our Q-GP-UCB algorithm has at most \(m=(d T)\) stages. For the SE kernel, our Q-GP-UCB algorithm has at most \(m=(( T)^{d+1})\) stages._

Note that for the linear kernel, our upper bound on \(m\) matches that of the Q-LinUCB algorithm from  (see Lemma 2 from ). The proof of Theorem 2 is given in App. D, and here we give a brief sketch of the proof. For stage \(s\), define \(V_{s} I+_{=1}^{s}(1/_{}^{2})(x_{} )(x_{})^{}\), in which \(()\) is the (potentially infinite-dimensional) RKHS feature mapping: \(k(x,x^{})=(x)^{}(x^{})\). Intuitively, \( V_{s}\) represents the amount of information contained in the first \(s\) selected inputs \(\{x_{1},,x_{s}\}\). As the first step in our proof, we prove that thanks to our choice of \(_{s}\) (line 3 of Algo. 1) and our design of the weights \(1/_{s}^{2}\), the amount of information \( V_{s}\) is doubled after every stage: \( V_{+1}=2 V_{},=0,,m-1\). This allows us to show that \(m 2=((V_{m})/(V_{0}))\) where \(V_{0}= I\). Next, we show that this term is also related to our weighted information gain: \(((V_{m})/(V_{0}))=2_{m}\), which allows us to upper-bound \(m\) in terms of \(_{m}\) and hence in terms of \(_{s=1}^{m}1/_{s}^{2}\) (see Corollary 1). This eventually allows us to derive an upper bound on \(m\) by using the fact that the total number of iterations (i.e., queries to the quantum oracles) is no larger than \(T\). Therefore, the key intuition of the proof here is that as long as we gather a sufficient amount of information in every stage, we do not need a large total number of stages.

### Confidence Ellipsoid

The next theorem proves the concentration of \(f\) around the weighted GP posterior mean (3).

**Theorem 3**.: _Let \( 2/T\), \( 1+\), and \(_{s} B+_{s-1}+1+(2/))}\). We have with probability of at least \(1-/2\) that_

\[|f(x)-_{s-1}(x)|_{s}_{s-1}(x),  s[m],x.\]

The proof of Theorem 3 is presented in App. E, and here we give a brief proof sketch. Denote by \(_{s-1}\) the \(\)-algebra \(_{s-1}=\{x_{1},,x_{s},_{1},,_{s-1}\}\) where \(_{k}=y_{k}-f(x_{k})\) is the noise. Recall that \(W_{s}^{1/2}(1/_{1},,1/_{s})\) (Sec. 4.1), and define \(_{1:s}[_{k}]_{k[s]}\). In the first part of the proof, we use the RKHS feature space view of the weighted GP posterior (3) (see App. A) to upper-bound \(|f(x)-_{s-1}(x)|\) in terms of \(_{s-1}(x)\) and \(||W_{s}^{1/2}_{1:s}||_{((_{s}+ I)^{-1}+I)^{-1}}\). Next, the most crucial step in the proof is to upper-bound \(||W_{s}^{1/2}_{1:s}||_{((_{s}+ I)^{-1}+I)^{-1}}\) in terms of \(_{s}\) by applying the self-normalized concentration inequality from Theorem 1 of  to _1-sub-Gaussian noise_. This is feasible because _(a)_ every \(_{s}=_{s-1}(x_{s})/\) is \(_{s-1}\)-measurable, and _(b)_ conditioned on \(_{s-1}\), the scaled noise term \(_{s}/_{s}\) (in \(W_{s}^{1/2}_{1:s}\)) is \(1\)-sub-Gaussian. The statement _(a)_ follows because \(_{s}\) only depends on \(\{x_{1},,x_{s}\}\) as we have discussed in Sec. 5.1. The statement _(b)_ follows since our QMC subroutine ensures that every noise term \(_{k}\) is bounded within \([-_{k},_{k}]\) (Sec. 4.2) because \(|y_{k}-f(x_{k})|_{k}\) (with high probability). This suggests that conditioned on \(_{s-1}\) (which makes \(_{s}\) a known constant as discussed above), \(_{s}/_{s}\) is bounded within \([-1,1]\) and is hence 1-sub-Gaussian.

This critical step in the proof is in fact also consistent with an interesting interpretation about our algorithm. That is, after \(x_{s}\) is selected in stage \(s\), we adaptively decide the number \(N_{_{s}}\) of queries to the quantum oracle in order to reach the target accuracy \(_{s}\) (line 4 of Algo. 1). This ensures that \(|y_{s}-f(x_{s})|_{s}\) (with high probability), which guarantees that the noise \(_{s}=y_{s}-f(x_{s})\) is (conditionally) \(_{s}\)-sub-Gaussian. Moreover, note that the vector of observations \(_{s}\) used in the calculation of the weighted GP posterior mean (3) is \(_{s}=W_{s}^{1/2}Y_{s}\). So, from the perspective of the weighted GP posterior (3), every noisy observation \(y_{s}\), and hence every noise \(_{s}\), is multiplied by \(1/_{s}\). So, the effective noise of the observations \(_{s}\) used in the weighted GP posterior (i.e., \(_{s}/_{s}\)) is \(_{s}/_{s}=1\)-sub-Gaussian. This shows the underlying intuition as to why our proof of the concentration of the reward function \(f\) using the weighted GP regression can make use of _1-sub-Gaussian noise_ (e.g., in contrast to the \(\)-sub-Gaussian noise used in the proof of classical GP-UCB ).

Our tight confidence ellipsoid around \(f\) from Theorem 3 is crucial for deriving a tight regret upper bound for our Q-GP-UCB (Sec. 5.4). Moreover, as we will discuss in more detail in Sec. 5.6, our proof technique for Theorem 3, interestingly, can be adopted to obtain a tighter confidence ellipsoid for the Q-LinUCB algorithm from  and hence improve its regret upper bound.

### Regret Upper Bound

Here we derive an upper bound on the cumulative regret of our Q-GP-UCB algorithm.

**Theorem 4**.: _With probability of at least \(1-\), we have that1_

\[R_{T} =d^{3/2}( T)^{3/2}(d T) ,\] \[R_{T} =( T)^{3(d+1)/2}(( T)^{d+1}) .\]

The proof of Theorem 4 is presented in App. F. The proof starts by following the standard technique in the proof of UCB-type algorithms to show that \(r_{s}=f(x^{*})-f(x_{s}) 2_{s}_{s-1}(x_{s})\). Next, importantly, our design of \(_{s}=_{s-1}(x_{s})/\) allows us to show that \(r_{s} 2_{s}_{s}\). After that, the total regrets incurred in a stage \(s\) can be upper-bounded by \(N_{_{s}} 2_{s}_{s}=( }(/)_{s}_{s})= (_{s}(/))\). Lastly, the regret upper bound follows by summing the regrets from all \(m\) stages. Of note, for the commonly used SE kernel, our regret upper bound is of the order \(( T)\), which is significantly smaller than the regret lower bound of \(()\) in the classical setting shown by . This improvement over the classical fundamental limit is mostly attributed to the use of the QMC (Lemma 1), i.e., line 6 of Algo. 1. If the QMC procedure is replaced by classical MC estimation, then in contrast to the \(N_{_{s}}=(1/_{s})\) queries by QMC, every stage would require \(N_{_{s}}=(1/_{s}^{2})\) queries. As a result, this would introduce an additional factor of \(1/_{s}\) to the total regrets in a stage \(s\) (as discussed above), which would render the derivation of our regret upper bound (Theorem 4) invalid. This verifies that our tight regret upper bound in Theorem 4 is only achievable with the aid of quantum computing. Our Theorem 4 is a demonstration of the immense potential of quantum computing to dramatically improve the theoretical guarantees over the classical setting.

Importantly, when the linear kernel is used, the regret upper bound of our Q-GP-UCB is in fact better than that of the Q-LinUCB algorithm from . This improvement arises from our tight confidence ellipsoid in Theorem 3. In Sec. 5.6, we will give a more detailed discussion of this improvement and adopt our proof technique for Theorem 3 to improve the confidence ellipsoid for Q-LinUCB, after which their improved regret upper bound matches that of our Q-GP-UCB with the linear kernel.

### Regret Upper Bound for Noise with Bounded Variance

Here we analyze the regret of our Q-GP-UCB when the variance of the noise is bounded by \(^{2}\), which encompasses common scenarios including \(\)-sub-Gaussian noises and Gaussian noises with a variance of \(^{2}\). Consistent with the analysis of Q-LinUCB , in order to apply the theoretical guarantee provided by QMC (Lemma 1) for noise with bounded variance, here we need to assume that the noise variance is not too small, i.e., we assume that \(>1/4\). In this case of noise with bounded variance, the following theorem gives an upper bound on the regret of our Q-GP-UCB.

**Theorem 5**.: _With probability of at least \(1-\), we have that_

\[R_{T} = d^{3/2}( T)^{3/2}(d T)( _{2} T)^{3/2}_{2}(_{2} T)\] (4) \[R_{T} =( T)^{3(d+1)/2}(( T)^{d+1})( _{2} T)^{3/2}_{2}(_{2} T)\]

The proof of Theorem 5 is given in App. G. Note that same as Theorem 4, the regret for the SE kernel in Theorem 5 is also of the order \(( T)\), which is also significantly smaller than the classical regret lower bound of \(()\). Moreover, Theorem 5 shows that for both the linear kernel and SE kernel, the regret of our Q-GP-UCB is reduced when the noise variance \(^{2}\) becomes smaller.

### Improvement to Quantum Linear UCB (Q-LinUCB)

As we have mentioned in Sec. 5.4, the regret upper bound of our Q-GP-UCB with the linear kernel is \(R_{T}=(d^{3/2}( T)^{3/2}(d T))\), which is better than the corresponding \(R_{T}=(d^{2}( T)^{3/2}(d T))\) for Q-LinUCB 2 by a factor of \(()\). This improvement can be attributed to our tight confidence ellipsoid from Theorem 3. Here, following the general idea of the proof of our Theorem 3, we prove a tighter confidence ellipsoid for the Q-LinUCB algorithm, i.e., we improve Lemma 3 from the work of . Here we follow the notations from , and we defer the detailed notations, as well as the proof of Theorem 6 below, to App. H due to space limitation.

**Theorem 6** (Improved Confidence Ellipsoid for ).: _With probability of at least \(1-\),_

\[^{*}_{s}=\{^{d}:\|-_{s}\|_{V_{s}}1+}{ }_{k=1}^{s}^{2}}+ S\}, s[m].\]

Note that the size of the confidence ellipsoid from our Theorem 6 is of the order \((^{s}1/_{k}^{2})})=( {d T})\) (we have used Lemma 2 here), which improves over the \(()=()=()=(d)\) from Lemma 3 of  by a factor of \(\). Plugging in our tighter confidence ellipsoid (Theorem 6) into the regret analysis of Q-LinUCB, the regret upper bound for Q-LinUCB is improved to \(R_{T}=d^{3/2}^{3/2}T(d T)\) (more details at the end of App. H). This improved regret upper bound exactly matches that of our Q-GP-UCB with the linear kernel (Theorem 4). Similar to Theorem 3, the most crucial step in the proof of Theorem 6 is to apply the self-normalized concentration inequality from Theorem 1 of  to _1-sub-Gaussian noise_. Again this is feasible because \(_{s}=_{s-1}(x_{s})/\) is \(_{s-1}\)-measurable, and conditioned on \(_{s-1}\), the scaled noise \(_{s}/_{s}\) is \(1\)-sub-Gaussian.

### Regret Upper Bound for the Matern Kernel

So far we have mainly focused on the linear and SE kernels in our analysis. Here we derive a regret upper bound for our Q-GP-UCB algorithm for the Matern kernel with smoothness parameter \(\).

**Theorem 7** (Matern Kernel, Bounded Noise).: _With probability of at least \(1-\), we have that_

\[R_{T}=}T^{3d/(2+d)}.\]

The proof is in App. I. Note that the state-of-the-art regret upper bound for the Matern kernel in the classical setting is \(R_{T}=(})=}(T^{(+d)/(2 +d)})\)[24; 30; 36; 7], which matches the corresponding classical regret lower bound (up to logarithmic factors in \(T\)) . Therefore, the regret upper bound of our Q-GP-UCB for the Matern kernel (Theorem 7) improves over the corresponding classical regret lower bound when \(>2d\), i.e., when the reward function is sufficiently smooth. Also note that similar to the classical GP-UCB  (with regret \(R_{T}=(_{T})=}(T^{(+3d/2)/ (2+d)})\)) which requires the reward function to be sufficiently smooth (i.e., \(>d/2\)) to attain sub-linear regrets for the Matern kernel, our Q-GP-UCB, as the first quantum BO algorithm, also requires the reward function to be smooth enough (i.e., \(>d\)) in order to achieve a sub-linear regret upper bound for the Matern kernel. We leave it to future works to further improve our regret upper bound and hence relax this requirement for smooth functions.

## 6 Experiments

We use the Qiskit python package to implement the QMC algorithm (Lemma 1) following the recent work of . Some experimental details are deferred to App. J due to space limitation.

**Synthetic Experiment.** Here we use a grid of \(||=20\) equally spaced points within \(\) as the \(1\)-dimensional input domain \(\) (\(d=1\)), and sample a function \(f\) from a GP prior with the SE kernel. The sampled function \(f\) is scaled so that its output is bounded within \(\), and then used as the reward function in the synthetic experiments. We consider two types of noises: _(a)_ bounded noise (within \(\)) and _(b)_ Gaussian noise, which correspond to the two types of noises in Lemma 1, respectively. For _(a)_ bounded noise, we follow the practice of  such that when an input \(x\) is selected, we treat the function value \(f(x)\) as the probability for a Bernoulli distribution, i.e., we observe an output of \(1\) with probability of \(f(x)\) and \(0\) otherwise. For _(b)_ Gaussian noise, we simply add a zero-mean Gaussian noise with variance \(^{2}\) to \(f(x)\). The results for _(a)_ bounded noise and _(b)_ Gaussian noise are shown in Figs. 1 (a) and (b), respectively. The figures show that for both types of noises, our Q-GP-UCB significantly outperforms the classical baseline of GP-UCB. Specifically, although our Q-GP-UCB incurs larger regrets in the initial stage, it is able to leverage the accurate observations provided by the QMC subroutine to rapidly find the global optimum. These results show that the quantum speedup of our Q-GP-UCB in terms of the tighter regret upper bounds (Theorems 4 and 5) may also be relevant in practice. We have additionally compared with linear UCB (LinUCB)  and Q-LinUCB , which are, respectively, the most representative classical and quantum linear bandit algorithms. The results in Fig. 2 (App. J) show that in these experiments where the rewardfunction is non-linear, algorithms based on linear bandits severely underperform compared with BO/kernelized bandit algorithms in both the classical and quantum settings.

**AutoML Experiment.** In our experiments on _automated machine learning_ (AutoML), we use our Q-GP-UCB algorithm to tune the hyperparameters of an SVM for a classification task. Here we consider a Gaussian noise with a variance of \(^{2}\) since it is more practical and more commonly used in real-world experiments. The results in Fig. 1 (c) show that our Q-GP-UCB again significantly outperforms the classical GP-UCB. Fig. 1 (d) additionally shows the comparisons with LinUCB and Q-LinUCB. The results again corroborate that BO/kernelized bandit algorithms are considerably superior in real-world applications with highly non-linear reward functions in both the classical and quantum settings. These results here further demonstrate the potential of our Q-GP-UCB to lead to quantum speedup in practical applications.

**More Realistic Experiments.** We have additionally tested the performance of our Q-GP-UCB algorithm after accounting for the effect of quantum noise, by incorporating into our Qiskit simulations a noise model based on the actual performance of real IBM quantum computers. The results (Fig. 3 in App. J) show that although the addition of quantum noise slightly deteriorates the performance of our Q-GP-UCB, it is still able to significantly outperform classical GP-UCB. Notably, we have additionally performed an experiment _using a real quantum computer_ (details in App. J), in which the performance of our Q-GP-UCB, although further worsened compared to the experiment using simulated noise, is still considerably better than classical GP-UCB (Fig. 4, App. J). Also note that the work of  has not shown that Q-LinUCB outperforms LinUCB in the presence of simulated quantum noise. Therefore, the consistently superior performances of our Q-GP-UCB over GP-UCB with both simulated quantum noise and a real quantum computer serve as important new support for the potential practical advantages of quantum bandit algorithms.

**Discussion.** In our experimental results (e.g., Fig. 1), our Q-GP-UCB usually has relatively larger regrets in the initial stages but quickly achieves zero regret thereafter (i.e., the curve plateaus), which has also been observed in  for the Q-LinUCB algorithm. This is because in the initial stages, our Q-GP-UCB explores a smaller number of unique arms than GP-UCB. However, after the initial exploration, our Q-GP-UCB quickly starts to perform reliable exploitation, because the accurate reward observations achieved thanks to our QMC subroutines allow us to rapidly learn the reward function and hence find the optimal arm.

## 7 Conclusion

We have introduced the first quantum BO algorithm, named Q-GP-UCB. Our Q-GP-UCB achieves a regret upper bound of \(( T)\), which is significantly smaller than the classical regret lower bound of \(()\). A limitation of our work is that the regret upper bound of our Q-GP-UCB: \((( T)^{3(d+1)/2})\) (ignoring polylog factors) has a worse dependency on the input dimension \(d\) than the regret of the classical GP-UCB: \((( T)^{d+1})\). A similar limitation is shared by Q-LinUCB, since its regret has a dependency of \((d^{3/2})\) in contrast to the \((d)\) of LinUCB. It is an interesting future work to explore potential approaches to remove this extra dependency on \(d\). Another limitation of our work is that for the Matern kernel, we require the reward function to be smooth enough in order to achieve a sub-linear regret upper bound (Sec. 5.7). So, another important future work is to further tighten the regret upper bound of our Q-GP-UCB for the Matern kernel when the reward function is less smooth, as well as for other kernels encompassing non-smooth functions such as the neural tangent kernel which has been adopted by recent works on neural bandits . Moreover, another interesting future work is to derive regret lower bounds in our setting of quantum kernelized bandits, which can help evaluate the tightness of our regret upper bounds.

Figure 1: Cumulative regret for synthetic experiments with (a) bounded noise and (b) Gaussian noise. (c) Cumulative regret for the AutoML experiment; (d) additionally includes results for linear bandits.