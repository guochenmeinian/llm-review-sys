# Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products

Tamas Sarlos

Google Research

stamas@google.com &Xingyou Song

Google Deepmind

xingyousong@google.com &David P. Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu &Qiuyi (Richard) Zhang

Google Deepmind

qiuyiz@google.com

###### Abstract

Inspired by fast algorithms in natural language processing, we study low rank approximation in the entrywise transformed setting where we want to find a good rank \(k\) approximation to \(f(U V)\), where \(U,V^{}^{n r}\) are given, \(r=O((n))\), and \(f(x)\) is a general scalar function. Previous work in sublinear low rank approximation has shown that if both (1) \(U=V^{}\) and (2) \(f(x)\) is a PSD kernel function, then there is an \(O(nk^{-1})\) time constant relative error approximation algorithm, where \( 2.376\) is the exponent of matrix multiplication. We give the first conditional time hardness results for this problem, demonstrating that both conditions (1) and (2) are in fact necessary for getting better than \(n^{2-o(1)}\) time for a relative error low rank approximation for a wide class of functions. We give novel reductions from the Strong Exponential Time Hypothesis (SETH) that rely on lower bounding the leverage scores of flat sparse vectors and hold even when the rank of the transformed matrix \(f(UV)\) and the target rank are \(n^{o(1)}\), and when \(U=V^{}\). Furthermore, even when \(f(x)=x^{p}\) is a simple polynomial, we give runtime lower bounds in the case when \(U V^{}\) of the form \(((n^{2-o(1)},(2^{p})))\). Lastly, we demonstrate that our lower bounds are tight by giving an \(O(n(k,2^{p},1/))\) time relative error approximation algorithm and a fast \(O(n(k,p,1/))\) additive error approximation using fast tensor-based sketching. Additionally, since our low rank algorithms rely on matrix-vector product subroutines, our lower bounds extend to show that computing \(f(UV)W\), for even a small matrix \(W\), requires \((n^{2-o(1)})\) time.

## 1 Introduction

The central idea behind the classic problem of low rank approximation (LRA) is to approximate a given matrix with a rank \(k\) matrix that preserves the important features of the original matrix, while being computationally more efficient and statistically more stable. One particular area of interest in LRA is the low rank decomposition of entrywise transformed matrices, where the entries of the original matrix are transformed through a non-linear function before being approximated, with various applications in kernel methods, self-attention, and likelihood computations (Choromanski et al., 2021; Levy and Goldberg, 2014). In these settings, the Gram matrix of a dot-product kernel, the attention module's product operator, or even the non-linear computation of activation in a deep network can all be represented as \(f(UV)\), where the inputs are low dimensional matrices \(U,V\). Note that while \(UV\) is low rank, \(f(UV)\) may not be, with the rank blowup dependent on the choice of the transformation \(f\).

For entrywise transformed low rank approximation, we aim to find an approximately optimal rank \(k\) approximation, in terms of relative Frobenius norm error, to the matrix \(A=f(UV)\) given \(U^{n r}\) and \(V^{r d}\), where \(f\) is a scalar transformation. In this paper, our goal is to study the functions \(f\) such that this task is solvable in subquadratic \(O(n^{2-})\) time and for ease of presentation, we assume \(n=d\) in this section. This problem was studied in the distributed setting in Woodruff and Zhong (2016) for functions \(f(x)=|x|^{p}\), where the goal was to minimize communication. It was also studied in the streaming setting, where it was shown that \(O(n(k/))\) memory suffices to solve rank \(k\) approximation with additive error \(O(\|U\|\|V\|)\) for the function \(f(x)=(|x|+1)\) in a single pass Jiang et al. (2021), improving the earlier work of Liang et al. (2020). In a related paper, Han et al. (2020) present an algorithm for low-rank approximation of polynomial entrywise transforms; however, they make no comparison to the optimal rank \(k\) error.

In the setting when \(U=V^{}\), certain choices of \(f(x)\) can surprisingly admit subquadratic relative error algorithms, such as when \(f\) represents a positive semi-definite (PSD) kernel Musco and Musco (2017). Moreover, for any PSD matrix of any rank, by sampling according to the leverage scores of the matrix square root of the kernel, recent work shows that relative error low rank decomposition is possible in \(O(n(k/)^{-1})\) time, where \( 2.373\) is the exponent of matrix multiplication; see Musco and Woodruff (2017); Bakshi et al. (2020). Note that in many cases when \(U=V^{}\), carefully choosing \(f\) will result in \(A=f(UV)\) being PSD. In fact, when \(u_{i}\) are all unit norm, if \(f\) admits a Taylor expansion with non-negative coefficients, then \(A=f(UU^{})\) is a PSD matrix and admit sub-quadratic low rank approximations.

On the surface, this astounding algorithmic result seems to conflict with subquadratic lower bounds for basic linear algebraic primitives for many kernel functions, especially the exponential kernel used in the attention architecture. Specifically, such work looks at kernel matrix vector products and shows they take \((n^{2-o(1)})\) time for relative and additive error approximations Keles et al. (2023). Furthermore, these bounds can be refined to hold in the restricted regime when the entries of the matrix are \(()\)Alman and Song (2023). These conditional lower bounds, as with others for linear algebraic problems, are derived from hardness based on the Strong Exponential Time Hypothesis (SETH) Lokshtanov et al. (2013).

The crucial observation to resolve this seeming contradiction is to recognize that the low rank approximation allows for an error bound that depends on the error of the best rank-\(k\) approximation, which can be much larger that the error tolerated in the previous lower bounds. Therefore, hardness for LRA is inherently different than hardness for approximating the entire matrix additively or than matrix-vector (MV) multiplication. In fact, we will show that hardness for MV multiplication is, in some sense, strictly easier to establish.

Some related lower bounds include the work of Backurs et al. (2017) that solving kernel Support Vector Machines (SVM), ridge regression, or Principal Component Analysis (PCA) problems to high accuracy or approximating kernel density estimates up to a constant factor for kernels with exponential tails, requires \(n^{2-o(1)}\) time assuming SETH. Also, Alman et al. (2020) show that for linear algebraic primitives for the Laplacian of a graph with weights given by a kernel, most operations, such as \(\)-approximate matrix-vector multiplication, are hard, although their hardness results assume a \((1/)\) dependence when reducing to SETH.

### Our Contributions

In the setting of entrywise-transformed low rank approximation, we show that we cannot get subquadratic relative error LRA generally when either 1) \(U V^{}\) even for PSD kernel functions or 2) for transformations that are approximately polynomials of \(|x|\) even for constant degree. Therefore, without multiple strong structural assumptions on \(A\), there is no subquadratic approximation algorithm, even when \(r=((n))\) and the rank of the transformed matrix \(f(UV)\) and the target rank are \(n^{o(1)}\). We emphasize these novel hardness results hold for a large class of transformations and in fact generalize to hardness for matrix vector multiplication, which can be derived as a corollary, implying that computing \(f(UV)z\) also requires \((n^{2-o(1)})\) time for many \(f\) that have not been studied before (see Theorem 3.2). On the positive side, for the polynomial activation \(f(x)=x^{p}\), we provide an \(O(n(r^{p},k,1/))\) time algorithm for relative error LRA and \(O(n(p,k,1/))\) time for additive error LRA via fast tensor-based sketches (see Algorithm 2).

We note that previous lower bound techniques do not apply for LRA since relative error LRA approximations of \(f(UV)\) are possible without approximating a matrix vector product, as shown by the \(O(n^{1+o(1)})\) time LRA algorithm for the popular exponential kernel \(f(x)=(x)\) by querying a sublinear number of entries of \(f(UU^{})\)[Musco and Woodruff, 2017, Bakshi et al., 2020]. Indeed, our novel lower bounds differ from previous reductions from Orthogonal Vectors Problem (OVP) by explicitly creating LRA instances and exploiting linear algebraic structural properties of the column spaces of a low-rank tensored matrix.

Specifically, our reduction uses an structural property that if there is a pair of input vectors to OVP which are orthogonal, then under mild assumptions, this implies we can find a _sparse_ vector in the column span of the low rank approximation of \(f(UV)\), where \(U,V\) are matrices containing the input vectors to OVP. Also, our algorithm (Algorithm 1) relies on another critical observation that this sparse vector has uniform magnitude in the non-zero entries, each of which corresponds to a vector with an orthogonal vector pair. By exploiting the structure of this vector, we can lower bound the leverage score of each row that corresponds to a non-zero entry of this sparse vector, and we can appeal to fast leverage score computation algorithms to find a small \(n^{o(1)}\)-size superset of the support of the sparse vector. Finally, we can quickly check which pairs of entries in the superset correspond to vectors in the original OVP problem via a brute-force search, completing the reduction. The precise reduction is a bit more technically involved, as we also need to allow for additive error for our applications.

In the setting when \(U V^{}\), we show novel lower bounds in the case when \(f(x)=x^{p}\) is a polynomial kernel function and admits a simple rank \(r^{p}\) decomposition. We show that we cannot do better than the naive decomposition and prove an \(((n^{2-o(1)},2^{(p)})\) lower bound (see Theorem 3.3). This implies in the setting when \(p=((n))\) and \(k=n^{o(1)}\), that surprisingly there is a separation between the (1) \(U=V^{}\) setting and the (2) \(U V^{}\) setting: when \(f(x)=x^{2p}\), in the first setting, previous works show that there admits an \(O(nk^{-1})\) approximation algorithm; however, in the second setting when \(U V^{}\), our lower bounds imply that there cannot exists a truly subquadratic time algorithm. We emphasize that these exponential lower bounds in the degree for the polynomial kernel are the first of their kind to rule out truly subquadratic algorithms for \(p=((n))\), even when \(f(x)=x^{p}\) is a simple polynomial.

We also give an \(O(n(r^{p}/))\) time algorithm for relative error approximation, which agrees with our lower bounds (see Theorem 4.1), showing that our lower bounds for the polynomial kernel when \(U V^{}\) are in fact tight. In addition, we provide an \(O(n(p,k,1/))\) time algorithm for additive error LRA that avoids the inherent exponential dependence on \(p\), highlighting a separation between additive and multiplicative error LRA when \(p=((n))\). Specifically, we can achieve the easier additive error approximation with fast tensor-based sketching matrices (see Theorem 4.2) by applying comparable techniques to an independent subsequent work on polynomial-based transformers, although the main difference is that they do not output a rank \(k\) approximation and suffers a worse dependence on \(p,1/\) due to their non-negativity guarantees [Kacham et al.,

   &  &  &  \\  Musco and Woodruff  & \(O(n(k/))\) & PSD, \(U=V^{}\) & Relative LRA \\  Bakshi et al.  & \(O(n(k/)^{-1})\) & PSD, \(U=V^{}\) & Relative LRA \\ 
**This work** & \(O(n(p,k/))\) & \(f(x)=x^{p},U V^{}\) & Relative LRA \\ 
**This work** & \(O(n(p,k/))\) & \(f(x)=x^{p},U V^{}\) & Additive LRA \\  
**Lower Bounds** & & & \\  Backurs et al.  & \((n^{2-o(1)})\) & Gaussian, \(U=V^{}\) & PCA, Regression \\  Alman et al.  & \((n^{2-o(1)})\) & Kernel Laplacians \(U=V^{}\) & MV product \\  Keles et al.  & \((n^{2-o(1)})\) & Exponential Kernels, \(U=V^{}\) & MV product \\ 
**This work** & \((n^{2-o(1)})\) & \(f(x)=|x^{p}+O(|x|^{p+1}),U=V^{}\) & Relative LRA, MV \\ 
**This work** & \(((n^{2-o(1)},2^{2(p)}))\) & \(f(x)=x^{p},U V^{}\) & Relative LRA, MV \\  

Table 1: Overview of upper bounds for low rank approximation (LRA) and lower bounds for relative linear algebraic primitives, where the dependence on \(r\) is omitted. This table illustrates multiple separation results: 1) LRA is strictly easier than matrix vector (MV) products for positive semidefinite kernels, 2) LRA is strictly easier for functions \(f(x)\) that are kernels, even when considering low-degree polynomials of \(|x|\), 3) LRA is strictly easier when \(U=V^{}\) for \(f(x)=x^{p}\) for \(p=((n))\), which has a \(2^{(p)}\) lower bound when \(U V^{}\), 4) additive error LRA is strictly easier than relative error LRA. Our work extends to any function \(f(x)\) that admits a Taylor series dominated by \(|x|^{p}\) around \(x=0\). For example, this includes \(f(x)=(|x|+1)=|x|+O(|x|^{2})\).

2023]. We remark that while the polynomial tensor matrix itself can be approximated relatively in \(O(n(p))\) runtime, we emphasize that the polynomial kernel, as a product of two tensor matrices, must incur additive error or suffer \((2^{p})\) runtime for relative error guarantees.

Finally, we observe that we can generalize our lower bounds for LRA by exploiting the fact that our algorithms for fast LRA reduce to matrix vector product! Therefore, our relative error algorithms are reductions that give MV lower bounds, which are automatically derived for a large number of entrywise transformed matrices, even when \(U=V^{}\) (see Theorem 4.3). This implies that in some sense, LRA is an easier problem than MV approximation, implying that our lower bounds for LRA are stronger. We summarize the most relevant prior results and our contributions in Table 1.

### Related Work to Transformers and Natural Language Processing

A primary downstream application from our work is in the field of natural language, as it is common to compute similarity matrices \(f(UV)\) from token embeddings \(U,V\). Such is the case for the popular use of Transformers [Vaswani et al., 2017] and their efficient attention variants [Tay et al., 2022]. The standard attention mechanism consists of the operation \((})W\), where in our notation, \(U,V,W\) are the "query", "key", and "value" matrices respectively. Simplifying and ignoring normalizations, this can be seen as \(f(UV)W\) where \(f(x)=(x)\). For a given \(f\), one can potentially _linearize_ the attention mechanism if there exist \(U^{},V^{}\) such that \(f(UV) U^{}V^{}\), as then the order of matrix multiplication can be rearranged into \(U^{}(V^{}W)\) which allows memory and runtime in \(O(ndk)\).

In the unnormalized softmax case where \(f(x^{}y)=(x^{}y)\), Choromanski et al.  notes that \((x^{}y)=_{(0,_{d})}[ (^{}x-}{2})(^{}y-}{2})]\) which thus allows defining \(U^{},V^{}\) as random feature matrices from sampled \(_{1},,_{k}\). Further use of kernel properties to improve the softmax approximation have been introduced in Choromanski et al. , Likhosherstov et al. . More generally, for \(f\) such that \(f(x^{}y)=(x,y)\) admits a kernel representation, one may consider using variants of Bochner's theorem [Feller, 1968] to provide approximations via random Fourier features [Rahimi and Recht, 2007].

While works such as Tsai et al. , Kacham et al.  have experimented with linear, polynomial, exponential, and RBF kernels, so far the dominant paradigm, termed the class of _Linear Transformers_ Katharopoulos et al. , is to conveniently instead consider the reverse case, where one defines a mapping \(:\) in order to define the kernel \((x,y)=(x)^{}(y)\). Unfortunately, usually this does not lead to a closed-form \(f\) such that \(f(x^{}y)=(x)^{}(y)\), which can lack interpretability and compatibility with classic attention mechanisms.

However, one may consider nonlinear functions \(f\) which do not admit a kernel; for example, the class of functions \(f(x)=^{c}(|x|+1)\) for \(c>0\) is used in Levy and Goldberg , Li et al.  to compute implicit word embeddings and corresponding generative models. Within this application domain, our work thus provides answers to the question: _for which classes of functions \(f\) can one efficiently compute low-rank approximations \(U^{}\), \(V^{}\) such that \(f(UV) U^{}V^{}\), especially when \(f(x^{}y)\) does not admit a kernel structure \((x,y)\)?_

## 2 Preliminaries

We let our implicit matrix \(A=f(UV)\) be \(n d\) where \(U^{n r}\) and \(V^{r d}\) and \(n d\) without loss of generality, where \(f:\) is a scalar function. We will also assume that \(d=n^{(1)}\) and it is often the case in empirical settings that \(d=n\). We say that a matrix \(A\) is positive semi-definite (PSD) if it is symmetric and only has non-negative eigenvalues; a function \(f\) is a kernel function if \(f(UU^{})\) is PSD for any matrix \(U\). The \(i\)-th leverage score \(_{i}\) of matrix \(B^{n d}\) is equal to its sensitivity, i.e. \(_{i}=_{x^{d}}^{T}x)^{2}}{\|B_{i}\|_{2}^{2}}= _{y(B)}}{\|y\|_{2}^{2}}\), where \(B_{i}\) is the \(i\)-th row of \(B\). The Khatri-Rao product of \(B^{n d}\) is \(C^{n d^{p}}\), where the \(i\)-th row of \(C\) is \(B_{i}\) tensored with itself \(p\) times. We use the standard notation that \((f)\) is \(O(f( f))\).

For the approximate rank \(k\) LRA problem, we want to find \(U^{}^{n k}\) and \(V^{}^{k n}\) such that

\[\|A-U^{}V^{}\|(1+)_{^{n k },^{k d}}\|A-\|\]where \(\|\|\) denotes the Frobenius norm, unless otherwise specified and we denote \([A]_{k}=^{*}^{*}\) as some rank \(k\) matrix that minimizes the objective. Note this approximation is a relative error approximation guarantee, but can be analogously defined for additive error. For stronger lower bounds, we consider a weakened version of this problem given by outputting only a best rank \(k\) projection, specifically we want to find orthogonal \(W^{n k}\): \(\|A-AWW^{}\|(1+)\|A-[A]_{k}\|\).

Our lower bounds will rely on reductions from the conditional hardness of OVP and Max-IP, whose hardness comes from SETH. We observe, from our remarks, that some more restrictive structure can be placed on the input vector sets \(A,B\) to OVP without removing the hard instances.

**Assumption 1**.: _(Hardness of Orthogonal Vectors Problem (OVP)[Williams, 2005]). Let \(A=\{a_{1},,a_{n}\}\) and \(B=\{b_{1},,b_{a}\}\) be sets, where \(a_{i},b_{j}\{0,1\}^{r}\) are binary vectors for all \(i[n]=\{1,2,,n\},j[d]\). Any algorithm which given any input \((A,B)\) decides with constant probability if there is at least one pair of vectors \(a A\) and \(b B\) such that \(a^{T}b=0\) requires \((nd)^{1-o(1)}\) time, provided \(r=( n)\) and \(r=n^{o(1)}\). Observe that the lower bounds also hold when \(A=B\) is enforced._

**Remark 2**.: _Previous work [Williams, 2005, Vassilevska Williams, 2015] has shown that Assumption 1 is true for \(n=d\) unless the Strong Exponential Time Hypothesis (SETH, Impagliazzo and Paturi ) is false. Given this assumption, one can handle general \(n\) and \(d\) by a padding argument: if one could solve OVP with an algorithm \(\) running in at most \((nd)^{1-C}\) time for a constant \(C>0\), and without loss of generality \(n d\), then one could solve the problem when \(|A|=|B|=n\) by splitting \(B\) into \((n/d)\) disjoint sets each of size at most \(d\), and solving the problem on each disjoint set in total time less than \((nd)^{1-C}(n/d)=n^{2-C}/d^{C} n^{2-C}\), contradicting the assumption in the \(|A|=|B|=n\) case._

**Remark 3**.: _In Assumption 1 we can enforce more restrictive structure on the input sets \(A,B\) without making the problem easier. Specifically, we can assume that there are at most \(n^{o(1)}\) distinct pairs with \(a A\) and \(b B\) for which \(a^{T}b=0\). Indeed, otherwise by sampling \((nd)^{2-(1)}\) pairs at random and checking if \(a^{T}b=0\), we would solve the OVP problem in \((nd)^{1-(1)} r=(nd)^{1-(1)}\) time, using that \(r=n^{o(1)}\). This would contradict Assumption 1._

**Assumption 4**.: _(Hardness of Apx-Max-IP\({}_{n,d}\) Problem, Definition 2.1, Remark 2.2, and Lemma 4.1 of Chen and Williams ). Two sets \(A=\{a_{1},,a_{n}\}\) and \(B=\{b_{1},,b_{n}\}\) are given, where \(a_{i},b_{i}\{0,1\}^{s}\) are binary vectors for all \(i[n]\), with \(s=((n))\). Let \(m=_{a A,b B}a b\). Any algorithm which outputs a number \([m/100,m]\) with constant probability requires \((nd)^{1-o(1)}\) time._

**Remark 5**.: _As in Remark 2 and Remark 3, we can reduce the general \(n\) and \(d\) case to the case \(n=d\) of previous work [Chen and Williams, 2019], and we can also enforce that there are at most \(n^{o(1)}\) pairs \(a A\) and \(b B\) for which \(a b m/100\), as otherwise sampling would solve the problem in less than \((nd)^{1-o(1)}\) time._

## 3 LRA Runtime Lower Bounds

We show that the complexity of low rank approximation of an entrywise transformed matrix turns out to heavily depend on the type of entrywise transformation. Based on standard complexity assumptions, we show an \((nd)^{1-o(1)}=n^{1+(1)}\) time lower bound for entrywise function \(f(x)=|x|^{p}\) for odd integers \(p\) in Theorem 3.1 below, while we show a weaker \(2^{(p)}\) lower bound for any integer in Theorem 3.3, which becomes \((nd)^{1-o(1)}\) for \(p=( n)\). This is no accident, as we show a matching \(2^{O(p)}n^{1+o(1)}\) upper bound in Theorem 4.1. Furthermore, our lower bounds extend to functions that are approximately odd-degree polynomials of \(|x|\), even if the degree is small. Specifically, this includes the commonly used \(f(x)=(|x|+1)|x|\).

Our proofs use variations of the Orthogonal Vectors Problem (OVP) to create two types of instances of implicit low rank approximation to create a fast \(O(n^{1+o(1)})\) time algorithm to solve OVP (see Algorithm 1) when given access to a low rank approximation algorithm with constant relative error and small constant additive error guarantees. Let \(A,B\) be the set of input vectors of OVP. Then, in our reduction, if there is a pair of input vectors which are orthogonal, then applying LRA, we can find either 1) a column span deviation of the low rank approximation or 2) a _sparse_ vector in the column span of the low rank approximation. Note that we do not look at the exact column spans, but allow an additive error depending on the additive error of the output.

In the latter case, we note that our reduction structure ensures that all entries in this vector have the same magnitude. Consequently, since the column span of the output has low rank, each such entry in the support of this sparse vector corresponds to a large _leverage score_, and so to find the support of this unknown sparse column we can use algorithms to compute or approximate all leverage scores given the low rank factorization of the entrywise transformed matrix. This enables us to find a small superset of the support of the sparse vector, and then brute-force which pairs of entries in the superset correspond to vectors in the original OVP problem that intersect. The overall time in the reduction is negligible compared to the time to solve OVP, and therefore it must be that finding the factorization of the entrywise transformed matrix itself was expensive.

In the former case, it follows that the _column spans_ of the output are necessarily far from each other in the two cases, and we can quickly check this and therefore solve the OVP problem. Together, it follows that the time for finding the factorization of the entrywise transformed matrix itself must have been large. While there are quantitative differences in the two cases of odd and even integers \(p\), the proofs both follow this strategy.

```
0: Sets \(=\{a_{1},,a_{n}\{0,1\}^{s}\}\) and \(=\{b_{1},,b_{n}\{0,1\}^{s}\}\)
0: YES if there exists \(a^{}b=0\) and NO otherwise.
1: Choose \(c\{-1,1\}^{n}\) uniformly at random
2: Let \(=[\:|\:c]^{n(s+1)}\) and \(^{}=[\:|\:c]^{n(s+1)}\) and \(r=s+1\).
3: Let \(=(,,f=|x|^{p})\) for constant relative error and \(\) additive error
4: Compute \(^{}=...^ {n r^{p}}\), \(\) tensored with itself \(p\) times and similarly for \(^{}\).
5: Let \(r_{i}^{2}=\|^{}^{}e_{i}\|_{2}^{2} -\|^{}^{}^{ }e_{i}\|_{2}^{2}\) for \(i\)-th column \(\)Calculate column distances of \(^{}^{}\) to column span of \(\).
6: If any of \(r_{i}^{2}>1.01\), output YES \(\) is any upper bound on additive error of LRA
7: Compute \(1/2\)-approximate leverage scores \(_{i}\) of \(^{}\) and let \(S=\{i[n]|_{i} 1/(100n^{o(1)})\}\)\(\)Finds the representative subset and the threshold is given by the OVP assumption.
8: Compute all dot products between \(a_{i}\) and \(\) for all \(i S\). Output YES if there exists a pair such that \(a_{i}^{}b_{j}=0\) and NO otherwise ```

**Algorithm 1** OVP to LRA Reduction

### General Functions of \(|x|^{p}\) for odd integers \(p\)

In the following theorem we will also allow for a tiny amount of additive error, as this will be useful for our later lower bound applications where we approximate other functions using a Taylor series and reduce from the following theorem.

**Theorem 3.1**.: _Suppose \(n\), \(f(x)=|x|^{p}\) for an odd integer constant \(p=O((n))\), and \(r=O((n))\). There is a positive integer \(k=n^{o(1)}\), such that for any approximation factor \( 1\) and any constant \(<2\), any possibly randomized, algorithm which given any input \(U^{n r}\) and \(V^{r d}\) outputs \(W^{n k}\) satisfying_

\[\|f(U V)WW^{}-f(U V)\|_{F}^{2}\|[f(U V)]_{k} -f(U V)\|_{F}^{2}+,\]

_with constant probability, requires \((nd)^{1-o(1)}\) time, under Assumption 1. Further, this holds even if \(U=V^{T}\). Here \([f(U V)]_{k}\) denotes the best rank-\(k\) approximation to \(f(U V)\) in the Frobenius norm._

Proof.: Suppose \(A\) and \(B\) are input sets to the OVP problem of Assumption 1 with parameter \(r=s+1\), where \(s\) is the dimension of the points. We let the rows of \(U\) be the points in \(A\), but we append one additional dimension, represented as column vector \(c\) to \(U\) that is chosen uniformly at random in \(\{-1,1\}^{n}\). Similarly, the columns of \(V\) correspond to the points in \(B\), but we append one additional row to \(V\), which is equal to \(c^{T}\). Observe that if \(A=B\), then necessarily \(U=V^{}\).

Let Case 1 be when there is no \(a A\) and \(b B\) with \(a^{T}b=0\), and Case 2 be when there is an \(a A\) and a \(b B\) with \(a^{T}b=0\), and we are deciding whether we are in Case 2. We claim that our algorithm (Algorithm 1) will always output NO when we are in Case 1 and will output YES with constant probability when we are in Case 2. This clearly suffices to solve OVP with constant probability.

[MISSING_PAGE_FAIL:7]

Recall the definition of leverage scores \(_{i}\) from Section 2 and that the sum of the \(n\) leverage scores of \(B^{n t}\) is exactly equal to the rank of \(B\), which is at most \(t\). Since \(v+e\) is in the column span of \(U^{}\), the \(i\)-th leverage score of \(U^{}\) satisfies \(_{i}(U^{})(v+e)_{i}^{2}/\|v+e\|_{2}^{2}=(1/n^{o(1)})\).

Consequently, each coordinate \(i\) in \(S\) satisfies \(_{i}(U^{})=(1/n^{o(1)})\). It is known  how, given a matrix \(B\), in \(O(nt n)+t^{O(1)}\) time, one can compute a list \(_{1}^{},,_{n}^{}\) with \(_{i}^{}=(_{i})\) for all \(i\) with probability \(1-1/n^{100}\). Consequently, using that \((U^{}) r^{p}\), given \(U^{}\) one can find a superset \(T\) containing \(S\) for which \(|T|=O(r^{p}n^{o(1)})\) in \(n^{1+o(1)}\) time, where recall \(r=s+1\) and \(s=n^{o(1)}\) and \(p\) is constant. Note that our bound on \(|T|\) follows since we just need to keep the leverage scores that are \((1/n^{o(1)})\) and the sum of all leverage scores is at most \(r^{p}=n^{o(1)}\), recalling \(r=s+1\) and \(s=n^{o(1)}\). 

Continuing the proof of Theorem 3.1, by our claim, when we are in Case 2, with at least constant probability, we find in time \(O(n^{1+o(1)})\) a representative subset \(A^{}\) of size \(O(n^{o(1)})\). Given our subset \(A^{}\), one can then compute all pairs of dot products between the points in \(A^{}\) and the points in \(B\) in \(O(n^{o(1)}ds)\) time, and since we may assume \(n d\) without loss of generality, we can compute all such pairs in \(n^{1+o(1)}\) time. Lastly, if no such \(a,b\) exist, when we are in Case 1, this algorithm cannot err. Therefore, we have an algorithm that can solve \(OVP\), which then violates Assumption 1, using that \((nd)^{1-o(1)}=n^{1+(1)}\). 

Our lower bound techniques also apply to a number of important function \(f\) that do not have the form of \(|x|^{p}\) for an integer \(p\). We show that our lower bounds for LRA in fact holds for any function \(g(x)=f(|x|)\), where \(f(x)\) is a function that admits a Taylor expansion with a dominant term of \(x^{p}\), for odd \(p\), around \(0\). Of particular interest in natural language processing  is the function \(g(x)=(1+|x|)\), where satisfies \(g(x)=f(|x|)\), where \(f(x)=x+O(|x|^{2})\) and thus we can appeal to our lower bound with \(p=1\).

**Theorem 3.2**.: _Suppose \(n\), \(r=O((n))\), and \(g(x)=f(|x|)\), where \(f\) admits a Taylor expansion \(f(x)=c_{p}x^{p}+O(|x|^{p+1})\) for odd \(p=O((n))\). There is a positive integer \(k=n^{o(1)}\), such that for any approximation factor \( 1\), any, possibly randomized, algorithm which given inputs \(U^{n r}\) and \(V^{r d}\) outputs \(W^{n k}\) satisfying_

\[\|g(U V)WW^{}-g(U V)\|_{F}^{2}\|[g(U V)]_{k }-g(U V)\|_{F}^{2}\]

_with constant probability, requires \((nd)^{1-o(1)}\) time, under Assumption 1. Further, this holds even if \(U=V^{}\)._

### Polynomials of All Integers \(p\)

We now give the proof for \(p\)-degree polynomials for all integers \(p\), which is weaker than Theorem 3.1 for odd integers, but this is the first such lower bound for even integers. The weaker bound cannot be substantially improved since in this setting \(f(x)=x^{p}\), so \(f(UV)\) is in fact a kernel and matrix vector multiplication can be performed in \(O(nr^{p})\) time. For intuition why our lower bound techniques do not extend, recall that our previous reduction forces the absolute value operation to essentially alter entries of \((UV)^{p}\) but only at entries of the original with zero dot product. Therefore, we can write as a sum of a low rank matrix (from tensor product) and a sparse matrix, whose sparse entries now represent the OVP pairs. However, when \(p\) is even, the absolute value operation leaves the entries unchanged and does not induce the additional sparse matrix. Therefore, our lower bounds for this setting rely on a slightly different variant of OVP, specifically quadratic lower bounds for finding the maximum dot product (Assumption 4).

Intuitively, our new reduction is as follows: Let \(\) be the maximum inner product and by assumption, consider this small set of large inner product pairs, which represents a small number of entries in that are large in magnitude so that when you apply a threshold at \(/100\), the resulting matrix is sparse. Since \(f(x)=x^{p}\) amplifies the magnitude differences, it follows that \((UV)^{p}\) is much closer relatively to an approximately sparse, and therefore low rank, matrix. Therefore an approximate low rank approximation (LRA) algorithm can recover this sparse low rank matrix well enough so that the span of the approximate matrix can be used, via leverage score computations, to solve the APX-Max-IP problem.

**Theorem 3.3**.: _Let \(U^{n r}\) and \(V^{r d}\) be given with \(r=O((n))\). Suppose \(f(x)=x^{p}\) for an integer \(p 1\). There is a positive integer \(k=n^{o(1)}\), such that for any approximation factor \( 1\), any, possibly randomized, algorithm which outputs \(W^{n k}\) satisfying \(\|f(U V)WW^{}-f(U V)\|_{F}^{2}\|[f(U V)]_{k} -f(U V)\|_{F}^{2}\), with constant probability for any constant \(>1\), requires \(((nd)^{1-o(1)},2^{(p)})\) time, under Assumption 4._

The lower bound for general functions \(f(x)\) in Theorem 3.2 with odd-degree dominating term in its Taylor expansion works because we can scale down the entries in our input matrix so that our relative-error approximation guarantee is still preserved but the function is largely approximated by only the leading polynomial term. By a combination of the same taylor expansion argument with a slight more general version of Theorem 3.3 that handles small additive error, we note that we can extend our results to when \(p\) is even.

We note that our lower bounds do not generalize to the case when \(U=V\) because as mentioned in our intuitive introduction, our reduction hinges on the sparse low-rank structure of \((UV)^{p}\). However, when \(U=V\), the sparsity structure is broken as the diagonal of the matrix is now larger than the maximum inner product of two different vectors, and this destroys the low rank structure. In some sense, the diagonal of our matrix is forced to include the dot product of \(u_{i}\) with itself and this shifts the entire matrix by a large multiple of the identity, crucially removing the sparse + low-rank structure that we exploited in our lower bound argument before. Indeed, as we remark below, the resulting positive semidefinite (PSD) structure allows us to derive a fast LRA algorithm and our lower bound no longer holds in this setting.

**Remark 7**.: _Recall that when \(U=V^{}\), there is an \(nr(k/)^{-1}\) subquadratic time algorithm when \(k=O(n^{o(1)})\), for \((1+)\)-relative error approximation, but reducing the \(U V^{}\) case to the \(U=V^{}\) case requires blowing up \(k\) to \(k+r^{p}\). Therefore, our lower bounds imply that the \(U V^{}\) case is strictly more difficult in terms of runtime in certain settings._

## 4 LRA Algorithms from Matrix Vector Products

In this section, we can show upper bounds for low rank approximation of entrywise transformed products that are \(O(n^{1+o(1)})\) when \(f(x)\) represents a kernel matrix but \(U V^{}\). Specifically, we focus on polynomial functions of the form \(f(x)=x^{p}\) and demonstrate that our lower bounds are tight for relative error LRA. Note that we have shown that low rank approximation guarantees for PSD matrices when \(U=V^{}\) cannot translate to the case when \(U V^{}\). Still, we demonstrate that relative error low rank approximation is possible in \(n^{1+o(1)}\) time for polynomial kernels with even degree, although there will be an exponential dependence on \(p\) for relative error low rank approximation. Our relative error algorithms are relatively standard and rely on low rank projections by using matrix vector products to perform randomized sketching to reduce our row or column dimension to \(O((k/))\)(Woodruff, 2014).

**Theorem 4.1**.: _Let \(U^{n r}\) and \(V^{r d}\). Suppose \(f(x)=x^{p}\) for an even integer \(p 1\) and \(k<r^{p}\). For any approximation factor \(>0\), there is an algorithm that outputs \(U^{}^{n k}\) and \(V^{}^{k d}\) satisfying \(\|U^{} V^{}-f(U V)\|_{F}^{2}(1+)\|[f(U  V)]_{k}-f(U V)\|_{F}^{2}\) with constant probability with runtime \(O((n+d)r^{p}k/^{3}+(r^{p}/))\)._

Proof.: Note that \(f(U_{i}V_{j})=(U_{i}V_{j})^{p}\). Thus, we may rewrite \(f(U V)=U^{}V^{}\), where each row of \(U^{}^{n r^{p}}\) is the Khatri-Rao product of itself \(p\) times, and each column of \(V^{}^{r^{p} d}\) is the Khatri-Rao product of itself \(p\) times. Note that the rank of \(U^{}V^{}\) is at most \(d^{p}\).

Let \(S\) be a random Gaussian sketching matrix with \(O(k/)\) rows, so we know that these matrices satisfy the \((,9/10,l)\)-JL property (Woodruff, 2014). Furthermore, let \(R\) be a random Gaussian matrix with \(O((k/^{3},r^{p}/^{2}))\) columns, so we know that it is a \((1+O())\)\(_{2}\) subspace embedding of the row space of \(SU^{}\). Then, by Theorem 47 of Woodruff (2014), the following is true with constant probability

\[\|(U^{}V^{}R)(SU^{}V^{}R)^{+}( SU^{}V^{})-f(UV)\|_{F}^{2}(1+)\|[f(U  V)]_{k}-f(U V)\|_{F}^{2}\]

Finally we bound the runtime of computing this product. Note that we may compute \(SU^{}\) and \(V^{}R\) in \(nr^{p}(k/+(k/^{3},r^{p}/^{2}))\) time. Then, note that the remaining products can be computed in

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_FAIL:11]

Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In _Advances in Neural Information Processing Systems_, 2014.
* Li et al. (2015) Shaohua Li, Jun Zhu, and Chunyan Miao. A generative word embedding model and its low rank positive semidefinite solution. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 1599-1609, 2015.
* Liang et al. (2020) Yingyu Liang, Zhao Song, Mengdi Wang, Lin Yang, and Xin Yang. Sketching transformed matrices with applications to natural language processing. In _International Conference on Artificial Intelligence and Statistics_, pages 467-481. PMLR, 2020.
* Likhosherstov et al. (2022) Valerii Likhosherstov, Krzysztof M Choromanski, Kumar Avinava Dubey, Frederick Liu, Tamas Sarlos, and Adrian Weller. Chefs' random tables: Non-trigonometric random features. In _Advances in Neural Information Processing Systems_, pages 34559-34573, 2022.
* Lokshtanov et al. (2013) Daniel Lokshtanov, Daniel Marx, Saket Saurabh, et al. Lower bounds based on the exponential time hypothesis. _Bulletin of EATCS_, 3(105), 2013.
* Musco and Musco (2017) Cameron Musco and Christopher Musco. Recursive sampling for the Nystrom method. In _Advances in Neural Information Processing Systems_, 2017.
* Musco and Woodruff (2017) Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of positive semidefinite matrices. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 672-683. IEEE, 2017.
* Rahimi and Recht (2007) Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In _Advances in Neural Information Processing Systems_, 2007.
* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _ACM Computing Surveys_, 55(6):1-28, 2022.
* Tsai et al. (2019) Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer's attention via the lens of kernel. In Kentaro Iuni, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 4343-4352. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1443. URL https://doi.org/10.18653/v1/D19-1443.
* Williams (2015) Virginia Vassilevska Williams. Hardness of easy problems: Basing hardness on popular conjectures such as the strong exponential time hypothesis (invited talk). In _10th International Symposium on Parameterized and Exact Computation (IPEC 2015)_. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems 2017_, pages 5998-6008, 2017.
* Williams (2005) Ryan Williams. A new algorithm for optimal 2-constraint satisfaction and its implications. _Theor. Comput. Sci._, 348(2-3):357-365, 2005.
* Woodruff (2014) David P Woodruff. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* Woodruff and Zhong (2016) David P Woodruff and Peilin Zhong. Distributed low rank approximation of implicit functions of a matrix. In _2016 IEEE 32nd International Conference on Data Engineering (ICDE)_, pages 847-858. IEEE, 2016.

Missing Lower Bound Proofs

Proof of Theorem 3.2.: Let \(U,V^{T}\{-1,0,1\}^{n r}\) be an instance of entrywise transformed low rank approximation with \(g\). WLOG, by scaling our solution, we can let \(c_{p}=1\). Let \(B=(n)\) be sufficiently large, and let \(=U/\) and \(=V/\). For each \(i,j[n]\), for \(B\) sufficiently large and using that \(r=n^{o(1)}\),

\[g(_{i}_{j}) = f(|_{i}_{j}|)=f(|U_{i}V_{j}|/B)=V_ {j}|^{p}}{B^{p}}-O(V_{j}|^{p+1}}{B^{p+1}})\] \[= V_{j}|^{p}}{B^{p}}-O(}{B^{p+1}}) =h(_{i}_{j})-O(}{B^{p+1}}).\]

where \(h(x)=|x|^{p}\). Hence, if \(^{}\), \(^{}\) are such that

\[\|^{}^{}-g()\|_{F}^ {2}\|[g()]_{k}-g()\|_ {F}^{2},\]

then by the triangle inequality,

\[\|^{}^{}-h( {V})\|_{F}  \|g()-h()\|_{F}+ \|[g()]_{k}-g()\|_ {F}\] \[ O(}{B^{p+1}})+\|[g( )]_{k}-g()\|_{F}.\]

Setting \(U^{}=^{}\) and \(V^{}=^{}\) and scaling both sides by \(B\), we have

\[\|U^{} V^{}-h(U V)\|_{F}  O(}{B^{p}})+B\|[g( )]_{k}-g()\|_{F}\] \[ O(}{B^{p}})+B\|[h( )]_{k}-g()\|_{F}\] \[ O(}{B^{p}})+B\|[h( )]_{k}-h()\|_{F}\] \[+B\|h()-g( )\|_{F}\] \[= O(nr^{p}}{B^{p}})+B\| [h()]_{k}-h()\|_{F}\] \[= O(nr^{p}}{B^{p}})+\| [h(U V)]_{k}-h(U V)\|_{F},\]

where in the second inequality we used that \([h()]_{k}\) has rank \(k\) whereas \([g()]_{k}\) is the best rank-\(k\) approximation to \(g()\) in Frobenius norm. Note that the third line is triangle inequality and the fourth line follows our entrywise approximation bounds.

Now by AM-GM, using that \(a b+c\) implies that \(a^{2} 2b^{2}+2c^{2}\) for \(a,b,c 0\), we have

\[\|U^{} V^{}-h(U V)\|_{F}^{2}=O(r ^{2p}}{B^{2p}})+2\|[h(U V)]_{k}-h(U V)\|_{F}^{2}.\]

Thus, \(U^{} V^{}\) is a rank-\(k\) approximation to \(h(U V)\) with additive error \((n)}\), by setting \(B\) to be large enough, and relative error \(2\). The total time to find \(U^{}\) and \(V^{}\) is the same as the time to solve entrywise transformed low rank approximation with respect to the function \(g\) on inputs \(\) and \(\), and thus by Theorem 3.1 is at least \((nd)^{1-o(1)}\). Note that applying the case \(U=V^{}\) in Theorem 3.1 establishes this theorem when \(=^{}\). 

Proof of Theorem 3.3.: Suppose \(A\) and \(B\) are the input sets to the Apx-Max-IP\({}_{n,d}\) Problem of Assumption 4 with parameter \(s=r\). We let the rows of \(U\) be the points in \(A\) and we let the columns of \(V\) be the points in \(B\). As in the proof of Theorem 3.1, we have \(f(U_{i}^{T}V_{j})=(U_{i}^{T}V_{j})^{p}\) so \(f(U V)=U^{} V^{}\). By Remark 5, there are at most \(n^{o(1)}\) entries of \(f(U V)\) that are at least \((m/100)^{p}\).

Hence, if we set \(k\) to be an appropriate value in \(n^{o(1)}\), then

\[\|[f(U V)]_{k}-f(U V)\|_{F}^{2}(nd)(m/100)^{2p},\]

since one possible rank-\(k\) approximation is just to zero out the at most \(n^{o(1)}\) entries of \(f(U V)\) that are at least \((m/100)^{p}\). By the correctness guarantee, with constant probability,

\[\|f(U V)WW^{}-f(U V)\|_{F}^{2}(nd)(m/100)^{2p}=O( nd)(m/100)^{2p}.\] (A.1)

We may assume WLOG that \(p C n\) for the moment, for a sufficiently large constant \(C>0\) since a trivial lower bound for outputting an LRA is \((n)\), so the inclusion of the \(2^{(p)}\) term in the lower bound allows us to make this assumption. Therefore, we conclude that \(f(U V)\) is in the column span of \(W\) up to \(1/(n)\) error. Consequently, consider the maximum entry \(m\), which we can assume is at least \(1\), as otherwise all points in \(A\) would have disjoint support from those in \(B\) but this can be verified in \(O(nr)=n^{1+o(1)}\) time, contradicting the \(n^{1+(1)}\) lower bound for \(d=n^{(1)}\) in Assumption 4.

It follows that if the maximum \(m\) occurs in the \((i,j)\)-th entry of \(f(U V)=U^{}V^{}\), then there exists a vector in the column span of \(U^{}\) for which the \(i\)-th entry is \([m^{p}(1-(n)),m^{p}(1+(n))]\) and all other entries are in the range \([-m^{p}/(n),m^{p}/(n)]\) since \((1/100)^{p}=1/(n)\). Consequently, since the column span of \(W\) is close to the column span of \(U^{}\) up to \(1/(n)\) error, we see that the column span of \(W\) also contains these almost-sparse vectors that have \(n^{o(1)}\) large entries. As argued before similarly in Theorem 3.1, the rows of these large dot products forces the row leverage scores of \(W\) to be \((1)\). Since \(W\) has rank \(k=n^{o(1)}\), in \(n^{1+o(1)}\) time we can find the set \(S\) of \(n^{o(1)}\) row leverage scores of \(W\) that are \((1)\). We can then explicitly compute all dot products between pairs of vectors in \(A S\) and \(B\) in \(n^{1+o(1)}\) total time, at which point we can output the maximum dot product, which includes \(m\). Therefore by Assumption 4, the total time to find \(W\) is at least \((nd)^{1-o(1)}\).

Proof of Theorem 4.3.: This proof follows by reducing low rank approximation to matrix multiplication by the same algorithm as in Theorem 4.1. Specifically, let us set \(k=n^{o(1)}\) and \(n=d\) and note that our low rank approximation algorithm's runtime is dominated by computing \(S f(UV)\) and \(f(UV) R\), where \(S,R\) are matrices with \((k/)\) rows and columns, respectively. In fact, those operations are the only terms that incur a polynomial dependence on \(n\).

Therefore, suppose there exists such a matrix multiplication algorithm that takes time \(O(n^{2-c})\) for some \(c\). Then this would directly imply that computing both \(S f(UV),f(UV) R\) takes \(O(n^{2-c})\) time as \([Sf(UV)]^{}=f(V^{}U^{})S^{}\). By our guarantees in Theorem 47 of Woodruff (2014), this implies a constant relative error LRA in time \(O(n^{2-c})\), which contradicts Theorem 3.2. 

## Appendix B Missing Upper Bound Proofs

Proof of Theorem 4.2.: Again, \(f(U_{i}V_{j})=(U_{i}V_{j})^{p}\) so we may rewrite \(f(U V)=U^{}V^{}\), where each row of \(U^{}^{n r^{p}}\) is the Khatri-Rao product of itself \(p\) times, and each column of \(V^{}^{r^{p} d}\) is the Khatri-Rao product of itself \(p\) times.

Using Theorem 1 of Ahle et al. (2020), we see that with probability \(0.9\), our approximate matrix product guarantees hold for \(U^{},V^{}\) such that if \(\) is an \(m r^{p}\) TensorSRHT matrix with \(m=(p/^{2})\), then

\[\|U^{}^{} V^{}-U^{}V^{ }\|_{F}\|U^{}\|_{F}\|V^{}\|_{F}\]

Therefore, we can use the approximate low rank sketches again to approximately solve:

\[_{UV}\|U^{}^{} V^{}-UV\|_{F}^{2}\]

Specifically, let \(S\) be a random Gaussian sketching matrix with \(O(k/)\) rows. We know that these matrices satisfy the \((,9/10,l)\)-JL property Woodruff (2014). Furthermore, let \(R\) be a random Gaussian matrix with \(O(m/^{2})\) columns, so we know that it is a \((1+O())\)\(_{2}\) subspace embedding of the row space of \(SU^{}^{}\), which has rank at most \(m\). Let \(U^{}=U^{}^{}\) and let \( V^{}=V^{}\). Then, by Theorem 47 of Woodruff , the following is true with constant probability

\[\|(U^{}V^{}R)(SU^{}V^{ }R)^{+}(SU^{}V^{})-U^{ }V^{}\|_{F}(1+)_{U,V }\|UV-U^{}V^{}\|_{F}\]

Finally we bound the runtime of computing this product. Note that we may compute \(U^{}\) and \(V^{}\) in time \(O(np(m+r))\) time. Then, computing \(SU^{}\), \(V^{}R\) can be done in \(nm(k/+m/^{2})\) time. Lastly, the remaining products can be computed in \((kp/)\) and the final rank \(k\) decomposition can be computed and we can find \(U^{},V^{}\) such that

\[\|U^{}V^{}-U^{}V^{}\|_{F}( 1+)_{U,V}\|UV-U^{}V^{}\|_{F}\]

Now, let \(U^{},V^{}\) be the optimal rank \(k\) decomposition of the original problem of \(f(UV)\), then by the guarantees of approximate matrix product and the triangle inequality, \(\|U^{}V^{}-U^{}V^{}\|_{F}\|U^{}V^{ }-U^{}^{} V^{}\|_{F}-\|U^{ }\|_{F}\|V^{}\|_{F}\).

Therefore, we conclude that

\[\|U^{}V^{}-U^{}^{} V^{ }\|_{F} (1+)\|U^{}V^{}-U^{}^{}  V^{}\|_{F}\] \[(1+)\|U^{}V^{}-U^{}V^{ }\|_{F}+2\|U^{}\|_{F}\|V^{}\|_{F}\]

We end by rewriting \(\|U^{}\|_{F}^{2}=(U^{}U^{ })=_{i=1}^{n}(U_{i}^{}U_{i})^{p}=_{i}\|U_{i}\|_{2}^{2p}\) and similarly for \(\|V^{}\|_{F}^{2}\) and then applying AM-GM and noting that \((1+)^{2}=1+O()\).