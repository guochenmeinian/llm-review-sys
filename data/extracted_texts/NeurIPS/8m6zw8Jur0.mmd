# Image2Struct: Benchmarking Structure Extraction for Vision-Language Models

Josselin Somerville Roberts

Stanford University

&Tony Lee

Stanford University

&Chi Heem Wong

Stanford University

Hitachi America, Ltd

&Michihiro Yasunaga

Stanford University

&Yifan Mai

Stanford University

&Percy Liang

Stanford University

These authors contributed equally to this work.

###### Abstract

We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data. In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot). The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score. This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures. We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention. We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs. Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty. For transparency, we release the full results at https://crfm.stanford.edu/helm/image2struct/v1.0.1/.

## 1 Introduction

Vision-language models (VLMs) unlock the ability to process both visual and language information. They have found uses in generative search engines , visual question-answering , text-driven image creation and alteration , image captioning , and robotics . However, evaluating responses from the VLMs is a major challenge in VLM benchmarking. Many existing benchmarks either require humans to evaluate long and complex outputs  or are designed as multiple choice question answering (MCQA) . The former is costly and slow while the latter cannot represent many real-world use cases such as code generation.

We address these issues in Image2Struct, a benchmark that measures the ability of VLMs to extract structures such as tables, equations, figures, or document forms from images. Our proposed task encompasses many real-world use cases such as converting an image of an equation to LaTeX orgenerating HTML given a screenshot of a webpage and the existence of non-VLM commercial software such as Mathpix to convert equations to LaTeX or Codia AI to convert an image to HTML further demonstrates the practicality and value of our proposed task.

Figure 1 illustrates the three-stage process in Image2Struct. First, we present an image \(x\) to a VLM to obtain the structure \(y\) (e.g., LaTeX code representing an equation). Note that there could be many valid structures for the same input image (see Appendix E for an example of multiple valid structures). Second, the output from the VLM is rendered into an image \(\) with a task-specific renderer (e.g., TeX engine for LaTeX code). Third, the rendered image is compared against the input image and their similarity is quantified using automatic metrics, including two that we developed: cosine similarity between the Inception vectors (CIS) and earth mover's similarity (EMS). CIS uses a deep convolutional neural network to quantify image similarity whereas the EMS modifies the typical earth mover distance to compute similarities between patches in an image. We show that these metrics have high correlation with the Levenshtein edit distance between the predicted structure and the ground-truth structure in Section 4.2. The round-trip comparison between the input image \(x\) and rendered image \(\) avoids the need to have ground-truth structures for our test instances. As such, we skip the labor-intensive process of annotating images with possibly non-unique ground-truth structures.

We instantiate Image2Struct in three domains: webpages (structures in HTML, CSS, and Javascript), LaTeX (consisting of equations, plots etc.), and music scores (in LilyPond). We obtain data by downloading real and fresh data from online sources with active communities of users (e.g., arXiv). Each instance consists of a screenshot, which will be the image input to the VLM. In addition to the screenshot, we obtain the underlying structure so as to provide ground-truth source codes to validate our image similarity metrics. We emphasise that the ground-truth structures are not necessary for evaluation in our benchmark. We provide an example instance from each domain in Figure 2. By being able to control the time that the data is uploaded to be recent, we minimize the risk of data leakage between the test data and the data used to train the model.

We evaluate the performances of 14 prominent VLMs: Claude 3 Opus , Claude 3 Sonnet , Claude 3.5 Sonnet , Gemini 1.0 Pro Vision , Gemini 1.5 Pro , GPT-4 Omni , GPT-4 Vision , LLaVA , LLaVA NeXT , IDEFICS Instruct 9B , IDEFICS Instruct 80B , IDEFICS2 8B , Palmyra Vision 003 , and Qwen-VL Chat . We find that performance varies considerably across models, indicating that Image2Struct is able to distinguish between models. At the time of writing, closed-API models outperform open-weight models, with GPT-4 Omni being the best-performing VLM when measured by the mean win rate across the different tasks. While it performs the best on Webpages and LaTeX, it ranks third in Musical Scores, indicating that no model dominates in all domains. We find that VLMs perform better on some domains than others (e.g., a maximum EMS of 0.660 for LaTeX vs 0.402 for sheet music), and on certain subsplits within a domains (e.g., a maximum EMS of 0.830 for equation vs 0.617 for plot in LaTeX). Overall, Image2Struct is a challenging benchmark where no model is able to perform well yet.

Figure 1: An overview of the evaluation process in Image2Struct. A VLM is presented with an image \(x\) and the instructions to generate the underlying structure (e.g., code representing an equation in LaTeX). The predicted structure \(y\) is used to create the rendered image \(\), which is then evaluated against the input image \(x\) to produce a score. In this example, the VLM produced a partially correct structure. Please refer to Section 2.2 for details about Earth Mover Similarity and other metrics.

For transparency, we publish a leaderboard and release all the text prompts, input images, predicted structures, reconstructed images, and scores at https://crfm.stanford.edu/helm/image2struct/v1.0.1/#/leaderboard.

## 2 Image2Struct

In an Image2Struct task, an input image \(x\) is fed into an VLM to produce a structure \(y=(x)\), which is then fed into a renderer to produce \(=(y)\). We instantiate Image2Struct with instances in three domains: **Webpages**, **LaTeX**, and **Musical Scores**. For webpages, VLMs are required to generate HTML, CSS, or Javascript code given their screenshots. For scientific documents, we restrict our test instances to screenshots of diagrams (such as charts, tables, equations, and algorithms) and make the VLMs generate the LaTeX code that recreates them. For sheet music, the VLMs are asked to generate LilyPond--a computer program for typesetting music scores--code from the image.

### Dataset collection

As can be seen from Figure 3, our general data and evaluation pipeline involves 1) downloading data from a live source, 2) data filtering and conditioning, 3) retrieving output from the VLMs, 4)

Figure 2: In Image2Struct tasks, given an input image, the goal is to produce a structure (e.g., LaTeX code), so that the rendering of the structure produces the original image. We include three domains: Webpages, LaTeX, and Musical scores. We show an example of the input image, model predicted structure, and rendered image for each of the domains in our benchmark.

rendering the outputs into images, and 5) computing the metric. We describe only (1) and (2) in this section and elaborate on (3)-(5) in the next.

We collect data from active communities of users who upload and consume new data on a regular basis, ensuring that we will have continued streams of fresh, human-generated data. We scrape only data that is uploaded between Jan 1, 2024 and Feb 29, 2024 to minimize the risk of data leakage between the test data and the data used to train the model.

The downloaded data is then filtered for relevance, diversity, and toxicity after being downloaded to ensure quality. The Perspective API is deployed to filter toxic content when needed. De-duplication across all tasks is achieved by computing and comparing the hashes of the images. We detail the idiosyncratic steps taken for each of the domains in sections 2.1.1 to 2.1.3. We collect 300 valid test instances per subdomain with a maximum of 40 instances on a single day in order to induce temporal diversity. In all, we collected a total of 900 instances for webpages (300 each for HTML, CSS and JS), 1200 instances for LaTeX (300 each for equations, tables, algorithms, and plots), and 300 for music for a grand total of 2400 test instances.

#### 2.1.1 Webpages

Webpages are downloaded from GitHub Pages, a developer platform that allows users to publish webpages from their code repositories. To do this, we first obtain a list of repositories and their upload dates using the GitHub Developer API. We identify only webpages that contain "github.io" in their names, contain mainly CSS, HTML, or Javascript as their main languages, and are served using Jekyll (the default engine used in Github Pages).

We download only repositories that are at most 100KB in total size and remove those that contain toxic content before applying the size filters to ensure that inputs and expected outputs are not too long. The size filters specify that a repository must contain i) more than just a README.md file, ii) at most 5 code (e.g., CSS, HTML, or Javascript) files, iii) at most 5 non-code assets (e.g., images), iv) between 10 and 1000 lines of HTML and Javascript code in total, and v) between 0 and 2000 lines of CSS code in total. Webpages in our dataset contain an average of one asset.

After downloading the repositories, we apply the Perspective toxicity filter to remove webpages with unsafe content. After which, we use Jekyll to serve the webpages from the code and deploy Selenium to visit the webpages. We take screenshots of the webpages without resizing or scrolling, similar to what a person would see on a browser with a native resolution of 1920x1080 pixels. As such, the dynamic aspects of webpages are not considered in Image2Struct. Screenshots that are completely or nearly completely white are removed from the dataset and the remaining images are de-duplicated. In all, we collect 900 test instances consisting of 300 each for HTML, CSS, and Javascript. The instances can be found at https://huggingface.co/datasets/stanford-crfm/image2struct-webpage-v1.

Figure 3: Our pipeline for evaluation using the example of LaTeX. First, we download data from online sources. Second, we filter and process the images. Third, we prompt the VLMs with these images to produce output structures. Fourth, we render the the structures and finally evaluate the rendered images by comparing the rendered images against the input images.

#### 2.1.2 LaTeX

We download scientific documents from arXiv, an open-access repository of academic pre-prints and papers. We first use the arxivscraper  to obtain the metadata of and links to the arXiv papers from the Open Archives Initiative (OAI) before downloading the papers directly from arXiv. We apply the Perspective toxicity filter on the text to remove unwanted documents and extract the desired portions (i.e., equation, algorithm, tikz, or table) from the documents. Custom LaTeX document headers are injected in order to standardize the rendering of the documents and the resulting LaTeX source code is rendered into PDFs before being converted to 200 DPI Portable Network Graphics (PNG) images using the pdf2Image library. The images are cropped to the smallest possible bounding box that includes all the LaTeX generation. Data points where the rendered image is mostly blank are discarded. We select the final data set in a way that balances the number of instances per subject (e.g., physics, maths, or economics) and structures (e.g., equation or algorithm). In all, we collect 1200 test instances consisting of 300 each for equations, tables, algorithms, and plots. The instances can be found at https://huggingface.co/datasets/stanford-crfm/image2struct-latex-v1.

#### 2.1.3 Musical Scores

We obtain sheet music from the International Music Score Library Project (IMSLP), a digital library that hosts sheet music that is either out of copyright or released under a Creative Commons license. We begin by downloading files that are uploaded within the desired time frame (i.e., Jan 01, 2024 - Feb 29, 2024) directly from the IMSLP website. Since many of the pages in a file are not sheet music (e.g., cover or blank page), we train a convolutional neural network to classify whether a page is a musical score. To do this, we manually labeled sheet music uploaded before 2012 to create a training data set consisting of 200 sheet music and 200 non-sheet music pages and a test set consisting of 500 examples. We then fine-tuned a ResNet-18 on the train set for two epochs using stochastic gradient descent with a learning rate of 0.001 and momentum of 0.9. The final model achieves an accuracy of 99.2% on the test set. We identify music sheets with the fine-tuned model and further filter for _digital_ sheet music (in contrast to scanned ones) by imposing the criterion that the most common color is white, which works because scanned scores tend to contain a greyish tint. We crop the sheet music to create random, short subsections so that the expected predicted structures fit within the context window of the VLMs; this is achieved by detecting alternating black and white pixel zones on a vertical line. Finally, de-duplication is performed on the images to create the final set of 300 test instances. We emphasize that there is no ground-truth structure for the test instances in this domain. The instances can be accessed at https://huggingface.co/datasets/stanford-crfm/image2struct-musicsheet-v1.

### Image similarity metrics

We would like to score a VLM based on how similar the generated image is to the original. A similarity metric takes two images, \(x\) and \(\), and computes a score, \((x,)\). We take the view that unsuccessful rendering should be counted as absolute failures (i.e., the score is zero if the code does not compile). In Image2Struct, we normalize metrics within the unit range so that they can be interpreted easily; a score of zero implies complete dissimilarity whereas a score of 1 implies that the images are identical. Without loss of generality, we assume both \(x\) and \(\) are of dimensions \(W H\).

We use 3 often used image similarity metrics--pixel similarity, Learned Perceptual Image Patch Similarity (LPIPS), structural similarity index measure (SSIM)--and introduce 2 new ones: cosine similarity between the Inception vectors (CIS) and Earth Mover Similarity (EMS), a novel adaptation of earth mover's distance that better captures content similarity. CIS and EMS are introduced to improve performance and computation speeds over 'classical' metrics. Separately, when a reference is available, we compute the Levenshtein edit distance between the predicted structure and the reference to check its correlation with the image metrics. Due to page constraints, we describe the new metrics here and elaborate on the classical metrics in Appendix A.

Cosine similarity between Inception vectors (CIS).CIS is a simplification of LPIPS; instead of taking the weighted distance between several layers of a neural network, we use only the penultimate layer of a convolutional neural network (CNN). This is driven by the intuition that two images are similar if their embeddings in the penultimate layer of the CNN are close. To compute the CIS, we feed the images \(x\) and \(\) through Inception v3 to obtain the activation vectors \(A(x)\) and \(A()\) andcalculate their cosine similarity, which is then scaled and shifted to be between 0 and 1.

\[(x,)=[1+) A(x)}{\|A() \|\|A(x)\|}]\] (1)

[Block-] Earth Mover Similarity (EMS).Inspired by the Earth Mover's distance (EMD) , which is a measure of dissimilarity between two frequency distributions, we introduce a new image metric that is scalable to high resolution images.

In the classic EMD, images are first converted to grayscale and their signatures are computed. A cost matrix is defined and an optimization problem is solved to obtain the minimum flow between probability masses. Spatial information is lost and the EMD metric is invariant to translation, reflection, and other pixel rearrangements. We refer readers to Appendix A for a detailed description of the classic EMD.

We modify the classic EMD by defining multidimensional signatures that consider the pixels' x- and y-coordinates in addition to their values. Let \(N\) be the number of possible pixel values and recall that \(W\) and \(H\) are the width and height of the images, respectively. The support of our multidimensional signature, \(_{p}\), is all the possible combinations of the x-coordinates (\(^{x}\)), y-coordinates (\(^{y}\)), and the \(N\) possible pixel values (\(^{v}\)):

\[_{p}(x)=\{((^{x},^{y},^{v})_{k},w_{k}):k\{0,1, ,WHN\}\}\] (2)

The probability mass, \(w_{k}\), takes the value of either \(\) or 0. The complexity of computing the cost matrix over \(_{p}\) is O(\(W^{2}H^{2}\)), making it difficult to compute for high resolution images. We therefore compute an approximated patch version of it, which we denoted as EMDblock.

In EMDblock, we first split two images, \(x\) and \(\), each into \(K\) patches of dimensions \(r s\): \(P^{0}_{x},,P^{K-1}_{x}\) (recall that \(x\) and \(\) are assumed to have the same dimensions). Our implementation sets \(r\) and \(s\) individually for every image such that there are 8\(\)8 patches in every image. To compare two patches \(P^{t}_{x}\) and \(P^{u}_{}\), we treat each patch as separate images and compute the EMD using the multidimensional signature defined in Equation (2), which we will denote as EMD\({}_{p}(P^{t}_{x},P^{u}_{})\). Note that each patch will have x- and y- coordinates within the original image. Next, we define a separate cost matrix, \(C_{p}\), such that the cost of moving one patch to another is the sum of the EMD between the patches and the Euclidean distance between them:

\[C_{p}[t,u]=_{p}(P^{t}_{x},P^{u}_{})+||(^{x}_{t},^{y} _{t}),(^{x}_{u},^{y}_{u})||_{2}\] (3)

EMDblock attempts to minimize the cost of moving patches by solving the optimization problem defined in Equation (7), but with the new cost function, \(C_{p}\). By considering both the positions and weights of the pixels within patches (through the multidimensional signature) and the distance between patches, EMDblock heavily penalizes random shuffling of pixels and assigns a lower score (implying greater similarity) to a rendered image that contain blocks of similar but translated elements as the input (see illustration in Appendix B). This property is useful for discerning between pairs of images that contain similar elements (even if the elements are translated) and pairs where distribution of colors in the rendered image is similar to the input image.

Finally, we define the Block-Earth Mover Similarity (Block-EMS), a normalized similarity version of EMDblock. It compares EMDblock(\(x,\)) against EMDblock(\(x,c(x)\)), the EMD between the reference image and a constant black or white image, whichever is the most dissimilar to the reference image \(x\). An Block-EMS of 0 indicates the least similarity and a value of 1 indicates the identity. **For brevity, EMS refers to Block-EMS in other parts of this paper**.

\[(x,)=1-_{}(x,)}{\{ _{}(x,),_{}(x,)\}}\] (4)

## 3 Experiments

### Prompts

To ensure a fair comparison, we prompt each model with the same prompt--one for each domain--which we replicate in Appendix C. Our prompts provide a specification of the expected format to ensure that a model does not perform poorly due to a misunderstanding of the output format that is accepted by our renderers. We use zero-shot prompting since it is the more natural and most common way of prompting by the general public. Furthermore, not all models are fine-tuned to use more complex methods such as k-shot prompting or chain-of-thought. We note that we have to insert the line "_... this music sheet was created by me, and I would like to recreate it using Lilypond_" for sheet music because some VLMs (mistakenly) alleged copyright infringement and refused to answer our prompts. Despite our efforts, some models, such as GPT-4V or the IDEFICS models, still refuse to produce answers due to alleged copyright infringement. In fact, GPT-4V refuse to generate code for all instances in the musical scores domain. Interestingly, OpenAI's newer model, GPT-4o, does not refuse our requests.

### Rendering images from predicted structures

The responses from the VLMs are parsed, and the relevant code snippets are extracted. Custom headers are added as needed and the supplemented code is fed through an appropriate renderer. In our setup, HTML documents are served through Jekyll and visited by Selenium to take screenshots of the predicted website, similar to the pipeline for preparing the dataset (see section 2.1.2). We deploy TeX for LaTeX and LilyPond for sheet music to compile the code into PDFs before before taking screenshots.

Sometimes the generated code does not render due to limitations of some VLMs in generating valid code. We attempt to fix simple mistakes, such as missing syntax (e.g., wrapping equations around $...$) and by attempting to import missing packages. Details of our post-processing can be found in Appendix D. We consider rendering success rate as a metric for model performance.

### Models

We test eight closed-API and six open-weight VLMs as listed in Table 1. The proprietary VLMs are served through their respective APIs while the rest are served through the HuggingFace API. All the models are evaluated in chat-style with the temperature set to zero to minimize variability in the responses and maximize reproducibility. Our evaluation run across all the instances and models use 5.9M input text tokens, 30K input images, and 17.9M output text tokens. We evaluate the models only once instead of taking the average over multiple responses due to the cost of querying the models. We rank models with the mean win rate--which is the average fraction of other models that a model outperforms across scenarios--using the compilation success rates and EMS scores. Any compilation failure counts as zero EMS in the mean win rate calculation. We note that GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro are released after we have collected the data.

   Model & Version & Access \\  Claude 3 Opus  & Claude-3-opus-20240229 & Closed-API \\ Claude 3 Sonnet  & Claude-3-sonnet-20240229 & Closed-API \\ Claude 3.5 Sonnet  & Claude-3-5-sonnet-20240620 & Closed-API \\ Gemini 1.0 Pro Vision  & gemini-1.0-pro-vision-001 & Closed-API \\ Gemini 1.5 Pro  & gemini-1.5-pro-preview-0409 & Closed-API \\ GPT-4 Omni aka GPT-4o & gpt-4o-2024-05-13 & Closed-API \\ GPT-4 Vision aka GPT-4V & gpt-4.1106-vision-preview & Closed-API \\ IDEFICS Instruct 9B  & idefics-9b-instruct & Open-weight \\ IDEFICS Instruct 80B  & idefics-80b-instruct & Open-weight \\ IDEFICS2 8B  & idefics2-8b & Open-weight \\ LLaV v1.5  & llava-1.5-13b-hf & Open-weight \\ LLaV v1.6  & llava-v1.6-vicuna-13b-hf & Open-weight \\ Palmyra Vision 003  & palmryra-vision-003 & Closed-API \\ Qwen-VL Chat  & gwen-vl-chat & Open-weight \\   

Table 1: VLMs that are tested in the initial benchmark.

## 4 Results

### Model performance

Table 2 summarizes the performances of the VLMs and the breakdown across the different tasks and subtasks can be found in Appendix H. In general, we find that VLMs are unable to perform well on our tasks, with the best performing model achieving a maximum EMS of 0.724 in Webpages. Across all the models and subtasks, the average EMS is 0.324 for LaTeX, 0.370 for Webpages, and 0.069 for musical scores, indicating that the benchmark is not saturated and that there is a lot of room for VLMs to improve.

The VLMs perform better on Webpages and LaTeX than Musical Scores. For example, the best overall performing model (GPT-4o) achieves a rendering success rate (RSR) of 0.807 and an EMS score of 0.660 for LaTeX, an RSR of 0.980 and EMS of 0.710 for Webpages, but a RSR of only 0.491 and an EMS of only 0.340 for Musical Scores. We hypothesize that the disparity in performance between the domains may be due to the relative abundance of training data points describing LaTeX, HTML, CSS, and other formats used in web development in contrast to LilyPond. We observe differences within the domains too; for example, most models perform well on equations in LaTeX but struggle on plots (see Table A2).

Our initial benchmarking also shows that closed-API models perform significantly better than open-weight ones. The best-performing open-weight model has a lower mean win rate than the worst closed-API model. The top-performing models have different niches, and there is no model that outperforms the rest in all the tasks. While GPT-4o claims the overall top spot on our leaderboard, by being the overall best in Webpages and LaTeX while ranking third in Musical Scores. We reproduce some model predictions in Figure 4 and Appendix F.

We perform an error analysis (see Appendix I) and notice that the VLMs can extract the elements (e.g., text, images) but are unable to pick up on visual nuances. Furthermore, while some models are able to produce valid LilyPond code, none of the models we tested have the capability to interpret sheet music (see Figure 4c).

### Comparison of metrics

We have introduced many image metrics to compare the rendered images against the input. In the case where ground-truth structures are available, we can compare the structures instead. Computing the structure similarity (e.g., using Levenshtein edit distance) potentially captures semantics to a

    &  &  &  \\   & Mean & Rendering & & Rendering & & Rendering & \\ Model & win rate & success & EMS & success & EMS & success & EMS \\  GPT-4 Omni & **0.923** & 0.807 & **0.660** & **0.980** & 0.710 & 0.491 & 0.340 \\ Claude 3.5 Sonnet & 0.821 & 0.756 & 0.611 & 0.972 & **0.724** & 0.455 & 0.317 \\ Gemini 1.5 Pro & 0.769 & 0.770 & 0.616 & 0.971 & 0.683 & 0.311 & 0.220 \\ Claude3 Opus & 0.731 & **0.836** & 0.632 & 0.655 & 0.378 & 0.551 & 0.367 \\ Gemini 1.0 Pro Vision & 0.615 & 0.519 & 0.402 & 0.789 & 0.499 & **0.583** & **0.402** \\ GPT-4 Vision & 0.577 & 0.758 & 0.598 & 0.957 & 0.653 & 0.000 & 0.000 \\ Palmyra Vision 003 & 0.564 & 0.738 & 0.537 & 0.884 & 0.640 & 0.103 & 0.072 \\ Claude3 Sonnet & 0.551 & 0.693 & 0.520 & 0.956 & 0.642 & 0.238 & 0.167 \\ LLaVA 1.6 (13B) & 0.385 & 0.345 & 0.247 & 0.731 & 0.447 & 0.417 & 0.258 \\ LLaVA 1.5 (13B) & 0.321 & 0.393 & 0.256 & 0.773 & 0.435 & 0.040 & 0.026 \\ Qwen-VL Chat & 0.256 & 0.732 & 0.525 & 0.031 & 0.008 & 0.000 & 0.000 \\ IDEFICS-instruct (9B) & 0.192 & 0.704 & 0.490 & 0.001 & 0.001 & 0.000 & 0.000 \\ IDEFICS 2 (8B) & 0.179 & 0.416 & 0.287 & 0.000 & 0.000 & 0.010 & 0.007 \\ IDEFICS-instruct (80B) & 0.115 & 0.357 & 0.240 & 0.001 & 0.001 & 0.000 & 0.000 \\   

Table 2: Image2Struct evaluation results. The EMS score is conditioned on successful rendering. VLMs generally perform the best on Webpages, followed by LaTeX and then Musical Scores. The best performing model (GPT-4o) achieves a maximum rendering success rate of 0.977 and EMS of 0.708 on the ‘easiest’ domain (Webpages), indicating that the benchmark is not saturated.

better extent. We show the correlation between the various metrics in Table A1 and observe that the Earth Mover's Similarity (EMS) and cosine similarity between Inception vectors (CIS) correlate highly (0.79 and 0.80, respectively) with structural similarity.

### Test on misspecified data

We test the models on 4 additional examples, 2 in LaTeX and 2 in Webpages. The input images for LaTeX are sourced from Word documents whereas those for Webpages are obtained from magazines (see Appendix J). The most powerful models, such as GPT-4o or Gemini 1.5 Pro, are able to produce valid structures despite the fact that images are generated from HTML or LaTeX. For LaTeX, the best rendered images contain the correct text and equations but cannot replicate the alignment of paragraphs. In some cases, the VLMs do not follow instructions to replicate the input but instead attempt to answer the questions in the images. For Webpages, the VLMs are able to extract most of the text but make mistakes in their positions or colors. This exercise demonstrates that our tasks are generalizable to structures that are not specified in the original language. Detailed results are available in Appendix K.

Figure 4: Example model predictions for LaTeX (equation), LaTeX (Plot), and Music tasks. Results in the domain of Webpages, as well as additional instances of LaTeX, can be found in Appendix F.

Related works

VLM benchmarks.Benchmarks have been proposed to assess the ability of VLMs to perform different tasks, such as image captioning, visual question answering, or action recognition, among others [40; 20; 42; 43; 10; 16; 8]. However, existing benchmarks suffer from several limitations, such as ambiguity in evaluation, difficulty in scaling, and data leakage . Benchmarks evaluate VLM responses either with human feedback [41; 40] or against reference outputs [20; 42; 43]. The former is suitable for the evaluation of long-form generations, but is expensive and difficult to reproduce. While some attempts use VLMs such as GPT-4V to evaluate the outputs of another, they are not yet reliable replacements for human annotators [45; 30]. Evaluation against reference outputs, on the other hand, are cheap and easily reproducible, but it is difficult to handle complex generation tasks. Furthermore, generating new test instances is non-trivial, and many benchmarks rely on existing data sources such as textbooks, prep books , or simply other databases [39; 30]. As a result, benchmarks are expensive to update and risk being incorporated into VLM training data, leading to data leakage . In contrast, Image2Struct captures real-world use cases, is fully automatic and does not require human judgment, and uses fresh data so that it is difficult to game.

Other benchmarks for image-to-code.Several datasets suitable for evaluating images-to-code have been released, some during Image2Struct's review period. Most of them focus on the task of converting images of webpages to HTML and contain data that are either manually curated or synthetic. Soselia et al.  creates two synthetic datasets with over 75,000 unique (code, screenshot) pairs to train a machine-learning model to produce HTML/CSS code from UI screenshots. Si et al.  curate a benchmark of 484 diverse real-world webpages by filtering the C4 dataset to test the ability of VLMs on converting visual designs into code implementations. Wan et al.  manually curate a dataset of "1,000 top-ranked real-world websites" and propose a method to translate webpage design to code. In contrast to Image2Struct, where the screenshots of the webpages are fed to the VLMs as-is, Wan et al. replaces images with placeholders and removes all external links and "elements that do not impact website's appearance" before rendering them as input images. Plot2Code  manually curates 132 matplotlib plots sourced from matplotlib galleries and tests VLMs in generating Python code from them. In contrast to these benchmarks, Image2Struct automatically obtains fresh, real-world data from online communities to test structure extraction from images across multiple domains.

Metrics.There is a body of research on image comparison. Apart the metrics used in Image2Struct (i.e., LPIPS , EMD, or SSIM ), one may also employ the CLIP Score  or cross-correlation, or CIEDE2000 color difference. The task of quantifying image similarity is related to pattern matching, where Siamese networks are used for facial recognition  and SIFT is used for local feature matching . If the images are first broken down into visual elements (aka blocks), Block-match--which measures the ratio of matched block sizes to all block sizes--and the mean distance between matched blocks can be used to assess similarity [30; 35].

Round-trip evaluation.The round-trip evaluation idea used in Image2Struct (input image \(\) structure \(\) rendered image) is related to backtranslation [28; 11; 12] and cycle-consistency [46; 9]. While existing works focus on _training_ models for bidirectional mapping, we focus on _evaluating_ the models.

## 6 Discussion

VLM sensitivity to prompts and adaptations.VLMs are sensitive to prompts and our standardized text prompts may impact model evaluations. This is most evident when some models initially refuse to produce LilyPond codes but then comply when we state that the images are created by us. Beyond zero-shot prompting, there exist a variety of adaptation methods--specific procedures for invoking a model--such as chain-of-thoughts  or auto-prompt  that can enhance the performance of the models. We leave measuring the performance of VLMs under other adaptations as future work.

Fine-tuning for structure extraction.Image2Struct proposes the task of extracting structure from images and uses a round-trip comparison methodology to evaluate the model. On the one hand, it would be interesting to explore fine-tuning VLMs using the round-trip supervision to perform these useful tasks. On the other hand, developers may simply fine-tune on the Image2Struct dataset and therefore overfit on the tasks. Ideally, VLMs will acquire the capability to extract and reason over any structured data on image media.

Updates with fresh data.Image2Struct is amenable to rapid updates; the active online communities from which we download test instances from provide ample fresh data and the round-trip evaluation methodology avoids the need to label the newly downloaded data. As such we can quickly test new models with unseen data and thereby avoid the issue of data leakage. We note that the benchmark must be rerun on all the models after a data refresh so that an apples-to-apples comparison can be made. An exponential weighing scheme that prioritizes the latest batch of test instances can be used to incorporate both past and new test data.

### Limitations

Imperfect metrics.The automated evaluation in Image2Struct hinges on having good metrics to compare the output image to the input image. The metrics introduced in this paper are not perfect, even though they have a high correlation with the edit distance between the generated and ground-truth source code. The EMS, for example, is still unable to discern if key elements exist in two images if the elements undergo other affine transformations beyond translation. Our work motivates future research in evaluating the similarity between rendered and ground-truth images, including developing evaluator VLMs that can be deployed as automatic evaluators to quantify image similarities.

Limited scope of evaluation.Our benchmark is limited in what it evaluates in several aspects. Firstly, it focuses solely on measuring structure extraction and does not capture tasks that involve either producing more abstract concepts or reasoning over extracted structure. Secondly, performing well on Image2Structure requires the VLMs to have knowledge of formal languages (e.g., HTML or LaTeX) in addition to visual understanding; as such, it does not discern between a model that is excellent at understanding structured data but is poor in a language, and, an model that is simply poor at understanding structured information. Thirdly, we do not measure robustness of the VLMs to noise (e.g., scans) and other perturbations (e.g., image skew). We leave these possible extensions as future work.

### Broader Impacts

The task of extracting the structured data embedded in an input is applicable to a very broad and practical set of tasks. Beyond parsing images of webpages, scientific documents, and sheet music, we envision the same framework can be extended to extract information from visual content in various domains (e.g., radiology images, electronic health records) and modalities (e.g., 3D graphics). It is possible that in the future, one will be able to command a VLM to convert a natural image into a structured form so that it can be edited.

This work also introduces automatic, repeatable, and reproducible evaluation methods, as well as method to produce fresh test sets for fair evaluation. This promotes transparency and accountability in model development and stakeholders (including researchers, developers, and policymakers) can better understand and compare the performance of different VLMs.

Our evaluation framework allows the development of powerful VLMs which may displace existing workers (e.g., data entry personnel). The ease of replicating and editing rendered structures can be misused and we urge developers to consider the implications of their technology and take appropriate measures to safeguard against misuse.

## 7 Conclusion

We introduced Image2Struct, a benchmark for evaluating VLMs in extracting structure from images, such as webpages, LaTeX, and music sheets. We enabled evaluation on fresh and diverse test examples by continuously downloading real, latest data, and developing automated metrics that compare images rendered from model predictions against the original images.