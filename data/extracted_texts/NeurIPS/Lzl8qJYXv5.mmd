# Total predictive uncertainty.

[MISSING_PAGE_FAIL:1]

**Related work.** Hallucination prediction and mitigation is an active area of research [9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30]. This work is most closely related to a subset of methods based on uncertainty quantification [31; 32; 33; 34; 35; 36; 37; 38]--particularly, those methods that aim to predict hallucinations based on uncertainty about the _meaning_ of generated responses [33; 34; 36]. Unlike those methods, the PHR does not require external information from auxiliary classifiers and is applicable beyond language tasks. Our approach is enabled by sampling ICL dataset completions from the predictive distribution, which was first explored in the context of sequential models by Fong et al. . Extensions to various CGMs are an active area of research [40; 41]. Notably, Falck et al.  explores the hypothesis that in-context learning (ICL) performs Bayesian inference. They propose a similar method to ours for testing this hypothesis and present several ICL scenarios where it does not hold. However, their tests are stringent, and while ICL does not conform to Bayesian inference under these strict conditions, we demonstrate in our work that adopting a Bayesian perspective for ICL still offers significant practical benefits.

**Contributions.** In Section 2, we introduce the _posterior hallucination rate_ for Bayesian CGMs, prove it is computable by sampling from the predictive distribution, and provide a finite-sample estimator for CGMs that only requires evaluating the log probabilities of responses. In Section 3, we empirically evaluate our methods. First, we study the PHR estimator with synthetic data to demonstrate that it can accurately predict the true hallucination rate. We then study our methods on natural language ICL problems with pre-trained CGMs from the Llama-2  and Gemma-2  model families. We demonstrate that the PHR estimator gives accurate estimates of the empirical error rate. We provide implementations of the method proposed and experiments used in this paper in https://github.com/blei-lab/phr.

## 2 The posterior hallucination rate and how to estimate it

In this section, we first review the fundamentals of conditional generative models (CGMs). Next, we define in-context learning (ICL) and discuss the Bayesian perspective. We then provide definitions for hallucinations and hallucination rates. Finally, we demonstrate how to estimate the hallucination rate given an ICL problem and a CGM.

**Conditional generative models and in-context learning.** A _conditional generative model_ (CGM) is a sequential model of the form \(p_{}((_{i})_{i}^{m})\) where \(\) represents the parameters of the model, \(\) represents an element in the domain of the distribution, and \(m\) is the number of elements in the sequence. If the support of each conditional distribution is over language tokens and the size of \(\) is large, these models are known as large language models (LLMs). We focus on LLMs, but our methods apply to many conditional generative models.

Implementing CGMs over sequences is often done using neural network architectures called Transformers . The parameters \(\) are set by performing stochastic maximization of the model likelihood over a training dataset \(=\{(_{i}^{j})_{1}^{m}\}_{j=1}^{n}\), where \(i\) is the index over elements of a sequence, and \(j\) is the index over sequences. The resulting generative model approximates the distribution of token sequences in the training dataset \(\).

Figure 1: An example of an in-context dataset and generated response examples for the last label. The correct response, “Sports,” is displayed in green. Wrong answers are in purple.

An _in-context learning_ (ICL) problem is a tuple \((p_{},_{n},)\) containing a model \(p_{}\), a dataset \(_{n}\), and a query \(\). The dataset, or "context," is a sequence of \(n\) examples, \(_{n}=(_{i},y_{i})_{1}^{n}=((_{1},_{1} ),,(_{n},_{n}))\). For a new query, \(\), the model is prompted with the query \(\) and context \((_{i},_{i})_{1}^{n}\) to generate a response according to \(p_{}(,(_{i},_{i})_{1}^ {n})\).

**ICL and Bayesian inference.** One way to understand ICL is through Bayesian statistics [7; 8; 45]. This perspective associates each ICL problem with a _latent mechanism_\(^{}\) defining the data generation process. Given \(^{}\), data \((_{i},_{i})\) are drawn independently from the distribution \(p_{}(_{i},_{i}^{})\).2

Under this assumption, when considering all ICL problems simultaneously, the joint distribution of all query-response pairs can be expressed as a mixture of latent mechanisms \(\):

\[p_{}(_{n})=_{i=1}^{n}p_{}(_{ i},_{i})\,d_{}().\] (1)

If we further assume that a pre-trained language model \(p_{}\) approximates the posterior predictive distribution under this distribution \(p_{}\):

\[p_{}(_{n+1}_{n+1}, _{n})  p_{}(_{n+1}_{n+1}, _{n})\] (2) \[= p_{}(_{n+1}_{n+1},)\,d_{}(_{n}),\] (3)

it follows that doing ICL using an LLM can be seen as a form of implicit Bayesian inference over the latent mechanisms \(\).

Although Equation (1) can be justified by the definition of an ICL problem--as in 
In this paper, we show how adopting this approach allows us to construct and estimate the _posterior hallucination rate_: the probability that a generated response \(\) from \(p_{}\) to a query \(\) will be in an unlikely region according to the "true" latent mechanism \(^{}\).

Before continuing, we provide a brief comment on notation. We use \(,,\) to emphasize when we refer to random variables, and \(,,\) otherwise. For clarity, we use \(p_{}\) when referring to the distribution of the CGM and \(p\) without a subscript when referring to the probability of the "true" data-generating process. The latter distribution can refer to \(p_{}\) or any other probability distribution that follows the factorization of Equation (1).

### Hallucinations and the posterior hallucination rate

Using these ideas, we now define hallucinations and the hallucination rate.

First, imagine a setting where we observe the mechanism \(^{}\). For a query \(\), what values of \(\) would we consider hallucinations? A straightforward idea is to characterize a hallucination as a value of \(\) that is unlikely to be generated from \(p(,^{})\). This choice motivates the following two definitions.

**Definition 1**.: _We define a (1-\(\))-likely set of \(\) and \(\) as any set \(A\) such that \(( A,) 1-\)._

**Definition 2**.: _For fixed (1-\(\))-likely sets \(A(,)\), we call a value \(y\) a hallucination with respect to \(\) and \(\), if \( A(,)\)._

As an intuitive example, assume that the actual generative model of \((_{i},_{i})_{1}^{n}\) is a Bayesian linear model with a known standard deviation \(\). Specifically, \((0,I_{d})\), \(_{i}(0,I_{d})\), and \(_{i}(^{}_{i},^{2})\), where \(,_{i}^{d}\) and \(_{i}\). If \(=0.05\), we could choose the interval between the \(2.5\) and \(97.5\) percentiles of the distribution \((^{},^{2})\) as our (1-\(\))-likely set and call everything outside of this interval a hallucination.

In practice, we do not observe \(^{}\). Rather, we make predictions with \(p(,_{n})\). We ask: At what rate are we hallucinating when we make predictions? The answer is the _true hallucination rate_.

**Definition 3**.: _We define the true hallucination rate (THR) as the probability of sampling a hallucination given true mechanism \(^{}\) and query \(\):_

\[h_{}^{}(^{},)\{  A(^{},)\}\,d( _{n},).\] (4)

_Where \(A(^{},)\) is an \((1-)\)-likely set of \(^{}\) and \(\)._

This value is higher when the posterior predictive \(p(,_{n})\) places high probability in regions unlikely under \(p(,^{})\), and, conversely, will be lower if the posterior predictive puts a lot of mass on areas likely under \(p(,^{})\). We expect the value to be higher when we don't have enough examples and lower as the dataset size increases.

Of course, we do not observe the true mechanism \(^{}\). But the dataset ("context") \(_{n}\) provides evidence for it, as summarized in the posterior \(p(_{n})\). With this distribution, we define the _posterior hallucination rate_, which is the focal point of this work.

**Definition 4**.: _We define the posterior hallucination rate (PHR) as_

\[h_{}()[h_{}^{}( ,)_{n}]=\{  A(,)\}\,d(, _{n})\,d(_{n}).\] (5)

In linear regression, the PHR is the probability that \(\) will land outside of the high probability interval of \(\) and \(\) when sampling \( p(_{n})\), the posterior over coefficients, and \( p(_{n},)\), the posterior predictive over responses.

Here, we remind the reader that directly calculating \(p(_{n})\) cannot be done using the CGM. We will address this problem in the next section.

As a final detail, we discuss how to construct (1-\(\))-likely sets. Ideally, we would like to define these sets so that evaluating \( A(,)\) is easy. Otherwise, it would be hard to know if a response \(\) is a hallucination. One way to do so is to define a statistic \(S\) that maps each value in \(\) to a value in \(\). We then define \(Q_{}(,)\) as the \(\) quantile of this statistic under \(p(,)\). Because it is a quantile,

\[(\{ S() Q_{}( ,)\},) 1-.\]

Thus, \(A(,)=\{ S() Q_{}( ,)\}\) is an (1-\(\))-likely set.

One convenient choice of \(S\) is \( p(,)\). Thus, moving forward, we let

\[A(,)=\{y: p(,) Q _{}(,)\}\] (6)

and we replace all statements \(\{ A(,)\}\) with \(\{ p(,)<Q_{}(,)\}\) in the definitions of a hallucination, the true hallucination rate, and the posterior hallucination rate.

### Calculating the posterior hallucination rate from predictive distributions

We want to calculate the posterior hallucination rate in Definition 4 using conditional generative models like LLMs. However, such models only provide an approximation to the predictive distribution \(p(,_{n})\) rather than the posterior over mechanisms \(p(_{n})\) or the response likelihood \(p(,)\). To overcome this, we propose Algorithm 1, which uses a CGM to estimate the PHR.

To start, we provide an intuitive explanation of Algorithm 1. A formal justification of how it produces an estimate of Definition 4 will follow. As input, Algorithm 1 receives an ICL problem, a budget for MC samples \(M\) and \(K\), and a maximum context length \(N\). First, it samples \(N-n\) new query-response pairs according to the predictive distribution \(p_{}(_{N},_{N},,_{n+1},_{ n+1}_{n})\) (Alg.1, lines 2-6). Intuitively, we can understand this as prompting the model to imagine future pairs it will receive. Next, it samples a new response \(_{new}\) from \(p(,_{n})\) (Alg.2, line 9) and asks: Is this response likely for \(\) given the task implied by \(=(_{1},_{1},,_{N},_{ N})\) (Alg.1, line 10)? If the model is confident about the task it is performing, the pair \((,_{new})\) will be coherent with \((_{i},_{i})_{n+1}^{N}\). If the model is uncertain, then \(_{new}\) will not be coherent with \((_{i},_{i})_{n+1}^{N}\). To determine coherence, we check whether \(_{new}\) is in the tails of \( p_{}(_{i},)\) (Algorithm 2, Lines 5 and 10). Finally, we average over \(K\) samples of \(_{new}\) and \(M\) samples of \((_{i},_{i})_{n+1}^{N}\). The result is an estimate of the PHR.

Building on this intuition, we present the theoretical justification for how this algorithm approximates Definition 4. This justification is based on Doob's theorem , which states that, as \(n\), drawing a value \(\) from \(p()\) and evaluating \(h()\) is equivalent to first sampling \((_{i},_{i})_{1}^{n}\) and then evaluating \([h()(_{i},_{i})_{1}^{n}]\). We state this result below.

**Theorem 1** (Doob's Informal).: _For \(\), \((_{i},_{i})\), if \((,(_{i},_{i})_{1}^{})\) is distributed such that \( p()\) and \(_{i},_{i} p(,)\) then, under general conditions, the posterior mean of \(h()\) given \((_{i},_{i})_{1}^{}\) is almost surely equal to \(h()\), as the number of samples goes to infinity. That is,_

\[_{n}[h()(_{i},_{i})_ {1}^{n}]=h()\;\;a.s.\]

Theorem 1 helps us transform statements about \(h()\) to statements about \([h()(_{i},_{i})_{1}^{n}]\), which only depends on \((_{i},_{i})_{1}^{n}\). Thus, we can proceed without direct access to \(p(_{n})\).

We now use this result to address the problem of estimating the posterior hallucination rate. The following theorem shows how to compute this rate without direct access to \(p(_{n})\). We provide its proof in Appendix C.

**Theorem 2** (PHR via Posterior Predictive).: _Assume that the conditions of Theorem 1 hold for \(\) and \(,\), then,_

\[h_{}()=\{ p(,)<Q_{}(,)\}d (,_{n})d( _{n})\] \[= \{_{N} p( ,(_{i},_{i})_{1}^{N})<Q_{}((_{ i},_{i})_{1}^{},)\}d( ,_{n})d(,_{n+1}^{ }_{n}),\]

_where \(Q_{}((_{i},_{i})_{1}^{},)\) is the \(\)-quantile of \(_{N} p(,(_{i},_{ i})_{1}^{N})\) under the limiting distribution \(_{N}p(,(_{i},_{i})_{1}^{N})\)._

Theorem 2 suggests a natural finite approximation to the PHR where we clip all limits in the expression to a sufficiently large \(N\). Using this approximation, the finite version of the true hallucination rate is

\[h_{,N}^{}((_{i},_{i})_{1}^{N},) \{ p(,(_{i},_{i})_{1}^{N})<Q_{}((_{i},_{i})_{1}^{N },)\}d(,_{n}),\] (7)

where \(Q_{}((_{i},_{i})_{1}^{N},)\) is analogously defined by limiting the number of samples to \(N\). The finite version of the posterior hallucination rate is

\[h_{,N}() h_{,N}^{}((_{ i},_{i})_{1}^{N},)\;d((_{i},_{i})_{1 }^{N}_{n}).\] (8)

Finally, we derive an estimator for Equation (8) by replacing \(p\) with \(p_{}\) and by using Monte Carlo to estimate the integrals and quantiles. Algorithm 1 and Algorithm 2 outline this procedure, where the former estimates Equation (8) and the latter estimates Equation (7).

```
0: Query x, extended context \(\), original context \(_{n}\), CGM \(p_{}\), number of samples \(K\).
1://Estimate quantiles
2:\(S\{\}\)
3:for\(i 1\) to \(\)do
4:\(_{i} p_{}(,_{n})\)
5:\(S S\{ p_{}(_{i}, )\}\)
6:\(\) quantile of \(S\)
7:// Frequency of hallucinations
8:for\(i 1\) to \(\)do
9:\(_{i} p_{}(,_{n})\)
10:\(h_{,i} 1 p_{}(_{i}, )<}\)
11:return\(_{i=1}^{K}h_{,i}\) ```

**Algorithm 2**\(}(,_{n},,p_{},K)\)\(p_{}(y,_{n}) p(y,_{ n})\). Second, we use Monte Carlo methods to estimate the quantiles and integrals in Algorithms 2 and 3. Third, we employ a truncation approximation by only generating \(N-n\) examples. Each of these approximations is a source of estimation error.

Lastly, while we present this methodology in the context of estimating the PHR, it is extendable to other measures of uncertainty. In Appendix D, we adapt these results to provide estimators for the mutual information between \(\) and \(y\) (epistemic uncertainty) and the posterior average entropy of \(y\) (aleatoric uncertainty). These extensions provide a complementary framework for understanding uncertainty in generative models.

## 3 Empirical evaluation

To empirically evaluate the accuracy and applicability of the Posterior Hallucination Rate (PHR) estimator, we first examine whether the PHR estimator accurately predicts the True Hallucination Rate (THR). To do so, we design a synthetic regression experiment for which we can calculate the THR. For ICL regression tasks, the PHR is a reliable predictor of the THR and robust to the choice of \(\) parameter value. Moreover, we observe that the accuracy of the PHR estimator is higher for smaller ICL dataset sizes.

We then evaluate the PHR estimator on natural language ICL tasks using pre-trained large language models (LLMs). In this setting, calculating the THR is not feasible, so we investigate two alternative questions: (1) does the PHR estimator accurately predict the model hallucination rate (defined below in Section 3.2), and (2) can the PHR accurately predict the empirical error rate? The PHR estimator reliably predicts the model hallucination rate, regardless of model performance on all ICL natural language tasks. Moreover, the estimator remains robust to different ICL dataset sizes and settings of the \(\) parameter. Additionally, the PHR estimator accurately predicts the empirical error rate of generated responses when \(\) is set to values greater than 0.5 and the LLM can perform the task better than a random classifier.

### Synthetic regression tasks

We implement a CGM trained on sequences of synthetic regression problems where the THR is available and compare the PHR estimates against the THR on new regression tasks.

**Setup.** We assume the true distribution of query-response pairs \(p(_{n})\) is generated by the following factorization: \(_{i=1}^{n}( p(_{i}_{i},) \,d(_{i}))d(),\) where \(p()\) is the distribution of randomly drawn MLPs with He initialization, \(p(_{i})\) is the uniform distribution over \([-2,2]\), and \(p(,)\) follows a normal distribution \(((),0.1)\).

We implement the model \(p_{}(,_{n})\) with a setup similar to a Conditional Neural Process  and with an architecture close to Llama 2  modified to model sequences of continuous variables. We train it using auto-regressive prediction, the standard for CGMs and LLMs. Training and test data are generated over non-overlapping sets of generated sequences. Example training datasets are shown in Figure 8(a) in Appendix F.1. Example datasets generated by the fit model when initialized with a single random query \(\) are shown in Figure 8(b) in Appendix F.1. The full model and dataset details are given in Appendices E.1 and F.1, respectively.

Figure 2: In the first and third panes, we see the neural process’s generated outcomes for \(n=2\) and \(n=100\). The blue region is the true (1-\(\))–likely set, while the purple is the likely set when conditioned on the blue data points. The second and fourth panes are the corresponding measures of the PHR and THR across the domain.

In our experiments, we set \(=0.05\) so that a response \(y\) is considered a hallucination if it falls outside the \(95\%\) confidence interval of a given sampled distribution conditioned on \(\). To compute the THR, we use the ground truth function \(\) that generated the dataset, replace \(p(,_{n})\) with \(p_{}(y,_{n})\), and estimate the integral by sampling 2000 values from this latter distribution. This approach is reasonable, as we ultimately care about the true probability of hallucinations when using the model \(p_{}\) to make predictions.

**Results.** The first and third panels of Figure 2 show the model's generated outcomes for \(n=2\) and \(n=100\), respectively. In these plots, the blue region represents the true (1-\(\))-likely set for the response distribution of a specific random ReLU neural network. The purple region depicts the model's (1-\(\))-likely set when conditioned on the blue data points and a query value \(\) in the domain [-2, 2]. As more context examples are provided, the confidence intervals narrow, and responses \(y\) are increasingly likely to fall within the blue region.

The second and fourth panels of Figure 2 illustrate the THR and the PHR for \(\) in the domain [-2, 2] for two settings of \(n\). We set \(N-n\) to 100, M to 40, and K to 2000. On the left, dips in PHR and hallucination probability at \(=0\) and \(=1\) correspond with the ground truth in-context examples. In the right panel, with a larger \(n\), both the PHR and hallucination probability remain low across all \(\) values. Notably, the PHR and hallucination probability closely align throughout the domain. In Appendix G.1, we demonstrate that these findings hold across various values of the \(\) parameter.

In Figure 2(a), we present the PHR and THR plotted against various context lengths, where each is averaged over 200 random test functions. For added interpretability, we report several metrics for evaluating the PHR as a linear predictor of the THR, including the mean squared error (MSE), the regression coefficient \(\), the \(p\)-value testing the null hypothesis that \(=0\), and the coefficient of determination (\(R^{2}\)).

Although the PHR aligns well with the THR, it tends to underestimate it, particularly as the number of examples increases. This is also clear given the lower \(R^{2}\) values as the context length increases. To investigate this further, Figure 2(b) visualizes individual PHR and THR predictions at different context lengths by plotting PHR values on the x-axis and the corresponding THR values on the y-axis. For small numbers of contextual examples, the PHR closely matches the THR, which is encouraging since accurately capturing the true probability of hallucination is crucial when few examples are available and errors are more likely.

**Discussion.** The results confirm that our method closely recovers the target estimand. However, the PHR estimator underestimates the THR. We do not believe this is due to the Monte Carlo or truncation approximations, as initial experiments with varying \(M\), \(K\), and \(N\) values exhibited the same pattern. Rather, we consider two main factors that likely contribute to this underestimation.

First, differences between the learned distribution \(p_{}(,_{n})\) and the true distribution \(p(,_{n})\) likely contribute to the bias. This is seen in the slight discrepancies in the confidence intervals in the third panel of Figure 2. It may also be due to other mismatches in our assumptions, such as the lack of perfect exchangeability in the transformer architecture; even after training on permuted sequences, the architecture may not fully achieve exchangeability.

Figure 3: **Synthetic data: (a) Plot of the average THR and PHR as a function of \(n\). The THR and PHR follow each other and decrease as the number of contextual examples \(n\) increases. (b) Plot of the estimated PHR vs the THR. Each point represents the predicted PHR and actual THR for a given instance. Perfect performance would have all points on the \(x=y\). Results show the PHR closely approximates THR, but performance degrades with context length.**

Second, even if the PHR estimate were entirely accurate, it would still differ from THR, as they represent distinct quantities. Therefore, some degree of bias is to be expected. We investigate this question further in Appendix G.1.2 where we compare our estimate with an analytic version of the PHR and find this problem is greatly reduced.

We leave it to future work to rigorously quantify these differences and develop methods that can more accurately align the learned and true distributions, particularly in scenarios with large context sizes.

### Natural language tasks

Here we evaluate the posterior hallucination rate estimator on common natural language in-context learning tasks using the Llama-2 family of LLMs . We also provide results for Gemma-2 9B in Appendix G.2.1. We evaluate the PHR by comparing it against the empirical error rate and the model hallucination rate (MHR), which we define below. These serve as proxies for the THR, which is no longer computable as we can't access \(^{}\) for these examples.

**Setup.** We consider tasks defined by six datasets: _Stanford Sentiment Treebank (SST2) _, _Subjectivity _, _AG News _, _Medical QP _, _RTE _, and _WNLI _. We filter out queries of length longer than 116 tokens. Descriptions of all data sets and pre-processing are given in Appendix F.2.

To implement ICL for a given dataset, we sample a response-balanced training set of query/response pairs \(_{n}=(_{i},_{i})_{1}^{n}\). We generate a response \(y\) from the predictive distribution given by an LLM \(p_{}(y,_{n})\). We structure the prompt by adding strings to distinguish between inputs and labels. An example prompt from the Subjectivity dataset is shown in Appendix E.2.

**Evaluation metrics.** It is a challenge to assess the accuracy of the the posterior hallucination rate on LLM tasks as we only have a one ground truth response for each query instead of the ground truth \(^{}\).

One evaluation metric to consider is the empirical error rate,

\[(,y;p_{},_{n}) _{i=1}^{K}\{_{i}\},_{i}  p_{}(,_{n}).\] (9)

However, this metric only provides a partial picture as it does not account for the inherent variability over responses given a mechanism, which would not constitute a hallucination.

To get a more complete picture, we invoke Doob's theorem and note that, when \((_{i},_{i}) p(,^{ })\) and for sufficiently large \(N\), we have

\[p(,^{})=p(,( _{i},_{i})_{1}^{}) p(,(_{i},_{i})_{1}^{N}) p_{}( ,(_{i},_{i})_{1}^{N}).\] (10)

Figure 4: Performance Metrics for Llama-2-7b Across Tasks.

This motivates us to estimate the reference THR by approximating the distribution \(p(y,^{})\) with \(p_{}(y,_{n}_{})\), where \(_{}\) is a set of additional \((_{i},_{i})\) examples from the same task that are not in \(_{n}\). We call this estimate the _model hallucination rate_ (MHR) as it is derived under the model \(p_{}\). Formally,

\[(;p_{},_{n},_{})_{i=1}^{K} p_{} _{i},_{n}_{}<_{}^{}},\]

where \(y_{i} p_{}(,_{n})\), \(K\) is the number of response samples, and \(_{}^{}\) is the \(\) quantile of the samples \( p_{}_{j},_ {n}_{}}\) with \(_{j} p_{}(,_{n} _{})\). This value is only proposed for model evaluation. Importantly, it cannot be used to replace the PHR, as it depends on \(_{}\), which is not available at test time. Intuitively, the MHR computes the probability that an answer \(y\), generated by a model conditioned on \(_{n}\), is considered a hallucination by a model conditioned on both \(_{n}\) and \(_{}\).

For each task and context length \(n\), we sample 50 random training datasets \(_{n}\), 50 evaluation datasets \(_{}\), and 10 random test samples. We report the same metrics used in the synthetic experiments, but here we evaluate the PHR as a linear predictor of both the error rate and MHR. These metrics are evaluated over all \(50 10\) test samples for each context length.

**Results.** We report results for Llama-2-7b. We set \(N-n=5\), \(M=10\), and \(K=50\). The top plots in Figure 4 show the MHR and estimated posterior hallucination rate against the number of in-context examples with \(=0.05\). It shows that the posterior hallucination rate is a good estimator of the MHR. We show that this trend holds for alternative settings of \(\) in Figure 23 of Appendix G.2.2.

The bottom plots in Figure 4 show the empirical error rate and estimated posterior hallucination rate against the number of in-context examples with \(=0.75\). The plots show that for the SST, Subjective and AG News tasks, the PHR follows the error rate closely, while it diverges for the Medical QP, RTE and WNLI tasks. We use a high value of epsilon so that only responses with very high probability are not taken to be hallucinations, aligning with our strict focus on correctness rather than modeling the full distribution. The impact of varying \(\) is explored in Figure 24 of Appendix G.

**Discussion.** The results highlight the usefulness of the PHR by demonstrating its ability to assess the model's uncertainty in language tasks and predict its error rate without requiring an evaluation dataset. Additionally, the discrepancy between the PHR and the error rate in Medical QP, RTE, and WNLI tasks can be attributed to the model's limited generalization ability in these tasks. This leads to an inadequate approximation of the posterior predictive; in other words, \(p_{}(,_{n}) p( ,_{n})\). This inadequacy is supported by Figure 4, which shows that in tasks where the PHR does not align with the error rate (green line), the model's performance is close to random guessing (dashed gray line). Hence the poor performance of the PHR in this scenarios is expected because our method relies on accurately approximating the posterior predictive and only accounts for hallucinations when the data distribution is properly modeled.

## 4 Conclusion

In this work, we have presented a new method for predicting the hallucination rate of in-context learning with conditional generative models. We provide a theoretical justification for our method. In synthetic experiments, we demonstrate that the PHR estimator yields accurate estimates of the actual probability of hallucination. With pre-trained LLMs that achieve non-trivial performance, we show that our method is valuable for predicting the error rate of natural language ICL tasks.

High-fidelity estimation of the PHR relies on two strong assumptions. The first is that the data of the in-context learning problem admits a de Finetti representation; the second is that the CGM \(p_{}\) is a faithful estimate of the in-context learning distribution \(p_{}\). While our results support the adoption of our method and these assumptions, divergences between \(p_{}\) and \(p_{}\) may introduce inaccuracies, as we saw for natural language tasks where Llama-2-7B only performs as well as a random classifier. Falck et al.  also report instances where properties of the predictive distribution of a pre-trained LLM differ from those of the reference Bayesian posterior predictive for synthetic ICL tasks. Two directions for further research are to understand how these divergences affect the accuracy of the PHR and to identify the settings under which our assumptions break down.

Nevertheless, we are optimistic about future work that adopts Bayesian perspectives on LLMs and other CGMs, as we believe these approaches hold significant practical utility for understanding uncertainty and hallucinations in these models.

## 5 Acknowledgements

This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R&E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence). The authors would like to thank Amir Feder, Alessandro Grande, Achille Nazaret, Yookoon Park, Kathy Perez, Sebastian Salazar, Claudia Shi, Brian Trippe, Al Tucker, and Luhuan Wu for their reviews, feedback, and support.