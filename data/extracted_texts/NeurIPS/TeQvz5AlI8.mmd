# DAT: Improving Adversarial Robustness via

Generative Amplitude Mix-up in Frequency Domain

 Fengpeng Li\({}^{1}\)   Kemou Li\({}^{1}\)   Haiwei Wu\({}^{2}\)   Jinyu Tian\({}^{3}\)   Jiantao Zhou\({}^{1}\)

\({}^{1}\)State Key Laboratory of Internet of Things for Smart City, University of Macau

\({}^{2}\)Department of Computer Science, City University of Hong Kong

\({}^{3}\)Faculty of Innovation Engineering, Macau University of Science and Technology

Corresponding author: Jiantao Zhou (jtzhou@um.edu.mo).

###### Abstract

To protect deep neural networks (DNNs) from adversarial attacks, adversarial training (AT) is developed by incorporating adversarial examples (AEs) into model training. Recent studies show that adversarial attacks disproportionately impact the patterns within the phase of the sample's frequency spectrum--typically containing crucial semantic information--more than those in the amplitude, resulting in the model's erroneous categorization of AEs. We find that, by mixing the amplitude of training samples' frequency spectrum with those of distractor images for AT, the model can be guided to focus on phase patterns unaffected by adversarial perturbations. As a result, the model's robustness can be improved. Unfortunately, it is still challenging to select appropriate distractor images, which should mix the amplitude without affecting the phase patterns. To this end, in this paper, we propose an optimized _Adversarial Amplitude Generator (AAG)_ to achieve a better tradeoff between improving the model's robustness and retaining phase patterns. Based on this generator, together with an efficient AE production procedure, we design a new _Dual Adversarial Training (DAT)_ strategy. Experiments on various datasets show that our proposed DAT leads to significantly improved robustness against diverse adversarial attacks. The source code is available at https://github.com/Feng-peng-Li/DAT.

## 1 Introduction

DNNs have been successfully applied to various tasks [21; 32; 25]. However, recent studies reveal that DNNs are vulnerable to adversarial examples (AEs), created by applying subtle yet deceptive adversarial perturbations to benign samples [40; 51]. Such vulnerabilities have sparked considerable interests, leading to the development of numerous adversarial attacks designed to deceive DNNs [40; 19; 39; 13; 20; 5; 6]. Furthermore, serious concerns about the trustworthiness of artificial intelligence have been raised, due to these fundamental vulnerabilities [47; 37]. To mitigate these threats, adversarial training (AT) has been developed to enhance model robustness by incorporating AEs into training through a min-max strategy [40; 61; 38]. Based on the typical method, PGD-AT , a variety of AT strategies have been devised [38; 29; 37] (see Appendix B for related works).

For image signals transformed into the frequency domain using, _e.g_. the discrete Fourier transform (DFT), several AT works [59; 52; 53; 43] explore the adversarial attacks' behavior on sample's frequency spectrum. Frequency spectra consist of amplitude and phase; the amplitude typically captures stylistic information, whereas the phase encompasses richer semantics . Recent studies [58; 63] find, as shown in Figure 1, that adversarial attacks often severely eliminate some semantics in the phase, making it difficult for models to extract features for correctly predicting AEs, while theimpacts on amplitude are relatively mild. Moreover, by forcing the model to focus on phase patterns,  confirm the model can learn more features unaffected by image corruptions, _e.g._, Gaussian noise and defocus blur, improving the model's performance on corrupted samples.

To explore the impact of adversarial perturbations on phase and amplitude patterns respectively, we conduct some studies (see Sec. 2 for details). We find that the standard model trained without AT has worse performance on samples with adversarial phase patterns than those with adversarial amplitude ones. Unlike the standard model, AT enhances the model's robustness against both phase- and amplitude-level adversarial perturbations, with a more noticeable improvement against adversarial perturbations on phase patterns. This indicates the potential for improving the model's robustness by focusing on phase patterns unaffected by adversarial perturbations in AT. Then, by mixing the training samples' amplitude with randomly selected distractor, we observe that the robust model performance, particularly at adversarial phase patterns, is further enhanced, without impacting that at adversarial amplitude ones. This demonstrates that training samples with mixed amplitude improve the model's performance on AEs, and maintain the model's robustness on amplitude-level.

Inspired by these observations, we in this work propose a new _Dual Adversarial Training (DAT)_ strategy by focusing on the phase patterns, with two adversarial procedures: adversarial amplitude generation and efficient AE production. Specifically, to guide the model to learn more phase patterns, we first attempt to mix the amplitude of training samples' frequency spectra with randomly selected distractor images. However, when the disparity between the distractor and the original image is too large, the recombined ones tend to disrupt original phase patterns, hindering the model from predicting AE correctly. Conversely, it is difficult for models to focus on phase patterns when the distractor closely resembles the original one . To tackle this challenge, we propose an optimized _Adversarial Amplitude Generator (AAG)_ to synthesize an adversarial amplitude, maximizing the model loss and limiting the model fitting amplitude patterns, thereby the model focusing on phase patterns for convergence. During the training process, the AAG and robust model are optimized jointly with the original and recombined images, together with their AEs. The robust model undergoes training by empirical risk minimization, and maximizes the model loss to update the AAG adversarially. Experiments across various benchmarks against a range of adversarial attacks confirm the superior effectiveness of our proposed DAT, surpassing state-of-the-art methods in robustness with big margins.

**Contribution.** The contributions of this work can be summarized as:

* We verify that adversarial perturbations significantly influence phase patterns, resulting in the model's difficulty for predicting AEs correctly. Moreover, by mixing the amplitude of a training image with that of a distractor, we find that the model robustness against AEs can be enhanced.
* We propose the novel DAT strategy with an optimized AAG to synthesize an adversarial amplitude. With the AAG, we enforce the model to better focus on phase patterns, enhancing the model's robustness. Also, an efficient AE generation module is incorporated to improve the AT's efficiency.
* Experiments on multiple datasets confirm that DAT significantly enhances the model's robustness against a variety of adversarial attacks. Specifically, DAT increases the model's robustness by \(\)2.1% on CIFAR-10, \(\)2.2% on CIFAR-100, and \(\)2.3% on Tiny ImageNet, on average.

**Notation.** Let \(=\{(_{i},y_{i})\}_{i=1}^{N}\) be a benign dataset comprising \(N\) samples from \(c\) classes, where each sample \(_{i}^{C H W}\) is an image with \(C\) channels, height \(H\), and width \(W\), and its label \(\ y_{i}[c]=\{1,,c\}\). \(\ f_{}:^{c}\) denotes a DNN function parameterized by \(^{d}\)

Figure 1: The adversarial perturbation severely damages phase patterns (especially in red rectangular) and the frequency spectrum, while amplitude patterns are rarely impacted. The AE is generated by PGD-20 \(_{}\)-bounded with radius \(}{{255}}\).

and \(F_{}()=_{y[c]}f_{}()_{y}\) is the predicted label of \(\). Moreover, \(f_{}=g h\), where \(h\) is the feature extractor and \(g\) is the classifier. Let \(^{m}\) be the \(m\)-dimensional feature space, \(h_{i}:\) be a feature mapping function, and \(()=[h_{1}(),,h_{m}()]^{} \) denote the feature map of \(\). Specifically, features induced from the amplitude and phase patterns of \(\) are \(h_{a}()\) and \(h_{p}()\), respectively. Define \(_{}[]=\{^{}\|^{ }-\|_{}\}\) as an \(_{}\)-ball centered on \(\) with radius \(\). Let \(()\) and \(^{-1}(,)\) denote the DFT and inverse DFT (IDFT) functions. Typically, DFT is independently applied to each channel of an image \(\) within the pixel space as:

\[()(u,v)=_{h=1}^{H}_{w=1}^{W}(h,w)\, ^{-2(u+v)},\]

where \((h,w)\) denotes the pixel coordinates of \(\), and \((u,v)[H][W]\) signifies coordinates in the frequency domain. The real and imaginary parts of \(()\) are denoted by \((())\) and \((())\), respectively. Then, the amplitude spectrum \(()\) and phase spectrum \(()\) are

\[()=(^{2}(())+ ^{2}(()))^{},()=((())}{ (())}).\] (1)

## 2 Motivation: On Improving Adversarial Robustness in Frequency Domain

To investigate the approach to enhance the model robustness and show the motivation of DAT, we perform exploration experiments in this section. As shown in Figure 1, adversarial perturbations severely compromise the semantics within phase patterns, resulting in the difficulty of the model predicting AEs correctly. Consequently, we examine the distinct effects of adversarial perturbations on amplitude and phase patterns. To achieve this target, we employ the standard and robust models, trained without and with AT as  on \(_{t}\) (the training subset of CIFAR-10). Moreover, on training samples with perturbed amplitude, the model tends to focus on phase patterns, in order to achieve the convergence [7; 58]. Following this line, we discuss the impact of perturbing amplitude by mixing the amplitude of training samples with those of distractors randomly selected from \(_{t}\). For \((,y)_{t}\), the recombined sample \(}\) is generated by amplitude-level mixing operations and IDFT, namely,

\[}=^{-1}((_{0})+(1- )(),()), (0,1),\] (2)

where \(_{0}\) is the distractor i.i.d. drawn from \(_{t}\). We use \((},y)\) to construct a dataset \(_{r}\) and train the perturbed model on it as . Then, for \((,y)_{}\) (the testing subset of CIFAR-10), we generate AE \(^{}\) by four representative adversarial attacks (PGD, C&W, FAB, and Square) and utilize DFT to derive the amplitude and phase of frequency spectra of both \(\) and \(^{}\). Furthermore, images composed of adversarial amplitude/phase and benign phase/amplitude (denoted by \(^{}_{amp}\)/\(^{}_{pha}\)) are obtained by

\[^{}_{amp}=^{-1}((^{}), ()),^{}_{pha}=^{-1}( (),(^{})).\]

For each \((,y)_{e}\), with every adopted adversarial attack, we use \((^{},y)\), \((^{}_{amp},y)\) and \((^{}_{pha},y)\) to combine evaluation datasets \(_{}\), \(_{amp}\) and \(_{pha}\), which are used to evaluate the robustness of

Figure 2: Test accuracy (%) on CIFAR-10 of (a) standard, (b) robust, and (c) amplitude-perturbed ResNet-18. ”Adv Amp./Pha.” refers to \(^{}_{amp}\)/\(^{}_{pha}\). AEs are generated by PGD-20, C&W\({}_{}\), FAB, and Square, with \(_{}\)-bounded perturbation budget \(=}{{255}}\) and inner step \(=}{{255}}\). The robust and perturbed models are trained by PGD-AT-10 following .

the standard, robust, and perturbed models, respectively. The experimental outcomes are illustrated in Figure 2, from which we can draw the following conclusion:

**Impact of Adversarial Attacks.** As shown in Figure 2a, under all four attacks, the standard model completely cannot predict samples in \(_{}\). Moreover, the standard model exhibits higher test accuracy on \(_{amp}\) than that on \(_{pha}\). These results suggest that adversarial attacks have a more substantial impact on the phase patterns than amplitude ones.

**Impact of AT.** As depicted in Figure 2b, AT simultaneously enhances the model's performance on \(_{}\) and \(_{amp}\) and \(_{pha}\), in comparison to the standard model. Notably, the robust model exhibits superior performance on \(_{pha}\) over \(_{amp}\), contrary to the standard model. That indicates the AT helps the model learn more phase patterns unaffected by adversarial attacks, enhancing the model's robustness on both adversarial phase patterns and AEs. The phenomena indicate that by forcing the model to focus on the phase patterns of samples, the model's robustness can be improved further.

**Impact of Mixing Amplitude.** As shown in Figure 2c, for the perturbed model trained with AT on \(_{r}\), compared with the robust one, its performance on \(_{}\) and \(_{pha}\) is improved further, while the performance on \(_{amp}\) is rarely changed. From these results, we can conclude that mixing the amplitude with that of the randomly selected distractor can force the model to focus on phase patterns, learning more patterns within the phase unaffected by adversarial perturbations. Then, the model's robustness on AEs is improved further, while robustness on amplitude patterns is retained.

Following the insight from these studies, we propose DAT to enhance the model's robustness, mixing training sample's amplitude with an adversarial one, generated by the adversarially optimized AAG.

## 3 Methodology

In this section, the details of our proposed DAT are introduced and analyzed. As illustrated in Figure 3, DAT consists of three stages. It first adopts the AAG \(G_{}\) to generate adversarial amplitude and obtain the recombined data for benign ones in Stage I. With the proposed loss \(_{}\) in Stage II, we produce the AEs for both benign and recombined samples. Then, taking both benign and recombined samples, total loss \(_{}\) for DAT are minimized to update robust model \(f_{}\), and maximized to optimize \(G_{}\) adversarially in Stage III. Stages I and III involve the adversarially trained AAG \(G_{}\) and commonly

Figure 3: The overview of DAT, which consists of three stages: (I) adversarial amplitude generation, (II) AE generation, and (III) joint optimization.

updated model \(f_{}\), both of which share the \(_{}\) and optimized jointly following the objective as:

\[_{}_{(,y)}[_{}_{} p(}|,) }[_{}(f_{}(),f_{}( }),y)]],\] (3)

where \(}\) is the recombined data of \(\), following a sample-dependent conditional distribution \(p(}|,)\). Stage II encompasses an efficient AE generation method, optimized using the proposed loss \(_{}\) as:

\[_{^{}_{}[]}_{( ,y)}[_{}(f_{}( ),f_{}(^{}),y)].\] (4)

Following the order of these three stages, we introduce (I) AAG in Sec. 3.1 and (II) the efficient AE generation in Sec. 3.2. Subsequently, building on these components, Sec. 3.3 details (III) the joint optimization. Ultimately, Sec. 3.4 theoretically analyzes the mechanism of how the adversarial amplitude spectrum generated by AAG enforces the model to focus on the phase-level patterns.

### Adversarial Amplitude Generator

To explain the proposed AAG, we first introduce some constraints that the recombined \(}\) with perturbed amplitude is expected to meet. With a small \(_{1}>0\) and an \(_{2}_{1}\), ideally, we expect \(}\) of \((,y)\) based on the generated adversarial amplitude to satisfy the following three conditions:

* **C1.**\(|h_{p}()-h_{p}(})|<_{1}\): Ensuring \(}\) retains the same semantics in the phase spectrum as \(\).
* **C2.**\(F_{}()=F_{}(})\): Ensuring \(}\) remains distinguishable with the same label as \(\) by \(f_{}\).
* **C3.**\(|h_{a}()-h_{a}(})|>_{2}\): Making \(}\) maximize the \(_{}\), causing the model's difficulty fitting the amplitude of images, and forcing the model to focus on phase patterns.

Let us first analyze the above conditions **C1-C3** when a randomly selected distractor image is used. Since \(}\) is recombined by the mixed amplitude with the distractor and original phase of \(\), then, **C1** can be easily satisfied. As stated in Sec. 1, when the gap between the randomly selected distractor and training sample is too large, the \(()\)'s information can be damaged, resulting in the inconsistent prediction between the \(\) and \(}\), destroying the **C1** and **C2**. Conversely, it is difficult to satisfy **C3**, limiting the model's attention to the patterns in \(()\). The above statement indicates that it is difficult for \(}\), using the amplitude of the randomly selected distractor, to meet the above three constraints.

Instead of searching for an appropriate distractor, we here resort to a generative approach, i.e., developing \(G_{}\) to generate an adversarial amplitude \(_{G}()\) as

\[_{G}()=G_{}(,f_{}()),}{}(,).\]

For efficient training, \(G_{}\) is constructed by four linear layers (detailed architecture in Appendix E.2). Moreover, the input with \(f_{}()\) can ease the difficulty of \(G_{}\)'s convergence. With \(G_{}\), since \(}\) is still recombined by \(_{G}()\) and the \(()\), then, **C1** is satisfied. For the outer minimization in Eq. (3), we retain the label of \(}\) same as \(\), minimizing \(_{}\) to update \(f_{}\), meeting the **C2**. To meets **C3**, \(G_{}\) is optimized adversarially by maximizing the \(_{}\) (further details in Sec. 3.3), shown as the inner step in Eq. (3). Thereby, \(}\) can limit \(f_{}\) to fit amplitude information. To achieve the convergence, \(f_{}\) has to focus on patterns in \(()\), learning more phase patterns unaffected by adversarial perturbations.

Since \(G_{}\) is adversarially trained by maximizing the \(_{}\), \(_{G}()\) is likely to compromise semantic integrity , hindering \(f_{}\) to retain the prediction consistency between \(}\) and \(\). Moreover, due to the loss of original amplitude information, using \(_{G}()\) to replace \(()\) entirely can also hurt the model's robustness . As shown in Sec. 2, the mix-up operation on amplitude rarely has impact on the amplitude level robustness. That indicates the linear mix-up operation can preserve the energy of the amplitude spectrum and maintain the original amplitude information with less impact on the sample's original information. Otherwise, the original amplitude could be compromised, thereby hindering accurate model predictions. Following this line, a mix-up operation is employed, ensuring that a portion of the original amplitude information is preserved following:

\[_{mix}()=_{G}()+(1- )(),(0,1).\]

The mix-up operation effectively satisfies **C1** and **C2**, ensuring \(}\) remains distinguishable by \(f_{}\), and keeping \((})\)'s patterns closer to those in \(()\) in manifold. Finally, \(}\) is obtained by IDFT as

\[}=^{-1}(_{mix}(),( )).\]

To elaborate on \(G_{}\), we perform some experiments and provide visual results in Appendixes C and F.7. Now we can use \(}\) as an augmentation of \(\), introduced to Stage II for generating AEs.

### Efficient Adversarial Example Generation

Since both \(\) and \(}\) are fed into Stage II for generating AE, it doubles the time consumption if we use the same AE generation strategies as existing methods, _e.g._, PGD-AT  and TRADES . To improve the training efficiency of DAT, we now suggest an efficient AE generation strategy. Since simply reducing the iteration step results in the difficulty of AEs' reaching the actual maximum in the \(_{}\)-ball , we propose the loss \(_{}\) to increase adversarial perturbation length in each iteration as

\[_{}(f_{}(),f_{}(^{}),y)=_{}(f_{}(^{}),y)+ _{}(f_{}(^{}),f_{ {}}()),\] (5)

where \(\) is a weighting parameter, and \(_{}\) and \(_{}\) are cross-entropy (CE) loss and Kullback-Leibler (KL) divergence. According to [49; 50], it is effective to enlarge adversarial perturbation length for each step by maximizing the distance between \(f_{}()\) and \(f_{}(^{})\). Following this line, based on PGD-AT maximizing the \(_{}\), the proposed \(_{}\) increases the adversarial perturbation step length by maximizing the KL divergence between benign sample and its AE. Then, with \(_{}\), AEs can use fewer iterative steps to achieve the actual maximum in the \(_{}\)-ball. As shown by experiments in Appendix F.5, DAT only needs _5 steps_ to generate AEs for both benign \(\) and recombined \(^{}\), significantly reducing the training time while maintaining the model's robustness. Enlarging the inner step size \(\) (in Eq. (10) of Appendix B.3) can also increase the adversarial perturbation length in each iteration. Due to the fact that the current AT methods typically employ a fixed \(\), we adopt an extra loss term for fair experimental comparisons. More details on the AE generation procedure, including pseudocodes and experimental analyses, are in Appendix A.1, F.3 and F.5.

With the AAG and efficient AE generation, Stage III attempts to improve the model's robustness.

### Joint Optimization

After the introduction to Stages I and II, we now delve into specifics of Stage III, joint optimization, where \(f_{}\) and \(G_{}\) are optimized jointly following the objective as Eq. (3). In Stage III, to satisfy **C2** in Sec. 3.1, keeping \(}\) with the same label as \(\) by \(f_{}\), recombined and benign samples and their AEs are fed into DAT for the model training, also reducing the negative impact of amplitude information loss. Moreover, \(_{}\) needs to be minimized on benign and recombined samples' AEs to enhance the robustness of \(f_{}\), enforcing \(f_{}\) to focus on the phase patterns in the meanwhile. For commonly updating \(\) and adversarially renewing \(\), we introduce the designed loss terms \(_{}\) as:

\[_{}(f_{}(),f_{}(}),y)=(_{}(f_{}(),y)+_{}(f_{}(}),y))+ _{}(f_{}(),f_{}(})),\] (6)

where \(_{}\) and \(_{}\) are adversarial training and consistency regularization losses respectively, and \(\) is the weighting parameter for \(_{}\). We discuss \(_{}\) and \(_{}\) below separately.

**Adversarial Training Loss \(_{}\).**\(_{}\) is the loss used to guide \(f_{}\) to learn robust features on AEs against adversarial attacks. As shown in Figure 1, although adversarial perturbations significantly damage phase patterns, there are still some unaffected features in the phase of AEs, important for \(f_{}\) to categorize AEs correctly. Since these unaffected phase patterns contain adversarial perturbations, it is difficult for \(f_{}\) to learn these features only with AEs. Therefore, we utilize benign samples and their AEs in AT, guiding the model to learn their shared features, significant for improving \(f_{}\)'s robustness. Following this line, \(_{}\) for \((,y)\) with AE \(^{}\) can be expressed as:

\[_{}(f_{}(),y)=_{} (f_{}(),y)+_{}(f_{}(^{}),f_{}()),\]

where \(\) is the weighting parameter identical to that in Eq. (5). \(}\) adopts the same AT loss as that of \(\).

**Consistency Regularization Loss \(_{}\).**\(_{}\) is the loss used to preserve the prediction consistency between \(\) and \(}\). In DAT, the amplitude of \(}\)'s frequency spectrum is mixed with the adversarial one generated by \(G_{}\), maximizing \(_{}\), showing the large gap between \(h_{a}()\) and \(h_{a}(})\). Since \(}\) has the same phase patterns as \(\), keeping the prediction consistency between \(\) and \(}\) can make the model learn more phase patterns further, enhancing the model's robustness. Inspired by the mechanism, we use the Jensen-Shannon (JS) divergence \(_{}\) to ensure the prediction consistency as:

\[_{}(f_{}(),f_{}(}))=_{}}()+f_{}(})}{2},f_{}( )+_{}}()+f_{}(})}{2},f_{}(}) .\] (7)

This concludes the introduction to \(_{}\). Notably, during the model training of DAT, the gap of batch normalization (BN) parameters between \(\) and \(}\) is quite large (details in Appendix D), posing challenges to model's convergence. To remedy this, we adopt different BNs for \(\) and \(}\) during training. Please refer to the pseudocode of DAT in Appendix A.2 for a comprehensive presentation.

### Theoretical Analysis

Through a convergence analysis of the empirical risk, this section theoretically discusses the effects of DAT on the model to focus on phase patterns. Detailed proofs are provided in Appendix G. Concretely, we instantiate \(g\) as a linear softmax classifier \(=[_{1},...,_{c}]^{m c}\) on top of the learned features \(\). Generally, for \((,y)\), suppose \(()\) represents the augmented distribution over data points, where \(\) can be transformed as anyone in \(\{,^{},},}^{}\}\). Then, the augmented data for \(\) can be denoted as \(t()()\). Since the augmentation will increase the discrepancy between original and augmented distributions w.h.p., we can establish a common assumption.

**Assumption 3.1**.: _Assume \(_{}[\|(t())-()\| ]>_{0}\), where \(_{0}>0\) is a relatively large value. Since only the amplitude spectrum is perturbed in the proposed DAT, it is reasonable that_

\[_{}[|h_{a}(t())-h_{a}()|]> _{}[|h_{p}(t())-h_{p}()\|.\]

**Theorem 3.2** (Weight Regularization of Amplitude Features).: _Grant Assumption 3.1, when the empirical risk \(\) is minimized with some convex loss function \(\) (e.g. CE loss):_

\[()|}_{(,y) }_{t()()}[ (^{}(t()),y)],\]

_we have \(w_{j,a} 0\) for all \(j[c]\), where \(w_{j,a}\) is the corresponding weights of amplitude features \(h_{a}\)._

**Corollary 3.3**.: _Suppose the predicted probability \(f_{}()=[p_{1},,p_{c}]^{}\), where_

\[p_{i}=_{i}^{})}{_{j=1}^{c}( _{j}^{})}=^{c}((_{j} -_{i})^{})}.\]

_For every \(i,j[c]\), we have \((w_{i,a}-w_{j,a})h_{a} 0\)._

**Remark.** Theorem 3.2 suggests that for weights \(w_{j,a}\) corresponding to features \(h_{a}\) derived from amplitude pattern, minimizing the empirical risk \(\) regularizes it to 0. As a result, it is difficult for the model to fit the adversarial amplitude generated by AAG. In order to converge, the model needs to reduce the reliance on \(h_{a}\) by restricting \(w_{j,a}\). Hence, the model would mitigate the impact of \(h_{a}\) on the predicted labels, as shown in Corollary 3.3, and pay more attention to features \(h_{p}\) derived from phase patterns, capturing more phase patterns unaffected by adversarial attacks. Therefore, we can verify the effectiveness of DAT in enhancing the model's robustness.

## 4 Experiments

In this section, we perform experiments to verify the effectiveness of DAT and explore the function of some DAT's parts. Sec. 4.1 shows experimental setups. Sec. 4.2 compares the model's robustness with existing AT methods with fixed \(\) and \(\). Sec. 4.3 compares DAT with existing AT methods over complex strategy, _e.g._, AWP and SWA. Sec. 4.4 presents the analysis of ablation studies.

### Experimental Setup

**Experimental and Evaluation Settings.** We select three datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet . For all experiments in this work, ResNet-18, WideResNet-28-10 (WRN-28-10), and WideResNet-34-10 (WRN-34-10) are used as model architectures (experiments are attached to Appendix F.1), with \(=15\) and \(=2\) (exploration in Appendix F.4). During training, the inner step size is fixed as \(=}{{255}}\) to generate adversarial perturbation \(_{}\)-bounded with constant radius \(=}{{255}}\) following [29; 37; 33] (details in Appendixes E.1 and E.2).

**Baselines.** We compare results with PGD-AT , TRADES , MART , and recent AT methods with competitive performance: LAS-AT , SCARL , and ST . We also compare the DAT with OA-AT , DAJAT , and IDBH , which uses extra strategies, _e.g._, AWP  and SWA . Introduction and details of baselines are attached in Appendixes B.2 and E.3.

### Comparison with Common Methods

In this subsection, we compare the robustness of DAT with common methods that use fixed \(\) and \(\) during the training procedure. Table 1 displays the results on CIFAR-10, CIFAR-100, andTiny ImageNet using ResNet-18. Compared with existing methods, DAT not only improves the model's robustness but also enhances the natural accuracy. On CIFAR-10, DAT achieves an average improvement of \(\)2.9% against FGSM, PGD-20, and PGD-100. For challenging C&W\({}_{}\) and AA, DAT obtains \(\)0.66% and \(\)0.85% improvement, respectively. Specifically, since SCARL adopts a contrastive learning strategy, DAT achieves less improvement against C&W\({}_{}\) compared to it. CIFAR-100 contains more classes but fewer samples for each class, thereby making it challenging for AT. DAT enhances the model's robustness by \(\)2.7% on average against FGSM, PGD-20, and PGD-100 compared to the existing methods. For the demanding C&W\({}_{}\) and AA, the model's robustness is improved by \(\)1.5% and \(\)1.3%. On the intricate real-world dataset Tiny ImageNet, DAT achieves \(\)2.9% better performance against FGSM, PGD-20, and PGD-100. For the challenging C&W\({}_{}\) and AA, we enhance the model's robustness by \(\)1.6% and \(\)1.2%. AWP is proved to be an effective method for protecting the model from robust overfitting. To further improve the model's robustness, we combine DAT with AWP, and compare the performance with the combination of various existing methods and AWP fairly. Since a few methods provide the codes for the combination with AWP, we choose only these methods for comparison. As shown in Table 1, the combination of DAT and AWP improves the model's robustness further and still outperforms previous methods.

Large-capacity models usually have better adversarial robustness. Thus, we also perform comparative experiments using WRN-34-10 on CIFAR-10 and CIFAR-100, as shown in Table 2. Compared to existing methods, DAT still retains better performance and has a lower negative impact on the model's natural accuracy, achieving \(\)1.51%, \(\)0.63%, and \(\)1.31% robust accuracy improvement

  
**Dataset** & **Method** & Natural & FGSM & PGD-20 & PGD-100 & C\&W\({}_{}\) & AA \\   & PGD-AT  & 82.78\(\)0.12 & 56.94\(\)0.17 & 51.30\(\)0.16 & 50.88\(\)0.26 & 49.72\(\)0.24 & 47.63\(\)0.08 \\  & TRADES  & 82.41\(\)0.12 & 58.47\(\)0.19 & 52.76\(\)0.08 & 52.47\(\)0.13 & 50.43\(\)0.17 & 49.37\(\)0.08 \\  & MART  & 80.70\(\)0.17 & 58.91\(\)0.24 & 54.02\(\)0.29 & 53.38\(\)0.30 & 49.35\(\)0.27 & 47.49\(\)0.23 \\  & ST  & 83.10\(\)0.10 & 59.42\(\)0.32 & 54.53\(\)0.14 & 54.31\(\)0.17 & 51.35\(\)0.21 & 50.51\(\)0.17 \\  & SCARL  & 80.67\(\)0.31 & 58.32\(\)0.13 & 54.24\(\)0.17 & 54.10\(\)0.13 & 51.93\(\)0.15 & 50.45\(\)0.11 \\  & DAT (Ours) & **84.17\(\)0.21** & **62.06\(\)0.19** & **57.55\(\)0.15** & **57.47\(\)0.17** & **52.59\(\)0.13** & **51.36\(\)0.14** \\   & TRADES+AWP & 81.16\(\)0.12 & 57.86\(\)0.14 & 54.56\(\)0.06 & 54.45\(\)0.14 & 50.95\(\)0.12 & 50.31\(\)0.10 \\  & SCARL+AWP & 81.46\(\)0.15 & 59.26\(\)0.16 & 58.30\(\)0.14 & 55.27\(\)0.13 & 52.15\(\)0.15 & 51.08\(\)0.11 \\  & DAT+AWP (Ours) & **82.63\(\)0.15** & **62.78\(\)0.21** & **58.78\(\)0.12** & **58.78\(\)0.15** & **52.88\(\)0.21** & **52.54\(\)0.12** \\   & PGD-AT  & 57.27\(\)0.21 & 31.81\(\)0.11 & 28.66\(\)0.11 & 28.49\(\)0.16 & 26.89\(\)0.08 & 24.60\(\)0.04 \\  & TRADES  & 57.94\(\)0.15 & 32.37\(\)0.18 & 29.25\(\)0.18 & 29.10\(\)0.20 & 25.88\(\)0.16 & 24.71\(\)0.04 \\  & MART  & 55.03\(\)0.10 & 33.12\(\)0.26 & 30.32\(\)0.18 & 30.20\(\)0.17 & 26.60\(\)0.11 & 25.13\(\)0.15 \\  & ST  & 58.44\(\)0.12 & 33.35\(\)0.23 & 30.53\(\)0.13 & 30.39\(\)0.17 & 26.70\(\)0.20 & 25.61\(\)0.07 \\  & SCARL  & 57.63\(\)0.11 & 33.14\(\)0.19 & 30.83\(\)0.24 & 30.77\(\)0.21 & 26.86\(\)0.16 & 25.82\(\)0.19 \\  & DAT (Ours) & **62.57\(\)0.17** & **36.63\(\)0.12** & **33.37\(\)0.15** & **31.35\(\)0.12** & **28.34\(\)0.14** & **27.11\(\)0.15** \\   & TRADES+AWP & 58.76\(\)0.07 & 33.82\(\)0.15 & 31.53\(\)0.14 & 31.42\(\)0.12 & 27.03\(\)0.16 & 26.06\(\)0.12 \\  & SCARL+AWP & 58.36\(\)0.12 & 34.25\(\)0.14 & 32.32\(\)0.14 & 32.26\(\)0.13 & 27.92\(\)0.11 & 26.83\(\)0.15 \\  & DAT+AWP (Ours) & **63.28\(\)0.11** & **38.22\(\)0.14** & **35.29\(\)0.13** & **35.18\(\)0.12** & **29.43\(\)0.17** & **28.09\(\)0.12** \\   & PGD-AT  & 43.66\(\)0.22 & 23.49\(\)0.39 & 20.41\(\)0.29 & 20.35\(\)0.37 & 17.86\(\)0.28 & 14.46\(\)0.31 \\  & TRADES  & 43.65\(\)0.35 & 21.37\(\)0.48 & 18.62\(\)0.48 & 18.56\(\)0.33 & 15.38\(\)0.35 & 13.32\(\)0.41 \\  & LAS-AT  & 45.27\(\)0.35 & 24.64\(\)0.24 & 21.82\(\)0.27 & 21.72\(\)0.23 & 18.07\(\)0.25 & 16.25\(\)0.22 \\  & SCARL  & 49.75\(\)0.17 & 25.52\(\)0.16 & 22.64\(\)0.11 & 22.58\(\)0.18 & 18.77\(\)0.27 & 16.31\(\)0.14 \\  & DAT (Ours) & **52.45\(\)0.21** & **28.45\(\)0.15** & **25.47\(\)0.12** & **25.36\(\)0.14** & **20.39\(\)0.17** & **17.51\(\)0.19** \\   & TRADES+AWP & 46.64\(\)0.15 & 25.

[MISSING_PAGE_FAIL:9]

explore the effectiveness of AAG, we combine AAG with some existing AT methods on ResNet-18, see Appendixes F.6 and F.7. Additionally, we compare the time consumption in Appendix F.3.

## 5 Conclusion

This work presents a novel Dual Adversarial Training (DAT) method to improve adversarial robustness against various adversarial attacks. We first illustrate the motivation through some exploration experiments. Subsequently, we delve into the efficient and effective DAT, discussing both its underlying motivation and detailed mechanics. Additionally, we theoretically validate the functionality of the Adversarial Amplitude Generator (AAG) and the convergence properties of the DAT model. Through experiments across multiple datasets against various adversarial attacks, we verify that the proposed DAT significantly improves model robustness. We also explore the hyper-parameters and discuss the function of specific components of DAT. In the future, we will design a more suitable amplitude generation and augmentation strategy to enhance robustness further.