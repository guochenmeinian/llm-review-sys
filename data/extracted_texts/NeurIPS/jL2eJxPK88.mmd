# Fast Partitioned Learned Bloom Filter

Atsuki Sato Yusuke Matsui

The University of Tokyo

Tokyo, Japan

a_sato@hal.t.u-tokyo.ac.jp matsui@hal.t.u-tokyo.ac.jp

###### Abstract

A Bloom filter is a memory-efficient data structure for approximate membership queries used in numerous fields of computer science. Recently, learned Bloom filters that achieve better memory efficiency using machine learning models have attracted attention. One such filter, the partitioned learned Bloom filter (PLBF), achieves excellent memory efficiency. However, PLBF requires a \((N^{3}k)\) time complexity to construct the data structure, where \(N\) and \(k\) are the hyperparameters of PLBF. One can improve memory efficiency by increasing \(N\), but the construction time becomes extremely long. Thus, we propose two methods that can reduce the construction time while maintaining the memory efficiency of PLBF. First, we propose fast PLBF, which can construct the same data structure as PLBF with a smaller time complexity \((N^{2}k)\). Second, we propose fast PLBF++, which can construct the data structure with even smaller time complexity \((Nk N+Nk^{2})\). Fast PLBF++ does not necessarily construct the same data structure as PLBF. Still, it is almost as memory efficient as PLBF, and it is proved that fast PLBF++ has the same data structure as PLBF when the distribution satisfies a certain constraint. Our experimental results from real-world datasets show that (i) fast PLBF and fast PLBF++ can construct the data structure up to 233 and 761 times faster than PLBF, (ii) fast PLBF can achieve the same memory efficiency as PLBF, and (iii) fast PLBF++ can achieve almost the same memory efficiency as PLBF. The codes are available at https://github.com/atsukisato/FastPLBF.

## 1 Introduction

Membership query is a problem of determining whether a given query \(q\) is contained within a set \(\). Membership query is widely used in numerous areas, including networks and databases. One can correctly answer the membership query by keeping the set \(\) and checking whether it contains \(q\). However, this approach is memory intensive because we need to maintain \(\).

A Bloom filter  is a memory-efficient data structure that answers approximate membership queries. A Bloom filter uses hash functions to compress the set into a bitstring and then answers the queries using the bitstring. When a Bloom filter answers \(q\), it can be wrong (false positives can arise); when it answers \(q\), it is always correct (no false negatives arise). A Bloom filter has been incorporated into numerous systems, such as networks [2; 3; 4], databases [5; 6; 7], and cryptocurrencies [8; 9].

A traditional Bloom filter does not care about the distribution of the set or the queries. Therefore, even if the distribution has a characteristic structure, a Bloom filter cannot take advantage of it. Kraska et al. proposed learned Bloom filters (LBFs), which utilize the distribution structure using a machine learning model . LBF achieves a better trade-off between memory usage and false positive rate (FPR) than the original Bloom filter. Since then, several studies have been conducted on various LBFs [11; 12; 13].

Partitioned learned Bloom filter (PLBF)  is a variant of the LBF. PLBF effectively uses the distribution and is currently one of the most memory-efficient LBFs. To construct PLBF, a machine learning model is trained to predict whether the input is included in the set. For a given element \(x\), the machine learning model outputs a score \(s(x)\), indicating the probability that \(\) includes \(x\). PLBF divides the score space \(\) into \(N\) equal segments and then appropriately clusters the \(N\) segments into \(k(<N)\) regions using dynamic programming (DP). Then, \(k\) backup Bloom filters with different FPRs are assigned to each region. We can obtain a better (closer to optimal) data structure with a larger \(N\). However, PLBF builds DP tables \((N)\) times, and each DP table requires \((N^{2}k)\) time complexity to build, so the total time complexity amounts to \((N^{3}k)\). Therefore, the construction time increases rapidly as \(N\) increases.

We propose fast PLBF, which is constructed much faster than PLBF. Fast PLBF constructs the same data structure as PLBF but with \((N^{2}k)\) time complexity by omitting the redundant construction of DP tables. Furthermore, we propose fast PLBF++, which can be constructed faster than fast PLBF, with a time complexity of \((Nk N+Nk^{2})\). Fast PLBF++ accelerates DP table construction by taking advantage of a characteristic that DP tables often have. We proved that fast PLBF++ constructs the same data structure as PLBF when the probability of \(x\) and the score \(s(x)\) are ideally well correlated. Our contributions can be summarized as follows.

* We propose fast PLBF, which can construct the same data structure as PLBF with less time complexity.
* We propose fast PLBF++, which can construct the data structure with even less time complexity than fast PLBF. Fast PLBF++ constructs the same data structure as PLBF when the probability of \(x\) and the score \(s(x)\) are ideally well correlated.
* Experimental results show that fast PLBF can construct the data structure up to 233 times faster than PLBF and achieves the same memory efficiency as PLBF.
* Experimental results show that fast PLBF++ can construct the data structure up to 761 times faster than PLBF and achieves almost the same memory efficiency as PLBF.

## 2 Preliminaries: PLBF

First, we define terms to describe PLBF . Let \(\) be a set of elements for which the Bloom filter is to be built, and let \(\) be the set of elements not included in \(\) that is used when constructing PLBF (\(=\)). The elements included in \(\) are called keys, and those not included are called non-keys. To build PLBF, a machine learning model is trained to predict whether a given element \(x\) is included in the set \(\) or \(\). For a given element \(x\), the machine learning model outputs a score \(s(x)\). The score \(s(x)\) indicates "how likely is \(x\) to be included in the set \(\)."

Next, we explain the design of PLBF. PLBF partitions the score space \(\) into \(k\) regions and assigns backup Bloom filters with different FPRs to each region (Figure 2). Given a target overall FPR,\(F(0,1)\), we optimize \(^{k+1}\) and \(^{k}\) to minimize the total memory usage. Here, \(\) is a vector of thresholds for partitioning the score space into \(k\) regions, and \(\) is a vector of FPRs for each region, satisfying \(t_{0}=0<t_{1}<<t_{k}=1\) and \(0<f_{i} 1\) (\(i=1 k\)).

Next, we explain how PLBF finds the optimal \(\) and \(\). PLBF divides the score space \(\) into \(N(>k)\) segments and then finds the optimal \(\) and \(\) using DP. Deciding how to cluster \(N\) segments into \(k\) consecutive regions corresponds to determining the threshold \(\) (Figure 2). We denote the probabilities that the key and non-key scores are contained in the \(i\)-th **segment by \(g_{i}\)** and \(h_{i}\), respectively. After we cluster the segments (i.e., determine the thresholds \(\)), we denote the probabilities that the key and non-key scores are contained in the \(i\)-th **region** by \(G_{i}\) and \(H_{i}\), respectively (e.g., \(g_{1}+g_{2}+g_{3}=G_{1}\) in Figure 2).

We can find the thresholds \(\) that minimize memory usage by solving the following problem for each \(j=k N\) (see the appendix for details); we find a way to cluster the \(1\)st to \((j-1)\)-th segments into \(k-1\) regions while maximizing

\[_{i=1}^{k-1}G_{i}_{2}(}{H_{i}}).\] (1)

PLBF solves this problem by building a \(j k\) DP table \(_{}^{j}[p][q]\) (\(p=0 j-1\) and \(q=0 k-1\)) for each \(j\). \(_{}^{j}[p][q]\) denotes the maximum value of \(_{i=1}^{q}G_{i}_{2}(}{H_{i}})\) one can get when you cluster the \(1\)st to \(p\)-th segments into \(q\) regions. To construct PLBF, one must find a clustering method that achieves \(_{}^{j}[j-1][k-1]\). \(_{}^{j}\) can be computed recursively as follows:

\[_{}^{j}[p][q]=0&(p=0 q=0)\\ -&((p=0 q>0)(p>0 q=0))\\ _{i=1 p}(_{}^{j}[i-1][q-1]+d_{ }(i,p))&(),\] (2)

where the function \(d_{}(i_{l},i_{r})\) is the following function defined for integers \(i_{l}\) and \(i_{r}\) satisfying \(1 i_{l} i_{r} N\):

\[d_{}(i_{l},i_{r})=(_{i=i_{l}}^{i_{r}}g_{i})_{2} (}^{i_{r}}g_{i}}{_{i=i_{l}}^{i_{r}}h_{i}}).\] (3)

The time complexity to construct this DP table is \((j^{2}k)\). Then, by tracing the recorded transitions backward from \(_{}^{j}[j-1][k-1]\), we obtain the best clustering with a time complexity of \((k)\). As the DP table is constructed for each \(j=k N\), the overall complexity is \((N^{3}k)\). The pseudo-code for PLBF construction is provided in the appendix.

We can divide the score space more finely with a larger \(N\) and thus obtain a near-optimal \(\). However, the time complexity increases rapidly with increasing \(N\).

## 3 Fast PLBF

We propose fast PLBF, which constructs the same data structure as PLBF more quickly than PLBF by omitting the redundant construction of DP tables. Fast PLBF uses the same design as PLBF and finds the best clustering (i.e., \(\)) and FPRs (i.e., \(\)) to minimize memory usage.

PLBF constructs a DP table for each \(j=k N\). We found that this computation is redundant and that we can also use the last DP table \(_{}^{N}\) for \(j=k N-1\). This is because the maximum value of \(_{i=1}^{k-1}G_{i}_{2}(}{H_{i}})\) when clustering the \(1\)st to \((j-1)\)-th segments into \(k-1\) regions is equal to \(_{}^{N}[j-1][k-1]\). We can obtain the best clustering by tracing the transitions backward from \(_{}^{N}[j-1][k-1]\). The time complexity of tracing the transitions is \((k)\), which is faster than constructing the DP table.

This is a simple method, but it is a method that only becomes apparent after some organization on the optimization problem of PLBF. PLBF solves \(N-k+1\) optimization problems separately, i.e., for each of \(j=k,,N\), the problem of "finding the optimal \(\) and \(\) when the \(k\)-th region consists of the \(j,,N\)th segments" is solved separately. Fast PLBF, on the other hand, solves the problems for \(j=k,,N-1\) faster by reusing the computations in the problem for \(j=N\). The reorganization of the problem in the appendix makes it clear that this reuse does not change the answer.

The pseudo-code for fast PLBF construction is provided in the appendix. The time complexity of building \(_{}^{N}\) is \((N^{2}k)\), and the worst-case complexity of subsequent computations is \((Nk^{2})\). Because \(N>k\), the total complexity is \((N^{2}k)\), which is faster than \((N^{3}k)\) for PLBF, although fast PLBF constructs the same data structure as PLBF.

Fast PLBF extends the usability of PLBF. In any application of PLBF, fast PLBF can be used instead of PLBF because fast PLBF can be constructed quickly without losing the accuracy of PLBF. Fast PLBF has applications in a wide range of computing areas [15; 5], and is significantly superior in applications where construction is frequently performed. Fast PLBF also has the advantage of simplifying hyperparameter settings. When using PLBF, the hyperparameters \(N\) and \(k\) must be carefully determined, considering the trade-off between accuracy and construction speed. With fast PLBF, on the other hand, it is easy to determine the appropriate hyperparameters because the construction time is short enough, even if \(N\) and \(k\) are set somewhat large.

## 4 Fast PLBF++

We propose fast PLBF++, which can be constructed even faster than fast PLBF. Fast PLBF++ accelerates the construction of the DP table \(_{}^{N}\) by taking advantage of a characteristic that DP tables often have. When the transitions recorded in computing \(_{}^{N}[p][q]\) (\(p=1 N-1\)) from \(_{}^{N}[p][q-1]\) (\(p=0 N-2\)) are represented by arrows, as in Figure 3, we find that the arrows rarely "cross" (at locations other than endpoints). In other words, the transitions tend to have few intersections (Figure 3(a)) rather than many (Figure 3(b)). Fast PLBF++ takes advantage of this characteristic to construct the DP table with less complexity \((Nk N)\), thereby reducing the total construction complexity to \((Nk N+Nk^{2})\).

Figure 4: Computing \(_{}[p][q]\) (\(p=1 N-1\)) from \(_{}[p][q-1]\) (\(p=0 N-2\)) via the matrix \(A\). The computation is the same as solving the matrix problem for matrix \(A\). When the score distribution is _ideal_, \(A\) is a _monotone matrix_.

Figure 5: Example of a _matrix problem_ for a \(4 5\)_monotone matrix_. The position of the maximum value in each row (filled in gray) is moved to the “lower right”.

First, we define the terms to describe fast PLBF++. For simplicity, \(_{}^{N}\) is denoted as \(_{}\) in this section. The \((N-1)(N-1)\) matrix \(A\) is defined as follows:

\[A_{pi}=-&(i=p+1,p+2,,N-1)\\ _{}[i-1][q-1]+d_{}(i,p)&().\] (4)

Then, from the definition of \(_{}\),

\[_{}[p][q]=_{i=1 N-1}A_{pi}.\] (5)

The matrix \(A\) represents the intermediate calculations involved in determining \(_{}[p][q]\) from \(_{}[p][q-1]\) (Figure 4).

Following Aggarwal et al. , we define the _monotone matrix_ and _matrix problem_ as follows.

**Definition 4.1**.: Let \(B\) be an \(n m\) real matrix, and we define a function \(J:\{1 n\}\{1 m\}\), where \(J(i)\) is the \(j\{1 m\}\) such that \(B_{ij}\) is the maximum value of the \(i\)-th row of \(B\). If there is more than one such \(j\), let \(J(i)\) be the smallest. A matrix \(B\) is called a _monotone matrix_ if \(J(i_{1}) J(i_{2})\) for any \(i_{1}\) and \(i_{2}\) that satisfy \(1 i_{1}<i_{2} n\). Finding the maximum value of each row of a matrix is called a _matrix problem_.

An example of a _matrix problem_ for a _monotone matrix_ is shown in Figure 5. Solving the _matrix problem_ for a general \(n m\) matrix requires \((nm)\) time complexity because all matrix values must be checked. Meanwhile, if the matrix is known to be a _monotone matrix_, the _matrix problem_ for this matrix can be solved with a time complexity of \((n+m n)\) using the divide-and-conquer algorithm . Here, the exhaustive search for the middle row and the refinement of the search range are repeated recursively (Figure 6).

We also define an _ideal score distribution_ as follows.

**Definition 4.2**.: A score distribution is _ideal_ if the following holds:

\[}{h_{1}}}{h_{2}}}{h_{N}}.\] (6)

An _ideal score distribution_ implies that the probability of \(x\) and the score \(s(x)\) are ideally well correlated. In other words, an _ideal score distribution_ means that the machine learning model learns the distribution ideally.

"Few crossing transitions" in Figure 3 indicates that \(A\) is a _monotone matrix_ or is close to it. It is somewhat intuitive that \(A\) is a _monotone matrix_ or is close to it. This is because the fact that \(A\) is a _monotone matrix_ implies that the optimal \(t_{q-1}\) does not decrease when the number of regions is fixed at \(q\) and the number of segments increases by \(1\) (Figure 7). It is intuitively more likely that \(t_{q-1}\) remains unchanged or increases, as in Figure 7(a), than that \(t_{q-1}\) decreases, as in Figure 7(b). Fast PLBF++ takes advantage of this insight to rapidly construct DP tables.

From Equation (5), determining \(_{}[p][q]\)\((p=1 N-1)\) from \(_{}[p][q-1]\)\((p=0 N-2)\) is equivalent to solving the _matrix problem_ of matrix \(A\). When \(A\) is a _monotone matrix_, the divide-and-conquer algorithm can solve this problem with \((N N)\) time complexity. (The same algorithm

Figure 6: A divide-and-conquer algorithm for solving a _matrix problem_ for a _monotone matrix_: (a) An exhaustive search is performed on the middle row, and (b) the result is used to narrow down the search area. (c) This is repeated recursively.

Figure 7: When the number of regions is fixed at \(q\) and the number of segments is increased by \(1\), the optimal \(t_{q-1}\) remains unchanged or increases if \(A\) is a _monotone matrix_ (a). When \(A\) is not a _monotone matrix_ (b), the optimal \(t_{q-1}\) may decrease.

can obtain a not necessarily correct solution even if \(A\) is not a _monotone matrix._) By computing \(_{}[p][q]\) with this algorithm sequentially for \(q=1 k-1\), we can construct a DP table with a time complexity of \((Nk N)\). The worst-case complexity of the subsequent calculations is \((Nk^{2})\), so the total complexity is \((Nk N+Nk^{2})\). This is the fast PLBF++ construction algorithm, and this is faster than fast PLBF, which requires \((N^{2}k)\) computations.

Fast PLBF++ does not necessarily have the same data structure as PLBF because \(A\) is not necessarily a _monotone matrix_. However, as the following theorem shows, we can prove that \(A\) is a _monotone matrix_ under certain conditions.

**Theorem 4.3**.: _If the score distribution is ideal, \(A\) is a monotone matrix._

The proof is given in the appendix. When the distribution is not _ideal_, matrix \(A\) is not necessarily a _monotone matrix_, but as mentioned above, it is somewhat intuitive that \(A\) is close to a _monotone matrix_. In addition, as will be shown in the next section, experiment results from real-world datasets whose distribution is not _ideal_ show that fast PLBF++ is almost as memory efficient as PLBF.

## 5 Experiments

This section evaluates the experimental performance of fast PLBF and fast PLBF++. We compared the performances of fast PLBF and fast PLBF++ with four baselines: Bloom filter , Ada-BF , sandwiched learned Bloom filter (sandwiched LBF) , and PLBF . Similar to PLBF, Ada-BF is an LBF that partitions the score space into several regions and assigns different FPRs to each region. However, Ada-BF relies heavily on heuristics for clustering and assigning FPRs. Sandwiched LBF is an LBF that "sandwiches" a machine learning model with two Bloom filters. This achieves better memory efficiency than the original LBF by optimizing the size of two Bloom filters.

To facilitate the comparison of different methods or hyperparameters results, we have slightly modified the original PLBF framework. The original PLBF was designed to minimize memory usage under the condition of a given false positive rate. However, this approach makes it difficult to compare the results of different methods or hyperparameters. This is because both the false positive rate at test time and the memory usage vary depending on the method and hyperparameters, which often makes it difficult to determine the superiority of the results. Therefore, in our experiments, we used a framework where the expected false positive rate is minimized under the condition of memory usage. This approach makes it easy to obtain two results with the same memory usage and compare them by

Figure 8: Histograms of the score distributions of keys and non-keys.

Figure 9: Ratio of keys to non-keys.

the false positive rate at test time. See the appendix for more information on how this framework modification will change the construction method of PLBFs.

**Datasets**: We evaluated the algorithms using the following two datasets.

* **Malicious URLs Dataset**: As in previous papers [11; 14], we used Malicious URLs Dataset . The URLs dataset comprises 223,088 malicious and 428,118 benign URLs. We extracted 20 lexical features such as URL length, use of shortening, number of special characters, etc. We used all malicious URLs and 342,482 (80%) benign URLs as the training set, and the remaining benign URLs as the test set.
* **EMBER Dataset**: We used the EMBER dataset  as in the PLBF research. The dataset consists of 300,000 malicious and 400,000 benign files, along with the features of each file. We used all malicious files and 300,000 (75%) benign files as the train set and the remaining benign files as the test set.

While any model can be used for the classifier, we used LightGBM  because of its speed in training and inference, as well as its memory efficiency and accuracy. The sizes of the machine learning model for the URLs and EMBER datasets are 312 Kb and 1.19 Mb, respectively. The training time of the machine learning model for the URLs and EMBER datasets is 1.09 and 2.71 seconds, respectively. The memory usage of LBF is the total memory usage of the backup Bloom filters and the machine learning model. Figure 8 shows a histogram of each dataset's score distributions of keys and non-keys. We can see that the frequency of keys increases and that of non-keys decreases as the score increases. In addition, Figure 9 plots \(g_{i}/h_{i}\)\((i=1 N)\) when \(N=1,000\). We can see that \(g_{i}/h_{i}\) tends to increase as \(i\) increases, but the increase is not monotonic (i.e., the score distribution is not _ideal_).

### Construction time

We compared the construction times of fast PLBF and fast PLBF++ with those of existing methods. Following the experiments in the PLBF paper, hyperparameters for PLBF, fast PLBF, and fast PLBF++ were set to \(N=1,000\) and \(k=5\).

Figure 10 shows the construction time for each method. The construction time for learned Bloom filters includes not only the time to insert keys into the Bloom filters but also the time to train the machine learning model and the time to compute the optimal parameters (\(\) and \(\) in the case of PLBF).

Ada-BF and sandwiched LBF use heuristics to find the optimal parameters, so they have shorter construction times than PLBF but have worse accuracy. PLBF has better accuracy but takes more than 3 minutes to find the optimal \(\) and \(\). On the other hand, our fast PLBF and fast PLBF++ take less than 2 seconds. As a result, Fast PLBF constructs 50.8 and 34.3 times faster than PLBF, and fast PLBF++ constructs 63.1 and 39.3 times faster than PLBF for the URLs and EMBER datasets, respectively. This is about the same construction time as sandwiched LBF, which relies heavily on heuristics and, as we will see in the next section, is much less accurate than PLBF.

Figure 10: Construction time.

### Memory Usage and FPR

We compared the trade-off between memory usage and FPR for fast PLBF and fast PLBF++ with Bloom filter, Ada-BF, sandwiched LBF, and PLBF. Following the experiments in the PLBF paper, hyperparameters for PLBF, fast PLBF, and fast PLBF++ were always set to \(N=1,000\) and \(k=5\).

Figure 11 shows each method's trade-off between memory usage and FPR. PLBF, fast PLBF, and fast PLBF++ have better Pareto curves than the other methods for all datasets. Fast PLBF constructs the same data structure as PLBF in all cases, so it has exactly the same accuracy as PLBF. Fast PLBF++ achieves almost the same accuracy as PLBF. Fast PLBF++ has up to 1.0019 and 1.000083 times higher false positive rates than PLBF for the URLs and EMBER datasets, respectively.

### Ablation study for hyper-parameters

The parameters of the PLBFs are memory size, \(N\), and \(k\). The memory size is specified by the user, and \(N\) and \(k\) are hyperparameters that are determined by balancing construction time and accuracy. In the previous sections, we set \(N\) to 1,000 and \(k\) to 5, following the original paper on PLBF. In this section, we perform ablation studies for these hyperparameters to confirm that our proposed methods can construct accurate data structures quickly, no matter what hyperparameter settings are used. We also confirm that the accuracy tends to be better and the construction time increases as \(N\) and \(k\) are increased, and that the construction time of the proposed methods increases much slower than PLBF.

Figure 12 shows the construction time and false positive rate with various \(N\) while the memory usage of the backup Bloom filters is fixed at 500 Kb and \(k\) is fixed at 5. For all three PLBFs, the false positive rate tends to decrease as \(N\) increases. (Note that this is the false positive rate on test data, so it does not necessarily decrease monotonically. The appendix shows that the expected value of the false positive rate calculated using training data decreases monotonically as \(N\) increases.) Also, as \(N\) increases, the PLBF construction time increases rapidly, but the fast PLBF construction time increases much more slowly than that, and for fast PLBF++, the construction time changes little. This is because the construction time of PLBF is asymptotically proportional to \(N^{3}\), while that of fast PLBF and fast PLBF++ is proportional to \(N^{2}\) and \(N N\), respectively. The experimental results

Figure 11: Trade-off between memory usage and FPR.

Figure 12: Ablation study for hyper-parameter \(N\).

show that the two proposed methods can achieve high accuracy without significantly changing the construction time with large \(N\).

Figure 13 shows the construction time and false positive rate with various \(k\) while the backup bloom filter memory usage is fixed at 500 Kb and \(N\) is fixed at 1,000. For all three PLBFs, the false positive rate tends to decrease as \(k\) increases. For the EMBER dataset, the false positive rate stops decreasing at about \(k=20\), while for the URLs dataset, it continues to decrease even at about \(k=500\). (Just as in the case of experiments with varying \(N\), this decrease is not necessarily monotonic.) In addition, the construction times of all three PLBFs increase proportionally to \(k\), but fast PLBF has a much shorter construction time than PLBF, and fast PLBF++ has an even shorter construction time than fast PLBF. When \(k=50\), fast PLBF constructs 233 and 199 times faster than PLBF, and fast PLBF++ constructs 761 and 500 times faster than PLBF for the URLs and EMBER datasets, respectively. The experimental results indicate that by increasing \(k\), the two proposed methods can achieve high accuracy without significantly affecting the construction time.

## 6 Related Work

Approximate membership query is a query that asks whether the query \(q\) is contained in the set \(\) while allowing for false positives with a small probability \(\). One can prove that at least \(||_{2}()\) bits of memory must be used to answer the approximate membership query if the elements in the set or queries are selected with equal probability from the universal set .

A Bloom filter  is one of the most basic data structures for approximate membership queries. It compresses the set \(\) into a bit string using hash functions. This bit string and the hash functions are then used to answer the queries. To achieve an FPR of \(\), a Bloom filter requires \(||_{2}()_{2}e\) bits of memory. This is \(_{2}e\) times the theoretical lower bound.

Various derivatives have been proposed that achieve better memory efficiency than the original Bloom filter. The cuckoo filter  is more memory efficient than the original Bloom filter and supports dynamic addition and removal of elements. Pagh et al.  proposed a replacement for Bloom filter that achieves a theoretical lower bound. Various other memory-efficient filters exist, including the vacuum filter , xor filter , and ribbon filter . However, these derivatives do not consider the structure of the distribution and thus cannot take advantage of it.

Kraska et al. proposed using a machine learning model as a prefilter of a backup Bloom filter. Ada-BF  extended this design and proposed to exploit the scores output by the machine learning model. PLBF  uses a design similar to that of Ada-BF but introduces fewer heuristics for optimization than Ada-BF. Mitzenmacher  proposed an LBF that "sandwiches" a machine learning model with two Bloom filters. This achieves better memory efficiency than the original LBF but can actually be interpreted as a special case of PLBF.

## 7 Limitation and Future Work

As explained in Section 4, it is somewhat intuitive that \(A\) is a monotone matrix or close to it, so it is also intuitive that fast PLBF++ achieves accuracy close to PLBF. Experimental results on the URLs and EMBER datasets also suggest that fast PLBF++ achieves almost the same accuracy as PLBF,

Figure 13: Ablation study for hyper-parameter \(k\).

even when the score distribution is not _ideal_. This experimental rule is further supported by the results of the artificial data experiments described in the appendix. However, there is no theoretical support for the accuracy of fast PLBF++. Theoretical support for how fast PLBF++ accuracy may degrade relative to PLBF is a future issue.

Besides, it is possible to consider faster methods by making stronger assumptions than fast PLBF++. Fast PLBF++ assumes the _monotonicity_ of matrix \(A\). We adopted this assumption because matrix \(A\) is proved to be _monotone_ under intuitive assumptions about the data distribution. However, by assuming stronger assumptions about matrix \(A\), the computational complexity of the construction could be further reduced. For example, if matrix \(A\) is _totally monotone_, the matrix problem for matrix \(A\) can be solved in \((N)\) using algorithms such as . Using such existing DP algorithms is a promising direction toward even faster or more accurate methods, and is a future work.

## 8 Conclusion

PLBF is an outstanding LBF that can effectively utilize the distribution of the set and queries captured by a machine learning model. However, PLBF is computationally expensive to construct. We proposed fast PLBF and fast PLBF++ to solve this problem. Fast PLBF is superior to PLBF because fast PLBF constructs exactly the same data structure as PLBF but does so faster. Fast PLBF++ is even faster than fast PLBF and achieves almost the same accuracy as PLBF and fast PLBF. These proposed methods have greatly expanded the range of applications of PLBF.