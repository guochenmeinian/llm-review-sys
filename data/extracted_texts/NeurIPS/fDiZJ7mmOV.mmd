# Non-Stationary Learning of Neural Networks

with Automatic Soft Parameter Reset

 Alexandre Galashov

Gatsby Unit, UCL

Google DeepMind

agalashov@google.com

&Michalis K. Titsias

Google DeepMind

mtitsias@google.com

&Andras Gyorgy

Google DeepMind

agyorgy@google.com

&Clare Lyle

Google DeepMind

clarelyle@google.com

&Razvan Pascanu

Google DeepMind

razp@google.com

&Yee Whye Teh

Google DeepMind

University of Oxford

yvteh@google.com

Maneesh Sahani

Gatsby Unit, UCL

maneesh@gatsby.ucl.ac.uk

Corresponding author

###### Abstract

Neural networks are most often trained under the assumption that data come from a stationary distribution. However, settings in which this assumption is violated are of increasing importance; examples include supervised learning with distributional shifts, reinforcement learning, continual learning and non-stationary contextual bandits. Here, we introduce a novel learning approach that automatically models and adapts to non-stationarity by linking parameters through an Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift draws the parameters towards the distribution used at initialisation, so the approach can be understood as a form of _soft_ parameter reset. We show empirically that our approach performs well in non-stationary supervised, and off-policy reinforcement learning settings.

## 1 Introduction

Neural networks (NNs) are typically trained using algorithms such as stochastic gradient descent (SGD), which implicitly assume that the data come from a stationary distribution. This assumption is incorrect for scenarios such as continual learning, reinforcement learning, non-stationary contextual bandits, and supervised learning with distribution shifts . Although the parameters can be updated online as new data are encountered, this approach often leads to a _loss of plasticity_, manifesting either as a failure to generalise to new data despite reduced training loss , or as an inability to reduce training error as the data distribution changes .

In , the authors argue for two factors that lead to loss of plasticity: shifts in the distribution of preactivations, leading to dead or dormant neurons , and growth in the parameter norm, causing training instabilities. These problems are often addressed using _hard resets_ based on heuristics such as detecting dormant units , assessing unit utility , or simply after a fixed number of steps has elapsed. Although effective at increasing plasticity, hard resets can be inefficient as they can discard valuable knowledge captured by the parameters.

We propose an algorithm that instead implements a form of _soft_ parameter reset, avoiding the pitfalls associated with hard resets. A _soft_ reset moves the parameters in the direction of the initial-value distribution, while maintaining dependence on their previous values. It also increases the learning rate applied to gradient-based updates, allowing new parameters to adapt faster to the changing data. The magnitude of the soft reset is governed by an Ornstein-Uhlenbeck _drift_ process linking the parameters in time, with the scale of the drift itself chosen adaptively. In effect, the model approximates the action of a dynamical Bayesian prior over the NN parameters, which is adapted online to new data.

Our contributions can be summarised as follows. We provide an explicit formalisation of the model for the drift in NN parameters as sketched above, and derive a procedure to estimate the parameters of the drift model online. Second, we use the learnt drift model to modify the NN parameter update algorithm. Third, we explore the effectiveness of the approach in supervised learning experiments, showing that it avoids the challenge of plasticity loss, as well as in an off-policy reinforcement learning setting.

## 2 Non-stationary learning with Online SGD

Consider a non-stationary supervised learning setting with changing data distributions \(p_{t}(x,y)\), where \(x^{L}\) and \(y^{K}\) (\(y\) may be restricted to "one-hot" categorical indicator vectors). Given a single-point loss \((,x,y)\), we define the time-dependent expected loss function for parameters \(^{D}\) to be

\[_{t}()=_{(x_{t},y_{t}) p_{t}}( ,x_{t},y_{t})\,.\] (1)

Our goal is to find a parameter sequence \(=(_{1},,_{T})\) that reduces the dynamic regret

\[R_{T}(,^{})=_{t=1}^{T}(_{t}( _{t})-_{t}(_{t}^{})),\] (2)

relative to an oracular reference sequence \(^{}=(_{1}^{},,_{T}^{})\) satisfying \(_{t}^{}_{}_{t}()\). One common approach to online learning employs online stochastic gradient descent (SGD) . An initial parameter value \(_{0}\) is updated sequentially for each batch of data \(_{t}=\{(x_{t}^{i},y_{t}^{i})\}_{i=1}^{B}\) s.t. \((x_{t}^{i},y_{t}^{i}) p_{t}(x_{t},y_{t})\). The update rule is:

\[_{t}=_{t-1}-_{t}_{}}_{t}( _{t-1}),\]

where \(}_{t}()=_{i=1}^{B}(,x_{t}^{i},y_{t}^{i})\) is the empirical loss on the batch, and \(_{t}\) is a learning rate. See also Appendix G for the connection of SGD to proximal optimization.

**Convex settings.** For convex losses, online SGD with a fixed learning rate \(\) can track non-stationarity . By selecting \(\) appropriately--potentially using additional knowledge about the reference sequence--we can optimise the dynamic regret in (2). In general, algorithms that adapt to the observed level of non-stationarity can outperform standard online SGD. For example, in , the authors propose to adjust the learning rate \(_{t}\), while in  and in , the authors suggest modifying the starting point of SGD from \(_{t}\) to an adjusted \(_{t}^{}\) proportional to the level of non-stationarity.

**Non-convex settings.** Non-stationary learning with NNs is more complex, since now there is a changing set of local minima as the data distribution changes. Such changes can lead to a loss of plasticity and other pathologies. Alternative optimization methods like Adam , do not fully resolve this issue [13; 37; 1; 42; 34]. Parameter resets [13; 48; 12] partially mitigate the problem, but may be too aggressive if the data distributions are similar.

## 3 Online non-stationary learning with learned soft parameter resets

Notation.We denote by \((;,^{2})\) a Gaussian distribution on \(\) with mean \(\) and variance \(^{2}\). We denote by \(^{j}\) the \(j\)-th component of the vector \(=(^{1},,^{D})\). We assume that the NN outputs define a distribution over targets \(y\), so that the single-point loss is the negative log likelihood \((,x,y)=- p(y|x,)\). As above, \(_{t}=\{(x_{t}^{i},y_{t}^{i})\}_{i=1}^{B}\) is a batch of data sampled from \(p_{t}(x,y)\) with a fixed batch size \(B\) (which could be \(1\)) and \(}_{t}()= p(_{t}|)= _{i=1}^{B}(,x_{t}^{i},y_{t}^{i})\) is the average negative log likelihood on batch \(_{t}\). Element-wise product is denoted by \(\).

Our proposed _soft reset_ scheme tracks changes in the distribution of non-stationary data using an explicit _drift_ model for the parameters \(p(_{t+1}|_{t},_{t})\). Specifically, the drift model we adopt assumesthat the parameters change in the direction of the initialization distribution at each time \(t\). The amount of change, and thus the strength of reset, is controlled by non-stationary hyperparameters \(_{t}\). These hyperparameters are themselves estimated online from the data.

From a probabilistic standpoint, the drift model implies an _empirical Bayesian_ prior on parameters \(_{t+1}\), which is displaced from the posterior on \(_{t}\) towards the prior distribution, and has increased variance (controlled by \(_{t}\)). As we argue below, the effect can also be interpreted in the context of SGD learning, as an adjustment in the starting point for batch \(_{t+1}\) to a point \(_{t}(_{t})\) between the previous estimate \(_{t}\) and the mean of initializing distribution, with a corresponding increase in the learning rate. This approach is inspired by prior work in online convex optimization for non-stationary environments [e.g., 25; 21; 8; 18; 29].

### Toy illustration of the advantage of drift models

Consider online Bayesian inference with 2-D observations \(y_{t}=^{}+_{t}\), where \(^{}^{2}\) are unknown _true_ parameters and \(_{t}(0;^{2}I)\) is Gaussian noise with variance \(^{2}\). Starting from a Gaussian prior \(p_{0}()=(;_{0},_{0})\), the posterior distribution \(p_{t+1}()=p(|y_{1},,y_{t+1})=(;_{t+1}, _{t+1})\) is updated using Bayes' rule

\[p_{t+1}() p(y_{t+1}|)p_{t}().\] (3)

The posterior update (3) arises from assumption that data are i.i.d. (Figure 1a), since then \(p_{t+1}() p_{0}()_{s=1}^{t+1}p(y_{s}|)\). By the Central Limit Theorem, the posterior mean \(_{t}\) converges to \(^{}\) and the covariance matrix \(_{t}\) shrinks to zero (the radius of red circle in Figure 1c).

Suppose now that the _true_ parameters \(_{t}^{}\) (which were fixed up to time \(t\)) change to new parameters \(_{t+1}^{}\) at time \(t+1\). The i.i.d. assumption is thus violated and the update (3) becomes problematic because the low uncertainty (small radius of red dashed circle in Figure 1d) in \(p_{t}()\) causes the posterior \(p_{t+1}()\) (see blue circle) to adjust slowly towards \(_{t+1}^{}\) (blue dot) as illustrated in Figure 1d.

The issue is addressed by allowing explicitly for the possibility that, before observing new data, the parameters _drift_ according to \(p(_{t+1}|_{t},_{t})\). The corresponding conditional independence structure is shown in Figure 1b. The posterior update then becomes:

\[p_{t+1}() p(y_{t+1}|) p(|_{t}^{}, _{t})p_{t}(_{t}^{})d_{t}^{}.\] (4)

For a suitable choice of drift model \(p(_{t+1}|_{t},_{t})\), this modification allows \(p_{t+1}()\) (blue circle) to adjust more rapidly towards the new \(_{t+1}^{}\) (blue dot), see Figure 1e. This is because the new prior \( p(|_{t}^{},_{t})p_{t}(_{t}^{})d_ {t}^{}\) has larger variance (green circle) than \(p_{t}()\) and its mean is closer to the center of the circle. Ideally, the parameter \(_{t}\) should capture the underlying non-stationarity in the data distribution in order to control the impact of the prior \( p(|_{t}^{},_{t})p_{t}(_{t}^{})d _{t}^{}\). For example, if at some point the non-stationarity disappears, we want the drift model to adaptively eliminate changes and recover the stationary posterior update (3). This highlights the importance of the adaptive nature of the drift model.

Figure 1: **Left**: graphical model for data generating process in the (a) stationary case and (b) non-stationary case with drift model \(p(_{t+1}|_{t},_{t})\). **Right**: (c) In a stationary online learning regime, the Bayesian posterior (red dashed circles) in the long run will concentrate around \(^{}\) (red dot). (d) In a non-stationary regime where the optimal parameters suddenly change from current value \(_{t}^{}\) to new value \(_{t+1}^{}\) (blue dot) online Bayesian estimation can be less data efficient and take time to recover when the change-point occurs. (e) The use of \(p(|_{t},_{t})\) and the estimation of \(_{t}\) allows to increase the uncertainty, by soft resetting the posterior to make it closer to the prior (green dashed circle), so that the updated Bayesian posterior \(p_{t+1}()\) (blue dashed circle) can faster track \(_{t+1}^{}\).

### Ornstein-Uhlenbeck parameter drift model

We motivate a specific choice of drift model which is useful for maintaining plasticity. Assume that the NN is flexible enough to capture any _stationary_ dataset in a fixed number of iterations starting from a good initialization \(_{0} p_{0}()\)[see, e.g., 24, 16]. Informally, we refer to the region of high probability under \(p_{0}\) as a _plastic region_.

In a non-stationary data setting, learning for the batch \(_{t+1}\) is initialized based on the learnt parameter at the previous time step, \(_{t}\). Empirically, if this parameter is far from the initial plastic region, the NN may suffer from a _loss of plasticity_ with SGD and similar online methods failing to adapt to the new data. In this case, a _hard reset_ of the parameters to \(_{0}\) (or an alternative draw from \(p_{0}\)) may be helpful. However, if changes in the data distribution are gradual then performance may be improved by retaining information about \(_{t}\). Furthermore, since \(\) is high-dimensional, each dimension might benefit from different updates.

**Drift model.** A drift model \(p(_{t+1}|_{t},_{t})\) which captures these desiderata is given by

\[p(|_{t},_{t})=_{j}(^{j};_{t}^{j} _{t}^{j}+(1-_{t}^{j})_{0}^{j},(1-_{t}^{j^{2}})_{0} ^{j^{2}}),\] (5)

defined independently for each parameter dimension \(^{j}\), where \(p_{0}(_{0}^{j})=(_{0}^{j};_{0}^{j};_{0}^{j^{2}})\) is the per-parameter prior distribution and \(_{t}=(_{t}^{1},,_{t}^{D})\) controls drift in each parameter separately. The model is a discretized Ornstein-Uhlenbeck (OU) process  (see Appendix B for the derivation).

The parameters \(_{t}^{j}\) control the degree to which each parameter is reset. When \(_{t}^{j}=1\), the model has \(_{t+1}^{j}\) and \(_{t}^{j}\) equal, and the data in \(_{t+1}\) are used to refine the estimate of the common parameter. When \(_{t}^{j}=0\), \(_{t+1}^{j}\) is drawn from the prior \(p_{0}\) independently of \(_{t}^{j}\), and so \(_{t+1}\) is used independently of previous data. A value of \(_{t}^{j}(0,1)\) interpolates between these two extremes. The process (5) has the property that for any current parameter \(_{t}\) and \(_{t}^{j}[0,1)\), as \(T\) the distribution of \(p(_{T}|_{t})\) will converge to the prior \(p(_{0})\). This behaviour depends on interplay between the mean shrinkage and the variance. Other choices of drift variance would result in the variance of \(p(_{T}|_{t})\) either going to \(0\) or growing to \(\), harming learning. Thus, the model (5) encourages parameters to move towards the plastic region (initialization). In Appendix C, we discuss this further and other potential choices for the drift model.

### Online estimation of drift

The parameters \(_{t}\) can themselves be selected within the empirical Bayesian framework by optimizing the _predictive likelihood_, which quantifies the probability of new data under the current parameters and drift model. We derive the drift estimation procedure in the context of approximate online variational inference  with Bayesian Neural Networks (BNN). Let \(_{t}=(_{1},,_{t})\) be the history of observed parameters of the drift model and \(_{t}=\{_{1},,_{t}\}\) be the history of observed data. The objective of approximate online variational inference is to propagate an _approximate_ posterior \(q_{t}(|_{t},_{t-1})\) over parameters, such that it is constrained to some family \(\) of probability distributions in the context of BNNs, it is typical  to assume a family \(=\{q():q()_{j=1}^{D}(^{j}; ^{j},{^{j}}^{2});=(^{1},,^{D})\}\) of Gaussian mean-field distributions over parameters \(^{D}\) (separate Gaussian per parameter). Let \(q_{t}() q_{t}(|_{t},_{t-1})\) be the Gaussian _approximate_ posterior at time \(t\) with mean \(_{t}\) and variance \(_{t}^{2}\) for every parameter. The new approximate posterior \(q_{t+1}()\) is found by

\[q_{t+1}()=*{arg\,min}_{q}[q()\|p( _{t+1}|)q_{t}(|_{t})],\] (6)

where the prior term is the approximate predictive look-ahead prior given by

\[q_{t}(|_{t})= q_{t}(_{t})p(|_{t},_{t}) d_{t}=(;_{t}(_{t}),_{t}^{2}( _{t}))\] (7)

that has parameters \(=(^{1},,^{D})\) and \(^{2}=(^{1}{}^{2},,^{D}{}^{2})\) such that \(_{t}^{j}(_{t})=_{t}^{j}_{t}^{j}+(1-_{t}^{j})_ {0}^{j},_{t}^{j}{}^{2}(_{t})={_{t}^{j}}^{2}{_{t }}^{j}^{2}+(1-{_{t}^{j}}^{2}){_{0}^{j}}^{2}\), see Appendix E.1 for derivation. The form of this prior \(q_{t}(|_{t})\) comes from the non i.i.d. assumption (see Figure 1b) and the form of the drift model (5). For new batch of data \(_{t+1}\) at time \(t+1\), the _approximate predictive log-likelihood_ equals to

\[ q_{t}(_{t+1}|_{t})= p(_{t+1}|)q_ {t}(|_{t})d.\] (8)

The log-likelihood (8) allows us to quantify predictions on batch of data \(_{t+1}\) given our current distribution \(q_{t}()\) and the drift model from (5). We find such \(_{t}^{*}\) that

\[_{t}^{*}*{arg\,max}_{_{t}} q_{t}(_{t+1}|_{t})\] (9)Using \(_{t}^{}\) in (5) modifies the prior distribution (7) to fit the most recent observations the best by putting more mass on the region where the new parameter could be found (see Figure 1,right).

**Gradient-based optimization for \(_{t}\).** The approximate predictive prior in (7) is Gaussian which allows us to use the so-called reparameterisation trick to optimize (8) via gradient descent. Starting from an initial value of drift parameter \(_{t}^{0}\) at time \(t\), we perform \(K\) updates with learning rate \(_{}\)

\[_{t,k+1}=_{t,k}+_{}_{} p( _{t+1}|_{t}(_{t,k})+_{t}(_{t,k}))(;0,I)d,\] (10)

The integral is evaluated by Monte-Carlo (MC) using \(M\) samples \(_{i}(;0,I)\), \(i=1,,M\)

\[ p(_{t+1}|_{t}(_{t,k})+_{t}(_{t,k}))(;0,I)d {M}_{i=1}^{M}p(_{t+1}|_{t}(_{t,k})+_{ i}_{t}(_{t,k}))\] (11)

Inductive bias in the drift model is captured by \(_{t}^{0}\), where \(_{t,0}=1\) encourages stationarity, while \(_{t,0}=_{t-1,K}\) promotes temporal smoothness. In practice, we found \(_{t,0}=1\) was the most effective.

**Structure in the drift model.** The drift model can be defined to be shared across different _subsets_ of parameters which reduces the expressivity of the drift model but also provides regularization to (10). We consider \(_{t}\) to be either defined for each _parameter_ or for each _layer_. See Section 5 for details as well as corresponding results in Appendix K.

**Interpretation of \(_{t}\).** By linearising \( p(_{t+1}|)\) around \(_{t}\) and denoting \(g_{t+1}=}_{t+1}(_{t})\), we compute (8) in a closed form and get the following loss for \(_{t}\) (see Appendix F) optimizing (9)

\[(_{t})=0.5(_{t}^{2}(_{t}) g_{t+1})^ {}g_{t+1}-(_{t}_{t}+(1-_{t})_{0})^{}g_{t+ 1},\] (12)

Adding the \(_{2}\) penalty \(||_{t}-_{t}^{0}||^{2}\) encoding the starting point \(_{t}^{0}\), gives us the closed form for \(_{t}\)

\[_{t}=_{t+1}^{T}(_{t}-_{0})+K _{t,0}}{(_{t+1}(_{0}^{2}-_{t}^{ 2}))^{T}_{t+1}+K},\] (13)

where we assumed that \(_{t}\) is shared for a subset of \(K\) parameters (see paragraph about structure in drift model) indexed by \(J_{K}=(j_{1},,j_{K})\) and \(=(x_{j_{1}},,x_{j_{K}})\) denotes a vector defined on this subset. We also clip parameters \(_{t}\) to \(\). The expression (13) gives us the geometric interpretation for \(_{t}\). The value of \(_{t}\) depends on the angle between \((_{t}-_{0})\) and \(_{t+1}\) When these vectors are aligned, \(_{t}\) is high and is low otherwise. When these vectors are orthogonal or the gradient \(_{t+1} 0\), the value of \(_{t}\) is heavily influenced by \(_{t}^{0}\). Moreover, when \(_{t+1} 0\), we can interpret it as being close to a local minimum, i.e., stationary, which means that we want \(_{t} 1\), therefore adding the \(_{2}\) penalty is important. Also, when the norm of the gradients \(_{t+1}\) is high, the value of \(_{t}\) is encouraged to decrease, introducing the drift. This means that using \(_{t}\) in the parameter update (see Section 3.5) encourages the norm of the gradient to stay small. In practice, we found that update (13) was unstable suggesting that linearization of the log-likelihood might not be a good approximation for learning \(_{t}\).

### Approximate Bayesian update of posterior \(q_{t}()\) with BNNs

The optimization problem (6) for the per-parameter Gaussian \(q()=_{j}(^{j};^{j},{^{j}}^{2})\) with a prior \(q_{t}()=_{j}(^{j};_{t}^{j},{_{t}^{j}}^{2})\) can be written (see Appendix E.1) to minimize the following loss

\[}_{t}(,,_{t})=_{ (0;I)}[}_{t+1}(+) ]+_{j=1}^{D}_{t}^{j}[-_{t}^{j} (_{t}))^{2}+[^{j}]^{2}}{2[_{t}^{j} (_{t})]^{2}}-[^{j}]^{2}],\] (14)

where \(_{t}^{j}>0\) are per-parameter temperature coefficients. The use of a small temperature parameter \(>0\) (shared for all NN parameters) was shown to improve the empirical performance of Bayesian Neural Networks . Given that in (14), the variance \(_{t}^{j\,2}(_{t})\) can be small, in order to control the strength of the regularization, we propose to use the temperature per parameter \(_{t}^{j}=[_{t}^{j}]^{2}\), where \(>0\) is a global constant. This leads to the following objective

\[}_{t}(,,_{t})=_{ (0;I)}[}_{t+1}(+) ]+_{j}r_{t}^{j}[(_{j}-_{t}^{j} (_{t}))^{2}+[^{j}]^{2}-[_{t}^{j}(_{t}) ]^{2}[^{j}]^{2}],\] (15)

where the quantity \(r_{t}^{j}=[_{t}^{j}]^{2}/[_{t}^{j}(_{t})]^{2}\) is a relative change in the posterior variance due to the drift. The ratio \(r_{t}^{j}=1\) when \(_{t}^{j}=1\). For \(_{t}^{j}<1\) since typically \({_{t}^{j\,2}}<{_{0}^{j\,2}}\), the ratio is \(r_{t}^{j}<1\).

Thus, as long as the non stationarity is detected (\(_{t}^{j}<1\)), the objective (15) favors the data term \(_{(0;I)}[}_{t+1}(+ )]\) allowing the optimization to respond faster to changes in the data distribution. Let \(_{t+1,0}=}(_{t})\) and \(_{t+1,0}=_{t}(_{t})\), and perform updates \(K\) on (15)

\[_{t+1,k+1}=_{t+1,k}-_{}}_{t}(_{t+1,k}, _{t+1,k},_{t}),\ \ _{t+1,k+1}=_{t+1,k}-_{}}_{t}(_{t+1,k },_{t+1,k},_{t}),\] (16)

where \(_{}\) and \(_{}\) are learning rates for the mean and for the standard deviation correspondingly. All derivations are provided in Appendix E.1. The full procedure is described in Algorithm 2.

``` Input: Data-stream \(_{T}=\{_{t}\}_{t=1}^{T}\)  Neural Network (NN) initializing distribution \(p_{init}()\) and specific initialization \(_{0} p_{init}()\)  Learning rate \(_{t}\) for parameters and \(_{}\) for drift parameters  Number of gradient updates \(K_{}\) on drift parameter \(_{t}\)  NN initial standard deviation (STD) scaling \( 1\) (see (24)) and ratio \(s\) defining \(}{_{0}}\). for step \(t=0,1,2,,T\)do  Receive batch of data \(_{t+1}\)  Initialize drift parameters \(_{t,0}=1\) for step \(k=0,1,2,,K_{}\)do  Sample \(_{0}^{} p_{init}()\)  Stochastic update (22) on drift parameter using specific initialization (26) \(_{t,k+1}=_{t,k}+_{}_{}[ p(_{t+1}|_{t}_{t}+(1-_{t})_{0}+_{0}^{} ^{2}+_{t}^{2}s^{2}})]_{_{t}=_{t,k}}\) endfor  Get \(_{t}(_{t,K})\) with (18) and \(_{t}(_{t,K})\) with (19)  Update parameters \(_{t+1}=_{t}(_{t,K})-_{t}(_{t,K}) _{}}_{t+1}(_{t}(_{t,K}))\) endfor ```

**Algorithm 1**_Soft-Reset_ algorithm

### Fast MAP update of posterior \(q_{t}()\)

As a faster alternative to propagating the posterior (6), we do MAP updates with the prior \(p_{0}()=_{j}(^{j};_{0}^{j};_{0}^{j\,2})\) and the approximate posterior \(q_{t}()=_{j}(^{j};_{t}^{j};_{t}^{j\,2} =s^{2}_{0}^{j\,2})\), where \(s 1\) is a hyperparameter controlling the variance of \(q_{t}()\). Since a fixed \(s\) may not capture the true parameters variance, using a Bayesian method (see Section 3.4) is preferred but comes at a high computational cost (see Appendix I for discussion). The MAP update is given by (see Appendix E.2 for derivations) finding a minimum of the following proximal objective

\[_{t+1}()=}_{t+1}()+ _{j=1}^{D}-_{t}^{j}(_{t})|^{2}}{ _{t}^{j}(_{t})}\] (17)

where the regularization target for the parameter dimension \(i\) is given by

\[_{t}^{j}(_{t})=_{t}^{j}_{t}^{j}+(1-_{t}^ {j})_{0}^{j}\] (18)

and the per-parameter learning rate is given as (assuming that \(_{t}\) the base SGD learning rate)

\[_{t}^{j}(_{t})=_{t}((_{t}^{j})^{2}+ {1-(_{t}^{j})^{2}}{s^{2}}).\] (19)

Linearising \(}_{t+1}()\) around \(_{t}(_{t})\) and optimizing (17) for \(\) leads to (see Appendix E.2)

\[_{t+1}=_{t}(_{t})-_{t}(_{t}) _{}}_{t+1}(_{t}(_{t})),\] (20)

For \(_{t}^{j}=1\), we recover the ordinary SGD update, while the values \(_{t}^{j}<1\) move the starting point of the modified SGD closer to the initialization as well as increase the learning rate. In Appendix D we describe additional practical choices made for the _Soft Resets_ algorithm. Algorithm 1 describes the full procedure. Similarly to the Bayesian approach (16), we can do multiple updates on (17). This _Soft Resets Proximal_ algorithm is given in Appendix E.2. Algorithm 3 describes the full procedure.

Related Work

**Plasticity loss in Neural Networks.** Our model shares similarities with reset-based approaches such as Shrink & Perturb (S&P)  and L2-Init ; however, whereas we learn drift parameters from data, these methods do not, leaving them vulnerable to mismatch between assumed non-stationarity and the actual realized non-stationarity in the data. Continual Backprop  or ReDO  apply resets in a data-dependent fashion, e.g. either based on utility or whether units are dead. But they use hard resets, and cannot amortize the cost of removing entire features. Interpretation (13) of \(_{t}\) connects to the notion of parameters utility from , but this quantity is used to prevent catastrophic forgetting by decreasing learning rate for high \(_{t}\). Our method increases the learning rate for low \(_{t}\) to maximize adaptability, and is not designed to prevent catastrophic forgetting.

**Non-stationarity.** Non-stationarity arises naturally in a variety of contexts, the most obvious being continual and reinforcement learning. The structure of non-stationarity may vary from problem to problem. At one extreme, we have a _piece-wise stationary_ setting, for example a change in the location of a camera generating a stream of images, or a hard update to the learner's target network in value-based deep RL algorithms. This setting has been studied extensively due to its propensity to induce _catastrophic forgetting_ [e.g. 31, 45, 51, 10] and _plasticity loss_. At the other extreme, we can consider more gradual changes, for example due to improvements in the policy of an RL agent  or shifts in the data generating process . Further, these scenarios might be combined, for example in _continual reinforcement learning_ where the reward function or transition dynamics could change over time.

**Non-stationary online convex optimization.** Non-stationary prediction has a long history in online convex optimization, where several algorithms have been developed to adapt to changing data [see, e.g., 25, 8, 22, 17, 21, 18, 29]. Our approach takes an inspiration from these works by employing a drift model as, e.g.,  and by changing learning rate as . Further, our OU drift model bears many similarities to the implicit drift model introduced in the update rule of  (see also ), where the predictive distribution is mixed with a uniform distribution to ensure the prediction could change quickly enough if the data changes significantly, where in our case \(p_{0}\) plays the same role as the uniform distribution.

**Bayesian approaches to non-stationary learning.** A standard approach is Variational Continual Learning , which focuses on preventing catastrophic forgetting and is an online version of "Bayes By Backprop" . This method does not incorporate dynamical parameter drift components. In , the authors applied variational inference (VI) on non-stationary data, using the OU-process and Bayesian forgetting, but unlike in our approach, their drift parameter is not learned. Further, in , the authors considered an OU parameter drift model similar to ours, with an adaptable drift scalar \(\) and analytic Kalman filter updates, but is applied over the final layer weights only, while the remaining weights of the network were estimated by online SGD. In , the authors propose to deal with non-stationarity by assuming that each parameter is a finite sum of random variables following different OU process. They derive VI updates on the posterior of these variables. Compared to this work, we learn drift parameters for every NN parameter rather than assuming a finite set of drift parameters. A different line of research assumes that the drift model is known and use different techniques to estimate the hidden state (the parameters) from the data: in , the authors use Extended Kalman Filter to estimate state and in , they propagate the MAP estimate of the hidden state distribution with \(K\) gradient updates on a proximal objective similar to (47), whereas in Bayesian Online Natural Gradient (BONG) , the authors use natural gradient for the variational parameters.

## 5 Experiments

**Soft reset methods.** There are multiple variations of our method. We call the method implemented by Algorithm 1 with \(1\) gradient update on the drift parameter _Soft Reset_, while other versions show different parameter choices: _Soft Reset_ (\(K_{}=10\)) is a version with \(10\) updates on the drift parameter, while _Soft Reset_ (\(K_{}=10\), \(K_{}=10\)) is the method of Algorithm 3 in Appendix E.2 with \(10\) updates on drift parameter, followed by \(10\) updates on NN parameters. _Bayesian Soft Reset_ (\(K_{}=10\), \(K_{}=10\)) is a method implemented by Algorithm 2 with \(10\) updates on drift parameter followed by \(10\) updates on the mean \(_{t}\) and the variance \(_{t}^{2}\) (uncertainty) for each NN parameter. Bayesian method performed the best overall but required higher computational complexity (see Appendix I). Unless specified, \(_{t}\) is shared for all the parameters in each layer (separately for weight and biases).

**Loss of plasticity.** We analyze the performance of our method on _plasticity benchmarks_. Here, we have a sequence of tasks, where each task consists of a fixed (for all tasks) subset of \(10000\) images images from either CIFAR-10  or MNIST, where either pixels are permuted or the label for each image is randomly chosen. Several papers  study a _memorization_ random-label setting where _SGD_ can perfectly learn each task from scratch. To highlight the data-efficiency of our approach, we study the _data-efficient_ setting where _SGD_ achieves only \(50\%\) accuracy on each task when trained from scratch. Here, we expect that algorithms taking into account similarity in the data, to perform better. To study the impact of the non-stationarity of the input data, we consider _permuted MNIST_ where pixels are randomly permuted within each task (the same task as considered by 34). As baselines, we use _Online SGD_ and _Hard Reset_ at task boundaries. We also consider _L2 init_, which adds \(L2\) penalty \(||-_{0}||^{2}\) to the fixed initialization \(_{0}\) as well as _Shrink&Perturb_, which multiplies each parameter by a scalar \( 1\) and adds random Gaussian noise with fixed variance \(\). See Appendix H.1 for all details. As metrics, we use _average per-task online accuracy_ (52), which is

\[_{t}=_{i=1}^{N}a_{i}^{t},\]

where \(a_{i}^{t}\) are the online accuracies collected on the task \(t\) via \(N\) timesteps, corresponding to the duration of the task. In Figure 5, we also use average accuracy over all \(T\) tasks, i.e.

\[_{T}=_{t=1}^{T}_{t}\]

The results are provided in Figure 2. We observe that _Soft Reset_ is always better than _Hard Reset_ and most baselines despite the lack of knowledge of task boundaries. The gap is larger in the _data efficient_ regime. Moreover, we see that _L2 Init_ only performs well in the _memorization_ regime, and achieves comparable performance to _Hard Reset_ in the _data efficient_ one. The method _L2 Init_ could be viewed as an instantiation of our _Soft Reset Proximal_ method optimizing (17) with \(_{t}=0\) at every step, which is sub-optimal when there is similarity in the data. _Bayesian Soft Reset_ demonstrates significantly better performance overall, see also discussion below.

In Figure 3, we compare different variants of _Soft Reset_. We observe that adding more compute for estimating \(_{t}\) (thus, estimating non-stationarity, \(K_{}=10\)) as well as doing more updates on NN

Figure 3: Different variants of _Soft Resets_. **Left:** performance on _permuted MNIST_. **Center:** performance on _random-label MNIST_ (data efficient). **Right:** performance on _random-label CIFAR-10_ (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (52).

Figure 2: Plasticity benchmarks. **Left:** performance on _permuted MNIST_. **Center:** performance on _random-label MNIST_ (data efficient). **Right:** performance on _random-label CIFAR-10_ (memorization). The x-axis is the task id and the y-axis is the per-task training accuracy (52).

parameters (thus, more accurately adapting to non-staionarity, \(K_{}=10\)) leads to better performance. All variants of _Soft Reset_\(_{t}\) parameters are shared for each NN layer, except for the Bayesian method. This variant is able to take advantage of a more complex _per-parameter_ drift model, while other variants performed considerably worse, see Appendix K.4. We hypothesize this is due to the NN parameters uncertainty estimates \(_{t}\) which Bayesian method provide, while others do not, which leads to a more accurate drift model estimation, since uncertainty is used in this update (10). But, this approach comes at a higher computational cost, see Appendix I. In Appendix K, we provide ablations of the structure of the drift model, as well as of the impact of learning the drift parameter.

**Qualitative behavior of _Soft Resets_.** For _Soft Reset_, we track the values of \(_{t}\) for the first MLP layer when trained on random-label tasks studied above (only \(20\) tasks), as well as the minimum encountered value of \(_{t}\) for each layer, which highlights the maximum amount of resets. Figure 3(b,c) shows \(_{t}\) as a function of \(t\), and suggests that \(_{t}\) aggressively decreases at task boundaries (red dashed lines). The range of values of \(_{t}\) depends on the task and on the layer, see Figure 3(a). Overall, \(_{t}\) changes more aggressively for long duration (memorization) random-label CIFAR-10 and less for shorter (data-efficient) random-label MNIST. See Appendix K.2 for more detailed results.

To study the behavior of _Soft Reset_ under input distribution non-stationarity, we consider a variant of Permuted MNIST where each image is partitioned into patches of a given size. The non-stationarity is controlled by permuting the patches (not pixels). Figure 4(a) shows the minimum encountered \(_{t}\) for each layer for different patch sizes. As the patch size increases and the problem becomes more stationary, the range of values for \(_{t}\) is less aggressive. See Appendix K.3 for more detailed results.

**Impact of non-stationarity.** We consider a variant of random-label MNIST where for each task, an image has either a random or a true label. The label assignment is kept fixed throughout the task and is changed at task boundaries. We consider cases of \(20\%\), \(40\%\) and \(60\%\) of random labels and we control the duration of each task (number of epochs). In total, the stream contains \(200\) tasks. In Figure 4(b), we show performance of _Online SGD_, _Hard Reset_ and in Figure 4(c), the one of _Soft Reset_ and of _Bayesian Soft Reset_. See Appendix H.2 for more details. The results suggest that for the shortest duration of the tasks, the performance of all the methods is similar. As we increase the duration of each of the task (moving along the x-axis), we see that both _Soft Resets_ variants perform better than SGD and the gap widens as the duration increases. This implies that _Soft Resets_ is more effective with infrequent data distribution changes. We also observe that Bayesian method performs better in all the cases, highlighting the importance of estimating uncertainty for NN parameters.

### Reinforcement learning

**Reinforcement learning experiments.** We conduct Reinforcement Learning (RL) experiments in the highly off-policy regime, similarly to , since in this setting _loss of plasticity_ was observed. We ran _SAC_ agent with default parameters from Brax  on the _Hopper_-v5 and _Humanoid_-v4 GYM  environments (from Brax ). To reproduce the setting from , we control the off-policyness of the agent by setting the _off-policy ratio_\(M\) such that for every \(128M\) gradient steps with batch size of \(256\) on the replay buffer. As baselines we consider ordinary _SAC_, hard-coded _Hard Reset_ where we reset all the parameters \(K=5\) times throughout training (every \(200000\) steps), while keeping the replay buffer fixed (similarly to ). We employ our _Soft Reset_ method as follows. After we have collected fresh data from the environment, we do one gradient update on \(_{t}\) (shared for all the parameters within each layer) with batch size of \(128\) on this new chunk of data and the previously collected one, i.e., two chunks of data in total. Then we initialize \(_{t}(_{t})\) and we employ the update rule (47) where the regularization \(_{t}(_{t})\) is kept constant for all the off-policy gradient updates on the replay buffer. See Appendix H.3 for more details.

Figure 4: **Left: the minimum encountered \(_{t}\) for each layer on random-label MNIST and CIFAR-10. Center: the dynamics of \(_{t}\) on the first 20 tasks on MNIST. Right: the same on CIFAR-10.**

The results are given in Figure 6. As the off-policy ratio increases, _Soft Reset_ becomes more efficient than the baselines. This is consistent with our finding in Figure 4(b),c, where we showed that the performance of _Soft Reset_ is better when the data distribution is not changing fast. Figure 8 in Appendix H.3 shows the value of learned \(_{t}\). It shows \(_{t}\) mostly change for the value function and not for the policy indicating that the main source of non-stationarity comes from the value function.

## 6 Conclusion

Learning efficiently on non-stationary distributions is critical to a number of applications of deep neural networks, most prominently in reinforcement learning. In this paper, we have proposed a new method, _Soft Resets_, which improves the robustness of stochastic gradient descent to nonstationarities in the data-generating distribution by modeling the drift in Neural Network (NN) parameters. The proposed drift model implements _soft reset_ mechanism where the amount of reset is controlled by the drift parameter \(_{t}\). We showed that we could learn this drift parameter from the data and therefore we could learn _when_ and _how far_ to reset each Neural Network parameter. We incorporate the drift model in the learning algorithm which improves learning in scenarios with plasticity loss. The variant of our method which models uncertainty in the parameters achieves the best performance on plasticity benchmarks so far, highlighting the promise of the Bayesian approach. Furthermore, we found that our approach is particularly effective either on data distributions with a lot of similarity or on slowly changing distributions. Our findings open the door to a variety of exciting directions for future work, such as investigating the connection to continual learning and deepening our theoretical analysis of the proposed approach.

Figure 5: **(a)** the x-axis denotes the layer, the y-axis denotes the minimum encountered \(_{t}\) for each convolutional and fully-connected layer when trained on permuted Patches MNIST, color is the patch size. The impact of non-stationarity on performance on random-label MNIST of Online SGD and Hard Reset is shown in **(b)** while the one of _Soft Resets_ is shown in **(c)**. The x-axis denotes the number of epochs each task lasts, while the marker and line styles denote the percentage of random labels within each task, circle (solid) represents \(20\%\), rectangle(dashed) \(40\%\), while thrombs (dashed and dot) \(60\%\). The y-axis denotes the average performance (over \(3\) seeds) on the stream of \(200\) tasks.

Figure 6: RL results. First row is humanoid, second is hopper. Each column corresponds to different replay ratio. The x-axis is the number of total timesteps, the y-axis the average reward. The shaded area denotes the standard deviation across \(3\) random seeds and the solid line indicates the mean.