# Evaluating Reward Models for Language Modeling

Nathan Lambert\({}^{}\) Valentina Pyatkin\({}^{}\) Jacob Morrison\({}^{}\)

LJ Miranda\({}^{}\) Bill Yuchen Lin\({}^{}\) Khyathi Chandu\({}^{}\) Nouha Dziri\({}^{}\)

Sachin Kumar\({}^{}\) Tom Zick\({}^{}\) Yejin Choi\({}^{}\) Noah A. Smith\({}^{}\) Hannaneh Hajishirzi\({}^{}\)

\({}^{}\)Allen Institute for Artificial Intelligence

\({}^{}\)University of Washington \({}^{}\)Berkman Klein Center, Harvard Law

contact: nathanl@allenai.org

###### Abstract

Reward models (RMs) are at the crux of successfully using RLHF to align pre-trained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.

## 1 Introduction

Reinforcement learning from human feedback (RLHF) is a necessary but opaque tool underlying the success of popular language models (LMs) such as OpenAI's ChatGPT (Schulman et al., 2022) and Anthropic's Claude (Bai et al., 2022). The prevalence of RLHF stems from its efficacy at circumventing one of the greatest difficulties in integrating human preferences into language models: specifying an explicit reward (Christiano et al., 2017). Reward models (RMs) are central to thisprocess. They are created by copying the original language model and training it on labeled preference data, producing a model that can predict whether one piece of text is likely to be preferred over another. A reinforcement learning optimizer then uses this reward model signal to update the parameters of the original model, improving performance on a variety of tasks (Ouyang et al., 2022; Touvron et al., 2023).

While the post-RLHF model (known as the _policy_) and even the pretrained model are extensively documented and evaluated, the basic properties of the RLHF process like the RMs receive far less attention. Recent work on training reward models (Zhu et al., 2023; Jiang et al., 2023c) has begun to fill this gap, but utilizes validation sets from previous RLHF training processes, such as Anthropic's Helpful and Harmless data (Bai et al., 2022a) or OpenAI's Learning to Summarize (Stiennon et al., 2020), which are known to have ceilings on accuracy between 60 and 70% due to inter-annotator disagreement (Wang et al., 2024). Moreover, newly released preference data aiming to expand the diversity of preference training datasets such as UltraFeedback (Cui et al., 2023), UltraInteract (Yuan et al., 2024a) and Nectar (Zhu et al., 2023a), do not have test sets, necessitating a new style of evaluation for RMs.

We begin to rectify the lack of evaluation methods by introducing RewardBench, the first toolkit for benchmarking reward models. RLHF is a broadly applicable process used to enhance specific capabilities of LMs such as safety (Dai et al., 2023) or reasoning (Lightman et al., 2023; Havrilla et al., 2024a) as well as general capabilities such as instruction following (Ouyang et al., 2022) or "steerability" (Askell et al., 2021; Bai et al., 2022a). Thorough evaluations of RMs will also cover these categories. In this work, we curate data to create structured comparisons across a variety of reward model properties. Each sample is formatted as a prompt with a human-verified chosen and rejected completion. We design subsets so as to vary in difficulty and coverage. Some subsets are solved by small RMs, reaching 100% accuracy, but others are harder to differentiate and still have state-of-the-art performance around 75%, with many models around the random baseline.

We aim to map the current landscape of openly available reward models via a leaderboard for RewardBench. We have evaluated over 80 models, such those trained as classifiers, including UltraRM (Cui et al., 2023), Starling (Zhu et al., 2023a), PairRM (Jiang et al., 2023c), SteamSHP (Ethayarajh et al., 2022), models from Reward rAnked FineTuning (RAFT) (Dong et al., 2023), and others. We also evaluate popular chat models trained with Direct Policy Optimization (DPO) (Rafailov et al., 2023), for example, Zephyr-\(\)(Tunstall et al., 2023), Qwen-Chat (Bai et al., 2023), StableLM (Bellagente et al., 2024), and Tulu 2 (Ivison et al., 2023) to ground recent debates on RLHF methods and showcase specific datasets where they fall short.

With these models, we compare scaling, test reasoning capabilities, highlight three buckets of refusal behavior, and share more details on the inner workings of RMs. The accompanying code-base provides a common inference stack for many variations of models and we release many text-score pairs to analyze their performance. With **RewardBench**, we:

1. Release a common **framework for evaluating the many different architectures of reward models**, along with tools for visualization, training, and other analysis. We also release all data used in the evaluation, composed of text-score pairs for all inputs, to enable further data analysis on the properties of reward models.1 2. Illustrate the **differences between DPO and classifier-based reward models** across a variety of datasets. DPO models, while more plentiful due to the method's simplicity, fail to generalize to popular preference data test sets and present a higher variance in performance.
3. Chart the **landscape of current state-of-the-art reward models**. We showcase the scaling laws, the propensity to refuse (or not), the reasoning capabilities, and more for popular RMs.
4. Show the **limitations of existing preference data test sets** for evaluating these models, showcasing common pitfalls of RMs on subtle, but challenging instruction pairs (e.g. intentionally modified rejected responses, which superficially look high quality but answer the wrong prompt).

## 2 Related Works

Reinforcement Learning from Human FeedbackUsing Reinforcement Learning to align language models with human feedback or preferences (Christiano et al., 2017; Ziegler et al., 2019) has led to improved chat models such as ChatGPT (Schulman et al., 2022) and Llama2 (Touvron et al., 2023). Incorporating human feedback into models in this way has been used to improve summarization (Stiennon et al., 2020; Wu et al., 2021), question answering (Nakano et al., 2021), image models (Lee et al., 2023) and instruction following in general (Ouyang et al., 2022).

RLHF often focuses on aspects of preference, where aspects could be more general concepts like _helpfulness_ or _harmlessness_(Bai et al., 2022), or more fine-grained ones (Wu et al., 2023), among others. In general, RLHF involves training a reward model on preference data collected from crowd-workers (Wang et al., 2024) (or from LM selected responses (Bai et al., 2022)). Given a reward model, a policy can be learned using RL algorithms like PPO (Schulman et al., 2017), which has been shown to work well for language policies (Ramamurthy et al., 2022). Another option is to directly optimize a policy with chosen and rejected pairs, using DPO (Rafailov et al., 2023). Some reward modeling extensions include process reward models (Luo et al., 2023; Lightman et al., 2023) and step-wise reward models (Havrilla et al., 2024), which are primarily used for reasoning tasks.

Reward Model & RLHF EvaluationPreference tuned models can be evaluated using downstream evaluations, for example using AlpacaFarm (Dubois et al., 2024), where LMs are used to simulate human preferences by comparing a model generated output with that of a reference model. The reported metric is the win-rate of the model over the reference model. Similarly, MT-Bench (Zheng et al., 2023), evaluates chatbots on multi-turn conversations that are judged by LMs as proxy for human judgments and Chatbot Arena (Zheng et al., 2023) crowdsources the preferences between

 Category & Subset & N & Short Description \\  Chat & AlpacaEval Easy & 100 & GPT4-Turbo vs. Alpaca 7bB from Li et al. (2023b) \\
**358 total** & AlpacaEval Length & 95 & Llama 2 Chat 70B vs. Guanaco 13B completions \\  & AlpacaEval Hard & 95 & Tulu 2 DPO 70B vs. Davinici003 completions \\  & MT Bench Easy & 28 & MT Bench ratings 10s vs. 1s from Zheng et al. (2023) \\  & MT Bench Medium & 40 & MT Bench completions rated 9s vs. 2-5s \\  Chat Hard & MT Bench Hard & 37 & MT Bench completions rated 7-8s vs. 5-6 \\
**456 total** & LLMBar Natural & 100 & LLMBar chat comparisons from Zeng et al. (2023) \\  & LLMBar Adver. Neighbor & 134 & LLMBar challenge comparisons via similar prompts \\  & LLMBar Adver. GPTInst & 92 & LLMBar comparisons via GPT4 similar prompts \\  & LLMBar Adver. GPTOut & 47 & LLMBar comparisons via GPT4 unhelpful response \\  & LLMBar Adver. Manual & 46 & LLMBar manually curated challenge completions \\  Safety & Refusals Dangerous & 100 & Preferring refusal to elicit dangerous responses \\
**740 total** & Refusals Offensive & 100 & Preferring refusal to elicit offensive responses \\  & XTest Should Refuse & 154 & Prompts that should be refused Röttger et al. (2023) \\  & XTest Should Respond & 250 & Preferring responses to queries with trigger words \\  & Do Not Answer & 136 & Questions that LLMs should refuse (Wang et al., 2023) \\  Reasoning & PRM Math & 447 & Human vs. buggy LLM answers (Lightman et al., 2023) \\
**1431 total** & HumanEvalPack CPP & 164 & Correct CPP vs. buggy code (Muennighoff et al., 2023) \\  & HumanEvalPack Go & 164 & Correct Go code vs. buggy code \\  & HumanEvalPack Javascript & 164 & Correct Javascript code vs. buggy code \\  & HumanEvalPack Java & 164 & Correct Java code vs. buggy code \\  & HumanEvalPack Python & 164 & Correct Python code vs. buggy code \\  & HumanEvalPack Rust & 164 & Correct Rust code vs. buggy code \\   Prior Sets & Anthropic Helpful & 6192 & Helpful split from test set of Bai et al. (2022a) \\
**17.2k total** & Anthropic HHH & 221 & HHH validation data (Askell et al., 2021) \\  & SHP & 1741 & Partial test set from Ethayarajh et al. (2022) \\  & Summarize & 9000 & Test set from Stiennon et al. (2020) \\   

Table 1: Summary of the dataset used in RewardBench. Note: Adver. is short for Adverserial.

two different model outputs. These types of setups only indirectly evaluate the reward model. Other works, directly analyze the reward model, such as Singhal et al. (2023), who found a strong correlation between output length and rewards by looking at the training dynamics of RMs. Another analysis looked at reward inconsistencies, by creating a benchmark of contrasting instructions (Shen et al., 2023). Clymer et al. (2023) study reward model performance under distribution shift.

## 3 Background

Reward ModelingThe first step of training a reward model, and therefore doing RLHF, is collecting preference data from a group of human labelers. Individuals are presented with _prompts_, \(x\), akin to a question or task, and asked to choose between a set of _completions_, \(y_{i}\), answering the request. The most common case is for only two completions to be shown with measurement of preference, such as win-loss-tie or a Likert scale indicating the magnitude of preference between completions (Bai et al., 2022), though other methods for labeling exist, such as ranking in a batch of 4+ answers (Ouyang et al., 2022). The resulting data is transformed into a set of prompt-chosen-rejected trios, where the _chosen_ completion is preferred over the _rejected_ completion for training. Training a reward model involves training a classifier to predict the human preference probability, \(p^{*}\), between two answers, as modeled by a Bradley-Terry model (Bradley and Terry, 1952):

\[p^{*}(y_{1} y_{x} x)=(r^{*}(x,y_{1}))}{(r^ {*}(x,y_{1}))+(r^{*}(x,y_{2}))}.\] (1)

Then, estimate the parameters of the RM by optimizing the maximum likelihood loss as follows: \((,)=_{\{x,y_{},y_{}\}}(1+e^{r_{}(x,y_{})\ -r_{}(x,y_{})})\).For language models, the RM is often implemented by appending a linear layer to predict one logit or removing the final decoding layers and replacing them with a linear layer. At inference time, a trained reward model returns a scalar, such that \(P(y_{1} y_{2} x)^{r(x,y_{1})}\) (which intuitively is the probability that the completion would be a preferred response, but is trained indirectly via the pairwise loss). Thus, a win between completions \(y_{1}\) and \(y_{2}\) is achieved when \(r(x,y_{1})\) > \(r(x,y_{2})\).

Direct Preference OptimizationDirect Preference Optimization solves the RLHF problem without needing to learn a separate reward model. It achieves this by reparameterizing the preference-based reward function using only the policy models (Rafailov et al., 2023) The implicit reward used in DPO is a function of the policy model probabilities (i.e. the model being trained), \((y|x)\), a regularization constant, \(\), the base model probabilities, \(_{}(y|x)\), and a partition function \(Z(x)\):

\[r(x,y)=\,}(y|x)}+\,\ Z(x).\] (2)

Given two completions to a prompt, we compare the rewards \(r(x,y_{1})\) and \(r(x,y_{2})\) as follows, where the score is computed via the log ratios of \(\): \(|x)}{_{}(y_{1}|x)}>|x)}{_{}(y_{2}|x)}\).

Figure 1: The scoring method of the RewardBench evaluation suite. Each prompt is accompanied by a chosen and rejected completion which are independently rated by a reward model.

The RewardBench Benchmark

In this section, we detail the design philosophy and construction of the evaluation dataset. The dataset is designed to provide a broad set of basic evaluations for reward models, covering chat, instruction following, coding, safety, and other important metrics for fine-tuned language models. The RewardBench dataset contains a combination of existing evaluation prompt-completion pairs, and those curated for this project.

A good reward function, and therefore a good RM broadly, is one that stably assigns credit to the classes of good or bad content.Given one verified answer that is better than another for factual or clear qualitative reasons (e.g. typos), a good reward model will choose the correct one 100% of the time. To evaluate this, each datapoint consists of a prompt and two completions, chosen and rejected. For each prompt, the score of the reward model is computed. The prompt is then categorized as a win if the score of the prompt with the verified chosen completion is higher than that of the verified rejected completion, as shown in Fig. 1. Finally, we report accuracy for each subset as the percentage of wins. For all the section scores of RewardBench (e.g. Chat or Safety) except Prior Sets, the average score is weighted per-prompt in the requisite subsets.

### RewardBench Dataset

The benchmark is broken down into five sections from different subsets - the first four compose the RewardBench dataset described in this section. We have broken down the dataset into these subsections to create one final RewardBench score in order to reasonably weigh different aspects of an RM's performance. The RewardBench dataset is released under the ODC-BY license2 and the code is released under Apache 2.03. The summary of the dataset is shown in Tab. 1 (see appendix F for full details) At a high level, the subsets consist of the following:

1. **Chat**: Testing a reward model's basic ability to distinguish a thorough and correct chat response in open-ended generation. Prompts and chosen, rejected pairs are selected from AlpacaEval (Li et al., 2023) and MT Bench (Zheng et al., 2023), two popular open-ended chat evaluation tools.
2. **Chat Hard**: Testing a reward model's abilities to understand trick questions and subtly different instruction responses. Prompts and chosen, rejected pairs are selected from MT Bench examples with similar ratings and adversarial data specifically for fooling LLM-as-a-judge tools from LLMBar's evaluation set (Zeng et al., 2023) (reformatted for RMs).
3. **Safety**: Testing the models' tendencies to refuse dangerous content and to avoid incorrect refusals to similar trigger words. Prompts and chosen, rejected pairs are selected from custom versions of the datasets XSTest (Rotter et al., 2023), Do-Not-Answer (Wang et al., 2023), and examples from an in-development refusals dataset at AI2, where the chosen response is a refusal and the rejected is harmful text of either dangerous or offensive nature.
4. **Reasoning**: Evaluating the models code and reasoning abilities. Code prompts are created by reformatting HumanEvalPack examples with correct code as chosen and rejected as one with bugs (Muennighoff et al., 2023). Reasoning prompts pair reference answers with incorrect model generations from the PRM800k dataset (Lightman et al., 2023).
5. **Prior Sets4**: For consistency with recent work on training reward models, we average performance over test sets from existing preference datasets. We use the Anthropic Helpful split (Bai et al., 2022) (the only multi-turn data), the Anthropic HHH subset of BIG-Bench (Askell et al., 2021), a curated subset of the test set from the Stanford Human Preferences (SHP) Dataset (Ethayarajh et al., 2022), and OpenAI's Learning to Summarize Dataset (Stiennon et al., 2020).

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_FAIL:7]

Evaluating across Chat Hard CategoriesTab. 5 compares different rewards models across Chat Hard categories (full results are shown in Tab. 11). The adversarial subsets from LLMBar (Zeng et al., 2023) are crucial to understanding RMs because they show examples where two answers are written in a similar style (e.g. the same GPT-4 model version), but with slightly different subjects. The difference between asking a factual question about a related but different object or slightly changing the context of a prompt, is hard to pick up with most reward models. The Chat Hard section (and to some extent Reasoning) is largely correlated with final performance, but some DPO models excel at it and not overall - even Qwen Chat and others with low average performance overall. The models scoring highly largely are trained on recent base models and preference datasets, showcasing recent progress on RM training.

Evaluating across Reasoning CategoriesThe Reasoning section of RewardBench has the widest, smooth variation in performance - e.g. models populate many levels, from 35% accuracy (well below random) all the way to 97% accuracy. The reasoning data largely relies on code examples where just one or two tokens are different between the chosen and rejected samples, showcasing precise classification abilities of the best RMs. Full reasoning results are included in Tab. 13.

Evaluating across Safety MetricsTab. 6 (full results in Tab. 12 in Appendix) compares different reward models across different _safety_ categories, indicating challenges on striking a balance between refusing too much or not refusing. Models, such as UltraRM-13b and zephyr-7b-gamma-v0.1

  &  &  \\ Reward Model & Score & Chat & Hard & Safety & Reason & Sets \\    } & HuggingFaceH4/zephyr-7b-alpha & **73.4** & 91.6 & 62.5 & **74.3** & 75.1 & **53.5** \\  & HuggingFaceH4/zephyr-7b-beta & 71.8 & 95.3 & **62.7** & 61.0 & **77.9** & 52.2 \\  & allenai/tulu-2-dpo-7b & 71.7 & **97.5** & 56.1 & 73.3 & 71.8 & 47.7 \\  & allenai/OLMo-7B-Instruct & 66.7 & 89.7 & 50.7 & 62.3 & 71.7 & 51.7 \\  & HuggingFaceH4/zephyr-7b-gamma-v0.1 & 66.4 & 95.8 & 49.6 & 52.9 & 74.6 & 51.7 \\    } & RLHFlow/ArmoRM-Llama3-8B-v0.1 & **89.0** & 96.9 & **76.8** & **92.2** & **97.3** & 74.3 \\  & RLHFlow/pair-preference-model-LLaMA3-8B & 85.7 & 98.3 & 65.8 & 89.7 & 94.7 & 74.6 \\  & faiarXC/sfairX-LLaMA3-RM-v0.1 & 83.6 & **99.4** & 65.1 & 87.8 & 86.4 & 74.9 \\  & openbmb/Eurus-RM-7b & 81.6 & 98.0 & 65.6 & 81.2 & 86.3 & 71.7 \\  & wqweasdas/RM-Mistral-7B & 79.3 & 96.9 & 58.1 & 87.1 & 77.0 & **75.3** \\  

Table 4: Comparing 7B class models. _Top_ shows some Zephyr-style fine-tuned models (Tunstall et al., 2023), showcasing the variance across base models and implementation. _Bottom_ is other top 7B models, trained with various methods and datasets. Icons refer to model types: Sequence Classifier (\(\)), Custom Classifier (\(\)), or DPO (\(\)).

  &  & LLMBar &  \\  Reward Model & Avg. & Hard & Natural & Neighbor & GPTInst & GPTOut & Manual \\    } & RLHFlow/ArmoRM-Llama3-8B-v0.1 & **76.8** & **86.5** & **93.0** & 67.9 & **77.2** & **66.0** & **69.6** \\  & Qwen/0wen1.5-14B-Chat & 70.2 & 67.6 & 71.0 & **83.6** & 62.0 & 46.8 & 71.7 \\  & upstage/SOLAR-10.7B-Instruct-v1.0 & 68.6 & 59.5 & 75.0 & 80.6 & 57.6 & 51.1 & 67.4 \\    } & openbmb/UltraRM-13b & 58.6 & 86.5 & 85.0 & 48.5 & 43.5 & 53.2 & 43.5 \\  & allenai/tulu-2-dpo-13b & 58.3 & 70.3 & 75.0 & 71.6 & 25.0 & 51.1 & 47.8 \\  & berkeley-nest/Starling-RM-34B & 57.2 & 91.9 & 91.0 & 31.3 & 39.1 & 76.6 & 47.8 \\    } & HuggingFaceH4/zephyr-7b-gamma-v0.1 & 49.6 & 83.8 & 74.0 & 44.0 & 17.4 & 53.2 & 45.7 \\  & IDEA-CCNL/Ziya-LLaMA-7B-Reward & 46.5 & 67.6 & 77.0 & 36.6 & 32.6 & 40.4 & 26.1 \\  & berkeley-nest/Starling-RM-7B-alpha & 45.8 & 78.4 & 80.0 & 31.3 & 23.9 & 48.9 & 28.3 \\  

Table 5: Different categories of performance on **Chat Hard**, where only a few models obtain strong results (_top_). _Middle_ shows where some of the top overall reward models land on the subset and _bottom_ shows how some average-overall RMs struggling on this section (performing worse than random). Icons refer to model types: Sequence Classifier (\(\)), DPO (\(\)), and random (\(\)).

show how a model focused on helpfulness without a strong notion of safety will score poorly on the should-refuse subsets of the safety section, but highly on XSTest Should Respond. Other models, namely those at the top of the overall leaderboard, clearly include safety information in the training process _and_ maintain strong performance on trick questions that could induce false refusals (XSTest Should Respond). Finally, the mirrored behavior, those models that score highly on prompts that they should refuse and poorly on those they should not are present, indicating a model that is likely to falsely refusal queries (e.g. the Qwen chat models). These three behavior modes indicate that RewardBench can be used as a quick check of the safety behavior of a candidate model, especially when trained with DPO (as it will not need further RL training like the classifiers).

### Limitations of Prior Test Sets

Many popular models trained with RLHF use new preference datasets such as UltraFeedback (Cui et al., 2023) or Nectar (Zhu et al., 2023a), which don't have publicly available validation sets. Given this, when training reward models, common practice is to compare model agreement with a variety of existing test sets from earlier work in RLHF. Some models scoring strongly on the Prior Sets section of RewardBench, such as UltraRM-13b and PairRM-hf were trained on the training splits of Anthropic HH, Stanford Human Preferences (SHP), and OpenAI's Learning to Summarize, but other top classifier models, such as the Starling models were not. Combining this with the very low average score of DPO models on these test sets indicates that substantial research is needed to understand the full limitations of these previous datasets. Full results are detailed in Tab. 14.

## 6 Conclusion

We present RewardBench, and show the variety of performance characteristics of current reward models in order to improve understanding of RLHF. While we covered a variety of topics important to alignment of LMs, a crucial next step is needed to correlate performance in RewardBench to RLHF usefulness. Initial experiments with ranking RMs with best-of-N sampling and downstream training with PPO are underway. We have taken a first step to understanding which values are embedded in the RLHF training across many base models and preference datasets. The toolkit we have released can easily be expanded include custom data to specifically audit a certain property of the RLHF process. Scores of RMs from private LM providers are on the public leaderboard, but are not in the paper because they are not reproducible. RewardBench is one of many tools which will help us understand the science of whose and what values are embedded in our language models.

    &  &  & Do Not \\  Reward Model & Avg. & Dang. & Offen. & Refuse & Respond & Answer \\   \% RLHFlow/ArmoRM-Llama3-8B-v0.1 \\  } & RLHFlow/ArmoRM-Llama3-8B-v0.1 & 92.2 & 93.0 & 97.0 & 100.0 & 87.2 & 79.4 \\  & Nexusflow/Starling-RM-34B & 88.2 & 84.0 & 97.0 & 97.4 & 93.6 & 61.8 \\  & allenai/tulu-2-dpo-70b & 83.9 & 82.0 & 89.0 & 85.7 & 90.4 & 70.6 \\   \% stabilityai/stablelm-2-12b-chat \\  } & stabilityai/stablelm-2-12b-chat & 82.6 & 93.0 & 95.0 & 91.6 & 56.8 & 78.7 \\  & Qwen/Qwen1.5-14B-Chat & 76.3 & 93.0 & 83.0 & 80.5 & 41.6 & 90.4 \\   \% IDEA-CCNL/Ziya-LLaMA-7B-Reward \\  } & IDEA-CCNL/Ziya-LLaMA-7B-Reward & 60.2 & 39.0 & 69.0 & 61.0 & 90.4 & 33.8 \\  & openbmb/UlraRM-13b & 54.3 & 18.0 & 21.0 & 66.2 & 94.8 & 37.5 \\   & HuggingFaceH4/zephyr-7b-gemma-v0.1 & 52.9 & 25.0 & 61.0 & 51.3 & 92.4 & 25.7 \\   

Table 6: A subset of results for the **Safety** category grouped by behavior type. Top: Example reward models that tend to correctly prefer refusals of sensitive prompts and prefer responding to prompts with potential trigger words. Middle: Example reward models that have a propensity to choose a refusal for every request, including those that should be responded to. Bottom: Example reward models that have a propensity to choose a compliance to every request, even those that should be refused. Model types: Sequence Classifier (\(\)), Custom Classifier (\(\)), and DPO (\(\)).