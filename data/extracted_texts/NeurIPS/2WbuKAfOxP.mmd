# The Scandinavian Embedding Benchmarks:

Evaluating Multilingual and Monolingual Text

Embedding for Scandinavian languages

 Kenneth Enevoldsen

Aarhus University

kenneth.enevoldsen@cas.au.dk

&Marton Kardos

Aarhus University

martonkardos@cas.au.dk

Niklas Muennighoff

n.muennighoff@gmail.com

&Kristoffer Laigaard Nielbo

Aarhus University

kln@cas.au.dk

###### Abstract

The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB1 and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.

## 1 Introduction

Natural language embeddings are used in a diverse range of applications, including clustering (Liu and Xiong, 2011; Angelov, 2020), text mining (Jiang et al., 2015), semantic search (Reimers and Gurevych, 2019; Muennighoff, 2022) and feature representation (Alayrac et al., 2022). Furthermore, embeddings are crucial in retrieval augmented generation (RAG) systems (Borgeaud et al., 2022), particularly for low- to mid-resource languages and domains. RAG systems enable the enrichment of generative models with the knowledge that might be underrepresented or absent during training. Thus, they can play a role in broadening linguistic and domain coverage.

With the breadth of applications for text embeddings, a proper evaluation of their quality is critical. Recent work has proposed Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023), a benchmark for evaluating the quality of document embeddings for a wide variety of tasks. MTEB improves upon prior benchmarks by addressing the lack of evaluations across tasks. This has led to the widespread adoption of the benchmark for evaluating natural language embeddings.

However, while MTEB substantially improves the evaluation of text embeddings, the benchmark has the following shortcomings:

1. **Support for non-English evaluation:** MTEB contains only limited support for evaluating non-English embeddings and multiple task categories are predominantly covered bytranslated datasets (classification) and important task such as retrieval has no multilingual support.
2. **Reproducibilty:** MTEB does not include model implementations in the benchmark's code2. This is especially problematic since recent approaches such as prompt-based embedding models (Muennighoff, 2022; Xiao et al., 2023; Su et al., 2023), Matryoshka embeddings (Kusupati et al., 2022) introduce variables which can dramatically influence performance. 3. **Coverage:** While MTEB has broad coverage across tasks, its domain coverage is still limited, as it primarily includes datasets from academic articles, social media, and web sources. This lack of coverage is especially pronounced for non-English tasks.

Our work is driven by the reality that Scandinavian research, public institutions, and industry have to make decisions about their choice of text embedding model for various use cases. These choices are currently made in the absence of a reliable standard to evaluate text embedding models' performance on Scandinavian languages. As a result, these institutions have relied on proxies, such as models' performance on predominantly English benchmarks or Bitext mining tasks. As we demonstrate, performance on these tasks is not necessarily transferable to Scandinavian applications, thus not properly accounting for these institutions' requirements. By introducing a benchmark tailored for Scandinavian languages, we aim to aid these organizations in making informed decisions. Additionally, SEB will presumably support the development of Scandinavian embedding models by providing a standardized means for evaluating new models and comparing them against previously existing ones.

### Contributions

To mitigate these issues, we present SEB a benchmark for embedding evaluation of the Mainland Scandinavian languages: Danish (da), Swedish (sv), and Norwegian (Bokmal (nb) and Nynorsk (nn)) as well as the Danish dialect Bornholmsk (da-bornholm). Due to the limited resources available for these languages we choose to utilize the substantial cross-lingual transfer between these languages demonstrated by Nielsen (2023); this supports collectively benchmarking the Mainland Scandinavian languages to broaden the coverage otherwise limited for these languages.3 SEB makes the following main contributions; 1 it greatly expands the evaluation of embedding for Scandinavian to multiple tasks (see Table 0(b)) as well as across a wide range of domains (see Table 0(a)); 2 SEB implements a model registry that allows for the easy addition of new models as well as documents the exact implementation of existing models evaluated in the benchmark. Lastly, 3 SEB expands and extends MTEB by porting all tasks, allowing for the expansion of MTEB to a fully-fledged multilingual benchmark for embeddings. Using SEB we evaluate 26 representative models and APIs within this work and present additional models in an interactive online dashboard.4

## 2 Related Work

### Benchmarks

Benchmarks are important tools for model development that enable the assessment of significant performance improvements. Prior benchmarks for evaluating text embeddings focused on specific embedding qualities; BEIR (Thakur et al., 2021) and MIRACL (Zhang et al., 2023) assessed embedding efficacy in information retrieval across diverse domains or languages, while SentEval (Conneau and Kiela, 2018) integrated various SemEval datasets for sentence encoding evaluation using semantic text similarity (STS) tasks. MTEB (Muennighoff et al., 2023) amalgamated and expanded these methodologies to cover eight different tasks. While MTEB includes more than 112 languages, most of this linguistic variation originates from only a handful of tasks, notably bitext mining (Tatoeba Project Contributors, 2023) or translated datasets (FitzGerald et al., 2023). Scandinavian languages

[MISSING_PAGE_EMPTY:3]

**b) Cultural integrity and model equity:** Recent studies (Berdicevskis et al., 2023; Nielsen, 2023; Muennighoff et al., 2023) have increasingly adopted the strategy of leveraging translated English datasets as a means to evaluate the performance of models in low-resource language contexts. However, we avoid adding such translations, aiming to represent Scandinavian contexts accurately and mitigate the risk of artificially inflating multilingual model capabilities. This decision stems from the recognition that multilingual models, often trained on parallel or translated data (Reimers and Gurevych, 2020), may exhibit inflated performance when evaluated on similarly translated tasks -- a hypothesis that, while plausible, remains to be conclusively shown. We choose to keep the existing translated datasets from MTEB within SEB to maintain compatibility.

**c) Cross-lingual generalization:** Given the limited availability of datasets for the Scandinavian languages, we rely on the high degree of cross-lingual transfer (Nielsen, 2023) to estimate model performance more accurately. This approach capitalizes on intrinsic linguistic similarities and shared cultural contexts to bridge data gaps.

**d) Reproducibility and Accessibility:** SEB expands upon the reproducibility of MTEB by including a model registry for all evaluated models to ensure the exact method (e.g., model prompts) for obtaining the results is known. Furthermore, to ensure that the benchmark is as widely accessible as possible, we have limited the size of most datasets to a maximum of 2048 examples. For most models, this allows running the benchmark on a consumer-grade laptop while ensuring proper performance estimation. The benchmark also implements a public cache, allowing users to experiment without needing to rerun models run by others.

In addition to these criteria, SEB follows the desiderata outlined by Muennighoff et al. (2023), allowing for easy extension of the benchmark and providing a simple API and command-line interface making it easy to benchmark models that are not part of SEB by default.

### Datasets

We present an overview of the tasks in SEB in Figure 1. Additionally, we have created an overview of the datasets in Table 6, including dataset statistics and a short description of each dataset. Subsection A.4 described the method of evaluation, and Subsection A.5 described the formalization of the specific datasets to the task. SEB seeks to cover a large variety of domains and task types, greatly expanding upon what was previously available for non-English languages within MTEB (see Table 0(a) and 0(b)). To allow for the exploration, we add an embedding map of samples from the dataset in Subsection A.3, where it is clearly seen that the datasets occupy different clusters. Similarly, Figure 2 reveals distinctly different clusters of datasets, e.g., the high similarity between SNL Retrieval and NorQuad as both are constructed from encyclopedic sources while distinct datasets such as SweFAQ (Berdicevskis et al., 2023), covering FAQ related to the public sector.

## 4 Methodology

We describe the construction of the datasets in Subsection A.5. To keep our benchmark compatible with MTEB we follow a similar approach for computing scores, these are described in Subsection A.4.

### Models

For our benchmarked models, we have chosen a series of representative models seeking to cover a range of model architectures, model sizes, and commercial APIs, as well as models claiming state-of-the-art results on various embedding tasks. In addition, the online dashboard includes additional models not represented here. We group the models into self-supervised and supervised methods.

**Self-supervised methods:**

**Encoders** such as BERT models (Devlin et al., 2019) including monolingual or Scandinavian models trained for Danish (Enevoldsen et al., 2023), Norwegian (Kummervold et al., 2021) and Swedish (Rekathati, 2021) as well as the multilingual model XLM-R (Conneau et al., 2020). We also include a SimCSE (Gao et al., 2021) version of the dfm-encoder-large to indicate the potential performance

Table 1: Coverage on Mainland Scandinavian languages. The green plus (+) indicates newly added, while ”++” indicates previously not covered in MTEB by any language. The parenthesis is due to the LCC Nielsen (2016) containing the domains, but only to a limited extent. Black checks (✓) indicate domains covered in MTEB for Scandinavian Languages, though only within translated datasets. The domains follow the categorization of the Universal Dependencies Nivre et al. (2017).

Table 1: Coverage on Mainland Scandinavian languages. The green plus (+) indicates newly added, while ”++” indicates previously not covered in MTEB by any language. The parenthesis is due to the LCC Nielsen (2016) containing the domains, but only to a limited extent. Black checks (✓) indicate domains covered in MTEB for Scandinavian Languages, though only within translated datasets. The domains follow the categorization of the Universal Dependencies Nivre et al. (2017).

Figure 2: Dataset similarity between the datasets included within SEB. Embeddings are obtained by applying the embed-multilingual-v3.0 on 100 randomly sampled documents. Similarity is computed using cosine similarity.

gain by self-supervised pre-training. This model is trained on sentences extracted from the Danish Gigaword (Stromberg-Derczynski et al., 2021) using default parameters5.

As a candidate for **Static Word Vectors**, we include four fastText (Joulin et al., 2016, 2017; Bojanowski et al., 2017) models for Danish, Swedish, and Norwegian Bokmal and Nynorsk respectively.

**Supervised Methods:**

For **encoders**, we benchmark LaBSE (Feng et al., 2022), which is based on BERT but further pre-trained on a parallel corpus. Further, we evaluate the multilingual MiniLM models and MPNet models (Reimers and Gurevych, 2019; Song et al., 2020; Wang et al., 2021), which are trained on diverse datasets. We also include the SONAR models (Duquenne et al., 2023) as they claim improved performance over LabSE. In addition, we include the Swedish sentence transformers (Rekathati, 2021) trained with knowledge distillation from an English model (Reimers and Gurevych, 2020).

    & & &  &  \\  & Avg. rank & Avg. & Bitext & Class. & Clust. & Retr. & da & nb & nn & sv \\  Num. Datasets (\(\)) & 24 & 24 & 2 & 12 & 3 & 7 & 12 & 11 & 3 & 9 \\    \\  dfm-encoder-large & 23 (19-26) & 41.4 & 46.8 & 56.5 & 26.9 & 20.1 & 47.7 & 47.4 & 72.5 & 43.7 \\ + SimCSE & 19 (16-22) & **46.6** & 50.9 & 58.4 & 26.9 & **33.7** & **52.2** & 51.3 & 74.3 & 42.0 \\ xlm-roberta-large & 25 (21-30) & 35.3 & 19.1 & 54.6 & 28.1 & 10.0 & 39.6 & 41.3 & 58.0 & 44.5 \\ nb-bert-large & **17** (13-20) & 46.0 & 47.3 & **59.3** & **35.7** & 27.3 & 46.8 & **57.2** & **80.4** & **50.2** \\ nb-bert-base & 21 (18-25) & 42.1 & **51.0** & 57.0 & 31.8 & 18.4 & 43.6 & 53.0 & 79.2 & 47.7 \\ bert-base-swedish & 28 (24-31) & 35.2 & 39.1 & 49.7 & 26.2 & 13.2 & 34.0 & 41.1 & 62.2 & 43.6 \\ fasttext-cc-da & 32 (29-34) & 37.3 & 42.4 & 48.8 & 21.8 & 22.7 & 39.0 & 43.2 & 66.4 & 38.7 \\ fasttext-cc-nn & 32 (29-35) & 35.8 & 47.6 & 46.2 & 22.1 & 20.4 & 34.6 & 43.9 & 69.1 & 37.1 \\ fasttext-cc-cb & 30.0 (27-32) & 37.5 & 43.2 & 48.7 & 24.2 & 22.2 & 37.5 & 45.6 & 67.7 & 38.9 \\ fasttext-cc-sv & 31.5 (29-34) & 36.0 & 43.3 & 47.3 & 22.0 & 20.4 & 34.9 & 41.3 & 63.4 & 40.6 \\   \\  multilingual-MiniLM-L12 & 20 (17-23) & 50.0 & 51.0 & 53.7 & 31.7 & 51.1 & 49.9 & 52.7 & 58.3 & 50.3 \\ multilingual-mpnet-base & 16 (13-20) & 53.2 & 52.7 & 56.5 & 32.7 & 56.5 & 53.0 & 55.8 & 59.6 & 53.3 \\ labSE & 18 (15-21) & 50.5 & 69.1 & 53.6 & 29.0 & 48.9 & 50.9 & 52.9 & 59.4 & 48.7 \\ sentence-bert-swedish & 23 (19-26) & 46.6 & 43.3 & 51.0 & 35.6 & 44.6 & 43.2 & 48.2 & 62.7 & 54.7 \\ e5-mistral-7b-instruct & **8.7** (6.8-12) & 60.4 & **70.8** & 61.7 & 35.7 & 66.0 & **61.7** & 62.9 & 68.8 & 60.4 \\ multilingual-e5-large & 8.8 (6.0-12) & **60.7** & 60.1 & **62.5** & 34.2 & **69.1** & 61.1 & **63.1** & **73.9** & **62.8** \\ multilingual-e5-base & 12 (9.4-15) & 57.9 & 61.4 & 60.1 & 34.0 & 63.5 & 58.6 & 60.9 & 72.0 & 58.5 \\ multilingual-e5-small & 14 (11-16) & 56.4 & 61.6 & 58.1 & **36.9** & 60.3 & 56.5 & 58.9 & 69.5 & 57.1 \\ translate-e5-large & 21 (18-24) & 47.7 & 50.7 & 54.7 & 27.3 & 43.4 & 49.0 & 50.1 & 59.2 & 59.2 \\ sonar-dan & 23 (20-26) & 43.4 & 70.5 & 53.5 & 19.6 & 28.6 & 48.3 & 46.0 & 63.7 & 42.9 \\ sonar-nob & 25 (21-28) & 41.5 & 63.2 & 52.9 & 18.5 & 25.6 & 45.2 & 45.9 & 64.7 & 42.4 \\ sonar-nno & 25 (22-28) & 41.5 & 65.5 & 52.8 & 17.3 & 25.7 & 45.5 & 45.1 & 63.2 & 42.6 \\ sonar-swe & 24 (21-27) & 42.8 & 70.7 & 52.9 & 19.4 & 27.6 & 47.1 & 45.4 & 63.1 & 42.9 \\   \\  text-embedding-3-large & **5.8** (3.3-8.2) & **65.0** & **68.8** & 63.5 & 38.7 & **77.9** & **63.7** & **69.0** & **74.7** & **65.5** \\ text-embedding-3-small & 9.4 (7.7-12) & 61.0 & 66.7 & 59.7 & 38.3 & 71.3 & 59.7 & 64.7 & 70.2 & 60.4 \\ embed-multilingual-v3.0 & 6.1 (3.8-8.9) & 64.1 & 64.2 & **63.6** & **40.2** & 75.2 & 62.6 & 68.5 & 74.1 & 64.3 \\   

Table 2: Performance across task-type categories and languages in SEB. The best score in each model category is highlighted in bold. Additional model evaluation can be found on the public Dashboard. Rank is calculated across all models within the benchmark. The brackets indicate the 95% confidence interval, obtained by bootstrapping 100 repetitions with tasks to minimize the impact of any single task. The symbol ”*” signifies when the top-performing model significantly outperforms the second-best model within the same category at a 0.05 significance threshold. Ranks are reported using two significant figures.

Because the development of Scandinavian **decoders** is only in its early stages (Enevoldsen et al., 2023; Ekgren et al., 2022), we utilize the e5-mistral model (Wang et al., 2022, 2023) as it presents a competitive model in the category.

**Commercial embedding APIs:** We additionally include the embedding APIs of Cohere 6 and OpenAI 7 to compare openly available models with commercial solutions.

Lastly, we add **Translate and embed** as a baseline model for comparing naively translating to English and then embedding with high-quality English models. To allow for comparison with multilingual models, we include both the large English e5 model and all sizes of its multilingual variants (Wang et al., 2022). We use the multilingual M2M100 model (Fan et al., 2020) for the translation. For translation, we assume the language is known. This avoids accumulating errors due to language detection, and in many applications, the language would be known. We assume Danish as the origin for tasks requiring multiple languages, such as bitext mining.

## 5 Results

In Table 2, we see that the best-performing model is either of the commercial APIs of OpenAI and Cohere followed by the publicly available multilingual e5 model series (Wang et al., 2022). This stands in contrast to developments observed from ScandEval (Nielsen, 2023), where notably smaller monolingual or Scandinavian models have proven to be competitive, often surpassing significantly larger multilingual models. Similar to MTEB (Muennighoff et al., 2023), we find a pronounced performance between self-supervised methods and their supervised counterparts, although we see that notable gains can be obtained from unsupervised pre-training (Gao et al., 2021). In general, however, utilizing unsupervised contrastive pretraining pales in comparison to popular multilingual models of smaller size.

In Table 5, we see the performance across domains. Generally, we see that model rankings remain relatively stable across these domains, with the e5 models (Wang et al., 2022) and the commercial APIs taking a consistent lead. However, we also see that in domains such as the legal domain, spoken language, and fiction, we see the e5-mistral-7b-instruct outcompeting commercial solutions.

If we examine individual subtask (see Subsection A.8) Pretrained encoders perform surprisingly well on language acceptability and language detection tasks. This is likely due to a trade-off between semantics and syntax. Self-supervised training on natural language will likely assign significance to syntactic nuances, while models trained on semantic tasks ignore some syntactical errors favoring semantics.

**Performance across task-types:** Models that have been contrastively trained on sentence pairs or finetuned for a set of common tasks typically outperform pre-trained models, especially in retrieval contexts, while LaBSE (Feng et al., 2022) and the SONAR models (Duquenne et al., 2023), which has been designed for bitext-mining purposes, excels at the task.

The largest gap between commercial and public models is in retrieval, where performance drops more than eight points. While notable improvements have been achieved in publicly available embedding models for English retrieval tasks (Wang et al., 2023), similar results are yet to be achieved in multilingual contexts. Bitext mining is the only category in which open solutions outperform commercial solutions.

**Translate then embed:** When comparing the 'translate-then-embed' model against the multilingual e5 models, we see that in almost all cases, the multilingual models perform better even when comparing across size categories. While performance could likely be improved by utilizing state-of-the-art embedding and translation models, we see few benefits to this approach due to increased computational costs, model complexity, and competitive approaches for knowledge distillation across languages (Reimers and Gurevych, 2020).

### Efficiency

We examine the trade-offs between performance and speed in Figure 3. Speed was benchmarked on Dell PowerEdge C6420 Intel(R) Xeon(R) Gold 6130 CPUs with 32 cores/CPU. We see the following categories of relevance;

**Highest Throughput** FastText models offer the highest throughput while maintaining an average performance exceeding that of the multilingual XLM-R (Conneau et al., 2020).

**Maximum Performance** Achieving optimal performance is possible with the multilingual-e5-large or the e5-mistral-7b-instruct, which rivals the smaller commercial embedding APIs.

**Balanced Performance:** The best balance between performance, throughput, and embedding size is seen in the multilingual e5 models series, which performs competitively on the benchmark. The multilingual-mpnet-base also offers a competitive balance between throughput and performance, despite its larger embedding size.

### Comparison with MTEB

As seen in Table 3, where we compare our results with those of MTEB we see that English-focused models (e5-size) perform significantly worse on SEB, while we see the inverse for multilingual models. The e5-mistral-7b-instruct model, based on the multilingual Mistral 7b Jiang et al. (2023), shows notably poorer performance on non-English data, likely due to Mistral's not being trained on Scandinavian languages. While English has several open-source models that perform on par with APIs, this trend is not as evident for Scandinavian languages. Among multilingual models, MTEB and SEB rankings generally align, though selecting models for Scandinavian languages based solely on MTEB results could lead to suboptimal choices (e.g., e5-mistral-7b-instruct, LaBSE).

### Limitations and Future Perspectives

**Domain Coverage**: Despite the advancements introduced by SEB, the benchmark could further benefit from encompassing domains crucial to the welfare states of Scandinavia, such as legal,

Figure 3: Performance and speed of embeddings models. The size of the circles denotes the embedding size, and the color denotes the model type. Note that commercial APIs are not included. WPS stands for words per second. We avoid highlighting all models to improve readability.

governmental, and medical fields, which are currently only partly covered or unaddressed. Current tasks predominantly feature non-fiction literature, such as encyclopedias and news, yet the rising interest in digital humanities (Su et al., 2020) suggests the inclusion of fiction, poetry, historical texts, and religious documents in future updates could be valuable. Additionally, the benchmark currently lacks some task categories, such as pair classification and document reranking.

**Future Directions:** While this work announces the release of SEB, we plan to continually expand upon the benchmark. As this work continues to develop, we invite researchers to join us in expanding the evaluation of embedding models across a broad range of languages.

**Effect of Training Data:** While we explore the influence of the learning objective on model performance, its important to acknowledge the significant role of training datasets. For example, the e5-mistral-7b-instruct model performs similarly to the much smaller multilingual-e5-large, likely due to Mistrals pre-training dataset reportedly not containing Scandinavian languages. However, the lack of transparency around the training datasets of many models means that such claims remain speculative. Future research should aim to investigate the impact of training data more thoroughly.

## 6 Conclusion

In this work, we introduced SEB, a framework that addresses the evaluation gap for the mainland Scandinavian languages. SEB encompasses 24 tasks covering ten subtasks in four task categories and spanning mainland Scandinavian languages.

We evaluate more than 50 models on SEB and show that there is still a notable gap in performance between publicly available text embedding models and their commercial counterparts, especially in retrieval contexts, as well as between monolingual and multilingual models. These findings highlight critical areas for future advancements. By open-sourcing SEB and integrating it with MTEB, we aim to encourage the development of robust Scandinavian and multilingual embedding models, inviting the research community to contribute to this evolving landscape.