# Dyadic Learning

in Recurrent and Feedforward Models

 Rasmus Kjaer Hoier

Chalmers University of Technology. Work begun during internship at Microsoft Research.

Kirill Kalinin

Microsoft Research

Maxence Ernoult

RAIN AI

Christopher Zach

Chalmers University of Technology

###### Abstract

From electrical to biological circuits, feedback plays a critical role in amplifying, dampening and stabilizing signals. In local activity difference based alternatives to backpropagation, feedback connections are used to propagate learning signals in deep neural networks. We propose a saddle-point based framework using dyadic (two-state) neurons for training a family of parameterized models, which include the symmetric Hopfield model, pure feedforward networks and a less explored _skew-symmetric Hopfield variant_. The resulting learning method reduces to equilibrium propagation (EP) for symmetric Hopfield models and to dual propagation (DP) for feedforward networks, while the skew-symmetric Hopfield setting yields a new method with desirable robustness properties. Experimentally we demonstrate that the new skew-symmetric Hopfield model performs on par with EP and DP in terms of the resulting model predictive performance, while exhibiting enhanced robustness to input changes and strong feedback and is less inclined to neural saturation. We identify the fundamentally different types of feedback signals propagated in each model as the main cause of differences in robustness and saturation.

## 1 Introduction

It has been proposed that the brain may perform some instance of approximate backpropagation, using neural activity differences to represent error signals [1; 2]. Activity difference based approaches come in two main flavours: (i) using _temporal_ differences between activities at different times to represent error signals [3; 4; 5], or (ii) using _spatial_ differences by subtracting activities of distinct neurons or neuronal compartments [6; 7; 8]. Temporal activity difference methods require at least two relaxations to equilibrium, or infinitely many through slow adiabatic oscillations of the _same_ circuit , yet at the expense of latency. Conversely, spatial activity difference methods require neurons to multiplex by propagating activities and finite difference error signals simultaneously and some degree of weight sharing, which may be problematic from a biological perspective.

In this manuscript, we take a closer look at the temporal activity difference based algorithm equilibrium propagation (EP)  for symmetric Hopfield models (HM), the spatial activity difference based algorithm dual propagation (DP) for feedforward models (FFM) , and the fundamental role feedback plays in each of the algorithms. In the context of this work, feedback refers to both the error signal propagated in a supervised learning setting and to top-down connections in a layered network architecture (and therefore introducing cycles in the underlying computational graph). In DP, the feedback signals reduce to top-down (finite difference) errors due to the lack of recurrent connections in feedforward models. While this is an asset for inference speed in practice, recurrent connections are a key feature underpinning attention and robustness in biological systems. On theother hand, in EP (and Hopfield nets in general) _positive_ feedback is always present: the sign of the signal send from neuron A to neuron B always matches the sign of the signal sent from B to A. This is a consequence of the symmetry of the underlying Hopfield model (\(W_{ij}=W_{ji}\)). Positive feedback connections in symmetric Hopfield nets yield many desirable properties, but may also lead to excessive amplification of neural activities. This is not too surprising as high sensitivity to input changes and excessive saturation are known features of positive feedback .

As FFMs without an explicit error signal lack any kind of feedback from downstream layers and HMs suffer from the aforementioned limitations of persisting positive feedback, it is natural to ask what a _negative_ feedback model would look and behave like? As a symmetric Hopfield model results in positive feedback, a _skew-symmetric Hopfield model_ (SSHM), with with \(W_{ij}=-W_{ji}\), would instead provide negative feedback. Negative feedback loops are typically used to decrease sensitivity to input perturbations and increase the linearity of a system  which could respectively endow the resulting model with a degree of biological plausibility and alleviate vanishing gradient issues during training.

We establish a family of models and their associated learning dynamics, which recover as special cases symmetric Hopfield models (HM), feedforward models (FFM) and skew-symmetric Hopfield models (SSHM). In the HM setting, the neural state and synaptic weight dynamics correspond to EP, and the FFM setting results in DP. The SSHM scenario yields a two-phased algorithm which exhibits reduced sensitivity to input perturbations and also to decreased neural saturation.

While the role of feedback in the brain is much more diverse and subject to constraints we are not modelling (such as weight transport and, Dale's principle) we hope this study helps bring attention to the importance of negative feedback in the context of learning. The importance of negative feedback is well known in both neuroscience and control theory, yet in the context of biologically inspired learning algorithms negative feedback has received little to no attention.

## 2 Related work

Contrastive learning algorithms.In energy-based models, inference amounts to energy minimization. Training can in this setting be achieved by contrasting the states inferred when applying different perturbations to the output units. In variations of contrastive Hebbian learning (CHL) [3; 11; 5], this is done by clamping output units. In EP, the output units are instead weakly nudged by a teaching signal derived from a loss function. EP and CHL typically assume a symmetric connectivity, which for instance naturally map to resistive networks where top-down currents inherently flow through the weights transpose [12; 13; 14; 15].

Credit assignment with dual compartments.DP introduces "dyadic" neurons, which permits representing errors and activities simultaneously as the difference and mean of each neurons compartments, which in terms permits turning the two distinct inference phases of CHL or EP into a single phase. The underlying motivation is conceptually similar, albeit different in terms of resulting algorithms, to the neuroscience inspired approximations of backpropagation [16; 7] which achieve single-phased learning by modelling pyramidal neurons with distinct compartments for integrating top-down and bottom-up signals.

Lifted neural networks.Lifted neural networks have roots in the community of mathematical optimization and frame neural network training as bilevel optimization over weights and activations [17; 18; 19; 20; 21; 22]. Although predating the term _lifted network_ also fits in this category. As in variants of CHL, neurons in lifted neural networks rely only on local information and require less synchronization than backpropagation. In the context of lifted networks robustness to input perturbations has previously been explored in .

## 3 A Saddle point objective for dyadic learning

We let subscript \(k\) denote row index and subscript \( k\) denote the removal of the \(k^{}\) element from a vector. Starting from the optimization problem

\[_{W}(s,y)\ \ \ s_{k}=*{ arg\,min}_{s^{}_{k}}E_{k}(s^{}_{k},s_{ k},W_{k,  k},_{0})\] (1)we derive the following relaxation (see details in appendix A).

\[_{W}_{s^{+}}_{s^{-}}(s^{+},y)+(s^{-}, y)+_{k}E_{k}(s^{+}_{k},_{ k})-E_{k}(s^{-}_{k}, _{ k})\] (2)

In the case of a Hopfield-like \(E_{k}\) this correponds to performing gradient descent on the following objective with respect to the weights.

\[(_{0},W) =_{s^{+}}_{s^{-}}(s^{+},y)+(s^{-},y)\] (3) \[+G(s^{+})-G(s^{-})-(s^{+}-s^{-})^{}_{0}x- s^{+}\\ s^{-}^{}W+W^{}&W-W^{}\\ -W+W^{}&-W-W^{}}_{:=}s^{+}\\ s^{-}\]

Here \(_{0}\) is an projection mapping datapoints \(x\) to the space of neural activities \(s\), while \(W\) governs interactions between elements of \(s\). The structure of \(W\) has profound impact on how this optimization can be physically realized. The general case of an entirely unstructured weight matrix \(W\) requires neurons to perform substantial multiplexing, making it challenging for physical implementations. Choosing instead to parametrize \(W_{}():=+(2-1)^{}\), where \(\) is lower triangular, allows smoothly interpolating from symmetric Hopfield models, through feedforward models, to skew-symmetric Hopfield models (each of which offer certain benefits for physical implementations). Employing this parametrization yields the following objective.

\[_{}(_{0},) =_{s^{+}}_{s^{-}}(s^{+},y)+(s^{-},y)\] (4) \[+G(s^{+})-G(s^{-})-(s^{+}-s^{-})^{}_{0}x- s^{+}\\ s^{-}^{}(+^{ })&(-^{})\\ (-+^{})&(--^{}) }_{:=_{}}s^{+}\\ s^{-}\]

In the following we will restrict our focus to the more hardware friendly settings offered by \(_{}\).

Dynamics.Training amounts to solving \(_{}_{}()\). Let the accent \(*\) denote optimality, then the gradient of \(_{}()\) with respect to \(\) is

\[_{}_{}()=- {2}(^{+}^{+}-^{-}^{-})+}{2}(^{-}^{+}-^{+}^{-})\] (5)

As \(_{}\) is already symmetric, the gradient with respect to \(s^{}\) is simple to compute

\[ s^{+}_{}\\  s^{-}_{}=_{s^{+}} (s^{+})\\ _{s^{-}}(s^{-})+(f^{ -1}(s^{+})\\ -f^{-1}(s^{-})-_{}s^{+}\\ s^{-}-_{0}x\\ -x).\] (6)

and the stationary conditions for \(^{}\) are

\[^{}=f_{0}x+W_{}(^{+}+^ {-})W_{}^{}(^{+}-^{-}) (^{})\,.\] (7)

We determine \(^{}\) using dampened fixed-point iterations (see appendix B for details) of the preactivations \(a^{}\) (defined through the relation \(s^{}=f(a^{})\)) of the form

\[a^{}(1-)a^{}+(( s^{}+s^{})+^{}( s^{}-s^{})+_{0}x (s^{})).\] (8)

In the remaining part of the paper we will further restrict \(\) to be strictly lower block triangular (rather than just strictly lower triangular). We write this in terms of layerwise weight matrices \(_{k}\).

\[=(0&0&&0\\ _{1}&&0\\ 0&_{2}&&0\\ &&&0)\] (9)

This approach is commonly applied to obtain layered Hopfield networks. In this setting we get mirror descent dynamics for the hidden layers of the form

\[a^{}_{k}(1-)a^{}_{k}+(_{k-1}( s^{ }_{k-1}+s^{}_{k-1})+^{}_{k}( s^{}_{k+1}- s^{}_{k+1})),\] (10)

where the subscript \(k\) now denotes layer index. When \(=1\) this reduces to layer-wise closed form updates akin to those employed in EP .

Recovering symmetric Hopfield models trained with EP.In the case \(=1\) and (i.e. \(=0\)) then the saddle-point problem equation 4 decouples into two separate minimization problems over \(s^{+}\) and \(s^{-}\), respectively. This corresponds to the two phases of EP in a deep Hopfield network, yielding the hidden layer dynamics:

\[a_{k}^{}(1-)a_{k}^{}+(_{k-1}s_{k-1}^{}+ _{k}^{}s_{k+1}^{})\] (11)

In a hardware realization, this means the two sub-problems can be either solved in parallel (which requires maintaining two identical sets of weights) or sequentially using the same hardware (which avoids weight doubling, but increases runtime). Figure 0(a) and 0(b) illustrates the connectivity matrix \(_{=1}\) for a small network.

Recovering feedforward models trained with DP.The choice \(==1/2\) recovers the DP dynamics.

\[a_{k}^{}(1-)a_{k}^{}+(_{k-1}(s _{k-1}^{}+s_{k-1}^{})+_{k}^{}(s_{k+1}^{}-_{k+1}^{}))\] (12)

In the absence of a teaching signal (\( 0\)) then \(s_{k}^{+}=s_{k}^{-}\) for all \(k\), hence the feedback term \(_{k}^{}(s_{k+1}^{}-_{k+1}^{})\) vanishes and the model behaves exactly like a feedforward model. For hardware implementations it is noteworthy that the two compartments \(s_{k,i}^{+}\) and \(s_{k,i}^{-}\) of a neuron \(i\) in layer \(k\) receive the same bottom up and the same absolute feedback signal (but opposite sign), which allows them to share weights in physical realizations.

Skew-symmetric Hopfield modelsFixing \(=0\) (i.e. \(=1\)) implies that \(W_{=0}\) is skew-symmetric (see Fig. 0(c)-d), and only the blocks governing interactions between \(s^{+}\) and \(s^{-}\) remain in \(_{}\). The dynamics in the layered setting we consider here are

\[a_{k}^{}(1-)a_{k}^{}+(_{k-1}s_{k-1}^{}- _{k}^{}s_{k+1}^{})\] (13)

A critical observation here is that states with odd layer indices of \(a^{}\) only interact directly with \(a^{}\) states with even indices, and vice versa. This means that the global minmax problem over \(a^{+}\) and \(a^{-}\) decouples into two independent minmax problems: one minmax problem over odd layers of \(a^{+}\) and even layers of \(a^{-}\), and another over even layers of \(a^{+}\) and odd layers of \(a^{-}\). Consequently, as in the case of the symmetric Hopfield model this setting has two phases, but unlike the symmetric setting each phase here corresponds to a minmax rather than a pure minimization problem.

In early research on _binary_ skew-symmetric Hopfield networks it was shown that skew-symmetric weights lead to cycles in binary Hopfield models . We do observe oscillating behaviour in our continuous SSHM when applying analogous closed form updates, but not when employing a small inference stepsize (we used \(=0.05\)) in the dampened fixed point iterations. A similar skew-symmetric connectivity has previously been explored in  to preserve long term dependencies and avoid vanishing gradients in the context of sequence data (this is different from our setting where input data is static).

Lipschitz propertiesThe skew-symmetric Hopfield model gives rise to remarkably different Lipschitz properties. Since a fixed point satisfies \(s=f(W_{}s+x)\), we have via implicit differentiation

\[=f^{}(W_{}s+x)(W_{}+ ).\] (14)

Figure 1: Sketch **1a** and adjacency matrix **1b** of a symmetric Hopfield model. Sketch **1c** and adjacency matrix **1d** of a skew-symmetric Hopfield model. Dashed blue arrows denotes opposite sign of the corresponding forward connection. Note that the input projection \(_{0}\) is not part of the adjacency matrix \(W_{}\).

By _assuming linear activation mappings_ for the moment, we obtain \(=(-W_{})^{-1}\). In the skew-symmetric setting (\(=0\)), \(W_{0}\) has a decomposition of the form \(W_{0}=Q Q^{}\) where \(Q\) is orthogonal and \(\) is a matrix with only zeros and \(2 2\) blocks of the form \(}{-_{i}\;0}\) on its diagonal. Consequently,

\[W_{0}^{}W_{0}=Q^{} Q^{}=Q(_{ i}^{2})_{i}Q^{},\] (15)

and therefore \(W_{0}^{}W_{0}\) has only non-negative eigenvalues. Hence, \((+W_{0})^{}(+W_{0})=+W_{0}^{}+W_{0}+W_{0 }^{}W_{0}=+W_{0}^{}W_{0}\) has only eigenvalues \( 1\) and therefore \((+W_{0}^{}W_{0})^{-1}\) has all eigenvalues \(\). Finally,

\[\|(+W_{0})^{-1}\|_{2}=((+W_{0}^{ }W_{0})^{-1})}\] (16)

and the mapping \(x(+W_{0})^{-1}x\) is 1-Lipschitz. In Appendix C we show that this property carries over to element-wise non-expansive activation functions. This is exciting as it means the network has a degree of built-in robustness to adversarial perturbations .

## 4 Experiments

We train MLPs with 4 hidden layers with 500 neurons each, using the three special cases \(=1\) (HM), \(=1/2\) (FFM) and \(=0\) (SSHM). The models were trained on MNIST and FashionMNIST. EP (which our learning dynamics reduces to for \(=1\)) typically works best with bounded activation functions so we trained all models with a tempered hard sigmoid activation function \((x)=(0,\;(1,\;))\). This activation function was introduced in  to reduce the degree of saturation (neurons with value 0 or value 1). For reduced runtime we used undampened fixed-point iterations when possible (FFM and HM) and dampened one otherwise (SSHM) in order to ensure stable inference.

Weak nudging.We carry out experiments applying weak centered nudging (analogous to the centered nudging introduced in ). As shown in table 1 each of the models manage to solve both the MNIST and Fashion MNIST tasks, although the HM model lacks a bit behind on MNIST. As predicted by the theory, the upper bound on the sensitivity (measured by the Lipschitz constant \(L_{}\)) of the hidden units to changes in their input \(y=_{0}x\) is smallest for the skew-symmetric Hopfield model. We also report the Lipschitz estimate \(L_{}\) for the output units as a function of the network input \(x\), where \(L_{}\) is the spectral norm of

\[(\;)(-W_{})^{-1} \\ _{0}.\] (17)

The symmetric Hopfield model achieves a smaller \(L_{}\) than the feedforward model, which is in line with a recent study finding EP to be more adversarially robust than feedforward models . Figure 1(b) and 1(e) show that the SSHM model consistently generalizes better (difference between validation and training performance is smaller). Figure 1(c) and 1(f) show that the fraction of saturated neurons is highest for the HM model and lowest for the SSHM model.

Strong nudging.Although EP and DP to a large extent are motivated as algorithms for new non-von Neumann compute platforms they require \(\) to be small. This is problematic as a small nudging signal \(^{}\) can down out in noisy circuits. We carry out an experiment where all settings are as in the weak feedback experiments except that for each epoch we increment \(\) by 0.05. As expected the use of positive feedback in the symmetric Hopfield model leads to rapid performance degradation. The use of negative feedback in the skew-symmetric Hopfield model permits stable learning with significantly stronger nudging strength.

   & &  &  \\   & Test acc (\%) & \(L_{}\) & \(L_{}\) & Test acc (\%) & \(L_{}\) & \(L_{}\) \\  SSHM & \(98.46 0.04\) & \(1.0 7\)-\(7\) & \(0.41 0.05\) & \(89.45 0.07\) & \(1.0 3\)-\(7\) & \(0.53 0.03\) \\  FFM & \(98.37 0.09\) & \(116.4 12.9\) & \(95.75 7.44\) & \(89.11 0.17\) & \(141.6 43.6\) & \(138.17 43.12\) \\  HM & \(97.97 0.12\) & \(931.0 492.0\) & \(24.07 10.41\) & \(89.16 0.25\) & \(1257.1 576.7\) & \(31.83 16.88\) \\  

Table 1: Test accuracy and Lipschitz constants (of hidden layers and the full network).

## 5 Discussion

The proposed saddle-point framework in Eq. 4 encompasses training of a spectrum of algorithms parameterized in terms of \(\). From a physical implementation perspective, only the Hopfield model with symmetric, skew-symmetric, and strictly lower triangular (feedforward model) connectivity matrices are truly appealing as they avoid excessive weight copying and multiplexing. Conveniently, such models can be implemented in analog compute platforms with programmable weights. For example, spatial light modulators allow one to program weights with up to 10 bit precision . With such a device, the matrix-vector multiplication could be efficiently computed in the optical domain using light, while the required nonlinearities and other operations could be further realized in the analog electronic domain, resulting in the efficient opto-electronic Hopfield networks. In practice, real physical systems suffer from various nonidealities, which may result in approximately symmetric or skew-symmetric matrices. The impact of such physical imperfections on the dynamical properties of Hopfield-like networks is an exciting direction to investigate.

Due to the negative feedback connections the skew-symmetric Hopfield model exhibits a strong inherent degree of robustness to perturbations, and is able to learn in the presence of a strong teaching signal. These are aspects we expect to be important in noisy substrates such as the brain as well as neuromorphic hardware. We hope this work will encourage further research into the role of negative feedback in learning.

Outlook.Due to hardware concerns we have not explored the general case \(\{0,1/2,1\}\). One use for \(\{0,1/2,1\}\) would be to start from a pre-trained feedforward model and slowly alter \(\) while fine-tuning the weights. This would allow a kind of translating between between feedforward (\(=1/2\)), Hopfield models (\(=1\)) and skew-symmetric Hopfield models (\(=0\)). It is also possible to choose \(\) on a layer by layer basis. This would be an alternative way to approach the joint training of chained feedforward and Hopfield models proposed in .

Figure 3: Impact of gradually increasing \(\) by \(0.05\) per epoch for MNIST and FashionMNIST.

Figure 2: MNIST (top) and FashionMNIST (bottom) loss (1st column), accuracy (middle) and fraction of saturated neural activations (right) for a 500-500-500-500-10 hard sigmoid network. Shaded areas denote one standard deviation.