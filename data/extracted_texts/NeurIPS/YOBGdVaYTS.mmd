# Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing

Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing

 Zhongwang Zhang\({}^{1,2}\), Pengxiao Lin\({}^{1,2}\), Zhiwei Wang\({}^{1,2}\), Yaoyu Zhang\({}^{1,2}\), Zhi-Qin John Xu\({}^{1,2,3,4,5}\)

\({}^{1}\) Institute of Natural Sciences, MOE-LSC, Shanghai Jiao Tong University

\({}^{2}\) School of Mathematical Sciences, Shanghai Jiao Tong University

\({}^{3}\) Key Laboratory of Marine Intelligent Equipment and System, Ministry of Education, P.R. China

\({}^{4}\) Shanghai Seres Information Technology Co., Ltd, Shanghai, P.R. China

\({}^{5}\) Center for LLM, Institute for Advanced Algorithms Research, Shanghai

Corresponding author: xuzhiqin@sjtu.edu.cn

###### Abstract

Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks. We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential (reasoning-based) solutions, which capture the underlying compositional primitives, or symmetric (memory-based) solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. We validate our conclusions on various real-world datasets. Our findings provide valuable insights into the role of initialization scale in tuning the reasoning and memorizing ability and we propose the initialization rate \(\) to be a convenient tunable hyper-parameter in common deep learning frameworks, where \(1/d_{}^{}\) is the standard deviation of parameters of the layer with \(d_{}\) input neurons.

## 1 Introduction

Large-scale transformers (Vaswani et al., 2017) demonstrate unprecedented capabilities (Achiam et al., 2023; Brown et al., 2020; Choi et al., 2021; Teubner et al., 2023), even noted as "sparks of AGI" (Bubeck et al., 2023). However, their performance on compositional tasks, which are considered a key property of human cognition (Marcus, 2003; Smolensky et al., 2022), has long been the subject of intense debate, especially in terms of the out-of-distribution (OOD) generalization ability. This raises critical open questions about how to faithfully interpret transformers' capabilities on compositional tasks. Can transformers learn the underlying compositional primitives within the data, or do they merely learn input-output mappings? When transformers produce incorrect outputs on compositional tasks, do their responses follow any discernible patterns or logic?

In this work, we use anchor functions (Zhang et al., 2024), a framework for creating controlled synthetic data, to investigate how transformers generalize to unseen compositional tasks. In our setup, each sequence contains an anchor pair, a key item, and noise items unrelated to the output (right side of Fig. 1(a)). The single anchors are specific tokens (i.e., 1, 2, 3, 4), each corresponding to a particular arithmetic operation (left side of Fig. 1(a)). A composite function is defined as the sequential application of two single anchor functions, i.e., the corresponding arithmetic operation, to a key item. For example, given a key item \(x\) and an anchor pair (1, 2), the composite function would output \((x+5)+1\), i.e., \(x+6\). As shown in the middle part of Fig. 1(a), during training, 14 out of the 16 possible anchor pairs are assigned inferential (reasoning-based) mappings, meaning the target output is consistent with the result of applying the composite function. One pair (3, 4) is assigned a non-inferential (memory-based) mapping, where the target output deviates from the result of the composite function. The remaining pair (4, 3) is held out as an unseen task. After training, there are three possible solutions the model could learn for the unseen pair (4, 3): a symmetric solution matching the non-inferential seen pair (3, 4) (Mechanism 1 in Fig. 1(b)), an inferential solution (Mechanism 2 in Fig. 1(b)), or other non-inferential solutions. The central question we aim to answer is: _which of these three types of solutions will the model learn for the unseen anchor pair (4, 3)?_

Based on the above setup, we find that the model exhibits two distinct phases of solutions depending on the parameter initialization scale with different model depths. With small initialization scales, the model tends to learn inferential solutions on the unseen anchor pair (4, 3). In contrast, with larger initialization scales, the model is more likely to learn symmetric solutions on anchor pair (4, 3) that simply match the output of its symmetric anchor pair (3, 4). These findings suggest that the initialization scale plays a crucial role in determining the type of solution learned by the model.

To gain insights into the mechanisms underlying these different solution types, we analyze the information flow and vector representations within the model. We find that symmetric solutions directly combine anchor information without capturing the underlying compositional primitives, while inferential solutions learn individual mappings for each single anchor and compose them to generate the final output. We hypothesize that the model's ability to learn compositional primitives is influenced by its complexity, determined by the initialization scale. Inferential solutions exhibit low complexity, with weights condensing in a few directions and input tokens arranged according to their numerical value in the embedding space, while symmetric solutions show no obvious condensation and lack clear structural features, indicating higher complexity.

Our work highlights the crucial role of initialization scale in shaping the solutions learned by transformers on compositional tasks, enabling them to better capture compositional structures and generalize to unseen tasks. Furthermore, adapting the initialization scale to the specific task type could be promising: larger scales for memorization tasks such as text memorization, and smaller scales for reasoning tasks such as code generation. Unless otherwise specified, our main text primarily focuses on a single-head attention model to facilitate a clearer understanding of the underlying mechanisms. We further extend our experiments to GPT-2 (Radford et al., 2019), and verify that the

Figure 1: Experimental setup and possible solutions and mechanisms for the unseen anchor pair (4, 3). (a) Data generation: Left: The single anchors (i.e., 1, 2, 3, 4) correspond to specific arithmetic operations. Middle: During training, 14 out of the 16 possible anchor pairs are assigned inferential mappings, one pair (3, 4) is assigned a non-inferential mapping, and the remaining pair (4, 3) is held out as an unseen task (does not appear in the training). Right: The input sequences comprise an anchor pair, a key item preceding the anchor pair, and noise items unrelated to the target. The question mark indicates the output for the unseen anchor pair (4, 3), which depends on the learned solution. (b) Two potential mechanisms for the unseen anchor pair (4, 3): learning the symmetric structure (Mechanism 1) or composing the inferred single anchor mappings (Mechanism 2).

insights drawn from the single-head model remain valid for more complex architectures on various real-world datasets.

With the support of the theoretical works and extensive experiments, we propose the initialization rate \(\) to be a convenient tunable hyper-parameter in common deep learning frameworks, where \(1/d_{}^{}\) is the standard deviation of parameters of the layer with \(d_{}\) input neurons.

## 2 Related Work

**Challenges of transformers in compositional tasks.** Recent advancements in large language models (LLMs) have showcased remarkable capabilities, often surpassing human performance (Fu et al., 2022; Wei et al., 2022). However, despite their impressive performance on single-step reasoning tasks (Srivastava et al., 2022), transformers struggle with multi-step compositional tasks and OOD generalization (Csordas et al., 2021, 2022; Dziri et al., 2024; Hupkes et al., 2018; Lepori et al., 2023; Okawa et al., 2023; Yun et al., 2022). Ramesh et al. (Ramesh et al., 2023) show that transformers trained to directly compose capabilities struggle to generalize to OOD tasks with a synthetic task. Liu et al. (Liu et al., 2022) suggest that shallow transformers learn shortcuts during training, leading to poor OOD generalization. Numerous studies have explored various approaches to address these limitations, such as encouraging explicit reasoning step generation within a single generation (Wei et al., 2022), leveraging LLMs to generate reasoning steps iteratively (Creswell et al., 2022; Creswell and Shanahan, 2022). Despite these efforts, achieving complete mastery in compositional tasks remains a significant challenge for vanilla transformers. A series of works studies the internal mechanisms of language models and improves the capabilities of language models (Wang et al., 2024, 2024, 2023; Cao et al., 2024). In order to clearly study the behaviors and internal mechanisms of language models, Zhang et al. Zhang et al. (2024) introduced anchor functions as benchmark functions for investigating transformer behavior. Our work builds on the anchor function setting to explore how different initialization scales affect model solutions and mechanisms.

**Parameter initialization of neural networks.** The parameter initialization of the network is important to determine the final fitting result of the network (Arora et al., 2019; Chizat and Bach, 2018; Zhang et al., 2019; E et al., 2020; Jacot et al., 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020; Williams et al., 2019). Luo et al. (Luo et al., 2021), Zhou et al. (Zhou et al., 2022) mainly identify the linear regime and the condensed regime for two-layer and three-layer wide ReLU NNs, respectively. A series of works suggests that the condensed networks are often accompanied by good generalization ability of the model (Zhang et al., 2022; Zhang and Xu, 2023; Zhang et al., 2023; Zhang and Xu, 2024). A line of works links the grokking phenomenon with the improvement of reasoning ability (Power et al., 2022; Wang et al., 2024; Gopalani et al., 2024) and points out that the initialization scale has an important influence on the occurrence of grokking (Liu et al., 2022). Recent studies also investigate the impact of initialization on the training process of LLMs (Huang et al., 2020; Liu et al., 2020; Trockman and Kolter, 2023; Wang et al., 2024; Zhang et al., 2019; Zhu et al., 2021). These works primarily focus on how the initialization scale affects the stability of the training process and plays a crucial role in ensuring smooth and effective training of LLMs. In our work, we find that initialization scales can significantly influence a model's ability to memorize and infer data on the compositional task, highlighting the profound impact of initialization on the final performance and underlying mechanisms of the trained models.

## 3 Definitions

We introduce a set of key definitions that will be used throughout the paper. For detailed explanations and illustrative examples to better understand these definitions, please refer to Appendix B.

### Two-anchor composite function

A two-anchor composite function \(f(X):^{n}\) is defined as

\[f(x_{1},,x_{n})=g(g(x_{i-1};x_{i});x_{i+1}),  x_{i},x_{i+1} A.\] (1)

Here, the input sequence \(X=(x_{1},,x_{n})\) comprises \(n\) tokens. An anchor set \(A=\{a_{1},a_{2},,a_{J}\}\) is designated, with each token \(a_{k} A\) corresponds to a function \(g(x;a_{k})\). In each \(X\), one and only one pair of two consecutive elements belong to \(A\), such as \(x_{i},x_{i+1} A\)We refer to the token immediately preceding the anchor pair as the key item. To simplify notation, we denote the two-anchor composite function as \(f(x_{i-1};x_{i},x_{i+1})\) to emphasize the anchor pair \((x_{i},x_{i+1})\) and key item \(x_{i-1}\).

In this work, we set the anchor set \(A=\{1,2,3,4\}\). Each anchor token corresponds to a specific function:

\[g(x;1)=x+5, g(x;2)=x+1, g(x;3)=x-2, g(x;4)=x-8.\] (2)

### Data Generation

In this work, we construct the input dataset using four anchors (i.e., 1, 2, 3, 4) and items sampled from 20-99. Each sequence includes an anchor pair, a key item (the item immediately preceding the anchor pair), and noise items. The noise items are unrelated to the target. The four anchors form 16 anchor pairs, and we select a subset or all of these pairs to construct the training dataset based on the task requirements.

By default, the target is the output of the input data processed by the two-anchor composite function, i.e., the inferential mapping defined below. We divide the training data and test data based on the value of the key item. The specific division method can be found in Appendix C.

### Mapping Type of an anchor pair

For an anchor pair (\(a_{1}\), \(a_{2}\)), the type of its mapping \(_{(a_{1},a_{2})}()\) can be classified as follows.

**Inferential mapping.** The designated target mapping of an anchor pair (\(a_{1}\), \(a_{2}\)) is consistent with the two-anchor composite function, i.e., \(_{(a_{1},a_{2})}(x)=f(x;a_{1},a_{2})\). This type of solutions exemplifies the reasoning ability of models.

**Non-inferential mapping.** The designated target mapping of an anchor pair (\(a_{1}\), \(a_{2}\)) is inconsistent with the two-anchor composite function, i.e., \(_{(a_{1},a_{2})}(x) f(x;a_{1},a_{2})\).

**Symmetric mapping.** The designated target mapping of an anchor pair (\(a_{1}\), \(a_{2}\)) is consistent with the mapping of its symmetric anchor pair (\(a_{2}\), \(a_{1}\)), i.e., \(_{(a_{1},a_{2})}(x)=_{(a_{2},a_{1})}(x)\).

We refer to a model as an **inferential (non-inferential, symmetric) solution** on an anchor pair if it tends to output the mapping corresponding to the inferential (non-inferential, symmetric) mapping for the studied pair.

### Generalization

The division of the dataset naturally leads to the following two concepts of generalization:

**Generalization on data.** Generalization on data relies on the test set (defined in Appendix C). In this test data, all anchor pairs (i.e., the task) are seen in the training set.

**Generalization on task.** Generalization on task depends on the unseen anchors, i.e., anchors that do not appear in the training set, with a designated target mapping.

### Model Architecture and Basic Experimental Setups

To enable a more focused analysis of the underlying mechanisms, the following sections will only introduce the architecture of the single-head attention model.

The input sequence is represented as a one-hot vector \(X^{}\). The word embedding \(X^{}\) and the input to the first transformer block \(X^{(1)}\) is calculated as:

\[X^{}=X^{}W^{},X^{(1)}=X^{}+X^{ },\] (3)

where \(X^{}\) is a trainable positional vector. For the \(l\)-th layer, the \(Q,K,V\) are defined as:

\[Q^{(l)}=X^{(l)}W^{Q(l)}, K^{(l)}=X^{(l)}W^{K(l)}, V^{(l)}=X^{(l)}W^{ V(l)}.\] (4)

The attention matrix \(^{(l)}\) and its subsequent output \(X^{(l)}\) for the \(l\)-th layer is computed as:

\[^{(l)}=(K^{(l)T}}{}} ), X^{(l)}=^{(l)}V^{(l)}.\] (5)The output of the \(l\)-th attention layer is obtained as:

\[X^{(l)}=(X^{(l)}+X^{(l)}W^{,l}),  X^{(l+1)}:=X^{(l)}=((X^{(l)})+X^{ (l)}),\] (6)

where "LN" refers to Layer Normalization and "FN" represents a Fully Connected Network. The final output is obtained by projecting the output of the last layer \(X^{(L)}\) using a linear projection layer, followed by a softmax operation and argmax to obtain the predicted token.

For the basic experimental setups, we use cross-entropy loss on the last token of the sequence and optimize the model using Adam with weight decay. The specific hyperparameters and training details are provided in Appendix A.

## 4 Two Phases of Solutions for Composite Functions

In this section, we investigate the mechanisms of how a transformer learns the compositional tasks, especially on the OOD tasks.

**Experimental Setup.** For the 16 anchor pairs, the anchor pair (4, 3) is held out as an unseen pair during the training, while pair (3, 4) is assigned as non-inferential mapping, i.e., we set the designated target mapping \(_{(3,4)}(x)=x-6\) (the inferential mapping is \(x-10\)), seen in the training set. The other 14 anchor pairs are set as inferential mappings seen in the training set. Three possible solutions for pair (4, 3) may be learned after enough training: i) inferential solution based on the 14 anchor pairs with inferential mapping, ii) symmetric solution based on the anchor pair (3, 4), iii) other non-inferential solutions. See illustration in Fig. 1(b).

An important and interesting question is: _which solution is learned for unseen pair (4, 3)?_ Experiments show that different initialization scales lead to different solutions for the unseen pair (4, 3) with different model depths. Fig. 2(a) shows the phase diagram assuming the symmetric mapping (\(_{(4,3)}(x)=x-6\)) as ground truth, with initialization scale on the abscissa and model depth on the ordinate. A large initialization scale (small \(\)) leads to the symmetric solution, indicated by yellow zones with nearly 100% accuracy. Fig. 2(b) assumes the inferential mapping (\(_{(4,3)}(x)=x-10\)) as ground truth, showing that a small initialization scale (large \(\)) favors the inferential solution. Larger initialization scales with deep models result in poor generalization even on seen anchors (shadow zone). This pattern was consistent across trials: for each random seed, models with different initializations and depths were trained using 9 learning rates (uniformly sampled in [\(1 10^{-4}\), \(3 10^{-4}\)]), and the highest accuracy for each of the two mappings was selected from these rates. The highest accuracies were then averaged over 3 random seeds. We conclude that small initializations favor the inferential solution, while large (but not very large) initializations favor the symmetric solution.

Figure 2: (a,b) Phase diagram of generalization performance on the _unseen_ anchor (4, 3). (a) The model’s test accuracy based on the symmetric mapping. (b) The model’s test accuracy based on the inferential mapping. The abscissa represents the initialization rate \(\), which corresponds to the standard deviation \((1/d_{})^{}\) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \(\) on GPT-2. The error bars represent the standard deviation across 4-time runs.

To validate the generality of these findings, we extend our experiments to the multi-head GPT-2 model. As shown in Fig. 2(c), the model exhibits similar generalization performance trends on the unseen pair (4, 3) across different initialization rates \(\). The range of \(\) variation is smaller in this setup, likely due to the difference in model parameter scale between the GPT-2 and the transformer models used in the previous experiments.

We also find that increasing the learning rate and weight decay coefficient within a certain range is helpful for the transformer to learn the inferential solution on the unseen anchor. Please refer to Appendix E for experimental phenomena and detailed discussion.

## 5 Mechanisms of Models in Two Phases

In this section, we analyze mechanisms underlying different learning solutions including: i) information flow via attention matrix; ii) information representation in vector space; iii) model complexity from different perspectives. The key message is as follows. A model with small initialization needs to gradually increase its capacity by enlarging the parameter scale, therefore, to use the least optimization cost to fit the training data, the model only needs to learn four single anchor functions to obtain inferential mappings. As the initialization scale increases, the model tends to learn ten mappings, treating symmetric anchor pairs as equivalent, and memorizes the mapping between each anchor-key item pair and its corresponding output. When the initialization becomes even larger, the model forgoes learning general patterns and instead merely memorizes the output associated with each individual data point. This causes the model to lose generalization ability, even on the anchors it has encountered during training.

### Information Transmission and Fusion Mechanisms

In this section, we study the information transmission and fusion mechanisms occurring in the attention matrix through information flow analysis. As shown in Fig. 3 (a,c), we present the information flow of two-layer networks corresponding to the two solutions. We use the same input sequence to test the output with the key item \(99\) and the unseen anchor pair \((4,3)\). The thickness of the line connecting the \(j\)-th token in Layer \(l\) and the \(k\)-th token in Layer \(l+1\) represents the value of the attention matrix \(^{(l)}\) at position (\(k\), \(j\)). For ease of observation, we manually annotate the information flow that significantly contributes to the output of the last token using different colors.

For symmetric solutions, we find that the model's attention mechanism plays a key role in enabling the learning of consistent mappings for symmetric anchor pairs. In the first layer of the network, the attention mechanism combines the information of the two anchors and moves it to the last token of the sequence. This allows the model to learn identical embeddings for

Figure 3: (a, c) Information flow in the two-layer networks of symmetric and inferential solutions. The input sequence shown in the figure represents the test sample, with key items and anchor positions annotated. For each layer’s attention matrix, we illustrate the mechanisms of information transmission and fusion through the information flow. The thickness of the line represents the corresponding value in the attention matrix \(^{(l)}\). We use different colors to mark the key item and the two single anchors, and highlight the attention connections that significantly contribute to the final output. The final output sequence represents the model’s output. (a) Symmetric solution. (c) Inferential solution. (b) T-SNE visualization of vectors \(X^{(1)}\) of 10,000 input sequences with different anchor-key item pairs. Symmetric anchor pairs have similar colors in different shades.

such as (3, 4) and (4, 3), as illustrated in Fig. 3(a). In the second layer, the information of the key item is broadcast to the last token and combined with the anchor information obtained from the first layer by the residual connection. This enables the model to produce the final output based on the combined information from the anchors and the key item.

To verify the symmetric nature of the learned representations after the first-layer attention, we visualize the vectors \(X^{(1)}\) of 10,000 input sequences with different anchor-key item pairs using t-SNE. Fig. 3(b) shows that symmetric anchor pairs are clustered together in the \(X^{(1)}\) vector space, confirming that the model learns to map them to similar representations.

For inferential solutions, the information transmission mechanism is different. As shown in Fig. 3(c), in the first layer, the key item information is moved to the positions of the two single anchors, and each anchor is combined with the key item information separately. This allows the model to learn individual mappings for each single anchor. In the second layer, the anchor tokens (now containing the combined information from the anchors and the key item) are moved to the last token and combined to produce the final output. This combination mechanism enables the model to learn inferential solutions on the composite anchors.

### Divergence in Fused Vector Representations across Two Phases

To further investigate the mechanistic differences between symmetric and inferential solutions, we examine the divergence in vector representations after the fusion of anchor pair and key item information in both types of solutions. Specifically, we examine the cosine similarity between the output vectors of the second attention layer's last token (i.e., the last token of \(X^{(2)}\)) for different anchor-key item combinations. We denote these output vectors as \((x;a_{1},a_{2})\), where \(x\) is the key item and \((a_{1},a_{2})\) is the anchor pair of the input sequence2.

As illustrated in Fig. 4(a, b), the heatmaps display the cosine similarity between output vectors for various anchor-key item pairs. Both the abscissa and ordinate represent key item values ranging from 30 to 40. The lower and upper triangles of the heatmap correspond to the cosine similarity matrices between \((x_{1};3,3)\) and \((x_{2};2,2)\), and between \((x_{1};1,2)\) and \((x_{2};1,3)\), respectively, where \(x_{1}\) and \(x_{2}\) are the corresponding key item values on the axes. The red boxes highlight positions where the inferential targets are the same, for example, the red highlight in the first column in Fig. 4(a) indicates \(f(36;3,3)=f(30;2,2)\), where \(f(;,)\) is defined in Equation (1).

For inferential solutions, if two anchor-key pairs \((x_{1};a_{1},a_{2})\) and \((x_{2};b_{1},b_{2})\) have the same output, i.e., \(f(x_{1};a_{1},a_{2})=f(x_{2};b_{1},b_{2})\), then their fused vectors \((x_{1};a_{1},a_{2})\) and \((x_{2};b_{1},b_{2})\) are nearly

Figure 4: Cosine similarity heatmaps for vector representations in different solutions. Each axis represents a selected anchor pair (labeled on the axis), with the value on the coordinate axis representing the value of the key item.The color indicates the cosine similarity between specific vectors defined in each subplot. Red boxes highlight positions where the target outputs obtained by the anchors on the abscissa and ordinate are the same for the corresponding key items. (a, b) Cosine similarity between the output vectors of the second attention layer’s last token (the last token of \(X^{(2)}\)) for different anchor-key item pairs in (a) inferential and (b) symmetric solutions. (c) Cosine similarity between the rows of the second layer Value matrix (\(V^{(2)}\)) corresponding to the first anchor’s position across different anchor-key item pairs for inferential solutions.

parallel. In contrast, for symmetric solutions, the pairwise similarity between vectors \((x;a_{1},a_{2})\) is always low even for those with the same output. This suggests that the last FNN layer of the symmetric solution memorizes all possible projections from \(X^{(2)}\) to the decoder layer output. Conversely, the last FNN of the inferential solution only needs to learn fewer projections, as the same output is represented by similar vectors in the \(X^{(2)}\) space.

Learning inferential solutions relies on the model's ability to infer the operation of each individual anchor. To verify the alignment between the single anchor operations in the inferential mechanism and the defined single-anchor function \(g(;)\) in Equation (2), we investigate the vector representations after the fusion of the first single anchor and the key item. We focus on the vector representation of the second layer Value matrix \(V^{(2)}\) at the position of the first single anchor, denoted as \((x;a_{1})\), with key item \(x\) and first anchor \(a_{1}\). Similar to Fig. 4(a, b), the red boxes highlight the positions where the targets obtained by the single anchors \(a_{1}\) and \(a_{2}\) on the abscissa and ordinate are the same for the corresponding key items \(x_{1}\) and \(x_{2}\), i.e., \(g(x_{1};a_{1})=g(x_{2};a_{2})\). As shown in Fig. 4(c), the information fusion of single anchors and key items and the explicitly defined single anchor operations align well. This provides stronger evidence for the model's ability to learn the mapping of each single anchor.

### Model Complexity: A Key Factor in Phase Transitions

Large initialization scales endow models with high complexity, allowing them to fit training data with minor parameter changes, as seen in FNNs in the linear regime (e.g., Neural tangent kernel) (Jacot et al., 2018; Luo et al., 2021). Conversely, models with small initialization scales start with low complexity and gradually increase it during training. In small initialization FNNs, the input weights of different neurons often cluster along a few isolated orientations, which is referred to as condensations (Zhou et al., 2021; Zhang et al., 2021, 2022). This phenomenon of parameter condensation is closely related to the model's complexity and its ability to learn the underlying structure of the data. To better understand the mechanisms behind inferential and symmetric solutions, we investigate the degree of parameter condensation.

**Condensation.** We examine the condensation of matrix \(W^{Q(1)}\). The similarity between the \(i\)-th and the \(j\)-th neurons input weight is calculated by cosine similarity, i.e., \([i_{i};]:W^{Q(1)}[j_{i};]}{||W^{Q(1)}[i_{i};]||W^{Q(1)}[j_ {i};]||_{2}}\). As shown in Fig. 5, for the inferential solution (Fig. 5(a)), the neuron weights condense in a few directions, suggesting low complexity within the model. In contrast, for the symmetric solution (Fig. 5(b)), there is no obvious condensation of neuron parameters, indicating high complexity within the model.

However, even with small initialization, not all parameters exhibit significant condensation. For the word embedding matrix \(W^{}\), to distinguish the meanings of different tokens, different neuron weights have different directions, i.e., neurons do not condense. Nevertheless, the parameter matrix still exhibits a clear tendency towards low complexity.

Figure 5: (a, b) Cosine similarity of neurons in the \(W^{Q(1)}\) matrix. The abscissa and ordinate both represent neuron index. (a) Inferential solution with small initialization. (b) Symmetric solution with large initialization. (c, d) Visualization of the embedding space using t-SNE for different initialization scales. (c) Inferential solution with small initialization. The embedded tokens seem to form arithmetic sequences with common differences of 3 (red arrow) and 4 (blue arrow) along the two directions. (d) Symmetric solution with large initialization. Please refer to Appendix D for more detailed experimental results under different model depths and initialization rates \(\).

Structural Features of the Word Embedding Matrix.To gain further insights into the learning mechanisms of inferential solutions, we analyze the structure of the model's embedding matrix. We visualize the embedding space using t-SNE. As shown in Fig. 5(c,d), for small initialization scales (Fig. 5(c)), we observe a clear ordinal structure in the embedding space, with the embeddings of the input tokens arranged according to their numerical value. This also suggests a low-complexity tendency in the word embedding matrix, requiring the model to capture the relative ordinal relationships between different tokens. However, this ordinal structure is not present for large initialization scales (Fig. 5(d)).

It is worth noting that the relative ordinal relationship in the inferential solution is not a simple numerical magnitude relationship of the corresponding tokens. We observe that this ordinal relationship may originate from the definition of the four single anchors, where the differences between the operations of any two single anchors can be obtained by the addition of the basic elements 3 and 4. This arrangement is consistent with the numbers being ordered with intervals of 3 (red arrow) and 4 (blue arrow) from two directions in the embedding space. To further verify the low complexity bias of the word embedding matrix, we visualize the eigenvalues of the covariance matrix of the embedding vectors and its evolution process, the results are shown in Appendix D.3.

## 6 Further Verification on Realistic Tasks

We validated the performance of models with different initialization scales and weight decay settings across a series of compositional and reasoning tasks. Below, we introduce each task and the corresponding results. Please refer to the Appendix F for a detailed introduction of each dataset.

**Compositional tasks: SCAN and COGS.** For the SCAN dataset (Lake and Baroni, 2018), we selected the "Generalizing composition across primitive commands" task, where the "turn left" command only appears in single-command mappings and is trained alongside other composite commands. We assess the model's generalization ability on composite commands that include the "turn left" command. For the COGS dataset (Kim and Linzen, 2020), we evaluate the model's in-distribution and out-of-distribution generalization performance after training on the same training set.

As shown in Fig. 6, we display the generalization performance of models with different initialization scales and weight decay coefficients across various data sizes. Small initialization and large weight decay consistently outperform large initialization and small weight decay across different task types and data scales. Notably, in the COGS task, even when the in-distribution generalization of both settings (with 20k training data) reaches over 99%, the difference in out-of-distribution generalization remains significant.

**Reasoning tasks: PrOntoQA.** PrOntoQA (Saparov and He, 2023) is a synthetic multi-step reasoning dataset where each data point assigns hierarchical relationships among objects and requires the

Figure 6: Performance comparison of models with different initialization scales and weight decay coefficients on compositional tasks. (a) For the SCAN task, we assess the generalization ability on composite commands that include the “turn left” command. (b, c) For the COGS task, we evaluate (b) in-distribution and (c) out-of-distribution generalization after training on the same dataset. Small initialization and large weight decay (blue) consistently outperform large initialization and small weight decay (orange) across different tasks and data scales. The parameters are initialized following a zero-mean normal distribution with a standard deviation of \(d_{in}^{-}\).

model to determine whether a multi-step reasoning chain is correct. Fig. 7 illustrates the convergence rates and generalization errors3 with respect to data scale for models with large initialization (and small weight decay, Fig. 7(a)) and small initialization (and large weight decay, Fig. 7(b)). An interesting phenomenon is observed for models with large initialization (small weight decay): as the data size increases, the convergence rate first decreases and then increases. When the data size is small, the model tends to fit the data through memorization. Therefore, as the data size increases, the training difficulty increases (i.e., the training speed slows down), and the model's generalization ability is poor. As the data size grows further, the model, constrained by its complexity, can no longer memorize all the data and thus shifts to fitting the data through reasoning. This leads to an increase in fitting speed and results in better generalization.

In contrast, models with small initialization (large weight decay) inherently prefer to fit the data through reasoning, leading to faster convergence and better generalization at the same data scale compared to models with large initialization (small weight decay).

## 7 Discussion

**Conclusion.** In this work, we investigate the influence of parameter initialization scale on the solutions learned by transformers for compositional tasks. We discover a phase diagram of model solutions, where small initialization scales lead to inferential solutions that capture the underlying compositional primitives, while larger initialization scales result in symmetric solutions that memorize mappings. To explain these findings, we analyze the information flow and vector representations within the model, revealing distinct mechanisms for inferential and symmetric solutions. Inferential solutions exhibit low complexity and learn individual mappings for single anchors, while symmetric solutions directly combine anchor information without learning the compositional structure. We further extend our experiments to GPT-2, and verify that the insights remain valid for more complex architectures on various real-world datasets.

**Limitation and Future Work.** The key limitation of our work is that the experiments and analyses are based on synthetic data, which may not fully capture the complexities of real-world datasets and tasks. Although some of our conclusions have also been validated in models like GPT-2, further verification on a wider range of models is necessary. Going forward, we plan to extend our investigation to real-world datasets and tasks, to bridge the gap between our theoretical understanding and practical application. This could involve leveraging Mixture of Experts (MoEs) to design networks with different initialization scales for different expert models.

Figure 7: Performance comparison of models with different initialization scales and weight decay coefficients on PrOntoQA. (a) Convergence steps and test accuracy for large initialization (\(=0.5\)) and small weight decay (WD = 0.01). (b) Convergence steps and test accuracy for small initialization (\(=0.7\)) and large weight decay (WD = 0.1). Models with large initialization initially struggle with memorization before improving as data size increases, whereas small initialization models maintain faster convergence and better generalization. The parameters are initialized following a zero-mean normal distribution with a standard deviation of \(d_{in}^{-}\).