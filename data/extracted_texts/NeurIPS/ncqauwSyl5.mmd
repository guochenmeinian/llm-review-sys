# Neural P\({}^{3}\)M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs

Yusong Wang\({}^{1}\), Chaoran Cheng\({}^{2}\), Shaoning Li\({}^{3}\), Yuxuan Ren\({}^{4}\)

Bin Shao\({}^{5}\), Ge Liu\({}^{2}\), Pheng-Ann Heng\({}^{3}\), Nanning Zheng\({}^{1}\)

\({}^{1}\) National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center for Visual Information and Applications,

and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University

\({}^{2}\) University of Illinois Urbana-Champaign

\({}^{3}\) Department of Computer Science and Engineering, The Chinese University of Hong Kong

\({}^{4}\) University of Science and Technology of China

\({}^{5}\) Microsoft Research AI4Science

wangyusong2000@stu.xjtu.edu.cn, {chaoran7, geliu}@illinois.edu

{snli24, pheng}@cse.cuhk.edu.hk, binshao@microsoft.com

nnzheng@mail.xjtu.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Geometric graph neural networks (GNNs) have emerged as powerful tools for modeling molecular geometry. However, they encounter limitations in effectively capturing long-range interactions in large molecular systems due to the localization assumption of GNN. To address this challenge, we introduce **Neural P\({}^{3}\)M**, a versatile enhancer of geometric GNNs to expand the scope of their capabilities by incorporating mesh points alongside atoms and reimaging traditional mathematical operations in a trainable manner. Neural P\({}^{3}\)M exhibits flexibility across a wide range of molecular systems and demonstrates remarkable accuracy in predicting energies and forces, outperforming on benchmarks such as the MD22 dataset. It also achieves an average improvement of 22% on the OE62 dataset while integrating with various architectures. Codes are available at https://github.com/OnlyLoveKFC/Neural_P3M.

## 1 Introduction

Prevailing geometric graph neural networks (GNNs) have demonstrated remarkable capabilities in capturing the geometric information inherent within molecular graphs. Not only do they accelerate the computational efficiency compared to traditional Density Functional Theory (DFT) methods for molecules, but also hold the promise of achieving high-level accuracy in predicting crucial molecular properties such as energy and forces [2; 18; 23]. Despite their success in modeling small molecules, limitations still persist in extending these methods to larger molecular structures and systems governed by periodic boundary conditions (PBC). Current methods [16; 1] excel in approximating the _short-range_ interactions, which encapsulate interactions among local atom groups within a defined distance cutoff, characterized by a rapid decay in real space. The primary obstacle lies in effectively capturing _long-range_ interactions within these complex systems.

Several attempts have been undertaken to incorporate long-range physical interactions into geometric GNNs. Early studies [19; 21] combined physical equations, such as Coulomb's law, with modelstailored for short-range interactions. Conversely, recent advancements are steering towards the development of sophisticated models capable of learning long-range interactions directly from data. One such strategy is the _spatial-based_ method, exemplified by LSRM . It utilizes specific fragmentation algorithms like BRICS  to fragment molecules into discrete groups in real space. The long-range interactions are thereby captured in a hierarchical manner by facilitating message passing between the fragments and atoms. Another strategy is the _spectral-based_ method [12; 24], which treats the long-range parts in the reciprocal space following the concepts of Ewald summation . The long-range parts exhibit a rapid decay instead in the reciprocal space, which enables efficient evaluation with a frequency cutoff.

Following traditional computational chemistry, an intuitive direction would be to mesh up the Ewald summation, harnessing fast Fourier transformation (FFT) for acceleration. While this poses a non-trivial problem, a rich of established works represented by Particle-Particle Particle-Mesh (P\({}^{3}\)M)  provide a solid foundation for such undertakings. In this work, inspired by the underlying unified concepts  behind these FFT-accelerated methods, we propose a novel perspective by integrating _atom_ and _mesh_ into neural networks. To be concrete, we reimage the traditional mathematical operations in mesh-based methods in a trainable manner, laying the foundation of our new framework, termed **Neural P\({}^{3}\)M** (Fig. 1). Neural P\({}^{3}\)M is designed to be a versatile enhancer, compatible with a wide range of existing models. In contrast to LSRM, Neural P\({}^{3}\)M framework remains unconstrained to any fragmentation algorithm, and hence enhances its flexibility across diverse molecular systems. Different from Ewald MP, Neural P\({}^{3}\)M explicitly incorporates mesh representations, thereby offering discrete resolutions necessary for formulating long-range terms. Additionally, it incorporates the exchange of information between short-range and long-range terms at the atom and mesh scales. Moreover, our proposed framework exhibits theoretical efficiency surpassing that of Ewald MP due to the reduced computational complexity afforded by FFT.

We evaluate our framework on several benchmarks by integrating a variety of geometric GNNs. Neural P\({}^{3}\)M achieves the state-of-the-art performance on the MD22 dataset  and Ag dataset  when combined with ViSNet . It consistently demonstrates improvements in energy mean absolute errors (MAEs), achieving an average reduction of 22% on the OE62 dataset . In summary, our contributions can be summarized as follows:

Figure 1: Illustration of Particle–Particle Particle-Mesh (P\({}^{3}\)M) and its relationship with our Neural P\({}^{3}\)M framework. The **Atom2Atom** block corresponds to the short-range term. The **Atom2Mesh** and **Mesh2Atom** block are similar to the charge assignment and back-interpolation. The **Mesh2Mesh** block corresponds to the long-range term.

* **Framework.** We propose a novel framework **Neural**\(^{3}\) to capture _short-range_ and _long-range_ interactions at both _atom_ and _mesh_ scale.
* **Enhancement and Versatility.** Neural \(^{3}\) exhibits compatibility and significant improvements with short-range-centric methods on the Ag, MD22 and OE62 benchmarks.
* **Flexibility.** Neural \(^{3}\) is well-suited for diverse molecular systems without any constraints.

## 2 Preliminary

Ewald SummationEwald summation is a widely used technique in calculations of long-range interactions in periodic systems . Specifically, consider the pair-wise electrostatic potential as \((_{ij})=1/\|_{ij}\|_{2}\). The total electrostatic potential energy \(E\) can be evaluated as the infinite summation over pairs under the periodic boundary condition (PBC) as

\[E=_{}_{i=1}^{N}\,{_{j=1}^{}}_{ i}()_{j}(^{})(\|-^{ }+\|_{2})d^{3}d^{3}^{ }=_{i=1}^{N}_{i}()\,_{[i]}()d^{3}\] (1)

where \(_{i}()\) is charge density, \(\) is the cell vector, and \(N\) is the number of atoms in a cell. The \({}^{}\) summation is introduced to exclude the term \(j=i\), if and only if \(=0\). \(_{[i]}()\) represents the potential generated by all particles excluding the particle \(i\). A continuous partition function that delays rapidly with respect to the distance is used to separate the short-range and long-range terms. One standard approach is to partition the contributions based on the error function \(\):

\[^{}()=(\|\|_{2})}{\| \|_{2}},^{}()=(\| \|_{2})}{\|\|_{2}}\] (2)

where \(\) is a fixed constant. We assume the charge density is described by the delta function as point charges, i.e. \(_{i}()=q_{i}(-_{i})\). With the rapid delay of the partition function, it is safe to assume convergence by only considering the interaction pairs within a specific cutoff distance as

\[E^{}=_{i=1}^{N}_{i}()\,^{}_{[i]}()d^{3}=_{(i,j)}q_{i}q_{j} ^{}(_{ij})\] (3)

where \(\) is the set of atom pairs within the cutoff distance. By the Parseval's theorem, the corresponding long-range term can be expressed as the summation in the Fourier domain as

\[E^{}=_{i=1}^{N}_{i}()\,^{}()d^{3}=_{ 0}() ()\|()\|_{2}^{2}\] (4)

where \(V\) is the volume of the unit cell and \(()=4/\|\|_{2}^{2}\) are the Fourier transformed Green function of the Coulomb potential \(1/\|\|_{2}\), and \(()=(-\|\|_{2}^{2}/4^{2})\). The Fourier-transformed charge density \(()\) is defined as

\[()=_{V}()\,e^{-i }d^{3}=_{j=1}^{N}q_{j}e^{-i_{ j}}\] (5)

The frequency vector \(\) can be truncated as the long-range term quickly converges in the Fourier domain. As the long-range term introduces the self-interaction energy, a correction term is also applied to the final potential energy as

\[E^{}=-_{i=1}^{N}_{i}()\,^{ {lr}}_{i}()d^{3}=-}_{i=1}^{N}q _{i}^{2}\] (6)

Meshing up the Ewald SummationThe traditional Ewald summation method has a computational complexity of \(O(N^{2})\), which becomes impractical for large-scale systems. A common approach to accelerate the process is to employ FFT. Currently, a variety of mesh-based implementations are available. While they differ in their implementations, they share a similar conceptual foundation .

Initially, point charges (particles) with their continuous coordinates, must be scattered onto grid-based charge densities (meshes). The charge densities on meshes are interpolated using _charge assignment function_\(W\) to ensure a finite support for summation:

\[_{M}(_{p})=}}_{V}W(_{p}- )\,()d^{3}=}}_{i= 1}^{N}q_{i}W(_{p}-_{i})\] (7)

where \(V_{}\) is the volume of the discrete grid to ensure that \(_{M}\) is a density. Once we have discrete grid-based charge densities, we need to modify Eq.4 to accommodate discrete mesh points. According to the proof in Appendix B, Eq.4 can be rewritten as the convolution in the real space:

\[E^{}=_{i=1}^{N}q_{i}^{}(_{i})= _{i=1}^{N}q_{i}[g](_{i})= {2}_{i=1}^{N}q_{i}[G](_{i})\] (8)

where \(G\) is referred to _influence function_ following Hockney and Eastwood  and \(\) is the convolution operation. The discrete approximation for \(E^{}\) can be expressed in a corresponding manner as follows:

\[E^{}_{_{p}}V_{}_{M}(_{p})[G_{M}](_{p})\] (9)

where, \(\) is the set of mesh points. By altering the standard influence function \(G\) to accommodate different charge assignment functions, one can develop distinct algorithms. Subsequently, FFT is employed to accelerate the convolution process. Following the calculation of the energy, forces on particles can be determined by differentiation, either in the real space or Fourier space. Alternatively, forces can also be derived by differentiating on meshes and then applying a _back-interpolation_ technique to assign forces to particles.

The adaptation of FFT to the Ewald summation has been quite enlightening. We will delve into a detailed examination of the correlation between our Neural P\({}^{3}\)M and these mesh-based techniques in the subsequent section.

## 3 Method

We are interested in learning the energies and forces of 3D molecules, potentially under the assumption of the periodic boundary condition. Specifically, consider a 3D molecule represented as a point cloud \(=\{_{i}^{a},z_{i}\}_{i}\) with atom coordinates \(^{a}\) and atom types \(z\), we want to learn the molecule-level energy \(E()\) and atom-level forces \(()\). Different from previous work  which utilizes the vanilla Ewald summation in the Fourier domain, our framework is mesh-based which provides discrete structural information and allows for information flow between long-range and short-range representations. Our fundamental concept is akin to these mesh-based methods mentioned in Section 2. We use short-range blocks on atoms to capture bonded terms and non-bonded short-range terms while applying long-range blocks on meshes to handle long-range terms. We enable the transfer of information between atoms and meshes via the representation assignment. A pseudocode for the Neural P\({}^{3}\)M block is provided in Appendix D.1 to enhance understanding. We further elaborate on the Neural P\({}^{3}\)M architecture as follows.

### Mesh Construction

Firstly, we construct meshes on which long-range interactions can be captured. In periodic systems such as crystals, the cell is naturally delineated. For non-periodic systems, we adopt the approach used by prevalent quantum chemistry software, which involves padding the bounding box with a specified margin to define the cell. Detailed information about the construction of the cell can be found in Appendix C. The coordinates of mesh points \(_{i,j,k}^{m}\) can be described as:

\[_{i,j,k}^{m}=+1/2}{N_{x}}_{x}++1/2}{N _{y}}_{y}++1/2}{N_{z}}_{z}\] (10)

where \(=[_{x},_{y},_{z}]^{}\) is the cell vector and \(N_{x},N_{y},N_{z}\) is the number of discretizations along each dimension. For convenience, we can regard meshes as a point cloud with a single subscript for the index as \(\{_{i}^{m}\}_{i}\).

### Embedding Block

Once coordinates of mesh points are established, we can proceed to construct a short-range atomic radius graph and a bipartite radius graph between atoms and meshes as follows:

\[^{}=\{e_{ij}:\|_{i}^{a}-_{j}^{a}\|_{ 2} r^{}, i,j\}.\] (11)

\[^{}=\{e_{ij}:\|_{i}^{a}-_{j}^{m} \|_{2} r^{}, i,j\}.\] (12)

where \(\) is the atom set and \(\) is the mesh set. Specifically, for periodic systems, the edges are also obtained by considering possible cross-boundary connections. The atom representation \(h_{i}^{0}\) is initialized as:

\[h_{i}^{0}=(z_{i})\] (13)

The initial mesh representation, denoted as \(m_{i}^{0}\), is obtained by averaging the representations of all neighboring atoms on the atom-mesh bipartite graph:

\[m_{i}^{0}=(i)|}_{j(i)}h_{j}^{0}\] (14)

where \((i)\) represents the set of neighboring nodes connected to mesh node \(i\) within \(^{}\). The edge features in both \(^{}\) and \(^{}\) can be expanded via a set of radial basis functions (RBF):

\[f_{ij}^{}=e^{}(\|_{i}^{a}-_{j}^{a} \|_{2}),f_{ij}^{}=e^{}(\|_{i}^{m}-_ {j}^{a}\|_{2})\] (15)

### Neural P\({}^{3}\)M Block

Short-range BlockThe short-range block (Fig.2(c)) updates the atomic representations using a graph neural network that is either SE(3)-equivariant or invariant. This process can be generally expressed as follows:

\[^{l}=(h^{l},^{},f^{})\] (16)

We noted that the usage of radius graphs inherits the localization assumptions in geometric GNNs and any node is only able to aggregation information from its direct geometric neighbors in one short-range block. Therefore, we naturally interpret it as capturing the short-range contribution to the energy and forces. As this part involves only atoms, we call such a module **Atom2Atom** which corresponds to the _particle-particle_ part (short-range term) in the P\({}^{3}\)M.

Figure 2: Overall framework architecture and details of each block. Geometric GNN models short-range interactions, Fourier neural operator (FNO) captures global long-range interactions, and continuous filter convolution (CFConv) exchanges information between two parts.

Long-range BlockThe long-range block (Fig.2(d)) updates mesh representations globally. Recalling Eq.9, the key aspect is to devise the influence function \(G\) and utilize FFT along with the convolution theorem for efficient computation of the convolution. Within our framework, we parameterize \(\) directly in the Fourier domain, and the updated mesh representations can be described as:

\[^{l}(W^{}m^{l}+(^{-1} ())(m^{l}))\] (17)

where \(,^{-1}\) are the Fourier transform and inverse Fourier transform on the discretized mesh, respectively. \(\) is the activation function. \(W^{}\) and \(\) are the learnable weights that parameterize the operator in the real space and Fourier space. If we consider \(m\) as a continuous function \(v(m)\), our formulation coincides with the Fourier neural operators (FNOs) on the discretized continuous function. Similarly, as the long-range block only involves interactions within meshes, we call it **Mesh2Mesh**.

Representation AssignmentThe representation assignment block (Fig.2(e)) allows for information flow between atom representations and mesh representations, effectively mixing short-range and long-range terms to obtain a more comprehensive descriptor of the molecule. By parameterizing the charge assignment function \(W\) in Eq.7 and substituting the charge density with the atom representation \(^{l}_{j}\), we can derive the continuous filter convolution (CFconv) proposed in SchNet . To elaborate further, we get additional mesh representations as:

\[(m a)^{l}_{i}=(_{j(i)}^ {l}_{j} W^{l}_{m a}f^{}_{ij})\] (18)

This **Atom2Mesh** module can be regarded as the information flow from the short-range part to the long-range part. Similarly, the **Mesh2Atom** module takes the same input and geometric graph but outputs additional atom representations \((a m)^{l}\), which could be viewed as the back-interpolation operation. It allows for the information flow in the inverse direction, from the long-range part to the short-range part. The long-range Mesh2Mesh module together with the Atom2Mesh and Mesh2Atom modules corresponds to the _particle-mesh_ part (long-range term) in the P\({}^{3}\)M.

Ultimately, as shown in Fig.2(b), we merge the information updated by each part itself with the normalized information received from the other part, and we also incorporate a residual connection to obtain the final output as:

\[h^{l+1}=h^{l}+^{l}+((a m)^{l})\] (19) \[m^{l+1}=m^{l}+^{l}+((m a)^{l})\] (20)

### Decoder Block

As we are interested in the prediction of molecule-level energies and atom-level forces, an additional decoder is applied to the final atom representations \(h^{L}\) and mesh representations \(m^{L}\) to get the atom-wise energies \(h^{}\) and mesh-wise energies \(m^{}\). We follow previous work to assume the additive property of energy to sum all atom-wise energies as the short part of the molecule energy \(^{}\).

\[^{}=_{j}h^{}_{j}=_{j }((h^{L}_{j}))\] (21)

We also sum all mesh-wise energies as the long part of the molecule energy \(^{}\).

\[^{}=_{j}m^{}_{j}=_{j }((m^{L}_{j}))\] (22)

The final potential energy is calculated as: \(=^{}+^{}\). Furthermore, although direct prediction of forces is possible, we instead use the negative gradient of the energy as the prediction of forces: \(=-_{}\). The final training objective is a weighted loss between energy and force:

\[=_{E}|E-|^{2}+}{3N}_{i=1}^{N} \|F_{i}+_{_{i}}\|^{2}\] (23)Experiment

### Experimental Setup

In this section, we conduct comprehensive validations of our Neural P\({}^{3}\)M framework using diverse datasets and configurations. First, we intuitively demonstrate the necessity of incorporating long-range interactions through a toy dataset Ag used in Allegro . Subsequently, we integrate various geometric GNNs [17; 9; 18; 8; 23] with our Neural P\({}^{3}\)M framework on two prevalent datasets OE62  and MD22  to demonstrate versatility and effectiveness. All results are evaluated using mean absolute error (MAE) on test sets, and the baseline results are sourced directly from the corresponding papers. Unless stated otherwise, almost all hyperparameters align with the baseline GNNs. For a more comprehensive overview of hyperparameter settings and implementation details, please refer to the Appendix D and F.

### Toy Dataset: Ag

The Ag dataset comprises 1,159 structures sampled from a 1,111K AIMD simulation . These structures were generated from a bulk face-centered-cubic lattice with a vacancy, encompassing 71 atoms subject to periodic boundary conditions. For consistency with Allegro, we randomly split them into 950 structures for training, 50 structures for validation and the remaining structures for testing. As shown in Fig. 3, compared to the strictly local Allegro model, ViSNet, which has only one layer, offers slightly improved force prediction, yet the energy prediction significantly deteriorates. This may be caused by the fact that the model can only perform message passing once, with a lack of long-range interactions. Long-range interactions can be complemented in theory by raising the cutoff from 4.0 A to 12.0 A, but this does not work in practice, because it could potentially lead to information over squashing problems, as mentioned in LSRM . When ViSNet with a single layer is integrated into our framework, long-range interactions can be effectively captured, significantly improving the accuracy of energy and force predictions compared to the vanilla ViSNet and Allegro. This toy experiment intuitively demonstrates the critical need to incorporate long-range interactions and emphasizes the significance of a well-crafted methodology in incorporating them.

### Md22

The MD22 dataset  consists of MD trajectory datasets, which present challenges due to their larger system sizes, ranging from 42 to 370 atoms. The number of structures in each molecule dataset ranges from 5,032 to 85,109. We calculate the diameter of each molecule, defined as the average of the maximum distance between any two atoms within a molecule. The smallest diameter observed is approximately 10.75 A, while the largest molecule measures about 32.39 A. We train a separate model for each molecule and randomly split the dataset according to sGDML .

Table 1 demonstrates the results of the ViSNet model incorporating with our Neural P\({}^{3}\)M framework (ViSNet-NP\({}^{3}\)M for short) on MD22. ViSNet-NP\({}^{3}\)M achieves the state-of-the-art (SoTA) performance on both energy and force predictions across the four largest molecules and also achieves the lowest mean absolute error (MAE) for energy or force predictions in the remaining three smaller molecules.

Figure 3: Mean absolute errors (MAEs) for energy and force predictions on Ag dataset are compared among Allegro, ViSNet, and our proposed framework.

When compared to the vanilla ViSNet, ViSNet-NP\({}^{3}\)M showed an average improvement of 34.6% and 21.2% in energy and force prediction, respectively. Notably, our framework exhibits a more substantial improvement when compared to ViSNet-LSRM and ViSNet-Ewald, both of which utilize ViSNet as the short-range model. As shown in Appendix Table 5, another state-of-the-art model, Equiformer, when integrated with our Neural P\({}^{3}\)M framework, also demonstrates significant enhancements to the short-range model itself. These impressive results highlight our framework's ability to effectively improve the learning of potential long-range interactions in large molecules.

It's worth noting that for the two supramolecules that cannot be fragmented by LSRM, our Neural P\({}^{3}\)M achieves a significant performance improvement in energy prediction, with a 57.48% increase for the double-walled nanotube and a 16.07% increase for the buckyball catcher. This suggests that our Neural P\({}^{3}\)M is a general solution for various molecules, which is not limited by traditional fragmentation methods like BRICS.

### Oe62

We further take our analysis by incorporating four prevailing geometric GNNs including SchNet , PaiNN , DimeNet++ , and GemNet-T  on the OE62 dataset  to confirm the framework's versatility. The OE62 dataset consists of about 62,000 large organic molecules, each with the energy calculated by Density Functional Theory (DFT). The structures within the OE62 dataset are non-periodic yet can span large spatial dimensions, exceeding 20 A. The dataset is strictly split into train, validation, and test set according to Ewald MP . The same dataset preprocessing process as Ewald MP is also applied.

The numerical results presented in Table 2 and Appendix Table 6 indicates that the Neural P\({}^{3}\)M framework, which combines four models, delivers more performance gains than Ewald MP and LSRM when using the same hyperparameters. Additionally, our framework exhibits a faster computation time than Ewald MP, likely due to the efficiency of FFT implementation by Pytorch. An unexpected observation is the speed performance of DimeNet++. Given that DimeNet++ does not inherently facilitate message passing between atom embeddings, Ewald MP compensates by integrating long-range interactions in each output block. In contrast, our approach exchanges short-range and long-range representations in each layer, which might account for our marginally slower speeds compared to Ewald MP. We also provide detailed profiling results for the number of model parameters, GPU memory usage, and other relevant metrics in Appendix G. For more details on the implementation on the four models, please refer to the Appendix D.

### Ablation Study

#### 4.5.1 Architecture

We first investigate the impact of the **Atom2Mesh** and **Mesh2Atom** modules. We remove the **Atom2Mesh** module from the original model to avoid the information flow from short-range blocks

   Molecule & Diameter (Å) &  s\&GIML \\  } &  SO\&RARATES \\  } &  &  &  &  \\   & & & & & & & & Baseline & Ewald & LSRM & Neural P\({}^{3}\)M \\  Ac-Ala3-NHMe & 10.75 &  energy \\ forces \\  & 0.3902 & 0.337 & 0.1019 & 0.0828 & **0.0230** & 0.0796 & 0.0775 & 0.0654 & 0.0719 \\  & & 0.7968 & 0.244 & 0.1068 & 0.0804 & 0.0867 & 0.0972 & 0.0814 & 0.0902 & **0.0788** \\  DHA & 14.58 &  energy \\ forces \\  & 1.3117 & 0.379 & 0.1153 & 0.1728 & 0.1317 & 0.1526 & 0.0932 & 0.0873 & **0.0712** \\  & & 0.7474 & 0.242 & 0.0732 & **0.0506** & 0.0646 & 0.0668 & 0.0664 & 0.0598 & 0.0679 \\  Stachyose & 13.87 &  energy \\ forces \\  & 0.4997 & 0.442 & 0.2485 & 0.140 & 0.1244 & 0.1283 & 0.1089 & 0.1055 & **0.0856** \\  & & 0.6744 & 0.435 & 0.0971 & **0.0635** & 0.0876 & 0.0896 & 0.0796 & 0.0767 & 0.0940 \\  AT-AT & 17.63 &  energy \\ forces \\  & 0.7235 & 0.178 & 0.1428 & 0.1309 & 0.1093 & 0.1688 & 0.1487 & 0.0772 & **0.0714** \\  & & 0.6911 & 0.216 & 0.0952 & 0.0660 & 0.0992 & 0.1070 & 0.0885 & 0.0781 & **0.0740** \\  AT-AT-CG-CG & 21.29 &  energy \\ forces \\  & 1.3885 & 0.345 & 0.3913 & 0.1510 & 0.1578 & 0.1995 & 0.1571 & 0.1135 & **0.1124** \\  & & 0.7028 & 0.332 & 0.1280 & 0.1252 & 0.1153 & 0.1563 & 0.1115 & 0.1063 & **0.0993** \\  Buckyball catcher & 15.89 &  energy \\ forces \\  & 1.1962 & 0.381 & 0.5258 & 0.3978 & 0.4812 & 0.4421 & 0.3575 & 0.4220 & **0.3543** \\  & & 0.6820 & 0.237 & 0.6887 & 0.1114 & 0.0853 & 0.1335 & 0.0909 & 0.1026 & **0.0846** \\  Double-walled nanotube & 32.39 & 
 energy \\ forces \\  & 4.0122 & 0.993 & 2.097 & 1.1945 & 0.1553 & 0.1339 & 0.7909 & 1.8230 & **0.7571** \\  & & 0.5231 & 0.727 & 0.3428 & 0.2747 & 0.2767 & 0.3959 & 0.2875 & 0.3391 & **0.2561** \\   

Table 1: Mean absolute errors (MAE) of energy (kcal/mol) and forces (kcal/mol/Å) for seven large molecules on MD22 compared with state-of-the-art models. The best one in each category is highlighted in **bold**.

to long-range blocks and vice versa. Table 3 demonstrates that both modules contribute synergistically to the model's overall performance. The results illustrate the necessity of enabling information exchange between the long-range and short-range blocks.

#### 4.5.2 Hyperparameters

Compared to the vanilla model, our framework introduces only two new hyperparameters: the assignment cutoff distance between mesh points and atoms, denoted as \(r^{}\), and the number of mesh points in each dimension, represented as \(N_{x},N_{y},N_{z}\).

We find that the selection of the number of mesh points is crucial for the model's final performance. As illustrated in Appendix Fig. 4(b), the mean absolute error (MAE) in energy increases with the number of mesh points, while the forward computation time also extends. This decline in performance may be attributed to instances where each atom is assigned to multiple mesh points simultaneously. As such occurrences become more frequent, the model may struggle to effectively learn the appropriate assignment rules. In practice, we typically set the cutoff distance to either 4.0 or 5.0 A, ensuring that the product of the number of mesh points and the cutoff distance is approximately equivalent to the cell size in each dimension.

Additionally, we provide further ablation studies on the impact of the assignment cutoff distance (without the k-NN graph) to examine the effects of multiple assignments. As shown in Appendix Fig. 4(c), all experiments exhibit a slight decrease in performance due to multiple assignments. However, an appropriately chosen cutoff (4 or 5 A) still yields relatively optimal results. Notably, the results do not worsen further as the assignment cutoff increases. We hypothesize that this may be because a larger assignment cutoff creates a broader neighborhood environment, facilitating the

    &  &  &  &  &  \\   & & MAE & Rel. & MAE & Rel. & Runtime & Rel. & Runtime & Rel. \\  & & \(\) & \(\%\) & \(\) & \(\%\) & ms/struct. \(\) & \(\%\) & ms/struct. \(\) & \(\%\) \\  SchNet & Baseline & 133.5 & - & 131.3 & - & **0.13** & - & **0.28** & - \\  & Embeddings & 144.7 & -8.4 & 136.7 & -4.1 & 0.14 & 15.2 & 0.33 & 17.8 \\  & Cutoff & 257.4 & -92.8 & 254.8 & -94.1 & 0.14 & 13.6 & 0.31 & 11.6 \\  & SchNet-LR & 86.6 & 35.1 & 89.2 & 32.1 & 0.32 & 156.0 & 0.75 & 171.7 \\  & Ewald & 79.2 & 40.7 & 81.1 & 38.2 & 0.70 & 461.6 & 1.03 & 271.4 \\  & Neural P\({}^{3}\)M & **70.2** & **47.4** & **69.1** & **47.4** & 0.37 & 184.6 & 0.57 & 103.6 \\  PaiNN & Baseline & 61.4 & - & 63.3 & - & **1.52** & - & **3.16** & - \\  & Embeddings & 63.5 & -3.4 & 63.1 & -0.2 & 1.54 & 1.4 & 3.28 & 3.8 \\  & Cutoff & 65.1 & -6.0 & 64.4 & -2.2 & 1.84 & 20.9 & 3.91 & 23.6 \\  & SchNet-LR & 58.3 & 5.1 & 58.2 & 7.7 & 1.84 & 20.7 & 4.21 & 33.1 \\  & Ewald & 57.9 & 5.7 & 59.7 & 5.7 & 2.29 & 50.5 & 4.57 & 44.4 \\  & Neural P\({}^{3}\)M & **54.1** & **11.9** & **52.9** & **16.4** & 2.17 & 42.8 & 4.19 & 32.6 \\  DimeNet++ & Baseline & 51.2 & - & 53.8 & - & **1.99** & - & **4.26** & - \\  & Embeddings & 50.4 & 1.6 & 53.4 & 0.7 & 2.25 & 12.9 & 4.93 & 15.8 \\  & Cutoff & 48.3 & 5.7 & 48.1 & 10.6 & 2.68 & 34.7 & 6.10 & 43.4 \\  & SchNet-LR & 51.4 & -0.5 & 54.4 & -1.1 & 2.37 & 19.0 & 4.73 & 11.2 \\  & Ewald & 46.5 & 9.2 & 48.1 & 10.6 & 2.70 & 35.5 & 5.93 & 39.5 \\  & Neural P\({}^{3}\)M & **40.9** & **20.1** & **41.5** & **22.9** & 3.11 & 56.3 & 5.62 & 31.9 \\  GemNet-T & Baseline & 51.5 & - & 53.1 & - & **3.07** & - & **6.96** & - \\  & Embeddings & 52.7 & -2.3 & 53.9 & -1.5 & 3.11 & 1.5 & 6.98 & 0.4 \\  & Cutoff & 47.8 & 7.2 & 47.7 & 10.2 & 4.02 & 31.2 & 8.88 & 27.7 \\  & SchNet-LR & 51.2 & 0.6 & 52.8 & 0.5 & 3.32 & 8.3 & 7.73 & 11.1 \\  & Ewald & 47.4 & 8.0 & 47.5 & 10.5 & 4.05 & 32.0 & 8.86 & 27.4 \\  & Neural P\({}^{3}\)M & **47.2** & **8.3** & **47.4** & **10.7** & 3.93 & 28.0 & 7.71 & 10.8 \\   

Table 2: Energy MAEs and computation times per input structure for the OE62 dataset compared with Ewald MP and other baseline methods. The data was sourced directly from .

   Architecture Variants & Energy MAE \\  Original & **69.10** \\ Without **Mesh2Atom** & 76.14 \\ Without **Atom2Mesh** & 74.48 \\ Without Both & 72.07 \\   

Table 3: Energy MAE of SchNet-NP\({}^{3}\)M variants on the OE62 test dataset. The best one is highlighted in **bold**.

learning of the assignment function with a fixed number of meshes, thereby alleviating the challenges associated with multiple assignments.

## 5 Related Work

Geometric Graph Neural NetworksGeometric graph neural networks preserve equivariance toward the rigid transformation in space, which can be categorized according to their emphasis on specific types of structural features and their respective methods of integration. SchNet  stands out as the pioneering approach to applying continuous filter convolution on molecular distances. Subsequently, DimeNet++  and GemNet  explicitly incorporate angles and dihedrals using Fourier-Bessel functions. To address the computational complexity associated with angles extractions, PaiNN  and ViSNet  adopt the density trick and reduce the complexity to linear time. Additionally, many works are based on high-order geometric tensors [2; 1; 16; 22], which ensure rigorous theoretical guarantees of equivariance through the use of Clebsch-Gordan product. Despite these advancements, all these existing methods are constrained to the local atomic environment, and are unable to approximate the long-range interactions. Hence, there is an urgent need for a comprehensive framework to address this challenge.

Long-range Interaction ModelingIncorporating long-range interactions into a short-range model is challenging. Early studies attempted to compensate these long-range effects by integrating physical equations with either hand-crafted terms  or predicted charges . While, recent works have shifted towards creating carefully designed models that can directly learn long-range interactions from data. The LSRM framework , for instance, captures long-range interactions in real space by using specific algorithms to fragment molecules into discrete groups and models their interactions hierarchically. Other methods [12; 24; 15] handle long-range components in reciprocal space, employing concepts like Ewald summation . Our approach differs from these works by introducing the discretized meshes and facilitating the exchange of information between long-range and short-range components.

## 6 Conclusion

In this paper, we introduce a novel framework, termed Neural P\({}^{3}\)M, designed to enhance the long-range interaction modeling for various geometric GNNs. In addition, Neural P\({}^{3}\)M stands out by not being confined to any specific fragmentation approach, making it adaptable to various molecular systems. Neural P\({}^{3}\)M achieves significant performance improvement on prevalent benchmarks by capturing short-range and long-range interactions at both atom and mesh scales, and enabling the exchange of information between them.

**Limitation and Societal Impacts:** The limitation of our study is that it does not thoroughly investigate the impact of the number of meshes, nor does it explore potentially more effective methods for modeling long-range interactions beyond FFT. Nonetheless, our paper offers the community a fresh perspective on molecular geometry modeling. Our proposed Neural P\({}^{3}\)M framework is an extensive of existing geometric GNNs for energy and force prediction of molecules. The prediction of molecular energies and forces has diverse applications in downstream tasks including molecular dynamics simulation and molecular property prediction. As our framework better captures the long-range interaction within the molecule, it can potentially accelerate the pharmaceutical discovery and understanding of diverse molecules that have positive impacts on treating diseases. On the other hand, we are also aware of the potential negative impact if the model is misused, as our understanding of different molecules is still very limited. We will work closely with both the machine learning and the science community to ensure the proper usage of our model for the good of society.

## 7 Acknowledgments and Disclosure of Funding

We thank the reviewers for their valuable comments. This work was supported by NSFC under grant No. 62088102.