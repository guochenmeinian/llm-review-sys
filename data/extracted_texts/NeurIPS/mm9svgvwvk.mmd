# A Causal Framework for

Decomposing Spurious Variations

 Drago Plecko

 Elias Bareinboim

Department of Computer Science

Columbia University

dp3144@columbia.edu, eb@cs.columbia.edu

###### Abstract

One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable \(X\) exerts influences over another variable \(Y\). In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Markovian models. We prove the first results that allow a non-parametric decomposition of spurious effects and provide sufficient conditions for the identification of such decompositions. The described approach has several applications, ranging from explainable and fair AI to questions in epidemiology and medicine, and we empirically demonstrate its use.

## 1 Introduction

Understanding the relationships of cause and effect is one of the core tenets of scientific inquiry and the human ability to explain why events occurred in the way they did. Hypotheses on possible causal relations in the sciences are often generated based on observing correlations in the world, after which a rigorous process using either observational or experimental data is employed to ascertain whether the observed relationships are indeed causal. One common way of articulating questions of causation is through the average treatment effect (ATE), also known as the total effect (TE), given by

\[[y do(x_{1})]-[y do(x_{0})],\] (1)

where \(do()\) symbolizes the do-operator , and \(x_{0},x_{1}\) are two distinct values attained by the variable \(X\). Instead of just quantifying the causal effect, researchers are more broadly interested in determining which causal mechanisms transmit the change from \(X\) to \(Y\). Such questions have received much attention and have been investigated under the rubric of causal mediation analysis .

Often, however, the causal relationship may be entirely absent or account only for a part of the initially observed correlation. In these cases, the spurious (or confounded) variations between \(X\) and \(Y\) play a central role in explaining the phenomenon at hand. Interestingly, though, tools for decomposing spurious variations are almost entirely missing from the literature in causal inference 1.

Phenomena in which spurious variations are of central importance are abundant throughout the sciences. For instance, in medicine, the phenomenon called the _obesity paradox_ signifies the counter-intuitive association of increased body fat with better survival chances in the intensive care unit (ICU) . While the full explanation is still unclear, evidence in the literature suggests that the relationship is not causal , i.e., it is explained by spurious variations. Spurious variations also play a central role in many epidemiological investigations . In occupational epidemiology, for example, the relationship of exposure to hazardous materials with cancer is confounded by other hazardous working conditions and lifestyle characteristics , and such spurious variations themselves may be the target of scientific inquiry. Quantities that measure such spurious variations (or a subset thereof) are called spurious effects in this paper.

Spurious variations are key in applications of fair and explainable AI as well. For instance, consider the widely recognized phenomenon in the literature known as _redlining_[15; 7], in which the location where loan applicants live may correlate with their race. Applications might be rejected based on the zip code, disproportionately affecting certain minority groups. Furthermore, in the context of criminal justice , the association of race with increased probability of being classified as high-risk for recidivism may in part be explained by the spurious association of race with other demographic characteristics (we take a closer look at this issue in Sec. 5). Understanding which confounders affect the relationship, and how strongly, is an important step of explaining the phenomenon, and also determining whether the underlying classifier is deemed as unfair and discriminatory.

These examples suggest that a principled approach for decomposing spurious variations may be a useful addition to the general toolkit of causal inference, and may find its applications in a wide range of settings from medicine and public health all the way to fair and explainable AI. For concreteness, in this paper we will consider the quantity

\[P(y x)-P(y do(x)),\]

which we will call the _experimental spurious effect_ (Exp-SE, for short). This quantity, shown graphically in Fig. 1, captures the difference in variations when observing \(X=x\) vs. intervening that \(X=x\), which can be seen as the spurious counterpart of the total effect. Interestingly, the Exp-SE quantity is sometimes evoked in the causal inference literature, i.e.,

\[P(y x)-P(y do(x))=0\] (2)

is known as the _zero-bias_ condition [2; 9, Ch. 6]. This condition allows one to test for the existence of confounding between the variables \(X\) and \(Y\). A crucial observation is that, in many cases, the quantity itself may be of interest (instead of only its _null_), as it underpins the spurious variations.

Against this background, we note that tools that allow for decomposing the Exp-SE quantity currently do not exist in the literature. Our goal in this manuscript is to fill in this gap, and provide a formalism that allows for non-parametric decompositions of spurious variations. Specifically, our contributions are the following:

1. We introduce the notion of a partially abducted submodel (Def. 1), which underpins the inference procedure called Partial Abduction and Prediction (Alg. 2) (akin to Balke & Pearl 3-step procedure [9, Ch. 7]). Building on this new primitive, we prove the first non-parametric decomposition result for spurious effects in Markovian models (Thm. 1),
2. Building on the insights coming from the new procedure, we prove the decomposition result for settings when unobserved confounding is present (Semi-Markovian models) (Thm. 3).
3. We develop sufficient conditions for identification of spurious decompositions (Thm 2, 4).

## 2 Preliminaries

We use the language of structural causal models (SCMs) as our basic semantical framework . A structural causal model (SCM) is a tuple \(:= V,U,,P(u)\), where \(V\), \(U\) are sets of endogenous (observables) and exogenous (latent) variables respectively, \(\) is a set of functions \(f_{V_{i}}\), one for each \(V_{i} V\), where \(V_{i} f_{V_{i}}((V_{i}),U_{V_{i}})\) for some \((V_{i}) V\) and \(U_{V_{i}} U\). \(P(u)\) is a strictly positive probability measure over \(U\). Each SCM \(\) is associated to a causal diagram \(\) over

Figure 1: Exp-SE representation.

the node set \(V\) where \(V_{i} V_{j}\) if \(V_{i}\) is an argument of \(f_{V_{j}}\), and \(V_{i}V_{j}\) if the corresponding \(U_{V_{i}},U_{V_{j}}\) are not independent . A model with no bidirected edges is called _Markovian_, while a model with bidirected edges is called Semi-Markovian. An instantiation of the exogenous variables \(U=u\) is called a _unit_. By \(Y_{x}(u)\) we denote the potential response of \(Y\) when setting \(X=x\) for the unit \(u\), which is the solution for \(Y(u)\) to the set of equations obtained by evaluating the unit \(u\) in the submodel \(_{x}\), in which all equations in \(\) associated with \(X\) are replaced by \(X=x\). In a slight abuse of notation, we also replace \(Y=y\) with just \(y\) whenever the former is clear from the context. We next introduce an important inferential procedure for solving different tasks in causal inference.

### Abduction, Action and Prediction

The steps of the _abduction-action-prediction_ method can be summarized as follows:

**Algorithm 1** (Abduction, Action and Prediction ).: _Given an SCM \(,P(u)\), the conditional probability \(P(Y_{C} E=e)\) of a counterfactual sentence "if it were \(C\) then \(Y\)", upon observing the evidence \(E=e\), can be evaluated using the following three steps:_

1. _Abduction - update_ \(P(u)\) _by the evidence_ \(e\) _to obtain_ \(P(u e)\)_,_
2. _Action - modify_ \(\) _by the action_ \(do(C)\)_, where_ \(C\) _is an antecedent of_ \(Y\)_, to obtain_ \(_{C}\)_,_
3. _Prediction - use the model_ \(_{C},P(u e)\) _to compute the probability of_ \(Y_{C}\)_._

In the first step, the probabilities of the exogenous variables \(U\) are updated according to the observed evidence \(E=e\). Next, the model \(\) is modified to a submodel \(_{C}\). The action step allows one to consider queries related to interventions or imaginative, counterfactual operations. In the final step, the updated model \(_{C},P(u e)\) is used to compute the conditional probability \(P(y_{C} e)\). There are two important special cases of the procedure. Whenever the action step is empty, the procedure handles queries in the first, associational layer of the Pearl's Causal Hierarchy (PCH, ). Whenever the abduction step is empty, but the action step is not, the procedure handles _interventional_ queries in the second layer of the PCH. The combination of the two steps, more generally, allows one to consider queries in all layers of the PCH, including the third, _counterfactual_ layer. In the following example, we look at the usage of the procedure on some queries.

**Example 1** (Abduction, Action, Prediction).: _Consider the following SCM:_

\[:\{   & f_{X}(U_{X},U_{XZ})\\ Z& f_{Z}(U_{Z},U_{XZ})\\ Y& f_{Y}(X,Z,U_{Y}),.\] (3) (4) \[:\{  & f_{X}(U_{X},U_{XZ})\\ Z& f_{Z}(U_{Z},U_{XZ})\\ Y& f_{Y}(X,Z,U_{Y}),.\] (5)

_with \(P(U_{X},U_{XZ},U_{Z},U_{Y})\) the distribution over the exogenous variables. The causal diagram of the model is shown in Fig. 1(a), with an explicit representation of the exogenous variables in Fig. 1(b)._

_We are first interested in the query \(P(y x)\) in the given model. Based on the abduction-prediction procedure, we can simply compute that:_

\[P(y x)=_{u}(Y(u)=y)P(u x)=_{u}(Y(u)=y)P(u _{z},u_{y})P(u_{x},u_{xz} x).\] (6)

_where the first step follows from the definition of the observational distribution, and the second step follows from noting the independence \(U_{Z},U_{Y} U_{X},U_{XZ},X\). In the abduction step, we can compute the probabilities \(P(u_{x},u_{xz} x)\). In the prediction step, query \(P(y x)\) is computed based on Eq. 6._

Figure 2: Graphical representations of the SCM in Ex. 1.

_Based on the procedure, we can also compute the query \(P(y_{x})\) (see Fig. 2c):_

\[P(y_{x})=_{u}(Y_{x}(u)=y)P(u)=_{u}(Y(x,u_{xz},u_{z},u_ {y})=y)P(u).\] (7)

_where the first step follows from the definition of an interventional distribution, and the second step follows from noting that \(Y_{x}\) does not depend on \(u_{x}\). In this case, the abduction step is void, since we are not considering any specific evidence \(E=e\). The value of \(Y(x,u_{xz},u_{z},u_{y})\) can be computed from the submodel \(_{x}\). Finally, using Eq. 7 we can perform the prediction step. We remark that_

\[(Y(x,u_{xz},u_{z},u_{y})=y)=_{u_{x}}(Y(u_{x},u_{xz},u_ {z},u_{y})=y)P(u_{x} x,u_{xz},u_{z},u_{y}),\] (8)

_by the law of total probability and noting that \(X\) is a deterministic function of \(u_{x},u_{xz}\). Thus, \(P(y_{x})\) also admits an alternative representation_

\[P(y_{x}) =_{u}(Y(u_{x},u_{xz},u_{z},u_{y})=y)P(u_{x} x, u_{xz},u_{z},u_{y})P(u_{xz},u_{z},u_{y})\] (9) \[=_{u}(Y(u)=y)P(u_{x} x,u_{xz})P(u_{xz},u_{z},u_ {y}),\] (10)

_where Eq. 10 follows from using the independencies among \(U\) and \(X\) in the graph in Fig. 2b. We revisit the representation in Eq. 10 in Ex. 2._

## 3 Foundations of Decomposing Spurious Variations

After getting familiar with the abduction-action-prediction procedure, our next task is to introduce a new procedure that allows us to decompose spurious effects. First, we define the concept of a _partially abducted submodel_:

**Definition 1** (Partially Abducted Submodel).: _Let \(U_{1},U_{2} U\) be a partition of the exogenous variables. Let the partially abducted (PA, for short) submodel with respect to the exogenous variables \(U_{1}\) and evidence \(E=e\) be defined as:_

\[^{U_{1},E=e}:=,P(u_{1})P(u_{2} u_{1},E).\] (11)

In words, in the PA submodel, the typically obtained posterior distribution \(P(u e)\) is replaced by the distribution \(P(u_{2} u_{1},e)\). Effectively, the exogenous variables \(U_{1}\) are _not updated according to evidence_. The main motivation for introducing the PA model is that spurious variations arise whenever we are comparing units of the population that are different, a realization dating back to Pearson in the 19th century . To give a formal discussion on what became known as _Pearson's shock_, consider two sets of differing evidence \(E=e\) and \(E=e^{}\). After performing the abduction step, the variations between posterior distributions \(P(u e)\) and \(P(u e^{})\) will be explained by _all the exogenous variables that precede the evidence \(E\)_. In a PA submodel, however, the posterior distribution \(P(u_{1})P(u_{2} u_{1},e)\) will differ from \(P(u_{1})P(u_{2} u_{1},e^{})\) only in variables that are in \(U_{2}\), while the variables in \(U_{1}\) will induce no spurious variations. Note that if \(U_{1}=U\), then the PA submodel will introduce no spurious variations, a point to which we return in the sequel.

We now demonstrate how the definition of a PA submodel can be used to obtain partially abducted conditional probabilities:

**Proposition 1** (PA Conditional Probabilities).: _Let \(P(Y=y E=e^{U_{1}})\) denote the conditional probability of the event \(Y=y\) conditional on evidence \(E=e\), defined as the probability of \(Y=y\) in the PA submodel \(^{U_{1},E=e}\) (i.e., the exogenous variables \(U_{1}\) are not updated according to the evidence). Then, we have that:_

\[P(Y=y E=e^{U_{1}})=_{u_{1}}P(U_{1}=u_{1})P(Y=y E=e,U_{1}=u_{1}).\] (12)

### Partial Abduction and Prediction

Based on the notion of a PA submodel, we can introduce the partial-abduction and prediction procedure:

**Algorithm 2** (Partial Abduction and Prediction).: _Given an SCM \(,P(u)\), the conditional probability \(P(Y=y E=e^{U_{1}})\) of an event \(Y=y\) upon observing the evidence \(e\), in a world where variables \(U_{1}\) are unresponsive to evidence, can be evaluated using the following two steps:_

1. _Partial Abduction_ _- update_ \(P(u)\) _by the evidence_ \(e\) _to obtain_ \(P(u_{1})P(u_{2} u_{1},e)\)_, where_ \((u_{1},u_{2})\) _is a partition of the exogenous variables_ \(u\)_,_
2. _Prediction_ _- use the model_ \(,P(u_{1})P(u_{2} u_{1},e)\) _to compute the probability of_ \(Y=y\)_._

In the first step of the algorithm, we only perform _partial abduction_. The exogenous variables \(U_{2}\) are updated according to the available evidence \(E=e\), while the variables \(U_{1}\) retain their original distribution \(P(u_{1})\) and remain unresponsive to evidence. This procedure allows us to consider queries in which only a subset of the exogenous variables respond to the available evidence. We next explain what kind of queries fall within this scope, beginning with an example:

**Example 2** (Partial Abduction and Prediction).: _Consider the model in Eq. 3-5. We are interested in computing the query:_

\[P(y x^{U_{xz},U_{z}}) =_{u}(Y(u)=y)P(u_{xz},u_{z})P(u_{x},u_{y} u_{xz },u_{x},x)\] (13) \[=_{u}(Y(u)=y)P(u_{xz},u_{z})P(u_{y})P(u_{x} u_{ xz},u_{x},x)\] (14) \[=_{u}(Y(u)=y)P(u_{xz},u_{z},u_{y})P(u_{x} u_{xz },u_{x},x),\] (15)

_where the first step follows from Prop. 1, and the remaining steps from conditional independencies between the \(U\) variables and \(X\). Crucially, the query yields the same expression as in Eq. 10 that we obtained for \(P(y_{x})\) in Ex. 1. Therefore, the conditional probability \(P(y x^{U_{xz},U_{z}})\) in a world where \(U_{XZ},U_{Z}\) are unresponsive to evidence is equal to the interventional probability \(P(y_{x})\)._

As the example illustrates, we have managed to find another procedure that mimics the behavior of the interventional (\(do(X=x)\)) operator in the given example. Interestingly, however, in this procedure, we have not made use of the submodel \(_{x}\) that was used in the abduction-action-prediction procedure. We next introduce an additional example that shows how the new procedure allows one to decompose spurious variations in causal models:

**Example 3** (Spurious Decomposition).: _Consider an SCM compatible with the graphical representation in Fig. 2(b) (with exogenous variables \(U\) shown explicitly in red), and the corresponding Semi-Markovian causal diagram in Fig. 2(a). We note that, based on the partial abduction-prediction procedure, the following two equalities hold:_

\[P(y x) =P(y x^{})\] (16) \[P(y_{x}) =P(y x^{U_{xz_{1}},U_{xz_{2}}}),\] (17)

_which shows that_

\[_{x}(y)=P(y x^{})-P(y x^{U_{xz_{1}},U_{xz_{2}}}).\] (18)

_The experimental spurious effect can be written as a difference of conditional probabilities \(y x\) in a world where all variables \(U\) are responsive to evidence vs. a world in which \(U_{XZ_{1}},U_{XZ_{2}}\) are

Figure 3: Graphical representations of the SCM in Ex. 1.

unresponsive to evidence. Furthermore, we can also consider a refinement that decomposes the effect_

\[_{x}(y)=)-P(y x^{U_{xz_{1}}})} _{U_{xz_{1}}}+}})-P(y x^{U_{xz_{1}}},U_{ xz_{2}})}_{U_{xz_{2}}},\] (19)

_allowing for an additive, non-parametric decomposition of the experimental spurious effect._

The first term in Eq. 19, shown in Fig. 7(a), encompasses spurious variations explained by the variable \(U_{XZ_{1}}\). The second term, in Fig. 3(b), encompasses spurious variations explained by \(U_{XZ_{2}}\).

For an overview, in Tab. 1 we summarize the different inferential procedures discussed so far, indicating the structural causal models associated with them.

## 4 Non-parametric Spurious Decompositions

We now move on to deriving general decomposition results for the spurious effects. Before doing so, we first derive a new decomposition result for the TV measure, not yet appearing in the literature (due to space constraints, all proofs are given in Appendix A):

**Proposition 2**.: _Define the total variation (TV) measure as \(_{x_{0},x_{1}}(y)=P(y x_{1})-P(y x_{0})\), and the total effect TE as \(_{x_{0},x_{1}}(y)=P(y_{x_{1}})-P(y_{x_{0}})\). The total variation measure can be decomposed as:_

\[_{x_{0},x_{1}}(y)=_{x_{0},x_{1}}(y)+(_{x_{1}}( y)-_{x_{0}}(y)).\] (20)

The above result clearly separates out the causal variations (measured by the TE) and the spurious variations (measured by Exp-SE terms) within the TV measure. The seminal result from  can be used to further decompose the TE measure. In the sequel, we show how the Exp-SE terms can be further decomposed, thereby reaching a full non-parametric decomposition of the TV measure.

### Spurious Decompositions for the Markovian case

When using the definition of a PA submodel, the common variations between \(X,Y\) can be attributed to (or explained by) the unobserved confounders \(U_{1},,U_{k}\). In order to do so, we first define the notion of an experimental spurious effect for a set of latent variables:

**Definition 2** (Spurious effects for Markovian models).: _Let \(\) be a Markovian model. Let \(Z_{1},,Z_{k}\) be the confounders between variables \(X\) and \(Y\) sorted in any valid topological order, and denote the corresponding exogenous variables as \(U_{1},,U_{k}\), respectively. Let \(Z_{[i]}=\{Z_{1},,Z_{i}\}\) and

  Procedure & SCM & Queries \\  Abduction-Prediction & \(,P(u E)\) & Layer 1 \\  Action-Prediction & \(_{x},P(u)\) & Layer 2 \\  Abduction-Action-Prediction & \(_{x},P(u E)\) & Layers 1, 2, 3 \\  Partial Abduction-Prediction & \(,P(u_{1})P(u_{2} E)\) & Layers 1, 2, 3 \\  

Table 1: Summary of the different procedures and the corresponding probabilistic causal models.

Figure 4: Graphical representation of how the Exp-SE effect is decomposed in Ex. 3.

\(Z_{-[i]}=\{Z_{i+1},,Z_{k}\}\). \(U_{[i]}\) and \(U_{-[i]}\) are defined analogously. Define the experimental spurious effect associated with variable \(U_{i+1}\) as_

\[_{x}^{U_{[i]},U_{[i+1]}}(y)=P(y x^{U_{[i]}})-P(y x^{U_{[i+1 ]}}).\] (21)

The intuition behind the quantity \(_{x}^{U_{[i]},U_{[i+1]}}(y)\) can be explained as follows. The quantity \(P(y x^{U_{[i]}})\) captures all the variations in \(Y\) induced by observing that \(X=x\) apart from those explained by the latent variables \(U_{1},,U_{i}\), which are fixed a priori and not updated. Similarly, the quantity \(P(y x^{U_{[i+1]}})\) captures the variations in \(Y\) induced by observing that \(X=x\), apart from those explained by \(U_{1},,U_{i},U_{i+1}\). Therefore, taking the difference of the two quantities measures the variation in \(Y\) induced by observing that \(X=x\) that is explained by the latent variable \(U_{i+1}\).

Based on this definition, we can derive the first key non-parametric decomposition of the experimental spurious effect that allows the attribution of the spurious variations to the latent variables \(U_{i}\):

**Theorem 1** (Latent spurious decomposition for Markovian models).: _The experimental spurious effect \(_{x}(y)\) can be decomposed into latent variable-specific contributions as follows:_

\[_{x}(y)=_{i=0}^{k-1}_{x}^{U_{[i]},U_{[i+1]}}(y) =_{i=0}^{k-1}P(y x^{U_{[i]}})-P(y x^{U_{[i+1]}}).\] (22)

An illustrative example of applying the theorem is shown in Appendix B.1. Thm. 1 allows one to attribute spurious variations to latent variables influencing both \(X\) and \(Y\). The key question is when such an attribution, as shown in Eq. 22, can be computed from observational data in practice (known as an _identifiability_ problem ). In fact, when variables are added to the PA submodel in topological order, the attribution of variations to the latents \(U_{i}\) is identifiable, as we prove next:

**Theorem 2** (Spurious decomposition identification in topological ordering).: _The quantity \(P(y x^{U_{[i]}})\) can be computed from observational data using the expression_

\[P(y x^{U_{[i]}})=_{z}P(y z,x)P(z_{-[i]} z_{[i]},x)P(z_{[i]}),\] (23)

_rendering each term of decomposition in Eq. 22 identifiable from the observational distribution \(P(v)\)._

We discuss in Appendix B.2 why a decomposition that does not follow a topological order of the variables \(U_{i}\) is not identifiable.

### Spurious Decompositions in Semi-Markovian Models

In the Markovian case, considered until now, there was a one-to-one correspondence between the observed confounders \(Z_{i}\) and their latent variables \(U_{i}\). This, however, is no longer the case in Semi-Markovian models. In particular, it can happen that there exist exogenous variables \(U_{j}\) that induce common variations between \(X,Y\), but affect more than one confounder \(Z_{i}\). We are interested in \(U_{j} U\) that have causal (directed) paths to both \(X,Y\), described by the following definition:

**Definition 3** (Trek).: _Let \(\) be an SCM corresponding to a Semi-Markovian model. Let \(\) be the causal diagram of \(\). A trek \(\) in \(\) (from \(X\) to \(Y\)) is an ordered pair of causal paths (\(g_{l}\), \(g_{r}\)) with a common exogenous source \(U_{i} U\). That is, \(g_{l}\) is a causal path \(U_{i} X\) and \(g_{r}\) is a causal path \(U_{i} Y\). The common source \(U_{i}\) is called the top of the trek (ToT for short), denoted \(top(g_{l},g_{r})\). A trek is called spurious if \(g_{r}\) is a causal path from \(U_{i}\) to \(Y\) that is not intercepted by \(X\)._

When decomposing spurious effects, we are in fact interested in all the exogenous variables \(U_{i}\) that lie on top of a spurious trek between \(X\) and \(Y\). It is precisely these exogenous variables that induce common variations between \(X\) and \(Y\). Using any subset of the variables that are top of spurious treks, we define a set-specific notion of a spurious effect:

**Definition 4** (Exogenous set-specific spurious effect).: _Let \(U_{sToT} U\) be the subset of exogenous variables that lie on top of a spurious trek between \(X\) and \(Y\). Suppose \(A,B U_{sToT}\) are two nested subsets of \(U_{sToT}\), that is \(A B\). We then define the exogenous experimental spurious effect with respect to sets \(A,B\) as_

\[_{x}^{A,B}(y)=P(y x^{A})-P(y x^{B}).\] (24)he above definition is analogous to Def. 2, but we are now fixing different subsets of the tops of spurious treks. Def. 2 supports partial abduction of exogenous variables that are not on top of a spurious trek, but we are seldom interested in these since they do not induce covariations of \(X,Y\). The quantity \(_{x}^{A,B}(y)\) is presented as a graphical contrast in Fig. 5. In particular, the set of tops of spurious treks \(U_{sToT}\) is partitioned into three parts \((U_{A},U_{B A},U_{B^{C}})\). The causal diagram in the figure is informal, and the dots \(()\) represent arbitrary possible observed confounders that lie along indicated pathways. On the l.h.s. of the figure, the set \(U_{A}\) does not respond to the conditioning \(X=x\), whereas \(U_{B A},U_{B^{C}}\) do. This is contrasted with the r.h.s., in which neither \(U_{A}\) nor \(U_{B A}\) respond to \(X=x\), whereas \(U_{B^{C}}\) still does respond to the \(X=x\) conditioning. The described contrast thus captures the spurious effect explained by the tops of spurious treks in \(U_{B A}\).

Analogous to Thm. 1, we next state a variable-specific decomposition of the spurious effect, which is now with respect to exogenous variables that are top of spurious treks:

**Theorem 3** (Semi-Markovian spurious decomposition).: _Let \(U_{sToT}=\{U_{1},,U_{m}\} U\) be the subset of exogenous variables that lie on top of a spurious trek between \(X\) and \(Y\). Let \(U_{[i]}\) denote the variables \(U_{1},,U_{i}\) (\(U_{}\) denotes the empty set \(\)). The experimental spurious effect \(_{x}(y)\) can be decomposed into variable-specific contributions as follows:_

\[_{x}(y)=_{i=0}^{m-1}_{x}^{U_{[i]},U_{[i+1]}}(y) =_{i=0}^{k-1}P(y x^{U_{[i]}})-P(y x^{U_{[i+1]}}).\] (25)

An example demonstrating the Semi-Markovian decomposition is given in Appendix B.3. We next discuss the question of identification. We begin by discussing how to annotate the exogenous variables given a Semi-Markovian causal diagram:

**Definition 5** (Top of trek from the causal diagram).: _Let \(\) be a Semi-Markovian model and let \(\) be the associated causal diagram. A set of nodes fully connected with bidirected edges is called a clique. A maximal clique \(C_{i}\) is such that there is no clique \(C_{i}^{}\) such that \(C_{i} C_{i}^{}\). The set of variables \(U_{sToT}\) can be constructed from the causal diagram in the following way:_

1. _initialize_ \(U_{sToT}=\)_,_
2. _for each maximal clique_ \(C_{i}\)_, consider the associated exogenous variable_ \(U_{C_{i}}\) _pointing to each node in the clique; if there exists a spurious trek between_ \(X\) _and_ \(Y\) _with a top in_ \(U_{C_{i}}\)_, add_ \(U_{C_{i}}\) _to_ \(U_{sToT}\)_._

After defining the explicit construction of the set \(U_{sToT}\), we define the notion of the anchor set:

**Definition 6** (Anchor Set).: _Let \(U_{1}, U_{l} U\) be a subset of the exogenous variables. We define the anchor set \((U_{1},,U_{l})\) of \((U_{1},,U_{l})\) as the subset of observables \(V\) that are directly influenced by any of the \(U_{i}\)s,_

\[(U_{1},,U_{l})=_{i=1}^{l}(U_{i}).\] (26)

Another important definition is that of anchor set exogenous ancestral closure:

Figure 5: Quantity \(_{x}^{A,B}(y)\) as a graphical contrast. Dots \(\) indicate arbitrary observed confounders along the indicated pathway.

**Definition 7** (Anchor Set Exogenous Ancestral Closure).: _Let \(U_{s} U\) be a subset of the exogenous variables. Let \((U_{s})\) denote the anchor set of \(U_{s}\), and let \(^{}(AS(U_{s}))\) denote all exogenous variables that have a causal path to any variable in \((U_{s})\). \(U_{s}\) is said to satisfy anchor set exogenous ancestral closure (ASEAC) if_

\[U_{s}=^{}(AS(U_{s})).\] (27)

Based on the above, we provide a sufficient condition for identification in the Semi-Markovian case:

**Theorem 4** (ID of variable spurious effects in Semi-Markovian models).: _Let \(U_{s} U_{sToT}\). The quantity \(P(y x^{U_{s}})\) is identifiable from observational data \(P(V)\) if the following hold:_

1. \(X(U_{s})\)_,_ \(Y(U_{s})\)__
2. \(U_{s}\) _satisfies anchor set exogenous ancestral closure,_ \(U_{s}=^{}(AS(U_{s}))\)_._

Some instructive examples grounding Defs. 5-7 and Thm. 4 can be found in Appendix B.4. In words, the conditional expectation of \(Y\) given \(X\) in the partially abducted submodel w.r.t. a set \(U_{s}\) is identifiable whenever (i) neither \(X\) nor \(Y\) are elements of the anchor set of \(U_{s}\) and (ii) the set \(U_{s}\) satisfies the anchor set exogenous ancestral closure. Thm. 4 provides a sufficient, but not a necessary condition for identification. An additional discussion of the conditions is given in Appendix C. We hope to address in future work an algorithmic way for identifying spurious effects in full generality.

## 5 Experiments

We now apply our framework to a synthetic example (called Synthetic A) with a known ground truth, summarized in Tab. 2 where the SCM \(\) and the causal diagram \(\) are given. The source code for the experiment can be found in our repository. For this example, we set the parameters \(_{1}=_{2}=_{3}=0.2\). We then vary each parameter \(_{i}[0,0.2]\) (while keeping the other two parameters fixed), which changes the value of the effect associated with latent variable \(U_{i}\). The effects associated with each \(U_{i},i\{1,2,3\}\) are computed based on the decomposition in Thm. 1:

\[^{U_{i}}_{x}(y) :=^{,U_{i}}_{x}(y)=P(y x^{})-P (y x^{U_{1}})\] (28) \[^{U_{2}}_{x}(y) :=^{U_{1},\{U_{1},U_{2}\}}_{x}(y)=P(y x^{U_{1}})- P(y x^{U_{1},U_{2}})\] (29) \[^{U_{3}}_{x}(y) :=^{\{U_{1},U_{2}\},\{U_{1},U_{2},U_{3}\}}_{x}(y)=P(y  x^{U_{1},U_{2}})-P(y x^{U_{1},U_{2},U_{3}}).\] (30)

The key task is to compute the ground truth values of \(P(y x^{U_{[i]}})\) for different values of \(i\). According to Def. 1, we want to obtain the conditional distribution of \(Y\) given \(X=x\) but subject to not updating \(U_{[i]}\) according to the evidence \(X=x\). Based on the true SCM, this can be done efficiently using rejection sampling as follows:

1. Take \(N\) samples from the SCM \(\) in Tab. 2,
2. For all samples \(k\{1,,N\}\) with \(u^{(k)}\) such that \[X(u^{(k)}) x,\] (31) re-sample the part of the unit \(u^{(k)}\) that is not included in \(U_{[i]}\) (e.g., if \(U_{[i]}=\{U_{1},U_{2}\}\), latent \(u^{(k)}_{1},u^{(k)}_{2}\) are not re-sampled but \(u^{(k)}_{3}\) is) and replace \(u^{(k)}\) with this new sample,

 SCM \(\) & Causal Diagram \(\) \\  \(Z_{1} B(0.5)\) & \\ \(Z_{2} B(0.4+0.2Z_{1})\) & \\ \(Z_{3} B(0.3+0.3Z_{1}Z_{2})\) & \\ \(X B(0.2+_{1}Z_{1}+_{2}Z_{2}+_{3}Z_{3})\) & \\ \(Y B(0.1+0.2X+_{1}Z_{1}+_{2}Z_{2}+_{3}Z_{3})\) & \\ 

Table 2: SCM and causal diagram for the Synthetic A example.

3. Evaluate the mechanisms \(\) of \(\) for all units \(u^{(k)}\),
4. If there exists a sample \(k\) with \(X(u^{(k)}) x\) go back to Step (2),
5. Return the mean of the \(Y\) variables \(_{k=1}^{N}Y^{(k)}\).

Notice that the described procedure gives us samples from the distribution \(P(y x^{U_{[i]}})\). The values of \(U_{[i]}\) are sampled only once and are not updated after the initial sampling. Other values in \(U\), however, are sampled anew until their values are such that they are compatible with the evidence \(X=x\). Therefore, the procedure guarantees that \(U_{[i]}\) do not respond to the evidence, whereas the complement of \(U_{[i]}\) does, allowing us to compute \(P(y x^{U_{[i]}})\) and in turn the expressions in Eqs. 28-30. The effects are also estimated from observational data based on the identification expressions in Thm. 2. Fig. 6 demonstrates that the SCM-based ground truth matches the estimates based on Thm. 2.

## 6 Conclusions

In this paper, we introduced a general toolkit for decomposing spurious variations in causal models. In particular, we introduced a new primitive called _partially abducted submodel_ (Def. 1), together with the procedure of partial abduction and prediction (Alg. 2). This procedure allows for new machinery for decomposing spurious variations in Markovian (Thm. 1) and Semi-Markovian (Thm. 3) models. Finally, we also developed sufficient conditions for identification of such spurious decompositions (Thms. 2, 4), and demonstrated the approach empirically (Sec. 5). The main limitation of our approach is the need for a fully-specified causal diagram, which may be challenging in practice. However, from a fully specified graph and the data, our tools for decomposing spurious effects give a fine-grained quantification of what the main confounders are. As is common in causal inference, the granularity of the obtained knowledge needs to be matched with the strength of the causal assumptions (in this case, specifying the causal diagram). Conversely, in the absence of such assumptions, fine-grained quantitative knowledge about these effects cannot be obtained in general , and we hypothesize that precise quantification of spurious effects is not attainable in the absence of a causal diagram. Finally, we discuss another technical solution that may alleviate some of the difficulty of causal modeling. Recently, cluster diagrams have been proposed , in which one can consider groups of confounders (instead of considering each confounder separately), and thus the specification of causal assumptions becomes less demanding (due to clustering, the number of nodes in the graph is smaller). However, causal decompositions as described in this paper can still be applied to cluster diagrams. This offers a way to choose a different level of granularity for settings where domain knowledge may not be specific enough to elicit a full causal diagram.

Figure 6: Experimental results on the Synthetic A example. Lines indicate the estimated values, dots the ground truth obtained from the SCM using rejection sampling, and the 95% confidence intervals are indicated with color. As expected, increasing the \(_{i}\) coefficient increases the spurious effect associated with the latent variable \(U_{i}\).