# Geometric Trajectory Diffusion Models

Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, Stefano Ermon

Stanford University

###### Abstract

Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and develop a novel transition kernel leveraging SE\((3)\)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.1

## 1 Introduction

Machine learning for geometric structures is a fundamental task in many natural science problems ranging from particle systems driven by physical laws [1; 26; 43; 2; 15] to molecular dynamics in biochemistry [22; 16; 45; 10]. Modeling such geometric data is challenging due to the physical symmetry constraint [56; 43], making it fundamentally different from common scalar non-geometric data such as images and text. With the recent progress of generative models, many works have been proposed in generating 3D geometric structures like small molecules [66; 42; 21; 64] and proteins [59; 23], showing great promise in solving the equilibrium states of complex systems.

Despite this success, these existing methods are limited to synthesizing static structures and neglect the fact that important real-world processes evolve through time. For example, molecules and proteins are not static but always varying with molecular dynamics, which plays a vital role in analyzing possible binding activities [8; 20]. In this paper, we aim to study the generative modeling of geometric trajectories with the additional temporal dimension. While this problem is more practical and important, it is highly non-trivial with several significant challenges. First, geometric dynamics in 3D ubiquitously preserve physical symmetry. With global translation or rotation applied to a trajectory of molecular dynamics, the entire trajectory still describes the same dynamics and the generative model should estimate the same likelihood. Second, trajectories inherently contain the correspondence between frames in different timesteps, requiring generative models to hold a high capacity for capturing the temporal correlations. Last, moving from a single structure to a trajectorycomposed of multiple ones, the distribution we are interested in becomes much higher-dimensional and more diverse, considering both the initial conditions as well as potential uncertainties injected along the evolution of dynamics.

To this end, we propose geometric trajectory diffusion models (GeoTDM), a principled method for modeling the temporal distribution of geometric trajectories through diffusion models [49; 52; 53; 18], the state-of-the-art generative model on various domains such as images , videos , and molecules . Our key innovation lies in designing an equivariant temporal diffusion over geometric trajectories, with the reverse process parameterized by equivariant transition kernels, ensuring the desired physical symmetry of the generated trajectory. To better excavate the complex spatial interactions and temporal correlations, we develop a novel temporal denoising network, where we stack equivariant spatial convolution and temporal attention. Our developments not only guarantee the desirable physical symmetry of the trajectories, but also capture the complex spatial and temporal correspondence encapsulated in the dynamics of geometric systems. Moreover, by leveraging generative modeling, GeoTDM enjoys high versatility in generating diverse yet high-quality geometric trajectories from scratch, performing interpolation and extrapolation, and optimizing noisy trajectory, all under the proposed diffusion framework.

In summary, we make the following contributions: **1.** We present GeoTDM, a novel temporal diffusion model for generating geometric trajectories. We design the diffusion process to meet the critical equivariance in modeling both unconditional and conditional distributions over geometric trajectories. Notably, we also propose a conditional learnable equivariant prior for enhanced flexibility in temporal conditioning. **2.** To fulfill the equivariance of the denoising network, we introduce EGTN, a graph neural network that operates on geometric trajectories, which also permits conditioning upon a given trajectory using equivariant cross-attention, making it suitable to serve as the backbone for GeoTDM. **3.** We evaluate our GeoTDM on both unconditional and conditional trajectory generation tasks including particle simulation, molecular dynamics, and pedestrian trajectory prediction. GeoTDM can consistently outperform existing approaches on various metrics, with up to 56.7% lower prediction score for unconditional generation and 16.8% lower forecasting error for conditional generation on molecular dynamics simulation. We also show GeoTDM successfully performs several additional applications, such as temporal interpolation and trajectory optimization.

## 2 Related Work

**Trajectory modeling for geometric systems.** Modeling the dynamics of geometric data is challenging since one must capture the interactions between multiple objects. Graph neural networks  have emerged as a natural tool to tackle this complexity [26; 41]. Subsequent works [43; 7; 2; 63] discovered equivariance as a critical factor for promoting model generalization. Among these efforts, Radial Fields  and EGNN  work with equivariant operations between scalars and vectors, while TFN  and SE(3)-Transformer  generalize to high-order spherical tensors. While considerable progress has been made, they only conduct (time) frame-to-frame prediction, which is subject to error accumulation when performing roll-out inference. Recently, EqMotion  approached the problem by learning to predict trajectories. By comparison, our GeoTDM leverages a generative modeling framework, which enables a wider range of tasks such as generation and interpolation.

**Generative models in geometric domain.** There is growing interest in developing generative models for geometric data, _e.g._molecule generation [42; 66; 21; 64], protein generation [59; 24; 69], and

Figure 1: Overview of GeoTDM. The forward diffusion \(q\) gradually perturbs the input while the reverse process \(p_{}\), parameterized by EGTN, denoises samples from the prior. The condition \(_{c}^{[T_{c}]}\), if available, is leveraged to construct the equivariant prior and as a conditioning signal in EGTN.

antibody design . Recently, diffusion-based models [21; 64] have been shown to yield superior performance compared to flow-based  and VAE-based  approaches in many of these tasks. Despite these fruitful achievements, most existing works only produce a snapshot of the geometric system, _e.g._, a molecule in 3D space, whereas our GeoTDM generalizes to generating a trajectory with multiple frames, _e.g._, an MD trajectory in 3D. DiffMD  specifically tackles MD modeling using Markovian assumption, while GeoTDM directly captures the joint distribution of all frames along the entire trajectory.

**Temporal diffusion models.** Diffusion models have been recently adapted to handle the natural temporality of data in tasks such as video generation [19; 58; 17], time series forecasting [37; 54], PDE simulation , human motion synthesis [55; 71] and pedestrian trajectory forecasting . Distinct from these works, GeoTDM models the temporal evolution of geometric data represented as a geometric graph and maintains the aforementioned vital equivariance constraint.

## 3 Preliminaries

**Diffusion models.** Diffusion models [49; 18; 52; 53] are a type of latent variable generative model that feature a Markovian forward diffusion process and reverse denoising process. The forward process progressively perturbs the input \(_{0}\) (_e.g._, image pixels or molecule coordinates) over \(\) steps using a Gaussian transition kernel \(q(_{}|_{-1})=(_{}; }_{-1},_{})\). Here, \(\{_{}\}_{=1}^{}\) are latent variables with the same dimension as the input and \(_{}\) are predefined using the noise schedule such that \(_{}\) is close to being distributed as \((,)\). The reverse process maps back from the prior distribution with \(p(_{})=(,)\) using the kernel \(p_{}(_{-1}|_{})=( _{-1};_{}(_{},),_{}^{2})\), where the variances \(_{}^{2}\) are usually fixed and the mean \(_{}\) is parameterized by a neural network with parameters \(\). The model is trained by optimizing the variational lower bound, defined as \(_{}=- p_{}(_{0}| _{1})+D_{}(q(_{}|_{0}) \|p(_{}))+_{=2}^{-1}D_{}(q (_{-1}|_{},_{0})\|p_{}(_{-1}|_{}))\). For training stability, [52; 18] suggest the noise-prediction objective:

\[_{}_{_{0}, {}(,),}()[\| -_{}(_{ },)\|^{2}],\] (1)

where \(_{0} p_{}\), \((1,)\), the weighting factors \(()\) are typically set to 1 to promote sample quality, \(_{}=_{}}_{0}+_{}}\) with \(_{}_{s=1}^{}_{s}=_{s=1}^{}(1 -_{s})\), and \(_{}\) is a specific parameterization of the mean satisfying \(_{}(_{},)=}}(_{}-}{_{}}}_{}(_{},))\).

**Equivariance.**_Functions._ A function \(f\) is equivariant _w.r.t_ a group \(G\) if \(f(g)=g f(), g G\). Furthermore, \(f\) is invariant if \(f(g)=f(), g G\). Here we focus on the group \((3)\) consisting of all 3D rotations and translations2. Each group element \(g(3)\) can be represented by a rotation matrix \(\) and a translation \(^{3}\). For geometrc graph with node features \(\) and coordinates \(\), if \(^{},^{}=f(,)\), we expect \(^{},^{}+=f(, +)\)3, _i.e._, the output node features are invariant while the updated coordinates are equivariant. _Distributions._ We call a density \(p()\) invariant _w.r.t._ a group \(G\) if \(p(g)=p(), g G\). Intuitively, geometries that are rotationally and translationally equivalent should share the same density, since they all refer to the same structure. A conditional distribution \(p(|)\) is equivariant if \(p(g|g)=p(|), g G\). Such a property is important in cases where the target distribution is conditioned on some given structures: if the observed geometry is rotated/translated, the target distribution should also rotate/translate accordingly.

**Geometric trajectories and the distributions.** We represent a geometric trajectory as \((^{[T]},,)\), where \(^{[T]}[^{(0)},^{(1)},, ^{(T-1)}]^{T N D_{}}\) is the sequence of temporal geometric coordinates, \(^{N D_{}}\) is the node feature, and \(\) is the set of edges representing the connectivity of the geometric graph. \(T\) is the number of time steps and \(D_{},D_{}\) refers to the dimension of the coordinate and node feature respectively, with \(D_{}\) normally being \(2\) or \(3\) depending on the input data. In this work, we are interested in modeling the distribution of geometric trajectories given the configuration of the geometric graph, _i.e._, \(p(^{[T]}|,)\).

**Conditioning.** Some applications like trajectory forecasting can be viewed as conditional generative tasks, where we seek to model the distribution of trajectories conditioning on certain observed timesteps, _i.e._, \(p(^{[T]}|_{c}^{[T_{c}]},,)\) where \(_{c}^{[T_{c}]}^{T_{c} N D_{}}\) is the provided trajectory in length \(T_{c}\).

**Equivariance for geometric trajectories.** Since the dynamics must be invariant to rotation or translation, the distribution of the geometric trajectories should also preserve such symmetry. This is formalized by the following invariance constraint:

\[p(^{[T]},)=p(g^{[T]} ,), g(3).\] (2)

where \(g^{[T]}[^{(0)}+,, ^{(T-1)}+]\). The conditional case should instead preserve:

\[p(^{[T]}|_{c}^{[T_{c}]},,)=p(g ^{[T]}|g_{c}^{[T_{c}]},,),\] (3)

for all \(g(3)\)4. Intuitively, if the given trajectory is rotated and/or translated, the distribution of the future trajectory should also rotate and/or translate by exactly the same amount. For simplicity, we omit writing the conditions \(\) and \(\) henceforth when describing the distributions of trajectories.

## 4 Geometric Trajectory Diffusion Models

In this section, we introduce the machinery of GeoTDM. We first present Equivariant Geometric Trajectory Network (EGTN) in SS 4.1, a general purpose backbone operating on geometric trajectories while ensuring equivariance. We then present GeoTDM in SS 4.2 for both unconditional and conditional generation using EGTN as the denoising network.

### Equivariant Geometric Trajectory Network

Our proposed Equivariant Geometric Trajectory Network (EGTN) is constructed by stacking equivariant spatial aggregation layers and temporal attention layers in an alternated manner, drawing inspirations from spatio-temporal GNNs [70; 61]. In particular, spatial layers characterize the structural interactions within the system and temporal layers model the temporal dependencies along the trajectory. For spatial aggregation, we employ the Equivariant Graph Convolution Layer (EGCL) ,

\[^{(t)},^{(t)}=(^{(t)}, ^{(t)},), t[T].\] (4)

The equivariant message passing is conducted independently for each frame \(t[T]\{0,1,,T-1\}\), with the goal of passing and fusing the geometric information based on the structure of the graph for each time step. Following such layer, we further develop a temporal layer equipped by self-attention, which has exhibited great promise for sequence modeling , to capture the temporal correlations encapsulated in the dynamics. We first compute Eqs. 5-6, where \(^{(t)},^{(t,s)},^{(t,s)}\) are the query, key, and value, respectively. In detail, \(^{(t)}=_{}(^{(t)})\), \(^{(t,s)}=_{}(^{(s)})+(t-s)\), and \(^{(t,s)}=_{}(^{(s)})+(t-s)\), with \((t-s)\) being the sinusoidal encoding  of the temporal displacement \(t-s\), akin to the relative positional encoding . Incorporating such information is crucial since the model is supposed to distinguish different time spans between two frames on the trajectory. Moreover, compared with directly encoding the absolute time step, our design is beneficial in that it ensures the temporal shift invariance of physical processes. The update of coordinates reuse the attention coefficients \(^{(t,s)}\) and the values \(^{(t,s)}\),

\[^{(t)}=^{(t)}+_{s[T]}^{( t,s)}_{}(^{(t,s)})(^{(t)}-^{(s)}),\] (7)

where \(_{}\) is an MLP that outputs a scalar to preserve rotation equivariance. The entire network \(f_{}\), with schematic depicted in Fig. 4, is constructed by alternating spatial and temporal layers, enjoying equivariance as desired (proof in Appendix A.4):

**Theorem 4.1** (\((3)\)-equivariance).: _Let \(^{[T]},^{[T]}=f_{}(^{[T]},^{[T]},)\). Then we have \(g^{[T]},^{[T]}=f_{}(g ^{[T]},^{[T]},), g(3)\)._

**Geometric conditioning.** In certain tasks like trajectory forecasting, we are additionally provided with some partially observed trajectories as side input. In order to leverage their geometric information,we augment the unconditional EGTN with _equivariant cross-attention_, a conditioning technique tailored for geometric trajectories, and more importantly, guaranteeing the crucial equivariance in Theorem 4.1. In principle, our equivariant cross-attention resembles Eqs. 5-7, but instead computes the attention between the conditioning trajectory \(_{c}^{[T_{c}]}\) and the target \(^{[T]}\). In detail, the attention coefficients are recomputed as \(^{(t,s)}=^{(t,s)}^{(t,s)}) }{_{u[T_{c}][T]}(^{(t)}^{(t,u)})}\). The updated node feature \(^{(t)}\) and coordinate \(^{(t)}\) in Eqs. 6-7 are further renewed by the cross-attention terms, yielding \(^{(t)}=^{(t)}+_{s[T_{c}]} ^{(t,s)}^{(t,s)}\) and \(^{(t)}=^{(t)}+_{s[T_{c}]} ^{(t,s)}_{}(^{(t,s)})(^{(t)}- _{c}^{(s)})\).

### Geometric Trajectory Diffusion Models

#### 4.2.1 Unconditional Generation

For unconditional generation, we seek to model the trajectory distribution subject to the SE\((3)\)-invariance (Eq. 2). To design a diffusion with the reverse marginal conforming to the invariance, we impose certain constraints to the prior and transition kernel, as depicted in the following theorem.

**Theorem 4.2**.: _If the prior \(p_{}(_{}^{[T]})\) is SE\((3)\)-invariant, the transition kernels \(p_{-1}(_{-1}^{[T]}_{}^{[T]}), \{1,,\}\) are SE\((3)\)-equivariant, then the marginal \(p_{}(_{}^{[T]})\) at any step \(\{0,,\}\) is also SE\((3)\)-invariant._

**Prior in the translation-invariant subspace.** Unfortunately, there is no properly normalized distribution _w.r.t._ Lebesgue measure on the ambient space \(:=^{T N D}\) that permits translation-invariance . We instead build the prior on a translation-invariant subspace \(_{}\) induced by a linear transformation \(\) with \(()=(TN-1)D\). Specifically, we choose the prior to be the projection of the Gaussian \((,)\) in \(\) to \(_{}\) by \(=_{D}(_{TN}-_ {TN}_{TN}^{})\), which corresponds to the function \(P(^{[T]})=^{[T]}-_{t=0}^{T-1}( ^{(t)})\), with \((^{(t)})=_{i=1}^{N}_{i}^{(t)}\) being the center-of-mass (CoM) of the system at time \(t\). We denote \(} P()\). Then the resulting distribution is a restricted Gaussian (denoted \(}(,)\)) with the variables supported only on the subspace (see App. A.1), and more importantly, is still isotropic and thus SO\((3)\)-invariant. To sample from the prior, one can alternatively sample \(^{[T]}(,)\) and then project it to the subspace to obtain the final sample \(}^{[T]}=P(^{[T]})_{}\).

**Transition kernel.** To be consistent with the prior, we also parameterize the transition kernel in the subspace \(_{}\), given by \(p_{}(}_{-1}^{[T]}}_{}^{[T]})=}(}_{}(}_{}^{[T]},),_{}^{2})\). In this way, if the mean function \(}_{}()\) is SO\((3)\)-equivariant, then the transition kernel is also guaranteed SO\((3)\)-equivariance. As suggested by , we re-parameterize \(}_{}(}_{}^{[T]}, )=}}(}_{}^{[T]}- }{}}}_{ }(_{}^{[T]},))\), where \(}_{}=P_{ }\), with \(_{}\) being an SO\((3)\)-equivariant adaptation of our proposed EGTN, fulfilled by subtracting the input coordinates from the output for translation invariance. The diffusion step \(\) is transformed via time embedding and concatenated to the invariant node features \(^{[T]}\) in the input.

**Training and inference.** We optimize the VLB for training, which, interestingly, still has a surrogate in the noise-prediction form when specifying the factors \(()\) as 1 (proof in App. A.1):

\[_{}_{_{0}^{[T]}, }}(,), (1,)}[\|}- }_{}(}_{}^{[ T]},)\|^{2}].\] (8)

The inference process is similar to  but with additional applications of \(P\) in intermediate steps to keep all samples in the subspace \(_{}\). Details are in Alg. 1 and 2.

#### 4.2.2 Conditional Generation

Distinct from the unconditional generation, in the conditional scenario the target distribution should instead be SE\((3)\)-equivariant _w.r.t._ the given frames, as elucidated in Eq. 3. The following theorem describes the constraints to consider when designing the prior and transition kernel.

**Theorem 4.3**.: _If the prior \(p_{}(_{}^{[T]}|_{c}^{[T_{c}]})\) is SE\((3)\)-equivariant, the transition kernels \(p_{-1}(_{-1}^{[T]}|_{}^{[T]},_{c}^{[ T_{c}]})\), \(\{1,,\}\) are SE\((3)\)-equivariant, the marginal5\(p_{}(_{}^{[T]}|_{c}^{[T_{c}]})\), \(\{0,,\}\) is SE\((3)\)-equivariant._

**Flexible equivariant prior.** There are in general many valid choices for the prior while satisfying \((3)\)-equivariance. We provide a guidance on distinguishing feasible designs when using Gaussian-based prior in the proposition below.

**Proposition 4.4**.: \(((_{c}^{[T_{c}]}),)\) _is \((3)\)-equivariant w.r.t. \(_{c}^{[T_{c}]}\) if \((_{c}^{[T_{c}]})\) is \((3)\)-equivariant._

Proof is in App. A.2. Notably, the mean of the prior \(_{r}^{[T]}:=(_{c}^{[T_{c}]})\) naturally serves as an anchor to transit the geometric information in the provided trajectory to the target distribution we seek to model. For instance, one can choose it as a linear combination of the CoMs of the given frames, _i.e._, \(_{r}^{[T]}=_{T N}_{s[T_{c}]}w^{(s)} }_{c}^{(s)}\), where \(_{s[T_{c}]}w^{(s)}=1\) are fixed parameters determined _a priori_[21; 13]. However, this choice does not leverage temporal consistency of the trajectory and incurs extra effort in optimization, since the model needs to learn to reconstruct the complex structures from points all located at the CoM. In contrast, we propose the following instantiation:

\[_{r}^{(t)}=_{s[T_{c}]}^{(t,s)}}_{c}^{(s)},_{s[T_{c}]}^{(t,s)}= ,\] (9)

for all \(t[T]\), where each \(_{r}^{(t)}\) is a point-wise linear combination of \(}_{c}^{(s)}\), an \((3)\)-equivariant transformation of the conditioning frames, with \(^{(t,s)}^{N}\) being the weights. We first obtain \(}_{c}^{[T_{c}]},}_{c}^{[T_{c}]}=_{ }(_{c}^{[T_{c}]},_{c}^{[T_{c}]})\) where \(_{}\) is a lightweight two layer EGTN that aims to synthesize the conditional information. The \(^{(t,s)}\) is then derived as,

\[_{t,s} =[}_{c}^{[T_{c}]}]_{t,s} ^{N},\] (10) \[^{(t,s)} =_{t,s}&s<T_{c}-1,\\ _{N}-_{s=0}^{T_{c}-2}_{t,s}&s=T_{c}-1.\] (11)

Here \(^{T}\) are learnable parameters, and \(^{(t,s)}\) is parameterized such that it has a sum of \(_{N}\) when \(s\) goes through \([T_{c}]\) to satisfy the constraint in Eq. 9 for translation equivariance. Interestingly, as we formally illustrated in Theorem A.4, our parameterization of the prior theoretically _subsumes_ the CoM-based priors [21; 13] and the fixed point-wise priors when \(\), \(}_{c}^{[T_{c}]}\), and \(}_{c}^{[T_{c}]}\) reduce to specific values. Such theoretical result underscores the benefit of our design since it permits the model to dynamically update the prior, leading to better optimization. The parameters \(\) and \(\) are updated during training with gradients coming from optimizing the variational lower bound.

**Transition kernel.** We need to modify the forward and reverse process such that they both match the proposed prior. The forward process is modified as \(q(_{r}^{[T]}|_{r-1}^{[T]},_{c}^{[T_{c}]}):= (_{r}^{[T]};_{r}+}(_{r-1}^{[T]}-_{r}),_{r})\), which ensures \(q(_{r}^{[T]}|_{c}^{[T_{c}]})\) matches the equivariant prior \(_{r}\) (proof in App. A.2). The reverse transition kernel is given by \(p_{-1}(_{-1}^{[T]}|_{}^{[T]},_{c}^{ [T_{c}]})=(_{}(_{}^{[T]},, _{c}^{[T_{c}]}),_{}^{2})\). Similar to the unconditional case, we also adopt the noise prediction objective by rewriting \(_{}(_{}^{[T]},_{c}^{[T_{c}]},)= _{r}^{[T]}+}}(_{}^{[ T]}-_{r}^{[T]}-}{_{}}}_{}(_{}^{[T]},_{c}^{[T_{c}]},))\). The denoising network \(_{}\) is implemented as an EGTN but with its output subtracted by the input for translation invariance, hence the translation equivariance of \(_{}\).

**Training and inference.** Optimizing the VLB of our diffusion yields the following objective:

\[_{}:=_{_{0}^{[T]},_{c}^{[T _{c}]},(,),(1,)}[\|-_{}(_{}^{[T]},_{c}^{[T_{c}]},)\|^{2}],\] (12)

after simplification (proof in App. A.2). The training and inference procedures are in Alg. 3 and 4.

## 5 Experiments

We evaluate GeoTDM on N-body physical simulation, molecular dynamics, and pedestrian trajectory forecasting, in both conditional (SS 5.1) and unconditional generation (SS 5.2) scenarios. We ablate our core design choices and demonstrate additional use cases in SS 5.3.

### Conditional Case

#### 5.1.1 N-body

**Experimental setup.** We adopt three scenarios in the collection of N-body simulation datasets, including **1.** Charged Particles , where \(N=5\) particles with charges randomly chosen between \(+1/-1\) are moving under Coulomb force; **2.** Spring Dynamics , where \(N=5\) particles with random mass are connected by springs with a probability of 0.5 between each pairs, and force on the spring follows Hooke's law; **3.** Gravity System , where \(N=10\) particles with random mass and initial velocity moves driven by gravitational force. For all three datasets, we use 3000 trajectories for training, 2000 for validation, and 2000 for testing. For each trajectory, we use 10 frames as the condition and predict the trajectory for the next 20 frames.

**Baselines.** We involve baselines from three families. Frame-to-frame prediction models: Radial Field , Tensor Field Network , SE\((3)\)-Transformer , and EGNN ; Deterministic trajectory model: EqMotion ; Probabilistic trajectory model: SVAE . Details in App. B.3.

**Metrics.** We employ Average Discrepancy Error (ADE) and Final Discrepancy Error (FDE), which are widely adopted for trajectory forecasting , given by \((^{[T]},^{[T]})=_{t=0}^{T-1} _{i=0}^{N-1}\|_{i}^{(t)}-_{i}^{(t)}\|_{2}\), and \((^{[T]},^{[T]})=_{i=0}^{N-1}\| _{i}^{(T-1)}-_{i}^{(T-1)}\|_{2}\). For probabilistic models, we report average ADE and FDE derived from \(K=5\) samples.

**Implementation.** The input data are processed as geometric graphs. For example, on Charged Particles, the node feature is the charge, and the graph is specified as fully connected without self-loops. We use 6 layers in EGTN with hidden dimension of 128. We use \(=1000\) and the linear noise schedule . More details in App. B.2.

**Results.** We present the results in Table 1, with the following observations. **1.** Trajectory models generally yield lower error than frame to frame prediction models since they mitigate errors accumulated in iterative roll-out. **2.** The equivariant methods, _e.g._, EGNN, EqMotion, and our GeoTDM significantly improves over the non-equivariant model SVAE, demonstrating the importance of injecting physical symmetry into the modeling of geometric trajectories. **3.** By directly modeling the distribution of geometric trajectories with equivariance, GeoTDM achieves the lowest ADE and FDE on all three tasks, showcasing the superiority of the proposed approach.

#### 5.1.2 Molecular Dynamics

**Experimental setup.** We employ the MD17  dataset, which contains the DFT-simulated molecular dynamics (MD) trajectories of 8 small molecules, with the number of atoms for each molecule

    &  &  &  & ^{[T]}\)}} & ^{[T]}\)ranging from 9 (Ethanol and Malonaldehyde) to 21 (Aspirin). For each molecule, we construct a training set of 5000 trajectories, and 1000/1000 for validation and testing, uniformly sampled along the time dimension. Different from , we explicitly involve the hydrogen atoms which contribute most to the vibrations of the trajectory, leading to a more challenging task. The node feature is the one-hot encodings of atomic number  and edges are connected between atoms within three hops measured in atomic bonds . We adopt the same set of baselines as the N-body experiments.

**Results.** As depicted in Table 3, GeoTDM achieves the best performance on all eight molecule MD trajectories, outperforming previos state-of-the-art approach EqMotion. In particular, GeoTDM obtains an improvement of 23.1%/15.3% on average in terms of ADE/FDE, compared with the previous state-of-the-art approach EqMotion, thanks to the probabilistic modeling which is advantageous in capturing the stochasticity of MD simulations.

#### 5.1.3 Pedestrian Trajectory Forecasting

**Experimental setup.** We apply our model to ETH-UCY  dataset, a challenging and large-scale benchmark for pedestrian trajectory forecasting. There are five scenes in total: ETH, Hotel, Univ, Zara1, and Zara2. Following standard setup , we use 8 frames (3.2 seconds) as input to predict the next 12 frames (4.8 seconds). The pedestrians are viewed as nodes and their 2D coordinates are extracted from the scenes. Edges are connected for nodes within a preset distance measured from the final frame in the given trajectory. The metrics are minADE/minFDE computed from 20 samples. For baselines, we compare with existing generative models that have been specifically designed for pedestrian trajectory prediction, including GANs: SGAN , SoPhie ; VAEs: PECNet , Traj++ , BiTraP , SVAE ; and diffusion: MID . Baseline results are taken from .

**Results.** From Table 2, we observe that our GeoTDM obtains the best predictions on 3 out of the 5 scenarios while achieving the lowest average ADE and FDE. It is remarkable since compared with these baselines specifically tailored for the task of pedestrian trajectory forecasting, GeoTDM does not involve special data preprocessing of the trajectories through rotations or translations, does not involve extra auxiliary losses to optimize during training, and does not require task-specific backbones, demonstrating its general effectiveness across different geometric domains.

### Unconditional Generation

**Experimental setup.** For generation we reuse the Charged Particle dataset and the MD17 dataset. We follow the same setup as the conditional case, except that we generate trajectories with length 20 from scratch. We compare with SGAN , SVAE  (slightly modified to enable generation from scratch), and a VAE-modified version of EGNN , dubbed EGVAE (see App. B). The results of SGAN on MD17 is omitted due to mode collapse during training.

**Metrics.** We adopt three metrics adapted from time series generation to quantify the generation quality of the geometric trajectories: _Marginal_ scores  measure the distance between the empirical probability density functions of the generated samples and the ground truths; _Classification_ scores  are computed as the cross-entropy loss given by a trajectory classification model, trained on the task of distinguish whether the trajectory is generated or real; _Prediction_ scores  are the MSEs of a train-on-synthetic-test-on-real trajectory prediction model (a 1-layer EqMotion) that takes as input the first half of the trajectories to predict the other half.

    &  &  &  &  \\   & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) \\  SVAE  & 3.628 & 6.80\( 10^{-5}\) & 0.0949 & 4.755 & 2.81\( 10^{-6}\) & 0.0181 & 2.735 & 2.39\( 10^{-5}\) & 0.0929 & 2.808 & 5.57\( 10^{-3}\) & 0.0346 \\ EGVAE  & 2.650 & 1.31\( 10^{-4}\) & 0.0386 & 3.677 & 1.50\( 10^{-4}\) & 0.0104 & 2.617 & 5.86\( 10^{-6}\) & 0.1131 & 2.767 & 1.73\( 10^{-6}\) & 0.0664 \\ GeoTDM & **0.726** & **3.48\( 10^{-2}\)** & **0.0212** & **0.597** & **1.62\( 10^{-8}\)** & **0.0019** & **0.314** & **4.63\( 10^{-1}\)** & **0.0235** & **0.403** & **3.35\( 10^{-1}\)** & **0.0146** \\   &  &  \\   & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) & Marg \(\) & Class \(\) & Pred \(\) \\  SVAE  & 3.150 & 2.50\( 10^{-2}\) & 0.2123 & 2.941 & 3.54\( 10^{-6}\) & 0.1312 & 3.083 & 8.29\( 10^{-5}\) & 0.2580 & 2.736 & 3.73\( 10^{-5}\) & 0.604 \\ EGVAE  & 3.007 & 3.17\( 10^{-4}\) & 0.0136 & 3.314 & 3.76\( 10^{-6}\) & 0.0221 & 2.054 & 2.77\( 10^{-5}\) & 0.0457 & 3.570 & 2.02\( 10^{-5}\) & 0.0212 \\ GeoTDM & **0.770** & **1.17\( 10^{-1}\)** & **0.0093** & **0.559** & **1.82\( 10^{-1}\)** & **0.0135** & **0.539** & **1.12\( 10^{-1}\)** & **0.0118** & **0.954** & **2.02\( 10^{-1}\)** & **0.0116** \\   

Table 4: MD Trajectory generation results on MD17. Marg, Class, and Pred refer to Marginal score, Classification score, and Prediction score respectively. GeoTDM performs the best on all 8 molecules.

**Results.** Quantitative results are displayed in Table 5 and 4 for N-body and MD17. Notably, GeoTDM delivers samples with much higher quality than the baselines. On Charged Particles, GeoTDM achieves a classification score of 0.556, indicating its generated samples are generally indistinguishable with the ground truths. We observe similar patterns on MD17, where GeoTDM obtains remarkably lower marginal scores, higher classification scores, and lower prediction scores, showcasing its strong capability to trajectories on various geometric data. Visualizations are in Fig. 5 and more in App. D.

### Ablation Studies and Additional Use Cases

**Ablations on diffusion prior.** We investigate different priors, including non-equivariant \((,)\) (_i.e._, DDPM ), equivariant but fixed CoM prior \(((_{c}^{(T_{c}-1)}),)\) and point-wise equivariant prior \((_{c}^{(T_{c}-1)},)\). In Table 6 we see that non-equivariant prior leads to significantly worse performance. The CoM prior, though equivariant, is still inferior due to extra overhead in denoising the nodes initialized around the CoM to the original geometry. GeoTDM yields the lowest error due to the flexible learnable prior.

**Ablations on EGTN.** We further ablate the designs of the denoising model. **1.** Equivariance. We replace all EGCL layers into non-equivariant MPNN  layers with same hidden dimension, leading to non-equivariant transition kernels. The performance becomes much worse, verifying the necessity of equivariance. **2.** Attention. We substitute the attentions in temporal layers by equivariant convolutions (see App. B). Compared with this variant, GeoTDM enjoys larger capacity with attention and yields lower prediction error especially on Charged Particle where the particles generally move faster. **3.** Temporal shift invariance. We employ relative temporal embeddings in attention, which enhances the generalization. Notably, the FDE improves from 0.330 to 0.258 on Charged Particle compared with the absolute temporal embedding.

**Temporal interpolation.** GeoTDM is able to perform interpolation as a special case of the conditional case. We demonstrate such capability on Charged Particle. The model is provided with the first 5 and last 5 frames, and the task is to generate the intermediate 20 frames as interpolation. GeoTDM reports an ADE of 0.055 on the test set, while a linear interpolation baseline reports an ADE of 0.171. From the qualitative visualizations in Fig. 2, we clearly see that GeoTDM can capture the complex dynamics and yield high-quality non-linear interpolations between the given initial and final frames.

**Optimization.** We further illustrate that GeoTDM can conduct optimization  on given trajectories (_e.g._, those simulated by an EGNN) by simulating \(K\) steps through the forward diffusion and then performing the reverse denoising. From Fig. 2 we see the distance between the optimized trajectory and GT gradually decreases as the optimization step grows. This reveals GeoTDM can effectively optimize the given trajectory towards the ground truth distribution.

    & Charge & Aspirin \\  GeoTDM \((_{c}^{(T)},)\) & **0.110/0.258** & **0.107/0.193** \\  Fixed \((,)\) & 0.220/0.485 & 0.235/0.393 \\ \(((_{c}^{(T_{c}-1)}),)\) & 0.135/0.298 & 0.119/0.212 \\ \((_{c}^{(T_{c}-1)},)\) & 0.123/0.282 & 0.110/0.204 \\  w/o Equivariance & 0.251/0.542 & 0.252/0.440 \\ w/o Attention & 0.133/0.312 & 0.114/0.208 \\ w/o Shift invariance & 0.139/0.330 & 0.112/0.212 \\   

Table 6: Ablation studies. The numbers refer to ADE/FDE.

Figure 2: (a) Unconditional generation samples on MD17. GeoTDM generates MD trajectories with much higher quality (see more in App. D). (b) Interpolation. _Left_: the given initial and final 5 frames. _Right_: GeoTDM interpolation and GT. (c) Optimization by GeoTDM on predictions of EGNN. Dis(Opt, GT)/Dis(Opt, EGNN) is the distance between optimized trajectories and GT/EGNN.

Discussion

**Limitations.** Akin to other diffusion models, GeoTDM resorts to multi-step sampling which may require more compute. We present empirical runtime benchmarks and more discussions in App. C.3.

**Conclusion.** We present GeoTDM, a diffusion model built over distribution of geometric trajectories. It is designed to preserve the symmetry of geometric systems, achieved by using EGTN, a novel SE(3)-equivariant geometric trajectory model, as the denoising network. We evaluate GeoTDM on various datasets for unconditional generation, interpolation, extrapolation and optimization, showing that it consistently outperforms the baselines. Future works include streamlining GeoTDM and extending it to more tasks such as protein MD, robot manipulation, and motion synthesis.