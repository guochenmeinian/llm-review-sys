# Quasi-Bayes meets Vines

David Huk

Department of Statistics

University of Warwick

David.Huk@warwick.ac.uk

&Yuanhe Zhang

Department of Statistics

University of Warwick

Yuanhe.Zhang@warwick.ac.uk

Mark Steel

Department of Statistics

University of Warwick

m.steel@warwick.ac.uk

&Ritabrata Dutta

Department of Statistics

University of Warwick

Ritabrata.Dutta@warwick.ac.uk

###### Abstract

Recently developed quasi-Bayesian (QB) methods  proposed a stimulating change of paradigm in Bayesian computation by directly constructing the Bayesian predictive distribution through recursion, removing the need for expensive computations involved in sampling the Bayesian posterior distribution. This has proved to be data-efficient for univariate predictions, however, existing constructions for higher dimensional densities are only possible by relying on restrictive assumptions on the model's multivariate structure. Here, we propose a wholly different approach to extend Quasi-Bayesian prediction to high dimensions through the use of Sklar's theorem, by decomposing the predictive distribution into one-dimensional predictive marginals and a high-dimensional copula. We use the efficient recursive QB construction for the one-dimensional marginals and model the dependence using highly expressive vine copulas. Further, we tune hyperparameters using robust divergences (eg. energy score) and show that our proposed Quasi-Bayesian Vine (QB-Vine) is a fully non-parametric density estimator with _an analytical form_ and convergence rate independent of the dimension of the data in some situations. Our experiments illustrate that the QB-Vine is appropriate for high dimensional distributions (\(\)64), needs very few samples to train (\(\)200) and outperforms state-of-the-art methods with analytical forms for density estimation and supervised tasks by a considerable margin.

## 1 Introduction

The estimation of joint densities is a cornerstone of machine learning as a looking glass into the underlying data-generating process of multivariate data. Methods that support explicit density evaluation are crucial in probabilistic modelling, with applications in variational methods , Importance Sampling , Sequential Monte Carlo , Markov Chain Monte Carlo (MCMC)  and simulation-based inference . A prominent example are Normalising Flows (NF) , leveraging deep networks with invertible transformations for analytical expressions and sampling. Despite impressive performances, they require meticulous manual hyperparameter tuning and large amounts of data to train. Bayesian methods are another attractive approach for analytical density modelling where the central object of interest is the predictive density, with the Dirichlet Process Mixture Model (DPMM)  as the canonical nonparametric choice. Similar to kernel density estimation, the DPMM can be interpreted as an infinite mixture of densities. It is composed of a density called a kernel, with a fixed parametric form, and of a mixing density responsible for assigning those parameters. The specification of this kernel and mixing density is thus importantas they regulate the expressivity of a DPMM. However, the computation of the DPMM's predictive density is laborious as it relies on expensive MCMC methods scaling poorly to high-dimensions1.

Recently,  proposed an appealing shift in Bayesian methods through the efficient constructions of Quasi-Bayesian (QB) sequences of predictive densities using only recursion, thereby circumventing the need for MCMC in Bayesian inference, and inspiring an offspring of works utilising this methodology . The seminal univariate Recursive Bayesian Predictive (R-BP) , its multivariate (R\({}_{d}\)-BP) extension  and the AutoRegressive Bayesian Predictive (AR-BP)  are all QB models targeting the predictive mean of the DPMM, with an analytical recursive form driven by bivariate copula updates. Notably, the AR-BP demonstrated superior density estimation capabilities compared to a suite of competitors on varied supervised and unsupervised datasets, is orders of magnitude faster than standard Bayesian methods and is data-efficient, i.e. does not require large amounts of training data to be effective.

In multivariate QB predictives, to derive analytical expressions, it is necessary to enforce assumptions on the DPMM kernel structure. The kernel of the R\({}_{d}\)-BP is set to be independent across dimensions, where this strong assumption is too constraining for more complex data. This is relaxed in the AR-BP through an autoregressive form of the kernel, where each kernel mean is a similarity function of previous dimensions modelled through a covariance function or a deep autoregressive network, depending on the complexity of the data. The former relies on a fixed form that is often too simplistic while the latter loses the appeal of a data-efficient predictive like in the R\({}_{d}\)-BP. Here, we posit that these assumptions, required for obtaining existing recursions, lack the flexibility to model multivariate data accurately.

In this paper, we introduce the Quasi-Bayesian Vine (QB-Vine) as a more general approach to forming multivariate QB recursions, utilising a copula decomposition to circumvent restrictive assumptions on the DPMM kernel. The QB-Vine is obtained by applying Sklar's theorem  to the joint predictive, thereby dissecting it into univariate marginal predictive densities and a multivariate copula. Marginal predictives are modelled with the data-efficient univariate R-BP while the multivariate copula is modelled with a simplified vine copula - a highly flexible copula model suited for high dimensions. Compared to the sequential constructions of previous work, the QB-Vine is inherently parallelizable over dimensions instead of sequential. The main contributions of our work are as follows:

* The copula decomposition frees us from the need to assume specific kernel structures of the DPMM, but preserves the data efficiency of QB methods while making it more effective on high-dimensional data.
* Under certain assumptions on the true dependence structure within the data, we show that the QB-Vine attains a convergence rate that is independent of the dimension.
* The above decomposition of the joint density and use of the energy score to tune hyperparameters makes the QB-Vine amenable to efficient parallelisation with significant computational gains.

Our paper is structured as follows. In Section 2 we introduce Quasi-Bayesian prediction, recapitulating the R-BP construction. In Section 3 we formulate the Quasi-Bayesian Vine model. We provide a succinct survey of related work in Section 4 and compare related methods to the QB-Vine in Section 5 on a range of datasets for density estimation, regression, and classification, achieving state-of-the-art performance with our model. We conclude with a discussion in Section 6.

## 2 Quasi-Bayesian prediction

Notation.Let \(()\) be a multivariate probability density function over \(^{d}\), from which we observe i.i.d. samples \(_{P}=\{_{k}\}_{k=1}^{n}()\). Similarly, let \(p^{1}(x^{1}),,p^{d}(x^{d})\) be the marginal densities of \(()\), each over (a subset of) \(\) with corresponding i.i.d. samples \(_{P^{i}}=\{x^{i}\}_{k=1}^{n} p^{i}(x^{i})\) for \(i=1,,d\), and assumed to all be continuous. Further, let \(\) and \(P^{1},,P^{d}\) be the respective cumulative distribution functions (cdfs) of the previously mentioned densities, in order of appearance. Finally, when discussing predictive densities, we will use a subscript, plain for data (e.g. \(x_{n}\)) and in parentheses for functions (e.g. \(p_{(n)}\)) to indicate the predictive at step \(n\), distinguishing them from the superscript kept for dimension, and use bold fonts exclusively for multivariate objects.

Bayesian predictive densities as copula updates.Consider the univariate Bayesian predictive density \(p_{(n)}\) for a future observation \(x\) given seen i.i.d. data \(x_{1:n}^{n}\) with a likelihood \(f\) of the data and a posterior \(_{(n)}\) for the model parameters \(\) after \(n\) observations. By Bayes rule, we have:

\[p_{(n)}(x|x_{1:n})=|)_{(n-1)}( |x_{1:n-1})\;d}{p_{(n-1)}(x_{n}|x_{1:n-1})}.\]

As discovered by , multiplying and dividing by the predictive from the previous step \(p_{(n-1)}\), we arrive at

\[p_{(n)}(x|x_{1:n}) =p_{(n-1)}(x|x_{1:n-1})|)_{(n-1)}(|x_{1:n-1})\;d}^{x,x^{n}}}_{x_{n}}(x|x_{1:n-1})\;d}_{x} \] \[=p_{(n-1)}(x|x_{1:n-1}) c_{(n)}(P_{(n-1)}(x),P_{(n-1)}(x _{n}))\]

which by Sklar's theorem  identifies the involvement of copulas2 in this recursive equation for the predictive densities. The second term on the right-hand side of (1) is seen to be a symmetric bivariate copula density function with the property that \(_{n}c^{(n)}(x,x^{n})=1\;a.s. x\) as a consequence of the almost sure convergence of \(p^{(n)}\) with \(n\). It can be shown that every univariate Bayesian model can be written in this form , and so has a unique copula sequence characterising its predictive updates by de Finetti's theorem . The choice of the predictive sequence then corresponds to an implicit choice of likelihood and prior .

Marginal recursive Bayesian predictive.Due to the integration over the posterior density often being intractable in practice, identifying the update copulas \(c_{(n)}\) analytically is generally impossible. Therefore,  propose a nonparametric density estimator termed recursive Bayesian predictive (R-BP) as a Dirichlet Process Mixture Model (DPMM) inspired recursion emulating (1). They derive the correct Bayesian update under a DPMM for step \(1\) and use it for all future steps \(m>1\) (derivations shown in Appendix B.3). The update copula of the R-BP is a mixture between the independent and Gaussian copula, thereby deviating from the true (unknown) form of the Bayesian recursion copulas for a DPMM. For an initial choice of predictive density \(p_{(0)}\) and distribution \(P_{(0)}\), the obtained analytical expression for the R-BP recursion has the following predictive density

\[p_{(n)}(x)=p_{(n-1)}(x)[(1-_{n})+_{n} c_{}(P_{( n-1)}(x),P_{(n-1)}(x_{n}))] \]

with \(c_{}\) being a bivariate Gaussian copula with covariance \(\). The corresponding cdf also admits an analytical expression as follows

\[P_{(n)}(x)=(1-_{n}) P_{(n-1)}(x)+_{n}H_{}(P_{(n-1)}(x),P_ {(n-1)}(x_{n})). \]

Here, for \(\) the standard univariate Gaussian distribution,

\[H_{}(u,v)=((u)-^{-1}(v)}{}})\]

is a conditional Gaussian copula distribution with covariance \(\) treated as a hyperparameter and where \(_{k}=(2-)\) is a sequence of weights converging to \(0\) (See supplement E of  for a more detailed explanation of the weights). The computational cost is \((n^{2})\) for initialising the recursion by computing \(P_{(k)}(x_{k})\) for \(x_{1:n}\), and \((n)\) to evaluate the pdf or cdf at a point. This univariate R-BP model has been shown in  (Theorem 5) to converge in total variation to a limiting distribution \(P_{()}\) with density \(p_{()}\) termed the _martingale posterior_, defined in Appendix B, which is a quasi-Bayesian object describing current uncertainty .  shows that the R-BP converges to the true density in the Kullback-Leibler distance. The recursive formulation of the R-BP with an analytical form ensures fast updates of predictives whilst the use of copulas bypasses the need to evaluate a normalising constant (as is the case in Newton's algorithm, a similar recursion for a DPMM's predictive ). Consequently, the R-BP is free from the reliance on MCMC to approximate a posterior density, making it much faster than regular DPMMs. While this formulation does not correspond to a Bayesian model, as argued by , if the recursive updates are conditionally identically distributed (as is the case for the recursion of (3)), they still exhibit desirable Bayesian characteristics such as coherence, regularization, and asymptotic exchangeability, motivating the Quasi-Bayesian name as used in .

Multivariate recursive Bayesian predictives.An extension to the multivariate case was studied by  and refined by  for data with more complex dependence structures. The multivariate DPMM is formulated as

\[f( G)=_{}K()dG( ),G(c,G_{0})\]

where \(K\) is a kernel for the observables \(^{d}\) parameterised by \(\), similarly to the kernel in kernel density estimation, and \(G\) is called the mixing distribution, upon which a Dirichlet process prior is placed with base measure \(G_{0}\) and concentration parameter \(c>0\). To address this shortcoming,  provide a copula-based recursion obtained by assuming \(K(|)=_{i=1}^{d}(x^{i}|^{i},1)\) and \(G_{0}()=_{i=1}^{d}(^{i}|0,^{-1})\), \(>0\), meaning both the kernel and base measure are assumed independent across dimensions, lacking the expressivity required to capture dependencies in the data. In , the form of the kernel is relaxed to be autoregressive with \(K(|)=_{i=1}^{d}(x^{i}|^{i}( ^{1:i-1}),1)\) where the kernel mean \(^{i}:^{i-1}\) is dependent on previous dimensions, and the base measure of  is a product of Gaussian Process priors \(G_{0}()=_{i=1}^{d}GP(^{i}|0,^{-1}k)\) for \(k:^{i-1}^{i-1}\) a covariance function.

Vine copulas as effective models for high dimensions.Vine copulas are a class of copulas that provide a divide-and-conquer approach to high-dimensional modelling by decomposing the joint copula density into \(\) bivariate copula terms. They are considered among the best current copula models for density estimation. The main ingredient of a vine copula decomposition is the following identity as a consequence of Sklar's theorem :

\[p^{a|b}(x^{a}|x^{b})=c^{a,b}(P^{a}(x^{a}),P^{b}(x^{b})) p^{a}(x^{a}) \]

where \(a,b\) are subsets of dimensions from \(\{1,,d\}\). Vine copulas rely on a conditional factorisation \((x^{1},,x^{d})=_{i=1}^{d}p^{i|1:i-1}(x^{i}|x^{1:i-1})\) to which they repeatedly apply (4), thereby splitting the joint density into the \(d\) marginal densities and \(\) bivariate copulas called pair copulas. The pair copulas for each \(i j\{1,,d\}\), take as input pairs of conditional distributions \((P^{i|^{ij}}(x^{i|^{ij}}),P^{j|^{ij}}(x^{j| ^{ij}}))\) where \(^{ij}\{1,,d\}\{i,j\}\) is decided by the choice of the vine. A vine copula model thus has the form

\[(P^{1}(x^{1}),,P^{d}(x^{d}))=_{i j}^{d(d-1)/ 2}c^{ij}(P^{i|^{ij}}(x^{i|^{ij}}),P^{j|^{ij}} (x^{j|^{ij}})|^{ij}). \]

We notice, that these pair copulas start as unconditional bivariate copulas and later capture higher orders of multivariate dependence by conditioning on the set \(\) itself. This decomposition is valid but only unique up to the permutation of indexes. We provide an example of a three-dimensional vine copula decomposition and an overview in Appendix A.3, referring the reader to  for an introduction. In practice, we use a _simplified vine copula_ model  which removes the conditional dependence of each of the copula \(c^{ij}\) on \(^{ij}\). This is an approximation which reduces the complexity of the model for dependency structure but provides significant computational gains by reducing the size of the model space. An example simplified vine model in \(d=3\) is shown below:

\[(P^{1}(x^{1}),P^{2}(x^{2}),P^{3}(x^{3}))=c^{12}(P^{1}(x^{1}),P^{2}(x^ {2})) c^{13}(P^{1}(x^{1}),P^{3}(x^{3})) c^{2,3|1}(P^{2}(x^{2}|x^{1}),P^{3}(x^{3}|x^{1})).\]

The number of pair copulas grows quadratically with the dimension, and the number of possible decompositions is exponential with the dimension, leading to greedy algorithms being used for model selection, see . The flexibility and efficacy of the vine has been studied in the literature, we refer to  among others. In particular, when the simplified vine assumption is true,  provides a dimension-independent convergence rate making simplified vine copulas greatly appealing for high-dimensional models.

## 3 Quasi-Bayesian Vine prediction

We propose the Quasi-Bayesian Vine (QB-Vine) for efficiently modelling a high-dimensional predictive density \(_{(n)}\) (and distribution \(_{(n)}\)). The efficiency is achieved by adapting Sklar's theorem (, see Appendix A) to split the joint predictive into predictive marginals for each dimension and a high-dimensional copula:

**Theorem 3.1**.: _Let \(_{(n)}\) be an \(d\)-dimensional predictive distribution function with continuous marginal predictive distributions \(P^{1}_{(n)},P^{2}_{(n)},,P^{d}_{(n)}\). Then there exists a copula distribution \(_{()}\) such that for all \(=(x^{1},x^{2},,x^{d})^{d}\):_

\[_{(n)}(x^{1},,x^{d})=_{(n)}(P^{1}_{(n)}(x^{1}), ,P^{d}_{(n)}(x^{d}))\]

_And if a probability density function is available:_

\[_{(n)}(x^{1},,x^{d})=p^{1}_{(n)}(x^{1}) p^{d} _{(n)}(x^{d})_{(n)}(P^{1}_{(n)}(x^{1}),,P^{d}_{(n)}(x^{d})) \]

_where \(p^{1}_{(n)}(x^{1}),,p^{d}_{(n)}(x^{d})\) are the marginal predictive probability density functions, and \(_{(n)}:^{d}\) is the copula probability density function._

By applying the decomposition in (6) to two consecutive predictive densities \(_{(m-1)}\) and \(_{(m)}\), we obtain a recursive update for joint predictive densities with two parts, of the form:

\[_{(m)}()}{_{(m-1)}()}= {_{i=1}^{d}_{(m)}(x^{i})}{p^{i}_{(m-1)}(x^{i})}} }_{}_{(m)}P ^{1}_{(m)}(x^{1}),,P^{d}_{(m)}(x^{d})}{_{(m-1)} P^{1}_{(m-1)}(x^{1}),,P^{d}_{(m-1)}(x^{d})}}_{}\,.\]

This decomposition fruitfully isolates updates to marginal predictive densities from updates to their dependence structure, allowing us to model each recursion separately; the marginal predictives follow a univariate recursion _a la_ (1) while the copulas are free to follow another recursive form. Particularly:

* As we are only interested in the joint predictive \(_{(n)}\), once marginal predictives are obtained, we only need to fit a single copula \(_{(n)}\) at step \(n\) to recover the joint predictive through (6).
* Unlike  where the recursion is done sequentially across dimensions, the QB-Vine can recurse marginal predictives in parallel by dimension.
* The model's dependence is not constrained by assumptions on the DPMM's form, instead, the QB-Vine is free to fit any copula that best matches the dependence of the data.

Marginal predictive density estimation with the R-BP.We model marginal predictive densities \(p^{i}_{(n)}(x_{i})\) and distributions \(P^{i}_{(n)}(x_{i})\), \( i=1,,d\) independently between dimensions. We use the univariate R-BP approach described in Section 2 to recursively obtain the analytical expression for both. For each dimension separately, starting with an initial density \(p^{i}_{(0)}\) and distribution \(P^{i}_{(0)}\), we follow the updates (2) for the density and (3) for the distribution.

Simplified vine copulas for high-dimensional dependence.After estimating marginal predictives, we model the joint density of \((u^{1}:=P^{1}_{\ (n)}(x^{1}),,u^{d}:=P^{d}_{\ (n)}(x^{d}))\) with a multivariate copula. We consider a highly flexible simplified vine copula, found in Equation (5), which decomposes the joint copula density \(c(u^{1},,u^{d})\) into \(\) bivariate copulas \(c^{ij}\) of the cdfs from dimensions \(i\) and \(j\) (possibly conditioned on additional dimensions) to capture the dependence structure of \(\). For the bivariate pair-copulas \(c^{ij}\), we use a nonparametric Kernel Density Estimator (KDE)3. Thus, each \(c^{ij}\) becomes a two-dimensional KDE copula with the following expression:

\[c^{ij}(u,v)\,=\,^{n}^{-1}(u)-^{-1}(u^{i}_{ k});0,b^{-1}(v)-^{-1}(v^{j}_{k});0,b }{^{-1}(u);0,b^{-1}(v);0,b }\]

where \((.;0,b)\) is the pdf of a normal with mean \(0\) and variance \(b\), \(^{-1}\) is the inverse standard normal cdf. Samples \(\{(u^{i}_{k},v^{j}_{k})\}_{k=1}^{n}\) are easily obtained by iteratively fitting KDE pair copulas on observed samples \(\{\{(u^{1}_{k},,u^{d}_{k})\}_{k=1}^{n}\).

```
0:\((M,N^{},d),(N,d),[B_{1},,B_{l}],V,J\).
```

**Algorithm 1** Joint, marginal, and copula density estimation with the Quasi-Bayesian Vine

Choice of Hyperparameters.We begin by choosing a Cauchy distribution as the initial predictive distribution \(p^{i}_{(0)}\) and the corresponding density \(p^{i}_{(0)}\)\( i=1,,d\), with details provided in Appendix E. The hyperparameter \(\) for the R-BP recursion is assumed different for each marginal predictive (_i.e._\(^{d}\)). Unlike previous R-BP works , the QB-Vine minimize the energy score to select \(^{d}\) rather than the log-score, both of which are strictly proper scoring rules (see Appendix C and ) and define statistical divergences. This choice was motivated by the robustness properties of the energy score . The energy score is computed between observations and \(J=100\) predictive samples from our marginal models conditional on previously observed data. As the R-BP is sensitive to the ordering of the data, we follow  by averaging the resulting R-BP marginal over \(M=10\) permutations of the data (see  for a discussion regarding the need of averaging over permutations).

We assume a same variance parameter \(b\) for all the KDE pair copula estimators in the simplified vine and select it using 10-fold cross-validation, in a data-dependent manner by minimizing the energy score between observations and J=100 copula samples. The assumption of a common bandwidth \(b\) is motivated by mapping all pair copulas to a Gaussian space, which results in a common distance used on the latent pair copula spaces. Another hyperparameter is the specific vine decomposition (the grouping of dimensions in (5), see Appendix A.3) for which we use a regular vine structure , selecting the best pair-copula decomposition with a modified Bayesian Information Criterion suited for vines .

We include an algorithmic description for estimating the marginal density as well as the copula with the QB-Vine in Algorithm 1, where \(M\) is the number of permutations, \(N^{}\) and \(N\) the train and test sizes, \(d\) the dimension of the data, \([B_{1},,B_{l}]\) the copula variances considered for \(b\), \(V\) the cross-validation size, and \(J\) the sample size for computing the energy score.

Computational benefits of the QB-Vine.Optimising the energy score instead of the log-score, together with the copula decomposition, provides us with some significant computational gains.

Firstly, as the energy score is a sample-based metric, we compute it through efficient univariate inverse probability sampling of \(P^{i}_{(n)}\). Thus, we only recurse cdfs \(P^{i}_{(n)}\) during training, thereby halving the time to tune hyper-parameters compared to using the log-score which requires densities \(p^{i}_{(n)}\) on top of cdfs \(P^{i}_{(n)}\) in (2). Secondly, Sklar's theorem implies the independence of marginal densities in the QB-Vine, allowing us to model them in parallel, reducing the cost to that of a single R-BP, _i.e._ constant with \(d\). Finally, the vine hyperparameter \(b\) is selected with a grid search using cross-validation, and parallelised across the grid and across cross-validation folds, each having the cost of a single vine.

Properties of the Quasi-Bayesian Vine.To quantify the approximation of the QB-Vine, we provide the following stochastic boundedness  result for univariate R-BP distributions with respect to the limiting _martingale posterior_\(P_{()}\) (see  and Appendix B). We note \(P_{()}\) is the univariate martingale posterior of the R-BP. The multivariate martingale posterior of the QB-Vine is not used in our results, but we discuss its properties, including a martingale condition, in Appendix B.2.

**Lemma 3.2**.: _(R-BP predictive distribution convergence) The error of the distribution function \(P_{(n)}(x)\) in (3) is stochastically bounded with_

\[_{x}P_{()}(x)-P_{(n)}(x)=_{p }(n^{-1/2}).\]

Appendix D.1 gives a proof. In comparison to univariate KDE with a mean-square optimal bandwidth \(b_{n}=(n^{-1/5})\), which converges at a rate \(_{a.s.}(n^{-2/5})\), the marginal R-BP has a better rate with sample size. In what follows, we assume that the true copula is a simplified vine of which we know the decomposition (a standard assumption in the vine copula literature ). We strengthen marginal guarantees with the theory on vine copulas to obtain the following convergence result for the estimate of the copula density. In the statement of the theorem, we consider marginal distributions \(\{P^{i}_{()}\}_{i=1}^{d}\) and \(\{P^{i}_{(n)}\}_{i=1}^{d}\) are implicitly applied to \(\) for respective copulas.

**Theorem 3.3**.: _(Convergence of Quasi-Bayesian Vine) Let \(_{()}()\) be the copula of \(\{P^{i}_{()}(x^{i})\}_{i=1}^{d}\) and let \(_{(n)}()\) be the copula of \(\{P^{i}_{(n)}(x^{i})\}_{i=1}^{d}\). Assuming that both copulas are simplified vine copulas with the same permutation of indexes in the decomposition of (5), the estimation error is stochastically bounded \(\,^{d}\) with_

\[|_{()}()-_{(n)}()|=_{ p}(n^{-r})\]

_where \(n^{-r}\) is the convergence rate of the KDE pair-copula._

We provide a proof in Appendix D.2. For a bivariate KDE pair-copula estimator with optimal bandwidth \(b_{n}=(n^{-1/6})\), we obtain \(n^{-r}=n^{-1/3}\). From , we note the optimal convergence rate of a nonparametric estimator is \(n^{-q/(2q+q)}\) where \(q\) is the number of times the estimator is differentiable. Therefore, as \(d\) increases, we expect large benefits from using a vine copula decomposition for the QB-Vine. When the simplifying assumption does not hold, the simplified vine copula converges to a partial vine approximation of the true copula, as defined in . Together, these two results guarantee accurate samples from the QB-Vine by inverse probability sampling arguments. By Theorem 3.3, the copula \(_{(n)}\) ensures samples \(=(u^{1},,u^{d})\) on the \(^{d}\) hypercube have a dependence structure representative of the data. Then, marginal distributions \(\{p^{i}_{(n)}\}_{i=1}^{d}\) recover dependent samples \(^{d}\) by evaluating the inverse of the distribution at \(u^{i}\) dimension-wise.

Adapting the QB-Vine for Regression/classification tasks.Our framework can accommodate regression and classification tasks in addition to density estimation by rewriting the conditional density as following:

\[p(y|)=)}{p()}=(y) _{i=1}^{d}\{p^{i}(x^{i})\}(y,x^{1},,x^{d} )}{_{i=1}^{d}\{p^{i}(x^{i})\}(x^{1},,x^{ d})}=(y,x^{1},,x^{d}) p_{y}(y)}

bandwidths \(b\) for the two copulas, to be estimated as in Algorithm 1. We note that for a copula decomposition to be unique, we require that the marginals involved be continuous. This assumption is violated in the classification for binary outcomes \(y\). As such, we make use of an approximation that transforms \(y\) to a continuous scale by setting negative examples to \(-10\), and positive examples to \(10\) and adds standard Gaussian noise to all examples, breaking ties on a distribution scale (a common approach taken in similar contexts [55; 61; 44]). The rationale behind this approximation is that by setting the two classes far apart on a marginal scale, we ensure no overlaps occur, thereby maintaining a clear cut-off between the classes on the distribution scale. Indeed, the separating boundary between the classes on a distribution scale will be the percentile \(q=}{T_{1}+T_{0}}\) where \(T_{0}\) and \(T_{1}\) are the numbers of negative and positive samples, respectively, in the training set. Consequently, we create different clusters in the copula \(^{d}\) hypercube according to the separation on the distributional scale, facilitating the identification of patterns in the data. We note that other approaches exist in the literature [14; 15; 18] for classification with copulas which our framework can be extended to.

## 4 Related Work

Our method shares similarities with existing work on QB predictive density estimation with analytical forms. The pivotal works of [75; 74] and the ensuing Predictive Recursion (PR) [37; 65; 66; 97; 63; 36; 64] propose a recursive solution to the same problem but are restrained to low dimensional settings due to the numerical integration of a normalising constant over a space scaling with \(d\). A sequential importance sampling strategy for PR is proposed in  termed as PRticle Filter. The R-BP of  and the multivariate extensions in [29; 35] also have a recursive form driven by bivariate copula updates. In the multivariate case, imposing assumptions on the kernel structure leads to a conditional factorisation of the joint predictive which recovers bivariate copula updates. In , an autoregressive Bayesian predictive (AR-BP) is used, where the dependence is captured by dimension-wise similarity functions modelled with kernels or deep autoregressive networks. The former relies on assumptions that might be too simplistic to capture complex data while the latter loses the appeal of a data-efficient predictive like in the R-BP. The Quasi-Bayesian Vine retains the advantages of the bivariate copula-based recursion for marginal predictives and circumvents the need for assumptions on the DPMM kernel. We achieve this via approximating the high-dimensional dependency through a simplified vine copula which is highly flexible and does not use a deep network to preserve data-efficiency, all the while maintaining an analytical expression. A relevant benchmark are the NFs of [80; 24] with analytical densities with a state-of-the-art performance across data types and tasks.

## 5 Experiments

In this section, we compare our QB-Vine model against competing methods supporting density evaluation with a closed-form expression. Further details on the experiments are included in Appendix E. Code is included at [https://github.com/Huk-David/QB-Vine](https://github.com/Huk-David/QB-Vine).

Density estimation.We evaluate the QB-Vine on density estimation benchmark UCI datasets  with small sample sizes ranging from \(89\) to \(506\) and dimensionality varying from \(12\) to \(30\), adding results for the QB-Vine and PRticle Filter to the experiments of . We report the log predictive score \(}_{k=1}^{n_{test}}-(_{(n_{ train})}(_{k}))}\) on a held-out test dataset of size \(n_{test}\) comprised of half the samples with the other half used for training, averaging results over five runs with random partitions each time. We compare the QB-Vine against the following models: Kernel Density Estimation , DPMM  with a diagonal (Diag) and full (Full) covariance matrix for each mixture component, MAF , RQ-NSF  as well as the closely related PRticle Filter , R-BP  and AR-BP . For the last two Bayesian predictive models, we add a subscript \(d\) to indicate that the \(\) hyperparameter possibly differs across dimensions, and the \(net\) suffix indicates a network-based selection of \(\) for dimensions. We observe in Table 1 that our QB-Vine method comfortably outperforms all competitors as the dimension increases, while getting close to the performance of the best alternative Bayesian predictive models for the lower dimensional WINE dataset. Our method's relative performance increases with the dimension of the data, particularly achieving a much smaller LPS for IONO - the dataset with the largest dimensions and a relatively small sample size. We accredit this performance to the copula decomposition as that is our main distinguishing factor from the other Bayesian predictive models.

[MISSING_PAGE_FAIL:9]

Scalability of the QB-Vine:In Appendix E.1, we assess the scalability of the QB-Vine, on large data sizes and dimensions, by fitting Gaussian mixture models with 20 random means and non-isotropic covariance in dimensions \(d=50\) to \(d=600\) with \(n=10000\) train and test sets. We compare our model to an RQ-NSF, reporting the LPS as well as the maximum mean discrepancy and the reverse Kullback-Leibler divergence to assess sample quality, showing superior performance.

## 6 Discussion

We introduced the Quasi-Bayesian Vine, a joint Bayesian predictive density estimator with an analytical form and easy to sample from. This extends the existing works on Quasi-Bayesian predictive densities, by using Sklar's theorem to decompose the predictive density into predictive marginals and a copula to model the high-dimensional dependency. This decomposition enables a two-part estimation procedure, employing Quasi-Bayesian recursive density estimation for the marginals and fitting a simplified vine copula for the dependence, resulting in a convergence rate independent of dimension for certain joint densities. We empirically demonstrate the advantage of QB-Vine on a range of datasets compared to other benchmark methods, showing excellent modeling capabilities in large dimensions with only a few training data.

However, there is potential for further improvements. The main bottleneck of the QB-Vine is the simplified vine, both computationally and methodologically. The non-uniqueness of the vine decomposition resulting in a search over an exponentially large model space during estimation and the hyperparameter selection of the KDE pair copulas could both lead to misspecified models. Further, our main assumption is the use of a simplified vine copula which is only an approximation to the true distribution. While these concerns stem from limited effective copula models for high dimensions being available, from a practical point of view, a simplified vine offers tractable and fast likelihood evaluations, and ultimately outperforms competitors as shown in experiments.

Future directions of this work include the incorporation of more effective copula models, or copulas accommodating different dependence structures . Another exciting direction are developments of new recursive Quasi-Bayes methods that can be merged into a Quasi-Bayesian Vine model .

Author Contributions:David Huk wrote the code, ran the experiments, derived the proofs and wrote the paper. Yuanhe Zhang helped write the initial code and the appendix. Ritabrata Dutta conceptualised the project and David Huk formulated the concrete method. Ritabrata Dutta and Mark Steel jointly supervised the project and helped to write the paper.

    &  &  \\  & BOSTON & CONCR & DIAB & IONO & PARKIN \\ \(n/d\) & 506/13 & 1.030/8 & 442/10 & 351/33 & 195/22 \\ Linear & \(0.87 0.03\) & \(0.99 0.01\) & \(1.07 0.01\) & \(0.33 0.01\) & \(0.38 0.01\) \\ GP & \(0.42 0.08\) & \(0.36 0.02\) & \(1.06 0.02\) & \(0.30 0.02\) & \(0.42 0.02\) \\ MLP & \(1.42 1.01\) & \(2.01 0.98\) & \(3.32 4.05\) & \(0.26 0.05\) & \(0.31 0.02\) \\ R-BP & \(0.76 0.09\) & \(0.87 0.03\) & \(0.15 0.03\) & \(0.26 0.01\) & \(0.37 0.01\) \\ R-BP & \(0.40 0.03\) & \(0.42 0.00\) & \(1.00 0.02\) & \(0.34 0.02\) & \(0.27 0.03\) \\ AR-BP & \(0.52 0.13\) & \(0.42 0.01\) & \(1.06 0.02\) & \(0.21 0.02\) & \(0.29 0.02\) \\ AR\({}_{d}\)-BP & \(0.37 0.10\) & \(0.39 0.01\) & \(0.99 0.02\) & \(0.20 0.02\) & \(0.28 0.03\) \\ ARnet-BP & \(0.45 0.11\) & \(\) & \(1.41 0.07\) & \(0.24 0.04\) & \(0.26 0.04\) \\  QB-Vine & \(\) & \(0.54 0.34\) & \(\) & \(\) & \(\) \\   

Table 2: Average LPS (lower is better) with error bars corresponding to two standard deviations over five runs for supervised tasks analysed by . The QB-Vine performs favourably against benchmarks, with relative performance improving as samples per dimension decrease.

Acknowledgments:We thank all the reviewers for their helpful feedback. We also thank Surya T. Tokdar, Fabrizio Leisen and Edwin Fong for useful discussions, further thanking Vaidehi Dixit for sharing Pricle Filter codes with us. David Huk is funded by the Center for Doctoral Training in Mathematical Sciences at Warwick. Ritabrata Dutta is funded by EPSRC (grant nos. EP/V025899/1 and EP/T017112/1) and NERC (grant no. NE/T00973X/1).