# Robust Data Valuation with Weighted Banzhaf Values

Weida Li

vidaslee@gmail.com &Yaoliang Yu

School of Computer Science

University of Waterloo

Vector Institute

yaoliang.yu@uwaterloo.ca

###### Abstract

Data valuation, a principled way to rank the importance of each training datum, has become increasingly important. However, existing value-based approaches (e.g., Shapley) are known to suffer from the stochasticity inherent in utility functions that render consistent and reliable ranking difficult. Recently, Wang and Jia (2023) proposed the noise-structure-agnostic framework to advocate the Banzhaf value for its robustness against such stochasticity as it achieves the largest safe margin among many alternatives. Surprisingly, our empirical study shows that the Banzhaf value is not always the most robust when compared with a broader family: weighted Banzhaf values. To analyze this scenario, we introduce the concept of Kronecker noise to parameterize stochasticity, through which we prove that the uniquely robust semi-value, which can be analytically derived from the underlying Kronecker noise, lies in the family of weighted Banzhaf values while minimizing the worst-case entropy. In addition, we adopt the maximum sample reuse principle to design an estimator to efficiently approximate weighted Banzhaf values, and show that it enjoys the best time complexity in terms of achieving an \((,)\)-approximation. Our theory is verified under both synthetic and authentic noises. For the latter, we fit a Kronecker noise to the inherent stochasticity, which is then plugged in to generate the predicted most robust semi-value. Our study suggests that weighted Banzhaf values are promising when facing undue noises in data valuation.

## 1 Introduction

In machine learning, data curation plays a crucial role in training powerful models, and one big difficulty facing practitioners is how to quantify the extent of usefulness each datum possesses (under a specific application). In practice, large datasets are typically noisy and often contain mislabeled data, which could harm the downstream performance of models trained on top. Sources that affect data quality include mechanical or human labelling errors (Frenay and Verleysen, 2013), and poisoning attacks from malicious adversaries (Steinhardt et al., 2017). Therefore, it would be extremely useful if the potential importance of each datum can be efficiently and reliably determined. Data valuation aims to lay out a principled way to filter out noisy data and hunt down data that are valuable to retain.

In the emerging field of data valuation, the concept of _value_, originally developed in cooperative game theory as an axiomatic and principled way to assign contributions to all players in a game, has become increasingly popular in machine learning (e.g., Ghorbani and Zou, 2019; Jia et al., 2019, 2022; Kwon and Zou, 2022; Lin et al., 2022; Wang and Jia, 2023). Typically, the procedures of training and testing are treated as games that constitute utility functions while each datum serves as a player. The contribution imputed to each player is then equivalent to the importance determined for each datum.

Recently, Wang and Jia (2023) pointed out that the inherent stochasticity in utility functions, which stems from the procedure of training models, is substantial enough to incur unexpected inconsistency in ranking data. To solve this issue, they introduced the noise-structure-agnostic framework todemonstrate that the Banzhaf value (Banzhaf, 1965) is the most robust against such stochasticity compared with other previously employed values. Surprisingly, our extensive experiments on various datasets, see Figure 1 for some examples, reveal that the Banzhaf value is not always the most robust compared with weighted Banzhaf values, to which the Banzhaf value belongs.

In this work, our goal is to shed further light on the above-mentioned phenomenon both theoretically and empirically. As one might expect, our extensive experiments confirm that overall there is no _single_ universally the best value, in terms of either noisy label detection or ranking consistency, across a variety of experiment settings. Nevertheless, we show that it is possible to _adaptively_ estimate a weighted Banzhaf value that achieves competitive robustness compared with the existing baselines. We summarize our contributions as follows:

1. First, we introduce the concept of Kronecker noise to parameterize the inherent stochasticity in practice, and prove that the uniquely most robust semi-value lies in the family of weighted Banzhaf values in terms of minimizing the worst-case entropy. Specifically, the parameter for the most robust semi-value can be directly derived from the underlying Kronecker noise.
2. Second, it is known that evaluating each semi-value is exponentially expensive w.r.t. the number of data being valuated, and thus we adopt the maximum sample reuse principle to design an efficient estimator for approximating weighted Banzhaf values. Particularly, we show that it enjoys the best time complexity in terms of achieving an \((,)\)-approximation.
3. Third, both synthetic and authentic noises are employed in our experiments to verify the robustness assertion in Theorem 1. For the latter, we fit a Kronecker noise to the inherent stochasticity by minimizing the Kullback-Leibler divergence between Gaussian distributions.
4. Last, our empirical study shows that it is often that some member of weighted Banzhaf values achieves the best performance in noisy label detection as well as ranking consistency when compared with the existing baselines. The empirical evidence highlights that there is no _single_ universally the best value across various experiment settings.

Our code is available at https://github.com/watml/weighted-Banzhaf.

Figure 1: The first row contains results from the experiment of ranking consistency, while the second row is from the experiment of noisy label detection. For both metrics, the higher the better. The error bars represent the standard deviation over \(10\) random seeds. Specifically, each Spearman’s rank correlation coefficient is averaged over \(=45\) pairs of independent rankings. Each parameter along the horizontal axis defines a weighted Banzhaf value, and \(0.50\) corresponds to the Banzhaf value.

## 2 Background

### Notations

Let \(_{tr}\) and \(_{val}\) be training and validation datasets, respectively. We write \(n=|_{tr}|\), and identify \(_{tr}\) with \([n]=\{1,2,,n\}\). The definition of utility function \(v:2^{[n]}\) is pivotal for employing valued-based data valuation methods. Typically, for each subset \(S[n]\), \(v(S)\) is set to be the performance of a chosen model trained on \(S\). For example, \(v(S)\) is usually the accuracy measured on \(_{val}\) for classification tasks. Specifically, \(v()\) is set to be the performance of randomly initialized models in this work. Let \(_{n}\) be the set that contains all possible utility functions with domain \(2^{[n]}\), and define \(=_{n 1}_{n}\). Particularly, the subscript of \(v_{n}\) is to indicate the number of data being valuated. For convenience, we write \(S i\) and \(S i\) instead of \(S\{i\}\) and \(S\{i\}\), and use \(s=|S|\) when \(S\) represents a set.

### Axioms for Values

Value-based data valuation methods take advantage of the axiomatic approach developed in cooperative game theory (Dubey et al., 1981; Shapley, 1953; Weber, 1977). A value is a function \(\) that maps each utility function \(v_{n}\) to a contribution vector \(^{n}\). In other words, \(_{i}(v)=c_{i}\) is the contribution of the \(i\)-th datum in \(_{tr}\) for the model performance achieved by training on \(_{tr}\). There are four axioms we expect a value to have, which are

1. **Linearity**: \((a u_{n}+v_{n})=a(u_{n})+(v_{n})\) for every \(u_{n},v_{n}\) and every \(a\);
2. **Dummy**: \(_{i}(v_{n})=C\) if \(v_{n}(S i)=v_{n}(S)+C\) for every \(S[n] i\);
3. **Monotonicity**: \(_{i}(v_{n}) 0\) if \(v_{n}(S i) v_{n}(S)\) for every \(S[n] i\);
4. **Symmetry**:1\((v_{n})=(v_{n})\) for every permutation \(\) of \([n]\). 
As proved by Weber (1977, Theorems 5 and 10),2 a value \(\) satisfies the above four axioms if and only if there exists a list of non-negative vectors \(=(^{n}^{n})_{n 1}\) such that, for each \(n 1\), \(_{s=1}^{n}p_{s}^{n}=1\) and

\[_{i}(v_{n};)=_{S[n] i}p_{s+1}^{n} (v_{n}(S i)-v_{n}(S))v_{n}i[n].\] (1)

All such values are called probabilistic values.

There is one more regularization implicitly used by Dubey et al. (1981) to filter out some undesired probabilistic values. The eventual regularized probabilistic values are called semi-values.34 We explicitly rephrase the regularization as an extra axiom in this paper. Let \(z\) be an extra datum labelled by \(n+1\), and extend each utility function \(v_{n}\) into \(_{n+1}\) defined by \(_{n+1}(S)=v_{n}(S[n])\) for every \(S[n+1]\). The extra axiom reads as follows:

Consistency: \(_{i}(v_{n};)=_{i}(_{n+1};)\) for every \(v_{n}\) and every \(i[n]\).

The axiom of consistency states that a value should not change the previously assigned values of importance if the added data do not contribute anything. Note that the proposed axiom of consistency is the discrete version of the dummy consistency proposed by Friedman (2003, Definition 7) in cost-sharing problems. We have the following characterization for semi-values:

**Proposition 1** (Dubey et al., 1981, Theorem 1(a)).: _A probabilistic value \(\) is a semi-value if and only if there exists a probability distribution \(P\) on the interval \(\) such that_

\[p_{s}^{n}=_{}t^{s-1}(1-t)^{n-s}P(t)1 s n.\] (2)_Moreover, the map from all such probability distributions to semi-values is one-to-one._

The axiom of consistency basically imposes that

\[p_{s}^{n}=p_{s}^{n+1}+p_{s+1}^{n+1}1 s n,\]

which is the key for connecting all components in \(\). All in all, combing Eqs. (1) and (2) yields the formula for each semi-value,

\[_{i}(v_{n};P)=_{0}^{1}_{S[n] i}t^{s}(1-t)^{n-1-s }(v_{n}(S i)-v_{n}(S))\,P(t)\] (3)

for every \(v_{n}\) and every \(i[n]\). It suggests that each semi-value is just the integral of weighted Banzhaf values w.r.t. the underlying probability distribution \(P\). Particularly, substituting the uniform distribution for \(P\) recovers the formula for the Shapley value (Shapley 1953) derived from the multilinear extension of utility functions (Owen 1972, Theorem 5).

### Semi-values for Data Valuation

As a pioneering work, Ghorbani and Zou (2019) proposed Data Shapley that includes G-Shapley and TMC-Shapley for data valuation, and demonstrated its effectiveness in applications. Later, Beta Shapley was developed to improve the overall performance of value-based data valuation (Kwon and Zou 2022). Recently, Wang and Jia (2023) set up a noise-structure-agnostic framework to demonstrate that their proposed Data Banzhaf is the most robust compared with the previous works. Basically, the distinction between these methods is the use of semi-values. The underlying cumulative density functions (CDFs) of \(P\) in Eq. (2) for these value-based data valuation methods are listed in Table 1. Specifically, the TMC-Shapley (if the indices for truncation are the same for all sampled permutations of \([n]\)) is some probabilistic value up to some scalar.

In cooperative game theory references, weighted Banzhaf values (which are semi-values) were discovered by Ding et al. (2010) and Marichal and Mathonet (2011, Theorem 10) based on the approximation of utility functions (Hammer and Holzman 1992).5 To obey the axiom of symmetry, the family of symmetric weighted Banzhaf values is taken herein, which can be parameterized by \(w\). Precisely, for \(w\)-weighted Banzhaf value, the corresponding coefficients in Eq. (1) are

\[p_{s}^{n}=w^{s-1}(1-w)^{n-s}1 s n.\]

Equivalently, its underlying CDF is \(_{[w,1]}\), while its underlying probability distribution is the Dirac delta distribution \(_{w}\) defined by

\[_{w}(S)=1&w S\\ 0&w SS.\]

Particularly, \(0.5\)-weighted Banzhaf value is exactly the Banzhaf value employed in Data Banzhaf, while \(1.0\)-weighted Banzhaf value is just the leave-one-out method, which is

\[_{i}(v_{n};_{1})=v_{n}([n])-v_{n}([n] i)v_{n}i[n].\]

   Method & G-Shapley & Beta(\(,\)) & Data Banzhaf \\  CDF & \(x\) & \(_{0}^{x}t^{-1}(1-t)^{-1}t\) & \(_{[0.5,1]}\) \\   

Table 1: The underlying CDFs on the interval \(\) for semi-values of interest. \(x\) denotes the variable of CDFs. \(_{[0.5,1]}\) is defined by \(_{[0.5,1]}(x)=1\) if \(x[0.5,1]\) and \(0\) otherwise. Beta(\(,\)) is a parameterized notation for the Beta Shapley values.

Main Work

In this section, we first introduce how to use the maximum sample reuse (MSR) principle to design an efficient estimator for weighted Banzhaf values, and also present its time complexity in terms of \((,)\)-approximation. Then, we introduce the concept of Kronecker noise to parameterize the underlying stochasticity in utility functions, through which we show that the uniquely most roust semi-value always lies in the family of weighted Banzhaf values while minimizing the worst-case entropy.

### Efficient Estimator for Weighted Banzhaf Values

To exactly compute each probabilistic value shown in Eq. (1), we have to evaluate the provided utility function \(2^{n}\) times in total, which is intractable when \(n\) is large, not to mention the expensive cost of training models. Therefore, in general, each probabilistic value can only be approximated using estimators. Specifically, the sampling lift estimator can be used to approximate every probabilistic value (Moehle et al., 2022), and its weighted version was employed by Kwon and Zou (2022) for approximating their proposed Beta Shapley values. Meanwhile, there are efficient estimators designed specifically for the Shapley value (e.g., Castro et al., 2009; Covert and Lee, 2021; Jia et al., 2019; Lundberg and Lee, 2017). Recently, Wang and Jia (2023) demonstrated how to use the MSR principle to design an efficient estimator for the Banzhaf value, but they also presented that such a principle can not extend to many other probabilistic values (e.g., the Beta Shapley values). Nevertheless, we show that MSR estimator exists for weighted Banzhaf values.

For approximating \(w\)-weighted Banzhaf value based on some \(v_{n}\), we first independently sample a sequence of Bernoulli vectors \(\{_{1},_{2},,_{m}\}\) where the entries of each \(_{j}^{n}\) are drawn from \(n\) independent Bernoulli distribution \(X\) with \(P(X=1)=w\); then, the sequence of Bernoulli vectors are transformed into a sequence of subsets \(\{S_{1},S_{2},,S_{m}\}\) of \([n]\) by defining \(i S_{j}\) if and only if the \(i\)-th entry of \(_{j}\) is \(1\); finally, the approximate value is computed by

\[_{i}(v_{n};_{w})=_{ i}|}_{S _{ i}}v_{n}(S)-_{ i}|}_{S _{ i}}v_{n}(S)i[n]\] (4)

where \(_{ i}=\{S_{j} i S_{j}\}\) and \(_{ i}=\{S_{j} i S_{j}\}\).

As shown in Eq. (4), each utility evaluation is employed for estimating the importance of every datum in \([n]\), while for the (reweighted) sampling lift it is two utility evaluations for only one datum; see Appendix D.2 for more details. Therefore, it could be expected that the MSR estimators are much more efficient, which is certainly confirmed by Figure 2. Meanwhile, such an MSR estimator enjoys the currently best time complexity bound demonstrated by Wang and Jia (2023, Theorem 4.9).

**Proposition 2**.: _Assume \(\|v_{n}\|_{} r\) for every \(v_{n}\). The estimator based on Eq.(4) requires \(O(})\) utility evaluations to achieve \(P(\|(v_{n};_{w})-(v_{n};_{w})\|_{2})\) for every utility function \(v_{n}\)._

### Robustness under Kronecker Noises

If \(2^{[n]}\) is ordered, each stochastic utility function \(v_{n}\) can be treated as a random vector in \(^{2^{n}}\), and thus its randomness can be summarized by its covariance matrix \(^{2^{n} 2^{n}}\). However, there are \((2^{n})\) parameters to characterize. To make the analysis tractable, our approach is to parameterize \(\) as simply as possible so that the most robust semi-value can be analyzed accordingly. Before proceeding, we introduce the binary ordering for \(2^{[n]}\), so that we can legally have the covariance matrix \(\).

Ordering of \(2^{[n]}\)Define a binary sequence \(_{2},_{2},,[2^{n}-1]_{2}\) by letting \([k]_{2}\) be the binary representation of \(k\) in \(n\)-digit format. We order \(\{S\}_{S[n]}\) along this binary sequence by assigning to each binary representation \([k]_{2}\) a subset \(S_{k}[n]\) defined by \(i S_{k}\) if and only if the \((n+1-i)\)-th digit of \([k]_{2}\) is \(1\). For example, if \(n=3\), the binary sequence is \(000,001,010,011,100,101,110,111\), and the corresponding ordering for the subsets of \(\) is \(,\{1\},\{2\},\{1,2\},\{3\},\{1,3\},\{2,3\}\). We refer to this ordering as binary ordering. In the rest of the paper, we treat each stochastic utility function \(v_{n}\) as a random vector in \(^{2^{n}}\) w.r.t. the binary ordering. The advantage of this orderingis that it aligns well with the Kronecker product so that we can parameterize the covariance matrix of each stochastic utility function using just one simple covariance matrix \(^{2 2}\).

**Assumption 1** (Kronecker noises).: _Let \(2^{[n]}\) be ordered w.r.t. the binary ordering. For each random vector \(v_{n}^{2^{n}}\) (equivalently, a stochastic utility function in \(\)), we assume \(v_{n}-[v_{n}]=_{n}_{n-1} _{1}\) where \((_{i})_{1 i n}\) are \(n\) independent continuous random vectors in \(^{2}\) that have the same positive-definite covariance matrix \(\)._

**Proposition 3**.: _Under Assumption 1, there is_

\[v_{n}(S)=[v_{n}(S)]+_{i S}_{i,2}_{i S} _{i,1},S[n],\]

\[(v_{n})= \ \ (n),\]

\[(v_{n}(S),v_{n}(T))=_{11}^{[n]( S T)|} _{12}^{[ S T(T S)]} _{22}^{[ S T]}S,T[n]\]

**Remark 1**.: _According to Proposition 3, Assumption 1 provides a simple parameterization for the covariance matrix of the stochasticity inherent in \(v_{n}\). Though such a simple modeling is yet ideal, it is sufficient for arguing theoretically that there is no single universally the most robust semi-value across different stochastic utility functions. Notably, as shown in Table 2, weighted Banzhaf values tend to be the most consistent in ranking data, and the most empirically robust one varies across different experiment settings. Overall, our theory emphasizes that a noise-structure-specific framework is able to provide a finer conclusion for robustness compared with the previous noise-structure-agnostic framework (Wang and Jia 2023)._

Next, we propose to use the entropy of continuous random vectors to define the worst-case uncertainty brought by the parameterized covariance matrix, which is

\[_{v_{n}_{n}(v_{n})=^{[n ]}}h((v_{n};P))\] (5)

where \(P\) is a probability distribution that defines a semi-value Eq. (3), \(h\) is the entropy that measures the uncertainty of continuous random vectors, and \(^{[n]}=\) (\(n\) repetitions). Intuitively, \((v_{n};P)\) tends to be deterministic as \(h((v_{n};P))\) decreases to zero. Since Eq. (5) defines the least upper bound on \(h((v_{n};P))\), the lower Eq. (5) is, the more robust (equivalently, more deterministic) the corresponding semi-value is. Therefore, the most robust semi-value is defined to be the one that minimizes

\[*{argmin}_{P}\ _{v_{n}_{n} (v_{n})=^{[n]}}h((v_{n};P))\] (6)

where \(\) is the set that contains all (Borel-measurable) probability distributions on the interval \(\). In other words, we try to determine a semi-value that can best tolerate the largest uncertainty brought by any noises having the covariance matrix \(^{[n]}\). As one may expect, the most robust semi-value depends on the choice of \(^{2 2}\).

Since \((v_{n};P)\) is linear in \(v_{n}_{n}\), the result of \(((v_{n};P))\) subject to \((v_{n})=^{[n]}\) only depends on \(^{[n]}\) and \(P\). Consequently, \(_{v_{n}_{n}(v_{n})=^{[n]} }h((v_{n};P))=_{()=}h( )\) where \(=((v_{n};P))\). It is known that \(_{()=}h()=(1+ 2)+\), and the maximum is achieved if \(\) is a Gaussian random vector. Besides, \(\) is Gaussian if \(v_{n}\) is Gaussian because \((v_{n};P)\) is linear in \(v_{n}_{n}\). All in all, the optimization problem (6) equivalently reduces to be

\[*{argmin}_{P} (((v_{n};P)))\] (7) s.t. \[(v_{n})=^{[n]}.\]

**Theorem 1**.: _Under Assumption 1, and further assume \(_{12}<(_{11},_{22})\), the Dirac delta distribution \(_{w}\). with_

\[w^{*}=-_{12}}{_{11}+_{22}-2_{12}}\] (8)

_is optimal to the problem (7) for every \(n 1\). In addition, the optimal solution is unique if \(n>3\)._

**Remark 2**.: _Theorem 1 suggests that \(0.5\)-weighted Banzhaf value (i.e., the Banzhaf value) is the most robust provided that \(_{11}=_{22}\), which coincides with (Wang and Jia 2023, Theorem 4.6). Specifically,they simulated some isotropic Gaussian noises added to deterministic utility functions to verify that the Banzhaf value is the most robust in ranking data, which is also supported by Theorem 1. However, the first column of Figure 3 shows that the Banzhaf value is not always the most robust if the added noises follow Kronecker noises with \(_{11}_{22}\). In practice, Table 2 also refutes that the Banzhaf value is universally the most robust._

## 4 Experiments

Our experiments contain three aspects: 1) approximation efficiency of the MSR estimator for weighted Banzhaf values; 2) verifying Theorem 1 using synthetic and authentic noises; 3) empirical study to show that the most robust/effective semi-values tend to be some weighted Banzhaf values.

All datasets used are from open sources, and are classification tasks. Except for MNIST and FMNIST, each \(_{tr}\) or \(_{val}\) is balanced between different classes. Without explicitly stated, we set \(|_{val}|=200\). All utility functions are set to be the accuracy reported on \(_{val}\) with logistic regression models being trained on \(_{tr}\), except that we implement LeNet (LeCun et al., 1998) for MNIST and FMNIST. To have the merit of efficiency, we adopt one-epoch one-mini-batch learning for training models in all types of experiments (Ghorbani and Zou, 2019). More experiment details and results are included in Appendices D and E.

### Approximation Efficiency

The MSR estimator is compared with reweighted sampling lift and sampling lift estimators. To calculate the exact values, we set \(|_{tr}|=16\). Besides, the learning rate is set to be \(1.0\). To have fair comparison, we plot relative difference \(\|}-\|_{2}/\|\|_{2}\) along the number of average utility evaluations, i.e. \(}{n}\). Figure 2 is plotted with \(10\) different random seeds used by estimators. It demonstrates that the MSR estimator is significantly the most efficient.

### Verification for the Theorem 1

Experiments based on synthetic and authentic noises are designed to verify Theorem 1. We set \(|_{tr}|=10\) so that the exact values can be cheaply computed.

For synthetic experiments, we simulate Kronecker noises with each \(_{i}\) independently and identically following a Gaussian distribution \((0,)\), which are employed as added noises to deterministic utility functions. Besides, the learning rate is set to be \(0.05\). Specifically, we fix \(_{11}\) and \(_{22}\) while varying \(_{12}\) along the horizontal axis. For each \(_{12}\), the corresponding added noises are simulated using \(10\) different seeds. The Spearman's rank correlation coefficient is used to measure the similarity between the rankings of the ground-truth value and any noisy one. The results are reported in the first column of Figure 3, where the one tagged as "robust semi-value" refers to the corresponding weighted Banzhaf values predicted by Eq. (8). It is clear that Theorem 1 predicts the most robust semi-value well, and that the Banzhaf value is not universally the most robust under these synthetic noises.

Next, we examine Theorem 1 by fitting a Kronecker noise to the stochasticity inherent in the given utility function, and then compute the predicted most robust semi-value using Eq. (8). To produce simple utility functions, we set \(|_{val}|=10\). Besides, the learning rate for utility functions is set to be \(1.0\). The underlying covariance matrix is empirically approximated by \(}\), and then we fit a Kronecker

Figure 2: Comparison of three estimators for \(0.8\)-Banzhaf value on 2dplanes dataset. The shaded region represents the standard deviation over 10 different random seeds.

noise by optimizing the Kullback-Leibler divergence between Gaussian distributions, which is

\[*{argmin}_{^{2 2}}\  (0,})(0, }^{10\ }.\]

The final results averaged over \(10\) random seeds are presented in the last two columns of Figure 3. Each parameter in \(\{0.00,0.05,0.10,,0.95,1.00\}\) defines a specific weighted Banzhaf value, while \(\{(16,1),(4,1),(1,4),(1,16)\}\) are parameters for the Beta Shapley values. The one tagged as "robust" is generated from the approximate Kronecker noise using Eq. (8). As demonstrated, the predicted "robust" semi-values do achieve the best in both ranking consistency and variance.

### Empirical Study

Following the previous study (Wang and Jia 2023), we implement an exploratory study on \(23\) datasets, including two types of experiment: 1) noisy label detection and 2) ranking consistency.

We fix \(|_{tr}|=1,000\) for all datasets except that it is \(|_{tr}|=2,000\) for MNIST and FMNIST, which means the induced stochasticity inherent in utility functions is more complicated than Assumption 1. For each dataset, the chosen model is individually fine-tuned for the learning rate. The randomness in each utility function stems from the underlying random seed that determines the order of feeding training data as well as the initialization of models. All results are reported using mean and standard deviation over \(10\) random seeds. For each estimator, the total number of utility evaluations is set to be \(400,000\). Permutation estimator (Castro et al. 2009) is used for Shapley value, reweighted sampling lift estimator for Beta Shapley, and MSR estimator for weighted Banzhaf values. Specifically, \(0\)-weighted Banzhaf value and \(1\)-weighted Banzhaf value require only \(n+1\) utility evaluations to compute exactly, and thus we do not use any estimators for them.

Figure 3: (a) Synthetic noises: Each point represents the Spearman’s rank correlation coefficient averaged over \(10\) random seeds. For each \(_{12}\), the robust semi-value is the weighted Banzhaf value parameterized by \((_{11}-_{12})/(_{11}+_{22}-2_{12})\), as predicted by Theorem 1. For the first row, we set \(_{11}=1.5\) and \(_{22}=0.5\), while it is \(_{11}=0.5\) and \(_{22}=1.5\) for the other. (b) Authentic noise: Ranking consistency is measured by the mean and standard deviation of the Spearman’s rank correlation coefficients between all pairs of \(10\) random seeds. The variance refers to \(_{i=1}^{10}\|}^{i}-\|_{2}^{2}/10\) where \(=_{i=1}^{10}}^{i}\). The computed “robust” parameters are \(0.5255\) and \(0.5341\) for iris and phoneme datasets, respectively.

Ranking ConsistencyIn data valuation, it could be the ranking of data that matters the most, but the randomness of utility functions may produce inconsistent rankings. For each semi-value, the averaged Spearman's rank correlation coefficients over all possible pairs, which is \(=45\) in total, are calculated to measure ranking consistency. The results are summarized in Table 2, while figures with more details are included in Appendix E. As presented, it is mostly some member of weighted Banzhaf values, not always the Banzhaf value, that achieves the highest ranking consistency compared with the baselines.

Noisy Label DetectionNotice that a more robust semi-value does not necessarily produce a higher quality of data valuation. This experiment is to present how well weighted Banzhaf values would achieve in noisy label detection. For each dataset, we randomly flip the labels of \(20\) percent of data in \(_{tr}\) to be any of the rest in a uniform manner. Those flipped data are treated as having noisy/incorrect labels. Particularly, such a flipped \(_{tr}\) is also used for reporting ranking consistency in Table 2. Then, each estimated semi-value is used to detect noisy labels by marking data assigned with values of importance less than or equal to \(20\) percent percentile as mislabeled. The results are summarized in Table 3, and figures that contain more details are included in Appendix E. As observed, the best performance is mostly achieved by weighted Banzhaf values, and most of them are not the Banzhaf value.

   Dataset & Weighted Banzhaf & Banzhaf & Beat(\(16,1\)) & Beta(\(4,1\)) & Beta(\(1,4\)) & Beta(\(1,16\)) & Shapley \\  MNIST (10) & 0.20 & **0.650\(\)** 0.012 & 0.581\(\) 0.075 & 0.073\(\) 0.082 & 0.020 \(\) 0.043 & \(-0.004 0.039\) & \(-0.009 0.037\) & \(0.011 0.019\) \\ FMNIST (10) & 0.30 & **0.670\(\)** 0.042 & 0.562\(\) 0.040 & 0.017 \(\) 0.002 & 0.007 \(\) 0.036 & \(-0.001 0.035\) & \(0.009 0.029\) & \(0.002 0.021\) \\ Zdplanes (2) & 0.80 & **0.596\(\)** 0.012 & 0.839\(\) 0.091 & 0.424\(\) 0.153 & 0.358\(\) 0.133 & 0.046\(\) 0.087 & \(0.001 0.130\) & \(0.256 0.083\) \\ bank-marketing (2) & 0.75 & **0.892\(\)** 0.028 & 0.814\(\) 0.029 & 0.599\(\) 0.096 & 0.445\(\) 0.093 & 0.015\(\) 0.094 & \(0.014 0.128\) & \(0.265 0.033\) \\ bioresponse (2) & 0.00 & **0.997\(\)** 0.001 & 0.948\(\) 0.003 & 0.193\(\) 0.032 & 0.169\(\) 0.033 & 0.016\(\) 0.037 & \(0.004 0.034\) & \(0.122 0.021\) \\ covertype (7) & 0.35 & **0.849\(\)** 0.032 & 0.832\(\) 0.032 & 0.618\(\) 0.066 & 0.490\(\) 0.051 & 0.035\(\) 0.134 & \(-0.028 0.27\) & \(0.234 0.206\) \\ cpu (2) & 0.20 & **0.926\(\)** 0.011 & 0.874\(\) 0.008 & 0.549\(\) 0.096 & 0.298\(\) 0.084 & 0.033\(\) 0.132 & \(-0.004 0.155\) & \(0.142 0.028\) \\ default (2) & 0.30 & **0.953\(\)** 0.011 & 0.941\(\) 0.007 & 0.542\(\) 0.078 & 0.369\(\) 0.059 & 0.022\(\) 0.090 & \(0.004 0.131\) & \(0.170 0.034\) \\ gas (6) & 0.05 & **0.959\(\)** 0.009 & 0.729\(\) 0.019 & 0.204\(\) 0.076 & 0.105\(\) 0.057 & 0.033\(\) 0.068 & \(-0.017 0.094\) & \(0.063 0.032\) \\ bar (6) & 0.95 & **0.956\(\)** 0.003 & 0.714\(\) 0.012 & 0.133\(\) 0.063 & 0.074\(\) 0.057 & \(-0.004 0.050\) & \(-0.001 0.058\) & \(0.051 0.031\) \\ letter (26) & 0.50 & **0.966\(\)** 0.004 & **0.966\(\)** 0.004 & 0.530\(\) 0.043 & 0.436\(\) 0.065 & 0.012\(\) 0.011 & \(-0.015 0.148\) & \(0.197 0.031\) \\ goldigits (10) & 0.25 & **0.923\(\)** 0.010 & 0.805\(\) 0.009 & 0.867\(\) 0.039 & 0.485\(\) 0.049 & 0.024\(\) 0.076 & 0.004\(In both types of experiments, it manifests itself that the semi-value that achieves the best in ranking consistency does not coincide with the one that achieves the highest F1-score on each dataset. Nevertheless, the detailed figures in Appendix E show that weighted Banzhaf values tend to be more robust than the Beta Shapley values, which include the Shapley value. Moreover, we can conclude that there is no single universally the best semi-value across various experiment settings in terms of either ranking consistency or noisy label detection. For each utility function, it would be useful if the best semi-value can be effectively determined in advance.

## 5 Conclusion

We introduce a noise-structure-specific framework to theoretically assert that it is always a member of weighted Banzhaf values that is deemed the most robust in terms of minimizing the worst-case entropy. Theorem 1 shows that the most robust semi-value would vary across different experiment settings. Our extensive empirical study demonstrates that weighted Banzhaf values are promising when dealing with inevitable noises inherent in utility functions. Particularly, all experiments suggest that there is no universally the best or the most robust semi-value for all experiment settings. Therefore, future investigation is to set up principled approaches that can pin down the most robust or the most effective value in advance.

## 6 Limitations

The concept of Kronecker noise is still ideal for fitting the stochasticity in practice. The verification of Theorem 1 using fitted Kronecker noises can only be done based on simple utility functions, as one may expect. Nevertheless, the theoretical and empirical evidence is sufficient to argue that there is no single universally the most robust or the most effective semi-value across various experiment settings, and that weighted Banzhaf values are promising in practice.