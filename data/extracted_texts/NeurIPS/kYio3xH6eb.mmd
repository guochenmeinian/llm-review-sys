# Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation

Xiaoying Zhang

ByteDance Research

zhangxiaoying.xy@bytedance.com

&Jean-Francois Ton

ByteDance Research

jeanfrancois@bytedance.com

&Wei Shen

Fudan University

wshen21@m.fudan.edu.cn

&Hongning Wang

Tsinghua University

wang.hongn@gmail.com

&Yang Liu

UC Santa Cruz

yangliu@ucsc.edu

Equal contribution.Work done during internship at Bytedance Research

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has been pivotal in aligning Large Language Models with human values but often suffers from overoptimization due to its reliance on a proxy reward model. To mitigate this limitation, we first propose a lightweight uncertainty quantification method that assesses the reliability of the proxy reward using only the last layer embeddings of the reward model. Enabled by this efficient uncertainty quantification method, we formulate AdvPO, a distributionally robust optimization procedure to tackle the reward overoptimization problem in RLHF. Through extensive experiments on the Anthropic HH and TL;DR summarization datasets, we verify the effectiveness of AdvPO in mitigating the overoptimization problem, resulting in enhanced RLHF performance as evaluated through human-assisted evaluation.

## 1 Introduction

Reinforcement Learning from Human Feedback (RLHF) is proven to be effective for aligning Large Language Models (LLMs) with human preferences . RLHF typically involves three main steps: 1) Supervised Fine Tuning (SFT) of a pretrained LLM using high-quality data, 2) Reward Modelling to capture human preferences that the LLM should follow, and 3) Reinforcement Learning (RL) based policy optimization (e.g., PPO ) where a policy initialized from the SFT model is further improved, guided by the reward model as a proxy for human feedback.

However, a critical issue arises in this process: the reward model, built from a finite dataset of human preferences, often fails to accurately represent the underlying human preferences. This approximation error, worsened by the distribution shifts during policy updates , leads to unreliable rewards during the RL stage. This directly causes the phenomenon of reward 'overoptimization', wherein the LLM exploits erroneous high-reward states, artificially inflating the estimated proxy reward, while the ground-truth reward decreases .

Current mitigation strategies against reward overoptimization, as proposed in , focus on penalizing samples with high reward uncertainty during RL-based policy training. Specifically,  quantify reward uncertainty by training an ensemble of reward models with different seeds during either the pre-training or fine-tuning phases. They then measure the variance in estimated rewards across the ensemble to assess uncertainty in the reward prediction. However, RL policy optimizationnecessitates maintaining multiple reward models in memory, rendering it impractical for large models in real-world applications and limiting the potential to achieve maximum performance, especially since'scaling laws' typically favour larger reward models [12; 40].

To reduce the memory footprint, recent works  instead suggest training multiple LoRA-based reward models with a diversity penalty for uncertainty quantification. However, even though LoRA ensembles reduce memory requirements by only having to train/store an adapter, they still incur high training costs, as well as computational overhead during policy optimization. More specifically, during the PPO stage,  requires querying each LoRA ensemble to compute the reward and uncertainty for every sample, which can easily become a serious computational bottleneck.

In this paper, we first introduce a lightweight method for quantifying reward uncertainty in the RLHF pipeline, using only the last layer embeddings of the reward model. This approach is easily integrated into any existing trained reward models, making it generally applicable. Building on these uncertainty estimates about reward prediction, we then propose Adversarial Policy Optimization, AdvPO, a distributionally robust optimization procedure to counter overoptimization during policy improvement. AdvPO contrasts with previous sample-wise uncertainty penalization methods [9; 11; 45], for which we theoretically prove that AdvPO handles reward uncertainty in a less pessimistic manner. As a result, AdvPO is more effective at improving policy and mitigating overoptimization, which we empirically confirm in extensive experiments. These favourable results are further supported through human-assisted assessments.

To summarize, our contributions are threefold:

* Firstly, we introduce a lightweight method to quantify reward uncertainty using only the last layer embeddings of the reward model into the RLHF pipeline. Extensive experiments confirm its effectiveness in identifying reward uncertainties and signalling overoptimization.
* Secondly, we introduce the Adversarial Policy Optimization (AdvPO), built on our efficient uncertainty estimates, to adversarially target the reward model's prediction confidence interval for policy optimization. AdvPO is proven to be less pessimistic than sample-wise uncertainty penalization methods [9; 11], thus more effective at enhancing the policy and mitigating overoptimization.
* Lastly, we empirically demonstrate that AdvPO effectively addresses the reward overoptimization issue on the Anthropic HH  and TL;DR summarization  datasets. We further validate the learnt LLMs through human-assisted evaluations by comparing AdvPO against existing methods incorporating uncertainty and standard PPO, showcasing its effectiveness in real-world scenarios.

## 2 Preliminaries

### Reinforcement Learning from Human Feedback

We start by providing an overview of the RLHF workflow . This helps us establish the notations and conceptual groundwork necessary for understanding our contributions. RLHF consists of three main steps: 1) Supervised Fine Tuning, 2) Reward Modelling, and 3) RL optimization.

**Supervised Fine Tuning.** RLHF typically begins with Supervised Fine Tuning (SFT), which fine-tunes a pre-trained LLM through supervised learning on high-quality samples from downstream tasks, such as summarization or dialogue generation. We denote the resulting model as \(_{}\).

**Reward Modelling.** The second phase of RLHF involves learning a reward model to capture human preferences through annotated data \(D=\{(x^{i},y_{c}^{i},y_{c}^{i})\}_{i=1}^{N}\) where \(y_{c}^{i}\) and \(y_{c}^{i}\) denote the chosen and rejected responses to prompt \(x^{i}\). The preferences are assumed to be generated by some unknown reward model \(r^{*}(x,y)\) following the Bradley-Terry (BT) model :

\[^{*}(y_{c} y_{r}|x)=(x,y_{c}))}{(r^{*}(x, y_{c}))+(r^{*}(x,y_{r}))}.\]

Typically, a reward model \(r_{}(x,y)\) is initialized from a pretrained LLM (usually \(_{}\)), with an additional projection layer added to map the last embedding layer to a scalar reward. To be more specific, let \(e(x,y):^{d}\) denote the last layer embedding of the prompt and response pair \((x,y)\), and \(:^{d}\) denote the additional projection layer. We define the reward model as \(r_{}(x,y):=^{}e(x,y)\), where \(\) includes all the tunable parameters in \(\) and \(e(x,y)\).

Given the preference data \(D\), the reward model \(r_{}\) is trained to assign higher reward to the chosen response \(y_{c}\) than to the rejected \(y_{r}\), by minimizing the negative log-likelihood of the BT model:

\[(r_{})=-_{(x,y_{c},y_{r}) D}[( (r_{}(x,y_{c})-r_{}(x,y_{r})))],\] (1)

where \(\) denotes the sigmoid function.

**RL optimization.** Lastly, the learned reward model \(r_{}(x,y)\) is employed to guide the RL policy optimization phase (e.g., PPO ). Intuitively, the aim is to learn a policy \(_{}\) that maximizes the reward \(r_{}\) while not drifting too far away from \(_{}\):

\[_{_{}}_{x D,y_{}}[r_{}(x, y)]-_{}[_{}(y|x)\|_{}(y| x)],\] (2)

where \(\) controls the deviation from the reference policy \(_{}\), thus maintaining a balance between reward maximization and adherence to the SFT policy behaviour.

### Reward Overoptimization

A notable limitation of RLHF lies in the fact that the RL process relies on the estimated reward \(r_{}\), as opposed to the oracle/gold reward \(r^{*}\). Though widely adopted, it overlooks the potential discrepancies between \(r_{}\) and \(r^{*}\), which may arise due to inaccuracies during the reward model training. Empirical studies have reported that the RL stage tends to 'hack' the reward such that while the estimated reward (i.e., proxy reward) increases, the oracle/gold reward decreases. This phenomenon is referred to as overoptimization [12; 9; 11; 33; 27].

To mitigate this problem, in addition to the KL penalty in the original RL objective, several recent studies [9; 11; 45] propose to leverage an ensemble of \(K\) reward models \(\{r_{_{k}}\}_{k=1}^{K}\). Given a prompt \(x\) and its response \(y\), these methods use the variance of rewards across different reward models to measure uncertainty in the estimated reward, i.e., \(U_{x,y}=(\{r_{_{k}}(x,y)\}_{k=1}^{K})\). The reward is then penalized based on the sample-wise uncertainty before being used in policy optimization:

\[r_{}(x,y)=(\{r_{_{k}}(x,y)\}_{k=1}^{K})- U _{x,y}\] (3)

where \(\) controls the degree of uncertainty-based penalty. Intuitively, samples with high uncertainty during policy training are penalized to reduce the risk of being misled by imperfect rewards. However, as previously mentioned, reward ensembles that are trained either by fine-tuning entire LLMs [9; 11] or by using LoRA  incur additional memory and computational overhead. This is due to the need of maintaining multiple reward models in memory for policy learning.

Thus, an intriguing question arises: Can we quantify reward uncertainty in a lightweight manner that can be easily integrated into any trained reward models, thereby addressing the overoptimization issue without the need for burdensome ensembles? And in the following section, we provide a positive answer to this important question.

## 3 Lightweight Uncertainty Estimation

In this section, we introduce a lightweight uncertainty quantification method, based solely on the final layer embeddings of a reward model. We start by offering a high-level intuition on why the last layer embedding captures essential information about uncertainties in the reward model's predictions. Following this, we present our lightweight uncertainty quantification method.

### Connection between last layer embeddings and reward uncertainty

As discussed in Section 2.1, reward modelling can be decomposed into two parts: (1) learning a good representation \(e(x,y)\) for the prompt and response pair through a pre-trained LLM; (2) projecting the learnt representation to a scalar reward using a mapping \(\). Very importantly, the training of LLMs on extensive text corpora, coupled with their vast number of parameters, enables these models to develop versatile representations that can even be used in zero/few-shot tasks [25; 40; 4], which demonstrate the generalizability of these representations.

However, the second part, which involves learning the projection weight \(\), is strictly tied to the preference data provided during the reward model training. Consequently, the reliability of predicted rewards is closely linked to the accuracy and generalizability of the projection weight.

The above claim has been widely supported in the deep learning literature [6; 17; 31; 43]. For instance, [17; 18; 19] demonstrate that by freezing the network up to its last layer and retraining only the projection head with a smaller data set, where spurious correlation is absent, it can greatly improve robustness of the neural network model against these spurious correlations. In the context of language models, recent experiments on weak-to-strong generalization  further reinforce this claim. Their findings reveal that even when fine-tuning an LLM's last layer embedding with noisy labels from weak supervision, the model can still excel in subsequent classification tasks if the projection weight is accurately derived from ground-truth labels. This highlights the generalizability and the rich information encapsulated in the last layer representation, accessible by simple projection weights.

Building upon the notion of generalized representations with specialized projection weights, we now zoom our attention to the last layer's ability to quantify the uncertainty of its output. The projection weight is strictly estimated based on the preference data encountered during reward model training. Therefore, when evaluating the prompt and response pairs during the RL stage, the pairs might deviate from what was observed during reward model training (suggesting a distribution shift ), hence rendering the predicted rewards unreliable.

In the next section, we show how the last layer embedding of a reward model, based on preference data, can act as a feature map for an underlying kernel (similarity measure). This kernel then allows us to determine whether new prompt response pairs are similar to the ones seen during training. If not, the corresponding uncertainty should be higher and penalized during policy optimization.

### Uncertainty via Last Layer Embeddings

Many methods, derived from a neural network model's final layer embeddings, have demonstrated their effectiveness for quantifying uncertainty in the network predictions, both theoretically and in practice [43; 31]. In this work, we specifically follow the line of uncertainty quantification methods in neural bandits , due to its computational efficiency and theoretical soundness.

We first present the following theorem on reward uncertainties when learning rewards through the Bradley-Terry preference model, assuming the reward model architecture is infinitely wide.

**Theorem 3.1**.: _Assuming the network architecture is infinitely wide and its neural tangent kernel matrix is positive definite, learning rewards through the Bradley-Terry preference model yields the following inequality for the width of the confidence interval of the estimated reward \(r_{}(x,y)\). Specifically, with probability \(1-\):_

\[|r^{*}(x,y)-r_{}(x,y)| bM_{D}^{-1}e(x,y)}+ ,\] (4)

_where \(b\) is a term related to the preference dataset \(D\) and \(\) (typically the smaller \(\) is, the larger \(b\) is), \(r^{*}\) and \(r_{}\) denote the unknown ground-truth reward and estimated reward model parameterized by \(\) respectively, and \(M_{D}\) summarizes all last layer embeddings observed in the preference dataset \(D\) for the reward model, i.e., \(M_{D}= I+_{i=1}^{N}_{y\{y_{i}^{c},y_{i}^{c}\}}e(x_{i},y)e(x_ {i},y)^{}\). Here \(\) is a regulariser for the inversion._

Intuitively, Theorem 3.1 bounds the absolute difference between the predicted reward \(r_{}\) and the ground-truth reward \(r^{*}\). Consequently, it is natural to define the uncertainty around the predicted reward \(r_{}(x,y)\) as

\[U_{x,y}^{CI}=bM_{D}^{-1}e(x,y)},\]

since a larger difference between \(r_{}\) and \(r^{*}\) implies greater uncertainty. This becomes even clearer when taking a closer look at \(U_{x,y}^{CI}\). When a new prompt-response pair \((x,y)\) is similar to samples in the training data, applying the inverse of \(M_{D}\), which is constructed using the training data, results in a smaller uncertainty \(U_{x,y}^{CI}\). Conversely, if the pair diverges significantly from the training samples, the uncertainty \(U_{x,y}^{CI}\) will be high. Note that the second term in Eq.(4) is constant and can thus be omitted when comparing reward uncertainties across prompt-response pairs. Finally, it is worth pointing out that computing \(U_{x,y}^{CI}\) is computationally cheap at the policy training stage (i.e., \((d^{2})\), where \(d\) is the dimension of the embeddings) as \(M_{D}^{-1}\) can be pre-computed.

**Remark on Assumptions in Theorem 3.1.** The derivation of Eq. (4) relies on certain assumptions regarding network architectures, specifically that the network width is infinitely wide and neural tangent kernel matrix is positive definite. Recent work  that studied the Neural Tangent Kernel (NTK) in language models has also adopted similar assumptions, and its effectiveness suggests the reasonableness of these assumptions in LLMs.

**Empirical verification.** In Section 5.1, we empirically examine the effectiveness of the proposed lightweight uncertainty estimation using a synthetic setup with known ground-truth rewards. Our findings indicate that \(U_{x,y}^{CI}\) accurately captures the divergence between the ground-truth and estimated proxy rewards, effectively signalling overoptimization.

Having detailed how to obtain uncertainty regions around the predicted reward, we will now illustrate in the next section how these uncertainty estimates can be used in policy optimization.

## 4 Utilizing Uncertainty to Mitigate Reward Overoptimization

This section introduces our framework, AdvPO, to address the issue of overoptimization during policy optimization by leveraging the aforementioned lightweight uncertainty estimation.

Instead of optimizing towards a potentially incorrect point estimate \(r_{}\), which causes overoptimization, AdvPO aim to optimize the following MaxMin objective which takes into account the confidence region around the imperfect reward model \(r_{}\) that contains the ground-truth reward \(r^{*}\):

\[_{_{}}_{ C_{}^{c}()}_ {x,y_{}(|x)}[r_{}(x,y)]-_{x,y_{}(|x)}[_{}[_{}(y|x )\|_{}(y|x)]],\]

Here, \(C_{}^{c}()\) contains all possible \(\) values centered around the current estimate \(\), but also includes the optimal \(^{*}\) that yields the ground truth reward, with a probability of \(1-\). Intuitively, AdvPO aims to maximize the same objective as in standard PPO (Eq. 2), while also adversarially searching for the pessimistic reward function within the predicted reward \(r_{}\)'s confidence ball containing the ground-truth reward \(r^{*}\). However, this MaxMin objective poses some practical issues, given the inner minimization over \(C_{}^{r}()\) is intractable. Hence AdvPO makes the following observation.

**Connection between Rewards and Projection Weights:** An important corollary to Theorem 3.1, crucial to AdvPO, is that \(U_{x,y}^{CI}\), the first term of the upper bound of the reward difference \(|r^{*}(x,y)-r_{}(x,y)|\) in Eq.(4), actually represents the uncertainty stemming from the inaccuracy in the estimated projection weight \(\).

Recall that \(e(x,y)\) denotes the last layer embedding of the prompt and response pair \((x,y)\). Let \(^{*}\) and \(\) be the optimal and estimated projection weights for reward prediction respectively. Under the assumption mentioned in Section 3.2, the ground-truth reward can be approximated by a linear function of optimal projection weight \(^{*}\) and \(e(x,y)\), plus a term that can be bounded, i.e., \(r^{*}(x,y)=e(x,y)^{}^{*}+\). Considering the linearity of rewards with respect to the last layer embeddings \(r_{}(x,y)=^{}e(x,y)\), and denoting the established confidence region of the projection weight as \(\|-^{*}\|_{M_{D}} b\), we show that:

\[|r^{*}(x,y)-r_{}(x,y)| ^{*}-e(x,y)^{}|+ }_{A_{1}}\] (5) \[-\|_{M_{D}}M_{D}^{-1}e(x,y)}}_{ V_{x,y}^{CI}}+\] (6)

Therefore, the objective of the inner optimization problem in AdvPO can be relaxed to optimize an upper bound, i.e., \(A_{1}\) in Eq. (5), where the minimization is now taken over the projection weights instead of the reward functions.

\[_{_{}}_{\|-\|_{M_{D}} b}_{x,y _{}(|x)}[r_{}(x,y)]-_{x,y _{}(|x)}[_{}[_{}(y|x)\| _{}(y|x)]],\] (7)

Here, with a bit of abuse of notations, we use \(r_{}()\) to denote the reward obtained when using the projection weight \(\), while keeping the representation encoder unchanged.

It is important to note that this approach diverges significantly from conventional reward penalization methods . Rather than focusing on the worst-case scenario for each sample, our objective function adopts a more holistic perspective by minimizing across the reward functions themselves. Further details on the differences will be elaborated later in this section (Lemma 4.2).

**Incorporating Reference Responses.** To prevent AdvPO from becoming overly pessimistic, we introduce reference responses \(\{y_{}\}\) into the objective, resulting in the final objective of AdvPO:

\[_{_{}} _{\|-\|_{M_{D}} b}_{x,y_{ }(|x)}[r_{}(x,y)]-_{x,y_{}} [r_{}(x,y_{})]\] \[-_{x,y_{}(|x)}[_ {}[_{}(y|x)\|_{}(y|x)]],\] (8)

As illustrated in Lemma D.1, incorporating reference responses enforces policy optimization towards the direction of the reference responses, i.e., \(_{x,y_{}}[e(x,y_{})]\), while optimizing against pessimistic rewards. Thus as long as the set of reference responses is reasonably good and achieves a positive ground-truth reward on average, i.e, \((_{x,y_{}}[e(x,y_{})])^{} ^{*}>0\), the policy is guided to outperform the reference, preventing AdvPO from being overly pessimistic. In practice, the reference responses can be any acceptable answers, such as annotated good responses from users or responses generated by the SFT policy.

Next, we show in Theorem 4.1 that the inner minimization of Eq.(8) has a closed-form solution and hence Eq.(8) reduces to an objective function that can be optimized using standard gradient ascent.

**Theorem 4.1**.: _The optimization problem in Eq.(8) is equivalent to the following objective:_

\[_{_{}} _{x,y_{}(|x)}[r_{}(x,y)- }e(x,y)^{}M_{D}^{-1}g]-_{x,y_{}} [r_{}(x,y_{})-}e(x,y_{} )^{}M_{D}^{-1}g]\] \[-_{x,y_{}(|x)}[_ {}[_{}(y|x)\|_{}(y|x)]],\] (9)

_where \(e(x,y)\) denotes the last layer embedding of the prompt-response pair \((x,y)\), and \(g=_{x,y_{}(|x)}[e(x,y)]-_{x, y_{}}[e(x,y_{})]\) and \(^{*}=M^{-1}g}{^{2}}}\)._

Theorem 4.1 shows that the initial MaxMin objective can be loosened and rewritten into a standard Max objective for which we can use standard gradient ascent.

**Comparison to previous approaches in utilizing uncertainty against overoptimization.** As mentioned above, recent works [9; 11; 45] utilize reward uncertainty on a per-sample basis, i.e., penalizing each sample's reward based on its individual uncertainty, as illustrated in Eq.(3). While both per-sample uncertainty penalization and AdvPO adopt a pessimistic approach to leverage reward uncertainty, the degree of pessimism is crucial [16; 30; 45]. Excessive pessimism, i.e., penalizing rewards too heavily based on uncertainties, is known to impede the discovery of the correct direction for optimization, thus failing to find a good policy. To demonstrate this, we prove the following:

**Lemma 4.2**.: _Compared with the sample-wise uncertainty penalization used in [9; 11], the distributionally robust optimization objective of AdvPO in Eq. (8) utilizes uncertainty less conservatively._

This demonstrates that AdvPO is more effective in enhancing policy performance while reducing over-optimization, which we will back up with extensive large-scale experiments in the next section.

## 5 Experiments

In this section, we present our empirical results. In Section 5.1, we evaluate the effectiveness of the proposed lightweight uncertainty estimation. The effectiveness of AdvPO is demonstrated through (1) assessing whether ADVPO can mitigate the over-optimization issue in Section 5.1, and (2) examining whether AdvPO leads to an improved policy in practice in Section 5.3.

**Datasets.** We used two widely adopted datasets, Anthropic HH  and TL;DR , for empirical investigation. Additional dataset descriptions can be found in Appendix A.1.

### Empirical effectiveness of lightweight uncertainty estimation

While our goal is to signal potential overoptimization during the RL stage, we specifically examine whether the quantified uncertainties \(U^{CI}_{x,y}\) in Section 3.2 can identify discrepancies between estimatedproxy rewards and ground-truth rewards during the RL stage. We adopt a synthetic setup widely used in the literature , where we train a significantly larger "_gold-standard_" reward model that simulates human preferences and provides labels for training a proxy reward model.

For both datasets, we trained a gold reward model using the LLama-13B model and established the reward and policy model in RLHF from the LLama-7B . More details, such as gold/proxy reward model training, PPO implementation, etc., can be found in Appendix A.3.

We log the generated samples every 10 steps during the PPO training stage. Subsequently, we compute their gold reward, proxy reward, as well as reward uncertainties associated with the proxy reward. In addition to the lightweight uncertainty estimation methods (denoted as **CI**), we also investigate two ensemble-based uncertainty quantification methods: (1) **ENS-7B**: Ensemble of three LLama7B reward models; (2) **ENS-3B**: Ensemble of three 3B reward models based on OpenLLaMA3B_v2 , aiming to match the ensemble model size roughly comparable to **CI**, which quantifies uncertainties based on LLama7B. We adopt OpenLLaMA  as there are no official 3B LLama models. OpenLLaMA is an open-source smaller reproduction of Meta AI's LLaMA, demonstrating comparable performance.

Note that **ENS-7B** has only been added for completeness. **ENS-7B** requires significantly more training, memory and inference compute compared to our proposed CI. Nevertheless, we believe that this side-by-side comparison illustrates the effectiveness of our lightweight uncertainty estimation.

**Results.** The results are presented in Figure 1. We have the following two key observations:

\(\)_The lightweight uncertainty effectively captures the discrepancy between proxy and gold rewards, signalling over-optimization._ First, we observe from Figure 0(b) and 0(d) that, as the difference between gold and proxy rewards increases, the uncertainty calculated by our CI also rises. This demonstrates that our proposed CI indeed captures information about when the proxy reward is drifting away from the ground-truth reward. Furthermore, in Figure 0(a) and 0(c), it is evident that when there is a divergence between proxy rewards (blue dashed line) and gold rewards (blue solid line), indicating overoptimization, the uncertainty calculated by CI (red line) generally increases with the optimization steps. This suggests the potential to leverage them to address the overoptimization issue.

\(\)_The lightweight uncertainty estimation surpasses reward ensembles with comparable parameter sizes._ Compared to CI, ENS-3B appears to be less correlated with the divergence between gold and proxy rewards, particularly on the TL;DR dataset . As shown in Figure 0(c) and 0(d), unlike our method CI (red line), the uncertainty calculated by ENS-3B (grey line) does not exhibit a monotonically increasing trend with the reward difference between gold and proxy rewards. This is likely due to the fact the smaller reward models, in this case, 3B models, are not able to capture the preference well, thus leading to worse predictions.

We also present quantitative results on uncertainty by calculating the Pearson correlation between the estimated uncertainty and the reward differences of three algorithms. Pearson correlation measures the linear relationship between two variables, ranging from -1 to +1, where +1 indicates perfect positive correlation, 0 indicates no correlation, and -1 indicates perfect negative correlation. A higher positive correlation in our context suggests that the estimated uncertainties reliably reflect actual

Figure 1: Comparison among lightweight uncertainty estimations. In Figure 0(a) and 0(c), the blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. In Figure 0(b) and 0(d), we plot the correlation between uncertainty and the difference between gold and proxy rewards.

divergences between gold and proxy rewards. The results are reported in Table 2 in Appendix C.1. CI achieves a positive Pearson correlation, similar to ENS-7B, indicating that higher uncertainty truly implies larger reward differences. In contrast, ENS-3B shows a significantly weaker correlation, even turning negative on the TL;DR datasets, suggesting its uncertainty estimates poorly align with actual reward divergences. This further supports our earlier findings.

Additional ablation studies exploring different pretrained model sizes and ensemble configurations are presented in Appendix C. We also evaluated alternative uncertainty quantification approaches, including Bayesian uncertainty on final-layer embeddings, with details provided in Appendix E.

### AdvPO mitigates reward overoptimization

Next, we transition to evaluating the effectiveness of AdvPO. We begin by assessing whether AdvPO can mitigate reward over-optimization under the same synthetic setup described in Section 5.1, where a significantly larger "gold-standard" reward model is used to simulate human preferences.

**Results.** Figure 1(a) and Figure 1(c) illustrate how the golden reward (solid lines) and proxy reward (dashed lines) progress concerning policy optimization steps on both datasets, while Figure 1(b) and Figure 1(d) capture the dynamics with respect to the square root KL divergence, i.e., \(}(_{}||_{})}\).

\(\)_PPO suffers from overoptimization, whereas AdvPO mitigates the issue._ We can observe that PPO suffers from overoptimization across both datasets, characterized by a significant increase in proxy reward (blue dashed line), while the golden reward (blue solid line) begins to decline after reaching certain steps for both datasets. However, AdvPO mitigates over-optimization towards high but unreliable rewards, ensuring it stays within a reliable region (small KL divergence) with high golden rewards (red lines). Moreover, as shown in Figure 5 in the Appendix, the uncertainties of generated responses remain stable under AdvPO, unlike the significant increase observed with PPO. This again highlights the effectiveness of AdvPO in addressing over-optimization.

### AdvPO results in an improved policy

Next, we investigate whether AdvPO can effectively learn an improved policy in practical scenarios. Unlike the experimental setup described above, in this section, the RLHF pipeline is conducted by training the reward model based on human preferences using two datasets. The algorithm's performance is then evaluated by assessing the quality of responses generated by the resulting policy.

**Baselines.** We compare AdvPO against the following: (1) **PPO**: the token-wise PPO algorithm ; (2) **PPO-ref**: a modified version of PPO which incorporates reference responses as in Eq.(8); (3) **ENS-s** (Uncertainty-weighted optimization UWO from ): the ensemble-based approach to address over-optimization which quantifies uncertainty via ENS-3B as described in Section 5.1,

    &  &  &  \\  & & Win\(\) & Tie & Lose\(\) & \(\) & Win\(\) & Tie & Lose\(\) & \(\) \\   & PPO & \(33.5\) & \(39.5\) & \(27.0\) & \(\)**6.5** & \(50.0\) & \(20.0\) & \(30.0\) & \(\)**20.0** \\  & ENS-s & \(39.5\) & \(29.5\) & \(31.0\) & \(\)**8.5** & \(64.0\) & \(8.00\) & \(26.0\) & \(\)**38.0** \\   & PPO & \(31.0\) & \(49.0\) & \(20.0\) & \(\)**11.0** & \(75.0\) & \(7.00\) & \(18.0\) & \(\)**57.0** \\  & PPO-ref & \(35.5\) & \(39.5\) & \(25.0\) & \(\)**10.0** & \(55.0\) & \(6.00\) & \(39.0\) & \(\)**16.0** \\  & LWUN-s & \(36.0\) & \(39.5\) & \(24.5\) & \(\)**11.5** & \(67.0\) & \(3.00\) & \(30.0\) & \(\)**37.0** \\  & LoraEns & \(65.5\) & \(15.5\) & \(19.0\) & \(\)**46.5** & \(84.0\) & \(0.00\) & \(16.0\) & \(\)**68.0** \\  & ENS-s & \(43.0\) & \(26.5\) & \(30.5\) & \(\)**12.5** & \(77.0\) & \(3.00\) & \(20.0\) & \(\)**57.0** \\  & ENS-ref & \(38.0\) & \(40.5\) & \(21.5\) & \(\)**16.5** & \(76.0\) & \(5.00\) & \(19.0\) & \(\)**57.0** \\  & ENS-s-7B & \(29.3\) & \(48.8\) & \(21.9\) & \(\)**7.4** & \(60.0\) & \(7.00\) & \(33.0\) & \(\)**27.0** \\  & AdvPO-noRef & \(36.5\) & \(33.0\) & \(30.5\) & \(\)**6.0** & \(74.0\) & \(9.00\) & \(17.0\) & \(\)**57.0** \\   

Table 1: The Win rate, Lose rate, and Tie rate express the percentage of the former model’s responses that are better, worse, or similar to the latter’s. A positive difference \(\) indicates the former response is superior, with a high \(\) suggesting a significant performance gap.

utilizing three 3B reward ensembles. It then applies a sample-wise uncertainty penalty during the RL stage to counter overoptimization; (4) **ENS-ref**: a variant of ENS-s that leverages the reference responses; (5) **ENS-s-7B**: the ensemble-based approach that uses three 7B reward ensembles; (6) **LoraEns3**: a recent work  that trains LoRA-based reward ensembles to save memory costs while using sample-wise uncertainty penalties during the RL stage. Five LoRA ensembles are trained, with LoRA dimensions set at 32 and LoRA-alpha at 64; (7) **LWUN-s**: the approach that utilizes reward uncertainty calculated through CI, but through sample-wise uncertainty penalization during the PPO stage; (8) **AdvPO-noRef**: a variant of AdvPO without incorporate reference responses in Eq. (8).

**Implementation & Evaluation Details.** While GPT-4 is often employed to gauge generation quality, we noted significant position bias issues in its output. Thus, for a fair assessment of responses, we combine GPT-4 evaluation with human labelling. For additional implementation and evaluation details, please refer to Appendix A.4 and B.

**Results.** We compare the models in pairs and report their win/lose/tie ratios in Table 1.

\(\)_Lightweight uncertainty works even with sample-wise penalization._ Despite implementing sample-wise uncertainty penalization [9; 11], leveraging lightweight-calculated uncertainty, as demonstrated by LWUN-s, aids in mitigating overoptimization during policy optimization. This results in an improved policy compared to PPO. Furthermore, LWUN-s outperforms ENS-s, highlighting the effectiveness of lightweight uncertainty compared to ensembles with similar parameter sizes.

\(\)AdvPO _outperforms all baselines, with high-quality reference responses further enhancing its performance._ From Table 1, it's evident that AdvPO consistently outperforms all baselines, showing significant performance improvements, especially when the quality of reference responses is high. Specifically, on the TL;DR dataset, where the reference responses exhibit considerable quality, AdvPO achieves substantial improvements. In contrast, the Anthropic HH dataset contains noise, with reference responses varying in quality, resulting in relatively smaller improvements. Still, its advantage over PPO-ref highlights the benefits of conservatively leveraging uncertainty to address overoptimization. Additionally, compared to AdvPO-noRef, incorporating a reference improves performance, ensuring AdvPO isn't overly conservative. Lastly, while AdvPO requires only one 7B reward model, it still outperforms ENS-7B, which utilizes three 7B reward models. This performance advantage is particularly evident in the TLDR dataset, where good reference responses are available.

## 6 Related Work

**Over-optimization in RLHF.** RLHF has been a crucial approach for fine-tuning language models to align with human preferences [26; 2]. However, the standard RLHF pipeline optimizes the policy towards the estimated reward model as a proxy for human feedback, a method shown to be susceptible to overoptimization issue. This vulnerability leads to potential misalignments with true user preferences and subsequent degradation in performance [12; 9; 11].

Several recent works [22; 28; 1; 10; 15] aim to directly learn the policy model without RL optimization. However, due to supervised learning's inherent limitations, these approaches encounter challenges in generalization and are especially susceptible to out-of-preference data [21; 42]. Another line of

Figure 2: Experimental results demonstrating the mitigation of overoptimization in RLHF with AdvPO. The gold reward is represented by the solid line, while the dashed line corresponds to the proxy reward. The x-axis of Figure 1(b) and Figure 1(d) have a square-root scale.

work  aims to directly address the overoptimization issue during policy optimization by penalizing samples with high reward uncertainty, measured as variance among reward ensembles. However, fully-finetuned ensembles  not only incur high computational costs but also hinder achieving maximum performance, as the "scaling laws" generally advocate for larger reward models. On the other hand, while LoRA-based ensembles  reduce memory requirements, they still incur additional training costs and computational overhead due to querying each ensemble for each sample to calculate reward and uncertainty. Additionally, several theoretical works consider the accuracy of reward models in RLHF, primarily from an offline RL perspective . However, these works mainly contribute to the theoretical understanding of RLHF without any empirical experiments.

**Adversarial Learning in RLHF.** In addition to approaches countering over-optimization , recent work  proposes an adversarial optimization framework for iteratively updating reward and policy models. However, they utilize a min-max objective, where the inner optimization learns a policy to maximize rewards, while the outer minimization refines reward models based on provided gold preference data. Their inner optimization still directly relies on estimated rewards, thus suffering from the overoptimization problem. In contrast, our framework employs a max-min objective, where the inner minimization with a confidence region searches for rewards pessimistically, based on which the policy is then maximized. Furthermore, their work is currently implemented only with rejection sampling as the LLM updating algorithm, unlike the RL optimization stage in our approach.

## 7 Conclusion and Future work

In this paper, we introduce AdvPO, a novel approach designed to address reward overoptimization in RLHF, motivated by the effectiveness of our proposed lightweight uncertainty quantification method. Empirical experiments on the Anthropic HH and TL;DR datasets show that AdvPO effectively mitigates overoptimization without the computational burden of ensembles, leading to improved policy in practical scenarios.

**Limitations:** In this work, we only considered constructing uncertainty from the last layer of the reward model. Future work could consider constructing possibly more accurate estimates with intermediate layers as well. In addition, we only explored the use of uncertainty for model training. Exploring uncertainty estimations to actively select data for RLHF training could be a promising future direction for iterative improvement. Lastly, additional experiments on larger scale model i.e in the order of 70B, would be interesting, however this is outside the scope of this paper.

## 8 Disclosure of Funding

No additional funding or competing interests to acknowledge for this work.