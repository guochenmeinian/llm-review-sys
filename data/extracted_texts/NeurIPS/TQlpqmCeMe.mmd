# Neural Injective Functions for Multisets, Measures

and Graphs via a Finite Witness Theorem

 Tal Amir\({}^{1}\) Steven J. Gortler\({}^{2}\) Ilai Avni\({}^{1}\) Ravina Ravina\({}^{1}\) Nadav Dym\({}^{1,3}\)

\({}^{1}\) Faculty of Mathematics, Technion - Israel Institute of Technology, Haifa, Israel

\({}^{2}\) School of Engineering and Applied Sciences, Harvard University, Cambridge, USA

\({}^{3}\) Faculty of Computer Science, Technion - Israel Institute of Technology, Haifa, Israel.

###### Abstract

Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on _neural moments_ -- whose injectivity on multisets has not been studied to date.

In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used. The number of moments required by our theory is optimal essentially up to a multiplicative factor of two. To prove this result, we state and prove a _finite witness theorem_, which is of independent interest.

As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks. We also provide two negative results: (1) moments of piecewise-linear neural networks cannot be injective multiset functions; and (2) even when moment-based multiset functions are injective, they can never be bi-Lipschitz.

## 1 Introduction

Multisets are a slight generalization of sets: like sets, they are an unordered collection of elements \(\{\!\{_{1},,_{k}\}\!\}\), but unlike sets, repetitions are allowed. Multisets arise naturally in many machine-learning tasks. They are the natural way to represent point clouds in \(^{3}\), neighborhoods of vertices in graphs, and any other data that has an intrinsic order that is immaterial to the task at hand.

We refer to functions and architectures whose inputs are multisets in \(^{d}\) as _multiset functions_ and _multiset architectures_. By definition, these functions do not depend on the order in which the multiset elements are given. This is important not only because the order is irrelevant and thus should not affect the output, but also because otherwise a model may overfit the training data by making predictions based on its intrinsic order.

Multiset architectures are typically constructed using a combination of permutation-invariant operations such as sum- and max-pooling , attention mechanisms  and sorting . One simple and popular approach, pioneered in the seminal Deep-Sets paper , employs multiset functions based solely on sum-pooling. Namely, if the elements of all multisets come from some fixed _alphabet_\(\), any function \(f:^{m}\) induces a multiset function \(\), to which we refer as the _moment_ of \(f\):

\[(\{\!\{_{1},,_{k}\}\!\})=_{i=1}^{k}f( _{i}).\] (1)While moment functions of the form (1) are simple, they are quite powerful. For example, in  it was shown that if \(\) is countable, and the multisets have no repetitions (so they are just sets), then for an appropriate \(f:\), the induced function \(\) maps the input sets _injectively_ to \(\).

Injectivity is indeed a desired property for multiset functions. The search for such functions stems from the quest to find an architecture that can approximate _all_ multiset functions. Clearly, if \(\) assigns the same value \((S_{1})=(S_{2})\) to two different multisets \(S_{1} S_{2}\), then any architecture based on \(\) will yield a poor approximation of a multiset function that assigns different values to \(S_{1}\) and \(S_{2}\).

The authors of  showed that injectivity is not only necessary for approximation, but also sufficient: Under the assumption that the alphabet \(\) is countable, if the moment \(\) of \(f\) maps multisets injectively to \(^{m}\), then any multiset function \(F\) can be written as a composition of the form \(F(\{\!\!\{_{1},,_{k}\}\!\!\})=g(_{i=1}^{k}f(_ {i}))\). Motivated by this observation, the authors proposed a neural architecture of this form, with the functions \(f\) and \(g\) replaced by Multi-Layer Perceptrons (MLPs). This step was justified by the universal approximation power of MLPs.

These intriguing results inspired further research, mainly focusing on seeking injective multiset functions of the form (1) for _continuous_ alphabets such as \(=^{d}\). Preferably, such functions should (a) have a minimal _embedding dimension_\(m\) while ensuring that \(f:^{m}\) induces an injective \(\); and (b) be practical to compute. We next summarize some of these results:

For a countable \(\), the Deep-Sets paper as well as  showed that an embedding dimension \(m=1\) is sufficient. For \(=\), if the multisets contain at most \(n\) elements, and \(f\) is continuous, then an embedding dimension of \(m n\) is necessary and sufficient for injectivity .

For \(=^{d}\) and multisets of size at most \(n\), it was shown in  that \(m nd\) is necessary for injectivity. As for an upper bound on the required \(m\), while some polynomials discussed in the literature achieve injectivity with a rather high exponential  or polynomial  dimension \(m\), recent work  achieved injectivity with \(m=2nd+1\), using a polynomial \(f\) with randomly chosen coefficients -- thus achieving the lower bound essentially up to a multiplicative factor of \(2\).

While the above works provide injective multiset functions with optimal or near-optimal embedding dimension, these functions are typically polynomials, and not the MLPs used in practice. As mentioned above, many papers  justify this by the fact that MLPs can approximate any function, and thus any polynomial. However, using this argument, we have no control on the number of neurons required for injectivity -- which in some cases may be infinite, as we show in Section 4. In this paper, we address this limitation by providing a practical and efficient method to construct functions of the form (1) that are provably injective while having a near-optimal number of neurons. We now state this formally.

### Problem Statement

Let \(^{d}\) be a set, to which we refer as an _alphabet_. Denote by \(_{ n}()\) the collection of all multisets \(\{\!\!\{_{1},,_{k}\}\!\!\}\) with \(_{1},,_{k}\) and \(k n\). Any function \(f:^{m}\) induces a _moment function_\(:_{ n}()^{m}\) as in (1). If \(\) is injective, we say that \(f\) is _moment injective_ on \(_{ n}()\).

We also consider a natural generalization from multisets to measures, by identifying each multiset \(\{\!\!\{_{1},,_{k}\}\!\!\}\) with the measure \(=_{i=1}^{k}_{_{i}}\), where \(_{}\) is the Dirac measure that assigns a unit weight to \(\). In this generalized setting, the induced multiset function \(\) of (1) is just the integral of \(f\) with respect to the measure \(\). More generally, we consider signed measures \(=_{i=1}^{n}w_{i}_{_{i}}\), with weights \(w_{i}\) that can be negative, and points \(_{i}\) that belong to an alphabet \(^{d}\). We denote the space1 of all such measures by \(_{ n}()\). A function \(f:^{m}\) induces a moment function \(:_{ n}()^{m}\) defined by

\[()=_{}f()d()=_{i=1}^{n}w_{i}f(_{ i}),=_{i=1}^{n}w_{i}_{_{i}}.\] (2)If \(\) is injective, we say that \(f\) is moment-injective on \(_{ n}()\). Naturally, injectivity on \(_{ n}()\) implies injectivity on subsets of this space, such as the space of measures in \(_{ n}()\) that are probability measures, or that have only positive weights. In particular, if \(f\) is moment-injective on \(_{ n}()\), then it is moment-injective on \(_{ n}()\).

To summarize, the main questions we focus on in this paper are:

**Main Questions:** (a) Under what conditions is an MLP \(f\) moment-injective on spaces of multisets \(_{ n}()\) or measures \(_{ n}()\)? (b) How many neurons are needed to achieve this injectivity?

## 2 Main Results

Interestingly, we find that the answers to these two questions largely depend on the activation function. Consider shallow neural networks \(f:^{d}^{m}\) of the form

\[f(;,)=(+),^ {m d},\ ^{m},\] (3)

with the activation function \(:\) applied entrywise to \(+\). Suppose that \(\) is analytic and non-polynomial; such activations include the sigmoid, softplus, tanh, swish and sin. In Section 3 we show that for a large enough \(m\), such networks \(f(;,)\) with random parameters \(\),\(\) are moment-injective on \(_{ n}()\) and on \(_{ n}()\); namely, their induced moment functions \(\) of (2) are injective. This holds for various natural choices of \(\).

The embedding dimension \(m\) required in (3) depends on the dimension \(d\) of \(\): For \(=^{d}\), to achieve injectivity on \(_{ n}()\) or \(_{ n}()\), it suffices to take \(m=2nd+1\) or \(m=2n(d+1)+1\) respectively. When \(\) is countable, \(m=1\) or \(m=2n+1\) are sufficient (corresponding to \(d=0\)). In Appendix C we show that in all these cases, these embedding dimensions are optimal essentially up to a multiplicative factor of two. These results are summarized in Table 1. In Appendix C we also discuss examples where the optimal embedding cardinality for \(_{ n}()\) is obtained.

At the core of our poof of moment injectivity is a theorem which we name the _finite witness theorem_. This theorem enables reducing an infinite family of analytic equality constraints \(\{F(;)=0\ |\ \}\) to a finite subset \(\{F(;_{i})=0\ |\ i=1,,m\}\). This theorem generalizes the results in , where a special case of it was proved for semialgebraic domains and functions. The theorem we prove here (see Appendix A) applies to a much wider class of domains and functions, among which are analytic functions. In addition to our main result, we use the finite witness theorem to prove moment injectivity of Gaussian functions (Proposition 3.5) and deep MLPs (Proposition 3.6), and we believe it shall find additional applications beyond those discussed in this work.

Negative ResultsWe also prove two negative results for moment-based multiset functions: We show that in contrast to analytic activations, with piecewise linear (PwL) activations, such as ReLU, leaky ReLU and hard arctan, moment injectivity on spaces of measures \(_{ n}()\) with infinite \(\) is _impossible_. On multiset spaces \(_{ n}()\), moment injectivity with PwL activations can be obtained for some irregular, countable \(\), such as the alphabet \(=_{}\) defined in Appendix B, but not for infinite alphabets that arise naturally, like \(=^{d},^{d}\) or \((0,1)^{d}\). These results are summarized in the bottom row of Table 1. The second negative result is that while moments of MLPs with analytic activations can be injective, they can never be stable in the bi-Lipschitz sense. This points to a possible advantage

   Domain & \(_{ n}(^{d})\) & \(_{ n}(^{d})\) & \(_{ n}()\) & \(_{ n}(^{d})\) & \(_{ n}(_{})\) \\  Analytic activation & \(2n(d+1)+1\) & \(2nd+1\) & \(2n+1\) & \(1\) & \(1\) \\ Piecewise-linear activation & \(\) & \(\) & \(\) & \(\) & \(1\) \\  Lower bound & \(n(d+1)\) & \(nd\) & \(n\) & \(1\) & \(1\) \\   

Table 1: The embedding dimension required for constructing injective functions of measures and multisets. \(^{d}\) is any infinite countable alphabet. First row: dimensions for which our theorems guarantee injectivity when using analytic non-polynomial activations. Second row: with infinite alphabets, moments of a neural network of any finite size with a piecewise-linear activation cannot be injective, except in the multiset case, with some special countable alphabets such as \(_{}\), defined in Appendix B.1. Third row: lower bounds on the embedding dimension required for injectivity. These bounds show that our results from the first row of the table are optimal essentially up to a factor of two.

of injective multiset functions that are not based on moments, but rather on sorting  or max-filters . These multiset functions are not only injective but also bi-Lipschitz.

Implications for learning on multisets and graphsThe result on moment injectivity of MLPs with analytic non-polynomial activation enables us to improve upon two seminal theoretical results in the study of functions on multisets and graphs:

**(a) Universality for multisets.** In Corollaries 6.1 and 6.2, we show that any continuous function on a space of multisets or measures respectively can be presented as a continuous vector-to-vector function composed with a moment function \(\) of an MLP of the form (3). The MLP has the same embedding dimension \(m\) as in Table 1. Essentially, this result replaces the moment-injective polynomials traditionally used in the characterization of multiset functions [45; 42] by MLPs.

**(b) Separation power of Graph Neural Networks.** Famously, the ability of _Message-Passing Neural Networks (MPNNs)_ to separate distinct graphs is at most that of the Weisfeiler-Leman (WL) graph isomorphism test, with equivalence taking place if the multiset functions used in the MPNN are injective . Injective multiset functions are also used in generalizations of this result, such as the equivalence of high-order _Graph Neural Networks (GNNs)_ to high-order WL tests [31; 26], and recent results on geometric GNNs and their corresponding WL tests [15; 16; 25; 33; 8].

Using the fact that an embedding dimension of one is sufficient to achieve injectivity on \(_{ n}()\) with countable \(\), we show in Theorem 6.3 that standard MPNNs with analytic non-polynomial activations and random parameters have the separation power of WL, even when their architecture only uses a single feature per node. This can be compared on the one hand with the construction in , which also requires a single node feature but uses multiset aggregators that are not MLPs, and on the other hand with works that do consider MLPs with ReLU activations [31; 1], but require a number of node features and parameters that depends polylogarithmically on the number of nodes. In contrast, our construction requires a single node feature and a fixed number of parameters (though we have no bound on the number of bits required for achieving separation using floating-point arithmetic). A numerical verification of these results is shown in Figure 1(a), where we show that, on the 600 graphs in the TUDataset , MPNNs with three different analytic activations were equivalent to the WL-test even with a single node feature, whereas three different PwL activations were in some cases weaker than WL when a small number of node features was used.

Independently of this work, it was recently proved in  that MPNNs with certain analytic activations can separate all trees of depth two, while separation fails with PwL and even piecewise-polynomial activations. Our results here are stronger in that we show separation for _all_ graphs separable by WL, and _all_ analytic non-polynomial activations.

### Notation

We denote vectors by boldface letters, e.g. \(,\), and scalars by plain letters \(x,y\). The inner product of \(,\) is denoted by \(\). Throughout this work, the term _measure_ always refers to signed measures.

Figure 1: (a) The number of failures of graph neural networks, with varying hidden dimension and activation, to achieve WL separation on the 600 graphs from the TUDataset . Analytic activations succeed on all graphs, as Theorem 6.3 predicts. (b) The normalized smallest singular value of multiset functions induced by piecewise-linear ReLU-networks and analytic SiLU-networks. Piecewise-linear networks have singularities on squares intersecting the diagonal, leading to non-injectivity. Analytic networks are moment injective, but have singularities on the diagonal, which leads to a non-Lipschitz inverse. See the end of Section 5 for more details.

## 3 Moment injectivity with analytic activations

In this section, we prove moment injectivity for MLPs with analytic non-polynomial activations. We begin by showing that for any non-polynomial function \(:\), a measure \(_{ n}()\) is uniquely determined by the integrals of all functions \(\{(+b)^{d},b\}\). When this holds, we say that \(\) is _discriminatory_:

**Definition 3.1**.: Let \(:\) be a continuous function. We say that \(\) is _discriminatory_ if for any two signed Borel measures \(,^{}\) on \(^{d}\) that are distinct (i.e. \(^{}\)), finite (i.e. \(|(A)|,|^{}(A)|<\) for all Borel \(A^{d}\)) and compactly supported, there exist \(^{d}\), \(b\) such that

\[_{^{d}}(+b)d()_{^{d}}(+b)d^{}().\] (4)

The definition of discriminatory activation functions comes from2 Cybenko's celebrated paper on the universality of MLPs , where it was proved that sigmoid-like activations are discriminatory. This, in turn, was used to prove the universality of MLPs with such activations. In later papers [24; 32] it was shown that universality can be achieved by _all_ continuous non-polynomial activations. In the following simple proposition, we use a reverse argument to that used by Cybenko, and show that activations that allow for universality are automatically discriminatory:

**Proposition 3.2**.: _Let \(:\) be a continuous function that is not a polynomial; then \(\) is discriminatory._

Proof idea.: Suppose that \((+b)d=(+b)d^{}\) for all \(,b\). By the universality theorem for shallow MLPs , all continuous functions can be approximated by linear combinations of functions of the form \((+b)\). Thus, for any continuous function \(f, fd= fd^{}\). Since a measure is uniquely determined by its integrals of all continuous functions, \(\) is equal to \(^{}\). 

Next, we shall prove our main result: If \(\) is _analytic_ and discriminatory, then shallow MLPs of reasonable width with \(\) as activation are moment injective.

**Theorem 3.3**.: _Let \(:\) be an analytic non-polynomial function. Let \(n,d\), and set \(m=2n(d+1)+1\). Then for Lebesgue almost any \(^{m d},^{m}\), the shallow MLP \(f()=(+)\) is moment injective on \(_{ n}(^{d})\); namely, the function \(:_{ n}()^{m}\) given by_

\[()=_{i=1}^{n}w_{i}(_{i}+ )\ \ \ \ =_{i=1}^{n}w_{i}_{_{i}}\] (5)

_is injective._

_For moment injectivity on \(_{ n}(^{d})\), it suffices to take \(m=2nd+1\). For \(_{ n}()\) or \(_{ n}()\) with countable \(\), \(m=2n+1\) and \(m=1\) respectively are sufficient._

Our proof of Theorem 3.3 is based on a separate theorem, which we name the _finite witness theorem_. This theorem enables us to show that, since any two measures can be discriminated by an integral \((+b)d()\) for some choice of parameters \(\),\(b\), there exists a finite number of _witness_ parameters \((_{i},b_{i})_{i=1}^{m}\) that are sufficient for discriminating between _any_ two measures. This holds under the assumption that the number of points in both measures is bounded. We shall now state a simple version of this theorem, which suffices for proving Theorem 3.3.

**Theorem 3.4**.: _(Finite Witness Theorem, simple version) Let \(^{L}\) be a countable union of affine sets, each of which is of dimension \( D\). Let \(^{D_{}}\) be open and connected. Let \(F(;):\) be an analytic function. Then for almost any \((^{(1)},,^{(2D+1)})^{2D+1}\), the following set equality holds:_

\[\{(,)\ |\ F(;)=F(;),\ \}=\] \[\{(,)\ |\ F(;^{(i)})=F (;^{(i)}),\  i=1, 2D+1\}.\]Using the finite witness theorem, we are now ready to prove Theorem 3.3.

Proof of Theorem 3.3.: Recall that a signed measure \(_{ n}(^{d})\) can be parameterized, albeit not uniquely, by a matrix \(=(_{1},,_{n})^{d n}\) representing \(n\) points in \(^{d}\), and a weight vector \(=(w_{1},,w_{n})\), such that \(=_{i=1}^{n}w_{i}_{_{i}}\). Let \(\) be the space of measure parameters

\[=\{(,)^{n}^{d n}\}.\]

Similarly, let \(\) be the space of parameters

\[=\{(,b)^{d}\}.\]

Define \(F:\) by

\[F(,;,b)=_{i=1}^{n}w_{i}(_{i}+b).\] (6)

We now invoke the finite witness theorem. Set \(m=2n(d+1)+1\), and note that \(m=2()+1\). Recall that \(F\) is analytic. According to Theorem 3.4, for almost any choice of \((_{i},b_{i})_{i=1}^{m}\),

\[&\{((,)\,,(^{},^{ }))\,\,F(,;,b )=F(^{},^{};,b),\;\,( ,b)\}=\\ &\{((,)\,,(^{},^{})) \,\,F(,;_{i},b_{i}) =F(^{},^{};_{i},b_{i}),\; i=1, ,m\}.\] (7)

Let \(^{m d}\) with rows \(_{1},,_{m}\), and \(=(b_{1},,b_{m})\). Suppose that \(\),\(\) indeed satisfy (7).

Let \(,^{}_{ n}()\) be two measures with parameters \((,)\), \((^{},^{})\) respectively. Equation (7) implies that if the function \(\) of (5) satisfies \(()=(^{})\), then \((,)\), \((^{},^{})\) are not separated by the entire family of functions \(\{F(\,\,,,b)\,\,^{d},b \}\). Since \(\) is discriminatory, this in turn implies that \(=^{}\). This concludes the proof of moment injectivity on \(_{ n}(^{d})\).

If we are only interested in moment injectivity on \(_{ n}(^{d})\), it is sufficient to apply the theorem to

\[=_{\{0,1\}^{n}}\{(,)\,\, ^{d n}\},\]

which is a finite union of affine subspaces of dimension \(D=nd\). Thus, Theorem 3.4 only requires \(m=2nd+1\) to achieve injectivity on \(_{ n}(^{d})\). Similarly, when considering \(_{ n}()\) with a countable \(\), the theorem can be applied to a domain \(\) that can be written as a countable union of affine spaces of dimension \(n\), which yields \(m=2n+1\). Finally, \(_{ n}()\) is a countable union of points, namely zero-dimensional affine subspaces, and therefore \(m=1\) is sufficient in this case. 

### More on the finite witness theorem

The finite witness theorem can be used to prove moment injectivity for functions beyond the activated inner-product form of (5). As an example, we show in the following proposition that Gaussian functions with random parameters are moment injective:

**Proposition 3.5**.: _Let \(n,d\) and set \(m=2n(d+1)+1\). Let \(=(,)^{d}_{+}\). Then for Lebesgue almost any \((_{i},_{i})_{i=1}^{m}^{m}\), the function_

\[f()=((--_{1}\|^{2}}{_{1}^{2}} ),,(--_{m}\|^{2}}{_{m}^{2}} ))\]

_is moment injective on \(_{ n}(^{d})\)._

Proof idea.: Any two measures with bounded support can be separated by the moment of a Gaussian function supported on a small ball around a point where the measures disagree. Thus, a measure in \(_{ n}()\) is uniquely defined by the continuous family of all its Gaussian moments. The finite witness theorem then shows that a finite number \(m\) suffices.

The full version of the finite witness theorem (Theorem A.2), discussed in Appendix A, is more general than Theorem 3.4. In this version, the class of sets admissible as \(\) is the class of \(\)-subanalytic sets. While its definition is technically involved (see Appendix A), this class is quite vast: it includes all open sets, all semialgebraic sets (including affine spaces, polygons, and closed \(_{2}\)-balls), and countable unions thereof. The analyticity assumption on \(F\) is also substantially relaxed to \(\)-subanalyticity, though this requires an additional condition (13) in the theorem assumptions.

The proof of the finite witness theorem is non-trivial, and we regard it as the main technical contribution of this work. In essence, the proof generalizes a similar result in , which only applies to polynomial functions on sets defined by polynomial constraints -- known as _semialgebraic sets_. This class of sets has several nice properties, which the proof in  relies on: It is closed under linear projections, finite unions, finite intersections, and complements. Moreover, any semialgebraic set is a finite union of smooth manifolds.

Our generalization from the polynomial to the analytic setting consists of two steps: First, we generalize the theorem to a larger class of sets, called _globally subanalytic sets_, which are known to be an _\(o\)-minimal system_ -- essentially, a family of sets that has the same nice properties of semialgebraic sets mentioned above. This generalization is straightforward; however, it does not allow \(F\) to be an arbitrary analytic function, and thus does not suffice even to prove the weaker version, Theorem 3.4. Our second step is then to observe that our proof carries through also when considering countable unions of globally subanalytic sets, which we name _\(\)-subanalytic sets_. This, in turn, paves the way to prove the full version of the finite witness theorem.

Using the more general version of the theorem, we can prove the following proposition, which in particular implies moment injectivity of _deep_ networks, provided that the last activation is analytic:

**Proposition 3.6**.: _Let \(:\) be an analytic non-polynomial function. Let \(n,d\) and set \(m=2n(d+1)+1\). Let \(f:^{d}^{L}\) be an injective function that is a composition of PwL functions and analytic functions. Then for Lebesgue almost any \(^{m L},^{m}\), the function \((f()+)\) is moment injective on \(_{ n}(^{d})\)._

In particular, \(F\) could be a neural network that has increasing widths, linear layers with full rank, and injective activations that are either PwL or analytic (such as leaky ReLU or sigmoid). Therefore, Proposition 3.6 shows that increasing the network depth will not have a negative effect on its moment-injectivity. While this may seem trivial, what is not immediate in this formulation is that the embedding dimension \(m\) depends linearly on \(n d\) rather than \(n L\). The reason this is true is that the shallow neural network applied to \(F()\) will only'see' inputs that originate from the set \(F(^{d})\), and in Appendix A we show that this is a \(\)-subanalytic set of dimension \( d\).

## 4 Failure of moment injectivity for piecewise-linear functions

In this section, we show that moments of neural networks with piecewise-linear activations (such as ReLU, leaky ReLU and the hard hyperbolic tangent) cannot be injective when the alphabet is infinite, except for some singular cases discussed below.

**Proposition 4.1**.: _Let \(d,m\) and \(n 2\) be natural numbers and \(^{d}\) an open set. If \(:^{d}^{m}\) is piecewise linear, then it is not moment injective on \(_{ n}()\)._

Proof.: There exists some open \(U^{d}\) such that \(()\) is of the form \(()=+\) in \(U\). Let \(_{0} U\) and let \(^{d}\). For small enough \(>0\), we have that \(_{0}+\) and \(_{0}-\) are in \(U\). It follows that the multisets \(\{\!_{0},_{0}\!\}\) and \(\{\!_{0}-,_{0}+\!\}\) have the same moments:

\[(_{0})+(_{0})=2(_{0}+ )=(_{0}-)++(_{0}+ )+=(_{0}-)+(_{0}+ ).\]

This proves that \(\) is not moment injective on \(_{ n}()\). 

The basic idea behind the above proof is that inside a linear region of \(\), different multisets with the same center of mass have the same moments. The same idea can be used to prove failure of moment injectivity of PwL functions on \(_{ n}()\) for _any_ infinite \(\), and on \(_{ n}(^{d})\). On the other hand, PwL networks can be moment injective on \(_{ n}()\) with finite \(\), as well as on \(_{ n}()\) when \(\) is a somewhat pathological infinite countable alphabet. These results are described in Appendix B.

## 5 Failure of bi-Lipschitzness for general moment functions

In Section 3 we have shown that a neural network \(f:^{d}^{m}\) with analytic non-polynomial activation can induce an injective multiset function \(:_{ n}(^{d})^{m}\). Ideally, we wish such \(\) to be _bi-Lipschitz_, meaning that there exist constants \(0<c C\) such that

\[c W_{2}(S_{1},S_{2})\|(S_{1})-(S_{2})\|\  C W_{2}(S_{1},S_{2}),  S_{1},S_{2}_{ n}(^{d}),\] (8)

where \(W_{2}(S_{1},S_{2})\) is the 2-Wasserstein distance between the two measures \(_{1},_{2}\) that assign uniform weights to the points in \(S_{1},S_{2}\) respectively. Unfortunately, we find that _any_ moment function \(\) induced by some \(f:^{d}^{m}\) cannot be bi-Lipschitz, assuming that \(f\) is differentiable in at least one point.

**Proposition 5.1**.: _Let \(n 2\), \(d,m\), and let \(f:^{d}^{m}\) be differentiable at some \(_{0}^{d}\). Then the induced moment function \(:_{ n}(^{d})^{m}\) defined in (1) is not bi-Lipschitz._

Figure 1(b) illustrates the underlying reason for this failure of bi-Lipschitzness, and its relation to the non-injectivity of PwL moments: consider a shallow neural network \(f:^{10}\) with ReLU activations, and its induced moment function \(\{\!\!\{x_{1},x_{2}\}\!\}\!=f(x_{1})+f(x_{2})\) on multisets in \(_{ 2}()\). The left-hand side visualizes the ratio \(_{2}/_{1}\) of the smallest and largest singular values of the differential matrix \(D\). The function \(f\) is PwL, with four linear regions \(I_{1},,I_{4}\) in \(\). The linear regions of \(\) in \(^{2}\) are thus the rectangles \(I_{i} I_{j}\). As seen in the figure, there are degeneracies in the rectangles that intersect the diagonal, as for small enough \(\), \((\{\!\!\{x_{0}+,x_{0}-\}\!\})=(\{\!\!\{x_{0},x_ {0}\}\!\})\) as in the proof of Proposition 4.1. The right-hand side visualizes the same ratio when the analytic SiLU activation is used instead of ReLU. We see that \(D\) is singular on the diagonal. Intuitively, this is because the differentiability of \(f\) implies that it behaves locally like an affine function. This leads to singularities of \(\) on the diagonal, which do not prevent it from being injective, but do prevent it from being bi-Lipschitz. A proof of this phenomenon is given in the appendix. See also Theorem 21 in , which independently proved a similar result for general invariant embeddings.

## 6 Applications: Universal Approximation and Graph Separation

As mentioned in the introduction, injective multiset functions can be used to construct multiset architectures with universal approximation power, and to prove separation results for graph neural networks. In this section, we present some immediate corollaries of our results for these two applications. Proofs are in Appendix D.3.

### Universal approximation of functions on multisets and measures

Our first approximation result focuses on multisets of a fixed size \(n\) with an alphabet \(K^{d}\) that is compact. Any such multiset is determined by a choice of \(n\) vectors in \(K\), possibly with repetitions and irrespective of order. Thus, multiset functions on this space are equivalent to permutation-invariant functions on \(K^{n}\). Using the finite witness theorem and a basic topological argument, we prove:

**Corollary 6.1**.: _Let \(n,d\) and set \(m=2nd+1\). Let \(:\) be an analytic non-polynomial function. Let \(K^{d}\) be a compact set. Then there exist \(^{m d},^{d}\) such that for any continuous permutation-invariant \(f:K^{n}\), there exists a continuous \(F:^{m}\) such that_

\[f()=F(_{j=1}^{n}(_{j}+)), =(_{1},,_{n}) K^{n}.\] (9)

Combining Corollary 6.1 with the universality of MLPs, we get that any continuous permutation-invariant function on \(K^{n}\) can be approximated by expressions of the form (9) with \(F\) replaced by an MLP. Similar results were obtained for moments of polynomials rather than of MLPs in .

It is worth noting that an analogue of Corollary 6.1 cannot hold with a piecewise-linear \(\), assuming that \(K\) has a non-empty interior. This is because by Proposition 4.1, any fixed moment function induced by a PwL MLP will not be able to separate all multisets, whereas any two distinct multisets can be separated by some continuous \(f\). Though, with a PwL \(\), one may approximate any given \(f\) toarbitrary precision, by taking the embedding dimension \(m\) to infinity. In contrast, with an analytic \(\), we are able to specify a _finite_\(m=2nd+1\) for which exact equality in (9) is guaranteed.

Since our injectivity results on multisets extend to measures, it is natural to seek an extension of the above approximation result to functions defined on measures. Denote by \(_{ n}(K)\) the space of probability measures supported on \( n\) points in \(K^{d}\), endowed with the 2-Wasserstein metric.

**Corollary 6.2**.: _Let \(n,d\) and set \(m=2n(d+1)+1\). Let \(:\) be analytic and non-polynomial. Let \(K^{d}\) be compact. Then there exist \(^{m d},^{m}\) such that for any continuous (in the \(2\)-Wasserstein sense) \(f:_{ n}(K)\), there exists a continuous \(F:^{m}\) such that_

\[f()=F(_{ K}(+)d()),_{ n}(K).\]

It follows from Corollary 6.2 that any continuous function \(f:_{ n}(K)\) with compact \(K^{d}\) can be approximated to arbitrary precision by functions of the form \(()=F(_{ K}(+)d())\), with \(^{m d},^{m}\), \(m=2nd+1\), and \(F\) being an MLP.

### Graph separation

We now discuss the implications of Theorem 3.3 for graph separation, using terminology from . Let \(_{ n}()\) be the collection of all graphs \(G=(V,E,^{(0)})\) with at most \(n\) vertices, endowed with vertex features \(^{(0)}_{v}\), where \(^{d}\) is a countable alphabet. We consider GIN-like  MPNNs that recursively, for \(t=1,,T\), calculate node features \(^{(t)}_{v}\) from the previous features \(^{(t-1)}_{v}\) by

\[^{(t)}_{v}=_{u(v)}(^{(t)}(^ {(t)}^{(t-1)}_{v}+^{(t-1)}_{u})+^{(t)}).\] (10)

After the \(T\) iterations are concluded, a global feature is computed via a _readout function_:

\[_{G}=_{v V}(^{(T+1)}^{(T)}_{v}+^{( T+1)}).\] (11)

We choose all features \(_{G}\) and \(^{(t)}_{v}\) for \(1 t T\), to have the same dimension \(m\). Based on the fact that MPNNs are equivalent to \(1\)-WL when the multiset functions are injective , and on our injectivity results for countable alphabets, we prove that:

**Theorem 6.3**.: _Let \(n,d,T\) and let \(^{d}\) be countable. Let \(m 1\) be any integer. Let \(:\) be an analytic non-polynomial function. Then for Lebesgue almost any choice of \(^{(t)},^{(t)}\) and \(^{(t)}\), the MPNN defined in (10) and (11) assigns different global features to any pair of graphs \(G_{1},G_{2}_{ n}()\) that can be separated by \(T\) iterations of 1-WL._

Graph separation with continuous featuresUp to now, we have discussed graphs with node features coming from a countable alphabet. Since Theorem 3.3 applies to multisets with continuous alphabets, it can be applied to separation of graphs with continuous node features as well. In particular, the paper  explains how the random semialgebraic multiset function from  can be used to construct architectures for graphs with continuous features, whose separation power is equivalent to WL tests. Their focus is on showing that the embedding dimension in their construction depends linearly on the dimension of the _graph space_, rather than grows exponentially with the number of message-passing iterations \(T\). Similar results can now be obtained by using our random analytic multiset functions. We leave a full description of these aspects to future work.

## 7 Experiments

Empirical injectivity and bi-Lipschitzness We empirically investigated the injectivity and bi-Lipschitzness of moments of shallow networks of the form (3), by randomly generating a large number of pairs of multisets of \(n\) vectors in \(^{d}\), and computing the optimal constants \(c,C\) for which (8) holds for the generated pairs. The ratio \(c/C\) for varying activations and embedding dimension \(m\) is shown in Figure 2. Here \(d=3\) and \(n=1000\). Similar qualitative results were obtained for other values of \(d\) and \(n\); see Figure 4 in Appendix E.

We observe several interesting phenomena: First, at low embedding-dimensions, the ratio \(c/C\) for PwL networks is exactly zero, indicating that they are not injective even on the finite sample set. In contrast, for analytic activations, \(c/C\) is always positive. Indeed, we expect analytic activations to be injective on a finite sample even with embedding dimension \(m=1\), since a finite sample set has an intrinsic dimension of zero. Next, we observe that \(c/C\) naturally improves as \(m\) increases. Finally, we note that even for high \(m\), \(c/C\) is rather small. Indeed, if it were possible to consider _all_ pairs of multisets when computing \(c/C\), we would get zero for all activations and all embedding dimensions, as follows from Theorem 5.1. For additional details on this experiment, see Appendix E.1.

Graph SeparationTo validate Theorem 6.3, we conducted the following experiment: we considered 600 graphs from the TUDataset . On each graph we ran three iterations of the WL test, and three iterations of MPNNs with the Graph Convolutional Layers from  with different activations and hidden dimensions. Our goal was to check in how many graphs the MPNNs returned a vertex coloring that differs from the coloring provided by the WL test. The results are shown in Figure 1(a). As seen in the table, with the three _analytic_ activations tested, the vertex coloring of MPNN was always equivalent to 1-WL, even with a hidden dimension of 1. On the other hand, for the three _PwL_ activations, there were inconsistencies in about 1% of the graphs, even with a hidden dimension of 50.

We note that while analytic activations fully succeeded in separation, in some cases the separation was rather weak: while the distance between features of non-equivalent nodes computed by the MPNNs was typically around \(0.1\), the least-separated features had a distance of \( 10^{-7}\). In future work, it could be interesting to investigate whether MPNNs can be trained to yield larger distances between the features of all non-equivalent nodes. Further details on this experiment appear in Appendix E.2.

## 8 Conclusion

We have shown that moments of neural networks with an analytic non-polynomial activation are injective on multisets and measures. We have also shown how this can be harnessed to construct universal approximators for multiset functions, as well as prove separation results for graph neural networks. A key advantage of our approach is that it enables constructing proofs using real models that are used in practice, rather than idealized versions of them as done in previous works.

It may seem tempting, due to our theoretical results, to conclude that analytic activations should perform on multisets better than piecewise-linear activations. We stress that we make no such claim. Indeed, while the separation results in Figure 1(a) corroborate our theory, PwL networks fail to separate only \(1\%\) of the graphs in our experiment. Furthermore, at high embedding dimensions, the empirical bi-Lipschitzness in Figure 2 does not seem to strongly depend on the analyticity of the activation function. Our claim is thus much more modest: we claim that multiset architectures with analytic activations are easier to _theoretically_ analyze, and we hope that pursuing this analysis shall lead to fruitful theoretical and practical insights, which may ultimately benefit multiset architectures with either type of activation.

Lastly, we note that the finite witness theorem, which is presented here as a tool for proving moment injectivity, may prove valuable as a general tool for reducing an infinite number of equality constraints to a finite number, and we believe it will find additional applications beyond the scope of this paper.