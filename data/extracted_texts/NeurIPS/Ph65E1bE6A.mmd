# Overcoming Recency Bias of Normalization Statistics

in Continual Learning: Balance and Adaptation

 Yilin Lyu\({}^{1}\) Liyuan Wang\({}^{2}\) Xingxing Zhang\({}^{2}\) Zicheng Sun\({}^{1}\)

Hang Su\({}^{2}\) Jun Zhu\({}^{2}\) Liping Jing\({}^{1}\)

\({}^{1}\)Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University

\({}^{2}\)Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint Center for ML, Tsinghua University

{yilinlyu, zichengsun, lpjing}@bjtu.edu.cn

wly19@tsinghua.org.cn, xxzhang1993@gmail.com

{suhangss, dcszj}@tsinghua.edu.cn

Equal contribution.Corresponding authors.

###### Abstract

Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB\({}^{2}\)N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.

## 1 Introduction

Continual learning aims to acquire, accumulate and exploit knowledge from sequentially arrived tasks. As the access to old training samples is typically limited, the acquired knowledge tends to be increasingly relevant to the more recent tasks. Such recency bias, also known as catastrophic forgetting , is generally attributed to the nature of gradient-based optimization in deep neural networks (DNNs) [9; 30], where the learning of network parameters depends on a tug-of-war over currently observed training samples. Numerous efforts have been devoted to addressing catastrophic forgetting of these learnable parameters through rectifying their gradients. However, as an important component of DNNs, the normalization layers employ particular updating rules that are only partially gradient-based, and their recency bias in continual learning remains to be addressed.

The normalization layers typically include _affine transformation parameters_ and _statistics parameters_ to normalize the internal representations, in order to accelerate convergence and stabilize training [11; 33; 2; 24]. Since these two kinds of parameters are updated respectively from the gradient and statistics (i.e., the empirical mean and variance) of currently observed training samples, the normalization layers require specialized strategies to overcome recency bias in continual learning. In particular, the most popular Batch Normalization (BN)  performs an exponential moving average (EMA) of statistics parameters for testing, which preserves historical information to some extent, and thus outperforms many other alternatives in continual learning [19; 8]. As the BN statistics of old tasks still decay exponentially, some recent work [19; 8; 34] enforced a balance in the contribution of individual tasks to the BN statistics, but with limited effectiveness and generality (see Table 1 for a conceptual comparison of their targets).

So, how to update normalization statistics for better continual learning: _balance_ or _adaptation_? In this work, we present an in-depth theoretical analysis of the most popular BN, demonstrating its potential challenges posed by continual learning. First, a regular EMA with constant momentum cannot guarantee the balance of BN statistics for testing, which yields more recency bias as more training batches of each task are observed. Although a modified EMA with normalized momentum of batch number can enforce balanced statistics, they tend to be outdated for learning each task and thus limit model adaptability. Besides, the sharp changes in task distributions can interfere with the benefit of BN in stabilization of training, which further affects continual learning.

Based on these theoretical insights, we propose Adaptive Balance of BN (AdaB\({}^{2}\)N) to improve continual learning. Specifically, we leverage a Bayesian-based strategy to decide an appropriate way of accommodating task-wise contributions in normalized representations to stabilize training. We further incorporate an adaptive parameter to reconcile the constant and normalized momentum of EMA, in order to strike a balance of BN statistics for testing while training each task with adequate adaptability. Extensive continual learning experiments demonstrate the superiority of our approach, which provides substantial improvements especially for the challenging yet realistic online scenarios.

## 2 Related Work

**Continual Learning:** The major focus of this area is to address recency bias aka catastrophic forgetting in DNNs [17; 30; 28; 29]. Representative strategies are generally designed for the learnable parameters with gradient-based optimization , such as weight regularization [13; 1; 27], experience replay [32; 6], gradient projection [15; 20], etc. In particular, experience replay of a few old training samples has proven to be usually the most effective strategy and is applicable to large-scale applications [30; 32; 31]. However, its reliance on the old training samples may further deteriorate the imbalance and overfitting of normalization statistics. Corresponding to the real-world dynamics, continual learning gives rise to many meaningful settings and particular challenges . For example, a desirable continual learning model should perform well in both task-incremental and class-incremental settings, which depend on whether the task identity is provided for testing. In contrast to offline scenarios of learning each task with multiple epochs, the model should also accommodate online scenarios of learning all tasks in a one-pass data stream, which is realistic but extremely more challenging.

**Normalization Layer:** The normalization layers have become a critical component for many representative DNN architectures, where Batch Normalization (BN)  is the most popular choice along with many other competitive alternatives designed for single-task learning [33; 24; 2]. However, the normalization layers require specialized strategies to perform appropriate parameter updates in continual learning. As the EMA of statistics parameters retains a certain amount of past information, BN is naturally the dominant choice for continual learning and indeed a strong baseline for overcoming

   Method & Statistics & Training Adaptability & Online & Offline \\  BN  & Unbalanced & ✗ & ✗ & ✗ \\ CN  & Unbalanced & ✓ & ✓ & ✗ \\ BNT  & Balanced & ✗ & ✗ & ✓ \\ TBBN  & Balanced & ✓ & ✗ & ✓ \\  AdaB\({}^{2}\)N (Ours) & Adaptively balanced & ✓ & ✓ & ✓ \\   

Table 1: Comparison of normalization methods for continual learning.

recency bias in normalization layers [19; 8]. To further improve BN, Continual Normalization (CN)  removed differences in task-wise distributions and then performed BN to acquire task-balanced statistics, providing benefits for online continual learning. BNT  and TBBN  constructed task-balanced batches to update BN, but only effective for offline continual learning. However, none of the current efforts can improve both online and offline scenarios , with the effectiveness of CN varying further with batch size in different online benchmarks. This motivates our work to provide a more in-depth investigation of BN with respect to the particular challenges of continual learning.

## 3 Analysis of Normalization Statistics in Continual Learning

In this section, we introduce the formulation of continual learning and representative normalization strategies, and then provide an in-depth theoretical analysis to diagnose the most popular BN. Additionally, a notation table is included in Appendix A, and proofs of theorems can be found in Appendix B.

### Notation and Preliminaries

Let's consider a general setting for continual learning, where a neural network with parameters \(\) needs to learn a sequence of tasks from their training sets \(_{1},,_{T}\) in order to perform well on their test sets. The training set of task \(t\) is defined as \(_{t}=\{(x_{t,n},y_{t,n})\}_{n=1}^{[_{t}]}\), where \((x_{t,n},y_{t,n})\) is a data-label pair of task \(t[1,T]\) and \(||\) denotes its amount. Since the old training sets \(_{1:t-1}=_{i=1}^{t-1}_{i}\) are unavailable when learning task \(t\), \(\) tends to catastrophically forget the previously learned knowledge. An effective strategy is to save a few old training samples in a small memory buffer, denoted as \(=\{_{k}\}_{k=1}^{t-1}\), in order to recover the old data distributions when learning the current task \(t\). We denote the \(m\)-th training batch as \(B_{m}\), which includes \(N_{t}\) training samples from \(_{t}\) and \(N_{k}\) training samples from \(_{k}\) for \(k=1,,t-1\). Here \(N=_{k=1}^{t}N_{k}=|B|\) denotes the batch size. The batch index \(m(m_{t-1},m_{t}]\), where \(m_{t}=|_{1:t}|/N E\) is the index of the last batch of task \(t\), \(m_{0}=0\), and \(E\) is the number of training epochs. We denote \(r_{m}=N_{t}/N\) as the proportion of the current task \(t\) in \(B_{m}\), where \(r_{m}<1\) indicates the use of replay while \(r_{m} 1\) indicates that no replay is used. Without loss of generality, we assume that the old training samples in each batch are task-balanced, i.e., \(N_{1}=N_{2}==N_{t-1}\). The test set follows the same distribution as the training set, with the task identity provided in task-incremental learning (Task-IL) while not provided in class-incremental learning (Class-IL) .

Inside the network, the internal representation of the \(\)-th layer of the \(m\)-th batch is denoted as \(a_{m}^{()}^{N C^{()} D^{()}}\), where \(C^{()}\) and \(D^{()}\) represent the number of channels and feature dimensions, respectively. For the sake of notation clarity, we omit the layer and batch index hereafter when no confusion arises. Then, the operation of a normalization layer can be generally defined as:

\[= a^{}+, a^{}=(a,(a))+}},\] (1)

where the statistics parameters (i.e., the empirical mean and variance) are calculated from the corresponding functions \(()\) and \(^{2}(,)\), respectively, \(\) is a small positive constant to avoid the denominator being zero, \(\) and \(\) are affine transformation parameters that are learned by gradient-based optimization. Representative normalization strategies differ mainly in the ways of computing statistics parameters of internal representations, i.e., the exact forms of \(\) and \(^{2}\), as detailed below.

**Batch Normalization** (BN)  is the most popular choice for a wide range of DNN architectures. During the training phase, BN calculates the statistics parameters of internal representations for each training batch through \(:^{N C D}^{C}\) and \(^{2}:^{N C D}^{C} ^{C}\), where

\[(a)=_{n=1}^{N}_{d=1}^{D}a_{[n,,d]},^{2 }(a,(a))=_{n=1}^{N}_{d=1}^{D}(a_{[n,,d]}-(a))^{ 2}.\] (2)

During the testing phase, to make the normalization independent of other samples in the same batch, BN replaces the batch statistics of Eq. (1) with the population statistics estimated by exponential moving average (EMA). For ease of notation, here we define a convenient function \(S:^{N C D}^{2 C}\) to obtain the two kinds of statistics, where

\[S()=[(),^{2}(,())]^{}.\] (3)Then we denote the statistics of internal representation \(a_{m}^{()}\) as \(S_{m}:=S(a_{m})\), with the layer index omitted. Now the population statistics of BN can be written as

\[}[S_{m}]:=}[S|a_{1},,a_{m}]=(1- )}[S|a_{1},,a_{m-1}]+ S_{m},\] (4)

where \((0,1)\) is the momentum of EMA and usually set to 0.1 in practice.

There are many alternative improvements of BN designed for _single-task learning_ without the use of EMA. **Group Normalization** (GN)  can be seen as a general version, which divides the channels into \(G\) groups and then normalizes the internal representations of each group. Specifically, let \(:^{N C D}^{N G K  D}\) be a reshaping function that groups the channels, where \(G\) is the group number and \(K=\). Then the mean and variance of GN are obtained through \(:^{N C D}^{N G}\) and \(^{2}:^{N C D}^{N G} ^{N G}\), where

\[(a)=_{k=1}^{K}_{d=1}^{D}(a)_{[:,:,:,k,d]}, ^{2}(a,(a))=_{k=1}^{K}_{d=1}^{D}((a)_{[:,:,k,d] }-(a))^{2}.\] (5)

In particular, GN is equivalent to **Layer Normalization** (LN)  by putting all channels into one group (\(G=1\)), and **Instance Normalization** (IN)  by separating each channel as a group (\(G=C\)). In contrast to these alternatives, **Continual Normalization** (CN)  is designed particularly for _(online) continual learning_, which performs GN with \(=1\) and \(=0\) to remove task-wise differences and then performs BN to acquire task-balanced statistics.

### Conflict of Balance and Adaptation

Although BN has become a fundamental component for many DNN architectures, its particular recency bias in continual learning remains under addressed. To quantitatively analyze the task-wise contributions in BN statistics, we define a statistical weight (of a certain batch) of each task. Then we diagnose the recency bias of BN statistics with a specified dilemma between balance and adaptation.

**Definition 1** (Statistical weight of a task).: _The statistical weight of task \(t\) w.r.t. the population statistics \(}[S_{m}]\) is the weight \(w_{t}^{+}\) such that \(w_{t}=_{i}^{m}w_{i}^{t}/Z\). \(w_{i}^{t}^{+}\) is the statistical weight of task \(t\) of the \(i\)-th batch such that \(_{t=1}^{T}_{i=1}^{m}w_{i}^{t}S_{i}^{t}=[S_{m}]\), where \(S_{i}^{t}:=S(a_{i,(( y_{n})_{t}]})\).3\(Z\) is the normalizing constant \(Z=_{t}^{T}_{i}^{m}w_{i}^{t}\)._

We now give rigorously the statistical weight for EMA in order to demonstrate the recency bias:

**Theorem 1** (Statistical weight for EMA).: _Let \(T,Z\) be defined as above, and \(\{m_{t}\}_{t=1}^{T}\) be an increasing sequence (i.e., \(m_{t}<m_{t+1}\)), where \(m_{t}^{+}\), \(:[1,m_{T}](0,1)\), and \(r:[1,m_{T}](0,1]\). The statistical weight of task \(t[1,T]\) w.r.t. the population statistics \(}[S_{m_{T}}]\) estimated by Eq. (4) is_

\[w_{t}=[_{i=m_{t-1}+1}^{m_{t}}_{i}r_{i}_{j=i+1}^{m_{T}}(1- _{j})+_{i=m_{t}+1}^{m_{T}}(1-r_{i})}{T-1}_{j=i+1}^{ m_{T}}(1-_{j})]/Z,\] (6)

_where we define \(_{i}:=(i)\) and \(r_{i}:=r(i)\) for brevity._

Eq. (6) specifies the impact of each task on the EMA population statistics, which exposes **a dilemma between balance and adaptation**, as we demonstrate below.

**Corollary 2** (Adaptation of BN statistics).: _Let \(\{m_{t}\}_{t=1}^{T}\) be defined as above. If \(r() 1\) and \(()=1-\), the statistical weight of task \(t[1,T]\) w.r.t. the population statistics \(}[S_{m_{T}}]\) is \(w_{t}=^{m_{T}-t}-^{m_{T}-t+1}}{1-^{m_{T}}}/Z\). When \(m_{T}-m_{T-1}=m_{T-1}-m_{T-2}==m_{1}\), we have \(w_{t}^{m_{1}(T-t)}/Z\), with approximation error \(||(^{m_{1}}}{1-^{m_{1}}})\)._

Corollary 2 shows that without the use of replay, a model dealing with evolving task distributions has a strong preference for the distribution of the current task, as the statistical weights of past tasks decrease exponentially. Replay of old training samples can balance statistical weights of all seen tasks to some extent. However, it cannot eliminate the recency bias except in an _unrealistic_ case:

**Corollary 3** (Balance of BN statistics).: _Let \(\{m_{t}\}_{t=1}^{T}\) be defined as above. If \(r() r\), \(()=1-\), and \(m_{T}-m_{T-1}=m_{T-1}-m_{T-2}==m_{1}\), then the statistical weight of each task \(t[1,T]\) w.r.t. the population statistics \(}[S_{m_{T}}]\) achieves (approximately) equal if and only if \(r 1/T\), with approximation error \(||(^{m_{1}})\)._

Corollary 3 shows that it is possible to balance the statistical weights by constructing a task-balanced batch with the memory buffer. However, this will severely slow down the convergence. Besides, as the memory buffer is usually limited in size, the model can easily overfit to the preserved old training samples , which is detrimental to generalization. Corollary 3 also demonstrates the importance of keeping flexibility of \(\). For instance, if we substitute \((i)=1/(1+i)\) into Eq. (4), this leads to a normalized momentum of batch number. In this case the population statistics can be considered as the cumulative moving average (CMA)  of the batch statistics, which can balance the statistical weights as below:

**Corollary 4** (Cma).: _Let \(\{m_{t}\}_{t=1}^{T}\) be defined as above. If \(r() r\), then the statistical weight of each task \(t[1,T]\) w.r.t. the population statistics \(}[S_{m_{T}}]\) achieves equal if \((i)=1/(1+i)\)._

However, the balanced statistical weights do not necessarily lead to better performance, as the statistics of past tasks may be outdated relative to the current model. This is empirically validated in , where CMA does not outperform EMA in continual learning. Therefore, an improved strategy of updating population statistics for testing is urgently needed to reconcile balance and adaptation.

In addition to the update of population statistics during the _testing_ phase, the current strategy of calculating batch statistics during the _training_ phase is also sub-optimal for continual learning, as detailed in the next subsection.

### Disturbed Training Stability

BN has been shown to stabilize training and improve generalization under traditional i.i.d. data setting [3; 21; 16]. However, it is in fact not clear whether BN can achieve the same benefit in continual learning, especially when the model has just switched to a new task with drastic changes in loss and parameters. We perform a preliminary analysis by the following theorem:

**Theorem 5** (Theorem 4.1 of , restated).: _Let \(,^{2},,,a,a^{}\) be defined as above. Let \(\) be the loss function of a BN model (i.e., using the normalized representations via Eq. (1)) and \(^{}\) be the identical loss function of an identical non-BN model (i.e., \(a^{} a\)). The gradient w.r.t. the internal representation \(a\) of the \(\)-th layer follows that_

\[\|_{a_{[..]}}\|^{2}}{_{[,j]}^{2}C }[C\|_{a_{[..]}}^{}\|^{2}-(^{} _{a_{[..]}}^{})^{2}-(_{a_{[..]}}^{}^{} a_{[..]}^{})^{2}].\] (7)

Theorem 5 demonstrates that BN can contribute to reducing the Lipschitz constant of the loss and thus stabilizing the training of a single task. However, as the model underfits to the distribution of each continual learning task, the upper bound of the gradient magnitude will be loosened by the increase of \(\{_{a}^{}^{} a^{}\}\) within the third term, i.e., the similarity of the normalized representation and its corresponding gradient, which potentially affects the benefit of BN. Here we present empirical results to validate the above claim. As shown in Fig. 1, a dramatic change in loss \(\) occurs whenever the task changes. We observe an increase of cosine similarity between the gradient and the normalized representation at this time, accompanied by a dramatic change in the gradient magnitude. This clearly demonstrates the importance of properly adapting BN statistics to each continual learning task, so as to better leverage the benefit of BN in stabilization of training.

## 4 Method

Following the theoretical analysis, we propose Adaptive Balance of BN (AdaB\({}^{2}\)N) from two aspects: (1) **training aspect**: we apply a Bayesian-based strategy to adjust inter-task weights and construct the normalized representation, allowing for learning an appropriate way of weighting task distributions to stabilize training; and (2) **testing aspect**: instead of using a fixed momentum \(\) in EMA, we turn to a generalized version that balances the statistical weights of past tasks while reducing the impacts of past batches. We now present both ideas as follows with a pseudo-code in Algorithm 1.

### Adaptive Balance of BN Statistics for Training

Theorem 5 demonstrates that the benefit of BN for optimization and generalization is potentially interfered in continual learning. To overcome this challenge, here we propose a Bayesian-based strategy for learning an appropriate way of weighting task contributions in normalization statistics. We assume the statistics of batch \(m\) for a certain layer follow a probability distribution \(S_{}(S|a_{m})\), where the randomness originates from the neural network parameters \(\), and then use the conditional expectation w.r.t. the distribution of statistics to calculate the normalized representation:

\[a_{m}^{}=-[|a_{m}]}{[^{2}|a _{m}]+}}.\] (8)

Specifically, \([|a_{m}]\) and \([^{2}|a_{m}]\) are the two elements of \([S|a_{m}]\) as described in Eq. (3):

\[[S|a_{m}]=_{S}_{t}_{}(S|t,a_{m}) _{}(t|a_{m})S^{t},\] (9)

where \(_{}(S|t,a_{m})\) is the distribution of statistics given the task identity, and \(_{}(t|a_{m})\) is the likelihood of the task identity occurring in \(a_{m}\). We treat \(_{}(t|a_{m})\) as a categorical distribution over the task identity, i.e., \(_{}(t|a_{m})=N_{t}/N\). Then, BN obtains its normalization statistics by the point estimate of the conditional expectation:

\[}[S|a_{m}]=_{t}_{}(t|a_{m})S_{m}^{t}=S_{m}.\] (10)

Figure 1: Dynamics of the loss of training batches, gradient similarity (i.e., cosine similarity between gradients and normalized representations) and gradient magnitude.

As analyzed in Section 3.3, it is critical to consider adaptability of the model parameters \(\) together with normalization statistics in model training, so as to enhance the gradient-representation correlation. Inspired by the theoretical works in Bayesian Inference [22; 12], we propose to capture the variability of \(\) through modeling the task distribution with a Bayesian principle, thus enhancing the benefit of BN for continual learning. Specifically, we assume that there is a random variable \(^{T-1}\) modeling the probability distribution of tasks, where \(^{T-1}\) denotes the \(T-1\) simplex, i.e., \(_{t}^{T}_{t}=1\) and \(_{t}>0, t\). Applying the Bayes' rule, we have

\[[S|a_{m}]=_{S}_{}_{t}_{}(S|,t,a_ {m})_{}(|a_{m})_{}(t|,a_{m})S^{t},\] (11)

where \(_{}(S|,t,a_{m})\) and \(_{}(t|,a_{m})\) are defined analogous to \(_{}(S|t,a_{m})\) and \(_{}(t|a_{m})\) in Eq. (9). \(_{}(|a_{m})\) serves as the prior captured from the model parameters \(\), which is generally intractable. For ease of computation, we approximate it with a Dirichlet distribution, i.e., the conjugate prior of the categorical distribution, which is parameterized by the concentration parameter \(^{T+}\):

\[_{}(|a_{m})=)}{_{t}( _{t})}_{t=1}^{T}_{t}^{_{t}-1},\] (12)

where \(()\) is the Gamma function, and \(=_{t}_{t}\). By taking advantage of algebraic convenience of the conjugate distribution, we can estimate the conditional expectation by a closed-form expression

\[}[S|a_{m}]=_{}_{t}_{}(|a_{m}) _{}(t|,a_{m})S_{m}^{t}=_{t}+N_{t}}{ +N}S_{m}^{t}.\] (13)

Intuitively, the concentration parameter \(\) plays the role of adding pseudo-observations to the task weighting procedure, which allows the normalization statistics to adaptively balance task-wise contributions by considering the current state of \(\). To optimize \(\), we introduce a learnable parameter \(^{T}\) and obtain \(=()\). It is worth noting that the dimension of \(\) is incremental with the number of seen tasks, so there is no need to know the total number of tasks in advance. As \(\) is expected to capture the information of \(\), we propose a regularization term to minimize the Euclidean distance between the adaptively balanced statistics and the estimated population statistics via Eq. (4) as follows:

\[_{ada}^{()}(,)=\|}[S|a_{m}]-}[S_{m}]\|^{2},\] (14)

where \(}[S_{m}]\) is an improved version introduced in Section 4.2. Eq. (14) provides \(\) with historical information across batches by aligning the population statistics, which contributes to obtaining statistics as good as joint training (detailed in Section 5). From Fig. 1 we observe that adding this regularization term can significantly improve the gradient similarity, leading to stabilized training dynamics. We optimize \(\) and \(\) (with \(\) and \(\) inside) simultaneously, and the final objective function can be written as

\[_{,}_{CL}()+_{}_{ada }^{()}(,),\] (15)

where \(_{}\) is the loss function of continual learning (e.g., a cross-entropy loss over \(_{t}\) and \(\{_{k}\}_{k=1}^{t-1}\) for classification tasks). \(\) is a hyperparameter to balance the two terms.

### Adaptive Balance of BN Statistics for Testing

Inspired by Corollaries 2 to 4, we propose to employ an appropriate \(\) function to strike an adaptive balance between EMA and CMA. Concretely, we design a modified momentum \(:[1,m_{T}](0,1)(0,1)\) satisfying the following recurrence relation:

\[_{i}:=(i,_{i-1})=}{_{i-1}+(1-)^{ }},_{0}:=^{},\] (16)

where \((0,1)\) is the modified momentum of EMA and \(\) is an additional hyperparameter to control the balance of normalization statistics. In particular, Eq. (16) degenerates to EMA when \( 1\) and to CMA when \( 0\). This allows us to find an inflection point for adaptive balance of statistical weights, so as to enjoy the benefits of both EMA and CMA.

## 5 Experiments

In this section, we first briefly introduce the experimental setups of continual learning, and then present the experimental results to validate the effectiveness of our approach.

**Benchmark**: We mainly focus on the setting of online continual learning , which is extremely challenging but realistic in application, and further consider the setting of offline continual learning. Here we evaluate three benchmarks that are widely used in both settings. Specifically, Split CIFAR-10  includes 10-class images of size \(32 32\), randomly split into 5 disjoint tasks with 2 classes each. Split CIFAR-100  includes 100-class images of size \(32 32\), randomly split into 20 disjoint tasks with 5 classes each. Split Mini-ImageNet  includes 100-class images of size \(84 84\), randomly split into 20 disjoint tasks with 5 classes each. We consider both Task-IL and Class-IL, depending on whether the task identity is provided at test time .

**Baseline**: Following the previous work , we employ ER-ACE  and DER++  as the baseline approaches for continual learning, both of which rely on a small memory buffer \(\) to replay old training samples. We further consider LiDAR  in offline scenarios, a recent approach for improving experience replay through regularizing Lipschitz continuity. We then compare our approach to three categories of normalization layers: (1) BN , which is the most popular choice that exhibits strong continual learning performance [19; 8]; (2) BN's alternatives designed for single-task learning, such as LN , IN  and GN  (GN can be seen as a general version of LN and IN); and (3) CN , which is the state-of-the-art baseline that improves BN for online continual learning.

**Implementation**: We follow the official implementation as many previous works [19; 4; 7; 6], derived from the _mammoth_ repository . Specifically, we use a ResNet-18 backbone and train all baselines with an SGD optimizer of learning rate 0.03 for Split CIFAR-10/-100 and 0.1 for Split Mini-ImageNet. Notably, we find the official implementation of CN  employs different batch sizes (either 10 or 32) for different online continual learning benchmarks. We argue that using a smaller batch size is more "online" and more challenging, as fewer training samples of the current task can be accessed at a time. Therefore, we reproduce all results of online continual learning with one epoch and a small batch size of 10, and further evaluate the effect of using a larger batch size of 32. Besides, we use 50 epochs with a batch size of 32 for offline continual learning. We adopt reservoir sampling to save

   &  &  &  \\  & \(||\)=500 & \(||\)=2000 & \(||\)=2000 & \(||\)=5000 & \(||\)=2000 & \(||\)=5000 \\  ER-ACE w/ BN & 86.34\(\)2.35 & 89.17\(\)1.21 & 61.21\(\)1.63 & 63.83\(\)1.94 & 63.62\(\)3.14 & 64.60\(\)1.73 \\ ER-ACE w/ LN & 78.01\(\)6.78 & 81.59\(\)4.39 & 42.29\(\)0.29 & 44.32\(\)0.35 & 45.91\(\)1.63 & 45.82\(\)1.91 \\ ER-ACE w/ IN & 84.57\(\)1.42 & 86.03\(\)1.99 & 49.46\(\)2.25 & 50.15\(\)1.71 & 41.07\(\)1.66 & 40.93\(\)4.38 \\ ER-ACE w/ GN & 81.46\(\)3.74 & 82.85\(\)2.14 & 44.84\(\)1.41 & 42.50\(\)2.06 & 43.45\(\)3.29 & 45.43\(\)0.62 \\ ER-ACE w/ CN & 88.32\(\)1.43 & 90.01\(\)0.95 & 61.00\(\)1.58 & 63.42\(\)0.65 & 64.23\(\)1.22 & 65.14\(\)1.48 \\ ER-ACE w/ Ours & **88.74\(\)1.77** & **90.84\(\)2.01** & **63.88\(\)1.58** & **67.01\(\)2.90** & **66.78\(\)1.91** & **68.03\(\)0.41** \\  DER++ w/ BN & 87.61\(\)1.67 & 90.42\(\)1.83 & 65.53\(\)1.17 & 66.23\(\)0.94 & 61.70\(\)2.40 & 61.28\(\)1.29 \\ DER++ w/ LN & 81.35\(\)4.25 & 82.80\(\)5.48 & 43.24\(\)0.86 & 44.42\(\)2.90 & 40.05\(\)3.67 & 37.64\(\)4.26 \\ DER++ w/ IN & 84.81\(\)2.99 & 87.04\(\)2.60 & 47.39\(\)1.46 & 49.17\(\)2.13 & 32.88\(\)1.16 & 35.23\(\)6.11 \\ DER++ w/ GN & 82.05\(\)1.24 & 83.34\(\)3.34 & 43.54\(\)1.47 & 45.12\(\)1.15 & 40.65\(\)1.76 & 38.26\(\)3.26 \\ DER++ w/ CN & 86.92\(\)0.89 & 89.75\(\)0.76 & 66.20\(\)0.38 & 67.39\(\)1.88 & 65.09\(\)1.76 & 66.14\(\)1.40 \\ DER++ w/ Ours & **90.15\(\)2.52** & **91.99\(\)0.81** & **70.33\(\)0.49** & **71.70\(\)0.84** & **66.42\(\)3.62** & **69.12\(\)2.51** \\  

Table 2: Performance of **online task-incremental learning** with batch size \(|B|=10\). We report the final average accuracy of all seen tasks (\(\)) with \(\) standard deviation.

Figure 2: Performance improvements (i.e., \(\) FAA) for online continual learning over different batch sizes \(|B|\) and memory sizes \(||\).

old training samples, which simulates the distribution of incoming batches, and further consider ring buffer that enforces a balanced number for each class . The memory buffer is sized differently to evaluate its impact. The group number of GN and CN is set to 32 as . All results are averaged over three runs with different random seeds and task splits. Please see Appendix C for more details.

**Overall Performance:** We first evaluate our approach for _online_ continual learning, including the settings of online Task-IL in Table 2 and online Class-IL in Table 3. Consistent with reported results , BN performs much better than its alternatives designed for single-task learning (i.e., GN, LN and IN), and is moderately improved by the state-of-the-art CN. In contrast, our approach can improve the performance of BN by a large margin (e.g., up to 7.68%, 6.86% and 4.26% for online Class-IL on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively), and outperforms CN on a wide range of _memory sizes_\(||\). When increasing the _batch size_\(|B|\) from 10 to 32, which introduces more training samples at once and makes online continual learning more "offline", we observe that the benefits of CN exhibit variation in many cases, while the benefits of our approach remain consistent and become even stronger (e.g., up to 15% on Class-IL of Split CIFAR-10, see Fig. 2). Meanwhile, the _selection strategies_ of the memory buffer \(\) have no substantial effect, where our approach can provide a similar magnitude of improvement under either reservoir sampling or ring buffer (see Appendix D). We further present the results of _offline_ Task-IL and Class-IL on the hardest Split Mini-ImageNet of the selected benchmarks (see Table 4), where CN even results in negative impacts on BN while our approach still provides strong performance gains in most cases.

**Ablation Study & Analysis:** We first analyze the introduced two _hyperparameters_ in our approach, i.e., \(\) of the regularization term for training and \(\) of the modified momentum for testing. As shown in Fig. 3, the two hyperparameters can clearly influence the performance of continual learning, which

   &  &  &  \\  & \(||\)=500 & \(||\)=2000 & \(||\)=2000 & \(||\)=5000 & \(||\)=2000 & \(||\)=5000 \\  ER-ACE w/ BN & 48.86\(\)2.87 & 52.25\(\)2.55 & 24.55\(\)1.37 & 26.07\(\)2.78 & 14.41\(\)1.31 & 14.53\(\)0.58 \\ ER-ACE w/ LN & 32.29\(\)0.37 & 36.64\(\)1.92 & 11.50\(\)0.10 & 11.96\(\)1.13 & 6.28\(\)1.08 & 6.21\(\)1.07 \\ ER-ACE w/ IN & 44.16\(\)1.98 & 45.86\(\)3.25 & 15.20\(\)0.87 & 16.15\(\)0.80 & 5.00\(\)0.82 & 5.13\(\)1.48 \\ ER-ACE w/ GN & 38.97\(\)2.04 & 39.60\(\)3.99 & 12.25\(\)0.83 & 11.86\(\)0.74 & 5.33\(\)0.83 & 6.29\(\)0.60 \\ ER-ACE w/ CN & 50.54\(\)4.94 & 53.97\(\)1.20 & 23.87\(\)1.14 & 27.23\(\)0.37 & 15.76\(\)0.70 & 15.39\(\)1.45 \\ ER-ACE w/ Ours & **52.51\(\)3.04** & **59.93\(\)4.49** & **25.39\(\)1.40** & **28.15\(\)3.24** & **16.41\(\)0.96** & **17.08\(\)1.06** \\  DER++ w/ BN & 52.85\(\)4.34 & 57.96\(\)2.57 & 18.84\(\)1.25 & 17.26\(\)1.91 & 7.04\(\)1.47 & 6.59\(\)0.67 \\ DER++ w/ LN & 34.08\(\)2.59 & 35.37\(\)6.18 & 6.42\(\)0.95 & 6.25\(\)0.60 & 2.94\(\)1.08 & 2.21\(\)0.43 \\ DER++ w/ IN & 45.21\(\)1.88 & 50.86\(\)2.61 & 8.52\(\)0.27 & 8.58\(\)0.46 & 1.88\(\)0.33 & 2.33\(\)0.83 \\ DER++ w/ GN & 39.54\(\)1.47 & 38.48\(\)1.13 & 7.44\(\)0.78 & 6.77\(\)0.52 & 2.80\(\)0.18 & 2.72\(\)0.54 \\ DER++ w/ CN & 55.28\(\)1.06 & 60.25\(\)3.39 & 18.84\(\)0.09 & 18.95\(\)1.05 & 10.16\(\)1.07 & 8.94\(\)0.94 \\ DER++ w/ Ours & **59.44\(\)2.65** & **64.83\(\)1.89** & **23.79\(\)2.23** & **24.12\(\)1.63** & **10.80\(\)2.76** & **10.85\(\)1.58** \\  

Table 3: Performance of **online class-incremental learning** with batch size \(|B|=10\). We report the final average accuracy of all seen classes (\(\)) with \(\) standard deviation.

Figure 3: Hyperparameter analysis of online task-incremental learning and class-incremental learning for ablation study.

validates the effectiveness of each component in AdaB\({}^{2}\)N, and obtain consistent improvements under a wide range of combinations in different settings. Besides, we visualize the _statistics parameters_ of normalization layers in continual learning, in order to explicitly demonstrate how our approach can improve BN (see Fig. 4 for a typical example and Appendix D for all results). We take joint training (JT) of all tasks as the upper bound baseline for continual learning. As more training batches are introduced, it can be obviously seen that the BN statistics are increasingly deviated while the CN statistics are enforced to stabilize. They correspond to the biased strategies of _adaptation_ and _balance_, respectively, but neither can fit into the curve of JT. In contrast, our approach achieves an adaptive balance of normalization statistics in continual learning, matching closely the upper bound baseline.

## 6 Discussion and Conclusion

In this work, we focus on the most popular BN in continual learning and theoretically analyze its particular challenges for overcoming recency bias. Our analysis demonstrates the dilemma between balance of all tasks and adaptation to the current task, reflected in both training and testing. Following such theoretical insights, we propose Adaptive Balance of BN (AdaB\({}^{2}\)N) with a Bayesian-based strategy and a modified momentum to implement BN in a continual learning fashion, which achieves strong performance gains across various scenarios. Interestingly, the robust biological neural networks address a similar dilemma with a combination of normalization mechanisms for neurons, synapses, layers and networks, some of which are thought to be analogous to the normalization layers in DNNs . More in-depth analysis and modeling of these mechanisms would be a promising direction. We also expect subsequent work to further improve continual learning of other DNN components and overcome potential incompatibilities in optimization, as suggested by our work on normalization layers.

Now we discuss some potential limitations and societal impacts. First, our work applies only to the DNN architectures that employ normalization layers, especially the most popular BN. Second, we need to compute a categorical distribution \(_{}(t|a_{m})\) over the task identity, which is difficult to adapt directly to the more general setting of task-agnostic continual learning. Third, the prior \(_{}(|a_{m})\) is approximated as a Dirichlet distribution for ease of computation, which may not be applicable in some practical scenarios. We argue that the broader impact is not obvious at the current stage, as this work is essentially a fundamental research in machine learning.