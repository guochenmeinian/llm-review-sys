# NeuMA: Neural Material Adaptor for Visual

Grounding of Intrinsic Dynamics

 Junyi Cao\({}^{1}\)   Shanyan Guan\({}^{2}\)   Yanhao Ge\({}^{2}\)   Wei Li\({}^{2}\)   Xiaokang Yang\({}^{1}\)   Chao Ma\({}^{1}\)

\({}^{1}\) MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

\({}^{2}\) vivo Mobile Communication Co., Ltd.

{junyicao, kkyang, chaoma}@sjtu.edu.cn

{guanshanyan, halege, liwei.yxgh}@vivo.com

This work was done during an internship at vivo.  \({}^{}\)Project lead.  \({}^{}\)Corresponding author.

###### Abstract

While humans effortlessly discern intrinsic dynamics and adapt to new scenarios, modern AI systems often struggle. Current methods for visual grounding of dynamics either use pure neural-network-based simulators (black box), which may violate physical laws, or traditional physical simulators (white box), which rely on expert-defined equations that may not fully capture actual dynamics. We propose the Neural Material Adaptor (_NeuMA_), which integrates existing physical laws with learned corrections, facilitating accurate learning of actual dynamics while maintaining the generalizability and interpretability of physical priors. Additionally, we propose Particle-GS, a particle-driven 3D Gaussian Splatting variant that bridges simulation and observed images, allowing back-propagate image gradients to optimize the simulator. Comprehensive experiments on various dynamics in terms of grounded particle accuracy, dynamic rendering quality, and generalization ability demonstrate that NeuMA can accurately capture intrinsic dynamics. Project Page: https://xjay18.github.io/projects/neuma.html.

## 1 Introduction

Teaching a machine to "see, understand, and reason" the physical world like humans has been a fundamental pursuit of machine learning and cognitive science. Imagine the flexibility, robustness, and generalizability of human intelligence: simply by observing an object falling on the ground and bouncing up, even a young child can make a plausible guess of its intrinsic dynamics (by telling what material it is made of), adapt to new scenarios (involving new objects and initial conditions) and predict interactions with other objects. However, modern AI systems still fail to match this cognitive ability, known as visual grounding, of humans.

Many efforts have thus been made to impart the ability of visual grounding of dynamics to AI systems , typically by training a differentiable simulator  with pixel supervision from a differentiable renderer . Depending on the formulation of the simulator, current works can be categorized into two types: black box or white box. Black box approaches  directly use a neural network to model the dynamic transition. Due to the deep coupling among extrinsic attributes (_e.g._, geometry) and intrinsic (physical) motion during rendering, black box approaches are prone to violating physical laws and have limited generalization capability without any physics constraints on the transition process .

In contrast, white box methods  use traditional physical simulators (_e.g._, Material Point Method ) to approximate object dynamics explicitly with partial-differential equations (PDE), _a.k.a._, motion equations. These methods back-propagate pixel differences from a renderer to thephysical coefficients (_e.g_., Young's modulus, Poisson's ratio) of an analytical simulator. Naturally, the estimated physical coefficients can be applied to new scenes. However, the motion equations are expert-designed and may not perfectly align with the actual dynamics. This raises the key question we ask in this work: _how to accurately infer the actual intrinsic dynamics from the visual observations?_

To answer this question, we propose the Neural Material Adaptor (_NeuMA_), a learning-based model with physics-informed priors. As shown in Figure 1, the core idea of NeuMA is to formulate the learning of intrinsic dynamics specified by physical law \(\) as a residual adaptation paradigm: \(_{0}+\), where \(_{0}\) is the expert-designed physical models, and \(\) represents the correction term grounding to the observed images. This paradigm enjoys two advancements: on one hand, unlike white-box methods solely relying on \(_{0}\), NeuMA can model the actual intrinsic dynamics by optimizing \(\) to align with observations (_more accurate and flexible_); on the other hand, unlike black-box methods ignoring any physical priors, NeuMA fits the actual dynamics based on commonly-agreed physical models \(_{0}\) (_more generalizable and physically interpretable_). Specifically, built upon the progress in physics simulation, NeuMA uses the Neural Constitutive Laws (NCLaw)  to formulate \(_{0}\), which is a network that encodes existing physical priors and constraints. As for \(\), we use a low-rank adaptor  that enjoys the efficient adaptation and preservation of the prior \(_{0}\). Then, to superrise the simulation module via rendering, we propose Particle-GS, a differentiable renderer in the form of a particle-driven 3D Gaussian Splitting (3DGS)  variant. It leverages the predicted motion of particles to drive Gaussian kernels through a pre-defined relationship between particles and kernels. We use Particle-GS as the bridge from simulation to visual images, which allows marrying image gradients to optimize the simulator.

We evaluate NeuMA on various dynamic scenes with different materials and initial conditions. It shows competitive results in object dynamics grounding and dynamic scene rendering while achieving good generalization to novel shapes, multi-object interactions, and extended-time prediction.

## 2 Problem Formulation

We tackle the problem of grounding the intrinsic dynamics of an object from a sequence of visual observation \(I=\{_{1},_{2},,_{T}\}\). Following common practice [18; 53; 60; 97], in this work, we adopt the elastodynamic equation  to describe the dynamical systems:

\[}=+,\] (1)

where \(\) is the divergence of the stress tensor \(\), \(\) is the object density, and \(\) is the given body force. Besides, \(\) denotes the displacement field to describe the object's deformation, and \(}\) is its acceleration. To solve Equation (1), material models \(\) (see Appendix A for the background), which delineate how the object responds (depicted by \(\)) under deformations (depicted by \(\)), should be prescribed. To realize differentiable grounding from observed videos, we then parameterize \(\) with learnable parameters \(\) (_i.e_., \(_{}\)) [32; 53; 79]. In summary, the dynamical system governed by Equation (1) can be described by a transition model \(\): \(_{t+1}=(_{t};_{}),\) where \(_{t}\) are the particle states (_e.g_., positions and velocities) at the \(t\)-th time step.

To achieve dynamics grounding using only visual data, we develop a differentiable renderer \(\). This enables pixel supervision to be used for backpropagation to the transition model \(\), allowing for the

Figure 1: **The core idea of NeuMA**: Learning to correct existing expert knowledge on object motions by fitting a neural material adaptor to ground-truth visual observations.

optimization of the neural material model \(_{}\). Specifically, \(\) learns to synthesize an image \(}_{t}\) given the state \(_{t}\), _i.e._, \(}_{t}=(_{t};K_{t},P_{t})\), where \(K_{t},P_{t}\) denotes the camera's intrinsic and extrinsic matrix at the \(t\)-th time step. The optimal weights \(^{*}\) of \(_{}\) is obtained by minimizing the visual difference \(_{}\) between predicted images and the ground-truth observations:

\[^{*}=*{arg\,min}_{}_{}= *{arg\,min}_{}\|}_{t}-_{t}\|_{2}.\] (2)

## 3 Method

We introduce NeuMA, a neural-network-based material adaptor, to learn the intrinsic dynamics specified by the material model \(\) from only visual observations \(I\). Our key insight is to implicitly represent the material model as a learnable, residual term \(_{}\) based on widely accepted physical priors \(_{0}\) (_e.g._, neo-Hookean elasticity for elastic objects , _etc._). This gives:

\[_{}_{0}+_{},\] (3)

where we implement \(_{}\) using the low-rank adaptation (LoRA)  to restrict our correction not to overturn the physical priors. We embed the whole material model \(_{}\) into a differentiable simulator \(\), followed by a differentiable renderer \(\) to enable supervised training directly on the outputs (_i.e._, synthesized images) of \(\). In view of recent advances in particle-based physical simulation, which achieve satisfactory performance for visual dynamics grounding [26; 45], we instantiate \(\) as the well-known Material Point Method (MPM) [31; 34; 72] since it can handle complex motion simulation for various materials and easily back-propagate gradients for end-to-end training [26; 45; 85]. Please refer to Appendix E for more details about MPM. We further embrace the state-of-the-art scene representation technique, 3D Gaussian Splatting (3DGS) , to build up our differentiable renderer, Particle-GS, due to its explicit and flexible representation for efficient deformation. Please see Appendix A for details about 3DGS.

The overall pipeline for NeuMA to conduct visual dynamics grounding, as shown in Figure 2, includes three steps: initial state acquisition, physics simulation, and dynamic scene rendering.

### Initial State Acquisition

Start with a set of calibrated multi-view observations \(I_{0}=\{_{0}^{1},_{0}^{2},,_{0}^{V}\}\) depicting an object at the initial time step (_i.e._, \(t=0\)), we are interested in obtaining (1) the 3D Gaussian kernels \((i)=\{(i),(i),(i),(i)\}\) representing the object, where \((i),(i),(i),(i)\) denote the

Figure 2: **The pipeline of NeuMA for visual grounding. During Stage I, we first reconstruct the 3D Gaussian kernels of the foreground object using masked multi-view images. Then, we uniformly sample the initial physical particles from the object volume and bind them to the reconstructed Gaussian kernels. In Stage II, we integrate the neural material adaptor into the PDE-based simulation framework to estimate the actual dynamics. In Stage III, we deform the Gaussian kernels according to the binding relationship (pre-computed in Stage I) and then render 2D images. The neural material adaptor is trained end-to-end using the difference between the rendered and observed images.**

center, opacity, covariance matrix, and spherical harmonic coefficients of the \(i\)-th Gaussian kernel; and (2) the initial state \(_{0}=\{_{0},_{0},_{0}^{c}\}\) for physics simulation, where \(_{0},_{0},_{0}^{c}\) are the initial particle positions, velocities, and elastic deformation gradients, respectively.

To obtain the 3D representation, we first follow PAC-NeRF  to separate foreground objects of interest out of the set of initial image frames by running an image matting algorithm (_e.g._, [38; 39]). We then adopt the standard training pipeline of 3DGS (optionally with the anisotropy regularizer used in [17; 85] to encourage Gaussian kernels to be more evenly shaped to reduce spiky artifacts) to get a dedicated object reconstruction represented by a set of Gaussian kernels \(\{(i)\}\).

Regarding the initial particle state, a precise description of object boundaries is critical for the simulator to properly handle the interaction between objects. Thus, we explicitly reconstruct a surface mesh for the foreground object via multi-view surface reconstruction . We uniformly sample points inside the mesh volume as the initial particle positions \(_{0}\). To acquire the initial particle velocities \(_{0}\), we additionally use first few frames from the set of visual observations \(I\) and back-propagate the visual difference loss \(_{}\) through the differentiable renderer \(\) and simulator \(\) to optimize \(_{0}\) while keeping other parameters fixed. The initial elastic deformation gradients \(_{0}^{c}\) are set to the identity matrices. In addition, we compute the mass of particles as the product of a constant density \(\) and their volume, which is estimated by taking an average of the object volume over particles inside.

Particle-GS.Although prior works [17; 85; 95] demonstrated the feasibility of direct adaptation of 3DGS to particle-based physics with an identical mapping between 3D Gaussian kernels and physical particles, we argue that such a kernel-particle relationship is sub-optimal. In particular, the scene reconstruction achieved by 3DGS is primarily optimized for visual appearance, leading to an unbalanced kernel distribution--sparse in texture-less regions and dense in texture-rich areas. Thus, direct simulation based on the original Gaussian kernels may result in unrealistic outcomes. To ameliorate this issue, we propose Particle-GS, which models the hierarchical relationship between Gaussian kernels and simulation particles via a novel particle binding mechanism.

Particle Binding.Refer to Algorithm 1, we start by binding each Gaussian kernel \((i)\) with a set of nearby particles based on the Mahalanobis distance \(d_{}\)[10; 57] between the kernel and particle position \(_{0}(j)\). We pre-define a confidence threshold \(\) and use the chi-squared test \(()\) to check whether \(d_{}\)\(()\) for each kernel-particle pair. We record the relationship between Gaussian kernels and simulation particles as a (sparse) binding matrix \(\), which is used in Stage III to deform Gaussian kernels according to the physical states of multiple internal particles at subsequent time steps (Section 3.3). Our binding mechanism offers two advantages. First, it makes the visual dynamics grounding more robust to the initial visual representations (_i.e._, 3DGS reconstruction) of the object of interest. Second, since Gaussian kernels generally grow around object surfaces, each kernel carries local object-part information. The proposed binding mechanism acts as a smooth filter to force the particles belonging to the same object part (_i.e._, Gaussian kernel) to have relatively uniform physical states, thus facilitating realistic simulation.

``` Input: Gaussian centers \(\{(i)\}_{i=1}^{N_{K}}\), Gaussian covariance \(\{(i)\}_{i=1}^{N_{K}}\), particle positions \(\{_{0}(j)\}_{j=1}^{N_{P}}\), confidence threshold \(\) Output: Binding matrix \(\)
1\(=(N_{K},N_{P})\);
2for\(i 1\)to\(N_{K}\)do
3for\(j 1\)to\(N_{P}\)do
4// Difference vector
5\(_{p}=_{0}(j)-(i)\); // Mahalanobis distance
6\(d_{}=_{p}^{}(i)^{-1}_{p}\); // Check the threshold
7if\(d_{}()\)then
8\((i,j)=1\);
9
10 end if
11
12 end for // Normalize for each row
13\((i,:)=(i,:)/(((I,:)))\);
14
15 end for ```

**Algorithm 1**Particle Binding

### Physics Simulation

Given the particle state \(_{0}=\{_{0},_{0},_{0}^{c}\}\) at the initial time step, we follow the standard MPM practice to perform the time integration scheme \(\) for physics simulation from \(t=1\) to \(t=T\). To correct expert-defined material models, _i.e._, \(_{0}\), to better align with visual observations, we implement NeuMA as \(_{_{i}}^{i}:=_{0}^{i}+_{_{i}} ^{i}\), where \(i\{e,p\}\) denotes either the elastic or the plastic material model, and \(_{i}\) is the associated trainable parameters. We adopt NCLaw's  optimized material models as fixed \(_{0}\) and leverage LoRA  to fulfill the material adaptation process during physics simulation. We embed NeuMA into the differentiable MPM simulator \(\) and iteratively predict the next physical states \(_{t+1}\) based on the current state \(_{t}\). Specifically, the simulation process of NeuMA for a single time step can be decomposed into three successive stages:

1. Computing the first Piolar-Kirchhoff stress \(_{t}\) (equivalently, the Cauchy stress \(\) or the Kirchhoff stress \(\)) using the elastic material model \(_{_{e}}^{e}\): \[_{t}=_{_{e}}^{e}(_{t}^{e}),\] (4)
2. Obtaining the updated positions \(_{t+1}\), velocities \(_{t+1}\), and trial elastic deformation gradient \(_{t+1}^{e,}\) of material points via the time integration of MPM (refer to Appendix E.6 for details): \[_{t+1},_{t+1},_{t+1}^{e,}=(_ {t},_{t},_{t}^{e},_{t}),\] (5)
3. Post-processing the trial elastic deformation gradient by projecting back to the admissible elastic region using the plastic material model \(_{_{p}}^{p}\): \[_{t+1}^{e}=_{_{p}}^{p}(_{t+1}^{e,}).\] (6)

### Dynamic Scene Rendering

In this subsection, we detail the dynamic scene rendering process achieved by our Particle-GS, which completes our pipeline of dynamics grounding from pixel-level video data. We first augment the static Gaussian kernels to have time-dependent kernel centers and covariance matrices, _i.e_. we use \(_{t}(i)=\{_{t}(i),(i),_{t}(i),(i)\}\) to denote the \(i\)-th Gaussian kernel at \(t\)-th time step. Following PhysGaussian , we assume that the opacity and the spherical harmonic coefficients are invariant over time. To render the predicted frame at \((t+1)\)-th time step for supervised training, we deform the Gaussian kernels according to the binding matrix \(\) obtained at the initial time step and the particle states output by the simulator \(\). Concretely, we calculate the deformed Gaussian centers \(_{t+1}\) as:

\[_{t+1} =(_{t+1}-_{t}),\] (7) \[_{t+1} =_{t}+_{t+1}.\]

We adopt a similar update scheme for the Gaussian covariance like , but additionally consider the binding relationship between kernels and particles:

\[}_{t+1}^{e} =_{t+1}^{e},\] (8) \[_{t+1} =}_{t+1}^{e}_{0}(}_{t+1}^{e})^{}.\]

After obtaining the updated Gaussian kernels \(\{_{t+1}(i)\}\), we splat these kernels onto 2D image plane as in Equation (9) to obtain the synthesized image \(}_{t+1}\).

### Training Details

We perform supervised training upon the output of our differentiable renderer \(\) to optimize NeuMA in an end-to-end manner based on the rendering loss \(_{}\) defined in Equation (2). During the initial state acquisition stage, we actually run the particle binding two times. We obtain the indices of Gaussian kernels without attached particles in the first run and then initialize new physical particles at these kernel centers to ensure each kernel at least binds to one particle. Otherwise, some kernels will remain static across the timeline and lead to inaccurate visual grounding. We then run the final particle binding to record the binding matrix. We optimize the neural material adaptor \(_{}\) using RAdam  optimizer with a cosine learning rate scheduler for \(1,000\) iterations for each scene. We choose \(=95\%\) and \(=1,000\) for all experiments unless otherwise specified.

## 4 Experiments

### Experimental Setup

**Baselines.** Given that NeuMA is a pioneering study aimed at inferring intrinsic dynamics from camera observations alone, making a fair comparison with existing methodologies is inherently challenging. Consequently, we make a best-effort comparison with the combinations of existing related works to evaluate the proposed method from different aspects.

* **PAC-NeRF** is the most relevant work to our research, which inverts material parameters (_e.g._, Young's modulus) of a heuristic simulator, _i.e._, MPM , from multi-view videos.
* **NCLaw** relies on pre-defined particle data to train a general material model without considering adaptation to new kinds of material. We adopt its pre-trained model as the basic material model (_i.e._, \(_{0}\) in Equation (3)) and compare our method with it to show the effect of material adaptation.
* **NCLaw + Particle-GS** (NCLaw+\(\)): Beyond particle-level comparison with NCLaw, we also conduct visual comparisons by integrating pre-trained NCLaw with our renderer, Particle-GS.
* **NeuMA _w/_ Particle Supervision** (NeuMA _w/_ P.S.): To evaluate the effect of visual supervision, we also report the performance of NeuMA trained with ground-truth 3D particles.
* **NeuMA _w/o_\(_{}\): We ablate the motion correction term in NeuMA and directly train \(_{0}\), in order to verify our core idea -- the residual motion adaptation paradigm. This variant is equivalent to _finetuning NCLaw with Particle-GS_ using visual observation.
* **NeuMA _w/o_ Bind**: We ablate the particle binding procedure by directly treating the 3D Gaussian kernels as physical particles, which is commonly used in previous works [17; 85].
* **Spring-Gaus** is a concurrent work that models elastic objects with Spring-Mass 3D Gaussians. It achieves dynamics grounding by tuning learnable material parameters (_e.g._, the spring stiffness) with an analytical simulator. We compare it with our method in real-world experiments.

DatasetFor a comprehensive evaluation, we consider both synthetic and real-world data.

* **Synthetic Data.** Following PAC-NeRF  and NCLaw , we use MPM  to simulate \(6\) kinds of dynamics with different initial conditions (object shape, velocities, positions), materials, and time intervals between simulation steps. We use Blender  to render high-fidelity and realistic images. The generated \(6\) benchmarks are named "BouncyBall", "JellyDuck", "RubberPawn"1, "ClayCat", "HoneyBottle", and "SandFish". We report the simulation details in Appendix B.

* **Real-world Data.** We adopt the real-world data provided by Spring-Gaus  to assess the visual grounding performance in the real world. We choose \(4\) scenes from the released dataset, _i.e._, "Bun", "Burger", "Dog" and "Pig", for experiments. Please refer to Section 4.1 of  for details.

MetricsFollowing previous works[26; 78], we calculate the L2-Chamfer distance [15; 52] between the grounded and ground-truth particles to measure the accuracy of object dynamics grounding. Note that we scale the values by \(10^{4}\) in all experiments unless otherwise specified. We follow 3DGS  and use PSNR , SSIM , and LPIPS  as the metrics for dynamic scene rendering.

Implementation DetailsIn the initial state acquisition stage, we generally obtain 10k\(\)30k Gaussian kernels after static scene reconstruction. We cull Gaussians whose opacities fall behind \(0.02\) before training NeuMA. The number of initial particles achieved for physics simulation is about 30k. We use 1- and \(3\)-view videos on synthetic and real-world data, respectively, as ground-truth observations for dynamics grounding. In implementing NeuMA, we follow  to explicitly regularize neural networks to adhere to two physical priors, _i.e._, frame indifference and undeformed state equilibrium. Please refer to  for the implementation. In addition, we set the rank \(r\) and \(\) value of \(_{}\) to 16 by default. Our MPM simulator operates in a \(^{3}\) cube with a fixed resolution of \(32^{3}\) and \(70^{3}\) for synthetic and real-world data unless otherwise specified. All the experiments are conducted on a single NVIDIA A100 GPU.

### Performance on Object Dynamics Grounding

Here, we compare NeuMA with considered baselines on visual dynamics grounding using synthetic data. We report the Chamfer distance between the predicted and the ground-truth particle positions in Table 1. It is observed that NeuMA achieves satisfactory results on each scene and the least Chamfer

   Method & BouncyBall & JellyDuck & RubberPawn & ClayCat & SandFish & HoneyBottle & Average \\  PAC-NeRF  & 516.30 & 137.73 & 15.47 & 15.38 & 1.71 & 2.21 & 114.80 \\ NCLaw  & 56.13 & 6.32 & 3.31 & 2.45 & 2.61 & 2.26 & 12.18 \\  NeuMA & **1.19** & **3.03** & 1.27 & **1.00** & **0.65** & **0.73** & **1.31** \\ NeuMA w/ P.S. & 1.45 & 3.74 & **1.25** & **1.00** & 0.80 & 0.84 & 1.51 \\ NeuMA w/o Bind & 3.34 & 28.42 & 4.62 & 1.26 & 0.97 & 1.01 & 6.60 \\ NeuMA w/o \(_{}\) & 1.87 & 3.63 & 1.42 & 1.36 & 1.21 & 1.23 & 1.79 \\   

Table 1: **Quantitative comparison in object dynamics grounding in Chamfer distance.**distance on average, suggesting the superiority of our overall pipeline for visual dynamics grounding. From the first three rows in Table 1, compared to PAC-NeRF , which requires an accurate initial guess of the physical parameters, and NCLaw , which uses pre-defined fixed material models, NeuMA introduces a learnable residual term \(_{}\) to flexibly correct prior knowledge to align with current observations, significantly improving performance.

We also find that NeuMA even outperforms variants with particle-level supervision (_i.e._, NeuMA w/ P.S.) in some cases. This is a very critical result, proving that the model's reasoning ability can match the human common sense of estimating object dynamics purely from visual observation. We attribute this to the introduction of \(_{}\), which preserves the expert prior of \(_{0}\) while fine-tuning to the given dynamic scene. Moreover, results in the fifth row indicate that properly binding physical particles and Gaussian kernels is critical to optimize the residual term \(_{0}\). To explain, our particle binding scheme, unlike previous work's one-to-one mapping between particles and Gaussian kernels , ensures that particles within a Gaussian kernel share relatively uniform physical states, thereby reducing the degrees of freedom during optimization.

We further show the Chamfer distance at each time step in Figure 3. We observe that the Chamfer distance of NeuMA consistently remains lower than that of other competitors with the simulation ongoing. This verifies that NeuMA can accurately learn the intrinsic dynamics laws of the actual observations. Please refer to the particle visualizations of JellyDuck in Figure 12 for an intuitive comparison. In summary, these results convincingly validate the effectiveness of our main contribution-a learnable, residual neural material adaptor that can correct physical priors from visual observations.

### Performance on Dynamic Scene Rendering

In Figure 4, we present averaged visual quality metrics on synthetic data to compare view synthesis performance over the entire simulation time. Our method generally outperforms all baselines, often by a large margin. Furthermore, we present the rendering results in Figure 5. Despite using only single-view video data to ground intrinsic dynamics, our model consistently generates high-fidelity, physically plausible image sequences. In the RubberPawn scene with delicate geometries, PAC

Figure 4: **Quantitative comparison in dynamic scene rendering.**

Figure 3: **Comparison in object dynamics grounding over the entire simulation sequence.**

NeRF  trained with multi-view data produces blurry artifacts due to limited implicit scene resolution. Although NCLaw  combined with our Particle-GS yields better visual quality, it fails to capture material motion accurately. These results verify that our residual adaptation can achieve accurate dynamics grounding and that Particle-GS can handle complex geometries and deformations.

We further evaluate NeuMA on real data in Figure 6. It is seen that the rendered sequences of NeuMA are more aligned with the observations than those of Spring-Gaus  both qualitatively and quantitatively. The results confirm the effectiveness of NeuMA in real-world scenarios.

### Experimental Analysis

**Dynamics Interpolation.** Recall that \(_{}\) acts as a residual term to correct the expert-designed material model \(_{0}\) to match the actual dynamics. To study the flexibility of the residual design, we apply different compositional weights \(w=\) to \(_{}\) (by adjusting \(\)) and visualize the resulting dynamics in Figure 7. It is observed that NeuMA can smoothly interpolate between the dynamics specified by material priors and by observed images, indicating \(_{}\) indeed learns the difference between the actual dynamics and the dynamics described by the expert-designed motion equations. These visual results verify the flexibility of our proposed residual adaptation paradigm (_i.e._, \(_{}:=_{0}+_{}\)) for grounding intrinsic dynamics.

**Dynamics Generalization.** To study whether NeuMA has learned intrinsic dynamics, we directly apply the trained NeuMA to predict the dynamics given a new object with various initial conditions (_e.g._, initial velocities and locations). We use the letters of "NeurIPS" as the test target, and the results are shown in Figure 8(a). The used material model is listed at the top. It is observed that new objects also have similar motion patterns as the applied dynamics, verifying that NeuMA indeed learns the intrinsic dynamics that can generalize to novel conditions. Furthermore, we ask: can the learned NeuMA be directly applied to multi-object interaction? The visual results in Figure 8(b), particularly the challenging case of the SandFish colliding with the JellyDuck, show that two trained NeuMA instances can seamlessly interact with each other to produce physically plausible dynamics.

We also quantify the generalization performance in these two scenarios by (1) applying the learned NeuMAs to a new object, _i.e._, the letter "N", and (2) incorporating two learned NeuMAs for multi

Figure 5: **The visual results for dynamic scene rendering.**

Figure 6: **Comparisons on real-world samples. We also present quantitative results (_i.e._, PSNR / LPIPS) between predictions and observations (with background filtered) in the bottom line.**

object interactions. The results in terms of Chamfer distance are shown in Table 2. We observe that NeuMA achieves favorable results over other variants. In summary, the generalization ability of NeuMA largely stems from its refinement of an expert-designed motion model \(_{0}\), which ensures physical interpretability and enables effective application in estimating multi-object interactions.

**Dynamics Prediction.** The hallmark of a good physics model is its ability to predict future dynamics accurately . In Figure 9, we investigate NeuMA's predicted dynamics for an extended time. It is seen that NeuMA achieves effective prediction for a period that is three times longer than the given observation sequence, which validates its potential to generate long-duration dynamic videos.

**Prior (\(_{0}\)) Correction.** Figure 10 presents the gradient norms of the elastic and plastic material models during the training process. As shown in Figure 5, the reference dynamics (_i.e_., visual

Figure 8: **The visual results for dynamics generalization. The blue text indicates the applied material for each object. (a) Generalization to new objects (_i.e_. the letters of “NeurIPS”); (b) Generalization to multi-object interactions.**

Figure 10: **Gradient norm analysis. Y-axis: average gradient norms across the training stage. Higher norms indicate more significant changes in the parameters of the corresponding material model.**

Figure 7: **The visual results for dynamics interpolation. By applying different weights to the residual term \(_{}\), NeuMA can generate diverse dynamics that smoothly translate prior dynamics to visual observation.**

Figure 9: **Results for dynamics prediction. Given visual data from \(t=0\) to \(400\), NeuMA can generate physically plausible prediction for a longer period. (a) Visualization of predicted images; (b) Comparison of the mean height of the BouncyBall between ground truth and our prediction.**

observations) of the BouncyBall deviate from NCLaw (_i.e._, the prior \(_{0}\) in our experiment) in that the reference exhibits more pronounced deformation before returning to its original shape. This deviation is primarily driven by the elastic material model. Interestingly, NeuMA's learning process shows a tendency to adjust the elastic model to better align with the ground-truth observations, as illustrated in Figure 10(a). For the RubberPawn, the main discrepancy from the prior lies in the degree of plastic deformation. As shown in Figure 10(b), NeuMA opts to adjust the plastic model more significantly than the elastic model. In conclusion, this analysis highlights NeuMA's interpretability in adaptively correcting priors on specific material models to better match the visual observations.

**Uneven Mass Distribution.** In our implementation, we, by default, assume a uniform mass distribution for each object. Here, we analyze how NeuMA performs when applied to objects with uneven mass. As shown in Figure 11, we assign particles with different mass densities \(\) to obtain a ClayCat with uneven mass. We follow the similar pipeline depicted in Section 3 for visual grounding and display the qualitative results. We also quantify the Chamfer distance between the grounded and ground-truth particles, and the result is \(1.33\). From these results, it is seen that NeuMA can handle objects with uneven mass, which validates its potential for visual grounding in complex scenarios.

**Inaccurate Application of \(_{0}\).** Here, we study the effect of inaccurate application of \(_{0}\) using two plastic objects from our synthetic dataset, _i.e._, RubberPawn and ClayCat. The specific settings and quantitative results (in terms of Chamfer distance) are shown in Table 3. Setting I is our default, where the correct material models are used as the physical prior. Settings II and III introduce inaccurate elastic models, but the resulting motion still follows plastic behavior. Settings IV and V are more challenging, as they are typically used for simulating elastic and granular objects. Our method handles moderate deviations from correct models (Settings II and III), but when the prior is completely incorrect (Settings IV and V), performance drops significantly. To address this, Large Vision Language Models like GPT-4o may be used to infer plausible material models given keyframes.

## 5 Conclusion and Limitations

**Conclusion.** We introduce Neural Material Adaptor (_NeuMA_), a novel framework to infer intrinsic dynamics from visual data. By integrating data-driven corrections with established physical laws, NeuMA combines the interpretability of white box models with the adaptability of black box models. Extensive experiments on object dynamics grounding, dynamic rendering, and its generalizability verify that NeuMA enhances the accuracy and generalization of physical simulations, offering a significant advancement in AI's ability to understand and predict dynamic scenes.

**Limitations.** Despite its strengths, NeuMA has several limitations. First, it requires acquiring initial particles via a multi-view surface reconstruction technique, which, however, requires calibrated cameras and does not perform well in complex scenes. Second, cumulative errors in the forward simulation have a negative effect on future predictions, requiring the design of new physics-informed constraints in the particle space. Additionally, since NeuMA relies on visual supervision, motion blur is a key factor that can degrade the grounding performance.

   Method & Bouncy & Rubber & Sand & Ball \& Cat \\  NeuMA & **0.99** & 0.36 & 0.33 & **0.71** \\ NeuMA _w/_ P.S. & 1.16 & **0.32** & **0.30** & 0.73 \\ NeuMA _w/o_ Bind & 4.26 & 14.99 & 0.48 & 1.19 \\ NeuMA _w/o_ LoRA & 1.78 & 0.45 & 0.36 & 0.91 \\   

Table 2: **Quantitative results on generalization.**

Figure 11: **Grounding result on an object with uneven mass.**

   Setting & \(_{0}^{c}\) & \(_{0}^{p}\) &  &  \\  & & & RubberPawn & ClayCat & Rubber \(\) “N” & Clay \(\) “N” \\  I & StVK & von Mises & 1.27 & 1.00 & 0.36 & 1.23 \\ II & Neo-Hookean & von Mises & 1.94 & 0.91 & 0.36 & 1.04 \\ III & Fixed Corotated & von Mises & 1.95 & 1.60 & 0.36 & 1.47 \\ IV & Fixed Corotated & Identity & 3.64 & 3.22 & 0.70 & 1.31 \\ V & StVK & Drucker-Prager & 30.26 & 12.91 & 3.91 & 3.31 \\   

Table 3: **Quantitative results given different physical prior \(_{0}\).**