# A two-scale Complexity Measure for Deep Learning Models

Massimiliano Datres\({}^{1,2}\), Gian Paolo Leonardi\({}^{1}\), Alessio Figalli\({}^{3}\), David Sutter\({}^{4}\)

\({}^{1}\) Department of Mathematics, University of Trento, Trento

\({}^{2}\) DSH, Bruno Kessler Fondation, Trento

\({}^{3}\) Department of Mathematics, ETH, Zurich

\({}^{4}\) IBM Quantum, IBM Research Europe, Zurich

###### Abstract

We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.

## 1 Introduction

Deep learning models are achieving outstanding performances in solving several complex tasks such as image classification problems, object detection [19; 21] and natural language processing . Over-parametrized regimes make DNNs able to extract valuable information from data . Quite surprisingly DNNs typically exhibit impressive generalization capabilities after training [25; 33] without suffering of the expected overfitting. Finding appropriate complexity measures for deep learning models can help in understanding and quantifying their generalization capabilities. Hereafter we propose some essential features that, ideally, should characterize a complexity measure for parametric models:

1. it should provide pre-training information consistent with post-training performances;
2. its computation should be more efficient and scalable in comparison with a full training & validation process;
3. in the case of a feedforward-type model, it should be "modular", i.e., computable in some iterative fashion1. 
Properties (P1)-(P3) are motivated by the goal of finding an efficient tool for model selection in the context of feedforward (stochastic) neural networks.

Notions of complexity measures have appeared in the context of machine learning, with early studies focusing, e.g., on the complexity of decision tree models  and logistic regression models [10; 15; 9]. From the perspective of statistical learning theory, the Vapnik-Chervonenkis dimension, commonly called VC dimension , is an established complexity measure defined in term of the largest number of points that can be shattered by a class of functions . This complexity dimension has been usedto establish data-independent generalization bounds for statistical models [30; 26].

Other notions of model complexity, specifically designed for deep learning models, have been more recently proposed, with the aim of quantifying the expressivity of a DNN [24; 27; 22; 14]. There is, however, a supported evidence that data-independent generalization bound are not universally effective [17; 4]. For this reason, data-dependent notions of complexity have also been introduced, like the Rademacher complexity and the Gaussian complexity. These notions of complexity evaluate the expected noise-fitting-ability of a function class over all data sets drawn according to an unknown data distribution. By means of such data-dependent complexity measures, one obtains generalization bounds that are considerably better than those involving the VC dimension [4; 26; 30]. In any case, computing or estimating the VC dimension, or the Rademacher complexity, is generally a challenging task, feasible only under strong model constraints. Some tight approximations and bounds have been obtained in some specific cases, however they are not general enough to be applicable to complex models like modern deep neural networks [32; 4; 3]. With the aim to provide more easily computable notions of complexity, other definitions have been considered based on the minimum description length (MDL) and on the notion of stochastic complexity [28; 12; 9].

Other complexity measures of more geometric flavour have been defined in terms of the Fisher information matrix (FIM) associated with the statistical model [29; 5; 2]. The geometric properties of the statistical manifold revealed by the FIM collectively contribute to our understanding of the intricate nature of the model, providing a geometric lens through which complexity can be defined, analyzed and interpreted.

**Our contributions.** In Definition 4.1 we propose a notion of model complexity, called _two-scale effective dimension_ (2sED), which is motivated by a metric-specific covering argument. We prove a generalization bound derived from 2sED, see Theorem 5.1, thereby theoretically substantiating its efficacy as a measure of complexity. Furthermore, we propose a modular version of the 2sED, called _lower 2sED_, that is specifically tailored for Markovian models. We finally present numerical simulations based on Monte Carlo approximations of 2sED and lower 2sED for various models and datasets. In particular, the experiments confirm properties (P1), (P2), and (P3) for the lower 2sED, thus promoting it as a potential effective tool for model selection.

## 2 Related works

Recently, [5; 2] propose a notion of _effective dimension_, that is, a box-covering dimension related to the number of "Fisher boxes" of a given size that are needed to cover the parameter space. The size of such boxes represents a "scale" at which the model is analysed. Under suitable regularity assumptions on the statistical model and on the loss functional, the generalization error (i.e., the gap between the population error and the empirical error) can be controlled by an expression involving the effective dimension computed with respect to an explicit scale parameter, that depends on the number of samples defining the empirical error. On the one hand, the generalization bounds proved in [2; 1] require the logarithm of the FIM to be Lipschitz which excludes the case of over-parametrized models . On the other hand, the original definition requires a global computation for the statistical model as a whole, hence it does not satisfy properties (P2) and (P3). Indeed, one of the main challenges in the computation of the effective dimension is to determine the eigenvalues of the Fisher information matrix. Note that, for high-dimensional models, even the storage of the Fisher information becomes impractical, despite sophisticated approximation methods such as K-FAC . We focus to address the issues that affect the previous definition by proposing the 2sED and the lower 2sED for Markovian models.

## 3 Preliminaries

Take \(^{d_{in}}\) and \(^{d_{out}}\) nonempty Borel sets, and denote by \((X,Y)\) a pair of random vectors with (unknown) joint probability distribution \(p=p(x,y)\). Let \((X_{1},Y_{1}),,(X_{N},Y_{N})\) be i.i.d. copies of \((X,Y)\). A dataset \(:=\{(x_{i},y_{i}):\,i=1,,N\}\) is understood as a realization of the \(N\) random pairs considered before. A _statistical model_ on the sample space \(\) is a collection

\[_{}(,):=\{p_{}:\, \},\]

where \(p_{}=p_{}(x,y)\) is a joint probability distribution on \(\) for each \(\), and \(^{d}\) is a bounded domain called _parameter space_. In order to stress the functional relation between the input and the output \(y\), it is customary to assume \(p_{}=p_{}(y|x)\,p(x)\) of the form \(p_{}(x,y)=p_{}(y|x)\,p(x)\) where \(p_{}(y|x)\) is a parametric conditional probability, and \(p(x)\) denotes the marginal of the unknown distribution \(p(x,y)\) on \(\). We will also assume that the parameter space \(\) is equipped with a probability measure and we denote as \(_{}\) the expectation with respect to this measure.

Under suitable regularity conditions,we define the _Fisher information matrix_ as:

\[F():=_{(x,y) p_{}}[(_{}\,l_{ }(x,y))(_{}\,l_{}(x,y))]\,,\] (1)

where by \(a^{ 2}:=a a\) we mean \(a a^{T}\) (with the convention that \(a\) is a column vector) and \(l_{}(x,y):=\,p_{}(x,y)\). In other words, the Fisher information matrix is the expectation of the orthogonal projector onto the direction of the gradient of the log-likelihood, scaled by the squared norm of that gradient. It is a symmetric and positive semidefinite \(d d\) matrix. Its empirical version is

\[F_{N}()=_{i=1}^{N}(_{}\,l_{ }(X_{i},Y_{i})))(_{}\,l_{}(X_{ i},Y_{i}))\,\]

for some \((X_{1},Y_{1}),,(X_{N},Y_{N})}{{}}p_{}\). For each \(\) and \(u,v^{d}\), if \(F()\) is smooth and positive-definite, then \( u,v_{}:= F()u,v\) defines a Riemannian metric on the parameter space \(\), that from now on will be called _Fisher metric_ (we shall adopt the same terminology also when the metric is degenerate). In general, the Fisher metric can be considered as the pull-back of a (possibly degenerate) Riemannian metric on \(_{}(,)\).

We also define the _pointed Fisher norm_ of a vector \(v^{d}\) as \(\|v\|_{A()}:=\) and the corresponding balls of radius \(>0\) are referred as _Fisher balls_ of radius \(\) defined as:

\[B_{}(_{0}):=\{:\,\|- _{0}\|_{F(_{0})}<\}\,.\] (2)

Some further terminology must be recalled before discussing the generalization bounds. Given a _loss function_\(\), i.e., a continuous function \(:[0,+)[0,+)[0,+)\) such that \((a,b)=0\) if and only if \(a=b\), we define the _population risk_

\[R():=_{(x,y) p}[(p_{}(y|x),p(y|x) )],\]

and the _empirical risk_

\[R_{n}():=_{i=1}^{n}(p_{}(Y_{i}|X _{i}),p(Y_{i}|X_{i}))\,,\]

where \((X_{i},Y_{i})}{{}}p\), \(i=1,,n\). Then, the _generalization error_ is defined as

\[\|R-R_{n}\|_{}=_{}\,|R()-R_{n}( )|\.\] (3)

## 4 The two-scale effective dimension

Here we consider a notion of complexity for a statistical model \(_{}\), that depends on the properties of the Fisher metric on the parameter space.

**Definition 4.1**.: Given \(0<<1\) and \(0<1\), we define the _two-scale effective dimension_ (or simply 2sED) as

\[d_{}()= d+(1-)_{} [(I_{d}+^{-1}()^{} )]}{|(^{-1})|}\,,\] (4)

where

\[():=_{}[ \,F()]}F()&_{}[\,F( )]>0\\ 0&\]

is the normalized Fisher information matrix, so that whenever the statistical model is not trivial (i.e. not constant with respect to \(\)) the expectation of the trace of \(\) satisfies

\[_{}[\,()]=d\,.\]Note that \(d_{}()\) is the convex combination of the dimension \(d\) of the parameter space with a slight variant of the effective dimension studied in , which is obtained in the special case \(=0\).

_Remark 4.2_.: The 2sED, as the original effective dimension, is inspired by the box-counting or Minkowsky dimension of the statistical manifold \(_{}\). Given a metric space \((X,d)\) and a subset \(S X\), the box-counting dimension of \(S\) is defined as

\[_{box}(S)=_{ 0}_{d}()}{| \,|}\,,\]

where \(_{d}()\) is the minimum number of \(\)-size balls (with respect to the metric \(d\)) needed to cover \(S\), also known as the \(\)-covering number of \(S\). The box-counting dimension quantifies how fast \(_{d}()\) changes as the radius \(\) approaches zero. Motivated by this notion of fractal dimension, we can define the effective dimension of a statistical model \(_{}\) at the scale \(\) as

\[_{,}(_{})=_ {}()}{|\,|}\,,\] (5)

where \(_{}()\) is the number of Fisher balls of size \(\) (defined in (2)) needed to cover \(\). The 2sED defined in (4) is motivated by an upper bound estimate of (5) which is computable for a given statistical model under reasonable regularity assumptions.

_Remark 4.3_.: Two parameters show up in the above definition: a micro-scale \(>0\) and an exponent \([0,1)\) defining a meso-scale \(=^{}\). The emergence of two scales is tied to the estimation argument for \(_{}()\), which requires weaker assumptions compared to those in . The micro-scale is related to the size \(\) of Fisher balls that are used to cover a component of the parameter space, while the meso-scale \(^{}\) represents the size of the components of a partition of the parameter space, that needs to be fixed in order to localise and adapt the micro-scale covering. This covering argument plays a fundamental role in the proof of the generalization bound (Theorem 5.1). A more formal and detailed discussion of the covering argument is reported in the proof of Lemma B.2 in Appendix B. Note that when \(=0\) we essentially obtain the effective dimension of , up to a slight technical difference due to the presence of the square root of the normalized FIM \(\). More generally, the 2sED is a convex combination of the dimension of the parameter space and the effective dimension.

_Remark 4.4_.: The effective dimension \(d_{}()\) can be shown to converge to \( d+(1-)\) as \( 0\), where \(:=_{}\,(())\), see Proposition A.1. The proof follows the strategy of Remark 1 of , and is presented in Appendix A for completeness.

## 5 Generalization bounds

It is known that the Fisher information of a statistical model degenerates asymptotically with the number of parameters . This suggests that, in the case of a large (over-parametrized) model, like a deep neural network with high-dimensional layers, the corresponding Fisher information matrix \(F()\) should have a lot of small (or possibly zero) eigenvalues. For this reason, in Theorem 5.1 below we will not require the Lipschitz regularity of \((F())\), as done in Theorem 1 of , because this assumption would imply uniform positive lower bounds on the eigenvalue of \(F()\). Without loss of generality, we directly assume \(F=\) and \(=^{d}\), as this can be enforced by a suitable scaling of the model.

We list below a set of hypotheses, that will be required in the generalization bounds:

* the map \( p_{}(y|x)\) is of class \(C^{1,1}\) uniformly in \((x,y)\);
* there exist two constants \(0<_{1}_{2}\) such that \[_{1} p(x,y),\;p_{}(x,y)_{2}\] for all \(x,y,\);
* the Fisher matrix field \(F()\) is \(L\)-Lipschitz with respect to the Frobenius norm;
* the loss function \(\) is bounded by \(2b\) and is \(\)-Lipschitz, for some \(b,>0\);
* the meso-scale parameter \(\) satisfies \([,1)\).

Some comments about the previous properties are in order. First, property (i) is a mild regularity assumption on the model. Property (ii) prevents degeneration of both probability densities \(p(x,y)\) and \(p_{}(x,y)\). The \(L\)-Lipschitz property (iii) is crucial to compare the pointed Fisher norm computed in different \(\) and it is satisfied for instance by models of class \(C^{1,1}\) (but possibly also by more general models). Lipschitz regularity and boundedness of the loss function \(\) (property (iv)) are standard assumptions (see, e.g., ). Finally, property (v) is structurally needed in the proof of the generalization bound (Theorem 5.1) and it is strictly related to the covering argument discussed Lemma B.2 in Appendix B.

**Theorem 5.1**.: _Let us assume (i)-(v). Then, there exist explicit constants2\(C,H,K,n_{0}>0\) such that for any \((0,1]\), \(n n_{0}\), and \(_{n}=()^{3/8}\), we obtain_

\[\!(_{}|R()-R_{n}() | C_{n}) H_{n}^{-d_{}(_{n}) }n^{-}\,.\] (6)

The proof of Theorem 5.1 is given in Appendix B.

_Remark 5.2_.: Under the assumption that the eigenvalues of \(F\) are smaller than \(\) for some fixed \(>0\), the above result implies the existence of \(_{0}>0\) such that, for \(0<<_{0}\), the right-hand side of (6) vanishes as \(n\). The upper bound \(_{0}\) is explicit and depends only on the dimension \(d\) and on the properties of the model, see (34). By choosing \(\) as above, the right-hand side of (6) gives an explicit upper bound of the generalization error, that is non-vacuous also for finite \(n\) (even though this can be granted only in the under-parametrized regime, i.e. for \(n\) large enough).

## 6 The effective dimension of a Markovian model

Markovian models are a family of probabilistic models characterized by a sequential, feed-forward-type structure, see the Markovian property stated below.

Let us consider an integer \(L 2\), a probability space \((,,)\), and a random vector \(X_{j}:_{j}\) for \(j=0,,L\). Given a parameter space \(=_{1}_{L}\), a parametric statistical model \(_{}(_{0},,_{L})\) satisfies the Markovian property if and only if for each \(p_{}(x_{0},,x_{L})_{}(_{0}, ,_{L})\) and for each \(=(_{1},,_{L})=_{1} _{L}\) we have:

\[p_{}(x_{0},,x_{L})=p(x_{0})p_{_{1}}(x_{1}|x_{0})  p_{_{L}}(x_{L}|x_{L-1})\] (7)

where \(_{1},,_{L}\) are the parameters associated to the model's distribution of \(X_{1},,X_{L}\) respectively. Many well-known and commonly used neural network architectures, such as feed-forward neural networks, can be interpreted as Markovian models with concentrated, Dirac-type probability distributions. A specific evaluation of the effective dimension of these models seems therefore particularly interesting. Exploiting the Markovian property, for \(j=1,,L\) we define:

\[F(_{j}|x_{j-1}):=_{_{j}}(_{_{j}}l_{ _{j}}(x_{j}|x_{j-1}))^{ 2}p_{_{j}}(dx_{j}|x_{j-1})\]

where \(l_{_{j}}(x_{j}|x_{j-1}):=\,p_{_{j}}(x_{j}|x_{j-1})\) and

\[F_{j}=F_{j}(_{1},,_{j}):=_{x_{0}} _{x_{1}|x_{0}}_{x_{j-1}|x_{j-2}}[F(_{j}|x_{j-1})]\,,\]

where \(_{x_{0}}\) and \(_{x_{j}|x_{j-1}}\) denote the (conditional) expectations with respect to \(p(x_{0})\) and \(p_{_{j}}(x_{j}|x_{j-1})\), respectively. Clearly \(F_{j}\) is a symmetric and positive semidefinite \(d_{j} d_{j}\) matrix (where \(d_{j}\) is the dimension of \(_{j}\)) and represents the \(j\)-th block of the Fisher information matrix

\[F()=F_{1}(_{1})&0&&0\\ 0&F_{2}(_{1},_{2})&&\\ &&&&\\ 0&&&F_{L}(_{1},,_{L})\,.\] (8)We recall that the two-scale effective dimension (2sED) is

\[d_{}()= d+ _{_{1}}_{_{L}}_{j=1}^{L}det_{j}\,d_{1}  d_{m}\,,\] (9)

where \(det_{j}=det_{j}(_{1},,_{j}):=(I_{j}+ ^{-1}F_{j}^{})\) and \(I_{j}\) denotes the \(d_{j} d_{j}\) identity matrix. Since \(F_{j}\) depends on all the parameters of the previous blocks, it is not possible to directly factorize the multiple integral in (9). Nevertheless, one obtains a more easily computable lower bound of \(d_{}()\), called _lower 2sED_, by a single application of Jensen's inequality as hereafter described. Let \(d_{}^{m}()\) be the 2sED associated with the composition of the first \(m\) layers, \(m 2\). Then:

\[d_{}^{m}()-d_{}^{m-1}() =(_{_{ m}}_{_{m}}det_{m}\,d_{m}\,d_{m})\] \[_{_{m}} _{_{m}}\,det_{m}\,d_{m}d_{m}\,,\]

where we have set \(_{m}:=_{1}_{m-1}\) and

\[d_{m}=_{m}(d_{1},,d_{m-1}):=^{m-1}|_{j}|}_{j=1}^{m-1}det_{j}\,d_{1} d _{m-1}\,.\]

Now, a lower bound of \(d_{}^{m}()\) can be iteratively defined for \(m=1,,L\) as follows:

\[_{}^{1}()& = d+_{_{1}} (I_{1}+^{-1}F_{1}(_{1})^{})\,d_{1} = d+_{_{1}}det_{1}\,d _{1}\\ &\\ _{}^{m}()&=_ {}^{m-1}()+_{_{m}}_{_{m}} det_{m}\,d_{m}\,d_{m}\,.\] (10)

From now on we set \(_{}=_{}^{L}\) and call it the _lower effective dimension_ of the Markovian model \(_{}\).

## 7 Experiments

In this section, we present experimental evidence that the behavior of the loss in the training of given parametric models is related both with 2sED (4) and the lower 2sED (10). We compute \(_{}\) and \(d_{}\) of different feed-forward neural networks (FNN) such as convolutional neural networks (CNN) and multi-layer perceptrons (MLP). The experiments rely on the computation of the exact eigenvalues via the _numpy.linalg.eig_ function. As discussed in Section 8, an interesting research direction (which goes beyond the goal of this paper) would be to investigate strategies to efficiently compute the (lower) 2sED via a suitable approximation of the spectrum of \(F\). The feed-forward neural network choice is justified by their architecture characterized by a Markovian dependency structure. Indeed, the flow of information in FNN is unidirectional from input to output, making them representable with a finite acyclic graph,. We evaluate \(_{}\) and \(d_{}\) on real-world datasets, including Covertype dataset , MNIST dataset , and CIFAR10 . All simulations are conducted on a 12th Gen Intel(R) Core(TM) i9-12900KF equipped by a NVIDIA GeForce RTX 4090. In the worst-case scenario, the experiments required a maximum RAM memory consumption of 17 GB.

**Description of the models.** To simplify notation and enhance readability, we denote with "MLP \(N_{0}\)-\(N_{1}\)". \(\)-\(N_{n}\)" a MLP with \(n\) linear layers, each with a width of \(N_{i}\) for \(i=0,,N\), followed by ReLU activation functions on all layers except the final layer \(n\). If we denote with \(W^{i} R^{N_{i} N_{i-1}}\) the parameters of the \(i\)-th layer, we can describe "MLP \(N_{0}\)-\(N_{1}\)-\(\)-\(N_{n}\)" through \(n\) blocks of operations defined as \(O_{i}()=ReLU(W^{i})\) for \(i=1,,n\). Similarly, "CNN \(N_{0}\)-\(N_{1}\)-\(\)-\(N_{n_{1}}|L_{1}\)-\(\)-\(L_{n_{2}}\)" refers to a convolutional neural network with \(n_{1}\) convolutional blocks each one of kernel size \(N_{i}\) for \(i=1,,N_{1}\) followed by a flattening layer and \(n_{2}\) MLP blocks of width \(L_{i}\) for \(i=1,,n_{2}\). Within each convolutional block, the operations of convolution, batch normalization, ReLU activation, and max pooling are performed sequentially. Moreover, the flattening operation is executed by applying a common convolutional kernel to all the channels of the last convolutional layer. Hence, given the input \(A^{N_{c} k k}\) the flattening operation \(Flat_{K}:^{N_{c} k k}^{N_{c}}\) is defined as follows:

\[[Flat_{K}(A)]_{l}:=A_{l::} K=_{i=1}^{k}_{j=1}^{k}A_{l,i,j}K_{i,j}\,,\]

for \(l=1,,N_{c}\) where \(A_{l::}\) denotes the \(^{k k}\) obtained by fixing the first dimension at index \(l\) and \(K\) is a \(k k\) convolutional kernel which is a parametric matrix in applications. This approach effectively reduce the number of parameters allowing us to compute the effective dimension in reasonable time.

**Introducing stochasticity.** In applications, the core architectures of many deep learning models is deterministic, and the stochasticity is usually introduced in the training pipeline rather than in the model itself. This makes deep learning models, like MLPs and CNNs, incompatible with our setting. Therefore, we approximate deterministic feed-forward neural networks with stochastic variants, where the output of each block is Gaussian with mean the current block deterministic output and a small fixed variance \(^{2}\). In other words, if \(N\) is the number of blocks, the output of the \(i\)-th block \(O_{i}^{}\) is given by

\[O_{i}^{}=O_{i}+(O_{i},^{2}I)\,,\]

where \(O_{i}\) is the deterministic output of the \(i\)-th block, \((0,^{2})\). For all the subsequent experiments, we will specifically focus on the computation of 2sED and lower-2sED for \(=0\) and consider the empirical Fisher information matrix \(_{N}\). Note we compare different network topologies while keeping more or less the same number of parameters. Hence, the informative part of the definition of the (lower) 2sED automatically becomes the log ratio.

**Sharpness of lower 2sED.** To empirically validate the lower 2sED, we compute \(_{0}\) and \(d_{0}\) for different stochastic perturbations of feed-forward neural networks, also varying the covering radius \(\). We keep constant both the 100 samples used to estimate \(_{N}\) and the 100 vectors of parameters employed for estimating the integrals appearing in (4) and (10). The results are visualized in Figure 0(a) and Figure 0(b).

Notably, the lower bound is sharp in both the MLP and the CNN case suggesting the conclusions regarding model complexity obtained using \(_{}()\) are equivalent to those obtained when considering \(d_{}()\) for all covering radius \(\). It is also worth noticing that lower 2sED exhibits a sequential form, reducing the computational demands when investigating how the model's complexity changes by modifying only its final components.

**Dependence on \(^{2}\).** We study now the impact of variance \(^{2}\) on 2sED and lower 2sED. We vary the values of \(^{2}\) while computing the \(_{}\) and \(d_{}\) for different models on Covertype and MNIST dataset. Figure 4 and Figure 5 show that the impact of \(^{2}\) on \(_{}\) and \(d_{}\) is negligible.

**Stability of Monte Carlo estimates.** Monte Carlo integration is crucial in the estimation of both the Fisher information matrix and the integral within \(\) appearing in (4). To ensure the reliability of our

Figure 1: (a) Difference between 2sED and lower 2sED can not be appreciated, which means that the second is a tight lower bound of the first. Here, the lower 2sED and 2sED of \(MLP\)\(54\)-\(16\)-\(7\) using 100 Covertype samples and 100 vectors of parameters for the Monte Carlo estimation of \(F_{N}\) is shown; (b)Lower 2sED and 2sED of \(CNN\)\(7\)-\(5|10\)-\(50\)-\(34\)-\(10\) using 100 MNIST samples and 100 vectors of parameters for the Monte Carlo estimation of \(F_{N}\) are extremely close.

results, we conduct a robustness analysis of the lower 2sED with respect to variations in the number of samples and parameterizations employed for integrals estimation. Figures 1(a), 1(b), 1(c), 1(d) confirm the stability of the lower 2sED plots with respect to the number of points used in the Monte Carlo approximation. The 2sED is computed for three different models on Covertype, MNIST and Cifar10 dataset.

**Lower 2sED and training curves.** Finally, we test the relationship between the lower 2sED and the loss minimization. We expect that models with higher values of the lower 2sED can achieve higher accuracy after training. Furthermore, it is crucial to gain a deeper understanding of the role played by the covering radius, denoted as \(\), in the 2sED definition. We compute the lower 2sED for three different models with similar dimension on CIFAR10 and Covertype dataset. The dimension of these models is reported in Table 1. MLP 54-10-2-10-25-7 is characterized by a bottleneck structure in the middle of its architecture. A loss of information due to this bottleneck is therefore expected as data are mapped into a significantly lower dimensional space. Consequently, the expressiveness of this model is expected to be lower compared to the other two models, even though it is bigger than MLP 54-16-7 in terms of number of parameters. In Figure 2(a), this expected behaviour is effectively captured by the lower 2sE, as indicated by the lower position of the red curve in comparison to the other two curves.

Furthermore, the position of the curves change varying the covering radius \(\). The MLP 54-16-7 model exhibits greater expressiveness within this range of \(\). Conversely, for smaller values of \(\), MLP 54-13-11-9-7 appears to be more expressive. This behaviour is empirically validated by the experiments. In Figure 2(c) and Figure 2(e) we observe the training loss curve for the three models when trained with only \(10000\) and \(100000\) data respectively. Note that MLP 54-16-7 is the one achieving the lower training loss using 10000 data, while MLP 54-13-11-9-7 is consistently better

Figure 2: (a) Stability of the lower 2sED of CNNs estimated using 100 Covertype samples and 100 vectors of parameters for the Monte Carlo approximation with the corresponding error margin; (b) Stability of the lower 2sED of MLPs estimated using 1000 Covertype samples and 1000 vectors of parameters for the Monte Carlo approximation with the corresponding error margin; (c) Stability of the lower 2sED of CNNs estimated using 100 Covertype samples and 100 vectors of parameters for the Monte Carlo approximation with the corresponding error margin; (d) Stability of the lower 2sED of CNNs estimated using 500 Covertype samples and 100 vectors of parameters for the Monte Carlo approximation with the corresponding error margin.

   Model & Number of Parameters \\  MLP 54-16-7 & 976 \\ MLP 54-13-11-9-7 & 1007 \\ MLP 54-10-2-10-25-7 & 1005 \\ CNN 7-5\(|\)10-50-34-10 & 4493 \\ CNN 3-5-3-6\(|\)10-50-34-10 & 10034 \\ CNN 3-6-5-3\(|\)10-50-34-10 & 10041 \\   

Table 1: Number of model’s parameters

Figure 3: (a) Estimated lower 2sED of three different MLP architectures using 100 Covertype samples and 100 different vectors of parameters for the Monte Carlo estimation of \(F_{N}\); (b) Estimated lower 2sED of three different CNN architectures using 100 CIFAR10 samples and 100 different vectors of parameters for the Monte Carlo estimation of \(F_{N}\); (c) Training loss plots of MLPs on 10000 random Covertype samples using Adam with learning rate \(1e^{-3}\) and a batch size \(64\); (d) Training loss plots of CNNs on CIFAR10 using Adam optimizer with learning rate \(1e^{-3}\) and a batch size \(512\);(e) Training loss plot of MLPs on 100000 Covertype samples using Adam optimizer with learning rate \(1e^{-3}\) and a batch size \(64\); (f) Training loss plots of CNNs on augmented CIFAR10 (double the original size) using Adam optimizer with learning rate \(1e^{-3}\) and a batch size \(512\).

with 100000 data. The empirical correlation between training losses and lower 2sED underscores the capacity of 2sED as a reliable measure for describing the training capabilities of neural networks. We conducted additional experiments, manipulating the number of training data points. The outcomes align consistently with the previously described results. This further confirms its effectiveness as a capacity metric.

**Other experiments.** Other experiments in this direction are performed on the CIFAR10 and MNIST dataset and the results are reported in Figures 2(b), 2(d), 2(f), 2(g), 2(h), 2(h), 2(h) varying batch sizes providing additional empirical evidence that supports our findings.

## 8 Conclusions

We propose a novel complexity measure called 2sED strongly related with the geometric properties of the statistical manifold. This notion allows to bound the generalization error under mild assumptions on the model (see Theorem 5.1) to theoretically justify the 2sED as a complexity measure. At the same time, a modular version of the 2sED, called lower 2sED, specifically tailored for Markovian models, is introduced as a tight lower bound for the 2sED. The lower 2sED can be computed sequentially layer-by-layer, which drastically reduces the computational effort and the storage consumption compared to the original 2sED. Consequently, the lower 2sED can be computed for more complex (deeper) models than those considered in previous works.

Finally, numerical simulations based on Monte Carlo approximation of the 2sED and the lower 2sED for various models and datasets are presented. The experiments remarkably confirm desirable properties (P1), (P2), and (P3) for the lower 2sED. These experiments show that the lower 2sED represents a tight approximation of the 2sED. The resulting relation between the scale parameter, the number of training data and the training error suggests that the lower 2sED is a reliable complexity measure that can be used as a tool for training-free model selection. Indeed, it can be accurately estimated for general models, distinguishing it from other complexity measures that are often challenging to compute directly and may only be estimated, often assuming a precise model structure.

**Limitations and Future perspectives.** We study a new notion of complexity for deep learning models and we show empirical evidence that the lower 2sED effectively captures the training behaviour. However, the computation of the lower 2sED for large-scale machine learning models is complicated by the dimension of the FIM, whose eigenvalue problem is computationally intractable. In light of the theoretical nature of this work, the implementation of the 2sED has not been optimized. Exploring code optimization would be an interesting direction for future research and necessary to study the lower 2sED of bigger models.

Therefore, an interesting research direction is to develop techniques for further reducing the computational cost of the lower 2sED. The main goal is to approximate the eigenvalue distribution of the FIM, rather than computing exactly each eigenvalue. This would consistently improve the effectiveness of the lower 2sED as a model selection criterion, e.g., in the step-by-step design of deep feedforward neural networks. Another avenue for future research is to design variants of 2sED and lower 2sED specifically adapted to very large neural networks, which characterize modern deep learning architectures. This not only would bring out a deeper understanding of the complexity and the generalization capabilities of huge machine learning models, but it could also provide more efficient approximations of the (lower) 2sED for them.