# Fast Rates for Bandit PAC Multiclass Classification

Liad Erez

Tel-Aviv University

liaderez@mail.tau.ac.il

&Alon Cohen

Tel-Aviv University

Google Research

alonco@tauex.tau.ac.il

&Tomer Koren

Tel-Aviv University

Google Research

tkoren@tauex.tau.ac.il

&Yishay Mansour

Tel-Aviv University

Google Research

mansour.yishay@gmail.com

&Shay Moran

Technion

Google Research

shaymoran1@gmail.com

###### Abstract

We study multiclass PAC learning with bandit feedback, where inputs are classified into one of \(K\) possible labels and feedback is limited to whether or not the predicted labels are correct. Our main contribution is in designing a novel learning algorithm for the agnostic \((,)\)-PAC version of the problem, with sample complexity of \(O((K)+1/^{2})(||/) \) for any finite hypothesis class \(\). In terms of the leading dependence on \(\), this improves upon existing bounds for the problem, that are of the form \(O(K/^{2})\). We also provide an extension of this result to general classes and establish similar sample complexity bounds in which \(||\) is replaced by the Natarajan dimension. This matches the optimal rate in the full-information version of the problem and resolves an open question studied by Daniely, Sabato, Ben-David, and Shalev-Shwartz (2011) who demonstrated that the multiplicative price of bandit feedback in realizable PAC learning is \((K)\). We complement this by revealing a stark contrast with the agnostic case, where the price of bandit feedback is only \(O(1)\) as \( 0\). Our algorithm utilizes a stochastic optimization technique to minimize a log-barrier potential based on Frank-Wolfe updates for computing a low-variance exploration distribution over the hypotheses, and is made computationally efficient provided access to an ERM oracle over \(\).

## 1 Introduction

Multiclass classification is a fundamental learning problem in which a learner is tasked with classifying objects into one of \(K\) possible labels. In _bandit multiclass classification_, upon making a prediction, the learner does not observe the true label, but only whether or not the prediction was correct. As a concrete example, consider a the task of classifying images, say, from the ImageNet dataset, with the number of labels \(K\) being several thousands. After the learner predicts a label for a particular image, both the image and the prediction are shown to a human rater who is asked if the prediction is correct or not, after which the answer is revealed to the learner. Thus, the learner faces a bandit multiclass classification instance as the true label is not revealed in case the prediction was deemed incorrect by the rater.

Much of previous work on the foundations of bandit multiclass classification focused on the _online_ setting , where the goal of the learner is to minimize the _regret_ compared to a given _hypothesis class_\(\), namely, the learner's total number of correct predictions throughout the learning process compared to that of the best fixed hypothesis from \(\). A central line of work focused on studying the properties of \(\) that allow for sublinear regret in this context, and on characterizing the achievable regret rates in terms of \(K\), \(||\) and the number of prediction rounds \(T\). For instance, Auer et al.  show that for any finite \(\) one can obtain a regret bound of \(O(|})\), by casting the classification problem as a (contextual) \(K\)-armed bandit problem. Daniely and Helbert  demonstrate that the dependence on \(||\) can be replaced by the Littlestone dimension of \(\), which may potentially be smaller than \(||\) and, in particular, may be finite even when \(\) is infinite. Very recently, Erez et al.  establish a characterization of the optimal regret rates for finite hypothesis classes and show that it is of the form \((\{|}\},||+)\), which is tight even when the labeled examples are drawn i.i.d. from a fixed distribution.

Here we focus on a different, yet closely related version of multiclass classification, viewed as a learning problem in a PAC framework . In this setting, the labeled examples are drawn from a fixed unknown distribution, and the learner's goal is to ultimately output a prediction rule which performs well, over this distribution, relative to the best hypothesis from the underlying class \(\). This problem has mainly been studied in the analogous full-information setting, that is, when the learner as access to a training set of i.i.d. examples along with their true labels, where the number of samples required to learn an \(\)-optimal hypothesis was shown to be \(O((1/^{2})(||/))\) for finite classes [22; 5; 10; 6]. In the bandit case, however--namely where the learner may repeatedly predict labels of drawn examples and obtain feedback only on whether the prediction was correct or not--a comprehensive understanding of the achievable sample complexity rates is still missing.

On the surface, the PAC bandit multiclass problem might be deemed trivial: a straightforward approach of uniformly approximating the losses of all hypotheses in \(\) by drawing i.i.d. examples and predicting labels uniformly at random, already gives rise to \(O((K/^{2})||)\) sample complexity which appears to be optimal due to the bandit feedback. Furthermore, the simple underlying algorithm can be implemented efficiently, provided that empirical risk minimization (ERM) can be carried out efficiently over the hypothesis class. However, this view regards multiclass classification as a generic \(K\)-armed (contextual) bandit problem and neglects a crucial aspect of the setting: each example has a single correct label, as opposed to a more general scenario where each label may be associated with its own loss, independently of other labels. And indeed, no non-trivial lower bounds can be found in the existing literature for this problem.1

The following questions thus remain:

1. _What is the optimal sample complexity of the bandit multiclass setting? In particular, can one improve upon the prototypical \(K/^{2}\) rate, representative of bandit problems?_
2. _Can this sample complexity be attained by an efficient (polynomial time) algorithm, whenever ERM can be computed efficiently over the underlying hypothesis class?_

### Summary of contributions

In this work, we address the questions above and establish a nearly-tight characterization of the achievable sample complexity in the bandit multiclass problem, along with an efficient algorithm. Our main contributions are summarized as follows.

1. For a finite hypothesis class \(\), we give an algorithm with sample complexity of \(O(((K)+1/^{2})(|H|/))\) for producing an \(\)-optimal classifier with probability at least \(1-\); see Theorem 1 in Section 3. Further, our algorithm is a proper learner and can be implemented efficiently provided a (weighted) ERM oracle for the class \(\). In terms of the leading dependence on \(\), this bound significantly improves over the previously mentioned \(O(K/^{2})\) bound, and matches the optimal rate in the full-information version of the problem.
2. For more general, possibly infinite hypothesis classes \(\) with finite Natarajan dimension \(d\), we establish a generalized sample complexity bound of \(O(((K)+1/^{2})d(1/))\); see Theorem 2 in Section 4. This improves over the previous \((Kd/^{2})\) bound due to Daniely, Sabato, Ben-David, and Shalev-Shwartz , and matches the known rate in the full-information case  up to logarithmic factors for sufficiently small \(\).

These results bear some interesting consequences. First, and perhaps most surprisingly, they imply that there is _no additional price for bandit feedback_ in PAC multiclass classification, as \( 0\)namely, that the ratio between the optimal sample complexity rates in the bandit and the full-information settings is \((1)\), and not \((K)\) as one might expect. Indeed, the latter is often the multiplicative price of bandit information in a multitude of scenarios (see, e.g., , and further, it is the tight price in the realizable case of bandit multiclass . This phenomenon occurs already in the case of a finite hypothesis class, and extends naturally to general Natarajan classes.

Second, the results reveal an unexpected gap between the attainable bounds in the online (i.e., regret minimization) and the PAC settings of the bandit multiclass problem. A recognized trademark of online learning is that online-to-batch conversions of regret bounds very often give sharp sample complexity rates in the i.i.d. PAC setting (see e.g., [7; 17; 8]). In bandit multiclass classification, however, we exhibit a stark separation between the two settings, where for sufficiently large hypothesis classes an online-to-batch conversion of the optimal regret rate results with sample complexity \((K/^{2})\), whereas the rate we establish here is \((1/^{2}+(K))\).2

The core novelty in our algorithmic approach is a stochastic optimization technique for efficiently recovering a low-variance exploration distribution over the hypotheses in \(\), with variance \(O(1)\) rather than the \(O(K)\) obtained by simple uniform exploration of labels. We show that through minimization of a stochastic objective akin to a log-barrier (convex) potential over the induced label probabilities, such a distribution can be computed in a sample-efficient way and in turn be used to uniformly estimate the losses of all hypotheses in \(\) via importance sampling. Moreover, we demonstrate how this stochastic optimization problem can be solved efficiently using a stochastic Frank-Wolfe method. More details about the algorithmic ideas and an overview of the analysis are given in Section 3.

### Additional related work

Bandit multiclass classification.In agnostic online multiclass classification with bandit feedback, regret bounds of the form \(O(|})\) can be obtained by viewing the problem as an instance of contextual multi-armed bandits ; this bound has been recently improved by  to \((\{|},||+\})\) for the classification setting.  show how to replace the \(||\) dependence in the first bound by the Littlestone dimension of \(\), and  show how \(K\) can be replaced with a refined quantity which encapsulates the effective number of labels. Additional refinements in the realizable setting include the _Bandit Littlestone dimension_ which provides a characterization of the optimal mistake bound for deterministic learning algorithms .

Contextual bandits.The PAC objective of the bandit multiclass classification problem can be seen as a special case of the problem of identifying an approximately optimal policy in contextual multi-armed bandits. In the more general contextual bandit framework, sample complexity lower bounds of \((K/^{2})\) are known , with regret upper bounds of the form \(()\) obtained in several previous works [4; 12; 1]. Reducing our problem to contextual bandits, however, ignores the special structure exhibited by the reward function in the classification setting, namely their sparsity (see  for a more detailed comparison), and indeed we establish improved sample complexity rates in this special case. In a bit more detail, in the works of [12; 1], one of the main technical ideas is to compute a distribution over policies which induces a reward estimator whose variance is bounded, uniformly over the policies, by \(O(K)\) (it is actually nontrivial to show that such a distribution even exists). Our approach involves similar ideas in the sense that we also aim to compute such a low-variance exploration distribution over the hypotheses, but the crucial difference is that in the classification setting, the sparse nature of the rewards allows us to uniformly bound the variance by \(O(1)\) rather than \(O(K)\). Li et al.  also consider the PAC objective for contextual bandits and establish a gap dependent sample complexity bound which never exceeds \(K/^{2}\), but to our understanding does not imply an improved bound for sparse rewards (or single-label classification), as even for the multi-armed bandit variant, in order to obtain improved sample complexity bounds which are instance dependent, one must leverage the variances of the arm rewards in addition to the gaps.

Problem Setup

Bandit multiclass classification.We consider a learning setting in which a learner is tasked with classifying objects from a set of examples \(\) with a single label from a set of \(K\) possible labels \(=\{1,,K\}\). A stochastic multiclass classification instance is specified by a hypothesis class \(\{\}\) and a joint distribution \(\) over example-label pairs over \(\). We focus on finite hypothesis classes and denote \(N||\). In the bandit setup, the learner interacts with the environment in an iterative manner according to the following protocol, over \(i=1,2,\):

1. The environment generates a pair \((x_{i},y_{i})\) and the example \(x_{i}\) is revealed to the learner;
2. The learner predicts a label \(_{i}\);
3. The learner observes whether or not the classification of \(x_{i}\) is correct, namely \(\{_{i}=y_{i}\}\).3

Agnostic PAC model.In the PAC version of the problem, given parameters \(,>0\) the goal of the learner is to produce a hypothesis \(:\),4 such that with probability at least \(1-\) (over the randomness of the environment as well as any internal randomization of the algorithm):

\[L_{}()-L_{}(h^{}),\]

where here \(L_{}(h)[h(x) y]\) is the expected zero-one loss of \(h\) with respect to \((x,y)\), and \(h^{}*{argmin}_{h}L_{}(h)\) is the best hypothesis in \(\). That is, the learner needs to identify a hypothesis which is \(\)-optimal in \(\) with respect to the expected zero-one loss, with probability at least \(1-\). In this model, the learner's performance is measured in terms of _sample complexity_, which is the number of interaction rounds with the environment as a function of \(,\) required for satisfying the guarantee stated above.

Weighted ERM oracle.For our computational results, we will assume a weighted empirical risk minimization (ERM) oracle access to the hypothesis class \(\). In more detail, we assume access to an oracle, denoted \(_{}\), defined as follows: given a sequence of examples, labels and weights \((x_{1},y_{1},_{1}),,(x_{t},y_{t},_{t}) \) as input, the oracle \(_{}\) returns

\[*{argmin}_{h}_{s=1}^{t}_{s} \{h(x_{s}) y_{s}\}.\] (1)

We remark that this is a version of the argmin oracle often considered in the more general contextual bandit setting (e.g., 12, 1), specialized for the classification setting we focus on here. For our runtime results, we will assume that each call to \(_{}\) takes \(O(1)\) time.

Additional notation.We denote by \(_{N}\{P_{+}^{N}_{i=1}^{N}P_{i}=1\}\) the \(N\)-dimensional _simplex_ which corresponds to the set of all probability distributions over \(\). Given \(P_{N}\) and \(h\) we use the notation \(P(h)\) to denote the probability assigned to \(h\) by the probability vector \(P\). Given an example-label pair \((x,y)\) we define the binary vector \(r_{x,y}\{0,1\}^{N}\) by

\[r_{x,y}(h)\{h(x)=y\} h,\]

that is, \(r_{x,y}(h)\) is the zero-one reward of the hypothesis \(h\) on the pair \((x,y)\). Given \(P_{N}\) and \((x,y)\) we denote the probability of choosing the label \(y\) when sampling a hypothesis from \(P\) on \(x\) by

\[W_{x,y}(P)_{h=}P(h)\{h(x)=y\}=P  r_{x,y},\]

and for \((0,1)\) we let \(W_{x,y}^{}(P)(1-)W_{x,y}(P)+/K\), which corresponds to mixing the distribution \(W_{x,y}(P)\) with a uniform distribution over labels with weight factor \(\).

## 3 Algorithm and Analysis

In this section we present and analyze our main contribution: an efficient bandit multiclass classification algorithm, detailed in Algorithm 1, which will be shown to satisfy the following agnostic PAC guarantee.

**Theorem 1**.: _If we set \(=\), \(M_{1}=(K^{8}(N/))\), \(M_{2}=(N/)K/+1/^{2} \) and use Algorithm 2 to solve the optimization problem defined in Eq. (2) for \(T=((K^{4}/^{4}))\) rounds with step sizes \(_{t}=1/t\) and batch sizes \(b_{t}=(/2K)^{2}t^{2}\) for \(t\{2^{k}-1\}_{k 1}\) and \(b_{t}=t\) otherwise; then with probability at least \(1-\) Algorithm 1 outputs \(\) with \(L_{}()-L_{}(h^{})\), using a total sample complexity of_

\[O((K^{9}+})).\]

_Furthermore, Algorithm 1 makes a total number of \(OK^{4}\) calls to the weighted ERM oracle \(_{}\), and runs in total time polynomial in \(K\), \(1/\) and \((N/)\)._

``` Parameters: \(M_{1},M_{2},(0,]\). Phase 1: Initialize \(S\). while\(|S|<M_{1}\)do  Environment generates \((x,y)\), algorithm receives \(x\).  Predict \(\) uniformly at random from \(\) and receive feedback \(\{=y\}\).  Update \(S S\{(x,y)\}\) if \(y=\), otherwise \(S\) is unchanged. endwhile  Solve the stochastic optimization problem defined in Eq. (2) up to an additive error of \(=^{2}/2K^{2}\) using the dataset \(S\). Let \(_{N}\) be its output.  Phase 2: for\(i=1,,M_{2}\)do  Environment generates \((x_{i},y_{i})\), algorithm receives \(x_{i}\).  With prob. \(\), pick \(_{i}\) uniformly at random; otherwise sample \(h_{i}\) and set \(_{i}=h_{i}(x_{i})\).  Predict \(_{i}\) and receive feedback \(\{_{i}=y_{i}\}\). endfor  Return: \[=_{}\{(x_{i},_{i},_{i})\}_{i =1}^{M_{2}},\] where \(_{i}=\{y_{i}=_{i}\}/W_{}^{}(x_{i},_{i})\). ```

**Algorithm 1** Bandit PAC Multiclass Classification via Log Barrier Stochastic Optimization

Algorithm 1 operates in two phases. In the first phase, we construct a dataset \(S\) of \(M_{1}=(K)(N/)\) i.i.d. samples from \(\) by predicting labels uniformly at random and taking into \(S\) the samples for which the correct label is predicted (and is thus known). We then feed these samples to a stochastic optimization scheme which finds an approximate solution to the following stochastic optimization problem:

\[ (P)_{(x,y)}(P;x,y),(P;x,y)-W_{x,y}^{ }(P)\] (2) s.t. \[P_{N}.\]

The approximate solution, \(_{N}\) will be shown to satisfy certain low-variance properties, which will allow us to repeatedly sample from \(\) for \(M_{2}=O(K/+1/^{2})(N/)\) and estimate the loss of hypotheses in \(\) rounds such that we are guaranteed to find an approximately optimal policy with high probability. We again emphasize that our algorithm is a proper learner in the sense that the returned hypothesis \(\) is a member of the underlying class \(\).

### Overview of analysis

We next outline the details and intuition behind the implementation of Algorithm 1.

Low-variance exploration distribution.The main goal of the first phase of Algorithm 1 is to compute an exploration distribution \(_{N}\) with the property that with probability at least \(1-/2\), the following holds:

\[ h:_{(x,y)} [(h)}{W^{}_{x,y}()}] C,\] (3)

where \(\) is some predefined parameter and \(C\) is an absolute constant, which crucially does not depend on \(K\). The intuition behind this property is that the quantity of the left-hand side of Eq. (3) constitutes an upper bound on the variance of the random variable

\[(h)\{y=\}}{W^{}_{x,y}()},\]

which is an unbiased estimator of the expected reward of the hypothesis \(h\), that is, of \(\{h(x)=y\}\) if the label \(\) is chosen according to an hypothesis drawn from \(\) and \((x,y)\) is drawn from \(\). Thus, we think of Eq. (3) as a property which guarantees that the exploration distribution \(\) can be used to estimate rewards, _uniformly_ for all hypotheses in \(\), with low (constant) variance. This in turn allows us to make use of variance-sensitive concentration bounds, namely Bernstein's inequality, in order to accurately approximate the optimal policy using a small number of samples (this is done in the second phase of the algorithm, described below).

Controlling variance via stochastic optimization.In some more detail, our approach for computing such a low-variance exploration distribution \(\) is via approximately solving the stochastic convex optimization problem defined in Eq. (2). The reason for the choice of \(\) as a convex potential to be minimized is the fact that its gradient is, up to a constant factor, given by

\[ h:((P))_{h}_{(x,y) }[(h)}{W^{}_{x,y}(P)}],\] (4)

so that finding a low-variance exploration distribution amounts to computing \(_{N}\) for which \(()\) is bounded in \(L_{}\) norm. Indeed, we show that the optimal solution to the optimization problem given in Eq. (2) satisfies such a property. This, together with the properties of \(\) as a self-concordant function allows finding such \(\) by approximately minimizing \(\) over the simplex: the self-concordance of \(\) acts as a "restricted strong convexity" property, which roughly implies that approximate minimizers of \(\) must also have small (low norm) gradients.

Efficient optimization via Stochastic Frank-Wolfe.In order to compute an approximate minimizer of the convex potential \(\) defined in Eq. (2), we employ a stochastic optimization procedure, formally described in Algorithm 2, which is based on a stochastic version of the Frank-Wolfe (FW) algorithm  with SPIDER gradient estimates . The reason for choosing a FW based approach, is that it allows for efficient optimization of \(\) in \(N\)-dimensional space, with runtime essentially independent of \(N\), by exploiting the weighted ERM oracle at our disposal. Furthermore, the FW algorithm, when performed over the simplex, generates iterates \(P_{1},P_{2},\) such that \(P_{t}\) is supported on at most \(t\) coordinates (provided that \(P_{1}\) is initialized at an arbitrary vertex of the simplex), allowing us to maintain a succinct representation of the FW iterates--again, essentially independently of the ambient dimension \(N\). Additionally, we remark that while existing analyses (e.g. ) of the stochastic FW algorithm rely on smoothness of the objective with respect to the \(L_{2}\)-norm, our objective of interest, namely \(\), is not smooth in this classical sense, however it is smooth with respect to the \(L_{1}\) norm. Therefore, we crucially rely on a different analysis of the FW algorithm with SPIDER gradient estimates, presented in Appendix C, which accommodates smoothness with respect to the \(L_{1}\) norm.

Final exploration phase.The second phase of Algorithm 1 is more straightforward, where we repeatedly predict labels using i.i.d. samples from a distribution which mixes the distribution over labels induced by \(\) (the exploration distribution computed in phase 1) with a uniform distribution over \(\). Using Bernstein's inequality and the uniform low-variance property of \(\), we show that \(M_{2}=O((K/+1/e^{2})(N/))\) samples suffice in order to ultimately output an hypothesis \(\) being \(\)-optimal with probability at least \(1-/2\). With this in hand, the desired PAC guarantee follows by a union bound over the failure probabilities of the two phases of the algorithm.

### Stochastic Frank-Wolfe with SPIDER Gradient Estimates

Next, we present the stochastic FW procedure used in our algorithm as subprocedure; see Algorithm 2. It is essentially the SPIDER FW algorithm of Yurtsever et al. , specialized for solving the stochastic optimization problem defined in Eq. (2). We remark that the algorithm makes calls to a linear optimization oracle over the simplex denoted by \(\), that given an input \(g^{N}\) computes \((g)=*{argmin}_{Q_{N}}Q g\). As we will show later, each of the calls Algorithm 2 makes to \(\) can be implemented by a call to the weighted ERM oracle. We also remark that, as discussed before, existing analyses of the stochastic FW procedure with SPIDER gradient estimates relies on \(L_{2}\) smoothness of the objective , and here we provide an analysis with respect to \(L_{1}\) smoothness being crucial in our case. We defer further details about SPIDER FW and its analysis to Appendix C.

``` Parameters: Dataset \(S\), step sizes \(\{_{t}\}_{t}\), batch sizes \(\{b_{t}\}_{t}\). Initialize \(P_{1}_{N}\) and initial gradient estimate \(g_{1}=0\). Let \(_{k}=2^{k}-1\) for\(k=1,2,\). for\(t=1,2,\)do  Draw \(b_{t}\) fresh samples \((x_{1},y_{1}),,(x_{b_{t}},y_{b_{t}})\) from \(S\); if\(t\{_{k}\}_{k 1}\)then  Set \[g_{t}=}_{i=1}^{b_{t}}(P_{t};x_{i},y_{i});\] else  Set \[g_{t}=g_{t-1}+}_{i=1}^{b_{t}}((P_{t};x_{i},y_{i})- (P_{t-1};x_{i},y_{i}));\] endif  Compute \(Q_{t}=(g_{t})\);  Update \(P_{t+1}=(1-_{t})P_{t}+_{t}Q_{t}\); endfor ```

**Algorithm 2** Stochastic Frank-Wolfe with SPIDER gradient estimates

### Proof of Theorem 1

We now turn to formally prove Theorem 1. For clarity of notation, we henceforth use \([]\) instead of \(_{(x,y)-}[]\) to denote an expectation of a random variable with respect to \(\). We begin this section by analyzing the first phase of Algorithm 1. The following lemma, which is proven in Appendix A, characterizes the optimal solution to the stochastic optimization problem defined in Eq. (2), and shows that the gradient of \(\) at that optimum is bounded by a constant in \(L_{}\) norm.

**Lemma 1**.: _Suppose \(\) and let \(P_{}*{argmin}_{P_{N}}(P)\), where \(:_{N}_{+}\) was defined in Eq. (2). Then for any \(P_{N}\) we have_

\[[^{}(P)}{W_{x,y}^{}(P_{})}]  1.\] (5)

_In particular, letting \(P\) be the delta distribution on some \(h\):_

\[[(h)}{W_{x,y}^{}(P_{})}] 2.\]

The next key lemma, whose proof is also in Appendix A ensures that a sufficiently approximate minimizer of \(\) is also a point in which the gradient \(\) is bounded in \(L_{}\)-norm, establishing the property given in Eq. (3).

**Lemma 2**.: _Suppose \(\) and assume that for all \(P_{}_{N}\) the following holds for \(\) which was computed in phase 1 of Algorithm 1:_

\[()-(P_{}).\]_Then,_

\[\|()\|_{} 2+}{^{2}}}.\]

_In particular, setting \(=^{2}/2K^{2}\) gives \(\|()\|_{} 3\)._

The fact that the property given in Eq. (3) can be guaranteed with high probability using as few as \(M_{1}=((K)(N/))\) samples relies on the following lemma, also proven in Appendix A, which follows from the analysis of the stochastic Frank-Wolfe procedure (see Appendix C for the detailed analysis).

**Lemma 3**.: _Suppose \(\). If \(M_{1}=58000K^{8}/^{8}(16N/)\) and \(T=240K^{4}/^{4}\), then with probability at least \(1-/2\), for all \(P_{}_{N}\) it holds that_

\[()-(P_{})}{2K^{2}}.\]

We are now in position to prove Theorem 1 by analyzing the second phase of Algorithm 1. We make use of the guarantee of the first phase given in Lemma 2 with respect to the exploration distribution \(\), which allows us to use Bernstein's concentration inequality in order to compute an \(\)-optimal hypothesis in high probability using only \( K/+1/^{2}\) samples.

Proof of Theorem 1.: We first show that we can in fact use Algorithm 2 as specified in the theorem's statement. That is, we prove that the calls to the linear optimization oracle, denoted by \(\) in Algorithm 2 can be implemented using the weighted ERM oracle \(_{}\). Indeed, we first note that for any \(g^{N}\) it holds that

\[(g)=*{argmin}_{P_{N}}\{P g \}=*{argmin}_{h}\{g_{h}\},\]

so it suffices to show that this can be represented in the form given in Eq. (1) for the SPDER gradient estimates used in Algorithm 2, which we denote by \(g_{t}\). It is straightforward to see each \(g_{t}\) is a linear combination of terms of the form \((P,x,y)\) where \(P\) is either \(P_{t}\) or \(P_{t-1}\). Thus, we show that for \(g=_{i=1}^{n}(P_{t},x_{i},y_{i})\), \((g)\) can be computed by a call to the weighted ERM oracle. Indeed,

\[(g)=*{argmin}_{h}\{-(1-) _{i=1}^{n}\{h(x_{i})=y_{i}\}}{W_{x_{i},y_{i}} ^{}(P_{t})}\}=*{argmin}_{h}_{i=1}^ {n}_{i}\{h(x_{i}) y_{i}\},\]

where \(_{i}=(1-)/nW_{x_{i},y_{i}}^{}(P_{t})\). Now, assume that after phase 1, Algorithm 1 computed \(_{N}\) with

\[_{h}[,y}(h)}{W_{x,y}^{}()}] 3.\]

Fix some \(h\). For \(i\{1,,M_{2}\}\) define \(X_{i}(h)=,y_{i}}(h)\{y_{i}=_{i}\}}{W_{x_{i},y_ {i}}^{}(P)}\). Note that \([X_{i}(h)]=[h(x)=y]\) and that \(X_{1}(h),,X_{M_{2}}(h)\) are i.i.d. Additionally, by the guarantee of phase 1:

\[[X_{i}(h)][X_{i}(h)^{2}] =[,y_{i}}(h)\{y_{i}=_{i}\}}{(W_ {x_{i},y_{i}}^{}())^{2}}]=[(h)} {W_{x,y}^{}()}] 3.\]

Define \((h)=}_{i=1}^{M_{2}}X_{i}(h)\), and note that by the form of the weighted ERM oracle, \(=*{argmax}_{h}(h)\). Therefore, by an application of Bernstein's inequality (see e.g. , page 86) and a union bound, we have with probability at least \(1-/2\) for all \(h\):

\[|(h)-[h(x)=y]|} }+}.\]Hence for \(\), with probability at least \(1-/2\) we have

\[L_{}()-L_{}(h^{}) =[(x) y]-[h^{}(x) y]\] \[=[h^{}(x)=y]-[(x)=y]\] \[[h_{}(x)=y]-(h^{})+()-[ (x)=y]\] \[}}+}.\]

Choosing \(M_{2}\{144(2N/)/^{2},8K(2N/)/( )\}\) gives \(L_{}()-L_{}(h^{})\). Thus, using Lemma 2 and Lemma 3, we are only left with proving that with high probability, collecting \(M_{1}\) samples for \(S\) takes at most \(O(K^{9}(N/))\) steps. This essentially follows from the fact that a binomial random variables is smaller than half of its expected value with very small probability. Formally, we use lemma F.4 of  to deduce that if \(X(4KM_{1},1/K)\) then with probability at least \(1-\) it holds that \(X 2M_{1}-(1/) M_{1}\), which means that \(4KM_{1}\) trials suffice to guarantee that with probability at least \(1-\), the dataset \(S\) will contain at least \(M_{1}\) samples. Thus, the proof of Theorem 1 is complete once we make the observation that \(2K/ K^{2}+1/^{2}\) (using the AM-GM inequality) so that the \(K/\) term is of lower order and can be dropped from the final bound. 

## 4 Extension to Natarajan Classes

In this section we extend our result for finite classes given in Theorem 1 to general, possibly infinite hypothesis classes \(\) with finite _Natarajan dimension_. The Natarajan dimension is an extension of the VC dimension to the multiclass setting, defined as follows:

**Definition 1** (Natarajan dimension ).: _The Natarajan dimension of a hypothesis class \(\) is the largest integer \(d\) for which there exist \(d\) points \(x_{1},,x_{d}\) and \(d\) pairs of distinct labels \(\{y_{1,1},y_{1,2}\},,\{y_{d,1},y_{d,2}\}|}{2}\) such that all \(2^{d}\) sequences of the form \((x_{1},y_{1}),,(x_{d},y_{d})\), with \(y_{i}\{y_{i1},y_{i2}\}\), are realizable by \(\)._

Note that in the binary case, when \(=\{0,1\}\), the Natarajan dimension reduces to the VC dimension. Our main result in this case effectively replaces the \(||\) term, relevant when \(\) is finite, with the Natarajan dimension \(d_{N}\) of \(\).

**Theorem 2**.: _A hypothesis class \(:\) is PAC learnable with bandit feedback if and only if (i) it has a finite Natarajan dimension, and (ii) there exists \(K\) such that \(|\{h(x):h\}| K\) for every \(x\)._

_Furthermore, let \(:\) be a hypothesis class of finite Natarajan dimension \(d_{N}\). Then there exists a bandit multiclass classification algorithm which outputs a hypothesis \(\) with \(L_{}()-_{h}L_{}(h)\) with a sample complexity of \(((K^{9}+1/^{2})d_{N}(1/))\)._

As discussed, this result improves the classical result of Daniely et al. , who provided an upper bound on the sample complexity of PAC learning with bandit feedback, given by \((Kd_{N}/^{2})\), and left obtaining tighter bounds as an open question. In particular, for small target excess loss (\(0\)), our bound eliminates the linear dependence on the number of labels \(K\), thereby matching the \((d_{N}/^{2})\) bound from the full information setting .

Theorem 2 follows directly from the following technical result, which is proven in Appendix B, when put together with Theorem 1.

**Proposition 1**.: _Assume that the sample complexity of PAC learning with bandit feedback a finite class of size \(N\) over a label-space of size \(K\) is at most \(m(N,K,,)\), where \(,\) are the error and confidence parameters. Then, the sample complexity of learning an infinite class \(\) is at most_

\[s+m(S,K,/2,/2),\]

_where_

\[s :=O(K)(1/)+(1/)}{ },\] \[S :=_{i=0}^{d_{N}}^{i}}^{d_{N}}K^{2d_{N}},\]_and \(d_{N}\) is the Natarajan dimension of \(\)._

The proposition is proven by using the first \(s\) examples to construct a finite discretization of size \(S\) of the class \(\), which is \((/2)\)-dense in \(\) in the sense that every \(h\) is \((/2)\)-close to some \(h^{}\) in the discretization. Then, we apply our algorithm for finite classes on this finite discretization. Most of the proof is dedicated to establishing that the discretization is dense and to upper bounding its size. A similar result for discretizing binary classes, with the VC dimension replacing the Natarajan dimension, is well known. In the VC case, the proof is based on considering the class of all symmetric differences of hypotheses from \(\) and analyzing the VC dimension of that class using standard techniques. In our proof, we follow a similar argument, but the analysis of the VC dimension of the symmetric difference class, which remains binary even in the multiclass setting, is more nuanced.

Proof of Theorem 2.: For the first part of the theorem, note that Item (i) is clearly necessary for learnability, as a finite Natarajan dimension is required even in the case of full information feedback. Item (ii) is also necessary: if there exists a point \(x\) such that \(\{h(x):h\}\) contains more than \(K\) distinct labels, then in the realizable case, the sample complexity with bandit feedback has a lower bound of \((K)\), provided \(K\) is a sufficiently large constant. This follows by setting the target distribution to assign probability \(1\) to a single example \((x,y)\), where \(y\) is drawn uniformly in advance from \(K\) labels in \(\{h(x):h\}\). An elementary coupon collector argument yields a lower bound of \(K/2\): indeed, if only \(m K/2\) examples are drawn, then the learner does not correctly guess the label during training with probability at least \(1/2\). Conditioned on this event, the population loss is \( 1/2\) (in fact, \(\)), leading to a lower bound of \(1/2 1/2=1/4\) on the population error.

For sufficiency, note that if Item (ii) holds, the class \(\) effectively reduces to a class over \(K\) total labels and is therefore learnable when its Natarajan dimension is finite, as follows from .

For the second part of the theorem, note that by Theorem 1, we have an upper bound on the sample complexity for PAC learning with bandit feedback over finite classes of size \(N\) of

\[m(N,K,,)=O(K^{9}+1/^{2})(N/).\]

Thus, the proof follows immediately from Proposition 1 together with the fact that:

\[\!() d_{N}\,\!(} )+2d_{N}\, K+,\]

where \(s\) and \(S\) are defined in the statement of Proposition 1 (note that \((s)\) only contributes logarithmic terms to the overall bound).

## 5 Conclusion and Open Problems

In this work, we establish a nearly-optimal sample complexity of \((((K)+1/^{2})(|H|/))\) for the bandit multiclass classification problem and design an efficient algorithm achieving this bound. While the dominant term in our sample complexity bound (which depends on \(\)) is optimal, our bound exhibits a \(K^{9}\) additive dependence on the size of the label space. We conjecture that the optimal bound is of the form \(OK/+1/^{2}\), for which a matching lower bound can be shown in a straightforward manner. One possible approach to improve the dependence on \(K\) is perhaps a more adaptive method which combines maximization of estimated rewards in tandem with lowering the variance of the sampling distribution gradually over time. We leave this as a very interesting question for future research.