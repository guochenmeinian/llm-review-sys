# Optimization and Bayes: A Trade-off for Overparameterized Neural Networks

Zhengmian Hu, Heng Huang

Department of Computer Science

University of Maryland

College Park, MD 20740

huzhengmian@gmail.com,henghuanghh@gmail.com

###### Abstract

This paper proposes a novel algorithm, Transformative Bayesian Learning (TransBL), which bridges the gap between empirical risk minimization (ERM) and Bayesian learning for neural networks. We compare ERM, which uses gradient descent to optimize, and Bayesian learning with importance sampling for their generalization and computational complexity. We derive the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior. Moreover, we show how to transform gradient-based optimization into importance sampling by incorporating a weight. While Bayesian learning has better generalization, it suffers from low sampling efficiency. Optimization methods, on the other hand, have good sampling efficiency but poor generalization. Our proposed algorithm TransBL enables a trade-off between generalization and sampling efficiency.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved remarkable success in machine learning and related applications. It repeatedly outperformed conventional machine learning approaches, resulting in ground-breaking research such as human-level performance in computer vision , substantial progress in natural language processing , and mastering the game of Go .

The success of DNNs is the result of a critical combination of the complexity and generalization. On the one hand, universal approximation theorem  guarantees that any continuous function can be approximated arbitrarily well by using a deep network. On the other hand, deep architectures, together with large-scale training data and back-propagation algorithm, present a good generalization ability towards unseen data.

Although expressive power and generalization ability are both desirable on their own, they are mutually incompatible. Being one of the main contributions of statistical learning theory, the probably approximately correct (PAC) learning  allows us to establish an upper bound of generalization gap by capacity of a hypothesis space. Generally speaking, if a model is capable of fitting random labels, then it must generalize poorly.

Recent result  shows that overparameterized DNNs do fit random label perfectly. However, it is observed that more steps of stochastic gradient descent (SGD) are needed to train a neural network to fit random labels. This phenomenon suggests that the learning capability of DNNs increases with the number of training steps. In light of this observation, the algorithm-dependent generalization bounds which control the complexity by maximum training steps are preferred over uniform convergence bounds.

In this paper, we follow the PAC-Bayesian approach to derive our algorithm-dependent generalization bounds. PAC-Bayesian bounds were first introduced by McAllester  and further developed in . These bounds apply for stochastic learning algorithms and focus on expected generalization error over a probability on parameter space. The generalization is related to the KL divergence between the output distribution of a learning algorithm and a prior distribution.

Typically, deep neural networks contain a large number of parameters and are trained by numerous training epochs through some gradient descent updates. The training dynamics of deep neural networks is complicated, and hence direct theoretical analysis on the KL divergence is intractable. Fortunately, such situation can be largely simplified in the infinite width limit . Under this limit, the output distribution of stochastic neural networks drawn from the prior is approximated by a Gaussian distribution. Moreover, the training dynamics of gradient-based optimization is governed by the kernel gradient descent, which guarantees that the evolution of network output only depends on the function values themselves. Thus, our first problem to study in this paper is: \(\)_In the infinitely wide neural network limit, can we theoretically derive the formula of this KL divergence?_

Optimizing the PAC-Bayesian bound of expected error gives rise to the Gibbs measure. It is also widely termed as posterior, as it shares the same form as posterior in Bayes' rule, given that error is interpreted as likelihood. Drawing samples from this posterior is very hard in practice. Markov Chain Monte Carlo (MCMC) methods have been explored for deep Bayesian learning. However, it suffers from slow convergence and high computational cost on high- dimensional parameter spaces. Non-exact minimization of PAC-Bayesian bound gives rise to variational approximation (VA) which is more computationally efficient, but biased due to the difference between variational distribution and true posterior.

Given that MCMC and VA have their own disadvantages in Bayesian learning, our paper investigates the second problem: \(\)_Does there exist a Bayesian learning method with non-diminishing sampling efficiency even for infinite wide neural network?_

Finally, we notice that the gradient-based optimization is efficient in training DNNs, but comes with larger generalization error. Bayesian learning, on the other hand, optimizes the expected loss but has lower efficiency. Our third question to study is: \(\)_Does there exist an interpolation between optimization and Bayesian learning for trade-off between computation efficiency and generalization error?_

In this paper, we give positive answers to above three questions and the main contributions of this paper are summarized as follows:

1. We analyze the infinitely wide neural network trained by gradient flow, or equivalently infinitesimal step size gradient descent. We show that the infinite width limit largely simplifies the training dynamics, and the KL divergence between the output distribution and Gaussian prior can be formulated as a function of training time and training data.
2. As a byproduct of our analysis on the generalization and sampling efficiency, we prove that the trace of Hessian for DNN is not diminishing under infinite width limit and depends on the initialization and training. To the best of our knowledge, this dynamics of Hessian trace is new and maybe of independent interest.
3. We show that if the determinant of Jacobian of optimization flow is available, we can compute a weight for each optimized predictor, such that the weighted output distribution is just posterior. The sampling efficiency in infinite width limit is also derived. We call this type of algorithm as Transformative Bayesian Learning (TransBL) because it is transformed from an optimization procedure.
4. We show that modifying the additional weight in TransBL gives rise to an interpolation between optimization and Bayesian learning. The behaviour of TransBL is increasingly similar to optimization when the weight is changed toward being uniform. This interpolation doesn't alter training dynamics, thus enables flexible trade-off between sampling efficiency and generalization.

## 2 Background

We first explain the setting and briefly review PAC-Bayesian and Bayesian learning as background. Other related works is discussed in Appendix A.

Problem SetupGiven input space \(\) and label space \(\), we assume that labeled training data are drawn independently from an unknown data distribution \(D\) over \(\). The training set is denoted as \(^{}=\{(s_{a},z_{a})|1,,m\}\), where \(m\) is the size of the training sample. A predictor is a function \(h:\) and the set of predictors is denoted as \(\). We introduce two losses \(l:\) and \(l^{s}:\), where the first one is used to measure the error rate and the second smooth surrogate loss with polynomially bounded second order derivative is used for providing learning signal in gradient-based training. We define expected loss as \(R(h)=_{(s,z) D}[l(h(s),z)]\) and empirical loss as \(r(h)=_{a=1}^{m}l(h(s_{a}),z_{a})\). The expected loss is the central quantity which we are interested in but cannot be unobserved in general. Therefore, the currently popular approach to statistical learning is to derive an upper bound of expected loss.

PAC-Bayesian BoundsWe will follow the PAC-Bayesian approach to the generalization problem.

**Theorem 1** (Theorem 1.2.6. in ).: _For any positive \(\) and \((0,1)\), with at least \(1-\) in the probability of training samples, for all distribution \(q\) on space \(\), we have_

\[_{h q}[R(h)]_{}^{-1}(_{h q }[r(h)]+}(q\|p)+}{}).\]

_The KL divergence is \(D_{}(q\|p)=_{x q}[]\) and the auxiliary function is defined as_

\[_{a}^{-1}(v)=.\]

Although there are huge amounts of research on PAC-Bayesian approach, all of them rely on the KL divergence between output distribution \(q\) of stochastic predictor learned from certain algorithm and a prior \(p\), therefore the analysis of this KL divergence is a natural question.

Bayesian LearningWe define Gibbs measure as \(p_{}()=}e^{- r()}p()\) and partition function as \(Z_{}=_{ p}[e^{- r()}]\). The Gibbs measure is related to posterior in Bayesian inference , and minimize right-hand side in theorem 1, inducing a much tighter bound:

\[_{h q}[R(h)] _{}^{-1}(-(Z_{} ))-)}}.\]

Despite the fact that expected loss is optimized, Bayesian learning bears significantly more difficulty than popular optimization-based algorithms. It is noted that the sampling from a high dimensional distribution with irregular shape may lead to high fluctuations and low efficiency, yet in typical neural network, the number of parameters to be trained is very large, and loss landscape is complicated. Many sampling techniques have been developed and explored, but the state-of-the-art is still far from satisfactory. In this paper, we concentrate on theoretical understanding of the Bayesian approach and consider a special class of importance sampling in Section 4 where we can compare the computation efficiency of Bayesian method and optimization approach.

## 3 Training by Optimization

We now derive the KL divergence under deterministic optimization setting. We assume all predictors are parameterized by some parameters \(\). The expected loss \(R(h)\) and empirical loss \(r(h)\) could therefore be regarded as functions of parameter \(\). An optimization flow is a function \(f:\), which typically relies on the training data to update the model's parameters \(\) and minimize the empirical loss \(r(h)\). The updating can be achieved through various methods such as single or multiple steps of gradient descent, gradient flow with infinitesimally small step sizes, or more sophisticated methods detailed in Appendix B. We require the Jacobian determinant \(f_{f}()=( f())\) exists and the optimization flow is a bijective, _i.e._ the inverse \(f^{-1}\) exists everywhere. This is satisfied for gradient flow and gradient descent when step size is smaller than \(\). More situations where this assumption holds is discussed in Appendix B. The starting point of optimization is initialized from a prior distribution \(p()\), and we define a corresponding energy \(V()=- p()+C\) where \(C\) is a parameter independent constant. The output distribution \(q\) could be characterized as follows:

\[_{q}( A)=_{p}(f() A),q()=p(f^{-1}()) J_{f^{-1}}().\]The KL divergence between output distribution and prior can be derived as follows:

\[D_{}(q\|p)= _{ q}[]=_{ p}[]=_{ p }[-|J_{f}()|].\]

Based on a physics analog of Helmholtz free energy change in isothermal process, we define energy term and entropy term separately:

\[_{f}V()=V(f())-V(),\ \ \ \ \ _{f}S()=|J_{f}( )|.\]

The energy term only depends on the prior. If Gaussian prior is used, \(_{f}V()\) corresponds to the change of squared norm of parameters. Although the KL divergence is positively related to the energy term, and energy term is monotonically increasing with norm of trained parameters, naively compressing parameters to reduce parameter norm doesn't lead to decrease in KL divergence, because the decrease in energy term is offset by the change in entropy term.

The entropy term determines the gain or loss of entropy caused by an optimization procedure. The pure gain means that the optimization flow maps a region of parameter space into another region with larger area. For example, consider the simple function \(f(x)=2x\) in a one-dimensional setting. The Jacobian of this function, which describes how much the function stretches or compresses space, is constant at \(2\). Imagine we start with an input \(x\) that is uniformly distributed within the interval \(\). After applying the function, the output \(f(x)\) becomes uniformly distributed over this larger interval \(\). Therefore the entropy increase by \((2)\). However, most optimization procedures pursue minimization of certain objective function which is only small in an extremely small region of parameter space, and thus loss of entropy is inevitable. In Section 7, we show that for gradient descent algorithm, the loss of entropy is related to the local curvature. We also extend the discussion to algorithms with enriched state space, such as Momentum SGD and Adagrad in Appendix F.2.

Finally, we have

\[D_{}(q\|p)=_{ p}[_{f}V()-_{f}S( )]\] (1)

The above KL divergence represents an increase in free energy, or the information gain from the training, depending on the point of view. Both energy term and entropy term can be evaluated empirically, though the computational cost depends on the choice of optimization method, network architecture and prior.

## 4 Transformative Bayesian Learning

In this section, we introduce a class of importance sampling algorithms solving the Bayesian learning problem. These algorithms are based on the optimization flow, energy change, and loss of entropy discussed in Section 3.

The most simple example of TransBL algorithm comes from using the output distribution \(q\) of an optimization flow \(f\) as the proposal distribution. For any function \(F\) on parameter space \(\), we have

\[_{ p_{}}[F()]= _{ q}F()( )}{q()}=}_{ p} F(f())e^{- r(f())}\] (2) \[= }_{ p}F(f( ))e^{- r(f())}e^{-_{f}V()}e^{_{f}S()} \]

The above expectation could be regarded as attaching an additional weight to the stochastic predictor obtained from ordinary optimization, and we should use a weighted average of the results obtained by different initialization. This process doesn't involve any more training of parameter \(\) than the optimization procedure that TransBL is based on, though additional computation might be required for the value of \(_{f}V()\) and \(_{f}S()\).

The unnormalized weight of above importance sampling is \(w_{}()=e^{- r(f())}e^{-_{f}V()}e^{ _{f}S()}\). That means not all results obtained by training are treated equally. In particular, the solution with lower empirical loss, lower energy increase, and producing higher entropy is more important than others.

One benefit of TransBL is that we can measure the computation efficiency by comparing it to the training by optimization. A simple way to do this is to calculate the ratio of effective sample size and sample size:

\[_{}=_{ p}[w_{}()])^{2}}{ _{ p}[w_{}^{2}()]}\] (3)

We define the above value as sampling efficiency, and a non-diminishing efficiency at certain limit indicates that Bayesian learning is at most a constant factor slower than optimization. The numerator of the above expression is determined by the partition function because \(_{ p}[w_{}()]=Z_{}\). Therefore, only the denominator depends on the optimization procedure, and we expect \(_{ p}[w_{}^{2}()]\) to be as small as possible for efficient Bayesian learning. Notice that this quantity again relies on energy change \(_{f}V()\) and entropy change \(_{f}S()\). We will give theoretical analysis of these two values for infinitely wide neural network in Section 7.

### An Illustrative Example

For illustration purpose, we consider a univariate loss function that presents two distinct global minima with zero loss. One of these is characterized as a sharp minimum, while the other represents a flat minimum. A direct initialization from the prior, followed by training using gradient flow, often results in an ensemble with significant deviation from the posterior. This is because the optimization process fails to recognize the presence of the sharp minimum, while insights from PAC Bayesian indicate that the flat minimum is surrounded by a higher posterior probability density. TransBL method applies a small weight to the solution found within the sharp minimum. Consequently, TransBL can adeptly recreate the posterior, as shown in the Figure 5(b). Due to space limitations, all figures for the illustration are moved to Appendix F.1.

## 5 Connections between Bayesian Learning and Optimization

### A Bayesian Perspective for Optimization

We show that, in order to achieve low expected loss, the optimization flow in deep learning should reshape the initial distribution to a good estimation of posterior. A formal argument relies on applying Donsker and Varadhan's variational formula  to obtain the following equation:

\[_{ q}[r()]+D_{}(q\|p)=- _{ p}[(- r())]+ D_{}(q\|p_{})\] (4)

The first term is independent of training procedure and only relies on the definition of prior. The second term measures the KL divergence between output distribution and Gibbs measure. This is also the gap between expected loss bound obtained by some training methods and the optimal one. Notice that the above KL divergence could also be expressed in terms of weight for TransBL, energy change, and entropy loss:

\[D_{}(q\|p_{})= Z_{}+_{ p}[-  w_{}()]= Z_{}+_{ p}[ r (f())+_{f}V()-_{f}S()].\] (5)

### Efficiency of Optimization Flows for TransBL

In this section, we show that, in order to achieve high sampling efficiency, the optimization flow in deep learning is again expected to reshape the initial distribution to a good estimation of posterior. The only difference to the previous section is that the deviation between output distribution and posterior is measured under a different divergence.

In order to see this, we define \(^{2}\) divergence as \(D_{^{2}}(p\|q)=_{x q}[()^{2}]-1\).

**Theorem 2**.: _For sampling efficiency of TransBL whose proposal distribution is \(q\), we have:_

\[D_{}(p_{}\|q)1+D_{^{2}}(p_{}\|q) =-_{}\] (6)

With the above result, we can establish the equivalence between optimal generalization bound and optimal sampling efficiency, _i.e._ an optimization flow is optimal in the sense it minimizes the expected loss upper bound if and only if the sampling efficiency of TransBL is 1. A more general correspondence between generalization and sampling efficiency is desired. However, the complication lies in different divergences used in two quantities. By comparing Eq. (4) and Eq. (6),we see that \(^{2}\) divergence is used for measuring sampling efficiency. Although an upper bound of KL divergence \(D_{}(p_{}\|q)\) could be obtained, this is not directly comparable to the \(D_{}(q\|p_{})\) used in generalization bound because of asymmetry of KL divergence. That difference also justifies the interpolation between optimization and Bayesian learning shown in the next section.

## 6 Interpolation of Optimization and Bayesian Learning

We recall that the output distribution \(q\) for the optimization and the posterior \(p_{}\) for Bayesian learning are connected by an additional weight \(w_{}\): \(p_{}()=}w_{}(f^{-1}())q()\). These two distributions have distinct properties. The distribution \(q\) bears worse generalization, but is easy to sample from given an established optimization oracle. The posterior \(p_{}\), on the other hand, is better shaped, but hard to sample from. Therefore, a natural question is whether we can find a distribution \(p_{}^{}\) as intermediate and interpolation between them.

Given our formulation of TransBL, it turned out to be quite easy to construct such an interpolation by modifying the weight. For a function \(v_{}:^{+}^{+}\) where \(\) is the parameter used for interpolation, we can define a modified weight as \(v_{}(w_{}())\). The interpolation distribution is

\[p_{}^{}()=^{}}v_{}(w_{ }(f^{-1}()))q(), Z_{}^{}=_{  p}[v_{}(w_{}())].\]

Note that the superscript doesn't mean power but means the interpolation is controlled by \(\). The expectation of a function \(F()\) on \(p_{}^{}\) could be computed similarly as Eq. (2):

\[_{ p_{}^{}}[F()]=^{ }}_{ p}[F(f())v_{}(w_{}())].\]

We note Eq. (4) still holds when we change \(q\) into \(p_{}^{}\), and the gap between the expected loss bound obtained by \(p_{}^{}\) and the posterior is:

\[D_{}(p_{}^{}\|p_{})=}{Z_{ }^{}}+^{}}_{ p} (w_{}())}{w_{}()} v_{}(w_{}()).\]

Notice that the above identity degenerates into Eq. (5) if we use \(v_{}()=1\), which corresponds to the original optimization.

The sampling efficiency of \(p_{}^{}\) is again defined as the ratio of effective sample size and sample size: \(_{}^{}=(_{ p}[v_{}(w_{ }()]))^{2}/_{ p}[v_{}(w_{}())^{2}]\) and enjoys a relation similar to Theorem 2:

\[D_{}(p_{}^{}\|q)1+D_{^{2}}(p_{ }^{}\|q)=-_{}^{}.\]

### Trade-off between Generalization and Sampling Efficiency

According to Eq. (5), we can see that the generalization is highly affected by extremely small \(w_{}()\). A small weight indicates that the optimization produces certain parameters too often. This issue could be addressed by assigning a small weight for these parameters in importance sampling. On the other hand, the sampling efficiency of Bayesian learning is very sensitive to large value of \(w_{}()\), according to Eq. (3). The high weight appears on the regime where output probability is low but posterior density is high. That essentially means the optimization oracle is not effective in exploring certain region, and the only way to bypass this bottleneck of Bayesian learning is changing the optimization oracle to produce a distribution with heavier tail than posterior. Due to the exponential dependence on energy change in weight, the distribution of weight is typically heavy-tailed, therefore the efficiency is low. Fortunately, this issue could be largely alleviated with interpolation. Since generalization bound is insensitive to large value of weight, it is possible to apply appropriate modification to weights such that the sampling efficiency is improved and generalization doesn't deteriorate too much.

Searching for the Pareto optimality among various trade-off methods is not a concern in this paper, although obviously this is an important issue and is a subject for future work. In this paper, we only consider a simple weight clipping method:\(v_{}(w)=w&w\\ &\).

At the limit of \( 0\), \(p_{}^{}\) degenerates to output distribution \(q\) of optimization. At the limit of \(\), \(p_{}^{}\) converges to posterior \(p_{}\). A lower bound of sampling efficiency \(_{}^{}(Z_{}^{}/)^{2}\) could be established. More importantly, weight clipping always achieves smaller generalization bounds than optimization, and higher sampling efficiency than Bayesian learning.

**Theorem 3**.: _For parameters \((0,)\), \(D_{}(p_{}^{}\|p_{})\) and \(_{}^{}\) are both monotonically decreasing functions of \(\)._

## 7 Overparameterized Neural Network

The energy change \(_{f}V()\) and entropy change \(_{f}S()\) play a pivot role in previous sections. All quantities of interest, including KL divergence in the PAC-Bayesian bounds, weights in TransBL and sampling efficiency, depend on these two terms. Therefore, it is essential to understand the dynamics of these quantities. In this section, we will explore some simple infinitely wide neural network. In particular, this analysis will provide us insight for further developments in the deep Bayesian learning and PAC-Bayesian bounds.

### Network Definition

We consider a feedforward network with \(d\) fully connected layers. The parameter \(=(W_{1},,W_{d})\) is a vector of flattened weights. For a specific input \(s_{a}\), the forward propagation of the network is defined as follows:

\[x_{0}(,s_{a})=s_{a},\ \ y_{i}(,s_{a})=c_{i}W_{i}x_{i-1}(,s_{ a}),\ \ x_{i}(,s_{a})=_{i}(y_{i}(,s_{a})).\]

The output \(y_{d}(,s_{a})\) is a scalar. The point-wise function \(_{i}(y)\) is an activation function with bounded first to fourth order derivative. The hidden layer width at \(i\)-th layer is \(n_{i}\) such that \(y_{i}(,s_{a}),x_{i}(,s_{a})^{n_{i}}\). \(W_{i}^{n_{i} n_{i-1}}\) are trainable weights. The coefficient \(c_{i}=1/}\) is used to ensure the output lies in a proper scale at infinite width limit, and is widely adopted in previous works [43; 51; 70; 40; 39; 3; 18; 59]. The network is initialized with Gaussian prior, such that \((W_{i})_{j,k}(0,1)\).

We introduce the following notation to simplify the result. For a multi-variable scalar function \(f(X)\), \(_{X}f(X)\) is a row vector when \(X\) is a vector. If \(X\) is a matrix, then \((_{X}f(X))_{i,j}=}\) and \(_{X}f(X)\) share the same shape as \(X^{T}\). For a multi-variable vector-value function \(F(X)\), we define \((_{X}F(X))_{i,j}=(X)}{ X_{j}}\).

The network is trained by gradient flow or infinitesimal step size gradient descent. Let output vector be \((Y())_{a}=y_{d}(,s_{a})\) and total loss function as \((Y)=_{a=1}^{m}l^{s}(Y_{a},z_{a})\), the gradient flow is defined as:

\[}{t}(t)=-(_{Y}(Y((t))) _{}Y((t)))^{T}.\]

### Energy Term

The energy for the prior is defined as \(V()=_{i=1}^{d}\|W_{i}\|_{}^{2}\). We define \(y_{d}^{(i)}(,s_{a})=(_{W_{i}}y_{d}(,s_{a})W_{i})\) and the dynamics of \(V((t))\) is:

\[}{t}V((t))=-_{Y}(Y((t )))_{i=1}^{d}Y^{(i)}((t)).\] (7)

We find that \(y_{d}^{(i)}(,s_{a})\) can be computed by a group of auxiliary vector as Eq. (27) detailed in the appendix, and could be regarded as the tangent vector at output space by propagating a tangent vector \(W_{i}\) from \(i\)-th layer.

For deriving the dynamics of \((Y^{(i)}())_{a}=y_{d}^{(i)}(,s_{a})\), we define \(^{(i)}()=(_{}Y^{(i)}())(_{}Y()) ^{T}\) and it follows

\[}{t}Y^{(i)}((t))=-^{(i)}((t))( _{Y}(Y((t))))^{T}.\] (8)

We note that the definition of \(Y^{(d)}\) follows \(Y=Y^{(d)}\), therefore \(^{(d)}\), as a gradient Gram matrix, is just neural tangent kernel (NTK) which has been studied in [39; 3; 18].

### Entropy Term

We write down the gradient flow in an abstract form \(}{t}(t)=-g((t))\) and denote the solution as \((t)=f(t,(0))=(0)-_{0}^{t}g((t))dt\). This gradient flow is a special example of optimization flow in Section 3. Here the entropy change is \(_{t}S()= J_{f(t,)}()\). According to Liouville formula, we have \(}{t}(_{t}S((0)))=- _{}((t))=-( g((t)))\). Given that the gradient has form \(g()=-(_{Y}(Y())_{}Y())^{T}\), the entropy change could be formulated as follows:

\[}{t}(_{t}S((0)))=- _{Y}_{Y}(Y((t)))^{(d)}((t)) -_{a=1}^{m}_{Y_{a}}(Y((t)))(H_{a}((t))).\] (9)

The content in the first trace term in Eq. (9) is called Gauss-Newton matrix, which only represents curvature information of loss \(l^{s}\). The matrix \(H_{a}=_{}_{}Y_{a}\) is the Hessian of \(y_{d}(,s_{a})\). It is known that spectral norm of Hessian vanishes for wide neural network . However, we find that the trace is not constant and depends on initialization and training. For all \(1 i j<k d\), \(=1,,n_{j}\), we introduce auxiliary vectors \((^{(i,j)})_{}=c_{i}^{2}{\|{x_{i-1}}\|}^{2}{\|{_ {y_{i}}(y_{j})_{}}\|}^{2}\) and \(_{k}^{(j)}=(_{x_{j}}y_{k})_{j}^{}(y_{j})\). Notice that we left out parameters \((,s_{a})\) for concision.

**Theorem 4**.: _Let \(\) be element-wise product, we have_

\[(H_{a}((t)))=_{j=1}^{d-1}(_{x_{j}}y_{d}) (_{j}^{}(y_{j})(_{i=1}^{j}^{(i,j)} )).\]

The above formula is a multi-variable version of \((f_{d-1} f_{1})^{}=_{j=1}^{d-1}(f_{d-1}  f_{j+1})^{}f_{j}^{}((f_{j-1} f_ {1})^{})^{2}\). \(^{(i,j)}\) plays the role of squared gradient and is a kind of diagonal NTK in middle layers.

We define \((^{(i)}())_{a}=_{d}^{(i)}(,s_{a})\) and matrices \(^{(i)}()=(_{}^{(i)}())(_{}Y( ))^{T}\). Then we have for all \(1 i<d\),

\[}{t}^{(i)}((t))=-^{(i)}((t))( _{Y}(Y((t))))^{T}.\] (10)

### Infinite Width Limit

The dynamics in previous two sections can be dramatically simplified in infinite width limit.

**Theorem 5**.: _When hidden layers width \(n=n_{1},,n_{d-1}\) approaches infinity, the random vectors \(Y^{(i)}((0))\) and \(^{(i)}((0))\) at initialization converges in law to a multivariate Gaussian distribution with zero mean and covariance matrix \(\). Moreover, the following quantities converge in probability to constant and don't change during finite training time:_

\[^{(i)}((t))^{(i)},\ \ ^{(i)}((t)) ^{(i)},\ \ (^{(i,j)}((t),s_{a}))_{} (^{(i,j)})_{a}.\] (11)

_The specific formulas of \(\), \(^{(i)}\), \(^{(i)}\), and \(^{(i,j)}\) are shown in Appendix D._

The last line of above limits is for all \(=1,,n_{j}\), therefore theorem 4 could be simplified to:

\[(H_{a}((t)))=_{j=1}^{d-1}(^{(j)}((t)))_ {a}(_{i=1}^{j}(^{(i,j)})_{a}).\]

Theorem 5 demonstrates a function space picture, where the dynamics of \(Y^{(i)}((t))\) and \(^{(i)}((t))\) only depend on these values themselves but not on internal dynamics of the network. The auxiliary vectors \(Y^{(i)}((t))\) and \(^{(i)}((t))\) could be obtained by solving ODEs (8) and (10), \(}{t}V\) and \(}{t}S\) can be integrated according to Eqs. (7) and (9). Therefore, the KL divergence is accessible as an expectation in Eq. (1). Based on this KL divergence, we can obtain the final result of the PAC Bayes bound formulated as a function of training time and training data. The complete formula of this PAC Bayes bound is shown in Appendix F.4.

In order to get a sense of the how the generalization bound and sampling efficiency are influenced by training time, we show the asymptotic behaviour under mild assumption on loss function.

**Corollary 6**.: _If there exist \(C_{1}\), \(C_{2}\) such that \(|}{y}l^{s}(y,t)| C_{1}\), \(|^{2}}{y^{2}}l^{s}(y,t)| C_{2}\), then \(D_{}(q_{t}\|p)(1)t+(1)t^{2}\), \(_{}(1)-(1)t- (1)t^{2}\) for any finite \(t\), where \((1)\) represents positive constant irrelevant to \(t\), \(q_{t}\) is the output distribution of an infinitely wide network after training for \(t\) length of time, and \(_{}\) is the sampling efficiency of importance sampling with \(q_{t}\) as proposal distribution._

## 8 Experiments

We first illustrate that Hessian trace doesn't vanish for overparameterized network and our analysis induces an efficient estimation of this value. Next, we verify our theoretical finding by comparing the dynamics of an overparameterized network in function space and parameter space. Finally, we demonstrate the interpolation of sampling and optimization.

### Non-Diminishing Hessian Trace and Efficient Estimation

We consider a three hidden layers feedforward network with \(2048\) as hidden layer width and \(\) as nonlinear function.

The synthetic dataset is constructed on a unit circle. We choose \(20\) values \(\) uniformly distributed between \(0\) to \(\) and we let 2-dimensional input to the network be \(s()=[()()]\).

For each input \(s()\), we are concerned about Hessian trace \((_{}_{}y_{d}(,s()))\). Fast Hessian trace estimation by itself is a challenging problem, due to the high dimensional parameters and complex structure of the network. We follow previous works [36; 6; 78] and adopt Hutchinson's method to estimate the ground truth value of Hessian trace.

On the other hand, our analysis on the dynamics of overparameterized network reveals that certain factors which Hessian trace depends on are insensitive to initialization and training, therefore could be estimated by a fixed value to reduce computation. More details are shown in Appendix F.3. We apply the Hutchinson's method with \(1000\) independent random Rademacher vectors. The \(3\) interval is plotted in the Figure 1. It is shown that our approximation aligns with ground truth well. Based on this approximation, the distribution of Hessian is also calculated in Figure 1.

### Comparing the Dynamics in Parameter Space and Function Space

We use the same toy model as the previous section and set target values to be \(t()=(3)\) and loss to be mean squared error (MSE). For dynamics in parameter space, we run SGD with finite step size \(0.01\) and mini-batch size \(1\). For dynamics in function space, we solve Eqs. (7) to (10) with fixed matrices \(^{(i)}\), \(^{(i)}\) and \(^{(i,j)}\). The result is plotted in Figure 2, where for SGD, the training time is defined as step size times iteration number. The ground truth of Hessian trace on one input is again estimated by Hutchinson's method. We can see from all three kinds of outputs that the function space

Figure 1: Hessian trace for one randomly initialized network (left) and the probability density of Hessian trace at initialization (right).

Figure 2: Comparison of dynamics in parameter space and function space.

dynamics produces similar output as SGD. The error between them is attributed to discretization error for finite step size and finite width fluctuation.

### Interpolation of Optimization and Bayesian Learning

We consider one-shot learning on Fashion-MNIST . We randomly select two classes for binary classification and select one sample for each class as training dataset. We use a single hidden layer network with width being \(1024\) and softplus activation. We use loss \(l(y,t)=1/(1+(yt))\) and surrogate loss \(l^{s}(y,t)=(1+(-yt))\) for gradient descent. For Gibbs measure, we fix \(=180\). The entropy change is approximately evaluated by integrating Eq. (9) with finite step size and fixed \(^{(d)}\).

We train \(10^{5}\) independent network and the results are shown in Figure 3. The shadow region represents \(3\) interval. We first notice that during the training, the expectation of TransBL prediction is stable, yet the variance decreases, indicating the distribution of ensemble is becoming closer to Gibbs measure. This could be verified in middle figure in Figure 3, as the distribution of weights concentrates toward the partition function \( Z_{}-43.6\).

We also show a clear trade-off between sampling efficiency and test accuracy for TransBL with weight clipping, where the parameter \(\) plays a pivotal role in balancing these aspects. When \(\) is set high, the model leans towards a Bayesian posterior framework. Although this setting noticeably reduces the number of effective samples obtained, we observed an enhancement in the model's generalization ability, as depicted in the right figure of Figure 3. Conversely, with a smaller \(\), the model's behavior tends to resemble that of a typical deep ensemble, indicating a high sampling efficiency but worse generalization.

## 9 Conclusion

We study the generalization and sampling efficiency of Bayesian learning with special attention to overparameterized network. We show that both KL divergence, which governs generalization in PAC-Bayesian theory, and \(^{2}\) divergence, which determines sampling efficiency for importance sampling, are related to the change of energy and entropy loss during training. The dynamics of these two quantities in DNNs are studied, leading to a function space picture in infinite width limit. Our study also contributes to the understanding of DNNs Hessian trace, due to its involvement in entropy loss. By considering importance sampling with output distribution from gradient-based optimization as proposal distribution, we bridge the gap between optimization and Bayesian learning problems, and provide a flexible interpolation for accuracy-computation trade-off.