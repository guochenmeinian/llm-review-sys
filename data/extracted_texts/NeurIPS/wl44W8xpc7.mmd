# Learning Infinitesimal Generators of Continuous Symmetries from Data

Gyeonghoon Ko, Hyunsu Kim, Juho Lee

Kim Jaechul Graduate School of AI

KAIST

Seoul, South Korea

{kog, kim.hyunsu, juholee}@kaist.ac.kr

###### Abstract

Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git.

## 1 Introduction

Symmetry is fundamental in many scientific disciplines, crucial for understanding the structure and dynamics of physical systems, datasets, and mathematical models. The ability to uncover and leverage symmetries has become increasingly important in machine learning and scientific research due to its potential to improve model efficiency, generalization, and interpretability. By capturing inherent symmetrical properties, models can learn more compact and informative representations, leading to improved performance in tasks like supervised learning , self-supervised learning , and generative models .

Previous methods for learning symmetry have often relied on the explicit parameterization of group representations based on predefined generators, which can be limited in capturing various symmetries, including transformations that do not align along the generators. For example, when searching for Lie group symmetries in images or physics data, existing methods  parameterize a group action \(g\) as the matrix exponential of a linear combination of linear or affine Lie algebra generators \(L_{i}\) with their learnable coefficients \(w_{i}\) as \(g=(_{i}w_{i}L_{i})\). In the affine transformations of images in\((x_{1},x_{2})\)-coordinates, there are six generators, each corresponding to translation, scaling, and shearing operations with respect to the \(x_{1}\)-axis and \(x_{2}\)-axis. Although there exist some methods that directly learn the generators, they are either bound to the general linear group \(GL(n)\), which cannot account for non-affine or non-linear transformations , or are not guaranteed to find the correct symmetry in real-world image datasets [9; 13].

When searching for symmetries in high-dimensional real-world datasets, we can take advantage of the fact that the data can be interpreted as a function \(f:\), such as images, which are functions from the 2D Euclidean space to the color space. Another notable example of such data is partial differential equations (PDEs), where the data take the form \(:\) and the Lie symmetries are defined as transformations on the space \(\). There have been significant advances in Lie symmetry analysis in recent years, for both academic and industrial purposes, mostly involving extensive symbolic calculations and relying on computer algebra systems . Discovering Lie symmetries of PDEs from data without prior knowledge is an unexplored topic, except for the work of Gabel et al. , which learns the symmetry generators of various PDEs in a supervised learning setup.

In this work, we propose a novel method for learning continuous symmetries, including non-affine transformations, from data without prior knowledge. By modeling one-parameter groups using Neural Ordinary Differential Equation (Neural ODE) , we establish a learnable infinitesimal generator capable of producing a sequence of transformed data through ODE integration. We design an appropriate _validity score_ function that measures how much the transformation violates the invariance to certain criteria defined depending on the target task, and learn the generators by optimizing towards the validity score of the data transformed through ODE integration. For example, in an image classification dataset, we use a pre-trained feature extractor and define the validity score to be the cosine similarity between the features extracted from the original image and the transformed image. For PDEs, the validity score is defined by the numerical errors of the original equations after the transform. The validity scores are chosen based on the characteristics of the target tasks, and designed to be fully differentiable, so that the symmetry can be learned via gradient descent in an end-to-end fashion. We also incorporate two regularizations, orthonormality and Lipschitz loss, which prevent the learned generators from converging to a trivial solution.

Subsequently, we demonstrate that our method indeed discovers the correct symmetries in both image and PDE datasets. To the best of our knowledge, our research is the first to retrieve affine symmetry in the entire space of continuous transformations using the CIFAR-10 classification dataset, as shown in Figure 1. Moreover, our method excels in identifying non-affine symmetries and approximate symmetries in PDE tasks. We further demonstrate that the learned generators can be leveraged to develop automatic augmentation generators, which can be used to produce augmented training data for both image classification tasks and neural operator learning tasks of PDEs . We provide empirical evidence that the models trained with data augmented by our learned generators perform competitively with those trained with traditional closed-form transforms such as Lie point symmetry (LPS) . Moreover, we show that the _approximate symmetries_ discovered by our method, which cannot be found by classical methods, can also boost the performance of the models, especially when the size of the training data is small.

Figure 1: (a) Examples of the vector fields. \(V_{3}\) is a learned symmetry which is approximately a rotation, while \(V_{7}\) is not a symmetry, thus having a high validity score. (b) Transformed CIFAR-10 images using the learned generators. All the vector fields and transformations learned from CIFAR-10 are presented in Figure 8 of Appendix C. (c) Transformation of PDEs (KS equation) with learned symmetries: time translation (t-tsl) and Galilean boost (gal).

## 2 Preliminaries: One-parameter Group

In this section, we present the basic definitions of a one-parameter group, which we use to parameterize the symmetric transformations learned from the data.

Consider an unknown Euclidean domain \(^{n}\) and a smooth vector field \(V:^{n}\). A path \(:I=(a,b)\) satisfying \((s)=V((s))\) for all \(s I\) is a curve that travels around the domain \(\) with a velocity given by the vector field \(V\). Along the curve \(\), a point \(_{0}=(a_{0})\) can be transported to \((a_{0}+s)\) by flowing along the vector field \(V\) by time \(s\). We define the flow \(_{s}^{V}\) by \(_{s}^{V}(_{0})=(a_{0}+s)\) of \(V\) as in Figure 2. This flow is computed by solving an ODE

\[_{s}^{V}()=V()\] (1)

with initial condition \(_{0}^{V}()=\) for all \(\).

The flow \(^{V}\) is governed by an autonomous ODE, i.e., an ODE independent of the temporal variable \(s\). Due to properties of autonomous ODEs, the flow \(^{V}\) exists uniquely and it is smooth in both variables \(s\) and \(\). Assuming a mild condition on \(V\), such as \(V\) extends to a compactly supported vector field \(\) on \(^{n}\), the ODE does not terminate in \(^{n}\) in finite time and hence \(_{s}^{V}\) is defined for all \(s\). In that case, the flow satisfies a group equation

\[_{s_{1}+s_{2}}^{V}()=_{s_{1}}^{V}_{s_{2 }}^{V}()\] (2)

for all \(s_{1},s_{2}\). It means that the flow can be regarded as a group action of \(\) on \(^{n}\), transforming elements of \(^{n}\). For this reason, \(_{s}^{V}\) is also called a _one-parameter group_, and the vector field \(V\) is called an _infinitesimal generator_ of the one-parameter group.

On \(=^{n}\), a constant vector field \(V()=^{n}\) gives rise to a translation \(+s\). For a matrix \(^{n n}\), a vector field \(V()=\) gives rise to an affine transformation \((s)\), where \(\) is the matrix exponentiation. Multiple infinitesimal generators may span a vector space \(\), and if \(\) satisfies some algebraic condition (closure under the Lie bracket), then \(\) forms a _Lie algebra_. Composing the elements of one-parameter groups of elements in \(\) gives rise to a _Lie group_\(G\). The correspondence between \(G\) and \(\) is called Lie group-Lie algebra correspondence.

_Continuous symmetries_ are commonly defined by a Lie group \(G\), acting on some domain and keeping the transformed objects _invariant_ with respect to some criterion. We model symmetries by specifying their infinitesimal generators whose composition of one-parameter groups comprises the symmetries of that domain.

Below, we describe two representative examples that will be discussed extensively in the remainder of the paper: images (interpreted as functions on 2D planes) and PDEs.

### Images and Their Symmetries

Consider a rescaled image of the form \(f:=[-1,1]^{2}^{2}=^{3} ^{3}\). The affine transformations on \(^{2}\) have the form \(=(x_{1},x_{2})+\) for a matrix \(^{2 2}\) and a vector \(^{2}\). The Affine transformations form the 6-dimensional Affine group \((2)\), and it has a corresponding 6-dimensional Lie algebra having a basis \(\{L_{1},,L_{6}\}\) given as in Table 1. The symmetries of images are often exploited as a data augmentation strategy for learning image classifiers, under an assumption that the transforms do not alter the identity or semantics of the images to be classified.

  
**Generator** & **Expression** & **One-Parameter Group** & **Description** \\  \(L_{1}\) & \((1,0)\) & \(+s(1,0)\) & translation in \(x_{1}\)-axis \\ \(L_{2}\) & \((0,1)\) & \(+s(0,1)\) & translation in \(x_{2}\)-axis \\ \(L_{3}\) & \((_{1},0)\) & \(+(_{0}^{V})\) & scaling of \(x_{1}\)-axis \\ \(L_{4}\) & \((0,x_{2})\) & \(()^{2}\) & scaling of \(x_{1}\)-axis \\ \(L_{5}\) & \((x_{2},0)\) & \(()^{2}\) & shear parallel to \(x_{1}\)-axis \\ \(L_{6}\) & \((0,x_{1})\) & \(()^{2}\) & shear parallel to \(x_{2}\)-axis \\  \(L_{3}+L_{4}\) & \((x_{1},x_{2})\) & \(()^{2}\) & uniform scaling \\ \(L_{6}-L_{5}\) & \((-2x_{2},x_{1})\) & \(()^{2}\) & rotation \\   

Table 1: Infinitesimal generators of the Affine group \((2)\). The set of six generators \(\{L_{1},,L_{6}\}\) forms a basis of the corresponding Lie algebra.

### PDEs and Their Symmetries

Given an \(n\)-dimensional independent variable \(=(x_{1},,x_{n})^{n}\) and an \(m\)-dimensional dependent variable \(=()=(u_{1}(),,u_{m}()) ^{m}\), we denote by \(^{(i)}\) the collection of all \(i\)-th partial derivatives of \(\) with respect to \(\). A _partial differential equation_\(\) on \(()\) of order \(k\) is defined by a set of algebraic equations \((,,^{(1)},,^{(k)})=0\) involving all the variables and their partial derivatives. For example, two scalar independent variables \(x,t\) and one scalar dependent variable \(u(x,t)\) governed by equation \(=u_{t}+uu_{x}+u_{xxx}=0\) gives the 1-dimensional Korteweg-de Vries (KdV) equation, where we denote partials using subscripts, e.g. \(u_{t}=\) and \(u_{xxx}=u}{ x^{3}}\). The KdV equation is commonly used to model the dynamics of solitons, e.g. shallow water waves . The KdV equation described above is an example of 1-dimensional scalar-valued evolution equation. Such an equation takes the form \(u=u(x,t)\) with its governing equation of the form

\[u_{t}=F(x,t,u,u_{x},u_{xx},u_{xxx},)\] (3)

for some function \(F\). In this paper, we only deal with 1D scalar-valued evolution equation on a fixed periodic domain \(x[0,L]\).

Continuous symmetries of PDEs are commonly parametrized by a one-parameter group on \(\). Denote \((,)=([,],[,])\) an infinitesimal generator defined on \(\). Then the PDE \(\) possesses the infinitesimal generator of symmetry \((,)\) if the equation is still satisfied after transforming both the independent variable \(\) and the dependent variable \(\)[27; 5; 26]. Symmetries of PDEs are categorized by how the generators \((,)\) depend on \((,)\). The symmetry is a Lie point symmetry (LPS) if the value of \((,)\) at each point \((,())\) depends only on the point value \((,())\) itself. If \((,)\) also depends on the derivatives \(^{(1)},,^{(k)}\) at that point, it is called a Lie-Backlund symmetry or generalized symmetry. If \((,)\) depends on integrals of \(\), then it is called a nonlocal symmetry. Finding an LPS of a PDE \(\) can be done algorithmically under some mild assumptions on \(\). However, there is no general recipe of finding Lie-Backlund symmetries or nonlocal symmetries, and discovering such symmetries remains an active area of research.

## 3 Related Work

Symmetry discovery.Approaches to learning symmetries can be categorized by addressing two questions: (a) _where do they search for symmetries_, and (b) _what are they aiming to learn_. One line of research aims to learn ranges, focusing on determining the ranges of transformation scales that enhance learning when employed as augmentation techniques. For example, Benton et al.  learns transformation ranges of predefined transformations by treating them as learnable parameters and backpropagating through differentiable transformations.

Another line of research aim to learn subgroups of bigger candidate groups, typically a linear group \(GL(n)\) or an affine group \((n)\). For example, Desai et al.  use the Generative Adversarial Network (GAN) to search for symmetries, with the generator transforming data by group elements sampled from the candidate group and the discriminator verifying whether the transformed data still lies in the data distribution. Similarly, Yang et al.  employ the GAN approach, but generator of GAN models infinitesimal generators instead of the subgroup itself, and learns affine symmetries such as rotation of images and Lorentz symmetry of high-energy particles. As an alternative, Moskalev et al.  proposed an idea of extracting symmetries from learned neural network by differentiating through it, and retrieved 2D rotation in the linear group using the rotation MNIST dataset.

Finally, _learning symmetries with minimal assumption_, i.e. without assuming the infinitesimal generators are linear or affine, is an area of large interest. An early attempt of Rao & Ruderman  models infinitesimal generator by a learnable matrix from the pixel space to the pixel space, and learn 2D rotation by solving a task that compares original images and rotated ones, where the images are \(5 5\) random pixels. Sohl-Dickstein et al.  takes the similar approach with eigen-decomposing the learnable matrix. Dehmamy et al.  builds a convolution operation whose kernel encodes learnable infinitesimal generators, and retrieved 2D rotation from random \(7 7\) images by comparing original and transformed ones, and Yang et al.  uses an autoencoder to simplify nonlinear symmetries into linear ones. Our work closely aligns with Liu & Tegmark  and Forestano et al. , which model one-parameter groups by an MLP and learn the symmetries from an invariant scalar quantity. To thebest of our knowledge, learning correct symmetries with minimal assumption was only achieved with toy datasets, far from real-world datasets such as CIFAR-10.

Utilizing symmetries in deep learning.An effective method for leveraging symmetries in deep learning is data augmentation . In the image domain, there are numerous augmentation techniques available , most of which are based on geometric properties of images. Although data augmentation techniques have been primarily explored in the context of images, recent studies by [4; 23] have demonstrated that symmetries can also be used for augmenting data in the training of neural PDE solvers. In addition to data augmentation, some approaches involve designing new neural network architectures that inherently reflect the group symmetries of the input data . Wang et al.  applied a similar strategy within the PDE domain.

## 4 Learning Continuous Symmetries with One-Parameter Groups

### Training Process

Given a learning task with a dataset \(\) in an underlying space \(=\{f|f:\}\), we aim to model symmetry by a one-parameter group \(_{s}\) acting on \(\), as explained in SS 4.3. We define a continuous symmetry by stating that \(\) is a symmetry of this task if there exists some \(>0\) such that for any data point \(f\) and transformation scale \(s[-,]\), the transformed data point \(_{s}(f)\) remains valid for this task. We assume the existence of a differentiable _validity score_\(S(_{s},f)\), such that \(_{s}(f)\) is valid if \(S(_{s},f)<C\) for a certain threshold \(C\). Then, a one-parameter group \(\) is a symmetry of the task if \(S(_{s},f)<C\) for all \(f\).

The validity score depends on the nature of the target task, though no strict criterion exists. As long as it is differentiable and the valid data aids learning, it is considered acceptable. For instance, we can define the validity based on a negative log-likelihood of a probabilistic model. In SS 4.2, we discuss the validity scores to be used for image and PDE data.

Once a validity score is defined, we learn a symmetry \(^{*}\) by minimizing the validity scores of transformed data,

\[^{*}=*{arg\,min}_{}_{f,s([-,])}[S(_{s},f)],\] (4)

where the \(*{arg\,min}\) is taken over the entire class of smooth one-parameter groups. Since the learning is performed in function space, we appropriately constrain the function space using a regularizer, as described in SS 4.4. Once symmetries are learned, they reveal the symmetrical properties of the target task, which can then be exploited to augment the training data.

### Task-specific Definition of Validity Score \(S\)

Images.In image-related tasks, we define a validity score using a pre-trained neural network. Let \(\) be an image classification dataset consisting of data of the form \((f,y)\), where \(f\) is an image and \(y\) is a label. Also let \(H_{} H_{}:\) be a learned neural network, where we denote by \(H_{}:^{k}\) the feature extractor and \(H_{}:^{k}\) the classifier. We define the validity score \(S(_{s},f)\) as the cosine similarity between the features before and after the transformation:

\[S(_{s},f)=*{sim}(H_{}(_{s}(f) ),H_{}(f)),\] (5)

where \(*{sim}\) is the cosine similarity defined as \(*{sim}(_{1},_{2})=_{1},_{2}|}{\|_{1}\|\|_{2}\|}\) for all \(_{1},_{2}^{k}\{\}\).

PDEs.Let \(()\) be a solution of a given PDE \(\), discretized on a rectangular grid \(_{}=\{_{i}\}_{i=1}^{N_{}}\). For a transformed data \(_{s}()\), we measure the violation of the equality \(=0\) to assess whether the transformed data is still a valid solution. Using an appropriate numerical differentiation method, we directly compute the value of the PDE, denoted as \((_{s}())\), which represents the error of \(_{s}()\)

Figure 3: Process of learning symmetry.

as a solution of \(\), taking a value \((_{s}())_{i}\) at grid point \(_{i}\). The validity score is defined by the summation of all PDE errors across the grid points:

\[S(_{s},)=_{i}|(_{s}())_{i} |.\] (6)

For example, for a solution \(u(x)\) of the 1D KdV equation, we examine whether the transformed solution \(=_{s}(u)\) satisfies \(_{t}+_{x}+_{xxx}=0\) where the partials are computed using a numerical differentiation method.

### Parametrization of One-Parameter Groups using Neural ODE

On a Euclidean domain \(^{n}\), we model an infinitesimal generator with an MLP \(_{}:^{n}^ {n}\). The infinitesimal generator \(_{}\) gives rise to a one-parameter group \(_{s}^{_{}}\). We sample a transformation scale \(([-,])\) for a predefined hyperparameter \(_{>0}\). To transform a point \(x\) along this one-parameter group by an amount \( 0\), we use a numerical ODE solver to solve the ODE for \(:[0,]\) satisfying

\[^{}(s)=_{}((s)), s[0, ],(0)=x\] (7)

and obtain a transformed data point \(}=_{}^{_{}}()=()\). If \(<0\), we compute \(_{}^{_{}}()=_{-}^{-_{}}()\) by integrating \(-_{}\) instead of \(_{}\) using the ODE solver. We can backpropagate through the numerical ODE solver using the adjoint method  to learn \(\).

Let \(f:\) be a data point on a domain \(\). As \(\) is a space of functions, naively modeling symmetry on \(\) may ignore the geometry implied in the input space \(\). Instead, we define two transformations: \(_{}\) on \(\) and \(_{}\) on \(\), and induce a transformation of \(f\) by

\[(_{}(f))(x)=f(_{}^{-1}(x)),( _{}(f))(x)=_{}(f(x)),\] (8)

where we abuse notation and write the transformed function as \(_{}(f)\) and \(_{}(f)\). For an image represented as a discretized function \(f:\) from \(=[-1,1]^{2}\) and \(=^{3}\), \(_{}\) corresponds to spatial transformations such as translation or rotation, and \(_{}\) corresponds to color space transformations. For a PDE, a 1D scalar-valued evolution equation on a fixed periodic domain takes the form \(u(x,t)=\) with \((x,t)[0,L][0,T]=^{2}\), and we parameterize an infinitesimal generator on a product space \(^{3}\) by an MLP. Then, a transformation on \((x,t,u)\) induces a transformation on the solution of the PDE \(u(x,t)\).

### Objective Functions

Symmetry loss.Let \(N_{}\) be the number of symmetries to be learned. Let \((_{}^{(a)})_{a=1}^{N_{}}\) be the infinitesimal generators computed from a single MLP. For each \(a\{1,,N_{}\}\), we sample a transformation scale \(s_{a}([-,])\) to transform \(f\) via numerical integration. The parameter \(\) is optimized by minimizing the average validity score over the training data,

\[_{}()=_{a=1}^{N_{}}_{f,s_{a}([-,])}[S _{s_{a}}^{_{}^{(a)}},f].\] (9)

Orthonormality loss.Learning only with the symmetry loss may result in trivial solutions such as the zero vector field or the same vector field repeated in multiple slots. To prevent this, we introduce the orthonormality loss to regularize the model towards learning orthonomral vector fields. Specifically, given two vector fields \(V_{1},V_{2}:^{n}\), we define an inner product as,

\[ V_{1},V_{2}=()}_{} ()(V_{1}() V_{2}())d_{}|}_{_{i}_{}} (_{i})(V_{1}(_{i}) V_{2}(_{i})),\] (10)

with a suitable weight function \((x):\) and a discretized grid \(_{}\) of \(\) of size \(|_{}|\). Given this definition, we first normalize each generator by its norm to ensure \(\|_{}^{(a)}\|^{2}=1\). Then we compute the orthonormality loss as,

\[_{}()=_{1 a<b N_{}} (_{}^{(a)}),_{}^{(b)} ,\] (11)where \(()\) denotes the stop-gradient operation to ensure that the constraint \(_{}^{(a)},_{}^{(b)}=0\) only affects the latter slot (\(b\)). By doing this, if the true number of symmetries \(N_{}^{*}\) is less than or equal to the assumed number of symmetries \(N_{}\), the learned symmetries will be aligned in the first \(N_{}^{*}\) slots.

Lipschitz loss.We further introduce inductive biases to the infinitesimal generators we aim to learn. For instance, an infinitesimal generator moving only a single pixel near the boundary by a large scale would be undesirable. This idea can be implemented using Lipschitz continuity. For a grid point \(_{i}_{}\) and its neighboring point \(_{j}(_{i})_{}\), we expect the vector field \(V\) to satisfy the Lipschitz condition,

\[(V;_{i},_{j})<(V; _{i},_{j})=_{i})-V(_{j})\|}{\|_{i}-_{j }\|}.\] (12)

To regularize the model toward the Lipschitz condition, we introduce the Lipshitz loss,

\[_{}()=_{a=1}^{N_{}}_{ _{i}_{},_{j}(_{i}) }((_{}^{(a)};_{i},_{j})-,0).\] (13)

Total loss and loss-scale-independent learning.We jointly minimize the three loss functions with suitable weights \(w_{},w_{},w_{}>0\) and learn the weights \(\) of MLP using a stochastic gradient descent:

\[^{*}=*{arg\,min}_{}w_{}_{}()+w_{}_{}()+w_{}_{}().\] (14)

To minimize the computational burden of hyperparameter tuning, we ensure that all the loss terms have a _natural scale_, i.e. a dimensionless scale independent of the context. For example, when penalizing the inner product in Equation 11, we apply \(\) to the normalized inner product to ensure the loss term lies in \([0,/2)\). Similarly, the scale of the PDE validity score \(S(_{s},)\) in Equation 9 depends on the scale of the data \(\). When penalizing it, we apply the log function so that the gradients are scaled automatically as \(_{}(S(_{s},))=_{}S( _{s},)/S(_{s},)\).

Here we describe the generic training process, but the actual implementation requires non-trivial task-specific designs, such as the choice of the weighting function \(w()\) or the method for locating the transformed data on the target grid. We defer these details for image and PDE tasks to Appendix A.

### Comparison With Other Methods

Here, we compare our method with other recent symmetry discovery methods. The differences mainly arise from (a) what they aim to learn (e.g., transformation scales or subgroups from a larger group) and (b) their assumptions about prior knowledge (e.g., complete, partial, or no knowledge of symmetry generators). Another important distinction is the viewpoint on symmetry: some methods learn symmetries that raw datasets inherently possess (implicit), while others learn symmetries from datasets explicitly designed to carry such symmetries (explicit).

Some recent symmetry discovery works are listed in Table 2. We emphasize that our method excels in two key aspects: (a) our learning method reduces infinitely many degrees of freedom, (b) our method works with high-dimensional real-world datasets. For example, while LieGAN  and LieGG  reduce a 6-dim space (affine) to a 3-dim space (translation and rotation) in an image dataset, ours reduces an \(\)-dim space to a finite one. L-conv  also does not assume any prior knowledge, but it is limited in finding rotation in a toy task, where it learns rotation angles of rotated images by comparing them with the original ones, which are 7x7 random pixel images.

## 5 Experiments

### Images

We use images of size \(32 32\) from the CIFAR-10 classification task. Since our method does not model discrete symmetry, we use horizontal flip with 50% probability by default. We train a ResNet-18 model, which will be used as the feature extractor \(H_{}\) in Equation 5. The weight function on the pixels is computed as explained in Appendix A.1. We expect to find 6 affine generators and we use an MLP modeling 10 vector fields in the pixel space \([-1,1]^{2}^{2}\), expecting the first six learned vector fields to be the affine generators. We learn the Equation 14 using stochastic gradient descent with \(w_{}=1\) and \(w_{},w_{}=10\). The parameter \(\), which controls the scale of transformation, is set to \(=0.4\), and the Lipschitz threshold \(\) is set to \(=0.5\). Other details are described in Appendix B.1. We conducted three trials with random initializations and report the full results in Appendix C.1. Furthermore, we also learn symmetries in the _color space_, and their results are shown in Appendix G.

Learned symmetries.Since we expect to learn affine symmetries, we compare the results with the affine basis \(\{L_{1},,L_{6}\}\) defined in Table 1. We compute the inner products \( V,L_{i}\) of the learned vector field \(V\) with \(L_{i}\) for \(i=1,,6\) to measure how much the learned vector fields contain the affine basis and measure the affine-ness of vector field by \((V)^{2}=_{i=1}^{6} V,L_{i}^{2}\).

In all experiments, we successfully retrieve six linearly independent affine generators in the first six slots. Figure 3(a) shows that the learned generators are orthogonal to each other, as desired. Figure 3(b) shows the inner product between the learned generators and the affine generators. Since the affine-ness measure of the first 6 learned generators in Figure 3(c) is almost close to \(1\), we can read out the affine components in Figure 3(b) and say that e.g., \(V_{1}(0,-0.98+0.12x_{2})\). Notably, two translation generators are found in the first two slots, indicating that the two translations are _the most invariant_ one-parameter group among the entire class of one-parameter groups on the pixel space. After the two translation generators, four affine generators are learned, indicating that affine transformations are _the next most invariant_ transformations. In particular, the third and fourth generators are close to the rotation generator and the scaling generator, respectively. The remaining four generators fix pixels close to the center and transform boundary pixels by a large magnitude.

    & **Augerino** & **LieGAN** & **LieGG** & **L-conv** & **Forestano et al.** & **Ours** \\ 
**Symmetry** & completely & partially & partially & completely & completely & completely \\
**generators** & known & known (affine) & known (affine) & unknown & unknown & unknown \\ 
**Learn what?** & transformation & symmetry & symmetry & symmetry & symmetry & symmetry & symmetry \\  & scales & (rotation / & generator & generator & generator & generator \\  & Lorentz) & & (rotation) & (rotation) & (in low-dim task) & (affine) \\ 
**Verified** & raw & rotation & random & & & raw \\
**with what?** & CIFAR-10 & MNIST / & rotation & \(7 7\) pixel & toy data & CIFAR-10 \\  & & Top tagging & MNIST & image & (dim \( 10\)) & \& PDEs \\ 
**Implicit or** & implicit & explicit & explicit & explicit & explicit & implicit \\ 
**How?** & optimize & compare & extracts from & compare & extracts from & extracts from \\  & while training & fake/true data & learned NN & rotated and & invariant oracle & validity score \\  & downstream & in GAN & using Lie & original & using Lie & using ODE \\  & task & framework & derivative & images & derivative & integration \\   

Table 2: Comparison with other symmetry discovery methods.

Figure 4: (a) Self inner-products of the learned generators. (b) Inner product comparison of the learned generators with the affine generators. (c) Affine-ness of learned generators.

[MISSING_PAGE_FAIL:9]

Interestingly, for the Burgers' equation and the cKdV equation, we additionally found the \(u\)-axis rescaling operation \((0,0,u)\) and the time translation \((0,1,0)\) respectively. Applying the \(u\)-axis rescaling \(u cu\) for \(c 1\) to the Burgers' equation \(u_{t}+uu_{x}- u_{x}x=0\) gives \(cu_{t}+c^{2}uu_{x}- cu_{x}x=c(c-1)uu_{x}\), leaving only the \(uu_{x}\) term. The \(uu_{x}\) term in Burgers' is called the _convection term_, and it is approximately zero in most region and spikes in some small region. Similarly, the time translation \(t t+c\) for \(c 0\) fixes the first three terms in the cKdV equation \(u_{t}+uu_{x}+u_{xxx}+u/(2(t+1))=0\) and only changes the last term \(u/(2(t+1))\) by a negligible amount. These are not LPSs of the given equations, but the error of the PDE after transformation is smaller than the error of the numerical differentiation method. These are _approximate symmetries_ (AS), and the theory of AS is also of great interest in the symmetry analysis of PDEs [2; 1].

Augmentation results.We use the learned symmetries as data augmentation and train Fourier Neural Operators (FNOs). The detailed experiment setting is described in Appendix B.2. Since FNOs are extremely sensitive to numerical error, we employ Whittaker-Shannon interpolation, explained in Appendix D.2, to resample the transformed results. The results are depicted in Figure 6. In all cases, data augmentation using the learned symmetries improve the performance, almost close to the results using the ground truth symmetries. The detailed results are in Appendix C.2. Additionally, we verify that the approximate symmetry of cKdV is also beneficial to training, as shown in Table 6, especially when the numbers of data points is low, proving the effectiveness of symmetries extracted from data.

Ablations.Additional ablation studies on numerical methods, such as numerical differentiation and interpolation, and hyperparameter sensitivity are conducted, and their results are presented in Appendices E and F.

## 6 Conclusion

We have introduced a novel method for learning continuous symmetries, including non-affine transformations, from data without prior knowledge. By leveraging Neural ODE, our approach models one-parameter groups to generate a sequence of transformations, guided by a task-specific validity score function. This approach captures both affine and non-affine symmetries in image and PDE datasets, enhancing automatic data augmentation in image classification and neural operator learning for PDEs. The learned generators produce augmented training data that improve model performance, particularly with limited training data.

Limitation.However, despite its flexibility, our method requires careful selection of numerical methods, such as numerical differentiation and interpolation, to ensure stable training and the ODE integration can be computationally large for augmentation generation compared to other augmentation methods. While we focus on image symmetries and LPSs of PDEs, the method could potentially model other symmetries and domains with proper validity scores, suggesting future applications in learning complex symmetries, including conditional and non-local symmetries, in various data types.

  
**\# Data** & **None** & **LPS** & **AS** & **LPS+AS** \\  \(2^{7}\) & \((3.70 0.09) 10^{-6}\) & \((3.49 1.06) 10^{-6}\) & \((3.20 0.94) 10^{-6}\) & \((2.66 0.36) 10^{-6}\) \\ \(2^{5}\) & \((7.90 1.21) 10^{-4}\) & \((5.90 0.17) 10^{-4}\) & \((4.45 0.15) 10^{-4}\) & \((3.70 0.15) 10^{-4}\) \\   

Table 6: Test NMSE comparison of augmentation using LPS and AS in FNO learning for cKdV.

Figure 6: Comparison of augmentation performances using the ground truth symmetries and the learned symmetries with various numbers of data. The symbol \(\) stands for no-augmentation.