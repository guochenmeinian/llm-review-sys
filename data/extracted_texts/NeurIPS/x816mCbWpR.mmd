# Recasting Continual Learning as Sequence Modeling

Soochan Lee

Seoul National University

soochan.lee@vision.snu.ac.kr &Jaehyeon Son

Seoul National University

sjh9876@snu.ac.kr &Gunhee Kim

Seoul National University

gunhee@snu.ac.kr

###### Abstract

In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.1

## 1 Introduction

Continual learning (CL) refers to the ability to learn from a non-stationary stream of data. A CL data stream is typically a series of distinct tasks, and the model cannot access previous data while learning the current one. For example, in CL benchmarks based on image classification, each task typically consists of a small subset of classes. Since the model's access to the past data is restricted, standard training with stochastic gradient descent (SGD) consistently overwrites previously learned knowledge, which often leads to catastrophic forgetting. As _learning_ usually implies SGD updates in the deep learning literature, most CL approaches try to combat catastrophic forgetting while relying on SGD to update model parameters. However, using SGD is generally not a necessity of CL; one may choose any update rule other than SGD.

In this regard, we point out that CL is inherently a sequence modeling problem. The basic structure of CL, which trains a model on a training stream \(((x_{1},y_{1}),,(x_{T},y_{T}))\) and evaluates it on a test set \(\{(x_{n},y_{n})\}_{n}\), can be considered as predicting the next token \(y_{n}\) of the sequence \((x_{1},y_{1},,x_{T},y_{T},x_{n})\). From this perspective, the CL process and the test-time inference are a sequence model's forward pass, with no SGD update. Instead, we can train the sequence model at the meta-level with a set of CL episodes. This meta-continual learning (MCL) framework is technically equivalent to conventional sequence model training, where a sequence model is trained on a set of sequences. Therefore, we argue that any recent and future advances in sequence models can be directly applied to MCL research. As a precaution, our approach should not be confused with continually learning a sequence model , which is a conventional CL setting (not MCL) where each datapoint \(x\) is a sequence.

As a particular instance of our new framework, we test Transformers , which are considered as the current state-of-the-art sequence model. In the past several years, Transformers have revolutionized various sequence modeling tasks such as language modeling , translation , speech recognition/generation , and protein analysis , to name a few. When Transformer-based language models (LMs) are scaled up and trained with a massive collection of text data, theyexhibit an interesting ability called _in-context learning_. Within a forward pass, they can learn new patterns and knowledge from the previous tokens in the current context and apply them to solve new tasks. Our framework maximizes this in-context learning ability of sequence models through meta-training and applies it as a solution to CL.

An immediate question may come to one's mind: if Transformer is a parallel architecture that can attend to all the training data at test time, is this even a valid CL setting? Borrowing the words of Katharopoulos et al. , Transformers with causal attention can be considered as recurrent neural networks (RNNs; 13). We can think of a Transformer's attention keys and values as the internal state of a recurrent model. When a new token comes in, this state is updated to include new key-value pairs. At test time, the Transformer queries the key-value pairs in the internal state, not the raw training data. Therefore, with proper implementation, using Transformers does not violate the assumptions of CL.

One drawback of standard Transformers is that the cost of handling each example grows linearly along with the internal state. Therefore, the computational complexity of handling an episode of length \(T\) becomes \((T^{2})\). Fortunately, there is a considerable amount of work on _efficient Transformers_, which have sub-quadratic complexity. Especially, the efficient Transformers with linear \((T)\) complexity maintain a constant computational cost to handle each token. In this work, we test Linear Transformer  and Performer  as such examples.

By conducting extensive experiments on seven diverse benchmarks covering both classification and regression tasks, we demonstrate that Transformers, including the efficient variants, have great potential as a general MCL approach, especially in large-data regimes. Furthermore, it is important to note that our approach is not limited to Transformers. By offering a novel perspective on MCL, we open up the possibility for other sequence models, including those that may be developed in the future, to be applied to this challenging problem.

Lastly, our approach offers a unique advantage in terms of biological plausibility. Despite its universality in modern deep learning, backpropagation has long been criticized for being biologically implausible [36; 26; 21; 12]. This is due to the lack of convincing proof to suggest that biological neurons perform the complex backpropagation process of (i) storing their activations during the forward pass, (ii) computing the error derivatives, and (iii) transmitting them backward. Our formulation is free from such criticism since the learning process is a simple forward pass of the sequence model. Only the meta-level optimization, which would have been performed by the evolutionary process in nature, is replaced by more efficient backpropagation.

## 2 Background and Related Work

### Meta-Continual Learning (MCL)

Due to the additional layer of complexity from meta-level optimization, compounded by the absence of a standardized lexicon, there exists a high likelihood of confusion in discerning the settings of MCL and related domains. Due to the limited space, this section summarizes only the most relevant works. For a comprehensive comparison of MCL with other fields, such as continual meta-learning, we recommend referring to .

**Continual Learning (CL).** A continual learning episode \(\) consists of a _stream_ of training data \(^{}=((x_{1},y_{1}),,(x_{T},y_{T}))\) and a test _set_\(^{}=\{(x_{1},y_{1}),,(x_{N},y_{N})\}\) where \(x_{*}\) and \(y_{*}\) are the input and target variables. The data in the training stream can only be accessed one at a time, and it is not possible to access previously seen data. If we assume a buffer that temporarily holds each task, this formulation can also cover the offline CL settings where the stream is segmented by tasks rather than examples. The training stream is generally assumed to be a concatenation of \(K\)_task_ streams, i.e., \(^{}=^{}_{1} ^{}_{K}\), each of which is a stationary sequence of data sampled from a distinct distribution. The test set is the union of the task-specific test sets: \(^{}=^{}_{1} ^{}_{K}\). The objective of the CL algorithm is to continually learn a good model \(f_{}:\) from the stream by optimizing parameter \(\). For example, in the popular CL benchmarks of image classification, \(f_{}\) is a classifier that takes an image \(x\) as input to output its class \(y\).

**Meta-Continual Learning (MCL).** Rather than manually crafting a CL algorithm, MCL aims to learn how to continually learn. More formally, we introduce the concept of _continual learner_\(H_{}:()()()\), which is a functional that takes a data point \((x_{t},y_{t})\) and an old model \(f_{_{t-1}}\) as inputs and produces an updated model \(f_{_{t}}\). For convenience, we additionally define another functional \(G_{}:()^{*}( )\) that sequentially processes a training stream as input and outputs a trained model: \(f_{_{t}}=G_{}(((x_{1},y_{1}),,(x_{t},y_{t})))=H_{}((x_{t},y_ {t}),f_{_{t-1}})\). This sequential refinement of the model parameter \(\) is commonly referred to as the inner loop, which is the training phase of standard CL. MCL adds another layer of optimization for tuning the meta-parameter \(\) of the learner, which is often called the outer loop. The contents of \(\) can vary widely depending on the method, e.g., the initial parameters of a model, a meta-learned encoder, etc. Unlike traditional CL settings where a model is trained on a single training data stream and evaluated on a test set, an MCL setting consists of multiple CL episodes split into the meta-training set and the meta-test set, as shown in Fig. 1a. The episodes of the two meta-splits are assumed to be drawn from the same distribution. Generally, they are created by first randomly splitting available tasks into two groups, where each episode is built as a random combination of the tasks from the group corresponding to its meta-split. Since there are no overlapping tasks between the meta-splits, one cannot achieve a high meta-test score by simply memorizing all tasks in meta-training. For each episode \(\), a model is produced by the learner, i.e., \(f_{_{T}}=G_{}(^{})\) and evaluated on the test set \(^{}\) to produce the meta-loss \((f_{_{T}},^{})\). Then \(\) is updated by _meta_-gradient descent, i.e., SGD with \(_{}(f_{_{T}},^{})\), to reduce this meta-loss during the meta-training phase. During the meta-test phase, the learner \(H_{}\) is evaluated on multiple CL episodes in the meta-test set. While this description represents the most basic form of MCL that our work is based on, meta-training can deviate from this scheme as long as the integrity of the meta-test is maintained, e.g., [16; 4].

**Prior Works in MCL.** Online aware Meta-Learning (OML; 16) splits a model into an encoder and a small prediction network on top of it. In the inner loop, only the prediction network is updated by SGD, while the encoder remains frozen. In the outer loop, the encoder and the initial parameters of the prediction network are optimized. A Neuromodulated Meta-Learning algorithm (ANML; 4) has an additional meta-learned component named neuromodulatory network with the same architecture as the encoder. Its output is passed through the sigmoid and multiplied to the encoder's output, gating some features. Meanwhile, there are several MCL approaches specialized to classification. Initially introduced as a few-shot learning method, Prototypical Network (PN; 38) has a meta-trained encoder and computes the average embedding, i.e., prototype, for each class during training. By computing the average embeddings online, PN can be directly used in MCL settings . Generative MCL (GeMCL; 3) extends PN by (i) additionally modeling a diagonal covariance matrix of the embeddings for each class and (ii) adopting a Bayesian formulation.

### Transformers

At the core of Transformers is the self-attention mechanism, which interchanges information among different positions within a sequence. In self-attention, each element in the input sequence of length \(T\) is linearly projected to produce \(d\)-dimensional queries, keys, and values, each represented by \(,,^{T d}\). The attention matrix is computed as \((^{}/)\) where \(\) is applied row-wise. It is then multiplied to the value matrix to produce the output of the layer \(=(^{}/)\). If self-attention is computed in this manner, it is more precisely called bi-directional self-attention since every token can attend to all the other tokens in both directions.

While it is used for encoding a sequence in parallel, uni-directional, or causal self-attention is generally used for decoding a sequence one token at a time. In causal attention, each token's output depends only on the preceding tokens, allowing new input tokens to be added without recomputing the existing embeddings. For efficient parallel training, this is typically implemented using a causal attention mask that zeros out the upper triangular elements in the attention matrix. Transformers that only have causal attention layers are called decoder-only Transformers  which have become the backbone of many large language models [28; 29; 6]. In addition to decoding, decoder-only Transformers are also suitable for encoding a streaming sequence where the tokens become available one at a time. This property makes them a perfect fit for our use case in MCL settings.

**Efficient Transformers.** One drawback of Transformers is their quadratic computational cost that arises from computing the \(T T\) attention matrix. The computational complexity of \((T^{2})\) can be a severe bottleneck in handling long sequences. Therefore, there has been extensive research on more efficient variants of Transformers. In the following, we describe the kernel-based approaches that we test in our experiments. These models have \((T)\) complexity and can be applied to decoding. Refer to  for a more comprehensive review of efficient Transformers.

**Kernel-Based Efficient Transformers (KETs).** It approximates the attention matrix as the product of \(^{}=()\) and \(^{}=()\) where \(:^{d}^{r}\) is a non-linear kernel function that transforms each row of \(\) and \(\) into an \(r\)-dimensional feature. The kernel function is meticulously designed such that \((^{})^{-1}(^{} ^{})\) where \(D=(^{}(^{}_{T}))\) is the normalizing term that ensures each row in the approximated attention matrix to sum to one. Through this approximation, we can change the computation order to compute \(^{}\) first: \((^{})^{-1}^{ }(^{})\). The overall complexity becomes linear to the sequence length, i.e., \((T)\). Linear Transformer  and Performer  are examples of KETs and differ only in their choice of kernel function. The kernel function of Linear Transformer is defined as \(()=()+_{d}\) where \(\) is the exponential linear unit (ELU; 9) applied element-wise. Performer  uses \(()=(-\|\|^{2}/2_{r})\) where the rows of \(^{r d}\) are orthogonal random vectors.

**Decoding with KETs.** In theory, the causal attention of KET can be computed by (i) computing the approximate attention matrix \(^{-1}^{}^{}\), (ii) applying the causal mask, and (iii) multiplying \(\). However, this naive process requires \((T^{2})\) computation, and it necessitates all tokens at once as input. Fortunately, an alternative method exists to sequentially compute the same output with \((T)\) complexity [17; 8]. Here we represent a vector corresponding to a row of the matrix as the lowercase letter, e.g., \(=[_{1};;_{T}]^{}\). At each time step \(t\), the new \(t\)-th input embedding is linearly projected to produce the query \(_{t}\), key \(_{t}\), and value \(_{t}\). The recurrent internal state at time \(t\) holds the matrix \(_{t}^{}_{t}=_{t^{}=1}^{t}(_{t^ {}})_{t}^{}\) and the sum of the key features \(_{t}=_{t^{}=1}^{t}(_{t^{}})\). Note that both can be computed recurrently: \(_{t}^{}_{t}=_{t-1}^{} _{t-1}+(_{t})_{t}^{}\) and \(_{t}=_{t-1}+(_{t})\). The attention output at time \(t\) is computed as \(_{t}=(_{t}^{}_{t})^{}(_ {t})/d_{t}^{}(_{t})\).

## 3 Continual Learning as Sequence Modeling

Many existing MCL (and also CL) approaches rely heavily on SGD to update the model [16; 4] as summarized in Fig. 0(b) and Alg. 1. When each data point \((x_{t},y_{t})\) is made available, the model prediction \(_{t}=f_{_{t-1}}(x_{t-1})\) is compared with \(y_{t}\) to produce the loss. The gradient of the loss w.r.t. the parameters \(_{t-1}\) is fed into an SGD-based update rule, which sometimes accompanies additional treatments to prevent forgetting, to produce updated parameters \(_{t}\).

Figure 1: Schematic illustrations of the key concepts. (a) In MCL, multiple CL episodes are split into meta-training and meta-test sets. For each CL episode, a continual learner produces a model from the training stream (blue), which is evaluated on the test set (green). The learner is meta-trained on multiple CL episodes in the meta-training set and evaluated on the meta-test set. (b) In many MCL approaches, the learner mainly depends on SGD to update the model in the inner loop. (c) In our framework, a recurrent sequence model plays both the roles of the learner and the model.

In contrast, we propose to formulate CL as a sequence modeling problem and let a sequence model play both roles of the continual learner and the model, as presented in Fig. 0(c). In our framework, the inner loop and outer loop are equivalent to the forward pass and SGD update of the sequence model, respectively. To comply with the assumptions of CL, the sequence model should be representable as a recurrent learner \(H_{}\) that iteratively refines an internal state \(h_{t}\) as new data arrive, i.e., \(h_{t}=H_{}(h_{t-1},(x_{t},y_{t}))\). We will explain in SS 3.1 that Transformers can be seen as a recurrent model. Since the Markov property holds between the internal states, \(h_{t}\) should summarize all the data seen up to a time step \(t\). The combination of \(h_{t}\) and the sequence model's parameter \(\) thus defines a _virtual model_\(f_{_{t}}:()\) with \(_{t}=(,h_{t})\). After forwarding a training stream \(^{}\) of length \(T\), we obtain a virtual model \(f_{_{T}}\) trained in-context. Then, each test input \(x_{n}\) is considered as the next token \(x_{T+1}\), and \(_{n}=f_{_{t}}(x_{n})\) is produced from the forward pass of the sequence model conditioned on \(h_{T}\). The meta-loss function is defined to maximize the conditional log-likelihood:

\[(f_{_{T}},^{})=_{(x_{n},y_{n}) ^{}}- p(y_{n}|x_{n};_{T})=_{(x_{n},y _{n})^{}}- p(y_{n}|x_{n},h_{T};).\] (1)

This meta-loss in the MCL perspective is technically equivalent to the popular next-word prediction loss in language modeling [28; 29]. That is, we update \(\) to minimize the meta-loss via SGD, as commonly done for language model training. Although we mainly assume all test examples come after the training stream, it is also possible to feed test examples in the middle of a training stream if the corresponding task is already learned. To avoid unnecessary complexity, however, we mainly assume all test examples to come after the training stream.

An important characteristic of our approach is that the target \(y\) is also taken as an input of the sequence model. Since conventional MCL algorithms maintain an explicit model \(f_{}:\), the only way for them to inject the knowledge from \(y\) into the model is the SGD update with the inner loss \(_{}(f_{_{t}}(x),y)\) (Alg. 0(c)). On the other hand, our sequence modeling approach replaces the SGD update with the forward pass, which incorporates \(y\)'s information by taking it as input. The forward pass can potentially learn a far more flexible update rule compared to SGD. As presented in Fig. 0(c), we denote the state after seeing \(x_{t}\) as \(h_{t-1}^{+}\) to differentiate it from \(h_{t}\), the state after seeing both \(x_{t}\) and \(y_{t}\).

Using sequence models can also be advantageous in terms of scalability. A major drawback of the SGD-based MCL is computing the _meta_-gradient \(_{}(f_{_{T}},^{})\) in the outer loop. Its computation involves calculating higher-order derivatives and large memory for the entire computation graph of the inner loop, which holds every revision of the parameters \(\{_{t}\}_{t=1}^{T}\), the gradients \(\{_{_{t-1}}L_{t}\}_{t=1}^{T}\), and their intermediate calculations. Thus, it is hard to scale the model size or the length of the inner loop. On the other hand, our framework has no such meta-gradient (i.e., our meta-loss is a normal sequence model loss), which allows us to use a much larger and more powerful architecture within the same computational budget.

It is worth mentioning that there are other meta-learning approaches that frame various learning scenarios as the forward pass of a neural network. For instance, [35; 32] employ RNN-based architectures for few-shot learning, while  utilize a combination of temporal convolution and attention layers for both supervised and reinforcement learning.

### Transformers as Meta-Continual Learning Methods

As an instantiation of our framework, we demonstrate that Transformers can be utilized as a general MCL mechanism. We only consider decoder-only Transformers, which are suitable for handling the streaming data in CL settings.

Initially, it may not be apparent how the decoder-only Transformers can be represented as a recurrent model in Fig. 0(c) since Transformers are frequently described as a parallel computation graph over the whole sequence. As highlighted in , however, Transformers with causal attention can be regarded as recurrent models. A decoder-only Transformer layer consists of two sublayers: a causal attention layer and a feed-forward layer. Since the feed-forward layers work on each token independently, information transfer across time steps occurs exclusively through the causal attention sublayer, and only in the temporal direction (\(\)). Therefore, at any time \(t\), all the past information is represented as keys \(\{_{t^{}}^{l}\}_{t^{}=1}^{t}\) and values \(\{_{t^{}}^{l}\}_{t^{}=1}^{t}\) in each attention head \(l\), which are accessed by the query \(_{t}^{l}\). We can think of the keys and values in each attention layer as the internal state of a recurrent sequence model: \(h_{t}=_{l}h_{t}^{l}\) where \(h_{t}^{l}=\{(_{t^{}}^{l},_{t^{}}^{l})\}_{t^{}=1}^{t}\). At every time step, this internal state is updated to include the keys and values from the current time step, as highlighted in Alg. 2. In Alg. 2, also notice that \(y_{t}\) is regarded as the input of Transformers (L8-9), and the virtual model \(f_{(,h_{T})}\) is evaluated on the test set to produce the meta-loss (L11).

### Kernel-Based Efficient Transformers for Efficient Meta-Continual Learning

Meanwhile, Transformers have been criticized for their consistently growing computational cost, which is more commonly referred to as the quadratic complexity in the Transformer literature. This limitation of Transformers triggered extensive research on _efficient Transformers_ which have sub-quadratic complexity. By bridging CL and sequence modeling, we allow this family of advanced Transformers to be immediately applied as CL methods. The only requirement for the model is to support decoding, which is required to sequentially take new examples in CL settings.

Among the large taxonomy of efficient Transformers, our work focuses on kernel-based efficient Transformers (KETs), which can be adapted to decoding while having \((T)\) complexity. As explained in SS2.2 the decoder version of KET works very similarly to an RNN with an internal state of a constant size. As summarized in L7 and L9 of Alg. 3, the update rule corresponding to each attention head \(l\) can be concisely expressed as adding the outer product of the key feature \((_{t}^{l})\) and the value vector \(_{t}^{l}\) to the hidden state of a fixed size. Note that \(1\) is appended to \(_{t}^{l}\) to compute the sum of the key features \(_{t}^{l}=_{t^{}=1}^{t}(_{t^{}}^{l})\) simultaneously. We test Linear Transformer  and Performer , and the only difference between the two is the kernel function \(\), as explained in SS2.2.

```
1:learner \(H_{}\), \(^{}\), \(^{}\)
2:\(T|^{}|\)
3:\(\) indices of attention heads in \(H_{}\)
4:\(h_{0}\{\}\)\(\) internal state
5:for\(t 1\) to \(T\)do
6:\(x_{t},y_{t}^{}[t]\)
7:\((\{_{t}^{l},_{t}^{l}\})_{l} H_{}(h_{t -1},x_{t})\)
8:\(h_{t-1} h_{t-1}\{(_{t}^{l},_{t}^{l})\}_{l}\)
9:\((\{_{t}^{l},_{t}^{l}\})_{l} H_{}(h_{t -1}^{l},y_{t})\)
10:\(h_{t}^{l} h_{t-1}^{+}\{(_{t}^{l},_{t}^{l})\}_{l }\)
11:endfor
12:return\((f_{(,h_{T})},^{})\)\(\) Meta-loss ```

**Algorithm 1** Inner loop of conventional SGD-based MCL

```
1:Transfer \(H_{}\), \(^{}\), \(^{}\)
2:\(T|^{}|\)
3:\(\) indices of attention heads in \(H_{}\)
4:\(h_{0}\{\}\)\(\) internal state
5:for\(t 1\) to \(T\)do
6:\(x_{t},y_{t}^{}[t]\)
7:\((\{_{t}^{l},_{t}^{l}\})_{l} H_{}(h_{t -1},x_{t})\)
8:\(h_{t-1}^{+} h_{t-1}\{(_{t}^{l},_{t}^{l})\}_{l }\)
9:\((\{_{t}^{l},_{t}^{l}\})_{l} H_{}(h_{t -1}^{+},y_{t})\)
10:\(h_{t}^{l} h_{t-1}^{+}\{(_{t}^{l},_{t}^{l})\}_{l }\)
11:endfor
12:return\((f_{(,h_{T})},^{})\)\(\) Meta-loss ```

**Algorithm 2** Inner loop of Transformer

**Connection to Dynamic/Static-Architecture CL Approaches.** Since the size of the internal states linearly grows as more data come in, standard Transformers can be grouped with the dynamic CL architectures. As the size and the computational cost of dynamic-architecture CL approaches typically grow linearly with the number of tasks \(K\), the complexity of handling a CL episode of length \(T\) is \((KT)\). In most CL scenarios where \(K T\), the complexity becomes \((T^{2})\), which is the same as Transformers. On the other hand, KETs' internal states remain at a constant size, which makes them the counterparts of static-architecture CL approaches.

### An Example: Continual Learning of Image Classification as Sequence Modeling

We describe a specific example of applying our approach to image classification, which has been widely used for evaluating MCL methods. In this setting, \(x\) is an image, while \(y\) is the corresponding class label. To feed into Transformers, both \(x\) and \(y\) should be encoded into a fixed-sized vector. While we can simply use a convolutional network (which can be jointly trained end-to-end) for \(x\), properly encoding \(y\) requires some considerations. Since a class label is essentially an abstract symbol, e.g., an arbitrary index of the class, it does not have any meaning on its own. Simply hard-assigning a trainable embedding to each class would fail, as novel classes would appear in the meta-test phase. Randomly sampling a fixed embedding every time a class appears also turns out to be inapplicable as we experiment. The random embedding is the same as an untrained token embedding, which is expected to be incompatible with the trained weights of Transformers.

The general solution used in language modeling is to express everything as a combination of known vocabulary . Following the same principle, we _tokenize_ a class label using a fixed vocabulary \(V\) and train an embedding for each token in the vocabulary. Since class labels do not impose any constraint on how they should be tokenized, we randomly sample a unique class code \((v_{1},...,v_{C})\), a random combination of the tokens \(v_{c} V\), when a new class appears in an episode. Note that if the code length \(C\) is greater than one, each token goes into a Transformer individually, i.e., an \((x,y)\) pair is converted to the sequence \((x_{},v_{1},...,v_{C})\) before going into the Transformer. At test time, the Transformer sequentially outputs the code to predict the corresponding class. To reduce complications, we keep \(C=1\) in the main experiments, and the vocabulary size is set to the maximum number of classes that can appear in an episode. Experiments with \(C>1\) can be found in Appendix B.

## 4 Experiments

We demonstrate the potential of our approach through experiments with seven benchmarks. Unless otherwise noted, each experiment is repeated five times, and the mean and standard deviation of the five runs are reported. Due to space constraints, this section contains only the essential information to understand the main result, while details, additional experiments and analyses are deferred to Appendix A, B, and C.

### Benchmarks

To show the generality of our approach, we conduct experiments with diverse types of tasks, including both classification and regression. To construct the meta-training and meta-test set for a dataset, we first divide it into a set of tasks. The tasks are then split into two disjoint sets, one for meta-training and the other for meta-testing. The CL episodes for both meta-splits are constructed in the same manner. To build a CL episode \(\), we randomly sample \(K\) unique tasks. By default, we set \(K=20\), while additionally testing the \(K=100\) setting to compare performances with longer episodes. For each task \(k\), the training stream \(_{k}^{}\) and the test set \(_{k}^{}\) contain five examples each (i.e., five shots). For each experiment, we meta-train for 50K steps with a batch size of 16 (i.e., 16 episodes in parallel) and meta-test with 1,024 episodes. Task identities are not provided in any case.

#### 4.1.1 Classification

For simplicity, we define each class as a task. All input images are resized to \(32 32\) resolution. We report classification errors in percentage as the evaluation metric.

**CIFAR-100 .** CIFAR-100 contains 60K images of 100 classes, with 600 images for each class. We randomly choose 60 classes for meta-training and the other 40 for the meta-test.

**Omniglot .** Omniglot is a handwriting dataset widely used as a benchmark for meta-learning or MCL. It has a total of 1,623 character classes gathered from 50 alphabets. The meta-train set consists of 963 classes (30 alphabets), and the meta-test set has 660 classes (20 alphabets). There are 20 images for each class, thus 19K images for meta-training and 13K images for meta-test.

**CASIA Chinese Handwriting Database (CASIA; 22).** This is another handwriting dataset that consists of 7,356 character classes in total. The classes comprise 7,185 Chinese characters, 52 English alphabets, 10 digits, and other frequently used symbols. The total number of images is 3.9M, which is 120 times larger than Omniglot. We randomly sample 1K classes for the meta-test and use the rest for meta-training. To the best of our knowledge, this dataset has not been used in meta-learning or MCL literature. However, its substantial size allows us to evaluate different aspects of MCL algorithms, which could not be examined with smaller datasets where meta-overfitting is a major factor in the evaluation. Our results suggest that the relative performance of MCL algorithms can differ in a large data regime.

**MS-Celeb-1M .** In addition to the CASIA dataset, we also propose to repurpose MS-Celeb-1M as a large-scale MCL benchmark. It is a facial image dataset with roughly 10M images of 100K celebrities. Similar to CASIA, we randomly select 1K classes, the celebrity IDs, for the meta-test and use the others for meta-training.

#### 4.1.2 Regression

**Sine Wave Reconstruction (Sine).** Inspired by the sine wave regression problem in , we design a more challenging synthetic regression problem for MCL. A sine wave \(()=A(2+)\) is characterized by amplitude \(A\), frequency \(\), and phase \(\). We define the target \(y\) as the values of the sine wave at 50 fixed points: \(y=[(_{1}),,(_{50})]\). All \(y\)'s in each task share the same frequency and phase, while they can vary in amplitudes. We corrupt \(y\) into \(x\) by shifting phase and adding Gaussian noise, where the phase shift amount is sampled for each task. We report the mean squared error between \(y\) and the model prediction \(\) as the evaluation metric.

**Image Rotation Prediction (Rotation).** A model is given an image rotated by an angle \([0,2)\) and tasked to estimate an angle \(\). The evaluation metric is \(1-(-)\); thus, a perfect model would score 0 while random guessing would score 1.0 on average. We use the CASIA images and define each class as a task using the same meta-split.

**Image Completion (Completion).** An image completion problem involves predicting the missing parts of an image based on the available parts. Derived from our CASIA classification benchmark, we change the input \(x\) to be the top half of the image and the target \(y\) to be the bottom half. The evaluation metric is the mean squared error of the predicted pixel values.

### Methods

For all methods, we use a five-layer CNN to encode image inputs and a three-layer MLP to encode vector inputs (e.g., the sine regression). All components are trained end-to-end from scratch. We group compared methods into four types: offline, class-incremental MCL, SGD-based MCL, and ours.

**Offline.** We compare the scores of offline learning since it is generally perceived as the performance upper bound of non-meta-CL. Note that MCL approaches can outperform offline learning by lever

    &  &  & Omniglot &  &  \\  & & 60K & / 0.1K & 32K & / 1.6K & **3.9M \(\)** / 7.2K \(\)** & **10M \(\)** / 100K \(\)** \\  & & Meta- & Meta- & Meta- & Meta- & Meta- & Meta- & Meta- & Meta- & Meta- \\  & train & test & train & test & train & test & train & test \\  Offline & Offline & N/A & \(^{ 7.2}\) & N/A & \(35.0^{ 6.8}\) & N/A & \(41.7^{ 6.2}\) & N/A & \(63.3^{ 7.7}\) \\   & PN & \(0.0^{ 0.0}\) & \(76.6^{ 0.3}\) & \(0.0^{ 0.0}\) & \(^{ 0.1}\) & \(0.2^{ 0.0}\) & \(^{ 0.0}\) & \(32.5^{ 0.1}\) & \(33.6^{ 0.1}\) \\  & GeMCL & \(0.0^{ 0.0}\) & \(76.6^{ 0.4}\) & \(0.0^{ 0.0}\) & \(^{ 0.1}\) & \(0.2^{ 0.0}\) & \(^{ 0.0}\) & \(32.1^{ 0.1}\) & \(33.3^{ 0.2}\) \\   & OML & \(0.6^{ 0.1}\) & \(89.9^{ 0.4}\) & \(0.1^{ 0.0}\) & \(24.8^{ 2.2}\) & \(2.8^{ 0.1}\) & \(3.2^{ 0.1}\) & \(41.8^{ 0.3}\) & \(42.5^{ 0.2}\) \\  & ANML & \(0.4^{ 0.1}\) & \(88.1^{ 1.4}\) & \(0.0^{ 0.0}\) & \(31.0^{ 6.2}\) & \(3.7^{ 0.5}\) & \(43.3^{ 0.5}\) & \(43.8^{ 0.3}\) & \(44.8^{ 0.4}\) \\   & Transformer & \(0.0^{ 0.0}\) & \(82.8^{ 0.8}\) & \(0.0^{ 0.0}\) & \(13.7^{ 0.6}\) & \(0.3^{ 0.0}\) & \(^{ 0.0}\) & \(29.1^{ 0.2}\) & \(^{ 0.2}\) \\  & Linear TF & \(0.1^{ 0.1}\) & \(83.4^{ 0.5}\) & \(0.0^{ 0.0}\) & \(36.0^{ 1.4}\) & \(0.4^{ 0.0}\) & \(0.7^{ 0.0}\) & \(31.1^{ 0.3}\) & \(^{ 0.3}\) \\   & Performer & \(0.0^{ 0.0}\) & \(82.9^{ 0.3}\) & \(0.1^{ 0.1}\) & \(37.1^{ 4.6}\) & \(0.5^{ 0.0}\) & \(0.7^{ 0.0}\) & \(32.5^{ 0.5}\) & \(33.7^{ 0.2}\) \\   

Table 1: Classification errors (%) in 20-task 5-shot MCL. Both meta-training and meta-test errors are reported to highlight the relationship between the degree of meta-overfitting and the scale of the dataset (\(\) : images, \(\): classes).

   Type & Method & CASIA & MS-Celeb-1M & Sine & Rotation & Completion \\  Offline & Offline & \(70.7^{ 3.7}\) & \(87.1^{ 3.3}\) & \(0.0180^{ 0.0038}\) & \(0.699^{ 0.049}\) & \(0.1713^{ 0.0047}\) \\   & PN & \(^{ 0.0}\) & \(44.5^{ 0.1}\) & N/A & N/A & N/A \\  & GeMCL & \(^{ 0.0}\) & \(44.4^{ 0.1}\) & N/A & N/A & N/A \\  SGD & OML & \(6.8^{ 0.9}\) & \(54.5^{ 0.2}\) & \(0.0498^{ 0.0004}\) & \(0.524^{ 0.087}\) & \(0.1087^{ 0.0001}\) \\   & Transformer & \(1.0^{ 0.0}\) & \(^{ 0.1}\) & \(^{ 0.0002}\) & \(^{ 0.001}\) & \(^{ 0.0001}\) \\  & Linear TF & \(2.3^{ 0.1}\) & \(45.3^{ 0.1}\) & \(^{ 0.0003}\) & \(^{ 0.002}\) & \(^{ 0.0001}\) \\   

Table 2: Classification errors (%) and regression errors of 100-task 5-shot MCL.

aging additional meta-training data. Since the model overfits to the training data we report the best test score achieved during training. We repeat each experiment ten times and report the average and standard error of mean.

**Class-Incremental MCL (CI).** We test PN  and GeMCL  as MCL approaches that are specialized for class-incremental settings. Although they cannot be applied to other settings, such as regression, they show strong performance in the classification benchmarks.

**SGD-Based MCL (SGD).** We select OML  as the main baseline since it can perform both classification and regression tasks. It has a two-layer MLP as the prediction network, on top of a meta-learned encoder. For 20-task 5-shot classification benchmarks, we also test ANML , which is an extension of OML with an additional modulatory network that regulates the features of the encoder. However, due to its extensive memory consumption, it could not be tested on longer episodes.

**Ours.** We test the vanilla Transformer , Linear Transformer , and Performer . All the models share a similar architecture: 4 layers, 8 heads, and 512 hidden dimensions. Although it is tiny compared to modern language models, it is still larger than the FC layers of the baselines. Surprisingly, however, they require comparable or less GPU memory and time for meta-training in our experiments due to the lack of meta-gradient computation, as noted in SS3. For more analysis on the computational cost, refer to Appendix C.

### Results

In Table 1, we report the meta-training and meta-test errors of the 20-task 5-shot classification benchmarks to show the necessity of large-scale MCL benchmarks. In CIFAR-100 and Omniglot, all methods show a severe degree of meta-overfitting, the gap between the meta-training and meta-test scores. Meta-overfitting is more serious in CIFAR-100 where the task (class) diversity is lower. In such situations, the Transformers still perform competitively, but the baselines may achieve a better meta-test score due to less meta-overfitting. On the other hand, if more data is available, as in the CASIA and MS-Celeb-1M benchmarks, the Transformers outperform OML and ANML by a large margin and perform on par or better than PN or GeMCL, which are specialized for classification. Table 2 shows the results of longer 100-task 5-shot experiments in both classification and regression domains. In this more challenging scenario, the Transformers still perform comparably to class-incremental methods and significantly better than SGD-based approaches. The scores of offline learning mostly fall behind other MCL methods, which implies that non-meta-CL methods are not suitable for our experimental settings where the amount of data is far from abundant.

A strength of our approach, which comes with the use of Transformers, is parallel meta-training. While the baselines require a sequential SGD update on the training stream, Transformers can process the whole training stream in parallel, which can dramatically improve the meta-training efficiency once combined with massively parallel processors like GPUs. As a result, the meta-training time of Transformers is several times shorter than the baselines. The details can be found in Appendix C.

The tradeoff between the standard Transformer and the KETs is also worth discussing. In all experiments, the KETs sacrifice some level of accuracy for greater efficiency. The accuracy degradation is more severe in longer episodes, which is expected given that more information has to be fit into the internal state of the same size. This tradeoff suggests that the choice between standard and efficient Transformers ultimately depends on the required accuracy and the available computational resources.

In Appendix B, we present more experiments and analyses, including different episode configurations and model sizes. Although preliminary, the experiments with Transformers of varying sizes are particularly interesting, as the performance scales with the number of parameters. This is reminiscent of the scaling law of large language models , suggesting that scaling might be a potential solution to more challenging MCL problems.

## 5 Limitations and Conclusion

By formulating MCL as sequence modeling, we pave the way for incorporating advanced sequence modeling technologies into the MCL field. Given the observation that our approach demonstrates greater efficacy with increased data, it seems consistent with _The Bitter Lesson_, an argument popularized by Sutton . It states that, in the long run, general-purpose learning algorithms capable of leveraging more computation and data are likely to be more effective than human-designed algorithms. We believe our work is a step forward in this direction, generalizing the SGD-based update rules in CL with the learnable forward pass of a sequence model.

However, it is important to acknowledge that the sequence models also bring their own limitations and challenges. Efficiently handling long sequences is one such challenge. According to our experiments, even the efficient Transformers, which are designed to solve the exact challenge, have to trade off a nonnegligible amount of performance for efficiency, which implies plenty of room to improve. Nonetheless, considering the current pace of development in the sequence modeling field, we are optimistic about the potential of our approach; if a new solution is developed for sequence modeling, it can be readily integrated into our formulation.