# MetaCURL: Non-stationary Concave Utility Reinforcement Learning

Bianca Marin Moreno

Inria

EDF R\(\&\)D

Margaux Bregere

Sorbonne Universite

EDF R\(\&\)D

Pierre Gaillard

Inria

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.

Nadia Oudjane

EDF R\(\&\)D

EDF R\(\&\)D

EDF R\(\&\)D

EDF R\(\&\)D

###### Abstract

We explore online learning in episodic Markov decision processes on non-stationary environments (changing losses and probability transitions). Our focus is on the Concave Utility Reinforcement Learning problem (CURL), an extension of classical RL for handling convex performance criteria in state-action distributions induced by agent policies. While various machine learning problems can be written as CURL, its non-linearity invalidates traditional Bellman equations. Despite recent solutions to classical CURL, none address non-stationary MDPs. This paper introduces MetaCURL, the first CURL algorithm for non-stationary MDPs. It employs a meta-algorithm running multiple black-box algorithms instances over different intervals, aggregating outputs via a sleeping expert framework. The key hurdle is partial information due to MDP uncertainty. Under partial information on the probability transitions (uncertainty and non-stationarity coming only from external noise, independent of agent state-action pairs), the algorithm achieves optimal dynamic regret without prior knowledge of MDP changes. Unlike approaches for RL, MetaCURL handles adversarial losses. We believe our approach for managing non-stationarity with experts can be of interest to the RL community.

## 1 Introduction

We consider the task of learning in an episodic Markov decision process (MDP) with a finite state space \(\), a finite action space \(\), episodes of length \(N\), and a probability transition kernel \(p:=(p_{n})_{n[N]}\) such that for all \((x,a)\), \(p_{n}(|x,a)_{}\). For any finite set \(\), we denote by \(_{}\) the simplex induced by this set, and by \(||\) its cardinality. For all \(d\) we let \([d]:=\{1,,d\}\). At each time step \(n\), an agent in state \(x_{n}\) chooses an action \(a_{n}_{n}(|x_{n})\) by means of a policy, and moves to the next state \(x_{n+1} p_{n+1}(|x_{n},a_{n})\), inducing a state-action distribution sequence \(^{,p}:=(^{,p}_{n})_{n[N]}\), where \(^{,p}_{n}_{}\) for all \(n[N]\).

In many applications of learning in episodic MDPs, an agent aims at finding an optimal policy \(\) maximizing/minimizing a concave/convex function \(F\) of its state-action distribution, known as the Concave Utility Reinforcement Learning (CURL) problem:

\[_{(_{})^{ N}}F(^{,p}).\] (1)

CURL extends reinforcement learning (RL) from linear to convex losses. Many machine learning problems can be written in the CURL setting, including: RL, where for a loss function \(\), \(F(^{,p})=,^{,p}\); pure RL exploration , where \(F(^{,p})=^{,p},(^{,p})\); imitation learning [26; 35] and apprenticeship learning [55; 1], where \(F(^{,p})=D_{g}(^{,p},^{*})\), with \(D_{g}\) representing a Bregmandivergence induced by a function \(g\) and \(^{*}\) being a behavior to be imitated; certain instances of mean-field control , where \(F(^{,p})=(^{,p}),^{,p}\); mean-field games with potential rewards ; among others. The CURL problem alters the additive structure inherent in standard RL, invalidating the classical Bellman equations, requiring the development of new algorithms.

Most of existing works on CURL focus on stationary environments [28; 57; 58; 5; 56; 25; 12; 11], where both the objective function \(F\) and the probability transition kernel \(p\) remain the same across episodes. However, in practical scenarios, environments are rarely stationary. The work of  is the first to address online CURL with objective functions that can change arbitrarily between episodes, also known as adversarial losses . However, their work assumes stationary probability kernels and presents results in terms of static regret (performance comparable to an optimal policy). In non-stationary scenarios, it is more relevant to minimize dynamic regret--the gap between the learner's total loss and that of any policy sequence (see Eq. (5) for formal definition). In this work we address this problem by introducing the first algorithm for CURL handling adversarial objective functions and non-stationary probability transitions, achieving near-optimal dynamic regret.

**High-level idea.** Our approach, called MetaCURL, draws inspiration from the online learning literature. In online learning , non-stationarity is often managed by running multiple black-box algorithm instances from various starting points and dynamically selecting the best performer using an "expert" algorithm. This strategy has demonstrated effectiveness in settings with complete information [29; 59; 47; 33]. With MetaCURL, we extend this concept to decision-making in MDPs. Unlike classical online learning, the main challenge faced is uncertainty. We assume that the probability transition kernel in each episode has a known deterministic structure but is affected by an external noise with unknown distribution, placing us in a setting with only partial information (see Section 2 for more details). The learner is then unable to observe the agent's loss under policies other than the one played.

MetaCURL is a general algorithm that can be applied with any black-box algorithm with low dynamic regret in near-stationary environments. CURL approaches suitable as black-boxes rely on parametric algorithms that would require prior knowledge of the MDP changes to tune their learning rate. MetaCURL also addresses this challenge by simultaneously running multiple learning rates and weighting them in direct proportion to their empirical performance. MetaCURL achieves optimal regret of order \(}T}+\{^{p}T },\ T^{2/3}(^{p})^{1/3}\}\), where \(_{}^{p}\) and \(^{p}\) represent the frequency and magnitude of changes of the probability transition kernel respectively, and \(^{^{*}}\) is the magnitude of changes of the policy sequence we compare ourselves with in dynamic regret (see Eqs. (6) and (7) for formal definitions). MetaCURL does not require previous knowledge of the degree of non-stationarity of the environment, and can handle adversarial losses. To ensure completeness, we show that Greedy MD-CURL from  fulfills the requirements to serve as a black-box algorithm. This is the first dynamic regret analysis for a CURL approach.

**Comparisons.** Without literature on non-stationary CURL, we review non-stationary RL approaches. Most methods [24; 13; 45; 17; 20; 40; 21] typically rely on prior knowledge of the MDP's non-stationarity degree, while MetaCURL does not. Let \(_{}^{l}\) and \(^{l}\) represent the frequency and magnitude of change in the RL loss function, respectively1. Recently,  achieved a regret of \(\{^{p}+_{}^{l})}T,\ T^{2/3}(^{p}+^{l})^{1/3}\}\), a near-optimal result as demonstrated by , without requiring prior knowledge of the environment's variation. However, this regret bound is tied to changes in loss functions, making it ineffective against adversarial losses. In contrast, rather than depending on the magnitude of variation of the loss function, MetaCURL's bound depends on the magnitude of variation of the policy sequence we use for comparison in dynamic regret. This allows it to handle adversarial losses, and to compare against policies with a more favorable bias-variance trade-off, which may not align with the optimal policies for each loss. In addition, we improve this dependency by paying it as \(}T}\) instead of \((^{^{*}})^{1/3}T^{2/3}\). We summarize comparisons in Table 1.

**Other related works.** The studies by [43; 42] examine the difference between optimizing the objective over infinite trials and the expectation of the objective over a single trial, challenging the traditional CURL formulation in Eq. (1). Here, we retain the classic formulation to align with existing CURL research. Other works on RL with nonlinear objective functions are [46; 16] focusing on rewards over trajectories rather than individual states. In addition to non-stationarity, there is a series of works on RL with adversarial losses but _stationary_ probability transitions, with results only on static regret [48; 30; 18; 50; 32; 14]. Another line of research is known as corruption-robust RL. It differs from non-stationary MDPs in that it assumes a ground-truth MDP and measures adversary malice by the degree of ground-truth corruption [31; 38; 10; 60; 53].

**Contributions.** We resume our main contributions below:

* We introduce MetaCURL, the first algorithm for non-stationary CURL. Under the framework described in Section 2, MetaCURL achieves the optimal dynamic regret bound of order \(}T}+\{^{p} T},T^{2/3}(^{p})^{1/3}\}\), without requiring previous knowledge of the degree of non-stationarity of the MDP. MetaCURL handles full adversarial losses and improves the dependency of the regret on the total variation of policies. MetaCURL is the first adaptation of Learning with Expert Advice (LEA) to deal with uncertainty in non-stationary MDPs.
* We also establish the first dynamic regret upper bound for an online CURL algorithm in a nearly stationary environment, which can serve as a black-box routine for MetaCURL.

**Notations.** Let \(\|\|_{1}\) be the \(L_{1}\) norm, and for all \(v:=(v_{n})_{n[N]}\), such that \(v_{n}^{}\) we define \(\|v\|_{,1}:=_{1 n N}\|v_{n}\|_{1}\).

## 2 General framework: non-stationary CURL

When an agent plays a policy \(:=(_{n})_{n[N]}\) in an episodic MDP with probability transition \(p\), it induces a state-action distribution sequence (also called the occupancy-measure ), which we denote by \(^{,p}:=(_{n}^{,p})_{n[N]}\), with \(_{n}^{,p}_{}\). It can be calculated recursively for all \((x,a)\) and \(n[N]\) by taking \(_{0}^{,p}(x,a)=_{0}(x,a)\) fixed, and

\[_{n}^{,p}(x,a)=_{(x^{},a^{}) }_{n-1}^{,p}(x^{},a^{})p_{n}(x|x^{},a^{ })_{n}(a|x).\] (2)

**Offline CURL.** The classic CURL optimization problem in Eq. (1) considers minimizing a function \(F:(_{})^{N}\), here defined as \(F():=_{n=1}^{N}f_{n}(_{n})\) with \(f_{n}\) a convex function over \(_{n}\) with values in \(\), across all policies that induce \(^{,p}\). Note that \(F\) is not convex on the policy \(\). To convexify the problem, we define the set of state-action distributions satisfying the Bellman flow of a MDP with transition kernel \(p\) as

\[_{_{0}}^{p}:=\ _{a^{} }_{n}(x^{},a^{})=_{x,a}p_{n}(x^{}|x,a)_{n-1}(x,a)\, x^{}, n[N] }.\] (3)

For any \(_{_{0}}^{p}\), there exists a strategy \(\) such that \(^{,p}=\). It suffices to take \(_{n}(a|x)_{n}(x,a)\) when the normalization factor is non-zero, and arbitrarily defined otherwise. There is thus an equivalence between the CURL problem (optimization on policies) and a convex optimization problem on state-action distributions satisfying the Bellman flow:

\[_{(_{})^{ N}}F(^{,p}) _{_{_{0}}^{p}}F().\] (4)

**Online CURL.** In this paper we consider the online CURL problem in a non-stationary setting. We assume a finite-horizon scenario with \(T\) episodes. An oblivious adversary generates a sequence of changing objective functions \((F^{t})_{t[T]}\), with \(F^{t}\) being fully communicated to the learner only at the end of episode \(t\). We assume \(F^{t}\) is \(L_{F}\)-Lipschitz with respect to the \(\|\|_{,1}\) norm for all \(t\). The probability transition kernel is also allowed to evolve over time and is denoted by \(p^{t}\) at episode \(t\). The learner's objective is then to calculate a sequence of strategies \((^{t})_{t[T]}\) minimizing a total loss \(L_{T}:=_{t=1}^{T}F^{t}(^{^{t},p^{t}})\), while dealing with adversarial objective functions \(F^{t}\) and changing probability transition kernels \(p^{t}\). To measure the learner's performance, we use the notion of dynamic regret (the difference between the learner's total loss and that of any policy sequence \((^{t,*})_{t[T]}\)):

\[R_{[T]}(^{t,*})_{t[T]}:=_{t[T]}F^{t}(^{^{t},p ^{t}})-F^{t}(^{^{t,*},p^{t}}).\] (5)

**Non-stationarity measures.** We consider the following two non-stationary measures \(^{p}_{}\) and \(^{p}\) on the probability transition kernels that respectively measure abrupt and smooth variations:

\[^{p}_{}:=1+_{t=1}^{T-1}_{\{p^{t} p^{t+1}\}}, ^{p}:=1+_{t=1}^{T-1}^{p}_{t},^{p}_{t}:=_{ n,x,a}\|p^{t}_{n}(|x,a)-p^{t+1}_{n}(|x,a)\|_{1}\,.\] (6)

Regarding dynamic regret, we define for any sequence of policies \((^{t,*})_{t[T]}\), its non-stationarity measure as

\[^{^{*}}:=1+_{t=1}^{T-1}^{^{*}}_{t},^{^{* }}_{t}:=_{n[N],x}\|^{t,*}_{n}(|x)-^{t+1,*}_{n} (|x)\|_{1}\,.\] (7)

Moreover, for any interval \(I[T]\), we write \(^{p}_{I}:=_{t I}^{p}_{t}\) and \(^{^{*}}_{I}:=_{t I}^{^{*}}_{t}\).

**Dynamic's hypothesis.** For each episode \(t\), let \((x^{t}_{0},a^{t}_{0})_{0}()\), and for all time steps \(n[N]\),

\[x^{t}_{n+1}=g_{n}(x^{t}_{n},a^{t}_{n},^{t}_{n}),\] (8)

where \(g_{n}\) represents the deterministic part of the dynamics, and \((^{t}_{n})_{n[N]}\) is a sequence of independent external noises such that \(^{t}_{n} h^{t}_{n}()\), where \(h^{t}_{n}\) is any centered distribution. Note that these dynamics imply that the probability transition kernel can be written as \(p^{t}_{n+1}(x^{t}|x,a)=g_{n}(x,a,^{t}_{n})=x^{ }\). Different variants of this problem can be considered, depending on the prior information available about the dynamics in Eq. (8). In this article we consider the case where \(g_{n}\) is fixed and known by the learner, but \(h^{t}_{n}\) is unknown and can change (hence the source of uncertainty and non-stationarity of the transitions). To the best of our knowledge, there are no black-box algorithms in the literature that achieve sublinear regret for online CURL with adversarial losses without relying on model assumptions. In using RL methods to CURL, we believe model-optimistic approaches like UCRL (Upper Confidence RL ) could be adapted. However, these methods are computationally expensive, as they require solving an additional optimization problem in every episode. The black-box algorithm for CURL we consider from  provides closed-form solutions, which is more computationally efficient, but requires the same dynamic assumption as in Eq. (8). Another class of RL methods is policy optimization (PO), which directly optimizes the policy and often yields closed-form solutions, leading to faster performance. Recent theoretical work  has shown that PO methods can achieve near-optimal regret without model assumptions. However, these methods rely on RL's Bellman equations, which do not apply to CURL due to its non-linear nature. We believe that the MetaCURL analysis could potentially be extended to the case where \(g_{n}\) is unknown but belongs to a parametric family. We leave this extension for future work.

This particular dynamic is also motivated by many real-world applications:

* Controlling a fleet of drones in a known environment, subject to external influences due to weather conditions or human interventions.
* Addressing data center power management aiming to cut energy expenses while maintaining service quality. Workload fluctuations cause dynamic job queue transitions, and volatile electricity prices lead to varying operational costs. The probabilities of task processing by each server are predetermined, but the probabilities of task arrival are uncertain .
* As renewable energy use increases and energy demand grows, balancing production and consumption becomes harder. Certain devices, like electric vehicle batteries and water heaters, can serve as flexible energy storage options. However, this requires electric grids to establish policies regulating when these devices turn on or off to match a desired consumption profile. These profiles can fluctuate daily due to changes in energy production levels. Despite knowing the devices' physical dynamics, household consumption habits remain unpredictable and constantly changing .

**Outline.** In this paper, we propose a novel approach to handle non-stationarity in MDPs, being the first to propose a solution to CURL within this context. We begin in Section 3 by discussing the idea behind our algorithm's construction and the key challenges within our framework. Section 4 introduces MetaCURL, while Section 5 presents the main results of our regret analysis. The proofs' specifics are provided in the appendix.

## 3 Main idea

**A hypothetical learner who achieves optimal regret.** Let \(m>1\). Assume a hypothetical learner that could compute a sequence of restart times \(1=t_{1}<<t_{m+1}=T+1\), where for each \(i[m]\) we let \(_{i}:=[t_{i},t_{i+1}-1]\), such that

\[_{_{i}}^{p}^{p}/m.\] (9)

Consider any algorithm that, when computing \((^{t})_{t I}\) with learning rate \(\) for any interval \(I[T]\), attains a dynamic regret relative to any sequence of policies \((^{t,*})_{t I}\) upper bounded by

\[R_{I}(^{t,*})_{t I} c_{1}|I|+^{-1}(c_{2 }_{I}^{^{*}}+c_{3})+|I|_{I}^{p},\] (10)

where \((c_{j})_{j}\) are constants that may depend on the MDP parameters, and on the interval size only in logarithmic terms. This kind of regret bound holds for Greedy MD-CURL from  as we show in Appendix G. Suppose the hypothetical learner could also access \(^{^{*}}\) to calculate the optimal learning rate. Hence, playing such an algorithm for all horizon \(T\) with the optimal learning rate, the learner would have a dynamic regret upper bounded by

\[R_{[T]}(^{t,*})_{t[T]} 2(c_{2}^{^{ *}}+c_{3}m)T}+T^{p}m^{-1}.\]

Optimizing over \(m\), the learner would obtain the optimal regret of order \(}T}+(^{p})^{1/3}T^{2/3}\). In the case where the MDP is piece-wise stationary, if the learner takes \(_{i}\) such that \(_{_{i}}^{p}=0\), it obtains a regret of order \(O(}T}+_{}}^{p}T})\), where \(_{}^{p}\) is the number of times the probability transitions of the MDP change over \([T]\).

**A meta algorithm to learn restart times.** In reality, the restart times of Eq. (9), and the optimal learning rate, are unknown to the learner. Hence, we propose to build a meta aggregation algorithm to learn both. Let \(\) represent a parametric black-box algorithm with dynamic regret as in Eq. (10). We introduce a meta algorithm \(\) that, takes as input a finite set of learning rates \(\), and at each episode \(t\), initializes \(||\) instances of \(\), denoted as \(^{t,}\) for each \(\). Each \(^{t,}\) operates independently within the interval \([t,T]\). At time \(t\), \(\) combines the decisions from the active runs \(\{^{s,}\}_{s t,}\) by weighted average. The idea is that at time \(t\), some of the outputs of \(\{^{s,}\}_{s t,}\) are not based on data prior to \(t^{}<t\), so if the environment changes at time \(t^{}\), these outputs can be given a greater weight by the meta algorithm, enabling it to adapt more quickly to the change. At the same time, we expect a larger weight will be given to the empirically best learning rate. Let \((,)\) be the complete algorithm.

**Remark 3.1**.: _The meta-algorithm increases the computational complexity of the parametric black-box algorithm by a factor of \(T||\), as it requires updating \(t||\) instances at each episode \(t\). By strategically designing intervals to run the black-box algorithms, previous works on online learning have reduced computational complexity to \(O((T))\)[15; 29; 27]. Extending our analysis to these intervals is straightforward, but it would complicate the presentation of the paper. Thus, we decided to present our results using the naive choice of intervals. Also, in Section 5, we show that a learning rate grid with \(||=(T)\) is sufficient to obtain the optimal regret._

**Regret decomposition.** Denote by \(^{t,s,}\) the policy output from \(^{s,}\) at episode \(t\), for learning rate \(\), for all \(s t\), and by \(^{t}\) the policy output by the meta algorithm \((,)\) to be played by the learner. The regret of \((,)\) can be decomposed as the sum of the regret suffered by the meta algorithm aggregated scheme, \(\), and the regret from the black-box algorithm, \(\), played with any learning rate \(\). The dynamic regret, defined in Eq. (5), can be decomposed, for any set of intervals \(_{i}=[t_{i},t_{i+1}-1]\), with \(1=t_{1}<<t_{m+1}=T+1\), and for any learning rate \(\), as

\[R_{[T]}(^{t,*})_{t[T]} =^{m}_{t_{i}}F^{t}(^ {^{t},p^{t}})-F^{t}(^{^{t,t_{i},},p^{t}})}_{}+_{i=1}^{m} _{i}}F^{t}(^{^{t,t_{i},},p^{t}})- F^{t}(^{^{t,*},p^{t}})}_{_{i}}\] \[:=R_{[T]}^{}+R_{[T]}^{}(^{t,*})_{t[T]}.\] (11)The black-box regret on \(_{i}\) is exactly the standard regret for \(T=|_{i}|\) with a learning rate of \(\). Hence, in order to prove low dynamic regret for \((,)\) we have to: show that \(\) incurs a low dynamic regret in each interval \(_{i}\); find a black-box algorithm \(\) for CURL that has dynamic regret as in Eq. (10), and build a learning rate grid \(\) allowing us to perform nearly as well as the optimal learning rate.

## 4 MetaCURL Algorithm

We call our meta-algorithm \(\) MetaCURL. It is based on sleeping experts, is parameter-free, and achieves optimal regret. Its construction is described below.

### Learning with expert advice

**General setting.** In Learning with Expert Advice (LEA), a learner makes sequential online predictions \(u^{1},,u^{T}\) in a decision space \(\), over a series of \(T\) episodes, with the help of \(K\) experts [22; 36; 9]. For each round \(t\), each expert \(k\) makes a prediction \(u^{t,k}\), and the learner combines the experts' predictions by computing a vector \(v^{t}:=(v^{t,1},,v^{t,K})_{K}\), and predicting the convex combination of experts' prediction \(u^{t}:=_{k=1}^{K}v^{t,k}u^{t,k}\). The environment then reveals a convex loss function \(^{t}:\). Each expert suffers a loss \(^{t,k}:=^{t}(u^{t,k})\), and the learner suffers a loss \(^{t}:=^{t}(u^{t})\). The learner's objective is to keep the cumulative regret with respect to each expert as low as possible. For each expert \(k\), this quantity is defined as \(_{[T]}(k):=_{t=1}^{T}^{t}-^{t,k}\).

**Sleeping experts.** In our case, each black-box algorithm is an expert that does not produce solutions outside its active interval. This problem can be reduced to the sleeping expert problem [8; 23], where experts are not required to provide solutions at every time step. Let \(I^{t,k}\{0,1\}\) define a signal equal to \(1\) if expert \(k\) is active at episode \(t\) and \(0\) otherwise. The algorithm knows \((I^{t,k})_{k[K]}\) and assigns a zero weight to sleeping experts (\(I^{t,k}=0\) implies \(v^{t,k}=0\)). We would like to have a guarantee with respect to expert \(k[K]\) but only when it is active. Hence, we now aim to bound a cumulative regret that depends on the signal \(I^{t,k}\). \(^{}_{[T]}(k):=_{t=1}^{T}I^{t,k}(^{t}-^ {t,k})\). There is a generic reduction from the sleeping expert framework to the general LEA setting [3; 2] (see Appendix A.1).

### Meta-aggregation scheme

In every episode \(t\), for every learning rate \(\) and \(s t\), an instance \(^{s,}\) of the black-box algorithm acts as an expert computing a policy \(^{t,s,}\). The meta algorithm aims to aggregate these predictions using a sleeping expert approach based on the expert's losses. However, within CURL's framework, the meta algorithm faces two challenges:

**Uncertainty.** At the episode's end, the learner has full information about the objective function \(F^{t}\). If the learner also knew \(p^{t}\), they could recursively calculate the corresponding state-action distribution \(^{^{t,s,},p^{t}}\) using Eq. (2) and observe the actual loss of each expert, denoted as \(F^{t}(^{^{t,s,},p^{t}})\). However, given that \(p^{t}\) is unknown to the learner, the true loss remains unobservable. Consequently, the meta-algorithm needs to create an estimator \(^{t}\) for \(p^{t}\) and utilize it to estimate the losses. We propose a method to compute an estimator \(^{t}\) in Subsection 4.3.

**Convexity.** As discussed in Section 2, the objective functions \(F^{t}\) are not convex over the space of polices. However, CURL is equivalent to a convex problem over the state-action distributions satisfying the Bellman's flow as shown in Eq. (4). Therefore, instead of aggregating policies, the meta algorithm aggregates the associated state-action distributions using the probability estimator \(^{t}\) and the recursive scheme at Eq. (2). We detail MetaCURL in Alg. 1 when employed with the Exponentially Weighted Average forecaster (EWA) as the sleeping expert subroutine (we detail EWA in Appendix A.2).

### Building an estimator of \(p^{t}\)

As discussed earlier, applying the learning with experts framework requires estimating the loss of non-played expert policies, which depends on estimating the non-stationary transition probabilities \(^{t}\). Standard RL techniques for bounding the \(L_{1}\) norm between the empirical estimator \(^{t}\) and the true dynamics \(p^{t}\)[44; 49] are not applicable here due to non-stationarity. To address this, we introduce a second layer of sleeping experts for each \((n,x,a)[N]\), where each expert provides an empirical estimate of \(p^{t}\) based on different intervals. We then propose a new loss function in Eq. (12) and conduct a novel regret analysis in Prop. 5.2 to achieve the optimal regret rate.

In each episode \(t\), the learner calculates independent samples \(x^{t}_{n,x,a} p^{t}_{n}(|x,a)\) utilizing the external noise sequence \((^{t}_{n})_{n[N]}\) observed (just let \(x^{t}_{n,x,a}=g_{n-1}(x,a,^{t}_{n-1})\), see Eq. (8)). Each expert outputs an empirical estimator of \(p^{t}_{n}(|x,a)\) using samples across different intervals. We assume \(T\) experts, with expert \(s\) active in interval \([s,T]\). Expert \(s\) at episode \(t>s\) outputs:

\[^{t,s}_{n}(x^{}|x,a)=_{n,x,a}(x^{})}{(t-s)}, \;\;N^{s:t-1}_{n,x,a}(x^{}):=_{q=s}^{t-1} _{\{x^{q}_{n,x,a}=x^{}\}}.\]

We let \(^{t}_{n}(|x,a)\) be the result of employing sleeping EWA with experts \(^{t,s}_{n}(|x,a)\), for \(s<t\). Typically, in density estimation with EWA, a logarithmic loss \(-()\) is used. However, in this case \(-()\) can be unbounded, so we opt here for a smoothed logarithmic loss, given by, for all \(q_{}\),

\[^{t}(q):=_{x}-q(x)+|} _{\{^{t}_{n,x,a}=x\}},\;\;^{t}_{ n,x,a}p^{t}_{n}(|x,a)+|}/2.\] (12)

The definition of this non-standard loss is further clarified during the regret analysis in Section 5. This loss function is \(1\)-exp concave (see Lemma 4 of ), hence the cumulative regret of EWA with respect to each expert \(s[T]\), for all episodes \([s,T]\), satisfies \(^{}_{[s,]}(s)=_{t=s}^{}^{t}(^{t}_ {n}(|x,a))-^{t}(^{t,s}_{n}(|x,a))(T)\) (for more information on the regret bounds of EWA with exp-concave losses, see Appendix A.2). We describe the complete online scheme to compute \(^{t}\) in Alg. 3 at Appendix B.

## 5 Regret analysis

This section presents the main result concerning MetaCURL's regret analysis. Subsection 5.1 shows an upper bound for \(R^{}\) when MetaCURL is played with EWA and \(^{t}\) is computed as in Subsection. 4.3. Subsection 5.2 introduces a learning rate grid for MetaCURL when the black-box algorithm meets the dynamic regret criteria in Eq. (10), providing an upper bound for \(R^{}\). Given the dynamic regret decomposition of Eq. (11), we see that the combination of these results leads to our main result, the full proof of which can be found in appendix (F) :

**Theorem 5.1** (Main result).: _Let \((0,1)\). Playing MetaCURL, with a parametric black-box algorithm \(\) with dynamic regret as in Eq. (10), with a learning rate grid \(:=\{2^{-j}|j=0,1,2,,_{2}(T)/2\}\), and with EWA as the sleeping expert subroutine, we obtain, with probability at least \(1-2\), for any sequence of policies \((^{t,*})_{t[T]}\),_

\[R_{[T]}(^{t,*})_{t[T]}}T}+^{p}},\;T^{2/3}(^{p})^{1/3} }.\]

### Meta-algorithm analysis

Given the uncertainty in the probability transition, the meta regret term can be decomposed as follows:

\[R_{[T]}^{} =^{T}F^{t}(^{^{t},p^{t}})-F^{t}(^{ ^{t},^{t}})}_{R_{[T]}^{^{t}}(^{t})\,^{t}$ estimation)}}\] (13) \[+^{m}_{t_{i}}F^{t}(^{ ^{t},^{t}})-F^{t}(^{^{t},t_{i},},^{t})}_{ {sleeping LEA regret}}+^{m}_{t_{i}}F^{t}( ^{^{t},t_{i},},^{t})-F^{t}(^{^{t},t_{i},},p^ {t})}_{_{i=1}^{m}_{t_{i}}R_{_{i}}^{ ^{t}}(^{t},t_{i},)\,^{t}}).\]

**Sleeping LEA regret.** Referring to Thm. A.1 in Appendix A, using sleeping EWA as the sleeping expert subroutine of MetaCURL, with signals \(I^{t,s}=1\) for active experts (\(s t\)), experts' convex losses \(^{t,s,}:=F^{t}(^{^{t,s,},^{t}})\), and learner loss \(^{t}:=F^{t}(^{^{t},^{t}})\), yields, for any \(m[T]\) and for any set of intervals \(_{i}=[t_{i},t_{i+1}-1]\), with \(1=t_{1}<<t_{m+1}=T+1\),

\[_{i=1}^{m}_{t_{i}}F^{t}(^{^{t},^{t}})-F^{t}(^{^{t},t_{i},},^{t}) =_{i=1}^{m}_{_{i}}^{}(t_{i})\] (14) \[_{i=1}^{m}_{i}|}{2}(T| |)}(T||)}.\]

\(^{t}\) **Estimation regret.** In a scenario without uncertainty in the MDP's probability transitions, the meta-algorithm's regret would simply be bounded by Eq. (14), the sleeping expert regret used as a subroutine. However, given the presence of uncertainty, the main challenge in analyzing the meta-regret comes from the regret terms associated with the estimator \(^{t}\). We outline this analysis in Prop. 5.2.

**Proposition 5.2**.: _Let \((0,1)\), \(C:=||||2^{||}T}{}}\), and \(L_{F}\) be the Lipschitz constant of \(F^{t}\), with respect to the norm \(\|\|_{,1}\), for all \(t[T]\). With a probability of at least \(1-\), MetaCURL obtains_

\[R_{[T]}^{}(^{t}):=_{t=1}^{T}F^{t}(^{^{t},p^{t}})-F^{t}( ^{^{t},^{t}}) 2L_{F}N^{2}|||}C^{2/3} (T)^{1/3}T^{2/3}(^{p})^{1/3}.\]

_For any \(m[T]\) and for any set of intervals \(_{i}=[t_{i},t_{i+1}-1]\), with \(1=t_{1}<<t_{m+1}=T+1\), the same bound is valid for \(_{i=1}^{m}_{t_{i}}R_{_{i}}^{}(^{t,t _{i},})\)._Proof.: The proof idea is based mainly on the formulation of \(^{t}\) described in Subsection 4.3. We start by using the convexity of \(F^{t}\) to linearize the expression, then we apply Holder's inequality and exploit the \(L_{F}\)-Lipschitz property of \(F^{t}\) to establish an upper bound based on the \(L_{1}\) norm difference of the state-action distributions induced by the true probability transition and the estimator. Using Lemma C.5 in Appendix C, we then obtain that

\[R_{[T]}^{} L_{F}_{t=1}^{T}_{n=1}^{N}_{j=1}^{n}_{x,a} _{j-1}^{^{t}_{j} p^{t}}(x,a)\|p_{j}^{t}(|x,a)-_{j}^{t}( |x,a)\|_{1}.\]

To use the results from Subsection 4.3, we first regularize \(p^{t}\) and \(^{t}\), for each \((n,x,a)\), by averaging each with the uniform distribution over \(\), that we denote by \(p_{0}:=1/||\). As both probabilities are now lower bounded, we can employ Pinsker's inequality to convert the \(L_{1}\) norm into a KL divergence. The sum over \(t[T]\) of the KL divergence can then be decomposed as follows:

\[_{t=1}^{T}^{t}(|x,a)+p_{0}}{2 }_{j}^{t}(|x,a)+p_{0}}{2}=_{i=1}^{m}_{ t_{i}}^{t}(|x,a)+p_{0}}{2} _{j}^{t,t_{i}}(|x,a)+p_{0}}{2}\] \[+_{i=1}^{m}_{t_{i}}_{ _{j,x,a}^{t}}_{j}^{t,t_{i}}(_{j,x,a}^{t}|x,a )+p_{0}-_{j}^{t}(_{j,x,a}^{t}|x,a)+p_{0} ,\]

where \(_{j}^{t,t_{i}}(|x,a)\) is the empirical estimate of \(p_{j}^{t}(|x,a)\) calculated with the observed data from \(t_{i}\) to \(t-1\), and the expectation is over \(_{j,x,a}^{t}(p_{j}^{t}(|x,a)+p_{0})/2\). The second term is the cumulative regret of computing \(^{t}\) using EWA with loss as in Eq. (12), and is bounded by \(m(T)\). We finish and give more details of the proof in Appendix D. 

Prop. 5.2 together with Eq. (14) yields the main result of this subsection:

**Proposition 5.3** (Meta regret bound).: _With the same assumptions as Prop. 5.2, for any \(m[T]\), with probability at least \(1-2\),_

\[R_{[T]}^{} 4L_{F}N^{2}|||}C^{2/3} (T)^{1/3}T^{2/3}(^{p})^{1/3}+(T||)}.\]

### Black-box algorithm analysis

Assuming \(\) is a parametric black-box algorithm with dynamic regret satisfying Eq. (10) for any learning rate \(>0\), we only need to address the selection of the \(\)s grid and optimize across \(\) to achieve our final bound on \(R_{[T]}^{}\).

**Learning rate grid.** The dynamic regret of Eq. (10) implies that any two \(\) that are a constant factor of each other will guarantee the same upper-bound up to essentially the same constant factor. We therefore choose an exponentially spaced grid

\[:=2^{-j}|j=0,1,2,,_{2}(T)/2}.\] (15)

The meta-algorithm aggregation scheme guarantees that the learner performs as well as the best empirical learning rate. We thus obtain a bound on \(R_{[T]}^{}\), with its proof in Appendix E:

**Proposition 5.4** (Black-box regret bound).: _Assume MetaCURL is played with a black-box algorithm satisfying dynamic regret as in Eq. (10), with learning rate grid as in Eq. (15). Hence, for any sequence of policies \((^{t,*})_{t[T]}\),_

\[R_{[T]}^{}(^{t,*})_{t[T]} N ^{^{*}}+c_{3}}{c_{1}}+c_{1}+3(c_{2 }^{^{*}}+c_{3}m)T}+}{m}.\]

**Greedy MD-CURL.** Greedy MD-CURL, developed by , is a computationally efficient policy-optimization algorithm known for achieving sublinear static regret in online CURL with adversarial objective functions within a stationary MDP. In Thm. G.3 of Appendix G, we extend this analysis showing that Greedy MD-CURL also achieves dynamic regret as in Eq. (10). To our knowledge, this is the first dynamic regret result for a CURL algorithm. Hence, Greedy MD-CURL can be used as a black-box for MetaCURL. We detail Greedy MD-CURL in Alg. 4 in Appendix G.

Conclusion, discussion, and future work

In this paper, we present MetaCURL, the first algorithm for dealing with non-stationarity in CURL, a setting covering many problems in the literature that modifies the standard linear RL configuration, making typical RL techniques difficult to use. We also employ a novel approach to deal with non-stationarity in MDPs using the learning with expert advice framework from the online learning literature. The main difficulty in analyzing this method arises from uncertainty about probability transitions. We overcome this problem by employing a second expert scheme, and show that MetaCURL achieves near-optimal regret.

Compared to the RL literature, our approach is more efficient, deals with adversarial losses, and has a better regret dependency concerning the varying losses, but to do so, we need to simplify the assumptions about the dynamics (all uncertainty comes only from the external noise, that is independent of the agent's state-action). There seems to be a trade-off in RL: all algorithms dealing with both non-stationarity and full exploration use UCRL-type approaches, and are thus computationally expensive. We thus leave a question for future work: _How can we effectively manage non-stationarity and adversarial losses using efficient algorithms, all while addressing full exploration?_