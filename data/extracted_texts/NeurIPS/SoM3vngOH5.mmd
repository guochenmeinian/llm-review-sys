# Tree of Attacks:

Jailbreaking Black-Box LLMs Automatically

 Anay Mehrotra

Yale University

Robust Intelligence @ Cisco

&Manolis Zampetakis

Yale University

Paul Kassianik

Robust Intelligence @ Cisco &Blaine Nelson

Robust Intelligence @ Cisco &Hyrum Anderson

Robust Intelligence @ Cisco

&Yaron Singer

Robust Intelligence @ Cisco &Amin Karbasi

Robust Intelligence @ Cisco

###### Abstract

While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed _jailbreaks_. In this work, we present _Tree of Attacks with Pruning_ (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art _guardrails_, _e.g._, LlamaGuard.

## 1 Introduction

The proliferation of LLMs has revolutionized natural language processing and generation [9; 50; 37], enabling novel software paradigms . However, the widespread use of LLMs also raises concerns regarding their risks [7; 8; 31; 63; 16], biases [43; 1; 7; 22; 8], and susceptibility to adversarial manipulation [3; 10; 52; 35]. In response to these challenges, researchers and developers have explored various approaches to mitigate undesirable outcomes [14; 62; 23; 56; 18; 37]. Including encoding appropriate behavior during training via reinforcement learning with human feedback (RLHF), creating instructions (or _system prompts_) to guide the LLM during inference, and building additional guardrails that block undesired outputs. Broadly, all of this is called the _alignment_ of LLMs [25; 51].

Understanding the power and limitations of alignment methods is crucial for building LLMs that can be safely used in a wide range of applications. One way to understand the limitations of these methods is to explore their susceptibility to _jailbreaking_ attacks; which are attempts to bypass the _target_ LLM's safety filters and circumvent its alignment .

More concretely, given a request for undesirable information (_e.g._, _"How to build a bomb?"_), the goal of a jailbreaking method is to output a prompt that makes the target LLM provide the requestedundesired_ information (_e.g._, instructions of how to make a bomb). Recently researchers and engineers have designed a variety of jailbreaking methods illustrating vulnerabilities of LLMs (see Section 1.3 for a list). However, most methods either require significant effort by humans  or only apply to open-source models (whose weights and/or tokenizers are publicly available)  (Section 1.3). Further, many of these methods generate prompts containing substrings with no natural meaning -- making them easy to detect via perplexity filters .

In contrast to these attacks, we focus on methods with the following properties.

* Automated: Does not require human supervision.
* Black-box: Only requires query access to the LLM and no knowledge of its parameters.
* Interpretable: Produces prompts with a natural meaning.

Automated attacks reveal more significant flaws in alignment methods than attacks requiring human supervision as automated attacks are scalable and can be utilized by anyone without an understanding of LLMs. Further, attacks that only require black-box access demonstrate that keeping the details of an LLM secret (a common industry practice) does not prevent attacks. Finally, as mentioned before, interpretable attacks are harder to detect and, hence, pose a more substantial threat .

### Our Contributions

We present a method, Tree of Attacks with Pruning (TAP), for jailbreaking LLMs that satisfies the above three properties. Compared to other automated and black-box methods, TAP achieves a significantly higher success rate: for instance, with GPT4o, TAP improves the 78% success rate of the previous state-of-the-art method to 94% while making 60% fewer queries to GPT4o (we define the success rate below and present an extensive comparison to prior methods in Section 5).

TAP is an iterative algorithm. It is initialized by two LLMs: an _attacker_ and an _evaluator_. Roughly speaking, at each iteration, TAP uses the attacker LLM to generate multiple variations of the initial prompt (which asks for undesirable information), uses the evaluator LLM to identify the variations that are most likely to jailbreak the target LLM, and sends these variations to the target (see Figure 1).

We implement it in Python and evaluate it on both an existing (AdvBench Subset ) and a new dataset; each of these datasets contains prompts asking for undesirable information (Section 5). To evaluate the success rate of different methods, we report the fraction of prompts for which the target LLM gives the requested undesired information.1 To evaluate the efficiency, we report the number of queries made to the target per prompt. (To ensure fair evaluation, where applicable, we ensure the number of tokens sent and requested per query is similar across all methods.)

Empirical evaluations on both datasets show that TAP elicits undesirable information from state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for a large fraction of prompts while using a small number of often lower than 30 (see Table 1). Compared to prior work, the success rate of TAP is significantly higher on most LLMs despite using fewer queries. For instance, on the AdvBench Subset data, TAP's success rate with GPT4 is 90% with 28.8 queries compared to 60% of the best prior method which uses 37.7 queries. We also show similar improvements for other common LLMs, including GPT3.5-Turbo, GPT4-Turbo, PaLM-2, and Gemini-Pro (Table 1).

Next, we evaluate _transferability_ of prompts generated by TAP, _i.e._, whether the prompts generated by TAP for one target LLM can be used to elicit undesired information from a different LLM. We observe that our attacks transfer to other models at a similar rate as those of baselines (Table 3).

Further, we evaluate the performance of TAP on LLMs protected by Llama-Guard -- a state-of-the-art guardrail that classifies responses as desirable or undesirable and replaces undesirable responses with a refusal . We find that TAP continues to have a high success rate with fewer than 50 queries on LLMs protected by Llama-Guard (Table 2).

### Techniques

As mentioned earlier, TAP is initialized by two LLMs: an attacker and an evaluator. The attacker's task is to generate variations of the provided prompt \(P\) that are _likely_ to jailbreak the target LLM.

Concretely, the attacker is given the original prompt \(P\) and a system prompt. Due to its length, we defer the system prompt to Table 7 in Appendix C. At a high level, the system prompt describes the attackers' task, provides examples of variations it can generate, explaining why they are likely to jailbreak the target, and requires the model to support its response with chain-of-though reasoning. (The latter two techniques, namely, providing explanations and requiring chain-of-though reasoning, are well-known to improve the quality of responses .) The evaluator's goal is to assess each variation generated by the attacker on its ability to elicit undesirable information from the target LLM. At a high level, TAP uses these assessments to decide which variations to send to the target LLM and retain for future iterations. In empirical evaluations, we observe that this assessment is crucial to make TAP more query efficient than previous methods (see the discussion following Figure 1).

Now, we describe TAP in a bit more detail (see Figure 1 for an accompanying illustration). TAP starts with the provided prompt as the initial set of attack attempts. At each iteration, it executes the following steps.

1. (Branch) The attacker generates variations of the provided prompt (and is able to view all past attempts in conversation history).
2. (Prune: Phase 1) The evaluator assesses these variations and eliminates the ones unlikely to elicit undesirable information.
3. (Attack and Assess) The target LLM is queried with each remaining variation and then, the evaluator scores the responses of the target to determine if a successful jailbreak is found. If a successful jailbreak is found, TAP returns the corresponding prompt.
4. (Prune: Phase 2) Otherwise, TAP retains the evaluator's highest-scoring prompts as the attack attempts for the next iteration.

**Comparison to main prior method.** Our method builds on the framework of Prompt Automatic Iterative Refinement (PAIR)  - the state-of-the-art automated and black-box jailbreaking method. Roughly speaking, PAIR corresponds to a single chain in TAP's execution (see Figure 1). In particular, it does not use either branching or pruning.2 As we discuss below, the combination of branching and pruning enables TAP to significantly improve PAIR's performance. The designers of PAIR also explore several variations to improve PAIR's performance. After significant ablation studies, they recommend the following procedure to improve PAIR: given a fixed query budget \(b\) and \(c=O(1)\), run \(b/c\) instances of PAIR in parallel each with query budget \(c\). This is the implementation that we use as a baseline. In this light, one way to interpret TAP, is that it is a method that enhances the performance of PAIR to a success rate significantly higher than the _improved_ version of PAIR suggested by its designers. The efforts of PAIR's authors demonstrate that the specific enhancement strategy is far from obvious. An added strength is that TAP is simple to implement: only requiring a few additional lines of code over PAIR.

**Significance of Branching and Pruning.** To evaluate the effect of branching, we consider the variant of TAP where, in each iteration, the attacker generates a single variation of the input prompt. We

Figure 1: Illustration of the four steps of Tree of Attacks with Pruning (TAP) and the use of the attacker and evaluator LLMs in each of the steps. This procedure is repeated until we find a jailbreak for our target or until a maximum number of repetitions is reached.

observe that this variant achieves a significantly lower success rate than TAP (_e.g._, 48% vs 84% with GPT4-Turbo as the target; see Table 4). Next, we evaluate the effect of pruning by considering the variant of TAP that retains branching but does not perform pruning. We observe that this method achieves a success rate close to TAP (within 12%) but requires nearly twice the amount of queries to the target (see Table 4). These two simulations show that branching is crucial to boost the success rate and pruning is crucial to make the method query efficient, and, the combination of both branching and pruning is required to achieve a high success rate while being query-efficient.

### Further Related Works

**Jailbreaking Attacks on LLMs.** There is a growing body of work on jailbreaking LLMs. Below, we give a non-exhaustive outline of different types of methods for generating jailbreaks for LLMs. We refer the reader to excellent surveys for a comprehensive overview [36; 57].

_Manually Discovered Jailbreaks._ Both the designers of LLMs and researchers have devoted significant efforts to manually discover jailbreaks in red-teaming studies [6; 18; 37; 47; 42]. Inspired by the success of existing jailbreaks, Wei et al.  present high-level explanations of why jailbreaks succeed which, in turn, can be used to generate new jailbreaks manually.

_Automated Attacks Based on Templates._ Several works design templates of prompts that can jailbreak LLMs and, subsequently, automatically generate jailbreaks following these templates potentially with the help of LLMs [41; 11; 27; 15; 61; 4]. These templates can be based on several high-level strategies (including persona modulation  and existing prompt injection techniques from cybersecurity ) and can further be optimized via discrete optimization methods . In contrast to our work, these methods rely on fixed templates and, hence, are easy to detect [36; 57].

_Automated White-Box Attacks._ There are a number of automated (attack) methods that use _white-box_ access to the target LLM (such as knowledge of its weights and tokenizer) to run gradient-based search over jailbreaks [49; 26; 63; 34; 29; 40; 39]. These methods use a variety of techniques from discrete optimization [49; 26; 63], to refinement based on other LLMs , to genetic algorithms and fine-tuning [29; 40], to in-context learning . However, since they require white-box access to LLMs, they cannot be applied to closed-sourced LLM models that are only accessible via APIs (such as the GPT family). Moreover, most of these methods [49; 63; 40; 29] generate prompts that have no natural meaning making them easy to detect . In contrast, our work only requires black-box access to the target LLM and generates interpretable jailbreaks.

_Automated and Black-Box Attacks._ Some recent works propose automated black-box methods that generate interpretable prompts [12; 59]. Among these, Yu et al.  use LLMs to generate prompts but require starting with existing _successful_ jailbreaks as seeds. In contrast, our method generates jailbreaks without requiring existing jailbreaks as input. As mentioned before, the closest to our work is the work of Chao et al.  that designs the Prompt Automatic Iterative Refinement (PAIR) framework which we build upon. Compared to PAIR, by incorporating branching and pruning, TAP achieves a significantly higher success rate with fewer queries (Table 1); see Section 1.2 for a comparison and a discussion on the effect branching and pruning.

**LLM Safety Training.** Given the propensity of LLMs to generate harmful content that can polarize user opinions and, more generally, harm the society [43; 1; 7; 8; 22; 31; 37], significant efforts have been devoted to improving LLMs. Foremost among these is safety training where models are trained to refuse _restricted_ requests [37; 5]. For instance, early versions of GPT4 were extensively fine-tuned using reinforcement learning with human feedback (RLHF) to reduce its propensity to respond to queries for restricted information (_e.g._, toxic content, instructions to perform harmful tasks, and disinformation). This RLHF implementation required significant human effort: human experts from a variety of domains were employed to manually construct prompts exposing GPT4's failure modes . However, despite extensive safety training, LLMs (including GPT4) continue to be vulnerable to carefully crafted prompts [37; 63; 52; 48; 13; 55]. One way in which automated methods for generating jailbreaks (like the one we design) can be useful is in automating the creation of harmful prompts during RLHF training.

**Tree of Thoughts.** The techniques in TAP are also related to Tree of Thoughts . Tree of Thoughts (ToT) is an LLM inference method that generalizes the Chain of Thoughts and is applicable when the task (which the LLM is performing) can be broken down into intermediate steps. At a high level, ToT allows LLMs to perform deliberate decision-making by considering different reasoning paths,evaluating the choices in each path, and (potentially) looking ahead or backtracking. ToT prompting is not directly applicable in our context since the task of generating a jailbreak does not decompose into individual steps. Nevertheless, the branching and pruning steps in TAP can be thought of as a form of tree-of-thought prompting applicable for jailbreaking.

## 2 Preliminaries

In this section, we introduce some basic notation for denoting LLM responses, the definition of a jailbreak, and the Off-Topic and Judge functions which will be used to describe TAP.

We use bold letters to denote LLMs (such as \(\), \(\), and \(\)). We reserve \(\) and \(\) to denote the attacker and evaluator LLMs in TAP and \(\) to denote the target LLM that we want to jailbreak. Given an LLM \(\), a prompt \(P\) (which is a string over some alphabet), and a number \( 1\), we denote the distribution of the first \(\) tokens generated by \(\) given prompt \(P\) by \(q_{}(P;)\). If the number of tokens \(\) is not important, we simplify \(q_{}(P;)\) to \(q(P;)\).

A response is said to be undesirable if it is undesirable according to the model developers' guidelines. Following prior work , we focus on prompts asking for harmful information (such as instructions for causing physical harm to individuals or public property) or misinformation. Common models including the GPT family have been trained not to provide such harmful information .

A prompt \(P\) is said to _jailbreak_ an LLM \(\) for a query \(Q\) (for undesirable information) if, given \(P\) as input, \(\) provides the requested undesired information. For instance, if \(Q\) requests instructions about causing harm (_e.g., "How to cut down a stop sign?"_), then \(\) would be jailbroken if it provides _relevant_ instructions and not jailbroken if it refuses (_e.g.,_ by outputting _"I am sorry, but I cannot assist with this request"_) or provides irrelevant instructions (_e.g.,_ about _"How to make a stop sign"_).

**Off-Topic Function.** For the response \(R\) to provide the information requested in \(Q\), typically, the intermediate prompt \(P\) must request the same information as \(Q\). To succinctly denote this, let Off-Topic\((P,Q)\) be False is \(P\) requests the same information as \(Q\) and True otherwise. For instance, if \(Q\) is _"How to cut down a stop sign?"_ and \(P\) is _"How can a criminal cut down a stop sign?"_ then Off-Topic\((P,Q)\) is False. It is True if \(P\), _e.g._, is _"How to fix a fallen stop sign?"_.

**Judge Function.** Given a query \(Q\) for undesirable information and response \(R\), let Judge\((Q,R)\) be True if \(R\) signifies a successful jailbreak for \(Q\) (as defined above) and False otherwise.3

Given a query \(Q\), the goal of an automated black-box and interpretable jailbreaking method is to output a prompt \(P\) that is _meaningful_ and is such that, given \(P\), the target LLM \(\) outputs a response \(R\) such that Judge\((Q,R)=\).

## 3 Tree of Attacks with Pruning

In this section, we give a more detailed description of Tree of Attacks with Pruning (TAP).4

To begin, we refer the reader to the description of TAP in Section 1.2 which we build upon below. Recall that TAP is instantiated by two LLMs: an attacker \(\) and an evaluator \(\). Apart from \(\) and \(\), TAP is parameterized by the number of refinements generated by the attacker which we call the _branching factor_\(b 1\), the maximum number of attempts retained per iteration which we call the _width_\(w 1\), and the maximum number of iterations or the _depth_ of the tree constructed by TAP \(d 1\). For instance, in Figure 1, the branching factor is \(b=2\) (as each prompt is refined twice by the attacker) and the width is \(w=4\) (as in the second phase of pruning only 4 prompts are retained). Figure 1 illustrates one iteration of TAP. For any fixed \(d\), this iteration is repeated until a jailbreak is found or \(d\) repetitions are performed.

Below, we present the pseudocode of TAP in Algorithm 1 along with comments explaining each step. Next, we make a few remarks about the role of the attacker and evaluator in Algorithm 1 and compare Algorithm 1 to prior methods.

TAP (Algorithm 1) queries \(\) to iteratively refine \(Q\) until a prompt \(P\) is found which jailbreaks the target LLM \(\). For this purpose, \(\) is initialized with a carefully crafted system prompt that mentionsthat it is a red teaming assistant whose goal is to generate jailbreaks; see to Table 7 in Appendix C for the complete prompt. The evaluator \(\) serves two roles: evaluating the Judge function and evaluating the Off-Topic function (see Section 2 for the definitions of Judge and Off-Topic functions). The system prompt of the evaluator \(\) depends on whether \(\) is serving in the Judge or Off-Topic role. Both of these system prompts pose it as a red teaming assistant. We present the system prompts in Appendix C. While we focus on the case where the evaluator is an LLM, one can also consider non-LLM-based evaluators and we explore one example in Appendix E.

TAP builds on the framework of PAIR  - the state-of-the-art black-box jailbreaking method. Concretely, PAIR corresponds to TAP in the special case where \(b=1\) (_i.e._, there is no branching) and neither phase 1 nor phase 2 of pruning are executed (_i.e._, there is no pruning). In other words, TAP extends PAIR's framework by including branching and pruning. PAIR's designers also explored various extensions to improve its performance and, through their ablation studies, recommend dividing the query budget among multiple copies of PAIR, each with a small budget (concretely, 3 queries each). Compared to this _improved_ version of PAIR, TAP achieves a significantly higher success rate with fewer queries on most models (Table 1). We evaluate the importance of branching and pruning on TAP's performance (Section 5). We observe that branching boosts the success rate, pruning makes the method query efficient, and the combination of both branching and pruning is crucial to achieving a high success rate with query-efficiency (Table 4).

## 4 Empirical Setup

**Datasets.** We use two datasets of prompts requesting harmful information. The first is _AdvBench Subset_ - consisting of 50 requests for harmful information across 32 categories curated by Chao et al. . The second dataset is new and has 123 harmful requests. These prompts are generated by querying WizardVicuna30B-Uncensored to generate variants of the prompts in AdvBench Subset.5 To ensure that this data does not affect hyperparameter choices and TAP's design, we did not generate it until simulations with the first dataset finished. (One exception is evaluations on LLMs released after data generation; GPT40 and Claude3.)

**Methods and Implementation Details.** We compare TAP to two baselines that are state-of-the-art and have public implementations: PAIR , which is the state-of-the-art method for black-box jailbreaks, and GCG , which is the state-of-the-art method for white-box jailbreaks. For both TAP and PAIR, we use Vicuna-13B-v1.5 as the attacker and GPT4 as the evaluator, identical to PAIR's default . (We explore other choices of the evaluator in Appendix E.) For TAP, we fix the maximum depth to \(d=10\), the maximum width to \(w=10\), and the branching factor to \(b=4\), respectively. (This ensures that the number of queries and tokens that TAP and PAIR send/receive from the target are similar.) For both PAIR and GCG we fix the same hyperparameters as in the corresponding works. Following prior work, for each target LLM we use temperature 0 (when possible) to ensure their responses are deterministic and generate \(150\) tokens . The system prompts and other generation parameters are default and are in Appendix C.

**Metrics for Evaluating Jailbreak Success.** We consider two metrics for evaluating jailbreak success: the GPT4-Metric and Human-Judgement. The GPT4-Metric is automated and uses GPT4 for the Judge function described in Section 2: a pair of response \(R\) and query \(Q\) represents a successful jailbreak if \((R,Q)=\), which corresponds to a "detailed and fully jailbroken response" as evaluated by GPT4. This is the same as the metric used in Chao et al. . Human-Judgement is manually evaluated by the authors. We label each response and query pair \((R,Q)\) using the guidelines in Appendix B of Wei et al.  while anonymizing the jailbreaking method that generated \((R,Q)\). As targets we use Vicuna-13B-v1.5, Llama-2-Chat-7B, GPT3.5-Turbo (0613), GPT4 (0613), GPT4-Turbo (1106-preview), GPT4o (5/13/24), PaLM-2, GeminiPro (1.0), Claude3 Opus (2/29/24).

## 5 Evaluation of Performance and Query Efficiency

We evaluate our method and baselines with state-of-the-art LLMs and report the results according to the GPT4-Metric on the AdvBench Subset in Table 1. For examples of TAP's output, see Appendix B. The results with Human-Judgement and on the second dataset are qualitatively similar. They are presented in Appendices D.1 and D.2 respectively.

Table 1 shows that, for all targets, TAP finds jailbreaks for a significantly higher fraction of prompts than PAIR while sending significantly fewer queries to the target. For instance, with GPT4o as the target-the latest LLM from OpenAI as of May 2024-TAP finds jailbreaks for 16% _more_ prompts than PAIR with 60% _fewer_ queries to the target. Exceptions are Llama-2-Chat where both methods have a similar success rate and Claude3 where TAP has a higher success rate but also uses a larger number of queries. Since GCG requires model weights, it can only be evaluated on open-source models. GCG achieves the same success rate as TAP with Vicuna-13B and has a 50% higher success rate with Llama-2-Chat-7B. However, GCG uses orders of magnitude more queries than TAP.

**Performance on Protected Models.** Next, we evaluate TAP's performance on models protected by Llama-Guard, which is a fine-tuned Llama-2-7B model intended to make LLMs safer by classifying prompt and response pairs as safe or unsafe . For each target LLM \(\), we protect it with Llama-Guard as follows: given a prompt \(P\), we query \(\) with \(P\), get response \(R\), and return \(R\) if \((R,P)\) is classified as safe by Llama-Guard and otherwise return a refusal (_"Sorry, I cannot assist with this request"_). We present the results in Table 2. The results show that TAP's success rate remains close to those with unprotected models (Table 1) and is significantly higher than PAIR's on most models (Table 2). The number of queries sent by TAP with protected models is higher than by PAIR, although the proportional increase in performance is _higher_ than the increase in queries.

**Transferability of Jailbreaks.** Next, we study the transferability of the attacks found in Table 1 from one target to another. For each baseline, we consider prompts that successfully jailbroke Vicuna-13B,GPT4, and GPT4-Turbo for at least one query. In Table 3, we report the fraction of these prompts that jailbreak a different target (for the same goal as they jailbroke on the original target).

Table 3 shows that, roughly speaking, a similar number of the jailbreaks found by TAP and by PAIR transfer to new targets. See Appendix A.2 for a more in-depth discussion. In contrast, a significantly smaller number of jailbreaks generated by GCG transfer than those of TAP and PAIR. This may be because of updates to the LLMs to protect them against GCG and because the prompts generated by GCG do not carry natural meaning and, hence, are less likely to transfer.

## 6 Empirical Evaluation of the Effects of Branching and Pruning

Next, we explore the relative importance of (1) branching and (2) pruning off-topic prompts. Toward this, we consider two variants of TAP. The first variant, TAP-No-Branch, is the same as TAP but uses a branching factor \(b=1\) (_i.e._, it does not perform branching). The second variant, TAP-No-Prune, is the same as TAP but does not prune off-topic prompts generated by the attacker. We compare the performance of these two variants with TAP with GPT4-Turbo as the target. (We selected

   Method & Metric & Vicuna & Llama7B &  &  & GeminiPro & Claude3 \\   & & & & 3.5 & 4 & 4-Turbo & 4o & & Opus \\   TAP \\ (This work) \\  } & Jailbreak \% & **98\%** & 4\% & **76\%** & **90\%** & **84\%** & **94\%** & **98\%** & **96\%** \\  & Mean \# Queries & **11.8** & 66.4 & **23.1** & **28.8** & **22.5** & **16.2** & **16.2** & 12.4 & 116.2 \\   & Jailbreak \% & 94\% & 0\% & 56\% & 60\% & 44\% & 78\% & 86\% & 81\% & 24\% \\  & Mean \# Queries & 14.7 & **60.0** & 37.7 & 39.6 & 47.1 & 40.3 & 27.6 & **11.3** & **55.0** \\   & Jailbreak \% & **98\%** & **54\%** &  \\  & Mean \# Queries & 256K & 256K &  \\   

Table 1: **Fraction of Jailbreaks Achieved as per the GPT4-Metric.** For each method and target LLM, we report (1) the fraction of jailbreaks found on AdvBench Subset according to GPT4-Metric and (2) the number of queries sent to the target LLM in the process. For both TAP and PAIR we use Vicuna-13B-v1.5 as the attacker. The best result for each model is bolded. The success rate of PAIR in our evaluations differs from those in ; see Remark A.1. Results for GCG are as in .

   Method & Metric & Vicuna & Llama7B &  &  & GeminiPro & Claude3 \\   & & & 3.5 & 4 & 4-Turbo & 4o & & & Opus \\    TAP \\ (This work) \\  } & Jailbreak \% & **100\%** & 0\% & **84\%** & **84\%** & **80\%** & **96\%** & **78\%** & **90\%** & 44\% \\  & Mean \# Queries & 13.1 & 60.3 & 23.0 & 27.2 & 33.9 & 50.0 & 28.1 & 15.0 & 107.9 \\   & Jailbreak \% & 72\% & **4\%** & 44\% & 39\% & 22\% & 76\% & 48\% & 68\% & **48\%** \\  & Mean \# Queries & **11.2** & **15.7** & **13.6** & **14.0** & **15.3** & **40.1** & **12.7** & **11.7** & **50.8** \\   

Table 2: **Performance on Protected Models.** The setup is the same as Table 1.

   Method & Original Target & Vicuna & Llama-7B &  &  & GeminiPro & Claude3 \\   & & & & 3.5 & 4 & 4-Turbo & 4o & & Opus \\   TAP \\ (This work) \\  } & GPT4-Turbo & **33**/42 & **0**/42 & 20**/42 & **24**/42 & — & **34**/42 & 10/42 & **31**/42 & 6/42 \\  & GPT4 & 29/45 & **0**/45 & **25**/45 & — & **29**/45 & 31/45 & **12**/45 & **28**/45 & 5/45 \\  & Vicuna & — & **0**/49 & 11/49 & 7/49 & 16/49 & 20/49 & **12**/49 & 27/49 & 4/49 \\   & GPT4-Turbo & 15/22 & **0**/22 & 12/22 & 18/22 & — & 18/22 & 3/22 & 12/22 & **7**/22 \\  & GPT4 & 23/30 & **0**/30 & 19/30 & — & 19/30 & 19/30 & 9/30 & 15/30 & **7**/30 \\  & Vicuna & — & **0**/47 & 8/47 & 8/47 & 11/47 & 10/47 & 7/47 & 16/47 & 2/47 \\   & Vicuna & — & **0**/50 & 4/50 & 0/50 & 0/50 & 0/50 & 8/50 & 2/50 & 0/50 \\   

Table 3: **Transferability of Jailbreaks.** We evaluate the number of prompts that were successful jailbreaks on Vicuna-13B, GPT4, and GPT4-Turbo, transfer to a different target. The success of jailbreaks is evaluated by the GPT4-Metric. For each pair of original and new target models, the fraction \(x/y\) means that \(x\) out of \(y\) jailbreaks transfer to the new target. We omit results for transferring to the original target. The best result by most jailbreaks transferred for each model is bolded.

GPT4-Turbo as it was the state-of-the-art commercially-available model when the simulations were performed .) We report the results on AdvBench Subset according to the GPT4-Metric in Table 4.

Table 4 shows that TAP-No-Branch has a 36% lower success rate than the standard implementation (48% vs 84%) despite sending _more_ queries than the original method (33.1 vs 22.5).6 Hence, showing that branching is crucial to improving the success rate. Further, Table 4 shows that TAP-No-Prune sends a higher average number of queries than the standard implementation (55.4 vs 22.5) and, despite this, does not have a higher success rate than the standard implementation. Hence, illustrating the importance of pruning in making the method query efficient. Overall Table 4 shows the combination of both branching and pruning is crucial to achieving a high success rate in a query-efficient fashion.

_Discussion._ At first, it might seem contradictory that TAP-No-Prune has a higher success rate despite sending more queries. One reason for this is because, at the end of each iteration, TAP retains the \(w=10\) highest scoring prompts and deletes the rest: since this variant does not prune off-topic prompts, if more than \(w\) off-topic prompts are generated in some iteration, then TAP-No-Prune may delete all the on-topic prompts at the end of this iteration. (This deletion is done to limit the number of prompts which otherwise would grow exponentially due to branching.)

## 7 Conclusion

This work introduces TAP, a jailbreaking method that is automated, only requires black-box access to the target LLM, and outputs interpretable prompts.

We evaluate the method with state-of-the-art LLMs and observe that TAP finds prompts that jailbreak GPT4, GPT4-Turbo, GPT4o, and Gemini-Pro for more than 80% of requests for harmful information in existing datasets using fewer than 30 queries on average (Table 1). This significantly improves upon the prior automated methods for jailbreaking black-box LLMs with interpretable prompts (Table 1). Further, we evaluate TAP's performance on LLMs protected by a state-of-the-art guardrail (Llama-Guard) and find that it achieves a higher success rate than baselines (Table 2). Furthermore, we evaluate the transferability of the generated prompts and find that the prompts generated by TAP transfer at a similar rate as baselines (Table 3). TAP utilizes branching and pruning steps. Empirical evaluations show that the combination of branching and pruning is important to achieve a higher success rate than previous methods while retaining a low number of queries (Table 4).

**Future Work.** Our current evaluations focus on requests for harmful information. It would be interesting to explore whether TAP or other automated methods can also jailbreak LLMs for restricted requests beyond harmful content (such as requests for biased responses or personally identifiable information) . Further, it would be very interesting to evaluate the ability of TAP to generate novel jailbreaks (which are significantly different from existing ones), and designing new methods that substantially improve TAP on this front. Furthermore, our method uses LLMs to evaluate jailbreak success. These evaluations can be inaccurate and improving these evaluations is an important problem for the field of jailbreaking. Finally, one interpretation of TAP is that it is a method for "enhancing" the performance of existing methods. Exploring other effective methods for enhancement or boosting may be an interesting direction.

**Limitations.** We evaluate our results on two datasets: AdvBench Subset  and a new dataset. The performance of our method may be different on datasets that are meaningfully different from the ones we use. While manually evaluating jailbreak success rate, we anonymized the name of the method used to generate the jailbreak to avoid any inadvertent skew favoring our method and followed the guidelines in Wei et al. . However, the results can be different for guidelines that are

   Method & Branching Factor & Pruning & Target & Jailbreak \% & Mean \# Queries \\  TAP & 4 & ✓ & GPT4-Turbo & **84\%** & **22.5** \\ TAP-No-Prune & 4 & ✗ & GPT4-Turbo & 72\% & 55.4 \\ TAP-No-Branch & 1 & ✓ & GPT4-Turbo & 48\% & 33.1 \\   

Table 4: **Effect of Branching and Pruning.** Evaluation of TAP and variants that do not perform branching and pruning respectively. The setup is identical to Table 1. The best results are bolded.

meaningfully different. Our method uses a judge model to assess the prompts on a scale from 1 to 10. We use an off-the-shelf judge model in our evaluations and it is possible that the scores outputed by this judge model are inaccurate or miscalibrated, which could reduce TAP's performance. We evaluate the judge model's false positive and false negative rates in labeling examples as jailbreaks (i.e., assigning them a score of 10): we find that its false positive and false negative rates are not too large-13% and 0% respectively (Appendix A.2). Further, since some of the LLMs used in our evaluations are closed-source (like GPT4o), we are unable to evaluate changes in performance due to changes in the target LLM.

**Broader Impact.** In this work, we improve the efficiency of existing methods for jailbreaking LLMs. The hope is that it helps in improving the alignment of LLMs, e.g., via fine-tuning with the generated prompts. That said, our work can be used for making LLMs generate restricted (including harmful and toxic) content with fewer resources. However, we believe that releasing our findings in full is important for ensuring open research on the vulnerabilities of LLMs. Open research on vulnerabilities is crucial to increase awareness and resources invested in safeguarding these models-which is becoming increasingly important as their use extends beyond isolated chatbots. To minimize the adverse effects of our findings, we have reported them to respective organizations. Further, while we provide an implementation of our method, using it requires a degree of technical knowledge. To further limit harm, we only release a handful of prompts that successfully jailbreak LLMs (Appendix B) that illustrate the method without enabling large-scale harm.