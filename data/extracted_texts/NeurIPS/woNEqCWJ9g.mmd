# Distributionally Robust Optimisation with Bayesian Ambiguity Sets

Charita Dellaporta

Department of Statistics

University of Warwick

C.Dellaporta@warwick.ac.uk

&Patrick O'Hara

Department of Computer Science

University of Warwick

Patrick.H.O-Hara@warwick.ac.uk

&Theodoros Damoulas

Department of Computer Science & Department of Statistics

University of Warwick

T.Damoulas@warwick.ac.uk

These authors contributed equally to this work.

###### Abstract

Decision making under uncertainty is challenging since the data-generating process (DGP) is often unknown. Bayesian inference proceeds by estimating the DGP through posterior beliefs about the model's parameters. However, minimising the expected risk under these posterior beliefs can lead to sub-optimal decisions due to model uncertainty or limited, noisy observations. To address this, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity Sets (DROBAS) which hedges against uncertainty in the model by optimising the worst-case risk over a posterior-informed ambiguity set. We show that our method admits a closed-form dual representation for many exponential family members and showcase its improved out-of-sample robustness against existing Bayesian DRO methodology in the Newsvendor problem.

## 1 Introduction

Decision-makers are regularly confronted with the problem of optimising an objective under uncertainty. Let \(x^{d}\) be a decision-making variable that minimises a stochastic objective function \(f:^{d}\), where \(\) is the data space and let \(^{*}()\) be the data-generating process (DGP) where \(()\) is the space of Borel distributions over \(\). In practice, we do not have access to \(^{*}\) but to \(n\) independently and identically distributed (i.i.d.) observations \(:=_{1:n}^{*}\). Without knowledge of the DGP, model-based inference considers a family of models \(_{}:=\{_{}:\} ()\) where each \(_{}\) has probability density function \(p(|)\) for parameter space \(^{k}\). In a Bayesian framework, data \(\) is combined with a prior \(()\) to obtain posterior beliefs about \(\) through \((|)\). Bayesian Risk Optimisation (Wu et al., 2018) then solves a stochastic optimisation problem:

\[_{x^{d}}\ _{(|)}[ _{_{}}f(x,)].\] (1)

However, our Bayesian estimator is likely different from the true DGP due to model and data uncertainty: the number of observations may be small; the data noisy; or the prior or model may be misspecified. The optimisation problem (1) inherits any estimation error, and leads to overly optimistic decisions on out-of-sample scenarios even if the estimator is unbiased: this phenomenon iscalled the optimiser's curse (Kuhn et al., 2019). For example, if the number of observations is small and the prior is overly concentrated, then the decision is likely to be overly optimistic.

To hedge against the uncertainty of the estimated distribution, the field of Distributionally Robust Optimisation (DRO) minimises the expected objective function under the worst-case distribution that lies in an ambiguity set \(U()\). Discrepancy-based ambiguity sets contain distributions that are close to a nominal distribution in the sense of some discrepancy measure such as the Kullback-Leibler (KL) divergence (Hu and Hong, 2013), Wasserstein distance (Kuhn et al., 2019) or Maximum Mean Discrepancy (Staib and Jegelka, 2019). For example, some model-based methods (Iyengar et al., 2023; Michel et al., 2021, 2022) consider a family of parametric models and create discrepancy-based ambiguity sets centered on the fitted model. However, uncertainty about the parameters is not captured in these works, which can lead to a nominal distribution far away from the DGP when the data is limited. The established framework for capturing such uncertainty is Bayesian inference.

The closest work to ours, using parametric Bayesian inference to inform the optimisation problem, is Bayesian DRO (BDRO) by Shapiro et al. (2023). BDRO constructs discrepancy-based ambiguity sets with the KL divergence and takes an _expected worst-case_ approach, under the posterior distribution. More specifically, let \(U_{}(_{}):=\{():d_{}(\|_{})\}\) be the ambiguity set centered on distribution \(_{}\) with parameter \([0,)\) controlling the size of the ambiguity set. Under the expected value of the posterior, Bayesian DRO solves:

\[_{x^{d}}\ _{(|)} \ _{ U_{}(_{})}_{ }[f(x,)],\] (2)

where \(_{(|)}[Y]:=_{}\ Y() (\ |\ )\,d\) denotes the expectation of random variable \(Y:\) with respect to \((\ |\ )\). A decision maker is often interested in protecting against and quantifying the worst-case risk, but BDRO does not correspond to a worst-case risk analysis. Moreover, the BDRO dual problem is a two-stage stochastic problem that involves a double expectation over the posterior and likelihood. To get a good approximation of the dual problem, a large number of samples are required, which increases the solve time of the dual problem.

We introduce DRO with Bayesian Ambiguity Sets (DRO-BAS), an alternative optimisation objective for Bayesian decision-making under uncertainty, based on a posterior-informed ambiguity set. The resulting problem corresponds to a worst-case risk minimisation over distributions with small expected deviation from the candidate model. We go beyond ball-based ambiguity sets, which are dependent on a single nominal distribution, by _allowing the shape of the ambiguity set to be informed by the posterior_. For many exponential family models, we show that the dual formulation of DRO-BAS is an efficient single-stage stochastic program.

Figure 1: Illustration of the construction of the BDRO and DRO-BAS optimisation problems for three i.i.d. posterior samples \(_{1},_{2},_{3}(\ |\ )\). BDRO seeks the decision that minimises the average worst-case risk between the three ambiguity sets shown in figure (a) whereas DRO-BAS targets the decision minimising the worst-case risk over the ambiguity set shown in (b).

## 2 DRO with Bayesian Ambiguity Sets

We propose the following DRO-BAS objective:

\[_{x^{d}}\ _{:_{}[D( ,_{})]}\ _{}\ [f_{x}()],\] (3)

where \(()\) is a distribution in the ambiguity set, \(f_{x}():=f(x,)\) is the objective function, \(D:()()\) is a divergence, and \([0,)\) is a tolerance level. The ambiguity set is informed by the posterior distribution \(\) by considering all probability measures \(()\) which are \(\)-away from \(_{}\) in expectation, with \(\) dictating the desired amount of risk in the decision.

The shape of our ambiguity set is flexible and driven by the posterior distribution. This is contrary to standard ambiguity sets which correspond to a ball around a fixed nominal distribution. The DRO-BAS problem (3) is still a worst-case approach, keeping with DRO tradition, instead of BDRO's expected worst-case formulation (2), see Figure 1.

The Bayesian posterior \((\ |\ )\) targets the KL minimiser between the model family and \(^{}\)(Walker, 2013), hence it is natural to choose \(D(,_{})\) to be the KL divergence of \(\) with respect to \(_{}\) denoted by \(d_{}(\|_{})\). This means that as \(n\) the posterior collapses to \(_{0}:=_{}d_{}(^{},_{})\) and the ambiguity set is just a KL-ball around \(_{_{0}}\). Using the KL divergence in the DRO-BAS problem in (3), it is straight-forward to obtain an upper bound of the worst-case risk for general models (see Appendix B.1 for a proof):

\[_{:_{}[d_{}(Q\| _{})]}\ _{}[f_{x}()]_{ 0 }\ +_{}[_{ _{}}\ [(()}{})]].\] (4)

Exact closed-form solutions of DRO-BAS can be obtained for a wide range of exponential family models with conjugate priors. When the likelihood distribution is a member of the exponential family, a conjugate prior also belongs to the exponential family (Gelman et al., 1995, Ch. 4.2). In this setting, before we prove the main result, we start with an important Lemma.

**Lemma 1**.: _Let \(p(\ |\ )\) be an exponential family likelihood and \((),\,(\ |\ )\) a conjugate prior-posterior pair, also members of the exponential family. Let \(_{0},_{n} T\) be hyperparameters of the prior and posterior respectively, where \(T\) is the hyperparameter space. Let \(_{n}\) depend upon \(_{n}\) and let \(G:T\) be a function of the hyperparameters. If the following identity holds:_

\[_{}[ p(\ |\ )]= p(\ |\ _{n})-G(_{n}),\] (5)

_then the expected KL-divergence can be written as:_

\[_{}[d_{}(\|_{ })]=d_{}(,_{_{n}})+G(_{n}).\] (6)

The condition in (5) is a natural property of many exponential family models, some of which are showcased in Table 1. Future work aims to prove this for all exponential family models. It is straightforward to establish the minimum tolerance level \(_{}\) required to obtain a non-empty ambiguity set. Since the KL divergence is non-negative, under the condition of Lemma 1, for any \(()\):

\[_{}[d_{}(\|_{})]=d_ {}(,_{_{n}})+G(_{n}) G(_{ n}):=_{}(n).\] (7)

We are now ready to prove our main result.

**Theorem 1**.: _Suppose the conditions of Lemma 1 hold and \(_{}(n)\) as in (7). Let \(_{n} T\), \(_{n}\), and \(G:T\). Then_

\[_{:_{}[d_{}(\| _{})]}\ _{}[f_{x}()]=_{ 0 }\ (-G(_{n}))+_{ p(_{n})}\ [(()}{})].\] (8)

To guarantee that the DRO-BAS objective upper bounds the expected risk under the DGP, the decision-maker aims to choose \(\) large enough so that \(^{}\) is contained in the ambiguity set. The condition in (5) yields a closed-form expression for the optimal radius \(^{}\) by noting that:

\[^{}=_{}[d_{}(^{}\| _{})]=d_{}(^{},_{_{n}})+G(_{n}).\] (9)

If the model is well-specified, and hence \(^{}\) and \(_{_{n}}\) belong to the same exponential family, it is straightforward to obtain \(^{}\) based on the prior, posterior and true parameter values. We give examples in Appendix A. In practice, since the true parameter values are unknown, we can approximate \(^{}\) using the observed samples. It follows that for any \(^{}_{}(n)\):

\[_{^{}}[f(x,)]_{: _{}[d_{}(Q\|_{})]}\ _{}[f_{x}()].\]

## 3 The Newsvendor Problem

**Experiment setup.** We evaluate DRO-BAS against the BDRO framework on a univariate Newsvendor problem with a well-specified univariate Gaussian likelihood with unknown mean and variance (Appendix D showcases a misspecified setting). The goal is to choose an inventory level \(0 x 50\) of a perishable product with unknown customer demand \(\) that minimises the cost function \(f(x,)=h(0,x-)+b(0,-x)\), where \(h\) and \(b\) are the holding cost and backorder cost per unit of the product respectively. We let \(^{}\) be a univariate Gaussian \((_{},_{}^{2})\) with \(_{}=25\) and \(_{}^{2}=100\). For random seed \(j=1,,200\), the training dataset \(_{n}^{(j)}\) contains \(n=20\) observations and the test dataset \(_{m}^{(j)}\) contains \(m=50\) observations. The conjugate prior and posterior are normal-gamma distributions (Appendix A.2). \(N\) is the total number of samples from each model. For each seed \(j\), we run DRO-BAS and BDRO with \(N=25,100,900\) and across 21 different values of \(\) ranging from 0.05 to 3. For DRO-BAS, \(N\) is the number of samples from \(p(_{n})\) and for BDRO, \(N=N_{} N_{}\) where \(N_{}\) is the number of posterior samples and \(N_{}\) likelihood samples due to the double expectation present; we set \(N_{}\) = \(N_{}\) to compare models on an equal \(N\) total samples regime. For a given \(\), we calculate the out-of-sample mean \(m()\) and variance \(v()\) of the objective function \(f(x_{}^{(j)},_{i})\) over all \(_{i}_{m}^{(j)}\) and over all seeds \(j=1,,200\), where \(x_{}^{(j)}\) is the optimal solution on training dataset \(_{n}^{(j)}\) (see Appendix C).

Analysis.Figure 2 shows that, for small sample size \(N=25,100\), our framework _dominates_ BDRO in the sense that DRO-BAS forms a Pareto front for the out-of-sample mean-variance tradeoff of the objective function \(f\). That is, for any \(_{1}\), let \(m_{}(_{1})\) and \(v_{}(_{1})\) be the out-of-sample mean and variance respectively of BDRO: then there exists \(_{2}\) with out-of-sample mean \(m_{}(_{2})\) and variance \(v_{}(_{2})\) of BAS-DRO such that \(m_{}(_{2})<m_{}(_{1})\) and \(v_{}(_{2})<v_{}(_{1})\). When \(N=900\), Figure 2 shows DRO-BAS and BDRO lie roughly on the same Pareto front. To summarise, BDRO requires more samples \(N\) than DRO-BAS for good out-of-sample performance,

   \(p()\) & \((_{n})\) & \(_{n}\) & \(G(_{n})\) \\  \((,^{2})\) & \((_{n},_{n}^{2})\) & \(_{n},^{2}\) & \(^{2}}{2_{n}^{2}}\) \\ \((,^{-1})\) & \((,_{n},_{n},_{n},_{n})\) & \(_{n},}{_{n}}\) & \((}+_{n}-(_{n}))\) \\ \(()\) & \((_{n},_{n})\) & \(}{_{n}}\) & \(_{n}-(_{n})\) \\   

Table 1: Examples for Theorem 1 of the parameter \(_{n}\) and the function \(G(_{n})\) for different likelihoods \(p()\) with conjugate posterior \((_{n})\) and posterior hyperparameters \(_{n}\). The normal, normal-gamma, exponential, and gamma distributions are denoted \(\), \(\), \(\), and \(\) respectively. See the supplementary material for the definitions of \(_{n}\) and the derivations of \(_{n}\) and \(G(_{n})\).

Figure 2: The out-of-sample mean-variance tradeoff (in bold) while varying \(\) for DRO-BAS and BDRO when the total number of samples from the model is 25 (left), 100 (middle), and 900 (right).

likely because BDRO must evaluate a double expectation over the posterior and likelihood, whilst DRO-BAS only samples from \(p(_{n})\). For fixed \(N\), the solve times for DRO-BAS and BDRO are broadly comparable (see Appendix C).

## 4 Discussion

We proposed a novel approach to Bayesian decision-making under uncertainty through a DRO objective based on posterior-informed Bayesian ambiguity sets. The resulting optimisation problem is a single-stage stochastic program with closed-form formulation for a variety of exponential-family models. The suggested methodology has good out-of-sample performance, as showcased in Figure 2. In our recent work (Dellaporta et al., 2024), we have also extended DRO-BAS to a general formulation for exponential family models and investigated alternative Bayesian ambiguity sets based on the posterior predictive distribution. Finally, whilst DRO-BAS offers protection against distributional ambiguity, the dependence of DRO-BAS on the Bayesian posterior makes it vulnerable to model misspecification. Future work will explore robust Bayesian ambiguity sets that address model misspecification through robust posteriors and discrepancies.