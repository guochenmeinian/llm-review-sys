# How does Inverse RL Scale to Large State Spaces?

A Provably Efficient Approach

Filippo Lazzati

Politecnico di Milano

Milan, Italy

filippo.lazzati@polimi.it

&Micro Mutti

Technion

Haifa, Israel

&Alberto Maria Metelli

Politecnico di Milano

Milan, Italy

###### Abstract

In online Inverse Reinforcement Learning (IRL), the learner can collect samples about the dynamics of the environment to improve its estimate of the reward function. Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the _feasible reward set_. However, none of the algorithms available in the literature can scale to problems with large state spaces. In this paper, we focus on the online IRL problem in Linear Markov Decision Processes (MDPs). We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large. As a consequence, we introduce the novel framework of _rewards compatibility_, which generalizes the notion of feasible set, and we develop CATY-IRL, a sample efficient algorithm whose complexity is independent of the cardinality of the state space in Linear MDPs. When restricted to the tabular setting, we demonstrate that CATY-IRL is minimax optimal up to logarithmic factors. As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound. Finally, we devise a unifying framework for IRL and RFE that may be of independent interest.

## 1 Introduction

Inverse Reinforcement Learning (IRL) is the problem of inferring the reward function given demonstrations of an optimal behavior, i.e., from an _expert_ agent. . Since its formulation, much of the research effort has been put into the design of efficient algorithms for solving the IRL problem . Indeed, the solution of the IRL problem opens the door to a variety of interesting applications, including Apprenticeship Learning (AL) , reward design , interpretability of the expert's behavior , and transferability to new environments .

Nowadays, the factor that most negatively impacts the adoption of IRL solutions in real-world applications is the intrinsic _ill-posedness_ of its formulation. The IRL problem has been historically defined as the problem of recovering _the_ reward function underlying the demonstrations , even though mere demonstrations can be equivalently explained by a _variety_ of rewards. In other words, the IRL problem is underconstrained, even in the limit of infinite demonstrations .

To overcome this weakness and to come up with a _single_ reward function, three main approaches are commonly adopted in the literature. (\(i\)) The first approach consists of the use of a _heuristic_ to select a specific reward function from the set of all the rewards that explain the demonstrations. Implicitly, these works re-define IRL as the problem of recovering _the_ reward function explaining the demonstrations _and_ complying with the heuristic. As an example,  select the reward that maximizes some notion of margin, and  implicitly chooses the reward returned by the optimization algorithm among those that maximize the likelihood. However, these approaches may generate issues in applications . (\(ii\)) In the second approach, additional _constraints_ beyondmere demonstrations are enforced to guarantee the uniqueness of the reward function to recover. In "reward identifiability" works, the additional information commonly concerns some structure of the environment , or multiple demonstrations across various environments . In Reward Learning (ReL) works , demonstrations of optimal behavior are combined with other kinds of expert feedback, like comparisons . (\(iii\)) As a third approach, recently,  proposed the alternative formulation of IRL as the problem of recovering _all_ the reward functions compatible with the demonstrations, i.e., the _feasible reward set_. In this manner, we are not subject to the limitations of the first approach, and we do not depend on additional information like in the second approach.

In practical applications, the chosen IRL formulation has to be tackled by algorithms that use a _finite_ number of demonstrations and a limited knowledge of the dynamics of the environment. In the common _online_ IRL scenario, the learner explores the (unknown) environment, and exploits this additional information to improve its performance on the IRL task [e.g., 39, 33, 38, 68, 31]. On this basis, the IRL approach (\(iii\)) based on the _feasible set_ displays desirable properties since "postpones" the choice of the heuristic and/or enforcement of additional constraints, with the advantage of analyzing the intrinsic complexity of the IRL problem only, without being obfuscated by other factors. In other words, this recent formulation of the IRL problem paves the way for the design and analysis of provably efficient IRL algorithms, endowed with solid theoretical guarantees.

However, the algorithms designed for learning the feasible set currently available in the literature [e.g., 39, 33, 38, 68, 31] struggle when attempting to scale them to IRL problems with _large state spaces_. This is apparent because their sample complexity exhibits an explicit dependence on the cardinality of the state space. This inevitably represents a major limitation since most real-world scenarios concern problems with large, or even continuous, state spaces [e.g., 15, 7, 40, 14].

In this context, function approximation represents an essential tool to tackle the curse of dimensionality and enforce generalization . Linear Markov Decision Processes (MDPs)  offer a simple but powerful structure, in which we assume the reward function and the transition model can be expressed as linear combinations of known features, that permits theoretical analysis of the sample complexity. Even though many extensions have been developed , the Linear MDPs framework typically represents one of the first function approximation settings to analyze when focusing on a novel problem, before moving to more complex settings [e.g., 63, 61].

In this paper, we aim to shed light on the challenges of scaling the feasible reward set to large-scale problems. Motivated by its limitations when dealing with large state spaces, we introduce the novel _Rewards Compatibility_ framework. Being a generalization of the notion of feasible set, it allows us to define the new _IRL Classification Problem_, a fourth approach to cope with the ill-posedness of the IRL formulation. This permits the development of CATY-IRL (CompATibilitY for IRL), a provably efficient IRL algorithm for Linear MDPs characterized by large or even continuous state spaces.

**Original Contributions.** The main contributions of the current work can be summarized as follows:

* We prove that the notion of feasible set can _not_ be learned efficiently in MDPs with large/continuous state spaces, even under the structure enforced by Linear MDPs. Nevertheless, we show that this problem disappears under the _assumption_ that the expert's policy is known, by providing a sample efficient algorithm for such setting (Section 3).
* To overcome the need for knowing the expert's policy exactly, we propose _Rewards Compatibility_, a novel framework that formalizes the intuitive notion of _compatibility_ of a reward function with expert demonstrations. It generalizes the feasible set and allows us to define an original learning setting, _IRL classification_, based on a new formulation of IRL _classification_ task (Section 4).
* For the newly-devised framework, we develop CATY-IRL (CompATibilitY for IRL), a new sample and computationally efficient IRL algorithm for both tabular and Linear MDPs. Remarkably, this CATY-IRL does not require the additional assumption that the expert's policy is known (Section 5).
* In the tabular setting, we prove a tight minimax lower bound to the sample complexity of the IRL classification problem of \(SA}{^{2}}(S+)\) episodes, where \(S\) and \(A\) are the cardinalities of the state and action spaces, \(H\) is the horizon, \(\) the accuracy and \(\) the failure probability. This bound is _matched_ by CATY-IRL, up to logarithmic factors. Exploiting a similar construction, we show that a lower bound with the same rate holds also for the Reward-Free Exploration (RFE) problem, improving by an \(H\) factor over the RFE state-of-the-art lower bound  (Section 6.1).
* Finally, we formulate a novel _Objective-Free Exploration_ (OFE) setting that isolates the challenges of exploration beyond Reinforcement Learning (RL), by unifying RFE and IRL (Section 6.2).

Additional related works and the proofs of all the results are reported in Appendix A and B -E.

## 2 Preliminaries

Notation.Given an integer \(N\), we define \([\![N]\!]\{1,,N\}\). Given sets \(\) and \(\), we denote \(_{d}(,)\{_{x} _{y}d(x,y),_{y}_{x}d(y,x)\}\) their Hausdorff distance with inner distance \(d\). We denote by \(^{}\) the probability simplex over \(\), and by \(^{}_{}\) the set of functions from \(\) to \(^{}\). Sometimes, we denote the dot product between vectors \(x,y\) as \( x,y x^{}y\). We employ \(,,\) for the common asymptotic notation and \(},,\) to omit logarithmic terms.

Markov Decision Processes.A finite-horizon Markov Decision Process (MDP) without reward  is defined as a tuple \((,,H,d_{0},p)\), where \(\) and \(\) are the measurable state and action spaces, \(H\) is the horizon, \(d_{0}^{}\) is the initial-state distribution, and \(p^{}_{ [H]}\) is the transition model. Given a (deterministic) reward function \(r[-1,1]^{[H]}\), we denote by \(}\{r\}\) the MDP obtained by pairing \(\) and \(r\). Each policy \(^{}_{[H]}\) induces in \(}\) a state-action probability distribution \(d^{p,}\{d^{p,}_{h}\}_{h[\![H]\!]}\) (we omit \(d_{0}\) for simplicity) that assigns, to each subset \(\), the probability of being in \(\) at stage \(h[\![H]\!]\) when playing \(\) in \(}\). We denote with \(^{p,}_{h}\) the set of states supported by \(d^{p,}_{h}\) for any action at stage \(h\), and with \(^{p,}\) the disjoint union of sets \(\{^{p,}_{h}\}_{h[\![H]\!]}\). The \(Q\)-function of policy \(\) in MDP \(}\) is defined at every \((s,a,h)[\![H]\!]\) as \(Q^{}_{h}(s,a;p,r)_{p,}[_{t=h}^{H}r_{t}(s_{t},a_{t })|s_{h}=s,a_{h}=a]\), and the optimal \(Q\)-function as \(Q^{}_{h}(s,a;p,r)_{}Q^{}_{h}(s,a;p,r)\), where the expectation \(_{p,}\) is computed over the stochastic process generated by playing policy \(\) in the MDP \(}\). Similarly, we define the \(V\)-function of policy \(\) at \((s,h)\) as \(V^{}_{h}(s;p,r)_{p,}[_{t=h}^{H}r_{t}(s_{t},a_{t })|s_{h}=s]\), and the optimal \(V\)-function as \(V^{}_{h}(s;p,r)_{}V^{}_{h}(s;p,r)\). We define the utility of \(\) as \(J^{}(r;p)_{s d_{0}}[V^{}_{1}(s;p,r)]\), and the optimal utility as \(J^{}(r;p)_{s d_{0}}[V^{}_{1}(s;p,r)]\). A forward (sampling) model of the environment permits to collect samples starting from \(s d_{0}\) and following some policy. A generative (sampling) model consists in an oracle that, given an arbitrary state-action-stage triple \(s,a,h\) in input, returns a sampled next state \(s^{} p_{h}(|s,a)\).

Linear MDPs.Based on , we say that an MDP \(}=(,,H,d_{0},p,r)\) is a _Linear MDP_ with a (known) feature map \(:^{d}\), if for every \(h[\![H]\!]\), there exist \(d\) unknown (signed) measures \(_{h}=[^{1}_{h},,^{d}_{h}]^{}\) over \(\) and an unknown vector \(_{h}^{d}\), such that for every \((s,a)\), we have \(p_{h}(|s,a)=(s,a),_{h}()\) and \(r_{h}(s,a)=(s,a),_{h}\). Without loss of generality, we assume \(\|(s,a)\|_{2} 1\) for all \((s,a)\), and \(\{\|_{h}\|_{2},\|_{h}|()\|_{2}\}\).1\(\) is a _Linear MDP without reward_ if its transition model satisfies the assumption described above.

BPI and RF.In both Best-Policy Identification (BPI)  and Reward-Free Exploration (RFE) , the learner has to explore the _unknown_ MDP to optimize a certain reward function. In BPI, the learner observes the reward function \(r\) during exploration, and its goal is to output a policy \(\) such that, in the true MDP with transition model \(p\) we have \(J^{}(r;p)-J^{}(r;p) 1-\) for every \(,(0,1)\). RFE considers the setting in which the reward to optimize is revealed _a posteriori_ of the exploration phase. Thus the goal of the agent in RFE is to compute an estimate \(\) of the true dynamics \(p\) so that \(_{r}\{J^{}(r;p)-J^{_{r}}(r;p) \} 1-\) for every \(,(0,1)\), where \(_{r}\) is the optimal policy in the MDP with \(\) as transition model and \(r\) as reward function.

Online IRL.We consider the online2 IRL setting  in which, similarly to the online AL setting , we are given a dataset \(^{E}=\{(s^{i}_{1},a^{i}_{1},,s^{i}_{H-1},a^{i}_{H-1},s^{i}_{H}) \}_{i[\![^{}\![]\!]}\) of \(^{E}\) trajectories collected by executing the expert's policy \(^{E}\) in a certain (unknown) MDP \(}=\{r^{E}\}\). We make the assumption that \(^{E}\) is optimal under the true (unknown) reward \(r^{E}\) in \(}\). Since the dynamics of \(}\) is unknown, we are allowed to actively explore the environment through a _forward_ model to collect a new state-action dataset \(\). The goal is to use the latter and demonstrations in \(^{E}\) to estimate a reward function that makes the expert's policy \(^{E}\) optimal. Sometimes, we will denote an IRL instance as \(\{^{E}\}\), and a Linear IRL instance with recovered reward \(r\) as an IRL instance in which \(\{r\}\) is a Linear MDP.

## 3 Limitations of the Feasible Set

In this section, after having characterized the feasible set formulation in Linear MDPs, we show that it suffers from _statistical_ (and _computational_) inefficiency in problems with large state spaces, even under the Linear MDP assumption. We will provide a solution to these issues in Section 4.

**The Feasible Set.** According to the standard definition [e.g., 39; 33; 38; 68; 31], the feasible set contains the rewards that make the expert's policy \(^{E}\) optimal, as defined below.

**Definition 3.1** (Feasible Set ).: _Let \(\) be an MDP without reward and let \(^{E}\) be the expert's policy. The feasible set \(_{p,^{E}}\) of rewards compatible with \(^{E}\) in \(\) is defined as:_

\[_{p,^{E}}:=\{r\,|\,\,J^{^{E}}(r;p)=J^{*}(r;p)\}.\]

Without function approximation, the feasible set contains a variety of rewards for any deterministic policy. In Linear MDPs, due to the feature map, the feasible set might exhibit some degeneracy.3 Definition 3.1 can be adapted to Linear MDPs with feature map \(\) as: \(_{,p,^{E}}:=\{r\,|\,J^{^{E}}(r;p)=J^{*}(r ;p):[\![H]\!]^{d},(s,a,h) [\![H]\!]:\,r_{h}(s,a)=(s,a),_{h} \}\). We omit \(\) in \(_{,p,^{E}}\) for notational simplicity.

**Proposition 3.1**.: _Let \(\) be a Linear MDP without reward with a finite state space, and let \(\) be a feature mapping. Let \(\{_{h}^{^{E}}\}_{h[\![H]\!]}\) and \(\{_{h}\}_{h[\![H]\!]}\) be the sets of expert's and non-expert's features, defined for every \(h[\![H]\!]\) as:_

\[_{h}^{^{E}}:=(s,a^{E})\,|\,s_{h}^{p,^{E}}, \,a^{E}_{h}^{E}(s)},_{h}:= (s,a)\,|\,s_{h}^{p,^{E}},\,a _{h}^{E}(s)},\]

_where \(_{h}^{E}(s):=\{a|_{h}^{E}(|s)>0\}\) for every \(s\). If for none of the \(H\) pairs of sets \((_{h}^{^{E}},_{h})\) there exists a separating hyperplane, then \(_{p,}=\{\}\), with \(_{h}(s,a)=0\,(s,a,h) \![H]\!]\) i.e., the feasible set with linear rewards in \(\) contains only the reward function that assigns zero reward everywhere._

Intuitively, expert's actions must have the largest optimal \(Q\)-value among all actions, and linearity imposes the "separability" requirement. The result holds also for MDPs with linear rewards only. We exemplify Proposition 3.1 in Appendix B.1.

**Learning the Feasible Set.** In order to highlight the challenges of learning the feasible set with large-scale MDPs, based on [38; 31], we devise the following PAC requirement.

**Definition 3.2** (PAC Algorithm).: _Let \(,(0,1)\), and let \(\) be an algorithm that collects \(^{E}\) samples about \(^{E}\) using a generative model, and \(\) episodes from a Linear MDP without reward \(=(,,H,d_{0},p)\) using a forward model. Let \(}\) be the estimate of the feasible set \(_{p,^{E}}\) outputted by \(\). Then, \(\) is \((,)\)-PAC for IRL if \(_{,}_{d}(_{p, ^{E}},}) 1-\), where \(_{,}\) is the probability measure induced by \(\) in \(\), and \(d(r,)_{}_{h[\![H]\!]} _{(s,a) d_{h}^{p,}(,)}\,|r_{h}(s,a)-_{h}(s,a)|.\)4 The sample complexity is the pair \((^{E},)\)._

It is worth noting that in Definition 3.2, we are considering a generative model for collecting samples from the expert's policy, which represents the easiest learning scenario. The following result shows that, even in this convenient setting, estimating the feasible set is statistically inefficient.

**Theorem 3.2** (Statistical Inefficiency).: _Let \(\{^{E}\}\) be a Linear IRL instance with finite state space \(\) and deterministic expert's policy, and let \(,(0,1)\). If an algorithm \(\) is \((,)\)-PAC, then \(^{E}=(S)\), where \(S:=||\) is the cardinality of the state space._

In other words, even under the easiest learning conditions (i.e., generative model and deterministic expert), the sample complexity scales directly with the cardinality of the state space \(S\), thus, it is infeasible when \(S\) is large or even infinite. Observe that this result extends to any class of MDPs that contains Linear MDPs. In Appendix B.2, we analyze if additional assumptions can drop the \((S)\) dependence. Nevertheless, if \(^{E}\) is _known_, it is possible to construct sample efficient algorithms. Algorithm 1 (whose pseudocode is presented in Appendix B.3), under the assumption that \(^{E}\) is known, makes use of an inner RFE routine (Algorithm 1 of ) to recover the feasible set.

**Theorem 3.3**.: _Assume that \(^{E}\) (along with its support \(^{p,^{E}}\)) is known. Then, for any \(,(0,1)\), Algorithm 1 is \((,)\)-PAC for IRL with a number of episodes \(\) upper bounded by:_

\[}d}{^{2}}d+ .\]

**Limitations of the Feasible Set.** We can now conclude that the feasible set suffers from two main limitations. \((i)\)_Sample Inefficiency_: If \(^{E}\) is unknown, it requires a number of samples that depends on the cardinality of the state space (Theorem 3.2). \((ii)\)_Lack of Practical Implementability_: It contains a continuum of rewards, thus, no practical algorithm can explicitly compute it. We will discuss in the next section how to overcome both these issues.

## 4 Rewards Compatibility

In this section, we present the main contribution of this work: _Rewards Compatibility_, a novel framework for IRL that allows us to conveniently rephrase the learning from demonstrations problem as a classification task. We anticipate that the presentation of the framework is completely general and independent of structural assumptions of the MDP (e.g., Linear MDP).

### Compatible Rewards

In the following, for ease of presentation, we consider the exact setting, i.e., when \(d_{0}\), \(p\), and \(^{E}\) are known. In addition, we will drop the dependence on \(p\) when clear from the context.

In IRL, an expert agent demonstrates policy \(^{E}\) assumed optimal under some (unknown) reward function \(r^{E}\), i.e., \(J^{}(r^{E})=J^{^{E}}(r^{E})\). The task is to recover a reward \(r\) such that \(J^{}(r)=J^{^{E}}(r)\). By definition, IRL tells us that \(r^{E}\) makes the demonstrated policy \(^{E}\) optimal, but what about other policies? We _do not_ and _cannot_ know. Since there are (infinite) rewards making \(^{E}\) optimal (but they differ in the performance attributed to other policies) we realize that there are many rewards equally "compatible" with \(^{E}\).5 Clearly, wih no additional information, we are unable to identify \(r^{E}\).

The feasible set considers only these rewards, i.e., \(r\) for which \(J^{}(r)=J^{^{E}}(r)\), and it refuses all the others. This can be interpreted as the feasible set carrying out a _classification_ of rewards based on a "hard" notion of _compatibility_ with demonstrations. In other words, rewards \(r\) satisfying condition \(J^{}(r)=J^{^{E}}(r)\) are compatible with \(^{E}\), and the others are not. Nevertheless, our insight is that some rewards are _"more" compatible_ with \(^{E}\) than others.

**Example 4.1**.: _Consider an MDP with one state and \(H=1\) in which the expert has three actions: Eating a muffin (M), a cake (C), or some (bad) vegetable soup (S). The true reward \(r^{E}\) assigns \(r^{E}(M)=+1,r^{E}(C)=+0.99\) and \(r^{E}(S)=-1\), i.e., the expert has a (weak) preference for the muffin over the cake, while she hates the soup; thus, she will demonstrate \(^{E}=M\). Let \(r_{g},r_{b}\) be:_

\[r_{g}(M)=+0.99,r_{g}(C)=+1,r_{g}(S)=-1, r_{b}(M)=-1,r_{b}(C)=-1,r_{b}(S )=+1.\]

_Intuitively, \(r_{g}\) is "more" compatible with \(^{E}\) than \(r_{b}\), because it establishes that M and C are much better than S, while reward \(r_{b}\) reverses the preferences. Clearly, we make a small error if we model the preferences of the expert with \(r_{g}\) instead of the true reward \(r^{E}\). However, the notion of feasible set is completely blind to the difference between \(r_{g}\) and \(r_{b}\) at modeling \(r^{E}\), and it refuses both of them._

We propose the following "soft" definition of (non)compatibility to capture this intuition.6

**Definition 4.1** (Rewards (non)Compatibility).: _Let \(\{^{E}\}\) be an IRL instance, and let \(r\) be any reward. We define the (non)compatibility \(}_{p,^{E}}:_{ 0}\) of reward \(r\) w.r.t. \(\{^{E}\}\) as:_

\[}_{p,^{E}}(r) J^{}(r;p)-J^{^{E}}(r;p).\]In words, the (non)compatibility of reward \(r\) w.r.t. policy \(^{E}\) in problem \(\) quantifies the _suboptimality_ of \(^{E}\) in the MDP \(\{r\}\). By definition, rewards \(r\) belonging to the feasible set (i.e., \(r_{p,^{E}}\)) satisfy \(}_{p,^{E}}(r)=0\), i.e., they have zero non-compatibility with \(^{E}\) in \(\).7

**Example 4.1** (Continued).: _(Non)compatibility discriminates between \(r_{g}\) and \(r_{b}\). Indeed, we have that \(}_{p,^{E}}(r^{E})=0\), \(}_{p,^{E}}(r_{g})=0.01\), and \(}_{p,^{E}}(r_{b})=2\). In words, reward \(r_{g}\) suffers from very small (non)compatibility, while \(r_{b}\) suffers from large (non)compatibility, thus we say that reward \(r_{g}\) is more compatible with \(^{E}\) than \(r_{b}\), as expected._

By definition of IRL, the true reward \(r^{E}\) makes the observed \(^{E}\) optimal, but reveals no information about the other policies. Thus, it is meaningful that \(}_{p,^{E}}\) considers the suboptimality of \(^{E}\) only, because demonstrations from \(^{E}\) do not provide information about other policies, as illustrated below.

**Example 4.2**.: _Let \(r^{}_{b}\) be such that \(r^{}_{b}(M)=+0.99,r^{}_{b}(C)=-1,r^{}_{b}(S)=+1\). Clearly, \(r^{}_{b}\) is much worse than \(r_{g}\) at modeling \(r^{E}\), because it does not capture the fact that the expert appreciates the cake but she hates the soup. However, demonstrations from \(^{E}\) alone do not provide information about C or S, but only about \(^{E}=M\) (i.e., the expert always eats the muffin). Thus, we have that \(}_{p,^{E}}(r_{g})=}_{p,^{E}}( r^{}_{b})=0.01\), i.e., \(r_{g}\) and \(r^{}_{b}\) are equally compatible with the given demonstrations._

For a discussion on comparing the (non)compatibility of different rewards, see Appendix C.4.

### The IRL Classification Formulation

Our goal is to overcome the limitations of the feasible set highlighted in Section 3. Drawing inspiration from the notion of "membership checker" algorithm in , we propose a novel formulation of IRL.

**Definition 4.2** (IRL Classification Problem and IRL Algorithm).: _An IRL Classification Problem instance is made of a tuple \((,^{E},,)\), where \(\) is an MDP without reward, \(^{E}\) is the expert's policy, \(\) is a set of rewards to classify, and \(_{ 0}\) is some threshold. The goal is to classify all and only the rewards \(r\) based on their (non)compatibility with \(^{E}\) in \(\) w.r.t. \(\). In symbols:_

\[ r:\ \ \ \ }_{p,^{E}}(r) \ \ \ \ \ ,\ \ \ \ .\]

_An IRL algorithm takes in input a reward \(r\) and outputs a boolean saying whether \(}_{p,^{E}}(r)\)._

Given \(r\), we output whether it makes the expert's policy \(^{E}\) at most \(\)-suboptimal or not. Intuitively, we classify rewards in \(\) based on how good \(^{E}\) performs w.r.t. them. A \(\)-(non)compatible reward guarantees that, among its \(\)-optimal policies, there is \(^{E}\), but the optimal policy might be different from \(^{E}\) (see Appendix C.3 for how this relates to (forward) RL). Note that we allow for \(\) to manage scenarios in which we have some prior knowledge on \(r^{E}\), i.e., \(r^{E}\).

**Remark 4.1**.: _Permitting non-zero (non)compatibility is equivalent to enlarging the feasible set. Let \(=\), and define the set of rewards positively classified as \(_{}\), i.e., \(_{}\{r\,|\,}_{p, ^{E}}(r)\}\). For any \(,^{}\) s.t. \(0^{} 2H\), we have: \(_{p,^{E}}=}_{0}_{ }_{^{}}_{2H}= \)._

**Discussion on Reward Compatibility.** It should be remarked that:

* _The limits of the rewards compatibility framework are the same as the limits of the feasible set._ We cannot identify \(r^{E}\) from the feasible set or among the rewards with small (non)compatibility. As aforementioned, this is an inherent limit of IRL and cannot be overcome with a more refined objective formulation, unless further information on \(r^{E}\) is available (e.g., preferences).
* _Rewards compatibility offers advantages over feasible set._ Differently from the feasible set, as we will see in Section 5, it is possible to _practically_ implement algorithms that solve the IRL classification problem, with guarantees of sample efficiency even when the state space is large.

### A Learning Framework for Online IRL Classification

In this section, we combine the online IRL setting presented in Section 2 with the IRL classification problem of Definition 4.2. Intuitively, the performance of an algorithm depends on its accuracy at estimating the (non)compatibility of the rewards, as formalized by the following PAC requirement.

**Definition 4.3** (PAC Framework).: _Let \(,(0,1)\), and let \(^{E}\) be a dataset of \(^{E}\) expert's trajectories. An algorithm \(\) exploring for \(\) episodes is \((,)\)-PAC for the IRL classification problem if:_

\[*{}_{,^{E},}_{r }|}_{p,^{E}}(r)-}(r)| 1-,\]

_where \(*{}_{,^{E},}\) is the joint probability measure induced by \(^{E}\) and \(\) in \(\), and \(}\) is the estimate of \(}_{p,^{E}}\) computed by \(\). The sample complexity is defined by the pair \((^{E},)\)._

Intuitively, our goal is to estimate the (non)compatibility of the rewards in \(\) with sufficient accuracy, so that, given a threshold \( 0\), we are able to classify "most" of them correctly w.h.p. (with high probability). The concept is exemplified in Figure 2. Note that the estimation problem is independent of the threshold \(\), which can be appropriately selected to cope with noise in the demonstrations, (unknown) expert suboptimality, or to manage the amount of "false negatives" and "false positives".

**Remark 4.2**.: _For \( 0\), let \(_{}\{r\,|\,}_{p, ^{E}}(r)\}\) and \(}_{}\{r\,|\,}(r)\}\) denote the sets of rewards positively classified using, respectively, the true (non)compatibility \(}_{p,^{E}}\) and the estimate \(}\) constructed by an \((,)\)-PAC algorithm. Then, with probability \(1-\), it holds that: \(}_{-}_{} }_{+}\). Thus, we can trade-off the amount of "false negatives" (resp. "false positives") by, e.g., choosing the threshold \(+\) (resp. \(-\))._

## 5 Caty-Irl:

### A Provably Efficient Algorithm for IRL

In this section, we present CATY-IRL (CompAtibility for IRL), a provably efficient algorithm for solving the _online_ IRL _classification_ problem. We consider three different kinds of structure for the MDPs: tabular MDPs, tabular MDPs with linear rewards, and Linear MDPs. Similarly to RFE, our online IRL classification setting is made of two phases: (\(i\)) an _exploration_ phase, in which the algorithm explores the environment using the knowledge of \(\) and of the expert's dataset \(^{E}\) to collect samples about the dynamics of the MDP, and (\(ii\)) a _classification_ phase, in which it performs the classification of a reward \(r\) without interactions with the environment. A flow-chart is reported in Figure 1 (pseudocode in Appendix D).

**Exploration phase.** The _exploration_ phase collects a dataset \(\) in a way that depends on the structure of the MDP and of the set of rewards \(\) to be classified. Specifically, for Linear MDPs,

Figure 1: Flow-chart of CATY-IRL.

CATY-IRL executes RFLin . Instead, for tabular MDPs (with or without linear reward), CATY-IRL instantiates either BPI-UCBVI  for each reward \(r\) (when \(||=(1)\), i.e., a "small" constant w.r.t. to the size of the MDP, where "small" depends on the size of the state space, see Appendix D.2) or RF-Express . Note that CATY-IRL in this phase does not use the expert's dataset \(^{E}\).

**Classification phase.** The _classification_ performs the estimation \(}(r)\) of the (non)compatibility term \(}_{p,^{E}}(r)\) for the single input reward \(r\) by splitting it into two independent estimates: \(}^{E}(r) J^{^{E}}(r;p)\), which is computed with \(^{E}\) only, and \(}^{*}(r) J^{*}(r;p)\), which is computed with \(\) only. Concerning \(}^{E}(r)\), when the reward is linear \(r_{h}(s,a)=(s,a),_{h}\), CATY-IRL uses \(^{E}\) to construct an empirical estimate \(^{E}^{p,^{E}}\) of the expert's expected feature count . Otherwise, it directly estimates \(^{E} d^{p,^{E}}\) the expert's occupancy measure. Such estimates can be used to derive \(}^{E}(r)\) straightforwardly. Regarding \(^{*}(r)\), CATY-IRL exploits the _planning_ phase of the corresponding RFE (or BPI) algorithm adopted at exploration phase.8 Finally, CATY-IRL applies the (potentially negative) input threshold \(\) to the difference \(^{*}(r)-}^{E}(r)\) to perform the classification. See Appendix D for the full pseudo-code. Clearly, CATY-IRL can be implemented in practice, since it considers a single reward at a time instead of computing the full feasible set, and it is computationally efficient in linear MDPs, since it uses a computationally efficient algorithm as subroutine (see ).

**Sample Efficiency.** The next result analyzes the sample complexity (Definition 4.3) of CATY-IRL.

**Theorem 5.1** (Sample Complexity of CATY-IRL).: _Let \(,(0,1)\). Then CATY-IRL is \((,)\)-PAC for IRL with a sample complexity upper bounded by:_

\[\] \[^{E} }SA}{^{2}} N+,\] \[\] \[^{E} }d}{^{2}} ,  }d}{^{2}} d+,\] \[\]

_where \(N=0\) if \(||=(1)\), and \(N=S\) otherwise._

Some observations are in order. We conjecture that the \(d^{2}\) dependence when \(||=(1)\) is unavoidable in Linear MDPs because of the lower bound for BPI in . In tabular MDPs with deterministic expert, one might use the results in  to reduce the rate of \(^{E}\) from \(}(SAH^{3}(^{-1})/^{2})\) to \(}(SH^{3/2}(^{-1})/^{2})\). Finally, note that the choice \(=\) allows us to positively classify all the rewards in the feasible set \(_{p,^{E}}\) w.h.p. and, in this case, other rewards positively classified have true (non)compatibility at most \(2e\) w.h.p. In light of this result we conclude that _rewards compatibility_ framework allows the _practical_ development of _sample efficient_ algorithms (e.g., CATY-IRL) in Linear MDPs with large/continuous state spaces.

## 6 Statistical Barriers and Objective-Free Exploration

In this section, we show that CATY-IRL is minimax optimal for the number of exploration episodes in tabular MDPs, and that RFE and IRL share the same theoretical sample complexity. This allows us to formulate _Objective-Free Exploration_, a unifying setting for exploration problems.

### The Theoretical Limits of IRL (and RFE) in the Tabular Setting

In CATY-IRL, we use a minimax optimal RFE algorithm for exploration. However, this does not entail that CATY-IRL is minimax optimal for the IRL classification problem. There might exist another PAC algorithm with a sample complexity smaller than CATY-IRL. The following result states that, in the tabular setting, the bound in Theorem 5.1 is tight for the number of episodes \(\).

**Theorem 6.1** (IRL Classification - Lower Bound).: _Let \(\) be an \((,)\)-PAC algorithm for the IRL classification in tabular MDPs. Let \(\) be the number of exploration episodes. Then, there exists an IRL classification instance such that:_

\[|| 1:\ SA}{ ^{2}},=:\  SA}{^{2}}S+ .\]

In both cases, the lower bound is _matched_ by CATY-IRL, up to logarithmic factors. Note that CATY-IRL explores without using \(^{E}\), thus, minimax optimality for \(\) can be achieved without the knowledge of \(^{E}\) at exploration phase. As a by-product, we observe that a similar lower bound construction can be made also for RFE, leading to the following result.

**Theorem 6.2** (RFE - Refined Lower Bound).: _Let \(\) be an \((,)\)-PAC algorithm for RFE in tabular MDPs. Let \(\) be the number of exploration episodes. Then, there exists an RFE instance such that:_

\[SA}{^{2}}S+ {}.\]

This bound improves the state-of-the-art RFE lower bound \((SA}{^{2}}(+))\) (obtained combining the bounds in  and ) by one \(H\) factor, and it is matched by RF-Express .

### Objective-Free Exploration (OFE)

What is the most efficient exploration strategy that can be performed in an unknown environment? It _depends_ on the subsequent task that shall be solved. However, if the task is unknown at the exploration phase, we need a strategy that suffices for all the tasks that one might be interested in solving. Let us denote by \(\) the set of RL and IRL classification tasks. Since CATY-IRL is a sample efficient algorithm for the IRL classification problem, and it uses RFE as a subroutine, we conclude that the RFE exploration strategy is sufficient (and also minimax optimal in tabular MDPs) to obtain guarantees for class \(\). Are there other problems for which RFE exploration suffices when the specific problem instance is revealed _a posteriori_ of the exploration phase? We believe so, and in Appendix E, we identify two additional problems, i.e., Matching Performance (MP) and Imitation Learning from Observations alone (ILFO) , that represent potential candidates to belong to \(\).

More in general, we formulate the _Objective-Free Exploration (OFE)_ problem as follows:

**Definition 6.1** (Objective-Free Exploration).: _Given a tuple \((,,(,))\), where \(\) is an unknown environment (e.g., MDP without reward), and \(\) is a certain class of tasks (e.g., all RL and IRL problems), the Objective-Free Exploration (OFE) problem aims to find an exploration of the environment \(\) (e.g., RFE exploration) that permits to solve any task \(f\) in an \((,)\)-correct manner._

This problem is called "objective-free" because it does not require the knowledge of the specific "objective" \(f\) to be solved. In Appendix F, we describe a use case for OFE. We believe this is an interesting problem to be studied in future.

## 7 Conclusions

In this paper, we have shown that the feasible set cannot be learned efficiently in problems with large/continuous state spaces even under the strong structure provided by Linear MDPs. For this reason, we have introduced the powerful framework of _compatible rewards_, which formalizes the intuitive notion of compatibility of a reward function with expert demonstrations, and it allows us to formulate the IRL problem as a _classification_ task. In this context, we have devised CATY-IRL, a provably efficient IRL algorithm for Linear MDPs with large/continuous state spaces. Furthermore, in tabular MDPs, we have demonstrated the minimax optimality of CATY-IRL at exploration by presenting a novel lower bound to the IRL classification problem. As a by-product, our construction improves the current state-of-the-art lower bound for RFE. Finally, we have introduced OFE, a unifying problem setting for exploration problems, which generalizes both RFE and IRL.

**Limitations.** A limitation of our contributions concerns the adoption of the _Linear MDP_ model, whose assumptions are overly strong to be consistently applied to real-world applications. Nevertheless, while the rewards compatibility framework is general and not tied to Linear MDPs, we believe that Linear MDPs represent an important initial step toward the development of provably efficient IRLalgorithms with more general function approximation structures. Although a lower bound for Linear MDPs is missing, we believe that it represents an interesting direction for future works. Finally, we note that the _empirical validation_ of the proposed algorithm is out of the scope of this work.

**Future Directions.** Promising directions for future works concern the extension of the analysis of the _rewards compatibility_ framework beyond Linear MDPs to general function approximation and to the offline setting. In addition, it might be fascinating to extend the notion of reward compatibility to other kinds of expert feedback (in the context of ReL), and to other IRL settings (e.g., suboptimal experts). Finally, we believe that OFE should be analysed in-depth given its practical importance.