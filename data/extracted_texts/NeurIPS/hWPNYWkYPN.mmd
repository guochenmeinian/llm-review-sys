# A new perspective on building efficient and expressive 3D equivariant graph neural networks

Weitao Du

Chinese Academy of Sciences

duweitao@mass.ac.cn

&Yuanqi Du1

Cornell University

yd392@cornell.edu

&Limei Wang1

Texas A&M University

limei@tamu.edu

&Dieqiao Feng

Cornell University

&Guifeng Wang

Zhejiang University

&Shuiwang Ji

Texas A&M University

Carla P Gomes

Cornell University

&Zhi-Ming Ma

Chinese Academy of Sciences

Equal contribution.

###### Abstract

Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these network architectures through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (**LSE**) and frame transition encoding (**FTE**). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out future design space for 3D equivariant graph neural networks. Our codes are available at https://github.com/yuanqidu/LeftNet.

## 1 Introduction

The success of many deep neural networks can be attributed to their ability to respect physical symmetry, such as Convolutional Neural Networks (CNNs)  and Graph Neural Networks (GNNs) . Specifically, CNNs encode translation equivariance, which is essential for tasks such as object detection. Similarly, GNNs encode permutation equivariance, which ensures that the node ordering does not affect the output node representations, by aggregating neighboring messages. Modeling 3D objects, such as point clouds and molecules, is a fundamental problem with numerous applications , including robotics , molecular simulation , and drug discovery . Different from 2D pictures and graphs that only possess the translation  and permutation  symmetry, 3D objects intrinsically encode the complex \(SE(3)/E(3)\) symmetry , which makes their modeling a nontrivial task in the machine learning community.

To tackle this challenge, several approaches have been proposed to effectively encode 3D rotation and translation equivariance in the deep neural network architectures, such as TFN , EGNN , and SphereNet . TFN leverages spherical harmonics to represent and update tensors equivariantly,while EGNN processes geometric information through vector update. On the other hand, SphereNet is invariant by encoding scalars like distances and angles. Despite rapid progress has been made on the empirical side, it's still unclear what 3D geometric information can equivariant graph neural networks capture and how the geometric information is integrated during the message passing process [16; 17; 18]. This type of analysis is crucial in designing expressive and efficient 3D GNNs, as it's usually a trade-off between encoding enough geometric information and preserving relatively low computation complexity. Put aside the \(SE(3)/E(3)\) symmetry, this problem is also crucial in analysing ordinary GNNs. For example, 1-hop based message passing graph neural networks  are computationally efficient while suffering from expressiveness bottlenecks (comparing with subgraph GNNs [20; 21]). On the other hand, finding a better trade-off for 3D GNNs is more challenging, since we must ensure that the message updating and aggregating process respects the \(SE(3)/E(3)\) symmetry.

In this paper, we attempt to discover better trade-offs between computational efficiency and expressive power for 3D GNNs by studying two specific questions: 1. What is the geometric expressiveness of 3D GNNs through a local 3D graph isomorphism lens? 2. What is expressiveness of 3D GNNs in representing global geometric information through local patches? The first question relates to the design of node-wise geometric messages, and the second question relates to the design of equivariant (or invariant) aggregation. To tackle these two problems, we take a local-to-global approach. More precisely, we first define three types of 3D graph isomorphism to characterize local 3D structures: tree, triangular, and subgraph isomorphism, following a local hierarchy. Our local hierarchy lies between the 1-hop and 2-hop geometric isomorphism defined in Joshi et al. , detailed in Appendix G.2; thus, it can be used to measure the expressive power of 3D GNNs by their ability of differentiating non-isomorphic 3D structures similar to the geometric WL test proposed in Joshi et al. . Under this theoretical framework, we summarize one essential ingredient for building expressive geometric messages on each node: local 3D substructure encoding (**LSE**), which allows an invariant realization. To answer the second question, we analyze whether local invariant features are sufficient for expressing global geometries by message aggregation, and it turns out that frame transition encoding (**FTE**) is crucial during the local to global process. Although **FTE** can be realized by invariant scalars, we further demonstrate that introducing equivariant messaging passing is more efficient. After presenting **LSE** and **FTE** modules, we are able to present a modular overview of 3D GNNs designs. In realization of our theoretical findings, we propose LEFTNet that efficiently implements **LSE** and **FTE** without sacrificing expressiveness. Empirical experiments on real-world scenarios, predicting scalar-valued property (e.g. energy) and vector-valued property (e.g. force) for molecules, demonstrate the effectiveness of LEFTNet.

## 2 Preliminary

In this section, we provide an overview of the mathematical foundations of \(E(3)\) and \(SE(3)\) symmetry, which is essential in modeling 3D data. We also summarize the message passing graph neural network framework, which enables the realization of \(E(3)/SE(3)\) equivariant models.

**Euclidean Symmetry.** Our target is to incorporate Euclidean symmetry to ordinary permutation-invariant graph neural networks. The formal way of describing Euclidean symmetry is the group \(E(3)=O(3) T(3)\), where \(O(3)\) corresponds to reflections (parity transformations) and rotations. For tasks that are anti-symmetric under reflections (e.g. chirality), we consider the subgroup \(SE(3)=SO(3) T(3)\), where \(SO(3)\) is the group of rotations. We will use \(SE(3)\) in the rest of the paper for brevity except when it's necessary to emphasize reflections.

**Equivariance.** A tensor-valued function \(()\) is said to be **equivariant** with respect to \(SE(3)\) if for any translation or rotation \(g SE(3)\) acting on \(^{3}\), we have

\[(g)=(g)(),\]

where \(()\) is a matrix representation of \(SE(3)\) acting on tensors. See Appendix A for a general definition of tensor fields. In this paper, we will use **bold** letters to represent an equivariant tensor, e.g., **x** as a position vector. It is worth noting that when \(()^{1}\) and \((g) 1\) (the constant group representation), the equivariant function \(()\) is also called an **invariant** scalar function.

**Message Passing Scheme for Geometric Graphs.** A geometric graph \(G\) is represented by \(G=(V,E)\). Here, \(v_{i} V\) denotes the set of nodes (vertices, atoms), and \(e_{ij} E\) denotes the set of edges. For brevity, the edge feature attached on \(e_{ij}\) is also denoted by \(e_{ij}\). \(=(_{1},,_{n})^{n 3}\) denotes the node positions which determine the geometric structure of \(G\).

A common machine learning tool for modeling graph-structured data is the Message Passing Neural Network (MPNN) . A typical 1-hop MPNN framework consists of two phases: (1) message passing; (2) readout. Let \(h_{i}^{l},h_{j}^{l}\) denote the \(l\)-th layer's node features of source \(i\) and target \(j\) that also depend on the 3D positions \((_{i},_{j})\), then the aggregated message is

\[m_{i}^{l}=_{j(i)}m_{ij}(h^{l}(_{i}),h^{l}( _{j}),_{ij}^{l}),\] (1)

and \(_{j(i)}\) is any permutation-invariant pooling operation between the 1-hop neighbors of \(i\). We also include the edge features \(_{ij}^{l}\) into the message passing phase for completeness. 3D **equivariant** GNNs (3D GNNs for short) require the message \(m_{i}\) to be equivariant with respect to the geometric graph. That is, for an arbitrary edge \(e_{ij}\):

\[m_{ij}(h^{l}(g_{i}),h^{l}(g_{j}))=(g)m_{ij}(h^ {l}(_{i}),h^{l}(_{j})),\] (2)

where \(g SE(3)\) is acting on the whole geometric graph simultaneously: \((_{1},,_{n})(g_{1},,g _{n})\). For example, the invariant model ComENet  satisfies Eq. 2 by setting \((g) 1\), and MACE  realized Eq. 2 for nonconstant irreducible group representations \((g)\) through spherical harmonics and Clebsch-Gordan coefficients.

## 3 A local hierarchy of 3D graph isomorphism

As presented in Section 2, defining expressive messages is an essential component for building powerful 3D GNNs. In this section, we develop a fine-grained characterization of local 3D structures and build its connection with the expressiveness of 3D GNNs.

Since the celebrated work , a popular expressiveness test for permutation invariant graph neural networks is the 1-WL graph isomorphism test , and Wijesinghe and Wang  has shown that the 1-WL test is equivalent to the ability to discriminate the **local** subtree-isomorphism. It motivates us to develop a novel (local) 3D isomorphism for testing the expressive power of 3D GNNs. However, this task is nontrivial, since most of the previous settings for graph isomorphism are only applicable to 2D topological features. For 3D geometric shapes, we should take the \(SE(3)\) symmetry into account. Formally, two 3D geometric graphs \(,\) are defined to be **globally** isomorphic, if there exists \(g SE(3)\) such that

\[=g.\] (3)

In other words, \(\) and \(\) are essentially the same, if they can be transformed into each other through a series of rotations and translations. Not that Eq. 3 is up to the permutation of nodes. Inspired by Wijesinghe and Wang , now we introduce a novel hierarchy of \(SE(3)\) equivariant local isomorphism to measure the local similarity of 3D structures.

Let \(_{i}\) represent the 3D subgraph associated with node \(i\). This subgraph contains all the 1-hop neighbors of \(i\) as its node set, along with all edges in \(E\) where both end points are one-hop neighbors of \(i\). For each edge \(e_{ij} E\), the mutual 3D substructure \(_{i-j}\) is defined by the intersection of \(_{i}\) and \(_{j}\): \(_{i-j}=_{i}_{j}\).

Given two local subgraphs \(_{i}\) and \(_{j}\) that correspond to two nodes \(i\) and \(j\) (not necessarily adjacent), we say \(_{i}\) is {tree-, triangular-, subgraph-} isometric to \(_{j}\), if there exists a bijective function

Figure 1: (a) \(_{i}\) and \(_{j}^{}\) share the same tree structure (edge lengths are identical), but they are not triangular isomorphic (different dihedral angles); (b) \(_{i}\) and \(_{j}\) are triangular isomorphic but not subgraph isomorphic (the relative distance between the two triangles is different).

\(f:_{i}_{j}\) such that \(h_{f(u)}=h_{u}\) for every node \(u_{i}\), and the following conditions hold respectively:

* **Tree Isometric:** If there exists a collection of group elements \(g_{iu} SE(3)\), such that \((_{f(u)},_{f(i)})=(g_{iu}_{u},g_{iu}_{ i})\) for each edge \(e_{iu}_{i}\);
* **Triangular Isometric:** If there exists a collection of group elements \(g_{iu} SE(3)\), such that the corresponding mutual 3D substructures satisfy: \(_{f(u)-f(i)}=g_{iu}_{u-i}\) for each edge \(e_{iu}_{i}\);
* **Subgraph Isometric:** for any two adjacent nodes \(u,v_{i}\), \(f(u)\) and \(f(v)\) are also adjacent in \(_{j}\), and there exist a single group element \(g_{i} SE(3)\) such that \(g_{i}_{i}=_{j}\).

Note that tree isomorphism only considers edges around a central node, which is of a tree shape. On the other hand, the mutual 3D substructure can be decomposed into a bunch of triangles (since it's contained in adjacent node triplets), which explains the name of triangular isomorphism.

In fact, the three isomorphisms form a hierarchy from micro to macro, in the sense that the following implication relation holds:

**Subgraph Isometric \(\) Triangular Isometric \(\) Tree Isometric**

This is an obvious fact from the above definitions. To deduce the reverse implication relation, we provide a visualized example. Fig. 1 shows two examples of local 3D structures: 1. the first one shares the same tree structure, but is not triangular-isomorphic; 2. the second one is triangular-isomorphic but not subgraph-isomorphic. In conclusion, the following diagram holds:

**Tree Isometric \(\) Triangular Isometric \(\) Subgraph Isometric**

One way to formally connect the expressiveness power of a geometric GNN with their ability of differentiating geometric subgraphs is to define geometric WL tests, the reader can consult . In this paper, we take an intuitive approach based on our nested 3D hierarchy. That is, if two 3D GNN algorithms A and B can differentiate all non-isomorphic local 3D shapes of tree (triangular) level, while A can differentiate at least two more 3D geometries which are non-isomorphic at triangular(subgraph) level than B, then we claim that algorithm A's expressiveness power is more powerful than B.

Since tree isomorphism is determined by the one-hop Euclidean distance between neighbors, distinguishing local tree structures is relatively simple for ordinary 3D equivariant GNNs. For example, the standard baseline SchNet  is one instance of Eq. 1 by setting \(e_{ij}^{t}=(d(_{i},_{j}))\), where \(()\) is a set of radial basis functions. Although it is powerful enough for testing tree non-isomorphism (assuming that \(()\) is injective), we prove in Appendix D that SchNet cannot distinguish non-isomorphic structures at the triangular level.

On the other hand, Wijesinghe and Wang  has shown that by leveraging the topological information extracted from local overlapping subgraphs, we can enhance the expressive power of GNNs to go beyond 2D subtree isomorphism. In our setting, the natural analogue of the overlapping subgraphs is exactly the mutual 3D substructures. Now we demonstrate how to merge the information from 3D substructures to the message passing framework (1). Given an \(SE(3)\)-**invariant** encoder \(\), define the 3D structure weights \(A_{ij}:=(_{i-j})\) for each edge \(e_{ij} E\). Then, the message passing framework (1) is generalized to:

\[m_{i}^{l}=_{j(i)}m_{ij}(h^{l}(_{i}),h^{l}( _{j}),A_{ij}h^{l}(_{j}),e_{ij}^{l}).\] (4)

Formula 4 is an efficient realization of enhancing 3D GNNs by injecting the mutual 3D substructures. However, a crucial question remains to be answered: _Can the generalized message passing framework boost the expressive power of 3D GNNs?_ Under certain conditions, the following theorem provides an affirmative answer:

**Theorem 3.1**.: _Suppose \(\) is a universal SE(3)-invariant approximator of functions with respect to the mutual 3d structures \(_{i-j}\). Then, the collection of weights \(\{\{A_{ij}:=(_{i-j})\}_{e_{ij} E}\}\) enables the differentiation of local structures beyond tree isomorphism. Furthermore, under additional injectivity assumptions (as described in Eq. 16), 3D GNNs based on the enhanced message passing framework (see Section 4) map at least two distinct local 3D subgraphs with isometric local tree structures to different embeddings._Proof.: The detailed proof is provided in Appendix D. 

This theorem confirms that the enhanced 3D GNN (formula 4) is more expressive than the SchNet baseline, at least in testing local non-isomorphic geometric graphs. The existence of such local encoder \(\) is proved in two ways : 1. Equivariant construction by the Atomic Cluster Expansion (ACE) ; 2. Invariant construction under scalarization by edge-wise equivariant frames . Note that there are other different perspectives on characterizing 3D structures, we will also briefly discuss them in Appendix D.

## 4 From local to global: the missing pieces

In the last section, we introduced a local 3D graph isomorphism hierarchy for testing the expressive power of 3D GNNs. Furthermore, we motivated adding a SE(3)-**invariant** encoder to improve the expressive power of one-hop 3D GNNs by scalarizing not only pairwise distances but also their mutual 3D structures in Theorem 3.1. However, to build a powerful 3D GNN, it remains to be analyzed how a 3D GNN acquires higher order (beyond 1-hop neighbors) information by accumulating local messages. A natural question arises: _Are local invariant messages enough for representing global geometric information?_

To formally formulate this problem, we consider a two-hop aggregation case. From fig. 2, the central atom \(a\) is connected with atoms \(b\) and \(c\). Except for the common neighbor \(a\), other atoms that connect to \(b\) and \(c\) form two 3D clusters, denoted by **B**, **C**. Suppose the ground-truth interaction potential of **B** and **C** imposed on atom \(a\) is described by a tensor-valued function \(f_{a}(,)\). Since **B** and **C** are both beyond the 1-hop neighborhood of \(a\), the information of \(f_{a}(,)\) can only be acquired after two steps of message passing: 1. atoms \(b\) and \(c\) aggregate message separately from **B** and **C**; 2. the central atom \(a\) receives the aggregated message (which contains information of **B** and **C**) from its neighbors \(b\) and \(c\).

Let \(S_{}\) (\(S_{}\)) denote the collection of all invariant scalars created by **B** (**C**). For example, \(S_{}\) contains all relative distances and angles within the 3D structure **B**. Then, the following theorem holds:

**Theorem 4.1**.: _Not all types of invariant interaction \(f_{a}(,)\) can be expressed solely as functions of the union of two sets \(S_{}\) and \(S_{}\). In other words, there exists \(E(3)\) invariant function \(f_{a}(,)\), such that it cannot be expressed as functions of \(S_{}\) and \(S_{}\): \(f_{a}(,)(S_{},S_{})\) for arbitrary invariant functions \(\)._

Proof.: The detailed proof is provided in Appendix E. 

This theorem essentially demonstrates that simply aggregating "local" scalar information from different clusters is insufficient for approximating "global" interactions, even in the case of simple **invariant** potential learning tasks. In contrast to the previous section, where the local expressiveness was evaluated based on the ability to classify geometric shapes, in this theorem, we constructed counterexamples using continuous regression functions that depend strictly on information beyond the combination of local invariant scalars. Intuitively, the theorem is proved by introducing two local **equivariant frames** (three independent equivariant vectors, see Appendix B for the definition) determined by **B** (**C**) separately, through which all scalars in \(S_{}\) (\(S_{}\)) can be expressed. However, the transition matrix between these frames is not encoded in the aggregation, resulting in **information loss** when aggregating geometric features from two sub-clusters. In other words, local frames provide local observations of geometric quantities, while the transition matrix reveals the global changes between local geometries. Importantly, the proof also highlights that the missing information that causes the expressiveness gap is solely the Frame Transition (FT) information, which we will define immediately.

Figure 2: Illustrations of different local frames and their transition.

**Frame Transition (FT).** Let \((e_{1}^{i},e_{2}^{i},e_{3}^{i})\) and \((e_{1}^{j},e_{2}^{j},e_{3}^{j})\) be two orthonormal frames in \(^{3}\). These frames are connected by an orthogonal matrix \(R_{ij} SO(3)\):

\[(_{1}^{i},_{2}^{i},_{3}^{i})=R_{ij}(_{ 1}^{j},_{2}^{j},_{3}^{j}).\] (5)

Furthermore, when \((e_{1}^{i},e_{2}^{i},e_{3}^{i})\) and \((e_{1}^{j},e_{2}^{j},e_{3}^{j})\) are equivariant frames, all elements of \(R_{ij}\) are invariant scalars. For instance, in the case of a geometric graph where \(i\) and \(j\) represent indexes of two connected atoms, the fundamental torsion angle \(_{ij}\) in ComeNet  corresponds to one element of \(R_{ij}\) (see Appendix E).

To address this expressiveness gap, one approach is to directly incorporate all invariant edge-wise frame transition matrices (**FT**) into the model. This can be achieved using a geometric formulation based on **neural sheaf diffusion**, as described in Appendix H. However, it is worth noting that this method becomes computationally expensive when dealing with a large number of local clusters, as it requires \(O(k^{2})\) pairs of **FT** for each node. Instead, we propose a more efficient approach by introducing equivariant tensor features for each node \(i\), denoted as \(_{i}\). These tensor features allow us to maintain equivariant frames directly within each \(_{i}\). We prove in Appendix E that **FT** can be easily derived through equivariant message passing and updating when utilizing these equivariant tensor features.

**Equivariant Message Passing.** Similarly with the standard one-hop message passing scheme 1, the aggregated tensor message \(_{i}\) from the \(l-1\) layer to the \(l\) layer can be written as: \(_{i}^{l-1}=_{j N(i)}_{j}^{l-1}\). Since summation does not break the symmetry rule, it is obvious that \(_{i}^{l-1}\) are still equivariant tensors. However, the nontrivial part lies in the design of the **equivariant update** function \(\):

\[_{i}^{l}=(_{i}^{l-1}).\] (6)

To fully capture the information of **FT**, it is necessary for \(\) to possess sufficient expressive power while maintaining \(SE(3)\) equivariance. Here, we propose a novel way of updating scalar and tensor messages by performing node-wise scalarization and tensorization blocks (the **FTE** module of Figure 3). From the perspective of Eq. 2, \((_{u})\) is transformed equivariantly as:

\[(g_{u})=_{i=0}^{l}^{i}(g)_{i}( _{u}),\ \ g SE(3).\] (7)

Here, \((_{u})\) is decomposed to \((_{0}(_{u}),,_{l}(_{u}))\) according to different tensor types, and \(\{^{i}(g)\}_{i=0}^{l}\) is a collection of different \(SE(3)\)**tensor representations** (see the precise definition in Appendix A).

To illustrate the benefit of aggregating equivariant messages from local patches, we study a simple case. Let \(f_{a}(,)=_{B}_{C}\) be an invariant function of **B** and **C** (see Fig. 2), then \(f_{a}\) can be calculated by a direction composition of scalar messages and equivariant vector messages: \(f_{a}(,)=[\|_{a}\|^{2}- \|_{B}\|^{2}-\|_{C}\|^{2}]\), where \(_{a}=_{B}+_{C}\) is an equivariant vector. Note that \(_{a}\) follows the local equivariant aggregation formula 6, and the other vectors' norm \(\|_{B}\|\) and \(\|_{C}\|\) are obtained through local scalarization on atoms \(b\) and \(c\). As a comparison, it's worth mentioning that \(f_{a}(,)\) can also be expressed by local scalarization with the additional transition matrix data \(R_{BC}\) defined by Eq. 5. Let \(_{B}\) and \(_{C}\) be the scalarized coordinates with respect to two local equivariant frames \(_{B}\) and \(_{C}\). Then \(f_{a}(,)=[\|R_{BC}^{-1}_{B}+ _{C}\|^{2}-\|_{B}\|^{2}-\|_{C }\|^{2}].\) However, it requires adding the transition matrix \(R_{BC}\) for each \((,)\) pair into the aggregation procedure, which is computationally expensive compared to directly implementing equivariant tensor updates.

## 5 Building an efficient and expressive equivariant 3D GNN

We propose to leverage the full power of **LSE** and **FTE** modules (along with a necessary **tensor update** module) to push the limit of efficient and expressive 3D equivariant GNNs design. Specifically, we improve a recently proposed method based on constructing local frames but without the implementation of **LSE** and **FTE**. We also analyze related works following this framework detailed in Appendix G.

**LSE Instantiation.** We propose to apply edge-wise equivariant frames (following ) to encode the local 3D structures \(_{i-j}\). By definition, \(_{i-j}\) contains edge \(e_{ij}\), nodes \(i\) and \(j\), and their common neighbors. We use the equivariant frame \(_{ij}\) built on \(e_{ij}\) (see the precise formula in Appendix F) to scalarize \(_{i-j}\). After scalarization (8), the equivariant coordinates of all nodes in \(_{i-j}\) are transformed into invariant coordinates: \(\{_{k}_{k}_{k}_{i-j}\}\). To encode these scalars sufficiently, we first weight each \(_{k}\) by the **RBF** distance embedding: \(_{k}(\|_{k}\|)( _{k})\) for each \(_{k}_{i-j}\). Note that to preserve the permutation symmetry, the MLP is shared among the nodes. Finally, the 3D structure weight \(A_{ij}\) is obtained by the average pooling of all node features.

**FTE Instantiation.** We propose to introduce equivariant tensor message passing and update function for encoding local **FT** information. At initialization, let \(^{l}(_{i},_{j})\) denote the embedded tensor-valued edge feature between \(i\) and \(j\). We split it into two parts: 1. the scalar part \(^{l}(_{i},_{j})\) for aggregating invariant messages; 2. the higher order tensor part \(^{l}(_{i},_{j})\) for aggregating tensor messages. To transform \(^{l}(_{i},_{j})\), we turn to the equivariant frame \(_{ij}\) once again. After scalarization by \(_{ij}\), \(^{l}(_{i},_{j})\) becomes a tuple of scalars \(}^{l}(_{i},_{j})\), which is then transformed by MLP. Finally, we output arbitrary tensor messages through equivariant tensorization 22:

\[}^{l}(_{i},_{j})[_ {ij}]{}^{l+1}(_{i},_{j}).\]

Further details are provided in Appendix F. As we have discussed earlier, the node-wise update function \(\) in Eq. 6 is also one of the guarantees for a powerful **FTE**. As a comparison, \(\) is usually a standard MLP for updating node features in 2D GNNs, which is a **universal approximator** of invariant functions. Previous works  updated equivariant features by taking linear combinations and calculating the invariant norm of tensors, which may suffer from information loss. Then a natural question arises: _Can we design an equivariant universal approximator for tensor update?_ We answer this question by introducing a novel node-wise frame. Consider node \(i\) with its position \(_{i}\), let \(}_{i}:=_{_{j} N(_{i})} _{j}\) be the center of mass around \(_{i}\)'s neighborhood. Then the orthonormal equivariant frame \(_{i}:=(_{1}^{i},_{2}^{i},_{3}^{i})\) with respect to \(_{i}\) is defined similar to the edge-wise frame between \(_{i}\) and \(}_{i}\) in , detailed in appendix B. Finally, we realize a powerful \(\) by the following theorem:

**Theorem 5.1**.: _Equipped with an equivariant frame \(_{i}\) for each node \(i\), the equivariant function \(\) defined by the following composition is a universal approximator of tensor transformations: \(:\)._

Proof.: The detailed proof is provided in Appendix F. 

**LEFTNet.** An overview of our \(\{,\}\) enhanced efficient graph neural network (LEFTNet) is depicted in Fig. 3. The detailed algorithm for LEFTNet is shown in Algorithm 1 of Appendix C. LEFTNet receives as input a collection of node embeddings \(\{v_{1}^{0},,v_{N}^{0}\}\), which contain the atom types and 3D positions for each node: \(v_{i}^{0}=(z_{i},_{i})\), where \(i\{1,,N\}\). For each edge \(e_{ij} E\)

Figure 3: Illustrations of our modular framework for building equivariant GNNs and the realization of LEFTNet. Each interaction block contains **LSE** to encode local 3D structures, equivariant message passing to update both invariant (unbold letters, e.g. \(h_{i}\)) and equivariant (**bold** letter, e.g. \(_{i}\)) features, and **FTE** to encode frame transition. \(_{i-j}\) is the local 3D structure of each edge \(e_{ij}\). \(_{ij}\) and \(_{i}\) are the equivariant frames for each edge \(e_{ij}\) and node \(i\). \(\) indicates element-wise multiplication, and \(\) indicates concatenation. Note that we do not include \(_{ij}\) in the figure since, practically, they are generated based on \(_{i}\) and \(_{j}\).

we denote the associated equivariant features consisting of tensors by \(_{ij}\). During each messaging passing layer, the **LSE** module outputs the scalar weight coefficients \(A_{ij}\) as enhanced invariant edge feature and feed into the interaction module. Moreover, scalarization and tensorization as two essential blocks are used in the equivariant update module that fulfills the function of **FTE**. The permutation equivariance of a geometric graph is automatically guaranteed for any message passing architecture, we provide a complete proof of SE(3)-equivariance for LEFTNet in Appendix F.

**SE(3) vs E(3) Equivariance.** Besides explicitly fitting the \(SE(3)\) invariant molecular geometry probability distribution, modeling the energy surface of a molecule system is also a crucial task for molecule property prediction. However, the Hamiltonian energy function \(E\) of a molecule is invariant under refection transformation: \(()=(R)\), for arbitrary reflection transformation \(R E(3)\). In summary, there exist two different inductive biases for modeling 3D data: **(1)** SE(3) equivariance, e.g. chirality could turn a therapeutic drug to a killing toxin; **(2)** E(3) equivariance, e.g. energy remains the same under reflections.

Since we implement \(SE(3)\) equivariant frames in LEFTNet, our algorithm is naturally \(SE(3)\) equivariant. However, our method is **flexible** to implement \(E(3)\) equivariant tasks as well. For \(E(3)\) equivariance, we can either replace our frames to \(E(3)\) equivariant frames, or modify the scalarization block by taking the absolute value: \(:= e_{1}, e_{2}, e_{3})}_{SE(3)} e_{1},| e_{2}|, e_{3})}_{E(3)}\).

Intuitively, since the second vector \(e_{2}\) is a pseudo-vector, projections of any equivariant vectors along the \(e_{2}\) direction are not \(E(3)\) invariant until taking the absolute value.

**Efficiency.** To analyze the efficiency of LEFTNet, suppose 3D graph \(G\) has \(n\) vertices, and its average node degree is \(k\). Our algorithm consists of three phases: 1. Building equivariant frames and performing local scalarization; 2. Equivariant message passing; 3. Updating node-wise tensor features through scalarization and tensorization. Let \(l\) be the number of layers, then the computational complexity for each of our three phases are: 1. \(O(nk)\) for computing the frame and local (1-hop) 3D features; 2. \(O(nkl)\) for 1-hop neighborhood message aggregation; 3. \(O(nl)\) for node-wise tensorization and feature update.

## 6 Experiments

We evaluate our LEFTNet on both scalar value (e.g. energy) and vector value (e.g. forces) prediction tasks. The scalar value prediction experiment is conducted on the QM9 dataset  which includes \(134k\) small molecules with quantum property annotations; the vector value prediction experiment is conducted on the MD17 dataset  and the Revised MD17(rMD17) dataset  which includes the energies and forces of molecules. We compare LEFTNet with a list of state-of-the-art equivariant (invariant) GNNs including SphereNet , PaiNN , Equiformer , GemNet , etc [28; 38; 13; 39; 40; 14; 16; 41; 42; 43; 44; 45; 46; 47; 48].The training details, results on rMD17, etc. are listed in Appendix I.

   Task & \(\) & \(_{E}\) & \(_{}\) & \(_{}\) & \(\) & \(C_{}\) & \(G\) & \(H\) & \(R^{2}\) & \(U\) & \(U_{0}\) & ZPVE \\ Units & bohr\({}^{3}\) & meV & meV & meV & D & cal/mol K & meV & meV & bohr\({}^{3}\) & meV & meV & meV \\  NMP &.092 & 69 & 43 & 38 &.030 &.040 & 19 & 17 &.180 & 20 & 20 & 1.50 \\ Cormorant &.085 & 61 & 34 & 38 &.038 &.026 & 20 & 21 &.961 & 21 & 22 & 2.03 \\ LieConv &.084 & 49 & 30 & 25 &.032 &.038 & 22 & 24 &.800 & 19 & 19 & 2.28 \\ TFN &.223 & 58 & 40 & 38 &.064 &.101 & - & - & - & - & - & - \\ SE(3)-Tr. &.142 & 53 & 35 & 33 &.051 &.054 & - & - & - & - & - \\ EGNN &.071 & 48 & 29 & 25 &.029 &.031 & 12 & 12 & **.106** & 12 & 11 & 1.55 \\ SEGAN &.060 & 42 & 24 & 21 &.023 &.031 & 15 & 16 &.660 & 13 & 15 & 1.62 \\ ClotNet &.063 & 53 & 33 & 25 & 040 &.027 & 9 &.610 & 9 & 8 & **.123** \\ EQGAT &.063 & 44 & 26 & 22 &.014 &.027 & 12 & 13 &.257 & 13 & 13 & 1.50 \\ Equiformer &.056 & **33** & **17** & **16** &.014 &.025 & 10 & 10 &.227 & 11 & 10 & 1.32 \\ LEFTNet (ours) & **.048** & 40 & 24 & 18 & **.012** & **.023** & **7** & **6** &.109 & **7** & **6** & 1.33 \\  Schnet &.235 & 63 & 41 & 34 &.033 &.033 & 14 & 14 &.073 & 19 & 14 & 1.70 \\ DimeNet++ &.044 & 33 & 25 & 20 &.030 &.023 & 8 & 7 &.331 & 6 & 6 & 1.21 \\ SphereNet &.046 & **32** & **23** & **18** &.026 & **.021** & 8 & 6 &.292 & 7 & 6 & **1.12** \\ ClotNet &.053 & 49 & 33 & 25 &.038 &.026 & 9 & 8 &.425 & 8 & 8 & 1.59 \\ PaiNN &.045 & 46 & 28 & 20 &.012 &.024 & 7 & 6 & **.066** & 6 & 6 & 1.28 \\ LEFTNet (ours) &.039 & 39 & **23** & **18** & **.011** &.022 & **6** & **5** &.094 & **5** & **5** & 1.19 \\   

Table 1: Mean Absolute Error for the molecular property prediction benchmark on QM9 dataset. (The best results are **bolded** and the second best are underlined.)

### QM9 - scalar-valued property prediction

The QM9 dataset is a widely used dataset for predicting molecular properties. However, existing models are trained on different data splits. Specifically, Cormorant , EGNN , etc., use 100k, 18k, and 13k molecules for training, validation, and testing, while DimeNet , SphereNet , etc., split the data into 110k, 10k, and 11k. For a fair comparison with all methods, we conduct experiments using both data splits. Results are listed in Table 1. For the first data split, LEFTNet is the best on 7 out of the 12 properties and improves previous SOTA results by 20% on average. Consistently, LEFTNet is the best or second best on 10 out of the 12 properties for the second split. These results validate the effectiveness of LEFTNet. Detailed ablation in Appendix I shows that both **LSE** and **FTE** contribute to the final performance. In addition, we compare the forward time of existing methods to show the efficiency of LEFTNet. Specifically, we use the same batch size for all methods and report the one epoch training time. Fig. 4 shows that LEFTNet can achieve the best performance using similar forward time as SchNet, ClofNet, and ComENet and is much faster than DimeNet and SphereNet. This is consistent with our efficiency analysis in Sec. 5 and Table 4.

### MD17 - vector-valued property prediction

We evaluate LEFTNet to predict forces on the MD17 dataset. Following existing studies [28; 38; 15], we train a separate model for each of the 8 molecules. Both training and validation sets contain 1000 samples, and the rest are used for testing. Note that all baseline methods are trained on a joint loss of energies and forces, but different methods use different weights of force over energy (WoFE). For example, SchNet  sets WoFE as 100, while GemNet  uses a weight of 1000. For a fair comparison with existing studies, we conduct experiments on two widely used weights of 100 and 1000 following Liu et al. . The results are summarized in Table 2. Results show that when WoFE is 100, LEFTNet outperforms all baseline methods on 5 of the 8 molecules. In addition, LEFTNet can outperform all baseline methods on 6 of the 8 molecules when WoFE is 1000. These experimental results on MD17 demonstrate the performance of LEFTNet on vector-valued property prediction tasks. The ablation results in Table 3 demonstrate that both **LSE** and **FTE** are important to the final results. Specifically, the algorithm for **LSE** only is in Algorithm 1 of Appendix C. Note that the original MD17 dataset we used has numerical noise , and a recomputed version of MD17, called Revised MD17 (rMD17)  is proposed to reduce the numerical noise. In addition to the original MD17, we also conduct experiments on the rMD17, and the results are listed in Appendix I.

## 7 Limitation and future work

In this paper, we seek a general recipe for building 3D geometric graph deep learning algorithms. Considering common prior of 2D graphs, such as permutation symmetry, has been incorporated in off-the-shelf graph neural networks, we mainly focus on the \(E(3)\) and \(SE(3)\) symmetry specific to 3D geometric graphs. Despite our framework being general for modeling geometric objects, we only conducted experiments on commonly used molecular datasets. It's worth exploring datasets in other domains in the future.

    &  &  &  \\  Molecule & \(\)GDML & SchNet & DimeNet & SphereNet & SpoolyNet & LeFTNet & SphereNet & GemNet & LEFTNet & PairNN & NetworkNet \\  Aspin & 0.68 & 1.35 & 0.499 & 0.430 & **0.258** & 0.300 & 0.209 & 0.217 & **0.196** & 0.371 & 0.348 \\ Benzene & 0.20 & 0.31 & 0.187 & 0.178 & - & **0.415** & 0.147 & 0.145 & **0.142** & - \\ Ethanol & 0.33 & 0.39 & 0.230 & 0.208 & **0.094** & 0.138 & 0.091 & **0.086** & 0.099 & 0.230 & 0.264 \\ Malondialdehydedehyde & 0.41 & 0.66 & 0.383 & 0.340 & **0.167** & 0.209 & 0.172 & 0.155 & **0.142** & 0.319 & 0.323 \\ Napublance & 0.11 & 0.58 & 0.215 & 0.178 & 0.089 & **0.073** & 0.048 & 0.051 & **0.044** & 0.083 & 0.084 \\ Salicylic acid & 0.28 & 0.85 & 0.374 & 0.360 & 0.180 & **0.167** & **0.113** & 0.125 & 0.117 & 0.209 & 0.197 \\ Tolerance & 0.14 & 0.57 & 0.210 & 0.155 & 0.087 & **0.084** & 0.054 & 0.060 & **0.049** & 0.102 & 0.088 \\ Uracil & 0.24 & 0.56 & 0.301 & 0.267 & 0.119 & **0.116** & 0.106 & 0.097 & **0.085** & 0.140 & 0.149 \\   

Table 2: Mean Absolute Error for per-atom forces prediction (kcal/mol Å) on MD17 dataset. The best results are **bolded**.

Figure 4: Comparisons of existing methods in terms of the training time and the MAE of the property \(U_{0}\) of QM9.

   Method & MAE \\  w/o **LSE** and **FTE** & 1.083 \\
**LSE** only & 0.451 \\
**LSE** + **FTE** & **0.300** \\   

Table 3: Ablation study on Aspirin of the MD17 dataset. Detailed ablations on other molecules are listed in Appendix ITo elucidate the future design space of equivariant GNNs, we propose two directions that are worth exploring. Firstly, our current algorithms consider fixed equivariant frames for performing aggregation and node updates. Inspired by the high body-order ACE approach  (for modeling atom-centered potentials), it is worth investigating in the future if equivariant frames that relate to many body (e.g., the PCA frame in ) can boost the performance of our algorithm. For example, to build the A-basis proposed in , we can replace our message aggregation Eq. 6 from summation to tensor product, which is also a valid pooling operation. Another direction is to explore geometric mesh graphs on manifolds \(M\), where the local frame is defined on the tangent space of each point: \((x) T_{x}M\). Since our scalarization technique (crucial for realizing **LSE** in LEFTNet) originates from differential geometry on frame bundles , it is reasonable to expect that our framework also works for manifold data .