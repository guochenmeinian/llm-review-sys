# On the Efficiency of ERM in Feature Learning

Ayoub El Hanchi

University of Toronto &

Vector Institute

aelhan@cs.toronto.edu

&Chris J. Maddison

University of Toronto &

Vector Institute

cmaddis@cs.toronto.edu

&Murat A. Erdogdu

University of Toronto &

Vector Institute

erdogdu@cs.toronto.edu

###### Abstract

Given a collection of feature maps indexed by a set \(\), we study the performance of empirical risk minimization (ERM) on regression problems with square loss over the union of the linear classes induced by these feature maps. This setup aims at capturing the simplest instance of feature learning, where the model is expected to jointly learn from the data an appropriate feature map and a linear predictor. We start by studying the asymptotic quantiles of the excess risk of sequences of empirical risk minimizers. Remarkably, we show that when the set \(\) is not too large and when there is a unique optimal feature map, these quantiles coincide, up to a factor of two, with those of the excess risk of the oracle procedure, which knows a priori this optimal feature map and deterministically outputs an empirical risk minimizer from the associated optimal linear class. We complement this asymptotic result with a non-asymptotic analysis that quantifies the decaying effect of the global complexity of the set \(\) on the excess risk of ERM, and relates it to the size of the subbepiliary sets of the suboptimality of the feature maps. As an application of our results, we obtain new guarantees on the performance of the best subset selection procedure in sparse linear regression under general assumptions.

## 1 Introduction

A central idea in modern machine learning is that of data-driven feature learning. Specifically, instead of performing linear prediction on top of handcrafted features, the current dominant paradigm suggests to use models that select useful features for linear prediction in a data-dependent way [e.g. KSH12; LBH15; He+16; Vas+17]. Of course, by putting the burden of picking a feature map on the model and data, we should expect that the resulting learning problem will require more samples to be solved. But just how many more samples do we need to learn such feature-learning-based models?

In this paper, we investigate this question in a general setting. We study the performance of empirical risk minimization (ERM) on regression tasks with square loss and over model classes induced by arbitrary collections of features maps. More precisely, let \(X\) be the random input taking value in a set \(\), and let \((_{t})_{t}\), \(_{t}:^{d}\), be a collection of feature maps indexed by a set \(\). For a given regression task and i.i.d. samples, our aim is to understand the performance of ERM over the class of predictors \(_{t}\{x(w,_{t}(x)) w^{d}\}\) as a function of the sample size, the distribution of the data, and relevant properties of the collection of feature maps \((_{t})_{t}\).

Classical uniform-convergence-based analyses would suggest that the performance of ERM in this setting is determined by the size of the model class, appropriately measured. The main message of this paper is that in this case, this is wrong in a strong sense. Specifically, we prove an upper bound on the excess risk of ERM on this problem whose dependence on the size of the model class decays monotonically with the sample size, and eventually depends only on the size of the model class induced by the collection of optimal feature maps, which is typically much smaller.

**Formal setup.** We briefly formalize our problem here. Let \(X\) be the random input taking value in a set \(\), and let \((_{t})_{t}\), \(_{t}:^{d}\), be a collection of feature maps indexed by a set \(\).1 Let \(Y\) be the output random variable, jointly distributed with the input \(X\). Our goal is to learn to predict the output \(Y\) given the input \(X\) as well as possible within the class of predictors \(x w,_{t}(x)(t,w) ^{d}}\). We evaluate the quality of a single prediction \(\) given the ground truth \(y\) through the loss function \((,y):=(-y)^{2}/2\), and the overall quality of a predictor \((t,w)^{d}\) through its risk

\[R(t,w):=[((w,_{t}(X)),Y)], R_{*}:=_{(t,w) ^{d}}R(t,w).\]

We assume that we have access to \(n\) i.i.d. samples \((X_{i},Y_{i})_{i=1}^{n}\) with the same distribution as \((X,Y)\), and perform empirical risk minimization

\[(_{n},_{n})*{argmin}_{(t,w) ^{d}}R_{n}(t,w) R_{n}(t,w):=n^{-1}_{i=1}^{n}(  w,_{t}(X_{i}),Y_{i}).\]

Our goal is to characterize the excess risk \((_{n},_{n}):=R(_{n},_{n})-R_{*}\).

**Related work.** The study of upper bounds on the excess risk of ERM in a general setting is a classical topic. It was initiated by Vapnik and Chervonenkis  who established a link between the excess risk of ERM and the uniform convergence of the underlying empirical process. More recently, and fuelled by the development of Talagrand's concentration inequality  and its refinements [e.g. BLM00; Bou02], a literature emerged that provided more fine-grained control of the excess risk of ERM [e.g. BBM05; Kol06; BM06]. A key idea emerging from this line of work is localization. This concept, and in particular the iterative localization method of Koltchinskii , plays an important role in our development. We refer the reader to the books , as well as the recent articles  for more on this idea.

Focusing on the task of regression with square loss, upper bounds on the excess risk of ERM are available for many classes of predictors, including finite [e.g. Aud07; JRT08; LM09], linear [e.g. LM16; Oli16; Mou22], and convex classes [e.g. LM16a; Men14; LRS15]. A key development in this area over the last decade has been the realization that such bounds can be obtained under much weaker assumptions than previously thought, owing to the fact that only one-sided control of a certain empirical process is needed, and which can be obtained under very weak assumptions . The line of work most closely related to ours is the one on random-design linear regression , and we view our work as an extension of this literature. We review these results in more detail in Section 2.

Finally, and on a more conceptual level, our work is related to the recent effort to understand the effect of feature learning on the performance of neural networks [e.g. Bac17; Gho+20; Ba+22]. Beyond this conceptual connection however, our work is quite distinct from this literature. Among other things, our setting is more general since we consider arbitrary features maps. In the same vein, it is worth mentioning the line of work on multiple kernel learning [e.g. Lan+04; GA11; SD16], although we are not aware of results from this literature that are directly relevant to our setup.

**Challenges.** Our class of predictors is somewhat unstructured (e.g. it is in general non-convex), so that off-the-shelf results from the above literature are not directly applicable. Nevertheless, the analysis of the performance of ERM on linear classes provides a good starting point as we review in Section 2. Compared to that setting however, we are faced with two additional challenges. First, we need to control an additional source of error arising from the fact that ERM might select a suboptimal feature map. Second, we are lead to study the suprema of certain \(\)-indexed empirical processes, which in the linear setting reduce to single random variables that are easily dealt with.

**Organization.** The rest of the paper is organized as follows. In Section 2, we review known results on the excess risk of linear regression under square loss. In Section 3, we state our main results that hold for the excess risk of ERM for general index sets \(\). In Section 4, we specialize our analysis to the case where the index set \(\) is finite, obtain more explicit guarantees, and discuss their implications on the sparse linear regression problem. We conclude in Section 5 with a brief discussion.

Background

The goal of this section is to provide more context for our results. We review known results on the excess risk of ERM over linear classes, which corresponds in our setting to the special case where the set \(\) indexing the feature maps is a singleton. As such, to avoid introducing further notation, we use the one from the previous section, while dropping the dependence on \(t\) whenever it occurs.

In the setting of linear regression with square loss, and when the sample covariance matrix of the feature map is invertible, there is a unique empirical risk minimizer and its excess risk admits an explicit expression. Specifically, define

\[:=(X)(X)^{T},_{n}:=_{i=1}^{n}(X_{i})(X_{i})^{T},\]

and let \(w_{*}\) denote the unique minimizer of the risk \(R(w)\).2 Then, an elementary calculation shows that when \(_{n}\) is invertible, there is a unique empirical risk minimizer and it satisfies

\[_{n}=w_{*}-_{n}^{-1} R_{n}(w_{*}).\] (1)

Furthermore, since the risk is a quadratic function of \(w\) whose gradient at \(w_{*}\) vanishes, replacing \(R(_{n})\) by the equivalent exact second order Taylor expansion around \(w_{*}\) yields

\[R(_{n})-R(w_{*})=\|_{n}-w_{*}\|_{}^{2}= {2}_{n}^{-1} R_{n}(w_{*})_{}^{2}.\] (2)

While exact, this expression is not readily interpretable. For example, how fast does this excess risk go to \(0\) as a function of the sample size? The following classical result from asymptotic statistics [e.g. 14, 15, 16] makes this rate more explicit. To state it, we define

\[g(X,Y):=_{w}( w_{*},(X),Y), G:= g(X,Y)g(X,Y)^{T}.\]

**Theorem 1**.: _Assume that for all \(j[d]\), \(_{j}^{2}(X)<\), \(Y^{2}<\), and \([\|g(X,Y)\|_{^{-1}}^{2}]<\). Then, as \(n\),_

\[n(_{n})[]{d}\|Z\|_{2}^{2},\]

_where \(Z(0,^{-1/2}G^{-1/2})\). In particular, for any \((0,0.1)\),_

\[_{n}n Q_{(_{n})}(1-) [\|g(X,Y)\|_{^{-1}}^{2}]+2_{}(^{-1/2}G^{-1 /2})(1/),\]

_where \(Q_{X}(p):=\{x(X x) p\}\) is the quantile function of a random variable \(X\), and where we write \(a b\) to mean that there exists absolute constants \(C,c\) such that \(c b a C b\). In the above statement, they can be taken as \(C=1\) and \(c=1/32\)._

We provide a proof in Appendix A for completeness. For our purposes, this theorem is most easily interpreted as follows: for large enough \(n\) and small enough \(\), if the excess risk of ERM is bounded by some quantity with probability at least \(1-\), then this quantity is, up to a constant, at least as large as the right-hand side of the second displayed equation divided by \(n\). While our primary interest is in non-asymptotic bounds, this asymptotic result, by virtue of its exactness, provides us with a benchmark against which such bounds can be compared. In particular, it identifies the quantity \([\|g(X,Y)\|_{^{-1}}^{2}]\) as an intrinsic parameter determining the excess risk of ERM on this problem.

For large enough \(n\), Theorem 1 gives an interpretable expression for the excess risk. However, it says nothing about how large \(n\) needs to be for this expression to be accurate. This motivates a non-asymptotic analysis of the excess risk of ERM, which has been carried out numerous times in recent years [e.g. 13, 14, 15]. A goal of this literature has been to obtain upper bounds on the excess risk of ERM that hold in probability under weak moment assumptions, building on the observation that this is indeed possible . The following theorem is comparable to the best known result in this area. We leave the proof to Appendix B. To state it, we define

\[V:=^{-1/2}(X)(X)^{T}^{-1/2}-I ^{2}, L:=_{v S^{d-1}}  v,^{-1/2}(X)^{2}-1^{2}.\]

**Theorem 2**.: _Assume that for all \(j[d]\), \(\![_{j}^{4}(X)]<\), \(\![Y^{2}]<\), and \(\![\|g(X,Y)\|_{^{-1}}^{2}]<\). Let \((0,1)\). If_

\[n(512_{}(V)+6)(ed)+(128L+11)(2/),\]

_then with probability at least \(1-\),_

\[(_{n}) 4(n)^{-1}\![\|g (X,Y)\|_{^{-1}}^{2}].\]

At a high-level, this result says that above a certain explicit minimal sample size, the asymptotic expression of the excess risk of Theorem 1 is correct, up to a significantly worse dependence on \(\). The restriction on the sample size is almost the best one can hope for. To see why, note that to get guarantees on the excess risk of _any_ empirical risk minimizer, we need at least that \(_{n}\) is invertible, otherwise there exists an empirical risk minimizer arbitrarily far away from \(w_{*}\). To get quantitative guarantees, we need slightly more control in the form of a lower bound on \(_{}(^{-1/2}_{n}^{-1/2})\). We refer the reader to a more detailed discussion in [1, Section 5].

This result has two key qualities, which we aim to reproduce in our results. First, it is assumption-lean, requiring nothing more than a fourth moment assumption on the coordinates of the feature map compared to Theorem 1. Second, it recovers the right dependence on the intrinsic parameter \(\![\|g(X,Y)\|_{^{-1}}^{2}]\) identified in Theorem 1. A downside of this generality is the bad dependence on \(\). Without further assumptions, this cannot be improved; we refer the reader to the recent literature on robust linear regression for more on this issue [e.g. 1, 10, 11].

## 3 Main Results

In this section we state our main results. They are most easily seen as extensions of Theorems 1 and 2 for general index sets \(\). In Section 3.1, we study the asymptotics of the excess risk of ERM in our setting, and in Section 3.2, we present a non-asymptotic upper bound on the excess risk.

To state our results, we require additional definitions and notation. We start with the population and the sample covariance matrices

\[(t):=\![_{t}(X)_{t}(X)^{T}], _{n}(t):=n^{-1}_{i=1}^{n}_{t}(X_{i})_{t}(X_{i})^{T}.\]

We define the following collection of minimizers,

\[w_{*}(t):=*{argmin}_{w^{d}}R(t,w), _{*}:=*{argmin}_{t}R(t,w_{*}(t)),\]

the first is uniquely defined, while the second is set-valued in general. We define the gradient of the loss at these minimizers and their corresponding covariance matrices

\[g(t,(X,Y)):=_{w}( w_{*}(t),_{t}(X),Y), G(t, s):=\![g(t,(X,Y))g(s,(X,Y))^{T}].\]

Finally, we introduce the following processes which play a key role in our development

\[_{n}(t):=_{}(I-^{-1/2}(t) _{n}(t)^{-1/2}(t)), G_{n}(t):=\|_{w}R_{n} (t,w_{*}(t))\|_{^{-1}(t)},\] (3)

as well as, for \(t_{*}_{*}\) and \(t_{*}\),

\[_{n}(t,t_{*}):=1-(t,w_{*}(t))-R_{n}(t_{ *},w_{*}(t_{*}))}{R(t,w_{*}(t))-R_{*}}.\] (4)

We note that the process \((_{n}(t,t_{*}))_{t_{*}}\) is an empirical process (see  for an introduction), while \((_{n}(t))_{t}\) and \((G_{n}(t))_{t}\) are partial suprema of empirical processes. In the sequel, we will slightly abuse this terminology, and call all of these empirical processes, with the understanding that they can be viewed as one with more indexing. We will further assume that these processes are separable; see [1, p.305-306] for a definition. This covers a wide range of applications, while avoiding delicate measurability issues. The suprema of such separable processes, which is the only way they enter our results, can be studied by taking the supremum over a countable dense subset of the index set. Therefore, without loss of generality, we assume that \(\) is countable.

Finally, in line with the literature on the theory of empirical processes , we say that a sequence of empirical processes is Glivenko-Cantelli if, when rescaled by \(n^{-1/2}\), the supremum of their absolute value taken over their index set converges to zero in probability as \(n\). In other words, the weak law of large numbers holds uniformly over the index set. Similarly, we say that a sequence of empirical processes is Donsker if it converges in distribution to its limiting Gaussian process.3 In other words, the central limit theorem holds uniformly over the index set.

### Asymptotic result

Our first main result is an asymptotic characterization of the quantiles of the excess risk of any sequence of empirical risk minimizers in our setting, which vastly generalizes that of Theorem 1.

**Theorem 3**.: _Assume that \(_{*}\) and for some \(t_{*}_{*}\), assume that the empirical processes \((_{n}(t))_{t}\), \(((t,t_{*}))_{t_{*}}\) and \((G_{n}(t))_{t}\) are Glivenko-Cantelli. Then, for all \(>0\),_

\[_{n}R(_{n},w_{*}(_{n}))-R_{*}> =0.\]

_Furthermore, if the sequence of processes \((G_{n}(t))_{t}\) is Donsker, then for any \((0,1)\),_

\[ Q_{Z^{-}}(1-)_{n}n Q_{(_{n},_{n})}(1-)_{n}n Q_{ (_{n},_{n})}(1-) Q_{Z^{+}}(1-),\]

_where \(Z^{-}:=_{s_{*}} Z(s)_{2}^{2}\), \(Z^{+}:=_{s_{*}} Z(s)_{2}^{2}\), and \((Z(t))_{t}\) is a mean-zero Gaussian process with covariance function \([Z(t)Z(s)^{T}]=^{-1/2}(t)G(t,s)^{-1/2}(s)\) for all \(t,s\)._

We note that, up to a factor of two in the upper bound on the asymptotic quantiles, Theorem 3 reduces to Theorem 1 when \(\) is a singleton, with the exact same assumptions. We are not aware of comparable results in the literature. The proof of Theorem 3 can be found in Appendix D.

_Remark 1_.: For small \(\), the upper bound admits the more interpretable expression

\[Q_{Z_{+}}(1-)_{s_{*}} Z (s)_{2}^{2}+2(1/)_{s_{*}}_{ }(^{-1/2}(s)G(s,s)^{-1/2}(s)).\] (5)

Furthermore, if \(_{*}\) is finite, the first term can be upper bounded as

\[_{s_{*}} Z(s)_{2}^{2}  80(1+_{*})_{s_{*}} [ g(s,(X,Y))_{^{-1}(s)}^{2}].\] (6)

To see why Theorem 3 is surprising, let us first focus on the case where \(_{*}\) has a unique element \(t_{*}\), so that \(Z_{+}=Z_{-}}{{=}} Z_{2}^{2}\) where \(Z(0,^{-1/2}(t_{*})G(t_{*},t_{*})^{-1/2}(t_{*}))\). Now consider the oracle procedure, which knows beforehand what the optimal feature map \(t_{*}\) is, and outputs \(t_{*}\) and a minimizer of \(R_{n}(t_{*},w)\). Theorem 3 says that, up to a factor of two, the asymptotic quantiles of the excess risk of ERM, which needs to learn over the large class \(_{t}x w,_{t}(x) w ^{d}}\), coincide with those of the oracle procedure (by Theorem 1), which only needs to learn over the _linear_ class \(x w,_{t_{*}}(x) w^{d}}\)!

More generally, Theorem 3 establishes that asymptotically, any ERM picks a near-optimal feature map with probability one. It furthers shows that the asymptotic quantiles of the excess risk of any sequence of ERMs is controlled from above and below by those of the extrema of the limiting Gaussian process of \((G_{n}(t))_{t}\) on the set of optimal feature maps \(_{*}\). This is surprising, as it implies that asymptotically, and outside of its role in determining whether the assumptions of Theorem 3 hold, the global complexity of the set \(\) is irrelevant to the excess risk of ERM.

Finally, we note that the Glivenko-Cantelli and Donsker assumptions in Theorem 3 can equivalently be viewed as restrictions on the size of \(\), for distribution and process dependent notions of size. We refer the reader to the books  for more on this connection. With this observation, the main takeaway from Theorem 3 can be stated as follows.

_Asymptotically, if \(\) is not too large, the excess risk of ERM depends, at worst, only on the complexity of the set of optimal feature maps \(_{*}\), and is independent of the global complexity of \(\)._

### Non-asymptotic result

The result in Theorem 3 hints at a dramatic localization phenomenon, whereby the influence of the size and complexity of the collection of feature maps \((_{t})_{t}\) on the excess risk of ERM vanishes as \(n\) under appropriate assumptions. The root of this localization phenomenon is the first statement of Theorem 3: eventually, ERM picks near-optimal feature maps with probability approaching one. For small enough sample sizes however, it is clear that ERM is likely to select suboptimal feature maps, so that this localization phenomenon cannot hold uniformly over \(n\). This raises a host of questions: (i) How fast, as measured by the sample size, does ERM learn the optimal feature map? (ii) What is the effect of this localization on the rate of decay of the excess risk of ERM non-asymptotically? (iii) What properties of the feature maps \((_{t})_{t}\) influence these rates?

Our answers to these questions in this very general setting are formally expressed in Theorem 4 below. To state it, we define the following parameter

\[L:=\,_{t} v_{t},^{ -1/2}(t)_{t}(X)^{2}-1^{2},\]

where the supremum is taken over vectors \((v_{t})_{t}\) such that \(_{t} v_{t}_{2}^{2}=1\). For \(n\) and \((0,1)\), we define the set function \(F_{n,}\), for any subset \(\), by

\[F_{n,}():=\{t\ |\ R(t,w_{*}(t))-R_{*} 2 (n)^{-1}[_{s}G_{n}^{2}(s)]\}\] (7)

This map acts as a contraction as shown in the next lemma, whose proof is deferred to Appendix E. For a function \(f\), we use \(f^{k}\) to denote \(f^{k}(x):=f(f^{k-1}(x))\) with \(f^{0}(x):=x\).

**Lemma 1**.: _Let \(n\), \((0,1)\), and assume that \(_{*}\). Then for all \(k\{0\}\),_

* \(F_{n,}^{k+1}() F_{n,}^{k}()\)_._
* _If_ \(\,n_{0},B\) _such that_ \([_{t}G_{n}^{2}(t)] B\) _for all_ \(n n_{0}\)_, then_ \(_{n 1}F_{n,}^{k}()=_{*}\)_._

With these definitions, we now state the second main result of the paper. A proof is in Appendix F.

**Theorem 4**.: _Assume that \(_{*}\), \([Y^{2}]<\), \((t,j)[d]\), \([_{t,j}^{2}(X)]<\), and \([ g(t,(X,Y))_{^{-1}(t)}^{2}]<\). Let \((0,1)\) and \(k\). If, for some \(t_{*}_{*}\), \(n\) satisfies_

\[n 64\,[_{t}_{n}(t)]+(128L+11)(6/ )+6^{-2}[_{t _{*}}_{n}(t,t_{*})],\]

_then, with probability at least \(1-\),_

\[_{n} F_{n,/2k}^{k}()=:_{n,,k},\]

_and_

\[(_{n},_{n}) 24(n)^{-1}[ _{s_{n,,k}}G_{n}^{2}(s)],\]

_where the processes \(_{n}\), \(_{n}\), and \(G_{n}\) are as in (3) and (4)._

We make a few remarks before interpreting the content of the theorem. First, we note that when the index set \(\) is a singleton, the last term in the sample size restriction vanishes, while the first matches the sample size restriction from Theorem 2 after an application of Lemma 3 below; further taking \(k=1\) in Theorem 4 recovers the upper bound on the excess risk of Theorem 2 up to a constant factor. Theorem 4 may therefore be viewed as a broad generalization of Theorem 2. Second, under Assumption 1 below, and by the second item of Lemma 1, the upper bound on the excess risk in Theorem 4 eventually matches the main term in the asymptotic bound of Theorem 3 as can be seen from (5), in the same way that Theorem 2 achieves this when compared with Theorem 1. Finally, the statement of Theorem 4 is very general, and in fact, too general for us to be able to interpret it precisely. As such, we will discuss it in the context of the following assumption.

**Assumption 1**.: There exists constants \(C_{}\), \(C_{}\), and \(C_{G}\) independent of the sample size, but possibly dependent on the remaining parameters of the problem, such that for all \(n\),

\[[_{t}_{n}(t)] C_{},_{t_{*}}_{n}(t,t_{*})  C_{},[_{t}G_{n}^{2}(t)]  C_{G},\]

where \(_{n}\), \(_{n}\), and \(G_{n}\) are as in (3) and (4).

These assumptions can be equivalently viewed as a restriction on the appropriately measured size of the index set \(\), and are slightly stronger than the assumptions of Theorem 3. They always hold for finite index sets, and we will derive in Section 4 explicit estimates of the constants in Assumption 1 in terms of moments of the feature maps and target as well as the cardinality of \(\).

Let us now interpret the content of Theorem 4, which comes with a free parameter \(k\), in the context of Assumption 1. We fix \(k\) here, and discuss its choice below. First, recalling the definition of \(F_{n,}\), this result says that above a certain sample size, both the suboptimality of the feature map picked by ERM and its excess risk decay at the fast rate \(n^{-1}\), answering the first question we raised at the beginning of the section. Second, this result provides an upper bound on the excess risk of ERM that depends on the index set \(\)_only_ through the size of shrinking subsets \(_{n,,k}\), which might be large for small \(n\), but which by Lemma 1 converge to the set of optimal feature maps \(_{*}\) as \(n\). This transparently shows the effect of the localization phenomenon on the rate of decay of the excess risk of ERM, answering the second question we raised. Finally, looking at the definition of \(_{n,,k}\), this result identifies the size of the sublevel sets of the suboptimality function \(R(t,w_{*}(t))-R_{*}\) defined over feature maps as a relevant property of the collection of feature maps \((_{t})_{t}\) that influences the rate of convergence of the excess risk of ERM in this setting, answering the final question we raised.

Finally, let us turn to the choice of \(k\). Practically, we select the one that minimizes the bound on the excess risk. Looking at the first item of Lemma 1, this optimal \(k\) balances the following trade-off: on the one hand, for small \(k\), applications of \(F_{n,/2k}\) constrain the input set more severely, but only a few iterations are performed; on the other hand, larger values of \(k\) allow more iterations, but at the cost of more weakly constraining the input set per application.

Stepping back, there are two main takeaways from Theorem 4. Firstly, and on a conceptual level, it shows that feature learning is easy when the suboptimality function \(R(t,w_{*}(t))-R_{*}\), defined over the set of features maps, has small sublevel sets. Secondly, and on a technical level, it provides a template which can be used to derive more explicit excess risk bounds on ERM given estimates on the expected suprema of the relevant empirical processes. Deriving such accurate estimates for infinite \(\) is a highly non-trivial task, and cannot be done at the level of generality we have been operating at. The case of finite \(\) however is tractable in a general setting as we discuss in the next section.

## 4 Case study: Finite index sets

In this section, we focus on the case where the index set \(\) is finite, and aim, among other things, at establishing explicit estimates on the various expected suprema appearing in Theorem 4 in terms of moments of the feature maps and of the target. This problem becomes tractable in the case of finite \(\) because, roughly speaking, a worst-case analysis still yields non-trivial upper bounds. This is decidedly not the case when \(\) is infinite, in which case these expected suprema can be infinite.

We start with a slight strengthening of Theorem 3, whose assumptions reduce to simple moments conditions when \(\) is finite. The straightforward proof can be found in Appendix H.

**Corollary 1**.: _Assume that \(\) is finite, for all \((t,j)[d]\), \(\![_{t,j}^{2}(X)]<\), \(\![Y^{2}]<\), and for all \(t\), \(\![\|g(t,(X,Y))\|_{^{-1}(t)}^{2}]<\). Then_

\[_{n}\!(_{n}_{*} )=0.\]

_Furthermore, for any \((0,1)\),_

\[ Q_{Z^{-}}(1-)_{n}n Q_{ (_{n},_{n})}(1-)_{n}n Q _{(_{n},_{n})}(1-) Q_{Z^{+} }(1-),\]

_where \(Z^{-}:=_{s_{*}}\|Z_{s}\|_{2}^{2}\), \(Z^{+}:=_{s_{*}}\|Z_{s}\|_{2}^{2}\), and the random vectors \((Z_{t})_{t}\) are jointly Gaussian with mean zero and covariance \(\![Z_{t}Z_{s}^{T}]=^{-1/2}(t)G(t,s)^{-1 /2}(s)\) for all \(t,s\). In particular, if \(_{*}=\{t_{*}\}\), then_

\[n(_{n},_{n})\|Z \|_{2}^{2},\]

_where \(Z(0,^{-1/2}(t_{*})G(t_{*},t_{*})^{-1/2}(t_{*}))\)._

The conclusions of Corollary 1 differ from those of Theorem 3 in two aspects. First, the feature map picked by ERM is guaranteed to be optimal rather than near-optimal with probability converging to one. Second, the upper bound on the asymptotic quantiles is improved by a factor of two, yielding the exact distribution of the rescaled excess risk when \(_{*}\) is a singleton.

Making Theorem 4 more explicit is a more laborious task. We recall here two known results that allow us to accomplish this. We start with the following bounds on the expectation of the supremum of a finitely-indexed empirical process, which we will later use to bound the suprema of the processes \((G_{n}(s))_{s}\) and \((_{n}(t,t_{*}))_{t_{*}}\) appearing in Theorem 4. A proof can be found in Appendix G.

**Lemma 2**.: _Let \(n,d\), and let \(Z\) be a random element taking value in a set \(\), and let \((Z_{i})_{i=1}^{n}\) be i.i.d. samples with the same distribution as \(Z\). Let \(\) be a finite collection of \(^{d}\)-valued measurable functions. Define_

\[^{2}():=_{f}\|f(Z)- [f(Z)]\|_{2}^{2}, r_{n}():= _{(i,f)[n]}\|f(Z_{i})-[f(Z)]\|_{2}^{2} ^{1/2},\]

_and let \(E_{n}(f):=(n^{-1}_{i=1}^{n}f(Z_{i})-[f(Z)])\). Then, we have_

\[()+()} {}_{f}\|E_{n}(f)\|_{2}^{2} ^{1/2} c(||)()+c^{2}(||)()}{},\]

_where \(c(m):=5\)._

Lemma 2 allows us to compute the expected supremum of a finitely-indexed empirical process, up to log factors in the size of the index set. It is known that these factors cannot be removed from the upper bound nor added to the lower bound without more assumptions, we refer the reader to a related discussion in . Finally, while the term \(r_{n}()\) might grow with \(n\), by bounding the maximum with the sum, it grows at most as \(\). In many applications however, the random vectors \(f(Z)\) are bounded almost surely, so that \(r_{n}()\) is of order one, which justifies our presentation choice.

The second result we recall is the expectation version of a one sided Matrix Bernstein inequality due to Tropp . We use it below to bound the supremum of the process \((_{n}(t))_{t}\) appearing in Theorem 4. We do not known of a matching non-asymptotic lower bound, but an asymptotic one is known [1, Proposition 17]. Upper and lower bounds similar to those of Lemma 2 hold if one considers the expected operator norm instead of only the maximum eigenvalue [16, Section 7].

**Lemma 3** (, Theorem 6.6.1.).: _Let \(n,d\) and for each \(i[n]\), let \(Z_{i}^{d d}\) be i.i.d. positive semi-definite matrices with the same distribution as \(Z\). Define_

\[V:=[Z]-Z^{2}, W_{n}:= [Z]-_{i=1}^{n}Z_{i}.\]

_Then, we have_

\[[_{}(W_{n})]} (V)(ed)}+}([Z])(ed)}{3}.\]

Equipped with these estimates, we may now control the expected suprema of the empirical processes appearing in Theorem 4. To apply Lemma 2, define the following classes, for \(\) and \(t_{*}_{*}\)

\[() :=(x,y)^{-1/2}(s)g(s,(x,y))\,\,s },\] \[(t_{*}) :=(x,y)(t),_{t}(x) ,y)-( w_{*}(t_{*}),_{t_{*}}(x),y)}{R(t,w_{*}(t))- R_{*}}\,\,t_{*}}.\]

Applying Lemma 2 on \(()\) bounds the expected supremum of the process \((G_{n}(s))_{s}\) while applying it on \((t_{*})\) bounds that of \((_{n}(t,t_{*}))_{t_{*}}\). To control the supremum of \((_{n}(t))_{t}\), the key idea is to notice that it can be expressed as the maximum eigenvalue of a block diagonal matrix whose blocks are \((I-^{-1/2}(t)_{n}(t)^{-1/2}(t))\). Looking at Lemma 3, the relevant parameter is therefore a block diagonal matrix \(V\) with the following blocks

\[V(t):=^{-1/2}(t)_{t}(X)_{t}(X)^{T} ^{-1/2}(t)-I^{2}.\]

As the bound in Lemma 3 depends only on the maximum eigenvalue of \(V\), the ordering of the blocks does not matter. Putting together these estimates, we arrive at a fully explicit version of Theorem 4.

**Corollary 2**.: _Assume that \(\) is finite and that for all \((t,j)[d]\), \(\![_{4,j}^{4}(X)]<,\![Y ^{4}]<\). Let \((0,1)\), \(k[1+|_{*}|]\), and \(c(),^{2}(),r_{n}()\) as in Lemma 2. If, for some \(t_{*}_{*}\),_

\[n(512_{}(V)+6)(ed||)+(128L+ 11)(6/)\\ +24^{-1} c(||)^{2}(( t_{*}))+10^{-1/2} c^{2}(||)r_{n}((t_{*})),\] (8)

_then, with probability at least \(1-\)_

\[_{n}_{n,/2k}^{k}()=:_{ n,,k},\]

_and_

\[(_{n},_{n}) 24(n)^{-1} A( _{n,,k}),\]

_where, for \(\),_

\[A() c^{2}()(( ))+c()(())}{ }^{2},\]

_and \(_{n,}()\) is the same as \(F_{n,}()\) defined in (7) but with \(A()\) replacing \(\![_{s}G_{n}^{2}(s)]\)._

We make a few remarks about Corollary 2; a proof sketch is in Appendix I. The set function \(A()\) controlling the contraction rate of the map \(_{n,}\) as well as the excess risk, has a pleasantly simple form. To first order, and ignoring constants, it is given by

\[(1+||)_{s}\![\|g(s,(X,Y))\|_{^{-1}(s)}^{2}].\]

As such, as \(n\) and by Lemma 1, the upper bound on the excess risk in Corollary 2 matches the main term in the asymptotic rate derived in Theorem 3, as can be seen from (6). As the sets \(_{n,,k}\) are shrinking with \(n\), the above expression clearly shows the decaying effect of the global complexity of \(\) on the excess risk. Finally, we note that the restriction on \(k\) in Corollary 2 is there only because after at most that many iterations, a fixed point is reached, and further iterations worsen the bound. We conclude this section with an example of an application of our results.

**Example 1** (Sparse linear regression).: Consider the sparse linear regression problem, and in particular the best subset selection (BSS) procedure . This procedure corresponds to ERM over the restricted linear class \(\{x w,(x)\|w\|_{0} s\}\) in the linear regression setup of Section 2, where \(\|w\|_{0}\) is the number of non-zero entries of \(w\) and \(s[d]\) is a user-chosen sparsity level.

The problem of computing the BSS procedure has attracted a lot of attention recently. While NP-hard and therefore difficult in the worst case , Bertsimas et al.  showed that it can be tractable on practical instances of moderate size. Since then, a rich literature has emerged that devises increasingly efficient methods [e.g. 1, 1, 2, 13, 14]. By comparison, the statistical performance of the BSS procedure is not yet completely understood as we discuss below.

To see how the sparse linear regression problem fits in our feature learning setting, notice that \(\{x w,(x)\|w\|_{0} s\}=\{x v, _{t}(x)(t,v)^{s}\}\) where \(\) is the set of all subsets of \([d]\) of size \(s\), and \(_{t}(x):=(_{j_{1}}(x),_{j_{2}}(x),,_{j_{s}}(x))^{s}\) where \((j_{1},j_{2},,j_{s})\) are the elements of \(t\) in increasing order. As such, Corollaries 1 and 2 are immediately applicable and provide general statements on the performance of an arbitrary BSS procedure. To simplify the discussion, we assume for the rest of the example that there is a unique risk minimizer \(w_{*}\) satisfying \(\|w_{*}\|_{0}=s\).

On the recovery side, the first item of Corollary 1 guarantees that we asymptotically exactly recover the support of \(w_{*}\). Non-asymptotically, the first item of Corollary 2 shows that if \(n\) further satisfies

\[n>_{k[]}4k()^{-1}  A(_{n,/2k}^{k-1}())}:=_{t_{*}}\{R(t,w_{*}(t))-R_{*}\},\]

then with probability at least \(1-\), the BSS procedure recovers the support of \(w_{*}\). Equivalently, these two statements say that for large enough \(n\), the BSS procedure coincides with the oracle procedure which knows the support of \(w_{*}\) a priori and outputs an ERM from the optimal linear class.

In practice however, the interesting regime is when \(n\) is only moderately large. Corollary 2 provides our guarantee in this case, and as such, we turn our attention to the sample size restriction (8). Typically, we expect the main restriction to come from the first term, which in this case is given by \(_{}(V) s(d/s)\), up to constants and lower order terms. This is because if an intercept isincluded, i.e. \(_{1}(X)=1\), then \(_{}(V) s-1\), so the first term scales as \(s^{2}(d/s)\) at least, while the remaining terms typically grow more slowly with \(s\). As a concrete example, when \((X)\) is a Gaussian vector, \(_{}(V)=s+1\), so in this case the estimate \(s^{2}(d/s)\) is tight. Under this sample size restriction, and if \(:=Y- w_{*},(X)\) satisfies \(\![^{2} X]^{2}\), Corollary 2 upper bounds the excess risk by \((^{2}s/n) a_{n}\) for a sequence of decreasing distribution-dependent constants \(a_{n}\) converging to one as \(n\), ignoring the dependence on \(\) and absolute constants.

The closest existing result in the literature we are aware of is due to Shen et al. , who arrived at comparable conclusions but in a substantially different setting. In particular, their result was obtained in the setting \(n,d\), with an implicit assumption on the distribution of \((X)\)[20, Equation 2], and dealt with the in-sample prediction risk instead of the excess risk. Another closely related result is due to Raskutti et al.  who showed that the minimax expected excess risk in a well-specified fixed-design setting is, up to constants, \(^{2}s(d/s)/n\); see also [1, Chapter 8]. Our results show that for moderate \(n\), in the random-design setting, and when focusing on a single instance, the \((d/s)\) factor can be replaced with another factor that decays to one as \(n\).

Coming back to the sample size restriction discussion, we strongly suspect that the factor \(s(d/s)\) is suboptimal, but we are unsure what the correct dependence is, even under Gaussian \((X)\). Indeed, this factor comes from the logarithmic factor in Lemma 3, when applied to the block diagonal matrix with blocks \(I-^{-1/2}(t)_{n}(t)^{-1/2}(t)\). One can improve this factor by instead using versions of this inequality based on the intrinsic dimension [14, Chapter 7]. However, this is also unlikely to be tight. Roughly speaking, this is because such logarithmic factors are tight only when the eigenvalues of the random matrix are near-independent. This is certainly not the case for the block diagonal matrix we are considering, since its blocks are sample covariance matrices of sub-vectors of the same random vector \((X)\). Capturing this dependence is beyond our reach and likely requires new tools; we refer the interested reader to the recent articles .

## 5 Conclusion

Broadly speaking, there are two main conclusions one can draw from this work. Firstly, in the large sample regime, and if the set of candidate feature maps is not too large under an appropriate measure of size, asking a model to additionally pick a feature map on top of learning a linear predictor has a negligible effect on the excess risk of ERM on regression problems with square loss. Secondly, for moderate sample sizes, the magnitude of this effect depends on the appropriately measured size of the subbevel sets of the suboptimality function \(t R(t,w_{*}(t))-R_{*}\). Plainly, learning feature maps is easy when only a small subset of them is good, as the bad ones can be quickly discarded.

The most tantalizing aspect of our results is their potential in explaining the experiments in . It was shown there that complex neural networks trained by ERM were able to achieve good performance despite being expressive enough to fit random labels. This is paradoxical if one assumes that the performance of ERM is driven by the complexity of the model class. Our results refute this assumption for a generic collection of feature-learning-based models. While there are many works offering explanations for this apparent paradox (see e.g.  for a survey), we are not aware of one that shows the vanishing influence of the size of the model class on the excess risk as Theorems 3 and 4 show. Formally connecting our statements to these experiments is beyond what we achieved here, yet, we believe that the new perspective we took might generate useful insights in this area.

We conclude by outlining a few limitations of our work. Firstly, we do not deal with the question of how to solve the ERM problem. Our focus is on understanding its statistical performance, and our setting is so general that such a question cannot be meaningfully tackled. Continuing on this last point, while the generality of our results is desirable in some aspects, it is detrimental in others. As an example, it would be desirable to specialize our results from Section 3 to specific infinite collections of feature maps used in practice. Let us also mention that it is a priori unclear whether ERM is an optimal procedure, in a minimax sense, for the model classes we consider; we suspect that recently developed tools might be relevant to address this question . Finally, while we focused on the case of regression with square loss, this was mostly done to simplify the presentation. Indeed, the only property of the loss used in the proofs is the exactness of its second order Taylor expansion. This is however not required if one can control the error term from above and below. It is known how to do this for many loss functions [e.g. 1, 2], and most importantly for logistic regression . We have purposefully selected generic notation to make translating such arguments easier.