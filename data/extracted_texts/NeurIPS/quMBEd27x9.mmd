# Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources

Feiyang Kang

Bradley Department of ECE

Virginia Tech

Blacksburg, VA 24061

fyk@vt.edu

&Hoang Anh Just

Bradley Department of ECE

Virginia Tech

Blacksburg, VA 24061

just@vt.edu

&Anit Kumar Sahu

Alexa AI

Amazon

Seattle, WA 98121

anit.sahu@gmail.com

&Ruoxi Jia

Bradley Department of ECE

Virginia Tech

Blacksburg, VA 24061

ruoxijia@vt.edu

Equal contribution. This work also appears at _Data-centric Machine Learning Research (DMLR)_ Workshop@ICML 2023. Code Repository: https://github.com/ruoxi-jia-group/projektor.

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

Traditionally, data selection has been studied in settings where all samples from prospective sources are fully revealed to a machine learning developer. However, in practical data exchange scenarios, data providers often reveal only a limited subset of samples before an acquisition decision is made. Recently, there have been efforts to fit scaling functions that predict model performance at any _size and data source composition_ using the limited available samples. However, these scaling functions are usually black-box, computationally expensive to fit, highly susceptible to overfitting, or/and difficult to optimize for data selection. This paper proposes a framework called projektor, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources. Our approach distinguishes itself from existing work by introducing a novel _two-stage_ performance inference process. In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes. In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws. We further derive an efficient gradient-based method to select data sources based on the projected model performance. Evaluation over a diverse range of applications (e.g., vision, text, fine-tuning, noisy data sources, etc.) demonstrates that projektor significantly improves existing performance scaling approaches in terms of both the accuracy of performance inference and the computation costs associated with constructing the performance predictor. Also, projektor outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions. We provide projektor as an open-source toolkit.

## 1 Introduction

The choice of training data is one of the most crucial components when it comes to extracting the best performance out of a model. Since data is typically acquired from various sources, such as different organizations or vendors, machine learning practitioners often encounter a central question: _how to select and combine samples from these data sources?_Although data selection has been extensively studied in the literature related to active learning , coreset selection , and data valuation [3; 4; 5; 6; 7; 8], most techniques are designed for a _fully-observable_ setting where all data sources are fully revealed to the model developer. The core ideas behind these techniques are to compare the relative importance of different data points or enumerate possible combinations of data points, all of which require complete knowledge of the entire collection of data points. While these methods have shown promising results, their practical applications in real-world scenarios are limited due to a significant gap: the acquisition decision-making processes require knowledge of the entire data sets, while data owners may only reveal limited samples before an acquisition decision is made (e.g., [9; 10] provide the examples in real-world data markets).

To bridge the gap, this paper explores strategic data selection in _partially observable settings_, where only limited samples of data sources (referred to as pilot datasets) are accessible. The goal is to determine an optimal allocation of the selection budget to each source, only based on the pilot datasets, such that the model trained on the mixture of collected data achieves the best performance at some given objectives.

**Technical challenges.** In the fully-observable setting, the evaluation and eventually ranking the candidate data selection decisions, including the number of samples to be selected and the ratio of samples from each source (_"mixing ratios"_), can be determined directly on the available datasets [11; 12]. However, the partially observable setting presents considerable challenges for evaluating a selection decision as one can no longer directly evaluate model performance on the entire data. With limited samples from each data source, the best possible evaluation is the resulting model performance for any combination of the pilot datasets. Then, to make an informed selection decision, it is necessary to understand the model's performance when trained on potentially larger datasets (target scales) at various mixing ratios. In other words, there is a need for prediction and projection of model performance onto larger data scales at different mixing ratios.

A recent study  proposes a performance scaling law that takes into account the data size and mixing ratio to predict model performance. Providing a preliminary exploration of this problem, though, this approach faces two major limitations: (1) The numerical instability of its high-order form for scaling functions causes significant difficulty in fitting its parameters, rendering the fitted function susceptible to overfitting and often fails to extrapolate model performance on unseen data mixtures. (2) It hypothesizes on the _separability_ of model performance scaling with data composition and with data size, which is generally untrue as evidenced by latest research  and leads to unsatisfactory performance prediction results in empirical observations. Besides, this method requires parameters that grow quadratically with the number of data sources, demanding many (mixing ratio, resulting performance) pairs to fit the function, resulting in substantial computational overhead. Thus, there remains a considerable lack of effective and practical approaches to this problem.

**Contributions.** The paper investigates two fundamental building blocks for strategic data selection in the partially observable setting:

(Q1) _How to provide an accurate projection of model performance trained on any combination of data sources based on limited samples_? and (Q2) _How to determine the optimal selection strategy_?

Figure 1: (a) Overview of projektor, which take as inputs the public pilot data from each source, a selection budget, a target model, a validation set representing the test distribution, and return the optimal combination of data sources as well as the prediction of the resulting model performance. (b) Optimal data source composition and performance projection in a practical autonomous driving data acquisition scenario . projektor achieves accurate model performance projection from 1K pilot samples and effective selection. Please see Evaluation Metrics (Section 5) for details on mAP.

Towards that end, the paper makes the following contributions.

\(\)_Parameter-Efficient Performance Prediction based on Optimal Transport (Addressing Q1)._ In contrast to existing model performance scaling methods that feature a _one-shot_ fitting of a non-informative parametric model (_"surrogate"_), our approach is a novel _two-stage_ performance inference process, where the first stage addresses the dependence of the model performance on the mixing ratio by fitting a parameter-efficient model between model performance and Optimal Transport  distance between the mixtures of training data and the validation data (Section 4.2). Then, for stage two, we propose a parameter-free mapping that directly projects model performance onto larger data scales, achieving remarkable accuracy as it fully preserves this the dependency of model performance scaling with data sizes and data distributions (Section 4.3).

\(\)_Determining optimal data selection strategies (Addressing Q2)._ We consider the typical data selection goal: maximizing the resulting model performance with fixed data acquisition budgets (data quantity). With model performance predicted by the proposed tools, these problems translate into convex losses that are optimized effectively via gradient-based methods (Section 4.4). We also provide in Appendix.B how it similarly applies to alternative objectives such as minimizing data acquisition costs for the resulting model performance to reach a given level.

\(\)_Experiments._ We experiment on a variety of applications (_vision_, _natural language processing (NLP)_ etc., with simple to complex models) with a rich diversity of tasks and real-world scenarios (e.g., autonomous driving, fine-tuning, noisy data sources, unlabeled data). The proposed approach is highly effective in performance prediction, demonstrating superior prediction precision to many baselines while being much more efficient to be constructed (Section 5). We test the performance of data selection by optimizing the performance predictor and show that it improves over existing methods by \(3\%\) on ImageNet-100.

## 2 Related Work

The recent line of research on **Data valuation** aims to assess the relative importance of each data source (_"value"_) to machine learning applications [3; 4; 5; 6; 7; 8]. While originally designed for data pricing, these values are frequently used to inform data selection [4; 17]: in more detail, one can rank the data sources based on their values and select the data points with the highest values. While value-based selection shows some promising results, data values are not directly related to model performance and hence cannot inform the prediction of model performance resulting from the selected data. Besides, values for different data sources typically cannot be combined to measure the value of their compositions [18; 8]. Notably, distributional distances including Optimal Transport have seen a major presence in data valuation as an implicit proxy for model performance [8; 19], but no connection has been made to directly relate data distance to model performance. Our work bridges this gap and directly addresses this long-standing problem. On another line, **Coreset selection** attempts to find a representative subset of data points to speed up learning . Coreset selection methods have been studied for different learning algorithms [20; 21]. For example, a popular coreset selection approach for neural networks is to cast data selection as a bilevel optimization problem that selects the optimal subset to maximize the performance evaluated on a validation dataset . However, coreset selection techniques rely on access to all the data samples to be chosen from, which limits their use in the partially observable setting. Besides, **Predicting** the resulting **model performance** associated with a dataset without performing actual training on it has attracted a lot of attention in different use cases, such as interpreting the model learning process [23; 24]-which leverages surrogate functions to model the black-box relationships between model performance and training data, or predicting performance under the distributional shift . Our work resembles the idea of predicting model performance from data but differs in the technique of leveraging the data distance in the performance predictor. **Scaling laws**, predicting how the model performance changes with the scale of training data, model parameters, and computation budget , have seen increasingly successful in a variety of tasks pertaining to vision and text processing . The performance of machine-learning models generally adheres to a power law relationship with the scale of these variables, which allows for predicting the model performance on larger scales with high precision  and provides a viable approach to predicting the potential usefulness of target data from only a small proportion of the set.  shows that data from different distributions generally scale at different rates. Our work provides a novel approach that materializes this dependency of scaling relationships with data distributions and achieves remarkable empirical results.

## 3 Problem Formulation

**Data provider.** Suppose that there are \(m\) prospective data providers. Datasets (_data sources_) held by these providers are denoted by \(D^{}_{1},,D^{}_{m}\), respectively. We focus on the case that only partial data (_samples_) from these sources are made available to the public, replicating practical data exchange scenarios . We refer to the public subset of each data source as _a pilot dataset_ and denote it by \(D^{}_{i}\), where \(D^{}_{i} D^{}_{i}\) and \(|D^{}_{i}|=n_{i}_{i}=|D^{}_{i}|\) for all \(i\). Each provider \(i\), upon accepting the _purchasing order_ for acquiring \(n_{i}\) samples (\(n_{i}_{i}\)), will randomly sample a subset \(S_{i}\) of size \(n_{i}\) from \(D^{}_{i}\) and return the subset to the requester.2

**Data collector (or requester, machine learning practitioner).** Now, consider a data collector who would like to acquire samples from the providers to train a model. Notably, _the collector's acquisition decisions must be made based only on the pilot datasets_. We assume the collector has a validation set \(D^{}\), representing the desired target data distribution. For ease of exposition, we assume the collector has a target learning algorithm \(\)3 that is going to be applied to the collected data as well as a target performance metric \(\) which takes the input of a trained model and a validation set and returns a performance score. The model performance resulting from training on any dataset \(S\) can be thus expressed as \(((S),D^{})\).

Given a selection budget of \(N\) samples, a mixing ratio of data sources \(=\{p_{1},,p_{m}\}\) such that \(_{i},0 p_{i} 1\) and \(_{i=1}^{m}p_{i}=1\), and \(m\) datasets \(D_{1},,D_{m}\) to be mixed, we denote the selected dataset by \((N,)=S_{1} S_{m}\), where each \(S_{i}\) is a random subset of \(D^{}_{i}\) and \(|S_{i}|=p_{i}N\). Using these notations, we now describe the typical acquisition goals that can be accommodated by our approach:

* (Primary) _Fixed-budget selection for maximal performance:_ The collector seeks to maximize the resulting model performance by strategically choosing the mixing ratio \(\) of \(m\) data sources at a _pre-specified_ selection budget \(N_{s}_{i=1}^{m}}\). The objective can be formalized as \(_{}(((N_{s},)),D^{ })\).
* (Alternative) _Flexible-budget selection for reaching performance threshold with minimal costs:_ The collector seeks to attain a target model performance \(u^{}\) by choosing _both_ the mixing ratio \(\) as well as the selection budget \(N\). More formally, the objective can be expressed as \(_{N,}N\) s.t. \(_{}(((N,),D^{}) u^{}\).

The alternative objective can be treated as a direct extension of the primary, where one solves the _"performance maximization"_ problem for different data quantities \(N\) and performs a _line search_ for minimal data quantity \(N\) that meets the performance requirement. We defer to Appendix.B for its complete solution procedure due to the similarity.

**Design challenge and key idea.** The primary challenge is that the collector cannot access \(D^{}_{1},,D^{}_{m}\) for decision making and hence cannot directly evaluate the two optimization objectives for every \(N\). Yet, as the pilot datasets are public, the collector can evaluate and observe the model performance associated with various mixtures of the pilot datasets for \(N\{N:Np_{i}|D^{}_{i}|,i=1,,m\}\) project the evaluations onto larger data scales. Our high-level idea to tackle the challenge is to first predict the model performance associated with any mixture of prospective unrevealed data sources based on observations on pilot datasets and project the predictions onto different data scales using scaling laws, then determine the data selection strategy by optimizing the predicted performance at the target scales.

## 4 Methodology of projektor : prediction, projection, and selection

### Preliminaries on Optimal Transport

_Optimal Transport (OT)_ is a metric for measuring the discrepancy between probability distributions . Compared to other measures such as the Kullback-Leibler Divergence  or Maximum Mean Discrepancies , OT enjoys advantageous analytical properties (is a valid metric; compatible with sparse-support distributions; stable with respect to deformations of the distributions' supports [31; 32]). Given probability measures \(_{t},_{v}\) over the space \(\), the OT distance is defined as \((_{t},_{v}):=_{(_{t},_{v})}_{ ^{2}}(z,z^{})d(z,z^{})\), where \((_{t},_{v}):=( )_{}(z,z^{})dz=_{t},\ \ _{}(z,z^{})dz^{ }=_{v}}\) denotes a collection of couplings between two distributions \(_{t}\) and \(_{v}\), \(:^{+}\) is a symmetric positive-definite cost function (with \((z,z)=0\)), respectively. A popular choice of \(\) is given in  by considering \(z\) as the feature-label pair \((x,y)\). The computation of the OT distance usually relies on the Sinkhorn algorithm , which attains almost linear time complexity and memory overhead with the state-of-the-art implementations and applies to large scales with parallel computing [31; 32]. Given \(D_{t}=\{(x_{i},y_{i})\}_{i=1}^{N}\) of size \(N\), and \(D_{v}=\{(x^{}_{i},y^{}_{i})\}_{i=1}^{T}\) of size \(T\), one can construct discrete probability measures \(_{t}(x,y):=_{i=1}^{N}_{(x_{i},y_{i})}\) and \(_{v}(x,y):=_{i=1}^{T}_{(x^{}_{i},y^{}_{i})}\), where \(\) is the Dirac delta function. With slight abuse of notation, we use \((D_{t},D_{v})\) to denote the OT distance between their corresponding discrete measures \((_{t}(x,y),_{v}(x,y))\).

Extensive theoretical studies show that the OT distance between two distributions provides an upper bound on the difference of a model's performance when evaluated on the two distributions [36; 37; 8]. Largely built upon Kantorovich-Rubinstein Duality , existing theoretical results require assumptions on the Lipschitz constant of the model with respect to the input space. However, the constant is rarely known in practice, nor can it be bounded tightly for complex models such as deep neural networks. As a result, despite its widespread popularity as a performance proxy [8; 36; 39], one cannot directly apply the existing theoretical results to directly estimate the model performance based on the OT distance, posing an important gap.

### Aligning data distance with performance predictions

Inspired by the theoretical results that the upper bound on the difference between training loss and validation loss can be tightly bounded by an affine transformation of the OT distance [33; 8], our first proposed approach is to directly estimate this transformation by empirically fitting data distances to model performance and then the estimated transformation can be used for predicting the model performance for different data mixtures. Formally, we consider the following performance estimator:

\[}(((N,)),D^{} )=a_{1}((N,),D^{} )+a_{0},\] (1)

where scaling parameter \(a_{1}\) and centering parameter \(a_{0}\) define the affine transformation. These two parameters can be estimated through least-square fitting. In particular, consider collecting the "training data" by forming the set of tuples \(\{(N_{j},_{j},(((N_{j},_ {j})),D^{}))\}_{j=1}^{j}\), where \(N_{j}\) is randomly sampled from \(\{1,,_{i=1}^{m}|D_{i}^{}|\}\) and \(_{j}\) is sampled from a probability simplex. Then, these parameters can be estimated as

\[(},})=_{a_{1},a_{0}}_{j=1}^{}}(((N_{j},_{j})),D^{ })-(((N_{j},_{j})),D^{ })^{2}.\]

We refer to this method as _center-scaling_ (projektor/CS). \(a_{1}\) can be considered an _empirical estimate of the Lipschitz constant_, and this treatment has been informally adopted in various works under different names [40; 41; 42]. projektor/CS has only two parameters that need to be estimated, which brings an important benefit of efficiency: we only need to have a few training iterations to get the "training data" for the above least-square fitting.

While proposed projektor/CS is sufficient to provide reliable performance predictions in most circumstances, we found it possible to further improve the prediction accuracy by making the scaling parameter and centering parameter a function of the mixing ratio. The intuition is that for samples from different data sources (i.e., data lying in different manifolds of the input space), the Lipschitz constant of the model along the combined manifold may vary with the mixing ratio. Hence, we supplement projektor/CS with simple nonlinear terms to characterize the dependence on each data source, leading to the _pseudo-quadratic_ (projektor/PQ) method, which is given as

\[}(((N,)),D^{})=_{ i=1}^{m}(b_{2}^{i} p_{i}^{i}+b_{1}^{i} p_{i}+b_{0})( (N,),D^{})+_{i=1}^{m}(c_{2}^{i} p_{i} ^{2}+c_{1}^{i} p_{i}+c_{0}),\] (2)

where \((},},},},}, })\) are pseudo-quadratic parameters where the fitting process is similar to Eq. (1). projektor/PQ has \((m)\) parameters (\(m\) is the number of data sources). So its fitting process will require more re-training than projektor/CS. However, as we will show in Section 5, it still significantly improves the efficiency over the existing baselines . This pseudo-quadratic form is chosen for it contains the simplest nonlinear terms and we want to preserve the convexity for numerical stability. We trimmed off the cross terms in the quadratic function as they often do not contribute much, resulting in the number of parameters growing linearly rather than quadratically with the number of data sources as in , greatly easing the computation burden in parameter fitting.

### Parameter-free performance projection onto larger data scales

Once the parameters are learned, the performance predictors (1) and (2) can be used to predict the validation performance associated with a training set by calculating the OT distance between the training and the validation set and plugging it as input to the predictors. Then, we need to project these predictions onto the target data scales. Neural scaling laws showcase the predictability of empirical performance with respect to the size of the training dataset, where it typically follows a log-linear scaling relationship as

\[_{V}[(((N,));D^{})]-(N)+C\]

where \(\) and \(C\) are some constants . Recent work  shows that when data from different sources differ in "quality" (e.g., noise level), which is the most likely scenario in practice, the scaling parameters are often vastly different, rendering parameters \(\) and \(C\)_functions_ of data composition \(\) and model performance for different data mixtures \(\) scales with different rates.  assumes the same constant \(\) for all data mixtures, leading to unsatisfactory scaling results. The difficulty underlying this dependence is that these scaling parameters differ for every data mixture and there is no closed-form expression available for this functional relationship. With the performance prediction tools proposed above, it is possible to directly predict model performance of any data mixture at the scale the tool has been fitted. That is, for any given data scale \(N_{0}\), as long as one completes the one-off fitting process of the performance predictor, model performance for any data composition at data size \(N_{0}\) can be inferred directly using Eq. (1) or Eq. (2). Thus, by performing the fitting process at different small scales for once, for any desired data mixture, we can directly fit the neural scaling laws for this particular distribution and project it onto larger data scales, without needing to train any additional model or make any further approximations. We formalize it into the following theorem.

**Theorem 1** (Data Composition Dependent Performance Projection).: _Consider log-linear performance scaling relationship depending on both data size \(N\) and data composition \(\) given as \(_{V}[(((N,));D^{})]=-()(N)+C()\). Assume one has completed the fitting of the performance predictor on two different scales \(N_{0}<N_{1}\), which gives \(}(((N_{0},));D^{})\) and \(}(((N_{1},));D^{})\) for all data mixtures \(\). Then, the model performance \(}(((N,));D^{})\) for any data mixture \(\) at any data scale \(N\) can be predicted as_

\[}(((N,);D^{})=(}{N_{0}})^{-1}[}}(((N_{1},));D^{})-}}(((N_{0}, );D^{})]\] (3)

without requiring fitting any additional parameters. The proof and derivations are given in Appendix.B. We refer to this method as _parameter-free projection_ for model performance. As this procedure does not rely on any additional assumption or parameterized surrogate, it requires minimal computational overhead while achieving considerably higher prediction accuracy compared to existing approaches such as . Not exclusive to the performance predictor proposed in this work, this method can be plugged into other predictors seamlessly and provides benefits at large, marking a novel contribution to performance projection in data acquisition problems.

### Performance-guided data selection

The intention of creating the proposed tools is not limited to providing predictions for model performance, rather, we expect the predictions to support determining the optimal data acquisition strategy. We show that these problems are convex and differentiable (Appendix.B) and thus can be solved effectively via gradient-based methods. Specifically, for our primary objective _fixed-budget selection for maximal performance_, with the proposed performance predictors with projection, we solve for \(^{*}=_{}}(( (N_{s},)),D^{})\). We solve it iteratively with the following procedure. First, we initialize the algorithm with \(=^{}\) where \(^{}\) can be chosen arbitrarily provided that \(_{i}p_{i}=1\). Then, at each step, we perform the gradient update as

\[}}+d^{t}.}(((N_{s},)),D^{})}{ }|_{=^{t}}\]

where \(d^{t}\) is the step size at iteration \(t\) and we obtain \(^{*}=}\) at convergence as the desired solution. Optimal Transport naturally provides its gradient w.r.t. the probability mass of data points in its dual solutions , which directly gives the gradients w.r.t. data mixtures \(\). This easy availability of gradients renders the optimization highly efficient in computation, resulting in remarkably fast solutions. We use the _calibrated gradient_ of OT from  which ensures the updated mixture \(\) remains within the simplex \(_{i}p_{i}=1\) at each step. We provide technical details of the gradient computation in Appendix.B. The alternative objective can be treated as a direct extension of the primary and we defer to Appendix.B for its solution procedure. The pseudo-code for projektor is provided in Appendix A.

## 5 Evaluation

In this section, we cover two main applications for projektor. **1)**_Performance projection_, where for any mixing ratio of data sources and any data scale, we want to predict the performance of the model trained on a given composed dataset. We also demonstrate efficiency and efficacy of our method for different scenarios of data sources, such as mislabeled or unlabeled data. **2)**_Optimal data source acquisition strategy_, where for a given data budget, we find a mixing ratio of data sources that can maximize the performance of a model. We present a solution to select optimal data source composition for two learning paradigms scenarios: training from scratch and model fine-tuning.

We compare with six existing baseline methods, where the first four: (1) Linear , which assumes a linear relationship with the data compositions; (2) Pseudo-Quadratic assumes a simple non-linear relationship; (3) Quadratic  assumes a fully quadratic relationship; (4) Rational  models a relationship through the sum of a set of rational functions. For \(m\) data sources, each function has \(m\) parameters, and there are m such functions, totaling \(m^{2}\) parameters; (5) LOO  measures the importance of a data source by computing the performance difference after removing that source; (6) Shapley  is a game-theoretic method which computes the average marginal contribution of a data source to different subsets of other sources. Baselines (5) and (6) are suitable for informing the selection of data sources but are unable to predict model performance, so we only include them in data source selection experiments. Details on implemented baselines are described in Appendix D and further explained in . For all experiments, we set up the problem with three data sources, where each source consists of different classes, and we refer the reader for additional information on the experimental setup, algorithm, datasets, models, implementations, code repository, and ablation studies on the number of data sources to Appendix D. We also showcase the runtime vs performance prediction trade-off and comparison with baseline methods.

**Evaluation Metrics.** We use mean absolute error (**MAE**) to measure the performance of our method by calculating the absolute difference between the predicted accuracy and the actual accuracy. For

Figure 2: Workflow chart for the _two-stage_ performance inference process of projektor.

the object detection task, we adopt a commonly used metric, mean average precision (**mAP**), which measures the average precision of a model across multiple classes or categories, providing a single value that represents the overall accuracy. The average precision represents the area under the precision-recall curve for a single class.

**Hyperparameters.** For practical reasons, we set the data scale \(N_{1}\) in Eq. 5 to be the size of the smallest pilot dataset, i.e. \(N_{1}=_{i}|D_{i}^{}|\). Upon selecting \(N_{1}\), we empirically choose \(N_{0}\) to be \(N_{1}\). For further investigations on selecting \(N_{0}\), we provide sensitivity analysis on \(N_{0}\) in Appendix D.

### Performance Prediction

In this experiment, we fit the parameters on limited compositions and extrapolate the prediction to unseen compositions. Specifically, we choose one data source and limit its maximum composition to \(<55\%\) of contribution in the training set of the performance predictor, then we predict accuracy on the compositions consisting of \( 55\%\) of contribution. As we observe in Figure 4, Linear and Pseudo-Quadratic methods cannot fit well the training data, which indicates that these methods do not have a strong representation power. While Quadratic and Rational baselines can fit the training data, they suffer from overfitting and do not generalize to unseen compositions. On the other hand, as seen in Table 1, our method projektor/PQ achieves the best training and extrapolation performance. projektor/CS achieves second best extrapolation performance. Furthermore, we analyze the efficiency of our method compared to other baselines. As observed in Figure 4, projektor not only achieves the lowest MAE score but also converges with around \(15\) training data for projektor/CS and \(25\) for projektor/PQ, which demonstrates low computational requirement of our method. With shown strong predictive power of projektor, we now proceed to practical applications in performance projections onto larger data scales.

**B. Performance Projection to Larger Data Scales**

**Mislabeled Data Sources**. In this experiment, we project performance onto larger data scales and also assume a more practical setting where data sources might not be of high quality and contain noisy labels . It is then critical to factor such irregularity for the performance prediction into our method. Given three mislabeled data sources formed by sampling CIFAR-10, each of which releases a pilot dataset of size 1K, we project performance for various mixing ratios onto larger data sizes, i.e. 2K, 5K, 7K, and 10K. We then measure the MAE value across all data scales. We observe in Fig. 5 that projektor achieves the best projection performance compared to all baseline methods. projektor/PQ achieves the lowest MAE score below \(2\%\) and projektor/CS is slightly above \(2\%\). The improved performance of our method can be attributed to the incorporation of actual data distance computation. This inclusion allows for a more accurate representation of mislabeling information in performance projection, unlike baseline methods that neglect this crucial information.

The promising results demonstrate the potential of our method to project performance of any composition to any data scale, which is important in the case of mislabeled data sources in the partially-revealed setting.

**Unlabeled Data Sources.** As mentioned earlier, data sources often contain noisy labels, and the process of labeling data can be costly. On the other hand, it is not uncommon to encounter unlabeled data sources. Therefore, we would like to extend our method to accommodate the setting of data sources without labeled data, and we aim to project performance of unlabeled data compositions from pilot data source mixtures of 1K samples from CIFAR-10. The three data sources contain unlabeled data from different classes of CIFAR-10. In this case, we compute the optimal transport distance on the feature space only, and we assume access to the labels of the pilot datasets, which enables us to train a model and obtain performance values. Consequently, we project performance across various mixing ratios onto larger data sizes (2K, 5K, 7K, and 10K). To visualize the performance of our method, we plot the difference between the projected performance and the actual performance. The closer the value approaches zero, the more optimal the projection becomes. Surprisingly, as we illustrate in Fig. 8, projektor can consistently maintain good projection performance for larger data sizes. Even at \(10K\), the largest error is below \(10\%\). These outcomes show that projektor can also be extended to unlabeled data sources, which demonstrates the flexibility and practicality of our method.

### Optimal Data Source Selection

Training a model requires a tremendous amount of resources to tune hyperparameters to achieve the highest performance. We demonstrate that by choosing data strategically, we can also improve model performance. We consider a setting, where we are facing the problem of choosing only \(50K\) to train ResNet-50 on ImageNet100 and would like to maximize the model's performance.

Figure 5: Performance projections from 1K CIFAR-10 samples across various mixing ratios and larger data scales: 2K, 5K, 7K, 10K. Comparison between projektor and baselines.

Figure 6: Optimal data source composition selection for 50K ImageNet-100 from 10K samples and the actual model performance.

Figure 7: Performance projection of selected _optimal_ mixture ratios in (Fig. 6) onto 50K ImageNet-100 from 10K samples. Comparison with the actual model performance.

However, we are provided with only a pilot dataset of size \(10K\) from each data source. As we observe in Figure 6, our optimized mixing ratio based on (4.4) achieves the highest model performance compared to all baselines. Further, using the functions from the previous step, we project the performance of our selected mixing ratio, and we observe in Figure 7 that projektor not only most closely predicts the accuracy to the actual accuracy but also attains the highest actual accuracy out of all methods. The improved selection of mixture ratios in our method can be attributed to our proposed selection approach (4.4). Unlike baseline methods that assume the same optimal composition for all data scales, our method finds optimal compositions specific to each data scale. For more experiments on CIFAR-10, we refer the reader to Appendix D.

### Application to Fine-tuning

As powerful architectures has been introduced and computation power has improved, larger models and datasets has become increasingly prevalent in training visual and natural language tasks. However, retraining these large pre-trained foundation models can be cost-prohibitive, which leads to widespread adoption for fine-tuning these models. However, these large pre-trained foundation models are expensive to retrain but are popular for fine-tuning for more customized tasks. In our case, we adopt a pre-trained Faster R-CNN model trained on COCO dataset. Our task is to fine-tune the model on the autonomous driving dataset for object detection, BDD100K . We assume each data source to specialize in taking pictures at a specific time of day, i.e. daytime, night, or dawn/dusk pictures. Similarly to the previous task, we select optimal data source composition and project the mean average precision (**mAP**) onto larger data scales. In Figure 1(b), we observe fine-tuning accuracy projection onto eight larger data scales from 1000 samples and observe that our predictions do not deviate from the actual accuracy by more than \(0.4\), which indicates projektor's extended capability of performance projection for fine-tuning.

## 6 Discussion and Outlook

This paper presents a novel framework to conduct data selection from partially revealed sources, addressing the practical challenge in emerging data market scenarios. In particular, in contrast to existing work that tries to directly fit non-informative parametric surrogates on the limited available samples to predict model performance at different data sizes and compositions of data sources, which suffers from pronounced computational burdens and often unsatisfactory results, our key technical contribution is an OT-based performance scaling method. The _take-away_ from our empirical study is that despite being extensively adopted in the past, fitting non-informative parametric surrogates for predicting performance scaling is actually suboptimal-computationally inefficient, often impractical, and less accurate; utilizing data distance in the performance prediction provides immediate benefits and presents a better pathway to construct the predictors.

Contributing a new perspective with performance and efficiency improvements, this work still has some **Limitations** and opens up many new investigation venues, such as lifting the requirement on validation data, accounting for malicious data owners, and extending to data sources that are misaligned in feature space. Additional discussions and **Broader Impacts** are provided in Appendix.E.

Figure 8: A direct visualization for the landscape of prediction errors in model performance for all data compositions as the scale of data to be predicted grows. We consider the setting of 3 data sources with unlabeled samples from CIFAR-10. We construct the proposed performance predictors within 1k samples that are considered accessible to the practitioner. We then predict model performance for all data compositions and on larger data scales (2-10k samples). X-Y axes represent the proportion of data from data sources #1 and #2 (consequentially, the proportion of data from data source #3 will be 100%-X-Y) and the prediction errors are visualized in Z-axis (horizontal plane represents zero error, red spikes blue dips represent deviations from over-/under-prediction). As the data scale gets larger, the performance predicted from the initial 1k samples becomes slightly less accurate, but prediction error remains mostly within 5% even at the scale of 10k data (10 times larger than the pilot data), retaining its effectiveness and functionality.