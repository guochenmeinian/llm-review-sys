# Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion

Yash Bhalgat  Iro Laina  Joao F. Henriques  Andrew Zisserman  Andrea Vedaldi

Visual Geometry Group

University of Oxford

{yashsb,iro,joao,az,vedaldi}@robots.ox.ac.uk

###### Abstract

Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a _slow-fast_ clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method.

## 1 Introduction

While the content of images is three-dimensional, image understanding has largely developed by treating images as two-dimensional patterns. This was primarily due to the lack of effective machine learning tools that could model content in 3D. However, recent advancements in neural field methods  have provided an effective approach for applying deep learning to 3D signals. These breakthroughs enable us to revisit image understanding tasks in 3D, accounting for factors such as multi-view consistency and occlusions.

In this paper, we study the problem of _object instance segmentation_ in 3D. Our goal is to extend 2D instance segmentation to the third dimension, enabling simultaneous 3D reconstruction and 3D instance segmentation. Our approach is to extract information from multiple views of a scene independently with a pre-trained 2D instance segmentation model and fuse it into a single 3D neural field. Our main motivation is that, while acquiring densely labelled 3D datasets is challenging, annotations and pre-trained predictors for 2D data are widely available. Recent approaches have also capitalized on this idea, demonstrating their potential for 2D-to-3D semantic segmentation  and distilling general-purpose 2D features in 3D space . When distilling semantic labels or features, the information to be fused is inherently consistent across multiple views: semantic labels are viewpoint invariant, and 2D features across views are typically learned with the same loss function. Additionally, the number of labels or feature dimensions is predetermined. Thus, 3D fusion amounts to multi-view aggregation.

When it comes to instance segmentation, however, the number of objects in a 3D scene is not fixed or known, and can indeed be quite large compared to the number of semantic classes. More importantly, when objects are detected independently in different views, they are assigned differentand inconsistent identifiers, which cannot be aggregated directly. The challenge is thus how to fuse information that is not presented in a viewpoint-consistent manner.

Recently, Panoptic Lifting  proposed to resolve the lack of multi-view consistency by explicitly fitting a permutation that aligns labels extracted from multiple views. Although this yields good results, there are two drawbacks to this approach. Firstly, determining the permutation matrix involves solving a linear assignment problem using Hungarian Matching for every gradient computation. The cost of this increases cubically with the number of identifiers, which may limit scalability when dealing with a large number of object instances. Secondly, the canonical label space, where the permutation maps each 2D label, may need to be extensive to accommodate a large number of objects.

In this study, we propose a more efficient formulation, which also leads to more accurate results. To understand our approach, consider first a 2D image segmenter: it takes an image \(I\) as input and produces a mapping \(y\) that assigns each pixel \(u^{2}\) to an object instance label \(y(u)\{1,,L\}\). It is natural to extend this mapping to 3D by introducing a function \(Y\) that associates each 3D point \(x^{3}\) with the label \(Y(x)\) of the corresponding object. To account for the fact that labels are arbitrary and thus inconsistent between views, Panoptic Lifting  seeks an image-dependent permutation matrix \(P\) such that \(Y(x)=P y(u)\), where \(u\) is the projection of \(x\) onto the image.

To address the aforementioned challenges with the linear-assignment-based approach, we identify the labels \(y(u)\) with coordinate vectors in the Euclidean space \(^{L}\). The functions \(y(u)\) can be reconstructed, up to a label permutation, from the distances \(d(y(u),y(u^{}))\!=\!\|y(u)-y(u^{})\|_{2}\) of such vectors, as they tell whether labels of two pixels \((u,u^{})\) are the same or different, without considering the specific labelling. Notably, similar to compressed sensing, we can seek lower-dimensional projections of the vectors \(y\) that preserve this information. With this in mind, we replace the 3D labelling function \(Y\) with a low-dimensional Euclidean embedding \((x)^{D}\). Then, we supervise the embeddings such that their distances \(d((x),(x^{}))\!\!d(y(u),y(u^{}))\) are sufficiently similar to that of corresponding 2D label embeddings.

This approach has two advantages. First, it only requires learning vectors of dimensionality \(D L\) which is independent of the number of objects \(L\). Second, learning this function does not require solving an assignment problem; rather, it only considers pairwise distances. Hence, the complexity of computing the learning objective is independent of the number of objects in the scene.

We translate this idea into a neural fusion field framework, which we call _Contrastive Lift_. We build on the recent progress in self-supervised learning, and combine two key ideas: the usage of a

Figure 1: **Contrastive Lift** takes as input several views of a scene (left), as well as the output of a panoptic 2D segmenter (middle). It then reconstructs the scene in 3D while fusing the 2D segments, which are noisy and generally labelled inconsistently between views, when no object association (tracking) is assumed. Our method represents object instances in 3D space by a low-dimensional continuous embedding which can be trained efficiently using a contrastive formulation that is agnostic to the inconsistent labelling across views. The result (right) is a consistent 3D segmentation of the objects, which, once imaged, results in more accurate and consistent 2D segmentations.

contrastive loss, and the usage of a slow-fast learning scheme for minimizing the latter in a stable manner. We believe to be the first to introduce these two ideas in the context of neural fields.

We compare our method to recent techniques including Panoptic Lifting  on standard 3D instance segmentation benchmarks, _viz._ ScanNet , Replica , and Hypersim . To better demonstrate the scalability of our method to a very large number of object instances, we introduce a semi-realistic Messy Rooms dataset featuring scenes with up to 500 objects.

## 2 Related Work

**Neural Radiance Fields (NeRFs).** NeRF  and its numerous variants [2; 5; 34; 36; 42] have achieved breakthrough results in generating photorealistic 3D reconstructions from 2D images of a scene. These systems typically represent the scene as a continuous volumetric function that can be evaluated at any 3D point, enabling high-quality rendering of novel views from any viewpoint.

**Objects and Semantics in NeRF.** While NeRF by default offers low-level modelling of radiance and geometry, recent methods have expanded the set of tasks that can be addressed in this context to include _semantic_ 3D modelling and scene decomposition. Some works use neural scene representations to decompose scenes into foreground and background without supervision or from weak signals [17; 41; 47; 56; 62; 63], such as text or object motion. Others exploit readily available annotations for 2D datasets to further extend the capabilities of NeRF models. For example, Semantic NeRF  proposes to incorporate a separate branch predicting semantic labels, while NeSF  predicts a semantic field by feeding a density field as input to a 3D semantic segmentation model.

Closer to our work are methods that employ NeRFs to address the problem of 3D panoptic segmentation [19; 26; 31; 48; 61]. Panoptic NeRF  and Instance-NeRF  make use of 3D instance supervision. In Panoptic Neural Fields , each instance is represented with its own MLP but dynamic object tracking is required prior to training the neural field. In this work, we focus on the problem of lifting 2D instance segmentation to 3D without requiring any 3D masks or object tracks. A paper most related to our work is Panoptic Lifting , which also seeks to solve the same problem, using linear assignment to make multi-view annotations consistent. Here, we propose a more efficient and effective technique based on learning permutation-invariant embedding vectors instead.

**Fusion with NeRF.** The aforementioned works, such as Semantic NeRF  or Panoptic Lifting , are also representative of a recent research direction that seeks to _fuse_ the output of 2D analysis into 3D space. This is not a new idea; multi-view semantic fusion methods [25; 35; 37; 38; 51; 59] predate and extend beyond NeRF. The main idea is that multiple 2D semantic observations (_e.g._, noisy or partial) can be combined in 3D space and re-rendered to obtain clean and multi-view consistent labels. Instead of assuming a 3D model, others reconstruct a semantic map incrementally using SLAM [32; 43; 53]. Neural fields have greatly improved the potential of this idea. Instead of 2D labels, recent works, such as FFD , N3F , and LERF , apply the 3D fusion idea directly to supervised and unsupervised dense features; in this manner, unsupervised semantics can be transferred to 3D space, with benefits such as zero-shot 3D segmentation.

**Slow-fast contrastive learning.** Many self-supervised learning methods are based on the idea of learning representations that distinguish different samples, but are similar for different augmentations of the same sample. Some techniques build on InfoNCE [54; 57] and, like MoCo  and SimCLR , use a contrastive objective. Others such as SWaV  and DINO  are based on online pseudo-labelling. Many of these methods stabilise training by using mean-teachers , also called momentum encoders . The idea is to have two versions of the same network: a _fast_ "student" network supervised by pseudo-labels generated from a _slow_ "teacher" network, which is in turn updated as the moving average of the student model. Our formulation is inspired by this idea and extends it to learning neural fields.

**Clustering operators for segmentation.** Some works [14; 18; 30; 45] have explored using clustering of pixel-level embeddings to obtain instance segment assignments. Recent works [64; 65] learn a pixel-cluster assignment by reformulating cross-attention from a clustering perspective. Our proposed method, Contrastive Lift, is similar in spirit, although we learn the embeddings (and cluster centers) using volumetric rendering from 2D labels.

## 3 Proposed Method: Contrastive Lift

Here and in Fig. 2, we describe Contrastive Lift, our approach for fusing 2D instance segmentation in 3D space. An image is a mapping \(I:^{3}\), where \(\) is a pixel grid in \(^{2}\), and the values are RGB colours. We have a set of images \(\) captured in the same scene and, for each image \(I\), we have its camera pose \( SE(3)\) as well as object identity labels \(y:\{1,,L\}\) obtained from a 2D instance segmentation model for the image \(I\). The labels \(y\) assigned to the 3D objects in one image \(I\) and the labels \(y^{}\) in another image \(I^{}\) are in general not consistent. Furthermore, these 2D label maps can be noisy across views.

We use this data to fit a neural field. The latter is a neural network that maps 3D coordinates \(x^{3}\) to multiple quantities. The first two quantities are density, denoted by \(:^{3}\), and radiance (colour), denoted by \(c:^{3}^{2}^{3}\). Following the standard neural radiance field approach , the colour \(c(x,d)\) also depends on the viewing direction \(d^{2}\). The third quantity is a \(D\)-dimensional instance embedding (vector) denoted as \(:^{3}^{D}\). Each 3D coordinate is also mapped to a semantic embedding that represents a distribution over the semantic classes.

**Differentiable rendering.** The neural field associates attributes (density, colour, and embedding vectors) to each 3D point \(x^{3}\). These attributes are projected onto an image \(I\) taken from a viewpoint \(\) via differentiable ray casting. Given a pixel location \(u\) in the image, we take \(N\) successive 3D samples \(r_{i}^{3}\), \(i=0,,N-1\) along the ray from the camera center through the pixel (so that \((u,f)^{-1}(r_{i})\) where \(f\) is the focal length). The probability that a photon is not absorbed when travelling from sample \(r_{i}\) to sample \(r_{i+1}\) is \((-(r_{i})_{i})\) where \(_{i}=\|r_{i+1}-r_{i}\|_{2}\) is the distance between points. The _transmittance_\(_{i}=(-_{j=0}^{i-1}(r_{j})_{j})\) is the probability that the photon travels through sample \(r_{i}\). The projection of any neural field \(\) onto pixel \(u\) is thus given by the rendering equation:

\[(u|,,)=_{i=0}^{N-1}(r_{i})(_{i} -_{i+1})=_{i=0}^{N-1}(r_{i})_{i}(1-(-(r_{i}) _{i}))\] (1)

In particular, the colour of a pixel is reconstructed as \(I(u)(u|c(,d_{u}),,)\) where the viewing direction \(d_{u}=r_{0}/\|r_{0}\|_{2}\). The photometric loss is thus:

\[_{}(c,|I)=_{u}\|I(u) -(u|c(,d_{u}),,)\|^{2}.\] (2)

**Instance embeddings and slow-fast contrastive learning.** The photometric loss (2) learns the colour and density fields \((c,)\) from the available 2D views \(\). Now we turn to learning the instance embedding field \(:^{3}^{D}\). As noted in Section 1, the goal of the embeddings is to capture the (binary) distances between pixel labels sufficiently well. By that, we mean that the segments can be recovered, modulo a permutation of their labels, by simply _clustering_ the embeddings a posteriori.

Figure 2: Overview of the Contrastive Lift architecture. See Section 3 for details.

We cast learning the embeddings as optimising the following contrastive loss function:

\[_{}(,|y)=-_{u} }_{y(u)=y(u^{})}(( _{u},_{u^{}};))}{_{u^{}}(( _{u},_{u^{}};))},\ \ \ _{u}=(u|,,),\] (3)

where \(\) is the indicator function, and \((x,x^{};)=(-\|x-x^{}\|^{2})\) is a Gaussian RBF kernel used to compute the similarity between embeddings in Euclidean space. Therefore, pixels that belong to the same segment are considered positive pairs, and their embeddings are brought closer, while the embeddings of pixels from different segments are pushed apart. It is worth emphasizing that, since the object identity labels obtained from the underlying 2D segmenter are not consistent _across_ images, \(_{}\) is only applied to positive and negative pixel pairs sampled from the _same_ image.

While Eq. (3) is logically sound, we found it to result in gradients with high variance. To address this, we draw inspiration from momentum-teacher approaches  and define a _slowly-updated_ instance embedding field \(\), with parameters that are updated with an exponential moving average of the parameters of \(\), instead of gradient descent. With this, we reformulate Eq. (3) as:

\[_{}(,|y,)=-|} _{u_{1}}_{2}}_{y(u) =y(u^{})}((_{u},_{u^{}};))}{ _{u^{}_{2}}((_{u},_{u^{}}; ))},\] (4)

where \(_{u}=(u|,,)\), and \(_{u^{}}=(u^{}|,,)\). Here, we randomly partition the pixels \(\) into two non-overlapping sets \(_{1}\) and \(_{2}\), one for the "fast" embedding field \(\), and another for the "slow" field \(\). This avoids the additional cost of predicting and rendering each pixel's embedding using both models, and allows the computational cost to remain the same as for Eq. (3).

**Concentration loss.** In order to further encourage the separation of the embedding vectors \(\) and thus simplify the extraction of the objects via _a posteriori_ clustering, we introduce a loss function that further encourages the embeddings to form concentrated clusters for each object:

\[_{}(,|y,)=|} _{u_{1}}\|_{u}-_{2}} _{y(u)=y(u^{})}_{u^{}}}{_{u^{} _{2}}_{y(u)=y(u^{})}}\|^{2}.\] (5)

This loss computes a centroid (average) embedding as predicted by the "slow" field \(\) and penalizes the squared error between each embedding (as predicted by the "fast" field \(\)) and the corresponding centroid. While this loss reduces the variance of the clusters, it is not a sufficient training objective by itself as it does not encourage the separation of different clusters, as done by Eqs. (3) and (4).

**Semantic segmentation.** For semantic segmentation, we follow the same approach as Semantic NeRF , learning additional embedding dimensions (one per semantic class), rendering labels in the same manner as Eq. (1), and using the cross-entropy loss for fitting the semantic field. Additionally, we also leverage the segment consistency loss introduced in  which encourages the predicted semantic classes to be consistent within an image segment.

**Architectural details.** Our neural field architecture is based on TensoRF . For the density, we use a single-channel grid whose values represent the scalar density field directly. For the colour, a multi-channel grid predicts an intermediate feature which is concatenated with the viewing direction and passed to a shallow 3-layer MLP to predict the radiance field. The viewing directions are encoded using a frequency encoding . For the instance embedding field \(\) (and also the "slow" field \(\) which has the exact same architecture as the "fast" field \(\)), we use a shallow \(5\)-layer MLP that predicts an embedding given an input 3D coordinate. The same architecture is used for the semantic field. We use raw 3D coordinates directly _without_ a frequency encoding for the instance and semantic components. More details are provided in the supplementary material.

**Rendering instance segmentation maps.** After training is complete, we sample \(10^{5}\) pixels from \(100\) random viewpoints (not necessarily training views) and render the _fast_ instance field \(\) at these pixels using the corresponding viewpoint pose. The rendered \(10^{5}\!\!D\) embeddings are clustered using HDBSCAN  to obtain centroids, which are cached. Now, for any novel view, the field \(\) is rendered and for each pixel, the label of the centroid nearest to the rendered embedding is assigned.

## 4 Messy Rooms Dataset

In order to study the scalability of our method to scenes with a large number of objects, we generate a semi-realistic dataset using Kubric . To generate a scene, we first spawn \(N\) realistically textured objects, randomly sampled from the Google Scanned Objects dataset , without any overlap. The objects are dropped from their spawned locations and a physics simulation is run for a few seconds until the objects settle in a natural arrangement. The static scene is rendered from \(M\) inward-facing camera viewpoints randomly sampled in a dome-shaped shell around the scene. Background, floor, and lighting are based on \(360^{}\) HDRI textures from PolyHaven  projected onto a dome.

Specifically, we create scenes with \(N=25,50,100\), and \(500\) objects. The number of viewpoints, \(M\) is set to \((1200, 600}{{25}}})\), and the rendered image resolution is \(512 512\). To ensure that the focus is on the added objects, we use background textures _old_room_ and _large_corridor_ from PolyHaven that do not contain any objects. A total of \(8\) scenes are generated. The use of realistic textures for objects and background environments makes them representative of real-world scenarios.

Additionally, we would like to maintain a consistent number of objects per image as we increase the total number of objects so that the performance of the 2D segmenter is not a factor in the final performance. Firstly, we ensure that the floor area of the scene scales proportionally with the number of objects, preventing objects from becoming densely packed. Secondly, the cameras move further away from the scene as its extent increases. To ensure that the same number of objects is visible in each image, regardless of the scene size, we adjust the focal length of the cameras accordingly, _i.e._, \(f=35.0}{{25}}}\), creating an effect similar to magnification. This approach ensures a comparable object distribution in each image, while enabling us to study the scalability of our method.

We render the instance IDs from each camera viewpoint to create ground-truth instance maps. These ground-truth instance IDs remain consistent (tracked) across views, as they are rendered from the same 3D scene representation.1 Figure 3 shows illustrative examples from the dataset, which we name _Messy Rooms_. For evaluation (Section 5), semantic maps are required. As there is a large variety of different object types in Kubric, there is no off-the-shelf detector that can classify all of these, and since we are interested in the instance segmentation problem, rather than the semantic classes, we simply lump all object types in a single "foreground" class, which focuses the evaluation on the quality of instance segmentation. More details about the dataset are provided in Appendix A.

## 5 Experiments

**Benchmarks and baselines.** We train and evaluate our proposed method on challenging scenes from the ScanNet , Hypersim , and Replica  datasets. We compare our method with Panoptic Lifting (PanopLi) , which is the current state-of-the-art for lifting 2D panoptic predictions to 3D, along with other 3D panoptic segmentation approaches: Panoptic Neural Fields  and

Figure 3: Messy Rooms dataset visualization. Left: physically realistic static 3D scene with \(N\) objects from GSO . Middle: \(M\) camera viewpoints sampled in a dome-shaped shell. Right: ground-truth RGB and instance IDs, and instance segmentations obtained from Detic .

DM-NeRF . We follow PanopLi  for the data preprocessing steps and train-test splits for each scene from these datasets. We also evaluate our proposed method and PanopLi on our Messy Rooms dataset (Section 4) that features scenes with up to \(500\) objects. These experiments aim to demonstrate the scalability of our proposed method as compared to the linear-assignment approach.

We compare two variants of our Contrastive Lift method: (1) _Vanilla_: uses the simple contrastive loss (Eq. (3)), and (2) _Slow-Fast_: uses slow-fast contrastive (Eq. (4)) and concentration (Eq. (5)) losses.

**Metrics.** The metric used in our evaluations is the scene-level Panoptic Quality (PQ\({}^{}\)) metric introduced in . PQ\({}^{}\) is a scene-level extension of standard PQ  that takes into account the consistency of instance IDs across views/frames (_aka_ tracking). In PQ\({}^{}\), predicted/ground-truth segments with the same instance ID across all views are merged into _subsets_ and all pairs of predicted/ground-truth _subsets_ are compared, marking them as a match if the IoU is greater than \(0.5\).

**Implementation Details.** We train our neural field model for 400k iterations on all scenes. Optimization-related hyper-parameters can be found in Appendix B.2. The density grid is optimised using only the photometric loss (\(_{}\)). While rendering the instance/semantic fields and computing associated losses (Eqs. (3) to (5)), gradients are stopped from flowing to the density grid.

For experiments on ScanNet, Hypersim and Replica, we use Mask2Former (M2F)  as the 2D segmenter to obtain the image-level semantic labels and instance identities. Although any 2D segmenter can be used, using M2F allows direct comparisons with other state-of-the-art approaches . We follow the protocol used in  to map the COCO  vocabulary to 21 classes in ScanNet.

For experiments on Messy Rooms, we use Detic  instead since the object categories are not isomorphic to the COCO vocabulary M2F uses. We use the LVIS  vocabulary with Detic. To show the scalability of our method compared to a linear-assignment-based approach, we train the PanopLi  model on this dataset. For fair comparison, we first train the density, colour and semantic fields, which are identical in PanopLi and our approach. We then separately train the instance field using the respective linear-assignment and slow-fast contrastive losses, with all other components frozen, ensuring that performance is only influenced by the quality of the learned instance field.

### Results

In Table 1, we compare the performance of our proposed approach with existing methods on three datasets: ScanNet , HyperSim , and Replica . Since the semantic field and underlying TensoRF  architecture we use is similar to Semantic-NeRF  and PanopLi , we only report the PQ\({}^{}\) metric here and have added an additional table to Appendix D where we show that the mIoU and PSNR of our method match the performance of prior methods as expected. We observe that the proposed _Slow-Fast_ approach consistently outperforms the baselines on all three datasets, while also outperforming the state-of-the-art Panoptic Lifting  method by \(+3.9\), \(+1.4\) and \(+0.8\)

   Method & ScanNet  & HyperSim  & Replica  \\  DM-NeRF  & 41.7 & 51.6 & 44.1 \\ PNF  & 48.3 & 44.8 & 41.1 \\ PNF + GT BBoxes & 54.3 & 47.6 & 52.5 \\ PanopLi  & 58.9 & 60.1 & 57.9 \\  Vanilla (**Ours**) & 60.5 & 60.9 & 57.8 \\ Slow-Fast (**Ours**) & **62.3** & **62.3** & **59.1** \\   

Table 1: Results on ScanNet, Hypersim, and Replica datasets. The performance of all prior work has been sourced from . For each dataset, we report the PQ\({}^{}\) metric.

    &  &  \\   & 25 Objects & 50 Objects & 100 Objects & 500 Objects & 25 Objects & 50 Objects & 100 Objects & 500 Objects \\  PanopLi  & 73.2 & 69.9 & 64.3 & 51.0 & 65.5 & 71.0 & 61.8 & 49.0 \\ Vanilla (**Ours**) & 74.1 & 71.2 & 63.6 & 49.7 & 67.9 & 69.3 & 62.2 & 47.2 \\ Slow-Fast (**Ours**) & **78.9** & **75.8** & **69.1** & **55.0** & **76.5** & **75.5** & **68.7** & **52.5** \\   

Table 2: Results on the Messy Rooms dataset. PQ\({}^{}\) metric is reported on “old room” and “large corridor” environments with increasing number of objects in the scene (\(N=25,50,100,500\)).

PQscene points on these datasets respectively. We note that the _Vanilla_ version of our method also performs comparably with PanopLi and outperforms other methods on all datasets.

Table 2 shows comparisons between our method and PanopLi  on scenes from our Messy Rooms dataset with \(25,50,100\), and \(500\) objects. We see that the margin of improvement achieved by Contrastive Lift over PanopLi is even larger on these scenes, which shows that the proposed method scales favorably to scenes with a large number of objects. Fig. 4 shows qualitative results on two of these scenes. Even though the 2D segments obtained using Detic  are noisy (sometimes _over-segmented_) and generally labelled inconsistently between views, the resulting instance segmentations rendered by Contrastive Lift are clearer and consistent across views. We also note that PanopLi sometimes fails to distinguish between distinct objects as pointed out in Fig. 3(b).

### Ablations

**Different variants of Contrastive Lift.** Our proposed method uses \(_{}\) (Eq. (4)) and \(_{}\) (Eq. (5)) to optimise the instance embedding field. To study the effect of these losses, we design a comprehensive set of variants of the proposed method: **(1)** Proposed (\(_{}+_{}\)), **(2)** Proposed without Concentration loss (\(_{}\)), **(3)** Vanilla contrastive (\(_{}\)), **(4)** Vanilla contrastive with Concentration loss applied to "fast" field since there is no "slow" field (\(_{}+_{}()\)). Table 3 shows these ablations.

**Effect of embedding size on performance.** We investigate the impact of varying the instance embedding size on the performance of our proposed Contrastive Lift method. Specifically, we evaluate the effect of different embedding sizes using the PQscene metric on ScanNet, Hypersim and Replica datasets. As shown in Fig. 5, we find that an embedding size as small as \(3\) is already almost optimal. Based on this, we use an embedding size of \(24\) for experiments with these datasets (_c.f._ Table 1). For experiments with Messy Rooms dataset (_c.f._ Table 2), we keep the embedding size to \(3\)

Figure 4: Qualitative comparisons of our method with PanopLi  and Detic  (underlying 2D segmenter model) on scenes from our Messy Rooms dataset. **Colour coding:** regions where PanopLi performs poorly are highlighted with **red** boxes, while regions where both PanopLi and our method exhibit poor performance are marked with **blue** boxes. Additionally, **red** arrows indicate instances where PanopLi fails to distinguish between different objects. Please zoom in to observe finer details.

   Dataset & \(_{}+_{}\) & \(_{}\) & \(_{}\) & \(_{}+_{}()\) \\  ScanNet  & **62.0** & 61.3 & 60.5 & 55.2 \\ Messy Rooms & **69.0** & 66.5 & 63.2 & 51.7 \\   

Table 3: Ablations of different variants of the Contrastive Lift method. PQscene metric averaged over the scenes of ScanNet and Messy Rooms datasets is reported. Embedding size of \(3\) is used.

**Qualitative evaluation: Slow-fast vs vanilla contrastive learning.** Fig. 6 shows how the embeddings are distributed in Euclidean space when learned using our proposed slow-fast contrastive loss (Eqs. (4) and (5)) and the vanilla contrastive loss (Eq. (3)). Embeddings learned with the slow-fast method are clustered more compactly and are easy to distinguish using any post-processing algorithm, such as HDBSCAN  which is used in this example.

**Comparison to underlying 2D instance segmentation model with tracking.** Before lifting, the predictions of the underlying 2D instance segmentation model (e.g., Mask2Former  or Detic ) are not consistent (_aka_ tracked) across frames/views. To achieve consistency and to allow comparisons with our approach, we post-process the 2D segmenter's predictions using Hungarian Matching for cross-frame tracking as follows:

1. **w/ Hungarian matching (2D IoU)**: Given sets of predicted segments (\(P_{i}\) and \(P_{i+1}\)) from consecutive frames, compute IoU matrix by comparing all segment pairs in \(P_{i} P_{i+1}\). Apply Hungarian matching to the IoU matrix to associate instance segments across frames.
2. **w/ Hungarian matching based on IoU after depth-aware pose-warping**: Use ground-truth pose and depth for warping \((i+1)\)-th frame's segmentation to frame \(i\). Compute IoU matrix using warped segmentations and apply Hungarian matching.
3. **w/ Hungarian matching using ground-truth pointcloud**: Using only consecutive frames leads to errors in long-range tracking. To address this, starting from the first frame, unproject 2D segments into the 3D point cloud. Iteratively fuse segments in 3D using Hungarian matching. This way, segments from preceding frames along with 3D information are used for tracking.

The last two baselines use 3D groundtruth for tracking. Table 4 shows that despite 3D information being used for matching, Contrastive Lift still significantly improves over the underlying 2D model.

**Frame-level improvement on underlying 2D segmentation models.** In addition to generating consistent (tracked) instance segmentations, our method also improves the per-frame quality (i.e., not considering tracking) of the underlying 2D segmentation model. To show this, we train Contrastive Lift on ScanNet scenes with different 2D models, _viz_. Mask2Former , MaskFormer  and Detic . In Table 5 we report the Panoptic Quality (PQ) metric (computed per frame) for these 2D models and for our method when trained with segments from each corresponding model.

Figure 5: Impact of the embedding size on the performance (PQ\({}^{}\)) of the instance module.

Figure 6: Embeddings obtained using vanilla (plain) contrastive learning and our proposed Slow-Fast contrastive learning. We use LDA  to project the embeddings to 2D for the illustration here.

Comparison of training speed with the linear-assignment loss method.While the exact number of objects present in a scene is unknown, linear assignment-based methods typically require a hyperparameter \(K\) that specifies the _maximum_ number of objects. Solving the linear assignment problem in PanopLi's loss is \(O(K^{3})\). Our method is agnostic to object count, eliminating the need for such a parameter. Our approach does rely on the size of the embedding size, but, as shown above, even a very small size suffices. In the slow-fast contrastive loss computation, the Softmax function dominates more than the pairwise similarity matrix calculation. Consequently, we find that the training speed of Contrastive Lift is largely unaffected by the choice of embedding size.

Table 6 compares the training speed, measured on a NVIDIA A40 GPU, between PanopLi and our method, showing that PanopLi iterations become slower as \(K\) increases. We only optimise the instance embedding field with associated losses, while the density/colour/semantic fields are frozen.

## 6 Limitations

Contrastive Lift improves noisy 2D input segmentations, but cannot recover from catastrophic failures, such as entirely missing object classes. It also requires the 3D reconstruction to work reliably. As a result, we have focused on static scenes, as 3D reconstruction remains unreliable in a dynamic setting. Contrastive Lift is a useful building block in applications, but has no particular direct societal impact. The datasets used in this paper are explicitly licensed for research and contain no personal data.

## 7 Conclusion

We have introduced Contrastive Lift, a method for fusing the outputs of 2D instance segmenter using a 3D neural fields. It learns a 3D vector field that characterises the different object instances in the scene. This field is fitted to the output of the 2D segmenter in a manner which is invariant to permutation of the object labels, which are assigned independently and arbitrarily in each input image. Compared to alternative approaches that explicitly seek to make multi-view labels compatible, Contrastive Lift is more accurate and scalable, enabling future work on larger object collections.

    &  \\   & \(K=25\) & \(K=50\) & \(K=100\) & \(K=500\) \\ 
**16.06 \(\) 2.34** & 13.01 \(\) 1.26 & 12.53 \(\) 0.92 & 12.10 \(\) 1.07 & 9.41 \(\) 0.60 \\   

Table 6: Training speed in iterations/second. Mean \(\) error margin measured over 8 runs.

   Method & PQ\({}^{}\) \\  Mask2Former  (M2F) (**non-tracked**) & \(32.3\) \\ M2F w/ Tracking method (1) & \(33.7\) \\ M2F w/ Tracking method (2) & \(34.0\) \\ M2F w/ Tracking method (3) & \(41.0\) \\ Contrastive Lift (**ours** trained w/ Mask2Former labels) & **62.3** \\   

Table 4: Comparison of our approach with the underlying 2D segmentations on ScanNet . For M2F predictions , consistency across frames is obtained with different tracking variants.

   Method & PQ \\  MaskFormer  & 41.1 \\ Contrastive Lift (w/ MaskFormer labels) & 61.7 \\ Mask2Former  & 42.0 \\ Contrastive Lift (w/ Mask2Former labels) & 61.6 \\ Detic  & 43.6 \\ Contrastive Lift (w/ Detic labels) & **62.1** \\   

Table 5: Improvement of per-frame segmentation quality as measured by Panoptic Quality (PQ).