# Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff

Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff

Jian Qian

MIT EECS

jianqian@mit.edu

Haichen Hu

MIT LIDS

huhc@mit.edu

David Simchi-Levi

MIT IDSS

dslevi@mit.edu

###### Abstract

Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression , we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon \(H\) (as known as CMDP with \(H\) layers). In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class \(\) containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only \(O(H T)\) calls to an offline density estimation algorithm (or oracle) across all \(T\) rounds of interaction. This number can be further reduced to \(O(H T)\) if \(T\) is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning.

## 1 Introduction

Markov Decision Processes (MDPs) model the long-term interaction between a learner and the environment and are used in diverse areas such as inventory management, recommendation systems, advertising, and healthcare . The Contextual MDP (CMDP) extends MDPs by incorporating external factors, known as _contexts_, such as gender, age, location, or device in customer interactions, or lab data and medical history in healthcare . In an \(H\)-layer CMDP, the learner receives an instantaneous reward at each step over \(H\) steps and aims to maximize the cumulative reward (return). For \(T\) rounds of interaction, the learner's performance is measured by _regret_, which is the difference between the total return obtained and that of an optimal policy.

In the special case of contextual bandits (one-layer CMDPs), a decade of research has led to algorithms with optimal regret bounds and efficient implementations with access to an offline regression algorithm (also termed as an _offline regression oracle_) . Most notably, Simchi-Levi and Xu  demonstrates an offline-oracle-based algorithm FALCON that achieves optimal regret for general (stochastic) contextual bandits with access to an offline regression oracle (e.g., the Empirical Risk Minimization (ERM) oracle). Moreover, the algorithm is efficient given the output of the offline oracle (also referred to as offline oracle-efficient) and requires only \(O( T)\) calls to the oracle across all \(T\) rounds if \(T\) is known. These properties are highly desirable in practice since they reduce the computational problem of contextual bandits to the classical problem of offline regression with little overhead. However, to the best of our knowledge, no algorithm with these properties is available in the literature for general (stochastic) CMDPs. So, in this paper, we study the following question: _Is there an offline-oracle-efficient algorithm that achieves the optimal regret for general (stochastic) CMDPs with only \(O(H T)\) number of oracle calls?_Several works have provided partial results for this question. Foster et al.  provides a general reduction from interactive decision making to online density estimation and has CMDP as an application. The proposed E2D algorithm achieves optimal regret but is online-oracle-efficient (as opposed to offline-oracle-efficient) since it requires access to an online density estimation algorithm. Foster et al.  provides a further reduction from online density estimation to offline density estimation, with the caveat that the reduction itself is inefficient. A similar online-oracle-efficient algorithm is developed by Levy et al. . A separate thread of optimism-based algorithms for CMDPs extending the UCCB algorithm for contextual bandits  is studied by Levy and Mansour , Deng et al.  with either assumption on the reachability of the CMDP or the representation structure of the CMDP (see Appendix B for more details). Last but not least, the algorithms proposed by Foster et al. [16; 18], Levy and Mansour , Deng et al.  all require \(O(T)\) number of oracle calls to the online/offline oracle respectively.

In this work, we present an affirmative answer to the question by introducing the algorithm of LOLIPOP (Algorithm 1). For \(S\) number of states, \(A\) number of actions, and a given model class \(\) where the underlying true model lies, the algorithm achieves the regret guarantee of \(((H,S,A)|})\). This regret guarantee is minimax optimal up to \((H,S,A)\) factor . The LOLIPOP algorithm assumes access to a Maximum Likelihood Estimation (MLE) oracle and is offline-oracle efficient. The results can be generalized to general offline density estimation oracles. The most notable technical features are: (1) It generalizes the FALCON algorithm by Simchi-Levi and Xu  to adapt to the multi-layer structure of a CMDP. More specifically, the FALCON algorithm is divided into \(O( T)\) epochs, each corresponding to an oracle call, a fixed randomized policy. However, it is known for the MDPs that the learner has to switch its randomized policy at least \((H)\) times to achieve sublinear regret . Indeed, we further divide each epoch into \(H\) segments, each with an oracle call, a new randomized policy for layerwise exploration-exploitation tradeoff. (2) In each segment, the exploration-exploitation tradeoff is done through Inverse Gap Weighting (IGW) on estimated regret for a set of explorative policies. The idea of running IGW on such a policy cover is proposed by Foster et al. . However, their policy cover is designed for \(H\)-layer exploration-exploration tradeoff and only works with strong online estimation oracles. In contrast, we refine the estimation of the occupancy measure layerwise by introducing the _trusted occupancy measures_. This refinement enables our algorithm to work with offline oracles. (3) Many other policy cover-based methods [12; 29; 30; 5] are developed for exploration tasks. Most notably, Mhammedi et al.  clips the occupancy measures on states with low reachability. Our approach takes a step forward to clip all transitions with low reachability to compute the trusted occupancy measures.

Besides all the above novelties, the LOLIPOP algorithm is versatile and applicable to the pure exploration task of reward-free reinforcement learning for CMDPs. Concretely, it obtains near-optimal sample complexity of \(OH^{7}S^{4}A^{3}(||/)/^{2}\) with only \(O(H)\) number of oracle calls. Both the sample complexity bound and computational efficiency result for reward-free learning for stochastic CMDPs are new to the best of our knowledge.

## 2 Preliminaries

We defer the standard notation and related works to Appendices A and B.

  Algorithm & Regret rate & Computational complexity & Assumption \\  E2D  & Optimal & \(O(T)\) online oracle calls & No \\ OMG-CMDP!  & Optimal & \(O(T)\) online oracle calls & No \\ RM-UCDD  & Suboptimal & \(O(T)\) offline oracle calls & Reachability \\ CMDP-VR  & Optimal & \(O(T)\) offline oracle calls & Varying Rep. \\ LOLIPOP (**this work**) & Optimal & \(O( T)\) offline oracle calls & No \\  

Table 1: Algorithmsâ€™ performance with general finite model class \(\) and i.i.d. contexts. The optimal rate here refers to \(((H,S,A)|})\). All algorithms assume realizability, so it is omitted from the table. The reachability assumption and the varying representation assumption are very stringent for tabular CMDP, for details we refer to Appendix B.

### Problem Setup

A Contextual Makovian Decision Process (CMDP) is defined by the tuple \((,M=\{M(c)\}_{c},,,s\)1), where \(\) is the contextual space, \(\) is the state space, \(\) is the action space and \(s^{1}\) is a fixed starting state independent of the context. We focus on tabular CMDPs which assumes \(S=||,A=||<\). For any context \(c\), \(M(c)=\{P_{M}^{h}(c),R_{M}^{h}(c)\}_{h=1}^{H}\) consists of \(H\)-layers of probability transition kernel \(\{P_{M}^{h}(c)\}_{h=1}^{H}\) and reward distributions \(\{R_{M}^{h}(c)\}_{h=1}^{H}\), where \(P_{M}^{h}(c)\) and \(R_{M}^{h}(c)\) are specified by \(P_{M}^{h}( s,a;c)()\) and \(R_{M}^{h}(s,a;c)()\) for all \(h[H]\) and \(s,a\). For simplicity, we also denote \(M=\{P_{M}^{h},R_{M}^{h}\}_{h=1}^{H}\), where \(P_{M}^{h}=\{P_{M}^{h}(c)\}_{c}\) and \(R_{M}^{h}=\{R_{M}^{h}(c)\}_{c}\). Let \(_{}\) denote the set of all randomized, non-stationary policies, where any \(=(^{1},,^{H})_{}\) has \(^{h}:()\) for any \(h[H]\). We use \(T\) to denote the total number of rounds, \(H\) to denote the horizon (the total number of layers). Let \(M_{}=\{P_{}^{h},R_{}^{h}\}_{h[H]}\) be the underlying true CMDP the learner interact with. The interactive protocal proceeds in \(T\) rounds, where for each round \(t\), the \(t\)-th trajectory is generated as:

* A context \(c_{t}\) is draw i.i.d. from an unknown distribution \(\) and \(s_{t}^{1}=s^{1}\).
* The learner chooses the policy \(_{t}\) based on the context.
* For \(h=1,,H\):
* The action is drawn from the randomized policy \(a_{t}^{h}_{t}^{h}(s_{t}^{h})\).
* The reward and the next state is drawn respectively from the reward distribution and the transition kernel, i.e., \(r_{t}^{h} R_{}^{h}(s_{t}^{h},a_{t}^{h};c_{t})\) and \(s_{t}^{h+1} P_{}^{h}( s_{t}^{h},a_{t}^{h};c_{t})\).

Without lose of generality, throughout the paper, we assume that the total reward \(0_{h=1}^{H}r^{h} 1\) almost surely. For any model \(M\), context \(c\) and policy \(\), we use \(M(,c)\) to denote the distribution of the trajectory \(c_{1},_{1},s_{1}^{1},a_{1}^{1},r_{1}^{1},,s_{t}^{H},a_{t}^{H},r_{t}^ {H}\) given \(M_{}=M\), \(c_{1}=c\), and \(_{1}=\). Also denote the probability and the expectation under \(M(,c)\) to be \(^{M,,c}()\) and \(^{M,,c}[]\) respectively. Given any policy \(\), state \(s\) and action \(a\), we define the action value function \(Q_{}^{h}(s,a;,c)\) at layer \(h\) and the value function \(V_{}^{h}(s;,c)\) at layer \(h\) under context \(c\) and policy \(\) as

\[Q_{}^{h}(s,a;,c)=_{j=h}^{H}^{M_{},,c}[r_{j}^{1}  s_{t}^{h},a_{1}^{h}=s,a]\;\;\;\;V_{}^{h}(s;,c)=_{a }Q_{}^{h}(s,a;,c).\]

We denote the optimal policy under context \(c\) as \(_{,c}\) and abbreviate its value function as \(V_{}^{h}(;c)\). For \(h=1\), we further simply the notation by denoting \(V_{}^{1}(c)=V_{}^{1}(s^{1};c)\) and \(V_{}^{1}(,c)=V_{}^{1}(s^{1};,c)\). The regret of policy \(\) under context \(c\) and the total regret2 of the learner are defined as

\[(,c)=V_{}^{1}(c)-V_{}^{1}(,c) (T):=_{t=1}^{T}_{t}[(_{t},c_{t})],\]

where \(_{t}[]\) is the conditional expectation given the interaction up to round \(t\).

**Assumption 2.1** (Realizability).: _The learner is given a model class \(\) where the true underlying model \(M_{}\) lies, that is, \(M_{}\)._

### Offline Densitiy Estimation Oracles

For any model class \(\), a general offline density estimation oracle associated with \(\), denoted by \(_{}\), is defined as an algorithm that generates a predictor \(\) based on the input data and \(\). In this paper, we measure the performance of the predictor in terms of the squared Hellinger distance, which is defined for any two distributions \(\) and \(\) for any common dominating measure \(^{2}\) by

\[D_{}^{2}(,):=}{d}}-}{d}}^{2}d,\]

Using squared Hellinger distance for reinforcement learning is popularized by Foster et al. , and we adopt such a divergence for our purpose as well. Concretely, we are interested in the following statistical guarantee.

**Definition 2.1** (Offline density estimation oracle).: _Let \(p\) be a map from a context to a distribution on the set of policies \(_{}\), that is, for any \(c\), \(p(c)(_{})\). Given \(n\) training trajectories \((c_{i},_{i},s_{i}^{1},a_{i}^{1},r_{i}^{1},,s_{i}^{H},a_{i}^{H},r_{i}^{H})\) i.i.d. drawn according to \(c_{i}\), \(_{i} p(c_{i})\) and\(s^{1}_{i},a^{1}_{i},r^{1}_{i},,s^{H}_{i},a^{H}_{i},r^{H}_{i}\) be the trajectory sampled according to \(M_{}(_{i},c_{i})\). The offline density estimation oracle \(}_{}\) returns a predictor \(\). For any \((0,1/2)\), with probability at least \(1-\), we have_

\[_{c, p(c)}D_{}^{2} (,c),M_{}(,c)_{, }(n).\]

The Maximum Likelihood Estimation estimator \(_{}\) is an example of an offline density estimation oracle that achieves \(_{,}(n)(||/)/n\) (see more details in Appendix C) and can be implemented using ERM on the log loss. Moreover, this implementation can be efficient for cases like the multinomial logit model (Srivastava et al., 2017, Example 2.4) and (Bengio et al., 2019).

## 3 Main Results and Algorithm

In this section, we present our main results and introduce the algorithm of LOLIPOP (Algorithm 1). First, we give an overview of the algorithm. Then, we discuss the theoretical guarantees obtained by this algorithm. Finally, we introduce the algorithm's different components with corresponding guarantees. All proofs are deferred to Appendix D.

### Main Results

Overview of Algorithm 1.The algorithm proceeds with epochs. The total number of \(T\) rounds is divided into \(N\) epochs. For an epoch schedule \(0=_{0}<_{1}<<_{N}=T/H\) to be specified later, the \(m\)-th epoch will last \(H(_{m}-_{m-1})\) rounds. Furthermore, each epoch is evenly divided into \(H\) segments, each consisting of \(_{m}-_{m-1}\) rounds. During the \(h\)-th segment in \(m\)-th epoch, a kernel \(p^{h}_{m}:()\) will be specified to determine the policy. More specifically, upon receiving the context \(c_{t}\), a policy \(_{t}\) will be sampled from \(p^{h}_{m}(c_{t})\) and executed. After collecting the trajectories \(\{c_{t},_{t}\}\{s^{j}_{t},a^{j}_{t},r^{j}_{t}\}_{j[H]}\) in the \(h\)-th segment of the \(m\)-th epoch for \(_{m-1}H+(_{m}-_{m-1})(h-1)+1 t_{m-1}H+(_{m}-_{ m-1})h\), the offline density estimation oracle \(}_{}\) is called with these trajectories as input. Denote the output \(^{h}_{m}\), we will only be interested in the \(h\)-th layer of this output, which we denote by \(\{^{h}_{m},^{h}_{m}\}\). Then the collections of estimators \(_{m}=\{^{h}_{m},^{h}_{m}\}_{h=1}^{H}\) will be used for the next epoch.

Throughout this paper, we will adopt the following convention for the free variables \(m,,c,h,s,a\). They will be used to denote an epoch index in \([N]\), a policy in \(_{}\), a context in \(\), a layer index in \([H]\), a state in \(\), and an action in \(\) respectively.

Before we dive into the details of the algorithm, we highlight first the theoretical guarantees obtained.

**Theorem 3.1**.: _If \(T\) is known, then by choosing the epoch schedule \(_{m}=2(T/H)^{1-2^{-m}}\) for \(m 1\) and the offline density estimation oracle \(}_{}=_{}\), the outputs \(\{_{t}\}_{t[T]}\) of Algorithm 1 satisfies that with probability at least \(1-\),_

\[(T)S^{4}A^{3}T(|| T /) T}\]

_with only \(O(H T)\) number of oracle calls to the \(_{}\) oracle for \((0,1/2)\)._

**Theorem 3.2**.: _If \(T\) is not known, then by choosing the epoch schedule \(_{m}=2^{m}\) for \(m 1\) and the offline density estimation oracle \(}_{}=_{}\), the outputs \(\{_{t}\}_{t[T]}\) of Algorithm 1 satisfies that with probability at least \(1-\),_

\[(T)S^{4}A^{3}T(|| T/ )}\]

_with \(O(H T)\) number of oracle calls to the \(_{}\) oracle for \((0,1/2)\)._

The theorems above show that Algorithm 1 with both epoch schedules achieve near-optimal statistical complexity that a matches the lower bound of \((|/ A})\) proven by Levy and Mansour (2017) up to a \((H,S,A)\) factor.

Computational efficiency.Consider the epoch schedule \(_{m}=2^{m}\) for \(m\) as discussed in Theorem 3.2. For any unknown \(T\), our algorithm operates over \(O( T)\) epochs, making one oracle call per epoch. Thus, the computational complexity is \(O( T)\) oracle calls over \(T\) rounds, with an additional per-round cost of \(O((H,S,A, T))\). This offers potential advantages over existing algorithms that achieve near-optimal rates without assumptions beyond realizability. The E2Dalgorithm , for instance, requires \(O(T)\) calls to an online density estimation oracle, involving significantly more calls to a more complex oracle for a general model class \(\). On the other hand, the Version Space Averaging + E2D algorithm  requires \(O(T)\) calls to an offline density estimation oracle and incurs a computational cost scaling with \(O(||)\) per round. Compared to our algorithm, this results in far more oracle calls and considerably higher computational costs per round.

If the total number of rounds \(T\) is known to the learner, we can further reduce the computational cost of LOLIPOP. For any \(T\), consider the epoch schedule \(_{m}=2(T/H)^{1-2^{-m}}\) as in Theorem3.1, similar to Simchi-Levi and Xu . In this scenario, LOLIPOP will run in \(O( T)\) epochs, making only \(O( T)\) oracle calls over \(T\) rounds while still maintaining a slightly worse regret guarantee.

Lower bound on switching cost.There is a lower bound on the switching cost of the scale \(( T)\), where the switching cost is the number of switches in the learner's randomized policy. Thus, any learner that only switches its randomized policy after an oracle call will need more than \(( T)\) number of oracle calls.

Technical challenge.The main technical challenge is to accurately estimate the occupancy measures for all layers. Naively, the upper bound of divergence between the true occupancy measure and the estimated occupancy measure accumulates exponentially with respect to the number of layers because, naively, the divergence at each layer is upper bounded by the summation of divergences from previous steps. This phenomenon is unavoidable when estimating all the occupancy measures from one dataset generated from a single policy. To avoid this exponential divergence, we apply two methods. First, we turn to layerwise design. Specifically, we generate for occupancy measures at each layer a new dataset from a different policy. This alleviates the exponential accumulation of divergence. Second, we turn to multiplicative guarantees between true occupancy measures and the approximated occupancy measures, i.e., they are equivalent up to a small constant. To achieve this, we construct the trusted occupancy measure (see Definition3.1), which discards the rarely visited state-action pairs. We then use the trusted occupancy measures to guide exploration.

### Detailed Construction and Guarantees of Each Component

In this section, we explain in detail our construction and the guarantees of each component along the dependence graph (Figure 1). We first introduce how the estimators \(\{^{h}_{m-1},^{h}_{m-1}\}_{h[H]}\) from the previous epoch are used in the new epoch. Then we proceed to introduce how to construct \(p^{h}_{m}(c_{t})\) given \(^{h}_{m}(c_{t})\). Next, we introduce how are \(^{h}_{m},^{h}_{m}\) obtained given \(p^{h}_{m}\). Subsequently, we present the definition of the set of trusted transitions \(}^{h}_{m}\) and trusted occupancy measure \(^{h}_{m}\). Finally, we present how the policy cover \(^{h}_{m}(c_{t})\) is constructed.

During epoch \(m\), we will be using the estimators \(\{^{h}_{m-1},^{h}_{m-1}\}_{h=1}^{h}\) from the previous epoch for regret estimation. More specifically, for \(,c,h,s\), we define the value functions with respect to the model \(\{^{h}_{m-1}(c),^{h}_{m-1}(c)\}_{h=1}^{H}\) as \(^{h}_{m-1}(s;,c)\). The optimal value function is denoted by \(^{1}_{m-1}(s;c)=_{}^{1}_{m-1}(s;,c)\). For \(h=1\), we further simply the notation by denoting \(^{1}_{m-1}(c)=^{1}_{m-1}(s^{1};c)\) and \(^{1}_{m-1}(,c)=^{1}_{m-1}(s^{1};,c)\). Also denote the optimal policy under context \(c\) by \(_{m-1,c}=_{}^{1}_{m-1}(,c)\). Thus, the regret is estimated to be

\[}_{m-1}(,c)=^{1}_{m-1}(c)-^{1} _{m-1}(,c).\]

At round \(t\), let \(m(t)\) and \(h(t)\) be the epoch in which the segment round \(t\) lies. We note that during each epoch \(m\) and segment \(h\), all of the notions \(^{h}_{m}(c_{t}),^{h}_{m}(c_{t})\), \(^{h}_{m}(c_{t})\), \(}^{h}_{m}(c_{t})\), \(^{h}_{m}(,,c_{t})\), \(p^{h}_{m}(c_{t})\), and \(^{h,s,a}_{m,c_{t}}\) will not depend on the specific time step \(t\), but only the context \(c_{t}\). Thus, we will use \(^{h}_{m}(c)\) to denote the policy cover if \(c_{t}=c\) when \(m(t),h(t)=m,h\). Similar conventions regarding the context \(c\) apply to the notations \(^{h}_{m},^{h}_{m},}^{h}_{m}, ^{h}_{m},p^{h}_{m},^{h,s,a}_{m,c}\). Under any context \(c\), the policy cover \(^{h}_{m}\) will include \(_{m-1,c}\) and has no more than \(SA+1\) policies. These two properties together guarantee that the Inverse Gap Weighting  randomized policy \(p^{h}_{m}(c)\) (Line 10) satisfies the following guarantee on the estimated regret.

**Lemma 3.1**.: _For any \(m\), \(h\), \(c\), the definition of the randomized policy \(p^{h}_{m}(c)\) is well defined, i.e., there exist \(^{h}_{m,c}[0,2SA]\) such that \(_{^{h}_{m}(c)}p^{h}_{m,c}()=1\). Furthermore, we have the estimated regret is bounded by \(_{ p^{h}_{m}(c)}}_{m-1}(,c) S^{4}A^{3}_{m}}\)._

The choice of \(^{h}_{m,c}\) here is for compactness of presentation. It can be chosen to be \(2SA\) for suboptimal arms and collect the probability remained to the optimal arm , which is computationally efficient. The computation for the policy \(^{h(t),s,a}_{m(t),c_{t}}\) for any \(t,s,a\) can be computed in \((H,S,A, T)\) time by formulating it as a linear fractional programming problem. We defer the details to Appendix G.

Since \(p^{h}_{m}\) maps \(\) to randomized policies, it is thus a policy kernel. This means the trajectories generated in epoch \(m\) and segment \(h\) follow an i.i.d. distribution as described in the definition of Definition 2.1. By applying the guarantee from Definition 2.1, we have the following guarantee on \(^{h}_{m},^{h}_{m}\).

**Lemma 3.2**.: _For any \(m\), \(h\), and \(c, p_{m}^{h}(c)\), we have with probability at least \(1-}\), that_

\[_{c,}[^{M_{*},,c}[D_{}^{2} _{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{}^{h}(s_{1}^{h},a_{1}^{h};c )+D_{}^{2}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),R _{}^{h}(s_{1}^{h},a_{1}^{h};c)]]_{m}.\]

If the offline density estimation oracle is chosen to be the Maximum Likelihood Estimation oracle \(_{}\), we will obtain \(_{m}(T/)/(_{m-1}-_{m-2})\).

The most involved part of our construction concerns the idea of _trusted transitions_ and _trusted occupancy measures_. This construction eliminates the parts of transitions that are too scarcely visited. The purpose will be clear in the guarantees (Lemmas 3.3 and 3.4) subsequent to the definitions.

**Definition 3.1**.: _For any \(m,,c,h,s,a\), we define iteratively the **trusted occupancy measures**\(_{m}^{h}(s;,c)\), \(_{m}^{h}(s,a;,c)\) and the set of **trusted transitions**\(}_{m}^{h}(c)\) at layer \(h\) as the following:_

\[_{m}^{h}(s;,c)&=(s=s^{1}),_{m}^{h}(s,a;,c)=(s=s^{1})^{1} (s,a),\\ _{m}^{h}(s;,c)&:=_{s^{},a^ {},s,c}_{m}^{h-1}(s^{},a^{};,c)_{m}^{h-1}(s|s^{},a^{};c),\\ _{m}^{h}(s,a;,c)&:=_{m} ^{h}(s;,c)^{h}(s,a).\] (2)

_For any \(m,h,c\), the set of trusted transitions are defined as the set of transitions_

\[}_{m}^{h}(c)\{(s,a,s^{}) _{}_{m}^{h}(s,a;,c)}{SA+_{m}}_{m-1}(,c)}_{m}^{h}(s^{}|s,a;c) }\},\] (3)

_where \(_{m}=}{8eH(H+1)^{2}}\). Notice that to define \(_{m}^{h}(s;,c)\) and \(_{m}^{h}(s,a;,c)\), we **only** need \(\{}_{m}^{j}(c),_{m}^{j}(c)\}_{j[h-1]}\). Thus the two definitions are iteratively well-defined. Meanwhile, we also define the **observable occupancy measures** as the occupancy measures of the true model going through only the trusted transitions, i.e.,_

\[ d_{m}^{1}(s;,c)&=(s=s ^{1}), d_{m}^{1}(s,a;,c)=(s=s^{1})^{1}(s,a),\\ d_{m}^{h}(s;,c)&:=_{s^{},a^{}, s}_{m}^{h-1}(c)}d_{m}^{h-1}(s^{},a^{}; ,c)P_{}^{h-1}(s|s^{},a^{};c),\\ d_{m}^{h}(s,a;,c)&:=d_{m}^{h}(s;,c)_{h}(s,a).\]

The computation of the set of trusted transitions need not enumerate all policies. The trusted transition set can be computed in \((H,S,A, T)\) time by formulating it as a linear fractional programming problem. We defer the details to Appendix G.

Define the estimated occupancy measures \(_{m}^{h}(s;,c):=^{_{m},,c}[(s_{1}^{h}=s)]\) and \(_{m}^{h}(s,a;,c):=^{_{m},,c}[ (s_{1}^{h},a_{1}^{h}=s,a)]\). The trusted occupancy measure, though it eliminates rarely visited transitions, remains a valid estimate for all policies because the divergence between the estimated occupancy measure and itself is bounded. Specifically, we have the following lemma.

**Lemma 3.3**.: _For all \(m,,h,s,a\), under any context \(c\), we have_

\[_{m}^{h}(s,a;,c)-_{m}^{h}(s,a;,c) 32eS ^{2}A_{m}}+}_{m-1}(;c)/(90HSA).\]

The next guarantee is the key to our analysis and is the most non-trivial guarantee of our construction. The following lemma states that if, for a context \(c\), the Hellinger divergence between \(\) and \(P_{}^{h}\) at layer \(h\) is small for all \(h[H]\), then the trusted occupancy measure is upper bounded by a scaling of the observable occupancy measure.

**Lemma 3.4**.: _For any \(m\) and \(c\), assume for all \(h[H]\),_

\[_{ p_{m}^{h}(c)}^{M_{*},,c}[D_{}^ {2}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{}^{h}(s_{1}^{h},a _{1}^{h};c)] H/_{m}.\]

_Then for the same \(m,c\) and all \(,h,s,a\), we have_

\[_{m}^{h}(s,a;,c)(1+1/H)^{2(h-1)}d_{m}^{h}(s,a;,c).\]Since the trusted occupancy measure is upper bounded up to scaling by the observable occupancy measure, then the state-action pairs with large trusted occupancy measures are guaranteed to be visited often in the true dynamics as well. This enables more accurate planning and is thus crucial to our analysis.

Finally, we state the coverage guarantee achieved by the construction of \(_{m}^{h}\) and \(p_{m}^{h}\). Concretely, we upper bound the trusted occupancy measure \(_{m}^{h}(,;,)\) of any policy \(\) by the trusted occupancy measure induced by policy kernel \(p_{m}^{h}\).

**Lemma 3.5**.: _For any \(m,,c,h,s,a\), we have_

\[_{m}^{h}(s,a;,c)& D _{}(P_{}^{h}(s,a;c),_{m}^{h}(s,a;c))\\ &}{e^{2}H} p_{m}^{h}(c,_{m,c}^{h,s,a}) _{m}^{h}(s,a;_{m,c}^{h,s,a},c) D_{}^{2}(P_{ }^{h}(s,a;c),_{m}^{h}(s,a;c))+_{m}^{},\] (4)

_where \(_{m}^{}=(2e^{2}_{m}}{H^{4}S^{2}A} }+S^{3}A^{2}}}_{m-1}(,c) )\!\!_{m}^{h}(s,a;,c)\),and \(p_{m}^{h}(c,_{m,c}^{h,s,a})\) is the probability of \(p_{m}^{h}(c)\) on \(_{m,c}^{h,s,a}\). The guarantee Eq.4 also holds replacing \(D_{}(P_{}^{h}(s,a;c),_{m}^{h}(s,a;c))\) with \(D_{}(R_{}^{h}(s,a;c),_{m}^{h}(s,a;c))\) on both sides of the inequality._

## 4 Regret Analysis

In this section, we provide a proof sketch of the regret analysis. Detailed proofs are deferred to Appendix E. We first aggregate the component-wise guarantees (Lemmas 3.1 to 3.5) from Section3 to present the following epoch-wise guarantee.

**Lemma 4.1**.: _For any \(m\), any policies \(\{_{c}\}_{c}\), and \((0,1/2)\), with probability at least \(1-/M\),_

\[_{c}_{m}^{1}(_{c},c)-V_{ }^{1}(_{c},c)_{c}}_{m-1}(_{c},c)+77eS^{4}A^{3} _{m}}.\]

Proof sketch of Lemma4.1.For simplicity, in this proof sketch, we assume the true reward distribution is known3. For this, we first apply the celebrated local simulation lemma (Lemma C.2) in reinforcement learning to relate the divergence of the value functions to the stepwise divergences as the following. Under any context \(c\),

\[|_{m}^{1}(_{c},c)-V_{}^{1}( _{c},c)|&_{h,s,a}_{m}^{h}(s,a;_{c},c)D_{}_{m}^{h}(s,a;c),P_{}^{h}(s,a ;c).\]

Then we can exchange the estimated occupancy measure \(_{m}^{h}(s,a;_{c},c)\) by the trusted occupancy measure through Lemma3.3, that is,

\[_{m}^{h}(s,a;_{c},c)D_{}_{m}^{h}(s,a;c),P_{}^{h}(s,a;c) _{m}^{h}(s,a;_{c},c)D_{} _{m}^{h}(s,a;c),P_{}^{h}(s,a;c)\] \[+32eS^{2}A_{m}}+ }_{m-1}(;c)/(90HSA).\]

Then by coverage guarantee Lemma3.5, for any \(h,s,a\), we can bound

\[_{m}^{h}(s,a;_{c},c)D_{}& _{m}^{h}(s,a;c),P_{}^{h}(s,a;c)\\ &}{e^{2}H} p_{m}^{h}(c,_{m,c}^{h,s,a} )_{m}^{h}(s,a;_{m,c}^{h,s,a},c) D_{}^{2}(P_{ }^{h}(s,a;c),_{m}^{h}(s,a;c))+_{m}^{}.\]

If the assumption in Lemma3.4 is satisfied, then by Lemma3.4 and the definition of \(p_{m}^{h}\), we have

\[_{h,s,a}}{e^{2}H}&  p_{m}^{h}(c,_{m,c}^{h,s,a})_{m}^{h}(s,a;_{m,c}^{h,s,a},c) D_{}^{2}(P_{}^{h}(s,a;c),_{m}^{h}(s,a;c))\\ &_{h,s,a}}{H} p_{m}^{h}(c,_{m,c}^{h,s,a})d_{m}^{h}(s,a;_{m,c}^{h,s,a},c) D_{}^{2}(P_{}^{h} (s,a;c),_{m}^{h}(s,a;c))\\ &}{H}_{h}_{ p_{m}^{h }(c)}[^{M_{},,c}[D_{}^{2}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{}^{h}(s_{1}^{h},a_{1}^{h};c) ]].\]If the assumptions in Lemma 3.4 are not satisfied, we have similar control as well (see the full proof Appendix E for details). Altogether with taking expectation on \(c\), by the offline density estimation bound from Lemma 3.2, we have

\[_{c}^{1}_{m}(_{c},c)-V^{1}_ {}(_{c},c)}_{m- 1}(_{c},c)/40+39eS^{4}A^{3}_{m}}.\]

A revised version of the regret analysis (Lemma E.1) in Simchi-Levi and Xu , which relates the epoch-wise guarantee to the regret estimation error, can be found in Appendix E. Combining Lemmas 4.1 and E.1, we arrive at the following general regret guarantee.

**Theorem 4.1**.: _The outputs \(\{_{t}\}_{t[T]}\) of Algorithm 1 satisfies with probability at least \(1-\) that_

\[(T)_{m=1}^{N}(_{m}-_{m-1}) S^{4}A^{3}_{m}}\]

_for \((0,1/2)\)._

## 5 Extension: Reward-free Reinforcement Learning for CMDPs

In this section, we introduce the application of Algorithm 1 in the task of reward-free reinforcement learning in (stochastic) CMDPs. All proofs in this section will be deferred to Appendix F.

Reward-free reinforcement learning .Reward-free reinforcement learning aims to efficiently explore the environment without relying on observed rewards. By doing so, it aims to enable the computation of a nearly optimal policy for any given reward function, utilizing only the trajectory data collected during exploration and without needing further interaction with the environment. This approach holds particular significance in scenarios where reward functions are refined over multiple iterations to encourage specific behaviors through trial and error, such as in constrained RL formulations. In such cases, repeatedly applying the same RL algorithm with varying reward functions can prove to be highly inefficient regarding sample usage, underscoring the efficiency of reward-free reinforcement learning.

Problem formulation.The major differences between the regret minimization setting (Section 2.1) and the reward-free reinforcement learning are that in the latter, no reward signals are observed during the interaction, and the goal of the latter is to output a CMDP prediction \(\) whose value functions are close to the underlying true CMDP \(M_{}\) for any reward distributions. To accommodate such a change, for any model \(M=\{P^{h}_{M},R^{h}_{M}\}_{h[H]}\) and reward distribution \(R=\{R^{h}\}_{h[H]}\), we define \(M(;R)=\{P^{h}_{M},R^{h}\}_{h[H]}=\{P^{h}_{M}(c),R^{h}(c)\}_{h[H],c }\) to be model \(M\) with the reward distribution part replaced by \(R\). Thus in the reward-free reinforcement learning setting, the underlying true model satisfies \(M_{}=M_{}(;0)\) where \(0\) is used to denote the reward distribution that is constantly \(0\).

For any model \(M\) reward distribution \(R\), context \(c\) and policy \(\), we use \(M(,c;R)\) to denote the distribution of the trajectory \(c_{1},_{1},s^{1}_{1},a^{1}_{1},r^{1}_{1},,s^{H}_{1},a^{H}_{1},r^{H}_{ 1}\) given \(M_{}=M(;R)\), \(c_{1}=c\), and \(_{1}=\). Also denote the probability and the expectation under \(M(,c;R)\) to be \(^{M,,c,R}()\) and \(^{M,,c,R}[]\) respectively. Given reward distribution \(R\), any policy \(\), state \(s\) and action \(a\), we define the action value function \(Q^{h}_{}(s,a;,c,R)\) at layer \(h\) and the value function \(V^{1}_{}(s;,c,R)\) at layer \(h\) under context \(c\) and policy \(\) as

\[Q^{h}_{}(s,a;,c,R)=_{j=h}^{H}^{M_{},,c,R}[r^{j} _{1} s^{h}_{1},a^{h}_{1}=s,a]\;\;\;\;V^{h}_{}(s;,c,R)= _{a}Q^{h}_{}(s,a;,c,R).\]

We denote the optimal policy with reward distribution \(R\) under context \(c\) as \(_{,c,R}\) and abbreviate its value function as \(V^{h}_{}(;c,R)\). For \(h=1\), we denote \(V^{1}_{}(c,R)=V^{1}_{}(s^{1};c,R)\) and \(V^{1}_{}(,c,R)=V^{1}_{}(s^{1};,c,R)\). We also denote \(V^{h}_{M}\) as the value functions when \(M_{}=M\).

**Assumption 5.1** (Realizability for reward-free RL).: _Suppose the learner is given a model class \(\) that contains the underlying true model \(M_{}\). Assume all models \(M\) have \(0\) reward._

For a given \(,>0\) and a model class \(\), the goal of the learner is to output a model \(\) at the end of the interaction such that for any reward distribution \(R\) and set of policies \(\{_{c}\}_{c}\), the model satisfies

\[_{c}V^{1}_{}(_{c},c,R)-V^{1}_{ }(_{c},c,R)\] (5)

with probability at least \(1-\). An algorithm that achieves this objective is called \((,)\)-learns the model class \(\). Then we have the following guarantee from Algorithm 1.

**Theorem 5.1**.: _If we choose \(_{1}=T/(2H)\) and \(_{2}=T/H\), the outputs \(_{2}\) of Algorithm 1 satisfies the reward-free objective Eq. (5) with probability at least \(1-\), with \(T\) at most bounded by_

\[T OH^{}S^{4}A^{3}(||/)/^{2} \]

_for \((0,1/2)\). Moreover, the algorithm requires \(O(H)\) number of oracle calls to the \(_{}\) oracle._

The proof follows a similar argument of Lemma 4.1. In addition, we have a matching lower bound up to a \((H,S,A)\) factor adapted from the non-contextual lower bound from Jin et al. .

**Theorem 5.2**.: _Fix \( 1\), \( 1/2\), \(H,A 2\). Suppose \(S L A\) for a large enough universal constant \(L\) and \(K 0\) large enough. Then, there exists a CMDP class \(\) with \(||=S\), \(||=A\), \(||=K\), and horizon \(H\) and a distribution \(\) on \(\) such that any algorithm \(\) that \((/24,)\)-learns the class \(\) satisfies \(_{M}_{M,}[T]||/ ^{2}\), where \(T\) is the number of trajectories required by the algorithm \(\) to achieve \((/24,)\) accuracy and \(_{M,}[]\) is the expectation under the interaction between the algorithm \(\) and model \(M\)._

## 6 Discussion

Extension to low rank CMDPs.Low rank MDPs represent a significant extension to tabular MDPs, as explored in various studies [6; 28; 21; 4]. Linear MDPs are typically the first step beyond tabular MDPs. Extending our approach to linear CMDPs would be a substantial achievement. The primary challenge lies in identifying the trusted transitions within linear CMDPs. The current construction for tabular CMDPs does not readily apply here because it does not utilize the low-rank structure.

Extension to model-free learning.Our approach is model-based. However, model-free methods are often more practical for real-world applications. The main challenge lies in effectively balancing exploration and exploitation using only the value functions, as opposed to our method which depends on the occupancy measure.

More efficient oracles.In this paper, we focus on offline density estimation oracles due to the necessity of a small Hellinger distance between the estimated model and the true model for our approach. An offline regression oracle would only provide a 2-norm distance guarantee, which is inadequate for our purposes. Nevertheless, it is interesting to explore whether a reduction from CMDPs to offline regression could be feasible.