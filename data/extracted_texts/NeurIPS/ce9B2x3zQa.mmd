# Max-Sliced Mutual Information

Dor Tsur

Ben-Gurion University

&Ziv Goldfeld

Cornell University

&Kristjan Greenewald

MIT-IBM Watson AI Lab

###### Abstract

Quantifying dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast computation and scalable estimation from samples. We show that mSMI retains favorable structural properties of Shannon's mutual information, like variational forms and identification of independence. We then study statistical estimation of mSMI, propose an efficiently computable neural estimator, and couple it with formal non-asymptotic error bounds. We present experiments that demonstrate the utility of mSMI for several tasks, encompassing independence testing, multi-view representation learning, algorithmic fairness, and generative modeling. We observe that mSMI consistently outperforms competing methods with little-to-no computational overhead.

## 1 Introduction

Dependence measures between random variables are fundamental in statistics and machine learning for tasks spanning independence testing [1; 2; 3], clustering [4; 5], representation learning [6; 7], and self-supervised learning [8; 9; 10]. There is a myriad of measures quantifying different notions of dependence, with varying statistical and computational complexities. The simplest is the Pearson correlation coefficient , which only captures linear dependencies. At the other extreme is Shannon's mutual information , which is a universal dependence measure that is able to identify arbitrarily intricate dependencies. Despite its universality and favorable properties, accurately estimating mutual information from data is infeasible in high-dimensional settings. First, mutual information estimation rates suffers from the curse of dimensionality, whereby convergence rates deteriorate exponentially with dimension . Additionally, computing mutual information requires integrating a log-likelihood ratio over a high-dimensional space, which is generally intractable.

Between these two extremes is the popular canonical correlation analysis (CCA) , which identifies maximally correlated linear projections of variables. Nevertheless, classical CCA still only captures linear dependence, which has inspired nonlinear extensions such as Hirschfeld-Gebelein-Renyi (HGR) maximum correlation [15; 16; 17], kernel CCA [18; 19], deep CCA [20; 7], and various other generalizations [21; 22; 23; 24]. However, HGR is computationally infeasible, while kernel and deep CCA can be burdensome in high dimensions, as they require optimization over reproducing kernel Hilbert spaces or deep neural networks, respectively. To overcome these shortcomings, this work proposesmax-sliced mutual information (mSMI)--a scalable information-theoretic extension of CCA that captures the full dependence structure while only requiring optimization over linear projections.

### Contributions

The mSMI is defined as the maximal mutual information between linear projections of the variables. Namely, the \(k\)-dimensional mSMI between \(X\) and \(Y\) with values in \(^{d_{x}}\) and \(^{d_{y}}\), respectively, is1

\[}_{k}(X;Y)_{(,) (k,d_{x})(k,d_{y})}(^{ }X;^{}Y),\]

where \((k,d)\) is the Stiefel manifold of \(d k\) matrices with orthonormal columns. Unlike the nonlinear CCA variants that use nonlinear feature extractors in the high-dimensional ambient spaces, mSMI retains the linear projections of CCA and captures nonlinear structures in the _low-dimensional_ feature space. This is done by using the mutual information between the projected variables, rather than correlation, as the optimization objective. Beyond being considerably simpler from a computational standpoint, this crucial difference allows mSMI to identify the full dependence structure, akin to classical mutual information. mSMI can also be viewed as the maximized version of the average-sliced mutual information (aSMI) , which averages \((^{}X;^{}Y)\) with respect to (w.r.t.) the Haar measure over \((k,d_{x})(k,d_{y})\). However, we demonstrate that compared to aSMI, mSMI benefits from improved neural estimation error bounds and a clearer interpretation.

We show that mSMI inherits important properties of mutual information, including identification of independence, tensorization, and variational forms. For jointly Gaussian \((X,Y)\), the optimal mSMI projections coincide with those of \(k\)-dimensional CCA , posing mSMI as a natural information-theoretic generalization. Beyond the Gaussian case, the solutions differ and mSMI may yield more effective representations for downstream tasks due to the intricate dependencies captured by mutual information. We demonstrate this superiority empirically for multi-view representation learning.

For efficient computation, we propose an mSMI neural estimator based on the Donsker-Varadhan (DV) variational form . Neural estimators have seen a surge in interest due to their scalability and compatibility with gradient-based optimization . Our estimator employs a single model that composes the projections with the neural network approximation of the DV critic, and then jointly optimizes them. This results in both the estimated mSMI value and the optimal projection matrices. Building on recent analysis of neural estimation of \(f\)-divergences , we establish non-asymptotic error bounds that scale as \(Ok^{1/2}(^{-1/2}+kn^{-1/2})\), where \(\) and \(n\) are the numbers of neurons and \((X,Y)\) samples, respectively. Equating \(\) and \(n\) results in the (minimax optimal) parametric estimation rate, which highlights the scalability of mSMI and its compatibility to modern learning settings.

In our empirical investigation, we first demonstrate that our mSMI neural estimator converges orders of magnitude faster than that of aSMI . This is because the latter requires (parallel) training of many neural estimators corresponding to different projection directions, while the mSMI estimator optimizes a single combined model. Notwithstanding the reduction in computational overhead, we show that mSMI outperforms average-slicing for independence testing. Next, we compare mSMI with deep CCA  by examining downstream classification accuracy based on representations obtained from both methods in a multi-view learning setting. Remarkably, we observe that even the linear mSMI projections outperform nonlinear representations obtained from deep CCA. We also consider an application to algorithmic fairness under the informin framework . Replacing their generalized Pearson correlation objective with mSMI, we again observe superior performance in the form of more fair representations whose utility remains on par with the fairness-agnostic model. Lastly, we devise a max-sliced version of the InfoGAN by replacing the classic mutual information regularizer with its max-sliced analog. We show that despite the low-dimensional projections, the max-sliced InfoGAN successfully learns to disentangle the latent space and generates quality samples.

## 2 Background and Preliminaries

**Notation.** For \(a,b\), we use the notation \(a b=\{a,b\}\) and \(a b=\{a,b\}\). For \(d 1\), \(\|\|\) is the Euclidean norm in \(^{d}\). The Stiefel manifold of \(d k\) matrices with orthonormal columnsis denoted by \((k,d)\). For a \(d k\) matrix \(\), we use \(^{}:^{d}^{k}\) for the orthogonal projection onto the row space of \(\). For \(^{d k}\) with \(()=r k d\), we write \(_{1}(),,_{r}()\) for its non-zero singular values, and assume without loss of generality (w.l.o.g.) that they are arranged in descending order. Similarly, the eigenvalues of a square matrix \(^{d d}\) are denoted by \(_{1}(),,_{d}()\). Let \((^{d})\) denote the space of Borel probability measures on \(^{d}\). For \(,(^{d})\), we use \(\) to denote a product measure, while \(()\) designates the support of \(\). All random variables throughout are assumed to be continuous w.r.t. the Lebesgue measure. For a measurable map \(f\), the pushforward of \(\) under \(f\) is denoted by \(f_{}= f^{-1}\), i.e., if \(X\) then \(f(X) f_{}\). For a jointly distributed pair \((X,Y)_{XY}(^{d_{x}}^{d_{y}})\), we write \(_{X}\) and \(_{XY}\) for covariance matrix of \(X\) and cross-covariance matrix of \((X,Y)\), respectively.

Canonical correlation analysis.CCA is a classical method for devising maximally correlated linear projections of a pair of random variables \((X,Y)_{XY}(^{d_{x}}^{d_{y}})\) via 

\[(_{},_{})=*{ argmax}_{(,)^{d_{x}}^{d_{y}}}^{}}{}_{XX} ^{}_{YY}}}=*{argmax}_{ {c}(,)^{d_{x}}^{d_{y}}:\\ ^{}_{X}=^{}_{Y}=1} ^{}_{XY},\] (1)

where the former objective is the correlation coefficient \((^{}X,^{}Y)\) between the projected variables and the equality follows from invariance of \(\) to scaling. The global optimum has an analytic form as \((_{},_{})=(_{X}^{-1/2}_{1}, _{Y}^{-1/2}_{1})\), where \((_{1},_{1})\) is the (unit-length) top left- and right-singular vector pair associated with the largest singular value of \(_{XY}_{X}^{-1/2}_{XY}_{Y}^{-1/2} ^{d_{x} d_{y}}\). This solution is efficiently computable in \(O((d_{x} d_{y})^{3})\) time, given that the population correlation matrices are known. CCA extends to \(k\)-dimensional projections via the optimization 

\[_{(,)^{d_{x} k} ^{d_{y} k}:\\ ^{}_{X}=^{}_{Y} =_{k}}(^{} _{XY}),\] (2)

with the optimal CCA matrices being \((_{},_{})=(_{X}^{-1/2} _{k},_{Y}^{-1/2}_{k})\), where \(_{k}\) and \(_{k}\) are the matrices of the first \(k\) left- and right-singular vectors of \(_{XY}\). The optimal objective value then becomes the sum of the top \(k\) singular values of \(_{XY}\) (namely, its Ky Fan \(k\)-norm).

Divergences and information measures.Let \(,(^{d})\) satisfy \(\), i.e., \(\) is absolutely continuous w.r.t. \(\). The Kullback-Leibler (KL) divergence is defined as \((\|)_{^{d}}(d/d)d\). We have \((\|) 0\), with equality if and only if (iff) \(=\). Mutual information and differential entropy are defined from the KL divergence as follows. Let \((X,Y)_{XY}(^{d_{x}}^{d_{y}})\) and denote the corresponding marginal distributions by \(_{X}\) and \(_{Y}\). The mutual information between \(X\) and \(Y\) is given by \((X;Y)(_{XY}\|_{X}_{Y})\) and serves as a measure of dependence between those random variables. The differential entropy of \(X\) is defined as \((X)=(_{X})-(_{X}\|)\). Mutual information between (jointly) continuous variables and differential entropy are related via \((X;Y)=(X)+(Y)-(X,Y)\); decompositions in terms of conditional entropies are also available .

## 3 Max-Sliced Mutual Information

We now define the \(k\)-dimensional mSMI, establish structural properties thereof, and explore the Gaussian setting and its connections to CCA. We focus here on the case of (linear) \(k\)-dimensional projections and discuss extensions to nonlinear slicing in Section 3.3.

**Definition 1** (Max-sliced mutual information).: _For \(1 k d_{x} d_{y}\), the \(k\)-dimensional mSMI between \((X,Y)_{XY}(^{d_{x}}^{d_{y}})\) is_

\[}_{k}(X;Y)_{(,) (k,d_{x})(k,d_{y})}(^{}X;^{}Y),\] (3)

_where \((k,d)\) is the Stiefel manifold of \(d k\) matrices with orthonormal columns._

The mSMI measures Shannon's mutual information between the most informative \(k\)-dimensional projections of \(X\) and \(Y\). It can be viewed as a maximized version of the aSMI \(}_{k}(X;Y)\) from [25; 26], defined as the integral of \((^{}X;^{}Y)\) w.r.t. the Haar measure over \((k,d_{x})(k,d_{y})\). For \(d=d_{x}=d_{y}\), we have \(}_{d}(X;Y)=}_{d}(X;Y)=(X;Y)\) due to invariance of mutual information to bijections. The supremum in mSMI is achieved since the Stiefel manifold is compact and the function \((,)(^{}X;^{ }Y)\) is Lipschitz and thus continuous (Lemma 2 of ).

**Remark 1** (Multivariate and conditional mSMI).: _The mSMI definition above extends to the multivariate and conditional cases as follows. Let \((X,Y,Z)_{XYZ}(^{d_{x}}^{d_{y}} ^{d_{z}})\). The \(k\)-dimensional multivariate and conditional mSMI functionals are, respectively, \(}_{k}(X,Y;Z)_{,,}(^{}X,^{}Y;^{ }Z)\) and \(}_{k}(X;Y|Z)_{,, }(^{}X;^{}Y| ^{}Z)\). Connections between \(}_{k}(X;Y)\) and its multivariate and conditional versions are given in the proposition to follow. We also note that one may generalize the definition of \(}_{k}(X;Y)\) to allow for projections into feature spaces of different dimensions, i.e., \((k_{x},d_{x})\) and \((k_{y},d_{y})\), for \(k_{x} k_{y}\). We expect our theory to extend to that case, but leave further exploration for future work._

In the spirit of mSMI, we define the max-sliced differential entropy.

**Definition 2** (Max-sliced entropy).: _The \(k\)-dimensional max-sliced (differential) entropy of \(X_{X}(^{d})\) is \(}_{k}(X)}_{k}() _{(k,d)}(^{}X)\)._

An important property of classical differential entropy is the maximum entropy principle , which finds the highest entropy distribution within given class. In Appendix B, we study the max-sliced entropy maximizing distribution in several common scenarios. For instance, we show that \(}_{k}\) is maximized by the Gaussian distribution under a fixed (mean and) covariance constraint. Namely, letting \(_{1}(m,)(^{d}) :\,()=^{d}\,,\,_{}[X]=m\,,\,_{ }(X-m)(X-m)^{}=}\), we have \(*{argmax}_{_{1}(,)}}_{k}()=(m,)\). An intimate connection between max-sliced entropy and PCA is established in the sequel, under the Gaussian setting.

**Remark 2** (Sliced divergences).: _The slicing technique has originated as a means to address scalability issues concerning statistical divergences. Significant attention was devoted to sliced Wasserstein distances as discrepancy measures between probability distributions [41; 42; 43; 44; 45; 46; 47]. As such, the sliced Wasserstein distance differs from mutual information and its sliced variants, which quantify dependence between random variables, rather than discrepancy per se. Additionally, as Wasserstein distances are rooted in optimal transport theory, they heavily depend on the geometry of the underlying data space. Mutual information, on the other hand, is induced by the KL divergence, which only depends on the log-likelihood of the considered distributions and overlooks geometry._

### Structural Properties

The following proposition lists useful properties of the mSMI, which are similar to those of the average-sliced variant (cf. [26, Proposition 1]) as well as Shannon's mutual information itself.

**Proposition 1** (Structural properties).: _The following properties hold:_

1. _Bounds:_ _For any integers_ \(k_{1}<k_{2}\)_:_ \(}_{k_{1}}(X;Y)}_{k_{1}}(X;Y) }_{k_{2}}(X;Y)(X;Y)\)_._
2. _Identification of independence:_ \(}_{k}(X;Y) 0\) _with equality iff_ \((X,Y)\) _are independent._
3. _KL divergence representation:_ _We have_ \[}_{k}(X;Y)=_{(,)(k, d_{x})(k,d_{y})}(^{}, ^{})_{\#}_{XY}(^{}, ^{})_{\#}_{X}_{Y},\]
4. _Sub-chain rule:_ _For any random variables_ \(X_{1},,X_{n},Y\)_, we have_ \[}_{k}(X_{1},,X_{n};Y)}_{k }(X_{1};Y)+_{i=2}^{n}}_{k}(X_{i};Y|X_{1},,X_{i-1 }).\]
5. _Tensorization:_ _For mutually independent_ \(\{(X_{i},Y_{i})\}_{i=1}^{n}\)_,_ \(}_{k}\{X_{i}\}_{i=1}^{n};\{Y_{i}\}_{i=1}^{n} =_{i=1}^{n}}_{k}(X_{i};Y_{i})\)_._

The proof follows by similar arguments to those in the average-sliced case, but is given for completeness in Supplement A.1. Of particular importance are Properties 2 and 3. The former renders mSMI sufficient for independence testing despite being significantly less complex than the classical mutual information between the high-dimensional variables. The latter, which represent mSMI as a supremized KL divergence, is the basis for neural estimation techniques explored in Section 4.

**Remark 3** (Relation to average-SMI).: _Beyond the inequality relationship in Property I above, Proposition 4 in  (paraphrased) shows that for matrices \(_{x},_{y}\) and vectors \(b_{x},b_{y}\) of appropriate dimensions, we have \(_{_{x},_{y},b_{x},b_{y}} }_{1}(_{x}^{}X+b_{x};_{y}^{}Y+b_{y})= }_{1}(X;Y)\), and the relation readily extends to projection dimension \(k>1\). In words, optimizing the aSMI over linear transformations of the high-dimensional data vectors coincides with the max-sliced version. This further justifies the interpretation of \(}_{k}(X;Y)\) as the information between the two most informative representations of \(X,Y\) in a \(k\)-dimensional feature space. It also suggests that mSMI is compatible for feature extraction tasks, as explored in Section 5.3 ahead._

### Gaussian Max-SMI versus CCA

The mSMI is an information-theoretic extension of the CCA coefficient \(_{}(X,Y)\), which is able to capture higher order dependencies. Interestingly, when \((X,Y)\) are jointly Gaussian, the two notions coincide. We next state this relation and provide a closed-form expression for the Gaussian mSMI.

**Proposition 2** (Gaussian mSMI).: _Let \(X(m_{X},_{X})\) and \(Y(m_{Y},_{Y})\) be \(d_{x}\)- and \(d_{y}\)-dimensional jointly Gaussian vectors with nonsingular covariance matrices and cross-covariance \(_{XY}\). For any \(k d_{x} d_{y}\), we have_

\[}_{k}(X;Y)=(_{}^{ }X;_{}^{}Y)=-_{i=1}^{k} 1-_{i}(_{XY})^{2},\] (4)

_where \((_{},_{})\) are the CCA solutions from (2), \(_{XY}=_{X}^{-1/2}_{XY}_{Y}^{-1/2}^{d_ {x} d_{y}}\), and \(_{k}(_{XY})_{1}(_{XY}) 1\) are the top \(k\) singular values of \(_{XY}\) (ordered)._

This proposition is proven in Supplement A.2. We first show that the optimization domain of \(}_{k}(X;Y)\) can be switched from the product of Stiefel manifolds to the space of all matrices subject to a unit variance constraint (akin to (2)), without changing the mSMI value. This implies that the CCA solutions \((_{},_{})\) from (2) are feasible for mSMI and we establish their optimality using a generalization of the Poincare separation theorem [48, Theorem 2.2]. Specializing Proposition 2 to one-dimensional projections, i.e., when \(k=1\), the mSMI is given in terms of the canonical correlation coefficient \(_{}(X,Y)_{(,)^{d_ {x}}^{d_{y}}}\)\((^{}X,^{}Y)\). Namely,

\[}_{1}(X;Y)=(_{}^{} X;_{}^{}Y)=-0.51-_{}(X,Y)^{2} ,\]

where \((_{},_{})\) are the global optimizers of \(_{}(X,Y)\).

**Remark 4** (Beyond Gaussian data).: _While the mSMI solution coincides with that of CCA in the Gaussian case, this is no longer expected to hold for non-Gaussian distributions. CCA is designed to maximize correlation, while mSMI has Shannon's mutual information between the projected variables as the optimization objective. Unlike correlation, mutual information captures higher order dependencies between the variables, and hence the optimal mSMI matrices will not generally coincide with \((_{},_{})\). Furthermore, the intricate dependencies captured by mutual information suggest that the optimal mSMI projections may yield representations that are more effective for downstream tasks. We empirically verify this observation in Section 5 on several tasks, including classification, multi-view representation learning, and algorithmic fairness._

Similarly to the above, the Gaussian max-sliced entropy is related to PCA . In Supplement A.3, we prove the following.

**Proposition 3** (Gaussian max-sliced entropy).: _For a \(d\)-dimensional Gaussian variable \(X(m,)\), we have \(}_{k}(X)=_{(k,d)} (A^{}X)=(_{}^{ }X)=0.5_{i=1}^{k}2 e_{i}()\), where \(_{}\) is optimal PCA matrix and \(_{1}(),_{k}()\) are the top \(k\) eigenvalues of \(\)._

Note that the eigenvalues \(_{1}(),_{k}()\) are non-negative since \(\) is a covariance matrix. Extrapolating beyond the Gaussian case, this poses max-sliced entropy as an information-theoretic generalization of PCA for unsupervised dimensionality reduction. An analogous extension using the Renyi entropy of order \(2\) was previously considered in  for the purpose of binary classification. In that regard, \(}_{k}(X)\) can be viewed as the \(\)-Renyi variant when \( 1\).

### Generalizations Beyond Linear Slicing

The notion of mSMI readily generalizes beyond linear slicing. Fix \(d_{x},d_{y} 1\), \(k d_{x} d_{y}\), and consider two (nonempty) function classes \(\{g:^{d_{x}}^{k}\}\) and \(\{h:^{d_{y}}^{k}\}\).

**Definition 3** (Generalized mSMI).: _The generalized mSMI between \((X,Y)_{XY}(^{d_{x}}^{d_{y}})\) w.r.t. the classes \(\) and \(\) is \(}_{,}(X;Y)_{(g,h) }g(X);h(Y)\)._

The generalized variant reduces back to \(}_{k}(X;Y)\) by taking \(=_{}\{^{}: \,(k,d_{x})\}\) and \(=_{}\{^{}: \,(k,d_{y})\}\), but otherwise allows more flexibility in the way \((X,Y)\) are mapped into \(^{k}\). We also have that if \(^{}\) and \(^{}\), then \(}_{,}(X;Y)}_{^{},}(X;Y)(X;Y)\), which corresponds to Property 1 from Proposition 1. Further observations are as follows.

**Proposition 4** (Properties).: _For any classes \(,\), we have that \(}_{,}\) always satisfies Properties 3-5 from Proposition 1. If further \(_{}\) and \(_{}\), then \(}_{,}\) also satisfies Property 2._

We omit the proof as it follows by the same argument as Proposition 1, up to replacing the linear projections with the functions \((g,h)\). In practice, the classes \(\) and \(\) are chosen to be parametric, typically realized by artificial neural networks. As discussed in Remark 5 ahead, this is well-suited to the neural estimation framework for mSMI (both standard and generalized). Lastly, note that \(}_{,}(X;Y)\) corresponds to the objective of multi-view representation learning , which considers the maximization of the mutual information between NN-based representation of the considered variables. We further investigate this relation in Section 5.3.

## 4 Neural Estimation of Max-SMI

We study estimation of mSMI from data, seeking an efficiently computable and scalable approach subject to formal performance guarantees. Towards that end, we observe that the mSMI is compatible with neural estimation [29; 38] due to its convenient variational form. In what follows we derive the neural estimator, describe the algorithm to compute it, and provide non-asymptotic error bounds.

### Estimator and Algorithm

Fix \(d_{x},d_{y} 1\), \(k d_{x} d_{y}\), and \(_{XY}(^{d_{x}}^{d_{y}})\); we suppress \(k,d_{x},d_{y}\) from our notation of the considered function classes. Neural estimation is based on the DV variational form:2

\[(X;Y)=_{f}_{}(f;\,_{XY}), _{}(f;\,_{XY})[f(X,Y)]- e^{[f(,)]},\]

where \((X,Y)_{XY}\), \((,)_{X}_{Y}\), and \(\) is the class of all measurable functions \(f:^{d_{x}}^{d_{y}}\) (often referred to as DV potentials) for which the expectations above are finite. As mSMI is the maximal mutual information between projections of \(X,Y\), we have

\[}_{k}(X;Y)=_{(,)(k,d _{x})(k,d_{y})}_{f}_{} f;\,(^{},^{})_{}_{XY} =_{f^{}}_{}(f;_{ XY}),\]

where \(^{}f(^{}, ^{})\,:\,f,\,(,) (k,d_{x})(k,d_{y})}\). The RHS above is given by optimizing the DV objective \(_{}\) over the _composed_ class \(^{}\), which first projects \((X,Y)(^{}X,^{}Y)\) and then applies a DV potential \(f:^{k}^{k}\) to the projected variables.

Neural estimator.Neural estimators parametrize the DV potential by neural nets, approximate expectations by sample means, and optimize the resulting empirical objective over parameter space. Let \(_{}\) be a class of feedforward networks with input space \(^{k}^{k}\) and real-valued outputs.3 Given i.i.d. samples \((X_{1},Y_{1}),,(X_{n},Y_{n})\) from \(_{XY}\), we first generate negative samples (i.e., from \(_{X}_{Y}\)) by taking \((X_{1},Y_{(1)}),,(X_{n},Y_{(n)})\), where \( S_{n}\) is a permutation such that\((i) i\), for all \(i=1,,n\). The neural estimator of \(_{k}(X;Y)\) is now given by

\[_{k}^{_{}}(X^{n},Y^{n}) _{f_{}^{}}_{i=1}^{n}f (X_{i},Y_{i})-(_{i=1}^{n}e^{f(X_{i},Y_{(i)})} ),\] (5)

where \(_{}^{}\{f(^{ },^{})\,:\,f_{},\,( ,)(k,d_{x})(k,d_{y})\}\) is the composition of the neural network class with the projection maps. The projection maps can be absorbed into the network architecture as a first linear layer that maps the \((d_{x}+d_{y})\)-dimensional input to a \(2k\)-dimensional feature vector, which is then further processed by the original \(f_{}\) network. Note that projection onto the Stiefel manifold can be efficiently and differentially computed via QR decomposition. Hence, the Stiefel manifold constraint can be easily enforced by setting \(A,B\) to be the projections of unconstrained \(d k\) matrices. Further details on the implementation are given in Supplement C.

**Remark 5** (Nonlinear slicing).: _For learning tasks that may need more expressive representations of \((X,Y)\), one may employ the nonlinear mSMI variant from Section 3.3. In practice, the classes \(=\{g_{}\}\) and \(=\{h_{}\}\) are taken to be parametric, realized by neural networks. These networks naturally compose with the DV critic \(f_{}\), resulting in a single compound model \(f_{}(g_{},h_{})\)._

### Performance Guarantees

Neural estimation involves three sources of error: (i) function approximation of the DV potential; (ii) empirical estimation of the means; and (iii) optimization, which comes from employing suboptimal (e.g., gradient-based) routines. Our analysis provides sharp non-asymptotic bounds for errors of type (i) and (ii), leaving the account of the optimization error for future work. We focus on a class of \(\)-neuron shallow ReLU networks, although the ideas extend to other nonlinearities and deep architectures. Define \(_{}^{}\) as the class of all \(f:^{k}^{k}\), \(f(z)=_{i=1}^{}_{i}( w_{i},z+b_{i})+  w_{0},z+b_{0}\), whose parameters satisfy \(_{1 i}\|w_{i}\|_{1}|b_{i}| 1\), \(_{1 i}|_{i}|}{2}\), and \(|b_{0}|,\|w_{0}\|_{1} a_{}\), where \((z)=z 0\) is the ReLU activation and \(a_{}= 1\).

Consider the neural mSMI estimator \(_{k}^{n,}_{k}^{_{}^{ }}(X^{n},Y^{n})\) (see (5)). We provide convergence rates for it over an appropriate distribution class, drawing upon the results of  for neural estimation of \(f\)-divergences. For compact \(^{d_{x}}\) and \(^{d_{y}}\), let \(_{}()\) be the set of all Lebesgue absolutely continuous joint distribution \(_{XY}\) with \((_{XY})\). Denote the Lebesgue density of \(_{XY}\) by \(f_{XY}\). The distribution class of interest is4

\[_{k}(M,b)\{_{XY}_{}( ): \,r_{b}^{k+3}( )&\\ & f_{XY}=r|_{ },\;(X;Y) M\},\] (6)

which, in particular, contains distributions whose densities are bounded from above and below on \(\) with a smooth extension to an open set covering \(\). This includes uniform distributions, truncated Gaussians, truncated Cauchy distributions, etc. The following theorem provides the convergence rate for the mSMI neural estimator, uniformly over \(_{k}(M,b)\).

**Theorem 1** (Neural estimation error).: _For any \(M,b 0\), we have_

\[_{_{X,Y}_{k}(M,b)}[|}_{k}(X;Y)-}_{k}^{n,}|] Ck^{} ^{-}+kn^{-}.\]

_where \(C\) depends on \(M\), \(b\), \(k\), and the radius of the ambient space \(\|\|_{(x,y) }\|(x,y)\|\)._

The theorem is proven in Supplement A.4 by adapting the error bound from [38, Proposition 2] to hold for \((^{}X;^{}Y)\), uniformly over \((,)(k,d_{x})(k,d_{y})\). To that end, we show that for any \(_{XY}_{k}(b,M)\), the log-density of \((^{}X,^{}Y)(^{},^{})_{}_{XY}\) admits an extension (to an open set containing the support) with \(k+3\) continuous and uniformly bounded derivatives.

**Remark 6** (Parametric rate and optimality).: _Taking \( n\), the resulting rate in Theorem 1 is parametric, and hence minimax optimal. This result implicitly assumes that \(M\) is known when picking the neural net parameters. This assumption can be relaxed to mere existence of (an unknown) \(M\), resulting in an extra \(()\) factor multiplying the \(n^{-1/2}\) term._

**Remark 7** (Comparison to average-SMI).: _Neural estimation of classic mutual information under the framework of  requires the density to have Holder smoothness \(s(d_{x}+d_{y})/2+3\). For \(}_{k}(X;Y)\), smoothness of \(k+3\) is sufficient (even though the ambient dimension is the same), which means it can be estimated over a larger distribution class. Similar gains in terms of smoothness levels were observed for asMI in . Nevertheless, we note that mSMI is more compatible with neural estimation than average-slicing [25; 26]. The mSMI neural estimator integrates the max-slicing into the neural network architecture and optimizes a single objective. The asMI neural estimator from  requires an additional Monte Carlo integration step to approximate the integral over the Steifel manifolds. This results in an extra \(k^{1/2}m^{-1/2}\) term in the error bound, where \(m\) is the number of Monte Carlo samples, introducing a burdensome computational overhead (see Section 5.1)._

**Remark 8** (Non-ReLU networks).: _Theorem 1 employs the neural estimation bound from , which relies on  to control the approximation error. As noted in , their bound extends to any other sigmoidal bounded activation with \(_{z-}(z)=0\) and \(_{z}(z)=1\) by appealing to the approximation bound from  instead. Doing so would allow relaxing the smoothness requirement on the extension to \(r_{b}^{k+2}\) in (6), but at the expense of scaling the hidden layer parameters as \(^{1/2}\) (as opposed to the ReLU-based bound, where the parameter scale is independent of \(\))._

## 5 Experiments

### Neural Estimation

We compare the performance of neural estimation methods for mSMI and aSMI on a synthetic dataset of correlated Gaussians. Let \(X,Z(0,1)\) be i.i.d. and set \(Y= X+}Z\), for \((0,1)\). The goal is to estimate the \(k\)-dimensional mSMI and aSMI between \((X,Y)\). We train our mSMI neural estimator and the aSMI neural estimator from [26, Section 4.2] based on \(n\) i.i.d. samples, and compare their performance as a function of \(n\). Both average and max-sliced algorithms converge at similar rates; however, aSMI has significantly higher time complexity due to the need to train multiple neural estimators (one for each projection direction). This is shown in Figure 1, where we compare the average epoch time for each algorithm against the dataset size. Implementation details are given in Supplement C.

### Independence Testing

In this experiment, we compare mSMI and aSMI for independence testing. We follow the setting from [26, Section 5], generating \(d\)-dimensional samples correlated in a latent \(d^{}\)-dimensional subspace and estimating the information measure to determine dependence. We estimate the aSMI with the method from , using \(m=1000\) Monte Carlo samples and the Kozachenko-Leonenko estimator for the mutual information between the projected variables . We then compute AUC-ROC over 100 trials,

Figure 1: Neural estimation performance with \(=0.5\). Convergence vs. \(n\) in upper figures and average epoch time vs. \(n\) in lower figure.

Figure 2: ROC-AUC comparison. Dashed and solid lines show results for aSMI  and mSMI (ours), respectively. Blue plots correspond to \((d,d^{})=(10,4)\), while red correspond to \((d,d^{})=(20,6)\).

considering various ambient and projected dimensions. For mSMI, as we cannot differentiate through the Kozachenko-Leonenko estimator, we resort to gradient-free methods. We employ the LIPO algorithm from  with a stopping criterion of 1000 samples. This choice is motivated by the Lipschitzness of \((,)|\{^{}X;^{ }Y\}.\) w.r.t. the Frobenius norm on \((k,d_{x})(k,d_{y})\) (cf. [26, Lemma 2]). Figure 2 shows that when \(k>2\), mSMI captures independence better than asMI, particularly in the lower sample regime. We hypothesize that this is due to the fact that the shared signal lies in a low-dimensional subspace, which mSMI can isolate and perhaps better exploit than asMI, which averages over all subspaces. When \(k\) is much smaller than the shared signal dimension \(d^{}\), mSMI fails to capture all the information and asMI, which takes all slices into account, may be preferable. Results are averaged over 10 seeds. Further implementation details are in Supplement C.

### Multi-View Representation Learning

We next explore mSMI as an information-theoretic generalization of CCA by examining its utility in multi-view representation learning--a popular CCA application. Without using class labels, we obtain mSMI-based \(k\)-dimensional representations of the top and bottom halves of MNIST images (considered as two separate views of the digit image). This is done by computing the \(k\)-dimensional mSMI between the views and using the maximizing projected variables as the representations. We compare to similarly obtained CCA-based representations, following the method of . Both linear and nonlinear (parameterized by an MLP neural network) slicing models are optimized with similar initialization and data but different loss functions. Performance is evaluated via downstream 10-class classification accuracy, utilizing the learned top-half representations. Results are averaged over 10 seeds. As shown in Table 1, mSMI outperforms CCA for learning meaningful representations. Interestingly, linear representations learned by mSMI outperform nonlinear representations from the CCA methodology, demonstrating the potency of mSMI. Full implementation details and additional results are given in Supplements C and D, respectively.

The asMI is not considered for this experiment since it does not provide a concrete latent space representation (as it is an averaged quantity). Moreover, if one were to maximize asMI as an objective to derive such representations, this would simply lead back to computing mSMI; cf. Remark 3.

### Learning Fair Representations

Another common application of dependence measures is learning fair representations of data. We seek a data transformation \(Z=f(X)\) that is useful for predicting some outcome or label \(Y\), while being statistically independent of some sensitive attribute \(A\) (e.g., gender, race, or religion of the subject). In other words, a fair representation is one that is not affected by the subjects' protected attributes so that downstream predictions are not biased against protected groups, even if the training data may have been biased. Following the setup of , we measure utility and fairness using the HGR maximal correlation \(_{}(,)=_{h,g}h(),g()\), seeking large \(_{}(Z,Y)\) and small \(_{}(Z,A)\) where \(h\) and \(g\) are parameterized by neural networks. As solving this minimax problem directly is difficult in practice, following  we learn \(Z\) by optimizing the bottleneck equation \(_{}(Z,Y)-_{k}(Z,A)\), where we use a neural estimator for the mSMI and \(\), \(k\) are hyperparameters.

Table 2 shows results on the US Census Demographic dataset extracted from the 2015 American Community Survey, which has 37 features collected over 74,000 census tracts. Here \(Y\) is the fraction of children below the poverty line in a tract, and \(A\) is the fraction of women in the tract. Following the same experimental setup as , the learned \(Z\) is 80-dimensional. As  showed that their "Slice" approach significantly outperformed all other baselines on this experiments under a computational

  \(k\) & **Linear CCA** & **Linear mSMI** & **MLP DCCA** & **MLP mSMI** \\ 
1 & 0.261\(\)0.03 & **0.274\(\)**0.02 & 0.284\(\)0.03 & **0.291\(\)**0.02** \\
2 & 0.32\(\)0.02 & **0.346\(\)**0.02 & 0.314\(\)0.03 & **0.417\(\)**0.02 \\
4 & 0.42\(\)0.01 & **0.478\(\)**0.02 & 0.441\(\)0.04 & **0.546\(\)**0.01 \\
8 & 0.553\(\)0.03 & **0.666\(\)**0.01 & 0.655\(\)0.02 & **0.665\(\)**0.01 \\
12 & 0.614\(\)0.02 & **0.751\(\)**0.01 & 0.697\(\)**0.01 & **0.753\(\)**0.01 \\
16 & 0.673\(\)0.02 & **0.775\(\)**0.01 & 0.730\(\)0.02 & **0.779\(\)**0.01 \\
20 & 0.704\(\)0.007 & **0.791\(\)**0.006 & 0.774\(\)0.01 & **0.798\(\)**0.01 \\  

Table 1: Downstream classification accuracy from MNIST representations by CCA and mSMI.

   & **N/A** & **Slice** &  \\    & & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) & \(\) & \(k=7\) \\  \(_{}(Z,Y)\) & 0.949 & 0.967 & 0.955 & 0.958 & 0.952 & 0.942 & 0.940 & 0.957 & 0.933 \\  \(_{}(Z,A)\) & 0.795 & 0.116 & 0.220 & 0.099 & 0.067 & 0.048 & 0.029 & **0.026** & 0.047 \\  

Table 2: Learning a fair representation of the US Census Demographic dataset, following the setup of . Results are shown as the median over 10 runs with random data splits. The fairest result is \(k=6\).

constraint5, we apply the same computational constraint to our approach and compare only to Slice and to the "N/A" fairness-agnostic model trained on the bottleneck objective with \(=0\). Note that for \(k>1\), mSMI learns a more fair representation \(Z\) (lower \(_{}(Z,A)\)) than Slice, while retaining a utility \(_{}(Z,Y)\) on par with the fairness agnostic N/A model. We emphasize that due to the reasons outlined in Section 5.3, aSMI is not suitable for the considered task and is thus not included in the comparison. Results on the Adult dataset are shown in Supplement E.

### Max-Sliced InfoGAN

We present an application of max-slicing to generative modeling under the InfoGAN framework . The InfoGAN learns a disentangled latent space by maximizing the mutual information between a latent code variable and the generated data. We revisit this architecture but replace the classical mutual information regularizer in the InfoGAN objective with mSMI. Our max-sliced InfoGAN is tested on the MNIST and Fashion-MNIST datasets. Figure 3 presents the generated samples for several projection dimensions. We consider \(3\) latent codes \((C_{1},C_{2},C_{3})\), which automatically learn to encode different features of the data. We vary the values of \(C_{1}\), which is a \(10\)-state discrete variable, along the column (and consider random values of \((C_{2},C_{3})\) along the rows). Evidently, \(C_{1}\) successfully disentangles the 10 class labels and the quality of generated samples is on par with past implementations [56; 26]. We stress that since mSMI relies on low-dimensional projections, the resulting InfoGAN mutual information estimator uses a reduced number of parameters (at the negligible cost of optimizing over linear projections). Additional details are given in Supplement C.

## 6 Conclusion

This paper proposed mSMI, an information theoretic generalization of CCA. mSMI captures the full dependence structure between two high dimensional random variables, while only requiring an optimized linear projection of the data. We showed that mSMI inherits important properties of Shannon's mutual information and that when the random variables are Gaussian, the mSMI optimal solutions coincide with classic \(k\)-dimensional CCA. Moving beyond Gaussian distributions, we present a neural estimator of mSMI and establish non-asymptotic error bounds.

Through several experiments we demonstrate the utility of mSMI for tasks spanning independence testing, multi-view representation learning, algorithmic fairness and generative modeling, showing it outperforms popular methodologies. Possible future directions include an investigation of an operational meaning of mSMI, either in information theoretic or physical terms, extension of the proposed formal guarantees to the nonlinear setting, and the extension of the neural estimation convergence guarantees to deeper networks. Additionally, mSMI can provide a mathematical foundation to mutual information-based representation learning, a popular area of self-supervised learning [10; 57].

In addition to the above, we plan to develop a rigorous theory for the choice of \(k\), which is currently devised empirically and is treated as a hyperparameter. When the support of the distributions lies in some \(d^{}<d\) dimensional subspace, the choice of \(k=d^{}\) is sufficient to recover the classical mutual information, and therefore it characterizes the full dependence structure. Extrapolating from this point, we conjecture that the optimal value of \(k\) is related to the intrinsic dimension of the data distribution, even when it is not strictly supported on a low-dimensional subset.

Figure 3: MNIST images generated via the max-sliced InfoGAN.