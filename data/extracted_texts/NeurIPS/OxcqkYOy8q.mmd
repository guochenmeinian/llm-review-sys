# Improved Sample Complexity Bounds for Diffusion Model Training

Shivam Gupta

UT Austin

shivamgupta@utexas.edu

&Aditya Parulekar

UT Austin

adityaup@cs.utexas.edu

&Eric Price

UT Austin

ecprice@cs.utexas.edu

&Zhiyang Xun

UT Austin

zxun@cs.utexas.edu

###### Abstract

Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works  have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the _sample complexity_ of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work  showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an _exponential improvement_ in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.

+
Footnote â€ : Authors listed in alphabetical order.

## 1 Introduction

Score-based diffusion models are currently the most successful methods for image generation, serving as the backbone for popular text-to-image models such as stable diffusion , Midjourney, and DALL-E  as well as achieving state-of-the-art performance on other audio and image generation tasks .

The goal of score-based diffusion is to produce a generative model for a possibly complicated distribution \(q_{0}\). This involves two components: _training_ a neural network using true samples from \(q_{0}\) to learn estimates of its (smoothed) score functions, and _sampling_ using the trained estimates. To this end, consider the following stochastic differential equation, often referred to as the _forward_ SDE:

\[x_{t}=-x_{t}\,t+\,B_{t}, x_{0} q _{0}\] (1)

where \(B_{t}\) represents Brownian motion. Here, \(x_{0}\) is a sample from the original distribution \(q_{0}\) over \(^{d}\), while the distribution of \(x_{t}\) can be computed to be

\[x_{t} e^{-t}x_{0}+(0,_{t}^{2}I_{d})\]

for \(_{t}^{2}=1-e^{-2t}\). Note that this distribution approaches \((0,I_{d})\), the stationary distribution of (1), exponentially fast.

Let \(q_{t}\) be the distribution of \(x_{t}\), and let \(s_{t}(y):= q_{t}(y)\) be the associated _score_ function. We refer to \(q_{t}\) as the _\(_{t}\)-smoothed_ version of \(q_{0}\). Then, starting from a sample \(x_{T} q_{T}\), there is a reverseSDE associated with the above forward SDE in (1) :

\[x_{T-t}=(x_{T-t}+2s_{T-t}(x_{T-t}))\,t+\,B_{ t}.\] (2)

That is to say, if we begin at a sample \(x_{T} q_{T}\), following the reverse SDE in (2) back to time \(0\) will give us a sample from the _original_ distribution \(q_{0}\). This suggests a natural strategy to sample from \(q_{0}\): start at a large enough time \(T\) and follow the reverse SDE back to time \(0\). Since \(x_{T}\) approaches \((0,I_{d})\) exponentially fast in \(T\), our samples at time \(0\) will be distributed close to \(q_{0}\). In particular, if \(T\) is large enough--logarithmic in \(}{}\)--then samples produced by this process will be \(\)-close in \(\) to being drawn from \(q_{0}\). Here \(m_{2}^{2}\) is the second moment of \(q_{0}\), given by \(m_{2}^{2}_{x q_{0}}[\|x\|^{2}]\).

In practice, this continuous reverse SDE (2) is approximated by a time-discretized process. That is, the score \(s_{T-t}\) is approximated at some fixed times \(0=t_{0} t_{1} t_{k}<T\), and the reverse process is run using this discretization, holding the score term constant at each time \(t_{i}\). This algorithm is referred to as "DDPM", as defined in .

Chen et al.  proved that as long as we have access to sufficiently accurate estimates for each score \(s_{t}\) at each discretized time, this reverse process produces accurate samples. Specifically, for _any_\(d\)-dimensional distribution \(q_{0}\) supported on the Euclidean Ball of radius \(R\), the reverse process can sample from a distribution \(\)-close in \(\) to a distribution (\( R\))-close in 2-Wasserstein to \(q_{0}\) in \((d,1/,1/)\) steps, as long as the score estimates \(_{t}\) used at each step are \((^{2})\) accurate in squared \(L^{2}\). That is, as long as

\[*{}_{x q_{t}}[\|_{t}(x)-s_{t}(x)\|^{2}] (^{2}).\] (3)

In this work, we focus on understanding the _sample complexity_ of learning such score estimates. Specifically, we ask:

How many samples are required for a sufficiently expressive neural network to learn accurate score estimates that generate high-quality samples using the DDPM algorithm?

We consider a training process that employs the empirical minimizer of the _score matching objective_ over a class of neural networks as the score estimate. More formally, we consider the following setting:

**Setting 1.1**.: _Let \((D,P,)\) be the class of functions represented by a fully connected neural network with ReLU activations and depth \(D\), with \(P\) parameters, each bounded by \(\). Let \(\{t_{k}\}\) be some time discretization of \([0,T]\). Given \(m\) i.i.d. samples \(x_{i} q_{0}\), for each \(t\{t_{k}\}\), we take \(m\) Gaussian samples \(z_{i}(0,_{t}^{2}I_{d})\). We take \(_{t}\) to be the minimizer of the score-matching objective:_

\[_{t}=*{arg\,min}_{f}_{i= 1}^{m}\|f(e^{-t}x_{i}+z_{i})-}{_{t}^{2}} \|_{2}^{2}.\] (4)

_We then use \(\{_{t_{k}}\}\) as the score estimates in the DDPM algorithm._

This is the same setting as is used in practice, except that in practice (4) is optimized with SGD rather than globally. As in , we aim to output samples from a distribution that is \(\)-close in \(\) to a distribution that is \( R\) or \( m_{2}\)-close in Wasserstein to \(q_{0}\). We thus seek to bound the number of samples \(m\) to get such a good output distribution, in terms of the parameters of Setting 1.1 and \(,\).

Block et al.  first studied the sample complexity of learning score estimates using the empirical minimizer of the score-matching objective (4). They showed sample complexity bounds that depend on the Rademacher complexity of \(\). Applied to our setting and using known bounds on Rademacher complexity of neural networks, in Setting 1.1 their result implies a sample complexity bound of \((}{^{3}^{2}}(^{2}P)^{D} )\). See Appendix E for a detailed discussion.

Following the analysis of DDPM by , more recent work on the iteration complexity of _sampling_ has given an _exponential improvement_ on the Wasserstein accuracy \(\), as well as replacing the uniform bound \(R\) by \(m_{2}\) -- the square root of the second moment. Inparticular,  show that \((}^{2})\) iterations suffice to sample from a distribution that is \( m_{2}\) close to \(q_{0}\) in \(2\)-Wasserstein, as long as the score estimates are \((^{2}/_{}^{2})\) accurate, i.e.,

\[*{}_{x q_{t}}[||_{t}(x)-s_{t}(x)\|^{2}] (}{_{t}^{2}}).\] (5)

Inspired by these works, we ask: is it possible to achieve a similar exponential improvement in the sample complexity of _learning_ score estimates using a neural network?

### Our Results

We give a new sample complexity bound of \((}{^{3}}PD^{3})\) for learning scores to sufficient accuracy for sampling via DDPM. Learning is done by optimizing the same score matching objective that is used in practice. Compared to , our bound has _exponentially_ better dependence on \(,\), and \(D\), and a better polynomial dependence on \(d\) and \(P\), at the cost of a worse polynomial dependence in \(\).

As discussed above, for the sampling process, it suffices for the score estimate \(s_{t}\) at time \(t\) to have error \((^{2}/_{t}^{2})\). This means that scores at larger times need higher accuracy, but they are also intuitively easier to estimate because the corresponding distribution is smoother. Our observation is that the two effects cancel out: we show that the sample complexity to achieve this accuracy for a fixed \(t\) is _independent_ of \(_{t}\). However, this only holds once we weaken the accuracy guarantee slightly (from \(L^{2}\) error to "\(L^{2}\) error over a \(1-\) fraction of the mass"). We show that this weaker guarantee is nevertheless sufficient to enable accurate sampling. Our approach lets us run the SDE to a very small final \(_{t}=\), which yields a final \(\) dependence of \(O(^{3})\) via a union bound over the times \(t\) that we need score estimates \(s_{t}\) for. In contrast, the approach in  gets the stronger \(L^{2}\)-accuracy, but requires a \((})\) dependence in sample complexity for each score \(s_{t}\); this leads to their \(()\) sample complexity overall.

To state our results formally, we make the following assumptions on the data distribution and the training process:

* The second moment \(m_{2}^{2}\) of \(q_{0}\) is between \(1/(d)\) and \((d)\).
* For the score \(s_{t}\) used at each step, there exists some function \(f(D,P,)\) (as defined in Setting 1.1) such that the \(L^{2}\) error, \(*{}_{x q_{t}}[||f(x)-s_{t}(x)\|^{2}]\), is sufficiently small.

That is: the data is somewhat normalized, and the smoothed scores can be represented well in the function class. Our main theorem is as follows:

**Theorem 1.2**.: _In Setting 1.1, suppose assumptions **A1** and **A2** hold. For any \(>0\), consider the score functions trained from_

\[m(PD}{^{3}} ^{3})\]

_i.i.d. samples of \(q_{0}\). With \(99\%\) probability, DDPM using these score functions can sample from a distribution \(\)-close in \(\) to a distribution \( m_{2}\)-close to \(q\) in 2-Wasserstein._

We remark that assumption A1 is made for a simpler presentation of our theorem; the (logarithmic) dependence on the second moment is analyzed explicitly in Theorem C.2. The quantitative bound for A2--exactly how small the \(L^{2}\) error needs to be--is given in detail in Theorem C.3.

Barrier for \(L^{2}\) accuracy.As mentioned before, previous works have been using \(L^{2}\) accurate score estimation either as the assumption for sampling or the goal for training. Ideally, one would like to simply show that the ERM of the score matching objective will have bounded \(L^{2}\) error of \(^{2}/^{2}\) with a number of samples that scales polylogarithmically in \(\). Unfortunately, this is _false_. In fact, it is information-theoretically impossible to achieve this in general without \(()\) samples. Since sampling to \( m_{2}\) Wasserstein error needs to consider a final \(_{t}=\), this leads to a \(()\) dependence. See Figure 1, or the discussion in Section 4, for a hard instance.

In the example in Figure 1, score matching + DDPM still _works_ to sample from the distribution with sample complexity scaling with \(()\); the problem lies in the theoretical justification for it. Given that it is impossible to learn the score in \(L^{2}\) to sufficient accuracy with fewer than \(()\) samples, such a justification needs a different measure of estimation error. We will introduce such a measure, showing (1) that it will be small for all relevant times \(t\) after a number of samples that scales _polylogarithmically_ in \(\), and (2) that this measure suffices for fast sampling via the reverse SDE.

The problem with measuring error in \(L^{2}\) comes from outliers: rare, large errors can increase the \(L^{2}\) error while not being observed on the training set. We proved that we can relax the \(L^{2}\) accuracy requirement for diffusion model to the \((1-)\)-quantile error: For distribution \(p\), and functions \(f,g\), we say that

\[D_{p}^{}(f,g)_{x p}[\|f(x)-g(x) \|_{2}].\] (6)

We adapt  to show that learning the score in our new outlier-robust sense also suffices for sampling, if we have accurate score estimates at each relevant discretization time.

**Lemma 1.3**.: _Let \(q\) be a distribution over \(^{d}\) with second moment \(m_{2}^{2}\) between \(1/(d)\) and \((d)\). For any \(>0\), there exist \(N=(+^{2}}^{2})\) discretization times \(0=t_{0}<t_{1}<<t_{N}<T\) such that if the following holds for every \(k\{0,,N-1\}\):_

\[D_{q_{T-t_{k}}}^{/N}(_{T-t_{k}},s_{T-t_{k}})}}\]

_then DDPM can sample from a distribution that is within \((+})\) in \(\) distance to \(q_{}\) in \(N\) steps._

With this relaxed requirement, our main technical lemma shows that for each fixed time, a good \((1-)\)-quantile accuracy can be achieved with a small number of samples, independent of \(_{t}\).

**Lemma 1.4** (Main Lemma).: _In Setting 1.1, suppose assumptions **A1** and **A2** hold. By taking \(m\) i.i.d. samples from \(q_{0}\) to train score function \(_{t}\), when_

\[m>(}}) PD}{ ^{2}_{}}(}})),\]

_with probability \(1-_{}\), the score estimate \(_{t}\) satisfies_

\[D_{q_{t}}^{_{}}(_{t},s_{t})/ _{t}.\]

Combining Lemma 1.4 with Lemma 1.3 gives us Theorem 1.2. The proof is detailed in Appendix C.

## 2 Related Work

Score-based diffusion models were first introduced in  as a way to tractably sample from complex distributions using deep learning. Since then, many empirically validated techniqueshave been developed to improve the sample quality and performance of diffusion models . More recently, diffusion models have found several exciting applications, including medical imaging and compressed sensing , and text-to-image models like DALL-E 2  and Stable Diffusion .

Recently, a number of works have begun to develop a theoretical understanding of diffusion. Different aspects have been studied - the sample complexity of training with the score-matching objective , the number of steps needed to sample given accurate scores , and the relationship to more traditional methods such as maximum likelihood .

On the _training_ side,  showed that for distributions bounded by \(R\), the score-matching objective learns the score of \(q_{}\) in \(L^{2}\) using a number of samples that scales polynomially in \(\). On the other hand, for _sampling_ using the reverse SDE in (2),  showed that the number of steps to sample from \(q_{}\) scales polylogarithmically in \(\) given \(L^{2}\) approximations to the scores.

Our main contribution is to show that while learning the score in \(L^{2}\)_requires_ a number of samples that scales polynomially in \(\), the score-matching objective _does learn_ the score in a weaker sense with sample complexity depending only _polylogarithmically_ in \(\). Moreover, this weaker guarantee is sufficient to maintain the polylogarithmic dependence on \(\) on the number of steps to sample with \( m_{2}\) 2-Wasserstein error.

Our work, as well as , assumes the score can be accurately represented by a small function class such as neural networks. Another line of work examines what is possible for more general distributions . For example,  shows that for general subgaussian distributions, the scores can be learned to \(L^{2}\) error \(/_{t}\) with \((d/)^{O(d)}\) samples. Our approach avoids this exponential dependence, but assumes that neural networks can represent the score.

## 3 Proof Overview

In this section, we outline the proofs for two key lemmas: Lemma 1.4 in Section 3.1 and Lemma 1.3 in Section 3.2. The complete proofs for these lemmas are provided in Appendix A and Appendix B respectively.

### Training

We show that the score-matching objective (4) concentrates well enough that the ERM is close to the true minimizer. Prior work on sampling  shows that estimating the \(\)-smoothed score to \(L^{2}_{2}\) error of \(}{^{2}}\) suffices for sampling; our goal is to get something close to this with a sample complexity independent of \(\).

Background: Minimizing the true expectation gives the true score.In this section, we show that if we could compute the true expectation of the score matching objective, instead of just the empirical expectation, then the true score would be its minimizer. For a fixed \(t\), let \(=_{t}\) and \(p\) be the distribution of \(e^{-t}x\) for \(x q_{0}\). We can think of a joint distribution of \((y,x,z)\) where \(y p\) and \(z N(0,^{2}I_{d})\) are independent, and \(x=y+z\) is drawn according to \(q_{t}\). With this change of variables, the score matching objective used in (4) is

\[*{}_{x,z}[\|s(x)-} \|_{2}^{2}].\]

Because \(x=y+z\) for Gaussian \(z\), Tweedie's formula states that the true score \(s^{*}=s_{t}\) is given by

\[s^{*}(x)=*{}_{z|x}[}].\]

Define \(=s^{*}(x)-}\), so \(*{}[ x]=0\). Therefore for any \(x\),

\[l(s,x,z):=\|s(x)-}\|_{2}^{2}\] (7)\[=\|s(x)-s^{*}(x)\|^{2}+2 s(x)-s^{*}(x), +\|\|^{2}.\] (8)

The third term does not depend on \(s\), and so does not affect the minimizer of this loss function. Also, for every \(x\), the second term is zero on average over \((z x)\), so we have

\[*{arg\,min}_{s}*{}_{x,z}[l(s,x,z)]= *{arg\,min}_{s}*{}_{x,z}[\|s(x)-s^{*} (x)\|^{2}]\]

This shows that the score matching objective is indeed minimized by the true score. Moreover, an \(\)-approximate optimizer of \(l(s)\) will be close in \(L^{2}\), as needed by prior samplers.

Understanding the ERM.The algorithm chooses the score function \(s\) minimizing the empirical loss,

\[*{}}_{x,z}[l(s,x,z)] _{i=1}^{m}l(s,x_{i},z_{i})\] \[=*{}}_{x,z}[\|s(x)-s^{*}(x) \|^{2}+2 s(x)-s^{*}(x),+\|\|^{2}].\]

Again, the \(*{}}[\|\|^{2}]\) term is independent of \(s\), so it has no effect on the minimizer and we can drop it from the loss function. We thus define

\[l^{}(s,x,z)\|s(x)-s^{*}(x)\|^{2}+2 s(x)-s^{* }(x),\] (9)

that satisfies \(l^{}(s^{*},x,z)=0\) and \(*{}[l^{}(s,x,z)]=*{}[ \|s(x)-s^{*}(x)\|^{2}]\). Our goal is now to to show that

\[*{}}_{x,z}[l^{}(s,x,z)]>0\] (10)

for all candidate score functions \(s\) that are "far" from \(s^{*}\). This would ensure that the empirical minimizer of the score matching objective is not "far" from \(s^{*}\). To do this, we show that (10) is true with high probability for each individual \(s\), then take a union bound over a net.

Boundedness of \(\).Now, \(z N(0,^{2}I_{d})\) is technically unbounded, but is exponentially close to being bounded: \(\|z\|\) with overwhelming probability. So for the purpose of this proof overview, imagine that \(z\) were drawn from a distribution of bounded norm, i.e., \(\|z\| B\) always; the full proof (given in Appendix A) needs some exponentially small error terms to handle the tiny mass the Gaussian places outside this ball. Then, since \(=}-*{}_{z x}[}]\), we get \(\|\| 2B/\).

Warmup:\(*{poly}(R/)\).As a warmup, consider the setting of prior work : (1) \(\|x\| R\) always, so \(\|s^{*}(x)\|}\); and (2) we only optimize over candidate score functions \(s\) with value clipped to within \(O(})\), so \(\|s(x)-s^{*}(x)\|}\). With both these restrictions, then, \(\|l^{}(s,x,z)|}{^{4}}+}\). We can then apply a Chernoff bound to show concentration of \(l^{}\): for \(*{poly}(,,B,}})\) samples, with \(1-_{}\) probability we have

\[*{}}_{x,z}[l^{}(s,x,z)] *{}_{x,z}[l^{}(s,x,z)]-}\] \[=*{}[\|s(x)-s^{*}(x)\|^{2}]- }{^{2}}\]

which is greater than zero if \(*{}[\|s(x)-s^{*}(x)\|^{2}]>}{^{2}}\). Thus the ERM would reject each score function that is far in \(L^{2}\). However, as we show in Section 4, restrictions (1) and (2) are both necessary: the score matching ERM needs a polynomial dependence on both the distribution norm and the candidate score function values to learn in \(L^{2}\).

The main technical contribution of our paper is to avoid this polynomial dependence on \(1/\). To do so, we settle for rejecting score functions \(s\) that are far in our stronger distance measure \(D_{p}^{_{}}\), i.e., for which

\[*{}[\|s(x)-s^{*}(x)\|>/] _{}.\] (11)Approach.We want to show (10), which is a concentration over \(x\) and \(z\). Now, \(x\) is somewhat hard to control, because it depends on the unknown distribution, but \(z N(0,^{2}I_{d})\) is very well behaved. This motivates breaking up the expectation over \(x,z\) into an expectation over \(x\) and \(z x\). Following this approach, we could try to show that

\[}_{x,z}[l^{}(s,x,z)]}_{x}[_{ z x}[l^{}(s,x,z)]]\]

However, this is not possible to show; the problem is that this could be unbounded, since \(s(x)\) is an arbitrary neural network that could have extreme outliers. So, we instead show this is true with high probability if we clip the internal value, making it

\[A_{x}:=}_{x}[(_{z x}[l^{}(s,x,z)]}_{z},}{^{2}})]=}_{x}[(\|s(x) -s^{*}(x)\|^{2},}{^{2}})].\]

Note that \(A_{x}\) is a function of the empirical samples \(x\). If \(s\) is a score that we want to reject under (11), then we know that \(A_{x}\) is an empirical average of values that are at least \(}{^{2}}\) with probability \(_{}\). It therefore holds that, for \(m>O(}}}{_{}})\), we will with \(1-_{}\) probability over the samples \(x\) have

\[A_{x}_{}}{^{2}}.\] (12)

Concentration about the intermediate notion.Finally, we show that for _every_ set of samples \(x_{i}\) satisfying (12), we will have

\[}_{z x}[l^{}(s,x,z)]}{2}>0.\]

This then implies

\[}_{x,z}[l^{}(s,x,z)]}_{x}[}{2}]_{score}}{^{2}},\]

as needed. For each sample \(x\), we split our analysis of \(_{z x}[l(s,x,z)]\) into two cases:

Case 1: \(\|s(x)-s^{*}(x)\|>O()\).In this case, by Cauchy-Schwarz and the assumption that \(\|\| 2B/\),

\[l^{}(s,x,z)\|s(x)-s^{*}(x)\|^{2}-O() \|s(x)-s^{*}(x)\|}{^{2}}\]

so these \(x\) will contribute the maximum possible value to \(A_{x}\), regardless of \(z\) (in its bounded range).

Case 2: \(\|s(x)-s^{*}(x)\|<O()\).In this case, \(|l^{}(s,x,z)| B^{2}/^{2}\) and

\[}(l^{}(s,x,z))=4\,[ s(x)-s ^{*}(x),^{2}]}{^{2}}\|s(x)-s^{*}(x) \|^{2}\]

so for these \(x\), as a distribution over \(z\), \(l^{}\) is bounded with bounded variance.

In either case, the contribution to \(A_{x}\) is bounded with bounded variance; this lets us apply Bernstein's inequality to show, if \(m>O(}}{^{2}A}) O( }}}{^{2}_{score}}),\) for every \(x\) we will have

\[}_{z}[l^{}(s,x,z)]>0\]

with \(1-_{}\) probability.

Conclusion.Suppose \(m>O(}}}{^{2}_{ }})\). Then with \(1-_{}\) probability we will have (12); and conditioned on this, with \(1-_{}\) probability we will have \(}_{x,z}[l^{}(s,x,z)]>0\). Hence this \(m\) suffices to distinguish any candidate score \(s\) that is far from \(s^{*}\). For finite hypothesis classes we can take the union bound, incurring a \(||\) loss (this is given as Theorem A.2). Lemma 1.4 follows from applying this to a net over neural networks, which has size \( H PD\).

### Sampling

Now we overview the proof of Lemma 1.3, i.e., why having an \(/_{T-t_{k}}\) accuracy in the \(D_{q}^{}\) sense is sufficient for accurate sampling.

To practically implement the reverse SDE in (2), we discretize this process into \(N\) steps and choose a sequence of times \(0=t_{0}<t_{1}<<t_{N}<T\). At each discretization time \(t_{k}\), we use our score estimates \(_{t_{k}}\) and proceed with an _approximate_ reverse SDE using our score estimates, given by the following. For \(t[t_{k},t_{k+1}]\),

\[x_{T-t}=(x_{T-t}+2_{T-t_{k}}(x_{T-t_{k}}))\,t+ \,B_{t}, x_{T}(0,I_{d}).\] (13)

This is _almost_ the reverse SDE that would give exactly correct samples, with two sources of error: it starts at \((0,I_{d})\) rather than \(q_{T}\), and it uses \(_{T-t_{k}}\) rather than the \(s_{T-t}\). The first error is negligible, since \(q_{T}\) is \(e^{-T}\)-close to \((0,I_{d})\). But how much error does the switch from \(s\) to \(\) introduce?

Let \(Q\) be the law of the reverse SDE using \(s\), and let \(\) be the law of (13). Then Girsanov's theorem states that the distance between \(Q\) and \(\) is defined by the \(L^{2}\) error of the score approximations:

\[(Q) =_{k=0}^{N-1}*{}_{Q}[_{T-t_{ k+1}}^{T-t_{k}}\|s_{T-t}(x_{T-t})-_{T-t_{k}}(x_{T-t_{k}})\|^{2}]\] \[_{k=0}^{N-1}*{}_{x q_{T-t_ {k}}}[\|s_{T-t_{k}}(x)-_{T-t_{k}}(x)\|^{2}](t_{k+1}-t_{ k})\] (14)

The first line is an _equality_. The second line comes from approximating \(x_{T-t}\) by \(x_{T-t_{k}}\), which for small time steps is quite accurate. So in previous work, good \(L^{2}\) approximations to the score mean (14) is small, and hence \((Q)\) is small. In our setting, where we _cannot_ guarantee a good \(L^{2}\) approximation, Girsanov actually implies that we _cannot_ guarantee that \((Q)\) is small.

However, since we finally hope to show closeness in \(\), we can circumvent the above as follows. We define an event \(E\) to be the event that the score is bounded well at all time steps \(x_{T-t_{k}}\), i.e.,

\[E:=_{k\{1,,N\}}(\|_{T-t_{k}}(x_{T-t_{k}})-s _{T-t_{k}}(x_{T-t_{k}})\|}}).\]

If we have a \(D_{q_{t_{k}}}^{/N}\) accuracy for each score, we have \(1-*{}[E]\). Therefore, if we look at the \(\) error between \(Q\) and \(\), instead of bounding \((Q)\). The \(\) between \(Q\) and \(\) can be then bounded by

\[(Q,)(1-*{}[E])+ ((Q E),( E)).\]

The second term \(((Q E),( E))\) is bounded because after conditioning on \(E\), the score error is always bounded, so now we can use (14) to bound the KL divergence between \(Q\) and \(\), then use Pinsker's inequality to translate KL into TV distance.

## 4 Hardness of Learning in \(L^{2}\)

In this section, we give concrete examples where it is difficult to learn the score in \(L^{2}\), even though learning to sufficient accuracy for sampling is possible. Previous works, such as , require the \(L^{2}\) error of the score estimate \(s_{t}\) to be bounded by \(/_{t}\). We demonstrate that achieving this guarantee is prohibitively expensive: sampling from a \(_{t}\)-smoothed distribution requires at least \((1/_{t})\) samples. Thus, sampling from a distribution \(\)-close in 2-Wasserstein to \(q_{0}\) requires polynomially many samples in \(\).

To show this, we demonstrate two lower bound instances. Both of these instances provide a pair of distributions that are hard to distinguish in \(L^{2}\), and emphasize different aspects of this hardness:

1. The first instance shows that even with a polynomially bounded set of distributions, it is _information theoretically impossible_ to learn a score with small \(L^{2}\) error with high probability, with fewer than \((1/)\) samples.

2. In the second instance, we show that even with a simple true distribution, such as a single Gaussian, distinguishing the score matching loss of the true score function from one with high \(L^{2}\) error can be challenging with fewer than \((1/)\) samples if the hypothesis class is large, such as with neural networks.

Information theoretically indistinguishable distributions:For our first example, consider the two distributions \((1-)(0,^{2})+( R,^{2})\), where \(R\) is polynomially large. Even though these distributions are polynomially bounded, it is _impossible_ to distinguish these in \(L^{2}\) given significantly fewer than \(\) samples. However, the \(L^{2}\) error in score incurred from picking the score of the wrong distribution is large. In Figure 1, the rightmost plot shows a simulation of this example, and demonstrates that the \(L^{2}\) error remains large even after many samples are taken. Formally, we have:

**Lemma 4.1**.: _Let \(R\) be sufficiently larger than \(\). Let \(p_{1}\) be the distribution \((1-)(0,^{2})+(-R,^{2})\) with corresponding score function \(s_{1}\), and let \(p_{2}\) be \((1-)(0,^{2})+(R,^{2})\) with score \(s_{2}\), such that \(=^{2}}{R^{2}}\). Then, given \(m<}{z^{2}^{2}}\) samples from either distribution, it is impossible to distinguish between \(p_{1}\) and \(p_{2}\) with probability larger than \(1/2+o_{m}(1)\). But,_

\[*{}_{x p_{1}}\|s_{1}(x)-s_{2}(x)\|^{2} }{^{2}} *{}_{x p_{2}}\|s_{1}(x)-s_{2}(x)\|^{2} }{^{2}}.\]

Simple true distribution:Now, consider the true distribution being \(N(0,^{2})\), and, for large \(S\), let \(\) be the score of the mixture distribution \((0,^{2})+(1-)(S,^{2})\), as in Figure 2. This score will have practically the same score matching objective as the true score for the given samples with high probability, as shown in Figure 2, since all \(m\) samples will occur in the region where the two scores are nearly identical. However, the squared \(L^{2}\) error incurred from picking the wrong score function \(\) is large We formally show this result in the following lemma:

**Lemma 4.2**.: _Let \(S\) be sufficiently large. Consider the distribution \(=(0,^{2})+(1-)(S,^{2})\) for \(=}{2}+10 S}}{10}\), and let \(\) be its score function. Given \(m\) samples from the standard Gaussian \(p^{*}=(0,^{2})\) with score function \(s^{*}\), with probability at least \(1-\),_

\[}[\|(x)-s^{*}(x)\|^{2}]}e^{-O(S)}*{}_{x p^{*}}\|(x)-s^{*}(x)\|^{2}}{m^{4}}.\]

Together, these examples show that even with reasonably bounded or well-behaved distributions, it is difficult to learn the score in \(L^{2}\) with fewer than \((R/)\) samples, motivating our \((1-)\)-quantile error measure.

## 5 Conclusion and Future Work

In this work, we have addressed the sample complexity of training the scores in diffusion models. We showed that a neural networks, when trained using the standard score matching objective, can be used for DDPM sampling after \((PD}{e^{3}}^{3})\) training samples. This is an exponentially better dependence on the neural network depth \(D\) and Wasserstein error \(\) than given by prior work. To achieve this, we introduced a more robust measure, the \(1-\) quantile error, which allows for efficient training with \(()\) samples using score matching. By using this measure, we showed that standard training (by score matching) and sampling (by the reverse SDE) algorithms achieve our new bound.

One caveat is that our results, as well as those of the prior work  focus on understanding the _statistical_ performance of the score matching objective: we show that the _empirical minimizer_ of the score matching objective over the class of ReLU networks approximates the score accurately. We do not analyze the performance of Stochastic Gradient Descent (SGD), commonly used to _approximate_ this empirical minimizer in practice. Understanding why SGD over the class of neural networks performs well is perhaps the biggest problem in theoretical machine learning, and we do not address it here.