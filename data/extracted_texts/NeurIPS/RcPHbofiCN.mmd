# Mixture of In-Context Experts

Enhance LLMs' Long Context Awareness

 Hongzhan Lin\({}^{1}\)  Ang Lv\({}^{1}\)   Yuhan Chen\({}^{2}\)

Chen Zhu\({}^{3}\)   Yang Song\({}^{4}\)\({}^{}\)   Hengshu Zhu\({}^{3}\)   Rui Yan\({}^{1}\)

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) XiaoMi AI Lab \({}^{3}\) Career Science Lab, BOSS Zhipin \({}^{4}\) NLP Center, BOSS Zhipin

{linhongzhan, anglv, ruiyan}@ruc.edu.cn

{chenyuhan5}@xiaomi.com

Equal contribution. Hongzhan Lin and Ang Lv proposed the idea of MoICE. Hongzhan Lin and Yuhan Chen designed the MoICE router architecture and implemented efficient code. Experiments were conducted by Hongzhan Lin, while Ang Lv led the writing. Code is available at https://github.com/plinksnow/MoICE.Corresponding authors: Rui Yan (ruiyan@ruc.edu.cn) and Yang Song (songyang@kanzhum.com)

###### Abstract

Many studies have revealed that large language models (LLMs) exhibit uneven awareness of different contextual positions. Their limited context awareness can lead to overlooking critical information and subsequent task failures. While several approaches have been proposed to enhance LLMs' context awareness, achieving both effectiveness and efficiency remains challenging. In this paper, for LLMs utilizing RoPE as position embeddings, we introduce a novel method called "Mixture of In-Context Experts" (MoICE) to address this challenge. MoICE comprises two key components: a router integrated into each attention head within LLMs and a lightweight router-only training optimization strategy: (1) MoICE views each RoPE angle as an 'in-context' expert, demonstrated to be capable of directing the attention of a head to specific contextual positions. Consequently, each attention head flexibly processes tokens using multiple RoPE angles dynamically selected by the router to attend to the needed positions. This approach mitigates the risk of overlooking essential contextual information. (2) The router-only training strategy entails freezing LLM parameters and exclusively updating routers for only a few steps. When applied to open-source LLMs including Llama, Mistral and Qwen, MoICE surpasses prior methods across multiple tasks on long context understanding and generation, all while maintaining commendable inference efficiency. Moreover, we also demonstrate the effectiveness of MoICE in pre-training a language model from scratch.

## 1 Introduction

Although large language models (LLMs) have demonstrated impressive capabilities across diverse NLP tasks, several studies  have pointed out that the contextual awareness of LLMs is not as powerful as widely believed, constraining their application in tasks demanding extensive contextual awareness, such as in-context learning , coherent long text generation  and Retrieval-Augmented Generation (RAG, ) tasks necessitating in-context retrieval . Liu et al.  identified a common issue termed the "lost-in-middle" phenomenon, indicating that LLMs often exhibit a weaker awareness of information situated in the middle of the long context compared to the beginning or end. Chen et al.  highlighted challenges arising from a mathematical property of RoPE , a wide-used positional embedding in LLMs, which impedes attention tospecific positions within the long context. Consequently, if critical information coincides with such positions, task performance suffers.

Many works [23; 49; 8; 48] have attempted to enhance the long-context awareness of LLMs. Central to these efforts is the enhancement of attention heads which serve as the linchpin for contextual awareness, given that FFNs in language models do not introduce token interaction. Chen et al.  proposed an inference algorithm named _Attention Buckets_ (AB), which enhanced the context awareness of LLMs by executing \(N\) inference instances, each with a distinct RoPE angle, and aggregated the outputs at the final layer. Zhang et al.  observed the varying awareness of attention heads to contextual positions. They proposed an inference algorithm named _Ms-PoE_. Ms-PoE enhances the utility of position-aware heads by re-scaling the positional embedding indices, equivalent to assigning each head a unique RoPE angle. Figure 1 illustrates these approaches. However, these approaches each come with their own drawbacks: AB conducts excessive redundant FFNs calculations, leading to high memory consumption. In Ms-PoE, determining a distinct re-scale factor for every attention head needs an additional forward pass. Meanwhile, each attention head still depends on a single re-scaled static RoPE. As highlighted by AB , this leads to limited awareness of certain contextual positions, thereby constraining its potential. Moreover, a significant drawback of both AB and Ms-PoE lies in their static assignment of the RoPE angle for each attention head throughout the generation. However, as the generation progresses, the positions of crucial tokens shift, necessitating corresponding adjustments in the required RoPE angles for each head.

In this study, we present _Mixture of In-Context Experts_ (MoICE), a novel plug-in of LLMs for enhancing context awareness. Specifically, We conceptualize a unique RoPE angle as an "in-context expert," as it can allocate a head's more attention to certain contextual positions . We integrate a router within each attention head, which discerns the potentially important tokens for the head and dynamically selects \(K\) RoPE angles that provide comprehensive awareness of these tokens for attention computation. Through the re-computation of only a few query-key dot products, attention patterns computed with selected RoPE angles are aggregated to produce the final attention pattern. This approach yields two primary advantages: (1) It eliminates unnecessary computational overhead in AB, enhancing efficiency. (2) The dynamic expert selection of each head for arbitrary tokens introduces flexibility not attained in previous studies. This minimizes the risk of the initial RoPE angle assigned to a head failing to work due to crucial token positions shifting during generation.

Consequently, MoICE not only surpasses AB's effectiveness but also achieves commendable efficiency. We name our approach as "Mixture of In-Context Experts" (MoICE) due to the aggregation of attention patterns calculated with different RoPE angles resembling the concept of "Mixture of Experts" (MoE, ). When applying MoICE to open-source LLMs, we freeze LLMs' parameters

Figure 1: Some methods developed to enhance LLMs’ context awareness. (a) Attention Buckets  selects \(N\) different RoPEs and conducts \(N\) parallel inferences for each input. The outputs are then aggregated in the final layer. (b) Ms-PoE  employs a unique RoPE angle for each attention head. However, it needs an additional forward pass for RoPE angle assignment. (c) MoICE integrates a router within each attention head. This novel plug-in selects several of the most suitable RoPE angles for each token. The selected RoPE angles collectively contribute to computing the attention scores. MoICE demonstrates superior memory efficiency and performance.

and conduct lightweight training only on the MoICE routers. With only a few quick updates, MoICE surpasses many competitive baselines in tasks involving long-context generation and understanding.

In summary, our main contribution is the introduction of MoICE, a novel plug-in for enhancing LLMs' context awareness. It achieves head-and token-specific dynamic multiple RoPE angles assignment, outperforms previous methods across various tasks, and maintains commendable inference efficiency.

## 2 Background

We introduce some background of _Mixture of In-Context Experts_, including (1) the rotary position embeddings commonly used by mainstream LLMs, (2) the primary problem addressed in this paper: the limited context awareness of LLMs, (3) an explanation of the underlying reasons for this limitation.

Position embeddingPositional embedding is crucial for Transformer  to perceive sequence order and compensate for the position-agnostic nature of the attention mechanism. In this paper, we mainly focus on LLMs using Rotary Position Embedding (RoPE, ) which is the prevalent position embedding in current LLMs. We discuss other position embeddings in Appendix D.

In a Transformer layer with \(H\) attention heads employing RoPE, where \(d\) represents the hidden state dimension of each attention head, let \(_{n}^{h}\) and \(_{m}^{h}\) denote the query vector at position \(n\) and key vector at position \(m\) in the \(h\)-th head. To encode position information, RoPE initially applies a rotary matrix to the query and key vectors:

\[}_{n}^{h}=_{_{j},n}_{n} ^{d},}_{m}^{h}=_{_{j},m} _{m}^{d},\] (1)

\[_{_{j},n}=[_{_{j, 0},n}&O&&O\\ O&_{_{j,1},n}&&O\\ &&&\\ O&O&&_{_{j,d/2-1},n}],\;\; \;_{_{j,i},n}=[ n_{j,i}& - n_{j,i}\\  n_{j,i}& n_{j,i}],O=[ []{cccc}0&0\\ 0&0].\] (2)

Here, \(_{j,i}=B_{j}^{-2i/d},i[0,,d/2-1]\), is termed as the rotary angle of RoPE, and \(B_{j}\) is typically a fixed base. The subscript \(j\) serves to differentiate various RoPE angles \(\), each associated with a distinct \(B_{j}\), a distinction necessary for discussions in Section 3. This approach effectively incorporates relative position information between \(m\) and \(n\) in the query-key product during attention computation:

\[}_{n}^{h}}_{m}^{h}=( _{_{j},n}_{n}^{h})^{}(_{_{j},m}_{m}^{h})=_{n}^{h} _{_{j},m-n}_{m}^{h},\] (3) \[_{nm}^{h}=(}_{n}^{h}}_{m}^{h}}{})= (_{n}^{h}_{_{j},m-n} _{m}^{h}}{}).\] (4)

Here, \(_{nm}^{h}\) denotes the attention score assigned by the \(h\)-th head at position \(n\) to position \(m\).

Context awareness of LLMsLLMs struggle with limited context awareness, significantly impacting their performance in tasks like long-text generation , Retrieval-Augmented Generation (RAG, [19; 6; 10; 40]), and multi-turn human-agent interactions  involving complex contexts. Liu et al.  identified a problem known as "Lost-in-the-Middle," where LLMs process the beginning and end of the context well but have reduced awareness of the middle. Chen et al.  observed that LLMs using RoPE exhibit uneven context awareness, favoring certain positions. Peysakhovich et al.  further highlighted that LLMs exhibit variable attention to document-level token segments based on their contextual positions. Lv et al.  observed that language models develop context awareness, especially in their ability to copy, through "grokking ." They suggest pre-training models with increased regularization to enhance this capability.

Figure 2: Different \(_{j}\) alter the upper bounds of attention scores between a token and its \(x\)-distance neighbors. Each angle is distinguished by its own base value \(B_{j}\).

Attention waveformsAccording to Chen et al. , LLM's uneven awareness of different contextual positions is due to RoPE's mathematical characteristics. Within RoPE, the attention score exhibits "waveforms" when retrieving the same token from the context, based on their relative positions. The troughs in these waveforms can impair task performance, especially when critical tokens are situated at these positions during generation. Different RoPE angles produce waveforms with troughs occurring at different positions. These phenomena are depicted in Figure 2. A detailed derivation of the depicted curves in Figure 2 is provided in Appendix B.

## 3 Mixture of In-Context Experts

In this section, we first introduce the core component of MoICE, the MoICE router, detailed in Section 3.1. Subsequently, we delve into the optimization of MoICE in Section 3.2. Figure 3 provides an overview of MoICE. The discussion in this section focuses solely on a single layer of transformer for clarity, with the same principles applying to any other layers.

### Architecture

We aim to design an enhanced attention mechanism in LLMs that dynamically attends to crucial information across various contextual positions required for completing the head's function. As a result, we can mitigate the performance drop caused by inadequate context awareness. Motivated by insights of Chen et al. , who demonstrated that a distinct RoPE angle \(_{j}\) could direct the attention heads more focus on specific contextual positions, we propose the integration of a contextual-aware routing mechanism. This routing mechanism is designed to select the appropriate RoPE angles for processing a token. We implement the router as a Multi-Layer Perceptron (MLP) with the SiLU activation function:

\[():=_{3}( (_{1})(_{2} )).\] (5)

Here, \(\) is the query vector that encapsulates the contextual information for the task. This router input indicates the specific information for which the current token is "quering." \(_{1},_{2}^{N d}\), and \(_{3}^{N N}\) are weight matrices, where \(N\) denotes the number of the number of RoPE angle candidates. Considering each head's distinct function , we integrate a router into every attention head in the LLM. Notably, a router's decision is independent of other heads and dynamic to the context.

As defined in Eq. 5, the router outputs an \(N\)-tuple distribution, indicating the weight it allocates for each RoPE angle. In each step in generation, the router selects \(K\) angles from a set of \(N\) angles \(\{_{1},_{2},\....\,_{N}\}\) for attention computation. Specifically, we first identify the \(K\) RoPE angles with the highest routing weights and normalize their relative weights using the Softmax function, resulting in \(_{n}^{h}^{K}\), representing the probability distribution over the selected RoPE angles within the \(h\)-th head:

\[_{n}^{h}=( (_{n}^{h}))[:K],\\ _{n}^{h}=(( _{n}^{h})[_{n}^{h}]),\] (6)

where \(_{n}^{h}\) represents the query at position \(n\) within the \(h\)-th attention head in a Transformer layer. Subsequently, we aggregate the attention scores computed with these chosen \(K\) in-context experts based on their routing weights to derive the final attention scores for head \(h\) from position \(n\) to position \(m\):

\[_{nm}^{h}=_{j_{n}^{h}}_{n}^{ h}[j](_{n}^{h}_{ _{j},m-n}_{m}^{h}}{}).\] (7)

Figure 3: The structure of MoICE. Only the router’s parameters are trainable when plugged into an LLM. For clarity, the figure illustrates a single head, with \(N\)=3 and \(K\)=2 as toy demonstration examples.

Considering RoPE angles impact how attention heads allocate attention and focus on specific contextual positions, we view each distinct RoPE angle as an in-context expert, in contrast to traditional in-weight experts [16; 21; 12; 18], where the experts are learnable parameter weights. Given that these in-context experts together augment LLMs' context awareness, we term this method Mixture of In-Context Experts (MoICE, Section 3). Figure 3 illustrates the overview of MoICE. Our proposed MoICE has three major advantages:

(1) We only add additional computational overhead to the query-key dot products, resulting in a minimal increase in memory usage and a negligible impact on inference speed (Section 4.2).

(2) MoICE dynamically selects suitable RoPE angles token-wise and head-wise, offering unprecedented flexibility and unlocking the full potential of each attention head.

(3) Concerning LLMs' context awareness enhancement, MoICE addresses a longstanding issue: the relative position of the relevant information will shift during generation, leading to previous static modification of the attention heads [8; 47] will be sub-optimal during practical generation. The contextual-aware dynamic routing in MoICE is not bothered by this issue.

### Router-only training

To train the newly incorporated MoICE routers in LLMs, the most straightforward way is to simultaneously update the LLMs' parameters alongside the routers. However, updating the original LLMs' parameters can result in catastrophic forgetting. Therefore, we propose a more effective and efficient strategy, the router-only training strategy, which freezes the LLMs' parameters and solely optimizing the routers.

Given an input sequence, we calculate the negative log-likelihood loss (\(_{nll}\)) for language modeling. During backward propagation, only the parameters of MoICE routers are updated. To mitigate the possibility of a router favoring specific experts disproportionately [16; 44], we incorporate an auxiliary loss \(_{aux}\) following . An ablation study on this auxiliary loss is in Table 11. Given an input of \(T\) tokens and \(N\) experts, we calculate the \(_{lb}\) by the scaled dot-product between frequency vector \(\) and probability vector \(\):

\[_{aux}= N_{j=1}^{N}_{j}_{j}.\] (8)

Eq. 8 avoids the router falling into a sub-optimal solution favoring specific experts overwhelmingly, as its minimal is achieved when the routing probability is uniform. Here, \(\) is the weighting factor for load balancing loss. \(_{j}\) denotes the proportion of the \(j\)-th expert selected across all positions and attention heads, while \(}_{j}\) denotes the proportion of router weight assigned to expert \(j\):

\[_{j}&= _{t=1}^{T}_{h=1}^{H}\{j_{t}^{h}\},\\ _{j}&=_{t=1}^{T} _{h=1}^{H}\{j_{t}^{h}\}_{t}^{h }[j].\] (9)

Our overall training objective is to minimize the following loss:

\[=_{nll}+_{aux}.\] (10)

## 4 Experiment

### Setup

To evaluate the efficacy of MoICE, we implement it with open-source LLMs, which we will introduce later, and conduct lightweight training of MoICE routers on a small and general dataset. Subsequently, we evaluate the enhanced LLM's capability to zero-shot undertake multiple tasks in long context understanding and generation, as detailed in Section 4.2 and Section 4.3.

Training dataWe use a training dataset3 which extracts the one thousand longest entries from OpenHermes . OpenHermes is a multi-source integrated dataset containing high-quality synthetically generated instruction and chat samples. A detailed analysis of other training data is in Section 5.3.

Hyperparameters for MoICE-router-only trainingWe froze all the original parameters of the open-source LLMs we used and only trained the MoICE router. Following Attention Buckets , we employed the RoPE angle set of \(N=7\) items, each assigned with base values as follows: \(\{1.0 10^{4},1.75 10^{4},1.8 10^{4},1.9 10^{4},2.0  10^{4},2.25 10^{4},2.5 10^{4}\}\) for Llama2-7B and Mistral-7B, \(\{1.0 10^{6},1.25 10^{6},1.4 10^{6},1.8 10^{6},1.9  10^{6},2.25 10^{6},2.5 10^{6}\}\) for Qwen1.5-7B. By default, unless otherwise specified, the attention head selects \(K\)=7 bases to ensure a fair comparison with . Section 4.2 introduces our baselines in detail. Section 5 delves into the impact of set size and the number of selected items.

We implement a warm-up strategy comprising 20% of the total steps, with a maximum learning rate of 0.0001. The batch size is 128. \(\) is set as 0.3. We train the MoICE routers for 1 epoch (about 8 minutes) on four A800-80G GPUs.

### Long context understanding and generation

Following the L-Eval benchmark , we evaluated the LLM with tasks categorized into two main groups: closed-ended and open-ended tasks. Closed-ended tasks primarily focus on the capacity for understanding and reasoning within long contexts, including tasks like multiple-choice questions from QuALITY , Coursera, 4 TOEFL , and True/False question answering from SFiction. 5 On the other hand, open-ended tasks include summarization generation and open-format question-answering tasks, requiring extracting information from lengthy in-context documents. The open-ended tasks comprise a subset of 181 questions drawn from 29 diverse long documents.

Baselines and open-source LLMsIn evaluating the efficacy of our proposed MoICE, we compare it against several state-of-the-art methods known for enhancing the capacity of LLMs to understand and generate long contexts. These baselines include two context extrapolation techniques: Positional Interpolation (PI, ) and Dynamic NTK . Additionally, we consider two inference algorithms for context-awareness enhancement: Ms-PoE  and Attention Buckets .

We evaluate all these methods alongside our MoICE on three representative open-source LLMs that utilize RoPE for positional embeddings: Llama2-7B-Chat , Mistral-7B-Instruct-v0.1  and Qwen1.5-7B-Chat . Llama2-7B and Qwen1.5-7B support a pre-trained context length of 4,096 and 32,768, respectively. Mistral-7B employs a sliding window attention (SWA) mechanism with a window size of 4,096 tokens, enabling it to accommodate longer contexts than the default. Therefore, we conduct experiments with a context length of 8,192 on Mistral-7B, using SWA as the exclusive baseline for comparison. For PI and Dynamic NTK, we apply a scaling ratio of 1.5, while for the remaining baselines, we adhere to the hyperparameters specified in their original papers. All methods are tested on a single A800-80G GPU, except for applying AB to Mistral-7B-8k, which needs 2 GPUs due to substantial memory requirements.

Evaluation metricsWe adopt the exact match for closed-ended tasks. For open-ended tasks, we employ _GPT-4-Turbo_ as the judge to evaluate the effectiveness of various enhancement methods on open-source LLMs. This evaluation compares their performance against _GPT3.5-Turbo-16k-0613_ across 181 questions.

Results and analysisWe report our experimental results in Table 1. MoICE significantly enhances the overall performance of Llama-2-7B-chat (with p-value < 0.02 in the t-test) in both closed-ended and open-ended tasks. On Mistral, MoICE outperforms all baseline models significantly (p-value < 0.02). We also report the mean and standard deviation of MoICE in Table 10. Standard fine-tuning degrades the performance of original LLMs, demonstrating catastrophic forgetting and proving that the improvement of MoICE does not stem from more training. These results underscore MoICE'sefficacy in enhancing LLMs' ability to understand and generate long contexts, both of which require high context awareness. Furthermore, these results underscore the broad applicability of MoICE across different LLMs.

Regarding efficiency, we provide practical inference time and memory costs associated with AB, Ms-PoE, and MoICE in Table 2. For a fair comparison, we utilize Flash Attention 2  across all approaches. While achieving superior overall performance, MoICE remains at an inference speed similar to Ms-PoE and notably excels in memory efficiency compared to these two baselines.

We also perform further experiments on one additional long context benchmark LongBench , which are detailed in Appendix A.

### Retrieval-augmented generation (RAG)

Retrieval-augmented generation (RAG) tasks involve retrieving numerous documents related to the current generation. The retrieved documents are arranged in the context. RAG necessitates that LLMs have robust context awareness to pinpoint crucial documents, process the retrieved information effectively, and integrate it to generate responses.

Following [8; 49], we employ the MDQA task to evaluate the efficacy of MoICE in enhancing LLMs' performance in RAG tasks. Meanwhile, MDQA offers the bonus of allowing flexible control over

    &  &  \\   & **Coursera** & **QuALITY** & **TOEFL** & **Sfiction** & **Average** & **wins** & **ties** & **win-rate\%\({}^{*}\)** \\  Llama2-7B-Chat  & 36.77 & 38.12 & 55.02 & 60.16 & 47.52 & 68 & 117 & 34.94 \\  + Fine-tuning & 32.85 & 30.20 & 51.30 & 59.38 & 43.43 & 65 & 91 & 30.52 \\ + PI  & 38.23 & 38.61 & 56.51 & 61.72 & 48.77 & 76 & 112 & 36.46 \\ + Dynamic NTK  & 40.26 & 39.11 & 55.76 & 62.50 & 49.41 & 82 & 112 & 38.12 \\ + Ms-PoE  & 39.24 & 40.10 & 55.76 & 63.28 & 49.60 & 86 & 110 & 38.95 \\ + AB  & **40.41** & 41.09 & **56.88** & 61.72 & 50.02 & 85 & 114 & 39.23 \\ + MoICE (Ours) & 39.83 & **42.08** & 56.13 & **64.84** & **50.72** & **89** & **118** & **40.88** \\    & 44.06 & 62.08 & 61.72 & 53.27 & 71 & 105 & 34.11 \\  + Fine-tuning & 25.29 & 26.73 & 25.65 & 50.00 & 31.92 & 53 & 85 & 26.38 \\ + SWA & 44.77 & 42.57 & 62.08 & 60.94 & 52.59 & 73 & 89 & 32.45 \\ + PI  & 44.19 & 44.06 & **64.68** & 62.50 & 53.86 & 73 & 96 & 33.43 \\ + Dynamic NTK  & 45.35 & 42.08 & 62.08 & **63.28** & 53.20 & 78 & 103 & 35.77 \\ + Ms-PoE  & 46.37 & 45.05 & 61.34 & 57.03 & 52.45 & 84 & 106 & 37.84 \\ + AB  & 46.08 & 42.57 & 62.08 & 62.50 & 53.31 & **87** & 110 & 39.22 \\ + MoICE (Ours) & **47.82** & **46.53** & **64.68** & 62.50 & **55.38** & 85 & **117** & **39.36** \\    & **78.44** & 61.88 & 61.19 & 69.53 & 67.76 & 83 & **119** & 40.83 \\ + PI  & 76.58 & 61.88 & 60.32 & 70.31 & 67.27 & 83 & 107 & 39.11 \\ + Dynamic NTK  & 78.07 & 62.38 & 60.32 & 70.31 & 67.77 & 84 & 111 & 40.20 \\ + Ms-PoE  & 75.47 & 60.89 & 60.47 & **71.88** & 67.18 & OOM & OOM & N/A \\ + AB  & **78.44** & OOM & OOM & OOM & N/A & OOM & OOM & N/A \\ + MoICE (Ours) & **78.44** & **62.87** & **61.77** & 71.09 & **68.54** & **91** & 105 & **41.59** \\  ^{*}\) Following , win-rate = (win counts + 0.5 * tie counts)} \\ 

Table 1: Experimental results on the L-Eval Benchmark . Applying to various models, MoICE demonstrate superior performance compared to previous competitive approaches. We emphasize the highest score in bold.

  
**Method** & **Coursera**\(\) & **QuALITY**\(\) & **TOEFL**\(\) & **Sfiction**\(\) & **Open-Ended**\(\) & **Average**\(\) \\  AB  & 10.9 / 78.7 & 18.1 / 62.5 & 19.9 / 56.5 & 5.0 / 33.2 & 45.9 / 78.2 & 20.0 / 61.8 \\ Ms-PoE  & 4.1 / 27.2 & 6.0 / 27.8 & 6.7 / 28.6 & 6.0 / 27.8 & 20.2 / 28.9 & 8.6 / 28.1 \\ MoICE (Ours) & 5.0 / 19.6 & 11.0 / 19.7 & 10.2 / 19.5 & 1.6 / 15.2 & 34.2 / 23.2 & 12.4 / 19.4 \\   AB  & OOM & OOM & 37.2 / 71.4 & OOM & OOM & N/A \\ Ms-PoE  & 14.1 / 50.3 & 11.2 / 48.4 & 9.8 / 25.4 & 4.5 / 50.3 & 72.8 / 62.4 & 22.5 / 47.4 \\ MoICE (Ours) & 13.4 / 25.7 & 7.7 / 22.9 & 11.3 / 20.4 & 2.3 / 22.8 & 77.8 / 29.3 & 22.5 / 24.2 \\   

Table 2: Practical inference time (in minutes) / GPU memory costs (GB) on a single A800-80G GPU for each method applied to Llama2-7B-Chat (top) and Mistral-7B-Instruct-8k (bottom), respectively. Due to out-of-memory issues, AB can not accomplish many tasks, denoted as OOM in the table.

the location of documents, enabling a more precise evaluation of LLMs' context awareness across various contextual positions.

Our MDQA experiments leverage a subset of NaturalQuestions-Open [25; 24], consisting of 2,655 queries, following [49; 27]. Each query is paired with a context consisting of 10 or 30 documents (with an average of 1,722 or 5,046 tokens), depending on the model (Llama-2-7B-chat or Mistral-7B-Instruct-8k), tasked with answering based on this contextual information. Only one document among these comprises useful information for the given query. We compare Ms-PoE, AB, and MoICE, testing each method through 5 iterations. For Llama, the relevant document is positioned 1st, 3rd, 5th, 7th, and 10th within the context, while for Mistral, it is positioned 1st, 8th, 15th, 23rd, and 30th, respectively.

In Table 3, MoICE on Llama demonstrates the highest average performance across most positions, showcasing its remarkable stability. Its accuracy scores show minimal variation, with only a marginal difference of 2.22 points between its highest and lowest values. On Mistral, MoICE exhibits significant average improvement (p-value < 0.02). Notably, when the relevant document is positioned at the end of the context, all methods on Llama exhibit a decrease compared to the original model, although MoICE shows a minimal decline. This phenomenon also happens in the Mistral model. We posit that this decline may stem from the original model predominantly directing attention towards nearest documents [27; 34]. However, as approaches enhance awareness of various contextual positions, the model's attention to the nearest documents is diffused by other positions, as its overall capacity for context awareness is constant and limited. Nevertheless, MoICE consistently emerges as the superior-performing method overall across language models.

## 5 Method analysis

In this section, we delve into a comprehensive analysis of the properties of MoICE. We illustrate how \(N\), the total number of in-context experts (Section 5.1), and \(K\), the specific number of selected in-context experts (Section 5.2), influence MoICE. We further demonstrate that MoICE is robust to training data (Section 5.3) Additionally, we present a case study demonstrating the dynamic selection of in-context experts for tokens during generation (Section 5.4). Finally, we verify the effectiveness of language model with MoICE architecture in pretraining stage (Section 5.5).

### The effect of expert total numbers \(N\)

We investigate the impact of the total number of experts. Employing the search algorithm proposed by Chen et al. , we obtain various sets of different sizes, each comprising complementary base values. The searched expert sets are detailed in Appendix E. We apply MoICE to Llama-2-7B-chat and test the model on L-Eval tasks. The results are presented in Table 4. The results of the original Llama are denoted as (\(N\)=1) in the table. As the table illustrates, MoICE demonstrates improvement to LLMs' context awareness with increasing \(N\), with noticeable improvements even when \(N\) is as small as 3. However, as \(N\) reaches 9, the average performance is close to \(N\)=7, indicating a performance plateau. This suggests that having \(N\)=7 experts is sufficient for general usage.

  
**Method** & **1** & **3** & **5** & **7** & **10** & **Gap** & **Avg.** \\  Llama2-7B-Chat & 64.14 & 65.95 & 64.97 & 62.67 & **67.53** & 4.86 & 65.05 \\  + Ms-PoE  & 66.06 & 64.29 & 63.99 & 62.22 & 64.75 & 3.84 & 64.34 \\ + AB  & **66.36** & 66.14 & 65.25 & 63.20 & 64.93 & 3.16 & 65.18 \\ + MoICE (Ours) & 65.50 & **66.33** & **65.61** & **64.11** & 65.84 & **2.22** & **65.48** \\  
**Method** & **1** & **8** & **15** & **23** & **30** & **Gap** & **Avg.** \\  Mistral-7B-Instruct-8k & 58.38 & 47.42 & 46.97 & 49.68 & 50.81 & **11.41** & 50.65 \\  + Ms-PoE  & 52.76 & 41.24 & 42.80 & 42.90 & 43.58 & 11.52 & 44.66 \\ + AB  & 58.57 & 47.57 & 47.12 & 49.83 & **50.96** & 11.45 & 50.81 \\ + MoICE (Ours) & **61.81** & **52.54** & **52.43** & **50.36** & 49.34 & 12.47 & **53.30** \\   

Table 3: The experiment results on the MDQA task. MoICE achieve superior average performance compared to previous competitive approaches. We emphasize the highest score in bold.

### The effect of selected experts number \(K\)

With a fixed number of experts (\(N\)=7), we examine the effect of different numbers of chosen experts \(K\) with values of 1, 3, 5, and 7. We consider two additional setups where 7 experts are selected with equal weights ("Equal Weights") and with random weights ("Random Weights" ), using Llama-2-7B-chat as the case study. The results are presented in Table 5. Setting \(K=1\) doesn't enhance or significantly degrade the model's performance, aligning with our assertion in the Introduction (Section 1): assigning a single and unique RoPE angle to each head inadequately explores the head's functionality. For \(K\) greater than 3, performance improvements become evident. This shows that the MoICE router in our method can select the appropriate combination of experts to better aware the context. Randomly selecting experts ruins the model's language modeling ability, leading to aberrant outputs.

### MoICE is robust to training data

We further analyze the impact of the data for training routers. We additionally use three instruction fine-tuning datasets from different sources: a self-instruct dataset, Airoboros ; and two datasets for LLM alignment with long context, Long-Alpaca , and LongAlign . The hyperparameters remain consistent as mentioned in Section 4. As presented in Table 6, MoICE exhibits almost identical scores when trained on different data, showcasing the robustness of our method.

### The visualization of dynamic routing states

We provide a case study exemplifying the dynamic routing mechanism within MoICE during text generation. Depicted in Figure 4 in the Appendix, the MoICE router of each head independently selects distinct experts. At each step of the generation process, these heads dynamically choose experts for each new token. This dynamic utilization of diverse RoPE angles within each attention

  
**Training Data** & **Coursera** & **QuALITY** & **TOEFL** & **SFiction** & **Avg.** \\  OpenHermes  & 39.83 & 42.08 & 56.13 & 64.84 & 50.72 \\ Airoboros  & 39.68 & 41.58 & 56.13 & 64.84 & 50.56 \\ Long-Alpaca  & 39.68 & 42.08 & 56.13 & 64.84 & 50.68 \\ LongAlign  & 39.68 & 41.58 & 56.13 & 64.84 & 50.56 \\   

Table 6: The improvement of context awareness of Llama-2-7B-chat by MoICE trained on various data.

  
**Method** & **Coursera** & **QuALITY** & **TOEFL** & **SFiction** & **Avg.** \\  Original (\(N\)=1) & 36.77 & 38.12 & 55.02 & 60.16 & 47.52 \\  \(N\)=3 & 37.65 & 40.10 & 55.76 & 62.50 & 49.00 \\ \(N\)=5 & 38.23 & 39.60 & **56.13** & 63.28 & 49.32 \\ \(N\)=7 & 39.83 & **42.08** & **56.13** & **64.84** & **50.72** \\ \(N\)=9 & **40.26** & 41.58 & **56.13** & **64.84** & 50.70 \\   

Table 4: The performance of Llama-2-7B-chat enhanced by MoICE with \(N\) in-context experts. We show results marked with color to emphasize the improvements over the original model.

  
**Method** & **Coursera** & **QuALITY** & **TOEFL** & **SFiction** & **Avg.** \\  Original (\(N\)=1) & 36.77 & 38.12 & 55.02 & 60.16 & 47.52 \\  \(K\)=1 & 35.03 & 35.64 & **56.51** & 61.72 & 47.22 \\ \(K\)=3 & **39.83** & 41.58 & 56.13 & **64.84** & 50.60 \\ \(K\)=5 & 38.52 & 39.60 & 56.13 & **64.84** & 49.77 \\ \(K\)=7 & **39.83** & **42.08** & 56.13 & **64.84** & **50.72** \\ Equal Weights & 36.48 & 38.12 & 53.90 & 61.72 & 47.56 \\ Random Weights & 15.55 & 28.71 & 21.75 & 8.59 & 18.65 \\   

Table 5: The improvement of context awareness of Llama-2-7B-chat by MoICE, wherein each head dynamically selects diverse \(K\) experts (\(N\)=7). We show results marked with color to emphasize the improvements over the original model.

head maximizes the potential of attention heads across various inputs, a capability not attained in prior research, including both Attention Buckets and Ms-PoE.

### Applying MoICE to the pretraining stage

We further evaluate the performance of a language model with MoICE architecture in pretraining stage. Specifically, we train a language model with a Llama architecture of 49M parameters, with and without MoICE respectively. We pretrain a small model from scratch and observe the effectiveness of MoICE. More experimental details can be found in Appendix C. We measure the model's context awareness on the Key-Value Retrieval  task, which uses multiple randomly generated key-value string pairs as prompts to evaluate the model's ability to extract the correct value corresponding to a given query from the context. One prompt example can be found in Figure 6.

From the results in Table 7, we can see that our model can significantly increase the contextual capabilities of the pretrained language model, which indicates the potential of scaling up our method in pretraining stage.

## 6 Conclusion

In this paper, we introduce a novel approach to enhancing the context awareness of LLMs termed _Mixture of In-Context Experts_ (MoICE). Through lightweight training, open-source LLMs such as Llama and Mistral, enhanced by MoICE, demonstrate improved context awareness. Across numerous tasks demanding substantial context awareness, MoICE-enhanced LLMs consistently outperform competitive baselines, all the while maintaining commendable efficiency. A distinctive feature of MoICE is that it first implements head- and token-specific RoPE angles assignment for attention heads, a pivotal factor contributing to its success. This paper underscores the need to address the inherent limitations in current LLMs and advocates for a thorough exploration of their existing capabilities.