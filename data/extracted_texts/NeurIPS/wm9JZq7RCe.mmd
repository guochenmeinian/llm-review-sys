# An Analysis of Tokenization: Transformers under Markov Data

Nived Rajaraman

UC Berkeley

nived.rajaraman@berkeley.edu &Jiantao Jiao

UC Berkeley

jiantao@berkeley.edu &Kannan Ramchandran

UC Berkeley

jiantao@berkeley.edu

###### Abstract

While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple \(k^{}\)-order Markov processes for \(k>1\), transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from \(k^{}\)-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.

## 1 Introduction

The training of language models is typically not an end-to-end process. Language models are often composed of a "tokenizer", which encodes a sequence of characters into a sequence of token ids, which map to substrings. The subsequent language modeling task is carried out by a neural network or transformer, which is pre-trained and fine-tuned on large datasets. The ideal goal is to jointly train the tokenizer and transformer with end-to-end accuracy as the objective. This is a challenging problem to solve efficiently, and thus, the tokenizer is generally adapted on a portion of the training dataset and frozen before the transformer is trained. In practice, byte-level/character level models such as ByT5 (Xue et al., 2022) and Canine(Clark et al., 2022) which avoid tokenization often perform worse for the reason that semantic relationships can be harder to capture at the character level (Libovicky et al., 2021; Itzhak and Levy, 2021).

Though used most commonly, tokenization at the subword level often has sharp edges. Test sequences may contain rare tokens which were never seen in the training dataset. The presence of such tokens may induce undesirable behavior in the outputs of models (Rumbelow and Watkins, 2023; Kharitonov et al., 2021; Yu et al., 2021) and present an attack surface for bad actors. Moreover, tokenized models struggle on tasks that involve manipulation at the character level, such as spelling out words or reversing sentences. For similar reasons, LLMs with standard tokenizers also struggle to carry out basic arithmetic (Golkar et al., 2023). Despite this brittleness, tokenization is used in nearly all state-of-the-art LLM architectures.

In this paper, we introduce a statistical formulation for tokenization for next-word-prediction. We study the class of models transformers are observed to express empirically under simple data generating processes, which often can have simpler descriptions. Taking a step back, rather than focusing on proxy evaluation metrics, which lead to an ever-changing goalpost, we focus on understanding the behavior of the end-to-end cross-entropy loss, \(()\). In this paper, we study a simplification of real world data generating processes and study the case where data sources are \(k^{}\)-order Markov processes. Within this framework we can compare tokenizers against each other, and in the process capture several interesting phenomena. Our main results are as follows,

1. There are very simple \(k^{}\)-order Markov processes such that in the absence of any tokenization, transformers trained on data drawn this source empirically predict characters according to a unigram model. This phenomenon is observed under a wide variety of hyperparameter choices. This is problematic because unigram models such as that induced by the stationary distribution are poor at modeling Markovian data and suffer from a high cross-entropy loss. This phenomenon was also recently observed in Makkuva et al. (2024).
2. When trained with tokenization, transformers are empirically observed to break through this barrier and are able to capture the probability of sequences under the Markov distribution near-optimally. In other words, in the presence of tokenization, transformers appear to achieve near-optimal cross-entropy loss. This phenomenon is observed with a multitude of tokenizers used commonly in practice.
3. We analyze a toy tokenizer which adds all length-\(k\) sequences into the dictionary and show that as dictionary size grows, unigram models trained on the tokens get better at modeling the probabilities of sequences drawn from Markov sources. We then theoretically prove that tokenizers used in practice, such as the LZW tokenizer (Zouhar et al., 2023) and a variant of the BPE tokenizer (Gage, 1994; Sennrich et al., 2016) which are learnt from data also satisfy this property but require much smaller dictionaries to achieve any target cross-entropy loss.

In our framework, the most challenging hurdle and the biggest departure from previous work such as (Zouhar et al., 2023) is the element of generalization - understanding how a tokenizer performs on new sequences that it was not trained on. This generalization turns out to be a delicate phenomenon - we show in Appendix D that there exist tokenizers which generalize poorly in the sense that they may compress the dataset they are trained on into a short sequence of tokens, but fail to generalize to new sequences. In Appendix E we show that there exist dictionaries which generalize well (in the sense of having low cross-entropy loss) to new sequences under one encoding algorithm, but completely fail to generalize under another.

### Related Work

Tokenization has a long history of empirical study in natural language processing. In the literature, a number of tokenizers have been developed for various domains such as math (Singh and Strouse, 2024), code (Zheng et al., 2023; Parr, 2013) and morphology-aware tokenizers for different languages like Japanese (Tolmachev et al., 2018; Den et al., 2007) and Arabic (Alyafeai et al., 2023) among many others. In modern LLMs, the most commonly used tokenizers are variants of BPE (Gage, 1994), Wordpiece (Schuster and Nakajima, 2012) and the Unigram tokenizer (Kudo, 2018) which learn a dictionary from data, rather than hard-coding language dependent rules. There has been a long line of work interpreting tokenization from various lenses (Grefenstette and Tapanainen, 1994; Palmer, 2000; Zouhar et al., 2023).

The theoretical study of transformers has also received much attention recently. We discuss the closest relatives to our work below. Edelman et al. (2024) study the learning trajectory of transformers trained on data drawn from \(1^{}\)-order Markov chains. While the authors empirically observe that the models eventually learn to predict tokens correctly according to the Markov kernel, simplicity bias slows down optimization - the models initially predict tokens according to a unigram model (in context unigrams), which delays learning the optimal solution. This phenomenon was also observed in Makkuva et al. (2024). On the positive side, Nichani et al. (2024) study an in-context causal learning task that generalizes learning in-context bigrams for \(1^{}\)-order Markov processes and analyze the trajectory of gradient descent.

Notation.All logarithms are base \(e\), unless specified otherwise. The Shannon entropy \(H(X)\) of a categorical random variable \(X\) is \(-_{x(X)}p(x) p(x)\). \(H_{}(p)\) captures the entropy of a Bernoulli random variable with parameter \(p\). The notation \(O_{p,q,r}(f(n))\) (likewise \(_{\{\}}\) and \(_{\{\}}\)) indicate that the underlying constant depends polynomially on the parameters \(p,q\) and \(r\) and \((f(n))\) (likewise, \(\) and \(\)) ignores \((n)\) terms. For a set \(S\), \(S^{}=_{k=1}^{}S^{k}\), the set of all sequences with elements drawn from \(S\). For a sequence \(\), \(_{i:j}=(_{i},_{i+1},,_{j})\) returns a slice.

## 2 Formulation

We consider a setting where the learner's objective is to learn a language model which models probabilities of sequences over an input alphabet \(\). The data to be modeled is generated according to an unknown probability model \(P:^{}\) over strings. A tokenizer is a tuple \(=(,(),())\). Here \(\) is a collection of tokens The encoding function \(():^{}^{}\), maps strings of characters to a sequence of tokens, and likewise, the decoding function \(():^{}^{}\) maps a sequence of tokens to a string of characters. We assume that the tokenizer is "consistent", namely, \((())\) is the identity function.

We consider a setting where the learner has access to a training dataset which is a sequence of length \(n\) sampled from a data source1. We study the likelihood maximization problem, where the objective of the learner is to learn an end to end model such that the cross-entropy loss is minimized. In the presence of tokenization, we have a model of the form \(Q_{}=Q()\) where \(Q\) is a joint distribution across sequences of tokens when the tokenizer corresponding to \(()\) is used. The cross-entropy loss, i.e. the log-perplexity, can be written down as,

\[_{m}(Q_{})-[ Q(( ))],\] (1)

with the objective to minimize it. Here, the expectation is over \(\), a fresh test sequence of length \(m\) sampled from the data generating process. Fixing a tokenizer, let \(\) denote a family of joint distributions over tokens (i.e. likelihood models). The objective is to jointly design a tokenizer (with encoding function \(()\)) and likelihood model \(Q\) with small test loss \(_{m}(Q())\).

Finally, for a dictionary \(\), the unigram family of models, \(_{}\), is defined as below: \(Q_{}\) associates probability \(Q(_{1},_{2},,_{j})=Q_{\#}(j)_{i=1}^{j}Q_{}(_{i})\) to the sequence of tokens \(_{1},,_{j}\) for measures \(Q_{\#}\) and \(Q_{}\) supported on \(\) and \(\) respectively.

### Data generating process

In this paper, we consider a simplification of real-world data generating processes by considering the case where the data generating distribution is a \(k^{}\)-order Markov process over characters. Studying the behavior of transformers trained on Markov data was the subject of the works Makkuva et al. (2024) and Edelman et al. (2024), where a number of interesting phenomena were unearthed. When a transformer is trained on data from certain simple Markov processes like the one considered in Figure 1, a very peculiar phenomenon occurs - within a reasonably large number of iterations, the transformer fails to improve beyond the loss incurred by the best unigram model. This phenomenon is reproducible across a wide number of hyperparameters, including the number of feed-forward layers in the model, the embedding dimension, and the number of attention heads. In Figure 2(a) this is made clearer - the transformer fails to improve its test loss beyond that of the best unigram model.

Figure 1: \(2\)_-state switching process._ The above state diagram describes the distribution of \(X_{n}\) conditioned on \(X_{n-1}\). \(k^{th}\)_-order extension:_ the conditional probability of \(X_{n}\) only depends on \(X_{n-k}\) through the kernel, \((X_{n}=1|X_{n-k}=0)=p\) and \((X_{n}=0|X_{n-k}=1)=q\).

How bad can a unigram model be? It turns out that the gap between the cross-entropy of the best unigram model and that of the optimal model can be characterized precisely.

**Theorem 2.1**.: _Consider any ergodic data source with stationary distribution over characters \(\). The unconstrained optimal likelihood model achieves cross-entropy loss, \(_{Q}_{m}(Q)=H(P)\). In contrast, the cross-entropy loss under any unigram model \(Q_{1}\) satisfies, \(_{m}(Q) mH()\)._

The ratio of the optimal loss \(H(P)\), and the optimal unigram loss, \(mH()\) can be arbitrarily large. In particular, for the switching chain in Figure 1, as \(p,q 0\), the ratio diverges to \(\).

While transformers are a powerful class of models, it is concerning that they fail to learn very simple distributions such as \(k^{}\)-order Markov processes. Why do they work so well in practice if they can be so slow to learn Markovian data? It turns out that there is a simple missing ingredient in all the architectures considered so far: tokenization. All the models trained in Figure 2(a) operate on raw character sequences drawn from the stochastic source. To understand the role of tokenization, we run another experiment and train the transformer on sequences generated from the stochastic source which are encoded into tokens by a BPE tokenizer learnt from data. The transformer now operates on sequences of tokens, rather than sequences of individual symbols. In Figure 2(b) we plot the results of this experiment - in the presence of tokenization, the cross-entropy loss of the end-to-end model breaks past the unigram barrier and approaches the optimal bound within a small number of iterations.

Let's peek into the model a bit more and understand its behavior. In Figure 2 we run the following experiment: we sample a random sequence of length \(2000\) from a Markov chain and feed it into the transformer after tokenization, resulting in \( 500\) tokens. We plot the next-token distribution predicted by the transformer at every single position in the input, generated by autoregressive masking. In Figure 2 we stitch together these next-token distributions, each of which is a narrow column heatmap. Visually, we observe that the plot is approximately homogeneous along the \(x\)-axis, implying that the next-token distribution learned does not depend strongly on the prefix at that position. Thus the transformer learns what is essentially a unigram model.

Thus, we come to a surprising conclusion: the behavior of the transformer on the \(k^{}\)-order switching source in Figure 1 with and without tokenization is essentially the same. In both cases, the model learns a unigram model over the tokens - in the absence of tokenization this unigram model is in fact the stationary distribution induced by the source. If the transformer learns a unigram model in both cases, how come there is such a large gap in performance between the two? To understand this in more detail, we analyze a toy tokenizer. As a simplification, we will analyze the behavior of an arbitrary, but exact unigram model under this tokenizer.

## 3 Unigram models under tokenization

Let's consider a toy tokenizer which assigns all possible substrings of length \(r\) as tokens in the dictionary and study what happens when a unigram model is trained on the tokenized sequences. The

Figure 2: Token distribution returned by the transformer tokenized by a learnt BPE encoder with a dictionary size of \(20\). A test sequence is generated from the stochastic source and encoded into a token sequence \(\). Each narrow vertical column represents the distribution over next tokens returned by the transformer when the first \(x\) tokens of \(\) are fed into the model, where \(x\) is varied from \(0\) to the length of \(\). For most values of \(x\), the model appears to predict the same distribution over the next token.

total dictionary size \(d=2^{r}\). A sequence of characters is mapped to a sequence of tokens by simply chunking it into a sequences of \(r\) characters which are replaced by the corresponding token index2. The resulting stochastic process on the tokens is still Markovian, but over a state space of size \(2^{r}\). For any unigram model \(Q\) on the tokens, the cross-entropy loss can be written down as,

\[_{m}(Q())=[_{ ()}(1/Q_{}( ))]+((m)),\]

where we choose \(Q_{}=([m])\), which contributes an additive \((m)\) to the loss. Choosing \(Q_{}()=(_{1})_{i=1}^{r-1}P( _{i+1}|_{i})\) as the stationary probability the Markov process associates with \(\),

\[_{m}(Q()) -[(P()+ _{i=0}^{m/k-1}(_{ki+1})}{P( _{ki+1}|_{ki})})]\] \[H(P)+(mH() -H(P))\] \[=(1-(d)})+(d)}.\] (2)

the approximation in \((i)\) uses the fact that as \(m\) grows large, \(_{i=0}^{m/k}(P(_{ki++1}|_{ ki+})\) approaches \(\). With \(d=2\) (i.e., \(r=1\)), we recover the performance of the character tokenizer in Theorem 2.1. An immediate implication of this simple calculation is that as \(m\), there is a unigram model which is nearly optimal as the dictionary size grows to \(\).

While this toy tokenizer allows us to glean this intuition behind why tokenization allows unigram models to be near-optimal, there are some obvious issues. One, the tokenizer does not adapt to the distribution of the data. Indeed, for the switching Markov source in Figure 1, as \(p=q= 0\), the source contains increasingly longer sequences of contiguous \(0\)'s and \(1\)'s. In this case, it makes since to have a dictionary containing such sequences, rather than all possible length-\(r\) sequences, many of which would be seen very few times (if at all) in a test sequence. At a more technical level, in eq. (2), to get to a cross-entropy loss of \(2H(P)\), the size of the dictionary required by the toy tokenizer is \(e^{mH()/H(P)}\). As discussed in Example A.1 for the switching Markov process with \(p=q=\), this dictionary size can be extremely large and scales exponentially (in \(1/\)) as \(e^{1/(1/)}\) when \(\) is

Figure 3: Transformers trained on the order-\(2\) switching Markov process (Figure 1) with \(p=q=0.8\). On the left we have the model trained without tokenization and on the right the model uses BPE with a dictionary of size \(10\) learnt from data.

small. In general, on stochastic sources on a much larger alphabet, such as English/ASCII, this toy tokenizer would result in a prohibitively large dictionary.

Larger dictionaries are usually correlated with the presence of rare tokens which appear infrequently at training time. This presents a problem in practice - a lot more data is often required to see enough examples of such tokens to learn good embeddings for them. More importantly, in the absence of this volume of data, rare tokens present an attack surface to elicit undesirable behavior in the model (Rumbelow and Watkins, 2023). In practice, this issue present with the toy tokenizer is, to an extent, resolved by using tokenization algorithms such as BPE or Wordpiece, which learn dictionaries from data. In the process, they are able to avoid learning extremely rare tokens, by enforcing a lower bound on the number of their occurrences in the training data to be allocated as a token. By minimizing the number of such rare tokens, the model is able to utilize its token budget in a more efficient manner.

We now introduce the main theoretical result of this paper, showing that with the appropriate tokenization algorithm with a token budget of \(d\), a unigram model is not only asymptotically able to achieve the optimal cross-entropy loss, but also requires far smaller dictionaries to match the performance of the toy tokenizer considered earlier. In order to avoid dealing with the transient characteristics of the source, we consider the cross-entropy loss in eq. (1) under the assumption that the test sequences \(\) are of length \(m\). Namely, define the normalized loss,

\[()=_{m}_{m}()\]

**Theorem 3.1**.: _Consider a Markov data generating process which satisfies Assumption 3.2. Let \(d\) denote a budget on the size of the dictionary. Then, there exists a tokenizer with at most \(d\) tokens and encoding function \(()\), such that,_

\[_{_{1:}}}(Q ())_{Q^{}}(Q ^{})\] (3)

_where \(\) is \((1/)/0.99(d)\)3. Furthermore, a tokenizer satisfying eq. (3) with probability \( 1-d^{-_{}((d))}\) can be learnt from a dataset of \(_{}(d)\) characters._

The tokenizers considered in this theorem are far more efficient with their token budget than the toy tokenizer - to achieve a cross entropy loss within a factor \(2\) of optimal, the dictionary size required by these tokenizer is \(d 1/^{2}\) on any source satisfying Assumption 3.2. In comparison, the toy tokenizer requires a dictionary size of \(e^{1/(1/)}\) to achieve the same error. We show that the LZW tokenizer proposed in (Zouhar et al., 2023) achieves the upper bound in eq. (3) when trained on a dataset of size \((d)\). Likewise, we also show that a sequential variant of BPE achieves the upper bound in eq. (3) up to a factor of \(2\) and with a worse dependency in \(\) when trained on a dataset of size \((d^{2})\). What is interesting is that neither of these algorithms explicitly learn a unigram likelihood model, \(Q\), while constructing the dictionary. Yet they are able to perform as well as the tokenizers which are jointly optimized with a likelihood model, such as the Unigram tokenizer (Kudo, 2018).

Key insight.While the toy tokenizer provides a high level intuition as to why tokenization might enable unigram models to model Markov sources well, here we present a different explanation which captures tokenization from an operational viewpoint. Tokenizers which do a good job at learning patterns in the data and assigning these frequent patterns as tokens in the dictionary are compatible with an i.i.d. model over tokens. A hypothetical example motivating this point: consider a tokenizer such that the distribution of tokens in the encoding of a fresh string sampled from the source is distributed i.i.d., except that whenever the token \(^{}\) appears, it is always followed by \(^{}\). An i.i.d. model on the tokens is a poor approximation since \(P(^{}^{}) P(^{})P(^{ })\). However, by merging \(^{}\) and \(^{}\) into a new token \(\) and adding this to the dictionary, the new distribution over tokens is i.i.d. In general, this motivates why it is desirable for a tokenizer to allocate new tokens to substrings which appear next to each other frequently, i.e. a pattern in the data. As more tokens are added to the dictionary, one might expect the cross-entropy loss incurred by the best unigram model to improve.

### Learning patterns in the source

The main result of this section is a generic reduction: dictionaries which typically encode new strings into a few long tokens (defined in a formal sense in Theorem 3.4), result in tokenizers achievingnear-optimal cross-entropy loss. We prove this result for Markovian sources under a regularity assumption, which is that the associated connectivity graph of the chain is complete. The analogous assumption for \(k^{}\)-order sources is that the transition kernel is entry-wise bounded away from \(0\). This assumption is satisfied by all the sources considered in the paper thus far, such as the \(k^{}\)-order switching processes in Figure 1.

**Assumption 3.2** (Data generating process).: Assume that the data source is an ergodic Markov process with transition \(P(|)\) and stationary distribution \(\). Assume that \(_{a,a^{}}P(a^{}|a)>0\).

_Remark 3.3_.: Assumption 3.2 (and its \(k^{}\)-order extension) impose that there is a small but non-zero probability of observing any particular symbol after any preceding sequence. This limits the applicability of these processes in real-world scenarios where such a phenomenon may not occur. However, our motivation for this assumption is different: \(\) allows parameterizing the Markov process in a way which interpolates between i.i.d. (\(=1/||\)) and highly non-i.i.d. (\( 0\)).

For a substring \(\) and a character \(a\), define \(P(|a)=P(_{1}|a)_{i=2}^{||}P(_{i}|_{i-1})\) denote the conditional probability of the substring \(\). We now state the main result of this section.

**Theorem 3.4** (Bound on cross-entropy loss of dictionaries under greedy encoder).: _Consider a source satisfying Assumption 3.2 and any tokenizer \(\) equipped with the greedy encoder, \(_{}()\) with finitely long tokens. Define, \(P()=_{a}[P(|a)]\) and suppose \(H(Q_{},P)(1/)\) for some \(<1\). Then,_

\[_{Q_{1:}}(Q_{ }())(Q)}{1-}.\]

Interpretation.\(H(Q_{},P)=_{ Q_{}}[(1/P())]\) is large when the encoder places higher mass (i.e. larger values of \(Q_{}()\)) on tokens which have low probability under \(P\), i.e. which correspond to longer substrings. Intuitively, this metric is higher for tokenizers which typically use long tokens (i.e. low \(P()\)) to encode new strings.

### LZW tokenizer

In this section we study the Lempel-Ziv-Welch (LZW) based tokenization scheme introduced by Zouhar et al. (2023) and establish guarantees of the form of Theorem 3.1 for this tokenizer.

**Definition 3.5** (LZW tokenizer).: Iterating from left to right, the shortest prefix of the training dataset which does not already exist as a token is assigned as the next token in the dictionary. This substring is removed and the process is iterated on the remainder of the dataset. The tokenizer uses the greedy encoding algorithm (Definition A.3) to encode new strings into tokens.

_An example of the LZW tokenizer:_ For the dataset \(010011\), the dictionary created is \(\{0,1,00,11\}\).

The LZW tokenizer is based on the LZW algorithm for compression (Ziv and Lempel, 1978; Welch, 1984). The dictionary satisfies the property that if some substring \(^{}\) exists as a token in the dictionary, then all of its prefixes must also belong to the dictionary. In the next theorem, we show that the LZW tokenizer approximately achieves the optimal cross-entropy loss.

**Theorem 3.6**.: _Suppose the LZW tokenizer is trained on a dataset of length at most \(d\) (thereby learning a dictionary with at most \(d\) tokens). For Markov sources satisfying Assumption 3.2, with probability \( 1-d^{-O_{}((d))}\), the resulting tokenizer satisfies,_

\[_{Q_{1:}}(Q_{ }())(Q)}{1-}.\]

_where \(=4\)._

The proof of this result considers all substrings \(\) with \(P() 1/d^{0.99}\). These substrings are reasonably high probability and observed many times in a dataset of \((d)\) characters. We show that with high probability, the LZW tokenizer learns _all_ of these substrings as tokens in the dictionary. Now, when processing a new string, since the greedy algorithm only emits the longest substringwhich matches a token, every token allocated must fall on the "boundary" of this set, having \(P() O(1/d^{0.99})\). By definition, this means that \(H(Q_{},P)=_{ Q_{}}[(1/P())]=0.99 (d)\). Combining this with Theorem 3.4 completes the proof. At a high level, on the infinite tree of substrings \(^{}\) we study which nodes are populated as tokens by LZW. This structure forms a Digital Search Tree (DST) and prior work analyzes the mean and variance of the profile of the DST under various source processes (Jacquet et al., 2001; Drmota and Szpankowski, 2011; Hun and Vallee, 2014; Drmota et al., 2021). A detailed proof of Theorem 3.6 is provided in Appendix A.6.

## 4 Experimental Results

Experiment 1 (Figures 3(a) and 3(b))In this experiment we study the order-\(1\) switching Markov chain. Transformers without tokenization empirically achieve a small cross-entropy on this learning task as seen in Figure 3(a) and earlier in Makkuva et al. (2024). We vary hyperparameters to find the smallest untokenized model which achieves a loss within \(10\%\) of the optimal-cross entropy within \(300\) epochs. Fixing a token dictionary size of \(20\), we also find the smallest tokenized model which achieves the same loss. Although the smallest model with tokenization is larger than the smallest model without tokenization in terms of the number of parameters, the wall-clock time taken to optimize the model to any target test loss is observed to be smaller. Thus, tokenization appears to reduce the compute time required to train the model to a target test loss in the toy example we consider. In Figure 3(b) we compare models with the same architecture trained with and without tokenization5. The model with tokenization appears to converge more quickly, although the limiting error achieved is subtly higher in comparison with the model without tokenization.

Experiment 1 (Figure 5).In this experiment, we train tokenizers on the Wikitext-103-raw-v1 dataset (Merity et al., 2016) and compare the performance of unigram models trained on the GLUE dataset as the model size scales. Since the character-level tokenizer operates on a fixed vocabulary, in order to compare with the other tokenizers, we plot the number of unique \(k\)-grams observed in the training dataset along the \(x\)-axis. While this is not an apples-to-apples comparison, we use the number of unique \(k\)-grams in the dataset as a proxy for the complexity of the likelihood model trained.

Figure 4: Test loss vs. wall-clock time for the tokenized and untokenized models when trained on the order-\(1\) switching Markov chain (Figure 1) with \(p=q=0.8\). The tokenizer used is BPE.

One may also use the total number of possible \(k\)-grams as a proxy; however a large fraction of these \(k\)-grams would likely never be observed in a real dataset (especially as \(k\) grows).

Experiment 2 (Table 1).In this experiment, we compare the cross entropy loss of the best unigram model trained on pre-trained tokenizers on an array of datasets. All the considered tokenizers have dictionary sizes in the range 31K-51K. The best bigram model under the character tokenizer is consistently outperformed by the best unigram likelihood model trained under a number of pre-trained tokenizers on a variety of datasets: Rotten Tomatoes (8.5K sequences), GLUE (105K), Yelp review (650K) and Wikitext-103-v1 (1.8M).

### Additional theoretical results

We present some additional theoretical results in the appendix which we discuss briefly below. In Appendix B, we do a theoretical study of the cross-entropy loss achieved by the popular BPE tokenizer. We show that a variant of BPE achieves the upper bound on the RHS of eq.3 (Theorem 3.1) up to a factor approaching \(2\) as the dictionary size grows. It is an interesting question for future research to understand whether this factor of \(2\) can be removed, since transformers are observed to achieve the near-optimal cross-entropy loss as the dictionary size grows (cf. Figure 2(b)). In Appendix C, we prove finite sample bounds on the end-to-end model under the LZW tokenizer with a smoothed empirical estimator as the unigram model. This analysis reveals that there is a sweet spot for the dictionary size - too small a dictionary, and the statistical error floor is significant, too large a dictionary, and the statistical error incurred by the likelihood model dominates the overall loss. We also take a closer look into the aspect of generalization for tokenizers, which arises from the fact that the tokenizer is evaluated on data that it was not trained on. Prior work such as Zouhar et al. (2023b) show that BPE is an approximation algorithm for finding the sequence of merges which minimizes the size of the compressed dataset. This does not imply any guarantees on the end-to-end performance, or even compression power of the tokenizer on new sequences. In particular, in Appendix D we show that there exist tokenizers which compress the dataset into a short sequence of tokens, but do so in a way which fails to generalize to new sequences. Thus measuring the performance of a tokenizer necessitates understanding its behavior on data it was not trained on. In Appendix E, we show a different kind of intricacy - there exist tokenizers under which the best unigram model achieves low cross-entropy loss. However, the same dictionary under a different encoding algorithm performs nearly as poorly as the character-level tokenizer. The interaction between the dictionary and encoding algorithm is a poorly studied subject in the tokenization literature; this result emphasizes the importance of understanding this relationship.

   & RT & Wiki & Yelp & GLUE \\  BERT & 1.58 & 1.55 & 1.60 & 1.50 \\  Tinyllama & 1.75 & 1.84 & 1.82 & 1.70 \\  GPT-neox & 1.57 & 1.64 & 1.66 & 1.48 \\  Mistral & 1.69 & 1.80 & 1.75 & 1.66 \\  Phi-2 & 1.54 & 1.62 & 1.64 & 1.45 \\   Character & 2.40 & 2.45 & 2.46 & 2.38 \\  

Table 1: Cross-entropy loss estimates (using eq.55) of unigram models trained on pre-trained tokenizers under a number of datasets. The last row (blue) is the character level tokenizer, on which a more powerful bigram model is trained. BERT is based on Wordpiece, and the remaining tokenizers are BPE based. The character-level tokenizer we use is ByT5.

Figure 5: _Performance vs. dictionary size._ Tokenizers are trained on the Wikitext-103 dataset. For all other tokenizers we train unigram models while for the the character-level tokenizer, we train \(k\)-gram models for \(k\{1,2,3,4\}\). Likelihood models are trained on the GLUE dataset. The parentheses indicates the number of distinct observed \(k\)-grams, which lower bounds the \(k\)-gram model complexity.

Open questions

In this section, we discuss some limitations of our work and open questions stemming from them. We show that when transformers are trained with or without tokenization, they learn to approximately represent \(k\)-gram models for different values of \(k\). Transformers are capable of representing far more complex behavior, which are elicited under more complex data generating processes. Extending our formulation to these settings presents an avenue to develop an even better understanding of tokenization, and would allow finer-grained comparisons between tokenizers. The behavior and role of tokenizers may be very different in these contexts. Below we discuss some concrete questions.

Our theory assumes that the underlying Markov chain has every transition occurring with non-zero probability, which is a limitation. However, the analysis for the toy tokenizer in eq.2 shows that when the dictionary size scales as \((mH()/H(P))\), even in the absence of Assumption3.2, the tokenizer achieves the optimal cross-entropy to within a factor of \(2\). This leads to the following conjecture.

**Conjecture 1**.: In the spirit of eliminating Assumption3.2, is it possible to establish a version of Theorem3.1 applicable to data drawn from any Markov chain, where \(=(1/)/0.99(d)\) is replaced by \(=(mH()/H(P))/0.99(d)\).

In AppendixB, we analyze a variant of the BPE tokenizer, which carries out a version of sample splitting, and establish a weaker variant of Theorem3.1 for this tokenizer. This is to simplify the statistical dependencies arising from the fact that while learning its dictionary, BPE makes a run over the entire training dataset each time a new token is added. It remains an open question to analyze and establish a variant of Theorem3.1 for the standard BPE tokenizer.

## 6 Conclusion

We present a theoretical framework to compare and analyze different tokenization algorithms. We study the end-to-end cross-entropy loss of the tokenizer + likelihood model, and focus on the case where the data generating process is Markovian. We empirically observe that transformers with tokenization are drastically more efficient at learning \(k^{}\)-order Markov processes, compared to without tokenization. We prove that algorithms such as LZW and a sequential variant of BPE learn tokenizers such that the best unigram likelihood model trained on them approaches the cross-entropy loss of the optimal likelihood model, as the vocabulary size \(d\) grows.

## 7 Acknowledgements

JJ and NR were partially supported by NSF Grants IIS-1901252 and CCF-2211209. KR was partially supported by NSF Grant CCF-2211209.