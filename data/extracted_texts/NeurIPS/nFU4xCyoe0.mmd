# Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models

Hyunsik Chae\({}^{}\), Seungwoo Yoon\({}^{}\), Chloe Yewon Chun\({}^{}\),

Gyehun Go\({}^{}\), Yongin Cho\({}^{}\), Gyeongmin Lee\({}^{}\), Ernest K. Ryu\({}^{}\)

\({}^{}\)Seoul National University, \({}^{}\)UCLA, Department of Mathematics

https://github.com/Atomic-Visual-Skills/AVS

###### Abstract

Recent Vision Language Models (VLMs) have demonstrated impressive multimodal comprehension and reasoning capabilities, but they often struggle with trivially simple visual tasks. In this work, we introduce the Atomic Visual Skills Benchmark (AVSBench) to evaluate whether VLMs possess capabilities to understand basic geometric features, which we refer to as atomic visual skills. Specifically, we systematically categorize the atomic visual skills and handcraft a set of 5,073 diverse questions designed to assess each individual atomic visual skill. Using AVSBench, we evaluate the current leading VLMs and find that they struggle with most of these atomic visual skills that are obvious to humans.

## 1 Introduction

Recent Vision Language Models (VLMs), also referred to more generally as Multimodal Large Language Models (MLLM), integrate vision components into language models and demonstrate an impressive breadth of multimodal comprehension and reasoning capabilities . At the same time, however, VLMs often struggle with trivially easy visual tasks as shown in Figure 1, a puzzling phenomenon that seems almost contradictory to their remarkable performance . We propose two hypotheses to explain the observed shortcomings of current vision-language models:

_Hypothesis 1: The comprehension of complex visual diagrams requires the composition of smaller atomic visual skills._

_Hypothesis 2: Current vision language models are incapable of such atomic visual skills._

In this work, we introduce the Atomic Visual Skills Benchmark (AVSBench) to test _Hypothesis 2_. AVSBench is designed to rigorously evaluate VLMs' ability to comprehend fundamental geometric features, which we refer to as _atomic visual skills_. We systematically categorize 36 atomic visual skills that encompass diagrams arising in high school-level geometry and _handcraft_ a set of 5,073 diverse questions designed to assess the understanding of the individual atomic visual skills.

We then evaluate the state-of-the-art VLMs on AVSBench, and the results clearly support _Hypothesis 2_. While our problems are designed to be trivial to humans, VLMs struggle; state-of-the-art models like Gemini-1.5-pro and GPT-4o score around 70%-75% on problems with the "easy" categorization, score around 60% on the "medium" problems, and 30% on "hard" problems. The confirmation of _Hypothesis 2_ also lends support to _Hypothesis 1_, and suggests a promising direction of future work of training vision language models specifically on atomic visual skills to improve their performance in comprehending complex visual diagrams.

## 2 Atomic Visual Skills Benchmark (AVSBench)

Many visual reasoning tasks in existing benchmarks, such as the ones listed in Section A, are composite tasks that can be broken down into more elementary components. This observation leads us to define a set of _atomic visual skills_ based on the following criteria: (i) each skill is intuitive and trivial for adult humans, (ii) each skill cannot be decomposed further, or doing so would be unnatural, and (iii) the list of atomic visual skills should comprehensively cover the abilities required for comprehending geometric diagrams arising in high-school level mathematics. While this definition is not a fully rigorous one, we found it to be sufficiently clear and substantive for our work.

Using these criteria, we identified 36 atomic visual skills, including the ability to understand concepts such as angle, boundary, orthogonality, curvature, and direction. The complete list and further illustrations are provided in D.

For adult humans, these skills are trivially simple and require little to no reasoning to perform. Therefore, we use the term _comprehension_ instead of _reasoning_ to emphasize our belief that these skills do not require much reasoning or thinking to perform, for both humans and VLMs. This belief is partially supported by Findings 3 of Section 3.1.

We then constructed the Atomic Visual Skills Benchmark (AVSBench) to evaluate VLMs' ability to perform the 36 atomic visual skills. AVSBench, as summarized in Figure 2, comprises 5,073 new handcrafted image-question-answer triplets with the following characteristics:

Figure 1: Examples of AVSBench problems and responses by GPT-4o. Other state-of-the-art models exhibit similar failures. These examples demonstrate a deficiency in the VLMsâ€™ understanding of basic geometric concepts.

Figure 2: List of 36 atomic visual skills and the number of easy, medium, and hard problems for each skill. The difficulty is judged by the authors. We provide a total of 5,073 new handcrafted problems.

* **Originality.** All images and questions are newly generated, ensuring that they are free from data contamination concerns.
* **Diversity.** Although we focus on the set of only 36 skills, the problems feature diverse expressions and formats, as illustrated by the sample problems in C.
* **Skill isolation.** Each question targets a specific atomic skill, minimizing the overlap with other skills. Recognizing the impossibility of achieving complete isolation, our method incorporates diverse tasks to mitigate the influence of any task or their relevant overlapping skills. For instance, to minimize the influence of other skills while evaluating the cardinal skill, we asked about cardinals of various concepts and objects, from colors to points, lines, and other figures.
* **Focus on high school geometry.** We focus on the visual skills required to solve high school-level geometry problems for the following reasons: (i) the scope of the high school mathematics curriculum is more or less clearly defined, (ii) (as our results of Section 3 show) these atomic visual skills are sufficiently challenging for current VLMs, (iii) the range of skills is broad enough to be applicable to other visual comprehension tasks, such as interpreting charts, tables, and scientific or mathematical figures.

## 3 Current vision language models struggle with atomic visual skills

We evaluate three types of VLMs on AVSBench: (i) state-of-the-art proprietary models: GPT-4o [40; 39] and Gemini-1.5-pro , (ii) popular mid-sized open-weight models: LLaVA-Next (7B, 13B) , LLaVA-OneVision (7B) , Phi-3.5-Vision (4B) , InternVL2 (8B) , Deepseek-VL (7B) , and (iii) VLMs specifically trained for geometry or other visual data: Math-LLaVA (13B) , Table-LLaVA (7B) . Further details of model versions are provided in Section E.

The evaluation protocol consists of three steps. First, we provide the VLM with the image-question pair and solicit a response. As we further discuss later, we also explore the effect of the chain-of-thought (CoT) prompting [54; 22]. Second, we extract the answer from the VLM's response using GPT-4o mini . Third, we ask GPT-4o mini to score the answer by comparing the extracted answer with the answer key. We award 1 point for a correct answer and 0 points otherwise without any partial credit. More details on our evaluation protocol are provided in F.

### Experimental results and findings

Figure 3 presents the results comparing the selected VLMs and the baseline corresponding to random guessing. Details including exact values are provided in G. On "easy" problems, models with about 10B parameters score between 32.5% and 51.0% while closed-source models, including GPT-4o and Gemini-1.5-pro, achieve over 70%, far above random chance (22.4%). On "medium" problems, models with about 10B parameters score between 23.8% and 37.8%, slightly above random chance

Figure 3: Evaluation results on AVSBench. +_CoT_ implies the performance of the model on the right with chain-of-thought (CoT) prompting . The area ratios of each colored section are aligned with the actual ratio of problem counts. Details about the models including their full name are in E. Full quantitative results are illustrated in Table 5.

(19.1%). Closed-source models achieve between 58.6% and 64.6%. For "hard" problems, most open models score close to random chance (11.7%). The closed-source models GPT-4o (32.3%) and Gemini-1.5-pro (26.9%) scored significantly better than random chance but clearly struggled.

Findings 1: Models share strengths and weaknesses.Figure 4 presents the accuracies of selected models on each skill. The performances across skills varied significantly. For example, most VLMs performed well on OCR, Absolute Position, and Shapes, but performed poorly on tangency, parallel, and angle. Interestingly, the different models largely shared the same set of skills they did well on and the same set they found challenging.

Findings 2: Domain-specific models are not better.Surprisingly, Math-LLaVA  and Table-LLaVA , which are VLMs specifically trained for geometry or visual data, did not perform better than general VLMs of similar size, on almost any skills within AVSBench as the results of Table 5 show.

Findings 3: Chain-of-thought is not helpful in enhancing atomic visual skills.We also evaluated the best-performing models--GPT-4o and Gemini-1.5-pro--with chain-of-thought (CoT) prompting  on AVSBench. Surprisingly, CoT did not help for most skills, and for some skills, it even worsened the performance, as shown in Table 5. This contrasts with prior work, which found CoT to be beneficial for certain visual reasoning tasks . We attribute this difference to our hypothesis that the atomic visual skills of AVSBench require simple "comprehension" and, therefore, do not benefit from the additional "reasoning" steps afforded by CoT prompting. More concrete comparison, see G.

## 4 Conclusion

We present the Atomic Visual Skills Benchmark (AVSBench), a benchmark designed to rigorously evaluate VLMs' ability to perform atomic visual skills in a decomposed manner. We then show that current state-of-the-art VLMs struggle with such atomic visual skills.

The failure of VLMs to carry out such simple atomic visual tasks raises the question: How is it that VLMs are sometimes successful at performing complex visual tasks? For this, we hypothesize that the existing impressive performance on complex tasks is due to overfitting or unimodal shortcuts. Indeed, recent studies such as  report that VLMs tend to depend on language shortcuts, as we further reference and discuss in Section A of the appendix.

Recall that our _Hypothesis 1_ posits that the atomic visual skills are necessary subcomponents for comprehending complex visual diagrams. In future work, we plan to train and fine-tune VLMs directly on the atomic visual skills and ascertain _Hypothesis 1_.

Figure 4: Accuracies of a leading model, 3 outstanding models, and random chance on each skill. The skills are ordered in descending order of accuracy, averaged over all models.