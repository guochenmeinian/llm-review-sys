# Adjustable Robust Reinforcement Learning for Online 3D Bin Packing

Yuxin Pan\({}^{1}\)  Yize Chen\({}^{2}\)1  Fangzhen Lin\({}^{3}\)1

\({}^{1}\)EMIA, The Hong Kong University of Science and Technology

\({}^{2}\)AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)

\({}^{3}\)CSE, The Hong Kong University of Science and Technology

yuxin.pan@connect.ust.hk  yizechen@ust.hk  flin@cs.ust.hk

###### Abstract

Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired balance of the policy's performance in average and worst-case environments. Specifically, we formulate the objective function as a weighted sum of expected and worst-case returns, and derive the lower performance bound by relating to the return under a mixture dynamics. To realize this lower bound, we adopt an iterative procedure that searches for the associated mixture dynamics and improves the corresponding policy. We integrate this procedure into two popular robust adversarial algorithms to develop the exact and approximate AR2L algorithms. Experiments demonstrate that AR2L is versatile in the sense that it improves policy robustness while maintaining an acceptable level of performance for the nominal case.

## 1 Introduction

The offline 3D bin packing problem (3D-BPP) is a classic NP-hard combinatorial optimization problem (COP) (de Castro Silva et al., 2003), which aims to optimally assign cuboid-shaped items with varying sizes to the minimum number of containers while satisfying physical constraints (Martello et al., 2000). In such a setting, a range of physical constraints can be imposed to meet diverse packing requirements and preferences (Gzara et al., 2020). The primary constraint requires items to be packed stably, without any overlaps, and kept within the bin. The online counterpart of 3D-BPP is more prevalent and practical in logistics and warehousing (Wang and Hauser, 2020), as it does not require complete information about all the unpacked items in advance. In this setting, only a limited number of upcoming items on a conveyor can be observed, and items must be packed after the preceding items are allocated (Seiden, 2002). Thus, besides physical constraints already presentin the offline counterpart, the item permutation on the conveyor introduces additional constraints, while the solution approach must take packing order into consideration.

Solution techniques for online 3D-BPP can be broadly categorized into heuristic and learning-based methods. As heuristics often heavily rely on manually designed task-specific score functions (Chazelle, 1983; Ha et al., 2017), they have limitations in expressing complex packing preferences and adapting to diverse scenarios. By comparison, learning-based methods usually involve utilizing deep reinforcement learning (DRL) techniques to develop a more adaptable packing policy capable of accommodating diverse packing requirements (Zhao et al., 2021, 2022; Song et al., 2023). While emerging DRL-based methods for online 3D-BPP are effective at optimizing average performance, they do not account for worst-case scenarios. This is because these methods often fail on "hard" box sequences arising from the inherent uncertainty in incoming box sequences. In addition, few studies have investigated the robustness of these methods, as incoming item sequences can vary a lot in practice. These limit the practical usage of learning-based approaches.

To study the algorithm robustness under worst-case scenarios, one plausible approach is to design an attacker that can produce perturbations commonly encountered in real-world settings. Various proposed attackers for perturbing RL models often add continuous-value noise to either received observations or performed actions (Tessler et al., 2019; Sun et al., 2021). Yet such methods are mostly not well-suited for online 3D-BPP, as the state is defined by the bin configuration and the currently observed items, and adding continuous-value noise may not correspond to real-world perturbations. As previously discussed, item permutations can significantly impact the performance of a given packing strategy, and in real-world scenarios, the packing strategy may face challenging box sequences. By contrast, perturbing the bin configuration may violate practical constraints or packing preferences. Therefore, we argue that _using diverse item permutations to evaluate the model's robustness_ is a more suitable approach. Some of general robust reinforcement learning frameworks against perturbations can be applied to online 3D-BPP (Pinto et al., 2017; Ying et al., 2022; Ho et al., 2022; Panaganti et al., 2022), yet these methods may develop overly conservative policies, as they usually prioritize improving worst-case performance at the expense of average performance. This motivates us to design a robust algorithm that can effectively handle worst-case problem instances while attaining satisfactory average performance.

In this paper, we develop a novel permutation-based attacker to practically evaluate robustness of both DRL-based and heuristic bin packing policies. We propose an Adjustable Robust Reinforcement Learning (AR2L) framework, which preserves high performance in both nominal and worst-case environments. Our learnable attacker selects an item and place it to the most preceding position in the observed incoming item sequence, as only the first item in this sequence has the priority to be packed first. Essentially, this attacker attempts to identify a problem instance distribution that is challenging for a given packing policy by simply reordering item sequences. The developed AR2L incorporates such novel attacker along with an adjustable robustness weight parameter. Specifically, to consider both the average and worst-case performance, we formulate the packing objective as a weighted sum of expected and worst-case returns defined over space utilization rate. We derive a lower bound for the objective function by relating it to the return under a mixture dynamics, which guarantees AR2L's performance. To realize this lower bound, we turn to identifying _a policy with the highest lower bound as the surrogate task_. In this task, we use an iterative procedure that searches for the associated mixture dynamics for the policy evaluation, and improves the policy under the resulting mixture dynamics. To further put AR2L into practice, we find the connecting link between AR2L with the Robust Adversarial Reinforcement Learning (RARL) algorithm (Pinto et al., 2017) and the Robust \(f\)-Divergence Markov Decision Process (RfMDP) algorithm (Ho et al., 2022; Panaganti et al., 2022), resulting in the exact AR2L and the approximate AR2L algorithms respectively. Empirical evidence shows our method is able to achieve competitive worst-case performance in terms of space utilization under the worst-case attack, while still maintaining good average performance compared to all previously proposed robust counterparts.

## 2 Related Work

Numerous heuristic Ha et al. (2017); Karabulut and Inceoglu (2005); Wang and Hauser (2019); Li and Zhang (2015); Hu et al. (2017, 2020) and learning-based methods (Hu et al., 2017; Verma et al., 2020; Zhao et al., 2021, 2022; Yang et al., 2021; Zhao et al., 2022; Song et al., 2023) have been proposed to solve online 3D-BPP. However, these methods typically only consider average performance and may not be robust against perturbations. For more detailed reviews of related algorithms on 3D-BPP, please refer to the Appendix.

**Adversarial Attacks in Deep RL.** More recently, several studies have demonstrated that deep RL algorithms are vulnerable to adversarial perturbations from attackers, and robust policies can be trained accordingly (Huang et al., 2017; Zhang et al., 2020, 2021; Sun et al., 2021; Ding et al., 2023). Yet standard DRL attack schemes cannot generate realistic attacks for 3D-BPP due to the setting of added perturbations. In the field of COP, Lu et al. (2023) used a RL-based attacker to modify the underlying graph. Yet this method is limited to offline COPs that can be formulated as graph. Kong et al. (2019) employed a generative adversarial network (GAN) (Goodfellow et al., 2020) to generate some worst-case problem instances from Gaussian noise under restricted problem setting.

**Robust RL.** In order to find policies that are robust against various types of perturbations, a great number of studies have investigated different strategies, such as regularized-based methods (Zhang et al., 2020; Shen et al., 2020; Oikarinen et al., 2021; Kumar et al., 2021; Kuang et al., 2022), attack-driven methods (Kos and Song, 2017; Pattanaki et al., 2017; Behzadan and Munir, 2017), novel bellman operators (Liang et al., 2022; Wang and Zou, 2021, 2022), and conditional value at risk (CVaR) based methods (Chow et al., 2017; Tang et al., 2019; Hiraoka et al., 2019; Ying et al., 2022). However, these methods are generally intended to deal with \(l_{p}\)-norm perturbations on vectorized observations, which could limit their applicability to online 3D-BPP. Built upon robust MDP framework (Iyengar, 2005), Pinto et al. (2017) modeled the competition between the agent and the adversary as a zero-sum two-player game, but such game formulation has an excessive prioritization of the worst-case performance. Jiang et al. (2021) introduced Monotonic Robust Policy Optimization (MRPO) to enhance domain generalization by connecting worst-case and average performance. Yet such approach imposes Lipschitz continuity assumptions, which are unaligned with 3D-BPP. The recently developed robust \(f\)-divergence MDP can approximate the worst-case value of a policy by utilizing nominal samples instead of relying on samples from an adversary (Ho et al., 2022; Panaganti et al., 2022). However, this method still cannot account for expected cases, as its focus is solely on learning values in worst-case scenarios.

## 3 Preliminaries

**MDP Formulation of Online 3D-BPP.** To learn a highly effective policy via RL, the online 3D-BPP is formulated as an MDP. Inspired by PCT (Zhao et al., 2022), in this formulation, the state \(s_{t}^{}^{}\) observed by the packing agent consists of the already packed \(N_{C}\) items \(_{t}\) in the bin, the observed incoming \(N_{B}\) items \(_{t}\), and a set of potential positions \(_{t}\). The packed items in \(_{t}\) have spatial properties like sizes and coordinates, while each item \(b_{t,i}_{t},i\{1,..,N_{B}\}\) only provides size information. The potential positions are typically generated for the most preceding item in \(_{t}\) using heuristic methods (Martello et al., 2000; Crainic et al., 2008; Ha et al., 2017). Then, the action \(a_{t}^{}^{}\) is to choose one position \(l_{t,j}_{t},j\{1,..,N_{L}\}\) for the first item in \(_{t}\). In the termination state \(s_{T}\) (\(T\) is the episode length), the agent cannot pack any more items. As a result, the agent receives a delayed reward \(r_{T}\) that represents the space utilization at \(s_{T}\), instead of immediate rewards (\(r_{t}=0,t<T\)). The discount factor \(\) here is set to 1. The aim is to maximize the space utilization by learning a stochastic packing policy \(_{}(l_{t,j}|_{t},_{t},_{t})\).

**Robust MDP.** The goal of robust MDP is to find the optimal robust policy that maximizes the value against the worst-case dynamics from an uncertainty set \(^{w}\). The uncertainty set is defined in the neighborhood of the nominal dynamics \(P^{o}=(P^{o}_{s,a},(s,a))\) and satisfies rectangularity condition (Iyengar, 2005), defined as \(^{w}=_{(s,a)}^{w}_ {s,a},^{w}_{s,a}=\{P_{s,a}():D_{TV}(P_{s,a} ||P^{o}_{s,a})\}\), where \(D_{TV}()\) denotes the total variation (TV) distance, \(\) is the Cartesian product, \(()\) is a set of probability measures defined on \(\), and \(\) is the radius of the uncertainty set. Consequently, the robust value function under a policy \(\) is \(V_{}^{}=_{P^{w}^{w}}V_{r}^{,P^{w}}\). The robust Bellman operator \(_{r}\) is defined as \(_{r}^{}V_{r}^{}(s)=_{a}[r(s,a)+_{P^ {w}_{s,a}^{w}_{s,a}}_{s P^{w}_{s,a}}[V_{r}^{}(s ^{})]]\), where \(s^{}\) denotes the next state. To empirically solve the robust MDP, Pinto et al. (2017) proposed RARL that learns the robust policy under the environment perturbed by a learned optimal adversary.

**Robust \(f\)-divergence MDP.** To avoid the need for a costly trained adversary, RfMDP (Ho et al., 2022; Panaganti et al., 2022) formulates the problem as a tractable constrained optimization task to approximate the robust value using samples from \(P^{o}\). The objective is to find a transition distribution that minimizes the robust value function, subject to the constraint in \(^{w}\) described by the \(f\)-divergence. As a result, a new robust Bellman operator is introduced by solving the dual form of this constrained optimization problem through the Lagrangian multiplier method.

## 4 Adjustable Robust Reinforcement Learning

In this section, we begin by introducing a novel yet practical adversary capable of generating worst-case problem instances for the online 3D-BPP. Next, we present the adjustable robust reinforcement learning (AR2L) framework to address the robustness-performance tradeoff. Finally, we integrate AR2L into both the RARL algorithm and the RfMDP algorithm to derive the exact and approximate AR2L algorithms in a tractable manner.

### Permutation-based Attacker

In online 3D-BPP, a problem instance is comprised of a sequence of items, and these items are randomly permuted and placed on a conveyor. Such randomness can result in certain instances where trained policy may fail. Therefore, this observation motivates us to design a simple yet realistic adversary called permutation-based attacker. By reordering the item sequence for a given packing policy, our approach can explore worst-case instances without compromising realism (See Figure 1 for attacker overview). In contrast to the approach of directly permuting the entire item sequence as described in (Kong et al., 2019), our method involves permuting the observable item sequence, thereby progressively transforming the entire item sequence. Through the behavior analysis of our permutation-based attacker in the Appendix, we observe the attacker appears to prefer smaller items when constructing harder instances as the number of observable items increases. This aligns with the findings mentioned in (Zhao et al., 2022), where larger items simplify the scenario while smaller items introduce additional challenges. As a result, the act of permuting the entire sequence might enable the attacker to trickily select certain types of items, and thus carries potential risks associated with shifts in the underlying item distribution. However, we aim to ensure that the attacker genuinely learns how to permute the item sequence to identify and create challenging scenarios. To mitigate these risks, we limit the attacker's capacity by restricting the number \(N_{B}\) of observable items to proactively address concerns related to changes in the item distribution.

To target any type of solver intended for solving the online 3D-BPP for robustness evaluation, we train a RL-based policy which acts as the permutation-based attacker. The permuted state \(s_{t}^{}^{}\) of the attacker is comprised of the current bin configuration \(_{t}\) and the item sequence \(_{t}\). The action \(a_{t}^{}^{}\) involves moving one item from \(_{t}\) to a position ahead of the other observed items, resulting in the reordered item sequence \(_{t}^{}=\{b_{t,i}^{}\}\). Then, the packing policy will pack the most preceding one into the bin based on the perturbed observation. The reward for the attacker is defined as \(r_{t}^{}=-r_{t}^{}\), since the objective is to minimize the space utilization by training an optimal stochastic policy \(_{}(b_{t,i}|_{t},_{t})\). The online packing process allows only the first item in the observable item sequence to be packed at each step. To address the issue of exponential growth in the action space as the number of observable items increases, the permutation-based attacker strategically selects one item and positions it ahead of the other items. We can consider a simplified value function represented as \(V^{_{}}(_{t},_{t}^{})=r_{t}^{ }+V^{_{}}(_{t+1},_{t+1}^{ })\). This function indicates that the value of \(_{}\) depends on \(r_{t}^{}\), \(_{t+1}\), and \(_{t+1}^{}\). Furthermore, \(r_{t}^{}\) and \(_{t+1}\) are influenced by the first item in \(_{t}^{}\), while the remaining items in \(_{t}^{}\) combine with a new item to form a new sequence, which undergoes further permutations as \(_{t+1}^{}\). As a result, the permutation of the remaining items at time step \(t\) is disregarded in the attack process. To model such

Figure 1: Overview of our attack framework.

an attacker, we use the Transformer (Vaswani et al., 2017) that is capable of capturing long-range dependencies in spatial data and sequential data involved in the attacker's observation. Please refer to the Appendix for more details on the implementations.

### Adjustable Robust Reinforcement Learning

For online 3D-BPP, item observed at each timestep is independently generated from a stationary distribution \(p_{b}(b_{t,i})\) that does not change over time. Once an item is placed into the container, the bin configuration \(_{t}\) becomes deterministic, and a new item \(b_{t+1,N_{B}}\) is appended to the observable item sequence to construct \(_{t+1}\). Thus, the nominal environment is defined as \(P^{o}(_{t+1},_{t+1}|_{t},_{t},a_{t}^{ })=p_{b}(b_{t+1,N_{B}})\). We omit \(_{t}\) for brevity. Since we use the permutation-based attacker to reorder the observable item sequence, the worst-case environment transition is \(P^{w}(_{t+1},^{}_{t+1}|_{t},^{ }_{t},a_{t}^{})=p_{b}(b_{t+1,N_{B}})_{}(b^{ }_{t+1,1}|_{t+1},_{t+1})\), with \(P^{w}\) as the worst-case dynamics.

Existing DRL-based methods are developed to learn a packing policy that maximizes space utilization (cumulative reward) under the nominal distribution \(P^{o}\). However, in real-world scenarios where the order of items can be adversarially permuted, it is not sufficient to only consider the expected return under \(P^{o}\). In contrast, the existing general robust methods that can be deployed to online 3D-BPP overly prioritize robustness at the cost of average performance by solely focusing on the return under the worst-case dynamics \(P^{w}\). To address these issues, we should be aware of the returns under both the average and worst-case scenarios. Given that nominal cases are more common than worst-case scenarios in the online 3D-BPP setting, the objective function is defined as

\[^{*}=_{}(,P^{o})+(,P^{w})\] (1)

where \((0,1]\) is the weight of robustness, \(()\) is the return, and \(P^{w}\) is built on the optimal permutation-based attacker. Here symbols without superscripts represent those used by the packing agent (e.g., \(_{}=\)).

However, how can we learn such a policy with the presence of both \(P^{o}\) and \(P^{w}\)? To address this matter, we derive a lower bound for the objective function defined in Equation 1 by relating it to the return under an unknown mixture dynamics defined as \(P^{m}\), as shown in the following theorem.

**Theorem 1**.: _The model discrepancy between two models can be described as \(d(P^{1}||P^{2})_{s,a}D_{TV}(P^{1}(|s,a)||P^{2}(|s,a))\). The lower bound for objective 1 is derived as follows:_

\[(,P^{o})+(,P^{w})(1+)(,P^{m})-}{(1-)^{2}}(d(P^{m}||P^{o})+ d(P^{m}||P^{w}))\] (2)

The RHS of Inequality 2 provides the lower bound for the objective function defined in Equation 1, where the first term represents the expected return of policy \(\) under the mixture dynamics \(P^{m}\), while the second term denotes the weighted sum of deviations of \(P^{m}\) from both \(P^{o}\) and \(P^{w}\). Motivated by Theorem 1, we turn to maximizing the RHS of Inequality 2 to concurrently improve both the average and worst-case performance. Therefore, the primal problem in Equation 1 is equivalently reformulated to a surrogate problem where we expect to identify an optimal policy with the maximal lower bound, represented as

\[^{*}=*{arg\,max}_{}_{P^{m}}( ,P^{m})-(d(P^{m}||P^{o})+ d(P^{m}||P^{w}));\] (3)

The objective of this optimization problem is to maximize the return by updating both the policy and the mixture dynamics controlled by robustness weight \(\), given the nominal and worst-case dynamics. Furthermore, the second term in Equation 3 can be viewed as a constraint that penalizes the deviations between environments, resulting in a constrained optimization problem written as \(^{*}=_{}\{_{P^{m}}(,P^{m}):d(P^{m }||P^{o})+ d(P^{m}||P^{w})^{}\}\), where \(^{}[0,1+]\). However, this constraint renders the constrained form of Equation 3 impractical to solve. Instead, a heuristic approximation is used for this constraint, which considers the TV distance at each state-action pair, leading to an uncertainty set for the mixture dynamics:

\[^{m}=_{(s,a)}^{m}_{ s,a};\ ^{m}_{s,a}=\{P_{s,a}():D_{TV}(P_{s,a}||P^{o}_{s,a}) + D_{TV}(P_{s,a}||P^{w}_{s,a})^{}\}.\] (4)

The uncertainty set \(^{m}\) satisfies the rectangularity condition Iyengar (2005). To solve the constrained optimization problem, we carry out an iterative procedure which searches for the associated mixture dynamics for the policy evaluation, and improves the policy under the resulting mixture dynamics. Therefore, we define the adjustable robust value function as \(V_{a}^{}=_{P^{m}^{m}}V_{a}^{,P^{m}}\) for the policy evaluation. And the adjustable robust Bellman operator \(_{a}\) can be defined as

\[_{a}V_{a}^{}(s)=_{a}[r(s,a)+ _{P^{m}_{s,a}^{m}_{s,a}}_{s^{} P^{m}_{s, a}}[V_{a}^{}(s^{})]].\] (5)

The adjustable robust Bellman operator assumes both the nominal and worst-case dynamics corresponding to \(\) are available for constructing the uncertainty set \(^{m}\). Unlike the traditional robust Bellman operator (Iyengar, 2005) designed for the minimal value, our proposed operator focus on maximizing the value of a given policy by updating the mixture dynamics. Meanwhile, to avoid the optimistic evaluation, the uncertainty set for the mixture dynamics used in Equation 13 is constrained with regard to both nominal and worst-case environment. Following policy evaluation, the policy is improved using samples drawn from the mixture dynamics. In addition, the following theorem guarantees that \(_{a}\) can facilitate the convergence of the value function to a fixed point.

**Theorem 2**.: _For any given policy \(\) and its corresponding worst-case dynamics \(P^{w}\), the adjustable robust Bellman operator \(_{a}\) is a contraction whose fixed point is \(V_{a}^{}\). The operator equation \(_{a}V_{a}^{}=V_{a}^{}\) has a unique solution._

**Exact AR2L Algorithm.** The AR2L algorithm evaluates and improves policies based on the mixture dynamics that exists in the neighborhoods of both the nominal and worst-case scenarios. However, this algorithm remains impractical since the adjustable robust Bellman operator defined in Equation 13 involves computing the expectations w.r.t. all models in the uncertainty set \(^{m}\). This potentially results in high computational complexity. Inspired by the classic RARL algorithm (Pinto et al., 2017) which trains a robust policy utilizing perturbations injected by a learned optimal adversary, we propose an iterative training approach that involves training a mixture-dynamics model \(_{}\) alongside the packing policy \(_{}\) and the permutation-based attacker \(_{}\) to obtain the exact AR2L algorithm. The mixture-dynamics model is corresponding to the mixture dynamics, and it permutes the observable item sequences \(_{t}\) to \(_{t}^{}\) based on observed packed items \(_{t}\). As illustrated in Figure 2(a), such a model receives the same reward as the packing policy (\(r_{t}^{}=r_{t}^{}\)). Based on Equation 3, the mixture-dynamics model strives to maximize the return under a given policy, while also penalizing deviations between dynamics. As \(P^{m}\) and \(P^{w}\) have the same form with the same underlying item distribution \(p_{b}\), we can only measure the discrepancy between \(P^{m}\) and \(P^{w}\) by evaluating the deviation between \(_{}\) and \(_{}\). In practice, the Kullback-Leibler (KL) divergence is adopted to constrain the deviation between distributions. Thus, the training loss is

\[_{}=-(_{},_{})+(D_{ KL}(_{}||_{\{x=b_{t+1,1}\}})+ D_{KL}(_{ }||_{})),\] (6)

where \(D_{KL}\) is the KL divergence, and \(_{\{x=b_{t+1,1}\}}\) denotes the indicator function. Thus, the mixture-dynamics model is optimized by minimizing both the DRL loss (the first term) and the distance loss (the second term). Based on this, the exact AR2L algorithm iterates through three stages to progressively improve the packing policy. First, the permutation-based attacker is optimized for a given packing policy, as is done in RARL. Next, the mixture-dynamics model \(_{}\) is learned using the loss function defined in Equation 6. Finally, the packing policy is evaluated and improved using problem instances that are permuted by \(_{}\).

Figure 2: Implementations of the AR2L algorithm. **Left:** The exact AR2L algorithm requires to learn a mixture-dynamics model to generate problem instances for the training of the packing policy. **Right:** The approximate AR2L algorithm relies on the samples from both the nominal dynamics and the permutation-based attacker to estimate adjustable robust values of the packing policy.

**Approximate AR2L Algorithm.** Training the mixture-dynamics model in exact AR2L introduces additional computation to the entire framework. We thus propose the approximate AR2L algorithm, which uses samples from both the nominal and worst-case dynamics to estimate adjustable robust values for the policy iteration as shown in Figure 2(b). Inspired by RfMDP (Ho et al., 2022; Panaganti et al., 2022), we use the dual reformulation of the adjustable robust Bellman operator, as given below.

**Proposition 1**.: _The policy evaluation of AR2L can be formulated as a constrained optimization problem. The objective is to maximize the value function over the uncertainty set \(^{m}\), subject to the constraint defined in Equation 4. By representing the TV distance using the \(f\)-divergence (Shapiro, 2017), the adjustable robust Bellman operator \(_{a}\) given in Equation 13 can be equivalently written as_

\[_{a}V_{a}^{}(s)=_{a}[r(s,a)+_{,_{1},_{2},_{3}}(_{s^{}  P_{s,a}^{}}[V_{a}^{}(s^{})-_{1}(s^{})]_{+}+\] (7) \[_{s^{} P_{s,a}^{}}[V_{a}^{}(s^{ })-_{2}(s^{})]_{+}++(1+))],\]

_where \([x]_{+}\) denotes \(\{x,0\}\), \(_{1}(s)\) and \(_{2}(s)\) are Lagrangian multipliers for each \(s\); \(=_{1}(s)+_{2}(s)\) holds for each \(s\), and \(=_{s}\{V_{a}^{}(s)-_{1}(s),V_{a}^{}(s)-_{2}(s),0\}\)._

The approximate AR2L algorithm eliminates the requirement of using samples from the mixture dynamics for policy evaluation, yet it introduces estimation errors into the process. In practice, this results in mild performance degradation and sometimes unstable learning. Nevertheless, it remains valuable as the approximate AR2L still demonstrates superior performance compared to RfMDP.

In order to implement the AR2L algorithm, we opt to use the PPO algorithm (Schulman et al., 2017) to train packing policy \(_{}\), permutation-based attacker \(_{}\) and mixture-dynamics model \(_{}\). The packing policy is implemented using the PCT method (Zhao et al., 2022a) to accommodate the continuous solution space. All of the relevant implementation details, pseudocode for the algorithms, and both derivations and proofs are included in the Appendix.

## 5 Experiments

**Training and Evaluation Settings** In the online 3D-BPP setting, the container sizes \(S^{d},d\{x,y,z\}\) are equal for each dimension (\(S^{x}=S^{y}=S^{z}\)), and the item sizes \(s^{d},d\{x,y,z\}\) are limited to no greater than \(S^{d}/2\) to create more complex scenarios. The stability of each item is checked based on constraints used in (Zhao et al., 2022a,b). Moreover, we adopt two different popular settings used in (Zhao et al., 2022a,b) for training DRL-based policies. In the discrete setting, where item sizes are uniformly generated from a discrete set, i.e., \(s^{d}\{1,2,3,4,5\}\), resulting in a total of 125 types of items, and the container size \(S^{d}\) is fixed at 10. In the continuous setting, the container size \(S^{d}\) is set to 1, and item sizes \(s^{d},d\{x,y\}\) follow a continuous uniform distribution \(U(0.1,0.5)\); and \(s^{z}\) is uniformly sampled from a finite set \(\{0.1,0.2,0.3,0.4,0.5\}\). Then, discrete and continuous datasets are created following instructions from the two aforementioned training settings to evaluate both heuristic and DRL-based methods. Each dataset consists of 3,000 problem instances, where each problem instance contains 150 items. We use three metrics to evaluate the performance of various packing strategies: \(Uti\). represents the average space utilization rate of the bin's volume; \(Std\). is the standard deviation of space utilization which evaluates the algorithm's reliability across all instances; \(Num\). evaluates the average number of packed items.

### Algorithm Robustness under Permutation-based Attacks

In our experiment, we evaluate the robustness of six heuristic methods, including deep-bottom-left (DBL) method (Karabulut and Inceoglu, 2005), best-match-first (BMF) method (Li and Zhang, 2015), least-surface-area heuristics (LSAH) (Hu et al., 2017), online bin packing heuristics (OnlineBPH) (Ha et al., 2017), heightmap-minimization (HMM) method (Wang and Hauser, 2019) and maximize-accessible-convex-space (MACS) method (Hu et al., 2020). For the DRL-based methods, we evaluate the constrained deep reinforcement learning (CDRL) (Zhao et al., 2021) algorithm, and the packing configuration tree (PCT) method (Zhao et al., 2022a). We train specific permutation-based attackers for each of them. Since the capacity of the permutation-based attacker is highly related to the number of observable items of the attacker, we choose \(N_{B}=5,10,15,20\) to obtain different attackers. Given that the number of observable items for the packing policy is typically limited to the first item in most methods, except for PCT, we can establish a consistent approach by setting the number of observable items for the packing policy to 1 in this case.

Table 1 presents the performance of various methods under perturbations from corresponding permutation-based attackers in the discrete setting. We can observe that though the HMM algorithm does not perform the best in the nominal scenario compared to other heuristic methods, it exhibits superior performance under attackers with different attack capacities. By comparison, the DBL method is the most vulnerable heuristic method. As attack capabilities increase, the CDRL algorithm demonstrates greater robustness compared to the PCT method, consistently achieving a higher average number of packed items with smaller variance of volume ratio. It is worth noting that the larger the value of \(N_{B}\) is, the more the performance can be degraded for all methods, indicating that harder problem instances can be explored by increasing the value of \(N_{B}\). In addition, the number of packed items does not necessarily decrease as attack capacity increases. This is because permutation-based attackers aim to minimize space utilization, which means they may select smaller items to make the packing problem more challenging for the packing policy.

### Performance of AR2L Algorithm

We then benchmark four other RL algorithms compatible with online 3D-BPP tasks, which include packing configuration tree (PCT) (Zhao et al., 2022) method, the CVaR-Proximal-Policy-Optimization (CPPO) (Ying et al., 2022) algorithm, the robust adversarial reinforcement learning (RARL) algorithm (Pinto et al., 2017), and the robust \(f\)-divergence MDP (RfMDP) (Ho et al., 2022; Panaganti et al., 2022). The PCT algorithm serves as the baseline method and is used as the packing policy for all other methods in our implementations. The CPPO algorithm trains the packing policy with worst-case returns as a constraint, which can potentially improve both average and worst-case performance. The RARL algorithm is the baseline model for the exact AR2L algorithm due to its reliance on worst-case samples from the attacker. Likewise, the RfMDP framework serves as the baseline model for the approximate AR2L algorithm because it involves approximating values of the unknown dynamics. As both the exact AR2L (ExactAR2L) algorithm and the approximate AR2L algorithm (ApproxAR2L) can develop policies with different levels of robustness by adjusting the value of \(\), we choose \(=0.3,0.5,0.7,1.0\) to explore the relationship between \(\) and policy robustness. Furthermore, we use different attackers with varying attack capabilities (\(N_{B}=5,10,15,20\)) to investigate the robustness of packing policies. It is important to note that in this scenario, the packing policy is permitted to observe the same number of items as its corresponding attacker. During testing, we construct mixture datasets by randomly selecting \(\%\) nominal box sequences and reordering them using the learned permutation-based attacker for each packing policy. The empirical results in the continuous setting are included in the Appendix. Since we implement PCT with different training configurations, we conducted a comparison between our implementation and the official implementation in the Appendix.

\(Uti\)**.** Table 2 presents the results of evaluating multiple robust methods on various datasets that differ in the number of problem instances perturbed by different attackers. We denote the different values of \(\) used by the ExactAR2L algorithm as ExactAR2L(\(\)), and similarly for ApproxAR2L(\(\)). The results unequivocally indicate that the ExactAR2L algorithm, when employed with a higher value of \(\), exhibits superior performance on nominal cases and competitive robustness in terms of space utilization when compared to other robust methods. When the value of \(\) is increased, it allows the packing policy to encounter and learn from more worst-case samples during training. By experiencing a wider range of adversarial situations, the packing policy can better adapt to mitigate the impact of attackers, enhancing its overall robustness. However, ExactAR2L cannot consistently improve its average performance across all the settings with different values of \(N_{B}\). For the scenario

    & =5\)} & =10\)} & =15\)} & =20\)} &  \\   &  & \(Std.\) & \(Num.\) & \(Uti.(\%)\) & \(Std.\) & \(Num.\) & \(Uti.(\%)\) & \(Std.\) & \(Num.\) & \(Uti.(\%)\) & \(Std.\) & \(Num.\) \\  DBL & 40.4 & 14.3 & 18.4 & 28.5 & 16.1 & 13.9 & 26.0 & 14.9 & 14.9 & 21.3 & 12.6 & 8.5 & 63.6 & 11.9 & 25.8 \\ BMF & 46.9 & 11.5 & 21.0 & 40.9 & 12.2 & 22.2 & 38.6 & 12.9 & 21.1 & 33.7 & 11.9 & 22.1 & 62.0 & 9.2 & 24.8 \\ LSAH & 46.0 & 10.1 & 20.3 & 42.2 & 11.3 & 20.9 & 38.6 & 11.0 & 19.4 & 35.6 & 11.8 & 21.3 & 60.9 & 10.9 & 24.6 \\ OnlineBPH & 47.1 & 21.0 & 19.8 & 44.0 & 18.9 & 22.8 & 29.8 & 18.5 & 14.6 & 22.4 & 12.7 & 14.1 & 64.1 & 8.9 & 25.8 \\ HMM & 49.1 & 11.1 & 22.5 & 46.5 & 13.8 & 22.5 & 43.4 & 13.0 & 21.4 & 40.1 & 10.0 & 24.5 & 16.1 & 10.4 & 22.6 \\ MACS & 43.0 & 9.7 & 21.9 & 40.8 & 9.0 & 24.8 & 39.0 & 9.8 & 23.7 & 38.3 & 9.0 & 27.0 & 53.0 & 10.8 & 21.5 \\ CDRL & 61.5 & 7.8 & 27.9 & 56.1 & 6.9 & 28.7 & **54.6** & 7.6 & 31.3 & **51.0** & 7.1 & 29.9 & 74.1 & 7.3 & 29.1 \\ PCT & **63.6** & 9.9 & 27.3 & **58.7** & 11.3 & 25.8 & 50.9 & 13.1 & 25.6 & 40.5 & 15.3 & 21.8 & **76.6** & 6.0 & 30.0 \\   

Table 1: The performance of existing methods under the perturbation in the discrete setting.

[MISSING_PAGE_FAIL:9]

cannot outperform PCT due to the larger distribution deviation caused by the increased number of observable items of the attacker. In a word, by including perturbed data with a small deviation from the nominal data during training, the enhanced generalization leads to the performance improvement in nominal cases (\(=0\)). Conversely, the large distribution deviation between the perturbed data and the nominal data can degenerate the performance in nominal cases, as indicated by the RARL algorithm. We also observe that as the value of \(\) increases, the ExactAR2L algorithm tends to favor a larger value of \(\). Furthermore, the ApproxAR2L algorithm exhibits a similar performance tendency in both nominal and worst-case scenarios as the ExactAR2L algorithm. Due to the introduced value estimation error, the ApproxAR2L algorithm cannot perform as well as the ExactAR2L algorithm. Despite this, the ApproxAR2L algorithm still performs better than its baseline model, the RfMDP algorithm. Additionally, as demonstrated in Figures 3(a) 3(b), the ExactAR2L algorithm can learn faster than other robust RL algorithms.

\(\) As shown in Table 2, ExactAR2L demonstrates its superiority over PCT with smaller \(Std.\) in 17 tasks, while producing a slightly larger \(Std.\) in 3 tasks. Thus, ExactAR2L can indeed improve the robustness. Furthermore, we observe when \(N_{B}=5,10\), ExactAR2L tends to choose \(=0.7\) for smaller \(Std.\). On the other hand, for \(N_{B}=15,20\), ExactAR2L favors \(=1.0\). Since ExactAR2L is trained on both nominal and worst-case dynamics, while RARL is trained only on the worst-case dynamics, the ExactAR2L policy is less conservative than the RARL policy. While the conservativeness may result in smaller \(Std.\) in most tasks, it produces worse results in terms of \(Uti.\) under the nominal dynamics. It is worth noting that the value of \(Std.\) from ExactAR2L is the closest to that of RARL. This observation shows ExactAR2L can trade off between conservative and risky behavior, as the \(Std.\) from ExactAR2L is between that of RARL and PCT. Similarly, ApproxAR2L is less conservative than RfMDP, which causes ApproxAR2L cannot achieve a smaller \(Std.\) in all tasks.

\(\) The ExactAR2L(1.0) algorithm can pack more items in 17 tasks compared to PCT, and shows a slight drop in 3 tasks, where the average drop is 0.2. We found that to pack more items, ExactAR2L consistently favors \(=1.0\) across various tasks. Compared to RARL, ExactAR2L(1.0) can pack at least the same number of items in 16 tasks. Thus ExactAR2L(1.0) can produce competitive results compared to RARL and PCT in terms of \(Num.\). Compared to the baseline method RfMDP, ApproxAR2L(0.5) can pack more items in 16 tasks, and shows a slight drop in only 4 tasks, where the average drop is 0.25.

**Hyperparameter Configurations.** Based on observations from Table 2, \(=1.0\) is the best choice for ExactAR2L across different test settings. When \(=50,75\), ExactAR2L(1.0) performs the best compared to baselines and ExactAR2L with other values of \(\) (with a slight drop compared to ExactAR2L(0.7) when \(=50,N_{B}=15\)). If \(=100\), ExactAR2L(1.0) can still produce competitive results compared to RARL and significantly outperforms PCT. When \(=25\), although \(=1.0\) is not the optimal choice, ExactAR2L(1.0) can still outperform other baselines. When \(=0\), ExactAR2L(1.0) significantly outperforms RARL, and the slight drop compared to PCT is acceptable, as our goal is to improve the robustness while maintaining average performance at an acceptable level. \(\) is only used in ApproxAR2L algorithm. As shown in Figures 3(c) 3(d), we choose different values of \(\) in different settings. We found that \(=0.1\) is a trustworthy choice for ApproxAR2L. Based on the observations from Table 2 and Figures 3(c) 3(d), we conclude that \(=0.1\) and \(=0.5\) are the best choice for ApproxAR2L, as it outperforms its corresponding baseline in almost all the tasks.

## 6 Conclusions and Limitations

In this work, we propose a novel reinforcement learning approach, Adjustable Robust Reinforcement Learning (AR2L), for solving the online 3D Bin Packing Problem (BPP). AR2L achieves a balance between policy performance in nominal and worst-case scenarios by optimizing a weighted sum of returns. We use a surrogate task of identifying a policy with the largest lower bound of the return to optimize the objective function. We can seamlessly bridge AR2L into the RARL and RfMDP algorithms to obtain exact and approximate AR2L algorithms. Extensive experiments show that the exact AR2L algorithm improves the robustness of the packing policy while maintaining acceptable performance, but may introduce additional computational complexity due to the mixture-dynamics model. The approximate AR2L algorithm estimates values without samples from the mixture dynamics, yet performance is not up to our exact AR2L agent due to the existence of estimation error. Though our AR2L framework is designed for online 3D-BPP, it can be also adapted to other decision-making tasks. We will investigate more efficient and generalizable AR2L framework.