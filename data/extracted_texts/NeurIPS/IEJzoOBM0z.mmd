# Estimating Causal Effects Identifiable from a Combination of Observations and Experiments

Yonghan Jung\({}^{1}\), Ivan Diaz\({}^{2}\), Jin Tian\({}^{3}\), and Elias Bareinboim\({}^{4}\)

###### Abstract

Learning cause and effect relations is arguably one of the central challenges found throughout the data sciences. Formally, determining whether a collection of observational and interventional distributions can be combined to learn a target causal relation is known as the problem of _generalized identification_ (or _g-identification_) [Lee et al., 2019]. Although g-identification has been well understood and solved in theory, it turns out to be challenging to apply these results in practice, in particular when considering the estimation of the target distribution from finite samples. In this paper, we develop a new, general estimator that exhibits multiply robustness properties for g-identifiable causal functionals. Specifically, we show that any g-identifiable causal effect can be expressed as a function of generalized multi-outcome sequential back-door adjustments that are amenable to estimation. We then construct a corresponding estimator for the g-identification expression that exhibits robustness properties to bias. We analyze the asymptotic convergence properties of the estimator. Finally, we illustrate the use of the proposed estimator in experimental studies. Simulation results corroborate the theory.

## 1 Introduction

Performing causal inferences is a crucial aspect of scientific research with broad applications ranging from the social sciences to economics, biology to medicine. It provides a set of principles and tools to draw causal conclusions from a combination of observations and experiments. Two significant tasks in the realization of these inferences are causal effect identification and estimation. _Causal effect identification_ concerns determining the conditions under which one can infer the causal effect \(P(Y=y|do(X=x))\) (shortly, \(P(y|do(x))\)) of the treatment \(X=x\) on the outcome \(Y=y\) from a combination of available data distributions and a causal graph depicting the data-generating process [Pearl, 2000, Bareinboim and Pearl, 2016]. _Causal effect estimation_ aims to develop an estimator for the identified causal effect expression using a set of finite samples.

Recent advances in the literature on generalized causal effect identification (g-identification) have developed algorithms that can identify causal effects by using a set of observational and experimental distributions and a causal graph. The result is an expression of the causal effect as a function of available observational and experimental distributions [Bareinboim and Pearl, 2012, Lee et al., 2019]. For concreteness, consider some practical scenarios that exemplify g-identification.

**Example 1**.: Many studies have investigated how a training program's eligibility (\(X\)) affects future salary (\(Y\)) (e.g.,[Glynn and Kashin, 2017]). Actual registration in the program (\(Z\)) determines the salary, and experimental studies have looked into how Z affects Y (e.g., [LaLonde, 1986]). Eligibility is determined by past average income (\(W\)), which is associated with both \(Z\) and \(Y\). The causalgraph in Fig. 0(a) shows the data-generating process, with bidirected edges indicating unmeasured confounders affecting the variables. According to Lee et al. (2019), the causal effect \(P(y|do(x))\) can be identified by combining the experimental distribution on \(Z\) (denoted \(P(|do(z))\)) with the observational distribution \(P\). It's given as \(P(y|do(x))=_{z,w}P(y|do(z))P(z|w,x)P(w)\). 

**Example 2**.: There have been many experimental studies on the effect of an antihypertensive drug (\(X_{1}\)) on blood pressure (\(W\)) (e.g., Hansson et al. (1999)) and on the effect of using an anti-diabetic drug (\(X_{2}\)) on cardiovascular disease (\(Y\)) (e.g., Ajjan and Grant (2006), Kumar et al. (2016)). \(R\) is a set of mediators. Their relations are depicted in Fig. 0(b). Recent studies report that simultaneously taking antihypertensive and anti-diabetic drugs may be harmful (Ferrannini and Cushman, 2012). This motivates the study of the combined causal effect of both treatments (i.e., \(P(y|do(x_{1},x_{2}))\)) by combining the two experimental studies (i.e., from \(P(|do(x_{1}))\) and \(P(|do(x_{2}))\)). According to Lee et al. (2019), it turns out that \(P(y|do(x_{1},x_{2}))=_{r,w}P(y|r,w,do(x_{2}))P(r|x_{2},do(x_{1}))_{x_{ 2}^{}}P(w|r,x_{2}^{},do(x_{1}))P(x_{2}^{}|do(x_{1}))\), which means that the joint treatment effects can be computed using the two experimental studies on \(X_{1}\) and \(X_{2}\). 

On the other hand, causal effect estimation has mainly focused on limited identification scenarios, relying on stringent assumptions such as the no unmeasured confounder assumption. Beyond these restrictions, recent progress has been made in developing statistically appealing estimators from observational data for any identification functional given by the complete identification algorithms (Jung et al., 2020, 2021, 2021, 2022, 2022, 2022, 2022). While these estimators are capable of estimating any identification expression from observational data, they are not yet sufficiently advanced to estimate g-identification, which involves multiple observations and experiments.

Recently, Jung et al. (2023) generalized existing doubly robust estimators (Mises, 1947, Bickel et al., 1993, Robins and Rotnitzky, 1995, Bang and Robins, 2005, Robins et al., 2009, van der Laan and Gruber, 2012, Luedtke et al., 2017, Chernozhukov et al., 2018, Rotnitzky et al., 2021) to estimate covariate adjustments (e.g., back-door adjustment (Pearl, 1995), sequential back-door (SBD) adjustment (Pearl and Robins, 1995) or multi-outcome SBD (mSBD) (Jung et al., 2021)) in the g-identification setting, where the expression is in the form of covariate adjustment involving multiple experimental distributions. However, the covariate adjustments only cover a limited portion of all g-identifiability scenarios as in Examples (1,2). On a different thread, Xia et al. (2023) developed a neural network-based estimation framework capable of taking a combination of observational/experimental data. Still, the derived estimators do not possess the doubly robustness property. In other words, there is still a gap between g-identification and causal effect estimation.

In this paper, our goal is to bridge the gap between g-identification and causal effect estimation. Specifically, this paper presents a framework for estimating identification expressions using multiple sets of samples from both observational and interventional distributions. This framework is a generalization of the results in Jung et al. (2021) since our results reduce to theirs when only observational data is available. Furthermore, our work subsumes the results in Jung et al. (2023) when the identification functional takes the form of covariate adjustments.

The contributions of our paper are as follows:

1. We show that any causal effects identifiable by g-identification can be expressed as a function of generalized mSBD adjustments. We provide a systematic procedure for specifying the function.

Figure 1: Causal graphs of examples 1 and 2. The nodes representing the treatment and the outcome are marked in blue and red, respectively.

2. After developing a doubly robust estimator for generalized mSBD adjustments, we construct an estimation framework for any g-identifiable causal effects, which enjoys multiply robustness against model misspecification and bias. Experimental studies corroborate our results.

### Preliminaries

We use bold letters (\(\)) to denote a random vector and a random value. Each random vector is represented with a capital letter (\(\)) and its realized value with a small letter (\(\)). Given a set \(=\{X_{1},,X_{n}\}\) aligned by an order \(\) such that \(X_{i} X_{j}\) for \(i<j\), we denote \(}^{i}\{X_{1},,X_{i}\}\) and \(}^{i,j}\{X_{i},,X_{j}\}\). For a discrete vector \(\), we use \(_{}()\) to represent the indicator function such that \(_{}()=1\) if \(=\); \(_{}()=0\) otherwise. We use \([n]\{1,,n\}\) a collection of index. For a discrete vector \(\), we use \(P() P(=)\) where \(P\) is a distribution. We use \(_{P}[f()]_{}f()P()\) for a function \(f\), where \(\) denote the support of \(\). We will use \(_{}\) to denote the domain of \(\). For a sample set \(D\{_{(i)}\}_{i=1}^{n}\) where \(_{(i)}\) denotes the \(i\)th samples, we use \(_{D}[f()](1/n)_{i=1}^{n}f(_{(i)})\). We use \(\|f\|_{P}_{P}[\{f()\}^{2}]}\). If a function \(\) is a consistent estimator of \(f\) having a rate \(r_{n}\), we will use \(-f=o_{P}(r_{n})\). We will say \(\) is \(L_{2}\)-consistent if \(\|-f\|_{P}=o_{P}(1)\). We will use \(-f=O_{P}(1)\) if \(-f\) is bounded in probability. Also, \(-f\) is said to be bounded in probability at rate \(r_{n}\) if \(-f=O_{P}(r_{n})\). We use the typical graph terminology \(pa()_{G},ch()_{G},de()_{G},an()_{G}\) to represent the union of \(\) with its parents, children, descendants, ancestors in the graph \(G\). We use \((;G)\) to denote the union of the predecessors of \(C_{i}\) given a topological order \(_{G}\) over a graph \(G\). We use \(G()\) to denote the subgraph of \(G\) over \(\). Throughout the paper, we will assume a fixed topological order \(_{G}\) over \(\) on \(G\). 

**Structural Causal Models (SCMs).** We use Structural Causal Models (SCMs) as our framework (Pearl, 2000; Bareinboim et al., 2022). An SCM \(\) is a quadruple \(=,,P(),F\). \(\) is a set of exogenous (latent) variables following a joint distribution \(P()\). \(\) is a set of endogenous (observable) variables whose values are determined by functions \(F=\{f_{V_{i}}\}_{V_{i}}\) such that \(V_{i} f_{V_{i}}(pa_{i},u_{i})\) where \(PA_{i} V\) and \(U_{i} U\). Each SCM \(\) induces a distribution \(P()\) and a causal graph \(G=G()\) over \(\) in which there exists a directed edge from every variable in \(PA_{i}\) to \(V_{i}\) and dashed-bidirected arrows encode common latent variables (e.g., see Fig. 0(a)). Performing an intervention fixing \(=\) is represented through the do-operator, \(do(=)\), which encodes the operation of replacing the original equations of \(X\) (i.e., \(f_{X}(pa_{x},u_{x})\)) by the constant \(x\) for all \(X\) and induces an interventional distribution \(P(|do())\). 

**Experimental Distributions and Samples** To clarify the connection between the experimental samples where the randomization is applied to \(\) and the distribution \(P_{}()\), we introduce the notation \(P_{()}()\) where \(()\) denotes that \(\) is randomized. The distribution \(P_{()}()\) is a distribution induced by the SCM in which the original equation \(Z f_{Z}(pa_{z},u_{z})\) for \(Z\) is replaced to the function assigning the value to \(Z=z\) at random without depending on other endogenous variables \(PA_{Z}\); e.g., \(Z=1\) and \(0\) at probability \(0.5\) for each. We note that \(P P_{()}\) when observational. For any set \(,,\), the interventional distribution can be represented as \(P(|do(),)=P_{()}(| =,)\) by the definition of the \(do\)-operator and \(P_{()}\) distribution. We use \(P_{}(|) P_{()}( |=,)\) to highlight that the distribution is induced from the randomization and conditioning on \(=\). The experimental samples from randomization \(()\) induces samples \(D_{()}\) following \(P_{()}()\). We use \(D_{}\) to denote the subsample of \(D_{()}\) fixing \(=\), which follows \(P_{}()\). 

**g-identification.** Let \(\{_{i}\}_{i=1}^{m}\) denote a collection of variables where \(_{i}\) can be an empty set. Let \(\{P_{(_{i})}(),\ _{i}\}\), a collection of distributions inducing experimental samples from trials randomizing \(_{i}\). A causal effect \(P(|do())\) is said to be \(g\)_-identifiable_ from \(\) in a causal graph \(G\) if \(P(|do())\) is uniquely computable from the combination of distributions in \(\) in any SCM that induces \(G\)(Lee et al., 2019, Def. 4). The complete g-identification algorithm developed by Lee et al. (2019) identifies the causal effect by decomposing so-called _confounded components_ (c-component). A _c-component_ is a maximal set of variables where every pair is connected by a bidirectional path composed of bidirectional edges (\(V_{i} V_{j}\)). For example, graphs in Figs. 0(a, 0(b)) form a single c-component since bidirectional paths connect any pairs of variables. For any sets \(\), the quantity \(Q[] P(|do())\) is called a _c-factor_. To identify the causal effect \(P(|do())\) from \(\) and \(G\), the g-identification algorithm in [Lee et al., 2019, Algo. 1] (and rewrote in Algo. 1) rewrites the causal effect as a marginalization over a product of c-factors, \(P(|do())=_{_ {}}_{i=1}^{k_{d}}Q[_{i}]\), where \( an()_{G()}\) and \(_{i}\) are c-components in \(G()\), and identifies each \(Q[_{i}]\) from \(\). 

### Problem Statement

This paper aims to develop an estimation framework for the g-identifiable causal effect \(P(|do())\) identified as a function of distributions in \(\) from experimental samples \(\{D_{_{i}} P_{(_{i})}( )\}\). We impose the following regularity assumptions:

**Assumption 1** (Regularity).: _For variables \(\) and distributions \(P_{()}\), the following conditions hold: (1) All variables in \(\) are discrete; (2) \(P_{()}()>c,\) for some \(c(0,1)\)._

We discuss the relaxation of the regularity assumption in Appendix C. This relaxation allows some subset of variables in \(\) can be a mixture of continuous and discrete random variables. Due to space constraints, all proofs are provided in Appendix B.

## 2 Expressing Causal Effects as a Combination of mSBD Adjustments

In this section, we present an algorithm that expresses any g-identifiable causal effects as a combination of _marginalization/multiplication/divisions_ of adjustment functionals defined in the following. We begin by formally defining the generalized multi-outcome sequential back-door adjustment (g-mSBD) functional, which strictly generalizes the mSBD adjustment proposed by Jung2021:

**Definition 1** (generalized-mSBD adjustment (g-mSBD)).: Let \((,)\) be a disjoint pair in \(\) topologically ordered as \((,)=\{_{0},W_{1},,_{m-1},W_{m}, _{m}\}\) by \(_{G}\), where \(_{i}\) can be empty. Let \(}^{i-1}\{W_{j}\}_{j=1}^{i-1}\) and \(}^{i-1}\{_{j}\}_{j=0}^{i-1}\) for \( i[m]\). Let \(\). Let \(_{0}\) be some set such that \(_{0}\), \(=\). Let \((_{0})\) denote a sequence \((_{1},,_{m})\) where \(_{i}\) denotes some realization of \(_{i}_{0}\) (same \(_{i}\) could appear multiple times in the sequence). Then, the g-mSBD adjustment is expressed as an operator \(A_{0}[,,;_{0},,G](,)\) defined by

\[A_{0}[,,;_{0},]( ,)_{_{ }}_{i:W_{i}}P_{_{i}}(w_{i}|}^{i-1},}^{i-1}_{i}). \]

The g-mSBD adjustment specializes to the mSBD adjustment [Jung et al., 2021b] when \(_{0}=\). The g-mSBD adjustment can be viewed as a variant of the g-formula [Robins, 1986] involving multiple distributions. The power of the g-mSBD adjustment lies in its ability to express the c-factor:

**Lemma 1** (c-component Identification [Jung et al., 2021b]).: _Let \(\) denote a \(c\)-component in \(G_{i} G(_{i})\) for some \(_{i}\). Let \( pa()_{G_{i}}\). Let \((,)\) be ordered as \((_{0},S_{1},,_{m-1},S_{m})\) by \(_{G}\). Let \(\) denote a set satisfying \(=an()_{G_{i}()}\). Let \(()\). Let \(_{0}\{_{i}\}\) and \((_{0})\) be a sequence of \(_{i}\) repeating \(m\) times. Then, the \(c\)-factor \(Q[]\) is g-identifiable as follows:_

\[Q[]=A_{0}[,,;_{0}\{ _{i}\},](,)=_{ _{}}_{j:V_{j}}P_{_{i}}(v_{j }|}^{j-1},}^{j-1}_{ i}). \]

We propose an identification algorithm, Algo. 1, which expresses any causal effect as a combination of marginalizations, multiplications, and divisions of g-mSBD operators. Here are some results used for the g-mSBD operation. An example of using these results is provided in Appendix A.

**Lemma 2** (Marginalization).: _Let \(A_{0}[,,;_{0},]( ,)\) denote the g-mSBD operator in Def. 1. Let \(_{0}\). Let \(_{}\{_{0},\}\) denote the vector formed by the following procedure: Starting from \(_{}=\), for \(j=m,,1\), \(_{}=_{}\{W_{j}\}\) if (1) \(W_{j}\{_{0},\}\) and (2) \( k\{j,,m\}\) such that \(_{j},,_{k-1}=\), \(}^{k+1:m}_{}\), and \(_{k}==_{j}\) and \(_{k}==_{j}\) and \(_{k}==_{j}\). Let \(^{}_{}\), \(^{}(^{};G)\) and \(^{}\{_{0},\}_{ }\). Let \(^{}_{0}\) denote the collection of \(_{i}\) corresponding to the variable in \(^{}\), and \(^{}\) the corresponding sequence. Then,_

\[_{_{0}_{W_{0}}}A_{0}[,, ;_{0},](, )=A_{0}[^{},^{},^{}; ^{},^{}](^{} ^{},^{}). \]This lemma provides a graphical criterion where \(_{_{0}_{W_{0}}}A_{0}[,,; _{0},](,)\) is given as a g-mSBD operator.

```
Input:\(,,\{_{i}\},\{P_{ ^{}(_{i})}(),\ _{i}\},G\) Output:Expression of \(P(|do())\) w.r.t. distributions in \(\)
1If\(_{i}\) such that \(P(|do())=P_{_{i}}()\) for some \(_{i}_{_{i}}\), thenreturn\(P_{_{i}}()\).
2 Let \( an()\); \(P() P(an())\); and \(G G(an())\).
3 Let \( an()_{G()}\).
4 Find the \(C\)-component of \(G()\): \(_{1},_{k_{d}}\).
5foreach\(_{j}\{_{1},_{k_{d}}\}\)do
6foreach\(_{i}\)do
7 Find the c-component \(_{j}^{i}\) in \(G(_{i})\) such that \(_{j}_{j}^{i}\).
8\(Q[_{j}^{i}]=A_{0}[_{j}^{i},,_{j}^{i} ;_{j}^{i}\{_{i}\},_{j}^{i}](_{j}^{i},_{j}^{i})\), where \(_{j}^{i} pa(_{j}^{i})_{G( _{i})}_{j}^{i}\). // By Lemma 1
9 Run \(Q[_{j}]=(_{j},_{j}^{i},Q[_{j}^{i}],G(_{j}^{i}))\).
10If\(Q[_{j}]\), then break.
11
12 end if
13
14If\(Q[_{j}]=\), then return FAIL.
15
16 end if
17\(P(|do())=_{ _{}}_{j=1}^{k_{d}}Q[_{j}]\). // Apply Lemmas (2,3,4) if viable return\(P(|do())\)
18Procedure\((,,Q[],G())\)
19 Let \( an()_{G()}=\{A_{1},A_{2},,A_{n_{a }}\}\) such that \(A_{1}_{G}_{G}A_{n_{a}}\) in \(G()\).
20 Let \(Q[]=_{_{}}Q[]\). // Apply Lemma 2 if viable
21If\(=\), then return\(Q[]\).
22
23If\(=\), then return\(\).
24else
25 Let \(\) be the c-component in \(G()\) such that \(\).
26 Let \(Q[]_{\{i:A_{i}\}} _{_{i+1}}}Q[]}{_{b_{i}_{_{i}}}Q[]}\) for \(_{i}}^{i-1}\). // Apply Lemmas (2,3,4) if viable return\((,,Q[],G())\)
27 end if
```

**Algorithm 1**\((,,,,G)\)

The \(C\)-component of \(G()\): \(_{1},_{k_{d}}\).

The \(Q[_{j}]\), then break.

This lemma provides a graphical criterion where \(_{_{0}_{W_{0}}}A_{0}[,, ;_{0},](,)\) is given as a g-mSBD operator.

**Lemma 3** (**Multiplication)**.: _Let \(A_{0}^{i} A_{0}[_{i},,_{i};_{i},^{i}](_{i},_{i})_{j=1}^{m^{i}}P_ {_{j}^{i}}(w_{i,j}|}_{i}^{j-1},}_{i}^{j-1}_{j}^{i})\) for \(i\{1,2\}\) where \(^{i}(_{j}^{i})_{j=1}^{m^{i}}\). Let \(_{1}_{2}\). Let \((_{1}_{2})\). Let \((,)\) be ordered by \(_{G}\). Let \(_{1}_{2}\). Assume the following: (1) \(_{1}_{2}=\); and (2) \( W_{j}, W_{i,k}_{i}\) such that \((}^{j-1},}^{j-1})=(}_ {i}^{k-1},}_{i}^{k-1})\)._

_Let \((_{j})_{j:W_{j}}\) where \(_{j}=_{k}^{i}\) for all \(j\). Then,_

\[A_{0}^{1} A_{0}^{2}=A_{0}[,,;, ](,)=_{j:W_{j}}P_{_{j }}(w_{j}|}^{j-1},}^{j-1}_{j}). \]

This lemma provides a graphical criterion where a product \(A_{0}^{1} A_{0}^{2}\) is given as a g-mSBD operator.

**Lemma 4** (**Division)**.: _Let \(A_{0}^{i} A_{0}[_{i},,_{i};_{i},^{i}](_{i},_{i})_{j=1}^{m^{i}}P_ {_{j}^{i}}(w_{i,j}|}_{i}^{j-1},}_{i}^{j-1}_{j}^{i})\) for \(i\{1,2\}\) where \(^{i}(_{j}^{i})_{j=1}^{m^{i}}\). Let \(_{1}_{2}\). Let \((_{1}_{2})(;G)\). Assume the following: (1) \(_{2}_{1}\); and (2) \( W_{j}, W_{1,k}_{1}\) such that \((}^{j-1},}^{j-1})=(}_ {1}^{k-1},}_{1}^{k-1})\), \(_{i,k}=_{j}\) and \(_{i,k}=_{j}\). Then,_

\[A_{0}^{1}/A_{0}^{2}=A_{0}[,,;_{1}, ^{1}](,)=_{j:W_{j}}P_{_{j }}(w_{j}|}^{j-1},}^{j-1}_{j}). \]This lemma provides a graphical criterion where a product \(A_{0}^{1}/A_{0}^{2}\) is given as a g-mSBD operator.

We have rewritten the identification algorithm proposed by Lee et al. (2019) as Algo. 1 to express the g-identifiable causal effect as a combination of marginalizations, multiplications, and divisions of g-mSBD. It's worth of noting that the identification algorithm proposed by Lee et al. (2019) and Algo. 1 are equivalent.

**Theorem 1** (Expression of g-Identifiable Causal Effects).: _Algo. 1 returns any g-identifiable causal effects as a function of a set \(\{A_{0}^{k}\}\) of g-mSBD adjustment operators in the form_

\[P(|do())=f(\{A_{0}^{k}\}_{k=1}^{K}), \]

_where the function \(f()\) applies marginalization, multiplication, or division over g-mSBD operators in \(\{A_{0}^{k}\}\) as specified by Algo. 1._

For concreteness, we demonstrate the application of Algo. 1 for Figs. ((a,b)), where the effects \(P(|do())\) are g-identifiable. Detailed and visually friendly demonstrations are described in Appendix A.

**Example 3** (Application of Algo. 1 to Example 1).: Note \(=\{,Z\}\). **Line 3-4: \(=\{Z,Y\}\)** where \(_{1}\{Z\}\) and \(_{2}\{Y\}\). **Line 5-13:** Identify \(Q[_{1}]\) from \(_{1}=\) as follow. Note \(_{1}^{0}\), where \(^{0}\) where \(Q[^{0}]=A_{0}[^{0},,;^{0} ,][^{0},)=P()\). Run \(Q[_{1}]=(_{1},^{0},Q[^ {0}],G)\) and obtain \(Q[_{1}]=A_{0}^{1} A_{0}[\{W,Z\},W,X;,](z, x)=_{w^{0}_{W}}P(z|x,w)P(w)\). Lemmas (2,3,4) are used in running the sub-procedure. Now, identify \(Q[_{2}]\) from \(_{2}=\{Z\}\) as follow. Note \(_{2}^{1}\{W,X,Y\}\), the c-component in \(G( Z)\). Note \(Q[^{1}]=A_{0}[^{1},,;^{1} \{Z\},^{1}][^{1},)=P_{z}(w,x,y)\), where \(^{1}=(z,z,z)\). We run \(Q[_{2}]=(_{2},^{1},Q[^ {1}],G(^{1}))\), and obtain \(Q[_{2}]=A_{0}^{2} A_{0}[Y,,;^ {1},^{1}][y,)=P_{z}(y)\). Lemma 2 is used in the sub-procedure. **Line 14-15: \(P(y|do(x))=_{z^{x}_{2}}A_{0}^{1}A_{0}^{2}\)**. 

**Example 4** (Application of Algo. 1 to Example 2).: Note \(=\{X_{1},X_{2}\}\). **Line 3-4: \(=\{R,W,Y\}\)** where \(_{1}\{R\}\), \(_{2}\{W\}\), and \(_{3}=\{Y\}\). **Line 5-13:** In \(G( X_{1})\), \(_{1}=^{1}_{1}\{R\}\). \(Q[_{1}]=Q[^{1}_{1}]=A_{0}^{1} A_{0}[R,,X _{2};^{1}\{X_{1}\},^{1}](r,x_{2})=P(r|do(x_{1} ),x_{2})\), where \(^{1}=(x_{1})\). In \(G( X_{1})\), \(_{2}^{1}_{2}\{X_{2},W,Y\}\). \(Q[^{1}_{2}]=A_{0}[^{1}_{2},,R;^{2} \{X_{1}\},^{2}](^{1}_{2},r)=P_{x_{1}}(x_{2})P_ {x_{1}}(w|x_{2},r)P_{x_{1}}(y|x_{2},w,r)\) where \(^{2}=(x_{1},x_{1},x_{1})\). Run \(Q[_{2}]=(_{2},^{1}_{2},Q[^{1}_{2}],G(^{1}_{2}))=A_{0}^{2} A_{0}[\{X_{2},W\},X_{2},R;^{2},^{2}](w,r)=_{x^{}_{2}^{x} _{2}}P_{x_{1}}(w|r,x^{}_{2})P_{x_{1}}(x^{}_{2})\) where \(^{2}=(x_{1},x_{1})\). Lemma 2 is used in the sub-procedure. Since \((_{3},^{1}_{2},Q[^{1}_{2}],G( ^{1}_{2}))\) return FAIL, we find the c-component \(^{1}_{2}\{Y\}\) where \(_{3}=^{1}_{2}\). Note \(Q[_{3}]=Q[^{1}_{2}]=A_{0}^{3} A_{0}[Y,, \{R,W\};^{3}\{X_{2}\},^{3}](y,\{r,w\})=P_{x_{2} }(y|w,r)\), where \(^{3}=(x_{2})\). **Line 14-15:** Applying Lemma 3, \(A_{0}^{13} A_{0}^{1} A_{0}^{3}=A_{0}[\{R,Y\},,\{X_{2},W\};^{13}\{X_{1},X_{2}\},^{13}](\{r,y\},\{x_{2},w\})=P_{x_{1}}(r|x_{2},P_{x_{2}}(y|r,w)\), where \(^{13}=(x_{1},x_{2})\). Then, \(P(y|do(x_{1},x_{2}))=_{r,w_{R,W}}A_{0}^{13}A_{0}^{2}\). 

## 3 Estimating g-Identifiable Causal Effects

In this section, we develop an estimator for \(P(|do())\) using samples \(\{D_{(_{i})} P_{(_{i} )}()\}\) obtained from randomized experiments and observations (where \(_{i}=\)). We use \(P_{()}\) instead of \(P_{}\) to highlight the distribution \(D_{(_{i})}\) follows.

We first introduce an estimator for the g-mSBD adjustment that exhibits the doubly robust property. The nuisance parameters for the g-mSBD adjustment are defined as follows:

**Definition 2** (**Nuisances for g-mSBD)**.: _Nuisances for g-mSBD \(A_{0}\) in Eq. (1) are \(\{_{0}^{i+1},_{0}^{i}\}_{i=1}^{m-1}\) defined as follows. Let \(_{0}^{m+1}=^{m+1}_{}( )\). For \(i=m-1,,1\),_

\[_{0}^{i+1}(}^{i},}^{1: i}) _{P_{(_{i+1})}}[_{0}^{i+2}(}^{i+1},_{i+1},}^{1:i})|}^{i},}^{1:i},_{0},_{i+1}] \] \[_{0}^{i}(}^{i},}^{1: i}) _{i})}(}^{i},}^{1: i-1}|_{i},_{0})}{P_{(_{i+1})}(}^{i}, }^{1:i-1}|_{i+1},_{0})}_{ _{i}}(_{i})}{P_{(_{i+1})}(_{i}| }^{i},}^{1:i-1},_{i+1}, _{0})}. \]

**Remark 1** (**Simplification of Nuisances**).: _Although the nuisances \(_{0}^{i}\) may seem complicated, they can be simplified in several important special cases. For example, \(_{0}^{i}=_{_{i}}(_{i})/P_{()}( _{i}|}^{i},}^{i-1},, _{0})\) if \(=\{\}\) for any \(\) where \(\) is possibly empty._In general, employing off-the-shelf classification methods for density ratio estimation is feasible, leveraging the techniques outlined in Section 5.4 of Diaz et al. (2021).

We now introduce a g-mSBD estimator exhibiting the robustness properties using these nuisances. This estimator is motivated by the double/debiased machine-learning style estimators (Chernozhukov et al., 2018, 2022):

**Definition 3** (**DR-g-mSBD Estimators)**.: Let \(D_{(_{i})}\) for \(_{i}\) denote the experimental samples from randomizing the variable \(_{i}\). Let \(_{_{i}}\) for \(_{i}_{_{i}}\) denote the subsamples of \(D_{(_{i})}\) fixing \(_{0}_{i}=_{0}_{i}\) and \(_{i}=_{i}\). The DR-g-mSBD estimator \(\) for the g-mSBD adjustment \(A_{0}[,,;_{0}\{_{i }\}_{i=1}^{m},(_{i})_{i=1}^{m}]( ,)\) is defined as follows:

1. Randomly partition \(_{_{i}}\) into \(\{_{_{i},}\}_{[L]}\); i.e., \(_{_{i}}=_{=1}^{L}_{_{i}, }\), \(_{i}\) and \(_{i}_{_{i}}\).
2. For each fold \([L]\), let \(_{}^{i+1}\) denote learned \(_{0}^{i+1}\) using \(_{_{i+1}}_{_{i+1},}\) for \(i=m,,2\); and \(_{}^{i}\) learned \(_{0}^{i}\) for \(i=1,,m-1\). Define \(_{}^{i+1}_{}^{i+1}(}^{i}, _{i},}^{1:i-1})\) and \(_{}^{i}_{j=1}^{i}_{}^{j}\).
3. Estimate \((\{_{}^{j+1},_{}^{j}\}_{j[m-1], [L]})(1/L)_{=1}^{L}_{}(\{_{}^{j+1},_{ }^{j}\}_{j[m-1]})\) where \[_{}_{}(\{_{}^{j+1},_{}^{j}\}_{j [m-1]})_{j=1}^{m-1}_{_{_{j+1},}}[_{}^{j}\{_{}^{j+2}-_{}^{j+1 }\}]+_{_{_{1},}}[_{ }^{2}],\] (9) where \(_{_{_{j},}}\). \([]\) is an empirical average over samples \(_{_{j},}\).

We now analyze the doubly robustness property of this estimator.

**Proposition 1** (**Asymptotic Analysis of g-mSBD Estimators)**.: _Assume that the nuisance estimates \(_{}^{i}\) and \(_{}^{i}\) are \(L_{2}\)-consistent; i.e., \(\|_{}^{i+1}-_{0}^{i+1}\|_{P_{(_{i+1})}}=o_{P_{ (_{i+1})}}(1)\), \(\|_{}^{i+2}-_{0}^{i+2}\|_{P_{(_{i+1})}}= o_{P_{(_{i+1})}}(1)\) and \(\|_{}^{i}-_{0}^{i}\|_{P_{(_{i+1})}}=o_{P_{( _{1})}}(1)\). Let \(n_{i}|_{_{i}}|\) for \(i\{1,,m\}\). Then,_

\[-A_{0}=_{i=1}^{m}R_{i}+_{=1}^{L}_{i=1}^{m-1 }O_{P_{(_{i+1})}}(\|_{}^{i+1}-_{0}^{i+1}\|\|_ {}^{i}-_{0}^{i}\|), \]

_where \(R_{i}\) is a random variable such that \(n_{i}^{1/2}R_{i}\) converges in distribution to a mean-zero normal random variable._

This estimator possesses a doubly robustness property since the estimator is bounded in probability at rate \(n^{-1/2}\) (for \(n\{n_{1},,n_{m}\}\), whenever \(O_{P_{(_{i+1})}}(\|_{}^{i+1}-_{0}^{i+1}\|\|_{ }^{i}-_{0}^{i}\|)=O_{P}(n_{i+1}^{-1/2})\) for all \(i\).

We now construct an estimator for the g-identification expression using the DR-g-mSBD estimator defined in Def. 3. The resulting estimator is called the MR-gID estimator:

**Definition 4** (**MR-gID Estimator)**.: The MR-gID estimator \(\) for the identification expression of the causal effect \(_{0} f(\{A_{0}^{k}\}_{k=1}^{K})\) in Theorem 1 is given as follows: For each \(A_{0}^{k}\) composing \(f(\{A_{0}^{k}\}_{k=1}^{K})\), let \(^{k}^{k}(\{_{k,}^{j+1},_{k,}^{j}\}_{j[m ^{k}-1],[L]})\) denote the DR-g-mSBD estimator with nuisance estimates \(\{_{k,}^{j+1},_{k,}^{j}\}\) for the true nuisances \(\{_{k,0}^{j+1},_{k,0}^{j}\}\). Then,

\[ f(\{^{k}\}_{k=1}^{K}). \]

We impose assumptions on the identification expression and its nuisances for further analysis.

**Assumption 2** (**Analysis of MR-gID )**.: _The identification function \(f(\{A^{k}\}_{k=1}^{K})\) in Thm. 1 and each nuisances \(\{_{k,}^{i+1},_{k,}^{i}\}_{k,}\) for \(^{k}\) satisfy the following properties:_

1. _Twice differentiability:_ \(f(\{A^{k}\}_{k=1}^{K})\) _is twice continuously Frechet differentiable w.r.t._ \(\{A^{k}\}_{k=1}^{K}\) _w.r.t._ \(\{A^{k}\}_{k=1}^{K}\)2. _Boundedness_: \( k[K]\) and \(_{i}\), \(_{^{k}}f(\{A^{j}_{0}\}_{j=1}^{K})[^{k}-A^{k}_{0}]=O_{P_{ (_{i})}}(^{k}-A^{k}_{0})\).
3. \(L_{2}\)**-Consistency**: \(\|^{i+1}_{k,}-^{i+1}_{k,0}\|_{P_{(^{k}_{i+1})}}=o_{P_ {(^{k}_{i+1})}}(1)\), \(\|^{i+2}_{k,}-^{i+2}_{k,0}\|_{P_{(^{k}_{ i+1})}}=o_{P_{(^{k}_{i+1})}}(1)\), \(\|^{i}_{k,}-^{i}_{k,0}\|_{P_{(^{k}_{i+1})}}=o_{P_{ (^{k}_{i+1})}}(1)\), and \(\|^{2}_{k,}-^{2}_{k,0}\|_{P_{(^{k}_{i}) }}=o_{P_{(^{k}_{i})}}(1)\).

Assumption 2 is imposed to limit the error of the MR-gID, which is a linear function of the errors of each DR-g-mSBD estimator.

**Theorem 2** (Asymptotic Analysis of MR-gID).: _Suppose Assumption 2 holds. Let \(n_{k,i}|_{^{k}_{i}}|\) for \(^{k}_{i}\) and \(^{k}_{i}_{^{k}_{i}}\). Let \(\) denote the MR-gID estimator in Def. 4 for the causal effect \(_{0} f(\{A^{k}_{0}\}_{k=1}^{K})\) in Theorem 1. Then, the error of \(\) is given as_

\[-_{0}=_{k=1}^{K}_{i=1}^{m^{k}}O_{P_{( ^{k}_{i})}}(n_{k,i}^{-1/2})+_{k=1}^{K}_{=1}^{ L}_{i=1}^{m^{k-1}}O_{P_{(^{k}_{i})}}(\|^{i+1}_{k,}- ^{i+1}_{k,0}\|\|^{i}_{k,}-^{i}_{k,0}\|). \]

We highlight that the MR-gID \(\) exhibits robustness property since \(-_{0}\) for \(_{0}=P(|do())\) is bounded at rate \(n^{-1/2}\) (for \(n=\{n_{k,i}\}\) and \(P\)) even when all nuisances \(\{^{i+1}_{k,},^{i}_{k,}\}\) are bounded at slower \(n^{-1/4}\) rate. Furthermore, the MR-gID estimator exhibits multiply robustness, as the error of the MR-gID is a linear function of the error of DR-g-mSBD, which demonstrates the doubly robustness property. Formally,

**Corollary 2** (**Multiply Robustness** (Corollary of Thm. 2)).: _Suppose (1) Assumption 2 holds; (2) Either \(^{i}_{k,}=^{i}_{k,0}\) or \(^{j}_{k,}=^{j}_{k,0}\) for \(j=i+1,,m^{k}\) for all \(i,,k\); and (3) all nuisances \(\{^{i}_{k,},^{i+1}_{k,}\}_{i,,k}\) are bounded by some constant. Then, the MR-gID \(\) (Def. 4) is consistent to \(_{0}\)._

For concreteness, we illustrate the application of Thm. 2 for Examples (1, 2). Detailed procedures are provided in Appendix A.

**Example 5** (Application of Thm. 2 to Example 1).: _Recall that \(P(y|do(x))=f(\{A^{1}_{0},A^{2}_{0}\})_{z_{Z}}A^{1} _{0}A^{2}_{0}\). The nuisance set for \(A^{1}_{0}\) is \(^{2}_{1,0}(X,W)_{P}[_{z}()|X,W]\) and \(^{1}_{1,0}(X,W)_{x}(X)/P(X|W)\). Then, the estimator for \(A^{1}_{0}\) is \(^{1}^{1}(\{^{1}_{1,},^{1}_{1,}\}_{[ L]})\) defined in Def. 3. The nuisance set for \(A^{2}_{0}\) is \(^{2}_{2,0}_{P_{(Z)}}[_{y}(Y)]\). Then, the estimator for \(A^{2}_{0}\) is \(^{2}^{2}(\{^{2}_{2,},^{1}_{[L]}\}\). Then, the estimator is constructed as Def. 4, as \(f(\{^{1},^{2}\})\). By Thm. 2, the error of the estimator is \(O_{P}(n_{0}^{-1/2})+O_{P}(n_{s}^{-1/2})+(1/L)_{=1}^{L}O_{P}(\|^{2 }_{1,}-^{2}_{1,0}\|\|^{1}_{1,}-^{1}_{1,0}\|)\), where \(n_{0}|D|\) and \(n_{z}|D_{z}|\) where \(D P\) and \(D_{z} P_{z}\)._

**Example 6** (**Application of Thm. 2 to Example 2)**.: _Recall that \(P(y|do(x_{1},x_{2}))=f(\{A^{2}_{0},A^{13}_{0}\})=_{r,w_{R,W} }A^{2}_{0}A^{13}_{0}\). The nuisance set for \(A^{2}_{0}\) is \(^{2}_{2,0}_{P_{z_{1}}}[_{w}(W)|R,X_{2}]\) and \(^{1}_{1,0}_{r}(R)/P_{x_{1}}(R|X_{2})\). Then, the estimator for \(A^{2}_{0}\) is \(^{2}^{2}(\{^{2}_{2,},^{1}_{1,}\}_{[ L]})\). The nuisance set for \(A^{13}_{0}\) is \(^{2}_{13,0}_{P_{z_{2}}}[_{r,y}(R,Y)|R,W]\) and \(^{1}_{13,0})}(R|x_{2},x_{1})}{P_{(X_{2}) }(R|x_{2})}(W)}{P_{(X_{2})}(W|R,x_{2})}\). Then, the estimator is \(^{13}=^{13}(\{^{2}_{13,0},^{1}_{13,0}\}_{[L]})\). Then, the estimator for \(A^{13}\) is \(f(\{^{13},^{2}\})\). By Thm. 2, the error of the estimator is \(O_{P_{(X_{1})}}(n_{1}^{-1/2})+O_{P_{(X_{2})}}(n_{2}^{-1/2})+(1/L) _{=1}^{L}\{O_{P_{(X_{1})}}(\|^{2}_{2,}-^{2}_{2,0}\|\| ^{2}_{2,}-^{2}_{2,0}\|)+O_{P_{(X_{2})}}(\|^{2}_{13,}- ^{2}_{13,0}\|\|^{1}_{13,}-^{1}_{13,0}\|)\}\), where \(n_{1}|D_{1}|\) and \(n_{2}|D_{2}|\) where \(D_{1} P_{x_{1}}\) and \(D_{2} P_{x_{2}}\)._

## 4 Experiments

In this section, we demonstrate the MR-gID estimator from Definition (4) through Examples (1,2) and Project STAR dataset. For each example, the proposed estimator is constructed using a dataset \(\{D_{_{i}},\ _{i}\}\) simulated from an underlying SCM. Our goal is to provide empirical evidence of the fast convergence behavior and the robustness property of the proposed estimator compared to competing baseline estimators. We consider two standard baselines in the literature: the'regression-based estimator (reg)' only uses the regression nuisance parameters \(^{2}\) for \(^{2}_{0}\) defined in Def. 2, and the 'probability weighting-based estimator (pw)' only uses the probability weighting parameters \(^{m-1}\) for \(^{m-1}_{0}\) defined in Def. 2, while our MR-gID uses both in estimating the g-mSBD operators \(A^{k}\) composing \(f(\{A^{k}\})\) in Thm. 1. Details of the regression-based ('reg') and the probability weighting-based ('pw') estimators are provided in Appendix A. The details of the simulation are in Appendix (D, E).

### Synthetic Dataset Analysis

**Accuracy Measure.** We compare the proposed estimator ('mr') in Def. 4 to the regression-based estimator ('reg') and the probability weighting-based estimator ('pw'). In particular, we use \(T^{}()\) for \(\{,,\}\) to denote the g-ID estimators that leverage regression-based ('reg'), probability weighting-based ('pw'), and MR-gID in estimating each operator \(A^{k}\) in the identification expression \(f(\{A^{k}\})\) of the causal effect \(P(|do())\). We assess the quality of the estimators by computing the _average absolute error_\(^{}_{}|}_{ _{}}|T^{}()-P(|do())|\) where \(|_{}|\) is the cardinality of \(_{}\). Nuisance functions are estimated using gradient boosting models called XGBoost (Chen and Guestrin, 2016). We ran 100 simulations for each \(n=\{200,400,600,800,1000\}\) for \(n|D_{}|\) for \(\). We label the box-plot for these AAEs as 'AAE-plot'.

**Experimental Setup.** We evaluate the \(^{}\) for Examples (1,2) in four scenarios:

* **(Scenario 1)** There were no noises in estimating nuisances.
* **(Scenario 2)** We introduced a converging noise \(\) in estimating the nuisance, decaying at a \(n^{-}\) rate (i.e., \((n^{-},N^{-2})\)) for \(=1/4\) to emphasize the errors induced by the finiteness of samples. This scenario is inspired by the experimental design discussed in (Kennedy, 2020).
* simulated by training the model with a random matrix having the same dimension as the input matrix.
* **(Scenario 4)** Nuisance \(\{^{i}_{k,}\}_{,k,i}\) are estimated incorrectly as in Scenario 3.

In Scenario 1, we aim to show that all estimators \(T^{},T^{},T^{}\) are converging to the true causal quantity \(P(|do())\). In Scenario 2, we aim to show that the MR-gID estimator exhibits fast convergence behavior compared to competing estimators. In Scenario (3,4), our goal is to highlight the multiply robustness property of the MR-gID estimator.

Figure 2: AAE Plots for Examples (1,2) for Scenarios {1,2,3,4} depicted in the Experimental Setup section. The \(x\)-axis and \(y\)-axis are the number of samples and AAE, respectively.

**Experimental Results.** The AAE plots for all scenarios are presented in Fig. 2. All the estimators ('reg', 'pw','mr') converge in Scenario 1 as the sample size grows. In Scenario 2, where the estimated nuisances are controlled to be bounded in probability at \(n^{-1/4}\) rate, the proposed MR-gID \(\) outperforms the other two estimators by achieving fast convergence. This result corroborates the robustness property in Thm. 2. In Scenarios (3,4), where the estimated nuisances for \(\{^{i}\}_{i=2}^{m}\) or \(\{^{i}\}_{i=1}^{m-1}\) are wrongly specified, the MR-gID estimator converges while other estimators fail to converge. This result corroborates the multiply robustness property in Coro. 2.

### Project STAR Dataset

This section provides an overview of the analysis using Project STAR dataset . Project STAR investigated the impact of teacher/student ratios on academic achievement for students in kindergarten through third grade. The dataset \(D\) includes class size (\(X_{1}\)), the academic outcome in kindergarten (\(W\)) for kindergarten, the academic outcome in second grade (\(R\)), class size (\(X_{2}\)), and the academic outcome for the third grade (\(Y\)). We assume that the SCM \(\) underlying Project STAR dataset \(D\) can be depicted in Figure 2(a). The target quantity is \([Y|do(x_{1},x_{2})]\) where \(P_{x_{1},x_{2}}(y)=_{r_{R}}P_{x_{1}}(r)_{x_{1}^{ },w_{X_{1},W}}P_{x_{2}}(y|x_{1}^{},w,r,x_{2})P_{x_{2}}(x _{1}^{},w)\). The detailed procedures are in Appendix E.

**Experimental Setup.** We generate two datasets \(D_{1}\) and \(D_{2}\) from the original dataset \(D\) to demonstrate the gID estimation. \(D_{1}\) is a random subsample of \(D\) with only \(\{X_{1},W,R\}\) and follows \(P_{(X_{1})}(X_{1},W,R)\). \(D_{2}\) is constructed by resampling from \(D\) in a way that the confounding bias between \(X_{1}\) and \(W\), and \(X_{1}\) and \(Y\) presents, following \(P_{(X_{2})}(X_{1},W,X_{2},R,Y)\). We conducted 100 simulations by generating new instances of \(D_{1}\) and \(D_{2}\) to create the AAE plot. Estimators were constructed solely from \(D_{1}\) and \(D_{2}\), with \(D\) used exclusively to construct the ground-truth estimate.

**Experimental Results.** We evaluated the AAE(r) of estimators \(T^{}\) for \(\{,,\}\). The AAE plots for scenarios (2,3,4) are in Figs. 2(b,c,d). Our findings indicate that the MR-gID estimator \(T^{}\) consistently provided reliable estimates for the ground-truth quantity.

## 5 Conclusions

We present a framework for estimating the causal effect \(P(|do())\) by combining multiple observational and experimental datasets and a causal graph \(G\). We introduce the generalized multi-outcome sequential back-door adjustment (g-mSBD) operator (Def. 1) and its operations. We show that any g-identifiable causal effects can be expressed as a function of the g-mSBD operators as specified in Algo. 1 (Thm. 1). We then develop an estimator called DR-g-mSBD (Def. 3) for the g-mSBD operator and analyze its statistical properties in Prop. 1. Based on the DR-g-mSBD estimator, we develop the MR-gID estimator (Def. 4) and analyze its statistical properties (Thm. 2 and Coro. 2) which exhibits fast convergence and multiply-robustness. Our experimental results demonstrate that the MR-gID estimator is a consistent and robust estimator of \(P(|do())\) against model misspecification and slow convergence.

Figure 3: A graph and the AAE-plot for Project STAR.