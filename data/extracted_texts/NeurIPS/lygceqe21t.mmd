# FairJob: A Real-World Dataset

for Fairness in Online Systems

Mariia Vladimirova, Eustache Diemert

Criteo AI Lab

{m.vladimirova,e.diemert}@criteo.com &Federico Pavone

Universite Paris Dauphine-PSL

federico.pavone@dauphine.psl.eu

###### Abstract

We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairness-focused resources for high-impact domains like advertising - the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.

The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob. Source code for the experiments is hosted at https://github.com/criteo-research/FairJob-dataset/.

## 1 Introduction

The intersection of technology and human dynamics presents both opportunities and challenges, particularly in the realm of artificial intelligence (AI). Despite advancements, persistent biases rooted in historical inequalities permeate our data-driven systems, perpetuating unfairness and exacerbating societal divides. Historical biases shape data collection, influencing AI model outcomes and often _amplifying_ existing inequalities (Bolukbasi et al., 2016; Zhao et al., 2017; Chen et al., 2023). Despite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments (Andrus et al., 2021). Moreover, evolving legal frameworks, exemplified by the recent AI Act and General Data Protector Regulation (UK Information Commissioner's Office, 2022), mandate the detection, prevention, and mitigation of biases, while imposing some restrictions on the use of sensitive data.

Recent advances in fairness often involve computer vision, natural language processing and speech recognition tasks (Gustafson et al., 2023; Andrews et al., 2024; Hall et al., 2024; Schumann et al., 2024; Veliche and Fung, 2023), while lacking attention to algorithmic decision-making that involves _tabular data_, where each row represents an individual or an observation, and each column represents a feature or attribute (Le Quy et al., 2022; Zhang et al., 2021), resulting in a very few benchmark papers (Gorishniy et al., 2021, 2022; Grinsztajn et al., 2022; Shwartz-Ziv and Armon, 2022; Matteucci et al., 2023). Tabular data is commonly used in various _high-risk domains_ such as finance, healthcare, hiring, criminal justice, and advertising (van Breugel and van der Schaar, 2024).

Algorithmic discrimination in advertising can be related to sensitive verticals which highlights beneficial employment, financial and housing opportunities, or about who sees potentially less desirable advertising, such as ads for predatory lending services (Lambrecht and Tucker, 2019). While unfairness in advertising is not punitive but rather assistive, i.e. fairness consists in providing equal access to precious opportunities, it is essential to _ensure fairness in advertising practices_. In some contexts such as housing or lending, such discrimination is explicitly _prohibited by law2_. Several studies conducted analyses on the fairness in advertising at different stages and observed discriminating behavior that was not necessarily intended by the ad-services (Speicher et al., 2018; Lambrecht and Tucker, 2019; Andreou et al., 2019; Ali et al., 2019). This emphasizes the need for better mechanisms to audit and prevent bias in ads.

Most of studies on discriminating behavior in advertising were conducted via creating advertising campaigns and choosing targeted audiences and analysing the data from the user perspective without accessing the algorithmic features (Speicher et al., 2018; Lambrecht and Tucker, 2019; Andreou et al., 2019; Ali et al., 2019). The absence of publicly available, realistic datasets leads researchers to publish results based on private data, resulting in non-reproducible claims (Geyik et al., 2019; Andreou et al., 2019; Timmaraju et al., 2023; Tang and Yu, 2022). This poses challenges for critical evaluation and building upon previous work in the scientific community. Tang and Yu (2022) highlights the lack of public benchmarking datasets to study the fairness related approaches in advertising.

In addition, most of the studies assume that the AI systems have an access to the protected attributes which is often _unrealistic_ due to privacy constraints or legal restrictions (Holstein et al., 2019; Lahoti et al., 2020; Molina et al., 2023; Timmaraju et al., 2023). In online advertising, decision-makers usually have access to a log of user interactions with the system, which they can use to guess the attributes. However, the level of inaccuracy can be significant, making it difficult to ensure that an ad campaign reaches a non-discriminatory audience (Gelauff et al., 2020). This makes it hard to meet fairness requirements (Lipton et al., 2018). We emphasize the need for thorough research in real-world situations where _access to protected attributes is limited_.

Contributions.To foster research in fairness within real-world scenarios, we release a large-scale fairness-aware dataset for advertising. The dataset contains pseudononymized users' context and publisher features that were collected from a job targeting campaign ran for 5 months. The data has been _sub-sampled non-uniformly_ to avoid disclosing business metrics. Feature names have been _anonymized_ for business confidentiality, and their _values randomly projected_ to preserve predictive power while making the _recovery of the original features or user context (i.e. re-indentification) practically impossible_, with accordance to the privacy-safety measures3. Although our dataset does not contain explicit sensitive attributes such as gender, it includes a _gender proxy derived from non-protected relevant attributes_, which we discuss in detail further.

This dataset provides a baseline according to the eligible audience generated by an advertiser's targeting criteria for a specific ad. This ensures that ads are tailored to individuals whom the advertiser can feasibly serve (such as those within a specific geographic region) and who are likely to be interested in their offerings, a practice already _governed by policies and standards in Housing, Employment, and Credit verticals_. Since advertiser targeting _adheres to policy constraints to prevent discriminatory practices4_ (such as prohibiting the use of gender criteria in employment ads), the resulting eligible audience remains independent of prediction algorithms, serving as a reasonable baseline metric.

With the released dataset we examine the stages in the advertising process where unfairness can occur and explore techniques to mitigate such biases. Taking into account possible induced biases, we propose an unbiased utility metric that help to analyse different bias mitigation techniques. We also perform experiments on the released dataset to verify how we can improve fairness and the possible trade-offs with utility.

## 2 Related works

Open-source datasets.A limited availability of publicly available fairness-aware tabular datasets challenges research advancements in algorithmic fairness (Le Quy et al., 2022; Hort et al., 2023). In 2022, Le Quy et al. (2022) studied datasets used at least 3 times in research publications on fairness, and found out there were only 15 open-source fairness datasets, most of which are criticized for being too small or far from real-world scenarios, including the most frequently used Adult (Dua and Graf, 2017) and COMPAS dataset (Larson et al., 2016). Even though there is a positive tendency on addressing this issue by open-sourcing privacy-complying datasets, such as BAF (Jesus et al., 2022) for bank fraud detection where the data was obtained via data generation techniques, or WCLD (Ash et al., 2024), a curated large-scale dataset from circuit courts to address criminal justice, there is still _lack in available datasets_ in other high-impact areas such advertising. It is important for academic researchers to have access to large datasets to study the problem rigorously (L. Cardoso et al., 2019; Li et al., 2022; Le Quy et al., 2022). Large-scale datasets are advantageous as they increase the likelihood of capturing significant performance differences in experiments with new methods. With larger dataset sizes, the variance of metrics decreases, enabling more reliable and meaningful comparisons between different approaches.

Bias mitigation methods.The initial step to enhance model fairness is to exclude the protected attribute as a feature during training, a strategy known as _fairness through unawareness_(Chen et al., 2019). However, this approach alone does not ensure fairness because the model may still learn correlations between other features and the protected attributes, see Section 3.1 and Figure 2b for details. To achieve a higher level of fairness, AI systems typically employ one of the additional methods: _pre-processing, in-training, or post-processing_. We refer to Hort et al. (2023) for the most up-to-date and thorough survey.

Fairness without demographics.The information on the protected attribute is often not available in practice (Holstein et al., 2019; Hort et al., 2023). Several works studied limited availability of the protected attribute such as via a proxy (Gupta et al., 2018) or assuming there is a partial access to the information (Hashimoto et al., 2018; Awasthi et al., 2020; Molina et al., 2023). Lahoti et al. (2020) relies on the assumption that protected groups are computationally-identifiable. However, if there were no signal about protected groups in the remaining features and class labels, we cannot make any statements about improving the model for protected groups. One of the possible solutions is to get data from secure multi-party computation (Veale and Binns, 2017; Kilbertus et al., 2018; Hu et al., 2019) or directly from users (Gkiouzepi et al., 2023). However, these tools are still to be adapted to real-world situations. In addition, transfer leaning can be useful when there is little available data on the protected attributes (Coston et al., 2019).

## 3 Fairness in advertising

The aim of ad-tech companies is to deliver the most relevant advertisements to users navigating publishers' webpages. By matching users' browsing histories and content preferences with products that align with their interests, targeted advertising creates a mutually beneficial ecosystem (Wang et al., 2017; Choi et al., 2020). Advertisers reach relevant audiences, users have access to free information and services in exchange of seeing ads related to their interests, and platforms profit from selling targeted ads.

Ad-tech companies grapple with vast volumes of noisy data, which encapsulate users' past actions. Leveraging this data, they predict potential clicks and conversions. However, if the data is biased, the algorithms can inadvertently perpetuate and even amplify these biases (Bolukbasi et al., 2016; Zhao et al., 2017; Chen et al., 2023). It is crucial to scrutinize the predictors for bias and devise solutions to mitigate it. Failing to do so can result in discrepancies between offline evaluations and onlinemetrics, ultimately harming user satisfaction and trust in the service of online systems (Chen et al., 2023). While advertising commonplace items carries little risk, companies must exercise caution with high-risk verticals like job offers (Speicher et al., 2018; Lambrecht and Tucker, 2019; Andreou et al., 2019; Ali et al., 2019). For instance, _if managerial positions are disproportionately shown to men over women, more men may apply, perpetuating historical biases and exacerbating gender disparities_.

Bias can be introduced at several stages in the advertising process, see Figure 1. First, when a user visits a webpage with an ad slot, ad-tech companies participate in a real-time bidding (RTB) auction. During this auction, companies select a campaign (e.g., job offers or clothing) based on attributes of the publisher and the user, including their log of past interactions such as seen ads, their context, the fact of clicks on the ads, see Section 3.2. This auction must be organized in a fair way, respecting both the companies placing bids and the publishers providing ad slots, see Section 3.3. After an ad-tech company wins the display auction, there is the choice of which product to show (e.g., a senior position job or an assistant job). This selection can also introduce bias with respect to the user, see Section 3.4. Ensuring fairness at this stage is critical to preventing the reinforcement of existing inequalities.

### Fairness definition

We base our discussion on a counterfactual fairness framework that explains the underlying connections between the variables in the system (Kusner et al., 2017). Let \(A\) denote a protected attribute (can be a set of protected attributes) of an individual, \(X\) denote the other observable attributes of any particular individual, \(Y\) denote the outcome to be predicted, and let \(\) be a predictor. The predictor takes into account the available data from logs of user interactions with the system and product descriptions and estimates the probability of a positive outcome, i.e. click of the user on the product. The system takes into account the prediction and then shows the best product to the user, which results into a possible positive outcome. In our analysis, we are interested in understanding how \(A\) and \(X\) influence \(Y\) and how well our predictor \(\) captures these relationships. Our goal is not just to predict outcomes accurately but also to ensure fairness and mitigate biases in the predictions with respect to \(A\). These random variables exhibit causal relationships, as modeled in Fig. 2, which we further explore in detail below.

Figure 1: Simplified scheme of online advertising process of ad selection: (i) user enters a webpage with available banner for an ad, (ii) webpage sends a request to participate in the real-time bidding auction which triggers campaign selection by an ad service for a given user, (iii) after the campaign is chosen, ad-service sends a bid proposition, (iv) if the proposed bid won the auction, the recommendation engine chooses the best ad from the chosen campaign and shows it on the webpage.

Figure 2: Causal graph depicting effects of variables appearing during model training under different constraints. The arrow between the nodes corresponds to the causal effect. The dashed arrow between \(\) and \(X\) can be interpreted as \(\) depends on \(X\), but conditionally on \(X\), \(\) is independent to its ancestors.

It is important to understand how the information flows during the training procedure to create the prediction. If we give a protected attribute as a feature to the prediction model during training like in Fig. 1(a), the model will learn directly the bias \(A\). The first step is to remove the protected attribute from the training features, in this case the model might learn the bias indirectly through the features \(A X\), see Fig. 1(b). Thus, without bias mitigation techniques, the prediction model learns the bias that exists in the data. From causal perspective, the correction techniques correspond to blocking the information flow from \(A\) to \(\) by enforcing the zero mutual information between these variables conditioned on \(X\), see Fig. 1(c). We refer to Hort et al. (2023) for the survey on bias mitigation methods. Theoretically, we can mitigate the protected attribute bias when having access to the information that is used by an algorithm during training and having at least partial information about the protected attribute (Lahoti et al., 2020; Hort et al., 2023).

From a causal perspective, fair outcome with respect to a protected attribute means that it would not have changed if the other (counterfactual) value for the protected attributed was imposed (Kusner et al., 2017):

\[(Y=y\,\,do(A=a),X=x)=(Y=y\, \,do(A=a^{}),X=x).\] (1)

Since in a real-world scenarios it is not possible to have counterfactual estimation (we cannot impose a user to change their gender), we consider average values for groups, e.g. demographic parity5:

\[(Y=y|A=a)=(Y=y|A=a^{})\] (2)

This is in line with current laws that aim to ensure fairness in how housing and job ads are presented6. These laws do not focus on whether certain people are wrongly included or excluded (individual fairness), rather on making sure the ads are representative (group fairness). The key measurement is the difference in average that different groups will be shown the ad, regardless of how likely each group is to actually respond to the ad.

### Selection bias in campaign choosing

In our setting, we are interested in assessing the fairness in specific campaign (e.g., job campaign) with respect to the protected attribute. For instance, we want to ensure that job advertisements for managerial roles are fair with respect to a binary protected attribute \(A\{0,1\}\) (e.g., gender). Typically, the data considered in this framework regards the job advertisements for users which have been assigned to the job campaign \(c\). However, the campaign selection process might introduce selection bias, which should be taken in account. In particular, \((A=1)\) and \((A=0)\) are the (internet)-population level of a binary protected attribute. This might be approximated to the census population frequencies of the protected attribute. Let \(C\) be a random variable of choosing a campaign, then \((A=1 C=c)\) and \((A=0 C=c)\) are the frequencies of the protected attribute in the job campaign data \(c\). These differ from the population levels due to selection bias.

Note that the recommendation engines predict \((Y=1 A=a,C=c)\) for a product in the campaign \(c\). Thus, if we use prediction bias mitigation techniques while considering data at the campaign level, in the best case scenario, we obtain fair predictions while being unfair outside of campaign, \((=1 A=0,C=c)=(=1 A=1,C=c)\) and \((=1 A=0)(=1 A=1)\). Thus, we have to take into account the selection bias to ensure demographic parity introduced in Eq. (2). Details on the derivation of the campaign selection bias and its correction are referred to supplemental material.

### Market bias

Lambrecht and Tucker (2019) found that women are a prized demographic, making them more expensive to advertise to. This implies that ads that are meant to be gender-neutral can be delivered in the way that appears to be discriminatory by RTB algorithms that focus on optimizing cost-effectiveness. Ali et al. (2019) explained that this is not solely the indication of the ingrained cultural bias nor a result of user profiles inputted into ads algorithms, but rather the product of competitive spillovers among advertisers. Additionally, the feedback loop mechanism considers imbalanced information-how recommendation systems expose content influences user behavior, which then becomes the training data for future predictions. This feedback loop not only introduces biases but also amplifies them over time, leading to a 'rich get richer' scenario known as the Matthew effect. (Chen et al., 2023). Imbalanced data with respect to a protected attribute also effects the learning of a prediction, since an algorithm that receives in real time less data about one group, will learn at different speeds (Lambrecht and Tucker, 2020). These effects are hard to estimate and should be addressed by the RTB process. Apart from users, advertisers can also be unfairly treated during the RBT auction process (Celis et al., 2019; Chen et al., 2023) but here we focus solely on the user discrimination.

### Recommendation bias

In the ad recommendation system, the goal is to choose best products for a user for a given banner that can have several displays at the same time. The goal is to maximize the number of clicks for a given banner, meaning that there can be several products clicked. When we have several displays to show to a user, the display rank position becomes important and creates position bias with respect to a positive outcome. The influence of this bias is hard to estimate, however, it is important to take it into account (Singh and Joachims, 2018, 2019; Morik et al., 2020; Usunier et al., 2022).

Let \(J\) be a random variable denoting the set of banner to be shown to a user, \(D\) be a display (chosen product, i.e. job offer) shown to a user on a banner \(J\). Let model \(f(x,d)\) predicts the following positive outcome: \((Y=1|X=x,D=d)\), i.e. the probability of a click for a chosen product \(d\) given user features \(x\). As discussed above, we have to take into account the display position which expressed via variable rank \(R\). However, the influence of the position on the utility is hard to estimate. Further, we suggest utility metrics for ads recommendation and in order to avoid the position bias, we suggest to compute them only on randomized displays, where the position of the products on the banner was chosen randomly.

Click-rank utility.The users' utility for a given model can be expressed as a positive engagement in the following way:

\[U(f)=_{J}_{X,D|J}(Y_{D}=1)\, _{J}f(X_{J},D),\] (3)

where \((Y_{D}=1)\) is the identity function of a positive outcome (e.g. click) for display \(D\). The function \(_{D}\) computes the ascending order rank within the set of displays for a banner \(J\). This metrics is based on estimation of the positive outcome based on the passed events for chosen users.

Product-rank utility for biased data.We notice that the metrics for the algorithm can be biased due to the selection bias discussed in Section 3.2 because the prediction algorithm estimates \((Y=1|A=a,C=c)\) instead of \((Y=1|A=a)\). Even if we correct the prediction bias in \((Y=1|A=a,C=c)\) based on the data provided for given campaign \(c\), it does not correct the final bias in \((Y=1|A=a)\) due to selection bias. We can adapt the click-rank utility to include possible selection bias into the metric, by explicitly considering that the product utility depends on a chosen campaign. Then, when correcting for the unfairness in the prediction, we might improve the utility metric taken into account the selection bias in the data:

Figure 3: Causal graph depicting effects of variables appearing during model training for an ad recommendation system under different constraints.

\[(f)=_{D}_{J|D}[(Y_{D}=1)\, (A=a_{X_{J}})}{(A=a_{X_{J}}|C=c)}\,_{J}f(X_ {J},D)],\] (4)

where \(a_{X}\) stands for a gender of a given user \(X\). Intuitively, if the prediction is biased with respect to the protected attribute \(A\), the final prediction \((Y=1|A=a)\) is even more biased due to selection bias with respect to the protected attribute of choosing a campaign \(C=c\): \((C=c A=a)\). In this case, the prediction model amplifies the existing historical bias. However, we can remove the selection bias by adding weights that correspond to the presence of the protected attribute in the whole population and given the campaign. If the user with protected attribute \(A=a\) has lower probability of click, and this group was underrepresented in the campaign \(C=c\), i.e. \((A=a)>(A=a|C=c)\), then in the utility function, the model's prediction will be higher, by addressing the possible bias due to under-representation in the data. This is our suggested metric to evaluate the recommendation system when the selection bias is present and known such as in the FairJob dataset.

## 4 FairJobs dataset

We introduce FairJobs7 dataset that contains fairness-aware data from a real-world scenario of advertising. The intended use of this dataset is to learn click predictions models and evaluate by how much their predictions are biased between different gender groups. The dataset consists of 1,072,226 rows that were collected during 5 months of a targeted job campaign8, each row represents a job and user features: 20 categorical and 39 numerical features; label click (binary, if the ad was clicked), protected_attribute (binary, proxy for user gender, see below for more thorough explanation), senior (binary, if the job offer was for a senior position), [user_id, impression_id, product_id] are unique identifiers of user, impression and product (job ad). More details and dataset statistics are referred to Appendix.

Details on gender proxy.Since we do not directly access user demographics, we have to find a way to get a proxy of relevant attribute9. Most of recent works leverage the use of external data or prior knowledge on correlations to obtain proxies to relevant attributes (Gupta et al., 2018; Hashimoto et al., 2018; Awasthi et al., 2020; Lahoti et al., 2020). We define a product gender, either given by a client, either by a category of the product. This gives us approximately 40% of products gender identified. Then, we follow the available statistics and choose the gender proxy based on the dominant gender of products the user interacts with. This gender proxy identities a behavior of a user, i.e. if a user tends to buy female or male products. The gender proxy does not necessarily correlate with the gender, as it often happens with the proxy variables (Gelauff et al., 2020). Verification of the accuracy of these approximations is challenging. Additionally, if there are no signal about protected groups in the remaining features and class labels, we cannot make any statements about improving the model for protected groups (Lahoti et al., 2020).

Limitations and interpretation.We remark that the proposed gender proxy does not give a definition of the gender. Since we do not have access to the sensitive information, _this is the best solution we have identified at this stage to idenitify bias on pseudonymised data_, and we encourage any discussion on better approximations. This proxy is reported as binary for simplicity yet we acknowledge gender is not necessarily binary. Although our research focuses on gender, this should not diminish the importance of investigating other types of algorithmic discrimination. While this dataset provides important application of fairness-aware algorithms in a high-risk domain, there are several fundamental limitation that can not be addressed easily through data collection or curation processes. These limitations include historical bias that affect a positive outcome for a given user, as well as the impossibility to verify how close the gender-proxy is to the real gender value. Additionally,there might be bias due to the market unfairness that we explained in Section 3.3. Such limitations and possible ethical concerns about the task should be taken into account while drawing conclusions from the research using this dataset. Readers should not interpret summary statistics of this dataset as ground truth but rather as _characteristics of the dataset_ only. Additional limitation comes from identifying the senior position label, as the definition of what constitutes a "senior" position can be subjective. We acknowledge that this method may introduce some noise, particularly if job titles are unconventional or if errors occur in categorization. Finally, we remark that in the dataset we assume that each user_id represents a single user; however, we acknowledge that multiple users could share one device, potentially affecting the user features. Additionally, user_id's are based on the company's identification technology, which means that a single user could have multiple user_id's across different browsing sessions. This is one of the complexities inherent in real-world data, particularly in online advertising. Such biases can influence model training, bias evaluation, and bias mitigation efforts.

## 5 Empirical observations

Challenges.The first challenge comes from handling the different types of data that are common in tables, the _mixed-type columns_: there are both numerical and categorical features that have to be embedded (Gorishniy et al., 2021, 2022; Grinsztajn et al., 2022; Shwartz-Ziv and Armon, 2022; Matteucci et al., 2023). In addition, some of the features have long-tail phenomenon and products have popularity bias, see Figure 4. Our datasets contains more than 1,000,000 lines, while current high-performing models are under-explored in _scale_, e.g. the largest datasets in Grinsztajn et al. (2022) are only 50,000 lines, while in Gorishniy et al. (2021, 2022) only one dataset surpasses 1,000,000 lines. Additional challenge comes from _strongly imbalanced data_: the positive class proportion in our data is less than 0.007 that leads to challenges in training robust and fair machine learning models (Jesus et al., 2022; Yang et al., 2024). In our dataset there is no significant imbalances in demographic groups users regarding the protected attribute (both genders are sub-sampled with 0.5 proportion, female profile users were shown less job ad with 0.4 proportion and slightly less senior position jobs with 0.48 proportion), however, there could be a hidden effect of a bias that we discussed in Section 3. This poses a problem in accurately assessing model performance (van Breugel et al., 2024). More detailed statistics and exploratory analysis are referred to the supplemental material.

Baselines.We choose two baseline regimes: (i) unfair, that uses all attributes for training, including the protected one; and (ii) unaware, that corresponds to fairness through unawareness, i.e. using all attributes during training except the protected one. We train (i) a Dummy classifier in the unaware regime to obtain the first baseline and (ii) XGB in two regimes (unaware and unfair) to achieve a more reasonable baseline performance, see Table 1. We notice that the Dummy classifier is perfectly fair with respect to demographic parity DP which is reasonable since it did not learn the dependence between features at the label at all, as can be seen from the negative log-likelihood NLLH results. However, the utility metrics \(U\) and \(\) of Dummy do not differ much from XGB which is due to very strong imbalance in the data. We remark that \(\) is expectedly higher for fairer (unaware) models, while \(U\) is better for the unfair model. There is a slight difference in terms of NLLH and AUC-ROC for

Figure 4: Examples of some feature statistics in FairJob dataset: number of impressions per user and banner size have long tail phenomenon (two plots on the left). The products have popularity bias (right plot), i.e. some products have much higher or lower than average number of clicks with senior job ads having more clicks on average.

XGB models, and higher \(U\) corresponds to higher AUC-ROC. We refer to Appendix B and C for more details.

Fair regime.To study a possible trade-off between utility and fairness, we use in-processing fairness methods of adding a fairness-inducing penalty to a loss function \((,Y)\) during training (Kamishima et al., 2011):

\[=\ \ (,Y)+(,Y,A),\] (5)

where parameter \(\), or fairness_multiplier, controls the trade-off between the model's predictive accuracy \((,Y)\) and fairness. Adjustments on \(\) allows to control the importance of fairness relative to accuracy. These methods remove the influence of protected attribute on the model's output without restrictions on the data (Kamishima et al., 2011; Bechavod and Ligett, 2017; Mary et al., 2019). We implement a penalty based on the approach described in Bechavod and Ligett (2017).

We train a logistic regression in the two baseline regimes and compare the results of these algorithms with a (iii) fair model that is trained without protected attribute with an additional fairness-inducing penalty (Kamishima et al., 2011; Bechavod and Ligett, 2017). The three models correspond to the situations described in Figure 2. We refer all the reproducibility details and additional experiments to Appendix B and C. The resulted prediction can be visually compared in Figure 5.

Fairness-utility trade-off.We illustrate the possible trade-off between performance and fairness metrics for the logistic regression model when varying fairness_multiplier, see Figure 6. We notice that for positive fairness_multiplier, DP improves, while NLLH degrades. For fairness_multiplier = 0.5 and 1.0 we notice slight improvements in utility metrics, especially \(\), with respect to the unfair model represented as a dashed line.

Additionally, we propose to restraint an access to the protected attribute and study the trade-off when we train the model on the whole train set but add fairness penalty only for some percentage of train set. In some scenarios, we could see improvements in fairness without sacrificing the overall performance. The loss in accuracy due to the imposed fairness constraints is often small as also noted in other works (Celis et al., 2019). We explore bias correction techniques tailored to address data limitations and preserve utility in large-scale. We demonstrate how prioritizing fairness in AI not only benefits users by fostering inclusivity but also contributes to the long-term success and ethical integrity of companies.

    & NLLH \(\) & AUC \(\) & DP \(\) & \(U\) & \(\) \\  Dummy unaware & 0.69239 & 0.50000 & **0.00000** & 0.01009 & 0.01245 \\ XGB unaware & **0.05491** & 0.75787 & 0.00278 & 0.01017 & **0.01276** \\ XGB unfair & 0.05736 & **0.76201** & 0.00323 & **0.01037** & 0.01236 \\   

Table 1: Performance comparison for single simulation of Dummy classifier (unaware) and XGBoost (unaware and unfair) with 100 trials for tuning.

Figure 5: Probability density distributions of click for different values of the protected attribute of three models trained in different ways: (i) _unfair_ – with a protected attribute included as a feature during training, (ii) _unaware_ – corresponds to fairness through unawareness, (iii) trained _with fairness penalty_ as a bias mitigation technique.

These findings suggest that the trade-off relationship between accuracy and fairness is context-dependent. It highlights the need for further research to better understand the conditions under which the accuracy fairness trade-off arises and identify strategies to mitigate or overcome it.

## 6 Conclusion

Addressing bias in AI goes beyond mere compliance with legal frameworks like the AI Act; it necessitates proactive measures to detect, prevent, and mitigate biases. Drawing from real-world challenges faced by industries, we highlight the limitations of existing bias mitigation strategies, particularly in environments where access to sensitive user attributes is restricted. We encourage other authors and practitioners to experiment with different AI or Fair AI algorithms on this dataset. We argue that specific problems can often be generalized to broader contexts. For example, if our dataset helps identify a method that effectively balances fairness and utility, this method could potentially be applicable to other recommendation systems across various domains. We expect that with this work, the quality of evaluation of novel AI methods increases, potentiating the development of the area, see more details in the broader impact section in Appendix E. Additionally, we hope it encourages other similar relevant datasets to be published from other authors and institutions.

## 7 Acknowledgements

Federico Pavone has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101034255. We also thank Martin Bompaire, David Rhode and Andre Cunha for helpful discussions.