# Decompose a Task into Generalizable Subtasks in Multi-Agent Reinforcement Learning

Zikang Tian\({}^{1,2,3}\), Ruizhi Chen\({}^{4}\), Xing Hu\({}^{1,5}\), Ling Li\({}^{2,4}\), Rui Zhang\({}^{1}\), Fan Wu\({}^{2,3,4}\),

**Shaohui Peng\({}^{4}\), Jiaming Guo\({}^{1}\), Zidong Du\({}^{1,5}\), Qi Guo\({}^{1}\), Yunji Chen\({}^{1,2}\)**

\({}^{1}\)SKL of Processors, Institute of Computing Technology, CAS, Beijing, China

\({}^{2}\)University of Chinese Academy of Sciences, Beijing, China

\({}^{3}\)Cambricon Technologies, Beijing, China

\({}^{4}\)Intelligent Software Research Center, Institute of Software, CAS, Beijing, China

\({}^{5}\)Shanghai Innovation Center for Processor Technologies, SHIC, Shanghai, China

{tianzikang21s, cyj}@ict.ac.cn,

Corresponding author.

###### Abstract

In recent years, Multi-Agent Reinforcement Learning (MARL) techniques have made significant strides in achieving high asymptotic performance in single task. However, there has been limited exploration of model transferability across tasks. Training a model from scratch for each task can be time-consuming and expensive, especially for large-scale Multi-Agent Systems. Therefore, it is crucial to develop methods for generalizing the model across tasks. Considering that there exist task-independent subtasks across MARL tasks, a model that can decompose such subtasks from the source task could generalize to target tasks. However, ensuring true task-independence of subtasks poses a challenge. In this paper, we propose to **d**ecompose a **t**ask **into** a series of **g**eneralizable **s**ubtasks (DT2GS), a novel framework that addresses this challenge by utilizing a scalable subtask encoder and an adaptive subtask semantic module. We show that these components endow subtasks with two properties critical for task-independence: avoiding overfitting to the source task and maintaining consistent yet scalable semantics across tasks. Empirical results demonstrate that DT2GS possesses sound zero-shot generalization capability across tasks, exhibits sufficient transferability, and outperforms existing methods in both multi-task and single-task problems.

## 1 Introduction

In the last few years, many works in MARL field prospers, including value-based algorithms [26; 25; 20; 27; 30; 39; 10; 17], and policy-based algorithms [6; 36; 35; 11; 24; 7]. However, these works mainly focused on the model's asymptotic performance in a single task, neglecting its transferability across tasks. Training an optimal model on a single task requires millions of interactions with the environment [21; 16], particularly when dealing with large-scale Multi-Agent Systems, whereas transferring the model across tasks can reduce training cost by dozens of times [2; 5]. As the number of tasks increases, the reduction in training costs due to model transfer will become even more significant.

Knowledge reuse is a common approach for model generalization across tasks in the MARL field . Recent knowledge reuse methods for online MARL can be roughly classified into two categories: network-design-based methods and task-embedding-based methods. Network-design-based methods [33; 9; 1] implicitly reuse knowledge extracted from the source task to the target task by constructinguniversal model structure across tasks by utilizing Population Invariant Structure such as Transformer  or GNN . However, it's unclear whether the knowledge extracted from the source task is suitable for the target task. Task-embedding-based methods [19; 23; 14] reuse knowledge by calculating task similarity using learned task embeddings that capture task dynamics. However, accurately mapping task dynamics to task embeddings requires numerous tasks as samples. In a word, current knowledge reuse methods for online MARL still have limitations, that is, inefficient knowledge reuse and reliance on a large number of tasks samples. Additionally, we provide a section for related work in Appendix A to introduce these methods more specifically.

Compared with the common knowledge reused in these methods above, there exists an alternative knowledge named task-independent subtasks that can relieve these two limitations. These task-independent subtasks, such as "hit and run", "focus fire", "disperse and line up", etc, are certainly applicable across tasks , improving the efficiency of knowledge reuse. Besides, these task-independent subtasks can be decomposed from few tasks, so as to remove the reliance on a large number of tasks samples. However, ensuring the task-independence of decomposed subtasks is a challenge. Task-independence of subtasks refers to their effectiveness across tasks, which requires two essential properties: **(1) avoiding overfitting to the source task, (2) maintaining consistent yet scalable semantics across tasks**. In this paper, we propose DT2GS, a novel framework devoted to generalize model across tasks by decomposing subtasks from the source tasks and endowing them with these two properties necessary for task-independence.

The proposed framework, DT2GS, aims to decompose a task **into** a series of **g**eneralizable **s**ubtasks, as shown in Figure 1. DT2GS is primarily composed of two parts: the scalable subtask encoder and the adaptive action decoder. The scalable subtask encoder assigns a subtask to each agent based on its {subtask, entity-observation} history rather than the {action, observation} history. This approach helps prevent the process of assigning subtasks from overfitting to the source task. The adaptive action decoder leverages the assigned subtasks and current entity-observations to calculate the specific actions for interacting with environment. Within this decoder, the adaptive subtask semantic module ensures that the assigned subtasks have consistent yet scalable semantics across tasks based on their effects on entities. Based on the experimental results, our framework exhibits several desirable properties as follows:

* **zero-shot generalization capability**: the trained model can be effectively deployed to multiple target tasks without any fine-tuning;
* **robust transferability**: significantly accelerated model convergence on complex target tasks and achieving an average speedup of \(100\);
* **better asymptotic performance**: achieved state-of-the-art (SOTA) results on multi-task and single-task problems;
* **better subtask interpretability**: decomposed task-independent subtasks with practical behavioral semantics that is consistent yet scalable across tasks.

## 2 Preliminary

### Background

In this paper, a fully cooperative multi-agent system (MAS) is described as a decentralized partially observable Markov decision processes (Dec-POMDP) , which is defined by a tuple \(G=<,,O,,n,,,R,P>\). \(\) is the global state space. \(\) is the action space shared for all agents. \(O\) is the shared individual observation space for all agents. \(\) is the discount factor. \(n\) is the number of agents. \(=\{1,...,n\}\) is the set of agents our algorithm controlled. At each timestep \(t\), agent \(i\) obtains an individual observation \(o_{i}^{t} O\) from dynamic environment according to the observation function \((s^{t},i)\). And if we suppose agent \(i\) are controlled by policy \(\), which takes \(o_{i}^{t}\) or history individual observations \(_{i}^{t}\) as input and parameterized by \(_{i}\), agent \(i\) will select an action \(o_{i}^{t}\) according to \((a_{i}^{t}|o_{i}^{t})\). Therefore, a joint action \(A^{t}=(a_{1},a_{2},...,a_{n})\) will be formed where \(a_{i}\) corresponds to agent \(i\). After passing the joint action \(A^{t}\) into the environment, a global reward signal \(r_{t}=R(s^{t},A^{t})\) shared by all agents will be received and the environment will transmit into next state \(s^{t+1}\) according to the transition function \(P(s^{t+1}|s^{t},A^{t}):\). The goal of agents is defined as finding the policy \(\) to maximize the objective function

\[J()=[_{t^{}=0}^{}^{t^{}}R(s^{t^{}}, A^{t^{}})] \]

where \(^{t^{}}R(s^{t^{}},A^{t^{}})\) is the discounted return of all agents.

### Generalizable model structure in MARL

For model generalization in MAS, a generalizable model structure is necessary for addressing the problem of varying state/observation/action space (\(/O/\)) across tasks. Here we define the agents controlled by MARL policy and agents built-in tasks as entities. In this paper, we use \(n,m,n_{ally},n_{enemy}\) denotes the number of agents, entities, allies and enemies, respectively. And the following equation holds: \(n=n_{ally}+1,m=n+n_{enemy}\). As demonstrated in ASN , an agent's observation \(o_{i}\) can be constructed as a concatenation of \(m\) entity-observations: \(o_{i}=[o_{i,1},o_{i,2},...,o_{i,m}]\), where \(o_{i,1}\) is the observation of agent \(i\) on itself and environment, and the rest are the observations of agent \(i\) on other \(m-1\) entities. Additionally, action space \(\) can be decomposed into two components: \(^{self}\), which consists of actions affecting the agent itself or the environment, and \(^{interactive}\), which contains actions that directly interact with other entities. This alignment between entity-observations and actions, which is referred as action semantics, forms the foundation for computing the value or probability of an action based on its aligning entity-observation, leading in a generalizable model structure across tasks.

## 3 DT2GS Framework

DT2GS primarily consists of two components: the scalable subtask encoder and the adaptive action decoder, as shown in Figure 1. DT2GS begins with the scalable subtask encoder, which assigns a subtask to each agent based on its {subtask, entity-observation} history instead of the {action, observation} history, so as to avoid overfitting to the source tasks. Then the adaptive action decoder calculates the specific actions for interacting with environment by leveraging the assigned subtasks

Figure 1: DT2GS Framework. The DT2GS Framework comprises two modules: the Scalable Subtask Encoder and the Adaptive Action Decoder, where the Adaptive Subtask Semantic module serves as the core of the Adaptive Action Decoder. The Scalable Subtask Encoder effectively assigns subtasks to agents without overfitting to the source task. With the Adaptive Subtask Semantic module endowing assigned subtasks with consistent yet scalable semantics across tasks, the action decoder takes the entity-observations and subtasks as inputs to generate actions for interacting with the environment.

and current entity-observations. Within this decoder, the adaptive subtask semantic module, which plays a core role, ensures that the assigned subtasks have consistent yet scalable semantics across tasks based on their effects on entities. With the proposed scalable subtask encoder and the adaptive subtask semantic module, DT2GS endows the decomposed subtasks with task-independence, leading model generalizable across tasks. A pseudocode is provided in Appendix H to illustrate the DT2GS framework.

### Scalable Subtask Encoder

An agent's history, which comprises its observations and behavioral history, contains valuable information about its characteristics, such as velocity, attack distance, etc. Therefore, analyzing an agent's history can help us identify the most suitable subtask for the agent to perform. Typically , a recurrent neural network, such as LSTM  or GRU , is employed to construct the agent's history representation from its {action, observation} history. The history representation is then passed to a softmax operator, which samples a one-hot subtask for the agent. However, since actions and observations are task-dependent, this approach may lead to subtask encoder that overfits to the source task, making the source model non-generalizable. Considering that the subtasks history of an agent not only reflects its behavior history, but also is task-independent, and the entity-observations history is more general across tasks than observations history, our scalable subtask encoder assigns a subtask to each agent based on its {subtask, entity-observation} history rather than {action, observation} history, as illustrated in the **red solid-line box** in Figure 2.

The detail of the scalable subtask encoder is demonstrated in Figure 1 right. By replacing observations with entity-observations, we first design a structure that can obtain fixed-dimensional observation embeddings, regardless of the number of entity-observations contained in the observation. Additionally, since permuting the order of entity-observations in the observation does not change the information, we use a Permutation Invariant operator, namely, Gaussian Product, to obtain the observation embedding. Specifically, we first use a MLP parameterized by \(_{e}\) to embed the entity-observation as an entity-embedding:

\[(_{i,j}^{t},_{i,j}^{t})=f_{_{e}}(o_{i,j}^{t}),j=1,...,m \]

and the observation embedding \(e_{i}^{t}\), which is also referred to as Env Cognition of agent \(i\) in Figure 1, is constructed as:

Figure 2: Scalable Subtask Encoder. The Scalable Subtask Encoder assigns subtasks to each agent based on its behavioral history representation, which is depicted by its {subtask, entity-observation} history instead of commonly used {action, observation} history. The red dashed-line box denotes the {action, observation} history, which is task-dependent. Specifically, at each timestep \(t\), agent \(i\) obtains its subtask \(k_{i}^{t}\) by utilizing its observation \(o_{i}^{t}\), last action \(a_{i}^{t-1}\), history latent embedding \(h_{i}^{t-1}\) produced by RNN. On the contrary, the **red solid-line box** denotes the {subtask, entity-observation} history, which is task-independent. Specifically, at each timestep \(t+1\), agent \(i\) obtains its subtask \(k_{i}^{t+1}\) by utilizing its entity-observation \(\{o_{i,j}^{t+1}\}=[o_{i,1}^{t+1},o_{i,2}^{t+1},...,o_{i,m}^{t+1}]\), last subtask \(k_{i}^{t}\), history latent embedding \(h_{i}^{t}\) produced by RNN.

\[e_{i}^{t}(_{i}^{t},_{i}^{t}),\ where\ (_{i}^{t}, _{i}^{t})_{j=1}^{m}(_{i,j}^{t},_{i,j}^{t}) \]

Then we utilize a trajectory encoder based on GRU, which is parameterized by \(_{h}\), to obtain an agent's history representation. The GRU takes an agent's observation embedding \(e_{i}^{t}\), last subtask \(k_{i}^{t-1}\) and previous hidden state \(h_{i}^{t-1}\) as inputs, and generates a new hidden state \(h_{i}^{t}\) as its history representation:

\[h_{i}^{t}=f_{_{h}}(e_{i}^{t},k_{i}^{t-1},h_{i}^{t-1}) \]

Afterwards, we use the Gumbel-Softmax trick with a categorical reparameterization  to assign a subtask \(k_{i}^{t}\) to agent \(i\) based on its current history representation \(h_{i}^{t}\):

\[k_{i}^{t}(h_{i}^{t}) \]

where \(k_{i}^{t}^{n_{k}}\) is a \(n_{k}\)-dimensional one-hot vector and \(n_{k}\) is a hyperparameter denoting the total number of subtasks. The Gumbel-Softmax operator allows our process of subtask assignment trainable by the way of gradient backpropagation.

### Adaptive Subtask Semantics

The generalizable subtasks should maintain consistent yet scalable semantics across tasks. The traditional method of constructing subtask semantics is embedding one-hot subtasks into vector representations based on MLP. However, this approach endows target subtasks with completely identical semantics as source subtasks when deploying the source model to the target tasks, neglecting the difference between source and target tasks so as to restrict the zero-shot generalization of subtask semantics across tasks. To overcome this limitation, our adaptive subtask semantic module leverages the inductive bias that subtask semantics refers to the effects of an agent on entities when it performs a given subtask. This approach endows subtasks with semantics that can adaptively adjust based on the agent's effects on entities, thereby enabling greater zero-shot generalization of subtask semantics across tasks.

Figure 3: Adaptive Subtask Semantic module. The Adaptive Subtask Semantic Module endows subtasks and actions with adaptive semantics. \(z_{k}\) denotes the embedding of subtask \(k\). Entity-observations are denoted by \(o=[o^{self},o^{ally_{1}},...,o^{ally_{n_{ally}}},o^{ enemy_{1}},...,o^{ enemy_{n_{ enemy}}}]\) (we replace \(n_{enemy}\) with \(n_{e}\) in Figure 3 for convenience). With subtask embedding and entity-observations, we obtain the semantics \(_{k}\) of subtask \(k\) and the semantics of actions \(=[^{self},^{ally_{1}},...,^{ally_{n_{ally}}}, ^{enemp_{1}},...,^{enemp_{n_{enemy}}}]\) by utilizing the Attention mechanism .

Since a subtask's semantics actually refers to the effects of an agent on entities when it performs this subtask, we construct subtask semantics as the weighted sum of entities, where the weight represents the degree of effects. And considering that we have obtained one-hot subtask and entity-observations, we utilize the Attention mechanism  without position embedding, which is scalable across tasks regardless of the number of entities and permutation invariant about the order of entity-observaitons in the observation, to obtain the subtask semantics, as illustrated in Figure 3. Specifically, we take one-hot subtask embedding

\[z_{i}^{t}=(k_{i}^{t}) \]

as the query and entity-observations' embedding as the keys as well as values: \(_{i}^{t}=W_{Q}z_{i}^{t},K_{i}^{t}=W_{K}o_{i}^{t},V_{i}^{t}=W_{V}o_{i}^{t}\), where \(W_{Q},W_{K},W_{V}\) are learnable parameters and \(o_{i}^{t}=[o_{i,1}^{t},o_{i,2}^{t},...,o_{i,m}^{t}]\). Then we construct adaptive subtask semantics \(_{i}^{t}\) as follows:

\[_{i}^{t}=(_{i}^{t}{K_{i}^{t}}^{T}}{} })V_{i}^{t},_{i}^{t}=W_{Q}z_{i}^{t} \]

where \(d_{K}\) is the feature dimension of \(K_{i}^{t}\). Additionally, to make the process of obtaining actions scalable across tasks, we extended the model structure of ASN . Since the alignment between actions and entity-observations may discrepant from the prior, we construct the adaptive action semantics \(_{i}^{t}=[_{i,1}^{t},_{i,2}^{t},...,_{i,m}^{t}]\) by utilizing the Self-Attention mechanism taking the entity-observations as inputs:

\[_{i}^{t}=(^{t}{K_{i}^{t}}^{T}}{}})V _{i}^{t}, Q_{i}^{t}=W_{Q}o_{i}^{t} \]

Subsequently, we calculate the value or probability of each action in current observation by comparing the similarity of subtask semantics with action semantics:

\[Q_{value}(a_{j}|o_{i}^{t})\ or\ Pr(a_{j}|o_{i}^{t})=similarity(_{i}^{t}, _{i.j}^{t}) \]

where \(similarity\) is a trainable MLP taking the concatenate of \(_{i}^{t}\) and \(_{i.j}^{t}\) as inputs.

## 4 Experiments

### Experimental Setup

We evaluated the performance of DT2GS on the StarCraft Multi-Agent Challenge (SMAC)  and the multi-agent particle world environments (MPE)  (shown in Appendix D). SMAC contains several tasks that are similar but different, such as the _marine_-series tasks ({3m, 8m, 8m vs 9m, 10m_vs_11m, 25m, 27m_vs_30m}) and the _stalker_zealot_-series tasks (2s3z, 3s5z, 3s5z_vs_3s6z). Besides, changing the number of agents and landmarks in MPE can also form a series of tasks. These tasks met our requirements of cross-task generalization, allowing us to evaluate the ability of DT2GS to generalize across different tasks.

In the generalization capability part of experiments, we selected ASN  and UPDeT  as baselines. ASN was chosen because it promotes the development of universal models across tasks in MARL. For the sake of fairness in comparison, we make ASN generalizable across tasks by utilizing the attention mechanism and use "ASN_G" to denote this generalizable ASN. UPDeT was selected because it constructs a universal model across tasks via policy decoupling with self-attention, which is also used in the adaptive subtask semantic module of DT2GS in Sec 3.2. In addition, we implemented DT2GS, UPDeT, and ASN_G based on MAPPO , which is considered SOTA in on-policy MARL.

In the asymptotic performance part of experiments, we added another five baselines for comparison: MAPPO, LDSA , ROMA , RODE , and HSD . LDSA, ROMA, RODE, and HSD focus on concepts like skills/options/roles in MARL, which are similar to subtasks studied in our method.

Additionally, We set the number of subtasks \(n_{k}\) to 4 and averaged all results over 4 random seeds. And the experiments are arranged as follows: Firstly, we evaluated the zero-shot generalization capability of DT2GS across tasks. Secondly, we analyzed the practical semantics of subtasks on the _marine_-series tasks. Thirdly, we conducted several experiments to exhibit the transferability of DT2GS across tasks. Finally, we demonstrated that DT2GS achieves SOTA performance on multi-task and most single-task problems in terms of asymptotic performance.

### Zero-Shot Generalization across Tasks

In this section, we designed 8 different zero-shot generalization scenarios, where each scenario includes a target task that is more difficult than the source task. That is to say, the relationship from the source task to the target task is extrapolated. For example, the target task may be larger in scale (like 8m \(\) 25m) or contain a greater disparity in military strength than the source task (3s_vs_4z \(\) 3s_vs_5z). We deployed the model trained on the source tasks to the target tasks without any finetune. As shown in Figure 4, DT2GS significantly outperforms UPDeT and ASN_G in terms of zero-shot generalization capability, achieving an average test winning rate surpass of about 22% and 34%, respectively, over all 8 zero-shot generalization scenarios.

### Analysis of Subtask Semantics

In this section, we analyzed the practical semantics of subtasks based on the _marine_-series tasks. We first trained a DT2GS model from scratch on the 8m task and then deployed it to 8m and 10m_vs_11m, as shown in Figure 5 left. In the 8m and 10m_vs_11m tasks, the DT2GS agents initially utilized subtask_1 to scatter the formation and advance towards the enemy at \(t=1\). When the enemies reached the attack range of DT2GS agents at \(t=6\), most of these agents selected subtask_2 to focus fire on enemies while also paying attention to their own health in order to move back when in danger, thereby avoiding the decrease in firepower caused by personnel reduction. And other agents chose subtask_4 to slightly adjust their relative position with the enemies to seek a better position for firepower output. At \(t=13\), some agents sensed that their allies' health was too low while their own health was safe enough. These agents switched subtask from subtask_2 to subtask_3, which means charging. And the specific behavior was to attack while moving closer to the enemies' position to attract hatred so as to avoid the decrease in firepower caused by the death of allies. At the end of the battle (after \(t=21\)), all agents changed their subtasks to subtask_3 to concentrate their firepower to end the battle. We observed that under the subtask of charge, even if an agent was close to death, it would not move back to disperse hatred as overall withdrawal would lead to failed combat.

Furthermore, we conducted zero-shot generalization experiments on additional _marine_ tasks, and the corresponding change process of subtask percentage is shown in Figure 6. We observed that the change process of each subtask's percentage in the source task (8m) is quite similar to that in the target tasks (10m_vs_11m, 5m_vs_6m, 8m_vs_9m, 25m, and 27m_vs_30m). This result demonstrates that the subtasks decomposed by DT2GS from the source task (8m) are task-independent across tasks and

Figure 4: The figure shows a comparison of zero-shot generalization capability between DT2GS, UPDeT, and ASN_G across various source and target tasks. The horizontal axis represents the source task \(\) the target task, where (a) 3s_vs_4z \(\) 3s_vs_5z, (b) 2s3z \(\) 3s5z, (c) 3s5z \(\) 3s5z_vs_36z, (d) 8m \(\) 8m_vs_9m, (e) 8m \(\) 10m_vs_11m, (f) 8m \(\) 25m, (g) 8m_vs_9m \(\) 25m, and (h) 8m_vs_9m \(\) 5m_vs_6m. The vertical axis represents the winning rate when deploying the source model to target task without any finetue. The red, green, and purple histograms correspond to DT2GS, UPDeT, and ASN_G, respectively. The missing histograms indicate that the winning rate of deploying the source model to target task is 0%.

therefore effective for the target tasks. Both results from Figure 5 and Figure 6 further suggest that DT2GS policy has sound zero-shot generalization capability by decomposing the task into a series of task-independent subtasks.

### Transferability across Tasks

Figure 7 (a) illustrates four scenarios that we designed to evaluate the transferability of DT2GS policy. In all four scenarios, the relationships from the source task to the target task are extrapolation, including an increase in entity types (3s5z_vs_3s6z \(\) 1c3s5z), an increase in the number of enemies (3s5z \(\) 3s5z_vs_3s6z and 3s_vs_4z \(\) 3s_vs_5z), as well as an increase in the total scale of entities (8m_vs_9m \(\) 27m_vs_30m).

As we can see, DT2GS_finetune demonstrated significantly efficient convergence as well as higher and more stable asymptotic performance compared with UPDeT_finetune and ASN_G_finetune in all transfer scenarios. Notably, in the scenario where the source task was 3s5z (easy) and the target task was 3s5z_vs_3s6z (superhard), DT2GS_finetune achieved optimal performance with only 6400 steps interaction with environment, whereas learning from scratch required at least 6 million steps. Furthermore, compared to other baselines of learning from scratch, DT2GS_finetune accelerated

Figure 5: Visualization of the practical semantics of subtasks (left) and the change process of subtask percentage (right) in 8m (up) and 10m_vs_11m (down). The results for both tasks are obtained from the source model’s evaluation on the corresponding task, where the source model is acquired by learning from scratch in task 8m.

Figure 6: Changes of subtask percentage when deploying the source model, which is learned from scratch on 8m, to 6 \(marine\) tasks. The subtask percentage is calculated as \(p(k)=}{N_{k}}\), where \(p(k)\) is the percentage of subtask \(k\), \(N_{k}\) is the number of agents that select subtask \(k\), and \(N_{a}\) is the total number of agents.

the model's convergence on target tasks by an average of \(100\). In conclusion, DT2GS_finetune exhibited better performance than baselines on the following evaluation metrics [28; 4]: jumpstart, time to threshold and asymptotic performance, demonstrating significantly sufficient transferability.

### Performance on Multi-task and Single-task

In this section, we first designed 2 multi-task problems, including the _marine_-series tasks ({3m, 8m, 8m_vs_9m, 10m_vs_11m}) and _stalker_zealot_-series tasks ({2s3z, 3s5z, 3s5z_vs_3s6z}), to demonstrate the representational capacity of DT2GS. In each multi-task problem, the policy interacted synchronously with multiple tasks that make up this multi-task to collect data, which was shuffled and used to update the policy. As shown in Figure 8, DT2GS exhibited better learning efficiency and more stable asymptotic performance compared with UPDeT and ASN_G.

Subsequently, we also compared DT2GS with baselines on single-task scenarios. We selected four representative scenarios, including superhard / hard tasks, as shown in Figure 7 (b), while performance on other scenarios is presented in the Appendix B. Our evaluation metrics included learning efficiency and final asymptotic performance. Compared to MAPPO, DT2GS significantly improved both metrics, particularly in superhard tasks such as 3s5z_vs_3s6z and 6h_vs_8z. In addition, compared to other baselines based on MAPPO, including UPDeT and ASN, DT2GS consistently outperformed them in terms of learning efficiency and asymptotic performance, on average.

Figure 8: Comparison of performance on multi-task problems between DT2GS and baselines, including UPDeT and ASN_G.

Figure 7: (a) Comparison of transferability between DT2GS and baselines, where baselines include transfer of UPDeT (UPDeT_finetune), ASN_G (ASN_G_finetune) and learning from scratch of DT2GS, UPDeT, ASN, MAPPO. (b) Comparison of performance on Single-Task between DT2GS and baselines, including UPDeT, ASN, MAPPO, HSD, RODE, ROMA and LDSA, on 3 superhard tasks (3s5z_vs_3s6z, 6h_vs_8z, corridor) and 1 hard task (5m_vs_6m).

Conclusion

Model generalization has emerged as a promising approach to reduce training costs. In this paper, we proposed DT2GS, an effective approach for addressing the problem of model generalization across tasks in the MARL field. Our insight is that task-independent subtasks exist across tasks, making it possible to generalize the model across tasks. Based on this insight, we assumed that the model can be generalized to target tasks if it can decompose task-independent subtasks from source tasks. The challenge then becomes ensuring that the subtasks we decompose from the source task are truly task-independent. Regarding this issue, we proposed two properties that enable task-independence of subtasks: (1) avoiding overfitting to the source task, (2) maintaining consistent yet scalable semantics across tasks. Then we proposed DT2GS to endow the subtasks with these two properties by introducing the scalable subtask encoder and the adaptive subtask semantic module, respectively. Empirical results demonstrated that DT2GS can decompose tasks into a series of generalizable subtasks, leading to a generalizable MARL policy. Nevertheless, it would be beneficial to consider task-specific subtasks as well when there is a significant distribution shift between source and target tasks. In our future work, we will focus on expanding the generalization scenarios of DT2GS to address this limitation.

## 6 Acknowledgements

This work is partially supported by the NSF of China(under Grants 61925208, U22A2028, 6222214, 62002338, 62102399, U19B2019, 92364202), CAS Project for Young Scientists in Basic Research(YSBR-029), Youth Innovation Promotion Association CAS and Xplore Prize.