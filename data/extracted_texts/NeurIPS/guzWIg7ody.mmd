# Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks

Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks

Zixuan Zhang

Georgia Tech

zzhang3105@gatech.edu

&Kaiqi Zhang

UC Santa Barbara

kzhang70@ucsb.edu

&Minshuo Chen

Northwestern University

minshuo.chen@northwestern.edu

&Yuma Takeda

University of Tokyo

utklav1511@gmail.com

&Mengdi Wang

Princeton University

mengdiw@princeton.edu

&Tuo Zhao

Georgia Tech

tourzhao@gatech.edu

&Yu-Xiang Wang

UC San Diego

yuxiangw@ucsd.edu

Equal contribution.

###### Abstract

Convolutional residual neural networks (ConvResNets), though _overparameterized_, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts trained with weight decay, which cover ConvResNets as a special case, from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of _overparameterized_ ConvResNeXts over conventional machine learning models.

## 1 Introduction

Deep learning has achieved significant success in various real-world applications, such as computer vision [14; 23; 26], natural language processing [2; 15; 42], and robotics . One notable example of this is in the field of image classification, where the winner of the 2017 ImageNet challenge achieved a top-5 error rate of just 2.25%  using Convolutional Residual Network (ConvResNets) on a training dataset of 1 million labeled high-resolution images in 1000 categories.

Researchers have attributed the remarkable performance of deep learning to its great flexibility in modeling complex functions, which has motivated many works on investigating the representation power of deep neural networks. For instance, early work such as Barron , Cybenko , Kohler and Krzyzak  initialized this line of research for simple feedforward neural networks (FNNs) [19; 20; 38; 37]. More recently, Suzuki , Yarotsky  gave more precise bounds on the model sizes in terms of the approximation error, and Oono and Suzuki  further established a bound for more advanced architectures - ConvResNets. Based on these function approximation theories,one can further establish generalization bounds of deep neural networks with finite samples. Taking Oono and Suzuki  as an example again, they showed that ConvResNets with \((n^{D/(2+D)})\) parameters can achieve a minimax optimal convergence rate \((n^{-2/(2+D)})\) while approximating a \(C^{}\) nonparametric regression function with \(n\) samples. Unfortunately, these theoretical results cannot well explain the empirical successes of deep learning well, as they require the model size to be no larger than \((n)\) (the generalization bounds become vacuous otherwise). However, in real applications, practical deep learning models are often overparameterized, that is the model size can greatly exceeds the sample size.

### Main Results

Overparameterization of neural networks has been considered as one of the most fundamental research problems in deep learning theories, where parameters can significantly exceed training samples. There has been substantial empirical evidence showing that overparameterization can help fit the training data, ease the challenging nonconvex optimization, and gain robustness. However, existing literature on deep learning theories under such an **overparameterized** regime is very limited. To the best of our knowledge, we are only aware of Zhang and Wang , which attempts to analyze overparameterized neural networks trained with weight decay. However, their work still suffers from two major restrictions: (1) They consider parallel FNN, which is rarely used in practice. Whether similar results hold for more practical architectures remains unclear; (2) Their generalization bound from the curse of dimensionality, where the sample size is require to scale exponentially with the input dimension.

To address (1), we propose to develop a new theory for nonparametric classification using overparameterized ConvResNeXts trained with weight decay . The ConvResNeXt generalizes ConvResNets and includes them as a special case [5; 18; 33; 44]. Compared with FNNs, ConvResNeXts exhibit three features: (i) Instead of using dense weight matrices, they use convolutional filters, which can naturally investigate the underlying structures of the input data such as images and acoustic signals; (ii) They are equipped with skip-layer connections, which divides the entire network into blocks. The skip-layer connection can effectively address the vanishing gradient issue and therefore allow the networks to be significantly deeper; (iii) They are equipped with parallel architectures, which enable multiple "paths" within each block of the network, and allows the network to learn a more diverse set of features. Figure 1b illustrates the structure of ConvResNeXts (detailed introductions of ConvResNeXts is deferred to Section 2.3). This architecture introduces a significantly more complex nested function form, presenting us with the challenge of addressing novel issues in bounding the metric entropy of the function class.

To address (2), our proposed theory considers the optimal classifier is supported on a \(d\)-dimensional smooth manifold \(\) isometrically embedded in \(^{D}\) with \(d D\). The low-dimensional manifold assumption is highly practical, since it aligns with the inherent nature of many real-world datasets. For example, images typically represent projections of 3-dimensional objects subject to various transformations like rotation, translation, and skeletal adjustments. Such a generating mechanism inherently involves a limited set of intrinsic parameters. More broadly, various forms of data, including visual, acoustic, and textual, often exhibit low dimensional structures due to rich local regularities, global symmetries, repetitive patterns, or redundant sampling. It is reasonable to model these data as samples residing in proximity to a low dimensional manifold.

Our theoretical results can be summarized as follows:

* We prove that when ConvResNeXts are overparameterized, i.e., the number of blocks is larger than the order of the sample size \(n\), they can still achieve an asymptotic minimax rate for learning Besov functions when trained with weight decay. That is, given that the target function belongs to the Besov space \(B^{}_{p,q}()\)2, the risk of the estimator given by the ConvResNeXt class converges to the optimal risk at the rate \((n^{-(1-o(1))})\) with \(n\) samples. Notably, the statistical rate of convergence in our theory only depends on the intrinsic dimension \(d\), which circumvents the curse of dimensionality in Zhang and Wang .

* Moreover, our theory shows that one can scale the number of "paths" \(M\) in each block with the depth \(N\) as roughly \(MN n^{}\), which does not affect the convergence rate. This partially justifies the flexibility of the ConvResNeXt architecture when designing the bottlenecks, which simple structures like FNNs **cannot** achieve. Moreover, we can exchange the number of "paths" \(M\) and depth \(N\) as long as their product remains the same. This further provides the architectural insight that we don't necessarily need parallel blocks when we have residual connections. To say it differently, we provide new insight into why "residual connection" and "parallel blocks" in ResNeXts are useful in both approximation and generalization.
* Another technical highlight of our paper is bounding the covering number of weight-decayed ConvResNeXts, which is essential for computing the critical radius of the local Gaussian complexity. Specifically, we adopted a more advanced method that leverages the Dudley's chaining of the metric entropy . This technique provides a tighter bound than choosing a single radius of the covering number as in Zhang and Wang .
* To the best of our knowledge, our work is the first to develop approximation and statistical theories for ConvResNeXts, as well as overparameterized ConvResNets.

### Related Works

Our work is closely related to Liu et al. , which studies nonparametric classification under a similar setup - the optimal classifier belongs to the Besov space supported on a low dimensional manifold. Despite they develop similar theoretical results to ours, their analysis does not allow the model to be overparameterized, and therefore is not applicable to practical neural networks. Moreover, they investigate ConvResNets, which is a special case of ConvResNeXt in our work.

Our work is closely related to the reproducing kernel methods, which are also often used for nonparametric regression. However, existing literature has shown that the reproducing kernel methods lack the adaptivity to handle the heterogeneous smoothness in estimating Besov space functions, and only achieve suboptimal rate of convergence in statistical estimation [9; 32].

Our work is closely related neural tangent kernel theories [21; 1], which study overparameterized neural networks. Specifically, under certain regularity conditions, they establish the equivalence between overparameterized neural networks and reproducing kernel methods, and therefore the generalization bounds of overparameterized networks can be derived based on the associated reproducing kernel Hilbert space. Note that neural tangent kernel theories can be viewed as special cases of the theories for general reproducing kernel methods. Therefore, they also lack the adaptivity to be successful in the Besov space thus do not capture the properties of overparameterized neural networks.

## 2 Preliminaries

In this section, we introduce some concepts on manifolds. Details can be found in  and . Then we provide a detailed definition of the Besov space on smooth manifolds and the ConvResNeXt architecture.

### Smooth manifold

Firstly, we briefly introduce manifolds, the partition of unity and reach. Let \(\) be a \(d\)-dimensional Riemannian manifold isometrically embedded in \(^{D}\) with \(d\) much smaller than \(D\).

**Definition 2.1** (Chart).: A chart on \(\) is a pair \((U,)\) such that \(U\) is open and \(:U^{d},\) where \(\) is a homeomorphism (i.e., bijective, \(\) and \(^{-1}\) are both continuous).

In a chart \((U,)\), \(U\) is called a coordinate neighborhood, and \(\) is a coordinate system on \(U\). Essentially, a chart is a local coordinate system on \(\). A collection of charts that covers \(\) is called an atlas of \(\).

**Definition 2.2** (\(C^{k}\) Atlas).: A \(C^{k}\) atlas for \(\) is a collection of charts \(\{(U_{i},_{i})\}_{i}\) which satisfies \(_{i}U_{i}=\), and are pairwise \(C^{k}\) compatible:

\[_{i}_{}^{-1}:_{}(U_{i} U_{})_{i}(U_ {i} U_{})\;\;\;\;_{}_{i}^{-1}:_{i}(U_ {i} U_{})_{}(U_{i} U_{})\]

are both \(C^{k}\) for any \(i,\). An atlas is called finite if it contains finitely many charts.

**Definition 2.3** (Smooth Manifold).: A smooth manifold is a manifold \(\) together with a \(C^{}\) atlas.

Classical examples of smooth manifolds are the Euclidean space, the torus, and the unit sphere. Furthermore, we define \(C^{s}\) functions on a smooth manifold \(\) as follows:

**Definition 2.4** (\(C^{s}\) functions on \(\)).: Let \(\) be a smooth manifold and \(f:\) be a function on \(\). A function \(f:\) is \(C^{s}\) if for any chart \((U,)\) on \(\), the composition \(f^{-1}:(U)\) is a continuously differentiable up to order \(s\).

We next define the \(C^{}\) partition of unity, which is an important tool for studying functions on manifolds.

**Definition 2.5** (Partition of Unity, Definition 13.4 in ).: A \(C^{}\) partition of unity on a manifold \(\) is a collection of \(C^{}\) functions \(\{_{i}\}_{i}\) with \(_{i}:\) such that for any \(\), there is a neighbourhood of \(\) where only a finite number of the functions in \(\{_{i}\}_{i}\) are nonzero, and \(_{i}_{i}()=1\).

An open cover of a manifold \(\) is called locally finite if every \(\) has a neighborhood that intersects with a finite number of sets in the cover. The following proposition shows that a \(C^{}\) partition of unity for a smooth manifold always exists.

**Proposition 2.6** (Existence of a \(C^{}\) partition of unity, Theorem 13.7 in ).: _Let \(\{U_{i}\}_{i}\) be a locally finite cover of a smooth manifold \(\). Then there is a \(C^{}\) partition of unity \(\{_{i}\}_{i=1}^{}\) where every \(_{i}\) has a compact support such that \((_{i}) U_{i}\)._

Let \(\{(U_{i},_{i})\}_{i}\) be a \(C^{}\) atlas of \(\). Proposition 2.6 guarantees the existence of a partition of unity \(\{_{i}\}_{i}\) such that \(_{i}\) is supported on \(U_{i}\). To characterize the curvature of a manifold, we adopt the geometric concept: reach.

**Definition 2.7** (Reach [12; 29]).: Denote

\[G=^{D}:\;\; \|-\|_{2}=\|- \|_{2}=_{}\|- \|_{2}}\]

as the set of points with at least two nearest neighbors on \(\). The closure of \(G\) is called the medial axis of \(\). Then the reach of \(\) is defined as

\[=_{}\;_{ G}\| -\|_{2}.\]

Reach has a simple geometrical interpretation: for every point \(\), the osculating circle's radius is at least \(\). A large reach for \(\) indicates that the manifold changes slowly.

### Besov functions on a smooth manifold

We next define the Besov function space on the smooth manifold \(\), which generalizes more elementary function spaces such as the Sobolev and Holder spaces. Roughly speaking, functions in the Besov space are only required to have weak derivatives with bounded total variation. Notably, this includes functions with spatially heterogeneous smoothness, which requires more locally adaptive methods to achieve optimal estimation errors . Examples of Besov class functions include piecewise linear functions and piecewise quadratic functions that are smoother in some regions and more wiggly in other regions; see e.g., Figure 2 and Figure 4 of Mammen and van de Geer .

To define Besov functions rigorously, we first introduce the modulus of smoothness.

**Definition 2.8** (Modulus of Smoothness [8; 32]).: Let \(^{D}\). For a function \(f:^{D}\) be in \(L^{p}()\) for \(p>0\), the \(r\)-th modulus of smoothness of \(f\) is defined by

\[w_{r,p}(f,t) =_{\|\|_{2} t}\|_{}^{r}( f)\|_{L^{p}},\] \[_{}^{r}(f)() =_{j=0}^{r}(-1)^{r-j}f( +j)&,+r,\\ 0&.\]

**Definition 2.9** (Besov Space \(B^{}_{p,q}()\)).: For \(0<p,q,>0,r=+1\), define the seminorm \(||_{B^{}_{p,q}}\) as

\[|f|_{B^{}_{p,q}()}:=(_{0}^{}(t^{- }w_{r,p}(f,t))^{q})^{}&q<,\\ _{t>0}t^{-}w_{r,p}(f,t)&q=.\]

The norm of the Besov space \(B^{s}_{p,q}()\) is defined as \(\|f\|_{B^{}_{p,q}()}:=\|f\|_{L^{p}()}+|f|_{B^{}_{p,q}( )}\). Then the Besov space is defined as \(B^{}_{p,q}()=\{f L^{p}()\}\|f\|_{B^{}_{p,q}}<\}\).

Moreover, we show that functions in the Besov space can be decomposed using B-spline basis functions in the following proposition.

**Proposition 2.10** (Decomposition of Besov functions).: _Any function \(f\) in the Besov space \(B^{}_{p,q},>d/p\) can be decomposed using B-spline of order \(m,m>\): for any \(^{d}\), we have_

\[f()=_{k=0}^{}_{ J(k)}c_{k,}(f)M_{m,k, }(),\] (1)

_where \(J(k):=\{2^{-k}:[-m,2^{k}+m]^{d}^{d}\}\), \(M_{m,k,}():=M_{m}(2^{k}(-))\), and \(M_{k}()=_{i=1}^{d}M_{k}(x_{i})\) is the cardinal B-spline basis function which can be expressed as a polynomial:_

\[M_{m}(z)=_{j=1}^{m+1}(-1)^{j}(z-j)_{+}^{m}.\] (2)

We next define \(B^{}_{p,q}\) functions on \(\).

**Definition 2.11** (\(B^{}_{p,q}\) Functions on \(\)).: Let \(\) be a compact smooth manifold of dimension \(d\). Let \(\{(U_{i},_{i})\}_{i=1}^{}\) be a finite atlas on \(\) and \(\{_{i}\}_{i=1}^{_{}}\) be a partition of unity on \(\) such that \((_{i}) U_{i}\). A function \(f:\) is in \(B^{}_{p,q}()\) if

\[\|f\|_{B^{}_{p,q}()}:=_{i=1}^{C_{}}\|(f_{ i})_{i}^{-1}\|_{B^{}_{p,q}(^{d})}<.\] (3)

Since \(_{i}\) is supported on \(U_{i}\), the function \((f_{i})_{i}^{-1}\) is supported on \((U_{i})\). We can extend \((f_{i})_{i}^{-1}\) from \((U_{i})\) to \(^{d}\) by setting the function to be \(0\) on \(^{d}(U_{i})\). The extended function lies in the Besov space \(B^{s}_{p,q}(^{d})\)[34, Chapter 7].

### Architecture of ConvResNeXt

We introduce the architecture of ConvResNeXts. ConvResNeXts have three main features: convolution kernel, residual connections, and parallel architecture.

Consider one-sided stride-one convolution in our network. Let \(=\{_{j,k,l}\}^{w^{} K w}\) be a convolution kernel with output channel size \(w^{}\), kernel size \(K\) and input channel size \(w\). For \(^{D w}\), the convolution of \(\) with \(\) gives \(^{D w^{}}\) such that

\[=, y_{i,j}=_{k=1}^{K}_{l=1}^{w} _{j,k,l}z_{i+k-1,l},\] (4)

where \(1 i D,1 j w^{}\) and we set \(z_{i+k-1,l}=0\) for \(i+k-1>D\), as demonstrated in Figure 0(a).

The building blocks of ConvResNeXts are residual blocks. Given an input \(\), each residual block computes \(+F()\), where \(F\) is a subnetwork called bottleneck, consisting of convolutional layers.

In ConvResNeXts, a parallel architecture is introduced to each building block, which enables multiple "paths" in each block. In this paper, we study the ConvResNeXts with rectified linear unit (ReLU) activation function, i.e., \((z)=\{z,0\}\). We next provide the detailed definition of ConvResNeXts as follows:

**Definition 2.12**.: Let the neural network comprise \(N\) residual blocks, each residual block has a parallel architecture with \(M\) building blocks, and each building block contains \(L\) layers. The number of channels is \(w\), and the convolution kernel size is \(K\). Given an input \(^{D}\), a ConvResNeXt with ReLU activation function can be represented as

\[ f()\!=\!_{} _{m=1}^{M}f_{N,m}+\!\!\!_{m= 1}^{M}f_{1,m}+\!\!P(),\\ f_{n,m}\!=\!_{L}^{(n,m)}\!\!_{L-1}^{(n,m)}\!\!\!\!_{1}^{(n,m)} \!,\] (5)

where \(\) is the identity operator, \(P:^{D}^{D w_{0}}\) is the padding operator satisfying \(P()=[,\ \ \ \ \ \ ]^{D w}\), \(\{_{l}^{(n,m)}\}_{l=1}^{L}\) is a collection of convolution kernels for \(n=1,,N,m=1,,M\), \(_{}^{w_{L}}\) denotes the linear operator for the last layer, and \(\) is the convolution operation defined in (4).

The structure of ConvResNeXts is shown in Figure 1b. When \(M=1\), the ConvResNeXt defined in (5) reduces to a ConvResNet. For notational simplicity, we omit biases in the neural network structure by extending the input dimension and padding the input with a scalar 1 (See Proposition F.4 for more details). The channel with 0's is used to accumulate the output.

## 3 Theory

In this section, we study a binary classification problem on \([-1,1]^{D}\). Specifically, we are given i.i.d. samples \(\{_{i},y_{i}\}_{i=1}^{n}\) where \(_{i}\) and \(y_{i}\{0,1\}\) is the label. The label \(y\{0,1\}\) follows the Bernoulli-type distribution

\[(y|)=())}{1+(f^{*}())}\]

for some \(f^{*}:\) belonging to the Besov space. More specifically, we make the following assumption on \(f^{*}\).

**Assumption 3.1**.: Let \(0<p,q\), \(d/p<<\). Assume \(f^{*} B_{p,q}^{}()\) and \(\|f^{*}\|_{B_{p,q}^{}()} C_{}\) for some constant \(C_{}>0\).

To learn \(f^{*}\), we minimize the empirical logistic risk over the training data:

\[=*{arg\,min}_{f^{}} _{i=1}^{n}y_{i}(1+(-f(_{i})))+(1-y_{i})(1+(f( _{i}))),\] (6)

where \(^{}\) is some neural network class specified later. For notational simplicity, we denote the empirical logistic risk function in (6) as \(_{n}(f)\), and denote the population logistic risk as

\[_{}[(f)]=_{(,y) }y(1+(-f()))+(1-y)(1+(f())).\]

Figure 1: (a) Demonstration of the convolution operation \(*z\), where the input is \(z^{D w}\), and the output is \(*z^{D w^{}}\). Here \(_{j,:,:}\) is a \(D w\) matrix for the \(j\)-th output channel. (b) Demonstration of the ConvResNeXt. \(f_{1,1} f_{N,M}\) are the building blocks, each building block is a convolution neural network.

We next specify the class of ConvResNeXts for learning \(f^{*}\):

\[^{}(N,M,L,K,w,B_{},B_{})= f f) with }N\] residual block has \[ML\] Each layer has kernel size bounded by \[K,w.\] \[_{n=1}^{N}_{m=1}^{M}_{=1}^{L}\|_{}^ {(n,m)}\|_{}^{2} B_{},\|_{}\|_{ }^{2} B_{},f() .}.\] (7)

Note that the hyperparameters of \(^{}\) will be specified in our theoretical analysis later.

As can be seen, \(^{}\) contains the Frobenius norm constraints of the weights. For the sake of computational convenience in practice, such constraints can be replaced with weight decay regularization the residual blocks and the last fully connected layer separately. More specifically, we can use the following alternative formulation:

\[=*{arg\,min}_{f^{}(N,M,L,K,w, ,)}_{n}(f)+_{1}_{n=1}^{N}_{m=1}^{M} _{=1}^{L}\|_{}^{(n,m)}\|_{}^{2}+_{2}\| _{}\|_{}^{2},\]

where \(_{1},_{2}>0\) are properly chosen regularization parameters.

### Approximation theory

In this section, we provide a universal approximation theory of ConvResNeXts for Besov functions on a smooth manifold:

**Theorem 3.2**.: _For any Besov function \(f_{0}\) on a smooth manifold satisfying \(p,q 1,-d/p>1,\)_

\[\|f_{0}\|_{B^{}_{p,q}()} C_{},\]

_for any \(P>0\) and any ConvResNeXt class \(^{}(N,M,L,K,w,B_{},B_{})\) satisfying \(L=L^{}+L_{0}-1,L^{} 3\), where \(L_{0}=\), and_

\[MN C_{}P,w C_{1}(dm+D),B_{} C_{2}L/K,B_{} C_{3}C_{}^{2}((dm+D)LK)^{L}(C_{}P)^{L-2/p},\]

_there exists \(f^{}(N,M,L,K,w,B_{},B_{})\) such that_

\[\|f-f_{0}\|_{} C_{}C_{}(C_{4}P^{-/d}+C_{5} (-C_{6}L^{} P)),\] (8)

_where \(C_{1},C_{2},C_{3}\) are universal constants and \(C_{4},C_{5},C_{6}\) are constants that only depends on \(d\) and \(m,\)\(d\) is the intrinsic dimension of the manifold and \(m\) is an integer satisfying \(0<<(m,m-1+1/p)\)._

The approximation error of the network is bounded by the sum of two terms. The first term is a polynomial decay term that decreases with the size of the neural network and represents the trailing term of the B-spline approximation. The second term reflects the approximation error of neural networks to piecewise polynomials, decreasing exponentially with the number of layers. The proof is deferred to Section 4.1 and the appendix.

### Estimation theory

**Theorem 3.3**.: _Suppose Assumption 3.1 holds. Set \(L=L^{}+L_{0}-1,L^{} 3\), where \(L_{0}=\), and_

\[MN C_{}P, P=O(n^{}),  w C_{1}(dm+D).\]

_Let \(\) be the global minimizer given in (6) with the function class \(=^{}(N,M,L,K,w,B_{},B_{})\). Then we have_

\[_{}[((x),y)]-_{ }[(f^{*}(x),y)] C_{}}{L-2}}w^{}L ^{}}{n}^{}\] \[+C_{8}(-C_{6}L^{}),\]

_where the logarithmic terms are omitted. \(C_{1}\) is the constant defined in Theorem 3.2, \(C_{7},C_{8}\) are constants that depend on \(C_{},C_{},d,m\), \(K\) is the size of the convolution kernel._We would like to make the following remarks about the results:

\(\)**Strong adaptivity:** By setting the width of the neural network to \(w=2C_{1}D\), the model can adapt to any Besov functions on any smooth manifold, provided that \(dm D\). This remarkable flexibility can be achieved simply by tuning the regularization parameter. The cost of overestimating the width is a slight increase in the estimation error.

\(\)**No curse of dimensionality:** the above error rate only depends polynomially on the ambient dimension \(D\) and exponentially on the hidden dimension \(d\). Since in real data, the hidden dimension \(d\) can be much smaller than the ambient dimension \(D\), this result shows that neural networks can explore the low-dimension structure of data to overcome the curse of dimensionality.

\(\)**Overparameterization is fine:** the number of building blocks in a ConvResNeXt does not influence the estimation error as long as it is large enough. In other words, this matches the empirical observations that neural networks generalize well despite overparameterization.

\(\)**Close to minimax rate:** The lower bound of the 1-Lipschitz error for any estimator \(\) is

\[_{}_{f^{*} B^{}_{p,q}}L((),f^{*})  n^{-}.\]

where \(\) notation hides a factor of constant. The proof can be found in Appendix E. Comparing to the minimax rate, we can see that as \(L\), the above error rate converges to the minimax rate up to a constant term. In other words, overparameterized ConvResNeXt can achieve close to the minimax rate in estimating functions in Besov class. In comparison, all kernel ridge regression including any NTKs will have a suboptimal rate lower bounded by \(\), which is suboptimal.

\(\)**Deeper is better:** with larger \(L\), the error rate decays faster and gets closer to the minimax rate. This indicates that deeper model can achieve better performance than shallower models.

\(\)**Tradeoff between width and depth:** With a fixed budget in the number of parameters, the tradeoff between width and depth is crucial for achieving the best performance, and this often requires repeated, time-consuming experiments. On the other hand, our results suggests that such a tradeoff less important in a ResNeXt. The lower bound of error does not depend on the arrangements of the residual blocks \(M\) and \(N\), as long as their product is large enough. This can partly explain the benefit of ResNeXt over other architecture.

By choosing \(L=O((n))\) in Theorem 3.3, the second term in the error can be merged with the first term, and close to the minimax rate can be achieved:

**Corollary 3.4**.: _Given the conditions in Theorem 3.3, set the depth of each block is \(L=O((n))\) and then the estimation error of the empirical risk minimizer \(\) satisfies_

\[_{}[((),y)]_{ }[(f^{*})]+(n^{-(1- (1))}),\]

_where \(()\) omits the logarithmic term._

The proof of Theorem 3.3 is deferred to Section 4.2 and Section D.2. The key technique is computing the critical radius of the local Gaussian complexity by bounding the covering number of weight-decayed ConvResNeXts. This technique provides a tighter bound than choosing a single radius of the covering number as in Suzuki , Zhang and Wang . The covering number of an overparameterized ConvResNeXt with norm constraint (Lemma 4.1) is one of our key contributions.

## 4 Proof overview

### Approximation error

We follow the method in Liu et al.  to construct a neural network that achieves the approximation error we claim. It is divided into the following steps:

\(\)**Step 1: Decompose the target function into the sum of locally supported functions.**

In this work, we adopt a similar approach to  and partition \(\) using a finite number of open balls on \(^{D}\). Specifically, we define \(B(_{i},r)\) as the set of unit balls with center \(_{i}\) and radius \(r\) such that their union covers the manifold of interest, i.e., \(_{i=1}^{C_{}}B(_{i},r)\). This allows us to partition the manifold into subregions \(U_{i}=B(_{i},r)\), and further decompose a smooth function on the manifold into the sum of locally supported smooth functions with linear projections. The existence of function decomposition is guaranteed by the existence of partition of unity stated in Proposition 2.6. See Section C.1 for the detail.

\(\)**Step 2: Locally approximate the decomposed functions using cardinal B-spline basis functions.** In the second step, we decompose the locally supported Besov functions achieved in the first step using B-spline basis functions. The existence of the decomposition was proven by Dung , and was applied in a series of works . The difference between our result and previous work is that we define a norm on the coefficients and bound this norm, instead of bounding the maximum value. The detail is deferred to Section C.2.

\(\)**Step 3: Approximate the polynomial functions using neural networks.** In this section, we follow the method in Zhang and Wang , Suzuki , Liu et al.  and show that neural networks can be used to approximate polynomial functions, including B-spline basis functions and the distance function. The key technique is to use a neural network to approximate square function and multiply function . The detail is deferred to the appendix. Specifically, Lemma F.3 proves that a neural network with width \(w=O(dm)\) and depth \(L\) can approximate B-spline basis functions, and the error decreases exponentially with \(L\); Similarly, Proposition C.3 shows that a neural network with width \(w=O(D)\) can approximately calculate the distance between two points \(d^{2}(;)\), with precision decreasing exponentially with the depth.

\(\)**Step 4: Use a ConvResNeXt to Approximate the target function.** Using the results above, the target function can be (approximately) decomposed as

\[_{i=1}^{C_{}}_{j=1}^{P}a_{i,k_{j},_{j}}M_{m,k_{j}, _{j}}_{i}( B(_{i},r)).\] (9)

We first demonstrate that a ReLU neural network taking two scalars \(a,b\) as the input, denoted as \(ab\), can approximate \(y( B_{r,i})\), where \(\) satisfy that \(y1=y\) for all \(y\), and \(y=0\) if any of \(x\) or \(y\) is 0, and the soft indicator function \(}( B_{r,i})\) satisfy \(}( B_{r,i})=1\) when \(x B_{r,i}\), and \(}( B_{r,i})=0\) when \(x B_{r+,i}\). The detail is deferred to Section C.3.

Then, we show that it is possible to construct \(MN=C_{}P\) number of building blocks, such that each building block is a feedforward neural network with width \(C_{1}(md+D)\) and depth \(L\), where \(m\) is an integer satisfying \(0<<min(m,m-1+1/p)\). The \(k\)-th building block (the position of the block does not matter) approximates \(a_{i,k_{j},_{j}}M_{m,k_{j},_{j}}_{i}( {x} B(_{i},r)),\) where \(i=ceiling(k/N),j=rem(k,N)\). Each building block has where a sub-block with width \(D\) and depth \(L-1\) approximates the chart selection, a sub-block with width \(md\) and depth \(L-1\) approximates the B-spline function, and the last layer approximates the multiply function. The norm of this block is bounded by

\[_{=1}^{L}\|_{}^{(i,j)}\|_{}^{2} O(2^{2k/L} dmL+DL).\] (10)

Making use of the 1-homogeneous property of the ReLU function, by scaling all the weights in the neural network, these building blocks can be combined into a neural network with residual connections, that approximate the target function and satisfy our constraint on the norm of weights. See Section C.4 for the detail.

By applying Lemma C.6, which shows that any \(L\)-layer feedforward neural network can be reformulated as an \(L+L_{0}-1\)-layer convolution neural network, the neural network constructed above can be converted into a ConvResNeXt that satisfies the conditions in Theorem 3.2.

### Estimation error

We first prove the covering number of an overparameterized ConvResNeXt with norm-constraint as in Lemma 4.1, then compute the critical radius of this function class using the covering number as in Corollary F.5. The critical radius can be used to bound the estimation error as in Theorem 14.20 in Wainwright . The proof is deferred to Section D.2.

**Lemma 4.1**.: _Consider a neural network defined in Definition 2.12. Let the last layer of this neural network is a single linear layer with norm \(\|W_{}\|_{}^{2} B_{}\). Let the input of this neural network satisfy \(\|\|_{2} 1, x\), and is concatenated with 1 before feeding into this neural network so that part of the weight plays the role of the bias. The covering number of this neural network is bounded by_

\[(,) w^{2}LB_{}^{}K ^{}(B_{}^{1/2}((KB_{}/ L)^{L/2}))^{}^{-},\] (11)

_where the logarithmic term is omitted._

The key idea of the proof is to split the building block into two types ("small blocks" and "large blocks") depending on whether the total norm of the weights in the building block is smaller than \(\). By properly choosing \(\), we prove that if all the "small blocks" in this neural network are removed, the perturbation to the output for any input \(\|x\| 1\) is no more than \(/2\), so the covering number of the ConvResNeXt is only determined by the number of "large blocks", which is no more than \(B_{}/\).

Proof.: Using the inequality of arithmetic and geometric means, from Proposition F.6, Proposition F.8 and Proposition F.9, if any residual block is removed, the perturbation to the output is no more than

\[p_{m}:=(KB_{m}/L)^{L/2}B_{}^{1/2}((KB_{}/L)^{L/2}),\]

where \(B_{m}\) is the total norm of parameters in this block. Because of that, the residual blocks can be divided into two kinds depending on the norm of the weights \(B_{m}<\) ("small blocks") and \(B_{m}\) ("large blocks"). If all the "small blocks" are removed, the perturbation to the output for any input \(\|\|_{2} 1\) is no more than

\[_{m:B_{m}<}p_{m}((KB_{}/L)^{L/2})K^{L/2}B_{ }B_{}^{1/2}(/L)^{L/2-1}/L.\]

Choosing \(=L}/L)^{L/2})K^{L/2}B_{ }B_{}^{1/2}}^{1/(L/2-1)},\) the perturbation above is no more than \(/2\). The covering number can be determined by the number of the "large blocks" in the neural network, which is no more than \(B_{}/\). As for any block, \(B_{}L_{} B_{}^{1/2}((KB_{}/L)^{L/2})\), taking our chosen \(\) finishes the proof, where \(B_{}\) is the upper bound of the input to this block defined in Proposition D.1, and \(L_{}\) is the Lipschitz constant of all the layers following the block.

_Remark 4.2_.: The proof of Lemma 4.1 shows that under weight decay, the building blocks in a ConvResNeXt are sparse, i.e. only a finite number of blocks contribute non-trivially to the network even though the model can be overparameterized. This explains why a ConvResNeXt can generalize well despite overparameterization, and provide a new perspective in explaining why residual connections improve the performance of deep neural networks.

## 5 Discussions

This paper focuses on developing insightful generalization bounds for the regularized empirical risk minimizer. We opt not to delve into the end-to-end analysis of optimization algorithms in order to explore the adaptivity of complex architectures such as ConvResNeXts, while works on optimization behaviour of neural networks are limited to simple network structures [28; 39]. Notably, this approach to decouple learning and optimization has been widely adopted [3; 17; 31; 6; 25]. We made the same choice in the interest of getting a more fine-grained learning theory. However, our paper considers weight decay and overparameterization which are tightly connected to real-world training of neural networks, and can be the most promising work to bridge the gap between optimization and statistical guarantees. We defer more details to the appendix, including discussions on the Besov space and numerical experiments for supporting our theories as well as supplementary technical proof.