# Voicebox: Text-Guided Multilingual

Universal Speech Generation at Scale

 Matthew Le\({}^{*}\) Apoorv Vyas\({}^{*}\) Bowen Shi\({}^{*}\) Brian Karrer\({}^{*}\) Leda Sari Rashel Moritz

**Mary Williamson** **Vimal Manohar** **Yossi Adi\({}^{}\) Jay Mahadeokar** **Wei-Ning Hsu\({}^{*}\)**

Equal contribution. Corresponding authors: {mattle,wnhsu}@meta.com

Fundamental AI Research (FAIR), Meta

FAIR & Hebrew University of Jerusalem.

###### Abstract

Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in https://voicebox.metademolab.com.

## 1 Introduction

Recent advances in large-scale generative models  have led to a major paradigm shift towards building general-purpose models, which can perform many new tasks not explicitly trained on. These generative models learn to predict the missing data given the context. Post training, we can directly input a question, optionally with a few contextual question-answer examples, instead of fine-tuning with labeled data. While the training objective appears simple, it subsumes many tasks as one can convert them into some form of context. For the model to perform well at every task, it implies that the estimation of \(p()\) needs to be accurate for every context. Hence, scale and diversity are the most crucial factors for building general-purpose models .

Despite the success of large-scale generative models in other areas, most speech models are still trained on datasets at the scale of tens to hundreds of hours . Previous works consider highly curated datasets such as VCTK , which contains only clean audio recorded in studio from about 100 speakers with little speaking style and text variation. Such models struggle to synthesize speech with rich variation in emotion, voice, background noise, acoustic condition, and have not been tested on the abilities to generalize to tasks not explicitly trained on.

This paper presents Voicebox, the most versatile text-conditioned speech generative model at scale. Voicebox is trained on a text-guided speech infilling task, where the goal is to generate masked speechgiven its surrounding audio and text transcript. This can be considered as a guided in-context learning problem, where audio style is inferred from the audio context and textual content is specified through transcript. Voicebox does not require any audio style labels (e.g., speaker, emotion, and noise), which differentiates Voicebox from the majority of prior work where such labels are used extensively. Prior work uses labels to make the mapping between input (text and audio style) and output (speech) more deterministic to reduce underfitting [60; 46]. We show that Voicebox's text-guided speech infilling approach is much more scalable in terms of data while subsuming many common generative tasks.

In terms of modeling, Voicebox is a non-autoregressive (NAR) continuous normalizing flow (CNF) model . Similar to diffusion models , CNFs model the transformation from a simple distribution to a complex data distribution, parameterized by a neural network. We train Voicebox with flow-matching , a recently proposed method that enables efficient and scalable training of CNFs via a simple vector field regression loss. In contrast to auto-regressive models, Voicebox can consume context not only in the past but also in the future. Moreover, the number of flow steps can be controlled at inference time to flexibly trade off quality and runtime efficiency.

Voicebox is trained on 60K hours of English audiobooks and 50K hours of multilingual audiobooks in 6 languages for the mono and multilingual setups. Voicebox achieves state-of-the-art performance on mono-lingual/cross-lingual zero-shot TTS, speech denoising, speech editing, diverse speech sampling and an application to data creation for speech recognition. To tackle the lack of comparability due to the use of subjective metrics, this paper presents a series of metrics using public models to facilitate reproducible comparison and model development for speech generation studies.

## 2 Related Work

**Generative speech models:** Most speech generative models are task-specific and trained on different datasets. One common type of task is _audio style conversion_, which aims to convert only a specific attribute while keeping other attributes the same. Voice conversion [27; 39], emotion conversion [53; 34], speech enhancement [63; 11; 55] belong to this category. Many of these models are supervised and trained on pairs of data that only differ in one attribute, for example, emotion . It is hard to obtain such data. Moreover, some attributes, such as speaking style, are hard to annotate. Hence, these models are often trained on small datasets.

Controllable text-to-speech synthesis (TTS) is another common task, which aims to synthesize speech in a target audio style given text. While some styles like voice can be specified through labels  or pre-trained embeddings like YourTTS  and Jia et al. ; others like prosody are hard to annotate or embed. Previous studies  tried to control them by learning a residual embedding. However, these models encode style in a low-dimensional space and impose an overly simple distribution of speech given text and residual embedding [51; 56]. They cannot generate realistic noisy speech given a low dimensional vector, and performance degrades when conditioned on noisy references .

Infilling can be considered as another type of task. It aims to predict speech given context [36; 4] and optionally text guidance [3; 5; 61]. Instead of learning an explicit embedding to control style, infilling models predict speech coherent to the context. In other words, these models perform in-context learning similar to Large Language Models (LLM). While this is a step toward building large

  
**Model** & **ZS TTS** & **Denoise** & **Edit** & **Sampling** \\  YourTTS & ✓ & ✗ & ✗ & ✓ \\ VALLE & ✓ & ✗ & ✗ & ✓ \\ A3T & ✓ & ✗ & ✓ & ✗ \\ Demacs & ✗ & ✓ & ✗ & ✗ \\  Voicebox & ✓ & ✗ & ✓ & ✓ \\   

Table 1: Comparing Voicebox with baselines on task capabilities. *Through infilling, A3T and Voicebox can remove transient noise but not stationary background noise.

Figure 1: Task generalization via in-context learning.

scale generalist models using little explicit supervision, most prior work using text guidance still assumes a deterministic mapping from text and context to target [3; 5], which is only realistic for very short segments. Voicebox is a text-guided infilling model, but it leverages the CNF model that can parameterize any distribution. Hence, Voicebox can infill speech of any length and can be trained on in-the-wild datasets with rich variation, and provide a general solution that subsumes many tasks in a text-guided fashion.

**Large scale in-context learning models:** With the advancement in neural codec for speech [22; 12; 67], many recent studies explore token-based language modeling for speech generation. The GSLM-family [36; 28; 41] are textless language models built upon HuBERT units  for speech continuation without using text. HuBERT units encode mostly content, and the generated speech does not preserve the voice of the prompt. To tackle this, AudioLM  considers a cascaded approach which first generates HuBERT-like tokens and then predicts SoundStream  tokens, a reconstruction based codec that preserves style. These models are not conditioned on text and are evaluated on spoken language modeling tasks.

VALL-E  is most related to Voicebox. It is a text conditioned LM trained on Encodec  tokens (similar to SoundStream). Encodec encodes each frame with 8 ordered codebooks at 75Hz using a residual quantization layer. VALL-E has two modules. The first is an auto-regressive (AR) model that predicts the first code of each frame given text and the audio prompt. The second is an NAR model that predicts the remaining seven codebooks sequentially.

VALL-E demonstrates state-of-the-art (SOTA) zero-shot TTS performance through in-context learning, where speech of the desired style is used as prompt. The model considers the prompt as part of the whole utterance such that it generates the rest of the utterance containing the target text in the same audio style. Voicebox has several design advantages compared to this. 1) Voicebox can use context both in the past and future, which is useful for editing where only a segment in the middle needs to be generated. 2) Voicebox can generate speech much faster than VALL-E because flow-matching can produce high quality samples with less than 10 NAR steps, while VALL-E requires 1 AR and 7 NAR steps. 3) Voicebox decouples duration and audio modeling, enabling finer grained alignment control. 4) Voicebox is compatible with any continuous features including Encodec embeddings.

## 3 Method

### Background: Flow Matching with an optimal transport path

Let \(^{d}\) be the data space with data points \(x^{d}\) drawn from some unknown distribution \(q(x)\). Continuous Normalizing Flows (CNFs)  are a family of generative models that learn the transformation from a simple prior distribution \(p_{0}\) (e.g., normal distribution) to the data distribution \(p_{1} q\). CNFs parameterize a time-dependent vector field \(v_{t}:^{d}^{d}\) that is used to construct a _flow_: \(_{t}:^{d}^{d}\) that pushes points from the prior towards the target distribution. The relationship is defined via the ordinary differential equation (ODE) as: \(d_{t}(x)/dt=v_{t}(_{t}(x))\) and \(_{0}(x)=x\). For a flow \(_{t}\), the _probability path_ (time-dependent probability density function) \(p:^{d}_{>0}\) can be derived via the change of variables formula: \(p_{t}(x)=p_{0}(_{t}^{-1}(x))[_{t}^{-1}(x)/ x]\). To sample from \(p_{t}(x)\), we first draw \(x_{0}\) from \(p_{0}\) and then solve the initial value problem (IVP) for \(_{t}(x_{0})\) given \(d_{t}(x)/dt=v_{t}(_{t}(x))\) and \(_{0}(x)=x_{0}\). We use \(x_{t}\) and \(_{t}(x_{0})\) interchangeably.

Let \(p_{t}\) be a probability path and \(u_{t}\) be the corresponding vector field that generates \(p_{t}\). The vector field \(v_{t}(x;)\) parameterized by a neural network \(\) can be trained with the Flow Matching objective: \(_{FM}()=_{t,p_{t}(x)}||u_{t}(x)-v_{t}(x;)||^{2}\), where \(t\) and \(x p_{t}(x)\). While the objective appears simple, in practice we do not have the prior knowledge of \(p_{t}\) or \(v_{t}\), and cannot directly compute the loss or its gradient estimator.

Let \(x_{1}\) be a random variable distributed according to data distribution \(q\). Lipman et al.  first notes that a probability path \(p_{t}(x)\) can be constructed via a mixture of simpler _conditional paths_\(p_{t}(x x_{1})\) whose vector field \(u_{t}(x x_{1})\) can be easily computed. To construct \(p_{t}(x)\), a conditional path is defined such that 1) \(p_{0}(x x_{1})=p_{0}(x)\) and 2) \(p_{1}(x x_{1})=(x x_{1},^{2}I)\), a Gaussian distribution centered at \(x_{1}\) with a sufficiently small \(\) (typically \(10^{-5}\)). The marginal path is computed as \( p_{t}(x x_{1})q(x_{1})dx_{1}\), which closely approximates \(q(x_{1})\) at \(t=1\). With that,  presents the Conditional Flow Matching (CFM) objective, \(_{CFM}()=_{t,q(x_{1}),p_{t}(x|x_{1})}||u_{t}(x x _{1})-v_{t}(x;)||^{2}\).

It is proven that FM and CFM have identical gradients w.r.t. \(\). More importantly, one can easily draw samples from \(p_{t}(x x_{1})\) and compute \(u_{t}(x x_{1})\) to derive an unbiased gradient estimator.

The next question is _how to choose a conditional flow._ A flow defines _trajectories_, which dictates how each point moves between \(p_{0}\) and \(p_{1}\). Intuitively, a simpler trajectory (e.g., a straight line) can be learned faster and the IVP can be solved more accurately and efficiently. Lipman et al.  presents a conditional flow called _optimal transport (OT) path_, which has the form of \(p_{t}(x x_{1})=(x tx_{1},(1-(1-_{min})t)^{2}I)\) and \(u_{t}(x x_{1})=(x_{1}-(1-_{})x)/(1-(1- _{})t)\). The flow is arguably simple because points move with a constant speed and direction. We adopt it for Voicebox.

Lipman et al.  also presents another flow that recovers the path of diffusion models , which is more complex than the OT path. We will present ablation studies comparing different paths (OT vs diffusion) and different objectives (CFM vs score-matching). Results show the superiority in performance and efficiency of CFM with OT path.

### Problem formulation

Given a dataset of transcribed speech \((x,y)\) where \(x\) and \(y\) denote an audio sample and its transcript, respectively, the goal is to build a single model that can perform many text-guided speech generation tasks through in-context learning. We propose to train such a generative model on the _text-guided speech infilling task_, which predicts a segment of speech given its surrounding audio and the complete text transcript. Let \(m\) be a binary temporal mask which is of the same length as \(x\), and \(x_{mis}=m x\) and \(x_{ctx}=(1-m) x\) be the complementary masked versions of \(x\). The generative model learns \(p(x_{mis} y,x_{ctx})\). In other words, \(y\) and \(x_{ctx}\) are the context and \(x_{mis}\) is the missing data.

### Model and Training

Motivated by the need that some applications require fine-grained alignment control between speech and text, we decouple Voicebox into two components: an audio model and a duration model. Let \(x=(x^{1},x^{2},,x^{N})\) be an audio sample of \(N\) frames, \(y=(y^{1},y^{2},,y^{M})\) be a text sequence of \(M\) phones, and \(l=(l^{1},l^{2},,l^{M})\) be the per-phone duration where \(l^{j}\) denotes how many audio frames \(y^{j}\) correspond to and \(_{j=1}^{M}l^{j}=N\). We further define \(z=(y,l)=(z^{1},z^{2},,z^{N})\) to be the frame-level phone transcript, which repeats each \(y^{j}\) by \(l^{j}\) times such that \(z^{i}\) denotes the phone label of the audio frame \(x^{i}\). For a pair of \((x,y)\), \(l\) and \(z\) can be estimated through _forced alignment_ using a speech recognition model. The estimation of \(q(x_{mis} y,x_{ctx})\) is then broken down into the audio model \(q(x_{mis} z,x_{ctx})\) and the duration model \(q(l_{mis} y,l_{ctx})\), where \(l_{mis}\) and \(l_{ctx}\) denote \(l\) masked by \(m^{}\) and \(1-m^{}\), and \(m^{}\) is downsampled from \(m\) based on \(l\), detailed in Appendix A.2.

**Audio Model:** Given a context \(z\) and \(x_{ctx}\) of length \(N\), the distribution of \(x_{mis}\) is highly stochastic especially when \(x_{mis}\) has a large temporal span. Hence, we parameterize it with a CNF and train

Figure 2: Illustration of Voicebox training and inference.

it using the flow matching objective with the optimal transport path. Audio \(x\) is represented as an \(80\)-dimensional log Mel spectrogram (\(x^{i}^{80}\)) extracted at a 100Hz frame rate. The audio context \(x^{i}_{ctx}=\) where \(m^{i}=1\) and \(x^{i}_{ctx}=x^{i}\) where \(m^{i}=0\). For simpler conditioning, we model the conditional distribution \(q(x z,x_{ctx})\) of all frames \(x\) instead of only masked frames \(x_{mis}\). A neural network is used to parameterize the conditional vector field \(v_{t}(x_{t},x_{ctx},z;)\) that additionally takes \(x_{ctx}\) and \(z\) as input. Note that \(x_{t}\) is a sample at flow step \(t\) and \(x=x_{1}\).

Given as input \(x_{ctx}^{N F}\), \(x_{t}^{N F}\), phone sequence \(z[K]^{N}\) with \(K\) denoting the number of phone classes, and a time step \(t\), we employ a Transformer model to parameterize the vector field \(v_{t}\). A lookup table, denoted as \(L^{K H}\), is used to embed the phone sequence \(z\), resulting in the embedded sequence \(z_{emb}^{N H}\) where \(z^{i}_{emb}=L(z^{i})\) for \(i 1,,N\). Subsequently, the three sequences (\(x_{t}\), \(x_{ctx}\), and \(z_{emb}\)) are concatenated frame-by-frame and projected by employing matrix \(W_{p}^{(2F+H) D}\), thereby obtaining the sequence \(H_{c}^{N D}\) where \(D\) represents the embedding dimension of the Transformer model.

To embed the flow step, a sinusoidal positional encoding is applied to map \(t\) to \(h_{t}^{D}\). The sequence \(_{c}^{(N+1) D}\), which serves as the input to the Transformer model, is derived by concatenating \(H_{c}\) with the vector \(h_{t}\) along the time dimension. Given the Transformer output \(v_{t}(x_{t},x_{mis},z;)^{N F}\), which is the sub-sequence corresponding to \(H_{c}\), the loss is computed as:

\[_{}()=_{t,m,q(x,z),p_{0}(x_{0})} ||u_{t}(x_{t} x)-v_{t}(x_{t},x_{ctx},z;)||^{2},\] (1)

by reparameterization. During training, given an audio sample \(x\) and a prior sample \(x_{0}\), we have \(x_{t}=(1-(1-_{})t)x_{0}+tx\) and \(u_{t}(x_{t} x)=x-(1-_{min})x_{0}\). This function computes the loss on all frames, including those that are not masked and would not be required during inference. To divert the model's focus to masked frames, we present a masked version of \(_{}\):

\[_{}()=_{t,m,q(x,z),p_{0}(x_{0}) }||m(u_{t}(x_{t} x)-v_{t}(x_{t},x_{ctx},z;))\,||^{2},\] (2)

where the loss is only computed on masked frames. Appendix B.3 shows it leads to better results

**Duration model:** We consider two solutions. The first one closely follows the audio model. It models \(q(l y,l_{ctx})\) via a conditional vector field which swaps \((x,x_{ctx},z)\) with \((l,l_{ctx},y)\) and accordingly for the flow, where \(l,l_{ctx}^{M 1}\) and \(y[K]^{M}\). The masked version of the CFM loss is used for training. On the other hand, previous studies have shown that regression duration models can produce reasonable speech [51; 37]. Hence we consider a second solution that regresses the masked duration \(l_{mis}\) given the context duration \(l_{ctx}\) and phonetic transcript \(y\). The same Transformer model is used, except that there are only two input sequences instead of three, and the time embedding is not used. The model is trained with an \(L_{1}\) regression loss on masked phones:

\[_{}()=_{m,q(l,y)}||m^{} (l_{mis}-g(l_{ctx},y;))\,||_{1},\] (3)

where \(g\) denotes the regression-based duration model. This is similar to the duration model used in FastSpeech2 , but with additional duration context \(l_{ctx}\) as input.

### Inference

To sample from the the learned audio distribution \(p_{1}(x z,x_{ctx})\), a noise \(x_{0}\) is first sampled from \(p_{0}\), and then an ODE solver is used to evaluate \(_{1}(x_{0})\) given \(d_{t}(x)/dt=v_{t}(_{t}(x),x_{ctx},z;)\) and the initial condition \(_{0}(x_{0})=x_{0}\). Intuitively, the ODE solver computes \(_{1}(x_{0})\) by evaluating \(v_{t}\) at multiple \(t\) to approximate the integration from \(t=0\) to \(t=1\) given the initial condition \(_{0}(x_{0})=x_{0}\). The number of function evaluation (NFE) is defined as how many times \(d_{t}(x_{0})/dt\) is evaluated. A higher NFE often leads to a more accurate solution of \(_{1}(x_{0})\) at the cost of longer run time. This provides great flexibility for users to decide the trade-off between speed and accuracy. Moreover, we find that empirically Voicebox can already generate very high quality speech with less than 10 NFEs, making it significantly faster compared to auto-regressive models.

### Classifier-Free Guidance

Classifier guidance (CG)  is a technique used to trade off mode coverage and sample fidelity for diffusion models post training. It modifies the score estimate of a diffusion model to include the gradient of the log likelihood of an auxiliary classifier. Ho and Salimans  notes that CG approximates sampling from \(p(x c)p(c x)^{}\) where \(c\) is the conditioner, and this can be simulatedwithout a classifier by mixing the score estimate of a conditional model and an unconditional model. The unconditional model can be jointly trained by dropping the conditioner \(c\) with some probability, and the same model provides score estimates for both \(p(x)\) and \(p(x c)\).

We extend the idea of classifier free guidance (CFG) to flow-matching models. The conditioner \(c\) is equivalent to \((z,x_{ctx})\) for audio models and \((y,l_{ctx})\) for duration models, which is dropped with \(p_{}\) during training. During inference, the modified vector field \(_{t}\) for the audio model becomes \(_{t}(w,x_{mis},z;)=(1+) v_{t}(w,x_{ctx},z;)-  v_{t}(w;)\), where \(\) is the strength of the guidance, and \(v_{t}(w;)\) is obtained by dropping \(x_{ctx}\) and \(z\). We use \(\) and \(_{dur}\) for the CFG strengths for the audio and the duration model, selected based on validation. Note that the computation is doubled for the same NFE when using CFG, because the model forward is called twice to compute \(_{t}\).

### Applications

We demonstrate that Voicebox exhibits in-context learning abilities similar to LLMs by presenting a few examples of how to create context to perform tasks Voicebox was not explicitly trained on. Fig. A1 shows a detailed diagram of how inputs are formatted for each task.

**Zero-shot TTS & alignment-preserved style transfer:** Given a target text \(\) and a transcribed reference audio \((x,y)\), zero-shot TTS aims to synthesize speech resembling the possibly unseen audio style of the reference. Voicebox performs the task by treating the reference audio and the target speech as one utterance where the target speech is masked. Let \(l\) and \(z\) be phone duration and frame-level transcript of \((x,y)\). The target duration \(\) is sampled given the duration context \(l\) and concatenated phone sequence \((y,)\). The target speech \(\) is then sampled given the context \(x\) and concatenated frame-level phones \((z,(,))\).

Voicebox can also convert the audio style for speech \(\) while preserving its alignment \(\). This is useful for editing audio that is synchronized with other modalities such as video. Similar to zero-shot TTS, Voicebox can simply perform the task by sampling target speech \(\) given the context \(x\) and concatenated frame-level phones \((z,)\)

**Transient noise removal & content editing:** When recording speech, one might misspeak a few words or the recording my be interrupted by unexpected background noise. In these scenarios it is desired to just edit the problematic segment instead re-recording the speech. Voicebox can perform transient noise removal through re-generating the noise corrupted segment given the original frame-level transcript and the surrounding clean audio.

For content editing, Voicebox first samples duration for the new phones given the edited phone transcript and the duration of existing phones to create the edited frame-level phone transcript. Given the new frame-level phone transcript and the audio for existing frames, Voicebox then samples the audio for frames corresponding to the new phones.

**Diverse speech sampling & alignment-preserved style shuffling:** Voicebox can generate diverse speech samples by infilling the whole utterance. We first use the duration model to sample \(\) given the phone transcript \(\). We then use the audio model to sample \(\) given \(=(,)\). Similar to style transfer, Voicebox can also shuffle the audio style while keeping the alignment by sampling \(\) conditioning on the frame-level transcript \(\) of the target speech clip \(\).

## 4 Metrics

The common goal of audio-conditioned tasks is to produce _realistic_ speech that is _coherent_ with the context and has the _correct_ textual content. For tasks not conditioned on audio context, it is desired to generate _diverse and realistic_ samples with distribution similar to training data. Prior studies often adopt subjective metrics like mean opinion scores (MOS)  which are not comparable across papers, or quantitative metrics like mel cepstral distortion  that assume the output is deterministic given input, which is often not realistic . In this paper, we advocate the following reproducible model-based perceptual metrics.

**Correctness and intelligibility:** We measure it by the word error rate (**WER**) of the synthesized speech's transcription with respect to the input text, which has been adopted in prior work . Public automatic speech recognition (ASR) models are used for comparability. For English-only setups, we follow  and use HuBERT-L  pre-trained on 60K hours of Librlight  and fine-tuned on 960 hours of Librispeech . For multilingual setups we use the Whisper large-v2 model .

**Coherence:** This is measured by the similarity between the embedding of generated speech and that of the audio context, where different embedding models would reflect coherence of different attributes. VALL-E proposed to use WavLM-TDCNN speaker embedding model, which maps an audio clip to a fixed dimensional vector, to measure voice similarity. We consider the same model to compare with VALL-E. In particular, VALL-E reports similarity with respect to _resynthesized_ audio context by its vocoder (Encodec-decoder), which we call **SIM-resyn (SIM-r)**. SIM-resyn is not comparable across models using different vocoders. Hence, we advocate for computing similarity against the original audio context, which we call **SIM-orig (SIM-o)**.

**Diversity and quality:** Frechet Inception Score (FID)  is widely adopted for image generation evaluations, which captures the similarity between generated and real images at the distribution level in some feature space. A shorter distance implies the distributions are more similar and generally reflects _both_ higher sample quality and diversity. We adapt the metric for speech by using self-supervised wav2vec 2.0 feature  and refer to it as Frechet Speech Distance (**FSD**). We verify its effectiveness in Appendix C.1 along with alternative features.

As supplementary metrics, we include quality MOS (**QMOS**) for subjective audio quality evaluation, and similarity MOS (**SMOS**) for subjective audio similarity evaluation given pairs of prompt and system-generated audio clips. Both of which are in the scale of 1 to 5 with 5 being the best. The MOS instructions and standalone metrics for duration models can be found in Appendix C.

## 5 Experiment

**Data:** We train the English-only model on 60K hours ASR-transcribed English audiobooks and the multilingual model on 50K hours of multilingual audiobooks from six languages: English (En), French (Fr), German (De), Spanish (Es), Polish (Pl) and Portuguese (Pt). The two models are abbreviated as VB-En and VB-Multi. The Montreal Forced Aligner (MFA)  is used to phenemize and force align the transcript based on the MFA phone set. Word position postfixes are added. Audio is represented as a 80-dimensional log Mel spectrogram and a HiFi-GAN vocoder trained on the same 60K hours English speech is used to generate waveform. More details about phone representation, data transformation, and vocoder can be found in Appendix A1-A3.

**Model:** Transformer  with convolutional positional embedding  and symmetric bi-directional ALiBi self-attention bias  are used for both the audio and the duration model. ALiBi bias for the flow step \(x_{t}\) is set to 0. More details in Appendix A.7. The audio model has 24 layers, 16 attention heads, 1024/4096 embedding/feed-forward network (FFN) dimension, 330M parameters. The duration model has 8 heads, 512/2048 embedding/FFN dimensions, with 8/10 layers for English/multilingual setup (28M/34M parameters in total).

**Training:** VB-En/VB-Multi audio models are trained for 500K/750K updates with an effective batch size of 240K frames. For training efficiency, audio length is capped at 1,600 frames and chunked randomly if length exceeds. Duration models are trained for 600K updates with an effective batch size of 60K frames. The Adam  optimizer is used with a peak learning rate of 1e-4, linearly warmed up for 5K steps and decays over the rest of training. The audio/duration sequence is masked with \(p_{}=0.3/0.2\), and otherwise a segment of \(r\%\) sequence length is masked, where \(r/\). \(p_{}\) is set to \(0.2\) for audio/duration models.

**Inference:** The torchdiffeq package is used, which implements both fixed and adaptive step ODE solvers. By default, the midpoint solver is used with a step size of 0.0625 (NFE=32). The regression duration model is used by default. Silence at both ends are trimmed to 0.1 second max.

**Baselines:** We consider three baselines: 1) VALL-E , SOTA for English zero-shot TTS trained on Librlight. 2) YourTTS , SOTA multilingual zero-shot TTS model trained on VCTK, LibriTTS, TTS-Portugese , and M-AILABS French. It is a flow-based model adapted from VITS  using a pre-trained speaker embedder. 3) A3T , SOTA for NAR speech editing and infilling trained with a regression loss on VCTK. We also consider Demucs , a SOTA speech enhancement model trained with regression and adversarial losses for denoising experiments.

### Monolingual and cross-lingual zero-shot TTS

Table 2 presents the zero-shot TTS results of the English model VB-En. Following , the test set is constructed by selecting 4 to 10 second long samples from Librispeech test-clean. We consider _cross-sentence_ prompting where a 3 second clip from another sample of the same speaker is used as audio context, and _continuation_ prompting where the first 3 seconds of each utterance is used. Voicebox outperforms all baselines on all metrics in both cases. In particular, Voicebox transfers style much more effectively (+0.101/+0.108 SIM-r on cross-sentence/continuation) than VALL-E, and the gap is even bigger when compared against raw audio (+0.141 SIM-o on continuation). MOS studies also confirm the quality and similarity of Voicebox are subjectively better than YourTTS.

Table 4 presents cross-lingual zero-shot TTS results, where the audio context and the target text are in different languages. Note that VB-Multi is _not_ trained on any sample with multiple languages in an utterance spoken by the same speaker. The test set is constructed using filtered MLS test split described in Appendix A.4. For each target text, we sample one 3-second long audio context from each language, which creates 36 language transfer directions in total. Voicebox yields better performance than YourTTS everywhere. Specifically, on En/Fr/Pt which YourTTS supports, Voicebox obtains 3.1%/5.9%/8.1% lower WERs and 0.136/0.141/0.160 higher similarity averaged across audio context in six languages. Addition studies on prompt lengths are presented in Appendix B.2

### Transient noise removal

We construct a noisy test set by mixing the filtered Librispeech test-clean from Section 5.1 with non-speech noise such that it overlaps with 50% of the duration at a -10dB signal-to-noise ratio. Additional conditions can be found in Appendix B.4. Table 3 presents the results comparing Voicebox with A3T and Demucs. It should be noted that A3T and Voicebox utilize transcript and location of the noise while Demucs does not. Compared to the baselines, Voicebox generates samples best on all metrics. A3T is better than Demucs on intelligibilty and quality, but the infilled speech is not coherent because it is only trained on VCTK.

    &  &  &  &  &  &  &  \\  & & **WER** & **SIM-o** & **WER** & **SIM-o** & **WER** & **SIM-o** & **WER** & **SIM-o** & **WER** & **SIM-o** & **WER** & **SIM-o** \\  GT & - & 5.9 & 0.725 & 5.0 & 0.636 & 4.1 & 0.729 & 5.2 & 0.714 & 4.9 & 0.743 & 5.8 & 0.725 \\   & De & n/a & n/a & 7.3 & 0.373 & n/a & n/a & 11.3 & 0.361 & n/a & n/a & 13.7 & 0.263 \\  & En & n/a & n/a & 7.0 & 0.403 & n/a & n/a & 11.4 & 0.298 & n/a & n/a & 14.1 & 0.234 \\  & Es & n/a & n/a & 7.6 & 0.327 & n/a & n/a & n/a & 11.6 & 0.316 & n/a & n/a & 13.5 & 0.256 \\  & Fr & n/a & n/a & 7.6 & 0.363 & n/a & n/a & 10.7 & 0.459 & n/a & n/a & 13.1 & 0.299 \\  & Pl & n/a & n/a & 7.8 & 0.349 & n/a & n/a & 11.8 & 0.370 & n/a & n/a & 15.1 & 0.308 \\  & **P** & n/a & n/a & 7.6 & 0.322 & n/a & n/a & n/a & 11.8 & 0.297 & n/a & n/a & 13.6 & 0.346 \\   & **AVG** & n/a & n/a & 7.5 & 0.356 & n/a & n/a & 11.4 & 0.350 & n/a & n/a & 13.9 & 0.299 \\    & De & 4.8 & 0.632 & 4.8 & 0.522 & 3.6 & 0.442 & 5.3 & 0.489 & 5.5 & 0.449 & 5.4 & 0.420 \\   & En & 5.9 & 0.435 & 4.2 & 0.535 & 4.1 & 0.423 & 6.8 & 0.423 & 8.3 & 0.402 & 7.6 & 0.385 \\   & Es & 4.9 & 0.460 & 4.3 & 0.479 & 3.6 & 0.613 & 5.3 & 0.473 & 5.2 & 0.436 & 5.4 & 0.435 \\   & Fr & 4.9 & 0.476 & 4.3 & 0.485 & 3.7 & 0.479 & 5.1 & 0.602 & 4.8 & 0.408 & 5.4 & 0.418 \\   & Pi & 4.7 & 0.491 & 3.8 & 0.503 & 3.5 & 0.528 & 5.1 & 0.503 & 4.0 & 0.641 & 4.9 & 0.476 \\   & Pi & 4.9 & 0.422 & 4.6 & 0.426 & 3.7 & 0.476 & 5.5 & 0.453 & 4.8 & 0.406 & 5.2 & 0.620 \\    & **AVG** & 5.0 & 0.486 & 4.4 & 0.492 & 3.7 & 0.494 & 5.5 & 0.491 & 5.5 & 0.457 & 5.7 & 0.459 \\   

Table 4: Multilingual zero-shot TTS results on filtered MLS test sets. GT/YT/VB-Multi refers to ground truth/YourTTS/multilingual Voicebox. “Ref” column shows the audio context language.

  
**Model** & **WER** & **SIM-o** & **QMOS** \\  Clean speech & 2.2 & 0.687 & 4.07\(\)0.15 \\ Noisy speech & 41.2 & 0.287 & 2.50\(\)0.15 \\  Demucs & 32.5 & 0.368 & 2.86\(\)0.17 \\ A3T & 11.5 & 0.148 & 3.10\(\)0.15 \\ VB-En (\(=0.7\)) & 2.0 & 0.612 & 3.87\(\)0.17 \\   

Table 3: Transient noise removal where noise overlaps with 50% of the speech at a -10dB SNR.

### Diverse speech sampling and application to ASR data generation

Table 5 compares the ability to generate diverse samples for Librispeech test-other text. We consider English Voicebox (VB-En) with regression (regr) or flow-matching (FM) duration models. VITS-VCTK additionally conditions on a speaker ID, which we randomly sample for each sentence. YourTTS conditions on text and a reference audio, which we draw from the LS train splits. Qualitatively, A3T generates the same robotic voice and VITS-LJ generates high quality but from a single voice, hence both yield high FSD (bad quality or diversity) but VITS-LJ has a low WER. VITS-VCTK improves the voice diversity and FSD and YourTTS further advances it as it is trained on more speakers. Voicebox models (with different duration samplers) outperform the baseline on FSD by large margins, showing Voicebox's ability to produce realistic and diverse samples whose distribution is close to the training data. Among them, the FM duration model creates more varying speaking styles compared to the regression one which ASR may struggle more to recognize.

We next train an ASR model using _only synthetic speech_ and evaluate it on _real speech_, which has not been successful before because synthetic data were not realistic and representative enough. Table 6 compares real and synthetic data from Voicebox and three baseline models. Each TTS model generates one sample per text from the Librispeech training set, resulting in 281K utterances per system. For real data, we consider train-960 and train-clean-100. Details about the ASR model and training are in Appendix A.5. The results are highly correlated with the FSD scores for synthetic data. In particular, the ASR model trained on Voicebox data with FM duration model reduces WERs by over 85% compared to baselines, and only lags behind real data by 0.4% and 1.7% absolute.

### Inference efficiency versus performance

We examine the trade-off between the metrics of interest (WER, SIM, FSD) for different settings of guidance strength (\(\)) and NFE specified by the user. Fig. 2(a) shows the Voicebox inference time to generate an audio sample of 10 seconds (including vocoding and predicting duration) as NFE varies and compares that to VALL-E.3 For NFE=2 without CFG, Voicebox takes about \(0.31\) seconds, about \(20\) times faster than VALL-E. At NFE=64, Voicebox is only \(4\%\) slower than VALL-E.

Next, we study the cross-sentence setup of Section 5.1 to analyze the impact on WER and SIM-r. We find that for all settings Voicebox has better WER than VALL-E. WER remains stable with mean of \(2.0\) and variance of \(0.005\). WER plot can be found in Appendix B.5. As shown in Fig. 2(b), in the case of SIM-r, lower classifier guidance strength values (\(=0\) or \(0.3\)) produce higher speaker similarity when operating in a lower NFE regime (\( 4\)). However, starting from NFE=8, a higher classifier guidance strength improves speaker similarity. Finally, in Fig. 2(c) we examine FSD by generating samples for Librispeech test-other text. We find that lower classifier guidance strength produces lower FSD scores and more diverse samples. Increasing the NFE for each setting improves FSD.

### Ablation on generative modeling approaches

We compare three generative modeling approaches: the proposed flow-matching with the OT path (FM w/ OT), flow-matching with the variance preserving (VP) diffusion path (FM w/ diff), and score

    &  \\  &  &  \\
**ASR training data** & test- & test- & test- & test- \\  Real audio (100hr) & 9.0 & 21.5 & 6.1 & 16.2 \\ Real audio (960hr) & 2.6 & 6.3 & 2.2 & 5.0 \\  VITS-LJ & 58.0 & 81.2 & 51.6 & 78.1 \\ VITS-VCTK & 33.8 & 55.5 & 30.2 & 53.1 \\ YourTTS (ref=LStrain) & 25.0 & 54.6 & 20.4 & 51.2 \\ VB-En (\(=0\), dur=regr) & 7.1 & 17.6 & 6.5 & 14.6 \\ VB-En (\(=0\), dur=FM, \(_{dev}=0\)) & 3.1 & 8.3 & 2.6 & 6.7 \\   

Table 6: Performance of ASR models trained on real or synthetic speech, tested on _real_ speech and decoded with or without a 4-gram language model.

  
**Model** & **WER** & **FSD** \\  Ground truth & 4.3 & 171.1 \\  _require additional input_ & & \\ VITS-VCTK & 10.6 & 306.6 \\ YourTTS (ref=LStrain) & 9.0 & 277.9 \\  _text-only_ & & \\ A3T & 37.9 & 373.0 \\ VITS-LJ & 5.6 & 344.2 \\ VB-En (\(=0\), dur=regr) & 3.1 & 155.7 \\ VB-En (\(=0\), dur=FM, \(_{dev}=0\)) & 5.6 & 159.8 \\   

Table 5: Diverse speech generation from LS test-other text.

matching with the VP diffusion path (SM w/ diff). A reduced setup described in B.3 is adopted, with a lowered learning rate (1e-4) and the loss in Eq. (1) to ensure convergence for all three objectives.

We vary the number of training and inference steps, and evaluate models on the zero-shot TTS task (Section 5.1). Results in Table 7 shows that FM w/ OT trains significantly faster than the other two objectives, achieving the best performance with 100K training steps, and even outperforms SM w/ diff using only 50K updates. Results in Table 8 shows superior inference efficiency of FM w/ OT, which can produce good results with just 8 NFEs, while FM w/ diff requires at least 8 NFEs and SM w/ diff requires over 32 NFEs. Complete results are in Table B5

## 6 Conclusion

This paper presents Voicebox, the most versatile generative model for speech. By learning to solve a text-guided speech infilling task on large scale multilingual datasets with a power model and training objective Voicebox demonstrates impressive task generalization capabilities. Voicebox achieves state-of-the-art performance on mono and cross-lingual zero-shot TTS, speech inpainting, and diverse speech sampling, and can generate speech 20 times faster than the best autoregressive models.

With high fidelity speech generation models like Voicebox, it brings the potential of misuse and unintended harm. To mitigate the risk, we also detail in Appendix B.1 that a highly effective classifier can be built to distinguish between authentic and synthetic speech. Voicebox is now trained only on read speech from audiobooks in six languages, and cannot transfer one attributes (e.g., emotion) from a reference while transferring another attribute (e.g., voice) from another reference. Due to space limit, we expand our discussion of limitation and broader impact in Appendix D. For future work, we would continue scaling the model and the data to include more languages and diverse types of speech such as conversations, and explore disentangled prompting for different attributes.

    &  &  &  \\  & WER & Sim-o & WER & Sim-o & WER & Sim-o \\  FM w/ OT (proposed) & **2.5** & **0.424** & **2.2** & **0.487** & **2.1** & **0.508** \\ FM w/ diff & 76.0 & 0.066 & 3.1 & 0.344 & 2.6 & 0.478 \\ SM w/ diff & 73.3 & 0.062 & 17.4 & 0.176 & 5.1 & 0.349 \\   

Table 7: Comparing different objectives on training efficiency. 32 NFEs are used for inference. Each model is evaluated on the monolingual zero-shot TTS task.

    &  &  &  &  \\  & WER & Sim-o & WER & Sim-o & WER & Sim-o & WER & Sim-o \\  FM w/ OT (proposed) & **2.4** & **0.410** & **2.2** & **0.481** & **2.2** & **0.503** & **2.1** & **0.508** \\ FM w/ diff & 11.5 & 0.171 & 3.0 & 0.359 & 2.7 & 0.447 & 2.6 & 0.478 \\ SM w/ diff & 94.5 & 0.054 & 42.3 & 0.076 & 11.5 & 0.218 & 5.1 & 0.349 \\   

Table 8: Comparing different objectives on inference efficiency. All models are trained for 150K updates. Each model is evaluated on the monolingual zero-shot TTS task.

Figure 3: Trade-off between NFE and different metrics. Inference time will be doubled with CFG.