# Optimal Classification under Performative Distribution Shift

Edwige Cyffers

Univ. Lille, Inria, CNRS, Centrale Lille,

UMR 9189 - CRIStAL, F-59000 Lille

edwige.cyffers@inria.fr

&Muni Sreenivas Pydi

Universite Paris Dauphine, Universite PSL,

CNRS, LAMSADE, 75016 Paris &Jamal Atif

Universite Paris Dauphine, Universite PSL,

CNRS, LAMSADE, 75016 Paris &Oliver Cappe

Ecole Normale Superieure, Universite PSL,

CNRS, Inria, DI ENS, 75005 Paris

###### Abstract

Performance learning addresses the increasingly pervasive situations in which algorithmic decisions may induce changes in the data distribution as a consequence of their public deployment. We propose a novel view in which these performative effects are modelled as push-forward measures. This general framework encompasses existing models and enables novel performative gradient estimation methods, leading to more efficient and scalable learning strategies. For distribution shifts, unlike previous models which require full specification of the data distribution, we only assume knowledge of the shift operator that represents the performative changes. This approach can also be integrated into various change-of-variable-based models, such as VAEs or normalizing flows. Focusing on classification with a linear-in-parameters performative effect, we prove the convexity of the performative risk under a new set of assumptions. Notably, we do not limit the strength of performative effects but rather their direction, requiring only that classification becomes harder when deploying more accurate models. In this case, we also establish a connection with adversarially robust classification by reformulating the minimization of the performative risk as a min-max variational problem. Finally, we illustrate our approach on synthetic and real datasets.

## 1 Introduction

Machine learning models are increasingly deployed in real-world scenarios where their predictions can influence the users' behaviors, thereby altering the underlying data distribution. This phenomenon, though rooted in long-standing economic theory , has recently attracted interest in the machine learning community under the name of _performance prediction_. Consider for instance a social ranking system: if it consistently favors a particular subpopulation of individuals, user behavior might shift towards mimicking the main characteristics of this subgroup or, conversely, some features of this subpopulation can undergo modification as a consequence of the selection by the system, both effects leading to subtle alterations of the original data distribution. More generally, performative learning captures dynamics at stake in strategic classification, where individuals are confronted by algorithmic decisions that impact their life - such as loan acceptance, college admission, probation - and might thus try to overturn predictions by optimizing some of their features.

This feedback loop, where predictions influence future data, poses new challenges and necessitates the development of novel approaches within statistical learning theory and practice .

2020, Jagadeesan et al., 2022, Drusvyatskiy and Xiao, 2023, Hardt and Mendler-Dunner, 2023, Zezulka and Genin, 2023]. Perdomo et al.  proposed to formalize performative learning as a generalized risk minimization problem, with the _performance risk_ being defined as

\[()=_{}[(Z;)], \]

where \(\) is a loss function, \(\) a model's parameters, and \(Z\) an observable random variable drawn from a distribution \(_{}\) also parametrized by \(\) itself. In light of the difficulty of minimizing \(()\) directly, one can define a _decoupled performative risk_ as \((,^{})=_{}[(Z;^{})]\), clarifying the interplay between the model's prediction and the distribution change. This can be seen as a Stackelberg game that stabilizes when neither the modeler (learned parameters) nor the environment (distribution) has incentive to change their states. Solving the performative learning problem consists in minimizing this risk under the constraint that \(=^{}\), because the testing samples will follow the distribution corresponding to the deployed model, and thus \(()=(,)\). Minimizing \((,^{})\) w.r.t. \(^{}\) for a fixed \(\) corresponds to the classical machine learning setting. In contrast, estimating the performative effect, i.e., knowing how to optimize \(\) for a given \(^{}\) is more challenging as, per definition, one can only perform statistics from samples collected for values of the parameters \(\) for which the model has already been deployed. Hence, performative learning does require some form of counterfactual extrapolation, i.e., what will happen to the data distribution when the parameter \(\) changes from its current setting?

Hence, instead of focusing on methods finding performatively optimal points, \(_{PO}*{arg\,min}()\), many previous works, following Perdomo et al. , focus on finding stable points \(_{PS}*{arg\,min}(_{PS},)\), through methods that iteratively minimize the empirical risk. This line of research is appropriate in settings where the performative effect can be tamed. If it is sufficiently small, explicitly taking into account the performative changes of distribution is not required and optimal and stable points will be close enough [Perdomo et al., 2020]. However, real use cases do not always satisfy such strong assumptions (see further discussion in Section 4). In general, stable points may not be good proxys for performatively optimal points, particularly in settings where the performative effect cannot be bounded a priori.

Towards this goal, another line of research focuses on finding the optimal points \(_{PO}\). Izzo et al.  propose to use Monte Carlo sample-based approximations of the gradient of the performative risk, \(_{}\,()\), based on the score function estimator (see Section 2 below). Miller et al.  use a two-stage approach that deploys random models to estimate the performative effect in the first stage, and then minimizes the estimated performative risk in the second stage. A drawback of both of these approaches is the restrictive set of assumptions needed to show that the algorithms converge. While Izzo et al.  assume the convexity of \(()\) along with smoothness and boundedness conditions, Miller et al.  assume that the loss function is simultaneously strongly convex and smooth. Moreover, the score function estimator of Izzo et al.  necessitates full knowledge of a parametric form of \(_{}\), which is unrealistic in practice. Alternatively, Jagadeesan et al.  resort to derivative-free (or zeroth-order) optimization strategies. However, such an approach is appropriate only when it is possible to sequentially deploy a large number of model instances, and it does not scale with the dimension of the parameter \(\).

The present work is connected to the second line of the research explored above, where the focus is on finding the optimal point \(_{PO}\). Our contributions are as follows.

**(i) Model the performative effect as a push-forward operator** This novel approach provides a _new explicit expression of the performative gradient_. Not only does this approach allow estimation of the performative gradient in settings where previous methods couldn't, but we show that in typical use cases, the variance of this new estimator is significantly smaller.

**(ii) Convexity for Performative Classification** We then focus on the specific task of strategic classification, as this performative learning problem encompasses various real-use cases with important societal impact such as college admission or credit decisions. Our second contribution is to provide new convexity results on the performative risk in this case. Whereas existing results were only proving convexity under assumptions restricting the performative effect to be small compared to the (assumed) strong convexity of the loss function \((z;)\), our results leverage structural assumptions on the performative effect that ensure that the performative risk is convex _without any restriction on the strength of the performative effect_.

**(iii) Linking Performative and Robust Learning** We establish a connexion between performative learning and adversarially robust learning, paving the way to transferring robustness results to the performative learning field. In particular, this result gives new insights on the empirical evidence in favor of using regularization in the presence of performative effets. Finally, we illustrate our findings on synthetic and real-world datasets.

## 2 Push-forward Model for Performative Effects

In this section, we study the general performative learning setting without yet specializing it to the classification context. In Section 2.1, we introduce the push-forward model of performative learning and derive the expression of the gradient of the performative risk under this model. In Section 2.2, we present a reparameterization-based estimator for the gradient of the performative risk, and compare it to the score function based estimator considered by Izzo et al. (2022).

### The Push-forward Model

We aim to minimize the performative risk defined in eq.1, where the observation \(Z\) is drawn from the distribution \(_{}\), which depends on the parameter \(^{p}\) of the learning model. For this to be tractable, one needs additional hypotheses on the nature of the performative effect. We propose to represent the performative effect through a push-forward measure, which matches the intuition of having an untouched distribution that is steered by the performative effect.

**Assumption 1** (Push-forward Performative Model).: _For a given model parameter \(^{p}\), the samples' distribution under the performative effect is given by \(_{}=(;)_{}\), where \((;)\) is a differentiable invertible mapping on \(^{d}\), depending on \(\)._

This assumption can be equivalently stated by the probabilistic representation \(Z}{{=}}(U;)\), where \(U\) (the symbol \(}{{=}}\) denoting equality in distribution). If \(\) admits a density, then so does \(_{}\) with density function given by \(p_{}(z)=|J_{z}(z;)|p((z;))\) where \((;)=^{-1}(;)\). In this last formula, \(J_{z}(z;)\) refers to the Jacobian matrix where \([J_{z}(z;)]_{ij}=}{ z_{j}}\).

From an abstract point of view, such a representation of a parametrized family of distributions exists under very general conditions. However, the above model is more interesting in scenarios where \((;)\) is a simple operator, for instance a linear one, as is the case in most of the examples considered by Perdomo et al. (2020); Miller et al. (2021), and the dependence with respect to \(\) can also be made explicit. This representation is also modular in the sense that \(\) could be chosen as the composition \((u;)=_{0}(_{1}(u;))\), where \(_{0}^{-1}(Z)\) corresponds to a fixed (not depending on \(\)) representation of \(Z\) in a feature space and \(_{1}(;)\) models the performative effect _in the representation space_. Although we will not explicitly consider such cases in the rest of the paper, this representation of the performative effect is particularly attractive when using embedding tools based on kernels (Hofmann et al., 2008), neural nets (e.g., VAEs (Kingma and Welling, 2013)) or normalizing flows (Papamakarios et al., 2021; Kobyzev et al., 2021).

This structural assumption on the performative effect yields a new estimator for the performative gradient, which may be seen as an instance of the "reparametrization trick" used in VAEs, normalizing flows or by Kucukelbir et al. (2017). Mohamed et al. (2020) also refer to this approach as "pathwise" gradient estimation.

**Theorem 1** (Performative Risk Gradient).: _Under Assumption 1, the gradient of the performative risk is given by_

\[_{}()=_{}[_{ }(Z;)]+_{}[_{ }^{T}((Z;);)_{z}(Z;)], \]

_where \(_{z}(z;)\) and \(_{}(z;)\) denote respectively the gradient with respect to the first and the second parameter of the loss, and \(_{}^{T}(u;)\) is the transpose of the Jacobian with respect to \(\)._

Proof.: Notice that under assumption 1, we can rewrite the decoupled risk with a change of variable as \((,^{})=[((U;); ^{})]\). This expression leads to the following.

\[_{}()=_{}[( (U;);)]=[_{}((U; );)+_{}^{T}(U;)_{z} ((U;));)],\]

which gives eq.2 under a change of variable.

### Estimating the Performance Gradient

From Theorem 1, it is clear that the gradient of performative risk in eq. (2) is composed of two terms - the first term corresponds to the classical risk minimization, while the second one - which we will refer to as the performative gradient in the following - captures the performative effect. The first term can be estimated by \(_{i=1}^{n}(Z_{i};)\) as usual. For the second term, we propose the following estimator.

**Definition 1** (Reparameterization-based Performative Gradient Estimator).: _The performative gradient \(_{}(,^{})|_{^{}=}\) admits as unbiased estimator:_

\[^{}_{}=_{i=1}^{n}_{ }^{T}((Z_{i};);)_{z}(Z_{i};). \]

This estimator allows performing gradient descent to minimize the performative risk and thus, if the performative objective is well behaved, to converge to the performative optimal point. \(^{}_{}\) should be compared to the following estimator used by Izzo et al. (2022), which relies on the well-known score function formula -see (L'Ecuyer, 1991; Kleijnen and Rubinstein, 1996; Mohamed et al., 2020) and references therein.

\[^{}_{}=_{i=1}^{n}(Z_{i};) _{} p_{}(Z_{i}).\]

While both \(^{}_{}\) and \(^{}_{}\) estimate the same quantity \(_{}(,^{})|_{^{}= }\), \(^{}_{}\) has two distinct advantages over \(^{}_{}\). First, computing \(^{}_{}\) requires access to the analytical form of \(p_{}\), which is fairly unrealistic in a learning scenario, whereas our estimator \(^{}_{}\) only requires knowledge of \(\), paving the way for a _semi-parametric approach_ in which the performative effect is modelled explicitly, but not the distribution of the data. For general maps \(\), \(^{}_{}\) still requires to use the inverse mapping \(\), however this is not required in situations where the Jacobian \(_{}(u;)\) does not depend on \(u\). Specifically, when \(\) is a shift operator, one obtains a very simple expression for \(^{}_{}\) as shown in the following example.

**Example 1** (Shift Operator).: _If the performative effect can be modelled by a shift operator, i.e., \((U;)=U+()\), the \(^{}_{}\) estimator is given by:_

\[^{}_{}=_{}^{T}()\,_{i=1}^{n}_{z}(Z_{i};),\]

_where \(_{}()\) is the Jacobian of the performative shift \(()\)._

In addition to removing the need to know \(p_{}\), a second advantage of \(^{}_{}\) is that it can lead to significant decrease of the variance of the estimates, as illustrated by the following example.

**Example 2** (Performative Gaussian Mean estimation).: _Let \((z;)=\|z-\|^{2}/2\), and \(ZU+\), that is, \(()=\) is a linear shift operator. We will assume \(U(0,^{2}I_{d})\), so that \(p_{}(z)[-\|z-\|^{2}/(2^{2})]\), where \(\) represents the performative effect. The gradient of \((,^{})\) w.r.t. the distributional parameter \(\) is given both by_

\[_{}(,^{}) =_{}[^{T}_{z}(Z;^{})]= ^{T}_{}[Z-^{}]=^{T}[U+a]\] _(reparameterization)_ \[=_{}[(Z;^{})_{} p _{}(z)]=^{T}}[\|U+a\|^{2}U]\] _(score function)_ _where \[a=-^{}\]. Hence, in this case \[^{}_{}=^{T}_{i=1}^{n}(U_{i}+a)\], while \[^{}_{}=^{T}}_{i=1}^{n}\|U_ {i}+a\|^{2}U_{i}\]. Both of these expressions have equal expectation \[^{T}(-^{})\] which corresponds to the gradient of \[(,^{})\] w.r.t. \[\]. However, the reparametrization estimator \[^{}_{}\] has covariance \[^{2}^{T}/n\] while the score-based estimator \[^{}_{}\] has covariance:_ \[^{T}(+6d+8)^{2}+2(d+4)\|a\|^{2}+\|a\|^{4}/ ^{2}}{4}I_{d}+aa^{T}).\]The details of this computation can be found in appendix A.1. Both estimators are unbiased but note that while \(^{}_{}\) would always be an unbiased estimator of the performative gradient without any further assumption on the distribution of \(U\), the unbiasedness of \(^{SF}_{}\) relies on the fact that \(U\) is Gaussian. \(^{RP}_{}\) has a covariance that does not depend on \(\), \(^{}\) nor on the dimension \(d\). In contrast, \(^{SF}_{}\)'s covariance includes a factor that increases with \(d^{2}\), making it unreliable in high dimensions. It also includes additional terms that grow with the norm of \(-^{}\), so the estimator becomes less reliable when the performative effect is strong.

One could argue that the previous result does not provide a fair comparison between both estimators, as \(G^{SF}\)'s variance can be reduced by subtracting a baseline. Indeed, \(^{}_{}=_{i=1}^{n}((Z_{i};)-m) _{} p_{}(Z_{i})\) is also an unbiased estimator of the gradient of the performative effect (for any choice of the baseline \(m\)), as the score function has, by definition, zero expectation. Tuning \(m\) properly may reduce the variance by creating a so-called _control variate_ --see, e.g., Greensmith et al. (2004) for the use of this principle in policy gradient methods. However, similar calculations (detailed in appendix A.1) show that the minimum covariance that can be achieved by subtracting a baseline is

\[^{T}(((1+d/2)^{2}+\|a\|^{2})I_{d}+aa^{T} ),\]

which is still larger than the covariance of \(^{}_{}\) by a factor that grows with the model dimension \(d\). The fact that the reparameterization-based estimator is preferable when considering the Gaussian distribution with quadratic loss function was observed before by Mohamed et al. (2020) in the scalar case. The above computations however show that the difference between the two approaches gets more and more significant as the dimension increases. The case of other distribution/loss function combination still needs to be investigated.

## 3 Classification under Performative Shift

In this section, we specialize to the setting of binary classification which encompasses various machine learning applications where performative effects are expected. Usually, this setting involves a desirable class and an undesirable one. For example, the desirable class might represent college admission, loan acceptance, no-spam email, or probation. In this setting, one can also expect that individuals belonging to the favored class - we designate this as class \(1\) - do not need to alter their features, or only with small changes. On the contrary, individuals with negative predictions - in class \(0\) - have an incentive to modify their features, resulting in a significant performative effect.

We particularize the arguments introduced in section 2 to the setting of binary classification, by fixing \(z=(x,y)\), with a covariate vector \(x^{d}\) and a label \(y\{0,1\}\). As is done classically --see, e.g., (Bach, 2024), we further assume that the classifier \(f_{}(x)\) is a real valued function that depends on a parameter \(^{p}\) and that a convex loss surrogate \(\) is used, such that the loss function \((z;)\) is equal to \(((-1)^{y}f_{}(x))\). We model the performative effect as label-dependent push forward models, i.e., that, under \(_{}\),

\[X|_{Y=1}}}{{=}}_{1}(U_{1};)X|_{Y=0}}}{{=}}_{0}(U_{0};),\]

where \(_{1}\) and \(_{0}\) represent the performative changes affecting class-conditional distributions of classes \(0\) and \(1\) respectively. If the classifier \(f_{}(x)\) is sufficiently expressive, changes such that \(_{1}=_{0}\) will not create performative effects. We thus focus on scenarios where the _performative changes affect each class-dependent distribution differently_. For concreteness, we will assume the following.

**Assumption 2**.: \(_{}(Y=1)=\) _is fixed and not subject to performative effects._

**Assumption 3**.: \(_{1}\) _does not depend on \(\), and for simplicity, we assume it is the identity function._

**Assumption 4**.: \(_{0}(u_{0};)=u_{0}+()\) _is a shift operator._

Assumption 2 is a consequence of the intuitive property that even if the distribution is modified, the ground truth labels are not impacted by the performative effect. Assumption 3 allows to focus on the performative effect on the unfavored class and simplifies the presentation but could be easily relaxed. Finally, assumption 4 is restricting the performative change to a shift, which does simplify the problem but still corresponds to a realistic model for feature alteration.

It is important to stress that, despite the fact that this performative effect is modelled as a shift, the joint distribution of \(Z=(X,Y)\) does not belong to the location-scale family discussed by Miller et al. (2021). Under these assumptions, the decoupled performative risk takes the following form:

\[(,^{})=_{}[((-1)^{Y} f_{^{}}(X))]=[(f_{^{}}(U_ {1}))]+(1-)[(-f_{^{}}(U_{0}+( )))], \]

where the performative effect is only manifested in the second term, which corresponds to class 0.

**Remark 1** (Localization of the Performative Shift).: _In eq. (4), we refer to \(U_{0}\) which corresponds to the covariates of the second class in the absence of performative effect, i.e., when \(=0\). However, for a shift operator, for any value of \(\), one may equivalently write that, under \(_{}\), \(X(U_{};)\), where \((u;)=u+()-()\) and \(U_{}\) is distributed under \(_{}\). Thus eq. (4) can equivalently be rewritten by taking expectation under an arbitrary parameter value \(\), upon defining the performative effect as \(U_{}+()-()\) and the linear model as \(f_{}(x)=x^{T}(-)\)._

## 4 Convexity of Performative Risk

Identification of the cases in which the performative risk \(()\) is convex is an important step towards generalizing results obtained in the context of traditional (ie., non performative) learning theory. Existing results mainly exploit the fact that if the loss function is strongly convex, a sufficiently small performative effect cannot break this convexity. For this reason, it is often assumed (Miller et al., 2021; Hardt and Mendler-Dunner, 2023) that the change in the distributions has a bounded sensitivity with respect to the parameters, using the 1-Wasserstein distance:

\[W_{1}(P_{},P_{^{}})\| -^{}\|_{2}.\]

Note that in our setting, such \(\) exists and corresponds to the operator norm of \(()\). In order to preserve convexity, it is then needed that \(/2L\) when the risk is \(L\)-smooth and \(\)-strongly convex. The pricing model, considered by Izzo et al. (2022), is a very simple example showing that the performative risk can be convex while not fulfilling this criterion.

**Example 3** (Pricing Model).: _Given a fixed set of \(d\) resources, the pricing model aims at finding the prices \(^{d}\) for the \(d\) resources that maximize the overall profit given the elasticity of the demand level:_

\[(z;)=-z^{T}Z(U;)=U- , \]

_where \(\) is a diagonal matrix with positive elements, encoding the elasticity of the demand level to a raise in price of each resource, and \(=[U]\) contains the baseline demand levels for each resource._

In this example, the performative risk \(()=-_{i=1}^{d}(_{i}-_{ii}_{i})_{i}\) is a strongly convex quadratic function minimized at \(_{i}^{*}=_{i}/(2_{ii})\). In contrast, the decoupled performative risk \((,^{})=-(-)^{T}^{}\) is still convex, but not strongly convex in \(\) and is always minimized in \(^{}\) at infinity. Despite its simplicity, this example is thus not covered by existing theorems, and retraining procedures considered by Perdomo et al. (2020) fail by diverging.

Moreover, this example highlights that requiring a small sensitivity for the performative effect does not match the true convexity conditions. The performative risk is indeed strongly convex as long as the \(_{ii}\) are positive, irrespectively of their magnitude. In contrast, in this example, the performative risk would become non-convex if one of the \(_{ii}\) were negative, even with a small magnitude.

This motivates the search for related phenomenons in the classification context, by looking at conditions for ensuring the convexity of eq. (4). Indeed, we show that in the classification setting, one can also observe convexity without restriction on the magnitude of the performative effect. For this, we consider the case of linearly parameterized models, which we denote for simplicity by \(f_{}(x)=x^{T}\). Note that, as discussed in Section 2, our model of performative effect is composable because, we could also consider the more general linearly parameterized model in which \(f_{}(x)=(x)^{T}\), with a linear-in-the-parameter performative effect in the feature space such that \((X)=U+()\). For ease of notation, we stick to the case where \(\) is the identity function in the following. Using standard arguments, the choice of a convex loss surrogate \(\) then entails that \((,^{})\) is a convex function of \(^{}\). For the same reason, if \(()=\) is a linear-in-parameters shift operator, \((,^{})\) is also convex in \(\). Note however that, unless there is no performative effect (i.e., if \(=0\)), eq. (4) is not jointly convex in \((,^{})\). The following result show that it is nonetheless the case that the performative risk \(()=(,)\) is convex under the condition that \(\) is a positive semidefinite matrix (see proof in appendix A.2).

**Theorem 2** (Convexity of Classification Performative Risk).: _Under assumptions 2 to 4, for linearly parameterized classifier \(f_{}(x)=x^{T}\) and linear shift operator \(()=\), the performative risk \(()\) is convex when \(\) is a positive semidefinite matrix and one of the following conditions holds._

* \(\) _is the quadratic loss function;_
* \(\) _is a convex non increasing function (such as hinge, logistic or exp loss)._

This theorem allows us to extend the known convexity results to losses that are not strongly convex, and to performative effects with arbitrary magnitude.

**Remark 2** (Generalization to Performative Effect Affecting Both Classes).: _One could remove assumption 3 to allow class \(1\) to change under performative effect. The convexity of \(()\) remains if \(_{1}(u;)=u-_{1}\), where \(_{1}\) is a positive semidefinite matrix. Similarly, the classification task becomes harder with performative effects and the performative risk is convex._

## 5 Connection with Robustness and Regularization

In order to enforce strong convexity of the loss function \((;)\), previous works on performative prediction have considered the use of an additional regularization term --see, e.g., Section 5.2 of  where logistic regression is used with a ridge regularizer. When doing so, it has been observed empirically that the retraining method performs quite well. To build on this observation, we show below that for linear-in-the-parameter performative effects that tends to make the classification task harder, the performative optimum may indeed be interpreted as a regularized version of the base classification problem. This regularization does not take the form of and additive penalty but can be interpreted as the solution of a specific adversarially robust classification objective. In this section, we use the slightly stronger assumption that \(\) is a symmetric positive definite matrix, in order to ensure that both \(\|v\|_{}=(v^{T} v)^{1/2}\) and \(\|v\|_{^{-1}}=(v^{T}^{-1}v)^{1/2}\) are norms on \(^{d}\).

**Theorem 3** (Variational Formulation of the performative Risk).: _Under assumptions 2 to 4, for linearly parameterized classifiers \(f_{}(x)=x^{T}\) and linear shift operators \(()=\), and assuming that \(\) is a convex non increasing function and that \(\) is symmetric positive definite, the performative risk may be rewritten as_

\[()=[(U_{1}^{T})]+(1-) [_{\{ U_{0}:\| U_{0}\|_{^{-1}}\|\|\}} (-(U_{0}+ U_{0})^{T})]. \]

Intuitively, for a classification-calibrated loss function  and classes with identical covariances, we expect \(\) to align with the direction of \(_{1}()-_{0}()\), so that, when \(\) is positive definite, the performative shift \(\) has itself a positive dot product with \(_{1}()-_{0}()\). The reformulation of the performative risk in eq. (6) formalizes this intuition by showing that the performative optimum is associated to an adversarially robust classification task  in which the points of class \(0\) are allowed to shift towards those of class \(1\), so as to increase the overall loss. Compared to objectives found in the robust

Figure 1: Profile risk for classifying two Gaussian centered in \(_{0}=(0,0)\) and \(_{1}=(-1,1)\) with quadratic loss and various values of \(\) for the diagonal coefficients of \(\). The performative risk remains convex as long as \(\) is positive semidefinite i.e. \( 0\), and becomes non-convex whenever some of the \(_{i}\) are negative.

classification literature, the specificity of eq. (6) lies in the fact that the tolerance (or budget) on the adversarial displacement \( U_{0}\) depends on both \(\) and \(\).

To understand the role played by the \(\|\|_{}\) and \(\|\|_{-1}\) norms, consider the particular case where only a subset of the variables have a performative effect, i.e., if we let \(\) and \(U_{0}\) be partitioned into

\[=_{p}\\ \\ _{s}U_{0}=U_{0,p}\\ \\ U_{0,s},\]

with \(_{p}= I\) and \(_{s}= I\), one obtains, letting \(\) tend to zero, that the performative risk is equal to

\[()=[(U_{1}^{T})]+(1-) [_{\{ U_{0,p}:\| U_{0,p}\|\|_{p }\|\}}(-(U_{0,s}^{T}_{s}+(U_{0,p}+ U_{0,p})^{T}_{p }))].\]

The above expression shows that in this case, only the coordinates subject to the performative effect appear in the adversarial reformulation.

In the proof of Theorem 3 (see appendix A.3), we observe that the second term of eq. (6) may also be rewritten as \((1-)[(-U_{0}^{T}-\|\|_{}^{2})]\). Similarly to the case studied by Ribeiro et al. (2023), the term \(\|\|_{}^{2}\) that appears inside the surrogate loss function \(\) has a regularization effect. Note however that it is not equivalent to the use of a standard ridge regression penalty on \(\). The following theorem provides a bound on the performative optimum that highlights the role played by \(\) on the significance of this regularization effect.

**Theorem 4** (Regularization Bound).: _Define \(_{i}=[U_{i}]\). Under assumptions 2 to 4, for linearly parameterized classifiers \(f_{}(x)=x^{T}\) and linear drift operators \(()=\), when \(\) is a convex non increasing function and \(\) a symmetric positive definite matrix, the minimizer \(^{*}\) of \(()\) satisfies the following condition._

\[\|^{*}\|_{}}(_{1}-(1-)_{0} )\|}{1-}. \]

Theorem 4 shows that the performative optimum has a smaller value in \(\|\|_{}\) norm when the performative effect is stronger, that is, when \(\) gets larger. In the particular case where \(= I\), eq. (7) rewrites as \(^{1/2}\|^{*}\|^{-1/2}\|_{1}-(1-)_{0}\|/( 1-)\) and thus larger values of \(\) decrease the r.h.s. while the l.h.s. increases for identical values of \(\|^{*}\|\), showing that \(\|^{*}\|\) has to decrease to zero.

## 6 Experiments

In this section1, we test the performance of our algorithm Reparametrization-based Performative Gradient (RPPerfGD) with respect to existing algorithms. Three baselines were introduced in Perdomo et al. (2020). First, _Repeated Risk Minimization (RRM)_ computes at each step the next \(\) to minimize the non-performative risk, leading to the update rule \(^{t+1}=_{^{}}(^{t},^{ })\). In practice, we found that this algorithm is unstable as soon as performative effects become significant. We thus report separately the results obtained with this algorithm in appendix B.2. A second baseline is _Repeated Gradient Descent (RGD)_, which ignores the performative effect but limits itself to a gradient step towards this minimization \(^{t+1}=^{t}-\;_{^{}})}_{|^{}=}\). In numerical experiments, it is often chosen to add a regularizer to the objective function, which is particularly interesting in the context of performative learning, as discussed in section 5. Hence, we report _Regularized Repeated Gradient Descent (RRGD)_, that corresponds to the repeated gradient with a loss function including an additional ridge penalty on \(\), leading to a more conservative behavior that is also more robust to performative effects. Finally, we also compare to the _Score Function Performative Gradient Descent (SFPerfGD)_ which estimates the performative part of the gradient using the \(_{}^{}\) estimator based on the score-function approach (see section 2). This was previously used in small dimensions in Miller et al. (2021).

Stability of the estimatorIn fig. 1(c), we illustrate the result of example 2, by training a classifier with the square loss, showing that the score-based estimator used in SFPerfGD becomes unstable in high dimensions. We use Gaussian distributions of dimension \(7\) with two dimensions subject to performative effects. We vary the variance of the distributions. When the scale \(\) is small, the variance of the estimator increases to the point of making learning impossible with unstable trajectories of the parameter \(\). Even when the scale is small enough to ensure convergence, RPPerfGD provides faster convergence illustrating its better scalability for high dimensions.

Estimation of \(\)We estimate \(\) in fig. 1(d) and fig. 1(e) by running a ridge regression along the successive deployments of the model as described in Algorithm 1: the ridge penalty ensures that initially the estimate of \(\) is close to zero making the RPPerfGD updates very similar to those of

Figure 2: **(a) Logistic regression to classify two Gaussian distributions centered in \((0,0)\) and \((-1,-1)\) and different magnitudes of performative effects \(\). We report the accuracy for three different magnitudes of the performative effects, from no performative effect (\(=0\)) to a strong one (\(=1\)). (b) we report the position of the parameter \(\) in its 2D-space, starting from \((0,0)\) and following different paths depending on the algorithm. (c) Accuracy of a classification with quadratic loss on two Gaussian distributions of dimension \(7\) with various levels of variance \(\) of the distributions. (d) Same experiments but using the learnt \(\) for RPPerfGD. (e) In this case, distance between the true matrix \(\) and the estimated version. Note that in RGD and RRGD the estimation of \(\) is not used in the algorithm. (f) Logistic regression for the Housing dataset with various magnitude of performative shift \(\) on the coordinates \(0\), \(4\) and \(6\). Accuracy is averaged over \(20\) runs.**

RGD and it is easy to check that order \(d\) deployments are enough to obtain a non void estimate of \(\). While this plug-in approach is not guaranteed to converge from a theoretical standpoint, we observe results that very similar to the case where \(\) is fully known.

Houses price predictionTo simulative performative effects from a dataset, we follow the methodology of Perdomo et al. (2020), by shifting the coordinate \(i\) of a factor \(_{i}\) if the \(i\)-th coordinate could be easily modified, and keeping its real value intact otherwise. We use the binarized version of the Housing dataset2, where the outcome is whether the price is high or not. Assuming that a seller wants to obtain a high price, the high price is the favored class. Some characteristics are harder to tamper with such as the location or the income, whereas other can be slightly adjusted such as the household and the number of bedrooms (a room could be promoted bedroom). Coordinates \(0\), \(4\) and \(6\) are thus shifted while other remains identical. We see that when the magnitude of the shift increases, RPPerfGD outperforms RGD. In particular, it seems that RPPerfGD succeeds in converging faster than the non performative approach.

```
Input : Stepsize \(\), regularizer \(\), starting \(_{0}\), Loss \(\) Output :Parameters \(_{K}\) and diagonal matrix \(_{K}\)
1\(_{0} 0_{d d}\)// initialize \(\) as a zero matrix of size \(d d\)
2for\(k\{0,,K-1\}\)do
3 Receive \(n\) samples \(\{x^{i}_{k}\}_{i=1}^{n} D(_{k})\) with \(n_{0}\) samples of label \(-1\) denoted \((x_{0,k})_{k}\)
4 Compute \(_{1}_{i=1}^{n}_{}(x^{i}_{k}, _{k})\)// Non performative part of the gradient
5 Compute \(_{2}_{k}^{}_{i=1}^{n_{0}}_{x} (x^{i}_{0,k},_{k})\)// Performative part of the gradient over negative samples
6\(_{k+1}_{k}-(_{1}+_{2})\)// Gradient Descent step
7\(_{k+1}*{argmin}_{j=1}^{k}_{l=1}^{n_{0}}\|x^ {l}_{0,j}--_{j}\|^{2}+\|\|^{2}\)// \(\) is the estimated mean of the class
```

**Algorithm 1**RPPerfGD with \(\) learning

## 7 Conclusion

In this work, we have investigated the consequences of assuming a novel, more explicit, model for performative effects under the form of a push-forward shift of distribution. We have demonstrated that it comes with practically important consequences, such as enabling more reliable performative gradient estimation in large dimensional models.

In the classification case, we observed that when the change of distribution is given by a linear-in-parameters shift, the performative risk is convex under relatively general assumptions. It would be interesting to study how these results may extend to non-linear models for the performative effect.

Finally, we have shown that certain kinds of performative effect induce implicit regularization of the risk minimization problem. Moreover, this regularization effect can alternatively be viewed through the lens of adversarial robustness. It would be useful to explore whether this reformulation can be used to optimize the performative risk without an explicit model for the performative effect.

## 8 Acknowledgments

This work was supported by grants ANR-20-THIA-0014 program "AI PhD@Lille" and ANR-22-PESN-0014 under the France 2030 program. The authors thank Francis Bach, Bruno Loureiro and Kamelia Daudel for their helpful insights.