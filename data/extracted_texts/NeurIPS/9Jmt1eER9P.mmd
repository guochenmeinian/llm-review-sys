# Optimization Algorithm Design via Electric Circuits

Stephen P. Boyd

Stanford University

&Tetiana Parshakova

Stanford University

&Ernest K. Ryu

UCLA

&Jaewook J. Suh

Rice University

Lead authors (author list ordered alphabetically)

###### Abstract

We present a novel methodology for convex optimization algorithm design using ideas from electric RLC circuits. Given an optimization problem, the first stage of the methodology is to design an appropriate electric circuit whose continuous-time dynamics converge to the solution of the optimization problem at hand. Then, the second stage is an automated, computer-assisted discretization of the continuous-time dynamics, yielding a provably convergent discrete-time algorithm. Our methodology recovers many classical (distributed) optimization algorithms and enables users to quickly design and explore a wide range of new algorithms with convergence guarantees.

## 1 Introduction

In the classical literature of optimization theory, optimization algorithms are designed with the goal of establishing fast worst-case convergence guarantees. However, these methods, designed with the pessimistic framework of worst-case analysis, often exhibit slow practical performance. In the modern machine learning literature, optimizers are designed with the goal of obtaining fast empirical performance on a set of practical problems of interest. However, these methods, designed without consideration of the feasibility of a convergence analysis, tend to be much more difficult to analyze theoretically, and such methods sometimes even fail to converge under nice idealized assumptions such as convexity .

In this work, we present a novel methodology for convex optimization algorithm design using ideas from electric RLC (resistor-inductor-capacitor) circuits and the performance estimation problem . (To clarify, our proposal does not involve building a physical circuit.) Specifically, our methodology provides a quick and systematic recipe for designing new, provably convergent optimization algorithms, including distributed optimization algorithms. The ease of the methodology enables users to quickly explore a wide range of algorithms with convergence guarantees.

Optimization problem formulation.We consider the standard-form optimization problem

\[&f(x)\\ &x(E^{}),\] (1)

where \(x^{m}\) is the optimization variable, \(f^{m}\{\}\) is closed, convex, and proper, and \(E^{n m}\). Assume we have \(n\) nets \(N_{1},,N_{n}\) forming a partition of \(\{1,,m\}\). More specifically, we let \(E^{n m}\) be a _selection matrix_ defined as

\[E_{ij}=\{+1&j N_{i}\\ 0&.\] (2)

Our goal is to find a primal-dual solution satisfying the KKT conditions [126, Theorem 28.3]

\[y f(x), x(E^{}), y (E).\] (3)

As we show through examples, this standard-form problem (1) conveniently models many optimization problem setups of practical interest.

In the analysis and design of optimization algorithms, a standard approach is to consider a continuous-time model of a given algorithm, corresponding to the limit of small stepsizes [124; 79; 6; 156; 143; 93; 86; 137]. Our work is based on the key observation that such continuous-time models can be interpreted as RLC circuits connected to the subdifferential operator \( f\), which we interpret as a nonlinear resistor. We expand on this observation and propose a general methodology for designing optimization algorithms by designing RLC circuits that relax to the nets defined by \(E\).

Example.Problem (1) represents a general form of distributed optimization, where the constraints enforce consensus among the primal variables. An example is the so-called consensus problem [30; SS7]

\[x_{1},,x_{N}^{m /N}}{} f_{1}(x_{1})++f_{N}(x_{N})\] subject to \[x_{1}==x_{N},\]

where \(x=(x_{1},,x_{N})\) is the decision variable, the objective function \(f(x)=f_{1}(x_{1})++f_{N}(x_{N})\) is block-separable, and \(E^{}=(I,,I)^{m m/N}\). Refer to sections SSAE and SF, for an overview of classical splitting methods and decentralized methods for solving (1).

### Preliminaries and Notation

We generally follow the standard definitions and notations of convex optimization [31; 118; 23; 119; 129]. Consider the extended-valued function \(f^{n}\{\}\). We say \(f\) is closed if its epigraph is closed set in \(^{n+1}\) and proper if its value is finite somewhere. We say \(f\) is CCP if it is closed, convex, and proper. For \(R>0\), we say \(f\) is \(R\)-smooth if \(f\) is finite and differentiable everywhere and \(\| f(x)- f(y)\| R\|x-y\|\) for all \(x,y^{n}\). For \(>0\), we say \(f\) is \(\)-strongly convex if \(f(x)-(/2)\|x\|^{2}\) is convex. Let \(f^{*}(y)=_{x^{n}}\{ y,x-f(x)\}\) denote the Fenchel conjugate of \(f_{1}\). For \(R>0\) and a CCP \(f\), define the \(R\)-Moreau envelope of \(f\) as \({}^{R}\!f(x)=_{z^{n}}\{f(z)+\|z-x\|^{2}\}\). One can show [23; Proposition 13.24] that the \(R\)-Moreau envelope is given by \({}^{R}\!f=(f^{*}+\|\|^{2})^{*}\). If \(f\) is \(1/R\)-smooth, we can define [23; Theorem 18.15] the \(R\)-pre-Moreau envelope of \(f\) as

\[=(f^{*}-\|\|^{2})^{*},\]

which is defined such that \({}^{R}()=f\).

Due to the limited space, we defer the review of prior works to SSA of the appendix.

### Contributions

Our work presents two technical novelties, one in continuous time and the other in discrete time. The first is the observation that many standard optimization algorithms can be interpreted as discretizations of electric RLC circuits connected to the subdifferential operator \( f\). The second is the use of the performance estimation problem to obtain an automated recipe for discretizing convergent continuous-time dynamics into convergent discrete-time algorithms, and we provide code implementing our automatic discretization methodology.

By combining these two insights, we provide a quick and systematic methodology for designing new, provably convergent optimization algorithms, including distributed optimization algorithms. We provide an open-source package that implements automatic discretization of our circuits:

https://github.com/cvxgrp/optimization_via_circuits

## 2 Continuous-time optimization with circuits

### Interconnects

We now describe two types of electric circuits that we call _static_ and _dynamic interconnects_. Both interconnects have \(m\) terminals, and we will later connect them to the \(m\) inputs of \( f\).

Static interconnect.The static interconnect is a set of (ideal) wires connecting \(m\) terminals and forming \(n\) nets. See Figure 1 for an example. Let \(x^{m}\) be a vector of terminal potentials and \(y^{m}\) be a vector of currents leaving the terminals. Using matrix \(E^{n m}\) as defined in (2), we can express Kirchhoff's voltage law (KVL) as \(x(E^{})\) and Kirchhoff's current law (KCL) as \(y(E)\). In other words, the static interconnect enforces the V-I relationship

\[(x,y)(E^{})(E).\] (4)

Dynamic interconnect.The dynamic interconnect is an RLC circuit with \(m\) terminals and \(1\) ground node. We assume all inductances and capacitances have values in \((0,)\) while the resistances have values in \([0,)\). (A 0-ohm resistor is an ideal wire. We do not permit ideal wire loop.) Each RLC component has two (scalar-valued) terminals: the \(+\) and \(-\) terminals.

Denote the number of nodes in the RLC circuit by \(\). Connect nodes \(1,2,,m\) to terminals \(1,2,,m\), and let the last node, node \(\), be the ground node. (This implies \( m+1\).) Denote the number of RLC components by \(\). We describe the topology with a reduced node incidence matrix (with the bottom row corresponding to the ground node removed) \(A^{(-1)}\) defined as

\[A_{ij} = \{+1&\\ -1&\\ 0&.\]

See Figure 2 for an example.

The ground node is designated to have \(0\) potential, and the _potential_ of any node is the potential relative to ground. The _voltage_ across a component is the difference of potentials between the \(+\) and \(-\) terminals. The _current_ through a component is defined as the current flowing from the \(+\) terminal to the \(-\) terminal.

Let \(x^{m}\) be the potentials at the \(m\) terminals, which are connected to nodes \(1,,m\), and \(y^{m}\) be the currents leaving the terminals. Denote the node potential vector with the ground node excluded (since the potential at ground is \(0\)) by

\[x\\ e^{r-1}.\]

So, \(e^{-1-m}\) denotes the potentials at the non-terminal nodes. Denote the vector of voltages by \(v^{}\) and the vector of currents by \(i^{}\). Then, the currents and voltages of the dynamic

Figure 2: Example of a dynamic interconnect with \(=8\) nodes, \(=7\) RLC components, \(m=5\) terminals, and \(1\) ground node. Reduced node incidence matrix \(A\) is provided. (\(R_{2}\) and \(R_{3}\) are \(0\)-ohm resistors.) This dynamic interconnect is admissible with respect to the static interconnect of Figure 1.

interconnect satisfy the following V-I relations

\[Ai=-y\\ 0v=A^{}x\\ e\] (iii) \[_{}=D_{}i_{} v_{}=D_{}i_{} i_{}=D_{}v_{}\]

where \(D_{}\), \(D_{}\), and \(D_{}\) are diagonal matrices respectively with resistances, inductances, and capacitances values in the diagonals.

Admissibility.When an RLC circuit reaches equilibrium, voltages across inductors and currents through capacitors are \(0\). We say a dynamic interconnect is _admissible_ if it relaxes to the static interconnect at equilibrium. Mathematically, this condition is expressed as

\[(x,y)\,\,Ai=-y\\ 0,v=A^{}x\\ e,v_{}=D_{}i_{},v_{}= 0,i_{}=0}=(E^{})(E).\]

As an example, the dynamic interconnect of Figure 2 is admissible with respect to the static interconnect of Figure 1.

### Composing interconnects with \( f\)

We view the subdifferential operator \( f\) as an \(m\)-terminal electric device that is also grounded. Let \(x^{m}\) be the potentials at the \(m\) terminals (excluding ground) and \(y^{m}\) be the currents flowing into the \(m\) terminals. The \( f\) operator enforces the V-I relation

\[y f(x).\]

We connect the \(m\) terminals of \( f\) to the \(m\) terminals of the static and dynamic interconnects. Immediately, connecting the static interconnect with \( f\) enforces the V-I relations (4) and \(y f(x)\), which combine to be the optimality condition (3). Therefore, the potentials at the \(m\) terminals as a vector in \(^{m}\) is an optimal \(x^{}^{m}\) solving (1). To clarify, connecting the static interconnect with \( f\) leads to a _static_ circuit in the sense that the potential \(x\) and current \(y\) do not depend on time.

Next, we compose (connect) the dynamic interconnect with \( f\). Due to capacitors and inductors, this circuit is _dynamic_ in the sense that the voltages \(v(t)\) and \(x(t)\) and currents \(i(t)\) and \(y(t)\) depend on time, although we often omit explicitly writing the \(t\)-dependence for notational convenience. Then, the V-I relations of the dynamic interconnect combined with \(y f(x)\) leads to the V-I relation

\[(v,i)\,\,y f(x),\ Ai=-y\\ 0,\ v=A^{}x\\ e,\] (5) \[v_{}=D_{}i_{},\ v_{} =D_{}i_{},\ i_{}=D_{} v_{},\ t(0,)},\]

where \(v(t)=(v_{}(t),v_{}(t),v_{}(t))^ {}\), \(i(t)=(i_{}(t),i_{}(t),i_{}(t))^ {}\), \(e(t)^{-m-1}\), \(x(t)^{m}\), and \(y(t)^{m}\) for \(t[0,)\).

Figure 3: The static interconnect of Figure 1 connected with \( f\). The potentials at the \(m\) terminals is an optimal \(x^{}^{m}\) solving (1).

Under appropriate conditions, the dynamics (5) is mathematically well-posed in the sense that there exist unique Lipschitz-continuous curves \(v(t)\), \(i(t)\), \(x(t)\), and \(y(t)\) satisfying the V-I relation (5) as formalized in the following Theorem 2.1. The proof, which utilizes the machinery of monotone operator theory , is provided in SSB of the appendix.

**Theorem 2.1**.: _Assume \(f\) is \(\)-strongly convex and \(M\)-smooth. Suppose \((v^{0},i^{0},x^{0},y^{0})\) satisfy_

\[A^{^{0}}=-y^{0}\\ 0, v^{0}=A^{}x^{0}\\ e, v^{0}_{}=D_{}i^{0}_{},  y^{0}= f(x^{0}).\]

_Then there is a unique Lipschitz continuous curve \((v,i,x,y)[0,)^{}^{} ^{m}^{m}\) satisfying the conditions in (5) and the initial condition \((v(0),i(0),x(0),y(0))=(v^{0},i^{0},x^{0},y^{0})\)._

Equillibrium yields a primal-dual solution.With the dynamic interconnect composed with \( f\), we generically expect the circuit state \((v(t),i(t),x(t),y(t))\) to converge (relax) to an equilibrium state. The admissibility condition ensures that at such an equilibrium, \((x,y)\) will be a primal-dual solution. We formally state this fact as Theorem C.2 of the Appendix.

### Energy dissipation

Let \((v^{},i^{},x^{},y^{})\) be an equilibrium of an admissible dynamic interconnect composed with \( f\). Since the voltages across resistors and inductors and the currents through capacitors are zero under equilibrium, we have

\[v^{}=(v^{}_{},v^{}_{},v^{}_{ })=(0,0,v^{}_{}), i^{}=(i^{}_{ },i^{}_{},i^{}_{})=(0,i^{}_{ },0).\]

(We formally show this in Theorem C.2 of the appendix.) Define the energy of the circuit at time \(t\) as

\[(t)=\|v_{}(t)-v^{}_{}\|^{2}_ {D_{}}+\|i_{}(t)-i^{}_{}\|^{ 2}_{D_{}},\] (6)

which is a dissipative (non-increasing) quantity:

\[ = v_{}-v^{}_{},i_{}- i^{}_{}+ i_{}-i^{}_{},v_{ }-v^{}_{}\] \[=-\|i_{}\|^{2}_{D_{}}- x-x^{}, y-y^{} 0.\] (7)

Here, we use \(i^{}_{}=0\) and \(v^{}_{}=0\) and the fact that the power dissipated by the resistors and \( f\) must come from the energy stored in the capacitors and inductors. This dissipativity property leads to the following continuous-time convergence.

**Theorem 2.2**.: _Assume \(f^{m}\) is strongly convex and smooth. Assume the dynamic interconnect is admissible, and let \((x^{},y^{})\) be a primal-dual solution pair. Let \((v(t),i(t),x(t),y(t))\) be a curve satisfying (5). Then,_

\[_{t}(x(t),y(t))=(x^{},y^{}).\]

Theorem 2.2 largely follows as a corollary of Theorem 2.1. The formal proof is provided in SSD of the appendix. In SS4, we present a systematic framework for finding discretized versions of Theorem 2.2 the corresponding discretized algorithms.

Figure 4: The dynamic interconnect of Figure 2 connected with \( f\). The potentials at the \(m\) terminals satisfy \(x(t) x^{}\) for an optimal \(x^{}^{m}\) solving (1) under the conditions of Theorem 2.2.

Circuits for classical algorithms

In this section, we present circuits recovering the classical Nesterov acceleration, decentralized ADMM, and PG-EXTRA. For additional examples and detailed derivations, refer to SSE and SSF of the appendix, where we provide circuits and analyses of classical algorithms such as gradient descent , proximal point method , primal gradient method , primal decomposition , dual decomposition , Douglas-Rachford splitting , Davis-Yin splitting , decentralized gradient descent , and diffusion .

Multi-wire notation.We start by quickly introducing the multi-wire notation depicted in Figure 5. When optimizing \(f^{m}\) and using the \(m\)-terminal device \( f\), we will often use dynamic interconnects that have the same RLC circuit across each net, _i.e._, the dynamic interconnect consists of \(m\) identical copies of the same RLC circuit for the \(m\) coordinates of \(x^{m}\). In this case, we use the diagonal-line notation depicted in Figure 5.

Moreau envelope.We use the following simple identity throughout this work: \( f\) composed with a resistor is equivalent to \(^{R}\!f(x)\).

To clarify, the equivalence means the two circuits impose the same V-I relation on the \(m\) pins of \(x\). To see this, note \([ f()=(x-)] [=_{Rf}(x)]\) and use the identity for the gradient of the Moreau envelope to conclude

\[^{R}\!f(x)=(x-_{Rf}(x))=(x-).\]

See SSE.1 of the appendix for further details.

### Nesterov acceleration

Let \(f^{m}\) be a \(1/R\)-smooth convex function. Then, the circuit corresponding to the classical Nesterov acceleration is given below.

The use of a _negative_ resistor \(-R\) may seem unconventional, but the fact that this circuit is stable is easier to see if we consider the equivalent circuit with the pre-Moreau envelope \(\), _i.e._, \(\) is the convex function such that \({}^{R}\!=f\). To clarify, negative resistors satisfy the same V-I relations of the standard resistors but with a negative slope. Negative resistors have also been considered in .

The V-I relations of this circuit lead to the ODE

\[}{dt^{2}}x+x+(-}{ L}) f(x)+ f(x)=0.\]

If we set \(R=\), which can be interpreted as an instance of critical damping , \(L=}\), and \(C=2\), we recover the Nesterov ODE 

\[}{dt^{2}}x+2x+ f(x)=0.\]

We also quickly point out that other choices of parameters lead to the high-resolution ODE introduced in . See SSE.3 of the appendix for further details.

Figure 5: Multi-wire notation.

### Decentralized ADMM

Let \(f_{1},,f_{N}^{m}\{\}\) be CCP functions. Consider a decentralized optimization setup with graph \(G\). We provide the full description of the decentralized setup and notations in \(@sectionsign\)F of the appendix. Define \(_{j}\) to be the neighbors of \(j\) in graph \(G\). For simplicity, we only illustrate the circuit related to nodes \(j\) and \(l\), where \(j\) and \(l\) are directly connected through an edge in the graph \(G\).

The circuit corresponding to decentralized ADMM  is given below.

In the following, the left column presents the dynamics of the continuous-time circuit and the right column presents the discretization with stepsize \(L/R\), recovering the standard decentralized ADMM:

\[a_{j} =|}_{l_{j}}(Ri_{j}+e _{jl})|a_{j}^{k+1}=|}_{l _{j}}(Ri_{j,l}^{k}+e_{jl}^{k})\\ x_{j}=_{(R/|_{j}|)f_{j}}(a_{j})\\ e_{jl}=(x_{j}+x_{l})\\ i_{jl}=(e_{jl}-x_{j}).\] \[a_{j}^{k+1} =(e_{jl}^{k+1}+(e_{jl}^{k+1}-x_{j}^{k+1})\]

for every node \(j=1,,N\) and every edge \((j,l)\) in graph \(G\).

### Pg-Extra

Let \(f_{1},,f_{N}^{m}\{\}\) be CCP functions and \(h_{1},,h_{N}^{m}\) be convex \(M\)-smooth functions. Consider a decentralized optimization setup with graph \(G\). The circuit corresponding to PG-EXTRA  is given below.

Define the mixing matrix \(W^{N N}\) with

\[W_{jl}=\{1-_{l_{j}}}& j=l\\ }&j l, l_{j}\\ 0&.\]In the following, the left column presents the V-I relations for the continuous-time circuit and the right column presents the discretization with stepsize \(\), recovering the standard PG-EXTRA:

\[x_{j} =_{Rf_{j}}(_{l=1}^{N}W_{jl}x_{l}-R h_{j }(x_{j})-w_{j})|x_{j}^{k+1}=_{Rf_{j}} (_{l=1}^{N}W_{jl}x_{l}^{k}-R h_{j}(x_{j}^{k})-w_{j}^{k}) \\ \\ w_{j}^{k+1}=w_{j}^{k}+(x_{j}^{k}-_{l=1}^{N}W_{jl}x_{l}^{k}) .\]

for every node \(j=1,,N\) and every edge \((j,l)\) in graph \(G\).

## 4 Automatic discretization

We discretize the continuous-time dynamics given by the circuit with an admissible dynamic interconnect using a two-stage Runge-Kutta method with parameters \(,\) and stepsize \(h>0\). The explicit form of the discretization is stated in SS6 of the appendix. Let \(\{(v^{k},i^{k},x^{k},y^{k})\}_{k=1}^{}\) be the iterates generated by the discretized algorithm. Then the energy stored in the circuit at time \(t=kh\) is

\[_{k}=\|v_{}^{k}-v_{}^{}\|_{ _{}}^{2}+\|i_{}^{k}-i_{}^{}\|_{_{}}^{2}.\]

To guarantee convergence of the discretized algorithm, we search for discretization parameters that ensure the \(_{1},_{2},\) sequence is dissipative in the following sense. Specifically, we say the algorithm or the discretization is _sufficiently dissipative_ if there is an \(>0\) such that

\[_{k+1}-_{k}+ x^{k}-x^{},y^{k}-y^{}  0,\] (8)

holds for all \(k=1,2,\). This requirement is analogous to the "sufficient decrease" conditions in optimization . The following Lemma 4.1, which proof we provide in SS6 of the appendix, states that sufficient dissipativity ensures convergence under suitable conditions.

**Lemma 4.1**.: _Assume \(f^{m}\{\}\) is a strictly convex function and the dynamic interconnect is admissible. If the two-stage Runge-Kutta discretization, as explicitly stated in SS6 of the appendix, generates a discrete-time sequence \(\{(v^{k},i^{k},x^{k},y^{k})\}_{k=1}^{}\) satisfying the sufficient dissipativity condition (8), then \(x^{k}\) converges to a primal solution._

We find such a discretization with the following automated methodology. Given a discretization characterized by \((,,h)\), the dissipativity condition (8) for a given \(>0\) is implied if the optimal value of the following optimization problem is non-positive:

\[&_{2}-_{1}+  x^{1}-x^{},y^{1}-y^{}\\ &_{s}=\|v_{}^{}-v_{ }^{}\|_{_{}}^{2}+\|i_{ }^{}-i_{}^{}\|_{_{}}^{2}, s\{1,2\}\\ &(v^{1},i^{1},x^{1},y^{1})\\ &(v^{2},i^{2},x^{2},y^{2})\\ &f,\] (9)

where \(f,v^{1},i^{1},x^{1},y^{1},v^{},i^{},x^{},y^{}\) are the decision variables and \(\) is a family of functions (_e.g._, \(L\)-smooth convex) that the algorithm is to be applied to. Here, we are using the fact that (8) is homogeneous with respect to \(k\) (_i.e._, (8) essentially has no \(k\)-dependence), and therefore it is sufficient to verify the condition for \(k=1\) but for all feasible initial points \((v^{1},i^{1},x^{1},y^{1})\). It turns out that (9) can be solved exactly as a semidefinite program (SDP) for many commonly considered function classes \(\). This technique was initially proposed as the performance estimation problem (PEP) , a computer-aided methodology for constructing convergence proofs of first-order optimization methods. See, _e.g._, PEPit  package that implements PEP in Python.

Further, (9) can be posed as a nonconvex quadratically constrained quadratic problem (QCQP) with only a few tens of variables and such problems can be solved exactly with spatial branch-and-bound algorithms .

In conclusion, we can solve a non-convex QCQP to find a provably convergent discretization of the continuous-time circuit with an admissible dynamic interconnect. We use the Ipopt  solver. Further details are provided in SS6 of the appendix.

Example.Consider the following example circuit for the minimization of a convex function \(f\). Let \(R_{1}=R_{2}=R_{3}=1\), and \(C_{1}=C_{2}=10\).

With our automatic discretization methodology, we find the sufficiently dissipative parameters

\[=6.66, h=6.66,=0,=1.\]

The resulting provably convergent algorithm is

\[x^{k} = _{(1/2)f}(z^{k}), y^{k}=2(z^{k}-x^{k})\] \[w^{k+1} = w^{k}-0.33(y^{k}+3w^{k})\] \[z^{k+1} = z^{k}-0.16(5y^{k}+3w^{k}).\]

is provably convergent2 under the condition that \(f\) is strictly convex, see SSH for details.

## 5 Experiments

In this section, we use our methodology to obtain a new algorithm and experiment with it on a specific problem instance. Consider a decentralized optimization problem with a communication graph \(G\) with \(N=6\) nodes and \(7\) edges, as shown in Figure 8. Specifically, we consider the optimization problem

\[^{100}}{}_{i\{4,5\}} (\|x-b_{i}\|_{2}+\|x-b_{i}\|_{2}^{2})+_{i\{4,5\}}\|x-b_{ i}\|_{2},\]

where each agent \(i\{1,,6\}\) holds the vector \(b_{i}^{100}\). To leverage the strong convexity of \(f_{4}\) and \(f_{5}\), we propose a modification to the DADMM circuit described in SSF.3. Given that a circuit with a capacitor and inductor corresponds to a momentum method (see SS3.1), and momentum is known to accelerate convergence for strongly convex functions , we add a capacitor to \(e_{45}\) to DADMM circuit as shown in the left column of Figure 9. We then discretize the circuit and refer the the resulting algorithm DADMM+C. We apply DADMM+C to the decentralized optimization problem and observe a speedup as shown in the right columns of Figure 9. The relative error for DADMM+C decreases to \(10^{-10}\) in \(66\) iterations, for DADMM in \(87\) iterations and for P-EXTRA in \(294\) iterations. For further details, see SS1.1 of the appendix.

Figure 8: Underlying graph \(G\).

Further, we define a general version of the DADMM+C method for any connected graph and establish a general convergence proof in Lemma I.1 of in SSI.1.1 of the appendix. This convergence analysis demonstrates how to use our methodology to discover a new family of methods with a classical convergence proof. Finally, we provide another set of similar experiments in SSI.2 of the appendix.

## 6 Conclusion

In this work, we present a novel approach to optimization algorithm design using ideas from electric RLC circuits. The continuous-time RLC circuit models combined with the automatic discretization method provide a foundation for designing algorithms that inherently possess convergence guarantees. Further, we provide code implementing the automatic discretization. Our framework opens the door to future research by applying this methodology to a broader range of optimization problems and extending the problem to other setups, such as the stochastic optimization setup.