# A Fast Convoluted Story:

Scaling Probabilistic Inference for Integer Arithmetic

 Lennert De Smet

KU Leuven

Belgium

&Pedro Zuidberg Dos Martires

Orebro University

Sweden

###### Abstract

As illustrated by the success of integer linear programming, linear integer arithmetic is a powerful tool for modelling combinatorial problems. Furthermore, the probabilistic extension of linear programming has been used to formulate problems in neurosymbolic AI. However, two key problems persist that prevent the adoption of neurosymbolic techniques beyond toy problems. First, probabilistic inference is inherently hard, #P-hard to be precise. Second, the discrete nature of integers renders the construction of meaningful gradients challenging, which is problematic for learning. In order to mitigate these issues, we formulate linear arithmetic over integer-valued random variables as tensor manipulations that can be implemented in a straightforward fashion using modern deep learning libraries. At the core of our formulation lies the observation that the addition of two integer-valued random variables can be performed by adapting the fast Fourier transform to probabilities in the log-domain. By relying on tensor operations we obtain a differentiable data structure, which unlocks, virtually for free, gradient-based learning. In our experimental validation we show that tensorising probabilistic linear integer arithmetic and leveraging the fast Fourier transform allows us to push the state of the art by several orders of magnitude in terms of inference and learning times.

## 1 Introduction

Integer linear programming (ILP)  uses linear arithmetic over integer variables to model intricate combinatorial problems and has successfully been applied to domains such as scheduling , telecommunications  and energy grid optimisation . If one replaces deterministic integers with integer-valued random variables, the resulting probabilistic arithmetic expressions can be used to model probabilistic combinatorial problems. In particular, many problems studied in the field of neurosymbolic AI can be described using probabilistic linear integer arithmetic.

Unfortunately, exact probabilistic inference for integer arithmetic is a #P-hard problem in general. Consequently, even state-of-the-art probabilistic programming languages with dedicated inference algorithms for discrete random variables, such as ProbLog  and Dice , fail to scale. The reason being that they resort to exact enumeration algorithms, as exemplified in Figure 1. Note that while approximate inference algorithms such as Monte Carlo methods and variational inference can be applied to probabilistic combinatorial problems, they come with their own set of limitations, as discussed by Cao et al. . For instance, conditional inference with low-probability evidence.

In order to mitigate the computational hardness of probabilistic inference over integer-valued random variables, we make the simple yet powerful observation that the probability mass function (PMF) of the sum of two random variables is equal to the convolution of the PMFs of the summands. The key advantage of this perspective is that the exact convolution for finite domains can be implemented efficiently using the fast Fourier transform (FFT) in \((N N)\), which avoids the traditionally quadratic behaviour of computing the PMF of a sum of two random variables (Figure 1). Moreover,efficient implementations of the FFT are readily available in modern deep learning libraries such as TensorFlow  and PyTorch , making our approach to probabilistic inference end-to-end differentiable by construction. In turn, differentiability allows us to apply our approach to prototypical problems in neurosymbolic AI.

Our main contributions are the following. **1)** We propose a tensor representation of the distributions of bounded integer-valued random variables that allows for the computation of the distribution of a sum of two such variables in \((N N)\) instead of \((N^{2})\) by exploiting the fast Fourier transform (Section 2). **2)** We formulate common operations in linear integer arithmetic, such as multiplication by constants and the modulo operation, as tensor manipulations (Section 3). These tensorised operations give rise to \(_{}\), a scalable and differentiable framework for Probabilistic Linear Integer Arithmetic.1 PLAIA supports two exact probabilistic inference primitives; taking expected values and performing probabilistic branching (Section 4). **3)** We provide experimental evidence that \(_{}\) outperforms the state of the art in exact probabilistic inference for integer arithmetic  in terms of inference time by multiple orders of magnitude (Section 5.1). Moreover, we deploy \(_{}\) in the context of challenging neurosymbolic combinatorial problems, where it is again orders of magnitude more efficient when compared to state-of-the-art _exact and approximate_ methods (Section 5.2).

## 2 Efficient Addition of Integer-Valued Random Variables

In what follows, we denote random variables by uppercase letters, while a specific realisation of a random variable is written in lowercase. That is, the value \(x\) is an element of the sample space \((X)\) of \(X\). We will also refer to \((X)\) as the domain of the random variable \(\). Furthermore, \((X)\) is assumed to be integer-valued, i.e. it is a finite subset of the integers \(\) with lower and upper bounds \(L(X)\) and \(U(X)\), respectively. In particular, we have that the cardinality \(|(X)|=U(X)-L(X)+1\). We will call these integer-valued random variables _probabilistic integers_ from here on.

The distribution of a probabilistic integer \(X\) is represented using its probability mass function (PMF) \(p_{X}:(X)\) with the conventional restrictions

\[ x(X):p_{X}(x) 0_{x(X)}p_ {X}(x)=1.\] (1)

### Probabilistic Integers and the Convolution Theorem

At the core of \(_{}\) and linear arithmetic in general is the addition of two probabilistic integers \(X_{1}\) and \(X_{2}\). Let us assume for now that \(X_{1}\) and \(X_{2}\) satisfy \(L(X_{1})=L(X_{2})=0\) and have upper bounds \(U(X_{1})=N_{1}\) and \(U(X_{2})=N_{2}\). Just as in Figure 1, we would now like to find the PMF of the random variable \(X\) such that \(X=X_{1}+X_{2}\). However, contrary to Figure 1, we wish to avoid the

Figure 1: On the left and in the middle we have two histograms representing the probability distributions of the random variables \(X_{1}\) and \(X_{2}\), respectively. The grid on the right represents the joint probability of the two distributions, with more intense colors indicating events with higher probability. The distribution of the random variable \(X=X_{1}+X_{2}\) can be obtained by summing up the diagonals of the grid as indicated in the figure. While this method of obtaining the distribution for \(X\) is valid and used by state-of-the-art neurosymbolic techniques [14; 21], the explicit construction of the joint is unnecessary and hampers inference and learning times (cf. Section 5).

explicit quadratic construction of all possible outcomes. To this end, we exploit that the PMF of the sum of two random variables is equal to the convolution of their respective PMFs 

\[p_{X}(x)=(p_{X_{1}}*p_{X_{2}})(x), X=X_{1}+X_{2}.\] (2)

Next, we apply the Fourier transform \(\) to both sides of the equation and use the convolution theorem (CT)  that states that the Fourier transform of two convoluted functions is equal to the product of their transforms

\[(p_{X})(x)=(p_{X_{1}}*p_{X_{2}})(x)}}{{=}}(p_{X_{1}})(x)(p_{X_{2}})(x)= _{X_{1}}(x)_{X_{2}}(x),\] (3)

where we also introduce the hat notation \((p_{X})=_{X}\) for a Fourier transformed PMF \(p_{X}\). As \(X=X_{1}+X_{2}\), we know that \((X)=\{0,,N_{1}+N_{2}\}\). Consequently, the PMF \(p_{X}\) is non-zero for just these \(M=N_{1}+N_{2}+1\) domain elements and can be represented using a vector of probabilities

\[_{X}[x]=p_{X}(x), x\{0,,N_{1}+N_{2}\}.\] (4)

All vectors of probabilities will be written using a boldface \(\) and their elements accessed using square brackets. Looking at Equation 3, we would now like to express the Fourier transformed probability vector \(_{X}\) as the point-wise product of the transformed vectors

\[F_{M}_{X}=}_{X_{1}}}_{X_{2}},\] (5)

where \(F_{M}^{M M}\) is the \(M\)-point discrete Fourier transform (DFT) matrix and the symbol \(\) denotes the Hadamard product. In order for Equation 5 to hold we need to have that

\[}_{X_{1}}=F_{M}_{X_{1}}}_{X_{2}}=F_{M}_{X_{2}}.\] (6)

At first sight these equalities seem to cause a problem; each probabilistic integer \(X_{i}\) has a domain of size \(N_{i}+1\) while its PMF should be represented with a probability vector \(_{X_{1}}^{M}\) for the multiplication with \(F_{M}\) to make sense. Fortunately, this problem is easily resolved by observing that we can extend the domain \((X_{i})\) of \(X_{i}\) by simply assigning a probability of zero to newly added elements. In practice, we simply pad the probability vectors \(_{X_{1}}\) and \(_{X_{2}}\) with \(N_{2}\) and \(N_{1}\) zeros at the end to obtain vectors of dimension \(M\). With this issue resolved, we can finally obtain the probability vector \(_{X}\) that represents the PMF \(p_{X}\) by using Equation 5 via

\[F_{M}_{X}=}_{X_{1}} }_{X_{2}}  F_{M}_{X}=F_{M}_{X_{1}}  F_{M}_{X_{2}}\] (7) \[_{X}=F_{M}^{-1}F_{M} _{X_{1}} F_{M}_{X_{2}}.\] (8)

### The Fast Log-Conv-Exp Trick

The attentive reader might have noticed that, even though we avoid the explicit construction of the joint probability distribution \(p_{X_{1}X_{2}}(x_{1},x_{2})\), we have not gained much. The matrix-vector products in Equation 8 still take \((M^{2})\) to compute. Fortunately, matrix-vector products where the matrix is the DFT matrix or its inverse can be computed in time \((M M)\) by using the fast Fourier transform (FFT), with \(M\) being the size of the vector. As a result, we can express Equation 8 as

\[_{X}=(_{X_{1}} )(_{X_{2}}).\] (9)

Computing the values of the vector \(_{X}\) can now be done in time \((M M)\). First we apply the FFT on the vectors \(_{X_{1}}\) and \(_{X_{2}}\). Then we multiply the transformed vectors pointwise and apply the inverse FFT on the result of this Hadamard product. We note that Equation 9 is a well known result from the signal processing literature, where convolutions are always computed in this fashion .

However, applying Equation 9 naively to the problem of probabilistic inference quickly results in numerical stability issues. The problem is that multiplying together small probabilities eventually results in numerical underflow. A well-known and widely used remedy to this problem is the log-sum-exp trick, which allows one to avoid underflow by performing computations in the log-domain instead of the linear domain. Inspired by the log-einsum-exp trick , we introduce the fast log-conv-exp trick, which allows us to perform the FFT on probabilities in the log-domain.

We first characterise a probability distribution \(p_{X}\) not by the vector of probabilities \(_{X}\) but by the vector of log-probabilities \(_{X}=_{X}\). In terms of log-probabilities, Equation 9 can be written as

\[_{X}=(_{X_{1}})(_{X_{2}}).\] (10)Now define \(_{i}_{x(X_{i})}_{X_{i}}[x]\) as the maximum value present in the vector \(_{X_{i}}\), which lets us write Equation 10 as

\[_{X} =((_{X_{1}}-_{X_ {1}}+_{X_{1}}))((_{X_{2}}-_{X_{2}}+_{ X_{2}}))\] (11) \[=((_{X_{1}}-_{ X_{1}}))((_{X_{2}}-_{X_{2}}))(_{ X_{1}})(_{X_{2}}).\]

Crucially, we were able to pull out the scalars \((_{X_{1}})\) and \((_{X_{2}})\) due to the linearity of the FFT transform and its inverse. Taking the logarithm of both sides results in the fast log-conv-exp trick

\[_{X}=( (_{X_{1}}-_{X_{1}}))((_{X _{2}}-_{X_{2}}))+_{X_{1}}+_{X_{2}},\] (12)

which expresses the log-probabilities \(_{X}\) in function of \(_{X_{1}}\) and \(_{X_{2}}\). It can still be computed in time \((M M)\) and avoids, at the same time, numerical stability issues by exponentiating \(_{X_{i}}-_{i}\) instead of \(_{X_{i}}\) directly.

While using the fast log-conv-exp trick is necessary to scale computations in a numerically stable manner, describing operations on probability mass functions in the log-domain is rather cumbersome. Hence, we will, for the sake of clarity, describe \(}\) using probability vectors \(\) (cf.. Sections 2.3, 3 and 4). We refer the reader to our implementation for the log-domain versions.

Another solution to the numerical instability of applying the FFT on probabilities was given in an application of the FFT to open-population \(N\)-mixture models . However, it has a major drawback when compared to the fast log-conv-exp trick: it relies on repeated applications of the traditional log-sum-exp trick within each of the \(N N\) iterations of the FFT. This drawback prevents the use of optimised, off-the-shelf FFT algorithms and adds computational overhead. In contrast, we utilise the linearity of the FFT transform to provide an implementation-agnostic solution that works with tensorised representations.

### Translational Invariance

In the previous sections we assumed that the first non-zero probability event of all probabilistic integers \(X\) was the event \(X=0\), i.e. \(L(X)=0\). However, we can remove this assumption by characterizing a PMF \(p_{X}\) not only by a vector of probabilities \(_{X}\), but also by an integer \(_{X}=L(X)\) encoding the non-zero lower bound of the domain \((X)\). Indeed, we can write any PMF \(p_{X}\) over an integer domain as the translation of a PMF \(p_{X}^{0}\) whose first non-zero probability event is \(X=0\)

\[p_{X}(x)=(_{_{X}}p_{X}^{0})(x)=p_{X}^{0}(x+_{X}).\] (13)

Since the PMF \(p_{X}^{0}\) can be represented by a probability vector \(_{X}\) as in the previous sections, it follows that the PMF of any probabilistic integer can be characterised by such a vector and an integer \(_{X}\) that shifts the domain. The upper bound of the domain is not important for the characterisation of \(p_{X}\) as it can be obtained via \(U(X)=_{X}+(_{X})-1\), where \((_{X})\) is the dimension of the probability vector \(_{X}\).

The inclusion of translations in our representation of PMFs is compatible with using convolutions to compute the PMF of the sum of two probabilistic integers because of the translational invariance of the convolution

\[_{k}(f*g)=(_{k}f*g)=(f*_{k}g),\] (14)

where \(_{k}\) denotes the translations by a scalar \(k\). In general, \(k\) can be real-valued, but for \(}\) we limit \(k\) to integers. Using Equation 13, we can write the PMF of the sum of two probabilistic integers \(X_{1}\) and \(X_{2}\) with non-zero lower bounds \(_{X_{1}}\) and \(_{X_{2}}\) as

\[p_{X}=(p_{X_{1}}*p_{X_{1}})=(_{_{X_{1}}}p_{X_{1}}^{0}* _{_{X_{2}}}p_{X_{1}}^{0}) =(_{_{X_{1}}}_{_{X_{2}}})(p_{X_{1}}^{0}*p_{ X_{1}}^{0})\] (15) \[=_{_{X_{1}}+_{X_{2}}}(p_{X_{1}}^{0}*p_{X_{1}}^{0}).\] (16)

This final equality shows that we can characterise the PMF \(p_{X}\) for \(X=X_{1}+X_{2}\) by the following lower bound and probability vector

\[_{X}=_{X_{1}}+_{X_{2}}_{X}=F_{M}^{ -1}F_{M}_{X_{1}} F_{M}_{X_{2}}.\] (17)

### Formalising \(_{}\)

\(_{}\) is concerned with computing the parametric form of the probability distribution of a linear integer arithmetic expression. It does so by representing random variables and linear combinations thereof as tensors whose entries are the log-probabilities of the individual events in the sample space of the random variable that is being represented. We define this formally as follows.

**Definition 2.1** (Probabilistic linear arithmetic expression).: Let \(\{X_{1},,X_{N}\}\) be a set of \(N\) independent probabilistic integers with bounded domains. A probabilistic linear integer arithmetic expression \(X\) is itself a bounded probabilistic integer of the form

\[X=_{i=1}^{N}f_{i}(X_{i}),\] (18)

where each \(f_{i}\) denotes an operation performed on the specified random variables that can be either one of the operations specified in Section 3 as well as compositions thereof.

Note that operations within \(_{}\) are closed. That is, performing either of the operations delineated in Section 3 will again result in a bounded probabilistic integer representable as a tensor of log-probabilities and an off-set parameter indicating the value of the smallest possible event (cf. Section 2.3). In Section 4, \(_{}\) will also be provided with probabilistic inference primitives that allow it to compute certain expected values efficiently as well as to perform probabilistic branching.

Assuming all \(f_{i}(X_{i})\) are computable in polytime, we can also compute \(_{}\) expressions (Equation 18) in polytime in \(N\). However, when computing \(_{}\) expressions recursively, the domain size of the random variables might grow super-polynomially - manifesting the #P-hard character of probabilistic inference.

## 3 Arithmetic on Integer-Valued Random Variables

The previous section introduced how \(_{}\) deals with the addition of two probabilistic integers. We discuss now five further operations: 1) addition of a constant, 2) negation, 3) multiplications by a constant, 4) integer division by a constant and 5) the modulo.

**Constant Addition.** The addition of a probabilistic integer \(X\) and constant scalar integer \(k\) forms a new probabilistic integer \(X^{}=X+k\). Adding a scalar integer is equivalent to a translation of the distribution of \(X\) (Figure 2, left). In other words, the lower bound and probability vector of \(X^{}\) are given by

\[_{X^{}}=_{X}+k_{X^{}}=_{X}.\] (19)

Figure 2: (Left) Adding a constant to a probabilistic integer simply means that we have to shift the corresponding histogram, shown here for \(X^{}=X+1\). (Middle) For the negation \(X^{}=-X\), the bins of the histogram reverse their order and the negation of the upper bound becomes the new lower bound. (Right) For multiplication, here show the case \(X^{}=3X\) by inserting zero probability bins.

**Negation.** The negation \(X^{}=-X\) of a probabilistic integer \(X\) is equally straightforward to characterise. Taking a negation mirrors the probability distribution of \(X\) around zero (Figure 2, middle). In terms of lower bound and probability vector, we get \(_{X^{}}=-(_{X}+(_{X})-1)\) and

\[_{X^{}}[x]=_{X^{}}[x] =_{X}[(_{X})-x-1],&0 x<( _{X}),\\ 0,&\] (20)

respectively. That is, the lower bound of \(X^{}\) is equal to the negated upper bound of \(X\) while the probability vector is flipped, taking into account that probability vectors have to start at \(0\).

**Constant Multiplication.** For the multiplication \(X^{}=X k\) of a probabilistic integer \(X\) with a scalar integer \(k\), we assume, without loss of generality, that \(k 0\). Multiplication by a scalar is then characterised as

\[_{X^{}}=_{X} k_{X^{ }}[x]=_{X}[],&x k=00<( _{X}),\\ 0,&\] (21)

Intuitively, only multiples of \(k\) get a non-zero probability equal to the probability of that multiple in \(_{X}\). The lower bound of \(X^{}\) is also immediately given by multiplying the lower bound of \(X\) by \(k\). In other words, we obtain \(_{X^{}}\) by inserting \(k-1\) zeros between every two subsequent entries of \(_{X}\) (Figure 2, right). The case \(k<0\) is obtained by first negating \(X\).

**Integer Division and Modulo.** For the case of integer division \(X^{}=}{{k}}\) and the modulo operation \(X^{}=X k\), the probability distribution of \(X^{}\) can be obtained by adequately accumulating probability mass from events in \((X)\). We demonstrate these operations by example in Figure 3 and refer the reader to Appendix A for the formal description.

## 4 Probabilistic Inference Primitives

### Computing Expected Values

PLIA\({}_{}\) supports the exact computation of two different forms of expected values. The first is a straightforward expectation of a probabilistic integer \(X\), given by weighing each element of \((X)\) with its probability

\[[X]=_{x(X)}x_{X}[x- _{X}].\] (22)

The second is computing the expectation of a linear comparative expression of probabilistic integers. Such a comparison can be an equality, inequality or negated equality. We only consider the equality and strictly larger inequality as the other cases follow from them. The strict inequality can itself always be reduced to an inequality with respect to zero and hence comprises a sum over all domain elements below zero

\[[_{X<0}]=_{x(X):x<0}_{X}[x-_{X}].\] (23)

Similarly, the computation of the expected value of an equality comparison can always be reduced to a comparison to zero. Hence, the expected value is computable by simple indexing

\[[_{X=0}]=_{X}[-_{X}].\] (24)

Figure 3: (Left) We show the histogram transformation for the integer division \(X^{}=}{{3}}\). The probability mass of three subsequent bins is accumulated in the bins for which \(x 3=0\) and \(}{{3}}(X)\). (Right) For the modulo \(X^{}=X 3\), the only non-zero elements of \((X^{})\) are elements of the set \(\{0,1,2\}\). The bins corresponding to these values then accumulate the probability masses of all other bins as indicated by the colors.

As computing expected values is no harder than computing the sum of the elements of a vector, we can conclude that we can compute these expected values in \(((_{X}))\). By using prefix sums  and harnessing parallel compute on GPUs, the complexity can further be reduced to \(((_{X}))\)

### Probabilistic Branching

Consider an if-then-else statement with condition \(c(x)=(f(x) 0)\), where \(f\) is a composition of the functions introduced in Section 3 and \(\{<,,=,>,,\}\). Furthermore, \(x\) belongs to the domain \((X)\) of a probabilistic integer \(X\). In the case of \(c(x)\) being true, a function \(g_{}\) is executed. If \(c(x)\) is false, another function \(g_{}\) is executed instead. We assume that both \(g_{}\) and \(g_{}\) are again linear arithmetic functions expressible in PLIA (Section 2.4).

The if-then-else statement defines a new probabilistic integer \(X^{}\) by combining both of its branches (Figure 4). These branches depend on \(X\) which itself influences a binary random variable \(C\) that represents the probabilistic condition of the if-then-else statement. To be precise, the PMF \(p_{X^{}}\) is given by the decomposition

\[p_{X^{}}(x^{})=p_{X^{}|C}(x^{}) p_{C}( )+p_{X^{}|C}(x^{}) p_{C}(),\] (25)

where \(p_{X^{}|C}\) is the conditional PMF of \(X^{}\) given \(C\). The true branch gives rise to a probabilistic integer \(X_{}\) with probability distribution

\[p_{X_{}}(x)=p_{X|C}(x)=( x)p_{X}(x)}{p_{C}( )}=_{c(x)}_{X}[x-_{X}]}{p_{C}()}.\] (26)

If \(C=\), then \(X^{}\) is given by an application of \(g_{}\) on the instances \(x(X)\) that satisfy \(c(x)\). Consequently, by applying \(g_{}\), we find that

\[p_{X^{}|C}(x^{})=p_{g_{}(X_{})}(x^{}).\] (27)

With the right-hand side of Equation 26, we now know how to obtain the probability vector for \(X_{}\). Using Equation 27 and the operations from Section 3, we can then compute the probability vector \(_{g_{}(X_{})}\), as well as the lower bound \(_{g_{}(X_{})}\). Also note that \(p_{C}()\) is nothing but an expected value as described by Equation 23 or Equation 24. Analogously, we obtain for the false branch that

\[p_{X_{}}(x)=_{c(x)})_{X}[x-_{X}]} {p_{C}()} p_{X^{}|C}(x^{})=p_{g _{}(X_{})}(x^{}).\] (28)

By plugging the expressions for \(p_{X^{}|C}(x^{})\) and \(p_{X^{}|C}(x^{})\) into Equation 25 we find that

\[p_{X^{}}(x^{})=p_{g_{}(X_{})}(x^{}) p_{C}( )+p_{g_{}(X_{})}(x^{}) p_{C}(),\] (29)

which are all quantities computable using either probabilistic linear arithmetic operations or expected values thereof.

Figure 4: Control flow diagram for probabilistic branching. The branching condition is probabilistically true and induces a binary random variable \(C\). In each of the two branches we then have two conditionally independent random variables \(X_{}\) and \(X_{}\) to which the functions \(g_{}\) and \(g_{}\) are applied in their respective branches. The probabilities of \(X^{}\) are then given by the weighted sums of the probabilities of \(g_{}(X_{})\) and \(g_{}(X_{})\) (Equation 29).

## 5 Experiments

We first compare \(_{}\) to the state of the art in probabilistic integer arithmetic  in terms of inference speed (Section 5.1). These experiments were performed using an Intel Xeon Gold 6230R CPU @ 2.10GHz, 256GB RAM for CPU experiments and an Nvidia TITAN RTX (24GB) for GPU experiments. In Section 5.2, we then illustrate how \(_{}\) fares against the state of the art in neurosymbolic AI [14; 33]. These experiments were performed using an Nvidia RTX 3080 Ti (12GB). We implemented \(_{}\) in TensorFlow  using the Einops library . This implementation is open-source and available at https://github.com/ML-KULeuven/probabilistic-arithmetic

### Exact Inference with Probabilistic Integers

The work of Cao et al.  exploits the structural properties and symmetries of integer arithmetic by proposing general encoding strategies for an arithmetic expression of probabilistic integers as logical circuits. That is, binary decision diagrams  obtained via knowledge compilation . This strategy allows them to avoid redundant calculations and repetition, leading to improved scalability over more naive encodings [8; 13].

We compare \(_{}\) and Cao et al.'s inference algorithm on four of their benchmark problems. In the first three benchmarks, expected values of the sum of two random variables need to be computed. Concretely, the expectations \([X_{1}+X_{2}]\) (cf. Equation 22), \([_{X_{1}+X_{2}<0}]\) (cf. Equation 23) and \([_{X_{1}+X_{2}=0}]\) (cf. Equation 24).

As a fourth benchmark we use a probabilistic version of the Luhn checksum algorithm , which necessitates summation of two probabilistic integers, negation, addition of a constant, multiplication by a constant, the modulo operations, as well as probabilistic branching. We provide further details on the Luhn algorithm in general and the encoding of its probabilistic variant in \(_{}\) in Appendix C.

As the probabilistic Luhn algorithm takes as input an identifier consisting of a sequence of probabilistic integers with domain \(\{0,,9\}\), we can increase the problem size by increasing the length of this sequence. For the other three benchmarks we vary the problem size by varying the domain size of the probabilistic integers in terms of their _bitwidth_. That is, a bitwidth of \(i\) indicates that we consider probabilistic integers ranging from \(0\) up until \(2^{i}-1\), increasing the problem size exponentially in terms of \(i\). In our experimental evaluation (Figure 5), we measure the time it took for \(_{}\) and Cao et al.'s method to terminate for varying problem sizes. The measured time includes all computational overhead inherent to each method, such as the construction of computational graphs and compilation time. Each method is also profiled in terms of memory, which we discuss further in Appendix B. Note that Cao et al.'s method is denoted by "Dice", the probabilistic programming language in which it was implemented.

For the first three benchmarks, we observe that \(_{}\) easily scales to probabilistic integers with a domain size of \(2^{24}\) on both the CPU and GPU as the highest runtime reached is less than 100 seconds on the CPU and approximately 1 second on the GPU. The similarity of the curves is due to the fact

Figure 5: We plot the runtime of Dice  and \(_{}\) against the domain size of the problems. From left to right, we have \([X_{1}+X_{2}]\), \([_{X_{1}+X_{2}<0}]\), \([_{X_{1}+X_{2}=0}]\) and probabilistic Luhn. All four plots share the same y-axis on the very left, which is in log-scale. Following the experimental protocol of Cao et al. , we report average runtimes for every integer on the x-axis, both bitwidths and identifier lengths. No significant deviations from the mean were found.

that the run time is dominated by computing the probability vector \(_{X_{1}+X_{2}}\) and not so much by computing the actual expected value.

In contrast, Dice, which only runs on CPU, already reaches a runtime of approximately \(1000\) seconds for integers with domain size \(2^{15}\), where \(_{}\) only takes around \(10^{-1}\) and \(10^{-2}\) seconds on the CPU and GPU, respectively. This is a rather considerable improvement in the order of \(}\). Dice can outperform \(_{}\) on the GPU (Figure 5, bitwidth smaller strictly below 5) due to the computational overhead of running on the GPU. However, much of this overhead can be avoided by running \(_{}\) on the CPU for smaller domain sizes, where it performs on par or better than Dice (Figure 5, bitwidth smaller strictly below 5).

On the probabilistic Luhn benchmark (Figure 5, extreme right) we observe that both methods exhibit similar linear scaling behaviors. However, the use of tensors as representations instead of logical circuits does result in a significant improvement in terms of run time in the order of \(10^{2}\) for the longest sequences of length \(350\).

### Neurosymbolic Learning

For the comparison of \(_{}\) to neurosymbolic systems, we use two standard benchmarks from the literature: MNIST addition  and visual sudoku . The common idea for both is to train neural networks to classify MNIST digits while only having access to distant supervision (Figure 6).

As an MNIST classifier outputs a distribution over the integers \(\{0,,9\}\), we can readily encode these predictions as probabilistic integers and enforce the constraints given by the two problems using the arithmetic operations developed in Section 2 and Section 3. We refer the reader to Appendix D (MNIST addition) and Appendix E (visual sudoku) for details on the encodings.

In the experimental evaluation we compare \(_{}\), which uses exact probabilistic inference, to one other exact method, DeepProbLog (DPL) [21; 22], and two approximate methods, Scallop  and A-NeSI . Similar to Dice (Section 5.1), DPL also relies on expensive knowledge compilation in order to obtain the distribution of the sum of two probabilistic integers. Essentially, it performs explicit enumeration as illustrated in Figure 1. Scallop approximates this explicit enumeration by only considering the top-\(k\) most likely solutions of a problem. The approximation of A-NeSI is based on optimising a neural surrogate model for the combinatorial problem. While this model sidesteps the computational complexity encountered by DPL and Scallop to a certain degree, training the surrogate model becomes prohibitively expensive for larger problems.

We compare the different methods along two dimensions, being prediction accuracy and training time. Specific details on training, e.g. neural architectures and hyperparameters, can be found in Appendix F. We report the statistics in Table 1, where we use numbers consisting of \(N\{2,4,15,50\}\) digits for the MNIST addition benchmark and grid size \(G\{4,9\}\) for the visual sudoku benchmark. We see that \(_{}\) significantly outperforms the other methods, both exact as well as approximate, in terms of training times. The difference is particularly apparent on the MNIST addition benchmark, where no other method was able to scale up to \(N=50\) without timing out. The reported accuracies also show that this advantage of training time for \(_{}\) with respect to the other methods does not come at the cost of predictive performance of the learned neural networks.

Figure 6: (Left) Example of an MNIST addition data point, consisting of two numbers given as a series of MNIST digits and an integer. The integer is the sum of the two numbers and constitutes the label of the data point. (Right) Data point from the visual sudoku data set, consisting of a \(9 9\) grid filled with MNIST digits. Data points are labeled with a Boolean value indicating whether the integers underlying the MNIST digits satisfy the constraints of sudoku.

## 6 Conclusion and Future Work

We introduced PLIA\({}_{}\), an efficient, differentiable and hyperparameter-free framework for linear arithmetic over integer-valued random variables. The efficiency of PLIA\({}_{}\) is due to representing probabilistic integers as tensors and exploiting the FFT for computing the sum of two probabilistic integers. Compared to state-of-the-art methods for inference  and learning  the concepts underlying PLIA\({}_{}\) are surprisingly simply: a tensorised calculus and the fast Fourier transform. This simple yet elegant approach has led to improvements in inference and learning times in the order of \(10^{5}\) and \(10^{2}\), respectively. We attest this advantage to PLIA\({}_{}\)'s formulation in terms of fundamental concepts shared across modern machine learning. As such, any algorithmic or hardware improvements will immediately improve PLIA\({}_{}\)'s performance. For instance, incorporating recent advancements for computing FFTs on GPUs  would directly benefit PLIA\({}_{}\)'s efficiency.

In future work we envisage to extend the probabilistic inference primitives of PLIA\({}_{}\) to a full-fledged neuroprobabilistic programming language, resulting in a user-friendly interface to an efficient neurosymbolic reasoning engine. In this regard, we deem ideas from the satisfiability modulo theory literature  as important. Specifically, probabilistic [17; 25] and neural  extensions thereof, as well as novel formula representations [10; 24].