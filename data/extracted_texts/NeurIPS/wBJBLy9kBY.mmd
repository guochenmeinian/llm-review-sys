# Ambient Diffusion:

Learning Clean Distributions from Corrupted Data

 Giannis Daras

UT Austin

giannisdaras@utexas.edu

Kulin Shah

UT Austin

kulinshah@utexas.edu

Yuval Dagan

UC Berkeley

yuvald@berkeley.edu

Aravind Gollakota

Apple

aravindg@cs.utexas.edu

Alexandros G. Dimakis

UT Austin

dimakis@austin.utexas.edu

Adam Klivans

UT Austin

klivans@utexas.edu

###### Abstract

We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize any individual training sample, since they never observe clean training data. Our main idea is to introduce _additional measurement distortion_ during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have \(90\%\) of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set.

## 1 Introduction

Diffusion generative models  are emerging as versatile and powerful frameworks for learning high-dimensional distributions and solving inverse problems . Numerous recent developments  have led to text conditional foundation models like Dalle-2 , Latent Diffusion  and Imagen  with an incredible performance in general image domains. Training these models requires access to high-quality datasets which may be expensive or impossible to obtain. For example, direct images of black holes cannot be observed  and high quality MRI images require long scanning times, causing patient discomfort and motion artifacts .

Recently, Carlini et al. , Somepalli et al. , and Jagielski et al.  showed that diffusion models can memorize examples from their training set. Further, an adversary can extract dataset samples given only query access to the model, leading to privacy, security and copyright concerns. For many applications, we may want to learn the distribution but not individual training images e.g. we might want to learn the distribution of X-ray scans but not memorize images of specific patient scans from the dataset. Hence, we may want to introduce corruption as a design choice. We show that it is possible to train diffusions that learn a distribution of clean data by only observing highly corrupted samples.

**Prior work in supervised learning from corrupted data.** The traditional approach to solving such problems involves training a restoration model using supervised learning to predict the clean image based on the measurements [47; 48; 63; 41]. The seminal NoiseNoise  work introduced a practical algorithm for learning how to denoise in the absence of any non-noisy images. This framework and its generalizations [5; 38; 59] have found applications in electron microscopy , tomographic image reconstruction , fluorescence image reconstruction , blind inverse problems [20; 5], monocular depth estimation and proteomics . Another related line of work uses Stein's Unbiased Risk Estimate (SURE) to optimize an unbiased estimator of the denoising objective without access to non-noisy data . We stress that the aforementioned research works study the problem of _restoration_, whereas are interested in the problem of _sampling_ from the clean distribution. Restoration algorithms based on supervised learning are only effective when the corruption level is relatively low . However, it might be either not possible or not desirable to reconstruct individual samples. Instead, the desired goal may be to learn to _generate_ fresh and completely unseen samples from the distribution of the uncorrupted data but _without reconstructing individual training samples_.

Indeed, for certain corruption processes, it is theoretically possible to perfectly learn a distribution only from highly corrupted samples (such as just random one-dimensional projections), even though individual sample denoising is usually impossible in such settings.

AmbientGAN  showed that general \(d\) dimensional distributions can be learned from _scalar_ observations, by observing only projections on one-dimensional random Gaussian vectors, in the infinite training data limit. The theory requires an infinitely powerful discriminator and hence does not apply to diffusion models. MCFlow  is a framework, based on a variant of the EM algorithm, that can be used to train normalizing flow models from missing data. Finally, MIWAE  and Not-MIWAE  are frameworks to learn deep latent models (e.g. VAEs) from missing data when the corruption process is known or unknown respectively.

**Our contributions.** We present the first diffusion-based framework to learn an unknown distribution \(\) when the training set only contains highly-corrupted examples drawn from \(\). Specifically, we consider the problem of learning to sample from the target distribution \(p_{0}(_{0})\) given corrupted

Figure 1: **Left panel:** Baseline method of vanilla finetuning Deepfloyd IF using \(3000\) images from CelebA-HQ. We show generated sample images and nearest neighbors from the finetuning set. As shown, the generated samples are often near-identical copies from training data. This verifies related work Carlini et al. , Somepalli et al. , and Jagielski et al.  that pointed out that diffusions often generate training samples. **Right panel:** We finetune the same foundation model (Deepfloyd IF) using our method and \(3000\) highly corrupted training images. The corruption adds noise and removes \(80\) percent random pixels. We show generated samples and nearest neighbors from the training set. Our method still learns the clean distribution of faces (with some quality deterioration, as shown) but does not memorize training data. We emphasize that our training is performed without ever accessing clean training data.

samples \(A_{0}\) where \(A p(A)\) is a random corruption matrix (with known realizations and prior distribution) and \(_{0} p_{0}(_{0})\). Our main idea is to introduce _additional measurement distortion_ during the diffusion process and require the model to predict the original corrupted image from the further corrupted image.

* We provide an algorithm that provably learns \([_{0}|(_{0}+_{t}),]\), for all noise levels \(t\) and for \( p( A)\) being a further corrupted version of \(A\). The result holds for a general family of corruption processes \(A p(A)\). For various corruption processes, we show that the further degradation introduced by \(\) can be very small.
* We use our algorithm to train diffusion models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) with training data at different levels of corruption.
* Given the learned conditional expectations we provide an approximate sampler for the target distribution \(p_{0}(_{0})\).
* We show that for up to \(90\%\) missing pixels, we can learn reasonably well the distribution of uncorrupted images. We outperform the previous state-of-the-art AmbientGAN  and natural baselines.
* We show that our models perform on par or even outperform state-of-the-art diffusion models for solving certain inverse problems even without ever seeing a clean image during training. Our models do so with a single prediction step while our baselines require hundreds of diffusion steps.
* We use our algorithm to finetune foundational pretrained diffusion models. Our finetuning can be done in a few hours on a single GPU and we can use it to learn distributions with a few corrupted samples.
* We show that models trained on sufficiently corrupted data do not memorize their training set. We measure the tradeoff between the amount of corruption (that controls the degree of memorization), the amount of training data and the quality of the learned generator.
* We open-source our code and models: https://github.com/giannisdaras/ambient-diffusion.

## 2 Background

Training a diffusion model involves two steps. First, we design a corruption process that transforms the data distribution gradually into a distribution that we can sample from [58; 14]. Typically, this corruption process is described by an Ito SDE of the form: \(=(,t)t+g(t)\), where \(\) is the standard Wiener process. Such corruption processes are _reversible_ and the reverse process is also described by an Ito SDE : \(=(,t)-g^{2}(t)_{} p_{t}( {x})\,t+g(t)\). The designer of the diffusion model is usually free to choose the drift function \((,)\) and the diffusion function \(g()\). Typical choices are setting \((,t)=,g(t)=_{t}^{2}}{ t}}\) (Variance Exploding SDE) or setting \((x,t)=-(t),g(t)=\) (Variance Preserving SDE). Both of these choices lead to a Gaussian terminal distribution and are equivalent to a linear transformation in the input. The goal of diffusion model training is to learn the function \(_{} p_{t}()\), which is known as the score function.

Figure 2: Illustration of our method: Given training data with deleted pixels, we corrupt further by erasing more (illustrated with green color). We feed the learner the further corrupted images and we evaluate it on the originally observed pixels. We can do this during training since the green pixel values are known to us. The score network learner has no way of knowing whether a pixel was missing from the beginning or whether it was corrupted by us. Hence, the score network learns to predict the clean image everywhere. Our method is analogous to grading a random subset of the questions in a test, but the students not knowing which questions will be graded.

To simplify the presentation of the paper, we will focus on the Variance Exploding SDE that leads to conditional distributions \(_{t}=_{0}+_{t}\).

Vincent  showed that we can learn the score function at level \(t\) by optimizing for the score-matching objective:

\[J()=_{(_{0},_{t})}| |_{}(_{t},t)-_{0}||^{2}.\] (2.1)

Specifically, the score function can be written in terms of the minimizer of this objective as:

\[_{_{t}} p_{t}(_{t})=_{^{ *}}(_{t},t)-_{t}}{_{t}}.\] (2.2)

This result reveals a fundamental connection between the score-function and the best restoration model of \(_{0}\) given \(_{t}\), known as Tweedie's Formula . Specifically, the optimal \(_{^{*}}(_{t},t)\) is given by \([_{0}|_{t}]\), which means that

\[_{_{t}} p_{t}(_{t})=[_{0}|_{t}]}^{}-_{t}}{_{t}}.\] (2.3)

Inspired by this restoration interpretation of diffusion models, the Soft/Cold Diffusion works [14; 4] generalized diffusion models to look at non-Markovian corruption processes: \(_{t}=C_{t}_{0}+_{t}\). Specifically, Soft Diffusion proposes the Soft Score Matching objective:

\[J_{}()=_{(_{0},_ {t})}||C_{t}(_{}(_{t},t)-_{0}) ||^{2},\] (2.4)

and shows that it is sufficient to recover the score function via a generalized Tweedie's Formula:

\[_{_{t}} p_{t}(_{t})=[ _{0}|_{t}]-_{t}}{_{t}}.\] (2.5)

For these generalized models, the matrix \(C_{t}\) is a design choice (similar to how we could choose the functions \(,g\)). Most importantly, for \(t=0\), the matrix \(C_{t}\) becomes the identity matrix and the noise \(_{t}\) becomes zero, i.e. we observe samples from the true distribution.

## 3 Method

As explained in the introduction, in many cases we do not observe uncorrupted images \(_{0}\), either by design (to avoid memorization and leaking of sensitive data) or because it is impossible to obtain clean data. Here we study the case where a learner only has access to linear measurements of the clean data, i.e. \(_{0}=A_{0}\), and the corruption matrices \(A:^{m n}\). We note that we are interested in non-invertible corruption matrices. We ask two questions:

1. Is it possible to learn \([_{0}|A(_{0}+_{t}),A]\) for all noise levels \(t\), given only access to corrupted samples \((_{0}=A_{0},A)\)?
2. If so, is it possible to use this restoration model \([_{0}|A(_{0}+_{t}),A]\) to recover \([_{0}|_{t}]\) for any noise level \(t\), and thus sample from the true distribution through the score function as given by Tweedie's formula (Eq. 2.3)?

We investigate these questions in the rest of the paper. For the first, the answer is affirmative but only after introducing additional corruptions, as we explain below. For the second, at every time step \(t\), we approximate \([_{0}|_{t}]\) directly using \([_{0}|A_{t},A]\) (for a chosen \(A\)) and substitute it into Eq. 2.3. Empirically, we observe that the resulting approximate sampler yields good results.

### Training

For the sake of clarity, we first consider the case of random inpainting. If the image \(_{0}\) is viewed as a vector, we can think of the matrix \(A\) as a diagonal matrix with ones in the entries that correspond to the preserved pixels and zeros in the erased pixels. We assume that \(p(A)\) samples a matrix where each entry in the diagonal is sampled i.i.d. with a probability \(1-p\) to be \(1\) and \(p\) to be zero.

We would like to train a function \(_{}\) which receives a corruption matrix \(A\) and a noisy version of a corrupted image, \(_{t}=A_{0}+_{t})}_{_{t}}\) where \((0,I)\), and produces an estimate for the conditional expectation. The simplest idea would be to simply ignore the missing pixels and optimize for:

\[J^{}_{}()=_{(_{0},_{t},A)}||A(_{}(A,A_{t},t)-_{0} )||^{2},\] (3.1)

Despite the similarities with Soft Score Matching (Eq 2.4), this objective will not learn the conditional expectation. The reason is that the learner is never penalized for performing arbitrarily poorly in the missing pixels. Formally, any function \(_{^{}}\) satisfying \(A_{^{}}(A,_{t},t)=A[_{0}|A_{t},A]\) is a minimizer.

Instead, we propose to _further corrupt_ the samples before feeding them to the model, and ask the model to predict the original corrupted sample from the further corrupted image.

Concretely, we randomly corrupt \(A\) to obtain \(=BA\) for some matrix \(B\) that is selected randomly given \(A\). In our example of missing pixels, \(\) is obtained from \(A\) by randomly erasing an additional fraction \(\) of the pixels that survive after the corruption \(A\). Here, \(B\) will be diagonal where each element is \(1\) with probability \(1-\) and \(0\) w.p. \(\). We will penalize the model on recovering all the pixels that are visible in the sample \(A_{0}\): this includes both the pixels that survive in \(_{0}\) and those that are erased by \(\). The formal training objective is given by minimizing the following loss:

\[J^{}()=_{(_{0},_{1},A, )}||A(_{}(,_{t},t )-_{0})||^{2},\] (3.2)

The key idea behind our algorithm is as follows: the learner does not know if a missing pixel is missing because we never had it (and hence do not know the ground truth) or because it was deliberately erased as part of the further corruption (in which case we do know the ground truth). Thus, the best learner cannot be inaccurate in the unobserved pixels because with non-zero probability it might be evaluated on some of them. Notice that the trained model behaves as a denoiser in the observed pixels and as an inpainter in the missing pixels. We also want to emphasize that the probability \(\) of further corruption can be arbitrarily small as long as it stays positive.

The idea of further corruption can be generalized from the case of random inpainting to a much broader family of corruption processes. For example, if \(A\) is a random Gaussian matrix with \(m\) rows, we can form \(\) by deleting one row from \(A\) at random. If \(A\) is a block inpainting matrix (i.e. a random block of fixed size is missing from all of the training images), we can create \(\) by corrupting further with one more non-overlapping missing block. Examples of our further corruption are shown in Figure 2. In our Theory Section, we prove conditions under which it is possible to recover \([_{0}|_{t},]\) using our algorithm and samples \((_{0}=A_{0},A)\). Our goal is to satisfy this condition while adding minimal further corruption, i.e. while keeping \(\) close to \(A\).

### Sampling

To sample from \(p_{0}(_{0})\) using the standard diffusion formulation, we need access to \(_{_{t}} p_{t}(_{t})\), which is equivalent to having access to \([_{0}|_{t}]\) (see Eq. 2.3). Instead, our model is trained to predict \([_{0}|_{t},]\) for all matrices \(A\) in the support of \(p(A)\).

We note that for random inpainting, the identity matrix is technically in the support of \(p(A)\). However, if the corruption probability \(p\) is at least a constant, the probability of seeing the identity matrix is exponentially small in the dimension of \(_{t}\). Hence, we should not expect our model to give good estimates of \([_{0}|_{t},]\) for corruption matrices \(A\) that belong to the tails of the distribution \(p(A)\).

The simplest idea is to sample a mask \( p()\) and approximate \([_{0}|_{t}]\) with \([_{0}|_{t},]\). Under this approximation, the discretized sampling rule becomes:

\[_{t- t}=}{_{t}}}_{ _{t}}_{t}+-_{t- t}}{_{t}}}_{1 -_{t}}[_{0}|_{t},]}_ {}_{0}}.\] (3.3)This idea works surprisingly well. Unless mentioned otherwise, we use it for all the experiments in the main paper and we show that we can generate samples that are reasonably close to the true distribution (as shown by metrics such as FID and Inception) even with \(90\%\) of the pixels missing.

We include pseudocode for our sampling in Algorithm 1. In the Appendix (Section E.4), we ablate an alternative choice for sampling that can lead to slight improvements at the cost of increased function evaluations.

``` noise schedule \(\{_{t}\}_{0}^{T}\), model parameters \(\), and step size \( t\) for discretizing the Reverse ODE.  Sample \(_{T}(0,_{T}^{2}I)\)\(\) Sample initial noise  Sample \( p()\)\(\) Sample a mask \(t T\) repeat \(_{t}_{t}\)\(\) Create measurements \(}_{0}_{}(_{t},,t)\)\(\) Use the network to estimate \([_{0}|_{t},]\)\(_{t- t}-_{t- t}}{_{t}}_{t}+ -_{t- t}}{_{t}}}_{0}\)\(\) Run one step of the Reverse ODE approximating \([_{0}|_{t}]\) with \(}_{0}\) \(t t- t\) until\(t t\)return\(_{0}\) ```

**Algorithm 1** Ambient Diffusion Fixed Mask Sampling (for VE SDE)

We underline that the Fixed Mask Sampler does not come with a theoretical guarantee of sampling from \(p_{0}(_{0})\). In the Appendix, we prove that whenever it is possible to reconstruct \(p_{0}(_{0})\) from corrupted samples, it is also possible to reconstruct it using access to \([_{0}|A_{t},A]\) (Lemma A.3). However, as stated in the Limitations section, we were not able to find any practical algorithm to do so. Nevertheless, experimentally the Fixed Mask Sampler has strong performance and beats previously proposed baselines for different classes of generative models.

## 4 Theory

As elaborated in Section 3, one of our key goals is to learn the best restoration model for the measurements at all noise levels, i.e., the function \((A,_{t},t)=[_{0}|_{t},A]\). We now show that under a certain assumption on the distribution of \(A\) and \(\), the true population minimizer of Eq. 3 is indeed essentially of the form above. This assumption formalizes the notion that even conditional on \(\), \(A\) has considerable variability, and the latter ensures that the best way to predict \(A_{0}\) as a function of \(_{t}\) and \(\) is to optimally predict \(_{0}\) itself. All proofs are deferred to the Appendix.

**Theorem 4.1**.: _Assume a joint distribution of corruption matrices \(A\) and further corruption \(\). If for all \(\) in the support it holds that \(_{A|}[A^{T}A]\) is full-rank, then the unique minimizer of the objective in equation 3 is given by_

\[_{^{*}}(,_{t},t)=[_{0} _{t},]\] (4.1)

Two simple examples that fit into this framework (see Corollaries A.1 and A.2 in the Appendix) are:

* Inpainting: \(A^{n n}\) is a diagonal matrix where each entry \(A_{ii}(1-p)\) for some \(p>0\) (independently for each \(i\)), and the additional noise is generated by drawing \(|A\) such that \(_{ii}=A_{ii}(1-)\) for some small \(>0\) (again independently for each \(i\)).1 * Gaussian measurements: \(A^{m n}\) consists of \(m\) rows drawn independently from \((0,I_{n})\), and \(^{m n}\) is constructed conditional on \(A\) by zeroing out its last row.

Notice that the minimizer in Eq 4.1 is not entirely of the form we originally desired, which was \((A,_{t},t)=[_{0} A_{t},A]\). In place of \(A\), we now have \(\), which is a further degraded matrix.

[MISSING_PAGE_FAIL:7]

models available. We choose this model over Stable Diffusion  because it works in the pixel space (and hence our algorithm directly applies).

Memorization.We show that we can finetune a foundational model on a limited dataset without memorizing the training examples. This experiment is motivated by the recent works of Carlini et al. , Somepalli et al. , and Jagielski et al.  that show that diffusion generative models memorize training samples and they do it significantly more than previous generative models, such as GANs, especially when the training dataset is small. Specifically, Somepalli et al.  train diffusion models on subsets of size \(\{300,3000,30000\}\) of CelebA and they show that models trained on \(300\) or \(3000\) memorize and blatantly copy images from their training set.

We replicate this training experiment by finetuning the IF model on a subset of CelebA with \(3000\) training examples. Results are shown in Figure 1. Standard finetuning of Deepfloyd's IF on \(3000\) images memorizes samples and produces almost exact copies of the training set. Instead, if we corrupt the images by deleting \(80\%\) of the pixels prior to training and finetune, the memorization decreases sharply and there are distinct differences between the generated images and their nearest neighbors from the dataset. This is in spite of finetuning until convergence.

  
**Dataset** & **Corruption Probability** & **Method** & **LPIPS** & **PSNR** & **NFE** \\  CelebA-HQ & & Ours & **0.037** & **31.51** & 1 \\   & 0.6 & DPS & 0.053 & 28.21 & 100 \\   & & DDRM & 0.139 & 25.76 & 35 \\  & & DDRM & 0.088 & 27.38 & 99 \\  & & & 0.069 & 28.16 & 199 \\   & & Ours & **0.084** & **26.80** & 1 \\   & & DPS & 0.107 & 24.16 & 100 \\   & 0.8 & & 0.316 & 20.37 & 35 \\  & & DDRM & 0.188 & 22.96 & 99 \\  & & & 0.153 & 23.82 & 199 \\   & & Ours & **0.152** & **23.34** & 1 \\   & 0.9 & & DPS & 0.168 & 20.89 & 100 \\   & & & 0.461 & 15.87 & 35 \\  & & DDRM & 0.332 & 18.74 & 99 \\  & & & 0.242 & 20.14 & 199 \\   AFHQ & & Ours & 0.030 & 33.27 & 1 \\   & 0.4 & & DPS & **0.020** & **34.06** & 100 \\   & & & 0.122 & 25.18 & 35 \\  & & DDRM & 0.091 & 26.42 & 99 \\  & & & 0.088 & 26.52 & 199 \\   & & Ours & 0.062 & 29.46 & 1 \\   & & DPS & **0.051** & **30.03** & 100 \\   & 0.6 & & 0.246 & 20.76 & 35 \\  & & DDRM & 0.166 & 22.79 & 99 \\  & & & 0.160 & 22.93 & 199 \\   & & Ours & 0.124 & **25.37** & 1 \\   & & DPS & **0.107** & 25.30 & 100 \\   & & & 0.525 & 14.56 & 35 \\  & & DDRM & 0.295 & 18.08 & 99 \\  & & & 0.258 & 18.86 & 199 \\ 

Table 1: Comparison of our model (trained on corrupted data) with state-of-the-art diffusion models on CelebA (DDIM  model) and AFHQ (EDM  model) for solving the random inpainting inverse problem. Our model performs on par with state-of-the-art diffusion inverse problem solvers, even though it has never seen uncorrupted training data. Further, this is achieved with a single score function evaluation. To solve this problem with a standard pre-trained diffusion model we need to use a reconstruction algorithm (such as DPS  or DDRM ) that typically requires hundreds of steps.

To quantify the memorization, we follow the methodology of Somepalli et al. . Specifically, we generate 10000 images from each model and we use DINO -v2  to compute top-\(1\) similarity to the training images. Results are shown in Figure 6. Similarity values above \(0.95\) roughly correspond to the same person while similarities below \(0.75\) typically correspond to random faces. The standard finetuning (Red) often generates images that are near-identical with the training set. Instead, finetuning with corrupted samples (blue) shows a clear shift to the left. Visually we never observed a near-copy generated from our process - see also Figure 1.

We repeat this experiment for models trained on the full CelebA dataset and at different levels of corruption. We include the results in Figure 8 of the Appendix. As shown, the more we increase the corruption level the more the distribution of similarities shifts to the left, indicating less memorization. However, this comes at the cost of decreased performance, as reported in Table 4.

**New domains and different corruption.** We show that we can also finetune a pre-trained foundation model on a _new domain_ given a limited-sized dataset in a few hours in a single GPU. Figure 5 shows generated samples from a finetuned model on a dataset containing \(155\) examples of brain tumor MRI images . As shown, the model learns the statistics of full brain tumor MRI images while only trained on brain-tumor images that have a random box obfuscating \(25\%\) of the image. The training set was resized to \(64 64\) but the generated images are at \(256 256\) by simply leveraging the power of the cascaded Deepfloyd IF.

Figure 4: Inception/FID results on random inpainting for models trained with our algorithm on CelebA-HQ, AFHQ and CIFAR-10.

Figure 5: **Left panel: We finetune Deepfloydâ€™s IF diffusion model to make it a generative model for MRI images of brains with tumors. We use a small dataset  of only \(155\) images that was corrupted by removing large blocks as shown. Right panel: Generated samples from our finetuned model. As shown, the model learns the statistics of full brain tumor MRI images. The training set was resized to \(64 64\) but the generated images are at \(256 256\). The higher resolution is obtained by simply leveraging the power of the cascaded IF model.**

Figure 3: Performance on CIFAR-10 as a function of the corruption level. We compare our method with a diffusion model trained without our further corruption trick and AmbientGAN . Ambient Diffusion outperforms both baselines for all ranges of corruption levels.

**Limitations.** Our work has several limitations. First, there is a tradeoff between generator quality and corruption levels. For higher corruption, it is less likely that our generator memorizes parts of training examples, but at a cost of degrading quality. Precisely characterizing this trade-off is an open research problem. Further, in this work, we only experimented with very simple approximation algorithms to estimate \([_{0}|_{t}]\) using our trained models. Additionally, we cannot make any strict privacy claim about the protection of any training sample without making assumptions about the data distribution. We show in the Appendix that it is possible to recover \([_{0}|_{t}]\) exactly using our restoration oracle, but we do not have an algorithm to do so. Finally, our method cannot handle measurements that also have noise. Future work could potentially address this limitation by exploiting SURE regularization as in .