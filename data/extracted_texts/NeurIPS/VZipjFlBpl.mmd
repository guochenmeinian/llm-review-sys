# Evidence from fMRI Supports a Two-Phase

Abstraction Process in Language Models

 Richard J. Antonello

Columbia University

New York City, New York

rja2163@columbia.edu

&Emily Cheng

Universitat Pompeu Fabra

Barcelona, Spain

emilyshana.cheng@upf.edu

These authors contributed equally to this work.

###### Abstract

Research has repeatedly demonstrated that intermediate hidden states extracted from large language models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most capable for this unique and highly general transfer task? In this work, we show that evidence from language encoding models in fMRI supports the existence of a two-phase abstraction process within LLMs. We use geometric methods to show that this abstraction process naturally arises over the course of training a language model and that the first "composition" phase of this abstraction process is compressed into fewer layers as training continues. Finally, we demonstrate a strong correspondence between layerwise encoding performance and the intrinsic dimensionality of representations from LLMs. We give initial evidence that this correspondence primarily derives from the inherent compositionality of LLMs and not their next-word prediction properties.

## 1 Introduction

How do brains and machines take low-level information, such as a collection of sounds or words, and compose it into the rich tapestry of ideas and concepts that can be expressed in natural language? This question of composition, or abstraction, is at the heart of most studies of human language comprehension. Recent work has shown that representations from large language models (LLMs) are able to successfully model human brain activity at varying spatial and temporal resolutions with only a linear transformation . This has led to questions about the reason for this brain-model similarity. Do LLMs and brains possess similar representations because they have similar learning properties or objectives?  Or is the similarity merely a consequence of shared abstraction, the ability to represent features not derivable from the lexical properties of language alone? 

In this work, we present new evidence that it is the abstractive, compositional properties of LLMs that drive predictivity between LLMs and brains. We do this by examining an underexplored and unexplained phenomenon of the similarity - the tendency for intermediate hidden layers of LLMs to be optimal for this linear transfer task. We show that an LLM layer's performance at predicting brain activity is strongly related to intrinsic dimensionality of that layer relative to other layers in the same network. Furthermore, we demonstrate that this relationship is itself an indicator that pretrained LLMs naturally split into an early _abstraction_, or composition, phase, and a later _prediction_, or extraction, phase, a result independently suggested in the LM interpretability literature . We suggest that it is the first abstraction phase, rather than the latter prediction phase, that primarily drives the observed correspondence between brains and LLMs.

## 2 Methods

We test the hypothesis that feature abstraction, not next-token prediction _per se_, drives brain-model similarity. To do so requires three observables. First, we measure the dependent variable, **(1)** brain-model representational similarity, by scoring the prediction performance of a learned linear mapping from LLM representations to brain activity. Then, we compute the **(2)** dimensionality of representations to measure abstract feature complexity over the LLM's layers. Finally, to test the alternate hypothesis that next-token prediction drives brain-LM similarity, as has been suggested by others [13; 8; 1], we compute the **(3)**_surprisal_, or next-token prediction error, from each layer.

### Brain-model similarity

fMRI dataWe used publicly available functional magnetic resonance imaging (fMRI) data collected from 3 human subjects as they listened to 20 hours of English language podcast stories over Sensimetrics S14 headphones. Stories came from podcasts such as _The Math Radio Hour_, _Modern Love_, and _The Anthropocene Reviewed_. Each 10-15 minute story was played during a separate scan. Subjects were not asked to make any responses, but simply to listen attentively to the stories. For encoding model training, each subject listened to roughly 95 different stories, giving 20 hours of data across 20 scanning sessions, or a total of ~33,000 datapoints for each voxel across the whole brain. Additional details of the MRI methods are summarized in Appendix D.

Neural encoding model trainingTo train encoding models, we use the method described in . High-level details of the method are summarized here. For each word in the stimulus set, activations were extracted by feeding that word and its immediate preceding context into the LLM. A sliding window was used to ensure each word received a minimum of 256 tokens of context. Activations were then downsampled using a Lanczos filter and FIR delays of 1,2,3 and 4 TRs were added to account for the hemodynamic lag in the BOLD signal. A linear projection from the downsampled, time-delayed features was trained using ridge regression. Encoding models were built using the OPT language model  (three sizes - 125M, 1.3B, 13B) and the 6.9B parameter deduped Pythia language model . To study model training, 9 different Pythia model checkpoints were used (at 1K, 2K, 3K, 4K, 8K, 16K, 32K, 64K, and 143K training steps). Model details are summarized in Table E.1.

### Dimensionality of neural manifolds

To measure representational complexity, we compute the _intrinsic dimensionality_\(I_{d}\) as well as the linear _effective dimensionality_\(d\) of activations at each layer. \(I_{d}\) and \(d\) describe different geometric properties of the representations: while the former is the dimension of the representations' underlying (nonlinear) manifold, the latter describes the number of linear directions that explain their variance up to a threshold. We will use _dimensionality_ to refer to both \(I_{d}\) and \(d\), specifying when necessary.

We are interested in an LLM's behavior on a representative sample of natural language, so that the computed dimensionality is informative about the model's general linguistic processing. For all models, we compute the ID on \(N=10000\) 20-word contexts randomly sampled from Pythia's training data,1 The Pile , over 5 random data samples. The model's tokenization scheme can produce sequences of variable length, so we aggregated representations at each layer by taking the last

   & OPT-125M & OPT-1.3B & OPT-13B & Pythia-6.9B \\  GRIDE \(I_{d}\) & 0.91 & **0.96** & 0.85 & **0.90** \\ PCA \(d\) & 0.91 & 0.93 & **0.96** & 0.86 \\ PR \(d\) & **0.94** & 0.82 & 0.85 & \(-0.05^{*}\) \\  

Table 1: The average voxelwise product-moment correlations between representational dimensionality and encoding performance are shown for \(I_{d}\), PCA-\(d\) (variance threshold of \(0.99\)), and PR-\(d\). Across models, the correlation is generally high no matter the dimensionality measure. All values, except those marked with (*), are significant to \(p<10^{-3}\), as computed by a permutation test.

token representation in the model's _residual stream_; this yields one \(N D\) matrix of activations per layer, \(D\) being the model's hidden dimension, or extrinsic dimension.

Nonlinear ID estimationTo compute \(I_{d}\), we apply the Generalized Ratios Intrinsic Dimension Estimator (GRIDE) , an extension of the popular TwoNN estimator  to general scales. GRIDE operates on ratios \(_{i,2k,k}:=r_{i,2k}/r_{i,k}\), where \(r_{i,j}\) is the Euclidean distance between point \(i\) and its \(j^{th}\) neighbor. Assuming local uniform density up to the \(2k^{th}\) neighbor, the ratios \(_{i,2k,k}\) follow a generalized Pareto distribution \(f_{_{i},2k,k}()=(^{I_{d}}-1)^{k-1}}{B(k,k)^{I_{d}(2k-1) +1}}\), where \(B(,)\) is the beta function. The \(I_{d}\) is then recovered by maximizing this likelihood over points \(i\) for several candidate scales \(k\).

Finally, in order to choose the proper \(I_{d}\), a scale analysis over \(k\), which controls the neighborhood size, is necessary: if \(k\) is too small, the \(I_{d}\) likely describes local noise, and if \(k\) is too large, the curvature of the manifold will produce a faulty estimate. Instead, it is recommended to choose a \(k\) for which the \(I_{d}\) is stable . We provide an example of such a scale analysis in Appendix B.

Linear dimensionality estimationIn addition to nonlinear \(I_{d}\), we computed linear effective dimensionality \(d\) two ways: using PCA with variance cutoff \(0.99\), and the Participation Ratio (PR), defined as \((_{i}_{i})^{2}/(_{i}_{i}^{2})\). By definition, \(I_{d}\)-dimensional manifolds can be embedded in \(d I_{d}\) dimensions, so we expect that \(d I_{d}\).

Figure 1: _Analyzing Layerwise Representational Trends_: **(a)**\(I_{d}\) is well correlated with encoding performance across model sizes. \(I_{d}\) is normalized here by the \(\) of embedding size to account for power law scaling. **(b)** The abstract-predict phase transition at layer 17 is shown for OPT-1.3B. At the peak of encoding performance (red dashed line), the next-token prediction loss (blue curve) sharply decreases, corresponding with a decrease in encoding performance. **(c)** A flatmap of the brain, for one subject, is shown colored voxelwise by the correlation over layers between \(I_{d}\) and encoding performance. With the exception of auditory cortex (bright), which captures low-level spectral information, encoding performance in brain regions thought to perform higher-level linguistic processing (dark) is well-captured by representational \(I_{d}\).**(d)** The layer-wise representational similarity computed with linear CKA is shown for OPT-1.3B.

### Measuring layerwise surprisal

To determine whether predictive coding explains brain-LLM representational similarity, on The Pile, we computed the next token's surprisal from intermediate layers using TunedLens . TunedLens learns an affine mapping from an intermediate layer to the vocabulary space in order to predict the next token, indicating how much intermediate layers (linearly) represent next-token identity. See Appendix C for implementation details.

## 3 Results

Layerwise encoding performance and representational dimensionality, linear and nonlinear, are highly correlated across brain areas involved in linguistic processing. Table 1 shows the correlation between encoding performance and dimensionality averaged over all voxels, and Figure 0(a) shows the correlation between average encoding performance and (normalized) \(I_{d}\) for OPT models. The positive relationship, \(=0.85\), between \(I_{d}\) and encoding performance suggests that in trained models, the \(I_{d}\) of layer activations captures linguistic feature complexity needed to support language comprehension.

Figure 0(b) overlays, for OPT-1.3B, the encoding performance, \(I_{d}\), and next-token prediction loss computed from each layer. Encoding performance peaks at layer 17, which exactly marks a sharp downwards turn in prediction loss. While Cheng et al.  showed pre-\(I_{d}\)-peak layers to extract syntactic and semantic features, our results also suggest a functional shift post-\(I_{d}\)-peak to next-token prediction. The sharp transition from abstraction to prediction is observed across OPT model sizes, but it is more gradual for Pythia (see Appendix F.1). To further evidence a transition in layer function, we report inter-layer representational similarity via linear Centered Kernel Alignment . In Figure 0(d), where lighter is more similar, the \(I_{d}\) peak approximately marks a point where preceding layers are no longer similar to following ones. Results hold across models, see Appendix F.3.

Figure 0(c) shows, for one subject, the voxelwise correlation of \(I_{d}\) with encoding performance across layers (dark red is better). Except for the primary auditory cortex, which processes low-level auditory information, encoding performance in brain areas thought to handle higher-level linguistic processing is well-predicted by \(I_{d}\). Results hold across subjects and models, see Appendix F.2.

The relationship between encoding performance and \(I_{d}\) arises nontrivially from learning. Figure 2 plots the encoding performance (left) and \(I_{d}\) (right) across layers over the course of training for Pythia-6.9B (each curve is a different checkpoint). We confirm an existing result from the literature that the \(I_{d}\) peak emerges and that \(I_{d}\) generally grows for all layers over training (Figure 2 right) . Furthermore, encoding performance and \(I_{d}\) increase at similar rates over training, seen by similar positions of the checkpoint curves in the two plots. The two plots are globally correlated with \(=0.94\). Lastly, the location of the \(I_{d}\) peak (red dots, right), changes over training, eventually settling at the same layers for peak encoding performance (red dots, left). This rules out that the \(I_{d}\) peak trivially reflects the Transformer architecture, e.g., layer index.

Figure 2: _Encoding Performance and Intrinsic Dimensionality Peaks Manifest Concurrently over Training_: **(a)** - The evolution of layerwise encoding performance over training of the Pythia 6.9B model is shown. A peak is reached at layer 13 of the model. **(b)** -Likewise, a peak in \(I_{d}\) at layer 13 manifests over training. Red dots in each figure denote maximal layers for the respective metric.

Discussion

Recent studies of the properties of language encoding models have observed that the intermediate layers of LLMs, rather than the output layers, have the highest linear similarity to measured brain activity. This is true regardless of the scanning modality (be it fMRI , ECoG , or MEG ), and regardless of the chosen LLM. Despite this very frequently observed trend, little research has been dedicated to explaining this phenomenon. Yet, an understanding of this trend would greatly benefit our understanding of both brains and LLMs, not least because layerwise differences in LLMs have highly useful epistemic properties. LLM layers are invariant to many confounding variables - each layer has seen the same data in the same order, has an identical architecture, was trained on the same loss term, and built using the same hyperparameters. Therefore, differences between layers can only arise either as a result of the compositional nature of the transition from earlier layers to later ones, or due to the "time pressure" exerted by the loss term on the final output layers.

These competing pressures, to first build up the most comprehensive representation of the input text possible, and to then ultimately use this representation to resolve towards a distribution over predicted next word outputs, have opposite effects, as we demonstrate here. The composition effect leads to a increase in encoding performance and dimensionality, whereas the prediction effect narrows the dimensionality to the detriment of encoding. Furthermore, we observe that as models get larger and more thoroughly trained, the best layer for encoding slowly drifts to earlier in the model, perhaps suggesting a saturation effect for this initial compositional phase.

What conclusions should we draw from this? Firstly, that it is not likely to be the autoregressive nature of language models that drives brain-model similarity [9; 1; 10]. As models get more potent at prediction, their most predictive and most descriptive layers drift apart.2 Secondly, we can draw that the multi-phase abstraction process in LLMs that has been proposed independently by other authors [12; 11] is supported by evidence from the only other system known to effectively reason with complex language, the human brain. As the present work only tests two model families, it will be necessary to test more models for conclusions to hold in the general case.

From a practical perspective, conclusions point to a potential new avenue for improving the performance of encoding models. If the spectral properties of different LLM layers can be measured and efficiently combined to produce a representation with higher \(I_{d}\) than any individual layer, then we might expect that new representation to outperform any single layerwise encoding model coming from the same LLM. As linear layerwise encoding models reach their limit, such methods may be necessary to see further benefits.