# MoEUT: Mixture-of-Experts Universal Transformers

Robert Csordas\({}^{1,2}\) Kazuki Irie\({}^{3}\) Jurgen Schmidhuber\({}^{2,4}\)

Christopher Potts\({}^{1}\) Christopher D. Manning\({}^{1}\)

\({}^{1}\)Stanford University, Stanford, CA, USA

\({}^{2}\)The Swiss AI Lab IDSIA, USI & SUPSI, Lugano, Switzerland

\({}^{3}\)Center for Brain Science, Harvard University, Cambridge, MA, USA

\({}^{4}\)AI Initiative, KAUST, Thuwal, Saudi Arabia

{rcsordas,cgpotts,manning}@stanford.edu

kirie@fas.harvard.edu,juergen@idsia.ch

Work started at IDSIA.

###### Abstract

Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced "moot"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.1

## 1 Introduction

Transformers [1; 2] are ubiquitous neural architectures in modern machine learning. They power large language models [3; 4; 5; 6; 7], modern image processors , offline reinforcement learning agents , and many others. Despite these successes, we should ask whether more optimal architectures exist.

One important candidate is the Universal Transformer (UT, ). The core characteristic of UTs is _recurrence in depth_ via _sharing parameters across layers_. This reintroduces the expressive power of recurrence provided by recurrent neural networks (RNNs, [11; 12; 13]). Layer sharing allows UTs to outperform regular Transformers on compositional problems such as logical inference tasks, while also yielding improvements on _small-scale_ language modeling and translation tasks. In particular, UTs have been shown to have better compositional generalization properties [14; 15] by being able to decompose structured problems without supervision and generalize to longer sequences .2 These empirical findings confirm that UTs are more general architectures with superiorgeneralization properties compared to standard Transformers, in principle. However, UTs suffer from a fundamental problem of _parameter-compute ratio_: sharing the parameters among \(L\) layers of an \(L\)-layer Transformer--while keeping the same model dimensionalities--results in a model with \(L\) times fewer parameters (ignoring the input/output layers to simplify the discussion). Upscaling the size of the layer to compensate for the loss of parameters (essentially by making it \(L\) times wider) usually yields a very big layer whose computational requirements in terms of compute and memory are prohibitive in practice [19; 20]. In sum, despite their potential, UTs are much less compute-efficient than standard Transformers, and thus, they are not popular for parameter-dominated tasks such as modern language modeling. Indeed, we are not aware of any previous work that has succeeded in developing compute-efficient UT models that yield competitive performance compared to standard Transformers on such tasks.

Here we bring new perspectives and a solution to UTs' fundamental compute-parameter ratio problem. We present Mixture-of-Experts Universal Transformers (MoEUTs, pronounced "moot"), a mixture-of-experts (MoE) architecture [21; 22; 23] for UTs enabling them to scale in a computationally and memory efficient way. We leverage various recent advances in MoEs for both feedforward and self-attention layers (Sec. 2.1 and 2.2), and combine them with two new innovations: (1) _layer grouping_, in which we recurrently stack groups of MoE-based layers, and (2) a _peri-layernorm_ scheme (which is "in-between" the standard pre- and post-layernorm), in which we apply layer norm only before linear layers that immediately precede sigmoid or softmax activations. Both are specifically designed for shared-layer MoE architectures, and strongly supported by empirical evidence.

MoEUTs allow us to build parameter- and resource-efficient UT language models outperforming standard Transformers with less compute and memory requirements on all scales on which we can afford to test (up to 1B parameters). We demonstrate their capabilities on the C4, SlimPajama, and peS2o language modeling datasets, as well as on The Stack code generation. Our experiments show that recurrence is essential for our models to achieve competitive performance. We also demonstrate good zero-shot performance on downstream tasks like BLiMP and Children's Book Test, Lambada, HellaSwag, PIQA and ARC-E.

## 2 The MoEUT Architecture

Our MoEUT architecture is a Transformer architecture with shared layer parameters, in which we address the parameter-compute ratio problem by using mixture-of-experts. While there are many recent works on MoE methods for Transformer language models (e.g., [24; 25; 26; 27; 28]), making them competitive against their dense counterparts in _parameter-equal_ comparisons is known to be challenging . Here we leverage recent advances in MoE methods for both the feedforward network block (FFN, or simply MLP layer or feedforward layer; Sec. 2.1) and the self-attention layer (Sec. 2.2) together with two novel methods that take into account the specific properties of shared-layer models, namely: layer grouping (Sec. 2.3) and signal propagation (Sec. 2.4), which, taken together, are crucial for achieving effective shared-layer MoE Transformers.

### MoE Feedforward Blocks

To parameterize the feedforward blocks of our shared-layer Transformers by an MoE, we use \(\)-MoE  with a few modifications. \(\)-MoE divides the feedforward block into \(N_{E}\) slices, called _experts_. Each expert has two sets of weights, \(_{1}^{e}^{d_{} d_{}}\) and \(_{2}^{e}^{d_{} d_{}}\), where \(e\{1,,N_{E}\}\) is the index of the expert. At each token position \(t\), given layer input \(_{t}^{d_{}}\), the MoE feedforward layer computes a score for each expert, yielding a vector \(^{N_{E}}\) computed as:

\[_{t}=(_{t}_{S})\] (1)

where \(_{S}^{d_{} N_{E}}\) is a trainable weight matrix, and \((x)=}\) is the element-wise sigmoid function. The MoE layer only selects \(K\) experts (out of \(N_{E}\)) corresponding to the top-\(K\) elements in \(_{t}^{N_{E}}\) to produce the layer output \(_{t}^{d_{}}\) as follows:

\[(_{t}) =(_{t},K)\{1,,N_{E}\}\] (2) \[_{t} =_{e(_{t})}_{t}[e](_{t}_{1}^{e})_{2}^{e}\] (3)where \(_{t}[e]\) is the \(e\)-th element of vector \(_{t}^{N_{E}}\). Our preliminary experiments revealed that the original regularization of \(\)-MoE tends to be unstable and sometimes causes loss explosion during training. To avoid this, we apply regularization only within the sequence (as opposed to all tokens in the batch). For a sequence of inputs \(_{t}\), \(t\{1,,T\}\) we compute the balancing loss \(L\) as:

\[L=_{e=1}^{N_{E}}[e][e],\ \ \ \ \ =_{t=1}^{T} (_{t}_{S})^{N_{E}}\] (4)

The loss is scaled with coefficient \(\) and added to the standard cross entropy loss. Unlike the original \(\)-MoE, no expert dropout is used in our experiments. It is important to note that, in contrast to the standard setup in the MoE literature, our experts are small (\(d_{}=128\), similarly to \(\)-MoE ), and there are 100s of them. This configuration is called _fine-grained_ mixture-of-experts  and is also advocated by Dai et al. . We analyze the effect of \(d_{}\) in Fig. 13 in the appendix.

### MoE Self-Attention Layers

To introduce MoE to the self-attention layers, we apply SwitchHead , which is an MoE method extending \(\)-MoE to attention layers. As in the standard multi-head attention layer, each head in the SwitchHead layer contains four transformations: query, key, value, and output projections. However, SwitchHead parameterizes the value and output projections using MoEs. That is, each head has one query and key projection associated with it and \(N_{A}\) value and output projections, which are chosen dynamically for each input. Keys and queries are computed "as usual": given an input at position \(t\), \(_{t}^{d_{}}\), \(_{t}^{h}=_{t}_{K}^{h}\) and \(_{t}^{h}=_{t}_{Q}^{h}\), where \(_{K}^{h}\) and \(_{Q}^{h}^{d_{} d_{}}\), where \(h\{1,,H\}\) is the head index. The expert selection for the values is computed as follows:

\[_{V,t}^{h} =(_{t}_{SV}^{h})^{N_{A}}\] (5) \[_{V}^{h}(_{t}) =(_{V,t}^{h},K_{A})\{1, ,N_{A}\}\] (6)

where \(_{SV}^{h}^{d_{} N_{A}}\) is the selection weight for the value and \(K_{A}\) is the number of simultaneously active experts per head, set to \(K_{A}=2\) in all of our experiments. The selection for the values and outputs are independent. The selection of the output is computed analogously using a different weight matrix \(_{SO}^{h}^{d_{} N_{A}}\): \(_{O,t}^{h}=(_{t}_{SO}^{h})^{N_{A}}\) and \(_{O}^{h}(_{t})=(_{O,t}^{h},K_{A })\{1,,N_{A}\}\). Then the output \(^{d_{}}\) is calculated as follows:

\[_{t}^{h} =_{e_{V}^{h}(_{t})}_{V,t}^{h}[e] {x}_{t}_{V}^{h,e}^{d_{}}\] (7) \[_{t}^{h} =(_{t}^{h},_{t}^{h},_{t} ^{h})^{T}\] (8) \[_{t} =_{h=1}^{H}_{e_{O}^{h}(_{t})}_ {O,t}^{h}[e]_{t}^{h}_{t}^{h}_{O}^{h,e}\] (9)

where \(_{V}^{h,e}^{d_{} d_{}}\) and \(_{O}^{h,e}^{d_{} d_{}}\) are head \(h\), expert \(e\), weight matrices for value and output respectively; \(_{V,t}^{h}[e],_{O,t}^{h}[e]\) are scores of expert \(e\) for head \(h\) at position \(t\) for value and output MoE respectively; and Attention denotes the standard softmax scaled dot attention  with \(_{t}^{h}=(_{1}^{h},,_{t}^{h}),_{t}^{h}=(_{1 }^{h},,_{t}^{h})^{T d_{}}\) e.g., for the auto-regressive setting. Note that, here we describe position-wise computations for clarity but in practice, they can be parallelized over the tokens through matrix operations. Unlike in the original SwitchHead, which uses no regularization, we apply the same entropy regularization we use in the feedforward layer (Eq. 4) with a regularization coefficient \(\). (The same value is used for both value and output.)

### Layer Grouping: MoE-efficient Layer Sharing & Sub-operations within an Operation

Even when using the two recent MoE methods above, which have been shown to be successful for the standard Transformer (Sec. 2.1 and 2.2), we experimentally observe that naive MoE-based UTs with a _single_ shared layer often struggle to achieve good performance at larger scales. We hypothesize that the reason is twofold. First, as the network scales, the number of experts in the layer grows rapidly, but we cannot increase the number of active experts \(K\) at the same rate without greatly increasing the required compute. This forces us to reduce the percentage of active experts, which is generally detrimental. Second, the total number of attention heads is kept relatively low, which might not be sufficient for a large model. Increasing their number is similarly prohibitively expensive.

Our solution to these problems is to stack multiple layers with _non-shared_ weights to form what we call a _group_ of layers, reducing the number of experts in each \(\)-MoE while increasing the total number of attention heads. The final network is obtained by recurrently stacking such _groups_ that share the same parameters (in a sense, redefining the _group_ as a shared "layer" in the UT). Fig. 3 provides an illustration; here, all layers denoted by "Layer A" (or "B" respectively) share the same parameters across the entire network. The size of the group, \(G\), is the number of non-shared layers in it. In our experiments, the group size is between 2 and 4, and the typical number of recurrent steps is 8 or 9.

As further observations in favor of the potential inductive bias introduced by such grouping, note that in a seminal work, Olsson et al.  reverse engineer one of the main mechanisms behind in-context learning: induction heads. They find that two successive layers where the attention performs different operations in each layer are required. Furthermore, Csordas et al.  also show that their shared-layer Transformers use two consecutive layers to perform a _single_ operation for relatively complex synthetic tasks, such as ListOps. Both of these observations indicate that the adjacent layers in Transformers often perform different sub-operations for a _single_ high-level step of computation that spans multiple layers. This is well aligned with our proposed grouping.

### Novel LayerNorm Scheme for Improved Signal Propagation in Universal Transformers

Virtually all modern Transformers make use of the so-called "pre-layernorm" scheme [33; 34] (as opposed to the "post-layernorm" one), that is, layer normalization  is applied before the attention layer (or analogously, the feedforward block), and their output is directly added to the residual. The residual is normalized only before the final classification layer. This design encourages better gradient flow and is often crucial for training deep models. This indicates that the norm of the residual vector should grow as we go deeper in the network (see Fig. 3 for an illustration). However, it is typically assumed that the information is carried in the _direction_ of the residual vector instead of its length [36; 37]. Because of this, late layers must learn to produce outputs with a larger norm so that they can apply the same order of modification to the residual as the earlier ones, despite having normalized inputs because of the layernorm.

These learning targets are easily achieved by standard Transformers, as they have separate parameters which can have different scalings in different layers, and this can be observed empirically (for more details, see Appendix A.2). This is not the case for UTs as they have a single, shared layer (or in our case multiple, repeated layers; see Sec. 2.3). If some circuits should be (re-)used in both early and late layers, scaling their output to compensate for the norm growth of the residual is nontrivial.

Post-layernorm does not have this problem, since the whole residual is normalized after each layer. This coincides with the observation of Tan et al.  that post-layernorm performs better for UTs than pre-layernorm, and with the fact that the original UT  is trained with post-layernorm. That said, as mentioned above, post-layernorm also has its own limitation in terms of gradient flow .

Here we propose an alternative method to avoid the aforementioned problems: we do not use layernorms in the "main data path". This means, for our UTs, that we apply no layernorm before the value projection of the attention and no layernorm before the \(\)-MoE layer. Rather, layernorm is used only before linear layers that are immediately followed by a sigmoid or softmax activation function (producing renormalized activations that are critical before these nonlinear layers), namely: the query and key projections in the attention, the expert selection on both the attention and feedforward layers, and before the final classification layer. This is illustrated in Fig. 3. Since only a ReLU activation function is used on the main data path inside the feedforward layer, the output updates will be proportional to the input, thus effectively solving the residual growth issue while also providing efficient gradient flow paths. We call this the "peri-layernorm" scheme as a scheme "between" pre- and post-layernorm, which positions layernorm "around" (but not on) the residual connections.

## 3 Main Experimental Results

We present our main experimental results on the performance and efficiency of MoEUT on language modeling using the popular C4 dataset . To demonstrate the versatility of our model, we also show our main results on the SlimPajama  and peS2o  language modeling datasets, and code generation on "The Stack" . For experimental evidence in support of the benefits of shared layers for compositional generalization, we refer to much previous work (e.g., [10; 15; 14; 16; 38]). Following prior work [27; 31], we measure the compute requirements in terms of the number of multiply-accumulate (MAC) operations needed in the forward pass.

Because our models are fully MoE, they decouple the number of parameters, compute and memory requirements, and different model dimensions such as \(d_{}\) and \(d_{}\), number of layers. Thus, they provide greater flexibility for model designers. We follow a simple procedure for setting the model's hyperparameters, as described below. All our models use RoPE positional encodings  with PyTorch's fast attention implementation. The baseline models are pre-layernorm Transformers. For each baseline, we construct a _parameter-matched_ MoEUT model. We set \(d_{}\) and the number of layers \(n_{}\) to be the same as for the dense baseline. We use the same tokenization for each model trained on the same dataset. The number of heads \(H\) for MoEUT is set to \(H\) of the corresponding dense model, \(d_{}\) is set to \(2d_{}\) of the corresponding dense model, and we set \(K_{A}=2\). This matches the number of MACs spent for the value and output projections in self-attention, and reduces the number of MACs spent on calculating keys and queries and the attention matrices itself. For the \(\)-MoE layers, we set the expert size \(d_{}=128\), and \(K=2d_{}/d_{}\). This halves the MAC requirements compared to the dense counterpart. We set the number of experts in the feedforward block, \(N_{E}\), and the number of attention experts, \(N_{A}\) such that the number of parameters is the same as for the dense baseline, and \(10{-}15\%\) of the model's parameter budget (excluding the embedding and classification layers) is spent in the attention computations. We set the group size \(G\) to 2 for all our models below 300M parameters, \(G=3\) for our 319M parameter model, and \(G=4\) for the bigger models. This helps keep the number of experts manageable and improves both the performance and the speed of the model. All models are trained with batch size 64 and context length 1024, for \(10^{5}\) steps. This protocol allows us to perform fair comparisons between different models within our

Figure 4: Scaling of different models on C4 (with perplexity measured on a held-out subset of C4). (a) MoEUT slightly outperforms parameter-matched models with no layer sharing. The gap grows with scale. (b) Given equal amounts of compute, MoEUT outperforms other models by a large margin.

computational budget, and it leads to high quality models, as measured by our benchmarks. For more details, see Appendix A.4.

Scaling compared to standard Transformers.Our main scaling results are shown in Fig. 4. The y-axis shows the perplexity on a held-out subset of C4. The plot shows that our MoEUT model slightly outperforms dense models with the same number of parameters (Fig. 3(a)), and the gap tends to grow with scale. Additionally, we compare to the non-shared \(\)-MoE model . This \(\)-MoE baseline has the same shape of feedforward layers (\(d_{}\), \(K\), \(d_{}\)) to our layer-shared MoEUT, but uses no attention experts to keep the proportion of the attention weights as close MoEUT as possible, and it also uses our peri-layernorm scheme (Sec. 2.4). We add this baseline as the model that is as close to our shared-layer model as possible. This model performs significantly worse than MoEUT, demonstrating the clear advantage of the shared layers. Additionally, Fig. 3(b) shows that in terms of the number of total MAC operations spent on all forward passes during training, MoEUT outperforms the baseline dense model by a large margin.

Performance on code generation.To confirm the effectiveness of our model on a different task domain, here we train it on a subset of the "The Stack" dataset  which is a code generation task. As we cannot afford a full epoch of training, we limit ourselves to a few languages only. We use a mixture of diverse languages: Python, HTML, C++, Rust, JavaScript, Haskell, Scala, and assembly. We evaluate our models on a held-out subset of the dataset. The results are shown in Fig. 6, and they are in line with our findings on the natural language domain: MoEUT outperforms the baseline.

Zero-shot performance on downstream tasks.Here we evaluate the zero-shot performance of our models on six different downstream tasks: LAMBADA , BLiMP , Children's Book Test (CBT) , HellaSwag , PIQA , and ARC-E . For LAMBADA, we use the detokenized version from OpenAI, and we evaluate the top-1 accuracy of the last word (it can span multiple tokens; here we use greedy decoding). For CBT and BLiMP, we measure the accuracy for each task and report the average of the tasks' accuracies. The results are shown in Tab. 1. We observe that our models and the baselines typically perform very similarly. MoEUT often outperforms the baseline, but the differences are marginal in all cases. This confirms that our models are indeed very capable compared to standard language models. We confirm this on peS2o and SlimPajama as well.

Comparing with SUT.Here we compare our MoEUT to another baseline, Sparse Universal Transformer (SUT; ), which is a recently proposed UT model that also makes use of MoE layers. We note that SUTs have not been evaluated previously on standard language modeling tasks. While both MoEUT and SUT make use of an MoE for both feedforward and attention layers, there are several technical differences at various levels between the two methods: SUT uses competitive expert selection (softmax), multiple load balancing losses, and much bigger expert sizes. Their model is post-layernorm and does not use layer grouping. Unlike ours, Adaptive Computation Time (ACT) is used in the layer dimension.

We took the original code released by Tan et al.  and ported it to our training pipeline for a fair comparison. As for MoEUT, we roughly match the model's dimensionalities and number of active channels to our dense baselines. We ran a hyperparameter optimization for the regularization losses,and we found that a minimal regularization is necessary for stabilizing the training. However, larger regularization tends to hurt performance significantly. All other hyperparameters are set based on Tan et al.'s biggest translation experiments. The results are shown in Fig. 7. Effectively, SUTs, which lack our specific methods, have a significant performance disadvantage compared to our MoEUT and the parameter-matched dense baseline. Upon careful investigation, we found that most of this poor performance comes from the ACT mechanism that the authors advertise as one of the main components of their model. After removing the ACT, the performance improves dramatically. However, even with this setup, it underperforms both MoEUT and the standard Transformer baseline. This is also confirmed on downstream tasks in Tab. 2 in the appendix. Moreover, as we show in Appendix A.7, our model runs much faster and uses only a fraction of the memory required for the SUT. To the best of our knowledge, we are not aware of any prior UT architectures that are both competitive and efficient in language modeling.

Evaluating layer grouping.We investigate the effect of the layer grouping (Sec. 2.3) on our 244M parameter MoEUT model in Fig. 6. Here, \(G\) denotes the number of non-shared layers within the group. \(G=2\) corresponds to the model used in all other analyses. \(G=1\) is a fully shared-layer model, without any grouping, and \(G=18\) corresponds to the baseline fully _non-shared_\(\)-MoE model . All hyperparameters are identical among all models, except for the number of MLP experts (\(N_{E}\)) and attention experts (\(N_{A}\)), which are adjusted to match the parameter count of the dense baseline. In Fig. 6, we observe that \(G=2\) is optimal, and the recurrence in the layer dimension is indeed beneficial. Another interesting question is whether the grouping described in Sec. 2.3 and Fig. 1 is the right way to stack layers. Let us call the two layers in the group A and B. The grouping we discussed so far stacks layers in the form of "ABABAB", e.g., for a 6-layer network. An alternative is to first repeat one of the layers multiple times, followed by the repeated version of the other: "AAABBB". The "AABB" column of Fig. 6 shows this setup for our best \(G=2\) model. It can be seen that the grouping proposed in Sec. 2.3 indeed works significantly better. In fact, the AABB-style stacking is almost as bad as not doing grouping at all.

Evaluating layernorm schemes.Here we evaluate our "peri-layernorm" scheme (Sec. 2.4). Fig. 8 shows the results. The proposed layernorm scheme consistently performs the best. The gap is more significant for the small models, while for the bigger ones the gains diminish (for the 719M-parameter

   Dataset & \#params & Model & PPL \(\) & LAMBADA \(\) & BLMP \(\) & CBT \(\) & HellaSwag \(\) & PIQA \(\) & ARC-E \(\) & Average \(\) \\   &  & Baseline & 18.97 & 21.9\% & 73.5\% & 81.3\% & 28.3\% & 59.9\% & 31.7\% & 49.4\% \\  & & MoEUT & **18.30** & **23.2\%** & **78.2\%** & **81.1\%** & **29.2\%** & **61.3\%** & **33.5\%** & **51.1\%** \\   &  & Baseline & 14.97 & **28.5\%** & 77.0\% & **84.4\%** & 31.7\% & 62.7\% & 35.2\% & 53.2\% \\  & & MoEUT & **14.76** & 27.2\% & **79.4\%** & 84.2\% & **32.3\%** & **64.4\%** & **35.3\%** & **53.8\%** \\   &  & Baseline & 13.40 & **33.1\%** & 78.5\% & **86.0\%** & 34.5\% & 64.9\% & **36.9\%** & **55.6\%** \\  & & MoEUT & **13.24** & 30.6\% & **79.7\%** & 85.3\% & **35.7\%** & **65.2\%** & 36.4\% & 55.5\% \\   &  & Baseline & 12.81 & **33.3\%** & 78.5\% & **87.2\%** & 36.1\% & **67.1\%** & 37.2\% & **56.6\%** \\  & & MoEUT & **12.65** & 30.8\% & **80.2\%** & 86.9\% & **37.3\%** & 67.0\% & **37.3\%** & **56.6\%** \\   &  & Baseline & 11.59 & **37.8\%** & 80.7\% & 88.2\% & 40.5\% & 67.7\% & 39.3\% & 59.0\% \\  & & MoEUT & **11.34** & 36.0\% & **80.8\%** & **88.4\%** & **41.8\%** & **69.2\%** & **39.6\%** & **59.3\%** \\   &  & Baseline & 11.15 & **38.4\%** & 81.2\% & 89.0\% & 42.0\% & 68.6\% & 39.7\% & 59.8\% \\  & & MoEUT & **10.90** & **38.4\%** & 81.6\% & **89.2\%** & **43.7\%** & **69.9\%** & **41.3\%** & **60.7\%** \\   &  & Baseline & 11.46 & **13.2\%** & 66.5\% & 68.6\% & **28.5\%** & **56.3\%** & **32.0\%** & 44.2\% \\  & & MoEUT & **11.09** & 13.1\% & **68.7\%** & **69.6\%** & 28.3\% & 55.1\% & 31.4\% & **44.4\%** \\   &  & Baseline & 8.55 & 18.7\% & 72.8\% & **78.0\%** & **30.4\%** & **56.3\%** & 35.0\% & 48.5\% \\  & & MoEUT & **8.52** & **19.4\%** & **73.5\%** & 77.4\% & 30.1\% & **56.3\%** & **35.6\%** & **48.7\%** \\   &  & Baseline & 16.42 & **20.0\%** & 72.8\% & 80.7\% & 27.5\% & 57.0\% & 31.6\% & 48.3\% \\  & & MoEUT & **15.77** & 19.8\% & **75.9\%** & **82.1\%** & **28.0\%** & **57.5\%** & **32.1\%** & **49.2\%** \\   &  & Baseline & 11.51 & **31.9\%** & 78.6\% & **87.3\%** & 31.7\% & 60.9\% & **36.6\%** & **54.5\%** \\   &  & Baseline & **11.47** & 30.7\% & **80.2\%** & 86.8\% & **32.0\%** & **61.7\%** & 35.8\% & **54.5\%** \\   &  & Baseline & 9.56 & **38.8\%** & 80.5\% & 89.9\% & 37.6\% & 64.5\% & 38.7\% & 58.3\% \\   &  & Baseline & **9.36** & 38.0\% & **82.5\%** & **90.2\%** & **38.1\%** & **64.6\%** & **39.1\%** & **58.7\%** \\   

Table 1: Zero-shot downstream performance and perplexity on various language modeling datasets. MoEUT marginally outperforms standard Transformers in most tasks, confirming that MoEUT is indeed a capable language model.

model, the gap between peri-norm and post-norm is marginal: 11.29 perplexity points compared to 11.32). At the same time, we also observe that the gap between peri-norm and post-norm increases with the number of training steps, leaving open the possibility of higher gains if the models are trained longer. All our MoEUT models in other experiments make use of this peri-norm scheme.

## 4 Analysis

Here we aim to better understand the learned expert selection of MoEUTs. In what follows, we analyze the expert selection in the MLP blocks (Sec. 2.1) of our 244M parameter MoEUT model trained on C4. All experiments in this section are performed by calculating statistics on the validation set of C4 for a model with \(G=2\) (i.e., two layers in the group; see Sec. 2.3). We only display behaviors of the first layer of the group, as we find that the results of the second layer are qualitatively similar.

**Expert (re)use across layers.** We first focus on whether nontrivial reuse occurs between different layers. Note that _MoE-based_ shared-layer models could, in theory, assign different experts to different layers to "emulate" regular non-shared models. If this were the case, the model would resemble a regular Transformer instead of a Universal one. To confirm that our model is more versatile than that, we analyze whether certain experts in the MLP layers are activated only in specific layers. We measure how many times each expert is activated in each layer. This allows us to visualize the distribution of layers that each expert prefers. To better visualize the structure, we reorder experts by a heuristic which we call "layer position score" defined as the average of the layer indices weighted by the number of expert activations in that layer. Fig. 9 shows the results. The yellow spot in the bottom right corner indicates that some experts are assigned mostly to the final layer. However, for the other experts, there is a wide range of layers where the expert is activated. Experts seem to be active in a continuous sequence of layers. This can be seen in the wide, vertically lined structure. We can conclude that MoEUT is capable of specializing in a specific layer if necessary and sharing weights between them when advantageous.

**Per-token expert selection diversity.** Here we analyze the diversity of expert selection in the MLP layers for a given input token across different layers and contexts. For this we measure the total number of unique experts activated for individual tokens at different layers across different positions/contexts. The result is shown in Fig. 10. On the x-axis, the tokens are ordered differently for each layer based on the number of experts used in that layer. We display the most "specialized" 1000 tokens. The minimum possible number of active experts is 16, corresponding to \(K\). If only \(K\) experts are consistently used by a token across different contexts, it means that the token is fully "specialized" to consistently use a single set of \(K\) experts. This is almost the case for many tokens if we look at the first layer (blue curve): the number of unique experts used is low, i.e., the selection mainly depends on the token's identity. However, in the subsequent layers, the situation is quite different: the total number of experts used increases significantly, indicating that the context of the token is taken into account for the expert selection. The diversity of the experts used peaks in the middle layers (here Layer 9) and falls slightly for layers closer to the output. For the converse analysis of expert specialization to tokens/layers, we refer to Appendix A.6.

**Expert selection dynamics in individual columns/positions.** So far, all the results have been cumulative statistics in different input sequences and positions. We might wonder about the selection behavior for the _individual_ Transformer columns.3 Is expert selection mostly constant throughout the layers for individual columns of MoEUT? To answer this question, we calculate the pairwise intersection-over-union of the set of selected experts between all layers in _individual columns_ and average this metric over the whole validation set. We show the result in Fig. 11. There is a non-negligible overlap between the selected experts in subsequent layers; however, it is far from complete overlap. This indicates that experts usually change dynamically in a single column, performing different functionality in different layers.

**Overall**, our analysis suggests that MoEUT is capable of dynamically adapting its expert selection mechanism to a diverse set of circumstances. Sometimes, experts are assigned to popular tokens, while in other cases, they are shared or specialized between layers, depending on what is better for the task.

## 5 Discussion and Limitations

More background on UT.We emphasize that our focus is on developing scalable and performant Universal Transformers for parameter-dominating tasks such as language modeling. This has been a long-standing limitation of UTs. For example, Lan et al.  study shared-layer BERT models  and find that layer-sharing hurts performance in exchange for better parameter efficiency. Kaplan et al.  report that even though Transformer language models with inter-layer parameter-sharing scale better in terms of number of parameters, they fail to achieve compute-efficiency--in contrast, our MoE-based approach is more compute-efficient than the corresponding dense baseline. On the other hand, UTs have been well-known for their compositional generalization capabilities. We refer the readers to the numerous corresponding works (e.g., [16; 15; 38; 52; 14; 10]) for results supporting the benefits of layer sharing. Future work may also use our MoEUT in such compositional settings as a generally more efficient UT architecture.

MoE for Transformer language models.MoE methods for Transformer language models have seen many recent advances. It is worth noting that despite many works on MoEs for Transformers (see, e.g., [24; 25; 26; 27; 28]), many of them have only focused on applying MoE to the feedforward layers. Notable exceptions are mixture-of-attention  and SwitchHead  (Sec. 2.2), which focus on MoE self-attention layers. In addition, until recently, it has been considered challenging to make MoE-based language models competitive against the dense baseline in the parameter-matched setting (unlike FLOPs/MAC-matched settings). In MoEUT, we use \(\)-MoE , an MoE design that has been shown to be competitive even in such a setting.

Figure 11: Instance-level average expert selection similarity between layers. Individual tokens are routed to a diverse set of experts across the layers.

Figure 10: No. of unique experts used in different layers. Tokens are routed to many different experts (depending on the context), especially in the middle layers.

Figure 9: Layer preference of different experts. Most experts are used in multiple layers, while some of them (see, bottom right) specialize to certain layers, showing the flexibility of our model.

Further related works on LayerNorm and layer grouping.There are other works that are closely related to ours regarding certain aspects of our model. Regarding the signal propagation and layernorm  in Transformers (Sec. 2.4), Xie et al.  analyze the growing residual norm in standard Transformers, and propose a dual, hybrid residual stream as a remedy. Regarding layer grouping, Takase and Kiyono  study various layer grouping variants to improve the efficiency of shared-layer Transformers, also showing that layer grouping outperforms vanilla Universal Transformers. However, they consider models with large group sizes (\(G=6\) for 12 layers) and few recurrent steps (2). We find that models with smaller \(G\) with more steps perform better. Sometimes, layer grouping is also used to up-scaling pretrained models .

Limitation/Implementation.Our current implementation of the MoE layers uses the Triton kernel released with \(\)-MoE  for both the attention and the MLP parts of the model. This implementation is known to be suboptimal . Compared to the standard Transformer with FlashAttention , our MoEUT model trains 1.5-2x slower. We estimate that with a more optimal implementation, the training speed should be close to the dense model, while inference should run faster.

Massive scaling.Our experiments used a modest training regime. This led to good models and allowed us to make rigorous comparisons, but scaling to massive training regimes for MoEUT remains an important avenue for future research. Such experiments would inevitably require a very large compute cluster, but the costs could also be mitigated somewhat by work optimizing our CUDA kernel.

## 6 Conclusion

We present MoEUT, a novel Mixture-of-Expert-based Universal Transformer (UT) model that addresses the fundamental limitation of the standard UT in terms of parameter-compute efficiency. MoEUT combines the most advanced MoE techniques for Transformers with our novel layer grouping method and layernorm scheme, which are both shown to be crucial for shared-layer models. Our MoEUT allows for training competitive UTs on parameter-dominated tasks such as language modeling, while being significantly less compute intensive than the baselines without layer sharing. We break this long standing limitation of UTs for the first time. Experimentally our model outperforms dense baselines from 44M to 1B parameter scale on C4, SlimPajama, peS2o, and The Stack datasets. Zero-shot experiments confirm that the performance of MoEUT holds on downstream tasks, including BLiMP, CBT, Lambada, HellaSwag, PIQA and ARC-E. We hope that this work helps revive research interest in Universal Transformers at larger scales, and serves as a stepping stone for achieving the superior generalization properties of UTs (typically limited to synthetic problems for now) in real-world settings.