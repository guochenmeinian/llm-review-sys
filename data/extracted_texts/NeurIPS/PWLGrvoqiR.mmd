# RaLEs: a Benchmark for Radiology Language Evaluations

Juan M Zambrano Chaves

Stanford University

jmz@stanford.edu

&Nandita Bhaskhar

Stanford University

&Maayane Attias

Stanford University

Jean-Benoit Delbrouck

Stanford University

&Daniel L. Rubin

Stanford University

&Andreas Loening

Stanford University

&Curtis Langlotz

Stanford University

&Akshay S. Chaudhari

Stanford University

akshaysc@stanford.edu

###### Abstract

The radiology report is the main form of communication between radiologists and other clinicians. Prior work in natural language processing in radiology reports has shown the value of developing methods tailored for individual tasks such as identifying reports with critical results or disease detection. Meanwhile, English and biomedical natural language understanding benchmarks such as the General Language Understanding and Evaluation as well as Biomedical Language Understanding and Reasoning Benchmark have motivated the development of models that can be easily adapted to address many tasks in those domains. Here, we characterize the radiology report as a distinct domain and introduce RaLEs, the Radiology Language Evaluations, as a benchmark for natural language understanding and generation in radiology. RaLEs is comprised of six natural language understanding and generation evaluations including the extraction of anatomical and disease entities and their relations, procedure selection, and report summarization. We characterize the performance of models designed for the general, biomedical, clinical and radiology domains across these tasks. We find that advances in the general and biomedical domains do not necessarily translate to radiology, and that certain more advanced models from the general domain can perform comparably to smaller clinical-specific models. The limited performance of existing pre-trained models on RaLEs highlights the opportunity to improve domain-specific self-supervised models for natural language processing in radiology. We propose RaLEs as a benchmark to promote and track the development of such domain-specific radiology language models. RaLEs is available at https://github.com/StanfordMIMI/RaLEs.

## 1 Introduction

Radiology reports convey a radiologist's interpretation of a medical image. The reports have characteristic content and structure that differentiate them from other types of text. Natural language processing of radiology reports can aid research efforts and ultimately lead to improved quality of care. However, many recent approaches focus on solving single radiology tasks, often report performance only on private datasets, fail to compare proposed methods with relevant baselines, and do not publish code or models . Further, reports are typically not publicly available due to patient privacy concerns. The development of private, single-task models limits the measurement of progress in NLP for radiology broadly.

Model benchmarking in other domains, such as general English or biomedical text, has enabled thorough comparisons of existing methods across tasks that evaluate model performance and alignment with human evaluation [53; 20; 35]. This has promoted the development of state of the art models that can be adapted to address multiple tasks. Radiology reports are excluded from existing biomedical or clinical benchmarks, which often feature evaluations on datasets that do not reflect real-world clinical use cases. In contrast, 75% of current current FDA-approved AI applications target radiology (), with a potential for impact on over 700 million studies (and associated reports) annually . However, it is unclear how advances in other domains translate to the unique domain of radiology language and reports.

To address the aforementioned challenges, in this work we develop RaLEs, a benchmark for evaluations of natural language understanding (NLU) and generation (NLG) in the radiology domain. Our main contributions are:

1. Curating a set of 6 datasets across 4 tasks, all of which are publicly available. Among these datasets, 1 is newly created and introduced in this work (procedure selection), and 1 is newly de-identified and released into the public domain (Stanza radiology named entity recognition).
2. We benchmark and report RaLEs multi-metric performance on 16 models from the general, biomedical, clinical and radiology domains. We find that on average, clinical and radiology-specific models outperform general and biomedical models by 1.5 and 0.5% on preferred metrics.
3. We consolidate progress by developing RaLEs NLU and NLG scores, and release code for dataset standardization, model fine-tuning and evaluation to spur future benchmarking.

## 2 Related work

### Radiology natural language processing

Prior work has reviewed the status of NLP in radiology. In a systematic review, Casey et al.  identified that <20% of previously published radiology NLP methods used deep learning, with the majority relying on other machine learning or rule-based systems. A significant portion of these studies primarily concentrated on tasks like information extraction (accounting for 45%) and classification (making up 50%). Regarding reproducibility, only 14 and 15 of the 164 studies reviewed made their data and code available, respectively. Fewer than 20% of studied compared proposed methods with alternate approaches. In a separate study, Pons et al.  reported that 20 of 67 studies reported operational use, the majority of which were not intended for integration into a clinical workflow.

Most existing applications of NLP in radiology can be framed as NLU or NLG tasks. These include information extraction (named entity recognition and/or relation extraction), text classification, and text summarization [40; 38]. These methods can enable applications such as extracting labels from reports to annotate images [26; 48], automatic image protocol selection, defining patient cohorts, monitoring appropriate use and clinical follow-up of medical images, and summarizing prior imaging studies [40; 38].

### Benchmarks in other domains

Benchmarks that systematically compare the performance of existing models in the general English domain, such as GLUE or SuperGLUE [53; 52], have enabled comparisons of models across a variety of NLU tasks, promoting the development of pre-trained models that can be adapted to a variety of tasks using transfer learning or other adaptation methods. Other benchmarks have been proposed for biomedical language understanding and reasoning, such as the BLURB Benchmark , which contains 13 datasets and 6 tasks, focusing on the performance of systems across scientific biomedical text. The Biomedical Language Understanding Evaluation (BLUE) benchmark , includes 6 biomedical scientific text datasets and 4 clinical datasets evaluating sentence similarity, named entity recognition, relation extraction, document classification and inference. While comprehensive, the BLUE benchmark does not include any radiology text-based tasks. This highlights a broader issue: no existing benchmarks broadly compare the performance of models in a curated set of radiology tasks, which makes it challenging to quantify the efficacy of general-purpose language models on domain-specific radiology tasks.

Radiology reports as a domain

Three key characteristics define the domain of radiology reports: a) content, b) context and c) closed source. In terms of content, radiology reports contain a limited specialized vocabulary, often existing only in the context of images. For example, words like _pneumothorax_, _cardiomegaly_, and _radiodensity_, referring to air in the space surrounding the lungs, an enlarged heart, and opacity to X-rays, are frequently found in radiology reports. Such words are contained in Radiology Lexicon1, a comprehensive set of radiology terms that contains approximately 30,000 entries. An additional aspect of content involves structure. Radiology reports typically follow a document structure comprised of a header containing patient history and exam-related information, followed by mentions of relevant comparison studies, details about the imaging technique utilized, a detailed description of image findings, and an impression that summarizes findings contextualized to the patient's condition [30; 23]. Sentences within reports are typically short, declarative and factual, and are written in the present tense. The content (unique vocabulary, format, and narrative style) of radiology reports is an important feature of this specialized domain.

Another aspect that characterizes this domain is the context surrounding radiology reports. Reports only exist in the presence of an accompanying medical image. Typically they exist within the context of electronic medical records, which are collections of documents, images and other signals that describe a patient's medical history. The content of these reports is embedded within the current radiology and medical knowledge.

Finally, due to existing concerns and regulations that protect patient health information, the vast majority of radiology reports exist within private data warehouses, often requiring institutional review board approval for access. The largest publicly available collections of reports, such as MIMIC-III, MIMIC-CXR, PadChest, and Open-i datasets [29; 28; 4; 15], have undergone de-identification and are typically made available subject to agreement to terms of data use protecting patient privacy.

## 4 RaLEs: Radiology Language Evaluations

The following sections outline RaLEs. The tasks and datasets were selected to reflect real-world use cases, ensuring they are both challenging and achievable. We outline multiple metrics of success for well-rounded evaluation for each dataset-task pair, as well as differentiate the performance of the model in data from institutions unseen during training where available.

### Tasks and datasets

Table 1 summarizes tasks and datasets within RaLEs. We include four tasks as part of the initial RaLEs: named entity recognition (NER), relation extraction (RE), document classification and document summarization. The datasets selected for each task are described as follows.

**RadGraph** consists of 600 manually annotated chest x-ray reports from MIMIC-CXR and CheXpert datasets [28; 26]. A board-certified radiologist labeled the reports with named entities, consisting of _Observation - definitely present_, _Observation - definitely absent_, _Observation - uncertain_, and _Anatomy_, reflecting key entities in reports corresponding to observations (which may be negated or hedged) as well as anatomical structures. Pairs of entities may be related by one of three types: _suggestive of_, _located at_, and _modify_. We use this dataset to evaluate models on NER and RE. Test

  
**Category** & **Dataset** & **Task** & **\# Train/Dev/Test** & **Anatomy/Modality** & **\# Sources** & **New** \\   & RadGraph  & RE & 425 / 75 / 100 & Chest / XR & 2 & \\  & RadGraph  & NER & 425 / 75 / 100 & Chest / XR & 2 & \\  & RadSpRL  & RE & 848 / 105 / 106 & Chest / XR & 1 & \\  & Stanza Radiology  & NER & 2,461 / 200 / 295 & Chest / CT & 3 & \\  & CT Procedure Selection & Clf. & 58,091 / 19,364 / 19,364 & Varied / CT & 1 & \\   & MEDIOA 2021 & Summ. & 91,544 / 4,000 / 600 & Chest / XR & 2 & \\  & BioNLP 2023 & Summ. & 59,320 / 7,413 / 13,057 & Varied / CT, MR & 1 & \\   

* XR=X-ray, CT=Computed Tomography, MR=Magnetic Resonance Imaging, NLU=Natural Language Understanding, NLG=Natural Language Generation, New= newly released with this work, Varied=Abdomen, Pelvis, Neck, Spine, Head, etc., see A and B

Table 1: Overview of datasets and tasks in RaLEs.

set evaluations are carried out separately on MIMIC-CXR and CheXpert, reflecting in and out of distribution performance, respectively.

**RadSpRL** consists of 2000 manually annotated chest x-ray reports from the Open-i dataset . A medical librarian and an MD annotated entities and relations that represent spatial relations. Spans of text were annotated as a relationship between a _Spatial indicator_ with another span of text consisting of one of four spatial roles: _Trajector_, _Landmark_, _Hedge_, and _Diagnosis_. We evaluate models on RE with this dataset. We use only documents with labeled relations for training and evaluation. Though prior performance is reported on cross-validation sets, we create a fixed train/dev/test split on the report level to limit excessive compute requirements of hyperparameter exploration across each split for each model.

**Stanza Radiology** consists of 150 manually annotated chest computed tomography reports from three hospitals. Two radiologists annotated spans of text in the reports with five entity types: _Anatomy_, _Observation_, _Anatomy modifier_, _Observation modifier_, and _Uncertainty_. This dataset is used for NER evaluations. As part of RaLEs, we have deidentified these reports using a publicly available hidden-in-plain-sight de-identification algorithm , and release this previously private dataset to the public.

**MIMIC-III procedure selection** is a newly created dataset, released alongside this work, that consists of 96,819 documents extracted from individual computed tomography reports from the MIMIC-III dataset . The reason for exam and procedure title were extracted from each report using regular expressions. The task consists of appropriately classifying reason for exam documents into one of 46 normalized procedure titles. The procedure titles were normalized to a standardized vocabulary  by manually mapping a set of extracted procedure titles to the vocabulary. Normalization was carried out by an MD and a board-certified radiologist. Additional details of the curation of this dataset are in A. This evaluation was developed to simulate the selection of a procedure given a clinician provided reason for exam, a task that often requires expert human oversight in current practice.

**MEDIQA 2021 report summarization** consists of 96,144 chest X-ray reports with extracted _Findings_ and _Impression_ sections. The task is summarizing the _Findings_ section of reports, using the _Impression_ as ground truth. Models are trained using reports from one dataset (MIMIC-CXR) and validated using reports from MIMIC-CXR and the Open-I dataset (from Indiana). The test evaluation is carried out on reports from an institution seen during validation (Indiana), as well as an institution present only in the test set (Stanford), which aims to measure out-of-domain generalization.

**BioNLP 2023 report summarization** consists of 79,790 multi-modal reports  extracted from the MIMIC-III dataset  that are separated into _Findings_ and _Impression_ sections. The task is to create a summary in the same fashion as MEDIQA 2021. The reports are of computed tomography and magnetic resonance imaging examinations, with head, chest, abdomen, spine and sinuses present as different anatomies (Details in B. Test evaluations include anatomies/modalities unseen in training data.

### Evaluation strategy

#### 4.2.1 Models

  
**Domain** & **Model** & **\# Params** \\   & BERT &  \\  & RoBERTa &  \\  & ELECTRA &  \\  & DeBERTa-v3 &  \\   & PubMedBERT & 110 \\  & BioLinkBERT &  \\   & BioClinicalBERT & 110 \\  & GatorTron & 345 \\   & RadBERT\({}_{1}\) & 110 \\  & RadBERT\({}_{2}\) & 125 \\   

Table 2: Masked language models evaluated. Number of parameters in millions.

We evaluate various pre-trained masked language models from the general, biomedical, clinical and radiology domains. Table 2 lists the models evaluated. We refer the reader to the respective publication for details on the pretraining strategy including source of vocabulary, corpus and model-specific optimizations.

From the general English domain, where models are trained using sources such as Wikipedia and Google Books, we evaluate BERT  in its base and large configurations, RoBERTa  in its base and large configurations, ELECTRA  in its small, base and large configurations, and DeBERTa-v3  in its base and large configuration.

From the biomedical domain, we examine models pre-trained on scientific text contained in PubMed: PubMedBERT-base , which pretrains a BERT-base model using biomedical scientific abstracts and full-text articles as a corpus, and BioLinkBERT-base and large , which uses a similar pretraining corpus but incorporates an additional pretraining objective consisting of identifying document links.

From the clinical domain, we evaluate BioClinicalBERT , a BioBERT  biomedical model (i.e., a BERT-base model continually pretrained on PubMed text), which is further continually pretrained on MIMIC-III clinical notes. In addition, we examine GatorTron-base, a MegaTronBERT model  pretrained using clinical notes from the University of Florida, PubMed text and Wikipedia articles (500GB of text).

Finally, we examine the performance of models developed specifically for the radiology domain, both named RadBERT by their creators. One model, which we refer to as RadBERT\({}_{1}\), corresponds to a BioBERT model continually pretrained on 4 million radiology reports from Stanford Health Care . The second model, which we refer to as RadBERT\({}_{2}\), corresponds to a BioMed-RoBERTa (RoBERTa-base model continually pretrained on 2.7 million scientific papers  ) model further pretrained on 4.4 million radiology reports from various facilities of the U.S. Department of Veterans Affairs health system.

#### 4.2.2 NLU evaluations

We fine-tune each model using all documents in training split and perform task-specific hyperparameter optimization as detailed in C. We select the best model according to the best run preferred metric on the validation set, as defined by each dataset. We report the performance of models on test sets using the best model for each model type/task. We employ different fine-tuning strategies for each task, described as follows.

For RE, datasets (RadGraph, RadSpRL), we use DyGIE++ , a multi-task NER and RE framework that learns a dynamic graph that models relationships between text spans. Span representations are obtained from a language model embedding. We use DyGIE++ as a NER extraction method for RadGraph models as we empirically observed improved performance compared to the Stanza NER approach which cannot leverage the relation labels. For document classification, a randomly initialized head of dimensions \((h,c)\) where \(h\) corresponds to the hidden size and \(c\) corresponds to the number of classes, is added as a final layer to classify the classification [CLS] token. For Stanza NER, a randomly initialized head of dimensions \((h,c)\) is added to classify each token. For words comprised of multiple sub-word tokens, the label is assigned to the first token.

Consistent with prior evaluations for existing datasets, the preferred metric for all evaluations is the micro-averaged F1 score. For the newly created procedure selection task, we select accuracy as the preferred metric as it more closely reflects success in clinical settings. We report a RaLEs NLU score as the average of the preferred metrics across the NLU datasets. In addition, we perform label-stratified sampling evaluations using 1% and 10% labels during training and validation to assess how dataset scaling across domain-specific and domain-agnostic models affects their performance. We also examine separability of representations generated by each model by keeping language model weights frozen and training only a linear probe, or only the graph layers in the case of DyGIE++ models. Furthermore, we evaluate models in metrics that may be relevant for deployment, focusing on model calibration and uncertainty. We include in RaLEs metrics measuring calibration and uncertainty, prediction quality, and information criteria. We detail the results for the document classification (procedure selection) task on these metrics in Appendix D. Model fine-tuning is performed in private infrastructure using a single NVIDIA TITAN RTX or RTX A6000 GPU. For each model, hyperparameter exploration takes on average 1 hour for DyGIE++ models, 1 hour for Stanza NER, and 3 hours for the procedure selection task (in total approximately 400 GPU hours).

[MISSING_PAGE_FAIL:6]

obtains improvements in both in GLUE as well as in RaLEs NLU. Similarly, though BioLinkBERT and PubMedBERT share similar scientific pretraining corpora, the document relation prediction objective of BioLinkBERT leads to improved performance in both BLURB as well as RaLEs NLU.

However, improvements in performance in other benchmarks, such as GLUE or BLURB, do not directly translate to improvements in RaLEs NLU. This is illustrated in Figure 3 which shows that 10 percentage point improvements in GLUE or BLURB lead to a 1-2 percent improvement in RaLEs NLU performance. We hypothesize that the decrease in benefit stems from the domain characteristics of radiology that differentiate it from other text, as proposed in Section 3. Furthermore, we observe that existing radiology domain adaptation hurts alternate domain performance, exemplified by the relatively low BLURB score for RadBERT\({}_{1,2}\) models seen in Figure 3. Finally, we find that given a fixed architecture, an increase in parameter count (\(O\)(300M) vs \(O\)(100M)) does not lead to improved overall performance. This observations holds for both English and biomedical models, which are the only ones publicly available in different sizes. Similar results have been previously observed in other domains [20; 45], though the impact of base model size on domain adaptation is yet to be systematically studied.

### Nlg

Table 4 presents the results for the report summarization task, with prior results referenced for comparison. Similarly to NLU results, no model is consistently superior across all metrics. GatorTron, the model from the clinical domain, outperforms the other evaluated fine-tuned models slightly on lexical similarity (ROUGE-2 and ROUGE-L). Using these metrics as reference, the encoder-decoder framework as implemented here performs inferiorly to the best existing performing models. The best MEDIQA 2021 prior model , in addition to an abstractive-summarization-specific architecture, uses a domain adaptation module to improve performance on Indiana reports. However, as can be seen from our evaluation on fully held out Stanford reports in Appendix G, most of the benefits of such specialized approaches may not generalize to sites not seen during training. Further, our approach seems to favor factual correctness, with BioLinkBERT and GatorTron models having the best performance according to the CheXbert and RG metrics. RadAdapt , a label efficient adaptation of a clinical large language model, matches the overall NLG performance of our best fine-tuned model. As the RadAdapt authors recognize, however, it is unclear to what extent its

Figure 1: Model size and domain vs RaLEs NLU performance

performance is affected by possible leakage of testing data during the base model pre-training stage. We aim to further evaluate other large language models in future versions of RaLEs.

Figure 3: General (English, left) and biomedical (right) benchmark vs RaLEs NLU performance.

Figure 2: RaLEs NLU performance by model domain. Values are averaged across model domain, error bars are standard deviation. There are no statistically significant pairwise differences between model categories within the same label availability, as determined by Mann-Whitney U-tests with Bonferroni correction.

### Ethical considerations

RaLEs provides a framework for benchmarking advances in radiology NLP. The current version of RaLEs uses the RadSpRL dataset, which has a CC BY 4.0 license. Datasets stemming from MIMIC, including RadGraph, have a PhysioNetCredentialed Health Data License 1.5.0 which prohibit commercial use, data sharing and patient or institution identification attempts. For the newly released Stanza NER dataset, the dataset will be accompanied by an analogous Research Use Agreement following institutional review board approval, which was obtained to access the reports. For all datasets we have followed the appropriate research use, have not attempted re-identification of individuals, and provide instructions for data access in the accompanying code. Demographic characteristics of individuals included in RadGraph  and MIMIC-III  datasets are described in their original publications. Demographic characteristics for the Stanza NER , MEDIQA 2021  and Indiana  dataset (from which the RadSpRL dataset is derived) are not reported since the radiology reports have been de-identified and these characteristics are not otherwise available. We do not foresee any potential risks following appropriate use of RaLEs as a tool to measure progress in NLP research. We strongly discourage the use of models trained using our framework for clinical care or advice without adequately studying the performance and limitations on specific patient populations that reflect intended use.

### Limitations

An important limitation of our analysis stems from the limited availability of publicly available radiology reports and datasets. This is consistent with prior observations that healthcare algorithms trained on US patient data rely on data from a handful of states . While we release a new dataset from a new institution, future efforts should promote the availability of training and/or evaluation datasets from additional institutions, geographic locations, and languages. Aside from geographic diversity, we note that most publicly available reports and evaluations focus on chest X-ray reports which tend to be shorter and simpler than reports of other modalities and anatomies (see example lengths in Table 13). Our newly introduced procedure selection dataset expands the scope of current data by focusing on a different modality (computed tomography) across all anatomies. Finally, while we present extensive evaluations across models of different domains, we intend RaLEs to be a dynamic benchmark, with expansions across newer datasets, tasks, and model evaluations (including adding newer models and estimates of variance of performance).

## 6 Conclusion

Radiology reports are defined by their content, context and restricted access. RaLEs defines a benchmark for measuring progress in NLP in the radiology domain, focusing on multifaceted evaluations that reflect real-world use cases in radiology research and/or practice. The results showed that no single model, including existing radiology-specific domain adapted models, outperforms others across all evaluations.

    &  &  \\ 
**Model** & **R-2** & **R-L** & **CheXbert** & **RG** & **R-2** & **R-L** & **RG** & **NLG score** \\  ELECTRA\({}_{}\) &.238 &.381 &.710 &.378 &.156 &.274 &.229 &.316 \\ BioLinkBERT\({}_{}\) &.245 &.388 & **.725** &.391 &.183 &.297 &.272 &.337 \\ GatorTron &.250 &.386 &.719 & **.406** &.189 &.303 &.283 & **.345** \\ RadBERT\({}_{2}\) &.237 &.382 &.709 &.381 &.184 &.300 &.271 &.334 \\ RadAdapt\$ & **.253** & **.393** & - &.345 & **.212** & **.324** & **.342** & **.345** \\ RadiologyGPT\$ &.074 &.148 &.601 &.135 &.127 &.209 &.242 &.184 \\  Prior SOTA\(\) & **.436** & **.557** &.718 & - & - & - & - \\ Prior Baseline\$ &.264 &.389 &.610 & - & - & - & - & - \\   

* R-2/L: ROUGE 2/L, RG: F1-RadGraph, \(\):MEDIQA, \(\):from , \(@sectionsign\):our evaluation of  and 

Table 4: Summary of results for NLG tasks (abstractive report summarization).

Acknowledgements

We thank Dave Van Veen for his contribution providing results for RadAdapt summarization models. We also thank the institutions that have provided funding for this work. Research reported in this publication was made possible in part by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contracts 75N92020C0008 and 75N92020C00021, GE Healthcare, and Stanford Knight Hennessy Scholars.