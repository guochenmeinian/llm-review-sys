# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

real-time clinical domains  and has potential for novel mind reading applications in brain-computer interfaces. Previous works mapped fMRI activity to the embeddings of image generation models via relatively simple mappings, usually ridge regression [3; 4; 5]. Here we propose MindEye, a novel approach that involves mapping via large-scale multilayer perceptrons (MLPs), contrastive learning, and diffusion models to achieve state-of-the-art image reconstruction. See Figure 1 for select samples of reconstructions.1

MindEye learns to map flattened spatial patterns of fMRI activity across voxels (3-dimensional cubes of cortical tissue) to the image embedding latent space of a pretrained CLIP  model. MindEye has an MLP backbone and 2 specialized submodules for retrieval and reconstruction. The retrieval submodule is contrastively trained and produces "disjointed CLIP fMRI" embeddings that have high cosine similarity with the corresponding image embeddings but differ in magnitude. To reconstruct images, we train a diffusion prior  from scratch to take in the outputs from the MLP backbone and produce aligned embeddings suitable as inputs to any pretrained image generation model that accepts CLIP image embeddings. In order to ensure that our reconstructions also match the original images' low-level features (e.g., color, texture, spatial position), we train a separate encoder that directly maps voxels to the embedding space of Stable Diffusion's  variational autoencoder (VAE), obtaining blurry image reconstructions that lack high-level semantic content but perform state-of-the-art on low-level image metrics. Combining the high-level "semantic" pipeline with the low-level "perceptual" pipeline in an img2img  setting allows MindEye to output state-of-the-art reconstructions across both low- and high-level image metrics.

In addition to image reconstruction metrics, our disjointed CLIP fMRI embeddings attain state-of-the-art performance on image retrieval and brain retrieval metrics. Image retrieval refers to finding the original seen image out of a pool of other images given a brain sample, while brain retrieval refers to finding the brain sample given an image. MindEye finds exact (top-1) matches in the pool of NSD test samples with >90% accuracy for both image and brain retrieval, outperforming previous state-of-the-art [11; 4] which showed <50% retrieval accuracies. These results demonstrate that MindEye brain embeddings possess fine-grained exemplar-level signal.

Our main findings are: (1) Specialized submodules for retrieval (using contrastive learning) and reconstruction (using a diffusion prior) enable a single model to achieve state-of-the-art results across both tasks even though the objectives exhibit a tradeoff. (2) Mapping to a deep MLP with a parameter count orders of magnitude higher than previous methods does not produce overfitting and instead directly benefits model performance. (3) A novel bidirectional version of mixup contrastive data augmentation further improves model performance in this low-sample setting. (4) State-of-the-art reconstructions for low-level image metrics can be obtained by independently mapping to Stable Diffusion's VAE latent space. (5) fMRI-to-image retrieval can find the exact original image even among highly similar candidates, suggesting that fine-grained image-specific information is contained in brain embeddings, thus allowing retrieval to be scaled up to large-scale databases like LAION-5B to output images without generative models.

Figure 1: Example images reconstructed from human brain activity corresponding to passive viewing of natural scenes. Reconstructions depict outputs from Versatile Diffusion  given CLIP fMRI embeddings generated by MindEye for Subject 1. See Figure 4 and Appendix A.4 for more samples.

## 2 MindEye

MindEye consists of two pipelines (see Figure 2), a high-level (semantic) pipeline where fMRI voxels are mapped to the CLIP ViT-L/14 image space and a low-level (perceptual) pipeline where the voxels are mapped to the image embedding space of a VAE. Both pipelines follow a common structure: a residual MLP backbone followed by two task-specific submodules. For the high-level pipeline the submodules are an MLP projector and a diffusion prior. For the low-level pipeline the submodules are an MLP projector and a CNN decoder that performs 4x upsampling. For both pipelines we observe that training the projector submodule with a contrastive loss and the second submodule with mean squared error (MSE) loss gives the best performance.

### High-Level (Semantic) Pipeline

The high-level pipeline is the core of MindEye as it maps voxels to CLIP image space to be fed through pretrained image generation models. We refer to it as a "high-level" pipeline because CLIP embeddings are inherently more semantic than perceptual, given that CLIP image encoders were trained to maximize similarity with text captions (low-level features like color and object location are not typically preserved in these captions). MindEye can be used without the low-level pipeline, which simply aids to better preserve low-level image features during reconstruction.

The MLP backbone for our high-level pipeline maps flattened voxels to an intermediate space of size \(257 768\), corresponding to the last hidden layer of CLIP ViT/L-14 (see Appendix 1 for PyTorch model code). The backbone consists of a linear layer followed by 4 residual blocks and a final linear projector. The embeddings from the backbone are fed to an MLP projector and a diffusion prior in parallel. The whole model is trained end-to-end with the prior getting an MSE loss and the projector getting a bidirectional CLIP loss. The projector outputs can be used for retrieval tasks and the diffusion prior outputs can be used by generative models to reconstruct images.

**Contrastive Learning:** Contrastive learning is an effective method for learning representations across modalities by maximizing cosine similarity for positive pairs while minimizing similarity for negative pairs. Previous work has shown the potential benefits of using contrastive learning alongside neural data [12; 13]. CLIP  is an example of a multimodal contrastive model that maps

Figure 2: MindEye overall schematic. A high-level “semantic” pipeline maps voxels to CLIP embeddings for image reconstruction (outputs from a diffusion prior feed through generative models like Versatile Diffusion) or retrieval tasks (such as K-nearest neighbor querying of brain embeddings to the CLIP embeddings of LAION-5B images). A low-level “perceptual” pipeline maps voxels to the variational autoencoder used by Stable Diffusion to obtain blurry reconstructions, which are used as the initialization for subsequent diffusion-based image generation. The contrastive loss for the low-level pipeline is omitted for simplicity; see Appendix A.3.2 for details.

images and text captions to a shared embedding space. MindEye is trained to introduce fMRI as an additional modality to the embedding space of a pretrained CLIP model, keeping the CLIP image space frozen as done with locked-image text tuning (LiT) . We use the CLIP loss  as our contrastive objective. This loss is bidirectional and helps improve both image and brain retrieval.

Recent work [15; 16; 17; 18] has explored novel data augmentation techniques that offer several benefits like improving performance, increasing robustness, and reducing training data requirements. Mixup  is one such technique that trains models on synthetic data created through convex combinations of two datapoint-label pairs . Kim et al.  introduce MixCo, an extension of mixup that uses the InfoNCE loss, and show that MixCo improves classification performance in a semi-supervised setting. Based on the same principle, we modify the bidirectional CLIP loss to use MixCo. While Kim et al.  observed that MixCo gives largest performance benefit for smaller models, we observe that it also helps large models in low data regimes.

To combine MixCo with CLIP loss, we mix voxels using a factor \(\) sampled from the Beta distribution with \(==0.15\).

\[x_{_{i,k_{i}}}=_{i} x_{i}+(1-_{i}) x_{k_{i} }, p_{i}^{*}=f(x_{_{i,k_{i}}}), p_{i}=f(x_{i}), t_{i} =_{}(y_{i})\] (1)

Here, \(x_{i}\) and \(y_{i}\) represent the \(i\)-th fMRI sample and image respectively. \(k_{i}[1,N]\) is an arbitrary mixing index for the \(i\)-th datapoint and \(f\) represents the combined MLP and projector. \(p^{*}\), \(p\) and \(t\) are L2-normalized. The CLIP loss with MixCo is defined as:

\[_{}&=-_{ i=1}^{N}[_{i}(^{*}  t_{i}}{})}{_{m=1}^{N}(^{*} t_{m}} {})})+(1-_{i})(^{*} t_{k_{i}}}{})}{_{m=1}^{N}(^{*}  t_{m}}{})})]\\ &-_{j=1}^{N}[_{j}(^{*} t_{j}}{})}{_{m=1}^{N}( {p_{m}^{*} t_{j}}{})})+_{\{l\}|k_{l}=j}(1-_{l} )(^{*} t_{j}}{})}{ _{m=1}^{N}(^{*} t_{j}}{})})] \] (2)

We term this bidirectional loss as BiMixCo. Here \(\) is a temperature hyperparameter, and \(N\) is the batch size.

Recent works [21; 22] have shown that stopping mixup augmentation after a certain number of epochs leads to better classification performance. As per these findings, we stop using mixup and switch from a hard contrastive loss to a soft contrastive loss one-third of the way through training. This improves our reconstructions without harming our retrieval performance (see Table 4). BiMixCo gives the highest retrieval performance but slightly hurts reconstructions (Table 4) likely due to how the reconstruction task needs absolute targets for the mixed inputs (which we generate by mixing the original targets in the same ratio as the mixup inputs), causing a slight shift in the distribution of target embeddings. Our final schedule combines BiMixCo and soft contrastive loss to strike the best balance between retrieval and reconstruction performance in a single model.

Our soft contrastive loss is inspired by knowledge distillation , where the authors argue that the softmax probability distribution produced by a powerful teacher model acts as a better teaching signal for a student than hard labels. To generate the soft labels we take the dot product of CLIP image embeddings in a batch with themselves. The loss (with bidirectional component omitted for brevity) is calculated between CLIP-CLIP and Brain-CLIP matrices as:

\[_{}=-_{i=1}^{N}_{j=1}^{N}[ t_{j}}{})}{_{m=1}^{N}( t_{m}}{})}( t_{ j}}{})}{_{m=1}^{N}( t_{m}}{} )})]\] (3)

**Diffusion Prior:** Using a diffusion model to align the outputs of a contrastive learning model was inspired by DALL-E 2 , where a "diffusion prior" was used to map CLIP text embeddings to CLIPimage space before using an unCLIP decoder to reconstruct images. Here we train our own diffusion prior from scratch to map CLIP fMRI embeddings to CLIP image space, which are then fed into a pretrained Versatile Diffusion model to generate image reconstructions. We modified an open-source implementation of the DALL-E 2 diffusion prior available on GitHub (see Appendix A.3.1). We used the same prior loss as Ramesh et al. . Our total end-to-end loss is defined as:

\[=_{|}+_{}\] (4)

We use \(=0.3\) and switch from BiMixCo to SoftCLIP after one-third of the train cycle. All our models are trained on a single A100 GPU for 240 epochs with a batch size of 32. Despite a high parameter count, MindEye (including both high- and low-level pipelines) can be trained on a single A100 in less than 18 hours. This efficiency is due to the bulk of the parameters stemming from MLPs, which are faster to compute than transformers or CNNs.

The diffusion prior is critical for reconstruction because contrastive learning only incentivizes the CLIP fMRI embeddings to match the vector direction of the associated CLIP image embeddings. This generates disjointed embeddings as observed by Ramesh et al. . Theoretically, multimodal contrastive learning will always produce disjointed embeddings because of the "modality gap" phenomenon whereby encoding modalities into a shared space restricts the effective embedding space to a narrow cone in geometric space . We use pre-trained models that expect CLIP image embeddings as input, thus motivating our training of a diffusion prior to align disjointed embeddings.

To rectify this issue, the diffusion prior learns a distribution of CLIP image embeddings conditioned on CLIP fMRI embeddings. UMAP  plots of disjointed CLIP fMRI embeddings next to aligned CLIP fMRI embeddings in Appendix A.5 show how the diffusion prior addresses the disjointed embedding spaces problem. We observe that the prior's role cannot be fulfilled by simply adding MSE loss to the MLP projector in Table 4. This is because there is a tradeoff between reconstruction and retrieval objectives and a model cannot effectively learn a single embedding space that does well on both.

### Low-Level (Perceptual) Pipeline

The low-level pipeline maps voxels to the embedding space of Stable Diffusion's VAE. The output of this pipeline can be fed into the VAE decoder to produce blurry image reconstructions that lack high-level semantic content but exhibit state-of-the-art low-level image metrics. We use img2img  to improve our final image reconstructions in terms of low-level metrics, with minimal impairment to high-level metrics, such that we start the diffusion process from the noised encodings of our blurry reconstructions rather than pure noise.

The MLP backbone for our low-level pipeline follows the same architecture as our high-level pipeline, except that the final outputs are of size \((16,16,64)\). These are upsampled to \((64,64,4)\) by a CNN upsampler. An MLP projector projects the backbone outputs to a \(512\) dimensional space where an auxiliary contrastive loss is applied. For more information on the low-level pipeline see Appendix A.3.2. See Appendix Figure 7 for example blurry reconstructions and Appendix Table 5 to see the effect of changing img2img strength on subsequent reconstruction metrics.

## 3 Results

For all experiments, we used the Natural Scenes Dataset (NSD) , a public fMRI dataset containing the brain responses of human participants passively viewing natural scenes from MS-COCO . By utilizing MS-COCO, this dataset provides measured brain responses to rich naturalistic stimuli, allowing us to study how well low- and high-level image features are reconstructed by MindEye. We used the same standardized train/test splits as other NSD reconstruction papers [3; 4; 28], training subject-specific models for each of 4 participants. We averaged across three same-image repetitions for the test set (leaving 982 test samples) but not the training set (24,980 training samples), similar to Takagi and Nishimoto . For more information on NSD and data preprocessing see Appendix A.2; for single-trial and reduced dataset results see Appendix A.9 and Appendix A.10.

### Image/Brain Retrieval

Image retrieval evaluations reveal the level of fine-grained image-specific information contained in the predicted brain embeddings. For example, if the model is given a dozen pictures of zebras and the brain sample corresponding to viewing one of those zebras, can the model correctly find the corresponding zebra? If the model can correctly deduce that the brain sample corresponds to an image of a zebra but cannot deduce the specific image amongst various candidates, this would suggest that category-level information but not exemplar-specific information is preserved in the CLIP fMRI embedding. MindEye not only succeeds in this zebra example but also demonstrates 93.2% overall accuracy for Subject 1 in finding the exact original image within the 982 test images (see Figure 3).

Although we use the full test dataset for retrievals in Figure 3, we followed the same procedure as Lin et al.  for calculating the retrieval metrics reported in Table 1. Brain retrieval performance was calculated according to the following procedure: for each test image, the image is converted to a CLIP image embedding and we compute the cosine similarity to both its respective ground truth disjointed CLIP fMRI embedding as well as 299 other randomly selected disjointed CLIP fMRI embeddings in the test set. For each test sample, success is determined if the cosine similarity is greatest between the ground truth CLIP embedding and its respective fMRI embedding (aka top-1 retrieval performance, chance=1/300). We average retrieval performance across all test samples and repeat the entire process 30 times to account for the variability in random sampling of batches. For image retrieval, the same procedure is used except image and brain samples are flipped such that the goal is to find the corresponding paired CLIP image embedding out of 300 possible CLIP embeddings in the batch. Lin et al.  refer to image retrieval as "forward retrieval" and brain retrieval as "backward retrieval" in their paper.

We can scale up image retrieval using a pool of billions of image candidates. In Figure 3 we show results querying the LAION-5B dataset  using our CLIP fMRI embeddings. The final layer CLIP ViT-L/14 embeddings for all 5 billion images are available at knn.laion.ai, and can be queried for K-nearest neighbor lookup via the CLIP Retrieval client . For each test sample, we first retrieve 16 candidate images using this method (using a variant of MindEye that maps voxels to the final

Figure 3: MindEye image retrieval. Given a pool of candidate images, nearest neighbor search in CLIP space enables searching for the original image based on brain activity. Top section depicts how, given 982 test NSD images (many containing very similar looking images, e.g., over a dozen zebras), MindEye top-1 performance is 93.2% for Subject 1. The ability to distinguish among confusable candidates suggests brain embeddings retain fine-grained, image-specific information. Bottom section depicts scaling up to the LAION-5B dataset (see Appendix A.4 for more examples). Even with billions of images, MindEye finds images similar to the original.

layer of CLIP, see Appendix A.7). The best image is then selected based on having the highest CLIP embedding cosine similarity to the CLIP fMRI embedding. This image retrieval approach is especially well-suited for tasks involving fine-grained classification, and can be used as an alternative to image reconstruction without a generative model (evaluations in Table 1).

### fMRI-to-Image Reconstruction

The diffusion prior outputs from MindEye are aligned CLIP fMRI embeddings that can be used with any pretrained image generation model that accepts latents from CLIP image space. We evaluate the outputs of MindEye reconstructions across several models including Versatile Diffusion , Stable Diffusion (Image Variations) , and Lafite [32; 11]. Here we report results from Versatile Diffusion since it yielded the best results, and we report results from the other models in Appendix A.7. We qualitatively compare our reconstructions side-by-side with outputs from other fMRI-to-image reconstruction models in Figure 4 and quantitatively compare against other models in Table 1, demonstrating state-of-the-art MindEye reconstructions.

For each subject, for each test brain sample, we output 16 CLIP image embeddings from MindEye and feed these embeddings through the image variations pipeline of Versatile Diffusion. This produces 16 image reconstructions per brain sample. For our reconstructions we use 20 denoising timesteps with UniPCMultistep noise scheduling  and start the denoising process from the noised output of our low-level pipeline (img2img). We then select the best of 16 reconstructions by computing last hidden layer CLIP embeddings and picking the image with the highest cosine similarity to the disjointed CLIP fMRI embedding. This automatic second-order selection was inspired by DALL-E 2 , which used a similar process of selecting the best of 2 generated samples.

Two-way identification refers to percent correct across comparisons gauging if the original image embedding is more similar to its paired brain embedding or a randomly selected brain embedding. Comparison was performed for AlexNet  (second and fifth layers), InceptionV3  (last pooling layer), and CLIP (final layer of ViT-L/14). We use the same settings as Ozcelik and VanRullen  for our metrics. For more details refer to Appendix A.6.

### Ablations

In this subsection we try to explain where MindEye performance improvements come from through ablations. To study the effects of architectural changes and training strategies we train only the retrieval pipeline (no diffusion prior) for 120 epochs with batch size 300. All models in this section are trained on Subject 1. Table entries with * correspond to the final version of MindEye's settings.

**Architectural Improvements:** To study the effect of model depth and parameter count we train multiple MLPs of various sizes (Table 2). Among models that map to the last hidden layer of CLIP ViT-L/14, we observe a clear trend of increased performance with added residual blocks. For 2 blocks, the effect of skip connections is not too significant but at 4 blocks the model does significantly worse without them, indicating that skip connections are important for training deeper models.

Figure 4: Side-by-side comparison of reconstructions from fMRI-to-Image NSD papers. The same test set was used across papers. All reconstructions come from Subject 1.

We also show a comparison with a 4-reshblock model that maps to the final layer of CLIP (only the CLS classification token). This model has \(7\) fewer parameters and does much worse than all other models. This indicates two things: (1) MindEye strongly benefits from a large parameter count MLP backbone and does not overfit even in the sample constrained settings of the NSD dataset, and (2) the fMRI voxels contain fine-grained information about images, allowing us to effectively predict all \(257\) CLIP image embeddings instead of just the CLS token.

**Training Strategies (Losses and Data Augmentations):** We observe that with InfoNCE, MindEye only does well on brain retrieval (Table 3). A similar trend was observed in Lin et al. . We attribute this to InfoNCE being a one-sided loss that only optimizes for one retrieval objective. Simply replacing InfoNCE with CLIP loss significantly improves image retrieval. MixCo augmentation helps both unidirectional and bidirectional losses.

   Method &  &  &  \\   &  \\  Lin et al.  & \(-\) & \(-\) & \(-\) & \(-\) & \(78.2\%\) & \(-\) & \(-\) & \(11.0\%\) & \(49.0\%\) \\ Takagi...  & \(-\) & \(-\) & \(83.0\%\) & \(83.0\%\) & \(76.0\%\) & \(77.0\%\) & \(-\) & \(-\) & \(-\) \\ Gu et al.  & \(.150\) & \(.325\) & \(-\) & \(-\) & \(-\) & \(-\) & \(.862\) & \(.465\) & \(-\) & \(-\) \\ Ozcelik...  & \(.254\) & \(\) & \(94.2\%\) & \(96.2\%\) & \(87.2\%\) & \(91.5\%\) & \(.775\) & \(.423\) & \(21.1\%\) & \(30.3\%\) \\ MindEye & \(\) & \(.323\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\) & \(\) & \(\%\) & \(\%\) \\  MindEye (Low-Level) & \(\) & \(\) & \(78.1\%\) & \(74.8\%\) & \(58.7\%\) & \(59.2\%\) & \(1.00\) & \(.663\) & \(-\) & \(-\) \\ MindEye (High-Level) & \(.194\) & \(.308\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\) & \(\) & \(\%\) & \(\%\) \\ MindEye (LAION) & \(.130\) & \(.308\) & \(84.0\%\) & \(92.6\%\) & \(86.9\%\) & \(86.1\%\) & \(.778\) & \(.477\) & \(-\) & \(-\) \\  Ozcelik... (Low-, S1) & \(.358\) & \(.437\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\) & \(\) & \(-\) & \(-\) \\ MindEye (Low-, S1) & \(\) & \(\) & \(87.1\%\) & \(84.1\%\) & \(61.6\%\) & \(62.4\%\) & \(.992\) & \(.638\) & \(-\) & \(-\) \\   

Table 1: Quantitative comparison of MindEye retrieval and reconstruction performance against other models. Top and middle sections average across the same 4 participants (see Appendix A.8 for individual subject models), except Lin et al.  which only analyzed Subject 1. Middle section reflects outputs from only the high- or low-level pipeline, and metrics when evaluating images retrieved from LAION-5B. Bottom section compares our low-level reconstructions to the low-level reconstructions from Ozcelik and VanRullen  which only reported metrics for Subject 1. Image retrieval refers to the percent of the time the correct image was retrieved out of 300 candidates, given the associated brain sample (chance=0.3%); vice-versa for brain retrieval. PixCorr=pixelwise correlation between ground truth and reconstructions; SSIM=structural similarity index metric ; EfficientNet-B1 (“Eff”)  and SwAV-ResNet50 (“SwAV”)  refer to average correlation distance; all other metrics refer to two-way identification (chance = 50%). Missing values are from papers not reporting all metrics or metrics being non-applicable. We followed the same image preprocessing as Ozcelik and VanRullen . Previous state-of-the-art Ozcelik and VanRullen  results are directly comparable to MindEye as the same test set and Versatile Diffusion model were used. Bold indicates best performance within sections.

   Method &  &  \\  InfoNCE & \(0.237\) & \(0.784\) \\ CLIP Loss & \(0.837\) & \(0.791\) \\ InfoNCE + MixCo & \(0.303\) & \(\) \\ CLIP Loss + MixCo (BiMixCo) & \(0.884\) & \(0.841\) \\ SoftCLIP Loss & \(0.837\) & \(0.816\) \\ BiMixCo + SoftCLIP (MindEye)* & \(\) & \(0.822\) \\   

Table 3: Effects of different losses and MixCo augmentation on MLP retrieval performance.

We also show the effect of training with our SoftCLIP loss. SoftCLIP improves over hard CLIP loss for brain retrieval but performs worse than BiMixCo. Our training regime combining SoftCLIP with BiMixCo gives the best image retrieval performance.

**Reconstruction Strategies:** To demonstrate the need for a separate diffusion prior, we train a version of MindEye where both contrastive and MSE losses are applied to the ouputs of the MLP backbone. We observe that this model does poorly in terms of retrieval metrics, indicating a tradeoff between retrieval and reconstruction objectives where it is difficult to learn a single embedding space that satisfies both objectives. Inspired by recent works in self-supervised learning [39; 40; 41; 42], we decouple these losses using a separate MLP projector, where MSE loss is applied to the outputs of the MLP backbone and contrastive loss is applied to the outputs of the projector. This model does slightly worse in terms of reconstruction but is much better at retrieval. Finally, we train a model with a diffusion prior but no MLP projector. Contrastive loss is computed for the MLP backbone and MSE loss is computed for the diffusion prior. This model is comparable to high-level MindEye in terms of reconstruction but does worse in retrieval, giving further evidence of a tradeoff. Example reconstructions for these models are in Appendix Figure 8.

## 4 Related Work

In the 2000s, researchers demonstrated that visual information, such as spatial position , orientation [44; 45], and coarse image category [46; 47] could be decoded from fMRI signals using linear classifiers. With the introduction of generative adversarial networks , more sophisticated decoding became feasible as researchers mapped brain activity to the latent space of these models to reconstruct handwritten digits , human faces [50; 51], and natural scenes [52; 5; 53]. More recently, with the release of multimodal contrastive models like CLIP , diffusion models [54; 55] like Stable Diffusion , and new large-scale fMRI datasets , fMRI-to-image reconstructions have reached an unprecedented level of quality [4; 3; 28].

Lin et al.  reconstructed NSD images by mapping voxels to CLIP space (see also Wang et al. ) and fed outputs through a fine-tuned Lafite  GAN (MindEye reconstructions using Lafite in Appendix A.7). Differences from MindEye include using a convolutional model, no projector to separate contrastive loss from MSE loss, InfoNCE instead of CLIP loss, fine-tuning of a pretrained GAN, no diffusion prior, and mapping to both CLIP image and text space. Ozcelik and VanRullen  used a low- and high-level pipeline with Versatile Diffusion . Differences include mapping to CLIP space via ridge regression, no contrastive learning or diffusion prior, and mapping to a VDVAE  for low-level reconstructions. Gu et al.  used a low- and high-level pipeline and extended on Ozcelik et al.  by reconstructing with IC-GAN ; they did not flatten voxels and mapped to SwAV  features with surface-based convolutional networks. Takagi and Nishimoto  used ridge regression to map to Stable Diffusion latents and CLIP text latents, using different voxel selections for different components. Mind-Vis  did not use the Natural Scenes Dataset but tackled the fMRI-to-image problem from the unique angle of first pre-training a masked brain model on a separate large-scale dataset. This enabled the authors to use a more informative latent as the input to their image reconstruction model. Mind-Video  subsequently extended on the Mind-Vis approach by reconstructing video rather than images. Overall, MindEye is unique in its use of reconstruction and retrieval submodules, a deep MLP backbone with 940 million parameters (parameter size comparison in Table 10), a diffusion prior for more accurate translation across brain and image modalities, and state-of-the-art reconstu

   Method &  &  &  \\   & PixCorr & SSIM & Alex(2) & Alex(5) & Incep & CLIP & Image & Brain \\  Only MLP Backbone & \(0.119\) & \(0.346\) & \(73.8\%\) & \(84.1\%\) & \(81.5\%\) & \(82.6\%\) & \(0.133\) & \(0.631\) \\ Backbone + Projector & \(0.154\) & \(0.296\) & \(73.2\%\) & \(85.2\%\) & \(75.2\%\) & \(77.3\%\) & \(0.888\) & \(0.849\) \\ Backbone + Prior & \(\) & \(\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(0.934\) & \(0.901\) \\ MindEye (only BiMixCo) & \(0.195\) & \(0.290\) & \(91.1\%\) & \(96.6\%\) & \(93.7\%\) & \(94.4\%\) & \(\) & \(0.942\) \\ MindEye (0.33 BiMixCo)* & \(0.198\) & \(0.302\) & \(91.6\%\) & \(96.8\%\) & \(94.6\%\) & \(95.0\%\) & \(0.972\) & \(\) \\   

Table 4: Effects of diffusion prior and MLP projector on reconstruction and retrieval metrics.

Conclusion

We present MindEye, a novel mental decoding approach that achieves state-of-the-art reconstructions of natural scenes presented to humans in the MRI machine. These reconstructions retain semantic and perceptual similarity to the original images due to the use of a combined high-level and low-level pipeline. The novel use of specialized submodules for contrastive-based retrieval and diffusion-based reconstruction allows MindEye to learn mappings for both tasks in parallel. MindEye brain latents contain fine-grained image-specific signal as demonstrated by the ability to select ground truth images out of a set of nearly 1,000 possible images (see Figure 3). We leveraged pretrained CLIP , a model trained with billions of image and text data samples, as a teacher to guide MindEye where we have a relative scarcity of brain data (less than 30,000 training samples per participant). Our diffusion prior submodule is trained from scratch and allows for accurate translation of brain embeddings into pretrained CLIP space such that any model that accepts CLIP image embeddings can be provided with CLIP fMRI embeddings without fine-tuning. This flexibility suggests that MindEye reconstructions will continue to improve as newer, more powerful image generation models are released.

**Privacy Concerns & Societal Benefits:** The ability to accurately reconstruct perception from brain activity prompts questions about broader societal impacts. For instance, it should be possible to generalize current reconstruction models from perception to mental imagery without training a new model . However, current models are not capable of across-subject decoding and each NSD participant spent up to 40 hours in the MRI machine to procure sufficient training data (see Appendix 9 for results using a subset of the total training data). Furthermore, non-invasive neuroimaging methods in general require compliance because participants can easily resist decoding by moving their head or thinking about unrelated information . MindEye is also limited to natural scenes such as those in MS-COCO; for other image distributions additional data collection and specialized generative models would be required. While high-quality image reconstruction via non-invasive neuroimaging is not currently practical for real-world applications, technology is constantly improving and it is important that brain data be carefully protected and companies collecting such data be transparent with their use.

Image reconstruction from brain activity can enable various potential societal benefits. Reconstructions are expected to be systematically distorted due to mental state, neurological conditions, etc. This could potentially enable novel clinical diagnosis and assessment approaches. For example, patients suffering from major depressive disorder might produce reconstructions where emotionally negative aspects of images are more salient . MindEye results also suggest potential for improved locked-in (pseudocoma) patient communication via fine-grained visual communication beyond simple classification , as well as brain-computer interface performance if adapted to real-time fMRI analysis  or non-fMRI neuroimaging modalities.

**Future Directions:** Future directions we wish to explore include mapping individual subjects to a shared embedding space to enable training models that are generalizeable across people (e.g., ), and exploring model intepretability approaches like GradCAM  or Network Dissection  to identify the fMRI voxels that most strongly respond to the presence of certain image features.

## 6 Open Research: 100% Transparent Volunteer-Driven Science

MindEye was openly developed through volunteer contributions in the MedARC Discord server. Source code was always accessible via a public GitHub repository throughout the lifespan of the project. Research discussions were held via public Discord channels, and weekly video conference calls were recorded and shared publicly. We continue to extend a global invitation to contribute to MedARC Neuroimaging & AI Lab projects to cultivate an internationally diversified, volunteer-driven research team composed of members from varied backgrounds possessing a wide array of expertise. We contend that fully transparent open-research initiatives such as this and others like EleutherAI, LAION, OpenBioML, and ML Collective could redefine the traditional framework of scientific research, democratizing entry into machine learning and medical research through the harnessing of crowd-sourced collective intelligence and community collaboration.

## 7 Author Contributions

For detailed author contributions see Appendix A.1.

Acknowledgements

Thanks to the MedARC community, including Jeremy Howard, Tommaso Furlanello, Mihir Tripathy, and Cesar Torrico for useful discussion and reviewing the manuscript. Thank you to Furkan Ozcelik, author of Brain-Diffuser, for sharing his code and expert knowledge with our group. We thank LAION for being the initial community where this project developed, and thank Romanin Beaumont and Zion English for useful discussion during that time. We thank Stability AI for sharing their high-performance computing workplace and giving us the computational resources necessary to develop MindEye. Thank you to Richard Vencu for help navigating the Stability HPC. Collection of the Natural Scenes Dataset was supported by NSF IIS-1822683 and NSF IIS-1822929.