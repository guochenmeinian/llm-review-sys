# Multi-modal Situated Reasoning in 3D Scenes

Xiongkun Linghu\({}^{1,*}\), Jiangyong Huang\({}^{1,2,*}\), Xuesong Niu\({}^{1,*}\), Xiaojian Ma\({}^{1}\), Baoxiong Jia\({}^{1,}\), Siyuan Huang\({}^{1,}\)

\({}^{1}\)State Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{2}\)Peking University

https://msr3d.github.io

###### Abstract

Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (_e.g._, text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.

## 1 Introduction

Understanding and interacting with the 3D physical world is fundamental to the development of embodied AI. A central challenge in equipping agents with these capabilities is the integration of situational awareness into models. This is particularly critical given the pivotal role of situation awareness in bridging agents' multi-modal local context (_e.g._, text descriptions, images, point clouds, _etc._) with the global environment status, thereby facilitating reasoning and planning in 3D scenes.

However, compared to recent advancement in 3D vision-language learning [9; 1; 72; 24; 30], the study of situation modeling in 3D scenes remained largely underexplored. This is primarily due to the absence of a scalable method to collect diverse multi-modal situational data. Previous studies have mainly relied on simulated environments [63; 55; 47] to generate egocentric observations of virtual agents. These approaches severely limit the quality of situational data due to the constrained diversity and complexity of available synthetic scenes. Recent efforts such as SQA3D  have aimed to extend situated understanding to real-world scenes like ScanNet  by collecting situated question-answer pairs under imaginative situations represented by locations and orientations in 3D scenes. Nonetheless, this data collection process is prohibitively expensive, thereby restricting the scale of situational data available for model learning and evaluation.

To address the aforementioned data limitations, we propose, **M**ulti-modal **S**ituated **Q**uestion **A**nswering (MSQA), _a high-quality, large-scale multi-modal dataset for 3D situated reasoning_. Specifically, we develop an automated pipeline for efficient and scalable data collection. First, we generate diverse situations (_i.e._, spatial locations and viewpoints) in complex real-world scenes sourced from ScanNet , 3RScan , and ARKitScenes . By adjusting the provided 3D scene graph of each scene based on sampled viewpoints, we create situated scene graphs and use them to generate high-quality situated question-answer pairs by meticulously designing prompts for large language models (LLMs). With this pipeline, we collect 251K situated QA pairs, surpassing existing datasets in scale, question scope, and quality. Additionally, we enrich this dataset with question-answer pairs targeting navigation actions necessary to move between different situations, providing comprehensive learning and evaluation data for embodied navigation. This curated navigation data directly evaluates the transfer of situation understanding from reasoning to action, thereby extending MSQA to cover the full spectrum of embodied tasks in 3D scenes.

With MSQA, we introduce evaluation benchmarks to precisely assess models' situation awareness, addressing limitations of existing benchmarks. Current benchmarks predominantly rely on single-modal descriptions of situations (_i.e._, texts), which can lead to ambiguity in situation identification, thereby restricting models' situation understanding capability (as shown in Fig. 2). To overcome this, we propose an _interleaved_ input setting that integrates _textual descriptions, images, and scene point clouds_ to describe situations and questions. This approach resolves ambiguity in situation descriptions and provides a versatile format for broader downstream applications. Leveraging this multi-modal interleaved setting, we establish two challenging benchmarking tasks, Multi-modal Situated Question Answering (MSQA) and Multi-modal Next-step Navigation (MSNN), aimed at evaluating models' capabilities in embodied reasoning and navigation. MSQA expands the scope of existing situated question-answering tasks to encompass object existence, counting, attributes, spatial relationships, _etc._ MSNN simplifies traditional multi-step embodied navigation to a single-step setting, focusing on the immediate next action based on the current situation and navigation target. This design separates long-horizon planning from situated understanding, targeting models' ability to ground actions and transition between actions. We provide an overview of these tasks in Fig. 1.

We provide comprehensive experimental analyses of existing vision-language models on these tasks, exposing their limitations in effectively modeling complex situations and fully utilizing interleaved multi-modal input. In response to identified limitations, we propose a powerful baseline model, MSR3D, specifically designed for handling interleaved multi-modal input with situation modeling that achieves superior results on both MSQA and MSNN. Our additional experiments on data scaling and cross-domain transfer further demonstrate the efficacy of pre-training on our MSQA and showcase the potential of MSR3D. In summary, our key contributions are as follows:

* We introduce MSQA, a large-scale 3D situated reasoning dataset comprising 251K situated QA pairs, curated using a scalable automated data generation pipeline across diverse real-world scenes.
* We propose the use of interleaved multi-modal input setting for model learning and evaluation, establishing two comprehensive benchmarking tasks, MSQA and MSNN, to assess models' capability in situated reasoning and navigation in 3D scenes.

Figure 1: **An overview of benchmarking tasks in MSQA. We use green boxes for objects mentioned in situation descriptions, red for objects in questions, and purple for objects in navigation instructions.**

* We conduct comprehensive experimental analyses comparing existing models with our proposed baseline MSR3D on MSQA and MSNN. We highlight the importance of handling multi-modal interleaved inputs and situation modeling. Through data scaling and cross-domain transfer experiments, we demonstrate the effectiveness of pre-training on MSQA data and the potential of MSR3D for multi-modal situated reasoning in 3D scenes.

## 2 Related Work

Situated understanding in 3D scenes.Existing efforts in 3D VL research primarily focus on understanding and reasoning within 3D scenes, including object grounding [9; 1; 70; 11; 46; 32; 64; 67], captioning [14; 12], and question answering [6; 41; 24]. Recently some initiatives propose unified frameworks for various 3D VL tasks [8; 15; 72; 26; 28; 13; 73; 62], yielding promising outcomes. Nonetheless, a prevailing limitation pertains to the absence of situated understanding in these tasks [40; 9; 1; 72; 6; 28], which accounts for a notable gap between 3D VL and embodied AI [4; 54; 49; 52; 55; 2; 20; 65]. While earlier works on situated reasoning [18; 21; 56] typically encompass answering simple questions via exploring the simulative environments, SQA3D  introduces real-world scenes with a particular focus on spatial reasoning and scene understanding. SIG3D  underscores situational awareness and proposes an effective method to address the challenge. In this paper, we extend the 3D situated reasoning task to more diverse and complex scenarios. Furthermore, we devise innovative multi-modal situated next-step navigation to consolidate the evaluation of situated reasoning.

LLM-assisted data generation.Large Language Models (LLMs) exhibit remarkable proficiency in text generation and serve as a valuable resource for collecting diverse textual instruction-following data [61; 57; 16] and multi-modal instruction-following data [38; 34; 37]. This method also exhibits notable promise to aid the scarcity of 3D VL data [40; 28; 35; 30]. However, the quality of LLM-generated data has been a common concern in the community, especially considering the inherent complexity of 3D scenes. To address this problem, existing efforts [24; 50; 28; 35] have improved the LLM prompting techniques and post-processing procedures to enhance the reliability and diversity of LLM-generated data. And some prior works [10; 19] attempt to evaluate the quality of LLM-generated data yet have not resolved the concerns on the quality of LLM-generated data and how it compares to human-annotated data. In this paper, in addition to advanced prompting techniques and post-processing procedures, we also conduct a human study on the quality of LLM-generated data to demonstrate the efficacy of our LLM-assisted data generation approach.

Interleaved multi-modal understanding.It has been a critical challenge to precisely delineate the situation within intricate 3D scenes. Natural as it is, adopting textual descriptions [56; 41] may encounter issues of object referral ambiguity, especially when situated within cluttered environments. On the other hand, ego-view visual observations [18; 4; 54; 23; 22] are widely adopted in embodied tasks but bridging the modality gap demands extra training. Recently, interleaved multi-modal data has become prevalent in both VL [58; 3; 27; 34; 71; 29; 69; 35] and embodied AI [53; 31; 36]. In the context of 3D situated reasoning, the interleaved multi-modal format can remedy the ambiguity and thus stands as a general scheme to delineate the situation. Such an interleaved multi-modal scheme strengthens the challenge of our situation reasoning task, requiring comprehensive capabilities of VL grounding and multi-modal situated reasoning.

Figure 2: **An illustration on resolving ambiguity with interleaved multi-modal input. With both chairs highlighted in purple and green boxes having the same textual description “chair is next to the table”, one can easily identify the target chair from the candidates by providing an image describing its location.**

## 3 Multi-modal Situated Reasoning Dataset

We propose a novel and scalable approach to collecting high-quality 3D situated reasoning data, guided by three core principles: (1) ensuring comprehensive and diverse situations, (2) crafting highly situation-dependent questions with accurate answers, and (3) accommodating the multi-modal interleaved input format for avoiding ambiguities. We construct the MSQA dataset by employing our data collection pipeline on complex real-world scenes sourced from ScanNet , 3RScan  and ARKitScenes . MSQA comprises 251K multi-modal situated question-answering data. Each data instance can be formulated as a tuple \((,,,)\), where \(\) denotes the scene point cloud; \(=(s^{txt,img},s^{loc},s^{rot})\) includes a multi-modal situation description \(s^{txt,img}\), the corresponding location \(s^{loc}\) and orientation \(s^{rot}\), the interleaved multi-modal question \(=^{txt,img}\) collected under situation \(\), and the ground truth answer \(\). In the following sections, we delineate our data collection pipeline in Appendix A.2 and present data statistics in Appendix A.6.

### Data Collection

As illustrated in Fig. 3, we meticulously devise an LLM-based automatic data collection pipeline comprising three stages: situation sampling, QA pairs generation, and data refinement. Our goal for data collection is to ensure the high quality of generated data. We detail the pipeline below.

Situation samplingThe situation consists of four components: (i) the location \(s^{loc}=(x,y,z)\), (ii) the orientation represented by a rotation angle \(s^{rot}\) within the XY plane, (iii) location description, and (iv) surrounding object descriptions. In our setup, we first sample the location and orientation considering four scenarios: (i) standable area on the floor with arbitrary viewpoint, (ii) sittable area with front viewpoint when sitting, (iii) reachable area of large objects (_e.g._, cabinets and fridge) with viewpoints facing or againsts to object, and (iv) reachable area of small objects (_e.g._, trashcan) with viewpoints directing standing point to object centers. We then generate location descriptions according to the interaction types (_e.g._, "I'm standing on/sitting on/in front of the fridge..."). For surrounding object descriptions, we first calculate the spatial relations between the location and surrounding objects, including distance, coarse direction (_e.g._, _left_), and fine-grained relative direction (_e.g._, _2 o'clock_). We then utilize these spatial relations to prompt GPT-3.5 for surrounding object descriptions. We provide more details and illustrative examples of sampled situations in Fig. 8.

QA pairs generationSimilar to prior works [28; 30], we adopt scene graphs to prompt LLM for data generation. We first instantiate each object in the scene graph with their attributes obtained by prompting GPT-4V  using the cropped object images. We then perform pair-wise calculations among the initialized objects to derive relations, which can be categorized into five types: in-contact vertical relations (_e.g._, support), non-contact vertical relations (_e.g._, above), horizontal distance (_e.g._, near), horizontal proximity relations (_e.g._, right) and multi-object relations (_e.g._, between).

Figure 3: **An overview of our data collection pipeline, including situated scene graph generation, situated QA pairs generation, and various post-processing procedures.**After establishing these relationships as edges in the scene graphs, we adjust the horizontal proximity relations according to the location and viewpoint of the sampled situation to obtain situated scene graphs. With these situated scene graphs, we design system prompts and hand-crafted examples to prompt GPT-3.5  to generate situated question-answer pairs. We focus on 9 distinct question scopes, spanning object attributes, counting, spatial relationships, navigation actions, _etc._ (as shown in Fig. 4(a)). During prompting, we instruct the LLM to output question categories. To further enhance the diversity of LLM-generated QA pairs, we use various combinations of seed examples and sample various situated sub-scene-graphs conditioning on different considered distances for question generation. We provide more details for QA pair generation in Appendix A.2.

Data refinementTo enhance the quality of the generated situated question-answer pairs, we conduct a refinement procedure encompassing two main aspects: (1) for the situated scene graphs, we examine the distribution of attributes and relations to mitigate any potential bias that could lead to hallucination, and (2) we manually review the LLM-generated QA pairs to validate their accuracy and devise filtering functions based on regular expression to detect and correct potential errors. Illustrative examples of the refinement procedures are provided in Appendix A.3.1. As prior works  have highlighted the importance of data balancing, we balance the answer distribution of generated data by filtering out imbalanced question-answer pairs. Through these procedures, we collect 251K multi-modal situated QA pairs across ScanNet, 3RScan, and ARKitScenes. We provide a comparison between MSQA and existing datasets in Tab. 1 and more statistics in Appendix A.6.

### Data Quality Control

Despite the scalability of the LLM-based data collection pipeline, the quality of generated data has raised major concerns, especially in 3D vision-language tasks where grounding language is challenging. To address these concerns, we conduct a human study comparing our generated data to human-annotated data in SQA3D. Specifically, we sample 100 data instances from MSQA and SQA3D and mix them for human assessment. The human evaluators are instructed to score the data on three aspects: (1) the naturalness and clarity of situation descriptions, (2) the situational dependence and clarity of questions, and (3) the accuracy and completeness of answers. Each aspect was rated on a scale from 1 to 5. Detailed information about the evaluation workflow is provided in Appendix B. The evaluation results, shown in Fig. 4(b), indicate that MSQA's quality is comparable to SQA3D across all aspects. Additionally, Fig. 4(c) shows that the proportion of high-scoring data (_i.e._, quality with score \( 4\)) in MSQA matches or exceeds that of SQA3D. This highlights the quality of MSQA and also the effectiveness of our data refinement procedures.

   Dataset & Situated & Multi-modal & Text collection & Quality Check & LLM scoring & Data sources & \#Scenes & \#Data \\  ScanQA  & ✗ & ✗ & human & ✓ & ✗ & ScanNet & 800 & 41k \\
3D-QA  & ✗ & ✗ & human & ✓ & ✗ & ScanNet & 806 & 5.8k \\ Scan2Cap  & ✗ & ✗ & human & ✓ & ✗ & ScanNet & 800 & 41k \\ ScanSemSemF  & ✗ & ✗ & template & ✗ & ✗ & 3RScan & 1185 & 90k \\
3D-LLM  & ✗ & ✗ & LLM-assisted & ✗ & ✗ & ScanNet, Habitat-Matterport  & 1.2k & – \\ LEO\({}^{}\) & ✗ & ✗ & LLM-assisted & ✗ & ✗ & 3RScan & 1185 & 191k \\ SOA3D  & ✓ & ✗ & human & ✓ & ✗ & ScanNet & 650 & 33.4k \\  MSQA & ✓ & ✓ & LLM-assisted & ✓ & ✓ & ScanNet, 3RScan, ARKitScenes & **1734** & **251K** \\   

Table 1: **A comparison between MSQA and existing 3D vision-language datasets. “Situated” indicates tasks with situation conditions. “Multi-modal” indicates whether the question contains multi-modal input. \({}^{}\) indicates we only consider the proportion of newly collected data.**

Figure 4: **Dataset statistics and quality evaluation**. We visualize (a) the distribution of question types in MSQA, (b) average quality scores of MSQA, and (c) the proportion of high-scoring data compared with SQA3D.

## 4 Evaluation Benchmarks

In this section, we give a detailed description of the evaluation tasks considered for multi-modal situated reasoning. Specifically, we consider the following two benchmarking tasks:

Multi-modal Situated Question Answering (MSQA)As mentioned Sec. 3, we evaluate models' capability in situation awareness and handling interleaved multi-modal input in MSQA. Specifically, given a multi-modal situation description, the model answers a text-image interleaved question grounded in the 3D scene. Since the responses are open-ended, former metrics, such as classification accuracy and exact-match accuracy can not give a correct evaluation. To solve this problem, we use a GPT-based evaluation metric for open-ended responses following OpenEQA  and extend its prompt sets for 3D situated reasoning (see detailed prompt in Appendix B.1.1). Above all, we report the correctness score \(C\) for the test set with \(N\) samples following OpenEQA, \(C\) could be calculated by:

\[C=_{i=1}^{N}-1}{4} 100\%,\]

where \(s_{i}\) (ranging from 1 to 5, the higher the better) is generated by the LLM when prompted with the question, ground truth answer, and the model response.

Multi-modal Situated Next-step Navigation (MSNN)In addition to MSQA, we also aim to evaluate the models' capability of situation awareness through embodied AI tasks such as navigation. To separate long-horizon planning from situated understanding, we propose the MSNN task, which focuses on predicting the best immediate next step action grounded by the current situation and navigation target in a 3D scene. Specifically, given the agent's current interleaved multi-modal description of the situation (_i.e._, location, orientation, and text description), textual goal description, and the overall scene, we instruct models to answer the immediate next action for navigating to the goal in a textual form. For evaluation, we generate MSNN data following a pipeline similar to situated QA pair generation with four critical components: (1) starting situation sampling, (2) goal sampling, (3) optimal trajectory prediction, and (4) calculation of ground truth immediate next-step action. The optimal trajectory is sampled by running an A* algorithm planning the shortest path from the starting location to the goal on the floor plan and the immediate next-step action is determined by following the direction of optimal trajectory relative to the starting situation. In total, we generate a dataset comprising 34K MSNN data samples across 378 3D scenes in ScanNet. This dataset is further utilized for supervised fine-tuning and MSNN evaluation. We provide more details on MSNN data generation and data statistics in the _Appendix_.

## 5 Experiment

### Model Settings

Inspired by recent advancement in 3D generalist models, LLMs and VLMs, we propose several potential approaches for MSQA and MSNN including models that can be directly applied to these tasks in a zero-shot setting, and models that require instruction tuning.

Figure 5: **The generation pipeline of the multi-modal situated next-step navigation (MSNN) task. We follow a generation pipeline similar to QA pairs for situated navigation action.**Zero-shot modelsWe investigate the ability of existing LLMs and VLMs (_i.e._, GPT-3.5  and GPT-4o ) for multi-modal situated reasoning. Recognizing the limitation of these models in handling 3D point clouds, we provide these models with textual descriptions of the 3D scenes as inputs. Specifically, the scene is described as a collection of objects, with each object characterized by its category, location, size, and attributes. This textual description of the scene is then integrated with the interleaved multi-modal situation descriptions, instructions, and questions, and further processed by the LLM or VLM. For text-only models (_i.e._, LLMs), we substitute images of objects with their corresponding object categories as model input. We also incorporate Claude-3.5-Sonnet  to eliminate the potential bias within the GPT family.

Instruction tuningFollowing recent advancement in 3D generalist models , we fine-tune existing 3D vision-language foundation models on MSQA and MSNN. In particular, we choose LEO  as a representative model given its superior performance in 3D VL understanding and reasoning. Since LEO does not naturally support interleaved multi-modal input, we modify the input by replacing the input images with their corresponding object categories similar to zero-shot models. Additionally, we extend LEO to accommodate the interleaved multi-modal input setting, resulting in our strong baseline model, MSR3D, tailored for situated reasoning and navigation. MSR3D deliberately models the situation by translating and rotating the point cloud input conditioned on the agent's situation. We choose MSR3D as our primary model for subsequent ablation studies and analyses. We provide more details on the design of MSR3D in Appendix C.

### Evaluation Results

In this section, we provide evaluation results of models on MSQA and MSNN. We report the average correctness score (as illustrated in Sec. 4) across test sets for both tasks. Additionally, we consider different settings on the modality of the situation and question input (_Input_), the representation of 3D scenes (_Scene_), and the model setting (_Setting_). For MSNN, we ablate the choice of pre-training data (_PT data_) as an additional axis to verify the usefulness of MSQA for embodied tasks.

#### 5.2.1 Multi-modal Situated Question Answering (MSQA)

We present the experimental results of MSQA in Tab. 2 and report the following findings:

**Zero-shot models struggle in situated spatial reasoning.** Zero-shot models excel in answering commonsense questions, such as those related to affordance and room type (categorized as _Other_), likely due to LLMs' proficiency in natural language tasks. Given that object attributes are provided in the list, these models show superior performance in attributes and descriptions compared to fine-tuned models. However, they fall short in addressing spatial relationships and navigation questions, highlighting their limitations in multi-modal situated reasoning.

**Situation modeling matters in situated spatial reasoning.** 3D vision-language models like LEO struggle without fine-tuning on MSQA, reflecting its limitations as a generalise foundation model. Our model trained without interleaved input outperforms LEO on spatial relationships and navigation, highlighting the importance of our situation modeling method. Meanwhile, the performance of MSR3D declines sharply in fine-tuning without 3D scene input (blind). This underscores the importance of situation awareness and 3D scene understanding in addressing MSQA.

**3D point cloud is a better scene representation compared to textual descriptions.** We conduct an additional experiment with solely textual descriptions, which are derived by prompting GPT-3.5 based on situated scene graphs. The situations used for generating textual descriptions are the same as those for QA pairs in MSQA. See examples of textual descriptions in Appendix A.3. The results in Tab. 2 (row "DES") indicate a notable drop when provided with textual descriptions, especially in object attribute, spatial relation, and navigation. To proceed, we probe the reason why "DES" shows better performance in counting. As shown in Tab. 3, "DES" is better for GT\(<3\) but worse for GT\( 3\). This is intuitive since "DES" explicitly depicts the target objects in the input. However, when the count of target objects exceeds a certain threshold, some target objects are likely to be truncated due to limited context length. In summary, the results demonstrate that the 3D point cloud serves as a more efficient representation for situated reasoning compared to textual descriptions.

**Situation component matters for situated reasoning.** To reveal the effectiveness of the situation for FT models, we add an FT baseline with the situation component entirely removed, retaining the 3D scene and question as input. The results in Tab. 2 (w/o situation) show a notable drop in performancesafter removing the situation component. In particular, the drop in questions related to navigation is more salient, which echoes the evaluation results in MSNN and highlights the importance of the situation component. More analyses can be found in Appendix D.3.

**Interleaved multi-modal input introduces new challenges for situated reasoning.** Despite the advantages of interleaved multi-modal input, we observe that MSR3D (T+I) shows a slightly inferior performance compared to text-only input (T). To investigate this subtle difference, we extract two subsets from the test set by making the images only appear in either situation or question. The evaluation results on these two subsets are reported in Tab. 4, which indicate that "T+I" suffers a significant drop in the subset where the images only appear in question. We conjecture that incorporating images in question strengthens the challenge of situated reasoning probably because identifying the queried objects from images requires extra grounding ability.

#### 5.2.2 Multi-modal Situated Next-step Navigation (MSNN)

We present the experimental results of MSNN in Tab. 5 and report the following findings:

   Input & S w/ ing, **q** w/o img & **S** w/o img, **q** w/ img \\  T & 55.54 & 56.41 \\ T+I & 56.48 & 43.58 \\ \(\) & 0.94 & -12.83 \\   

Table 4: Interleaved multi-modal input (T+I) _vs_ text-only input (T), on subsets where images appear in either situation (**S**) or question (**q**).

   Model & _Input_ & _Scene_ & _Setting_ & _Count._ & _Exist._ & _Attr._ & _Spatial_ & _Navi._ & _Others_ & Overall \\  GPT-3.5 & T & OBJ & zero-shot & 34.84 & 74.48 & **75.77** & 27.86 & 42.95 & 88.02 & 50.65 \\ GPT-do & T+I & OBJ & zero-shot & 31.20 & 71.41 & 75.21 & 31.50 & 36.67 & **88.03** & 49.68 \\ Claude-3.5-Sonnet & T+I & OBJ & zero-shot & 32.57 & 66.28 & 69.88 & 30.10 & 45.48 & 83.61 & 49.73 \\   & T & PCD & zero-shot & 0.79 & 15.51 & 11.83 & 7.27 & 2.31 & 15.34 & 7.84 \\  & T & PCD & FT & **36.22** & **88.46** & 51.66 & 46.88 & 56.82 & 80.25 & 55.86 \\   & T+I & PCD & FT (blind) & 11.91 & 31.02 & 20.57 & 22.26 & 25.77 & 34.45 & 22.92 \\  & T & PCD & FT & 33.46 & 87.45 & 53.65 & **48.91** & 61.89 & 75.00 & **56.48** \\   & T+I & PCD & FT & 33.46 & 86.28 & 50.88 & 42.79 & **62.56** & 73.31 & 54.13 \\   & T+I & DES & FT & 35.82 & 88.20 & 43.91 & 35.52 & 52.42 & 73.10 & 50.05 \\   & T+I & PCD & FT (w/o situation) & 30.78 & 85.51 & 45.35 & 42.66 & 52.97 & 71.00 & 51.20 \\   

Table 2: **Experimental results on MSQA. We use _Attr._ for question categories including object attributes and descriptions, _Spatial_ for spatial relationship and object referral, and _Others_ for affordance and room type. “FT” denotes models fine-tuned on MSQA, “T+I” for the interleaved text-image input, and “PCD/OBJ” for scene point cloud and object attribute list, respectively. _Input_ includes both situation and question except the last row.**

   Input & Overall & GT=1 & GT=2 & GT=3 & GT=4 & GT=5 \\  PCD & 33.46 & 21.62 & 61.36 & 36.73 & 3.22 & 14.29 \\ DES & 35.82 & 32.43 & 82.95 & 10.20 & 0 & 0 \\ \(\) & 2.36 & 10.81 & 21.59 & -26.53 & -3.22 & -14.29 \\   

Table 3: Comparison of counting performance between point cloud input (PCD) and textual description input (DES).

Figure 6: **Qualitative visualization on MSQA. Top left: spatial relationship. Top right: navigation. Bottom left: object existence. Bottom right: object refer.**

**MSNN is challenging.** The results in Tab. 5 indicate that both state-of-the-art LLMs (_i.e._, GPT-3.5 and GPT-4o) and 3D VL models encounter considerable challenges in solving MSNN. This implies the value of the proposed MSNN task for 3D situated reasoning and embodied AI.

**MSQA is beneficial as a pretraining source for embodied AI.** We find that adopting MSQA for pretraining (both LEO and MSR3D) significantly improves the performances on MSNN, which indicates the effectiveness of MSQA as a pretraining source for addressing embodied navigation.

**Situation modeling of MSR3D is effective.** We find that MSR3D (T), endowed with situation modeling, shows a significantly higher accuracy in navigation action prediction (+8.56\(\%\)) compared with LEO (T). This demonstrates the effectiveness of our situation modeling method. Additionally, we test MSR3D without situation by masking the location and orientation of the agent, which leads to a great performance drop as shown in Tab. 5 (w/o situation). Such a drop demonstrates the critical role of situation information and that MSR3D can utilize the situation information well.

### Additional Analysis

Scaling effectWe explore the scaling effect on MSQA by training MSR3D with different data scales. We investigate three factors for scaling: QA (randomly downsampling QA pairs), situation (downsampling both QA pairs and situations), and scene (downsampling both QA pairs and scenes). As shown in Fig. 7, we observe a consistent trend of improvement when scaling up along the three factors, which exhibits a significant scaling effect and manifests the potential of further scaling up. We also provide additional analysis of the scaling effect on the MSNN task in Appendix D.1.

**Cross-domain transfer** We divide the MSQA data into three subsets according to the scene domain: ScanNet , 3RScan  and ARKitScenes . Then we investigate cross-domain transfer by training MSR3D on each subset and evaluating on all the subsets, respectively. The results in Tab. 6 show that the best performance on each subset is achieved by in-domain training (**bold**) rather than cross-domain transfer, showcasing the domain gap. And training on ARKitScenes elicits inferior cross-domain transfer results. Considering the relatively simple scenes in ARKitScenes, it implies that training on complex scenes would be beneficial for cross-domain generalization.

## 6 Conclusion

In this paper, we introduce Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset collected with a scalable data generation pipeline. MSQA comprises 251K situated QA pairs across a variety of real-world scenes, presented in a unified format with interleaved text, images, and point clouds. We present a challenging benchmark based on MSQA for evaluating multi-modal situated reasoning in 3D scenes. Additionally, we propose Multi-modal Situated Next-step Navigation (MSNN), a task to assess the capability of situated reasoning and embodied navigation. Our comprehensive experiments highlight the value of our dataset and benchmarks. We hope this work will advance the development of situated scene understanding and embodied AI.

    & ScanNet & 3RScan & ARKitScenes \\  ScanNet & **55.40** & 43.33 & 52.16 \\
3RScan & 50.15 & **45.08** & 55.35 \\ ARKitScenes & 40.08 & 40.07 & **63.34** \\   

Table 6: **Experiments on cross-domain transfer. The left column denotes the source domain for training, and the top row denotes the target domain to transfer.**

   Model & _Input_ & _Scene_ & _Settings_ & _PT data_ & Accuracy \\  GPT-3.5 & T & OBJ & zero-shot & – & 20.1 \\ GPT-4o & T+1 & OBJ & zero-shot & – & 27.55 \\   & T & PCD & FT & LEO-align & 31.44 \\  & T & PCD & FT & MSQA & 37.05 \\   & T+1 & PCD & FT & – & 45.46 \\  & T & PCD & FT & MSQA & 45.61 \\   & T+1 & PCD & FT (w/o situation) & MSQA & 31.66 \\   & T+1 & PCD & FT & MSQA & **48.4** \\   

Table 5: **Experimental results on MSNN. We use the same notations for input, scene, and settings following Tab. 2. LEO-align is the pre-training dataset proposed in .**Limitations and future work.Our work proposes an automatic pipeline to scale up multi-modal situated reasoning data based on existing real-world 3D assets. We also introduce an innovative evaluation task MSNN for situated reasoning and embodied navigation. Despite our contributions, some limitations remain to be addressed.

Firstly, LLM-generated data needs further alignment with human preference to achieve higher data quality. Despite our meticulous design in refinement procedures and data balance, some unnatural data remains due to the rule-based scene graph and biases of LLMs. For instance, LLMs may select distant objects for situational descriptions, which might be an improbable behavior for humans. We encourage further exploration of human feedback integration in the data generation process to better align with human preference.

Secondly, we have not yet fully leveraged the available 3D assets. Expanding our data generation pipeline to cover more real-world and synthetic 3D scenes will further enhance the scale and diversity of the situated reasoning data, probably inducing stronger models. Given the expense of creating large-scale QA pairs by prompting LLMs with situated scene graphs, we anticipate that training a specific LLM tailored for generating QA pairs from situated scene graphs could substantially reduce the cost of data generation. We leave this path for future research.

Finally, the evaluation tasks for assessing situational awareness and situated reasoning should not be confined to question answering and action prediction. For example, some other tasks focusing on scene understanding like object grounding could also be considered. We would explore more evaluation suites in future work.