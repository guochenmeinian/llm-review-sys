# Appendix

[MISSING_PAGE_FAIL:1]

State representation learning has become an essential aspect of RL research, aiming to improve sample efficiency and enhance agent performance. Initial advancements like CURL  utilized a self-supervised contrastive InfoNCE objective  for state representation, yet it overlooks the temporal dynamics of the environment. Subsequent works, including CPC , ST-DIM , and ATC , made progress to rectify this by integrating temporal elements into the contrastive loss, linking pairs of observations with short temporal intervals. The objective here was to develop a state representation capable of effectively predicting future observations. A more comprehensive approach was taken by DRIML , which incorporated the first action of the action sequence into the temporal contrastive learning framework. However, these methods, while innovative, have their shortcomings. The positive relations in their contrastive loss designs are often policy-dependent, potentially leading to instability during policy updates throughout the training process. Consequently, they lack the theoretical foundation needed to capture all information representing the optimal policy. Furthermore, these methods except for CURL and ATC typically focus on environments such as Atari games with well-represented, abstract discrete action spaces, thereby overlooking the importance of action representation in continuous control tasks . However, by learning an action representation that groups semantically similar actions together in the latent action space, the agent can better generalize its knowledge across various state-action pairs, enhancing the sample efficiency of RL algorithms. Therefore, learning both state and action representations is crucial for enabling the agent to more effectively reason about its actions' long-term outcomes for continuous control tasks.

In this paper, we introduce **T**emporal **A**ction-driven **CO**ntrastive Learning (TACO) as a promising approach to visual continuous control tasks. TACO simultaneously learns a state and action representation by optimizing the mutual information between representations of current states paired with action sequences and representations of the corresponding future states. By optimizing the mutual information between state and action representations, TACO can be theoretically shown to capture the essential information to represent the optimal value function. In contrast to approaches such as DeepMDP  and SPR , which directly model the latent environment dynamics, our method transforms the representation learning objective into a self-supervised InfoNCE objective. This leads to more stable optimization and requires minimal hyperparameter tuning efforts. Consequently, TACO yields expressive and concise state-action representations that are better suited for high-dimensional continuous control tasks.

We demonstrate the effectiveness of representation learning by TACO through extensive experiments on the DeepMind Control Suite (DMC) in both online and offline RL settings. TACO is a flexible plug-and-play module that could be combined with any existing RL algorithm. In the online RL setting, combined with the strong baseline DrQ-v2, TACO significantly outperforms the SOTA model-free visual RL algorithms, and it even surpasses the strongest model-based visual RL baselines such as Dreamer-v3 . As shown in Figure 1, across nine challenging visual continuous control tasks from DMC, TACO achieves a 40% performance boost after one million environment interaction steps on average. For offline RL, TACO can be combined with existing strong offline RL methods to further improve performance. When combined with TD3+BC  and CQL , TACO outperforms the strongest baselines across offline datasets with varying quality.

We list our contributions as follows:

1. We present TACO, a simple yet effective temporal contrastive learning framework that simultaneously learns state and action representations.
2. The framework of TACO is flexible and could be integrated into both online and offline visual RL algorithms with minimal changes to the architecture and hyperparameter tuning efforts.
3. We theoretically show that the objectives of TACO is sufficient to capture the essential information in state and action representation for control.
4. Empirically, we show that TACO outperforms prior state-of-the-art model-free RL by 1.4x on nine challenging tasks in Deepmind Control Suite. Applying TACO to offline RL with SOTA

Figure 1: Comparison of average episode reward across nine challenging tasks in Deepmind Control Suite after one million environment steps.

algorithms also achieves significant performance gain in 4 selected challenging tasks with pre-collected offline datasets of various quality.

## 2 Preliminaries

### Visual reinforcement learning

Let \(=,,,,\) be a Markov Decision Process (MDP). Here, \(\) is the state space, and \(\) is the action space. The state transition kernel is denoted by \(:()\), where \(()\) is a distribution over the state space. \(:\) is the reward function. The objective of the Reinforcement Learning (RL) algorithm is the identification of an optimal policy \(^{*}:()\) that maximizes the expected value \(_{}[_{t=0}^{}^{t}r_{t}]\). Additionally, we can define the optimal \(\) function as follows: \(Q^{*}(s,a)=E_{^{*}}_{t=0}^{}^{t}r(s_{t},a_{t})|s_{0 }=s,a_{0}=a\), such that the relationship between the optimal \(\) function and optimal policy is \((s)=_{a}Q^{*}(s,a)\). In the domain of visual RL, high-dimensional image data are given as state observations, so the simultaneous learning of both representation and control policy becomes the main challenge. This challenge is exacerbated when the environment interactions are limited and the reward signal is sparse.

### Contrastive learning and the InfoNCE objective

Contrastive Learning, a representation learning approach, imposes similarity constraints on representations, grouping similar/positive pairs and distancing dissimilar/negative ones within the representation space. Contrastive learning objective is often formulated through InfoNCE loss  to maximize the mutual information between representations of positive pairs by training a classifier. In particular, let \(X,Y\) be two random variables. Given an instance \(x p(x)\), and a corresponding positive sample \(y^{+} p(y|x)\) as well as a collection of \(Y=\{y_{1},...,y_{N-1}\}\) of \(N-1\) random samples from the marginal distribution \(p(y)\), the InfoNCE loss is defined as

\[_{N}=_{x})}{_{y Y \{y^{+}\}}f(x,y)}\] (1)

Optimizing this loss will result in \(f(x,y)\) and one can show that InfoNCE loss upper bounds the mutual information, \(_{N}(N)-(X,Y)\).

## 3 \(\): temporal action-driven contrastive Loss

\(\) is a flexible temporal contrastive framework that could be easily combined with any existing RL algorithms by interleaving RL updates with its temporal contrastive loss update. In this section, we first presnt the overall learning objective and theoretical analysis of \(\). Then we provide the architectural design of \(\) in detail.

### Temporal contrastive learning objectives and analysis

In the following, we present the learning objectives of \(\). The guiding principle of our method is to learn state and action representations that capture the essential information about the environment's dynamics sufficient for learning the optimal policy. This allows the agent to develop a concise and expressive understanding of both its current state and the potential effects of its actions, thereby enhancing sample efficiency and generalization capabilities.

Let \(S_{t}\), \(A_{t}\) be the state and action variables at timestep \(t\), \(Z_{t}=(S_{t})\), \(U_{t}=(A_{t})\) be their corresponding representations. Then, our method aims to maximize the mutual information between representations of current states paired with action sequences and representations of the corresponding future states:

\[_{}=(Z_{t+K};[Z_{t},U_{t},...,U_{t+K-1}])\] (2)

Here, \(K 1\) is a fixed hyperparameter for the prediction horizon. In practice, we estimate the lower bound of the mutual information by the InfoNCE loss, with details of our practical implementation described in SS3.2.

We introduce the following theorem extending from Rakely et al.  to demonstrate the sufficiency of TACO objective:

**Theorem 3.1**.: _Let \(K^{+}\), and \(_{}=(Z_{t+K};[Z_{t},U_{t},...,U_{t+K-1}])\). If for a given state and action representation \(_{Z},_{U}\), \(_{}\) is maximized, then for arbitrary state-action pairs \((s_{1},a_{1}),(s_{2},a_{2})\) such that \((s_{1})=(s_{2}),(a_{1})=(a_{2})\), it holds that \(Q^{*}(s_{1},a_{1})=Q^{*}(s_{2},a_{2})\)._

This theorem guarantees that if our mutual information objective Equation (2) is maximized, then for any two state-action pairs \((s_{1},a_{1})\) and \((s_{2},a_{2})\) with equivalent state and action representations, their optimal action-value functions, \(Q^{*}(s_{1},a_{1})\) and \(Q^{*}(s_{2},a_{2})\), will be equal. In other words, maximizing this mutual information objective ensures that the learned representations are sufficient for making optimal decisions.

### TACO implementation

Here we provide a detailed description of the practical implementation of TACO. In Figure 2, we illustrate the architecture design of TACO. Our approach minimally adapts a base RL algorithm by incorporating the temporal contrastive loss as an auxiliary loss during the batch update process. Specifically, given a batch of state and action sequence transitions \(\{(s_{t}^{(i)},[u_{t}^{(i)},...,u_{t^{}-1}^{(i)}],s_{t^{}}^{(i)}) \}_{i=1}^{N}\) (\(t^{}=t+K\)), we optimize:

\[_{}(,,W,G_{},H_{})=- _{i=1}^{N}^{(i)}}^{}_{t}{W}h_{t^{}}^{(i)}}{{ _{j=1}^{N}{g_{t}^{(i)}}^{}{W}h_{t^{}}^{(j)}}}\] (3)

Here let \(z_{t}^{(i)}=(s_{t}^{(i)})\), \(u_{t}^{(i)}=(a_{t}^{(i)})\) be state and action embeddings respectively. \(g_{t}^{(i)}=G_{}(z_{t}^{(i)},u_{t}^{(i)},...,u_{t^{}-1}^{(i)})\), and \(h_{t}^{(i)}=H_{}(z_{t^{}}^{(i)})\), where \(G_{}\) and \(H_{}\) denote two learnable projection layers that map the latent state \(z_{t}^{(i)}\) as well as latent state and action sequence \((z_{t}^{(i)},u_{t}^{(i)},...,u_{t^{}-1}^{(i)})\) to a common contrastive embedding space. \(W\) is a learnable parameter providing a similarity measure between \(g_{i}\) and \(h_{j}\) in the shared contrastive embedding space. Subsequently, both state and action representations are fed into the agent's \(Q\) network, allowing the agent to effectively reason about the long-term effects of their actions and better leverage their past experience through state-action abstractions.

In addition to the main TACO objective, in our practical implementation, we find that the inclusion of two auxiliary objectives yields further enhancement in the algorithm's overall performance. The first is the CURL  loss:

\[_{}(,,W,H_{})=-_{i=1}^{ N}^{(i)}}^{}{W}{h_{t}^{(i)}}^{+}}{{h_{t}^{(i)}}^{ }{W}{h_{t}^{(i)}}^{+}+_{j i}{h_{t}^{(i)}}^{}{W}h_{t}^{(j)}}\] (4)

Here, \({h_{t}^{(i)}}^{+}=H_{}(({s_{t}^{(t)}}^{+}))\), where \({s_{t}^{(t)}}^{+}\) is the augmented view of \(s_{i}^{(t)}\) by applying the same random shift augmentation as DrQ-v2 . \(W\) and \(H_{}\) share the same weight as the ones in TACO objective. The third objective is reward prediction:

\[_{}(,,_{})=_{i=1}^{N} _{}(z_{t}^{(i)},u_{t}^{(i)},...,u_{t^{}-1}^{(i)})-r^{(i)} ^{2}\] (5)

Figure 2: A demonstration of our temporal contrastive loss: Given a batch of state-action transition triples \(\{(s_{t}^{(i)},[a_{t}^{(i)},...,a_{t+K-1}^{(i)}],s_{t+K}^{(i)})\}_{i=1}^{N}\), we first apply the state encoder and action encoder to get latent state-action encodings: \(\{(z_{t}^{(i)},[u_{t}^{(i)},...,u_{t+K-1}^{(i)}],z_{t+K}^{(i)})\}_{i=1}^{N}\). Then we apply two different projection layers to map \((z_{t}^{(i)},[u_{t}^{(i)},...,u_{t+K-1}^{(i)}])\) and \(z_{t+K}^{(i)}\) into the shared contrastive embedding space. Finally, we learn to predict the correct pairings between \((z_{t},[u_{t},...,u_{t+K-1}])\) and \(z_{t+K}\) using an InfoNCE loss.

Here \(r^{(i)}=_{j=t}^{t^{}-1}r_{j}^{(i)}\) is the sum of reward from timestep \(t\) to \(t^{}-1\), and \(_{}\) is a reward prediction layer. For our final objective, we combine the three losses together with equal weight. As verified in Section 4.1, although TAC0 serves as the central objective that drives notable performance improvements, the inclusion of both CURL and reward prediction loss can further improve the algorithm's performance.

We have opted to use DrQ-v2  for the backbone algorithm of TAC0, although in principle, TAC0 could be incorporated into any visual RL algorithms. TAC0 extends DrQ-v2 with minimal additional hyperparameter tuning. The only additional hyperparameter is the selection of the prediction horizon \(K\). Throughout our experiments, we have limited our choice of \(K\) to either 1 or 3, depending on the nature of the environment. We refer the readers to Appendix A for a discussion on the choice of \(K\).

## 4 Experiments and results

This section provides an overview of our empirical evaluation, conducted in both online and offline RL settings. To evaluate our approach under online RL, we apply TAC0 to a set of nine challenging visual continuous control tasks from Deepmind Control Suite (DMC) . Meanwhile, for offline RL, we combine TAC0 with existing offline RL methods and test the performance on four DMC tasks, using three pre-collected datasets that differ in the quality of their data collection policies.

### Comparison between TAC0 and strong baselines in online RL tasks

**Environment Settings**: In our online RL experiment, we first evaluate the performance of TAC0 on nine challenging visual continuous control tasks from Deepmind Control Suite : Quadruped Run, Quadruped Walk, Hopper Hop, Reacher Hard, Walker Run, Acrobot Swingup, Cheetah Run, Finger Turn Hard, and Reach Duplo. These tasks demand the agent to acquire and exhibit complex motor skills and present challenges such as delayed and sparse reward. As a result, these tasks have not been fully mastered by previous visual RL algorithms, and require the agent to learn an effective policy that balances exploration and exploitation while coping with the challenges presented by the tasks.

In addition to Deepmind Control Suite, we also present the results of TAC0 on additional six challenging robotic manipulation tasks from Meta-world : Hammer, Assembly, Disassemble, Stick Pull, Pick Place Wall, Hand Insert. Unlike the DeepMind Control Suite, which primarily concentrates on locomotion tasks, Meta-world domain provides tasks that involve complex manipulation and interaction tasks. This sets it apart by representing a different set of challenges, emphasizing precision and control in fine motor tasks rather than broader locomotion skills. In Appendix G, we provide a visualization for each Meta-world task.

**Baselines**: We compare TAC0 with four model-free visual RL algorithms **CURL**, **DrQ**, **DrQ-v2**, and **A-LIX**. **A-LIX** builds on **DrQ-v2** by adding adaptive regularization to the encoder's gradients. While TAC0 could also extend **A-LIX**, our reproduction of results from its open-source implementation does not consistently surpass **DrQ-v2**. As such, we do not choose **A-LIX** as the backbone algorithm for TAC0. Additionally, we compare with two state-of-the-art model-based RL algorithms for visual continuous control, **Dreamer-v3** and **TDMPC**, which learn world models in latent space and select actions using either model-predictive control or a learned policy.

TACO **achieves a significantly better sample efficiency and performance compared with SOTA visual RL algorithm.** The efficacy of TAC0 is evident from the findings presented in Figure 4 (DMC), Table 1 (DMC), and Figure 5 (Meta-world). In contrast to preceding model-free visual RL algorithms, TAC0 exhibits considerably improved sample efficiency. For example, on the challenging Reacher Hard task, TAC0 achieves optimal performance in just 0.75 million environment steps, whereas DrQ-v2 requires approximately 1.5 million steps. When trained with only 1 million environment steps, TAC0 on average achieves 40% better performance, and it is even better than the model-based visual RL algorithms Dreamer-v3 and TDMPC on 6 out of 9 tasks. In addition, on more demanding tasks such as Quadruped Run, Hopper Hop, Walker Run, and Cheetah Run, TAC0 continues to outshine competitors, exhibiting superior overall performance after two or three million steps, as illustrated in Figure 4. For robotic manipulation tasks, as shown in Figure 5, TAC0 also significantly outperform the baseline model-free visual RL algorithms, highlighting the broad applicability of TAC0.

**Concurrently learning state and action representation is crucial for the success of TACO.** To demonstrate the effectiveness of action representation learning in TACO, we evaluate its performance on a subset of 4 difficult benchmark tasks and compare it with a baseline method without action representation, as shown in Figure 3. The empirical results underscore the efficacy of the temporal contrastive learning objectives, even in the absence of action representation. For instance, TACO records an enhancement of 18% on Quadruped Run and a substantial 51% on Reacher Hard, while the remaining tasks showcase a performance comparable to DrQ-v2. Furthermore, when comparing against TACO without action representation, TACO achieves a consistent performance gain, ranging from 12.2% on Quadruped Run to a significant 70.4% on Hopper Hop. These results not only emphasize the inherent value of the temporal contrastive learning objective in TACO, but also underscore the instrumental role of high-quality action representation in bolstering the performance of the underlying RL algorithms.

**TACO learns action representations that group semantically similar actions together.** To verify that indeed our learned action representation has grouped semantically similar actions together, we conduct an experiment within the Cheetah Run task. We artificially add 20 dimensions to the action

   \\  Environment (1M Steps) & TACO & **DrQv2** & **A-LIX** & **DrQ** & **CURL** & **Dreamer-v3** & **TDMPC** \\  Quadruped Run & \(\) & \(407 21\) & \(454 42\) & \(179 18\) & \(181 14\) & \(331 42\) & \(397 37\) \\ Hopper Hop & \(261 52\) & \(189 35\) & \(225 13\) & \(192 41\) & \(152 34\) & **369\(\)** & \(159 18\) \\ Walker Run & \(637 11\) & \(517 43\) & \(617 12\) & \(451 73\) & \(387 24\) & \( 32\) & \(600 28\) \\ Quadruped Walk & \(\) & \(680 52\) & \(560 175\) & \(120 17\) & \(123 11\) & \(353 27\) & \(435 16\) \\ Cheetah Run & \(\) & \(691 42\) & \(676 41\) & \(474 32\) & \(657 35\) & \(728 32\) & \(565 61\) \\ Finger Turn Hard & \(632 75\) & \(220 21\) & \(62 54\) & \(91 9\) & \(215 17\) & \( 58\) & \(400 113\) \\ Acrobot Swingup & \(\) & \(128 8\) & \(112 23\) & \(24 8\) & \(5 1\) & \(210 12\) & \(224 20\) \\ Reacher Hard & \(\) & \(572 51\) & \(510 16\) & \(471 45\) & \(400 29\) & \(499 51\) & \(485 31\) \\ Reach Duplo & \(\) & \(206 32\) & \(199 14\) & \(36 7\) & \(8 1\) & \(119 30\) & \(117 12\) \\  

Table 1: Episode reward of TACO and SOTA visual RL algorithms on the image-based DMControl 1M benchmark. Results are averaged over 6 random seeds. Within the table, entries shaded represent the best performance of model-free algorithms, while text in **bold** signifies the highest performance across all baseline algorithms, including model-based algorithms.

Figure 4: **(Deepmind Control Suite) Performance of TACO against two strongest model-free visual RL baselines. Results of DrQ-v2 and A-LIX are reproduced from their open-source implementations, and all results are averaged over 6 random seeds.**

Figure 3: 1M Performance of TACO with and without action representation with and without action representation without the Cheetah Run task. We artificially add 20 dimensions to the actionspace of task Cheetah Run, although only the first six were utilized in environmental interactions. We first train an agent with TACO online to obtain the action representation. Then we select four actions within the original action space \(a_{1},a_{2},a_{3},a_{4}\) to act as centroids. For each of the four centroids, we generate 1000 augmented actions by adding standard Gaussian noises to the last 20 dimensions. We aim to determine if our action representation could disregard these "noisy" dimensions while retaining the information of the first six. Using t-SNE for visualization, we embed the 4000 actions before and after applying action representation. As shown in Figure 6, indeed our learned action representation could group the four clusters, demonstrating the ability of our action representation to extract control relevant information from the raw action space.

**The effectiveness of our temporal contrastive loss is enhanced with a larger batch size.** As is widely acknowledged in contrastive learning research , our contrastive loss sees significant benefits from utilizing a larger batch size. In Figure 6(a), we illustrate the performance of our algorithms alongside DrQv2 after one million environment steps on the Quadruped Run task. As evident from the plot, batch size greatly influences the performance of our algorithm, while DrQ-v2's baseline performance remains fairly consistent throughout training. In order to strike a balance between time efficiency and performance, we opt for a batch size of 1024, which is 4 times larger than the 256 batch size employed in DrQ-v2, but 4 times smaller than the 4096 which is commonly used in the contrastive learning literature . For an analysis on how batch size affects the algorithm's runtime, we direct the reader to Appendix B.

**Reward prediction and CURL loss serves an auxiliary role to further improve the performance of TACO, while the temporal contrastive loss of TACO is the most crucial component.** In the practical deployment of TACO, two additional objectives, namely reward prediction and CURL loss,

Figure 5: (**Meta-world**) Performance of TACO against DrQ-v2 and A-LIX. All results are averaged over 6 random seeds.

Figure 6: **Left**: t-SNE embedding of actions with distracting dimensions. **Right**: t-SNE embedding of latent representations for actions with distracting dimensions.

are incorporated to enhance the algorithm's performance. In Figure 6(b), we remove one objective at a time on the Quadruped Run task to assess its individual impact on the performance after one million environment steps. As illustrated in the figure, the omission of TACO' temporal contrastive objective results in the most significant performance drop, emphasizing its critical role in the algorithm's operation. Meanwhile, the auxiliary reward prediction and CURL objectives, although secondary, contribute to performance improvement to some degree.

**InfoNCE-based temporal action-driven contrastive objective in TACO outperforms other representation learning objectives including SPR , ATC , and DRIML **. In Table 2, we have showcased a comparison between our approach and other visual RL representation learning objectives such as **SPR**, **ATC**, and **DRIML**. Given that **SPR** and **DRIML** were not initially designed for continuous control tasks, we have re-implemented their learning objectives using the identical backbone algorithm, DrQ-v2. A similar approach was taken for ATC, with their learning objectives also being reimplemented on DrQ-v2 to ensure a fair comparison. (Without the DrQ-v2 backbone algorithm, the performance reproduced by their original implementation is significantly worse.) Furthermore, recognizing the significance of learning action encoding, as discussed earlier, we have integrated action representation learning into all these baselines. Therefore, the model architecture remains consistent across different representation learning objectives, with the sole difference being the design of the temporal contrastive loss. For **DRIML**, given that only the first action of the action sequence is considered in the temporal contrastive loss, TACO and **DRIML** differ when the number of steps \(K\) is greater than one. Thus, we indicate N/A for tasks where we choose \(K=1\) for TACO.

Table 2 showcases that while previous representation learning objectives have proven benefit in assisting the agent to surpass the DrQ-v2 baseline by learning a superior representation, our approach exhibits consistent superiority over other representation learning objectives in all five evaluated environments. These results reinforce our claim that TACO is a more effective method for learning state-action representations, allowing agents to reason more efficiently about the long-term outcomes of their actions in the environment.

### Combining TACO with offline RL algorithms

In this part, we discuss the experimental results of TACO within the context of offline reinforcement learning, emphasizing the benefits our temporal contrastive state/action representation learning objective brings to visual offline RL. Offline visual reinforcement learning poses unique challenges, as algorithms must learn an optimal policy solely from a fixed dataset without further interaction with the environment. This necessitates that the agent effectively generalizes from limited data while handling high-dimensional visual inputs. The state/action representation learning objective of TACO plays a vital role in addressing these challenges by capturing essential information about the environment's dynamics, thereby enabling more efficient generalization and improved performance. TACO can be easily integrated as a plug-and-play module on top of existing strong offline RL methods, such as **TD3+BC** and **CQL**.

For evaluation, we select four challenging visual control tasks from DMC: Hopper Hop, Cheetah Run, Walker Run, and Quadruped Run. For each task, we generate three types of datasets. The **medium** dataset consists of trajectories collected by a single policy of medium performance. The precise definition of "medium performance" is task-dependent but generally represents an intermediate level of mastery, which is neither too poor nor too proficient. The **medium-replay** dataset contains trajectories randomly sampled from the online learning agent's replay buffer before it reaches a medium performance level. The **full-replay** dataset includes trajectories randomly sampled throughout the online learning phase, from the beginning until convergence. The dataset size for Walker, Hopper, and Cheetah is 100K, while for the more challenging Quadruped Run task, a larger dataset size of 500K is used to account for the increased difficulty. We compute the normalized reward by diving the offline RL reward by the best reward we get during online TACO training.

   Environment & TACO & **SPR** & **ATC** & **DRIML** & **DrQ-v2** \\  Quadruped Run & \(\) & \(448 79\) & \(432 54\) & **N/A** & \(407 21\) \\ Walker Run & \(\) & \(560 71\) & \(502 171\) & **N/A** & \(517 43\) \\ Hopper Hop & \(\) & \(154 10\) & \(112 98\) & \(216 13\) & \(192 41\) \\ Reacher Hard & \(\) & \(711 92\) & \(863 12\) & \(835 72\) & \(572 51\) \\ Acrobot Swingup & \(\) & \(198 21\) & \(206 61\) & \(222 39\) & \(210 12\) \\   

Table 2: Comparison with other objectives including SPR , ATC , and DRIML We compare the performance of **TD3+BC** and **CQL** with and without TACO on our benchmark. Additionally, we also compare with the decision transformer (**DT**) , a strong model-free offline RL baseline that casts the problem of RL as conditional sequence modeling, **IQL**, another commonly used offline RL algorithm, and the behavior cloning (**BC**) baseline. For **TD3+BC**, **CQL** and **IQL**, which were originally proposed to solve offline RL with vector inputs, we add the their learning objective on top of DrQ-v2 to handle image inputs.

Table 3 provides the normalized reward for each dataset. The results underscore that when combined with the strongest baselines **TD3+BC** and **CQL**, TACO achieves consistent performance improvements across all tasks and datasets, setting new state-of-the-art results for offline visual reinforcement learning. This is true for both the medium dataset collected with a single policy and narrow data distribution, as well as the medium-replay and replay datasets with a diverse distribution.

## 5 Related work

### Contrastive learning in visual reinforcement learning

Contrastive learning has emerged as a powerful technique for learning effective representations across various domains, particularly in computer vision [47; 8; 22; 23; 49]. This success is attributed to its ability to learn meaningful embeddings by contrasting similar and dissimilar data samples. In visual reinforcement learning, it's used as a self-supervised auxiliary task to improve state representation learning, with InfoNCE  being a popular learning objective. In CURL , it treats augmented states as positive pairs, but it neglects the temporal dependency of MDP. CPC , ST-DIM , and ATC  integrate temporal relationships into the contrastive loss by maximizing mutual information between current state representations (or state histories encoded by LSTM in CPC) and future state representations. However, they do not consider actions, making positive relationships in the learning objective policy-dependent. DRIML  addresses this by maximizing mutual information between state-action pairs at the current time step and the resulting future state, but its objective remains policy-dependent as it only provides the first action of the action sequence. Besides, ADAT  and ACO  incorporate actions into contrastive loss by labeling observations with similar policy action outputs as positive samples, but these methods do not naturally extend to tasks with non-trivial continuous action spaces. A common downside of these approaches is the potential for unstable encoder updates due to policy-dependent positive relations. In contrast, TACO is theoretically sufficient, and it tackles the additional challenge of continuous control tasks by simultaneously learning state and action representations.

In addition to the InfoNCE objective, other self-supervised learning objective is also proposed. Approahces such as DeepMDP , SPR , SGI , and EfficientZero  direct learn a latent-space transition model. Notably, these methods predominantly target Atari games characterized by their small, well-represented, and abstract discrete action spaces. When dealing with continuous control tasks, which often involve a continuous and potentially high-dimensional action space, the relationships between actions and states become increasingly intricate. This complexity poses a significant challenge in effectively capturing the underlying dynamics. In contrast, by framing the latent dynamics model predictions as a self-supervised InfoNCE objective, the mutual information

   &  &  &  &  &  &  &  \\   & Medium & \(\) & \(51.2 0.8\) & \(\) & \(46.7 0.2\) & \(40.5 3.6\) & \(2.0 1.7\) & \(48.2 0.8\) \\  & Medium-replay & \(\) & \(62.9 0.1\) & \(\) & \(68.7 0.1\) & \(65.3 1.8\) & \(57.6 1.4\) & \(25.9 3.2\) \\  & Full-replay & \(\) & \(83.8 2.3\) & \(\) & \(94.2 2.0\) & \(92.4 0.3\) & \(47.7 2.2\) & \(65.7 2.7\) \\   & Medium & \(\) & \(66.1 0.3\) & \(\) & \(66.7 1.7\) & \(64.3 0.7\) & \(1.7 1.1\) & \(62.9 0.1\) \\  & Medium-replay & \(\) & \(61.1 0.1\) & \(\) & \(67.3 1.1\) & \(67.0 0.6\) & \(26.5 3.2\) & \(48.0 3.1\) \\  & Full-replay & \(\) & \(91.2 0.8\) & \(\) & \(65.0 3.9\) & \(89.6 1.4\) & \(14.6 3.7\) & \(69.0 0.3\) \\   & Medium & \(\) & \(48.0 0.2\) & \(\) & \(49.4 0.9\) & \(47.3 0.3\) & \(4.4 0.4\) & \(46.2 0.6\) \\  & Medium-replay & \(\) & \(62.3 0.2\) & \(\) & \(59.9 0.9\) & \(61.7 1.1\) & \(14.4 2.8\) & \(15.8 0.8\) \\  & Full-replay & \(\) & \(84.0 1.6\) & \(\) & \(79.8 0.6\) & \(81.6 0.8\) & \(18.1 3.7\) & \(30.8 1.8\) \\   & Medium & \(\) & \(60.0 0.2\) & \(\) & \(55.9 9.1\) & \(14.6 3.8\) & \(0.8 0.8\) & \(56.2 1.1\) \\  & Medium-replay & \(\) & \(58.1 0.5\) & \(\) & \(61.2 0.9\) & \(19.5 2.2\) & \(58.4 4.4\) & \(51.6 3.3\) \\  & \(\) & \(89.3 0.4\) & \(\) & \(85.2 2.5\) & \(14.5 1.1\) & \(36.3 5.9\) & \(57.6 0.7\) \\   &  &  &  &  &  &  &  \\  

Table 3: Offline Performance (Normalized Reward) for different offline RL methods. Results are averaged over 6 random seeds. \(\) captures the standard deviation over seeds.

guided approach used by TACO is better suited for continuous control task, resulting in more stable optimization and thus better state and action representations.

### Action representation in reinforcement learning

Although state or observation representations are the main focus of prior research, there also exists discussion on the benefits and effects of learning action representations. Chandak et al.  propose to learn a policy over latent action space and transform the latent actions into actual actions, which enables generalization over large action sets. Allshire et al.  introduce a variational encoder-decoder model to learn disentangled action representation, improving the sample efficiency of policy learning. In model-based RL, strategies to achieve more precise and stable model-based planning or roll-out are essential. To this end, Park et al.  propose an approach to train an environment model in the learned latent action space. In addition, action representation also has the potential to improve multi-task learning , where latent actions can be shared and enhance generalization.

## 6 Conclusion

In this paper, we have introduced a conceptually simple temporal action-driven contrastive learning objective that simultaneously learns the state and action representations for image-based continuous control -- TACO. Theoretically sound, TACO has demonstrated significant practical superiority by outperforming SOTA online visual RL algorithms. Additionally, it can be seamlessly integrated as a plug-in module to enhance the performance of existing offline RL algorithms. Despite the promising results, TACO does present limitations, particularly its need for large batch sizes due to the inherent nature of the contrastive InfoNCE objective, which impacts computational efficiency. Moving forward, we envisage two primary directions for future research. Firstly, the creation of more advanced temporal contrastive InfoNCE objectives that can function effectively with smaller data batches may mitigate the concerns related to computational efficiency. Secondly, the implementation of a distributed version of TACO, akin to the strategies employed for DDPG in previous works , could significantly enhance training speed. These approaches offer promising avenues for further advancements in visual RL.

## 7 Acknowledgement

Zheng, Wang, Sun and Huang are supported by National Science Foundation NSF-IIS-FAI program, DOD-ONR-Office of Naval Research, DOD Air Force Office of Scientific Research, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD), Adobe, Capital One and JP Morgan faculty fellowships.