# CLAVE: An Adaptive Framework for Evaluating

Values of LLM Generated Responses

 Jing Yao, Xiaoyuan Yi, Xing Xie

Microsoft Research Asia

{jingyao,xiaoyuanyi,xing.xie}@microsoft.com

Corresponding author

###### Abstract

The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing the values embedded in LLMs' generated responses can help expose their misalignment, but this relies on reference-free value evaluators, e.g. fine-tuned LLMs or closed-source models like GPT-4. Nevertheless, two key challenges emerge in open-ended value evaluation: the evaluator should adapt to changing human value definitions with minimal annotation, against their own bias (_adaptability_); and remain robust across varying value expressions and scenarios (_generalizability_). To handle these challenges, we introduce CLAVE, a novel framework that integrates two complementary LLMs: a large model to extract high-level value concepts from diverse responses, leveraging its extensive knowledge and generalizability, and a small model fine-tuned on these concepts to adapt to human value annotations. This dual-model framework enables adaptation to any value system using \(<\)100 human-labeled samples per value type. We also present ValEval, a comprehensive dataset comprising 13k\(+\) (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the performance of 15\(+\) popular LLM evaluators and fully analyze their strengths and weaknesses. Our findings reveal that CLAVE combining a large prompt-based model and a small fine-tuned one serves as an optimal balance in value evaluation.

## 1 Introduction

The past years have witnessed unprecedented breakthroughs of Large Language Models (LLMs) [1; 2; 3; 4], leading a new wave of AI technology . Despite such progress, these powerful LLMs also pose risks [6; 7], including the generation of socially biased [8; 9], toxic [10; 11] and illegal content [12; 13]. To ensure their responsible development, it is imperative to assess LLMs' potential risks . Nevertheless, existing benchmarks tailored for specific risks gradually become inadequate [15; 16] due to the expanding risk types [17; 18]. Given the correlations between LLMs' values and harmful behaviors [19; 20], assessing their _values_ offers a comprehensive insight into their potential misalignment [21; 22], through moral judgment [23; 24; 25; 26], value questionnaires [27; 28] or generative value evaluation [20; 29; 30]. This work focuses on _generative value evaluation_, which directly deciphers LLMs' values from their responses generated in provocative scenarios. This provides a better measure of LLMs' true value conformity rather than knowledge of values .

However, the open-ended value evaluation paradigm heavily relies on reference-free _value evaluators_, due to the lack of ground truth responses. LLMs equipped with extensive knowledge and advanced capabilities [2; 33] are promising to serve as such evaluators, which have been successfully applied to various Natural Language Generation (NLG) tasks [34; 35; 36]. Existing related works fall into two categories: 1) _prompt-based evaluator_, which adopts strong LLMs as off-the-shelf evaluators to assess texts through meticulous prompt designing [37; 38], benefiting from their remarkableinstruction-following and in-context learning abilities [39; 40]; and 2) _tuning-based evaluator_, which fine-tunes smaller LLMs on datasets specialized in evaluating certain NLG tasks [41; 42; 43; 44]. However, current evaluators face two primary challenges in the context of _human value_ assessment, as shown in Fig. 1 (a). **Challenge 1 (Adaptability)**: human values are diverse and evolving, often cultural, regional and even personalized [45; 46], for which evaluators need to efficiently adapt to these new and dynamic value systems. For instance, the value of '_protecting copyrights_ and 'property' has evolved in the age of LLMs to consider AI-generated content. It is difficult for closed-source LLMs to consistently _align_ their static and biased knowledge with shifting human perspectives, particularly for less popular or customized values, as evidenced by the significant drop in GPT-4's performance in Fig. 1 (a1). **Challenge 2 (Generalizability)**: Evaluators should be robust and generalizable to identify underlying value across varying expressions, diverse and even unforeseen scenarios, as LLMs may be applied to various contexts. For example, the value of '_promoting social security_' should be accurately detected in different security breaches, from financial crimes to drug abuse. However, fine-tuned small LLMs tend to overfit to datasets with specific evaluation schemes, thereby losing generality and robustness  as shown in Fig 1 (a2).

To address these challenges, we argue that large proprietary models and small tuning-based models offer _complementary_ advantages. Hence, we introduce **CLAVE**, a novel framework that integrates two Complementary Language models for Adaptive Value Evaluation. CLAVE links the two complementary LLMs using fundamental _value concepts_, which act as highly generalized indicators of certain values, _e.g._ '_advocating_ for personal choice in life-affecting decisions' represents a concept of the value'self-direction' . Concretely, a large but proprietary LLM as a _concept extractor_, derives representative value concepts from a handful of manually annotated training samples, and then accurately identifies these concepts for testing cases. Leveraging its knowledge and advanced capability, this evaluator is robust to variations of text expressions and scenarios, _addressing challenge 2_. A small LLM is fine-tuned on annotated sample as the _value recognizer_ to make value judgments based on the extracted concepts rather than diverse raw texts, allowing for efficient alignment with human value definitions, _tackling challenge 1_. This dual-model framework, as illustrated in Fig. 1 (b), enables calibration with arbitrary value system with minimal annotation and training cost.

To standardize value evaluation for LLM generated texts, we further present **ValEval**, a comprehensive benchmark comprising 13k\(+\) manually annotated (text, value, label) tuples across diverse scenarios and three well-recognized value systems, _i.e._, social risk taxonomy , Schwartz Basic Values  and Moral Foundation Theory . For each value system, three test sets (_i.i.d., perturbed, OOD_) are curated to evaluate model robustness. We benchmark 15+ popular LLM evaluators and provide an in-depth analysis of their strengths and weaknesses in value assessment.

In summary, our contributions are three-fold. We 1) propose a novel framework that integrates complementary large and small LLMs for adaptive and robust value evaluation of LLM-generated texts; 2) introduce a comprehensive dataset of 13k\(+\) samples across three value systems; and 3) benchmark 15+ popular LLM evaluators, analyzing their pros and cons in value assessment. Code and benchmark are released at https://github.com/ValueCompass/Clave_Value_Evaluator.

Figure 1: (a) Performance of two LLM-based evaluators. Closed-source LLMs suffer more from the unfamiliar Schwarts value system while the fine-tuned one is more sensitive to the perturbed test set. (b) Less similar texts can share the same essential concept, which works as a robust value indicator.

Related Work

**Evaluating LLMs' Values** To expose potential misalignment in LLMs, a series of benchmarks have been curated to assess their risks, ethics and values. Each differs in collection methods, complexity, formats and underlying value systems. Most existing ones target specific safety issues, ranging from social bias [51; 52; 53], toxicity [10; 32; 54], illegal activities [12; 49], to broader trustworthiness [55; 14]. Considering the increasing diversity of risks [17; 16], extensive benchmarks are aggregated to provide a systematic evaluation [29; 26; 15; 14]. However, these benchmarks with a limited number of risk categories fail to keep pace with rapid-evolving LLMs and might overlook some unforeseen issues. As a solution, human-centered value theories [50; 48] are introduced to assess LLMs' values from a more holistic perspective, where values are considered as a sort of _latent variables_ generalizing relevant risky behaviors . These values are revealed through 1) _discriminative evaluation_, usually with ground truth, such as moral judgment [24; 23; 25] and multiple-choice questionnaires [27; 28; 57], or 2) _generative evaluation_, which prompts LLMs with a scenario and identify the values reflected in their responses [20; 29; 30]. This work focuses on _generative value evaluation_, which can measure LLMs' true value conformity more reliably, rather than only knowledge of values .

**LLM as Automatic Evaluator** The emergent capabilities of LLMs, like in-context learning and instruction-following [39; 40], position them as potential tools to replace humans NLG task evaluation, such as text summarization [36; 58], dialogue  and text generation [60; 34]. Existing approaches fall into two categories based on whether LLMs are fine-tuned for the evaluation task.

(1) _Prompt-based Evaluation_, which leverages powerful LLMs to evaluate text by providing carefully designed instructions, criteria and demonstrations. Three primary protocols dominate, namely intuitive scoring-based evaluation , multiple-choice evaluation  and pairwise comparison [62; 60]. To further enhance LLMs' evaluation performance, few-shot examples  and Chain-of-Thought (CoT) [64; 65] are usually employed. Besides, balanced position calibration and multiple evidence calibration [62; 60] are developed to address position bias where LLMs exhibit preferences for text based on its position. Strategies like role-playing , agent-debating  and communication  are also effective. ALLURE  and AUTOCALIBRATE  calibrate LLMs to align with human labels by iteratively prompting them with training examples. However, this paradigm highly relies on the LLM's own capabilities, which is robust to text variations but hard to fully align with uncommon value systems, as shown in Fig. 1 (a1).

(2) _Finetuning-based Evaluation_ Several limitations remain for prompt-based methods, including high API costs, sub-optimal performance in specific domains and concerns about reproducibility and transparency. Fine-tuning smaller language models presents a practical alternative and is widely-used in alignment research [49; 70; 71]. AUTO-J  is fine-tuned with massive real-world scenarios and diverse evaluation protocols to ensure generalizability and flexibility. Beyond labels, fine-grained feedback and explanations are also incorporated to enhance evaluations [42; 72; 73; 74]. Moreover, multiple evaluators are developed and fine-tuned on dedicated evaluation benchmarks for specific values, such as Kaleidoscope tuned on ValuePrism , MD-Judge on SaladBench for social risks  and so on [75; 31]. While this paradigm allows for easier alignment with human value understandings, it is prone to overfitting , thus sensitive to variations in expressions and failing to cope with out-of-domain cases, as manifested in Fig. 1 (a2).

**Combination of Large and Small LLMs** Recently, the combination of large and smaller LLMs has drawn growing attention due to the potential of benefiting from both superior capabilities and computational efficiency. The most popular strategy is knowledge distillation, where large LLMs' outputs serve as supervision signals to train smaller models [42; 76]. Besides, switch strategies, such as cascading and routing, have also been explored to dynamically select between large and small models, balancing effectiveness and efficiency [77; 78; 79]. However, these methods typically assume large models are more effective than small ones, which doesn't always hold in value evaluation.

## 3 Methodology

### Problem Definition

In this paper, we concentrate on the task of automatically identifying the values reflected in LLM-generated responses under given contexts. Suppose we have a training dataset with \(N\) distinguished samples \(D=\{(x_{n},v_{n},y_{n})\}_{n=1}^{N}\). \(x_{n}=(p_{n},r_{n})\) includes a prompt \(p_{n}\) that describes a provocative scenario and the response \(r_{n}\) generated by an LLM. \(v_{n}\) is a value dimension, such as'self-direction' in Schwartz's Theory of Basic Values , and \(y_{n}\) is the label that indicates how this value is reflected in the text, falling into three classes: _adhere to, oppose to_ and _unrelated to_ (_i.e._, the response shows no evidence towards this value). In some value systems, _adhere to_ and _unrelated to_ are uniformed as a single category _not violate_. Given the diversity of value systems and each value system contains multiple value dimensions, we define the task consistently as classifying how a specific value \(v\) is reflected in a generated text \(x\). Given an LLM-based value evaluator \(\), the task is formalized as:

\[y=(x,v), y\{,,\}.\] (1)

### The CLAVE Framework

LLM-based evaluators encounter two challenges in value assessment: _adaptability_ and _generalizability_, as discussed in Sec. 1. To handle these challenges, we introduce **CLAVE** that leverages a large but closed-source LLM with rich knowledge and robust text comprehension capabilities to deal with diverse scenarios, while fine-tunes a smaller LLM on manually annotated samples to align with human perspectives. The two complementary models are connected via **value concepts**, which refer to key behaviors or implications that act as highly generalized indicators of certain values. For example, _'advocating for personal choice in life-affecting decisions'_ is a representative concept for the value'self-direction'. The whole architecture is depicted in Fig. 2, with a three-step workflow.

**Step 1. Value Concept Extraction**. Using the training dataset \(D\) with human labels, a large LLM functions as a _concept extractor_ to identify highly generalized value concepts that are coherent with the labels. We extract value concepts for each value dimension separately. Given a specific value \(v\) and the associated training subset \(D_{v}=\{(x_{n},v,y_{n})\}_{n=1}^{N_{v}}\), we derive a concept set \(C_{v}=\{c_{1}^{v},c_{2}^{v},\}\).

**Step 2. Value Concept Mapping**. Given a sample \((x,v)\), the _concept extractor_ first identifies concepts from text \(x\) that are critical for evaluating the value \(v\), denoted as \(K_{x,v}=\{k_{x,v}^{1},k_{x,v}^{2},\}\). Then, we map each of the newly extracted value concepts \(k_{x,v}^{i}\) to a most relevant existing concept in \(C_{v}\), obtaining \(C_{x,v}=\{c_{x,v}^{1},c_{x,v}^{2},\}\).

**Step 3. Value Recognition**. Taking the generated text \(x\), the definition of value \(v\) and the value concepts \(C_{x,v}\) as the input, the smaller LLM acts as the _value recognizer_ to predict the result \(y\). Since smaller models could fail to follow the output format, we can compute its probability to generate each possible label and treat the one with the highest probability as the result during inference.

The whole workflow can be formulated as following equations:

\[y=_{S}(x,v,C_{x,v}),\;C_{x,v}=_{L}(x,v).\] (2)

\(_{L}\) and \(_{S}\) represent the large _concept extractor_ and small _value recognizer_ respectively. The small LLM is fine-tuned using LoRA  and the negative-log-likelihood loss. All prompts used for large LLM value extraction and small model value prediction are detailed in Appendix A.

Figure 2: Illustration of CLAVE framework, with a three-step workflow.

CLAVE introduces a value concept set \(C_{v}\) for each value dimension, which contains highly generalized indicators of values, thus enhancing its generalizability across various even unforeseen scenarios. By extracting key value concepts from raw texts for assessment and ignoring extraneous text information, CLAVE also yields better robustness. In addition, the small LLMs can also learn generalized concepts rather than duplicated cases, improving the data-efficiency and minimizing training costs. In the next, we elaborate on the value concept extraction and mapping steps.

### Value Concept Extraction

We employ a large, closed-source LLM to extract value concepts from the training dataset \(D\). For each value dimension \(v\), we build its value concepts set \(C_{v}\) separately from its associated training samples \(D_{v}=\{(x_{n},v,y_{n})\}_{n=1}^{N_{v}}\). The whole process comprises two main stages, outlined below.

**Concept Extraction**. In general, we present batches of labeled samples from \(D_{v}\) to the large LLM extractor, ask it to learn from these samples and extract value concepts that support the labels. Considering that semantically similar samples may reflect the same or opposing value concepts, grouping them together for extraction can yield more essential and generalized concepts. To achieve this, we first compute the text embedding on \(x_{n}\) for all samples in \(D_{v}\) with OpenAI Embedding API and cluster them into groups with the K-Means algorithm. Then, batches of samples are constructed within groups for concept extraction, as \(\{B_{1}^{v}=\{(x_{i},v,y_{i})\}_{i=1}^{b},B_{2}^{v},\}\). To further ensure the concept quality, we include three criteria in the LLM's prompt. 1) _Essential_, concepts should be rationales to explain the value decision, rather than extraneous textual details. 2) _Generalized_, concepts should not be specifically tied to current samples, but generalized to a class of similar cases. 3) _Decoupled_, each concept should involve only one characteristic for value evaluation. If a sample contains multiple value aspects, we split them into separate concepts.

This step results in a preliminary concept set for \(v\), denoted as \(C_{v}^{B}=\{c_{B_{1},1}^{v},c_{B_{1},2}^{v},,c_{B_{i},j}^{v},\}\).

**Concept Clustering**. Since concepts are produced from different sample batches in the above step, there might be multiple concepts with highly similar ideas but different textual expressions. This can also introduce textual variety and complicate the alignment process for the smaller model. To enhance the stability and data efficiency of the small model fine-tuning, we aggregate different concepts with highly similar meanings as a representative one. We perform a hierarchical clustering procedure  on the preliminary concept set \(C_{v}^{B}\), where the distance between two value concepts \(c_{B_{i},j}^{v},c_{B_{i^{}},j^{}}^{v}\) is calculated as:

\[d(c_{B_{i},j}^{v},c_{B_{i^{}},j^{}}^{v})=1-(E^{c_{B_{i},j}^{v},E^{c_{B_{i^{}},j^{}}^{v}}}).\] (3)

Here, \(E^{c_{B_{i},j}^{v}}\) means the text embedding outputted by OpenAI Embedding API.

After the clustering process, we compute the average distance of each concept to others within its cluster and retain the most representative concept per cluster. Finally, we obtain the value concept set for the value dimension \(v\), represented as \(C_{v}=\{c_{1}^{v},c_{2}^{v},\}\). An algorithm is shown in Appendix A.2.

### Value Concept Mapping

After constructing a concept set \(C_{v}\) for each value \(v\), these concepts are used to fine-tune the small value recognizer along with the training samples, which would be well-recognized by the small model. Given any new sample \((x,v)\) for inference, we tend to retrieve relevant concepts from the well-recognized \(C_{v}\) to represent the sample, promoting assessment accuracy. First, we embed all concepts in \(C_{v}\) to build an index. Next, we use the concept extractor to identify value concepts for \((x,v)\) from scratch, obtaining \(K_{x,v}=\{k_{x,v}^{1},k_{x,v}^{2},\}\). For each newly extracted concept \(k_{x,v}^{i}\), we compute its embedding and retrieve the top \(m\) most relevant concepts from \(C_{v}\), forming \(C_{x,v}=\{c_{x,v}^{1},c_{x,v}^{2},\}\).

Although we have employed several strategies such as clustering and prompt design in the extraction stage to maximize the generalizability of value concepts, it is still challenging to cover all scenarios. Some upcoming samples cannot find any relevant concepts from the existing set. Inspired by the complementary learning theory in cognitive science , where the hippocampus manages new events and the neocortex maintains well-understood knowledge, we also process all scenarios hybridly by setting a concept relevance threshold \(\). For samples that cannot match any existing concepts with a relevance above \(\), we make predictions on their newly extracted concepts. The way to integrate these new concepts into the existing concept set can be explored in the future.

## 4 Benchmark

To standardize the evaluation of values in LLM-generated texts, we present a benchmark **ValEval**.

**Data Composition** ValEval is a comprehensive benchmark comprising 13k\(+\) manually annotated tuples of (text, value, label), across three well-recognized value systems. A piece of text includes a prompt and the response generated by an LLM, and the label could be {_adhere to, oppose to_ or _not related to_}. To rigorously measure the accuracy, generalization and robustness of value evaluators, we include three different subsets for each value system as follows. 1) **Original**: this is the primary split, including both the training data to fine-tune evaluators and a testing set collected from the same distribution, i.e. _i.i.d._ 2) **Perturbation**: this subset contains perturbed versions of original testing samples to evaluate robustness against variations in text expressions. It is newly generated in this paper, using two types of perturbation strategies that could induce model vulnerability to value assessment. One is modifying texts irrelevant to value implication, e.g., synonym replacement, paraphrasing and repetition, but not altering the value label. The other is conducting minimal changes to the text to make the value label flipped, for which we locate and modify value-relevant parts. We first instruct an LLM to generate perturbed data and filter reasonable data by human. Since we benchmark GPT-4 in this paper, we use the Mistral-Large API to generate the perturbation texts and thus avoid possible leakage in evaluation. 3) **Generalization**: we also introduce a distinct dataset for each value system to verify the generalization across different scenarios and distributions, i.e. _OOD_.

Specifically, the data sources and construction method for each value system are elaborated as follows.

_Social Risk Categories_. This is the most popular perspective in measuring the value of LLMs. We build the _original_ split and _perturbation_ split on the BeaverTails  benchmark that comprises QA pairs of adversarial questions and responses from the Alpaca-7B model. Each QA-pair is annotated with the safety label to 14 risk categories, such as hate speech and financial crime. About the _generalization_ split, we select another dataset Do-not-Answer  curated on a risk taxonomy for safeguard evaluation. To keep the value dimension consistent, we filter questions of those highly relevant risk categories and map them to the categories of BeaverTails according to the risk definition.

_Schwartz Theory of Basic Human Values_. This theory identifies ten motivationally distinct value dimensions to explain universal human desires, which are widely recognized across cultures. The original and perturbation subsets are derived from the Value Fulcra dataset , which pairs adversarial questions with LLM outputs, identifying their underlying basic values labeled as adhere to, not related or opposed to. In addition, we also filter and convert samples from the Do-not-Answer benchmark to obtain the generalization subset.

_Moral Foundation Theory_. This theory summarizes five groups of moral foundations to understand human moral decision-making, i.e. Care/Harm, Fairness/Cheating, Loyalty/Betrayal, Authority/Subversion, and Sanctity/Degradation. The primary and generalization splits correspond to: 1) DenEvil : each sample includes a paragraph generated by LLMs, a relevant moral foundation and the label. 2) Moral Stories: this benchmark consists of samples with a piece of norm, a situation, a normative action and a divergent action. We map each norm to the most consistent moral foundation.

For each value dimension in a value system, we include 100 instances for each label category to form the original training set, i.e. 100 for _adhere to_, 100 for _oppose to_ and 100 for _unrelated to_. We randomly sample 1,000 entries from the primary subset as the original testing set. The statistics and distribution variance are shown in Table 1.

**Data Preprocess and Labeling** To ensure dataset quality and the reliability of evaluation results, we clean the whole benchmark. We remove noisy and extreme data, mainly samples that contain empty texts, lots of special characters and significantly long or short texts. Furthermore, since only a part

    &  &  &  &  \\   & \#data & sim & \#data & sim & \#data & sim & \#data & sim \\  Social Risks & 2,800 & 1 & 1,000 & 0.8228 & 668 & 0.7290 & 397 & 0.5131 \\ Schwartz Theory & 2,463 & 1 & 1,000 & 0.8698 & 603 & 0.7911 & 406 & 0.6102 \\ Moral Foundation & 1,500 & 1 & 1,000 & 0.8823 & 800 & 0.7677 & 400 & 0.5225 \\   

Table 1: Statistics and distribution similarity (sim) to the original training set of each value system.

of datasets we used have manual annotations and some labels are low-quality, we recruit multiple qualified human annotators through a vendor to label or re-label all samples. At least undergraduates majoring in psychology or sociology are involved to ensure accuracy. More details about recruiting and data labeling are in Appendix B.3.

## 5 Experiments and Analysis

### Experimental Settings

We benchmark the capabilities of 15+ popular LLM evaluators on ValEval to analyze their strengths and weaknesses, organized into the following categories.

(1) **Prompt-based Evaluators**. Basically, we design a vanilla prompt template to provide LLM APIs with the official value definition, the sample to be evaluated, the instruction and the output format. Furthermore, we incorporate more advanced prompting techniques, including **Few-Shot**, **Chain-of-thought** (CoT)  and **G-Eval**. Several ensemble-based approaches that benefit from multiple LLMs or repeat runs are also evaluated, i.e **FairEval**, **WideDeep**, and **ChatEval**. Besides, there are advanced LLM evaluators that align with human annotations through in-context learning, such as **AutoCalibrate** and **ALLURE**.

(2) **Tuning-based Evaluators**. We fine-tune open-sourced LLMs of varying sizes, including **GPT-2-Large** (774M), **Phi-3** (3.8B), **Mistral-7B** (7B) and **Llama-2-7b-chat** (7B). For a more comprehensive comparison, we also upload the ValEval training data to finetune large-scale LLMs: **Llama-2-70b-chat** (70B), **Mistral-Large-Latest** and **GPT-3.5-Turbo**.

(3) **Value-Specific Evaluators**. We compare the LLM-based evaluators designed for specific values in existing research. **KaleidoScope** is fine-tuned on the ValuePrism dataset, which focuses on a broad spectrum of human values, rights, and duties. **Value Fulcra** includes a fine-tuned evaluator to identify Schwartz Basic Values from LLM-generated texts, while the evaluator in **DeNEVIL** is targeted to the judgment of moral foundations. For these baselines, we employ their published checkpoints or a model trained with their released code and data.

**CLAVE** is our proposed evaluation framework that integrates large LLMs and smaller ones. We also provide the results ensembled from three crowd workers as an additional reference. All evaluations report accuracy as the metric. More details about experimental settings and implementations can be found in Appendix C. We would release the code and benchmark at https://github.com/ValueCompass/Clave_Value_Evaluator.

### Overall Performance on Value Assessment

The whole evaluation results of 15+ LLM-based evaluators, our CLAVE framework and crowd workers on the curated ValEval dataset are detailed in Table 2.

From the results, we obtain three main findings: 1) _Prompt-based evaluation with large LLMs indeed performs well on popular social risks, with considerable robustness and generalizability_, maintaining consistent performance across three testing splits. This indicates their strong generalization capabilities under textual perturbation and distribution changes. _However, their effectiveness wanes in handling less common value theories_, such as Schwartz value and Moral Foundation Theory. This implies a limitation in their adaptability to diverse value frameworks. 2) _Tuning-based evaluators achieve great results across both widespread and less popular value theories, indicating their adeptness at value differentiation_. Nevertheless, _their robustness and generalizability are compromised_. For example, Mistral-7b shows superior performance in the original testing split, but its effectiveness diminishes in perturbed and generalized contexts. Though fine-tuned larger LLMs handle perturbed and OOD cases better, especially the fine-tuned GPT-3.5-Turbo, they require high computational cost, which we fully analyze in Appendix C.4. In addition, the value-specific evaluators suffer from handling noisy and OOD samples. 3) _Our CLAVE framework emerges as an effective solution, reaching a superior balance between adaptability and generalizability_. It consistently shows the best or comparable performance across all value theories and testing splits, even surpassing the fine-tuned larger LLMs. This underscores CLAVE's advantages in leveraging the strengths of large LLMs to ensure generalizability while effectively aligning smaller LLM's value understanding with humans.

### Analysis of Training Data Amount

Given limited annotated value data and high cost of expertise annotations, especially for less popular theories, we conduct a comparative analysis of our CLAVE method against a tuning-based baseline with varying amounts of training samples. As stated in Sec. 4, our training set contains 100 samples for each label of each value. Thus, we experiment with 10, 20, 50, 100 samples respectively. We randomly sample different subsets or shuffle all training samples to repeat each experiment 5 times. The results with error bars are displayed in Fig. 3.

First, we observe the performance of both CLAVE and Llama-2-7b improves as the number of training samples increases. Notably, the improvement of Llama-2-7b is more significant, such as Llama on Social Risk original split, suggesting a strong reliance on training data. When the data is limited, our method better outperforms the baseline, demonstrating superior data efficiency. For

    &  &  &  \\  & Original & Perturbed & Generalized & Original & Perturbed & Generalized & Original & Perturbed & Generalized \\   \\  Vanilla & 84.89 & 81.20 & 89.60 & 53.79 & 68.13 & 71.62 & 39.01 & 54.19 & 25.26 \\ Few-shot & 79.61 & 82.07 & 88.11 & 54.79 & 67.62 & 66.98 & 53.07 & 53.68 & 25.45 \\ Chain-of-thought & 83.25 & 83.86 & 89.53 & 54.04 & 68.39 & 73.68 & 39.34 & 53.17 & 24.87 \\ G-Eval & 84.68 & 83.40 & 87.23 & 52.76 & 67.88 & 69.36 & 39.65 & 51.43 & 24.37 \\ FairEval & 85.83 & **86.83** & 91.08 & 40.83 & 81.50 & 82.35 & 38.33 & 50.83 & 25.75 \\ ChatEval & 82.50 & 83.75 & **92.16** & 16.46 & 81.42 & 82.35 & & \\ WideDeep & 82.50 & 84.38 & 90.54 & 25.00 & 80.42 & 82.35 & 33.12 & 43.75 & 27.51 \\ Calibrate & 85.20 & 84.43 & 89.60 & 55.53 & 68.49 & 70.74 & 39.13 & 54.71 & 24.75 \\ Allure & 85.66 & 83.10 & 88.11 & 53.59 & 67.42 & 67.86 & 52.87 & **56.25** & 25.37 \\   \\  GPT-2.Large & 85.86 & 65.28 & 24.59 & 69.02 & 60.49 & 77.36 & 57.38 & 41.99 & 30.25 \\ Pii-3 & 84.82 & 73.59 & 48.11 & 71.93 & 68.19 & 72.93 & 66.52 & 47.41 & 34.75 \\ Llama-2.7b & 83.57 & 68.60 & 22.43 & 64.26 & 58.83 & 77.69 & 59.25 & 45.74 & 28.01 \\ Mistral-7b & **88.57** & 76.50 & 53.51 & **76.29** & 70.89 & 76.19 & 67.56 & 46.98 & 29.01 \\   \\  Llama-2.70b & 86.32 & 80.75 & 71.01 & 75.99 & 70.63 & 82.88 & 62.13 & 41.77 & 32.95 \\ Mistral-Large & 82.38 & 78.48 & 70.54 & 67.17 & 69.02 & 79.44 & 49.18 & 41.73 & 33.75 \\ GPT-3.5-Turbo & 88.32 & 78.48 & 74.05 & 77.97 & 70.56 & 83.31 & 67.62 & 50.87 & 32.19 \\   \\  Kaleido-xl & 38.54 & 36.46 & 22.43 & 35.42 & 50.62 & 60.40 & 22.71 & 26.67 & **64.75** \\ Value Fulxra & - & - & 72.40 & 61.67 & 77.72 & - & - & - \\ DeNeVIL & - & - & - & - & - & - & 44.60 & 44.67 & 24.60 \\  Crowdworker & 86.00 & 86.00 & 89.18 & 60.21 & 68.65 & 88.91 & **85.75** & 82.66 & 49.25 \\  CLAVE-Llama & 85.03 & 78.79 & 85.41 & 69.85 & **82.12** & **83.71** & 63.62 & 48.98 & 34.50 \\ CLAVE-Mistral & 88.36 & 83.99 & 88.65 & 75.26 & 75.05 & 82.45 & 67.56 & 51.25 & 35.25 \\   

Table 2: Evaluation accuracy (%) on ValEval of various LLM-based evaluators. The best performances are shown in bold. The best performances of fine-tuned models are shown with underlines.

Figure 3: Evaluation performance curves with increasing amount of training samples. ‘#/value’ means the number of samples for each value type. The error bars indicate significant improvements.

example, the difference observed on '#10', '#20' of the Social Risk and Schwarzt Value datasets is more pronounced compared to that with 50, 100 samples. Second, the baseline sometimes shows decreased performance on the generalization splits as training data increases. We attribute this to overfitting to the specific distribution of the training data, thus impacting the model's generalizability. Nevertheless, the generalizability of our method is hardly affected, even improves as more data becomes available. We infer this is due to that our method learns value concepts as general knowledge rather than specific patterns to a particular distribution.

### Analysis of Different Combinations

We conduct experiments to analyze CLAVE's adaptability across different large and small models. We select widely used large models with notable capability differences, i.e. ChatGPT and GPT-4, along with diverse smaller models of different sizes and origins, i.e., Phi-3, Llama-2-7b and Mistral-7b. The results are displayed in Fig. 4, with an alternative visualization in Appendix C.5.

Our key observations are as follows. 1) _Stronger large and small LLMs yield better performance_. Generally, CLAVE combinations using GPT-4 with smaller LLMs outperform those with ChatGPT, and combinations with Mistral as the small model show stronger performance than those with other smaller models. 2) _Large LLMs play a more pivotal role in enhancing performance in the CLAVE framework_ via extracting more accurate and generalized value concepts. This is evident in that integrating with GPT-4 brings a more significant improvement over small LLMs than with ChatGPT. Moreover, a large LLM can notably boost small LLMs in perturbation and generalization sets, even if its own performance is much poorer. For example, on the Schwartz generalization subset, ChatGPT+Phi outperforms the fine-tuned Phi alone, both of which significantly exceed the performance of ChatGPT alone. 3) _The optimal combination depends on the application scenarios_. For the original subset, using fine-tuned small LLMs alone is almost enough, while the CLAVE framework can significantly enhance the results on perturbed and OOD subsets. Focusing on uncommon value types like Schwarts and MFT, less powerful small LLMs (e.g., Phi3 and Llama) show easier steering and reduce overfitting. For common value types like social risk, combining a stronger small LLM would be better.

Figure 4: Experiments on different combinations of large and small LLMs in CLAVE. We identify large LLMs by patterns, small LLMs by colors and each combination by pattern & color.

### Case Study

To illustrate the challenges of _adaptability_ and _generalizability_ in value evaluation and validate the advantages of CLAVE, we conduct case studies. The results are depicted in Fig. 5.

From **case 1**, we observe that while GPT-4 accurately assesses the value of a specific social risk embedded in the given scenario, it makes errors on the same scenario when evaluating the less popular Schwartz value dimension. This indicates a deficiency in the LLM's understanding of less popular value theories, underscoring the necessity of alignment with human perspectives. **Case 2** highlights the vulnerability of smaller models to textual perturbations. For the same scenario, slightly modifications to the text led to erroneous judgments by the Llama model. In contrast, value concepts demonstrate robustness against such textual changes, as it captures essential behaviors related to values which could remain constant despite minor textual variations. We find the value concepts across the two examples are the same, thus value assessment based on value concepts would be more stable. In case 3, we compare Llama2 and CLAVE in handling generalized scenarios, where value concepts exhibit strong scenario generalization. When extracting value concepts, we require them to be generic and not be tied to specific scenarios, promoting generalizability.

## 6 Conclusion and Limitations

This study concentrates on two challenges of using LLMs for generative value evaluation: adaptability to evolving human values and generalizability to varying expressions. We propose CLAVE, a novel framework that integrates complementary large proprietary models and small tuning-based LLMs through value concepts. Furthermore, we present ValEval, a comprehensive benchmark to standardize the value evaluation of LLM generated texts. We conduct extensive experiments on this benchmark to analyze the strengths and weaknesses of various LLM-based evaluators, and also reveal that CLAVE achieves a superior balance between accuracy and generalizability across value systems.

Though this paper verifies the great effects of value concepts for LLM-based value assessment, there are several limitations and future research directions. 1) The extracted value concepts enable uncovering the rationale behind LLM's decision-making on value evaluation, while this property has not been fully explored. We could further study _the advantages of value concepts on transparency_. 2) This paper directly combines large black-box LLMs and small fine-tuned LLMs distinguished by their sizes. However, there is a wide range of options available for both types of LLMs, including capabilities, model size and tuning method. 3) The proposed benchmark is primarily in English and maybe biased towards value issues of English-speaking regions. Multilingual value analyses would be helpful for achieving more reliable value evaluation. 4) In addition to the three well-recognized ones, more value systems could further extend our scope. 5) Though CLAVE has demonstrated satisfactory performance, e.g., 84.86% accuracy on the Perturbation Set, further improvement can be achieved. More in-depth discussion about the limitations can be found in Appendix E.

Figure 5: Case study on the adaptability and generalizability of value assessment.