# Designing Robust Transformers using

Robust Kernel Density Estimation

 Xing Han

Department of ECE

University of Texas at Austin

aaronhan223@utexas.edu

&Tongzheng Ren

Department of Computer Science

University of Texas at Austin

tongzheng@utexas.edu

Tan Minh Nguyen

Department of Mathematics

University of California, Los Angeles

tannnguyen89@ucla.edu

&Khai Nguyen

Department of Statistics and Data Sciences

University of Texas at Austin

khainb@utexas.edu

&Joydeep Ghosh

Department of ECE

University of Texas at Austin

jghosh@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

University of Texas at Austin

minhnhat@utexas.edu

###### Abstract

Transformer-based architectures have recently exhibited remarkable successes across different domains beyond just powering large language models. However, existing approaches typically focus on predictive accuracy and computational cost, largely ignoring certain other practical issues such as robustness to contaminated samples. In this paper, by re-interpreting the self-attention mechanism as a non-parametric kernel density estimator, we adapt classical robust kernel density estimation methods to develop novel classes of transformers that are resistant to adversarial attacks and data contamination. We first propose methods that down-weight outliers in RKHS when computing the self-attention operations. We empirically show that these methods produce improved performance over existing state-of-the-art methods, particularly on image data under adversarial attacks. Then we leverage the median-of-means principle to obtain another efficient approach that results in noticeably enhanced performance and robustness on language modeling and time series classification tasks. Our methods can be combined with existing transformers to augment their robust properties, thus promising to impact a wide variety of applications.

## 1 Introduction

Attention mechanisms and transformers (Vaswani et al., 2017) have drawn lots of attention in the machine learning community (Lin et al., 2021; Tay et al., 2020; Khan et al., 2021). Now they are among the best deep learning architectures for a variety of applications, including those in natural language processing (Devlin et al., 2019; Al-Rfou et al., 2019; Dai et al., 2019; Child et al., 2019; Raffel et al., 2020; Baevski and Auli, 2019; Brown et al., 2020; Dehghani et al., 2019), computer vision (Dosovitskiy et al., 2021; Liu et al., 2021; Touvron et al., 2021; Ramesh et al., 2021; Radford et al., 2021; Fan et al., 2021; Liu et al., 2022), and reinforcement learning (Chen et al., 2021; Janner et al., 2021). They are also known for their effectiveness in transferring knowledge from various pretraining tasks to different downstream applications with weak supervision or no supervision (Radford et al., 2018, 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019).

While there have been notable advancements, the robustness of the standard attention module remains an unresolved issue in the literature. In this paper, our goal is to reinforce the attention mechanism and construct a comprehensive framework for robust transformer models. To achieve this, we first revisit the interpretation of self-attention in transformers, viewing it through the prism of the Nadaraya-Watson (NW) estimator (Nadaraya, 1964) in a non-parametric regression context. Within the transformer paradigm, the NW estimator is constructed based on the kernel density estimators (KDE) of the keys and queries. However, these KDEs are not immune to the issue of sample contamination (Kim and Scott, 2012). By conceptualizing the KDE as a solution to the kernel regression problem within a Reproducing Kernel Hilbert Space (RKHS), we can utilize a range of state-of-the-art robust KDE techniques, such as those based on robust kernel regression and median-of-mean estimators. This facilitates the creation of substantially more robust self-attention mechanisms. The resulting suite of robust self-attention can be adapted to a variety of transformer architectures and tasks across different data modalities. We carry out exhaustive experiments covering vision, language modeling, and time-series classification. The results demonstrate that our approaches can uphold comparable accuracy on clean data while exhibiting improved performance on contaminated data. Crucially, this is accomplished without introducing any extra parameters.

**Related Work on Robust Transformers:** Vision Transformer (ViT) models (Dosovitskiy et al., 2020; Touvron et al., 2021) have recently demonstrated impressive performance across various vision tasks, positioning themselves as a compelling alternative to CNNs. A number of studies (e.g., Subramanya et al., 2022; Paul and Chen, 2022; Bhojanapalli et al., 2021; Mahmood et al., 2021; Mao et al., 2022; Zhou et al., 2022) have proposed strategies to bolster the resilience of these models against common adversarial attacks on image data, thereby enhancing their generalizability across diverse datasets. For instance, Mahmood et al. (2021) provided empirical evidence of ViT's vulnerability to white-box adversarial attacks, while demonstrating that a straightforward ensemble defense could achieve remarkable robustness without compromising accuracy on clean data. Zhou et al. (2022) suggested fully attentional networks to enhance self-attention, achieving state-of-the-art accuracy on corrupted images. Furthermore, Mao et al. (2022) conducted a robustness analysis on various ViT building blocks, proposing position-aware attention scaling and patch-wise augmentation to enhance the model's robustness and accuracy. However, these investigations are primarily geared toward vision-related tasks, which restricts their applicability across different data modalities. As an example, the position-based attention from Mao et al. (2022) induces a bi-directional information flow, which is limiting for position-sensitive datasets such as text or sequences. These methods also introduce additional parameters. Beyond these vision-focused studies, robust transformers have also been explored in fields like text analysis and social media. Yang et al. (2022) delved into table understanding and suggested a robust, structurally aware table-text encoding architecture to mitigate the effects of row and column order perturbations. Liu et al. (2021) proposed a robust end-to-end transformer-based model for crisis detection and recognition. Furthermore, Li et al. (2020) developed a unique attention mechanism to create a robust neural text-to-speech model capable of synthesizing both natural and stable audios. We have noted that these methods vary in their methodologies, largely due to differences in application domains, and therefore limiting their generalizability across diverse contexts.

**Other Theoretical Frameworks for Attention Mechanisms:** Attention mechanisms in transformers have been recently studied from different perspectives. Tsai et al. (2019) show that attention can be derived from smoothing the inputs with appropriate kernels. Katharopoulos et al. (2020); Choromanski et al. (2021); Wang et al. (2020) further linearize the softmax kernel in attention to attain a family of efficient transformers with both linear computational and memory complexity. These linear attentions are proven in Cao (2021) to be equivalent to a Petrov-Galerkin projection (Reddy, 2004), thereby indicating that the softmax normalization in dot-product attention is sufficient but not necessary. Other frameworks for analyzing transformers that use ordinary/partial differential equations include Lu et al. (2019); Sander et al. (2022). In addition, the Gaussian mixture model and graph-structured learning have been utilized to study attentions and transformers (Tang and Matteson, 2021; Gabbur et al., 2021; Zhang and Feng, 2021; Wang et al., 2018; Kreuzer et al., 2021). Nguyen et al. (2022) has linked the self-attention mechanism with a non-parametric regression perspective, which offers enhanced interpretability of Transformers. Our approach draws upon this viewpoint, but focuses instead on how it can lead to robust solutions.

Self-Attention Mechanism from a Non-parametric Regression Perspective

Assume we have the key and value vectors \(\{_{j},_{j}\}_{j[N]}\) that is collected from the data generating process \(=f()+\), where \(\) is some noise vectors with \([]=0\), and \(f\) is the function that we want to estimate. We consider a random design setting where the key vectors \(\{_{j}\}_{j[N]}\) are i.i.d. samples from the distribution \(p()\), and we use \(p(,)\) to denote the joint distribution of \((,)\) defined by the data generating process. Our target is to estimate \(f()\) for any new queries \(\). The NW estimator provides a non-parametric approach to estimating the function \(f\), the main idea is that

\[f()=[|]=_{^{D}} p (|)d=_{^{D}} p( ,)}{p()}d,\] (1)

where the first equation comes from the fact that \([]=0\), the second equation comes from the definition of conditional expectation, and the last equation comes from the definition of conditional density. To provide an estimation of \(f\), we just need to obtain estimations for both the joint density function \(p(,)\) and the marginal density function \(p()\). KDE is commonly used for the density estimation problem (Rosenblatt, 1956; Parzen, 1962), which requires a kernel \(k_{}\) with the bandwidth parameter \(\) satisfies \(_{^{D}}k_{}(-^{})d=1,^{}\), and estimate the density as

\[_{}(,)=_{j[N]}k_{}( [,]-[_{j},_{j}])_{}( )=_{j[N]}k_{}(-_{j}),\] (2)

where \([,]\) denotes the concatenation of \(\) and \(\). Specifically, when \(k_{}\) is the isotropic Gaussian kernel, we have \(_{}(,)=_{j[N]}k_{}( -_{j})k_{}(-_{j})\). Combine this with Eq. (1) and Eq. (2), we can obtain the NW estimator of the function \(f\) as

\[_{}()=_{j}k_{}( {k}-_{j})}{_{j[N]}k_{}(-_{j})}.\] (3)

Furthermore, it is not hard to show that if the keys \(\{_{j}\}_{j[N]}\) are normalized, the self-attention mechanism \(_{}(_{i})\) in Eq. (3) is exactly the standard Softmax attention \(_{}(_{i})=_{j[N]}(^{ }_{j}/^{2})_{j}\). Such an assumption on the normalized key \(\{_{j}\}_{j[N]}\) can be mild, as in practice we always have a normalization step on the key to stabilizing the training of the transformer (Schlag et al., 2021). If we choose \(^{2}=\), where \(D\) is the dimension of \(\) and \(_{j}\), then \(_{}(_{i})=_{i}\). As a result, the self-attention mechanism in fact performs a non-parametric regression with NW-estimator and isotropic Gaussian kernel when the keys are normalized.

KDE as a Regression Problem in RKHSWe start with the formal definition of the RKHS. The space \(_{k}=\{f f:\}\) is the RKHS associated with kernel \(k\), where \(k:\), if it is a Hilbert space with inner product \(,_{_{k}}\) and following properties:

* \(k(,)_{k},\);
* \( f_{k}\), \(f()= f,k(,)_{_{k}}\). Aka the reproducing property.

With slightly abuse of notation, we define \(k_{}(,^{})=k_{}(-^{})\). By the definition of the RKHS and the KDE estimator, we know \(_{}=_{j[N]}k_{}(_{j},) _{k_{}}\), and can be viewed as the optimal solution of the following least-square regression problem in RKHS:

\[_{}=*{arg\,min}_{p_{k_{}}}_{j [N]}\|k_{}(_{j},)-p\|_{_ {k_{}}}^{2}.\] (4)

Figure 1: The contour plots illustrate the density estimation of the two-dimensional query vector embedding within a transformer’s attention layer. The left plot employs the regular KDE method, as defined in Eq. (4), whereas the right plot utilizes a robustified version of the KDE method, which enhances KDE’s robustness against outliers.

Note that, in Eq. (4), the same weight factor \(1/N\) is applied uniformly to each error term \(\|k_{}(_{j},)-p\|_{_{k_{}}}^{2}\). This approach functions effectively if there are no outliers in the set \(\{k_{}(_{j},)\}_{j[N]}\). However, when outliers are present (for instance, when there is some \(j\) such that \(\|k_{}(_{j},)\|_{_{k_{}}}\|k_{}( {x}_{i},)\|_{_{k_{}}}\), \( i[N],i j\)), the error attributable to these outliers will overwhelmingly influence the total error, leading to a significant deterioration in the overall density estimation. We illustrate the robustness issue of the KDE in Figure 1. The view that KDE is susceptible to outliers, coupled with the non-parametric understanding of the self-attention mechanism, implies a potential lack of robustness in Transformers when handling outlier-rich data. We now offer a fresh perspective on this robustness issue, introducing a universal framework that is applicable across diverse data modalities.

## 3 Robust Transformers that Employ Robust Kernel Density Estimators

Drawing on the non-parametric regression formulation of self-attention, we derive multiple robust variants of the NW-estimator and demonstrate their applicability in fortifying existing Transformers. We propose two distinct types of robust self-attention mechanisms and delve into the properties of each, potentially paving the way for Transformer variants that are substantially more robust.

### Down-weighting Outliers in RKHS

Inspired by robust regression (Fox and Weisberg, 2002), a direct approach to achieving robust KDE involves down-weighting outliers in the RKHS. More specifically, we substitute the least-square loss in Eq. (4) with a robust loss function \(\), resulting in the following formulation:

\[_{}=*{arg\,min}_{p_{k_{ }}}_{j[N]}(\|k_{}(_{j},)-p\|_{_{k _{}}})=_{j[N]}_{j}k_{}(_{j},).\] (5)

Examples of the robust loss function \(\) include the Huber loss (Huber, 1992), Hampel loss (Hampel et al., 1986), Welsch loss (Welsch and Becker, 1975) and Tukey loss (Fox and Weisberg, 2002). We empirically evaluate different loss functions in our experiments. The critical step here is to estimate the set of weights \(=(_{1},,_{N})_{N}\), with each \(_{j}(\|k_{}(_{j},)-_{}\|_{_{k_{}}})\), where \((x):=(x)}{x}\). Since \(_{}\) is defined via \(\), and \(\) also depends on \(_{}\), one can address this circular definition problem via an alternative updating algorithm proposed by Kim and Scott (2012). The algorithm starts with randomly initialized \(^{(0)}_{n}\), and performs alternative updates between \(_{}\) and \(\) until the optimal \(_{}\) is reached at the fixed point (see details in Appendix A).

However, while this technique effectively diminishes the influence of outliers, it also comes with noticeable drawbacks. Firstly, it necessitates the appropriate selection of the robust loss function, which may entail additional effort to understand the patterns of outliers. Secondly, the iterative

Figure 2: The application of Transformers with robust KDE attention on image and text is shown. (Left) The robust KDE self-attention generates varying weight factors for image patch embeddings under adversarial attacks or data corruption. The adversely impacted regions that would otherwise lead to incorrect predictions are down-weighted, ensuring enhanced accuracy and robustness. (Right) In the field of language modeling, the weight factors lend significance to essential keywords (highlighted in red). In the face of word swap attacks, the fortified self-attention mechanism, particularly when utilizing the medians-of-means principle, is proficient in disregarding or reducing the importance of less consequential words (marked in green). Consequently, this results in a more resilient procedure during self-attention computations.

updates might not successfully converge to the optimal solution. A better alternative is to assign higher weights to high-density regions and reduce the weights for atypical samples. The original KDE is scaled and projected to its nearest weighted KDE according to the \(L_{2}\) norm. Similar concepts have been studied by Scaled and Projected KDE (SPKDE) (Vandermeulen & Scott, 2014), which offer an improved set of weights that better defend against outliers in the RKHS space. Specifically, given the scaling factor \(>1\), and let \(_{}^{N}\) be the convex hull of \(k_{}(_{1},),,k_{}(_{N},)_{k_{}}\), i.e., the space of weighted KDEs, the optimal density \(_{}\) is given by

\[_{}=_{p_{}^{N}}\|_{j[N]}k_{}(x_{j},)-p\|_{_{k_{ }}}^{2},\] (6)

which is guaranteed to have a unique minimizer since we are projecting in a Hilbert space and \(_{}^{N}\) is closed and convex. Notice that, by definition, \(_{}\) can also be represented as \(_{}=_{j[N]}_{j}k_{}(x_{j},),\ ^{N}\), which is same as the formulation in Eq. (5). Then Eq. (6) can be written as a quadratic programming (QP) problem over \(\):

\[_{}\ ^{}G-2q^{},^{N},\] (7)

where \(G\) is the Gram matrix of \(\{_{j}\}_{j[N]}\) with \(k_{}\) and \(q=G\). Since \(k_{}\) is a positive-definite kernel and each \(_{i}\) is unique, the Gram matrix \(G\) is also positive-definite. As a result, this QP problem is convex, and we can leverage commonly used solvers to efficiently obtain the solution and the optimal density \(_{}\).

Robust Self-Attention MechanismWe now introduce the robust self-attention mechanism that down-weights atypical samples. We consider the density estimator of the joint distribution and the marginal distribution when using isotropic Gaussian kernel:

\[_{}(,)=_{j[N]}_{j}^{}k_{}([_{j},_{j}],[,]),_{ }()=_{j[N]}_{j}^{}k_{}(_{j},).\] (8)

Following the non-parametric regression formulation of self-attention in Eq. (3), we obtain the robust self-attention mechanism as

\[}_{i}=_{j}_{j}^{}k_{}(_{i}-_{j})}{_{j[N]}_{j}^{}k_{}(_{i}-_{j})},\] (9)

where \(^{}\) and \(^{}\) are obtained via either alternative updates or the QP solver. We term Transformers whose density from the non-parametric regression formulation of self-attention employs Eq. (5) and Eq. (6) as Transformer-RKDE and Transformer-SPKDE, respectively. Figure 2 presents an example of the application of the attention weight factor during the learning process from image and text data. The derived weight factor can potentially emphasize elements relevant to the class while reducing the influence of detrimental ones. Note that, the computation of \(\{_{j}^{}\}_{j[N]}\)and \(\{_{j}^{}\}_{j[N]}\) are separate as \(_{j}^{}\) involves both keys and values vectors. During the empirical evaluation, we concatenate the keys and values along the head dimension to obtain the weights for the joint density \(_{}(,)\) and only use the key vectors for obtaining the set of weights for the marginal \(_{}()\). In addition, \(^{},^{}^{j i}\) for \(i,j=1,,N\) are 2-dimensional matrices that include the pairwise weights between each position of the sequence and the rest of the positions. The weights are initialized uniformly across a certain sequence length dimension. For experiments related to language modeling, we can leverage information from the attention mask to initialize the weights on the unmasked part of the sequence.

LimitationsWhile constructing attention weight factors proves effective, they necessitate iterative algorithms to calculate the set of weights when computing self-attention at each layer, resulting in increased overall complexity. Moreover, the foundational contamination model for these methods is the classical Huber contamination model (Huber, 2011), which requires assumptions about contamination distributions and parameters that may not be universally applicable, especially in discrete settings. To address these limitations, we propose a novel approach that circumvents these computational constraints while effectively fostering robust self-attention mechanisms.

### Robust Self-Attention via Median-of-Means Principle

The Median-of-Means (MoM) principle (Jerrum et al., 1986; Alon et al., 1996) is one other way to construct robust estimators. Rather than taking the average of all the observations, the sample is split into several blocks over which the median is computed. The MoM principle also improved the robustness of KDE and demonstrated its statistical performance under a less restrictive outlier framework (Humbert et al., 2022). More importantly, it can be easily adapted to self-attention. Specifically, we randomly divide the keys \(\{_{j}\}_{j=1}^{N}\) into \(B\) subsets \(I_{1},,I_{B}\) of equal size, namely, \(|I_{1}|=|I_{2}|==|I_{B}|=\). Then, the robust estimator of \(p()\) takes the following form:

\[_{}()\{_{,I_ {1}}(),,_{,I_{B}}()\},\] (10)

where we define \(_{,I_{l}}()=}_{j I_{l}}k_{ }(-_{j})\) for any \(l[B]\). Similarly, the robust estimator of \(p(,)\) is as follows:

\[_{}(,)\{_ {,I_{1}}(,),,_{,I_{B}}(, {k})\},\] (11)

where \(_{,I_{l}}(,)=}_{j I_{l }}k_{}(-_{j})k_{}(-_{j})\) for any \(l[B]\). We now propose the self-attention mechanism utilizing the median-of-means principle.

Median-of-Means Self-Attention MechanismGiven the robust estimators in Eq. (10) and (11), we can consider the following robust estimation of the attention:

\[}_{i}=}_{j I_{l}}v_{j}k_{ }(_{i}-_{j})}{\{_{,I_{1}}( _{i}-),,_{,I_{B}}(_{i}-)\}},\] (12)

where \(I_{l}\) is the block such that \(_{}(_{i}-)\) achieves its median value in equation (11). In this context, the random subsets apply to input sequences rather than individual data points, distinguishing this approach from stochastic batches. It's worth noting that our proposed attention mechanism assumes that key and query vectors achieve their median on the same block. Consequently, we apply the median block \(I_{l}\) obtained from the denominator into the numerator, rather than considering the median over all potential blocks, which leads to a process that is faster than computing median blocks on both sides. Moreover, the original MoM principle mandates that each subset be non-overlapping, i.e. \(I_{l_{1}} I_{l_{2}}=\) for any \(1 l_{1} l_{2} B\). However, for structured, high-dimensional data, dividing into non-overlapping blocks may result in the model only gaining a partial perspective of the dataset, leading to sub-optimal performance. Therefore, we construct each subset by sampling with replacement from the original dataset, maintaining the sequential relationship thereafter. In particular, we have found this strategy to be effective in discrete contexts, such as identifying and filtering out aberrant words in a sentence. As illustrated in the bottom right segment of Figure 2, under a word swap attack, the MoM robust attention retains the subsequence that is most relevant to the content, while discarding unhelpful parts. The downside of MoM self-attention is also apparent: since the attention mechanism only accesses a portion of the sequence, it is likely to result in suboptimal performance on _clean datasets_. The simplicity of MoM allows for easy integration with many existing models. We initially demonstrate that the MoM self-attention mechanism can enhance the recent state-of-the-art FourierFormer (Nguyen et al., 2022c). The theoretical explanation for the ability to remove outliers using MoM-Fourier attention can be found in Appendix C.

### Incorporating Robust Self-Attention Mechanisms into Transformers

Computational EfficiencyThe two types of robust attention mechanisms proposed above have their own distinct advantages. To expedite the computation for Transformer-RKDE and obtain the attention weight factor in a more efficient manner, we employ a single-step iteration on the alternative updates to approximate the optimal set of weights. Empirical results indicate that this one-step iteration can produce sufficiently precise results. For Transformer-SPKDE, as the optimal set of weights is acquired via the QP solver, it demands more computation time but yields superior performance on both clean and contaminated data. As an alternative to weight-based methods, Transformer-MoM offers significantly greater efficiency while providing competitive performance, particularly with text data. The complete procedure for computing the attention vector for Transformer-RKDE, Transformer-SPKDE, and Transformer-MoM is detailed in Algorithm 1.

Training and InferenceWe incorporate our robust attention mechanisms into both training and inference stages of Transformers. Given the uncertainty about data cleanliness, there's a possibility of encountering contaminated samples at either stage. Therefore, it is worthwhile to defend against outliers throughout the entire process. In the training context, our methods modify the computation of attention vectors across each Transformer layer, making them less susceptible to contamination from outliers. This entire process remains nonparametric, with no introduction of additional model parameters. However, the resulting attention vectors, whether shaped by re-weighting or the median-of-means principle, diverge from those generated by the standard softmax attention. This distinction influences the model parameters learned during training. During inference, the test data undergoes a similar procedure to yield robust attention vectors. This ensures protection against potential outlier-induced disruptions within the test sequence. However, if we assume the availability of a clean training set -- where contamination arises solely from distribution shifts, adversarial attacks, or data poisoning during inference -- it is sufficient to restrict the application of the robust attention mechanism to just the inference phase. This could considerably reduce the computational time required for robust attention vector calculation during training. In our empirical evaluation, we engaged the robust attention mechanism throughout both phases and recorded the associated computational time during training. We found that on standard datasets like ImageNet-1K, WikiText-103, and the UEA time-series classification, infusing the robust attention led to a drop in training loss. This suggests that training data itself may contain inherent noise or outliers.

## 4 Experimental Results

In this section, we provide empirical validation of the benefits of integrating our proposed robust KDE attention mechanisms (Transformer-RKDE/SPKDE/MoM) into Transformer base models. We compare these with the standard softmax Transformer across multiple datasets representing different modalities. These include language modeling on the WikiText-103 dataset (Merity et al., 2016) (Section 4.1) and image classification on ImageNet (Russakovsky et al., 2015; Deng et al., 2009). Furthermore, we assess performance across multiple robustness benchmarks, namely ImageNet-C (Hendrycks and Dietterich, 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019) (Section 4.2), as well as UEA time-series classification (Section 4.3). Our proposed robust trans

    &  &  \\   & Valid PPL/Loss & Test PPL/Loss & Valid PPL/Loss & Test PPL/Loss \\  Transformer (Vaswani et al., 2017b) & 33.15/3.51 & 34.29/3.54 & 72.28/4.45 & 74.56/4.53 \\ Performer (Choromanski et al., 2021) & 32.35/3.48 & 33.49/3.51 & 71.64/4.42 & 73.48/4.49 \\ Transformer-MGK (Nguyen et al., 2022b) & 32.8/3.47 & 33.21/3.51 & 69.78/4.38 & 71.03/4.41 \\ FourierFormer (Nguyen et al., 2022c) & 31.86/3.44 & 32.85/3.49 & 65.76/4.32 & 68.33/4.36 \\  Transformer-RKDE (Huber) & 31.22/3.42 & 32.39/3.47 & 52.14/3.92 & 55.68/3.99 \\ Transformer-RKDE (Hampel) & 31.24/3.42 & 32.35/3.48 & 55.61/3.98 & 57.92/4.03 \\ Transformer-SPKDE & **31.05/3.41** & **32.18/3.46** & 51.36/3.89 & 54.97/3.96 \\ Transformer-MoM & 33.56/3.52 & 34.68/3.55 & 48.29/3.82 & 52.14/3.92 \\ FourierFormer-MoM & 32.26/3.47 & 33.14/3.50 & **47.66/3.81** & **50.96/3.85** \\   

Table 1: Perplexity (PPL) and negative likelihood loss (NLL) of our methods (lower part) and baselines (upper part) on WikiText-103. The best results are highlighted in bold font and the second best are highlighted in underline. On clean data, Transformer-SPKDE achieves better PPL and NLL than other baselines. Under random swap with outlier words., Transformers with MoM self-attention show much better performance.

formers are compared with state-of-the-art models, including Performance Choromanski et al. (2021), MGK Nguyen et al. (2022), RVT Mao et al. (2022), and FourierFormer Nguyen et al. (2022). All experiments were conducted on machines with 4 NVIDIA A-100 GPUs. For each experiment, Transformer-RKDE/SPKDE/MoM were compared with other baselines under identical hyperparameter configurations.

### Robust Language Modeling

WikiText-103 is a language modeling dataset that contains collection of tokens extracted from good and featured articles from Wikipedia, which is suitable for models that can leverage long-term dependencies. We follow the standard configurations in Merity et al. (2016); Schlag et al. (2021) and splits the training data into \(L\)-word independent long segments. During evaluation, we process the text sequence using a sliding window of size \(L\) and feed into the model with a batch size of \(1\). The last position of the sliding window is used for computing perplexity except in the first segment, where all positions are evaluated as in Al-Rfou et al. (2019); Schlag et al. (2021).

In our experiments, we utilized the small and medium (shown in Appendix E) language models developed by Schlag et al. (2021). We configured the dimensions of key, value, and query to \(128\), and set the training and evaluation context length to \(256\). We contrasted our methods with Performer Choromanski et al. (2021), Transformer-MGK Nguyen et al. (2022) and FourierFormer Nguyen et al. (2022), which have demonstrated competitive performance. For self-attention, we allocated \(8\) heads for our methods and Performer, and \(4\) for Transformer-MGK. The dimension of the feed-forward layer was set to \(2048\), with the number of layers established at \(16\). To prevent numerical instability, we used the log-sum-exp trick in equation (3) when calculating the attention probability vector through the Gaussian kernel. We employed similar tactics when computing the attention weight factor of Transformer-RKDE, initially obtaining the weights in log space, followed by the log-sum-exp trick to compute robust self-attention as outlined in equation (9). For Transformer-MoM, the sampled subset sequences constituted \(80\%\) of the length of the original sequence.

Table 1 presents the validation and test perplexity (PPL) for several methods. Both Transformer-RKDE and SPKDE models exhibit an improvement over baseline PPL and NLL on both validation and test sets. However, MoM-based models display slightly higher perplexity, a result of using only a portion of the sequence. When the dataset is subjected to a word swap attack, which randomly substitutes selected keywords with a generic token "\(AAA\)" during evaluation, our method, particularly MoM-based robust attention, yields significantly better results. This is particularly evident when filtering out infrequent words, where the median trick has proven its effectiveness. We also noticed superior robustness in RKDE/SPKDE-based robust attention compared to other baseline methods that were not protected from the attack. Our implementation of the word swap is based on the publicly available TextAttack code by Morris et al. (2020)1. We employed a greedy search method with constraints on stop-word modifications provided by the TextAttack library.

### Image Classification under Adversarial Attacks and Data Corruptions

For initial simplicity, we utilize the original ViT (tiny) as our base model. However, our methods are versatile and can be integrated with more advanced base models to enhance their robustness. As our

    &  &  &  &  \\   & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\  ViT Dosovitskiy et al. (2020) & 72.23 & 91.13 & 52.61 & 82.26 & 41.84 & 76.49 & 48.34 & 79.36 \\ DeiT Touvron et al. (2021) & 74.32 & 93.72 & 53.24 & 84.07 & 41.72 & 76.43 & 49.56 & 80.14 \\ RVT Mao et al. (2022) & **74.37** & **93.89** & 53.67 & 84.11 & 43.39 & 77.26 & 51.43 & 80.98 \\ FourierFormer Nguyen et al. (2022)c & 73.25 & 91.66 & 53.08 & 83.95 & 41.34 & 76.19 & 48.79 & 79.57 \\  ViT-RKDE Huber & 72.83 & 91.44 & 55.83 & 85.89 & 44.15 & 79.06 & 52.42 & 82.03 \\ ViT-RKDE Hampel & 72.94 & 91.63 & 55.92 & 85.97 & 44.23 & 79.16 & 52.48 & 82.07 \\ ViT-SPKDE & 73.22 & 91.95 & **56.03** & **86.12** & **44.51** & **97.47** & **52.64** & **82.33** \\ ViT-MoM & 71.94 & 91.08 & 55.76 & 85.23 & 43.78 & 78.85 & 49.38 & 80.02 \\ FourierFormer-MoM & 72.58 & 91.34 & 53.25 & 84.12 & 41.38 & 76.41 & 48.82 & 79.68 \\   

Table 2: Top-1 and top-5 accuracy (%) on ImageNet. The best results are highlighted in bold font and the second best are highlighted in underlines. RVT Mao et al. (2022) and DeiT Touvron et al. (2021) achieve better results on clean data; meanwhile, Transformers incorporating robust self-attention hold stronger defense under different adversarial attacks while still achieving competitive performance on the original ImageNet.

approaches do not alter the model architecture, each model employs \(5.7M\) parameters. We have also implemented leading-edge methods, including DeiT with hard distillation (Touvron et al., 2021), FourierFormer (Nguyen et al., 2022), and Robust Vision Transformer (RVT) (Mao et al., 2022), as our baselines. It's important to note that, for a fair comparison with RVT, we only incorporated its position-aware attention scaling without further architectural modifications. Consequently, the resulting RVT model comprises approximately \(7.2M\) parameters. To evaluate adversarial robustness, we utilized adversarial examples generated by untargeted white-box attacks, which included the single-step attack method FGSM (Goodfellow et al., 2014), multi-step attack method PGD (Madry et al., 2017), and score-based black-box attack method SPSA (Uesato et al., 2018). These attacks were applied to the entire validation set of ImageNet. Each attack distorts the input image with a perturbation budget \(=1/255\) under \(l_{}\) norm, while the PGD attack uses \(20\) steps with a step size of \(=0.15\). In addition, we assessed our methods on multiple robustness benchmarks, which include images derived from the original ImageNet through algorithmic corruptions or outliers.

Table 2 presents the results under adversarial attack. On clean ImageNet, our performance aligns closely with RVT and DeiT, the leading performers. Notably, our methods outperform RVT under several adversarial attack types, particularly when employing the ViT-SPKDE method. Figure 3 illustrates the relationship between accuracy and perturbation budget across three attack methods. We observe that transformers equipped with robust self-attention mechanisms offer significantly enhanced defense capabilities across different perturbation budgets, with their advantages amplifying as the level of perturbation increases, as expected. We provide more ablation studies in Appendix E that explore diff

Figure 4: Comparison of averaged computation speed (measured by iteration per second) and maximum GPU memory (measured by the CUDA max_memory_allocated function) on ImageNet classification task. The results are measured under the Transformer base models with different capacities: Tiny (5.7M parameters), Small (22M), and Base (86M).

   Dataset & ImageNet-C & ImageNet-A & ImageNet-O & ImageNet-R & ImageNet-Sketch \\  Metric & mCE\(\) & Top-1 Acc\(\) & AUPR\(\) & Top-1 Err Rate\(\) & Top-1 Acc\(\) \\  ViT & 71.14 & 0.18 & 18.31 & 96.89 & 37.13 \\ DeiT & 70.26 & 0.73 & 19.56 & 94.23 & 41.68 \\ RVT & 68.57 & **9.45** & 22.14 & 63.12 & 50.07 \\ FourierFormer & 71.07 & 0.69 & 18.47 & 94.15 & 38.36 \\  ViT-RKDE (Huber) & 68.69 & 6.98 & 25.61 & 64.33 & 45.63 \\ ViT-RKDE (Hampel) & 68.55 & 6.34 & 26.14 & 62.16 & 45.76 \\ ViT-SPKDE & **68.34** & 8.29 & 28.42 & **61.39** & **50.13** \\ ViT-MoM & 69.11 & 2.29 & 28.08 & 76.44 & 36.92 \\ FourierFormer-MoM & 70.62 & 2.42 & **28.86** & 74.15 & 39.44 \\   

Table 3: We evaluated the performance of proposed models across multiple robustness benchmarks, using appropriate evaluation metrics for each. In the majority of cases, our methods outperformed the baselines.

Figure 3: The top-1 classification _accuracy v.s. perturbation budget \(\) 255_ curves on ImageNet against three untargeted attack methods under the \(l_{}\) norm. The proposed set of ViT with robust self-attention mechanisms shows stronger defense under all attack methods with different perturbation budgets.

ferent design choices for each proposed robust KDE attention. Table 3 displays the results across multiple robustness benchmarks, employing appropriate evaluation metrics for each. In most instances, the best performance is achieved by our proposed methods, which clearly improve upon existing baselines. Additionally, Figure 4 demonstrates the scalability of our methods as model size increases. We have excluded the SPKDE-based method from this analysis due to its heightened computational demands. The results indicate that both the computation speed and memory increase as model capacity expands for all methods. The MoM-based robust attention closely mirrors the vanilla Transformer, while the RKDE-based robust attention, with one-step approximation, also demonstrates scalability with larger models.

### UEA Time Series Classification

Lastly, we conducted experiments using five datasets from the UEA Time-Series Classification Archive (Bagnall et al., 2018), and compared the outcomes across various methodologies (Table 4). The baseline implementation and datasets were adapted from Wu et al. (2022). Our findings indicate that our proposed approaches can notably enhance classification accuracy.

## 5 Conclusion and Future Work

In this work, we explored the link between the dot-product self-attention mechanism and non-parametric kernel regression. This led to the development of a family of fortified transformers, which leverage robust KDE as an alternative to dot-product attention, mitigating the impacts of contaminated samples. We proposed two variants of robust self-attention mechanisms designed to either down-weight or filter out potential corrupted data, both of which can be seamlessly integrated into commonly used transformer models. As our ongoing effort, we are exploring more efficient techniques for estimating the weight set for robust KDE in extremely large models, and incorporating regularization strategies to mitigate the instability of kernel regression with outliers.

## 6 Acknowledgment

Xing Han and Joydeep Ghosh acknowledge support from Intuit Inc. Nhat Ho acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning.