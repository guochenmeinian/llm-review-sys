# A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries

in Probabilistic Models

 Shivvrat Arya

Department of Computer Science

The University of Texas at Dallas

shivvrat.arya@utdallas.edu

&Tahrima Rahman

Department of Computer Science

The University of Texas at Dallas

tahrima.rahman@utdallas.edu

&Vibhav Gogate

Department of Computer Science

The University of Texas at Dallas

vibhav.gogate@utdallas.edu

###### Abstract

We propose a novel neural networks based approach to efficiently answer arbitrary Most Probable Explanation (MPE) queries--a well-known NP-hard task--in large probabilistic models such as Bayesian and Markov networks, probabilistic circuits, and neural auto-regressive models. By arbitrary MPE queries, we mean that there is no predefined partition of variables into evidence and non-evidence variables. The key idea is to distill all MPE queries over a given probabilistic model into a neural network and then use the latter for answering queries, eliminating the need for time-consuming inference algorithms that operate directly on the probabilistic model. We improve upon this idea by incorporating inference-time optimization with self-supervised loss to iteratively improve the solutions and employ a teacher-student framework that provides a better initial network, which in turn, helps reduce the number of inference-time optimization steps. The teacher network utilizes a self-supervised loss function optimized for getting the exact MPE solution, while the student network learns from the teacher's near-optimal outputs through supervised loss. We demonstrate the efficacy and scalability of our approach on various datasets and a broad class of probabilistic models, showcasing its practical effectiveness.

## 1 Introduction

Probabilistic representations such as Probabilistic Circuits (PCs) , graphical models  such as Bayesian Networks (BNs) and Markov Networks (MNs), and Neural Autoregressive Models (NAMs)  are widely used to model large, multi-dimensional probability distributions. However, they face a significant challenge: as the complexity of these distributions increases, solving practically relevant NP-hard inference tasks such as finding the Most Probable Explanation (MPE) via exact inference techniques [39; 40] becomes increasingly difficult and time-consuming. In particular, although various exact and approximate solvers exist for the MPE task in PCs, BNs and MNs, exact solvers are often too slow for practical use, and approximate solvers tend to lack the necessary accuracy, particularly in autoregressive models that currently rely on slow hill-climbing/beam search methods.

In recent work, Arya et al.  proposed a method to overcome the limitations of existing approximate methods by using neural networks (NNs) to solve the MPE task in PCs.1 Their method draws inspiration from the learning to optimize literature . Given a PC and a _predefined partition of variables into query and evidence sets_, the core idea is to train a NN that takes an assignment to the evidence variables as input and outputs the most likely assignment to the query variables w.r.t. the distribution defined by the PC. Arya et al. suggest using either supervised or self-supervised learning techniques to train the NN; the former requires access to exact inference schemes, while the latter does not and is therefore more practical.

In this paper, we address a more general and complex version of the MPE task than the one considered by Arya et al. Specifically, we assume that there is _no predefined partition of the variables into evidence and query sets_, which we refer to as the **any-MPE** task. The complexity of the any-MPE task arises from the exponential increase in the number of input configurations, compounded by the exponential number of possible divisions of variables into evidence and query sets. Furthermore, our method applies to a broad class of probabilistic models, including BNs, MNs and NAMs, whereas Arya et al.'s method is limited to PCs. In addition, Arya et al.'s method does not fully exploit the capabilities of self-supervision, and the benefits of combining supervised and self-supervised loss functions.

This paper presents a novel approach that uses a NN for solving the any-MPE task in a broad class of probabilistic models (PMs) and achieves technical advancements in three key aspects:

**1. Efficient MPE Inference via Encoding Scheme and Loss Function:** We introduce a new encoding scheme that tailors the NN architecture to the specific structure of the input PM. This scheme not only delineates the input and output nodes for the NN but also establishes a methodology for setting input values and extracting the MPE solution from the NN's outputs. Furthermore, we propose a tractable, and differentiable self-supervised loss function, enabling efficient training.

**2. Inference Time Optimization with ITSELF:** We introduce a novel inference technique called Inference Time Self Supervised Training (ITSELF). This technique iteratively refines the MPE solution during the inference process itself. It utilizes gradient descent (_back-propagation_) to update the NN's parameters using our proposed self-supervised loss, leading to continual (anytime) improvement towards near-optimal solutions. ITSELF fully utilizes the power of our self-supervised loss, as it does not require labeled data or an external MPE solver.

**3. Two-Phase Pre-training with Teacher-Student Architecture:** To address challenges associated with self-supervised learning and ITSELF, we propose a two-phase pre-training strategy that leverages a teacher-student architecture. Self-supervised learning can suffer from overfitting and requires careful regularization. Additionally, ITSELF, especially with random initializations, might necessitate a substantial number of gradient updates to converge on optimal solutions. Our approach addresses these issues using the following methodology: (i) The teacher network first overfits the training data using ITSELF and (ii) The student network is then trained using supervised loss functions (e.g., binary cross-entropy) by treating the teacher network's output as pseudo-labels. This supervised training phase improves and regularizes the parameter learning process of the student network. It also provides a robust starting point for ITSELF, significantly reducing the required optimization steps and leading to substantial performance gains.

Finally, we conduct a detailed experimental comparison of our method with existing approaches on several types of PMs such as PCs, PGMs and NAMs. Our results demonstrate that our method surpasses state-of-the-art approximate inference techniques in terms of both accuracy and speed.

## 2 Background and Motivation

Without loss of generality, we use binary variables which take values from the set \(\{0,1\}\). We denote a random variable by an uppercase letter (e.g., \(X\)), and a value assigned to it by the corresponding lowercase letter (e.g., \(x\)). We denote a set of random variables by a bold uppercase letter (e.g., \(\)) and an assignment of values to all variables in the set by the corresponding bold lowercase letter (e.g., \(\)).

Throughout the paper when we use the term probabilistic models (PMs), we are referring to a broad class of probabilistic models in which computing the likelihood2 of an assignment to all variables in the model can be done in polynomial (preferably linear) time in the size of the model. This class includes, among others, Bayesian and Markov networks collectively called Probabilistic Graphical Models (PGMs) , smooth and decomposable Probabilistic Circuits (PCs) , and Neural Autoregressive Models (NAMs) such as NADE  and MADE .

We are interested in solving the most probable explanation (MPE) task in PMs, namely the task of finding the most likely assignment to all unobserved (non-evidence) variables given observations (evidence). Formally, let \(\) denote a probabilistic model defined over a set of variables \(\) that represents the distribution \(_{}()\). We categorize the variables \(\) into evidence \(\) and query \(\) groups, ensuring that \(=\) and \(=\). Then, given an assignment \(\) to the set of evidence variables \(\), the MPE task can be formulated as:

\[(,)=*{argmax}_{} _{}(|)=*{argmax} _{}\{_{}(,)\}\] (1)

It is known that the MPE task is NP-hard in general and even hard to approximate [9; 11; 36; 41; 44].

**Motivation:** The goal of this paper is to develop a method that trains a NN for a given PM and, at test time, serves as an approximate MPE solver for any-MPE query posed over the PM. By any-MPE, we mean that the NN can take an assignment to an arbitrary subset of variables (evidence) as input and output the most likely assignments to the remaining (query) variables. Recently, Arya et al.  proposed a NN-based solution for solving the MPE task in PCs under the constraint that the partition of the variables into evidence and query sets _is known before training the NN_. This constraint is highly restrictive because, for generative models, it is unlikely that such a partition of variables is known in advance. In such cases, one would typically train a discriminative model rather than a generative one. Unlike Arya et al.'s method, our approach yields an any-MPE solver. Additionally, Arya et al.'s approach has several limitations in that it does not fully exploit the benefits of self-supervision during inference time and requires the use of relatively large NNs to achieve good performance in practice. Our proposed approach, described next, addresses these limitations.

## 3 A Self-Supervised Neural Approximator for any-MPE

In this section, we develop a neural network (NN) based approach for solving the _any-MPE_ task. Specifically, given a PM, we develop an input encoding (see Section 3.1) that determines the number of input nodes of the NN and sets their values for the given MPE query. Additionally, we develop an output encoding scheme that specifies the number of NN output nodes required for the given PM and enables the recovery of the MPE solution from the outputs. For training the NN, we introduce a tractable and differentiable self-supervised loss function (see Section 3.2), whose global minima aligns with the MPE solutions to efficiently learn the parameters of the NN given _unlabeled data_.

### An Encoding For any-MPE Instances

Since NNs require fixed-sized inputs and outputs, we introduce input and output encodings that generate fixed-length input and output vectors for each PM from a given MPE problem instance \((,)\). To encode the input, for each variable \(X_{i}\), we associate two input nodes in the NN, denoted by \(_{i}\) and \(_{i}\). Thus for a PM having \(n\) (namely, \(||=n\)) variables, the corresponding NN has \(2n\) input nodes. Given a query \((,)\), we set the values of the input nodes as follows: (1) If \(X_{i}\) and \(X_{i}=0\) is in \(\), then we set \(_{i}=0\) and \(_{i}=1\); (2) If \(X_{i}\) and \(X_{i}=1\) is in \(\), then we set \(_{i}=1\) and \(_{i}=0\); and (3) If \(X_{i}\) then we set \(_{i}=0\) and \(_{i}=0\). (The assignment \(_{i}=1\) and \(_{i}=1\) is not used.) It is easy to see that the input encoding described above yields an _injective_ mapping between the set of all possible MPE queries over the given PM and the set \(\{0,1\}^{2n}\). This means that each unique MPE query \((,)\) will yield a unique 0-1 input vector of size \(2n\).

The output of the neural network comprises of \(n\) nodes with sigmoid activation, where each output node is associated with a variable \(X_{i}\). We ignore the outputs corresponding to the evidence variables and define a loss function over the outputs corresponding to the query variables in the set \(\). The MPE solution can be reconstructed from the output nodes of the NN by thresholding the output nodes corresponding to the query variables appropriately (e.g., if the value of the output node is greater than 0.5, then the query variable is assigned the value \(1\); otherwise it is assigned to \(0\)).

### A Self-Supervised Loss Function for any-MPE

Since the output nodes of our proposed NN use sigmoid activation, each output is continuous and lies in the range \(\). Given an MPE query \((,)\), let \(^{c}^{||}\) denote the (continuous) _Most Probable Explanation_ (MPE) assignment predicted by the NN. In MPE inference, given \(\), we want to find an assignment \(\) such that \(_{}(,)\) is maximized, namely, \(-_{}(,)\) is minimized. Thus, a natural loss function that we can use is \(-_{}(,)\). Unfortunately, the NN outputs a continuous vector \(^{c}\) and as a result \(_{}(^{c},)\) is not defined.

Next, we describe how to solve the above problem by leveraging the following property of the class of PMs that we consider in this paper--specifically BNs, MNs, PCs and NAMs. In these PMs, the function \((,)=-_{}(, )\), which is a function from \(\{0,1\}^{n}\) is either a multi-linear polynomial or a neural network, and can be computed in linear time in the size of the PM. To facilitate the use of continuous outputs, we define a loss function \(^{c}(^{c},):^{n}\) such that \(^{c}\) coincides with \(\) on \(\{0,1\}^{n}\). For PGMs and PCs, \(\) is a multi-linear function and \(^{c}\) is obtained by substituting each occurrence of a discrete variable \(q_{i}\) with the corresponding continuous variable \(q_{i}^{c}^{c}\) where \(q_{i}^{c}\). In NAMs, \(\) is a NN and we can perform a similar substitution--we substitute each binary input \(q_{i}\) in the NN with a continuous variable \(q_{i}^{c}\). This substitution transforms the discrete NN into a continuous function while preserving its functional form.

An important property of \(^{c}\) is that it can be evaluated and differentiated in polynomial time. Moreover, when \(\) is defined by either a neural network (in NAMs) or a multilinear function (in BNs, MNs and PCs), the minimum value of \(^{c}\) over the domain \(^{n}\) is less than or equal to the minimum value of the original function \(\) over the discrete domain \(\{0,1\}^{n}\). Formally,

**Proposition 1**.: _Let \(l(,):\{0,1\}^{n}\) be either a neural network or a multilinear function, and let \(l^{c}(^{c},):^{n}\) be its continuous extension obtained by substituting each binary input \(q_{i}\) with a continuous variable \(q_{i}^{c}\). Then,_

\[_{^{c}^{n}}^{c}(^{c},)_ {\{0,1\}^{n}}(,)\]

Following Arya et al. , we propose to improve the quality of the loss function by tightening the lower bound given in proposition 1 with an entropy-based penalty (\(_{E}\)), governed by \(>0\).

\[_{E}(^{c},)=-_{j=1}^{||}[q_{j}^{ c}(q_{j}^{c})+(1-q_{j}^{c})(1-q_{j}^{c})]\] (2)

This penalty encourages discrete solutions by preferring \(q_{j}^{c}\) values close to 0 or 1, where \(\) modulates the trade-off. Setting \(\) to 0 yields the continuous approximation; conversely, an \(\) value of \(\) results exclusively in discrete outcomes. From proposition 1 and by using the theory of Lagrange multipliers, we can show that for any \(>0\), the use of the entropy penalty yields a tighter lower bound:

**Proposition 2**.: \[_{^{c}^{n}}^{c}(^{c},)_ {^{c}^{n}}^{c}(^{c},)+_{E}( ^{c},)_{\{0,1\}^{n}}(, )\]

**How to use the Loss Function:** Given a PM defined over \(n\) variables, we can use the self-supervised loss function \(^{c}(^{c},)+_{E}(^{c},)\) (treating \(\) as a hyper-parameter) to train any neural network (NN) architecture that has \(2n\) input nodes and \(n\) output nodes. This trained NN can then be used to answer any arbitrary MPE query posed over the PM. The training data for the neural network consists of assignments (evidence \(\)) to a subset of the variables. Each training example can be generated using the following three-step process. We first sample a full assignment \(\) to all variables in the PM using techniques like Gibbs sampling or perfect sampling for tractable distributions such as PCs and BNs. Second, we choose an integer \(k\) uniformly at random from the range \(\{1,,n\}\) and designate \(k\) randomly selected variables as evidence variables \(\), and the remaining \(n-k\) as query variables \(\). Finally, we project the full assignment \(\) on \(\). The primary advantage of using the self-supervised loss function is that it eliminates the need for access to a dedicated MPE solver to provide supervision during training; gradient-based training of the neural network provides the necessary supervision.

### Inference-Time Neural Optimization using Self-Supervised Loss

At a high level, assuming that the NN is over-parameterized, if we use the self-supervised loss and repeatedly run (stochastic) gradient updates over the NN for a given dataset, theoretical results [2; 13] as well as prior experimental work [46; 56] suggest that the parameters of the NN will converge to a point near the global minimum of the self-supervised loss function. This means that through gradient updates, the network will find a near-optimal MPE assignment for each training example. This strategy of performing gradient updates over the NN can also be used _during inference (test) time to iteratively improve the MPE solution_, thereby maximizing the benefits of self-supervision.

Specifically, at test time, given a test dataset (or example), we initialize the NN either randomly or using a pre-trained model and then run gradient-based updates over the NN iteratively until convergence. The gradient is computed w.r.t. the self-supervised loss function \(^{c}(^{c},)+_{E}(^{c},)\). We call the resulting algorithm ITSELF (Inference Time Optimization using SELF-Supervised Loss), as detailed in Figure 1. The performance of ITSELF typically improves with each iteration until the loss converges.

Our proposed method, ITSELF, is closely related to test-time training approaches which are widely used to solve problems in deep learning [1; 10; 30; 31; 32; 38; 49; 51; 57]. Our method differs from these previous approaches in that the global minima of our proposed self-supervised loss correspond to the MPE solutions, provided that the penalty \(\) is sufficiently large.

## 4 Supervised Knowledge Transfer from ITSELF

A drawback of our self-supervised loss function is that, unlike supervised loss functions such as binary cross entropy, it is a non-convex function of the NN outputs3. As a result, it has a significantly larger number of local minima compared to the supervised loss function, but also a potentially exponential number of global minima, because an MPE problem can have multiple optimal solutions , all of which have the same loss function value. Thus, optimizing and regularizing using the self-supervised loss is difficult compared to a supervised loss, especially when the number of training examples is large.

Moreover, our experiments show that large datasets necessitate large, over-parameterized neural networks (NNs) to achieve near-optimal MPE solutions for all examples. However, when the training data is limited and the NN is sufficiently over-parameterized, our preliminary findings, along with theoretical and empirical results from prior studies [3; 28; 27; 6; 23], suggest that the NN is more likely to approach the global optima. Specifically, with a reasonably sized NN and a small dataset, the algorithm ITSELF tends to yield near-optimal MPE solutions. A further challenge with ITSELF is that even for small datasets, achieving convergence from a random initialization requires numerous iterations of gradient descent, rendering the training process inefficient and slow.

### Teacher-Student Strategy

To address these challenges (using small datasets with ITSELF; designing better initialization for it; and using non-convex loss functions for training), we propose a two-network teacher-student strategy [7; 16; 20; 21; 22; 24; 37; 52; 53; 54], where we have two networks with the same structure that are trained via mini-batch gradient updates. The teacher network is overfitted to the mini-batch using our self-supervised loss via the ITSELF algorithm, and the student network is subsequently trained with a supervised loss function such as binary cross entropy. By overfitting the teacher network via ITSELF on the mini-batch, we ensure that it finds near-optimal MPE assignments for all (unlabeled) examples in the mini-batch and eventually over the whole training dataset.

The student network then learns from the teacher's outputs, using them as soft labels in a supervised learning framework. This transfer of knowledge mitigates the optimization difficulties associated with the non-convex self-supervised loss, allowing the student network to achieve faster convergence and better generalization with a more manageable model size. Additionally, this strategy reduces the need for severe over-parameterization and extensive training iterations for the teacher network because it is operating on a smaller dataset. It also helps achieve better initialization for ITSELF.

### Training Procedure

Our proposed training procedure, which we call \(\), is detailed in Algorithm 1. The algorithm trains a two-network system comprising a teacher network (\(\)) and a student network (\(\)) with the same structure. The goal is to train the student network using a combination of self-supervised and supervised learning strategies. The algorithm takes as input the training data \(\), along with the teacher and student networks, \(\) and \(\), respectively and outputs a trained network \(\). A database (\(DB\)) is utilized to store the best MPE assignment and corresponding loss value for each example in \(\). The parameters of \(\) and \(\), and the entries in \(DB\), are randomly initialized at the start.

In each epoch, a mini-batch \(^{}\) is sampled from the training data \(\). The parameters of the teacher network \(\) are then updated using the ITSELF algorithm (which uses a self-supervised loss), applied to the mini-batch \(^{}\) (the mini-batch helps address large data issues associated with ITSELF). For each example \(_{i}\) in \(^{}\), we perform a forward-pass over \(\) to obtain an MPE assignment \(_{i}\). The database \(DB\) is subsequently updated with \(_{i}\) if it has a lower loss value than the current entry for \(_{i}\).

Following this, the parameters of the student network \(\) are updated using the mini-batch \(^{}\), the labels from \(DB\), and a supervised loss function (\(_{sup}\)) such as Binary Cross Entropy or \(L2\) loss. Finally, the parameters of the teacher network \(\) are reinitialized with the updated parameters of the student network \(\) to prepare for the next epoch (addressing the initialization issue associated with ITSELF). Figure 2 illustrates a single training epoch of GUIDE.

Thus, at a high level, Algorithm 1 leverages the strengths of both self-supervised and supervised learning to improve training efficiency and reduce the model complexity, yielding a student network \(\). Moreover, at test time, the student network can serve as an initialization for ITSELF.

Experiments

This section evaluates the ITSELF method (see section 3.3), the \(\) teacher-student training method (see section 4) and the method that uses only self-supervised training, which we call SSMP (see section 3.2). We benchmark these against various baselines, including neural network-based and traditional polynomial-time algorithms that directly operate on the probabilistic model. We begin by detailing our experimental framework, including competing methods, evaluation metrics, neural network architectures, and datasets.

### Datasets and Graphical Models

We used twenty binary datasets extensively used in tractable probabilistic models literature [5; 18; 34; 50]--referred to as TPM datasets--for evaluating PCs and NAMs. For the purpose of evaluating PGMs, we utilized high treewidth models from previous UAI inference competitions .

To train Sum Product Networks (SPNs), our choice of PCs, we employed the DeeProb-kit library , with SPN sizes ranging from \(46\) to \(9666\) nodes. For NAMs, we trained Masked Autoencoder for Distribution Estimation (MADE) models using PyTorch, following the approach in Germain et al. . For Markov Networks (MNs), a specific type of PGM, we applied Gibbs sampling to generate 8,000, 1,000, and 1,000 samples for the training, testing, and validation sets, respectively. The query ratio (\(qr\)), defined as the fraction of variables in the query set, was varied across the set \(\{0.1,0.3,0.5,0.7,0.8,0.9\}\) for each probabilistic model (PM).

### Baseline Methods and Evaluation Criteria

**PCs** - We used three polynomial-time baseline methods from the probabilistic circuits and probabilistic graphical models literature as benchmarks [41; 45].

* MAX Approximation (MAX)  transforms sum nodes into max nodes. During the upward pass, max nodes output the highest weighted value from their children. The downward pass, starting from the root, selects the child with the highest value at each max node and includes all children of product nodes.
* Maximum Likelihood Approximation (ML)  computes the marginal distribution \(p_{}(Q_{i}|)\) for each variable \(Q_{i}\), setting \(Q_{i}\) to its most likely value.
* Sequential Approximation (Seq)  iteratively assigns query variables according to an order \(o\). At each step \(j\), it selects the \(j\)-th query variable \(Q_{j}\) in \(o\) and assigns to it a value \(q_{j}\) such that \(_{}(q_{j}|,)\) is maximized, where \(\) is an assignment of values to all query variables from \(1\) to \(j-1\).

We further evaluated the impact of initializing stochastic hill climbing searches using solutions from all baseline approaches and our proposed methods for MPE inference, conducting 60-second searches for each MPE problem in our experiments, as detailed in Park and Darwiche .

**NAMs** - As a baseline, we used the stochastic hill-climbing search (HC) algorithm. Following a procedure similar to that used for PCs, we conducted a 60-second hill-climbing search for each test example, with query variables initialized randomly and setting evidence variables according to the values in the given example.

**PGMs** - We employed the distributed AND/OR Branch and Bound (AOBB) method  as a baseline, using the implementation outlined in Otten . Since AOBB is an anytime algorithm, we set a 60-second time limit for inference per test example.

**Neural Baselines** - Arya et al.  introduced Self-Supervised learning based MMAP solver for PCs (SSMP), training a neural network to handle queries on a fixed variable partition within PCs. We extend this approach to address the any-MPE task in PMs (see Section 3.2), using a single network to answer any-MPE queries as an additional neural baseline.

**Evaluation Criteria** - We evaluated competing approaches based on log-likelihood (LL) scores, calculated as \( p_{}(,)\), and inference times for given evidence \(\) and query output \(\). Higher log-likelihood scores indicate better performance, while shorter inference times are preferable.
We implemented two neural network training protocols for each PM and query ratio: SSMP and \(\). Each model was trained for 20 epochs following the training procedure outlined by Arya et al.  for SSMP. Both protocols employed two distinct inference strategies, thus forming four neural-based variants. In the first strategy, we performed a single forward pass through the network to estimate the values of query variable, as specified by Arya et al. . The second strategy utilized our novel test-time optimization-based ITSELF approach for inference. The ITSELF optimization terminates after 100 iterations or upon loss convergence for both PCs and PGMs. For NAMs, we increase the limit to 1,000 iterations while keeping the convergence criterion.

We standardized network architectures for PMs across all experiments. For PCs, we used fully connected Neural Networks (NN) with three hidden layers (128, 256, 512 nodes). For NAMs and PGMs, a single hidden layer of 512 nodes was employed. All hidden layers featured ReLU activation, while the output layers used sigmoid functions with dropout for regularization . We optimized all models using Adam  and implemented them in PyTorch  on an NVIDIA A40 GPU.

**Results for PCs**: We compare methods--including three polynomial-time baselines, neural network-based SSMP, and our ITSELF and \(\) methods--on 20 TPM datasets as shown in the contingency table in figure 2(a) (detailed results in the supplementary materials). We generated 120 test datasets for the MPE task using 20 PCs across 6 query ratios (\(qr\)). Each cell \((i,j)\) in the table represents how often (out of 120) the method in row \(i\) outperformed the method in column \(j\) based on average log-likelihood scores. Any difference between 120 and the combined frequencies of cells \((i,j)\) and \((j,i)\) indicates cases where the compared methods achieved similar scores. We present similar contingency tables for Hill Climbing Search over PCs (Fig. 2(b)), NAMs (Fig. 2(c)), and PGMs (Fig. 2(d)) to benchmark the proposed methods against the baselines.

The contingency table for PC (Fig. 2(a)) shows that methods incorporating ITSELF consistently outperform both polynomial-time and traditional neural baselines, as indicated by the dark blue cells in the corresponding rows. Notably, \(\) + ITSELF is superior to all the other methods in almost two-thirds of the 120 cases, while SSMP + ITSELF is better than both SSMP and \(\). In contrast, the polynomial-time baseline MAX is better than both SSMP and \(\) (as used in Arya et al. ), highlighting ITSELF's significant role in boosting model performance for the complex _any-MPE_ task.

We compare MAX and \(\) + ITSELF using a heatmap in Figure 3(a). The y-axis presents datasets by variable count and the x-axis represents query ratio. Each cell displays the percentage difference in mean LL scores between the methods, calculated as \(\%=100(l_{nn}-l_{max})/||l_{max}|\). The heatmap shows that \(\) + ITSELF achieves performance comparable to MAX for small query sets. As the problem complexity increases with an increase in query set size, our method consistently outperforms MAX across all datasets, except for NLTCS and Tretail, as highlighted by the green cells. In the 12 cases where \(\) + ITSELF underperforms, the performance gap remains minimal, as indicated by the limited number of red cells in the heatmap.

Figure 3: MPE method comparison across PMs. Blue shows row superiority, red shows column superiority; darker shades indicate larger values.

Figure 2(b) further analyzes the performance of our proposed methods against various baselines as initialization strategies for Hill Climbing Search. This comparison evaluates the effectiveness of ITSELF and \(\) in enhancing _anytime methods_ compared to conventional heuristic initialization approaches. Notably, methods incorporating ITSELF provide superior initialization for local search-based algorithms.

**Results for NAMs**: The contingency table in Figure 2(c) presents our evaluation of several methods for NAMs, including HC and two neural network approaches, SSMP and \(\), each tested with two inference schemes. We evaluated these methods on 20 TPM datasets, creating 80 test sets for the MPE task using 20 MADEs across four query ratios (\(qr\)).

The \(\) + ITSELF approach demonstrates superior performance compared to both baseline methods and other neural inference schemes, aligning with observations from PC. While HC outperforms SSMP, both \(\) and the combination of SSMP-based training with ITSELF-based inference surpass HC, highlighting their advantages over the baseline.

The heatmaps in Figure 3(b) further highlight the superior performance of \(\) + ITSELF for NAMs, particularly in larger datasets where it outperforms the HC baseline by over 50% in most cases, as indicated by the dark green cells. The combination of \(\)-based learning with ITSELF-based inference consistently outperforms the baseline across most datasets, with exceptions only in the Mushrooms, Connect 4, and Retail. Overall, the \(\) + ITSELF approach significantly enhances the quality of the MPE solutions in NAM models.

**Results for PGMs**: The contingency table in 2(d) compares the performance of AOBB and four neural-network-based methods on PGMs across four high-treewidth networks. For this evaluation, we generated 16 test datasets for the MPE task using four PGMs across four query ratios (\(qr\)).

Consistent with results from previous PMs, methods using ITSELF for inference consistently outperform the baseline methods AOBB and SSMP across most scenarios. Both \(\) and SSMP outperform AOBB in at least 50 percent of the tests. The supplementary material presents comparisons against exact solutions, conducted on less complex probabilistic models where ground truth computation remains tractable.

**Does a teacher-student-based network outperform a single network trained with the self-supervised loss? (\(\) vs. SSMP):**

This analysis aims to evaluate the performance of \(\) against traditional neural network training methods used in SSMP across different PMs and inference schemes. Using traditional inference scheme (i.e., one forward pass through the network), \(\) consistently outperforms SSMP, demonstrating its superiority in 60% of scenarios for PCs, more than 80% for NAM models, and 75% for PGM models. When employing ITSELF-based inference, \(\) maintains this advantage, achieving higher quality solutions in more than 75%, 85%, and 80% of cases for PCs, NAMs, and PGMs, respectively. Therefore, models trained using \(\) are consistently superior to those trained with SSMP for the _any-MPE_ task.

Figure 4: Heatmaps showing LL % Differences. Top: PC; Bottom: NAM. Green cells: our method is better. Darker shades indicate larger values.

#### Does inference time optimization improve performance? (One-Pass vs. Multi-Pass):

In this analysis, we compare the performance of the single-pass inference method to that of the proposed multi-pass inference method (ITSELF). ITSELF combined with SSMP training outperforms the other methods in over 85% cases for PC, and more than 75% for NAM and PGM models. When used on models trained with \(\), ITSELF demonstrates even better results, achieving superior performance in nearly 90% of PC cases and 75% for both NAMs and PGMs. Overall, \(\) with ITSELF inference emerges as the most effective method across all experiments. Empirical evidence consistently demonstrates ITSELF's superiority over single-pass inference across PMs.

The inference time analysis, detailed in the supplementary material, compares computational efficiency across methods using the natural logarithm of execution time in microseconds. Neural network-based approaches with traditional inference demonstrate the fastest performance across all PMs, as they only require a single forward pass to compute query variable values. For MADE, models trained with \(\) and ITSELF are the next most efficient. In PGMs, \(\) + ITSELF ranks third, followed by SSMP + ITSELF. For PCs, MAX is marginally faster than both \(\) + ITSELF and SSMP + ITSELF, while ML and Seq have the longest computational times. In general, models trained with \(\) achieve shorter inference times than those trained with the self-supervised loss (SSMP), as they require fewer ITSELF iterations due to more effective initial training.

**Summary:** Our experiments demonstrate that \(\) + ITSELF outperforms both polynomial-time and neural-based baselines across various PMs, as evidenced by higher log-likelihood scores. Notably, ITSELF demonstrates significant advantages over traditional single-pass inference in addressing the complex _any-MPE_ query task within probabilistic models, emphasizing the importance of Inference Time Optimization. Furthermore, the superior performance of models trained with \(\) compared to SSMP highlights the effectiveness of the dual network approach, which improves initial model quality and establishes an optimal starting point for ITSELF.

## 6 Conclusion and Future Work

We introduced novel methods for answering Most Probable Explanation (MPE) queries in probabilistic models. Our approach employs self-supervised loss functions to represent MPE objectives, enabling tractable loss and gradient computations during neural network training. We also proposed a new inference time optimization technique, ITSELF, which iteratively improves the solution to the MPE problem via gradient updates. Additionally, we introduced a dual-network-based strategy that combines supervised and unsupervised training which we call \(\) to provide better initialization for ITSELF and addressing various challenges associated with self-supervised training. Our method was tested on various benchmarks, including probabilistic circuits, neural autoregressive models, and probabilistic graphical models, using 20 binary datasets and high tree-width networks. It outperformed polytime baselines and other neural methods, substantially in some cases. Additionally, it improved the effectiveness of stochastic hill climbing (local) search strategies.

Future work includes solving complex queries in probabilistic models with constraints; training neural networks with losses from multiple probabilistic models to embed their inference mechanisms; boosting performance by developing advanced encoding strategies for similar tasks; implementing sophisticated neural architectures tailored to probabilistic models; etc.