# WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks

Leo Boisvert\({}^{*}{}^{1}{}^{2}{}^{3}\)  Megh Thakkar\({}^{*}{}^{1}{}^{2}{}^{4}\)  Maxime Gasse\({}^{}{}^{1}{}^{2}{}^{3}\)  Massimo Caccia\({}^{}{}^{1}{}^{1}\)

**Thibault Le Sellier De Chezelles\({}^{}{}^{1}{}^{3}\)  Quentin Cappart\({}^{2}{}^{3}\)  Nicolas Chapados\({}^{1}{}^{2}{}^{3}\)  Alexandre Lacoste\({}^{}{}^{1}\)  Alexandre Drouin\({}^{}{}^{1}{}^{2}\) \({}^{1}\)**

\({}^{1}\)ServiceNow Research, \({}^{2}\)Mila, \({}^{3}\)Polytechnique Montreal, \({}^{4}\)Chandar Research Lab

{leo.boisvert, megh.thakkar, alexandre.drouin}@servicenow.com

Equal contribution.Core contributors.Equal supervision.

###### Abstract

The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.

## 1 Introduction

In 2024, the average adult spends about 400 minutes every day interacting with computer software and the internet . While some of this time is devoted to productive work, entertainment or creative endeavors, a significant portion is consumed by monotonous, low-value tasks, particularly in enterprise settings. Employees often engage in tedious tasks such as searching for information hidden deeply in knowledge bases, coordinating group discussions to schedule meetings, or filling expense reports in counter-intuitive user interfaces designed for functionality rather than user-friendliness . One easily envisions a future where autonomous assistants--AI _agents_--handle these tasks, allowing humans to focus on more complex, skill-intensive work that generates greater value. Recent advances in the reasoning and planning capacities of large language models (LLMs), with developments such as Chain-of-Thought , ReAct , and Tree-of-Thought  (see  for a review) suggest that this future might be within reach.

While the development of autonomous agents has long been a major research topic both in academia and industry, the recent advances in LLM capabilities have led to a massive surge of interest forLLM-based autonomous agents (Wang et al., 2024). One major application is software control, with a large body of work focused on using LLMs to interact via Application Programming Interfaces (APIs) (Hao et al., 2024; Du et al., 2024). Another line of research focuses on using LLMs for human-like interactions, by directly manipulating graphical User Interfaces (UIs) on mobile devices (Li et al., 2020; Rawles et al., 2023), desktops (Xie et al., 2024), or websites (He et al., 2024). This last category encompasses the field of _web agents_, which can automate software interaction even in environments without APIs, improve human productivity, and accessibility for users with disabilities.

In this work we propose _WorkArena++_, a challenging new benchmark to study the proficiency of web agents at solving common knowledge work tasks in enterprise settings. Built on top of the ubiquitous ServiceNow platform, which reported a customer base of more than 7,000 companies and 85% of the Fortune 500 in 2023 (Mastantonouo, 2023), it provides a free and robust, yet a realistic evaluation environment for task automation in the workplace. _WorkArena++_ expands the _WorkArena_ benchmark introduced by Drouin et al. (2024) with 682 challenging new tasks. While the tasks in _WorkArena_ are far from being solved by current web agents, they remain predominantly atomic, with simple goals such as filling out a single form with explicit values or navigating explicit menu entries. _WorkArena++_ enhances the scale, realism, and complexity of _WorkArena_ with composite tasks designed to require skills like problem-solving and memorization, in order to better evaluate the capabilities of web agents at solving complex work tasks. Our contributions are as follows:

* We introduce the WorkArena++ benchmark, considerably expanding the work of Drouin et al. (2024) from \(33\) to \(682\) tasks through the addition of realistic office worker trajectories, exemplified in Fig. 1, requiring skills like problem-solving, data-driven decision-making, and more (SS 3). Importantly, this is the first benchmark for web agents to require such complex skills.
* We make a series of technical contributions to WorkArena, such as added visual diversity through the introduction of 10 fictitious companies along with customized UI color schemes, improved database isolation between tasks to facilitate parallel evaluation, a new framework for easily expanding the set of tasks through the composition of low-level building blocks, and the possibility of extracting Oracle-based observation-action traces for fine-tuning (SS 3.3).
* We conduct an empirical study to assess both the difficulty and feasibility of our benchmark, with autonomous agents based on state-of-the-art (visual) language models, both closed and open source, as well as human agents as a baseline. Results indicate that _WorkArena++_ presents considerable challenges for current web agents while being reasonably easy for humans (SS 4.3), suggesting its value and relevance as an evaluation benchmark for the scientific community.

Figure 1: Example WorkArena++ task: Restock low inventory items. Here, the agent acts as an IT worker tasked with restocking items that are below some threshold in stock: 1 As is common, it receives instructions via a ticket assigned to them in the system; 2 it must then read the dashboard to extract all items whose stock count is low; 3 reorder the items from the service catalog to match a minimum stock quantity, and 4 close the ticket assigned to them once the task is completed.

## 2 Background

Before diving into our main contribution WorkArena++, we summarize BrowserGym and WorkArena, which respectively provide the environment in which agents interact with the benchmark, and the atomic tasks upon which WorkArena++ is built.

### BrowserGym - A Gym Environment for Web Agents

WorkArena++ is integrated into BrowserGym (Drouin et al., 2024) (Fig. 2b), a gym environment that facilitates the design and evaluation of web agents and includes many common benchmarks, such as MiniWob (Shi et al., 2017; Liu et al., 2018) WebArena (Zhou et al., 2023) and WorkArena (Drouin et al., 2024). The salient features of BrowserGym include: i) chat-based agent-human interactions, ii) enriched multimodal observations (HTML, accessibility tree (Zhou et al., 2023), screenshot, set-of-marks (He et al., 2024), element coordinates, etc.), and iii) a standardized and flexible action space. In the rest of the paper, all agents interact with WorkArena++ using BrowserGym.

### WorkArena

The starting point for our work is WorkArena, the first benchmark to measure the performance of web agents at solving work-related tasks in enterprise settings (Drouin et al., 2024). Below, we outline some of its key properties.

Task complexityWhile challenging, the tasks included in WorkArena do not require complex problem-solving skills. They rather measure the ability of agents to perform basic interactions with the ServiceNow platform using the main UI components of its user interface, outlined in Fig. 2a. For example, one of the tasks consists in filling out a form after receiving the explicit list of desired values for each field. While solving these tasks is a first step toward achieving anything useful in the workplace, they remain extremely simplistic and trivial for humans.

CertifiabilityAn interesting property of WorkArena is that the successful completion of all tasks is certifiable. Each task comes with an _oracle_ and a _validator_. The oracle is a human-coded solution that uses browser automation (through Playwright (Microsoft, 2023)) to solve the task. The validator is a function that verifies if the task was solved correctly (e.g., by inspecting the database and the current page) and returns a success reward (0 or 1). Our newly proposed WorkArena++ benchmark builds on the same mechanisms and extends them further to handle more complex tasks.

Architecture and availabilityWorkArena requires agents to interact with remote-hosted clones of the ServiceNow platform called _Personal Developer Instances_. These can be requested for free

Figure 3: In WorkArena(++), the agent interacts with the frontend of a remote-hosted ServiceNow instance via BrowserGym. Task validation then inspects both the state of the database and any open page using backend (REST) and frontend (JS) ServiceNow APIs.

Figure 2: Background: (a) In WorkArena, tasks measure the ability of web agents to interact with basic UI components in the ServiceNow platform, illustrated above. (b) In BrowserGym, the agent receives a natural-language goal from a human user via chat. It then perceives the environment (web browser) through a set of multimodal observations (e.g., HTML and screenshot) and controls it via a standardized set of available actions. Reproduced from Drouin et al. (2024) with permission.

through ServiceNow's Developer Program (see Fig. 3 for an illustration). Of note, WorkArena consists of open-source code that interacts with ServiceNow instance APIs and does not rely on any proprietary code. WorkArena++ follows the same design pattern.

In what remains, we will refer to the tasks from WorkArena as the **L1** tasks (for level 1), while WorkArena++ introduces two new levels **L2** and **L3** with increased difficulty, outlined below.

## 3 WorkArena++: Taking WorkArena to the Next Levels

In WorkArena++, each task consists of a logical combination of simpler atomic tasks, chained together to form a realistic workflow. For example, consider the task of "onboarding a new employee" from the perspective of an IT agent. The process would require the agent to: i) navigate to the appropriate page to create a new user, ii) create a new user account by filling out a form, iii) access a service catalog, iv) order a new laptop, and v) complete a form to assign the laptop to the user in the system. Each of these steps corresponds to a WorkArena L1 task4. Next, we provide details on how the tasks in WorkArena++ are categorized across two new levels of difficulty: L2 and L3 (SS 3.1), and five skill categories (SS 3.2).

### Difficulty Levels L2 and L3

WorkArena++ introduces two new levels of difficulty, L2 and L3, which each cover the same set of 341 workflows presented to the agent in different ways, either as explicit or implicit goals using ServiceNow's ticket mechanism. This makes for \(2 341\!=\!682\) tasks in total. While the workflows to execute in L2 and L3 tasks remain the same, the difficulty is increased in L3 due to the less explicit and more realistic way instructions are provided. Examples of L2 and L3 tasks are showcased in SS C.

L2 - ExplicitThe goal is provided to the agent as detailed instructions sent as a user message in the chat. This message contains the precise steps required to complete the task (e.g., start by navigating to the "users" list, then create a new entry with the following values [...] for an 'onboard user' task.). In addition to following these steps, succeeding at L2 tasks requires thinking, reasoning and contextual understanding.

L3 - Via ticket + knowledge baseThe goal is provided to the agent in a manner that mimics how human agents receive work assignments as knowledge workers - through a ticket assigned to them (Fig. 1). The ticket includes key details necessary to complete the task (e.g., the name of the new employee for an 'onboard user' task) but does not specify the steps required to solve it. Instead, the agent solving the task is informed that additional instructions can be found in the company's knowledge

Figure 4: Overview of WorkArena++: a) Distribution of tasks across the skills introduced in § 3.2 for all 682 tasks in the L2/L3 sets. b) Task length as estimated by the number of actions required for completion by the Oracle (see § 2.2) for all 470 L2/L3 task instances in the _agent curriculum_ (§ 4.1). Tasks from the L1 set are also included for comparison (33 tasks x 5 seeds).

base if needed. This level is more challenging, as the agent must memorize the task details from a web page and retrieve instructions from a knowledge base before organizing its steps to succeed, requiring consolidating information across multiple sources. We present a comparison of WorkArena++ across different dimensions with various benchmarks in Tab. 1.

To concretely distinguish between an L2 and L3 task, we consider the easy expense management task for example. The goal in L2 is displayed below. While the goal highlights all the necessary steps to complete the task, it does not describe how to accomplish them (e.g. how to use the menu to navigate):

|}  
**Benchmark** & MiniWoB & WebArena & WorkArena (L1) & WorkArena++ (Ours) \\  \# tasks & 125 & 190 & 33 & 682 \\ Env & Custom web interfaces & Diverse Websites & ServiceNow enterprise platform & ServiceNow enterprise platform \\
**Nature of tasks** & Toy tasks like clicking buttons, filling fields, etc & Real-world inspired tasks from website interactions such as e-commerce and social forums & Basic enterprise software such as sorting a list, filling a form & Complex real-world enterprise software tasks performed everyday by knowledge workers \\
**Abilities required** & Basic task and observation space understanding space understanding, information extraction and retrieval & Interface and action adaptability for enterprise software & Planning, logical and arithmetic reasoning, long context understanding and memorization, information extraction and retrieval \\
**Backend** & Selenium & Requires setting up docker for different categories of tasks & Out-of-the-box with browsergym \\   

|}  
**Training Your Batisfing Expenses (L2)** \\  \# tasks & 125 & 190 \\
**Env** & Custom web interfaces & Diverse Websites \\
**Nature of tasks** & Toy tasks like clicking buttons, filling fields, etc & Real-world inspired tasks from website interactions such as e-commerce and social forums & Basic enterprise software tasks performed everyday by knowledge workers \\
**Abilities required** & Basic task and observation space understanding space understanding, information extraction and retrieval & Interface and action adaptability for enterprise software & Planning, logical and arithmetic reasoning, long context understanding and memorization, information extraction and retrieval \\
**Backend** & Selenium & Requires setting up docker for different categories of tasks & Out-of-the-box with browsergym \\   

For the same task, the L3 goal provided to the agent is simply:

|}  
**Training Your Batisfing Expenses (L3)** \\   

Please complete the following task.

, accompanied by a ticket that says to refer to a knowledge base article containing the rules for expense management (refer to Fig. 17b). Therefore L3 requires the agent to understand it needs to solve the task from the starting web page, navigate to the KB article, remember the expense management rules and apply them, while in L2 the agent directly has the rule to apply.

### Skills and Abilities Evaluated in WorkArena++

Further, all 682 tasks in WorkArena++ fall into one of 5 categories of skills, based on the abilities they require for being solved. The distribution of tasks across categories is displayed in Fig. 4a, and further detailed in SS C. We provide a description of each skill category below, along with their number of tasks.

**Planning and problem solving (66\(\) 2 tasks)** We evaluate these abilities through tasks that require decision-making under constraints to achieve an expected outcome. One notable example consists of scheduling a series of work items within a given time frame while satisfying various constraints based on criticality, duration, and overlap with other items. Other examples include tasks commonly

|}  
**Benchmark** & MiniWoB & WebArena & WorkArena (L1) \\  \# tasks & 125 & 190 \\
**Env** & Custom web interfaces & Diverse Websites \\
**Nature of tasks** & Toy tasks like clicking buttons, filling fields, etc & Real-world inspired tasks from website interactions such as e-commerce and social forums & Basic enterprise software tasks performed everyday by knowledge workers \\
**Abilities required** & Basic task and observation space understanding space understanding, information extraction and retrieval & Interface and action adaptability for enterprise software & Planning, logical and arithmetic reasoning, long context understanding and memorization, information extraction and retrieval \\
**Backend** & Selenium & Requires setting up docker for different categories of tasks & Out-of-the-box with browsergym \\   

Table 1: Comparing existing web-agent evaluation benchmarks with WorkArena++.

performed by managers, such as redistributing work among employees based on occupancy, and dispatching work to employees based on expertise.

Information retrieval (\(\) tasks)We formulate a series of tasks that require retrieving information from either dashboards or lists (see Fig. 1(a)) before performing follow-up tasks based on the retrieved values. For example, one task consists of reading a dashboard to find which item is the lowest in stock and restocking by ordering additional items.

Data-driven decision making and reasoning (\(\) tasks)This skill, essential to several knowledge work roles, is evaluated through tasks that require interpreting data, performing logical or mathematical reasoning, and taking subsequent actions. One notable example is a task where the agent must select investments to maximize expected return within a limited budget, effectively solving a small instance of a knapsack5 problem.

Sophisticated memorization (\(\) tasks)An essential skill for web agents is the ability to gather information by navigating through a series of pages, memorizing key details along the way, and finally using it to achieve a specific goal. For instance, in the "onboard new employee" task described earlier, the agent receives all the information about the employee, including hardware requirements, and must navigate through multiple pages, filling relevant information at each step to complete the task.

Contextual understanding through infeasible tasks (\(\) tasks)Finally, we introduce a set of infeasible tasks to verify if the agent can identify them. We consider two kinds: one where the agent is required to simply declare the task infeasible and another where it must additionally justify its decision. For example, if a task were infeasible due to requesting to fill an inexistent form field, the agent would be evaluated based on its ability to name that field in the justification.

### Salient Features of WorkArena++

WorkArena++ does not only augment WorkArena with new tasks, but also includes a number of technical improvements over it which we outline below. For more detail on this, refer to SS E.

Increased visual diversity and realismWorkArena lacked visual diversity, as all the pages presented to the agent had a similar style. To better assess agents' ability to generalize across different enterprise settings, we introduce 10 fictitious brands, each with distinct styles of the ServiceNow interface. For instance, the "Charlie's Cookies" brand is shown in Fig. 1. In WorkArena++, a company brand is randomly sampled at the start of each task, enhancing realism and visual diversity. This changes the colors of visual elements as well as the company logo. More visual diversity could be addressed in future works.

Standardization and task isolationFor a benchmark to be robust, it must ensure a standardized level of difficulty, regardless of variables like parallel inference or the hardware used for experiments. Achieving this on a remote-hosted instance without access to proprietary code presents a challenge. In WorkArena++, we enhance robustness through several measures. First, we provide an installer that configures the instance with standardized system parameters, UI themes, knowledge bases, and layouts for components like lists and forms. Second, we implement sandboxing by running each task in a new user account, created at its start and automatically deleted at its end. This allows agents to interact freely with the system (e.g., changing visible columns in a list) without affecting subsequent tasks. Together, these improvements result in a more robust benchmark.

Extendability and extraction of fine-tuning dataTasks in WorkArena++ are created by carefully composing the _oracle_ and _validator_ functions (see SS 2.2) of simpler tasks, such as those in the L1 set. Notably, our framework allows the combination of simple oracle functions to generate human-coded ground truths for more complex compositional tasks. Additionally, this framework facilitates the collection of observation-action traces, regardless of the task's length and complexity, providing valuable fine-tuning data for LLMs and VLMs in web-agent interactions.

## 4 Experiments

We now present a series of experimental results on WorkArena++. As will be shown, our proposed benchmark poses a significant challenge for state-of-the-art web agents while being relatively simple for humans to solve. This contrast underscores the benchmark's potential to drive advancements in the field, providing the community with a valuable tool for evaluating and improving web agents.

### Evaluation Curriculum: Standardizing WorkArena++ as a Benchmark

Each WorkArena++ task can be instantiated with variability from thousands of valid configurations per task. Hence, the benchmark can be viewed as a rich distribution over task instances. In order to make the cost of evaluation accessible, improve reproducibility, and have a uniform test of various skills required for solving the tasks, we propose a standardized way to sample a collection of task instances, which we refer to as a curriculum. Concretely, we provide a mechanism that takes a numerical seed as input and can produce an arbitrary number of task instances, sampled uniformly across skills in a reproducible way. We reserve seeds 0-9 for evaluation and the remaining seeds may be used for agent tuning6. Additional details in SS D.

### Agent Design

We mostly follow the same agent design as Drouin et al. (2024), which consists in using an LLM augmented with chain-of-though prompting (Wei et al., 2022b) to produce the next best action for solving the task, based on the current observation of the web browser.

Observation spaceThe main elements presented to our web agents are the current goal, the current page's accessibility tree (AXTree) (Zhou et al., 2023) which can be seen as a compressed representation of the HTML, and the error message (if any) that resulted from the execution of the previous action. Additionally, our VLM-based agent is given a screenshot of the current page augmented with set-of-marks (He et al., 2024). As most tasks in WorkArena++ require long context understanding, we also add to the prompt the history of their previous actions and thoughts (from chain of thoughts) since the start of the episode. This simple mechanism provides a crude memorization mechanism to otherwise memory-less agents, giving them more chances of solving L2 and L3 tasks.

Action spaceAll our agents use the high-level action space from BrowserGym, restricted to the chat, infeas and bid action sets, which allow sending messages to the chat, declaring a task infeasible, and interacting with the current webpage via primitives using element identifiers (bid attribute), e.g., clicking on an element, filling a text box, etc. Our agents are restricted to producing only one action at a time (as in (Drouin et al., 2024)), and implement a retry mechanism that can re-prompt the agent up to 4 times in case of parsing errors in their output.

Language modelsWe evaluate closed-source models GPT-3.5 and GPT-4o (OpenAI, 2023) and study the impact of providing screenshots of the pages with GPT-4o vision. On the open-source side, we evaluate Llama3-70b (Meta, 2024) and Mistral-8x22b (Jiang et al., 2024), deployed using Hugging Face's Text Generation Inference (TGI) library on 4 A100 GPUs. We use a maximum context length of 40K tokens for GPT-4o, 15K for GPT-3.5, 8K for Llama3 and 32K for Mistral. To ensure that prompts do not exceed those limits, we progressively truncate the accessibility tree from the end until it fits in the context. For more information on agent design and the setup, please refer to SS B.

Maximum number of stepsFor budget reasons, we run our agents for a maximum of 50 time-steps before the tasks are terminated. According to our oracle analysis in Fig. 3(b), this gives our agents the chance to solve most of the tasks in our _evaluation curriculum_.

### Agent Results

We evaluate all baseline agents on the standardized curriculum introduced in SS4.1 and report the results in Tab. 2. A notable takeaway is that all agents, whether closed-source or open-source, and regardless of being LLM or VLM-based, generally fail to achieve any reasonable success on WorkArena++, despite performing reasonably well on existing benchmarks. Only GPT-4o and GPT-4o-v succeed at some tasks, particularly memorization tasks within the L2 set, with no successes observed in the L3 set. Interestingly, we observe that, in contrast to its unimodal counterpart GPT-4o, the GPT-4o-v agent succeeds at solving a few retrieval tasks involving reading values off charts, suggesting that the vision modality can be beneficial in WorkArena++. Additionally, as expected, the GPT-4o-based agent significantly outperforms its GPT-3.5 counterpart.

These results raise important questions: i) Are these tasks actually solvable? and ii) Why do the agents fail? In what follows, we address each of these questions through human evaluation (SS4.4) and error analysis (SS4.5), respectively.

### Human Evaluation

To assess the feasibility of the benchmark and measure the gap between humans and agents, we conducted a study with 15 human subjects tasked with solving WorkArena++ tasks. Given the limited number and availability of subjects, we devised a shorter curriculum comprising 98 task instances, sampled uniformly across skills and the L2/L3 sets. Each participant solved a subset of the tasks using a custom-made evaluation tool that exactly matched the interface available to agents. We report these results in Tab. 2 under the Human Curriculum column, along with the corresponding performance of our GPT-4o agent on the same subset of tasks. The numbers are striking, with an overall 93.9% success rate for humans and 2.1% for GPT-4o. These establish WorkArena++ as a valuable benchmark that is both solvable and relatively straightforward for humans, while being challenging for state-of-the-art LLMs, emphasizing its value as a new milestone for the community.

We note that all subjects consented to participate in the study without compensation. Most had little to no familiarity with ServiceNow products and underwent only a brief training session, consisting of a 15-minute video outlining the components in Fig. 1(a), followed by 15 minutes of self-guided exploration on the platform. Details on the task curriculum, training received, demographics, and the evaluation platform are included in SS A.

### Error Analysis

To understand the poor performance of agents on WorkArena++, we conducted an in-depth study of their execution traces. We focused on the best-performing open- and closed-source models, Llama3 and GPT-4o. This analysis identified salient types of errors, which sheds light on current model limitations and highlight areas for improvement.

Information RetrievalThe models tend to successfully navigate to the correct information sources (e.g., dashboards). However, they occasionally fail to accurately retrieve the relevant information from the observations. Furthermore, agents sometimes fail to identify relevant elements on the page and attempt to act on incorrect ones.

    &  &  \\  &  &  \\ 
**Task Category** (task instances count) & **GPT-3.5** & **GPT-4o** & **GPT-4o-v** & **Llama3** & **Mikrat** & **Human** & **GPT-4o** \\ 
**WorkArena L3** (235) & **0.0**\(\)0.0 & **0.0**\(\)0.0 & **0.0**\(\)0.0 & **0.0**\(\)0.0 & **0.0**\(\)0.0 & **93.9**\(\)3.4 & **0.0**\(\)0.0 \\ Contextual Understanding (32) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 87.5 \(\)11.7 & 0.0 \(\)0.0 \\ Data-driven Decision-Making (55) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 100.0 \(\)0.0 & 0.0 \(\)0.0 \\ Planning and Problem Solving (44) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 87.5 \(\)11.7 & 0.0 \(\)0.0 \\ Information Retrieval (56) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 100.0 \(\)0.0 & 0.0 \(\)0.0 \\ Sophisticated Memorization (48) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 91.7 \(\)0.0 & 0.0 \(\)0.0 \\ 
**WorkArena L2** (235) & **0.0**\(\)0.0 & **3.0**\(\)1.1 & **3.8**\(\)1.1 & **0.0**\(\)0.0 & **0.0**\(\)0.0 & **93.9**\(\)3.4 & **2.1**\(\)2.0 \\ Contextual Understanding (32) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 100.0 \(\)0.0 & 0.0 \(\)0.0 \\ Data-driven Decision-Making (55) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 84.6 \(\)11.0 & 0.0 \(\)0.0 \\ Planning and Problem Solving (44) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 100.0 \(\)0.0 & 0.0 \(\)0.0 \\ Information Retrieval (56) & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 3.6 \(\)2.5 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 100.0 \(\)0.0 & 0.0 \(\)0.0 \\ Sophisticated Memorization (48) & 0.0 \(\)0.0 & 14.6 \(\)1.1 & 14.6 \(\)1.0 & 0.0 \(\)0.0 & 0.0 \(\)0.0 & 91.7 \(\)1.0 & 8.3 \(\)1.0 \\ 
**WorkArena L1** (33 \(\) 10 seeds) & **6.1**\(\)1.3 & **42.7**\(\)1.7 & **41.8**\(\)2.7 & **17.9**\(\)1.1 & **12.4**\(\)1.8 & – – – – – \\
**MiniWoB** (125 \(\) 5 seeds) & **43.4**\(\)1.6 & **71.3**\(\)1.5 & **72.5**\(\)1.5 & **68.2**\(\)2.6 & **62.4**\(\)1.6 & **93.5** & – – \\
**WebArena** (812) & **6.7**\(\)0.9 & **23.5**\(\)1.5 & **24.0**\(\)1.5 & **11.0**\(\)1.1 & **12.6**\(\)0.5 & **78.2** & – \\   

Table 2: Success rate\(\): Sundated error (SR \(\):SS6) of all agents on MiniWoB, WorkArena, and WebArena, with numbers reported in %. Bolded numbers represent the average success rate over the entire corresponding benchmark. Results on WorkArena L1, WebArena and MiniWoB are extracted from Drouin et al. (2024). Human evaluation numbers for MiniWoB and WebArena are taken from Humphreys et al. (2022) and Zhou et al. (2023) respectively. The number of task instances is for the agent curriculum. For more detail on human curriculum, refer to § A.

ExplorationSome tasks require to explore the page for hidden information, such as opening different tabs in a form or expanding a section of foldable elements. The agents often struggle due to a lack of curiosity, leaving them stuck in their location.

Hallucination_Made-Up Actions:_ The models sometimes hallucinate actions that would be convenient for the task at hand, but that are not available in BrowserGym. _Imaginary Buttons:_ Similarly, we observed cases of interaction with made-up buttons that would solve tasks in one click, such as buttons that create the exact filter required. _Asking for Help:_ When confused about the next steps, models sometimes ask for help via chat, indicating a lack of confidence or capacity in planning the next steps.

Goal Understanding_Thought/Action Consistency:_ There are instances where the models' thought process correctly identifies the next action, but the action produced is different and incorrect. This inconsistency undermines performance. _Low-Level Understanding in L3 Tasks:_ In more complex L3 tasks, the models fail to comprehend the necessary subtasks fully. For example, they might start by attempting to modify ticket values that are locked, showing a misunderstanding of the task requirements.

Action Consequences Assessment_Hallucinated Consequences:_ The models often hallucinate the consequences of their actions, believing they have made progress on the task when no actual progress has occurred. _Repeated Actions:_ When an action does not change the state of the webpage, models tend to retry the same action repeatedly instead of trying a different approach.

These errors illustrate the current limitations of state-of-the-art web agents in handling complex enterprise tasks. Addressing these issues is crucial for developing more reliable and effective autonomous agents capable of performing real-world knowledge work. Detailed examples are included in SS F.

## 5 Related Work

Early benchmarks for web agents primarily utilized synthetic web environments where agents executed low-level keyboard and mouse actions (Shi et al., 2017; 20; Liu et al., 2018). More recently, Zhou et al. (2023) introduced WebArena, comprising 190 tasks based on realistic websites that emulate real-world domains such as e-commerce, social forums, and content management. OSworld (Xie et al., 2024) introduces a scalable, real computer environment for benchmarking multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across operating systems.

In terms of datasets, Deng et al. (2023) proposed Mind2Web, a large-scale collection of 2,000 web interactions from 137 websites curated by human annotators. Similarly, Lu et al. (2024) introduced WebLINX, a curated dataset of web interactions with 2337 expert demonstrations from 155 different real-world websites. He et al. (2024) proposed 300 information-retrieval tasks from 15 real-world consumer websites, evaluating WebVoyager, a vision-based web agent's capabilities. WorkArena (Drouin et al., 2024) focuses on real-world enterprise software applications by including 33 interactions tasks representative of realistic workflows typically performed by knowledge workers.

Building on this foundation, WorkArena++ introduces tasks requiring advanced skills like problem-solving and data-driven decision-making. Unlike previous benchmarks, WorkArena++ evaluates agents on their ability to perform complex, multi-step tasks that closely mimic real-world enterprise scenarios. Additionally, WorkArena++ emphasizes task isolation, uniform setup, and robust evaluation guidelines, enabling fair comparisons and reproducibility. This approach makes WorkArena++ unique in evaluating the capabilities of LLM and VLM-based web agents.

## 6 Conclusion and Future Work

We propose WorkArena++, a novel benchmark consisting of 682 tasks to evaluate web agents by mimicking realistic workflows performed routinely by knowledge workers using enterprise software. WorkArena++ tests various complex skills of LLM and VLM-based agents including planning, decision-making, retrieval, logical and arithmetic reasoning, as well as the ability to identify infeasible tasks. Our benchmark promotes standardized evaluation, realistic visual diversity, and provides a method for generating large amounts of fine-tuning data in the form of web-interaction traces. Empirical evaluations reveal that state-of-the-art LLMs and VLMs struggle with our benchmark, while humans achieve extremely high performance. Through our error analyses and qualitative studies, we hope WorkArena++ will be a significant step towards developing more capable autonomous web agents.

In future work, we aim to continue developing new sets of tasks on the ServiceNow platform. Of particular importance would be tasks for evaluating safety and cybersecurity around agents as well as a hidden test set for hosting competitions. Furthermore, our framework offers the ability to generate a vast amount of fine-tuning data through web interaction traces to train more robust LLM and VLM-based web agents. Our ultimate goal is to close the significant gap between autonomous agents and humans on WorkArena++ and other benchmarks.

## 7 Limitations and Potential Societal Impacts

LimitationsWhile WorkArena++ includes diverse and realistic workflows, it does not exhaustively cover all possible knowledge-work tasks and personas. Achieving comprehensive coverage would require thousands of additional tasks. However, our task set is designed to be easily extendable by the community, and we welcome such contributions. Additionally, while this work evaluates the reasoning abilities of LLM-based web agents, it does not assess their safety and robustness against various malicious behaviors, which remains an important barrier to their adoption in real-world settings. Moreover, the benchmark does not include tasks that require interaction with software external to the ServiceNow platform, which would improve diversity and realism. We leave such assessments to OS-level benchmarks like that of Xie et al. (2024). Finally, we note that additional open and closed-source LLMs and VLMs could have been included in the experiments, particularly those with extremely long context lengths, such as Gemini 1.5 pro with 1 million tokens (Reid et al., 2024).

Societal ImpactsThis work is likely to inspire the development of agents as valuable workplace assistants, positively impacting society in several ways. It can increase productivity, enabling agents to handle more complex and value-creating tasks. Additionally, it can improve accessibility for impaired users, potentially opening new job opportunities. However, there are potential negative impacts. Advanced agents may lead to job displacement as such systems take over human tasks. Reliance on these agents raises data privacy and security concerns and could erode human skills and decision-making abilities over time. Moreover, the significant computational resources required to support these agents lead to substantial energy use, contributing to negative environmental impacts.

The authors are grateful to the individuals who participated in our evaluation study: Arjun Ashok, Aayush Bajaj, Ghazwa Darwiche, Jerry Huang, Raymond Li, Marie-Eve Marchand, Etienne Marcotte, Shravan Nayak, Sebastien Paquet, Soham Parikh, Fanny Rancourt, Gopesh Subbaraj, Jordan Prince Tremblay, and Andrew Williams. Your contributions were invaluable in showcasing that, for now, humans are still the reigning champions of intelligence. We also extend our thanks to Chris Pal, Issam Laradji, and David Vazquez for their insightful feedback and suggestions, which were almost as brilliant as our participants' defense of human ingenuity.