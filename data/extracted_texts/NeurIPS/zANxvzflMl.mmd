# Towards Foundation Models

for Scientific Machine Learning:

Characterizing Scaling and Transfer Behavior

 Shashank Subramanian

Lawrence Berkeley National Lab

shashanksubramanian@lbl.gov

&Peter Harrington

Lawrence Berkeley National Lab

pharrington@lbl.gov

&Kurt Keutzer

UC Berkeley

keutzer@eecs.berkeley.edu

&Wahid Bhimji

Lawrence Berkeley National Lab

wbhimji@lbl.gov

&Dmitriy Morozov

Lawrence Berkeley National Lab

dmorozov@lbl.gov

&Michael W. Mahoney

ICSI, LBNL, and Department of Statistics

mmahoney@stat.berkeley.edu

&Amir Gholami

ICSI, UC Berkeley

amirgh@berkeley.edu

###### Abstract

Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that--when fine-tuned appropriately--transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behaviour across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the "pre-train and fine-tune" paradigm for SciML problems, demonstrating a path towards building SciML foundation models. Our code is available as open-source at .

## 1 Introduction

Foundation models have received considerable interest recently . This terminology refers to certain models that are trained on extremely large and diverse quantities of data and applied to a wide range of tasks. Rather than being designed for any single task, a foundation model serves as a "prior"or "foundation" upon which other models can be built. It does so by using transfer learning (TL) methods to fine-tune or adapt the foundation model to a wide range of downstream tasks, using minimal additional data for each additional task. Perhaps the most well-known foundation models are pre-trained large-language models (LLMs) such as BERT  and the GPT models . The scaling with respect to the amount of data, the size of the model, and the amount of compute  is key to the training of these models. An important aspect of a trained foundation model is the ability to perform tasks seemingly different than those for which it was trained by leveraging shared features across the training tasks. This approach to model development is quite different than the traditional approach of training a one-off model from scratch for each specific problem and each specific dataset. Naturally, it is of interest how broadly this methodological approach can be applied.

Scientific machine learning (SciML)  is an area that combines tools from ML and scientific computing to address domain-specific scientific and engineering challenges. It holds promise to drive the next wave of data-driven discovery in the physical and engineering sciences. Recent work has highlighted the promise  as well as some of the many challenges  of developing SciML models--in general as well as with the traditional one-off learning approach. Many SciML models emulate physical systems described by Partial Differential Equations (PDEs). For example, Physics-Informed Neural Networks  impose the PDE as a soft penalty in the loss function. However, they are restricted to solving a single instance of the PDE. The Neural Network (NN) needs to be retrained for each new set of PDE physics coefficients, sources, and/or initial/boundary conditions (IC/BCs). Subsequent models have been developed to learn the full solution operator  by training across different coefficients (and/or initial and boundary conditions). These _neural operators_ learn mappings between two function spaces from a finite collection of input-output pairs (that represent the coefficients/initial or boundary conditions as the input and the PDE solution function as the output). This makes them more general and versatile in emulating any PDE system. However, with new coefficients/sources or new differential operators, they too need to be retrained from scratch.

In this paper, we adopt and evaluate the methodology that has been applied successfully in CV and NLP to develop foundation models, with the goal of determining whether such a model is even possible for SciML problems. In particular, we provide an extensive analysis of the scaling and TL behavior of neural operators trained on diverse training datasets from multiple PDE systems. An important aspect of this approach is to explore several dimensions that include the model (architecture and scale), data (diversity and scale), training recipes (pre-training and fine-tuning), and out-of

Figure 1: Our setup consists of creating diverse training datasets, sampling both PDE coefficients and source functions simultaneously with different PDE operators and input data (coefficients, sources) distributions for pre-training. A neural operator is then pre-trained to predict the PDE solutions given these inputs and the ground truth solutions (computed through PDE solvers). The pre-trained model is then adapted with minimal fine-tuning (zero-shot or few-shot), and it is used in various downstream tasks (PDE systems) that can be in-domain or out-of-domain from the pre-training datasets. The pre-training with multiple solution operators allows the same model to transfer to several very different systems. For instance, PDE 2 (Helnholtz) manifests highly oscillatory solutions compared to PDE 1 (Advection-Diffusion) or PDE 3 (Poissonâ€™s). We further characterize the scaling and transfer properties of this model as a function of downstream data scale and model size scale.

distribution (OOD) transfer behaviour. For transfer behavior, a major difference from CV/NLP is the diversity of scales, features, and solution behaviors across PDEs. For example, while OOD shifts in CV/NLP typically involve things like different color distributions or semantic contexts, in SciML, it is possible to get significantly different behavior of a PDE solution as physics parameters and conditions change that manifest as novel features (for example, increased emphasis on very different frequency scales of the solution causing significant shifts in detail, sharpness, and other image attributes). For LLMs, given the maturity of the NLP community within ML, these dimensions are well-explored. For SciML problems, in contrast, all these dimensions are open questions. Here, we explore several of these questions. We do so in the context of a specific model architecture, namely, the _Fourier Neural Operator (FNO)_, as a prototypical SciML model that has demonstrated promising results modeling PDEs across a wide range of scientific applications . We focus on the scaling and TL behavior of the FNO on common PDE systems that include Poisson's, Advection-Diffusion, and Helmholtz PDE systems. These systems underpin a wide range of physical systems: fluid flow systems; biological simulations; wave propagation systems; and many others.

See Fig. 1 for a schematic summary of our methodological approach. Our main results demonstrate the potential of the "pre-train and fine-tune" paradigm for SciML problems, demonstrating a path towards building SciML foundation models. In more detail, our main contributions are the following.

1. **Pre-training dataset generation.** We develop a large suite of datasets, and we train our models on data where all the variables (inputs) of any PDE operator are sampled. This is an important step towards developing NNs that can generalize across a variety of downstream tasks, and it extends several previous works, including the original FNO , where certain inputs are kept fixed. Not sampling can trivially push the neural operator OOD if, e.g., the source function was changed. We study transfer learning to both in-domain and out-of-domain distributions, characterized by different samplings of PDE coefficients (diffusion, advection, wavenumbers, etc.) and inhomogeneous source functions. We emphasize the construction of pre-trained datasets with sufficient diversity as well as normalization strategies, without which we observe significant degradation of performance.
2. **Downstream (target) data scaling.** We study the effect of the scale of downstream (target) data in TL performance from the pre-trained model. Here, we assume that a large amount of data is available for pre-training, and data are limited for the downstream tasks (as in many scientific examples); and we are interested in reaching desired accuracy levels with the least amount of additional downstream data. We consider both zero-shot and few-shot TL: zero-shot is the direct evaluation of the pre-trained model on the downstream dataset; and few-shot involves using O(10) downstream data to fine-tune the pre-trained model. While few-shot typically refers to prompting with in-context examples during inference in NLP, here we define it as fine-tuning the model with a few examples. We observe that TL from the pre-trained model can lead to significant performance gains over training the model from scratch on the downstream (target) data, with orders of magnitude less data needed to reach a desired accuracy level (see Fig. 2). We observe this gain over a wide range of data scales, until we enter the "large target data" regime (as much data as pre-training), where we observe similar accuracies for TL as training from scratch.
3. **Model (parameter) size scaling.** We study the parameter scaling of the model by scaling our model sizes from \(64\)K to \(256\)M parameters (a multiplicative factor of \(4\)K). We observe an error saturation at small model sizes (due to insufficient model expressivity) that monotonically drops as we increase the model size. While both fine-tuned models and models trained from scratch exhibit gains with increased model size, we observe that fine-tuning achieves greater performance gains with parameter scaling (see Fig. 3).
4. **Transfer learning behavior over underlying physics.** We study the effect of varying the underlying physics in the target domain. In SciML (unlike traditional non-scientific ML), there are typically fundamental constraints such as conservation laws that govern the behaviour of the solution. In some cases, we may even have access to parameters of the underlying physical constraints, and thus a physical understanding of the distribution of the data. It is natural, and necessary, to systematically quantify the effect of these parameters as our downstream tasks go OOD, as this provides a good way to test the (OOD) generalization capability of pre-trained models for SciML applications. We find that for in-distribution TL, the pre-trained model can significantly outperform a model trained from scratch, irrespective of how many new data examples were used for fine-tuning, until the large target data regime (e.g., see Fig. 4a), showing orders of magnitude better accuracies than training from scratch. We also observe these gains for downstream tasks that are moderately OOD, with few-shot fine-tuning providing again orders of magnitude better accuracies (see Fig. 3(b), Fig. 3(c)). As we systematically go further OOD (see the quantification in Tab. 1), we observe the performance gains expectedly reduce, with more significant drop in the low data regimes (e.g., see Fig. 3(d)).
5. **Transfer learning behavior over multiple operators.** We study the effect of simultaneous pre-training on multiple PDE systems that exhibit qualitatively different solution behaviors (e.g., Poisson's and Helmholtz operator solutions show very dissimilar patterns, given a source function). We include the coefficient/source functions for all the operators as inputs to the model, with zero values if those terms do not exist in a given PDE instance. During inference, the zero inputs restrict the neural network to make predictions for the correct operator (see SS4 for details). Among other things, we show that the same model pre-trained on different operators retains its gains across different downstream tasks (see Fig. 5), paving the way for it to be used in the foundational sense.

## 2 Related work

In recent years, there has been widespread interest in modeling PDE systems with neural operators or operator learning in a broad variety of science and engineering applications [32; 30; 27; 41; 33; 37; 25; 2; 49; 4; 29; 26]. Following the success of TL in CV and NLP tasks [54; 43; 48; 13; 36; 40; 22], there have been several investigations into how TL can be leveraged for SciML problems involving differential equations. Most have focused on applications of Physics-Informed Neural Networks (PINNs) [35; 19; 15; 8; 18; 17; 50; 14; 10; 31], where models can be fine-tuned or adapted using a physics-based loss function determined by the specific target PDE/ODE system. Another common theme is evaluating how well TL can account for the diversity of geometries [6; 47; 50; 16] and discretizations [34; 7; 44; 46] found in scientific computing. Recently, attention has also been devoted to TL for neural operators in SciML, where some works have explored certain components of our analysis in isolation. This includes studies on the TL performance of DeepONet [51; 16; 55] and FNO [31; 9], where one of either the target domain, PDE coefficients, or PDE source functions shift, are varied. In , the authors evaluate different model and training dataset sizes for different neural operator architectures, but their analysis is mostly limited to within-distribution experiments and single operators, and they do not analyze the fine-tuning performance of such models (particularly on OOD data). The works above have not considered the pre-training regimes over a diverse set of physical systems and most have not characterized their TL as a function of data and model scaling, nor included multiple PDE operators during training. To the best of our knowledge, the work of  (which has appeared concurrent to this work) is the first (along with ours) to consider the TL potential of neural networks across operators and data distributions. There, the authors adapt in-context learning (from LLMs) to solve differential equations with transformer-based models. The main difference with  is that the authors focus on in-context learning which requires prompting (with up to five example demos) to solve OOD tasks including different differential equation coefficients and operators than those seen at train time (similar to our work). In contrast, we focus on TL with zero-shot and few-shot learning through fine-tuning. Thus the two approaches (TL through fine tuning vs in-context learning) are complementary but different. We also note that their investigation was performed at a much smaller scale (from both model and data perspective), and was limited to simpler dynamical systems, as compared to the scales tested in our work. Overall, the prior related work provide important initial results on the interpolation and extrapolation behavior of neural operators in the context of differential equations, and/or limited investigation into the behavior as model and dataset sizes are increased. However, none of them consider these aspects simultaneously with a more diverse pre-training corpus (by varying all input variables such as source functions and PDE coefficients and/or including different operators for pre-training), which is closer to methodology adopted by CV and NLP in the development of their foundation models, with emphasis on the importance of characterizing scaling properties [23; 21].

## 3 Methods

Pre-training a foundation model requires that we first collect a large amount of diverse training data and then train a base model that could subsequently be used with TL for a downstream application. There are several possible methods for TL of the foundation model. One approach is in-context learning, where the foundation model is prompted with few-shot input-output examples for the downstream problem followed by the target input. The model then sees these examples and learns how to compute the target output. This is the approach used by modern NLP models such as GPT  as well as the work of . While this approach is very useful for cases with very few training datapoints available, it is often better to fine-tune a foundation model for a downstream task, when one has access to more downstream training data. This is also supported by NLP models and it often results in better performance--if enough training data is available. We focus on the latter setup, and we study how the TL performance behaves for different problem setups. Our goal is to understand the different moving parts associated with training a foundation model for SciML applications and, specifically, the impact of model scaling, dataset size, and different physics involved in the problem. Below, we discuss (i) the different physics operators considered and our pre-training setup that includes the training dataset generation, (ii) NN model architecture setup for training and inference, and (iii) performance metrics.

**PDE/physics system setup.** We consider three PDE systems that are common building blocks for many scientific application: 2D Poisson's; Advection-Diffusion; and Helmholtz equations. These PDE systems can be formulated as follows:

1. _Poisson's_ (SYS-1): We consider a prototypical elliptic system with periodic boundary conditions in domain \(=^{2}\): \[-\, u=f\ ,\] (1) where \(u()\) is the solution (state) function, \(f()\) is a source (forcing) function, and \(\) is the diffusion coefficient tensor. We use \(\) to quantify the physics of this system.
2. _Advection-Diffusion_ (SYS-2): We also consider a steady-state advection-diffusion equation that illustrates competing physical processes (advevective and diffusive processes) through two differential operators. We use periodic boundary conditions in domain \(=^{2}\): \[-\, u+ u=f\ ,\] (2) where \(u()\) is the solution (state) function, \(f()\) is a source (forcing) function, \(\) is the diffusion coefficient tensor, and \(\) is the velocity vector. To quantify the competing advective/diffusive scales of this system, we define the ratio of advection to diffusion as \(=\| u\|/\|\, u\|\).
3. _Helmholtz_ (SYS-3): Finally, we also consider the inhomogeneous Helmholtz equation with periodic boundary conditions in domain \(=^{2}\). We take this as an example challenging system that can exhibit high-frequency oscillatory spatial patterns that can be difficult to generalize. This system is formulated as: \[- u+ u=f\ ,\] (3) where \(u()\) is the solution (state) function, \(f()\) is a source (forcing) function, \(>0\) is the wavenumber used to quantify the underlying physics of this system.

**Data setup.** For the above PDE systems, we are interested in (i) a large and diverse training dataset for pre-training and (ii) several downstream datasets (tasks) to quantify the TL performance. Given that we can solve these PDEs numerically, we can generate diverse set of training and testing datasets in a controllable fashion by varying the different parameters in these PDEs. In particular, we vary the following parameters for dataset generation:

1. _Source function sampling:_ We sample different source functions \(f(,s)\), where \(\) is a distribution that generates diverse and heterogeneous functions. (see Fig. 1 and Fig. A.1 for examples). Here, \(\) represents a parameterization of the source function as a linear combination of \(n_{g}\) radial (Gaussian) basis functions \(\{_{i}()\}_{i=1}^{n_{g}}\), where \(_{i}()=(-})\) is a Gaussian function centered at grid point \(_{i}\). Specifically: \(f()=_{i=1}^{n_{g}}_{i}()p_{i}\), with \(=\{p_{i}\}_{i=1}^{n_{g}}\) as the parameterization vector. The spatial profile controlled by \(\), the standard deviation of the Gaussian function, is preset to a small value to encourage high variability. Examples are sampled by uniformly randomly sampling \(p_{i}(0,1)\). We further introduce heterogeneity by controlling the sparsity \(s\) of \(\) (\(s\) defined as the number of non-zero components; see Appendix SSA.1 for full details).
2. _PDE coefficient sampling:_ In SYS-1, we sample diffusion coefficient tensors \(()\), where \(\) is a distribution that generates varying scales of anisotropy and spread in the diffusion process: \(=^{-1}\) with \(=(1,e)\) and \(=()\), where \(e\) is an eigenvalue of the tensor that controls the anisotropy and extent of diffusion and \(()\) is a rotation matrix with angle \((0,2)\) that controls the general diffusion direction. In SYS-2, we additionally also sample the velocity vector \(\) direction from \((0,2)\). The ratio of advection to diffusion \(\) is changed by scaling the velocity. In SYS-3, we sample the wavenumber \(\) as uniform integers. We visualize the sampling process and resulting solutions with different ranges of underlying physics in Fig. A.1.

For each of the problems that we consider in the results section, we generate \(2^{15}\) input-output samples (pairs) of data, where the inputs include the source \(f\) as well as any PDE coefficients (\(,,\)), along with \(2^{12}\) validation and testing samples each. The validation dataset is used for hyperparameter optimization, and the testing dataset is used for quantifying model performance. The pre-trained model is trained on the \(2^{15}\) training examples.

We then perform different experiments to evaluate how this model can adapt/TL to different downstream tasks whose data could come from the following distributions: (i) same distribution as in the pre-training dataset (i.e., different input/output pairs but drawn from the same distribution of PDE coefficients/sources); and (ii) the harder task of adapting/TL to a downstream problem that can have slight/large deviation from the dataset used to pre-train the model. For the latter, we create the OOD data by keeping the PDE operator the same as the pre-training task, but sample the coefficients from a different range as in the pre-training. Given this dataset, we then study the TL behaviour for each case (both within distribution and OOD) by scaling both the downstream dataset size, as well the model architecture size which is discussed next.

**Pre-training method for training and inference.** The inputs to our model are 2D spatial functions discretized at \(h w\) and represent the sources and PDE coefficients.1 These input discretized functions are batched together to form an input tensor in \(^{h w c}\). The output of the model is the numerical solution of the PDE in \(^{h w}\). For the model architecture, we consider the FNO (details in Appendix SSA.2). This model bears similarities to both vision transformer-like architectures (fixed image/feature resolution across the depth of the network) and convolutional architectures (successive global convolutions facilitated via FFTs). FNO is also a good choice for our setup as the problems we consider all have periodic boundary conditions (the FNO can also be adapted for non-periodic boundaries). The main modification that we make to the FNO is to incorporate a pre-instance normalization layer in the model. We found that this is a critical component as the norm of the input data spans a wide range of values (up \(100\) for our dataset). Please see Appendix SSA.5 for details. Furthermore, we consider an \(_{2}\) loss function for pre-training. That is, the model is trained to predict the output solution given the input by minimizing a mean-squared error loss between the prediction and the ground truth.

To test how this pre-trained model can adapt to different downstream applications we consider the following cases. First, we consider zero-shot adaptation where the pre-trained model is tested on a downstream application without any fine-tuning. Second, we consider few-shot fine-tuning, where the pre-trained model can be fine-tuned on the downstream training dataset. We consider varying sizes for this downstream dataset size. Ideally, we prefer the pre-trained model to achieve good performance with as few examples as needed from the downstream application. We perform an ablation study by training a model from scratch on the downstream dataset alone to see how much gain can be achieved by using a pre-trained model. We also test how the pre-training adaptation performance changes as we scale the model size. This is motivated by observations in NLP  where we expect larger models to show better adaptation performance. To do the model scaling, we focus on two model hyperparameters: the embedding dimension \(d\) and the number of Fourier modes used in the FNO \(m\). (See Appendix SSA.2 for the details.) We first fix \(d=16\) and scale \(m\{4,16\}\), then fix \(m=32\) and scale \(d\{32,128\}\) to approximately increase the parameter count \(16\) in each scaling experiment from \(64K\) to \(256M\) parameters.

The main limitation of our work is that we focus on only the FNO model, whereas several other architectures (e.g. ViT models, DeepONets) exist. As discussed in SS1, the FNO has been applied to a wide range of applications in SciML and is representative of a performant SciML architecture-restricting our analysis to one model architecture makes the scope of this study more feasible. However, this analysis needs to be done across other models architectures, as well.

## 4 Results

Our main results are the following. We demonstrate that pre-training a model on a diverse corpus of data and then fine-tuning it on downstream tasks leads to significantly better performance than training a model from scratch. This holds even when the downstream data falls outside of the pre-training distribution, including when different physics models are combined. The advantage of pre-training is especially pronounced when the downstream data is limited, which is the most significant setting in practice, motivating the creation of foundation models for scientific machine learning.

To justify these conclusions, we focus on four key questions. What is the effect of **(Q1)** downstream dataset size and **(Q2)** neural operator model parameter size on TL? What is the TL behavior of the neural operator **(Q3)** over the underlying physics and **(Q4)** over multiple solution operators?

**(Q1): Downstream dataset scaling.** For SYS-1, we consider the pre-training system SYS-1(1,5), with diffusion constructed by sampling eigenvalue \(e(1,5)\) of the diffusion tensor \(\). See Fig. A.1 for visualizations. This represents near isotropic to \(5\) anisotropic diffusion. We use \(e(5,10)\) as the downstream dataset SYS-1(5,10). This represents \(5\)\(-10\) anisotropic diffusion and is moderately out-of-distribution (OOD) from the pre-training dataset. While we systematically quantify the OOD effects in **(Q2)**, we use this specific test-case to illustrate the effect of the size of the downstream dataset. We plot the behaviour of testing error as a function of downstream examples in Fig. 1(a). We train an FNO model for each number of downstream examples (x-axis on the plot) starting from "scratch" (random initialization) as well as from the pre-trained model parameters, with tuned hyperparameters for each experiment (see details in Appendix SSA.4).

We illustrate the extent of distributional shift between the pre-training and downstream datasets through the range of diffusion tensor eigenvalue \(e\)--in this test case, a modest shift with no overlap (but relatively close). The testing error monotonically decreases as more downstream data is used for training, as we expect. The zero-shot TL shows excellent performance despite the moderate OOD shift of downstream examples. When training from scratch, we define "zero-shot" predictions as the output of the model with random initialization. With "few-shot" learning (\(O(10)\) downstream examples), we observe a consistent performance increase over training from scratch. Given a desired error for downstream performance, TL from the pre-trained model can require orders of magnitude less data--for example, a desired error of 1e-2 needs only about \(64\) downstream data examples for fine-tuning, whereas training from scratch requires \(8K\) (about \(100\) more) examples to reach the same accuracy level. We further explore this in Appendix SSB.1 and find the pre-trained model generally saves \(O(1K-10K)\) data compared to training from scratch in the few-shot learning setting, and outperforms training from scratch at all scales. Finally, with greater amounts of downstream data, the pre-training provides consistent performance gains until we enter the "large target data" regime, where the number of fine-tuning examples approaches the size of the entire pre-training dataset, and we see diminishing returns from pre-training. We repeat this experiment for SYS-2, using the following test-case: the pre-training system SYS-2(0.2,1) consists of advection-to-diffusion rates \(\)\((0.2,1)\), representing about \(1\)-\(5\) diffusion (relative to advection). For the downstream test, we use a modest TL with SYS-2(1,2) with \(\)\((1,2)\) representing \(1\)-\(2\) advection

Figure 2: **Addressing **(Q1)**. Testing error as a function of downstream examples for SYS-1 and SYS-2. We visualize the distribution of pre-training and downstream dataset physics at the top to illustrate (and quantify) the extent of distributional shifts. We observe excellent zero-shot and few-shot TL performance of the pre-trained model despite the modest OOD shifts and in medium-data regimes about \(100\) increase in data efficiency. We observe diminishing returns from pre-training at the large-data regime (\(O(2^{15})\) examples), which has as many examples as used in pre-training.

(relative to diffusion). We visualize this shift in Fig. 2b (top) and also show the testing error as a function of downstream examples. Both experiments reveal the same trend: TL delivers much higher performance, which improves with fine-tuning up to a point of diminishing returns.

**(Q2): Model (parameter count) scaling.** As described in our model setup, we vary the embedding \(d\) and maximum Fourier modes \(m\) to approximately increase the parameter count \(16\) in each scaling experiment from \(64K\) to \(256M\) parameters. For models trained from scratch, we repeat the data scaling experiments for each parameter count. For the pre-trained model, we first identify the ideal hyperparameters (through grid-search hyperparameter tuning) for each model scale and repeat the above training experiments. We visualize the testing errors as a function of downstream examples used for SYS-1(5,10) (pre-training dataset used: SYS-1(1,5) signifying moderate OOD) for the different model scales in Fig. 3 (left). At the \(64K\) parameter regime, the model capacity is insufficient, with large errors (greater than \(1\)e-\(2\)) for either training recipe across the whole range of downstream example counts. As we move to larger models, both training from scratch and fine-tuning show higher performance that increase with more examples. Fine-tuning the pre-trained model boosts its performance, compared to training from scratch, as we increase the model scale and particularly across a wide range of downstream example counts (with \(256M\) parameter model showing the least errors). We repeat the model scaling process for Advection-Diffusion SYS-2 (pre-training dataset SYS-2(0.2,1) with moderately OOD SYS-2(1,2) for downstream) and observe similar trends in Fig. 3 (right).

**(Q3): TL behavior over underlying physics.** We test both in-domain and out-of-domain physics effects by constructing downstream datasets that systematically deviate from the pre-training dataset. For SYS-1, we sample different ranges for \(e\) with varying overlap with the pre-training dataset. Similarly, for SYS-2, we use different ranges of advection-to-diffusion ratio \(\) showing different overlap. We highlight these systems (downstream and pre-training) in Tab. 1 for the two PDE systems.

 Pre-training & Downstream & Shift \\  (1,5)\)} & SYS-1(1,2.5): \(e(1,2.5)\) & None \\  & SYS-1(2.5,7.5): \(e(2.5,7.5)\) & Mild \\  & SYS-1(5,10): \(e(5,10)\) & Med \\  & SYS-1(10,20): \(e(10,20)\) & Large \\   & SYS-2(0.2,0.4): \(\)\((0.2,0.4)\) & None \\  & SYS-2(0.2,0.4): \(\)\((0.2,0.4)\) & Mild \\   & SYS-2(1,2): \(\)\((1,2)\) & Med \\   & SYS-2(2,5): \(\)\((2,5)\) & Large \\ 

Table 1: Different downstream datasets and extents of overlap with the pre-training dataset for SYS-1 and SYS-2, controlled by extent of anistropy (eigenvalue \(e\)) in diffusion tensor for SYS-1 and amount of advection relative to diffusion (ratio \(\)) for SYS-2.

Figure 3: Addressing (Q2). Model size scaling for SYS-1 and SYS-2 from \(64K\) to \(256M\) parameters for medium OOD test-cases. While finetuning consistently improves the model performance and data efficiency, we observe higher errors for small parameter regimes at \(64K\) due to insufficient model capacity. The performance gains are significantly boosted through finetuning with a larger model set sizes monotonically up to \(256M\) parameters.

We repeat our downstream dataset scaling experiments on the different downstream tasks and show the trends for SYS-1 in Fig. 4. In particular, in Fig. 3(a), we consider the downstream dataset within distribution of the pre-training dataset (as visualized by the \(e\) distribution at the top). We observe excellent zero-shot performance that is unaffected by further fine-tuning. In Fig. 3(b), the downstream dataset is shifted mildly OOD. Although the zero-shot performance drops, it still shows low errors, significantly smaller than training from scratch. Further, the performance is improved with few-shot TL up to the point of diminishing returns with large numbers of downstream data examples. With further distributional shift (no overlap) in Fig. 3(c), the zero-shot performance suffers more, but with a larger amount of fine-tuning recovers good performance. Finally, for large distributional shifts in Fig. 3(d), the zero-shot and few-shot performance is poor, with TL showing relatively high errors, but even here TL improves over training from scratch. In this case, due to larger anisotropy, the system is also harder to emulate and might require more data in general. We repeat this analysis for SYS-2 and observe similar trends across different OOD downstream tasks (see Appendix B.1).

**(Q4): TL behavior over multiple operators.** We further diversify the pre-training by including examples with different solution operators. We combine the datasets from three PDEs-- Poisson's SYS-1 (1,5), Advection-Diffusion SYS-2(0.2,1), and Helmholtz SYS-3(1,10) (where the wavenumber \((1,10)\)). Here, we have additionally included the Helmholtz PDE, a challenging system due to the highly oscillatory behavior of the solutions (see PDE 2 in Fig. 1 and Fig. A.1, for examples), very sensitive to the range of wavenumbers. When pre-training a single model on this "mixed" dataset, we simply use zero channels for those coefficients that do not exist when using examples from a specific operator. For example, the Helmholtz equation has a diffusion tensor input (identity matrix) with an additional input for the wavenumber but no advection (zero channel), while the Poisson's equation only has a diffusion tensor input and hence we append zero channels to signify no wavenumbers and advection; similarly for Advection-Diffusion. This, effectively, serves

Figure 4: **Addressing (Q3).** Testing error as a function of downstream examples for different downstream tasks used in SYS-1. We show the extent of overlap (signifying distributional shifts) between the pre-trained and downstream dataset at the top using the range of sampled diffusion tensor eigenvalue. For datasets within distribution, zero-shot TL is optimal. As the downstream dataset shifts moderately OOD, the zero-shot learning suffers gradually and is recovered through fine-tuning. This recovery is slower as the distributional shifts increase.

as selection of the solution operator during the forward pass to predict the solution to the right operator. While more advance techniques such as in-context prompting (from LLMs) exist, here we are interested in understanding if this simple and minimal selection/prompting is sufficient for the model to transfer effectively to downstream tasks. For the downstream tasks, we consider three within-distribution tasks of Poisson's SYS-1(1,2.5), Advection-Diffusion SYS-2(0.2,0.4), and Helmholtz SYS-3(1,5) and show our dataset scaling results in Fig. 5.

The results support our most compelling conclusion: fine-tuning from the mixed dataset retains the substantial performance gains over training from scratch for all downstream tasks. The same model (pre-trained on three different tasks) is useful in all downstream tasks, in both the zero-shot and the fine-tuning settings. This indicates the input coefficient channels are sufficient to prompt the model to predict the correct downstream solution. We show the OOD downstream performance of this model in Appendix SSB.2 and observe similar behavior.

## 5 Conclusions

We have provided an extensive analysis of the scaling and transfer behavior of neural operator models on multiple PDE systems. This involved characterizing behavior as a function of model size, downstream dataset size, underlying physics of the downstream tasks in relation to pre-training, the adaptation of these models to multiple downstream PDEs, and how all these behaviors scale with relevant problem and model parameters. Among other things, we have shown that it is possible and beneficial to develop more general SciML models capable of solving multiple tasks with the _same_ set of weights, even when downstream tasks involve small-to-moderate distribution shifts relative to the pre-training data. All in all, this demonstrates the potential of the "pre-train and fine-tune" paradigm for SciML problems, paving a path towards building SciML foundation models. Moving forward, many questions remain. These include further exploration of model architectures (balancing expressivity and flexibility), pre-training protocols (including self-supervision components at scale), fine-tuning strategies, and the integration of these within a specific compute (and memory) envelope. There are a number of future directions raised by our work. Our pre-train and fine-tune recipe may need more sophisticated prompting during inference, especially if only the operator form changed between two different PDE systems. Also, we do not look at self-supervision with the PDE loss penalty as a means of large-scale pre-training, and we limit our analysis to 2D spatial systems. Moving to other architectures, larger scales, and more complex PDEs in space-time is a focus of future work.

Figure 5: _Addressing (Q4). Testing error as a function of downstream examples for SYS-1, SYS-2, and SYS-3 with fine-tuning from their respective PDE systems and from the mixed dataset (combination of SYS-1, SYS-2, and SYS-3). The model pre-trained on the mixed dataset performs better than training from scratch. More importantly, the same pre-trained model yields low errors on all the downstream PDEs with both zero-shot and task-specific fine-tuning. We show the results for SYS-2 and OOD performance in SSB.2._