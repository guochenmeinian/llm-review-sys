# Variational Last Layers for Bayesian Optimization

Paul Brunzema

RWTH Aachen University

brunzema@dsme.rwth-aachen.de &Mikkel Jordahn

Technical University of Denmark

jordahnm@gmail.com &John Willes

Vector Institute for AI

john.willes@vectorinstitute.ai &Sebastian Trimpe

RWTH Aachen University

trimpe@dsme.rwth-aachen.de &Jasper Snoek

Google DeepMind

jsnoek@google.com &James Harrison

Google DeepMind

jamesharrison@google.com

###### Abstract

Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured, such as those defined by Euclidean metrics. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult. While Bayesian neural networks are a promising direction for higher capacity surrogate models, they have so far seen limited use due to a combination of cost of use and poor performance. In this paper, we explore the potential of neural networks with variational Bayesian last layers (VBLLs), which offer a simple and computationally lightweight approach to Bayesian uncertainty quantification in neural networks. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complicated input correlations, and match the performance of well-tuned GPs on established benchmark tasks. These results highlight their promise as an alternative surrogate model for BO.

## 1 Introduction

Bayesian optimization (BO) has become an immensely popular method for optimizing black-box functions that are expensive to evaluate, and has seen large success in a variety of applications . In BO, the goal is to optimize some black-box objective \(f\) (where \(^{d}\)) in as few samples as possible whilst only having access to sequentially sampled, potentially noisy, data points from the objective. Gaussian processes (GPs) have long been the de-facto surrogate models in BO due to their well-calibrated uncertainty quantification and strong performance in small-data regimes. However, their application becomes challenging in high-dimensional, non-stationary, and structured data environments such as drug-discovery  and materials science . Here, often prohibitively expensive or bespoke kernels are necessary to capture meaningful correlations between data points. Furthermore, the scaling of GPs to large datasets typically associated with high-dimensional spaces can be limiting-especially if combined with online hyperparameter estimation. To address these challenges, integrating Bayesian Neural Networks (BNNs) into BO as alternative surrogate models has gained increasing attention . While BNNs inherently scale with data, challenges like efficiently conditioning on new data and consistency across tasks persist. Our work demonstrates howVariational Bayesian Last Layer (VBLL) neural networks  can address these issues and achieve state-of-the-art performance with the same architecture across various optimization problems.

**Contributions:** In this work we investigate the use of VBLL neural networks  for the first time in BO (cf. Fig. 1), and explore avenues for further improving these models. In particular, our main contributions and findings are: _(i)_ VBLL models outperform I-BNN models, a recently proposed surrogate model , on smooth synthetic benchmarks; _(ii)_ VBLL models outperform GPs on problems with complex input correlations; _(iii)_ VBLL models (and NN surrogate models generally) are sensitive to the training strategy used. Finally, we discuss implications for future work.

### Related Work and Background

Various flavors of Bayesian or partially-Bayesian networks have been explored for BO, including mean field BNNs , networks trained via stochastic gradient Hamiltonian Monte Carlo [12; 16], and last layer Laplace approximation BNNs [17; 18]. In , the authors find that infinite-width BNNs (I-BNNs) [19; 20; 21], perform particularly well especially on high-dimensional, non-stationary and non-Euclidean problems, a setting where standard GPs tend to struggle.

While BNNs are promising, they have often proven to be challenging to train and complex to use in practice. Bayesian last layer networks--which consider uncertainty only over the output layer--provide a simple (and often much easier to train) partially-Bayesian neural network model [11; 22; 23; 24; 25; 26]. Concretely, the standard model for regression with Bayesian last layer networks and a one-dimensional output is \(y=^{}_{}()+\), where \(^{m}\), and \(_{}\) are the features learned by a neural network backbone with parameters \(\). The noise \((0,)\) is assumed to be independent and identically distributed. With this observation model, fixed features \(_{}\), and a Gaussian prior on the weights as \(p()=(},S)\), posterior inference for the weights is analytically tractable via Bayesian linear regression, yielding the posterior \(p( X,Y)=(},S)\) and posterior predictive

\[p(y,,)=(}^{}_ {}(),\,_{}()^{}S_{ {}}()+)(},S).\] (1)

Since the predictive distribution is Gaussian, it pairs nicely with conventional acquisition functions in Bayesian optimization and bandit tasks [11; 25].

Usually, such BLL models are trained using gradient descent on the exact (log) marginal likelihood over all data points either via \(_{} p(Y X,)\) (which is computationally expensive and often unstable) or mini-batches  (which yields biased gradients and results in over-concentration when the model is conditioned on all data). To increase efficiency, recent work [15; 27] developed a deterministic variational lower bound to the exact marginal likelihood and proposed to optimize this instead resulting in the _variational_ Bayesian last layer (VBLL) model. Following [15; Theorem 1], the variational lower bound for regression with BLLs (under the prior defined previously) is

\[ p(Y X,)_{t}^{T}((y_{t} }^{}_{t},\,)-_{t}^{}S _{t}^{-1})-(q() p ())\] (2)

where \(_{t}_{}(_{t})\) and \(q()=(},S)\) is the variational posterior. The variational posterior over the last layer is trained with the network weights \(\) yielding a lightweight Bayesian formulation.

Figure 1: Variational Bayesian last layer model as a surrogate model for BO on a toy example. The VBLL model can capture _in-between_ uncertainty and analytic posterior samples are easily obtained through its parametric form making it a suitable surrogate for BO.

[MISSING_PAGE_FAIL:3]

both features and the variational posterior from scratch at each iteration, and relearning every \(5\) iterations and using continual learning (CL) in between.

In all subsequent experiments, we set the number of initial points equal to the input dimensionality and the batch size to one. We compare the performance of all surrogates for the following acquisition functions: _(i)_ log expected improvement (logEI) , a numerically more stable version of standard expected improvement, _(ii)_ upper confidence bound (UCB)  with constant \(=2\), _(iii)_ and Thompson sampling (TS) [36; 37]. The results for UCB are in Appendix D.

### Problem Settings

**Benchmark Problems:** We begin by examining a set of standard benchmark problems commonly used to assess the performance of BO algorithms [28; 34]. Figure 2 illustrates the performance of all surrogates on these benchmark problems. It can be observed that, as expected, GPs perform well. The BNN baselines also demonstrate strong performance on lower-dimensional problems, although they do not match the performance of GPs on the Hartmann function. Interestingly, for TS, we notice that on the Ackley5D benchmark, the VBLLs with analytic optimization of the Thompson samples even surpass the performance of GPs. The continual learning baseline shows the same performance as the standard VBLLs but with reduced compute.

**High-Dimensional and Non-Stationary Problems:** GPs without tailored kernels often struggle in high-dimensional and non-stationary environments ; areas where deep learning approaches are expected to excel. Our results on the \(200\)D NHdraw benchmark , the real-world \(25\)D Pestcontrol benchmark , and the \(12\)D Lunarlander benchmark  are shown in Fig. 3. On these bench

Figure 3: _High-dimensional and non-stationary benchmarks._ Performance of all surrogates for logEI (top) and TS (bottom). VBLLs demonstrate strong performance on high-dimensional problems.

Figure 2: _Classic benchmarks._ Performance of all surrogates for logEI (top) and TS (bottom).

marks, VBLLs significantly outperform the other baselines; especially for TS. While GPs perform well with logEI on NNdraw and the I-BNNs show good performance on Pestcontrol, the VBLLs are consistently the best performing surrogate. Similar to the classic benchmarks, the continual learning version of the VBLLs shows similar performance to the VBLLs.

## 4 Discussion

In this paper, we explored using the recently proposed VBLLs for BO. Our findings show that VBLLs perform on par with GPs on standard low-dimensional benchmarks, yet significantly outperform GPs in high-dimensional and non-stationary problems. Furthermore, VBLLs outperform I-BNNs, a recently proposed BNN surrogate. In future work, we will explore concepts such as variational continual learning  to reduce the computational time for VBLLs. Finding the optimal balance between reinitializing the network and applying continual learning updates-whether through recursive updates of the last layer or re-learning the variational posterior-will be crucial for effectively integrating VBLLs into BO for real-world problems.