# An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient

Yudong Luo\({}^{1,4}\), Guiliang Liu\({}^{2}\), Pascal Poupart\({}^{1,4}\), Yangchen Pan\({}^{3}\)

\({}^{1}\)University of Waterloo, \({}^{2}\)The Chinese University of Hong Kong, Shenzhen,

\({}^{3}\)University of Oxford, \({}^{4}\)Vector Institute

yudong.luo@uwaterloo.ca, liuguiliang@cuhk.edu.cn,

ppoupart@uwaterloo.ca, yangchen.pan@eng.ox.ac.uk

###### Abstract

Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.

## 1 Introduction

The demand for avoiding risks in practical applications has inspired risk-averse reinforcement learning (RARL). For example, we want to avoid collisions in autonomous driving , or avoid huge financial losses in portfolio management . In addition to conventional RL, which finds policies to maximize the expected return , RARL also considers the control of risk.

Many risk measures have been studied for RARL, for instance, exponential utility functions , value at risk (VaR) , conditional value at risk (CVaR) , and variance . In this paper, we mainly focus on the variance-related risk measures given their popularity, as variance has advantages in interpretability and computation . Such a paradigm is referred to as mean-variance RL. Traditional mean-variance RL methods consider the variance of the total return random variable. Usually, the total return variance is treated as a constraint to the RL problem, i.e., it is lower than some threshold . Recently,  proposed a reward-volatility risk measure, which considers the variance of the per-step reward random variable.  shows that the per-step reward variance is an upper bound of the total return variance and can better capture the short-term risk.  further simplifies 's method by introducing Fenchel duality.

Directly optimizing total return variance is challenging. It either necessitates double sampling  or calls for other techniques to avoid double sampling for faster learning . As for the reward-volatility risk measure,  uses a complicated trust region optimization due to the modified reward's policy-dependent issue.  overcomes this issue by modifying the reward according to Fenchel duality. However, this reward modification strategy can possibly hinder policy learning by changing a "good" reward to a "bad" one, which we discuss in detail in this work.

To overcome the limitations of variance-based risk measures, we propose to use a new risk measure: Gini deviation (GD). We first review the background of mean-variance RL. Particularly, we explain the limitations of both total return variance and per-step reward variance risk measures. We then introduceGD as a dispersion measure for random variables and highlight its properties for utilizing it as a risk measure in policy gradient methods. Since computing the gradient using the original definition of GD is challenging, we derive the policy gradient algorithm from its quantile representation to minimize it. To demonstrate the effectiveness of our method in overcoming the limitations of variance-based risk measures, we modify several domains (Guarded Maze , Lunar Lander , Mujoco ) where risk-aversion can be clearly verified. We show that our method can learn risk-averse policy with high return and low risk in terms of variance and GD, when others fail to learn a reasonable policy.

## 2 Mean-Variance Reinforcement Learning

In standard RL settings, agent-environment interactions are modeled as a Markov decision process (MDP), represented as a tuple \((,,R,P,_{0},)\). \(\) and \(\) denote state and action spaces. \(P(|s,a)\) defines the transition. \(R\) is the state and action dependent reward variable, \(_{0}\) is the initial state distribution, and \((0,1]\) is the discount factor. An agent follows its policy \(:[0,+)\). The return at time step \(t\) is defined as \(G_{t}=_{i=0}^{}^{i}R(S_{t+i},A_{t+i})\). Thus, \(G_{0}\) is the random variable indicating the total return starting from the initial state following \(\).

Mean-variance RL aims to maximize \([G_{0}]\) and additionally minimize its variance \([G_{0}]\)[8; 9; 12]. Generally, there are two ways to define a variance-based risk. The first one defines the variance based on the Monte Carlo **total return**\(G_{0}\). The second defines the variance on the **per-step reward**\(R\). We review these methods and their limitations in the following subsections. We will refer to \(\), \(_{}\) and \(\) interchangeably throughout the paper when the context is clear.

### Total Return Variance

Methods proposed by [8; 9; 12] consider the problem

\[_{}[G_{0}],\ \ \ [G_{0}] \]

where \(\) indicates the user's tolerance of the variance. Using the Lagrangian relaxation procedure , we can transform it to the following unconstrained optimization problem: \(_{}[G_{0}]-[G_{0}]\), where \(\) is a trade-off hyper-parameter. Note that the mean-variance objective is in general NP-hard  to optimize. The main reason is that although variance satisfies a Bellman equation, it lacks the monotonicity of dynamic programming .

**Double Sampling in total return variance.** We first show how to solve unconstrained mean-variance RL via vanilla stochastic gradient. Suppose the policy is parameterized by \(\), define \(J()=_{}[G_{0}]\) and \(M():=_{}(_{t=0}^{}^{t}R(S_{t},A_{t}) )^{2}\), then \([G_{0}]=M()-J^{2}()\). The unconstrained mean-variance objective is equivalent to \(J()=J()-M()-J^{2}()\), whose gradient is

\[_{}J_{}(_{t})=_{}J(_{t})- _{}(M()-J^{2}()) \]

\[=_{}J(_{t})-_{}M()-2J( )_{}J() \]

The unbiased estimates for \(_{}J()\) and \(_{}M()\) can be estimated by approximating the expectations over trajectories by using a single set of trajectories, i.e., \(_{}J()=_{}[R_{}_{}()]\) and \(_{}M()=_{}[R_{}^{2}_{}()]\), where \(R_{}\) is the return of trajectory \(\) and \(_{}()=_{t}_{}_{}(a_{t}|s_{t})\). In contrast, computing an unbiased estimate for \(J()_{}J()\) requires two distinct sets of trajectories to estimate \(J()\) and \(_{}J()\) separately, which is known as double sampling.

**Remark.** Some work claims that double sampling cannot be implemented without having access to a generative model of the environment that allows users to sample at least two next states . This is, however, not an issue in our setting where we allow sampling multiple trajectories. As long as we get enough trajectories, estimating \(J()_{}J()\) is possible.

Still, different methods were proposed to avoid this double sampling for faster learning. Specifically,  considers the setting \(=1\) and considers an unconstrained problem: \(_{}L_{1}()=[G_{0}]- g[G_{0} ]-\), where \(>0\) is a tunable hyper-parameter, and penalty function \(g(x)=(\{0,x\})^{2}\). This method produces faster estimates for \([G_{0}]\) and \([G_{0}]\) and a slower updating for \(\) at each episode, which yields a two-time scale algorithm.  considers the setting \(<1\) and converts Formula 1 into an unconstrained saddle-point problem: \(_{}_{}L_{2}(,)=-[G_{0}]+ [G_{0}]-\), where \(\) is the dual variable. This approach uses perturbation method and smoothed function method to compute the gradient of value functions with respect to policy parameters.  considers the setting \(=1\), and introduces Fenchel duality \(x^{2}=_{y}(2xy-y^{2})\) to avoid the term \(J()_{}J()\) in the gradient. The original problem is then transformed into \(_{,y}L_{3}(,y)=2y[G_{0}]+ -y^{2}-[G_{0}^{2}]\), where \(y\) is the dual variable.

**Limitations of Total Return Variance.** The presence of the square term \(R_{}^{2}\) in the mean-variance gradient \(_{}M()=_{}[R_{}^{2}_{}()]\)(Equation 2) makes the gradient estimate sensitive to the numerical scale of the return, as empirically verified later. This issue is inherent in all methods that require computing \(_{}[G_{0}^{2}]\). Users can not simply scale the reward by a small factor to reduce the magnitude of \(R_{}^{2}\), since when scaling reward by a factor \(c\), \([G_{0}]\) is scaled by \(c\) but \([G_{0}]\) is scaled by \(c^{2}\). Consequently, scaling the reward may lead to different optimal policies being obtained.

### Per-step Reward Variance

A recent perspective uses per-step reward variance \([R]\) as a proxy for \([G_{0}]\). The probability mass function of \(R\) is \((R=x)=_{s,a}d_{}(s,a)_{r(s,a)=x}\), where \(\) is the indicator function, and \(d_{}(s,a)=(1-)_{t=0}^{}^{t}(S_{t}=s,A_{t}=a|,P)\) is the normalized discounted state-action distribution. Then we have \([R]=(1-)[G_{0}]\) and \([G_{0}][R]}{(1-)^{2}}\) (see Lemma 1 of ). Thus,  considers the following objective

\[_{}()=[R]-[R]=[R- (R-[R])^{2}] \]

This objective can be cast as a risk-neutral problem in the original MDP, but with a new reward function \((s,a)=r(s,a)-r(s,a)-(1-)[G_{0}]^{2}\). However, this \((s,a)\) is nonstationary (policy-dependent) due to the occurrence of \([G_{0}]\), so standard risk-neutral RL algorithms cannot be directly applied. Instead, this method uses trust region optimization  to solve.

 introduces Fenchel duality to Equation 4. The transformed objective is \(_{}()=[R]-[R^{2}]+_{y} (2[R]y-y^{2})\), which equals to \(_{x,y}J_{}(,y)=_{s,a}d_{}(s,a)r(s,a)- r(s,a )^{2}+2 r(s,a)y- y^{2}\). The dual variable \(y\) and policy \(\) are updated iteratively. In each inner loop \(k\), \(y\) has analytical solution \(y_{k+1}=_{s,a}d_{_{k}}(s,a)r(s,a)=(1-)_{_{k}}[G_{0}]\) since it is quadratic for \(y\). After \(y\) is updated, learning \(\) is a risk-neutral problem in the original MDP, but with a new modified reward

\[(s,a)=r(s,a)- r(s,a)^{2}+2 r(s,a)y_{k+1} \]

Since \((s,a)\) is now stationary, any risk-neutral RL algorithms can be applied for policy updating.

**Limitations of Per-step Reward Variance. 1) \([R]\) is not an appropriate surrogate for \([G_{0}]\) due to fundamentally different implications.** Consider a simple example. Suppose the policy, the transition dynamics and the rewards are all deterministic, then \([G_{0}]=0\) while \([R]\) is usually nonzero unless all the per-step rewards are equal. In this case, shifting a specific step reward by a constant will not affect \([G_{0}]\) and should not alter the optimal risk-averse policy. However, such shift can lead to a big difference for \([R]\) and may result in an invalid policy as we demonstrated in later example. **2) Reward modification hinders policy learning.** Since the reward modifications in  (Equation 4) and  (Equation 5) share the same issue, here we take Equation 5 as an example. This modification is likely to convert a positive reward to a much smaller or even negative value due to the square term, i.e. \(- r(s,a)^{2}\). In addition, at the beginning of the learning phase, when the policy performance is not good, \(y\) is likely to be negative in some environments (since \(y\) relates to \([G_{0}]\)). Thus, the third term \(2 r(s,a)y\) decreases the reward value even more. This prevents the agent to visit the good (i.e., rewarding) state even if that state does not contribute any risk. These two limitations raise a great challenge to subtly choose the value for \(\) and design the reward for the environment.

**Empirical demonstration of the limitations.** Consider a maze problem (a modified version of Guarded Maze ) in Figure 1. Starting from the bottom left corner, the agent aims to reach the green goal state. The gray color corresponds to walls. The rewards for all states are deterministic (i.e., \(-1\)) except for the red state whose reward is a categorical distribution with mean \(-1\). The reward for visiting the goal is a positive constant value. To reach the goal, a risk-neutral agent prefers the path at the bottom that goes through the red state, but \([G_{0}]\) will be nonzero. A risk-averse agent prefers the white path in the figure even though \([G_{0}]\) is slightly lower, but \([G_{0}]=0\). Per-step reward variance methods aim to use \([R]\) as a proxy of \([G_{0}]\). For the risk-averse policy leading to the white path, ideally, increasing the goal reward by a constant will not effect \([G_{0}]\), but will make a big difference to \([R]\). For instance, when the goal reward is \(10\), \([R]=10\). When goal reward is \(20\), \([R] 36.4\), which is much more risk-averse. Next, consider the reward modification (Equation 5) for the goal

Figure 1: A modified Guarded Maze . Red state returns an uncertain reward (details in text).

reward when it is \(20\). The square term in Equation 5 is \(-400\). It is very easy to make the goal reward negative even for small \(\), e.g., \(0.1\). We do find this reward modification prevents the agent from reaching the goal in our experiments.

## 3 Gini Deviation as an Alternative of Variance

To avoid the limitations of \([G_{0}]\) and \([R]\) we have discussed, in this paper, we propose to use Gini deviation as an alternative of variance. Also, since GD has a similar definition and similar properties as variance, it serves as a more reasonable proxy of \([G_{0}]\) compared to \([R]\).

### Gini Deviation: Definition and Properties

GD , also known as Gini mean difference or mean absolute difference, is defined as follows. For a random variable \(X\), let \(X_{1}\) and \(X_{2}\) be two i.i.d. copies of \(X\), i.e., \(X_{1}\) and \(X_{2}\) are independent and follow the same distribution as \(X\). Then GD is given by

\[[X]=[|X_{1}-X_{2}|] \]

Variance can be defined in a similar way as \([X]=[(X_{1}-X_{2})^{2}]\).

Given samples \(\{x_{i}^{1}\}_{i=1}^{n}\) from \(X_{1}\) and \(\{x_{j}^{2}\}_{j=1}^{n}\) from \(X_{2}\). The unbiased empirical estimations for GD and variance are \(}[X]=}_{i=1}^{n}_{j=1}^{n}|x_{i}^{1}-x_ {j}^{2}|\) and \(}[X]=}_{i=1}^{n}_{j=1}^{n}(x_{i}^{1}-x_ {j}^{2})^{2}\).

Both risk profiles aim to measure the variability of a random variable and share similar properties . For example, they are both location invariant, and can be presented as a weighted sum of order statistics.  argues that the GD is superior to the variance as a measure of variability for distributions far from Gaussian. We refer readers to this paper for a full overview. Here we highlight two properties of \([X]\) to help interpret it. Let \(\) denote the set of real random variables and let \(^{p}\), \(p[1,)\) denote the set of random variables whose probability measures have finite \(p\)-th moment, then

* \([X]\ [X]\) for all \(X^{2}\).
* \([cX]=c[X]\) for all \(c>0\) and \(X\).

The first property is known as Glasser's inequality , which shows \([X]\) is a lower bound of \([X]\) if \(X\) has finite second moment. The second one is known as positive homogeneity in coherent measures of variability , and is also clear from the definition of GD in Equation 6. In RL, considering \(X\) is the return variable, this means GD is less sensitive to the reward scale compared to variance, i.e., scaling the return will scale \([X]\) linearly, but quadratically for \([X]\). We also provide an intuition of the relation between GD and variance from the perspective of convex order, as shown in Appendix 7. Note also that while variance and GD are both measures of variability, GD is a _coherent_ measure of variability . Appendix 12 provides a discussion of the properties of coherent measures of _variability_, while explaining the differences with coherent measures of _risk_ such as conditional value at risk (CVaR).

### Signed Choquet Integral for Gini Deviation

This section introduces the concept of signed Choquet integral, which provides an alternative definition of GD and makes gradient-based optimization convenient. Note that with the original definition (Equation 6), it can be intractable to compute the gradient w.r.t. the parameters of a random variable's density function through its GD.

The Choquet integral  was first used in statistical mechanics and potential theory and was later applied to decision making as a way of measuring the expected utility . The signed Choquet integral belongs to the Choquet integral family and is defined as:

**Definition 1** (, Equation 1).: _A signed Choquet integral \(_{h}:X,X^{}\) is defined as_

\[_{h}(X)=_{-}^{0}h(X x)-h(1) dx+_{0}^{}h(X x)dx \]

_where \(^{}\) is the set of bounded random variables in a probability space, \(h\) is the distortion function and \(h\) such that \(=\{h:,h(0)=0,h\ \}\)._

This integral has become the building block of law-invariant risk measures 1 after the work of . One reason for why signed Choquet integral is of interest to the risk research community is that it is not necessarily monotone. Since most practical measures of variability are not monotone, e.g., variance, standard deviation, or deviation measures in , it is possible to represent these measures in terms of \(_{h}\) by choosing a specific distortion function \(h\).

**Lemma 1** (, Section 2.6).: _Gini deviation is a signed Choquet integral with a concave \(h\) given by \(h()=-^{2}+,\)._

This Lemma provides an alternative definition for GD, i.e., \([X]=_{-}^{}h(X x)dx,h()= -^{2}+\). However, this integral is still not easy to compute. Here we turn to its quantile representation for easy calculation.

**Lemma 2** (, Lemma 3).: \(_{h}(X)\) _has a quantile representation. If \(F_{X}^{-1}\) is continuous, then \(_{h}(X)=_{1}^{0}F_{X}^{-1}(1-)dh()\), where \(F_{X}^{-1}\) is the quantile function (inverse CDF) of X._

Combining Lemma 1 and 2, \([X]\) can be computed alternatively as

\[[X]=_{h}(X)=_{0}^{1}F_{X}^{-1}(1-)dh()=_{0} ^{1}F_{X}^{-1}()(2-1)d \]

With this quantile representation of GD, we can derive a policy gradient method for our new learning problem in the next section. It should be noted that variance cannot be directly defined by a \(_{h}\)-like quantile representation, but as a complicated related representation: \([X]=_{h}_{h}(X)-\|h^{ }\|_{2}^{2}}\), where \(\|h^{}\|_{2}^{2}=_{0}^{1}(h^{}(p))^{2}dp\) if \(h\) is continuous, and \(\|h^{}\|_{2}^{2}:=\) if it is not continuous (see Example 2.2 of ). Hence, such representation of the conventional variance measure is not readily usable for optimization.

## 4 Policy Gradient for Mean-Gini Deviation

In this section, we consider a new learning problem by replacing the variance with GD. Specifically, we consider the following objective

\[_{}[G_{0}]-[G_{0}] \]

where \(\) is the trade-off parameter. To maximize this objective, we may update the policy towards the gradient ascent direction. Computing the gradient for the first term has been widely studied in risk-neutral RL . Computing the gradient for the second term may be difficult at the first glance from its original definition, however, it becomes possible via its quantile representation (Equation 8).

### Gini Deviation Gradient Formula

We first give a general gradient calculation for GD of a random variable \(Z\), whose distribution function is parameterized by \(\). In RL, we can interpret \(\) as the policy parameters, and \(Z\) as the return under that policy, i.e., \(G_{0}\). Denote the Probability Density Function (PDF) of \(Z\) as \(f_{Z}(z;)\). Given a confidence level \((0,1)\), the \(\)-level quantile of \(Z\) is denoted as \(q_{}(Z;)\), and given by

\[q_{}(Z;)=F_{Z_{}}^{-1}()=\{z:(Z_{} z )\} \]

For technical convenience, we make the following assumptions, which are also realistic in RL.

**Assumption 1**.: \(Z\) _is a continuous random variable, and bounded in range \([-b,b]\) for all \(\)._

**Assumption 2**.: \(}q_{}(Z;)\) _exists and is bounded for all \(\), where \(_{i}\) is the \(i\)-th element of \(\)._

**Assumption 3**.: \((z;)}{_{i}}/f_{Z}(z;)\) _exists and is bounded for all \(,z\). \(_{i}\) is the \(i\)-th element of \(\)._

Since \(Z\) is continuous, the second assumption is satisfied whenever \(}f_{Z}(z;)\) is bounded. These assumptions are common in likelihood-ratio methods, e.g., see . Relaxing these assumptions is possible but would complicate the presentation.

**Proposition 1**.: _Let Assumptions 1, 2, 3 hold. Then_

\[_{}[Z_{}]=-_{z Z_{}} _{} f_{Z}(z;)_{z}^{b}2F_{Z_{}}(t)-1 dt \]

Proof.: By Equation 8, the gradient of \([Z_{}]=_{h}(Z_{})\) (\(h()=-^{2}+,\)) is

\[_{}[Z_{}]=_{}_{h}(Z_{})=_ {0}^{1}(2-1)_{}F_{Z_{}}^{-1}()d=_{0}^ {1}(2-1)_{}q_{}(Z;)d. \]

This requires to calculate the gradient for any \(\)-level quantile of \(Z_{}\), i.e., \(_{}q_{}(Z;)\). Based on the assumptions and the definition of the \(\)-level quantile, we have \(_{-b}^{q_{}(Z;)}f_{Z}(z;)dz=\). Taking a derivative and using the Leibniz rule we obtain

\[0=_{}_{-b}^{q_{}(Z;)}f_{Z}(z;)dz=_{-b}^{q_ {}(Z;)}_{}f_{Z}(z;)dz+_{}q_{}( Z;)f_{Z}q_{}(Z;); \]

Rearranging the term, we get \(_{}q_{}(Z;)=-_{-b}^{q_{}(Z;)}_{ }f_{Z}(z;)dzf_{Z}q_{}(Z;); ^{-1}\). Plugging back to Equation 12 gives us an intermediate version of \(_{}[Z_{}]\).

\[_{}[Z_{}]=-_{0}^{1}(2-1)_{-b}^{q_{ }(Z;)}_{}f_{Z}(z;)dzf_{Z}q_{ }(Z;);^{-1}d \]

By switching the integral order of Equation 14 and applying \(_{}(x)=_{}x\), we get the final gradient formula Equation 11. The full calculation is in Appendix 8.1.

### Gini Deviation Policy Gradient via Sampling

In a typical application, \(Z\) in Section 4.1 would correspond to the performance of a system, e.g., the total return \(G_{0}\) in RL. Note that in order to compute Equation 11, one needs access to \(_{} f_{Z}(z;)\): the sensitivity of the system performance to the parameters \(\). Usually, the system performance is a complicated function and calculating its probability distribution is intractable. However, in RL, the performance is a function of trajectories. The sensitivity of the trajectory distribution is often easy to compute. This naturally suggests a sampling based algorithm for gradient estimation.

Now consider Equation 11 in the context of RL, i.e., \(Z=G_{0}\) and \(\) is the policy parameter.

\[_{}[G_{0}]=-_{g G_{0}}_{ } f_{G_{0}}(g;)_{g}^{b}2F_{G_{0}}(t)-1dt  \]

To sample from the total return variable \(G_{0}\), we need to sample a trajectory \(\) from the environment by executing \(_{}\) and then compute its corresponding return \(R_{}:=r_{1}+ r_{2}+...+^{T-1}r_{T}\), where \(r_{t}\) is the per-step reward at time \(t\), and \(T\) is the trajectory length. The probability of the sampled return can be calculated as \(f_{G_{0}}(R_{};)=_{0}(s_{0})_{t=0}^{T-1}[_{}(a_{t}| s_{t})p(r_{t+1}|s_{t},a_{t})]\). The gradient of its log-likelihood is the same as that of \(P(|)=_{0}(s_{0})_{t=0}^{T-1}[_{}(a_{t}|s_{t})p(s_{t+ 1}|s_{t},a_{t})]\), since the difference in transition probability does not alter the policy gradient. It is well known that \(_{} P(|)=_{t=0}^{T-1}_{}_{ }(a_{t}|s_{t})\).

For the integral part of Equation 15, it requires the knowledge of the CDF of \(G_{0}\). In practice, this means we should obtain the full value distribution of \(G_{0}\), which is usually not easy. One common approach to acquire an empirical CDF or quantile function (inverse CDF) is to get the quantile samples of a distribution and then apply some reparameterization mechanism. For instance, reparameterization is widely used in distributional RL for quantile function estimation. The quantile function has been parameterized as a step function [34; 35], a piece-wise linear function , or other higher order spline functions . In this paper, we use the step function parameterization given its simplicity. To do so, suppose we have \(n\) trajectory samples \(\{_{i}\}_{i=1}^{n}\) from the environment and their corresponding returns \(\{R_{_{i}}\}_{i=1}^{n}\), the returns are sorted in ascending order such that \(R_{_{1}} R_{_{2}}... R_{_{n}}\), then each \(R_{_{i}}\) is regarded as a quantile value of \(G_{0}\) corresponding to the quantile level \(_{i}=(+)\), i.e., we assume \(q_{_{i}}(G_{0};)=R_{_{i}}\). This strategy is also common in distributional RL, e.g., see [34; 38]. The largest return \(R_{_{n}}\) is regarded as the upper bound \(b\) in Equation 15.

Thus, given ordered trajectory samples \(\{_{i}\}_{i=1}^{n}\), an empirical estimation for GD policy gradient is (a detailed example is given in Appendix 8.2)

\[-_{i=1}^{n-1}_{i}_{t=0}^{T-1}_{}_ {}(a_{t,i}|s_{i,t}),\;\;_{i}=_{j=i}^{n-1}R_{_{j+1}}-R_{_{j}}-R_{_{n}}-R_{_{i}}  \]

The sampled trajectories can be used to estimate the gradient for \([G_{0}]\) in the meantime, e.g., the well known vanilla policy gradient (VPG), which has the form \(_{}[R_{}_{t=0}^{T-1}_{}_{}(a_{t} |s_{t})]\). It is more often used as \(_{}[_{t=0}^{T-1}_{}_{}(a_{t}|s_{t} )^{t}g_{t}]\), where \(g_{t}=_{t^{}=t}^{T-1}^{t^{}-t}r(s_{t^{}},a_{t^{ }})\), which is known to have lower variance. Usually \(g_{t}\) is further subtracted by a value function to improve stability, called REINFORCE with baseline. Apart from VPG, another choice to maximize \([G_{0}]\) is using PPO .

### Incorporating Importance Sampling

For on-policy policy gradient, samples are abandoned once the policy is updated, which is expensive for our gradient calculation since we are required to sample \(n\) trajectories each time. To improve the sample efficiency to a certain degree, we incorporate importance sampling (IS) to reuse samples for multiple updates in each loop. For each \(_{i}\), the IS ratio is \(_{i}=_{t=0}^{T-1}_{}(a_{i,t}|s_{i,t})/_{}(a_{i, t}|s_{i,t})\), where \(\) is the old policy parameter when \(\{_{i}\}_{i=1}^{n}\) are sampled. Suppose the policy gradient for maximizing \([G_{0}]\) is REINFORCE baseline. With IS, the empirical mean-GD policy gradient is

\[_{i=1}^{n}_{i}_{t=0}^{T-1}_{}_{ }(a_{i,t}|s_{i,t})(g_{i,t}-V(s_{i,t}))+_{i=1}^{n- 1}_{i}_{i}_{t=0}^{T-1}_{}_{}(a_{i,t}|s_{ i,t}) \]

where \(g_{i,t}\) is the sum of rewards-to-go as defined above. \(V(s_{i,t})\) is the value function. The first part can also be replaced by PPO-Clip policy gradient. Then we have

\[_{i=1}^{n}_{t=0}^{T-1}_{}((a_{i,t}|s_{i,t})}{_{}(a_{i,t}|s_{i,t})}A_{i,t},f( ,A_{i,t}))+_{i=1}^{n-1}_{i}_{i} _{t=0}^{T-1}_{}_{}(a_{i,t}|s_{i,t}) \]

where \(A_{i,t}\) is the advantage estimate, and \(f()\) is the clip function in PPO with \(\) being the clip range, i.e. \(f(,A_{i,t})=((a_{i,t}|s_{i,t})}{_{ }(a_{i,t}|s_{i,t})},1-,1+)A_{i,t}\).

The extreme IS values \(_{i}\) will introduce high variance to the policy gradient. To stabilize learning, one strategy is that in each training loop, we only select \(_{i}\) whose \(_{i}\) lies in \([1-,1+]\), where \(\) controls the range. The updating is terminated if the chosen sample size is lower than some threshold, e.g., \( n,(0,1)\). Another strategy is to directly clip \(_{i}\) by a constant value \(\), i.e., \(_{i}=(_{i},)\), e.g., see . In our experiments, we use the first strategy for Equation 17, and the second for Equation 18. We leave other techniques for variance reduction of IS for future study. The full algorithm that combines GD with REINFORCE and PPO is in Appendix 9.

## 5 Experiments

Our experiments were designed to serve two main purposes. First, we investigate whether the GD policy gradient approach could successfully discover risk-averse policies in scenarios where variance-based methods tend to fail. To accomplish this, we manipulated reward choices to assess the ability of the GD policy gradient to navigate risk-averse behavior. Second, we sought to verify the effectiveness of our algorithm in identifying risk-averse policies that have practical significance in both discrete and continuous domains. We aimed to demonstrate its ability to generate meaningful risk-averse policies that are applicable and valuable in practical settings.

**Baselines.** We compare our method with the original mean-variance policy gradient (Equation 2, denoted as MVO), Tamar's method  (denoted as Tamar), MVP , and MVPI . Specifically, MVO requires multiple trajectories to compute \(J()_{}J()\). We use \(\) trajectories to estimate \(J()\) and another \(\) to estimate \(_{}J()\), where \(n\) is the sample size. MVPI is a general framework for policy iteration whose inner risk-neutral RL solver is not specified. For the environment with discrete actions, we build MVPI on top of Q-Learning or DQN . For continuous action environments, MVPI is built on top of TD3  as in . We use REINFORCE to represent the REINFORCE with baseline method. We use MG as a shorthand of mean-GD to represent our method. In each domain, we ensure each method's policy or value nets have the same neural network architecture.

For policy updating, MVO and MG collect \(n\) episodes before updating the policy. In contrast, Tamar and MVP update the policy after each episode. Non-tabular MVPI updates the policy at each environment step. In hyperparameter search, we use the parameter search range in MVPI  as a reference, making reasonable refinements to find an optimal parameter setting. Please refer to Appendix 10 for any missing implementation details. Code is available at3.

### Tabular case: Modified Guarded Maze Problem

This domain is a modified Guarded Maze  that was previously described in Section 2.2. The original Guarded Maze is asymmetric with two openings to reach the top path (in contrast to a single opening for the bottom path). In addition, paths via the top tend to be longer than paths via the bottom. We modified the maze to be more symmetric in order to reduce preferences arising from certain exploration strategies that might be biased towards shorter paths or greater openings, which may confound risk aversion. Every movement before reaching the goal receives a reward of \(-1\) except moving to the red state, where the reward is sampled from \(\{-15,-1,13\}\) with probability \(\{0.4,0.2,0.4\}\) (mean is \(-1\)) respectively. The maximum episode length is \(100\). MVO and MG collect \(n=50\) episodes before updating the policy. Agents are tested for \(10\) episodes per evaluation.

**The failure of variance-based baselines under simple reward manipulation.** We first set the goal reward to \(20\). Here, we report the optimal risk-aversion rate achieved during training. Specifically, we measure the percentage of episodes that obtained the optimal risk-averse path, represented by the white color path in Figure 1, out of all completed episodes up to the current stage of training.

Notice that MVO performs well in this domain when using double sampling to estimate its gradient. Then we increase the goal reward to \(40\). This manipulation does not affect the return variance of the optimal risk-averse policy, since the reward is deterministic. However, the performances of MVO, Tamar, MVP all decrease, since they are more sensitive to the numerical scale of the return (due to the \([G_{0}^{2}]\) term introduced by variance). MVPI is a policy iteration method in this problem, whose learning curve is not intuitive to show. It finds the optimal risk-averse path when the goal reward is \(20\), but it fails when the goal reward is \(40\). An analysis for MVPI is given in Appendix 10.2.2. We compare the sensitivity of different methods with respect to \(\) in Appendix 10.2.4.

**Remark.** Scaling rewards by a small factor is not an appropriate approach to make algorithms less sensitive to the numerical scale for both total return variance and per-step reward variance, since it changes the original mean-variance objective in both cases.

### Discrete control: LunarLander

This domain is taken from OpenAI Gym Box2D environments . We refer readers to its official documents for the full description. Originally, the agent is awarded \(100\) if it comes to rest. We divide the ground into two parts by the middle line of the landing pad, as shown in Figure 10 in Appendix. If the agent lands in the right area, an additional noisy reward sampled from \((0,1)\) times \(90\) is given. A risk-averse agent should learn to land at the left side as much as possible. We include REINFORCE as a baseline to demonstrate the risk-aversion of our algorithm. REINFORCE, MVO and MG collect \(n=30\) episodes before updating their policies. Agents are tested for \(10\) episodes per evaluation.

We report the rate at which different methods land on the left in Figure 3(b) (we omit the failed methods), i.e, the percentage of episodes successfully landing on the left per evaluation. MVO, Tamar, and MVP do not learn reasonable policies in this domain according to their performances in Figure 3(a). MVP learns to land in the middle of the learning phase, but soon after fails to land. Since successfully landing results in a large return (success reward is \(100\)), the return square term (\([G_{0}^{2}]\)) introduced by variance makes MVP unstable. MVPI also fails to land since \([R]\) is sensitive to the numerical scale of rewards. In this domain, the success reward is much larger than other reward values. Furthermore, reward modification in MVPI turns large success rewards into negative values, which prevents the agent from landing on the ground. MG achieves a comparable return with REINFORCE, but clearly learns a risk-averse policy by landing more on the left.

Figure 3: (a) Policy evaluation return and (b) left-landing rate (i.e., risk-averse landing rate) v.s. training episodes in LunarLander. Curves are averaged over 10 seeds with shaded regions indicating standard errors. For landing left rate, higher is better.

Figure 2: (a) Policy evaluation return and (b,c) optimal risk-aversion rate v.s. training episodes in Maze. Curves are averaged over 10 seeds with shaded regions indicating standard errors. For optimal risk-aversion rate, higher is better.

### Continuous control: Mujoco

Mujoco  is a collection of robotics environments with continuous states and actions in OpenAI Gym . Here, we selected three domains (InvertedPendulum, HalfCheetah, and Swimmer) that are conveniently modifiable, where we are free to modify the rewards to construct risky regions in the environment (Through empirical testing, risk-neutral learning failed when similar noise was introduced to other Mujoco domains. Consequently, identifying the cause for the failure of risk-averse algorithms on other domains became challenging). Motivated by and following [43; 44], we define a risky region based on the X-position. For instance, if X-position \(>0.01\) in InvertedPendulum, X-position \(<-3\) in HalfCheetah, and X-position \(>0.5\) in Swimmer, an additional noisy reward sampled from \((0,1)\) times \(10\) is given. Location information is appended to the agent's observation. A risk-averse agent should reduce the time it visits the noisy region in an episode. We also include the risk-neutral algorithms as baselines to highlight the risk-aversion degree of different methods.

All the risk-averse policy gradient algorithms still use VPG to maximize the expected return in InvertedPendulum (thus the risk-neutral baseline is REINFORCE). Using VPG is also how these methods are originally derived. However, VPG is not good at more complex Mujoco domains, e.g., see . In HalfCheetah and Swimmer, we combine those algorithms with PPO-style policy gradient to maximize the expected return. Minimizing the risk term remains the same as their original forms. MVPI is an off-policy time-difference method in Mujoco. We train it with 1e6 steps instead of as many episodes as other methods. MVO and MG sample \(n=30\) episodes in InvertedPendulum and \(n=10\) in HalfCheetah and Swimmer before updating policies. Agents are tested for 20 episodes per evaluation. The percentage of time steps visiting the noisy region in an episode is shown in Figure 4(b,d,f). Compared with other return variance methods, MG achieves a higher return while maintaining a lower visiting rate. Comparing MVPI and TD3 against episode-based algorithms like MG is not straightforward within the same figure due to the difference in parameter update frequency. MVPI and TD3 update parameters at each environment time step. We shown their learning curves in Figure 5. MVPI also learns risk-averse policies in all three domains according to its learning curves.

We further design two domains using HalfCheetah and Swimmer. The randomness of the noisy reward linearly decreases when agent's forward distance grows. To maximize the expected return and minimize risk, the agent has to move forward as far as possible. The results are shown in Figures 18,19 in Appendix. In these two cases, only MG shows a clear tendency of moving forward, which suggests our method is less sensitive to reward choices compared with methods using \([R]\).

The return variance and GD during learning in the above environments are also reported in Appendix 10. In general, when other return variance based methods can find the risk-averse policy, MG maintains a lower or comparable return randomness when measured by both variance and GD. When

Figure 4: (a,c,e) Policy evaluation return and (b,d,f) location visiting rate v.s. training episodes in Mujoco of episode-based methods. Curves are averaged over 10 seeds with shaded regions indicating standard errors. For location visiting rate, lower is better.

other methods fail to learn a reasonably good risk-averse policy, MG consistently finds a notably higher return and lower risk policy compared with risk-neutral methods. MVPI has the advantage to achieve low return randomness in location based risky domains, since minimizing \([R]\) naturally avoids the agent from visiting the noisy region. But it fails in distance-based risky domains.

## 6 Conclusion and Future Work

This paper proposes to use a new risk measure, Gini deviation, as a substitute for variance in mean-variance RL. It is motivated to overcome the limitations of the existing total return variance and per-step reward variance methods, e.g., sensitivity to numerical scale and hindering of policy learning. A gradient formula is presented and a sampling-based policy gradient estimator is proposed to minimize such risk. We empirically show that our method can succeed when the variance-based methods will fail to learn a risk-averse or a reasonable policy. This new risk measure may inspire a new line of research in RARL. First, one may study the practical impact of using GD and variance risk measures. Second, hybrid risk-measure may be adopted in real-world applications to leverage the advantages of various risk measures.

**Limitations and future work.** Our mean-GD policy gradient requires sampling multiple trajectories for one parameter update, making it less sample efficient compared to algorithms that can perform updates per environment step or per episode. As a result, one potential avenue for future work is to enhance the sample efficiency of our algorithm. This can be achieved by more effectively utilizing off-policy data or by adapting the algorithm to be compatible with online, incremental learning.