# Failure Prediction from Few Expert Demonstrations

Anjali Parashar, Kunal Garg, Joseph Zhang, Chuchu Fan

Massachusetts Institute of Technology

Email:{anjalip,kgarg,jzha,chuchu}@mit.edu

Anjali Parashar is the corresponding author. Project website: https://mit-realm.github.io/few-demo/

###### Abstract

This extended abstract presents a novel three-step methodology for discovering failures that occur in the true system by using a combination of a minimal number of demonstrations of the true system and the failure information processed through sampling-based testing of a model dynamical system. The proposed methodology comprises a) exhaustive simulations for discovering failures using model dynamics; b) design of initial demonstrations of the true system using Bayesian inference to learn a GPR-based failure predictor; and c) iterative demonstrations of the true system for updating the failure predictor. As a demonstration of the presented methodology, we consider the failure discovery for the task of pushing a T block to a fixed target region with UR3E collaborative robot arm using a diffusion policy and present the preliminary results on failure prediction for the true system.

## 1 Introduction

Testing and validation are essential tools to ensure the safety of autonomous systems prior to deployment [1; 2; 3; 4]. Most of the state-of-the-art tools for model-based validation and falsification of the autonomous system assume access to the true system . These model-based tools mainly use sampling-based methods for failure discovery [6; 7; 8]. While sampling-based techniques allow efficient exploration of the search-space, they require large number of samples to work efficiently, and are therefore well suited for simulation based testing. Most of these approaches assume that the model dynamics and simulation testing environment adequately represent realistic testing conditions. However, this can be misleading, since a sim-to-real gap can lead to unexpected failures that were unobserved in the simulation environment on which the policy was trained . Additionally, uncertainties in state estimation and dynamics can also affect the performance of the autonomous systems. Collectively, these issues lead to failure modes that remain undiscovered, despite exhaustive simulation testing. The resulting sim-to-real gap is especially concerning from the perspective of safety, as the discovered failure modes in simulation may not reflect the true severity of real failures, i.e. a failure not reported as unsafe behavior in simulation may be unsafe and catastrophic for the true system. In this study, we analyze the sim-to-real gap from the perspective of falsification, using a sampling-based testing pipeline for simulation for efficient exploration of failures, while working with limited data from the true system to enable better prediction of failures.

## 2 Problem formulation

Consider a discrete-time closed-loop dynamics:

\[x_{t+1}=f(x_{t},(y_{t},z))+_{1}, y_{t}=Cx_{t}+_{2},\] (1)

where \(f:^{n}^{m}^{n}\) and \(C^{l n}\), with state \(x_{t}^{n}\) at time \(t\) and policy \(:^{l}^{d}^{m}\) which outputs actions based on environmental variables \(z^{d}\) and systemoutput \(y_{t}^{l}\), where \(_{1}\) and \(_{2}\) are disturbances in dynamics and state estimation, respectively. The environment variable \(z\) represents variables that can be independently controlled by the user, such as initial conditions of the system \(x_{0}\), and environmental information exogenous to the agent. In this work, we assume that the disturbances come from zero-mean Gaussian distributions given by \(_{1}(,_{1})\) and \(_{2}(,_{2})\), where the covariance matrices \(_{1}^{n n},_{2}^{l l}\) of the distributions are defined using scalars \(_{1},_{2}>0\) as \(_{i}=_{i}\), for \(i=1,2\), where \(\) is an identity matrix of the appropriate size. We consider two set of dynamics in this paper: model (known to the user) and true (unknown to the user). _True dynamics_ corresponds to the actual dynamics of the agent, which is unknown whereas _Model dynamics_ corresponds to the estimate of the true dynamics and is described as in (1). We denote a trajectory rollout of (1) for a given environment variable \(z\) under a given \((_{1},_{2})\) as \(X_{(z|_{1},_{2})}=(x_{i})_{i=0}^{T}\).2 The trajectory rollout of the true dynamics for a given environment variable \(z\) is denoted as \(X_{z}^{*}\).

In this work, we address the problem of discovering failures of the true dynamical system with limited demonstrations. For this purpose, we consider a user-defined risk function \(R:^{d}\) where \(R(z)=R(z,X_{z})\) denotes the risk corresponding to the trajectory rollout \(X_{z}\) for a given environment variable \(z\). Based on this risk function, we define failure of the system when the risk \(R(z)\) for a corresponding \(z\) exceeds a user-defined threshold \(R_{}\). As thus, the falsification problem can be mathematically formulated as discovering the set \(_{}^{*}:=\{z R(z,X_{z}^{*})>R_{}\}\). We aim to solve this under the constraint that we can query the true system only a few times \(N>0\) to obtain \(N\) trajectory rollouts \(\{X_{z}^{*}\}_{i=1}^{N}\) for \(z_{i}\). We present a three-step methodology to discover failures occurring in the true system by using a combination of a minimal number of demonstrations \(\{X_{z}^{*}\}\) and the failure information from the model dynamics obtained through sampling-based falsification.

## 3 Methodology

We assume that the model can capture a subset of the failures that could occur with the true system, i.e., \(_{}(f)_{}^{*}\). Based on this assumption, we obtain that the \(_{}^{*}_{}(f) _{}\), i.e, failures of the true system are a combination of algorithmic failures on the model system \(_{}(f)\) and failures due to the mismatch between model dynamics and actual dynamics, disturbances and other unknown reasons \(_{}\). We say that the set \(_{}(f)\) captures algorithmic failures as we assume that

Figure 1: The proposed methodology constitutes a) discovering failures using model information; b) design of initial demonstrations to learn true system failures using Bayesian inference; and c) sequential demonstration from low predicted-risk regions for GPR-based risk prediction update.

the policy \(\) is trained for the model \(f\) but still leads to failures. The first step of our methodology focuses on identifying failures that can be obtained using the model information.

### Pre-processing of failures: utilizing model information

We define the set of the environment variables for the algorithmic failures as:

\[_{}(f)\{z R(z,X_{(z_{1}=0,_ {2}=0)})>R_{}\},\] (2)

which captures the algorithmic failure of the model dynamics. This set can be readily obtained through extensive simulations using the model information. Next, we aim to capture the failures due to the mismatch between model dynamics and true dynamics, disturbances, and potentially other unknown reasons. For this, we sample \(_{1},_{2}\) from a bounded region given by \([_{1}^{},_{1}^{}][_ {2}^{},_{2}^{}]\), and observe the risk \(R(z,X_{z})\) corresponding to each disturbance, and collect values of \(z\) for which a failure is observed across all disturbances:

\[_{}\{z R(z,X_{(z_{1},_{2} )})>R_{}\ \ \ _{1},_{2}\{(0,0)\}\},\] (3)

so that \(_{}_{}\). The region \(_{}_{}_{ }(f)\) captures failures that can be discovered using model dynamics. Next, we aim to discover failures that the model system cannot capture through sampling \(z\) from the region \(_{}\) and obtaining demonstrations from true dynamics. Since \(_{}\) is not known, we obtain these samples from the region \(_{}\) as discussed in the next section.

### Sampling from sensitive regions: Design of experiments

Since we have a limited budget on the number of demonstrations we can obtain from the true dynamics, we aim to maximize the state-space covered in each of these demonstrations. For a given \(z\), we define a coverage function, given by \(C:\) given as \(C(z)=C(z,X_{z})\), which is a monotonic function of the state-space explored along the trajectory \(X_{z}\), and aim to sample \(z_{}\) from a distribution \(\) corresponding to high coverage:

\[z(z C(z,X_{z})>C_{},z _{}),\] (4)

where \(C_{}>0\) is a user-defined coverage threshold. Directly sampling from this distribution is intractable, so we use a Bayesian inference framework  with the posterior distribution as defined in (4). To ensure that the generated samples lie in the region \(_{}\), we use a Normalizing Flows based framework for classification, called Flow-GMM  to learn Gaussian distributions in latent space \(^{d}\) corresponding to the sets \(_{}\) and \(_{}\), given by \(_{1}\) and \(_{2}\) respectively, and reconstruct the Bayesian inference in \(\). We then use Metropolis-Hastings algorithm  to sample from the defined posterior distribution and apply a projection operator on the generated samples to ensure that we sample exclusively from \(_{2}\). The details of posterior construction and projection can be found in Appendix B. The pipeline discussed so far generates a collection of samples \(Z_{}=\{z_{} C(z,X _{z})>C_{}\}\). Once we have generated the samples, we choose \(N/2\) candidate values of \(z\) distributed uniformly across the search-space. This is achieved by dividing \(Z_{}\) into \(N/2\) clusters using K-means clustering, and choosing the geometric centers of the generated clusters for demonstrations. This allow us to collect \(N/2\) data points \(Z_{1}=\{z_{j}\}\) with corresponding risk values given by \(R_{1}=\{R(z_{j},X_{z_{j}}^{*})\}\). We also obtain \(M\) data points \(Z_{2}=\{z_{i}\}\) from the region \(_{}\), with the corresponding risk values given by \(R_{2}=\{R(z_{i},X_{z_{i}})\}\) using model dynamics \(f\). We define \(_{1}=[Z_{1},R_{1}]\) and \(_{2}=[Z_{2},R_{2}]\) as the dataset of demonstrations obtained from true and model system, respectively, and use them to train a model \(_{}:\) to predict risk \(=_{}(z)\) for a given \(z\), as illustrated in the next section where \(\) denotes the model parameters.

### Sequential failure prediction and training using Gaussian Processes

Motivated by the success of Gaussian Process Regression (GPR) in learning from limited demonstrations , we use GPR as the backbone of the risk prediction pipeline in this section. Using the dataset \(_{f}=_{1}_{2}\), we construct the marginal log likelihood \( p_{_{}}(R_{f}|Z_{t})\) for learning the model \(_{}\) using a sum of the marginal log likelihoods from both sources of data as:

\[ p_{_{}}(R_{f}|Z_{t})= p_{_{}}(R_{1}|Z_{1})+ p _{_{}}(R_{2}|Z_{2}),\] (5)where \(Z_{f}=[Z_{1},Z_{2}]\) and \(R_{f}=[R_{1},R_{2}]\). The training objective can be formulated as the maximization of the marginal log likelihood in (5) with \(_{}\) as the decision variable:

\[_{^{*}}=*{arg\,max}_{} p_{_{}}(R_{f} |Z_{f}).\] (6)

We first learn a model \(_{_{0}}\) for risk prediction using \(_{1}\) and \(_{2}\) and subsequently refine the model by a sequence of \(N/2\) demonstrations on the true system with sequential optimization of \(_{}\) solving (6) with the updated dataset and generation of data-point for the next demonstration. The details of sequential demonstration and risk prediction update can be found in Appendix C.

## 4 Falsification of diffusion policy on Push-T

As a demonstration of the our methodology, we consider the task of pushing a T-block to a fixed target region with a circular end-effector using a diffusion policy from  which predicts actions conditioned on observations. Fig. 4 in Appendix D shows the setup corresponding to the model and true system respectively. Appendix D has details of the model dynamics and experimental setup.

We restrict the number of hardware demonstrations to \(N=20\). Fig. 1 (see the plot under 'Discovering algorithmic failures') shows the region \(_{}\) discovered in simulation using the model dynamics. For validating the learned failure prediction using the learned model \(_{}\), we record the risk for \(10\) randomly sampled test demonstrations on the the true system, and compare against the predictions from our method and two other baselines. Table. 1 shows the risk prediction error with GPR using three methods, namely, data collected only using model dynamics (reported as Simulation), data corresponding to \(_{}_{}\) from model dynamics and uniformly chosen \(N\) data points on the true system (reported as Simulation+Exp (I)), and data collected using our approach (reported as Simulation+Exp (II)). The individual predictions from all three approaches and their comparison against the ground truth (risk from true system) is shown in the right plot in Fig. 2. We observe that the mean prediction error decreases with the chosen data collection scheme, however, the maximum error is higher for our approach, when compared to Simulation + Exp (I). This is due to the fact that the uniform sampling was able to discover failures that were unobserved with our method due to lack of sufficient exploration. To address this issue, we aim to examine a combination of exploration and exploitation in the data collection schemes. Appendix D provides a detailed analysis of the results summarized in this section.

    & Mean Error & Max Error & Std. Deviation \\  Simulation & 0.38 & 0.85 & 0.33 \\ Simulation + Exp (I) & 0.23 & 0.54 & 0.2 \\ Simulation + Exp (II) & 0.21 & 0.72 & 0.25 \\   

Table 1: Failure prediction baseline comparison

Figure 2: The left plot demonstrates model prediction on \(10\) data points where the predicted risk by the learned model \(_{}\) is either very low (\(R<0.3\)) or very high (\(R>0.3\)). The demonstration on true system marked as ‘Ground Truth’ illustrates the prediction to be accurate. The right plot shows the prediction of models on 10 random data points with data collected using different methods considered in Table. 1, compared against the risk observed from demonstrations.

We also validate the learned risk by conducting demonstrations on \(10\) data points sampled from predicted high-risk and predicted low-risk regions. The predicted and ground truth values of risk corresponding to these points is shown in the left plot in Fig. 2. As we can see, the GPR model accurately predict 'fail' and 'not fail' across all \(10\) data points, where failure for a chosen value of \(z\) corresponds to the predicted risk being higher than the threshold \(R_{}=0.3\). The complete set of results and hardware demonstrations for the Push-T task and additional tasks considered in this work can be found at the project website 3

## 5 Conclusion

In this paper, we present a novel scheme for discovering failures that occur due to sim-to-real gap using Bayesian inference principles. The pipeline presented for initial estimation of failures from model dynamics in Section 3.1 is built on Bayesian inference principles and leverages the expressivity of sampling techniques, which is followed by a sequential failure prediction pipeline. The approach for sequential failure prediction presented in Section 3.3 can be expressed more formally through Bayesian Experimental Design (BED) , which comprises future scope of work. The usage of Gaussian Processes for failure learning as a model choice works well with limited demonstration setting, however, poses scalability challenges, since the number of demonstrations required increases with the dimension of search space. We aim to address this in the future work, potentially by leveraging data-efficient and scalable models such as Variational Gaussian Processes (VGP) .