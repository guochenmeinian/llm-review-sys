# The ToMCAT Dataset

Adarsh Pyarelal\({}^{1}\), Eric Duong\({}^{2}\), Caleb Jones Shibu\({}^{2}\), Paulo Soares\({}^{2}\), Savannah Boyd\({}^{3}\),

**Payal Khosla\({}^{4}\), Valeria Pfeifer\({}^{3}\), Diheng Zhang\({}^{3}\), Eric Andrews\({}^{3}\), Rick Champlin\({}^{1}\), Vincent Raymond\({}^{5}\), Meghavarshini Krishnaswamy\({}^{6}\), Clayton Morrison\({}^{1}\), Emily Butler\({}^{4}\), Kobus Barnard\({}^{2}\)**

\({}^{1}\)School of Information, \({}^{2}\)Department of Computer Science, \({}^{3}\)Department of Psychology,

\({}^{4}\)Norton School of Human Ecology, \({}^{5}\)Lum AI, \({}^{6}\)Department of Linguistics

University of Arizona

adarsh@arizona.edu

https://toncat.ivilab.org

###### Abstract

We present a rich, multimodal dataset consisting of data from 40 teams of three humans conducting simulated urban search-and-rescue (SAR) missions in a Minecraft-based testbed, collected for the Theory of Mind-based Cognitive Architecture for Teams (ToMCAT) project. Modalities include two kinds of brain scan data--functional near-infrared spectroscopy (INIRS) and electroencephalography (EEG), as well as skin conductance, heart rate, eye tracking, face images, spoken dialog audio data with automatic speech recognition (ASR) transcriptions, game screenshots, gameplay data, game performance data, demographic data, and self-report questionnaires. Each team undergoes up to six consecutive phases: three behavioral tasks, one mission training session, and two collaborative SAR missions. This dataset will support studying a large variety of research questions on topics including teamwork, coordination, plan recognition, affective computing, physiological linkage, entrainment, and dialog understanding. We provide an initial public release of the de-identified data, along with analyses illustrating the utility of this dataset to both computer scientists and social scientists.

## 1 Introduction

Teams of the future will increasingly involve humans and AI agents working together as trusted partners, leveraging their complementary skills to achieve shared goals. The efficacy of AI teammates will be enhanced if they are able to understand the beliefs, desires, and intentions of their human teammates, i.e., if they have a _machine theory of mind_ (MToM) . However, this capability alone is not sufficient--they will need to understand the interpersonal dynamics _between_ their human teammates, that is, they will need a _machine theory of teams_. A natural first step for constructing a (computational) theory of teams is to draw upon the vast amount of existing literature on teamwork in purely human teams. However, as Roberts et al.  note, significant additional work is needed to extend existing models of teamwork to human-machine teams.

We present the ToMCAT (Theory of Mind-based Cognitive Architecture for Teams) dataset--a rich, multimodal dataset developed to significantly advance our understanding of teaming in both purely human and hybrid human-machine teams. The dataset contains data from experiments in which teams of three humans (and optionally, an AI advisor) execute complex collaborative tasks--specifically,urban search-and-rescue (USAR) missions--in a virtual Minecraft-based testbed . However, we emphasize that the primary focus of this paper is not on the USAR missions themselves, but rather the complex social behaviors that the testbed is designed to elicit. Additionally, we instrument the participants with additional sensors that capture data via the following modalities: spoken communications, gaze, facial image captures, galvanic skin response (GSR), electroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS), and have them perform a set of novel behavioral baseline tasks (SS 4).

**Main contributions. 1)** A rich multi-person, multimodal dataset for teams collaborating on complex, time-constrained tasks, **2)** Three structured behavioral baseline tasks designed to ground the physiological data, as well as serve as independent multi-person, multi-task, multimodal datasets; and **3)** Exploratory analyses suggestive of the huge space of inquiry possible with this data.

## 2 Related work

This paper builds upon multiple lines of research related to human-machine teaming, synthetic task environments, and interpersonal coordination as evidenced by physiological linkage. The primary motivation for this dataset is to accelerate the development of effective artificial agent teammates with artificial social intelligence through a deeper understanding of purely human and human-machine teaming in cognitively complex, time-constrained scenarios.

Machine Theory of MindGiven the core role played by theory of mind (ToM)  in human social intelligence, there has been steadily increasing interest in developing artificial agents with MToM . A significant portion of the literature on MToM, however, evaluates it in contexts that are either disembodied (e.g., image classification and purely language-based tasks ) or 'lightly-embodied' (e.g., text adventure games  and small 2D gridworlds ). Voxel-based environments such as Minecraft  represent a natural step up in complexity.

Minecraft for AI researchMinecraft is an open-world adventure game that is gaining popularity as an AI testbed due to its ability to support diverse tasks , modifiability, and large user base. Of the projects that use Minecraft for AI research, a large number use it as a testbed for reinforcement learning , or for training AI agents to perform tasks based on natural language commands . However, following others , we are interested in using Minecraft as a testbed for ToM and human-machine teaming--that is, rather than having AI agents execute tasks within a Minecraft-based environment, we have _humans_ executing the tasks, with AI agents (if present) acting as passive observers or advisors.

USAR tasks in synthetic task environmentsThe environment we use  is an example of a _synthetic task environment_ (STE) --a medium-fidelity simulation environment that strikes a pragmatic balance between highly abstracted lab settings (more controlled, cheap, less likely to generalize to real-world situations) and high-fidelity simulations/real-world environments (less controlled, expensive, more representative of real-world scenarios). The USAR task was chosen due to its time-constrained, cognitively demanding nature, coupled with the potential for humans and robots to perform complementary team roles in real-world USAR scenarios . The use of Minecraft-based USAR STEs to study human-machine teaming is relatively well-established--they have been used for small-scale studies  as well as large-scale datasets  upon which numerous analyses have been performed .

The dataset most closely related to ours is the ASIST Study 3 dataset . We use the same STE  and Minecraft USAR tasks . However, our data differs from theirs in several ways, the most important of which is the inclusion of a number of of additional sensing modalities in our experiments: gaze, facial image capture, EKG, GSR, EEG, and fNIRS (see SS 2.1 for more details), which opens up a number of additional research opportunities.

First, since we are concerned with theory of mind, recording EEG and fNIRS signals provides us a way to ground inferences about cognitive and affective components of human mental state in rich data that reflects the actual underlying brain activity of the participants--in a sense, getting us closer to the 'ground truth'. Notably, the study of the affective component of human mental states (i.e., emotions) is conspicuously absent from existing works on MToM, despite affect playing a crucial role in human social interactions and decision making. Additionally, affect is reflected in other forms,e.g. facial expressions and changes in heart rate, neither of which are represented in the ASIST Study 3 dataset. While we do not expect, say, USAR team members to wear FNIRS/EEG caps in the field, we believe that the modalities in this dataset will allow us to create mappings between surface-level indicators of affect and coordination (e.g., facial expressions, tone of voice) and deeper underlying affect and coordination as detected from fNIRS/EEG signals.

Second, physiological linkage (PL)--i.e., statistical association between the physiological markers of two or more people over time --has been shown to be predictive of performance and attributes [47; 48]. Furthermore, PL may depend on the interaction of individual differences and context--e.g., differences in individual social skills and attachment styles have been found to be associated with qualitatively distinct forms of PL in competitive and collaborative contexts . Therefore, PL may be a promising predictor and outcome of team dynamics. Through the addition of physiological sensing modalities, our dataset enables the study of PL in the context of human-machine teams.

Third, unlike in ASIST Study 3, the participants in our study perform a set of tasks designed to compare physiological changes and phonetic entrainment resulting from performing team tasks. Finally, instead of running multiple AI advisor experimental conditions, we only have one (the ToMCAT agent), resulting in a much larger amount of data for this single advisor, which will enable increased statistical power for analyses that are not focused on comparing the outcomes of interventions by different AI advisors (one of the primary goals of ASIST Study 3).

Open-access fNIRS datasetsNotably, to the best of our knowledge, our dataset is also the **largest** open-access fNIRS dataset to date. It is approximately **13.5 times** the size of the fNIRS2MW  dataset (1.5\(\) more subjects and \(\) 9\(\) more fNIRS data per subject).

### Physiological measures

We simultaneously record multiple subjects' neural activities (hyperscanning) during real-time social interactions . Based on previous literature on team cognition and hyperscanning , we selected EEG, fNIRS, EKG, GSR, and gaze as our main physiological measurement modalities. These were collected for each participant for the duration of the group session (see SS 3). This approach provides opportunities for data analysis at various time resolutions , optimal variable control by using data from different modalities for denoising, and higher-level feature selection/construction . Details on the equipment and procedures for data acquisition and signal processing are provided in the appendices.

EegEEG is a non-invasive measure of the scalp electrical activity generated from the cerebral cortex. It provides data on brain activity with temporal resolution on the order of a millisecond . This high temporal resolution affords us opportunities for event-related (task-based) analysis (event-related potential, ERP), which has been widely adopted in cognitive science research on various topics including decision-making, emotion elicitation, and team cognition . EEG hyperscanning has yielded fruitful results for human-human social interaction research . Sinha et al.  found that inter-brain synchrony calculated from simultaneous EEG recordings of paired subjects was found to be significantly higher when the subjects were in a cooperative scenario compared to when they were in a competitive scenario. EEG has also been used to study human-machine teaming scenarios--e.g., Shayesteh, Ojha, and Jebelli  measured EEG signals from subjects performing a collaborative construction task with a virtual robot in an immersive environment, and found that a \(k\)-nearest neighbors model (kNN) trained on EEG signals was able to predict the human's level of trust in the robot with an accuracy of \(\)88%.

fNIRSfNIRS is an non-invasive optical brain imaging technology that assesses the contrast between oxygenated and de-oxygenated hemoglobin in the cortex, and uses the hemodynamic fluctuation as an indirect measure of brain activity in targeted brain areas . While fNIRS has a lower temporal resolution than EEG  (on the order of 10 ms), it is highly portable and less susceptible to motion, making it an increasingly popular modality for social interaction experimental settings . For this study, we use optodes that mainly cover the frontal lobe area, based on previous research [52; 59; 60] that found that greater interpersonal brain synchronization occurs at the frontopolar area, indicating better coordination performance. Oxygenation changes in the prefrontal cortex (PFC) have also found to be related to performance on various individual cognitive tasks , including language translation and switching , verbal fluency , and mental manipulation .

EkgAn electrocardiogram (EKG) measures heart activity over time and offers high temporal resolution. Common EKG signal derivatives include inter-beat interval (i.e., the time between heartbeats) and respiratory sinus arrhythmia (i.e., variability in heart rate due to breathing). These signals result from coordinated biological activity within a person and are commonly used to model coordination between people, as reflected by physiological linkage. Additionally, EKG data can be used to filter out systematic cardiac activity noise (1-1.5 Hz ) from fNIRS data.

GsrGalvanic skin response (GSR), is a measure of electrical conductivity on the surface of the skin . Sweat gland activity varies unconsciously and automatically, peaking approximately 1-5 seconds after stimulus onset. GSR is commonly used in the study of emotion processes and teamwork. For example, studies have found that GSR activity is associated with team performance , mental effort , and self-reported emotion during team tasks . For our study, we were interested in the peak amplitudes (i.e., change from stimulus onset to highest peak), which can be used to examine sweat gland activity within and between teammates following a stimulus presentation or during team-based tasks.

GazeAn eye tracker works by shining infrared light onto the eyes of a participant, creating reflections on the corneae that are then used to identify the locations of their pupils. By capturing their eye/pupil movements, the eye tracker software can infer the point of gaze (where the participant is looking) in real time . Eye-tracking is widely used in a variety of disciplines, including psychology , marketing , and UI/UX research . Additionally, eye-tracking can provide event markers for other modalities such as EEG, fNIRS, and EKG.

## 3 Experimental design

The study was held at the University of Arizona. Individuals were deemed eligible if they were at least 18 years of age, read and spoke English, and did not have any major physical limitations that would interfere with completing tasks on a computer. Interested individuals contacted the research team via e-mail, text, or phone. Details on the ethical review and the recruitment process are provided in SS A.3 and Appendix B respectively. All participants were compensated with either an Amazon gift card or course credit. Participants were asked to complete a 30-minute individual 'pre-session' and a 3-hour 'group session'.

Figure 1: Our experimental setup for data collection. Figure 0(a) shows the layout of the participants. The ‘Lion’, ‘Tiger’, and ‘Leopard’ stations are for regular participants, while the ‘Cheetah’ station is used for the experimenter that joins the group session for the competitive ping-pong task (§ 4.3). The ‘Lion’ and ‘Tiger’ stations are separated from the ‘Leopard’ and ‘Cheetah’ stations by a divider, in order to reduce audio cross-contamination between the participants’ microphones. Figure 0(b) shows a more detailed view of an individual participant, who is instrumented to record EEG, fNIRS, GSR, EKG, and gaze data and in the midst of a Minecraft SAR mission.

### Pre-session

Participants started by completing the online consent form if they had not already done so. We then measured their heads to select an appropriately sized EEG/fNIRS cap for them.

Speech elicitation tasksWe conducted two speech elicitation tasks, collecting speech data for each participant prior to their interaction with their teammates. This created a speech baseline for each player prior to their interactions with their teammates and was used to study phonetic entrainment between teammates during the course of the Minecraft missions. For more details, see Appendix K. Entrainment is a useful method for assessing the dynamics of a social interaction and levels of rapport . Prior work has found strong correlations between entrainment and success in group tasks .

QuestionnairesThe following questionnaires were administered: (i) a COVID-19 health screener, (ii) a survey that collected information about basic demographics (e.g., sex, racial background, household income, highest level of education), experience with video games, and health (e.g., speech/hearing, language, impairments, diagnoses, psychoactive medication, etc.), (iii) the Big Five Inventory- 2 Short Form (BFI-2-SF) personality questionnaire , and (iv) the Attachment Style Questionnaire that assessed adult attachment .

### Group session

When required, the participants interacted with each other through a keyboard and mouse located at each experimental station. Each participant was positioned in front of a computer monitor, with dividers used to increase physical separation between the participants (see Figure 1). If only two out of the three planned participants showed up to the group session, or if one of the participants dropped out in the midst of it, a confederate (i.e., a member of our research team) would step in to take their place. Out of the 1014 task instances for which data was supposed to be recorded for regular participants, 147 (i.e., 14.5%) had experimenters filling in for participants.

Baseline tasksParticipants started by conducting a set of behavioral baseline tasks (SS 4).

Search-and-rescue missionsNext, participants conducted the Minecraft-based SAR missions described by Huang et al. . Each team conducted a 20-minute tutorial mission, followed by two 17-minute main missions: Saturn A and Saturn B. The tutorial mission consists of a series of tasks designed to familiarize participants with the game environment and their avatars' specific abilities. The first 2 minutes of the main missions were devoted to planning--participants were encouraged to discuss strategies and review good and bad practices they adopted in the previous mission. In the next 15 minutes, access to the building is unblocked and participants can effectively start to earn points, which happens after they find, treat and move victims of an in-game building collapse to assigned safe areas. A subset of the teams were advised by the ToMCAT AI agent , which was designed to improve team coordination by intervening on team communication.

Post-game surveyAfter completing the Minecraft tasks, participants completed a brief post-game survey. They were asked to rate their emotions due to (i) ther AI agent teammate, (ii) how the game went, and (iii) the other team members, on a scale of 'not at all' to 'a very large amount'. In addition, participants indicated their impression of the agent and other team members by sliding a bar between a pair of adjectives (e.g., intelligent-unintelligent, inexpert-expert, etc.). Lastly, participants responded on the extent to which they disagreed or agreed to general statements along with statements about other team members and the agent (e.g., "It seemed like my emotional reaction was wrong or incorrect because of the agent's response.").

## 4 Baseline tasks

We collected rest state physiological data and conducted three behavioral baseline tasks to ground complex physiological signals expected in collaborative missions to simpler, well-studied settings. For example, we can map patterns in the Minecraft tasks to patterns of coordination established in a simple task. These tasks closely resemble well-established ones, facilitating connections withexisting research. Our approach extends prior studies by (i) addressing the underexplored area of emotion in hyperscanning  through affective behavioral tasks and (ii) expanding from the prevalent two-person paradigm to experiments involving three participants.

### Rest state and finger tapping tasks

Rest state taskThis task was designed to collect baseline physiological data from the participants while they were in a resting state. This is consistent with standard neurophysiological research  and important for comparison purposes--by establishing a resting baseline, we can better understand an individual's functional baseline in the absence of exposure to the stimuli in the other tasks. In this task, participants sat quietly for 5 minutes without engaging in any activity, with their monitor displaying a countdown timer showing them the time remaining in the task.

Finger-tapping taskThis task was designed to record physiological data during team synchronization in a cooperative activity. It allows us to observe neural processes during synchronization. Results from Tognoli et al.  suggest certain neural correlates when participants are tasked with finger-tapping with and without visual cues from each other. Furthermore, hyperscanning  studies have shown an association between EEG signals and behaviors . Our finger-tapping task is a variant of the one proposed by Tognoli et al. .

### Affective task

In this task, participants viewed a curated set of images (see Appendix L) designed to elicit various emotions. We employed Russell's valence-arousal scale --a widely recognized tool in affective research--to quantitatively assess emotions. The task aims to collect physiological and emotional data for interpreting emotional experiences based on physiological responses in subsequent tasks. Prior studies indicate a connection between fNIRS, EEG, and autonomic functioning during the processing of emotions--specifically, PFC activation  and potential dual motive systems in the brain . These findings align with literature on the PFC's role in memory, emotion regulation, and cognition . The affective task includes 'individual' and 'team' affective subtasks.

Individual Affective TaskFor each image, the participant was shown the following sequence: (i) a black screen for one second, (ii) a '+' icon at the center of the screen (guiding the participant's attention to the center) for 0.5 seconds, (iii) the image itself for 5 seconds. After viewing each image, participants had 20 seconds to rate the emotions they experienced during their observation. The rating screen presented a 5-point valence scale (-2 for _upset_ to +2 for _happy_) and a 5-point arousal scale (-2 for 'calm' to +2 for 'excited'), with 0 denoting 'neutral'. These scales were adapted from the Self-Assessment Manikin (SAM)  pictorial rating--a non-verbal technique for gauging individual affect. Participants were prompted to register their emotional responses and submit them before the onset of the subsequent image.

Team Affective TaskThis task is similar to the individual affective task, except that participants viewed each image together instead of separately, after which they discussed their emotional experience and submitted a single rating representing their collective emotional experience in response to the image.

### Ping-pong task

The primary objective of this task is to collect neurophysiological data when participants are engaged in competitive and cooperative scenarios. A range of tasks including card games, ping-pong, and music and rhythm synchronization exercises have been used in the hyperscanning literature--e.g., studies have found evidence of inter-brain synchronization among participants collectively perform a piece of music  and in a cooperative task based on the Prisoner's Dilemma . The ping-pong task is divided into two subtasks: competitive and cooperative.

Competitive Ping Pong TaskInspired by Sinha et al. , our competitive ping-pong task has participants compete against each other in a 2-minute 1-on-1 computer-based ping-pong game. Typically, two of the three participants would compete against each other, while the third competed against a confederate. Players controlled an on-screen paddle with a mouse. Paddles were positioned on the left and right sides of the screen, and constrained to move solely vertically. Participants scored a point whenever the ball hit the wall of the side opposite to that of their paddle. After hitting the wall, the ball would ricochet back with the vertical component of its velocity being randomized. Prior to the match, a 10-second familiarization phase was provided during which the participants could practice moving their paddles while the ball remained stationary at the center of the screen.

Cooperative Ping Pong TaskThis task was similar to the competitive version, except that all three participants were on the same side, playing against an AI agent instead of against each other. The horizontal component of the ball's velocity during the cooperative task was higher than that in the competitive task. The participants' paddles could move through each other and were on the left side of the screen, while the AI agent's paddle was on the right. Similar to the competitive task, a 10-second familiarization phase was provided before the start of the match.

## 5 Exploratory experiments

We developed two simple experiments to illustrate the large scope of new studies our data set can support. The first is designed to compare the power of EEG and INIRS data to predict self-reported affect, and the second explores whether synchronization of EEG and fNIRS signals among team members is predictive of team performance. Note that while we do not fuse the data from the two modalities, these experiments are intrinsically multimodal since the EEG and fNIRS modalities are recorded simultaneously for each participant, thus enabling us to compare their predictive power.

We emphasize that these experiments are _exploratory_ (i.e., meant to recognize novel patterns in the data, which can potentially lead to the generation of new hypotheses) rather than _confirmatory_ (i.e., testing existing hypotheses). Both types of experiments are required for scientific progress . Exploratory experiments are especially appropriate when the topic of research--in our case, machine learning based on brain data--is relatively less well-studied compared to other modalities (due to the inherently challenging nature of collecting brain data) such as images, text, and audio.

### Predicting affect from brain scan data

Predicting affect from brain scan data is a challenging, yet intriguing endeavor in the realm of neuroscience and affective computing. In this study, we use a multimodal dataset comprising EEG and fNIRS data to examine the feasibility of predicting individuals' self-reported valence and arousal using data from the individual affective task (SS 4.2). We focus on specific regions of the head to ensure spatial alignment of EEG and fNIRS data, enabling the comparison of these two modalities.

Most work on mapping brain scan data to valence and arousal uses EEG data. This includes Rayatdoost et al. , who developed a deep domain adversarial neural network (DANN) to link EEG data to valence and arousal, achieving average classification accuracies of 72.8% and 65.0% for valence and arousal respectively on the MAHNOB-HCI database , and accuracies of 69.8% and 57.6% for valence and arousal classification on the DAI-EF database . Galvao, Alarcao, and Fonseca  used kNN on features derived from the EEG frequency domain to predict valence and arousal values, achieving accuracies of 79.4%, 83%, and 80.6% on the DEAP , AMIGOS , and DREAMER  datasets, respectively. For all five datasets, participants watched videos and subsequently rated their emotions using SAM scales  for valence and arousal (a 9-point scale for MAHNOB-HCI, DAI-EF, DEAP, and AMIGOS, and a 5-point scale for DREAMER)

Bandara et al.  used a support vector machine (SVM) to predict valence and arousal scores from fNIRS data, achieving an \(F_{1}\) score of 0.74 on the DEAP dataset. For images, Trambaiolli, Biazoli, Cravo, et al.  achieved a classification accuracy of 89% in discerning positive from negative valence using fNIRS signals and a linear discriminant analysis (LDA). Finally, Sun, Ayaz, and Akansu  combined EEG and fNIRS data, and used an SVM to classify with 75% accuracy.

While many studies use a SAM scale with ranges of 1-9, 1-5, or 1-10, we use a scale with a range of \(-2\) to \(+2\) for a more compact scale and a clear neutral reference. We use brain scan data obtained from EEG and fNIRS recording trimmed to sets whose EEG and fNIRS samples overlap as best as possible. Specifically, for EEG we selected channels FC5, FCz, FC6, F7, F8, AFF1h, and AFF2h, whereas for fNIRS we selected channels Fz-F1, Fz-F2, F3-F7, F3-F1, F4-F2, F4-F6, AF3-F7, AF3-Afz, AF4-F6, and AF4-Afz. We trained separate CNNs  for fNIRS and EEG data. The fNIRS data were kept in the spatial domain, while EEG data were segmented into frequency bands (theta, alpha, beta, and gamma) and their wavelet features were extracted with four levels of decomposition.

fNIRS signals exhibit a phenomenon known as the hemodynamic response factor (HRF) [111; 112] which represents the relationship between neural activity and the corresponding changes in blood oxygenation levels that occur in response to that activity (when a participant views a specific image). The HRF consists of two phases: (i) the _initial dip_, which typically lasts for 1-2s and involves an initial drop in the concentration of oxygenated hemoglobin (HbO) and a simultaneous increase in deoxygenated hemoglobin (HbR) shortly after neural activation, and (ii) the _hemodynamic response peak_, in which there is an increase in HbO concentration and a decrease in HbR concentration, resulting in a peak in HbO concentration that occurs \(\) 4-6s after activation.

We present our results in Table 1. We tried several offsets for the fNIRS data to see if there was a noticeable effect due to the HRF as discussed above. We note that accuracies for EEG and fNIRS are not precisely comparable due to the disparity in participant numbers--97 for EEG and 102 for fNIRS. Unfortunately, our attempt at using basic CNNs for classification did not perform any better than the baseline. In contrast to prior work, we held out data in units of participant-image pairs, rather than predicting solely within participants. The relationship between functional brain regions and cap location varies among participants. While the training data did contain some data for held out participants looking at _other_ images, most of the training data was for other participants. Success on this task will likely require addressing functional regions to individualized cap locations, as well as more effort on neural network design. Given basically baseline performance, we are not surprised that accounting for the HRF with an offset did not make any real difference. Further details and quantitative results can be found in the confusion matrices provided in SS F.1.

### Linking temporal correlation of brain signals with scores

Shared cognition in team environments is gaining interest, particularly in understanding social dynamics that lead to successful performance, and in developing intervention techniques to enhance collaboration [113; 114]. Research has consistently shown that cooperation plays a vital role in influencing overall task performance , enabling coordination and information sharing, which enhances the effectiveness of the team [115; 116]. Studies have found behavioral and neurological synchronization between subjects during cooperative tasks [115; 116; 117], reinforcing the idea that cooperation may go beyond mere action coordination or knowledge sharing and may suggest a shared mental model within cooperative settings that includes coordination and sharing of social content like emotion and intentions [113; 119].

Research efforts on these fronts will benefit from more comprehensive data, specifically, data containing a larger set of modalities, and from multiple interacting participants. Existing studies mostly focus on limited brain regions with a single modality (i.e., either fNIRS or EEG but not both) during a single task [116; 117], which undermines their ability to capture the complex nature of shared mental models. Our research extends previous methods of classifying cognitive processes in single-participant studies  to identifying cognitive processes in teams, thus enabling the study of shared mental models of teams.

In this second experiment to illustrate the potential of our dataset, we study whether synchronization of EEG and fNIRS signals between team members can predict team performance. Building on research establishing a connection between synchronization and higher cooperation levels [115; 116; 117], we examine the brain holistically for associations between the correlation of EEG and fNIRS data

   Model & Offset (s) & Window size (s) &  & Loss \\   & & & Valence & Arousal & \\  CNN\({}_{}\) & 0 & 2 & 26.8 \(\) 1.2 & 30.1 \(\) 0.8 & 3.02 \(\) 0.012 \\ CNN\({}_{}\) & 2 & 2 & 30.4 \(\) 1.2 & 28.0 \(\) 0.8 & 3.02 \(\) 0.008 \\ CNN\({}_{}\) & 5 & 2 & 28.4 \(\) 1.2 & 30.3 \(\) 0.8 & 3.01 \(\) 0.007 \\ CNN\({}_{}\) & 0 & 1 & 29.3 \(\) 1.4 & 29.8 \(\) 1.3 & 2.80 \(\) 0.004 \\ Baseline\({}_{}\) & N/A & N/A & 29.0 \(\) 0.9 & 29.7 \(\) 0.1 & N/A \\ Baseline\({}_{}\) & N/A & N/A & 27.5 \(\) 0.7 & 30.3 \(\) 0.5 & N/A \\   

Table 1: Accuracy and loss (mean \(\) standard error of the mean, computed over 5 folds) for classification of valence and arousal scores.

[MISSING_PAGE_FAIL:9]

Dataset usage

Accessing the datasetThe dataset and its documentation is available at https://tomcat.ivilab.org. We provide access to the data in two ways. The first is through a Datasette  instance, which provides graphical and programmatic interfaces for users to explore the data, retrieve subsets of it that they are interested in, or simply download the backing SQLite database. The second is in the form of pre-built files containing subsets of data that we (i) used for the experiments in this paper, and (ii) expect will be commonly requested by other researchers.

A key issue with using the raw data is that it comprises data from multiple asynchronous data streams. Hence, aligning the data for multiple participants entails interpolation. A second issue is recording specific issues encountered during data collection, such as an experimenter stepping in for a participant. Finally, derived data will also include standard data transformations and cleaning.

Continued engagementThe scale and complexity of this dataset make it infeasible for us to annotate every type of label that might be of interest. Rather, we hope that the release of the dataset will seed the development of a community that works together to fully explore this data. We envision a process by which researchers build upon this dataset by adding layers of annotations for labels of interest, unlocking the ability to answer additional research questions. We are also happy to include pointers on our website to papers that use this dataset, in order to facilitate connections between researchers working with this dataset.

We _highly_ encourage users to sign up for our mailing list to get updates on data issues and annotation layers, benchmarks, and documentation.

### Additional usage examples

In SS 5, we presented illustrative experiments in that study the affective component of ToM and the relationship between brain signal correlations and team performance on a collaborative task. One could study other aspects of ToM as well.

Intention detectionFor example, _intentions_ are commonly considered part of human mental states. Consider a researcher who is interested in developing algorithms to infer participant intentions from observed behavior (i.e., _plan recognition_). This could be done by encoding the observations as sequences of discrete actions--either manually or semi-automatically, depending on the temporal, spatial, and semantic granularity of interest. Data that could be used for this include the participant's in-game position and velocity, as well as the semantic contents of their utterances.

Utterance classificationAnother example use case involves labeling participant utterances with sentiment, emotion, and dialog act labels will enable developing models for sentiment, emotion and dialog act classification in task-related dialog. Unlike previous dialog datasets, the ToMCAT dataset contains text, speech, and physiological data that can be simultaneously leveraged for these classification tasks.

## 7 Conclusion

In this transdisciplinary work, we integrate numerous threads of research in computer science, psychology, and cognitive science to present the ToMCAT dataset, which to our knowledge is the only dataset that contains both (i) data on human-machine teaming in a complex synthetic task environment and (ii) rich physiological data from a number of sensors, including fNIRS and EEG, thus enabling the exploration of fundamental research questions related to the interplay of competition, cooperation, and neurophysiological responses in human-machine teams. Furthermore, to our knowledge, this is the largest open-access fNIRS dataset currently available. Finally, we conduct exploratory experiments linking valence, arousal, and team performance to brain signals, illustrating the potential of the dataset for exploring a variety of research questions. We are excited to share this dataset with the community and look forward to seeing the research findings it enables.