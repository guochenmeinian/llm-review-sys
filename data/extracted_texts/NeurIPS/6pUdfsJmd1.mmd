# AI-Assisted Generation of Difficult Math Questions

Vedant Shah\({}^{1,2}\) Dingli Yu\({}^{3}\) Kaifeng Lyu\({}^{3}\) Simon Park\({}^{3}\) Jiatong Yu\({}^{3}\) Yinghui He\({}^{3}\) Nan Rosemary Ke\({}^{1}\) Michael Mozer\({}^{4}\) Yoshua Bengio\({}^{1,2}\) Sanjee Arora\({}^{3}\) Anirudh Goyal\({}^{*}\)\({}^{1}\)

\({}^{1}\)Mila - Quebec AI Institute \({}^{2}\)Universite de Montreal \({}^{3}\)Princeton University

\({}^{4}\)University of Colorado, Boulder

Correspondance: vedantshah2012@gmail.com, anirudhgoyal9119@gmail.com

###### Abstract

Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is an unmet demand for diverse and challenging mathematics questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty. We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. Initially, leveraging LLM metacognition skills [Didolkar et al., 2024], a strong LLM is used to extract core "skills" from existing math datasets. These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills that must be utilized in the question. The use of two very different skills within each question makes finding such questions an "out of distribution" task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions. Applying this pipeline on skills extracted from MATH dataset [Hendrycks et al., 2021] resulted in **MATH\({}^{2}\)** - a dataset of higher quality math questions, as evidenced by lower performance of all models on MATH\({}^{2}\) than on MATH. Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of _scalable oversight_. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH\({}^{2}\) is the square on MATH. This suggests that successfully solving the question in MATH\({}^{2}\) requires a nontrivial combination of two distinct math skills.

## 1 Introduction

Significant improvement in the capabilities of LLMs [Chowdhery et al., 2023, Anil et al., 2023, Team, 2023, Team et al., 2023, Abdin et al., 2024, Achiam et al., 2023, Touvron et al., 2023] to understand and generate complex mathematical content has been achieved by leveraging all the public data and a fair bit of private data. Sources of high-quality, varied, and difficult mathematical questions are drying up. Even finding new questions for evaluation is getting difficult since newly-released human exams are somewhat similar to past exams, which are potentially present in the LLMs' training datasets. Hence, there is a pressing need for innovative methods to create new, diverse, and challenging questions.

Expert mathematicians and educators possess the deep understanding required to create questions that not only test a wide range of skills but also push the boundaries of what the learners, and by extension, the models, can handle. However, relying solely on human experts is not scalable. Generating synthetic questions using LLMs is feasible at scale (Trinh et al., 2024; Li et al., 2024; Gunasekar et al., 2023; Patel et al., 2024; Toshniwal et al., 2024; Gupta et al., 2023; Lu et al., 2024; Honovich et al., 2022), but often fall short in terms of the necessary difficulty. Huang et al. (2024) employs a similar approach as ours where they extract _topics_ and corresponding _keypoints_ from a set of seed problems using GPT-4, and then combine the _topic_ to generate new questions, again using GPT-4). However, the generated data is meant to be used for the finetuning of models as compared to serving as an evaluation set in our case. As a result, the questions generated in Huang et al. (2024) are not sufficiently difficult. Similarly, limited work exists on ensuring the necessary diversity in the generated synthetic data. Chan et al. (2024) proposes prompting frontier models to generate questions where each question is generated in the context of a _persona_ as a way of ensuring diversity. They use 1M different personas to generate questions, which are then used for finetuning models, leading to significant improvements. This dichotomy between the quality of human-generated questions and the scalability of LLM-generated questions presents a significant challenge (Yu et al., 2024).

### Evaluation Saturation Phenomenon

LLM evaluations are becoming saturated due to improvements from better training and larger datasets, but also from evaluation-specific optimizations like supervised fine-tuning (SFT) on synthetic question-answer pairs. These pairs, generated by leading proprietary models or filtered from the model's responses, can significantly boost performance. For instance, just 1 million synthetic examples raised Llama 7B's MATH dataset performance to GPT-4 levels (Li et al., 2024).

The distinction between general and evaluation-specific improvements is key, as the latter can lead to overfitting rather than real skill acquisition. This issue was evident when models showed performance drops on a new GSM8K dataset version and on newer Chinese GaoKao exams, suggesting shallow understanding of mathematics (Zhang et al., 2024).

### Proposed Framework: AI-assisted Generation of Difficult Math Questions

Recent research (Arora and Goyal, 2023; Didolkar et al., 2024) demonstrated that top LLMs possess a robust understanding of mathematical skills, including the capability to identify the skills required to solve given questions (Reid et al., 2024; Achiam et al., 2023). This naturally raises the question: _can LLMs operate in the reverse direction, i.e., generate math problems when given a list of skills that have to be tested?_ Our initial attempts yielded mixed results. While leading models could produce creative math questions when provided with a list of skills, the majority of these questions exhibited one or more of the following shortcomings: too similar to existing questions in datasets; have errors or nonsensical elements; are too tedious or mechanical to be engaging for human annotators. (See Section A.5.) Moreover, they often conflate "difficulty" with tedious calculations, which actually would play to the strength of machines to leverage external tools such as calculators or Python interpreters.

Nevertheless, there were promising instances where LLMs generated interesting and correct questions that they were unable to solve, due to incomplete or incorrect reasoning. This observation led us to the concept of _AI-assisted creation of evaluation datasets_. Our process may also be of interest for human pedagogy since it begins with the extraction of core "skills" from existing math datasets, which serve as the foundational elements of mathematical questions. The current paper focuses on the MATH dataset (Hendrycks et al., 2021), a mainstay of LLM evaluation in recent years.

In our AI-assisted process, human experts played a crucial role. Using the (question, answer) pairs generated by LLMs and leveraging API access to leading models, experts identified promising questions--often those incorrectly answered by the LLMs but containing many correct ideas. Experts then refined these questions to enhance their engagement value and provided gold-standard answers. The AI-assisted process not only boosted human productivity but also resulted in high-quality, novel questions distinct from those in existing datasets.

## 2 Pipeline for AI-Assisted Question Generation

We present a structured approach to generating challenging mathematics questions by combining the capabilities of large language models (LLMs) and human expertise. Given below is a high-level overview of the process before delving into the details of each step.

We begin our pipeline with **skill extraction** - identifying and cataloging distinct mathematical skills from a dataset, as described in Didolkar et al. (2024). This step creates a repository of skills linked to specific questions. The motivation behind this is to systematically generate and analyze questions that require specific skills, ensuring a comprehensive evaluation framework.

We employ a five-step approach to generate difficult math questions using advanced models. For each round of generation, we randomly sample a pair of skills and three sample question-solution pairs corresponding to each skill from the skill repository. These reference examples are sourced from the MATH dataset.

**Step 1: Skill Pair Validation.** We begin by asking the LLM (GPT-4 or Claude) to validate a randomly sampled skill pair by assessing the qualitative similarity of the two skills. Reference examples are provided in-context to enrich the model's understanding of the skills. If the model deems the skills too similar, they are flagged and excluded from question generation, as similar skills might lead to simpler questions.

**Step 2: Question Generation.** Next, we prompt the LLM to generate a question and a brief solution requiring the application of both skills in the sampled pair. We specify two conditions to ensure high-quality questions: the question should either require an exact answer or specify that an approximate answer is acceptable, and it should ask for only a single final result. In-context, we provide two multi-turn conversations between a human and an AI assistant. These conversations demonstrate the human providing feedback on the AI-generated questions, which the AI then refines. This helps the model anticipate and avoid practical issues, such as insufficient involvement of skills or logical inconsistencies. Appendix A.6 provides examples of the responses of different models in the question generation step.

**Step 3: Solution Attempt.** The model then attempts a solution to the generated question, adopting an adversarial approach to identify flaws such as insufficient information, ambiguity, self-contradiction, or excessive computation. If any issues are found, the model stops solving and clearly states the problems. Otherwise, it completes the solution. During this step, the model does not receive the skill names or reference examples to ensure unbiased problem-solving.

**Step 4: Question Validation.** We give LLM the generated question and its solution for validation against a fixed rubric consisting of seven criteria. We detail the validation criteria in Appendix A.1. The model uses reference examples and validation exemplars - model generated examples of validating questions, to facilitate this step. We employ majority voting (maj @ 4) to enhance robustness.

Figure 1: (a) **AI-assisted question generation**: The five-step pipeline includes: (i) Skill Pair Validation, ensuring distinct skills; (ii) Question Generation, producing a problem that combines both skills; (iii) Attempted Solution, where the model solves using a _defeatist_ approach; (iv) Question Validation, assessing correctness, rigor, and quality; and (v) Final Solution, applying advanced techniques to enhance accuracy. (b) **Comparison of Zero-Shot Performance**: This figure shows zero-shot Chain of Thought (CoT) performance on MATH and MATH\({}^{2}\). Proprietary models show the smallest performance drop on MATH\({}^{2}\), while smaller models experience larger drops. Detailed results are in Table 1.

**Step 5: Final Solution and Re-validation.** For questions classified as valid, we ask the LLM to re-solve the question to obtain a final solution. Reference examples are provided in-context to improve the model's understanding. We use majority voting (maj @ 4) to ensure consistency. If all the answers obtained in this step are unique, indicating potential ambiguity, the question is discarded.

The questions obtained from the above pipeline are further screened by humans. This structured approach not only generates challenging and novel math questions but also ensures their quality through rigorous validation, effectively combining the strengths of AI and human oversight. For detailed examples of prompts used at each step, refer to Appendix A.7.

## 3 Experiments and Findings

Through our experiments, we demonstrate the difficulty and quality of the MATH2 while also analyzing the behavior of different models on this task of _compositional generalization_. Firstly, we evaluate a wide range of models spanning a large range of parameter counts on MATH2 and compare against their performance on MATH (Hendrycks et al., 2021) which is the base dataset used for extracting skills, showing that the MATH2 is necessarily harder than MATH. Next, we further demonstrate the difficulty and quality of questions in MATH2 by showing that they are better in-context exemplars as compared to standardly used exemplars. We describe the experimental setup below.

### Experimental Setup

We follow the pipeline proposed in (Didolkar et al., 2024) to extract skills from the MATH dataset (Hendrycks et al., 2021). The MATH dataset encompasses seven high-level topics, allowing us to identify and extract finer-grained skills within each topic and label each question accordingly. At the end of the skill-extraction process, we identify a set of 114 skills. We then remove a few simple skills, such as basic_arithmetic and arithmetic_operations, before using the remaining set to generate questions using the proposed approach. We generate and verify 210 difficult questions to create the MATH2 dataset. Out of the 210 questions, 116 questions were generated using GPT-4 Turbo, 3 using GPT-4 Omni, 51 using Claude-3 Opus and 40 using Gemini-1.5-Pro. Figure 3 shows the distribution of skills in MATH2.

Table 2 presents details of the changes made to the questions during the human verification process. In total, 33.81% of the question-answer pairs in MATH2 appear exactly as phrased by their LLM creator.

We evaluate the generated set of questions on a variety of language models, both small and large. Specifically, we assess the MetaMath (Yu et al., 2023), MAMmoTH (Yue et al., 2023), Gemmma (Team et al., 2024), and Llama-3 series, Phi-3, deepseek-math as well as one Mixture-of-Experts model Mistral-8\(\)7B-Instruct. Additionally, we include evaluations of larger proprietary models such as GPT-4o, GPT-4 Turbo2 (OpenAI, 2023), Gemini-1.5-Pro, Claude 3.5

   Model & MATH2 (Y) & MATH (X) & \% Drop \\  GPT-4 Omni & 64.29\% & 77.21\% & **16.73\%** \\ Claude 3.5 Sonnet & 46.15\% & 73.54\% & 37.24\% \\ GPT-4 Turbo & 53.11\% & 73.27\% & 27.51\% \\ Gemini-1.5-Pro & 39.71\% & 67.70\% & 41.34\% \\ Claude 3 Opus & 37.14\% & 61.20 \% & 39.31\% \\ Llama-3.1-70B-Instruct & 50.48\% & 67.40\% & 25.10\% \\ Llama-3-70B-Instruct & 18.09\% & 47.89\% & 62.23\% \\ MetaMath-70B & 8.61\% & 26.27\% & 67.22\% \\ MAMmoTH-70B & 6.19\% & 19.31\% & 67.94\% \\ Mixtral-8\(\)7B-Instruct & 10.00\% & 31.52\% & 68.27\% \\ MetaMath-13B & 6.19\% & 21.32\% & 70.96\% \\ MAMmoTH-13B & 2.38\% & 10.99\% & 78.34\% \\ Deepseek-math-7b-instruct & 16.83\% & 45.05\% & 62.64\% \\ Llama-3.1-8B-Instruct & 28.09\% & 50.92\% & 44.83\% \\ Llama-3-8B-Instruct & 9.05\% & 28.62\% & 68.38\% \\ Gemma-1.1-7B-Instruct & 6.19\% & 23.36\% & 73.50\% \\ MetaMath-7B & 1.91\% & 18.69\% & 89.78\% \\ MAMmoTH-7B & 0.48\% & 7.90\% & **93.92\%** \\ Phi-3-mini-128k-instruct & 23.34\% & 48.29\% & 51.67\% \\ Gemma-1.1-2B-Instruct & 2.38\% & 7.52\% & 68.35\% \\   

Table 1: **Comparison of Zero-Shot CoT Performance (Accuracy) on the Generated Dataset vs. MATH Test Set**: GPT-4 Omni demonstrates the least drop in percentage terms (16.73%) whereas MAMmoTH-7B shows the highest relative drop (93.92%).

Sonnet 3 and Claude 3 Opus4. We compare the performances of these models on our generated questions against their performance on the MATH dataset [Hendrycks et al., 2021].

We observe that the performance of models on MATH\({}^{2}\) follows a quadratic relationship with the performance of models on MATH. We also demonstrate the high quality of the generated questions by showing that they act as superior in-context exemplars for MATH dataset. We refer the reader to Appendix A.4 for more discussion on these observations and for further implementation and compute details.

## 4 Conclusions

We introduced a framework that leverages the complementary strengths of humans and AI to generate new, challenging mathematics questions. Building on recent insights into LLM metaknowledge, we use LLMs to extract and name key skills necessary for solving math problems. Using these insights, we developed a pipeline that employs named skills from the well-known MATH dataset, and leverages multi-turn interactions with advanced LLMs to generate questions that combine pairs of skills. These questions were subsequently reviewed and refined by human raters. The proposed pipeline produced questions with greater novelty and difficulty compared to those in the original MATH dataset. This framework also resulted in a new math evaluation **MATH\({}^{2}\)**, that assesses the same skills as the MATH dataset but is significantly more challenging for leading models because each question involves two skills from different parts of MATH.

We plan to release detailed information about our pipeline to encourage further research and development in the field of open-source math models.

**Limitations and Future Work.** Our pipeline incurs moderately high costs due to extensive API-based use of frontier models as well as significant human verification. To improve efficiency, future work should focus on using open weights models and optimizing prompting strategies to produce higher-quality questions initially, thereby reducing the need for extensive filtering. Additionally, reducing human verification through the development of automated validation tools is crucial. This could include leveraging code generation and autorormalization capabilities of LLMs to generate responses which can be compiled using compilers or interpreters. Enhancing our pipeline by integrating a training-based feedback loop, where the model is trained on the questions that pass human verification, could further streamline the process by progressively improving question quality. These measures will reduce dependency on expensive proprietary models, lower overall operational costs, and maintain or even enhance the quality of the generated math evaluation benchmarks.

Looking ahead, an even more exciting prospect is the potential application of the proposed framework to efficiently produce high-quality data in domains beyond mathematics.

## 5 Acknowledgements

This research used compute resources provided by Mila (mila.quebec) and GPT4 access as well as compute resources provided by Princeton Language and Intelligence (PLI). The Princeton participants were funded by NSF, DARPA, and PLI. VS would like to thank Aniket Didolkar for helpful discussions throughout the project and for proof reading the paper. AG would like to thank Melvin Johnson, James McClelland and Yoram Bachrach for helpful discussions and useful feedback. AG would also like to thank Daan Wierstra, Melvin Johnson, Siamak Shakeri, Murray Shanahan, John Quan, Theophane Weber, Olivier Tieleman, David Silver, Charles Blundell, Behnam Neyshabur, Ethan Dyer and Nicolas Heess for support and guidance.