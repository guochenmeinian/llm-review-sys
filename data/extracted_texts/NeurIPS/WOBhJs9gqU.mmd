# Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss

Yifei Zhang

University of Chinese Academy of Sciences

zhangyifei21a@mails.ucas.ac.cn

&Huan-ang Gao

AIR, Tsinghua University

gha24@mails.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

AIR, Tsinghua University

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

AIR, Tsinghua University

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

AIR, Tsinghua University

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

AIR, Tsinghua University

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

AIR, Tsinghua University

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhou Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Institute of Technology

jzian@bit.edu.cn

&Hao Zhao

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhaohao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy of Artificial Intelligence

zhao@air.tsinghua.edu.cn

&Zhao Jiang

Beijing Academy ofin-domain data for optimal results. This demand poses a significant challenge to the AI4Science field, especially in 3D PTV where collecting suitable data is complicated due to the need for precisely selected tracer particles, tailored illumination, and camera settings . Additionally, certain scenarios like flow fields under unique geometric conditions or cytoplasmic flows in disease contexts are rare, making it nearly impossible to compile a comprehensive dataset.

To alleviate the aforementioned challenge, in this paper, we introduce a novel purely **self-supervised** framework with **test-time optimization** designed specifically for dual-frame fluid motion estimation in the 3D PTV process, as highlighted in the right panel of Fig. 1. Concerning the intrinsic difficulties associated with PTV data collection, especially in specialized contexts [6; 74; 64], we consider working under a limited size of dataset, as little as 1% typically used by existing fully-supervised methods (**notably without accessing labels**). Fluid particles have special physical properties, for which we resort to the inherent zero-divergence principle of incompressible fluid velocity fields and design a novel zero-divergence self-supervised loss tailored for fluid. Per implementation, we introduce the successful idea of splat in high-dimensional filtering  and random fields  and design a splat-based zero-divergence loss that is both efficient and effective.

Moreover, since our method is self-supervised, it naturally supports test-time optimization. Thus we introduce a module termed _Dynamic Velocimetry Enhancer_ (DVE), shown in the right panel of Fig. 1, which optimizes the initial predefined flow during test-time based on the specific input data on the fly, ensuring an improved level of accuracy across various testing scenarios. This is critical for **cross-domain robustness**. The difficulty in collecting diverse PTV data leads to the common practice of using synthetic datasets. However, since synthetic data is generated based on hand-crafted priors, it cannot accurately represent specific real-world distributions, resulting in models that lack the necessary cross-domain robustness for practical applications.

Through comprehensive experiments, we demonstrate that our purely self-supervised framework (right panel of Fig. 1) significantly outperforms its fully-supervised counterparts (left two panels of Fig. 1), even under data-constrained conditions (using as low as 1% data). Additionally, our cross-domain robustness analyses confirm the framework's intrinsic ability to generalize to unseen domains, including leave-one-out synthetic domains and real-world physical/biological domains, underscoring the practical utility of our approach for real-world 3D PTV applications.

To summarize, our main contributions are: 1. A novel self-supervised framework with test-time optimization for dual-frame fluid motion estimation, surpassing fully-supervised methods with minimal samples (as low as 1%). 2. A splat-based zero-divergence self-supervised loss for fluid dynamics, which is both efficient and effective. 3. A test-time optimization module named Dynamic Velocimetry Enhancer (DVE) that significantly improves cross-domain robustness.

Figure 1: **Paradigm Shift**: Given two frames of flow particles \(X_{t}\) and \(X_{t+ t}\), DeepPTV  adopts a two-stage network for large- and small-scale motion refinement. GotFlow3D  trains a correspondence learning network and an RNN-based residual prediction network. They are trained in a fully supervised manner with annotated data and do not support test-time optimization. Our purely self-supervised method diverges from these approaches and employs DVE (see Sec. 3.3) for on-the-fly test-time optimization. The ”Snowflake” denotes frozen weights.

Related Work

### Test-time Optimization and Test-time Domain Adaptation

Test-time optimization, also known as test-time refinement (TTR), exploits the inherent structure of data in a self-supervised manner without requiring ground truth labels [14; 80; 24; 21]. Applications of TTR include point cloud registration , depth estimation [7; 9], object recognition [80; 89], human motion capture , and segmentation with user feedback [73; 77; 31]. Test-time domain adaptation (TTA), a specific form of TTR, adapts a model trained on a source domain to a new target domain using an unsupervised loss function based on the target distribution [90; 54; 104]. One significant challenge in TTR is achieving per-sample adaptation at test time without compromising inference efficiency. Recent studies [85; 67] have explored using generative models to enable efficient test-time adaptation. In this work, we introduce our DVE module (Sec. 3.3), which conducts test-time optimization but maintains efficiency when compared with prior methods.

### Learning-based Scene Flow Estimation

We include this section because our research is closely related to point-based, learning-driven scene flow estimation from point clouds--a key component in understanding scenes through point clouds [8; 84; 43; 22]. Both areas of study concentrate on learning flows or correspondences from two frames of data [97; 102; 103; 100; 55]. Advances in scene flow estimation have been driven by benchmarks such as KITTI Scene Flow  and FlyingThings3D . Drawing from the related field of optical flow [15; 30; 79; 82], recent developments in scene flow estimation utilize methods including encoder-decoder architectures [23; 57], multi-scale representations [11; 44; 94], recurrent modules [36; 83; 92], and other strategies [42; 70].

**Self-supervision and Test-time Optimization for Scene Flow.** Self-supervised learning has received attention for scene flow estimation from point cloud data [95; 61; 5; 40; 46; 75; 45] and monocular images [29; 10; 105]. PointPwcNet  introduces cycle consistency loss, inspiring Mittal et al.  to incorporate it with nearest neighbor loss for establishing point cloud correspondence. This method also employs Chamfer Distance , smoothness constraints, and Laplacian regularization for self-supervision. SLIM  addresses self-supervised scene flow estimation and motion segmentation simultaneously. Flowstep3d  uses a soft point matching module for pairwise point correspondence. Self-supervision naturally supports test-time optimization. Pontes et al.  eschew model training for real-time optimization by minimizing the graph Laplacian over source points to enforce rigid flow. Li et al.  replace the explicit graph with a neural prior using a coordinate-based MLP to implicitly regularize the flow field. SCOOP  combines pre-training on a subset of data to learn soft correspondences and secures initial flows with optimization-based refinement steps.

Our work is distinct from these scene flow methods as our data source is a specific domain: flow particles. Fluid particles differ from typical scene flow point clouds due to their disordered local distribution  (see Sec. 3.2.1) and unique physical properties. We propose a graph-based feature extractor and zero-divergence regularization to leverage these properties (see Sec. 3.2).

### Particle Tracking Velocimetry (PTV)

Particle tracking is a fundamental tool in turbulence analysis, progressing from traditional methods like streak photography  to advanced techniques such as Laser Speckle Photography (LSV) . This evolution establishes the foundation for Particle Tracking Velocimetry (PTV). PTV gains prominence with the development of automatic tracking algorithms , which represents a significant advancement over manual methods . Modern PTV calculates velocities by matching particle pairs between frames  and has been applied widely across various fields such as materials science, hydrodynamics, biomedical research, and environmental science [28; 35; 101; 27].

**Deep Learning Methods of Dual-frame Fluid Motion Estimation in PTV.** Before the advent of deep learning, PTV algorithms primarily focused on improving particle matching by considering group particle movement , using multiple time step data , or conducting spatial area segmentation . With the onset of deep learning, deep neural networks have been designed for particle motion estimation from point cloud pairs [57; 50; 53; 70; 96; 92]. Among them, DeepPTV  and GotFlow3D  are specifically tailored for fluid flow learning and in a fully supervised manner, as demonstrated in Fig. 1. Our work follows these prior efforts, aiming to develop a data-efficient and cross-domain robust motion estimation technique through self-supervision and test-time optimization.

## 3 Methods

### Problem Formulation

To elucidate the architecture and functionality of our proposed method for dual-frame fluid motion estimation, we outline the problem as follows: The method processes two consecutive, unstructured sets of 3D particles, \(}^{n_{1} 3}\) and \(}^{n_{2} 3}\), recorded at times \(\) and \(+\). It outputs the predicted flow motion \(^{n_{1} 3}\), mapping each particle \(_{i}\) from \(}\) to a vector \(_{i}\) that indicates its movement between the two frames, capturing the flow dynamics in the turbulent 3D environment.

### Training with Fewer Samples

In the training phase, we aim to learn the patterns of fluid flow using considerably fewer samples, as low as 1% of what conventional approaches require, given the inherent difficulties in gathering data for specific scientific domains. We design the network as depicted at the top of Fig. 2 to train a graph-based feature extractor (Fig. 2b) that extracts per-point features for the following soft point matching. These features initialize the flow between the point clouds using the optimal transport module (Fig. 2c), and we employ self-supervision losses, as shown in Fig. 2(e,f,g), for training. However, fluid particles exhibit complex motion features compared to typical LiDAR point clouds (Sec. 3.2.1), which complicates feature learning under self-supervision with limited data. Consequently, we employ a strong graph-based feature extractor (Sec. 3.2.2) and propose a novel zero-divergence loss (Sec. 3.2.4.3.1) tailored to address these challenges.

#### 3.2.1 Complexity of Fluid Flow

A common assumption in LiDAR scene flow estimation is the smoothness of flow. However, this is not enough for fluid particles due to their unique geometric distribution, as shown in the left of Fig. 3. The fluid velocity field is smooth only at a coarse scale but remains complex at a fine local scale. Therefore, we need a strong relation-based graph feature extractor and more specific regularization to capture the intricate properties of fluid particles.

#### 3.2.2 Graph-based Feature Extractor

Point cloud-based extractors, including PointNet  and PointNet++ , are commonly used in LiDAR scene flow estimation . While these extractors effectively discern broader spatial structures, their capability to grasp intricate local relationships, which is vital for analyzing fluid dynamics, can be inadequate. In contrast, graph-based feature

Figure 2: **Upper: Training Phase**. First, we (a) use input point clouds to construct graphs, which are then passed through a trainable (b) feature extractor, and we solve a (c) optimal transport problem using self-supervised loss terms including (g) reconstruction loss, (f) smooth loss, and (e) zero-divergence loss for initial flow estimation. **Lower: (h) Test-Time DVE**. With the initial flow estimate \(_{}\), we optimize a residual \(\) to generate the final flow \(\) using another reconstruction loss (g*).

extractors excel at capturing local patterns by considering the relationships between proximate nodes, or in our context, particles. Hence, drawing inspiration from GotFlow3D , we opt for a graph-based feature extraction backbone, as depicted in Fig. 2b. Initially, we construct a static nearest-neighbor graph from the input point cloud. This graph is then processed through several GeoSetConv layers  to form a high-dimensional geometric local feature. To further enrich the feature, we construct a dynamic graph using EdgeConv  based on the high-dimensional feature, forming a GNN that outputs static-dynamic features. The dynamic graph expands the receptive field and focuses on geometric feature properties. Further details can be found in Appendix A.1.1.

**3.2.3 Solving Optimal Transport for Soft Correspondence.** With the static-dynamic feature from the feature extractor (Sec. 3.2.2), we formulate the correspondence linking problem through the framework of optimal transport , where a higher transport cost between two points indicates a lower similarity within the extracted feature space. The optimal transport plan yields the soft correspondence weight between \(}\) and \(}\), as shown in Fig. 2c, which can be used to formulate an initial flow estimate \(_{}\). This follows the common scene flow method, thus we leave the details to Appendix A.2.

**3.2.4 Self-supervised Losses.** Since manually linking particles between sets is notably intricate, we advocate for the adoption of self-supervised losses (Fig. 2e-g).

_3.2.4.1. Reconstruction Loss._ A core principle guiding self-supervised flow learning is the fact that \(}+\) and \(}\) should be similar. The Chamfer distance (CD) is a standard metric used to measure the shape dissimilarity between point clouds in point cloud completion. Therefore, we adopt it as our reconstruction loss. We also add a regularization term  to prevent degeneration:

\[L_{}=^{}|}_{^{}_{i }^{}}p_{i}_{_{j}}|| ^{}_{i}-_{j}||_{2}^{2}+_{}^{}|}_{^{}_{i}^{}}(1 -p_{i})\] (1)

Here, \(^{}\) represents the estimated point cloud formed by \(}+_{}\), and \(\) is the target point cloud formed by \(}\). \(p_{i}\) denotes the confidence of matching in the Optimal Transport (Sec. 3.2.3), which is the weighted sum of transport costs. \(_{}\) term is used to avoid the trivial solution \(p_{i}=0\).

_3.2.4.2. Smooth Loss._ Given the infinitely differentiable characteristic of the velocity field, it is postulated that the field should exhibit a certain level of continuous and smooth transitions (at a coarse scale). In light of this theoretical underpinning, we introduce a smooth regularization loss to enforce and maintain this continuous behavior in the velocity field, which is defined as,

\[L_{}=_{_{i}}_{k_{ i}(_{i})}_{i}-_{k}||_{1}}{| ||(_{i})|},\] (2)

Here, \(\) represents the point cloud formed by \(}\). \(_{i}(_{i})\) represents the index set of the \(l\) closest points to \(_{i}\). \(_{i}\) and \(_{k}\) denote the estimated flow vectors at points \(_{i}\) and \(_{k}\), respectively.

3.2.4.3.1 Zero-divergence Loss. Smooth Loss is not enough for fluid particles, as mentioned in Sec. 3.2.1. Concerning the intrinsic properties of the velocity field, we note that incompressible fluids exhibit zero divergence by definition. Moreover, compressible fluids can also be approximated as incompressible under conditions like low Mach numbers, justifying this in many engineering contexts. Hence, we introduce a zero-divergence regularization, which also compensates for the shortcomings of Smooth Loss, as we will show later.

3.2.4.3.2 Splat-based Implementation. Splatting, first used in high-dimensional Gaussian filtering , embeds input values in a high-dimensional space. Studies like  and  followed this

Figure 3: **LEFT: A visualization of fluid flow in Fluidflow3D data. **RIGHT:** The divergence loss in our training phase is obtained by splatting the original sparse flow to grid points and then minimizing the divergence loss on the resulting grid points.

Splat-Blur-Slice pipeline. Inspired by these, we implemented a splat-based zero-divergence loss: to compute divergence, we need the partial derivative of the field. The irregular arrangement of particles in 3D complicates this. Thus, we propose "splatting" unstructured flow estimates onto a uniform 3D grid, then applying zero-divergence regularization at these grid points, as shown in the right of Fig. 3. In formal terms, the dense grid is denoted by \((sj,sk,sl)^{T}\), with \(j,k,l\) indicating the 3D indices of the grid point. The parameter \(s\) corresponds to the grid's spacing. Then, given a grid point \(=(sj,sk,sl)^{T}\), we employ the inverse squared distance as interpolation weights to approximate the flow at that particular point,

\[()=)|}_{_{i} N( )}_{i}}{\|_{i}-\|_{2}^{2}+}\] (3)

where \(_{i}\) is the estimated flow value at point \(_{i}\). \(N()\) denotes the neighborhood among the point set of \(_{}\) for grid point \(\). The parameter \(\) is introduced to maintain numerical stability. By employing splatting, we convert the variable particle distance into fixed grid spacing, thus achieving efficiency and effectiveness.

3.2.4.3.3 Divergence Calculation. Once Splatting has been employed, the divergence at that point, specified by \(=(sj,sk,sl)^{T}\), can be defined as: \(()()=_{k=1}^{3}( +su_{k})-(-su_{k})}{2s}\), where \(u_{k}\) is a unit vector with \(1\) at the \(k\)-th entry. Finally, the zero-divergence regularization can be formulated as, \(L_{}=_{j=0}^{J-1}_{k=0}^{K-1}_{l=0}^{L-1} \|()((sj,sk,sl)^{T})\|_{1},\) where \(J\), \(K\), and \(L\) represent the number of grid points along the respective dimensions.

3.2.4.3.4 Zero-Divergence Loss v.s. Smooth Loss Zero-Divergence loss is similar to Smooth loss in that it computes spatial gradients and requires the norm of the gradient to be small, essentially penalizing the case that neighboring flow vectors are totally irrelevant. However, Smooth regularization is too strict for fluid particles. While the divergence constraint only requires the total divergence to be zero, it does not necessitate that any two vectors be oriented in the same direction, thus allowing for locally complex particle dynamics. In practice, we set the neighborhood set size for calculating Smooth Loss to be much larger than that for calculating Zero-Divergence Loss, because smoothness is a more coarse-scale regularization. Finally, we note that Zero-Divergence Loss is calculated along three specific axes, whereas Smooth Loss is not. Therefore, using the same method (KNN) for calculating Zero-Divergence Loss as for Smooth Loss is not efficient.

To summarize, our final self-supervised training loss is

\[L_{}=L_{}+_{}L_{}+ _{}L_{}\]

### Efficient Test-time Optimization with Dynamic Velocimetry Enhancer

As shown at the bottom of Fig. 2, with the initial flow estimate from the trained network, we introduce a novel _Dynamic Velocimetry Enhancer_ (DVE) module during the test phase for test-time optimization. This provides added flexibility to accommodate unseen situations and address potential inaccuracies arising from the limited training data context, which will be demonstrated in Sec. 4.4. In principle, our approach seeks a residual flow vector \(\) such that \(=_{}+\), which can be optimized to rectify the inaccuracies. Formally, DVE is essentially an optimization process using the \(L_{}\) objective function, with the formulation as follows:

\[^{*}=^{|^{}| 3 }}{}(^{}|}_{_{i}^{ }^{}}p_{i}_{_{j}}\| _{i}^{}+_{i}-_{j}\|_{2}^{2})\] (4)

This test-time supervision (Fig. 2(g*)) is similar to \(L_{}1\) without the regularization \(_{}\). Solved using an Adam optimizer, it only involves parameters from an \(n_{1} 3\) matrix. Concerning that existing test-time optimization modules  are slow, DVE is very efficient, as demonstrated later in Sec. 4.1.

**Selection of Self-supervised Losses During the Test Phase** We omit both \(L_{}\) and \(L_{}\) during the test stage. During the training phase, our objective is to embed prior knowledge about particle flow into the network. \(L_{}\) and \(L_{}\) serve not only to foster a comprehensive understanding of fluid behaviors but also function as regularizers, mitigating overfitting caused by the unconstrained reconstruction loss. However, in the test phase, our focus shifts to specific sparse particle sets.

In certain scenarios, such as when flows adhere to boundary conditions, these particles may not strictly adhere to the expected norms of ideal smoothness or zero-divergence typical in a flow field. Additionally, given that the initial flow estimate should be sufficiently accurate, regularizers become unnecessary. Our approach to customizing the loss functions in this manner aims to enhance the robustness of our model against the complex challenges encountered in real-world applications, **thereby improving data efficiency and cross-domain robustness.**

## 4 Experiments

We conduct comprehensive evaluations using different data domains on our proposed framework. First, we compare our method with SOTA fully supervised methods (Sec. 4.1). Next, we examine its performance under the constrained size of training data, reflecting real-world situations where domain-specific data is limited (Sec. 4.2). We then assess the framework's performance under different domains with increasing domain shift, highlighting its cross-domain robustness (Sec. 4.3). Additionally, we conduct comprehensive ablation studies on the components of our framework (Sec. 4.4) to validate their effects. Following the previous SOTA method GotFlow3D , our datasets include FluidFlow3D  and its six fluid cases, DeformationFlow  and AVIC . **Due to page limit, experimental settings including implementation details, datasets, and evaluation metrics can be found in Appendix A.3.**

### Comparison with state-of-the-art methods

Since DeepPTV is not open-sourced, we enrich the comparison by including scene-flow methods [56; 70; 96; 92] as our baselines. We benchmark our method against established fully supervised models, such as FlowNet3D , FLOT , PointPWC-Net , PV-RAFT , and GotFlow3D , all utilizing the FluidFlow3D training set. All baseline models are evaluated using the default hyperparameters. As shown in Figure 4, our purely self-supervised approach outperforms all the fully supervised baselines. Additionally, we introduce _Ours (1%)_--our method trained on just 1% of the data--which still demonstrates comparable performance.

**Comparison Across Flow Cases:** We further analyze our framework's performance across six distinct flow cases from the FluidFlow3D dataset, with details available in Appendix A.3.1. For three representative cases--Uniform Flow, Turbulent Channel Flow, and Forced MHD Turbulence--we present detailed results in Fig. 4, while the remaining are documented in Appendix A.4.1. In the simple Uniform Flow case, our method shows slight improvement over the baseline. However, in more complex scenarios, such as Forced MHD Turbulence, our method significantly outperforms the baseline, reducing the EPE/NPE metric by nearly half compared to the SOTA GotFlow3D.

**Test-time Efficiency:** In Figure 4, the \(T_{}\) column in the table illustrates the time consumption of each method during the test phase. Our method demonstrates time efficiency, incurring less inference time cost than even baseline supervised methods (without test-time optimization). Our method requires only a few epochs to converge (See Appendix A.5.3). Furthermore, our network's relatively small size (refer to \(P_{}\) comparison in Figure 4) facilitates a rapid forward pass. Time profiling is conducted on a single RTX 3090 Ti.

### Training with Limited Data

Handling rare scenarios, such as unique geometric flow fields  or cytoplasmic flows in disease contexts, presents challenges in assembling large datasets. To address this, we explore an evaluation setup with limited training data (still without labels) by randomly sampling from the FluidFlow3D training dataset. We established three distinct training settings: a 100% sampling rate (13,621 samples), a 10% sampling rate (1,300 samples), and a 1% sampling rate (130 samples). Testing was conducted on the FluidFlow3D test data (see Appendix A.3.1). We compared our method with FLOT , PV-RAFT , and the current SOTA GotFlow3D . We present the results of the major metric, EPE, with further details in Appendix A.4.2. Fig. 5(a) illustrates the robustness of our approach to reductions in training data size. Our metrics remain stable even with significant decreases in training samples, while other methods show substantial performance declines. This disparity becomes more pronounced in complex cases, as discussed below.

Performance Drop Across Flow Cases:We further tested our method with limited training data on different flow cases from the FluidFlow3D test data mentioned above. The performance drop associated with limited data is shown in Fig. 5(b). As illustrated, complex flow cases such as Forced Isotropic Turbulence are more susceptible to limited data, while simpler flow cases like Uniform Flow and Turbulent Channel Flow maintain stable EPE as the training data decreases.

### Analysis of Robustness Across Different Domains

Natural fluids exhibit a range of behaviors, including convection and laminar flow. Gathering data under all possible conditions presents significant challenges. To address this, we examine the cross-domain knowledge transfer capability of our proposed method. We explore a gradual increase in domain shift: initially, we investigate fluid case domain shifts (Sec. 4.3.1), where we train on five

Figure 4: (Top) **Benchmarking Against Fully Supervised Methods**. \(P_{}\) signifies the count of trainable parameters. \(T_{}\) stands for inference time for each sample. The best results are marked in bold. (Bottom) **Performance Across Flow Cases**. The best results are marked in bold, with the runners-up underlined. The subplots on the right visualize these three cases. The warmer color indicates a higher flow speed. All models are trained on full data, except Ours (1%).

certain fluid cases and test on the leave-one-out domain within the same dataset. Next, we examine the Sim2Real domain shift (Sec. 4.3.2), where training occurs on a synthetic fluid dataset and testing on real-world fluid data. Lastly, we assess a more extensive Sim2Real domain shift (Sec. 4.3.3) by training on synthetic physical fluid data and testing on biological datasets.

#### 4.3.1 Testing within the Same Synthetic Fluid Dataset:

In this section, we employ the complete FluidFlow3D training set in a six-fold cross-validation setup, training on five sub-cases and testing on the remaining one. We benchmark our method solely against the state-of-the-art fluid motion learning method, GotFlow3D , as other baselines are not fluid-specific. The EPE metric results, shown in Fig.5(c) (with additional results in Appendix A.4.3), indicate that our method outperforms GotFlow3D in various scenarios, especially in complex conditions like MHD and isotropic turbulence. Moreover, our method shows consistent performance, highlighting its robustness in unfamiliar scenarios.

_Sim2Real Experimental Setting:_ Synthetic data with ground-truth labels often serves as a benchmark for method evaluation. However, the domain gap between synthetic and real data can negatively impact performance, underscoring the importance of validation on real-world datasets. In this and the following section, we validate our method using two real-world datasets, DeformationFlow  and Aortic Valve Interstitial Cell (AVIC)  (details in Appendix A.3.1), to demonstrate its practical application potential. Two challenges in real-world evaluation are: 1) the lack of ground-truth labels in real-world data, and 2) the requirement for a complete PTV method for particle tracking application. Therefore, we use the following setting: our method, trained on FluidFlow3D, is integrated into PTV algorithms (specified below) to provide initial motion estimates. Due to the absence of ground truth, we primarily demonstrate the generalizability of our method across various domains through qualitative results. Quantitatively, we emphasize efficiency in the physical domain, where multiple frames are involved. By contrast, in the biological domain, we focus on validating the plausibility of our estimates, especially given the significant cell deformation, where efficiency is less critical.

Figure 5: (a) Leave-one-out domain EPE Comparison: “Flow Cases” stands for the flow case we test on with the model trained on the rest five cases. (b) Comparison of EPE with Limited Training Data. (c) Performance Drop related to Limited Training Data. The Y-axis shows the major matric EPE, and the X-axis indicates the percentage of the training dataset utilized.

Figure 6: (a) DeformationFlow data. (b) Initial estimation by our method. (c) Time-consumption comparison between _SerialTrack_ and _Ours+ST_. “PerIt” denotes time per PTV iteration.

#### 4.3.2 Testing from Synthetic to Real-World Fluid Data:

We employ SerialTrack (ST)  as the PTV framework and designate our integrated version as _Ours+ST_. Quantitative results highlight the advantages of our method: it identifies 22,882 matches, exceeding the 22,001 particles tracked by vanilla SerialTrack, and significantly reduces tracking time by providing accurate initial estimates that expedite match finding. Results in Fig. 6 showcase the initial estimations and time efficiency of _Ours+ST_. These outcomes affirm that our method delivers sufficiently precise estimations to improve PTV, demonstrating strong simulation-to-reality (Sim2Real) capabilities.

#### 4.3.3 Testing from Synthetic Physical Fluids to Biological Data:

This study analyzes a dataset of AVIC images embedded in a PEG hydrogel, using microspheres to track hydrogel movements. AVICs were subjected to three conditions: regular, Cyto-D exposure, and Endo 1 treatment. Cell deformation, challenging to observe directly, was quantified by tracking nearby particles with _Farmac_, using our framework for initialization. Results are presented in Tab.1, and a visualization of the cell deformation we estimate is in AppendixA.4.4. We evaluate particle movement using the neighbor distance score (See Appendix A.3.2), with higher scores indicating less accurate estimations. Our results in Tab. 1 show _Ours+Fm_ slightly outperforming _Fm-track_. Notably, our method, trained exclusively on the FluidFlow3D Dataset, demonstrates strong adaptability across domains and provides insights into biological fluid dynamics.

### Ablation study on different modules.

In addition to the aforementioned assessments, an ablation study regarding different proposed modules including different feature extractors, Zero-divergence Loss, and DVE is performed to demonstrate their effectiveness. We show our method w/ and w/o Div Loss (Zero-Divergence Loss) and DVE module in Fig. 4. The full results are illustrated in Appendix A.5.

## 5 Conclusion

In this paper, we introduce a test-time self-supervised framework for learning 3D fluid motion from dual-frame unstructured particle sets. We address the challenge of improving data efficiency and ensuring cross-domain robustness, which are crucial for practical applications. We demonstrate the viability of our approach through two real-world studies and suggest that our findings could inform further research into extensive real-world applications, the exploration of constraints specific to particular scenarios, and the development of novel model architectures for enhanced adaptability.

   &  &  \\   & C2E & C2N & E2N & C2E & C2N & E2N \\  Fm-track & 8040 & 7744 & 8097 & 0.358 & 0.283 & 0.399 \\ Ours+Fm & 8048 & 7750 & 8097 & 0.357 & 0.246 & 0.359 \\  

Table 1: Comparison on AVIC data. C2E, C2N, E2N stands for 3 settings: Cyto-D treatment to Endo-1 treatment, Cyto-D treatment to Normal and Endo-1 treatment to Normal. MNDS stands for the mean neighbor distance score.