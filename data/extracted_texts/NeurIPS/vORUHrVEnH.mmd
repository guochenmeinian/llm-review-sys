# Going Beyond Linear Mode Connectivity:

The Layerwise Linear Feature Connectivity

 Zhanpeng Zhou\({}^{1}\), Yongyi Yang\({}^{2}\), Xiaojiang Yang\({}^{1}\), Junchi Yan\({}^{1}\)1, Wei Hu\({}^{2}\)2

\({}^{1}\) Dept. of Computer Science and Engineering & MoE Key Lab of AI, Shanghai Jiao Tong University

\({}^{2}\) Dept. of Electrical Engineering & Computer Science, University of Michigan

{zzp1012,yangxiaojiang,yanjunchi}@sjtu.edu.cn

{yongyi,vvb}@umich.edu

###### Abstract

Recent work has revealed many intriguing empirical phenomena in neural network training, despite the poorly understood and highly complex loss landscapes and training dynamics. One of these phenomena, Linear Mode Connectivity (LMC), has gained considerable attention due to the intriguing observation that different solutions can be connected by a linear path in the parameter space while maintaining near-constant training and test losses. In this work, we introduce a stronger notion of linear connectivity, _Layerwise Linear Feature Connectivity (LLFC)_, which says that the feature maps of every layer in different trained networks are also linearly connected. We provide comprehensive empirical evidence for LLFC across a wide range of settings, demonstrating that whenever two trained networks satisfy LMC (via either spawning or permutation methods), they also satisfy LLFC in nearly all the layers. Furthermore, we delve deeper into the underlying factors contributing to LLFC, which reveal new insights into the permutation approaches. The study of LLFC transcends and advances our understanding of LMC by adopting a feature-learning perspective. We released our source code at https://github.com/zzp1012/LLFC.

## 1 Introduction

Despite the successes of modern deep neural networks, theoretical understanding of them still lags behind. Efforts to understand the mechanisms behind deep learning have led to significant interest in exploring the loss landscapes and training dynamics. While the loss functions used in deep learning are often regarded as complex black-box functions in high dimensions, it is believed that these functions, particularly the parts encountered in practical training trajectories, contain intricate benign structures that play a role in facilitating the effectiveness of gradient-based training . Just like in many other scientific disciplines, a crucial step toward formulating a comprehensive theory of deep learning lies in meticulous empirical investigations of the learning pipeline, intending to uncover quantitative and reproducible nontrivial phenomena that shed light on the underlying mechanisms.

One intriguing phenomenon discovered in recent work is Mode Connectivity : Different optima found by independent runs of gradient-based optimization are connected by a simple path in the parameter space, on which the loss or accuracy is nearly constant. This is surprising as different optima of a non-convex function can very well reside in different and isolated "valleys" and yet this does not happen for optima found in practice. More recently, an even stronger form of modeconnectivity called Linear Mode Connectivity (LMC) was discovered . It depicts that different optima can be connected by a _linear_ path of constant loss/accuracy. Although LMC typically does not happen for two independently trained networks, it has been consistently observed in the following scenarios:

* **Spawning**: A network is randomly initialized, trained for a small number of epochs (e.g. 5 epochs for both ResNet-20 and VGG-16 on the CIFAR-10 dataset), and then spawned into two copies which continue to be independently trained using different SGD randomnesses (i.e., for mini-batch order and data augmentation).
* **Permutation**: Two networks are independently trained, and the neurons of one model are permuted to match the neurons of the other model while maintaining a functionally equivalent network.

The study of LMC is highly motivated due to its ability to unveil nontrivial structural properties of loss landscapes and training dynamics. Furthermore, LMC has significant relevance to various practical applications, such as pruning and weight-averaging methods.

On the other hand, the success of deep neural networks is related to their ability to learn useful _features_, or _representations_, of the data , and recent work has highlighted the importance of analyzing not only the final outputs of a network but also its intermediate features . However, this crucial perspective is absent in the existing literature on LMC and weight averaging. These studies typically focus on interpolating the weights of different models and examining the final loss and accuracy, without delving into the internal layers of the network.

In this work, we take a feature-learning perspective on LMC and pose the question: what happens to the internal features when we linearly interpolate the weights of two trained networks? Our main discovery, referred to as _Layerwise Linear Feature Connectivity (LLFC)_, is that the features in almost all the layers also satisfy a strong form of linear connectivity: the feature map in the weight-interpolated network is approximately the same as the linear interpolation of the feature maps in the two original networks. More precisely, let \(_{A}\) and \(_{B}\) be the weights of two trained networks, and let \(f^{()}()\) be the feature map in the \(\)-th layer of the network with weights \(\). Then we say that \(_{A}\) and \(_{B}\) satisfy LLFC if

\[f^{()}(_{A}+(1-)_{B}) f^{ ()}(_{A})+(1-)f^{()}(_{B}), ,.\] (1)

See Figure 1 for an illustration. While LLFC certainly cannot hold for two arbitrary \(_{A}\) and \(_{B}\), we find that it is satisfied whenever \(_{A}\) and \(_{B}\) satisfy LMC. We confirm this across a wide range of settings, as well as for both spawning and permutation methods that give rise to LMC.

LLFC is a much finer-grained characterization of linearity than LMC. While LMC only concerns loss or accuracy, which is a single scalar value, LLFC (1) establishes a relation for all intermediate feature maps, which are high-dimensional objects. Furthermore, it is not difficult to see that LLFC applied to the output layer implies LMC when the two networks have small errors (see Lemma 1); hence, LLFC can be viewed as a strictly stronger property than LMC. The consistent co-occurrence of LLFC and LMC suggests that studying LLFC may play a crucial role in enhancing our understanding of LMC.

Figure 1: Illustration of Layerwise Linear Feature Connectivity (LLFC). Each tensor comprises three feature maps when provided with three input images.

Subsequently, we delve deeper into the underlying factors contributing to LLFC. We identify two critical conditions, _weak additivity for ReLU function_ and a _commutativity property_ between two trained networks. We prove that these two conditions collectively imply LLFC in ReLU networks, and provide empirical verification of these conditions. Furthermore, our investigation yields novel insights into the permutation approaches: we interpret both the activation matching and weight matching objectives in Git Re-Basin  as ways to ensure the satisfaction of commutativity property.

In summary, our work unveils a richer set of phenomena that go significantly beyond the scope of LMC, and our further investigation provides valuable insights into the underlying mechanism behind LMC. Our results demonstrate the value of opening the black box of neural networks and taking a feature-centric viewpoint in studying questions related to loss landscapes and training dynamics.

## 2 Related Work

**(Linear) Mode Connectivity.** Freeman and Bruna , Draxler et al. , Garipov et al.  observed Mode Connectivity, i.e., different optima/modes of the loss function can be connected through a non-linear path with nearly constant loss. Nagarajan and Kolter  first observed Linear Mode Connectivity (LMC), i.e., the near-constant-loss connecting path can be linear, on models trained on MNIST starting from the same random initialization. Later, Frankle et al.  first formally defined and thoroughly investigate the LMC problem. Frankle et al.  observed LMC on harder datasets, for networks that are jointly trained for a short amount of time before going through independent training (we refer to this as the spawning method). Frankle et al.  also demonstrated a connection between LMC and the Lottery Ticket Hypothesis . Fort et al.  used the spawning method to explore the connection between LMC and the Neural Tangent Kernel dynamics. Lubana et al.  studied the mechanisms of DNNs from mode connectivity and verified the mechanistically dissimilar models cannot be linearly connected. Yunis et al.  showed that LMC also extends beyond two optima and identified a high-dimensional convex hull of low loss between multiple optima. On the theory side, several papers [10; 21; 32; 26; 25; 16] were able to prove non-linear mode connectivity under various settings, but there has not been a theoretical explanation of LMC to our knowledge.

**Permutation Invariance.** Neural network architectures contain permutation symmetries : one can permute the weights in different layers while not changing the function computed by the network. Ashmore and Gashler  utilized the permutation invariance of DNNs to align the topological structure of two neural networks. Tatro et al.  used permutation invariance to align the neurons of two neural networks, resulting in improved non-linear mode connectivity. In the context of LMC, Entezari et al. , Ainsworth et al.  showed that even independently trained networks can be linearly connected when permutation invariance is taken into account. In particular, Ainsworth et al.  approached the neuron alignment problem by formulating it as bipartite graph matching and proposed two matching methods: activation matching and weight matching. Notably, Ainsworth et al.  achieved the LMC between independently trained ResNet models on the CIFAR-10 dataset using weight matching.

**Model Averaging Methods.** LMC also has direct implications for model averaging methods, which are further related to federated learning and ensemble methods. Wang et al.  introduced a novel federated learning algorithm that incorporates unit permutation before model averaging. Singh and Jaggi , Liu et al.  approached the neuron alignment problem in model averaging by formulating it as an optimal transport and graph matching problem, respectively. Wortsman et al.  averaged the weights of multiple fine-tuned models trained with different hyper-parameters and obtained improved performance.

## 3 Background and Preliminaries

**Notation and Setup.** Denote \([k]=\{1,2,,k\}\). We consider a classification dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\), where \(_{i}^{d_{0}}\) represents the input and \(y_{i}[c]\) represents the label of the \(i\)-th data point. Here, \(n\) is the dataset size, \(d_{0}\) is the input dimension and \(c\) is the number of classes. Moreover, we use \(^{d_{0} n}\) to stack all the input data into a matrix.

We consider an \(L\)-layer neural network of the form \(f(;)\), where \(\) represents the model parameters, \(\) is the input, and \(f(;)^{c}\). Let the \(\)-th layer feature (post-activation) of the network be \(f^{()}(;)^{d_{}}\), where \(d_{}\) is the dimension of the \(\)-th layer (\(0 L\)) and \(d_{L}=c\). Note that \(f^{(0)}(;)=\) and \(f^{(L)}(;)=f(;)\). For an input data matrix \(\), we also use \(f(;)^{c n}\) and \(f^{()}(;)^{d_{} n}\) to denote the collection of the network outputs and features on all the datapoints, respectively. When \(\) is clear from the context, we simply write \(f^{()}()=f^{()}(;)\) and \(f()=f(;)\). Unless otherwise specified, in this paper we consider models trained on a training set, and then all the investigations are evaluated on a test set.

We use \(_{}()\) to denote the classification error of the network \(f(;)\) on the dataset \(\).

**Linear Mode Connectivity (LMC).** We recall the notion of LMC in Definition 1.

**Definition 1** (**Linear Mode Connectivity**).: _Given a test dataset \(\) and two modes3\(_{A}\) and \(_{B}\) such that \(_{}(_{A})_{ }(_{B})\), we say \(_{A}\) and \(_{B}\) are linearly connected if they satisfy_

\[_{}(_{A}+(1-) _{B})_{}(_{A}), .\] (2)

As Definition 1 shows, \(_{A}\) and \(_{B}\) satisfy LMC if the error metric on the linear path connecting their weights is nearly constant. There are two known methods to obtain linearly connected modes, the _spawning method_[9; 7] and the _permutation method_[6; 1].

**Spawning Method.** We start from random initialization \(^{(0)}\) and train the model for \(k\) steps to obtain \(^{(k)}\). Then we create two copies of \(^{(k)}\) and continue training the two models separately using independent SGD randomnesses (mini-batch order and data augmentations) until convergence. By selecting a proper value of \(k\) (usually a small fraction of the total training steps), we can obtain two linearly connected modes.

**Permutation Method.** Due to permutation symmetry, it is possible to permute the weights in a neural network appropriately while not changing the function being computed. Given two modes \(_{A}\) and \(_{B}\) which are independently trained (and not linearly connected), the permutation method aims to find a permutation \(\) such that the permuted mode \(^{}_{B}=(_{B})\) is functionally equivalent to \(_{B}\) and that \(^{}_{B}\) and \(_{A}\) are linearly connected. In other words, even if two modes are not linearly connected in the parameter space, they might still be linearly connected if permutation invariance is taken into account.

Among existing permutation methods, Git Re-Basin  is a representative one, which successfully achieved linear connectivity between two independently trained ResNet models on CIFAR-10. Specifically, a permutation \(\) that maintains functionally equivalent network can be formulated by a set of per-layer permutations \(=\{^{()}\}_{=1}^{L-1}\) where \(^{()}^{d_{} d_{}}\) is a permutation matrix. Ainsworth et al.  proposed two distinct matching objectives for aligning the neurons of independently trained models via permutation: _weight matching_ and _activation matching_:

\[_{}\|_{A}-(_{B})\|^{2}.\] (3)

\[_{}_{=1}^{L-1}\|^{( )}_{A}-^{()}^{()}_{B}\|_{F}^{2}.\] (4)

Here, \(^{()}_{A}=f^{()}(_{A};)\) is the \(\)-th layer feature matrix, and \(^{()}_{B}\) is defined similarly.

**Main Experimental Setup.** In this paper, we use both the spawning method and the permutation method to obtain linearly connected modes. Following Frankle et al. , Ainsworth et al. , we perform our experiments on commonly used image classification datasets MNIST , CIFAR-10 , and Tiny-ImageNet, and with the standard network architectures ResNet-20/50 , VGG-16 , and MLP. We follow the same training procedures and hyper-parameters as in Frankle et al. , Ainsworth et al. . Due to space limit, we defer some of the experimental results to the appendix. Notice that  increased the width of ResNet by 32 times in order to achieve zero barrier, and we also followed this setting in the experiments of the permutation method. The detailed settings and hyper-parameters are also described in Appendix B.1.

## 4 Layerwise Linear Feature Connectivity (LLFC)

In this section, we formally describe Layerwise Linear Feature Connectivity (LLFC) and provide empirical evidence of its consistent co-occurrence with LMC. We also show that LLFC applied to the last layer directly implies LMC.

**Definition 2** (**Layerwise Linear Feature Connectivity**).: _Given dataset \(\) and two modes \(_{A}\), \(_{B}\) of an \(L\)-layer neural network \(f\), the modes \(_{A}\) and \(_{B}\) are said to be layerwise linearly feature connected if they satisfy_

\[[L],, c>0,cf^{()}( _{A}+(1-)_{B})= f^{()}(_{A})+(1-)f^{()}(_{B}).\] (5)

LLFC states that the per-layer feature of the interpolated model \(_{}=_{A}+(1-)_{B}\) has the same direction as the linear interpolation of the features of \(_{A}\) and \(_{B}\). This means that the feature map \(f^{()}\) behaves similarly to a linear map (up to a scaling factor) on the line segment between \(_{A}\) and \(_{B}\), even though it is a nonlinear map globally.

**LLFC Co-occurs with LMC.** We now verify that LLFC consistently co-occurs with LMC across different architectures and datasets. We use the spawning method and the permutation method described in Section 3 to obtain linearly connected modes \(_{A}\) and \(_{B}\). On each data point \(_{i}\) in the test set \(\), we measure the cosine similarity between the feature of the interpolated model \(_{}\) and

Figure 3: Comparison of \(_{}[1-_{}(_{i})]\) and \(_{}[1-_{A,B}(_{i})]\). The permutation method is used to obtain two linearly connected modes \(_{A}\) and \(_{B}\). Results are presented for different layers of various model architectures on different datasets, with \(\{0.25,0.5,0.75\}\). Standard deviations across the dataset are reported by error bars. More results are in Appendix B.2.

linear interpolations of the features of \(_{A}\) and \(_{B}\) in each layer \(\), as expressed as \(_{}(_{i})=[f^{()}(_{A}+(1-)_{B};_{i}), f^{()}(_{A};_{i})+(1- )f^{()}(_{B};_{i})]\). We compare this to the baseline cosine similarity between the features of \(_{A}\) and \(_{B}\) in the corresponding layer, namely \(_{A,B}(_{i})=[f^{()}(_{A};_{i}),f^{( )}(_{B};_{i})]\). The results for the spawning method and the permutation method are presented in Figures 2 and 3, respectively. They show that the values of \(_{}[1-_{}(_{i})]\) are close to \(0\) across different layers, architectures, datasets, and different values of \(\), which verifies the LLFC property. The presence of small error bars indicates consistent behavior for each data point. Moreover, the values of \(_{}[1-_{A,B}(_{i})]\) are not close to \(0\), which rules out the trivial case that \(f^{()}(_{A})\) and \(f^{()}(_{B})\) are already perfectly aligned.

To further verify, we also compare the values of \(_{}[1-_{}(_{i})]\) of two linearly connected modes with those of two modes that are independently trained (not satisfying LMC). We measure \(_{0.5}\) of the features of two modes that are linearly connected and two modes that are independently trained, denoted as \(_{LMC}\) and \(_{not~{}LMC}\) correspondingly. In Figure 4, the values of \(_{}[1-_{LMC}(_{i})]\) are negligible compared to \(_{}[1-_{not~{}LMC}(_{i})]\). The experimental results align with Figures 2 and 3 and thus we firmly verify the claim that LLFC co-occurs with LMC.

**LLFC Implies LMC.** Intuitively, LLFC is a much stronger characterization than LMC since it establishes a linearity property in the high-dimensional feature map in every layer, rather than just for the final error. Lemma 1 below formally establishes that LMC is a consequence of LLFC by applying LLFC on the output layer.

**Lemma 1** (Proof in Appendix A.1).: _Suppose two modes \(_{A}\), \(_{B}\) satisfy LLFC on a dataset \(\) and \(\{_{}(_{A}),_{} (_{B})\}\), then we have_

\[,_{}(_{A}+(1- )_{B}) 2.\] (6)

In summary, we see that the LMC property, which was used to study the loss landscapes in the entire parameter space, extends to the internal features in almost all the layers. LLFC offers much richer structural properties than LMC. In Section 5, we will dig deeper into the contributing factors to LLFC and leverage the insights to gain new understanding of the spawning and the permutation methods.

## 5 Why Does LLFC Emerge?

We have seen that LLFC is a prevalent phenomenon that co-occurs with LMC, and it establishes a broader notion of linear connectivity than LMC. In this section, we investigate the root cause of LLFC, and identify two key conditions, _weak additivity for ReLU activations_ and _commutativity_. We verify these conditions empirically and prove that they collectively imply LLFC. From there, we provide an explanation for the effectiveness of the permutation method, offering new insights into LMC.

### Underlying Factors of LLFC

For convenience, we consider a multi-layer perceptron (MLP) in Section 5, though the results can be easily adapted to any feed-forward structure, e.g., a convolutional neural network4 (CNN). For

Figure 4: Comparison of \(_{}[1-_{LMC}(_{i})]\) and \(_{}[1-_{not~{}LMC}(_{i})]\). Both the spawning and permutation methods are used to obtain two linearly connected modes. Standard deviations across the dataset are reported by error bars.

an \(L\)-layer MLP \(f\) with ReLU activation, the weight matrix in the \(\)-th linear layer is denoted as \(^{()}^{d_{} d_{-1}}\), and \(^{()}^{d_{}}\) is the bias in that layer. For a given input data matrix \(^{d_{0} n}\), denote the feature (post-activation) in the \(\)-th layer as \(^{()}=f^{()}(;)^{d_{} n}\), and correspondingly pre-activation as \(}^{()}^{d_{} n}\). The forward propagation in the \(\)-th layer is:

\[^{()}=(}^{()}),}^{()}= ^{()}^{(-1)}+^{()}_{d_{}}^{}.\]

Here, \(\) denotes the ReLU activation function, and \(_{d_{}}^{d_{}}\) denotes the all-one vector. Additionally, we use \(_{i}^{()}\) to denote the \(i\)-th row of \(^{()}\), and \(}_{i}^{()}\) to denote the \(i\)-th row of \(}^{()}\), which correspond to the post- and pre-activations of the \(i\)-th input at layer \(\), respectively.

**Condition I: Weak Additivity for ReLU Activations.5**

**Definition 3** (**Weak Additivity for ReLU Activations.**5**

_Given a dataset \(\), two modes \(_{A}\) and \(_{B}\) are said to satisfy weak additivity for ReLU activations if_

\[[L],,(}_{A} ^{()}+(1-)}_{B}^{()})=(}_ {A}^{()})+(1-)(}_{B}^{()}).\] (7)

Definition 3 requires the ReLU activation function to behave like a linear function for the pre-activations in each layer of the two networks. Although this cannot be true in general since ReLU is a nonlinear function, we verify it empirically for modes that satisfy LMC and LLFC.

We conduct experiments on various datasets and architectures to validate the weak additivity for ReLU activations. Specifically, given two modes \(_{A}\) and \(_{B}\) and a data point \(_{i}\) in the test set \(\), we compute the normalized distances between the left-hand side and the right-hand side of Equation (7) for each layer \(\), varying the values of \(\). We denote the maximum distance across the range of \(\) as \(_{}(}_{i,A},}_{i,B})=_{ }((}_{i,A}+(1-)}_{i,B}),(}_{i,A})+(1-)( }_{i,B}))\), where \((,):=\|-\|^{2}/(\|\|\|\|)\). To validate the weak additivity condition, we compare the values of \(_{}\) of two modes that are linearly connected with those of two modes that are independently trained.

Figure 5: Comparison between the distribution of the normalized distance \(_{}(}_{i,A},}_{i,B})\) and \(_{}(}_{i,C},}_{i,D})\). Here, \(}_{i,A}\) and \(}_{i,B}\) are features of two linearly connected modes, i.e., \(_{A}\) and \(_{B}\) (founded by the spawning method). \(}_{i,C}\) and \(}_{i,D}\) comes from two modes that are independently trained. Results are presented for different layers of MLP on MNIST and VGG-16 on CIFAR-10.

Both spawning and permutation methods are shown to demonstrate the weak additivity condition. For spawning method, we first obtain two linearly connected modes, i.e, \(_{A}\) and \(_{B}\), and then two independently trained modes, i.e, \(_{C}\) and \(_{D}\) (not satisfying LMC/LLFC). We compare the values of Dist\({}_{}\) of \(_{A}\) and \(_{B}\) with those of \(_{C}\) and \(_{D}\). In Figure 5, we observe that across different datasets and model architectures, at different layers, Dist\({}_{}(}_{i,A},}_{i,B})\) are negligible (and much smaller than the baseline Dist\({}_{}(}_{i,C},}_{i,D})\)). For permutation methods, given two independently trained modes, i.e., \(_{C}\) and \(_{D}\), we permute the \(_{D}\) such that the permuted \((_{D})\) are linearly connected with \(_{C}\). Both activation matching and weight matching are used and the permuted \((_{D})\) are denoted as \(_{D_{act}}\) and \(_{D_{wgt}}\) respectively. In Figure 6, the values of Dist\({}_{}(}_{i,C},}_{i,D_{act}})\) and Dist\({}_{}(}_{i,C},}_{i,D_{wgt}})\) are close to zero compared to Dist\({}_{}(}_{i,C},}_{i,D})\). Therefore, we verify the weak additivity condition for both spawning and permutation methods.

**Condition II: Commutativity.**

**Definition 4** (**Commutativity)**.: _Given a dataset \(\), two modes \(_{A}\) and \(_{B}\) are said to satisfy commutativity if_

\[[L],\,_{A}^{()}_{A}^{(-1)}+_{B}^{( )}_{B}^{(-1)}=_{A}^{()}_{B}^{(-1)}+_{ B}^{()}_{A}^{(-1)}.\] (8)

Commutativity depicts that the next-layer linear transformations applied to the internal features of two neural networks can be interchanged. This property is crucial for improving our understanding of LMC and LLFC. In Section 5.2, we will use the commutativity property to provide new insights into the permutation method.

We conduct experiments on various datasets and model architectures to verify the commutativity property for modes that satisfy LLFC. Specifically, for a given dataset \(\) and two modes \(_{A}\) and \(_{B}\) that satisfy LLFC, we compute the normalized distance between the left-hand side and the right-hand side of Equation (8), denoted as Dist\({}_{com}=((_{A}^{()}_{A}^{(-1)}+_{ B}^{()}_{B}^{(-1)}),(_{A}^{()}_{B}^{( -1)}+_{B}^{()}_{A}^{(-1)}))\). Furthermore, we compare Dist\({}_{com}\) with the normalized distance between the weight matrices of the current layer \(\), denoted as Dist\({}_{W}\), and the normalized distances between the post-activations of the previous layer \(-1\), denoted as Dist\({}_{H}\). These distances are expressed as Dist\({}_{W}=((_{A}^{()}),(_{B}^{( )}))\) and Dist\({}_{H}=((_{A}^{(-1)}),( _{B}^{(-1)}))\), respectively. Figure 7 shows that for both spawning and permutation methods, Dist\({}_{com}\) is negligible compared with Dist\({}_{W}\) and Dist\({}_{H}\), which confirms the commutativity condition. Note that we also rule out the trivial case where either the weight matrices or the post-activations in the two networks are already perfectly aligned, as weights and post-activations often differ significantly. We also add more baseline experiments for comparison (see Appendix B.3 for more results).

Additionally, we note that commutativity has a similarity to model stitching [19; 3]. In Appendix B.4, we show that a stronger form of model stitching (without an additional trainable layer) works for two networks that satisfy LLFC.

Figure 7: Comparison of Dist\({}_{com}\), Dist\({}_{W}\), and Dist\({}_{H}\). In the first row, the spawning method is used to acquire modes that satisfy LLFC, whereas the permutation method is used for the second row. For ResNet-20, Dist\({}_{com}\), Dist\({}_{W}\) are calculated in the first Conv layer of each block. The results are presented for different layers of MLP on the MNIST and ResNet-20 on the CIFAR-10. More results are in Appendix B.3.

**Conditions I and II Imply LLFC.** Theorem 1 below shows that weak additivity for ReLU activations (Definition 3) and commutativity (Definition 4) imply LLFC.

**Theorem 1** (Proof in Appendix A.2).: _Given a dataset \(\), if two modes \(_{A}\) and \(_{B}\) satisfy weak additivity for ReLU activations (Definition 3) and commutativity (Definition 4), then_

\[,[L],\;f^{()}(_{A}+(1-)_{B})= f^{()}( _{A})+(1-)f^{()}(_{B}).\]

Note that the definition of LLFC (Definition 2) allows a scaling factor \(c\), while Theorem 1 establishes a stronger version of LLFC where \(c=1\). We attribute this inconsistency to the accumulation of errors6 in weak additivity and commutativity conditions, since they are only approximated satisfied in practice. Yet in most cases, we observe that \(c\) is close to \(1\) (see Appendix B.2 for more results).

### Justification of the Permutation Methods

In this subsection, we provide a justification of the permutation methods in Git Re-Basin : weight matching (3) and activation matching (4). Recall from Section 3 that given two modes \(_{A}\) and \(_{B}\), the permutation method aims to find a permutation \(=\{^{()}\}_{=1}^{L-1}\) such that the permuted \(^{}_{B}=(_{B})\) and \(_{A}\) are linearly connected, where \(^{()}\) is a permutation matrix applied to the \(\)-th layer feature. Concretely, with a permutation \(\), we can formulate \(_{B}}^{()}\) and \(_{B}}^{()}\) of \(^{}_{B}\) in each layer \(\) as \(_{B}}^{()}=^{()}_{B}}^{(-1)}\)\({}^{}\) and \(_{B}}^{()}=^{()}_{B}}\).7 In Section 5.1, we have identified the commutativity property as a key factor contributing to LLFC. The commutativity property (8) between \(_{A}\) and \(^{}_{B}\) can be written as

\[(^{()}_{A}-_{B}}^{()}) (^{(-1)}_{A}-_{B}}^{(-1)} )=,\]

or

\[(^{()}_{A}-^{()}^{( )}_{B}^{(-1)})(^{(-1)}_{A}- ^{(-1)}^{(-1)}_{B})=.\] (9)

We note that the weight matching (3) and activation matching (4) objectives can be written as \(_{}_{=1}^{L}\|^{()}_{A}-^ {()}^{()}_{B}^{(-1)}\|_{F}^{2}\) and \(_{}_{=1}^{L-1}\|^{()}_{A}-^ {()}^{()}_{B}\|_{F}^{2}\), respectively, which directly correspond to the two factors in Equation (9). Therefore, we can interpret Git Re-Basin as a means to ensure commutativity. Extended discussion on Git Re-basin  can be found in Appendix B.5.

## 6 Conclusion and Discussion

We identified Layerwise Linear Feature Connectivity (LLFC) as a prevalent phenomenon that co-occurs with Linear Mode Connectivity (LMC). By investigating the underlying contributing factors to LLFC, we obtained novel insights into the existing permutation methods that give rise to LMC. The consistent co-occurrence of LMC and LLFC suggests that LLFC may play an important role if we want to understand LMC in full. Since the LLFC phenomenon suggests that averaging weights is roughly equivalent to averaging features, a natural future direction is to study feature averaging methods and investigate whether averaging leads to better features.

We note that our current experiments mainly focus on image classification tasks, though aligning with existing literature on LMC. We leave the exploration of empirical evidence beyond image classification as future direction. We also note that our Theorem 1 predicts LLFC in an ideal case, while in practice, a scaling factor \(c\) is introduced to Definition 2 to better describe the experimental results. Realistic theorems and definitions (approximated version) are defered to future research.

Finally, we leave the question if it is possible to find a permutation directly enforcing the commutativity property (9). Minimizing \(\|(^{()}_{A}-^{()}^{()}_ {B}^{(-1)})\)\((^{(-1)}_{A}-^{(-1)}^{(-1)}_{B})\|_{F}\) entails solving Quadratic Assignment Problems (QAPs) (See Appendix A.3 for derivation), which are known NP-hard. Solving QAPs calls for efficient techniques especially seeing the progress in learning to solve QAPs .