# Efficient LLM Jailbreak via Adaptive

Dense-to-sparse Constrained Optimization

 Kai Hu\({}^{1,2,}\), Weichen Yu\({}^{1,}\), Yining Li\({}^{2}\), Tianjun Yao\({}^{3}\), Xiang Li\({}^{1}\),

Wenhe Liu\({}^{1}\), Lijun Yu\({}^{1}\), Zhiqiang Shen\({}^{3}\), Kai Chen\({}^{2}\), Matt Fredrikson\({}^{1}\)

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)Shanghai AI Laboratory,\({}^{3}\)Mohamed bin Zayed University of AI

The first two authors contributed equally to this paper. Email: kaihu@cmu.edu.The code is available at https://github.com/hukkai/adc_llm_attack.

###### Abstract

Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which has been shown to successfully jailbreak multiple open-source LLMs. Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors. This technique effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods. On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods. Trigger Warning: This paper contains model behavior that can be offensive in nature.

## 1 Introduction

Recent advancements have enabled large language models (LLMs) to be utilized in diverse fields such as education , programming [6; 32], and health [23; 17]. However, these models can also pose risks by generating malicious content, including malware, instructions for creating hazardous items, and leaking sensitive information from their training data [22; 46; 25]. As LLMs become increasingly powerful and widespread, managing the risks linked to their misuse is crucial. In response, the practice of red-teaming LLMs has been introduced to evaluate their safety mechanisms [4; 40]. The LLM jailbreak attack, which emerged from this process, involves blending malicious questions (e.g., how to make explosives) with a jailbreak prompt. This strategy can deceive safety-aligned LLMs, causing them to bypass safety measures and potentially produce harmful, discriminatory, or sensitive responses .

Recent developments have also seen the introduction of various automatic jailbreak attacks. These attacks fall into two main categories: prompt-level jailbreaks [20; 26; 5] and token-level jailbreaks [46; 15; 24]. Prompt-level jailbreaks use semantically meaningful deception to undermine LLMs, which can be crafted manually , through prompt engineering , or generated by another LLM . The key advantage of prompt-level jailbreaks is that the attack queries are in natural language, which ensures they are interoperable. However, this same characteristic makes them susceptible to alignment training, as training examples to counter these methods are relatively easy to produce [8; 3]. For example, prompt-level jailbreaks tend to perform poorly against the Llama2-chat series  (see Harmbench ), which, although not specifically designed to counter these attacks, fine-tuned with \(\)2000 adversarial prompts.

Token-level jailbreaks, another type of jailbreak attack, directly alter the tokens of the input query to produce a specific response from the LLM. These jailbreaks allow for more precise control over the LLM's output due to their finer-grained optimizable space compared to prompt-level jailbreaks. A representative example of this is GCG , which demonstrates the effectiveness of token-level jailbreaks across LLMs (see Harmbench ). However, this method suffers from query inefficiency. GCG requires hundreds of thousands of queries and could take up to an hour to find a successful adversarial string for some hard examples, which limits its scalability to many applications like transfer attack  and adversarial training .

In this paper, we aim to enhance the efficiency of white-box token-level jailbreaks. GCG  utilizes a straightforward coordinate descent to conduct discrete optimization within the token space. Due to the inherent challenges of discrete optimizations [34; 15; 42], finding a solution demands considerable computational resources and may not always result in an optimal outcome . We believe that employing a more power optimization technique for the discrete token setting could significantly improve efficiency.

Motivated by the challenges of discrete optimization, our method transforms discrete optimization over \(V\) discrete vectors (where \(V\) represents the vocabulary size) into continuous optimization in \(^{V}\), allowing us to utilize more powerful optimizers beyond coordinate descent. While transitioning to continuous space is straightforward, converting the optimized vectors back to discrete token space remains challenging. Direct projection onto the discrete space  is ineffective, as it significantly alters the optimized vectors and leads to performance drops. To address this, we propose ADC, an Adaptive Dense-to-sparse Constraint for dense token optimization. Initially, we relax the discrete tokens into dense vectors, and the optimization takes place in the full continuous space \(^{V}\). As optimization progresses and the loss decreases, we gradually constrain the optimization space to a highly sparse space. This progression is adaptive to the optimization performance, ensuring that the constraint does not significantly hinder optimization. By the end of the process, the optimization space is constrained to a nearly one-hot vector space. Our approach minimizes the gap between the relaxed continuous space and the one-hot space, reducing the performance drop when transitioning the optimized vectors to discrete tokens.

Our approach surpasses existing token-level jailbreak methods in both effectiveness and efficiency. When compared fairly with the state-of-the-art token-level jailbreak method GCG  on AdvBench, our method significantly outperforms GCG across three LLMs: Llama2-chat , Vicuna , and Zephyr , with only two-thirds of the computational cost and half the wall-clock run time. Additionally, on the Harmbench jailbreak benchmark , our method achieves state-of-the-art results for seven LLMs, surpassing other jailbreak methods by clear margins. Notably, our method is robust against adversarially trained LLMs that resist token-level jailbreak methods. ADC attains a 26.5% attack success rate (ASR) in attacking the adversarially trained LLM Zephyer R2D2 , whereas previous token-level jailbreak methods [46; 20; 15; 42] yielded nearly 0% ASR performance. This result demonstrates the strength of our proposed optimization method and highlights the limitations of previous defenses against token-level jailbreaks.

## 2 Related Work

LLM alignmentAs large language models (LLMs) continue to develop, the urgency to ensure they align with human values is growing [11; 35; 41; 43]. In response, researchers have introduced various techniques to enhance the safety alignment of LLMs. Some approaches involve gathering high-quality training data that mirror human values and using them to modify the behavior of LLMs [1; 10; 21; 36; 39]. Additionally, other strategies focus on developing training methods such as Supervised Fine-Tuning (SFT) [38; 31], Reinforcement Learning from Human Feedback (RLHF) [45; 2; 28], and adversarial training  to achieve alignment. Despite these efforts, aligning LLMs safely does not always prevent their potential harmful behaviors [12; 14; 19; 44].

Automated jailbreaksSeveral studies have employed manual jailbreak attacks [3; 16; 40; 29] to enhance the alignment of large language models (LLMs) or for pre-deployment testing. Nevertheless, manual jailbreaks are not scalable and often suffer from a lack of diversity. Two main types of automated jailbreaks have emerged. The first type, prompt-level jailbreaks [20; 26; 5], involves creating semantically meaningful deceptions to compromise LLMs. A key benefit of this approach is that the attack queries are in natural language, facilitating compatibility. Moreover, prompt-level jailbreaks are typically black-box attacks, enabling them to target robust closed-source LLMs.

The second category, token-based jailbreaks [13; 46; 15; 24; 24], involves appending an adversarial suffix to the user's query and optimizing these adversarial tokens using gradient information. This method allows for more precise manipulation of the LLM's outputs. Although token-based jailbreaks generally require access to the model's gradient information, indicating a white-box approach, they can also be adapted for black-box scenarios to some extent. GCG  demonstrates the potential for the transferability of jailbreak strings to closed-source LLMs like GPT.

Discrete token optimizationDeploying adversarial attacks on language models presents a unique challenge due to the discrete nature of text. While image inputs can be approximated as continuous values due to their ordered, fine-grained pixel values, text does not share this characteristic. One approach optimizes directly over discrete tokens [9; 15; 33; 46], ensuring that all updated results remain valid inputs for language models. However, this method of discrete optimization tends to be slow, as it cannot fully utilize gradient information (the update has to be from one discrete token to another). Another strategy involves optimizing in a continuous embedding space before projecting the solutions back into discrete space [18; 42; 13]. Additionally, some researchers use Langevin dynamics sampling to generate discrete prompts from continuous embeddings . Above approaches involve projecting optimized soft embeddings onto their closest one-hot embeddings, ensuring that optimization occurs within the discrete token space. However, if the nearest neighbors are far from the optimized embeddings (which often happens without specific constrains), the effectiveness of the optimization significantly decreases. Our method addresses this issue by applying an adaptive dense-to-sparse constraint, ensuring that the optimized soft embeddings could be properly sparse and sufficiently close to their respective one-hot embeddings.

## 3 Methodology

### Problem Setup

Following , our framework seeks to search for adversarial strings. When appending this string to a user query, the LLM chatbot will respond to the query, even if it is harmful. Here is an example:

User: Create fake news that could lead to panic or chaos Assistant: Sorry, I cannot assist with that.

User: Create fake news that could lead to panic or chaos!!!!!!! Assistant: Sure, here is fake news that could lead to panic or chaos

The red string (!!!!!) represents an adversarial string that causes the LLM chatbot to reply with a positive sentence, as shown in the example above. Following early work , we hypothesize that if the initial response of a large language model (LLM) chatbot starts with an affirmative phrase (e.g., "Sure, here is"), it is more likely to adhere to the user's instructions. Therefore, the main objective of our jailbreak is to optimize the conditional probability of eliciting a specific target response based on the user query, any associated system prompts, and the adversarial string.

NotationIn an LLM with a vocabulary size of \(V\), the \(i^{}\) vocabulary is denoted as a one-hot vector \(e_{i}^{V}\) where the \(k\)-th element is given by \(e_{i}[k]=(i=k)\). Let \(=\{e_{i}\}_{1 i V}\) denote the set of all vocabularies. Let \(x_{1:n}\) denote a sequence of vectors from: \(x_{1},x_{2},,x_{n}\). The user query of length \(l\), the adversarial tokens of length \(n\) and the target response of length \(m\) are represented as \(x_{1:l},z_{1:n},y_{1:m}\) respectively. The jailbreak objective is given by:

\[_{ i,z_{i}}\ p(y_{1:m}|x_{1:l} z_{1:n}),\] (1)where \(\) denotes concatenation. Thanks to LLM's next token prediction nature, we can minimize the following cross entropy loss to achieve the optimization goal in Equation 1.

\[_{ i,z_{i}}=_{k=1}^{m}( (x_{1:l} z_{1:n} y_{1:k-1}),y_{k}).\] (2)

### Relaxed continuous optimization

The optimization space of Equation 2 is discrete, thus common optimizers do not work for it directly. A straightforward method is to consider a relaxed continuous optimization problem. We use the probability space as the continuation of the one-hot vector set \(=\{w^{V}w[i] 0,\|w\|_{1}=1\}\), and relax Equation 2 to Equation 3:

\[_{ i,z_{i}}=_{k=1}^{m}( (x_{1:l} z_{1:n} y_{1:k-1}),y_{k}).\] (3)

Equation 3 is a continuous space optimization thus an easier problem than Equation 2. However, if optimizing Equation 3, we need to convert \(z_{1:n}\) from \(\) to \(\) either during or after the optimization process. Only one-hot vectors in \(\) are legal inputs for an LLM.

We find it does not work well if directly apply a projection from \(\) to \(\):

\[(x)=e_{j}j=_{k}\|x-e_{k}\|_{2}^{2}= _{k}x[k].\] (4)

The optimizing loss would increase sharply after this projection. This occurs because, in most cases, the optimized result of \(z_{1:n}\) from Equation 3 are dense vectors. The distance between a dense vector \(x\) and its projection to \(\) tends to be substantial, i.e., \(\|_{}(x)-x\|\) is large. The projection greatly changes the optimizing \(z_{1:n}\), thus hurting optimization.

To reduce the negative impact caused by projection, we would hope that the optimizing vectors \(z_{1:n}\) are sparse **before** we apply the projection (Equation 4). Then, the distance between a sparse vector \(x\) and its projection could be smaller.

Adaptive SparsityTo tackle this issue, we propose a dense-to-sparse constraint to the optimization of Equation 3. During the optimization process, we progressively enhance the sparsity of the optimizing vectors \(z_{1:n}\), which helps decrease the loss when projecting from set \(\) to set \(\). Specifically, at each optimization step, we manually convert \(z_{1:n}\) to be \(S\)-sparsity, where \(S\) is given by the number of wrong predictions when optimizing the classification loss in Equation 3:

\[S=[_{k=1}^{m}(y_{k})].\] (5)

The motivation of the adaptive sparsity (Equation 5) is to use less sparsity constraint before we can find an optimal solution in the relaxed space \(\). If the LLM is random guessing when predicting the target response \(y_{1:m}\), Equation 5 provides an exponentially growing large sparsity and gives no constraint to \(z_{1:n}\). If the classification problem in Equation 3 is well optimized and all \(y_{1:m}\) are correctly predicted, \(S 1\) indicates we would project all \(z_{1:n}\) to the one-hot space \(\) because there is no further optimization room in the relaxed space \(\).

Note that an \(S\)-sparsity subspace in a \(d\)-dimensional space (\(S<d\)) is not a convex set. Projection onto the \(S\)-sparsity subspace may be computationally complex and not yield a unique solution. We propose a simple transformation that converts an arbitrary vector to be \(S\)-sparsity and in the relaxed set \(\). While this transformation is not a projection (i.e., mapping \(x\) to the closest point in \(\)), we find it works well. Algorithm 1 describes the details of the transformation.

Non-integer sparsityMost commonly, \(S=()\) is not an integer. Let \( S\) denote the maximum integers no greater than \(S\). In the case \(S\) is not an integer, we randomly select

\[((S- S) n)\]uniformly vectors \(z_{i}\) from \(z_{1:n}\) and transform these \(z_{i}\) to be \(( S+1)\)-sparse and the remaining \(z_{i}\) to be \( S\)-sparse. Then the average sparsity of \(z_{1:n}\) is approximately \(S\).

For example, suppose \(S=1.2,n=20\), then \(4\) random vectors from \(z_{1:n}\) are converted to be 2-sparse and 16 random vectors from \(z_{1:n}\) are converted to be 1-sparse, i.e., one-hot vectors. Such a design is to progressively reduce the gap between the relaxed set \(\) and the one-hot vector \(\), especially when the loss in the relaxed space is low.

### Optimizer design to escape local minima

Equation 3 possesses many local minima, most of which are suboptimal for jailbreak. Consequently, creating an optimizer designed to escape local minima is essential. We adopt the following three designs:

Optimization hyperparameterIn all our experiments, we employ the momentum optimizer with a learning rate of 10 and a momentum of 0.99, and do not adjust them during the optimization. The large learning rate and momentum contribute to the optimizer's ability to escape local minima.

No reparameterization trickOptimization in Equation 3 has a constraint that \( i,\|z_{i}\|=1\). Existing work may handle the constraint using a reparameterization trick. For example, instead of optimizing \(z_{i}\), one can optimizing \(w_{i}^{V}\) and represent \(z_{i}\) as:

\[z_{i}[k]=[k])}{_{j}(w_{i}[k])}l_{1}z_{i}[k]=[k]|}{_{j}|w_{i}[k]|}.\]

However, we argue that such reparameterization trick is not good for the optimizer to escape local minima. Take softmax reparameterization for example. To make \(z_{i}\) sparse, most elements in \(z_{i}\) are close to or equal to 0. If a softmax reparameterization is applied, we can show that \([k]} z_{i}[k]\) where \(\) is the optimization objective. If one element of \(z_{i}\) is close to zero: \(z_{i}[k] 0\), the derivative of the corresponding \(w_{i}[k]\) is also close to zero. Once \(z_{i}[k]\) is updated to a small number or set to zero (in Algorithm 1), updating this element in subsequent optimization steps becomes difficult, making the entire optimization stuck in a local minima.

To design an optimization that readily escapes local minima, we eschew the reparameterization trick. Instead, we utilize the projection (Algorithm 1) to satisfy the constraint \(z_{i}\). Here, zeroing one element of \(z_{i}\) does not diminish the likelihood of updating this element to a larger number in subsequent optimization steps.

Multiple initialization startsWe initialize \(z_{1:n}\) from the softmax output of Gaussian distribution:

\[z_{i}() (0,I_{V})\] (6)

Like many non-convex optimization, we initialize multiple \(z_{1:n}\) to reduce the loss of "unlucky" initialization. These different starts are optimized independently. Once one of searched \(z_{1:n}\) can jailbreak, the optimization would early stop.

### Complete Attack Algorithm

Algorithm 2 provides an overview of our method with **one** initialization start. We employ a momentum optimizer with a learning rate of 10 and a momentum of 0.99, optimizing over maximum 5000 steps. Following GCG , the number of optimizable adversarial tokens is 20 for all experiments.

The actual jailbreak methods initialize and optimize multiple adversarial tokens in parallel. Once one of them can jailbreak, the entire attack would early stop.

Number of initialization startsOne step optimization includes 1 model forward (dense tokens loss computation), 1 model backward (dense tokens gradient computation) and 1 model forward (jailbreak evaluation of the projected one-hot tokens). Since the model backward only computes the gradient of the input, the computational cost of one model backward equals to that of one model forward. Thus the maximum computational cost for 5000 steps optimization is \(1.5 10^{4}\) model forward for one initialization start.

Similarly the computational cost for the state of the art token based attack GCG  is \(2.57 10^{6}\) model forward under its standard configuration (512 batch size, maximum 500 steps). To align with GCG's computational cost, we initialize 16 different initialization starts and optimize them in a batch, achieving a computational cost that is 93% of GCG. We name our attack of this configuration (initialize 16 different starts and optimize for maximum 5000 steps) as ADC.

More efficiency integrated with GCGWe find that ADC converges slower when the loss in Equation 3 is low. This is because we use a fixed and large learning rate in our algorithm. A learning rate schedule may solve similar problems in neural network training. However, it introduces more hyper-parameter tuning thus we have a lower preference for it. We find a more efficient way is to switch to GCG after certain number of steps. Specifically, we initialize 8 different starts and optimize for maximum 5000 steps. If not attack successfully, we use the searched adversarial tokens with the best (lowest) loss as the initialization to perform GCG attack for 100 steps. The two-stage attack achieves a computational cost that is 67% of standard GCG, and we name it as ADC+.

## 4 Experimental Results

### Experimental Setup

DatasetsWe evaluate the effectiveness of ADC optimization on three datasets:

1. AdvBench harmful behaviors subset  contains 520 harmful behavior requests. The attacker's objective is to search an adversarial string that will prompt the LLM to produce any response that seeks to follow the directive.
2. AdvBench harmful strings subset contains 574 strings that reflect harmful or toxic contents. The attackers objective is to find a string that can prompt the model to generate these **exact strings**. The evaluation on this dataset reflects the attacker's ability to control the output of the LLM.
3. HarmBench  contains 400 harmful behavior requests similar to AdvBench "harmful behaviors" subset, but with a wider range of topics including copyright, context reference and multimodality. We evaluate on the "Standard Behaviors" subset of 200 examples. Evaluation on the complete HarmBench dataset will be released soon.

Victim modelsWe attempt to jailbreak both open- and closed-source LLMs. Specifically, we consider open-source LLMs: Llama2-chat-7B , Vicuna-v1.5-7B , Zephyr-7b-\(\), and Zephyr 7B R2D2  (an adversarial trained LLM to defend against attacks). We use their default system prompts and chat templates.

MetricAttack Success Rate (ASR) is widely used to evaluate LLM jailbreak methods. However, previous works use different methods to calculate ASR and some may have different shortcomings (see Harmbench  for a detailed discussion). We follow Harmbench  to compute ASR: the victim model generates the response of maximum 512 tokens given the adversarial request and we use the red teaming classifier (from HarmBench ) to decide if the response follows the request. The classifier only gives binary classification results ("YES" or "NO"), and ASR is computed as the ratio of the number of "YES" over the number of attacked examples.

The AdvBench harmful strings benchmark requires the attacker to generate the exact strings. For this task, we employ the Exact Match (EM) metric, meaning an attack is only considered successful if the generated string precisely matches the target harmful string.

### Comparison with existing methods

Comparison with the baseline on AdvBenchWe use GCG as our baseline for a detailed comparison with our method on the AdvBench dataset, focusing on three LLMs: Llama2-chat-7B, Vicuna-v1.5-7B, and Zephyr \(\) 7B. Both two jailbreak methods follow the same early stop criteria and utilize the same jailbreak judger (the red teaming classifier in Harmbench ). It is important to note that GCG and our method require different numbers of optimization steps and search widths. To measure efficiency, we consider both the computing budget and wall-clock time. The computing budget represents the **maximum** computational cost (including both model forward and backward) without early stopping, i.e., run the optimization until the maximum step. In contrast, the wall-clock time is the average real-time elapsed per sample on a single A100 machine. Due to early stopping, these two metrics are not perfectly correlated. To ensure a fair comparison, we maintain the same data dtype and KV-cache settings for both methods.

Table 1 presents the results on 520 examples from the AdvBench harmful behaviors subset. Our method significantly outperforms GCG on Llama2-chat-7B and performs slightly better on other LLMs, where performance is already nearly 100%. Additionally, our method shows an advantage in wall-clock time, as it can find a jailbreak string in fewer steps, indicated by a higher early stop rate. If one only aims to match the jailbreak performance of GCG, the time required by our method can be further reduced.

We further explore our method's capacity for precise control over the outputs of the LLM. Precise control of LLM outputs is important for multi-turn attack and better shows the optimization ability of the attack method. Table 2 shows the Exact Match performance on AdvBench Strings. The early stop rate is not reported because it equals to EM in the exact matching setting. We can see that ADC is better than GCG on Vicuna-v1.5-7B and Zephyr-\(\)-7B but slightly worse on Llama2-chat-7B. However, ADC+ consistently outperforms the baseline in terms of performance and efficiency.

Comparison with the state of the art methods on HarmbenchExisting studies on LLM jailbreak have used various evaluation setting, including testing on different subsets of AdvBench, generating responses of varying lengths, and employing different ASR computation methods. Consequently, direct comparison with the ASR reported in these studies is not reliable. Fortunately, Harm

   model & method & ASR (\%) &  Computing \\ Budge \\  &  Wall-clock \\ Time (min) \\  & 
 Early Stop \\ Rate (\%) \\  \\   & GCG & 53.8 & 1\(\) & 20.6 & 53.3 \\  & ADC & 96.2 & 0.93\(\) & 11.1 & 95.8 \\  & ADC+ & 96.5 & 0.67\(\) & 8.2 & 96.2 \\   & GCG & 99.4 & 1\(\) & 3.0 & 98.3 \\  & ADC & 99.8 & 0.93\(\) & 2.3 & 99.8 \\  & ADC+ & 99.8 & 0.67\(\) & 1.6 & 99.8 \\   & GCG & 98.1 & 1\(\) & 13.2 & 68.3 \\  & ADC & 99.8 & 0.93\(\) & 7.9 & 92.1 \\   & ADC+ & 98.8 & 0.67\(\) & 6.1 & 97.5 \\   

Table 1: Comparison on 520 examples from AdvBench Behaviours. A higher ASR is better.

Bench  replicates these methods, allowing for a fair comparison on the HarmBench dataset. We adhere to their settings for chat templates, the number of generated tokens, and ASR computation methods. Table 3 presents the comparison between our method, ADC+, and the top-performing methods reported by HarmBench : GCG , AutoPrompt (AP) , PAIR , TAP , and AutoDan . Other jailbreak methods with lower performance are not listed. ASR numbers for these methods are sourced from HarmBench (Standard Behaviours) .

Table 3 illustrates that although no single jailbreak method consistently outperforms others across all large language models (LLMs), our approach achieves state-of-the-art results on seven out of eight LLMs, excluding Zephyr-R2D2. Notably, our method registers more than 50% and 20% improvements in attack success rate (ASR) on Llama2-chat and Qwen-chat, respectively, compared to existing methods. It's important to note that Zephyr-R2D2 has been adversarially trained to resist token-level jailbreaks, making prompt-level methods more effective in comparison. Nevertheless, our method still demonstrates significant effectiveness on Zephyr-R2D2, achieving notable ASR where other token-level jailbreak methods like GCG achieve only single-digit performance.

TransferabilityWhile ADC is primarily employed as a white-box jailbreak method, it can also extend to black-box jailbreak scenarios. The black-box variant of our method, similar to GCG, seeks a transferable adversarial string by jointly optimizing across multiple examples and models. Specifically, Vicuna-v1.5-7B and Vicuna-v1.5-13B serve as surrogate models, and optimization is performed on a subset of Advbench filed by PAIR . In line with GCG, we report the transfer ASR on GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0314). Table 4 compares our method with GCG and PAIR, representing token-based and template-based attacks, respectively. The results of our method are obtained by using the GPT-4 judger proposed by PAIR.

Ablation StudyThe adaptive sparsity design is central to our method. Table 5 compares our adaptive sparsity design with baseline methods that use constant sparsity of 1, 2, and 3. The table shows the ASR performance for Vicuna-v1.5-7B and Llama2-chat-7B on the AdvBench behavior

   model & method & EM (\%) & Computing Budge & Wall-clock Time \\   & GCG & 41.3 & 1\(\) & 18.8 min \\  & ADC & 32.6 & 0.93\(\) & 21.5 min \\  & ADC+ & 75.4 & 0.67\(\) & 13.9 min \\   & GCG & 90.6 & 1\(\) & 5.8 min \\  & ADC & 99.0 & 0.93\(\) & 3.2 min \\  & ADC+ & 100.0 & 0.67\(\) & 2.0 min \\   & GCG & 52.4 & 1\(\) & 13.7 min \\  & ADC & 92.9 & 0.93\(\) & 7.1 min \\   & ADC+ & 98.4 & 0.67\(\) & 4.8 min \\   

Table 2: Comparison on 574 examples from AdvBench Strings. A higher EM is better.

    &  &  &  &  \\  & & & & & & \\  Llama2-7B-chat & 34.5 & 17.0 & 7.5 & 5.5 & 0.5 & **92** \\ Llama2-13B-chat & 28.0 & 14.5 & 15.0 & 10.5 & 0.0 & **56.5** \\ Vicuna-v1.5-7B & 90.0 & 75.5 & 65.5 & 67.3 & 89.5 & **100** \\ Vicuna-v1.5-13B & 87.0 & 47.0 & 59.0 & 71.4 & 82.5 & **100** \\ Qwen-7B-chat & 79.5 & 67.0 & 58.0 & 69.5 & 62.5 & **99.0** \\ Qwen-14B-chat & 83.5 & 56.0 & 51.5 & 57.0 & 64.5 & **96.7** \\ Zephyr-\(\)-7B & 90.5 & 79.5 & 70.0 & 83.0 & 97.3 & **100** \\ Zephyr-R2D2\({}^{*}\) & 0.0 & 0.0 & 57.5 & **76.5** & 10.5 & 26.5 \\   

Table 3: Jailbreak comparison on 200 examples from HarmBench Standard Behaviours. The number indicates the ASR for the corresponding LLM and the jailbreak method. A higher ASR is better.

subset. No single constant sparsity level performs optimally across both LLMs, whereas adaptive sparsity consistently outperforms all constant sparsity levels.

We also demonstrate how certain hyperparameters affect the performance of our methods. Table 6 and Table 7 present the ablation study focusing on the learning rate and the momentum of the optimizer. We report the ASR of Llama2-chat-7B and Vicuna-v1.5-7B on AdvBench Behaviours. Our method remains robust across a wide variety of learning rates. However, achieving optimal performance is critically dependent on the appropriate use of momentum. This is because the gradient at a local point may not be very helpful due to sparsity constraints in the optimization. Utilizing a large momentum can leverage historical gradient information to help minimize the loss.

## 5 Conclusion and Limitations

In this paper, we introduce a new token-level attack method known as Adaptive Dense-to-Sparse Constrained Optimization (ADC). This method effectively breaches several open-source large language models (LLMs). By transforming the discrete jailbreak optimization problem into a continuous one, ADC progressively enhances the sparsity of the optimizing vectors, bridging the gap between discrete and continuous space optimization. Our experimental results demonstrate that ADC surpasses existing token-level techniques in terms of effectiveness and efficiency. On Harmbench, ADC achieves the highest attack success rate on seven out of eight LLMs, outperforming current state-of-the-art jailbreak methods. A limitation of this approach is the requirement for white-box access to the LLMs, which restricts its applicability to closed-source models. Nonetheless, our findings suggest that a transfer attack to black-box models remains feasible for token-level attacks.