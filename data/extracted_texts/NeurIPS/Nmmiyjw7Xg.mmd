# Safe and Sparse Newton Method for

Entropic-Regularized Optimal Transport

 Zihao Tang

School of Statistics and Data Science

Shanghai University of Finance and Economics

tangzihao@stu.sufe.edu.cn

Yixuan Qiu

School of Statistics and Data Science

Shanghai University of Finance and Economics

qiuyixuan@sufe.edu.cn

Corresponding author.

###### Abstract

Computational optimal transport (OT) has received massive interests in the machine learning community, and great advances have been gained in the direction of entropic-regularized OT. The Sinkhorn algorithm, as well as its many improved versions, has become the _de facto_ solution to large-scale OT problems. However, most of the existing methods behave like first-order methods, which typically require a large number of iterations to converge. More recently, Newton-type methods using sparsified Hessian matrices have demonstrated promising results on OT computation, but there still remain a lot of unresolved open questions. In this article, we make major new progresses towards this direction: first, we propose a novel Hessian sparsification scheme that promises a strict control of the approximation error; second, based on this sparsification scheme, we develop a _safe_ Newton-type method that is guaranteed to avoid singularity in computing the search directions; third, the developed algorithm has a clear implementation for practical use, avoiding most hyperparameter tuning; and remarkably, we provide rigorous global and local convergence analysis of the proposed algorithm, which is lacking in the prior literature. Various numerical experiments are conducted to demonstrate the effectiveness of the proposed algorithm in solving large-scale OT problems.

## 1 Introduction

In recent years, optimal transport (OT, ) has received massive attentions from the deep learning community, and has become a fundamental modeling tool in modern statistical machine learning . One major challenge of applying OT to real-life problems, the computation of large-scale OT, has also gained many progresses in the direction of approximate OT methods, especially the entropic-regularized OT .

Consider two discrete probability measures \(=_{i=1}^{n}a_{i}_{x_{i}}\) and \(=_{j=1}^{m}b_{j}_{y_{j}}\), where \(a=(a_{1},,a_{n})^{T}\) and \(b=(b_{1},,b_{m})^{T}\) are two vectors satisfying \(_{i=1}^{n}a_{i}=_{j=1}^{m}b_{j}=1\), \(a_{i}>0\), \(b_{j}>0\), \(i=1,,n\), \(j=1,,m\), \(\{x_{i}\}_{i=1}^{n}\) and \(\{y_{j}\}_{j=1}^{m}\) are two sets of data points, and \(_{x}\) is the Dirac measure at position \(x\). Without loss of generality we assume that \(n m\), as their roles can be exchanged. Let \(M=(M_{ij})^{n m}\) be a cost matrix, whose entry \(M_{ij}\) typically represents thedistance between data points \(x_{i}\) and \(y_{j}\). Also define \((a,b)=\{T^{n m}:T_{m}=a,T^{T}_{n}=b,T  0\}\), where the inequality sign applies elementwisely. Then OT between the two measures \(\) and \(\) can be characterized by the following linear programming problem,

\[_{P(a,b)} P,M,\] (1)

where \( A,B=(A^{T}B)\). Assuming \(n=m\), the computational complexity of solving (1) using standard linear programming solvers is typically at the order of \(O(n^{3}(n))\), which can be difficult even for moderate \(n\) and \(m\). One approach to approximating the solution is to add an entropic penalty term to the objective function, leading to the entropic-regularized OT problem :

\[_{T(a,b)} T,M- h(T),\] (2)

where \(h(T)=_{i=1}^{n}_{j=1}^{m}T_{ij}(1- T_{ij})\) is the entropy term, and \(>0\) is a regularization parameter. The objective function in (2) is \(\)-strongly convex on \((a,b)\), so (2) has a unique global solution, denoted as \(T^{*}\). The matrix \(T^{*}\) is often referred to as the Sinkhorn transport plan. Problem (2) can be solved by the celebrated Sinkhorn algorithm , which is based on efficient matrix-vector multiplication operations. Due to its computational advantage, the Sinkhorn algorithm has become the _de facto_ solution to large-scale OT problems for a long time.

However, from a practical point of view, the Sinkhorn algorithm typically demonstrates a sub-linear convergence speed, thus requiring a large number of iterations for a moderately high precision. More recently, second-order methods, such as the Newton method, have attracted a growing number of researchers to rethink the solution to (2) . It has been shown that (2) has a dual problem (Proposition 4.4 of ):

\[_{,}\,(,)_{,}\, ^{T}a+^{T}b-_{i=1}^{n}_{j=1}^{m}e^{^{-1}(_{ i}+_{j}-M_{ij})},^{n},^{m}.\] (3)

Let \(^{*}=(_{1}^{*},,_{n}^{*})^{T}\) and \(^{*}=(_{1}^{*},,_{m}^{*})^{T}\) be one optimal solution to (3), and then the Sinkhorn transport plan \(T^{*}\) can be recovered as \(T^{*}=(^{*},^{*})\), where for two vectors \(=(_{1},,_{n})^{T}\) and \(=(_{1},,_{m})^{T}\), \((,)\) is a matrix with entries

\[[(,)]_{ij}=\{^{-1}(_{i}+_{j}-M_{ij}) \}.\]

Remarkably, (3) is equivalent to a smooth and unconstrained convex optimization problem, so it can be solved using first-order methods such as gradient descent, or second-order methods including the Newton method. It is well known that under some smoothness assumptions on the objective function, the Newton method has a fast quadratic local convergence rate, and hence it typically requires much fewer iterations than the Sinkhorn algorithm. However, each iteration in the Newton method involves solving a dense linear system, resulting in a per-iteration cost of \(O(n^{3})\), which is nearly of the same order as that of an unregularized OT problem. To this end,  proposes the Sinkhorn-Newton-Sparse (SNS) algorithm, which approximates the Hessian matrix of (3) by a sparse one, so that each iteration roughly has a computational cost of \(O(n^{2})\), the same order as the Sinkhorn algorithm.

The SNS algorithm demonstrates promising results for solving the entropic-regularized OT, yet there remain a number of open questions. First, the SNS algorithm relies on initial Sinkhorn iterations to achieve a moderately small optimality gap, but in practice, the cost for this warm initialization also needs to be taken into account. Second, though SNS advocates using the conjugate gradient (CG) method to solve the sparse linear systems, there is no strong guarantee on the invertibility of the sparsified Hessian matrix. Third,  conjectures that SNS has a super-linear local convergence rate, but it is not yet proven by theoretical analysis.

In this article, we make major progresses on the second-order method for entropic-regularized OT by resolving the three challenges above. First, we carefully design a new sparsification algorithm that has a well-controlled approximation error, and then propose a _safe_ Newton-type algorithm, in the sense that the linear systems for computing the search directions are always positive definite. This property addresses the invertibility issues of existing sparsified Newton methods, and is crucial for practical implementation. We also show that the proposed algorithm has a global convergence guarantee, so no initial Sinkhorn iterations are needed. Most importantly, we provide solid theoretical analysis of the proposed method, and show that it achieves a quadratic local convergence rate similar to the vanilla Newton method.

ContributionOur main contribution compared to prior art is summarized as follows:

1. We propose a novel Hessian sparsification scheme that promises a strict control of the approximation error.
2. Based on this sparsification scheme, we prove that the sparsified Hessian matrix is always positive definite regardless of the sparsification parameter. This property then leads to a _safe_ Newton-type method that is guaranteed to avoid singularity in computing the search directions.
3. The developed algorithm has a clear implementation for practical use, avoiding most hyperparameter tuning. An efficient implementation is included in the **RegOT** Python package2. 4. We provide rigorous global and local convergence analysis of the proposed algorithm, which is lacking in prior literature.

## 2 Background and Related Work

NotationThroughout this article, we use \(f(x)\), \(g(x)\), and \(H(x)\) to represent the objective function, gradient, and Hessian matrix at point \(x\), respectively. For a matrix \(A\), let \(A_{i}\) be the vector of the \(i\)-th row of \(A\), and \(A_{ j}\) be the vector of the \(j\)-th column of \(A\). For a vector \(v=(v_{1},,v_{n})^{T}\), let \(\) denote the vector \((v_{1},,v_{n-1})^{T}\), and for a matrix \(A=(A_{ 1},,A_{ n})\), let \(\) represent the matrix \((A_{ 1},,A_{ n-1})\). The symbol \(\|\|\) stands for the Euclidean norm for a vector argument, and represents the operator norm for matrices. The \(_{1}\)-norm and infinity norm are denote by \(\|\|_{1}\) and \(\|\|_{}\), respectively, applying to both vectors and matrices.

The main objectiveAs has been explained in Section 1, the key to the computation of entropic-regularized OT is to solve its smooth dual problem (3), which is equivalent to the convex optimization problem \(_{,}-(,)\). However, it is worth noting that the variables \((,)\) have one redundant degree of freedom: if \((^{*},^{*})\) is one solution to (3), then so is \((^{*}+c_{n},^{*}-c_{m})\) for any \(c\). Therefore, to ensure identifiability, we globally set \(_{m}=0\) without loss of generality. In what follows, we use the vector \(x=(^{T},^{T})^{T}^{n+m-1}\) to collect all free dual variables. Clearly, we always have \(=(^{T},_{m})^{T}=(^{T},0)^{T}\), so \(\) and \(\) will be used interchangeably when we consider functions of \(\). Then the main target of this article is to efficiently solve the optimization problem

\[_{x^{n+m-1}}f(x)-(,)= _{n}^{T}(,)_{m}-^{T}a-^{T}b.\] (4)

It is known that \(f(x)\) is a strictly convex function, so if (4) has a solution, then it is unique. It has also been proven in the existing literature that the gradient and Hessian matrix of \(f(x)\) have the following closed-form expressions (see for example ):

\[g(x) =-_{}(,)\\ _{}(,)= T_{m}-a\\ ^{T}_{n}-, T=(,),\] (5) \[H(x) =-_{}^{2}(,)& _{}(_{}(,)) \\ _{}(_{}(, ))^{T}&_{}^{2}(, )=^{-1}(T_{m})& \\ ^{T}&(^{T}_{n}).\]

Computational OTThere are a huge number of methods developed to solve (2) and (4). The Sinkhorn algorithm  can be interpreted as applying the block coordinate descent (BCD), also known as the alternating minimization (AM) method, to (4). Along this direction, many extensions of the Sinkhorn algorithm have been proposed. For example,  develops an accelerated version of AM, and a greedy variant of the Sinkhorn algorithm, named Greenkhorn, is developed in . Accelerated first-order methods, such as the adaptive primal-dual accelerated gradient descent (APDAGD, ) and adaptive primal-dual accelerated mirror descent (APDAMD, ), have been proposed to solve the constrained problem (2). Moreover, since the dual problem (4) is smooth, various quasi-Newton methods, such as the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method , have also been used . Second-order methods are less common in the literature to solve (4), mainly due to the high cost of solving the linear systems, and  proposes a practical sparsified Newton method to approximate the true Hessian by a sparse matrix. Another direction to utilize the sparsity is the importance sparsification method , which replaces the dense kernel matrix in the Sinkhorn algorithm by a sparsified one through random sampling.

Newton-type methodsThe Newton method is a classical second-order optimization algorithm that has been extensively studied. Assuming \(f\) is twice continuously differentiable, the Newton method generates a sequence of iterates \(\{x_{k}\}\) using the updating formula \(x_{k+1}=x_{k}-[H(x_{k})]^{-1}g(x_{k})\). However, computing the linear system \([H(x_{k})]^{-1}g(x_{k})\) with a dense Hessian matrix generally costs \(O(n^{3})\) of computation, so the sparsification of \(H(x_{k})\) with a solid convergence guarantee is the main focus of this article. The Newton method also has many extensions, for example the regularized Newton method to solve problems with non-isolated solutions , and the inexact Newton method  that allows inexact solutions to the linear system \([H(x_{k})]^{-1}g(x_{k})\). Our proposed method use a combination of these ideas to reduce the computational cost of the classical Newton method.

OT in machine learningOT is a blooming research topic in modern machine learning research. As a powerful tool to characterize the transformation of statistical distributions, OT has wide applications in deep generative models , domain adaptation and transfer learning , and fairness in machine learning , among many others. Readers are referred to review articles such as  for a summary of machine learning tasks and methods that utilize OT.

## 3 The Proposed Algorithm

To complement existing second-order methods for entropic-regularized OT, in this article we propose the SSNS algorithm, short for safe and sparse Newton method for Sinkhorn-type OT. It has two central components: a new scheme to sparsify the Hessian matrix, and a Newton-type algorithm to update iterates.

### Sparsifying the Hessian matrix

Ideally, the sparsified Hessian matrix, denoted by \(H_{}(x)\), should meet two criteria: first, it should be close to the true \(H(x)\) with a tunable approximation error; second, it needs to preserve the positive definiteness of the original Hessian matrix. The first point is to ensure that \(H_{}(x)\) does not lose too much information of the true \(H(x)\). The second point is especially important in implementing the Newton method, since a linear system \([H_{}(x)]d=-g(x)\) needs to be solved to compute the search direction \(d\), and one needs to make sure that \(H_{}(x)\) is invertible. These two points are briefly mentioned in  as an implementation practice, but without a strong theoretical guarantee.

To this end, we introduce a new adaptive method to sparsify \(H(x)\), which takes the two points above as _first principles_ in both theoretical analysis and practical implementation. To describe the whole algorithm, first define an operator select_small\((v,)\), which takes a vector \(v=(v_{1},,v_{n})^{T}\) and a scalar \(>0\) as inputs, and outputs a mask vector \(=(_{1},,_{n})^{T}\) in the following way. Suppose that \(v_{(1)} v_{(2)} v_{(n)}\) are the sorted values of \(v\), where \((s)\) is the original index of the \(s\)-th smallest element in \(v\). We also define \(^{-1}()\) to be the inverse of \(()\), _i.e._, \(^{-1}(i)\) is the integer such that \((^{-1}(i))=i\). Let \(S\) be the largest integer such that \(_{s=1}^{S}v_{(s)}\). Then we set \(_{i}=1\) if \(^{-1}(i) S\), and \(_{i}=0\) otherwise. For example, if \(v=(2,1,3,5,2)^{T}\) and \(=6\), then we have \(=(1,1,0,0,1)^{T}\), since \(1+2+2\) but \(1+2+2+3>\).

Next, define an operator apply_mask\((v,)\), which inputs a vector \(v=(v_{1},,v_{n})^{T}\) and a mask \(=(_{1},,_{n})^{T}\), and outputs a vector \(u=(u_{1},,u_{n})^{T}\) with \(u_{i}=_{i} v_{i}\). For the example above, we have apply_mask\((v,)=(2,1,0,0,2)^{T}\). Then given the current dual variables \(x=(^{T},^{T})^{T}\), Algorithm 1 outputs a sparse matrix \(H_{}\) as an approximation to the true Hessian matrix \(H(x)\).

It is worth noting that in the construction of \(H_{}\), the diagonal elements \((T_{m})\) and \((^{T}_{n})\) use the _original_\(T\) matrix, whereas the off-diagonal elements are the sparsified \(T_{}\) matrix, with the last column removed. In this way, Theorem 2 shows that \(H_{}\) indeed has a well-controlled approximation error as requested by our first criterion.

**Theorem 1** (Approximation error of sparsification).: _Let \(H_{}\) be output by Algorithm 1 with a given vector \(x\), and define \(D=H(x)-H_{}\). Then for any \( 0\), we have \(D 0\), where the inequality sign holds elementwisely, and for each \(i,j=1,2,,n+m-1\),_

\[\|D_{i}\!\!\|_{1}^{-1},\|D\!\!\|_{1}^{-1 }.\]

_Moreover, \(\|D\|^{-1}\)._Theorem 1 shows that all the row-wise and column-wise \(_{1}\)-norms of the residual matrix \(D\) is bounded by the pre-specified threshold \(\), up to a constant multiplier \(^{-1}\). More importantly, the overall operator norm of \(D\) also has the same upper bound, and this property plays a key role in analyzing the convergence behavior of the SSNS algorithm later in Section 3.2.

Next, Theorem 2 validates our second criterion on \(H_{}\): the sparsified Hessian matrix by Algorithm 1 is guaranteed to be positive definite, _regardless of_ the threshold parameter \(\). Therefore, it can be _safely_ used to compute the Newton search directions.

**Theorem 2** (Positive definiteness).: _Suppose that \(n m\). For any \(^{n}\), \(^{m-1}\), and \(M^{n m}\), let \(T=(,)\), \(r=T_{m}\), and \(c=T^{T}_{n}\). Then for any \( 0\), the matrix \(H_{}\) output by Algorithm 1 is always positive definite. Specifically, we have_

\[_{}(H_{}) 2^{-1}\{\|r\|_{},\|c\|_{ }\},_{}(H_{})^{-1} _{i,j}T_{ij},\]

_where \(_{}()\) and \(_{}()\) represent the largest and smallest eigenvalues of a symmetric matrix, respectively._

The two features presented by Theorem 1 and Theorem 2 are crucial in designing the optimization algorithm and analyzing its theoretical properties, which we introduce in details in Section 3.2.

### The SSNS algorithm

In Theorem 2, we have shown that the sparsified Hessian matrix \(H_{}\) is always positive definite, thus invertible. However, in actual computation, \(H_{}\) may be nearly singular, thus causing numerical instability. To further stabilize the optimization procedure, we propose a safe and sparse Newton method inspired by the regularized Newton method  and the Levenberg-Marquardt algorithm [16; 22].

One of the key ingredients of SSNS is to introduce a shift parameter \(\) in solving the Newton linear system, leading to a search direction of the form \(p=-(H_{}+ I)^{-1}g(x)\) in each iteration. The shift parameter \(\) has multiple functions. First, it stabilizes the vanilla Newton linear system \(H_{}^{-1}g(x)\), as the matrix \(H_{}+ I\) has a smaller condition number than \(H_{}\), which potentially accelerates iterative linear solves such as the CG method. Second, if we allow \(\) to vary along iterations, \(\) has the effect of adjusting the search direction when \(-H_{}^{-1}g(x)\) is not a good candidate. Third, as we will later show in the convergence analysis, an adaptive \(\) plays an important role in both the global and local convergence of the algorithm.

On the other side, the sparsification threshold \(\) is also essential to the design of the algorithm, as we need to control the approximation error of \(H_{}\) during the iterations. Intuitively, when the current iterate \(x_{k}\) is close to the optimum, \(H_{}\) should be also close to the true Hessian matrix \(H(x_{k})\) to achieve the fast convergence rate, whereas when \(x_{k}\) is far away, it may be beneficial to use a large \(\), since \(H_{}\) is then more sparse and leads to a faster computation of the linear system \(-(H_{}+ I)^{-1}g(x)\).

Overall, in SSNS we allow \(\) and \(\) to vary in each iteration, and use the notation \(_{k}\) and \(_{k}\) to reflect this adaptivity. Intuitively, both \(_{k}\) and \(_{k}\) cause approximation errors to the true Hessian matrix in determining the Newton search direction, so they should tend to zero when \(x_{k}\) is sufficiently close to the optimum. On the other hand, we do not want \(_{k}\) and \(_{k}\) to decay too fast, since otherwise they make \(H_{}\) too dense and the linear system may be ill-conditioned. To this end, we set their values according to the current gradient norm \(\|g(x_{k})\|\), which can be viewed as an indicator of the distance between \(x_{k}\) and the optimum \(x^{*}\). Specifically, we set \(_{k}=_{k}\|g(x_{k})\|\) and \(_{k}=_{0}\|g_{k}\|^{}\), where \(_{k}\) is a parameter that dynamically changes in each iteration, and \(_{0}>0\) and \( 1\) are two constants. These settings are justified by the convergence analysis later in Theorems 3 and 5.

Before presenting the full details, we first introduce a few functions and quantities that play important roles in the algorithm. In each iteration, SSNS computes a quantity \(_{k}\),

\[_{k}=)-f(x_{k}+_{k}p_{k})}{m_{k}(0)-m_{k}(_{k}p_{k})},\]

where \(_{k}>0\) is a step size parameter, and the \(m_{k}()\) function is a local quadratic approximation to the objective function:

\[m_{k}(p)=f(x_{k})+[g(x_{k})]^{T}p+p^{T}H_{_{k}}p.\] (6)

The quantity \(_{k}\) has two functions: first, it adjusts the \(_{k}\) parameter, and second, it also determines whether one should accept the proposed new iterate \(x_{k}+_{k}p_{k}\). A negative value of \(_{k}\) means that the objective function actually increases at the new iterate, so one should reject the proposed point, and increase \(_{k}\) in the next iteration for a potentially better search direction. The overall SSNS algorithm is then given in Algorithm 2.

```
0: Initial point \(x_{0}\), parameters \(\{_{0},_{0},c_{l},c_{u},\}>0\), \( 1\), \(_{0}(0,)\), \(_{tol}>0\)
0: Default values:\(_{0}=1\), \(_{0}=0.01\), \(c_{l}=0.1\), \(c_{u}=1\), \(=0.001\), \(=1\), \(_{0}=\)
0:\(x_{k}\)
1:for\(k=0,1,2,\)do
2: Compute \(g_{k}=g(x_{k})\), \(_{k}=_{0}\|g_{k}\|^{}\)
3:if\(\|g_{k}\|<_{tol}\)then
4:return\(x_{k}\)
5: Compute \(H_{_{k}}\) according to Algorithm 1 with \(x x_{k}\)
6: Compute \(p_{k}=-(H_{_{k}}+_{k}\|g_{k}\|I)^{-1}g_{k}\)
7: Select any \(_{k}[c_{l},c_{u}]\)
8: Compute \(_{k}=)-f(x_{k}+_{k}p_{k})}{m_{k}(0)-m_{k}(_{k}p_{k})}\), \(m_{k}()\) is defined in (6)
9: Update \(_{k+1}=4_{k},&_{k}<_{0}\\ \{_{k}/2,\},&_{k} 1-_{0}\\ _{k},&\)
10:if\(_{k}>0\)then
11:\(x_{k+1}=x_{k}+_{k}p_{k}\)
12:else
13:\(x_{k+1}=x_{k}\) ```

**Algorithm 2** Safe and sparse Newton method for Sinkhorn-type optimal transport.

### Step size selection

It is worth noting that in Algorithm 2, the step size \(_{k}\) can be taken an _arbitrary_ value from a fixed interval \([c_{l},c_{u}]\). This is quite different from typical line search methods, which require the new iterate \(x_{k}+_{k}p_{k}\) to satisfy certain conditions, such as the sufficient decrease condition and the curvature condition . In other words, it is one of the advantages of SSNS that the cost of step size selection is predictable: we can consider a _fixed_ number of candidates, and select one of them according to some criterion. At one extreme, it is completely acceptable to always set \(_{k}=1\). However, in practice, we advocate Algorithm 3 to heuristically choose \(_{k}\), which considers a fixed number of candidates, and computes the objective function value for each candidate. The algorithm will early stop if a decrease in function value is found. If all candidates increase the function value, then return the step that results in the smallest function value. Based on our empirical results, setting \((_{},_{},_{},_{})=(1,0.5,0.25,0.1)\) gives reasonably good performance in most numerical experiments.

## 4 Theoretical Guarantees on Convergence

In this section, we present our major theoretical contributions to the sparsified Newton method for entropic-regularized OT. We first show that starting from an arbitrary initial point \(x_{0}\), the iterates generated by Algorithm 2 eventually converge to the unique global optimum \(x^{*}\). Therefore, SSNS does not require a warm initialization using the Sinkhorn algorithm as in .

**Theorem 3** (Global convergence guarantee).: _Let \(\{x_{k}\}\) be generated by Algorithm 2, and \(x^{*}\) is an optimal point of (4). Then either Algorithm 2 terminates in finite iterations, or \(x_{k}\) satisfies_

\[_{k}\|g(x_{k})\|=0,_{k}\|x_{k}-x^{*}\|=0.\]

The next theorem characterizes the behavior of SSNS when the iterates are sufficiently close to the optimum. Since in Algorithm 2, the proposed new iterate \(x_{k}+_{k}p_{k}\) is rejected if \(_{k} 0\), it is important to show that such rejections will be very rare as the algorithm proceeds.

**Theorem 4**.: _There exists an integer \(K>0\) such that for all \(k K\), \(_{k} 1-_{0}\), \(_{k+1}\), and \(x_{k+1}=x_{k}+_{k}p_{k}\)._

Theorem 4 indicates that eventually the parameter \(_{k}\) is upper bounded by the pre-specified value \(\), making the shift parameter of the Newton method, namely, \(_{k}\|g_{k}\|\), tend to zero. This is consistent with the intuition we have described in Section 3.2. More importantly, the quantity \(_{k}\) will also be greater than the threshold \(1-_{0}\), so the new step proposed by the Newton direction will always be accepted, regardless of the bounds \(c_{l}\) and \(c_{u}\). This makes SSNS quite flexible in picking the step sizes, unlike classical line search algorithms that pose various conditions that need to be satisfied.

Finally, Theorem 5 shows that SSNS indeed has a quadratic local convergence rate that matches the Newton method based on genuine and dense Hessian matrices.

**Theorem 5** (Quadratic local convergence rate).: _Fix \(_{k} 1\). Then there exists an integer \(K^{}>0\) and a constant \(L>0\) such that for all \(k K^{}\),_

\[\|x_{k+1}-x^{*}\| L\|x_{k}-x^{*}\|^{2}.\]

Overall, Theorems 3 to 5 provide solid theoretical guarantees on the proposed SSNS algorithm, which are a major complement to the existing literature on solving entropic-regularized OT using realistic second-order methods. More importantly, they validate that SSNS is indeed a "safe" algorithm that is quite robust to the choice of initial value, step sizes, strength of sparsification, and hyperparameters.

## 5 Numerical Experiments

In this section, we test the performance of the proposed SSNS algorithm on various numerical experiments3. There are a huge number of algorithms developed for entropic-regularized OT, and we focus on representative ones from each category of optimization methods: 1. the Sinkhorn algorithm as the default option for entropic-regularized OT, interpreted as a BCD method; 2. the APDAGDalgorithm for accelerated first-order method; 3. the L-BFGS algorithm for quasi-Newton method; 4. the globalized Newton method with line search; 5. the proposed SSNS algorithm.

We first consider three benchmark datasets to define the OT problem, each with two ways of constructing the cost matrices. More explanations of the experiment setting are given in Section A.2.

* **(Fashion-)MNIST**: OT between a pair of images from the MNIST  or Fashion-MNIST  dataset. The \(a\) and \(b\) vectors are flattened and normalized pixel values, and the cost matrix holds the \(_{1}\)-distances or squared Euclidean distances between individual pixels. The problem size is \(n=m=784\).
* **ImageNet**: OT between two categories of images from the ImageNet dataset . We use a subset of ImageNet from the Imagenette Github repository4, which contains ten classes of ImageNet images. Approximately 1000 images per category are selected. We map each image to a 30-dimensional feature vector by first passing the image to a ResNet18 network, resulting in a 512-dimensional vector, then followed by a dimension reduction by principal component analysis. Let \(x_{i}^{30}\) be the feature vector of an image in the first category, \(i=1,,n\), and \(y_{j}^{30}\) be the feature vector of an image in the second category, \(j=1,,m\). Then \(a=n^{-1}_{n}\), \(b=m^{-1}_{m}\), and the cost matrix is \(M_{ij}=\|x_{i}-y_{j}\|_{1}\) or \(M_{ij}=\|x_{i}-y_{j}\|^{2}\). The problem size is \(n m 1000\). 
To make the regularization parameter \(\) comparable in different settings, we normalize all cost matrices to have unit infinity norms, namely, \(M M/\|M\|_{}\). Then we consider two settings of the regularization parameter, \(=0.01\) and \(=0.001\).

For entropic-regularized OT, a commonly-used criterion to evaluate optimality is the marginal error of the estimated transport plan. Let \(x_{k}=(_{k}^{T},_{k}^{T})^{T}\) be the current iterate for dual variables, and set \(T_{k}=(_{k},_{k})\). Then the marginal error is given by \(_{m}-a\|^{2}+\|T_{k}^{T}_{n}-b\|^{2}}\). Coincidentally, this is exactly the current gradient norm \(\|g(x_{k})\|\) as indicated by (5). Figure 1 shows the plots of marginal errors against iteration number or run time for different algorithms on the three benchmark datasets.

From the top row of Figure 1, it is clear that second-order methods have much faster convergence speed compared to first-order and quasi-Newton methods. However, for the vanilla Newton method, this advantage is weakened by its high per-iteration cost, resulting in less competitive run time performance as shown in the bottom row of Figure 1. The SSNS algorithm avoids this issue by using sparse matrix operations, and hence for the run time results, it still shows an order of magnitude speedup for MNIST and Fashion-MNIST data. The run time for SSNS is relatively longer in the ImageNet dataset, mostly because the transport plan is more dense under \(=0.01\). We show in

Figure 1: Top: Marginal error vs. iteration number for different algorithms on three datasets. Bottom: Marginal error vs. run time. The cost matrix is based on the \(_{1}\)-distance, and \(=0.01\).

Figure 2 that as \(\) becomes smaller, the Hessian matrix can be better approximated by a sparse matrix, thus enlarging the advantage of SSNS. This point is consistent with Theorem 1 of , which states that smaller \(\) in general results in better sparse approximation. More discussions on the impact of regularization parameter is given in Section A.3.

We also note that with \(=0.001\), the Newton method encounters numerical issues in solving the linear systems: the Hessian matrix is nearly singular, and the line search procedure fails along the computed search direction. Therefore, the Newton method does not generate any valid results in Figure 2. This issue further validates our intuition in Section 3.2: when \(H(x)\) is nearly singular, \(-[H(x)]^{-1}g(x)\) may not be a good search direction, and the introduction of the shift parameter \(\) in SSNS may give a better candidate \(-(H_{}+ I)^{-1}g(x)\).

In the ImageNet experiments above, we map each image to a 30-dimensional feature vector to compute the cost matrix. To study the impact of the feature dimension, in Section A.4, we conduct additional experiments with \(d=60,90,200,300,500\), under the same setting as in Figure 2.

Next, we show in Figure 3 the results for cost matrices based on squared Euclidean distances. The implications are similar: the Newton method fails in the ImageNet dataset, and costs too much run time in the other two datasets. In contrast, SSNS shows great computational advantage. The experiment results for more test cases are given in Section A.5 in the appendix.

Finally, to study the scalability of SSNS, we consider the following synthetic OT problem that can generate data with arbitrary dimensions.

* **Large-scale synthetic data**: The basic setting is to approximate the OT between two continuous distributions: the source is an exponential distribution with mean one, and the target is a normal mixture distribution \(0.2 N(1,0.2)+0.8 N(3,0.5)\). We discretize the problem in the following way: let \(x_{i}=5(i-1)/(n-1)\), \(i=1,,n\), and \(y_{j}=5(j-1)/(m-1)\), \(j=1,,m\), which are equally-spaced points on . Define the cost matrix as \(M_{ij}=(x_{i}-y_{j})^{2}\). Let \(f_{1}\) and \(f_{2}\) be the density functions of the source and target distributions, respectively. Then we set \(_{i}=f_{1}(x_{i})\), \(_{j}=f_{2}(y_{j})\), \(a_{i}=_{i}/(_{k=1}^{n}_{k})\), and \(b_{j}=_{j}/(_{k=1}^{m}_{k})\).

Similar to the previous experiment setting, we normalize the cost matrix, \(M M/\|M\|_{}\), and set \(=0.001\). We then solve the problem at the scales of \(n=m=1000,5000\), and \(10000\), but only considering BCD and SSNS, as other methods are too time-consuming to proceed. The results are visualized in Figure 4, whose pattern is clear: BCD demonstrates a linear-like convergence rate, and SSNS has a fast convergence speed consistent with the theoretical quadratic rate. Thanks to the Hessian sparsification, SSNS does not suffer from a high per-iteration cost, so overall it provides an efficient solver for entropic-regularized OT even on very large problems.

Figure 2: Top: Marginal error vs. iteration number for different algorithms on three datasets. Bottom: Marginal error vs. run time. The cost matrix is based on the \(_{1}\)-distance, and \(=0.001\).

## 6 Conclusion

In this article, we have carefully designed and analyzed a new variant of the sparsified Newton method for solving large-scale entropic-regularized OT problems. As demonstrated by , the sparsified Newton method typically enjoys visibly faster convergence speed than first-order methods, and meanwhile maintains a comparable computational cost per iteration. This paper substantially complements the existing literature on sparsified Newton method on the following aspects. First, a new Hessian sparsification method is developed, which nicely interacts with the convergence theory. Second, based on this scheme, we prove that the sparsified Hessian matrix is always positive definite, thus promising a safe Newton direction computation. Third, we have a thorough theoretical analysis of the proposed SSNS algorithm, which affirmatively answers a previous conjecture made in  on the convergence speed. Finally, our algorithm features an easy and clear implementation that does not require warm initialization or heavy hyperparameter tuning, thus being user-friendly for practical use.

LimitationsA potential limitation of the proposed method is that for some specific OT problems, the transport plan may not be well approximated by a sparse matrix. In such cases, the Hessian matrix may be too dense even after the adaptive sparsification step.

Figure 4: Comparing BCD and SSNS on large OT problems. Top: Marginal error vs. iteration number for different problem sizes. Bottom: Marginal error vs. run time.

Figure 3: Top: Marginal error vs. iteration number for different algorithms on three datasets. Bottom: Marginal error vs. run time. The cost matrix is based on the squared Euclidean distance, and \(=0.001\).