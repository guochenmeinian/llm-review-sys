# Multi-Player Zero-Sum Markov Games with

Networked Separable Interactions

 Chanwoo Park

MIT

cpark97@mit.edu

&Kaiqing Zhang

University of Maryland, College Park

kaiqing@umd.edu

&Asuman Ozdaglar

MIT

asuman@mit.edu

Alphabetical order.

###### Abstract

We study a new class of Markov games, _(multi-player) zero-sum Markov Games_ with _Networked separable interactions_ (zero-sum NMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define a zero-sum NMG as a model where the payoffs of the auxiliary games associated with each state are zero-sum and have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as a zero-sum NMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the product of per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov _stationary_ CCE in infinite-horizon discounted zero-sum NMGs is PPAD-hard, unless the underlying network has a "star topology". Then, we propose fictitious-play-type dynamics, the classical learning dynamics in normal-form games, for zero-sum NMGs, and establish convergence guarantees to Markov stationary NE under a star-shaped network structure. Finally, in light of the hardness result, we focus on computing a Markov _non-stationary_ NE and provide finite-iteration guarantees for a series of value-iteration-based algorithms. We also provide numerical experiments to corroborate our theoretical results.

## 1 Introduction

Nash Equilibrium (NE) has been broadly used as a solution concept in game theory, since the seminal works of . Perhaps equally important, NE is also deeply rooted in the prediction and analysis of _learning dynamics_ in multi-agent strategic environments: it may appear as a natural outcome of many non-equilibrating learning processes of multiple agents interacting with each other . A prominent example of such learning processes is fictitious play (FP) , in which myopic agents estimate the opponents' play using history, and then choose a best-response action (based on their payoff matrix) against this estimate, as if the opponents use it as their stationary strategy. The focus of these studies has initially been on the convergence to NE in zero-sum games (see , and also ), and in games with aligned objective (identical-interest and potential games, see ). Since then, FP has been shown to converge to NE in more important classes of games, including \(2n\) games , "one-against-all" games , and zero-sum polymatrix games , justifying the prediction power of NE in learning in normal-form/matrix games.

Some of these results have recently been extended to the _stochastic game_ (also known as the _Markov_ game (MG)) setting, a model for multi-agent sequential decision-making with state transition dynamics, first introduced in . In particular,  have studied best-response-type learning dynamics in two-player zero-sum MGs, and  have studied that in multi-playeridentical-interest games. Following the same path as studying matrix games, one natural question arises: _Are there other classes of MGs beyond two-player zero-sum and identical-interest cases that allow natural learning dynamics, e.g., fictitious play, to justify NE as the long-run emerging outcome?_

On the other end of the spectrum, it is well-known that for _general-sum_ normal-form games, the special case of MGs without the state transition dynamics, _computing_ an NE is intractable . Relaxed solution concepts as (coarse) correlated equilibrium ((C)CE) have thus been favored when it comes to equilibrium computation for general-sum, multi-player games . Encouragingly, when the interactions among players have some networked separable structure, also known as being _polymatrix_, computing NE may be made tractable even in multi-player settings. This has been instantiated in the seminal works  in the normal-form game setting, which showed that any CCE collapses to the NE in such games when the payoffs are zero-sum. Thus, any algorithms that can efficiently compute the CCE in such games will lead to the efficient computation of the NE.

In fact, besides being of theoretical interest, multi-player zero-sum games with networked separable interactions also find a range of applications, including security games , fashion games , and resource allocation problems . These examples, oftentimes, naturally involve some _state_ transition that captures the dynamics of the evolution of the environment in practice. For example, in the security game, the protection level or immunity of a target increases as a function of the number of past attacks, leading to a smaller probability of an attack on the target being successful. Hence, it is imperative to study such multi-player zero-sum games with state transitions. As eluded in the recent results , such a transition from _stateless_ to _stateful_ cases may not always yield straightforward and expected results, e.g., computing stationary CCE can be computationally intractable in stochastic games, in stark contrast to the normal-form case where CCE can be efficiently computed. This naturally prompts another question: _Are there other types of (multi-player) MGs that may circumvent the computational hardness of computing NE/CCE?_

In an effort to address these two questions, we introduce a new class of Markov games - _(multi-player) zero-sum Markov games with networked separable interactions_ (zero-sum NMGs). We summarize our contributions as follows, and defer a more detailed literature review to Appendix A.

Contributions.First, we introduce a new class of non-cooperative Markov games: (multi-player) zero-sum MGs with Networked separable interactions (zero-sum NMGs), wherein the payoffs of the _auxiliary-games_ associated with each state, i.e., the sum of instantaneous reward and expectation of any estimated state-value functions, possess the multi-player zero-sum and networked separable (i.e., polymatrix) structure as in  for normal-form games, a strict generalization of the latter. We also provide structural results on the reward and transition dynamics of the game, as well as examples of this class of games. Specifically, for a Markov game to qualify as a zero-sum NMG, if and only if its reward function has the zero-sum polymatrix structure, and its transition dynamics is an _ensemble_ of multiple single-controller transition dynamics that are sampled randomly at each state (see Remark 3 for more details). This transition dynamics covers the common ones in the MG literature, including the single-controller and turn-based dynamics. Second, we show that Markov CCE and Markov NE _collapse_ in that the product of per-state marginal distributions of the former yields the latter, making it sufficient to focus on the former in equilibrium computation. We then show the PPAD-hardness  of computing the _Markov stationary_ equilibrium, a natural solution concept in infinite-horizon discounted MGs, unless the underlying network has a star-topology. This is in contrast to the normal-form case where CCE is always computationally tractable. Third, we study the fictitious-play property  of zero-sum NMGs, showing that the fictitious-play dynamics  converges to the Markov stationary NE, for zero-sum NMGs with a star-shaped network structure. Finally, in light of the hardness of computing stationary equilibria, we develop a series of value-iteration-based algorithms for computing a Markov _non-stationary_ NE of zero-sum NMGs, with finite-iteration guarantees. We also provide numerical experiments to corroborate our theoretical results in Section G. We hope our results serve as a starting point for studying this networked separable interaction structure in non-cooperative Markov games.

Notation.For a real number \(c\), we use \((c)_{+}\) to denote \(\{c,0\}\). For an event \(\), we use \(()\) to denote the indicator function such that \(()=1\) if \(\) is true, and \(()=0\) otherwise. We define multinomial distribution with probability \((w_{i})_{i}\) as \(((w_{i})_{i})\). We denote the uniform distribution over a set \(\) as \(()\). We denote the Bernoulli distribution with probability \(p\) as \((p)\). The sgn function is defined as \((x)=2(x 0)-1\). The KL-divergence between two probability distributions \(p,q\) is denoted as \((p,q)=_{p}[(p/q)]\). For a graph \(=(,)\), we denote the set of neighboring nodes of node \(i\) as \(_{i}\) (without including \(i\)). The maximum norm of a matrix \(X^{m n}\), denoted as \(\|X\|_{}\), is defined as \(\|X\|_{}:=_{i[m],j[n]}|X_{i,j}|\).

Preliminaries

### Markov games

We define a Markov game as a tuple \((,,,H,(_{h})_{h[H]},(r_{h,i})_{h[ H],i},)\), where \(=[n]\) is the set of players, \(\) is the state space with \(||=S\), \(_{i}\) is the action space for player \(i\) with \(|_{i}|=A_{i}\) and \(=_{i}_{i}\), \(H\) is the length of the horizon, \(_{h}:()\) captures the state transition dynamics at timestep \(h\), \(r_{h,i}[0,R]\) is the reward function for player \(i\) at timestep \(h\), bounded by some \(R>0\), and \((0,1]\) is a discount factor. An MG with a finite horizon (\(H<\)) is also referred to as an _episodic_ MG, while an MG with an infinite horizon (\(H=\)) and \(<1\) is referred to as an infinite-horizon \(\)-discounted MG. When \(H=\), we will consider the transition dynamics and reward functions, denoted by \(\) and \((r_{i})_{i}\), respectively, to be independent of \(h\). Hereafter, we may use _agent_ and _player_ interchangeably.

**Policy.** Consider the stochastic Markov policy for player \(i\), denoted by \(_{i}\), as \(_{i}:=\{_{h,i}:(_{i})\}_{h[H]}\). A _joint_ Markov policy is a policy \(:=\{_{h}:()\}_{h[H]}\), where \(_{h}:()\) decides the joint action of all players that can be potentially correlated. A joint Markov policy is a _product_ Markov policy if \(_{h}:_{i}(_{i})\) for all \(h[H]\), and is denoted as \(=_{1}_{2}_{n}\). When the policy is independent of \(h\), the policy is called a _stationary_ policy. We let \(_{h}\) denote the joint action of all agents at timestep \(h\). Unless otherwise noted, we will work with Markov policies throughout. We denote \((s)()\) as the joint policy at state \(s\).

**Value function.** For player \(i\), the value function under joint policy \(\), at timestep \(h\) and state \(s_{h}\) is defined as \(V^{}_{h,i}(s_{h}):=_{}_{h^{}=h}^{H}^{h^ {}-h}r_{h^{},i}(s_{h^{}},_{h^{}})\,s_{h} ,\) which denotes the expected cumulative reward for player \(i\) at step \(h\) if all players adhere to policy \(\). We also define \(V^{}_{h,i}():=_{s_{h}}[V^{}_{h,i}(s_{h})]\) for some state distribution \(()\). We denote the \(Q\)-function for the \(i\)-th player under policy \(\), at step \(h\) and state \(s_{h}\) as \(Q^{}_{h,i}(s_{h},_{h}):=_{}_{h^{}=h}^{ H}^{h^{}-h}r_{h^{},i}(s_{h^{}},_{h^{}}) \,s_{h},_{h},\) which determines the expected cumulative reward for the \(i\)-th player at step \(h\), when starting from the state-action pair \((s_{h},_{h})\). For the infinite-horizon discounted setting, we also use \(V^{}_{i}\) and \(Q^{}_{i}\) to denote \(V^{}_{1,i}\) and \(Q^{}_{1,i}\) for short, respectively.

**Approximate equilibrium.** Define an \(\)-approximate _Markov perfect Nash equilibrium_ as a product policy \(\), which satisfies \(_{i}_{_{i}((_{i}))^{|S| H}} (V^{_{i},^{-i}-}_{h,i}()-V^{}_{h,i}())\) for _all_\(()\) and \(h[H]\), where \(_{-i}\) represents the marginalized policy of all players except player \(i\). Define an \(\)-approximate _Markov coarse correlated equilibrium_ as a joint policy \(\), which satisfies \(_{i}_{_{i}((_{i}))^{|S| H}} (V^{_{i},^{-i}}_{h,i}()-V^{}_{h,i}())\) for _all_\(()\) and \(h[H]\). In the infinite-horizon setting, they can be equivalently defined as satisfying \(_{s}_{i}_{_{i}(( _{i}))^{|S|}}(V^{_{i},^{-i}-}_{i}(s)-V^{}_{i}(s))\). If the above conditions only hold for certain \(\) and \(h=1\), we refer to them as Markov _non-perfect_ NE and CCE, respectively. Unless otherwise noted, we hereafter focus on Markov perfect equilibria, and sometimes refer to them simply as _Markov equilibria_ when it is clear from the context. In the infinite-horizon setting, if additionally, the policy is _stationary_, then they are referred to as a _Markov stationary_ NE and CCE, respectively.

### Multi-player zero-sum games with networked separable interactions

As a generalization of _two-player_ zero-sum matrix games, _(multi-player)_ zero-sum polymatrix games have been introduced in [31; 28; 23; 24]. A _polymatrix game_, also known as a _separable network game_ is defined by a tuple \((=(,_{r}),=_{i} _{i},(r_{i,j})_{(i,j)_{r}})\). Here, \(\) is an undirected connected graph where \(=[n]\) denotes the set of players and \(_{r}\) denotes the set of edges describing the rewards' networked structures, where the graph neighborhoods represent the interactions among players. For each edge, a two-player game is defined for players \(i\) and \(j\), with action sets \(_{i}\) and \(_{j}\), and reward functions \(r_{i,j}:_{i}_{j}\), and similarly for \(r_{j,i}\). The reward for player \(i\) for a given joint action \(=(a_{i})_{i}_{i}_{i}\) is calculated as the sum of the rewards for all edges involving player \(i\), that is, \(r_{i}()=_{j:(i,j)_{r}}r_{i,j}(a_{i},a_{j})\). To be consistent with our terminology later, hereafter, we also refer to such games as _(multi-player) Games with Networked separable interactions (NGs)_.

In a _zero-sum_ polymatrix game (i.e., a (multi-player) _zero-sum_ Game with Networked separable interactions (zero-sum NG)), the sum of rewards for all players at any joint action \(=(a_{i})_{i}_{i}_{i}\) equals zero, i.e., \(_{i}r_{i}()=0\). One can define the policy of agent \(i\), i.e., \(_{i}(_{i})\), so that the agent takes actions by sampling \(a_{i}_{i}()\). Note that \(_{i}\) can be viewed as the reduced case of the policy defined in Section 2.1 when \(=\) and \(H=1\). The expected reward for player under \(\) can then be computed as:

\[r_{i}():=_{j:(i,j)_{r}}_{a_{i}_{i},a_{j} _{j}}r_{i,j}(a_{i},a_{j})_{i}(a_{i})_{j}(a_{j})= _{i}^{}_{i},\] (1)

where \(_{i}\) denotes the matrix \(_{i}:=(r_{i,1},,r_{i,(i-1)},,r_{i,(i+1)},,r_{i,n}) ^{|_{i}|_{i}|_{i}|}\) and \(:=(_{1}^{},_{2}^{},,_{n}^{})^{}^{_{i}|_{i}|}\). We define \(:=(_{1}^{},_{2}^{},,_{n}^{ })^{}^{_{i}|_{ i}|_{i}|_{i}|}\). Then in this case we have \(_{i}r_{i}()=0\) for any policy \(\). See more prominent application examples of zero-sum polymatrix games in [23; 24].

## 3 Multi-Player (Zero-Sum) MGs with Networked Separable Interactions

We now introduce our model of multi-player zero-sum MGs with networked separable interactions.

### Definitions

**Definition 1**.: An infinite-horizon \(\)-discounted MG is called a _(multi-player) MG with Networked separable interactions (NMG)_ characterized by a tuple \((=(,_{Q}),,,,(r_{i})_{i},)\) if for any function \(V:\), defining \(Q_{i}^{V}(s,):=r_{i}(s,)+_{s^{}} (s^{} s,)V(s^{})\), there exist a set of functions \((Q_{i,j}^{V})_{(i,j)_{Q}}\) and an undirected connected graph \(=(,_{Q})\) such that \(Q_{i}^{V}(s,)=_{j_{Q}}Q_{i,j}^{V}(s,a_{i},a_{j})\) holds for every \(i\), \(s\), \(\), where \(_{Q,i}\) denotes the neighbors of player \(i\) induced by the edge set \(_{Q}\) (without including \(i\)). When it is clear from the context, we represent the NMG tuple simply as \(=(,_{Q})\).

A finite-horizon MG is called a _(multi-player) MG with Networked separable interactions_ if for any set of functions \(V:=\{V_{h}\}_{h[H+1]}\) where \(V_{h}:\), defining \(Q_{h,i}^{V}(s,):=r_{h,i}(s,)+_{s^{} }_{h}(s^{} s,)V_{h+1}(s^{})\), there exist a set of functions \((Q_{h,i,j}^{V})_{(i,j)_{Q},h[H]}\) such that \(Q_{h,i}^{V}(s,)=_{j_{Q},Q}Q_{h,i,j}^{V}(s,a_{i},a_{j})\) holds for every \(i\), \(s\), \(s[H]\), \(\).

A (multi-player) NMG is called a _(multi-player) zero-sum MG with Networked separable interactions (zero-sum NMG)_ if additionally \((=(,_{Q}),=_{i }_{i},(r_{i,j}(s):=Q_{i,j}^{}(s))_{(i,j)_{Q}})\) forms a zero-sum NG for all \(s\) in the infinite-horizon \(\)-discounted case, or \((=(,_{Q}),=_{i }_{i},(r_{h,i,j}(s):=Q_{h,i,j}^{}(s))_{(i,j)_{ Q}})\) forms a zero-sum NG for all \(s\) and \(h[H]\) in the finite-horizon case.

Regarding the assumption that the above conditions hold under any (set of) functions \(V\), one may understand this as a structural requirement to inherit the polymatrix structure in the Markov game case. It is natural since \(\{Q_{i}^{V}\}_{i}\) would play the role of the _payoff matrix_ in the normal-form case, when value-(iteration) based algorithms are used to solve the MG. As our hope is to exploit the networked structure in the payoff matrices to develop efficient algorithms for solving such MGs, if we do not know _a priori_ which value function estimate \(V\) will be encountered in the algorithm update, the networked structure may easily break if we do not assume them to hold for _all_ possible \(V\). Moreover, such a definition easily encompasses the normal-form case, by preserving the polymatrix structure of the _reward functions_ (when substituting \(V\) to be a zero function). Some alternative definition (see Remark 2) may not necessarily preserve the polymatrix structure of even the reward functions in a consistent way (see Section B.4 for a concrete example). We thus focus on Definition 1, which at least covers the polymatrix structure of the reduced case regarding only reward functions.

Indeed, such a networked structure in Markov games may be fragile. We now propose both sufficient and _necessary_ conditions for the reward function's structure and the transition dynamics of the MG, to be an NMG. Here we focus on the infinite-horizon discounted setting for a simpler exposition. For finite-horizon cases, a similar statement holds, which is deferred to Appendix B. We also defer the full statement and proof of the following result to Appendix B. We first introduce the definition of decomposability and the set \(_{}\), which will be used in establishing the conditions. For a graph \(=(,)\), we define \(_{C}:=\{i(i,j)j\}\), which may be an empty set if no such node \(i\) exists.

**Definition 2** (Decomposability).: A non-negative function \(f:X^{||}^{+}\{0\}\) is decomposable with respect to a set \(\) if there exists a set of non-negative functions \((f_{i})_{i}\) with \(f_{i}:X^{+}\{0\}\), such that \(f(x)=_{i}f_{i}(x_{i})\) holds for any \(x X^{||}\). A non-negative function \(f:X^{||}^{+}\{0\}\) is decomposable with respect to a set \(=\), if there exists a non-negative constant \(f_{o}\) such that \(f(x)=f_{o}\) holds for any \(x X^{||}\).

**Proposition 1**.: For a given graph \(=(,_{Q})\), an MG \((,,,,(r_{i})_{i},)\) with more than two players is an NMG with respect to \(\) if and only if: (1) \(r_{i}(s,a_{i},)\)**is decomposable with respect to \(_{Q,i}\)** for each \(i,s,a_{i}_{i}\), i.e., \(r_{i}(s,)=_{j_{Q,i}}r_{i,j}(s,a_{i},a_{j})\) for a set of functions \(\{r_{i,j}(s,a_{i},)\}_{j_{Q,i}}\), and (2) **the transition dynamics \((s^{}\,|\,s,)\) is decomposable with respect to** the set \(_{C}\) of this \(\), i.e., \((s^{}\,|\,s,)=_{i_{C}}_{i}(s^{}\,|\,s,a_{i})\) for a set of functions \(\{_{i}(s^{}\,|\,s,)\}_{i_{C}}\) if \(_{C}\), or \((s^{}\,|\,s,)=_{o}(s^{}\,|\,s)\) for some constant function (of \(\)) \(_{o}(s^{}\,|\,s)\) if \(_{C}=\). Moreover, an MG qualifies as a zero-sum NMG if and only if it satisfies an additional condition: the NG, characterized by \((,,(r_{i,j}(s))_{(i,j)_{Q}})\), is a zero-sum NG for all \(s\). In the case of two players, every (zero-sum) Markov game becomes a (zero-sum) NMG.

**Remark 1** (Stronger sufficient condition).: We note that for an MG \((,,,,(r_{i})_{i},)\), if for every agent \(i\), \(r_{i}(s,a_{i},)\) is decomposable with respect to some \(_{r}_{Q}\), and \((s^{}\,|\,s,)\) is decomposable with respect to some \(_{P}_{C}\), then one can still prove the _if_ part, i.e., there exists some \(=(,_{Q})\) such that the game is an NMG with respect to this \(\). See Figure 1 for the illustration. This is because by our definition, being decomposable with respect to a subset implies being decomposable with respect to a larger set, as one can choose the functions \(f_{i}\) for the \(i\) in the complement of the subset to be simply zero. We chose to state as in Proposition 1 just for the purpose of presenting both the _if_ and _only if_ conditions in a concise and unified way.

**Remark 2** (An alternative NMG definition).: Another reasonable definition of NMG may be as follows: if for any _policy_\(\), there exist a set of functions \((Q_{i,j}^{})_{(i,j)_{Q}}\) and an undirected connected graph \(=(,_{Q})\) such that \(Q_{i}^{}(s,)=_{j_{Q,i}}Q_{i,j}^{}(s,a_{i },a_{j})\) holds for every \(i\), \(s\), \(\). Note that such a definition can be useful in developing _policy_-based algorithms (while Definition 1 is more amenable to developing _value_-based algorithms), e.g., policy iteration, policy gradient, actor-critic methods, where the \(Q\)-value under certain _policy_\(\) will appear in the updates and may need to preserve certain decomposability structure, for any policy \(\) encountered in the algorithm updates. However, in this case, we cannot always guarantee the decomposability of \((s^{}\,|\,s,)\) or \(r_{i}(s,a_{i},)\). For example, if we assume that \(r_{i}(s,)=0\) for every \(i\), \(s\), \(\), then \(Q_{i}^{}(s,)=_{j_{Q,i}}0\) and thus \(Q_{i}^{}(s,a_{i},)\) is always decomposable regardless of \((s^{}\,|\,s,)\). However, interestingly, we can show that the decomposability of the transition dynamics and the reward function as in Proposition 1 can still be guaranteed, as long as some _degenerate_ cases as above do not occur. In particular, if there exist no \(i\) and \(s\) such that \(Q_{i}^{}(s,)\) is a constant function of \(\) for any \(\), then the results in Proposition 1 and hence after still hold. We defer a detailed discussion on this alternative definition to Appendix B.

**Remark 3** (Implication of decomposable transition dynamics).: For an MG to be an NMG, by Proposition 1 the transition dynamics should be decomposable, i.e., \((\,|\,s,)=_{j_{C}}_{j }(\,|\,s,a_{j})\) or \((\,|\,s,)=_{o}(s^{}\,|\,s)\). We first focus on the discussion of the former case. Define \(w_{j}(s,a_{j}):=_{s^{}}_{j}(s^{}\,|\,s,a _{j})\). If we fix the value of \(s\) and \(a_{-j}\), then \(w_{j}(s,a_{j})\) has to be the same for different values of \(a_{j}\) due to the fact \(_{j_{C}}w_{j}(s,a_{j})=1\). Also note that by definition, \(w_{j}(s,a_{j})\) does not depend on the choice of \(a_{-j}\). Therefore, such a \(w_{j}(s,a_{j})\) can be written as \(w_{j}(s)\), where \(w_{j}(s)=_{s^{}}_{j}(s^{}\,|\,s,a_{j})\) for all \(a_{j}_{j}\). We can thus rewrite \((_{j})_{j_{C}}\) using some actual probability distributions \((_{j})_{j_{C}}\), such that if \(w_{j}(s) 0\), then we rewrite \(_{j}\) as \(_{j}(s^{}\,|\,s,a_{j})=w_{j}(s)_{j}(s^{}\,| \,s,a_{j})}{w_{j}(s)}=w_{j}(s)_{j}(s^{}\,|\,s,a_{j})\), and if \(w_{j}(s)=0\), we rewrite \(_{j}\) as \(_{j}(s^{}\,|\,s,a_{j})=w_{j}(s)_{j}(s^{}\,|\,s,a _{j})\) for an arbitrary probability distribution \(_{j}(\,|\,s,a_{j})\). Notice that \(_{s^{}}_{j}(s^{}\,|\,s,a_{j})=1\) for any \(j_{C}\). Then, the decomposable transition dynamics can be represented as \((\,|\,s,)=_{j_{C}}w_{j}(s)_{j}(\,|\,s,a_{j})\), i.e., an _ensemble_ of the transition dynamics that is only controlled by single controllers. The model's transition dynamics thus act according to the following two steps: (1) sampling the controller according to the distribution \((w_{i}(s))_{i_{C}}\), and (2) transitioning the state following the sampled controller's dynamics. Such a model has also been investigated under the name of transition dynamics with _additive structures_ in . Note that our model is more general and thus covers the single-controller MG setting , where there is only one agent controlling the transition dynamics at _all_ states. It also covers the setting of turn-based MGs , where in each round, depending on the current state \(s\), the transition dynamics is by turns affected by only one of the agents. This can be captured by the proper choice of \((w_{i}(s))_{i_{C}}\)) that takes value \(1\) only for one agent at each state \(s\) (while takes value \(0\) for all other non-controller agents at each state \(s\)). Additionally, the second case where \((\,|\,s,)=_{o}(s^{}\,|\,s)\) corresponds to the one with no ensemble of controller agents.

**Proposition 2** (Decomposition of \((Q_{i}^{V})_{i}\)).: For an infinite-horizon \(\)-discounted NMG with \(=(,_{Q})\) such that \(_{C}\), if we know that \((s^{}\,|\,s,)=_{i_{C}} _{i}(s^{}\,|\(r_{i}(s,)=_{j_{Q,i}}r_{i,j}(s,a_{i},a_{j})\) for some \(\{_{i}\}_{i_{C}}\) and \(\{r_{i,j}\}_{(i,j)_{Q}}\), then the \(Q^{V}_{i,j}\) given in Definition 1 can be represented as

\[Q^{V}_{i,j}(s,a_{i},a_{j})=r_{i,j}(s,a_{i},a_{j})+_{s^{}}((j_{C})_{j}(s^{} s,a_{j})+ (i_{C})_{i,j}(s)_{i}(s^{} s,a_{ i}))V(s^{})\]

for any non-negative \((_{i,j}(s))_{(i,j)_{Q}}\) such that \(_{j_{Q,i}}_{i,j}(s)=1\), for all \(i\) and \(s\). We call it the _canonical_ decomposition of \(\{Q^{V}_{i}\}_{i}\) when \(Q^{V}_{i,j}\) can be represented as above with \(_{i,j}(s)=1/|_{Q,i}|\) for \(j_{Q,i}\). The case with \(_{C}=\) is deferred to Appendix B.

We introduce this canonical decomposition since the representation of \(Q^{V}_{i,j}\) is in general not unique, and we may use this canonical form to simplify the algorithm design later.

### Examples of multi-player (zero-sum) NMGs

We now provide several examples of (multi-player) MGs with networked separable interactions here and in Section B.5.

Example 1 (Markov fashion games).Fashion games are an intriguing class of games [25; 26; 27] that plays a vital role not only in Economics theory but also in practice. A fashion game is a networked extension of the Matching Pennies game, in which each player has the action space \(_{i}=\{-1,+1\}\), which means _light_ and _dark_ color fashions, respectively, for example. There are two types of players: _conformists_ (\(\)), who prefer to conform to their neighbors' fashion (action), and _rebels_ (\(\)), who prefer to oppose their neighbors' fashion (action). Such interactions with the neighbors are exactly captured by polymatrix games. We denote the interaction network between players as \(=(,)\).

Such a game naturally involves the following state transition dynamics: we introduce the state \(s=\) by setting \(s_{0}=0\) and \(s_{t+1} s_{t}+((a_{t,c})_{c})\), which indicates the _fashion trend_ where \(\) is the set of influencers. The fashion trend favors either light or dark colors if \(s 0\) or \(s<0\), respectively. We can think of dynamics as the impact of the influencers on the fashion trend at time \(t\). For each \((s,)\), the reward function for player \(i\), depending on whether she is a conformist or a rebel, are defined as \(r_{,i}(s,)=_{j_{i}}r_{,i_{j}} (s,a_{i},a_{j})=_{j_{i}}(_{i}|}( (s)=a_{i})+(a_{i}=a_{j}))\) and \(r_{,i}(s,)=_{j_{i}}r_{,i_{j}} (s,a_{i},a_{j})=_{j_{i}}(_{i}|}( (s) a_{i})+(a_{i} a_{j}))\), respectively. This is an NMG as defined in Definition 1. Moreover, if the conformists and rebels constitute a bipartite graph, i.e., the neighbors of a conformist are all rebels and vice versa, it becomes a multi-player constant-sum MG with networked separable interactions, and we can subtract the constant offset to make it a zero-sum NMG.

### Relationship between CCE and NE in zero-sum NMGs

A well-known property for zero-sum NGs is that marginalizing a CCE leads to a NE, which makes it computationally tractable to find the NE [23; 24]. We now provide below a counterpart in the Markov game setting, and provide a more detailed statement of the result in Appendix B.

Proposition 3.: Given an \(\)-approximate Markov CCE of an infinite-horizon \(\)-discounted zero-sum NMG, marginalizing it at each state results in an \(\)-approximate Markov NE of the zero-sum NMG. The same argument also holds for the finite-horizon episodic setting with \((1-)^{-1}\) being replaced by \(H\).

This result holds for both stationary and non-stationary \(\)-approximate Markov CCEs. We defer the proof of Proposition 3 to Appendix B. This proposition suggests that if we can have some algorithms to find an approximate Markov CCE for a zero-sum NMG, we can obtain an approximate Markov NE by marginalizing the approximate CCE at each state. We also emphasize that the _Markovian_ property of the equilibrium policies is important for the result to hold. As a result, the learning algorithm in , which learns an approximate Markov _non-stationary_ CCE with polynomial time and samples, may thus be used to find an approximate Markov _non-stationary_ NE in zero-sum NMGs. However, as the focus of  was the more challenging setting of _model-free learning_, the complexity therein has a high dependence on the problem parameters, and the algorithm can only find non-perfect equilibria. When it comes to (perfect) equilibrium computation, one may exploit the multi-player zero-sum structure of zero-sum NMGs, and develop more natural and faster algorithms to find a Markov non-stationary NE. Moreover, when it comes to _stationary_ equilibrium computation, even Markov CCE is _not_ tractable in general-sum cases . Hereafter, we will focus on approaching zero-sum NMGs from these perspectives.

## 4 Hardness for Stationary CCE Computation

Given the results in Section 3.3, it seems tempting and sufficient to compute the Markov CCE of the zero-sum NMG. Indeed, computing CCE (and thus NE) in zero-sum polymatrix games is known to be tractable . It is thus natural to ask: _Is finding Markov CCE computationally tractable?_ Next, we answer the question with different answers for finding stationary CCE (in infinite-horizon \(\)-discounted setting) and non-stationary CCE (in finite-horizon episodic setting), respectively.

For _two-player_ infinite-horizon \(\)-discounted _zero-sum_ MGs, significant progress in computing/learning the (Markov) stationary NE has been made recently . On the other hand, for _multi-player general-sum_ MGs, recent results in  showed that computing (Markov) stationary CCE can be PPAD-hard and thus believed to be computationally intractable. We next show that this hardness persists in most non-degenerate cases even if one enforces the zero-sum and networked interaction structures in the multi-player case. We state the formal result as follows, whose detailed proof is available in Section C.

**Theorem 1**.: There is a constant \(>0\) for which computing an \(\)-approximate Markov perfect stationary CCE in infinite-horizon \(\)-discounted zero-sum NMGs, whose underlying network structure contains either a triangle or a 3-path subgraph, is PPAD-hard. Moreover, given the PCP for PPAD conjecture , there is a constant \(>0\) such that computing even an \(\)-approximate Markov non-perfect stationary CCE in such zero-sum NMGs is PPAD-hard.

Proof Sketch of Theorem 1.: Due to space constraints, we focus on the case with three players, and the underlying network structure has a triangle subgraph. Proof for the \(3\)-path case is similar and can be found in Section C. We will show that for _any_ general-sum two-player _turn-based_ MG **(A)**, the problem of computing its Markov stationary CCE, which is inherently a PPAD-hard problem , can be reduced to computing the Markov stationary CCE of a three-player zero-sum MG with a triangle structure networked separable interactions **(B)**. Consider an MG **(A)** with two players, players 1 and 2, and reward functions \(r_{1}(s,a_{1},a_{2})\) and \(r_{2}(s,a_{2},a_{1})\), where \(a_{i}\) is the action of the \(i\)-th player and \(r_{i}\) is the reward function of the \(i\)-th player. The transition dynamics is given by \((s^{}\,|\,s,a_{1},a_{2})\). In even rounds, player 2's action space is limited to Noop2, and in odd rounds, player 1's action space is limited to Noop1, where Noop1 is an abbreviation of "no-operation", i.e., the player does not affect the transition dynamics or the reward in that round. We denote player 1's action space in even rounds as \(_{1,}\) and player 2's action space in odd rounds as \(_{2,}\), respectively.

Now, we construct a three-player zero-sum NMG. with a triangle network structure. We set the reward function as \(_{i}(s,)=_{j i}_{i,j}(s,a_{i},a_{j})\) and \(_{i,j}(s,a_{i},a_{j})=-_{j,i}(s,a_{j},a_{i})\). The reward functions are designed so that \(_{i,j}=-_{j,i}\) for all \(i,j\), \(_{1,2}+_{1,3}=r_{1}\), and \(_{2,1}+_{2,3}=r_{2}\), where \(r_{1},r_{2}\) are the reward functions in game **(A)**, by introducing a dummy player, player 3. In even rounds, player 2's action space is limited to Noop2, and in odd rounds, player 1's action space is limited to Noop1. Player 3's action space is always limited to Noop3 in all rounds. The transition dynamics is defined as \(}(s^{}\,|\,s,a_{1},a_{2},a_{3})=(s^{ }\,|\,s,a_{1},a_{2})\), since \(a_{3}\) is always chosen from Noop3. In other words, player 3's action does not affect the rewards of the other two players, nor the transition dynamics, and players 1 and 2 will receive the reward as in the two-player turn-based MG. Also, note that due to the turn-based structure of the game **(A)**, the transition dynamics satisfy the decomposable condition in our Proposition 1, and it is thus a zero-sum NMG. In fact, turn-based dynamics can be represented as an ensemble of single controller dynamics, as we have discussed in Section 3.

Note that the new game **(B)** is still a turn-based game, and thus the Markov stationary CCE is the same as the Markov stationary NE. Also, note that by construction, the equilibrium policies of players \(1\) and \(2\) at the Markov stationary CCE of the game **(B)** constitute a Markov stationary CCE of the game **(A)**. If the underlying network is more general than a triangle, but contains a triangle subgraph, we can specify the reward and transition dynamics of these three players as above, and specify all other players to be dummy players, whose reward functions are all zero, and do not affect the reward functions of these three players, nor the transition dynamics. This completes the proof. 

Figure 2 briefly explains how we may reduce the equilibrium computation problem of **(A)** to that of **(B)**. In fact, a connected graph that does not contain a subgraph of a triangle or a 3-path has to be a _star-shaped_ network (Proposition 7), which is proved in Section C. Hence, by Theorem 1, we know that in the infinite-horizon discounted setting, finding Markov stationary NE/CE/CCE is a computationally hard problem unless the underlying network is star-shaped. This may also imply that _learning_ Markov stationary NE in zero-sum NMGs, e.g., using natural dynamics like fictitious play to reach the NE, can be challenging, unless in the star-shaped case. In turn, one may hope fictitious-play dynamics to converge for star-shaped zero-sum NMGs. We instantiate this idea next in Section 5. Furthermore, in light of Theorem 1, we will shift gear to computing Markov _non-stationary_ NE by utilizing the structure of networked separable interactions, as to be detailed in Section 6.

## 5 Fictitious-Play Property

In this section, we study the fictitious-play property of multi-player zero-sum games with networked separable interactions, for both the matrix and Markov game settings. Following the convention in , we refer to the games in which fictitious-play dynamics converge to the NE as the games that have the _fictitious-play property_. We defer the matrix game case results to Section D, where we have also established convergence of the well-known variant of FP, smooth FP , in zero-sum NGs.

Echoing the computational intractability of computing CCE of zero-sum NMG unless the underlying network structure is star-shaped in the infinite-horizon discounted setting (c.f. Theorem 1), we now consider the FP property in such games. Note that by Proposition 1, \(_{Q}\) is a star-shape if and only if the reward structure is a star shape and \(_{C}=\{1\}\), where player 1 is the center of the star (Figure 2), or there are only two players in zero-sum NMG. There is already existing literature for the latter case , so we focus on the former case, which is a single-controller case where player 1 controls the transition dynamics, i.e., \((s^{}\,|\,s,)=_{1}(s^{}\,|\,s,a_{1})\) for some \(_{1}\). We now introduce the fictitious-play dynamics for such zero-sum NMGs.

Each player \(i\) first initializes her beliefs of other players' policies as uniform distributions, and also initializes her belief of the \(Q\)-value estimates with arbitrary values. Then, at iteration \(k\), player \(i\) takes the _best-response_ action based on her belief of other players' policies \((_{-i}^{(k)}(s^{(k)}))\), and their \(Q\) beliefs \(_{i}^{(k)}(s^{(k)},)\):

\[a_{i}^{(k)}*{argmax}_{a_{i}_{i}}\ _{i}^{(k)}(s^{(k)},e_{a_{i}},_{-i}^{(k)}(s^{(k)})).\]

Then, player \(i\) implements the action \(a_{j}^{(k)}\), observes other players' actions \(a_{-i}^{(k)}\), and updates her beliefs as follows: for each player \(i\), she updates her belief of the opponents' policies as

\[_{-i}^{(k+1)}(s)=_{-i}^{(k)}(s)+(s=s ^{(k)})^{N(s)}(e_{a_{-i}^{(k)}}-_{-i}^{(k)}(s))\]

for all \(s\), with stepsize \(^{N(s)} 0\) where \(N(s)\) is the visitation count for the state \(s\); then if \(i=1\), this player \(1\) updates the belief of \(Q_{1,j}\) for all \(j/\{1\}\) and her own \(_{1}(s,)\) for all \(s\) as

Figure 2: (Left, Middle): PPAD-hardness reduction visualization of \(_{Q}\). (Right): A star-shaped zero-sum NMG.

\[_{1,j}^{(k+1)}(s,a_{1},a_{j})=_{1,j}^{(k)}(s,a _{1},a_{j})\\ +(s=s^{(k)})^{N(s)}r_{1,j}(s,a_{1},a_{j})+ _{s^{}}_{1}(s^{} s,a_{ 1})}{n-1}_{1}^{(k)}(s^{})-_{1,j}^{(k)}(s,a_{ 1},a_{i}),\]

which is based on the canonical decomposition given in Proposition 2, where \(_{1}^{(k)}(s)=_{a_{1}_{1}}_{1}^{(k)}(s,e_{a_{1}},_{-1}^{(k)}(s))\), and \(^{N(s)} 0\) is the stepsize. The agent then updates \(_{1}^{(k+1)}(s,)=_{j/\{1\}} _{1,j}^{(k+1)}(s,a_{1},a_{j})\), for all \(s,\). Otherwise, if \(i 1\), then player \(i\) updates the belief of her \(_{i,1}(s,)\) for all \(s,\) as

\[_{i,1}^{(k+1)}(s,a_{i},a_{1})=_{i,1}^{(k)} (s,a_{i},a_{1})\\ +(s=s^{(k)})^{N(s)}r_{i,1}(s,a_{i},a_{1})+ _{s^{}}_{1}(s^{} s,a_{1}) _{i}^{(k)}(s^{})-_{i,1}^{(k)}(s,a_{i},a_{1}) ,\]

where \(_{i}^{(k)}(s)=_{a_{i}_{i}}_{i}^{(k)}(s,e_{a_{i}},_{-i}^{(k)}(s))\), and we let \(_{i}^{(k+1)}(s,)=_{i,1}^{(k+1)}(s,a_{i},a _{1})\) for these \(i 1\). The overall dynamics are summarized in Algorithm 4, which resembles the FP dynamics for two-player zero-sum  and identical-interest  MGs. Now we are ready to present the convergence guarantees.

**Assumption 1**.: The sequences of step sizes \(^{k}(0,1]}_{k 0}\) and \(^{k}(0,1]}_{k 0}\) satisfy the following conditions: (1) \(_{k=0}^{}^{k}=\), \(_{k=0}^{}^{k}=\), and \(_{k}^{k}=_{k}^{k}=0\); (2) \(_{k}}{^{k}}=0\), indicating that the rate at which the beliefs about \(Q\)-functions are updated is slower than the rate at which the beliefs about policies are updated.

**Theorem 2**.: Suppose Assumption 1 holds and Algorithm 4 visits every state infinitely often with probability \(1\). Then, for a star-shaped multi-player zero-sum NMG, the belief \((^{(k)})_{k 0}\) converges to a Markov stationary NE and the belief \((^{(k)})_{k 0}\) converges to the corresponding NE value of the zero-sum NMG with probability 1, as \(k\).

We defer the proof to Section D.2 due to space constraints. Note that to illustrate the idea, we only present the result for the _model-based_ case, i.e., when the transition dynamics \(\) is known. With this result, it is direct to extend to the model-free and learning case, where \(\) is not known , still using the tool of stochastic approximation . See Section D for more details.

**Remark 4** (Challenges for analyzing general cases).: One might ask why we had to focus on a star-shaped structure. First, for general networked structures, even in the matrix-game case, it is known that the NE _values_ of a zero-sum NG may not be unique . Hence, suppose one performs _Nash-value iteration_, i.e., solving for the NE of the stage game and conducting backward induction, this value iteration process does not converge in general as the number of backward steps increases, since the solution at each stage is not even unique, and there may not exist a unique fixed point. This is in stark contrast to the \(\) and \(\) operators in the value iteration updates for single-player and two-player zero-sum cases, respectively. By exploiting a star-shaped structure, we managed to reformulate a _minimax_ optimization problem when solving each stage game, which makes the corresponding value iteration operator _contracting_, and thus iterating it infinitely converges to the unique fixed point. Second, suppose there exists some other network structure (other than star-shaped ones) that also leads to a contracting value iteration operator, then for a fixed constant \(\), the fixed point (which corresponds to the Markov stationary CCE/NE of the zero-sum NMG) becomes unique and can be computed efficiently, which contradicts our hardness result in Theorem 1. Indeed, it was the exclusion of a star-shaped structure in Theorem 1 that inspired us to consider this structure in proving the convergence of FP dynamics. That being said, we note that having a contracting value iteration operator is only a _sufficient_ condition for the FP dynamics to converge. It would be interesting to explore other structures that enjoy the FP property for reasons beyond this contraction property. We leave this as an immediate future work.

Next, we present another positive result in light of the hardness in Theorem 1, regarding the computation of _non-stationary_ equilibria in multi-player zero-sum NMGs.

Non-Stationary NE Computation

We now focus on computing an (approximate) Markov _non-stationary_ equilibrium in zero-sum NMGs. In particular, we show that when relaxing the stationarity requirement, not only CCE, but NE, can be computed efficiently. Before introducing our algorithm, we first recall the folklore result that approximating Markov non-stationary NE in _infinite-horizon_ discounted settings can be achieved by finding approximate Markov NE in _finite-horizon_ settings, with a large enough horizon length (c.f. Proposition 10). Hence, we will focus on the finite-horizon setting from now on.

Before delving into the details of our algorithm, we introduce the notation \(_{h,i}(s)\) and \(_{h}(s)\) for \(h[H],i,s\) as follows:

\[_{h,i}(s):=(Q_{h,i,1}(s),,Q_{h,i,i-1}(s),,Q_{h,i,i+1}(s) ,Q_{h,i,n}(s))^{|_{i}|_{i}|_{i}|}\]

\[_{h}(s):=((_{h,1}(s))^{},(_{h,2}(s))^{}, ,(_{h,n}(s))^{})^{}^{_{i }|_{i}|_{i}|_{i}|}.\]

Here, \(Q_{h,i,j}\) represents an estimate of the equilibrium value function with canonical decomposition (Proposition 2). Hereafter, we similarly define the notation of \(_{h,i}^{}\) and \(_{h}^{}\). Our algorithm is based on value iteration, and iterates three main steps from \(h=H\) to 1 as follows: (1) \(Q\)-value computation: compute \(Q_{h,i,j}\), which estimates the equilibrium \(Q\)-value with a canonical decomposition form; in particular, when \(_{C}\), \(Q_{h,i,j}\) is updated for all \(s,(i,j)_{Q},a_{i}_{i}\), and \(a_{j}_{j}\):

\[Q_{h,i,j}(s,a_{i},a_{j})=r_{h,i,j}(s,a_{i},a_{j})+_{s^{} }_{Q,i}|}(i_{C}) _{h,i}(s^{} s,a_{i})+(j_{C})_{h,j}(s^{} s,a_{j})V_{h+1,i}(s^{}),\]

(2) Policy update: update \(_{h}(s)\) with an NE-ORACLE: finding (approximate)-NE of some zero-sum NG \((,,(Q_{h,i,j}(s))_{(i,j)_{Q}})\) for all \(s\), and (3) Value function update: compute \(V_{h,i}\), which estimates the equilibrium value function as follows for all \(s,i\): \(V_{h,i}(s)=_{h,i}^{}(s)_{h,i}(s)_{h}(s)\). The overall procedure is summarized in Algorithm 6.

Ne-ORACLE and iteration complexity.The NE-ORACLE in Algorithm 6 can be instantiated by several different algorithms that can find an NE in a zero-sum NG. Depending on the algorithms, the convergence guarantees can be either in terms of average-iterate, best-iterate, or last-iterate. Note that for algorithms with average-iterate convergence, one may additionally need to _marginalize_ the output joint policy, i.e., the approximate CCE, and combine them as a _product_ policy that is an approximate NE (Proposition 6). For those with best-/last-iterate convergence, by contrast, the best-/last-iterate is already in product form, and one can directly output it as an approximate NE. Moreover, last-iterate convergence is known to be a more favorable metric than the average-iterate one in learning in games [45; 46; 47; 48; 49], which is able to characterize the _day-to-day_ behavior of the iterates and implies the stability of the update rule. Hence, one may prefer to have last-iterate convergence for solving zero-sum N(M)Gs. To this end, two algorithmic ideas may be useful: adding regularization to the payoff matrix [39; 42; 50; 51; 52], and/or using the idea of optimism [47; 36; 53]. Recent results [54; 51] have instantiated the ideas of _optimism-only_ and _optimism + regularization_, respectively, for best-/last-iterate convergence in zero-sum polymatrix games. We additionally established results for the idea of _regularization-only_ in obtaining last-iterate convergence in these games. Specifically, we propose to study the vanilla Multiplicative Weight Update (MWU) algorithm  in the regularized zero-sum NG, as tabulated in Algorithm 9. We have also introduced a variant with diminishing regularization, and summarize the update rule in Algorithm 10.

Given the results above, aggregating \(\)-approximate NE for the zero-sum NGs \((,,(Q_{h,i,j}(s))_{(i,j)_{Q}})\) for all \(h[H],i,s\) provides an \(H\)-approximate NE for the corresponding zero-sum NMG. We have the following formal result.

**Proposition 4.** Suppose that for all \(h[H],i,s\), \((,,(Q_{h,i,j}(s))_{(i,j)_{Q}})\) provides an \(_{h,s}\)-approximate NE for the zero-sum NG \((,,(Q_{h,i,j}(s))_{(i,j)_{Q}})\) in Algorithm 6. Then, the output policy \(\) in Algorithm 6 is an \((_{h[H]}_{s}_{h,s})\)-approximate NE for the corresponding zero-sum NMG \((=(,_{Q}),,,H,(_{h})_{h[H]},(r_{h,i,j}(s))_{(i,j)_{Q},s})\).

The proof of Proposition 4 is deferred to Section F. In light of Proposition 4 and Table 1, we obtain Table 2, which summarizes the iteration complexities required to find an \(\)-NE for zero-sum NMGs, with different NE-ORACLE subroutines. Note that the iteration complexities are all polynomial in \(H,n,||\), and inherit the order of dependencies on \(\) from Table 1 for the matrix-game case. In particular, Algorithm 6 with the OMWU in  yields the fast rate of \(}(1/)\) for the last iterate.