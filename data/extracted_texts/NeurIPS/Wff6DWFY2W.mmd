# Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning

Chang Lu\({}^{}\), Chandan K. Reddy\({}^{}\), Ping Wang\({}^{}\), Yue Ning\({}^{}\)

\({}^{}\) Department of Computer Science, Stevens Institute of Technology

\({}^{}\) Department of Computer Science, Virginia Tech

\({}^{}\) {clui13, ping.wang, yue.ning}@stevens.edu

\({}^{}\) reddy@cs.vt.edu

###### Abstract

Automatic coding of International Classification of Diseases (ICD) is a multi-label text categorization task that involves extracting disease or procedure codes from clinical notes. Despite the application of state-of-the-art natural language processing (NLP) techniques, there are still challenges including limited availability of data due to privacy constraints and the high variability of clinical notes caused by different writing habits of medical professionals and various pathological features of patients. In this work, we investigate the semi-structured nature of clinical notes and propose an automatic algorithm to segment them into sections. To address the variability issues in existing ICD coding models with limited data, we introduce a contrastive pre-training approach on sections using a soft multi-label similarity metric based on tree edit distance. Additionally, we design a masked section training strategy to enable ICD coding models to locate sections related to ICD codes. Extensive experimental results demonstrate that our proposed training strategies effectively enhance the performance of existing ICD coding methods.

## 1 Introduction

The adoption of electronic health records (EHR) data has become widespread in modern healthcare facilities as they provide a centralized platform to maintain comprehensive medical information of patients, including diagnoses, procedures, laboratory tests, and clinical notes . To efficiently manage and categorize diseases and procedures, EHR data utilizes the International Classification of Diseases (ICD) system developed by the World Health Organization. The ICD system provides a hierarchical structure that maps diseases/procedures to digital codes. Clinical notes in EHR data are generally stored as free text, while diagnosis and procedure codes are extracted from these notes and saved as structured data. The process of extracting ICD codes from clinical notes is referred to as **ICD coding** and is a crucial task in medical services such as medical records management, medical billing , and insurance reimbursement . It also supports healthcare research endeavors such as diagnosis prediction  and medication recommendation .

The traditional ICD coding task relies on human effort, which is both time-consuming and prone to errors . Incorrect code assignments can be costly. For instance, the error payout rate due to wrong code assignment reached 6.8% in 2000, as stated by the US Centers for Medicare and Medicaid's statistics . Consequently, researchers are exploring automated ICD coding methods to assign ICD codes to medical documents with algorithms. Recent methods generally treat the ICD coding task as a multi-label classification problem , as one clinical note can contain multiple diagnosis/procedure codes. To capture the relationship between text and codes, code representations have been studied by incorporating the semantic information of code names  with hierarchical structures, synonyms, and co-occurrence of codes to provide fine-grained code representations andrelationships [4; 30; 29]. However, due to the nature of clinical notes, automatic ICD coding tasks still present _certain challenges_ when it comes to learning the representation of clinical notes:

1. **Limited availability of data.** Due to privacy constraints, EHR data can be challenging to access. For example, a publicly available EHR dataset, MIMIC-III , only contains around 50,000 clinical notes that can be used for ICD coding. Furthermore, the appearance of codes in an EHR dataset follows a long-tail distribution. Around 51.6% of codes occur less than 6 times, and 60.2% of codes occur less than 10 times, making it even more difficult to train data-driven deep learning models for these codes due to data paucity.
2. **Ignoring structural information.** Based on our observation, most clinical notes have common sections such as "physical exam", "history of present illness", "discharge followups", and "brief hospital course", as depicted in Figure 1(a). These sections reflect correlated health information in long clinical notes. However, most of the existing ICD coding models treat clinical notes as a single sequence, disregarding these semi-structures that represent essential elements of diagnoses.
3. **Variability of clinical notes.** Clinical notes are written by various medical professionals, who may have different writing habits. Different clinical notes may present different orders and combinations of sections, adding to the variability of the data. Additionally, different patients may also lead to a diverse content of clinical notes due to their unique diseases or physical exams. This variability becomes a more negative factor in training an ICD coding model with limited data.

In this paper, our goal is to utilize the semi-structured format and reduce the variability of clinical notes with limited data. As shown in Figure 1(b), existing methods typically treat clinical notes as long sequences of words [16; 30; 29] without considering the semi-structured format of clinical notes. As a result, these models can easily be affected by the variability of clinical notes with limited data. To overcome this challenge, we propose to automatically segment a clinical note into multiple sections to build an order-agnostic structure. Based on the extracted sections, we introduce a contrastive learning framework to initially reduce the variability in clinical notes in pre-training and allow the text encoder to better understand the relationship of sections with limited training data. This proposed contrastive learning method defines a soft multi-label similarity between section pairs from the same and different clinical notes. Finally, we design a masked section training method to further minimize the variability of clinical notes in the training of ICD coding models.

Main contributionsIn summary, the main contributions of this paper are listed as follows:

* We propose a content-based algorithm that automatically segments clinical notes into sections. To the best of our knowledge, our work is one of the first to investigate automatic semi-structured segmentation for clinical notes in ICD coding.
* We present a contrastive learning framework based on a soft multi-label similarity with tree edit distance and a masked section training strategy to alleviate the variability of clinical notes with limited EHR data.
* We conduct extensive experiments on real-world EHR datasets and demonstrate that our proposed section-based learning can enhance the performance of existing ICD coding methods.

## 2 Related work

The task of ICD coding involves predicting ICD codes from discharge summaries and can be approached as a multi-label text categorization problem. In the past, natural language processing (NLP) techniques have been commonly used to learn the representation of clinical notes. For instance, Perotte _et al._ used the term frequency-inverse document frequency (TF-IDF) features of clinical notes and employed support vector machine (SVM) classifiers for ICD coding. Mullenbach _et al._ proposed CAML based on convolutional neural networks (CNN), while Baumel _et al._ employed a two-layer recurrent neural network (RNN) to encode clinical notes. Liu _et al._ applied squeeze-and-excitation networks in CNN and used the focal loss to deal with rare codes.

Figure 1: An example of a clinical note in the format of multiple sections and one sequence of words.

In addition to the conventional RNN/CNN-based models, attention and graph neural networks have also been explored in the context of ICD coding. Li _et al._ proposed a multi-filter residual CNN model incorporating label attention between codes and text. Xie _et al._ utilized the hierarchical structure of ICD codes and developed a graph neural network to capture the relation between codes. Cao _et al._ considered both the hierarchical structure and the co-occurrence of ICD codes, embedding the codes into a hyperbolic space. Yuan _et al._ proposed to improve the matching of the ICD code names that occur in clinical notes using attention with code synonyms.

To alleviate the lack of training data and labels, weak supervision has also been studied for ICD coding. It aims to automatically generate weakly labeled training data using rules, heuristics, or medical domain knowledge. Dong _et al._ adopted an existing named entity linking tool called SemEHR to identify rare diseases from clinical notes. Gao _et al._ proposed a labeling function called KeyClass by extracting n-grams as keywords and computing the cosine similarity of word embeddings between keywords and labels.

Although large language models (LLMs) are popular and effective in many NLP tasks such as machine translation and question-answering systems, it has been shown by Pascual _et al._ and Ji _et al._ that pre-trained LLMs such as BERT  do not help to improve the performance of ICD coding due to the long text of clinical notes and the difference of training data between ICD coding and pre-training tasks of LLMs. To overcome these problems, Liu _et al._ split the clinical notes into chunks to fit the pre-defined maximum input length of transformer-based models. However, splitting a document into chunks can break coherent information in clinical notes. Yang _et al._ introduced KEPT, a transformer-based model that uses Longformer  to encode the long text, pre-trained with a contrastive learning method for code synonyms. They also designed a prompt learning framework for the prediction. However, the Longformer and prompt learning used in KEPT require a huge number of model parameters and extremely long input, which is barely applicable in training.

As previously discussed, most current methods only treat clinical notes as long sequences without considering their semi-structured format, making it challenging to handle the variability of clinical notes. Additionally, while some models, like KEPT, incorporate pre-training, it is designed only for labels but not for clinical notes. Thus, these models are not effective in comprehending the relationship among different sections of clinical notes. In light of these limitations, our paper aims to investigate the semi-structured format of clinical notes and improve the model's ability to learn the representations of long-text clinical notes with limited data.

## 3 Preliminaries

Problem formulationConsider the ICD codes as a set \(=\{l_{i}\}_{i=1}^{L}\), where \(L=||\) is the number of codes. Specifically, \(l_{i}=\{w_{j}\}_{j=1}^{m}\) with \(m\) tokens is the description of the \(i\)-th label, where \(w_{j}\), and \(\) is the vocabulary of all tokens in the ICD code descriptions and clinical notes. Given a clinical note \(S=\{w_{j}\}_{j=1}^{n}\) with \(n\) tokens, the ICD Coding task is to train a model \(\) to predict a binary vector \(}\{0,1\}^{L}\), where \(}_{i}=1\) means the code \(l_{i}\) exists in the clinical note \(S\).

General ICD coding frameworkTo better demonstrate key parts of the ICD coding, we simplify it as flat multi-label classification. A general ICD Coding framework \(\) contains three modules:

* _Clinical note encoder_ (\(_{}\)): Given a clinical note \(S\), the clinical encoder is a text encoder \(_{}\) that first encodes the words into embeddings and uses RNN, CNN, or Transformer encoder to compute hidden representations \(_{}\) of words: \(_{}=_{}(S)^{n d}\).
* _ICD code encoder_ (\(_{}\)): This module can be regarded as a domain knowledge encoder that incorporates the text description of all codes (i.e., code names) in the ICD system, which are agnostic to the training-data. It is also a text encoder that first calculates the hidden word representations of a code name and then uses a pooling layer (e.g., mean/max pooling) on words to get the code representation for one ICD code: \(_{}^{i}=(_{}(l _{i}))^{d}\). Eventually, we have the hidden representations of all the ICD codes: \(_{}^{L d}\).
* _Fusion between note and code_ (\(\)): This module aggregates the representations of clinical notes and ICD codes to generate predictions, denoted as \(}=(_{},_{ })\). To achieve this, it first applies an attention mechanism between the codes and notes by calculating \(_{}=(_{},_ {},_{})^{L d}\), where the query is the code representation \(_{}\) and the key and value are note representation \(_{}\). It then takes the dot product between the attention output and the code representation to obtain the final output \(=_{}_{}^{L}\). Finally, a sigmoid function is applied to get the final prediction \(}\).

Both \(_{}\) and \(_{}\) contain a text encoder \(_{}\). It is a common practice to share the parameters of these two text encoders including word embeddings and model weights.

## 4 Method

We first present an algorithm to automatically extract section titles and segment clinical notes into sections. Then, we introduce the proposed training strategies for existing ICD coding models: contrastive pre-training and masked section training based on the extracted sections to reduce the variability of clinical notes with limited training data.

### Automatic section-based segmentation

As mentioned in Section 1, clinical notes typically contain sections with standard titles, but the order of these sections may vary depending on the writing style of medical professionals. To reduce the variability in clinical notes, it is important to extract sections related to ICD codes. The initial step is to identify all possible section titles for further segmentation. However, since clinical notes are written in plain text, there are no universal rules to extract these titles. Consequently, an automatic segmentation algorithm based on the content of clinical notes is needed to extract the section titles.

Inspired by TF-IDF which can retrieve keywords in a document, we propose an n-gram document frequency-inverse average phrase frequency (DF-IAPF) algorithm to extract section titles. TF-IDF captures the unique importance of a word for a document. In TF-IDF, a word becomes a keyword of a document when it has a high term frequency in this document while few documents contain this word. However, extracting section titles is different from extracting keywords for the following reasons:

1. Section titles are usually phrases instead of single words (e.g., "history of present illness");
2. Unlike keywords that are common in a document but less frequent in a corpus, most clinical notes have similar section titles but they often appear only once within a clinical note.

Based on these two properties of section titles, we introduce DF-IAPF to automatically extract section titles based on the corpus-level frequency and uniqueness of phrases in the document. We first define the DF-IAPF score for a phrase \(t=(w_{1},w_{2},,w_{N})\) that contains \(N\) words (n-gram).

Document frequency-inverse average phrase frequencyWe first let \((t)\) be the relative frequency of documents containing \(t\), and \((t)\) be the inverse average phrase frequency of \(t\) in all documents containing \(t\):

\[(t)=}{n_{d}},(t)=} _{i=1}^{n_{d}}f_{t,i}}=}{_{i=1}^{n_{d}}f_{t,i}},\] (1)

where \(n_{d}\) is the total number of documents, \(n_{t}\) is the number of documents containing \(t\), and \(f_{t,i}\) is the occurrence number of \(t\) in the document \(i\). The document frequency-inverse average phrase frequency (DF-IAPF) is defined as follows:

\[(t)=(t)(t)=}{n_{d}} }{_{i=1}^{n_{d}}f_{t,i}}=^{2}}{n_{d}_{i=1}^{n_{d }}f_{t,i}}.\] (2)

The DF-IAPF algorithm assigns a higher score to phrases that appear frequently across all documents but occur less frequently within each document on average. For example, the formal section title, "brief hospital course", should have a higher score than a random phrase "this patient has". This is because most clinical notes contain a unique section titled "brief hospital course", while the phrase "this patient has" is more commonly used and appears multiple times in a clinical note, which lowers its score in the DF-IAPF algorithm. Then, we iterate through all n-grams with a maximum word count of \(\) in clinical notes to select candidates for section titles. Since we use n-gram to extract phrases, finally, we filter out shorter titles that are subsequences of longer titles with high scores. The specific algorithm to extract candidates and complexity analysis are presented in Appendix B.

Once the section title candidates with the highest DF-IAPF scores have been retrieved, we manually select phrases from this small candidate set to form a title subset \(\{t_{1},t_{2},,t_{T}\}\) with \(T\) titles. This selection process is completed by medical experts to ensure the correctness of selected titles. Since section titles are mostly unique within a clinical note, we use the first occurrence position of the extracted section titles as anchors to segment each clinical note into multiple sections \(\{s_{k}\}_{k=1}^{T}\) and build an order-agnostic structure. Given a clinical note \(S\), the segmentation process from the plain text \(S\) to sections \(s_{k}\) with \(n_{k}\) words can be summarized as follows:

\[S[]{}\{t_{k}:s_{k}\}_{k=1}^{T},\] (3)

where \(t_{k}\) denotes a section title and \(s_{k}=(w_{1},w_{2},,w_{n_{k}})\) is the content under the section \(t_{k}\).

### Supervised tree-based contrastive learning on sections

In a general ICD coding framework, Fusion is an attention mechanism that enables the selection of significant words in clinical notes related to code descriptions. Ideally, a clinical note should contain sections called "discharge diagnoses" and "major procedures", which encompass all code descriptions corresponding to the labels. In this case, the model can accurately extract codes from these sections. However, many clinical notes lack these two sections. Even if they exist in some clinical notes, the descriptions may be incomplete. Typically, these sections contain only primary diagnosis or procedure codes, while the labels include all secondary codes. Under these circumstances, the model must locate related records from other sections such as "physical exam" or "discharge medications", given that these sections may imply the key expressions for the diagnoses or procedures. Thus, it is necessary to improve the model's ability to comprehend the content of each section.

To accomplish this, we design a contrastive learning framework based on sections. It makes the clinical note encoder distinguish sections from the same clinical note or different clinical notes so that the model can be aware of similar clinical notes and recognize related sections.

Construction of contrastive samplesSince we formulate ICD coding as a multi-label classification task, it is hard to find two clinical notes with the same labels as a positive pair, and it is too trivial to obtain negative pairs using two clinical notes with different labels. Therefore, we construct positive/neighbor section pairs for further training. _Positive pairs:_ For each clinical note \(S_{i}=\{t_{k}:s_{k}^{i}\}_{k=1}^{T}\), we randomly select an anchor section \(s_{k}^{i}\). To increase the connectivity between sections in the same note, we then sample a different section \(s_{k^{}}^{i}\) from the same clinical note \(S_{i}\) to build a positive pair \((s_{k}^{i},s_{k^{}}^{i})\), where \(t_{k^{}} t_{k}\). Furthermore, from a different clinical note \(S_{j}\), we sample two sections \(s_{k}^{j}\) and \(s_{k^{}}^{j}\) that correspond to \(t_{k}\) and \(t_{k^{}}\) in \(S_{i}\), to build two neighbor pairs: \((s_{k}^{i},s_{k}^{j})\) and \((s_{k^{}}^{i},s_{k^{}}^{j})\). Note that, \(s_{k}^{j}\) and \(s_{k^{}}^{j}\) are also a positive pair. Finally, a new sample for contrastive learning is a quadruple including the anchor, positive section, and two neighbor sections, i.e., \((s_{k}^{i},s_{k^{}}^{i},s_{k}^{j},s_{k^{}}^{j})\).

Soft multi-label similarityTo achieve the goal of contrasting section pairs in the same or different clinical notes, an intuitive idea is to calculate the Jaccard similarity between two label sets or the cosine similarity between label vectors. Unfortunately, these metrics cannot capture the underlying disease relationships in the ICD system. For example, the cosine or Jaccard similarity for two sets of disease labels: {_diabetes type I_} and {_diabetes type II_}, will be zero because they have no overlap. However, these two diseases both belong to diabetes in the ICD system. Instead of assigning hard contrastive label 0 or 1 to two label sets, we design a soft similarity of two label sets that considers disease relationships in the ICD hierarchical structure by utilizing the _tree edit distance_ that measures the minimum number of node edit operations (add, delete, and replace) required to transform one tree into another.

**Definition 1** (Spanning super-tree).: Given the ICD hierarchical structure \(\) as a tree, the label set \(_{i}\) of clinical notes \(S_{i}\), the spanning super-tree \(_{i}\) of \(_{i}\) is defined as a minimum tree that has the same root as \(\) and contains all label node of \(_{i}\) and all ancestors of \(_{i}\): \(_{i}=_{l_{i}}(l)_{i} \). Here, \((l)\) denotes all the ancestors of one label node \(l\).

Based on the spanning super-tree, the similarity \(_{ij}\) between two label sets \(_{i}\) and \(_{j}\) is defined as:

\[_{ij}=1-(_{i},_{j})}{| _{i}_{j}|-1}[-1,1],\] (4)

where dist denotes the tree edit distance  between \(_{i}\) and \(_{j}\).

In this similarity metric, we consider both the tree edit distance and the cardinality of trees. In the denominator, we use \(|_{i}_{j}|-1\) because every pair of \(_{i}\) and \(_{j}\) share the same root. Figure 2 shows two super-tree examples. In the first tree, nodes 5 and 7 are the labels of the clinical note \(S_{1}\), while nodes 1, 2, and 3 are their ancestors. In the second tree, nodes 2 and 6 are labels of another clinical note, and nodes 1 and 3 are their ancestors. The tree with all colored nodes forms the spanning super-tree. The distance between these two spanning super-trees is 2 because we can delete node 5 and replace node 7 with 6 in the first tree to transform it into the second tree. Thus, the similarity of these two trees is \(1-=0.2\), since \(|_{i}_{j}|=|\{1,2,3,5,6,7\}|=6\). Although \(_{1}\) and \(_{2}\) do not have the same label nodes, they still share some similarities given their topological categories in the ICD hierarchical structure: the label node 2 in \(_{2}\) is the parent of 5 in \(_{1}\), while the label node 6 in \(_{2}\) is a sibling of 7 in \(_{1}\).

Contrastive pre-trainingIn the pre-training of the clinical note encoder \(_{}\), given a quadruple \((s^{i}_{k},s^{i}_{k^{}},s^{j}_{k},s^{j}_{k^{}})\), we first calculate the note representation \(_{}\) and apply a max pooling layer to obtain section representations for all these sections:

\[^{\{i,j\}}_{(k,k^{})}=(_{ }(s^{\{i,j\}}_{(k,k^{})}))^{d}.\] (5)

Then, we aim to utilize the soft similarity between the labels of two clinical notes to guide the similarity of these four section representations. Note that the similarity between sections in a positive pair is 1 while the similarity of negative pairs is \(\). Thus, we design a contrastive loss \(\) using the mean absolute error (MAE) as follows:

\[=_{m}(1,(^{i}_{k},^{i}_{k^{ }}))+_{m}(_{ij},(^{i}_{k},^{j} _{k}))+_{m}(1,(^{j}_{k},^{j}_{k^{}} ))+_{m}(_{ij},(^{i}_{k^{}},^{ j}_{k^{}})).\] (6)

Here, \(_{m}\) denotes the MAE loss, \(_{ij}\) denotes the aforementioned tree edit distance similarity between two label sets, and \((,)\) is the cosine similarity function between two vectors. Figure 3 intuitively shows the proposed contrastive loss for the quadruple with soft similarity.

In summary, there are two main benefits of using contrastive learning on sections:

1. The model gains a better understanding of the relationships between sections within a clinical note. This is particularly beneficial when a clinical note lacks explicit indicators of ICD codes, such as the "discharge diagnoses" section. With contrastive pre-training, the model can infer ICD codes by the content of related sections.
2. It helps the model adapt to different writing styles of medical professionals and various demographic features of patients so that the model can focus on the text related to ICD codes.

### Masked section training

As discussed earlier, existing methods mostly consider clinical notes as long sequences. Inspired by the denoising techniques for text , we develop a simple yet effective training strategy with permutation and masking on sections. It further mitigates the variability caused by the section order.

Given a clinical note that has been segmented into sections, denoted by \(S=\{t_{k}:s_{k}\}_{k=1}^{T}\), we first shuffle the order of the sections to create an order-agnostic structure. Then, similar to the dropout technique  used to avoid overfitting in training deep learning models, we randomly mask a subset of the sections, subject to a threshold \(\) where \(0<1\). The remaining sections are concatenated back into a long sequence \(S^{}\) that is suitable for the input of existing ICD coding models. It is important to note that we do not aim to modify the original model architecture, but rather to generate samples that can help to reduce the variability of clinical notes in training. In summary, this section masking process can be described as follows:

\[S^{}=_{k(T)}s^{}_{k},s^{}_{k}=s_{k}&|s_{k}|>0 U,\\ &\] (7)

Figure 3: Contrastive loss.

Figure 2: Examples of two spanning super-trees. Nodes with orange and green colors denote label nodes and their ancestor nodes.

Here, \(\) denotes the concatenation operation, and \(U\) means the uniform distribution from 0 to 1. By using perm, we can generate a random permutation of \((1,2,,T)\) of section indices, which is a random shuffle of all sections in a clinical note.

With shuffling and masking in training, the ICD coding model is no longer limited by the order of the clinical notes. Additionally, certain sections, such as "discharge diagnoses", may not always play a deterministic role in the prediction. This allows the model to focus more on other sections that are also relevant to the predicted ICD codes. Note that, in the inference step, we do not perform shuffling and masking, but use the original sequence as input for an ICD coding model.

## 5 Experiments

### Dataset, tasks, and evaluation metrics

The MIMIC-III  dataset is a popular publicly available EHR dataset that contains the discharge summaries and corresponding ground-truth ICD codes. We follow the ICD coding tasks in prior work [29; 30] and conduct three prediction tasks:

* MIMIC-50 prediction: Predicting the top 50 frequent ICD codes in the MIMIC-III dataset.
* MIMIC-rare-50 prediction: Predicting the rare 50 ICD codes that occur less than 10 times.
* MIMIC-full prediction: Predicting the entire (8,692) ICD codes in the MIMIC-III dataset.

The detailed training/dev/test dataset statistics for each task are listed in Table 1. Our experiments are conducted with cross-validation on the dev set to adjust hyper-parameters.

We use the following evaluation metrics which have been used in prior ICD coding studies [29; 30]. The metrics for MIMIC-full prediction are Macro/Micro \(F_{1}\) and precision at 8/15 (P@8, P@15). For MIMIC-50 prediction, we use Macro/Micro \(F_{1}\) and precision at 5 (P@5). For MIMIC-rare-50 prediction, we use Macro/Micro \(F_{1}\).

### Backbone models

To verify the effectiveness of our proposed section-based contrastive pre-training and masked section training (CM), we choose the following state-of-the-art ICD coding models as backbones1:

* **MultiResCNN**: It encodes clinical notes with multi-filter residual CNN and label attention.
* **HyperCore**: It also uses a convolutional encoder for text. Moreover, it applies hyperbolic embedding for ICD codes and uses GCN to model code co-occurrence.

  
**Task** & **Item** & **Train** & **Dev** & **Test** \\   & \# Docs. & 8,066 & 1,573 & 1,729 \\  & Avg. \# words per Doc. & 1,478 & 1,739 & 1,763 \\  & Avg. \# codes per Doc. & 5.7 & 5.9 & 6.0 \\  & Total \# codes & 50 & 50 & 50 \\   & \# Docs. & 249 & 20 & 142 \\  & Avg. \# words per Doc. & 1,770 & 1,930 & 2,071 \\  & Avg. \# codes per Doc. & 1.0 & 1.0 & 1.0 \\  & Total \# codes & 50 & 50 & 50 \\   & \# Docs. & 47,723 & 1,631 & 3,372 \\  & Avg. \# words per Doc. & 1,434 & 1,724 & 1,731 \\   & Avg. \# codes per Doc. & 15.7 & 18.0 & 17.4 \\   & Total \# codes & 8,692 & 3,012 & 4,085 \\   

Table 1: Data statistics for the MIMIC-50, MIMIC-rare-50, and MIMIC-full tasks.

* **JointLAAT**: It uses a bidirectional LSTM to encode clinical notes and proposes a joint learning method to predict ICD codes and their parent codes in the ICD hierarchical structure.
* **EffectiveCAN**: Similar to MultiResCNN, it also applies a convolutional encoder with multiple residual squeeze-and-excitation networks.
* **PLM-ICD**: It is a transformer model (Roberta-base) that splits clinical notes into chunks to satisfy the maximum length of pre-trained large language models.
* **Hierarchical**: It is hierarchical a transformer model (Roberta-large) by splitting clinical notes into paragraphs.
* **MSMN**: It is an LSTM text encoder and incorporates the synonym of code descriptions to make the model better understand the variety of code names.

### Implementation details

For DF-IAPF, we set the maximum word number (\(\)) in n-gram to 5. We set \(K\) in top-\(K\) candidates to 50 for the review of medical professionals. For contrastive pre-training, the batch size is 16, the learning rate is \(5 10^{-4}\), the optimizer is AdamW, and the epoch number is 20. The contrastive pre-training only uses the training dataset of each task to avoid data leakage. For the masked section training, we set \(\) to 0.2 for MIMIC-full prediction and 0.3 for MIMIC-50/MIMIC-rare-50 prediction.

The backbone models except HyperCore  and EffectiveCAN  are implemented using their publicly released code and the optimal parameters reported in their papers. For HyperCore and EffectiveCAN, the authors do not release the code. Therefore, we implemented a version that has a close performance to the original paper. For the MIMIC-50 and MIMIC-rare-50 tasks, we run every baseline 5 times and report their average and standard deviations (std).

All programs are executed using a machine with Python 3.9.3, CUDA 11.7, an Intel i9-11900K CPU, 64GB memory, and an NVIDIA RTX 3090 GPU. The code of the proposed DF-IAPF method and training strategies can be found at: https://github.com/LuChang-CS/semi-structured-icd-coding.

### Experimental results

Extracted section titlesTo demonstrate the effectiveness of our proposed DF-IAPF algorithm to extract section titles, we compare it with a rule-based extraction algorithm . It designs special rules for every observed section title based on colons and occurrence frequencies to segment clinical notes into sections. We list the extracted section titles and analyze the effectiveness and advantages of the proposed DF-IAPF algorithm in Appendix C.2.

MIMIC-50-predictionWe report the results of MIMIC-50 in Table 2. Here, we run each backbone model 5 times and report their mean value and std. Among all backbone models, MSMN achieves the best result on all metrics without the proposed CM. Additionally, with the proposed CM strategies, the performance of all backbone models is improved, and Macro \(F_{1}\) is improved by 1.5% on average. Additionally, we also run a paired t-test on the Macro \(F_{1}\) score between the backbone models w/ CM and w/o CM. The \(p\)-values for all backbone models are less than \(5 10^{-2}\), indicating that the improvement brought by the CM strategies is statistically significant over the original models.

    &  &  \\   & Macro \(F_{1}\) & Micro \(F_{1}\) & P@5 & Macro \(F_{1}\) & Micro \(F_{1}\) & P@5 \\  MultiResCNN & 60.8 (0.3) & 67.1 (0.1) & 64.3 (0.3) & 62.2 (0.3) & 68.1 (0.1) & 65.1 (0.2) \\ HyperCore & 61.1 (0.2) & 66.2 (0.2) & 63.5 (0.3) & 62.0 (0.2) & 67.4 (0.2) & 64.5 (0.3) \\ JointLAAT & 66.4 (0.1) & 71.6 (0.2) & 67.3 (0.4) & 67.2 (0.2) & 72.0 (0.3) & 67.9 (0.1) \\ EffectiveCAN & 66.7 (0.1) & 71.5 (0.2) & 66.4 (0.2) & 67.5 (0.2) & 71.8 (0.1) & 67.8 (0.1) \\ PLM-ICD & 64.5 (0.3) & 69.3 (0.2) & 64.5 (0.4) & 65.2 (0.1) & 70.3 (0.2) & 65.6 (0.2) \\ Hierarchical & 65.3 (0.1) & 70.6 (0.3) & 66.5 (0.1) & 66.1 (0.2) & 71.8 (0.4) & 67.2 (0.3) \\ MSMN & 68.1 (0.2) & 72.0 (0.1) & 67.5 (0.1) & 69.1 (0.1) & 72.5 (0.1) & 68.3 (0.2) \\   

Table 2: Results (%) of MIMIC-50 when trained with and without the proposed contrastive pre-training and masked training (CM) strategies. Cells with the green color denote an improvement of w/ CM compared to w/o CM.

**MIMIC-rare-50-prediction**

We report the results of MIMIC-rare-50 in Table 3. We observe that, with limited training samples in this task, the performance of backbone models is not as good as the MIMIC-50 prediction. However, the proposed CM strategies can significantly improve the prediction results. On average, the Macro \(F_{1}\) score is improved by 55.8%. Note that, the contrastive pre-training is conducted on every task instead of the entire clinical notes. Therefore, we can conclude that the proposed CM strategies can learn a good initialization of the ICD coding models in pre-training and serve as an effective data augmentation method in training with limited data.

Due to space constraints, we show the results of MIMIC-full in Appendix C.3. In summary, these experiments prove the capability of the DF-IAPF algorithm in extracting section titles and demonstrate the effectiveness of the proposed CM strategies in enhancing existing ICD coding models.

### Ablation studies

To validate the effectiveness of the contrastive pre-training and masked section training, we conduct an ablation study by only applying one strategy in the MIMIC-50 prediction task and replacing the tree edit distance with Jaccard similarity. Here, we choose MIMIC-50 prediction and use JointLAAT, EffectiveCAN, and MSMN as the backbones. Figure 4 demonstrates the prediction results when training without CM (w/o CM), with only contrastive pre-training using Jaccard similarity (w/ J), with only contrastive pre-training (w/ C) using tree edit distance, with only masked section training (w/ M), and with both two strategies (w/ CM). In this figure, pre-training with simple Jaccard similarity even decreases the performance. It shows that this similarity cannot appropriately guide contrastive learning because many clinical notes have disjoint labels. We notice that both contrastive pre-training and masked section training contribute to improving the performance of the original ICD coding models. Specifically, the masked section training is slightly better than the contrastive pre-training. We think it is because the masked section training is directly applied to training ICD coding models, while the contrastive pre-training learns good initialization before training.

### Case studies

To intuitively demonstrate the effectiveness of the proposed contrastive pre-training and masked training, in Figure 5, we give an example of a clinical note snippet and the predictions w/o CM and w/ CM using MSMN on the MIMIC-50 prediction task. Here, the ground truth contains 4 ICD

Figure 4: Ablation studies of the MIMIC-50 task when trained without or with the proposed contrastive pre-training (C) and masked section training (M). We also report a variant by replacing the tree edit distance with the simple Jaccard similarity (J).

    &  &  \\  Model & Macro \(F_{1}\) & Micro \(F_{1}\) & Macro \(F_{1}\) & Micro \(F_{1}\) \\  MultiResCNN & 11.2 (2.1) & 13.1 (1.8) & 22.8 (1.3) & 23.3 (1.9) \\ HyperCore & 12.5 (1.3) & 15.6 (2.2) & 23.4 (1.9) & 25.2 (1.2) \\ JointLAAT & 20.2 (1.9) & 21.6 (1.5) & 28.6 (1.1) & 27.8 (1.4) \\ EffectiveCAN & 19.8 (1.4) & 22.5 (2.1) & 27.1 (2.4) & 28.0 (1.5) \\ PLM-ICD & 22.6 (2.5) & 24.3 (1.9) & 30.3 (1.5) & 29.5 (1.3) \\ Hierarchical & 23.1 (1.7) & 24.6 (1.4) & 32.0 (1.2) & 31.3 (2.2) \\ MSMN & 23.7 (1.0) & 24.1 (1.9) & 31.2 (1.3) & 30.6 (1.7) \\   

Table 3: Results (%) of MIMIC-rare-50 when trained with and without the proposed CM strategies. Cells with the green color denote an improvement of w/ CM compared to w/o CM.

codes, 45.13 (procedure), 530.81 (disease), 96.04 (procedure), and V15.82 (disease). The MSMN model w/o CM predicts two codes correctly while failing to predict 96.04 and V15.82 because they do not occur in the discharge diagnosis and procedure sections. However, with CM, the MSMN model successfully predicts all four ICD codes by locating them in related sections, including "CXR Endotracheal tube" in physical exam and "former smoker" in social history.

## 6 Conclusion

In this work, we aim to minimize the variability of clinical notes in the ICD coding task by studying the semi-structured format of clinical notes. To reduce human effort, we propose an automatic algorithm to extract section titles and segment clinical notes into sections. We also design contrastive pre-training and masked section training to let the ICD coding model better locate sections related to predictions. Additionally, a tree-edit distance is designed in the loss function to measure the similarity of positive/negative pairs. Extensive experiments demonstrate the effectiveness of the proposed section title extraction algorithm and training strategies. It is worth emphasizing that our proposed methodology is versatile, as it can not only be applied to clinical notes but also employed in general multi-label classification tasks that involve semi-structures such as sections. In the future, we are committed to exploring the broader applicability of our approach across various domains.

**Limitations:** Although the proposed training strategies are able to enhance existing ICD coding models, they are dependent on the design of these models. If the model is well-designed and has many parameters, it is generally overfitting with limited training data. In this case, our proposed training strategies are a good enhancement. Additionally, we only focus on the variability caused by the order of sections in this work, but there are other formats of variability such as typos and synonyms. In the future, we plan to design new ICD coding models based on sections and consider more types of variability to further improve the robustness of the training process.