# Retrieval-Augmented Diffusion Models

for Time Series Forecasting

 Jingwei Liu\({}^{1,2}\) Ling Yang\({}^{3}\)\({}^{}\) Hongyan Li\({}^{1,2}\) Shenda Hong\({}^{3,4,5}\)

\({}^{1}\)School of Intelligence Science and Technology, Peking University

\({}^{2}\) National Key Laboratory of General Artificial Intelligence, Peking University

\({}^{3}\)Institute of Medical Technology, Peking University Health Science Center

\({}^{4}\) National Institute of Health Data Science, Peking University

\({}^{5}\) Institute for Artificial Intelligence, Peking University

jingweiliu1996@163.com, yangling0818@163.com

{leehy, hongshenda}@pku.edu.cn

Contact: Jingwei Liu, jingweiliu1996@163.com

\({}^{}\)Contributed equally.Corresponding Authors: Hongyan Li, Shenda Hong

###### Abstract

While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable. Factors limiting time series diffusion models include insufficient time series datasets and the absence of guidance. To address these limitations, we propose a Retrieval-Augmented Time series Diffusion model (RATD). The framework of RATD consists of two parts: an embedding-based retrieval process and a reference-guided diffusion model. In the first part, RATD retrieves the time series that are most relevant to historical time series from the database as references. The references are utilized to guide the denoising process in the second part. Our approach allows leveraging meaningful samples within the database to aid in sampling, thus maximizing the utilization of datasets. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of existing time series diffusion models in terms of guidance. Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks. Our code is available at https://github.com/stanliu96/RATD

## 1 Introduction

Time series forecasting plays a critical role in a variety of applications including weather forecasting [15; 11], finance forecasting [7; 5], earthquake prediction  and energy planning . One way to approach time series forecasting tasks is to view them as conditional generation tasks [32; 42], where conditional generative models are used to learn the conditional distribution \(P(^{P}|^{H})\) of predicting the target time series \(^{P}\) given the observed historical sequence \(^{H}\). As the current state-of-the-art conditional generative model, diffusion models  have been utilized in many works for time series forecasting tasks [28; 36; 30].

Although the performance of the existing time series diffusion models is reasonably well on some time series forecasting tasks, it remains unstable in certain scenarios (an example is provided in 1(c)). The factors limiting the performance of time series diffusion models are complex, two of them are particularly evident. First, most time series lack direct semantic or label correspondences, which often results in time series diffusion models lacking meaningful **guidance** during the generationprocess(such as text guidance or label guidance in image diffusion models). This also limits the potential of time series diffusion models.

The second limiting factor arises from two shortcomings of the time series datasets: **size insufficient** and **imbalanced**. Compared to image datasets, time series datasets typically have a smaller scale. Popular image datasets (such as LAION-400M) contain 400 million sample pairs, while most time series datasets usually only contain tens of thousands of data points. Training a diffusion model to learn the precise distribution of datasets with insufficient size is challenging. Additionally, real-world time series datasets exhibit significant imbalance. For example, in the existing electrocardiogram dataset MIMIC-IV, records related to diagnosed pre-excitation syndrome (PS) account for less than 0.025% of the total records. This imbalance phenomenon may cause models to overlook some extremely rare complex samples, leading to a tendency to generate more common predictions during training, thus making it difficult to handle complex prediction tasks, as illustrated in Figure 1.

To address these limitations, we propose the Retrieval-Augmented Time series Diffusion Model (RATD) for complex time series forecasting tasks. Our approach consists of two parts: the embedding-based retrieval and the reference-guided diffusion model. After obtaining a historical time series, it is input into the embedding-based retrieval process to retrieve the k nearest samples as references. The references are utilized as guidance in the denoising process. RATD focuses on making maximum utilization of existing time series datasets by finding the most relevant references in the dataset to the historical time series, thereby providing meaningful guidance for the denoising process. RATD focuses on maximizing the utilization of insufficient time series data and to some extent mitigates the issues caused by data imbalance. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of guidance in existing time series diffusion models. Our approach demonstrates strong performance across multiple datasets, particularly on more complex tasks.

To summarize, our main contributions are summarized as follows:

* To handle complex time series forecasting, we for the first time introduce Retrieval-Augmented Time series Diffusion (RATD), allowing for greater utilization of the dataset and providing meaningful guidance in the denoising process.
* Extra Reference Modulated Attention (RMA) module is designed to provide reasonable guidance from the reference during the denoising process. RMA effectively simply integrates information without introducing excessive additional computational costs.
* We conducted experiments on five real-world datasets and provided a comprehensive presentation and analysis of the results using multiple metrics. The experimental results demonstrate that our approach achieves comparable or better results compared to baselines.

Figure 1: (a) The figure shows the differences in forecasting results between the CSDI  (left) and RATD (right). Due to the very small proportion of such cases in the training set, CSDI struggles to make accurate predictions, often predicting more common results. Our method, by retrieving meaningful references as guidance, makes much more accurate predictions. (b) A comparison between our method’s framework(bottom) and the conventional time series diffusion model framework(top). (c) We randomly selected 25 forecasting tasks from the electricity dataset. Compared to our method, CSDI and MG-TSD  exhibited significantly higher instability. This indicates that the RATD is better at handling complex tasks that are challenging for the other two methods.

Related Work

### Diffusion Models for Time Series Forecasting

Recent advancements have been made in the utilization of diffusion models for time series forecasting. In TimeGrad , the conditional diffusion model was first employed as an autoregressive approach for prediction, with the denoising process guided by the hidden state. CSDI  adopted a non-autoregressive generation strategy to achieve faster predictions. SSSD  replaced the noise-matching network with a structured state space model for prediction. TimeDiff  incorporated future mix-up and autoregressive initialization into a non-autoregressive framework for forecasting. MG-TSD  utilized a multi-scale generation strategy to sequentially predict the main components and details of the time series. Meanwhile, mr-diff  utilized diffusion models to separately predict the trend and seasonal components of time series. These methods have shown promising results in some prediction tasks, but they often perform poorly in challenging prediction tasks. We propose a retrieval-augmented framework to address this issue.

### Retrival-Augmented Generation

The retrieval-augmented mechanism is one of the classic mechanisms for generative models. Numerous works have demonstrated the benefits of incorporating explicit retrieval steps into neural networks. Classic works in the field of natural language processing leverage retrieval augmentation mechanisms to enhance the quality of language generation [16; 10; 4]. In the domain of image generation, some retrieval-augmented models focus on utilizing samples from the database to generate more realistic images [2; 44]. Similarly,  employed memorized similarity information from training data for retrieval during inference to enhance results. MQ-ReTCNN  is specifically designed for complex time series forecasting tasks involving multiple entities and variables. ReTime  creates a relation graph based on the temporal closeness between sequences and employs relational retrieval instead of content-based retrieval. Although the aforementioned three methods successfully utilize retrieval mechanisms to enhance time series forecasting results, our approach still holds significant advantages. This advantage stems from the iterative structure of the diffusion model, where references can repeatedly influence the generation process, allowing references to exert a stronger influence on the entire conditional generation process.

## 3 Preliminary

The forecasting task and the background knowledge about the conditional time series diffusion model will be discussed in this section. To avoid conflicts, we use the symbol "s" to represent the time series, and the "t" denotes the t-th step in the diffusion process.

**Generative Time Series Forecasting.** Suppose we have an observed historical time series \(^{H}=\{s_{1},s_{2},,s_{l}\,|\,s_{i}^{d}\}\), where \(l\) is the historical time length, \(d\) is the number of features per observation and \(s_{i}\) is the observation at time step \(i\). The \(^{P}\) is the corresponding prediction target \(\{s_{l+1},s_{l+2},,s_{l+h}\,|\,s_{l+i}^{d^{{}^{}}}\}\) (\(d^{{}^{}} d\)), where \(h\) is the prediction horizon. The task of generative time series forecasting is to learn a density \(p_{}(^{P}|^{H})\) that best approximates \(p(^{P}|^{H})\), which can be written as:

\[_{p_{}}D(p_{}(^{P}|^{H})||p(^{P}|^{H})),\] (1)

where \(\) denotes parameters and \(D\) is some appropriate measure of distance between distributions. Given observation \(x\) the target time series can be obtained directly by sampling from \(p_{}(^{P}|^{H})\). Therefore, we obtain the time series \(\{s_{1},s_{2},,s_{n+h}\}=[^{H},^{P}]\).

**Conditional Time Series Diffusion Models.** With observed time series \(^{H}\), the diffusion model progressively destructs target time series \(^{P}_{0}\) (equals to the \(^{P}\) mentioned in the previous context) by injecting noise, then learns to reverse this process starting from \(^{P}_{T}\) for sample generation. For the convenience of expression, in this paper, we use \(_{t}\) to refer to the t-th time series in the diffusion process, with the letter "P" omitted. The forward process can be formulated as a Gaussian process with a Markovian structure:

\[ q(_{t}|_{t-1})&:=(_{t};}_{t-1},^{H},_{t}),\\ q(_{t}|_{0})&:=(_{t} ;_{t}}_{0},^{H},(1-_{t}) ),\] (2)where \(_{1},,_{T}\) denotes fixed variance schedule with \(_{t}:=1-_{t}\) and \(_{t}:=_{s=1}^{t}_{s}\). This forward process progressively injects noise into data until all structures are lost, which is well-approximated by \((0,)\). The reverse diffusion process learns a model \(p_{}(_{t-1}|_{t},^{H})\) that approximates the true posterior:

\[p_{}(_{t-1}|_{t},^{H}):=(_{t-1};_{ }(_{t}),_{}(_{t}),^{H}),\] (3)

where \(_{}\) and \(_{}\) are often computed by the Transformer. Ho _et al._ improve the diffusion training process and optimize following objective:

\[(_{0})=_{t=1}^{T}}_{q(_{t}|_{0}|^{H})}||_{}(_{t},t|^{H})-(_{t}, _{0}|^{H})||^{2},\] (4)

where \((_{t},_{0}|^{H})\) is the mean of the posterior \(q(_{t-1}|_{0},_{t})\) which is a closed from Gaussian, and \(_{}(_{t},t|^{H})\) is the predicted mean of \(p_{}(_{t-1}_{t}|^{H})\) computed by a neural network.

## 4 Method

We first describe the overall architecture of the proposed method in 4.1. Then we will introduce the strategy of building datasets in Section 4.2. The embedding-based retrieval mechanisms and reference-guided time series diffusion model are introduced in Section 4.3.

### Framework Overview

Figure 2(a) shows the overall architecture of RATD. We built the entire process based on DiffWave , which combines the traditional diffusion model framework and a 2D transformer structure. In the forecasting task, RATD first retrieves motion sequences from the database base \(^{R}\) based on the input sequence of historical events. These retrieved samples are then fed into the Reference-Modulated Attention (RMA) as references. In the RMA layer, we integrate the features of the input \([^{H},^{t}]\)at time step t with side information \(_{s}\) and the references \(^{R}\). Through this integration, the references guide the generation process. We will introduce these processes in the following subsections.

### Constructing Retrieval Database for Time Series

Before retrieval, it is necessary to construct a proper database. We propose a strategy for constructing databases from time series datasets with different characteristics. Some time series datasets are size-insufficient and are difficult to annotate with a single category label (_e.g._, electricity time series), while some datasets contain complete category labels but exhibit a significant degree of class imbalance (_e.g._, medical time series). We use two different definitions of databases for these two different types of datasets. For the first definition, the entire training set is directly defined as the database \(^{}\):

\[^{}:=\{_{i}|_{i}^{ {train}}\}\] (5)

where \(_{i}=\{s_{i},,s_{i+l+h}\}\) is the time series with length \(l+h\), and \(^{}\) is the training set. In the second way, the subset containing samples from all categories in the dataset is defined as the database

Figure 2: **Overview** of the proposed RATD. The historical time series \(^{H}\) is inputted into the retrieval module to for the corresponding references \(^{R}\). After that, \(^{H}\) is concatenated with the noise as the main input for the model \(_{}\). \(^{R}\) will be utilized as the guidance for the denoising process.

\[^{R^{}}:\] (6)

where \(x_{i}^{k}\) is the \(i\)-th sample in the \(k\)-th class of the training set, with a length of \(l+h\). \(\) is the category set of the original dataset. For brevity, we represent both databases as \(^{R}\).

### Retrieval-Augmented Time Series Diffusion

Embedding-Based Retrieval MechanismFor time forecasting tasks, the ideal references \(\{s_{i},,s_{i+h}\}\) would be samples where preceding \(n\) points \(\{s_{i-n},,s_{i-1}\}\) is most relevant to the historical time series \(\{s_{j},,s_{j+n}\}\) in the \(^{R}\). In our approach, the overall similarity between time series is of greater concern. We quantify the reference between time series using the distance between their embeddings. To ensure that embeddings can effectively represent the entire time series, pre-trained encoders \(E_{}\) are utilized. \(E_{}\) is trained on representation learning tasks, and the parameter set \(\) is frozen in our retrieval mechanism. For time series (with length \(n+h\)) in \(^{R}\), their first \(n\) points are encoded, thus the \(^{R}\) can be represented as \(^{R}_{}\):

\[^{R}_{}=\{\{i,E_{}(^{i}_{[0:n]}),^{i}_{[ n:n+h]}\}|^{i}^{R}\}\] (7)

where \([p:q]\) refers to the subsequence formed by the \(p\)-th point to the \(q\)-th point in the time series. The embedding corresponding to the historical time series can be represented as \(^{H}=E_{}(^{H})\). We calculate the distances between \(^{H}\) and all embeddings in \(^{R}_{}\) and retrieve the references corresponding to the \(k\) smallest distances. This process can be expressed as:

\[(^{H})=*{arg\,min}_{^{i} ^{R}_{}}||^{H}-E_{}(^{i}_{[0:n]})||^{2}\] (8) \[^{R}=\{^{j}_{[n:n+h]}| j(^{H})\}\]

where \(()\) represents retrieved index given \(_{}\). Thus, we obtain a subset \(^{R}\) of \(^{R}\) based on a query \(^{H}\), _i.e.\(_{k}:^{H},^{R}^{R}\)_, where \(|^{R}|=k\).

Reference-Guided Time Series Diffusion ModelIn this section, we will introduce our reference-guided time series diffusion model. In the diffusion process, the forward process is identical to the traditional diffusion process, as shown in Equation (2). Following  the objective of the reverse process is to infer the posterior distribution \(p(^{tar}|^{c})\) through the subsequent expression:

\[p(|^{H})= p(_{T}|^{H})_{t=1}^{T}p_{}( _{t-1}|_{t},^{H},^{R})_{1:T},\] (9)

where \(p(_{T}|^{H})(_{T}|^{H},)\), \(p_{}(_{t-1}|_{t},^{H},^{R})\) is the reverse transition kernel from \(_{t}\) to \(_{t-1}\) with a learnable parameter \(\). Following most of the literature in the diffusion model, we adopt the assumption:

\[p_{}(_{t-1}|_{t},)=(_{t-1};_{ }(_{t},^{H},^{R},t),_{}(_{t}, ^{H},^{R},t))\] (10)

where \(_{}\) is a deep neural network with parameter \(\). After similar computations as those in , \(_{}(_{t},^{H},^{R},t))\) in the backward process is approximated as fixed. In other words, we can achieve reference-guided denoising by designing a rational and robust \(_{}\).

Figure 3: The structure of \(_{}\). (a) The main architecture of \(_{}\) is the time series transformer structure that proved effective. (b) The structure of the proposed RMA. We integrate three different features through matrix multiplication.

Denoising Network ArchitectureSimilar to DiffWave  and CSDI , our pipeline is constructed on the foundation of transformer layers, as shown in Figure 3. However, the existing framework cannot effectively utilize the reference as guidance. Considering attention modules to integrate the \(^{R}\) and \(_{t}\) as a reasonable intuition, we propose a novel module called Reference Modulated Attention (RMA). Unlike normal attention modules, we realize the fusion of three features in RMA: the current time series feature, the side feature, and the reference feature. To be specific, RMA was set at the beginning of each residual module Figure 3. We use 1D-CNN to extract features from the input \(_{t}\), references \(^{R}\), and side information. Notably, we concatenate all references together for feature extraction. Side information consists of two parts, representing the correlation between variables and time steps in the current time series dataset Appendix B. We adjust the dimensions of these three features with linear layers and fuse them through matrix dot products. Similar to text-image diffusion models , RMA can effectively utilize reference information to guide the denoising process, while appropriate parameter settings prevent the results from overly depending on the reference.

Training ProcedureTo train RATD (_i.e._, optimize the evidence lower bound induced by RATD), we use the same objective function as previous work. The loss at time step \(t-1\) are defined as follows respectively:

\[ L_{t-1}^{(x)}&=_{t}^{2}}\|_{}(_{t},}_{0})-(_{t}, }_{0})\|^{2}\\ &=_{t}\|_{0}-}_{0}\|\] (11)

where \(}_{0}\) are predicted from \(_{t}\), and \(_{t}=_{t-1}_{t}^{2}}{2_{t}^{2}(1-_{t})^{2}}\) are hyperparameters in diffusion process. We summarize the training procedure of RATD in Algorithm 1 and highlight the differences from the conventional models, in cyan. The process of sampling is shown in Appendix A.

```
0: Time series dataset \(^{}\), neural network \(_{}\),, diffusion step \(T\), external database \(^{R}\), pre-trained encoder \(E_{}\), number of references \(k\)
1: Retrieve references with top-\(k\) high similarity from \(^{R}\) using \(E\) to obtain \(^{R}\) as described in Section 4.3
2:while\(_{}\) not converge do
3: Sample diffusion time \(t(0,,T)\)
4: Compute the side feature \(_{s}\)
5: Perturb \(_{0}\) to obtain \(_{t}\)
6: Predict \(}_{0}\) from \(_{t}\), \(_{s}\) and \(^{R}\) (Equation (10))
7: Compute loss \(L\) with \(}_{0}\) and \(_{0}\) (Equation (11))
8: Update \(\) by minimizing \(L\)
9:endwhile ```

**Algorithm 1** Training Procedure of RATD

## 5 Experiments

### Experimental Setup

**Datasets** Following previous work [45; 38; 8; 30], experiments are performed on four popular real-world time series datasets: (1) _Electricity_+, which includes the hourly electricity consumption data from 321 clients over two years:, (2) _Wind_, which contains wind power records from 2020-2021. (3) _Exchange_, which describes the daily exchange rates of eight countries (Australia, British, Canada, Switzerland, China, Japan, New Zealand, and Singapore); (4) _Weather_+, which documents 21 meteorological indicators at 10-minute intervals spanning from 2020 to 2021.; Besides, we also applied our method to a large ECG time series dataset: MIMIC-IV-ECG . The MIMIC-IV-ECG dataset contains clinical electrocardiogram data from over 190,000 patients and 450,000 hospitalizations at Beth Israel Deaconess Medical Center (BIDMC).

**Baseline Methods** To comprehensively demonstrate the effectiveness of our method, we compare RATD with four kinds of time series forecasting methods. Our baselines include (1) Time series diffusion models, including CSDI , m-Diff , D\({}^{3}\)VAE , TimeDiff ; (2) Recent time series forecasting methods with frequency information, including FiLM , Fedformer  and FreTS  ; (3) Time series transformers, including PatchTST , Autotformer , Pyraformer , Informer  and iTransformer ; (4) Other popular methods, including TimesNet , SciNet , Nlinear , DLinear  and NBeats .

**Evaluation Metric** To comprehensively assess our proposed methodology, our experiment employs three metrics: (1) Probabilistic forecasting metrics: Continuous Ranked Probability Score (CRPS) on each time series dimension . (2) Distance metrics: Mean Squared Error (MSE), and Mean Average Error(MAE) are employed to measure the distance between predictions and ground truths.

**Implementation Details** The length of the historical time series was 168, and the prediction lengths were (96, 192, 336), with results averaged. All experiments were conducted on an Nvidia RTX A6000 GPU with 40GB memory. During the experiments, the second strategy of conducting \(^{R}\) was employed for the MIMIC dataset, while the first strategy was utilized for the other four datasets. To reduce the training cost, we preprocessed the retrieval process by storing the reference indices of each sample in the training set in a dictionary. During the training on the diffusion model, we accessed this dictionary directly to avoid redundant retrieval processes. More details are shown in Appendix B.

### Main Results

Table 1 presents the primary results of our experiments on four daily datasets. Our approach surpasses existing time series diffusion models. Compared to other time series forecasting methods, our approach exhibits superior performance on three out of four datasets, with competitive performance on the remaining dataset. Notably, we achieve outstanding results on the wind dataset. Due to the lack of clear short-term periodicity (daily or hourly), some prediction tasks in this dataset are exceedingly challenging for other models. Retrieval-augmented mechanisms can effectively assist in addressing these challenging prediction tasks.

Figure 4 presents a case study randomly selected from our experiments on the wind dataset. We compare our prediction with iTransformer and two popular open-source time series diffusion models, CSDI and D\({}_{3}\)VAE. Although CSDI and D\({}_{3}\)VAE provide accurate predictions in the initial short-term period, their long-term predictions deviate significantly from the ground truth due to the lack of guidance. iTransformer captures rough trends and periodic patterns, yet our method offers higher-quality predictions than the others. Furthermore, through the comparison between the predicted

   Dataset &  &  &  &  \\  Metric & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS \\ 
**RATD (ours)** & **0.013** & **0.073** & **0.339** & **0.784** & **0.579** & **0.673** & 0.151 & 0.246 & **0.373** & **0.281** & **0.293** & **0.301** \\ TimeDiff & 0.018 & 0.091 & 0.589 & 0.896 & 0.687 & 0.917 & 0.193 & 0.305 & 0.490 & 0.327 & 0.312 & 0.410 \\ CSDI & 0.077 & 0.194 & 0.397 & 0.1066 & 0.741 & 0.941 & 0.379 & 0.579 & 0.480 & 0.356 & 0.374 & 0.354 \\ mr-Diff & 0.016 & 0.082 & 0.397 & 0.881 & 0.675 & 0.881 & 0.173 & 0.258 & 0.429 & 0.296 & 0.324 & 0.347 \\ D\({}_{3}\)VAE & 0.200 & 0.301 & 0.401 & 1.118 & 0.779 & 0.979 & 0.286 & 0.372 & 0.389 & 0.315 & 0.380 & 0.381 \\  Fedformer & 0.133 & 0.233 & 0.631 & 1.113 & 0.762 & 1.235 & 0.238 & 0.341 & 0.561 & 0.342 & 0.347 & 0.319 \\ FreTS & 0.039 & 0.140 & 0.440 & 1.004 & 0.703 & 0.943 & 0.269 & 0.371 & 0.634 & 0.351 & 0.354 & 0.391 \\ FiLM & 0.016 & 0.079 & 0.349 & 0.984 & 0.717 & 0.798 & 0.210 & 0.320 & 0.671 & 0.327 & 0.336 & 0.556 \\  iTransformer & 0.016 & 0.074 & 0.343 & 0.932 & 0.676 & 0.811 & 0.192 & 0.262 & 0.402 & 0.358 & 0.401 & 0.318 \\ Autoformer & 0.056 & 0.167 & 0.769 & 1.083 & 0.756 & 1.201 & 1.026 & 0.313 & 0.602 & 0.360 & 0.354 & 0.754 \\ Pyraformer & 0.032 & 0.112 & 0.532 & 1.061 & 0.735 & 0.994 & 0.273 & 0.379 & 0.732 & 0.394 & 0.385 & 0.485 \\ Informer & 0.073 & 0.192 & 0.631 & 1.168 & 0.772 & 1.065 & 0.292 & 0.383 & 0.749 & 0.385 & 0.364 & 0.821 \\ PatchTST & 0.047 & 0.153 & 0.629 & 1.001 & 0.672 & 1.026 & 0.225 & 0.394 & 0.801 & 0.782 & 0.670 & 0.370 \\  SCINet & 0.038 & 0.137 & 0.624 & 1.055 & 0.732 & 0.997 & 0.171 & 0.280 & 0.499 & 0.329 & 0.344 & 0.814 \\ DLinear & 0.022 & 0.102 & 0.538 & 0.899 & 0.686 & 0.957 & 0.215 & 0.336 & 0.527 & 0.488 & 0.444 & 0.791 \\ NLinear & 0.019 & 0.091 & 0.481 & 0.989 & 0.706 & 0.974 & 0.147 & **0.239** & 0.419 & 0.369 & 0.328 & 0.738 \\ TimesNet & 0.023 & 0.120 & 0.520 & 0.982 & 0.771 & 1.001 & **0.141** & 0.361 & 0.403 & 0.313 & 0.364 & 0.491 \\ NBeats & 0.016 & 0.081 & 0.399 & 1.069 & 0.741 & 0.981 & 0.269 & 0.370 & 0.697 & 0.744 & 0.420 & 0.871 \\   

Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.

results and references in the figure, although references provide strong guidance, they do not explicitly substitute for the entire generated results. This further validates the rationality of our approach.

Table 2 presents the testing results of our method on the MIMIC-IV-ECG dataset. We selected some powerful open-source methods as baselines for comparison. Our experiments are divided into two parts: in the first part, we evaluate the entire test set, while in the second part, we select rare cases (those accounting for less than 2% of total cases) from the test set as a subset for evaluation. Prediction tasks in the second part are more challenging for deep models. In the first experiment, our method achieved results close to iTransformer, while in the second task, our model significantly outperformed other methods, demonstrating the effectiveness of our approach in addressing challenging tasks.

### Model Analysis

Influence of Retrieval MechanismTo investigate the impact of the retrieval augmentation mechanism on the generation process, we conducted an ablation study and presented the results in Table 3. The study addresses two questions: whether the retrieval augmentation mechanism is effective and which retrieval method is most effective. Firstly, we removed our retrieval augmentation mechanism from the RATD as a baseline. Besides, the model with random time series guidance is another baseline. The references retrieved by other methods have all positively impacted the prediction results. This suggests that reasonable references are highly effective in guiding the generation process.

We also compared two different retrieval mechanisms: correlation-based retrieval and embedding-based retrieval. The first method directly retrieves the reference in the time domain (_e.g._, using Dynamic Time Warping (DTW) or Pearson correlation coefficient). Our approach adopts the second mechanism: retrieving references through the embedding of time series. From the results, the correlation-based methods are significantly inferior to the embedding-based methods. The former methods fail to capture the key features of the time series, making it difficult to retrieve the best references for forecasting. We also evaluate the embedding-based methods with various encoders for comparison. The comprehensive results show that methods with different encoders do not significantly differ. This indicates that different methods can all extract meaningful references, thereby producing similar improvements in results. TCN was utilized in our experiment because TCN strikes the best balance between computational cost and performance.

Effect of Retrieval DatabaseWe conducted an ablation study on two variables, \(n\) and \(k\), to investigate the influence of the retrieval database \(^{R}\) in RATD, where \(n\) represents the number of samples in each category of the database, and \(k\) represents the number of reference exemplars. The results in Figure 4(a) can benefit the model in terms of prediction accuracy because a larger \(^{R}\) brings higher diversity, thereby providing more details beneficial for prediction and enhancing the generation

Figure 4: Visualizations on _wind_ by CSDI, D3VAE, iTransformer and the proposed RATD (with reference).

process. Simply increasing k does not show significant improvement, as utilizing more references may introduce more noise into the denoising process. In our experiment, the settings of \(n\) and \(k\) are 256 and 3, respectively.

Inference EfficiencyIn this experiment, we evaluate the inference efficiency of the proposed RATD in comparison to other baseline time series diffusion models (TimeGrad, MG-TSD, SSSD). Figure 6 illustrates the inference time on the multivariate _weather_ dataset with varying values of the prediction horizon (\(h\)). While our method introduces an additional retrieval module, the sampling efficiency of the RATD is not low due to the non-autoregressive transformer framework. It even slightly outperforms other baselines across all \(h\) values. Notably, TimeGrad is observed to be the slowest, attributed to its utilization of auto-regressive decoding.

Effectiveness of Reference Modulated AttentionTo validate the effectiveness of the proposed RMA, we designed additional ablation experiments. In these experiments, we used the CSDI architecture as the baseline method and added extra fusion modules to compare the performance of these modules (linear layer, cross-attention layer, and RMA). The results are shown in the Table 4.

Through our experiments, we found that compared to the basic cross-attention-based approach, RMA can integrate an edge information matrix (representing correlations between time and feature dimensions) more effectively. The extra fusion is highly beneficial in experiments, guiding the model to capture relationships between different variables. In contrast, linear-based methods concatenate inputs and references initially, which prevents the direct extraction of meaningful information from references, resulting in comparatively modest performance.

   Method &  &  &  &  &  \\  Metric & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS \\  MIMIC-IV (All) & 0.174 & **0.263** & 0.299 & 0.219 & 0.301 & 0.307 & 0.193 & 0.311 & 0.310 & 0.268 & 0.331 & 0.369 & **0.172** & 0.270 & **0.293** \\ MIMIC-IV (Rare) & 0.423 & 0.315 & 0.379 & 0.483 & 0.379 & 0.407 & 0.627 & 0.359 & 0.464 & 0.499 & 0.359 & 0.324 & **0.206** & **0.299** & **0.301** \\   

Table 2: Performance comparisons on MIMIC datasets with popular time series forecasting methods. Here, “MIMIC-IV (All)” refers to the model’s testing results on the complete test set, while “MIMIC(Rare)” indicates the model’s testing results on a rare disease subset.

   Dataset &  &  &  &  \\  Metric & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS & MSE & MAE & CRPS \\  - & 0.077 & 0.194 & 0.397 & 1.066 & 0.741 & 0.941 & 0.379 & 0.579 & 0.480 & 0.356 & 0.374 & 0.354 \\ Random & 0.153 & 0.203 & 0.599 & 1.593 & 0.903 & 0.996 & 0.471 & 0.639 & 0.701 & 0.431 & 0.473 & 0.461 \\  DTW & 0.075 & 0.195 & 0.403 & 1.073 & 0.791 & 0.942 & 0.357 & 0.564 & 0.449 & 0.361 & 0.375 & 0.356 \\ Pearson & 0.091 & 0.207 & 0.411 & 1.099 & 0.831 & 0.953 & 0.361 & 0.571 & 0.483 & 0.370 & 0.364 & 0.391 \\  DLinear & 0.022 & 0.081 & 0.361 & 0.941 & 0.735 & 0.895 & **0.159** & **0.250** & **0.390** & 0.297 & 0.304 & 0.332 \\ Informer & 0.019 & 0.078 & 0.371 & 0.841 & 0.645 & 0.861 & 0.170 & 0.263 & 0.411 & 0.291 & 0.305 & 0.330 \\ TimesNet & **0.013** & 0.074 & 0.341 & **0.781** & **0.572** & **0.669** & 0.167 & 0.263 & 0.397 & 0.286 & 0.295 & **0.311** \\ TCN & **0.013** & **0.073** & **0.339** & 0.784 & 0.579 & 0.673 & 0.161 & 0.256 & 0.391 & **0.281** & **0.293** & 0.313 \\   

Table 3: Ablation study on different retrieval mechanisms. “-” means no references was utilized and “Random” means references are selected randomly. Others refer to what model we use for retrieval references.

Figure 5: The effect of hyper-parameter \(n\) and \(k\). Figure 6: Inference time (ms) on the Electricity with different prediction horizon \(h\)

Predicting \(_{0}\) vs Predicting \(\).Following the formulation in Section 4.3, our network is designed to forecast the latent variable \(_{0}\). Since some existing models [28; 36] have been trained by predicting an additional noise term \(\), we conducted a comparative experiment to determine which approach is more suitable for our framework. Specifically, we maintained the network structure unchanged, only modifying the prediction target to be \(\). The results are presented in Table 5. Predicting \(_{0}\) proves to be more effective. This may be because the relationship between the reference and \(_{o}\) is more direct, making the denoising task relatively easier.

Rma positionWe investigate the best position of RMA in the model. Front, middle, and back means we set the RMA in the front of, in the middle of, and the back of two transformer layers, respectively. We found that placing RMA before the bidirectional transformer resulted in the most significant improvement in model performance. This also aligns with the intuition of network design: cross-attention modules placed at the front of the model tend to have a greater impact.

## 6 Discussion

Limitation and Future WorkAs a transformer-based diffusion model structure, our approach still faces some challenges brought by the transformer framework. Our model consumes a significant amount of computational resources dealing with time series consisting of too many variables. Additionally, our approach requires additional preprocessing (retrieval process) during training, which incurs additional costs on training time (around ten hours).

ConclusionIn this paper, we propose a new framework for time series diffusion modeling to address the forecasting performance limitations of existing diffusion models. RATD retrieves samples most relevant to the historical time series from the constructed database and utilize them as references to guide the denoising process of the diffusion model, thereby obtaining more accurate predictions. RATD is highly effective in solving challenging time series prediction tasks, as evaluated by experiments on five real-world datasets.