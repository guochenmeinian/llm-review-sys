# Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification

Neel Guha*

Stanford University

&Mayee F. Chen*

Stanford University

&Kush Bhatia*

Stanford University

&Azalia Mirhoseini

Anthropic

&Frederic Sala

University of Wisconsin-Madison

&Christopher Re

Stanford University

These authors contributed equally to this work.

###### Abstract

Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. The challenge is that while writing an initial prompt is cheap, improving a prompt is costly--practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. Our work asks whether it is possible to improve prompt-based learning _without_ additional labeled data. We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction. We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction. In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.

## 1 Introduction

Acquiring labeled data for domains like medicine and law is essential to training machine learning models or performing basic data analysis (e.g., "how many contracts contain a choice-of-forum clause" or "how many patient medical histories discuss an adverse reaction to a drug?") . However, building large labeled datasets is difficult, and efforts like  show that manual labeling with domain experts is cost-prohibitive. Recent works have begun exploring if language models (LMs) could learn annotation tasks _in-context_ and replace manual labeling at scale . The promise of this approach is that LMs' in-context capabilities enable them to learn tasks from descriptions of the the task (i.e., _prompts_). However, the challenge is that producing high performanceprompts is still expensive, as practitioners require labeled data in order to measure the impact of modifications to a prompt . Existing work has thus focused on how domain experts can optimally construct prompts for a task (_prompt-engineering_), while minimizing reliance on labeled data . Yet, because language models are sensitive to even small changes in prompt language, these techniques are imperfect and still produce erroneous predictions .

Our work approaches the challenge of improving prompt performance without labels from an orthogonal perspective: given the predictions of any prompted LM, can we identify and correct mis-predictions using _unlabeled_ data? We describe this as the problem of _prompt-patching_. In the context of data annotation tasks, prompt-patching methods should meet three goals. First, they should be theoretically explainable, so that practitioners can understand when and how to apply them. Second, they should be fast, so that practitioners can efficiently integrate them into existing workflows. Finally, they should rarely be wrong, so that they don't worsen the performance of predictions.

Our work presents Embroid: a method for automatically identifying and correcting LM predictions with unlabeled data and no expert supervision. Recent work has shown that for many tasks, samples close-by in embedding spaces (produced by models like BERT) have the same label . Embroid applies this intuition to the prompt-patching regime. Specifically, after LM predictions for all samples have been generated, Embroid retrieves the \(k\) most similar samples for each input, under \(N\) different embedding functions. For each embedding function, Embroid computes a scaled-modified majority vote over the LM's predictions for the \(k\) retrieved samples. Embroid then combines these \(N\) votes with the original LM prediction for the test sample using a simple latent variable graphical model that is learned with a fast method-of-moments estimator . The intuition behind Embroid is that good prompts are _smooth_ with respect to their predictions over a dataset--samples which are proximate under an embedding function should receive consistent predictions. Thus, modifying the predictions of a prompt to increase neighborhood agreement can improve the accuracy of those predictions. Lastly, because a single embedding space may imperfectly capture similarities between samples, retrieving neighbors from multiple embedding spaces improves robustness .

Because Embroid relies on weak-supervision--the subject of recent rigorous study --it is possible to theoretically analyze and explain _why_ and _when_ Embroid will improve performance. In particular, we find that performance is a function of the quality of the embeddings and the performance of the initial prompt. We also empirically study Embroid, conducting experiments over six LMs, on up to 95 tasks, with several different prompt strategies. We find that Embroid rarely worsens performance, and often improves F1 by a substantial margin. For instance, Embroid improves GPT-3.5 by an average of 4.9 points F1 per task, and GPT-JT by an average of 7.3 points per task. The magnitude of Embroid's gains are such that it enables a 1.3B parameter model to outperform an instruction-tuned 6.7B parameter model. Embroid is also complementary to advanced prompt engineering strategies, and achieves performance improvements when applied to prompts designed

Figure 1: The Embroid method for prompt-patching.

using chain-of-thought , AMA , and selective annotation . Finally, Embroid can be extended to specialized domains like law, through the use of already-available domain specific embeddings.

Succinctly, our contributions in this paper are: (1) Embroid, a simple prompt-patching framework for improving LM predictions over text classification tasks; (2) a theoretical analysis of Embroid which explains performance improvements in terms of embedding smoothness and base accuracy; and (3) an empirical evaluation of Embroid covering up to 95 tasks and six different LMs.

## 2 Related work

Improving LM performanceImproving the in-context generalization abilities of LMs has been intensely studied. The first family of approaches focuses on adapting LMs in order to make them more amenable to prompting. This includes task-specific finetuning [26; 27; 47], training on instruction data [9; 59], RLHF , and weight-surgery methods which attempt to "correct" incorrect information stored in model weights [10; 22; 43; 44]. A second family of approaches explores strategies for optimizing prompts to models, either through the specific textual features of the prompt [29; 45; 64], the use of task decompositions or LM recursion , implicit prompt representations [36; 37], or external databases . Prompt-patching, in contrast, focuses on identifying mistakes in the predictions generated from a particular prompt. The most related approaches are aggregation methods, in which the outputs of multiple prompts are combined with an ensembling method [2; 41]. We find that Embroid outperforms many such baselines, and can be applied to enhance their outputs.

Weak supervisionEmbroid leverages statistical techniques developed in the weak supervision literature. The objective in weak supervision is to generate probabilistic labels for unlabeled data by combining the predictions of multiple noisy heuristics [14; 53; 54; 57; 62; 66]. Embroid's novelty is that it uses embeddings to construct additional synthetic predictions, which are combined with the original predictions. In contrast, recent weak supervision approaches which incorporate embeddings use them to produce more fine-grained accuracy parameters , detect and discard training points , and as the basis for label propagation with final weak supervision predictions .

## 3 Problem setup and background

Problem setupOur problem setup comprises three elements: an unlabeled dataset, predictions from a LM for each sample in this dataset, and embedding representations of our dataset. Our goal is to improve the accuracy of LM predictions, by using the embedding representations to identify and correct predictions likely to be incorrect. Because recent work has explored how predictions from multiple prompts can be combined for a task , we present a generalized version of Embroid in which we have access to multiple LM predictions. In our empirical evaluation however, we show that Embroid performs well regardless of the number of predictions per sample available.

More formally, we focus on a binary classification task where \(x\) denotes a sentence or paragraph and \(y=\{-1,1\}\) is the binary label. We assume we are given an unlabeled dataset \(=\{x_{i}\}_{i=1}^{n_{u}}\) of \(n_{u}\) points. Each point \(x\) is sampled i.i.d. from a distribution \(_{x}\), and there exists a true underlying distribution \(\) on the joint \((x,y)\). Following the true few-shot regime , we assume the only labels available are those used in the prompt. We denote a language model (e.g., GPT-3) as \(_{}\), and a task-specific prompt as \(\), which prepends task instructions to input \(x\) (e.g., "Does the clause contain an audit provision? Yes or No."). We consider the "prompt" to include both the task description and the in-context samples, consistent with . The prediction this prompt induces for \(_{}\) over \(x\) is \(_{}((x))\).1 Varying \(\) by changing the task description, in-context demonstrations, or punctuation will alter the prediction generated for \(x\). For a set of \(m\) prompts \([_{1},,_{m}]\), we denote their respective predictions on \(x\) as a vector of _weak sources_\((x)=[_{}(_{1}(x)),,_{}(_{m}(x))]\). For convenience, we denote \(_{}(_{i}(x))\) as \(_{i}(x)\) or \(_{i}\) when the \(x\) is obvious, and similarly use \(\) instead of \((x)\). We distinguish between two regimes: in the _single-prompt_ regime with \(m=1\), we have access to a LM prediction for each point \(x\), while in the _multi-prompt_ regime with \(m>1\), we have access to multiple predictions.

We assume access to \(N\) embedding models \(=[E_{1},,E_{N}]\), each represented as a fixed mapping \(E_{i}:_{i}\) from an input \(x\) to an embedding vector \(z\). These auxiliary embedding models provide representations of \(x\) which encode different types of similarity information. Through model repositories like HuggingFace , it is possible to download a number of models which generate representations for text sentences (e.g., BERT or RoBERTa ). These models have the property that semantically similar sentences are close-by in embedding space .

Weak supervision backgroundWeak supervision uses a graphical model to combine votes from multiple noisy sources into a single prediction, by estimating the accuracy of each source. It models \((y,(x))\) as a latent variable graphical model and uses \(=_{y}(y|(x))\) to produce label estimates, where \(\) represents the learned model. The graphical model is based on a graph \(G=(V,E)\), where \(V=y\) and \(E\) consists of edges from \(y\) to each \(_{j}\). We assume no dependencies between sources, although simple extensions can incorporate them . The formal graphical model is:

\[(y,(x))=(y}_{(I)}+ (x)y}_{(II)})\] (1)

where \(Z\) is the partition function used for normalization, \((I)\) represents a label balance term with parameter \(_{y}\) controlling the prior of \((y=1)\), and \((II)\) represents the source accuracy term where each \(_{i}\) is an _accuracy parameter_ for the \(i\)th source. Note that from this model, sources are conditionally independent: \(_{i}_{j}|y\) for any \(i,j[m]\). Our use of this model has two steps. First, we must learn the accuracy parameters of \((y,(x))\) without access to \(y\). We use the triplet method introduced in , which is an efficient method-of-moments estimator for the parameters. Then, at inference we compute \((y|(x))\). Appendix B contains more details.

## 4 Embroid

First, Embroid uses the embedding models \(\) to compute additional votes for each \(x\). Let \(_{j,k}(x)\) be the \(k\)-nearest neighbors of sample \(x\) under the embedding function \(E_{j}\). We define the smoothed neighborhood prediction vector \(_{,j}(x)\{-1,0,1\}^{m}\) as follows, with \(_{,j}[i](x)\) being the \(i\)th element:

\[_{j}[i](x)=_{ _{j,k}(x)}_{i}()\\ _{,j}[i](x)=1&_{j} [i](x)>_{i}^{+}\\ -1&_{j}[i](x)<_{i}^{-}\\ 0&\] (2)where \(_{i}^{+}[-1,1]\) and \(_{i}^{-}[-1,1]\) act as shrinkage parameters for \(_{i}\) which control the level of agreement amongst the neighbors of \(x\) necessary to generate a particular vote. The scalar \(_{,j}[i](x)\) is the average vote of \(_{i}\) amongst the neighbors of \(x\) in \(E_{j}\). When \(_{,j}[i](x)\) is sufficiently positive, i.e., \(_{,j}[i](x)>_{i}^{+}\), Embroid sets \(_{,j}[i](x)\) to be a positive vote. When \(_{,j}[i](x)\) is sufficiently negative, i.e., \(_{,j}[i](x)<_{i}^{-}\), Embroid sets \(_{,j}[i](x)\) to be a negative vote. Otherwise, \(_{,j}[i](x)\) is set to be an abstain. The intuition is that \(_{,j}[i](x)\) will be an accurate vote over \(x\) whenever two conditions are met: (1) the LM is generally accurate, i.e., \(_{j}\) is usually correct, and (2) \(E_{j}\) is _smooth_, i.e., nearest-neighbors share the same task label.

Next, we augment our base model in equation (1) to incorporate these auxiliary neighborhood predictions \(_{}=[_{,1},,_{ ,N}]\{-1,0,1\}^{Nm}\) computed using the embeddings:

\[(y,,_{})= _{y}y+^{}y+_{j=1}^{N}_{ j}^{}_{,j}y,\] (3)

where the vector \(_{j}^{m}\) represents the quality parameters for the \(j^{th}\) embedding model when used with the \(m\) different prompts. To solve this model and produce label estimates, we note that it has the same format as (1) if we concatenate \(\) and \(_{}\) into one set of weak sources. Therefore, we can use the triplet method from  to learn parameters and output estimates \((y|(x),_{}(x))\) for each \(x\) at inference time (see Appendix B for details).

Parameters \(\) and \(_{j}\) in (3) allow us to trade-off two different sources of information--one presented by directly prompting an LM to obtain a label and the other by incorporating similarity information from the embedding models--and to further account for varying error modes among the embedding models. Our use of the neighborhood predictions in (3) yields a more expressive model than the standard weak supervision framework solely on LLM predictions in (1), which we can recover when \(k=0\), and can thus help make corrections to the LLM predictions. In practice, we find that setting \(_{i}^{+}=_{i}^{-}=[_{i}]\) (i.e., the average source vote) yields good performance (Appendix G).

## 5 Theoretical analysis

We analyze Embroid, discussing the advantages of using \(_{}\) in addition to \(\), and show that embedding smoothness and base prediction accuracy play a critical role in information gain. Appendix F provides synthetics demonstrating these tradeoffs and comparing to weak-supervision baselines.

First, we provide a result on the generalization error of our model \((y|,_{})\). Define the generalization error as the expected cross-entropy loss, \(L(,_{},)= _{y,(x),_{}(x), }[-(y|(x),_{ }(x))]\). We use \([_{1},,_{(N+1)m}]\) to represent \([,_{}]\) and denote by \(a_{}=_{i}[_{i}(x)y]\) the largest accuracy (scaled to \([-1,1]\)) of any source, and by the minimum expected pairwise product between any two sources. Assume that all sources are better than random, e.g., \((_{i}=y)>0.5\). These terms and assumptions are from using the triplet method.

**Proposition 5.1**.: _Suppose that the data \(x,y,,_{}\) follows the model in (3). The generalization error of \((y|,_{})\) can be decomposed into_

\[L(,_{},) ,_{})}_{ }+}}_{}+o( 1/n_{u}),\]

_where \(C=}^{2})}{8b_{}^{2}(1-a_{}^{2})} }^{2}}+}^{2}}\)._

In the bound above, the variance term comes from estimation error when learning the parameters via the triplet method. The irreducible error depends on quality of \(\) and \(_{}\). If knowledge of the LLM prediction and neighborhood prediction significantly reduces uncertainty in \(y\), the conditional entropy term \(H(y|(x),_{}(x))\) is low.

Information gain from using both \(,_{}\)We compare upper bounds on generalization error when both \(,_{}\) are modeled versus when only \(\) is modeled, as in (1) corresponding to classical weak supervision. Based on the bound in Proposition 5.1, modeling both \(\) and \(_{}\) increases the variance term by a constant multiplicative factor.

Here, we examine how the irreducible error is affected, that is, the difference \(H(y|)-H(y|,_{})\). Since this quantity is always nonnegative, we focus on bounding the _pointwise_ difference in conditional entropy--which we call the information gain--for a given \(x_{0}\) on which the LLM is incorrect. For simplicity, suppose we have one embedding \(E\). An embedding \(E\) is _\(M\)-smooth_ with respect to the label if

\[(=c|y=c,\|E(x)-E()\|) M_{E}( ),\] (4)

where \(c\), \(>0\) and \(M_{E}()\) is decreasing in its input. Note that this definition requires knowledge of ground-truth labels, and is thus impossible to use as a metric for selecting which embeddings to use.

Define \(_{i}=(_{i}=y)\) as the accuracy of \(_{i}\) and \(p_{}=(y=1|(x_{0}))\) as the prediction on \(x_{0}\) given only access to \(\). Let \(_{k}=_{_{k}(x)}\|E(x)-E()\|\) be the maximum distance between \(x_{0}\) and its \(k\) neighbors. Without loss of generality, assume the label on \(x_{0}\) is \(y=1\).

**Theorem 5.2**.: _Assume that \(E\) is \(M\)-smooth. The pointwise information gain on \(x_{0}\) is_

\[H(y|(x_{0}))-H(y|(x_{0}),_{ }(x_{0}))\] \[2(1-p_{})_{i=1}^{m}1-[-2k( _{_{k},i}-0.5)^{2}]-0.5\]

_where \(_{_{k},i}=_{_{k}}(_{i}()=y)_{i}M_{E}(_{k})\) is the neighborhood accuracy._

A few observations on the bound are in order.

* **Improvement over WS:** If the neighborhood accuracy is bounded sufficiently far from \(\) and \(k\) is large, using Embroid has better irreducible error than just using \(\). For example, setting \(m=1\), \(k=10\), \(_{_{k},i}=0.7\), and \(p_{}=0.25\) gives us an improvement of \(0.076\) nats.
* **Smoothness:** If \(E\) is highly smooth, then \(M_{E}(_{k})\) will be large and irreducible error will be small.
* **Base prediction accuracy:** If the original sources \(\) have high accuracy (\(_{i}\)), irreducible error will be small.

Additionally, we observe that if \(p_{}\) is a high-quality prediction close to the true label \(1\), the information gain is small. Choice of the \(k\) parameter presents a performance trade-off: increasing \(k\) will increase \(_{k}\) and incorporate farther-away, less reliable predictions, but it will also reduce the noise of the majority vote. We also comment on the information gain when using both \(\) and \(_{}\) over just \(_{}\) in Appendix C.

## 6 Results

Our empirical evaluation focuses on three questions: (1) How robust is Embroid's performance across LMs? (2) How does Embroid, as a prompt-patching method, compare to high performance prompt-engineering methods? (3) How sensitive is Embroid to the embeddings and dataset size?

TasksWe study tasks where sentence embeddings can capture information relevant to the task, leading us to focus on sentence classification datasets. We consider a collection of 95 class-balanced sentence classification tasks, derived from binarizing existing multi-class legal, scientific, and general domain classification benchmarks like CUAD, AGNews, DBpedia-14, FewRel, and several others [21; 25; 30; 67; 69].2 Example tasks include, "Classify if the following texts discuss a recording label" or "Classify if the following contractual clauses contain an audit rights provision."Choice of embedding modelsFollowing prior work illustrating the benefits of domain specific representation [20; 71], Embroid uses different embeddings for each task domain. For law tasks, we rely on two BERT-variants trained on different legal corpora [24; 71]. For science tasks, we rely on three BERT-variants trained on science, biology, and medical texts [3; 17; 35]. For general domain tasks, we rely on BERT, Roberta, and SentenceBert embeddings [12; 42; 55].

PromptsPromptsPrompts are constructed using fixed instructions, and by manually selecting three random samples (from each class) as in-context demonstrations (Appendix E). We follow the true few-shot regime , in that we assume the only labeled data available to the data scientist are the labels used for in-context demonstrations. Prior work has found this regime to most realistically represent real-world workflows.

ModelsWe evaluate on two API-access models: GPT-3.5 (text-davinci-003) and J1-Jumbo . Because API models raise significant privacy and compliance concerns for data scientists working with sensitive data , we also evaluate on open-source models. We select models in the 6-7B parameter range, as these are the largest models which fit on commonly available 40GB A100 machines. Specifically, we evaluate Bloom  and OPT . Given the increasing popularity of instruction-tuning, we also evaluate on GPT-JT , an 6.7B parameter instruction tuned version of GPT-J. Because of cost-constraints, we evaluate API-access models on a representative selection of 12 tasks, while evaluating all other models on the full suite of 95 tasks. Appendix D provides details.

### By how much does prompt-patching improve performance?

Performance across LM familiesWe examine if Embroid achieves improvements for different _types_ of LMs. For each LM, we select three different combinations of in-context demonstrations (i.e., three prompts), generate predictions for each prompt, and apply Embroid to independently each prompt's predictions. This produces \(3 95=285\) trials for open-source models, and \(3 12=36\) trials for API-models. We report _win-rate_, i.e., the proportion of trials for which Embroid outperforms the original predictions, and _improvement_, i.e., the average difference in F1 points (across all trials) between Embroid and the original predictions.

As Table 1 illustrates, Embroid improves performance for a substantial proportion of prompts, by a substantial margin, across all models. On GPT-3.5 for instance, Embroid achieves a win-rate of 80.6%, with an average of improvement of 4.9 points. Embroid also improves for open source models, with a win-rate of 91.2% on OPT-6.7 and an average improvement of 11.6 points. Finally, Embroid achieves similar gains on an instruction tuned model, with a win-rate of 89.1% and an average improvement of 7.3 points.

Performance when prompts are goodWe additionally investigate how Embroid's performance improvements change as a function of the performance of the base prompt. Hypothetically, one could imagine that better performing prompts are _smoother_ with respect to embeddings, thus diminishing (or negating) Embroid. In Figure 2 (upper left), we plot the improvement of Embroid against the performance of the base prompt for GPT-JT. Even when the base prompt performs well (i.e., F1 > \(0.8\)), Embroid improves on 89% of tasks by an average of 4.1 points.

    & LM & Win rate (\%) & Avg. Improvement (F1) \\   & J1-Jumbo (17GB) & 72.2 & 10.6 \\  & GPT-3.5 (\(>\) 170B) & 80.6 & 4.9 \\   & Bloom (7.1B) & 91.2 & 10.1 \\  & OPT (6.7B) & 91.2 & 11.6 \\  Instruction Tuned & GPT-JT (6B) & 89.1 & 7.3 \\   

Table 1: We evaluate the extent to which Embroid improves the original prompt on different models in terms of win rate and relative improvement (defined in-line). All models are run with three trials. For each model, we report the percentage of tasks (across all trials) for which Embroid improves, and the average improvement (in F1 points). Additional details provided in Appendix.

Measuring performance in parameter countA trend in recent literature has been to measure the magnitude of improvements to prompt performance in terms of parameter count , by showing how a particular method makes a smaller method equivalent in performance to a larger model. We find that Embroid enables the non-instructed tuned 1.3B GPT-Neo model to outperform an instruction tuned 6.7B model; across all trials, GPT-JT scores an average F1 of 67.8, while Embroid +GPT-Neo-1.3B scores an average of 68.5.

### Comparing prompt-patching to prompt-engineering

Our work distinguishes between prompt-construction methods--which control how a prompt is generated--and prompt-patching methods--which attempt to identify and correct errors in the predictions produced by a prompt. We use Embroid to further study the difference between these frameworks in two ways. First, we compare Embroid's performance improvement over a base prompt to that of several specialized prompting strategies. Second, we examine the extent to which Embroid--when applied to the predictions produced by these prompting strategies--can generate further performance improvements. We study three prompting strategies:

1. Ensemble strategies, in which the predictions of multiple prompts are combined using an unsupervised ensembling model. Specifically, we compare to two ensembling methods previously studied for LLMs (AMA  and majority vote ), one ensembling method which incorporates embedding information (Liger ), and one well regarded weak supervision baseline (FlyingSquid ). Each baseline is run over the predictions generated by three different prompts.
2. Chain-of-thought prompting , in which for each in-context demonstration, we provide a step-by-step explanation for the demonstration's label.
3. Selective annotation (SA) , in which we use embeddings to select a subset of \(k\) data samples to label, and then, for each input sample, retrieve the most similar samples (under some embedding function) from this pool to use as in-context demonstrations.

Ensemble methodsWe evaluate two versions of Embroid. In the first version, we run Embroid with the predictions of only one prompt (Embroid-1). In the second version, we run Embroid with the predictions of three different prompts (Embroid-3). Note that this requires performing inference over the few-shot LM three times for each sample, which can be expensive. This baseline is comparable to applying Embroid to the outputs of an ensemble method. In Table 2, we observe that Embroid-1 is competitive with the ensemble baselines (while using substantially fewer predictions), while Embroid-3 consistently outperforms these baselines (across different LMs).

Chain-of-thoughtWe compare Embroid to chain-of-thought (CoT) prompting for a subset of a representative subset of 10 tasks on GPT-3.5. For each task, we manually construct a base prompt consisting of six demonstrations, and a CoT prompt where an explanation is provided for each demonstration. We first find that Embroid's performance improvement over the base prompt exceeds that of chain-of-thought prompting (Table 3). Using Embroid to modify the base prompt is better

 LM & MV & Liger & FlyingSquid & AMA & Embroid-1 & Embroid-3 \\  J1-Jumbo & 47.4 & 48.7 & 50.5 & 60.7 & 60.4 & **64.5** \\ GPT-3.5 & 81.4 & 82.5 & 82.1 & 84.7 & 83.9 & **86.0** \\ Bloom-7.1B & 54.6 & 55.8 & 54.3 & 63.0 & 64.7 & **69.1** \\ OPT-6.7 & 46.1 & 46.8 & 46.3 & 56.3 & 59.8 & **64.2** \\ GPT-JT & 69.3 & 69.4 & 70.1 & 74.6 & 75.1 & **79.0** \\  

Table 2: We evaluate how Embroid compares to common ensemble approaches for improving prompt prediction performance. All ensemble baselines are run with three sets of predictions. Embroid-1 is run with one set of predictions, and Embroid-3 is run with three set of predictions. For each method, we report the average macro-F1 over all tasks. We observe that Embroid-1 is competitive with ensemble methods which use many more predictions, while Embroid-3 outperforms all other methods by a substantial margin.

on average than CoT prompting, outperforming CoT on six out of the ten tasks. Second, applying Embroid to predictions generated by a CoT prompt yields further improvements, outperforming vanilla CoT predictions on eight of ten tasks.

Selective annotation (SA)We compare Embroid to selective annotation with a label budget of \(\) (Figure 2, upper-right). For each task, we run selective annotation using a domain specific embedding. Embroid (applied to a prompt with randomly chosen samples) outperforms selective annotation with a label budget of 25 samples. When a label budget of 100 samples is available, Embroid improves the performance of a prompt constructed using selective annotation on 88% of tasks, by an average of 4.3 points.

### Ablations

Finally, we perform several ablations of Embroid to study how performance changes as a function of (1) the domain specificity of the embedding used, (2) the quality of the embedding spaces used, and (3) the size of the dataset. Additional ablations are presented in Appendix G.

   Base prompt & +CoT & +Embroid & + CoT + Embroid \\ 
76.3 & 80.1 & 81.9 & **85.4** \\   

Table 3: We evaluate Embroid compared to, and applied to, CoT prompting on GPT-3.5 for a subset of 10 tasks. We report the average across the studied tasks.

Figure 2: **Upper left**: The Embroid F1 plotted against the F1 score for the original prompt for GPT-JT. Even for high performing prompts, Embroid is capable of improving performance. The dashed line \(y=x\) is plotted for visual aid. **Upper right**: A comparison of Embroid to selective annotation (SA) over all tasks for GPT-JT. **Bottom left**: For each task (using GPT-JT), we plot the performance improvement of Embroid against the average smoothness of the embeddings used. We observe a positive correlation (\(r=0.39\)). **Bottom right**: Across all tasks, we measure the performance improvement of Embroid against the size of the task.

Domain specific embeddings improve performanceWe compare how performance on the legal and science tasks changes when we shift from domain specialized embeddings to general domain embeddings. On law tasks for GPT-JT, we find that using two legal embedding spaces outperforms using BERT and RoBERTa for 77% of tasks, by up to 6 points F1 on certain tasks . For science tasks for GPT-JT, we find that using two science embedding spaces  outperforms using BERT and RoBERTa for 92% of tasks, by up to 4.3 points F1 on certain tasks.

Embedding qualityBuilding on Section 5, we compare Embroid's performance improvement over the base prompt to the average smoothness of the embedding spaces with respect to each task (Figure 2). We observe a positive correlation: smoother embedding spaces are associated with larger performance gains (with a Pearson coefficient of \(r=0.39\)). Applying this insight, we explore how performance changes when _extremely_ high quality embeddings are added. For a subset of 19 tasks we generate OpenAI text-embedding-ada-002 embeddings, and find that adding them to Embroid improves performance by up to 13 points F1 (at an average of 2 points across all studied tasks).

Dataset sizeFinally, we study how Embroid's performance improvement changes as the dataset size changes. Because Embroid relies on nearest-neighbors in different embedding spaces, we might expect performance to be poor when the dataset being annotated is small. In Figure 2 (bottom right), we see that Embroid achieves performance improvements even for "small" datasets with only several hundred samples.

## 7 Conclusion

We study the problem of improving prompt-based learning, by developing a method (Embroid) for detecting and correcting erroneous predictions without labeled data. We validate Embroid across a range of datasets and LMs, finding consistent improvement in many regimes. We take a moment to address the societal impact of our work: while we do not foresee any _direct_ harmful impacts arising from our work, we caution that any use of language models in meaningful applications should be accompanied by conversations regarding risks, benefits, stakeholder interests, and ethical safeguards.

## 8 Acknowledgements

We are grateful to Arjun Desai, Avanika Narayan, Benjamin Spector, Dilara Soylu, Gautam Machiraju, Karan Goel, Lucia Zheng, Rose E. Wang, Sabri Eyuboglu, Sarah Hooper, Simran Arora, Maya Varma, and other members of the Hazy Research group for helpful feedback and conversations on this project.

We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), 1937301 (RTML) and CCF2106707; US DEVCOM ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEP-TUNE); NXP, Xilinx, LETICEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), the Wisconsin Alumni Research Foundation (WARF), the Center for Research on Foundation Models (CRFM), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.

Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.

[MISSING_PAGE_FAIL:11]

*  Neel Guha, Daniel E Ho, Julian Nyarko, and Christopher Re. Legalbench: Prototyping a collaborative benchmark for legal reasoning. _arXiv preprint arXiv:2209.06120_, 2022.
*  Neel Guha, Julian Nyarko, Daniel E. Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N. Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M. Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models, 2023.
*  Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don't stop pretraining: adapt language models to domains and tasks. _arXiv preprint arXiv:2004.10964_, 2020.
*  Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. _arXiv preprint arXiv:1810.10147_, 2018.
*  Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. _arXiv preprint arXiv:2111.13654_, 2021.
*  Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. Annollm: Making large language models to be better crowdsourced annotators, 2023.
*  Peter Henderson, Mark S Krass, Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, and Daniel E Ho. Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset. _arXiv preprint arXiv:2207.00220_, 2022.
*  Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. CUAD: an expert-annotated NLP dataset for legal contract review. _CoRR_, abs/2103.06268, 2021.
*  Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023.
*  Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. _arXiv preprint arXiv:2210.11610_, 2022.
*  Qiao Jin, Bhuwan Dhingra, William W Cohen, and Xinghua Lu. Probing biomedical embeddings from language models. _arXiv preprint arXiv:1904.02181_, 2019.
*  Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Miaeutic prompting: Logically consistent reasoning with recursive explanations. _arXiv preprint arXiv:2205.11822_, 2022.
*  Martin Krallinger, Obudila Rabal, Saber Ahmad Akhondi, Martin Perez Perez, Jesus Santamaria, Gael Perez Rodriguez, Georgios Tsatsaronis, Ander Intxaurrondo, J. A. Lopez, Umesh K. Nandal, Erin M. van Buel, Ambika Chandrasekhar, Marleen Rodenburg, Astrid Legreid, Marius A. Doornenbal, Julen Oyarzabal, Analia Lourenco, and Alfonso Valencia. Overview of the biocreative vi chemical-protein interaction track. 2017.
*  Taja Kuzman, Igor Mozetic, and Nikola Ljubesic. Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification. _ArXiv, abs/2303.03953_, 2023.
*  Stanford Legal Design Lab. Legal issues taxonomy, 2023.
*  Suffolk Law School Legal Innovation & Technology Lab. Spot's training data, 2022.

*  Hunter Lang, Aravindan Vijayaraghavan, and David Sontag. Training subset selection for weak supervision, 2022.
*  Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240, 2020.
*  Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.
*  Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
*  Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.
*  Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. _White Paper. AI21 Labs_, 1, 2021.
*  Lucy H Lin and Noah A Smith. Situating sentence embedders with nearest neighbor overlap. _arXiv preprint arXiv:1909.10724_, 2019.
*  Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35, 2023.
*  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
*  Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In _Advances in Neural Information Processing Systems_, 2022.
*  Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. _arXiv preprint arXiv:2210.07229_, 2022.
*  Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. _arXiv preprint arXiv:2109.07830_, 2021.
*  Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In _International Conference on Machine Learning_, pages 15817-15831. PMLR, 2022.
*  Akihiro Nakamura and Tatsuya Harada. Revisiting fine-tuning for few-shot learning. _arXiv preprint arXiv:1910.00216_, 2019.
*  OpenAI. Gpt-4 technical report, 2023.
*  Laurel Orr. Manifest. https://github.com/HazyResearch/manifest, 2022.
*  Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
*  Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. _Advances in neural information processing systems_, 34:11054-11070, 2021.
*  Rattana Pukdee, Dylan Sam, Maria-Florina Balcan, and Pradeep Ravikumar. Label propagation with weak supervision. _arXiv preprint arXiv:2210.03594_, 2022.
*  A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Re. Training complex models with multi-task weak supervision. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Honolulu, Hawaii, 2019.

* Ratner et al.  Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. Snorkel: Rapid training data creation with weak supervision. In _Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases_, volume 11, page 269. NIH Public Access, 2017.
* Reimers and Gurevych  Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* Scao et al.  Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* Shin et al.  Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Carl Roberts, and Frederic Sala. Universalizing weak supervision. In _International Conference on Learning Representations (ICLR)_, 2022.
* Su et al.  Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better few-shot learners, 2022.
* Taori et al.  Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023.
*  Together. Releasing v1 of gpt-jt powered by open-source ai. https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai. Accessed: 2023-01-25.
* Varma et al.  Paroma Varma, Frederic Sala, Ann He, Alexander Ratner, and Christopher Re. Learning dependency structures for weak supervision models. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 6418-6427. PMLR, 09-15 Jun 2019.
* Vishwakarma and Sala  Harit Vishwakarma and Frederic Sala. Lifting weak supervision to structured prediction. _Advances in Neural Information Processing Systems_, 35:37563-37574, 2022.
* Wang et al.  Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* Wei et al.  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* Wolf et al.  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45, 2020.
* Wu et al.  Renzhi Wu, Shen-En Chen, Jieyu Zhang, and Xu Chu. Learning hyper label model for programmatic weak supervision. _arXiv preprint arXiv:2207.13545_, 2022.
* Zhang et al.  Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. Wrench: A comprehensive benchmark for weak supervision. _arXiv preprint arXiv:2109.11377_, 2021.
* Zhang et al.  Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* Zhang et al.  Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in neural information processing systems_, 28, 2015.

*  Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models, 2021.
*  Lucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings. In _Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law_, pages 159-168, 2021.

## Appendix A Notation

The glossary is given in Table 4 below.

   Symbol & Used for \\  \(x\) & Input sentence or paragraph \(x\). \\ \(y\) & Binary task label \(y=\{-1,+1\}\). \\ \(\) & Unlabeled dataset \(=\{x_{i}\}_{i=1}^{n_{u}}\) of \(n_{u}\) points. \\ \(,_{x}\) & The joint distribution of \((x,y)\) and the marginal on \(x\), respectively. \\ \(n_{l}\) & Number of labeled in-context examples used in querying the LLM (5-10 examples). \\ \(_{}(),()\) & Users interact with a language model \(_{}\) via a prompt \(\) on \(x\). \\ \(m\) & Number of prompts that we have access to. \\ \(\) & \((x)=[_{1}(x),,_{m}(x)]\) where \(_{i}(x)\) is shorthand for \(_{}(_{i}(x))\). \\ \(\) & The set of \(N\) embedding models, \(=\{E_{1},,E_{N}\}\) where each embedding is represented as a fixed mapping \(E_{i}:_{i}\). \\ \(Z\) & Partition function for normalization of (1). \\ \(_{y},\) & \(_{y}\) is a label balance parameter and each \(_{i}\) is a scalar accuracy parameter for the \(i\)th source in (1). \\ \(_{j,k}(x)\) & The \(k\)-nearest neighbors of \(x\) in embedding space \(E_{j}\). \\ \(_{}\) & \(_{}(x)=[_{,1},,_ {,N}]\{-1,0,1\}^{Nm}\), where \(_{,j}=[_{,j},_{ ,j}[m]]\) and \(_{,j}[i](x)\) is the smoothed neighborhood prediction of \(_{i}(x)\) in \(E_{j}\) (eq. (2)). \\ \(_{i}^{+},_{i}^{-}\) & Shrinkage parameters for determining when \(_{,j}[i](x)\) is set to \(0\), \(-1\), or \(1\). \\ \(c_{j}\) & Vector of \(m\) accuracy parameters for \(E_{j}\) when used with \(m\) prompts in (3). \\ \(L(,_{,m},)\) & Generalization error of Embroid ( expected cross-entropy loss). \\ \(a_{}\) & The largest scaled accuracy of any source, \(a_{}=_{i}[_{i}(x)y]\). \\ \(b_{}\) & The smallest expected pairwise product between any two sources, \(b_{}=_{i,j}\{[_{i}_{j}], }[_{i}_{j}]\}\). \\ \(M_{E}()\) & An embedding is \(M\)-smooth if \((=c|y=c,\|E(x)-E()\|) M_{E}( )\) for all \(c\) and any \(>0\), \\  & where \(M_{E}()\) is decreasing in its input. \\ \(_{i}\) & The accuracy of \(_{i}\), \(_{i}=(_{i}=y)\). \\ \(p_{}\) & The prediction on \(x_{0}\) given only access to \(\), \(p_{}=(y=1|(x_{0}))\). \\ \(_{k}\) & The maximum distance between \(x_{0}\) and its \(k\) neighbors, \(_{k}=_{_{k}(x)}\|E(x)-E()\|\). \\   

Table 4: Glossary of variables and symbols used in this paper.

Weak supervision background

In this section, we provide details on the inference and learning procedures for solving the graphical model defined in equation (1). The content from this section is derived from  and .

**Pseudolabel inference.** To perform inference, we compute \((y|(x))\) for some \(x\). This is done via Bayes' rule and the conditional independence of weak sources:

\[(y=1|(x))=^{m}(_{i}(x)|y=1)(y=1)}{ ((x))}.\] (5)

We assume that the class balance is known; for our datasets, the class balance is \((y=1)=0.5\). More generally, it can be estimated . The latent parameter of interest in this decomposition is \((_{i}=1|y=1)\), which corresponds to the accuracy of \(_{i}\).

``` Input: Dataset \(\), weak sources \((x)\). for\(i[m]\)do for\(j,k[m] i\)do  Estimate \(}[_{i}_{j}]\) over \(\), and similarly estimate \(}[_{i}_{k}]\) and \(}[_{j}_{k}]\).  Compute \(_{i}^{j,k}=}[_{i}_ {j}]}[_{i}_{k}]}{ }[_{j}_{k}]}|}\). endfor  Calculate average \(_{i}=(_{i}^{j,k}\ \  j,k[m] i)\).  Compute estimated accuracy \((_{i}=y)=_{i}}{2}\). endfor Output: Accuracies \((_{i}=y)\) for all \(i[m]\). ```

**Algorithm 2** Triplet method 

**Source parameter estimation: Triplet method.** Previous approaches have considered how to estimate \((_{i}=1|y=1)\) via the _triplet method_, which exploits conditional independence properties. First, by the properties of the graphical model in (1), it holds that the accuracy of \(_{i}\) is symmetric: \((_{i}=1|y=1)=(_{i}=-1|y=-1)=(_{i}=y)\) (Lemma 4 of ). Therefore, \((_{i}=1|y=1)\) can be written in terms of \([_{i}y]\) with \([_{i}y]=2(_{i}=1|y=1)-1\).

Define \(a_{i}=[_{i}y]\). The graphical model in (1) tells us that \(_{i}y\!\!\!_{j}y\) if \(_{i}\!\!\!_{j}|y\), which holds for all \(i,j[m]\) (Proposition \(1\) of ). As a result, \([_{i}y][_{j}y] =[_{i}_{j}y^{2}]=[_{ i}_{j}]\), which is a quantity that can be computed from observed LLM predictions. That is, we have that \(a_{i}a_{j}=[_{i}_{j}]\). If we introduce a third \(_{k}\), we can generate a system of equations over \(a_{i},a_{j},a_{k}\) in terms of their pairwise rates of agreements:

\[a_{i}a_{j} =[_{i}_{j}]\] (6) \[a_{i}a_{k} =[_{i}_{k}]\] (7) \[a_{j}a_{k} =[_{j}_{k}].\] (8)

Solving, we get that

\[|a_{i}|:=[_{i}_{j}] [_{i}_{k}]}{[_{j} _{k}]}|},\] (9)

and likewise for \(a_{j},a_{k}\). If we assume that each weak source is better than random over the dataset, then \(a_{i}=|a_{i}|>0\), so we can uniquely recover the accuracy of each source by selecting two other sources and computing the above expression by using empirical expectations over \(\). We then set \((_{i}=1|y=1)=_{i}}{2}\) and plug this into the expression for \((y=1|(x))\) in (5).

This approach is formally described in Algorithm 2.

Proofs

### Proof of proposition 5.1

We note that \([,_{}]\) can be viewed as a set of sources in the weak supervision set up used in [7; 14]. Therefore, we can apply Theorem 1 from  to our problem setting, noting that we do not perform their clustering step and that our predictions do not abstain and output \(0\) in addition to \(\{-1,1\}\). We have a total of \((N+1)m\) sources, so

\[L(,_{},) H(y|, {}_{})+^{2})}{8b_{}^{2}(1-a_{}^{2})} ^{4}}+^{2}}}+o(1/n_{u}).\] (10)

### Proof of theorem 5.2

We can write the change in point-wise irreducible error as follows:

\[H(y|(x_{0}))-H(y|(x_{0}),_{ }(x_{0})) =[-(y|(x_{0}))+(y|(x_{0}),_{}(x_{0}))]\] (11) \[=[(x_{0}),_{}(x_{0}))}{(y|(x_{0}))}]\] (12) \[=[(x_{0}),_{}(x_{0})|y)(y)}{((x_{0}),_{ }(x_{0}))}(x_{0}))}{((x_{0} )|y)(y)}]\] (13) \[=[_{}(x_{0})| (x_{0}),y)}{(_{}(x_{0})|(x_{ 0}))}].\] (14)

Next, we use the fact that \((x_{0})_{}(x_{0})|y\) to simplify the expression into

\[[_{}(x_{0})|y)}{(_{}(x_{0})|y=1)(y=1|(x_{0}))+( _{}(x_{0})|y=-1)(y=-1|(x_{0}))}].\] (15)

The exact \(_{}(x_{0})\) is unknown but is drawn from the distribution \((_{}|y=1)\) since \(x_{0}\)'s label is \(1\). Then, this expression becomes an expectation over \(_{}\):

\[_{_{}|y=1}[_{ }|y=1)}{(_{}|y=1)p_{}+(_{}|y=-1)(1-p_{})}].\] (16)

Given that our \(_{}\) is high-quality, we suppose that \(_{}(x_{0})\) all equal \(1\) with high probability, and then we can lower bound our expression by

\[(_{}=1|y=1)_{}=1| y=1)}{(_{}=1|y=1)p_{}+(_{ }=1|y=-1)(1-p_{})}.\] (17)

The key quantity of interest is \((_{}=1|y=1)=_{j=1}^{m}(_{},[i ]=1|y=1)\). We focus on bounding \((_{},[i]=1|y=1)\) next. Suppose that the \(k\) neighbors of \(x_{0}\) are \(x_{1},,x_{k}\). Define \(p_{j}=(_{i}(x_{j})=1|y=1)\) for all \(j[k]\). Note that \(_{i}(x_{j})_{i}(x_{j^{}})|y\) for any \(j,j^{}[k]\) (while \(_{},[i]\) as a whole is dependent on \(y\), individual neighbors are still conditionally independent). Then, the event that \(_{},[i]=1|y=1\) is as least as likely as the event that Binomial\((k,_{i[k]}p_{i})\). Let \(p_{}=_{i[k]}p_{i}\), and assume that \(p_{}\). Then,

\[(_{},[i]=1|y=1) (k,p_{})= _{j=1}^{k}X_{j}\] (18) \[=_{j=1}^{k}X_{j} p_{}-p_{ }-,\] (19)where \(X_{j}(p_{})\). Next, let \(=p_{}-\). We can apply Hoeffding's inequality to get

\[(k,p_{}) =_{j=1}^{k}X_{j} p_{}- =1-_{j=1}^{k}X_{j} p_{}- 1- (-2^{2}k)\] (20) \[=1-(-2k(p_{}-0.5)^{2}).\] (21)

All that's left is to lower bound \(p_{}\). Without loss of generality, suppose that \(p_{}\) corresponds to an arbitrary \(p_{j}=(_{i}(x_{j})=1|y=1)\). We can decompose this probability into

\[(_{i}(x_{j}) =1|y=1)=(_{i}(x_{j})=1,y(x_{j})=1|y=1)+(_{i} (x_{j})=1,y(x_{j})=-1|y=1)\] (22) \[=(_{i}(x_{j})=1|y(x_{j})=1)(y(x_{j})=1|y=1)+( _{i}(x_{j})=1|y(x_{j})=-1)(y(x_{j})=-1|y=1).\] (23)

Since \((_{i}(x_{j})=1|y(x_{j})=1)\) is over all \(x_{j}_{x}\), this quantity is just equal to the accuracy of \(_{i}\), \(a_{i}\). Next, recall that \(\|E(x_{j})-E(x)\|_{k}\), where \(_{k}=_{x_{j}(x)}\|E(x)-E(x_{i})\|\) is the maximum distance from the \(k\) neighbors to \(x\). Then, we can write \((y(x_{j})=1|y=1)\) as \((y(x_{j})=1|y=1,\|E(x_{j})-E(x)\|_{k}) M_{E}( _{k})\), since we have assumed that \(E\) is \(M\)-smooth. We can now bound \(p_{}\):

\[p_{} a_{i}M_{E}(_{k})+(1-a_{i})(1-M_{E}(_{k})).\] (24)

Therefore, we have that

\[(_{}=1|y=1)_{i=1}^{m}1-[- 2k(a_{i}M_{E}(_{k})-0.5)^{2}].\] (25)

Before we plug in \((_{}=1|y=1)\) into (17), we simplify the expression. Note that \((_{}=1|y=1)\) can be written as \(_{i=1}^{m}p_{i}\) for some \(p_{i}\), and \((_{}=1|y=-1)\) can be written as \(_{i=1}^{m}(1-p_{i})\). A simple proof by induction shows that \(_{i=1}^{m}(1-p_{i}) 1-_{i=1}^{m}p_{i}\). Therefore, we can write that (17) is lower bounded by

\[(_{}=1|y=1)_{}=1|y=1)}{(_{}=1|y=1)p_{ }+(1-(_{}=1|y=1))(1-p_{ })}\] (26)

Let's abbreviate \((_{}=1|y=1)\) as \(x\) and define the function

\[f(x)=x}+(1-x)(1-p_{ })}.\] (27)

We note that for \(x 0.5\), \(f(x)\) is convex and can thus be lower bounded by \(f(x) f^{}(0.5)(x-0.5)\). We compute \(f^{}(x)=}}{xp_{}+(1 -x)(1-p_{})}\), so \(f^{}(0.5)=2(1-p_{})\). Therefore, \(f(x) 2(1-p_{})(x-0.5)\). Our final bound on the pointwise difference in irreducible error on \(x_{0}\) is

\[H(y|(x_{0}))-H(y|(x_{0}),_{}(x_{0})) 2(1-p_{})_{i=1}^{m} 1-[-2k(a_{i}M_{E}(_{k})-0.5)^{2}]-0.5.\] (28)

Information gain from using \(,_{}\) over \(_{}\)We briefly comment on the opposite direction--how much does using both LLM predictions and neighborhood predictions help over just using neighborhood predictions?

The quantity we aim to lower bound is \(H(y|_{}(x_{0}))-H(y|(x_{0}), _{}(x_{0}))\) for a point of interest \(x_{0}\). We can write this quantity as

\[H(y|_{}(x_{0}))-H(y|(x_{0}), _{}(x_{0}))=[(x_{0})|y)}{((x_{0})|_{}(x_{0}))}]\] (29)Without loss of generality, suppose that the true label on \(x_{0}\) is \(y=1\), and that for each \(_{i}\), the neighborhood around \(x_{0}\) consists of a balanced mix of \(_{i}=1\) and \(_{i}=-1\). Then, with high probability we have that \((y=1|_{}(x_{0}))=p_{_{ }} 0.5\). From our proof of Theorem 5.2, we can thus write

\[H(y|_{}(x_{0}))-H(y|(x_{0}), _{}(x_{0}))=_{(x_{0})}[ (x_{0})|y=1)}{((x_{0})|y=1)p_{ _{}}+((x_{0})|y=-1)(1-p_{_{}})}]\] (30)

\[((x_{0})=1|y=1)) (x_{0})=1|y=1)}{((x_{0})=1|y=1)p_{ _{}}+((x_{0})=1|y=-1)(1-p_{_{}})}\] (31)

\[ 2(1-p_{_{}})(((x_{0} )=1|y=1)-0.5)\] (32)

If \(\) on \(x_{0}\) has high accuracy and \(p_{_{}}\) is low, then we can have significant point-wise information gain from modeling both \(\) and \(_{}\) rather than just \(_{}\).

Datasets

**Motivation**. We study the performance of our method across a diverse collection of _task definitions_. In our setting, a task definition denotes a specific classification that a data scientist wishes to perform. For instance, a data scientist working on quantifying the breadth of legal issues that individuals face may wish to identify which posts in an online forum refer implicate legal issues related to housing.

This evaluation strategy is motivated by the observation that task definitions vary in their smoothness across embedding spaces, as different embeddings may do a better job of capturing features relevant for the task. For instance, out-of-the-box Sentence-BERT embeddings are better than traditional BERT at capturing the topicality of a sentence . By focusing on a broad range of task definitions, we can better forecast how our method might perform for new tasks that practitioners may need to create classifiers for. We also avoid issues with leakage that may arise as the practice of finetuning LLMs on tasks increases .

In total, we study 95 distinct task definitions, encompassing 100,418 total samples. Each task varies between 108 and 3308 samples.

**Legal tasks**. The emergence of LLMs is exciting for law and finance, where expert-annotations are especially difficult to acquire . Drawing on recent benchmarks and released datasets framing the potential use cases for LLMs in law, we study the following datasets:

* CUAD : The original CUAD dataset consists of 500 contracts spanning an array of sectors, with clauses manually into one of 41 legal categories. Following , we adapt the original dataset for clause-by-clause classification. We turn each clause type into a binary classification task, where the objective is to distinguish clauses of that type from clauses of other types (i.e. "negatives"). Negative clauses are sampled randomly so as to make the task class balanced. We ignore clauses for which there are insufficient annotations in the original dataset.
* Learned Hands : The Learned Hands dataset consists of legal questions that individuals publicly posted to an online forum (r/legaladvice). The questions have been coded by experts into legal categories according to the Legal Issues Taxonomy . We consider several such issue classes, and create a binary classification task for each issue. Negative clauses are sampled randomly so as to make the task class balanced. Because these questions can be long, we truncate them at 50 tokens.

**Science tasks**. LLMs have generated similar excitement for medical and science informatics applications . We study established classification/extraction benchmarks.

* Chemprot : ChemProt consists of sentences from PubMed abstracts describing chemical-protein relationships. We study seven relations, and create a binary task for each one. Each task is class balanced, with negatives sampled from the other relations.
* RCT : The RCT dataset consists of PubMed abstracts for papers discussing randomized control trials, where sentences in the abstract are annotated according to their semantic role (e.g., background, methods, results, etc). There are five roles, and we create a binary task for each one. Each task is class balanced, with negatives sampled from the other relations.

**General domain tasks**. Finally, we study the following "general domain" tasks, derived from popular sentence classification and information extraction benchmarks.

* FewRel : This is a relationship classification/extraction dataset, where each sample corresponds to a sentence mentioning the relationship between two entities. We select 20 relations, and for each relation construct a binary classification task with 700 positive instances of the relation, and 700 randomly sampled sentences (corresponding to other relations).
* Spam Detection : We study the YouTube spam detection task from the WRENCH benchmark. This task requires classifying YouTube comments as spam/not spam.
* Toxic content detection : This task requires classifying whether posted comments are toxic or not. We use a sampled subset of the CivilComments dataset.

* AG News : The original dataset organizes news snippets into four categories: World, Sports, Business, and Science/Technology. We create a separate task for each category. Negatives are sampled from the remaining classes.
* DBPedia : DBPedia is a 14-way ontology classification dataset. We convert this into 14 distinct tasks, corresponding to each of the ontology types.

 p{113.8pt} p{113.8pt}}
**Task** & **Description/Intent** & **Size** \\  Affiliate License-Licenseee (CUAD) & Does the clause describe a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor? & 208 \\  Anti-Assignment (CUAD) & Does the clause require consent or notice of a party if the contract is assigned to a third party? & 1212 \\  Audit Rights (CUAD) & Does the clause discuss potential audits? & 1224 \\  Cap On Liability (CUAD) & Does the clause specify a cap on liability upon the breach of a party’s obligation? & 1262 \\  Change Of Control (CUAD) & Does the clause give one party the right to terminate if such party undergoes a change of control? & 426 \\  Competitive Restriction Exception (CUAD) & Does the clause mention exceptions or carcouts to Non-Compete, Exclusivity and No-Solicit of Customers? & 226 \\  Covenant Not To Sue (CUAD) & Does the clause mention if a party is restricted from contesting the validity of the counterparty’s ownership of intellectual property? & 318 \\  Effective Date (CUAD) & Does the clause mention when the contract becomes effective? & 246 \\  Exclusivity (CUAD) & Does the clause mention an exclusive dealing commitment with the counterparty? & 770 \\  Expiration Date (CUAD) & Does the clause mentions a date when the contract’s term expires? & 892 \\  Governing Law (CUAD) & Does the clause mentions which state/country’s laws govern interpretation of the contract? & 910 \\  Insurance (CUAD) & Does the clause mention a requirement for insurance? & 1040 \\  Ip Ownership Assignment (CUAD) & Does the clause mention if intellectual property created by one party become the property of the counterparty? & 590 \\  Irrevocable Or Perpetual License (CUAD) & Does the clause describe a license grant that is irrevocable or perpetual? & 300 \\  Joint Ip Ownership (CUAD) & Does the clause provide for joint or shared ownership of intellectual property between the parties to the contract? & 198 \\  License Grant (CUAD) & Does the clause describe a license granted by one party to its counterparty? & 1430 \\  Liquidated Damages (CUAD) & Does the clause award either party liquidated damages for breach or a fee upon the termination of a contract (termination fee)? & 226 \\  Minimum Commitment (CUAD) & Does the clause specifies a minimum order size or minimum amount or units pertime period that one party must buy from the counterparty? & 778 \\  

Table 5: Legal tasks

 p{113.8pt} p{113.8pt}}
**Task** & **Description/Intent** & **Size** \\  No-Solicit Of Employees (CUAD) & Does the clause restricts a party’s soliciting or hiring employees and/or contractors from the counterparty, whether during the contract or after the contract ends (or both). & 150 \\  Non-Compete (CUAD) & Does the clause restrict the ability of a party to compete with the counterparty or operate in a certain geography or business or technology sector? & 450 \\  Non-Disparagement (CUAD) & Does the clause require a party not to disparage the counterparty? & 108 \\  Non-Transferable License (CUAD) & Does the clause limit the ability of a party to transfer the license being granted to a third party? & 558 \\  Notice Period To Terminate Renewal (CUAD) & Does the clause requires a notice period to terminate renewal? & 234 \\  Post-Termination Services (CUAD) & Does the clause subject a party to obligations after the termination or expiration of a contract, including any post-termination transition, payment, transfer of IP, wind-down, last-buy, or similar commitments? & 816 \\  Renewal Term (CUAD) & Does the clause mention a renewal term for after the initial term expires? & 398 \\  Revenue-Profit Sharing (CUAD) & Does the clause require a party to share revenue or profit with the counterparty for any technology, goods, or services? & 784 \\  Roff-Rofo-Roftn (CUAD) & Does the clause provide a party with a right of first refusal? & 698 \\  Source Code Escrow (CUAD) & Does the clause requires one party to deposit its source code into escrow with a third party, which can be released to the counterparty upon the occurrence of certain events (bankruptcy, insolvency, etc.)? & 126 \\  Termination For Convenience (CUAD) & Does the clause state that one party can terminate this contract without cause (solely by giving a notice and allowing a waiting period to expire)? & 302 \\  Uncapped Liability (CUAD) & Does the clause state that a party’s liability is uncapped upon the breach of its obligation in the contract? & 328 \\  Volume Restriction (CUAD) & Does the clause describe a fee increase or consent requirement if one party’s use of the product/services exceeds certain threshold? & 328 \\  Warranty Duration (CUAD) & Does the clause mentions the duration of any warranty against defects or errors in technology, products, or services provided under the contract? & 326 \\  BU (Learned Hands) & Does the text discuss issues relating to business or intellectual property? & 200 \\  CO (Learned Hands) & Does the text discuss issues relating to courts and lawyers? & 194 \\  CR (Learned Hands) & Does the text discuss issues relating to criminal issues? & 644 \\  ES (Learned Hands) & Does the text discuss issues relating to estates or wills? & 182 \\  FA (Learned Hands) & Does the text discuss issues relating to family or divorce? & 794 \\  

Table 5: continued from previous page

**Task** & **Description/Intent** & **Size** \\  Agonist (Chemprot) & Does the sentence describe an agonist relationship? & 896 \\  Antagonist (Chemprot) & Does the sentence describe an antagonist relationship? & 1330 \\  Downregulator (Chemprot) & Does the sentence describe a downregulator relationship? & 3038 \\  Part\_of (Chemprot) & Does the sentence describe a part-of relationship? & 1210 \\  Regulator (Chemprot) & Does the sentence describe a regulator relationship? & 2876 \\  Substrate (Chemprot) & Does the sentence describe a substrate relationship? & 2384 \\  Upregulator (Chemprot) & Does the sentence describe an upregulator relationship? & 2404 \\  Background (RCT) & Does the sentence describe background on the study? & 2000 \\  Conclusions (RCT) & Does the sentence state a conclusion? & 2000 \\  Methods (RCT) & Does the sentence describe a scientific experimental method? & 2000 \\  Objective (RCT) & Does the sentence describe the goal of the study? & 2000 \\  Results (RCT) & Does the sentence describe experimental results? & 2000 \\  

Table 6: Science tasks

**Task** & **Description/Intent** & **Size** \\  Business (AGNews) & Does the article discuss business news? & 2000 \\  Sports (AGNews) & Does the article discuss sports news? & 2000 \\  Technology (AGNews) & Does the article discuss technology news? & 2000 \\  World (AGNews) & Does the article discuss global affairs or world events? & 2000 \\  Civil Comments & Does the sentence contain toxic or hateful content? & 2500 \\  Album (DBPedia) & Is the entity discussed in the sentence an example of a album? & 2000 \\  Animal (DBPedia) & Is the entity discussed in the sentence an example of a animal? & 2000 \\  Artist (DBPedia) & Is the entity discussed in the sentence an example of a artist? & 2000 \\  Athlete (DBPedia) & Is the entity discussed in the sentence an example of a athlete? & 2000 \\  Building (DBPedia) & Is the entity discussed in the sentence an example of a building? & 2000 \\  Company (DBPedia) & Is the entity discussed in the sentence an example of a company? & 2000 \\  Educational institution (DBPedia) & Does the sentence discuss a school, university, or college? & 2000 \\  

Table 7: General domain tasks

**Task** & **Description/Intent** & **Size** \\  HE (Learned Hands) & Does the text discuss issues relating to health? & 248 \\  HO (Learned Hands) & Does the text discuss issues relating to housing? & 1270 \\  MO (Learned Hands) & Does the text discuss issues relating to payments or debt? & 740 \\  TO (Learned Hands) & Does the text discuss issues relating to accidents or harassment? & 454 \\  TR (Learned Hands) & Does the text discuss issues relating to cars or traffic? & 516 \\  WO (Learned Hands) & Does the text discuss issues relating to employment or job? & 726 \\  

Table 5: continued from previous page

**Task** & **Description/Intent** & **Size** \\   Film (DBPedia) & Is the entity discussed in the sentence an example of a film? & 2000 \\  Mean of transportation (DBPedia) & Does the sentence discuss a car, ship, train, or plane? & 2000 \\  Natural place (DBPedia) & Is the entity discussed in the sentence an example of a natural & 2000 \\  & landscape or environment? & \\  Office holder (DBPedia) & Is the entity discussed in the sentence an example of a office holder? & 2000 \\  Plant (DBPedia) & Is the entity discussed in the sentence an example of a plant? & 2000 \\  Village (DBPedia) & Is the entity discussed in the sentence an example of a village? & 2000 \\  Written work (DBPedia) & Is the entity discussed in the sentence a writing? & 2000 \\  Architect (FewRel) & Does the text mention an architect? & 600 \\  Composer (FewRel) & Does the text mention a musical composer? & 600 \\  Country (FewRel) & Does the text mention a country? & 600 \\  Developer (FewRel) & Does the text mention development? & 600 \\  Director (FewRel) & Does the text mention a film director? & 600 \\  Distributor (FewRel) & Does the text mention a film distributor? & 600 \\  Father (FewRel) & Does the text mention a father? & 600 \\  Genre (FewRel) & Does the text mention the genre of a song or artist? & 600 \\  Instrument (FewRel) & Does the text mention an instrument? & 600 \\  League (FewRel) & Does the text mention a sports competition, league, or division? & 600 \\  Military Branch (FewRel) & Does the text mention a military branch? & 600 \\  Movement (FewRel) & Does the text mention an art movement? & 600 \\  Occupation (FewRel) & Does the text mention a professional occupation? & 600 \\  Participating Team (FewRel) & Does the text mention a sports team? & 600 \\  Platform (FewRel) & Does the text mention an online platform? & 600 \\  Sibling (FewRel) & Does the text mention a sibling? & 600 \\  Successful Candidate (FewRel) & Does the text mention an election winner? & 600 \\  Taxonomy Rank (FewRel) & Does the text mention a taxonomy class of animals? & 600 \\  Tributary (FewRel) & Does the text mention a tributary? & 600 \\  Winner (FewRel) & Does the text mention a competition winner? & 600 \\  YouTube Spam Detection & Does the comment ask the user to check out another video? & 1836 \\   

Table 7: continued from previous pagePrompts

We construct a prompt for each task by randomly selecting three examples of each class to use as in-context demonstrations . We manually defined "instructions" for each task. An example of an abridged prompt is shown in Figure 3.

Figure 3: An example of the instruction classification prompt for the dbpedia_animal task, with two in-context demonstrations. Here, the task instructions are in red, the in-context demonstrations are in blue, and the sample for which we want a label is in green.

Synthetics

We conduct synthetic experiments which provide additional insights on Embroid.

For our setup, we create two equal clusters of data of \(500\) points each, \(C_{1}\) and \(C_{2}\), in \(^{2}\). We assign labels to the points in each cluster i.i.d. according to a probability \(p\), where \((y=1|x C_{1})=p\) and \((y=1|x C_{2})=1-p\). When \(p=0.5\), both the clusters have a uniform label distribution (non-smooth) while \(p=1\) ensures each cluster has one class (smooth). We set \(k=20\), \(_{+}=P(_{i}=1)\) and \(_{-}=P(_{i}=-1)\).

**Improvement over weak supervision**. We show that Embroid offers improvement over methods that only use \(\). We fix \(p=0.8\) and \(_{i}=(_{i}=y)=0.6\) for each \(i[m]\). We compare Embroid against the standard weak supervision approach from , which requires \(m 3\), in Figure 3(a).

**Smoothness**. Embroid's performance depends on the smoothness of the embedding as defined in eq.4. We consider one LM prediction \(m=1\) and vary the smoothness \(p\) from \(0.5\) to \(1.0\) and generate predictions using \(_{i}=0.6\). Figure 3(b) exhibits that Embroid's accuracy is positively correlated with the embedding smoothness.

**Base prediction accuracy**. Finally, we show that Embroid's performance depends on the base prediction accuracy, \(_{i}\). We consider \(m=1\) and set \(p=0\), \(_{i}=0.8\). We use a parameter \(\) to denote the probability that \(_{i}\) is incorrect on points in cluster \(C_{1}\). As \(\) varies from \(0\) to \(1\), the predictions of \(_{i}\) become biased towards \(1\) and effectively reduces \(_{i}\) to \(0.5\). In Figure 3(c), we observe that as the base prediction accuracy decreases, Embroid's performance decreases and eventually goes below the base LM performance.

## Appendix G Ablation

We perform additional ablations of Embroid. We focus on two aspects:

* The role of \(^{-}\)/\(^{+}\).
* The impact of weak supervision in combining \(_{,j}\).

### Ablation over \(\)

In our experiments, we set \(_{i}^{+}=_{i}^{-}=[_{i}]\), or the average vote of source \(_{i}\). This has the following effect on the neighborhood vote \(_{,j}[i]\) for source \(_{i}\) under \(E_{j}\):

* When the average vote for a source \(_{i}\) in a neighborhood under \(E_{j}\) for \(x\) is more negative than the average overall vote for a source, then \(_{,j}[i](x)=-1\).

Figure 4: Synthetic experiments. (a) Comparison of Embroid to weak supervision by varying number of LLM sources. Increasing sources consistently improves both Embroid and WS and the gains remain constant. (b) Embroid’s performance as embedding smoothness (eq.4) varies. Embroid’s accuracy linearly improves as a function of embedding smoothness. (c) Embroid’s performance with varying probability of LLM being incorrect in \(C_{1}\). As the LM becomes incorrect, the cluster becomes less homogeneous and this degrades Embroid’s performance.

* When the average vote for a source \(_{i}\) in a neighborhood under \(E_{j}\) for \(x\) is more positive than the average overall vote for a source, then \(_{,j}[i](x)=1\).

In general, we find that this setting provides good performance, while requiring no additional tuning or validation. For example, the Figure 5 below compares a setting of \(_{i}^{+}\)/\(_{i}^{-}\) against the F1 score for GPT-JT on ag_news_business.

### Role of weak supervision aggregation

We quantify the extent to which performance gains are derived from (1) the computation of \(_{}\), as opposed to (2) the use weak-supervision  to combine \(_{}\) and \(\). Specifically, we replace Equation 3 with a simple majority vote classifier which combines the original prediction with the computed neighborhood votes.

### Choice of embeddings

We provide additional discussion on how practitioners may go about choosing embeddings for Embroid. At the outset, we note that selecting/evaluating embeddings is particularly difficult in our regime, where practitioners do not have access to labeled data. Importantly, this motivates Embroid's "mixture of embeddings" approach. Previous methods (e.g., Liger ) operate on only a single embedding space. Thus, practitioners must be certain that the embedding space chosen is good for a task, and a poorly chosen embedding space will reduce performance. Because Embroid ensembles different embedding spaces, practitioners can be less certain in the quality of any individual embedding space.

A natural strategy may be to start with an initial large set of candidate embeddings, and drop embeddings according to task-specific measurements (akin to feature selection). Several criteria may be used to determine which embeddings to maintain:

 LM & Base prompt & Majority vote aggregation & Embroid \\  j1-jumbo & 0.498 & 0.569 & 0.604 \\ openai\_text-davinci-003 & 0.806 & 0.844 & 0.855 \\ bloom-7b1 & 54.7 & 61.2 & 64.7 \\ opt-6.7b & 48.2 & 56.1 & 59.8 \\ GPT-JT-6B-v1 & 67.8 & 73.2 & 75.1 \\  

Table 8: We evaluate how Embroid compares to a majority vote aggregation over neighborhood vote vectors. We report macro-F1 average across all tasks and prompts, mirroring results in Table 1.

* Looking at the MLM loss of the embedding model (e.g. BERT) on the task data. Prior literature on domain specificity has suggested that this may be a promising heuristic .
* Looking to the performance of embedding models on other related tasks may also be helpful .
* Looking at the extent to which a potential embedding space generates embeddings which are geometrically similar to embeddings already included in Embroid. If the embedding space is similar, then the additional value of incorporation is low.

The number of embeddings used presents a bias-variance tradeoff. Under Proposition 5.1, increasing the number of embedding spaces (1) increases the variance due to estimation error, but lowers (2) the conditional entropy of \(H(y|)\). Precisely characterizing this tradeoff is challenging because the variance is an upper bound, and \(H(y|)\) can only be estimated. However, these bounds/estimates can be used to derive heuristic stopping rules: for instance, add embedding spaces until the marginal decrease in the conditional entropy is less than the upper bound on the marginal increase in variance.

Experiments

### Implementation details

ComputeInference for API-access models (e.g., GPT-3.5 and J1-Jumbo) were run using the HELM API . Inference for open source models (OPT, GPT-JT, and Bloom) were run using the Manifest library  on 40GB A100 NVIDIA GPU machines.

HyperparametersEmbroid was run with \(k=10\), \(_{i}^{+}=P(_{i}=1)\), and \(_{i}^{-}=P(_{i}=-1)\).

### Space demand

We provide additional information on the space requirements for Embroid. First, we clarify that Embroid requires only one large model. This model can be on the order of GPT-3 (176B parameters), or a much smaller open-source equivalent like GPT-JT (7B parameters). Next, Embroid requires embeddings from 2-3 additional models. However, as our results show, these models can be relatively small. For instance, we rely on BERT-base and SentenceBERT models-both of which are approximately 110 million parameters. Finally, the latent variable graphical model has 2 parameters for class prior and 1+N parameters per LLM prediction, where N is the number or embedding spaces. So, when Embroid is run on a single LLM's predictions with two embedding spaces, the total number of parameters is 2 + (1+2)*1 = 5.

### Runtime

In terms of time complexity, we observe that computing LLM predictions is the largest bottleneck. On a dataset of 1800 samples for instance:

* Computing predictions on GPT-3.5 takes 1440 seconds (24 minutes).
* Computing embeddings using BERT takes 5 seconds.
* Computing nearest-neighbors using the (unoptimized) scikit-learn library takes 3 seconds.
* Solving the Embroid graphical model takes less than a second.

API-model tasksDue to cost constraints, we study the API access models (GPT-3.5 and J1-Jumbo) on a subset of tasks. These are:

* ag_news_world
* ag_news_sports
* dbpedia_educational institution
* dbpedia_athletechemprot_regulator
* chemprot_upregulator
* rct_objective
* rct_methods
* CUAD_Audit Rights
* CUAD_Non-Compete
* learned_hands_HE
* learned_hands_HO
* few_rel_architect
* few_rel_league

### Robustness across models

We provide the results for each LM on each task as CSV files in the supplemental attachment. We visualize the improvements in Table 1 below, by plotting the original prompt performance on the x-axis, and the performance after Embroid on the y-axis (Figure 6).

### Comparison to AMA

We visualize the improvements in Table 2 below, by plotting the performance of AMA on the x-axis, and the performance of Embroid-3 on the y-axis (Figure 7).

### Chain-of-thought experiments

We compare Embroid to chain-of-thought (CoT)  on the following tasks:

* ag_news_business
* ag_news_sports
* CUAD_Affiliate License-Licenseee
* CUAD_Audit Rights
* dbpedia_album
* dbpedia_building
* few_rel_architect
* few_rel_country
* learned_hands_HO
* learned_hands_MO

Figure 6: We visualize the improvement from Embroid over the base prompt for each model’s tasks. All models except for gpt-neo-1.3B were run thrice per task, with each run using different in-context samples for the prompt. Each dot corresponds to a task. The x-axis measures the average macro F1 of the base prompt, and the y-axis measures the average macro F1 of Embroid (across all runs). Because GPT-3.5 and J1-Jumbo are studied on a subset of 12 tasks, there are fewer dots in the plot.

We manually write logical chains for each prompt. Because CoT primarily succeeds on "large" models , we focus our experiments on GPT-3.5.

### Comparison to GPT-4

GPT-4  was made publicly available after this work was completed. Nonetheless, we offer preliminary results evaluating Embroid on predictions produced by GPT-4. We evaluated EmbroidEmbroid+GPT-4 on a subset of 14 tasks. These are:

* ag_news_world
* ag_news_sports
* dbpedia_educational institution
* dbpedia_athletechemprot_regulator
* chemprot_upregulator
* rct_objective
* rct_methods
* CUAD_Audit Rights
* CUAD_Non-Compete
* learned_hands_HE
* learned_hands_HO

Figure 7: We visualize the improvement from Embroid over AMA  for each model’s tasks. Each dot corresponds to a task. The x-axis measures the average macro F1 of AMA, and the y-axis measures the average macro F1 of Embroid. Because GPT-3.5 and J1-Jumbo are studied on a subset of 12 tasks, there are fewer dots in the plot.

* few_rel_architect
* few_rel_league

We note that overall, GPT-4 appears to saturate our evaluated tasks, scoring greater than 95% on 5 of them. Given that we evaluate on publicly accessible benchmarks, this could be explained by leakage into GPT-4's pre-training data. We find that Embroid is strictly better than the base prompt on 42% of tasks, and effectively equivalent or better (i.e., no worse than 1 point F1) on 73% of tasks. On two tasks Embroid improves by 4+ points.

Though Embroid's absolute gains over base prompts may diminish for stronger base LMs like GPT-4, our evaluation and findings focused on open-source models because of their practical feasibility. Such models are cheaper and better suited for sensitive domains, as they allow data to remain on-premises. This is a particularly salient concern for applications in medicine, law, or government. On these models, we found that Embroid offered substantial gains, with a win-rate > 88% and an average improvement > 7 points F1.

### Comparison to self-consistency

We evaluate Embroid against self-consistency  over GPT-3.5, and find that (1) it can provide competitive performance to self-consistency, and (2) that it can improve predictions generated from self-consistency.

We study a subset of five tasks: ag_news_business, CUAD_Audit Rights, dbpedia_albun, few_rel_architect, learned_hands_HO. For self-consistency, we generate 5 predictions per sample, using a temperature of 0.7. For each task, we consider a "base" prompt which does not use chain-of-thought, but otherwise uses the same task instructions and in-context demonstrations. For 4/5 tasks, Embroid applied to the output of the base prompt outperforms self-consistency. On all tasks, Embroid applied to the predictions generated from self-consistency improve performance, by a median of six points.

We additionally note one important distinction between self-consistency and Embroid. While self-consistency requires multiple predictions from a LM for a single sample (thus accruing additional cost in hardware usage or API calls), Embroid requires only one prediction per-sample.

### Multi-class experiments

We present initial results evaluating Embroid in the multi-class setting. We create a multi-class version of the contract classification task by combining five different binary classification datasets: CUAD_Audit right, CUAD_Governing law, CUAD_Non-compete, CUAD_Volume restriction, and CUAD_Expiration date. The total size of this dataset is 1858, with the largest class (602 samples) occurring nearly 3.5x as much as the smallest class (165 samples).

We evaluate a one-vs-all version of Embroid, where Embroid is applied five times--each time predicting one of the classes. A final class prediction is generated by comparing each of the class-specific Embroid classifier's predicted probability for the class, and choosing the class with the highest probability. Overall, we find that this variant of Embroid improves the quality of the base prompt (by 4 points on balanced-accuracy and 1.6 points on macro-F1).