# Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning

Takuo Matsubara

The University of Edinburgh

Edinburgh, EH9 3JZ

takuo.matsubara@ed.ac.uk

###### Abstract

Gradient boosting is a sequential ensemble method that fits a new weaker learner to pseudo residuals at each iteration. We propose Wasserstein gradient boosting, a novel extension of gradient boosting, which fits a new weak learner to alternative pseudo residuals that are Wasserstein gradients of loss functionals of probability distributions assigned at each input. It solves distribution-valued supervised learning, where the output values of the training dataset are probability distributions. In classification and regression, a model typically returns, for each input, a point estimate of a parameter of a noise distribution specified for a response variable, such as the class probability parameter of a categorical distribution specified for a response label. A main application of Wasserstein gradient boosting in this paper is tree-based evidential learning, which returns a distributional estimate of the response parameter for each input. We empirically demonstrate the competitive performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with existing uncertainty quantification methods.

## 1 Introduction

Gradient boosting is a celebrated machine learning algorithm that has achieved considerable success with tabular data . Gradient boosting has been extensively used for point forecasts and probabilistic classification, yet a relatively small number of studies have been concerned with the predictive uncertainty of gradient boosting. Predictive uncertainty of machine learning models plays a growing role in today's real-world production systems . It is vital for safety-critical systems, such as medical diagnoses  and autonomous driving , to assess the potential risk of their actions that partially or entirely rely on predictions from their models. Gradient boosting has already been applied in a diverse range of real-world applications, including click prediction , ranking systems , scientific discovery , and data competition . There is a pressing need for methodology to harness the power of gradient boosting to predictive uncertainty quantification.

In classification and regression, we typically specify a noise distribution \(p(y)\) of a response variable \(y\) and use a model to return a point estimate \((x)\) of the response parameter for each input \(x\). In recent years, the importance of capturing uncertainty in the model output \((x)\) has increasingly been emphasised . A variety of approaches have been proposed to obtain a distributional estimate \(p( x)\) of the response parameter for each input \(x\) [e.g. 9, 10, 11]. For example, Bayesian neural networks (BNNs) quantify uncertainty in network weights and propagate it to the space of network outputs. Marginalising the predictive distribution \(p(y)\) over the distributional estimate \(p( x)\) has been demonstrated to confer enhanced predictive accuracy and robustness against adversarial attacks . Furthermore, the dispersion of the distributional estimate has been used as a powerful indicator for out-of-distribution (OOD) detection .

In this context, a line of research based on the concept of _evidential learning_ has recently gained significant attention . The idea can be broadly interpreted as making use of the 'individual-level' posterior \(p( y_{i})\) of the response parameter \(\) conditional on each individual datum \(y_{i}\), which arises from the response-distribution likelihood \(p(y_{i})\) and a user-specified prior \(p()\). If each individual-level posterior falls into a closed form characterised by some hyperparameter, neural networks can be trained by using the finite-dimensional hyperparameter as a target value for each input. Outstanding performance and computational efficiency of the existing approaches have been delivered in a wide spectrum of engineering and medical applications . However, the existing approaches are limited to neural networks and to the case where every individual-level posterior is in closed form so that the finite-dimensional hyperparameter can be predicted by proxy. In general, posterior distributions are known only up to their normalising constants and, therefore, require an approximation typically by particles .

Without closed-form expression, each individual-level posterior needs to be treated as an infinite-dimensional output for each input. This challenge poses the following fundamental question:

Consider a supervised learning setting whose outputs are probability distributions. Given a training set of input values and output distributions \(\{x_{i},_{i}\}_{i=1}^{D}\), can we build a model that receives an input \(x\) and returns a _nonparametric_ prediction of the output distribution?

Motivated by this question, we propose a general framework of Wasserstein gradient boosting (WGBoost). WGBoost receives an input and returns a particle approximation of the output distribution. Figure 1 illustrates inputs and outputs of WGBoost. In this paper, we focus on application of WGBoost to evidential learning, where the individual-level posterior \(p( y_{i})\) of the response parameter \(\) is used as the output distribution \(_{i}\) for each input \(x_{i}\) in the training set. Figure 2 compares the pipeline of evidential learning based on WGBoost with that of Bayesian learning.

**Contributions**: Our contributions are summarised as follows:

1. **Methodology of WGBoost**: Section 2 establishes the general framework of WGBoost. It is a novel family of gradient boosting that returns a set of particles that approximates an output distribution assigned at each input. In contrast to standard gradient boosting that fits a weak learner to the gradient of a loss function, WGBoost fits a weak learner to the estimated Wasserstein gradient of a loss functional over probability distributions.
2. **Application to Evidential Learning**: Section 3 establishes tree-based evidential learning based on WGBoost, with the loss functional specified by the Kullback-Leibler (KL) divergence. Following modern gradient-boosting libraries  that uses second-order gradient boosting (c.f. Section 2.2), we implement a concrete second-order WGBoost algorithm built on an approximate Wasserstein gradient and Hessian of the KL divergence.
3. **Experiment on Real-world Data**: Section 4 demonstrates the performance of probabilistic regression, and classification with OOD detection, on real-world tabular datasets. To the author's knowledge, WGBoost is the first framework that enables evidential learning for (i) boosted tree models and (ii) cases without closed form of individual-level posteriors.

Figure 1: Illustration of WGBoost trained on a set \(\{x_{i},_{i}\}_{i=1}^{10}\) whose inputs are 10 grid points in \([-3.5,3.5]\) and each output distribution is a normal distribution \(_{i}()=((x_{i}),0.5)\) over \(\). The blue area indicates the \(95\)% high probability region of the conditional distribution \(((x),0.5)\). WGBoost returns \(N=10\) particles (red lines) to predict the output distribution for each input \(x\). This illustration uses the Gaussian kernel regressor for every weaker learner.

## 2 Wasserstein Gradient Boosting

This section establishes the general formulation of WGBoost. Section 2.1 recaps the notion of Wasserstein gradient flows, a 'gradient' system of probability distributions that minimises an objective functional in the space of probability distributions. Section 2.2 recaps the notion of gradient boosting, a sequential ensemble method that fits a new weak learner to the 'gradient' of the remaining loss at each iteration. Section 2.3 combines the above two notions to establish WGBoost, a novel family of gradient boosting that enables to solve distribution-valued supervised learning.

Notation and SettingLet \(\) and \(\) denote the space of inputs and responses in classification and regression. Suppose \(=^{d}\). Let \(_{2}\) be the 2-Wasserstein space i.e. a set of all probability distributions on \(\) with finite second moment equipped with the Wasserstein metric . We identify a probability distribution in \(_{2}\) with its density whenever it exits. Denote by \(\) and \(\), respectively, elementwise multiplication and elementwise division of two vectors in \(^{d}\). Let \(\) be the gradient operator. Let \(_{}^{2}\) be a second-order gradient operator that takes the second derivative at each coordinate i.e. \(_{}^{2}f()=[^{2}f()/_{1}^{2},,^{2}f()/_{d}^{2}]^{}^{d}\).

### Wasserstein Gradient Flow

In the Euclidean space, a gradient flow of a function \(f\) means a curve of points \(x_{t}\) that solves a differential equation \((d/dt)x_{t}=- f(x_{t})\) from some initial value \(x_{0}\). That is the continuous-time limit of gradient descent, which minimises the function \(f\) as \(t\). A Wasserstein gradient flow means a curve of probability distributions \(_{t}\) minimising a given functional \(\) on the 2-Wasserstein space \(_{2}\) from some initial distribution \(_{0}\). The Wasserstein gradient flow \(_{t}\) is characterised as a solution of a partial differential equation, known as the _continuity equation_:

\[_{t}=-(_{t}_{W}(_{t})) _{0}_{2},\] (1)

where \(_{W}():\) denotes the _Wasserstein gradient_ of \(\) at \(\)[24; 25]. Appendix A recaps the derivation of the Wasserstein gradient, presenting the examples for several functionals.

One of the elegant properties of the Wasserstein gradient flow is casting the infinite-dimensional optimisation of the functional \(\) as a finite-dimensional particle update . The continuity equation (1) can be reformulated as a dynamical system of a random variable \(_{t}_{t}\), such that

\[_{t}=-[_{W}(_{t})](_{t} )_{0}_{0},\] (2)

in the sense that the law \(_{t}\) of such a random variable \(_{t}\) is a weak solution of the continuity equation. Consider the case where the initial measure \(_{0}\) is set to the empirical distribution \(_{0}\) of \(N\) particles \(\{_{0}^{n}\}_{n=1}^{N}\). Discretising the continuous-time system (2) by the Euler method with a small step size \(>0\) yields an iterative update scheme of \(N\) particles \(\{_{m}^{n}\}_{n=1}^{N}\) from step \(m=0\):

\[_{m+1}^{1}\\ \\ _{m+1}^{N}=_{m}^{1}\\ \\ _{m}^{N}+-[_{W}(_{m})](_{m}^{1})\\ \\ -[_{W}(_{m})](_{m}^{N}),\] (3)

Figure 2: Comparison of the pipeline of (a) Bayesian learning and (b) evidential learning based on WGBoost. The former uses the (global-level) posterior \(p(w\{x_{i},y_{i}\}_{i=1}^{D})\) of the model parameter \(w\) conditional on all data, and samples multiple models from it. The latter uses the individual-level posterior \(p( y_{i})\) of the response parameter \(\) as the output distribution of the training set, and trains WGBoost that returns a particle-based distributional estimate \(p( x)\) of \(\) for each input \(x\).

where \(_{m}\) denotes the empirical distribution of the particles \(\{_{m}^{n}\}_{n=1}^{N}\) at step \(m\).

In practice, it is common that the Wasserstein gradient of a chosen functional \(\) is not well-defined for empirical distributions. In such case, the particle update scheme (3) is not directly applicable because it depends on the Wasserstein gradient \(_{W}(_{m})\) at the empirical distribution \(_{m}\). For example, the KL divergence \(()=()\) with a reference distribution \(\) leads to the Wasserstein gradient \([_{W}()]()=-(()-( ))\) ill-defined if \(\) is an empirical distribution. Hence, the particle update scheme (3) is often performed with the estimated or approximated Wasserstein gradient well-defined for empirical distributions (e.g. 26; 27; 28; 29; 30). A main application of WGBoost in Section 3 uses the'smoothed' Wasserstein gradient of the KL divergence .

### Gradient Boosting

Gradient boosting  is a sequential ensemble method of \(M\) weak learners \(f_{1},,f_{M}\). It iteratively constructs an ensemble \(F_{m}\) of \(m\) weak learners \(f_{1},,f_{m}\) from step \(m=0\) to \(M\). Given the current ensemble \(F_{m}\) at step \(m\), it trains a new weak learner \(f_{m+1}\) to construct the next ensemble by

\[F_{m+1}(x)=F_{m}(x)+ f_{m+1}(x),\] (4)

where \(\) is a shrinkage hyperparameter called a _learning rate_. The initial state of the ensemble \(F_{0}(x)\) at step \(m=0\) is typically set to a constant that best fits the data. Any learning algorithm can be used as a weak learner in principle, although tree-based algorithms are most used .

The fundamental idea of gradient boosting is to train the new weak learner \(f_{m+1}\) to approximate the negative gradient of the remaining error of the current ensemble \(F_{m}\). Suppose that a loss function \(L\) measures the remaining error \(R_{i}(F_{m}(x_{i})):=L(F_{m}(x_{i}),y_{i})\) for each output vector \(y_{i}^{d}\). The new weak learner \(f_{m+1}\) is fitted to the set \(\{x_{i},g_{i}\}_{i=1}^{D}\) whose target variable \(g_{i}\) is each specified by

\[g_{i}:=- R_{i}(F_{m}(x_{i}))^{d}.\]

The target \(g_{i}\) is often called a _pseudo residual_. For each data input \(x_{i}\), the boosting scheme (4) updates the output of the current ensemble \(F_{m}(x_{i})\) in the steepest descent direction of the error \(R_{i}(F_{m}(x_{i}))\). Although  originally suggested an additional line search to determine a scaling constant of each weak learner, the line search has been reported to have a negligible influence on performance .

In modern gradient-boosting libraries, such as XGBoost  and LightGBM , the standard practice is to use the diagonal (coordinatewise) Newton direction of the remaining error \(R_{i}(F_{m}(x_{i}))\) in lieu of the negative gradient \(g_{i}\). The new base leaner \(f_{m+1}\) is instead fitted to the set \(\{x_{i},g_{i} h_{i}\}_{i=1}^{n}\), where the negative gradient \(g_{i}\) is divided elementwise by the Hessian diagonal \(h_{i}\) given by

\[h_{i}:=_{}^{2}R_{i}(F_{m}(x_{i}))^{d}.\]

The target variable \(g_{i} h_{i}\) is the diagonal Newton direction that minimises the second-order Taylor approximation of the remaining error \(R_{i}(F_{m}(x_{i}))\) for each coordinate independently. Combining the second-order gradient boosting framework with tree-based weak learners has demonstrated exceptional scalability and performance [34; 35]. Although it is possible to use the 'full' Newton direction as the target variable of each weak learner, the impracticality of the full Newton direction has been pointed out (e.g. 36; 37). In addition, the coordinatewise computability of the diagonal Newton direction is suitable for popular gradient-boosting tree algorithms .

### General Formulation of Wasserstein Gradient Boosting

Now we consider the setting of distribution-valued supervised learning, where we are given a training set of input vectors and output distributions \(\{x_{i},_{i}\}_{i=1}^{D}_{2}\). Our goal is to construct a model that receives an input and returns a set of \(N\) particles whose empirical distribution approximates the output distribution. We specify a loss functional \(()\) between two probability distributions--such as the KL divergence--to measure the remaining error \(_{i}()=(_{i})\) for each \(i\)-th training output distribution \(_{i}\). Our idea is to combine gradient boosting with Wasserstein gradient, where we iteratively construct a set of \(N\) boosting ensembles \(F_{m}^{1},,F_{m}^{N}\) from step \(m=0\) to \(M\).

Here, the output \(F_{m}^{n}(x)\) of each \(n\)-th boosting ensemble represents the \(n\)-th output particle for an input \(x\). Given the current set of \(N\) ensembles \(F_{m}^{1},,F_{m}^{N}\) at step \(m\), WGBoost trains a set of new weak learners \(f^{1}_{m+1},,f^{N}_{m+1}\) and computes the next set of \(N\) ensembles by

\[F^{1}_{m+1}(x)\\ \\ F^{N}_{m+1}(x)=F^{1}_{m}(x)\\ \\ F^{N}_{m}(x)+f^{1}_{m+1}(x)\\ \\ f^{N}_{m+1}(x)\] (5)

where \(\) is a learning rate. Similarly to standard gradient boosting, we specify the initial state of \(N\) ensembles \(F^{1}_{0},,F^{N}_{0}\) at step \(m=0\) by a set of constants. Throughout, denote by \(_{m,i}\) the empirical distribution of the \(N\) output particles \(F^{1}_{m}(x_{i}),,F^{N}_{m}(x_{i})\) for each \(i\)-th training input \(x_{i}\).

As discussed in Section 2.1, the Wasserstein gradient often needs to be estimated for empirical distributions. For better presentation, let \(_{i}()\) denote an estimate of the Wasserstein gradient \(_{W}_{i}()\) of the \(i\)-th remaining error \(_{i}()\), which is well-defined for any distribution \(\). If the Wasserstein gradient \(_{W}_{i}()\) is originally well-defined for any distribution \(\), it is a trivial choice of the estimate, i.e., \(_{i}()=_{W}_{i}()\). Otherwise, any suitable estimate can be used as \(_{i}()\). The foundamental idea of WGBoost is to train the \(n\)-th new learner \(f^{n}_{m+1}\) to approximate the estimated Wasserstein gradient \(-_{i}(_{m,i})\) evaluated at the \(n\)-th boosting output \(F^{n}_{m}(x_{i})\) for each \(x_{i}\), so that,

\[f^{1}_{m+1}(x_{i})\\ \\ f^{N}_{m+1}(x_{i})-[_{i} (_{m,i})](F^{1}_{m}(x_{i}))\\ \\ -[_{i}(_{m,i})](F^{N}_{m}(x_{i })).\]

For each data input \(x_{i}\), the boosting scheme (5) approximates the particle update scheme (3) for the output particles \(F^{1}_{m}(x_{i}),,F^{N}_{m}(x_{i})\) under the estimated Wasserstein gradient. The output particles are updated in the direction to decrease the remaining error \(_{i}(_{m,i})=(_{m,i}_{i})\) at each step \(m\).

Algorithm 1 summarises the general procedure of WGBoost. See Figure 1 for illustration of WGBoost. In Section 3, we choose the KL divergence as a loss functional \(\) and use a kernel smoothing estimate of the Wasserstein gradient. See Appendix A for the Wasserstein gradient of other divergences.

**Remark 1** (**Stochastic WGBoost)**.: Stochastic gradient boosting  uses only a randomly sampled subset of data to fit a new weak learner at each step \(m\) to reduce the computational cost. The same subsampling approach can be applied for WGBoost whenever the dataset is large.

**Remark 2** (**Second-Order WGBoost)**.: If any estimate of the Wasserstein 'Hessian' of the remaining error \(_{i}\) is available, the Newton direction of \(_{i}\) may also be computable [e.g. 39, 40]. Implementation of a second-order WGBoost algorithm is immediate by plugging such a Newton direction into \(_{i}()\) in Algorithm 1. Our default WGBoost algorithm for tree-based evidential learning is built on a diagonal approximate Newton direction of the KL divergence, aligning with the standard practice in modern gradient-boosting libraries to use the diagonal Newton direction.

``` Input: training set \(\{x_{i},_{i}\}_{i=1}^{D}\) of input \(x_{i}\) and output distribution \(_{i}_{2}\) Parameter: loss \(\), estimate \(_{i}()\) of the Wasserstein gradient \(_{W}\,(_{i})\), particle number \(N\), iteration \(M\), learning rate \(\), weak learner \(f\), initial constants \((^{1}_{0},,^{N}_{0})\) Output: set of \(N\) boosting ensembles \((F^{1}_{M},,F^{N}_{M})\) at final step \(M\) \((F^{1}_{0}(),,F^{N}_{0}())(^{1}_{0},, ^{N}_{0})\)\(\) set initial state of \(N\) boosting ensembles for\(m 0, M-1\)do for\(i 1,,D\)do \(_{m,i}\) empirical distribution of output values \((F^{1}_{m}(x_{i}),,F^{N}_{m}(x_{i}))\) for input \(x_{i}\) for\(n 1,,N\)do \(g^{n}_{i}-[_{i}(_{m,i})](F^{n}_{m}(x_ {i}))\)\(\) compute target value of \(n\)-th new weak learner end for for\(n 1,,N\)do \(f^{n}_{m+1}\) fit\((\,\{x_{i},g^{n}_{i}\}_{i=1}^{D}\,)\)\(\) fit \(n\)-th new weak learner \(F^{n}_{m+1}() F^{n}_{m}()+ f^{n}_{m+1}()\)\(\) set next state of \(n\)-th boosting ensemble end ```

**Algorithm 1**Wasserstein Gradient Boosting

**Remark 3** (**Second-Order WGBoost)**.: If any estimate of the Wasserstein 'Hessian' of the remaining error \(_{i}\) is available, the Newton direction of \(_{i}\) may also be computable [e.g. 39, 40]. Implementation of a second-order WGBoost algorithm is immediate by plugging such a Newton direction into \(_{i}()\) in Algorithm 1. Our default WGBoost algorithm for tree-based evidential learning is built on a diagonal approximate Newton direction of the KL divergence, aligning with the standard practice in modern gradient-boosting libraries to use the diagonal Newton direction.

Application to Evidential Learning

This section provides our default setting to implement a concrete WGBoost algorithm for evidential learning, which enables classification and regression with predictive uncertainty. The individual-level posterior \(p( y_{i})\) of a response distribution \(p(y)\) is used as the output distribution \(_{i}\) of the training set \(\{x_{i},_{i}\}_{i=1}^{D}\). Section 3.1 recaps derivation of the individual-level posterior \(p( y_{i})\), followed by Section 3.2 discussing the default choice of the prior. We choose the KL divergence as a loss functional of WGBoost. Section 3.3 recaps a widely-used estimate of the Wasserstein gradient of the KL divergence based on kernel smoothing . A further advantage of the kernel smoothing estimate is that the approximate Wasserstein Hessian is available, with which Section 3.4 establishes a second-order WGBoost algorithm similarly to modern gradient-boosting libraries.

### Derivation of Individual-Level Posteriors and Predictive Distribution

Suppose that a response distribution \(p(y)\) of a response variable \(y\) is specified, as is typically done for probabilistic prediction. Suppose also that a prior distribution \(p_{i}()\) of the response parameter \(\) is specified for each individual data input \(x_{i}\). For each individual data pair \((x_{i},y_{i})\), the response-distribution likelihood \(p(y_{i})\) and the prior \(p_{i}()\) determine the individual-level posterior

\[p( y_{i}) p(y_{i})p_{i}()\]

by Bayes' theorem. This individual-level posterior is set to the output distribution \(_{i}\) of the training set \(\{x_{i},_{i}\}_{i=1}^{D}\) of WGBoost. The framework of WGBoost then constructs a model that returns a particle approximation of the output distribution \(_{i}()=p( y_{i})\) for each data input \(x_{i}\).

For a new input \(x\), the constructed WGBoost model provides a set of particles \((^{1}(x),,^{N}(x))\) as a distributional prediction \(p( x)\) of the response parameter \(\). We can define a predictive distribution \(p(y x)\) of the response \(y\) for the new input \(x\) via marginalisation of the output particles:

\[p(y x)=_{}p(y)p( x)d=_ {i=1}^{N}p(y^{i}(x)).\] (6)

We can also define a point prediction \(\) for the new input \(x\) via the individual-level Bayes action \(=_{y}\;_{}U(y,)p( x )d\), which minimises the average of some error \(U:\). For example, the Bayes action \(\) is simply the mean of the output particles if \(U(y,)=(y-)^{2}\).

In general, the explicit form of the individual-level posterior \(p( y_{i})\) is known only up to the normalising constant. Our full algorithm in Section 3.4 requires no normalising constant of the individual-level posterior \(p( y_{i})\). Our algorithm depends only on the log-gradient of the individual-level posterior \( p( y_{i})\) that cancels any constant term by the gradient. Hence, knowing the form of the response-distribution likelihood \(p(y_{i})\) and the prior \(p_{i}()\) suffices.

**Remark 3** (Difference from Bayesian Learning).: Given a response distribution \(p(y)\) and a model \(=f(x,w)\) with the parameter \(w\), Bayesian learning of the model \(f\) means the use of the posterior \(p(w\{x_{i},y_{i}\}_{i=1}^{D})\) over \(w\) conditional on all data. The predictive distribution \(p(y x)\) of the response \(y\) is defined via marginalisation over \(w\): \(_{}p(y=f(x,w))p(w\{x_{i},y_{i}\}_{i=1}^{D})dw\). In contrast, WGBoost returns a distributional prediction \(p( x)\) of the response parameter \(\), circumventing the marginalisation over the model parameter \(w\) that can be ultra-high dimensional.

### Choice of Individual-Level Priors

The prior distribution \(p_{i}()\) of the response parameter \(\) is specified at each individual data input \(x_{i}\). The approach to eliciting the prior may differ, depending on whether past data are available. If past data are available, they can be utilised to elicit a reasonable prior for unobserved data. If no past data are available, we recommend the use of a noninformative prior that have been developed as a sensible choice of prior in the absence of past data; see (e.g. 41) for the introduction. To avoid numerical errors, if a noninformative prior is improper (i.e. nonintegrable), we recommend the use of a proper probability distribution that approximates the noninformative prior sufficiently well.

**Example 1** (Normal Location-Scale Response).: Consider a scalar-valued response variable \(y\) for regression. A normal location-scale response distribution \((y m,)\) has the mean and scale parameters \(m\) and \((0,)\). A typical noninformative prior of \(m\) and \(\) are given by,respectively, \(1\) and \(1/\) which are improper. At every data point \((x_{i},y_{i})\), we use a normal prior \((m 0,_{0})\) over \(m\) and an inverse gamma prior \((_{0},_{0})\) over \(\), with the hyperparameters \(_{0}=10\) and \(_{0}=_{0}=0.01\), which approximate the non-informative priors.

**Example 2** (**Categorical Response**).: Consider a label response variable \(y\{1,,k\}\) for \(k\)-class classification. A categorical response distribution \((y q)\) has the class probability parameter \(q=(q_{1},,q_{k})\) in the \(k\)-dimensional simplex \(_{k}\). If \(k=2\), it corresponds to the Bernoulli distribution. A typical noninformative prior of \(q\) is given by \(1/(q_{1} q_{k})\) which are improper. At every data point \((x_{i},y_{i})\), we use the logistic normal prior--a multivariate generalisation of the logit normal distribution --over \(q\) with the mean \(0\) and identity covariance matrix scaled by \(10\).

**Remark 4** (**Reparametrisation and Standardisation**).: Section 2 supposed \(=^{d}\) for some dimension \(d\) without no loss of generality. Any parameter that lies in a subset of the Euclidean space (e.g. \(\) in Example 1) can be reparametrised as one in the Euclidean space (e.g. \(\)). Appendix D details the reparametrisation used for Examples 1 and 2. In addition, if one's dataset has scalar outputs of a low or high order of magnitude, we recommend standardising the outputs.

### Approximate Wasserstein Gradient of KL Divergence

We consider the KL divergence \((_{i})\) as a loss functional of WGBoost. One challenge of the KL divergence is that the resulting Wasserstein gradient \([_{i}^{}()]():=-( _{i}()-())\) is not well-defined when \(\) is an empirical distribution. A particularly successful solution--which originates in  and has been applied in wide contexts [26; 44; 45]--is to smooth the Wasserstein gradient through a kernel integral operator \(_{}[_{i}^{}()](^{*}k(,^ {*})d(^{*})\). By integration-by-part (see [e.g. 43]), the smoothed Wasserstein gradient, denoted \(_{i}^{*}()\), falls into the following form that is well-defined for any distribution \(\):

\[[_{i}^{*}()]():=-_{^{*} }_{i}(^{*})k(^{*},)+ k(^ {*},)^{d},\] (7)

where \( k(^{*},)\) denotes the gradient of \(k\) with respect to the first argument \(^{*}\). An approximate Wasserstein gradient flow based on the smoothed Wasserstein gradient \(_{i}^{*}()\) is called the Stein variational gradient descent  or kernelised Wasserstein gradient flow . In most cases, the kernel \(k\) is set to the Gaussian kernel \(k(,^{*})=(-\|-^{*}\|^{2}/h)\) with the scale \(h>0\). Appendix B discusses a choice of kernel. This work uses the Gaussian kernel with \(h=0.1\) throughout.

Another common approach to approximating the Wasserstein gradient flow of the KL divergence is the Langevin diffusion approach . The discretised algorithm, called the unadjusted Langevin algorithm , is a stochastic particle update scheme that adds a Gaussian noise at every iteration. However, several known challenges, such as asymptotic bias and slow convergence, often necessitate an ad-hoc adjustment of the algorithm . Appendix B discusses a variant of WGBoost built on the Langevin algorithm, although it is not considered the default implementation.

### Second-Order Implementation of WGBoost

We use a diagonal (coordinatewise) approximate Wasserstein Newton direction of the KL divergence, following the standard practice in modern gradient-boosting libraries [21; 22] to use the diagonal Newton direction of a loss. Similarly to smoothed Wasserstein gradient \(_{i}^{*}()\), the approximate Wasserstein Hessian of the KL divergence \((_{i})\) can be obtained through the kernel smoothing. The diagonal of the approximate Wasserstein Hessian, denoted \(_{i}^{*}()\), is defined by

\[[_{i}^{*}()]():=_{^{*}} -_{}^{2}_{i}(^{*})k(,^{*})^{2 }+ k(,^{*}) k(,^{*}) ^{d}.\] (8)

The diagonal approximate Wasserstein Newton direction of the KL divergence is then defined by \(-[_{i}^{*}()]()[_{i}^{*}( )]()\). Appendix C provides the derivation based on  who derived the Newton direction of the KL divergence in the context of nonparametric variational inference.

The second-order WGBoost algorithm is established by plugging it into \(_{i}()\) in Algorithm 1, that is,

\[[_{i}()]()=[_{i}^{*}()] ()[_{i}^{*}()]().\] (9)

Algorithm 1 under the choice (9) is considered our default WGBoost algorithm for evidential learning. We refer this algorithm to as the _Wasserstein-boosted evidential learning_ (WEvidential). The explicit pseudocode is provided in Algorithm 2 for full clarity.

``` Input: dataset \(\{x_{i},y_{i}\}_{i=1}^{D}\) of input \(x_{i}\) and response \(y_{i}\) of classification or regression Parameter : individual-level posterior \(p( y_{i})\) of response distribution \(p(y)\), particle number \(N\), iteration \(M\), learning rate \(\), weak learner \(f\), initial constants \(\{_{0}^{n}\}_{n=1}^{N}\) Output: set of \(N\) boosting ensembles \((F_{M}^{1},,F_{M}^{N})\) at final step \(M\) \((F_{0}^{1}(),,F_{0}^{N}())(_{0}^{1},, _{0}^{N})\)\(\) set initial state of \(N\) boostings for\(m 0,,M-1\)do for\(i 1,,D\)do \((_{1}^{1},,_{N}^{N})(F_{1}^{1}(x_{i}),,F_{m }^{N}(x_{i}))\)\(\) get output particles for \(i\)-th data input for\(n 1,,N\)do \(g_{i}^{n}_{k=1}^{N} p(_{i}^{k} y_ {i})k(_{i}^{k},_{i}^{n})+ k(_{i}^{k},_{i}^{n})\) \(h_{i}^{n}_{k=1}^{N}-_{}^{2} p( _{i}^{k} y_{i})k(_{i}^{k},_{i}^{n})^{2}+ k(_ {i}^{k},_{i}^{n}) k(_{i}^{k},_{i}^{n})\)  end for for for\(n 1,,N\)do \(f_{m+1}^{n}\{x_{i},g_{i}^{n} h_{i}^{n} \}_{i=1}^{D}\)\(\) fit \(n\)-th new weak learner \(F_{m+1}^{n}() F_{m}^{n}()+ f_{m+1}^{n}()\)\(\) set next state of \(n\)-th boosting  end for  end for ```

**Algorithm 2**Wasserstein-Boosted Evidential Learning

**Remark 5** (**Computation)**.: The diagonal Newton direction (9) has the computational complexity \((N d)\) same as that of the smoothed Wasserstein gradient. Hence, there is essentially _no reason not to use_ the diagonal Newton direction (9) instead of the smoothed Wasserstein gradient. Although it is possible to use the full Newton direction with no diagonal approximation, the computation requires the inverse and product of \((N d)(N d)\) matrices that result in the complexity up to \((N^{3} d^{3})\). Appendix D presents a simulation study to compare computational time and convergence speed of four WGBoost algorithms built on different estimates of the Wasserstein gradient.

## 4 Experiment on Real-world Tabular Data

We empirically demonstrate the performance of the WGBoost algorithm through three experiments using real-world tabular data. The first application illustrates the output of WGBoost through a simple conditional density estimation. The second application benchmarks the probabilistic regression performance. The third application demonstrates the classification and OOD detection performance. The source code is available in https://github.com/takuomatsubara/WGBoost.

**Common Hyperparameters** Throughout, we set the number of output particles \(N\) to \(10\) and set each weak learner \(f\) to the decision tree regressor  with maximum depth \(1\) for Section 4.1 and \(3\) for the rest. We set the learning rate \(\) to \(0.1\) for regression and \(0.4\) for classification, unless otherwise stated. Appendix E contains further details, including a choice of the initial constant \(\{_{0}^{n}\}_{n=1}^{N}\).

### Illustrative Conditional Density Estimation

We illustrate the output of WEvidential by estimating a conditional density \(p(y x)\) from one-dimensional scalar inputs and outputs \(\{x_{i},y_{i}\}_{i=1}^{D}\). The normal output distribution \((y m,)\) and the prior \(p_{i}(m,)\) in Example 1 were used to define the individual-level posterior \(p(m, y_{i})\), in which case the output of the WGBoost algorithm is a set of \(10\) particles \(\{(m^{n}(x),^{n}(x))\}_{n=1}^{10}\) of the mean and scale parameters for each input \(x\). We chose the number of weak learners \(M\), drawing on an early-stopping approach used in , where we held out 20% of the training set as a validation set and chose the number \(1 M 4000\) achieving the least validation error. Once the number \(M\) was chosen, WEvidential was trained again using all the entire training set.

The conditional density is estimated using the predictive distribution (6) by WEvidential. We used two real-world datasets, _bone mineral density_ and _old faithful geyser_. Figure 3 depicts the result for the former dataset, demonstrating that the WGBoost algorithm captures the heterogeneity of the conditional density on each input well. The result for the latter dataset is contained in Appendix E.1.

### Probabilistic Regression Benchmark

We examine the regression performance of WEvidential using a standard benchmark protocol that originated in  and has been used in a number of subsequent works [10; 9; 32]. The benchmark protocol uses real-world tabular datasets from the UCI machine learning repository , each with one-dimensional scalar responses. As in Section 4.1, the normal response distribution \((y m,)\) and the prior \(p_{i}(m,)\) in Example 1 were used to define the individual-level posterior \(p(m, y_{i})\).

We followed the data splitting protocol in  and randomly held out 10% of each dataset as a test set. The negative log likelihood (NLL) is measured by using the predictive distribution (6). The root mean squared error (RMSE) is measured by using the point prediction by the mean value of the predictive distribution. We chose the number of weak learners \(M\) by the same approach as in Section 4.1. We repeated this procedure 20 times for each dataset, except the _protein_ and _year msd_ datasets for which we repeated five times and once. For the year msd dataset only, we subsampled 10% of data to fit each weak learner and used the learning rate 0.01 due to the large dataset size.

Table 1 compares the performance of WEvidential with five other methods: Monte Carlo Dropout (MCDropout) , Deep Ensemble (DEnsemble) , Concrete Dropout (CDropout) , Natural Gradient Boosting (NGBoost) , and Deep Evidential Regression (DEvidential) . Appendix E provides further details on the experiment and a limited yet additional comparison. The WGBoost algorithm achieves the best score or a score sufficiently close to the best score most often.

   Dataset & Criteria & WEvidential & MCDropout & DEnsemble & CDropout & NGBoost & DEvidential \\   boston \\ concrete \\ energy \\ kin8nm \\  } & \(\) & \(2.46 0.06\) & \(\) & \(2.72 0.01\) & \(\) & \(\) \\  & \(\) & \(3.04 0.02\) & \(3.06 0.18\) & \(3.51 0.00\) & \(3.04 0.17\) & \(3.01 0.02\) \\   & \(\) & \(1.99 0.02\) & \(1.38 0.22\) & \(2.30 0.00\) & \(\) & \(1.39 0.06\) \\   & -0.44 \(\) 0.03 & -0.95 \(\) 0.01 & -1.20 \(\) 0.02 & -0.65 \(\) 0.00 & -0.49 \(\) 0.02 & -\(\) \\   & NLL & -5.47 \(\) 0.03 & -3.80 \(\) 0.01 & -5.63 \(\) 0.05 & **-5.87\(\) 0.05** & -5.34 \(\) 0.04 & -5.73 \(\) 0.07 \\   & power & \(\) & \(2.80 0.01\) & \(2.79 0.04\) & \(2.75 0.01\) & \(2.79 0.11\) & \(2.81 0.07\) \\   & \(2.70 0.01\) & \(2.89 0.00\) & \(2.83 0.02\) & \(2.81 0.00\) & \(2.81 0.03\) & \(\) \\   & wine & \(\) & \(0.93 0.01\) & \(\) & \(1.70 0.00\) & \(\) & \(\) \\   & yacht & \(\) & \(1.55 0.03\) & \(1.18 0.21\) & \(1.75 0.00\) & \(\) & \(1.03 0.19\) \\   & year msd & \(3.50 0.01\) & \(3.59 0.01\) & \(\) & NA\(\) NA & NA \(\) NA \\   boston \\ concrete \\ energy \\ kin8nm \\  } & \(\) & \(2.97 0.19\) & \(\) & \(\) & \(\) & \(3.06 0.16\) \\   & \(\) & \(5.23 0.12\) & \(6.03 0.58\) & \(4.46 0.16\) & \(5.06 0.61\) & \(5.85 0.15\) \\   & \(\) & \(1.66 0.04\) & \(2.09 0.29\) & \(0.46 0.02\) & \(\) & \(2.06 0.10\) \\   & \(\) & \(0.10 0.00\) & \(0.09 0.00\) & \(\) & \(0.16 0.00\) & \(0.09 0.00\) \\   & \(\) & \(0.01 0.00\) & \(\) & \(\) & \(\) & \(\) \\   & power & \(\) & \(4.02 0.04\) & \(4.11 0.17\) & \(3.70 0.04\) & \(3.79 0.18\) & \(4.23 0.09\) \\   & protein & \(4.09 0.02\) & \(4.36 0.01\) & \(4.71 0.06\) & \(\) & \(4.33 0.03\) & \(4.64 0.03\) \\   & wine & \(\) & \(\) & \(\) & \(0.62 0.00\) & \(\) & \(\) \\   & yacht & \(\) & \(1.11 0.09\) & \(1.58 0.48\) & \(0.57 0.05\) & \(\) & \(1.57 0.56\) \\   & year msd & \(9.11 0.00\) & \(\) & \(8.89 0.00\) & NA\(\) NA & NA & NA \(\) NA \\   

Table 1: The NLLs and RMSEs for each dataset, where the best score is underlined and the scores whose standard deviation ranges include the best score are in bold. Results of MCDropout, DEnsembles, CDropout, NGBoost, and DEvidential were reported in [9; 10], ,  and  respectively.

Figure 3: Conditional density estimation for the bone mineral density dataset (grey dots) by WEvidential, where the normal response distribution \((y m,)\) is used for the response variable \(y\). Left: distributional estimate (10 particles) of the location parameter \(\{m^{n}(x)\}_{n=1}^{10}\) for each input. Right: estimated conditional density (6) through marginalisation of the output particles \(\{(m^{n}(x),^{n}(x))\}_{n=1}^{10}\).

### Classification and Out-of-Distribution Detection

We examine the classification and anomaly OOD detection performance of WEvidential on two real-world tabular datasets, _segment_ and _sensorless_, following the protocol used in . The categorical response distribution \((y q)\) and the prior \(p_{i}(q)\) in Example 2 were used to define the individual-level posterior \(p(q y_{i})\), in which case the output of the WGBoost algorithm is a set of \(10\) particles \(\{q^{n}\}_{n=1}^{10}\) of the class probability parameter \(q\) in the simplex \(^{k}\) for each input \(x\). We set the number of weak learners \(M\) to \(4000\) without early stopping to reduce the computational cost.

The segment and sensorless datasets have 7 and 11 classes in total. For the segment dataset, the data subset that belongs to the last class was kept as the OOD samples. For the sensorless dataset, the data subset that belongs to the last two classes was kept as the OOD samples. For each dataset, 20% of the non-OOD samples is held out as a test set to measure the classification accuracy. There exist several ways of defining a OOD score for each input . For the WGBoost algorithm, the inverse of the maximum norm of the output-particle variance was used as the OOD score. We measured the OOD detection performance by the area under the precision recall curve (PR-AUC), viewing non-OOD test data as the positive class and OOD data as the negative class. We repeated this procedure five times.

Table 2 compares the performance of WEvidential with four other methods: MCDropout, DEnsemble, and Distributional Distillation (DDistillation) , and Posterior Network (PNetwork) . Appendix E provides further details on the experiment. Figure 4 exemplifies how the dispersion of the output particles differ between OOD and non-OOD inputs. WEvidential demonstrates a high classification and OOD detection accuracy simultaneously. Although PNetwork has the best OOD detection performance for the sensorless dataset, the performance of the WGBoost algorithm also exceeds 80%, which is distinct from MCDropout, DEnsemble, and DDistillation.

## 5 Discussion

This work established the general framework of WGBoost and developed the concrete algorithm WEvidential for evidential learning. The established framework of WGBoost offers exciting avenues for future research. Important directions for future study include (i) exploring alternative loss functionals to the KL divergence, (ii) investigating the convergence properties, and (iii) evaluating robustness of obtained predictive uncertainty in comparison to other methods. A particular limitation of WGBoost may arise when data are not tabular, as is the case of standard gradient boosting. These questions require careful examination and are critical for future study.

   Dataset & Criteria & WEvidential & MCDropout & DEnsemble & DDistillation & PNetwork \\   & Accuracy & \(96.57 0.6\) & \(95.25 0.1\) & \(\) & \(96.21 0.1\) & \(96.92 0.1\) \\  & OOD & \(\) & \(43.11 0.6\) & \(58.13 1.7\) & \(35.83 0.4\) & \(96.74 0.9\) \\   & Accuracy & \(\) & \(89.32 0.2\) & \(99.37 0.0\) & \(93.66 1.5\) & \(99.52 0.0\) \\   & OOD & \(81.13 5.3\) & \(40.61 0.7\) & \(50.62 0.1\) & \(31.17 0.2\) & \(\) \\   

Table 2: The classification accuracies and OOD detection PR-AUCs for each dataset, where the best score is underlined and in bold. The results other than WEvidential were reported in .

Figure 4: Examples of the output particles (red dot) of WEvidential on the segment dataset, where the coloured area indicate the kernel density estimation of the output particles for each class.