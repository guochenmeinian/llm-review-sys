# The Map Equation Goes Neural:

Mapping Network Flows with Graph Neural Networks

Christopher Blocker

Data Analytics Group

Department of Informatics

University of Zurich, Switzerland

christopher.bloecker@uzh.ch &Chester Tan

Chair of Machine Learning for Complex Networks

Center for Artificial Intelligence and Data Science

Julius-Maximilians-Universitat Wurzburg, Germany

chester.tan@uni-wuerzburg.de

Also at Data Analytics Group, Department of Informatics, University of Zurich, Switzerland

Ingo Scholtes

Also at Data Analytics Group, Department of Informatics, University of Zurich, Switzerland

###### Abstract

Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets.

## 1 Introduction

Many real-world networked systems are organised in communities: groups of nodes that are more similar to each other than to the rest. Communities provide insights into network structure at the mesoscale, revealing sub-systems by analysing link patterns. Motivated by different research questions, several characterisations of what constitutes "good" communities have been proposed , however, neither of them is fundamentally more correct than any other. Moreover, no single community-detection method outperforms all others on any given network , motivating the ongoing efforts of research on community detection. Typically, community-detection approaches formulate an objective function that calculates a quality score for a given partition of the network's nodes into communities. Finding the best partition is an NP-hard search problem and often involves custom heuristic algorithms that attempt to minimise their objective function .

Graph neural networks (GNNs) have enabled applying deep learning to graph-structured data by utilising the input graph as the neural network's computational graph [7; 8; 9]. Typical tasks for GNNs include node labelling, graph labelling, and link prediction, all of which involve learning meaningful representations jointly from the graph's topology, the nodes' features, and, possibly, the edges' features. Graph labelling relies on coarse-graining the graph through identifying groups of "similar" nodes and aggregating their links and features, also referred to as pooling [10; 11; 12], which is related to graph clustering , however, these two tasks have different goals.

While GNNs excel at incorporating node and edge features with graph topology, including this information is also possible but more challenging with traditional network science approaches, typically requiring modelling or adjusting objective functions and their optimisation algorithms. On the other hand, objective functions for community detection provide precise interpretations as to why one partition is considered better than another while deep-learning-based approaches are black boxes. Model selection in deep learning is often done through regularisation techniques or cross-validation; in contrast, objective functions that are based on the minimum description length (MDL) principle naturally implement Occam's razor, preventing overfitting and enabling principled model selection without requiring extra regularisation or cross-validation [14; 15].

Here, we combine the benefits of traditional community-detection approaches and deep learning and consider the map equation, an information-theoretic objective function for community detection . By adapting the map equation for soft cluster assignments and implementing it in differentiable tensor form, we enable end-to-end optimisation of the map equation as a loss function with gradient descent and GNNs. In analogy to the map equation's stochastic optimisation algorithm Infomap , we call our approach Neuromap and evaluate it against Infomap and several recent GNN-based graph clustering methods. Applied to synthetic and real-world networks, Neuromap demonstrates competitive performance against recent deep graph clustering baselines.

Our key contributions can be summarised as follows:

1. We adapt the map equation as a differentiable loss function for end-to-end deep graph clustering and propose Neuromap, a deep-learning-based alternative to the popular Infomap algorithm for unsupervised community detection with the map equation. Neuromap is compatible with any neural network architecture, detects overlapping communities, leverages node features for improved performance on real-world networks, and, by following the minimum description length principle, does not require explicit regularisation.
2. We extensively evaluate Neuromap on hundreds of synthetic and ten real datasets against recent baselines paired with various neural network architectures. Neuromap outperforms the baseline on the synthetic networks in most settings and is amongst the best performers in seven out of ten real datasets.
3. By choosing a higher maximum number of clusters than previous works, we show empirically that recent baselines tend to overfit and report considerably more than the ground-truth number of communities. Moreover, we find that choosing a small maximum number of communities is often detrimental to graph clustering performance.

## 2 Related work

Community detection._Communities_, also called _clusters_ or _modules_, are groups of nodes that are more "similar" to each other than to the rest, often understood as having more links inside than between groups [1; 2]. However, this rather general characterisation leaves precise details of what constitutes a community open. Modularity compares the observed link densities inside communities against a randomised version of the network . The stochastic block model and its variants assume a latent block structure where the probability that two nodes are connected depends only on their block memberships [18; 19]. The map equation identifies communities as regions where a random walker tends to stay for a relatively long time [16; 20]. Traditional clustering approaches, such as k-means, group nodes based on their proximity in space, however, here we consider identifying communities from the link patterns in networked systems. For a detailed overview of community detection in complex networks, we refer to [1; 2].

Minimum description length principle.The minimum description length principle (MDL) is a model-selection approach that formalises Occam's razor and frames learning as a compression problem [14; 15]. MDL states that the best model for data \(D\) is the one that compresses the data the most. In traditional MDL, the data's two-part description length \(L(D)=_{M}L(M)+L(D M)\) is the smallest achievable length over all models \(M\), where \(L(M)\) is the model's description length, and \(L(D M)\) is the data's description length, given the model. MDL has been adopted for a wide range of applications, including regularising neural networks' weights , investigating deep neural networks' data-compression capabilities , analysing the characteristics of datasets , and community detection [16; 24].

Deep graph clustering and pooling.Graph clustering has long been a research focus in machine learning [13; 25]. Spectral approaches cluster, for example, the eigenspace of a graph's Laplacian matrix or identify communities through graph cuts [26; 27]. Methods based on neural embeddings involve learning node representations with, for example, DeepWalk  or node2vec , followed by applying standard clustering approaches such as k-means, assuming that similar nodes are embedded at similar locations [30; 31]. Other approaches include graph autoencoders [32; 33], contrastive learning , and self-expressiveness . Recently, minimum cuts [11; 12], modularity [36; 37], and the Bernoulli-Poisson model  have been integrated with GNNs as loss functions for graph pooling and clustering. Such GNN-based approaches can incorporate graph structure as well as node and edge features in end-to-end optimisation of the clustering objective. Inspired by pooling in convolutional neural networks, graph pooling coarse-grains links, node features, and edge features to summarise graphs, enabling GNNs with improved performance on node and graph classification tasks [10; 12]. Consequently, graph pooling has become a research focus for GNNs, emphasising the importance of graph clustering as a primary objective [10; 12; 36]. For recent surveys of deep graph clustering, we refer to [39; 40].

## 3 Background: the map equation

The map equation [16; 20] is an information-theoretic objective function for unsupervised community detection that follows the MDL principle , and has demonstrated high performance in synthetic and real networks from across domains [41; 42; 43]. The map equation formulates community detection as a compression problem and uses random walks as a proxy to model dynamic processes on networks, also called _flow_. The goal is to describe the random walk as efficiently as possible by minimising its expected per-step description length - also called _codelength_ - by partitioning the network into groups of nodes, called _modules_, where the random walker tends to stay for a relatively long time. In practice, however, the map equation does not simulate random walks; instead, the codelength is calculated analytically.

Figure 1: Coding principles behind the map equation. Colours indicate modules, codewords are shown next to nodes, and the black trace shows a sequence of random-walker steps. **Left:** All nodes belong to the same module and all codewords are unique. Encoding the random walk sequence requires \(60\) bits, or \(3.72\) bits per step in the limit. **Right:** Partitioning the network enables reusing codewords across modules, reducing the codelength. However, for a unique encoding, we need to introduce codewords for entering and exiting modules, shown next to the arrows pointing into and out of the modules. With this modular coding scheme, we can compress the description to \(48\) bits, or \(3.01\) bits per step in the limit. **Middle:** The two encodings of the random walker’s steps.

Let \(G=(V,E)\) be a graph with nodes \(V\), links \(E\), and let \(w_{uv}_{0}^{+}\) denote the non-negative link weight on the link from node \(u\) to \(v\). When all nodes are assigned to the same module, the codelength is defined by the Shannon entropy \(H\) over the nodes' visit rates , \(H(P)=-_{u V}p_{u}_{2}p_{u}\), where \(p_{u}\) is node \(u\)'s visit rate and \(P=\{p_{u}\,|\,u V\}\) is the set of node visit rates. In undirected graphs, we compute visit rates directly as \(p_{u}=s_{u}/_{v V}s_{v}\), where \(s_{u}=_{v V}w_{uv}\) is node \(u\)'s strength. In directed graphs, we compute the visit rates numerically with smart teleportation  and a power iteration. When we partition the nodes into modules, the codelength becomes a weighted average of the modules' entropies and the entropy at the so-called index level for switching between modules. Figure 1 illustrates the coding principle behind the map equation using Huffman codes ; note, however, that these codewords are only for illustration and we only care about their expected length in the limit to evaluate the map equation.

Minimising the map equation means balancing between small modules to achieve low module-level entropies and large modules for low index-level entropy. This trade-off between module- and index-level entropies prevents trivial solutions where all nodes are assigned to the same module or each node is assigned to a singleton module . The map equation calculates the codelength for a partition \(\),

\[L()=qH(Q)+_{}p_{}H(P _{})\,. \]

Here, \(q=_{}q_{}\) is the random walker's module entry rate, \(q_{}=_{u}_{v}p_{u}t_{uv}\) is module \(\)'s entry rate, and \(Q=\{q_{}/q\;|\;\}\) is the set of normalised module entry rates; \(p_{}=_{}+_{u}p_{u}\) is the rate at which the random walker moves in module \(\), including the module exit rate \(_{}=_{u}_{v}p_{u}t_{uv}\), and \(P_{}=\{_{}/p_{}\} \{p_{u}/p_{}\;|\;u\}\) is the set of normalised node visit and exit rates for module \(\). The random walker's transition probability from node \(u\) to \(v\) is \(t_{uv}=w_{uv}/_{v V}w_{uv}\). We can rewrite the map equation as (see Appendix A)

\[L()=q_{2}q-_{}q_{}_{2}q_{}-_{}m_{}_{2} _{}-_{u V}p_{u}_{2}p_{u}+_{ }p_{}_{2}p_{}. \]

The map equation framework has been extended for overlapping communities through state-space expansions with higher-order network models [47; 48], avoiding over-partitioning in sparse networks using a Bayesian regularisation approach , and to deal with sparse constrained structures . Moreover, the map equation framework can incorporate node features through an extension  or by preprocessing data . Detecting communities relies on Infomap , a greedy stochastic search algorithm that optimises the map equation. However, each of the above extensions requires preprocessing the input data, adjusting the loss function, or adapting the search algorithm. In contrast, adapting the map equation as a loss function for optimisation with gradient descent does not require any custom algorithm, thus enabling flexible experimentation with variations, scalability to GPU clusters, and incorporating it into other loss functions.

## 4 The map equation goes neural

We set out to detect communities by optimising the map equation with GNNs through gradient descent, which essentially means learning coarse-graining node representations in the form of communities (Figure 2). While the standard map equation considers hard clusters where each node is assigned to exactly one module, we introduce a soft cluster assignment matrix \(_{n s}\) to make the map equation differentiable and enable overlapping clusters. We optimise \(=(_{}( ,))\) indirectly by optimising the GNN's parameters \(\), that is, its weights, with respect to the codelength \(L\). Here, \(_{n n}\) is the graph's adjacency matrix, \(_{n d}\) is the node features matrix, \(n=|V|\) is the number of nodes, \(s\) is the maximum allowed number of clusters, and \(d\) is the node feature dimension.

Without loss of generality, we assume directed networks. We denote the graph's total weight as \(w_{}=_{i V}_{j V}w_{ij}\). Let \(_{n n}\) be the random walker's transition matrix and \(^{}\) be the vector of weighted node in-degrees with

\[_{ij}=}{_{j V}w_{ij}}& _{j V}w_{ij}>0,\\ 0&^{}_{j}=_{i  V}w_{ij}.\]

To compute the vector \(\) of node visit rates, we use smart teleportation  and the power iteration method: With probability \(\), the random walker teleports to a random node, chosen proportionally to the nodes' in-degrees, or follows a link with probability \(1-\). This approach leads to the iterative update rule \(^{(t+1)}}}^{}+ (1-)^{(t)}\), and we set \(^{(0)}=^{}\). The graph's flow matrix \(_{n n}\) encodes the flow between each pair of nodes, where \(=}}+(1-) {diag}()\). We obtain the flow \(_{s s}\) between clusters from \(\) and \(\) as \(=^{}\). Following Equation (2), we define

\[q=1-()_{}= _{s}-() _{}=(_{s}^{})^{}- ()_{}=_{}+_{s}^{}\]

and assemble the map equation

\[L(,)=q_{2}q-(_{} _{2}_{})_{s}-(_{ }_{2}_{})_{s}-(_{ 2})_{n}+(_{}_{2}_{})_{s} \]

where \(_{k}\) is the \(k\)-dimensional vector of ones, and logarithms are applied component-wise. The third term is constant since it only depends on the node visit rates and can be omitted during optimisation.

The map equation naturally incorporates Occam's razor by following the MDL principle for balancing between model complexity and fit [14; 15], choosing the optimal number of communities automatically, but at most \(s\). In contrast, recent GNN-based clustering approaches require explicit regularisation to avoid over-partitioning [10; 36; 37; 38], and our results show that they often return the maximum allowed number of communities instead of determining the number of communities in a data-driven fashion (see Section 5). In principle, any neural network architecture, such as a multi-layer perceptron (MLP) or GNN, can be used to learn the soft cluster assignment matrix \(\). Since the map equation involves logarithms, we add a small constant \(\) to each value in the output \(\) before the backpropagation step to ensure differentiability. We refer to the combination of using map equation loss (Equation (3)) together with a (graph) neural network to learn (overlapping) communities as _Neuromap_.

Complexity and limitations.The most expensive calculation is the pooling operation \(=^{}\) which depends on the network's density. When \(s n\) and the number of edges is \(m=(n)\), the complexity of Neuromap is linear in \(n\). When the network is dense, \(m=(n^{2})\), or the maximum number of clusters approaches the number of nodes \(s n\), we approach quadratic complexity. Therefore, we recommend keeping \(s n\) for scalability.

We assume connected networks, otherwise, clustering should be run on the individual components. The node features \(\) aid the GNN in learning patterns, however, they do not contribute to the loss. When no node features are available, Neuromap can use, for example, the adjacency matrix as node features; designing expressive low-dimensional node features remains an active research area .

## 5 Experimental evaluation

We evaluate Neuromap on synthetic and real-world networks with different neural network architectures: a 2-layer graph convolutional network (GCN) , a 2-layer graph isomorphism network (GIN) , and a 2-layer SAGE network . To investigate whether GNNs are required for clustering, we also include a fully connected linear layer and a 2-layer MLP. We include a learnable temperature

Figure 2: Illustration of the setup for GNN-based community detection with the map equation. We learn soft cluster assignments \(\) from the graphs adjacency matrix \(\) and the node features \(\). Here, we allow up to four clusters. When no node features are available, we set \(=\).

parameter for the softmax operation, which we found speeds up convergence. In all cases, we use the models provided by PyTorch Geometric with SELU activation . Because the specifics between architectures differ, such as message-passing details and aggregation functions, they may be interpreted as using different search algorithms which return different communities. We use the Adam optimiser , apply batch normalisation, and for comparability between different methods, set the learning rate for the linear layer to \(10^{-1}\), for MLP to \(10^{-2}\), and for GCN, GIN, and SAGE to \(10^{-3}\). We train all models for up to \(10{,}000\) epochs with a patience of \(100\) epochs and dropout probability \(0.5\). Because the datasets contain hard clusters, we convert the resulting communities to hard clusters, assigning each node to that cluster where it has its strongest membership. As baselines, we use Infomap  and five recent approaches for unsupervised graph clustering with GNNs: DMoN , NOCD , DiffPool , MinCut , and Ortho . We base our implementation3 on PyTorch  and PyTorch Geometric  and ran our experiments on a workstation with an Intel i9-11900K @ 3.50GHz CPU, 32 GB of RAM, and a GeForce RTX 3090 with 24 GB of memory.

### Synthetic networks with planted communities

We generate directed and undirected Lancichinetti-Fortunato-Radicchi (LFR) benchmark networks with planted ground-truth communities  with \(n=1000\) nodes, average node degree \(k\{ n,2 n\}\), maximum node degree \(k_{}=2\), both rounded to the nearest integer, and mixing parameter \(\) between \(0.1\) and \(0.8\) with a step size of \(0.1\). We set the power-law exponents for the node degree distribution to \(_{1}=2\), and for the community size distribution to \(_{2}=1\). For each combination of parameters, we generate \(10\) LFR networks using the implementation provided by the authors,4 resulting in a total of 320 networks. For each parameter combination, there are 10 LFR networks; for each of these LFR networks, we run each model 10 times, measuring its performance as the average adjusted mutual information (AMI)  against the ground truth, and plot the average of those AMI values over the 10 networks per parameter combination. To verify that the number of communities is inferred from the data, we set the maximum number of communities to \(s=n\). Since LFR networks do not have node features, we use the adjacency matrix as node features.

Figure 3: Performance for Neuromap using a dense linear layer, MLP, GCN, GIN, and SAGE architectures with two layers and Infomap on directed and undirected LFR networks with planted communities. The results show averages of partition quality measured with AMI, number of detected communities \(||\), and codelength \(L\). The shaded areas show one standard deviation from the mean.

We find that the detected communities' quality depends on the choice of neural network architecture (Figure 3). Neuromap achieves the best AMI scores with SAGE. GCN, MLP, and Infomap perform slightly worse, however, with some variation depending on the networks' properties. The dense linear layer and GIN show weaker performance but still identify relevant communities. In the sparser directed networks, Infomap performs slightly better than SAGE when the mixing \(\) is low. However, the AMI values do not tell the whole story: Infomap and MLP tend to report considerably more communities than are present in the ground truth whereas the dense linear layer and GIN tend to report much fewer communities than the ground truth, especially for higher mixing values. GCN reports more communities than are present in the ground truth in the sparser undirected networks but fewer in the directed networks. SAGE detects close to the true number of communities in all cases. Infomap achieves the lowest codelength across all networks. GCN, MLP, and SAGE achieve close to Infomap's codelength, whereas the dense linear layer and GIN have slightly higher codelength.

We compare Neuromap against recent deep-learning-based community detection methods on the same networks by swapping out the loss function while keeping everything else the same, with the exception of using weight decay for NOCD as per the original paper . For DiffPool, Mincut, Ortho, and DMoN, we use the implementation from PyTorch Geometric, for NOCD, we use the implementation provided by the authors.5 Figure 4 shows the results for SAGE; in Appendix B, we also include results for the remaining architectures. Neuromap outperforms the baselines across all architectures, except for NOCD which performs better than Neuromap with GIN and with GCN on directed networks. Different from previous works, we have not limited the maximum number of communities, which allows us to analyse the methods' overfitting behaviour: While Neuromap reports close to the ground-truth number of communities, the remaining methods often overfit the networks' structure and report considerably more communities (note the logarithmic scale). MinCut fails to identify meaningful communities on directed networks for mixing values \(>0.3\). NOCD performs best with the GCN architecture, which was also used in the original paper . Neuromap performs best with SAGE in our experiments.

### Real-world networks with node features

We benchmark Neuromap on ten real-world datasets (Table 1) from PyTorch Geometric , PyTorch Geometric Signed Directed , and Open Graph Benchmark , and compare it against the same baselines as before. In contrast to previous works that choose a fixed number of hidden dimensions and set the maximum number of communities to a constant  or the "ground-truth" number of communities , we reflect the networks' sizes in our choices: We set the number of hidden dimensions to \(4\) and the maximum number of communities to \(s=\). Our choices are based on empirical observations showing that the number of communities typically scales as \(()\).

Figure 4: SAGE-based results for deep learning community-detection methods on synthetic LFR networks with planted communities. We show averages of partition quality measured by AMI and number of detected communities \(||\). The shaded areas show one standard deviation from the mean.

However, a few words of caution are in order: while nodes' true communities determine the link patterns in synthetic networks, it is generally infeasible to obtain ground truth communities for real networks. Often, metadata labels are used as a drop-in, and the inferred communities' quality depends on how well the metadata, which is potentially noisy, aligns with the unknown ground truth . Moreover, determining the number of communities in a network is hard and setting \(s=\) should be seen as a simplification rather than an attempt to guess the exact number.

For each method and architecture, we run 25 trials and show the average achieved AMI in Figure 5; we include a similar plot for the number of detected communities as well as the average AMI and the detected number of communities, both with standard deviations, in tabulated form in Appendix C. When several of the best-performing methods achieve similar AMI, we use an independent two-sample t-test to determine whether one of them can be considered to perform better than the other (see Appendix C). In cases where their performances do not differ significantly, we mark both as best.

Neuromap and NOCD are amongst the best performers in seven cases and DiffPool in two. The GCN architecture performs best in seven cases, MLP and SAGE in four cases, and GIN in two cases. A

   Dataset & Source & Type & \(|V|\) & \(|E|\) & \(|X|\) & \(|Y|\) & \(\) \\  Cora & PyG & Undirected & 2,708 & 5,278 & 1,433 & 7 & 0.19 \\ CiteSeer & PyG & Undirected & 3,327 & 4,614 & 3,703 & 6 & 0.26 \\ Pubmed & PyG & Undirected & 19,717 & 44,325 & 500 & 3 & 0.20 \\ Amazon Computer (PC) & PyG & Undirected & 13,752 & 143,604 & 767 & 10 & 0.22 \\ Amazon Photo & PyG & Undirected & 7,650 & 71,831 & 745 & 8 & 0.17 \\ Coauthor CS & PyG & Undirected & 18,333 & 81,894 & 6,805 & 15 & 0.19 \\ Coauthor Physics & PyG & Undirected & 34,493 & 247,962 & 8,415 & 5 & 0.07 \\ Cora ML & PyG-SD & Directed & 2,995 & 8,416 & 2,879 & 7 & 0.21 \\ Wiki CS & PyG-SD & Directed & 11,701 & 297,110 & 300 & 10 & 0.31 \\ ogb-arxiv & OGB & Directed & 169,343 & 583,121 & 128 & 40 & 0.35 \\   

Table 1: Properties of the real-world datasets obtained from PyTorch Geometric (PyG) , PyTorch Geometric Signed Directed (PyG-SD) , and Open Graph Benchmark (OGB) . \(|V|\) is the number of nodes, \(|E|\) the number of edges, \(|X|\) the node feature dimension, \(|Y|\) the number of communities, and \(\) the mixing for the given communities.

Figure 5: Average achieved AMI on real-world networks (higher is better) with \(s=\). Colours indicate methods while shapes indicate neural network architectures. DiffPool ran out of memory on the ogb-arxiv dataset. Detailed tabulated results with standard deviations are included in Appendix C.

possible explanation for why the simpler linear layer and MLP architectures perform well in several cases could be that the map equation loss function captures global information in the random walker's flow patterns, making GNNs superfluous in some cases. All methods tend to detect more communities than are present in the "ground truth", however, this tendency is most pronounced in DMoN, Ortho, and MinCut, which may have gone unnoticed in previous evaluations where the maximum number of communities was set to a much lower, constant value , thus artificially preventing overfitting. Infomap is the only baseline that does not utilise node features; instead, it relies solely on topological information, which may explain the large number of detected communities. Comparing Neuromap's performance against Infomap's performance suggests that incorporating node features substantially improves the detected communities' quality in most cases (see Appendix C for significance tests) while drastically reducing the number of detected communities.

We repeat the same experiments with 512 hidden features and \(s=|Y|\), that is, the "true" number of communities, following  (results in Appendix D). Limiting the number of allowed communities often leads to better performance for DMoN, MinCut, and Ortho, however, with a few exceptions, it diminishes the performance of Neuromap, NOCD, and DiffPool across all datasets and neural network architectures. Appendix E tabulates the differences in average AMI score between setting the hidden features to \(4\) and \(s=\) versus using \(512\) hidden features and \(s=|Y|\). In the case of Neuromap, imposing a lower bound on the number of communities interferes with the MDL principle, limiting what models for the data may be explored.

### Synthetic networks with overlapping communities

We apply Neuromap and the baselines to a small synthetic network with overlapping communities . We set the maximum number of communities to \(s\{2,3\}\), run each combination of loss function and neural network architecture for 10 trials, and keep the solutions with the lowest loss. Figure 6 shows the results obtained with GCN, Appendix F shows results for the remaining architectures.

We find that Neuromap, DMoN, NOCD, and MinCut identify the correct communities for \(s=2\). DiffPool does not detect overlapping communities and Ortho assigns each node to two communities. For \(s=3\), only Neuromap identifies the correct communities. DiffPool returns the same communities as for \(s=2\). All remaining methods return three communities. These results provide further evidence that the baselines suffer from overfitting when they are not provided with the correct number of communities, which, in general, is unknown. Neuromap identifies meaningful communities while inferring the number of communities in a data-driven fashion by following the MDL principle. However, we leave a more rigorous study of overlapping communities for future work.

Figure 6: Synthetic network with overlapping communities where the leftmost network shows the true community structure. Nodes are drawn as pie charts to visualise their community assignments. The top and bottom rows show results for a maximum of \(s=2\) and \(s=3\) communities, respectively.

Conclusion

Network science and deep learning on graphs tackle community detection from different perspectives. Community detection in network science typically relies on custom heuristic optimisation algorithms to optimise objective functions but often does not leverage recent deep learning advances. Recently, deep graph learning methods have started to incorporate methods from network science for deep graph clustering. We contribute to this young field by adapting the map equation, a popular unsupervised information-theoretic community-detection approach, as a differentiable loss function for end-to-end optimisation with (graph) neural networks through gradient descent, and use PyTorch to implement our approach, which we call Neuromap.

We evaluated Neuromap on various synthetic and real-world datasets, using different neural network architectures to detect communities. Our results show that Neuromap achieves competitive performance and detects close to the ground-truth number of communities across datasets while the baselines tend to overfit and report considerably more communities. Across all tested methods, the achieved performance depends on the used neural network architecture. However, on several real-world benchmarks, Neuromap outperforms several of the the baselines even with simpler, non-GNN, neural network architectures. We hypothesise that this may be because the map equation builds on capturing flow patterns, which contain global information.

While we have considered first-order networks with two-level community structures, complex real-world networks often involve higher-order dependencies and can have multi-level communities , prompting a generalisation of our approach. Furthermore, incorporating our method for graph pooling as well as uncovering the precise connection between the utilised neural network architecture and the achieved community-detection performance requires further empirical and theoretical studies.