# Maximum Average Randomly Sampled: A Scale Free and Non-parametric Algorithm for Stochastic Bandits

Masoud Moravej Khorasani

Department of Electrical and Electronic Engineering

The University of Melbourne

Parkville, Melbourne, Victoria, Australia

m.moravej.kh@gmail.com

&Erik Weyer

Department of Electrical and Electronic Engineering

The University of Melbourne

Parkville, Melbourne, Victoria, Australia

ewey@unimelb.edu.au

The code to reproduce the results of the paper can be found at https://github.com/Masoud-Moravej/MARS-NeurIPS2023.

###### Abstract

Upper Confidence Bound (UCB) methods are one of the most effective methods in dealing with the exploration-exploitation trade-off in online decision-making problems. The confidence bounds utilized in UCB methods tend to be constructed based on concentration equalities which are usually dependent on a parameter of scale (e.g. a bound on the payoffs, a variance, or a subgaussian parameter) that must be known in advance. The necessity of knowing a scale parameter a priori and the fact that the confidence bounds only use the tail information can deteriorate the performance of the UCB methods. Here we propose a data-dependent UCB algorithm called MARS (**M**aximum **A**verage of **R**andomly **S**ampled **R**ewards) in a non-parametric setup for multi-armed bandits with symmetric rewards. The algorithm does not depend on any scaling, and the data-dependent upper confidence bound is constructed based on the maximum average of randomly sampled rewards inspired by the work of Hartigan in the 1960s and 70s. A regret bound for the multi-armed bandit problem is derived under the same assumptions as for the \(\)-UCB method without incorporating any correction factors. The method is illustrated and compared with baseline algorithms in numerical experiments.

## 1 Introduction

The classical stochastic multi-armed bandit problem introduced by  has been studied extensively in statistics, computer science, electrical engineering, and economics. New applications in machine learning and data science such as personalized new recommendation and Monte-Carlo tree search have generated renewed interest in the problem.

The problem is described as follows: The classical stochastic multi-armed bandit problem is an online decision-making problem where each arm represents an action. At each round \(t\{1,,n\}\) the agent chooses an action (arm) \(A_{t}\) and observe a reward \(X_{t}\) where \(\) is the set of available actions and its finite cardinality is \(K 2\). The reward \(X_{t}\) is a sample from an underlying distribution\(_{A_{i}}\) with bounded mean \(_{A_{i}}\). Hence, a stochastic bandit is a set of distributions \(=\{_{i}:i\}\) which is unknown for the agent.

The agent aims to minimize the following regret over \(n\) rounds.

\[R_{n}=n_{i}_{i}-[_{t=1}^{n}X_{t}].\] (1)

with respect to the actions \(A_{1},,A_{n}\).

The minimization of the regret necessitates balancing the trade-off between exploration and exploitation where exploration means investigation to accumulate more information about arms and exploitation means the maximization of the immediate performance. See the book  and survey  for comprehensive discussion.

A class of algorithms for multi-armed bandit problems that strikes a balance between exploration and exploitation is Upper Confidence Bound (UCB) methods [3; 4; 12; 1].

The UCB algorithm is based on _the principle of optimism in the face of uncertainty_. According to this principle, one should construct probabilistically optimistic guesses for the expected payoff (mean) of all actions and play the arm with the highest guess. The performance of the UCB algorithms in dealing with the exploration-exploitation dilemma depends on the tightness of the upper confidence bound. The literature includes several approaches which construct upper confidence bound based on concentration inequalities such as Hoeffding-type inequalities  and self-normalized inequalities .

As an example \(\)-UCB Method introduced in  assumes that there exist a convex function \(()\) which satisfy the following conditions.

\[ t, 0,\ \ \{e^{(X_{t}-\{X_{t }\})}\}()\{e^{(\{X_{t}\}-X_{t })}\}().\] (2)

Let \(^{*}()\) be the Legendre-Fenchel transform of \(()\), that is

\[^{*}()_{}( -()),\]

Then, with probability at least \(1-\),

\[_{i,s}+(^{*})^{-1}(( ))>_{i}\]

where \(_{i,s}\) is the sample mean of rewards obtained by pulling arm \(i\) for \(s\) times.

At time \(t\), using the obtained UCB and letting \(=1/t^{2}\), we select

\[A_{t}=_{i}[_{i,T_{i}(t-1)}+(^{*})^{-1}( (t-1)})]\]

where \(T_{i}(t-1)\) denotes the number of times action \(i\) was chosen by the learner after the end of round \(t-1\). It was proved  that:

\[R_{n}_{i:_{i}>0}(}{^{*}(_{i}/2)} (n)+3)\] (3)

where \(_{i}\) is the suboptimality gap \(_{i}=_{i}_{i}-_{i}\).

However, there are two weaknesses associated with the bounds obtained by concentration inequalities as above. First, they are commonly conservative since they are not data-dependent and only care about tail information of distribution. Second, those bounds usually depend on scaling factors or functions, e.g. \(()\) in \(()\)-UCB and poor choices can make the bounds conservative.

### Related Works

Recently several works have focused on non-parametric bandit algorithms based on subsampling and bootstrapping [5; 6; 18; 17; 22]. Those works use the empirical distribution of the data instead of fitting a given model to the data. The Garbage In, Reward Out (GIRO) method  relies on the history of past observed rewards and enhances its regret bound by augmenting fake samples into the history. The Perturbed-History Exploration method (PHE)  serves as a faster and memory-efficient alternative to GIRO. However, PHE has the limitation of being restricted to bounded distributions and involves a tunable parameter. The Residual Bootstrap exploration (ReBoot)  perturbs the history in order to improve the regret bound.

Bootstrapping Upper Confidence Bound uses bootstrap to construct sharper confidence intervals in UCB-type algorithm . However, a second-order correction is used to guarantee the non-asymptotic validity of the bootstrap threshold. The second-order correction is not sharp, and it includes scaling factors.

Another line of works which include the Best Empirical Sampled Average method (BESA)  and the Sub-sampling Duelling Algorithms (SDA)  use subsampling to conduct pairwise comparison (duels) between arms. BESA organize the duels between arms and find the winner by comparing the empirical average of sub-sampled rewards. SDA extends the concept of BESA duels to a round-based comparison by incorporating a sub-sampling scheme and it eliminates the need for forced exploration.

Apart from Reboot which was analysed for Gaussian distributions, and SDA which was analysed for a family of distribution satisfying a balance condition (including Gaussian and Poisson), the other algorithms were analysed for distributions with known bounded support.

### Contributions

In this paper, we propose a new non-parametric UCB algorithm which is fully data-dependent without any scale parameter. The proposed approach called MARS (Maximum Average **R**andomly **S**ampled) constructs a data-dependent upper confidence bound by selecting the maximum average among randomly sampled rewards. This upper confidence bound is inspired by Leave-out Sign-dominant Correlation Regions (LSCR) method which construct non-asymptotic confidence regions for parameters of dynamical systems [8; 9; 11; 15; 20; 16]. The upper confidence bound inspired by LSCR is _non-asymptotic_ and _probabilistically guaranteed_, i.e. it is larger than the true value with an exact user-chosen probability and for a finite number of data points.

The problem-dependent regret bound of the method is given for multi-armed bandits where all underlying distributions \(\{_{i},i\}\) are symmetrically distributed about their mean e.g. exponential, uniform, or logistic distribution and inequality (2) is satisfied. As \(()\) does not need to be known, MARS avoids the problem with sub-optimal choice of \(()\), and still achieves logarithmic regret.

### Notation

A subset of positive integers up to and including a constant \(k\) is denoted as \([k]\{1,,k\}\). \( x\) is the smallest integer greater than or equal to \(x\). The cardinality of a set \(S\) is denoted by \(|S|\), e.g. \(|[k]|=k\). \(\){A predicate} is the indicator function defined as

\[\{\}=1&,\\ 0&,\]

Without loss of generality assume the first arm is optimal i.e. \(_{1}=_{i}_{i}\). Define the sub-optimality gap \(_{i}=_{1}-_{i}\) for \(i\{2,,K\}\). Further, let

\[T_{i}(t)=_{s=1}^{t}\{A_{s}=i\}\]

be the number of times action \(i\) was chosen by the learner after the end of round \(t\). \(X_{i,j}\) is the reward the \(j\)th time arm \(i\) is pulled. \((a,b)\) is a uniform distribution bounded by \(a\) and \(b\) (\(a<b\)), and \((a)\) is a Bernoulli distribution with mean \(a\).

The remainder of the paper is organized as follows. In the next section, a new randomized approach to compute an upper confidence bound is presented. In Section 3 the non-parametric UCB algorithm is introduced and a regret bound for the multi-armed bandit problem with symmetric rewards is given. Section 4 explores the numerical performance of the proposed MARS method, and Section 5 concludes the paper.

A New Randomized Upper Confidence Bound

In this section, an approach to compute a data-dependent upper confidence bound for a finite number of observations is proposed.

Hartigan in  proposed an approach which is useful to compute confidence interval for an unknown parameter. The confidence interval was computed by a number of estimates of the mean using different subsets of the data. The estimates of the mean in  was computed using a number of balanced sets of subsamples determined by group theory.

The paper  used random strings instead of balanced sets of subsamples which brings computational advantage. Now, we use the method in  and  to propose a new non-asymptotic upper confidence bound.

We have the observations \(\{X_{i,s}\}_{s=1}^{T_{i}}\) from arm \(i\) and we want to find an upper confidence bound \(_{i}\) with probability \(1-\) such that

\[\{_{i}(T_{i},)_{i}\} 1-\]

In this section \(T_{i}\) is assumed to be a fixed number of observations which does not depend on other actions taken or observed rewards. In Section 3, the situation where \(T_{i}\) is dependent on the past actions and observed rewards will be considered.

Before introducing the new upper confidence bound, let us define typical values.

**Definition 1**.: _The set of ascending-ordered and continuously distributed random variables \(Z_{(1)},Z_{(2)},,Z_{(M-1)}\) is a set of typical values for \(\) if the probabilities that \(\) belongs to each of following intervals are the same_

\[(-,Z_{(1)}),(Z_{(1)},Z_{(2)}),,(Z_{(M- 1)},),\]

_that is_

\[\{(-,Z_{(1)})\}=\{(Z_ {(1)},Z_{(2)})\}==\{(Z_{(M-1)}, )\}=.\]

Now, based on Theorem 1 in  the following theorem introduces a set of typical values for the means of \(i\)th arm \(_{i}\).

**Theorem 1**.: _We have the observations \(\{X_{i,s}\}_{s=1}^{T_{i}}\) from arm \(i\) and \(\{X_{i,s}\}=_{i}\) for all \(s[T_{i}]\). Assume that the observations \(\{X_{i,s}\}\) admit continuous distributions and are independent and symmetrically distributed about their mean._

_Then, a set of typical values for \(_{i}\) is given by_

\[_{i}^{j}=^{T_{i}}h_{j,s}}_{s=1}^{T_{i}}h_{j,s} X_{i,s}j[M-1]\] (4)

_where \(M 2^{T_{i}}\) is a positive integer and \(\{h_{j,t}\}\) are independent random sequences such that_

\[\{h_{j,s}=0\}=\{h_{j,s}=1\}=.\]

_A string \(\{h_{j,s}\}_{s=1}^{N}\) is discarded if it turns out to be equal to an already constructed string._

Proof.: Follows along the same line as the Proof of Theorem 1 in . See Section 1 in the supplement for further details. 

Theorem 1 shows that \(M-1\) estimates of mean computed by random sub-sampling are a set of typical values for \(_{i}\) and partition the real line into equiprobable segments where \(_{i}\) belong to each one of those segments with equal probability. As a consequence of Theorem 1 we conclude that

\[\{_{j[M-1]}_{i}^{j}>_{i}\}=1-1/M.\]

\(M\) is a user-chosen parameter and hence the probability of the confidence bound can be tuned by the user. Since \(M 2^{T_{i}}\) the maximum confidence which can be achieved by computing subsample means is \(1-1/2^{T_{i}}\) and we can not get an upper confidence bound with arbitrary high confidence. To overcome this problem, we use the fact that all means \(_{i}\) are bounded and the following upper confidence bound is proposed.

Compute \(M=^{-1}\) random subsample means as in (4). The non-parametric upper confidence bound is given in Table 1.

The probability of the constructed upper confidence bound is presented in the next theorem.

**Theorem 2**.: _Let \(T_{i}\) be a fixed number of observations from arm \(i\). For each \(i[K]\) assume \(\{X_{i,s}\}_{s=1}^{T_{i}}\) are independent random variables with continuous distribution functions which are symmetrically distributed about its mean \(_{i}\). Let the user-chosen parameter \((0,1)\) be chosen such that \(^{-1}\) is an integer. Then for all \(T_{i}>0\) the following non-asymptotic equality holds_

\[\{_{i}(T_{i},)_{i}\}=1-\] (5)

_Note that \(T_{i}\) is deterministic, and the probability is with respect to both \(\{X_{i,s}\}\) and \(\{h_{j,s}\}\)._

Proof.: See Section 2 in the supplement. 

Theorem 2 shows that the probability of the proposed confidence region is exact and hence it is not conservative. This result holds for a finite number of observations as long as the reward is symmetrically distributed around its mean and \(T_{i}\) is a fixed given number.

Example 1In Figure 1, we compare different approaches to calculate 95% UCB for the population mean based on samples from a Gaussian and an uniform distribution. The upper confidence bounds are calculated for different number of data points. The almost exact quantile is computed by finding 95% quantile of the population of sample means available by repeating experiment 10000 times. Naive bootstrap threshold and bootstrapped threshold were computed by equation (2.6) in Remark 2.3 in .

The figures shows that for most sample sizes the MARS bound is sharper than naive bootstrapped threshold, bootstrapped threshold, sub-Gaussian bound, and Hoeffding bound. The naive bootstrap is sharper for small sample sizes, but as shown in , naive bootstrap do not produce reliable results for small samples sizes. It is also notable that the MARS bound is probabilistically guaranteed for a finite number of data points as shown in Theorem 2.

## 3 Algorithm and Regret Bound

The UCB method using the proposed upper confidence bound for \(n\) rounds is given in Algorithm 1. At the initial rounds, the algorithm tends to adopt an optimistic outlook in the face of uncertainty. This is reflected in the upper confidence bound, which is set to infinity or the highest reward observed. As the number of rounds increases, the upper confidence bounds approach the mean reward for each arm with high probability. Consequently, the algorithm progressively leans towards exploitation.

If two or more UCBs in line 15 of Algorithm 1 take on the same value, randomly choose an action among the minimizers.

MARS necessitates keeping \( 1/\) sub-sampled means for each arm. This leads to a memory requirement of \(O(Kn)\) when \(=1/n\). The computational complexity of MARS is also depends

   UCB\({}_{i}(T_{i},)=\{_{s}X_{i,s}& 2^{T_{i}}\\ &1- 2^{T_{i}}\\ _{j}_{i}^{j}(t),&\\   .\),  if \(T_{i}<)}{(2)}\) \\   

Table 1: Data-dependent and scale-free UCB.

on the choice of \(\), as updating \( 1/\) sub-sampled means is performed in each round. Notably, to reduce the computational burden, the required Bernoulli variables in the algorithm can be generated and stored before the start of the game.

Figure 1: 95% UCBs of sample means computed by different approaches. The underlying distributions are uniform between \(-1\) and \(+1\) in (a) and Gaussian with mean \(0\) and variance \(1\) in (b). The results are averaged over 10000 realisations.

The Next theorem provides a regret bound for the proposed MARS algorithm when applied to multi-armed bandit problems with symmetric distributions.

**Theorem 3**.: _Let \(T_{i}(n)>0\) be the total number of observation from arm \(i\) and \(=1/n^{2}\). The following assumptions about all arms \(i[K]\) are in place._

* \(\{X_{i,s}\}_{s=1}^{T_{i}(n)}\) _are continuously distributed and independent random variables symmetrically distributed about its mean_ \(_{i}\)_. The observations_ \(X_{i,s}\) _for each arm_ \(i[K]\) _are independent of the observations from other arms and actions._
* _For all_ \(i[K]\) _and_ \(s[T_{i}(n)]\)_, there is a convex function_ \(_{i}()\) _such that:_ \[\{e^{(X_{i,s}-\{X_{i,s}\})}\}_{i}()\]

_Then the regret is bounded by_

\[R(n)_{i:_{i}>0}(3+})_{i},\] (6)

_where_

\[x_{i}=-(+(-_{i}^{*}(_{i}))),\]

_and \(_{i}^{*}()\) is the Legendre-Fenchel transform of \(_{i}()\), that is_

\[_{i}^{*}()_{}( -_{i}()).\]

_Note that the expectation in the regret is with respect to both \(\{X_{i,s}\}\) and \(\{h_{j,s}\}\)._

Proof.: See Section 3 in the supplement. 

In Theorem 3 we set \(=1/n^{2}\) to simplify analysis. A similar regret bound can also be achieved using \(_{t}=1/t^{2}\). It is also notable that the regret bound in Theorem 3 was achieved without any knowledge about the functions \(_{i}()\) apart from its existence. In particular, it was not used when the upper confidence bounds were computed. However, the \(\)-UCB method assumed that \(_{i}()\)s are known and used it in constructing the upper confidence bound.

The next corollary characterises a relationship between the regret bounds of the current method and those of the \(\)-UCB method.

**Corollary 1**.: _Provided that \(0<_{i}^{*}(_{i})<1.59\), the regret bound (6) is simplified as follows_

\[R(n)_{i:_{i}>0}(3+^{*}(_{i}) })_{i}.\] (7)

_Now, let for all arms \(i[K]\), the corresponding reward is \(_{i}\)-sub Gaussian. Then, \(_{i}^{*}(_{i})=(_{i}^{2})/(2_{i}^{2})\). Accordingly when \((_{i}^{2})/(2_{i}^{2})<1.59\) the regret bound is simplified to_

\[R(n)_{i:_{i}>0}(3_{i}+(n)}{ _{i}}).\] (8)

Proof.: See Section 4 in the supplement. 

As shown in equation (6), the regret bound for MARS is always \(O((n))\) without relying on the use of \(()\). This demonstrates the effectiveness of the introduced non-parametric UCB method. When \(_{i}^{*}(_{i})<1.59\), the task becomes more challenging as identifying the optimal arms becomes harder. In such scenarios, both the regret bounds for the proposed MARS and the \(\)-UCB, which employs \(()\), become dependent on the function \((n)/_{i}^{*}(_{i})\). Corollary 1 also explore the effectiveness of MARS when dealing with subgaussian rewards. It demonstrates that even without prior knowledge of the \(_{i}\) values, MARS successfully addresses bandit problems, achieving a regret bound of \(O(_{i:_{i}>0}(n)/_{i})\) for challenging scenarios where \((_{i}^{2})/(2_{i}^{2})<1.59\).

Experiments

In this section, the performance of the proposed MARS method is compared with upper confidence bound based on concentration inequalities (Vanilla UCB, See e.g. Chapter 7 and 8 in ), Thompson sampling with normal prior (Normal-TS) , Bootstrapped UCB , Garbage In, Reward Out (GIRO) , and Perturbed-History Exploration (PHE) . The package PymaBandits2 was used to implement these baseline methods.

The probability of confidence regions in the Vanilla UCB, Bootstrapped UCB, and the proposed MARS UCB is set to \(0.999\), i.e. \(=1/1000\). In GIRO, the parameter \(a\), which represents pseudo-rewards per unit of history, is set to \(1\). Due to the high sensitivity of the PHE approach to the choice of the tunable parameter \(a\), simulations were performed for two values of \(a\).

The number of arms is \(K=5\) and the means are

\[_{1}=1,\ _{2}=,\ _{3}=,\ _{4}=,\ _{5}=.\]

First, consider the case where the rewards are Gaussian with variance \(1\) for all arms. The cumulative regrets are show in Figure 1(a). Since Normal-TS uses distribution knowledge and the variances are correct it is the best as expected. The Vanilla UCB algorithm demonstrates comparable or superior performance compared to both the Bootstrapped UCB, MARS, and GIRO. The performance of the PHE approach is heavily dependent on the parameter \(a\). When \(a=2.1\), it shows a linear regret. However, for \(a=5.1\), it outperforms most other approaches, except for Thompson sampling.

Both Vanilla UCB and Normal-TS depend on the variances which were assumed known in the previous simulation. We repeat the simulation with the variances incorrectly set to 2. The result is shown in Figure 1(b). Evidently MARS, GIRO, and Bootstrapped UCB outperform both Vanilla UCB and Normal-TS when incorrect values for the variances are used. MARS demonstrates superior performance over Bootstrapped UCB and GIRO after an initial set of rounds. Moreover, unlike GIRO and Bootstrapped UCB, MARS does not require the full storage of reward history, resulting in lower computational complexity. As previously observed, PHE with a value of \(2.1\) demonstrated the poorest performance, whereas PHE with a value of \(5.1\) has the best performance.

The MARS and GIRO do not use the tail information of the rewards. However, Vanilla UCB, Normal-TS, and Bootstrapped UCB use the distribution and the tail information of the rewards respectively and their performance can deteriorate when the prior knowledge is wrong or conservative. To illustrate this we repeated the simulation for the cases where the rewards admit uniform distribution over \([-1,1]\). The results are shown in Figure 3. It shows that MARS which does not use the distribution of the rewards and the tail information, outperforms the other methods except PHE (\(a=2.1\)) in this case. An interesting observation is that PHE (\(a=5.1\)) exhibits outstanding performance in a Gaussian setup, yet it performs poorly in a Uniform setup, indicating a strong reliance on the tunable

Figure 2: Cumulative regret for Gaussian bandit. The variances of rewards are true in (a) and wrong in (b). The results are averaged over 2000 realizations.

parameter. This dependence on the parameter could pose challenges in real-world applications where the environment is unknown. In such cases methods like MARS and GIRO may be more practical alternatives.

## 5 Conclusion

In this paper, a non-parametric and scale-free confidence bound was proposed based on random sub-sampling of rewards. The confidence bound was used to propose a data-driven UCB algorithm. A regret bound of the method for multi-armed bandit problems with symmetric distribution was presented, and it was shown that the regret bound is always \(O((n))\). Moreover, the regret bound is \(O(_{i:_{i}>0}(n)/_{i})\) for a class of Bandit problems. Numerical experiments show that the proposed method performs well, and it outperforms baselines algorithms when the prior information is mis-specified.

Extending the method to other bandit and reinforcement learning problems are interesting topics for future works e.g. evaluating the method on non-stationary bandits and contextual bandits using the ideas in  and . As MARS does not use any concentration inequalities in the construction of the confidence bound, the application of MARS to bandits with heavy tail is another interesting direction. Another interesting future direction is evaluation of the method on asymmetric rewards and robustification of the approach following ideas in .