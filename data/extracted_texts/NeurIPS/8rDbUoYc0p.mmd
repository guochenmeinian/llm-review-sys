# Constant Approximation for Individual Preference Stable Clustering

Anders Aamand

MIT

aamand@mit.edu

Justin Y. Chen

MIT

justc@mit.edu

&Allen Li

MIT

cliu568@mit.edu

&Sandeep Silwal

MIT

silwal@mit.edu

Pattara Sukprasert

Databricks

pat.sukprasert@databricks.com

&Ali Vakilian

TTIC

vakilian@ttic.edu

&Fred Zhang

UC Berkeley

zo@berkeley.edu

Significant part of the work was done while P.S. was a Ph.D. candidate at Northwestern University.

###### Abstract

Individual preference (IP) stability, introduced by Ahmadi et al. , is a natural clustering objective inspired by stability and fairness constraints. A clustering is \(\)-IP stable if the average distance of every data point to its own cluster is at most \(\) times the average distance to any other cluster. Unfortunately, determining if a dataset admits a \(1\)-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an \(o(n)\)-IP stable clustering always _exists_, as the prior state of the art only guaranteed an \(O(n)\)-IP stable clustering. We close this gap in understanding and show that an \(O(1)\)-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient near optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters.

## 1 Introduction

In applications involving and affecting people, socioeconomic concepts such as game theory, stability, and fairness are important considerations in algorithm design. Within this context, Ahmadi et al.  (ICML 2022) introduced the notion of _individual preference stability (IP stability)_ for clustering. At a high-level, a clustering of an input dataset is called \(1\)-IP stable if, for each individual point, its average distance to any other cluster is larger than the average distance to its own cluster. Intuitively, each individual prefers its own cluster to any other, and so the clustering is stable.

There are plenty of applications of clustering in which the utility of each individual in any cluster is determined according to the other individuals who belong to the same cluster. For example, in designing _personalized medicine_, the more similar the individuals in each cluster are, the more effective medical decisions, interventions, and treatments can be made for each group of patients. Similarly, stability guarantees are desired in designing personalized learning environments or marketing campaigns to ensure that no individual wants to deviate from their assigned cluster. Furthermore the focus on individual utility in IP stability (a clustering is only stable if every individual is "happy") enforces a notion of individual fairness in clustering.

In addition to its natural connections to cluster stability, algorithmic fairness, and Nash equilibria, IP stability is also algorithmically interesting in its own right. While clustering is well-studied with respect to global objective functions (e.g. the objectives of centroid-based clustering such as \(k\)-meansor correlation/hierarchical clustering), less is known when the goal is to partition the dataset such that every point in the dataset is individually satisfied with the solution. Thus, IP stability also serves as a natural and motivated model of individual preferences in clustering.

### Problem Statement and Preliminaries

The main objective of our clustering algorithms is to achieve IP stability given a set \(P\) of \(n\) points lying in a metric space \((M,d)\) and \(k\), the number of clusters.

**Definition 1.1** (Individual Preference (IP) Stability ).: The goal is to find a disjoint \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\) such that every point, _on average_, is closer to the points of its own cluster than to the points in any other cluster. Formally, for all \(v P\), let \(C(v)\) denote the cluster that contains \(v\). We say that \(v P\) is IP stable with respect to \(\) if either \(C(v)=\{v\}\) or for every \(C^{}\) with \(C^{} C\),

\[_{u C(v)}d(v,u)|}_{u C ^{}}d(v,u).\] (1)

The clustering \(\) is 1-IP stable (or simply IP stable) if and only if every \(v P\) is stable with respect to \(\).

Ahmadi et al.  showed that an arbitrary dataset may not admit an IP stable clustering. This can be the case even when \(n=4\). Furthermore, they proved that it is NP-hard to decide whether a given a set of points have an IP stable \(k\)-clustering, even for \(k=2\). This naturally motivates the study of the relaxations of IP stability.

**Definition 1.2** (Approximate IP Stability).: A \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\) is \(\)-approximate IP stable, or simply \(\)-IP stable, if for every point \(v P\), the following holds: either \(C(v)=\{v\}\) or for every \(C^{}\) and \(C^{} C\),

\[_{u C(v)}d(v,u)|}_{ u C^{}}d(v,u).\] (2)

The work of  proposed algorithms to outputting IP stable clusterings on the one-dimensional line for any value of \(k\) and on tree metrics for \(k=2\). The first result implies an \(O(n)\)-IP stable clustering for general metrics, by applying a standard \(O(n)\)-distortion embedding to one-dimensional Euclidean space. In addition, they give a bicriteria approximation that discards an \(\)-fraction of the input points and outputs a \(O(n}{})\)-IP stable clustering for the remaining points.

Given the prior results, it is natural to ask if the \(O(n)\) factor for IP stable clustering given in  can be improved.

### Our Results

New Approximations.Improving on the \(O(n)\)-IP stable algorithm in , we present a deterministic algorithm which for general metrics obtains an \(O(1)\)-IP stable \(k\)-clustering, for any value of \(k\). Note that given the existence of instances without \(1\)-IP stable clusterings, our approximation factor is optimal up to a constant factor.

**Theorem 1.3**.: _(Informal; see Theorem 3.1) Given a set \(P\) of \(n\) points in a metric space \((M,d)\) and a number of desired clusters \(k n\), there exists an algorithm that computes an \(O(1)\)-IP stable \(k\)-clustering of \(P\) in polynomial time._

Our algorithm outputs a clustering with an even stronger guarantee that we call uniform (approximate) IP stability. Specifically, for some global parameter \(r\) and for every point \(v P\), the average distance from \(v\) to points in its own cluster is upper bounded by \(O(r)\) and the average distance from \(v\) to points in any other cluster is lower bounded by \((r)\). Note that the general condition of \(O(1)\)-IP stability would allow for a different value of \(r\) for each \(v\).

We again emphasize that Theorem 1.3 implies that an \(O(1)\)-IP stable clustering always exists, where prior to this work, only the \(O(n)\) bound from  was known for general metrics.

Additional \(k\)-center clustering guarantee.The clustering outputted by our algorithm satisfies additional desirable properties beyond \(O(1)\)-IP stability. In the \(k\)-center problem, we are given \(n\) points in a metric space, and our goal is to pick \(k\) centers as to minimize the maximal distance of any point to the nearest center. The clustering outputted by our algorithm from Theorem 1.3 has the added benefit of being a constant factor approximation to the \(k\)-center problem in the sense that if the optimal \(k\)-center solution has value \(r_{0}\), then the diameter of each cluster outputted by the algorithm is \(O(r_{0})\). In fact, we argue that IP stability is more meaningful when we also seek a solution that optimizes some clustering objective. If we only ask for IP stability, there are instances where it is easy to obtain \(O(1)\)-IP stable clusterings, but where such clusterings do not provide insightful information in a typical clustering application. Indeed, as we will show in Appendix B, randomly \(k\)-coloring the nodes of an unweighted, undirected graph (where the distance between two nodes is the number of edges on the shortest path between them), gives an \(O(1)\)-IP stable clustering when \(k O(}{ n})\). Our result on trees demonstrates the idiosyncrasies of individual objectives thus our work raises further interesting questions about studying standard global clustering objectives under the restriction that the solutions are also (approximately) IP stable.

Max and Min-IP Stability.Lastly, we introduce a notion of \(f\)-IP stability, generalizing IP stability.

**Definition 1.4** (\(f\)-IP Stability).: Let \((M,d)\) be a metric space, \(P\) a set of \(n\) points of \(M\), and \(k\) the desired number of partitions. Let \(f:P 2^{P}^{ 0}\) be a function which takes in a point \(v P\), a subset \(C\) of \(P\), and outputs a non-negative real number. we say that a \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\) is \(f\)-IP stable if for every point \(v P\), the following holds: either \(C(v)=\{v\}\) or for every \(C^{}\) and \(C^{} C\),

\[f(v,C(v)\{v\}) f(v,C^{}).\] (3)

Note that the standard setting of IP stability given in Definition 1.1 corresponds to the case where \(f(v,C)=(1/|C|)_{v^{} C}d(v,v^{})\). The formulation of \(f\)-IP stability, therefore, extends IP stability beyond average distances and allows for alternative objectives that may be more desirable in certain settings. For instance, in hierarchical clustering, average, minimum, and maximum distance measures are well-studied.

In particular, we focus on max-distance and min-distance in the definition of \(f\)-IP stable clustering in addition to average distance (which is just Definition 1.1), where \(f(v,C)=_{v^{} C}d(v,v^{})\) and \(f(v,C)=_{v^{} C}d(v,v^{})\). We show that in both the max and min distance formulations, we can solve the corresponding \(f\)-IP stable clustering (nearly) optimally in polynomial time. We provide the following result:

**Theorem 1.5** (Informal; see Theorem 4.1 and Theorem 4.2).: _In any metric space, Min-IP stable clustering can be solved optimally and Max-IP stable clustering can be solved approximately within a factor of \(3\), in polynomial time._

We show that the standard greedy algorithm of \(k\)-center, a.k.a, the Gonzalez's algorithm , yields a \(3\)-approximate Max-IP stable clustering. Moreover, we present a conceptually clean algorithm which is motivated by considering the minimum spanning tree (MST) to output a Min-IP stable clustering. This implies that unlike the average distance formulation of IP stable clustering, a Min-IP stable clustering always exists. Both algorithms work in general metrics.

Empirical Evaluations.We experimentally evaluate our \(O(1)\)-IP stable clustering algorithm against \(k\)-means++, which is the empirically best-known algorithm in . We also compare \(k\)-means++ with our optimal algorithm for Min-IP stability. We run experiments on the Adult data set2

  Metric & Approximation Factor & Reference & Remark \\   ID Line metric & \(1\) &  & \\  Weighted tree & \(1\) &  & Only for \(k=2\) \\  General metric & \(O(n)\) &  & \\  General metric & \(O(1)\) & **This work** & \\  

Table 1: Our results on IP stable \(k\)-clustering of \(n\) points. All algorithms run in polynomial time.

used by . For IP stability, we also use four more datasets from UCI ML repository  and a synthetic data set designed to be a hard instance for \(k\)-means++. On the Adult data set, our algorithm performs slightly worse than \(k\)-means++ for IP stability. This is consistent with the empirical results of . On the hard instance3, our algorithm performs better than \(k\)-means++, demonstrating that the algorithm proposed in this paper is more robust than \(k\)-means++. Furthermore for Min-IP stability, we empirically demonstrate that \(k\)-means++ can have an approximation factors which are up to a factor of \(\) worse than our algorithm. We refer to Section 5 and Appendix C for more details.

### Technical Overview

The main contribution is our \(O(1)\)-approximation algorithm for IP stable clustering for general metrics. We discuss the proof technique used to obtain this result. Our algorithm comprises two steps. We first show that for any radius \(r\), we can find a clustering \(=(C_{1},,C_{t})\) such that (a) each cluster has diameter \(O(r)\), and (b) the average distance from a point in a cluster to the points of any other cluster is \((r)\).

Conditions (a) and (b) are achieved through a ball carving technique, where we iteratively pick centers \(q_{i}\) of distance \(>6r\) to previous centers such that the radius \(r\) ball \(B(q_{i},r)\) centered at \(q_{i}\) contains a maximal number of points, say \(s_{i}\). For each of these balls, we initialize a cluster \(D_{i}\) containing the \(s_{i}\) points of \(B(q_{i},r)\). We next consider the annulus \(B(q_{i},3r) B(q_{i},2r)\). If this annulus contains less than \(s_{i}\) points, we include all points from \(B(q_{i},3r)\) in \(D_{i}\). Otherwise, we include _any_\(s_{i}\) points in \(D_{i}\) from the annulus. We assign each unassigned point to the _first_ center picked by our algorithm and is within distance \(O(r)\) to the point. This is a subtle but crucial component of the algorithm as the more natural "assign to the closest center" approach fails to obtain \(O(1)\)-IP stability.

One issue remains. With this approach, we have no guarantee on the number of clusters. We solve this by merging some of these clusters while still maintaining that the final clusters have radius \(O(r)\). This may not be possible for any choice of \(r\). Thus the second step is to find the right choice of \(r\). We first run the greedy algorithm of \(k\)-center and let \(r_{0}\) be the minimal distance between centers we can run the ball carving algorithm \(r=cr_{0}\) for a sufficiently small constant \(c\). Then if we assign each cluster of \(\) to its nearest \(k\)-center, we do indeed maintain the property that all clusters have diameter \(O(r)\), and since \(c\) is a small enough constant, all the clusters will be non-empty. The final number of clusters will therefore be \(k\). As an added benefit of using the greedy algorithm for \(k\)-center as a subroutine, we obtain that the diameter of each cluster is also \(O(r_{0})\), namely the output clustering is a constant factor approximation to \(k\)-center.

### Related Work

Fair Clustering.One of the main motivations of IP stable clustering is its interpretation as a notion of individual fairness for clustering . Individual fairness was first introduced by  for the classification task, where, at high-level, the authors aim for a classifier that gives "similar predictions" for "similar" data points. Recently, other formulations of individual fairness have been studied for clustering [17; 2; 7; 8], too.  proposed a notion of fairness for centroid-based clustering: given a set of \(n\) points \(P\) and the number of clusters \(k\), for each point, a center must be picked among its \((n/k)\)-th closest neighbors. The optimization variant of it was later studied by [19; 20; 24]. studied a pairwise notion of fairness in which data points represent people who gain some benefit from being clustered together. In a subsequent work,  introduced a stochastic variant of this notion.  studied the setting in which the output is a distribution over centers and "similar" points are required to have "similar" centers distributions.

Stability in Clustering.Designing efficient clustering algorithms under notions of stability is a well-studied problem4. Among the various notion of stability, _average stability_ is the most relevant to our model . In particular, they showed that if there is a ground-truth clustering satisfying the requirement of Equation (1) with an additive gap of \(>0\), then it is possible to recover the solution in the list model where the list size is exponential in \(1/\). Similar types of guarantees are shown in the work by . While this line of research mainly focuses on presenting faster algorithms utilizing the strong stability conditions, the focus of IP stable clustering is whether we can recover such stability properties in general instances, either exactly or approximately.

Hedonic Games.Another game-theoretic study of clustering is hedonic games [10; 5; 13]. In a hedonic game, players choose to form coalitions (i.e., clusters) based on their utility. Our work differs from theirs, since we do not model the data points as selfish players. In a related work,  proposes another utility measure for hedonic clustering games on graphs. In particular, they define a closeness utility, where the utility of node \(i\) in cluster \(C\) is the ratio between the number of nodes in \(C\) adjacent to \(i\) and the sum of distances from \(i\) to other nodes in \(C\). This measure is incomparable to IP stability. In addition, their work focuses only on clustering in graphs while we consider general metrics.

## 2 Preliminaries and Notations

We let \((M,d)\) denote a metric space, where \(d\) is the underlying distance function. We let \(P\) denote a fixed set of points of \(M\). Here \(P\) may contain multiple copies of the same point. For a given point \(x P\) and radius \(r 0\), we denote by \(B(x,r)=\{y P d(x,y) r\}\), the ball of radius \(r\) centered at \(x\). For two subsets \(X,Y P\), we denote by \(d(X,Y)=_{x X,y Y}d(x,y)\). Throughout the paper, \(X\) and \(Y\) will always be finite and then the infimum can be replaced by a minimum. For \(x P\) and \(Y P\), we simply write \(d(x,Y)\) for \(d(\{x\},Y)\). Finally, for \(X P\), we denote by \((X)=_{x,y X}d(x,y)\), the diameter of the set \(X\). Again, \(X\) will always be finite, so the supremum can be replaced by a maximum.

## 3 Constant-Factor IP Stable Clustering

In this section, we prove our main result: For a set \(P=\{x_{1},,x_{n}\}\) of \(n\) points with a metric \(d\) and every \(k n\), there exists a \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\) which is \(O(1)\)-approximate IP stable. Moreover, such a clustering can be found in time \((n^{2}T)\), where \(T\) is an upper bound on the time it takes to compute the distance between two points of \(P\).

AlgorithmOur algorithm uses a subroutine, Algorithm 1, which takes as input \(P\) and a radius \(r\) and returns a \(t\)-clustering \(=(D_{1},,D_{t})\) of \(P\) with the properties that (1) for any \(1 i t\), the maximum distance between any two points of \(D_{i}\) is \(O(r)\), and (2) for any \(x P\) and any \(i\) such that \(x D_{i}\), the average distance from \(x\) to points of \(D_{i}\) is \((r)\). These two properties ensure that \(\) is \(O(1)\)-approximate IP stable. However, we have no control on the number of clusters \(t\) that the algorithm produces. To remedy this, we first run a greedy \(k\)-center algorithm on \(P\) to obtain a set of centers \(\{c_{1},,c_{k}\}\) and let \(r_{0}\) denote the maximum distance from a point of \(P\) to the nearest center. We then run Algorithm 1 with input radius \(r=cr_{0}\) for some small constant \(c\). This gives a clustering \(=(D_{1},,D_{t})\) where \(t k\). Moreover, we show that if we assign each cluster of \(\) to the nearest center in \(\{c_{1},,c_{k}\}\) (in terms of the minimum distance from a point of the cluster to the center), we obtain a \(k\)-clustering \(=(C_{1},,C_{k})\) which is \(O(1)\)-approximate IP stable. The combined algorithm is Algorithm 2.

We now describe the details of Algorithm 1. The algorithm takes as input \(n\) points \(x_{1},,x_{n}\) of a metric space \((M,d)\) and a radius \(r\). It first initializes a set \(Q=\) and then iteratively adds points \(x\) from \(P\) to \(Q\) that are of distance greater than \(6r\) from points already in \(Q\) such that \(|B(x,r)|\), the number of points of \(P\) within radius \(r\) of \(x\), is maximized. This is line 5-6 of the algorithm. Whenever a point \(q_{i}\) is added to \(Q\), we define the annulus \(A_{i}:=B(q_{i},3r) B(q_{i},2r)\). We further let \(s_{i}=|B(q_{i},r)|\). At this point the algorithm splits into two cases. If \(|A_{i}| s_{i}\), we initialize a cluster \(D_{i}\) which consists of the \(s_{i}\) points in \(B(x,r)\) and any arbitrarily chosen \(s_{i}\) points in \(A_{i}\). This is line 8-9 of the algorithm. If on the other hand \(|A_{i}|<s\), we define \(D_{i}:=B(q_{i},3r)\), namely \(D_{i}\) contains all points of \(P\) within distance \(3r\) from \(q_{i}\). This is line 10 of the algorithm. After iteratively picking the points \(q_{i}\) and initializing the clusters \(D_{i}\), we assign the remaining points as follows. For any point \(x P_{i}D_{i}\), we find the minimum \(i\) such that \(d(x,q_{i}) 7r\) and assign \(x\) to \(D_{i}\). This is line 13-16 of the algorithm. We finally return the clustering \(=(D_{1},,D_{t})\).

We next describe the details of Algorithm 2. The algorithm iteratively pick \(k\) centers \(c_{1},,c_{k}\) from \(P\) for each center maximizing the minimum distance to previously chosen centers. For each center \(c_{i}\), it initializes a cluster, starting with \(C_{i}=\{c_{i}\}\). This is line 4-7 of the algorithm. Letting \(r_{0}\) be the minimum distance between pairs of distinct centers, the algorithm runs Algorithm 1 on \(P\) with input radius \(r=r_{0}/15\) (line 8-9). This produces a clustering \(\). In the final step, we iterate over the clusters \(D\) of \(\), assigning \(D\) to the \(C_{i}\) for which \(d(c_{i},D)\) is minimized (line 11-13). We finally return the clustering \((C_{1},,C_{k})\).

AnalysisWe now analyze our algorithm and provide its main guarantees.

**Theorem 3.1**.: _Algorithm 2 returns an \(O(1)\)-approximate IP stable \(k\) clustering in time \(O(n^{2}T+n^{2} n)\). Furthermore, the solution is also a constant factor approximation to the \(k\)-center problem._

In order to prove this theorem, we require the following lemma on Algorithm 1.

**Lemma 3.2**.: _Let \((D_{1},,D_{t})\) be the clustering output by Algorithm 1. For each \(i[t]\), the diameter of \(D_{i}\) is at most \(14r\). Further, for \(x D_{i}\) and \(j i\), the average distance from \(x\) to points of \(D_{j}\) is at least \(\)._

Given Lemma 3.2, we can prove the the main result.

Proof of Theorem 3.1.: We first argue correctness. As each \(c_{i}\) was chosen to maximize the minimal distance to points \(c_{j}\) already in \(S\), for any \(x P\), it holds that \(\{d(x,c_{i}) i[k]\} r_{0}\). By Lemma 3.2, in the clustering \(\) output by Ball-Carving\((P,r_{0}/15)\) each cluster has diameter at most \(r_{0}<r_{0}\), and thus, for each \(i[k]\), the cluster \(D\) which contains \(c_{i}\) will be included in \(C_{i}\) in the final clustering. Indeed, in line 11 of Algorithm 2, \(d(c_{i},D)=0\) whereas \(d(c_{j},D)r_{0}\) for all \(j i\). Thus, each cluster in \((C_{1},,C_{k})\) is non-empty. Secondly, the diameter of each cluster is at most \(4r_{0}\), namely, for each two points \(x,x^{} C_{i}\), they are both within distance \(r_{0}+r_{0}<2r_{0}\) of \(c_{i}\). Finally, by Lemma 3.2, for \(x D_{i}\) and \(j i\), the average distance from \(x\) to points of \(D_{j}\) is at least \(}{60}\). Since, \(\) is a coarsening of \(\), i.e., each cluster of \(\) is the disjoint union of some of the clusters in \(\), it is straightforward to check that the same property holds for the clustering \(\). Thus \(\) is \(O(1)\)-approximate IP stable.

We now analyze the running time. We claim that Algorithm 2 can be implemented to run in \(O(n^{2}T+n^{2} n)\) time, where \(T\) is the time to compute the distance between any two points in the metric space. First, we can query all pairs to form the \(n n\) distance matrix \(A\). Then we sort \(A\) along every row to form the matrix \(A^{}\). Given \(A\) and \(A^{}\), we easily implement our algorithms as follows.

First, we argue about the greedy \(k\)-center steps of Algorithm 2, namely, the for loop on line 4. The most straightforward implementation computes the distance from every point to new chosen centers. At the end, we have computed at most \(nk\) distances from points to centers which can be looked up in \(A\) in time \(O(nk)=O(n^{2})\) as \(k n\). In line 8, we only look at every entry of \(A\) at most once so the total time is also \(O(n^{2})\). The same reasoning also holds for the for loop on line 10. It remains to analyze the runtime.

Given \(r\), Alg. 1 can be implemented as follows. First, we calculate the size of \(|B(x,r)|\) for every point \(x\) in our dataset. This can easily be done by binary searching on the value of \(r\) along each of the (sorted) rows of \(A^{}\), which takes \(O(n n)\) time in total. We can similarly calculate the sizes of \(|B(x,2r)|\) and \(|B(x,3r)|\), and thus the number of points in the annulus \(|B(x,3r) B(x,2r)|\) in the same time to initialize the clusters \(D_{i}\). Similar to the \(k\)-center reasoning above, we can also pick the centers in Algorithm 1 which are \(>6r\) apart iteratively by just calculating the distances from points to the chosen centers so far. This costs at most \(O(n^{2})\) time, since there are at most \(n\) centers. After initializing the clusters \(D_{i}\), we finally need to assign the remaining unassigned points (line 13-16). This can easily be done in time \(O(n)\) per point, namely for each unassigned point \(x\), we calculate its distance to each \(q_{i}\) assigning it to \(D_{i}\) where \(i\) is minimal such that \(d(x,q_{i}) 7r\). The total time for this is then \(O(n^{2})\). The \(k\)-center guarantees follow from our choice of \(r_{0}\) and Lemma 3.2. 

_Remark 3.3_.: We note that the runtime can possibly be improved if we assume special structure about the metric space (e.g., Euclidean metric). See Appendix A for a discussion.

We now prove Lemma 3.2.

Proof of Lemma 3.2.: The upper bound on the diameter of each cluster follows from the fact that for any cluster \(D_{i}\) in the final clustering \(=\{D_{1},,D_{t}\}\), and any \(x D_{i}\), it holds that \(d(x,q_{i}) 7r\). The main challenge is to prove the lower bound on the average distance from \(x D_{i}\) to \(D_{j}\) where \(j i\).

Suppose for contradiction that, there exists \(i,j\) with \(i j\) and \(x D_{i}\) such that the average distance from \(x\) to \(D_{j}\) is smaller than \(r/4\), i.e., \(|}_{y D_{j}}d(x,y)<r/4\). Then, it in particular holds that \(|B(x,r/2) D_{j}|>|D_{j}|/2\), namely the ball of radius \(r/2\) centered at \(x\) contains more than half the points of \(D_{j}\). We split the analysis into two cases corresponding to the if-else statements in line 7-10 of the algorithm.

Case 1: \(|A_{j}| s_{j}\):In this case, cluster \(D_{j}\) consists of at least \(2s_{j}\) points, namely the \(s_{j}\) points in \(B(q_{j},r)\) and the set \(S_{j}\) of \(s_{j}\) points in \(A_{j}\) assigned to \(D_{j}\) in line 8-9 of the algorithm. It follows from the preceding paragraph that, \(|B(x,r/2) D_{j}|>s_{j}\). Now, when \(q_{j}\) was added to \(Q\), it was chosen as to maximize the number of points in \(B(q_{j},r)\) under the constraint that \(q_{j}\) had distance greater than \(6r\) to previously chosen points of \(Q\). Since \(|B(x,r)|>|B(x,r/2)|>|B(q_{j},r)|\), at the point where \(q_{j}\) was chosen, \(Q\) already contained some point \(q_{j_{0}}\) (with \(j_{0}<j\)) of distance at most \(6r\) to \(x\) and thus of distance at most \(7r\) to any point of \(B(x,r/2)\). It follows that \(B(x,r/2) D_{j}\) contains no point assigned during line 13- 16 of the algorithm. Indeed, by the assignment rule, such a point \(y\) would have been assigned to either \(D_{j_{0}}\) or potentially an even earlier initialized cluster of distance at most \(7r\) to \(y\). Thus, \(B(x,r/2) D_{j}\) is contained in the set \(B(q_{j},r) S_{j}\). However, \(|B(q_{j},r)|=|S_{j}|=s_{j}\) and moreover, for \((y_{1},y_{2}) B(q_{j},r) S_{j}\), it holds that \(d(y_{1},y_{2})>r\). In particular, no ball of radius \(r/2\) can contain more than \(s_{j}\) points of \(B(q_{j},r) S_{j}\). As \(|B(x,r/2) D_{j}|>s_{j}\), this is a contradiction.

Case 2: \(|A_{j}|<s_{j}\):In this case, \(D_{j}\) includes all points in \(B(q_{j},3r)\). As \(x D_{j}\), we must have that \(x B(q_{j},3r)\) and in particular, the ball \(B(x,r/2)\) does not intersect \(B(q_{j},r)\). Thus,

\[|D_{j}||B(x,r/2) D_{j}|+|B(q_{j},r) D_{j}|>|D_{j}|/2+s_{j},\]

so \(|D_{j}|>2s_{j}\), and finally, \(|B(x,r/2) D_{j}|>|D_{j}|/2>s_{j}\). Similarly to case 1, \(B(x,r/2) D_{j}\) contains no points assigned during line 13- 16 of the algorithm. Moreover, \(B(x,r/2) B(q_{j},3r) A_{j}\). In particular, \(B(x,r/2) D_{j} S_{j}\), a contradiction as \(|S_{j}|=s_{j}\) but \(|B(x,r/2) D_{j}|>s_{j}\). 

## 4 Min and Max-IP Stable Clustering

The Min-IP stable clustering aims to ensure that for any point \(x\), the _minimum_ distance to a point in the cluster of \(x\) is at most the minimum distance to a point in any other cluster. We show that a Min-IP stable \(k\)-clustering always exists for any value of \(k[n]\) and moreover, can be found by a simple algorithm (Algorithm 3).

```
1:Input: Pointset \(P=\{x_{1},,x_{n}\}\) from a metric space \((M,d)\) and integer \(k\) with \(2 k n\).
2:Output: \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\).
3:\(L\{(x_{i},x_{j})\}_{1 i<j n}\) sorted according to \(d(x_{i},x_{j})\)
4:\(E\)
5:while\(G=(P,E)\) has \(>k\) connected components do
6:\(e\) an edge \(e=(x,y)\) in \(L\) with \(d(x,y)\) minimal.
7:\(L L\{e\}\)
8:if\(e\) connects different connected components of \(G\)then\(E E\{e\}\)
9:endwhile
10:return the connected components \((C_{1},,C_{k})\) of \(G\). ```

**Algorithm 3** Min-IP-Clustering

The algorithm is identical to Kruskal's algorithm for finding a minimum spanning tree except that it stops as soon as it has constructed a forest with \(k\) connected components. First, it initializes a graph \(G=(V,E)\) with \(V=P\) and \(E=\). Next, it computes all distances \(d(x_{i},x_{j})\) between pairs of points \((x_{i},x_{j})\) of \(P\) and sorts the pairs \((x_{i},x_{j})\) according to these distances. Finally, it goes through this sorted list adding each edge \((x_{i},x_{j})\) to \(E\) if it connects different connected components of \(G\). After computing the distances, it is well known that this algorithm can be made to run in time \(O(n^{2} n)\), so the total running time is \(O(n^{2}(T+ n))\) where \(T\) is the time to compute the distance between a single pair of points.

**Theorem 4.1**.: _The \(k\)-clustering output by Algorithm 3 is a Min-IP stable clustering._

Proof.: Let \(\) be the clustering output by the algorithm. Conditions (1) and (2) in the definition of a min-stable clustering are trivially satisfied. To prove that (3) holds, let \(C\) with \(|C| 2\) and \(x C\). Let \(y_{0} x\) be a point in \(C\) such that \((x,y_{0}) E\) (such an edge exists because \(C\) is the connected component of \(G\) containing \(x\)) and let \(y_{1}\) be the closest point to \(x\) in \(P C\). When the algorithm added \((x,y_{0})\) to \(E\), \((x,y_{1})\) was also a candidate choice of an edge between connected components of \(G\). Since the algorithm chose the edge of minimal length with this property, \(d(x,y_{0}) d(x,y_{1})\). Thus, we get the desired bound:

\[_{y C\{x\}}d(x,y) d(x,y_{0}) d(x,y_{1})=_{y P  C}d(x,y).\]

**Theorem 4.2**.: _The solution output by the greedy algorithm of \(k\)-center is a \(3\)-approximate Max-IP stable clustering._

Proof.: To recall, the greedy algorithm of \(k\)-center (aka Gonzalez algorithm ) starts with an arbitrary point as the first center and then goes through \(k-1\) iterations. In each iteration, it picks a new point as a center which is furthest from all previously picked centers. Let \(c_{1},,c_{k}\) denote the selected centers and let \(r:=_{v P}d(v,\{c_{1},,c_{k}\})\). Then, each point is assigned to the cluster of its closest center. We denote the constructed clusters as \(C_{1},,C_{k}\). Now, for every \(i j[k]\) and each point \(v C_{i}\), we consider two cases:* \(d(v,c_{i}) r/2\). \[_{u_{i} C_{i}}d(v,u_{i})  d(v,c_{i})+d(u_{i},c_{i}) 3r/2,\] \[_{u_{j} C_{j}}d(v,u_{j})  d(v,c_{j}) d(c_{i},c_{j})-d(v,c_{i}). r/2\]
* \(d(v,c_{i})>r/2\). \[_{u_{i} C_{i}}d(v,u_{i})  d(v,c_{i})+d(u_{i},c_{i}) 3d(v,c_{i}),\] \[_{u_{j} C_{j}}d(v,u_{j})  d(v,c_{j}) d(v,c_{i}).\]

In both cases, \(_{u_{i} C_{i}}d(v,u_{i}) 3_{u_{j} C_{j}}d(v,u_{j})\). 

## 5 Experiments

While the goal and the main contributions of our paper are mainly theoretical, we also implement our optimal Min-IP clustering algorithm as well as extend the experimental results for IP stable clustering given in . Our experiments demonstrate that our optimal Min-IP stable clustering algorithm is superior to \(k\)-means++, the strongest baseline in , and show that our IP clustering algorithm for average distances is practical on real world datasets and is competitive to \(k\)-means++ (which fails to find good stable clusterings in the worst case ). We give our experimental results for Min-IP stability and defer the rest of the empirical evaluations to Section C. All experiments were performed in Python 3. The results shown below are an average of \(10\) runs for \(k\)-means++.

MetricsWe measure the quality of a clustering using the same metrics used in  for standardization. Considering the question of \(f\)-IP stability (Definition 1.4), let the violation of a point \(x\) be defined as \((x)=_{C_{i} C(x)})}{f(x,C_{i})}\).

For example, setting \(f(x,C)=_{y C}d(x,y)/|C|\) corresponds to the standard IP stability objective and \(f(x,C)=_{y C}d(x,y)\) is the Min-IP formulation. Note point \(x\) is stable iff \((x) 1\).

We measure the extent to which a \(k\)-clustering \(=(C_{1},,C_{k})\) of \(P\) is (un-)stable by computing \(=_{x P}(x)\) (i.e. maximum violation) and \(=_{x P}(x)/|P|\) (mean violation).

ResultsFor Min-IP stability, we have an optimal algorithm; it always return a stable clustering for all \(k\). We see in Figures 1 that for the max and mean violation metrics, our algorithm outperforms \(k\)-means++ by up to a factor of **5x**, consistently across various values of \(k\). \(k\)-means ++ can return a much worse clustering under Min-IP stability on real data, motivating the use of our theoretically-optimal algorithm in practice.

Figure 1: Maximum and mean violation for Min-IP stability for the Adult dataset, as used in ; lower values are better.

Conclusion

We presented a deterministic polynomial time algorithm which provides an \(O(1)\)-approximate IP stable clustering of \(n\) points in a general metric space, improving on prior works which only guaranteed an \(O(n)\)-approximate IP stable clustering. We also generalized IP stability to \(f\)-stability and provided an algorithm which finds an exact Min-IP stable clustering and a 3-approximation for Max-IP stability, both of which hold for all \(k\) and in general metric spaces.