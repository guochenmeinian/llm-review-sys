# Fixing the NTK: From Neural Network Linearizations

to Exact Convex Programs

 Rajat Vadiraj Dwaraknath

Stanford University

rajatvd@stanford.edu &Tolga Ergen

LG AI Research

tergen@lgresearch.ai &Mert Pilanci

Stanford University

pilanci@stanford.edu

###### Abstract

Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data. A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set. By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network. We also provide several numerical simulations corroborating our theory. Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso.

## 1 Introduction

Neural Networks (NNs) have become popular in various machine learning applications due to their remarkable modeling capabilities and generalization performance. However, their highly nonlinear and non-convex structure precludes an effective theoretical analysis. Therefore, developing theoretical tools to understand the fundamental mechanisms behind neural networks is still an active research topic. To tackle this problem,  studied the training dynamics of neural networks trained with Stochastic Gradient Descent (SGD) in a regime where each layer has infinitely many neurons and SGD uses an infinitesimally small learning rate, i.e., gradient flow. Thus, they related the training dynamics of neural networks to the training dynamics of a fixed kernel called the Neural Tangent Kernel (NTK). However,  showed that neurons barely move from their initial values in this regime so that neural networks fail to learn useful features from the training data. This is in contrast to their finite width counterparts, which are able to learn predictive features in practice . Moreover,  provided further theoretical and empirical evidence to show that existing kernel approaches are not able to explain the remarkable performance of finite width networks. Therefore, although NTK and similar kernel based approaches enable theoretical analysis unlike standard finite width networks, they fail to explain the effectiveness of finite width neural networks that are employed in practice.

Recently a series of papers [6; 7; 8; 9; 10; 11; 12; 13; 14; 15] introduced an analytic framework to analyze finite width neural networks by leveraging certain convex duality arguments. Particularly, they showed that the standard regularized non-convex training problem can be equivalently cast as a finite dimensional convex program. This convex approach has two major advantages over standard non-convex training: **(1)** Since the training objective is convex, one can find globally optimal parameters of the network efficiently and reliably unlike standard nonconvex training which can get stuck at a local minimum, and **(2)** As we show in this work, a class of convex reformulations can be interpreted as an instance of Multiple Kernel Learning (MKL)  which allows us to characterize the corresponding finite width networks by a learned data-dependent kernel that can be iteratively computed. This is in contrast to the infinite-width kernel characterization in which the NTK stays constant throughout training.

**Notation and Preliminaries.** In the paper, we use lowercase and uppercase bold letters to denote vectors and matrices respectively. We also use subscripts to denote a certain column or element. We denote the identity matrix of size \(k k\) as \(_{k}\). To denote the set \(\{1,2,,n\}\), we use \([n]\). We also use \(_{p}\) and \(_{F}\) to represent the standard \(_{p}\) and Frobenius norms. Additionally, we denote 0-1 valued indicator function and ReLU activation as \(1\{x 0\}\) and \((x)_{+}:=\{x,0\}\), respectively.

In this paper, we focus on analyzing the regularized training problem of ReLU networks. Particularly, we consider a two-layer ReLU network with \(m\) neurons whose output function is defined as follows

\[f(,)\!:=_{j=1}^{m}(^{T}_{j}^{(1)})_{+}w_{j}^{(2)}=_{j=1}^{m}(\{ ^{T}_{j}^{(1)} 0\}^{T}_{j}^{(1)} )w_{j}^{(2)}.\] (1)

where \(_{j}^{(1)}^{d}\) and \(w_{j}^{(2)}\) are the \(j^{th}\) hidden and output layer weights, respectively and \(:=\{(_{j}^{(1)},w_{j}^{(2)})\}_{j=1}^{m}\) represents all trainable parameters. Given a training data matrix \(^{n d}\) and a target vector \(^{n}\), we minimize the following weight decay regularized training objective

\[_{^{(1)},^{(2)}}_{j=1}^{m}( _{j}^{(1)})_{+}w_{j}^{(2)}-_{ 2}^{2}+_{j=1}^{m}(_{j}^{(1)}_ {2}^{2}+|w_{j}^{(2)}|^{2}),\] (2)

where \(>0\) is the regularization coefficient. We also use \(f(,)=_{j=1}^{m}(_{j }^{(1)})_{+}w_{j}^{(2)}\) for convenience. We discuss extensions to generic loss and deeper architectures in appendix.

**Our Contributions.**

* In section 4, we show that the convex formulation of the _Gated ReLU_ network is equivalent to _Multiple Kernel Learning_ with a specific set of _Masking Kernels_. (Theorem 4.3).
* In section 5, we connect this formulation to the Neural Tangent Kernel by showing that the NTK is a specific weighted combination of masking kernels (Theorem 5.1).
* In Corollary 5.2, we show that on the training set, the **NTK is suboptimal when compared to the optimal kernel learned by MKL**, which is equivalent to the model learnt by our convex Gated ReLU program.
* In section 6, we also derive bounds on the prediction error of this optimal kernel and specify how to choose the regularization parameter \(\) (Theorem 6.1).

## 2 Convex Optimization and the NTK

Here, we briefly review the literature on convex training and the NTK theory of neural networks.

### Convex Programs for ReLU Networks

Even though the network in (1) has only two layers, previous studies show that (2) is a challenging optimization problem due to the non-convexity of the objective function. Thus, local search heuristics such as SGD might fail to globally optimize the training objective [17; 18; 19; 20]. To eliminate the issues associated with the inherent non-convexity,  introduced an exact convex reformulation of (2) as the following constrained optimization problem

\[_{_{i},_{i}^{}}\|_{i=1}^{p}_{i} (_{i}-_{i}^{})-\|_{2}^{2}+ _{i=1}^{p}(\|_{i}\|_{2}+\| _{i}^{}\|_{2})(2_{i}-) _{i}\\ (2_{i}-)_{i}^{}  i,\] (3)

where \(_{i}_{}\) are \(n n\) binary masking diagonal matrices given by \(p\!:=|_{}|\) and

\[_{}\!:=\{(\{ \}):^{d }\}.\] (4)

The mask set \(_{}\) can be interpreted as the set of all possible ways to separate the training data \(\) by a hyperplane passing through the origin. With these masks, we can characterize a single ReLU activated neuron on the training data as follows: \((_{i})_{+}=( \{_{i}\}) _{i}=_{i}_{i}\) provided that \((2_{i}-_{n})_{i} 0\). Therefore, by enforcing these cone constraints in (3), we maintain the masking property of the ReLU activation and parameterize the neural network as a linear function of the weights, thus make the learning problem convex. We refer the reader to  for more details.

Although (3) is convex and therefore eliminates the drawbacks associated with the non-convexity of (2), it might still be computationally complex to solve. Precisely, a worst-case upper-bound on the number of variables is \((n^{r})\), where \(r=()\{n,d\}\). Although this is still significantly better than brute-force search over \(2^{mn}\) ReLU patterns, it could still be exponential in the dimension \(d\). To mitigate this issue,  proposed a relaxation of (1) called the _gated ReLU_ network

\[f_{}(,)\!:=_{j=1}^{m} (\{^{T}_{j}\} ^{T}_{j}^{(1)})w_{j}^{(2)},\] (5)

where \(\!:=\{_{j}\}_{j=1}^{m}\) is a set of _gate_ vectors that are also optimized throughout training. Then, the corresponding non-convex learning problem is as follows

\[_{^{(1)},^{(2)},}\|_{j=1}^{m} (\{_{j} \})_{j}^{(1)}w_{j}^{(2)}-\|_{2 }^{2}+_{j=1}^{m}(\|_{j}^{(1)}\|_{2}^{2}+ |w_{j}^{(2)}|^{2}).\] (6)

By performing this relaxation, we decouple the dependence between the indicator function and the linear term in the exact ReLU network (1). To express the equivalent convex optimization problem corresponding to the gated ReLU network, we introduce the notion of complete gate sets.

**Definition 2.1**.: _A gate set \(\) is complete with respect to a dataset \(\) if the corresponding set of hyperplane arrangements covers all possible arrangement patterns for \(\) defined in (4), i.e.,_

\[\{(\{_{j} \}):_{j}\}=_{ }.\]

_Additionally, \(\) is minimally complete if \(||=|_{}|=p\)._

Now, with this relaxation,  showed that the optimal value for (6) can always be achieved by choosing \(\) to be a complete gate set. Therefore, by working only with complete gate sets, we can modify (6) to only optimize over the network parameters \(^{(1)}\) and \(^{(2)}\)

\[_{^{(1)},^{(2)}}\|_{j=1}^{m} (\{_{j}\}) _{j}^{(1)}w_{j}^{(2)}-\|_{2}^{2}+ _{j=1}^{m}(\|_{j}^{(1)}\|_{2}^{2}+|w_{j}^{(2)}|^{ 2}).\] (7)

Additionally,  also showed that we can set \(m=p\) without loss of generality. Then,  showed that the equivalent convex optimization problem for (6) and also (7) in the complete gate set setting is

\[_{_{i}}\|_{i=1}^{p}_{i}_{i }-\|_{2}^{2}+_{i=1}^{p}\|_{i}\| _{2}.\] (8)

Notice that (8) is a least squares problem with _group lasso_ regularization . Therefore, this relaxation for (3) can be efficiently optimized via convex optimization solvers. Furthermore,  proved that after solving the relaxed problem (8), one can construct an equivalent ReLU network from a gated ReLU network via a convex optimization based procedure called _cone decomposition_. We discuss the computational complexity of this approach in Section E of the supplementary material.

### The Neural Tangent Kernel

Previous works [1; 2; 5; 23; 24] characterized the training dynamics of SGD with infinitesimally small learning rates on neural networks in the _infinite-width_ limit, i.e., as \(m\), via the NTK. [25; 26] also present analyses in this regime via a mean-field approach. In this section, we provide a brief overview of this theory and refer the reader to [1; 27; 28] for more details. The main idea behind the NTK theory is to approximate the neural network model function \(f(,)\) by _linearizing it_ with respect to the parameters \(\) around its initialization \(_{0}\):

\[(,) f(,_{0})+_{}f(,_{0})^{T}(-_{0}).\]

The authors of  show that if \(f\) is a neural network (with appropriately scaled output), and parameters \(\) initialized as i.i.d standard Gaussians, the linearization \(\) better approximates \(f\) in the infinite-width limit. We can interpret the linearized model \(\) as a kernel method with a feature map given by \(()=_{}f( ,_{0})\). The corresponding kernel induced by this feature map is termed as the NTK. Note that this is a random kernel since it depends on the random initialization of the parameters denoted as \(_{0}\). The main result of  in the simplified case of two-layer neural networks is that, in the infinite-width limit this kernel approaches a fixed deterministic limit given by \(H,^{}:=_{ }f,}_{0}^ {T}_{}f^{},}_{0}\), where \(}\) corresponds to the parameters of a single neuron. Furthermore,  show that in this infinite limit, SGD with an infinitesimally small learning rate is equivalent to performing kernel regression with the fixed NTK.

To link the convex formulation (8) with NTK theory, we first present a scaled version of the gated ReLU network in (5) as follows

\[_{}(,)\!:= {}_{j=1}^{m}(\{^{T}_{j} \}^{T}_{j}^{(1)})w_{j}^{(2)}.\] (9)

In the next lemma, we provide the infinite width NTK of the scaled gated ReLU network in (9).

**Lemma 2.2**.: 1

_The infinite width NTK of the gated ReLU network (9) with i.i.d gates sampled as \(_{j}(,})\) and randomly initialized parameters as \(_{j}^{(1)}(,})\) and \(w_{j}^{(2)}(0,1)\) is_

\[H(,^{})\!:=(- (^{T}^{}}{\|\|_{2} \|^{}\|_{2}}))^{T} ^{}.\] (10)

Additionally, we introduce a reparameterization of the standard ReLU network (1) with \(:=\{(_{j}^{(+)},_{j}^{(-)} )\}_{j=1}^{m}\), with \(_{j}^{(+)}\), \(_{j}^{(-)}^{d}\) which can still represent all the functions that (1) can

\[f_{r}(,)\!:=}_{j=1}^{m} (^{T}_{j}^{(+)})_{+}-(^{T} _{j}^{(-)})_{+}.\] (11)

**Lemma 2.3**.: _The gated ReLU network (9) and the reparameterized ReLU network (11) have the same infinite width NTK given by Lemma 2.2._

Next, we present an equivalence between the gated ReLU network and the MKL model [16; 29].

## 3 Multiple Kernel Learning and Group Lasso

The Multiple Kernel Learning (MKL) model [16; 29] is an extension of the standard kernel method that learns an optimal data-dependent kernel as a convex combination of a set of fixed kernels and then performs regression with this learned kernel. We provide a brief overview of the MKL setting based on the exposition in . Consider a set of \(p\) kernels given by corresponding feature maps \(_{i}:^{d}^{d_{i}}\). Given \(n\) training samples \(^{n d}\) with targets \(^{n}\), we define the feature matrices on this data by stacking the feature vectors as \(_{i}\!:=[_{i}(_{1})^{T}\!:; _{i}(_{n})^{T}]^{n d_{i}}\). Then, the corresponding \(n n\) kernel matrices are given by \(_{i}\!:=_{i}_{i}^{T}\). A convex combination of these kernels can be written as \(()\!:=_{i=1}^{p}_{i}_{i}\) where \(_{p}:=\{:^{T}=1,\ \}\) is a set of weights in the unit simplex. By noticing that the feature map corresponding to \(()\)is obtained by taking a weighted concatenation of \(_{i}\) with weights \(}\), we can write the MKL optimization problem in terms of the feature matrices as

\[_{_{p},_{i}^{d_{i}}} \|_{i=1}^{p}}_{i}_{i}- \|_{2}^{2}+_{i=1}^{p}\|_{i}\|_{2}^ {2},\] (12)

where \(>0\) is a regularization coefficient. For a set of fixed weights \(\), the optimal objective value of the kernel regression problem over \(\) is proportional to \(^{T}(_{i=1}^{p}_{i}_{i}+_{ })^{-1}\) up to constant factors . Thus, the MKL problem can be equivalently written as the following problem

\[_{_{p}}^{T}( ()+_{})^{-1} .\] (13)

In this formulation, we can interpret MKL as finding the optimal data-dependent kernel that can be expressed as a convex combination of the fixed kernels given by \(_{i}\). In the next section, we link this kernel learning formulation with the convex group lasso problem in (8).

### Equivalence to Group Lasso

We first show that the MKL problem in (12) can be equivalently stated as a group lasso problem.

**Lemma 3.1** ([16; 29]).: _The MKL problem (12) is equivalent to the following kernel regression problem using a uniform combination of the fixed kernels with squared group lasso regularization where the groups are given by parameters corresponding to each feature map_

\[_{_{i}^{d_{i}}}\|_{i=1}^{p} _{i}_{i}-\|_{2}^{2}+(_{i=1}^{ p}\|_{i}\|_{2})^{2}.\] (14)

We now present a short derivation of this equivalence. Using the variational formulation of the squared group \(_{1}\)-norm 

\[(_{i=1}^{p}\|_{i}\|_{2})^{2} =_{_{p}}_{i=1}^{p}_{i} \|_{2}^{2}}{_{i}},\]

we can rewrite the group lasso problem (14) as a joint minimization problem over both the parameters \(\) and regularization weights \(\) as follows

\[_{_{p}}_{_{i}^{d_{i}}} \|_{i=1}^{p}_{i}_{i}-\|_{2}^ {2}+_{i=1}^{p}_{i}\|_{2}^{2}}{ _{i}}.\]

Finally, with a change of variables given by \(_{i}=_{i}/}\), we recover the MKL problem (12). We note that the MKL problem (12) is also equivalent to the following standard group lasso problem

\[_{_{i}^{d_{i}}}\|_{i=1}^{p} _{i}_{i}-\|_{2}^{2}+_{i=1}^{p}\| _{i}\|_{2}.\] (15)

This is due to the fact that squared and standard group lasso problems have the same regularization paths , so (14) and (15) are equivalent when \(=^{p}\|_{i}^{*}\|_ {2}}\), where \(^{*}\) is the solution to (14).

### Solving Group Lasso by Iterative Reweighting

Previously, we used a variational formulation of the squared group \(_{1}\)-norm to show equivalences to MKL. Now, we present the Iteratively Reweighted Least Squares (IRLS) algorithm [32; 33; 34; 35] to solve the group lasso problem (15) using the following variational formulation of the group \(_{1}\)-norm 

\[_{i=1}^{p}\|_{i}\|_{2}=_{_{ +}^{p}}_{i=1}^{p}(_{i}\|_{2}^{ 2}}{_{i}}+_{i}).\]Based on this, we rewrite the group lasso problem (15) as the following minimization problem

\[_{_{+}^{p}}_{_{i}^{d_{i}}} \|_{i=1}^{p}_{i}_{i}-\|_{2}^{ 2}+_{i=1}^{p}(_{i}\|_{2}^ {2}}{_{i}}+_{i}).\]

Since the objective is jointly convex in \((,)\), it can be solved using alternating minimization . Particularly, note that the inner minimization problem in \(_{i}\)'s is simply a \(_{2}\) regularized least squares problem with different regularization strengths for each group and this can be solved in closed form

\[_{_{i}^{d_{i}}}\|_{i=1}^{p} _{i}_{i}-\|_{2}^{2}+_{i=1}^{p}_{i}\|_{2}^{2}}{_{i}}.\] (16)

The outer problem in \(\) is also directly solved by setting \(_{i}=\|_{i}\|_{2}\). To avoid convergence issues and instability around \(_{i}=0\), we approximate the reweighting by adding a small positive constant \(\). We use this procedure to solve the group lasso formulation of the gated ReLU network (8) by setting \(_{i}=_{i}\). A detailed description is provided Algorithm 1. For further details regarding convergence, we refer the reader to .

```
1:Set iteration count \(k 0\)
2:Initialize weights \(_{i}^{(0)}\)
3:Set \(_{i}\!:=\!_{i},\,\,_{i} _{}\)
4:while not converged and \(k\) iteration count do
5: Solve the weighted \(_{2}\) regularized least squares problem: \[\{_{i}^{(k)}\}_{i}=*{argmin}_{\{_ {i}\}_{i}}\|_{i=1}^{p}_{i}_{i}-\|_ {2}^{2}+_{i=1}^{p}_{i}\|_{2}^{2}}{ _{i}^{(k)}}\]
6: Update the weights: \(_{i}^{(k+1)}=_{i}^{(k)}\|_{2}+}\)
7: Increment iteration count: \(k k+1\)
8:endwhile
9:Optional: Convert the gated ReLU network to a ReLU network (see Section E for details) ```

**Algorithm 1** Iteratively Reweighted Least Squares (IRLS) for gated ReLU and ReLU networks

## 4 Gated ReLU as MKL with Masking Kernels

Motivated by the MKL interpretation of group lasso, we return to the convex reformulation (8) of the gated ReLU network. Notice that this problem has the same structure as the MKL equivalent group lasso problem (15) with a specific set of feature maps that we define below.

**Definition 4.1**.: _The masking feature maps \(_{j}:^{d}^{d}\) generated by a fixed set of gates \(\) are defined as \(_{j}()=\{^{T}_{j} 0 \}\)._

These feature maps can be interpreted as simply passing the input unchanged if it lies in the positive halfspace of the corresponding gate vector \(_{j}\), i.e., \(^{T}_{j} 0\), and returning zero if the input does not lie in this halfspace. Since \((\{_{j} \})_{},\,\,\,\,_{j}\) holds for an arbitrary gate set \(\), we can conveniently express the corresponding feature matrices of these masking feature maps on the data \(\) in terms of fixed diagonal data masks as \(_{j}=_{j}\), where \(_{j}=(\{_ {j}\})\). Similarly, the corresponding masking kernel matrices take the form \(_{j}=_{j}^{T}_{j}\). Note that for an arbitrary set of gates, the generated masking feature matrices on \(\) may not cover the entire set of possible masks \(_{}\). Additionally, multiple gate vectors can result in identical masks if \((\{_{i}\})=(\{ _{j}\})\) for \(i j\) leading to degenerate feature matrices. However, if we work with minimally complete gate sets as defined in Definition 2.1, we can rectify these issues.

**Lemma 4.2**.: _For a minimally complete gate set \(\) defined in Definition 2.1, we can uniquely associate a gate vector \(_{i}\) to each data mask \(_{i}_{}\) such that \(_{i}=(\{ _{i}\}), i[p]\)._Consequently, for minimally complete gate sets \(\), the generated masking feature matrices \( i[p]\) can be expressed as \(_{i}=_{i}\) and the masking kernel matrices take the form \(_{i}=_{i}^{T}_{i}\). In the context of the gated ReLU problem (7), since \(\) is complete, we can replace it with a minimally complete subset of \(\) without loss of generality since  showed that increasing \(m\) beyond \(p\) cannot reduce the value of the regularized training objective in (7). We are now ready to combine the MKL-group lasso equivalence with the convex reformulation of the gated ReLU network to present the following characterization of the nonconvex gated ReLU learning problem.

**Theorem 4.3**.: _The non-convex gated ReLU problem (7) with a minimally complete gate set \(\) is equivalent to performing multiple kernel learning (12) with the masking feature maps generated by \(\)_

\[_{_{p},_{i}^{d}} \|_{i=1}^{p}}_{i}_{i}- \|_{2}^{2}+_{i=1}^{p}\|_{i} \|_{2}^{2}.\]

This theorem implies that the gated ReLU network finds the optimal combination of linear models restricted to the different masked datasets \(_{i}\) generated by the gates. By optimizing with all possible data maskings, we obtain the best possible gated ReLU network. From the kernel perspective, we have characterized the problem of finding an optimal finite width gated ReLU network as learning a data-dependent kernel and then performing kernel regression. This is in contrast to the NTK theory where the training of an infinite width network by gradient flow is characterized by regression with a constant kernel that is not learned from data. We further explore this connection below.

## 5 NTK as a Weighted Masking Kernel

We now connect the NTK of a gated ReLU network with the masking kernels generated by its gates.

**Theorem 5.1**.: _Let \(_{}(})^{n  n}\) be the weighted masking kernel obtained by taking a convex combination of the masking feature maps generated by a minimally complete gate set \(\) with weights given by \(_{i}=[(\{ \})=_{i}]\) where \((,_{})\) and let \(^{n n}\) be the infinite width NTK of the gated ReLU network (10) evaluated on the training data, i.e., the \(ij^{th}\) entry of \(\) is defined as \(_{ij}=H(_{i},_{j})\). Then, \(_{}(})=\)._

A rough sketch of the proof of this theorem is to express the matrix \(\) as an expectation of indicator random variables using the definition of the NTK . Then, by conditioning on the event that these indicators equal the masks \(_{i}\), we can express the NTK as a convex combination of the masking kernels \(_{i}\). The weights end up being precisely the probabilities that are described in Theorem 5.1. A detailed proof is provided in the supplementary material.

This theorem implies that the outputs of the gated ReLU network obtained via (16) with regularization weights \(}\) on the training data is identical to that of kernel ridge regression with the NTK.

**Corollary 5.2**.: _Let \(}\) be the solution to (16) with feature matrices \(_{i}=_{i}\) and regularization weights \(_{i}=[(\{ \})=_{i}]\) where \((,_{})\) and let \(}=(+_{})^{-1} \) be the outputs of kernel ridge regression with the NTK on the training data. Then, \(_{i=1}^{p}_{i}}_{i}=}\)._

Since \(\) is minimally complete, the weights in Theorem 5.1 satisfy \(_{i=1}^{p}_{i}=1\). In other words, \(}_{p}\). Therefore, we can interpret Theorem 5.1 as follows - the NTK evaluated on the training data lies in the convex hull of all possible masking kernels of the training data. So \(\) lies in the feasible set of kernels for MKL using these masking kernels. By Theorem 4.3, we can find the optimal kernel in this set by solving the group lasso problem (8) of the gated ReLU network. **Therefore, we can interpret solving** (8) **as fixing the NTK by learning an improved data-dependent kernel.**

**Remark 5.3** (Suboptimality of NTK).: _Note that the weights \(}\) do depend on the training data \(\), but do not depend on the target labels \(\). Since MKL learns the optimal kernel using both \(\) and \(\), the NTK still cannot perform better than the optimal MKL kernel on the training set. Thus, we fix the NTK._

## 6 Analysis of Prediction Error

In this section, we present an analysis of the in-sample prediction error for the gated ReLU network given in (5) along the lines of existing consistency results [36; 37] for the group lasso problem (8).

We assume that the data is generated by a noisy ReLU neural network model \(=f(,^{*})+\) where \(f\) is the ReLU network defined in (1) with true parameters \(^{*}\) and the noise is distributed as \(0,^{2}_{n}\). By the seminal universal approximation theorem of , this model is able to capture a broad class of ground-truth functions by using a ReLU network with enough neurons. We can transform \(^{*}\) to the weights \(^{*}\) in the convex ReLU model and write \(f(,^{*})=_{i=1}^{p}_{i} _{i}^{*}\). We denote by \(}\) the solution of the group lasso problem (8) for the gated ReLU network with an additional \(\) factor on the loss to simplify derivations,

\[}=*{argmin}_{_{i}^{d}} \|_{i=1}^{p}_{i}_{i}- \|_{2}^{2}+_{i=1}^{p}\|_{i}\|_ {2}.\] (17)

We now present our main theorem which bounds the prediction error of the gated ReLU network obtained from the solution \(}\) below.

**Theorem 6.1** (Prediction Error of Gated ReLU).: _For some \(t>0\), let the regularization parameter in (17) be \(=t\|\|_{F}/n\). Then, with probability at least \(1-2e^{-t^{2}/8}\), we have_

\[\|f_{},}-f (,^{*})\|_{2}^{2} 2_{i=1}^{p} \|_{i}^{*}\|_{2}\]

_where \(f_{},}=_{i=1}^{p} _{i}}_{i}\) are the predictions of the gated ReLU network obtained from \(}\)._

The proof closely follows the analysis of the regular lasso problem presented in , but we extend it to the specific case of the group lasso problem corresponding to the gated ReLU network and leverage the masking structure of the lifted data matrix to obtain simplified bounds.

## 7 Experiments

Here, we empirically corroborate our theoretical results via experiments on several datasets.2

Figure 1: Plot of objective value of problem (8) which is solved using IRLS (algorithm 1) for a toy 1D dataset with \(n=5\). The iterates are compared to the optimal value obtained by solving (8) using CVXPY (blue). Notice that the solution to (16) with regularization weights given by the NTK weights \(}\) from Theorem 5.1 (green) is sub-optimal for problem (8), and running IRLS by initializing with these weights (red) converges to the optimal objective value. We also include plots of IRLS initialized with random weights (black). The right plot shows the corresponding learned functions. The blue curve shows the output of the solution to the group lasso problem (8) after performing a cone decomposition to obtain a ReLU network. The dashed curve shows the output of running gradient descent (GD) on a ReLU network with 100 neurons. The green curve is the result of kernel ridge regression (KRR) with the NTK. **We observe that our reweighted kernel method (Group Lasso) produces an output that matches the output of the NN trained via GD. In contrast, NTK produces an erroneous smooth function due to the infinite width approximation.**

**1D datasets.** For the 1D experiments in Figure 1, we add a second data dimension with value equal to \(1\) for all data points to simulate a bias term in the first layer of the gated ReLU network. Also, in this case we can enumerate all \(2n\) data masks \(_{i}\) directly. We use the cone decomposition procedure described in  to obtain a ReLU network from the gated ReLU network obtained by solving (8). We also train a \(100\) neuron ReLU network using gradient descent (GD) and compare the learned output functions in the right plot in Figure 1. Exact details can be found in the supplementary material.

**Student-teacher setting.** We generate the training data by sampling \((,_{})\) and computing the targets \(\) using a fixed, randomly initialized gated ReLU teacher network. In Figure 2, \(n=10\), \(d=5\), and we use a teacher network with width \(m=10\), with gates and parameters randomly drawn from independent standard multivariate Gaussians. To solve the convex formulation (8), we estimate \(_{}\) by randomly sampling unique hyperplane arrangements.

**Computing the NTK weights \(}\).** The weights induced by the NTK are given as in Theorem 5.1 by \(_{i}=[(\{\})=_{i}]\) where \((,_{})\) is a standard multivariate Gaussian vector. These probabilities can be interpreted either as the orthant probabilities of the multivariate Gaussians given by \((2_{i}-_{})\) or as the solid angle of the cones given by \(\{^{d}:(2_{i}-_{})\}\). Closed form expressions exist for \(d=2,3\), and  present approximating schemes for higher dimensions. We calculate these weights exactly for the 1D example presented in Figure 1, and estimate them using Monte Carlo sampling for the student-teacher example in Figure 2.

  
**Dataset** & \(n\) & \(d\) & **NTK** & **Ours (Alg 1)** \\  acute-inflammation & \(120\) & \(6\) & \(1.000\) & \(1.000\) \\ acute-nephritis & \(120\) & \(6\) & \(1.000\) & \(1.000\) \\ balloons & \(16\) & \(4\) & \(0.75\) & \(0.75\) \\ blood & \(748\) & \(4\) & \(0.524\) & **0.583** \\ breast-cancer & \(286\) & \(9\) & \(0.417\) & **0.625** \\ breast-cancer-wisc-prog & \(699\) & \(9\) & \(9.06\) & **0.966** \\ breast-cancer-wisc-diag & \(569\) & \(30\) & **0.965** & \(0.915\) \\ breast-cancer-wisc-prog & \(198\) & \(33\) & **0.7** & \(0.66\) \\ congressional-voting & \(435\) & \(16\) & \(0.266\) & **0.266** \\ conn-bench-sonar-mines-rocks & \(208\) & \(60\) & \(6.635\) & **0.712** \\ credit-approval & \(690\) & \(15\) & \(0.838\) & **0.844** \\ cylinder-bands & \(512\) & \(35\) & \(0.773\) & **0.82** \\ echocardiogram & \(131\) & \(10\) & \(0.758\) & **0.788** \\ fertility & \(100\) & \(9\) & \(0.76\) & \(0.76\) \\ haberman-survival & \(306\) & \(3\) & \(0.481\) & **0.532** \\ heart-huangarian & \(294\) & \(12\) & \(0.743\) & **0.878** \\ hepatitis & \(155\) & \(19\) & **0.923** & \(0.897\) \\ ilpd-indian-liver & \(583\) & \(9\) & \(0.432\) & **0.555** \\ ionosphere & \(351\) & \(33\) & \(0.955\) & **0.966** \\ mammographic & \(961\) & \(5\) & \(0.783\) & **0.792** \\ molec-biol-promoter & \(106\) & \(57\) & \(0.63\) & **0.815** \\ musk-1 & \(476\) & \(166\) & \(0.782\) & **0.866** \\ oocytes\_trisoperus\_nucleus\_2f & \(912\) & \(25\) & **0.781** & 0.759 \\ parkinsons & \(195\) & \(22\) & 0.939 & **0.959** \\ pima & \(768\) & \(8\) & \(0.552\) & **0.599** \\ pittsburg-bridges-T-OR-D & \(102\) & \(7\) & \(0.731\) & **0.846** \\ planning & \(182\) & \(12\) & \(0.435\) & **0.543** \\ stadog-australian-credit & \(690\) & \(14\) & **1.0** & \(0.74\) \\ stadog-german-credit & \(1000\) & \(24\) & \(0.512\) & **0.576** \\ stadog-heart & \(270\) & \(13\) & **0.779** & \(0.765\) \\ tic-ta-toe & \(958\) & \(9\) & \(1.0\) & \(1.0\) \\ trains & \(10\) & \(29\) & \(0.667\) & \(0.667\) \\ vertebral-column-2classes & \(310\) & \(6\) & **0.821** & \(0.731\) \\   &  &  \\ 

Table 1: Test accuracies for UCI experiments with \(75\%-25\%\) training-test split. Our approach achieves either higher or the same accuracy for \(26\) out of \(33\) datasets.

Figure 2: Plot of the **group lasso objective** in the student-teacher setting with \(d=5\). Training data is generated using a teacher network with width \(m=10\). The NTK weights \(}\) are estimated using Monte Carlo sampling, and are again sub-optimal. IRLS initialized with these weights successfully fixes the NTK and converges to the optimal weights.

**Fixing the NTK weights by IRLS.** We solve (16) with \(_{i}=_{i}\) and regularization weights given by the NTK weights \(}\) to obtain the solution \(}\). We use efficient least squares solvers from . By Theorem 5.1, this corresponds to choosing the weights \(\) in the MKL problem (13) such that the resulting kernel matrix is equal to the NTK matrix \(\). We find the exact solution of the group lasso problem (8) using CVXPY. Comparing the optimal value of the group lasso problem (8) with the objective value of \(}\) (given by the green line) in Figures 1 and 2, we observe that the NTK weighted solution is sub-optimal. This means that \(\) is not the optimal kernel that would be learnt by MKL (which is expected since \(\) has no dependence on the targets \(\)). By applying IRLS initialized with the NTK weights, we _fix_ the NTK and find the weights of the optimal MKL kernel. In Figures 1 and 2, we observe that IRLS converges to the solution of the group lasso problem (8) and fixes the NTK.

**UCI datasets.** We compare the regularized NTK with our IRLS algorithm (Algorithm 1) on the UCI ML Repository datasets. We follow the procedure described in  for \(n 1000\) to extract and standardize the datasets. We observe that our method achieves higher (or the same) test accuracy for \(26\)**out of \(33\)** datasets (see Table 1 for details) while the NTK achieves higher (or the same) test accuracy for \(14\) datasets which empirically supports our main claim that the IRLS procedure fixes the NTK. Details of the experiments can be found in Section B of the supplementary material.

## 8 Discussion and Limitations

In this work, we explored the connection between finite-width theories of neural networks given by convex reformulations and infinite-width theories of neural networks given by the NTK. To bridge these theories, we first interpreted the group lasso convex formulation of the gated ReLU network as a multiple kernel learning model using the masking kernels generated by its gates. Then, we linked this MKL model with the NTK of the gated ReLU network evaluated on the training data. Specifically, we showed that the NTK is equivalent to the weighted masking kernel with weights that depend only on the input data \(\) and not on the targets \(\). We contrast this with the MKL interpretation of the gated ReLU network which learns the optimal data-dependent kernel using both \(\) and \(\). Therefore, the NTK cannot perform better than the optimal MKL kernel. To fix the NTK, we improve the weights induced by it using the iteratively reweighted least squares (IRLS) scheme to obtain the optimal solution of the group lasso formulation of the gated ReLU network. We corroborated our theoretical results by empirically running IRLS on toy datasets.

While our theory is able to link the optimization properties of the NTK with those of finite width networks via the MKL characterization of group lasso, we do not derive explicit generalization results on the test set. Applying existing generalization theory for kernel methods  to the MKL interpretation of the convex reformulation could be a promising direction for future work.

Finally, although we studied fully connected networks in this paper, our approach can be directly extended to various neural network architectures, e.g., threshold/binary networks , convolution networks , generative adversarial networks , NNs with batch normalization , autoregressive models , and Transformers .