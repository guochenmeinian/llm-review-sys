# SkiLD: Unsupervised Skill Discovery

Guided by Factor Interactions

 Zizhao Wang\({}^{1}\)   Jiaheng Hu\({}^{1}\)1   Caleb Chuck\({}^{1}\)1   Stephen Chen\({}^{1}\)

Roberto Martin-Martin\({}^{1}\) Amy Zhang\({}^{1}\)   Scott Niekum\({}^{2}\)   Peter Stone\({}^{1,3}\)

\({}^{1}\)University of Texas at Austin  \({}^{2}\)University of Massachusetts, Amherst  \({}^{3}\)Sony AI

{zizhao.wang,jiahengh,stephem.chen,robertomm}@utexas.edu

amy.zhang@austin.utexas.edu, sniekum@umass.edu, {calebc,pstone}@cs.utexas.edu

###### Abstract

Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free environment interaction. Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states. However, in complex environments with many state factors (e.g., household environments with many objects), learning skills that cover all possible states is impossible, and naively encouraging state diversity often leads to simple skills that are not ideal for solving downstream tasks. This work introduces Skill Discovery from Local Dependencies (SkiLD), which leverages state factorization as a natural inductive bias to guide the skill learning process. The key intuition guiding SkiLD is that skills that induce **diverse interactions** between state factors are often more valuable for solving downstream tasks. To this end, SkiLD develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment. We evaluate SkiLD in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where SkiLD successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage. Code and visualizations are at https://wangzizhao.github.io/SkiLD/.

## 1 Introduction

Reinforcement learning (RL) achieves impressive successes when solving decision-making problems with well-defined reward functions [65; 20; 33]. However, designing this reward function is often not trivial [7; 61]. In contrast, humans and other intelligent creatures can learn, without external reward supervision, behaviors that produce repeatable and predictable changes in the environment . These behaviors, which we call _skills_, can be later repurposed to solve downstream tasks efficiently. One of the promises of this form of unsupervised RL is to endow artificial agents with similar capabilities to discover reusable skills without explicit rewards.

One predominant strategy of prior skill discovery methods focuses on training skills to reach diverse states while being distinguishable [19; 59; 50]. However, in complex environments that contain many _state factors_--distinct elements such as individual objects in a household (a formal description in Sec. 2.1), the exponential number of distinct states makes it impossible to learn skills that cover every state. Consequently, these methods typically result in simple skills that only change the easy-to-control factors (e.g., in a manipulation task moving the agent itself to diverse positions ormanipulating each factor independently), and fail to cover other desirable but challenging behaviors. Meanwhile, in a factored state space, many downstream tasks require inducing interactions between state factors, e.g., cooking requires using a knife to cut the ingredients and cooking them in a pan, etc. Unsurprisingly, these simple skills often struggle to solve such tasks, resulting in poor downstream performance.

Our key insight is to utilize interactions between state factors as a powerful inductive bias for learning useful skills. In factored state spaces and their downstream tasks, there usually exist bottleneck states that an agent must pass through to explore different regions of the environment, and many of them can be characterized by interactions between state factors. For example, in a household environment, a robot must first grasp the knife before moving it to different locations, with the bottleneck being the interaction between the robot and the knife. In environments that have a large state space due to many state factors, rather than inefficiently relying on randomly visiting different states to reach such bottlenecks, we propose to train the agent to actively induce these critical interactions.

To this end, we introduce Skill Discovery from Local Dependencies (SkiLD), a novel skill discovery method that explicitly learns skills that induce diverse interactions. Specifically, SkiLD models the interactions between state factors using the framework of _local dependencies_ (where local refers to state-specific, see details in Sec. 2.2) and proposes a novel intrinsic reward that 1) encourages the agent to induce specified interactions, and 2) encourages the agent to discover diverse ways of inducing specified interaction, as visualized in Figure 1. During skill learning, SkiLD gradually discovers new interactions and learns to induce them, based on the skills that it already mastered, resulting in a diverse set of interaction-inducing behaviors that can be readily repurposed for downstream tasks. During task learning, the skill policy is reused, and a task-specific policy is learned to select (a sequence of) skills to maximize task rewards efficiently.

We evaluate the performance of SkiLD on factor-rich environments with 10 downstream tasks against existing unsupervised reinforcement learning methods. Our experiments indicate that SkiLD learns to induce diverse interactions and outperforms other methods on most of the examined tasks.

## 2 Background

In this paper, our unsupervised skill discovery method is set up in a factored Markov decision process and builds off previous diversity-based methods, as described in Sec. 2.1. To enhance the expressivity of skills, our method further augments the skill representation with interactions between state factors, which we formalize as local dependencies as described in Sec. 2.2.

### Factored Markov Decision Process (Factored MDP)

We consider unsupervised skill discovery in a reward-free Factored Markov Decision Process  defined by the tuple \(=(,,\)\(p)\). \(=^{1}^{N}\) is a factored state space with \(N\) subspaces, where each subspace \(^{i}\) is a multi-dimensional continuous or discrete random variable. Then, correspondingly, each state \(s\) consists of \(N\) state factors, i.e., \(s=(s^{1},,s^{N}),s^{i}^{i}\). In this paper, we use uppercase letters to denote random variables and lowercase for their specific values (e.g.,

Figure 1: **Skill Discovery from Local Dependencies** (SkiLD) describes skills that encode interactions (i.e., local dependencies) between state factors. In contrast to prior diversity-based methods that can easily get stuck by moving the robot to diverse, but non-interactive states, and factor-based methods that are trained to manipulate the hammer and nail, but not their interactions, SkiLD not only manipulate each object (left, middle) but also induce interactions between them (right), by specifying different local dependencies. These skills are often more useful than the “easy” skill learned by previous methods for downstream task-solving.

\(S\) denotes the random variable for states \(s\)). \(\) is the action space, and \(p\) is an unknown Markovian transition model that captures the probability distribution over the next state \(S^{} p(|S,A)\).

The factorization in \(\) inherently exists in many environments, and is a common assumption in prior unsupervised skill discovery works [22; 29; 27]. For example, in robotics, an environment typically consists of a robot and several objects to manipulate, and, for each object, \(S^{i}\) would represent its attributes of interest, like pose. In this work, we explore how we can utilize a given state factorization to improve unsupervised skill discovery. In practice, the factorization can either be directly provided by the environment or obtained from image observations with existing disentangled representation learning methods [47; 31].

Following prior work, our method consists of two stages--skill learning and task learning. During the skill learning phase, we seek to learn a skill policy \(_{}(|s,z)\), which defines a conditional distribution over actions given the current state \(s\) and some skill representation \(z\), where skills indicate the desired behaviors of the agent. Once the skills are learned, they can be chained together to solve downstream tasks during the task learning phase through an extrinsic reward-optimizing policy. During task learning, a downstream task reward function \(r:\) is provided by the environment. A high-level policy \((z|s)\) is then trained to optimize the expected return through outputting correct skills \(z\) given state \(s\).

### Identifying Local Dependencies between State Factors

A key insight of SkiLD is to utilize interactions between state factors (or, formally, local dependencies) as part of the skill representation. In later sections, these local dependencies are compiled into a binary matrix \((s,a,s^{})=\{0,1\}^{N(N+1)}\) representing the local dependencies between all factors. In this section, we first formally define local dependencies, introduce their identification, and finally discuss their application to factored MDPs.

SkiLD takes a causality-inspired approach for defining and detecting local dependencies [6; 58], where we use _local_ to refer to a particular assignment of values for a random variable, as opposed to _global_ which applies to all values. Formally, for an _event of interest_\(Y=y\) and its potential causes \(X=(X^{1},,X^{N})\), given the value of \(X=x\), local dependencies focus on which \(X^{i}\)s are the state-specific cause of the outcome event \(Y=y\) (for simplicity of presentation, in this section we overload \(N\) as the number of potential causes rather than number of variables and \(p\) as the transition function according to a subset of the variables). Formally, we denote the general data generation process of \(Y\) as \(p:X Y\) and the data generation process when \(Y\) is _only influenced_ by a subset of \(X\) as \(p^{}: Y\), where \( X\). Then, given the value of all variables, \(X^{1}=x^{1},,X^{N}=x^{N}\) and \(Y=y\), we say \(Y\) locally depends on \(\), if \(\) is the _minimal_ subset of \(X\) such that knowing their values is necessary and sufficient to generate the result of \(Y=y\), i.e.,

\[*{arg\,min}_{ X}|| p ^{}(Y=y|=)=p(Y=y|X=x),\] (1)

where \(||\) is the number of variables in \(\). For example, suppose that a robot opens a refrigerator door in a particular transition. The event of interest \(Y=y\) is the refrigerator door becoming open, and it locally depends on two factors: the robot and the refrigerator door, while other state factors such as objects inside the refrigerator do not locally influence \(Y\).

To identify local dependencies, one can conduct a conditional independence test \(y\!\!\! x^{i}|\{x/x^{i}\}\) to examine whether a variable \(X^{i}\) is necessary for predicting \(Y=y\). In prior works, one form of this test is to examine whether the pointwise conditional mutual information (pCMI) is greater than 0,

\[(y;x^{i}|\{x/x^{i}\})=)}(y|\{x/x^{i} \})}>0.\] (2)

If so, then it suggests that knowing \(X^{i}=x\) provides additional information about \(Y\) that is not present in \(\{X/X^{i}\}\), and \(Y\) locally depends on \(X^{i}\). As the data generation processes are generally unknown, one has to approximate them with learned models. Recent work in RL has utilized various approximations such as attention weights , Granger causality , and input gradients .

In this work, for a transition \((S=s,A=a,S^{}=s^{})\), the event of interest is each next state factor being \((S^{i})^{}=(s^{i})^{}\), and we infer whether it locally depends on each state factor \(S^{j}\) and the action \(A\) (i.e., whether there is an interaction between state factors \(i\) and \(j\), where factor \(j\) influences \(i\)). Thenwe aggregate all local dependencies into a state-specific dependency graph (abbreviated in this work to _dependency graph_). This overall dependency graph is represented with \((s,a,s^{})=\{0,1\}^{N(N+1)}\), and an edge \(^{ij}(s,a,s^{})\) denotes, during the transition \((s,a,s^{})\), that state factor \((s^{i})^{}\) (the "\(Y=y\)") locally depends on \(s^{j}\) (one of the \(X^{j}\)):

\[^{ij}((x^{i})^{};x^{j}|\{x/x^{j}\})>0\] (3)

This graph is used to enhance skill representation, as explained in detail in Section 3.

## 3 Skill Discovery from Local Dependencies (SkiLD)

In this section, we describe SkiLD, which enhances skills using local dependencies. SkiLD represents local dependencies as _state-specific dependency graphs_, defined in Sec. 2.2, and learns to induce different dependency graphs in the environment for different skills. To intelligently generate target dependency graphs during training, SkiLD frames unsupervised skill discovery as a hierarchical RL problem described in Fig. 2 and Alg. 1, where a high-level graph selection policy chooses target local dependencies to guide exploration and skill learning, and a graph-conditioned skill policy learns to induce the specified local dependencies using primitive actions.

This framework requires formalizing three components: (1) the skill representation \(\), presented in Sec. 3.1, (2) the graph selection policy \(_{}(z|s)\) and its reward function \(_{}\), presented in Sec. 3.2, and (3) the skill policy \(_{}(a|s,z)\) and its corresponding reward function \(_{}\), presented in Sec. 3.3.

### Skill Representation \(\)

Prior unsupervised skill discovery methods usually focus skill learning on changing the state or each factor diversely, which is inefficient when there exist bottleneck states for explorations. Consequently, they are can be limited to learning simple skills, for example, only changing the easiest-to-control factor in the state (i.e., the agent itself). To address this problem, SkiLD not only focuses on changing the state but also considers the interactions between state factors.

**Skill Representation.** SkiLD represents the skill space as the combination of two components: \(=\), where \(g\) is a state-specific dependency graph that specifies the _desired_ local dependencies between state factors (e.g., hammering the nail), and \(b\) is a diversity indicator the same as that used in Eysenbach et al. . While the agent inducing particular local dependencies \(g\), we use \(b\) to further encourage it to visit distinguishable states (e.g., under different \(b\) values, training the agent to hammer the nail into different locations). Specifically, the dependency graph is represented as a binary matrix \(=\{0,1\}^{N(N+1)}\). As described in Sec. 2.2, each edge \(^{ij}\) denotes, during the transition \((s,a,s^{})\), whether the state factor \((s^{i})^{}\) locally depends on \(s^{j}\). The diversity indicator \(\) can be discrete or continuous. In this work, without loss of generality, we follow the procedure of Eysenbach et al.  and use a discrete \(b\) sampled uniformly from \(\{1,,K\}\), where \(K\) is a predefined number.

Given this skill space, SkiLD learns the skills as a skill-conditioned policy \(_{}:\), where \(_{}\) is trained to reach diverse states while ensuring that local dependencies specified by the graph

Figure 2: During **skill learning** of SkiLD, the graph-selection policy specifies desired local dependencies for the skill policy to induce, and the induced dependency graph is identified by the dynamics model and used to update both policies. During **task learning** (right), the skill policy is kept frozen and a task policy is trained to select skills to maximize task reward.

are induced. Before we describe \(_{}\) training in Sec. 3.3, we first discuss how to select the skill \(z\) for \(_{}\) to follow during the skill learning stage.

### High-Level Graph-Selection Policy \(_{}\)

To acquire skills that are useful for downstream tasks, the skill policy \(_{}\) needs to learn to induce a wide range of local dependencies _sample- efficiently_. To this end, we propose to learn a graph-selection policy \(_{}:\) to guide the training of \(_{}\). Specifically, training \(_{}\) requires a wise selection of graphs -- as graph space \(\) increases super-exponentially in the number of state factors \(N\), many graphs are not inducible. To this end, we only select target graphs for the skill policy from a history of all seen graphs. As the agent learns to induce existing graphs in diverse ways, new graphs may be encountered, gradually expanding the set of seen graphs.

However, though this history guarantees graph inducibility, two challenges still remain: (1) How to efficiently explore novel local dependencies, especially hard-to-visit ones? (2) For all seen graphs, which one should \(_{}\) learn next to maximize training efficiency? We address these challenges based on the following insight -- compared to well-learned skills, \(_{}\) should focus its training on underdeveloped skills. Meanwhile, learning new skills opens up the possibility of visiting novel local dependencies, e.g., learning to grasp the hammer makes it possible for the robot to hammer the nail.

According to this idea, we learn a graph-selection policy \(_{}\) that guides the exploration and training of the skill policy \(_{}\). Specifically, \(_{}:\) selects a new dependency graph the skill policy should induce for the next \(L\) time steps. To increase the likelihood of visiting hard graphs, \(_{}\) is trained to maximize the following graph novelty reward

\[_{}=})}},\] (4)

where \(C(g_{})\) is the number of times that we have seen the graph in the collected transition.

### Low-Level Skill Policy \(_{}\)

Given the skill parameter \(z\) from the graph-selection policy, SkiLD learns skills as a skill-conditioned policy \(_{}:\), where \(_{}\) learns to reach diverse states while ensuring that the local dependencies specified by \(g\) are induced. During skill learning, we select actions by iteratively calling the skill policy \(_{}\), and we denote \(g_{}(s,a,s^{})\) as the graph that describes the local dependencies induced in a transition \((s,a,s^{})\) when executing a selected action \(a\). We design the reward function of the skill policy as:

\[_{}=[g_{}(s,a,s^{})=g] (1+_{}),\] (5)

where \([g_{}(s,a,s^{})=g]\) measures whether the induced dependency graph matches the desired graph, \(_{}\) is the weighted diversity reward that further encourages visiting diverse states when the desired graph is induced, and \(\) is the coefficient of diversity reward. In the following paragraphs, we describe how we infer \(g_{}(s,a,s^{})\) and estimate \(_{}\) for each transition.

**Inferring Induced Graphs.** To infer the induced graph for a transition \((S=s,A=a,S^{}=s^{})\), we need to determine, for each \((^{})^{i}\), whether it locally depends on each factor \(^{j}\) and the action \(\). Following Sec. 2.2, we evaluate the conditional dependency \((s^{i})^{}\). \(s^{j}|\{s/s^{j},a\}\) by examining whether their pointwise conditional mutual information (pCMI) is greater than a predefined threshold \(\). If pCMI\({}^{ij}=)^{i}|s,a)}{p((s^{i})^{}|\{s/s^{j},a\})}\), it suggests that \(s^{j}\) is necessary to predict \((s^{i})^{}\) and thus the local dependency exists. Meanwhile, as the transition probability \(p\) is unknown, we approximate it with a learned dynamics model that is trained to minimize prediction error.

Finally, after obtaining the induced dependency graph, we evaluate \([g_{}(s,a,s^{})=g]\) by examining whether each edge \(g_{}^{ij}\) matches the corresponding edge in the desired graph \(g^{ij}\). As \(_{}\) only provides sparse rewards to the skill policy when the desired graph is induced, we use hindsight experience replay  to enrich learning signals, by relabelling induced graphs as desired graphs in some episodes.

**Diversity Rewards.** When the skill policy induces the desired graph, \(_{}\) further encourages it to visit different distinguishable states under different diversity indicators \(b\), e.g., hammering the nail to different locations. This diversity enhances the applicability of learned skills. To this end, we design the diversity reward \(_{}\) as the forward mutual information between visited states and the diversity indicator \(I(s;b)\), following DIAYN. To estimate the mutual information, we approximate it with a variational lower bound \(I(s;b)_{b,s} q(b|s)\), where \(q(b|s)\) is a neural network discriminator trained to predict the diversity indicator \(b\) from the visited state.

In practice, rather than learning a single low-level skill to handle all graphs, SkiLD utilizes a factorized lower-level policy. When the target dependency graph is specified, SkiLD identifies which state factor should be influenced and uses its corresponding policy to sample primitive actions. More details about this subdivision can be found in Appendix A.

### Downstream Task Learning

In SkiLD, after the skill learning stage, we utilize hierarchical RL to solve reward-supervised downstream tasks with the discovered skills. The skill policy, \(_{}\) acts as the low-level policy while a task policy, \(_{}:\), is learned to select which skill \(z=(g,b)\) to execute for \(L\) steps. Compared to diversity-based skills that are limited to simple behaviors, our local-dependency-based skills enable a wide range of interactions between state factors, leading to more efficient exploration and superior performance of downstream task learning.

## 4 Experiments

In this section we aim to provide empirical evidence towards the following questions: **Q1)** Do the skills learned by SkiLD induce a diverse set of interactions among state factors? **Q2)** Do the skills learned by SkiLD enable more efficient downstream task learning compared to other unsupervised reinforcement learning methods? Our learned skills are visualized at https://sites.google.com/view/skild/.

### Domains

In this work, we focus on addressing the challenge of vast state space brought by the number of state factors. Hence, we evaluate our method on two challenging _object-rich_ embodied AI benchmarks: Mini-behavior  and Interactive Gibson .

The **Mini-behavior (Mini-BH) domain** (Figure 2(a)) contains a set of gridworld environments where an agent can move around and interact with a variety of objects to accomplish certain household tasks. While conceptually simple, due to highly **sequentially interdependent** state factors (see details in the Appendix), this domain has been shown to be extremely challenging for the agent's exploration ability, especially under sparse reward . Each Mini-BH environment contains different objects and different success criteria. We tested on three particular environments in Mini-behavior, including:

* **Installing Printer**: A relatively simple environment with three state factors: the agent, a table, and a printer that can be installed.
* **Cleaning Car**: An environment where the objects have rich and complex interactions. The state factors include the agent, a toggleable sink, a piece of rag that can be soaked in the sink, a car that the rag can clean, a soap and a bucket which can together be used to clean the rag.
* **Thawing**: An environment with lots of movable objects. The state factors include the agent, a sink, a fridge that can be opened, and three objects that can be thawed in the sink: fish, olive, and a date.

The **Interactive Gibson (iGibson)** domain  (Figure 2(b)) contains a realistic simulated Fetch Robot that operates in a kitchen environment with a refrigerator, sink, knife, and peach. The peach can be washed or cut. This domain is very difficult especially when using low-level motor commands because much of the domain is free space, meaning that only a minute fraction of action sequences will manipulate the objects meaningfully.

Both Mini-BH and iGibson require learning long-horizon policies spanning many low-level actions from sparse reward, making these challenging domains (see details in Appendix).

### Baselines

Before evaluating the empirical questions, we provide a brief description of the baselines. These baselines include unsupervised skill learning, and causal and hierarchical methods.

**Diversity is all you need** (DIAYN ): This method learns unsupervised state-covering skills using a mutual information objective. SkiLD utilizes a version of this for state-diversity skills modulated by a desired dependency graph. This baseline determines how incorporating graph information affects the algorithm.

**Controllability-Aware Skill Discovery** (CSD ): Extends DIAYN with a factorization based on controllability. This baseline is a comparable skill learning method that leverages state factorization but does not encode local dependencies.

**Exploration via Local Dependencies** (ELDEN ): This method utilizes gradient-based techniques to infer local dependencies for exploration. However, without a skill learning component, it can struggle to chain together complex behavior.

**Chain of Interaction Skills** (COInS ): This is a hierarchical algorithm that constructs a chain of skills using Granger-causality to identify local dependencies. Because it is restricted to pairwise interactions, it struggles to represent the rich policies necessary for these tasks.

**Vanilla RL**: This baseline uses PPO  to directly train an agent with the extrinsic reward. Unlike other baselines, this method does not have a pertaining phase. Since all the task rewards are sparse and the tasks are often long horizon, vanilla RL often struggles.

Figure 3: **Evaluation domains**: Mini-behavior: Installing Printer, Thawing and Cleaning Car, and iGibson.

### Interaction Graph Diversity

We first evaluate whether SkiLD is indeed capable of achieving complex interaction graphs (Q1), comparing against two strong skill discovery baselines introduced earlier: DIAYN and CSD.

Each of these methods is trained for 10 Million steps without having access to any reward. Then to evaluate their learned skills, we unroll each of them for 500 episodes with randomly sampled skills \(z\) and examine the diversity of the interaction graphs they can induce. Figure 4 illustrates the percentages of episodes where some hard local dependencies have been induced at least once, in Mini-BH Cleaning Car (for simplicity of presentation, see Appendix for results on all inducible local dependency graphs and their meanings). We find that DIAYN and CSD are limited to skills that only manipulate one object individually, for example, picking up the rag (agent, rag, action \(\) rag) or the soap (agent, soap, action \(\) soap). By contrast, SkiLD learns to induce more complicated causal interactions, such as soaking the rag in the sink (sink, rag \(\) rag) and cleaning the car with the soaked mug (car, rag \(\) car).

### Sample Efficiency and Performance

Next, we evaluate whether the local dependency coverage provided by SkiLD leads to a performance boost in downstream task learning under the same number of environment interactions (Q2). We follow the evaluation setup in the unsupervised reinforcement learning benchmark , where for a given environment, an agent is first pre-trained without access to task reward for \(K_{}\) steps, and then finetuned for \(K_{}\) steps. Importantly, the same pre-trained skills are reused on multiple distinct downstream tasks within the same environment, so that only the upper-level skill-selection policy is task-specific. We have \(K_{}=2M\), \(K_{}=1M\) for installing printer, \(K_{}=10M\), \(K_{}=5M\) for thawing and cleaning car, and \(K_{}=4M\), \(K_{}=2M\) for iGibson, and evaluate each method for each task across 5 random seeds. Hyperparameter details can be found in Appendix D. Specifically, we evaluate on the following downstream tasks:

* **Installing Printer**: We have a single downstream task in this environment, where the agent needs to pick up the printer, put it on the table, and turn it on.
* **Thawing**: We have three downstream tasks: thawing the fish or the olive or the date.
* **Cleaning Car**: We consider three downstream tasks, where each task is a pre-requisite of the following one. The tasks are: soak the rag in the sink; clean the car with the rag; and clean the dirty rag using the soap in the bucket.
* **IGibson**: The tasks for this domain are: grasping the peach, washing the peach in the sink, and cutting the peach with a knife.

After skill learning, we train a new upper-level policy that uses \(z\) as actions and is trained with extrinsic reward, as described in Section 3.4. Figure 5 illustrates the improvement of SkiLD as compared to other methods. Without combining dependency graphs with skill learning, other methods struggle with any but the simpler tasks. COInS performs poorly because of its chain structure, which restricts the agent controlling policy from picking up objects. ELDEN's exploration reaches graphs, but without skills struggles to utilize that information in downstream tasks. DIAYN learns skills, but few manipulate the objects, so a downstream model struggles to utilize those skills to achieve meaningful rewards. By comparison, SkiLD achieves superior performance on 9 of the 10 downstream tasks evaluated. In the two hardest tasks which require a very long sequence of precise controls, Clean Rag

Figure 4: The percentage of episodes where a dependency graph is induced through random skill sampling. Standard deviation is calculated across five random seeds.

and Cut Peach, SkiLD is the only method that can achieve a non-zero success rate (although still far from fully mastering the tasks), showcasing the potential of local dependencies for skill learning.

### Graph and Diversity Ablations

We also explore the functionality of the graph and diversity components of the skill parameter \(z\) by assessing the downstream performance of SkiLD without these components. This produces two ablative versions of SkiLD: SkiLD without diversity and SkiLD without dependency graphs. To isolate learning from the effect of learned local dependencies, we use ground truth dependency graphs for ablative evaluations where relevant. In Figure 6, learning without graphs results in zero performance, consistent with DIAYN results. In addition, removing diversity produces a notable decline in performance, especially on more challenging tasks like learning the rag. These evaluations demonstrate that SkiLD benefits from both the incorporation of dependency graphs and diversity.

## 5 Related Work

This work lies in the unsupervised skill learning framework , where the agent must discover a set of useful skills which are reward independent. It then extends these skills to construct a 2-layer hierarchical structure , where the upper policy receives reward both for achieving novel skills, and can then be tuned to utilize the learned skills to accomplish an end task. Finally, the skills are identified using token causality, a specific problem identified in causal literature.

### Unsupervised Skill Learning

This work describes a framework for utilizing local dependency graphs and diversity to discover unsupervised skills. Diversity-based state coverage skills have been explored in literature  utilizing forward and backward mutual information techniques to learn a goal space \(\), and a skill encoder \(q(z|)\). This unsupervised paradigm has been extended with Lipschitz constraints , contrastive objectives , information bottleneck , population based methods such as particle estimation , quality diversity  and mixture of experts . These skills can then be used for hierarchical policies or planners [56; 67; 23], which mirrors the same structure as SkiLD. Unlike these methods, SkiLD adds additional subdivision through dependency graphs, which mitigates the combinatorial explosion of skills that can result from trying to cover a large factored space.

### Hierarchical Reinforcement Learning

The hierarchical policy structure in SkiLD where a higher level policy passes a parameter to be interpreted by low-level planners has been formalized in , and learned using deep networks utilizing

Figure 5: Training curves of SkiLD and baselines on multiple downstream tasks (reward supervised second phase). Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. SkiLD outperforms all baselines for most tasks, converging faster and to higher returns.

extrinsic reward [3; 62], attention mechanisms , initiation critera [34; 4] and deliberation cost . Hierarchies of goal-based policies  has been extended with object-centric representations , offline data , empowerment  and goal counts . In practice, SkiLD uses graph and diversity parameters similar to goal-based methods. However, the space of goals can often be intractable large, and methods to address this use graph laplacians  causal chains [13; 14] or general causal relationships . SkiLD is similar to these causal methods but utilizes local dependence along with general two-layer architectures, thus showing increased generalizability.

### Causality in Reinforcement Learning

This work investigates the application of local dependency to hierarchical reinforcement learning. This kind of reasoning has been described as "local causality" or "interactions" in prior RL work for data augmentation [53; 54], learning skill chains [13; 14] and exploration . This work is the first synthesis of unsupervised skill learning and local dependencies applied to general 2-layer hierarchical reinforcement learning. Other general causality work investigates action-influence detection [58; 28], affordance learning , model learning [30; 21], critical state identification , and disentanglement . In the context of relating local dependency and causal inference, we provide a discussion in Appendix C. SkiLD incorporates causality-inspired local dependence to skill learning, resulting in a set of diverse skills.

## 6 Conclusion

Unsupervised skill discovery is a powerful tool for learning useful skills in long-horizon sparse reward tasks. However, many unsupervised skill-learning methods do not take advantage of factored environments, resulting in poor performance in complex environments with several objects. Skill Discovery from Local Dependencies utilizes state-specific dependency graphs, identified using learned pointwise conditional mutual information models, to guide skill discovery. The framework of defining skills according to a dependency graph and diversity goal, combined with a learned sampling scheme, achieves difficult downstream tasks. In domains where hand-coded primitive skills are typically given to the agent, like Mini-behavior and Interactive Gibson, SkiLD can achieve high performance without requiring explicit domain knowledge. These impressive results arise intuitively from incorporating local dependencies as skill targets, illuminating a meaningful direction for unsupervised skill learning to be applied to a wider array of environments.

**Limitations and Future Work** An important assumption of SkiLD is its access to factored state space. While factored state space can often be naturally obtained from existing RL benchmarks and many real-world environments, developments in disentangled representation learning [47; 31] will help with extending SkiLD to unfactored image domains. Secondly, SkiLD requires accurate detection of local dependencies. While off-the-shelf methods [63; 58] work well for detecting local dependencies in our experiments, future works that can more accurately detect local dependencies will be beneficial to the performance of SkiLD.

Figure 6: A figure illustrating the ablative performance of SkiLD without diversity or without graphs. Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. Without graphs, the method collapses completely, while removing diversity results in a noticeable reduction in downstream performance.

Acknowledgement

This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (W911NF-23-2-0004, W911NF-17-2-0181), Lockheed Martin, and Good Systems, a research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.