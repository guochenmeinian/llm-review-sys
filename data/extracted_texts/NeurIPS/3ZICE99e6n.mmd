# ReTR: Modeling Rendering Via Transformer for

Generalizable Neural Surface Reconstruction

 Yixun Liang\({}^{1}\)  Hao He\({}^{1,2}\)  Ying-Cong Chen\({}^{1,\,2}\)

\({}^{1}\) The Hong Kong University of Science and Technology (Guangzhou).

\({}^{2}\) The Hong Kong University of Science and Technology.

yliang982@connect.hkust-gz.edu.cn, hheat@connect.ust.hk, yingcongchen@ust.hk

Equal contribution.Corresponding author

###### Abstract

Generalizable neural surface reconstruction techniques have attracted great attention in recent years. However, they encounter limitations of low confidence depth distribution and inaccurate surface reasoning due to the oversimplified volume rendering process employed. In this paper, we present Reconstruction TRansformer (ReTR), a novel framework that leverages the transformer architecture to redesign the rendering process, enabling complex render interaction modeling. It introduces a learnable _meta-ray token_ and utilizes the cross-attention mechanism to simulate the interaction of rendering process with sampled points and render the observed color. Meanwhile, by operating within a high-dimensional feature space rather than the color space, ReTR mitigates sensitivity to projected colors in source views. Such improvements result in accurate surface assessment with high confidence. We demonstrate the effectiveness of our approach on various datasets, showcasing how our method outperforms the current state-of-the-art approaches in terms of reconstruction quality and generalization ability. _Our code is available at_[https://github.com/YixunLiang/ReTR](https://github.com/YixunLiang/ReTR).

## 1 Introduction

In the realm of computer vision and graphics, extracting geometric information from multi-view images poses a significant challenge with far-reaching implications for various fields, including robotics, augmented reality, and virtual reality. As a popular approach to this problem, neural implicit reconstruction techniques  are frequently employed, generating accurate and plausible geometry from multi-view images by utilizing volume rendering and neural implicit representations based on the Sign Distance Function (SDF)  and its variant. Despite their efficacy, these methods possess inherent limitations such as the lack of cross-scene generalization capabilities and the necessity for extensive computational resources for training them from scratch for each scene. Furthermore, these techniques heavily rely on a large number of input views.

Recent studies such as SparseNeuS  and VolRecon  have attempted to overcome these challenges by integrating prior image information with volume rendering methods, thereby achieving impressive cross-scene generalization capabilities while only requiring sparse views as input. Nevertheless, these solutions are still based on volume rendering, which poses some intrinsic drawbacks in surface reconstruction. Specifically, volume rendering is a simplification of the physical world and might not capture the full extent of its complexity. It models the interactions of incident photons and particles into density, which is predicted solely based on sampled point features. This oversimplified modeling fails to disentangle the contribution of the light transport effect and surface properties to the observed color, resulting in an inaccurate assessment of the actual surface. Moreover, the prediction of colorblending heavily relies on the projected color of the source views, thereby overlooking intricate physical effects. These shortcomings can lead to less confident surface predictions, producing a depth distribution with low kurtosis, and may be accompanied by high levels of noise, thereby compromising the overall quality of the reconstruction as illustrated in Fig. 1 (b).

In this paper, we first propose a more generalized formulation for volume rendering. Then based on this formulation, we introduce the Reconstruction TRansformer (ReTR), a novel approach for generalizable neural surface reconstruction. Our method utilizes the transformer to redesign the rendering process, allowing for accurate modeling of the complex render interaction while retaining the fundamental properties that make volume rendering effective. Particularly, we propose a learnable token, the _meta-ray token_, that encapsulates the complex light transport effect. By leveraging this token and the "cross-attention" mechanism, we simulate the interaction of rendering with the sampled points and aggregate the features of each point to render the observed color in an end-to-end manner. Additionally, we introduce a unidirectional transformer and continuous positional encoding to simulate photon-medium interaction, effectively considering occlusion and the interval of sample points. Moreover, as our model operates in the feature space rather than the color space, to further enhance the accuracy of semantic information and enable seamless adaptation, we propose a novel hybrid extractor designed to efficiently extract 3D-aware features.

Our method provides an excellent solution for generalizable neural surface reconstruction, offering several advantages over traditional volume rendering techniques. First, the ability to generalize complicated physical effects in a data-driven way provides an efficient approach to decoupling the contribution of light transport and surface to the observed color. Second, the entire process takes place in a high-dimensional feature space, rather than the color space, reducing the model's sensitivity to projected color in source views. Moreover, the transformer employs a re-weighted _softmax_ function of the attention map, driving the learning of a depth distribution with positive kurtosis. As a result, our method achieves a more confident distribution, leading to reduced noise and improved quality, As shown in Fig. 1 (c).

In summary, our contribution can be summarized as:

* We identify the limitation and derive a general form of volume rendering. By leveraging this form, we can effectively tailor the rendering process for task-specific requirements.
* Through the derived general form, we propose ReTR, a learning-based rendering framework utilizing transformer architecture to model light transport. ReTR incorporates continuous positional encoding and leverages the hybrid feature extractor to enhance performance in generalizable neural surface reconstruction.
* Extensive experiments conducted on DTU, BlendedMVS, ETH3D, and Tanks & Temples datasets [10; 11; 12; 13] validate the efficacy and generalization ability of ReTR.

Figure 1: Generalizable neural surface reconstructions from three input views in (a). VolRecon  produces depth distribution with low kurtosis and a noisy surface as shown in (b). In contrast, our proposed **ReTR** successfully extracts plausible surfaces with sharper depth distribution that has high kurtosis as shown in (c).

Related Works

Multi-View Stereo (MVS).Multi-view stereo methods is another branch for 3D reconstruction, which can be broadly categorized into three main branches: depth maps-based [14; 15; 16; 17; 18; 19], voxel grids-based [20; 21; 22], and point clouds-based [23; 24; 25]. Among these, depth maps-based methods are more flexible and hence more popular than the others. Depth maps-based methods typically decouple the problem into depth estimation and fusion, which has shown impressive performance with densely captured images. However, these methods exhibit limited robustness in situations with a shortage of images, thereby highlighting the problem we aim to address.

**Neural Surface Reconstruction.** With the advent of NeRF , there has been a paradigm shift towards using similar techniques for shape modeling, novel view synthesis, and multi-view 3D reconstruction. IDR  uses surface rendering to learn geometry from multi-view images, but it requires extra object masks. Several methods [2; 3; 1; 28; 29] have attempted to rewrite the density function in NeRF using SDF and its variants, successfully regressed plausible geometry. Among them, NeuS  first uses SDF value to model density in the volume rendering to learn neural implicit surface, offering a robust method for multi-view 3D reconstruction from 2D images. However, these methods require lengthy optimization to train each scene independently. Inspired by the recent success of generalizable novel view synthesis [30; 31; 32; 33; 34]. SparseNeuS  and VolRecon  achieve generalizable neural surface reconstruction using the information from the source images as the prior to neural surface reconstruction. However, these methods suffer from oversimplified modeling of light transport in traditional volume rendering and color blending, resulting in extracted geometries that fail to make a confident prediction of the surface, leading to distortion in the reconstructions. However, there are also some attempts [35; 36; 37] that adopt other rendering methods in implicit neural representation, such as ray tracing. However, these models are still based on enhanced explicit formulations; thus, their capacity to model real-world interaction is still limited. In contrast, our method introduces a learning-based rendering, providing an efficient way to overcome such limitations.

**Learning Based Rendering.** Unlike volume rendering, another line of work [38; 39; 40] explores deep learning techniques to simulate the rendering process. Especially Recurrent neural networks, which naturally fit the rendering process. Specifically, DeepVoxels  employs GRU to process voxel features along a ray, and its successor SRN , leverages LSTM for ray-marching. However, such methods recursively process features and demand large computation resources. The computation constraints inhibit such methods' ability to produce high-quality renderings. Unlike RNN-based methods, ReTR leverages the transformer to parallel compute each point's hitting probability. Greatly improve the efficiency of the rendering process and achieve high-quality renderings.

**Transformers With Radiance Field.** The attention mechanism in transformers  has also been widely used in the area of radiance field. In image-based rendering, IBRNet  proposes a ray transformer to process sampled point features and predict density. NeRFormer  utilizes a transformer to aggregate source views and construct feature volumes. NeuRays  leverages neural networks to model and address occlusions, enhancing the quality and accuracy of image-based rendering. GPBR  employs neural networks to transform and composite patches from source images, enabling versatile and realistic image synthesis across various scenes. However, these methods only use the transformer to enhance feature aggregation, and the sampled point features are still decoded into colors and densities and aggregated using traditional volume rendering, leading to unconfident surface prediction. Recently, GNT  naively replaces classical volume rendering with transformers in image-based rendering, which overlooks the absence of occlusion and positional awareness within the transformer architecture. In contrast to GNT, we improved the traditional transformer architecture in those two limitations based on an in-depth analysis of the fundamental components to make volume rendering work.

## 3 Methodology

In this section, we present an analysis of the limitations of existing generalizable neural surface reconstruction approaches that adopt volume rendering from NeRF  and propose ReTR, a novel architecture that leverages transformer to achieve learning-based rendering. We introduce the formulations of volume rendering and revisit its limitations in generalizable neural surface reconstruction in Sec 3.1. We then depict the general form of volume rendering in Sec. 3 and present our learning-based rendering in Sec. 3.3. To effectively extract features for our proposed rendering, we further introduce the hybrid extractor in Sec. 3.4. Our loss functions are explained in Sec. 3.5.

### Preliminary

Generalizable neural surface reconstruction aims to recover the geometry of a scene \(\), which is represented by a set of \(M\) posed input views \(=\{_{j},_{j}\}_{j=1}^{M}\), where \(_{j}^{H W 3}\) and \(_{j}^{3 4}\) are the \(j\)-th view's image and camera parameters, respectively. Existing approaches [9; 8] generate the features of the radiance field from \(\) using a neural network \(_{enc.}\), and we formalize the process as:

\[^{v},\{_{1}^{img},,_{M}^{img}\}= _{enc.}(), \]

where \(^{v}^{R R R D}\) is the volume feature with resolution \(R\) and \(_{j}^{img}^{h w D}\) is the image feature with dimension \(D\). To decode the color of a sampled ray \(=(,)\) passing through the scene, where \(\) and \(\) denotes as ray original and direction, the existing approaches [9; 8] sample \(N\) points along the ray from coarse to fine sampling between near and far planes and obtain the location \(^{3}\) of each sample point:

\[_{i}=+t_{i}, i=1,,N. \]

The predicted SDF from features first converts to weights using the conversion function , denoted as \(_{i}\). The weights are then used to accumulate the re-weighted projected colors along the ray using volume rendering. Specifically:

\[C()=_{i=1}^{N}T_{i}(1-(-_{i})) _{i}, T_{i}=(-_{j=1}^{i-1}_ {j}), \]

\[_{i}=_{j=1}^{M}_{weight.}(_{i}^{v},\{( _{k}^{img},_{i})\}_{k=1}^{M})(_{j},_{i}). \]

Here the \(_{weight.}()\) denotes the module that predicts the weight of each projected color. The \(_{i}^{v}\) represents the volume feature at \(_{i}\) obtained using trilinear interpolation in \(^{v}\). \((,)\) denotes the operation that projects \(\) onto the corresponding input grid \(\) and then extracts the feature at the projected location through bilinear interpolation.

**Limitation.** The volume rendering, denoted by Eq. (3), greatly simplifies light transport processes, thus introducing key limitations. A complete physical model for light transport categorizes the process into absorption, emission, out-scattering, and in-scattering. Each represents a nonlinear photon-particle interaction, with incident photon properties being influenced by both their inherent nature and medium characteristics. However, Eq. (3) condenses these complexities into a single density value, predicted merely based on sampled point features, leading to an _oversimplification_

Figure 2: Our ReTR pipeline comprises several steps: (1). Extracting features through the proposed hybrid extractor model from source views, (2). Processing features in each sample point using the feature fusion block, and (3). Using the occlusion transformer and render transformer to aggregate features along the ray and predict colors and depths.

of incident photon modeling_. Moreover, the color determination method, influenced by a weighted blend of projected colors akin to Eq. (4), _over-relies on input view projected colors, overlooking intricate physical effects_. As a result, the model requires a wider accumulation of projected colors from points near the exact surface, resulting in a "murky" surface appearance, as depicted in Fig. 1.

### Generalized Rendering Function

To overcome the limitations we mentioned in Sec. 3.1, we propose an improved rendering equation that takes into consideration of incident photon modeling and feature-based color decoding. As we revisit Eq. (3) and determine its key components to facilitate our redesign. This differentiable equation consists of three parts: The \(T_{i}\) term accounts for the accumulated transmittance and gives the first surface a bias to contribute more to the observed color. The \((1-(-_{i}))\) term denotes the alpha value of traditional alpha compositing, which represents the transparency of \(_{i}\) and is constrained to be non-negative. The color part of Eq. (3) denotes the color of \(_{i}\). Based on these analyses, we summarize three key rendering properties that the system must hold:

1. _Differentiable_. To enable effective learning, the weight function needs to be differentiable to the training network with observed color through back-propagation.
2. _Occlusion-aware_. In line with the bias towards the first surface in the original equation, the weight function needs to be aware that the points close to the first _exact surface_ should have a larger contribution to the final output color than other points.
3. _Non-negative_. Echoing the non-negativity constraint in the alpha value, the weight of each point also needs to be positive.

Having identified these key properties, we can now reformulate our approach to meet these requirements. Specifically, we propose a more general form of Eq. (3), which can be formalized as:

\[C()=_{i=1}^{N}(_{1},,_{i} )(_{i}), \]

where \(_{i}\) represents the set comprising image feature \(f^{img}\) and volume feature \(f^{v}\) in the \(_{i}^{3}\), \(()\) is the weight function that satisfies those three key properties we mentioned above, and and \(()\) denotes the color function. Specifically, color \(c\) can then be interpreted as characteristic of each feature point in 5. Therefore, feature at each point can be aggregated in a manner analogous to RGB value, and enabling us to deduce the primary feature points. This can be mathematically expressed as:

\[C()=C(_{i=1}^{N}(_{1},,_{i} )_{i}), \]

where the \(C()\) represents the color function that maps the feature into RGB space.

### Reconstruction Transformer

Note that Eq. (3) can be considered as a special form of Eq. (5). With this generalized form, we can reformulate the rendering function to overcome the oversimplification weakness of Eq. (3). Based on Eq. (5), we introduce the Reconstruction Transformer (ReTR) that preserves essential rendering properties while incorporating sufficient complexity to implicitly learn and model intricate physical processes. ReTR is composed of a Render Transformer and an Occlusion Transformer. The Render Transformer leverages a learnable "meta-ray token" to encapsulate complex render properties, enhancing surface modeling. The Occlusion Transformer utilizes an attention mask to enable the occlusion-aware property. Also, ReTR works in the high-dimensional feature space instead of the color space, and thus allows for more complex and physically accurate light interactions. Consequently, ReTR not only overcomes the restrictions of Eq. (3) but also preserves its fundamental rendering characteristics. We elaborate on the design of these components as follows.

**Render Transformer.** Here, we discuss the design of the render transformer. Specifically, we introduce a global learnable token, refer to as "meta-ray token" and denote as \(^{tok}^{D}\), to capture and store the complex render properties. For each sample ray, we first use the FeatureFusion block to combine all features associated with each sample point of the ray, resulting in FeatureFusion\((_{i})^{D}\). We then employ the cross-attention mechanism within the Render Transformer to simulate the interaction of sample points along the ray. It can be formalized as follows:

\[C()=(_{i=1}^{N}softmax(^{tok })k(_{i}^{f})^{}}{})v(_{i}^{f})), \]

where \(q(),k(),v()\) denotes three linear layers and \(()\) in this formulation is an MLP structure to directly regress observed color from the aggregated feature, and \(W()\) translates to \(softmax(^{tok})k(_{i}^{f})^{}}{})\). Furthermore, \(C()\) is operationalized as MLP, which serves to decode the integrated feature into its corresponding RGB value. Then, the hitting probability will be normalized by \(softmax\) function, which is _Non-negative_ and encourages the network to learn a weight distribution with positive kurtosis. And we can extract the attention map from the Render Transformer and derive the rendered depth as:

\[D()=_{i=1}^{N}_{i}t_{i}, _{i}=softmax(^{tok})k(_{i}^{f})^{}} {}), \]

where \(_{1},,_{N}\) denotes the attention map extracted from the Eq. (7). The rendered depth map can be further used to generate mesh  and point cloud .

**Occlusion Transformer.** To further make our system _Occlusion-aware_ and enable of simulation of photon-medium interaction. We introduce Occlusion Transformer. Similar to previous works [46; 48], we introduce an attention mask to achieve that the sample point interacts only with the points in front of it and the meta-ray token. Such unidirectional processes encourage the later points to respond to the preceding surface. This process can be formalized as:

\[^{f}=\{^{tok},_{1}^{f},_{2}^{f},, _{N}^{f}\}, \]

\[^{occ}= (Q,K,V=_{f}), \]

Where MHA denotes the multi-head self-attention operation  and \(_{i}^{occ}\) denotes the refine feature of \(_{i}\) which obtained from the render transformer. In addition, \(R^{f}\) is the collective set of tokens, and \(R^{occ}\) signifies the occlusion transformer that employs \(R^{f}\) for cross attention. Then, our Eq. (7) can be rewritten as:

\[C()=(_{i=1}^{N}softmax(^{ tok})k(_{i}^{occ})^{}}{})v(_{i}^{f})). \]

**Continuous Positional Encoding.** Following the traditional transformer design, we need to introduce a positional encoding to make the whole structure positional-aware. However, positional encoding proposed in  ignores the _actual distance_ between each token, which is unsuitable for our situation. Furthermore, weighted-based resampling  would lead to misalignment of positional encoding when an increased number of sample points are used.

To solve this problem, we extend the traditional positional encoding formula to continuous scenarios. Specifically, it can be formulated as follows:

\[PE_{(_{i},2i)} =sin( t_{i}/10000^{2i/D}), \] \[PE_{(_{i},2i+1)} =cos( t_{i}/10000^{2i/D}).\]

Here, \(i\) represents the positional encoding in the \(i_{th}\) dimension and \(\) is a scale hyperparameter we empirically set to 100. The updated formula successfully solves the misalignment of traditional positional encoding, results are shown in Tab. 4. The specific proofs will be included in the Appendix section.

### Hybrid Extractor

In our learning-based rendering, a finer level of visual feature is necessary, which is not achieved by traditional feature extractors that rely on high-level features obtained through FPN. These featuresare highly semantic abstract and not suitable for low-level visual feature matching . To overcome this limitation, inspired by NeuralRecon , we further propose Hybrid Extractor. Rather than relying on FPN to generate one feature map from high-level features of CNN, and using a 3D U-Net to process projected features as shown in Fig. 3, we leverage all level features from various layers to construct multi-level volume features. Then, we adopt a 3D CNN decoder to fuse and decode the multi-level volume features, producing the final global volume features.

Our approach enables us to perceive both low and high-level features, which is crucial for generalizable neural surface reconstructions that require detailed surface processing. Second, by avoiding the use of the encoder part of the 3D U-Net, we reduce the computational complexity and allow us to build a higher resolution volume feature within the same computational budget.

### Loss Functions

Our overall loss function is defined as the weighted sum of two loss terms:

\[=_{}+_{}, \]

where \(_{}\) constrains the observed colors to match the ground truth colors and is formulated as:

\[_{}=_{s=1}^{S}\|C()-C_{g}()\|_{2}, \]

Here, \(S\) is the number of sampled rays for training, and \(C_{g}()\) represents the ground truth color of the sample ray \(r\). The depth loss \(_{}\) is defined as

\[_{}=}_{s=1}^{S_{1}}|D( )-D_{g}()|, \]

where \(S_{1}\) is the number of pixels with valid depth and \(D_{g}()\) is the ground truth depth. In our experiments, we set \(=1.0\).

## 4 Experiments

**Datasets.** The DTU dataset  is a large-scale indoor multi-view stereo dataset consisting of 124 different scenes captured under 7 different lighting conditions. To train our frameworks, we adopted the same approach as in previous works [9; 8]. Furthermore, we evaluated our models' generalization capabilities by testing them on three additional datasets: Tanks & Templates , ETH3D , and BlendedMVS , where no additional training was performed on the testing datasets.

Figure 3: Comparision of the original extractor used in [8; 9] (top) and our **hybrid extractor** (bottom). The original extractor primarily discerns high-level features, demonstrating less efficacy. In contrast, our hybrid extractor excels in integrating multi-level features, demonstrating superior efficacy.

[MISSING_PAGE_FAIL:8]

### Generalization

To evaluate the generalization ability of our model without retraining, we use three datasets, namely Tank & Temples, BlendedMVS, and ETH3D [12; 11; 13]. The high-quality reconstruction of large-scale scenes and small objects in different domains, as shown in Fig. 6, demonstrates the effectiveness of our method in terms of generalization capability.

## 5 Ablation Study

We conduct ablation studies to examine the effectiveness of each module in our design. The ablation study results are reported on sparse view reconstruction in test split following SparseNeuS  and VolRecon .

**Effectiveness of Modules.** We evaluate key components of our approach to generalizable neural surface reconstruction, as shown in Tab. 3. For the evaluation of the occlusion transformer, we keep the original transformer architecture while removing the special design we proposed in Sec. 3.3, to ensure the training parameter would not affect the evaluation. For the hybrid extractor part, we replace this module with the original extractor that has been used in . Our results demonstrate that our approach can better aggregate features from different levels and use them more effectively. These evaluations highlight the importance of these components in our approach.

**Robustness of Different Sampling.** Tab. 4 displays the effects of altering the number of sample points on the reconstruction quality of VolRecon and ReTR. Our method surpasses the current SoTA, even when the number of sampling points decreases. These results suggest that existing methods that rely on sampling points of the ray struggle to provide confident predictions of the surface due to the nature of volume rendering. Our approach, which uses learning-based rendering, is more resilient to sampling strategies and can provide reliable depth estimations even with fewer samples. Meanwhile, the effectiveness of continuous P.E. in Sec. 3.3 is proved through the result.

**Unsupervised Neural Surface Reconstruction.** Our approach is still applicable to unsupervised neural surface reconstruction using only colors for training, which remove \(_{}\). Meanwhile, we find

   Method & Acc. \(\) & Comp. \(\) & Chamfer \(\) & \(<\)1mm \(\) & \(<\)2mm \(\) & \(<\)4mm \(\) \\  MVSNet  & 0.55 & 0.59 & 0.57 & 29.95 & 52.82 & 72.33 \\ SparseNeuS  & 0.75 & 0.76 & 0.76 & 38.60 & 56.28 & 68.63 \\ VolRecon  & 0.55 & 0.66 & 0.60 & 44.22 & 65.62 & 80.19 \\
**ReTR (Ours)** & **0.54** & **0.51** & **0.52** & **45.00** & **66.43** & **81.52** \\   

Table 2: Quantitative results of **full view** reconstruction on 15 testing scenes of DTU dataset . For the Accuracy (ACC), Completeness (COMP), and Chamfer Distance, the lower is the better. For depth map evaluation, threshold percentages (\(<\)1mm, \(<\)2mm, \(<\)4mm) are reported in percentage (%). The best scores are in **bold**.

Figure 5: Full view reconstruction visualization in test set of DTU , comparison with VolRecon  (left), our proposed ReTR (right) reconstructs better point clouds, _e.g._ fewer holes, the skull head top, and the house roof gives a much complete representation, _e.g._ finer details, the house window, and the skull cheek, provides much finer details. Best viewed on a screen when zoomed in.

    Reder \\ Trans. \\  &  Occ. \\ Trans. \\  & 
 Hybrid \\ Ext. \\  & Chamfer\(\) \\  ✓ & \(\) & \(\) & 1.31 \\ ✓ & \(\) & ✓ & 1.29 \\ ✓ & ✓ & \(\) & 1.28 \\ ✓ & ✓ & ✓ & **1.17** \\   

Table 3: Model component ablation. All of these parts are described in Sec. 3.3.

[MISSING_PAGE_FAIL:10]