# CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss

Rakshith S Srinivasa Jaejin Cho Cho Chochang Yang

Yashas Malur Saidutta Ching-Hua Lee Yilin Shen Hongxia Jin

Samsung Research America, Mountain View, CA

{r.srinivasa, jaejin.cho, c.yang1}@samsung.com

{ym.saidutta, chinghua.1, yilin.shen, hongxia.jin}@samsung.com

###### Abstract

This paper considers contrastive training for _cross-modal zero-shot transfer_ wherein a pre-trained model in one modality is used for representation learning in another domain using pairwise data. The learnt models in the latter domain can then be used for a diverse set of tasks in a zero-shot way, similar to "Contrastive Language-Image Pre-training (CLIP)"  and "Locked-image Tuning (LiT)"  that have recently gained considerable attention. Most existing works for cross-modal representation alignment (including CLIP and LiT) use the standard contrastive training objective, which employs sets of _positive_ and _negative_ examples to align similar and repel dissimilar training data samples. However, similarity amongst training examples has a more continuous nature, thus calling for a more _non-binary_ treatment. To address this, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL) that employs a continuous measure of similarity. With CWCL, we seek to align the embedding space of one modality with another. Owing to the continuous nature of similarity in the proposed loss function, these models outperform existing methods for zero-shot transfer across multiple models, datasets and modalities. Particularly, we consider the modality pairs of image-text and speech-text and our models achieve 5-8% (absolute) improvement over previous state-of-the-art methods in 0-shot image classification and 20-30% (absolute) improvement in 0-shot speech-to-intent classification and keyword classification.

## 1 Cross-modal alignment and transfer

Learning visual representations using natural language supervision has proven to be a powerful way to unlock impressive zero-shot performance on a number of downstream tasks [1; 2; 3; 4; 5; 6]. In this paper, we draw inspiration from these works and study the task of _cross-modal alignment for zero-shot transfer_ for pairs of modalities. Let \(\) and \(\) denote a pair of modalities. For example, \(\) may be text modality, and \(\) maybe image modality. We are interested in the following: given a pre-trained model \(f_{}:\) for data in \(\) (where \(\) denotes the embedding space), how can we use a paired dataset of the form \(\{u,v\},u,\ v\), to best learn a model \(g_{}:\) (where \(\) is the embedding space corresponding to \(\)) such that the learnt structure in the embedding space \(\) can be aligned with that of \(\)? Once trained, the models \(g_{}\) and \(f_{}\) can be used on a diverse set of downstream tasks including interfacing with Large Language Models (LLMs) (see ) in a zero-shot way, thus avoiding the need for costly, task-specific, labeled datasets.

Our motivation in studying the above problem lies in the fact that powerful pre-trained models existing in certain modalities, but are lacking in other modalities. For example, the recent advancesin language models have resulted in very powerful models to process text data, while no such models exist for speech and audio data. Unlike text based models that can now generalize to new tasks in a zero-shot way, speech and audio models are still trained in a task-specific way (for example, automatic speech recognition (ASR)). Further, collecting labeled datasets in speech domain offers its own set of challenges including quality control, noise, removing silence to name a few [8; 9]. Similarly, even when pre-trained models are available for certain modalities such as images, there might be challenging sub-modalities (or domains) like medical imaging on which pre-trained models may not be trained on . However, large scale _paired datasets_ maybe available, which connect the above modalities. For example, large datasets of speech and the associated (possibly noisy) transcripts are available easily on the internet. Similarly, pairs of text and images, pairs of medical and raw text  maybe more easily available. Based on this observation, methods have been proposed to train image and text encoders by aligning features corresponding to paired image and text data [1; 3]. Upon training, these models demonstrate impressive zero-shot performance on a number of downstream tasks such as image classification and image-text retrieval. While in these works both encoders are trained from scratch, authors in  showed that using a _frozen_ pre-trained image classification model as the image encoder and only training the text encoder significantly boosts downstream zero-shot performance. We observe that this abstract concept of _using a pre-trained model in one modality to supervise models in another modality using pairwise data_ can then be applied to any pair of modalities.

Our main focus in this paper is on **how best to train such cross-modal models that leverage pre-trained models in one modality.** We find that standard contrastive loss used in training such models is _inefficient_ at _maximizing the amount of supervision_ that can be extracted from the pre-trained models. In particular, to learn the embeddings in the "unfrozen" modality, existing methods only use the embedding of the corresponding paired data from the other modality for supervision. However, there maybe many samples from the supervising modality that are similar, and to various degrees of similarity. To address this inefficiency, we propose a new loss function called **continuously weighted contrastive loss (CWCL)** for contrastive training of multi-modal models. The proposed loss function leads to better supervision and hence better alignment between the two modalities.

We study our proposed loss function using two pairs of modalities, image-text and speech-text. For image-text pair, we find that the proposed loss function leads to an **improvement of 6-8% (absolute)** compared to the best baseline on zero-shot image classification tasks. For speech-text, it leads to a **20-30% (absolute) improvement** on zero-shot speech-to-intent classification and zero-shot keyword spotting tasks. Further, our models achieve performance comparable to models trained with supervision using task-specific datasets. As shown in Figure 1, we find that models trained using the proposed loss function are data and compute-efficient. They achieve higher accuracy with fewer pairs of data samples during training. Further, embeddings of downstream test datasets generated using our models show strong alignment among data that belong to the same class, even though the models have never been exposed to these datasets. We show an example in Figure 2, where embeddings extracted from speech signals in the SLURP test dataset show significantly improved sense of similarity for data from the same class, even though no label information was provided to the model.

## 2 Continuously weighted contrastive loss

### Existing frameworks for contrastive training

Various forms of contrastive learning has been successfully employed in both self-supervised learning [11; 12; 1; 2; 13; 14] and in supervised learning .

**Contrastive loss for self-supervised and multi-modal learning:** The traditional contrastive loss

Figure 1: Comparison of zero-shot transfer performance between baseline CL and proposed CWCL. (Left): zero-shot image classification accuracy measured across training epochs for the image-text modality pair. (Right): zero-shot speech-to-intent classification measured across training epochs for the speech-text modality pair. CWCL consistenly performs better than CL.

function is used in both single-modality self-supervised learning as well as multi-modal alignment. We explain the formulation used in multi-modal alignment and briefly explain how the same function is used in the single-modality setting. Let \(\) denote a batch of training data consisting of pairs of data samples from two modalities of size \(N\): \(=\{(u_{i},v_{i})\}_{i=1,,N}\), where \(u_{i}\) is from modality \(\) and \(v_{i}\) is from modality \(\). Let \(u_{i}\) and \(v_{i}\) be encoded into embeddings denoted as \(p_{i}\), \(q_{i}\) respectively. This can be done by separate, modality-specific encoders or by shared encoder. Then, the traditional contrastive loss function (CL) (to align \(\) with \(\)) is defined over \(\) as

\[_{CL}=_{i=1}^{N} ,q_{i}/)}}{_{j[N]},q_{j}/)}},\] (1)

where \([N]\) denotes the set \(\{1,2,,N\}\). Note that a similar loss function \(_{CL,}\) maybe defined and the total loss function is given as \(_{CL,}+_{CL,}\). By minimizing (1), the encoders _learn to align pairs of data_. Note that in doing so, for each \(u_{i}\), \(v_{i}\) is considered as a _positive example_ and all other samples \(\{v_{i}\}_{j[N],j i}\) are considered to be _negative examples_. This is also illustrated in Figure 4, where the diagonal matrix indicates the set of positive examples chosen (for each row and column). As an example, in [1; 2], for each image, _only the corresponding text_ is used as a positive example and _all other text samples_ are used as negative examples (and vice-versa).

**Contrastive loss for supervised learning:** It is conceivable that in a given training batch, _there is more than one "positive" sample_. However the information about which samples are related to each other may be missing in self-supervised learning. However, this information is available in a supervised learning setup. Let \(\) denote a batch of training data of size \(M\) consisting of samples and labels: \(=\{(x_{i},y_{i})\}\). Further, let \(z_{i}\) be the embedding generated by the model. Then, it is clear that the set \(_{i}=\{x_{j},j i|y_{j}=y_{i}\}\) forms _a set of positive examples_. This idea was explored in , where the following loss function 1 was proposed to leverage the label information:

\[_{}=_{i=1}^{M}_{j  P(i)},z_{j}/)}}{_{k[N],k  i},z_{k}/)}}.\] (2)

Note that the above loss function can be interpreted as _taking the average of pair-wise \(_{CL}\) over the positive set_. The authors show that a combination of the above loss and the task loss yields better performance than using the task loss alone. However, this method _requires labeled datasets_.

Figure 2: The similarity matrix between embeddings of the two modalities that are aligned via (Left): baseline CL and (Right): proposed CWCL. The axis labels correspond to the intent of utterances (for example, “news_query” represents utterances with news-related questions). CWCL results in a more “block” diagonal pattern than CL, indicating that speech and text samples with the same intent are more aligned while samples with different intents are more separated. This can be attributed to the continuous weighting mechanism of CWCL. Note that these embeddings are from a _downstream test dataset_ which was never exposed to the model during training. The visualization confirms that CWCL leads to a higher degree of alignment between similar data samples.

In the above two loss functions and other similar variants studied in the literature, we find two shortcomings. Firstly, **other similar examples that may be present in the training batch are not considered**. In the self-supervised setting, all the other similar samples are considered as negative examples. In the supervised setting, some classes might be similar to each other (for example, multiple breeds of dogs), but are considered to be negative examples to each other. Secondly, **similarity is considered to be binary**. As a result, _all "positive examples" are attracted equally, and all "negative examples" are repelled equally_. However, we observe that _samples in a training batch maybe similar to each other to varying degrees_. Some samples might be _more similar_ to each other, a few others less so many others may be _dissimilar_. For a more detailed explanation, see Figure 3.

### Can we account for non-binary similarity?

To address the above shortcomings, we propose a novel loss function called Continuously Weighted Contrastive Loss (CWCL). We use the same setup as that in multi-modal training used to define (1). The loss function (to align \(p_{i}\) with other \(q_{j}\)'s) is defined as

\[_{}=_{i= 1}^{N}w_{ij}^{}}_{j[N]}w_{ij}^{ },q_{j}/)}{_{k[ N]}( p_{i},q_{k}/)},\] (3)

where \(w_{ij}^{}\)'s denote the **intra-modal similarity weights** between \(v_{i}\) and \(v_{j}\) in modality \(\). Note that a similar loss function to align modality \(\) with modality \(\) maybe defined, with the intra-modal similarity weights computed between \(u_{i}\) and \(u_{j}\). We will refer to the intra-modal similarity weights simply as weights for ease of usage and we will drop the superscript, unless the modality needs to be specified. Note that the weights are computed _pair-wise_, within each training batch.

Before we describe how these weights may be computed, we highlight the properties that they need to have. Firstly, we normalize the weights to be between 0 and 1: \(w_{ij}\). Secondly, _"similar" samples from within a given domain should have higher weights and dissimlar samples should have lower weights._ With these properties, note that \(_{}\) provides a way to interpolate between the self-supervised and fully-supervised variants described earlier. When the weights are given as \(w_{ij}=_{\{i\}}(j)\) where \(_{}\) denotes the indicator function w.r.t set \(\), it is equivalent to \(_{}\). On the other hand, in the supervised setting, if \(w_{ij}\) is defined as \(w_{ij}=1\) for all pairs \(i,j\) belonging to the same class, but 0 otherwise, it is equivalent to \(_{}\). More importantly, \(_{}\) allows the model to use a **a continuous sense of similarity**, by i) computing a softmax function for all pairs in the

Figure 3: Existing contrastive learning methods treat samples in a batch as either strictly positive or negative. However, similarity between data samples has a more continuous and non-binary nature. In this figure, we provide an example of the nature of similarity in the context of paired image-text data. Note that the ‘weight’ terms in the figure are contrived for illustration purposes. The proposed CWCL loss function attracts all other data samples to a degree proportional to their similarity. Similarity itself is measured using intra-modal inner product between samples’ embeddings.

training batch (inner summation in Equation. (3)) and ii) weighting these softmax terms by the similarity weights. Further, note that all pair-wise inner products are already computed even in (1), (2). Therefore, computing \(_{}\) is similar in computational complexity to \(_{}\) and \(_{}\).

### How can we obtain intra-modal similarity weights?

In the traditional self-supervised setting, no information about similarity between training data points maybe available. This might also be the case in multi-modal learning such as in, where the modality encoders are initialized randomly. However, authors in  explored the idea of using pre-trained models as initialization in multi-modal models. Further, they find that _freezing_ the pre-trained model (except maybe for a final linear layer) yields the best performance. This setup offers a natural way to obtain the similarity weights. We can measure the similarity between the embeddings from the pre-trained model. We focus on this setup, where we use frozen, pre-trained models for one modality to train models in another modality. Note that even though the model encoding the first modality is frozen, follow-up layers maybe added and trained. Let \(\) be the "frozen" modality with a pre-trained initialization. Then to align modality \(\) with \(\) using (3), \(w_{ij}^{V}\) maybe computed as \(w_{ij}^{}= q_{i},q_{j}/2+0.5\) in order for \(w_{ij}\). We do not explore other formulations in this paper. A natural question is about how such weights can be computed for the modality \(\). If the model in modality \(\) is also initialized using a pre-trained model, the weights may be computed in a similar way. However, in this paper, we only focus on the _cross-modal transfer_, with similarity weights being computed only in the _frozen modality_ initialized with a pre-trained model. Assuming the modality \(\) is the frozen one, our loss function is given as

\[_{}=_{,\;}+_{CL},\;.\] (4)

Note that the exact configuration and choice of which modality to freeze will depend on the pairs of modalities being considered, the quality of pre-trained models and paired datasets available.

## 3 Related work

**CLIP like models:** CLIP  and ALIGN  introduced a set of foundational vision-language models where the encoders, one for the image, another for the text modality output embeddings in a shared space. In this set of works the encoders for all the modalities are randomly initialized and trained from scratch. Other works have looked at extending the concept to other modalities like image-audio , others have explored richer embedding , adaptive prompt learning , architectural advancements like Mixture-of-Experts . Some works also considered the problem setting where embeddings of both image and text are processed together so that specialized text query relevant image embeddings can be obtained [18; 19]. Another notable work, , the authors obtained impressive performance by improving individual encoders by processing image and text embeddings together to minimize caption-loss. Additionally,  proposed an extension to the cross-modal contrastive loss that can leverage labeled training data by combining it with SupCon . Recent works such as [21; 22; 23] consider the alignment between images and text that do not belong to the same pair, similar to our proposed method. In [21; 23], both encoders are trained from scratch

Figure 4: The classical CL-based methods (e.g., CLIP , LiT , etc) can be interpretes as using a binary weight matrix for choosing the positive examples. The proposed CWCL utilizes a continuous weight matrix to account for the non-binary nature of similarity for improved alignment.

by using a self-distillation process. Such a training process requires careful parameter tuning and generally has lower performance ( achieves about 42.4% 0-shot on ImageNet) compared to using pre-trained models, as demonstrated by the metrics. Another difference between our work and the above works is that we consider intra-modal similarity to attract image-text pairs. Owing to the availability of strong pre-trained uni-modal models, intra-modal offers a clear way to identify similar data samples. In , the authors consider using a third, object detection model to obtain similarity between images. However, their method is specific to image-text modality pair. It may also lead to performance degradation, as seen in the zero-shot metrics reported.

**LiT like models:** Alternatively, LiT  proposed the idea of leveraging strong pretrained models in one domain and aligning the embeddings of another domain to the pretrained model's embedding space. Works in this line have looked at extending to multiple domains like image-audio-text , music-audio , speech-text for speech translation ; fine-grained query specific image embeddings  and benefits of cross-modal alignment for regularizing unimodal classifiers . Along with building a model capable of conversation,  proposed the use of cross-modal attention layers to improve image-text cross-modal alignment. However, none of these works consider the problem of similarity across samples within the same modality that is explored in our work. Further, all these works are complementary to CWCL and can be improved by it. Handful of works explore similarity amongst samples [30; 13; 30] propose removing certain samples from the negative set used to compute contrastive loss (1) if their average similarity to the other samples in the batch is greater than a certain threshold ;  propose using a threshold on similarity to decide which samples are positive pairs and negative pairs and combining it with . However, the above works still consider similarity as a binary entity, where as CWCL uses a continuous model.

**Incorrect negatives in contrastive learning:** Contrastive learning incorrectly assumes that for a given sample, every other sample in the dataset is dissimilar . In the self-supervised learning one of the remedies proposed is to re-weight the negative part of the contrastive loss' denominator to account for the presence of similar (or positive) samples . However, in the case of cross-modal alignment with pretrained models, the pretrained model is a better indicator of the similarity [30; 13].

## 4 Experiments

In this section, we provide experimental results that demonstrate that CWCL leads to better zero-shot transfer performance. We study two pairs of domains, namely image-text and speech-text. For image-text pair, we demonstrate zero-shot transfer to image classification and image/ text retrieval. On both tasks, CWCL shows improved performance over existing methods for zero-shot transfer. Next, we report results for speech-text modality pair, where we consider the tasks of speech-to-intent classification and keyword spotting. Given the difficulties in collecting task-specific speech datasets, we expect CWCL-based zero-shot transfer to have a large impact in this domain. Note that our main goal is to study the effect of using CWCL. _We use open source, publicly available and easily accessible datasets for our study and leave the task of training with larger datasets to future work._

### Cross-modal transfer between image and text modalities

**Model architecture:** Our model architecture follows that in  and has a vision encoder and a text encoder. For the vision encoder, we use the ViT-L/16 model architecture  pre-trained on ImageNet. We compute the similarity weights using the embeddings from before the final linear layer that is not frozen during training. For the text encoder, we consider two architectures: transformer encoder architecture with 12 layers,output dimension 768, and number of heads set to 12 and we also consider the BERT-large architecture.

**Datasets for contrastive training:** All our experiments are based on the combination of two publicly available datasets, CC12M and YFCC15M. The CC12M dataset is a subset of the Conceptual Captions dataset  defined in . We use a set of 10 million images that are still available in the set of URLs (since the rest of them have been taken down). The YFCC15M dataset is a subset of the Yahoo Flicker Creative Commons dataset  defined by  by filtering for high quality English text. It contains a set of 15 million image-text pairs. Model training details are provided in A.1.

#### 4.1.1 Zero-shot image classification

For zero-shot image classification, we experiment on 5 datasets: ImageNet  validation, ImageNet-V2 , ImageNet-R [39; 40], ImageNet-Aand ObjNet , similar to . We provide our experimental results in Tables. 1, 2. The results for SimCon , and LiT  are obtained from our own experimentation. For , we use their loss function in our set up. For , we use their recommended experimental settings from their paper. Note that the metrics are obtained by using the same model architecture and dataset for all the methods being compared. CWCL yields a significant boost over the other methods in zero-shot performance. Further, as shown in Figure1, CWCL achieves higher accuracy with fewer image-text training pairs. Owing to the CWCL formulation, the text embeddings generated by our model are designed to be similar to a larger set of similar images than the baseline methods, hence leading to better generalization.

#### 4.1.2 Zero-shot Image-text retrieval

We also examine the zero-shot image-text retrieval capabilities of our proposed method. Note that our experiments are only towards comparing standard contrastive loss with CWCL. We leave the task of training with larger datasets [1; 2; 3] and using multi-objective training (which maybe used along with contrastive tuning to obtain better retrieval performance) [34; 29; 19] for future exploration. In our experiment, we simply compare the performance of models trained with contrastive loss (as done in ) to that of models trained using CWCL. We use the MS-COCO validation dataset  to study zero-shot retrieval performance of these models. We report our results in Table 3. Retrieval metrics for the ViT-L/16+12 layer transformer configuration model are provided in Table 6 in the Appendix. Models trained with CWCL outperform those trained using the standard contrastive loss function.

#### 4.1.3 Robustness to templates for zero-shot classification

An added benefit of the proposed CWCL formulation is that our model is robust to the templates/ prompts used in zero-shot tasks. In zero-shot image classification, the labels are converted to text prompts in order to adapt the task of classification to that of alignment. In particular, both [1; 2] use a set of 80 "template" sentences to convert each label into 80 sentences, extract the text embeddings for all the sentences and use their mean embedding as the representation of the corresponding class. We expect that CWCL leads to robustness w.r.t the choice of such templates or prompts. We study this

   Method & ImageNet (\%) & ImageNet-V2(\%) & ImageNet-R(\%) & ImageNet-A(\%) & ObjNet(\%) \\  CLIP & 31.3 & - & - & - & - \\ OpenCLIP & 34.8 & 30 & - & - & - \\ SimCon & 67.9 & 58.57 & 59.32 & 37.16 & 44.9 \\ LiT & 66.84 & 58.82 & 61.28 & 37.31 & 45.08 \\
**CWCL (Ours)** & **74.41** & **66.25** & **67.37** & **45.58** & **50.5** \\   

Table 1: Zero-shot image classification performance using the ViT-L/16 + 12-layer transformer configuration. **CWCL achieves a significant improvement in zero-shot image classification** across multiple datasets, including out-of-domain datasets such as ObjectNet.

   Method & ImageNet (\%) & ImageNet-V2(\%) & ImageNet-R(\%) & ImageNet-A(\%) & ObjNet(\%) \\  LiT & 71.2 & 62.98 & 63.8 & 40.28 & 48.1 \\
**CWCL (Ours)** & **76.48** & **67.86** & **68.7** & **47.27** & **52.38** \\   

Table 2: Zero-shot image classification using the ViT-L/16 +BERT-large configuration. CWCL-based training achieves **state-of-the-art** (when trained on publicly available datasets) performance on all of zero-shot experiments.

   Method &  &  \\   & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  LiT & 34.58 & 59.78 & 70.68 & 28.49 & 54.04 & 65.87 \\ 
**CWCL (Ours)** & **40.36** & **66.62** & **77.76** & **30.04** & **54.84** & **66.06** \\   

Table 3: Zero-shot retrieval results on MS-COCO dataset by using ViT-L/16+BERT-large configuration as the image and text encoders respectively.

by changing the number of template sentences used to build the classifier embeddings. In particular, we design simple templates such as "this is a photo of ", and "this is an image of " and experiment over \(k=1,5,10\) templates. We provide further details on the templates in Section A.1.2. We report the results for our model and that of  in Figure 5. As can be seen, models trained using CWCL are able to obtain peak performance with fewer number of templates, whereas models trained using standard contrastive loss require a higher number of templates to build better classifier embeddings. We believe that robustness to the choice and the number of template sentences/prompts used is crucial to improve the ease of usage of such models.

### Cross-modal transfer between speech and text modalities

The proposed method can be applied to speech-text cross-modal learning to transfer semantic knowledge from text embeddings to speech embeddings. Speech models with language understanding are desired for tasks in the field of spoken language understanding (SLU) [44; 45]. SLU differs from automatic speech recognition (ASR), which simply generates a transcription, but does not have language understanding. SLU models, unlike ASR models can then be used on a wide variety of downstream tasks such as intent classification (in multiple domains) , keyword spotting .

In general, speech pre-training schemes usually include information about the phonemes or paralinguistic information (e.g. speaker, emotion, pathology, etc.), but they do not include semantics in language. While some works have explored the usage of contrastive learning to train SLU models, they use the standard contrastive training method . However, similar to the image-text case, this may not be efficient. For instance, "Turn on the volume", is closer in meaning to "Increase the sound" than "Set an alarm for 7:00 AM tomorrow morning". In this case, the standard cross-modal contrastive loss is unable to learn the cross-modal relationship between the text of the first sentence and the speech of the second sentence since they are considered to be a "negative pair". This is precisely what is address by CWCL. As we demonstrate later, CWCL achieves a significant boost in performance on downstream tasks.

We train a speech-text multi-modal model with a dataset where speech and its corresponding transcript are available. Note that this is a generic dataset that is not specific to any SLU task. We use a pre-trained, frozen text encoder, owing to the availability of strong pre-trained language models. A trainable linear layer is added on top of the frozen text encoder to match the dimensionality of the speech and text embedding. We also use a pre-trained speech encoder that is robust to diverse acoustic condition and further train it using the proposed loss function.

**Model architecture:** For the speech model, we used the encoder of the pre-trained Whisper ASR , which is expected to be robust to different acoustic conditions. For the text models, we found 49 publicly available hugging face models by searching with filters as, task: Zero-shot classification, libraries: Transformers, and languages: English. We manually added one RoBERTa-based model fine-tuned on MASSIVE  data. All 50 models were compared on zero-shot text-to-intent classification using the SLURP dataset  and the top 2 models were selected. The best model (we call it RoBERTa+S) was the RoBERTa-based model fine-tuned on MASSIVE data since the data includes SLURP (only the text data) 2. The second model was a BART-based model fine-tuned on Yahoo Answers topic classification (we call it BART+Y) 3.

Figure 5: Comparison of robustness to templates. (Left): the baseline CL method of LiT . (Right): the proposed CWCL approach. The number displayed at each bar reflects the decrease in accuracy due to using only a subset of templates compared to using the full set.

**Datasets for contrastive training** For cross-modal training, we used the Common Voice Corpus 13.0 . This dataset consists of roughly 2400 hours of speech data and the corresponding transcripts obtained using crowd-sourcing and includes speech from a diverse set of demographics across age and gender. We use the English subset. Model training details are provided in A.2.

#### 4.2.1 Zero-shot speech-to-intent classification

After the cross-modal embedding alignment stage, we evaluated the models on the zero-shot speech-to-intent classification task. The task is to classify a given speech sequence into one of the intent classes. The main difference between the zero-shot and supervised intent classification is the zero-shot classification can be done without training a classifier.

**Class embedding generation:** Similar to the image-text case, we compute the embeddings of a given speech signal and compute its similarity with the text embeddings for all the intent classes. These class embeddings are obtained as averaged embedding of text sentences' embeddings of the corresponding classes. During inference, the class embedding that has the highest similarity score with the input speech embedding is chosen as the predicted class.

**Dataset:** We used the SLURP  and STOP  datasets for evaluation. In the SLURP dataset, we used all the text sentences in the _train_ subset to generate the class embeddings for 60 intent classes where intent is defined as the concatenation of scenario and action labels, following ESPnet-SLU . We did not use the _train_synthetic_ subset since more than half of the text sentences overlap with the _devel_ and _test_ subsets. On average, 191 text sentences were used per class. We compare the systems by evaluating them on the _devel_ and _test_ subsets. In the STOP dataset, we used the 8 unique domain labels as intent labels. Although not intents in a strict sense, the domain labels can be considered a simpler version of the intent labels. Since significantly more sentences are available in STOP, we randomly extracted 200 sentences per domain from the training set to generate the class embeddings. The evaluation was done on the validation and test sets.

**Results**: In previous works, speech-to-intent classification has been done with an ASR-NLU pipeline system where the speech is first transcribed by ASR (speech-to-text), after which the transcription is classified into an intent using NLU (text-to-intent) . We refer to the text-to-intent performance achieved by the pre-trained text encoders as the "reference" performance. This provides an estimate of the performance that can be expected from the speech encoder on the speech-to-intent task.

The first results of speech-to-intent classification are shown in the SLURP and STOP (without the superscript \({}^{\#}\)) columns in Table 4. In all cases, multi-modal training with the CWCL loss outperformed the CL loss. On the SLURP dataset, RoBERTa+S has a higher reference performance compared to BART+Y because the fine-tuning data for RoBERTa+S included the SLURP text data. This also leads to a better performance compared to using the BART+Y model as the text encoder.

On the STOP dataet, RoBERTa+S has a lower reference compared to BART+Y, implying that the RoBERTa+S' text model overfits the SLURP data. However, the RoBERTa+S-based speech intent classification was still better than the BART+Y-based one. This implies that the text model architecture could be another factor that contributes to transferring performance to the speech model. To be specific, the RoBERTa+S was RoBERTa which consists of only encoder layers while the BART+Y was the encoder-decoder-based BART model. Another thing to note is that CWCL with RoBERTa+S outperforms the text-to-intent reference performance on the STOP (87.87 vs. 84.78) dataset. This is because, during the cross-modal alignment stage using CWCL, the speech tower might have learned how to utilize acoustic cues in addition to linguistic information from a given speech utterance, to align its embedding to the semantic embedding from the text tower. However, this did not happen in the case of SLURP, because the SLURP dataset includes more intent classes than STOP (60 vs 8 classes), thus being more challenging in transferring knowledge from text to speech during the cross-modal alignment stage.

**Experimenting with different templates to generate text embeddings:** So far, each class embedding used in zero-shot intent classification was generated by averaging all the corresponding text sentences' embeddings from the class in the training subset. Although collecting the text data with the intent labels can be less expensive than collecting speech data with the intent labels, the former may not always be possible. To address this, we manually devised a fixed set of general templates that were applied to every class. For example, templates are of the form "This audio is about [class]", and "The utterance is related to [class]", and the text embeddings are averaged to obtain the class embedding. For the exact templates we used, readers may refer to Appendix A.2.5. The results areshown in the SLURP\({}^{\#}\) and STOP\({}^{\#}\) columns in Table 4. We again observe that the proposed CWCL loss outperforms the CL loss.

**Comparison to supervised training**: We also present results of a supervised SLU model on SLURP, based on ESPnet-SLU . Considering our system is zero-shot, the result is noteworthy. For STOP, we could not find previous supervised works that evaluated systems the same way.

Due to lack of space, we present the following results in A.2.2. In Table 7, we found that leveraging pre-trained models was more beneficial than training from scratch for speech-text embedding alignment. As seen in Table 8, locking the text encoder and fine-tuning the speech encoder gave the best performance. We found that batch size is not a critical factor, as shown in Table 9.

#### 4.2.2 Zero-shot keyword spotting (KWS)

We also tested our model for KWS using the Google Speech Command Dataset V2 (GSCV2)  where we classified among the 35 keywords in the Google Speech Command. The result is shown in the columns after the thick vertical line in Table 4. For the results in the first column (without \({}^{\#}\)), we used each keyword as is to extract the class embedding from the text model. For the second column (with \({}^{\#}\)), we used the general template used in the speech-to-intent experiments. The results show that the proposed method outperforms the baseline. With CWCL, the KWS\({}^{\#}\) outperformed KWS. This could be because the text models that generate the class embeddings are usually trained with sentence-level samples, not word-level ones whereas the keywords are words, i.e., the KWS class embeddings are extracted from words whereas KWS\({}^{\#}\) are extracted from sentences constructed using templates, thus resulting in better class embedding.

**Comparison to supervised training**: Interestingly, results achieved with the BART-based text model are comparable to the supervised learning mechanisms of [53; 54; 55]. Note that the self-supervised mechanisms use training data to train the final linear classifier [54; 55]. However, our models without any training data still achieve close to \(90\%\) accuracy. This will be useful when defining new keywords as collecting large datasets for keyword classification becomes difficult . Additional results are provided in Table 10 and in Table 11, respectively in Appendix A.2.

## 5 Conclusion

In this paper, we make the observation that existing contrastive learning based methods for cross-modal alignment using pre-trained models are not efficient in extracting supervision from the pre-trained embeddings. In particular, many similar examples that do not form pairs in the training data are ignored. We address this by developing a novel loss function that accounts for the continuous nature of similarity and uses information from all similar examples in a training batch. We train models for two pairs of modalities using this loss function, namely image-text and speech-text. In both cases, we observe a significant increase in zero-shot performance on downstream tasks. We believe that the proposed loss function will be impactful in leveraging powerful pre-trained models and transfering the learnt knowledge to other modalities and domains.

   Method & Text model & SLURP & SLURP\({}^{\#}\) & STOP & STOP\({}^{\#}\) & GSCV2 & GSCV2\({}^{\#}\) \\  CL & RoBERTa+S & 40.35 & 23.68 & 70.13 & 50.56 & 64.74 & 59.65 \\ (baseline) & BART+Y & 22.73 & 8.06 & 55.67 & 42.07 & 56.33 & 45.54 \\ 
**CWCL** & RoBERTa+S & 63.80 & 40.75 & 87.87 & 67.77 & 81.02 & 82.77 \\ (**Ours**) & BART+Y & 53.12 & 30.51 & 80.99 & 73.08 & 88.81 & 89.43 \\  Text-to-intent & RoBERTa+S & 88.19 & 59.86 & 84.78 & 69.10 & 100 & 98.20 \\ (reference) & BART+Y & 77.03 & 45.93 & 92.93 & 79.11 & 100 & 100 \\   ESPnet  & - & 77.00 & - & - & - & - & - \\ Att. RNN  & - & - & - & - & - & 93.9 & - \\ Wav2Vec2  & - & - & - & - & - & 96.6 & - \\ M2D  & - & - & - & - & - & 98.5 & - \\   

Table 4: Top-1 accuracy for zero-shot speech-to-intent classification (SLURP and STOP) and keyword spotting (GSCV2) after thick vertical line. Superscript \({}^{\#}\) is used to indicate use of general templates for class embedding extraction. Supervised results are provided in gray after the double-horizontal line:  is for speech-to-intent and [53; 54; 55] are for keyword spotting.