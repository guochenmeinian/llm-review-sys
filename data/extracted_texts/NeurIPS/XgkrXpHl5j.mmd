# Generalized Multimodal Fusion via

Poisson-Nernst-Planck Equation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Previous studies have highlighted significant advancements in multimodal fusion. Nevertheless, such methods often encounter challenges regarding the efficacy of feature extraction, data integrity, consistency of feature dimensions, and adaptability across various downstream tasks. This paper proposes a generalized multimodal fusion method (GMF) via the Poisson-Nernst-Planck (PNP) equation, which adeply addresses the aforementioned issues. Theoretically, the optimization objective for traditional multimodal tasks is formulated and redefined by integrating information entropy and the flow of gradient backward step. Leveraging these theoretical insights, the PNP equation is applied to feature fusion, rethinking multimodal features through the framework of charged particles in physics and controlling their movement through dissociation, concentration, and reconstruction. Building on these theoretical foundations, GMF disassociated features which extracted by the unimodal feature extractor into modality-specific and modality-invariant subspaces, thereby reducing mutual information and subsequently lowering the entropy of downstream tasks. The identifiability of the feature's origin enables our approach to function independently as a frontend, seamlessly integrated with a simple concatenation backend, or serve as a prerequisite for other modules. Experimental results on multiple downstream tasks show that the proposed GMF achieves performance close to the state-of-the-art (SOTA) accuracy while utilizing fewer parameters and computational resources. Furthermore, by integrating GMF with advanced fusion methods, we surpass the SOTA results.

## 1 Introduction

The world is inherently multimodal; individuals perceive and integrate diverse sensory inputs to form a more comprehensive understanding of their surroundings. Similarly, multimodal learning processes inputs from multiple modalities, offering potential applications in complex downstream tasks such as cross-modal retrieval and multi-modal classification. Nevertheless, features from different modalities often differ significantly, even when describing the same event [1; 2]. Consequently, fusing features from different modalities is challenging, requiring a dedicated fusion phase before being applied in tasks, bridging the semantic gap between different modalities is crucial for valid feature fusion.

Theoretical works on multimodal fusion have proposed more generalized schemes. MBT  exchanges mutual information between different modalities to enhance understanding. Perceiver  stacks various features and extracts fusion features from transformer blocks to condense task-related features. Uni-Code  distinguishes between modality-invariant and modality-specific features, optimizing feature utilization. Moreover, in downstream tasks, innovative fusion methods are applied. MAP-IVR  considered that image features belong to the subset of video features, UAVM  fuses different modalities using an independent fusion block.

Although existing methods for feature fusion show considerable improvements, they often rely on several incomplete assumptions: **1)Feature dimension consistency:** Feature dimensions across different modalities are perfectly aligned [7; 8], leading to inefficient representations, thus impairing model performance; **2)Data reliability:** In reality, poor quality data (e.g. missing modalities) directly degrades performance [9; 10], even though datasets are assumed to be complete; **3)Downstream task applicability:** Feature fusion requirements are uniform across different tasks, but matching tasks [11; 12; 13; 14; 5] require modality-invariant features (common to all modalities), whereas detection tasks [15; 16] necessitate modality-specific features (specific to each modality) additionally: **4)Feature extraction effectiveness:** Loss function in feature fusion does not affect the feature extractor's gradients [17; 18] (See Appendix A), often results in feature extractor homogenization , deteriorating performance in downstream tasks . Furthermore, the fixed quantity of modal features often limit the generalizability of proposed fusion methods .

This paper introduces a generalized multimodal fusion method (GMF) that operates independently of the usual constraints. We formulate the learning objectives for traditional multimodal tasks and propose new definitions based on information entropy theory [19; 20]. Taking inspiration from the Poisson-Nernst-Planck equation (PNP) , treating features as charged particles to disassociate them, employing GMF for multimodal feature fusion. Leveraging the principles of the PNP equation, GMF orchestrates the guided migration of features within a high-dimensional space, segregating modality-invariant from modality-specific features within the disassociated feature landscape, reducing the mutual information between features further decreases the relevant entropy of downstream tasks. Specifically, the proposed method incorporates a reversible feature dissociation-concentration step and applies reasonable regional constraints to the reconstruction gradient, emphasizing the connection between the feature extractor and the loss of a downstream task, enabling GMF to generalize effectively and serve as the frontend for other fusion modules. We evaluated our method on multiple datasets across specific downstream tasks. It consistently demonstrated significant performance and generalization capabilities. In summary, our contributions are as follows:

1. We propose a novel theory for multimodal feature fusion based on the Poisson-Nernst-Planck equation and information entropy with an exhaustive proof, demonstrating its effectiveness through theoretical analysis and preliminary experiments.
2. We have devised a generalized feature fusion method GMF, grounded in entropy theory and the PNP equation, which stands independent of both feature extractors and downstream tasks.
3. Experiments demonstrate that GMF achieves comparable performance to SOTA with fewer computational demands and parameters, while also showing robustness to missing modalities. Moreover, when integrated with advanced fusion methods, its performance and robustness are notably enhanced, surpassing SOTA and ensuring greater reliability in real-world applications.

## 2 Related Works

Innovative advancements in multimodal fusion methods, both theoretically  and structurally , have significantly propelled the progress of generalized multimodal tasks (denote as **GMTs**). Some SOTA methods focusing on downstream tasks propose fusion methods specifically tailored for them. However, the fusion challenges vary with the diversity of downstream tasks. In this paper, we categorize multimodal tasks into two types: Native Multimodal Tasks (denote as **NMTs**) and Extended Multimodal Tasks (denote as **EMTs**), based on whether corresponding single-modal tasks exist. Specifically, cross-modal retrieval and matching tasks such as Image-Video retrieval [14; 5] and Image-Text matching [12; 13; 11] usually belong to NMT and only require the similarity of modalities. For example, CLIP  transforms the image classification task into an image-text retrieval task, achieving stunning zero-shot performance. Multi-modal classification, recognition, and detection tasks such as emotion recognition  and event classification  usually belong to EMT. Different modalities often have inconsistent perspectives, and fully aligned features will affect the performance of such tasks.

To illustrate the generalization capabilities of these methods and their impact on downstream tasks, Tab 1 is presented. The "Type" column categorizes methods by GMT support. "Align." indicates feature alignment across modalities. "Grad. Ref." assesses if fusion affects feature extractor gradients. "Gene." denotes uniformity of fusion requirements across tasks. "Avail." indicates handling of missing modalities during inference. Lastly, "Complexity" reflects computational complexity regarding (\(n\)) modalities. Perceiver  does not report multimodal correlation experiments.

It is worth noting that the evaluation of gradient correlation is simply whether there is an explicit excitation of the loss function. Some downstream methods introduce ways such as concat (e.g., classifier of AVoiD-DF ) in the classification stage, and the modal missing adaptation in the fusion stage does not represent the adaptation for this task. In addition, for NMTs, the complete modal input is necessary, so the conclusion of this part is "-"; Here, the complexity takes the highest value, which does not represent the final computation cost. (e.g., the disentangled loss of MISA  is \(O(n^{2})\).

## 3 Theory

In this subsection, we briefly introduce the notation system used in this paper and the general structure of multimodal tasks, representing the information entropy at different stages of multimodal learning. After that, we generalize the information entropy to multi-modality and redefine the entropy reduction objective for multi-modal learning. Finally, we evaluate the impact of linear dimension mapping on the performance of downstream tasks and present the preamble theorem.

### Formulation and Traditional Objective Definition

Consider inputs with \(d\) modalities, where \(j\{1,2,,d\}\) represents different modalities. Examine a dataset comprising \(n\) samples. Let the input be \(X=\{X_{1},X_{2},,X_{n}\}\), where a specific sample \(i\{1,2,,n\}\) is represented as \(X_{i}=\{X_{i}^{(1)},X_{i}^{(2)},,X_{i}^{(d)}\}\). The output is \(Y=\{Y_{1},Y_{2},,Y_{n}\}\), and each \(\{X_{i},Y_{i}\}\) forms a sample pair. \(X_{i}^{(j)}\) represents the original sample of modality \(j\) with varying shapes, while the shape of \(Y_{i}\) depends on the specific datasets and downstream tasks. For each modality \(j\), specific feature extractors \(f^{(j)}(,^{(j)})\) and parameters \(^{(j)}\) are employed for feature extraction. The fused features capturing multimodal interactions for sample \(i\) are denoted as \(Z_{i}=\{Z_{i}^{(1)},Z_{i}^{(2)},,Z_{i}^{(d)}\}\). The set of global fea

  Method & Type & Align. & Grad. Ref. & Gene. & Avail. & Complexity & Mentioned Multimodal Related Task \\  CLIP  & NMT & ✓ & ✓ & \(\) & - & \(O(n^{2})\) & 
 I-T, Contrastive Learning \\  \\ ALBEF  & NMT & ✓ & ✓ & \(\) & - & \(O(n^{2})\) & I-T, Contrastive Learning and Matching \\ ViLT  & NMT & ✓ & ✓ & \(\) & - & \(O(n^{2})\) & I-T, Matching \\ METER  & NMT & ✓ & ✓ & \(\) & - & \(O(n^{2})\) & I-T, Matching \\ APIVR  & NMT & ✓ & ✓ & \(\) & - & \(O(n^{2})\) & I-V, Retrieval \\ MAP-IVR  & NMT & \(\) & ✓ & \(\) & - & \(O(n^{2})\) & I-V, Retrieval \\ AVoiD-DF  & EMT & ✓ & ✓ & ✓ & ✓ & \(O(n^{2})\) & A-V, Deepfake Detection \\ MISA  & EMT & ✓ & ✓ & \(\) & \(\) & \(O(n^{2})\) & A-V-T, Emotion Recognition \\ UAVM  & EMT & ✓ & \(\) & ✓ & ✓ & \(O(n^{2})\) & A-V, Event Classification \\ DrFuse  & EMT & \(\) & ✓ & \(\) & \(\) & \(O(n^{2})\) & EHR-CXR, Representation \\ MBT  & GMT & ✓ & \(\) & ✓ & ✓ & \(O(n^{2})\) & A-V, Event Classification \\ Perceiver  & GMT & \(\) & ✓ & ✓ & \(\) & \(O(n)\) & - \\ Uni-Code  & GMT & \(\) & ✓ & ✓ & ✓ & \(O(n^{2})\) & A-V, Event Classification; localization \\  

Table 1: Comparison of multimodal method proposed in the fusion phase.

Figure 1: Stages of information entropy change. Where \(Z_{i}\) might be a set of vectors (\(\{Z_{i}^{A},,Z_{i}^{M}\}\)) or a vector, depending on the fusion method \(F()\), and \(C()\) stands for classifier.

tures is expressed as \(f(X,)=[f^{(1)}(X^{(1)},^{(1)});f^{(2)}(X^{(2)},^{(2)});;f ^{(d)}(X^{(d)},^{(d)})]\), where \(=\{^{(1)},^{(2)},,^{(d)}\}\).

The multimodal task is depicted in Figure 1, delineating three key parameters: the feature extractor \(\), fusion parameter \(^{F}\), and classifier parameter \(^{C}\). Optimization of these parameters aims at maximizing performance. Regarding entropy, \(F()\) represents the fused mapping, extending the learning objective from feature extraction to fusion:

\[_{,^{F}}\{H(F(f(X,),^{F}) F(f(X,))\}\] (1)

Similarly, we employ \(C()\) to represent the mapping for downstream tasks and generalize it to embody the learning objective fused with downstream tasks:

\[_{,^{F},^{C}}\{H(Y C(F[f(X,),^{F}], ^{C})])\}\] (2)

In Eq. (2), these parameters are optimized by downstream task losses. If there is a loss in the fusion stage, then it optimizes the parameters in Eq. (1).

### Information Entropy and Objective Redefinition

Feature extraction through dimensionality reduction involves reducing data uncertainty , as quantified by information entropy \(H\). In Figure 1, we show a simplified approach to single-modal learning. The feature extractor and classifier (dotted arrow) directly minimize the information entropy of both the input \(X_{i}^{(j)}\) and the output \(Y_{i}\) by adjusting the parameters of the feature extractor \(f^{(j)}(,^{(j)})\) and the classifier \(C^{(j)}(,^{C^{(j)}})\) for modality \(j\):

\[_{^{(j)},^{C}}H[Y_{i}|C^{(j)}(f^{(j)}(X^{(j)},^{(j)}), ^{C^{(j)}})]\] (3)

This process, facilitated by feature extractors, condenses data samples into a feature space, preserving pertinent attributes for downstream tasks. Think loss as stimulation of entropy reduction, maximize mutual information about related features . Expanding to the multimodal fusion stage, the objective is to minimize the entropy of the fused features compared to the sum of the entropy of each input feature. In the context of multimodal fusion, where outputs from disparate modalities are integrated post-feature extraction, the total information entropy of the system can be estimated using the joint entropy formula, and for constant \(X\):

\[H(f(X,))=_{j=1}^{d}H(f^{(j)}(X^{(j)},^{(j)}))-_{}_{}H(f(X,))_{}I(f(X, ))\] (4)

Downstream objectives are typically structured to minimize mutual information, consequently leading to a reduction in entropy. However, in fusion stage, disparities observed among the equations (1), (2), and (3) suggest that certain fusion-method might not establish a straightforward correspondence between network inputs and outputs. Achieving complete consistency between modalities, where mutual information is zero, may not always lead to optimal outcomes , potentially increasing entropy in downstream task-related features . This observation is substantiated by the diminishing performance of certain multimodal methods  compared to earlier unimodal methods, indicating a decline in their capacity to extract distinctive features from individual modalities when confronted with the absence of certain modalities. Thus, optimization objectives for multimodal tasks should balance minimizing entropy during fusion with maintaining or reducing entropy in downstream task-related features. This highlights the necessity of aligning deep learning tasks with downstream objectives and minimizing information entropy when designing loss functions for these tasks.

**Theorem 3.1:** The overarching objective of multimodal tasks lies in minimizing entropy during the fusion stage without amplifying the entropy of downstream task-related features:

\[_{,^{F},^{C}}\{H(Y C(F[f(X,), ^{F}],^{C})])\}\] (5) \[ j\{1,2,,d\},^{(j)} _{^{(j)}}H(Y|f^{(j)}(X^{(j)},^{(j)}))\]Some approaches introduce the fused results as residuals, which demonstrate a certain degree of improvement, and this theory provides a better rationale for such enhancement. However, given that the forward pass necessarily involves the operation of \(F()\), it becomes challenging to fully meet this precondition. During gradient backward, the loss incurred during the fusion stage for the feature extractor should align with the loss of the downstream task or be zero.

### Modality Feature Dissolution and Concentration

Adding too many parameters, or overcharacterization, can improve the model's ability to fit the data, acting like a parameterized memory function . However, it's important to balance this with the amount of data available for the next task to prevent learning too much noise and overfitting . On the other hand, having too few parameters may weaken the model's ability to represent complex patterns, resulting in lower performance across different methods (See C).

**Theorem 3.2:** The dimension of the feature that is best suited to the downstream task varies, and there is always an optimal value for this feature. The dimension multiple relationship between each layer of the feature extractor is fixed, and the initial dimension is adjusted. Too low dimension of the final output will lead to inefficient representation, and too high dimension will introduce noise. The existence of an integer \(l_{}\) such that for any integer \(l\) distinct from \(l_{}\), the conditional entropy of the model's predictions \(f_{l}(X,_{l})\) is greater than that of the model's predictions \(f_{l_{}}(X,_{l_{}})\).

\[ l_{}, l,l l_{},H(Y|f_{l}(X,_{l}))>H(Y|f_{l_{}}(X,_{l_{}}))\] (6)

**Theorem 3.3:** The feature extractor is fixed, and its original output feature dimension \(l\) is mapped to \(nl\), and finally back to \(l\). The mapping result is used as the basis for the downstream task. The performance of downstream tasks is infinitely close to the original performance as \(n\) increases, but never greater than the original performance. For magnification \(n>1,n\), mapping matrix \(_{1}^{l nl}\) and \(_{2}^{nl l}\), For the output features \(f(X,)^{l}\) and \(Y\):

\[H(Y|f(X,))<H(Y|_{1}(_{2} f(X,)))\] (7)

\[lim_{n}H(Y|_{1}(_{2} f(X, )))=H(Y|f(X,)\] (8)

**Conjecture 3.1:** Rely on Theorem 3.1, 3.2, 3.3, we propose an conjecture that a boundary of performance limitation exists, determined by downstream-related entropy. Theoretically, by establishing a direct correspondence between the extractor and classifier, fusion method can enhance the limitation boundary, further improve performance.

### Poisson-Nernst-Planck Equation

The Nernst-Planck equation represents a mass conservation equation that characterizes the dynamics of charged particles within a fluid medium. This equation modifies Fick's law of diffusion to include scenarios where particles are also mobilized by electrostatic forces relative to the fluid. The equation accounts for the total flux of particle \(p\{+,-\}\), denoted as \(_{p}\), of charged particles, encompassing both diffusion driven by concentration gradients and migration induced by electric fields. Since fusion features are usually one-dimensional, we only consider the \(x\) direction here. For a given charged particle \(i\), the equation describes its movement as follows:

\[_{p}= c_{p}(x,t)}_{}+ (x,t)}_{}+z_ {p}e}{k_{B}T}c_{p}(x,t)}_{}\] (9)

\(p\) is abstracted as elements in the modality-invariant feature and the modality-specific feature. Here, \(c_{p}(x,t)\) denotes the concentration of particle, while \(D_{p}\) (diffusivity of \(p\)), \(k_{B}\) (Boltzmann constant), \(z_{p}\) (valence also electric charge), and \(e\) (elementary charge) are constants. \(T\) is a hyperparameter, represent temperature. \(\) represents the electric field of the entire system, and \(\) represents the flow rate. The Poisson equation describes the relationship between the distribution of a field and the potential energy it induces, represented by the expression:

\[^{2}(x)=-},=e(z_{+}c_{+}(x,t)+z_{-}c_ {-}(x,t))\] (10)

\(\) signifies the potential, considered as an external excitation, \(_{0}\) represent dielectric constant. By integrating the relationship between the concentration of charged particles and the electromigrationterm in the Poisson equation, we derive the Poisson-Nernst-Planck (PNP) equation. Assuming that the dissociation process approaches equilibrium, for feature elements without magnetic field and flow velocity, we can consider the time-dependent change in concentration \(c_{p}(x,t)\) of the charged particle \(i\) over time \(t\) is negligible:

\[(x,t)}{ t}=D_{p}(c_{p}(x,t)}{  x^{2}}-eF}{k_{B}T_{0}}c_{p}(x,t)(z_{+}c_{+}(x,t)+z _{-}c_{-}(x,t)+e}{k_{B}T}(x,t)}{ x} {d(x)}{dx}) 0\] (11)

When the final state is stable, a sufficiently large 1D electrolytic cell of length \(l\), at the potential equilibrium boundary \(b\), it can be equivalent to (See Appendix B):

\[((0)-(b))-e_{0}^{b}c_{-}(x,t)z_{-}dx e_{0}^{l}c_{+}(x, t)z_{+}-c_{-}(x,t)z_{-}dx\] (12)

In this context, \((x)\) represents an external influence from another modality feature. We assume that modality-invariant feature elements have a positive charge, while modality-specific feature elements have a negative charge. The difference \((0)-(b)\) indicates the enrichment potential of modality-invariant feature elements for the excitation modality. This potential attracts modality-specific feature elements in dissociated modality towards dissociation.

**Theorem 3.4**:: Following dissociation and Theorem3.3, in line with the principles of matter and information conservation, the excitation and attraction features can revert back to their original state. A cyclic feature electrolytic cell is generalized, using a loss function as stimulation:

\[^{(j)}_{i}=_{dis}f^{(j)}(X^{(j)}_{i},^{(j)})\] (13)

\[=||^{(j)}_{con}[^{(j)}_{i}(1:b^{j});^{(j+1)} _{i}(b^{(j+1)}+1:nl^{(j+1)})]-f^{(j)}(X^{(j)}_{i},^{(j)})||^{2}\] (14)

\(\) is loss function. \(l^{(j)}\) and \(b^{(j)}\) are feature dimension and dissociation boundary of modality \(j\), respectively. Around this boundary, features are explicitly distinguished. The mapping matrix \(^{(j)}_{dis}^{nl^{(j)} l^{(j)}}\), \(^{(j)}_{con}^{l^{(j)}(nl^{(j+1)}+b^{(j)}-b^{(j+1) })}\) is learnable. \(^{(j)}_{i}^{nl^{(j)}}\) is the result of \(f^{(j)}(X^{(j)}_{i},^{(j)})^{l^{(j)}}\) being linearly mapped (dissolved) into a higher dimensional space.

## 4 Methodology

Set the dissociation boundary \(b^{(j)}\) and feature dimension \(l^{(j)}\) of modality \(j\). The feature with the smallest dimension is denoted as \(l^{*}\). The feature dimension of the dissociation is \(nl^{(j)}\), with a uniform magnification of \(n>2\).

Combining information entropy theory with the PNP equation, we propose GMF method to optimize fusion feature mutual information on the premise of maintaining the downstream task related information of input features. Following Assumption3.1, GMF has only four learnable matrices for each modality, enforces correlations without complex structure, as shown in Fig 2.

GMF is divided into three stages, for each modality \(j\), applying different learnable mapping matrices: dissolve matrix \(^{(j)}_{dis}^{nl^{(j)} l^{(j)}}\), concentrate matrix \(^{(j)}_{cinv}^{b^{(j)} l^{*}}\) and \(^{(j)}_{cspec}^{(nl^{(j)}-b^{(j)}) l^{(j)}}\), reconstruct matrix \(^{(j)}_{recon}^{l^{(j)}(l^{(j)}+l^{*})}\).

\[Z_{i}=(f(X_{i},),^{GMF}),\ \ ^{GMF}=\{^{(j)}_{dis}, ^{(j)}_{cinv},^{(j)}_{cspec},^{(j)}_{recon}\}\] (15)

First, to make sure the features move, we map (dissolve) them to higher dimensions. Next, for the feature of each modality, after dimension elevation, the goal is explicitly divided as specific and invariant by abstracting different kinds of features into positive and negative charged particles:

\[^{(j)}_{i}=^{(j)}_{dis}(f^{(j)}(X^{(j)}_{i},^{(j)})), \ \ \ (^{(j)}_{i})_{inv}=^{(j)}_{i}(1:b^{(j)}),\ \ \ (^{(j)}_{i})_{spec}=^{(j)}_{i}(b^{(j)}+1:nl^{(j)})\] (16)

\(f^{(j)}(X^{(j)}_{i},^{(j)})^{l^{(j)}}\), and \(^{(j)}_{i}^{nl^{(j)}}\). Referencing Eq. (4), irrespective of the initial length \(l^{(j)}\) of a feature, partitioning it into invariant \((Z^{(j)}_{i})_{inv}^{l^{*}}\) and specific \((Z^{(j)}_{i})_{spec}^{l^{(j)}}\) components aims to minimize output feature dimensions, thereby mitigating entropy disturbance. After concentrate, finally, the output \(Z^{(j)}_{i}^{(l^{(j)}+l^{*})}\) is obtained:

\[(Z^{(j)}_{i})_{inv}=^{(j)}_{cinv}(^{(j)}_{i})_{inv},\ \ \ (Z^{(j)}_{i})_{spec}=^{(j)}_{spec}(^{(j)}_{i})_{spec},\ \ \ Z^{(j)}_{i}=[(Z^{(j+1)}_{i})_{inv};(Z^{(j)}_{i})_{spec}]\] (17)Eventually the entire system can be restored to its original state. A loss function is given as an external incentive to force the features to move in different directions. Following the Theorem3.4, we use \(^{(j)}_{recon}\) to map the features back to \(f^{(j)}(X^{(j)}_{i},^{(j)})\) and apply the disassociation loss.

\[_{dis}=_{j=1}^{d}||(f^{(j)}(X^{(j)}_{i},^{(j)})- ^{(j)}_{recon}Z^{(j)}_{i}||^{2}\] (18)

## 5 Experiment

In this section we briefly introduce the experimental dataset, evaluation metrics, implementation details, experimental results and analysis. Our evaluation focuses on solving the limitations mentioned in Section 1 and verifying our theory and hypothesis, so we pay more attention to the fusion performance under the same feature extraction ability.

### Datasets and experimental tasks

We performed the NMT task for image-video retrieval on ActivityNet  dataset and the EMT task for audio-video event classification on VGGSound  and deepfake detection on FakeAVCeleb , and compared the NMT, EMT and GMT methods (as defined in the Related Work) respectively. We conduct three sets of comparison experiments:

1. Input the same features to simulate the freezing of the feature extractor, and evaluate the entropy reduction effect of the fusion method on the existing information.
2. Complete the training of the whole model including the same feature extractor, and evaluate the impact of the fusion method on the gradient of the feature extractor.
3. Select a set of method-specific feature extractors to test the limitation performance.

For EMTs, VGGSound dataset evaluate (1) and (2)1, the evaluation metric is the classification accuracy ACC(%). FakeAVCeleb dataset evaluate (3), due to the imbalance of data samples, the evaluation focuses on the Area under the Curve of ROC (AUC). For NMTs, ActivityNet dataset evaluate (4), the evaluation metric is the matching accuracy mAP, mAP@\(n\) represents that the matching task target is selected from \(n\) samples.

Figure 2: Structure of GMF. The input is taken from \(f(X_{i},)\) and the output is taken as \(Z_{i}\). This is done in three steps: dissociation concentration, and reconstruction. As a front-end, the output can be directly used for classification or can be connected to other fusion modules. See Appendix J

[MISSING_PAGE_FAIL:8]

basis, but the improvement is limited due to the sparse features. MAP-IVR  employs fixed-length mappings, while Perceiver  inputs an indistinguishable feature mapping, so the actual number of parameters relative to input dimensions is not apparent. GMF achieving competitive performance in (128) with minimal additional parameters and computations. Furthermore, the experiments (128-4096) demonstrate the necessity of unequal-length fusion, ensuring not only the flexibility of the method but also profoundly impacting its performance and additional parameters. In the experiments of unequal-length fusion, GMF achieved state-of-the-art performance. Given that GMF is composed of linear layers, an increase in input dimensionality leads to an escalation in parameter count.

We performed a theoretical performance evaluation on FakeAVCeleb , as shown in Table 4. We use a feature extractor that is more compatible with the proposed method and remove the linear layer, denote as GMF-MAE (in Appendix, Fig. 14). For other SOTA methods involved in the comparison, we choose the feature extractor proposed in the original paper as much as possible (MISA utilizes sLSTM , UAVM adopts ConvNeXT-B , GMF-MAE employs MAE [32; 33]). The remaining methods, including Baseline employs R(2+1)D-18 . Due to the imbalance in the dataset, with a ratio of approximately 1:39, the audio ratio is 1:1 and the video ratio is 1:19. UAVM  learns a unified representation, thus the easier classification of audio significantly impacts the overall results. Both DrFuse  and MISA  perform below our expectations; one potential explanation could be the influence of sample imbalance on their performance.

The performance of GMF remains consistent with the conclusions drawn from Table 2. Furthermore, GMF's insensitivity to missing modalities effectively mitigates the impact of sample imbalance, avoiding an excessive emphasis on any particular modality. The combination of GMF and MAE [32; 33] demonstrates optimal performance limits, validating our approach's effectiveness in addressing the challenges posed by downstream tasks. We provide a more comprehensive comparison with methods focused on deepfake detection in Table 7 (in Appendix).

## 6 Conclusion

In this paper, we combine the PNP equation with information entropy theory to introduce a multimodal fusion method for unrelated input features and downstream task features. The aim is to reduce the joint entropy of input features while decreasing the downstream task-related information entropy. Experimental results demonstrate that the proposed method takes a step forward in the generalization and robustness of multimodal tasks. Meanwhile, the additional burden can be negligible.

GMF comprises basic linear layers and is consequently susceptible to the inherent characteristics of linear operations, which exhibit growth in parameter count relative to input dimensionality. However, as per our theoretical framework, the effective component is proportional to the feature dimension. In forthcoming research, we intend to concentrate on sparsifying mapping matrices to further diminish parameter count.

    & **Baseline** & **MISA ** & **UAVM ** & **DrFuse ** & **Perceiver ** & **GMF** & **G-Perceiver** & **GMF-MAE** \\ 
**ACC** & 97.68 & 97.68 & 78.64 & 97.68 & 97.68 & 97.68 & 98.21 & **99.99** \\
**AUC** & 69.33 & 79.22 & 43.92 & 78.56 & 93.45 & 91.88 & 96.71 & **99.97** \\   

Table 4: Comparison of fusion methods based on different feature extractors on FakeAVCeleb.

   Method & mAP@10 & mAP@20 & mAP@50 & mAP@100 & Params & FLOPs \\  CLIP (4096) & 0.235 & 0.221 & 0.213 & 0.205 & - & - \\ METER (4096) & 0.252 & 0.245 & 0.235 & 0.228 & 62.96M & 0.13G \\  Perceiver (128) & 0.264 & 0.253 & 0.241 & 0.232 & 44.54M & 45.56G \\ MAP-IVR (128) & 0.341 & 0.323 & 0.306 & 0.294 & 3.81M & 0.01G \\ GMF (128) & **0.349** & **0.335** & **0.323** & **0.308** & **0.32M** & **0.00G** \\  APIVR (128-4096) & 0.264 & 0.255 & 0.249 & 0.232 & **2.19M** & **0.00G** \\ MAP-IVR (128-4096) & 0.349 & 0.337 & 0.322 & 0.311 & 11.94M & 0.02G \\ GMF (128-4096) & **0.355** & **0.341** & **0.327** & **0.315** & 119.21M & 0.23G \\   

Table 3: Comparison of NMTs and GMTs methods on ActivityNet.