# Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift

Saurabh Garg*

Carnegie Mellon University

sgarg2@andrew.cmu.edu

&Amrith Setlur*

Carnegie Mellon University

asetlur@andrew.cmu.edu

&Zachary C. Lipton

Carnegie Mellon University

zlipton@andrew.cmu.edu

&Sivaraman Balakrishnan

Carnegie Mellon University

sbalakri@andrew.cmu.edu

&Virginia Smith

Carnegie Mellon University

smithv@andrew.cmu.edu

&Aditi Raghunathan

Carnegie Mellon University

aditirag@andrew.cmu.edu

Equal contribution.

###### Abstract

Self-training and contrastive learning have emerged as leading techniques for incorporating unlabeled data, both under distribution shift (unsupervised domain adaptation) and when it is absent (semi-supervised learning). However, despite the popularity and compatibility of these techniques, their efficacy in combination remains surprisingly unexplored. In this paper, we first undertake a systematic empirical investigation of this combination, finding (i) that in domain adaptation settings, self-training and contrastive learning offer significant complementary gains; and (ii) that in semi-supervised learning settings, surprisingly, the benefits are not synergistic. Across eight distribution shift datasets (_e.g._, BREEDs, WILDS), we demonstrate that the combined method obtains 3-8% higher accuracy than either approach independently. Finally, we theoretically analyze these techniques in a simplified model of distribution shift demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail.

## 1 Introduction

Even under natural, non-adversarial distribution shifts, the performance of machine learning models often drops . While we might hope to retrain our models on labeled samples from the new distribution, this option is often unavailable due to the expense or impracticality of collecting new labels. Consequently, researchers have investigated the Unsupervised Domain Adaptation (UDA) setting. Here, given labeled source data and unlabeled out-of-distribution (OOD) target data, the goal is to produce a classifier that performs well on the target. Because UDA is generally underspecified , researchers have focused on two main paths: (i) domain adaptation papers that explore heuristics for incorporating the unlabeled target data, relying on benchmark datasets ostensibly representative of "real-world shifts" to adjudicate progress ; and (ii) theoretically motivated papers that explore structural assumptions under which UDA problems are well posed . This work engages with the former focusing on two popular methods: self-training and contrastive pretraining.

Self-training  and contrastive pretraining  were both proposed, initially, for traditional Semi-Supervised Learning (SSL) problems, where the labeled and unlabeled data are drawn from the same distribution. Here the central challenge is statistical: to exploit the unlabeled data to learn a better predictor than one would get by training on the (small) labeled data alone. Morerecently, these methods have emerged as favored empirical approaches for UDA, demonstrating efficacy on many popular benchmarks [70; 30; 12; 76]. In self-training, one first learns a predictor using source labeled data. The predictor then produces pseudolabels for the unlabeled target data, and a new predictor is trained on the pseudolabeled data. Contrastive pretraining learns representations from unlabeled data by enforcing invariance to specified augmentations. These representations are subsequently used to learn a classifier. In UDA, the representations are trained on the union of the source and target data. In attempts to explain their strong empirical performance, several researchers have attempted analyses under various assumptions on the data, task, and inductive biases of the function class[87; 39; 73; 76; 12; 40; 38; 11]. Despite the strong strong results, there has been surprisingly little work (both empirically and theoretically) exploring when either might be expected to perform best and whether the benefits might be complementary.

In this paper, we investigate the complementary benefits of self-training and contrastive pretraining. Interestingly, we find that the combination yields significant gains in UDA despite producing negligible gains in SSL. In experiments across eight distribution shift benchmarks (_e.g._ BREEDs , FMoW , Visda ), we observe that re-using unlabeled data for self-training (with FixMatch ) after learning contrastive representations (with SwAV ), yields \(>5\%\) average improvement on OOD accuracy in UDA as compared to \(<0.8\%\) average improvement in SSL (Fig. 1).

Next, we address the question _why the combination of self-training and contrastive learning_ proves synergistic in distribution shift scenarios. To facilitate our analysis, we consider a simplified distribution shift setting that includes two types of features: (i) invariant features that perfectly predict the label; and (ii) domain-dependent features that are predictive of the label in just source. Our theoretical analysis reveals that self-training can achieve optimal target performance but requires a "good" enough classifier to start with. We observe that source-only ERM fails to provide a "good" initialization. On the other hand, contrastive pretraining on unlabeled data performs better than ERM but is still sub-optimal. This implies that contrastive pretraining ends up decreasing reliance on domain-dependent features (as compared to ERM) but doesn't completely eliminate them. Nevertheless, contrastive pretraining does provide a "good" initialization for self-training, _i.e._, "good" initial pseudolabels on the target unlabeled data. As a result, self-training on top of contrastive learned features effectively unlearns the reliance on domain-dependent features and generalizes perfectly OOD. In contrast, for SSL settings (_i.e._, in distribution), our analysis highlights that contrastive pretraining already acquires sufficient predictive features such that linear probing with (a small amount of) labeled data picks up those features and attains near-optimal ID generalization.

Finally, we connect our theoretical understanding of "good" representations from contrastive learning and improved linear transferability from self-training back to observed empirical gains. We linearly probe representations (fix representations and train only the linear head) learned by contrastive pretraining vs. no pretraining and find: (i) contrastive pretraining substantially improves the ceiling on

Figure 1: _Self-training over Contrastive learning (STOC) improves over Contrastive Learning (CL) under distribution shift._**(a)** We observe that in SSL settings, where labeled and unlabeled data are drawn from the same distribution, STOC offers negligible improvements over CL. In contrast, in UDA settings where there is distribution shift between labeled and unlabeled data, STOC offers gains over CL. Results aggregated across 8 benchmarks. Results on individual data in Table 1 and 2. **(b)** 2-D illustration of our simplified distribution setup, depicting decision boundaries learned by ERM and CL and how Self-Training (ST) updates those. 1, 2, and 3 summarize our theoretical results in Sec. 4.

the target accuracy (performance of optimal linear probe) compared to ERM; (ii) self-training mainly improves linear transfer, _i.e_. OOD performance for the linear probe trained with source labeled data.

## 2 Setup and Preliminaries

**Task.** Our goal is to learn a predictor that maps inputs \(x^{d}\) to outputs \(y\). We parameterize predictors \(f=h:^{d}\), where \(:^{d}^{k}\) is a feature map and \(h^{k}\) is a classifier that maps the representation to the final scores or logits. Let \(},}\) be the source and target joint probability measures over \(\) with \(}\) and \(}\) as the corresponding probability density (or mass) functions. The distribution over unlabeled samples from both the union of source and target is denoted as \(}=(1/2)}(x)+(1/2)}(x)\).

We study two particular scenarios: (i) Unsupervised Domain Adaptation (UDA); and (ii) Semi-Supervised Learning (SSL). In UDA, we assume that the source and target distributions have the same label marginals \(}(y)=}(y)\) (_i.e_., no label proportion shift) and the same Bayes optimal predictor, _i.e_., \(*{arg\,max}_{y}}(y x)=*{arg\,max} _{y}}(y x)\). We are given labeled samples from the source, and unlabeled pool from the target. In contrast in SSL, there is no distribution shift, _i.e_., \(}=}=}\). Here, we are given a small number of labeled examples and a comparatively large amount of unlabeled examples, both drawn from the same distribution, which we denote as \(}\).

Unlabeled data is typically much cheaper to obtain, and our goal in both these settings is to leverage this along with labeled data to achieve good performance on the target distribution. In the UDA scenario, the challenge lies in generalizing out-of-distribution, while in SSL, the challenge is to generalize in-distribution despite the paucity of labeled examples. A predictor \(f\) is evaluated on distribution \(\) via its accuracy, _i.e_., \(A(f,)=*{_{P}}(*{arg\,max}f(x)=y)\).

**Methods.** We now introduce the algorithms used for learning from labeled and unlabeled data.

1. _Source-only ERM (ERM):_ A standard approach is to simply perform supervised learning on the labeled data by minimizing the empirical risk \(_{i=1}^{n}(h(x),y)\), for some classification loss \(:\) (_e.g_., softmax cross-entropy) and labeled points \(\{(x_{i},y_{i})\}_{i=1}^{n}\).
2. _Contrastive Learning (CL):_ We first use the unlabeled data to learn a feature extractor. In particular, the objective is to learn a feature extractor \(_{}\) that maps augmentations (for e.g. crops or rotations) of the same input close to each other and far from augmentations of random other inputs [13; 16; 93]. Once we have \(_{}\), we learn a linear classifier \(h\) on top to minimize a classification loss on the labeled source data. We could either keep \(_{}\) fixed or propagate gradients through. When clear from context, we also use CL to refer to just the contrastively pretrained backbone without training for downstream classification.
3. _Self-training (ST):_ This is a two-stage procedure, where the first stage performs source-only ERM by just looking at source-labeled data. In the second stage, we iteratively apply the current classifier on the unlabeled data to generate "pseudo-labels" and then update the classifier by minimizing a classification loss on the pseudolabeled data .

## 3 Self-Training Improves Contrastive Pretraining Under Distribution Shift

**Self-Training Over Contrastive learning (STOC).** Finally, rather than starting with a source-only ERM classifier, we propose to initialize self-training with a CL classifier, that was pretrained on unlabeled source and target data. ST uses that same unlabeled data again for pseudolabeling. As we demonstrate experimentally and theoretically, this combination of methods improves substantially over each independently.

**Datasets.** For both UDA and SSL, we conduct experiments across eight benchmark datasets: four BREEDs datasets --Entity13, Entity30, Nonliving26, Living17; FMoW [47; 18] from WillDS benchmark; Officehome ; Visda [64; 63]; and CIFAR-10 . Each of these datasets consists of domains, enabling us to construct source-target pairs (e.g., CIFAR10, we consider CIFAR10\(\)CINIC shift ). In the UDA setup, we adopt the source and target domains standard to previous studies (details in App. C.2). Because the SSL setting lacks distribution shift, we do not need to worry about domain designations and default to using source alone. To simulate limited supervision in SSL, we sub-sample the original labeled training set to 10%.

Experimental Setup and Protocols.SwAV  is the specific algorithm that we use for contrastive pretraining. In all UDA settings, unless otherwise specified, we pool all the (unlabeled) data from the source and target to perform SwAV. For self-training, we apply FixMatch , where the loss on source labeled data and on pseudolabeled target data are minimized simultaneously. For both methods, we fix the algorithm-specific hyperparameters to the original recommendations. For SSL settings, we perform SwAV and FixMatch on in-distribution unlabeled data. We experiment with Resnet18, Resnet50  trained from scratch (random initialization). We do not consider off-the-shelf pretrained models (_e.g._, on Imagenet ) to avoid confounding our conclusions about contrastive pretraining. However, we note that our results on most datasets tend to be comparable to and sometimes exceed those obtained with ImageNet-pretrained models. For source-only ERM, as with other methods (FixMatch, SwAV), we default to using strong augmentation techniques: random horizontal flips, random crops, augmentation with Cutout , and RandAugment . Moreover, unless otherwise specified, we default to full finetuning with source-only ERM, both from scratch and after contrastive pretraining, and for ST with FixMatch. For UDA, given that the setup precludes access to labeled data from the target distribution, we use source hold-out performance to pick the best hyperparameters. During pretraining, early stopping is done according to lower values of pretraining loss. For more details on datasets, model architectures, and experimental protocols, see App. C.

Results on UDA setup.Both ST and CL individually improve over ERM across all datasets, with CL significantly performing better than ST on 5 out of 8 benchmarks (see Table 1). Even on datasets where ST is better than CL, their performance remains close. Combining ST and CL with STOC shows an \(3\)-\(8\%\) improvement over the best alternative, yielding an absolute improvement in average accuracy of \(5.2\%\).

Note that by default, we train with CL on the combined unlabeled data from source and target. However, to better understand the significance of unlabeled target data in contrastive pretraining, we perform an ablation where the CL model was trained solely on unlabeled source data (refer to this as CL (source only); see App. C.4). We observe that ST on top of CL (source only) improves over ST (from scratch). However, the average performance of ST over CL (source only) is similar to that of standalone CL, maintaining an approximate 6% performance gap observed between CL and ST. This brings two key insights to the fore: (i) the observed benefit is not merely a result of the contrastive pretraining objective alone, but specifically CL with unlabeled target data helps; and (ii) both CL and ST leverage using target unlabeled data in a complementary nature.

Results on SSL setup.While CL improves over ST (as in UDA), unlike UDA, STOC doesn't offer any significant improvements over CL (see Table 2; ERM and ST results (refer to App. C.5). We conduct ablation studies with varying proportions of labeled data used for SSL, illustrating that there's considerable potential for improvement (see App. C.5). These findings highlight that the complementary nature of STOC over CL and ST individually is an artifact of distribution shift.

   Method & Living17 & Nonliv26 & Entity13 & Entity30 & FMoW & Visda & OH & CIFAR\(\) & Avg \\  CL & \(91.15\) & \(84.58\) & \(90.73\) & \(85.47\) & \(43.05\) & \(97.67\) & \(49.73\) & \(91.78\) & \(79.27\) \\ STOC (ours) & \(92.00\) & \(85.95\) & \(91.27\) & \(86.14\) & \(44.43\) & \(97.70\) & \(49.95\) & \(93.06\) & \(80.06\) \\   

Table 2: _Results in the SSL setup._ We report accuracy on hold-out ID data. Recall that SSL uses labeled and unlabeled data from the same distribution during training. Refer to App. C.5 for ERM and ST.

   Method & Living17 & Nonliv26 & Entity13 & Entity30 & FMoW & Visda & OH & CIFAR & Avg \\  CL & \(91.15\) & \(84.58\) & \(90.73\) & \(85.47\) & \(43.05\) & \(97.67\) & \(49.73\) & \(91.78\) & \(79.27\) \\ STOC (ours) & \(92.00\) & \(85.95\) & \(91.27\) & \(86.14\) & \(44.43\) & \(97.70\) & \(49.95\) & \(93.06\) & \(80.06\) \\   

Table 1: _Results in the UDA setup._ We report accuracy on target (OOD) data from which we only observe unlabeled examples during training. For benchmarks with multiple target distributions (_e.g._, OH, Visda), we report avg accuracy on those targets. Results with source performance, individual target performance, and standard deviation numbers are in App. C.4.

Theoretical Analysis and Intuitions

Our results on real-world datasets suggest that although self-training may offer little to no improvement over contrastive pretraining for in-distribution (_i.e._, SSL) settings, it leads to substantial improvements when facing distribution shifts in UDA (Sec. 3). Why do these methods offer complementary gains, but only under distribution shifts? In this section, we seek to answer this question by first replicating all the empirical trends of interest in a simple data distribution with an intuitive story (Sec. 4.1). In this toy model, we formally characterize the gains afforded by contrastive pretraining and self-training both individually (Secs. 4.2, 4.3) and when used together (Sec. 4.4).

Data distributionWe consider binary classification and model the inputs as consisting of two kinds of features: \(x=[x_{},x_{}]\), where \(x_{}^{d_{}}\) is the invariant feature that is predictive of the label across both source \(}\) and target \(}\) and \(x_{}^{d_{}}\) is the spurious feature that is correlated with the label \(y\) only on the source domain \(}\) but uncorrelated with label \(y\) in \(}\). Formally, we sample \(\{-1,1\}\) and generate inputs \(x\) conditioned on \(\) as follows:

\[}\,:\,\,x_{} ( yw^{},_{})\,\, \,x_{}=_{d_{}}\] \[}\,:\,\,x_{} ( yw^{},_{})\,\, \,x_{}(,_{}),\] (1)

where \(\) is the margin afforded by the invariant feature2. We set the covariance of the invariant features \(_{}=_{}^{2}(_{d_{} }-w^{}w^{})\). This makes the variance along the latent predictive direction \(w^{}\) to be zero. Note that the spurious feature is also completely predictive of the label in the source data. In fact, when \(d_{}\) is sufficiently large, \(x_{}\) is more predictive (than \(x_{}\)) of \(\) in the source. In the target, \(x_{}\) is distributed as a Gaussian with \(_{}=_{}^{2}_{d_{}}\). We use \(w_{}\!\!=\!\![w^{},0,...,0]^{}\) to refer to the invariant direction/feature, and \(w_{}=[0,...,0,1_{4}\!/\!}}]^{}\) for the spurious direction.

Data for UDA vs. SSLFor convenience, we assume access to infinite unlabeled data and replace their empirical quantities with population counterparts. For SSL, we sample both finite labeled and infinite unlabeled data from the same distribution \(}\), where spurious features are absent (to exclude easy-to-generalize features). For UDA, we assume infinite labeled data from \(}\) and infinite unlabeled from \(}\). Importantly, note that due to distribution shift, population access of \(}\) doesn't trivialize the problem as "ERM" on infinite labeled source data _does not_ achieve optimal performance on target.

Methods and objectivesRecall from Section 2 that we learn linear classifiers \(h\) over feature extractor \(\). For our toy setup, we consider linear feature extractors i.e. \(\) is a matrix in \(^{d k}\) and the prediction is given by \((h^{} x)\). We use the exponential loss \((f(x),y)=\).

Self-trainingST performs ERM in the first stage using labeled data from the source, and then subsequently updates the head \(h\) by iteratively generating pseudolabels on the unlabeled target:

\[_{}(h;) := _{}(x)}(h^{} x,(h^ {}(x)))\] (2) \[\,\,\,h^{t+1} = -_{h}_{}(h^{t};)}{  h^{t}-_{h}_{}(h^{t};) _{2}}\]

For ERM and ST, we train both \(h\) and \(\) (equivalent to \(\) being identity and training a linear head).

Contrastive pretrainingWe obtain \(_{}:=_{}_{}()\) by minimizing the Barlow Twins objective , which prior works have shown is also equivalent to spectral contrastive and non-contrastive objectives . Given probability distribution \(}(a x)\) for input \(x\), and marginal \(}\), we consider a constrained form of Barlow Twins in (3) which enforces features of "positive pairs" \(a_{1},a_{2}\) to be close while ensuring feature diversity. We assume a strict regularization \((=0)\) for the theory arguments in the rest of the paper, and in App. D.2 we prove that all our claims hold for small \(\) as well. For augmentations, we scale the magnitude of each co-ordinate uniformly by an independent amount, i.e., \(a}(\, x)= x\), where \(^{d}\). We try to mirror practical settings where the augmentations are fairly "generic", not encoding information about which features are invariant or spurious, and hence perturb all features symmetrically.

\[_{}() :=_{x}}_{a_{1},a_{2} }(\, x)}\,\,(a_{1})-(a_{2}) _{2}^{2}\] s.t. \[_{a}}[(a)(a)^ {}]-_{}_{F}^{2}\] (3)Keeping the \(_{}\) fixed, we then learn a linear classifier \(h_{}\) over \(_{}\) to minimize the exponential loss on labeled source data (refer to as _linear probing_). For STOC, keeping the \(_{}\) fixed and initializing the linear head with the CL linear probe (instead of source only ERM), we perform ST with (2).

**Example 1**.: _For the setup in (1), we choose \(=0.5\), \(_{}^{2}=1\)., and \(_{}^{2}=0.05\) with \(d_{}=5\) and \(d_{}=20\) for our running example. \(}{{}}}}\) controls signal to noise ratio in the source such that spurious feature is easy-to-learn and the invariant feature is harder-to-learn. \(_{2}\) controls the noise in target which we show later is critical in unlearning the spurious feature with CL._

### Simulations and Intuitive Story: A Comparative Study Between SSL and DA

Our setup captures real-world trends in UDA setting.Our toy setup (in Example 1) accentuates the behaviors observed on real-world datasets (Fig. 2(a)): (i) both ERM and ST yield close to random performance (though ST performs slightly worse than ERM); (ii) CL improves over ERM but still yields sub-optimal target performance; (iii) STOC then further improves over CL, achieving near-optimal target performance. Note that, a linear predictor can improve target performance only by reducing its dependence on spurious feature \(x_{}\), and increasing it on invariant feature \(x_{}\) (along \(w^{}\)). Given this, we can explain our trends if we understand the following: (i) how ST reduces dependence on spurious feature when done after CL; (ii) why CL helps reduce but not completely eliminate the reliance of linear head on spurious features. Before we present intuitions, we ablate over a key problem parameter that affects both the target performance and conditions for ST to work.

Effect of \(/_{}\) on success of ST.By increasing the ratio of margin \(\) and variance of spurious feature on target \(_{}\) (keeping others constant), the problem becomes easier because \(\) directly affects the signal on \(x_{}\) and reducing \(_{}\) helps ST to unlearn \(x_{}\) (see App. D.3). In Fig. 2(c), we see that a phase transition occurs for ST, _i.e._, after a certain threshold of \(/_{}\), ST successfully recovers the optimal target predictor. This hints that ST has a binary effect, where beyond a certain magnitude of \(/_{}\), ST can amplify the signal on domain invariant feature to obtain optimal target predictor. On the other hand, the performance of CL and ERM improve gradually where CL achieves high performance even at small ratios of \(/_{}\). One way of viewing this trend with CL is that it magnifies the effective \(/_{}\) in its representation space, because of which a linear head trained these representations have a good performance at low values of the ratio. Consequently, the _phase transition_ of STOC occurs much sooner than that of ST. Finally, we note that for CL the rate of performance increase diminishes at high values of \(/_{}\) because CL fails to reduce dependency along \(x_{}\) beyond a certain point.

An intuitive story.We return to the question of why self-training improves over contrastive learning under distribution shift in our Example 1. When the classifier at initialization of ST relies more on spurious features, ST aggravates this dependency. However, as the problem becomes easier (with increasing \(/_{}\)), the source-only ERM classifier will start relying more on invariant rather than spurious feature. Once this ERM classifier is sufficiently accurate on the target, ST unlearns any dependency on spurious features achieving optimal target performance. In contrast, we observe that CL performs better than ERM but is still sub-optimal. This implies that CL ends up decreasing reliance on spurious features (as compared to ERM) but doesn't completely eliminate them. Combining ST and CL, a natural hypothesis explaining our trends is that CL provides a "favorable" initialization

Figure 2: _Our simplified model of shift captures real-world trends and theoretical behaviors:_ **(a)** Target (OOD) accuracy separation in the UDA setup (for problem parameters in Example 1). **(b)** Comparison of the benefits of STOC (ST over CL) over just CL in UDA and SSL settings, done across training iterations for contrastive pretraining. **(c)** Comparison between different methods in UDA setting, as we vary problem parameters \(\) and \(_{}\), connecting our theory results in Sec. 4.

for ST by sufficiently increasing signal on invariant features. Our empirical findings emphasize an intriguing contrast suggesting that ST and CL improve target performance in complementary ways.

Why disparate behaviors for out-of-distribution vs. in distribution?In the SSL setup, recall, there is no distribution shift. In Example 1, we sample \(50k\) unlabeled data and \(100\) labeled data from the same (target) distribution to simulate SSL setup. Substantiating our findings on real-world data, we observe that STOC provides a small to negligible improvement over CL (refer to App. D). To understand why such disparate behaviors emerge, recall that in the UDA setting, the main benefit of STOC lies in picking up reliance on "good" features for OOD data, facilitated by CL initialization. While contrastive pretraining uncovers features that are "good" for OOD data, it also learns more predictive source-only features (which are not predictive at all on target). As a result, linear probing with source-labeled data picks up these source-only features, leaving considerable room for improvement on OOD data with further self-training. On the other hand, in the SSL setting, the limited ID labeled data might provide enough signal to pick up features predictive on ID data, leaving little to no room for improvement for further self-training. Corroborating our intuitions, throughout the CL training in the toy setup, when CL doesn't achieve near-perfect generalization, the improvements provided by STOC for each checkpoint remain minimal. On the other hand, for UDA setup, after reaching a certain training checkpoint in CL, STOC yields significant improvement (Fig. 2(b)).

In the next sections, we formalize our intuitions and analyze why ST and CL offer complementary benefits when dealing with distribution shifts. Formal statements and proofs are in App. E.

### Conditions for Success and Failure of Self-training over ERM from Scratch

In our results on Example 1, we observe that performing ST after ERM yields a classifier with near-random target accuracy. In Theorem 2, we characterize conditions under which ST fails and succeeds.

**Theorem 2** (Informal; Conditions for success and failure of ST over ERM).: _The target accuracy of ERM classifier, is given by \(0.5({-}{{2}}}/{(}},_{})})\). Then ST performed in the second stage yields: (i) a classifier with \( 0.5\) target accuracy when \(<}{{2}}_{}\) and \(_{} 1\); and (ii) a classifier with near-perfect target accuracy when \(_{}\)._

The informal theorem above abstracts the exact dependency of \(,_{}\), and \(d_{}\) for the success and failure of ST over ERM. Our analysis highlights that while ERM learns a perfect predictor along \(w_{}\) (with norm \(\)), it also learns to depend on \(w_{}\) (with norm \(}}\)) because of the perfect correlation of \(x_{}\) with labels on the source. Our conditions depict that when the \(/_{}\) is sufficiently small, then ST continues to erroneously enhance its reliance on the \(x_{}\) feature for target prediction, resulting in near-random target performance. Conversely, when \(/_{}\) is larger than 1, the signal in \(x_{}\) is correctly used for predictor on the majority of target points, and ST eliminates the \(x_{}\) dependency, converging to an optimal target classifier.

Our proof analysis shows that if the ratio of the norm of the classifier along in the direction of \(w^{}\) is smaller than \(w_{}\) by a certain ratio then the generated pseudolabels (incorrectly) use \(x_{}\) for its prediction further increasing the component along \(w_{}\). Moreover, normalization further diminishes the reliance along \(w^{}\), culminating in a near-random performance. The opposite occurs when the ERM classifier achieves a signal along \(w^{}\) that is sufficiently stronger than along \(w_{}\). Upon substituting the parameters used in Example 1, the ERM and ST performances as determined by Theorem 2 align with our empirical results, notably, ST performance on target being near-random.

### CL Captures Both Features But Amplifies Invariant Over Spurious Features

Here we show that minimizing the contrastive loss (3) on unlabeled data from both \(}}\) and \(}}\) gives us a feature extractor \(_{}\) that has a higher inner product with the invariant feature over the spurious feature. First, we derive a closed form expression for \(_{}\) that holds for any linear backbone and augmentation distribution. Then, we introduce assumptions on the augmentation distribution (or equivalently on \(w^{}\)) and other problem parameters, that are sufficient to prove amplification.

**Proposition 3** (Barlow Twins solution).: _The solution for (3) is \(U_{k}^{}_{}^{-1/2}\) where \(U_{k}\) are the top \(k\) eigenvectors of \(_{}^{-1/2}}_{} ^{-1/2}\). Here, \(_{}_{a}}}[aa ^{}]\) is the covariance over augmentations, and \(}_{x}}}[(x)(x)^{}]\) is the covariance matrix of mean augmentations \((x)_{}}(a|x)}[a]\)._

The above result captures the effect of augmentations through the matrix \(U_{k}\). If there were no augmentations, then \(_{}=}\), implying that \(U_{k}\) could then be any random orthonormal matrix. Onthe other hand if augmentation distributions change prevalent covariances in the data, _i.e._, \(_{}\) is very different from \(\), the matrix \(U_{k}\) would bias the CL solution towards directions that capture significant variance in marginal distribution on augmented data, but have low conditional variance, when conditioned on original point \(x\)--precisely the directions with low invariance loss. Hence, we can expect that CL would learn components along both invariant \(w_{}\) and spurious \(w_{}\) because: (i) these directions explain a large fraction of variance in the raw data; (ii) augmentations that randomly scale down dimensions would add little variance along \(w_{}\) and \(w_{}\) compared to noise directions in their null space. On the other hand it is unclear which of these directions is amplified more in \(_{}\). The following assumption and amplification result conveys that when the noise in target \((_{})\) is sufficiently large, the CL solution amplifies the invariant feature over the spurious feature.

**Assumption 4** (Informal; Alignment of \(w^{}\) with augmentations).: _We assume that \(w^{}\) aligns with \(}( x)\), i.e., \( x\), \(_{a x}[a^{}w^{}]=}{{2}} x^{} (_{d})w^{}\) is high. Hence, we assume \(w^{}=}{{4_{}}}}}\)._

One implication of Assumption 4 is that when \(w^{}=}{{4_{}}}}}\), only the top two eigenvectors lie in the space spanned by \(w_{}\) and \(w_{}\). To analyze our amplification with fewer eigenvectors from Proposition 3 while retaining all relevant phenomena, we assume \(w^{}=}{{4_{}}}}}\) for mathematical convenience. While Assumption 4 permits a tighter theoretical analysis, our empirical results in Sec. 4.1 hold more generally for \(w^{}(0,_{d_{}})\).

**Theorem 5** (Informal; CL recovers both \(w_{}\) and \(w_{}\) but amplifies \(w_{}\)).: _Under Assumption 4, the CL solution \(_{}\)=\([_{1},_{2},...,_{k}]\) satisfies \(_{j}^{}w_{}=_{j}^{}w_{}=0\)\( j 3\), \(_{1}=c_{1}w_{}+c_{3}w_{}\) and \(_{2}=c_{2}w_{}+c_{4}w_{}\). For constants \(K_{1},K_{2}>0\), \(=K_{2}}}{{_{}}}\), \(d_{}=}^{2}}}{{^{2}}}\), \(>0\), \(_{_{0}}\), such that for \(_{}_{_{0}}\), \(/c_{3}.}}{{c_{3}}}-K_{2}^{2}d_{ }}}{{2L_{}}}(d_{}-1)\), and \(/c_{4}|}}{{-L}/}}} \), where \(L=1+K_{2}^{2}\)._

We analyze the amplification of \(w_{}/w_{}\) with contrastive learning in the regime where \(_{}\) is large enough. In other words, if the target distribution has sufficient noise along the spurious feature, the augmentations prevent the CL solution from extracting components along \(w_{}\). Thus, in our analysis, we first analyze the amplification factors asymptotically \((_{})\), and then use the asymptotic behavior to draw conclusions for the regime where \(_{}\) is large but finite.

Theorem 5 conveys two results: (i) CL recovers components along both \(w_{}\) and \(w_{}\) through \(_{1},_{2}\); and (ii) it increases the norm along \(w_{}\) more than \(w_{}\). The latter is evident because the margin separating labeled points along \(w_{}\) is now amplified by a factor of \(/c_{4}|}}{{(L}}/)}}\) in \(_{2}\). Naturally, this will improve the target performance of a linear predictor trained over CL representations. At the same time, we also see that in \(_{1}\), the component along \(w_{}\) is still significant (\(/c_{3}}}{{c_{3}}}=(}{{L_{ {in}}^{2}}})\)). Intuitively, CL prefers the invariant feature since augmentations amplify the noise along \(w_{}\) in the target domain. At the same time, the variance induced by augmentations along \(w_{}\) in source is still very small due to which the dependence on \(w_{}\) is not completely alleviated. Due to the remaining components along \(w_{}\), the target performance for CL can remain less than ideal. Both the above arguments on target performance are captured in Corollary 6.

**Corollary 6** (Informal; CL improves OOD error over ERM but is still imperfect).: _For \(,_{},d_{}\) defined as in Theorem 5, \(_{1}\) such that for all \(_{}_{1}\), the target accuracy of CL (linear predictor on \(_{}\)) is \( 0.5({-L^{}}{{ _{}}}})\) and \( 0.5({-L^{}}{{ _{}}}})\), where \(L^{}=^{2}K_{1}}}{{_{}^{2}(1-}{{4_{}}})}}\). When \(_{1}>_{}}{{d_{}}}}\), the lower bound on accuracy is strictly better than ERM from scratch._

While \(_{}\) is still not ideal for linear probing, in the next part we will see how \(_{}\) can instead be sufficient for subsequent self-training to unlearn the remaining components along spurious features.

### Improvements with Self-training Over Contrastive Learning

The result in the previous section highlights that while CL may improve over ERM, the linear probe continues to depend on the spurious feature. Next, we characterize the behavior STOC. Recall, in the ST stage, we iteratively update the linear head with (2) starting with the CL backbone and head.

**Theorem 7** (Informal; ST improves over CL).: _Under the conditions of Theorem 5 and \(d_{} K_{1}^{2} K_{2}^{2/3}\), the target accuracy of ST over CL is lower bounded by \(0.5({-|{c2/c4}|}{{ _{})}}}}) 0.5({-L}}} }{{_{}}}})\) where \(c_{2}\) and \(c_{4}\) are the coefficients of feature \(_{2}\) along \(w_{}\) and \(w_{}\) learned by BT._The above theorem states that when \(}/{_{}} 1\) the target accuracy of ST over CL is close to 1. In Example 1, the lower bound of the accuracy of ST over CL is \((-) 2\) showing near-perfect target generalization. Recall that Theorem 6 shows that CL yields a linear head that mainly depends on both the invariant direction \(w_{}\) and the spurious direction \(w_{}\). At initialization, the linear head trained on the CL backbone has negligible dependence on \(_{2}\) (under conditions in Theorem 6). Building on that, the analysis in Theorem 7 captures that ST gradually reduces the dependence on \(w_{}\) by learning a linear head that has a larger reliance on \(_{2}\), which has a higher "effective" margin on the target, thus increasing overall dependency on \(w_{}\).

Theoretical comparison with SSL.Our analysis until now shows that linear probing with source labeled data during CL picks up features that are more predictive of source label under distribution shift, leaving a significant room for improvement on OOD data when self-trained further. In UDA, the primary benefit of ST lies in picking up the features with a high "effective" margin on target data that are not picked up by linear head trained during CL. In contrast, in the SSL setting, the limited ID labeled data may provide enough signal in picking up high-margin features that are predictive on ID data, leaving little to no room for improvement for further ST. We formalize this intuition in App. E.

### Reconciling Practice: Implications for Deep Non-Linear Networks

In this section, we experiment with deep non-linear backbone (_i.e_., \(_{}\)). When we continue to fix \(_{}\) during CL and STOC, the trends we observed with linear networks in Sec. 4.1 continue to hold. We then perform full fine-tuning with CL and STOC, i.e., propagate gradients even to \(_{}\), as commonly done in practice. We present key takeaways here but detailed experiments are in App. D.4.

Benefits of augmentation for self-training.ST while updating \(_{}\) can hurt due to overfitting issues when training with the finite sample of labeled and unlabeled data (drop by >10% over CL). This is due to the ability of deep networks to overfit on confident but incorrect pseudolabels on target data . This exacerbates components along \(w_{}\) and we find that augmentations (and other heuristics) typically used in practice (_e.g_. in FixMatch ) help avoid overfitting on incorrect pseudolabels.

Can ERM and ST over contrastive pretraining improve features?We find that self-training can also slightly improve features when we update the backbone with the second stage of STOC and when the CL backbone is early stopped sub-optimally (_i.e_. at an earlier checkpoint in Fig. 2(b)). This feature finetuning can now widen the gap between STOC and CL in SSL settings, as compared to the linear probing gap (as in 2). This is because STOC can now improve performance beyond just recovering the generalization gap for the linear head (which is typically small). However, STOC benefits are negligible when CL is not early stopped sub-optimally, _i.e_., trained till convergence. Thus, it remains unclear if STOC and CL have complementary benefits for feature learning in UDA or SSL settings. Investigating this is an interesting avenue for future work.

## 5 Connecting Experimental Gains with Theoretical Insights

Our theory emphasizes that under distribution shift contrastive pretraining improves the representations for target data, while self-training primarily improves linear classifiers learned on top. To investigate different methods in our UDA setup, we study the representations learned by each of them. We fix the representations and train linear heads over them to answer two questions: (i) How good are the representations in terms of their _ceiling_ of target accuracy (performance of the optimal linear probe)?--we evaluate this by training the classifier head on target labeled data (_i.e_., target linear probe); and (ii) How well do heads trained on source generalize to target?--we assess this by training a head on source labeled data (source linear probe) and evaluate its difference with target linear probe. For both, we plot target accuracy. We make two _intriguing_ observations Fig. 3):

**Does CL improve representations over ERM features?** Yes. We observe a substantial difference in accuracy (\( 14\%\) gap) of target linear probes on backbones trained

Figure 3: _Target accuracy with source and target linear probes, which freezes backbones trained with various objectives and trains only the head in UDA setup. Avg. accuracy across all datasets. We observe that: (i) ST improves the linear transferability of source probes, and (ii) CL improves representations._

with contrastive pretraining (_i.e_. CL, STOC) and without it (_i.e_., ERM, ST) highlighting that CL significantly pushes the performance ceiling over non-contrastive features. As a side, our findings also stand in contrast to recent studies suggesting that ERM features might be "good enough" for OOD generalization [67; 46]. Instead, the observed gains with contrastively pretrained backbones (_i.e_. CL, STOC) demonstrate that target unlabeled data can be leveraged to further improve over ERM features.

**Do CL features yield _perfect_ linear transferability from source to target?** Recent works [40; 76] conjecture that under certain conditions CL representations, linear probes learned with source labeled data may transfer perfectly from source to target. However, we observe that this doesn't hold strictly in practice, and in fact, the linear transferability can be further improved with ST. We first note a significant gap between the performance of source linear probes and target linear probes illustrating that linear transferability is not perfect in practice. Moreover, while the accuracy of target linear probes doesn't change substantially between CL and STOC, the accuracy of the source linear probe improves significantly. Similar observations hold for ERM and ST, methods trained without contrastive pretraining. This highlights that ST performs "feature refinement" to improve source to target linear transfer (with relatively small improvements in their respective target probe performance). _The findings highlight the complementary nature of benefits on real-world data: ST improves linear transferability while CL improves representations._

## 6 Connections to Prior Work

Our empirical results and our analyses offer a perspective that contrasts with the prior literature that argues for the individual optimality of contrastive pretraining and self-training. We outline the key differences from existing studies here, and delve into other related works in App. A.

**Limitations of prior work analyzing contrastive learning** Prior works [40; 76] analyzing CL first make assumptions on the consistency of augmentations with labels [39; 11; 73; 44], and specifically for UDA make stronger ones on the augmentation graph connecting examples from same domain or class more than cross-class/cross-domain ones. While this is sufficient to prove linear transferability, it is unclear if this holds in practice when augmentations are imperfect, _i.e_. if they fail to mask the spurious features completely--as corroborated by our findings in Sec. 5. We show why this also fails in our simplified setup in App. F.1.

**Limitations of prior work analyzing self-training** Prior research views self-training as consistency regularization, ensuring pseudolabels for original samples align with their augmentations [12; 87; 79]. This approach abstracts the role played by the optimization algorithm and instead evaluates the global minimizer of a population objective promoting pseudolabel consistency. It also relies on specific assumptions about class-conditional distributions to guarantee pseudolabel accuracy across domains. However, this framework doesn't address issues in iterative label propagation. For example, when augmentation distribution has long tails, the consistency of pseudolabels depends on the sampling frequency of "favorable" augmentations (for more discussion see App. F.2). Our analysis thus follows the iterative examination of self-training .

## 7 Conclusion

In this study, we highlight the synergistic behavior of self-training and contrastive pretraining under distribution shift. Shifts in distribution are commonplace in real-world applications of machine learning, and even under natural, non-adversarial distribution shifts, the performance of machine learning models often drops. By simply combining existing techniques in self-training and constrastive learning, we find that we can improve accuracy by 3-8% rather than using either approach independently. Despite these significant improvements, we note that one limitation of this combined approach is that performing self-training sequentially after contrastive pretraining increases the computation cost for UDA. The potential for integrating these benefits into one unified training paradigm is yet unclear, presenting an interesting direction for future exploration.

Beyond this, we note that our theoretical framework primarily confines the analysis to training the backbone and linear network independently during the pretraining and fine-tuning/self-training phases. Although our empirical observations apply to deep networks with full fine-tuning, we leave a more rigorous theoretical study of full fine-tuning for future work. Our theory also relies on a covariate shift assumption (where we assume that label distribution also doesn't shift). Investigating the complementary nature of self-training and contrastive pretraining beyond the covariate shift assumption would be another interesting direction for future work.