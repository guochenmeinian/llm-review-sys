# Which Models have Perceptually-Aligned Gradients?

An Explanation via Off-Manifold Robustness

 Suraj Srinivas

Harvard University

Cambridge, MA

ssrinivas@seas.harvard.edu

&Sebastian Bordt

University of Tubingen, Tubingen AI Center

Tubingen, Germany

sebastian.bordt@uni-tuebingen.de

&Himabindu Lakkaraju

Harvard University

Cambridge, MA

hlakkaraju@hbs.edu

Equal Contribution

###### Abstract

One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via _off-manifold robustness_, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via gradient norm regularization, randomized smoothing, and adversarial training with projected gradient descent. Quantifying the perceptual alignment of model gradients via their similarity with the gradients of generative models, we show that off-manifold robustness correlates well with perceptual alignment. Finally, based on the levels of on- and off-manifold robustness, we identify three different regimes of robustness that affect both perceptual alignment and model accuracy: weak robustness, bayes-aligned robustness, and excessive robustness. Code is available at https://github.com/tml-tuebingen/pags.

## 1 Introduction

An important desideratum for machine learning models is _robustness_, which requires that models be insensitive to small amounts of noise added to the input. In particular, _adversarial robustness_ requires models to be insensitive to adversarially chosen perturbations of the input. Tsipras et al.  first observed an unexpected benefit of such models, namely that their input-gradients were "significantly more human-aligned" (see Figure 1 for examples of perceptually-aligned gradients). Santurkar et al.  built on this observation to show that robust models could be used to perform rudimentary image synthesis - an unexpected capability of models trained in a purely discriminative manner. Subsequent works have made use of the perceptual alignment of robust model gradients to improve zero-shotobject localization , perform conditional image synthesis , and improve classifier robustness [5; 6]. Kaur et al.  coined the term _perceptually-aligned gradients_ (PAGs), and showed that it occurs not just with adversarial training , but also with randomized smoothed models . Recently, Ganz et al.  showed that the relationship between robustness and perceptual alignment can also work in the opposite direction: approximately enforcing perceptual alignment of input gradients can increase model robustness.

Despite these advances, the underlying mechanisms behind the phenomenon of perceptually aligned gradients in robust models are still unclear. Adding to the confusion, prior works have used the same term, PAGs, to refer to slightly different phenomena. We ground our discussion by first identifying variations of the same underlying phenomenon.

**Phenomenon 1** (Perceptual Alignment).: _The gradients of robust models highlight perceptually relevant features , and highlight discriminative input regions while ignoring distractors ._

**Phenomenon 2** (Generative Capabilities).: _Robust models have rudimentary image synthesis capabilities, where samples are generated by maximizing a specific output class by iteratively following the direction dictated by the input-gradients ._

**Phenomenon 3** (Smoothgrad Interpretability).: _Smoothgrad visualizations (which are gradients of randomized smooth models) tend to be visually sharper than the gradients of standard models ._

While Kaur et al.  refer to Phenomenon 2 as PAGs, Ganz et al.  use this term to refer to Phenomenon 1. Since they are all related to the gradients of robust models, we choose to collectively refer to all of these phenomena as PAGs.

While the various phenomena arising from PAGs are now well-documented, there is little to no work that attempts to explain the underlying mechanism. Progress on this problem has been hard to achieve because PAGs have been described via purely qualitative criteria, and it has been unclear how to make these statements quantitative. In this work, we address these gaps and make one of the first attempts at explaining the mechanisms behind PAGs. Crucially, we ground the discussion by attributing PAGs to gradients lying on a manifold, based on an analysis of Bayes optimal classifiers. We make the following contributions:

1. We establish the first-known theoretical connections between Bayes optimal predictors and the perceptual alignment of classifiers via _off-manifold_ robustness. We also identify the manifold w.r.t. which this holds, calling it the _signal manifold_.
2. We experimentally verify that models trained with gradient-norm regularization, noise augmentation and randomized smoothing all exhibit off-manifold robustness.
3. We _quantify_ the perceptual alignment of gradients by their perceptual similarity with the score of the data distribution, and find that this measure correlates well with off-manifold robustness.

Figure 1: A demonstration of the perceptual alignment phenomenon. The input-gradients of robust classifiers (“robust gradient”) are perceptually similar to the score of diffusion models , while being qualitatively distinct from input-gradients of standard models (”standard gradient”). Best viewed in digital format.

4. We identify **three regimes of robustness:**_weak_ robustness, _bayes-aligned_ robustness and _excessive_ robustness, that differently affect both perceptual alignment and model accuracy.

## 2 Related Work

Robust training of neural networksPrior works have considered two broad classes of model robustness: adversarial robustness and robustness to normal noise. Adversarial robustness is achieved by training with adversarial perturbations generated using project gradient descent , or by randomized smoothing , which achieves certified robustness to adversarial attacks by locally averaging with normal noise. Robustness to normal noise is achieved by explicitly training with noise, a technique that is equivalent to Tikhonov regularization for linear models . For non-linear models, gradient-norm regularization  is equivalent to training with normal noise under the limit of training with infinitely many noise samples . In this paper, we make use of all of these robust training approaches and investigate their relationship to PAGs.

Gradient-based model explanationsSeveral popular post hoc explanation methods[16; 17; 12] estimate feature importances by computing gradients of the output with respect to input features and aggregating them over local neighborhoods . However, the visual quality criterion used to evaluate these explanations has given rise to methods that produce visually striking attribution maps, while being independent of model behavior . While  attribute visual quality to implicit score-based generative modeling of the data distribution,  propose that it depends on explanations lying on the data manifold. While prior works have attributed visual quality to generative modeling of data distribution or explanations lying on data manifold, our work demonstrates for the first time that (1) off-manifold robustness is a crucial factor, and (2) it is not the data manifold / distribution, rather the signal manifold / distribution (defined in Section 3.2) that is the critical factor in explaining the phenomenon of PAGs.

## 3 Explaining Perceptually-Aligned Gradients

Our goal in this section is to understand the mechanisms behind PAGs. We first consider PAGs as lying on a low-dimensional manifold, and show theoretically that such on-manifold alignment of gradients is equivalent to off-manifold robustness of the model. We then argue that Bayes optimal models achieve both off-manifold robustness and on-manifold gradient alignment. In doing so, we introduce the distinction between the data and the signal manifold, which is key to understanding the manifold structure of PAGs. Finally, we present arguments for why empirical robust models also exhibit off-manifold robustness.

NotationThroughout this paper, we consider the task of image classification with inputs \(^{d}\) where \(\) and \(y[1,2,...C]\) with \(C\)-classes. We consider deep neural networks \(f:^{d}^{C-1}\) which map inputs \(\) onto a \(C\)-class probability simplex. We assume that the input data \(\) lies on a \(k\)-dimensional manifold in the \(d\)-dimensional ambient space. Formally, a \(k\)-dimensional differential manifold \(^{d}\) is locally Euclidean in \(^{k}\). At every point \(\), we can define a projection matrix \(_{x}^{d d}\) that projects points onto the \(k\)-dimensional tangent space at \(\). We denote \(_{x}^{}=-_{x}\) as the projection matrix to the subspace orthogonal to the tangent space.

### Off-Manifold Robustness \(\) On-Manifold Gradient Alignment

We now show via geometric arguments that off-manifold robustness of models and on-manifold alignment of gradients are identical. We begin by discussing definitions of on- and off-manifold noise. Consider a point \(\) on manifold \(\), and a noise vector \(\). Then, \(\) is **off-manifold** noise, if \(_{x}(+)=\), which we denote \(:=_{}\), and \(\) is **on-manifold** noise, if \(_{x}(+)=+\), which we denote \(:=_{}\). In other words, if the noise vector lies on the tangent space then it is on-manifold, otherwise it is off-manifold. Given this definition, we can define relative off-manifold robustness. For simplicity, we consider a scalar valued function, which can correspond to one of the \(C\) output classes of the model.

[MISSING_PAGE_EMPTY:4]

Distinguishing Between the Data and the Signal Manifold.Given a classification problem, one can often decompose the inputs into a _signal_ component and a _distractor_ component. For intuition, consider the binary task of classifying cats and dogs. Given an oracle data generating distribution of cats and dogs in diverse backgrounds, the label must be statistically independent of the background and depend purely on the object (cat or dog) in the image. In other words, there exist no spurious correlations between the background and the output label. In such a case, we can call the object the signal and the background the distractor. Formally, for every input \(\) there exists a binary mask \(()\{0,1\}^{d}\) such that the signal is given by \(s()=()\) and the distractor is given by \(d()=(1-())\). The signal and distractor components are orthogonal to one another (\(s()^{}d()=0\)), and we can decompose any input in this manner (\(=s()+d()\)). We provide an illustrative figure in Figure 2. Using this, we can now define the signal-distractor distributions.

**Definition 3**.: _Given a data distribution \(p( y)\) for \(y[1,C]\), we have masking functions \(()\) such that the resulting distribution \(p((1-) y)=p((1-))\) is statistically independent of \(y\). The sparsest such masking function (such that \(^{*}=*{arg\,min}_{}_{  p(|y)}\|()\|_{0}\)), yields a corresponding distribution \(p((1-^{*}()))\), which is the distractor distribution, and its counterpart \(p(^{*}() y)\) the signal distribution._

While it is possible to define signals and distractors more generally as any linear projections of the data, in this work, we restrict ourselves to masking to relate to feature attributions . While the subject of finding such optimal masks is the topic of feature attribution , in this discussion, we shall assume that the optimal masks \(^{*}\) and the corresponding signal and distractor distributions are known. Similar to decomposing any point on the manifold, we can also decompose any vector on the tangent space on the data manifold into signal and distractor components using the optimal mask \(^{*}\). In other words, we can write

\[_{}p( y)=}p(  y)^{*}()}_{)$ has information about $y$}}+}p( y)(1- ^{*}())}_{)$ (independent of $y$)}}\]

Finally, we note that this signal-distractor decomposition does not meaningfully exist for all classification problems in that it can be trivial with the entire data distribution being equal to the signal, with zero distractors. Two examples of such cases are: (1) ordinary MNIST classification has no distractors due to the simplicity of the task, as the entire digit is predictive of the true label and the background is zero. (2) multi-class classification with a large set of classes with diverse images also have no distractors due to the complexity of the task, as the background can already be correlated with class information. For example, if the dataset mixes natural images with deep space images, there is no single distractor distribution one can find via masking that is independent of the class label. This disadvantage arises partly because we aim to identify important pixels, whereas a more general definition based on identifying subspaces can avoid these issues. We leave the exploration of this more general class of methods to future work.

Given this definition of the signal and distractor distributions, we are ready to make the following theorem: the input-gradients of a Bayes optimal classifier lie on the signal manifold, as opposed to the general data manifold.

Figure 2: An illustration of a signal-distractor decomposition for a bird classification task. The signal represents the discriminative parts of the input, while the distractor represents the non-discriminative parts.

**Theorem 2**.: _The input-gradients of Bayes optimal classifiers lie on the signal manifold._

Proof Idea.: We first show that the gradients of the Bayes optimal predictor lie in the linear span of the gradients of the gradients of the data distribution, thus lying on the tangent space of the data distribution. Upon making a signal-distractor decomposition of the gradients of the Bayes optimal classifier, we find that the distractor term is always zero, indicating that the gradients of the Bayes optimal classifier always lie on the signal manifold. The proof is given in the supplementary material. 

The full proof is given in the Supplementary material. This theorem can be intuitively thought of as follows: the optimal classifier only needs to look at discriminative regions of the input in order to classify optimally. In other words, changing the input values at discriminative signal regions is likely to have a larger effect on model output than changing the inputs slightly at unimportant distractor regions, indicating that the gradients of the Bayes optimal classifier highlight the signal. Thus, the Bayes optimal classifier does not need to model the distractor; only the signal is sufficient. This fact inspires us to make the following hypothesis to help us ground the discussion on the perceptual alignment of gradients:

**Hypothesis 1**.: _The input gradients of classifiers are perceptually aligned if and only if they lie on the signal manifold, thus highlighting only discriminative parts of the input._

This indicates that Bayes optimal model's gradients are perceptually aligned. Recall that the signal component only provides information about discriminative components of the input, and thus, we expect this to connect to perceptual alignment from Phenomenon 1. Next, we ask whether the gradients of practical models, particularly robust models, are also off-manifold robust.

### Connecting Robust Models and Off-Manifold Robustness

Previously, we saw that Bayes optimal classifiers have the property of relative off-manifold robustness. Here, we argue that off-manifold robustness also holds for robust models in practice. This is a non-trivial and a perhaps surprising claim: common robustness objectives such as adversarial training, gradient-norm regularization, etc are isotropic, meaning that they do not distinguish between on- and off-manifold directions.

**Hypothesis 2**.: _Robust models are off-manifold robust w.r.t. the signal manifold._

We provide two lines of argument in support for this hypothesis. Ultimately, however, our evidence is empirical and presented in the next section.

Argument 1: Combined robust objectives are non-isotropic.While robustness penalties itself are isotropic and do not prefer robustness in any direction, they are combined with the cross-entropy loss on data samples, which lie on the data manifold. Let us consider the example of gradient norm regularization. The objective is given by:

\[*{}_{}(f(),y())+\|_{}f()\|^{2}=*{ }_{}(),y())+ \|^{}_{}f()\|^{2}}_{}+}_{}f()\|^{2}}_{})\]

Here we have used, \(\|_{}f()\|^{2}=\|^{}_{}f( )+^{}_{}f()\|^{2}=\|^{ }_{}f()\|^{2}+\|^{}_{}f ()\|^{2}\) due to \(^{}_{}f()^{}^{}_{ }f()=0\), which is possible because the on-manifold and off-manifold parts of the gradient are orthogonal to each other. Assuming that we are able to decompose models into on-manifold and off-manifold parts, these two objectives apply to these decompositions independently. This argument states that in this robust training objective, there exists a trade-off between the cross-entropy loss and the on-manifold robustness term, whereas there is no such trade-off for the off-manifold term, indicating that it is much easier to minimize off-manifold robustness than on-manifold. This argument also makes the prediction that increased on-manifold robustness must be accompanied by higher train loss and decreased out-of-sample performance, and we will test this in the experiments section.

However, there are nuances to be observed here: while the data lies on the data manifold, the gradients of the optimal model lie on the signal manifold so this argument may not be exact. Nonetheless, for cases where the signal manifold and data manifold are identical, this argument holds and can explain a preference for off-manifold robustness over on-manifold robustness.

Argument 2: Robust linear models are off-manifold robust.It is a well-known result in machine learning (from, for example the representer theorem) that the linear analogue of gradient norm regularization, i.e., weight decay causes model weights to lie in the linear span of the data. In other words, given a linear model \(f()=^{}\), its input-gradient are the weights \(_{}f()=\), and when trained with the objective \(=_{}(f()-y())^{2}+ \|\|^{2}\), it follows that the weights have the following property: \(=_{i=1}^{N}_{i}_{i}\), i.e., the weights lie in the span of the data. In particular, if the data lies on a _linear subspace_, then so do the weights. Robust linear models are also infinitely off-manifold robust: for any perturbation \(_{}\) orthogonal to the data subspace, \(^{}(+_{})=^{} \), thus they are completely robust to off-manifold perturbations.

In addition, if we assume that there are input co-ordinates \(_{i}\) that are uncorrelated with the output label, then \(_{i}_{i}\) is also uncorrelated with the label. Thus the only way to minimize the mean-squared error is to set \(_{i}=0\) (i.e., a solution which sets \(_{i}=0\) has strictly better mean-squared error than one that doesn't), in which case the weights lie in the signal subspace, which consists of the subspace of all features correlated with the label. This shows that even notions of signal-distractor decomposition transfer to the case of linear models.

## 4 Experimental Evaluation

In this section, we conduct extensive empirical analysis to confirm our theoretical analyses and additional hypotheses. We first demonstrate that robust models exhibit relative off-manifold robustness (Section 4.1). We then show that off-manifold robustness correlates with the perceptual alignment of gradients (Section 4.2). Finally, we show that robust models exhibit a signal-distractor decomposition, that is they are relatively robust to noise on a distractor rather than the signal (Section 4.3). Below we detail our experimental setup. Any additional details can be found in the Supplementary material.

Figure 3: **Top Row: Robust models are off-manifold robust. The figure depicts the inverse on- and off-manifold robustness of Resnet18 models trained with different objectives on CIFAR-10 (larger values correspond to less robustness). As we increase the importance of the robustness term in the training objective, the models become increasingly robust to off-manifold perturbations. At the same time, their robustness to on-manifold perturbations stays approximately constant. This means that the models become off-manifold robust. As we further increase the degree of robustness, both on- and off-manifold robustness increase. Bottom Row: The input gradients of robust models are perceptually similar to the score of the probability distribution, as measured by the LPIPS metric. We can also identify the models that have the most perceptually-aligned gradients (the global maxima of the yellow curves). Figures depict mean and minimum/maximum across 10 different random seeds. Additional results including the smoothness penalty can be found in Supplement Figure 9.**

**Data sets and Robust Models.** We use CIFAR-10 , ImageNet and ImageNet-64 , and an MNIST dataset  with a distractor, inspired by . We train robust Resnet18 models  with (i) gradient norm regularization, (ii) randomized smoothing, (iii) a smoothness penalty, and (iv) \(l_{2}\)-adversarial robust training with projected gradient descent. The respective loss functions are given in the Supplementary material. On ImageNet, we use pre-trained robust models from .

**Measuring On- and Off-Manifold Robustness.** We measure on- and off-manifold robustness by perturbing data points with on- and off-manifold noise (Section 3.1). For this, we estimate the tangent space of the data manifold with an auto-encoder, similar to . We then draw a random noise vector and project it onto the tangent space. Perturbation of the input in the tangent direction is used to measure on-manifold robustness. Perturbation of the input in the orthogonal direction is used to measure off-manifold robustness. To measure the change in the output of a classifier with \(C\) classes, we compute the \(L_{2}\)-norm \(||f(x)-f(x+u)||_{2}\).

**Measuring Perceptual Alignment.** To estimate the perceptual alignment of model gradients, we would ideally compare them with the gradients of the Bayes optimal classifier. Since we do not have access to the Bayes optimal model's gradients \(_{}p(y)\), we use the score \(_{} p( y)\) as a proxy, as both lie on the same data manifold. Given the gradient of the robust model and the score, we use the Learned Perceptual Image Patch Similarity (LPIPS) metric  to measure the perceptual similarity between the two. The LPIPS metric computes the similarity between the activations of an AlexNet  and has been shown to match human perception well . In order to estimate the score \(_{} p( y)\), we make use of the diffusion-based generative models from Karras et al. . Concretely, if \(D(,)\) is a denoiser function for the noisy probability distribution \(p(,)\) (compare Section 3.2), then the score is given by \(_{} p(,)=(D(,)-)/^{2}\)[10, Equation 3]. We use noise levels \(=0.5\) and \(=1.2\) on CIFAR-10 and ImageNet-64, respectively.

### Evaluating On- _vs._ Off-manifold Robustness of Models

We measure the on- and off-manifold robustness of different Resnet18 models on CIFAR-10, using the procedure described above. We measure how much the model output changes in response to an input perturbation of a fixed size (results for different perturbation sizes can be found in Supplement Figure 9). The models were trained with three different robustness objectives and to various levels of robustness. The results are depicted in the top row of Figure 3. Larger values in the plots correspond to a larger perturbation in the output, that is less robustness. For little to no regularization (the left end of the plots), the models are less robust to random changes off- than to random changes on- the data manifold (the red curves lie above the green curves). Increasing the amount of robustness makes the models increasingly off-manifold robust (the red curves decrease monotonically). At the same time, the robustness objectives do not affect the on-manifold robustness of the model (the green curves stay roughly constant). This means that the robust models become relatively off-manifold robust (Definition 1). At some point, the robustness objectives also start to affect the on-manifold behavior of the model, so that the models become increasingly on-manifold robust (the green curves start to fall). As can be seen by a comparison with the accuracy curves in the bottom row of Figure 3, increasing on-manifold robustness mirrors a steep fall in the accuracy of the trained models (the

Figure 4: The input gradients of robust models trained with projected gradient descent on ImageNet and Imagenet-64x64 are perceptually similar to the score of the probability distribution, as measured by the LPIPS metric. On Imagenet-64x64, we also trained excessively robust models.

green and blue curves fall in tandem). Remarkably, these results are consistent across the different types of regularization.

### Evaluating the Perceptual Alignment of Robust Model Gradients

We now show that the input gradients of an intermediate regime of accurate and relatively off-manifold robust models (_"Bayes-aligned_ robust models") are perceptually aligned, whereas the input gradients of _weakly_ robust and _excessively_ robust models are not. As discussed above, we measure the perceptual similarity of input gradients with the score of the probability distribution. The bottom row of Figure 3 depicts our results on CIFAR-10. For all three robustness objectives, the perceptual similarity of input gradients with the score, as measured by the LPIPS metric, gradually increases with robustness (the orange curves gradually increase). The perceptual similarity then peaks for an intermediate amount of robustness, after which it begins to decrease. Figure 4 depicts our results on ImageNet and ImageNet-64x64. Again, the perceptual similarity of input gradients with the score gradually increases with robustness. On ImageNet-64x64, we also trained excessively robust models that exhibit a decline both in accuracy and perceptual alignment.

To gain intuition for these results, Figure 6, as well as Supplementary Figures 10-15, provide a visualization of model gradients. In particular, Figure 6 confirms that the model gradients belonging to the left and right ends of the curves in Figure 3 are indeed not perceptually aligned, whereas the model gradients around the peak (depicted in the middle columns of Figure 6) are indeed perceptually similar to the score.

While we use the perceptual similarity of input gradients with the score as a useful proxy for the perceptual alignment of input gradients, we note that this approach has a theoretical foundation in the energy-based perspective on discriminative classifiers . In particular, Srinivas and Fleuret , Zhu et al.  have suggested that the input gradients of softmax-based discriminative classifiers could be related to the score of the probability distribution. To the best of our knowledge, our work is the first to quantitatively compare the input gradients of robust models with independent estimates of the score.

### Evaluating Signal _vs._ Distractor Robustness for Robust Models

We now show that robust models are relatively robust to noise on a distractor. Since a distractor is by definition not part of the signal manifold (Section 3.2), this serves as evidence that the input

Figure 5: Robust Models are relatively robust to noise on a distractor.

Figure 6: The figure depicts the input gradients of models belonging to different regimes of robustness. The numbers above the images indicate the perceptual similarity with the score, as measured by the LPIPS metric. Top row: ImageNet-64x64. Bottom row: CIFAR-10. **Weakly robust** models are accurate but not off-manifold robust. **Bayes-aligned** robust models are accurate and off-manifold robust. These are exactly the models that have perceptually aligned gradients. **Excessively robust** models are excessively on-manifold robust which makes them inaccurate. Best viewed in digital format.

gradients of robust models are aligned with the signal manifold. In order to control the signal and the distractor, we create a variant of the MNIST dataset where we artificially add the letter "A" as a distractor. This construction is inspired by Shah et al. , for details see Supplement Figure 16. Because we know the signal and the distractor by design, we can add noise to only the signal or only the distractor and then measure the robustness of different models towards either type of noise. We call the ratio between these two robustness values the relative noise robustness of the model. Figure 5 depicts the relative noise robustness both for a standard- and an adversarially robust Resnet18. From Figure 5, we see that the standard model is already more robust to noise on the distractor. The robust model, however, is relatively much more robust to noise on the distractor. Since distractor noise is by definition off-manifold robust w.r.t. the signal manifold, this result serves as evidence for Hypothesis 2. Moreover, the input gradients of the robust model (depicted in the supplement) are perceptually aligned. Hence, this experiment also serves as evidence for Hypothesis 1.

## 5 Discussion

**Three Regimes of Robustness.** Our experimental results show that different robust training methods show similar trends in terms of how they achieve robustness. For small levels of robustness regularization, we observe that the classifiers sensitivity to off-manifold perturbations slowly decreases (_weak_ robustness), eventually falling below the on-manifold sensitivity, satisfying our key property of _(relative) off-manifold robustness_, as well as alignment with the Bayes classifier (_Bayes-aligned_ robustness). Excessive regularization causes models to become insensitive to on-manifold perturbations, which often corresponds to a sharp drop in accuracy (_excessive_ robustness).

The observation that robust training consists of different regimes (weak-, Bayes-aligned-, and excessive robustness) calls us to _rethink standard robustness objectives and benchmarks_, which do not distinguish on- and off-manifold robustness. An important guiding principle here can be not to exceed the robustness of the Bayes optimal classifier.

**The Limits of Gradient-based Model Explanations.** While Shah et al.  find experimentally that gradients of robust models highlight discriminative input regions, we ground this discussion in the signal-distractor decomposition. Indeed, as the gradients of the Bayes optimal classifier are meaningful in that they lie tangent to the signal manifold, we can also expect gradients of robust models to mimic this property. Here the zero gradient values indicates the distractor distribution. However, if a meaningful signal-distractor decomposition does not exist for a dataset, i.e., the data and the signal distribution are identical, then we cannot expect gradients even the gradients of the Bayes optimal classifier to be "interpretable" in the sense that they highlight all the input features as the entire data is the signal.

**Limitations.** First, while our experimental evidence strongly indicates support for the hypothesis that off-manifold robustness emerges from robust training, we do not provide a rigorous theoretical explanation for this. We suspect that it may be possible to make progress on formalizing **Argument 1** in Section 3.3 upon making suitable simplifying assumptions about the model class. Second, we only approximately capture the alignment to the Bayes optimal classifier using the score-gradients from a SOTA diffusion model as a proxy, as we do not have access to a better proxy for a Bayes optimal classifier.