# Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with _noisy_ and _incomplete_ count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the _time-varying_ transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by the specifically-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior simulation. Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.

## 1 Introduction

In recent years, there has been an increasing interest in modeling count time series. For instance, some previous works  are concerned with how to learn the evolving topics behind text corpus (frequencies of words) over time. Some works  try to predict global immigrant trends underlying international population movements. Count time series are often _overdispersed_, _sparse_, _high-dimensional_, and thus can not be well modeled by widely used dynamic models such as linear dynamical systems . Recently, many works  prefer to choose distributions of the gamma-Poisson family to build their hierarchical Bayesian models. In particular, these models enjoy strong explainability and can estimate uncertainty especially when the observations are _noisy_ and _incomplete_. Among these works, Poisson-Gamma Dynamical Systems (PGDSs)  received a lot of attention because PGDS can learn how the latent dimensions excite each other to capture complicated dynamics in observed count series. For instance, a very inspiring research paper may motivate other researchers to publish papers on related topics . The outbreak of COVID-19 in one state, may lead to the rapid rising of COVID-19 cases in the nearby states and vice versa . In particular, PGDS can be efficiently learned with a tractable Gibbs sampling scheme via Poisson-Logarithmic data augmentation and marginalization technique . Due to its strong flexibility, PGDS achieves better performance in predicting missing entities and future observations, compared with related models .

Despite these advantages, PGDS still can not capture the time-varying transition dynamics underlying observed count sequences, which are commonly observed in real-world scenarios . For instance,during the initial stage of the COVID-19 pandemic, the worldwide counts of infectious patients were significantly affected by various local policies, government interventions, and emergent events [20; 21; 22]. The cross transition dynamics among the different monitoring areas were also evolving as the corresponding policies and interventions changed over time. Hence, PGDS unavoidably makes a certain amount of approximation error in capturing the aforementioned non-stationary count time series, using a _time-invariant_ transition kernel.

To mitigate this limitation, Non-Stationary Poisson-Gamma Dynamical Systems (NS-PGDSs), a novel kind of Poisson-gamma dynamical systems with non-stationary transition dynamics are developed. More specifically, NS-PGDS captures the evolving transition dynamics by the specifically-designed Dirichlet Markov chains. Via the Dirichlet-Multinomial-Beta data augmentation strategy, the Non-Stationary Poisson-Gamma Dynamical Systems can be inferred with a conjugate-yet-efficient Gibbs sampler. Our contributions are summarized as follows:

* We propose a Non-Stationary Poisson-Gamma Dynamical System (NS-PGDS), a novel Poisson-gamma dynamical system with time-evolving transition matrices that can well capture non-stationary transition dynamics underlying observed count series.
* Three Dirichlet Markov chains are dedicated to improving the flexibility and expressiveness of NS-PGDSs, for capturing the complex transition dynamics behind sequential count data.
* Fully-conjugate-yet-efficient Gibbs samplers are developed via Dirichlet-Multinomial-Beta augmentation techniques to perform posterior simulation for the proposed Dirichlet Markov chains.
* Extensive experiments are conducted on four real-world datasets, to evaluate the performance of the proposed NS-PGDS in predicting missing and future unseen observations. We also provide exploratory analysis to demonstrate the explainable latent structure inferred by the proposed NS-PGDS.

## 2 Preliminaries

Let \(^{(t)}=[y_{1}^{(t)},,y_{V}^{(t)}]^{} ^{V}\) be a vector of nonnegative count valued observations at time \(t\). To capture the latent dynamics underlying count sequences, some previous works [23; 24] model the observations as

\[^{(t)}=p(^{(t)}),\ \ ^{(t)}=f^{-1}(^{(t) }),\]

where \(p()\) is the observation likelihood function, and \(f()\) is an invertible link function that maps the parameters of observation component to continuous-valued latent variables \(^{(t)}^{K}\). The latent factor \(^{(t)}\) evolves over time according to a linear dynamical system (LDS) given by \(^{(t)}(^{(t-1)},^{-1})\), where \(\) is the state transition matrix of size \(K K\), and \(=(_{1},,_{K})\) is the inverse covariance matrix with \(_{k}^{-1}\) determining the variance of \(k\)-th latent dimension. Han et al.  adopted the Extended Rank likelihood function to model count observations using LDS with time complexity \(((K+V)^{3})\), which prevents it from practical applications for analyzing large-scale count data.

Recently, Acharya et al.  and Schein et al. [13; 16] developed Poisson-gamma family models for sequential count observations. Gamma Process Dynamic Poisson Factor Analysis (GP-DPFA)  models count data as \(y_{v}^{(t)}(_{k=1}^{K}_{k}_{vk}_{k}^{(t)})\), where \(_{k}^{(t)}\) represents the strength of \(k\)-th latent factor at time \(t\), and \(_{vk}\) captures the involvement degree of \(k\)-th factor to \(v\)-th observed dimension. To ensure the model identifiability, we can impose a restriction as \(_{v}_{vk}=1\), and thus place a Dirichlet prior over \(}=[_{1k},,_{Vk}]^{T}\) as \(}(_{0},,_{0})\).

To capture the underlying dynamics, the latent factor \(_{k}^{(t)}\) evolves over time according to a gamma Markov chain as \(_{k}^{(t)}(_{k}^{(t-1)},c_{t})\), where \(c_{t}\) is the rate parameter of the gamma distribution to control the variance of the gamma Markov chains. Although GP-DPFA can well fit one-dimensional count sequences, it fails to learn how the latent dimensions interact with each other.

To address this concern, Schein et al.  developed Poisson-gamma dynamical systems to capture the underlying transition dynamics. In particular, \(_{k}^{(t)}\) evolves over time as \((_{0}_{k_{2}=1}^{K}_{kk_{2}}_{k_{2}}^{(t-1)},_{0})\), where \(_{kk_{2}}\) represents how \(k_{2}\)-th latent factor excites the \(k\)-th latent factor at next time step, and \(_{k=1}^{K}_{kk_{2}}=1\).

## 3 Non-Stationary Poisson-Gamma Dynamical Systems

Real-world count time sequences are often _non-stationary_ because the external interventional environments are always changing over time. The stationary PGDS with a time-invariant transition kernel fails to capture such time-varying transition dynamics. For instance, the transition dynamics behind COVID-19 infectious processes are time-varying, and highly affected by various interventional policies. Hence, to mitigate this limitation, we model the count sequences as

\[y_{v}^{(t)}(^{(t)}_{k=1}^{K}_{vk}_{k }^{(t)}),\]

in which, the latent factors are specified by

\[_{k}^{(t)}(_{0}_{k_{2}=1}^{K}_{kk_{2}}^{ (t-1)}_{k_{2}}^{(t-1)},_{0}), \]

where the multiplicative term \(^{(t)}(_{0},_{0})\) and the transition matrices are time-varying as \(^{(t)}[_{kk_{2}}^{(t)}]_{k,k_{2}=1}^{K}\). As shown in Figure 2, to model the time-varying transition dynamics, we assume the whole time interval can be divided into \(I\) equally-spaced sub-intervals. The transition kernel behind complicated dynamic counts is assumed to be _static_ within each sub-interval, while evolving over sub-intervals, to capture non-stationary behaviours. In another word, the proposed model allows the latent factors to evolve over time steps while the transition matrices change over sub-intervals but assumed to be stationary within each sub-interval, as shown in Figure 1. In particular, we let each sub-interval contains \(M\) time steps, and the \(i\)-th interval contains time steps \(\{t t=(i-1)M+1,,iM\}\). We define \(i(t)\) as the function that maps time step \(t\) to its corresponding sub-interval.

**Dirichlet-Dirichlet Markov processes.** To capture how the underlying transition kernel smoothly evolves over sub-intervals, we first propose the Dirichlet-Dirichlet (Dir-Dir) Markov chain as

\[_{k}^{(i)}_{k}^{(i-1)}( K _{1k}^{(i-1)},, K_{Kk}^{(i-1)}), \]

where \(_{k}^{(i)}\) represents the \(k\)-th column of \(^{(i)}\), and the prior of the scaling parameter \(\) is given by \((_{0},f_{0})\).

Figure 1: The graphical representation of the NS-PGDS. The time interval is divided into equally-spaced sub-intervals. Each sub-interval contains \(M\) time steps. The transition dynamics is stationary within a sub-interval. In particular, the transition matrices evolve over sub-intervals via Dirichlet Markov processes while latent factors evolve over time steps via Eq.(1).

Figure 2: An example illustrates the Poisson-gamma dynamical systems with non-stationary transition kernels. The three gamma dynamic processes independently evolve over time during the (\(i-1\))-th interval. During \(i\)-th interval, \(_{1}^{(t)}\) and \(_{2}^{(t)}\) gradually starts to interact with each other while \(_{3}^{(t)}\) remains independent to the other two dimensions. During (\(i+1\))-th interval all the three latent components start to interact with each other.

The initial states are defined as \(_{k}^{(1)}(_{0}_{k},_{0})\). The prior for the transition kernel of the first sub-interval is given by \(_{k}^{(1)}(_{1}_{k},,_{k}, ,_{K}_{k})\), where \(_{k}(},)\) and \(,(_{0},_{0})\). Note that the expectation and variance of the transition kernel at \(i\)-th sub-interval can be calculated as

\[[_{k}^{(i)}_{k}^{(i-1)}]=_{k}^ {(i-1)},[_{k_{1}k}^{(i)}_{k}^{(i-1)} ]=k}^{(i-1)}(1-_{k_{1}k}^{(i-1)})}{ K+1},\]

respectively. The transition dynamics of \(i\)-th sub-interval inherits the information of the previous sub-interval, and also adapts to the data observed in the current sub-interval. The scaling parameter \(\) controls the variance of the transition matrices.

The prior specification defined in Eq.(2) by rescaling the transition matrix at the previous sub-interval allows the transition dynamics to change smoothly, and thus might be insufficient to capture the rapid changes observed in complicated dynamics. To further improve the flexibility of the transition structure, two modified Dirichlet Markov chains are studied to capture the correlation structure between the dimensions of the transition matrices over time. **Dirichlet-Gamma-Dirichlet Markov processes.** We first introduce the Dirichlet-Gamma-Dirichlet (Dir-Gam-Dir) Markov chain to model the evolving transition matrices as

\[_{k}^{(i)} (_{1k}^{(i)},,_{Kk}^ {(i)}),\] \[_{k_{1}k}^{(i)} (_{k}^{(i-1)}_{k_{2}=1}^{K} _{kk_{1}k_{2}}^{(i-1)}_{k_{2}k}^{(i-1)},c_{k}^{(i)}), \]

where we use \(_{kk_{1}k_{2}}^{(i-1)}\) to capture the mutation between two consecutive sub-intervals, and its prior is given by

\[(_{k1k_{2}}^{(i-1)},,_{kkK_{2}}^{(i-1)}) (_{0},,_{0}),\]

and \(_{k}^{(i)},c_{k}^{(i)}(_{0}, _{0})\). Compared with the construction defined by Eq.(2), the expectation of Dirichlet-Gamma-Dirichlet Markov chain is

\[[_{k}^{(i)}_{k}^{(i-1)}]=_{ k}^{(i-1)}_{k}^{(i-1)}.\]

This construction takes interactions among components of columns into account. Hence it will dramatically improve the flexibility of our model and thus better fit more complicated dynamics, compared with Dir-Dir Markov chains that only yield smoothing transition dynamics.

**Poisson-randomized-gamma-Dirichlet Markov processes.** By leveraging the Poisson-randomized gamma distribution , we introduce another type of time-varying transition kernels, which also model the interactions among components like Dir-Gam-Dir construction but may induce different properties such as sparsity. The Poisson-randomized-gamma-Dirichlet (PR-Gam-Dir) Markov chain can be formulated as

\[_{k}^{(i)}(_{1k}^{(i)},, _{Kk}^{(i)}),\;_{k_{1}k}^{(i)}( ^{},_{k}^{(i-1)}_{k=2}^{K}_{kk_{1}k_{2}}^{(i-1)}_{k_{2} k}^{(i-1)},c_{k}^{(i)}), \]

where \(()\) denotes the randomized gamma distribution of the first type. Similarly, for \(_{kk_{1}k_{2}}^{(i-1)}\), \(_{k}^{(i)}\), and \(c_{k}^{(i)}\), the priors are given by

\[(_{k1k_{2}}^{(i-1)},,_{kkK_{2}}^{(i-1)}) (_{0},,_{0}),\;_{k}^ {(i)},c_{k}^{(i)}(_{0},_{0}), .\]

The diagrams of three Dirichlet Markov constructions are shown in Figure 3.

## 4 Markov Chain Monte Carlo Inference

In this section, we present the Gibbs sampler for the proposed NS-PGDS. We only illustrate the key points of the derivation and the details can be found in the appendix.

Figure 3: Diagrams of the proposed Dirichlet Markov constructions. (a) is the Dir-Dir construction. (b) is the Dir-Gam-Dir construction which takes mutation into account. (c) illustrates the PR-Gam-Dir construction which adopts Poisson randomized gamma distribution and can be equivalently represented as Eq.(5).

**Lemma 1**: _If \(y(a,g())\) and \(l(y,a)\), where \(()\) refers to negative-binomial distribution, \(()\) represents Chinese restaurant table distribution , and \(g(z)=1-(-z)\). Then the joint distribution of \(y\) and \(l\) can be equivalently distributed as \(y(l,g())\) and \(l(a)\), i.e._

\[(y;a,g())(l;y,a)= (y;l,g())(l;a ),\]

_where \((l,g())=_{i=1}^{l}x_{i}\) and \(x_{i}(g())\) are independently and identically logarithmic distributed random variables ._

**Lemma 2**: _Suppose \(=(n_{1},,n_{K})\) and_

\[ n(n,r_{1},,r_{K}),\]

_where \(()\) refers to Dirichlet-multinomial distribution. We sample the augmented variable \(q n(n,r)\), where \(r.=_{k=1}^{K}r_{k}\). According to , conditioning on \(q\), we have \(n_{k}(r_{k},q)\)._

**Sampling \(y_{vk}^{(t)}\) :** Use the relationship between Poisson and multinomial distributions, we sample

\[((y_{vk}^{(t)})_{k=1}^{K}-)(y_{v}^{(t)},(_{k}^{(t )}}{_{k=1}^{K}_{vk}_{k}^{(t)}})_{k=1}^{K }).\]

**Sampling \(}\) :** Via Dirichlet-multinomial conjugacy, the posterior of \(}\) is

\[(}-)(_{0}+ _{t=1}^{T}y_{1k}^{(t)},,_{0}+_{t=1}^{T}y_{Vk }^{(t)}).\]

**Sampling \(_{k}^{(t)}\) :** To sample from the posterior of \(_{k}^{(t)}\), we first sample the auxiliary variables. Setting \(l_{ k}^{(T+1)}=0\) and \(^{(T+1)}=0\), we sample the augmented variables backwards from \(t=T,,2\),

\[(l_{k}^{(t)}-)(y_{ k} ^{(t)}+l_{ k}^{(t+1)},_{0}_{k_{2}=1}^{K} _{kk_{2}}^{i(t-1)}_{k_{2}}^{(t-1)}),\]

\[(l_{k1}^{(t)},,l_{kK}^{(t)}-) (l_{k}^{(t)},(^{i(t -1)}_{1}^{(t-1)}}{_{k_{2}=1}^{K}_{kk_{2}}^{i (t-1)}_{k_{2}}^{(t-1)}},,^{i (t-1)}_{K}^{(t-1)}}{_{k_{2}=1}^{K}_{kk_{2}} ^{i(t-1)}_{k_{2}}^{(t-1)}})).\]

Let us define \(l_{ k}^{(t)}=_{k_{1}=1}^{K}l_{k_{1}k}^{(t)}\) and \(^{(t)}=(1+}{_{0}}+^{ (t+1)})\). After sampling the auxiliary variables, then for \(t=1,,T\), by Poisson-gamma conjugacy, we obtain

\[(_{k}^{(t)}-)(y_{ k} ^{(t)}+l_{ k}^{(t+1)}+_{0}_{k_{2}=1}^{K} _{kk_{2}}^{i(t-1)}_{k_{2}}^{(t-1)},_{0}+ ^{(t)}+^{(t+1)}_{0}).\]

**Sampling \(^{(i)}\) :** We only illustrate Gibbs sampling algorithm for PR-Gam-Dir construction, sampling algorithms for other constructions can be found in the appendix. We define \(M\) as the length of each sub-interval, and \(I\) as the number of intervals. For \(i=I,,2\), because \((l_{1k}^{(i)},,l_{Kk}^{(i)})\) and \((g_{ 1k}^{(i+1)},,g_{ Kk}^{(i+1)})\) are multinomially distributed, where \(l_{k_{1}k}^{(i)}=_{(i-1)M+1}^{iM}l_{k_{1}k}^{(t )}\) refers to the summation of \(l_{k_{1}k}^{(t)}\) over \(i\)-th sub-interval and same notation for other variables. By the definition of Dirichlet-multinomial distribution and Lemma 2, defining \(g_{k_{1}k}^{(1+1)}=0\), we sample the auxiliary variables as \((q_{k}^{(i)}-)(l_{ k}^{(i)}+g_{  k}^{(i+1)},_{k}^{(i)})\), then we have \((l_{k_{1}k}^{(i)}+g_{ k_{1}k}^{(i+1)})(_{k_{1}k}^{(i)},q_{k}^{(i)})\). Then we further sample \((h_{k_{1}k}^{(i)}-)(l_{k_{1}k}^{(i )}+g_{ k_{1}k}^{(i+1)},_{k_{1}k}^{(i)})\). Via Lemma 1, we obtain \(h_{k_{1}k}^{(i)}(-_{k_{1}k}^{(i)} (1-q_{k}^{(i)}))\). For Dirichlet-Randomized-Gamma-Dirichlet Markov construction defined by Eq.(4), we can equivalently represent it as

\[_{k_{1}k}^{(i)}(g_{k_{1}k}^{(i)} +^{},c_{k}^{(i)}),\ g_{k_{1}k}^{(i)}= (^{(i-1)}_{k=2}^{K}_{kk_{1}k_{2}}^{ (i-1)}_{k_{2}k}^{(i-1)}). \]

We define \(_{k_{1}k}^{(i-1)}_{k}^{(i-1)} _{k=2}^{K}_{kk_{1}k_{2}}^{(i-1)}_{k_{2}k}^{(i-1 )}\) for notation conciseness. By Poisson-gamma conjugacy, we have \((_{k_{1}k}^{(i)}-)(g_{k_{1}k}^{(i )}+^{}+h_{k_{1}k}^{(i)},c_{k}^{(i)}- (1-q_{k}^{(i)}))\). If \(^{}>0\), we cansample the posterior of \(g^{(i)}_{k_{1}k}\) via \((g^{(i)}_{k_{1}k}-)(^{}-1,2_{ k_{1}k}c^{(i)}_{k}^{(i-1)}_{k_{1}k}})\), where Bessel \(()\) denotes Bessel distribution. If \(^{}=0\), we sample \(g^{(i)}_{k_{1}k}\) via

\[(g^{(i)}_{k_{1}k}-)\{ (_{k_{1}k}^{(i-1)}_{k_{1}k}}{c^{(i)}_{k}-(-g^ {(i)}_{k_{1}k})})&h^{(i)}_{k_{1}k}=0\\ (h^{(i)}_{k_{1}k},_{k}^{(i-i)}_{k_{1}k}}{c^{ (i)}_{k}-(-g^{(i)}_{k})})&,.\]

where \(()\) denotes the shifted confluent hypergeometric distribution . Defining \(g^{(i)}_{k_{1}k}=g^{(i)}_{k_{1}k}=_{k=2}^{K}g^{(i)}_{k_{1}k_{2}k}\), we first augment

\[(g^{(i)}_{k_{1}1k},,g^{(i)}_{k_{1}Kk})(g^ {(i)}_{k_{1}k},(^{(i-1)}_{kk_{1}k_{2}}^{(i-1)}_{k_{2}k})^{K}_ {k_{2}=1}),\]

then we obtain \(g^{(i)}_{k_{1}k_{2}k}(^{(i-1)}^{(i-1)}_{kk_{1}k_{2}} ^{(i-1)}_{k_{2}k})\). By Dirichlet-multinomial conjugacy, we have

\[((^{(i-1)}_{k1k_{2}},,^{(i-1)}_{kKK_{2}} )-)(_{0}+g^{(i)}_{1k_{2}k},, _{0}+g^{(i)}_{Kk_{2}k}),\] \[(^{(i-1)}_{k}-)(^ {(i-1)}_{1k}+l^{(i-1)}_{1k}+g^{(i)}_{1,k},,^{(i-1)}_{Kk}+l^{(i-1)} _{Kk}+g^{(i)}_{ Kk}).\]

Specifically, we have \(^{(1)}_{k_{1}k}=_{k_{1}}_{k}\), if \(k_{1} k\), and \(^{(1)}_{k_{1}k}=_{k}\), if \(k_{1}=k\).

## 5 Related Work

Modeling count time sequences has been receiving increasing attentions in statistical and machine learning communities. Han et al.  adopted linear dynamical systems to capture the underlying dynamics of the data and leveraged Extended Rank likelihood function to model count observations. Some Poisson-gamma models assume that the count vector at each time step is modeled by Poisson factor analysis (PFA)  and leverage special stochastic processes to model the temporal dependencies of latent factors. For example, gamma process dynamic Poisson factor analysis (GP-DPFA)  adopts gamma Markov chains which assumes the latent factor of the next time step is drawn from a gamma distribution with the shape parameter be the latent factor of the current time step. Schein et al.  proposed Poisson-gamma dynamical systems (PGDSs), which take the interactions among latent dimensions into account and use a transition matrix to capture the interactions. Deep dynamic Poisson factor analysis (DDPFA)  adopts recurrent neural networks (RNNs) to capture the complex long-term dependencies of latent factors. Yang and Koeppl  applied Poisson-gamma count model to analyze relational data arising from longitudinal networks, which can capture the evolution of individual node-group memberships over time. Many modifications of PGDS have been proposed in recent years. Guo et al.  proposed deep Poisson-gamma dynamical systems which aim to capture the long-range temporal dependencies. Schein et al.  employed Poisson-randomized gamma distribution to build a new transition process of latent factors. Chen et al.  proposed Switching Poisson-gamma dynamical systems (SPGDS), allowing PGDS to select from several transition matrices, and thus can better adapt to nonlinear dynamics. In contrast to SPGDS, the number of transition matrices of the proposed NS-PGDS is not limited and thus can be adopted to analyze various complicated non-stationary count sequences. Filstroff et al.  extensively analyzed many gamma Markov chains for non-negative matrix factorization and introduced new gamma Markov chains with well-defined stationary distribution (BGAR).

## 6 Experiments

We conducted experiments for both predictive and exploratory analysis to demonstrate the ability of the proposed model in capturing non-stationary count time sequences. The baseline models included in the experiments are: \(1)\) Gamma process dynamic Poisson factor analysis (GP-DPFA) . GP-DPFA models the evolution of latent components as \(^{(t)}_{k}(^{(t-1)}_{k},c_{t})\), in which each component evolves independently of the other components. \(2)\) Gamma Markov chains on the rate parameter of gamma distribution (GMC-RATE) . GMC-RATE adopts gamma Markov chains defined via the rate parameter of the gamma distribution to model the evolution of as \(_{k}^{(t)}(,/_{k}^{(t-1)})\). 3) Gamma Markov chains on the rate parameter with hierarchical auxiliary variable (GMC-HIER) . GMC-HIER models the evolution of latent components with an auxiliary variables as \(z_{k}^{(t)}(_{s},_{s}_{k}^{(t-1)})\) and \(_{k}^{(t)}(a_{},_{}z_{k}^{(t)})\). 4) Autogressive beta-gamma process (BGAR) . BGAR is also a gamma Markov model. In contrast to the above models, there is a well-defined stationary distribution for BGAR. 5) Poisson-gamma dynamical system (PGDS)  takes interactions among latent dimensions into account, and models the evolution of \(_{k}^{(t)}\) as \(_{k}^{(t)}(_{0}_{k_{2}=1}^{K}_{kk_{2}} _{k_{2}}^{(t-1)},_{0})\).

The real-world datasets used in the experiments are: \(1)\)**Integrated Crisis Early Warning System (ICEWS)**: ICEWS is an international relations event dataset, comprising interaction events between countries extracted from news corpora. For ICEWS dataset, we have \(T=365\) time steps and \(V=6197\) dimensions, and we set \(M=30\). \(2)\)**NIPS**: NIPS dataset contains the papers published in the NeurIPS conference from 1987 to 2015. We have \(T=28\) time steps and \(V=2000\) dimensions for NIPS dataset and we set \(M=5\). \(3)\)**U.S. Earthquake Intensity (USEI)**: USEI contains a collection of damage and felt reports for U.S. (and a few other countries) earthquakes. We use the monthly reports from 1957-1986 and have \(T=348,V=64\) and set \(M=34\). \(4)\)**COVID-19**: This dataset contains daily death cases data for states in the United States, spanning from March 2020 to June 2020. For this dataset, we have \(V=51\) dimensions and \(T=90\) time steps and set \(M=20\).

### Predictive Analysis

To compare the predictive performance of the proposed model with the baselines, we considered two standard tasks: data smoothing and forecasting. For data smoothing task, our objective is to predict \(^{(t)}\) given the remaining data observation \(^{(t)}\). To this end, we randomly masked 10 percents of the observed data over non-adjacent time steps, and predicted the masked values. For forecasting task, we held out data of the last \(S\) time steps, and predicted \(^{(T+1)},,^{(T+S)}\) given \(^{(1)},,^{(T)}\). In this experiment we set \(S=2\). We ran the baseline models including GP-DPFA, PGLS, GMC-RATE, GMC-HIER, BGAR, using their default settings as provided in . For the NS-PGDS, we set \(K=100\) for ICEWS, \(K=10\) for other datasets, and set \(_{0}=1,_{0}=50,_{0}=0.1\). We performed 4000 Gibbs sampling iterations. In the experiments, we found that the Gibbs sampler started to converge after 1000 iterations, and thus we set the burn-in time be 2000 iterations. We retained every hundredth sample, and averaged the predictions over the samples. Mean relative error (MRE) and mean absolute error (MAE) are adopted to evaluate the model's predictive capability, which are defined as \(=_{t}_{v}^{(t)}-_{v}^{(t)}|} {1+y_{v}^{(t)}}\) and \(=_{t}_{v}|y_{v}^{(t)}-_{v}^{(t)}|\) respectively, where \(y_{v}^{(t)}\) indicates the true count and \(_{v}^{(t)}\) is the prediction.

As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks. We attribute this enhanced capability to the time-varying transition kernels, which effectively adapt to the non-stationary environment, and thus achieve improved predictive performance. For some datasets (e.g. ICEWS) and tasks, the effectiveness of the Dir-Gam-Dir and Pr-Gam-Dir constructions does not be exhibited in the numerical results. However, these two constructions indeed induce more informative patterns compared with Dir-Dir construction, as shown in the exploratory analysis.

    & & GP-DPFA & GMC-RATE & GMC-HIER & BGAR & PGDS & 
 NS-PGDS \\ (Dir-Dir) \\  & NS-PGDS & NS-PGDS \\  & & & & & & & & (Dir-Dir) & (Pr-Gam-Dir) \\  ICEWS & MAE S & 0.259 ±0.005 & 0.258 ±0.005 & 0.256 ±0.006 & 0.264 ±0.006 & 0.215 ±0.007 & 0.215 ±0.008 & **0.214** ±0.008 & 0.215 ±0.008 \\  & F & 0.176 ±0.005 & 0.187 ±0.003 & 0.185 ±0.016 & 0.222 ±0.043 & 0.185 ±0.003 & **0.167** ±0.009 & 0.169 ±0.006 & 0.169 ±0.009 \\  & MRE S & 0.125 ±0.0

### Exploratory Analysis

We used ICEWS and NIPS datasets for exploratory analysis, and chose the NS-PGDS with Dirichlet-Dirichlet Markov chains for illustration. Figure 4(a) and Figure 4(b) demonstrate the top 2 latent factors inferred by NS-PGDS from ICEWS dataset. From Figure 4(a) we can see that the main labels are "Iraq (IRQ)-United States (USA)", "Iraq (IRQ)-United Kingdom (UK)", "Russia (RUS)-United States (USA)", and so on. This latent factor probably corresponds to the topic about Iraq war. Besides, in Figure 4(a), there is a peak around March, 2003, and we know that the Iraq war broke out exactly on 20 March, 2003. In addition, the most dominant labels shown in Figure 4(b) are "Japan (JPN)-United States (USA)", "China (CHN)-United States (USA)", "North Korea (PRK)-United States (USA)", "South Korea (KOR)-United States (USA)", and so on. We can infer that this latent factor corresponds to "Six-Party Talks" and other accidents about it.

Figure 4(c) demonstrates the evolving trends of the top 5 latent factors inferred by the NS-PGDS from NIPS dataset, and the legend indicates the representative words of the corresponding latent factors. Clearly, the green and blue lines correspond to the latent factors of neural network research which started to decline from the 1990s. From the 1990s we see that the latent factors about statistical and probabilistic methods began to dominate the NeurIPS conference. In addition, the NS-PGDS also captured the revival of neural networks (blue line) from the 2010s. The above observations from the latent structure inferred by the NS-PGDS match our prior knowledge.

Next, we explored the time-varying transition matrices inferred by the NS-PGDS. We chose NIPS dataset for illustratiuon, and set \(K=10\) and the interval length \(M\) to be 5. The time-varying transition matrices are shown from Figure 5(b) to Figure 5(f). At the beginning, matrices shown in Figure 5(b) and Figure 5(c) are close to identity matrices. Then the transition matrices tend to become block diagonal matrices with 2 blocks, as shown in Figure 5(d)-5(f). The representative words for latent factors in the first block are "state-linear-classification", "network-neural-networks", "kernel-image-space", "network-neural-networks", "neural-networks-state". The representative words for latent factors in the second topics about neural networks. The second block reflects that, from the 1990s, statistical learning and Bayesian methods began to dominate, and these topics are highly correlated. Figure 5(a) illustrates the transition matrix inferred

Figure 4: The latent factors inferred by the NS-PGDS. (a) and (b) illustrate the top 2 latent factors inferred from ICEWS dataset, (a) corresponds to Iraq war and (b) corresponds to the Six-Party Talks. (c) illustrates the evolving trends of the top 5 latent factors inferred from NIPS dataset.

Figure 5: Transition matrices inferred from NIPS dataset. (a) illustrates the transition matrix inferred by the PGDS. (b)-(f) illustrate the time-varying transition matrices inferred by the NS-PGDS.

by the PGDS, which is averaged over all time steps. Compared with the NS-PGDS, the PGDS can not capture the informative time-varying transition dynamics. We also analyzed the features of the proposed Dirichlet Markov chains. The left column of Figure 6 demonstrates transition matrices of the first four sub-intervals of ICEWS dataset inferred by the NS-PGDS (Dir-Dir). Because of the Dir-Dir construction, the consecutive transition matrices smoothly change over time and thus the NS-PGDS may lack sufficient flexibility to capture rapid dynamics. The middle column of Figure 6 illustrates the transition matrices inferred by the NS-PGDS (Dir-Gam-Dir), which takes mutations among latent components into account and captured more complicated patterns. Transition matrices inferred by the PR-Gam-Dir construction are shown in the right column of Figure 6, these matrices not only exhibited sufficient flexibility but also captured sparser patterns compared with the Dir-Gam-Dir construction.

## 7 Conclusion

The Poisson-gamma dynamical systems with time-varying transition matrices, have been proposed to capture complicated dynamics observed in _non-stationary_ count sequences. In particular, Dirichlet Markov chains are constructed to allow the underlying transition matrices to evolve over time. Although the Dirichlet Markov processes lack conjugacy, we have developed tractable-but-efficient Gibbs sampling algorithms to perform posterior simulation. The experiment results demonstrate the improved performance of the proposed NS-PGDS in data smoothing and forecasting tasks, compared with the PGDS with a stationary transition kernel. Moreover, the experimental results on several real-world data sets show the explainable structures inferred by the proposed NS-PGDS. For the future work, we plan to design a method that can find the point of change and thus the length of each sub-interval can be determined automatically instead of a constant. We also consider to generalize Dirichlet belief networks by incorporating the proposed Dirichlet Markov chain constructions, which allow the hierarchical topics to mutate across layers, and thus can generate more rich text information. And we also consider to capture non-stationary interaction dynamics among individuals over online social networks in the future research.

Figure 6: From top to bottom are the first four transition matrices inferred by different Dirichlet Markov chains from ICEWS dataset. Top row: Matrices inferred by the Dir-Dir construction. Middle row: Matrices inferred by the Dir-Gam-Dir construction. Bottom row: Matrices inferred by the PR-Gam-Dir construction.