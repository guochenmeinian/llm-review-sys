# Agnostically Learning Single-Index Models using Omnipredictors

Aravind Gollakota

Aprikshit Gopalan

Apple

Adam R. Klivans

UTAustin

Konstantinos Stavropoulos

UT Austin

###### Abstract

We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary monotone and Lipschitz activations. All prior work either held only in the realizable setting or required the activation to be known. Moreover, we only require the marginal to have bounded second moments, whereas all prior work required stronger distributional assumptions (such as anticoncentration or boundedness). Our algorithm is based on recent work by Gopalan et al. (2023) on omniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and relies on the relationship between Bregman divergences (or matching losses) and \(_{p}\) distances. We also provide new guarantees for standard algorithms like GLMtron and logistic regression in the agnostic setting.

## 1 Introduction

Generalized Linear Models (GLMs) and Single-Index Models (SIMs) constitute fundamental frameworks in statistics and supervised learning McCullagh (1984); Agresti (2015), capturing and generalizing basic models such as linear and logistic regression. In the GLM framework, labeled examples \((,y)\) are assumed to satisfy \(u([y|])=\) (or \([y|]=u^{-1}()\)), where \(u\) is a known monotone function (called the link function) and \(\) is an unknown vector. Single-Index Models (SIMs) are a generalization in which the monotone link function \(u\) is also unknown.

In the realizable setting where the labels are indeed generated according to a GLM with a known Lipschitz link function, the GLMtron algorithm of Kakade et al. (2011) is a simple and efficient learning algorithm. When the ground truth is only assumed to be a SIM (i.e. the link function is unknown), it can be learned efficiently by the Isotron algorithm (Kalai and Sastry, 2009; Kakade et al., 2011).

In this work, we consider the significantly more challenging _agnostic_ setting, where the labels are arbitrary and not necessarily realizable by any SIM. Importantly, we do not fix a link function in advance; our goal is to output a predictor that has squared error comparable to that of the optimal SIM with an arbitrary monotone and Lipschitz link function. We can equivalently view this as a natural squared-error regression problem in which the final optimality guarantee must hold with respect to all SIMs with bounded weights and monotone, Lipschitz link functions.1

Formally, consider a distribution \(D\) over \(^{d}\) and denote the squared error of a function \(h:^{d}\) by \(_{2}(h)=_{(,y) D}[(y-h())^{2}]\). Let \(()\) denote optimal value of \(_{2}(h)\) over all SIMs \(h\) with bounded weights and arbitrary \(1\)-Lipschitz monotone activations (we call the inverse \(u^{-1}\) of a link function \(u\) the activation function). Given access to samples from \(D\), the goal of an agnostic learning algorithm is to compute a predictor \(p:^{d}\) with error \(_{2}(p)\) that, with high probability over the samples, is comparable to the error of the optimal SIM:

\[_{2}(p)()+.\]Our main result is the first efficient learning algorithm with a guarantee of this form.

**Theorem 1.1** (Informal, see Theorem 3.1).: _Let \(_{B}\) denote the class of SIMs of the form \( u^{-1}()\) for some \(1\)-Lipschitz function \(u^{-1}\) and \(\|\|_{2} B\). Let \(D\) be any distribution over \(^{d}\) whose marginal on \(^{d}\) has bounded second moments. There is an efficient algorithm (Algorithm 1) that agnostically learns \(_{B}\) over \(D\) up to error_

\[_{2}(p) OB(_{B})} +.\]

This result provides a guarantee comparable to that of the Isotron algorithm (Kalai and Sastry, 2009; Kakade et al., 2011) (which also tackles the SIM setting, where the link function is unknown) but for the challenging agnostic setting rather than the realizable setting (where \((_{B},D)=0\)). Moreover, Isotron's guarantees require the distribution to be supported on the unit ball, whereas we only require a mild second moment condition.

In our view, this result helps establish Algorithm 1 (and indeed any algorithm to compute omnipredictors, as we introduce and discuss shortly) as an efficient and powerful _off-the-shelf_ supervised learning algorithm, akin to random forests or gradient-boosted trees.

A natural question is whether our guarantees are near-optimal, e.g., whether we can obtain a guarantee of the form \(_{2}(p)()+\). However, there is strong evidence that such results cannot be obtained using efficient algorithms (Goel et al., 2019; Diakonikolas et al., 2020; Ouel et al., 2020; Ouel et al., 2021). We partially justify the form of our guarantee by showing in Section 5 (adapting a result due to Diakonikolas et al. (2022)) that one cannot avoid a dependence on the norm bound \(B\). Further closing the gap between upper and lower bounds is an important direction for future work.

Overview of techniques.We now describe the main ingredients and techniques that go into proving Theorem 1.1. Our starting point is the connection between GLMs and so-called matching losses (Auer et al., 1995). This connection arises from the fact that fitting a GLM with a known link function is equivalent to minimizing (over the class of all linear functions) a certain convex loss known as the matching loss corresponding to that link function (see Definition 1.8). (We refer the reader to (Gopalan et al., 2023, Sec 5.1) for a more detailed discussion.)

Importantly for us, minimizing the matching loss corresponding to a link function \(u\) yields a meaningful guarantee even in the agnostic setting, where the Bayes optimal predictor (i.e. \([y|]\)) is not necessarily a GLM at all. Specifically, it turns out to be equivalent (via Fenchel-Legendre duality) to finding the closest predictor to the Bayes optimal predictor in the metric of the _Bregman divergence_ associated with the link function \(u\) (see e.g. (Gopalan et al., 2023, Lemma 5.4)).

This is a powerful observation, but it still assumes that we have a fixed and known link function \(u\) in mind. In the agnostic setting, it is arguably unrealistic to assume that we know the best link function for the distribution at hand. Remarkably, recent work by Gopalan et al. (2022, 2023) has shown that there exist efficient learning algorithms that simultaneously minimize all matching losses corresponding to arbitrary monotone and Lipschitz link functions. Their solution concept is called an _omnipredictor_, i.e., a single predictor that is able to compete with the best-fitting classifier in a class \(\) as measured by a large class of losses (as opposed to just a single pre-specified loss, as is standard in machine learning). They obtain such predictors through calibrated multiaccuracy (Gopalan et al., 2023) or multicalibration (Gopalan et al., 2022).

From the point of view of ordinary supervised learning or regression, however, an optimality guarantee in terms of such matching losses or Bregman divergences is hard to interpret. A much more standard metric is simply that of squared error. The key final step for our results is to find a way of translating (a) optimality (ranging over all linear functions) in terms of all matching losses simultaneously into (b) optimality (ranging over all SIMs) in terms of squared error. We do so by proving simple analytic _distortion inequalities_ relating matching losses to \(_{p}\) losses, which we believe may be of independent interest.

On a technical level, to prove these distortion inequalities we first prove strong versions of such inequalities for matching losses arising from bi-Lipschitz link functions (see Lemma 2.2). We then obtain our results for general Lipschitz link functions by carefully approximating them using bi-Lipschitz link functions (see Lemma 3.3).

Further applications.As further applications of our approach, if we let \((_{u^{-1},B})\) denote the optimal value of \(_{2}(h)\) over all GLMs of the form \( u^{-1}()\), where \(\|\|_{2} B\), we obtain the following result about bi-Lipschitz link functions (including, for example, the Leaky ReLU).

**Theorem 1.2** (Informal, see Theorem 2.1).: _Let \(u:\) be a bi-Lipschitz invertible link function. Then, any predictor \(p:^{d}\) that is an \(\)-approximate minimizer of the population matching loss that corresponds to \(u\), with respect to a distribution \(D\) over \(^{d}\) satisfies_

\[_{2}(p) O(_{u^{-1},B})+ O()\]

This guarantee holds under milder distributional assumptions than are required by comparable prior work on agnostically learning GLMs or single neurons (Frei et al., 2020; Diakonikolas et al., 2022). Moreover, when we focus on distortion bounds between the logistic loss and the squared loss, we obtain a near-optimal guarantee of \(((_{u^{-1},B}))\) for logistic regression, when \(u\) is the logit link function (i.e., when \(_{u^{-1},B}\) is the class of sigmoid neurons).

**Theorem 1.3** (Informal, see Theorem 4.1).: _Let \(u(t)=()\). Then, any predictor \(p:^{d}\) that is an approximate \(\)-minimizer of the population logistic loss, with respect to a distribution \(D\) over \(^{d}\) whose marginal on \(^{d}\) has subgaussian tails in every direction satisfies_

\[_{2}(p)(_{u^{-1},B })+O()\]

While our error guarantee for this problem is weaker than that of Diakonikolas et al. (2022), we do not make the anti-concentration assumptions their results require.

### Background and Relation to Prior Work

We note that matching losses have been studied in various previous works either implicitly (Kakade et al., 2011) or explicitly (Auer et al., 1995; Diakonikolas et al., 2020, Gopalan et al., 2023) and capture various fundamental algorithms like logistic and linear regression (McCullagh, 1984; Agresti, 2015). However, to the best of our knowledge, our generic and direct approach for transforming matching loss guarantees to squared error bounds, has not been explored previously. Furthermore, our results do not depend on the specific implementation of an algorithm, but only on the matching loss bounds achieved by its output. In this sense, we provide new agnostic error guarantees for various existing algorithms of the literature. For example, our results imply new guarantees for the GLMtron algorithm of Kakade et al. (2011) in the agnostic setting, since GLMtron can be viewed as performing gradient descent (with unit step size) on the matching loss corresponding to a specified link function.

Matching losses over linear functions are also linked to the Chow parameters (O'Donnell and Servedio, 2008) through their gradient with respect to \(\), as observed by Diakonikolas et al. (2020). In fact, the norm of the matching loss gradient is also linked to multiaccuracy, a notion that originates to fairness literature (Hebert-Johnson et al., 2018; Kim et al., 2019). A stationary point \(\) of a matching loss that corresponds to a GLM with link \(u\) is associated with a multiaccurate predictor \(p()=u^{-1}()\), i.e., a predictor such that for all \(i[d]\), \([_{i}(y-p())]=0\). The work of (Gopalan et al., 2022, 2023) on omnipredictors presents a single predictor that is better than any linear model \(\) for every matching loss. The results of Gopalan et al. (2022) show that a multicalibrated predictor (with respect to the features \(_{i}\)) is an omnipredictor for all convex losses, whereas Gopalan et al. (2023) shows that the simpler condition of calibrated multicarucary suffices for matching losses that arise from GLMs. In view of the relationship between multicarucary and the gradient of the matching loss, our results show that, while multiaccuracy implies bounds on agnostically learning GLMs, the additional guarantee of calibration is sufficient for agnostically learning all SIMs.

The work of Shalev-Shwartz et al. (2011) showed strong agnostic learning guarantees in terms of the absolute error (rather than the squared error) of the form \(+\) for a range of GLMs, but their work incurs an exponential dependence on the weight norm \(B\). In comparison, for the absolute loss, we get a bound of the form \(B\,(1/)\) for logistic regression (see Theorem 4.3). In more recent years, the problem of agnostically learning GLMs has frequently also been phrased as the problem of agnostically learning single neurons (with a known activation). For the ReLU activation, work by Goel et al. (2017) showed an algorithm achieving error \(+\) in time \((d)(1/)\) over marginals on the unit sphere, and Diakonikolas et al. (2020) showed an algorithm achieving error \(O()+\) in fully polynomial time over isotropic log-concave marginals. The work of Frei et al. (2020); Diakonikolas et al. (2022) both show guarantees for learning general neurons (with known activations) using the natural approach of running SGD directly on the squared loss (or a regularized variant thereof). Frei et al. (2020) achieves error \(O()\) for any given strictly increasing activation and \(O(})\) for the ReLU activation, but they assume that the marginal distribution is bounded. In contrast, we only assume that the marginal distribution has bounded second moments and we do not consider that the activation is known. Diakonikolas et al. (2022) proved an \(O()\) guarantee for a wide range of activations (including the ReLU), in the setting where the activation is known and over a large class of structured marginals, which need, however, to satisfy strong concentration and anti-concentration properties.

In terms of lower bounds and hardness results for this problem, the work of (Goel et al., 2019; Diakonikolas et al., 2020; Goel et al., 2020; Diakonikolas et al., 2021, 2022) has established superpolynomial hardness even for the setting of agnostically learning single ReLUs over Gaussian marginals.

Limitations and directions for future work.While we justify a certain dependence on the norm bound \(B\) in our main result on agnostically learning SIMs, we do not completely justify the exact form of Theorem 1.1. An important direction for future work is to tightly characterize the optimal bounds achievable for this problem, as well as to show matching algorithms.

### Preliminaries

For the following, \((,y)\) is used to denote a labelled sample from a distribution \(D\) over \(^{d}\), where \(\) denotes the interval \(\) unless it is specified to be the set \(\{0,1\}\). We note that, although we provide results for the setting where the labels lie within \(\), we may get similar results for any bounded label space. We use \(_{D}\) (resp. \(_{D}\)) to denote the probability (resp. expectation) over \(D\) and, similarly, \(_{S}\) (resp. \(_{S}\)) to denote the corresponding empirical quantity over a set \(S\) of labelled examples. Throughout the paper, we will use the term differentiable function to mean a function that is differentiable except on finitely many points. Our main results will assume the following about the marginal distribution on \(^{d}\).

**Definition 1.4** (Bounded moments).: For \( 1\) and \(k\), we say that a distribution \(D_{}\) over \(^{d}\) has \(\)-bounded \(2k\)-th moments if for any \(^{d-1}\) we have \(_{ D_{}}[()^{2k}]\).

For a concept class \(:^{d}\), we define \((,D)\) to be the minimum squared error achievable by a concept \(c:^{d}\) in \(\) with respect to the distribution \(D\). We now define our main learning task.

**Definition 1.5** (Agnostic learning).: Let \(:^{d}\) be a concept class, let \(:\) be an increasing function, let \(D\) be a distribution over \(^{d}\) and \(>0\). We say that an algorithm \(\) agnostically learns the class \(\) up to error \(((,D))+\) if algorithm \(\), given a number of i.i.d. samples from \(D\), outputs, with probability at least \(2/3\) over the samples and the randomness of \(\), a hypothesis \(h:^{d}\) with \(_{D}[(y-h())^{2}]((,D))+\).

We will also provide results that are specific to the sigmoid activation and work under the assumption that the marginal distribution is sufficiently concentrated.

**Definition 1.6** (Concentrated marginals).: For \(>0\) and \(\), we say that a distribution \(D_{}\) over \(^{d}\) is \((,)\)-concentrated if for any \(^{d-1}\) and \(r 0\) we have \(_{ D_{}}[|| r] (-r^{})\).

**Definition 1.7** (Fenchel-Legendre pairs).: We call a pair of functions \((f,g)\) a Fenchel-Legendre pair if the following conditions hold.

1. \(g^{}:\) is continuous, non-decreasing, differentiable and \(1\)-Lipschitz with range \((g^{})(0,1)\) and \(g(t)=_{0}^{t}g^{}()\ d\), for any \(t\).
2. \(f:(g^{})\) is the convex conjugate (Fenchel-Legendre transform) of \(g\), i.e., we have \(f(r)=_{t}r t-g(t)\) for any \(r(g^{})\).

For such pairs of functions (and their derivatives \(f^{},g^{}\)), the following are true for \(r(g^{})\) and \(t(f^{})\) (note that \((f^{})\) is not necessarily \(\) when \(g^{}\) is not invertible).

\[g^{}(f^{}(r))=r\;\;\;\;f(r)=rf^{}(r)-g (f^{}(r)),\;\;r(g^{})\] (1.1) \[f^{}(g^{}(t))=t\;\;\;\;g(t)=tg^{}(t)- f(g^{}(t)),\;\;t(f^{})\] (1.2)Note that \(g^{}\) will be used as an activation function for single neurons and \(f^{}\) corresponds to the unknown link function of a SIM (or the known link function of a GLM). We say that \(g^{}\) is bi-Lipschitz if for any \(t_{1}<t_{2}\) we have that \((g^{}(t_{2})-g^{}(t_{1}))/(t_{2}-t_{1})[,]\). If \(g^{}\) is \([,]\) bi-Lipschitz, then \(f^{}\) is \([,]\) bi-Lipschitz. However, the converse implication is not necessarily true when \(g^{}\) is not strictly increasing.

**Definition 1.8** (Matching Losses).: For a non-decreasing and Lipschitz activation \(g^{}:\), the matching loss \(_{g}:\) is defined pointwise as follows:

\[_{g}(y,t)=_{0}^{t}g^{}()-y\ d,\]

where \(g(t)=_{0}^{t}g^{}\). The function \(_{g}\) is convex and smooth with respect to its second argument. The corresponding population matching loss is

\[_{g}(c\ ;D)=*{}_{(,y) D} _{g}(y,c())\] (1.3)

In Equation (1.3), \(c:^{d}\) is some concept and \(D\) is some distribution over \(^{d}\). In the specific case where \(c\) is a linear function, i.e., \(c()=\), for some \(^{d}\), then we may alternatively denote \(_{g}(c\ ;D)\) with \(_{g}(\ ;D)\).

We also define the Bregman divergence associated with \(f\) to be \(_{f}(q,r)=f(q)-f(r)-(q-r)f^{}(r)\), for any \(q,r(g^{})\). Note that \(_{f}(q,r) 0\) with equality iff \(q=r\).

**Definition 1.9** (SIMs and GLMs as Concept Classes).: For \(B>0\), we use \(_{B}\) to refer to the class of all SIMs of the form \( g^{}()\) where \(\|\|_{2} B\) and \(g^{}:\) is an arbitrary \(1\)-Lipschitz monotone activation that is differentiable (except possibly at finitely many points). We define \(_{g^{},B}\) similarly except for the case where \(g^{}\) is fixed and known.

We also define the notion of calibrated multiaccuracy that we need to obtain omnipredictors in our context.

**Definition 1.10** (Calibrated Multiaccuracy).: A predictor \(p:^{d}\) is called \(\)-multiaccurate if for all \(i[d]\), \(|*{}[_{i}(y-p())]|\). It is called \(\)-calibrated if \(|*{}_{p()}*{}_{y|p( )}[y-p()]|\).

## 2 Distortion Bounds for the Matching Loss

In this section, we propose a simple approach for bounding the squared error of a predictor that minimizes a (convex) matching loss, in the agnostic setting. We convert matching loss bounds to squared loss bounds in a generic way, through appropriate pointwise distortion bounds between the two losses. In particular, for a given matching loss \(_{g}\), we transform guarantees on \(_{g}\) that are competitive with the optimum linear minimizer of \(_{g}\) to guarantees on the squared error that are competitive with the optimum GLM whose activation (\(g^{}\)) depends on the matching loss at hand.

We now provide the main result we establish in this section.

**Theorem 2.1** (Squared Error Minimization through Matching Loss Minimization).: _Let \(D\) be a distribution over \(^{d}\), let \(0<\) and let \((f,g)\) be a Fenchel-Legendre pair such that \(g^{}:\) is \([,]\) bi-Lipschitz. Suppose that for a predictor \(p:^{d}(g^{})\) we have_

\[_{g}(f^{} p\,;D)_{\|\|_{2} B} _{g}(\,;D)+\] (2.1)

_Then we also have: \(_{2}(p)( _{g^{},B})+2\)._

The proof of Theorem 2.1 is based on the following pointwise distortion bound between matching losses corresponding to bi-Lipschitz link functions and the squared distance.

**Lemma 2.2** (Pointwise Distortion Bound for bi-Lipschitz link functions).: _Let \(0<\) and let \((f,g)\) be a Fenchel-Legendre pair such that \(f^{}:(g^{})\) is \([,]\) bi-Lipschitz. Then for any \(y,p(g^{})\) we have_

\[_{g}(y,f^{}(p))-_{g}(y,f^{}(y))=_{f}(y,p) [(y-p)^{2},(y-p)^{2}]\]In the case that \(f^{}\) is differentiable on \((0,1)\), the proof of Lemma 2.2 follows from an application of Taylor's approximation theorem of degree \(2\) on the function \(f\), since the Bregman divergence \(_{f}(y,p)\) is exactly equal to the error of the second degree Taylor's approximation of \(f(y)\) around \(p\) and \(f^{}()[,]\) for any \((g^{})\). The relationship between \(_{g}\) and \(_{f}\) follows from property (1.2). Note that when \(g^{}\) is \([,]\) bi-Lipschitz, then \(f^{}\) is \([,]\) bi-Lipschitz.

Theorem 2.1 follows by applying Lemma 2.2 appropriately to bound the error of a predictor \(p\) by its matching loss \(_{g}(f^{} p)\) and bound the matching loss of the linear function corresponding to \(^{*}\) by the squared error of \(g^{}(^{*})\), where \(g^{}(^{*})\) is the element of \(_{g^{},B}\) with minimum squared error.

Although Theorem 2.1 only applies to bi-Lipschitz activations \(g^{}\), it has the advantage that the assumption it makes on \(p\) corresponds to a convex optimization problem and, when the marginal distribution has certain concentration properties (for generalization), can be solved efficiently through gradient descent on the empirical loss function. As a consequence, for bi-Lipschitz activations we can obtain \(O()\) efficiently under mild distributional assumptions in the agnostic setting.

## 3 Agnostically Learning Single-Index Models

In this section, we provide our main result on agnostically learning SIMs. We combine the distortion bounds we established in Section 2 with results from Gopalan et al. (2023) on Omniprediction, which can be used to learn a predictor \(p\) that satisfies the guarantee of Theorem 2.1 simultaneously for all bi-Lipschitz activations. By doing so, we obtain a result for all Lipschitz and non-decreasing activations simultaneously.

**Theorem 3.1** (Agnostically Learning SIMs).: _Let \(,B 1\), \(>0\) and let \(D\) be a distribution over \(^{d}\) with second moments bounded by \(\). Then, Algorithm 1 agnostically learns the class \(_{B}\) over \(D\) up to squared error \(O(B(_{B},D)})+\) using time and sample complexity \((d,B,,)\). Moreover, the same is true for any algorithm with an omniprediction guarantee like the one of Theorem 3.2._

In order to apply Theorem 2.1, we use the following theorem which is a combination of results in Gopalan et al. (2023), where they show that the matching losses corresponding to a wide class of functions can all be minimized simultaneously by an efficiently computable predictor.

**Theorem 3.2** (Omnipredictors for Matching Losses, combination of results in Gopalan et al. (2023)).: _Let \(,L,R,B 1\), \(>0\) and let \(D\) be a distribution over \(^{d}\) whose marginal on \(^{d}\) has \(\)-bounded second moments. Then, Algorithm 1, given sample access to \(D\), with probability at least \(2/3\) returns a predictor \(p:^{d}(0,1)\) with the following guarantee. For any Fenchel-Legendre pair \((f,g)\) such that \(g^{}:\) is \(L\)-Lipschitz, and \(f^{}\) takes values within the interval \([-R,R]\), \(p\) satisfies_

\[_{g}(f^{} p\;;D)_{\|\|_{2} B} _{g}(\;;;D)+.\]

_The algorithm requires time and sample complexity \((,B,L,R,)\)._

Algorithm 1 is a version of the algorithm of Gopalan et al. (2023) for calibrated multiaccuracy, specific to our setting. For a more quantitative version of Theorem 3.2, see Theorem C.3 in the appendix.

We aim to apply Theorem 3.2 to the class of all Lipschitz activations (which is wider than the class of bi-Lipschitz activations). This is enabled by the following lemma, whose proof is based on Theorem 2.1 and the fact that the error of a predictor is bounded by the sum of the error of another predictor and the squared expected distance between the two predictors.

**Lemma 3.3**.: _Let \(D\) be a distribution over \(^{d}\). Let \(g^{}:\) be some fixed activation, and \(f^{}\) its dual. Consider the class \(_{g^{},B}\), and let \(^{*}\) be the weights achieving \((_{g^{},B},D)\). Let \(^{}:\) be an \([,]\) bi-Lipschitz function (differentiable except possibly at finitely many points) that we wish to approximate \(g^{}\) by. Any predictor \(p:^{d}\) that satisfies_

\[_{}(f^{} p\;;D)_{\|\|_{2} B} _{}(\;;D)+\]

_also satisfies the following \(_{2}\) error guarantee:_

\[_{2}(p)\,(_{g^{ },B})+\;[(g^{}( ^{*})-^{}(^{*}))^{2} ]+2.\]By combining Theorem 3.2 with Lemma 3.3 (whose proofs can be found in Appendix C), we are now ready to prove our main theorem.

Proof of Theorem 3.1.: We will combine Theorem 3.2, which states that there is an efficient algorithm that simultaneously minimizes the matching losses corresponding to bounded, non-decreasing and Lipschitz activations, with Lemma 3.3, which implies that minimizing the matching loss corresponding to the nearest bi-Lipschitz activation is sufficient to obtain small error. Note that we may assume that \(<1/2\), since otherwise the problem is trivial (output the zero function and pick \(C=2\)).

As a first step, we show that link functions corresponding to bi-Lipschitz activations are bounded (according to Definition C.1, for \(=0\)). In particular, let \(^{}:\) be an \([,]\) bi-Lipschitz activation for some \(>0\) such that \(^{}(s)[-1,2]\) for some \(s\) and let \(^{}\) be the inverse of \(^{}\) (\(^{}\) is invertible since it is strictly increasing). We will show that \(^{}(r)[-R,R]\) for any \(r\) and \(R=O(|s|+1/)\).

We pick \(r_{0}=0\), \(r_{1}=1\) and get that \(|^{}(^{}(s))-^{}(r_{0})||^{ }(s)-r_{0}|\). Hence \(^{}(r_{0})^{}(^{}(s))-=s- \). Similarly, we get \(^{}(r_{1}) s+\). Therefore, \(^{}(r)[^{}(0),^{}(1)][-|s|-,|s|+]\), for any \(r\), due to monotonicity of \(^{}\).

For a given non-decreasing and \(1\)-Lipschitz \(g^{}\), we will now show that there is a bounded bi-Lipschitz activation \(^{}\) such that if the assumption of Lemma 3.3 is satisfied for \(^{}\) by a predictor \(p\), then the error of \(p\) is bounded by

\[_{2}(p) OB((_{g^{ },B}))^{1/2}+O( B^{2})\]

Suppose, first, that \((_{g^{},B})^{2}\). Then, we pick \(^{}(t)=g^{}(t)+ t\). Note that \(^{}\) is \([,1+]\) bi-Lipschitz. Moreover, since \((_{g^{},B})^{2}\), we must have some \(s\) with \(|s| 2 B^{2}\) such that \(g^{}(s)[-1,2]\). Otherwise, we would have \((_{g^{},B})=[(g^{}(^{ *})-y)^{2}][(g^{}(^{*} )-y)^{2}|^{*} 2 B^{2}|] [|^{*}| 2 B^{2}] 1- [|^{*}|>2 B^{2}|> ^{2}\), due to Chebyshev's inequality, the fact that \(^{*}\) and the bounded moments assumption. Therefore, \(^{}\) is \((R=2 B^{2}+,=0)\)-bounded and we have

\[[(g^{}(^{*})-^{} (^{*}))^{2}]^{2}\ ((^{*})^{2}]^{2} B^{2}\]

As a consequence, under the assumption of Lemma 3.3 for \(^{}\), the error of the corresponding predictor \(p\) is \(_{2}(p) 2(1+)+2(1+)  B^{2}+2(1+)=O( B^{2})\).

In the case that \((_{g^{},B})>^{2}\), we pick \(^{}(t)=g^{}(t)+t\ (_{g^{},B})}}{B }\). We may also assume that \((_{g^{},B}) 1/2\), since otherwise any predictor with range \(\) will have error at most \(2(_{g^{},B})\). Then, \(\) is \((O( B^{2}+}{}),0)\)-bounded, \(^{}\) is \([(_{g^{},B})}/,1+ ]\) bi-Lipschitz which gives

\[[(g^{}(^{*})-^{} (^{*}))^{2}]( _{g^{},B})}{B^{2}}\ [(^{*})^{2}] (_{g^{},B})\]

As a consequence, under the assumption of Lemma 3.3 for \(^{}\), the error of the corresponding predictor \(p\) is \(_{2}(p) 4(1+)B( _{g^{},B})}+2(1+)\). Using a similar approach as for the case \((_{g^{},B})\), we can show that \(^{}\) is polynomially bounded (as per Definition C.1), since \((_{g^{},B})\).

To conclude the proof of Theorem 3.1, we apply Theorem 3.2 with appropriate (polynomial) choice of parameters (\(R=O( B^{2}+}{})\), \(L=2\)), to show that there is an efficient algorithm that outputs a predictor \(p:^{d}(0,1)\) for which the assumption of Lemma 3.3 holds simultaneously for all bi-Lipschitz activations (\(^{}\)) with sufficiently bounded inverses (\(^{}\)). 

## 4 Stronger Guarantees for Logistic Regression

In this section, we follow the same recipe we used in Section 2 to get distortion bounds similar to Theorem 2.1 for the sigmoid activation (or, equivalently, the logistic model) under the assumption that the marginal distribution is sufficiently concentrated (see Definition C.1). In particular, Theorem 2.1 does not hold, since the sigmoid is not bi-Lipschitz and our main Theorem 3.1 only provides a guarantee of \(O(})\) for squared error. We use appropriate pointwise distortion bounds for the matching loss corresponding to the sigmoid activation and provide guarantees of \(()\) for logistic regression with respect to both squared and absolute error, under appropriate assumptions about the concentration of the marginal distribution. The proofs of this section are provided in Appendix D.

For the logistic model, the link function \(f^{}\) is defined as \(f^{}(r)=()\), for \(r(0,1)\) and the corresponding activation \(g^{}\) is the sigmoid \(g^{}(t)=}\) for \(t\). The corresponding matching loss is the logistic loss.

Squared error.We first provide a result for squared loss minimization. In comparison to Theorem 2.1, the qualitative interpretation of the following theorem is that, while the sigmoid activation is not bi-Lipschitz, it is effectively bi-Lipschitz under sufficiently concentrated marginals.

**Theorem 4.1** (Squared Loss Minimization through Logistic Loss Minimization).: _Let \(D\) be a distribution over \(^{d}\) whose marginal on \(^{d}\) is \((1,2)\)-concentrated. Let \(g^{}:\) be the sigmoid activation, i.e., \(g^{}(t)=(1+e^{-t})^{-1}\) for \(t\). Assume that for some \(B>0\), \(>0\) and a predictor \(p:^{d}(0,1)\) we have_

\[_{g}(f^{} p\,;D)_{:\|\|_{2}  B}_{g}(\,;D)+\] (4.1)

_If we let \(_{g}=_{\|\|_{2} B}_{2}( g^{}_{})\), then for the predictor \(p\) and some universal constant \(C>0\) we also have_

\[_{2}(p) C_{g}(B^{2}+\,_{g}}})+2.\]

In particular, the squared error of \(p\) is upper bounded by \((_{g})\), since the function \(t(^{1/2}t)\) is asymptotically smaller than any polynomial function \(t t^{}\) with \(>0\).

Once more, the proof of our result is based on an appropriate pointwise distortion bound which we provide below. It follows by the fact that the Bregman divergence corresponding to the sigmoid is the Kullback-Leibler divergence and by combining Pinsker's inequality (lower bound) with Lemma 4.1 of Gotze et al. (2019) (upper bound).

**Lemma 4.2** (Pointwise Distortion Bound for Sigmoid).: _Let \(g^{}\) be the sigmoid activation. Then, for any \(y,p(0,1)\) we have that \(_{f}(y,p)=_{}(y\|p)=y(y/p)+(1-y)( {1-p})\). Moreover_

\[_{g}(y,f^{}(p))-_{g}(y,f^{}(y))=_{}(y \|p)[(y-p)^{2},\ }(y-p)^{2}]\]

We translate Lemma 4.2 to a bound on the squared error of a matching loss minimizer (in this case, the logistic loss) using an approach similar to the one for Theorem 2.1. In order to use the upper bound on the surrogate loss provided by Lemma 4.2, we apply it to \(p g^{}()\), where \(g^{}\) is the sigmoid function, and observe that the quantity \(\) is exponential in \(||\). Hence, when the marginal is \((,2)\)-concentrated (subgaussian concentration), then \(\) is effectively bounded.

Absolute error.All of the results we have provided so far have focused on squared error minimization. We now show that our approach yields results even for the absolute error, which can also be viewed as learning in the p-concept model (Kearns and Schapire, 1994). In particular, for a distribution \(D\) over \(^{d}\), we define the absolute error of a predictor \(p:^{d}\) as follows.

\[_{1}(p)=*{}_{(,y) D}[| y-p()|]\]

In the specific case when the labels are binary, i.e., \(y\{0,1\}\), we have

\[_{1}(p)=*{}_{(,y) D}[| y-p()|]=*{}_{(,y,y_{p}) D_{p}}[y  y_{p}]\] (see Proposition A.2 )

where the distribution \(D_{p}\) is over \(^{d}\{0,1\}\{0,1\}\) and is formed by drawing samples \((,y)\) from \(D\) and, given \(\), forming \(y_{p}\) by drawing a conditionally independent Bernoulli random variable with parameter \(p()\). We provide the following result.

**Theorem 4.3** (Absolute Loss Minimization through Logistic Loss Minimization).: _Let \(D\) be a distribution over \(^{d}\{0,1\}\) whose marginal on \(^{d}\) is \((1,1)\)-concentrated. Let \(g^{}:\) be the sigmoid activation, i.e., \(g^{}(t)=(1+e^{-t})^{-1}\) for \(t\). Assume that for some \(B>0\), \(>0\) and a predictor \(p:^{d}(0,1)\) we have_

\[_{g}(f^{} p\,;D)_{:\|\|_{2}  B}_{g}(\,;D)+\] (4.2)

_If we let \(_{g}=_{\|\|_{2} B}_{1}( g^{}_{})\), then for the predictor \(p\) and some universal constant \(C>0\) we also have_

\[_{1}(p) C\,B_{g}\,_{g}}+\]

The corresponding distortion bound in this case is between the absolute and logistic losses and works when the labels are binary.

**Lemma 4.4** (Pointwise Distortion between Absolute and Logistic Loss).: _Let \(g^{}\) be the sigmoid activation. Then, there is a constant \(c\) such that for any \(y\{0,1\}\) and \(p(0,1)\), we have_

\[_{g}(y,f^{}(p))-c[|y-p|\,\ 2( )|y-p|]\]

The bound of Theorem 4.3 implies an algorithm for learning an unknown sigmoid neuron in the p-concept model, by minimizing a convex loss. While there are algorithms achieving stronger guarantees (Diakonikolas et al., 2022b) for agnostically learning sigmoid neurons, such algorithms typically make strong distributional assumptions including concentration, anti-concentration and anti-anti-concentration or boundedness.

Moreover, it is useful to compare the bound we provide in Theorem 4.3 to a lower bound by Diakonikolas et al. (2020c, Theorem 4.1), which concerns the problem of agnostically learning halfspaces by minimizing convex surrogates. In particular, they show that even under log-concave marginals, no convex surrogate loss can achieve a guarantee better than \(O((1/))\), where \(\) is measured with respect to the \(_{1}\) error (which is equal to the probability of error). The result is not directly comparable to our upper bound, since we examine the sigmoid activation. Their setting can be viewed as a limit case of ours by letting the norm of the weight vector grow indefinitely (the sigmoid tends to the step function), but the main complication is that our upper bound is of the form \(O(B(1/))\), which scales with \(B\). However, their lower bound concerns marginal distributions that are not only concentrated, but are also anti-concentrated and anti-anticoncentrated, while our results only make concentration assumptions.

## 5 Necessity of Norm Dependence

In this final section, we use a lower bound due to Diakonikolas et al. (2022a) on agnostic learning of GLMs using SQ algorithms and compare it with our main result (Theorem 3.1). For simplicity, we specialize to the case of the standard sigmoid or logistic function. A modification of their proof ensures that the bound holds under isotropic marginals.2

**Theorem 5.1** (SQ Lower Bound for Agnostically Learning GLMs, variant of (Diakonikolas et al., 2022a, Thm C.3)).: _Let \(g^{}:^{d}\) be the standard logistic function. Any SQ algorithm either requires \(d^{(1)}\) queries or \(d^{-(1)}\) tolerance to distinguish between the following two labeled distributions:_

* _(Labels have signal.)_ \(D_{}\) _on_ \(^{d}\) _is such that_ \((_{g^{},B},D_{})(- (^{1/4}d))=o(1)\) _for some_ \(B=(d)\)_._
* _(Labels are random.)_ \(D_{}\) _on_ \(^{d}\) _is such that the labels_ \(y\) _are drawn i.i.d. from_ \(\{a,b\}\) _for certain universal constants_ \(a,b\)_. In particular,_ \((_{g^{},B},D_{})=(1)\) _for any_ \(B\)_._

_Both \(D_{}\) and \(D_{}\) have the same marginal on \(^{d}\), with \(1\)-bounded second moments._

Let us consider applying our main theorem (Theorem 3.1) to this setting, with \(D\) being either \(D_{}\) or \(D_{}\), and with the same \(B=(d)\) as is required to achieve small error in the "labels have signal" case. We would obtain a predictor with \(_{2}\) error at most \(B(_{g^{},B})}\) (or indeed with \(_{B}\) in place of \(_{g^{},B}\)). Since this is \((1)\), this guarantee is insufficient to distinguish the two cases above, which is as it should be since our main algorithm indeed fits into the SQ framework.

Theorem 5.1 does, however, justify a dependence on the norm \(B\) in our main result. In particular, it is clear that a guarantee of the form \((_{g^{},B})^{c}\) for any universal constant \(c>0\) (independent of \(B\)) would be too strong, as it would let us distinguish the two cases above. In fact, this lower bound rules out a large space of potential error guarantees stated as functions of \(B\) and \((_{g^{},B})\). For instance, for sufficiently large \(d\), it rules out any error guarantee of the form \((O(^{1/5}B))(_{g^{},B})^{c^{}}\) for any universal constant \(c^{}>0\).