# InstanT: Semi-supervised Learning with

Instance-dependent Thresholds

 Muyang Li\({}^{1}\), Runze Wu\({}^{2}\), Haoyu Liu\({}^{2}\), Jun Yu\({}^{3}\), Xun Yang\({}^{3}\), Bo Han\({}^{4}\), Tongliang Liu\({}^{1}\)

\({}^{1}\)Sydney AI Center, The University of Sydney; \({}^{2}\)FUXI AI Lab, NetEase;

\({}^{3}\)University of Science and Technology of China; \({}^{4}\)Hong Kong Baptist University

Correspondence to Tongliang Liu (tongliang.liu@sydney.edu.au)

###### Abstract

Semi-supervised learning (SSL) has been a fundamental challenge in machine learning for decades. The primary family of SSL algorithms, known as pseudo-labeling, involves assigning pseudo-labels to confident unlabeled instances and incorporating them into the training set. Therefore, the selection criteria of confident instances are crucial to the success of SSL. Recently, there has been growing interest in the development of SSL methods that use dynamic or adaptive thresholds. Yet, these methods typically apply the same threshold to all samples, or use class-dependent thresholds for instances belonging to a certain class, while neglecting instance-level information. In this paper, we propose the study of instance-dependent thresholds, which has the highest degree of freedom compared with existing methods. Specifically, we devise a novel instance-dependent threshold function for all unlabeled instances by utilizing their instance-level ambiguity and the instance-dependent error rates of pseudo-labels, so instances that are more likely to have incorrect pseudo-labels will have higher thresholds. Furthermore, we demonstrate that our instance-dependent threshold function provides a bounded probabilistic guarantee for the correctness of the pseudo-labels it assigns. Our implementation is available at https://github.com/tmllab/2023_NeurIPS_InstanT.

## 1 Introduction

In recent years, machine learning algorithms trained on abundant accurately labeled data have demonstrated unparalleled performance across different domains. However, in practice, it is often financially infeasible to collect reliable labels for all samples in large-scale datasets. A more practical solution is to select a subset of the data and use expert annotations to assign labels to them . This scenario, where the majority of the data is unlabeled and only a portion has reliable labels, is known as semi-supervised learning (SSL). In SSL, our aim is usually to learn a classifier that has comparable performance to the one trained in a fully supervised manner. To achieve this, the majority of existing approaches adopt a training strategy named pseudo-labeling. More specifically, a model will be trained on a small set of labeled data first, and this model will then be applied to the larger unlabeled set to assign predicted pseudo-labels to them. If the model's confidence in an instance exceeds a certain threshold, then this instance along with its predicted pseudo-label will be added to the training set for this iteration. Thus by iteratively expanding the training set, if the expanded instances are indeed assigned with correct labels, then the classification error will be gradually reduced .

However, since the model is bound to make mistakes, the newly added training samples are not always assigned with correct labels, which generates label noise [5; 46; 44]. Under the influence of label noise, the model will gradually over-fits to noisy supervision, hence accumulating generalization error. Since the model makes predictions based on instance features, it is usually assumed that for"hard" examples, the classifier is more likely to give incorrect predictions to them , making the label noise pattern **instance-dependent**.

Consequently, selecting an appropriate confidence threshold for pseudo-label assignment becomes a key factor that determines the performances of SSL methods. For predictions that are more likely to have incorrect labels, we want to assign higher thresholds to avoid selecting them prematurely. And for predictions that are more likely to be correct, we want to assign lower thresholds to encourage adding them to the training set at an earlier stage. Conventionally, such a threshold is usually determined by empirical experience in an ad-hoc manner, where only a few of the existing works consider the theoretical implication of threshold selection. Recently, increasing research focused on the investigation of dynamic or adaptive thresholds , where such confidence threshold is conditional on some external factors, such as the current training progress. Notably, the vast majority of those methods consider **a single threshold for all instances** and overlook their instance-level information, such as the instance-level label noise rate. As illustrated in Figure 1, the instance-dependent threshold is the most flexible among existing threshold types.

Motivated by this challenge, in this paper, we propose a new SSL algorithm named **Instance-dependent Th**rsholding (**InstanT**). Since the learned model is subject to the influence of instance-dependent label error, we aim to quantify and reduce such error by estimating an instance-dependent confidence threshold function based on the potential label noise level and instance ambiguity (clean class posterior). In addition, we can derive a lower-bounded probability, for samples that satisfy the thresholds will be assigned to a correct label. From our main theorem, we show that as the training progresses, this probability lower-bound will asymptotically increase towards one, hence guaranteeing the pseudo-label quality for instances that satisfies our proposed thresholds.

We summarize our contributions as follows: (1) We propose an SSL algorithm named InstanT, which assigns thresholds to individual unlabeled samples based on the instance-dependent label noise level and prediction confidence. (2) We prove InstanT has a bounded probability to be correct, which vouch for the reliability of the pseudo-labels it assigns with a theoretical guarantee. (3) To the best of our knowledge, this is the first attempt to estimate the instance-dependent thresholds in SSL, which has the highest degree of freedom compared with existing methods. (4) Through extensive experiments, we show that our proposed method is able to surpass state-of-the-art (SOTA) SSL methods across multiple commonly used benchmark datasets.

Related work.As we mentioned, some existing works have already been considered to improve pseudo-labeling by leveraging "dynamic" or "adaptive" thresholds. More specifically, Dash  considers a monotonically decreasing dynamic threshold, based on the intuition that as training progresses, the model at a later stage will provide more reliable predictions than at early stages. FlexMatch  further introduced class-dependent learning status in addition to the dynamic threshold. Adsh  employs adaptive threshold under class imbalanced scenario by optimizing the number of pseudo-labels per class. As for adaptive thresholds, AdaMatch  considers using a pre-defined threshold factor multiplied by the averaged top-1 prediction confidence, making it adaptive to the

Figure 1: Illustration on the differences between uniform thresholds, class-dependent thresholds, and instance-dependent thresholds in SSL. The black solid lines are the decision boundaries generated by the classifier, and the colored dashed lines are the confidence thresholds, the colored square, triangle, and circle represent the unlabeled instances with their predicted class. We can observe that a uniform threshold does not depend on any factors, class-dependent thresholds only depend on the predicted pseudo-label, and instance-dependent thresholds depend on the features of unlabeled instances.

model's current confidence level. FreeMatch  considers a combination of adaptive global and local thresholds, which combines the training progress and class-specific information.

In addition to the aforementioned more recent methods, most of the classical methods use pre-defined fixed thresholds, which remain constant throughout the entire training process [8; 18; 19; 29; 36]. Such thresholds are usually set to be high enough to prevent the occurrence of incorrect pseudo-labels. Notably, other approaches such as reweighting [10; 22], distribution alignment [20; 31; 42], contrastive learning [24; 49], consistency regularization [1; 34] were also employed to improve SSL.

## 2 Preliminaries

### Notation and Problem Setting

We consider the general setting from SSL, namely, we have a small group of labeled instances \(X_{l}:=\{_{1},...,_{}\}\) with their corresponding labels \(Y_{l}:=\{y_{1},...,y_{n}\}\). And there is an unlabeled instance group \(X_{u}:=\{},...,}\}\) with their latent labels \(Y_{u}:=\{y_{n+1},...,y_{n+m}\}\), usually, we have \(m>>n\). Furthermore, we assume that the labeled instances and unlabeled instances are independent and identically distributed (i.i.d), i.e. \(X:=\{X_{l},X_{u}\}\), \(Y:=\{Y_{l},Y_{u}\}\).

We term the model trained with clean and pseudo-labels as \(\), parameterized by \(\). Since \(_{}\) is bound to make mistakes, its generated pseudo-label \(_{u}\) will contain label errors. Hence the softmax predictions of \(_{}\) are actually approximating the noisy class posterior. We fix the notation for the real noisy class posterior as \(P(|X)_{t}\) at \(t\)-th iteration, whereas our approximated noise class posterior is \((|X)_{t}\), which can be obtained from the softmax predictions of the model \(_{}\) at \(t\)-th iteration.

As the clean label for the unlabeled set is unknown, we instead wish to recover the _Bayes optimal label_ for all the unlabeled instances, which is the label class that maximizes the clean class posterior [50; 56]. The Bayes optimal label is generated by the hypothesis that minimizes the risk within the hypothesis space, i.e. the Bayes optimal classifier, which we denote as \(h^{*}\). And we can denote the Bayes optimal label for \(\) as \(h^{*}()\).

### Instance-dependent Label Noise in SSL

As mentioned in the previous section, label noise is an inevitable challenge in SSL. Moreover, since \(P(|X)\) is dependent on \(X\), we say the label noise is instance-dependent [12; 45]. Concretely, we have \(P(|X=)=T()P(Y|X=)\)[32; 35], where \(T(})\) is the **instance-dependent transition matrix**, which models the generation of label noise. \(P(Y|X=)\) represents the latent clean class posterior for unlabeled samples. We can define the \(ij\)-th entry of \(T()\) as:

\[T_{i,j}()=P(=j|Y=i,X=),\] (1)

which means the \(ij\)-th entry of \(T()\) models the probability that \(\) with clean label \(y=i\) will be predicted as \(=j\).

### Tsybakov Margin Condition

We assume our learned classifier satisfies Tsybakov Margin Condition , which essentially restrained the level of complexities of the classification problem by assuming the data are separable enough. It is a commonly used assumption for various Machine Learning problems [2; 6; 56] including SSL . Without the loss of generality, we will directly present the multi-class Tsybakov Margin Condition [2; 56], whereas the binary case will be provided in the Appendix B.

**Assumption 1** (Multi-class Tsybakov Margin Condition).: _For some finite constant \(C,>0\), and \(_{0}(0,1]\), the Tsybakov Margin Condition holds if \((0,_{0}]\), we have_

\[P[P(Y=|X=)-P(Y=s|X=)] C^{ }.\]

Where \(P(Y=|X=)\) and \(P(Y=s|X=)\) are the largest and second-largest clean class posterior probability for \(\).

### Quality-Quantity Trade-off in SSL

To understand the choice of pseudo-label thresholds in SSL, it is essential to consider the trade-off between the _quality_ and the _quantity_ of the pseudo-labels [10; 41]. Typically, a higher threshold is presumed to yield superior label quality, as it assigns pseudo-labels solely to instances where the classifier exhibits a high level of confidence. Conversely, elevating the threshold diminishes the quantity of pseudo-labels, as it leads to the exclusion of instances where the classifier lacks sufficient confidence. Hence causing the dilemma of quality-quantity trade-off.

One possible approach to address the quantity-quality trade-off in SSL is to apply a dynamic threshold, which is dependent on the training progress. This is because the evaluation metric for confident samples is usually not stationary throughout the training process of neural networks. As the model's predicted probability is subject to the level of over-fitting [16; 30], designing a thresholding function that is dependent on the training process is crucial to the success of SSL algorithms[9; 41]. Intuitively, in the early stage of the training process, where the model has just started to fitting, the threshold should not be set too high to filter out too many samples, and as the model has already well-fitted to the training data, the threshold should be elevated to combat falsely confident instances caused by confirmation bias .

## 3 Instance-dependent thresholds - A theoretical perspective

In this section, we focus on answering the specific research question: _For SSL, can we derive instance-dependent pseudo-label assignment thresholds with theoretical guarantees?_

The answer to this question is affirmative. We will present our main theorem and the proof to show that for samples that satisfy our instance-dependent threshold function, their likelihood of being correct is lower-bounded.

### Lower-bound Probability of Correctness

For a classifier trained with sufficient accurately labeled data, its prediction \(\) will be made if \(P(Y=|X=)>P(Y=s|X=)\). However, under the existence of label errors, the fidelity of the classifier can no longer be guaranteed. Recently, Zheng _et al._ showed that the classifier influenced by the label noise can still have a bounded error rate. We show that in SSL tasks, where the label noise is instance-dependent, we can establish a lower-bound probability for the predictions to be correct through similar derivations.

Recall Assumption 1, we have \(:=*{arg\,max}_{i}P(Y=i|X=})\), \(s:=*{arg\,max}_{i k}P(Y=i|X=})\), \(k:=*{arg\,max}_{i}(=i|X=})=_{u}\). And let \(_{}\) be the estimation error between our estimated noisy class posterior \((|X)\) and true noisy class posterior \(P(|X)\). We can then formulate our main theorem:

**Theorem 1**.: _Assume estimated noisy class posterior \((|X)\) satisfies Assumption 1 with \(C,>0\), \(_{0}(0,1]\), and \(_{}_{0}_{i}T_{i,i}(})\), we have:_

\[P[k=h^{*}(}),(=k|X=})(}) ]>1-C[O(_{})]^{},\] (2)

_where \((})\) is the instance-dependent threshold function:_

\[(})=T_{k,k}(})P(Y=s|X=})+_{i k}^{|Y|}T_ {i,k}(})P(Y=i|X=}).\] (3)

The proof of Theorem 1 is provided in Appendix C. Theorem 1 substantiates that, as our model over-fits to the label error (as \(_{}\) decreases), and the transition matrix can be successfully estimated, the probability lower-bound of the assigned pseudo-labels are correct will increase asymptotically. This guarantees the quality of the pseudo-labels \((})\) assigned. Note that, \((})\) requires the clean class posterior of unlabeled instances, which can be provably inferred upon the successful estimation of \(T()\) and the noisy class posterior [26; 32].

To gain a deeper understanding of Theorems 1, we will try to intuitively understands the outputs of \((})\). Our aim is to examine the behavior of \((})\) in relation to the unlabeled sample \(}\), whichhas a predetermined clean class posterior. If we observe an increase in the sum of column \(k\) in the transition matrix, it will result in a corresponding increase in the threshold \((})\). This implies that when \(}\) is highly likely to be assigned with an incorrect pseudo-label, \((})\) will assign a higher threshold in order to prevent its premature inclusion of \(\{},_{u}\}\) in the training set. Conversely, when \(}\) is less likely to be assigned with a incorrect pseudo-label, \((})\) will assign a lower threshold to encourage adding \(\{},_{u}\}\) to the training set.

### Identification of Instance-dependent Transition Matrix in SSL

Theorem 1 builds upon the assumption that \(T()\) can be successfully identified. Yet, the identification of \(T()\) has been a long-standing issue in the field of label noise learning. Fortunately, in SSL, we argue that \(T()\) can be provably identified under mild conditions. We will first justify our argument from a theoretical perspective based on the Theorems from Liu _et al._. Subsequently, we will also propose the empirical methods for estimating \(T()\) from an intuitively understandable aspect in the following section.

We will start by defining the identifiability, we use \(\) to represent an observation space and use \(\) to represent a general parametric space. Then, for a distribution with parameter \(^{{}^{}}\), such that \(^{{}^{}}\), a model \(P_{^{{}^{}}}\) on \(\) can be defined . We further define its identifiability as:

**Definition 1** (Identifiability).: _Parameter \(^{{}^{}}\) is said to be identifiable if \(P_{^{{}^{}}} P_{^{{}^{}}}\), \(^{{}^{}}^{{}^{}}\)._

In our case, \(^{{}^{}}():=\{T(),P(Y|X=)\}\), i.e., \(P_{^{{}^{}}}\) is a distribution defined by the transition probability and clean class posterior. To determine the identifiability of \(T()\), we also need to define the term _informative noisy labels_. Specifically, we have:

**Definition 2** (Informative noisy labels).: _For a given sample \((,y)_{i}\), its noisy label \(_{i}\) is said to be informative if \((T(}))=|Y|\)._

Then, we can make the following theorem:

**Theorem 2**.: _With i.i.d \((,y)_{i}\) pairs, three **informative noisy labels \(_{i}\)** are sufficient and necessary for the identification of \(T(})\)._

The Theorem 2 is based on Kruskal's Identifiability Theorem, the proof is provided in the Appendix D. From Theorem 2 and Definition 2, we can conclude that, in SSL, if we have more than three labeled samples per class, transition matrix \(T()\) is identifiable, more discussions on this can also be found in Appendix D.

Figure 2: Overview of InstanT. **Left:** training process of transition matrix estimator \(_{^{{}^{}}}\). For a labeled instance \(}\), whose label is \(\), the classifier \(_{}\) will first generate its noisy class posterior. Then, we want the _second row_ of the transition matrix to approximate the noisy class posterior by minimizing \(L(^{{}^{}})\) from Equation 7. **Right:** The inference process of \((})\). When calculating \((})\), \(}\) will be simultaneously passed to the classifier and transition matrix estimator. Since \(\) is the predicted class label from the model that minimizes the forward loss (Equation 8), we will use the _first column_ from our estimated transition matrix to modulate threshold based on Theorem 1.

InstanT: Instance-dependent thresholds for pseudo-label assignment

### Instance-dependent Threshold Estimation

While Theorem 1 provides a theoretical guarantee for the correctness of \((})\), as discussed in section 2.4, SSL algorithms can benefit from non-constant thresholds [9; 41]. Here we define the dynamic threshold \(_{t}\) at iteration \(t\) from the relative confidence threshold (RT) in AdaMatch :

\[_{t}=_{i=1}^{n}_{j[1,,|Y|]}(=j| X=})_{t},\] (4)

where \(n\) is the number of labeled samples, and \(\) is a fixed discount factor. Subsequently, we present the instance-dependent threshold function at iteration \(t\):

\[(})_{t}=[1,_{k,k}(})_{t}(Y=s|X= })_{t}+_{i k}^{|Y|}_{i,k}(Y=i|X=})_{t} +_{t}].\] (5)

Note that this estimation process requires the clean class posterior, which can be estimated or approximated using a transition matrix via importance re-weighting [26; 46] or loss correction [32; 45], details will be introduced in the following section. We also remark here that a less strict instance-dependent version of the threshold function can be used with the class-dependent transition matrix [32; 46; 52], which can lead to better empirical results under certain cases, it is also important to note that even under class-dependent transition matrix, the threshold function is still instance-dependent, as the noise class posterior are instance-dependent.

### Modelling Instance-dependent Label Noise

Now we will introduce a piratical method for estimating \(T()\) in SSL. Our approach involves approximating the label noise transition pattern using a Deep Neural Network. As we recall from Equation 1, \(T()\) can be described as a mapping function \(T:X^{|Y||Y|}\), since we already know that, in general cases, \(T()\) is identifiable in SSL, we only need to find the correct mapping function. In addition, since we have clean label \(Y_{l}\) for \(X_{l}\), and we also have the noisy pseudo-labels \(_{l}\) for \(X_{l}\), this enables us to directly fit a model to approximate \(T()\). Therefore, we want our transition matrix estimator, parameterized by \(^{{}^{}}\), to approximate the label noise transition process at every iteration \(t\):

\[_{i,j}(})=(_{l}=j|Y_{l}=i,X_{l}=}; ^{{}^{}}) P(=j|Y=i,X=)=T().\] (6)

The above approximation holds since label noise is assumed to be i.i.d, therefore both labeled and unlabeled samples share the same noise transition process. As we can estimate the noisy class posterior for labeled samples, we can therefore collect distribution \(D:=\{X_{l},Y_{l},(|X=})\}\). Observing Equation 6, we can design following objective for \(^{{}^{}}\) to minimize:

\[L_{D}(^{{}^{}})=-_{l=1}^{n}}_{l}( }(})).\] (7)

Where \(}\) is the one-hot vector for the label of labeled samples, thus \(}(})\) is equivalent to finding the class-conditional noise class posterior \((_{l}=j|Y_{l}=i,X_{l}=};^{{}^{}})\). \(}}\) is the one-hot noisy pseudo-label of \(}\), minimizing \(L_{D}(^{{}^{}})\) will result \((};^{{}^{}})\) approximate \(T()\). As shown in Figure 2, \(_{^{{}^{}}}\) will be trained in parallel to the main classifier and updated in every epoch.

Notably, the estimator for the transition matrix in InstanT is not constrained to one specific method, there has been a wide versatile of \(T\) estimator in the field of label noise learning [25; 45; 51], we have incorporated some of them into the implementation of InstanT, indicating the good extensibility of InstanT.

### Distribution Alignment

When the prediction of the classifier becomes imbalanced, Distribution Alignment (DA) is used to modulate the prediction so that the poorly predicted class will not be overlooked completely [7; 9; 20; 42]. This is achieved by estimating the prior of the pseudo-label \(P()\) during training, and setting a clean class prior \((Y)\), which is usually assumed to be uniformly distributed. The distribution alignment process can be described as \((=j|X=})=((=j|X=})P (Y=j)/P(=j))\), where \(\) is a total-sum scaling. Therefore, if the distribution of \(\) is severely imbalanced, DA will force the prediction for the minority class to be scaled up, and the prediction for the over-populated class to be scaled down.

### Loss Correction

While the classifier trained with incorrect pseudo-labels is subject to the influence of noisy supervision, in order to satisfy Theorem 1, the estimated softmax predictions from \(_{}\) must approximate the clean class posterior . As we have already demonstrated that the transition matrix can be identified in standard SSL setting, in this part, we will show how to infer clean class posterior from noisy predictions and transition matrix, by utilizing forward correction . Specifically, let \(_{}\) be a proper composite loss  (e.g. softmax cross-entropy loss), forward correction is defined as:

\[_{}^{}(_{u}},_{}(}))= (_{u}},(})^{}^{-1}_{}( {x_{u}})),\] (8)

where \(\) is an invertible link function. It has been proven that minimizing the forward loss with an accurately estimated transition matrix is equivalent to minimizing the loss defined over clean latent pseudo-labels . Therefore, minimizing \(_{}^{}\) will let the softmax prediction of classifier \(_{}\) approximate the clean class posterior.

### Training with Consistency Regularization Loss

Lastly, we provide an overview of the training objective of InstanT. We employ the widely adopted consistency regularization loss [36; 47], which assumes a well-learned and robust model should generate consistent predictions for random perturbations of a given instance. Previous methods have commonly employed augmentation techniques to introduce perturbations. The training loss of classifier \(_{}\) consists of two parts: supervised loss \(L_{D_{s}}()\) and unsupervised loss \(L_{D_{u}}()\). \(L_{D_{s}}()\) is calculated by the labeled samples, specifically, we have:

\[L_{D_{s}}()=_{l=1}^{n}(},_{}( (})),\] (9)

where \(n\) is the number of labeled samples, \(}\) is the one-hot label for \(}\), and \(\) is a weak augmentation function [36; 47], and \(\) is the cross-entropy loss function. Unsupervised loss \(L_{D_{u}}()\) is calculated by the unlabeled samples \(X_{u}\) and their predicted pseudo-labels \(_{u}\), which can be defined as:

\[L_{D_{u}}()=_{u=1}^{m}((=k|X= (}))>(})_{t})_{}^{}(_{u}},_{}((})),\] (10)

where \(m\) is the number of unlabeled instances, \(_{u}}\) is the one-hot pseudo-label for \(}\), and \(\) is a strong augmentation function [7; 14; 15]. \(\) is an indicator function that filters unlabeled instances using \((})_{t}\). Combining the supervised and unsupervised loss, we then have the overall training objective:

\[L_{D}()=L_{D_{s}}()+ L_{D_{u}}(),\] (11)

where \(\) is used to control the influence of the unsupervised loss.

## 5 Experiments

### Experiment Setup

We use three benchmark datasets for evaluating the performances of InstanT, they are: CIFAR-10, CIFAR-100 , and STL-10 . CIFAR-10 has 10 classes and 6,000 samples per class. CIFAR-100 has 100 classes and 600 samples per class. STL-10 has 10 classes and 500 labeled samples per class, 100,000 unlabeled instances in total. For each dataset, we set varying numbers of labeled samples to create different levels of difficulty. Following recently more challenging settings [10; 41], where the labeled samples could be extremely limited, we set the number of labeled samples per class on CIFAR-10 as \(\{1,4,25\}\), for CIFAR-100, we set as \(\{2,4,25\}\), for STL-10, we set as \(\{1,4,10\}\).

To ensure fair comparison between our method and all baselines, and to allow simple reproduction of our experimental results, we implemented InstanT and conducted all experiments within USB (Unified SSL Benchmark) framework . To improve the training efficiency, e.g. faster convergence, we use pre-trained ViT as the backbone model for all baselines with the same hyper-parameters in part I. We use AdamW  as the default optimizer, where the learning rate is set as \(5e-4\) for CIFAR-10/100, \(1e-4\) for STL-10 . The total training iterations \(K\) are set as 204,800 for all datasets. The training batch size is set as 8 for all datasets. For a more comprehensive evaluation of our proposed method, we also conduct experiments with Wide ResNet  trained from scratch, the detailed settings are aligned with existing works [7; 9; 36; 47; 48; 54], these results are summarized in part II. We select a range of popular baseline methods to evaluate against InstanT, which includes Pseudo-Label (PL) , MeanTeacher (MT) , VAT , MixMatch , UDA , FixMatch , Dash , FlexMatch  and AdaMatch . More comprehensive setting details, including full details of hyper-parameters, can be found in Appendix A.

### Main results

Part I.Our main results will be divided into two parts: part I and part II. For training efficiency, we will use a pre-trained Vision Transformer as the backbone model in part I, enabling us to compare a more diverse collection of baselines over a broader range of benchmark settings. In Part II, we will select baselines with better performance from Part I and train them from scratch for a more comprehensive evaluation.

Experimental results from part I are summarized in Table 1, where we can observe that, overall, InstanT achieves the best performances in most of the settings. More specifically, in the setting where label amount is extremely limited, InstanT exhibits the most significant improvement over all datasets, including an average 2.17% increase in accuracy on CIFAR-10 (10) over SOTA. While the identifiability of the transition matrix cannot be guaranteed with extremely limited labels, this could be remedied by including highly confident instances and their pseudo-labels to the labeled set and using them to better estimate the transition matrix. However, as the number of labeled samples increases, the

   Dataset &  &  &  \\  \# Label & 10 & 40 & 250 & 200 & 400 & 2500 & 10 & 40 & 100 \\  PL & 37.65\({}_{-3.1}\) & 81.21\({}_{-3.5}\) & 95.42\({}_{-0.4}\) & 63.34\({}_{-2.0}\) & 73.13\({}_{-0.9}\) & 84.28\({}_{-0.1}\) & 30.74\({}_{-0.6}\) & 57.16\({}_{-1.4}\) & 73.44\({}_{-1.5}\) \\ MT & 64.57\({}_{-4.9}\) & 87.15\({}_{-2.5}\) & 95.25\({}_{-0.5}\) & 59.50\({}_{-0.8}\) & 69.42\({}_{-0.9}\) & 82.91\({}_{-0.4}\) & 42.72\({}_{-7.8}\) & 66.80\({}_{-1.3}\) & 77.71\({}_{-1.8}\) \\ MixMatch & 65.04\({}_{-2.6}\) & 97.16\({}_{-0.6}\) & 97.95\({}_{-0.1}\) & 60.36\({}_{-0.1}\) & 72.26\({}_{-0.1}\) & 83.84\({}_{-0.2}\) & 10.68\({}_{-1.1}\) & 27.58\({}_{-1.6}\) & 61.85\({}_{-1.3}\) \\ VAT & 60.07\({}_{-6.3}\) & 93.33\({}_{-6.6}\) & 97.67\({}_{-0.2}\) & 65.89\({}_{-1.8}\) & 75.33\({}_{-0.4}\) & 83.42\({}_{-0.4}\) & 20.57\({}_{-4.4}\) & 65.18\({}_{-7.0}\) & 80.94\({}_{-1.0}\) \\ UDA & 78.76\({}_{-3.6}\) & 97.92\({}_{-0.2}\) & 97.96\({}_{-0.1}\) & 65.49\({}_{-1.8}\) & 75.85\({}_{-0.6}\) & 83.81\({}_{-0.2}\) & 48.37\({}_{-3.7}\) & 79.67\({}_{-4.9}\) & 89.46\({}_{-0.1}\) \\ FixMatch & 66.50\({}_{-1.5}\) & 97.44\({}_{-0.9}\) & 97.97\({}_{-0.1}\) & 65.29\({}_{-1.4}\) & 75.52\({}_{-1.1}\) & 83.98\({}_{-0.1}\) & 40.13\({}_{-3.4}\) & 77.72\({}_{-4.4}\) & 88.41\({}_{-1.6}\) \\ FlexMatch & 70.54\({}_{-9.6}\) & 97.78\({}_{-0.3}\) & 97.88\({}_{-0.2}\) & 63.76\({}_{-0.9}\) & 74.01\({}_{-0.5}\) & 83.72\({}_{-0.2}\) & 60.63\({}_{-12.9}\) & 78.17\({}_{-3.7}\) & 89.54\({}_{-1.3}\) \\ Dash & 74.35\({}_{-4.5}\) & 96.63\({}_{-0.2}\) & 97.90\({}_{-0.3}\) & 63.33\({}_{-0.4}\) & 74.54\({}_{-0.2}\) & 84.01\({}_{-0.2}\) & 41.06\({}_{-4.4}\) & 78.03\({}_{-3.9}\) & **89.56\({}_{-2.0}\)** \\ AdaMatch & 85.15\({}_{-2.0}\) & **97.94\({}_{-0.1}\)** & 97.92\({}_{-0.1}\) & 73.61\({}_{-0.1}\) & 78.59\({}_{-0.4}\) & **84.49\({}_{-0.1}\)** & 68.17\({}_{-2.7}\) & 83.50\({}_{-0.4}\) & 89.25\({}_{-1.5}\) \\  InstanT & **87.32\({}_{-1.0}\)** & 97.93\({}_{-0.1}\) & **98.08\({}_{-0.4}\)** & **74.17\({}_{-0.3}\)** & **78.80\({}_{-0.4}\)** & 84.28\({}_{-0.5}\) & **69.39\({}_{-1.4}\)** & **85.09\({}_{-1.8}\)** & 89.35\({}_{-1.9}\) \\   

Table 1: Top-1 accuracy with pre-trained ViT. The best performance is bold and the second best performance is underlined. All results are averaged with three random seeds {0,1,2} and reported with a 95% confidence interval.

   Method & s/epoch \\  FixMatch & 30.1 \\ AdaMatch & 30.1 \\ InstanT & 31.1 \\   

Table 2: Running time on STL-10(40)2.

dominance of InstanT becomes less pronounced. We hypothesize that this can be attributed to the classifier \(_{}\) performing better with more labeled samples, resulting in fewer label errors. Since InstanT primarily aims to increase the threshold for instances likely to have noisy pseudo-labels, the impact of InstanT becomes less significant in the presence of reduced label noise. Moreover, comparing the performance gap between InstanT and its closest counterpart, AdaMatch, it becomes apparent that InstanT consistently outperforms AdaMatch in nearly all cases. This observation underscores the non-trivial improvement and contribution brought forth by InstanT.

Part II.In this part, we will present the experiment results of part II, which are summarized in Table 3. Specifically, we will compare InstanT against other popular baseline methods on CIFAR-10/100 trained from scratch. For all methods, we use WRN-28-2 as default backbone model, trained with \(2^{20}\) iterations. We use SGD as the default optimizer, with momentum set as 0.9, initial learning rate as 0.03 and a cosine learning rate scheduler.

Observing from experiment results, we can summarize similar patterns as the results from part I. Overall, InstanT brings most significant performance increases when the number of labeled samples are limited, e.g., when there is only 4 labeled samples per-class. Notably, for CIFAR-100 (400), InstanT exhibits an increase in accuracy for nearly 2%, which can be view as a significant improvement over SOTA baseline. While on other cases such as CIFAR-10 (250) and CIFAR-100 (10000), InstanT outperformed by other baseline methods, we emphasize that no single baseline consistently outperforms InstanT in all cases.

Another question of interest is whether InstanT will be as efficient as other methods in training time. While learning transition matrix estimator does sacrifice time and space complexity to some extent, we show that InstanT can still maintain high efficiency. As shown in Table 2, compared with FixMatch and AdaMatch, InstanT almost does not introduce further significant computational burden.

To gain a better understanding of the key differences between InstanT and existing methods, we now focus on the visualizations of InstanT compared to representative baselines. Specifically, we will display the classification accuracy, unlabeled sample utilization ratio, and pseudo-label accuracy of InstanT, FreeMatch , AdaMatch , and FixMatch  in Figure 3. These models are trained from scratch on CIFAR-100 with 400 labeled samples, following commonly used settings [36; 41].

Examining the classification accuracy in Figure 3(a), we observe that while InstanT does not converge as rapidly as FreeMatch, it ultimately achieves superior accuracy in the later stages of training. This is attributed to InstanT's ability to find a better balance between the quantity and quality of pseudo-labels. As depicted in Figure 3(b) and 3(c), although InstanT utilizes fewer unlabeled samples compared to FreeMatch, it demonstrates significantly improved accuracy on pseudo-labels. On the other hand, AdaMatch exhibits similar pseudo-label accuracy to InstanT but utilizes a smaller proportion of unlabeled instances. The key distinction lies in the instance-dependent threshold function employed by InstanT, which allows us to set a lower \(_{t}\) for InstanT compared to AdaMatch, as InstanT mitigates the negative impact by increasing the instance-dependent thresholds for instances more prone to label noise, whereas AdaMatch will incorporate too many instances with incorrect pseudo-labels.

### Ablation study

   Dataset &  &  \\  \# Label & 40 & 250 & 4000 & 400 & 2500 & 10000 \\  UDA & 91.60\(\)1.5 & 94.44\(\)0.3 & 95.55\(\)0.0 & 40.60\(\)1.8 & 64.79\(\)0.8 & 72.13\(\)0.2 \\ FixMatch & 91.45\(\)1.7 & **94.87\(\)0.1** & 95.51\(\)0.1 & 45.08\(\)3.4 & 65.63\(\)0.4 & 71.65\(\)0.3 \\ FlexMatch & 94.53\(\)0.4 & 94.85\(\)0.1 & **95.62\(\)0.1** & 47.81\(\)1.4 & 66.11\(\)0.4 & **72.48\(\)0.2** \\ Dash & 84.67\(\)4.3 & 94.78\(\)0.3 & 95.54\(\)0.1 & 45.26\(\)2.6 & 65.51\(\)0.1 & 72.10\(\)0.3 \\ AdaMatch & 94.64\(\)0.0 & 94.76\(\)0.1 & 95.46\(\)0.1 & 52.02\(\)1.7 & 66.36\(\)0.7 & 72.32\(\)0.2 \\  InstanT & **94.83\(\)0.1** & 94.72\(\)0.2 & 95.57\(\)0.0 & **53.94\(\)1.8** & **67.09\(\)0.0** & 72.30\(\)0.4 \\   

Table 3: Top-1 accuracy with WRN-28. The best performance is bold and the second best performance is underlined. All results are averaged with three random seeds {0,1,2} and reported with a 95% confidence interval.

In this section, we will be verifying the effects of each component of InstanT, specifically, we will focus on evaluating the parts that we adapted from existing works, and determine how many performance improvements are attributed to our instance-dependent thresholds. Results of this ablation study are summarized in Table 4, where all experiments are averaged over three random seeds and ran on STL-10(40) using the setting of part I. First, we observe that InstanT-I removed both Distribution Alignment (DA) and relative thresholds (RT), which makes it equivalent to adding \((})\) on top of the fixed threshold of FixMatch. Notably, this also brings a significant improvement over FixMatch for over 2%, which further verifies the effectiveness of our proposed instance-dependent thresholds. Simply removing RT or DA will also cause a performance drop, which is aligned with the results from AdaMatch .

## 6 Conclusion

In this paper, we introduce a novel approach to thresholding techniques in SSL called instance-dependent confidence threshold. This approach offers the highest level of flexibility among existing methods, providing significant potential for further advancements. We then present InstanT, a theoretically guided method designed under the concept of instance-dependent threshold. InstanT assigns unique confidence thresholds to each unlabeled instance, considering their individual likelihood of having incorrect labels. Additionally, we demonstrate that our proposed method ensures a bounded probability of assigning correct pseudo-labels, a characteristic rarely offered by existing SSL approaches. Through extensive experiments, we demonstrate the competitive performance of InstanT when compared to SOTA baselines.

## 7 Acknowledgement

Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031. Muyang Li, Runze Wu and Haoyu Liu are supported by NetEase Youling Crowdsourcing Platform. This research was undertaken with the assistance of resources from the National Computational Infrastructure (NCI Australia), an NCRIS enabled capability supported by the Australian Government. The authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.

   Method & RT & DA & Acc. \\  FixMatch & ✗ & ✗ & 77.72\({}_{ 4.4}\) \\ AdaMatch & ✓ & ✓ & 83.50\({}_{ 2}\) \\ InstanT-I & ✗ & ✗ & 79.92\({}_{ 5.8}\) \\ InstanT-II & ✓ & ✗ & 81.49\({}_{ 4.9}\) \\ InstanT-III & ✗ & ✓ & 82.97\({}_{ 2.7}\) \\ InstanT & ✓ & ✓ & **85.09\({}_{ 2.8}\)** \\   

Table 4: Ablation study on STL-10(40).

Figure 3: Results from InstanT and selected baselines, trained from scratch on CIFAR-100(400).