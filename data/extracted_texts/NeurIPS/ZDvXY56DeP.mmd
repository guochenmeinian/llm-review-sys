# Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning

Shengyi Huang\({}^{1,2}\)1 &Quentin Gallouedec\({}^{1,3}\)1 &Florian Felten\({}^{4}\) &Antonin Raffin\({}^{5}\)

**Rousslan Fernand Julien Dossa\({}^{6}\)** &Yanxiao Zhao\({}^{7,8}\) &Ryan Sullivan\({}^{9}\) &Viktor Makoviychuk\({}^{10}\)

**Denys Makoviichuk\({}^{11}\)** &Mohamad H. Danesh\({}^{12}\) &Cyril Roumegous\({}^{13}\) &Jiayi Weng

**Chufan Chen\({}^{14}\)** &Md Masudur Rahman\({}^{15}\) &Joao G. M. Araujo\({}^{16}\) &Guorui Quan\({}^{17}\)

**Daniel C.H. Tan\({}^{18,19}\)** &Timo Klein\({}^{20,21}\) &Rujikorn Charakorn\({}^{22}\) &Mark Towers\({}^{23}\)

**Yann Berthelot\({}^{24,25}\)** &Kinal Mehta\({}^{26}\) &Dipam Chakraborty\({}^{27}\) &Arjun KG

**Valentin Charraut\({}^{28}\)** &Chang Ye\({}^{29}\) &Zichen Liu\({}^{30}\) &Lucas N. Alegre\({}^{31}\) &Alexander Nikulin\({}^{32}\)

**Xiao Hu\({}^{33}\)** &Tianlin Liu\({}^{34}\) &Jongwook Choi\({}^{35}\) &Brent Yi\({}^{36}\)**

###### Abstract

In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark (ORLB), a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. ORLB is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. It covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, ORLB comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of ORLB in practice. To the best of our knowledge, ORLB is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.

## 1 Introduction

Reinforcement Learning (RL) research is based on comparing new methods to baselines to assess progress (Patterson et al., 2023). This process requires the availability of the data associated with these baselines (Raffin et al., 2021) or, alternatively, the ability to replicate them and generate the data oneself (Raffin, 2020). In addition, reproducible results allow the methods to be compared with new benchmarks and to identify the areas in which the methods excel and those in which they are likely to fail, thus providing avenues for future research.

In practice, the RL research community faces complex challenges in comparing new methods with reference data. The unavailability of reference data requires researchers to reproduce experiments, which is difficult due to insufficient source code documentation and evolving software dependencies.

Implementation details, as highlighted in past research, can significantly impact results (Henderson et al., 2018; Huang et al., 2022). Moreover, limited computing resources play a crucial role, hindering the reproduction process and affecting researchers without substantial access.

The lack of standardized metrics and benchmarks across studies not only impedes comparison but also results in a substantial waste of time and resources. To address these issues, the RL community must establish rigorous reproducibility standards, ensuring replicability and comparability across studies. Transparent sharing of data, code, and experimental details, along with the adoption of consistent metrics and benchmarks, would collectively enhance the evaluation and progression of RL research, ultimately accelerating advancements in the field.

ORLB presents a rich collection of tracked RL experiments and aims to set a new standard by providing a diverse training dataset. This initiative prioritizes the use of existing data over re-running baselines, emphasizing reproducibility and transparency. Our contributions are:

* **Extensive dataset:** Offers a large, diverse collection of tracked RL experiments.
* **Standardization:** Establishes a new norm by encouraging reliance on existing data, reducing the need for re-running baselines.
* **Comprehensive metrics:** Includes diverse tracked metrics for method-specific and system evaluation, in addition to episodic return.
* **Reproducibility:** Emphasizes clear instructions and fixed dependencies, ensuring easy experiment replication.
* **Resource for research:** Serves as a valuable and collaborative resource for RL research.
* **Facilitating exploration:** Enables reliable exploration and assessment of new and exisiting RL methods.

## 2 Comprehensive overview of ORLB: content, methodology, tools, and applications

This section provides a detailed exploration of the contents of ORLB, including its diverse set of libraries and environments, and the metrics it contains. We also look at the practical aspects of using ORLB, highlighting its ability to ensure accurate reproducibility and facilitate the creation of data visualizations thanks to its CLI.

### Content

ORLB data is stored and shared with Weights and Biases (Biewald, 2020). The data is contained in a common entity named openrlbenchmark. Runs are divided into several _projects_. A project can correspond to a library, but it can also correspond to a set of more specific runs, such as envpool-cleanrl in which we find CleanRL runs (Huang et al., 2022) launched with the EnvPool

Figure 1: Example of learning curves obtained with Open RL Benchmark. These compare the episodic returns obtained by different implementations of PPO and DQN on three Atari games.

implementation of environments (Weng et al., 2022). A project can also correspond to a reference implementation, such as TD3 (project sfujim-TD3) or Phasic Policy Gradient (Cobbe et al., 2021) (project phasic-policy-gradient). ORLB also includes reports, which are interactive documents designed to enhance the visualization of selected representations. These reports provide a more user-friendly format for practitioners to share, discuss, and analyze experimental results, even across different projects. Figure 2 shows a preview of one such report.

At the time of writing, ORLB contains nearly 25,000 runs, for a total of 72,000 hours (more than 8 years) of tracking. In the following paragraphs, we present the libraries and environments for which runs are available in ORLB, as well as the metrics tracked.

LibrariesORLB contains runs for several reference RL libraries. These libraries are: abcdRL (Zhao, 2022), Acme (Hoffman et al., 2020), Cleanba (Huang et al., 2023), CleanRL (Huang et al., 2022), jaxrl (Kostrikov, 2021), moolib (Mella et al., 2022), MORL-Baselines (Felten et al., 2023), OpenAI Baselines (Dhariwal et al., 2017), rlgames (Makoviichuk and Makoviychuk, 2021) Stable Baselines3 (Raffin et al., 2021; Raffin, 2020) Stable Baselines Jax (Raffin et al., 2021) and TorchBeast (Kuttler et al., 2019).

EnvironmentsThe runs contained in ORLB cover a wide range of classic environments. They include Atari (Bellemare et al., 2013; Machado et al., 2018), Classic control (Brockman et al., 2016), Box2d (Brockman et al., 2016) and MuJoCo (Todorov et al., 2012) as part of either Gym (Brockman et al., 2016) or Gymnasium (Towers et al., 2023) or EnvPool (Weng et al., 2022). They also include Bullet (Coumans and Bai, 2016), Procgen Benchmark (Cobbe et al., 2020), Fetch environments (Plappert et al., 2018), PandaGym (Gallouedec et al., 2021), highway-env (Leurent, 2018), Minigrid (Chevalier-Boisvert et al., 2023) and MO-Gymnasium (Alegre et al., 2022).

Tracked metricsMetrics are recorded throughout the learning process, consistently linked with a global step indicating the number of interactions with the environment, and an absolute time, which allows to compute the duration of a run. We categorize these metrics into four distinct groups:

* **Training-related metrics:** These are general metrics related to RL learning. This category contains, for example, the average returns obtained, the episode length or the number of collected samples per second.

Figure 2: An example of a report on the Weights and Biases platform, dealing with the contribution of QDagger (Agarwal et al., 2022), and using data from ORLB. The URL to access the report is https://wandb.ai/openrlbenchmark/openrlbenchmark/reports/Atari-CleanRL-s-Qdaggerger–\(\_\)VmlldzoONTG10DY5.

* **Method-specific metrics:** These are losses and measures of key internal values of the methods. For PPO, for example, this category includes the value loss, the policy loss, the entropy or the approximate KL divergence.
* **Evolving configuration parameters:** These are configuration values that change during the learning process. This category includes, for example, the learning rate when there is decay, or the exploration rate (\(\)) in the Deep Q-Network (DQN) (Mnih et al., 2013).
* **System metrics:** These are metrics related to system components. These could be GPU memory usage, its power consumption, its temperature, system and process memory usage, CPU usage or even network traffic.

The specific metrics available may vary from one library to another. In addition, even where the metrics are technically similar, the terminology or key used to record them may vary from one library to another. Users are advised to consult the documentation specific to each library for precise information on these measures.

### Everything you need for perfect repeatability

Reproducing experimental results in computational research, as discussed in Section 4.3, is often challenging due to evolving codebases, incomplete hyperparameter listings, version discrepancies, and compatibility issues. Our approach aims to enhance reproducibility by ensuring users can exactly replicate benchmark results. Each experiment includes a complete configuration with all hyperparameters, frozen versions of dependencies, and the exact command, including the necessary random seed, for systematic reproducibility. As a example, CleanRL (Huang et al., 2022b) introduces a unique utility that streamlines the process of experiment replication (see Figure 3). This tool produces the command lines to set up a Python environment with the necessary dependencies, download the run file, and the precise command required for the experiment reproduction. Such an approach to reproduction facilitates research and makes it possible to study in depth unusual phenomena, or cases of rupture2, in learning processes, which are generally ignored in the results presented, either because they are deliberately left out or because they are erased by the averaging process.

### The CLI for generating figures in one command line

ORLB offers convenient access to raw data from RL libraries on standard environments. It includes a feature for easily extracting and visualizing data in a paper-friendly format, streamlining the process of filtering and extracting relevant runs and metrics for research papers through a single command. The CLI is a powerful tool for generating most metrics-related figures for RL research and notably, all figures in this document were generated using the CLI. The data in ORLB can also be accessed by custom scripts, as detailed in Appendix A.2. Specifically, the CLI integrated into ORLB provides users with the flexibility to:

Figure 3: CleanRL’s module reproduce allows the user to generate, from an ORLB run reference, the exact command suite for an identical reproduction of the run.

* Specify algorithms' implementations (from which library) along with their corresponding git commit or tag;
* Choose target environments for analysis;
* Define the metrics of interest;
* Opt for the additional generation of metrics and plots using RLiable (Agarwal et al., 2021).

Concrete example usage of the CLI and resulting plots are available in Appendix A.1.

## 3 ORLB in action: an insight into case studies

ORLB offers a powerful tool for researchers to evaluate and compare different RL algorithms. In this section, we will explore two case studies that showcase its benefits. First, we propose to investigate the effect of using TD(\(\)) for value estimation in PPO (Schulman et al., 2017) versus using Monte Carlo (MC). This simple study illustrates the use of ORLB through a classic research question. Moreover, to the best of our knowledge, this question has never been studied in the literature. We then show how ORLB is used to demonstrate the speedup and variance reduction of a new IMPALA implementation proposed by Huang et al. (2023). By using ORLB, we can save time and resources while ensuring consistent and reproducible comparisons. These case studies highlight the role of the benchmark in providing insights that can advance the field of RL research.

### Easily assess the contribution of TD(\(\)) for value estimation in PPO

In the first case study, we show how ORLB can be used to easily compare the performance of different methods for estimating the value function in PPO (Schulman et al., 2017), one of the many implementation details of this algorithm (Huang et al., 2022a). Specifically, we compare the commonly used Temporal Difference (TD)(\(\)) estimate to the Monte-Carlo (MC) estimate.

PPO typically employs Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to update the actor. The advantage estimate is expressed as follows:

\[A_{t}^{(,)}=_{l=0}^{N-1}()^{l} _{t+l}^{V}\] (1)

where \(\) adjusts the bias-variance tradeoff and \(_{t+l}^{V}=R_{t+l}+(S_{t+l+1})-(S_{t+l})\). The target return for critic optimization is estimated with TD(\(\)) as follows:

\[G_{t}^{}=(1-)_{n=1}^{}^{n-1}G_{t:t+n}\] (2)

where \(G_{t:t+n}=_{k=0}^{n-1}^{k}R_{t+k+1}+^{n}V(S_{t+n})\) is the \(n\)-steps return. In practice, the target return for updating the critic is computed from the GAE value, by adding the minibatch return, a detail usually overlooked by practitioners (Huang et al., 2022a, point 5). While previous studies (Patterson et al., 2023) have shown the joint benefit of GAE and over MC estimates for actor and critic, we focus on the value function alone. To isolate the influence of the value function estimation, we vary the method used for the value function and keep GAE for advantage estimation.

The first step is to identify the reference runs in ORLB. Since PPO is a well-known baseline, there are many runs available; we decided to use those from Stable Baselines3 for this example. We then retrieve the exact source code and command used to generate the runs - thanks to the pinned dependencies that come with them - and make the necessary changes to the source code. For each selected environment, we start three learning runs using the same command as the one we retrieved. The runs are saved in a dedicated project3. For fast and user-friendly rendering of the results, we 

[MISSING_PAGE_FAIL:6]

the IMPALA results. However, these shared results are limited to the paper's presented curves, which provide a smoothed measure of episodic return as a function of interaction steps on a specific set of Atari tasks. It is worth noting that these tasks are not an exact match for the widely recognized Atari 57, and the raw data used to generate these curves is unavailable.

Recognizing the lack of raw data for existing IMPALA implementations, the authors reproduced the experiments, tracked the runs and integrated them into ORLB. As a reminder, these logged data include not only the return curves, but also the system configurations and temporal data, which are crucial to support the Cleanba authors' optimization claim. Comparable experiments have been run, tracked and shared on ORLB with the proposed Cleanba implementation.

Using ORLB CLI, the authors generated several figures. In Figure 6, taken from (Huang et al., 2023), the authors show that the results in terms of sample efficiency compare favorably with the baselines, and that for the same system configuration, convergence was temporally faster with the proposed implementation, thus proving claims (1) and (2). Figure 7 demonstrates that Cleanba variants maintain consistent learning curves across different hardware configurations. Conversely, moolib's IMPALA shows marked variability in similar settings, despite identical hyperparameters, confirming the authors' third claim.

## 4 Current practices in RL: data reporting, sharing and reproducibility

Many new methods have emerged in recent years, with some becoming standard baselines, but current practices in the field make it challenging to interpret, compare, and replicate study results. In this section, we highlight the inconsistent presentation of results, focusing on learning curves as an example. This inconsistency can hinder interpretation and lead to incorrect conclusions. We also note the insufficient availability of learning data, despite some positive efforts, and examine challenges related to method reproducibility.

Figure 6: Median human-normalized scores with 95% stratified bootstrap CIs of Cleanba (Huang et al., 2023) variants compared with moolib (Mella et al., 2022) and monobeast (Küttler et al., 2019). The experiments were conducted on 57 Atari games (Bellemare et al., 2013). The data used to generate the figure comes from ORLB, and the figure was generated with a single command from ORLB’s CLI. Figure from (Huang et al., 2023).

Figure 7: Aggregated normalized human scores with stratified 95% bootstrap CIs, showing that unlike moolib (Mella et al., 2022), Cleanba (Huang et al., 2023) variants have more predictable learning curves (using the same hyperparameters) across different hardware configurations. Figure from (Huang et al., 2023).

### Analyzing learning curve practices

Plotting learning curves is a common way to show an agent's performance over learning. We closely examine the components of learning curves and the choices made by key publications. We find a lack of uniformity, with presentation choices rarely explained and sometimes not explicitly stated.

AxisTypically, the \(y\) axis measures either the return acquired during data collection or evaluation. Some older papers, like (Schulman et al., 2015; Mnih et al., 2016; Schulman et al., 2017), fail to specify the metric, using the vague term _learning curve_. The first approach sums the rewards collected during agent rollout (Dabney et al., 2018; Burda et al., 2019). The second approach suspends training, averaging the agent's return over episodes, deactivating exploration elements (Fujimoto et al., 2018; Haarnoja et al., 2018; Hessel et al., 2018; Janner et al., 2019; Badia et al., 2020; Ecoffet et al., 2021; Chen et al., 2021). This method is prevalent and provides a more precise evaluation. Regarding the \(x\) axis, while older baselines (Schulman et al., 2015; Mnih et al., 2016) use policy updates and learning epochs, the norm is to use interaction counts with the environment. In Atari environments, it is often the number of frames, adjusting for frame skipping to match human interaction frequency.

Shaded areaData variability is typically shown with a shaded area, but its definition varies across studies. Commonly, it represents the standard deviation (Chen et al., 2021; Janner et al., 2019) and less commonly half the standard deviation (Fujimoto et al., 2018). Haarnoja et al. (2018) uses a min-max representation to include outliers, covering the entire observed range. This method offers a comprehensive view but amplifies outliers' impact with more runs. Ecoffet et al. (2021) adopts a probabilistic approach, showing a 95% bootstrap confidence interval around the mean, ensuring statistical confidence. Unfortunately, Schulman et al. (2015, 2017); Mnih et al. (2016); Dabney et al. (2018); Badia et al. (2020) omit statistical details or even the shaded area, introducing uncertainty in data variability interpretation, as seen in (Hessel et al., 2018).

Normalization and aggregationPerformance aggregation assesses method results across various tasks and domains, indicating their generality and robustness. Outside the Atari context, aggregation practices are uncommon due to the lack of a universal normalization standard. Without a widely accepted normalization strategy, scores are typically not aggregated, or if they are, it relies on a min-max approach lacking absolute significance and unsuitable for comparisons. Early Atari research did not use normalization or aggregate results (Mnih et al., 2013). There has been a shift towards normalizing against human performance, though this has weaknesses and may not reflect true agent mastery (Toromanoff et al., 2019). Aggregation methods vary: the mean is common but influenced by outliers, leading some studies to prefer the more robust median, as in (Hessel et al., 2018). Many papers now report both mean and median results (Dabney et al., 2018; Hafner et al., 2023; Badia et al., 2020). Recent approaches, like using the Interquartile Mean (IQR), provide a more accurate performance representation across diverse games (Lee et al., 2022), as suggested by Agarwal et al. (2021).

### Spectrum of data sharing practices

While the mentioned studies often have reference implementations (see Section 4.3), the sharing of training data typically extends only to the curves presented in their articles. This necessitates reliance on libraries that replicate these methods, offering benchmarks with varying levels of completeness. Several widely-used libraries in the field provide high-level summaries or graphical representations without including raw data (e.g., Tensorforce (Kuhnle et al., 2017), Garage (garage contributors, 2019), ACME (Hoffman et al., 2020), MushroomRL (D'Eramo et al., 2021), ChainerRL (Fujita et al., 2021), and TorchRL (Bou et al., 2023)). Spinning Up (Achiam, 2018) offers partial data accessibility, providing benchmark curves but withholding raw data. TF-Agent (Guadarrama et al., 2018) is slightly better, offering experiment tracking with links to TensorBoard.dev, though its future is uncertain due to service closure. Tianshou (Weng et al., 2022) provides individual run reward data for Atari and average rewards for MuJoCo, with more detailed MuJoCo data available via a GoogleDrive link, but it is not widely promoted. RLLib (Liang et al., 2018) maintains an intermediate stance in data sharing, hosting run data in a dedicated repository. However, this data is specific to select experiments and often presented in non-standard, undocumented formats, complicating its use. Leading effective data-sharing platforms include Dopamine (Castro et al., 2018) and Sample Factory (Petrenko et al., 2020). Dopamine consistently provides accessible raw evaluation data for various seeds and visualizations, along with trained agents on Google Cloud. Sample Factory offers comprehensive data via Weights and Biases (Biewald, 2020) and a selection of pre-trained agents on the Hugging Face Hub, enhancing reproducibility and collaborative research efforts.

### Review on reproducibility

The literature shows variations in these practices. Some older publications like (Schulman et al., 2015, 2017; Bellemare et al., 2013; Mnih et al., 2016; Hessel et al., 2018) and even recent ones like (Reed et al., 2022) lack a codebase but provide detailed descriptions for replication6. However, challenges arise because certain hyperparameters, important but often unreported, can significantly affect performance (Andrychowicz et al., 2020). In addition, implementation choices have proven to be critical (Henderson et al., 2018; Huang et al., 2023, 2022a; Engstrom et al., 2020), complicating the distinction between implementation-based improvements and methodological advances.

Recognizing these challenges, the RL community is advocating for higher standards. NeurIPS, for instance, has been requesting a reproduction checklist since 2019 (Pineau et al., 2021). Recent efforts focus on systematic sharing of source code to promote reproducibility. However, codebases are often left unmaintained post-publication (with rare exceptions (Fujimoto et al., 2018)), creating complexity for users dealing with various dependencies and unsolved issues. To address these challenges, libraries have aggregated multiple baseline implementations (see Section 2.1), aiming to match reported paper performance. However, long-term sustainability remains a concern. While these libraries enhance reproducibility, in-depth repeatability is still rare.

## 5 Discussion and conclusion

Reproducing results in RL research is often difficult due to limited access to data and code, as well as the impact of minor implementation variations on performance. Researchers typically rely on imprecise comparisons with paper figures, making the reproduction process time-consuming and challenging. To address these issues, we introduce ORLB, a large collection of tracked experiments spanning various algorithms, libraries and benchmarks. ORLB records all relevant metrics and data points, offering detailed resources for precise reproduction. This tool facilitates access to comprehensive datasets, simplifies the extraction of valuable information, enables metric comparisons, and provides a CLI for easier data access and visualization. As a dynamic resource, ORLB is regularly updated by both its maintainers and the user community, gradually improving the reliability of the available results.

Despite its strengths, ORLB faces challenges in user-friendliness that need to be addressed. Inconsistencies between libraries in evaluation strategies and terminology can make it difficult for users. Scaling community engagement becomes a challenge with more members, libraries, and runs. The lack of Git-like version tracking for runs adds to these limitations.

ORLB is a major step forward in addressing the needs of RL research. It offers a comprehensive, accessible, and collaborative experiment database, enabling precise comparisons and analysis. It improves data access and promotes a deeper understanding of algorithmic performance. While challenges remain, ORLB has the potential to raise the standard of RL research.

## Affiliations

\({}^{1}\)Hugging Face

\({}^{2}\)Drexel University

\({}^{3}\)Univ. Lyon, Centrale Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR 5205

\({}^{4}\)SnT, University of Luxembourg

\({}^{5}\)German Aerospace Center (DLR) RMC, Wessling, Germany

\({}^{6}\)Graduate School of System Informatics, Kobe University, Hyogo, Japan

\({}^{7}\)School of Computer Science and Technology, University of Chinese Academy of Sciences

\({}^{8}\)Chengdu Institute of Computer Applications, Chinese Academy of Sciences

\({}^{9}\)University of Maryland, College Park

\({}^{10}\)NVIDIA

\({}^{11}\)Snap Inc.

\({}^{12}\)School of Computer Science, McGill University

\({}^{13}\)Polytech Montellier DO

\({}^{14}\)Zhejiang University

\({}^{15}\)Department of Computer Science, Purdue University

\({}^{16}\)Work done while at Cohere

\({}^{17}\)Chinese University of Hong Kong, Shenzhen

\({}^{18}\)University College London

\({}^{19}\)Agency for Science, Technology and Research

\({}^{20}\)Faculty of Computer Science, University of Vienna, Vienna, Austria

\({}^{21}\)UniVie Doctoral School Computer Science, University of Vienna

\({}^{22}\)Vidyasirimedhi Institute of Science and Technology (VISTEC)

\({}^{23}\)University of Southampton

\({}^{24}\)Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIS4AL

\({}^{25}\)Saint-Gobain Research Paris

\({}^{26}\)International Institute of Information Technology, Hyderabad, India

\({}^{27}\)Alcrowd SA

\({}^{28}\)Valeo Driving Assistance Research

\({}^{29}\)New York University

\({}^{30}\)Sea AI Lab

\({}^{31}\)Institute of Informatics, Federal University of Rio Grande do Sul

\({}^{32}\)AIRI

\({}^{33}\)Department of Automation, Tsinghua University

\({}^{34}\)University of Basel

\({}^{35}\)University of Michigan

\({}^{36}\)UC Berkley