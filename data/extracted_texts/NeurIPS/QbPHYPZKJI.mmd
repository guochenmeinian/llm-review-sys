# Learning Distributions on Manifolds

with Free-Form Flows

 Peter Sorrenson*, Felix Draxler*, Armand Rousselot*, Sander Hummerich, Ullrich Kothe

Computer Vision and Learning Lab, Heidelberg University

*equal contribution, firstname.lastname@iwr.uni-heidelberg.de

###### Abstract

We propose Manifold Free-Form Flows (M-FFF), a simple new generative model for data on manifolds. The existing approaches to learning a distribution on arbitrary manifolds are expensive at inference time, since sampling requires solving a differential equation. Our method overcomes this limitation by sampling in a single function evaluation. The key innovation is to optimize a neural network via maximum likelihood on the manifold, possible by adapting the free-form flow framework to Riemannian manifolds. M-FFF is straightforwardly adapted to any manifold with a known projection. It consistently matches or outperforms previous single-step methods specialized to specific manifolds. It is typically two orders of magnitude faster than multi-step methods based on diffusion or flow matching, achieving better likelihoods in several experiments. We provide our code at https://github.com/vislearn/FFF.

## 1 Introduction

Generative models have achieved remarkable success in various domains such as image synthesis (Rombach et al., 2022), natural language processing (Brown et al., 2020), scientific applications (Noe et al., 2019) and more. However, the approaches are not directly applicable when dealing with data inherently structured in non-Euclidean spaces, which is common in fields such as the natural sciences, computer vision, and robotics. Examples include earth science data on a sphere, the orientation of real-world objects given as a rotation matrix in \(SO(3)\), or data on special geometries modeled by meshes or signed distance functions. Representing such data naively using internal coordinates, such as angles, can lead to topological issues, causing discontinuities or artifacts.

Luckily, many generative models can be adapted to handle data on arbitrary manifolds. However, the predominant methods compatible with arbitrary Riemannian manifolds involve solving differential equations--stochastic (SDEs) or ordinary (ODEs)--for sampling and density estimation (Rozen et al., 2021; Mathieu and Nickel, 2020; Huang et al., 2022; De Bortoli et al., 2022; Chen and Lipman, 2024). These methods are computationally intensive due to the need for numerous function evaluations during integration, slowing down inference.

To address these challenges, we introduce a novel approach for modeling distributions on arbitrary Riemannian manifolds that circumvents the computational burden of previous methods. This is achieved by using a single feed-forward neural network on an embedding space as a generator, with outputs projected to the manifold (Fig. 1). We learn this network as a normalizing flow, facilitated by generalizing the free-form flow framework (Draxler et al., 2024; Sorrenson et al., 2024) to Riemannian manifolds. The core innovation is estimating the gradient of the negative log-likelihood within the tangent space of the manifold.

In particular, we make the following contributions:* We extend free-form flows to Riemannian manifolds, yielding manifold free-form flows (M-FFF) in Section 4.
* M-FFF can easily be adapted to arbitrary Riemannian manifolds, requiring only a projection function from an embedding space.
* It only relies on a single function evaluation during training and sampling, speeding up inference over multi-step methods typically by two orders of magnitude.
* M-FFF consistently matches or outperforms previous single-step methods on several benchmarks on spheres, tori, rotation matrices, hyperbolic space and curved surfaces (see Fig. 1 and Section 5). In addition, it is consistently faster than multi-step methods by two orders of magnitude, while also outperforming them in terms of likelihood in several cases.

Together, manifold free-form flows offer a novel and efficient approach for learning distributions on manifolds, applicable to any Riemannian manifold with a known embedding and projection.

## 2 Related work

Existing work on learning distributions on manifolds can be broadly categorized as follows: (i) leveraging Euclidean generative models; (ii) building specialized architectures that respect one particular kind of geometry; and (iii) learning a continuous time process on the manifold. We compare our method to these approaches in Table 1 and give additional detail below.

**Euclidean generative models.** One approach maps the \(n\)-dimensional manifold to \(^{n}\) and learns the resulting distribution (Gemici et al., 2016). Another approach generalizes the reparameterization trick to Lie groups by sampling on the Lie algebra which can be parameterized in Euclidean space (Falorsi et al., 2019). These approaches come with the downside that a Euclidean representation

    & Respects topology & Single step sampling & Arbitrary manifolds \\  Euclidean & ✗ & ✓ & ✓ \\ Specialized & ✓ & ✓ & ✗ \\ Continuous time & ✓ & ✗ & ✓ \\ M-FFF (ours) & ✓ & ✓ & ✓ \\   

Table 1: Feature comparison of generative models on manifolds. We give a “✓” if any method in a category meets this requirement.

Figure 1: Manifold Free-Form Flows (M-FFF) learn generative models on a variety of manifolds. _(Left)_ The learned distributions _(colored surface)_ accurately match the test points _(black dots)_. _(Right)_ We parameterize M-FFF using a neural network in an embedding space, whose outputs are projected to the manifold. This enables simulation-free training and inference, and naturally respects the corresponding geometry, yielding fast sampling and continuous distributions regardless of the manifold.

may not respect the geometry of the manifold sufficiently, e.g. mapping the earth to a plane causes discontinuities at the boundaries. This can be overcome by learning distributions on overlapping charts that together span the full manifold (Kalatzis et al., 2021). An orthogonal solution is to embed the data and add noise to it in the off-manifold directions, so that the distribution can be learnt directly in an embedding space \(^{m}\)(Brofos et al., 2021); this only gives access to an ELBO instead of the exact density. Our method also works in the embedding space so that it respects the geometry of the manifold, but directly optimizes the likelihood on the manifold.

**Specialized architectures** take advantage of the specific geometry of a certain kind of manifold to come up with special coupling blocks for building normalizing flows such as \(SO(3)\)(Liu et al., 2023), \(SU(d),U(d)\)(Boyda et al., 2021; Kanwar et al., 2020); hyperbolic space (Bose et al., 2020); tori and spheres (Rezende et al., 2020). Manifold free-form flows are not restricted to one particular manifold, but can be easily applied to any manifold for which an embedding and a projection to the manifold is known. As such, our model is an alternative to all of the above specialized architectures.

**Continuous time models** build a generative model based on parameterizing an ODE or SDE on any Riemannian manifold, meaning that they specify the (stochastic) differential equation in the tangent space (Rozen et al., 2021; Falorsi, 2021; Falorsi and Forre, 2020; Huang et al., 2022; Mathieu and Nickel, 2020; De Bortoli et al., 2022; Chen and Lipman, 2024; Lou et al., 2020; Ben-Hamu et al., 2022). These methods come with the disadvantage that sampling and density evaluation integrates the ODE or SDE, requiring many function evaluations. Our manifold free-form flows do not require repeatedly evaluating the model, a single function call followed by a projection is sufficient.

At its core, our method generalizes the recently introduced free-form flow (FFF) framework (Draxler et al., 2024) based on an estimator for the gradient of the change of variables formula (Sorrenson et al., 2024). We give more details in Section 3.1.

## 3 Background

In this section, we provide the background for our method: We present an introduction to free-form flows and Riemannian manifolds.

### Free-form flows

Free-form flows are a class of generative models that generalize normalizing flows to work with arbitrary feed-forward neural network architectures (Draxler et al., 2024).

Euclidean change-of-variablesTraditionally, normalizing flows are based on invertible neural networks (INNs, see Kobyzev et al. (2021) for an overview) that learn an invertible transformation \(z=f_{}(x)\) mapping from data \(x^{n}\) to latent codes \(z^{n}\). This gives an explicitly parameterized probability density \(p_{}(x)\) via the change-of-variables:

\[ p_{}(x)= p_{Z}(f_{}(x))+|f^{}_{}(x)|,\] (1)

where \(f^{}_{}(x)^{n n}\) is the Jacobian matrix of \(f_{}(x)\) with respect to \(x\), evaluated at \(x\); \(|f^{}_{}(x)|\) is its absolute determinant. The distribution of latent codes \(p_{Z}(z)\) is chosen such that the log-density is easy to evaluate and it is easy to sample from, such as a standard normal. Normalizing flows can be trained by minimizing the negative log-likelihood over the training data distribution:

\[_{}_{}=_{}_{p_{ }(x)}[- p_{}(x)].\] (2)

This is equivalent to minimizing the Kullback-Leibler-divergence between the true data distribution and the parameterized distribution \((p_{}\|p_{})\). Sampling from the model is achieved by pushing samples from the latent distribution \(z p_{Z}\) through the inverse of the learned function: \(x=f^{-1}_{}(z) p_{}\).

Euclidean gradient estimatorNaively computing the volume change \(|f^{}_{}(x)|\) in Eq. (1) is expensive since it contains the full Jacobian matrix \(f^{}_{}(x)^{n n}\), requiring \((n)\) automatic differentiation steps to compute. Normalizing flow architectures usually avoid this expensive computation by further restricting the architecture to allow fast computation. Luckily, even if such a shortcut is not available, its _gradient_, which is all we need for training, can still be efficiently estimated as follows:

**Theorem 1** (Volume change gradient estimator, Draxler et al. (2024)).: _Let \(f_{}:^{n}^{n}\) be a diffeomorphism. Let \(v^{n}\) be a random variable with zero mean and unit covariance. Then, the derivative of the volume change has the following trace expression, where \(z=f_{}(x)\):_

\[_{}|f^{}_{}(x)| =((_{}f^{}_{}(x))f^{ -1}_{}(z))\] (3) \[=_{v}[v^{T}(_{}f^{}_{}(x) ){f^{-1}_{}}^{}(z)v].\] (4)

Replacing the expected value over \(v\) by a single sample, and using a stop-grad (SG) operation, Theorem 1 allows us to compute a term whose gradient is an unbiased estimator for the gradient of Eq. (2):

\[_{}_{}(x)_{}(- p_{Z}( z)-v^{T}f^{}_{}(x)({f^{-1}_{}}^{}(z)v)).\] (5)

Comparing Eqs. (1) and (5) reveals that \(|f^{}_{}(x)|\) is replaced by a single vector-Jacobian \(v^{T}f^{}_{}(x)\) and a Jacobian-vector product \({f^{-1}_{}}^{}(z)v\), each of which require only one automatic differentiation operation. Note that while the gradient estimate is unbiased, computing the term in the brackets is not informative about \(_{}\). Thus, for density estimation at inference, Eq. (1) is explicitly evaluated using the full Jacobian.

Free-form architecturesThe central idea of free-form flows is to soften the restriction that the learned model be invertible. Instead, they learn two separate networks, an encoder \(f_{}\) and a decoder \(g_{}\) coupled by a reconstruction loss, circumventing the need for an invertible neural network \(f_{}\):

\[_{}=_{p_{}}(x)}[\|g_{} (f_{}(x))-x\|^{2}].\] (6)

Together, this gives the loss of free-form flows with \(\), the reconstruction weight as a hyperparameter:

\[_{}=_{}+_{}.\] (7)

This allows replacing constrained invertible architectures with free-form neural networks. Since \(f_{}\) is not restricted to efficiently compute the volume change, free-form flows use Eq. (5) to compute the gradient of \(_{}\). To compute Eq. (5), free-form flows approximate \({f^{-1}_{}}^{}(z)\) (which is not tractable) by \(g^{}_{}(z)\) during training:

\[_{}_{}(x)_{}(- p_{Z}( z)-v^{T}f^{}_{}(x)(g^{}_{}(z)v)).\] (8)

For density estimation at inference, Draxler et al. (2024) recommend using the explicit decoder Jacobian for the volume change.

### Riemannian manifolds

A manifold is a fundamental concept in mathematics, providing a framework for describing and analyzing spaces that locally resemble Euclidean space, but may have different global structure. For example, a small region on a sphere is similar to the Euclidean plane, but walking in a straight line on the sphere in any direction will return back to the starting point, unlike on a plane.

Mathematically, an \(n\)-dimensional manifold, denoted as \(\), is a space where every point has a neighborhood that is topologically equivalent to \(^{n}\). A Riemannian manifold \((,G)\) extends the concept of a manifold by adding a Riemannian metric \(G\) which introduces a notion of distances and angles. At each point \(x\) on the manifold, there is an associated tangent space \(_{x}\) which is an \(n\)-dimensional Euclidean space, characterizing the directions in which you can travel and still stay on the manifold. The metric \(G\) acts in this space, defining an inner product between vectors. From this inner product, we can compute the length of paths along the manifold, distances between points as well as volumes (see next section).

In this paper, we consider Riemannian manifolds globally embedded into an \(m\)-dimensional Euclidean space \(^{m}\), with \(n m\). Embedding means that we represent a point on the manifold \(x\) as a vector in \(^{m}\) confined to an \(n\)-dimensional subspace; we write \(x^{m}\) and denote by \(:^{m}\) a projection from the embedding space to the manifold. A global embedding is a smooth, injective mapping of the entire manifold into \(^{m}\), its smoothness preserving the topology.

In most cases, we work with, but are not limited to, isometrically embedded manifolds, meaning that the metric is inherited from the ambient Euclidean space. Intuitively, this means that the length of apath on the manifold is just the length of the path in the embedding space. We note that due to the Nash embedding theorem (Nash, 1956), every Riemannian manifold has a smooth isometric embedding into Euclidean space of some finite dimension, so in this sense using isometric embeddings is not a limitation. Nevertheless, for some manifolds (especially with negative curvature, e.g. hyperbolic space) there may not be a sensible isometric embedding.

## 4 Manifold free-form flows

The free-form flow (FFF) framework allows training any pair of parameterized encoder \(f_{}(x)\) and decoder \(g_{}(z)\) as a generative model, see Section 3.1. In this section, we demonstrate how to generalize the steps in Section 3.1 to arbitrary Riemannian manifolds. Note that for simplicity, we choose the same manifold in data and latent spaces, i.e. \(_{X}=_{Z}=\), but the method readily applies to \(_{X}_{Z}\) or \(G_{X} G_{Z}\) as long as they are topologically compatible, like a sphere and a closed 3D surface without holes. The detailed derivations in the appendix consider this generalization.

Manifold change of variablesThe volume change on manifolds generalizes the Euclidean variant in Eq. (1) by (a) considering the change of volume in the tangent space and (b) accounting for volume change due to changes in the metric:

**Theorem 2** (Manifold change of variables).: _Let \((,G)\) be a \(n\)-dimensional Riemannian manifold embedded in \(^{m}\), i.e., \(^{m}\). Let \(p_{X}\) be a probability distribution on \(\) and let \(f:\) be a diffeomorphism. Let \(p_{Z}\) be the pushforward of \(p_{X}\) under \(f\) (i.e., if \(p_{X}\) is the probability density of \(X\), then \(p_{Z}\) is the probability density of \(f(X)\))._

_Let \(x\). Define \(Q^{m n}\) as an orthonormal basis for \(_{x}\) and \(R^{m n}\) as an orthonormal basis for \(_{f(x)}\)._

_Then, the probability densities \(p_{X}\) and \(p_{Z}\) are related under the change of variables \(x f(x)\) by the following equation:_

\[ p_{X}(x)= p_{Z}(f(x))+|R^{T}f^{}(x)Q|+ G(f(x))R|}{|Q^{T}G(x)Q|}.\] (9)

_where \(Q\) and \(R\) depend on \(x\) and \(f(x)\), respectively, although this dependency is omitted for brevity._

To give an intuition for this result, Fig. 2 shows how the volume change is computed for an isometrically embedded manifold, that is \(G=I\) so that \(|R^{T}GR|=|Q^{T}GQ|=1\). This simplifies Eq. (9) to:

\[ p_{X}(x)= p_{Z}(f(x))+|R^{T}f^{}(x)Q|.\] (10)

This is very similar to the familiar change of variables formula in the Euclidean case in Eq. (1), the only difference being that the determinant is evaluated on the \(n n\) projection of \(f^{}(x)\) into the tangent spaces. These projections are necessary as the Jacobian of \(f\) is singular in the embedding space, since its action is restricted to the local tangent spaces. See the full proof in Appendix A.1.

Manifold gradient estimatorWe now generalize the volume change gradient estimator in Theorem 1 to an invertible function on the manifold \(f_{}:\). We find that taking the gradient of the manifold change of variables in Eq. (9) results in essentially the same computation as in the Euclidean case, but the trace in is now evaluated in the local tangent space:

**Theorem 3**.: _Under the assumptions of Theorem 2 with \(f=f_{}\). Let \(v^{m}\) be a random variable with zero mean and covariance \(RR^{T}\). Then, the derivative of the change of variables term has the

   Manifold & Dimension \(n\) & Embedding & Projection \\  Generic & \((^{}((x)))\) & \(\{x^{m}:(x)=x\}\) & \(x(x)\) \\  Rotations \(SO(d)\) & \((d-1)d/2\) & \(\{Q^{d d}:QQ^{T}=I, Q=1\}\) & \(R*{arg\,min}_{ SO(d)}\|Q-R\|_{F}\); see Eq. (102) \\ Sphere \(^{n}\) & \(n\) & \(\{x^{n+1}:\|x\|=1\}\) & \(x x/\|x\|\) \\ Torus \(^{n}=(^{1})^{n}\) & \(n\) & \(\{X^{n 2}:\|X_{i}\|=1i=1...n\}\) & \(X_{i} X_{i}/\|X_{i}\|\) for \(i=1...n\) \\ Hyperbolic \(}^{n}\) & \(n\) & \(\{x^{n}:\|x\|<1\}\) & \(x x\{1,(1-)/\|x\|\}\) \\   

Table 2: Manifolds, a global embedding and corresponding projections considered in this paper.

following trace expression, where \(z=f_{}(x)\):_

\[_{}|R^{T}f^{}_{}(x)Q| =(R^{T}(_{}f^{}_{}(x)){f_ {}^{-1}}^{}(z)R)\] (11) \[=_{v}[v^{T}(_{}f^{}_{}(x)){ f_{}^{-1}}^{}(z)v].\] (12)

This shows that the adaptation of free-form flows for an invertible function \(f\) to isometrically embedded manifolds is remarkably simple (see full proof in Appendix A.2; if the manifold is not isometrically embedded, add the corresponding term in Eq. (9)):

\[_{}_{}=_{} _{x,v}[- p_{Z}(z)-v^{T}f^{}_{}(x)[{f _{}^{-1}}^{}(z)v]].\] (13)

The only change is that \(v\) must have covariance \(RR^{T}\) rather than the identity. We achieve this by sampling standard normal vectors \(^{m}\) and then projecting them into the tangent space using the Jacobian of the projection function:

\[v=^{}(f_{}(x)).\] (14)

Constructing \(v\) like this fulfills the conditions of Theorem 3 because \(_{v}[v]=0\), and:

\[[v]=_{}[^{}(f_{ }(x))^{T}^{}(f_{}(x))^{T}]=^{ }(f_{}(x))^{}(f_{}(x))^{T}=RR^{T}.\] (15)

Just like (Sorrenson et al., 2024; Draxler et al., 2024), we further normalize \(v\) to reduce the variance of the trace estimator. Equation (13) now allows training invertible architectures on manifolds even if the volume change \(|R^{T}f^{}_{}(x)Q|\) is not tractable.

Despite using a stochastic estimator for the gradient, we argue in Appendix A.5 that the scaling of the estimator variance with dimension is comparable to the variance due to stochasticity in flow matching and similar methods.

Free-form manifold-to-manifold neural networksAs discussed in Section 2, invertible architectures have to be specially constructed for each manifold. To overcome this limitation, we now soften the constraint that the learned model be analytically invertible. Instead, we learn a pair of free-form manifold-to-manifold neural networks, an encoder \(f_{}(x)\) and a decoder \(g_{}(z)\) as arbitrary functions on the manifold:

\[f_{}(x):, g_{}(z): .\] (16)

We choose to fulfill Eq. (16) using feed-forward neural networks \(_{},_{}:^{m}^{m}\) working in an embedding space \(^{m}\) of \(\), but ensure that their outputs lie on the manifold by appending a projection \(:^{m}\), mapping points from the embedding space \(^{m}\) to the manifold \(\):

\[f_{}(x)=(_{}(x)), g_{}(z)=(_{ }(z)).\] (17)

Figure 2: Computation of the volume change in the tangent space of the manifold: The manifold change of variables formula in Eq. (10) requires to compute the change of a volume element in the tangent spaces under f, which in this example is given by the ratio of lengths of \(dt\) and \(dt^{}\). Since \(f\) is a map in the embedding space, \(f^{}(x)\) defines a linear map between vectors from the embedding space. To correctly compute the change in volume, we use \(Q\) and \(R\) to change coordinates to the intrinsic tangent spaces, resulting in the linear map \(R^{T}f^{}(x)Q:_{x}_{f(x)}\), which maps \(dt\) to \(dt^{}\).

Figure 1 illustrates this for an example on a circle \(=^{1}\).

Just like in the Euclidean case, we employ a reconstruction loss to make \(f_{}\) and \(g_{}\) approximately inverse to one another:

\[_{}=_{}[\|g_{}(f_{}(x) )-x\|^{2}].\] (18)

We measure the distance in the embedding space; one can modify this to use an on-manifold distance (e.g. great circle distance for the sphere) but we find that ambient Euclidean distance works well in practice, since it is almost identical for small distances and this is the regime we work in.

This allows us to substitute \({f_{}^{-1}}^{}(z) g_{}^{}(z)\) in Eq. (13):

\[_{}_{}_{} _{x,v}[- p_{Z}(z)-v^{T}f_{}^{}(x)[g_ {}{}^{}(z)v]].\] (19)

In Theorem 5 we show that the error of the gradient estimator is bounded by a measure of the mismatch between the encoder and decoder Jacobian matrices. When the encoder and decoder are true inverses, the error reaches zero.

Regularization and final lossWe find that adding the following two regularizations to the loss improve the stability and performance of our models. Firstly, the reconstruction loss on points sampled uniformly from the data manifold:

\[_{}=_{x()}[\|g_{ }(f_{}(x))-x\|^{2}],\] (20)

helps ensure that we have a globally consistent mapping between the data and latent manifolds in low data regions. Secondly, the squared distance between the output of \(_{}\) and its projection to the manifold:

\[_{}=_{p_{}(x)}[\|_{}( x)-f_{}(x)\|^{2}]\] (21)

discourages the output of \(_{}\) from entering unprojectable regions, for example the origin when the manifold is \(^{n}\). The same regularizations can be applied starting from the latent space.

The full loss is:

\[=_{}+_{}_ {}+_{}_{}+_{}_{}\] (22)

where the gradient of \(_{}\) is computed using Eq. (19), and \(_{}\), \(_{}\) and \(_{}\) are hyperparameters. We give our choices in Appendix B.

## 5 Experiments

We now demonstrate the practical performance of manifold free-form flows on various manifolds. We choose established experiments to ensure comparability with previous methods, and find:

* M-FFF matches or outperforms previous single-step methods. M-FFF uses a simple ResNet architecture, whereas previous methods were specialized to the given manifolds, hindering adoption to novel manifolds.
* M-FFF generates samples faster by typically two orders of magnitude than methods sampling in several steps. Despite this great reduction in compute, it achieves a higher generative quality in several cases.

In our result tables, we mark as bold (a) the best method overall (both single- and multi-step), and (b) the best single-step method. We provide reconstruction losses of our method and all details necessary to reproduce the experiments in Appendix B. Furthermore, our code is available at https://github.com/vislearn/FFF. We run each experiment multiple times with different data splits and report the mean and standard deviation of those runs.

Synthetic distribution over rotation matricesThe group of 3D rotations \(SO(3)\) can be represented by rotations matrices with positive determinant, i.e., all \(Q^{3 3}\) with \(Q^{T}Q=I\) and \( Q=1\). We choose \(^{3 3}\) as our embedding space and project to the manifold by solving the constrained Procrustes problem via SVD (Lawrence et al., 2019) (see Appendix B.2).

We evaluate M-FFF on synthetic mixture distributions proposed by De Bortoli et al. (2022) with \(M\) mixture components for \(M=16,32\) and \(64\). Samples from one of the distributions and samples from our model are depicted in Fig. 3.

Table 3 shows that M-FFF outperforms the normalizing flow developed for \(SO(3)\) by Liu et al. (2023), as well as the diffusion-based approaches for the mixtures \(M=32\) and \(64\).

Earth data on the sphereWe evaluate manifold free-form flows on spheres with datasets from the domain of earth sciences. We use four established datasets compiled by Mathieu and Nickel (2020) for density estimation on \(^{2}\): Volcanic eruptions (NGDC/WDS, 2022b), earthquakes (NGDC/WDS, 2022a), floods (Brakenridge, 2017) and wildfires (EOSDIS, 2020).

Figure 1 shows an example for a model trained on flood data. As the reconstruction error sometimes does not drop to a satisfactory level we employ the method described in Appendix B.1 to ensure that the measured likelihoods are accurate. Table 4 shows that M-FFF again outperforms the specialized single-step model; the performance compared to multi-step methods is mixed. We think that multi-step models have an advantage on the considered data, as there are large regions of empty space between highly concentrated data points (see density and sample plots in Appendix B.3).

    & Volcano & Earthquake & Flood & Fire & Fast inference? \\  Riemannian CNF (Mathieu and Nickel, 2020) & -6.05 \(\) 0.61 & 0.14 \(\) 0.23 & 1.11 \(\) 0.19 & -0.80 \(\) 0.54 & \(\) \(\)100 steps \\ Moser flow (Rozen et al., 2021) & -4.21 \(\) 0.17 & -0.16 \(\) 0.06 & 0.57 \(\) 0.10 & -1.28 \(\) 0.05 & \(\) \(\)100 steps \\ Stereographic score-based (De Bortoli et al., 2022) & -3.80 \(\) 0.27 & -0.19 \(\) 0.05 & 0.59 \(\) 0.07 & -1.28 \(\) 0.12 & \(\) \(\)100 steps \\ Riemannian score-based (De Bortoli et al., 2022) & -4.92 \(\) 0.25 & -0.19 \(\) 0.07 & 0.45 \(\) 0.15 & -1.33 \(\) 0.06 & \(\) \(\)100 steps \\ Riemannian diffusion (Huang et al., 2022) & -6.61 \(\) 0.97 & **-0.40 \(\) 0.05** & 0.43 \(\) 0.07 & -1.38 \(\) 0.05 & \(\) \(\)100 steps \\ Riemannian flow matching (Chen and Lipman, 2024) & **-7.93 \(\) 0.07** & -0.28 \(\) 0.05 & **0.42 \(\) 0.08** & **-1.86 \(\) 0.11** & \(\) : 1000 steps \\  Mixture of Kent (Peel et al., 2001) & -0.80 \(\) 0.67 & 0.33 \(\) 0.05 & 0.73 \(\) 0.07 & -1.18 \(\) 0.06 & \(\) \\ M-FFF (ours) & **-2.25 \(\) 0.02** & **-0.23 \(\) 0.01** & **0.51 \(\) 0.01** & **-1.19 \(\) 0.03** & \(\) \\  Dataset size & 827 & 6120 & 4875 & 12809 & \\   

Table 4: M-FFF significantly outperforms the previous single-step density estimator (Peel et al., 2001) on the sphere on real-world earth datasets in terms of negative log-likelihood (lower is better). Baseline values are collected from De Bortoli et al. (2022), Huang et al. (2022), Chen and Lipman (2024).

Figure 3: Manifold free-form flows on a synthetic \(SO(3)\) mixture distribution with \(M=64\) mixture components proposed by De Bortoli et al. (2022). _(Left)_ 10,000 samples each from the ground truth distribution and _(right)_ our model. This visualization computes three Euler angles, which fully describe a rotation matrix, and then plot the first two angles on the projection of a sphere and the last by color (Murphy et al., 2021). We find that our model nicely samples from the distribution with few outliers between the modes.

Torsion angles of molecules on toriTo benchmark manifold free-form flows on tori \(^{n}\), we follow (Huang et al., 2022) and evaluate our model on two datasets from structural biology. We consider the torsion (dihedral) angles of the backbone of protein and RNA substructures respectively.

We represent a tuple of angles \((_{1},,_{n})[0,2]^{n}\) by mapping each angle to a position on a circle: \(X_{i}=(_{i},_{i})^{1}\). Then we stack all \(X_{i}\) into a matrix \(X^{n 2}\), compare Table 2.

The first dataset is comprised of 500 proteins assembled by (Lovell et al., 2003) and is located on \(^{2}\). The three dimensional arrangement of a protein backbone can be described by the so called Ramachandran angles (Ramachandran et al., 1963)\(\) and \(\), which represent the torsion of the protein backbone around the \(N\)-\(C_{}\) and \(C_{}\)-\(C\) bonds. The data is split into four distinct subsets _General_, _Glycine_, _Proline_ and _Pre-Proline_, depending on the residue of each substructure.

The second dataset is extracted from a subset of RNA structures introduced by Murray et al. (2003). As the RNA backbone structure can be characterized by seven torsion angles, in this case we are dealing with data on \(^{7}\).

We report negative log-likelihoods in Table 5, finding that M-FFF outperforms a circular spline coupling flow, a normalizing flow particularly developed for data on tori (Rezende et al., 2020) as well as the multi-step methods on one of the datasets. In addition to the quantitative results, we show the log densities of the M-FFF models for the four protein datasets inFig. 5 in Appendix B.4.

Toy distributions on hyperbolic spaceWe apply M-FFF to the Poincare ball model, which embeds the 2-dimensional hyperbolic space \(^{2}\) of constant negative curvature -1 in the 2-dimensional Euclidean space \(^{2}\), as specified in Table 2. As this embedding is not isometric, and distances between points grow when moving away from the origin, we must include the last term of Eq. (9) when changing variables under a map on this embedded manifold.

We show that M-FFF can be applied to non-isometric embeddings using Eq. (9) and visualize learned densities in Fig. 1 and in Fig. 6 in Appendix B.5 for several toy datasets defined on the 2-dimensional Poincare ball model. Further details can be found in Appendix B.5.

Manifold with non-trivial curvatureFinally, we follow Chen and Lipman (2024) and train M-FFF given by synthetic distributions on the Stanford bunny (Turk and Levoy, 1994) on the data provided with their paper, see Fig. 1. The natural embedding of this mesh is \(^{3}\), and we train a separate neural network to project from the embedding space to the mesh. This ensures that the projection is continuously differentiable, which we identify to be important for stable gradients.

Table 6 shows that M-FFF performs well on this manifold, outperforming Riemannian flow matching in two out of three cases. This experiment underlines the flexibility of our model: We only need a projection function to the manifold in order to train a generative model.

    & \(k=10\) & \(k=50\) & \(k=100\) & Fast inference? \\  Riemannian Flow Matching (w/ diffusion) (Chen and Lipman, 2024) & \(1.16 0.02\) & \(1.48 0.01\) & \(1.53 0.01\) & \(\): 1000 steps \\ Riemannian Flow Matching (w/ biharmonic) (Chen and Lipman, 2024) & \(\) & \(1.55 0.01\) & \(1.49 0.01\) & \(\): 1000 steps \\  M-FFF (ours) & \(\) & \(\) & \(\) & \(\) \\   

Table 6: Test NLL on Stanford bunny data proposed by (Chen and Lipman, 2024), living on a manifold with nontrivial curvature (see Fig. 1). M-FFF outperforms the multi-step model for datasets with more modes.

    & General & Glycine & Proline & Pre-Promo & RNA & Fast inference? \\  Riemannian diffusion (Huang et al., 2022) & \(1.04 1.002\) & \(1.97 1.012\) & \(\) & \(1.24 0.008\) & -3.70 0.07 & \(\): \(\)1000 steps \\ Riemannian flow matching (Chen and Lipman, 2024) & \(\) & \(1.90 0.005\) & \(0.15 0.027\) & \(\) & -\(\) & \(\): 1000 steps \\  Mixture of power spherical (Huang et al., 2022) & \(1.15 0.002\) & \(2.08 0.009\) & \(0.27 0.008\) & \(1.34 0.015\) & \(4.08 0.014\) & ✓ \\ Circular Spline Coupling Flows (Rezende et al., 2020) & \(\) & \(1.91 0.008\) & \(0.21 0.008\) & \(1.24 0.008\) & -4.01 \( 0.034\) & ✓ \\ M-FFF (ours) & \(\) & \(\) & \(\) & \(\) & -\(\) & ✓ \\   

Table 5: M-FFF consistently outperforms normalizing flows specialized to tori (Rezende et al., 2020) on torus datasets, without requiring the development of a specialized architecture. In addition, our method comes close to the performance of the multi-step methods and even outperforms them on the Glycine dataset. Baseline values are due to Huang et al. (2022), Chen and Lipman (2024).

## 6 Limitations

Manifold Free-From Flows achieve high generative quality on manifolds despite the approximations made during training: First, the exact inverse of the encoder Jacobian is approximated by the decoder Jacobian, which is implicitly regularized via the reconstruction loss (see Eq. (19)). Second, the final gradient computation in Eq. (8) is estimated with a single \(v\) for each item in the batch, adding noise to the system.

At inference time, the negative log-likelihoods we report in all tables are based on the _decoder_ Jacobian. We choose this because even if the decoder ends up not to be invertible after training (that is several latent codes \(z\) yield the same generation \(x=g_{}(z)\)), the computed densities are a conservative estimate of the true probability density. The downside is that if the reconstruction loss is high, the likelihoods become inaccurate, see Appendix B.1 for details. We therefore ensure that the final reconstruction losses are vanishing in Table 8.

From a high level perspective, we observe that M-FFF performs less favorable compared to multi-step methods when the density changes sharply or very low density regions are present.

## 7 Conclusion

In this paper, we present Manifold Free-Form Flows (M-FFF), a generative model designed for manifold data. To the best of our knowledge, it is the first generative model on manifolds with single-step sampling and density estimation readily applicable to arbitrary Riemannian manifolds. This significantly accelerates inference and allows for deployment on edge devices.

M-FFF matches or outperforms single-step architectures specialized to particular manifolds. It also surpasses multi-step methods in several cases, despite reducing the inference compute by typically two orders of magnitude.

Adapting M-FFF to new manifolds is straightforward and only requires selecting an embedding space and a projection to the manifold. In contrast, competing multi-step methods are more challenging to adapt as they require implementing a diffusion process or computing distances on the manifold.