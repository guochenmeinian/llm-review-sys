# Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics

Koen Minartz\({}^{1}\)   Yoeri Poels\({}^{1,2}\)   Simon Koop\({}^{1}\)   Vlado Menkovski\({}^{1}\)

\({}^{1}\)Data and Artificial Intelligence Cluster, Eindhoven University of Technology

\({}^{2}\)Swiss Plasma Center, Ecole Polytechnique Federale de Lausanne

{k.minartz, y.r.j.poels, s.m.koop, v.menkovski}@tue.nl

###### Abstract

Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces _equivariant distributions over trajectories_, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for probabilistic simulation. More specifically, we demonstrate that incorporating equivariance in EPNS improves simulation quality, data efficiency, rollout stability, and uncertainty quantification. We conclude that EPNS is a promising method for efficient and effective data-driven probabilistic simulation in a diverse range of domains.

## 1 Introduction

The advent of fast and powerful computers has led to numerical simulations becoming a key tool in the natural sciences. For example, in computational biology, models based on probabilistic cellular automata are used to effectively reproduce stochastic cell migration dynamics , and in physics, numerical methods for solving differential equations are used in diverse domains such as weather forecasting , thermonuclear fusion , and computational astrophysics , amongst others. Recently, data-driven methods driven by neural networks have emerged as a complementary paradigm to simulation. These _neural simulators_ are particularly useful when numerical methods are infeasible or impractical, for example due to computationally expensive simulation protocols or lack of an accurate mathematical model .

Figure 1: Schematic illustration of EPNS applied to stochastic cellular dynamics. EPNS models distributions over trajectories that are equivariant to permutations \(\) of the cell indices, so that \(p_{}(x^{t+k}|x^{t})=p_{}(x^{t+k}|x^{t})\).

Notably, most neural simulators are deterministic: given some initial condition, they predict a single trajectory the system will follow. However, for many applications this formulation is insufficient. For example, random effects or unobserved exogenous variables can steer the system towards strongly diverging possible evolutions. In these scenarios, deterministic models are incentivized to predict an 'average case' trajectory, which may not be in the set of likely trajectories. Clearly, for these cases probabilistic neural simulators that can expressively model _distributions over trajectories_ are required. Simultaneously, it has been shown that incorporating domain symmetries in model architectures can be highly beneficial for model accuracy, data efficiency, and parameter efficiency , all of which are imperative for a high-quality neural simulator. As such, both the abilities to perform probabilistic simulation and to incorporate symmetries are crucial to develop reliable data-driven simulators.

Accordingly, prior works have proposed methods for probabilistic simulation of dynamical systems, as well as for symmetry-aware simulation. For probabilistic spatiotemporal simulation, methods like neural stochastic differential equations (NSDEs) [20; 27], neural stochastic partial differential equations (NSPDEs) , Bayesian neural networks [34; 58], autoregressive probabilistic models , and Gaussian processes  have been successfully used. Still, these methods do not incorporate general domain symmetries into their model architectures, hindering their performance. In contrast, almost all works that focus on incorporating domain symmetries into neural simulation architectures are limited to the deterministic case [7; 18; 19; 31; 53; 54]. As such, these methods cannot simulate stochastic dynamics, which requires expressive modeling of distributions over system evolutions. A notable exception is , which proposes a method for probabilistic equivariant simulation, but only for two-dimensional data with rotation symmetry.

Although the above-mentioned works address the modeling of uncertainty or how to build equivariant temporal simulation models, they do not provide a general method for _equivariant stochastic simulation of dynamical systems_. In this work, we propose a framework for probabilistic neural simulation of spatiotemporal dynamics under equivariance constraints. Our main contributions are as follows:

* We propose Equivariant Probabilistic Neural Simulation (EPNS), an autoregressive probabilistic modeling framework for simulation of stochastic spatiotemporal dynamics. We mathematically and empirically demonstrate that EPNS produces single-step distributions that are equivariant to the relevant transformations, and that it employs these distributions to autoregressively construct equivariant distributions over the entire trajectory.
* We evaluate EPNS on two diverse problems: an n-body system with stochastic forcing, and stochastic cellular dynamics on a grid domain. For the first problem, we incorporate an existing equivariant architecture in EPNS. For the second, we propose a novel graph neural network that is equivariant to permutations of cell indices, as illustrated in Figure 1.
* We show that incorporating equivariance constraints in EPNS improves data efficiency, uncertainty quantification and rollout stability, and that EPNS outperforms existing methods for probabilistic simulation in the above-mentioned problem settings.

## 2 Background and related work

### Equivariance

For a given group of transformations \(G\), a function \(f:X X\) is \(G\)-equivariant if

\[f((g)x)=(g)f(x)\] (1)

holds for all \(g G\), where \((g)\) denotes the group action of \(g\) on \(x X\)[10; 12]. In the context of probabilistic models, a conditional probability distribution \(p(x|y,z)\) is defined to be \(G\)-equivariant with respect to \(y\) if the following equality holds for all \(g G\):

\[p(x|y,z)=p((g)x|(g)y,z).\] (2)

The learning of such equivariant probability distributions has been employed for modeling symmetrical density functions, for example for generating molecules in 3D (Euclidean symmetry) [25; 43; 57] and generating sets (permutation symmetry) [6; 28]. However, different from our work, these works typically consider time as an internal model variable, and use the equivariant probability distribution as a building block for learning invariant probability density functions over static data points by starting from a base distribution that is invariant to the relevant transformations. For example, each 3D orientation of a molecule is equally probable. Another interesting work is , which considers equivariant learning of stochastic fields. Given a set of observed field values, their methods produce equivariant distributions over interpolations between those values using either equivariant Gaussian Processes or Steerable Conditional Neural Processes. In contrast, our goal is to produce equivariant distributions over temporal evolutions, given only the initial state of the system.

### Simulation with neural networks

Two commonly used backbone architectures for neural simulation are graph neural networks for operating on arbitrary geometries  and convolution-based models for regular grids . Recently, the embedding of symmetries beyond permutation and translation equivariance has become a topic of active research, for example in the domains of fluid dynamics and turbulence modeling , climate science , and various other systems with Euclidean symmetries . Although these works stress the relevance of equivariant simulation, they do not apply to the stochastic setting due to their deterministic nature.

Simultaneously, a variety of methods have been proposed for probabilistic simulation of dynamical systems. Latent neural SDEs  and ODE\({}^{2}\)VAE  simulate dynamics by probabilistically evolving a (stochastic) differential equation in latent space. Alternatively, methods like Bayesian neural networks (BNNs) and Gaussian processes (GPs) have also been applied to probabilistic spatiotemporal simulation . Further, NSPDEs are able to effectively approximate solutions of stochastic partial differential equations . Due to their probabilistic nature, all of these methods are applicable to stochastic simulation of dynamical systems, but they do not incorporate general domain symmetries into their models. Finally, closely related to our work,  addresses the problem of learning equivariant distributions over trajectories in the context of multi-agent dynamics. However, their method is limited to two spatial dimensions, assumes a Gaussian one-step distribution, and only takes the symmetry group of planar rotations into account. In contrast, we show how general equivariance constraints can be embedded in EPNS, and that this leads to enhanced performance in two distinct domains.

## 3 Method

### Modeling framework

Problem formulation.At each time \(t\), the system's state is denoted as \(x^{t}\), and a sequence of states is denoted as \(x^{0:T}\). We presuppose that we have samples from a ground-truth distribution \(p^{*}(x^{0:T})\) for training, and for inference we assume that \(x^{0}\) is given as a starting point for the simulation. In this work, we assume Markovian dynamics, although our approach can be extended to non-Markovian dynamics as well. Consequently, the modeling task boils down to maximizing Equation 3:

\[_{x^{0:T} p^{*}}_{t U\{0,,T-1\}} p_{ }(x^{t+1}|x^{t}).\] (3)

As such, we can approximate \(p^{*}(x^{1:T}|x^{0})\) by learning \(p_{}(x^{t+1}|x^{t})\) and applying it autoregressively. Furthermore, when we know that the distribution \(p^{*}(x^{1:T}|x^{0})\) is equivariant to \((g)\), we want to guarantee that \(p_{}(x^{1:T}|x^{0})\) is equivariant to these transformations as well. This means that

\[p_{}(x^{1:T}|x^{0})=p_{}((g)x^{1:T}|(g)x^{0})\] (4)

should hold, where \((g)x^{1:T}\) simply means applying \((g)\) to all states in the sequence \(x^{1:T}\).

Generative model.The model \(p_{}(x^{t+1}|x^{t})\) should adhere to three requirements. First, since our goal is to model equivariant distributions over trajectories in diverse domains, the type of model used for \(p_{}\) should be amenable to incorporating general equivariance constraints. Second, we argue that generative models with iterative sampling procedures, although capable of producing high-quality samples , are impractical in this setting as we need to repeat the sampling procedure over potentially long trajectories. Third, the model should be able to simulate varying trajectories for the same initial state, especially in the case of sensitive chaotic systems, so good mode coverage is vital.

As such, we model \(p_{}(x^{t+1}|x^{t})\) as \(_{z}p_{}(x^{t+1}|z,x^{t})p_{}(z|x^{t})dz\) by using a latent variable \(z\), following the conditional Variational Autoencoder (CVAE) framework . Specifically, to generate \(x^{t+1}\), \(x^{t}\) is first processed by a _forward model_\(f_{}\), producing an embedding \(h^{t}=f_{}(x^{t})\). If there is little variance in \(p^{*}(x^{t+1}|x^{t})\), \(h^{t}\) contains almost all information for generating \(x^{t+1}\). On the other hand, if the system is at a point where random effects can result in strongly bifurcating trajectories, \(h^{t}\) only contains implicit information about possible continuations of the trajectories. To enable the model to simulate the latter kind of behavior, the embedding \(h^{t}\) is subsequently processed by the conditional prior distribution. Then, \(z p_{}(z|h^{t})\) is processed in tandem with \(h^{t}\) by the decoder, producing a distribution \(p_{}(x^{t+1}|z,h^{t})\) over the next state. The sampling process is summarized in Figure 1(a).

Incorporating symmetries.Using the approach described above, we can design models that can simulate trajectories by iteratively sampling from \(p_{}(x^{t+1}|x^{t})\). Now, the question is how we can ensure that \(p_{}(x^{1:T}|x^{0})\) respects the relevant symmetries. In Appendix A, we prove Lemma 1:

**Lemma 1**.: _Assume we model \(p_{}(x^{1:T}|x^{0})\) as described in Section 3.1. Then, \(p_{}(x^{1:T}|x^{0})\) is equivariant to linear transformations \((g)\) of a symmetry group \(G\) in the sense of Definition 2 if:_

1. _The forward model is_ \(G\)_-equivariant:_ \(f_{}((g)x)=(g)f_{}(x)\)_;_
2. _The conditional prior is_ \(G\)_-invariant or_ \(G\)_-equivariant:_ \(p_{}(z|(g)h^{t})=p_{}(z|h^{t})\)_, or_ \(p_{}((g)z|(g)h^{t})=p_{}(z|h^{t})\)_;_
3. _The decoder is_ \(G\)_-equivariant with respect to_ \(h^{t}\) _(for an invariant conditional prior), or_ \(G\)_-equivariant with respect to both_ \(h^{t}\) _and_ \(z\) _(for an equivariant conditional prior):_ \(p_{}(x^{t+1}|z,h^{t})=p_{}((g)x^{t+1}|z,(g)h^{t})\)_, or_ \(p_{}(x^{t+1}|z,h^{t})=p_{}((g)x^{t+1}|(g)z,(g)h^{t})\)_._

The consequence of Lemma 1 is that, given an equivariant neural network layer, we can stack these layers to carefully parameterize the relevant model distributions, guaranteeing equivariant distributions _over the entire trajectory_. To understand this, let us consider the case where the conditional prior is \(G\)-invariant, assuming two inputs \(x^{t}\) and \((g)x^{t}\). First, applying \(f_{}\) to \(x^{t}\) yields \(h^{t}\), and since \(f_{}\) is \(G-\)equivariant, applying it to \((g)x^{t}\) yields \((g)h^{t}\). Second, due to the invariance of \(p_{}(z|h^{t})\), both \(h^{t}\) and \((g)h^{t}\) lead to exactly the same distribution over \(z\). Third, since \(z p_{}(z|h^{t})\) is used to parameterize the decoder \(p_{}(x^{t+1}|z,h^{t})\), which is equivariant with respect to \(h^{t}\), the model's one-step distribution \(p_{}(x^{t+1}|x^{t})\) is equivariant as well. Finally, autoregressively sampling from the model inductively leads to an equivariant distribution over the entire trajectory.

Invariant versus equivariant latents.When \(p_{}(z|h^{t})\) is equivariant, the latent variables are typically more local in nature, whereas they are generally more global in the case of an invariant conditional prior. For example, for a graph domain, we may design \(p_{}(z|h^{t})\) to first perform a permutation-invariant node aggregation to obtain a global latent variable, or alternatively to equivariantly map each node embedding to a node-wise distribution over \(z\) to obtain local latent variables. Still, both design choices result in an equivariant model distribution \(p_{}(x^{1:T}|x^{0})\).

### Training procedure

Following the VAE framework [23; 39], an approximate posterior distribution \(q_{}(z|x^{t},x^{t+1})\) is learned to approximate the true posterior \(p_{}(z|x^{t},x^{t+1})\). During training, \(q_{}\) is used to optimize the

Figure 2: EPNS model overview.

Evidence Lower Bound (ELBO) on the one-step log-likelihood:

\[ p_{}(x^{t+1}|x^{t})_{q_{}(z|x^{t},x^{t} +1)}[p_{}(x^{t+1}|z,x^{t})]-KL[q_{}(z|x^{t},x^{t+1})|| p_{}(z|x^{t})].\] (5)

In our case, \(q_{}\) takes both the current and next timestep as input to infer a posterior distribution over the latent variable \(z\). Naturally, we parameterize \(q_{}\) to be invariant or equivariant with respect to both \(x^{t}\) and \(x^{t+1}\) for an invariant or equivariant conditional prior \(p_{}(z|x^{t})\) respectively.

A well-known challenge for autoregressive neural simulators concerns the accumulation of error over long rollouts [9; 48]: imperfections in the model lead to its output deviating further from the ground-truth for increasing rollout length \(k\), producing a feedback loop that results in exponentially increasing errors. In the stochastic setting, we observe a similar phenomenon: \(p_{}(x^{t+1}|x^{t})\) does not fully match \(p^{*}(x^{t+1}|x^{t})\), resulting in a slightly biased one-step distribution. Empirically, we see that these biases accumulate, leading to poor coverage of \(p^{*}(x^{t+k}|x^{t})\) for \(k 1\), and miscalibrated uncertainty estimates. Fortunately, we found that a heuristic method can help to mitigate this issue. Similar to pushforward training , in _multi-step_ training we unroll the model for more than one step during training. However, iteratively sampling from \(p_{}(x^{t+1}|x^{t})\) can lead to the simulated trajectory diverging from the ground-truth sample. Although this behavior is desired for a stochastic simulation model, it ruins the training signal. Instead, we iteratively sample a reconstruction \(^{t+1}\) from the model, where \(^{t+1} p_{}(x^{t+1}|^{t},z)\) and \(z q_{}(z|x^{t+1},^{t})\) as illustrated in Figure 1(b). Using the posterior distribution over the latent space steers the model towards roughly following the ground truth. Additionally, instead of only calculating the loss at the last step of the rollout as done in , we calculate and backpropagate the loss at every step.1

## 4 Applications and model designs

### Celestial dynamics with stochastic forcing

Problem setting.For the first problem, we consider three-dimensional celestial mechanics, which are traditionally modeled as a deterministic n-body system. However, in some cases the effects of dust distributed in space need to be taken into account, which are typically modeled as additive Gaussian noise on the forcing terms [5; 33; 45]. In this setting, the dynamics of the system are governed by the following SDE:

\[dx_{i} =v_{i}dt  i[n]\] (6) \[dv_{i} =_{j i}(x_{j}-x_{i})}{||x_{j}-x_{i}||^{3}}dt+ (x)dW_{t}  i[n]\] (7)

where \((x)\) is 1% of the magnitude of the deterministic part of the acceleration of the \(i\)'th body. We approximate Equation 6 with the Euler-Maruyama method to generate the data. Because of the chaotic nature of n-body systems, the accumulation of the noising term \((x)dW_{t}\) leads to strongly diverging trajectories, posing a challenging modeling problem.

Symmetries and backbone architecture.We model the problem as a fully connected geometric graph in which the nodes' coordinates evolve over time. As such, the relevant symmetries are (1) permutation equivariance of the nodes, and (2) E(n)-equivariance. We use a frame averaging GNN model as detailed in Section 3.3 of  as backbone architecture, denoted as FA-GNN, since it has demonstrated state-of-the-art performance in n-body dynamics prediction . Using FA-GNN, we can parameterize functions that are invariant or equivariant to Euclidean symmetries. We also tried the EGNN , but during preliminary experimentation we found that training was relatively unstable in our setting, especially combined with multi-step training. The FA-GNN suffered substantially less from these instabilities.

Model design.Before explaining the individual model components, we first present the overall model structure, also depicted in Figure 2(a):

* Forward model \(f_{}\): \(_{}(x^{t})\)
* Conditional prior \(p_{}(z|h^{t})\): \((^{}(h^{t}),^{}(h^{t}))\)
* Decoder \(p_{}(x^{t+1}|z,h^{t})\): \((^{}(z,h^{t}),^{}(z,h^{t}))\)As \(f_{}\) needs to be equivariant, we parameterize it with the equivariant version of the FA-GNN, denoted as FA-GNNequivariant. Recall that \(p_{}(z|h^{t})\) can be either invariant or equivariant. We choose for node-wise latent variables that are invariant to Euclidean transformations. As such, we parameterize \(^{}\) and \(^{}\) with FA-GNNinvariant. Finally, \(p_{}(x^{t+1}|z,h^{t})\) needs to be equivariant, which we achieve with a normal distribution where the parameters \(^{}\) and \(^{}\) are modeled with FA-GNNequivariant and FA-GNNinvariant respectively. More details can be found in Appendix C.1.

### Stochastic cellular dynamics

Problem setting.The second problem considers spatiotemporal migration of biological cells. The system is modeled by the Cellular Potts model (CPM), consisting of a regular lattice \(L\), a set of cells \(C\), and a time-evolving function \(x:L C\). Furthermore, each cell has an associated type \((c)\). See also Figures 1 and 3(b) for illustrations. Specifically, we consider an extension of the cell sorting simulation, proposed in , which is a prototypical application of the CPM. In this setting, the system is initialized as a randomly configured culture of adjacent cells. The dynamics of the system are governed by the Hamiltonian, which in the cell sorting case is defined as follows:

\[H=,l_{j}(L)}J(x(l_{i}),x(l_{j}) )(1-_{x(l_{i}),x(l_{j})})}_{}+ _{V}(V(c)-V^{*}(c))^{2}}_{},\] (8)

where \((L)\) is the set of all adjacent lattice sites in \(L\), \(J(x(l_{i}),x(l_{j}))\) is the contact energy between cells \(x(l_{i})\) and \(x(l_{j})\), and \(_{x,y}\) is the Kronecker delta. Furthermore, \(C\) is the set of all cells in the system, \(V(c)\) is the volume of cell \(c\), \(V^{*}(c)\) is the target volume of cell \(c\), and \(_{V}\) is a Lagrange multiplier. To evolve the system, an MCMC algorithm is used, which randomly selects an \(l_{i} L\), and proposes to change \(x(l_{i})\) to \(x(l_{j})\), where \((l_{i},l_{j})(L)\). The change is accepted with probability \((1,e^{- H/T})\), where \(T\) is the _temperature_ parameter of the model. The values of the model parameters can be found in Appendix B.

Symmetries and backbone architecture.As the domain \(L\) is a regular lattice, one relevant symmetry is shift equivariance. Further, since \(x\) maps to a _set_ of cells \(C\), the second symmetry we consider is equivariance to permutations of \(C\); see Figure 1 for an intuitive schematic. To respect both symmetries, we propose a novel message passing graph neural network architecture. We first transform the input \(x\) to a one-hot encoded version, such that \(x\) maps all lattice sites to a one-hot vector of cell indices along the channel axis. We then consider each channel as a separate node \(h_{i}\). Subsequently, message passing layers operate according to the following update equation:

\[h_{i}^{l+1}=^{l}(h_{i}^{l},_{j(i)}^{l}(h_ {j}^{l})),\] (9)

Figure 3: Schematic overviews of model designs.

where \(\) is a permutation-invariant aggregation function, performed over neighboring nodes. Usually, \(^{l}\) and \(^{l}\) would be parameterized with MLPs, but since \(h^{l}_{i}\) lives on a grid domain, we choose both functions to be convolutional neural nets. We refer to this architecture as SpatialConv-GNN.

Model design.We again start by laying out the overall model structure, also shown in Figure 2(b):

* Forward model \(f_{}\): SpatialConv-GNN\((x^{t})\)
* Conditional prior: \(p_{}(z|h^{t})\): \((^{}(h^{t}),^{}(h^{t}))\)
* Decoder: \(p_{}(x^{t+1}|z,h^{t})\): \((^{}(z,h^{t}))\)

Since \(f_{}\) is parameterized by a SpatialConv-GNN, it is equivariant to translations and permutations. At any point in time, each cell is only affected by itself and the directly adjacent cells, so we design \(p_{}(z|h^{t})\) to be equivariant with respect to permutations, but invariant with respect to translations; concretely, we perform alternating convolutions and spatial pooling operations for each node \(h_{i}\) to infer distributions over a latent variable \(z\) for each cell. Finally, the decoder is parameterized by \(^{}(z,h^{t})\), a SpatialConv-GNN which equivariantly maps to the parameters of a pixel-wise categorical distribution over the set of cells \(C\). More details can be found in Appendix C.2.

## 5 Experiments

### Experiment setup

Objectives.Our experiments aim to **a)** assess the effectiveness of EPNS in terms of simulation faithfulness, calibration, and stability; **b)** empirically verify the equivariance property of EPNS, and investigate the benefits of equivariance in the context of probabilistic simulation; and **c)** compare EPNS to existing methods for probabilistic simulation of dynamical systems. We report the results conveying the main insights in this section; further results and ablations are reported in Appendix E.

Data.For both problem settings, we generate training sets with 800 trajectories and validation and test sets each consisting of 100 trajectories. The length of each trajectory is 1000 for the celestial dynamics case, saved at a temporal resolution of \(dt=0.01\) units per step. We train the models to predict a time step of \( t=0.1\) ahead, resulting in an effective rollout length of 100 steps for a full trajectory. Similar to earlier works [38; 44; 59], we choose \(n=5\) bodies for our experiments. For the cellular dynamics case, we use trajectories with \(|C|=64\) cells. The length of each trajectory is 59 steps, where we train the models to predict a single step ahead. Additionally, we generate test sets of 100 trajectories starting from the same initial condition, which are used to assess performance regarding uncertainty quantification.

Implementation and baselines.The EPNS implementations are based on the designs explained in Section 4. The maximum rollout lengths used for multi-step training are 16 steps (celestial dynamics) and 14 steps (cellular dynamics). We use a linear KL-annealing schedule , as well as the 'free bits' modification of the ELBO as proposed in  for the cellular dynamics data. For celestial dynamics, we compare to neural SDEs (NSDE)  and interacting Gaussian Process ODEs (iGPODE) , as these methods can probabilistically model dynamics of (interacting) particles. For cellular dynamics, we compare to ODE\({}^{2}\)VAE , as it probabilistically models dynamics on a grid domain. For each baseline, we search over the most relevant hyperparameters, considering at least seven configurations per baseline. We also compare to a non-equivariant counterpart of EPNS, which we will refer to as PNS. All models are implemented in PyTorch  and trained on a single NVIDIA A100 GPU. Further details on the baseline models can be found in Appendix D. Our code is available at https://github.com/kminartz/EPNS.

### Results

Qualitative results.Figure 4 shows a sample from the test set, as well as simulations produced by neural simulators starting from the same initial condition \(x^{0}\) for both datasets. More samples are provided in Appendix E. For celestial dynamics, both EPNS and iGPODE produce a simulation that looks plausible, as all objects follow orbiting trajectories with reasonable velocities. As expected, the simulations start to deviate further from the ground truth as time proceeds, due to the accumulation of randomness. In the cellular dynamics case, the dynamics produced by EPNS look similar to the ground truth, as cells of the same type tend to cluster together. In contrast, ODE\({}^{2}\)VAE fails to capture this behavior. Although cells are located at roughly the right location for the initial part of the trajectory, the clustering and highly dynamic nature of the cells' movement and shape is absent.

Comparison to related work.We consider two metrics to compare to baselines: the first is the ELBO on the log-likelihood on the test set, denoted as LL. LL is a widely used metric in the generative modeling community for evaluating probabilistic models, expressing goodness-of-fit on unseen data, see for example . Still, the calculation of this bound differs per model type, and it does not measure performance in terms of new sample quality. This is why the second metric, the Kolmogorov-Smirnov (KS) test statistic \(D_{}\), compares empirical distributions of newly sampled simulations to ground truth samples. Specifically, \(D_{}\) is defined as \(D_{}(y)=|F^{*}(y)-F_{}(y)|\), the largest absolute difference between the ground truth and model empirical cumulative distribution functions \(F^{*}\) and \(F_{}\). \(D_{}=0\) corresponds to identical samples, whereas \(D_{}=1\) corresponds to no distribution overlap at all. We opt for this metric since we have explicit access to the ground-truth simulator, and as such we can calculate \(D_{}\) over 100 simulations starting from a fixed \(x^{0}\) in order to assess how well EPNS matches the ground-truth distribution over possible trajectories. The results are shown in Table 1. EPNS performs comparative to or better than the other methods in terms of LL, indicating a distribution that has high peaks around the ground truth samples. In terms of \(D_{}\), EPNS performs best, meaning that simulations sampled from EPNS also match better to the ground truth in terms of the examined properties of Table 1.

Uncertainty quantification.In addition to Tables 1 and 3, we further investigate how distributions produced by EPNS, PNS, and EPNS trained with one step training (EPNS-one step) align with the ground truth. Figure 5 shows how properties of 100 simulations, all starting from the same \(x^{0}\), evolve over time. In the case of celestial dynamics, the distribution over potential energy produced by EPNS closely matches the ground truth, as shown by the overlapping shaded areas. In contrast, EPNS-one step effectively collapses to a deterministic model. Although PNS produces a distribution that overlaps with the ground truth, it doesn't match as closely as EPNS. For the cellular dynamics

    &  \\   & LL \(\) & \(D_{}(KE)\) &  & LL \(\) & \(D_{}(\#)\) \\  & \( 10^{3}\) & t=50 & t=100 & & \( 10^{4}\) & t=30 & t=45 \\  NSDE  & -5.0\(\)0.0 & 0.80\(\)0.01 & 0.65\(\)0.03 & ^{2}\)VAE } &  &  &  \\ iGPDE  & -8.5\(\)0.1 & 0.42\(\)0.07 & 0.57\(\)0.02 & & & \\  PNS (ours) & **10.9\(\)**0.4 & 0.61\(\)0.18 & 0.20\(\)0.06 & & PNS (ours) & -16.4\(\)0.3 & 0.70\(\)0.10 & 0.77\(\)0.05 \\ EPNS (ours) & **10.8\(\)**0.1 & **0.14\(\)**0.03 & **0.14\(\)**0.04 & & EPNS (ours) & **-5.9\(\)**0.1 & **0.58\(\)**0.09 & **0.58\(\)**0.05 \\   

Table 1: Comparison of EPNS to related work. For celestial dynamics, the kinetic energy of the system is used to calculate \(D_{}\), while for cellular dynamics, we use the number of cell clusters of the same type. We report the mean\(\)standard error over three independent training runs.

Figure 4: Qualitative results for celestial dynamics (left) and cellular dynamics (right).

case, the distribution produced by EPNS is slightly off compared to the ground truth, but still matches considerably better than EPNS-one step and PNS. These results indicate that both equivariance and multi-step training improve uncertainty quantification for these applications.

Verification of equivariance.To empirically verify the equivariance property of EPNS, we sample ten simulations for each \(x^{0}\) in the test set, leading to 1000 simulations in total, and apply a transformation \((g)\) to the final outputs \(x^{T}\). \((g)\) is a random rotation and random permutation for celestial dynamics and cellular dynamics respectively. Let us refer to the transformed outcomes as distribution 1. We then repeat the procedure, but now transform the _inputs_\(x^{0}\) by the same transformations before applying EPNS, yielding outcome distribution 2. For an equivariant model, distributions 1 and 2 should be statistically indistinguishable. We test this by comparing attributes of a single randomly chosen body or cell using a two-sample KS-test. We also show results of PNS to investigate if equivariance is learned by a non-equivariant model. The results are shown in Table 2. In all cases, distributions 1 and 2 are statistically indistinguishable for EPNS (\(p 0.05\)), but not for PNS (\(p 0.05\)). As such, EPNS indeed models equivariant distributions over trajectories, while PNS does not learn to be equivariant.

Data efficiency.We now examine the effect of equivariance on data efficiency. Table 3 shows the ELBO of both EPNS and PNS when trained on varying training set sizes. In the celestial dynamics case, the ELBO of EPNS degrades more gracefully when shrinking the training set compared to PNS. For cellular dynamics, the ELBO of EPNS does not worsen much at all when reducing the dataset to only 80 trajectories, while the ELBO of PNS deteriorates rapidly. These results demonstrate that EPNS is more data efficient due to its richer inductive biases.

Rollout stability.As we cannot measure stability by tracking an error metric over time for stochastic dynamics, we resort to domain-specific stability criteria. For celestial dynamics, we define a run to be unstable when the energy in the system jumps by more than 20 units, which we also used to ensure

    &  &  \\   & x-coordinate & y-coordinate & z-coordinate & distance & traveled \\ Model & \(D_{}\) & p-value & \(D_{}\) & p-value & \(D_{}\) & p-value & \(D_{}\) & p-value \\  PNS & 0.09 & 0.00 & 0.22 & 0.00 & 0.09 & 0.00 & 0.503 & 0.00 \\ EPNS & 0.04 & 0.37 & 0.02 & 0.99 & 0.02 & 0.98 & 0.036 & 0.536 \\   

Table 2: KS-test results for equivariance verification.

  
**\# Training** & **Celestial Dynamics** & **Cellular Dynamics** \\
**samples** & EPNS & PNS & EPNS & PNS \\ 
80 & \(9.9\) & \(7.7\) & \(-59.6\) & \(-326.5\) \\
400 & \(10.2\) & \(10.0\) & \(-61.0\) & \(-173.9\) \\
800 & \(10.8\) & \(10.9\) & \(-59.4\) & \(-163.9\) \\   

Table 3: Test set ELBO values (\( 10^{3}\)) for varying amounts of training samples.

Figure 5: Distributions of the potential energy in a system (top row, celestial dynamics) and the number of cell clusters of the same type (bottom row, cellular dynamics) over time. The shaded area indicates the \((0.1,0.9)\)-quantile interval of the observations; the solid line indicates the median value.

the quality of the training samples. For cellular dynamics, we define a run to be unstable as soon as 20% of the cells have a volume outside the range of volumes in the training set. Although these are not the only possible criteria, they provide a reasonable proxy to the overall notion of stability for these applications. The fraction of stable simulations over time, starting from the initial conditions of the test set, is depicted in Figure 6. For both datasets, EPNS generally produces simulations that remain stable for longer than other models, suggesting that equivariance improves rollout stability. Although stability still decreases over time, most of EPNS' simulations remain stable for substantially longer than the rollout lengths seen during training.

## 6 Conclusion

Conclusions.In this work, we propose EPNS, a generally applicable framework for equivariant probabilistic spatiotemporal simulation. We evaluate EPNS in the domains of stochastic celestial dynamics and stochastic cellular dynamics. Our results demonstrate that EPNS outperforms existing methods for these applications. Furthermore, we observe that embedding equivariance constraints in EPNS improves data efficiency, stability, and uncertainty estimates. In conclusion, we demonstrate the value of incorporating symmetries in probabilistic neural simulators, and show that EPNS provides the means to achieve this for diverse applications.

Limitations.The autoregressive nature of EPNS results in part of the simulations becoming unstable as the rollout length increases. This is a known limitation of autoregressive neural simulation models, and we believe that further advances in this field can transfer to our method [9; 48; 49]. Second, the SpatialConv-GNN has relatively large memory requirements, and it would be interesting to improve the architecture to be more memory efficient. Third, we noticed that training can be unstable, either due to exploding loss values or due to getting stuck in bad local minima. Our efforts to mitigate this issue resulted in approximately two out of three training runs converging to a good local optimum, and we believe this could be improved by further tweaking of the training procedure. Finally, our experimental evaluation has shown the benefits of equivariant probabilistic simulation using relatively small scale systems with stylized dynamics. An interesting avenue for future work is to apply EPNS to large-scale systems with more complex interactions that are of significant interest to domain experts in various scientific fields. Examples of such applications could be the simulation of Langevin dynamics or cancer cell migration.

## 7 Acknowledgements

The authors would like to thank Jakub Tomczak and Tim d'Hondt for the insightful discussions and valuable feedback. This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. EINF-3935 and grant no. EINF-3557.

Figure 6: Fraction of simulations that remain stable over long rollouts.