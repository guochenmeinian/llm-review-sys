# Boosting Vision-Language Models with Transduction

Maxime Zanella

UCLouvain, UMons

&Benoit Gerin

UCLouvain

&Ismail Ben Ayed

ETS Montreal

###### Abstract

Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy. We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs). TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances. Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets. We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint.

Code: https://github.com/MaxZanella/transduction-for-vlms

## 1 Introduction

Figure 1: TransCLIP improves significantly the averaged top-1 accuracy on 11 datasets when used on top of inductive zero-shot \(\), 2-shot CoOp prompt tuning and 2-shot \(\) adapter for various encoder sizes.

Combining vision and language modalities can greatly enhance expressiveness and reduce ambiguities in the understanding and interpretation of our environment. This principle is central in the development of Vision-Language Models (VLMs), such as CLIP , which learns visual representations through natural-language supervision. In the pre-training phase, an input image \(\) and associated text description \(\) are encoded by separate vision and text encoders. This yields feature representations \(=_{v}()\) and \(=_{t}()\), which can be aligned by contrastive learning. Such a joint embedding space for the visual and textual modalities facilitates zero-shot recognition and yields powerful adaptation capabilities for a large variety of tasks. The recent literature on adapting VLMs has grown substantially, in both the zero-shot and few-shot learning settings . However, so far, these techniques predominantly align with _induction_, i.e., inference for each test sample is performed independently from the other samples within the target dataset.

In contrast, _transduction_ performs joint inference on all the test samples of a task, leveraging the statistics of the target unlabeled data . In the context of standard vision-based classifiers, this has enabled transductive methods to outperform inductive-inference approaches as evidenced by benchmarks over large-scale datasets such as ImageNet .

Within the scope of deep learning, transduction has mainly been explored for few-shot learning to address the inherent challenges of training under limited supervision. This recent and quite abundant few-shot literature, e.g., , among others, has focused on adopting standard vision-based pre-training models (such as ImageNet pre-training). However, as we will show in our experiments (Table 4), the direct application of existing transductive few-shot methods to VLMs yields poor performances, sometimes underperforming the inductive zero-shot predictions. This might explain why the transductive paradigm has been overlooked in zero-shot and few-shot learning for VLMs so far. The low performance of current transductive few-shot methods in the context of VLMs could be explained by the fact that the underlying objective functions do not account for the text knowledge. In this new multi-modal paradigm, additional supervision could be leveraged from the textual descriptions of the classes (prompts) , e.g., \(_{k}=\) of a [kth class name], along with their corresponding representation \(_{k}=_{t}(_{k})\) derived from the language encoder. We utilize the interleaved representation of text prompts and images with their cosine2 similarity \(^{}_{k}\), which yields text-based prediction \(}_{k}\), thereby guiding our transductive optimization procedure with text-encoder knowledge. Our method optimizes a new objective function integrating a text-driven penalty. Optimization is carried out efficiently w.r.t the assignment variables associated with the unlabeled samples, which are then used as final predictions.

Adapting VLMs has recently attracted wide attention in the literature, predominantly focusing on inductive methods. Motivated by findings in NLP, which indicate that better prompt strategies could enhance performance , substantial efforts were directed towards prompt tuning  for VLMs, with CoOp  standing out as the pioneering work along this line. Following CoOp, prompt tuning has become the favorite strategy for adapting VLMs in a variety of contexts, including unsupervised  and few-shot  learning. Meanwhile, there have been a few efforts towards computationally more efficient adapters . Our transduction formulation aligns with this initiative. By operating solely on the output embeddings (i.e., in a black-box setting), TransCLIP is computationally efficient and does not make assumptions on the underlying encoder architectures. Still, our method is orthogonal to these design choices and could be applied atop any of the above-mentioned inductive approaches.

**Main contributions.****(i)** We introduce a transductive formulation that enhances the zero-shot and few-shot generalization capabilities of VLMs by leveraging the structure of unlabeled data (Figure 1). Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a Kullback-Leibler (KL) divergence penalty integrating the text-encoder knowledge and guiding the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets, such as ImageNet. **(ii)** Our method can be used as a plug-and-play module on top of current inductive zero-shot models and few-shot learning methods, consistently boosting their performance. Also, **(iii)** our approach substantially outperforms recent transductive few-shot methods in the literature, notably due to the KL-based language supervision as a critical success factor.

Related Work

Transduction for vision-only classifiers.The use of unlabeled test data at inference time has received attention lately in rapidly emerging subjects, such as few-shot learning and unsupervised test-time adaptation. Examples include adjusting batch normalization layer statistics  and minimizing the entropy of predictions , which can be supplemented by pseudo-labeling strategies . In the few-shot literature solely based on vision models, transduction leverages both the few labeled samples and unlabeled test data, outperforming inductive methods [76; 5; 24; 37; 75]. One of the first works introducing transduction in vision-based few-shot learning proposes propagating the labels from the support (labeled) to the query (unlabeled) set with a meta-learned graph . Building on this idea, another work proposes to iteratively augment the support set to improve label propagation . LaplacianShot  also exploits the inherent structure of the data through a graph-Laplacian clustering, which discourages disparate class predictions for samples with close features, while matching each query set point to the nearest support prototype. Alternative approaches propose directly learning the class prototypes. For instance, Transductive Fine-Tuning (TF)  uses the prediction entropy on the query samples as a regularization term, while TIM and its variants [5; 59] employ the mutual information between the query samples and their predictions. BD-CSPN  refines the class prototypes by reducing the feature biases between the support set and the most confident query samples. An additional group of methods performs clustering in the feature space, for instance, by solving an optimal transport problem like PT-MAP , by projecting features into sub-spaces to facilitate clustering , or by revisiting the standard K-means with an additional partition-complexity regularizer to control the number of predicted classes .

Zero- and few-shot learning in VLMs.Thanks to their extensive pre-training, VLMs exhibit stronger generalization capabilities than vision-only models but may also fail [50; 69; 57]. In response, substantial recent efforts have been directed towards using their general knowledge and adapting them on more specific tasks [63; 73; 70]. Arguably, the most popular strategy is prompt tuning , which is explored both in the unsupervised [43; 15; 41; 1] and few-shot [73; 72; 40; 12; 65; 6; 74; 8; 9; 29; 30] settings. The pioneering work, CoOp , updates input text-prompt tokens by leveraging the context provided by the few labeled samples (i.e., the support set). Building on this success, various strategies have been developed to enhance this approach, especially through additional regularization. For instance, ProGrad  guides the prompts towards the original hand-crafted ones by gradient projection. Prompt tuning has also been explored in the zero-shot setting, e.g., using the predictive confidence to generate pseudo-labels [25; 41]. Despite its popularity, prompt tuning remains tedious in terms of computations, due to the many back-propagations through the text encoder. This challenge is compounded in the recent developments, which introduce visual tokens [29; 30] alongside the text tokens. In contrast, there has been limited efforts so far in developing black-box methods [48; 17; 16; 66; 62], which only access the final embedding states. These methods often rely on the so-called adapters , like Tip-Adapter(-F) , which adds a classifier at the output of the vision encoder, in the form of a cache model involving the few-shot samples. Lately, a strong baseline based on Gaussian discriminant analysis clustering  demonstrates VLMs' adaptation abilities with a Gaussian hypothesis on the embedding space.

Transductive inference in VLMs.Despite the growing interest in unsupervised, zero-shot and few-shot learning for VLMs, the transductive-inference paradigm has not been explored so far in this new multi-modal context, except for the very recent work in , which was deployed for small-size tasks (\( 10^{2}\) test samples). However, the method in  may not be computationally tractable for large-scale query sets, due to expensive inner loops for estimating the Dirichlet distribution's parameters. We provide a computationally efficient solution, which can scale up to large target datasets (such as ImageNet), while being easily amenable as a plug-and-play module on top of state-of-the-art inductive methods. It is worth mentioning that test-time adaptation methods also employ the transduction paradigm, but their settings are very different from those studied in this work. For instance, SwapPrompt  has been designed to make batch predictions on-the-fly, and has continual-learning mechanisms such as an exponential moving average prompt across batches. TPT  work on a single sample with many data augmentations to train one prompt per image. Both methods require access to model weights for training (i.e., do not operate in a black-box setting) and an expensive training procedure. We also note that prompt tuning does not scale well with the model size and is even impractical on very large models such as EVA-CLIP-8B . We still report the performances of this class of methods in the Appendix (Table 9).

TransCLIP: Transduction for Vision-Language Models

In this section, we describe our objective function for transductive inference in vision-language models, and derive a block majorize-minimize (BMM) algorithm for minimizing it, with guaranteed convergence and decoupled sample-assignment updates. When dealing with a zero-shot classification problem based on a vision-language model, such as CLIP, and given a set of \(K\) candidate classes, one creates textual descriptions, the so-called prompts , each corresponding to a class, e.g., \(_{k}\) = a photo of a [kth class name], \(k=1,,K\). Let \(_{k}=_{t}(_{k})\) denotes the corresponding normalized (unit hyper-sphere) embedding representation, with \(_{t}\) representing the language encoder. Similarly, each test image \(_{i}\), \(i=1,,N\), is projected onto a normalized embedding space of the same dimension, using visual encoder \(_{v}\): \(_{i}=_{v}(_{i})\). In the standard inductive zero-shot inference, classification of a given image \(_{i}\) is done by evaluating the cosine similarity between these two encoded modalities and predicting the class corresponding to the most similar text embedding: \(=*{argmax}_{k}\ _{i}^{}_{k}\). Furthermore, one can compute pseudo-labels corresponding to these zero-shot predictions by applying the softmax function with a temperature scaling3\(\), which yields the following probability-simplex vector for each sample:

\[}_{i}=(_{i,k})_{1 k K}_{K}; _{i,k}=_{i}^{}_{k})}{_{j}( _{i}^{}_{j})}\] (1)

where \(_{K}\) denotes the probability simplex. Let \(=\{i:1 i N\}=\ \ \) denotes the samples indices of the target dataset, with \(\) the set of unlabeled _query_ samples indices, i.e., those for which we want to make a prediction, and \(\) the set of labeled _support_ samples indices in the few-shot setting.

Note that, in the zero-shot setting, \(=\). We define a Gaussian Mixture Model-clustering (GMM) term in our objective function by modeling the likelihood of these target data as a balanced mixture of multivariate Gaussian distributions, each representing a class \(k\) and parameterized by mean vector \(_{k}\) and a diagonal covariance matrix \(\):

\[p_{i,k}=(_{i},k;_{k},) ()^{-}(-(_{i}-_{k})^{}^{-1}(_{i}- _{k}))\]

Notation \(p_{i,k}\) is introduced here to simplify the equations in the sequel. Notice that, unlike standard GMMs, we deploy a common diagonal covariance matrix \(\) across all classes. Interestingly, in our experiments, we found this simplifying choice improves the performance while reducing the computational load as there are substantially fewer parameters to learn. This is particularly the case when dealing with large numbers of classes as in large-scale target datasets such as ImageNet.

### Proposed objective function

Our objective function depends on two types of variables: (i) Sample-to-class assignment variables within the probability simplex: \(_{i}=(z_{i,k})_{1 k K}_{K}\), \(i\); and (ii) GMM parameters \(=(_{k})_{1 k K}\) and \(\).

We propose to minimize the following objective, which integrates a GMM-clustering term, a Laplacian regularizer and a Kullback-Leibler (KL) divergence penalty encoding the text-encoder knowledge and guiding the transductive learning process:

\[_{}(,,)=}_{i}^{}(_{i})}_{}-}_{j}w_{ij} _{i}^{}_{j}}_{}+}_{}(_{i}||}_{i} )}_{}\] (2)

where \(_{i}=(p_{i,k})_{1 k K}_{K}\) concatenates the GMM probabilities, \(w_{ij}\) denotes some measure of affinity between visual embeddings \(_{i}\) and \(_{j}\), and the sample-wise parameterized4 KL terms are given by:

\[_{}(_{i}||}_{i})=_{i}^{ }_{i}-_{i}^{}}_{i}, i ;>0\] (3)

In the following, we describe the effect of each term in our objective function in (2):* **GMM-based clustering**: This unsupervised-learning term is akin to the GMM-based maximum-likelihood estimation objective in the standard EM algorithm . By taking the negative logarithm, its minimization corresponds to maximizing the likelihood of the data. It can also be viewed as a probabilistic generalization of the K-means clustering objective . Indeed, assuming \(\) is the identity matrix reduces the first term in (2) to the K-means objective.
* **Laplacian regularization**: The second term in (2) is the Laplacian regularizer, widely used in the context of graph/spectral clustering  and semi-supervised learning . This term encourages nearby samples in the visual-embedding space (i.e., pairs of samples with high affinity \(w_{i,j}\)) to have similar \(\) assignments. In our case, we propose to build a positive semi-definite (PSD) affinity matrix based on the cosine similarities as \(w_{ij}=_{i}^{}_{j}\) (Gram matrix). As we see below, this PSD condition is important to obtain a convergent Majorize-Minimize optimizer with decoupled (parallel) sample-wise updates for the \(\)-assignments, yielding a highly efficient transduction for large-scale target datasets (such as ImageNet).
* **Text-guided KL divergence:** This term is dedicated to vision-language models and, as we will see in our experiments (ablation studies in Tables 4 and 6), has a substantial effect on performance. It encourages the prediction not to deviate significantly from the zero-shot predictions, thereby providing text supervision to the other two unsupervised-learning terms. Furthermore, being convex over \(_{i}\), \(i\), this term facilitates the optimization of the objective w.r.t the assignment variables.

### Extension to the few-shot setting

Our zero-shot formulation naturally extends to the few-shot setting. We integrate supervision from the labeled-support samples, in the form of a cross-entropy, which corresponds to minimizing the following overall loss:

\[_{}(,,)=- {||}_{i}_{i}^{}(_{i} )+|}_{}(,, )\] (4)

Note that, in the first term, the \(_{i}\) are fixed, with \(_{i}\) = \(_{i}\), \(i\) and \(_{i}\) the one-hot ground-truth label associated with the corresponding shot.

### Block Majorize-Minimize (BMM) optimization

As our objective depends on three types of variables (\(\), \(\), \(\)), we proceed with a BMM procedure, alternating three sub-step optimizers. Each sub-step optimizes over one block of variables while the other two are fixed, ensuring the overall objective does not increase. Importantly, the obtained \(\)-updates (Eq. (5)) are decoupled, yielding computationally efficient transduction for large-scale datasets. Also, our overall procedure is guaranteed to converge (Theorem 1).

Majorize-Minimize (MM) with respect to the \(\)-blockWhen \(\) and \(\) are fixed, both the GMM- and KL-based terms are convex w.r.t \(_{i}\). However, the Laplacian term is concave5 (for PSD matrix \(\)). Therefore, we proceed with inner iterations, each minimizing a linear and tight upper bound, the so-called majorizing function in the MM-optimization literature [33; 21; 31], which guarantees the overall objective does not increase. To obtain the tight linear bound, let us write the Laplacian term conveniently in the following matrix form: \(^{}\), with \(=-\), where \(\) denotes the Kronecker product and \(\) is the \(N N\) identity matrix. Note that \(\) is negative semi-definite for a positive semi-definite \(\). Therefore, \(^{}\) is a concave function with respect to \(\), and its first-order approximation at current solution \(^{l}\) (\(l\) being the iteration index) gives the following tight6 upper bound on the Laplacian term:

\[^{}(^{l})^{} ^{l}+(^{l})^{}(-^{l})\]

Replacing the quadratic Laplacian term by this linear bound yields a majorizing function on our overall objective. Importantly, this majorizing function is a sum of decoupled objectives, each corresponding to one assignment variable \(_{i}\), yielding a highly efficient optimizer for large-scale target datasets. Indeed, using simplex constraints \(_{i}_{K},i\), and solving the Karush-Kuhn-Tucker (KKT) conditions independently for each \(_{i}\), we obtain the following decoupled update rules for the \(\)-block:

\[_{i}^{(l+1)}=}_{i}^{} ((_{i})+_{j}w_{ij}_{j}^{(l)})}{( }_{i}^{}((_{i})+_{j }w_{ij}_{j}^{(l)}))^{}_{K}}\] (5)

Closed-form updates of \(\) and \(\)When both \(\) and \(\) are fixed, our objective in (4) is convex. It can be minimized by setting its gradient w.r.t each \(_{k}\) to zero, which yields the following closed-form updates:

\[_{k}=|}_{i }z_{i,k}_{i}+|}_{i}z_{i,k}_{i}}{|}_{i}z_{ i,k}+|}_{i}z_{i,k}}\] (6)

Similarly, when both \(\) and \(\) are fixed, the following closed-form updates minimize the overall objective w.r.t \(\):

\[()=|}_{i }_{k}z_{i,k}(_{i}-_{k})^{2}+|}_{i}_{k}z_{i,k}(_{i}- _{k})^{2}}{+1}\] (7)

The complete procedure is summarized in Appendix B. Note that, after convergence, we use the sample-to-class assignment variables \(_{i}\) as predictions for each sample \(i\) of the query set \(\) using the argmax operation for conventional classification.

### Convergence

Our optimizer can be viewed as an instance of the general Block Majorize-Minimize paradigm for optimization , which optimizes a majorizing function for each block of variables. The convergence of general BMM procedures is well studied in the optimization community . Indeed, under certain conditions (such as the strong convexity of the block-wise majorizing functions), we can establish convergence of our procedure using the following result (more details in Appendix A):

**Theorem 1** (Convergence of BMM ): _Assume that, for each block, the majorizing function is quasi-convex, and its first-order behavior is the same as the original objective locally. Furthermore, assume that the sub-problem solved for each block has a unique solution. Then, every limit point of the iterates generated by BMM is a coordinate-wise minimum of the overall objective._

## 4 Experiments

Datasets.Following the setting of previous works [73; 43], we assess TransCLIP on ImageNet  and ten datasets for fine-grained classification of scenes (SUN397 ), aircraft types (Aircraft ), satellite imagery (EuroSAT ), automobiles (Cars ), food items (Food ), pet breeds (Pets ), flowers (Flowers ), general objects (Caltech101 ), textures (DTD ) and human actions (UCF101 ). We additionally measure performance on four variants of ImageNet (Adversarial , ImageNetV2 , Rendition , Sketch ). Numerical results are reported in terms of the top-1 accuracy with the ViT-B/16 encoder, averaged over three random seeds.

Benchmarks.We aim to show the breadth of potential applications of transduction in the context of VLMs. Notably, employing supervised fine-tuning, followed by transduction with TransCLIP on the unlabeled test samples, emerges as a powerful and efficient solution. This is particularly convenient when the labeled samples (the support set) and/or computational power are not accessible at inference (i.e., test) time7. To this end, we first study the applicability of our zero-shot formulation TransCLIP-ZS (Eq. (2)) across three settings: (i) on top of inductive _zero-shot_ learning and popular _few-shot_ learning methods; (ii) on top of 16-shot ImageNet pretraining for _cross-dataset_ transferability, and 

[MISSING_PAGE_FAIL:7]

generalization tasks. However, we observe in Table 1 that transductive gains sometimes decrease with the number of shots, presumably because data structure information can be partially captured in the shots. These results underline the value of considering the structure of the unlabeled test samples during prediction, especially on top of zero- and low-shot models or when facing domain shifts, an aspect not leveraged by the current zero- and few-shot VLM literature. More detailed results for five different backbone architectures and comparisons with unsupervised non-transductive methods are provided in Appendix C.1 for the zero-shot setting, in Appendix C.2 for TransCLIP on top of popular few-shot methods, in Appendix C.3 for cross-dataset transferability and in Appendix C.4 for domain generalization. _With its hyper-parameters unchanged_, TransCLIP exhibits strong generalization from convolutional networks to transformer-based models, as also depicted in Figure 1.

Transductive few-shot learning.We compare TransCLIP-FS, TransCLIP-FS without text regularization (i.e., \(=0\)) and state-of-the-art transductive few-shot methods. It is important to note that these few-shot methods were primarily developed for vision-centric tasks. Hence, they rely on visual information, omitting the textual elements. This allows us to study the impact of our text-based regularization term. Table 4 shows that incorporating language in the transductive paradigm boosts the performance over vision-only methods. Especially for the 1- to 4-shot settings, our language-driven KL penalty enhances the performance by a large margin on many tasks (e.g., ImageNet, SUN397, StanfordCars, DTD). As the number of shots increases, the text-driven penalty becomes less useful, especially for the datasets capitalizing on the visual shots rather than the text-encoder knowledge (e.g., EuroSat and Flowers). This points to promising future directions involving more flexible text regularization (e.g., an adaptable \(\) taking into account the number of shots and the quality of the text embeddings). Detailed results for five different encoder architectures are provided in Appendix C.5, consistently showing similar conclusions.

    &  &  &  \\   & & ImageNet & Adversarial & ImageNetV2 & Redation & Sketch & Average & Average & Average & OOD \\   & **CLIP-VI-VI6** \(\)s/ photo of a & 66 & 67.9 & 60.6 & 73.8 & 46.0 & 59.0 & 57.1 \\  & **+ TransCLIP-25** & 70.36\(\)32 & 49.54\(\)47 & 62.33\(\)70 & 57.92\(\)32 & 61.3\(\)42 & 61.3\(\)42 & 60.22\(\)21 \\   & **CLIP-VI-VI-6**\(\)s/ carbon repelers** & 66.8 & 50.6 & 62.3 & 77.8 & 48.4 & 61.6 & 59.8 \\  & **+ TransCLIP-25** & 71.92\(\)72 & 52.41\(\)64 & 68.42\(\)**51 & 78.16\(\)22 & 51.4\(\)27 & 60.24\(\)61 & 51.1\(\)33 & 61.1\(\)3 \\   & **CLIP-VI-6**\(\)s/ _empty training (C-D)_ & 71.9 & 49.4 & 41.1 & 75.1 & 47.2 & 5.0 & 5.0 & 5.0 \\  & **+ TransCLIP-25** & 73.33\(\)14 & 50.8\(\)**14 & 61.6\(\)**60.8** & 75.84\(\)**67 & 50.93\(\)**63 & 63.0\(\)**60 & 60.4\(\)**61 & 60.4\(\)**61 \\   & **CLIP-VI-6**\(\)s/ _target (TaskRees)_ & 71.0 & 50.3 & 65.6 & 77.8 & 49.2 & 63.2 & 64.7 \\   & **+ TransCLIP-25** & 74.14\(\)11 & 51.9\(\)**16 & 65.5\(\)**62 & 78.44\(\)**60 & 51.6\(\)**62 & 64.3\(\)**31 & 61.8\(\)**61 & 61.8\(\)**61 \\   

Table 3: Domain Generalization evaluation with improved manual prompting strategy (custom templates are given in Table 24b), 16-shot prompt-tuning and 16-shot adapter.

    &  &  &  \\   & & ImageNet & Adversarial & ImageNetV2 & Redation & Sketch & Average & Average & Average & OOD \\   & **CLIP-VI-VI-6**\(\)s/ photo of a & 66 & 66.7 & 47.9 & 60.6 & 73.8 & 46.0 & 59.0 & 57.1 \\  & **+ TransCLIP-25** & 70.36\(\)32 & 49.54\(\)47 & 62.33\(\)70 & 57.92\(\)32 & 61.3\(\)42 & 61.3\(\)42 & 60.22\(\)21 \\   & **CLIP-VI-6**\(\)s/ carbon repelers** & 66.8 & 50.6 & 62.3 & 77.8 & 48.4 & 61.6 & 59.8 \\  & **+ TransCLIP-25** & 71.92\(\)72 & 52.41\(\)64 & 68.42\(\)**51 & 78.16\(\)22 & 51.4\(\)27 & 60.24\(\)61 & 51.1\(\)33 & 61.1\(\)33 \\   & **CLIP-VI-6**\(\)s/ _empty training (C-D)_ & 71.9 & 49.4 & 41.1 & 75.1 & 47.2 & 5.0 & 5.0 \\  & **+ TransCLIP-25** & 73.33\(\)14 & 50.8\(\)**14 & 61.6\(\)**60.8** & 75.84\(\)**67 & 50.93\(\)**63 & 63.0\(\)**60 & 60.4\(\)**61 & 60.4\( with prompt learning.Following current VLMs literature, adapting the input prompt instead of GMM parameters could be seen as a more straightforward solution. For a fair comparison, we adapt Unsupervised Prompt Learning (UPL)  for the transductive setting and reevaluate its main hyper-parameter (see Appendix C.1). Table 5 shows clearly that TransCLIP outperforms UPL while being two to three orders of magnitude faster. Additional details on runtime are provided in Table 8 of the Appendix.

### Ablation studies

Components of TransCLIP.We study the impact of the principal components involved in the TransCLIP procedure over four diverse datasets. Table 5(a) shows that updating \(\) and \(\) allows to significantly boost TransCLIP's performance. This indicates the importance of having a dynamic parametric model instead of a fixed one. Table 5(b) demonstrates the critical role of text-driven penalty for TransCLIP in the zero-shot setting. Additional results on the sensitivity of \(\) in the few-shot setting are depicted in Figure 2 of the Appendix. Alongside the prior findings from Table 4, it is evident that incorporating text information is key to the success of TransCLIP and its wide applicability across the zero- and few-shot learning scenarios. The number of nearest-neighbors considered in the Laplacian term (Eq. (2)) does not make a significant difference in TransCLIP's performance as suggested by Table 5(c). However, removing the Laplacian regularization (Table 5(a)) leads to inferior results on some datasets such as ImageNet and EuroSAT. We choose to consider 3 nearest-neighbors to make the affinity matrix \(W\) sparse and reduce memory consumption. We also investigate the diagonal covariance matrix design by restricting it to be isotropic (i.e., \(=^{2}I_{d}\) with \(I_{d}\) the identity matrix). Table 5(d) shows that a non-isotropic \(\) performs better without significantly increasing the amount of trainable parameters.

Scaling to larger VLMs.We report TransCLIP-ZS performance on EVA-CLIP 8 billion parameter version  (approximately 42 times larger than the CLIP-ViT-B/16). It is worth mentioning that TransCLIP is easily applicable to multi-billion parameter models since it does not necessitate gradient computation or model parameter training (i.e., it only requires the memory needed for single-sample inference because the whole dataset processing can be performed one sample at a time). Table 7 shows that transduction can also bring significant improvements to larger models (details in Appendix C.1).

Table 6: Analysis on the components and sensitivity to hyper-parameters of TransCLIP-ZS.

Table 5: Performance and runtime comparison between TransCLIP and prompt learning solutions on average over ImageNet and the 10 fine-grained classification datasets. UPL\({}^{*}\) is a transductive adaptation of the original unsupervised procedure in , more details in Appendices C.1 and C.5.

## 5 Conclusion

In this work, we studied the transductive paradigm in the context of Vision-Language Models and proposed the TransCLIP method. Our algorithm is highly efficient, as it operates solely in the output embedding space (i.e., black-box setting), making it suitable for a wide range of models, including very large ones. This also enables TransCLIP to be compatible with models that are accessible only through APIs. We first showed how TransCLIP can bring transduction to the inductive zero-shot setting, achieving consistent gains without additional supervision. Then, we proposed a new setting that applies transduction on top of popular few-shot methods, offering a convenient strategy to combine computationally intensive supervised fine-tuning with efficient test-time transduction. Finally, we highlighted the limitations of current transductive few-shot methods and proposed a simple extension of TransCLIP to incorporate labeled samples. In all our experiments, TransCLIP's text-guided KL divergence term appears as a key factor in its success. Future work may focus on further enhancing this regularization term, for example, by making it more resilient (e.g., with adaptive class-wise weighting) when text prompts are less reliable.

## 6 Acknowledgments

M. Zanella and B. Gerin are funded by the Walloon region under grant No. 2010235 (ARIAC by DIGITALWALLONIA4.AI). The present research benefited from computational resources made available on Lucia, infrastructure funded by the Walloon Region under grant No. 1910247.

    &  &  &  \\   & & Zero-shot & w/ TransCLIP-ZS & relative \(\) & Zero-shot & w/ TransCLIP-ZS & relative \(\) \\  CLIP-ViT-B/16 & 177M & 66.6 & 70.3\(\)**3.7** & **+11**\% & 65.3 & 70.3\(\)**5.0** & **+14**\% \\ CLIP-ViT-L/14 & 427M & 72.9 & 77.2\(\)**4.3** & **+16**\% & 72.5 & 77.4\(\)**4.9** & **+18**\% \\ EVA-CLIP-SB & 7.5B & 82.5 & 84.6\(\)**2.1** & **+12**\% & 81.5 & 85.8\(\)**4.3** & **+23**\% \\   

Table 7: Performance of TransCLIP-ZS for increasingly large VLMs. Relative \(\) is the improvement normalized by the zero-shot error: \(_{}_{}\) / \((100_{}\).