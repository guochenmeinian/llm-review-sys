# Wai Lam\({}^{1}\)**  **Luo Si\({}^{2}\)**  **Lidong Bing\({}^{2}\)

From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader+
Footnote †: This work was supported by Alibaba Group through Alibaba Research Intern Program. The work described in this paper was also partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200620). \({}^{}\) This work was done when Weiwen Xu and Meng Zhou interned at Alibaba DAMO Academy. \({}^{}\) Xin Li is the corresponding author.

Weiwen Xu\({}^{1}\)\({}^{2}\) Xin Li\({}^{2}\) Wenxuan Zhang\({}^{2}\) Meng Zhou\({}^{3}\)\({}^{1}\)The Chinese University of Hong Kong

\({}^{2}\)DAMO Academy, Alibaba Group

\({}^{3}\)Carnegie Mellon University

{wwwu,wlam}@se.cuhk.edu.hk  mengzhou@andrew.cmu.edu

{xinting.lx,saike.zwx,luo.si,l.bing}@alibaba-inc.com

###### Abstract

We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR also has the potential to serve as a unified model for tackling various extraction and classification tasks in the MRC formulation.2

## 1 Introduction

Span extraction, such as Extractive Question Answering (EQA) and Named Entity Recognition (NER), is a sub-topic of natural language understanding (NLU) with the goal of detecting token spans from the input text according to specific requirements like task labels or questions . Discriminative methods were used to execute such tasks and achieved state-of-the-art performance. As shown in the left part of Figure 1, these works tailored a task-specific fine-tuning head on top of pre-trained language models (PLMs) to perform sequence tagging or machine reading comprehension (MRC) . The base PLMs are usually selected from pre-trained masked language models (MLM), such as RoBERTa  or BART  due to their comprehensive bi-directional modeling for the input text in the encoder. However, given the disparate nature of the learning objectives and different model architectures of MLM pre-training and task-specific fine-tuning, the discriminative methods are less effective for adapting MLMs to downstream tasks when there is limited fine-tuning data available, leading to poor low-resource performance .

As shown in the middle part of Figure 1, generative fine-tuning is a popular solution to mitigate the gap between pre-training and fine-tuning . This solution achieves remarkable few-shot performance in various span extraction tasks . Specifically, generative methods formulate the downstream tasks as a language modeling problem in which PLMs generate response words for a given _prompt_ (i.e., a task-specific template) as task prediction. Despite its success, tackling extraction tasks in a generative manner leads to several disadvantages. First, if it is used to generate the label token (e.g., "person" for PER entities) for a candidate span, the generative method needs to enumerate all possible span candidates to query PLMs . This requirement can be computationally expensive for tasks with a long input text, such as EQA. Second, if the desired predictions are target spans (e.g., the "answer" in the EQA task), generative methods usually need to explore a large search space to generate span tokens. Moreover, it is also challenging to accurately generate structured outputs, e.g., the span-label pairs in the NER task, with PLMs originally trained on unstructured natural language texts. These limitations impede PLMs from effectively learning extraction patterns from increased volumes of training data. As a result, even instruction-tuned large language models like ChatGPT3 are less effective than discriminative methods with smaller MLMs on extraction tasks .

To bridge the gap between pre-training and fine-tuning without suffering from the aforementioned disadvantages, we propose a novel Pre-trained Machine Reader (PMR) as a retrofit of pre-trained MLM for more effective span extraction. As shown in the right part of Figure 1, PMR resembles common MRC models and introduces an MRC head on top of MLMs. But PMR is further enhanced by a comprehensive continual pre-training stage with large-scale MRC-style data. By maintaining the same MRC-style learning objective and model architecture as the continual pre-training during fine-tuning, PMR facilitates effective knowledge transfer in a discriminative manner and thus demonstrates great potential in both low-resource and rich-resource scenarios. Given that MRC has been proven as a universal paradigm , our PMR can be directly applied to a broad range of span extraction tasks without additional task design.

To establish PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data based on Wikipedia anchors (i.e., hyperlinked texts). As shown in Figure 2, for each Wikipedia anchor, we composed a pair of correlated articles. One side of the pair is the Wikipedia article that contains detailed descriptions of the hyperlinked entity, which we defined as the _definition

Figure 1: Comparison among three fine-tuning strategies for NER and EQA, namely, vanilla discriminative fine-tuning, generative fine-tuning, and fine-tuning by using the proposed PMR.

_article_. The other side of the pair is the article that mentions the specific anchor text, which we defined as the _mention article_. We composed an MRC-style training instance in which the anchor is the answer, the surrounding passage of the anchor in the _mention article_ is the context, and the definition of the anchor entity in the _definition article_ is the query. Based on the above data, we then introduced a novel Wiki Anchor Extraction (WAE) problem as the pre-training task of PMR. In this task, PMR determines whether the context and the query are relevant. If so, PMR extracts the answer from the context that satisfies the query description.

We evaluated PMR on two representative span extraction tasks: NER and EQA. The results show that PMR consistently obtains better extraction performance compared with the vanilla MLM and surpasses the best baselines by large margins under almost all few-shot settings (up to 6.3 F1 on EQA and 16.3 F1 on NER). Additionally, we observe that sequence classification can be viewed as a special case of extraction tasks in our MRC formulation. In this scenario, it is surprising that PMR can identify high-quality rationale phrases from input text as the justifications for classification decisions. Furthermore, PMR has the potential to serve as a unified model for addressing various extraction and classification tasks in the MRC formulation.

In summary, our contributions are as follows. Firstly, we constructed a large volume of general-purpose and high-quality MRC-style training data to retrofit MLMs to PMRs. Secondly, by unifying pre-training and fine-tuning as the same discriminative MRC process, the proposed PMR obtains state-of-the-art results under all few-shot NER settings and three out of four few-shot EQA settings. Thirdly, with a unified MRC architecture for solving extraction and classification tasks, PMR also shows promising potential in explaining the sequence classification predictions and unifying NLU tasks.

## 2 Pmr

This section describes PMR from the perspectives of model pre-training and downstream fine-tuning. For pre-training, we first introduce the proposed model with the training objective of WAE and then describe the curation procedure of WAE pre-training data from Wikipedia. For fine-tuning, we present how PMR can seamlessly be applied to various extraction tasks and solve them in a unified MRC paradigm.

### Pre-training of PMR

PMR receives MRC-style data in the format of (\(Q\), \(C\), \(\{A^{k}\}_{k=1}^{K}\)), where \(Q\) is a natural language query and \(C\) is the input context that contains the answers \(\{A^{k}\}_{k=1}^{K}\) to the query. Each answer is a consecutive token span in the context, and zero (\(K=0\)) or multiple (\(K>1\)) answers may exist.

Model Architecture.PMR has two components: an MLM encoder and an extractor (Figure 3). The encoder receives the concatenation of query \(Q\) and context \(C\) as input \(X\) and represents each input token as hidden states \(H\).

\[ X&=[,Q,,C,]\\ H&=(X)^{M d}\] (1)

Figure 2: Construction of MRC-style data by using Wikipedia anchors.

where \([]\) and \([]\) are special tokens inserted into the sequence, \(M\) is the sequence length, and \(d\) is the dimension of the hidden states. The encoder \(()\) denotes any pre-trained text encoder for retrofitting, e.g. RoBERTa.

The extractor receives the hidden states of any two tokens and predicts the probability score that tells if the span between the two tokens should be output as an answer. We applied the _general_ way to compute the score matrix \(S\):

\[S=((H)^{T}H)^{M M}\] (2)

where **FFN** is the feed-forward network , and \(S_{i,j}\) is the probability to extract the span \(X_{i:j}\) as output. The _general_ way avoids creating a large \(^{M M 2d}\)-shape tensor of the _concatenation_ way , achieving higher training efficiency with fewer resources.

Training Objective.PMR is pre-trained with the WAE task, which checks the existence of answers in the context and extracts the answers if they exist. For the first goal, PMR determines whether the context contains spans that can answer the query:

\[L_{cls}=(S_{1,1},Y^{cls})\] (3)

where **CE** is the cross-entropy loss and \(S_{1,1}\) at the [\(\)] token denotes the query-context relevance score. If \(Y^{cls}=1\), the query and the context are relevant (i.e. answers exist). This task mimics the downstream situation in which there may be no span to be extracted in the context (e.g. NER) and encourages the model to learn through the semantic relevance of two pieces of text to recognize the unextractable examples.

Secondly, the model is expected to extract all correct spans from the context as answers, which can be implemented by predicting the answer positions:

\[L_{ext}=_{N<i j<M}(S_{i,j},Y^{ext}_{i,j})\] (4)

where \(Y^{ext}_{i,j}=1\) indicates that \(X_{i:j}\) is an answer to \(Q\), and \(N\) is the positional offset of the context in \(X\). Note that only \(X_{i:j}\) with \(N<i j<M\) are legal answer span candidates (i.e., spans from the context). MRC-NER  predicted the start and end probabilities as two additional objectives. However, we find that these objectives are redundant for our matrix-based objective and incompatible with multi-span extraction.

The overall pre-training objective \(L_{wae}\) is:

\[L_{wae}=L_{cls}+L_{ext}\] (5)

Figure 3: Model architecture of PMR. “-” indicates illegal candidate spans.

Data Preparation.MLM training can be easily scaled to millions of raw texts with a self-supervised learning objective . In contrast, training PMR in the MRC paradigm requires labeled triplets (query, context, and answers) as supervision signals, which is expensive to prepare for large-scale pre-training. To address this limitation, we automated the construction of general-purpose and high-quality MRC-style training data by using Wikipedia anchors.

As illustrated in Figure 2, a Wikipedia anchor hyperlinks two Wikipedia articles: the definition article that provides detailed descriptions of the anchor entity "Silicon", and the mention article where the anchor is mentioned. We leveraged the large scale of such hyperlink relations in Wikipedia as the distant supervision to automatically construct the MRC triplets. Specifically, we regarded an anchor as the MRC answer for the following context and query pair. The sentences surrounding the anchor in the mention article serve as the MRC context. The sentences from the first section of the definition article, which usually composes the most representative summary for the anchor entity , comprise the query. The query provides a precise definition of the anchor entity, and thus serves as a good guide for PMR to extract answers (i.e., anchor text) from the context.

Concretely, we considered sentences within a window size \(W\) of the anchor as the MRC context and used the first \(T\) sentences from the definition article as the query. Note that the context may cover multiple mentions of the same anchor entity. In this case, we treated all mentions as valid answers (i.e., \(K>1\)) to avoid confusing the model training. More importantly, the preceding scenario naturally resembles multi-span extraction tasks like NER. To prevent information leakage, we anonymized the anchor entity in the query by using "it" to substitute text spans that overlapped more than 50% with the anchor entity name. we did not use the "[MASK]" token because it does not exist in the data of downstream tasks.

In addition to the above answerable query and context pairs prepared through hyperlink relation, we introduced unanswerable examples by pairing a context with an irrelevant query (i.e., query and context pairs without the hyperlink association). The unanswerable examples are designed to help the model learn the ability to identify passage-level relevance and avoid extracting any answer (i.e., \(K=0\)) for such examples.

### Fine-tuning PMR for Extraction Tasks

We unified downstream span extraction tasks in our MRC formulation, which typically falls into two categories: (1) span extraction with pre-defined labels (e.g., NER) in which each task label is treated as a query to search the corresponding answers in the input text (context) and (2) span extraction with natural questions (e.g., EQA) in which the question is treated as the query for answer extraction from the given passage (context). Then, in the output space, we tackled span extraction problems by predicting the probability \(S_{i,j}\) of context span \(X_{i:j}\) being the answer. The detailed formulation and examples are provided in Appendix A.2.

## 3 Experimental Setup

Implementation.We used the definition articles of the entities that appear as anchors in at least 10 other articles to construct the query. As mentioned in Sec. 2, we prepared 10 answerable query and context pairs for each anchor entity. Then, we paired the query with 10 irrelevant contexts to formulate unanswerable MRC examples. The resulting pre-training corpus consists of 18 million MRC examples (6.4 billion words). We also tried various advanced data construction strategies, such as relevance-driven and diversity-driven ones, to construct query and context pairs. However, no significant performance gain is observed. A detailed comparison is provided in Appendix A.4.

The encoder of PMR is initialized with RoBERTa, a popular MLM with competitive downstream performance. The extractor is randomly initialized, introducing additional 1.6M parameters. In terms of the pre-training efficiency, with four A100 GPUs, it only takes 36 and 89 hours to complete 3-epoch training of PMR for base-sized and large-sized models, respectively. Additional data preprocessing details and hyper-parameter settings can be found in Appendix A.3.

Downstream Extraction Tasks.We evaluated two extraction tasks: EQA and NER.

**EQA:** We evaluated PMR on MRQA benchmark . For the few-shot setting, we used the few-shot MRQA datasets sampled by Splinter . Although **BioASQ** and **TbQA** are originally used for OOD

[MISSING_PAGE_FAIL:6]

model to achieve better transferability in an extremely low-resource EQA setting (i.e., 16 shot), processing such large output is far more complicated than the discriminative MRC and thus is prone to overfitting. MRC-based PMR demonstrates higher effectiveness in learning extraction capability from more training examples than FewshotBART, consequently yielding better performance on EQA when at least 32 examples are provided. Note that the compared baselines for EQA and NER are slightly different due to their different applicability. For example, UIE mainly emphasizes a structured prediction and is not applicable to complicated extraction tasks like EQA. EntLM, which aims to generate label tokens, is also not applicable to EQA. The findings further reveal that PMR can work reasonably well as a zero-shot learner (Appendix A.5).

OOD Generalization.Domain generalization is another common low-resource scenario in which the knowledge can be transferred from a resource-rich domain to a resource-poor domain. We evaluated the domain generalization capability of the proposed PMR on the MRQA benchmark. The MRQA benchmark provides meaningful OOD datasets that are mostly converted from other tasks (e.g., multi-choice QA) and substantially differ from SQuAD in terms of text domain.

Table 3 shows that PMR significantly surpasses RoBERTa on all six OOD datasets (+4.5 F1 on average), although they have similar in-domain performance on SQuAD. This finding verifies that our PMR with MRC-style pre-training can help capture QA patterns that are more generalizable to unseen domains. In addition, PMR with less than half the parameters of T5-v1.1, achieves a better generalization capability.

Full-resource Results.Although using the full-resource training data can alleviate the pretraining-finetuning discrepancy, MRC-style continual pre-training still delivers reasonable performance gains. As shown in Table 4, PMR achieves 0.9 and 1.5 F1 improvements over RoBERTa on EQA and NER, respectively. Further analysis shows that PMR can do better at comprehending the input text (Appendix 5.3). We also explore the upper limits of PMR by employing a larger and stronger MLM, i.e. \(_{}\), as the backbone. The results show additional improvements of our PMR over \(_{}\) on EQA (Appendix A.6).

  
**Model** & **Size** & **Unified** & **EQA** & **NER** \\  RBT-Post & 355M & ✗ & 81.9 & 79.8 \\ SpanBERT & 336M & ✗ & 81.7 & 77.3 \\ T5-v1.1 & 800M & ✓ & 82.0 & 76.0 \\ UIE & 800M & ✓ & - & 79.6 \\  RoBERTa & 355M & ✗ & 84.0 & 80.8 \\ PMR & 355M & ✓ & **84.9** & **82.3** \\   

Table 4: Full-resource results on EQA and NER. For EQA, we reported the average F1 score on six MRQA in-domain dev sets. For NER, we used four datasets.

  
**Model** & **Size** & **Unified** & **CoML** & **WNUT** & **ACE04** & **ACE05** & **Avg.** & **CoML** & **WNUT** & **ACE04** & **ACE05** & **Avg.** \\  RoBERTa & 125M & ✗ & 32.4,-2 & 29.73,-8 & 47.54,-5 & 48.13,-7 & 39.4 & 47.27,-4 & 35.12,-0 & 63.72,-4 & 58.27,-4 & 51.2 \\ RBT-Post & 125M & ✗ & 31.87,-8 & 28.73,-6 & 48.74,-3 & 44.34,-3 & 38.4 & 43.77,-1 & 35.04,-8 & 61.82,-2 & 58.32,-6 & 49.7 \\ EntLM & 125M & ✗ & **66.76**,-2 & 27.18,-8 & - - & - & - & - & - & - & - & - & - \\ UIE & 220M & ✓ & 52.03,-8 & 28.31,-4 & 43.13,-0 & 41.71,-0 & 66.52,-0 & 39.16,-5 & 52.48,-5 & 51.82,-5 & 52.5 \\ \(_{}\) & 125M & ✓ & 65.12,-4 & **40.83**,-1 & **65.35**,-5 & **60.72**,-9 & **58.0** & **73.92**,-4 & **41.13**,-0 & **70.71**,-5 & **68.0**,-1 & **63.4** \\ \(_{}\) & 355M & ✓ & 65.74,-3 & 40.53,-8 & 65.25,-6 & 66.12,-8 & 59.4 & 70.34,-4 & 46.92,-6 & 71.71,-5 & 69.92,-6 & 64.6 \\  RoBERTa & 125M & ✗ & 77.81,-8 & 47.91,-6 & 76.80,-3 & 74.40,-6 & 69.2 & 80.81,-5 & 50.61,-1 & 78.71,-3 & 77.90,-6 & 72.0 \\ RBT-Post & 125M & ✗ & 77.10,-4 & 45.81,-9 & 75.70,-7 & 73.60,-6 & 88.1 & 80.91,-0 & 49.82,-8 & 79.21,-4 & 77.51,-1 & 71.9 \\ EntLM & 125M & ✗ & 78.90,-9 & 24.40,-9 & - & - & - & - & - & - & - & - & - & - \\ UIE & 220M & ✓ & 79.91,-6 & 46.21,-2 & 68.00,-5 & 66.30,-8 & 65.0 & 83.20,-8 & 48.12,-7 & 74.06,-7 & 72.20,-4 & 69.6 \\ PMR\({}_{}\) & 125M & ✓ & **81.72**,-2 & **50.31**,-4 & **79.01**,- & **76.91**,-3 & **72.0** & **84.40**,-9 & **51.52**,-5 & **81.60**,-8 & **79.50**,-5 & **74.3** \\   

Table 2: NER results (F1) in four few-shot settings. EntLM is not applicable for nested NER tasks.

    & **Size** & **SQuAD** & **BioASQ** & **DROP** & **DuoRC** & **RACE** & **RE** & **TbQA** & **Avg.** \\  T5-v1.1 & 800M & 93.9 & **72.8** & 47.3 & 63.9 & **57.5** & 87.1 & **61.5** & 65.0 \\ RoBERTa & 355M & 94.2 & 65.8 & 54.8 & 58.6 & 49.0 & 88.1 & 54.7 & 61.8 \\ PMR & 355M & **94.5** & 71.4 & **62.7** & **64.1** & 53.6 & **88.2** & 57.5 & **66.3** \\   

Table 3: Performance on OOD EQA. We used the SQuAD training data to train the models and evaluate them on MRQA OOD dev sets.

[MISSING_PAGE_FAIL:8]

### Unifying Extraction and Classification with PMR

In the previous sections, we demonstrate that various extraction and classification tasks can be separately tackled in the same MRC formulation. We further explore the potential that fine-tuning a unified model for solving multiple tasks of different types.

Settings.We use two datasets, one from CoNLL NER (of extraction type) and the other from DREAM (of classification type), to train a multi-task model. For evaluation, we conduct three groups of experiments, where the models are evaluated on (1) Held-in: testing sets from training tasks, (2) Held-out Datasets: testing sets from other tasks of the same type with training tasks, and (3) Held-out Tasks: testing sets from unseen tasks

Results.As shown in Figure 4, the held-in results show that the multi-task fine-tuned RoBERTa suffers from a significant performance drop on DREAM compared to the single-task fine-tuned RoBERTa. This indicates that multi-task learning is difficult for a discriminative model if the task head is not well-trained. In contrast, the multi-task PMR is on par with PMR on CoNLL and slightly underperforms PMR on DREAM. Such a finding suggests that the MRC-style pre-training enhances PMR's capability to learn extraction and classification patterns from downstream NLU tasks, enabling PMR to serve as a unified model for solving various NLU tasks. Though generative models like T5-v1.1 (800M) could also unify NLU tasks through a conditional generation manner , the overall performance, especially the held-out performance, is lower than the smaller-sized PMR (355M). This suggests that the discriminative PMR may be better than generative models at unifying NLU tasks.

### Better Comprehending capability

To verify that PMR can better comprehend the input text, we feed the models with five different query variants during CoNLL evaluation. The five variants are:

* Defaulted query: "[Label]". [Label description]
* Query template (v1): What is the "[Label]" entity, where [Label description]?
* Query template (v2): Identify the spans (if any) related to "[Label]" entity. Details: [Label description]
* Paraphrasing label description with ChatGPT (v1): "[Label]". [Paraphrased Label description v1]
* Paraphrasing label description with ChatGPT (v1): "[Label]". [Paraphrased Label description v2]

In Figure 5, we show the statistic results of the three models on CoNLL when five different query templates are used respectively during evaluation. Among the models, PMR demonstrated significantly higher and more stable performance than RoBERTa and T5-v1.1. Such a finding verifies our assumption that PMR can effectively comprehend the latent semantics of the input text despite being rephrased with varying lexical usage from the default query used for fine-tuning models.

## 6 Related Work

Gaps between Pre-training and Fine-tuning.The prevailing approaches tackle span extraction tasks in a discriminative manner with tailored task-specific classifiers (e.g. tagging or MRC head)

Figure 4: The results of Held-in, Held-out Datasets, and Held-out Tasks evaluation. The six data points in the Held-in group denote the single-task fine-tuned performance of three models on two tasks respectively.

[19; 9; 12]. Recent research reported that generative fine-tuning methods can effectively bridge the gaps between pre-training and fine-tuning and achieve remarkable few-shot NLU performance by representing downstream NLU tasks as the same language model pre-training problem [49; 50; 34; 16; 35]. In the scope of span extraction, these works can be classified into three categories: generating label tokens based on the prompt span [10; 40], generating span tokens based on the prompt label [6; 38], and directly generating label-span pairs with soft prompt . Another way to mitigate the gap is to adapt MLMs into an appropriate paradigm for solving span extraction. Typically, this can be achieved through the use of task-specific data from similar tasks, referred to as "pre-finetuning" [24; 1; 68]. However, these methods may not be effective in domain-specific tasks or for non-English languages due to the lack of labeled data. Several studies leveraged abundant raw text to construct MRC examples for retrofitting MLMs [30; 18; 46]. By contrast, PMR employs ubiquitous hyperlink information to construct MRC data, which guarantees highly precise MRC triplets. In addition, PMR also provides a unified model for both sequence classification and span extraction, thereby enabling strong explainability through the extraction of high-quality rationale phrases.

Hyperlinks for NLP.Hyperlinks are utilized in two ways. First, hyperlinks can be regarded as a type of relevance indicator in model pre-training , passage retrieval [7; 51], and multi-hop reasoning [2; 66]. Second, the anchors labeled by hyperlinks can serve as entity annotations for representation learning [64; 5]. PMR is the first one to combine the advantages of both scenarios. In this work, we paired MRC query and context based on the relevance of hyperlinks and automatically labeled the anchors as MRC answers.

## 7 Conclusions

This work presents a novel MRC-style pre-training model called PMR. PMR can fully resolve the learning objective and model architecture gaps that frequently appear in fine-tuning existing MLMs. Experimental results from multiple dimensions, including effectiveness in solving few-shot tasks and OOD generalization, show the benefits of bridging the gap between pre-training and fine-tuning for span extraction tasks. PMR also shows promising potential in explaining the sequence classification process and unifying NLU tasks.