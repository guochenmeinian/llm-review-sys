# Q-VLM: Post-training Quantization for Large Vision-Language Models

Changyuan Wang\({}^{1}\), Ziwei Wang\({}^{3}\), Xiuwei Xu\({}^{2}\), Yansong Tang\({}^{1*}\), Jie Zhou\({}^{2}\), Jiwen Lu\({}^{2}\)

\({}^{1}\)Shenzhen International Graduate School, Tsinghua University, China

\({}^{2}\)Department of Automation, Tsinghua University, China

\({}^{3}\)School of Electrical and Electronic Engineering, Nanyang Technological University

{wangchan22@mails.,xxw21@mails.,tang.yansong@sz.}tsinghua.edu.cn; {jzhou@,lujiwen@}tsinghua.edu.cn; ziwei.wang@ntu.edu.sg

###### Abstract

In this paper, we propose a post-training quantization framework of large vision-language models (LVLMs) for efficient multi-modal inference. Conventional quantization methods sequentially search the layer-wise rounding functions by minimizing activation discretization errors, which fails to acquire optimal quantization strategy without considering cross-layer dependency. On the contrary, we mine the cross-layer dependency that significantly influences discretization errors of the entire vision-language model, and embed this dependency into optimal quantization strategy searching with low search cost. Specifically, we observe the strong correlation between the activation entropy and the cross-layer dependency concerning output discretization errors. Therefore, we employ the entropy as the proxy to partition blocks optimally, which aims to achieve satisfying trade-offs between discretization errors and the search cost. Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained decomposition of search space, so that the search cost is further reduced without harming the quantization accuracy. Experimental results demonstrate that our method compresses the memory by 2.78x and increase generate speed by 1.44x about 13B LLaVA model without performance degradation on diverse multi-modal reasoning tasks.1

## 1 Introduction

Large vision-language models (LVLMs)  have achieved outstanding performance in a large number of multi-modal reasoning tasks such as visual question answering , embodied instruction following  and robot navigation , which are benefited from numerous network parameters and vast training data. Despite of the high accuracy and generalization ability across different tasks, the extreme computational cost hinders the deployment on resource-limited mobile devices in wide realistic deployment scenarios. Moreover, LVLMs sequentially generate the response with multiple forward passes, which further increases the computation burden to accomplish the task. Therefore, it is highly demanded to reduce the model complexity of LVLMs in practical deployment.

To reduce the model complexity, model compression techniques have been presented to accelerate computation and save the storage space including pruning , quantization , low-rank decomposition  and efficient architecture design . Among these methods, quantization replaces the float numbers with quantized ones and substitutes multiply-accumulate (MAC) operations with integer arithmetic for significant efficiency enhancement. Due to the intractability of the training data and the unbearable training cost of LVLMs, post-training quantization  is leveragedto reduce bitwidths of weights and activations, which only searches rounding functions with a small calibration set with frozen network parameters. Searching rounding functions that minimize model prediction errors causes extremely high search cost due to the large space, and conventional methods [12; 29] sequentially search the layer-wise rounding functions by minimizing the activation discretization errors. However, ignoring cross-layer dependency of discretization errors fails to acquire the optimal rounding strategy and degrades the performance significantly.

In this paper, we present an accurate post-training quantization framework called Q-VLM to accelerate large vision-language models for efficient multi-modal reasoning. Different from existing methods which sequentially search the layer-wise rounding functions, we mine the cross-layer dependency of output discretization errors across layers, and employ the dependency to efficiently search the optimal rounding functions that minimize the quantization noise of the entire model. More specifically, we observe the significant correlation between the activation entropy and the discretization error dependency with the following layers. We then employ the entropy as the proxy to decompose the large search space from the entire model to smaller blocks containing multiple layers, and rounding functions are searched with the goal of minimizing the block-wise discretization errors. Therefore, the quantized model remains competitive performance with original full-precision counterparts with trivial additional search cost. Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained search space decomposition, so that precise rounding functions can be acquired with further reduced search cost. Our Q-VLM can still generate plausible response in multi-modal reasoning with 4-bit quantization because of the precise rounding functions, and compresses the memory by 2.78x and increase the generate speed by 1.44x about 13B LLaVA model. We evaluate our method with the LLaVA and the MoE-LLaVA models in different bitwidth settings, and the results in various visual question answering datasets demonstrate that our Q-VLM outperforms the state-of-the-art post-training methods significantly with negligible search overhead.

## 2 Related Work

### Large Vision-language Model

Large vision-language models (LVLMs) have achieved remarkable performance because of the fast adaptation on different downstream tasks with high generalization ability, which benefits from large-scale image-text pairs [39; 21] and strong generalization capabilities of pre-trained large language models (LLMs) [4; 40]. The instruction-following ability and multi-modal representations extracted by LVLMs are general across tasks, which are usually applied in a wide variety of multi-modal reasoning tasks such as visual question answering [45; 28], embodied instruction following  and robot navigation [2; 16]. Early attempts introduced rich commonsense in LLMs to vision-language representation learning, which effectively exploits LLMs by treating visual inputs as conditional information. In particular, BLIP [25; 24] leveraged data filtering techniques to enhance performance in tasks such as visual question answering (VQA) and image captioning. While these models exhibited extraordinary vision-language reasoning capabilities, their zero-shot abilities were limited due to the absence of explicit instruction during training. Recent studies including LLaVA  and InstructBLIP  aim to enhance LVLMs' zero-shot capabilities by aligning them more closely with human preferences. They finetuned LVLMs with visual instruction samples where the models were required to complete the human instruction according to the visual information. Despite the notable performance gains from the large model sizes, the computational complexity and the storage cost prohibit LVLMs from being deployed in resource-limited devices for realistic deployment. Lightweight LVLMs such as TinyGPT-V and TinyLLaVA  endeavors explore the extensive domain of large multimodal models focusing on leveraging small-scale models and achieve efficient LVLMs architecture designs. MoE-LLaVA  constructs a spare MoE-based model, which identifies a sparse pathway by simultaneously handling image and text features to achieve comparable performance with fewer activated parameters. However, the model inference cost still exceeds the resource budget of mobile devices or robots because of the low compression ratio.

### Post-training Quantization

Network quantization substitutes full-precision tensors with low-precision values and replaces multiply-accumulate operations with integer arithmetics, which significantly reduces the storage and computational cost of neural networks. Traditional quantization-aware training (QAT) methods [6; 32] need to finetune network weights with the full training set for rounding, which is less practical since the data and resources for training may not be accessible for most users. Recently, post-training quantization (PTQ) [11; 33; 44; 10; 42; 41] has aroused extensive interest, which leverages a small calibration set to search for the optimal threshold in rounding functions with significantly reduced data demand and optimization cost. Choukroun _et al._ minimized the \(l_{2}\) distance between quantized and full-precision tensors to mitigate evident task performance degradation, while Zhao _et al._ duplicated channels with outliers and halved their values to reduce clipping loss without amplifying rounding errors. Liu _et al._ preserved relative ranking orders of self-attention in vision transformers to mitigate information loss during post-training quantization and explored a mixed-precision quantization strategy based on the nuclear norm of attention maps and features. Zero-shot PTQ further extends the boundaries for efficiently quantizing neural networks without real image data. Cai _et al._ optimized pixel values of generated images to align sample batch statistics with those of batch normalization (BN) layers in full-precision networks. Li _et al._ extended the PTQ framework to transformer architectures by diversifying self-attention across different patches using patch similarity metrics. Meanwhile, deploying PTQ to large language models (LLMs) dynamically search the optimal rounding function for each input sample instead of applying a learnable one, because the activation distributes very significantly across different samples in large models. LLM.int8() , SmoothQuant  and ZeroQuant  handled activation outliers to achieve accurate quantization function learning by eliminating the extreme values with equivalent transformation, yet encountered challenges in effectively scaling to extremely large models due to the unbearable computational costs. Furthermore, GPTQ , AWQ , and QLoRA  deployed low-precision quantization on weight quantization to further reduce the computational complexity. However, these methods employ layer-wise searching strategy to search the rounding functions sequentially, which deviates from the optimal ones due to the lack of cross-block dependency.

## 3 Approach

In this section, we first introduce the preliminaries of post-training quantization for LVLMs and then detail the cross-layer dependency mining for LVLM quantization. Finally, we demonstrate the visual encoder optimization to minimize the quantization errors with negligible search cost overhead.

### Post-training Quantization for LVLMs

Network quantization decreases the bitwidth of weights and activations to save computation memory and accelerate inference speed. Conventional quantization-aware training (QAT) optimize all parame

Figure 1: The overall pipeline of our method. We employ entropy as the proxy to represent cross-layer dependency for efficient block assignment, which decomposes the large search space from the entire model to blocks containing multiple layers. Moreover, the visual encoder is further optimized for fine-grained search space decomposition.

ters in original full-precision networks for optimal quantization, which is unpractical because of the unacceptable training cost and inaccessibility of the large training datasets. Post-training quantization (PTQ) leverages a small calibration set \(X\) to search the optimal threshold in minor rounding functions with frozen network parameters, which significantly reduces data requirements and optimization costs. Specifically, the optimal solution for quantization function learning is acquired by minimizing the distribution discrepancy between quantized outputs and full-precision ones of the entire model. The optimization objective \(J\) can be formulated as follows:

\[_{\{Q_{k}\}} J=\|W_{q}^{(n)}X_{q}^{(n)}-W_{r}^{(n)}X_{r}^{(n)}\|_{2}^{2}\] \[s.t. X_{q}^{(k+1)}=Q_{k}(W_{q}^{(k)}X_{q}^{(k)})\] (1)

where \(W_{q}^{(k)}\) and \(X_{q}^{(k)}\) mean the quantized weights and activations for the \(k_{th}\) layer, and \(W_{r}^{(k)}\) and \(X_{r}^{(k)}\) represent their full-precision counterparts. \(Q_{k}\) means the rounding function for the \(k_{th}\) layer and \(n\) is the total number of the layers in the LVLM. Directly searching the optimal rounding function is NP-hard because the search space increases exponentially with the layer number. Therefore, conventional PTQ methods sequentially search the rounding functions by minimizing the quantization errors for each layer in the greedy way:

\[_{Q_{k}} J=\|W_{q}^{(k)}X_{q}^{(k)}-W_{r}^{(k)}X_{r}^{(k)}\|_{2} ^{2}\] (2)

where the layer index gradually increases to search the rounding function from bottom to top layers. However, the greedy search ignores the cross-layer dependency of discretization errors, which leads to accumulated discretization errors of model output even for the rounding function with small errors in the bottom layers.

### Mining Cross-layer Dependency for LVLM Quantization

Directly search the solution to (1) causes unacceptable search cost, while sequentially search the layer-wise quantization functions results in suboptimal solution. Therefore, we partition the entire model into different blocks consisting of multiple layers. Searching the optimal rounding function by considering the output quantization errors for each block achieves better trade-off between the search cost and the quantization accuracy, which is formulated as follows:

\[_{\{Q_{k}\} B_{i}} J=\|W_{q}^{(L_{i})}X_{q}^{(L_{i})}-W_{r}^{(L_{i})}X_{r}^{(L_{i} )}\|_{2}^{2}\] \[s.t. X_{q}^{(k+1)}=Q_{k}(W_{q}^{(k)}X_{q}^{(k)})\] (3)

where \(B_{i}\) represents the \(i_{th}\) block in our partition and \(L_{i}\) is the index of the last layer in block \(B_{i}\). Our goal is to obtain the optimal block partition for rounding function search, where we expect the dependency for layers in each block to be strong. Therefore, we can search the rounding functions for layers in each block by minimizing the discretization errors for the block output, and the output errors of the entire model are still minimized.

Explicitly evaluating the cross-layer dependency requires multiple forward pass of LVLMs for given input, where the correlation for discretization errors is calculated from data statistics. To avoid extremely high cost, we have to explore an efficient proxy to approximate the cross-layer dependency. We leverage Information Entropy of the activation to judge the sensitive layers with homogeneous distribution. For sensitive layers, the noise in the former layer caused by the deviation from the global optimal value usually leads to higher deviation for the current layer, so that jointly search these layers decrease the block output quantization errors. As a result, discretization error difference (DED) between layer-wise search and joint search is obvious

Figure 2: The correlation between discretization error difference (DED) and the activation entropy in 15th layer.

for sensitive layers with high entropy. In order to reduce the deviation from the optimal rounding points, we should search the quantization function by joint consideration of the current layer and the former one with strong dependency.

Meanwhile, we also empirically verified our assumption shown in Figure 2. Figure 2 is produced with the activations in the 15th layer of the LLaVA architectures on SQA dataset. The horizontal axis demonstrates the entropy of the activations, and the vertical axis depicts the discretization error difference (DED) between layer-wise search and joint search. Different markers mean different input multimodal samples, where DED and the activation entropy are strongly correlated. Therefore, the cross-layer dependency \(D(k,k+1)\) between the current layer \(k\) and the following layer \(k+1\) can be defined as follows:

\[D(k,k+1)=-_{ij}p(x_{q,ij}^{(k)},x_{q,ij}^{(k+1)})logp(x_{q,ij}^{(k+1)}|x_{q,ij}^{(k)})\] (4)

where \(^{k}\) represents the set of possible values about the quantized activation \(x_{q}^{k}\) and \(x_{q}^{k+1}\) in the \(k_{th}\) and \(k+1_{th}\) layer, and \(p\) means the probability of the variable. \(Q_{k}\) is searched with different quantization levels for rounding function optimization. We leverage uniform quantization to round the full-precision tensors due to the high compatibility with real hardware. In realistic implementation, each element is quantized to the nearest rounding point deterministically. In order to acquire the entropy of the quantized tensor, we approximate the deterministic quantization process with the following distribution:

\[p(x_{q,ij}^{(k)})=^{(k)}-q_{m})^{2}/)}{_{m=1}^{ M}(-(x_{r,ij}^{(k)}-q_{m})^{2}/)}(x_{q,ij}^{(k)}-q_{m})\] (5)

where \(\) is the interval between two consecutive rounding points, and \(\) represents the pulse distribution. \(q_{m}\) means the \(m_{th}\) rounding point in the quantization out of \(M\) quantization levels. The cross-layer dependency of two consecutive layers can be depicted by the discretization error difference (DED) between the rounding function searched sequentially and jointly, and larger difference indicates the former layer has significant influence on the discretization errors of the following one. Figure 2 demonstrates the positively correlation between DED and the activation entropy across different input samples with high correlation coefficients. Higher conditional entropy indicates that the activation distribution is homogenized with accumulated quantization errors, which represents larger cross-layer dependency for obvious influence with the following layers. To evaluate cross-layer dependency \(D(k_{r},k_{s})\) of non-consecutive layer \(k_{r}\) and layer \(k_{s}\), we consider the summation of entropy in all intermediate layers between them:

\[D(k_{r},k_{s})=-_{k=k_{r}}^{k_{s}}_{ij}p(x_{q,ij}^{(k)},x_{q,ij}^{(k+1 )})log(x_{q,ij}^{(k+1)}|x_{q,ij}^{(k)})\] (6)

Since searching rounding functions within blocks with large number of layers, we constrain the maximum layer numbers for each block. We partition blocks in the LVLM based on the acquired cross-layer dependency:

\[B_{i}=\{_{k=k_{r}}^{k_{s}}Q_{k}|D(k_{r},k_{s})>(k_{s}-k_{r})h_{0}\}\] (7)

If the average cross-layer dependency between two layers is larger than the threshold \(h_{0}\), all intermediate layers are assigned into a single block for joint rounding function search because the discretization errors of the block output are sensitive to all former layers within a block. Finally, we search the optimal rounding function by minimizing the output discretization errors of each block, where we select the optimal percentile \(p\) of the full-precision tensor distribution as the bounds for the uniform quantization.

### Optimizing Visual Encoders for LVLM Quantization

LVLMs leverage a visual encoder to extract informative representations for image input, and align visual embedding and text embedding with a projection layer. As the visual encoder significantly modifies the distribution of the activations in LVLMs, the rounding function in visual encoder can be optimized to minimize the activation entropy to enhance the searching efficiency. As a result, there are fewer layers in each block for rounding function search and the search cost remains low.

Simultaneously minimizing the activation entropy and weakening the cross-layer dependency for all layers causes optimization difficulties, because the large number of layers in LLaMA usually provide conflicted supervision for the visual encoder. Since different layers usually have various influence on the quantization errors of the output from the entire model, we assign different importance weights for the entropy minimization objective across layers which are acquired from the Jacobian:

\[L_{ent}=_{k=1}^{n}||}{ X_{r}^{(k)}}|| _{ij}p(x_{q,ij}^{(k)},x_{q,ij}^{(k+1)})logp(x_{q,ij}^{(k+1)}|x_{q,ij}^{(k)})\] (8)

where \(E^{(n)}\) represents the quantization errors of the final layer in the model. The Jacobian indicates the influence of the current layer to the quantization errors of the final output . Larger Jacobian magnitudes represent the higher influence on the overall discretization errors, and assigning larger weights to those layers can reduce the cross-layer dependency with fast model convergence. Meanwhile, our objective also includes the minimization of discretization errors for both the output of the visual encoder and the LVLM, which can enhance the quantization accuracy for visual representation learning and multi-model reasoning:

\[L_{err}=||X_{q}^{v}-X_{r}^{v}||+||X_{q}^{(n)}-X_{r}^{(n)}||\] (9)

where \(X_{q}^{v}\) and \(X_{r}^{v}\) respectively represent the quantized and full-precision output of the visual encoder, and \(\) is a hyperparameter to balance the importance between the discretization errors of the visual encoder and the LVLM. Finally, the overall objective for visual encoder optimization can be written as follows with the hyperparameters \(_{1}\) and \(_{2}\):

\[L=L_{reg}+_{1}L_{ent}+_{2}L_{err}\] (10)

where \(L_{reg}\) means the auto-regressive loss adopted in training original LVLM to minimize the discrepancy of predicted and target tokens. By optimizing the the visual encoder, we can search the rounding function in more fine-grained blocks with fewer layers to reduce the search cost, while the quantization accuracy still remains high due to the weak cross-layer dependency.

## 4 Experiments

In this section, we conduct extensive experiments for LLaVA and MoE-LLaVA benchmarks on ScienceQA multi-modal question answering dataset to evaluate the effectiveness of our methods. We first introduce the implementation details of our method. We then conduct ablation studies to evaluate the effectiveness of cross-layer dependency mining and visual encoder optimization. Finally, we compare our Q-VLM with the state-of-the-art post-training quantization methods to show its superiority.

### Implementation Details

We utilize the large vision-language frameworks for post-training quantization including LLaVA  and MoE-LLaVA  with their pre-trained weights for multi-modal question answering tasks. We set the bitwidth of quantized weight and activation to 6 and 4 to evaluate our method in different quality-efficiency trade-offs uniform quantization scheme where the interval between adjacent rounding points was equal. We followed the initialization of the quantization function parameters in QLoRA  for the baseline methods and our Q-VLM, where we minimized the lp distance [37; 29] between the full-precision and quantized activations to optimize the value range for clipping. We set the maximum layer depth to 3 within a block to achieve satisfying trade-offs between the discretization errors and the search cost. In the LVLM quantization exploration, we adjust hyperparameters \(p\) of percentile ranging from 1.0 to 0.98 with 0.005 interval for cross-layer dependency mining. Meanwhile, we modified the hyperparameter \(\) to demonstrate the effect of the discretization loss which optimizes the visual encoder in 9. For the parameter learning in LVLM quantization, we randomly select 64 vision-language pairs from the datasets for hyper-network learning where the batchsize was assigned with 8 for calibration set construction. The quantization function parameters were updated for 10epochs in searching process, and the acquired discretization function was directly employed for multi-modal question answering. The multi-modal answer reasoning dataset is ScienceQA , which contains 21k vision-language multiple choice questions. We also contain VizWiz  and VQA-v2  datasets.

### Ablation Study

Since previous layer-wise searching methods ignore cross-layer dependency, we employ joint searching strategy with dependency mining. In order to investigate the influence of the block-wise searching strategy, we vary the maximum number of layer depths contained in a single block with different trade-off between discretization errors and searching cost. Meanwhile, we adjust the searching space of percentile by modifying the hyperparameter \(p\) and \(\) for cross-layer dependency mining and visual encoder optimizing to select the optimal clipping range in quantization function. Finally, we randomly select 64 vision-language pairs from ScienceQA dataset to finetune quantized models and demonstrate the effectiveness of the proposed cross-layer dependency mining and visual encoder optimizing method. All experiments in the ablation study were conducted with ScienceQA dataset and the LLaVA-v1.3-7B framework.

**Performance w.r.t. the maximum layer depth within a block:** Integrating multiple layers with cross-layer dependency into a single block can exert considerable influence on the clipping and rounding errors throughout the entire model, albeit at the expense of exponentially escalating search costs. Figure 4(a) illustrates the answering accuracy for our method that astrtic different maximum joint layers, where the performance enhancement for layer depth exceeds 3 is slight with significantly increased complexity overhead. To ensure efficient quantization of LVLMs with sizable accuracy increase, we assign the maximum layer depth within a block to 3 in subsequent experiments.

**Performance w.r.t. different method we proposed in question answering process:** To verify the effectiveness of different method we proposed in LVLM quantization, we conduct the ablation study on ScienceQA dataset under different bitwidth. Table 4 illustrates the memory usage in inference, searching cost in calibration, and answering accuracy for our method under different bitwidth for LLaVA-v1.3-7B model. Observing the second rows, the cross-layer dependency mining(CDM) module is important for the final performance, because mining the cross-layer dependency and

   &  &  \\   & Memory & Search cost & Accuracy & Memory & Search cost & Accuracy \\  QLoRA & & 23.7 & 88.43 & & 23.5 & 77.53 \\ +CDM & 9.7G & 26.9 & 88.95 & & 25.9 & 78.66 \\ +VEO & & 24.1 & 88.72 & & 23.7 & 78.35 \\ Q-VLM & & 25.1 & 89.34 & & 24.6 & 79.79 \\  

Table 1: Effect of different LVLM quantization method we proposed. “CDM” means cross-layer dependency mining and “VEO” stands for visual encoder optimization. We report the result of LLaVA-7B model on ScienceQA dataset.

Figure 3: (a)The answering accuracy and searching cost w.r.t. different maximum layer depth within a block. (b) The answering accuracy and searching cost w.r.t. different hyperparameters across various vision-language models. (c) Quantization errors w.r.t. different maximum layer depth across various layers.

leveraging block-wise optimization strengthen the cooperation between layers and minimize the overall quantization errors. Visual encoder optimization (VEO) without cross-layer dependency also significantly modifies the distribution of the activations in LVLMs for discretization function learning. Q-VLM disentangles the cross-layer dependency for fine-grained search space decomposition, so that precise rounding functions can be acquired with further reduced search cost.

**Performance w.r.t. hyperparameters \(p\) and \(\):** The hyperparameters \(p\) control the search space for cross-layer dependency mining and \(\) balances the importance between the discretization errors of the visual encoder and the LVLM. Larger \(p\) leads to reduced rounding errors by diminishing the influence of outliers, although excessively large values result in excessive clipping of significant information in data distribution. Figure 5 depicts the answering accuracy for different hyperparameter settings, where the medium value for both parameters achieves the highest performance and achieves the trade-off between quantization errors and searching space.

**Visualization reasoning examples:** We further provide qualitative visual reasoning example of the LLaVA-v1.3-13B model in Figure 4 from ScienceQA dataset. Q-VLM improves the responses compared to AWQ baseline for W4A4 quantization setting, leading to more reasonable and particular answers for vision-language question pairs. In this first example, Q-VLM correctly answers the question about the highlighted ocean, while AWQ appears to have limited comprehension of the image information. For the second example, AWQ under W4A4 lost substantial information to produce sound reasoning about whether Fromia monilis cells make their own food, while Q-VLM answered correctly and even produced a table for precise classification about the six broad organisms called kingdom. Q-VLM improves the visual reasoning ability of LVLMs by reducing factual errors in the responses.

Figure 4: Visual reasoning examples from LLaVA-13B model. Q-VLM improves over the AWQ baseline for W4A4 quantization, reducing quantization errors and providing more reasonable answers. We color the text to show the correct or wrong responses.

### Comparison with the State-of-the-art Methods

In this section, we compare our proposed method with the state-of-the-art post-training quantization frameworks. As far as we know, we are the first to complete multi-modal LVLMs under W4A4 setting, so we conduct a series of baseline methods by combining the conventional state-of-the-art post-training quantization methods for fair comparison. For weight quantization, we follow the experiment setting of AWQ  and QLoRA . Meanwhile, as the activations in language modal exhibit significant variations in value range across different channels, we reproduce RPTQ  with per-channel activation quantization. As the activation distribution appears larger variance across different tokens in vision modal, we utilize per-token quantization following Outlier Suppression . The answering accuracy of the baseline methods is acquired by implementing the officially released code and pre-trained model.

Table 2 shows the comparison of top-1 accuracy of different post-training quantization methods across various LVLMs architectures including LLaVA-v1.3-7B, LLaVA-v1.3-13B  and MoE-LLaVA-1.6B , where bitwidths of weights for quantized layers select from 4 and 6. AWQ searched the optimal scale to protect the salient weight channels and decreased the quantization errors for weight quantization, while weight-only quantization scaling significantly increased outlier in activation and led to significant quantization loss. QLoRA with RPTQ employs sequentially search the layer-wise rounding functions by minimizing activation discretization errors. However, ignoring cross-layer dependency of discretization errors fails to acquire the optimal rounding strategy and degrades the performance significantly. On the contrary, our Q-VLM mines the cross-layer dependency of output distribution across layers and decomposes the large search space from the entire model to blocks containing multiple layers, which minimizes the block-wise discretization errors to avoid suboptimal quantization, and further optimizes the visual encoder to disentangle the cross-layer dependency for fine-grained search space decomposition. As a result, our method outperforms QLoRA by 2.26 (79.79 vs. 77.53) for answering accuracy in ScienceQA dataset under 4-bit in LLaVA-7B model. The computational cost remains the same for baseline methods and our Q-VLM due to the stored rounding parameters. The advantage of our method becomes more obvious for 4-bit LVLMs because quantization errors and cross-layer dependency are more important for networks with low capacity.

    & Bits & Method &  &  &  \\  & & & NAT & SOC & LAN & TXT & IMG & NO & \\   **P1-1** \\  } & FP & - & 89.39 & 96.06 & 85.64 & 88.71 & 87.65 & 88.50 & 89.81 \\   &  & AWQ & 85.39 & 92.01 & 83.27 & 84.80 & 83.54 & 85.99 & 86.23 \\  & & QLoRA & 88.45 & 94.71 & 84.45 & 87.63 & 86.07 & 87.87 & 88.43 \\  & & Q-VLM & 89.43 & 95.73 & 84.00 & 88.71 & 87.51 & 87.25 & **89.34** \\   &  & AWQ & 74.33 & 72.22 & 74.82 & 73.41 & 67.13 & 77.98 & 74.02 \\  & & QLoRA & 77.53 & 75.48 & 79.18 & 76.64 & 70.70 & 81.95 & 77.53 \\  & & Q-VLM & 80.86 & 75.93 & 80.73 & 80.01 & 72.48 & 83.90 & **79.79** \\   **P1-1** \\  } & FP & - & 90.19 & 93.14 & 87.09 & 89.39 & 87.06 & 89.83 & 90.00 \\   &  & AWQ & 88.03 & 92.60 & 84.00 & 86.02 & 85.18 & 86.41 & 87.57 \\  & & QLoRA & 88.87 & 92.89 & 85.64 & 87.59 & 86.56 & 87.53 & 88.87 \\  & & Q-VLM & 89.54 & 93.18 & 86.50 & 88.12 & 87.01 & 88.85 & **89.70** \\   &  & AWQ & 80.71 & 70.61 & 78.49 & 79.46 & 70.76 & 81.82 & 77.91 \\  & & QLoRA & 79.62 & 71.43 & 82.45 & 78.25 & 68.42 & 85.30 & 78.64 \\  & & Q-VLM & 82.55 & 73.32 & 83.18 & 81.03 & 70.82 & 86.74 & **80.78** \\   **P1-1** \\  } & FP & - & 64.01 & 58.57 & 63.30 & 62.80 & 54.78 & 66.97 & 62.68 \\   &  & AWQ & 59.83 & 57.78 & 61.48 & 58.94 & 52.95 & 64.25 & 59.83 \\  & & QLoRA & 62.98 & 57.78 & 61.85 & 61.87 & 53.99 & 65.37 & 61.60 \\  & & Q-VLM & 64.14 & 58.68 & 62.85 & 62.80 & 55.23 & 66.69 & **62.46** \\   &  & AWQ & 53.69 & 49.58 & 54.27 & 52.52 & 47.81 & 57.65 & 52.98 \\  & & QLoRA & 54.48 & 49.69 & 55.55 & 53.20 & 47.30 & 57.58 & 53.24 \\   & & Q-VLM & 55.06 & 51.94 & 56.27 & 54.57 & 48.03 & 58.34 & **54.72** \\   

Table 2: Comparisons with the state-of-the-arts post-training quantization methods for LLaVA-v1.3 and MoE-LLaVA models across bitwidth setting.Results (accuracy) on Science QA dataset. Question classes: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context.

We also evaluate our method on other datasets with different architectures to verify the generalization ability. Table 3 demonstrates the accuracy of different post-training quantization methods of LVLMs on the VQA dataset. Our method achieves the highest accuracy on different datasets, which means that our method can be robustly deployed in diverse downstream tasks. Table 4 depicts the efficiency and accuracy of different methods. Both AWQ and our method can reduce the search time and the inference memory compared with the original full-precision LVLMs, while our method can further reduce the memory because of the additional quantization of the CLIP. When deploying the quantized LVLMs on mobile devices, our method is more practical due to the limited memory footprint.

## 5 Conclusion

In this paper, we have presented a novel post-training quantization framework of large vision-language models for efficient multi-modal reasoning. Different from conventional methods which sequentially search the layer-wise rounding functions without considering cooperation between layers, we mine the cross-layer dependency and decompose the large search space from the entire model to blocks containing multiple layers. The proxy of entropy prompts the efficient and optimal block partition for rounding function search with the goal of minimizing the block-wise discretization errors. Therefore, the quantized model remains competitive performance with original full-precision counterparts while the search cost is low. Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained search space decomposition, so that precise rounding functions can be acquired with further reduced search cost. Extensive experiments demonstrate that our methods highly compress the large vision-language models without performance degradation and achieve higher answer reasoning ability than the state-of-the-art post-training quantization methods across large vision-language models with various architectures even under W4A4.

**Limitations:** One limitation of our work is that applying our framework to the setting of extremely low bitwidths degrade the performance very significantly, which is also a common issue in post-training quantization of foundation models. As foundation models are much larger than the resource limit of mobile devices, we will design efficient post-training quantization method to deploy the LVLMs on embedded equipment such as cellphones and wearable devices.