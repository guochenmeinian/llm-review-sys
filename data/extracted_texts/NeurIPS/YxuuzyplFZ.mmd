# _EyeGraph_: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking

_EyeGraph_: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking

 Nuwan Bandara\({}^{}\), Thivya Kandappu\({}^{}\), Argha Sen\({}^{*}\), Ila Gokarn\({}^{}\), Archan Misra\({}^{}\)

\({}^{}\)Singapore Management University, \({}^{*}\)Indian Institute of Technology, Kharagpur

\({}^{}\)Singapore-MIT Alliance for Research and Technology (SMART)

{pmnsbandara, thivyak}@smu.edu.sg, arghasen10@gmail.com,

ila.gokarn@smart.mit.edu, archanm@smu.edu.sg

###### Abstract

Continuous tracking of eye movement dynamics plays a significant role in developing a broad spectrum of human-centered applications, such as cognitive skills modeling, biometric user authentication, and foveated rendering. Recently neuromorphic cameras have garnered significant interest in the eye-tracking research community, owing to their sub-microsecond latency in capturing intensity changes resulting from eye movements. Nevertheless, the existing approaches for event-based eye tracking suffer from several limitations: dependence on RGB frames, label sparsity, and training on datasets collected in controlled lab environments that do not adequately reflect real-world scenarios. To address these limitations, in this paper, we propose a dynamic graph-based approach that uses the event stream for high-fidelity tracking of pupillary movement. We first present _EyeGraph_, a large-scale, multi-modal near-eye tracking dataset collected using a wearable event camera attached to a head-mounted device from 40 participants - the dataset was curated while mimicking in-the-wild settings, with variations in user movement and ambient lighting conditions. Subsequently, to address the issue of label sparsity, we propose an unsupervised topology-aware spatio-temporal graph clustering approach as a benchmark. We show that our unsupervised approach achieves performance comparable to more onerous supervised approaches while consistently outperforming the conventional clustering-based unsupervised approaches.

## 1 Introduction

Fine-grained, high-frequency eye tracking is increasingly of interest as an enabler of a wide variety of applications, such as biometric user authentication , foveated rendering for augmented and virtual reality , and monitoring of cognitive attention/overload . However, rapid and intricate eye movements  (with pupillary acceleration reaching values as high as \(24,000^{}/s^{2}\)), such as fixations (moments when the eyes are stationary and focused on a particular point), saccades (quick movements of both eyes between fixation points in the same direction), and microsaccades (small involuntary eye movements within fixation points) are difficult to capture with conventional RGB cameras  due to their poor temporal resolution, susceptibility to motion blur, and constrained capability to accurately detect and track pupils under low lighting conditions. In this paper, we explore the possibility of using Neuromorphic event cameras  as an alternative to traditional RGB-based eye tracking. Neuromorphic vision sensors capture changes in the visual scene asynchronously, only recording data when a significant event occurs, leading to a more efficient, high-frequency, and finer-grained depiction of eye movement dynamics.

Recent approaches for event-based eye-tracking predominantly adopt a supervised learning approach that uses either (i) RGB frames to guide pupil localization in event streams  or (ii)exclusive event-based pupil segmentation [35; 8]. Our main goal is to accurately and continuously record eye movements with high temporal resolution by (a) processing a stream of sparse events as asynchronously and temporally evolving graphs, and (b) adopting unsupervised modularity-aware graph representation learning to effectively cluster distinct ocular regions as a means for accurate and continuous pupil tracking. Our core idea is depicted in Figure 1.

In this paper, we specifically tackle four core shortcomings of event-based eye-tracking: (a) lack of event-based datasets that are representative of real-world scenarios (such as varying illuminance and user mobility - existing datasets are curated in a controlled lab environment with no head movements), (b) label sparsity (eye/gaze coordinates are only available on a fixed time interval, typically at coarser temporal resolution, e.g., EV-Eye  provides Points of Gaze at 100Hz), (c) dense 2D framed representation (events accumulated over a fixed time or event volume) that inadequately captures the underlying geometric, spatial, and temporal relationships, and (d) RGB-guided gaze inference (RGB cameras capture frames at fixed intervals, leading to a mismatch with the asynchronous nature of event-based sensors).

We make the following key contributions to address these limitations:

* To address the challenge (a), we present _EyeGraph_, a large-scale multi-modal near-eye tracking monocular dataset collected using a wearable event camera attached to a head-mounted device. It is important to note that the dataset was collected while mimicking in-the-wild settings, under varying ambient illuminance, and with individuals exhibiting unrestricted head and body movements.
* To tackle the challenges (b), (c), and (d) we adopt an unsupervised topology-aware graph-based approach. We propose (i) a novel temporally evolving dynamic graph representation for event-only eye tracking, and (ii) a novel topologically guided modularity-aware graph clustering approach that balances spatial proximity and temporal continuity of events. This approach ensures that the resulting clusters are accurately reflecting the sequential nature of eye movements.

To the best of our knowledge, our proposed benchmark is the first unsupervised event-based eye tracking in the literature.

## 2 Related Work

Event-based Supervised Eye/Gaze Tracking:Event cameras  mimic the human retina to record per-pixel changes in light intensity, yielding high \(O( s)\) temporal resolution, low \(O(mW)\) power consumption, and asynchronous and sparse event streams. Recent works on eye/gaze tracking explore (i) hybrid RGB+Event approaches to pupil detection (with RGB frames) and tracking (with events) for near-eye gaze estimation [17; 73; 33; 71; 2], and (ii) pure-event approaches that aggregate events into framed representations for inference by DNN-based pupil segmentation and tracking [59; 34; 9; 8; 35; 64] and traditional computer vision methods like ellipse fitting for pupil-related events . Hybrid approaches still rely on RGB frames, present lower perceptual throughput, and do not leverage the high temporal resolution of event data effectively. On the other hand, existing event-based processing suffers from challenges such as label sparsity, and inefficient framed representations.

Figure 1: Broad overview of our approach, starting from (A) and follows the proposed steps: (B) in Section 5, (E) in Section 6 and (G) in Section 6 towards (H) continuous event-based eye tracking.

Graph-based Object Tracking:Recent works have explored graph-based processing to maintain the sparse and asynchronous nature of event streams during processing and inference to improve _object tracking fidelity_. Early works in this paradigm construct _static, compact_ graphs from event streams over distinct chunks of time, sacrificing the low-latency nature of event data in favour of aggregating information from events within each group or time period [5; 44] for more efficient graph processing. In contrast, recent works explore dynamic graph construction strategies such as (i) sliding window-based event-by-event graph construction , (ii) spatio-temporal graphs constructed from density-insensitive downsampled key-events , and (iii) "evolving" spatio-temporal graphs which restrict recomputation of network activations only to those nodes which are affected by incoming events . These approaches reduce processing latency and computational complexity while maintaining tracking accuracy. However, Mondal et. al.  show how unsupervised clustering of events in graphs can support the detection of _distinct_ moving objects, resulting in superior object tracking fidelity. To achieve efficient graph clustering, recent works explore strategies such as (i) unsupervised graph pooling [61; 7; 6; 69], (ii) autoencoders [41; 48; 29], and (iii) learnt patched representations [63; 72].

Our work cuts across the two fields of work to propose a novel dynamic graph construction mechanism with unsupervised _topological_ clustering to efficiently isolate and track eye (pupillary) movement.

## 3 Motivation

### Monocular Eye Tracking

While correlated, _pupil trajectory_, which is the movement of the pupil within the eye over time, and _gaze direction_, which is the direction in which a person looks relative to their environment, represent different aspects of eye movement. As an example, changes in lighting conditions or cognitive load can cause fluctuations in pupil movement without necessarily corresponding to changes in gaze direction, whereas reflexive eye movements, such as saccades or smooth pursuit, can cause rapid changes in gaze direction while the pupil trajectory remains relatively stable. Since our objective in this work is to track the reflexive, physiologically-driven spatiotemporal dynamics of the pupil, we focus on tracking the pupil's spatial coordinates in monocular fashion, rather than the trajectory of the gaze direction.

Our approach of tracking a single pupil is based on the assumption that users demonstrate ideal conjugate eye movements, reflecting synchronized ocular motions consistent with typical oculomotor function. Further, the broader understanding of saccadic and smooth pursuit eye movements suggests that both eyes move in a highly coordinated manner due to their control by shared neural circuits. To this end, many works in ophthalmology research discuss the stability and coordination of saccadic eye movements and imply that both eyes generally maintain similar velocity profiles, with the role of dominance not significantly altering this aspect of eye movement [46; 56; 32]. In addition, several prior works have used monocular eye tracking for: (a) 3D gaze tracking [74; 42], (b) emotion recognition [70; 67], (c) cognitive modelling [38; 25; 23], (d) virtual and mixed reality , and (e) user authentication .

### Dynamic Graph Construction

Contrary to the conventional cameras (where the intensity of light across the visible spectrum incident on the sensor is captured at discrete points in time), event cameras or Dynamic Vision Sensors (DVS) only record changes in brightness (events) at each pixel asynchronously and with high temporal resolution, resulting in sparse data streams that encode motion and brightness changes in real-time. The event camera outputs a series of events on a per-pixel level - an event \(e_{i}\) (\(i\)) is denoted by a tuple \((x_{i},y_{i},p_{i},t_{i})\), where \((x_{i},y_{i})\) denotes the corresponding pixel coordinates where the event is generated, \(p_{i}\) represents the change in polarity (positive vs. negative), and \(t_{i}\) is the time of the corresponding event.

Recent works on event processing involve Graph Neural Networks (GNNs) to process events as "static" spatio-temporal graphs [53; 5; 45; 36; 75]. These graphs are inherently "sparse", capturing the essential spatial and temporal relationships with a focus on efficient computation and representation. Inspired by the Hebbian learning principle, "pixels that fire together wire together", in this work, to efficiently process the event stream, we propose a dynamic and temporally _evolving_ spatiotemporal graph-based approach with adaptive edge construction. More specifically, the adaptive edge construction framework focuses on a Gaussian Mixture Model (GMM)-based soft clustering approach to spatially group distinct macroscopic parts of the human eye. Subsequently, the edges in the temporal plane are formed by connecting the nodes that are both spatially and temporally together. Our premise is that by preserving the local and global structure of the eye anatomy and its movements, the dynamic graphs can accurately represent the movement of various parts of the eye.

### Unsupervised Topological Graph Clustering

Topological graph clustering partitions eye-tracking data into spatially and temporally coherent clusters, taking into account the modular structure of the underlying graph representation. It identifies densely connected subgraphs (clusters) that exhibit high within-cluster connectivity and low between-cluster connectivity. By considering both spatial proximity and temporal adjacency of nodes within the graph, the nodes within the same cluster are not only close in space but also temporally contiguous, reflecting the natural progression of gaze behavior over time. Further, we operate under the premise that each connected subgraph can represent a distinct anatomical or functional region of the eye, such as the pupil, upper eyel, or a segment of the eyebrow. To be more specific, different parts of the eye exhibit (a) unique shapes, and (b) movement profiles. This allows the use of topological clustering on dynamically evolving graphs to (a) ensure that nodes within clusters are spatially close, reflecting physical proximity on the eye (i.e., shape of the eye anatomy), and (b) ensure clusters represent continuous movements by considering temporal order of events (i.e., distinct movement profiles of each eye region). To have a better visual understanding, in Figure 1(B), we show (i) the cross-section view on the spatial plane where the GMM-based soft clustering approach helps to identify the volume of events that are triggered by the movements of iris (i.e., the events forming a circle/oval contour), and (ii) the 3-dimensional view where we can witness that the temporal trajectory of iris movement is represented as a connected and directed sub-graph.

## 4 _EyeGraph_: A Large-Scale Mobile Event Dataset

### Experimental Setup

During the data collection process, the participants wear a custom-built head-mounted device (HMD) equipped with a DAVIS\(346\) camera . The HMD was secured around the forehead using a Velcro fastener. The camera is positioned adjacent to the right eye, while the participants are directed to track the visual stimuli using their left eye. To elicit natural eye movements, the visual stimulus appears at the top left corner of the screen and then moves continuously in random directions. To guide the gaze movement of the participants, we displayed the visual stimulus on a \(1920 1080\), \(23.8\)-inch monitor. The distance between the monitor and the participant varied between \(45cm\) and \(50cm\), resulting in a field of view between \(56^{} 34^{}\) and \(62^{} 37^{}\), except when the participant moves freely. To collect reference for cross-modal investigations, the participants wear the off-the-shelf Pupil-Core eye tracker  at which their gaze is guided by replaying an identical visual stimulus.

To record eye-tracking data for a wider range of practical and in-the-wild conditions, as illustrated in Figure 2, we use three experimental setups: (i) _conventional lab settings_ - the participant is seated in an office environment (default illuminance) while watching the visual stimulus on a screen. The participants can move their heads without maintaining a fixed/rigid posture, (ii) _changing ambient

Figure 2: In-the-wild experiments under (a) lab settings; DAVIS\(346\) (b) lab settings; Pupil-Core eye tracker, (c) varying illuminance, and (d) user mobility and head/body movements.

_illuminance_: the experiment is repeated both under regular lighting (348 Lux) and under lower illuminance (24 Lux), with the corresponding _near-eye_ Lux values being 65 and 8 Lux, respectively, and (iii) _user mobility_ - the participants are asked to move around freely within the lab while carrying a laptop (\(3024 1964\), \(14\)-inch screen) that displays the visual stimuli, resulting in natural head and body movement.

### Dataset Collection

Our participant pool consists of 40 participants, including 28 males and 12 females, representing diverse ethnic backgrounds (ages ranging from 21 to 32 years, \(=26.08\) years and \(=2.99\)) 1. Prior to the data collection, the wearable devices were calibrated mechanically to ensure an optimal capture of each participant's eye region. The visual stimulus is a solid white circle (diameter 80 pixels) on a black background. While focusing their gaze on the white circle, each participant predominantly exhibited smooth pursuit and fixation states when the circle was moving smoothly. Similarly, saccadic states were triggered by the occasional discontinuous "jump" in the location of the white circle. The same experiment is repeated with variations in (i) the ambient illuminance from 348 Lux (default) to 24 Lux, and (ii) user movements.

### Dataset Characteristics

To our knowledge, _EyeGraph_ is the only dataset that comprehensively captures eye-tracking data under naturalistic indoor conditions. In Table 1, we present a comparative summary of _EyeGraph_ vs. four publicly available event-based eye-tracking datasets, highlighting the distinctive attributes and advantages of each. Further, a detailed analysis on _EyeGraph_ is presented in supplementary materials.

## 5 Dynamic Graph Construction

PrerequisitesA graph (\(G\)) is defined as \(G=(V,E)\) with vertices \(V=(v_{1},...,v_{n})\); \(|V|=n\) and edges \(E=(e_{1},...,e_{m}) V V\); \(|E|=m\). We denote the \(n n\) adjacency matrix (without self-loops) of \(G\) by \(\) where \(A_{ij}=1\) iff \(\{v_{i},v_{j}\} E\) given \(i j\), and \(A_{ij}=0\) otherwise. Further, each node is embedded with a \(d-\)dimensional feature vector \(_{v_{i}}^{d}\) whereas each edge is embedded with a scalar feature \(_{e_{i}}\). Further, the degree of a node \(v_{i}\) is defined as its number of connections: \(_{j=1}^{n}A_{ij}\) and a graph is said to be directed iff \( v_{i},v_{j} V;i,j\{k:k n\};i j:\{v_{i},v_{ j}\} E\) where \(\{v_{i},v_{j}\}\) is an ordered pair.

Problem FormulationConsidering each event is encoded in a tuple: \((x_{i},y_{i},t_{i},p_{i})\,\,i\), we represent raw events as a sparse and asynchronous spatio-temporal point cloud (as depicted in Figure 1) where events are represented as nodes in the graph. Therefore, the position of each node is denoted as \(_{v_{i}}=[_{1}t_{i},_{2}x_{i},_{3}y_{i}] ^{3}\) and the corresponding node feature vector is represented as \([_{1}t_{i},_{2}x_{i},_{3}y_{i},p_{i}]^{4}\) where \(_{1},_{2}\) and \(_{3}\) are normalization factors. This representation

   Feature & EBV-Eye  & EV-Eye  & 3ET  & 3ET+  & _EyeGraph_ \\  Tracking End Goal & Gaze & Gaze & Pupil & Pupil & Pupil \\ Representation & 2D frame & 2D frame & 2D frame & 2D frame & Graph \\ Learning & supervised & supervised & supervised & supervised & unsupervised \\ Has Grayscale/RGB Frame Data? & \(\) & \(\) & \(\) & \(\) & \(\) \\ Is data from human participants? & \(\) & \(\) & \(\) & \(\) & \(\) \\ Is Monocular? & \(\) & \(\) & \(\)/A & \(\) & \(\) \\ Is Multi-modal? & \(\) & \(\) & \(\) & \(\) & \(\) \\ Number of participants & 24 & 48 & N/A & 13 & 40 \\ Is head-movement allowed? & \(\) & \(\) & N/A & \(\) & \(\) \\ Accounts lighting changes? & \(\) & \(\) & \(\) & \(\) & \(\) \\ Accounts participant mobility? & \(\) & \(\) & N/A & \(\) & \(\) \\   

Table 1: Comparison of publicly available near-eye event datasets with _EyeGraph_allows us to effectively capture the temporal resolution of events (as opposed to the other event-based fixed representations depicted in Figure 1) resulting in efficient processing of the incoming events through sparse, but complete, graph updates . Inspired by the Hebbian learning principle, "pixels that fire together wire together", our goal is to construct event-based dynamic graphs where the edges are formed to preserve the local and global structure of the eye's anatomical parts. This facilitates the tracking of each anatomical part's movement over time, represented as evolving snapshots in the temporal domain.

Dynamic Theresholding-based Edge Construction StrategyThe existing works for event-based vision tasks using GNNs  mostly overlook the importance of edge construction mechanism, which is critical for preserving the object structure information . The conventional methods such as fixed-radius or k-nearest neighbours (kNN) are not specifically tailored to accommodate the unique characteristics of event vision (See Figures 3(a), 3(b), 3(c)). Hence, we construct edges based on a _dynamic threshold-based radius_, where the threshold changes based on the event volume and the movement dynamics.

To better understand the correlation between the events, in Figure 3(d), we depict the distribution of the Euclidean distances between spatio-temporal points. These distances can be represented as a mixture of Gaussian distributions, arising from the distinct eye parts and their varying movement profiles. We thus use Equation 1 for constructing the edges over a fixed event volume.

\[F(c,)=f_{k\,1 k c}^{2}(k,f_{ v _{i},v_{j} V}^{1}[\|_{v_{i}}-_{v_{j}}\|])\] (1)

where \(f^{1}(.)\) is the transformation function to retrieve the upper triangular matrix such that \(D=[d_{ij}]\) iff \(d_{ij}=0\, i j\) given \(d_{ij}=\|_{v_{i}}-_{v_{j}}\|\) and \(f^{2}(.)\) is the Gaussian mixture model (GMM) fitting function : \((_{a},_{a})=_{a=1}^{k}_{a}(x|_{a}, _{a}),\,1 k c\); with the objective of Bayesian information criterion (\(BIC\)) approaching minimum: \(BIC[F(.)]\) (see supplementary materials for details). In addition, \(c\) is the maximum number of clusters to be considered in GMM fitting which is heuristically determined as \(5\) due to the prominent anatomical clusters available in near-eye tracking: pupil, iris, lower and upper eyelids/lashes, and eyebrows. \(\) denotes the set of node positions: \(=(_{v_{1}},...,_{v_{n}})\).

After GMM fitting, the dynamic threshold for the accumulated event volume is set by considering the statistical relevance: \(_{1}= min(_{i}-3_{i})\) where \(_{i}\) and \(_{i}\) are the mean and standard deviation of each fitted Gaussian distributions where the number of Gaussian distributions lies in \([1,c]\). Here, \(\) is the scaling factor. Subsequently, this dynamic threshold is utilized in constructing radius-graph edges in the spatial plane: \(\{v_{i},v_{j}\} E\, t_{i}=t_{j}:\| _{v_{i}}-_{v_{j}}\|_{1}\) with a node degree condition: \(_{j=1}^{n}_{ij} N\) to regularize the graph where \(N\) is the allowed maximum degree. However, the edges in the temporal domain are constructed by following Equation 2 in a directed fashion to reflect the evolving nature of the graph in time.

\[\{v_{i},v_{j}\} E\, t_{i}<t_{j}\,\, _{1}(t_{j}-t_{i})_{2}\,(x_{j},y_{j})\{(x_{k},y _{k})|k[i-,i+],\}\] (2)

where \(_{2}\) and \(\) are heuristically determined such that the evolving present and past events are well-connected, both spatially and temporally, in the node neighbourhood during the graph construction.

Figure 3: An artificially set-threshold in radius graphs results in either (a) more disconnected components (i.e., global structure loss) or (b) more unintuitive edges (i.e., local structure loss) especially when the eye movement dynamics rapidly change. (c) kNN graphs are also susceptible to blur structure information of the events, due to the necessity to create edges up to an artificially-set threshold. (d) spatio-temporal distance distribution of an event volume.

Edge Feature AssignmentIn our approach, edge features are utilized to represent the relative movement profiles of eye parts. To capture the influence of historical events holistically, we adopt a Hawkes process-based  attribution mechanism. The pseudo-code for the proposed edge feature assignment is depicted in Figure 4(a).

## 6 Unsupervised Topological Clustering

Problem FormulationAs empirically observed in Figure 1, event-based eye movement data suggests an underlying anatomical structure of the eye within the spatial plane. Simultaneously, it maintains the continuous movement dynamics of each component of the eye across the temporal domain. Consequently, it appears intuitive to use spatio-temporal clustering of the nodes within the graph representation, with the intent of extracting distinct movement profiles and dynamics for each anatomical part. To this end, our goal of clustering is to develop a graph partitioning function (without any label-support, i.e., unsupervised), \(:V\{1,...,c\}\) s.t. \(V_{i}=\{v_{j}:(v_{j})=i\}\) to split the set of nodes into \(c\) partitions via a graph encoder \(f_{}\) that maps the graph space (\(G\)) and the corresponding low-dimensional latent vector space: \(f_{}(G)=^{n d_{h}}\) (where \(=\{z_{i}\,|\,i[1,n]\}\) and \(d_{h}\) is the embedding dimension), ensuring the (i) anatomical topology of the eye with the temporal evolution are well-preserved while (ii) the cluster quality in terms of modularity measure is maximized.

Variational Graph AutoencoderTo learn the \(f_{}\), we implement a variational graph autoencoder (VGAE)  due to their demonstrated performance in highlighting the topology of the graphs through edge reconstruction. The encoder of the proposed VGAE is comprised of graph convolutional network layers  to generate node embeddings through the message passing rule: \(^{(l+1)}=(}^{(l)}^{(l)})\) for \(l\{0,..,L-1\}\). In summary, the encoder can be modelled as \(q(\,|\,,)=_{i=1}^{n}(z_{i}\,|\, _{i},diag(_{i}^{2}))\) while the decoder is \(p(\,|\,)=_{i=1}^{n}_{j=1}^{n}p(A_{ij}\,|\,z_{i}, z_{j})\) with \(p(A_{ij}=1\,|\,z_{i},z_{j})=(z_{i}^{}z_{j})\).

Learning ObjectivesWe propose a joint objective function to guide the model to learn topological information  embedded with the graph which maps to eye anatomy while maximizing the modularity  such that the edges fall within clusters are dense whereas edges between clusters are sparse. Therefore, we implement the weighted objective function in Equation 3.

\[L=_{1}_{q(\,|\,,)}[ p( \,|\,)]-_{2} ^{})}{2m}-_{3}KL[q(\,|\,,)\|p( )]\] (3)

where \(KL[q(.)]\|p(.)]\), \(\) and \(_{i;i\{1,2,3\}}\) are the Kullback-Leibler (KL) divergence (between \(q(.)\) and \(p(.)\)), modularity matrix and scaling factors respectively. Here, topological guidance and modularity maximization are achieved through the edge reconstruction loss (i.e., first term) and pairwise cluster membership loss (i.e., second term) respectively, while KL divergence (i.e., third term) works as a regularizer.

Pupil Coordinates EstimationThrough our empirical evaluations, we observe that the iris and pupil collectively exhibit a tunnel (i.e., a combination of cylinders and tori)-like movement profile in the spatio-temporal event cloud and thereby in the constructed graph representation (as depicted

Figure 4: PyTorch-style pseudo-codes for (a) Hawkes-based edge feature attribution, (b) RANSAC-based pupil coordinates estimation.

in Figure 1). Therefore, after assigning the nodes in the graph into mutually-exclusive clusters and with the premise that the targeted movement profile of iris and pupil are separated into one cluster, we implement a custom tube-model-based random sample consensus (RANSAC)  method to estimate the center-line movement of the pupil. The pseudo-code of the proposed method is depicted in Figure 4(b).

## 7 Experiments and Results

### Baseline Methods

We use the following baseline edge construction mechanisms to evaluate the performance on predicting the pupil coordinates: (i) **Fixed-radius graph** (\(\{v_{i},v_{j}\} E\,\,\|_{v_{i}}-_{v_{j }}\|_{d}\)), (ii) **kNN graph**, (iii) **Nearest neighbour graph** (NN), (iv) **Furthest point sampling graph** (FPS) , and (v) **Gabriel graph**.

The following clustering (CL) and supervised (SP) baselines are compared against our proposed approach for graph clustering-based pupil coordinate estimation (see supplementary materials for details): (i) Using spatio-temporal \(\) as features for CL: **k-means** (with local Lloyd algorithm  and k-means++ seeding strategy ), **Affinity Propagation**, **Meanwhile**, **Spectral Clustering**[58; 12], **DBSCAN**; (ii) Using graph structure for CL: **SBM**; (iii) using spatio-temporal \(\) as features and (graph) structure for CL: **Vanilla Graph Autoencoder** (GAE) , **GSCEvent-Mod**, **DMoN**, **DGI**; (iv) Non-graph based pupil coordinate estimation methods using SP techniques: **MambaPupil**, **bigBrains**, **FreeEVs**, **GoSparse**, **Efficient**.

### Other Datasets

In addition to the _EyeGraph_ dataset, we evaluate the performance of dynamic graph construction and graph clustering techniques on the publicly available EBV-Eye dataset . EBV-Eye does not contain pupil coordinate labels as the dataset end-goal is gaze tracking. To evaluate the accuracy of our proposed unsupervised pupil localization, we use 3ET+ dataset . This dataset was recently used in the AIS 2024 challenge  and we use several supervised methods proposed as part of the challenge to compare the performance of _EyeGraph_. 3ET+ consists of human-labelled ground-truths for pupil coordinates at \(100Hz\).

### Evaluation Metrics

Since our goal is to build modularity and topology-aware clustering capability where each cluster represents a distinct anatomical region of the eye, we employ the clustering quality as the primary evaluation target. Therefore, we utilize four distinct metrics for evaluating the cluster quality: (i) mean Silhouette coefficient (SC) , (ii) Davies-Bouldin score (DB) , (iii) modularity (Mo) , and (iv) conductance (Cd) . Further, to compare with supervised approaches for pupil coordinate estimation, we use the p-accuracy , mean Euclidean (\(l_{2}\)) and Manhattan (\(l_{1}\)) distances following the literature.

### Results

As tabulated in Table 2, our method for dynamic graph construction achieves superior or comparable performance across all evaluation metrics and various datasets. We observe that our method achieves higher SC and Mo scores, and lower Cd scores, indicating that dynamic graph construction helps in accurately representing the event-based eye region data. Similarly, in Table 3, for graph clustering, our proposed unsupervised topology-guided clustering consistently achieves higher Mo scores, hinting that the resultant spatio-temporal clusters are well separated. Further, in Table 4, we compare _EyeGraph_ approach against supervised and standard unsupervised methods in estimating the pupil coordinates. While _EyeGraph's p\(10\)_ accuracy is \(\) 8% lower than the best performing supervised approach proposed in , it performs superior among the unsupervised methods. In addition, we can see that the incorporation of our clustering approach is beneficial even to the DMoN approach, as it helps achieve 14% improvement in the p10 value. Further, since our dataset is collected using a mobile setup, accounting varying mobility and ambient lighting conditions, we evaluate the robustness of the proposed method in compared with the existing supervised methods (in Table 4). We see that,even under poor lighting and conditions that include severe motion artifacts, we achieve an accuracy of 93.67%, indicating its capability to accurately represent and track event-based eye movement data. In comparison, even though MambaPupil achieves slightly better performance i.e., 2% improvement in p10 value, it is noteworthy that our approach does not demand any extensive labeling effort to achieve comparable performance.

## 8 Discussion and Conclusion

LimitationsWhile our results show the superior performance of _EyeGraph_ in unsupervised eye-tracking, we identify the following open areas to be investigated. _EyeGraph_ currently uses wearable-based near-eye tracking of only one eye, and thus cannot directly take advantage of gaze-related features such as saccades and fixation. Our benchmark technique assumes that the users typically tend to exhibit conjugate eye movement, moving both eyes in tandem having similar velocity profiles. However, it is possible that microscopic distinctions may exist between the pupillary movements of the right and left eyes, perhaps because of the differences in ocular muscle strength (most people have one dominant eye). In addition, we recognize the critical importance of binocular eye tracking data to support physiological conditions/medical diagnostics use cases, such as capturing neurodevelopmental disorders. Conversely, monocular tracking proves to be sufficiently effective in applications where depth perception is not a priority, such as user authentication, human-computer interaction, and emotion recognition. Further, the users are to follow the visual stimuli that randomly move across the screen in various directions. However, to mimic more in-the-wild settings, the data stimuli should involve natural cues such as interacting in the physical world, reading articles etc.

Future Works and ApplicationsWe are working to augment our dataset, and/or release new iterations, with more naturalistic, but visual stimuli-driven, in-the-wild studies in both indoor and outdoor environments so as to capture finer-grained continuous variation in illuminance, and during

    &  &  \\  Method & SC\(\) & DB\(\) & Mo\(\) & Cd\(\) & SC\(\) & DB\(\) & Mo\(\) & Cd\(\) \\  Radius (\(=10^{-4}\)) & 0.55 & 0.99 & 61.34 & 14.89 & 0.47 & 0.93 & 63.43 & 18.05 \\ Radius (\(=10^{-1}\)) & 0.53 & 0.92 & 62.45 & 14.90 & 0.42 & 0.99 & 63.85 & 13.14 \\ Radius\({}^{}\) (\(_{sd}=10^{-3}\)) & 0.64 & 0.97 & 68.80 & 12.45 & 0.49 & **0.90** & 67.86 & 10.71 \\ kNN (\(k=8\)) & 0.62 & **0.89** & 69.30 & 16.34 & 0.43 & 1.02 & 70.11 & 6.21 \\ NN & 0.61 & 1.02 & 68.30 & 15.74 & 0.52 & 0.91 & 63.26 & 6.82 \\ FPS  & 0.52 & 1.03 & 60.54 & 15.49 & 0.41 & 1.34 & 68.45 & 15.87 \\ Gabriel  & 0.59 & 1.01 & 63.23 & 12.10 & 0.52 & 1.04 & 72.45 & 20.69 \\ Ours & **0.66** & 0.91 & **69.34** & **11.30** & **0.54** & **0.90** & **75.70** & **5.07** \\   

Table 2: Performance of dynamic graph construction on _EyeGraph_ and EBV-Eye using CL metrics. \({}^{}\)3d spatio-temporal edges; instead of the proposed dual-step edges.

    &  &  \\  Method & SC\(\) & DB\(\) & Mo\(\) & Cd\(\) & SC\(\) & DB\(\) & Mo\(\) & Cd\(\) \\  kmeans  & 0.30 & 0.99 & 49.45 & 30.23 & 0.31 & 1.56 & 54.56 & 20.45 \\ Affinity  & 0.31 & 1.20 & 40.67 & 32.37 & 0.29 & 1.67 & 50.34 & 27.88 \\ Meanshift  & 0.33 & 1.33 & 40.50 & 28.64 & 0.40 & 1.34 & 55.89 & 19.80 \\ Spectral  & 0.39 & 1.44 & 52.34 & 27.46 & 0.36 & 1.48 & 42.67 & 17.88 \\ DBSCAN  & 0.40 & 1.18 & 55.34 & 20.76 & 0.41 & 1.62 & 60.24 & 15.78 \\ SBM  & 0.45 & 1.20 & 60.36 & 17.77 & **0.56** & 1.22 & 62.88 & 14.90 \\ GAE  & 0.53 & 1.03 & 57.90 & 18.44 & 0.51 & 1.20 & 65.65 & 16.68 \\ GSCEVentMod  & 0.51 & 1.00 & 62.67 & 12.07 & 0.50 & 1.02 & 67.88 & 10.34 \\ DGI  & 0.67 & 0.93 & 67.04 & **10.55** & 0.50 & 1.30 & 70.34 & 5.46 \\ DMoN  & **0.68** & **0.90** & 68.80 & 13.00 & 0.54 & **0.82** & 69.46 & 9.36 \\ Ours & 0.66 & 0.91 & **69.34** & 11.30 & 0.54 & 0.90 & **75.70** & **5.07** \\   

Table 3: Performance of the proposed graph clustering on _EyeGraph_ and EBV-Eye using CL metrics diverse set of physical activities. In addition, in our future data collection efforts, we plan to include more natural cue-based stimuli, via real-world interaction scenarios instead of the utilized controlled visual stimuli. Further, we believe that _EyeGraph_ dataset will also be instrumental in studying fine-grained pupillary movements for diverse applications such as continuous biometric authentication and affective-cognitive modelling. In contrast, we hope that our _EyeGraph_ methods will also be useful in other domains such as automotive vision and robotics.

ConclusionIn this paper, we present _EyeGraph_, a large-scale multi-modal near-eye tracking dataset collected using a wearable event camera attached to a head-mounted device. We then propose an unsupervised topology-aware spatio-temporal graph clustering approach as a benchmark. Using extensive evaluations, we show that our unsupervised approach achieves comparable performance against the supervised approaches while consistently outperforming the conventional clustering approaches.

AcknowledgementsThis work was supported by both the Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant (Grant ID: 22-SIS-SMU-044), and by the National Research Foundation, Prime Minister's Office, Singapore under its NRF Investigatorship grant (NRF-NRFI05-2019-0007). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.