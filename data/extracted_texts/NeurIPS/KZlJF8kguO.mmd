# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

multimodal, and uses intracranial recordings -- a high-spatial and high-temporal resolution recording method.

The Brain Treebank is foremost a treebank, like the Penn Treebank, and is annotated in the universal dependencies (UD) format. What distinguishes it, is that it is accompanied by both multimodal annotations and by neural recordings collected from 10 subjects who heard 223,068 annotated words while they watched Hollywood films. Subjects watched a total of 26 films (43.5 hours) as data was recorded from a total of 1,688 electrodes. To this, we add manual and automated annotations.

**Scene labels**: every scene in the movie was labeled according to the Places365 schema, , resulting in 46,935 scenes total. **Word onsets and offset**: while automatic speech recognition performs acceptably, errors are common, which were manually corrected. In addition, automated systems are simply not trained to offer extreme accuracy, at the millisecond level, when determining the start and end of words. Word onsets had to be manually annotated on spectrograms for every word to ensure alignment with the neural recordings. **Part of speech (POS) tags and parses**: Sentences were automatically parsed into the Universal Dependencies framework and then each part of speech tag and dependency relationship was manually corrected. While POS tagging is fairly accurate, numerous parser errors existed. **Speaker identity**: A unique identifier, which can be traced back to a given character, was given to every speaker in every movie. This was done manually, as no automated system exists to do so with any reasonable accuracy. Finally, we also curated a list of 16 automated video, audio, and language features that we provide to save computing time (see Table 4). We release all our data with a Creative Commons Attribution 4.0 International (CC BY 4.0) license.

Large scale stimuli for the neuroscience of language and multimodal understanding can enable natural experiments: the kind of post-hoc analysis of large-scale datasets that has propelled NLP and machine learning in general forward. In the long term we hope that treebanks such as ours, coupled with neural recordings, will help the creation of theories of language understanding that span linguistics, neuroscience, and NLP. To demonstrate the utility of the dataset, in addition to providing the raw data, we also take new steps toward understanding language in the brain; our contributions are:

1. A dataset of intracranial recordings across 26 different movie viewings (43.5 hours total).
2. Localization of electrode positions and alignment with common brain atlases.
3. Multiple layers of manual annotations to enable numerous experiments: scene labels, word onsets and offsets, part of speech tags, parses in universal dependencies format, and speaker identity.
4. Multiple automated annotations for 16 other language, audio, and visual features.
5. Quantitative results that show neural responsiveness to word onset and differential activation based on the position of a word within a sentence.

## 2 Related work

Previous works have studied language processing in the context of Magnetoencephalography (MEG) [17; 18], Electroencephalography (EEG) [19; 20], and functional magnetic resonance imaging (fMRI) [21; 22; 23; 24; 25; 26]. In this work, we present Stereoelectroencephalography (sEEG) data with both high temporal resolution and naturalistic stimuli.

Recording the brain's response to naturalistic stimuli is critical to neuroscientific progress . There exist fMRI datasets for naturalistic speech [28; 29; 30], vision [31; 32], and movies [33; 34]. And similar data has been collected for the EEG modality (speech , vision , and movies ). There are also movie datasets that cover both modalities . However, when it comes to intracranial recordings, which provide better temporal resolution, but require invasive surgery to implant probes,

  
**Data** & **Quantity** & **Data** & **Quantity** \\  Total subjects & 10 & Total sentences & 38,572 \\ Total hours & 43.5 & Unique sentences & 30,244 \\ Total electrodes & 1,688 & Avg. words per sentence & 6.5 \\ Avg. electrodes per subject & 16 & Total words & 223,068 \\ Total movies & 21 & Unique words & 12,412 \\ Unique movies & 26 & Unique speakers & 937 \\ Number of scenes & 46,935 & Unique part of speech labels & 17 \\ 

Table 1: Quantitative overview of Brain Treebankdata is much more sparse. There exist intracranial datasets for pose , speech production , and parts of speech , but none of these involve the complex natural language and concomitant visual inputs available from movie stimuli. The most similar work to our dataset, Berezutskaya et al. , presents participants with vastly less stimuli: a 6.5 minute short movie, compared to our average of 4.3 hours of movie per patient.

Already, the brain-recordings themselves, without annotation have proven useful for representation learning, as we show in Wang et al. . And combined with the transcribed audio tracks, these have allowed for successful study of multimodal integration in the brain, as we show in Subramaniam et al. . Now, for the first time, we release the complete annotated recordings for all subjects, as well as the accompanying Universal Dependency parse trees.

Figure 1: **Schematic of the approach. Top**: A film (**a-b**) was presented as visual and audio stimulus to the subject. Invasive neural recordings were performed while subjects watched the movie. A transcript of speech in the film is aligned to both the audio (**b**) and neural (**c**) signals. Shown here is a short signal segment from an exemplar electrode in the left superior temporal gyrus aligned to sentence onset at \(t=0\) ms. Word locations are shown as shaded regions between dashed lines. **Bottom**: Schematic overview of selected visual (**d**), audio (**e**), and language (**f-g**) features used for the General Linear Model (GLM) for each word. See Table 4 for a full list and description of features. Visual features **(d)** include the number of faces (yellow boxes), and the magnitude and angle of optical flow (green arrows). Audio features **(e)** include the average pitch (top) and volume (bottom) during each word (shaded gray regions between dashed lines). Word features **(f)** include part-of-speech and the position of each wordâ€™s dependency head. A surprisal feature, **(g)**, computed using GPT-2 , a large language model, is the negative log probability of the word given the preceding context.

Data

**Dataset construction** Stereoeelectroencephalography (sEEG) neural recordings were collected from 10 subjects (5 male, 5 female), aged 4-19 years (\(\) 11.9, \(\) 4.6), under treatment for epilepsy at Boston Children's Hospital (BCH); see supplementary Table 2 for per-subject statistics. All subjects were implanted with intracranial electrodes to localize seizure foci for potential surgical resection. All experiments were approved by BCH/Harvard IRB and were carried out with the subjects' informed consent. IRB documents are available upon request, but are otherwise sensitive. Electrode types, number, and position were driven solely by clinical considerations. Recorded data was anonymized, and identifying patient information was redacted.

**Task and stimuli** Stimuli consisted of 21 recent animated/action Hollywood movies; see supplementary Table 3 for per-movie statistics. On average, movies were 2.07 hours long (\(\)0.68) and contained 1,443 sentences (\(\)333), 8,966 total words (\(\)2068), 1,749 unique words (\(\)315), 1,328 unique lemmas (\(\)251), 1,218 nouns (\(\)271), 610 unique nouns (\(\)126), 1,343 verbs (\(\)293), and 508 unique verbs (\(\)98). Each subject was given a choice of which movies to watch, viewing an average of 2.6 movies (\(\)1.7) corresponding to 4.3 hours (\(\)3.6). For further details, see Appendix A.3.

**Data acquisition and signal processing** Clinicians implanted subjects with intracranial stereoelectronecephalographic (sEEG) depth probes containing 6-16 0.8 mm diameter 2 mm long contact electrodes recording Intracranial Field Potentials (IFPs). Each subject had multiple (12 to 18) such probes implanted in locations determined by clinical concerns entirely unrelated to the experiment, informed by a functional analysis . The number of electrodes per subject ranged between 106 and 246 (\(\)167, \(\)38) for a total of 1,688 total electrodes; see supplementary Table 2 for a per-subject breakdown. Data collected during periods of seizures or immediately following a seizure was discarded. For each electrode, a notch filter was applied at 60 Hz and harmonics before analysis. No other processing (downsampling, filtering specific frequency bands, etc.) was performed on the neural recordings. For further details, see Appendix A.4. Finally, the location of all electrodes was identified and mapped to the common brain atlases (Desikan et al.  and Destrieux et al. ). For the purposes of region analyses, electrodes in white matter are projected to the grey-white boundary and assigned to the closest atlas region. Region analyses in this paper are given with respect to the Desikan-Killiany atlas. See Appendix A.5 for further details.

**Audio transcription and alignment** For each movie, the timestamps for all words in the audio were transcribed and timestamps for each word were found programmatically and then manually corrected by trained annotators (see Appendix A.1 for further details). The pipeline developed for this audio transcription and alignment effort is an independently useful source of annotated stimuli, which can now be used for further experiments. We described this pipeline more completely in a separate technical paper: Yaari et al. . Part of speech tags and dependency parses were manually corrected and speaker identity and scene labels were manually annotated from scratch by an in-house expert hired at MIT.

**Feature annotation** To model the neural responses during the complex movies, we considered a series of 16 features (Table 4). These features include 5 visual attributes (pixel brightness, global optical flow magnitude, optical flow angle, and number of faces, Figure 1d), 4 auditory attributes (volume, pitch, delta volume, and delta pitch, Figure 1e), and 6 language attributes (GPT-2 surprisal, word time length, word time difference, index in sentence, word head, and part of speech tag, Figure 1f-g). All of these features were aligned to and computed for each word. Table 4 provides a brief description of each feature, and their calculation is described in Appendix A.2. Additionally, scenes and speakers were labeled for each movie. Scenes were extracted based on camera cuts using PySceneDetect . Each scene was labeled based on the corresponding image environment and labels were extracted from the Places365 dataset . Finally, for each sentence in the audio transcript, the speaker identity was manually annotated (see Appendix A.2).

## 4 Quantitative analyses of language function with the dataset

**Word onsets triggered strong neural responses** After aligning the neurophysiological data to the occurrence of words (Figure 1a-c), we assessed whether the neural responses were modulated by word onset by comparing the mean activity in 5 pairs of consecutive windows of 100ms duration Figure 2: **Alignment to word onsets reveals strong neural responses.****a.** Raster (top) and mean (bottom) plots of neural activity aligned to word onsets (\(t=0\) ms) for an exemplar electrode (inset; shown in red) in the left superior temporal sulcus. Each line in the raster is a separate word (\(>6,000\) words) in the movie. Shading in the mean plot indicates standard error. Asterisks indicate the significance (double-tailed paired t-test) of the response, measured by comparing mean activity in pre- and post- word-onset intervals (see Section 4). A GLM was fitted to predict the average response in the 500ms window after word onset (Section 4). The magnitude of the beta coefficients for all features is shown for the same example electrode (**b**) and averaged across all electrodes in the temporal lobe (**c**). Features are shown colored by category (blue: language, orange: audio, purple: visual). Asterisks indicate statistical significance of the beta coefficient for the example electrode (see Section 4). Neural responses are shown from the same example electrode separated by **(d)** index in sentence, **(e)** part of speech, **(f)** word length, **(g)** GPT-2 surprisal. Asterisks on horizontal brackets indicate the significance of the neural response, i.e., the difference between pre- and post- word-onset activity, as in (**a**). Vertical brackets show the differences in mean sub-sampled activity (see Section 4). In **h**, the fraction of electrodes per regions for which a significant word-onset response can be observed even after sub-sampling for visual and audio features is shown. The precise location of these electrodes is shown in **i**.

before (-500 ms to 0 ms) versus after (500 to 1000ms) word onset. We defined an electrode to be _word-responsive_ if it yielded a statistically significant difference in at least one of the 5 pairs (paired t-test, p<0.05, Bonferroni corrected, see Appendix A.6). Figure 1(a) shows the neural responses of an example electrode located in the left superior temporal sulcus (Figure 1(a) inset). The raster plot (top) and average activity (bottom) show strong activation triggered by the onset of each word. This activity can be readily appreciated for almost every word in the more than 7,000 words (raster plot) of a single movie. Interestingly, the activity of this electrode begins to show a slight deviation from baseline _before_ the onset of words at time 0. The complex nature of natural stimuli like the movie implies that multiple variables could in principle drive the neural responses. Indeed, the responses to individual words in Figure 1(a) show a strong degree of heterogeneity. To gain insight into what could drive these diverse responses, we considered a set of 16 visual, auditory, and language features (Table 4, Figure 1(d)-g). We built a Generalized Linear Model (GLM) that included all 16 features. The absolute value of the coefficients for each feature indicated how much each annotation contributed to explaining the neural responses (Figure 1(b)). For this example electrode, auditory and language features both showed a statistically significant contribution to explaining the neural response. Among the strongest contributors were the four language features shown in Figure 1(d)-g. The average of all coefficients across the 339 electrodes in the temporal lobe is shown in Figure 1(c), for which we note that the features with the highest averaged coefficients were the index in sentence, part of speech, and delta volume (further regions shown in Figure 9).

To better understand the contribution of the language features with the largest coefficients for the example electrodes, we plotted the neural responses for words that had different values for those features. In Figure 1(d), we separated the words of a movie into those that appeared early in a sentence (quartile with lowest index in sentence, light gray) and those that appeared late in a sentence (quartile with highest index, dark gray). The average neural responses for this example electrode revealed notable differences between these two groups. Common to both groups, there was a deflection from baseline well before t=0. Words with high indices led to reduced voltages and words with low indices led to high voltages after t=0. In a similar fashion, we observed responses separated by nouns versus verbs (Figure 1(e)), high and low word length (Figure 1(f)), and high and low GPT-2 surprisal (Figure 1(g)). In all of these cases, words elicited neural responses across different features even as the neural responses were modulated by those features. Similar conclusions for this electrode can be drawn when considering auditory (supplementary Figure 7(a)) or visual features (supplementary Figure 7(b)).

Next, we asked whether the neural responses are due purely to language, and whether an audio and/or visual explanation can be ruled out. Across all electrodes, we found that there exist 244 (\(\) 16%) electrodes for which there was a significant (\(p<0.05\), Bonferroni corrected) word response, after controlling for all audio and visual features. The fraction of such electrodes per region is shown in Figure 1(h) and the locations of these electrodes are shown in Figure 1(i).

**Sentence position modulates neural activity** The results for the example electrode in Figure 1(d) suggest that the position of a word within a sentence can have a strong impact on the neural responses. To systematically evaluate whether neural signals are dependent on word position, we first categorized words according to their linear position (Figure 2(a)), separating them into sentence _onsets_, sentence _offsets_, and sentence _midsets_, which are the words that occur in between. Figure 2(a) shows the neural responses from an example electrode located in left superior temporal gyrus. This electrode showed stronger responses to sentence onsets (left) compared to midsets (middle) and offsets (right). These differences were evident even in single words (raster plots, top), as well as in the average responses (bottom) and are summarized in Figure 2(b), which shows the mean neural activity for onsets, midsets, and offsets in a 100ms window. Similar to our analysis in the previous section, we evaluated mean neural activity at five evenly spaced 100ms windows, starting from the word onset. The activity shown in Figure 2(b) was taken from the window with the most significant difference between onset, midset, and offset activity (f-test, \(p<0.05\), Bonferroni corrected). Asterisks in Figure 2(b) denote the significance of this difference.

It might be the case that sentence onsets could be associated with a confounding feature, such as increased volume. We therefore separately plotted the responses to words in different sentence positions for cases with high and low volume. The strong modulation by part of sentence persisted across different volume levels (Figure 2(c)-d). Next, in addition to volume, we considered all the 16 features that we annotated in the movie, using a GLM model as illustrated in the previous section. The feature with the third highest coefficient in the GLM model was the index in sentence (Figure 2(e)). Running the GLM analysis for all electrodes revealed 235 electrodes (\(\) 15% of total electrodes) for Figure 3: **Neural signals distinguish between different positions within the sentence.****a.** Raster (top) and mean (bottom) neural responses for an example electrode in the left superior temporal gyrus (see electrode location on right) for words occurring at sentence onset (left), offset (right), or in between (midset, middle). The format and conventions follow Figure 2a. The box-plots (**b**) show the mean activity in a 100ms window. Asterisks show the significance of the difference between activities (f-test, Bonferroni corrected). **c.** Neural responses from the same electrode separated by trials with high volume (dark grey) or low volume (light grey). Vertical brackets and asterisks show the difference between the two conditions (two-tailed t-test). In both cases, the difference due to sentence position persists (shown by horizontal brackets and asterisks in **d**). **e.** Beta coefficients from a fitted GLM for all features, colored by category (format as in Figure 2b). Coefficients shown here are for the same electrode as in (**a). **f.** Per region, the fraction of electrodes (shown as blue bars) in each region for which there is a significant (\(p<0.05\), Bonferroni corrected) beta coefficient for position in sentence and the fraction of electrodes (white bars) which exhibit a significant (\(p<0.05\), f-test, Bonferroni corrected) difference in activity due to sentence position after controlling for all confounds. **g.** The exact location of the electrodes from (**f**), shown as blue and white points respectively, projected onto the surface of the brain.

which the sentence position feature has a significant (\(p<0.05\), Bonferroni corrected) beta coefficient in the fitted GLMs (Figure 3f, Figure 3g blue dots).

Among these electrodes which we identified to be modulated by position in sentence, we also used a different, more stringent test to determine the influence that the position in sentence has on mean activity. For each of these electrodes, the analysis that was discussed previously with respect to Figure 3c-d was repeated for all features. Across these electrodes, controlling for all co-occurring features, revealed 114 electrodes (\( 7\%\) of total electrodes) that showed a significant modulation by sentence position (f-test, \(p<0.05\), Bonferroni corrected). These electrodes were predominantly located in the transverse temporal cortex and the banks of the superior temporal sulcus (Figure 3f-g).

**The temporal-course of speech decodability reveals the dynamics of language processing** We also used a linear decoder to answer questions about when and where certain language induced activity is available in the neural signal. To that end, we fitted a linear regression for every 250ms interval in a [-1000ms,1000ms] window. As discussed in the previous sections, we had observed language responses to be stronger at sentence onsets, so we first considered the case of trying to decode whether or not a sentence onset was occurring. However, the case for generic word onsets was also considered (see supplementary Figure 6), and is discussed below as well.

For each electrode, we created a training dataset of neural activity (see Appendix A.9). Positive examples consisted of sentence onsets and negative examples were taken from portions of the movie where no dialogue is occurring. We fit a regression on the train data, validate using 5-fold cross validation, and report the ROC-AUC on the test set. Figure 4a. shows the _peak_ decoding performance per electrode. Here, the peak performance is the maximum performance achieved over the course of the entire considered time interval. Figure 4c shows the test-set performance per time interval in the temporal and frontal lobe, averaged across the 10 electrodes that had the highest peak decoding performance on the train set. In the frontal region, decoding peaked later than in the temporal region (300ms vs 100ms). We performed the same decoding for generic word onsets (see Figure 6). Here

Figure 4: **Sentence onsets are linearly decodable.** A linear decoder is trained to classify portions of the movies according to whether or not a sentence onset is occurring, based on the corresponding neural activity. This decoding is done for activity in 0.25s windows, shifted in 0.1s increments, from -1s before the sentence onset to 1s after the sentence onset. The _peak_ decoding performance for an electrode is the max ROC-AUC achieved across all increments. **a.** The spatial distribution of peak decoding scores. **b.** Decodability, as a function of time for an electrode in the banks of the superior temporal sulcus on the right hemisphere. **c.** The time course of decodability on the test set, for the top 10 electrodes that had the highest peak ROC-AUC score on the train set, in the temporal lobe and the frontal lobe. The test set is balanced between positive and negative examples so that chance performance is 0.5. Together, these curves reveal that for sentence onsets, information is processed before word onset enters the decoding window (dashed grey line). Error bars show a 95% confidence interval over performance per electrode. Comparing the curves reveals the mirrored time course of language processing in the frontal and temporal lobes. See supplementary Figure 6 for the same analysis, performed for word-onsets.

we found a similar pattern as in figure Figure 4. Decoding in the temporal lobe reached a peak at 200ms, compared to 300ms in the frontal lobe.

Finally, we also attempted to linearly decode the noun vs. verb distinction in the brain (supplementary Figure 7). We saw that the noun vs. verb distinction is most decodable in the frontal lobe, where decoding performance peaks after word onset (\(t=400\)ms).

## 5 Conclusion

The Brain Treebank has a unique combination of large scale, high temporal resolution, high spatial resolution, naturalistic stimuli, and many layers of manual annotation. Because naturalistic stimuli contain many uncontrolled co-occurring features, scale is critical in order to find natural experiments with controls post-hoc. We demonstrate two such an experiments: first, how response to words and sentences can be identified, even after controlling for co-occurring features, and second, how linear decoding reveals the time course of word and sentence processing. This only begins to explore what can be done with these data and annotations, and it remains to be seen what is detectable if more powerful decoding tools are applied.

**Limitations** Subjects only watched each movie once, thus one cannot simply average over repetitions of exactly the same stimulus. Although, each movie does repeat the same words and often shows the same characters, naturalistic stimuli are harder to work with than controlled experiments. Subjects all saw different movies, making the cross-subject analysis more difficult. At the same time, this means that there are more opportunities to find interesting phenomena because of the diversity of the movies that subjects saw. As with all studies that involve naturalistic stimuli, controlling for confounds can be difficult. Intracranial recordings are only possible because subjects require neurosurgery for some condition, in this case epilepsy; it is possible that this could result in some sampling bias. Additionally, the corpus includes only movies in English, although we are adding Spanish movies and subjects shortly. In this vein, we are actively working on collecting more data and hope that others who intend to collect data can collect it for the movies we have annotated here. Tools and techniques to run experiments on naturalistic data are much newer and more limited at the moment.

We have not begun to scratch the surface of the kinds of analyses possible with the Brain Treebank. For example, we have never used the speaker identities and hardly exploited multimodality, nor have we made use of the parses aside from the POS tags. We hope that the Brain Treebank will enable the development of new tools and new kinds of neuroscientific experiments at scale with natural stimuli, as well as bring the neuroscience, NLP, and linguistics communities closer together with a shared resource that has components from each.