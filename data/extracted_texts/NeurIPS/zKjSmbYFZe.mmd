# Efficient Post-Processing for Equal Opportunity

in Fair Multi-Class Classification

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Fairness in machine learning is of growing concern as more instances of biased model behavior are documented while their adoption continues to rise. The majority of studies have focused on binary classification settings, despite the fact that many real-world problems are inherently multi-class. This paper considers fairness in multi-class classification under the notion of parity of true positive rates--an extension of binary class equalized odds --which ensures equal opportunity to qualified individuals regardless of their demographics. We focus on algorithm design and provide a post-processing method that derives fair classifiers from pre-trained score functions. The method is developed by analyzing the representation of the optimal fair classifier, and is efficient in both sample and time complexity, as it is implemented by linear programs on finite samples. We demonstrate its effectiveness at reducing disparity on benchmark datasets, particularly under large numbers of classes, where existing methods fall short.

## 1 Introduction

Algorithmic fairness has emerged as a topic of significant concern in the field of machine learning, due to the potential for models to exhibit discriminatory behavior towards historically disadvantaged demographics [9; 4; 6], all while their adoption continues to rise in domains including high-stakes areas such as criminal justice, healthcare, and finance [3; 7]. To address the concern, a variety of fairness criteria have been proposed (e.g., demographic parity, equalized odds) along with mitigation methods [10; 19; 23; 26]. On classification problems, the majority of work focuses on the binary class setting [2, Table 1], where one class is typically considered to be more favorable (e.g., the approval vs. rejection of a credit card application).

Yet, many real-world problems are multi-class in nature. In the case of credit card applications, issuers may opt to assigning higher-tier interest rates to high-risk applicants rather than outright rejecting them, which creates opportunities to applicants who would otherwise be denied credit and also generates returns for the banks. Similarly, in online advertising, recruiting platforms can employ machine learning models to match users to relevant job postings across multiple occupation categories. There are evidences, however, for such systems to exhibit gender bias [8; 13; 44]; for instance, models that are trained to identify occupation from biography tend to show higher accuracy (recall) on male biographies than on their female counterparts in occupations that are historically male-dominated .

In the example above, unfairness is manifested in a disparity of _true positive rates_ (TPRs) across demographic groups \(A\) (generalizing the true positive and negative rates in binary classification),

\[_{a}()_{y}(=y Y=y,A= a), y[k],\,a[m].\]

A classifier satisfying parity of TPRs, i.e., \(_{a}=_{a^{}}\) for all \(a,a^{}\), ensures that individuals with the same qualification (\(Y\)) will have _equal opportunity_ of receiving their favorable outcome (\(=Y\)regardless of demographics , e.g., being shown job postings on recruiting platforms for which the user is qualified. When the classes are binary, this fairness notion recovers _equalized odds_.

In this paper, we focus on the design of algorithm for mitigating TPR disparity and provide an efficient _post-processing_ method that derives _attribute-aware_ fair classifiers from (pre-trained) scoring models. Our method works on multi-class and multi-group classification problems, guarantees fairness by a sample complexity bound, can be implemented by linear programs, and achieves higher reductions in disparity compared to existing algorithms that are applicable to multi-class--a recently proposed post-processing method based on model projection , and adversarial debiasing , an in-processing method--especially when the number of classes is large.

Organization.We introduce the problem setup and objectives in Section 2, then describe our post-processing method for TPR parity in Section 3, along with suboptimality analyses; in particular, our method yields the optimal fair classifier when applied to the _Bayes optimal_ score function. Our method is instantiated for finite sample estimation in Section 4, and we also provide sample complexity bounds to complete the analysis. Finally, in Section 5, we compare our algorithm with existing methods for disparity reduction on benchmark datasets.1 A high-level summary of our results is provided in Section 1.1.

### Summary of Results

One way to interpret and understand TPR parity is through visualizing the feasible regions of TPRs. In Fig. 1, we plot the feasible regions (achievable by probabilistic classifiers) of two groups on a (hypothetical) binary classification problem on the left, and those on a three-class problem on the right, where each axis represents the TPR of a class. Achieving optimal TPR parity amounts to first finding the TPR that maximizes the overall utility (e.g., accuracy) in the intersection of feasible regions, and subsequently an (attribute-aware) classifier attaining that target TPR on all groups. Note that the left figure is equivalent to the ROC curve (with a flip of the horizontal axis, because the TPR of class \(1\) equals one minus the false negative rate by treating class-\(1\) as the negative class), which was used by Hardt et al.  for studying equalized odds. And thus, the TPR (hyper)surface plots in higher dimensions are a natural generalization of the ROC curve to multi-class settings.

Step one of finding the optimal fair TPR can be formulated as a linear program when estimating from finite samples. For the second step, our method derives a classifier attaining the target TPR from the score function; in particular, it yields the optimal fair classifier when the score is Bayes optimal:

**Theorem 1.1**.: _Let \(f_{*}^{*},,f_{n}^{*}:_{k}\) denote the Bayes score function on each group, \(f_{a}^{*}(x)[Y X=x,A=a]\), and \(q_{1},,q_{m}_{k}\) be arbitrary. Then under a continuity assumption (2.3), \(_{1},,_{m}\) and \(_{1},,_{m}^{k}\) s.t. the probabilistic attribute-aware classifier_

\[(x,a)\{ &_{y^{}}(_{a})_{y^{ }} f_{a}^{*}(x)_{y^{}}&&1-_{a}\\ & y&&_{a}(q_{a})_{y},\  y[k].\] (1a) _achieves the maximum utility subject to TPR parity._

Figure 1: Feasible region of TPRs on a binary class (left) and a three-class problem (right). The black (resp. colored) arrow indicates the utility-maximizing direction (of each group).

The post-processed classifier returned by our method is a mixture of two models (weighted by \(\)). Eq. (1a) returns the class with the highest likelihood after a class-wise rescaling, called a _tilting_, which generalizes the concept of _thresholding_ in binary classifiers. Eq. (1b) makes random assignments sampled from a \((q)\) distribution, which handles situations where the fair TPR lies in the interior of the feasible region (see Fig. 1, where the optimum is located within the interior of group 2 feasible region). To alleviate potential ethical concerns regarding this randomization, we point out that the parameter \(q_{a}\)'s used in class sampling can be specified per-group by the practitioner responsibly, e.g., uniform \(/k\), or \(e_{y^{}}\) with \(y^{}\) being an advantaged outcome.

Among the possibly infinitely many fair classifiers derived from the score function \(f\), our method specifically seeks the simplistic representation in Eq. (1) because it can be estimated via linear programs from finite samples. More importantly, it immediately extrapolates to unseen examples, and provides good generalization performance at the rate of \(()\) thanks to its low function complexity (Theorem 4.2).

When the score function being post-processed is not Bayes optimal, our method is still applicable, but the resulting classifier may not be optimal nor exactly achieve TPR parity without access to labeled data (the method itself only needs unlabeled data with the sensitive attribute) or additional knowledge of the model. But these suboptimalities are minimized if the model is _calibrated_ (Theorem 3.5); this answers the question raised in  about the effects of base model inaccuracies on downstream post-processing.

### Related Work

Fairness Criteria.The notion of TPR parity has appeared in the literature as _conditional procedure accuracy equality_, _avoiding disparate mistreatment_, and (multi-class) _equal opportunity_ (to be distinguished from the fairness criterion with the same name in ). Other group fairness notions that extend to multi-class include (but not limited to) _equalized odds_ (of which TPR parity is a necessary condition), and _demographic parity_ (DP)  (where Xian et al.  recently proposed an optimal post-processing method). However, DP may be less desirable than TPR parity in some use cases because the perfect classifier is not permitted under DP when the base rates differ . It is worth noting that TPR parity implies _accuracy parity_. In addition to group fairness, there are notions defined on the individual level .

Mitigation Methods.Our method is based on post-processing . There are also in-processing methods via fair representation learning  or solving zero-sum games , and pre-processing methods that debias the data prior to model training ; see  for a survey.

For multi-class TPR parity, the only applicable post-processing method to date, to our knowledge, is due to Alghamdi et al.  (which is the primary baseline for our method in our experiments). It is a general-purpose method that transforms the scores to satisfy fairness while minimizing the distributional divergence (e.g., KL) between the transformed scores and the original. However, the tradeoff between model performance and fairness is unclear as they did not relate the divergence to utility. Furthermore, while the authors provided a sample complexity bound for their optimization objective, it is not explicitly related to the violation of the fairness criteria.

## 2 Preliminaries

A \(k\)-class classification problem is defined by a joint distribution \(\) of input \(X\), demographic group membership \(A[m]\{1,,m\}\) (a.k.a. the sensitive attribute), and class label \(Y[k]\). We denote the joint distribution of \((X,A)\) by \(^{X,A}\), and, the \((k-1)\)-dimensional probability simplex by \(_{k}\{z_{ 0}^{k}:\|z\|_{1}=1\}\).

Let \(f:_{k}\) be an attribute-aware (pre-trained) score function, whose outputs are probability vectors that estimate the class probabilities as in \(f(x,a)_{y}_{}(Y=y X=x,A=a)\). We will write \(f_{a}:_{k}\) to denote the component of \(f\) associated with group \(a\), i.e., \(f_{a}(x) f(x,a)\). Our goal is to find fair (probabilistic) post-processing maps \(g_{1},,g_{m}:_{k}\) to derive a classifier \((x,a) g_{a} f_{a}(x)\) that satisfies TPR parity while maximizing utility (e.g., classification accuracy).

We allow for controllable tradeoffs between utility and fairness through the following relaxation of TPR parity, and call a classifier \(\)_-fair_ if it satisfies \(\)-TPR parity:

**Definition 2.1** (Approximate TPR Parity).: Let \(\). A predictor \(\) is said to satisfy \(\)-TPR parity if \(_{}()\), where

\[_{}()_{a,a^{}} _{a}()-_{a^{}}( )_{},\] (2)

and \(_{a}()( Y=y,A=a)[ 0,1]^{k}\); \(\) includes the randomness of the predictor.

Beyond classification accuracy, we also allow for any utility functions that depend only on the TPRs:2

**Definition 2.2** (Utility).: The utility function \(u:[k][k]\) is defined for some \(v^{k}\) by

\[u(,y)_{y^{}[k]}v_{y^{}}\,[y=y^{ },=y^{}].\]

E.g., accuracy, \([y=]\), is obtained by setting \(=_{k}\). The term \(\) will appear in our analyses, and the significance of considering utilities of this form is that we could evaluate a classifier by a weighted sum of its TPRs. Define \(p_{ay}_{}(A=a,Y=y)\), then

\[()=\,u(,Y)=_{a[m],y[k]} _{y}p_{ay}\,_{a}()_{y}( _{1}(),,_{m}()).\] (3)

Finally, we make the following continuity assumption on the distributions of score to avoid technical complexities related to tie-breaking (on the atoms). This assumption has also appeared in prior work on fair post-processing ; it holds when the input distributions are continuous and the score function is injective, or can be satisfied by adding small random perturbations to the scores.

**Assumption 2.3**.: The conditional distribution of score, \((f_{a}(X) A=a)\), is (Lebesgue absolutely) continuous, \( a[m]\).

## 3 TPR Parity via Post-Processing

Given a score function \(f:_{k}\), and access to the (unlabeled) joint distribution \(^{X,A}\) (i.e., no estimation error), we describe a method for deriving an attribute-aware \(\)-fair classifier while maximizing utility, in the form of \((x,a) g_{a} f_{a}(x)\), where the \(g_{a}\)'s are (probabilistic) fair post-processing maps for each group. That is, we want to solve

\[_{g_{1},,g_{m}}()_ {}()=g_{A}  f_{A}(X).\]

Although the method only returns classifiers derived from \(f\) as opposed to searching over the space of all classifiers \(h:\), it would yield the optimal fair classifier provided that the information of \((A,Y)\) is preserved in the output of \(f\); this is the case when the score function is Bayes optimal.

### Deriving Optimal Fair Classifier From Bayes Score Function

In this section, we explain how to obtain an optimal fair classifier by deriving from the Bayes score function \(f^{*}\), thereby providing a _proof of Theorem 1.1_ (omitted proof are deferred to the appendix).

Step 1 (Finding Utility-Maximizing Fair TPRs).Let \(D_{a}^{k}\) denote the set of feasible TPRs on group \(a\) achieved by probabilistic classifiers. The first step is to find utility-maximizing fair TPRs contained in an \(_{}\)-ball of diameter \(\) per Definition 2.1 of \(\)-TPR parity (left figure of Fig. 2):

\[_{t_{1} D_{1},,t_{m} D_{m}}(t_{1},,t_{m}) \|t_{a}-t_{a^{}}\|_{},\; a, a^{}[m].\] (4)

When \(=0\), this reduces to finding a single \(t_{a}D_{a}\), and because each \(D_{a}\) is convex (since probabilistic classifiers are allowed), it can be found with ternary search as suggested in . If instead the \(t_{a}\)'s are to be estimated from finite samples, then the empirical \(_{a}\)'s are described by polytopes and the problem can be formulated as a linear program (Section 4).

The feasible regions of TPR generally differ across groups, due to uncertainties that are inherent to each group in the task of interest, or to inadequate and biased collection or sourcing of data. The more the \(D_{a}\)'s differ, the greater the tradeoff between fairness and utility; hence TPR parity incentivizes the practitioner to improve data collection and aspects of modeling that induces a balanced predictive capability on all groups .

Because \(f^{*}(X,A)\) is _sufficient statistic_ for \(Y\), the fair TPRs we found above are always achievable by classifiers derived from \(f^{*}\). Or more concretely,

**Proposition 3.1**.: _Let \(f^{*}:_{k}\) denote the Bayes score function, then \(D\{(h)^{k} h:\) (probabilistic) \(\}=\{(g f^{*})^{k} g:_{k}\) (probabilistic)\(\}\)._

Step 2 (Obtaining Fair Classifier of Desired Form).Having found the utility-maximizing fair TPR \(t_{a}\)'s, the next step is to derive a classifier that attains \(t_{a}\) on each group. This is provided by the following theorem:

**Theorem 3.2**.: _Let \(f^{*}:_{k}\) denote the Bayes score function, and \(q_{k}\) be arbitrary. Then under Assumption 2.3, \( t D\), there exists \(\) and \(^{k}\) s.t. \((h)=t\), where_

\[h(x)=\{ _{y^{}} _{y^{}}f^{*}(x)_{y^{}}&1-\\ y& q_{y},\  y[k]..\]

The construction uses the observation that the boundary of \(D\), denoted by \( D\), is given by the set of TPRs attained by tiltings of the Bayes score:

**Proposition 3.3**.: _Let \(f^{*}:_{k}\) denote the Bayes score function, then \(h:\) (probabilistic) satisfies \((h) D\) if and only if \(^{k}\), \( 0\) s.t. \(h(x)_{y}_{y}f^{*}(x)_{y}\)._

Proof of Theorem 3.2.: If the target TPR lies on the boundary of \(D\), then by Proposition 3.3, it is achieved by a tilting of the Bayes score without any randomization (i.e., \(=0\); center figure of Fig. 2). This holds due to Assumption 2.3, because we may break ties arbitrarily without affecting TPR, since the set of tied scores (finite union of \((k-2)\)-d subspaces) has (Lebesgue) measure zero.

Otherwise, and generally, there must exists \(t^{} D\) and \(\) s.t. \(t\) can be written as a linear combination of \(t= q+(1-)t^{}\). This is simply because \(q_{k} D\), and the line connecting \(q\) and \(t\) must intersect \( D\) at some point \(t^{}\) (right figure of Fig. 2). Since the TPR of the input-agnostic randomization according to \((q)\) equals \(q\), and \(t^{}\) is achieved by a tilting of the score per Proposition 3.3, their \(\)-mixture achieves the target TPR \(t\) by linearity. 

### Deriving From Any Score Function

The post-processing method described in the previous section, which only requires unlabeled data \((X,A)\), yields the optimal \(\)-fair classifier when applied to Bayes scores \(f^{*}\). Yet, in practice, there

Figure 2: Achieving \(\)-TPR parity on a binary class problem. First, the utility-maximizing TPRs residing in an \(_{}\)-ball of diameter \(\) are found (left). Then, classifiers achieving the fair TPRs are obtained: a tilting of the scores when the TPR lies on the boundary (middle), otherwise, a mixture of tilting and randomization (right). The simplex \(_{k}\) is always inscribed in the feasible region.

is the concern that Bayes score functions could be arbitrarily complex and are often not exactly learnable due to limited data or computational constraints .

Nonetheless, our method is still applicable to arbitrary (approximations to the Bayes) score functions \(f:_{k}\) for deriving classifiers that are approximately fair and optimal, by treating them as _if they were Bayes optimal_ (Algorithm 1). Where, the only tweak we made is replacing the ground-truth TPRs and feasible regions (which are unknown without access to the Bayes score) by approximations _induced_ by \(f\), i.e.,

\[_{a}}_{a}(h)^{k} \ \ h:\ ()},\] (5)

where

\[}_{a}(h)_{y}_{ay}}_{x }f_{a}(x)_{y}\,(h(x)=y)\,^{X,A}(x,a), _{ay}_{x}f_{a}(x)_{y}\,^{X,A} (x,a).\] (6)

It is not hard to show that they are equal to their ground-truth counterparts when \(f=f^{*}\).

We may control and minimize the suboptimalities of the classifier returned from Algorithm 1 by performing _group-wise distribution calibration_ to the score function \(f\) (using labeled data \((X,A,Y)\)):

**Definition 3.4** (Distribution Calibration).: A score \(R\) is said to be (group-wise) distribution calibrated if \((Y=y\ |\ R=s)=s_{y}\), \( s_{k},y[k]\) (resp. \((Y=y\ |\ R=s,A=a)=s_{y}\), \( a[m]\)).

Distribution calibration is a multi-class generalization of the original definition of calibration for binary predictors , requiring the predicted score to match the underlying class distribution conditioned on the score across all classes, not just the most confident one . Although this definition is convenient to work with mathematically, it could be difficult to achieve in practice. In the proof of Theorem 3.5, we relax it to a recently proposed notion of _decision calibration_ (w.r.t. the set of all tiltings; derived from _multicalibration_), which could be achieved in polynomial time.

**Theorem 3.5**.: _Let \(f:_{k}\) be a score function, and \(h:\) the (probabilistic) classifier derived from \(f\) using Algorithm 1. Then under Assumption 2.3, for any group-wise calibrated reference score function \(:_{k}\),_

\[}-(h)_{a[m],y[k]}3 v_{y}_{ay},_{}(h)+_{a[m],y[k]} }{p_{ay}},\]

_where \(p_{ay}_{}(A=a,Y=y)\), \(v\) is from the utility function in Definition 2.2, \(}\) denotes the utility achieved by the optimal \(\)-fair classifier derived from the calibrated reference \(\), and_

\[_{ay}_{a}(X)_{y}-f_{a}(X)_{y} [A=a]\]

_is the \(L^{1}()\) difference between \(f\) and the calibrated reference \(\) on group \(a\) and class \(y\)._

We draw two conclusions from this result. First, by using the Bayes score function \(f^{*}\) as the reference, it states that the suboptimality of the derived classifier when \(f f^{*}\) is upper bounded by the difference between the approximate scores and the ground-truth; this answers the question raised in  regarding the impact of base model inaccuracies. Second, if \(f\) satisfies calibration, then by using itself as the reference, the result guarantees that the classifier derived using Algorithm 1 exactly achieves the desired level of fairness, and is optimal among all fair classifiers derived from \(f\) (which cannot be improved without labeled data).

## 4 Finite-Sample Algorithm and Guarantees

We instantiate the post-processing method above for TPR parity to the case where we do not have access to the distribution \(^{X,A}\) but only samples drawn from it (i.e., to perform estimation), and analyze the sample complexity.

**Assumption 4.1**.: We have \(n\) i.i.d. (unlabeled) samples of \((X,A)\), which are independent of the score function \(f\) being post-processed.

Denote the number of samples from group \(a\) by \(n_{a}\), and the samples themselves by \((x_{a,i})_{i[n_{a}]}\).

### Algorithm

We adapt Algorithm 1 to handle finite samples by replacing \(_{a}\) and \(\) with their empirical counterparts (essentially calling it with the empirical distribution \(^{X,A}\) formed by the samples as the argument), and implement the optimization problems on Lines 3, 5 and 6 using linear programs.

**Step 1** (Finding Utility-Maximizing Fair TPRs).: The empirical induced feasible region of TPRs, \(_{a}\), can be computed via evaluating the TPRs of all (probabilistic) classifiers acting on the samples--by representing them using \(n_{a} k\) lookup tables (each row gives the probabilities of the random class assignment on the corresponding sample):

\[_{a}}_{a}(_{a})\; \;_{a}_{ 0}^{n_{a} k},\,_{y[k]}( _{a})_{i,y}=1,\, i[n_{a}]},\]

where

\[}_{a}()_{y}_{ay}}_{ i[n_{a}]}f_{a}(x_{a,i})_{y}(_{a})_{i,y},_{ay} _{i[n_{a}]}f_{a}(x_{a,i})_{y}\]

(cf. Line 2 and Eqs. (5) and (6)). Note that \(_{a}\) is a polygon, as it is specified by linear constraints.

To obtain the utility-maximizing fair TPR \(_{a}\)'s, we take the empirical maximizer subject to the \(\)-TPR constraint via solving a linear program (cf. Line 3 and Eqs. (3) and (4)):

\[():_{_{1}_{1},,_{m} _{m}}}(_{1},,_{m}) \|_{a}-_{a^{}}\|_{},\;  a,a^{}[m],\]

where \(}(_{1},,_{m})_{a,y}{}_{ y}_{ay}(_{a})_{y}\) is the empirical utility.

**Step 2** (Obtaining Fair Classifier of Desired Form).: The next step is finding a classifier that achieves \(_{a}\)'s on the empirical distribution, i.e., Lines 5 and 6. To implement Line 5, note that another way of approaching this problem is to realize that among all eligible \((_{a},h_{a})\)-pairs, the \(h_{a}\) associated with the maximum \(_{a}\) value must satisfy \(}_{a}(h_{a})_{a}\) (otherwise, a contradiction can be reached using the fact that \(_{a}^{k}\) is compact; also see the right figure of Fig. 2). Combined with the strategy above of representing classifiers using lookup tables, we get the following linear program:

\[(t,q):_{,} t=(1- )}()+ q _{ 0}^{n k},\,_{y[k]}_{i,y}=1,\, i[n].\]

Finally, on Line 6, we find a tilting \(_{a}\) s.t. after coordinate-wise multiplied by the scores, the argmax class assignment has nonzero probability according to the classifier \(_{a}\) found in the preceding step:

\[():_{}0_{y}f(x_ {i})_{y}_{y^{}}f(x_{i})_{y^{}} i[n],\,y,y^{}[k],\,_{i,y}>0.\]

The feasible set of this problem is nonempty by Proposition 3.1, because we are _treating \(f\) as if it were the Bayes score function, and the empirical distribution \(^{X,A}\) as the population_.

All combined, our algorithm involves solving \((2m+1)\) linear programs, where \(\) is the dominating one with \(O(nk)\) variables and constraints; solving which (to near-optimality) takes, e.g., \(((nk))\) time using interior point methods .

### Sample Complexity

Thanks to the low function complexity of post-processing maps used in our algorithm to derive classifiers (Eq. (1)), it enjoys the following efficient sample complexity:

**Theorem 4.2**.: _Let \(f:_{k}\) be a score function, and \(h:\) the (probabilistic) classifier derived from \(f\) using Algorithm 1 with the empirical distribution formed by samples from Assumption 4.1 as the argument. Then under Assumption 2.3, for any group-wise calibrated (Definition 3.4) reference score function \(:_{k}\), and \(n(_{a,y}(mk/)/p_{ay})\),_

\[}-(h) O _{a[m],y[k]}v_{y}}{n}} ++_{ay},\] \[_{}(h)+O_{a[m],y[k]} }}+}+}{p_{ay}},\]

_where \(}\) denotes the utility achieved by the optimal \(\)-fair classifier derived from the calibrated reference \(\), and \(_{ay}[|_{a}(X)_{y}-f_{a}(X)_{y}|[A=a]]\)._

The bound consists of a calibration error \(_{ay}\) as discussed in the remarks of Theorem 3.5, an estimation error from applying uniform convergence (the Natarajan dimension of the set of tiltings is \(O(k)\)), and a \(k/n\) term that comes from the disagreement over class assignments on the samples between the (deterministic) tilting found on Line 6 and the (probabilistic) classifier on Line 5 due to tie-breaking.

## 5 Experiments

We evaluate Algorithm 1 for reducing TPR disparity on benchmark datasets, and demonstrate its effectiveness compared to existing post-processing as well as in-processing bias mitigation methods.

Datasets.The first task is income prediction, for which, we use the ACSIncome dataset --an extension of the UCI Adult dataset  with much more examples (1.6 million vs. 30,162), allowing us to compare methods confidently. We consider a binary setting where the sensitive attribute is gender and the target is whether the income is over $50k, as well as a multi-group multi-class setting with five race categories and five income buckets. The second is text classification, of identifying occupations (28 in total) from biographies in the BiasBios dataset ; sensitive attribute is gender.

Baselines and Setup.The main baseline is FairProjection --the only post-processing algorithm applicable for multi-class TPR parity to our knowledge.3 In the binary setting, we also compare to RejectOption. To demonstrate the deficiencies of existing methods at reducing TPR disparity, we additionally include in-processing results using Reductions and Adversarial.45

On each task, we first create a pre-training split from the dataset and train a linear logistic regression scoring model (with isotonic calibration and five-fold cross-validation as implemented in scikit-learn), then randomly split the remaining data for post-processing and testing with 10 different seeds and aggregate the results (the pre-trained model remains the same). For in-processing, we use the same splits but merge the pre-training and post-processing data for training. On BiasBios, linear logistic regression is performed on the embeddings of the biographies computed by a previously fine-tuned BERT model  (in other words, head-tuning). Additional details including hyperparameters are included in the appendix.

Results.In Fig. 3, we plot the tradeoff curves from varying the fairness tolerance (\(\) for our method). Our method is consistently the most effective at minimizing TPR disparity, particularly under multi-class settings, where existing algorithms only manage to partially reduce \(_{}\) (and at a greater cost to accuracy when using FairProject and RejectOption). It also outperformsthe in-processing Reductions on binary ACSIncome, and Adversarial in terms of \(_{}\), which, although enjoys higher accuracies because of the use of the more expressive feedforward networks as the prediction model, fails to reduce TPR parity. Sharper drops in accuracies are observed when applying our method with small \(\) settings, e.g., \(0.001\) to \(0.0001\). We saw this happen when the randomized component in Eq. (1b) is activated (i.e., \(>0\)), meaning that Line 3 finds a fair TPR that lies in the interior of the feasible region of the better-performing group in order to match the feasible TPR on the worse-performing one(s). Hence the drop is expected because utility is being sacrificed to achieve TPR parity.

Although our method greatly reduces TPR disparity, there remains a gap to reaching \(_{}=0\), especially on tasks with more classes (i.e., BiasBios, where a higher variance is also observed). While this could be due to miscalibration, or potentially a violation of Assumption 2.3, the main reason is suspected to be insufficient sample size. Recall from Theorem 4.2 that the sample complexity for \(_{}\) scales as \((})\) in the worse-case \((a,y)\), which is itself at least \((/n})\). Thus, learning generalizable classifiers that satisfy TPR parity under more groups and classes is much harder in terms of data requirement (and by extension, computing resource).

Lastly, we emphasize the necessity of group-wise calibration for achieving low \(_{}\), as the definition of the criterion involves conditioning on the true label (it is also reflected by the calibration error term \(_{ay}\) in Theorem 4.2). In an ablation study in the appendix, a larger (minimum achievable) \(_{}\) is observed when no efforts are made to calibrate the scoring model. It is therefore necessary for model vendors to provide accurate uncertainty quantifications, and for practitioners building fair classifiers to verify and improve calibration.

## 6 Conclusions and Limitations

We described a post-processing method for reducing TPR disparity for equal opportunity in multi-class classification, and demonstrated its performance in comparison to existing algorithms on benchmarks datasets, especially when the number of classes is large. We analyzed the sample complexity of our method, and established its optimality under model calibration.

The effectiveness of our method at reducing TPR disparity is largely contributed to the tailored analysis, although it limits our method to this fairness notion only. Some use cases may demand equalized odds (\( A Y\)) beyond TPR parity (\([=Y] A Y\)), which is a more stringent criterion: TPR parity only needs to match the main diagonal of the (conditional) confusion matrix across groups, whereas equalized odds requires matching all \(k^{2}\) entries. The design of efficient algorithms for achieving equalized odds remains an open problem.6

Figure 3: Tradeoff curves between accuracy and \(_{}\) (Eq. (2)). The base model is logistic regression (except for Adversarial, which uses a feedforward network). Error bars indicate the standard deviation over 10 runs with different random dataset splits. Running time is reported in the appendix.