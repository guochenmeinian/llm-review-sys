# Mitigating the Impact of Labeling Errors on Training via Rockafellian Relaxation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Labeling errors in datasets are common, if not systematic, in practice. They naturally arise in a variety of contexts--human labeling, noisy labeling, and weak labeling (i.e., image classification), for example. This presents a persistent and pervasive stress on machine learning practice. In particular, neural network (NN) architectures can withstand minor amounts of dataset imperfection with traditional countermeasures such as regularization, data augmentation, and batch normalization. However, major dataset imperfections often prove insurmountable. We propose and study the implementation of Rockafellian Relaxation (RR), a new loss reweighting, architecture-independent methodology, for neural network training. Experiments indicate RR can enhance standard neural network methods to achieve robust performance across classification tasks in computer vision and natural language processing (sentiment analysis). We find that RR can mitigate the effects of dataset corruption due to both (heavy) labeling error and/or adversarial perturbation, demonstrating effectiveness across a variety of data domains and machine learning tasks.

## 1 Introduction

Labeling errors are systematic in practice, stemming from various sources. For example, the reliability of human-generated labels can be negatively impacted by incomplete information, or the subjectivity of the labeling task - as is commonly seen in medical contexts, in which experts can often disagree on matters such as the location of electrocardiogram signal boundaries , prostate tumor region delineation, and tumor grading . As well, labeling systems, such as Mechanical Turk1 often find expert labelers being replaced with unreliable non-experts . For all these reasons, it would be advisable for any practitioner to operate under the assumption that their dataset is corrupted with labeling errors, and possibly to a large degree.

In this paper, we propose a loss-reweighting methodology for the task of training a classifier on data having higher levels of labeling errors. We show that our method relates to optimistic and robust distributional optimization formulations aimed at addressing adversarial training (AT). These findings underscore our numerical experiments on NNs that suggest this method of training can provide test performance robust to high levels of labeling error, and to some extent, feature perturbation. Overall, we tackle the prevalent challenges of label corruption and class imbalance in training datasets, which are critical obstacles for deploying robust machine learning models. Our proposed approach implements Rockafellian Relaxations  to address corrupted labels and automatically manage class imbalances without the need for clean validation sets or sophisticated hyper-parameters - common constraints of current methodologies. This distinct capability represents our key contribution, making our approach more practical for handling large industrial datasets.

We proceed to discuss related works in section 2, and our specific contributions to the literature. In section 3 we discuss our methodology in detail and provide some theoretical justifications that motivate the effectiveness of our methodology. The datasets and NN model architectures upon which our experimental results are based are discussed in sections 4 and 5, respectively. We then conclude with numerical experiments and results in section 6.

## 2 Related Work

Corrupted datasets are of concern, as they potentially pose severe threats to classification performance of numerous machine-learning approaches , including, most notably, NNs [15; 33]. Naturally, there have been numerous efforts to mitigate this effect [28; 8]. These efforts can be categorized into robust architectures, robust regularization, robust loss function, _loss adjustment_, and sample selection . Robust architecture methods focus on developing custom NN layers and dedicated NN architectures. This differs from our approach, which is architecture agnostic and could potentially "wrap around" these methods. While robust regularization methods like data augmentation , weight decay , dropout , and batch normalization  can help to bolster performance, they generally do so under lower levels of dataset corruption. Our approach, on the other hand, is capable of handling high levels of corruption, and can seamlessly incorporate methods such as these. In label corruption settings, it has been shown that loss functions, such as robust mean absolute error (MAE)  and generalized cross entropy (GCE)  are more robust than categorical cross entropy (CCE). Again, our method is not dependent on a particular loss function, and it is possible that arbitrary loss functions, including robust MAE and GCE, can be swapped into our methodology with ease. Our approach resembles the loss adjustment methods most closely, where the overall loss is adjusted based on a (re)weighting scheme applied to training examples.

In _loss adjustment_ methods, individual training example losses are typically adjusted multiple times throughout the training process prior to NN updates. These methods can be further grouped into loss correction, loss reweighting, label refurbishment, and meta-learning . Our approach most closely resembles the _loss reweighting_ methods. Under this scheme each training example is assigned a unique weight, where smaller weights are assigned to examples that have likely been corrupted. This reduces the influence of corrupted examples. A training example can be completely removed if its corresponding weight becomes zero. Indeed, a number of loss reweighting methods are similar to our approach. For example, Ren et al.,  learn sample weights through the use of a noise-free validation set. Chang et al.  assign sample weights based on prediction variances, and Zhang et al.  examine the structural relationship among labels to assign sample weights. However, we view the need for a clean dataset, or at least one with sufficient class balance, by these methods as a shortcoming, and our method, in contrast, makes no assumption on the availability of such a dataset.

Satoshi et al.  propose a two-phased approach to noise cleaning. The first phase trains a standard neural network to determine the top-\(m\) most influential training instances that influence the decision boundary; these are subsequently removed from the training set to create a cleaner dataset. In the second phase, the neural network is retrained using the cleansed training set. Their method demonstrates superior validation accuracy for various values of \(m\) on MNIST and CIFAR-10. Although impressive, their method does not address the fact that most industrial datasets have a reasonably large amount of label corruption  which, upon complete cleansing, could also remove informative examples that lie close to the decision boundary. Additionally, the value of \(m\) is an additional hyper-parameter that could require significant tuning on different datasets and sources.

Mengye et al.  propose dealing with label noise and class imbalance by learning exemplar weights automatically. They propose doing so in the following steps: a) Create a pristine noise-free validation set. b) Initially train on a large, noisy training dataset, compute the training loss on the training set, train on the clean validation set, and compute the training loss on the validation set. c) Finally, compute the exemplar weights that temper the training loss computed in step two with validation loss. This approach is algorithmically the most similar to ours, with some key differences. The major difference is that it treats noise and class imbalance similarly. Our approach deals with noisy labels explicitly and can cope with almost any amount of class imbalance automatically, as tested in our experiments with the open-source Hate-Speech dataset, where we experimented with different prevalence levels of Hate-Speech text. The biggest drawback of the method proposed by Mengye et al. is that it requires a clean validation set, which in practice is almost impossible to obtain; if it were possible, it would not be very prohibitive to clean the entire dataset. Noise, typically, is an artifact of the generative distribution which cannot be cherry-picked as easily in practice. Our approach does not require a clean dataset to be operational or effective.

## 3 Methodology

### Mislabeling

Let \(\) denote a _feature_ space, with \(\) a corresponding _label_ space. Then \(:=\) will be a collection of feature-label pairs, with an unknown probability distribution \(D\). Throughout the forthcoming discussions, \(\{(x_{i},y_{i})\}_{i=1}^{N}\) will denote a sample of \(N\) feature-label pairs, for which some pairs will have a mislabeling. More precisely, we begin with a collection \((x_{i},_{i})\) drawn i.i.d. from \(D\), but there is some unknown set \(C\{1,,N\}\) denoting (corrupted) indices for which \(y_{i}=_{i}\) if and only if \(i C\). For those \(i C\), \(y_{i}\) is some incorrect label, selected uniformly at random, following the Noise Completely at Random (NCAR) model  also known as _uniform label noise_.

### Rockafellian Relaxation Method (RRM)

We adopt the empirical risk minimization (ERM)  problem formulation:

\[_{}_{i=1}^{N}J(;x_{i},y_{i})+r()\] (1)

as a baseline against which our method is measured. Given an NN architecture with (learned) parameter setting \(\) that takes as input any feature \(x\) and outputs a prediction \(\), \(J(;x,y)\) is the loss with which we evaluate the prediction \(\) with respect to \(y\). Finally, \(r()\) denotes a regularization term.

In ERM it is common practice to assign each training observation \(i\) a probability \(p_{i}=1/N\). However, when given a corrupted dataset, we may desire to remove those samples that are affected; in other words, if \(C\{1,...,N\}\) is the set of corrupted training observations, then we would desire to set the probabilities in the following alternative way:

\[p=(p_{1},...,p_{N})p_{i}=0,&i C\\ ,&i\{1,...,N\} C,\] (2)

where \(|C|\) is the cardinality of the unknown set \(C\). In this work, we provide a procedure - the _Rockafellian Relaxation Method_ (RRM) - with the intention of aligning the \(p_{i}\) values closer to the desired (but unknown) \(p\) of (2) in self-guided, automated fashion. It does so by adopting the Rockafellian Relaxation approach of . More precisely, we consider the problem

\[_{}v():=_{a U}_{i=1}^{N}(+u_{i} ) J(;x_{i},y_{i})+\|u\|_{1},\] (3)

where \(U:=\{u^{N}:_{i=1}^{N}u_{i}=0,+u_{i} 0\ \  i=1,,N\}\), and some \(>0\).

We proceed to comment on this problem that is nonconvex in general, before providing an algorithm.

### Analysis and Interpretation of Rockafellian Relaxation

Although problem (3) is nonconvex in general, the computation of \(v()\) for any fixed \(\) amounts to a linear program. The following result characterizes the complete set of solutions to this linear program, and in doing so, provides an interpretation of the role that \(\) plays in the loss-reweighting action of RRM.

**Theorem 3.1**.: _Let \(>0\) and \(c=(c_{1},,c_{N})^{N}\), with \(c_{min}:=_{i}c_{i}\), and \(c_{max}:=_{i}c_{i}\). Write \(I_{min}:=\{i:c_{i}=c_{min}\}\), \(I_{big}:=\{i:c_{i}=c_{min}+2\}\), and for any \(S_{1} I_{min},S_{2} I_{big}\),_\[conv(_{S_{1},S_{2}}U_{S_{1},S_{2}}^{*})=*{arg\,min}_{u  U}_{i=1}^{N}(+u_{i}) c_{i}+\|u\|_{1}.\] (4)

The theorem explains that the construction of any optimal solution \(u^{*}\) essentially reduces to categorizing each of the losses among \(\{c_{i}=J(;x_{i},y_{i})\}_{i=1}^{N}\) as "small" or "big", according to their position in the partitioning of \([c_{min},)=[c_{min},c_{min}+2)[c_{min}+2,)\). For losses that occur at the break points of \(c_{min}\) and \(c_{min}+2\), this classification can be arbitrary - hence, the use of \(S_{1}\) and \(S_{2}\) set configurations to capture this degree of freedom.

In particular, those points with losses \(c_{i}\) exceeding \(c_{min}+2\) are down-weighted to zero and effectively removed from the dataset. And in the event that \(c_{max}-c_{min}<2\), no loss reweighting occurs. In this manner, while lasso produces sparse solutions in the model parameter space, RRM produces sparse weight vectors by assigning zero weight to data points with high losses.

Consequently, if \(:=\{i:c_{i}(c_{min}+2,)\}\) converges over the course of any algorithmic scheme, e.g., Algorithm 1, to some set \(C\), then we can conclude that these data points are effectively removed from the dataset even if the training of \(\) might proceed. This convergence was observed in the experiments of Section 6. It is hence of possible consideration to tune \(\) for consistency with an estimate \(\) of labeling error in the dataset \(\{(x_{i},y_{i})\}_{i=1}^{N}\). More precisely, we may tune \(\) so that \(\).

### RRM and Optimistic Wasserstein Distributionally Robust Optimization

In this section, we discuss RRM's relation to distributionally robust and optimistic optimization formulations. Indeed, \((3)\)'s formulation as a min-min problem bears resemblance to optimistic formulations of recent works, e.g., . We will see as well that the minimization in \(u\), as considered in Theorem 3.1, relates to an approximation of a data-driven Wasserstein Distributionally Robust Optimization (DRO) formulation .

#### 3.4.1 Loss-reweighting via Data-Driven Wasserstein Formulation

For this discussion, as it relates to reweighting, we will lift the feature-label space \(=\). More precisely, we let \(:=_{+}\) denote a space of _weights_. Next, we say \(\) has an unknown probability distribution \(\) such that \(_{}=D\) and \(_{}(\{1\})=1\). In words, all possible (w.r.t. \(D\)) feature-label pairs have a weight of \(1\). Finally, we define an _auxiliary loss_\(:\) by \((w,z;):=w J(x,y;)\), for any \(z=(x,y)\).

Given a sample \(\{(1,x_{i},y_{i})\}_{i=1}^{N}\), just as in Section 3.2, we can opt not to take as granted the resulting empirical distribution \(_{N}\) because of the possibility that \(|C|\)-many have incorrect labels (i.e., \(y_{i}_{i}\)). Instead, we will admit alternative distributions obtained by shifting the \(_{N}\)'s probability mass off "corrupted" tuples \((1,x_{i},y_{i})_{i C}\) to possibly \((0,x_{i},y_{i})\), \((1,x_{i},_{i})\), or even some other tuple \((1,x_{j},_{j})\) with \(j C\) for example - equivalently, eliminating, correcting, or replacing the sample, respectively. In order to admit such favorable corrections to \(_{N},\) we can consider the optimistic  data-driven problem

\[_{}(v_{N}():=_{}:W_{1}(_{ N},})}_{}}[(w,z; )]),\] (5)

in which for each parameter tuning \(\), \(v_{N}()\) measures the expected auxiliary loss with respect to the most favorable distribution within an \(\) - prescribed \(W_{1}\) (1- Wasserstein) distance of \(_{N}\). It turns out that a budgeted deviation of the weights alone (and not the feature-label pairs) can approximate (up to an error diminishing in \(N\)) \(v_{N}()\). More precisely, we derive the following approximation along similar lines to .

**Proposition 3.2**.: _Let \(>0\), and suppose for any \(\), \(_{(x,y)}|J(;x,y)|<\). Then there exists \( 0\) such that for any \(\), the following problem_

\[v_{N}^{MIX}():=_{u_{1},,u_{N}} _{i=1}^{N}(+u_{i}) J(;x_{i},y_{i})+ _{}_{i=1}^{N}|u_{i}|\] _s.t._ \[u_{i}+ 0\;\;i=1,,N\]

_satisfies \(v_{N}()+ v_{N}^{MIX}() v_{N}()\)._

_In particular, \(-_{}_{i}J(;x_{i},y_{i})\), and \(\{i:J(;x_{i},y_{i})>_{}\}\) are all down-weighted to zero, i.e., \(u_{i}^{*}=-\) for any \(u^{*}\) solving \(v_{N}^{MIX}()\)._

In summary, while the optimistic Wasserstein formulation would permit correction to \(_{N}\) with a combination of reweighting and/or feature-label revision, the above indicates that a process focused on reweighting alone could accomplish a reasonable approximation; further, upon comparison to (3), we see that RRM is a constrained version of this approximating problem, that is,

\[v() v_{N}^{MIX}() v_{N}().\]

Hence, in some sense, we can confirm that RRM is an optimistic methodology but that it is less optimistic than the data-driven Wasserstein approach.

### RRM Algorithm

Towards solving problem (3) in the two decisions \(\) and \(u\), we proceed iteratively with a block-coordinate descent heuristic outlined in Algorithm 1, whereby we update the two separately in cyclical fashion. In other words, we update \(\) while holding \(u\) fixed, and we update \(u\) whilst holding \(\) fixed. The update of \(\) is an SGD step on a batch of \(s-\) many samples. The update of \(u\) reduces to a linear program. In light of the discussion in 3.4, we also outline an Adversarial Rockafellian Relaxation method (A-RRM), an execution of RRM that includes a perturbation (parameterized by \( 0\)) to the feature \(x\) of a sample \((x,y),\) for the purposes of adversarial training.

```
0: Perturbation Multiplier \(\), Number of epochs \(\), Batch size \(s 1\), learning rate \(>0,\) regularization parameter \(>0\), reweighting step \((0,1)\). \(u 0^{N}\) repeat for\(e=1,,\)do \(\{(x_{i}^{b},y_{i}^{b})\}_{i=1}^{s}\) Draw Batch of size \(s\) from \(\{(x_{i},y_{i})\}_{i=1}^{N}\) for i = 1,..., s do \(x_{i}^{b} x_{i}^{b}+ sign(_{x}J(;(x_{i} ^{b},y_{i}^{b})))\) endfor \(-_{i=1}^{s}(+u_{i}) _{}J(;(x_{i}^{b},y_{i}^{b}))\) endfor endfor \(u^{*}_{u U}_{i=1}^{N}(+u_{i})  J(;x_{i},y_{i})+\|u\|_{1}\) \(u u^{*}+(1-)u\) untilDesired Validation Accuracy or Loss ```

**Algorithm 1** (Adversarial) Rockafellian Relaxation Algorithm (A-RRM/RRM)

The stepsize parameters \(,\) and the regularization parameter \(\) are hyper-parameters that may be tuned, or guided by the general discussions above in Section 3.3.

The RRM algorithm, in which \(=0,\) is meant for contexts in which only label corruption and no feature corruption occurs. The A-RRM algorithm, for which \(>0,\) is intended for contexts in which both label and feature corruption is anticipated.

## 4 Datasets

We select several datasets to evaluate RRM. In some cases, the selected dataset is nearly pristine. In these cases we perturb the dataset to achieve various types and levels of corruption. Other datasets consist of weakly labeled examples, which we maintain unaltered. The varied data domains and regimes of corruption enable a robust evaluation of RRM.

**MNIST**: A multi-class classification dataset consisting of 70000 images of digits zero through nine. 60000 digits are set aside for training and 10000 for testing. 0%, 5%, 10%, 20%, and 30% of the training labels are swapped for different, randomly selected digits. The test set labels are unmodified.

**Toxic Comments**: A multi-label classification problem from JIGSAW that consists of Wikipedia comments labeled by humans for toxic behavior. Comments can be any number (including zero) of six categories: toxic, severe toxic, obscene, threat, insult, and identity hate. We convert this into a binary classification problem by treating the label as either none of the six categories or at least one of the six categories. This dataset is a public dataset used as part of the Kaggle Toxic Comment Classification Challenge.

**IMDB**: A binary classification dataset consisting of 50000 movie reviews each assigned a positive or negative sentiment label. 25000 reviews are selected randomly for training and the remaining are used for testing. 25%, 30%, 40%, and 45% of the labels of the training reviews are randomly selected and swapped from positive sentiment to negative sentiment, and vice versa, to achieve four training datasets of desired levels of label corruption. The test set labels are unmodified.

**Tissue Necrosis**: A binary classification dataset consisting of 7874 256x256-pixel hematoxylin and eosin (H&E) stained RGB images derived from . The training dataset consists of 3156 images labeled non-nerotic, as well as 3156 images labeled necrotic. The training images labeled non-nerotic contain no necrosis. However, only 25% of the images labeled necrotic contain necrotic tissue. This type of label error can be expected in cases of weakly-labeled Whole Slide Imagery (WSI). Here, an expert pathologist will provide a slide-level label for a potentially massive slide consisting of gigapixels, but they lack time or resources to provide granular, segmentation-level annotations of the location of the pathology in question. Also, the diseased tissue often occupies a small portion of the WSI, with the remainder consisting of normal tissue. When the gigapixel-sized WSI is subsequently divided into sub-images of manageable size for typical machine-learning workflows, many of the sub-images will contain no disease, but will be assigned the "weak" label chosen by the expert for the WSI. The test dataset consists of 718 necrosis and 781 non-nerosis 256x256-pixel H&E images, which were also derived from . For both the training and test images,  provide segmentation-level necrosis annotations, so we are able to ensure a pristine test set, and, in the case of the training set, we were able to identify the corrupted images for the purpose of algorithm evaluation.

## 5 Architectures

We do not strive to develop a novel NN architectures capable of defeating current state-of-the-art (SOA) performance in each data domain. Nor do we focus on developing _robust architectures_ as described in . Rather, we select a reasonable NN architecture and measure model performance with and without the application of RRM. This approach enables us to demonstrate the general superiority of RRM under varied data domains and NN architectures. We discuss the underlying NN architectures that we employ in this section.

**MNIST**: The MNIST dataset has been studied extensively and harnessed to investigate novel machine-learning methods, including CNNs . We adopt a basic CNN architecture with a few convolutional layers. The first layer has a depth of 32, and the next two layers have a depth of 64. Each convolutional layer employs a kernel of size three and the ReLU activation function followed by a max-pooling layer employing a kernal of size 2. The last convolutional layer is connected to a classification head consisting of a 100-unit dense layer with ReLU activation, followed by a 10-unit dense layer with softmax activation. In total, there are 159254 trainable parameters. Categorical cross-entropy is employed for the loss function.

**Toxic Comments**: We use a simple model with only a single convolutional layer. A pretrained embedding from FastText is first used to map the comments into a 300 dimension embedding space, followed by a single convolutional layer with a kernel size of two with a ReLU activation layer followed by a max-pooling layer. We then apply a 36-unit dense layer, followed by a 6 unit dense layer with sigmoid activation. Binary cross-entropy is used for the loss function.

**IMdb**: Transformer architectures have achieved SOA performance on the IMDb dataset sentiment analysis task [7; 32]. As such, we a adopt a reasonable transformer architecture to assess RRM. We utilize the DistilBERT  architecture with low-rank adaptation (LoRA)  for large language models, which reduces the number of trainable weights from 67584004 to 628994. In this manner, we reduce the computational burden, while maintaining excellent sentiment analysis performance. Binary cross-entropy is employed for the loss function.

**Tissue Necrosis**: Consistent with the computational histopathology literature , we employ a convolutional neural network (CNN) architecture for this classification task. In particular, a ResNet-50 architecture with pre-trained ImageNet weights is harnessed. The classification head is removed and replaced with a dense layer of 512 units and ReLU activation function, followed by an output layer with a single unit using a sigmoid activation function. All weights, with the exception of the new classification head are frozen, resulting in 1050114 trainable parameters out of 24637826. Binary cross-entropy is employed for the loss function.

## 6 Experiments and Results

In this work, we have discussed errors/perturbations/corruption to features and labels. We now perform experiments to see how RRM performs under one or the other, or both. The MNIST experiments are performed under a setting of both adversarial perturbation, as well as label corruption. The Toxic Comments experiments are performed under settings of label corruption only. All experiments are performed using a combination of GPU resources, both cloud-base, as well as access to an on-premise high-performance computing (HPC) facility. We refer the reader to the Appendix (Sections 6.3 and 6.4) for the experiments on IMDb and Tissue Necrosis.

### Mnist

Twenty percent of the training data is set aside for validation purposes. Using Tensorflow 2.10 , 50 iterations of RRM are executed with \(=10\) epochs per iteration for a total of 500 epochs for a given hyperparameter setting. For RRM, the hyperparameter settings of \(\) and \(\) at 0.5 and 2.0, respectively, are based on a search to optimize validation set accuracy. For contrast, we perform a comparable 500 epochs using ERM. Both ERM and RRM employ stochastic gradient descent (SGD) with a learning rate (\(\)) of \(0.1\). Each time a batch is drawn, each training image is perturbed using the Fast Gradient Sign Method (FGSM)  adversarial attack: \(adv_{x}=x+ sign(_{x}J(,x,y))\), where \(adv_{x}\) is the resulting perturbed image, \(x\) is the original image, \(y\) is the image label, \(\) is a multiplier controlling the magnitude of the image perturbation, \(\) are the model parameters, and \(J\) is the loss. An \(=1.0\) is used for all training image perturbations.

For each of the 0%, 5%, 10%, 20%, and 30% training label corruption levels, we compare adversarial training (AT) and adversarial RRM (A-RRM) performance under various regimes of test set perturbation (\(_{test}\)). In Table 1 we show the test set accuracy achieved when validation set accuracy peaks. We can see that training with an \(_{train}=1.0\) and testing with lower \(_{test}\) levels of \(0.00,0.10\), and \(0.25\), results in a drastic degradation in accuracy for AT for corruption levels greater than 0%. This performance collapse is not observed when using A-RRM. Given that it may be difficult to anticipate the adversarial regime in production environments, A-RRM seems to confer a greater benefit than AT.

We examine the \(u_{i}\)-value associated with each training observation, \(i\), from iteration-to-iteration of the heuristic algorithm. Table 2 summarizes the progression of the \(u_{i}\)-vector across its 49 updates for the dataset corruption level of 20%. Column "1. iteration" shows the distribution of \(u_{i}\)-values following the first u-optimization for both the 9600 corrupted training observations and the 38400 clean training observations. Initially, all \(u_{i}\)-values are approximately equal to 0.0. It is once again observed that, over the course of iterations, the \(u_{i}\)-values noticeably change. In column "10. iteration" it can be seen that a significant number of the \(u_{i}\)-values of the corrupted training observationsachieve negative values, while a large majority of the \(u_{i}\)-values for the clean training observations remain close to 0.0. Finally, column "49. iteration" displays the final \(u_{i}\)-values. 9286 out of 9600 of the corrupted training observations have achieved a \(u_{i}(-2.08,-1.56] 10-5\). This means these training observations are removed, or nearly-so, from consideration because this value cancels the nominal probability \(1/N\) = 2.08 \(\) 10-5. It is observed that a large majority (35246/38400) clean training observations remain with their nominal probability. This helps explain the performance benefit of A-RRM over AT. A-RRM "removes" the corrupted data points in-situ, whereas AT does not. It appears that under adversarial training regimes with corrupted training data, it is essential to identify and "remove" the corrupted examples, especially if the level adversarial perturbation encountered in the test set is unknown, or possibly lower than the level of adversarial perturbation applied to the training set.

### Toxic Comment

We use the Toxic Comment dataset to test the efficacy of RRM on low prevalence text data. The positive (toxic) comments consist of only 3% of the data and we corrupt anywhere from 1% to 20% of the labels. There are a total of 148,000 samples, and we set aside 80% for training and 20% for test. \(=2\) with 3 iterations of the heuristic algorithm results in a total of 6 epochs, and ERM is run for a total of 6 epochs to make the results comparable. Since the data is highly imbalanced, we look at the area under the curve of the precision/recall curve to assess the performance of the models. Unsurprisingly, as the noise increase, the model performance decreases. We note that RRM outperforms ERM across all noise levels tested, though as the noise increase, the gap between RRM and ERM decreases.

   &  \\   &  &  &  &  &  \\   & AT & A-RRM & AT & A-RRM & AT & A-RRM & AT & A-RRM \\  
0.00 & **97** & 96 & 63 & **95** & 57 & **97** & 58 & **96** & 26 & **86** \\ 
0.10 & **95** & 93 & 64 & **92** & 71 & **94** & 61 & **93** & 20 & **82** \\ 
0.25 & **93** & 90 & 83 & **91** & 88 & **92** & 84 & **90** & 74 & **81** \\ 
50 & **91** & 88 & **94** & 91 & **94** & 90 & **90** & 88 & **97** & 80 \\ 
1.00 & **86** & 83 & **95** & 90 & **94** & 86 & **88** & 83 & **98** & 77 \\  

Table 1: Test accuracy (%) for AT and A-RRM on MNIST under different levels of corruption \(C\) and test-set adversarial perturbation \(_{test}\).

   &  &  &  \\  \(u_{i}\) value & corrupted & clean data points & corrupted & clean data points & corrupted & clean data points & clean data points \\   \( 0\) & 0 & 1 & 0 & 4. & 0 & 25 \\  \( 0\) & 8844 & 38385 & 2058 & 37524 & 91 & 35246 \\  (-0.52, 0.00) \(\) 10-5 & 0 & 0 & 7 & 36 & 146 & 1655 \\  (-1.04, -0.52) \(\) 10-5 & 0 & 0 & 41 & 45 & 43 & 155 \\  (-1.56, -1.04) \(\) 10-5 & 756 & 14 & 415 & 174 & 34 & 168 \\  (-2.08, -1.56] \(\) 10-5 & 0 & 0 & 7079 & 617 & 9286 & 1151 \\  

Table 2: Evolution of u-vector across 9600 corrupted data points and 38400 clean data points. Note that 1/(9600 + 38400) = 2.08 \(\) 10-5.

   &  \\   & 1\% & 5\% & 7\% & 10\% & 15\% & 20\% \\  ERM (train) & 0.2904 & 0.2006 & 0.1589 & 0.1302 & 0.1073 & 0.0920 \\  RRM (train) & 0.6875 & 0.4458 & 0.3805 & 0.3087 & 0.2438 & 0.1966 \\   ERM (test) & 0.5861 & 0.3970 & 0.3246 & 0.2550 & 0.2013 & 0.1717 \\  RRM (test) & **0.6705** & **0.4338** & **0.3619** & **0.2824** & **0.2208** & **0.1861** \\  

Table 3: Comparison of training and test area under the precision/recall curve for ERM and RRM at noise levels ranging from 1% to 20%.

### IMDb

Twenty percent of the training data is set aside for validation purposes. Using Pytorch 2.1.0 , 30 iterations of RRM are executed, with \(=10\) epochs per iteration for a total of 300 epochs for a given hyperparameter setting. For RRM, the hyperparameter settings of \(\) and \(\) at 0.5 and 0.4, respectively, are based on a search to optimize validation set accuracy. For contrast, we perform a comparable 300 epochs using ERM. Both ERM and RRM employ stochastic gradient descent (SGD) with a learning rate (\(\)) of \(0.001\). In Table 4 we record both the test set accuracy achieved when validation set accuracy peaks, as well as the maximum test set accuracy. At these high levels of corruption RRM consistently achieves a better maximum test set accuracy.

### Tissue Necrosis

Twenty percent of the training data is set aside for validation purposes, including hyperparameter selection. 60 iterations of RRM are executed, with \(=10\) epochs per iteration, for a total of 600 epochs for a given hyperparameter setting. For RRM, the hyperparameter settings of \(\) and \(\) at 0.5 and 0.016, respectively, are based on a search to optimize validation set accuracy. For contrast, we perform a comparable 600 epochs using ERM. Both ERM and RRM employ stochastic gradient descent (SGD) with a learning rate (\(\)) of \(5.0\) and \(1.0\), respectively. RRM achieves a test set accuracy at peak validation accuracy of **74.6**, and a maximum test set accuracy **77.2**, whereas ERM achieves 71.7 and 73.2, respectively. RRM appears to confer a performance benefit under this regime of weakly labeled data.

## 7 Conclusion

In this study, we demonstrate the robustness of the A-RRM algorithm in a variety of data domains, data corruption schemes, model architectures and machine learning applications. In the MNIST example we show that conducting training in preparation for deployment environments with varied levels of adversarial attacks, one can benefit from implementation of the A-RRM algorithm. This can lead to a model more robust across levels of both feature perturbation and high levels of label corruption. We also demonstrate the mechanism by which A-RRM operates and confers superior results: by automatically identifying and removing the corrupted training observations at training time execution.

The Toxic Comment example presents another challenging classification problem, characterized by a low prevalence target class amidst label noise. Our experiments demonstrate that as the amount of label noise increases, standard methods become increasingly ineffective. However, RRM remains reasonably robust under varying degrees of label corruption. Therefore, RRM could be a valuable addition to the set of tools being developed to enhance the robustness of AI-based decision engines.

In the IMDb example we demonstrate that RRM can confer benefits to the sentiment analysis classification task using pre-trained large models under conditions of high label corruption. The success of fine-tuning in LLMs depends, in large part, on access to high quality training examples. We have shown that RRM can mitigate this need by allowing effective training in scenarios of high training data corruption. As such, resource allocation dedicated to dataset curation may be lessened by the usage of RRM.

In the Tissue Necrosis example, we demonstrate that RRM also confers accuracy benefits to the necrosis identification task provided weakly labeled WSIs. Again, RRM can mitigate the need for expert-curated, detailed pathology annotations, which are costly and time-consuming to generate.

   &  \\   & 25\% & 30\% & 40\% & 45\% \\  ERM & _90.2_, 90.2 & 89.5, 89.6 & 86.4, 86.6 & _80.7_, 81.1 \\  RRM & _90.1_, **90.4** & _90.2_, **90.4** & _88.4_, **88.7** & 76.9, **82.6** \\  

Table 4: Test accuracy (%) for ERM and RRM on IMDb under different levels of corruption. Test set accuracy at peak validation accuracy and maximum test set accuracy are recorded.