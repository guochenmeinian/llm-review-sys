# DafnyBench: A Benchmark for Formal Software Verification

Chloe Loughridge

Harvard College

cloughridge@college.harvard.edu

&Qinyi Sun

Massachusetts Institute of Technology

wendysun@mit.edu

&Seth Ahrenbach

seth.ahrenbach@omnifederal.com

&Federico Cassano

Northeastern University

cassano.f@northeastern.edu

&Chuyue Sun

Stanford University

chuyues@stanford.edu

&Ying Sheng

Stanford University

ying1123@stanford.edu

&Anish Mudide

Massachusetts Institute of Technology

amudide@mit.edu

&Md Rakib Hossain Misu

University of California Irvine

mdrh@uci.edu

&Nada Amin

Harvard University

namin@seas.harvard.edu

&Max Tegmark

Massachusetts Institute of Technology

tegmark@mit.edu

Equal contribution. Order determined alphabetically.Corresponding author.

###### Abstract

We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification. We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough annotations for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code. The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and annotations. We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality.

## 1 Introduction

Rapidly improving Large Language Models (LLMs)  are helping accelerate software development through program synthesis tools. But how can we ensure that LLM-generated code meets our specifications and reliably does precisely what it is supposed to do? Indeed, this remains a persistent problem even with human-written code: major code-testing efforts failed to prevent e.g. bugs causing an Ariane-V rocket explosion  and security vulnerabilities in ssh  and the Bash shell .

Although _formal verification_ can guarantee reliability, providing rigorous mathematical proof that software meets specification, it has yet to gain widespread adoption. Formally verifying code is oftena significant burden on the developer [7; 8]. Also, existing formal verification tools involve a major learning curve above and beyond just coding, greatly reducing the pool of people able to do this work.

Machine learning methods have the potential to minimize a common pain point of formal methods, i.e., writing formal specifications. To support automation of formal verification, this paper's goal is to build a benchmark by assembling a suite of formally verified programs written in _Dafny_, a formal verification language developed for easy adoption by programmers due to its similarity with popular imperative programming languages such as Python . For formal verification to succeed, most of these programs require supplementary "annotations" to guide the automated theorem prover.

## 2 Related Work

As summarized in Table 1 below, there is a striking lack of training data for formal verification: while there are hundreds of thousands of training examples for proving mathematical theorems and over ten thousand training examples for synthesizing programs, there are only \(66+153=219\) for proving program correctness. This motivates our work in the current paper to expand the largest existing formal verification benchmarks from _Clover_ and _dafny-synthesis_.

## 3 DafnyBench Construction

### Sourcing Ground Truth Programs

In total, our DafnyBench benchmark contains 782 ground_truth stand-alone Dafny programs that compile and verify. These programs come from the following sources:

* **GitHub Scrape**: We scraped all publicly available Dafny files on GitHub published on and before the end of 2023. We adapted a deduplication script from  to retain a unique set of the scraped Dafny files. The deduplication process reduced the number of.dfy files from \(\)15,000 to \(\)5,000. We removed any files that did not verify, which left 1,112 files. We found 374 of these files lacked ensures statements (postconditions) and 459 of them lacked assert and invariant clauses (annotations), and removed the union of these sets, which left 556 ground_truth files. Out of these files, 113 verify without any annotations.
* **Clover**: We added 62 ground truth textbook Dafny programs provided by the _Clover_ benchmark . Out of these files, 23 verify without any annotations.

  
**Category** & **Dataset** & **Size** \\   & CoqGym  & 71,000 proofs \\  & LeanDojo  & 98,734 proofs \\  & PISA  & 138,000 proofs \\  & Natural Proofs  & 15,000 proofs \\  & Archive of Formal Proofs  & 1 million lines of code \\   & APPS  & 10,000 programs \\  & HumanEvalX [18; 19] & 165 programs \\   & MBPP  & 974 programs \\   & SWEBench  & 2,294 programs \\   & LiveCodeBench  & grows weekly \\   & Clover  & 66 programs \\   & Dafny-synthesis  & 153 programs \\   

Table 1: Summary of popular machine learning benchmark datasets for proving mathematical theorems, synthesizing programs, and formally verifying programs. Size is measured by the number of samples in each dataset. In the formal reasoning datasets, each sample is usually a math problem or a theorem. In the program synthesis and verified software programming benchmarks, each sample is a program.

* **Dafny-synthesis**: Finally, we included 164 Dafny programs provided by the _dafny-synthesis_ benchmark. These problems have been translated from the MBPP benchmark . Out of these files, 72 verify without any annotations.

The ground_truth programs in our dataset have on average 2.12 methods, 1.03 functions, and 1.40 lemmas. This places the mean complexity of our examples at a level higher than _Clover_ alone, which has only one stand-alone method per example. For more detailed summary statistics of DafnyBench dataset, see Appendix A.

### Task Design: Fill Annotations

We implemented the fill_annotations task. For this task, we took a ground_truth program, removed all of its annotations (all of the assert and invariant statements in the body of the code), and asked LLM to fill annotations back in so that the resulting program could be verified with Dafny.

Evaluation MetricAn LLM's attempt to fill annotations back in for a test program is counted as a success if all following conditions are satisfied: 1) The reconstructed program is verified with Dafny; 2) LLM preserves all preconditions (requires statements) and postconditions (ensures statements); and 3) LLM does not use {:verify false} or {assume false} to "cheat."

## 4 Experiments

### Hyperparameters & Prompts

We set max_tokens\(=4096\), which corresponds to the lowest max output token limit among all the evaluated models, and we set temperature\(=0.3\). We gave each model up to \(n=10\) attempts at a given file. If the model failed on any of the intermediate attempts, it received the Dafny error message and was asked to fill in annotations again with the error message taken into consideration. If it failed on all \(n\) attempts, it was considered to fail on that specific test program. See Appendix C for prompts.

### Basic Results

We tested GPT-4o, GPT-4 Turbo , GPT-3.5 Turbo , Claude 3 Opus , and CodeLlama-7b-Instruct-hf  on the 782 programs. Table 2 shows that Claude 3 Opus achieved the highest success rate \( 68\%\).

Figure 1: A verified ground_truth program. To create fill_annotations task, we remove the invariant lines, and ask LLM to fill back in equivalent lines so that the resulting program verifies.

### Difficulty Utilizing Dafny Error Messages

Figure 2 shows how the cumulative success rate improved with more attempts \(n\). We see that the best models succeeded on the first try about 54%, with rapidly diminishing returns after that, approaching a plateau at about 65% for \(n 5\). This suggests that the LLMs are not great at taking Dafny error messages into consideration, or struggle to cope with the underlying task.

### Difficulty Grows with Program Length

Figure 2(a) shows that the success rate drops with program length. An obvious explanation could be that there is more to verify. Also, as a program gets longer, there may be more dependencies among variables, functions, methods, and classes, increasing the overall verification difficulty level.

### Difficulty Grows with Annotation Quantity

Figure 2(b) shows that the success rate drops with annotation quantity, defined as the number of characters in the lines of annotations. In other words, the success rate drops with the amount of work that LLM needs to do (the amount of text that it needs to insert in the right places).

### Models' Common Failure Types

To analyze where LLMs failed on the benchmark, we categorized failures into nine types, including verification logic error, code logic error, type error, resolution error, syntax issue, altered specification,

   Model & \% Success \\  No LLM & \(26.9\) \\ GPT-3.5 Turbo & \(44.0 1.8\) \\ GPT-4 Turbo & \(59.8 1.8\) \\ GPT-4o & \(59.3 1.8\) \\ Claude 3 Opus & \( 1.7\) \\ CodeLlama-7b-Instruct-hf & \(28.0 1.6\) \\   

Table 2: Models’ success rates at writing annotations for DafnyBench, with \(n=10\) attempts given. Dafny succeeds in auto-verifying some programs even without annotations, corresponding to the “No LLM” \(26.9\%\) success rate baseline.

timeout, trivial verification, and others. For a test program that a model failed at, we: 1) checked for timeout, cheating by altering specification, and cheating by trivial verification; and 2) passed Dafny error message from the failed program to Claude and asked it to classify the failure type. Table 3 explains each failure type, and Figure 4 gives by-model statistics of failure types.

## 5 Discussion & Conclusions

We have assembled the largest machine learning benchmark to date for formal software verification and made it publicly available on GitHub at https://github.com/sun-wendy/DafnyBench.

### Benchmark Evaluation Limitations

Data contamination emerges as a potentially significant limitation for evaluating LLMs on DafnyBench. Scraping data from platforms such as GitHub introduces risks of leveraging previous models' training data into the benchmark evaluation, potentially inflating the abilities of certain models.

Another limitation emerges in that DafnyBench does not assess a model's competence in translating natural language into concise formal specifications. Arguably, this conversion is a demanding and crucial skill we seek from language models: the capacity to validate, beyond merely verifying code. The pivotal question is whether a model can assist in identifying the essential properties an algorithm must fulfill. This provides an exciting frontier for future work, which we begin to brainstorm in Appendix D.

For further discussion on LLM's potential for auto-verifying program synthesis and synthesizing specifications from natural language, see Appendix E.

Figure 4: **Counts of failures by failure type and by model**. Note that a model could have multiple failures for a single test program (for example, it might have both verification logic error and syntax issue). Also note that the closed-source models had most of their failures at verification logic, while the open-source model had most of its failures at syntax issues and cheating by altering specification.

  
**Failure Type** & **Examples** \\  Code logic error & Index out of range / Target object might be null \\ Verification logic error & Cannot prove termination / Assertion might not hold \\ Syntax issue & lbrace/rbrace expected / Semicolon expected / Unresolved identifier \\ Type error & Value does not satisfy the subset constraints of ’nat’ \\ Resolution error & Boogie program had... resolution errors \\ Timeout & Verification timeout \\ Trivial verification & Cheating by using \{:verify false\} or assume false \\ Altered specification & Cheating by altering provided specification \\ Other & Failure type not belonging to any listed category above \\   

Table 3: **Examples of failure types**. Note that the examples are samples, not a complete list, for each failure type.

**Acknowledgements:** The authors wish to thank Clark Barrett, Rustan Leino, Daniel Windham, David Brandfonbrener, William Byrd, Josh Engels, and Anastasiya Kravchuk for helpful discussions.