# Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias

Yue Yu\({}^{1}\)

Yuchen Zhuang\({}^{1}\)

Jieyu Zhang\({}^{2}\)

Yu Meng\({}^{3}\)

Alexander Ratner\({}^{2}\)

Ranjay Krishna\({}^{2}\)

Jiaming Shen\({}^{4}\)

Chao Zhang\({}^{1}\)

\({}^{1}\) Georgia Tech \({}^{2}\) University of Washington \({}^{3}\) UIUC \({}^{4}\) Google Research

{yueyu, yczhuang, chaozhang}@gatech.edu, yumeng5@illinois.edu

{jieyuz2, ajratner, ranjay}@cs.washington.edu, jmshen@google.com

###### Abstract

Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5% of the querying cost of ChatGPT associated with the latter2.

## 1 Introduction

Large language models (LLMs) have demonstrated exceptional performance across a broad range of NLP tasks . In recent research, LLMs have been proposed as training data generators, particularly for text classification, aiming to alleviate the need for task-specific data and annotations . While these efforts have showcased the effectiveness of LLMs as data generators, the focus has primarily been on advancing the training stage, where the generated data are utilized to train task-specific models, leaving the upstream data generation process relatively unexplored. Notably, the prevailing approach employs a simple class-conditional prompt for querying LLMs during data generation, potentially limiting the diversity of the generated data  and inheriting systematic biases inherent in LLMs . We refer to this simple class-conditional prompt as SimPrompt, providing an example in Table 1.

In this work, we ground the LLM to ChatGPT  for its ability to generate high-quality, human-like text, and consider four challenging topic classification tasks with high cardinality from various domains. Our investigation primarily revolves around assessing the bias and diversity present within the generated training set through the lens of _data attributes_. In particular, data attributes encompass multiple attribute dimensions and their corresponding attribute values, where the latter representpossible instantiations of the former. For example, an attribute value such as "_shorter than 200 words_" could serve as an instantiation of the attribute dimension "_length_".

On one hand, we employ a trained attribute classifier to examine the _attribute bias_ present in the dataset generated using SimPrompt. When analyzing the "_location_" attribute in the NYT news dataset, we observe a striking bias towards "_North America_" in the predicted values of the generated data, accounting for a significant majority (68.01%). In contrast, instances associated with "_Africa_" are remarkably rare, comprising only 0.69% of the dataset (100 times less prevalent than "_North America_"). This regional bias exhibited in the generated dataset can pose substantial challenges when constructing reliable machine learning models [27; 4].

On the other hand, we explore the influence of _attribute diversity_ on the downstream model performance. Specifically, we leverage ChatGPT to generate attributed data by incorporating desired attributes as constraints in the prompts. By comparing the performance of models trained on datasets generated using prompts with random attributes against those with fixed attributes, we observe a substantial underperformance of the latter, uncovering the importance of attribute diversity of the generated dataset. The workflow of AttrPrompt is shown in Figure 1.

To alleviate attribute biases and enhance the attribute diversity of the generated data, we propose to generate data with diversely attributed prompts. For a given classification task, we start by identifying attribute dimensions and their corresponding attribute values in an interactive, semi-automated process facilitated by the LLM. Subsequently, we generate diverse prompts by combining attributes randomly, replacing the simple class-conditional prompt typically used for querying data from the LLM. We refer to these diversely attributed prompts as AttrPrompt. An example of such prompts can be found in Table 1, where the LLM is instructed to generate training data based on attributes such as location and style.

On the four classification tasks, we empirically evaluate the generated datasets by measuring the performance of models trained using two scenarios: 1) solely on the generated dataset, and 2) on a merged dataset comprising the real training set and the generated set. In both scenarios, the dataset generated with AttrPrompt significantly outperforms its counterpart generated with SimPrompt. Furthermore, we demonstrate the superiority of AttrPrompt over SimPrompt in terms of data/budget

  
**Method** & **Prompt** \\  SimPrompt & Suppose you are a news writer. Please generate a {topic-class} news in NYT. \\   & Suppose you are a news writer. Please generate a {topic-class} news in NYT following the requirements below: \\  & 1. Should focus on {subtopic}; \\  & 2. Should be in length between {length:min-words} and {length:max-words} words; \\  & 3. The writing style of the news should be {style}; \\  & 4. The location of the news should be in {location}. \\   

Table 1: Prompt template for the NYT news dataset.

Figure 1: The overall workflow of AttrPrompt.

efficiency and compatibility with different model sizes/various LLM-as-training-data-generator approaches. Notably, AttrPrompt achieves the performance of SimPrompt while utilizing only 5% of the querying cost of ChatGPT associated with SimPrompt. Lastly, we extend the LLM-as-training-data-generator paradigm to the more challenging multi-label classification tasks for the first time, and AttrPrompt outperforms SimPrompt across all evaluation metrics.

## 2 Related Work

LLMs as Training Data Generators.With the remarkable success of large language models (LLMs), researchers have recently attempted to leverage them as training data generators. Such applications include generating tabular data , medical dialogue , sentence pairs , instruction data [40; 50; 53; 48], _etc._. Among these applications, we anchor on training data generation for topic classification in a zero-shot setting where no labeled data is available. In this direction, existing approaches typically use simple class-conditional prompts while focusing on mitigating low-quality issues after generation. In particular, SuperGen  and ZeroGen  took initial steps to explore using LLM as a training data generator for text classification with simple class-conditional prompts, and leveraged additional noise robust learning techniques [26; 36; 52] to deal with the low-quality issue of the generated data. SunGen  reweights the generated data during training with learned data quality weight, and ProGen  leverages the model feedback to select highly influential generated data which then serve as labeled examples for generation. In this work, we instead explore attributed prompts to reduce the issue of low informativeness and redundancy, which can be readily incorporated into the existing systems mentioned above. Notably, Chen et al.  also explores prompts to advance the data generation process, yet it adopts soft prompts and requires a white-box LLM and seed examples to tune them. In contrast, our method is applicable to black-box LLMs and even LLM APIs (_e.g._, ChatGPT) and does not rely on any labeled examples. Similarly, WANLI  also considers human-AI collaboration for creating more challenging training data, but requires an initial dataset and a strong task model. Instead, we aim to generate training data without any initial dataset or a pre-existing task model, which allows us to effectively handle resource-limited scenarios.

Discrete Prompt Optimization.Several works attempt to optimize discrete prompts for querying LLMs, such as prompt paraphrasing , prompt editing , or prompt searching . Recently, [45; 59; 35] use large language models to optimize prompts. More related to us,  reframe prompts by decomposing a complex task instruction into multiple simple ones. However, these approaches mainly focus on the _inference_ stage for directly predicting the answer and may rely on additional labeled examples for validation. Our focus is on an orthogonal setting, optimizing prompts for LLMs with attributes to diversify the generated training data. This approach improves the model's overall performance without the need for additional labeled examples.

## 3 Large Language Model as Attributed Training Data Generator

### Datasets

While previous research has primarily focused on binary classification datasets [54; 31; 55] or datasets containing a maximum of 14 classes [15; 57], the performance of LLM as a data generator for topic classification with high cardinality (_i.e._, many topic classes) remains unclear. Thus, we consider the following datasets from various domains with the number of topics ranging from 23 to 50:

* **NYT**: The NYT dataset comprises news articles that were authored and published by _The New York Times_. These articles are categorized into 26 fine-grained categories.
* **Amazon**: The Amazon dataset contains customer reviews on products from Amazon's online store. It covers products from 23 different categories.
* **Reddit**: The Reddit dataset consists of a vast collection of user-generated content from the popular social media platform Reddit. It encompasses a wide range of topics, discussions, and interactions among users across numerous communities.
* **StackExchange**: The StackExchange dataset is a rich collection of structured data encompassing various online communities and knowledge-sharing platforms. It contains a vast array of questions, answers, comments, tags, and user interactions about specific technical problems.

We summarize the statistics of used dataset in Table 2, from which we can see that the involved datasets not only have high cardinality but also come with high imbalance ratio, _i.e._, the ratio of the sample size of the majority class to that of the minority class, which reflects the long-tail class issue in real applications.

### Attributes

Our initial step involves identifying various types of data attributes (or metadata) that can be manipulated to generate attributed data samples. To facilitate this process, we employ ChatGPT to help establish both attribute dimensions and attribute values. Specifically, we begin by engaging ChatGPT in generating essential attribute dimensions. This is achieved by posing questions such as "_Which attribute dimensions do you consider vital in determining the topic of a news article?_" for the NYT dataset, resulting in responses like "_subtopics, length, location, reader group, style, time_". Then, we manually select the attribute dimensions of the highest quality that best suit the dataset. Similarly, we prompt ChatGPT (the prompt format is listed in Appendix G) to suggest potential attribute values within each attribute dimension and choose high-quality candidates.

Attribute dimensions and values.There are two types of attribute dimensions: _class-independent_ attributes and _class-dependent_ attributes. Class-independent attributes, such as "_length_", remain unchanged across different classes, while class-dependent attributes, like "_subtopic_", have varying attribute values for each class. We list attribute dimensions and values for all datasets in Table 3. These data attributes provide a human-manipulatable interface for generating attributed data. In this study, we explore the potential of leveraging attributes to enhance the data generation process, while leaving the search for the optimal data attributes for a specific task to future work.

Class-dependent attribute value filtering.When dealing with class-dependent attributes, it is crucial to ensure that their attribute values are specifically associated with the corresponding class to avoid ambiguity and potential connections to multiple classes. For example, in the case of the "_economy_" class in the NYT dataset, a candidate attribute value generated by ChatGPT for the "_subtopic_" could be "_effect of trade tariffs on manufacturing companies_", which is also relevant to the "_international business_" class in the NYT. This overlap may introduce ambiguity in the generated data. To address this issue, we employ a filtering process called Class-Dependent Attribute Value Filtering (CAF). First, we query ChatGPT for the top-5 similar classes and then check with ChatGPT whether each class-dependent attribute value is related to these top-5 similar classes. Then, if the answer is positive which indicates a potential ambiguity, we remove that attribute value for the specific class.

 
**Dataset** & **\# configurations/ class** & **Attribute dimension** &  \\   & & Subtopic* & & & & & & \\  &  & Location & & Axis, North America, South America, Africa, Oceania, Europe & & & & \\   & & Writing Style & Investigative journalism, Op-Eds, Feature writing, News analysis, Profiles and interviews & & & & \\  & & Length & & short (30-80 words); long (100-150 words) & & & \\   &  & Product Barnab* & & & & & & \\   & & Product Name* & & & & & & \\   & & Usage Experience & & & & & & \\   & & Writing Style & & & & & & \\   & & Length & & & & & & \\   &  & Resources* & & & & & & \\   & & Experience* & & & & & & \\   & & Writing Style & & & & & & \\   & & Length & & & & & & \\   &  & Scenario* & & & & & & \\   & & Technical Depth & & & & & & \\   & & Writing Style & & & & & & \\   & & Length & & & & & & \\  

Table 2: Statistics of datasets.

 
**Dataset** & **Domain** & **Task** & **\# Train** & **\# Valid** & **\# Test** & **\# Class** & **Imbalance Ratio** \\  Amazon  & Reviews & Multi-class & 15.0K & 0.2K & 1.2K & 23 & 155.6 \\ NYT  & News & Multi-class & 9.0K & 0.2K & 1.2K & 26 & 357.0 \\ Reddit  & Web & Multi-class & 26.6K & 0.2K & 2.3K & 45 & 447.4 \\ StackExchange  & Web & Multi-class & 27.0K & 0.3K & 2.5K & 50 & 1283.7 \\  

Table 3: Attribute dimensions and values. Attributes with an asterisk* are class-dependent attributes.

[MISSING_PAGE_FAIL:5]

dataset generated with AttrPrompt exhibits lower cosine similarity and the distribution is close to that of the Gold, which shows AttrPrompt could render more diverse data. Apart from the above automatic evaluation processes, we also conduct _human study_ in Appendix E to manually evaluate the quality of the generated training data.

### Is there attribute bias in real and generated data?

We study the attribute bias in both real dataset (Gold) and generated dataset of SimPrompt using dataset generated by AttrPrompt as a probe. In particular, we leverage the attributes associated with each data of AttrPrompt to train an _attribute classifier_, which is in turn used to make attribute predictions on Gold and SimPrompt dataset. Note that the attribute values associated with each data of AttrPrompt are not necessary the ground truth, yet since ChatGPT has shown remarkable performance in following instructions , the generated data could decently reflect the desired attributes and therefore the attribute classifier trained with them could partially reveal the underlying attribute distribution of tested dataset, _i.e._, Gold and SimPrompt.

We pick the "_location_" attribute of the NYT data and visualize the distributions of the predicted "_location_" of Gold and SimPrompt in pie charts (Figure 3). One can see that both the Gold and SimPrompt dataset are largely biased towards "_North America_" in terms of the whole dataset (subfigures (a)&(e) in Figure 3). As to the "_location_" distribution for specific classes, we can see that Gold and SimPrompt are biased towards continents other than "_North America_". In contrast, with attributed prompts, the generated dataset of AttrPrompt comes with relatively balanced attribute distributions (Appendix F.2). While existing works using LLMs as data generators usually overlook the bias embedded in the generated data, we hope that this preliminary analysis could raise the attention of the community to the attribute bias behind the generated data of LLMs such as ChatGPT.

### How important the attribute diversity is?

We investigate the impact of attribute diversity within AttrPrompt on model performance. Specifically, we conduct experiments by fixing one attribute dimension to a candidate value while keeping other

    &  &  &  &  \\   &  &  &  &  &  &  &  &  \\  Gold & 70.8k & 11.3k & 44.7k & 6.64k & 50.8k & 4.62k & 52.3k & 3.60k \\ SimPrompt & 20.6k & 3.13k & 11.6k & 2.50k & 19.9k & 3.06k & 13.3k & 2.20k \\ AttrPrompt & 21.4k & 3.50k & 14.0k & 2.76k & 25.4k & 3.64k & 17.8k & 2.93k \\   

Table 5: Comparison of the vocabulary size of different datasets.

    &  &  \\   & Inter-Class APS & Intra-Class APS & APS & INGF & Inter-Class APS & Intra-Class APS & APS & INGF \\  Gold & 0.098 & 0.358 & 0.122 & 7618.1 & 0.101 & 0.251 & 0.114 & 4992.1 \\ SimPrompt & 0.101 & 0.568 & 0.135 & 5277.2 & 0.207 & 0.620 & 0.241 & 2266.5 \\ AttrPrompt & 0.159 & 0.474 & 0.182 & 6688.6 & 0.225 & 0.483 & 0.246 & 2605.5 \\   &  \\   & Inter-Class APS & Intra-Class APS & APS & INGF & Inter-Class APS & Intra-Class APS & APS & INGF \\  Gold & 0.044 & 0.261 & 0.054 & 9079.6 & 0.056 & 0.196 & 0.063 & 5492.4 \\ SimPrompt & 0.173 & 0.818 & 0.201 & 2697.8 & 0.282 & 0.804 & 0.302 & 2259.8 \\ AttrPrompt & 0.106 & 0.474 & 0.122 & 3994.5 & 0.105 & 0.375 & 0.114 & 2464.3 \\   

Table 6: Comparison of two quantitative metrics on diversity: the average pairwise sample similarity (APS) and inter-sample N-gram frequency (INGF) of different datasets. For APS, the _lower_ stands for better diversity. For INGF, the _higher_ stands for better diversity.

Figure 2: The distribution of cosine similarity of text pairs sampled from the same class.

attribute values random. Then, we generate 50 data per class using such a one-fixed-others-random configuration to compose a dataset and evaluate the performance of the trained model. Note that for class-dependent attributes, we sample one value for each class and repeat it 5 times, since it is _computationally prohibitive_ to enumerate all combinations of class-dependent attribute values. In Figure 4, each bar stands for a specific one-fixed-others-random configuration; compared to random configurations, most of one-fixed-others-random configurations result in a performance drop.

To further reduce the attribute diversity, we pick the attribute value with the best performance for each attribute dimension (the highest bar within each attribute dimension) and compose them to a single configuration (the dashed blue line). We can see that the dashed blue line is significantly worse than the random configuration, even though it is composed of individually best attribute values. This illustrates the importance and necessity of designing prompts with diverse attributes.

## 5 Experiments on the Trained Models

### Training with generated data

We quantitatively evaluate the quality of generated datasets via the test performance of models trained with them. Apart from the AttrPrompt and the direct baseline SimPrompt, we include an additional baseline MetaPrompt  which leverage LLM to generate additional guidance information for improving upon SimPrompt. The details for MetaPrompt are shown in Appendix K. We also directly use ChatGPT as a zero-shot predictor for comparison. The results are in Table 7. Besides the test

Figure 4: Bar charts of model performance with different attribute configurations of AttrPrompt.

Figure 3: Pie charts of the distributions of “_location_” predicted by an attribute classifier for the NYT SimPrompt and Gold dataset. (a) and (e) are “_location_” distribution over the whole dataset, while others are for specific classes. Illustration on types of biases on other datasets can be found in Appendix F.1.

performance, we include the cost of querying the ChatGPT per 1000 data in the table. From the results, we can draw the following conclusions. First, the AttrPrompt consistently renders better performance compared to the SimPrompt with a margin of 6-10 points3. Second, the class-dependent attribute value filter (CAF) is beneficial since the AttrPrompt outperforms its variant without CAF4. Third, although MetaPrompt  attempts to optimize the class-dependent prompt, they are less effective than AttrPrompt as they do not resolve the unique bottleneck (i.e. limited diversity) of the current prompting strategies for the training data generation task. Fourth, out of the four datasets, the AttrPrompt outperforms the LLM zero-shot method on three datasets in terms of accuracy, while for the F1 score, the AttrPrompt surpasses the LLM zero-shot on all the datasets; combined with the observation that the LLM zero-shot inference incurs much higher costs compared to data generation and the fact that the generated data is re-usable for training any model, we argue that for topic text classification generating training data could be a better practice of leveraging LLM than direct zero-shot inference. Lastly, in most cases, the generated data underperform the original training set, indicating that there is still room for future improvement. We conduct further studies in Appendix D.5 to illustrate the performance over different classes.

### Augmenting existing dataset with generated data

Here, we merge the generated dataset and the original training set into a single training set, and then test the model performance when it is trained with the merged dataset to see whether the generated dataset can further improve model performance with the original training set available. We present the results in Table 8. From the table, we can see that the generated dataset is an effective complement to the original training set, since most of the generated datasets introduce performance gain when combined with the original training set, especially our AttrPrompt which leads to improvement for all the cases. This notable improvement with simple dataset merge may motivate future studies of more advanced ways of using the generated data as augmentations to boost existing dataset.

### The budget and sample efficiency of the generated data

Here, we aim to study two types of efficiency of the generated dataset, _i.e._, budget efficiency and sample efficiency, on the model performance. We use Amazon and NYT as examples, and the results for other datasets are deferred to Appendix D.3. First, in Figure 5(a) and 5(b), we compare the budget efficiency of AttrPrompt against that of SimPrompt. Surprisingly, AttrPrompt only requires 5% of budget to be on par with or outperform SimPrompt with 100% of budget across all the datasets. This observation highlights the significance of diverse prompts in the training data generation process.

    &  &  &  &  \\   & Acc. & F1 & Price/lk & Acc. & F1 & Price/lk & Acc. & F1 & Price/lk & Acc & F1 & Price/lk \\  LLM Zero-Shot & 74.16 & 69.84 & 5.44 & 59.55 & 54.56 & 2.11 & 67.00 & 56.66 & 2.89 & 44.70 & 43.80 & 3.12 \\  Gold & 83.80 & 81.02 & — & 82.23 & 81.12 & — & 84.22 & 83.38 & — & 67.56 & 63.28 & — \\ SimPrompt & 75.47 & 76.22 & 0.76 & 57.34 & 56.96 & 0.77 & 53.48 & 53.81 & 0.65 & 42.88 & 41.30 & 0.69 \\ MetaPrompt & 79.58 & 79.83 & 0.87 & 56.35 & 55.98 & 0.84 & 54.61 & 54.30 & 0.74 & 44.81 & 44.02 & 0.83 \\ AttrPrompt w/o CAF & 80.40 & 80.92 & 0.91 & 61.67 & 61.57 & 0.82 & 61.22 & 60.18 & 0.72 & 45.90 & 44.84 & 0.81 \\ AttrPrompt & 81.30 & 82.26 & 1.05 & 66.08 & 65.65 & 0.87 & 63.33 & 63.10 & 0.84 & 48.99 & 47.42 & 0.90 \\   

Table 7: Performance of the models trained with created datasets and the cost of constructing the datasets. The results are averaged over five runs. The gain of AttrPrompt has passed the statistical test with \(p<0.05\). We also include the performance and cost of using LLM as a zero-shot predictor.

    &  &  &  &  \\   & Acc. & F1 & Acc. & F1 & Acc. & F1 & Acc. & F1 & Acc & F1 \\  SimPrompt & 85.56 & +1.76 & 36.34 +5.32 & 81.85 +0.38 & 80.23 -0.89 & 85.11 +0.89 & 84.88 +1.50 & 74.53 +6.97 & 74.23 +10.95 \\ Metaprompt & 87.14 & +3.34 & 87.33 +6.31 & 82.12 -0.11 & 80.14 -0.98 & 84.71 +0.49 & 84.62 +1.24 & 76.02 +8.46 & 75.70 +12.42 \\ AttrPrompt w/o CAF & 85.71 & +1.91 & 87.18 +6.16 & 82.24 +0.01 & 80.76 -0.36 & 85.86 +1.64 & 85.65 +2.27 & 75.16 +7.60 & 74.64 +11.36 \\ AttrPrompt & **87.47** & +3.67 & 88.06 +7.04 & 83.95 +17.22 & 83.93 +2.81 & 86.08 +1.86 & 85.98 +2.60 & 76.86 +9.30 & 76.53 +182.5 \\   

Table 8: Performance of the model trained with the original training set/augmented with the generated dataset. We present the performance gain/drop compared to using the original training set in green/red.

Secondly, we examine the sample efficiency of Gold, SimPrompt, and AttrPrompt in Figure 5(c) and 5(d). While both SimPrompt and AttrPrompt exhibit better sample efficiency than Gold in the low-data regime, with superior performance when the dataset size is relatively small, Gold data shows better sample efficiency in the high-data regime. Overall, AttrPrompt renders better sample efficiency than SimPrompt, which suggests that increasing the diversity of the prompts could be an effective way to improve the unsatisfactory data scaling trend of using LLM as data generator .

### The performance with respect to model parameter size

**Effect of the model size for LLM Generators.** To study the effect of different LLMs on AttrPrompt, we use other instruction-finetuned GPT models as the generator, namely text-ada-001, text-babbage-001, text-curie-001, and GPT-4  (due to budget constraints, we only generate a subset with 10% size of the original dataset). Under all settings, our model outperforms the direct baseline SimPrompt by a great margin. Besides, the performance is generally better with larger models, as they often have better instruction-following capabilities. In addition, an interesting finding is that for SimPrompt (but not for AttrPrompt), the average performance of using ChatGPT is worse than text-curie-001. This suggests that straightforward class-dependent prompts might not exploit the capabilities of LLMs as effectively as our proposed approaches.

**Effect of the model size for classifiers.** We experiment with other model choices in addition to the BERT-base-uncased used throughout the paper. They are TinyBERT, DistillBERT, DeBERTa-V3-base, and DeBERTa-V3-Large with parameter size from 14M to 435M5. We visualize the results in Figure 7. Overall, AttrPrompt outperforms SimPrompt by a large margin yet underperforms the Gold across different model choices. With a light-weighted backbone of 66M parameters, AttrPrompt can often outperform the SimPrompt trained with the model containing 435M parameters. This indicates that diversely attributed prompts could help close the performance gap between the Gold and simple class-conditional prompts, and such an improvement is robust to model parameter size.

### Plugging AttrPrompt in existing approaches

In this section, we demonstrate that AttrPrompt can be painlessly integrated with prior zero-shot training data generation techniques. Table 9 shows the results for several recently proposed methods, which design additional techniques based on the noisy-robust loss to further reduce the effect of noisy

Figure 5: The comparisons on budget efficiency and data efficiency on Amazon and NYT datasets.

Figure 6: The barplot of performance with models of different parameter sizes. Note that due to budget limit, for GPT-4 model, the size of the generated dataset is only 10% of the full set thus the result is not directly comparable with other models.

labeled data [15; 31; 55], and leverage in-context examples for data generation . Despite these approaches achieving notable performance gains on simple binary classification tasks, their gains become more marginal for fine-grained classification: the performance gain is less than 2% for all methods on two datasets. Instead, using AttrPrompt lead to consistent performance boosts (more than 5% in all cases) for those approaches, indicating that compared with label noise, _data diversity_ is a more crucial bottleneck for existing dataset generation methods.

More interestingly, AttrPrompt even benefits dataset generation approaches that do not use LLMs. To demonstrate this, we use the LLM-generated contents (subtopics for NYT and product name for Amazon) to enrich the label names used in ReGen , a retrieval-based approach for training data generation. With the expanded label names, AttrPrompt largely improves (14%-26% absolute gain) the performance of ReGen on fine-grained classification tasks. These results justify the advantage of AttrPrompt for serving as a generic plug-in module for existing training data generation approaches.

### Extension to multi-label classification

In this section, we take the first attempt to extend the paradigm of using the LLM as a training data generator to the more challenging multi-label classification setting. In particular, we adopt the arXiv dataset  consisting of 98 fine-grained classes, on which we apply both SimPrompt and AttrPrompt. Following [18; 49], we consider different evaluation metrics including Micro/Macro-F1, Precision@\(k\), nDCG@\(k\), and MRR. The experimental details are in Appendix B. We present the results in Table 10. Similar to our findings for single-label classification, AttrPrompt largely outperforms SimPrompt across all the metrics, which not only strengthens the superiority of AttrPrompt but also opens the door to using LLM as a training data generator for multi-label classification for future work.

## 6 Conclusion

We delve into the realm of training data generation using complex, attributed prompts, which possess the potential to produce a wide range of diverse and attributed generated data. Specifically, we focus on datasets characterized by diverse domains and high cardinality and class-imbalance, and our results demonstrate the superior performance of attributed prompts compared to simple class-conditional prompts. Furthermore, we present a comprehensive empirical study on training data generation that covers essential aspects such as bias, diversity, and efficiency.

  
**Method** & Macro F1 & Micro F1 & Precision@1 & Precision@5 & NDCG@5 & MRR & Price/1k \\  Gold & 27.34 & 58.22 & 73.71 & 27.00 & 79.70 & 82.16 & — \\ SimPrompt & 21.03 & 26.75 & 37.00 & 15.08 & 42.49 & 49.60 & 1.41 \\ AttrPrompt & 27.10 & 37.88 & 49.27 & 18.79 & 54.74 & 61.23 & 1.53 \\   

Table 10: Multi-label classification performance on the arXiv dataset.

    &  &  &  &  &  &  &  &  \\  & w/ & AttrPrompt & & w/ & AttrPrompt & & w/ & AttrPrompt & & w/ & AttrPrompt \\  NYT & Acc. & 76.11 & 82.05 & 75.82 & 81.65 & 77.05 & 80.93 & 70.01 & 82.18 \\  & F1 & 76.80 & 82.62 & 76.52 & 82.70 & 76.70 & 81.32 & 68.14 & 82.50 \\  Amazon & Acc. & 58.17 & 66.76 & 54.30 & 63.89 & 58.40 & 66.43 & 34.70 & 58.40 \\  & F1 & 56.06 & 66.33 & 53.50 & 63.76 & 56.95 & 66.02 & 30.93 & 56.00 \\   

Table 9: Performance comparison when AttrPrompt serves as a plug-in for existing approaches.

Figure 7: The performance curves with models of different parameter sizes.