# Conservative State Value Estimation for Offline Reinforcement Learning

Liting Chen

McGill University

Montreal, Canada

98chenlitting@gmail.com

&Jie Yan +

Microsoft

Beijing, China

dasistyanjie@gmail.com

Zhengdao Shao

University of Sci. and Tech. of China

Hefei, China

zhengdaoshao@mail.ustc.edu.cn

&Lu Wang

Microsoft

Beijing, China

wu@microsoft.com

&Qingwei Lin

Microsoft

Beijing, China

qlin@microsoft.com

&Saravan Rajmohan

Microsoft 365

Seattle, USA

saravar@microsoft.com

&Thomas Moscibroda

Microsoft

Redmond, USA

moscitho@microsoft.com

&Dongmei Zhang

Microsoft

Beijing, China

dongmeiz@microsoft.com

Work done during the internship at Microsoft. \(\) Work done during full-time employment at Microsoft.

###### Abstract

Offline reinforcement learning faces a significant challenge of value over-estimation due to the distributional drift between the dataset and the current learned policy, leading to learning failure in practice. The common approach is to incorporate a penalty term to reward or value estimation in the Bellman iterations. Meanwhile, to avoid extrapolation on out-of-distribution (OOD) states and actions, existing methods focus on conservative Q-function estimation. In this paper, we propose Conservative State Value Estimation (CSVE), a new approach that learns conservative V-function via directly imposing penalty on OOD states. Compared to prior work, CSVE allows more effective state value estimation with conservative guarantees and further better policy optimization. Further, we apply CSVE and develop a practical actor-critic algorithm in which the critic does the conservative value estimation by additionally sampling and penalizing the states _around_ the dataset, and the actor applies advantage weighted updates extended with state exploration to improve the policy. We evaluate in classic continual control tasks of D4RL, showing that our method performs better than the conservative Q-function learning methods and is strongly competitive among recent SOTA methods.

## 1 Introduction

Reinforcement Learning (RL) learns to act by interacting with the environment and has shown great success in various tasks. However, in many real-world situations, it is impossible to learn from scratch online as exploration is often risky and unsafe. Instead, offline RL() avoids this problem by learning the policy solely from historical data. Yet, simply applying standard online RL techniques to static datasets can lead to overestimated values and incorrect policy decisions when faced with unfamiliar or out-of-distribution (OOD) scenarios.

Recently, the principle of conservative value estimation has been introduced to tackle challenges in offline RL[3; 4; 5]. Prior methods, e.g., CQL(Conservative Q-Learning ), avoid the value over-estimation problem by systematically underestimating the Q values of OOD actions on the states in the dataset. In practice, it is often too pessimistic and thus leads to overly conservative algorithms. COMBO  leverages a learned dynamic model to augment data in an interpolation way. This process helps derive a Q function that's less conservative than CQL, potentially leading to more optimal policies.

In this paper, we propose CSVE (Conservative State Value Estimation), a novel offline RL approach. Unlike the above methods that estimate conservative values by penalizing the Q-function for OOD actions, CSVE directly penalizes the V-function for OOD states. We theoretically demonstrate that CSVE provides tighter bounds on in-distribution state values in expectation than CQL, and same bounds as COMBO but under more general discounted state distributions, which potentially enhances policy optimization in the data support. Our main contributions include:

* The conservative state value estimation with related theoretical analysis. We prove that it lower bounds the real state values in expectation over any state distribution that is used to sample OOD states and is up-bounded by the real state values in expectation over the marginal state distribution of the dataset plus a constant term depending on sampling errors. Compared to prior work, it enhances policy optimization with conservative value guarantees.
* A practical actor-critic algorithm implemented CSVE. The critic undertakes conservative state value estimation, while the actor uses advantage-weighted regression(AWR) and explores states with conservative value guarantee to improve policy. In particular, we use a dynamics model to sample OOD states that are directly reachable from the dataset, for efficient value penalizing and policy exploring.
* Experimental evaluation on continuous control tasks of Gym  and Adroit  in D4RL  benchmarks, showing that CSVE performs better than prior methods based on conservative Q-value estimation, and is strongly competitive among main SOTA algorithms.

## 2 Preliminaries

**Offline Reinforcement Learning.** Consider the Markov Decision Process \(M:=(,,P,r,,)\), which comprises the state space \(\), the action space \(\), the transition model \(P:(})\), the reward function \(r:\), the initial state distribution \(\) and the discount factor \((0,1]\). A stochastic policy \(:\) selects an action probabilistically based on the current state. A transition is the tuple \((s_{t},a_{t},r_{t},s_{t+1})\) where \(a_{t}(|s_{t})\), \(s_{t+1} P(|s_{t},a_{t})\), and \(r_{t}=r(s_{t},a_{t})\). It's assumed that the reward values adhere to \(|r(s,a)| R_{max}, s,a\). A trajectory under \(\) is the random sequence \(=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},,s_{T})\) which consists of continuous transitions starting from \(s_{0}\).

Standard RL is to learn a policy \(\) that maximize the expected cumulative future rewards, represented as \(J_{}(M)=_{M,}[_{t=0}^{}^{t}r_{t}]\), through active interaction with the environment \(M\). At any time \(t\), for the policy \(\), the value function of state is defined as \(V^{}(s):=_{M,}[_{s=0}^{}^{t+k}r_{t+k}|s_{t}=s]\), and the Q value function is \(Q^{}(s,a):=_{M,}[_{s=0}^{}^{t+k}r_{t+k}|s_{t}= s,a_{t}=a]\). The Bellman operator is a function projection: \(^{}Q(s,a):=r(s,a)+\,_{s^{} P(|s,a ),a^{}(|s^{})}[Q(s^{},a^{})]\), or \(^{}V(s):=_{a(|s)}[r(s,a)+\,_{s^{} P(|s,a)}[V(s^{})]]\), resulting interactive value updates. Bellman consistency implies that \(V^{}(s)=^{}V^{}(s), s\) and \(Q^{}(s)=^{}Q^{}(s,a), s,a\). When employing function approximation in practice, the empirical Bellman operator \(}^{}\) is used, wherein the aforementioned expectations are estimated with data. Offline RL aims to learn the policy \(\) from a static dataset \(D=\{(s,a,r,s^{})\}\) made up of transitions collected by any behavior policy, with the objective of performing well in the online setting. Note that, unlike standard online RL, offline RL does not interact with the environment during the learning process.

**Conservative Value Estimation.** One main challenge in offline RL arises from the over-estimation of values due to extrapolation in unseen states and actions. Such overestimation can lead to the deterioration of the learned policy. To address this issue, conservatism or pessimism is employed in value estimation. For instance, CQL learns a conservative Q-value function by penalizing the value of unseen actions:

\[^{k+1}*{arg\,min}_{Q}}_{s,a,s^{} D}[(Q(s,a)-_{}^{k}(s,a))^{2 }]+\,(}_{s D\\ a(|s)}[Q(s,a)]-}_{ s D\\ a_{}(|s)}[Q(s,a)]) \]

where \(_{}\) and \(\) are the behaviour policy and learnt policy separately, \(\) is an arbitrary policy different from \(_{}\), and \(\) represents the factor for balancing conservatism.

**Constrained Policy Optimization.** To address the issues of distribution shift between the learning policy and the behavior policy, one approach is to constrain the learning policy close to the behavior policy [10; 11; 12; 13; 1]. As an example, AdvantageWeighted Regression(AWR)[14; 12] employs an implicit KL divergence to regulate the distance between policies:

\[^{k+1}*{arg\,max}_{}}_{ s,a D}[(A^{^{k} }(s,a))]\]

Here, \(A^{^{k}}\) is the advantage of policy \(^{k}\), and \(Z\) serves as the normalization constant for \(s\).

**Model-based Offline RL.** In RL, the model is an approximation of the MDP \(M\). Such a model is denoted as \(:=(,,,,,)\), with \(\) and \(\) being approximation of \(P\) and \(r\) respectively. Within offline RL, the model is commonly used to augment data [15; 6] or act as a surrogate of the real environment during interaction .However, such practices can inadvertently introduce bootstrapped errors over extended horizons. In this paper, we restrict the use of the model to one-step sampling on the next states that are approximately reachable from the dataset.

## 3 Conservative State Value Estimation

In the offline setting, the value overestimation is a major problem resulting in failure of learning a reasonable policy [13; 1]. In contrast to prior works[4; 6] that get conservative value estimation via penalizing Q function for OOD state-action pairs, we directly penalize V function for OOD states. Our approach provides several novel theoretic results that allow better trade-off of conservative value estimation and policy improvement. All proofs of our theorems can be found in Appendix A.

### Conservative Off-policy Evaluation

We aim to conservatively estimate the value of a target policy using a dataset to avoid overestimation of OOD states. To achieve this, we penalize V-values evaluated on states that are more likely to be OOD and increase the V-values on states that are in the distribution of the dataset. This adjustment is made iteratively::

\[^{k+1}*{arg\,min}_{V}}_{s d_{u}(s)}[(^{}}^{k}(s)-V(s))^{2} ]+(}_{s^{} d(s)}V(s^{})- }_{s d_{u}(s)}V(s)) \]

where \(d_{u}(s)\) is the discounted state distribution of D, \(d(s)\) is any state distribution, and \(^{}}\) is the empirical Bellman operator (see appendix for more details). Considering the setting without function approximation, by setting the derivative of Eq. 2 as zero, we can derive the V function using approximate dynamic programming at iteration \(k\)::

\[^{k+1}(s)=^{}}^{k}(s)-[}{d _{u}(s)}-1],\;\; s,k. \]

Denote the function projection on \(^{k}\) in Eq. 3 as \(^{}\). We have Lemma 3.1, which ensures that \(^{k}\) converges to a unique fixed point.

**Lemma 3.1**.: _For any \(d\) with \(*{supp}d*{supp}d_{u}\), \(^{}\) is a \(\)-contraction in \(L_{}\) norm._

**Theorem 3.2**.: _For any \(d\) with \(*{supp}d*{supp}d_{u}\) (\(d d_{u}\)), with a sufficiently large \(\) (i.e., \(}_{s d(s)}}_{a (a|s)}R_{}}{(1-)}/}_{s d(s)}[(s)}-1]\)) ), the expected value of the estimation \(^{}(s)\) under \(d(s)\) is the lower bound of the true value, that is: \(}_{s d(s)}[^{}(s)]}_{s d(s)}[V^{}(s)]\)._\(^{}(s)=_{k}^{k}(s)\) is the converged value estimation with the dataset \(D\), and \(R_{max}}{(1-)}\) is related to sampling error that arises when using the empirical operator instead of the Bellman operator. If the counts of each state-action pair is greater than zero, \(|D(s,a)|\) denotes a vector of size \(||||\) containing counts for each state-action pair. If the counts of this state action pair is zero, the corresponding \(}\) is a large yet finite value. We assume that with probability \( 1-\), the sampling error is less than \(R_{max}}{(1-)}\), while \(C_{r,t,}\) is a constant (See appendix for more details.)

Note that if the sampling error can be disregarded, \(>0\) can ensure the lower bound results.

**Theorem 3.3**.: _The expected value of the estimation, \(^{}(s)\), under the state distribution of the original dataset is the lower bound of the true value plus the term of irreducible sampling error. Formally:_

\[_{s d_{u}(s)}[^{}(s)]_{s d_{u}(s)}[V ^{}(s)]+_{s d_{u}(s)}(I- P^{})^{-1}\,_{a (a|s)}R_{max}}{(1-)}.\]

where \(P^{}\) refers to the transition matrix coupled with policy \(\) (see Appendix for details).

Now we show that, during iterations, the gap between the estimated V-function values of in-distribution states and OOD states is higher compared to the true V-functions.

**Theorem 3.4**.: _For any iteration \(k\), given a sufficiently large \(\), our method amplifies the difference in expected V-values between the selected state distribution and the dataset state distribution. This can be represented as: \(_{s d_{u}(s)}[^{k}(s)]-_{s d(s)}[^{k }(s)]>_{s d_{u}(s)}[V^{k}(s)]-_{s d(s)}[V^{k}(s)]\)._

Our approach, which penalizes the V-function for OOD states, promotes a more conservative estimate of a target policy's value in offline reinforcement learning. Consequently, our policy extraction ensures actions align with the dataset's distribution.

To apply our approach effectively in offline RL algorithms, the preceding theorems serve as guiding principles. Here are four key insights for practical use of Eq. 2:

**Remark 1**.: According to Eq. 2, if \(d=d_{u}\), the penalty for OOD states diminishes. This means that the policy will likely avoid states with limited data support, preventing it from exploring unseen actions in such states. While AWAC employs this configuration, our findings indicate that by selecting a \(d\), our method surpasses AWAC's performance.

**Remark 2**.: Theorem 3.3 suggests that under \(d_{u}\), the marginal state distribution of data, the expectation estimated value of \(V^{}\) is either lower than its true value or exceed it, but within a certain limit. This understanding drives our adoption of the advantage-weighted policy update, as illustrated in Eq. 9.

**Remark 3**.: As per Theorem 3.2, the expected estimated value of a policy under \(d\), which represents the discounted state distribution of any policy, must be a lower bound of its true value. Grounded in this theorem, our policy enhancement strategy merges an advantage-weighted update with an additional exploration bonus, showcased in Eq. 10.

**Remark 4**.: Theorem 3.4 states \(_{s d(s)}[V^{k}(s)]-_{s d(s)}[^{k}(s)]> _{s d_{u}(s)}[V^{k}(s)]-_{s d_{u}(s)}[^{k}( s)]\). In simpler terms, the underestimation of value is more pronounced under \(d\). With the proper choice of \(d\), we can confidently formulate a newer and potentially superior policy using \(^{k}\). Our algorithm chooses the distribution of model predictive next-states as \(d\), i.e., \(s^{} d\) is implemented by \(s D,a(|s),s^{}(|s,a)\), which effectively builds a soft 'river' with low values encircling the dataset.

**Comparison with prior work:** CQL (Eq.1), which penalizes Q-function of OOD actions, guarantees the lower bounds on state-wise value estimation: \(^{}(s)=E_{(a|s)}(^{}(s,a)) E_{(a|s)}(Q^{}(s,a ))=V^{}(s)\) for all \(s D\). COMBO, which penalizes the Q-function for OOD states and actions of interpolation of history data and model-based roll-outs, guarantees the lower bound of state value expectation: \(_{s_{0}}[^{}(s)]_{s_{0}}[V^{ }(s)]\) where \(_{0}\) is the initial state distribution (Remark 1, section A.2 of COMBO ); which is a special case of our result in Theorem 3.2 when \(d=_{0}\). Both CSVE and COMBO intend to enhance performance by transitioning from individual state values to expected state values. However, CSVE offers the same lower bounds but under a more general state distribution. Note that \(_{0}\) depends on the environment or the dynamic model during offline training. CSVE's flexibility, represented by \(d\), ensures conservative guarantees across any discounted state distribution of the learned policy, emphasizing a preference for penalizing \(V\) over the Q-function.

### Safe Policy Improvement Guarantees

Now we show that our method has the safe policy improvement guarantees against the data-implied behaviour policy. We first show that our method optimizes a penalized RL empirical objective:

**Theorem 3.5**.: _Let \(^{}\) be the fixed point of Eq. 3, then \(^{*}(a|s)=_{}^{}(s)\) is equivalently obtained by solving:_

\[^{*}*{arg\,max}_{}J(,)-*{}_{s d_{M}^{}}[(s)}-1]. \]

Building upon Theorem 3.5, we show that our method provides a \(\)-safe policy improvement over \(_{}\).

**Theorem 3.6**.: _Let \(^{*}(a|s)\) be the policy obtained in Eq. 4. Then, it is a \(\)-safe policy improvement over \(^{}\) in the actual MDP M, i.e., \(J(^{*},M) J(^{},M)-\) with high probability 1- \(\), where \(\) is given by:_

\[= 2(}{1-}+C_{T,} }{(1-)^{2}})*{}_{s d_{M}^{}(s)}[c }_{a(a|s)}[(a |s)}]}]\] \[-,)-J(_{},))}{ *{}_{s d_{M}^{}(|d _{u}^{()}-1]}}c=|}/(s)|}.\]

## 4 Methodology

In this section, we propose a practical actor-critic algorithm that employs CSVE for value estimation and extends Advantage Weighted Regression with out-of-sample state exploration for policy improvement. In particular, we adopt a dynamics model to sample OOD states during conservative value estimation and exploration during policy improvement. The implementation details are in Appendix B. Besides, we discuss the general technical choices of applying CSVE into algorithms.

### Conservative Value Estimation

Given a dataset \(D\) acquired by the behavior policy \(_{}\), our objective is to estimate the value function \(V^{}\) for a target policy \(\). As stated in section 3, to prevent the value overestimation, we learn a conservative value function \(^{}\) that lower bounds the real values of \(\) by adding a penalty for OOD states within the Bellman projection sequence. Our method involves iterative updates of Equations 5 - 7, where \(^{k}}\) is the target network of \(^{k}\).

\[^{k+1} *{arg\,min}_{V}L_{V}^{}(V;^{k}}) \] \[=*{}_{s D}[(*{ }_{a(|s)}[^{k}}(s,a)]-V(s))^{2}]+ (*{}_{s D,a( |s)\\ s^{}(s,a)}[V(s^{})]-*{ }_{s D}[V(s)])\] \[^{k+1}*{arg\,min}_{Q}L_{Q}^{}(Q ;^{k+1})= *{}_{s,a,s^{} D}[(r(s,a)+ ^{k+1}(s^{})-Q(s,a))^{2}]\] (6) \[^{k+1}} (1-)^{k}}+^{k+1} \]

The RHS of Eq. 5 is an approximation of Eq. 2, with the first term representing the standard TD error. In this term, the target state value is estimated by taking the expectation of \(^{k}}\) over \(a\), and the second term penalizes the value of OOD states. In Eq. 6, the RHS is TD errors estimated on transitions in the dataset \(D\). Note that the target term is the sum of the reward \(r(s,a)\) and the next step state's value \(^{k+1}(s^{})\). In Eq. 7, the target Q values are updated with a soft interpolation factor \((0,1)\). \(^{k}}\) changes slower than \(^{k}\), which makes the TD error estimation in Eq. 5 more stable.

**Constrained Policy.** Note that in RHS of Eq. 5, we use \(a(|s)\) in expectation. To safely estimate the target value of \(V(s)\) by \(*{}_{a(|s)}[}(s,a)]\), we almost always requires \(*{supp}((|s))*{supp}((|s))\). In Eq. 7, we use \(a(|s)\) in expectation. In the next step, we use \(a(|s)\) in expectation.

\((_{}(|s))\). We achieve this by the _advantage weighted policy update_, which forces \((|s)\) to have significant probability mass on actions taken by \(_{}\) in data, as detailed in section 3.2.

**Model-based OOD State Sampling.** In Eq. 5, we implement the state sampling process \(s^{} d\) in Eq. 2 as a flow of \(\{s D;a(a|s),s^{}(s^{}|s,a)\}\), that is the distribution of the predictive next-states from \(D\) by following \(\). This approach proves beneficial in practice. On the one hand, this method is more efficient as it samples only the states that are approximately reachable from \(D\) by one step, rather than sampling the entire state space. On the other hand, we only need the model to do one-step prediction such that it introduces no bootstrapped errors from long horizons. Following previous work , We use an ensemble of deep neural networks, represented as \(p^{1},,p^{B}\), to implement the probabilistic dynamics model. Each neural network produces a Gaussian distribution over the next state and reward: \(P^{i}_{}(s_{t+1},r|s_{t},a_{t})=(u^{i}_{}(s_{t},a_{t}),^{i}_{}(s_{t},a_{t}))\).

**Adaptive Penalty Factor \(\).** The pessimism level is controlled by the parameter \( 0\). In practice, we set \(\) adaptive during training as follows, which is similar to that in CQL()

\[_{ 0}[(_{s^{} d}[V_{}(s^{})]- _{s D}[V_{}(s)]-)], \]

where \(\) is a budget parameter. If the expected difference in V-values is less than \(\), \(\) will decrease. Otherwise, \(\) will increase, penalizing the OOD state values more aggressively.

### Advantage Weighted Policy Update

After learning the conservative \(^{k+1}\) and \(^{k+1}\) (or \(^{}\) and \(^{}\) when the values have converged), we improve the policy by the following advantage weighted update .

\[*{arg\,min}_{^{}}L_{}(^{})=- *{}_{s,a D}[^{}(a|s)( ^{k+1}(s,a))] \]

where \(^{k+1}(s,a)=^{k+1}(s,a)-^{k+1}(s)\). Eq.9 updates the policy \(\) by applying a weighted maximum likelihood method. This is computed by re-weighting state-action samples in \(D\) using the estimated advantage \(^{k+1}\). It avoids explicit estimation of the behavior policy, and its resulting sampling errors, which is an important issue in offline RL .

**Implicit policy constraints.** We adopt the advantage-weighted policy update which imposes an implicit KL divergence constraint between \(\) and \(_{}\). This policy constraint is necessary to guarantee that the next state \(s^{}\) in Eq. 5 can be safely generated through policy \(\). As derived in  (Appendix A), Eq. 9 is a parametric solution of the following problem (where \(\) depends on \(\)):

\[_{^{}}_{a^{}(|s)}[^{k+1}(s,a)]\] \[s.t.\ \ _{}(^{}(|s)\ ||\ _{}( |s)),_{a}^{}(a|s)da=1.\]

Note that \(_{}(^{}\ ||\ _{})\) is a reserved KL divergence with respect to \(^{}\), which is mode-seeking . When treated as Lagrangian it forces \(^{}\) to allocate its probability mass to the maximum likelihood supports of \(_{}\), re-weighted by the estimated advantage. In other words, for the space of \(A\) where \(_{}(|s)\) has no samples, \(^{}(|s)\) has almost zero probability mass too.

**Model-based Exploration on Near States.** As suggested by remarks in Section 3.1, in practice, allowing the policy to explore the predicted next states transition \((s D)\) following \(a^{}(|s))\) leads to better test performance. With this kind of exploration, the policy is updated as follows:

\[*{arg\,min}_{^{}}L_{}(^{})- *{}_{s D,a^{}(s)}[r(s,a )+^{k+1}(s^{})]. \]

The second term is an approximation to \(E_{s d_{}(s)}[V^{}(s)]\). The optimization of this term involves calculating the gradient through the learned dynamics model. This is achieved by employing analytic gradients through the learned dynamics to maximize the value estimates. It is important to note that the value estimates rely on the reward and value predictions, which are dependent on the imagined states and actions. As all these steps are implemented using neural networks, the gradient is analytically computed using stochastic back-propagation, a concept inspired by Dreamer. We adjust the value of \(\), a hyper-parameter, to balance between optimistic policy optimization (in maximizing V) and the constrained policy update (as indicated by the first term).

### Discussion on implementation choices

Now we examine the technical considerations for implementing CSVE in a practical algorithm.

**Constraints on Policy Extraction.** It is important to note that the state value function alone does not suffice to directly derive a policy. There are two methods for extracting a policy from CSVE. The first method is model-based planning, i.e., \(_{}_{s d,a(|s)}(s,a)+\,_{s^{} P(s,a)}[V(s^{})]\), which involves finding the policy that maximizes the expected future return. However, this method heavily depends on the accuracy of a model and is difficult to implement in practice. As an alternative, we suggest the second method, which learns a Q value or advantage function from the V value function and experience data, and then extracts the policy. Note that CSVE provides no guarantees for conservative estimation on OOD actions, which can cause normal policy extraction methods such as SAC to fail. To address this issue, we adopt policy constraint techniques. On the one hand, during the value estimation in Eq.5, all current states are sampled from the dataset, while the policy is constrained to be close to the behavior policy (ensured via Eq.9). On the other hand, during the policy learning in Eq.10, we use AWR  as the primary policy extraction method (first term of Eq.10), which implicitly imposes policy constraints and the additional action exploration (second term of Eq.10) is strictly applied to states in the dataset. This exploration provides a bonus to actions that: (1) themselves and their model-predictive next-states are both close to the dataset (ensured by the dynamics model), and (2) their values are favorable even with conservatism.

**Taking Advantage of CSVE.** As outlined in Section 3.1, CSVE allows for a more relaxed lower bound on conservative value estimation compared to conservative Q values, providing greater potential for improving the policy. To take advantage of this, the algorithm should enable exploration of out-of-sample but in-distribution states, as described in Section 3. In this paper, we use a deep ensemble dynamics model to support this speculative state exploration, as shown in Eq. 10. The reasoning behind this is as follows: for an in-data state \(s\) and any action \(a(|s)\), if the next state \(s^{}\) is in-data or close to the data support, its value is reasonably estimated, and if not, its value has been penalized according to Eq.5. Additionally, the deep ensemble dynamics model captures epistemic uncertainty well, which can effectively cancel out the impact of rare samples of \(s^{}\). By utilizing CSVE, our algorithm can employ the speculative interpolation to further improve the policy. In contrast, CQL and AWAC do not have this capability for such enhanced policy optimization.

    & & AWAC & CQL & CQL-AWR & COMBO & IQL & TD3-BC & PBRL & CSVE \\    } & HalfCheetah & 13.7 & \(17.5 1.5\) & \(16.9 1.5\) & \(\) & 18.2 & \(11.0 1.1\) & \(13.1 1.2\) & \(26.8 1.5\) \\  & Hopper & 8.7 & \(7.9 0.4\) & \(8.7 0.5\) & 17.9 & 16.3 & \(8.5 0.6\) & \(31.6 0.3\) & \(26.1 7.6\) \\  & Walker2D & 2.2 & \(5.1 1.3\) & \(0.0 1.6\) & 7.0 & 5.5 & \(1.6 1.7\) & \(8.8 6.3\) & \(6.2 0.8\) \\    } & HalfCheetah & 50.0 & \(47.0 0.5\) & \(50.9 0.6\) & 54.2 & 47.4 & \(48.3 0.3\) & \(58.2 1.5\) & \(48.4 0.3\) \\  & Hopper & **97.5** & \(53.0 28.5\) & \(25.7 37.4\) & **94.9** & 66.3 & \(59.3 4.2\) & \(81.6 1.45\) & \(96.7 5.7\) \\  & Walker2D & **89.1** & \(73.3 17.7\) & \(62.4 24.4\) & 75.5 & 78.3 & \(83.7 2.1\) & \(90.3 1.2\) & \(83.2 1.0\) \\    } & HalfCheetah & 44.9 & \(45.5 0.7\) & \(40.0 0.4\) & **55.1** & 44.2 & \(44.6 0.5\) & \(49.5 0.8\) & \(84.5 0.6\) \\  & Hopper & **99.4** & \(88.7 12.9\) & \(91.0 13.0\) & 73.1 & 94.7 & \(60.9 1.8\) & \(100.7 0.4\) & \(91.7 0.2\) \\  & Walker2D & 80.0 & \(83.3 2.7\) & \(66.7 12.1\) & 56.0 & 73.9 & \(81.8 5.5\) & \(86.2 3.4\) & \(78.0 1.5\) \\    } & HalfCheetah & 62.8 & \(75.6 25.7\) & \(73.4 2.0\) & 90.0 & 86.7 & \(90.7 4.3\) & \(93.1 0.2\) & \(93.1 0.3\) \\  & Hopper & 87.2 & \(105.6 12.9\) & \(102.2 7.7\) & 111.1 & 91.5 & \(98.0 9.4\) & \(111.2 0.7\) & \(94.1 3.0\) \\  & Walker2D & **109.8** & \(107.9 16.6\) & \(98.0 21.7\) & 96.1 & \(109.6 10.5\) & \(109.8 0.2\) & \(109.0 0.1\) \\    } & HalfCheetah & 20.0 & \(96.3 1.3\) & \(87.3 8.1\) & - & 94.6 & \(96.7 1.1\) & \(96.2 2.3\) & \(93.8 0.1\) \\  & Hopper & 111.6 & \(96.5 28.0\) & \(\) & - & 109.0 & \(107.8 7\) & \(110.4 0.3\) & \(111.3 0.6\) \\   & Walker2D & **110.6** & \(108.5 0.5\) & \(75.1 60.7\) & - & 109.4 & \(110.2 0.3\) & \(108.8 0.2\) & \(108.5 0.1\) \\   & 65.8 & 67.4 & 60.6 & 64.1 & 69.7 & 67.5 & 76.7 & 74.8 \\   

Table 1: Performance comparison on Gym control tasks v2. The results of CSVE are over ten seeds and we reimplement AWAC using d3rpy. Results of IQL, TD3-BC, and PBRL are from their original papers ( Table 1 in , Table C.3 in , and Table 1 in  respectively). Results of COMBO and CQL are from the reproduction results in  (Table 1) and  respectively, since their original results were reported on v0 datasets.

## 5 Experiments

This section evaluates the effectiveness of our proposed CSVE algorithm for conservative value estimation in offline RL. In addition, we aim to compare the performance of CSVE with state-of-the-art (SOTA) algorithms. To achieve this, we conduct experimental evaluations on a variety of classic continuous control tasks of Gym and Adroit in the D4RL benchmark.

Our compared baselines include: (1) CQL and its variants, CQL-AWR (Appendix D.2) which uses AWR with extra in-sample exploration as policy extractor, COMBO which extends CQL with model-based rollouts; (2) AWR variants, including AWAC which is a special case of our algorithm with no value penalization (i.e., \(d=d_{u}\) in Eq. 2) and exploration on OOD states, IQL which adopts expectile-based conservative value estimation; (3) PBRL, a strong algorithm in offline RL, but is quite costly on computation since it uses the ensemble of hundreds of sub-models; (4) other SOTA algorithms with public performance results or high-quality open source implementations, including TD3-BC, UWAC and BEAR). Comparing with CQL variants allows us to investigate the advantages of conservative estimation on state values over Q values. By comparing with AWR variants, we distinguish the performance contribution of CSVE from the AWR policy extraction used in our implementation.

### Overall Performance

**Evaluation on the Gym Control Tasks.** Our method, CSVE, was trained for 1 million steps and evaluated. The results are shown in Table 1.Compared to CQL, CSVE outperforms it in 11 out of 15 tasks, with similar performance on the remaining tasks. Additionally, CSVE shows a consistent advantage on datasets that were generated by following random or sub-optimal policies (random and medium). The CQL-AWR method showed slight improvement in some cases, but still underperforms compared to CSVE. When compared to COMBO, CSVE performs better in 7 out of 12 tasks and similarly or slightly worse on the remaining tasks, which highlights the effectiveness of our method's better bounds on V. Our method has a clear advantage in extracting the best policy on medium and medium-expert tasks. Overall, our results provide empirical evidence that using conservative value estimation on states, rather than Q, leads to improved performance in offline RL. CSVE outperforms AWAC in 9 out of 15 tasks, demonstrating the effectiveness of our approach in exploring beyond the behavior policy. Additionally, our method excels in extracting the optimal policy on data with mixed policies (medium-expert) where AWAC falls short. In comparison to IQL, our method achieves higher scores in 7 out of 9 tasks and maintains comparable performance in the remaining tasks. Furthermore, despite having a significantly lower model capacity and computation cost, CSVE outperforms TD3-BC and is on par with PBRL. These results highlight the effectiveness of our conservative value estimation approach.

**Evaluation on the Adroit Tasks.** In Table 2, we report the final evaluation results after training 0.1 million steps. As shown, our method outperforms IQL in 8 out of 12 tasks, and is competitive with other algorithms on expert datasets. Additionally, we note that CSVE is the only method that

    & & AWAC & BC & BEAR & UWAC & CQL & CQL-AWR & IQL & PBRL & CSVE \\    } & Pen & 18.7 & 34.4 & -1.0 & \(10.1 3.2\) & 37.5 & \(8.4 7.1\) & \(71.5\) & \(35.4 3.3\) & \(\) \\  & Hammer & -1.8 & 1.5 & \(0.3\) & \(1.2 0.7\) & \(\) & \(0.3 0.0\) & \(1.4\) & \(0.4 0.3\) & \(3.5 2.6\) \\  & Door & -1.8 & 0.5 & \(-0.3\) & \(0.4 0.2\) & \(\) & \(3.5 1.8\) & \(4.3\) & \(0.1 0.0\) & \(2.8 2.4\) \\  & Relocate & -0.1 & 0.0 & -0.3 & \(0.0 0.0\) & \(\) & \(0.1 0.0\) & \(0.1 0.0\) & \(0.1 0.0\) \\    } & Pen & 27.2 & 56.9 & 26.5 & \(23.0 6.9\) & 39.2 & \(29.3 7.1\) & 37.3 & \(\) & \(54.5 5.4\) \\  & Hammer & -1.8 & 0.8 & 0.3 & \(0.4 0.0\) & \(\) & \(0.31 0.06\) & \(\) & \(0.8 0.5\) & \(0.5 0.2\) \\  & Door & -2.1 & -0.1 & \(-0.1\) & \(0.0 0.0\) & \(0.4\) & \(-0.2 0.1\) & \(1.6\) & \(\) & \(1.2 1.0\) \\  & Relocate & -0.4 & -0.1 & -0.3 & \(-0.3 0.2\) & -0.1 & \(-0.3 0.0\) & \(0.0\) & \(-0.1 0.0\) & \(-0.3 0.0\) \\    } & Pen & 60.9 & 85.1 & 105.9 & \(98.2 9.1\) & 107.0 & \(47.1 6.8\) & 117.2 & \(135.7 3.4\) & \(\) \\  & Hammer & 31.0 & \(\) & \(\) & \(107.7 21.7\) & 86.7 & \(0.2 0.0\) & \(\) & \(\) & \(126.5 0.3\) \\  & Door & 98.1 & 34.9 & \(\) & \(\) & \(\) & \(85.0 15.9\) & \(105.2\) & \(95.7 12.2\) & \(104.2 0.8\) \\  & Relocate & 49.0 & \(\) & \(\) & \(105.5 3.2\) & \(95.0\) & \(7.2 12.5\) & \(105.9\) & \(84.5 12.2\) & \(\) \\  Average & & 23.1 & 36.7 & 38.4 & 37.6 & 40.3 & 15.1 & 47.6 & 46.6 & 53.8 \\   

Table 2: Performance comparison on Adroit tasks. The results of CSVE are over ten seeds. Results of IQL are from Table 3 in  and results of other algorithms are from Table 4 in .

can learn an effective policy on the human dataset for the Pen task, while maintaining medium performance on the cloned dataset. Overall, our results empirically support the effectiveness of our proposed tighter conservative value estimation in improving offline RL performance.

### Ablation Study

**Effect of Exploration on Near States.** We analyze the impact of varying the factor \(\) in Eq. 10, which controls the intensity on such exploration. We investigated \(\) values of \(\{0.0,0.1,0.5,1.0\}\) in the medium tasks, fixing \(=0.1\). The results are plotted in Fig. 1. As shown in the upper figures, \(\) has an obvious effect on policy performance and variances during training. With increasing \(\) from 0, the converged performance gets better in general. However, when the value of \(\) becomes too large (e.g., \(=3\) for hopper and walker2d), the performance may degrade or even collapse. We further investigated the \(L_{}\) loss as depicted in the bottom figures of Eq. 9, finding that larger \(\) values negatively impact \(L_{}\); however, once \(L_{}\) converges to a reasonable low value, larger \(\) values lead to performance improvement.

**Effect of In-sample Policy Optimization.** We examined the impact of varying the factor \(\) in Eq. 9 on the balance between behavior cloning and in-sample policy optimization. We tested different \(\) values on mujoco medium datasets, as shown in Fig.2. The results indicate that \(\) has a significant effect on the policy performance during training. Based on our findings, a value of \(=3.0\) was found to be suitable for medium datasets. Additionally, in our implementation, we use \(=3.0\) for random and medium tasks, and \(=0.1\) for medium-replay, medium-expert, and expert datasets. More details can be found in the ablation study in the appendix.

## 6 Related work

The main idea behind offline RL algorithms is to incorporate conservatism or regularization into the online RL algorithms. Here, we briefly review prior work and compare it to our approach.

**Conservative Value Estimation:** Prior offline RL algorithms regularize the learning policy to be close to the data or to an explicitly estimated behavior policy. and penalize the exploration

Figure 1: Effect of \(\) to performance scores (upper figures) and \(L_{}\) losses (bottom figures) in Eq. 9 on medium tasks.

ofthe OOD region, via distribution correction estimation [26; 27], policy constraints with support matching  and distributional matching [1; 25], applying policy divergence based penalty on Q-functions [28; 29] or uncertainty-based penalty  on Q-functions and conservative Q-function estimation . Besides, model-based algorithms  directly estimate dynamics uncertainty and translate it into reward penalty. Different from this prior work that imposes conservatism on state-action pairs or actions, ours directly does such conservative estimation on states and requires no explicit uncertainty quantification.

**In-Sample Algorithms:** AWR  updates policy constrained on strictly in-sample states and actions, to avoid extrapolation on out-of-support points. IQL uses expectile-based regression to do value estimation and AWR for its policy updates. AWAC, whose actor is AWR, is an actor-critic algorithm to accelerate online RL with offline data. The major drawback of AWR method when used for offline RL is that the in-sample policy learning limits the final performance.

**Model-Based Algorithms:** Model-based offline RL learns the dynamics model from the static dataset and uses it to quantify uncertainty , data augmentation  with roll-outs, or planning [16; 31]. Such methods typically rely on wide data coverage when planning and data augmentation with roll-outs, and low model estimation error when estimating uncertainty, which is difficult to satisfy in reality and leads to policy instability. Instead, we use the model to sample the next-step states only reachable from data, which has no such strict requirements on data coverage or model bias.

**Theoretical Results:** Our theoretical results are derived from conservative Q-value estimation (CQL) and safe policy improvement . Compared to offline policy evaluation, which aims to provide a better estimation of the value function, we focus on providing a better lower bound. Additionally, hen the dataset is augmented with model-based roll-outs, COMBO  provides a more conservative yet tighter value estimation than CQL. CSVE achieves the same lower bounds as COMBO but under more general state distributions.

## 7 Conclusions

In this paper, we propose CSVE, a new approach for offline RL based on conservative value estimation on states. We demonstrated how its theoretical results can lead to more effective algorithms. In particular, we develop a practical actor-critic algorithm, in which the critic achieves conservative state value estimation by incorporating the penalty of the model predictive next-states into Bellman iterations, and the actor does the advantage-weighted policy updates enhanced via model-based state exploration. Experimental evaluation shows that our method performs better than alternative methods based on conservative Q-function estimation and is competitive among the SOTA methods, thereby validating our theoretical analysis. Moving forward, we aim to delve deeper into designing more powerful algorithms grounded in conservative state value estimation.

Figure 2: Effect of \(\) to performance scores on medium tasks.