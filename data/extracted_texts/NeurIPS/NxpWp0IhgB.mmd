# LeanFlex-GKP: Advancing Hassle-Free Structured Pruning with Simple Flexible Group Count

Jiamu Zhang

Department of Computer and Data Sciences, Case Western Reserve University

Shaochen (Henry) Zhong

Equal contribution, order determined by a coin flip. Department of Computer and Data Sciences, Case Western Reserve University

Azirui Liu

Department of Computer Science, Rice University

Kaixiong Zhou

Institute for Medical Engineering & Science, Massachusetts Institute of Technology

Xia Hu

Department of Computer Science, Case Western Reserve University

Shuai Xu

Department of Computer and Data Sciences, Case Western Reserve University

###### Abstract

Densely structured pruning methods -- which generate pruned models in a fully dense format, allowing immediate compression benefits without additional demands -- are evolving due to their practical significance. Traditional techniques in this domain mainly revolve around coarser granularities, such as filter pruning, thereby limiting performance due to restricted pruning freedom. Recent advancements in _Grouped Kernel Pruning (GKP)_ have enabled the utilization of finer granularities while maintaining a densely structured format. We observe that existing GKP methods often introduce dynamic operations to different aspects of their procedures at the cost of adding complications and/or imposing limitations (e.g., requiring an expensive mixture of clustering schemes) or contain dynamic pruning rates and sizes among groups that result in a reliance on custom architecture support for its pruned models. In this work, we argue that the best practice to introduce these dynamic operations to GKP is to make Conv2d(groups) (a.k.a. group count) flexible under an integral optimization, leveraging its ideal alignment with the infrastructure support of _Grouped Convolution_. Pursuing such a direction, we present a one-shot, post-train, data-agnostic GKP method that is more performant, adaptive, and efficient than its predecessors while simultaneously being a lot more user-friendly, with little-to-no hyper-parameter tuning or handcrafting of criteria required.

## 1 Introduction

Despite having a proven track record revolving around computer vision tasks, modern convolutional neural networks (CNNs) face deployment challenges for growing model capacities. To address this issue of over-parameterization, _network pruning_ -- a field studying how to insightfully remove components from the original model without significant degradation to its properties and performance -- has undergone constant development for being an intuitive way of potentially reducing the computation and memory footprint required to practically utilize a model (Blalock et al., 2020).

In this work, we advance the progress on _Grouped Kernel Pruning (GKP)_(Zhong et al., 2022), a recently developed structured pruning granularity with many deployment-friendly properties, byinvestigating a common design choice among existing GKP methods: **dynamic operations** -- which is an act of applying different operations to the same task (e.g., clustering CNN filters with various combinations of dimensionality reduction and clustering techniques, as in TMI-GKP (Zhong et al., 2022)). We find that current GKP designs tend to include such operations in a sub-optimal manner, resulting in various complications and limitations. As a solution, we propose that **the best practice to implement dynamic operations to GKP is to make** Conv2d(groups) **(a.k.a. group count) flexible** under an integral optimization, leveraging its ideal alignment with the existing and future infrastructure support of _Grouped Convolution_(Krizhevsky et al., 2012). Our empirical evaluation showcases that by making these group counts flexible, we can afford to "lean down" on the rest of the typical GKP procedures, and therefore obtain a new one-shot, post-train, data-agnostic GKP method that is more performant, adaptive, and efficient than its predecessors while simultaneously being a lot more user-friendly with little-to-no hyper-parameter tuning or handcrafted criteria required. We can concisely summarize our contribution as _"advancing hassle-free structured pruning,"_ as suggested in the title.

Given that our work develops upon specific observations made on existing adaptations of **grouped kernel pruning (Zhong et al., 2022), a recently proposed structured pruning granularity with limited exposure, we hereby provide a rather extensive background on the procedure at the risk of being redundant**. Additionally, we refer readers to Zhong et al. (2022) and He & Xiao (2023) for more information regarding different structured pruning granularities.

### Trading performance for deployability: the practical advantage of structured pruning

Under the general realm of network pruning, two categories of techniques have been proposed, which are commonly known as _unstructured pruning_ and _structured pruning_(Mao et al., 2017; Blalock et al., 2020; He & Xiao, 2023). While it can be faithfully concluded that these two categories have very different focuses and approaches, there is, unfortunately, no universally agreed distinction between what pruning methods constitute structured pruning and what do not.

Nonetheless, the general understanding follows a performance-deployability trade-off: an unstructured pruning method typically tends to enjoy a higher degree of pruning freedom -- and thus better performance -- but it is done so at the cost of leaving the pruned network to be sparse without a reduction in size and consequently require special libraries or hardware support to realize compression/acceleration benefits (Yang et al., 2018) (e.g., weight pruning (LeCun et al., 1989)). Conversely, a structured pruning method often removes model components in groups that follow the architecture design of the original network, potentially resulting in a smaller network. Specifically, the majority of structured pruning methods (e.g., filter pruning (Zhou et al., 2016; Li et al., 2017)) are capable of delivering pruned models that are reduced in dimension yet entirely dense (a.k.a. _densely structured_) and therefore provide immediate compression benefits without additional demand.

### Exploring structured pruning with finer granularities: grouped kernel pruning (GKP)

To narrow the performance gap between unstructured and structured pruning methods, many structured pruning works have been exploring finer pruning granularities, which are often regarded as _intra-channel pruning_ due to the two most prevalent structured pruning approaches -- channel pruning and filter pruning -- and essentially drive their pruning operations upon the in and out channels of the original CNN model.

However, one major issue with these intra-channel explorations is that their pruned models are no longer dense and therefore lose the benefits of staying densely structured, such as increasing network efficiency without additional environment or hardware support (Yang et al., 2018). This is evident

Figure 1: Different Structured Pruning Granularities

in Figure 1: it can be seen that if we naively seek out a finer pruning granularity than filter/channel pruning, we'd naturally have kernel pruning, which is intrinsically sparse. This is also the case for all _intra-kernel_ pruning methods (e.g., stride pruning (Anwar et al., 2017), N:M sparsity (Zhou et al., 2021)), in which kernel-level sparsity is introduced. These methods might be "structured" by definition -- they indeed "remove model components in groups" -- but they often cannot provide efficiency benefits without external support due to the sparsity introduced to pruned models.

In order to achieve both increased pruning freedom and remaining densely structured, a special type of intra-channel pruning granularity called _Grouped Kernel Pruning (GKP)_(Zhong et al., 2022) has been proposed1, where a finer pruning granularity than filter/channel pruning was achieved without introducing sparsity by leveraging the format of grouped convolution (Krizhevsky, 2014), as illustrated in Figure 2. To the best of our knowledge, GKP provides the highest degree of pruning freedom under the context of being densely structured and thus attracts the interests of the pruning community (Zhong et al., 2022; Zhang et al., 2022; Park et al., 2023; He & Xiao, 2023).

### A common recipe for GKP-based methods: dynamic operations

Although GKP is still a fairly under-developed pruning granularity given its recency, we have observed a consistent pattern among the few existing successful works in GKP (e.g., TMI-GKP (Zhong et al., 2022) and DSP (Park et al., 2023)). Both methods introduce _dynamic operations_ to different stages of its procedure and achieve significant performance improvement than GKP methods with only deterministic operations.

As shown in Figure 3: TMI-GKP opts to include dynamic choices of clustering schemes in each of its convolutional layers. Similarly, in Figure 4, DSP makes its filter grouping and group kernel pruning stages dynamic in the sense that they may enjoy different group sizes and different in-group pruning rates for components within the same layer. While both methods deliver impressive performance, we notice that their adoption of dynamic operations results in various complications and limitations. For instance, several clustering schemes trialed in TMI-GKP can be very expensive to run. Yet, many of the produced clustering results are eventually discarded according to their tickets magnitude increase (TMI) scores. On the other hand, DSP essentially prunes grouped kernels in different sizes, where the

Figure 3: Procedure of TMI-GKP

Figure 2: General Procedure of Grouped Kernel Pruning

resultant pruned network is irregularly shaped (i.e., having different dimensions of tensors within the same layer) and therefore relies on custom model definitions and convolutional operators to undergo training and inference -- more on this in Section 2.1.

To mitigate the complications and limitations caused by dynamic operations in existing GKP methods, we propose a new method to include the dynamic operation within Conv2d(groups) (a.k.a. "group count" or "number of groups" as of grouped convolution). This means we allow each convolutional layer to take a flexible number of groups when grouping filters. **We argue this is the best area to integrate dynamic operations into a GKP procedure**, as this setup is directly supported by the well-adopted grouped convolution operator in modern ML frameworks, and is therefore able to make use of existing and future infrastructure updates and support for grouped convolutions. Empirical evaluation also supports the effectiveness of our approach.

Moreover, **after employing a flexible group count, we can simultaneously reduce the complexity and dependency of the rest of the GKP procedure and drastically improve the efficiency and usability of our method.** As an example, we utilize only one simple clustering operation rather than selecting one of the multiple expansive TMI-score-dependent clustering schemes. This makes our method usable without needing access to the training snapshots or checkpoints of the unpruned model (unlike TMI-GKP). This is a meaningful trait, given the prevalent utilization of pretrained models. We name our method LeanFlex-GKP, emphasizing that it is a GKP method that is more "leaned down" than others, utilizing flexible group counts as its primary mechanism.

We summarize the traits of our proposed method and the contributions of our work as follows:

* **Advancing the progress of GKP by identifying and solving a common pain point: dynamic operations**. We recognize the significance of dynamic operations to GKP, as well as the challenges of integrating them into a GKP procedure. By utilizing flexible group counts as a medium, we actfully introduce such operations to our GKP procedure while avoiding the complications and limitations typically found in other GKP methods. Extensive empirical evaluation supports the effectiveness of our method.
* **Providing an efficient, hassle-free experience.** By reducing the complexity of various stages in the typical GKP procedure, our method provides a drastic advantage in terms of efficiency and adaptability. LeanFlex-GKP is a post-train, one-shot, data-agnostic procedure with little-to-no hyper-parameter tuning or setting handcrafting required, making it one of the most usable structured pruning methods.
* **Guiding future developments of GKP**. Aside from the proposed method itself, our work contains the most comprehensive empirical evaluation and ablation studies currently done on GKP. Given that GKP is an underdeveloped pruning granularity with many attractive properties, we believe our investigation may provide valuable insights and guidance to future scholars working to adopt GKP and its variants.

Due to our introduction's extensive coverage of tightly related pruning methods, we refer readers to Appendix A for more discussion on related works due to page limitation-related concerns.

Figure 4: Procedure of Dynamic Structure Pruning (DSP)

Motivation

### Flexible group count as the dynamic operation in GKP

As mentioned in Section 1.3, the involvement of dynamic operations plays a significant role to the GKP procedure. Yet, current GKP methods tend to adopt dynamic operations at the cost of adding complications or imposing limitations. Take TMI-GKP (Zhong et al., 2022) and Dynamic Structure Pruning (DSP) (Park et al., 2023) as examples: TMI-GKP trials different _clustering schemes2_ at its filter grouping stage per each convolutional layer of the unpruned model, forming a dynamic choice of clustering schemes across the depth of the pruned model. DSP, with the term "dynamic" in its name, allows for dynamic group sizes and in-group pruning ratios upon the formed filter groups and thus enjoys a higher degree of pruning freedom than TMI-GKP.

While both methods demonstrate performance advantages over GKP methods with purely deterministic operations (e.g., KPGP by Zhang et al. (2022) and many of the other alternative GKP procedures introduced in the appendix of Zhong et al. (2022)), the addition of such dynamic operations also comes with its own respective costs.

In the context of TMI-GKP, certain _clustering schemes_, which consist of combining a dimensionality reduction technique with a clustering algorithm like \(k\)-PCA + \(k\)-Means, my incur significant computational costs. e.g., \(k\)-PCA -- one of the candidate dimensionality reduction techniques utilized in TMI-GKP -- requires an eigen decomposition of a convolutional layer's weight tensor, which is an expensive procedure requiring a complexity more than \((n^{3})\) for a \(n n\) matrix (Pan & Chen, 1999). Yet, all but a single produced clustering result are discarded if they are not preferred by its tickets magnitude increase (TMI) score: a weight-shift related metric inspired by series of works on the _lottery ticket hypothesis_(Frankle & Carbin, 2019). This makes the use of TMI-GKP challenging should the width of the target network become large.

In DSP, dynamic behavior is present in both the filter grouping and grouped kernel pruning stages, where the (learn-based) filter groups are allowed to be in different sizes, yet each filter group may opt to remove a different amount of grouped kernels, resulting in a pruning granularity that is finer than typical equal-group-equal-pruning-ratio GKP methods (Yu et al., 2017; Zhong et al., 2022; Zhang et al., 2022). However, with the pruned network having different tensor shapes within the same layer, it can no longer be reconstructed into a grouped convolution format and instead relies on custom-defined model definitions and operators, therefore diminishing its practical adaptability.

In this work, we integrate dynamic operations on Conv2D(groups); also commonly known as "group count" or "number of groups" under a grouped convolution context. This means we may group convolutions with different groups settings across model layers. We emphasize this setup is supported by the grouped convolution operator, and is therefore able to take advantage of the existing and future coming infrastructure updates and support. This flexible group count setup is different to that of TMI-GKP, where a hard-coded groups=8 is applied for all models and layers, yet TMI-GKP decide the grouping result without consideration of the subsequent pruning (but our method does). Such a constant grouping schema and individual approach are revealed to be sub-optimal by our ablation studies in Appendix C. Our setup is also different from DSP, as the end results still have an equal group size and an identical pruning ratio among groups, and thus can be implemented without custom support.

### Leaning out for an efficient GKP procedure

Granted the effectiveness of flexible group counts, we may simultaneously afford to reduce the complexity of various GKP procedures. For example, instead of trialing different cluster schemes or employing a learn-based regularization procedure like TMI-GKP and DSP, we may simply utilize a \(k\)-Means\({}^{++}\) inspired clustering procedure to determine grouping, which drastically decreases the complexity and dependency requirement of filter grouping (Section 3.2).

During the grouped kernel pruning stage, methods like TMI-GKP formalize the procedure as a graph search problem and solve it with a multiple-restart greedy procedure, which is showcased to have a significant performance advantage over vanilla magnitudes or distance-based alternatives (Zhang et al., 2022). However, we decided to use a tactfully designed distance and magnitude-basedheuristic to achieve similar, if not better, accuracy retention rates to the unpruned models (Section 3.3). The removal of this procedure significantly reduces the runtime of our pruning procedure (as clocked in Table 7), and improves its usability on wide models.

### Towards a hassle-free experience

Although the after-prune performance and the efficiency of pruning procedures are certainly reasonable criteria when evaluating a method under a practical context, **usability across a broad scenario and being user-friendly along the process are another important set of factors to consider**. In fact, some of the most widely adopted pruning methods do not necessarily offer the best performance or the fastest runtime, but they are often extremely user-friendly as they can be run and deployed with minimal adjustments. Two examples of such work are OTOv2 (Chen et al., 2023) and DepGraph (Fang et al., 2023), which are architecture-agnostic methods capable of pruning any model, with OTOv2 capable of pruning from scratch.

Our method, LeanFlex-GKP, being a GKP method limited to CNNs, is not at the same level of generalization as OTOv2 or DepGraph. Still, we strive to maximize its usability under constraints by making it a post-train, one-shot, data-agnostic pruning method with standard fine-tuning procedures. This means as long as one has access to the weights of the CNN model and fine-tuning data is provided, one may adopt our pruning method to prune their model and fine-tune via standard SGD with no further interference. In comparison, previous GKP methods like TMI-GKP require access to the training snapshot/checkpoint of the unpruned model, and iterative GKP methods like DSP require regularization learning and pruning operations during the fine-tuning/retraining procedure.

On the note of user-friendliness, our method has little-to-no hyperparameters in place or handcrafted settings, making it extremely easy to use (and simultaneously reduces the human and resource effort of trial-and-error testing different settings). Furthermore, **the user of our method can reliably predict the pruned model size and computation requirement by simply multiplying the pruning rate by the original unpruned model**, making the whole pruning procedure a standardized and predictable experience. Note, this is a useful property surprisingly lacking in many modern pruning methods, such as Lin et al. (2020, 2019a); Park et al. (2023) and Chen et al. (2023), where the user will typically need to trial-and-error various hyperparameter combinations to achieve a certain pruning reduction. **We'd say the importance of being able to predictably obtain a pruned model at a certain size cannot be overly emphasized in a practical context**, as the alternative will require massive computation or even manual effort to search the suitable hyperparameter setting; sometimes, it is even impossible to prune to a specific reduction requirement.

## 3 Proposed method

Our proposed method, LeanFlex-GKP, consists of a four-stage procedure:

1. **Filter grouping**: where we group filters within a certain convolution layer into \(n\) equal-sized filter groups according to their distance towards \(k\)-Means\({}^{++}\) determined centers (Figure 5).
2. **Group kernel pruning:** where we prune a certain amount of grouped kernels out of all filter groups within the same layer. The pruning is determined by each grouped kernel's \(L_{2}\) norm and distance to their geometric median (Figure 6).
3. **Post-prune group count evaluation:** where we evaluate all grouping and pruning strategies obtained under different group count settings and then select the one where the preserved group kernels have the maximum inter-group distance and the minimum intra-group distance (Figure 7).
4. **Grouped convolution reconstruction:** where we convert the pruned model to a grouped convolution format, just like we showcased in the standard GKP procedure (Figure 2). As a general overview, the theme of our proposed method is to use the most lightweight and dependency-free measures to fulfill the purpose of each GKP stage. In the sections below, we will walk through the technicalities of our method, as well as demonstrate that **a SOTA-capable GKP method with many favorable properties can be forged by discerningly putting basic tools together and leveraging the power of flexible group counts**.

### Preliminaries

Suppose there is a convolutional neural network model \(\) with \(L\) convolutional layers, then the layer with index \(l\) is denoted as \(^{l}\). A layer can be viewed as a 4D tensor \(^{l}^{C^{l}_{out} C^{l}_{in} H^{l} W ^{l}}\), in which \(C^{l}_{in}\) is the number input channels on layer \(l\) (number of kernels in a filter), \(C^{l}_{out}\) is the number output channels on layer \(l\) (number of filters in a layer), and \(H^{l} W^{l}\) is the kernel size. The task to perform a grouped convolution reconstruction upon \(^{l}\), as illustrated in Figure 2, can be described as converting \(^{l}\) to a \(^{l}^{n C_{out}^{l} m H^{l} W^{l}}\), where \(n\) stands for the group count setting of this conversion, and \(m=C_{out}^{l}/n\) representing the group size.

### KPP-aware filter grouping

The general goal of filter grouping is to cluster filters that are similar to each others within the same group, so that when such filters are "partially removed" due to pruning, the leftover components can hopefully cover the representation power of the removed components. In previous works like TMI-GKP (Zhong et al., 2022) and DSP (Park et al., 2023), this procedure is rather resource-intensive, with TMI-GKP trialing expensive clustering schemes under the guidance of its TMI score, and DSP employing a learning-based procedure.

In order to streamline the grouping process and mitigate complexity, we devised an cost-effective filter clustering algorithm based on the clustering centers obtained by \(k\)-Means\({}^{++}\) (KPP). Distinguishing itself from direct utilization of KPP cluster assignments, our approach exclusively leverages the clustering centers, reinforced by two straightforward greedy strategies. Our procedure is illustrated in Figure 5. We denote \(n\) to be the group count and \(m=C_{out}^{l}/n\) to be the group size (number of filters within each filter group). In this particular visualization, we have \(n=3\) and \(m=4\). We demonstrate the efficiency and performance advantage of our method with wall-clock results in Table 7 and accuracy results in Table 2, support our claims made in Section 2.2 and Section 2.1.

### \(L_{2}\) & geometric median-based grouped kernel pruning

Previous methods like TMI-GKP converted its grouped kernel selection problem as a graph search problem, added with the help of a greedy procedure and multiple restarts. While such a procedure is generally efficient, it is still time and resource-consuming given a layer with a large amount of in_channels. Thus, inspired by the toolsets proposed in FPGM (He et al., 2019), we utilize a simple combination of \(L_{2}\) norm and Geometric Median-based distance to form a lightning-fast pruning procedure, as illustrated in Figure 6.

Again, we demonstrate the efficiency advantage of our method with Table 7, as we claimed in Section 2.2.

### Post-prune group count evaluation

One primary motivation for our work is that our method makes use of flexible group counts under a GKP procedure. However, it is intrinsically challenging to evaluate clustering quality under different group counts (e.g., previously, Zhong et al. (2022) suggests metrics like a Silhouette score have little bearing under a network pruning context). Thus, we simply employ another Geometric

Figure 5: Visualization of the LeanFlex-GKP KPP-Aware Filter Grouping Procedure. We first cluster filters (the circles) via KPP into \(n\) groups with no constraint on having an equal group size to determine clustering centers (the squares), as in (a). Then, our operation can be viewed as a cycle between assigning \(m\) nearest filters into a KPP center to form a filter group, then finding the next KPP center to do subsequent filter assignments, as in (b) \(\) (c); until \(n\) filter groups are formed (the first KPP center is picked at random). Last, we conduct a multiple restart and repeat the (b)\(\)(c) center-finding-filter-assignments, as showcased in (d). After all multiple restarts, we are left with \(n\) candidate filter grouping strategies, and select the strategy that has filters with the least intra-group distance to their respective KPP centers (having less summed length on red arrows).

Median-based evaluation as we have already done so in Section 3.3. We illustrate our evaluation as Figure 7 and provide **a walk-through of the complete LeanFlex-GKP procedure in pseudocode as Algorithm 1**. Given each group count evaluation is conducted upon a pruned convolutional layer (after being grouped with different Conv2d(groups)), our method makes connections between the (originally independent) filter grouping and grouped kernel pruning stage. Ablation study results in Table 4 confirm the advantage of this integral optimization design over other alternative setups.

## 4 Experiments

We evaluate the effectiveness of our method against 32 other densely structured pruning methods (Table 8) with coverage including BasicBlock (20/32/56/110) and BottleNeck ResNets (50/101) (He et al., 2016), VGG11/13/16 (Simonyan & Zisserman, 2015), DenseNet40 (Huang et al., 2017), and MobileNetV2 (Sandler et al., 2018). The datasets we used include CIFAR10/100 (Krizhevsky et al., 2009), Tiny-ImageNet (Wu et al., 2017), and ImageNet-1k (Deng et al., 2009). Please refer to Appendix D for full details on experiment settings.

### Results

Due to page limitations, here we only provide an abbreviated version of our experiments at Table 1. **We refer our readers to Table 9 to 16 in Appendix D for the full experiment results**, where we compared against 32 different structured pruning methods illustrated in Table 8 and evaluated our methods under 20 different settings specified in Table 5. **We also provide a series of ablation studies in Appendix C to facilitate an anatomical understanding of our proposed method.**

For all experiment results, **DA** represents if the method is data-agnostic (pruning can be done without access to data), **IP** indicates if a method is considered an iterative pruning method (utilizing a train-prune cycle), and **RB** reports recovery budget (in terms of epochs). All other reported criteria are in terms of \(\%\). **BA** and **Pruned** respectively report the unpruned (baseline) accuracy and the pruned accuracy. Methods marked with \({}^{*}\) are drawn from their original or (third-party) replicated publication; the rest are replicated by us to ensure a fair comparison (often with an identical baseline). Generally speaking, a method that is **DA**, **IP**, and demands a smaller **RB** is likely to be more user-friendly.

Figure 6: Visualization of LeanFlex-GKP \(L_{2}\) & Geometric Median-based Grouped Kernel Pruning Procedure. Given an unpruned filter group as in (a), we first calculate the Geometric Median (GM) of its Grouped Kernels (GKs), as well as each GK’s distance to the GM and their \(L_{2}\) norm. These distances and the \(L_{2}\) norm are visualized in (b) as the length of black arrows and the area of green circles, respectively. The GKs with large \(L_{2}\) norms and small distances to their GMs are preserved and eventually reconstructed to the grouped convolution format, as shown (c) to (d) — please refer to Appendix B.1 for details.

Figure 7: Visualization of LeanFlex-GKP Group Count Evaluation. We first compute the GM among retained grouped kernels and then calculate the inner and outer distance among them. After a normalization w.r.t. the group count, the one with the highest average (Outer Distance — Inner Distance) is chosen; please refer to Appendix B.2 for details.

  
**Method** & **DA** & **IP** & **RB** & **BA** & **Pruned** & \(\)**Acc** & \(\)**MACs** & \(\)**Params** \\   & MACs \(\) 313.4M & Params \(\) 14.7M & \\  CC (Li et al., 2021) & ✗ & ✗ & 300 & 93.94 & 94.14 & \(\) 0.20 & 43.18 & - \\ GAL (Lin et al., 2019b) & ✗ & ✓ & 300 & 93.94 & 91.29 & \(\) 2.65 & 35.16 & 47.40 \\ HRank (Lin et al., 2020) & ✗ & ✓ & 300 & 93.94 & 93.57 & \(\) 0.37 & 32.28 & 40.82 \\ L1Norm (Li et al., 2017) & ✓ & ✗ & 300 & 93.94 & 92.88 & \(\) 1.06 & 42.71 & 37.85 \\ KPGP (Zhang et al., 2022b) & ✓� & ✗ & 300 & 94.27 & \(\) 0.13 & 43.15 & 43.59 \\ TMI-GKP (Zhong et al., 2022) & ✓� & ✗ & 300 & 93.94 & 94.07 & \(\) 0.10 & 25.00 & - \\
**LeanFlex-GKP (ours)** & ✓� & 300 & 93.94 & **94.15** & \(\)**0.21** & 43.15 & 43.59 \\   & MACs \(\) 255.0M & Params \(\) 1.73M & \\  TMI-GKP (Zhong et al., 2022) & ✓� & ✗ & 300 & 94.26 & 94.90 & \(\) 0.64 & 43.31 & 43.52 \\ L1Norm-B (Li et al., 2017) & ✓� & 300 & 94.26 & 92.96 & \(\) 1.30 & 43.17 & 36.69 \\ CC (Li et al., 2021) & ✗� & ✗ & 300 & 94.26 & 94.31 & \(\) 0.05 & 44.54 & 39.47 \\ SEP (He et al., 2018a) & ✗ & ✓ & 300 & 94.26 & 94.44 & \(\) 0.18 & 43.42 & 43.52 \\ GAL (Lin et al., 2019a) & ✗ & ✓ & 300 & 94.26 & 93.42 & \(\) 0.84 & 29.14 & 31.37 \\ FPGM (He et al., 2019) & ✗� & ✓ & 300 & 94.26 & 94.18 & \(\) 0.08 & 43.39 & 43.52 \\ NPPM (Gao et al., 2021) & ✗� & ✗ & 300 & 94.26 & 94.16 & \(\) 0.10 & 42.46 & 35.19 \\ HRank (Lin et al., 2020) & ✗� & ✓ & 300 & 94.26 & 92.96 & \(\) 1.30 & 18.57 & 5.38 \\ DHP (Li et al., 2020) & ✗� & ✓ & 300 & 94.26 & 92.53 & \(\) 1.73 & 60.25 & 64.58 \\ LRF (Joo et al., 2021) & ✗� & ✗ & 300 & 94.26 & 94.94 & \(\) 0.23 & 43.37 & 42.30 \\ OTO2 (Chen et al., 2023) & ✗� & ✓ & 300 & 94.26 & 91.58 & \(\) 2.68 & 37.83 & 42.44 \\ KPGP* (Zhang et al., 2022b) & ✓� & ✗ & 300 & 93.76 & 94.01 & \(\) 0.25 & 43.3 & 43.5 \\
**LeanFlex-GKP (ours)** & ✓� & 300 & 94.26 & **94.92** & \(\)**0.66** & 43.31 & 43.52 \\   & MACs \(\) 98.768M & Params \(\) 2.383M & \\  DCP* (Zhang et al., 2018) & ✗ & - & 400 & 94.47 & 94.69 & \(\) 0.22 & 26.00 & - \\ SCOP* (Tang et al., 2020) & ✗ & - & 400 & 94.48 & 94.24 & \(\) 0.24 & 49.30 & - \\ WM* (Zhang et al., 2018) & ✗ & - & 400 & 94.47 & 94.17 & \(\) 0.30 & 26.00 & - \\ DMC* (Gao et al., 2020) & ✗ & - & 160 & 94.23 & 94.49 & \(\) 0.26 & 40.00 & - \\ MDP* (Guo et al., 2020a) & ✗ & - & 95.02 & 95.14 & \(\) 0.12 & 28.71 & - \\ GDP* (Guo et al., 2021) & ✗ & - & 350 & 94.89 & 95.15 & \(\) 0.26 & 46.22 & - \\ ChipNet* (Tiwari et al., 2021) & ✗ & ✓ & 300 & 93.55 & 92.58 & \(\) 0.97 & 20.00 & - \\
**LeanFlex-GKP (ours)** & ✓� & 300 & 93.87 & 94.30 & \(\)**0.43** & 28.74 & 26.98 \\   & MACs \(\) 255.001M & Params \(\) 1.734M & \\  TMI-GKP (Zhong et al., 2022) & ✓� & ✗ & 300 & 72.99 & 72.79 & \(\) 0.20 & 43.31 & 43.37 \\ L1Norm-A (Li et al., 2017) & ✓� & ✗ & 300 & 73.20 & 69.85 & \(\) 3.35 & 43.74 & 44.41 \\ CC (Li et al., 2021) & ✗� & ✗ & 300 & 73.20 & 73.21 & \(\) 0.01 & 43.43 & 19.78 \\ NPPM (Gao et al., 2021) & ✗� & ✗ & 300 & 73.20 & 72.38 & \(\) 0.82 & 42.77 & 18.69 \\ LRF (Joo et al., 2021) & ✗� & ✗ & 300 & 73.20 & 73.58 & \(\) 0.38 & 43.38 & 42.16 \\ LCCL* (Dong et al., 2017) & ✗ & - & 300 & 72.79 & 70.78 & \(\) 2.01 & 31.3 & - \\ SPP* (He et al., 2018a) & ✗ & ✓ & 300 & 74.14 & 71.28 & \(\) 2.86 & 52.3 & - \\ FPGM* (He et al., 2019) & ✗ & ✓ & 300 & 74.14 & 71.25 & \(\) 1.59 & 52.3 & - \\ TAS* (Dong \& Yang, 2019) & ✗ & 300 & 75.06 & 73.16 & \(\) 1.90 & 52.6 & - \\
**LeanFlex-GKP (ours)** & ✓� & 300 & 73.20 & **73.63** & \(\)**0.43** & 43.31 & 43.36 \\   & MACs \(\) 506.254M & Params \(\) 0.865M & \\  TMI-GKP (Zhong et al., 2022) & ✓� & ✗ & 300 & 56.13 & 55.52 & \(\) 0.61 & 37.05 & 36.76 \\ L1Norm-A (Li et al., 2017) & ✓� & 300 & 56.13 & 55.41 & \(\) 0.72 & 35.51 & 32.14 \\ SPP (He et al., 2018a) & ✗ & ✓ & 300 & 56.13 & 53.65 & \(\) 2.48 & 33.96 & 35.38 \\ FPGM (

## 5 Discussion and Conclusion

We believe it is fair to conclude that our proposed method showcases SOTA-competitive (if not beyond) performance across comprehensive combinations of models and datasets. Out of all 20 reported results of LeanFlex-GKP, 17 of them showcased improvements after pruning (yet, no other compared method is able to provide positive \(\)Acc under the three exception setups), suggesting our pruning method actually help on the generalization of the model should there be a reasonable setup.

We also note the compute (MACs) and memory (Params) reduction of our pruned models are almost always within 1% of their assigned pruning rates (e.g., see Table 15 and Table 16), which is a useful characteristic not found in many compared methods3. This supports one of the hassle-free claims we made in Section 2.3. Additionally, we would like to mention the combinations of BasicBlock ResNets with CIFAR10 -- though being some of the most commonly evaluated combinations (Blalock et al., 2020) -- are potentially getting saturated, as methods with significant performance gaps on more difficult model-dataset combinations tend to show little difference upon BasicBlock ResNets and CIFAR10.

In general, our empirical evaluation supports the efficacy of our flexible group count design as well as our goal of assembling a GKP method with only lightweight and low-dependency operations. Following the exposure of Park et al. (2023) for winning an _oral_ recognition at AAAI 2023, our work serves as a more performant, efficient, and user-friendly advancement to the _grouped kernel pruning_ granularity and can be of particular interest for both scholars of the pruning community or end users with practical application needs.