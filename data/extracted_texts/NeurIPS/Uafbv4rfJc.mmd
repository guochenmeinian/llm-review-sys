# Active Negative Loss Functions for Learning with Noisy Labels

Xichen Ye

Shanghai University

Shanghai, China

yexichen0930@shu.edu.cn

&Xiaoqiang Li

Shanghai University

Shanghai, China

xqli@shu.edu.cn

&Songmin Dai

Shanghai University

Shanghai, China

laodar@shu.edu.cn

&Tong Liu

Shanghai University

Shanghai, China

tong_liu@shu.edu.cn

&Yan Sun

Shanghai University

Shanghai, China

yansun@shu.edu.cn

&Weiqin Tong

Shanghai University

Shanghai, China

wqtong@shu.edu.cn

Corresponding Author

###### Abstract

Robust loss functions are essential for training deep neural networks in the presence of noisy labels. Some robust loss functions use Mean Absolute Error (MAE) as its necessary component. For example, the recently proposed Active Passive Loss (APL) uses MAE as its passive loss function. However, MAE treats every sample equally, slows down the convergence and can make training difficult. In this work, we propose a new class of theoretically robust passive loss functions different from MAE, namely _Normalized Negative Loss Functions_ (NNLFs), which focus more on memorized clean samples. By replacing the MAE in APL with our proposed NNLFs, we improve APL and propose a new framework called _Active Negative Loss_ (ANL). Experimental results on benchmark and real-world datasets demonstrate that the new set of loss functions created by our ANL framework can outperform state-of-the-art methods. The code is available at https://github.com/Virusdoll/Active-Negative-Loss.

## 1 Introduction

Relying on large-scale datasets with high quality annotations, such as ImageNet , deep neural networks (DNNs) achieve good performance in various supervised classification tasks. However, in practice, the process of labeling large-scale datasets is costly and inevitably introduces noisy (mislabeled) samples. Moreover, empirical studies show that over-parameterized DNNs can easily fit a randomly labeled dataset , which implies that DNNs may have a poor evaluation performance when trained on a noisy dataset. As a result, noisy label learning has received a lot of attention.

Different approaches have been proposed to solve the noisy label learning problem, and one popular research line is to design noise-robust loss functions, which is also the main focus of this paper. Ghosh et al.  have theoretically proved that, symmetric loss functions such as Mean Absolute Error (MAE), are robust to noise, while others like commonly used Cross Entropy (CE) are not. However, MAE treats every sample equally, leading to significantly longer training time before convergence and even making learning difficult, which suggests that MAE is not suitable for training DNNs with challenging datasets . Motivated by this, several works proposed partially robust loss functions, including Generalized Cross Entropy (GCE) , a generalized mixture of CE and MAE, and Symmetric Cross Entropy (SCE) , a combination of CE and a scaled MAE, Reverse CrossEntropy (RCE). Recently, Active Passive Loss (APL)  framework has been proposed to create fully robust loss functions.

APL is one of the state-of-the-art methods, and shows that any loss function can be made robust to noisy labels by a simple normalization operation. Moreover, to address the underfitting problem of normalized loss functions, APL first characterizes existing loss functions into two types and then combines them. The two types are 1) "Active" loss, which only explicitly maximizes the probability of being in the labeled class, and 2) "Passive" loss, which also explicitly minimizes the probabilities of being in other classes. However, by investigating several robust loss functions created by the APL framework, we find that their passive loss functions are always scaled versions of MAE. As we mentioned before, MAE is not conducive to training. As a result, to address the underfitting problem, APL combines active loss functions with MAE, which again may lead to difficulty in training and further limits its performance.

The fact that APL still struggles with MAE motivates us to investigate new robust passive loss functions. In this paper, we propose a new class of passive loss functions different from MAE, called _Negative Loss Functions_ (NLFs). We show that, by combining 1) complementary label learning [7; 8] and 2) a simple "vertical flipping" operation, any active loss function can be made into a passive loss function. Moreover, to make it theoretically robust to noisy labels, we further apply the normalization operation on NLFs to obtain _Normalized Negative Loss Functions_ (NNLFs). By replacing the MAE in APL with NNLF, we propose a novel framework called _Active Negative Loss_ (ANL). ANL combines a normalized active loss function and a NNLF to build a new set of noise-robust loss functions, which can be seen as an improvement of APL. We show that under our proposed ANL framework, several commonly-used loss functions can be made robust to noisy labels while ensuring sufficient fitting ability to achieve state-of-the-art performance for training DNNs with noisy datasets. Our key contributions are highlighted as follows:

* We provide a method to build a new class of robust passive loss functions called _Normalized Negative Loss Function_s (NNLFs). By replacing the MAE in APL with our proposed NNLFs, we propose a novel framework, _Active Negative Loss_ (ANL), to construct a new set of robust loss functions.
* We demonstrate the theoretical robustness of our proposed NNLFs and ANL to noisy labels, and discuss how replacing the MAE in APL with our NNLFs enhances performance in noisy label learning.
* Our empirical results show that the new set of loss functions, created using our proposed ANL framework, outperform existing state-of-the-art methods.

## 2 Preliminaries

### Risk Minimization and Label Noise Model

Consider a typical K-class classification problem. Let \(^{d}\) be the \(d\)-dimensional feature space from which the samples are drawn, and \(=[k]=\{1,,K\}\) be the label space. Given a clean training dataset, \(=\{(_{n},y_{n})\}_{n=1}^{N}\), where each \((_{n},y_{n})\) is drawn _i.i.d._ from an unknown distribution, \(\), over \(\). We denote the distribution over different labels for sample \(\) by \((k|)\), and \(_{k=1}^{K}(k|)=1\). Since there is only one corresponding label \(y\) for a \(\), we have \((y|)=1\) and \((k y|)=0\).

A classifier, \(h()=_{i}f()_{i}\), where \(f:\), \(^{K},\), \(^{T}=1\), is a function that maps feature space to label space. In this work, we consider \(f\) as a DNN ending with a softmax output layer. For each sample \(\), \(f()\) computes its probability \((k|)\) of each label \(k\{1,,K\}\), and \(_{k=1}^{K}(k|)=1\). Throughout this paper, as a notation, we call \(f\) itself the classifier. Training a classifier \(f\) is to find an optimal classifier \(f^{*}\) that minimize the empirical risk defined by a loss function: \(_{n=1}^{N}(f(_{n}),y_{n})\), where \(:^{+}\) is a loss function, and \((f(),k)\) is the loss of \(f()\) with respect to label \(k\).

When label noise is present, our model can only access a corrupted dataset \(_{}=\{(_{n},_{n})\}_{n=1}^{N}\), where each sample is drawn _i.i.d._ from an unknown distribution, \(_{}\). In this paper, we consider a popular approach for modeling label noise, which simply assumes that, given the true label \(y\), the corruption process is conditionally independent of input features \(\). So we can formulate noisy label \(\) as:

\[=y&(1-_{y})\\ j,j[k],j y&_{yj},\] (1)

where \(_{yj}\) denotes the probability that true label \(y\) is corrupted into label \(j\), and \(_{y}=_{j y}_{yj}\) denotes the noise rate of label \(y\). Under our assumption of label noise model, label noise can be either _symmetric_ or _asymmetric_. The noise is called _symmetric_, if \(_{ij}=}{K-1}, j y\) and \(_{i}=, i[k]\), where \(\) is a constant. And for _asymmetric_ noise, \(_{ij}\) is conditioned on both the true label \(i\) and corrupted label \(j\).

### Active Passive Loss Functions

Ghosh et al.  have shown, under some mild assumptions, a loss function \(\) is noise tolerant if it is symmetric: \(_{k=1}^{K}(f(),k)=C,\), where \(C\) is some constant. Based on this, Ma et al.  proposed the normalized loss functions, which normalize a loss function \(\) by:

\[_{}=(f(),y)}{_{k=1}^{K} (f(),k)}.\] (2)

This simple normalization operation can make any loss function robust to noisy labels, since we always have \(_{k}^{K}_{}=1\). For example, the Normalized Cross Entropy (NCE) is:

\[NCE=^{K}(k|)(-(k|))}{_{j=1}^{K} _{k=1}^{K}(y=j|)(k|)}.\] (3)

Similarly, we can normalize FL, MAE, and RCE to obtain Normalized Focal Loss (NFL), Normalized Mean Absolute Error (NMAE), and Normalized Reverse Cross Entropy (NRCE), respectively.

But a normalized loss function alone suffers from the underfitting problem. To address this, Ma et al.  characterize existing loss functions into two types: _Active_ and _Passive_. Denote the function of loss \((f(),y)\) by \((f(),k)\), that is \((f(),y)=_{k=1}^{K}(f(),k)\) (e.g., let \(\) be CE, then \((f(),y)\)\(=_{k=1}^{K}(k|)(-(k|))\), and \((f(),k)=(k|)(-(k|))\)), we have the following definitions:

**Definition 1** (Active loss function).: \(_{}\) is an active loss function if \((,y), k y,(f(),k)=0\).

**Definition 2** (Passive loss function).: \(_{}\) is a passive loss function if \((,y), k y,(f(),k) 0\).

Active loss functions only explicitly maximize \((y|)\), the classifier's output probability at the class position specified by the label \(y\). In contrast, passive loss functions also explicitly minimize \((k y|)\), the probability at least one other class positions. Accordingly, the active loss functions include CE, FL, NCE, and NFL, while the passive loss functions include MAE, RCE, NMAE, and NRCE. These two types of loss functions can mutually boost each other to mitigate underfitting, and we refer the reader to  for more detailed discussions. By combining them, Ma et al. proposed the Active Passive Loss (APL):

\[_{}=_{}+ _{},\] (4)

where \(,>0\) are parameters. As an example, by combining NCE and RCE, Ma et al get NCE+RCE, one of the state-of-the-art methods.

### APL struggles with MAE

As shown in the previous subsection, there are four passive loss functions available to APL, including MAE, RCE, NMAE, and NRCE. However, we can show that all these passive loss functions are scaled versions of MAE. Specifically, \(NMAE= MAE\), \(RCE=- MAE\), and \(NRCE= MAE\) (detailed derivations can be found in appendix A.1). Thus, we can rewrite APL as follows:

\[_{}=_{}+ MAE.\] (5)This indicates that MAE is a necessary component of the current APL. However, as we mentioned before, MAE requires longer training time and even makes learning difficult. Thus, on the one hand, APL needs passive loss functions to mitigate active loss functions underfitting, yet on the other hand, MAE is not training friendly, which may limit the performance of APL. This motivates us to investigate new robust passive loss functions.

## 3 Active Negative Loss Functions

### Method

**Normalized Negative Loss Functions.** Our goal is to find a method that creates robust passive loss functions from existing active loss functions. This method must consist of three components that: 1) let the loss function optimize the classifier's output probability for at least one other class position that is not specified by the label \(y\), 2) let the loss function minimize the classifier's output probability instead of maximizing it, and 3) let the loss function robust to noisy labels. Inspired by NLNL  and APL , we use 1) complementary label learning, 2) "vertical flipping" and 3) normalization operation as these three components respectively.

Complementary label learning [7; 8] is an indirect learning method for training CNNs, which randomly selects complementary labels and trains the CNN to recognize that "input does not belong to this complementary label". Unlike the usual training approach, complementary label learning focuses on the loss of classifier's predictions with complementary labels, which naturally fits with the passive loss function. Here, we only use its basic idea of letting the loss function focus on all classes \(\{1,,K\}\) except the labeled class \(y\).

"Vertical flipping" is a simple operation that can convert the loss function from "maximizing" to "minimizing". As shown in the fig. 1, given an active loss function \((f(),k)\), the new loss function \(A-(f(),k)\) is obtained by flipping \((f(),k)\) vertically with axis loss \(=A\). It should be noted that, \(A-(f(),k)\) is the opposite of \((f(),k)\), and it focuses on optimizing \((k|)\) to \(0\).

Based on these two components, given an active loss function \(\), we propose Negative Loss Functions (NLFs) as follows:

\[_{}(f(),y) =_{k=1}^{K}(1-(k|))(A-(f(),k)),\] (6) \[A =([,_{},]^{T},y).\] (7)

Here, \([,_{},]^{T}\) is some probability distribution that may be output by the classifier \(f\), where \((y|)=_{}\), the minimum value of \((k|)\) (e.g., \(0\)). Therefore \(A\) is some constant, the maximum loss value of \(\). In practice, setting \(_{}=0\) could cause some computational problems, for example, if \(\) is CE and \(_{}=0\), then \(A=- 0=+\). So in this paper, unless otherwise specified, we define \(_{}=1 10^{-7}\). This technique is similar to the clipping operation implemented by most deep learning frameworks.

Our proposed NLF can transform any active loss function into a passive loss function, where 1) \((1-(k|))\) ensures that the loss function focuses on classes \(\{1,,K\}\{y\}\), and 2) \((A-(f(),k))\) ensures that the loss function aims to minimize the output probability \((k|)\).

Next, to make our proposed passive loss functions robust to noisy labels, we perform a normalization operation on NLFs. Given an active loss function \(\), we propose Normalized Negative Loss Functions (NNLFs) as follows:

\[_{}(f(),y)=1-(f(),y)}{_{k =1}^{K}A-(f(),k)},\] (8)

where \(A\) has the same definition as eq. (7). The detailed derivation of NNLFs can be found in appendix A.2. Additionally, NNLFs have the property that \(_{}\). Accordingly, we can create NNLFs from active loss functions as follows.

The Normalized Negative Cross Entropy (NNCE) is:

\[NNCE=1-(y|))}{_{k=1}^{K}A-(-(k|))},\] (9)

where \(A=-_{}\).

The Normalized Negative Focal Loss (NNFL) is:

\[NNFL=1-(y|))^{}(y|))}{_{k=1 }^{K}A-(-(1-(k|))^{}(k|))},\] (10)

where \(A=-(1-_{})^{}_{}\).

**ANL Framework.** We can now create new robust loss functions by replacing the MAE in APL with our proposed NNLF. Given an active loss function \(\), we propose the Active Negative Loss (ANL) functions as follows:

\[_{}=_{}+ _{}.\] (11)

Here, \(\) and \(\) are parameters greater than 0, \(_{}\) denotes the normalized \(\) and \(_{}\) denotes the Normalized Negative Loss Function corresponding to \(\). Accordingly, we can create ANL from the two mentioned active loss functions as follows.

For Cross Entropy (CE), we have ANL-CE:

\[_{}=_{}+ _{}.\] (12)

For Focal Loss (FL), we have ANL-FL:

\[_{}=_{}+ _{}.\] (13)

### Robustness to noisy labels

**NNLFs are symmetric.** We first prove that our proposed Normalized Negative Loss Functions (NNLFs) are symmetric. Detailed proofs can be found in appendix B.

**Theorem 1**.: _Normalized negative loss function \(_{nn}\) is symmetric._

**NNLFs are robust to noise.** In reference to Theorem 1 and Theorem 3 from , it has been proven that symmetric loss functions, under some mild assumptions, exhibit noise tolerant in the face of both symmetric and asymmetric noise. Given that our NNLFs fall under the category of symmetric loss functions, they inherently possess the attribute of noise tolerant.

**ANL is robust to noise.** In light of Lemma 3 from , it is understood that the combination of two noise tolerant loss functions retains the noise tolerant attribute. It is noteworthy that both \(_{}\) and \(_{}\) within our ANL are noise tolerant, which makes ANL as a whole noise tolerant.

### NNLFs focus more on well-learned samples

As shown in the fig. 2, by replacing MAE with our proposed NNCE, NCE+NNCE and ANL-CE show better fitting ability. This raises the question: _why does NNLF perform better than MAE?_ In the following, taking NNCE as an example, we analysis the gradients of MAE and NNCE to provide a preliminary answer to this question. Detailed derivations and proofs can be found in appendix C.

The gradient of the MAE with respect to the classifier's output probability can be derived as:

\[_{}}{(j|)}=\{ 1,&j y\\ -1,&j=y..\] (14)The gradient of the NNCE with respect to the classifier's output probability can be derived as:

\[_{}}{(j|)} =\{(j|)}(y|)}{(_{k=1}^{K}A+(k|))^{2}},& j y\\ -(j|)}A+(k|)}{ (_{k=1}^{K}A+(k|))^{2}},&j=y..\] (15)

For the purpose of analysis, we consider how the gradients of NNCE would differ from MAE in the following two scenarios: 1) given the classifier's output probability of sample \(\), we analyze the difference in gradient for each class, 2) given the classifier's output probabilities of sample \(_{1}\) and \(_{2}\), we analyze the difference in gradient between these two samples.

**Theorem 2**.: _Given the classifier's output probability \((|)\) for sample \(\) and normalized negative cross entropy \(_{}\). If \((j_{1}|)<(j_{2}|)\), \(j_{1} j_{2} y\), then \(_{}}{(j_{1}|)}>_{}}{(j_{2}|)}\)._

**Theorem 3**.: _Given the classifier's output probabilities \((|_{1})\) and \((|_{2})\) of sample \(_{1}\) and \(_{2}\), where \((y|_{1})(k|_{1})\), \(p(y|_{2})(k|_{2})\), \( k\{1,,K\}\), \((j|_{1})=(j|_{2})\), \(j y\), and normalized negative cross entropy \(_{}\). If \((y|_{1})>(y|_{2})\) and \((k|_{1})(k|_{2})\), \( k\{1,,K\}\)\(\{j,y\}\), then \(_{}}{(j|_{1})}>_{}}{(j|_{2})}\)._

theorem 2 and theorem 3 demonstrate that, for the gradient of the non-labeled classes, our NNCE focuses more on the classes and samples that have been well learned compared to MAE, which treats every class and sample equally This property may enhance the model's performance in noisy label learning. Some studies  have shown that during the training process, DNNs would first memorize clean samples and then noisy samples. According to the property revealed by theorem 2 and theorem 3, apart from robustness, our NNLF may potentially help the model to continuously learn the clean samples that the model has memorized in the previous stages of training and ignore the unmemorized noisy samples.

## 4 Experiments

In this section, we empirically investigate our proposed ANL functions on benchmark datasets, including MNIST , CIFAR-10/CIFAR-100  and a real-world noisy dataset WebVision .

### Empirical Understandings

In this subsection, we explore some properties of our proposed loss functions. Unless otherwise specified, all detailed experimental settings are the same as those in section 4.2. More experiments and discussions about gradient, parameter analysis, and \(_{}\) can be found in the appendix D.1.

Figure 2: Training and test accuracies of different loss functions. (a) - (e): CIFAR-10 under 0.8 symmetric noise. (f) - (j): CIFAR-100 under 0.6 symmetric noise. The accuracies of noisy samples in training set should be as low as possible, since they are mislabeled.

**Overfitting problem.** In practice, we find that ANL can lead to overfitting in some experimental settings. To motivate this problem, as an example, we train networks using different loss functions on CIFAR-10 under 0.8 symmetric noise and CIFAR-100 under 0.6 symmetric noise, and the experimental results are shown in fig. 2. As can be observed, in the setting of CIFAR-10 under 0.8 symmetric noise, the training set accuracy of ANL-CE (w/ L2) keeps increasing while the test set accuracy keeps decreasing. We identify this problem as an _overfitting problem_.

It is worth noting that although overfitting occurs, unlike CE, the gap between the clean sample accuracies and the noisy sample accuracies of the training set does not shrink, which indicates that our ANL has some robustness to noisy labels even in the case of overfitting. Moreover, we conjecture that the overfitting is caused by the property of NNLF focusing more on well-learned samples. When the noise rate is high, one might assume that the model has been trained on only a fairly small number of clean samples, with a very large gradient, which can lead to overfitting.

**The choice of regularization method.** We also find that the commonly used L2 regularization may struggle to mitigate the overfitting problem. To address this, we decide to try using other regularization methods. We consider 2 other regularization methods: L1 and Jacobian . To compare the performance of these methods, we apply them to ANL-CE for training on CIFAR-10 under 0.8 symmetric noise. For simplicity and fair comparison, we use \(\) as the coefficient of the regularization term and consider it as an additional parameter of ANL. We keep \(\) and \(\) the same as in section 4.2, tune \(\) for all three methods by following the parameter tuning setting in appendix D.2. We also train networks without using any regularization method. The results reported in fig. 3. As can be observed, among the three regularization methods, only L1 can somewhat better mitigate the overfitting problem. If not otherwise specified, all ANLs in this paper use L1 regularization.

**Robustness and fitting ability.** We conduct a set of experiments on CIFAR-10/-100 to verify the robustness and fitting ability of our proposed loss functions. We set the noise type to be symmetric and the noise rate to 0.8 for CIFAR-10 and 0.6 for CIFAR-100. In each setting, we train the network using different loss functions, including: 1) CE, 2) MAE, 3) NCE+RCE, 4) ANL-CE (w/ L2), and 5) ANL-CE (w/ L1). For ANL-CE (w/ L2), we set its parameters \(\) and \(\) to be the same as ANL-CE (w/ L1) and set its weight decay to be the same as NCE+RCE.

As can be observed in fig. 2: 1) CE is not robust to noise, the accuracies of clean and noisy samples in the training set are continuously close to each other, 2) MAE is robust to noise, the accuracies of clean and noisy samples in the training set keep moving away from each other, but its fitting ability is insufficient, especially when the dataset becomes complex, 3) NCE+RCE is robust to noise and has better fitting ability compared to MAE, 4) ANL-CE (w/ L2) is robust to noise and has stronger fitting ability, but suffers from over-fitting. and 5) ANL-CE is robust to noise and mitigates the impact of overfitting to achieve the best performance. To summarize, our proposed loss functions are robust to noise, NNLF shows better fitting ability than MAE, and L1 regularization addresses the overfitting problem of NNLF.

  Methods & Clean (\(=0.0\)) & \(=0.2\) & \(=0.4\) & \(=0.6\) & \(=0.8\) \\  NCE & 88.68 & 81.65 & 74.80 & 63.14 & 37.52 \\ NNCE & 91.51 & 90.09 & 86.91 & 82.16 & 57.06 \\ ANL-CE & 91.66\(\)0.04 & **90.02\(\)0.23** & **87.28\(\)0.02** & **81.12\(\)0.30** & **61.27\(\)0.55** \\  

Table 1: Test accuracies (%) of different methods on CIFAR-10 datasets with clean and symmetric (\(\{0.2,0.4,0.6,0.8\}\)) label noise. The top-1 best results are in **bold**.

Figure 3: Test accuracies of ANL-CE on CIFAR-10 under 0.8 symmetric noise with different regularization methods and different parameters. \(\) is the weight of regularization term of ANL-CE.

**Active and passive parts separately.** In table 1, we show the results of the active and passive parts separately. We separately train NCE and NNCE on CIFAR-10 with different symmetric noise rates while maintaining the same parameters as ANL-CE. Specifically, for \(\), we set \(\) to \(5.0\) and for \(\), we set \(\) to \(5.0\), while \(\) is set to \(5 10^{-5}\) for both. As indicated in the results, the test set accuracies of NNCE are very close to those of ANL-CE, except in the case with a 0.8 noise rate. This suggests that NNCE performs well on its own at low noise rates. However, at very high noise rates, a combination of active losses is needed to achieve better performance.

### Evaluation on Benchmark Datasets

**Baselines.** We consider several state-of-the-art methods: 1) Generalized Cross Entropy (GCE) ; 2) Symmetric Cross Entropy (SCE) ; 3) Negative Learning for Noisy Labels (NLNL) ; 4) Active Passive Loss (APL) , including NCE+MAE, NCE+RCE, and NFL+RCE; 5) Asymmetric Loss Functions (AFLs) , including NCE+AEL, NCE+AGCE, and NCE+AUL. For our proposed ANL, we consider two loss functions: 1) ANL-CE and 2) ANL-FL. Additionally, we train networks using Cross Entropy (CE), Focal Loss (FL) , and Mean Absolute Error (MAE).

**Experimental Details.** The full experimental results and the detailed settings of noise generation, networks, training and parameters can be found in the appendix D.2.

**Results.** The main experimental results under symmetric and asymmetric label noise are reported in table 2. For more experimental results, please see appendix D.2. As can be observed, our ANL-CE and ANL-FL show significant improvement for most label noise settings of CIFAR-10/-100, especially when the data become more complex and the noise rate becomes larger. For example, on CIFAR-10 under 0.8 symmetric noise, our ANL-CE outperform the state-of-the-art method (55.62% of NCE+AGCE) by more than 5.0%. Overall, the experimental results demonstrate that our ANL can show outstanding performance on different datasets, noise types, and noise rates, which validates the effectiveness of our proposed NNLFs and ANL.

   &  &  &  \\  & & & 0.4 & 0.6 & 0.8 & 0.2 & 0.3 & 0.4 \\   & CE & 99.20\(\)0.02 & 74.46\(\)0.28 & 49.19\(\)0.05 & 22.51\(\)0.23 & 94.02\(\)0.18 & 88.90\(\)0.07 & 81.79\(\)0.34 \\  & MAE & 99.16\(\)0.03 & 98.80\(\)0.02 & 97.69\(\)0.20 & 70.35\(\)1.16 & **91.01\(\)0.08** & 98.42\(\)0.09 & 87.40\(\)0.41 \\  & GCE  & 99.81\(\)0.01 & 98.61\(\)0.13 & 80.80\(\)0.31 & 33.95\(\)0.48 & 96.95\(\)0.07 & 88.99\(\)0.27 & 81.91\(\)0.58 \\  & SCE  & 99.30\(\)0.07 & 97.48\(\)0.16 & 88.38\(\)0.77 & 32.88\(\)0.81 & 97.95\(\)0.23 & 94.00\(\)0.41 & 84.54\(\)0.14 \\  & NCL  & 98.61\(\)0.13 & 97.17\(\)0.09 & 95.49\(\)0.04 & 88.64\(\)1.43 & 98.95\(\)0.01 & 97.51\(\)0.15 & 98.54\(\)0.26 \\  & NCE+RCE  & 99.43\(\)0.02 & 98.51\(\)0.09 & 95.61\(\)0.12 & 74.01\(\)1.38 & 98.90\(\)0.10 & 95.16\(\)0.08 & 91.60\(\)0.22 \\  & NCE+AGCE  & 99.10\(\)0.03 & **98.91\(\)0.04** & **98.90\(\)0.07** & **96.93\(\)0.13** & 99.04\(\)0.02 & **99.94\(\)0.04** & **98.41\(\)0.04** \\  & **ANL-CE** & 99.98\(\)0.05 & 98.45\(\)0.05 & 98.42\(\)0.08 & **96.21\(\)0.12** & 99.00\(\)0.04 & 98.91\(\)0.07 & 98.91\(\)0.10 \\   & **ANL-FL** & 99.13\(\)0.05 & **98.90\(\)0.05** & **98.46\(\)0.12** & 95.32\(\)0.22 & **99.05\(\)0.09** & **99.93\(\)0.02** & **98.18\(\)0.01** \\    & CE & 90.83\(\)0.11 & 85.19\(\)0.12 & 8

### Evaluation on Real-world Noisy Labels

Here, we evaluate our proposed ANL methods on large-scale real-world noisy dataset WebVision , which contains more than 2.4 million web images crawled from the internet by using queries generated from the 1,000 class labels of the ILSVRC 2012  benchmark. Here, we follow the "Mini" setting in , and only take the first 50 classes of the Google resized image subset. We evaluate the trained networks on the same 50 classes of both the ILSVRC 2012 validation set and the WebVision validation set, and these can be considered as clean validation sets. We compare our ANL-CE and ANL-FL with GCE, SCE, NCE+RCE, and NCE+AGCE. The experimental details can be found in the appendix D.3. The results are reported in table 3. As can be observed, our proposed methods outperform the existing loss functions. This verifies that our proposed ANL framework can help the trained model against real-world label noise.

Moreover, in addition to WebVision, to further validate the effectiveness of our method on real-world noisy datasets, we also conduct a set of experiments on CIFAR-10N/-100N , Animal-10N , and Clothing-1M . The experimental details and results can be found in the appendix D.5, which demonstrate the effectiveness of our method on different real-world noisy datasets.

## 5 Limitations

We believe that the main limitation of our approach lies in the choice of regularization method. Although we have experimentally verified that L1 is the most efficient among the three regularization methods (L1, L2, and Jacobian), we lack further theoretical analysis of it. Furthermore, although we only consider relatively simple regularization methods for the sake of fair comparison, other regularization methods, such as dropout  or regmixup , might be more effective in mitigating the overfitting problem caused by NNLF. And we believe that a better solution to overfitting can further improve the performance of our method.

## 6 Related Work

In recent years, some robust loss-based methods have been proposed for robust learning with noisy labels. Here, we briefly review the relevant approaches. Ghosh et al.  theoretically proved that symmetric loss functions, such as MAE, are robust to label noise. Zhang and Sabuncu  proposed Generalized Cross Entropy (GCE), a generalization of CE and MAE. Wang et al.  suggested a combination of CE and scaled MAE, and proposed Symmetric Cross Entropy (SCE). Menon et al.  proposed composite loss-based gradient clipping and applied it to CE to obtain PHuber-CE. Ma et al.  proposed Active Passive Loss (APL) to create fully robust loss functions. Feng et al.  applied the Taylor series to derive an alternative representation of CE and proposed Taylor-CE accordingly. Zhou et al.  proposed Asymmetric Loss Functions (ALFs) to overcome the symmetric condition. Inspired by complementary label learning, NLNL  and JNPL  use complementary labels to reduce the risk of providing the wrong information to the model.

## 7 Conclusion

In this paper, we propose a new class of theoretically robust passive loss functions different from MAE, which we refer to as _Normalized Negative Loss Functions_ (NNLFs). By replacing the MAE in APL with our NNLF, we propose _Active Negative Loss_ (ANL), a robust loss function framework with stronger fitting ability. We theoretically demonstrate that our NNLFs and ANLs are robust to noisy labels and also highlight the property that NNLFs focus more on well-learned samples. We found in our experiments that NNLFs have a potential overfitting problem, and we suggest using L1 regularization to mitigate it. Experimental results verified that our ANL can outperform the state-of-the-art methods on benchmark datasets.