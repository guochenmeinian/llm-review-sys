# VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation

Youpeng Wen\({}^{1}\)1, Junfan Lin\({}^{2}\)1, Yi Zhu\({}^{3}\), Jianhua Han\({}^{3}\),

Hang Xu\({}^{3}\), Shen Zhao\({}^{1}\)1, Xiaodan Liang\({}^{1}\)2\({}^{}\)

\({}^{1}\)Shenzhen Campus of Sun Yat-Sen University,

\({}^{2}\)Peng Cheng Laboratory, \({}^{3}\)Huawei Noah's Ark Lab

Equal contribution, \({}^{}\)Corresponding author

###### Abstract

Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose **VidMan** (**Video** Diffusion for Robot **Man**ipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment's dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7% relative improvement, and demonstrates over 9% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.

## 1 Introduction

In the rapidly advancing field of robotics, accurately predicting and executing precise actions based on sensory inputs is crucial. While traditional approaches [1; 2; 3; 4; 5] for robot manipulation often rely on labor-intensive hand-engineered features and models prone to errors, data-driven methods [6; 7; 8] offer promising solutions. However, the challenge lies in the difficulty and cost of acquiring high-quality robotic data. Recent advancements [9; 10; 11; 12], particularly those utilizing large-scale online video data for learning a video generator, demonstrate significant potential in comprehending complex physical dynamics of the real world. These models, trained on diverse datasets [13; 8], possess a nuanced understanding of the world, suggesting the feasibility of amalgamating and leveraging varied robot visual trajectory data [14; 15; 16] to develop a unified dynamics-aware model for enhanced robot manipulation. Yet, achieving this unification poses challenges; merely fitting data without considering the relationship between visual observations and actions could lead to suboptimal utilization of the data. Hence, there is a pressing need to develop efficient training mechanism and model architecturethat can effectively leverage existing cross-robot and cross-scene data to enhance action prediction accuracy.

As shown in Fig. 1, to optimize the utilization of diverse robot data , we draw upon insights from neuroscience's dual process theory , which unveils the complex mechanisms of information processing and decision-making in the human brain. This theory distinguishes between two cognitive processes: System 1, responsible for rapid, intuitive responses based on immediate sensory inputs, and System 2, which involves slower, long-horizon planning grounded in abstract concepts and understanding of world dynamics . Inspired by these insights, we adopt a two-stage paradigm for robot learning, exemplified by our innovative framework, terms as **VidMan** (**Video** diffusion for robot **M**nipulation). VidMan leverages the power of the video diffusion generation method Open-Sora  for robot imitation learning, tapping into the awareness of long-horizon dynamics inherent in video diffusion models to achieve more nuanced and dynamics-modulated robot action prediction. By learning different facets of data at distinct stages, VidMan acquires an inductive bias of inverse dynamics of robot control, wherein actions are the outcomes of state sequences, significantly enhancing the method's generalization performance, especially under scenarios with limited data.

Specifically, our VidMan employs a two-stage training mechanism, akin to the principles of dual process theory, to enhance stability and significantly improve data utilization: 1) In the first stage, namely the _Dynamics-aware Visionary Stage_, VidMan undergoes pre-training on the Open X-Embodiment  dataset (OXE) using a video diffusion model to predict future trajectories based on historical observations and language instructions. This stage involves the robot learning the dynamics of state transitions from data and accurately perceiving the current environmental state, enabling the model to develop a deep understanding of the environment's dynamics, forming a robust foundation for accurate action prediction; 2) In the second stage, dubbed the _Dynamics-modulated Action Stage_, VidMan incorporates a flexible yet powerful layer-wise self-attention adapter  to seamlessly integrate the pre-trained knowledge from the first stage into action prediction. Through shared neural architecture and parameters with the dynamics-aware visionary stage, this phase transforms VidMan into an implicit inverse dynamics model that infers dynamics-modulated actions without explicitly generating future visual trajectories, rendering it suitable for real-world robot control scenarios.

The performance of VidMan has been evaluated against SOTA baselines on the CALVIN  benchmark, where it achieves a 11.7% relative improvement. In addition, VidMan has shown notable effectiveness on the OXE small-scale dataset, achieving over 9% precision gains. This improvement is particularly evident when the data from the target robot is small, underscoring the effective data utilization of our method. Extensive ablation studies have been conducted to analyze the effects of various design decisions within our method. These experimental results suggest that VidMan represents a meaningful advancement in robotics, providing a valuable tool for developing more capable and responsive robotic systems.

Figure 1: VidManâ€™s two-stage training paradigm mirrors dual process theory: its first stage (like System 2) pre-trains on understanding environment dynamics through video diffusion, forming a foundation for accurate action prediction, while its second stage (like System 1) was adapted from the first stage to leverage the learned dynamics knowledge for rapid, low-level action inference.

Related Work

**Language-guided Robot Manipulation.** Language-guided robot manipulation has emerged as an elastic and straightforward approach to instructing robots to perform various tasks [14; 23]. Some existing methods [24; 25; 10; 26; 27; 28; 29; 30] leverage large language models (LLMs) to plan over the task domain and pass instructions to low-level action policies to generate robot actions. The hierarchical 2D policies [31; 32; 33] predict latent features or images of subgoals given a language instruction, which they feed into lower-level subgoal-conditioned policies. 3D policies [34; 35] combine 3D representations with diffusion objectives to learn manipulation from demonstrations using depth maps and camera extrinsics. Some methods [36; 37] also utilize 3D [38; 39; 40] or 2D [41; 42; 43] detection to identify objects and use constrained optimization methods to control robot operations Another line of work learns language-conditioned policies from unstructured play data, which contains demonstrations with and without language labels [44; 32]. These methods leverage sequence-to-sequence conditional variational auto-encoders to generate latent plans, which are then used to condition the action policy.

**Pre-training for Robot Manipulation.** The field of pre-training for robot learning has garnered significant attention in recent years [9; 26; 11; 45]. Some methods aim to learn useful visual representations through masked image modeling  and contrastive learning . Previous research [46; 47; 45; 25; 48] has focused on empowering robots and other agents with the capability to comprehend and execute language instructions, typically by learning policies conditioned on language. GR-1  and RoboFlamingo  use a GPT-style framework to model action prediction as a token prediction task in CALVIN dataset , and achieve good results. Our approach is most similar to GR-1 in that it utilizes GPT-style framework to predict both video and action, while ours directly uses video's ability to capture future information to predict long sequences of actions.

## 3 Preliminaries

**Robot Dynamics and Inverse Dynamics.** The dynamics of a robot are typically characterized by the forward dynamics and the inverse dynamics. The forward dynamics describe how the current state and action determine the next state. Formally, given a state space \(\) and an action space \(\), the forward dynamics can be represented as the conditional probability distribution: \(P(s_{t+1} s_{t},a_{t})\), where \(s_{t}\), \(s_{t+1}\) are the robot state at timestep \(t\) and \(t+1\), respectively, and \(a_{t}\) is the action taken. Conversely, inverse dynamics describe the probability of an action given a transition from one state to another. The inverse dynamics are particularly important in scenarios where only passive observation data is available, such as data collected from the internet, which often lacks explicit action information. Formally, the inverse dynamics are given by:

\[P(a s_{t},s_{t+1}).\] (1)

**Observations to States.** In many real-world applications, direct access to the state space \(\) is infeasible, and instead, sequences of image observations are provided. Let \(\) denote the observation space, _e.g._, image, proprio. To infer the underlying sequence of states \(S=\{s_{1},s_{2},,s_{T}\}\) from a sequence of observations \(O=\{o_{1},o_{2},,o_{T}\}\). This process can be formally described by finding the sequence of states that maximizes the posterior probability given the observations:

\[S^{*}=_{S}P(S O).\] (2)

Using Bayes' theorem, this can be further expressed as:

\[S^{*}=_{S}P(O S)P(S),\] (3)

where \(P(O S)\) is the likelihood of the observations given the states, and \(P(S)\) is the prior probability of the sequence of states. This formulation is essential in various applications, including hidden Markov models (HMMs) and other state estimation techniques. With more observations, the likelihood \(P(O S)\) becomes more peaked around the true state sequence. This is because more data points allow for better discrimination between different state sequences.

## 4 Method

In this paper, we aim to enhance the precision of robot action prediction by exploiting the intricate dynamics encoded within robot visual trajectories. We propose VidMan, a novel framework leveragingVideo diffusion for robot Manipulation. VidMan employs a dual-stage training strategy: in the first stage, the _Dynamics-aware Visionary Stage_, we enable the model to forecast and imagine potential future trajectories based on historical observations, leveraging the multi-frame prediction capability of the video diffusion model. Through this stage, the model is optimized to understand the dynamics of the environment. In the second stage, the _Dynamics-modulated Action Stage_, we introduce a lightweight layer-wise adapter to seamlessly integrate the visionary predictive stage with fast, adaptive action prediction. This approach decouples the knowledge of the world and embodiment into distinct processes while ensuring seamless integration through the training and utilization of shared parameters. Overview of our method is shown in Fig. 2. In the following sections, we will formulate each of these stages.

### Dynamics-aware Visionary Stage

To endow the model with the knowledge of world dynamics, we formulate this knowledge acquisition stage as future image trajectory generation, which captures dynamic priors and predicts future state transitions more accurately according to Equ. (3). Specifically in our context, the goal is to predict future frames based on historical frames and language instructions. To achieve this, we leverage the capabilities of a multi-frame generation framework based on the video diffusion transformer model (VDT) Open-Sora , which has shown a remarkable ability to generate diverse and physically authentic successive frames aligned with language instructions. For simplicity, we use VDT to represent Open-Sora in the following content.

To prepare the training data, we utilize a pre-trained video tokenizer  to encode successive robot images in a trajectory \(O_{s}=[O_{h},O_{f}]\) into embeddings \(V_{s}=[V_{h},V_{f}]\), where \(V_{h}^{m W H C}\) and \(V_{f}^{n W H C}\) stand for embeddings for \(m\) history images \(O_{h}\) and \(n\) future images \(O_{f}\), respectively. During the forward diffusion process \(V_{s}^{k}(V_{s},,k)\), the noise \((0;1)\) is added to trajectory embeddings \(V_{s}\) according to the diffusion step \(k[1,K]\). The diffusion model is optimized to predict the added noise \(\) from \(V_{s}^{k}\) given the condition hints. As the condition hints, we pad \(V_{h}\) with zero-valued embeddings \(V^{0}\) to the same shape as \(V_{s}^{k}\), which are concatenated with \(V_{s}^{k}\) along the channel dimension to form the condition visual embeddings \(V_{c}^{k}^{(m+n) W H 2C}\). And the language instruction \(y\) is encoded by an text tokenizer  to obtain the language embedding. Mathematically, the denoised diffusion model is optimized according to:

\[_{v}()=_{(V_{c}^{k},y,k)}[\|- _{}(V_{c}^{k},y,k)\|_{2}^{2}].\] (4)

Figure 2: **Overview of VidMan. (a) We use Video Tokenizer to tokenize the uniform sampled robot visual trajectory \(O_{s}\) to video tokens \(V_{s}\). (b) In the 1st Stage, we concatenate the video tokens processed through the diffusion process with the historical tokens along the channel dimension to form \(V_{c}^{k}\). \(V_{c}^{k}\) along with the language tokens and diffusion step \(k\) are fed into Open-Sora for video prediction training. In the 2nd Stage, we use a learnable action token through a layer-wise adapter applied to the output of the Open-Sora Block to obtain tokens \(V_{}\) that integrate future frame information. \(V_{}\) are then fed into the Diffusion Action Head \(_{_{dec}}\) for action prediction training.**

In this training stage, we use only a third-person camera to predict the representation. This approach has two main advantages: a) most robot datasets include only third-person view data; b) training with a fixed third-person viewpoint can reduce the influence of view changes and help the model focus on predicting the transitions of the robotic arm itself. Additionally, our method can easily be extended to multiple cameras by simply inputting multiple cameras of viewpoint.

### Dynamics-modulated Action Stage

According to Equ. (1), actions can be accurately predicted given the states with the inverse dynamics model. One straightforward approach to combine an inverse dynamics model with the VDT learned in the first stage is to separately construct an inverse dynamics model that maps from image observations to actions. During deployment, this model could predict actions from the generated observations of the dynamics-aware visionary stage. However, by this means, actions are only predictable after the VDT conducts a time-consuming iterative denoising diffusion process, which is not ideal for high-frequency robot control. Moreover, the accuracy of the actions is heavily dependent on the accuracy of the predicted observations. Since not all pixels are important for predicting actions, this method could introduce unnecessary bias and time costs. Additionally, learning a separate inverse dynamics model from scratch does not leverage the pre-trained parameters of the VDT.

To address these issues, we propose directly adapting the VDT into an inverse dynamics model. In this way, the dynamics knowledge and implicit states learned in VDT can be seamlessly leveraged to facilitate the prediction of actions. Below, we introduce a lightweight adapter module that effectively transforms the VDT into an implicit inverse dynamics model. The output of this adapted model is then decoded into a sequence of actions using a diffusion-based action head.

**Implicit Inverse Dynamics Adapter.** To transform the VDT into an inverse dynamics model, we incorporated a layer-wise adapter which is inspired by  after each layer in VDT. Each adapter includes a multi-head self-attention and a feed-forward network (FFN) with a gated mechanism. We use \(h\) learnable action tokens \(Q_{}\), concatenating them with the features output from each layer of the VDT, and input them into the layer-wise adapter. This fuses the knowledge of each layer of the VDT to produce \(h\) final action embeddings \(V_{}\). Since we only reuse VDT parameters without its observation generation function, we disable the iterative denoising process by using a fixed diffusion step \(k K\), which turns \(V_{s}^{k}\) into pure Gaussian noise. Formally, the action embeddings are fused with the introduce layer-wise adapter parameterized by \(\):

\[V_{}=_{(,_{})}(V_{c}^{K},y,K,Q_{ }),\] (5)

where \(_{(,_{})}\) represents the VDT parameters incorporated with the layer-wise self-attention adapter with parameters \(_{}\). The flexibility of the layer-wise adapter allows for the integration of domain-specific knowledge into the action embedding. For example, if the downstream robotic manipulation task includes proprioceptive information, we can use a proprioception embedder to convert it into tokens, which are then concatenated with the output of each layer of the VDT to serve as the additional keys and values for the layer-wise adapter. Please refer to Appendix A.2 for more details.

**Diffusion-based Action Head.** The fused action embeddings, \(V_{}\), are subsequently translated into low-level control signals. To achieve this, we utilize a diffusion-based action head , \(_{_{}}\), responsible for decoding action embeddings into executable action signals, such as determining the 7 degrees of freedom (DoF) for the end-effector pose and gripper status. Similar to the dynamics-aware visionary stage, the objective of action prediction is:

\[_{a}(,_{},_{})=_{(V_ {},l)}[\|^{}-_{_{}} (V_{},^{},l),l\|_{2}^ {2}],\] (6)

where \(^{}(0,1)\), \(l[1,L]\) stands for the diffusion step and \(\) denotes the forward diffusion process. Note that this diffusion-based action head is relatively small compared to the VDT, making its computational cost and time consumption almost negligible.

## 5 Experiment

In this section, we conduct comprehensive experiments to validate the effectiveness of our methods, along with in-depth ablation studies.

### Experiment Settings

#### 5.1.1 2-stage Training Setting

**Settings of dynamics-aware visionary stage.** We initialized our model with the weights from Open-Sora's  16x256x256 text-to-video model, which was trained on internet data . Since we concatenated historical frames along the channel dimension, we append a zero matrix to the parameters of the tokens embedder layer in Open-Sora accordingly. We use Open X-Embodiment Dataset  to train our model, which was constructed by pooling 60 existing robot datasets from 34 robotic research labs around the world. We referred to the Octo  codebase to selected 25 datasets from it, which are heterogeneous not just in terms of the robot type, but also in the sensors (e.g., including or not including wrist cameras) and labels (e.g., including or not including language instructions). Unless otherwise specified, we sampled video sequences from trajectories at intervals of 3, resulting in 4-frame video sequences, with 2 historical frames and 2 future frames. At this stage, we only used images from the third-person camera with 256 \(\) 256 resolution. We use a maximum of \(K=1000\) diffusion step.

**Settings of dynamics-modulated action stage.** In this training stage, we used data from both the Open X-Embodiment (OXE) and CALVIN  datasets for training. We additionally incorporated images from a wrist camera as extra observations. For the data in OXE, we predicted 12 action steps, whereas for the CALVIN data, we predicted 10 steps. For the diffusion policy head, we set the noise addition steps to 100. We used a 224 \(\) 224 third-person camera and a 224 \(\) 224 wrist camera. In this stage, we set VDT's diffusion step to the maximum, i.e., \(k=K\), and used pure noise as \(V_{s}^{K}\). The VDT does not conduct the iterative denoising process. As for the diffusion-based action head, we set the maximum diffusion step as \(L=100\). More details can be found in Appendix A.2

#### 5.1.2 Benchmark and Baselines

**Simulation Evaluation:** The CALVIN benchmark utilizes the PyBullet simulator and involves a Franka Panda Robot arm interacting with various environments labeled A, B, C, and D. Each environment includes a desk, a sliding door, a drawer, a button controlling an LED, a switch for a lightbulb, and three colored blocks (red, blue, and pink). These environments differ in desk textures and object positions. CALVIN provides 24 hours of unstructured tele-operated play data, with 1% annotated with language descriptions. Each instruction chain consists of five sequential language instructions for execution. Evaluation follows a zero-shot generalization setup, training models on environments A, B, and C and testing on D. Performance metrics include success rates and average completion of sequential tasks, as per prior studies. Notably, CALVIN lacks a motion planner, requiring all models to predict robot pose trajectories.

**Offline Evaluation:** We also report offline metrics, including the average of xyz accuracy and euler angle accuracy (Avg xyz ang) and MSE for end-to-end action prediction on Bridge , Taco Play , Cable Routing  and Autolab UR5 , which are presented in OXE . Following Octo , we use continuous action space. XYZ accuracy measures the precision of the robot's predicted 3D position (X, Y, Z coordinates) compared to the ground truth values during evaluation. Euler angle accuracy measures the precision of the robot's predicted orientation angles (rotations around X, Y, and Z axes) compared to the ground truth values during offline evaluation. Specifically, XYZ accuracy refers to whether we predicted the XYZ delta within 0.5 radians and 50% of the norm while in motion. Euler angle accuracy indicates whether we predicted the rotation delta within 0.5 radians during movement. Additionally, we reported the mean squared error (MSE) which reflects how well each model predicts the actions.

**Baselines:** On the CALVIN benchmark, we compare our approach to the hierarchical 2D policies of MCIL , HULC , and SuSIE , which predict latent features or subgoal images based on language instructions and feed them into lower-level subgoal-conditioned policies. These methods can train the low-level policy using the full CALVIN dataset, rather than being restricted to the language-annotated subset. We also compare against large-scale 2D transformer-based policies like RT-1 , RoboFlamingo , and GR-1 , which pretrain on extensive interaction or observational (video-only) data. Additionally, to better highlight the performance of our method, we compare with the 3D methods, 3D Diffusion Policy  and 3D Diffuser Actor . Both share a similar goal of combining 3D representations with diffusion objectives to learn manipulation from demonstrations using depth maps and camera extrinsics. As for OXE benchmark, we compared our method withRT-1-X  and Octo . RT-1-X is an openly available generalist robot policy which are trained on OXE using RT-1  framework. Octo is a transformer-based diffusion policy that supports flexible task and observation definitions, pretrained on OXE. These baselines are used to validate the benefit of our two-stage training strategy since both RT-1-x and Octo are optimized with all data in a train-from-scratch manner. To demonstrate the effectiveness of using diffusion models in learning dynamics priors and modeling states in the first stage, we implemented a GPT-style (VidMan-GPT) policy, as a baseline. This baseline autoregressively predicts the next images and actions in the first stage. This allows us to compare, in a fair manner, the benefits of simultaneously predicting multiple frames in a diffusion-like manner versus predicting only the next frame in a GPT-like fashion for modeling dynamics priors and state representations. We only predict a single frame and the corresponding action. We discarded the diffusion scheduler and used random masking  to reconstruct images to calculate the video loss.

### Comparison Results

**Multi-Task Performance.** We first conducted the initial stage of training on OXE, then utilized CALVIN's static camera and wrist camera as the two input sources for the second stage of training on CALVIN. Following the setup in GR-1, we used the portion of the CALVIN dataset that includes language instruction labels, while also incorporating CALVIN's proprioceptive state data as additional input into the layer-wise adapter. Tab. 1 compares the performance of mainstream methods such as SuSIE, RT-1, RoboFlamingo, GR-1, and 3D Diffuser Actor against VidMan. Compared to hierarchical 2D policies like SuSIE, which struggle to effectively leverage language information at the low level despite being able to train on the entire dataset ("All"), these methods generally underperform in terms of generalization compared to end-to-end policies like GR-1 and RoboFlamingo. Our method adopts an end-to-end training approach, outperforming the best hierarchical architecture, SuSIE, by 0.73 in Avg. Len., while using less data. When compared to large-scale 2D transformer-based policies such as GR-1 and RoboFlamingo, which also use an end-to-end output approach, our method is highly comparable. Thanks to pretraining on the large-scale robotics dataset OXE and utilizing the Open-Sora architecture, which is more vision-friendly, our method outperforms GR-1 by a clear margin on average length (11.7% relative improvement). In comparison to 3D Diffuser Actor, which leverages depth information to predict actions, our model remains competitive due to the ability to pretrain on large-scale datasets without depth information, which are easier to collect.

**Offline Performance.** To evaluate offline performance, in this part, the second stage of our model is optimized with OXE dataset. We evaluate performance on in-distribution tasks. Specifically, we conduct two parts of evaluation: 1) the evaluation on domains that have small-scale sub-datasets in OXE (Taco Play, Cable Routing and AUTOLab UR5), where we would expect pretraining with larger datasets can significantly improve performance; 2) the evaluation on domains with large-scale dataset (Bridge), where we expect further improvement to be more challenging. We compare our results with RT-1-X, Octo-small, Octo-base, and VidMan-GPT on the four sub-datasets. We compared the average

    &  &  \\   & & 1 & 2 & 3 & 4 & 5 & Avg. Len. \\ 
3D Diffusion Policy  & Lang & 28.7 & 2.7 & 0 & 0 & 0 & 0.31 \\ MCIL  & All & 30.4 & 1.3 & 0.2 & 0 & 0 & 0.31 \\ HULC  & All & 41.8 & 16.5 & 5.7 & 1.9 & 1.1 & 0.67 \\ RT-1  & Lang & 53.3 & 22.2 & 9.4 & 3.8 & 1.3 & 0.9 \\ RoboFlamingo  & Lang & 82.4 & 61.9 & 46.6 & 33.1 & 23.5 & 2.48 \\ SuSIE  & All & 87 & 69 & 49 & 38 & 26 & 2.69 \\ GR-1  & Lang & 85.4 & 71.2 & 59.6 & 49.7 & 40.1 & 3.06 \\
3D Diffuser Actor  & Lang & 93.8 & 80.3 & 66.2 & 53.3 & 41.2 & 3.35 \\ VidMan(Ours) & Lang & **91.5** & 76.4 & 68.2 & 59.2 & 46.7 & 3.42 \\   

Table 1: **Zero-shot long-horizon evaluation on CALVIN.**_All_ denotes that the model is trained on the entire dataset, including visual data without language annotations, while _Lang_ refers to training on only the language-labeled data. Our method outperforms the hierarchical 2D policies (MCIL , HULC  and SuSIE ) and large-scale 2D transformer-based policies (RT-1  RoboFlamingo  and GR-1 ), while also remaining competitive compared to 3D-based policies (3D Diffusion Policy  and 3D Diffuser Actor ).

values of xyz accuracy and angular accuracy (Avg xyz ang), the higher the better, as well as the MSE (the lower the better). As shown in Fig. 3, it can be observed that our method outperforms the state-of-the-art open-source method, Octo, in both evaluation settings. Particularly on the small-scale sub-datasets CableRouting and Autolab UR5, our method improves the offline average xyz angle accuracy by 9.9% and 9.0% over Octo, respectively. The key difference between Octo and our method is in the optimization approach. Octo employs a co-training strategy, utilizing all data at once. However, our two-stage training strategy demonstrates a significant improvement, especially in domains with limited data. This improvement is attributed to our approach's ability to utilize the training data more efficiently by implicitly leveraging the inductive bias of inverse dynamics. Regarding Taco Play, the improvement we achieved is less notable. This is likely because the Taco Play scenes are relatively simple, and various methods tend to achieve similar performance levels in such environments. Additionally, our method shows improvements of 5.6% on the large-scale dataset Bridge which also demonstrates the effectiveness of our method. We have also achieved significant improvements over our own baseline, VidMan-GPT, across all four datasets, demonstrating the superiority of our model architecture. Details can be found in Tab. 7

### Extra Ablation Studies

In this section, we conduct ablation studies for VidMan to answer the following questions: 1) Can co-training obtain a similar performance to our two-stage training? 2) Whether pretrained with general video data on the internet help improve the performance? 3) How much performance is gained by using the layer-wise self-attention block? 4) Does increasing the frame sampling interval positively impact action prediction? In the following, we answer each of these questions.

**The importance of two-stage training.** In our second stage, we only use action as a supervision signal to learn an implicit inverse dynamics model \(P(a_{t} s_{t},s_{t+1})\) in Equ. (1). In other words, it suggests that we should not co-train with both actions and frames generation at the same time, otherwise a joint distribution \(P(a_{t},s_{t+1} s_{t})\) is learning. To validate this viewpoint, we conducted experiments where the frames generation loss \(_{v}\) is also used in the second stage along with \(_{a}\). The result of this variant is presented as "co-train" in Tab. 1(a). In comparison to using only \(_{a}\) in the second stage, namely "action-only" in the table, the average score of "co-train" degrades severely and approaches the performance of baseline methods in Tab. 1(a). This further supports the effectiveness of our two-stage training strategy.

Table 2: **Ablation Studies on Key Factors of VidMan..** We conduct finetune experiments on CALVIN. Average length is used. Best practice settings are marked in \(\).

(a) **2-stage training.** Training only the action prediction in the second stage is crucial.

Figure 3: **Offline Performance.** The average accuracy (Avg xyz ang) of xyz accuracy and angle accuracy and MSE correspond to the left and right y-axes of the graph respectively. All models were trained on OXE and validated on offline performance across four datasets. VidMan outperformed Octo-base  by 5.6% on Bridge, 2.6% on Taco Play, 9.9% on Cable Routing, and 9.0% on Autolab UR5. Additionally, Our method also shows improvements over the VidMan-GPT approach.

**Pretrained with general video data.** To evaluate whether pretraining with general video data (not specific to robotics) from the internet helps improve performance, we conducted two control experiments. "w/o Pre-train" refers to our model without video pretraining, "w/ Ego4d" represents pretraining during the first stage using the general dataset Ego4d  for video prediction, and "w/ OXE" refers to pretraining using the robot-specific dataset OXE. For the Ego4d dataset, we followed the same processing pipeline as GR-1. As shown in Tab. 1(b), pretraining with robot-specific video data resulted in a 0.53 increase in the average task length on CALVIN compared to "w/o Pre-train", while pretraining with general data ("w/ Ego4d") slightly decreased performance by 0.13. This suggests that training on web-based general data can yield marginal improvements, and pretraining in robotics domains significantly boosts performance.

**Effect of layer-wise adapter.** To measure the effect of the layer-wise adapter, we removed it (w/o adapter) and directly extracted the observed information into the action policy head by concatenating learnable action token to observation tokens before Open-Sora blocks. We found that this significantly reduced performance, comparing row 1 and row 3 in Tab. 1(c). Additionally, we found that training only the layer-wise adapter while keeping the Open-Sora blocks fixed leads to faster convergence, as shown in Fig. 4. The training loss of the "freeze" curve converges faster than the "not freeze" curve. If both the adapter and Open-Sora blocks are trained, better accuracy can be achieved, by comparing row 2 and row 3 in Tab. 1(c). In this sense, we can quickly adapt to a specific robotic scenario with the adapter being trained. In brief, layer-wise adapter can provide good performance. If Open-Sora blocks' parameters are not fixed, better performance can be achieved; if the parameters are fixed, faster convergence can be obtained.

**Effect of frame sampling interval.** We compare the effectiveness of different frame sampling intervals (_i.e._ 1, 2, 3, and 4) on Bridge Dataset and CALVIN. With a larger frame interval, a longer historical information our model can perceive and a longer future information needed to be predicted. The total length of the four experimental frames is 4, including 2 historical frames and 2 future frames. Results are shown in Tab. 3. It is observed that increasing the intervals from 1 to 3 improves performance on Bridge and CALVIN. The potential reason is that consecutive frames are very similar, and predicting frames that are farther away from the current step helps the robot gain a better understanding of the future. However, when the interval increases from 3 to 4, this improvement saturates quickly. We speculate that predicting frames too far into the future may not offer effective guidance for immediate local action prediction.

## 6 Conclusion

In this paper, we propose VidMan, a novel framework utilizing video diffusion models for robot imitation learning, which addresses the limitations of current GPT-style paradigms in real-time applications. By combining a Dynamics-aware Visionary Stage, which develops a deep understanding of environment dynamics through pre-training on the Open X-Embodiment dataset, with a Dynamics-modulated Action Stage that efficiently integrates this knowledge into action prediction, VidMan achieves both high precision and computational efficiency. This two-stage approach, ensures robust and rapid action generation, significantly improving performance on benchmarks like CALVIN and the OXE dataset. In the future, we will expand VidMan to be able to perceive more dimensions of information.

    & & &  &  \\ interval & FID\(\) & FVD\(\) & MSE \(\) & xyz \(\) & angle \(\) & Avg. Len. \(\) \\ 
1 & 29.5 & 327 & 2.80 & 42.8 & 46.6 & 2.24 \\
2 & 32.1 & 345 & 1.29 & 48.1 & 51.2 & 2.82 \\
3 & 38.4 & 376 & **0.89** & **51.5** & **58.2** & **3.42** \\
4 & 51.9 & 422 & 1.20 & 48.2 & 53.1 & 3.03 \\   

Table 3: **Effect of Frame Sampling Interval.** Setting the frame sampling interval to 3 is effective.

Figure 4: Efficiency comparison between two types of training.