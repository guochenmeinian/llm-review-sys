# Structure Consistent Gaussian Splatting with

Matching Prior for Few-shot Novel View Synthesis

Rui Peng1,2 Wangze Xu1 Luyang Tang1,2 Liwei Liao1 Jianbo Jiao3 Ronggang Wang\({}^{ 1,2}\)

###### Abstract

Despite the substantial progress of novel view synthesis, existing methods, either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian Splatting (3DGS), suffer significant degradation when the input becomes sparse. Numerous efforts have been introduced to alleviate this problem, but they still struggle to synthesize satisfactory results efficiently, especially in the large scene. In this paper, we propose _SCGaussian_, a Structure Consistent Gaussian Splatting method using matching priors to learn 3D consistent scene structure. Considering the high interdependence of Gaussian attributes, we optimize the scene structure in two folds: rendering geometry and, more importantly, the position of Gaussian primitives, which is hard to be directly constrained in the vanilla 3DGS due to the non-structure property. To achieve this, we present a hybrid Gaussian representation. Besides the ordinary non-structure Gaussian primitives, our model also consists of ray-based Gaussian primitives that are bound to matching rays and whose optimization of their positions is restricted along the ray. Thus, we can utilize the matching correspondence to directly enforce the position of these Gaussian primitives to converge to the surface points where rays intersect. Extensive experiments on forward-facing, surrounding, and complex large scenes show the effectiveness of our approach with state-of-the-art performance and high efficiency. Code is available at https://github.com/prstrive/SCGaussian.

## 1 Introduction

Few-shot novel view synthesis (NVS) aims to reconstruct the scene given only a sparse collection of views, which has always been a cornerstone and challenging task in computer vision. Neural radiance field (NeRF) , emerged as an excelled 3D representation, has shown great success in rendering photo-realistic novel views. However, such impressive results require an expensive and time-consuming collection of dense images which impedes many practical applications, _e.g._, the input is typically much sparser in autonomous driving, robotics, and virtual reality. Although many attempts have been proposed to solve this challenging few-shot rendering problem from the aspect of pre-training , regularization terms , external priors , _etc._, these NeRF-based methods still suffer from low rendering speed and high computational cost, _i.e._, each scene requires hours or even days of training time.

Recently, an efficient representation 3D Gaussian Splatting (3DGS)  is proposed to leverage a set of Gaussian primitives (initialized from the Structure-from-motion (SFM)  points) along with some attributes to explicitly model the 3D scene. Through replacing the cumbersome volume rendering in NeRF methods with the efficient differentiable splatting, which directly projects the Gaussian primitives onto the 2D image plane, 3DGS has expressed remarkable improvement in bothrendering quality and speed, _i.e._, high-resolution images can be rendered in real-time. Even with this unprecedented performance, 3DGS still relies on dense image captures and faces the same problem of novel view degeneration with NeRF methods, when only a few inputs are available.

In this paper, we aim to address this issue by establishing a few-shot 3DGS model with a consistent structure to pursue high-quality and efficient novel view synthesis. Compared with the dense counterpart, this few-shot system introduces more challenging problems, _e.g._, the trivial multi-view constraints that the model can only be supervised from sparse viewpoints, and the high interdependence between Gaussian attributes make their optimization ambiguity, _i.e._, optimizing the position _vs_ optimizing the shape. Although some recent efforts [23; 65] have attempted to use monocular depth priors [36; 37] to stabilize the optimization, _e.g._, the monocular depth consistency of sampled virtual viewpoints  and the hard-soft monocular depth regularization , as shown in Fig. 1, the inherent scale and multi-view inconsistency of monocular depth make it hard to guarantee a consistent scene structure and lead to unsatisfactory rendering results, especially in complex scenes.

To this end, we are motivated to exploit the matching prior, which exhibits worthwhile characteristics indicating the ray/pixel correspondence between views and the multi-view visible region. Based on this, we propose _SCGaussian_, a framework that leverages matching priors to explicitly enforce the optimization of scene structure to be 3D consistent. A straightforward idea for this purpose is to use the ray correspondence to supervise the reprojection error of the rendering depth. However, we observe that the rendering geometry is not always consistent with the scene structure due to the interdependence of Gaussian attributes. In this paper, we argue that in addition to the rendering geometry, the more important aspect to ensure the consistency of the scene structure is to optimize the _position of Gaussian primitives_. To achieve this, we design a hybrid representation, which consists of ray-based Gaussian primitives besides the ordinary non-structure Gaussian primitives. For these rays-based ones, we bind them to matching rays and restrict the optimization of their position along the ray, thus we can utilize the matching correspondence to optimize the position of Gaussian primitives to converge to the consistent surface position where rays intersect. In this dual optimization solution, both the position and shape of the Gaussian primitives can be constrained properly.

Extensive experiments on LLFF , IBRNet , DTU , Tanks and Temples  and Blender  datasets show the effectiveness of our SCGaussian, which is capable of synthesizing detail and accurate novel views in these forward-facing, surrounding, and complex large scenes, achieving new state-of-the-art performance in both rendering quality (\(}\) improvement on challenging complex scenes ) and efficiency (\(\)**200 FPS** rendering and 1-minute convergence speed).

## 2 Related Works

**Novel view synthesis.** Novel view synthesis is a task to render realistic images of unseen views given a set of training images. Many methods are proposed to address this problem in both traditional [7; 4; 45; 22; 55] and deep-learning based [64; 63; 11; 10] manners. In particular, NeRF  achieves photo-realistic rendering and has become one of the most popular methods in recent years, which

Figure 1: **Comparisons in view synthesis and geometry rendering. 3DGS  can synthesize high-quality novel views and plausible geometry with excessive inputs, but suffers from significant degradation in the sparse scenario. Even using the monocular depth prior, DNGaussian  still struggles to generate accurate geometry and novel views. In contrast, our method can learn the more consistent scene structure and render the more realistic images.**

successfully combines multi-layer perceptrons (MLP) and volume rendering. The following works try to improve NeRF in many aspects, _e.g._, quality [1; 2], pose-free [54; 28; 25; 50], dynamic view synthesis [35; 12; 24; 33], training and rendering efficiency [14; 31; 13; 47; 5; 26]. And more recently, a point-based method 3D Gaussian Splatting  represents the scene as 3D Gaussians and significantly improve the rendering speed to a real-time level. And it has shown an advantage in many aspects [56; 59; 61; 18; 27; 49; 3] compared with NeRF-based methods. However, these methods need dense input views, which makes them unsuitable for many practical applications.

**Few-shot novel view synthesis.** Compared with the ordinary NVS, the few-shot NVS is a more practical task but also more challenging. The original rendering methods always suffer from dramatic degradation when applied directly to the sparse scenario. Many works have attempted to solve this problem. Specifically, one thread of works [60; 52; 17; 6] attempt to pre-train a generalizable model on the large-scale datasets first and apply it to the target scene with sparse inputs. Another alternate approach is to optimize the model from scratch for each scene. [8; 39] try to add the depth supervision from the SFM points or depth completion model, and [62; 51] adopt the more practical monocular depth prior. To exploit smoothness and semantic priors, works [32; 15] choose to render some patches first and introduce the geometry and appearance regularization. These methods are all based on the NeRF and rely on volume rendering to synthesize novel views, which is always time-consuming. Some recent methods [65; 23; 57] combine the efficient 3DGS representation with monocular depth  and multi-view stereo  prior to improve the efficiency of the few-shot NVS task. However, since 3DGS relies on the initialization of sparse SFM points and adequate multi-view constraints, which are hard to observe in the sparse scenario, how to learn the globally consistent structure is the crucial bottleneck.

## 3 Methodology

In this section, we introduce the proposed new few-shot approach, SCGaussian, which can learn consistent 3D scene structure using matching priors. The overall framework of our model is illustrated in Fig. 2. In Sec. 3.1, we first review the 3DGS. Then we elaborate on the challenge of few-shot 3DGS and the motivation of using matching priors in Sec. 3.2, and the design of our Structure Consistent Gaussian Splatting will be introduced in Sec. 3.3. The full loss function and training detail will be described in Sec. 3.4.

Figure 2: **Framework of SCGaussian. We first extract the matching prior from the sparse input, and randomly initialize the hybrid Gaussian representation. The ray-based Gaussian primitives are bound to matching rays, and are explicitly optimized using the matching correspondence. The rendering geometry optimization is further conducted to optimize the shape of all types of Gaussian primitives. Combined with the ordinary photometric loss, SCGaussian can learn the consistent scene structure.**

### Preliminary of 3D Gaussian Splatting

3DGS  explicitly represents the 3D scene through a collection of anisotropic 3D Gaussians. Each Gaussian is defined by a center vector \(^{3}\) and a covariance matrix \(^{3 3}\), and the influence for a position \(x\) is defined as:

\[G(x)=e^{-(x-)^{T}^{-1}(x-)}.\] (1)

To ensure positive semi-definiteness and effective optimization, the covariance matrix is decomposed into a scaling matrix \(S\) and a rotation matrix \(R\) as:

\[=RSS^{T}R^{T},\] (2)

where these two matrices can be derived by a scaling factor \(s^{3}\) and a rotation factor \(r^{4}\). Additionally, each Gaussian also contains the appearance feature \(sh^{k}\) represented by a set of spherical harmonics (SH) coefficients and an opacity value \(\).

To render the image, the 3DGS is projected to the 2D image plane via a view transformation matrix \(W\) and the Jacobian of the affine approximation of the projective transformation \(J\):

\[^{}=JW W^{T}J^{T}.\] (3)

Using the point-based rendering, the color \(C\) of a pixel can be computed by blending \(N\) ordered Gaussians overlapping the pixel:

\[C=_{i N}c_{i}_{i}^{}_{j=1}^{i-1}(1-_{j}^{ }),\] (4)

where \(c_{i}\) denotes the view-dependent color of \(i\)-th Gaussian, and \(_{i}^{}\) is determined by the multiplication of \(^{}\) and the opacity \(_{i}\). Similarly, we can also render the depth image \(D\) through:

\[D=_{i N}d_{i}_{i}^{}_{j=1}^{i-1}(1-_{j}^{ }),\] (5)

where \(d_{i}\) refers to the depth of \(i\)-th Gaussian.

In summary, each Gaussian point \(\) can be characterized by the set of attributes: \(=\{,s,r,,sh\}\). To optimize the model, 3DGS takes the photometric loss, which is measured with the combination of L1 and SSIM , between the rendering image \(\) and the ground-truth image \(I\):

\[L_{photo}=(1-)L_{1}(,I)+ L_{ssim}(,I),\] (6)

where \(\) is always set to 0.2. Facilitated by the highly-optimized rasterization pipeline, 3DGS can achieve remarkably fast rendering and training speed and enables real-time view synthesizing.

### Motivation of Matching Priors

Through mimicking the image-formation process at training views, the model aims to find a set of optimal Gaussians \(=\{_{i}\}_{i=1}^{N}\) to build a photo-realistic mapping function \(f_{}:P\), _i.e._, mapping the image for arbitrary camera pose \(P\):

\[=*{arg\,min}_{}L_{photo}().\] (7)

With adequate training views, the optimized model is capable of generating great novel view rendering results. However, as shown in Fig. 3, when the input becomes sparse, the 3DGS model always overfits training views and suffers from a significant degradation in test poses.

We observe that the challenge of the few-shot 3DGS mainly comes from the failure of learning the 3D consistent scene structure, _e.g._, the learned Gaussian primitives cannot distribute over the accurate surface region and the rendering geometry is multi-view inconsistent. In the sparse scenario, the supervision signal only comes from a few training poses, and this trivial multi-view constraint makes it hard to bias the model towards learning a 3D consistent solution. Conversely, as shown in Fig. 3 (a), 3DGS model tends to learn the inconsistent Gaussians for each view separately, _e.g._,the model learns a "wall" extremely close to each camera, in which case the training loss is still small. Furthermore, we find that 3DGS has an obvious optimization ambiguity due to the high interdependence of attributes, _e.g._, shape versus position. Theoretically, the model needs to learn more small-sized Gaussian primitives over the texture region to recover high-frequency details, but in practice, it prefers to increase the size of Gaussian primitives to cover these pixels as shown in Fig. 3 (b), resulting in an overly smooth view synthesis. To ensure that the learned structure is consistent, a heuristic strategy in the vanilla 3DGS is to use sparse SFM points as initialization and guide the model's optimization, which is especially crucial for complex scenes. However, in the sparse scenario, it's pretty hard to stably extract enough SFM points, and usually, only random initialization can be used like . This amplifies the challenge of learning the consistent structure.

Although some methods  attempt to use the monocular depth to regularize the geometry, the inherent scale and multi-view ambiguity of monocular depth make it difficult to solve the aforementioned problems. Thus, we are interested in the question: _how can we make 3DGS without SFM point initialization to learn 3D consistent scene structure under sparse input?_ In this paper, we consider exploiting matching priors using the pre-trained matching model , which doesn't face the ambiguity problem like monocular depth. Matching priors have two important characteristics: ray correspondence and ray position.

**Ray correspondence.** The pair of matching rays represent the corresponding 2D position of a consistent 3D point in different views, which can serve as the prominent multi-view constraint, _i.e_., the matching rays should theoretically intersect at the same surface position. Given a pair of matching rays \(\{r_{i},r_{j}\}\) at image \(I_{i}\) and \(I_{j}\), and the corresponding pixel coordinates are \(\{p_{i},p_{j}\}\), supposing we have computed the position of the surface point intersect with each ray as \(X_{i}\) and \(X_{j}\), we can get the following equation:

\[X_{i}=X_{j}.\] (8)

Meanwhile, given the camera intrinsics \(\{K_{i},K_{j}\}\) and extrinsics \(\{[R_{i},t_{i}],[R_{j},t_{j}]\}\), we can further project the surface point to another 2D image plane and get the projected pixel coordinate, _e.g._, the projection from \(i\) to \(j\) can be modeled as:

\[p_{i j}(X_{i})=(K_{j}R_{j}^{T}(X_{i}-t_{j})),\] (9)

where \(\) is the projection operator \(([x,y,z]^{T})=[x/z,y/z]^{T}\). Thus we have the equation in pixel coordinate: \(p_{j}=p_{i j}\), and similarly, we have \(p_{i}=p_{j i}\).

Figure 3: **Visualization of some challenges faced by few-shot 3DGS. (a) The expected Gaussian in the surface region cannot be learned, and the model tends to learn the inconsistent Gaussian and overfit the training views. While the training loss is small enough, the testing error is pretty bad. (b) The attributes of Gaussian primitives are interdependent and the model tends to increase the size to cover the pixels rather than correct the position.**

**Ray position.** In the matching prior, the position of matching rays exactly indicates the region that is commonly visible to at least two views. This multi-view visible region plays a crucial role in the reconstruction model, as it's meaningless when there is no overlapping region between views. In the sparse scenario, the stereo correspondence is insignificant and the non-overlapping region can even harm the model training, while the importance of the multi-view visible region is magnified.

### Structure Consistent Gaussian Splatting

To fully exploit the characteristics of matching prior, our SCGaussian explicitly optimizes the scene structure in two folds: the position of Gaussian primitive and the rendering geometry. Optimizing the position of Gaussian primitive is non-trivial due to the non-structural properties of Gaussian primitives. To address this, we present a hybrid Gaussian representation. Besides ordinary non-structure Gaussian primitives used to recover the background region visible in a single view, our model also consists of ray-based Gaussian primitives which are bound to matching rays, in which case their positions are restricted to be optimized along the ray.

**Initialization and densification.** Different from existing methods that initialize with either SFM points  or random points , we initialize with ray-based Gaussian primitives and bind them to matching rays. For convenience, here we discuss two input images \(I_{i}\) and \(I_{j}\). Suppose we have \(N\) pairs of matching rays \(\{r_{i}^{k},r_{j}^{k}\}_{k=1}^{N}\), we can initialize \(N\) pairs of ray-based Gaussian primitives \(\{_{i}^{k},_{j}^{k}\}_{k=1}^{N}\). Similar to 3DGS, each primitive is equipped with a set of learnable attributes but with a different position representation. The position of the ray-based Gaussian primitive \(^{}\) is defined as:

\[^{}=o+zd,\] (10)

where \(o\) and \(d\) refer to the camera center and ray direction respectively, and \(z\) is a learnable distance factor, which is randomly initialized.

For densification, we follow the same strategy in  to determine the "under-reconstruction" candidates using the average magnitude of view-space position gradients, and generate the non-structure Gaussian primitives, whose positions can be optimized in arbitrary directions.

**Optimize the position of Gaussian primitives.** As analyzed in Sec. 3.2, the accurate position of Gaussian primitives plays a fundamental role in the learned scene structure. Since the matching correspondence between ray-based Gaussian primitives can be constructed using the binding strategy, we can conveniently optimize their positions.

For a pair of matching rays \(\{r_{i},r_{j}\}\) in image \(I_{i}\) and \(I_{j}\), thanks to our binding strategy, we can get a pair of binding Gaussian primitives \(\{_{i},_{j}\}\), and their positions in 3D space are \(_{i}^{}=o_{i}+z_{i}d_{i}\) and \(_{j}^{}=o_{j}+z_{j}d_{j}\) respectively. According to Eq. (8) and Eq. (9), we can get the projected 2D coordinate from \(i\) to \(j\): \(p_{i j}(_{i}^{})\) and from \(j\) to \(i\): \(p_{j i}(_{j}^{})\). Thus we can get the projection error of this pair of Gaussian primitives as:

\[\{L_{gp}^{i j}=\|p_{j}-p_{i j}(_{i}^{ })\|\\ L_{gp}^{j i}=\|p_{i}-p_{j i}(_{j}^{})\|..\] (11)

The final Gaussian position loss \(L_{gp}\) is computed as the average error of all binding Gaussian pairs.

**Optimize the rendering geometry.** Due to the interdependence of Gaussian attributes, the rendering geometry is not consistent with Gaussian positions, _e.g._, the incorrect scaling or rotation can lead to wrong rendering geometry and affect the rendering results even with the correct Gaussian position.

We first render the depth image \(D_{i}\) and \(D_{j}\) through Eq. (5) and get the estimated depth for the pair of matching rays \(\{D_{i}(p_{i}),D_{j}(p_{j})\}\). Then we lift the pixel coordinate to 3D space:

\[_{i}=R_{i}(D_{i}(p_{i})K_{i}^{-1}_{i}))+t_{i},\] (12)

where \(\) refers to the 2D homogeneous of \(p\). Similarly, we can get the 3D position \(_{j}\) of ray \(j\). Then we get the projected 2D coordinate \(p_{i j}(_{i}^{})\) and \(p_{j i}(_{j}^{})\) according to Eq. (9) as mentioned above and compute the projection error based on the rendering depth as:

\[\{L_{rg}^{i j}=\|p_{j}-p_{i j}(_{i}^{ })\|\\ L_{rg}^{j i}=\|p_{i}-p_{j i}(_{j}^{})\|,.\] (13)

and we take the average error of all ray pairs as the final rendering geometry loss \(L_{rg}\).

### Overall pipeline

**Loss function.** Our loss function consists of three parts: the ordinary photometric loss \(L_{photo}\), the Gaussian position loss \(L_{gp}\), the rendering geometry loss \(L_{rg}\), and the full function is defined as:

\[L=L_{photo}+ L_{gp}+ L_{rg}.\] (14)

**Training details.** During training, we set \(=1.0\). To avoid the model falling into sub-optimization in the early stage of training, we set \(=0\) and then increase it to \(=0.3\) after 1k iterations. To ensure that the Gaussian primitive converges to the optimal position, we use a caching strategy in the first 1k iterations, _i.e_., cache the position with the minimum Gaussian position loss \(L_{gp}\) at each iteration. Meanwhile, considering there are some mismatched ray pairs in the matching prior, we further filter out those primitives with large Gaussian position loss \(L_{gp}>\). During optimization, the ray-based primitive will not be pruned. We build our model based on the official 3DGS codebase, and train the model for 3k iterations with the same setting as 3DGS but set the learning rate of the learnable distance factor \(z\) to 0.1 at the beginning and decrease to \(1.6 10^{-6}\).

## 4 Experiments

In this section, we demonstrate the performance of our model in popular datasets and conduct ablation studies to verify the effectiveness of our designs. Next, we first describe the common datasets and the selected baselines for comparison, then analyze the results.

**Datasets & metrics.** We evaluate our model on forward-facing, complex large-scale and surrounding datasets under the sparse setting: LLFF , IBRNet , Tanks and Temples (T&T) , DTU  and NeRF Blender Synthetic dataset (Blender) . LLFF dataset contains 8 real scenes, and following previous methods [32; 51], every 8-th images are held out for testing, and sparse views are evenly sampled from the remaining images for training. IBRNet dataset is also a real forward-facing dataset, and we select 9 scenes for evaluation and adopt the same split as in LLFF. T&T is a large-scale dataset collected from more complex realistic environments containing both indoor and outdoor scenes, and we use 8 scenes for evaluation and also apply the same split as in LLFF. DTU is an object-centric dataset, which contains more texture-poor scenes. We use the same evaluation strategy as  on DTU. For Blender, containing 8 object-centric synthetic scenes, we follow  to train with 8 images and test on 25 images. We report PSNR, SSIM, and LPIPS scores to measure our reconstruction quality and also report the geometric average (AVG) of \(=10^{-/10}\), \(}\) and LPIPS as in .

**Baselines.** We compare our model against both NeRF-based and 3DGS-based few-shot NVS methods. For NeRF-based methods, we compare with methods with relatively high performance, including MipNeRF , DietNeRF , RegNeRF , FreeNeRF  and SparseNeRF . For 3DGS-based methods, we compare with the vanilla 3DGS and its recent few-shot follow-ups like FSGS  and DNGaussian .

### Results

**Results on LLFF and IBRNet.** We use the aforementioned split method to sample 3 images for training. The quantitative comparisons on two datasets with recent SOTA methods are summarized in Tab. 1. Although 3DGS-based methods natively have a weakness in invisible areas due to their

    &  &  &  \\  & & PSNR \(\) & SSIM \(\) & LPIPS \(\) & AVG \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & AVG \(\) \\  Mip-NetRF  &  & 14.62 & 0.351 & 0.495 & 0.246 & 15.83 & 0.406 & 0.488 & 0.223 \\ RegNeRF  & & 19.08 & 0.587 & 0.336 & 0.149 & 19.05 & 0.542 & 0.377 & 0.152 \\ FreeNeRF  & & 19.63 & 0.612 & 0.308 & 0.134 & 19.76 & 0.588 & 0.333 & 0.135 \\ SparseNeRF  & & 19.86 & 0.624 & 0.328 & 0.127 & 19.90 & 0.593 & 0.364 & 0.137 \\ 
3DGS  &  & 16.46 & 0.440 & 0.401 & 0.192 & 17.79 & 0.538 & 0.377 & 0.166 \\ FFSGS  & & 20.43 & 0.682 & 0.248 & - & 19.84 & 0.648 & 0.306 & 0.130 \\ DNGaussian  & & 19.12 & 0.591 & 0.294 & 0.132 & 19.01 & 0.616 & 0.374 & 0.151 \\
**SCGaussian (Ours)** & & **20.77** & **0.705** & **0.218** & **0.105** & **21.59** & **0.731** & **0.233** & **0.097** \\   

Table 1: **Quantitative comparisons on the LLFF and IBRNet datasets with 3 training views. Best results are in **bold**. We run our method 5 times and report the error bar in the appendix.

[MISSING_PAGE_EMPTY:8]

[MISSING_PAGE_FAIL:9]

consistently outperform the SOTA method , and the advantage becomes more significant as the number of views increases.

**Triangulation initialization.** To prove the effectiveness of our optimization strategy, we perform some comparisons with methods that directly use the triangulation points of matched pixels for initialization. The results are shown in Tab. 5, which indicate that using the triangulation initialization can improve the performance of the baseline especially equipped with more structured ScaffoldGS  or OctreeGS  models. Even though, our model still achieves the best performance and demonstrates the effectiveness of our model.

**Robustness to matching models.** We perform more experiments in Tab. 7 to verify the robustness of our model to different pre-trained matching models. Concretely, we use the same optimization and testing configuration for all models, and additionally use the DKM , LoFTR  and SuperGlue  models to extract the matching prior. The results in Tab. 7 show that all these matching models can bring a satisfactory improvement to the baseline, and our method can even achieve better performance when using weaker matching models (_e.g._, GIM _vs._ LoFTR). These results prove that our strategy is robust to different matching models.

**Efficiency.** With a single NVIDIA RTX 3090 GPU, the training of our method consumes about 3GB memories and converges within 1 minute on LLFF 3-view setting, which is much faster than existing methods, _e.g._, [58; 51] need about 10 hours and  needs about 10 minutes. Our method also achieves a real-time inference speed of over 200FPS at \(504 378\) resolution, superior to NeRF-based methods (_e.g._,  at 0.04FPS) and comparable to 3DGS-based methods (_e.g._,  at 181FPS).

**Limitation.** Following the common pipeline in the research field of few-shot NVS, our model requires an accurate camera pose, which may not always be available. Thus liberating this limitation could further improve our work to be more practical, and we will investigate this in future work.

## 5 Conclusion

In this paper, we observed the main challenge of few-shot 3DGS is learning the 3D consistent scene structure, and we exploited the matching prior to construct a Structure Consistent Gaussian Splatting method named _SCGaussian_. Due to the optimization ambiguity of Gaussian attributes between the position and shape, we presented two approaches to optimize the scene structure: explicitly optimize the rendering geometry and the position of Gaussian primitives. While directly constraining the position is non-trivial in the vanilla 3DGS, we introduced a hybrid Gaussian representation, consisting of ordinary non-structure Gaussian primitives and ray-based Gaussian primitives. In this way, both the position and shape of Gaussian primitives can be optimized to be 3D consistent. To evaluate our method as comprehensively as possible, we conducted experiments on forward-facing, complex large-scale, and surrounding datasets. The results consistently demonstrate that our method achieves new state-of-the-art performance while being highly efficient.

   Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & AVG \(\) \\  Ours + GIM & 20.77 & 0.705 & 0.218 & 0.105 \\ Ours + DKM & 20.92 & 0.732 & 0.189 & 0.099 \\ Ours + LoFTR & **20.94** & **0.737** & **0.182** & **0.097** \\ Ours + SuperGlue & 20.25 & 0.689 & 0.221 & 0.110 \\   

Table 7: **Robustness to different matching models, conducted on the LLFF dataset.**

Figure 7: **Qualitative comparisons on DTU dataset with 3 training views.**