# Segment Any Point Cloud Sequences by Distilling Vision Foundation Models

Youquan Liu\({}^{1,}\) Lingdong Kong\({}^{1,2,}\)1 Jun Cen\({}^{3}\) Runnan Chen\({}^{4}\) Wenwei Zhang\({}^{1,5}\)

 Liang Pan\({}^{5}\) Kai Chen\({}^{1}\) Ziwei Liu\({}^{5,}\)

\({}^{1}\)Shanghai AI Laboratory \({}^{2}\)National University of Singapore

\({}^{3}\)The Hong Kong University of Science and Technology \({}^{4}\)The University of Hong Kong

\({}^{5}\)S-Lab, Nanyang Technological University

{liuyouquan,konglingdong,zhangwenwei,chenkai}@pjlab.org.cn

jcenaa@connect.ust.hk {liang.pan,ziwei.liu}@ntu.edu.sg

Youquan and Lingdong contributed equally to this work. \(\) Ziwei serves as the corresponding author.

###### Abstract

Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce _Seal_, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: _i) Scalability_: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. _ii) Consistency_: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. _iii) Generalizability:_ Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable \(45.0\%\) mIoU on nuScenes after linear probing, surpassing random initialization by \(36.9\%\) mIoU and outperforming prior arts by \(6.1\%\) mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across \(20\) different few-shot fine-tuning tasks on all eleven tested point cloud datasets. The code is available at this link2.

## 1 Introduction

Inspired by the achievements of large language models (LLMs) , a wave of vision foundation models (VFMs), such as SAM , X-Decoder , and SEEM , has emerged. These VFMs are revolutionizing the field of computer vision by facilitating the acquisition of pixel-level semantics with greater ease. However, limited studies have been conducted on developing VFMs for the 3D domain. To bridge this gap, it holds great promise to explore the adaptation or extension of existing 2D VFMs for 3D perception tasks.

As an important 3D perception task, accurately segmenting the surrounding points captured by onboard LiDAR sensors is crucial for ensuring the safe operation of autonomous vehicles . However, existing point cloud segmentation models rely heavily on large annotated datasets for training, which poses challenges due to the labor-intensive nature of point cloud labeling . To address this issue, recent works have explored semi-  or weakly-supervised  approaches to alleviate the annotation burden. While these approaches show promise, the trained point segmentors tend to perform well only on data within their distribution, primarily due tosignificant configuration differences (such as beam number, camera angle, emit rate) among different sensors . This limitation inevitably hampers the scalability of point cloud segmentation.

To address the aforementioned challenges, we aim to develop a framework that can learn informative features while addressing the following objectives: _i)_ Utilizing raw point clouds as input, thereby eliminating the need for semi or weak labels and reducing annotation costs. _ii)_ Leveraging spatial and temporal cues inherent in driving scenes to enhance representation learning. _iii)_ Ensuring generalizability to diverse downstream point clouds, beyond those used in the pretraining phase. Drawing inspiration from recent advancements in cross-modal representation learning  and building upon the success of VFMs , we propose a methodology that distills semantically-rich knowledge from VFMs to support self-supervised representation learning on challenging automotive point clouds. Our core idea is to leverage the 2D-3D correspondence in-between LiDAR and camera sensors and construct high-quality contrastive samples for cross-modal representation learning. As shown in Fig. 1, VFMs are capable of generating semantic superpixel groups from the camera views3 and provide off-the-shelf semantic coherence for distinct objects and backgrounds in the 3D scene. Such consistency can be distilled from 2D to 3D and further yields promising performance on downstream tasks, which we will revisit more formally in later sections.

Compared to previous framework , our VFM-assisted contrastive learning has three notable advantages: _i)_ The semantically-aware region partition mitigates the severe "self-conflict" problem in driving scene contrastive learning. _ii)_ The high-quality contrastive samples gradually form a more coherent optimization landscape, yielding a faster convergence rate than previous configurations. _iii)_ A huge reduction in the number of superpixels also extenuates the overhead during pretraining.

Since a perfect calibration between LiDAR and cameras is often hard to meet, we further design a superpoint temporal consistency regularization to mitigate the case that there are potential errors from the sensor synchronization aspect. This objective directly resorts to the accurate geometric information from the point cloud and thus improves the overall resilience and reliability. As will be shown in the following sections, our framework (dubbed _Seal_) is capable of segmentation any point cloud sequences across a wide range of downstream task on different automotive point cloud datasets.

To summarize, this work has the following key contributions:

* To the best of our knowledge, this study represents the first attempt at utilizing 2D vision foundation models for self-supervised representation learning on large-scale 3D point clouds.

Figure 1: The proposed _Seal_ distills semantic awareness on cameras views from VFMs to the point cloud via superpixel-driven contrastive learning. **[1st row]** Semantic superpixels generated by SLIC  and recent VFMs , where each color represents one segment. **[2nd row]** Semantic superpoints grouped by projecting superpixels to 3D via camera-LiDAR correspondence. **[3rd row]** Visualizations of the linear probing results of our framework driven by SLIC and different VFMs.

* We introduce \(\), a scalable, consistent, and generalizable framework designed to capture semantic-aware spatial and temporal consistency, enabling the extraction of informative features from automotive point cloud sequences.
* Our approach demonstrates clear superiority over previous state-of-the-art (SoTA) methods in both linear probing and fine-tuning for downstream tasks across 11 different point cloud datasets with diverse data configurations.

## 2 Related Work

**Vision Foundation Models**. Recent excitement about building powerful visual perception systems based on massive amounts of training data [80; 50] or advanced self-supervised learning techniques [8; 74] is revolutionizing the community. The segment anything model (SAM)  sparks a new trend of general-purpose image segmentation which exhibits promising zero-shot transfer capability on diverse downstream tasks. Concurrent works to SAM, such as X-Decoder , OpenSeeD , SegGPT , and SEEM , also shed lights on the direct use of VFMs for handling different kinds of image-related tasks. In this work, we extend this aspect by exploring the potential of VFMs for point cloud segmentation. We design a new framework taking into consideration the off-the-shelf semantic awareness of VFMs to construct better spatial and temporal cues for representation learning.

**Point Cloud Segmentation**. Densely perceiving the 3D surroundings is important for autonomous vehicles [94; 41]. Various point cloud segmenters have been proposed, including methods based on raw points [42; 90; 43; 114; 78], range view [69; 106; 93; 118; 20; 16; 52], bird's eye view [115; 119; 9], voxels [19; 120; 37; 36], and multi-view fusion [62; 89; 107; 121; 17; 79]. Despite the promising results achieved, existing 3D segmentation models rely on large sets of annotated data for training, which hinders the scalability . Recent efforts seek semi [55; 56; 59], weak [95; 40; 86; 63; 60], and active [64; 44; 117] supervisions or domain adaptation techniques [48; 47; 54; 76; 82; 68] to ease the annotation cost. In this work, we resort to self-supervised learning by distilling foundation models using camera-to-LiDAR associations, where no annotation is required during the pretraining stage.

**3D Representation Learning**. Stemmed from the image vision community, most 3D self-supervised learning methods focused on object-centric point clouds [84; 77; 83; 14; 97; 45; 110; 29; 91] or indoor scenes [38; 15; 22; 108; 10; 39; 57; 58; 109] by either pretext task learning [21; 70; 71; 112; 30], contrastive learning [11; 32; 12; 34; 27; 13; 46; 98; 104] or mask modeling [105; 31; 24; 61], where the scale and diversity are much lower than the outdoor driving scenes [67; 6]. PointContrast , DepthContrast , and SegContrast  are prior attempts aimed to establish contrastive objectives on point clouds. Recently, Sautier _et al._ proposed the first 2D-to-3D representation distillation method called SLidR for cross-modal self-supervised learning on large-scale point clouds and exhibits promising performance. The follow-up work  further improves this pipeline with a semantically tolerant contrastive constraint and a class-balancing loss. Our framework also stems from the SLidR paradigm. Differently, we propose to leverage VFMs to establish the cross-modal contrastive objective which better tackles this challenging representation learning task. We also design a superpoint temporal consistency regularization to further enhance feature learning.

## 3 Seal: A Scalable, Consistent, and Generalizable Framework

In this section, we first revisit the 2D-to-3D representation distillation  as preliminaries (Sec. 3.1). We then elaborate on the technical details of our framework, which include the VFM-assisted spatial contrastive learning (Sec. 3.2) and the superpoint temporal consistency regularization (Sec. 3.3).

### Preliminaries

Given a point cloud \(^{t}=\{_{i}^{t},_{i}^{t}|i=1,..,N\}\) consists of \(N\) points collected by a LiDAR acquisition at timestamp \(t\), where \(_{i}^{3}\) denotes the point coordinates and \(_{i}^{L}\) is the feature embedding (intensity, elongation, _etc._), our goal is to transfer the knowledge from an image set \(^{t}=\{\{_{i}^{t},...,_{j}^{t}\}|j=1,...,V\}\) captured by \(V\) synchronized cameras at \(t\) to point cloud \(^{t}\), where \(^{3 H W}\) is a single image with height \(H\) and width \(W\). Prior works [85; 66] achieve this goal by first aggregating image regions that are similar in the RGB space into a superpixel set \(_{}=\{\{_{1},...,_{m}\}|m=1,...,M\}\), via the unsupervised SLIC  algorithm. The corresponding superpoint set \(_{}=\{\{_{1},...,_{m}\}|m=1,...,M\}\) can be obtained by leveraging known sensor calibration parameters to establish correspondence between the points and image pixels. Specifically, since the LiDAR and cameras usually operate at different frequencies [7; 26], we transform each LiDAR point cloud \(_{i}=(x_{i},y_{i},z_{i})\) at timestamp \(t_{l}\) to the pixel \(}_{i}=(u_{i},v_{i})^{2}\) in the image plane at timestamp \(t_{c}\) via the coordinate transformation. The point-to-pixel mapping is as follows:

\[[u_{i},v_{i},1]^{}=}_{K}_{ _{}}}_{_{}}}_{ _{}}}_{_{}}}[x_{i},y_{i},z_{i},1]^{},\] (1)

where symbol \(_{K}\) denotes the camera intrinsic matrix. \(_{_{}}}\), \(_{_{}}}\), \(_{_{}}}\), and \(_{_{}}}\) are the extrinsic matrices for the transformations of ego to camera at \(t_{c}\), global to ego at \(t_{c}\), ego to global at \(t_{l}\), and LiDAR to ego at \(t_{l}\), respectively.

### Semantic Superpixel Spatial Consistency

**Superpixel Generation**. Prior works resort to SLIC  to group visually similar regions in the image into superpixels. This method, however, tends to over-segment semantically coherent areas (as shown in Fig. 1) and inevitably leads to several difficulties for contrastive learning. One of the main impediments is the so-called "self-conflict", where superpixels belonging to the same semantics become negative samples . Although  proposed a semantically-tolerant loss to ease this problem, the lack of high-level semantic understanding still intensifies the implicit hardness-aware property of the contrastive loss. We tackle this challenge by generating semantic superpixels with VFMs. As shown in Fig. 1 and Fig. 4, these VFMs provide semantically-rich superpixels and yield much better representation learning effects in-between near and far points in the LiDAR point cloud.

**VFM-Assisted Contrastive Learning**. Let \(F_{_{p}}:^{N(3+L)}^{N C}\) be a 3D encoder with trainable parameters \(_{p}\), that takes a LiDAR point cloud as input and outputs a \(C\)-dimensional per-point feature. Let \(G_{_{i}}:^{H W 3}^{}{}C}\) be an image encoder with parameters \(_{i}\), which is initialized from a set of 2D self-supervised pretrained parameters. The goal of our VFM-assisted contrastive learning is to transfer the knowledge from the pretrained 2D network to the 3D network via contrastive loss at the semantic superpixel level. To compute this VFM-assisted contrastive loss, we first build trainable projection heads \(H_{_{p}}\) and \(H_{_{i}}\) which map the 3D point features and 2D image features into the same \(D\)-dimensional embedding space. The point projection head \(H_{_{p}}:^{N C}^{N D}\) is a linear layer with \(_{2}\)-normalization. The image projection head

Figure 2: Overview of the _Seal_ framework. We generate, for each {LiDAR, camera} pair \(\{^{t},^{t}\}\) at timestamp \(t\) and another LiDAR frame \(^{t+n}\) at timestamp \(t+n\), the semantic superpixel and superpoint by vision foundation models (VFMs). Two pertaining objectives are then formed, including spatial contrastive learning between paired LiDAR and camera features (Sec. 3.2) and temporal consistency regularization between point segments at different timestamps (Sec. 3.3).

\(H_{_{i}}:^{ C} ^{ D}\) is a convolution layer with a kernel size of \(1\), followed by a fixed bilinear interpolation layer with a ratio of \(4\) in the spatial dimension, and outputs with \(_{2}\)-normalization.

We distill the knowledge from the 2D network into the 3D network which is in favor of a solution where a semantic superpoint feature has a strong correlation with its corresponding semantic superpixel feature than any other features. Concretely, the superpixels \(_{}\) are used to group pixel embedding and point embedding features. Then, an average pooling function is applied to each grouped point and pixel embedding features, to extract the superpixel embedding features \(^{M D}\) and superpoint embedding features \(^{M D}\). The VFM-assisted contrastive loss is formulated as:

\[^{vfm}=(,)=- _{i=1}^{M}_{i},_{i})/)}}{ _{j i}e^{((_{i},_{j})/)}+e^{((_{i}, _{i})/)}},\] (2)

where \(_{i},_{j}\) denotes the scalar product between superpoint embedding features and superpixel embedding features to measure the similarity. \(\) is the temperature term.

**Role in Our Framework**. Our VFM-assisted contrastive objective exhibits superiority over previous methods from three aspects: _i)_ The semantically-rich superpixels provided by VFMs mitigate the "self-conflict" problem in existing approaches. _ii)_ As we will show in the following sections, the high-quality contrastive samples from VFMs gradually form a much more coherent optimization landscape and yield a faster convergence rate than the unsupervised superpixel generation method. _iii)_ Using superpixels generated by VFMs also helps our framework run faster than previous works, since the embedding length of \(\) and \(\) has been reduced from a few hundred (SLIC ) to dozens (ours).

### Semantic Superpoint Temporal Consistency

The assumption of having perfectly synchronized LiDAR and camera data might become too ideal and cannot always be fulfilled in actual deployment, which limits the scalability. In this work, we resort to accurate geometric information from point clouds to further relieve this constraint.

**Implicit Geometric Clustering**. To group coarse instance segments in a LiDAR frame, we first partition non-ground plane points \(^{t}\) by eliminating the ground plane of a point cloud \(^{t}\) at timestamp \(t\) in an unsupervised manner via RANSAC . Then, we group \(^{t}\) to yield a set of \(M_{k}\) segments \(^{t}=\{^{t}_{1},...,^{t}_{M_{k}}\}\) with the help of HDBSCAN . To map different segment views at different timestamps, we transform those LiDAR frames across different timestamps to the global frame and aggregate them with concatenations. The aggregated point cloud is denoted as \(}=\{}^{t},...,}^{t+n}\}\). Similarly, we generate non-ground plane \(}=\{}^{t},...,}^{t+n}\}\) from \(}\) via RANSAC . In the same manner as the single scan, we group \(}\) to obtain \(M_{k}\) segments \(}=\{}_{1},...,}_{M_{k}}\}\). To generate the segment masks for all \(n+1\) scans at \(n\) consecutive timestamps, _i.e._, \(}=\{}^{t},...,}^{t+n}\}\), we maintain the point index mapping from the aggregated point cloud \(}\) to the \(n+1\) individual scans.

**Superpoint Temporal Consistency**. We leverage the clustered segments to compute the temporal consistency loss among related semantic superpoints. Here we assume \(n=1\) (_i.e._ the next frame) without loss of generalizability. Specifically, given a sampled temporal pair \(}^{t}\) and \(}^{t+1}\) and their corresponding segments \(}^{t}\) and \(}^{t+1}\), we compute the point-wise features \(}^{t}^{N D}\) and \(}^{t+1}^{N D}\) from the point projection head \(H_{_{p}}\). As for the target embedding, we split the point features \(}^{t}\) and \(}^{t+1}\) into \(M_{k}\) groups by segments \(}^{t}\) and \(}^{t+1}\). Then, we apply an average pooling operation on \(}^{t+1}\) to get \(M_{k}\) target mean feature vectors \(^{t+1}=\{^{t+1}_{1},^{t+1}_{2},...,^ {t+1}_{M_{k}}\}\), where \(^{t+1}_{M_{k}}^{1 D}\). Let the split point feature \(}^{t}\) be \(^{t}=\{^{t}_{1},^{t}_{2},...,^{t}_{ M_{k}}\}\), where \(^{t}_{M_{k}}^{k D}\) and

Figure 3: The positive feature correspondences in the contrastive learning objective in our contrastive learning framework. The _circles_ and _triangles_ represent the instance-level and the point-level features, respectively.

\(k\) is the number of points in the corresponding segment. We compute the temporal consistency loss \(^{t t+1}\) to minimize the differences between the point features in the current frame (timestamp \(t\)) and the corresponding segment mean features from the next frame (timestamp \(t+1\)) as follows:

\[^{t t+1}=-}_{i=1}^{M_{k}}_{i}^{t},_{i}^{t+1}>/)}}{_{ j i}e^{(<_{i}^{t},_{j}^{t+1}>/ )}+e^{(<_{i}^{t},_{i}^{t+1}>/ )}}.\] (3)

Since the target embedding for all points within a segment in the current frame serves as the mean segment representation from the next frame, this loss will force points from a segment to converge to a mean representation while separating from other segments, implicitly clustering together points from the same instance. Fig. 3 provides the positive feature correspondence in our contrastive learning framework. Furthermore, we swap \(}^{t}\) when generating the target mean embedding features to form a symmetric representation. In this way, the correspondence is encouraged from both \(t t+1\) and \(t+1 t\), which leads to the following optimization objective: \(^{tmp}=^{t t+1}+^{t+1 t}\).

**Point to Segment Regularization**. To pull close the LiDAR points belonging to the same instance at timestamp \(t\), we minimize the distance between the point feature \(^{t}\) and the corresponding mean cluster feature \(^{t}\). To implement this, we leverage a max-pooling function to pool \(^{t}\) according to the segments to obtain \(^{t}=\{^{t}_{1},^{t}_{2},...,^{t}_ {M_{k}}\}\), where \(^{t}_{M_{k}}^{1 D}\). The point-to-segment regularization is thus achieved via the following loss function:

\[^{p2s}=-N_{k}}_{i=1}^{M_{k}}_{a=1}^{N_{k}} _{a=1}^{N_{k}}_{i}^{t}, _{i,a}^{t}>/)}}{_{j i}e^{(< _{i}^{t},_{j,a}^{t}>/)}+e^{(< _{i}^{t},_{i,a}^{t}>/)}},\] (4)

where \(N_{k}\) represents the number of points within the corresponding segment. The final optimization objective is to minimize the aforementioned semantic spatial consistency loss \(^{vfm}\), temporal consistency loss \(^{tmp}\), and the point-to-segment regularization loss \(^{p2s}\).

**Role in Our Framework**. Our semantic superpoint temporal consistency resorts to the accurate geometric information from the point cloud and exploits the different views of an instance across different timestamps to learn a temporally consistent representation. Considering the worst-case scenario that the 2D-3D correspondence between the LiDAR and camera sensors becomes unreliable, this geometric constraint can still effectively mitigate the potential errors that occur in inaccurate cross-sensor calibration and synchronization. Besides, our point-to-segment regularization mechanism can serve to aggregate the spatial information thus contributing to the effect of better-distinguishing instances in the LiDAR-acquired scene, _e.g._, "car" and "truck". As we will show in the following sections, our experimental results are able to verify the effectiveness and superiority of the proposed consistency regularization objectives, even under certain degrees of perturbation in-between sensors.

Figure 4: The **cosine similarity** between a query point (denoted as the **red dot**) and the feature learned with SLIC  and different VFMs . The queried semantic classes from top to bottom examples are: “car”, “manmade”, and “truck”. The color goes from **violet** to yellow denoting **low** and high similarity scores, respectively. Best viewed in color.

## 4 Experiments

### Settings

**Data**. We verify the effectiveness of our approach on _eleven_ different point cloud datasets. \({}^{1}}\)[7; 26], \({}^{2}}\), and \({}^{3}}\) contain large-scale LiDAR scans collected from real-world driving scenes; while the former adopted a Velodyne HDL32E, data from the latter two datasets are acquired by 64-beam LiDAR sensors. \({}^{4}}\) shares the data with  but are weakly annotated with line scribbles. \({}^{5}}\) is a multimodal dataset collected in an off-road campus environment. \({}^{6}}\) is a small-scale set with an emphasis on dynamic instances. \({}^{7}}\) consist of LiDAR scans from adverse weather conditions. \({}^{8}}\), \({}^{9}}\), and \({}^{10}}\) are synthetic datasets collected from simulators. We also conduct extensive robustness evaluations on the \({}^{11}}\) dataset proposed in the Robo3D benchmark , a comprehensive collection of eight out-of-distribution corruptions that occur in driving scenarios. More details on these datasets are in the Appendix.

**Implementation Details**. We use MinkUNet  as the 3D backbone which takes cylindrical voxels of size \(0.10\)m as inputs. Similar to [85; 66], our 2D backbone is a ResNet-50  pretrained with MoCoV2 . We pretrain our segmentation network for 50 epochs on two GPUs with a batch size of 32, using SGD with momentum and a cosine annealing scheduler. For fine-tuning, we follow the exact same data split, augmentation, and evaluation protocol as SLidR  on _nuScenes_ and _SemanticKITTI_, and adopt a similar procedure on other datasets. The training objective is to minimize a combination of the cross-entropy loss and the Lovasz-Softmax loss . We compare the results of prior arts from their official reporting [85; 66]. Since PPKT  and SLidR  only conducted experiments on _nuScenes_ and _SemanticKITTI_, we reproduce their best-possible performance on the other nine datasets using public code. For additional details, please refer to the Appendix.

**Metrics**. We follow the conventional reporting of Intersection-over-Union (IoU) on each semantic class and the mean IoU (mIoU) across all classes. For robustness probing, we follow the Robo3D protocol  and report the mean Corruption Error (mCE) and mean Resilience Rate (mRR) scores calculated by using the MinkUNet\({}_{18}\) (torchsparse) implemented by Tang _et al._ as the baseline.

### Comparative Study

**Comparison to State-of-the-Arts**. We compare \(\) with random initialization and five existing pretraining approaches under both linear probing (LP) protocol and few-shot fine-tuning settings on _nuScenes_ in Table 1. We observe that the pretraining strategy can effectively improve the accuracy of downstream tasks, especially when the fine-tuning budget is very limited (_e.g._\(1\%\), \(5\%\), and \(10\%\)). Our framework achieves \(44.95\%\) mIoU under the LP setup - a \(4.47\%\) mIoU lead over to the prior art ST-SLidR  and a \(6.15\%\) mIoU boost compared to the baseline SLidR . What is more, \(\) achieves the best scores so far across all downstream fine-tuning tasks, which verifies the superiority of VFM-assisted contrastive learning and spatial-temporal consistency regularization. We also show that recent out-of-context augmentation  could enhance the feature learning during fine-tuning, which establishes a new state of the art on the challenging _nuScenes_ benchmark.

    &  &  & **Waymo** & **Synth4D** \\  & LP & \(1\%\) & \(5\%\) & \(10\%\) & \(25\%\) & Full & \(1\%\) & \(1\%\) & \(1\%\) \\  Random & \(8.10\) & \(30.30\) & \(47.84\) & \(56.15\) & \(65.48\) & \(74.66\) & \(39.50\) & \(39.41\) & \(20.22\) \\ PointContrast [ECCV’20] & \(21.90\) & \(32.50\) & - & - & - & - & \(41.10\) & - & - \\ DepthContrast [ICCV’21] & \(22.10\) & \(31.70\) & - & - & - & - & \(41.50\) & - & - \\ PPKT [arXiv21] & \(35.90\) & \(37.80\) & \(53.74\) & \(60.25\) & \(67.14\) & \(74.52\) & \(44.00\) & \(47.60\) & \(61.10\) \\ SLidR [CVPR’22] & \(38.80\) & \(38.30\) & \(52.49\) & \(59.84\) & \(66.91\) & \(74.79\) & \(44.60\) & \(47.12\) & \(63.10\) \\ ST-SLidR [CVPR’23] & \(40.48\) & \(40.75\) & \(54.69\) & \(60.75\) & \(67.70\) & \(75.14\) & \(44.72\) & \(41.93\) & - \\
**Seal (Ours)** & **44.95** & **45.84** & **55.64** & **62.97** & **68.41** & **75.60** & **46.63** & **49.34** & **64.50** \\ 
**Seal \({}^{}\) (Ours)** & - & \(48.41\) & \(57.84\) & \(65.52\) & \(70.80\) & \(77.13\) & - & - & - \\
**Seal \({}^{}\) (Ours)** & - & \(49.53\) & \(58.64\) & \(66.78\) & \(72.31\) & \(78.28\) & - & - & - \\   

Table 1: Comparisons of different pretraining methods pretrained on _nuScenes_ and fine-tuned on _nuScenes_, _SemanticKITTI_, _Waymo Open_, and _Synth4D_. **LP** denotes linear probing with frozen backbones. Symbol \(\) denotes fine-tuning with the LaserMix augmentation . Symbol \(\) denotes fine-tuning with semi-supervised learning. All mIoU scores are given in percentage (%).

**Downstream Generalization**. To further verify the capability of _Seal_ on segmenting _any_ automotive point clouds, we conduct extensive experiments on eleven different datasets and show the results in Table 1 and Table 2. Note that each of these datasets has a unique data collection protocol and a diverse data distribution - ranging from diverse sensor types and data acquisition environments to scales and fidelity - making this evaluation a comprehensive one. The results show that our framework constantly outperforms prior arts across all downstream tasks on all eleven datasets, which concretely supports the effectiveness and superiority of our proposed approach.

**Semi-Supervised Learning**. Recent research [55; 59] reveals that combining both labeled and unlabeled data for semi-supervised point cloud segmentation can significantly boost the performance on downstream tasks. We follow these works to implement such a learning paradigm where a momentum-updated teacher model is adopted to assign pseudo-labels for the unlabeled data. The results from the last row of Table 1 show that _Seal_ is capable of providing reliable supervision signals for data-efficient learning. Notably, our framework with partial annotation is able to surpass some recent fully-supervised learning methods. More results on this aspect are included in the Appendix.

**Robustness Probing**. It is often of great importance to assess the quality of learned representations on out-of-distribution data, especially for cases that occur in the real-world environment. We resort to the recently established _nuScenes_-\(C\) in the Robo3D benchmark  for such robustness evaluations. From Table 3 we observe that the self-supervised learning methods [65; 85] in general achieve better robustness than their baseline . _Seal_ achieves the best robustness under almost all corruption types and exhibits superiority over other recent segmentation backbones with different LiDAR representations, such as range view , BEV , raw points , and multi-view fusion .

**Qualitative Assessment**. We visualize the predictions of each pertaining method pretrained and fine-tuned on _nuScenes_ in Fig. 5. A clear observation is the substantial enhancement offered by all pretraining methods when juxtaposed with a baseline of random initialization. Diving deeper into the comparison among the trio of tested techniques, _Seal_ stands out, delivering superlative segmentation outcomes, especially in the intricate terrains of driving scenarios. This is credited to the strong spatial and temporal consistency learning that _Seal_ tailored to excite during the pretraining. We have cataloged additional examples in the Appendix for more detailed visual comparisons.

### Ablation Study

**Foundation Model Comparisons**. We provide the first study on adapting VFMs for large-scale point cloud representation learning and show the results in Table 4. We observe that different VFMs exhibit

    &  &  &  &  &  &  \\  & 1\% & 10\% & 1\% & 10\% & Half & Full & Half & Full & 1\% & 10\% & Half & Full \\  Random & \(23.81\) & \(47.60\) & \(38.46\) & \(53.60\) & \(46.26\) & \(54.12\) & \(48.03\) & \(48.15\) & \(19.89\) & \(44.74\) & \(74.32\) & \(79.38\) \\ PPKT  & \(36.50\) & \(51.67\) & \(49.71\) & \(54.33\) & \(50.18\) & \(56.00\) & \(50.92\) & \(54.69\) & \(37.57\) & \(46.48\) & \(78.90\) & \(84.00\) \\ SLidR  & \(39.60\) & \(50.45\) & \(49.75\) & \(54.57\) & \(51.56\) & \(55.36\) & \(52.01\) & \(54.35\) & \(42.05\) & \(47.84\) & \(81.00\) & \(85.40\) \\
**Seal (Ours)** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Comparisons of different pretraining methods pretrained on _nuScenes_ and fine-tuned on different downstream point cloud datasets. All mIoU scores are given in percentage (%).

    & **Initial** & **Backbone** & \(\) & \(\) & **Fog** & **Wet** & **Snow** & **Move** & **Beam** & **Cross** & **Echo** & **Sensor** \\   & PPKT  & MinkUNet & \(183.44\) & \(\) & \(30.65\) & \(35.42\) & \(28.12\) & \(29.21\) & \(32.82\) & \(19.52\) & \(28.01\) & \(20.71\) \\  & SLidR  & MinkUNet & \(179.38\) & \(77.18\) & \(34.88\) & \(38.09\) & \(\) & \(26.44\) & \(33.73\) & \(\) & \(31.54\) & \(21.44\) \\  & **Seal (Ours)** & MinkUNet & \(\) & \(75.38\) & \(\) & \(\) & \(29.93\) & \(\) & \(\) & \(20.31\) & \(\) & \(\) \\   & Random & PolarNet & \(115.09\) & \(76.34\) & \(58.23\) & \(69.91\) & \(64.82\) & \(44.60\) & \(61.91\) & \(40.77\) & \(53.64\) & \(42.01\) \\  & Random & CENet & \(112.79\) & \(76.04\) & \(67.01\) & \(69.87\) & \(61.64\) & \(58.31\) & \(49.97\) & \(\) & \(53.31\) & \(24.78\) \\  & Random & Paffleon & \(106.73\) & \(72.78\) & \(56.07\) & \(73.93\) & \(49.59\) & \(59.46\) & \(65.19\) & \(33.12\) & \(\) & \(44.01\) \\  & Random & Cylinder3D & \(105.56\) & \(78.08\) & \(61.42\) & \(71.02\) & \(58.40\) & \(56.02\) & \(64.15\) & \(45.36\) & \(59.97\) & \(43.03\) \\  & Random & SPVCNN & \(106.65\) & \(74.70\) & \(59.01\) & \(72.46\) & \(41.08\) & \(58.36\) & \(65.36\) & \(36.83\) & \(62.29\) & \(\) \\  & Random & difficult & \(112.20\) & \(72.57\) & \(62.96\) & \(70.65\) & \(55.48\) & \(51.71\) & \(62.01\) & \(31.56\) & \(59.64\) & \(39.41\) \\  & PPKT  & MinkUNet & \(105.64\) & \(76.06\) & \(64.01\) & \(72.18\) & \(59.08\) & \(57.17\) & \(63.88\) & \(36.34\) & \(60.59\) & \(39.57\) \\  & SLidR  & MinkUNet & \(106.08\) & \(75.99\) & \(65.41\) & \(72.31\) & \(56.01\) & \(56.07\) & \(62.87\) & \(41.94\) & \(61.16\) & \(38.90\) \\  & **Seal (Ours)** & MinkUNet & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(57.44\) & \(59.87\) & \(39.85\) \\   

Table 3: Robustness evaluations under eight out-of-distribution corruptions in the _nuScenes_-\(C\) dataset from the Robo3D benchmark . All mCE, mRR, and mIoU scores are given in percentage (%).

diverse abilities in encouraging contrastive objectives. All VFMs show larger gains than SLIC  with both frameworks, while SEEM  in general performs the best. Notably, SAM  tends to generate more fine-grained superpixels and yields better results when fine-tuning with more annotated data. We conjecture that SAM  generally provides more negative samples than the other three VFMs which might be conducive to the superpixel-driven contrastive learning. On all setups, _Seal_ constantly surpasses SLidR  by large margins, which verifies the effectiveness of our framework.

**Cosine Similarity**. We visualize three examples of feature similarity across different VFMs in Fig. 4. We observe that our contrastive objective has already facilitated distinction representations before fine-tuning. The semantically-rich VFMs such as X-Decoder , OpenSeeD , and SEEM  offers overt feature cues for recognizing objects and backgrounds; while the unsupervised (SLIC ) or too fine-grained (SAM ) region partition methods only provide limited semantic awareness. Such behaviors have been reflected in the linear probing and downstream fine-tuning performance (see Table 4), where SEEM  tends to offer better consistency regularization impacts during the cross-sensor representation learning.

**Component Analysis**. Table 5 shows the ablation results of each component in the _Seal_ framework. Specifically, direct integration of VFMs (row #3) or temporal consistency learning (row #2) brings \(4.20\%\) and \(1.65\%\) mIoU gains in LP, respectively, while a combination of them (row #4) leads to a \(5.21\%\) mIoU gain. The point-to-segment regularization (row #5) alone also provides a considerable performance boost of around \(4.55\%\) mIoU. Finally, an integration of all proposed components (row #6) yields our best-performing model, which is \(6.15\%\) mIoU better than the prior art  in LP and also outperforms on all in-distribution and -out-of-distribution downstream setups.

**Sensor Misalignment**. An accurate calibration is crucial for establishing correct correspondences between LiDAR and cameras. In most cases, those sensors should be well-calibrated for an autonomous car. It is unusual if the calibration is completely unknown, but it is possible to be imprecise due to a lack of maintenance. Hence, we conduct the following experiments to validate the robustness of our method. For each point coordinate \(_{i}=(x_{i},y_{i},z_{i})\) in a LiDAR point cloud, its corresponding pixel \(}_{i}=(u_{i},v_{i})\) in the camera view can be found via Eq. 1. To simulate the misalignment between LiDAR and cameras, we insert random noises into the camera extrinsic matrix \(_{K}\), with a relative proportion of \(1\%\), \(5\%\), and \(10\%\). Table 6 shows the results of PPKT , SLidR , and _Seal_ under such noise perturbations. We observe that the possible calibration errors between LiDAR

    &  &  &  & **Waymo** & **SynthD** \\  & & \(\) & \(1\%\) & \(5\%\) & \(10\%\) & \(25\%\) & Full & \(1\%\) & \(1\%\) & \(1\%\) \\  Random & - & \(8.10\) & \(30.30\) & \(47.84\) & \(56.15\) & \(65.48\) & \(74.66\) & \(39.50\) & \(39.41\) & \(20.22\) \\   & SLIC  & \(38.80\) & \(38.30\) & \(52.49\) & \(59.84\) & \(66.91\) & \(74.79\) & \(44.60\) & \(47.12\) & \(63.10\) \\  & SAM  & \(41.49\) & \(43.67\) & \(\) & \(\) & \(\) & \(\) & \(43.35\) & \(48.64\) & \(63.15\) \\  & X-Decoder  & \(41.71\) & \(43.02\) & \(54.24\) & \(61.32\) & \(67.35\) & \(75.11\) & \(45.70\) & \(48.73\) & \(63.21\) \\  & OpenSeeD  & \(42.61\) & \(43.82\) & \(54.17\) & \(61.03\) & \(67.30\) & \(74.85\) & \(\) & \(48.64\) & \(\) \\  & SEEM  & \(\) & \(\) & \(53.03\) & \(60.84\) & \(67.38\) & \(75.21\) & \(45.72\) & \(\) & \(63.13\) \\   & SLIC  & \(40.89\) & \(39.77\) & \(53.33\) & \(61.58\) & \(67.78\) & \(75.32\) & \(45.75\) & \(47.74\) & \(63.37\) \\  & SAM  & \(43.94\) & \(45.09\) & \(\) & \(62.35\) & \(69.08\) & \(\) & \(46.53\) & \(49.00\) & \(63.76\) \\  & X-Decoder  & \(42.64\) & \(44.31\) & \(55.18\) & \(62.03\) & \(68.24\) & \(75.56\) & \(46.02\) & \(49.11\) & \(64.21\) \\  & OpenSeeD  & \(44.67\) & \(44.74\) & \(55.13\) & \(62.36\) & \(69.00\) & \(75.64\) & \(46.13\) & \(48.98\) & \(64.29\) \\  & SEEM  & \(\) & \(\) & \(55.64\) & \(\) & \(68.41\) & \(75.60\) & \(\) & \(\) & \(\) \\   

Table 4: Ablation study on pretraining frameworks (ours _vs._ SLidR ) and the knowledge transfer effects from different vision foundation models. All mIoU scores are given in percentage (%).

    &  &  &  &  &  &  & **Waymo** \\  & & & & & \(\) & \(1\%\) & \(5\%\) & \(10\%\) & \(25\%\) & Full & \(1\%\) & \(1\%\) \\  (1) & ✓ & & & & \(38.80\) & \(38.30\) & \(52.49\) & \(59.84\) & \(66.91\) & \(74.79\) & \(44.60\) & \(47.12\) \\  (2) & ✓ & & ✓ & & \(40.45\) & \(41.62\) & \(54.67\) & \(60.48\) & \(67.61\) & \(75.30\) & \(45.38\) & \(48.08\) \\ (3) & ✓ & ✓ & & & \(43.00\) & \(44.02\) & \(53.03\) & \(60.84\) & \(67.38\) & \(75.21\) & \(45.72\) & \(48.75\) \\ (4) & ✓ & ✓ & ✓ & & \(44.01\) & \(44.78\) & \(55.36\) & \(61.99\) & \(67.70\) & \(75.00\) & \(46.49\) & \(49.15\) \\ (5) & ✓ & ✓ & & ✓ & \(43.35\) & \(44.25\) & \(53.69\) & \(61.11\) & \(67.42\) & \(75.44\) & \(46.07\) & \(48.82\) \\  (6) & ✓ & ✓ & ✓ & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Ablation study of each component pretrained on _nuScenes_ and fine-tuned on _nuScenes_, _SemanticKITTI_, and _Waymo Open_. **Cand cameras will cause performance degradation for different pretraining approaches (compared to Table 1). The performance degradation for PPKT  is especially prominent; we conjecture that this is because the point-wise consistency regularization of PPKT  relies heavily on the calibration accuracy and encounters problems under misalignment. Both SLidR  and _Seal_ exhibit certain robustness; we believe the superpixel-level consistency is less sensitive to calibration perturbations. It is worth mentioning that _Seal_ can maintain good performance under calibration error, since: _i)_ Our VFM-assisted representation learning tends to be more robust; and _ii)_ We enforce superpoint temporal consistency during the pertaining which does not rely on the 2D-3D correspondence.

## 5 Concluding Remark

In this study, we presented _Seal_, a versatile self-supervised learning framework capable of segmenting _any_ automotive point clouds by encouraging spatial and temporal consistency during the representation learning stage. Additionally, our work pioneers the utilization of VFMs to enhance 3D scene understanding. Extensive experimental results on \(20\) downstream tasks across eleven different point cloud datasets verified the effectiveness and superiority of our framework. We aspire for this research to catalyze further integration of large-scale 2D and 3D representation learning endeavors, which could shed light on the development of robust and annotation-efficient perception models.

**Potential Limitations**. Although our proposed framework holistically improved the point cloud segmentation performance across a wide range of downstream tasks, there are still some limitations that could hinder the scalability. _i)_ Our model operates under the assumption of impeccably calibrated and synchronized LiDAR and cameras, which might not always hold true in real-world scenarios. _ii)_ We only pretrain the networks on a single set of point clouds with unified setups; while aggregating more abundant data from different datasets for pretraining would further improve the generalizability of this framework. These limitations present intriguing avenues for future investigations.

    &  &  &  \\  & 1\% & 5\% & 10\% & 25\% & 1\% & 5\% & 10\% & 25\% & 1\% & 5\% & 10\% & 25\% \\  PPKT  & \(34.94\) & \(51.11\) & \(58.54\) & \(65.01\) & \(33.69\) & \(51.40\) & \(58.00\) & \(64.11\) & \(33.35\) & \(50.98\) & \(57.84\) & \(63.52\) \\ SLidR  & \(37.92\) & \(53.08\) & \(59.89\) & \(66.90\) & \(38.00\) & \(52.36\) & \(60.01\) & \(64.10\) & \(37.30\) & \(51.11\) & \(58.50\) & \(64.50\) \\ 
**Seal (Ours)** & \(45.23\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 6: Ablation study on the possible misalignment between the LiDAR and camera sensors. The perturbation is randomly generated and inserted. All mIoU scores are given in percentage (%).

Figure 5: **qualitative results** of different point cloud pretraining approaches pretrained on the raw data of _nuScenes_ and fine-tuned with \(1\%\) labeled data. To highlight the differences, the correct / incorrect predictions are painted in gray / red, respectively. Best viewed in color.