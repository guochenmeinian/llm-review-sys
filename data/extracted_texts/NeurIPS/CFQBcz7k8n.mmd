# Adversarially Robust Learning with Uncertain Perturbation Sets

Tosca Lechner

University of Waterloo, Waterloo, Ontario, Canada, tlechner@uwaterloo.ca

&Vinayak Pathak

Layer 6 AI, Toronto, Ontario, Canada, vinayak@layer6.ai

&Ruth Urner

York University, Toronto, Ontario, Canada, uruth@eecs.yorku.ca

###### Abstract

In many real-world settings exact perturbation sets to be used by an adversary are not plausibly available to a learner. While prior literature has studied both scenarios with completely known and completely unknown perturbation sets, we propose an in-between setting of learning with respect to a class of perturbation sets. We show that in this setting we can improve on previous results with completely unknown perturbation sets, while still addressing the concerns of not having perfect knowledge of these sets in real life. In particular, we give the first positive results for the learnability of infinite Littlestone classes when having access to a perfect-attack oracle. We also consider a setting of learning with abstention, where predictions are considered robustness violations, only when the wrong label prediction is made within the perturbation set. We show there are classes for which perturbation-set unaware learning without query access is possible, but abstention is required.

## 1 Introduction

Adversarial perturbations, imperceivably small manipulations to input instances that cause unexpected failures of learned predictors, pose significant safety concerns for their employment. The phenomenon, first exposed a decade ago in image classification tasks (Szegedy et al., 2014), has since received substantial attention, both in the practical and theoretical machine learning research communities. Studies in both lines of research often skip over the question of how to suitably model "imperceivable perturbations" and rather investigate how to defend against various (fixed) types of attacks. In theoretical studies, this is typically modeled by defining an _adversarial loss_ that, in addition to misclassification, penalizes the existence of a perturbed instance on which a classifier assigns a different label. This type of loss definition then crucially depends on a given _perturbation type_, that is some function \(u: 2^{}\) that assigns every domain instance a set of points that would be viewed as an admissible perturbation (often balls of some radius with respect to some norm are considered).

However, in most realistic settings, exact perturbation sets to be used by an adversary are not plausibly available to a learner. Moreover, it has been shown that encouraging robustness with respect to a perturbation type that is not suitable for the task easily degrades classification accuracy (Tsipras et al., 2019; Zhang et al., 2019; Yang et al., 2020). In response, some work has then explored the question of learnability when the perturbation type is not known at all in advance (Montasser et al., 2021), but information about possible perturbations can be accessed through queries. It is not difficult to be convinced though that without any restrictions or knowledge of the perturbations to be used, robustlearning from samples only is impossible. Even for a small collection of possible perturbation types, as soon as these induce inconsistencies between optimal predictors a sample based learner cannot be simultaneously successful with respect to all options.

In this work we explore a middle ground between fixed and unknown perturbation types, which we term _learning with uncertain perturbation sets_. We will assume that the true perturbation type is a member of a fixed class of perturbation types. One can think of this as knowing that we should encourage robustness with respect to some type of norm induced ball, but may not be certain about which type of norm or which radius is most suitable for the task. We then study which structures (on the interplay between perturbation type class and the class of predictors to be learned) allow for PAC-type learnability guarantees.

Given a class \(\) of perturbation types and hypothesis class \(\), we define an \(\)-induced partial order on \(\). We show that when \(\) is actually totally ordered in this sense, statistical (PAC) learning is possible in the realizable case as soon as \(\) has finite VC-dimension. \(\) being totally ordered applies to a wide variety of settings, for example the case where the various perturbation types are balls in some metric space while the radius varies over \(\) (and actually also does not need to be constant over domain \(\) for each \(u\)). However, we also show that, without the realizability assumption learning from samples alone is not feasible, even when \(\) is totally ordered with respect to \(\).

We thus explore two natural remedies: (1) We allow the learner to interact with a _perfect attack oracle_(Montasser et al., 2021). Given an instance \(x\) and classifier \(f\), such an oracle certifies robustness or provides an adversarial perturbation of \(x\) for \(f\). Previous work on learning with unknown perturbation sets has employed such an oracle and shown learnability for classes with finite Littestone dimension (Montasser et al., 2021). In this work, we show that provided certain structures (\(\) being totally ordered or a finite union of totally ordered perturbation type classes) learning with a perfect attack oracle becomes feasible for all classes of finite VC-dimension (which is a much wider space of natural hypothesis classes than those of finite Littlestone dimension). (2) We allow the learner to output a classifier that sometimes _abstains from prediction_. For this, we define a modification of the adversarial loss, where a hypothesis does not suffer loss when it abstains on an instance that was actually manipulated. We show that this again yields learnability for VC-classes when the perturbation type class is a finite union of totally ordered classes. We then consider the case in which \(\) has finite disagreement-coefficient (Hanneke, 2007) and show that such \(\) can be learned with respect to every class of perturbation types. And finally, we define a notion of \((,)\)-cover for a class of perturbation types \(\), introduce learners for this kind of cover and show that a class \(\) is robustly learnable with respect to \(\) if there is a finite-disagreement-cover learner for \((,)\).

Our results on learning with abstensions show that different strategies for guarantees for different kinds of robustness (for example different types of perturbation sets) can be combined into an abstention learner that only predicts in the agreement region. The guarantees for each learning strategy then yield a guarantee for the combined classifier that depends on the number of strategies used. This highlights a different aspect to the power of abstention in adversarially robust learning from what has been established in prior work (Balcan et al., 2020, 2023). We show that abstentions can also be used to address uncertainty in perturbation types.

### Related work

Providing PAC type learning guarantees under adversarial robustness requirements has received an enormous amount of research attention in the past decade (Feige et al., 2015; Cullina et al., 2018; Wang et al., 2018; Awasthi et al., 2019; Montasser et al., 2019; Attias et al., 2019; Montasser et al., 2020; Ashtiani et al., 2020; Montasser et al., 2021; Gourdeau et al., 2021; Montasser et al., 2022; Attias and Hanneke, 2022; Attias et al., 2022; Bhattacharjee et al., 2021; Awasthi et al., 2022; Mao et al., 2023). We will focus here on aspects within this literature that are most relevant to our settings.

Most prior works study the sample complexity of adversarial robustness with respect to a fixed type of adversarial perturbation (often a metric ball with a certain radius). However, recent research has developed frameworks of analysis that go beyond learning with respect to a fixed type of perturbations. It has been argued that the "correct" type of admissible adversarial perturbations (which originally were "imperceivable" modifications to input instances that lead to misclassification by a learned predictor) should depend on the underlying data-generating process (Bhattacharjee and Chaudhuri,2021, Chowdhury and Urner, 2022]. These studies define a notion of "adaptive robustness" with respect to which a predictor should be evaluated. The drawback of these notions is that the correct perturbation type is not available to the learner and thus a predictor cannot be straightforwardly evaluated with respect to this loss. A different way of relaxing the assumption of a fixed, known perturbation type is provided in the framework of _tolerance_[Ashtiani et al., 2023, Bhattacharjee et al., 2023]. Here, a learner is evaluated with respect to some fixed perturbation type, while being compared to the optimal achievable loss with respect to a larger perturbation type, relaxing the requirement for the learner and reflecting that the exact relevant perturbation type is typically not known (or even uniquely existing).

Most relevant to our work is a recent study on PAC learning with respect to _unknown perturbation sets_[Montasser et al., 2021]. This work provides learning guarantees for the case that the robustness requirement will be with respect to an entirely unknown perturbation type without any prior knowledge about the nature of these perturbations. Those learning bounds assume access to a perfect attack oracle (as we do in some parts of this work), and require the class to have finite Littlestone dimension. In this work, we show that adding some structure to the class of perturbation types (as well as the promise that the true perturbation type will be a member of this class), allows for sample and query complexity bounds that are independent of the hypothesis class's Littlestone dimension.

Finally, there is a long line of work studying the benefits of abstentions for classification tasks [Bartlett and Wegkamp, 2008, El-Yaniv and Wiener, 2010, Wiener and El-Yaniv, 2015, Yuan and Wegkamp, 2010, Zhang and Chaudhuri, 2016]. Some recent studies have also shown that the ability to abstain from prediction for manipulated instances can be beneficial for adversarial robustness requirements [Balcan et al., 2020, 2023]. However, the former studies a different setting for the generation of adversarial perturbations, and the latter considers fixed perturbation types.

### Overview of results

In Section 2, we define our setup, introduce the notions of learnability and loss we investigate and provide an overview of the different kinds of prior knowledge and oracle access we employ. In particular, we there define our partial order on classes of perturbation types.

In Section 3 we analyze perturbation type classes for which this order is a total order. Theorem 1 states that every hypothesis class with finite VC dimension can be robustly learned in the realizable case with respect to any totally ordered perturbation type class. We then prove that this result cannot be extended to the agnostic case (Observation 1). This motivates investigating the benefits of a perfect attack oracle. Theorem 2 shows that the impossibility can be overcome with access to a perfect attack oracle and provides a finite sample and oracle-query bound for the agnostic case.

In Section 4, we investigate classes of perturbation types \(\) that are not necessarily totally ordered. We first extend our previous results on learning with access to a perfect-attack-oracle to the case where \(\) is a union of finitely many totally ordered perturbation classes (Theorem 3). We then establish that there are classes of perturbation types and hypothesis classes which cannot be learned without abstention, even with access to a perfect attack oracle (Observation 3), but which can be learned with abstention without the need for access to a perfect-attack-oracle (Observation 4). This motivates our investigations into learning with respect to our adversarial abstention loss (Definition 5).

We show that in the realizable case, every hypothesis class with finite VC-dimension can be learned with respect to a finite union of totally ordered perturbation types in Theorem 4. We then consider the case in which \(\) has finite disagreement-coefficient and show that such \(\) can be learned with respect to every class of perturbation types (Theorem 5). Lastly, we define a notion of \((,)\)-cover for a class of perturbation types \(\), which states that for every \(u U\) there is a close-to-optimal hypothesis for \(u\) in the cover (Definition 7) and introduce the notion of learners for this kind of cover (Definition 8). We then generalize our previous results by showing that a class \(\) is robustly learnable with respect to \(\) if there is a finite-disagreement-cover learner for \((,)\) (Theorem 6). While we provide some proof sketches in the main body of the paper, the detailed proofs for all results are in the supplement.

## 2 Setup

For sets \(A\) and \(B\), we let \(2^{A}\) denote the powerset of \(A\) and \(A^{B}\) the set of all functions from \(B\) to \(A\). Let \(\) be a domain and \(=\{0,1\}\) be a label space. We model the data generation as a distribution \(P\) over \(\), and let \(P_{}\) denote its marginal over \(\). A _hypothesis_ or _predictor_ is a function \(h:\{0,1,\}\). Thus, on input \(x\), a hypothesis \(h\) either predicts label \(0\) or \(1\) or abstains by outputting \(\). We call a hypothesis _non-abstaining_ if \(h(x)\) for all \(x\). A _hypothesis class_\(\) is a set of non-abstaining predictors. We will let \(=\{0,1,\}^{}\) denote the set of all predictors over \(\).

The performance of a predictor \(h\) is measured by a loss function \(:\). For loss \(\), we use the notation \(_{P}(h)=_{(x,y) P}[(h,x,y)]\), to denote the _expected loss_ of predictor \(h\) with respect to distribution \(P\) and \(_{S}(h)=_{i=1}^{|S|}(h,x_{i},y_{i})\) the _empirical loss_ on dataset \(S=((x_{1},y_{1}),,(x_{m},y_{m}))\). Further, for a hypothesis class \(\) we will let \(_{P}()=_{h}_{P}(h)\) denote the _approximation error_ of class \(\) with respect to \(P\) and loss \(\).

The standard loss for classification tasks is the binary loss \(^{0/1}(h,x,y)=[h(x) y]\), where \([]\) denotes the indicator function. The standard loss for adversarial robustness additionally penalizes \(h\) on instance \((x,y)\) for the existence of a perturbation on \(x\) that \(h\) does not label with \(y\). This loss is defined for a specified _perturbation type_, which is a function \(u: 2^{}\) that assigns every instance \(x\) a _set_\(u(x)\) with \(x u(x)\) of _admissible perturbations_. The _adversarial loss_ with respect to \(u\) is then defined as

\[^{u}(h,x,y)=[ z u(x):h(z) y].\]

We will use the notation \(^{u,}(h,x)\) to denote the _adversarial component loss_, which indicates whether a domain point \(x\) lies close to the decision boundary with respect to the perturbation type \(u\):

\[^{u,}(h,x)=[ z u(x):h(z) h(x)].\]

For hypotheses that have the option to abstain from prediction, we propose a variation of the adversarial loss that allows for a predictor to abstain from prediction on perturbed instances (but not on unperturbed instances) without suffering loss. The following definition captures this modification:

**Definition 1** (Adversarial abstention loss).: _For a perturbation type \(u: 2^{}\), the adversarial abstention loss is defined by_

\[^{u,}(h,x,y)=1&h(x) y\\ 1& z u(x)h(z) yh(z)\\ 0&\]

The main difference with adversarial loss is that if \(h(z)=\) for an adversarial point \(z u(x)\) with \(z x\), then there is no penalty and \(^{u,}(h,x,y)=0\), thus allowing the predictor to abstain on points that were perturbed. This definition models the scenario where if the predictor can correctly detect that an adversarial attack has happened and abstains, then it is not penalized. However, abstaining on non-perturbed points is still penalized. Note that in case a predictor \(h\) never abstains (that is \(h(x)\) for all \(x\)) the two definitions of adversarial loss coincide. We let \(_{P}^{u}(h)\), \(_{P}^{u,}(h)\), \(_{S}^{u}(h)\), \(_{S}^{u,}(h)\) denote the corresponding expected and empirical losses, and \(_{P}^{u}()\), \(_{P}^{u,}()\) the corresponding approximation errors of a class \(\) and distribution \(P\).

Uncertain perturbation typeIn the literature, adversarial robustness is mostly treated with respect to a fixed (known) perturbation type \(u\), while learning with respect to an entirely unknown perturbation type has also been investigated. In this work, we introduce a setting that naturally interpolates between these two extremes. We assume that the true perturbation type \(u^{*}\) is unknown to the learner, but promised to lie within a fixed _class \(\) of perturbation types_. That is, the learner has prior knowledge of a class \(\) with \(u^{*}\), where \(u^{*}\) is the perturbation type that the learner will be evaluated with (see Definition 3 below). We can think of \(u^{*}\) as the perturbation type that the true adversarial environment will employ to manipulate input instances and induce misclassifications. We let \(^{}\) denote the class that contains all possible perturbation types.

It is easy to see that without any restrictions on the class \(\) and any capability of the learner to (interactively) gather information about \(u^{*}\) and no option to abstain, a learner cannot succeed (see also Observations 1 and 2 below). To gain control over potentially infinite classes of perturbation types, we will now define a partial order on a class \(\) that is induced by a hypothesis class \(\). This structure will prove to be conducive for learnability.

**Definition 2**.: _Let \(\{0,1\}^{}\) be a hypothesis class. For perturbation types \(u_{1},u_{2}\) we say that \(u_{1}\) is smaller than \(u_{2}\) with respect to class \(\), and write \(u_{1}_{H}u_{2}\), if and only if for all \(h\) and all \(x\) we have_

\[^{u_{1},}(h,x)^{u_{2},}(h,x).\]

_We say a set of perturbation types \(\) is totally ordered, with respect to \(_{}\), if for every \(u_{1},u_{2}\), we have either \(u_{1}_{}u_{2}\) or \(u_{2}_{}u_{1}\)._

We have \(u_{1}_{H}u_{2}\) if, for every function \(h\) the margin area (the points which receive positive adversarial component loss) with respect to \(u_{1}\) is included in the margin area with respect to \(u_{2}\). This is, for example, the case if all perturbation sets with respect to \(u_{1}\) are included in those induced by \(u_{2}\).

Resources to learners and learnabilityA _learner_ is a function \(A:()^{*}\) that takes in a finite sequence \(S=((x_{1},y_{1}),(x_{m},y_{m}))\) of labeled examples and outputs a predictor \(A(S):\{0,1,\}\). We will study the following PAC-type (Valiant, 1984) notion of learnability of hypothesis classes \(\) with perturbation type classes \(\).

**Definition 3** (Learnability without abstentions).: _Let \(\{0,1\}^{}\) be a hypothesis class and \(\) a class of perturbation types. We say that \(\) is robustly learnable with respect to perturbation class \(\) if there exists a learner \(A\) and a function \(m:(0,1)^{2}\) such that for every \(,>0\), every \(m m(,)\), every \(u^{*}\) and every data-generating distribution \(P\) we have_

\[_{S P^{m}}[_{P}^{u^{*}}(A(S))_ {P}^{u^{*}}()+] 1-.\]

_We speak of learnability in the realizable case if the above requirement is relaxed to only hold for all distributions \(P\) with \(_{P}^{u^{*}}()=0\). Without this assumption, we also speak of learnability in the agnostic case._

If the above definition is fulfilled for a class \(\), then the function \(m:(0,1)^{2}\) in the above definition is an upper bound on the _sample complexity_ of the learning task. Note that, while we don't require the learner in the above definition to output a non-abstaining hypothesis, predicting \(\) will always cause loss with respect to \(^{u}\). We can thus assume all considered predictors are non-abstaining classifiers for this setting.

Note that the above definition generalizes and unifies previous notions of adversarially robust learnability. We obtain learnability with respect a _known perturbation type_ (the setting that is mostly considered in prior works) when \(=\{u^{*}\}\) is a singleton class. Learning with _unknown perturbations_(Montasser et al., 2021) on the other hand, is the setting where \(\) is the set of all possible perturbation types. In this work, we are interested in studying more fine-grained conditions on the structure of (the combination of) \(\) and \(\) that allow for learnability. We will call the case where \(\) is neither a singleton set nor the set of all possible perturbation types _learning with uncertain perturbation sets_.

We will show that, even when the class of perturbation types \(\) in consideration is totally ordered with respect to hypothesis class \(\), agnostically learning \(\) with \(\) is impossible even in very simple cases (see Observation 1). We will thus, as has been done in earlier studies (Ashtiani et al., 2020; Montasser et al., 2021) in addition allow the learner access to a _perfect attack oracle_, which we model as follows:

**Definition 4** (Perfect Attack Oracle).: _A perfect attack oracle for perturbation type \(u\) is a function \(_{u}:\{ {robust}\}\), that takes as input a non-abstaining predictor \(f\) and a domain point \(x\) and either certifies that \(f\) is robust on \(x\) with respect to \(u\) by outputting \(\) or returns an admissible perturbation of \(x\) for \(f\). That is_

\[_{u}(f,x)=f(z)=f(x)z u(x)\\ z u(x)f(x) f(z)\]

We say that a class \(\) and perturbation class \(\) are _learnable with access to a perfect attack oracle_ if there exists a function \(n:(0,1)^{2}\) such that the conditions in Definition 3 are satisfied for a learner that additionally makes at most \(n(,)\) queries to the perfect attack oracle. The function \(n(,)\) then is an upper bound on the attack _oracle complexity_ of the learning problem.

In Section 4 we will consider more general pairs of hypothesis and perturbation classes \(\) and \(\). We will show that if \(\) is not necessarily totally ordered with respect to \(\) (or a finite union of such totallyordered classes) there are tasks on which no learner (even with access to a perfect attack oracle) can succeed (see Observation 3 below). For such cases we will explore learnability with respect to the adversarial abstention loss:

**Definition 5** (Learnability with abstentions).: _Let \(\{0,1\}^{}\) be a hypothesis class and \(\) a class of perturbation types. We say that \(\) is robustly learnable with abstentions with respect to perturbation class \(\) if there exists a learner \(A\) and a function \(m:(0,1)^{2}\) such that for every \(,>0\), every \(m m(,)\), every \(u^{*}\) and every data-generating distribution \(P\) we have_

\[_{S P^{m}}[_{P}^{u^{*},}(A(S)) _{P}^{u^{*}}()+] 1-.\]

Note that since \(\) contains only non-abstaining predictors, we have \(_{P}^{u^{*}}()=_{P}^{u^{*},}( )\).

Discussion on additional parameters and assumptionsThroughout this paper, we will also work with various standard complexity measures, such as bounded VC-dimension or bounded Littlestone dimension of the hypothesis class \(\), bounded VC-dimension of the loss class or the adversarial component loss class. We refer the reader to the appendix for a reminder of the definitions of these notions.

We will further assume that learners have the capability to identify empirical risk minimizing hypotheses from a class. It is standard (though often implicit) to assume this as oracle access for the learner as well. We consider the standard ERM oracle, a robust ERM (RERM) oracle (for a fixed perturbation type \(u\)) and a maximally robust ERM (MRERM) oracle (for a class \(\) of perturbation types, and realizable samples only). We define the following oracles:

\[_{} :S h_{S}_{h}_{S }^{0/1}(h)\] \[_{}^{u} :S h_{S}^{u}_{h}_{S }^{u}(h)\] \[_{}^{} :S(h_{S}^{u_{S}^{*}},u_{S}^{*})h_{S}^{u_{S}^{*}}_{h}_{S }^{u_{S}^{*}}(h),\\ u_{S}^{*}_{}\{u _{h}_{S}^{u}(h)=0\}\\ _{S}^{u}(h)>0uh \]

That is, we will assume that \(_{}^{}\) will return an error if the input sample \(S\) is not \(^{u}\)-realizable for any \(u\). If, on the other hand, there does exist a \(u\) for which \(S\) is \(^{u}\)-realizable, then \(_{}^{}\) will return a maximal (with respect to \(_{}\)) such perturbation type \(u_{S}^{*}\) and corresponding hypothesis \(h_{S}^{u_{S}^{*}}\) with \(_{S}^{u^{*}}(h_{S}^{u^{*}})=0\). In particular, we assume that for every non-empty sample \(S\) there exists such a maximal element \(u_{S}^{*}\) and corresponding ERM hypothesis from \(\). While these do not always exist in \(\) and \(\) a priori, it has recently been shown that it is possible to embed \(\) and \(\) into slightly larger classes so that the \(_{}^{}\) oracle is always well defined [Lechner et al., 2023]. See Appendix Section A for more details on this.

Since we focus on studying sample complexity (independently of computational considerations) and consider learners as functions throughout, assuming access to the above oracles is not restricting the validity of our bounds. It is still interesting to understand when is their existence reasonable to assume. Both ERM and RERM oracles have been widely used in the literature and they are often computationally hard to implement. Assuming their existence, however, an MRERM oracle can be easily implemented for the case where \(\) is finite and totally ordered by running a binary search over \(\) and calling RERM for the comparison step. If we assume \(\) is parametrized by a \(k\)-bit number, then this requires \(O(k)\) queries to RERM.

## 3 Learning with a totally ordered perturbation class

We will start with investigating the case where the perturbation class \(\) is totally ordered with respect to hypothesis class \(\) (see Definition 2). Note that if two perturbation types \(u_{1}\) and \(u_{2}\) we satisfy \(u_{1}(x) u_{2}(x)\) for all \(x\), then \(u_{1}\) is smaller than \(u_{2}\), \(u_{1}_{}u_{2}\), for every hypothesis class \(\{0,1\}^{}\). This is, for example, the case when \(u_{1}\) and \(u_{2}\) assign balls (with respect to some metric) centered at \(x\) to every domain point \(x\) and \(u_{2}\) always assigns a ball of radius at least as large as the ball assigned by \(u_{1}\) (while the balls assigned by \(u_{1}\) and \(u_{2}\) are not necessarily required to have the same radii uniformly over the space \(\)).

In this section, we will focus on learning with respect to the standard adversarial loss and thus only consider non-abstaining predictors. We first show that for \(\) and totally ordered \(\), whenever the class \(\) has bounded VC-dimension we get learnability in the realizable case. This result is based on adapting a compression argument for adversarially robust learning (Montasser et al., 2019) to the case of uncertain perturbation sets. A very similar adaptation has recently been made for the related problem of strategic classification (Lechner et al., 2023). The proof of this result can be found in the appendix.

**Theorem 1**.: _Let \(\) be a perturbation class that is totally ordered with respect to a hypothesis class \(\) with \(()=d<\). Then \(\) is learnable in the realizable case with respect to \(\) with sample complexity \(O((1/)}{})\)._

Recall that realizability here means \(_{P}^{u^{*}}()=0\) for the true perturbation type \(u^{*}\) (with respect to which the learner is evaluated). We now show that without this assumption learning guarantees are impossible without equipping the learner with additional resources (or weakening the success criteria).

**Observation 1**.: _There exists a class \(\) with \(()=1\) and a perturbation class \(\) that is totally ordered with respect to \(\) that is not learnable (in the sense of Definition 3)._

We will now show that allowing a learner access to a perfect attack oracle yields learnability in the agnostic case.

**Theorem 2**.: _Let \(\) be a perturbation class that is totally ordered with respect to a hypothesis class \(\) with \(()=d<\) and assume access to a perfect attack oracle. Then \(\) is learnable in the agnostic case with respect to \(\) with sample complexity \(m(,)=O((1/)}{^{2}})\) and oracle complexity \(O(m(,)^{2})\)._

Proof.: We will employ a well-known reduction from agnostic learning to realizable compression-based learning (Montasser et al., 2019). Since our proof for Theorem 1 for the realizable case of learning with uncertain, totally ordered perturbation sets employs a compression-based learner, for the agnostic case it suffices to show that, given any sample \(S=((x_{1},y_{1}),,(x_{m},y_{m}))\), we can identify a _largest_ subsample \(S^{}\) of \(S\) that is realizable with respect to the underlying perturbation type \(u^{*}\). We will now outline how the perfect attack oracle can be employed to achieve this.

Employing the \(^{}_{}\)-oracle, the learner can generate an ordered list \(T=((S_{1},h_{1},u_{1}),(S_{n},h_{n},u_{n})))\) of subsamples of \(S\) that are realizable with respect to some \(u\), such that \((h_{i},u_{i})=^{}_{}(S_{i})\) for all \(i\) and \(u_{i}_{}u_{j}\) for all \(i j\). Note that we have \(u^{*}_{}u_{i}\) if and only if the perfect attack oracle returns \(\) for all sample points in \(S_{i}\) when tested on \(h_{i}\), that is \(_{u^{*}}(h_{i},x)=\) for all \(x\) with \((x,y) S_{i}\) for some \(y\). Thus, using a binary search over the list \(T\), we can identify the smallest index \(\) for which \(u^{*}_{}u_{}\). Since we have \(n 2^{m}\) and every test in the search employs at most \(m\) calls to the perfect attack oracle, this search terminates after at most \(m^{2}\) queries. Finally, note that all subsets \(S_{i}\) from \(T\) for which \(i\) are realizable with respect to \(u^{*}\). Thus we can choose any largest among \(S_{}, S_{n}\) as a maximal realizable subset with respect to the unknown perturbation type \(u^{*}\). 

In Section B we provide additional guarantees for the agnostic case.

## 4 Beyond totally ordered perturbation classes

We now consider a more general setting where \(\) is not necessarily totally ordered with respect to \(\).

### Learning with the standard adversarial loss

We will start by showing that \(\), \(\) are learnable with access to a perfect attack oracle if \(\) is the union of \(k\) sub-classes which are each totally ordered with respect to \(\). This naturally contains the case that \(\) is finite. Another natural case where this occurs is considering perturbation sets that are balls around domain points of certain radii and with respect to one of a number of possible norms. Consider \(=^{d}\) and the set

\[=\{u_{r}^{p}:x B_{r}^{p}(x)\;\;r,\;p 0,1,2,\}\]where we let \(B_{r}^{p}(x)\) denote the \(^{p}\)-norm ball around \(x\). Then the above class \(\) is a union of \(4\) totally ordered perturbation classes (one for balls of each norm) with respect to any hypothesis class \(\).

**Theorem 3**.: _Let \(\) be a hypothesis class with \(()=d<\), and let \(\) be a perturbation class, which is a union \(=_{1}_{2}_{k}\) of \(k\) subclasses which are each totally ordered with respect to class \(\). Then \(\) is learnable with access to a perfect attack oracle \(_{u^{*}}\) in the realizable case with respect to \(\) with sample complexity \(O((k/)}{}+})\) and query complexity \(O((} )^{2})\)._

Proof.: We can generate \(k\) hypotheses \(h_{1},h_{2},,h_{k}\) that are the output of an \((,)\)-successful learner for \(\) with respect to each of the \(_{i}\). Since each of the classes \(_{i}\) are totally ordered with respect to \(\), the result of Theorem 1 tells us that \(O((k/)}{})\) sample points suffice to guarantee that, with probability at least \(1-\), we have \(^{u^{*}}(h_{i})\) for all \(i\) with \(u^{*}_{i}\). Thus the class \(_{k}=\{h_{1},h_{2},,h_{k}\}\) of these \(k\) learning outputs has approximation error at most \(\) with respect to the data generating distribution \(P\). Note that, since we have \(|_{k}|=k\), this class has also bounded Littlestone dimension at most \((k)\). Employing a result (Montasser et al. (2021), Theorem 3) on agnostically learning finite Littlestone classes with access to a perfect attack oracle, an additional \(O(})\) samples and the stated number of queries suffice to output a hypothesis \(h\) with \(_{P}^{u^{*}}(h) 3\). 

It is not difficult to see that, even in the realizable case, and when \(\) is the union of just two totally ordered subclasses, a learner without access to a perfect attack oracle cannot succeed with respect to the standard adversarial loss.

**Observation 2**.: _There exists a class \(\) with \(()=1\) and a perturbation class \(\) that is the union of two totally ordered subclasses with respect to \(\) that are not learnable in the realizable case (in the sense of Definition 3) without access to a perfect attack oracle._

For the proof of this observation we refer the reader to the supplementary material. Now we show that if we remove even more structure from the set of perturbation types \(\) then learning success becomes impossible even with access to a perfect attack oracle. The following observation will then motivate to investigate learning with respect to the adversarial abstention loss (rather than the standard adversarial loss).

**Observation 3**.: _There exists a class \(\) with \(()=1\) and a perturbation class \(\) that are not learnable (in the sense of Definition 3), even with access to a perfect attack oracle._

Proof.: We consider the same domain, class, and distribution as in the proof of the previous observation: \(=\), \(P\) with \(P((-3,1)=P((3,0))=0.5\), and

\[=\{h_{t}:x\;[x t]\;|\;t\}.\]

Further, we consider the class \(=\{u_{a,b}\;|\;a,b\}\) of perturbation types \(u_{a,b}\), that assign all points \(x 0\) to a ball of radius \(a\) around \(x\) and all points \(x>0\) to a ball of radius \(b\). Then, even when promised realizability, a learner cannot distinguish the cases where \(u^{*}=u_{a,b}\) for some values \(a,b\) with \(a+b 6\) from samples and any finite number of queries to the perfect attack oracle. In particular, if \(u^{*}=u_{a,b}\) for some \(a,b\) with \(a+b=6\), predicting with respect to one such pair \(a,b\) will induce loss \(0.5\) for all other such pairs. 

### Learning with the adversarial abstention loss

We will now consider learning with respect to the adversarial abstention loss. We will first show that learning with abstentions can actually help overcome some of the impossibilities shown above. For this, let us revisit the example seen in Observation 3.

**Observation 4**.: _There exists a class \(\) with \(()=1\) and a perturbation class \(\) that are not learnable (in the sense of Definition 3), even with access to a perfect attack oracle, but that is robustly learnable with abstentions (in the sense of Definition 5) in the realizable case (without access to a perfect attack oracle)._Proof sketch.: We consider the example from the proof of Observation 3. Now consider the subclasses \(_{1}=\{u_{a,0}:a\}\) and \(_{2}=\{u_{0,b}:b\}\). Since both \(_{1}\) and \(_{2}\) are totally ordered we can learn successful hypotheses \(h_{1}\) and \(h_{2}\) respectively. We now define \(h:\{0,1,\}\) by \(h(x)=y\) if \(h_{1}(x)=h_{2}(x)=y\) and \(h(x)=\) otherwise. This is a successful hypothesis. For more detail, we refer the reader to the appendix. 

We will now continue by showing that, similar to the case where we have access to a perfect-attack-oracle, a finite union \(=_{i=1}^{k}_{i}\) of totally ordered perturbation sets \(_{i}\) is robustly learnable with abstention in the robustly realizable case.

**Theorem 4**.: _Let \(=_{i=1}^{k}_{i}\), where every \(_{i}\) is totally ordered. Furthermore, let \(\) have a finite VC dimension. Then \(\) is robustly learnable with abstention with respect to \(\) in the robustly realizable case with sample complexity \(O( k^{2}(k/)}{})\)._

#### 4.2.1 Abstention learning with disagreement coefficient

The proof of Theorem 4 shows how bounding the mass of the region on which two learners with small \(0/1\)-loss disagree, yields a robust error guarantee for learning with abstentions. In this section, we will review the definition of the _disagreement coefficient_(Hanneke (2007), Hanneke et al. (2014)), and show that hypothesis classes \(\) with finite disagreement coefficient are robustly learnable with abstention in the realizable case with respect to every class of perturbation types \(\).

For a distribution \(P_{}\) over \(\), we let the \(P_{}\)-induced difference between two hypotheses be denoted by

\[d_{P_{}}(h,h^{})=_{x P_{}}[h(x) h ^{}(x)].\]

The ball in \(\) around a hypothesis \(h\) with radius \(r\) is then denoted by

\[B_{P_{}}(h,r)=\{h^{}:d_{P_{}}(h,h^{ }) r\}.\]

Lastly, let the disagreement region of a set of hypotheses \(\) be

\[()=\{x: h,h^{}h(x) h^{}(x)\}.\]

We can now state the definition of the disagreement coefficient.

**Definition 6** (Hanneke et al. (2014)).: _For a distribution \(P\) over \(\) and a class \(\{0,1\}^{}\), let \(h^{*}_{h^{}}_{P}(h^{})\). The disagreement-coefficient of \(\) with respect to \(P\) is defined by:_

\[_{P}()=_{r(0,1)}}((B(h ^{*},r)))}{r}.\]

_Furthermore, let the worst-case disagreement coefficient be \(()=_{P()}_{P} ()\)._

**Theorem 5**.: _Let \(\) be a hypothesis class with \(()<\) and \(VC()=d<\). Then the class \(\) is robustly learnable with abstention in the realizable case with respect to the class of all perturbation types \(^{}\) with sample complexity \(O()}{(^{}())^{2}})\)._

The proof can be found in Appendix D.6, but the main idea is that for any point \(x\), we return a \(y\{0,1\}\) if and only if every \(h\) with \(_{S}^{0/1}(h)=0\) agrees with \(y\), otherwise we abstain. Because of the bound on the disagreement coefficient, one can show that for a large enough \(S\), this leads to a hypothesis with low abstention loss.

#### 4.2.2 Abstention learning via \(\)-cover

We will now generalize our previous results, by introducing the concept of an \((,)\)-cover.

**Definition 7** (\((,)\)-cover of \(\)).: _Let \(\{0,1\}^{}\) be a hypothesis class, \(P\) a distribution over \(\) and \(\) a class of perturbation types. A set of hypotheses \(^{}\) is a \((,)\)-cover for \(\) with respect to \(P\), if for every \(u\), there exists an \(h^{}^{}\) such that \(_{P}^{u}(h^{})<_{h}_{P}^{u}(h)+\)._

We will now provide the definition of a successful cover learner.

**Definition 8**.: _Let \(\{0,1\}^{}\) be a hypothesis class, and \(\) be a class of perturbation types. We say a function \(_{cover}:()^{*} 2^{\{0,1\}^{ }}\) that maps a sample \(S\) to a hypothesis class \(}\) is a successful finite-disagreement-cover learner for \((,)\), if there is a function \(m_{(,),cover}:(0,1)^{3}\), such that for every \(,,(0,1)\), every \(m m_{(,),cover}(,,)\) and every distribution \(P\) over \(\{0,1\}\) with probability \(1-\) over \(S P^{m}\) both of the following statements hold:_

* \((S)\) _is an_ \((,)\)_-cover for_ \(\) _with respect to_ \(P\)__
* \(P_{}(((S))\,\)_._

We note that any successful cover learner, that is guaranteed to output a finite set is a successful finite-disagreement cover learner. The case, where \(\) is a union of \(k\) totally ordered perturbation type classes \(_{i}\), we have seen before, can be interpreted as first learning a cover \(\) of size \(k\), where each of the element of the union "covers" one of the classes. The case in which \(\) has a finite disagreement coefficient can furthermore be seen as a case where the set of all \(_{}\) hypotheses constitute an \((,)\)-cover \(^{}\) for every class of perturbation types \(\). The mass of the disagreement region of \(\) is then bounded by the disagreement coefficient. We now state our result generalizing these observations.

**Theorem 6**.: _A class \(\) is robustly learnable with abstentions with respect to \(\) in the robustly realizable case, if there is a successful finite-disagreement-cover learner for \((,)\). Furthermore, the sample complexity of robustly learning with abstentions is then bounded by \(m_{,}(,) m_{,, cover}(/2,/2,)\)._

Inspired by considerations of adaptive robustness radii (Chowdhury and Urner, 2022; Bhattacharjee and Chaudhuri, 2021), in Section C of the appendix we discuss a setting of abstention learning in which we allow the size of the perturbation sets to vary locally between different regions of the domain. We provide a learning guarantee for this setting that is not covered by our previous results.

## 5 Conclusion

In this work we relaxed the assumption that we know exactly the perturbation sets that are to be used by an adversary. Our model provides an interpolation between knowing the perturbation sets exactly and not knowing them at all. Many of our results rely on realizability. We have also shown that without realizability, learning is not possible unless the learner receives other feedback, such as access to a perfect attack oracle. It might be interesting to consider some milder relaxations of realizability. For example, instead of saying \(_{P}^{u^{*}}()=0\), what if we only know that the adversarial Bayes loss is zero for some \(u^{*}\)?

While our work focuses on statistical aspects, it will also be interesting to understand the kinds of computation required for our results. For example, we rely on the existence of an MRERM oracle. When are they computationally feasible? Do we require accurate MRERM oracles or can approximate (and hence potentially more tractable) versions also be used for the learning task?

Another question is to find a dimension that characterizes learning in various settings. This was only recently resolved for adversarially robust learning (for the case when \(u\) is known)Montasser et al. (2022), however, the dimension proposed in the paper is not easy to define. On the other hand, in the setting when the only information we can obtain about \(u\) is through a perfect attack oracle, the Littlestone dimension of \(\) has been shown to characterize adversarially robust learning Montasser et al. (2021). It will be interesting to see if such a simple dimension can be obtained for the settings considered in our paper.