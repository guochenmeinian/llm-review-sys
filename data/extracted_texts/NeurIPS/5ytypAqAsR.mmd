# No Representation Rules Them All in Category Discovery

Sagar Vaze

Andrea Vedaldi

Andrew Zisserman

Visual Geometry Group

University of Oxford

www.robots.ox.ac.uk/~vgg/data/clevr4/

###### Abstract

In this paper we tackle the problem of Generalized Category Discovery (GCD). Specifically, given a dataset with labelled and unlabelled images, the task is to cluster all images in the unlabelled subset, whether or not they belong to the labelled categories. Our first contribution is to recognize that most existing GCD benchmarks only contain labels for a single clustering of the data, making it difficult to ascertain whether models are using the available labels to solve the GCD task, or simply solving an unsupervised clustering problem. As such, we present a synthetic dataset, named 'Clevr-4', for category discovery. Clevr-4 contains four equally valid partitions of the data, _i.e_. based on object _shape_, _texture_, _color_ or _count_. To solve the task, models are required to extrapolate the taxonomy specified by the labelled set, rather than simply latching onto a single natural grouping of the data. We use this dataset to demonstrate the limitations of unsupervised clustering in the GCD setting, showing that even very strong unsupervised models fail on Clevr-4. We further use Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a new method which addresses these shortcomings, leveraging consistent findings from the representation learning literature to do so. Our simple solution, which is based on'mean teachers' and termed \(\)GCD, substantially outperforms implemented baselines on Clevr-4. Finally, when we transfer these findings to real data on the challenging Semantic Shift Benchmark (SSB), we find that \(\)GCD outperforms all prior work, setting a new state-of-the-art.

## 1 Introduction

Developing algorithms which can classify images within complex visual taxonomies, _i.e._ image recognition, remains a fundamental task in machine learning . However, most models require these taxonomies to be _pre-defined_ and _fully specified_, and are unable to construct them automatically from data. The ability to _build_ a taxonomy is not only desirable in many applications, but is also considered a core aspect of human cognition . The task of constructing a taxonomy is epitomized by the Generalized Category Discovery (GCD) problem : given a dataset of images which is labelled only in part, the goal is to label all remaining images, using categories that occur in the labelled subset, or by identifying new ones. For instance, in a supermarket, given only labels for'spaghetti' and 'penne' pasta products, a model must understand the concept of 'pasta shape' well enough to generalize to'macaroni' and 'fusilli'. It must _not_ cluster new images based on, for instance, the color of the packaging, even though the latter _also_ yields a valid, but different, taxonomy.

GCD is related to self-supervised learning  and unsupervised clustering , which can discover _some_ meaningful taxonomies automatically . However, these _cannot_ solve the GCD problem, which requires recovering _any_ of the different and incompatible taxonomies that apply to the same data. Instead, the key to GCD is in _extrapolating a taxonomy_ which is only partially known. In this paper, our objective is to better understand the GCD problem and improve algorithms' performance.

To this end, in section 3, we first introduce the Clevr-4 dataset. Clevr-4 is a synthetic dataset where each image is fully parameterized by a set of four attributes, and where each attribute defines an equally valid grouping of the data (see fig. 1). Clevr-4 extends the original CLEVR dataset  by introducing new _shapes_, _colors_ and _textures_, as well as allowing different object _counts_ to be present in the image. Using these four attributes, the same set of images can be clustered according to _four_ statistically independent taxonomies. This feature sets it apart from most existing GCD benchmarks, which only contain sufficient annotations to evaluate a _single_ clustering of the data.

Clevr-4 allows us to probe large pre-trained models for biases, _i.e._, for their preference to emphasize a particular aspect of images, such as color or texture, which influences which taxonomy can be learned. For instance, contrary to findings from Geirhos _et al._, we find almost every large model exhibits a strong shape bias. Specifically, in section 4, we find unsupervised clustering - even with very strong representations like DINO  and CLIP  - fails on many splits of Clevr-4, despite CLEVR being considered a 'toy' problem in other contexts . As a result, we find that different pre-trained models yield different performance traits across Clevr-4 when used as initialization for category discovery. We further use Clevr-4 to characterize the weaknesses of existing category discovery methods; namely, the harms of jointly training feature-space and classifier losses, as well as insufficiently robust pseudo-labelling strategies for 'New' classes.

Next, in section 5, we leverage our findings on Clevr-4 to develop a simple but performant method for GCD. Since category discovery has substantial overlap with self-supervised learning, we identify elements of these methods that are also beneficial for GCD. In particular, mean-teacher based algorithms  have been very effective in representation learning [14; 18], and we show that they can boost GCD performance as well. Here, a 'teacher' model provides supervision through pseudo-labels, and is maintained as the moving average of the model being trained (the'student'). The slowly updated teacher is less affected by the noisy pseudo-labels which it produces, allowing clean pseudo-labels to be produced for new categories. Our proposed method, '\(\)GCD' ('mean-GCD'), extends the existing state-of-the-art , substantially outperforming it on Clevr-4. Finally, in section 6, we compare \(\)GCD against prior work on real images, by evaluating on the challenging Semantic Shift Benchmark (SSB) . We substantially improve state-of-the-art on this evaluation.

In summary, we make the following key contributions: (i) We propose a new benchmark dataset, Clevr-4, for GCD. Clevr-4 contains four independent taxonomies and can be used to better study the category discovery problem. (ii) We use Clevr-4 to garner insights on the biases of large pre-trained models as well as the weaknesses of existing category discovery methods. We demonstrate that even very strong unsupervised models fail on this 'toy' benchmark. (iii) We present a novel method for GCD, \(\)GCD, inspired by the mean-teacher algorithm. (iv) We show \(\)GCD outperforms baselines on Clevr-4, and further sets a new state-of-the-art on the challenging Semantic Shift Benchmark.

Figure 1: What is the key difference between Generalized Category Discovery (GCD) and tasks like self-supervised learning or unsupervised clustering? GCD’s key challenge is extrapolating the _desired_ clustering of data given only a subset of possible category labels. We present a synthetic dataset, Clevr-4, which contains _four_ possible clusterings of the same images, and hence can be used to isolate the GCD task. Above, one can cluster the data based on object _count_, _shape_ or _texture_.

Related Work

**Representation learning.** The common goal of self-, semi- and unsupervised learning is to learn representations with minimal labelled data. A popular technique is contrastive learning [21; 22], which encourages representations of different augmentations of the same training sample to be similar. Contrastive methods are typically either based on: InfoNCE  (_e.g._, MoCo  and SimCLR ); or online pseudo-labelling (_e.g._, SWAV  and DINO ). Almost all contrastive learning methods now adopt a variant of these techniques [25; 26; 27; 28]. Another important component in many pseudo-labelling based methods is'mean-teachers'  (or momentum encoders ), in which a 'teacher' network providing pseudo-labels is maintained as the moving average of a'student' model. Other learning methods include cross-stitch , context-prediction , and reconstruction . In this work, we use mean-teachers to build a strong recipe for GCD.

**Attribute learning.** We propose a new synthetic dataset which contains multiple taxonomies based on various attributes. Attribute learning has a long history in computer vision, including real-world datasets such as the Visual Genome , with millions of attribute annotations, and VAW, with 600 attributes types . Furthermore, the _disentanglement literature_[34; 35; 36] often uses synthetic attribute datasets for investigation [37; 38]. We find it necessary to develop a new dataset, Clevr-4, for category discovery as real-world datasets have either: noisy/incomplete attributes for each image [32; 39]; or contain sensitive information (_e.g._ contain faces) . We find existing synthetic datasets unsuitable as they do not have enough categorical attributes which represent'semantic' factors, with attributes often describing continuous 'nuisance' factors such as object location or camera pose [37; 38; 41].

**Category Discovery.** Novel Category Discovery (NCD) was initially formalized in . It differs from GCD as the unlabelled images are known to be drawn from a _disjoint_ set of categories to the labelled ones [43; 44; 45; 46; 47]. This is different from _unsupervised clustering_[10; 48], which clusters unlabelled data without reference to labels at all. It is also distinct from _semi-supervised learning_[17; 26; 49], where unlabelled images come from the _same_ set of categories as the labelled data. GCD [7; 8] was recently proposed as a challenging task in which assumptions about the classes in the unlabelled data are largely removed: images in the unlabelled data may belong to the labelled classes or to new ones [19; 50; 51; 52]. We particularly highlight concurrent work in SimGCD , which reports the best current performance on standard GCD benchmarks. Our method differs from SimGCD by the adoption of a mean-teacher  to provide more stable pseudo-labels training, and by careful consideration of model initialization and data augmentations.  also adopt a momentum-encoder, though only for a set of class prototypes rather than in a mean-teacher setup.

## 3 Clevr-4: a synthetic dataset for generalized category discovery

**Generalized Category Discovery (GCD).** GCD  is the task of fully labelling a dataset which is only partially labelled, with labels available only for a subset of the categories. Formally, we are presented with a dataset \(\), containing both labelled and unlabelled subsets, \(_{}=\{(_{i},y_{i})\}_{i=1}^{N_{i}} _{}\) and \(_{}=\{(_{i},y_{i})\}_{i=1}^{N_{}} _{}\). The model is then tasked with categorizing all instances in \(_{}\). The unlabelled data contains instances of categories which do not appear in the labelled set, meaning that \(_{}_{}\). Here, we _assume knowledge_ of the number of categories in the unlabelled set, _i.e._\(k=|_{}|\). Though estimating \(k\) is an interesting problem, methods for tackling it are typically independent of the downstream recognition algorithm [7; 42]. GCD is related to Novel Category Discovery (NCD) , but the latter makes the not-so-realistic assumption that \(_{}_{}=\).

**The category discovery challenge.** The key to category discovery (generalized or not) is to use the labelled subset of the data to _extrapolate a taxonomy_ and discover novel categories in unlabelled images. This task of extrapolating a taxonomy sets category discovery apart from related problems. For instance, unsupervised clustering  aims to find the single most natural grouping of unlabelled images given only weak inductive biases (_e.g._, invariance to specific data augmentations), but permits limited control on _which_ taxonomy is discovered. Meanwhile, semi-supervised learning  assumes supervision for all categories in the taxonomy, which therefore must be known, in full, a-priori.

A problem with many current benchmarks for category discovery is that there is no clear taxonomy underlying the object categories (_e.g._, CIFAR ) and, when there is, it is often ill-posed to understand it given only a few classes (_e.g._, ImageNet-100 ). Furthermore, in practise, there are likely to be many taxonomies of interest. However, few datasets contain sufficiently complete annotations to evaluate multiple possible groupings of the same data. This makes it difficult to ascertain whether a model is extrapolating information from the labelled set (category discovery) or just finding its own most natural grouping of the unlabelled data (unsupervised clustering).

**Clevr-4.** In order to better study this problem, we introduce Clevr-4, a synthetic benchmark which contains four equally valid groupings of the data. Clevr-4 extends the CLEVR dataset , using Blender  to render images of multiple objects and place them in a static scene. This is well suited for category discovery, as each object attribute defines a different taxonomy for the data (_e.g._, it enables clustering images based on object _shape_, _color_ etc.). The original dataset is limited as it contains only three shapes and two textures, reducing the difficulty of the respective clustering tasks. We introduce _2 new colors_, _7 new shapes_ and _8 new textures_ to the dataset, placing between 1 and 10 objects in each scene.

Each image is therefore parameterized by object _shape_, _texture_, _color_ and _count_. The value for each attribute is sampled uniformly and independently from the others, meaning the image label with respect to one taxonomy gives us no information about the label with respect to another. Note that this sets Clevr-4 apart from existing GCD benchmarks such as CIFAR-100  and FGVC-Aircraft . These datasets only contain taxonomies at different _granularities_, and as such the taxonomies are highly correlated with each other. Furthermore, the number of categories provides no information regarding the specified taxonomy, as all Clevr-4 taxonomies contain \(k=10\) object categories.

Finally, we create GCD splits for each taxonomy in Clevr-4, following standard practise and reserving half the categories for the labelled set, and half for the unlabelled set. We further subsample 50% of the images from the labelled categories and add them to the unlabelled set. We synthesize \(8.4K\) images for GCD development (summarized in table 1), and further make a larger 100\(K\) image dataset available. The full generation procedure is detailed in appendix A.1.

**Performance metrics.** We follow standard practise [7; 47] and report _clustering accuracy_ for evaluation. Given predictions, \(_{i}\), and ground truth labels, \(y_{i}\), the clustering accuracy is \(ACC=_{ S_{h}}}_{i=1}^{N_{v}}\{_{i }=(y_{i})\}\), where \(S_{k}\) is the symmetry group of order \(k\). Here \(N_{u}\) is the number of unlabelled images and the \(\) operation is performed (with the Hungarian algorithm ) to find the optimal matching between the predicted cluster indices and ground truth labels. For GCD models, we also report \(ACC\) on subsets belonging to 'Old' (\(y_{i}_{}\)) and 'New' (\(y_{i}_{}_{}\)) classes. The most important metric is the 'All' accuracy (overall clustering performance), as the precise 'Old' and 'New' figures are subject to the assignments selected in the Hungarian matching.

## 4 Learnings from Clevr-4 for category discovery

In this section, we use Clevr-4 to gather insights into the category discovery problem. In section 4.1, we assess the limitations of large-scale pre-training for the problem, before using Clevr-4 to examine the weaknesses of existing category discovery methods in section 4.2. Next, in section 4.3, we use our findings to motivate a stronger method for GCD (which we describe in full in section 5), finding that this substantially outperforms implemented baselines on Clevr-4.

### Limitations of large-scale pre-training for category discovery

Here, we show that pre-trained representations develop certain 'biases' which limit their performance when used directly or as initialization for category discovery.

**Unsupervised clustering of pre-trained representations (table 2).** We first demonstrate the limitations of unsupervised clustering of features as an approach for category discovery (reporting results with semi-supervised clustering in fig. 11). Specifically, we run \(k\)-means clustering  on top of features extracted with self- [9; 14; 21], weakly- , and fully-supervised [2; 3; 58] backbones, reporting performance on each of the four taxonomies in Clevr-4. The representations are trained on up to 400M images and are commonly used in the vision literature.

    &  &  &  &  \\  Examples & [metal, rubber] & [red,blue] & [torus, cube] &  \\ \(|_{}|\) & 5 & 5 & 5 & 5 \\ \(|_{}|\) & 10 & 10 & 10 & 10 \\  \(|_{}|\) & 2.1\(K\) & 2.3\(K\) & 2.1\(K\) & 2.1\(K\) \\ \(|_{}|\) & 6.3\(K\) & 6.1\(K\) & 6.4\(K\) & 6.3\(K\) \\ \(|_{}|+|_{}|\) & 8.4\(K\) & 8.4\(K\) & 8.4\(K\) & 8.4\(K\) \\   

Table 1: **Clevr-4 statistics for the different splits of the dataset. Note that the _same data_ must be classified along independent taxonomies in the different splits.**We find that most models perform well on the _shape_ taxonomy, with DINOV2 almost perfectly solving the task with 98% accuracy. However, none of the models perform well across the board. For instance, on some splits (_e.g._, _color_), strong models like DINOV2 perform comparably to random chance. This underscores the utility of Clevr-4 for delineating category discovery from standard representation learning. Logically, it is _impossible_ for unsupervised clustering on _any_ representation to perform well on all tasks. After all, only a single clustering of the data is produced, which cannot align with more than one taxonomy. We highlight that such limitations are _not_ revealed by existing GCD benchmarks; on the CUB benchmark (see table 5), unsupervised clustering with DINOV2 achieves 68% ACC (\( 140\) random).

**Pre-trained representations for category discovery (table 3).** Many category discovery methods use self-supervised representation learning for initialization in order to leverage large-scale pre-training, in the hope of improving downstream performance. However, as shown above, these representations are biased. Here, we investigate the impact of these biases on a state-of-the-art method in generalized category discovery, SimGCD . SimGCD contains two main loss components: (1) a contrastive loss on backbone features, using self-supervised InfoNCE  on all data, and supervised contrastive learning  on images with labels available; and (2) a contrastive loss to train a classification head, where different views of the same image provide pseudo-labels for each other. For comparison, we initialize SimGCD with a lightweight ResNet18 trained scratch; a ViT-B/16 pre-trained with masked auto-encoding ; and a ViT-B/14 with DINOV2  initialization. For each initialization, we sweep learning rates and data augmentations.

Surprisingly, and in stark contrast to most of the computer vision literature, we find inconsistent gains from leveraging large-scale pre-training on Clevr-4. For instance, on the _count_ taxonomy, pre-training gives substantially _worse_ performance that training a lightweight ResNet18 from scratch. On average across all splits, SimGCD with a randomly initialized ResNet18 actually performs best. Generally, we find that the final category discovery model inherits biases built into the pre-training, and can struggle to overcome them even after finetuning. Our results highlight the importance of carefully selecting the initialization for a given GCD task, and point to the utility of Clevr-4 for doing so.

### Limitations of existing category discovery methods

Next, we analyze SimGCD , the current state-of-the-art for the GCD task. We show that on Clevr-4 it is not always better than the GCD baseline  which it extends, and identify the source of this issue in the generation of the pseudo-labels for the discovered categories. In more detail, the

   Pre-training Method & Pre-training Data & Backbone & Texture & Shape & Color & Count & Average \\  SWaV  & ImageNet-1K & ResNet50 & 13.1 & 65.5 & 12.1 & **18.9** & 27.4 \\ MoCoV2  & ImageNet-1K & ResNet50 & 13.0 & 77.5 & 12.3 & 18.8 & 30.4 \\ Supervised  & ImageNet-1K & ResNet50 & 13.2 & 76.8 & 15.2 & 12.9 & 29.5 \\ Supervised  & ImageNet-1K & ConvNeXT-B & 13.4 & 83.5 & 12.1 & 13.1 & 30.5 \\  DINO  & ImageNet-1K & ViT-B/16 & **16.0** & 86.2 & 11.5 & 13.0 & 31.7 \\ MAE  & ImageNet-1K & ViT-B/16 & 15.1 & 13.5 & **64.7** & 13.9 & 26.8 \\ iBOT  & ImageNet-1K & ViT-B/16 & 14.4 & 85.9 & 11.5 & 13.0 & 31.2 \\  CLIP  & WIP-400M & ViT-B/16 & 12.4 & 78.7 & 12.3 & 17.9 & 30.3 \\ DINOV2  & LVD-142M & ViT-B/14 & 11.6 & **98.1** & 11.6 & 12.8 & **33.5** \\ Supervised  & ImageNet-21K & ViT-B/16 & 11.8 & 96.2 & 11.7 & 13.0 & 33.2 \\   

Table 2: **Unsupervised clustering accuracy (ACC) of pre-trained models on Clevr-4.** We find most models are strongly biased towards _shape_, while MAE  exhibits a _color_ bias.

   Method & Backbone & Pre-training (Data) & Texture & Shape & Color & Count & Average \\  SimGCD & ResNet18 & - & 58.1 & 97.8 & 96.7 & **67.6** & **80.5** & **2.0** \\ SimGCD & ViT-B/16 & MAE  (ImageNet-1k) & 54.1 & 99.7 & **99.9** & 53.0 & 76.7 & **2.0** \\ SimGCD & ViT-B/14 & DINOV2  (LVD-142M) & **76.5** & **99.9** & 87.4 & 51.3 & 78.8 & **2.0** \\   

Table 3: **Effects of large-scale pre-training on category discovery accuracy (ACC) on Clevr-4.** Contrary to dominant findings in the vision literature, we find that large-scale pre-training provides inconsistent gains on Clevr-4. For instance, on _count_, training a lightweight model (ResNet18) from scratch substantially outperforms initializing from DINOV2 (ViT-B/14) trained on 142M images.

GCD baseline uses only one of the two losses used by SimGCD, performing contrastive learning on features, followed by simple clustering in the models' embedding space. To compare SimGCD and GCD, we start from a ResNet18 feature extractor, training it from scratch to avoid the potential biases identified in section 4.1. We show results in table 4, reporting results for 'All', 'Old' and 'New' class subsets. We follow standard practise when reporting on synthetic data and train all methods with five random seeds [34; 35] (error bars in the appendix B.1), and sweep hyper-parameters and data augmentations for both methods. We also train a model with full supervision and obtain 99% average performance on Clevr-4 (on independent test data), showing that the model has sufficient capacity.

Overall, we make the three following observations regarding the performance of GCD and SimGCD on Clevr-4: (i) Both methods' performance on _texture_ and _count_ is substantially worse than on _shape_ and _color_. (ii) On the harder _texture_ and _count_ splits, the GCD baseline actually outperforms the SimGCD state-of-the-art. Given that SimGCD differs from GCD by adding a classification head and corresponding loss, this indicates that jointly training classifier and feature-space losses can hurt performance. (iii) Upon closer inspection, we find that the main performance gap on _texture_ and _count_ comes from accuracy on the 'New' categories; both methods cluster the 'Old' categories almost perfectly. This suggests that the 'New' class pseudo-labels from SimGCD are not strong enough; GCD, with no (pseudo-)supervision for novel classes, achieves higher clustering performance.

### Addressing limitations in current approaches

Given these findings, we seek to improve the quality of the pseudo-labels for 'New' categories. Specifically, we draw inspiration from the mean-teacher setup for semi-supervised learning , which has been adapted with minor changes in many self-supervised frameworks [14; 18; 24]. Here, a'student' network is supervised by class pseudo-labels generated by a 'teacher'. The teacher is an identical architecture with parameters updated with the Exponential Moving Average (EMA) of the student. The intuition is that the slowly updated teacher is more robust to the noisy supervision from pseudo-labels, which in turn improves the quality of the pseudo-labels themselves. Also, rather than _jointly optimizing_ both SimGCD losses, we first train the backbone _only_ with the GCD baseline loss, before _finetuning_ with the classification head and loss.

These changes, together with careful consideration of the data augmentations, give rise to our proposed \(\)GCD (mean-GCD) algorithm, which we fully describe next in section 5. Here, we note the improvements that this algorithm brings in Clevr-4 on the bottom line of table 4. Overall, \(\)GCD outperforms SimGCD on three of the four Clevr-4 taxonomies, and further outperforms SimGCD by nearly 5% on average across all splits. \(\)GCD underperforms SimGCD on the _shape_ split of Clevr-4 and we analyse this failure case in the appendix B.2.

## 5 The \(\)GCD algorithm

In this section, we detail a simple but strong method for GCD, \(\)GCD, already motivated in section 4 and illustrated in fig. 2. In a first phase, the algorithm proceeds in the same way as the GCD baseline , learning the representation. Next, we append a classification head and fine-tune the model with a'mean teacher' setup , similarly to SimGCD but yielding more robust pseudo-labels.

Concretely, we construct models, \(f_{}\), as the composition of a feature extractor, \(\), and a classification head, \(g\). \(\) is first trained with the representation learning framework from  as described above, and the composed model gives \(f=g\) with values in \(^{k}\), where \(k\) is the total number of categories in the dataset. Next, we sample a batch of images, \(\), and generate two random augmentations of

   &  &  &  &  &  &  \\   & & All & Old & New & All & Old & New & All & Old & New & All & Old & New & All \\  Fully supervised & ResNet18 & 99.1 & – & – & 100.0 & – & – & 100.0 & – & – & – & 96.8 & – & – & **99.0** \\  GCD & ResNet18 & 62.4 & 97.5 & 45.3 & 93.9 & 99.7 & 90.5 & 90.7 & 95.0 & 88.5 & 71.9 & 96.4 & 60.1 & 79.7 \\ SimGCD & ResNet18 & 58.1 & 95.0 & 40.2 & **97.8** & 98.9 & **97.2** & 96.7 & 99.9 & 95.1 & 67.6 & 95.7 & 53.9 & 80.1 \\  \(\)GCD (Ours) & ResNet18 & **69.8** & **99.0** & **55.5** & 94.9 & **99.7** & 92.1 & **99.5** & **100.0** & **99.2** & **75.5** & **96.6** & **65.2** & **84.9** \\  

Table 4: **Category discovery accuracy (ACC) on Clevr-4.** Compared to our reimplementation of the GCD baseline  and SimGCD state-of-the-art , our method provides substantial boosts on average across Clevr-4. Results are averages across five random seeds. Also shown is the classification accuracy of a fully-supervised upper-bound on a disjoint test-set.

every instance. We pass one view through the student network \(f_{_{S}}\), and the other through the teacher network \(f_{_{T}}\), where \(_{S}\) and \(_{T}\) are the network parameters of the student and teacher, respectively. We compute the cross-entropy loss between the (soft) teacher pseudo-labels and student predictions:

\[^{u}(_{S};)=-|}_{ }_{T}(),\,(_{S}()), _{*}()=(f_{_{*}}();_{*}),\] (1)

where \(_{*}()^{k}\) are the softmax outputs of the student and teacher networks, scaled with temperature \(_{*}\). We further use labelled instances in the batch with a supervised cross-entropy component as:

\[^{s}(_{S};_{})=-_ {}|}_{i_{}}(),\, (_{S}()),\] (2)

where \(_{}\) is the labelled subset of the batch and \(()\{0,1\}^{k}\) is the one-hot class label of the example \(\). Finally, we add a mean-entropy maximization regularizer from  to encourage pseudo-labels for all categories:

\[^{r}(_{S})=-}_{S},\,(}_{S}) ,}_{S}=|}_{}_{S}().\] (3)

The student is trained with respect to the following total loss, given hyper-parameters \(_{1}\) and \(_{2}\): \((_{S};)=(1-_{1})^{u}(_{S} ;)+_{1}^{s}(_{S};_{}) +_{2}^{r}(_{S}).\) The teacher parameters are updated as the moving average \(_{T}=(t)_{T}+(1-(t))_{S},\) where \((t)\) is a time-varying momentum.

**Augmentations.** While often regarded as an 'implementation detail', an important component of our method is the careful consideration of augmentations used in the computation of \(^{u}\). Specifically, on the SSB, we pass different views of the same instance to the student and teacher networks. We generate a _strong_ augmentation which is passed to the student network, and a _weak_ augmentation which is passed to the teacher, similarly to . The intuition is that, while contrastive learning benefits from strong data augmentations [9; 14], we wish the teacher network's predictions to be as stable as possible. Meanwhile, on Clevr-4, misaligned data augmentations -- _e.g._, aggressive cropping for _count_, or color jitter for _color_ -- substantially degrade performance (see appendix B.5).

**Architecture.** We adopt a 'cosine classifier' as \(g\), which was introduced in  and leverages \(L^{2}\)-normalized weight vectors and feature representations. While it has been broadly adopted for many tasks [8; 9; 19; 25; 43], we demonstrate _why_ this component helps in section 7.1. We find that normalized vectors are important to avoid collapse of the predictions to the labelled categories.

## 6 Results on real data

**Datasets.** We compare \(\)GCD against prior work on the standard Semantic Shift Benchmark (SSB) suite . The SSB comprises three fine-grained evaluations: CUB , Stanford Cars  and

Figure 2: **Our ‘\(\)GCD’ method. We begin with representation learning from the GCD baseline, followed by finetuning in a mean-teacher style setup. Here, a ‘teacher’ provides supervision for a ‘student’ network, and maintains parameters as the exponential moving average (EMA) of the student.**

FGVC-Aircraft . Though the SSB datasets do not contain independent clusterings of the same images (as in Clevr-4) the evaluations do have well-defined taxonomies -- _i.e._ birds, cars and aircrafts. Furthermore, the SSB contains curated novel class splits which control for semantic distance with the labelled set. We find that coarse-grained GCD benchmarks do not specify clear taxonomies in the labelled set, and we include a long-tailed evaluation on Herbarium19  in appendix C.3.

Model initialization and compared methods.The SSB contains fine-grained, object-centric datasets, which have been shown to benefit from greater shape bias . Prior GCD methods [7; 52; 63] initialize with DINO  pretraining, which we show in table 2 had the strongest shape bias among self-supervised models. However, the recent DINOV2  demonstrates a substantially greater shape bias. As such, we train our model both with DINO and DINOV2 initialization, further re-implementing GCD baselines [7; 68] and SimGCD  with DINOV2 for comparison.

Implementation details.We implement all models in PyTorch  on a single NVIDIA P40 or M40. Most models are trained with an initial learning rate of 0.1 which is decayed with a cosine annealed schedule . For our EMA schedule, we ramp it up throughout training with a cosine function : \((t)=_{T}-(1-_{base})(()+1)/2\). Here \(t\) is the current epoch and \(T\) is the total number of epochs. Differently, however, to most self-supervised learning literature , we found a much lower initial decay to be beneficial; we ramp up the decay from \(_{base}=0.7\) to \(_{T}=0.999\) during training. Further implementation details can be found in appendix E.

### Discussion.

In table 5, we find that \(\)GCD outperforms the existing state-of-the-art, SimGCD , by over 2% on average across all SSB evaluations when using DINO initialization. When using the stronger DINOV2 backbone, we find that the performance of the simple \(k\)-means baseline nearly doubles in accuracy, substantiating our choice of shape-biased initialization on this object-centric evaluation. The gap between the GCD baseline  and the SimGCD state-of-the-art  is also reduced from over 10% to under 5% on average. Nonetheless, our method outperforms SimGCD by over 3% on average, as well as on each dataset individually, setting a new state-of-the-art.

    &  \\   & All & Old & New \\  \(\)GCD (Ours) & **65.7** & 68.0 & **64.6** \\  (1) W/o GCD init. & 61.7 & 66.2 & 59.6 \\ (2) W/o stronger student augmentation & 58.1 & **72.5** & 50.9 \\  (3) With \(_{t}:=1\) & 1.6 & 1.1 & 1.8 \\ (4) With \(_{t}:=0.0\) & 62.7 & 66.4 & 60.9 \\ (5) With \(_{t}:=0.7\) & 64.1 & 65.1 & 63.6 \\  (6) W/o cosine classifier & 54.9 & 64.2 & 50.3 \\ (7) W/o ME-Max regularizer & 42.0 & 41.8 & 42.1 \\  

Table 6: **Ablations. We find that a proper initialization, momentum decay schedule, and augmentation strategy are critical to strong performance.**

    &  &  &  & Average \\   & & All & Old & New & All & Old & New & All & Old & New & All \\  \(k\)-means  & DINO & 34.3 & 38.9 & 32.1 & 12.8 & 10.6 & 13.8 & 16.0 & 14.4 & 16.8 & 21.1 \\ RankStats+  & DINO & 33.3 & 51.6 & 24.2 & 28.3 & 61.8 & 12.1 & 26.9 & 36.4 & 22.2 & 29.5 \\ UNO*  & DINO & 35.1 & 49.0 & 28.1 & 35.5 & 70.5 & 18.6 & 40.3 & 56.4 & 32.2 & 37.0 \\ ORCA  & DINO & 35.3 & 45.6 & 30.2 & 23.5 & 50.1 & 10.7 & 22.0 & 31.8 & 17.1 & 26.9 \\ GCD  & DINO & 51.3 & 56.6 & 48.7 & 39.0 & 57.6 & 29.9 & 45.0 & 41.1 & 46.9 & 45.1 \\ XCon  & DINO & 52.1 & 54.3 & 51.0 & 40.5 & 58.8 & 31.7 & 47.7 & 44.4 & 49.4 & 46.8 \\ OpenCon  & DINO & 54.7 & 63.8 & 54.7 & 49.1 & 78.6 & 32.7 & - & - & - & - \\ MIB  & DINO & 62.7 & 75.7 & 56.2 & 43.1 & 66.9 & 31.6 & - & - & - & - \\ PromptCAL  & DINO & 62.9 & 64.4 & 62.1 & 50.2 & 70.1 & 40.6 & 52.2 & 52.2 & 52.3 & 55.1 \\ SimGCD  & DINO & 60.3 & 65.6 & 57.7 & 53.8 & 71.9 & 45.0 & 54.2 & 59.1 & 51.8 & 56.1 \\  \(\)GCD (Ours) & DINO & 65.7 & 68.0 & 64.6 & 56.5 & 68.1 & 50.9 & 53.8 & 55.4 & 53.0 & 58.7 \\  \(k\)-means* & DINOV2 & 67.6 & 60.6 & 71.1 & 29.4 & 24.5 & 31.8 & 18.9 & 16.9 & 19.9 & 38.6 \\ GCD* & DINOV2 & 71.9 & 71.2 & 72.3 & 65.7 & 67.8 & 64.7 & 55.4 & 47.9 & 59.2 & 64.3 \\ SimGCD* & DINOV2 & 71.5 & **78.1** & 68.3 & 71.5 & 81.9 & 66.6 & 63.9 & **69.9** & 60.9 & 69.0 \\  \(\)GCD (Ours) & DINOV2 & **74.0** & 75.9 & **73.1** & **76.1** & **91.0** & **68.9** & **66.3** & 68.7 & **65.1** & **72.1** \\   

Table 5: **Category discovery accuracy (ACC) on the Semantic Shift Benchmark . We report results from prior work using DINO initialization , and reimplement GCD baselines and SimGCD with DINOV2 pre-training  (noted with *).**

**Ablations.** We ablate our main design choices in table 6. L(1) shows the importance of pre-training with the GCD baseline loss  (though we find in section 4 that jointly training this loss with the classifier, as in SimGCD , is difficult). L(2) further demonstrates that stronger augmentation for the student network is critical, with a 7% drop in CUB performance without it. L(3)-(5) highlight the importance of a carefully designed EMA schedule, our use of a time-varying decay outperforms constant decay values. This is intuitive as early on in training, with a randomly initialized classification head, we wish for the teacher to be updated quickly. Later on in training, slow teacher updates mitigate the effect of noisy pseudo-labels within any given batch. Furthermore, in L(6)-(7), we validate the importance of entropy regularization and cosine classifiers in category discovery. In section 7.1, we provide evidence as to _why_ these commonly used components [8; 19; 43] are necessary, and also discuss the design of the student augmentation.

## 7 Further Analysis

In this section we further examine the effects of architectural choices in \(\)GCD and other category discovery methods. Section 7.1 seeks to understand the performance gains yielded by _cosine classifiers_, and section 7.2 visualizes the feature spaces learned by different GCD methods.

### Understanding cosine classifiers in category discovery

Cosine classifiers with entropy regularization have been widely adopted in recognition settings with limited supervision [14; 25], including in category discovery [19; 43]. In fig. 3, we provide justifications for this by inspecting the norms of the learned vectors in the final classification layer.

Specifically, consider a classifier (without a bias term) as \(g=^{d k}\), containing \(k\) vectors of \(d\) dimension, one for each output category. In fig. 3, we plot the magnitude of each of these vectors trained with different constraints on CUB  (one of the datasets in the SSB ). Note that the classifier is constructed such that the first 100 vectors correspond to the 'Old' classes, and are trained with ground truth labels. In our full method, with normalized classifiers, the norm of all vectors is enforced to be the unit norm (blue dashed line). If we remove this constraint (solid orange line), we can see that the norms of vectors which are _not_ supervised by ground truth labels (indices 101-200) fall substantially. Then, if we further remove the entropy regularization term (solid green line), the magnitudes of the 'Old' class vectors (indices 1-200) increases dramatically.

This becomes an issue at inference time, with per-class logits computed as:

\[l_{m}=_{m},=|_{m}|||()  m\{1...k\}\]

with the class prediction returned as \( l_{m}\). In other words, we show that without appropriate regularisation, our GCD models trivially reduce the weight norm of 'New' class vectors (\(|_{m}| m>100\)), leaving all images to be assigned to one of the 'Old' classes. The effects of this are visualized in the right panel of fig. 3, which plots the histogram of class predictions for an unregularized GCD classifier. We can see that exactly zero examples are predicted to 'New' classes.

Figure 3: **Left:** Norms of weight vectors in GCD classifiers, with and without regularizations. **Right:** Prediction histogram of a classifier without weight norm and max-entropy regularization.

We further highlight that this effect is obfuscated by the evaluation process, which reports non-zero accuracies for 'New' classes through the Hungarian assignment operation.

### PCA Visualization.

Finally, we perform analysis on the _count_ split of Clevr-4. Uniquely amongst the four taxonomies, the _count_ categories have a clear order. In fig. 4, we plot the first two principal components  of the normalized features of the GCD baseline , SimGCD  and \(\)GCD. It is clear that all feature spaces learn a clear 'number sense'  with image features placed in order of increasing object count. Strikingly, this sense of numerosity is present even beyond the supervised categories (count greater than 5) as a byproduct of a simple recognition task. Furthermore, while the baseline learns elliptical clusters for each category, SimGCD and \(\)GCD project all images onto a one-dimensional object in feature space. This object can could be considered as a'semantic axis': a low-dimensional manifold in feature space, \(^{d}\), along which the category label changes.

## 8 Conclusion and final remarks

**Limitations and broader impacts.** In this paper we presented a synthetic dataset for category discovery. While synthetic data allows precise manipulation of the images, and hence more controlled study of the GCD problem, findings on synthetic data do not always transfer directly to real-world images. For instance, the failure case of \(\)GCD on the _shape_ split of Clevr-4 (see section 4.3) occurs due to some classification vectors being unused (see section 7.1). This ceases to be an issue on real-world data with hundreds of categories (see table 5). In appendix F, we fully discuss the difficulties in developing a dataset like Clevr-4 with real-world data. Furthermore, while GCD has many real-world applications, like any form of unsupervised or partially-supervised machine learning, it can be unreliable and must be applied with caution in practise.

**Remarks on Clevr-4.** We note that Clevr-4 can find broader applicability in related machine learning fields. As examples, the dataset can be used for disentanglement research (see appendix F) and as a simple probing set for biases in representation learning. For instance, we find in section 4.1 that most of the ImageNet trained models are biased towards _shape_ rather than _texture_, which is in contrast to popular findings from Geirhos _et al._. Furthermore, larger models are often explicitly proposed as 'all-purpose' features for 'any task' ; here we find simple tasks (_e.g._, _color_ or _count_ recognition) where initialization with such models hurts performance compared to training from scratch. Note that practical problems -- _e.g._, vehicle re-identification  or crowd counting  -- may require understanding of such aspects of the image.

**Conclusion.** In this paper we have proposed a new dataset, Clevr-4, and used it to investigate the problem of Generalized Category Discovery (GCD). This included probing the limitations of unsupervised representations for the task, as well as for identifying weaknesses in existing GCD methods. We further leveraged our findings, together with consistent trends in related literature, to propose a simple but performant algorithm, \(\)GCD. We find that \(\)GCD not only provides gains on Clevr-4, but further sets a new state-of-the-art o n the standard Semantic Shift Benchmark.

Figure 4: **PCA  of features from the GCD baseline , SimGCD  and \(\)GCD on the _count_ split of Clevr-4. While the baseline learns elliptical clusters for each category, SimGCD and \(\)GCD project images onto a one-dimensional object in feature space, which be considered as a ‘semantic axis’ along which the category changes. Clustering accuracy is reported for ‘All’/‘Old’/‘New’ classes.**