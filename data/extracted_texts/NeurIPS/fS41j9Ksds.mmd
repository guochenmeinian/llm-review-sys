# Look-Ahead Selective Plasticity for

Continual Learning of Visual Tasks

 Rouzbeh Meshkinnejad

Department of Computer Science

Western University

London, ON N6A3K7

{rmeshkin}@uwo.ca

Jie Mei

Department of Computer Science

Western University

London, ON N6A3K7

{jie.mei}@it-u.at

Zeduo Zhang

Department of Computer Science

Western University

London, ON N6A3K7

{zzhan762}@uwo.ca

Daniel Lizotte

Department of Computer Science

Western University

London, ON N6A3K7, Canada

{dlizotte}@uwo.ca

Yalda Mohsenzadeh

Department of Computer Science

Western University

London, ON N6A3K7, Canada

{ymohsenz}@uwo.ca

Also affiliated with Department of Anatomy, University of Quebec in Trois-Rivieres and Interdisciplinary Transformation University (ITU) Austria.Also affiliated with Department of Anatomy, University of Quebec in Trois-Rivieres and Interdisciplinary Transformation University (ITU) Austria.The source code is available here.

###### Abstract

Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place at task boundaries, i.e., when one task ends and another begins. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method uses the first few samples of the new task to identify and retain parameters that contribute most to the transferability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet, and demonstrate state-of-the-art performance in task-incremental, class-incremental, and domain-incremental continual learning scenarios. 2

## 1 Introduction

Deep neural networks (DNNs) have solved a variety of computer vision tasks with high performance. While this feat has been achieved by accessing large and diverse datasets, in many practical scenarios, the data is not initially available in its entirety and becomes available over time, potentially containing new unseen classes and different target distributions. When presented with a sequence of classificationtasks to learn and remember, DNNs suffer from a well-known catastrophic forgetting problem , abruptly losing their performance on previous classification datasets. To address this problem, various continual learning algorithms have been proposed.

Regularization-based methods aim to preserve knowledge from previous tasks by applying penalties to changes in weights or functions. Techniques like Elastic Weight Consolidation (EWC) , Synaptic Intelligence (SI) , RWalk , XKFAC , and one similar to our work, Attention-based Structural Plasticity  penalize parameter changes based on their importance to past tasks but often fail to consider the relevance of these parameters for future tasks. Other approaches, such as ResCL , AFEC , and , mitigate intransiance by renormalizing new task solutions with the old model. Techniques like Learning without Forgetting (LwF) , LwM , FROMP , and DRI  use knowledge distillation to apply regularization to intermediate or final outputs of the prediction function. Methods like NPC  focus on network dynamics, slowing down the learning rate for important neurons. However, despite these efforts, regularization-based methods still struggle with forward transfer, scalability, and inter-task interference in complex scenarios.

Rehearsal-based methods such as iCaRL , GSS , GEM , and CLS-ER , among others  store a small number of training samples from previous tasks in a memory, while other methods such as  train a generative model to produce training samples similar to previous tasks. During the training of the current task, the network is simultaneously trained on the current task samples as well as samples from the memory. A key challenge in these methods is selecting a subset of past samples that best represent all previous tasks. These subsets are often chosen through random sampling , by proximity to class means , or using more advanced, gradient-based optimization techniques . In this work, we will use simple rehearsal and replay of samples as part of our proposed method to mitigate forgetting.

While methods that use shared parameters may lead to inter-task interference, architecture-based approaches address this by assigning separate network components through parameter isolation  and dynamic architectures  for different tasks. Parameter isolation methods assign a distinct subset of the network's parameters to each task. For instance, H\({}^{2}\) performs element-wise parameter selection based on sensitivity measures, combined with a model search that utilizes continuous relaxation to explore the optimal architecture. Dynamic architecture methods either expand task-specific components as tasks increase, such as in , or use parallel sub-networks  to separate tasks. While these approaches effectively prevent interference between tasks, they come with significant trade-offs. Dynamic architectures, require increasing memory and thus computational complexity as tasks accumulate which can limit scalability in real-world applications. In contrast, fixed network architectures that isolate parameters for different tasks offer a more memory-efficient solution but sacrifice flexibility, as they restrict the model's ability to reuse parameters across tasks.

Recent work has advanced beyond traditional methods by leveraging self-supervised learning  and pre-trained networks  to develop robust representations, or by adjusting the optimization process . Additionally, some methods  combine different strategies to create more effective solutions. For example,  applies regularization to task-specific latent distributions and replays both past inputs and distributions. Co\({}^{2}\)L integrates supervised contrastive learning for individual tasks with a self-supervised loss to transfer knowledge between old and new models.

There are also promising meta-learning approaches to continual learning, such as Meta Experience Replay (MER) , Online Meta-Learning (OML) , the Neuromodulated Meta-Learning Algorithm (ANML) , and La-MAML .

While the aforementioned continual learning methods are successful to some extent in mitigating forgetting, it is not clear whether regularization or isolation of parameters, distillation, or meta-learning will help in learning new unseen tasks. In fact, in regularization and parameter isolation approaches, parameters are identified as important by some form of evaluation on past tasks, without regard to whether these parameters will transfer to future tasks. Similarly, rehearsal-based approaches rely on some form of regularization or gradient alignment with respect to past task data to achieve good performance. While recent work  considers features learned from new task data, they do not encourage learning of features that generalize to all tasks seen so far and are more likely to transfer. Similarly, recent meta-learning approaches such as La-MAML  use gradient-alignment heuristics to modulate the plasticity of parameters, but pay little attention to redundancy and the contribution of parameters to generalizability, and are computationally expensive compared to other continual learning methods. Thus, there has been a general lack of attention to the transfer of continually learned knowledge to future tasks. To our interest, the recent approach named Co\({}^{2}\)L  questioned whether preserved past knowledge generalizes to future tasks and observed that contrastively learned representations [50; 51] transfer better and forget less, compared to learning based on the cross entropy loss.

Aiming for a continual learning approach that mitigates forgetting while learning representations that transfer well to unseen data, we were inspired to build on the contrastive learning framework [31; 51]. In contrastive learning, we work with an encoder mapping input images to vectors (_representations_), a projection head mapping representations to vectors (called _embeddings_) on which a contrastive loss is defined, and a decoder (linear transformation) mapping the extracted representations to class probabilities at inference time. We build our approach around Co\({}^{2}\)L , but importantly, we will selectively regularize the produced embeddings and network parameters based on how likely they are to transfer to future tasks. In doing so, we revisit assumptions about access to data at each point in time, and outline our inspiration from event models theorized to enable update of context representations in the brain.

**Task Boundaries and Event Models:** Events are based on how we understand the world around us. While the world appears to be a continuous stream of twists and turns, evidence suggests that we perceive it as discrete events at different spatiotemporal scales [52; 53; 54]. The brain has been theorized to operate and make sense of the world by updating and maintaining representations of the current situation, also known as _event models_[52; 53; 54]. Inspiring our work, event models are believed to be updated mainly at _event boundaries_[52; 53; 54]. These boundaries are thought to be detected by an increase in perceptual prediction error, i.e., when the brain's visual model makes predictions about the world that start to diverge from what is actually happening [52; 53; 54]. Interestingly, the said boundaries also exist in the field of continual learning at the moment the first batch of new task data arrives (or at any time when the model's prediction accuracy drops significantly). We will refer to these boundaries as _task boundaries_. While performing various types of computations during task boundaries is not new in continual learning, methods that perform such computations (e.g., EWC ) do not make use of all the information available at task boundaries. In the specific case of EWC , a regularization strength for each network parameter is computed using the data from the previous tasks, ignoring the first batch of data from the new task. To date, continual learning approaches have been focused on using previous task data and models to overcome forgetting. Assuming a stream of data where the data distribution changes, we can mark each time the model's performance on a batch of data drops as a task boundary and assign the data before this batch to the previous task. Consequently, the batch of data on which the model did not perform well will belong to a new task.

**Redundancy in Contrastive Learning:** Recent work suggests that most continual learning methods favor stability over plasticity, that is, they focus on not forgetting past tasks by preserving learned parameters and sacrificing flexibility to learn new knowledge . Thus, it is advantageous to introduce less regularization into continual learning methods by retaining only parts of the learned network that are essential for performance on previous tasks _and_ produce highly generalizable representations. Research on the properties of learned representations and the projection head of networks trained by contrastive loss has shown that over-parameterized (and sufficiently wide) neural networks learn embeddings with redundancy [56; 57; 58]. Specifically, the vector space in which the contrastive loss is defined is thought to suffer from a dimensional collapse problem [57; 58], i.e., the produced embeddings are in a lower-dimensional subspace of their nominal dimensionality. While this has been identified as an inefficiency in the normal supervised learning setting [57; 58], it provides an opportunity for continual learning: _regularization of DNN outputs can be defined only on parts of the embeddings instead of their entirety_. Similar to , we observe that a small subset of contrastively learned embeddings (i.e., a subset of output neurons combined) is able to replicate the performance of the entirety of embeddings on previous tasks. Moreover, we observe that different subsets of the embeddings of a DNN perform differently. By sampling random subsets of the embeddings produced by a DNN and evaluating them on previous and future tasks, we see that the variation in performance between subsets is higher on future tasks. These observations motivated us to define loss/regularization only on a small subset of the network's outputs, chosen so that it's likely to transfer to future tasks.

To select a highly generalizable subset, we propose to evaluate the network on the first batch of new task data (as a surrogate for unseen future data) during task boundaries. We introduce a novel procedure to identify the parts of the embeddings that perform best (a subset), and a novel loss to regularize this high-performing subset. We then introduce a novel extension of the excitation backprop  to measure the contribution of each network parameter in producing the identified subset, and a novel method to modulate the gradients based on this contribution. We will describe the details of our methods in the Methods section, followed by the experimental setup and results. In the ablation studies, we will justify our design choices and conclude with a discussion of the methods used and how they can be improved in the future.

## 2 Methods

We will use Co\({}^{2}\)L  as our baseline and briefly review its methods. We will then build our proposed methods around it.

**Continual Learning Settings:** In continual learning,a model is trained on a sequence of tasks \(_{1},_{2},...,_{n}\). Each task is defined by its corresponding input and target datasets \((X_{t},Y_{t})\) which are drawn from a task-specific distribution \(D_{t}\). Continual learning is mainly studied in three settings: task-incremental, class-incremental, and domain-incremental. In the task-incremental setting, the samples in each task are accompanied by a task identifier. As a result, during inference, a model can use the task identifier to drastically constrain target predictions. In the class- and domain-incremental settings, there is no knowledge of the task identifier at inference time, and the targets to be predicted can be any of the classes seen so far by the model. While the set of target classes in each task is disjoint in the task- and class-incremental settings, the set of classes remains the same in the domain-incremental setting (the \(Y_{t}\) distribution stays the same while the \(X_{t}\) distribution varies).

**Contrastive Learning and Co\({}^{2}\)L Overview:** Supervised contrastive learning  generally involves a feature extractor mapping input samples to representations and a projection head mapping representations to embeddings. Formally, denoting a feature extractor parameterized by \(\) as \(f_{}\), representations by \(\), projection head parameterized by \(\) as \(g_{}\), and embeddings by \(\), supervised contrastive learning  and Co\({}^{2}\)L  augment each input sample \(x\) in the minibatch twice to get \(_{1}\) and \(_{2}\), known as views. Representations are generated by passing the views \(_{1}\) and \(_{2}\) to the feature extractor (\(_{1}=f_{}(_{1}),_{2}=f_{}(_{2})\)). Embeddings are then created by passing the extracted representations to the projection head (\(_{1}=g_{}(_{1}),_{2}=g_{}(_{2})\)). Both embeddings and representations are normalized to unit length (\(||=1\), \(||=1\)). A contrastive loss is then defined on these embeddings and used to train the network. In the specific case of Co\({}^{2}\)L , this loss is called the Asymmetric Supervised Contrastive Loss (Async SupCon) and is defined as:

\[^{}_{}=_{i S}_{i}|}_{j_{i}}(_{i} _{j}/)}{_{k i}(_{i}_{ k}/)})\]

where \(S\) includes the index of views in the current task, \(_{i}\) holds the index of views in the minibatch that belong to the same class as the \(i\)th view \(_{i}\) except for \(_{i}\) itself, \(\) is a temperature hyperparameter, and \(_{i}\) is the embedding of the \(i\)th view. To facilitate comparison with previous work, we also use the Async SupCon loss to train the network.

Figure 1: At task boundaries, the feature importance module is added on top of the embeddings to identify the salient subset. The mask marking the salient subset is trained based on a nearest class-mean classifier and regularized to be minimal (criterion 3).

To adapt supervised contrastive learning to solve a continual learning problem and to mitigate forgetting, Co\({}^{2}\)L  uses an instance-wise relation distillation loss (IRD). IRD computes a similarity matrix by measuring the similarity of each view to other views in the minibatch (one row) for both the old model (a snapshot of the current model taken at the start of training on the current task and parameterized by \(\)) and the model currently being trained (parameterized by \(\)). The resulting two similarity matrices are then regularized to be similar to each other. Formally, the similarity of the views \(}\) and \(}\) is computed as follows:

\[R_{,_{1}}[i,j]=(},},_{1},) =_{i}_{j}/_{1})}{_{k i}^{2N} (_{i}_{k}/_{1})}\] (1)

where \([i,j]\) denotes the element in the \(i\)th row and \(j\)th column of the pairwise similarity matrix, \(\) is the similarity function, \(_{1}\) is a temperature hyperparameter, and \(N\) is the number of samples in the minibatch. The IRD loss is then defined as:

\[_{}=_{i=1}^{2N}_{j=1}^{2N}-R_{,_{2}} [i,j](R_{,_{1}}[i,j])\] (2)

We believe that this distillation loss is too restrictive and reduces the model's ability to learn new generalizable representations, since redundant parts of the embeddings are also regularized. We will modify this distillation loss so that it is only applied to a subset of embeddings. This subset will be identified by our novel _feature importance module_ and regularized by the _selective distillation loss_. Similar to rehearsal-based continual learning approaches, we will use a small memory to store samples. The memory size will be similar to previous work for comparison and each class will be assigned an equal amount of memory. In addition, we extend the excitation backprop  framework to measure the contribution of individual network parameters in producing the identified salient subset as its salience. The proposed _gradient modulation_ method will then use these salience values to decrease the gradients for salient network parameters. In the following sections, we will introduce the building blocks for these methods in more detail.

**Proposed Salient Subset Selection:** To improve the IRD loss , we try to identify a subset of the embeddings that satisfies the following criteria and refer to it as _salient_:

1. Transfers better to unseen data than other subsets (based on performance on unseen tasks),
2. Contains more information about past tasks compared to other subsets (based on performance on past tasks),
3. Is minimal, i.e., has no subset that performs as well on past and future tasks.

In the general continual learning formulation, criterion 1 and 2 cannot be evaluated for a subset, since we cannot store all the samples seen so far and future samples are yet to be seen. At a task boundary, however, we can use the samples stored in memory \(\) as a surrogate for past tasks, and the first batch of new task data \(_{t}\) (before the model is trained on it) as a surrogate for future tasks. We can create a dataset \(_{}\) to use for finding the salient subset. This dataset can be created by combining \(\) and \(_{t}\) (the _combined_ setting, \(_{}=_{t}\)), or using memory samples only (the _onlypast_ setting, \(_{}=\)), or using the first batch only (the _onlycurrent_ setting, \(_{}=_{t}\)). These options will be evaluated in the ablation studies.

Identifying a salient subset of the embeddings is essentially a search problem. Here, for simplicity and speed, we adopt an approach similar to a previous work called Neural Similarity Learning , which represents which parts of the embeddings to select. Using the same notation as before, let \(\) be a vector of the same size as \(\), \(\) denotes the sigmoid function, and \(h_{}()\) be a Nearest Class Mean Classifier (NCMC) parameterized by \(\) that takes in embeddings and assigns them to the class with the nearest mean embedding. Before training \(h_{}()\), we need to compute the class means. Let \(_{c}\) denote the samples in the dataset \(_{}\) that belongs to class \(c\), then the mean of class \(c\) (denoted by \(_{c}\)) can be computed as:

\[_{c}=_{c}}g_{}(f_{}(x))}{| _{c}|}\] (3)

where \(x\) is a view of an input sample and \(t\) is the class it belongs to.

To train \(h_{}\) we will minimize the following loss function:

\[_{}(_{})=_{}}()}{|()|} _{i}()}{|_{i}( )|}}{|_{}|}+||_{1}\] (4)

where \(\) is the element-wise product. An \(_{1}\) norm loss (with \(\) as a hyperparameter controlling the strength) is added to \(\) to ensure that it marks a minimal subset (criterion 3). The size of this subset can vary depending on the redundancy of the embeddings. After training the NCMC for a number of randomly initialized mask vectors and selecting the best-performing mask, \(}=()\) can be used to identify which parts of the embedding should be regularized. Compared to neural similarity learning , multiplying \(}\) by the output of the encoder and the class means is similar to implementing a weighted dot product, weights that are used to mask parts of the encoder's output in our case.

**Proposed Selective Distillation:**_Selective Distillation_ modifies the IRD loss so that it is only applied only to the salient subset of the generated embeddings. By selectively applying this distillation loss, we try to retain only parts of the embeddings that are salient, improving the model's flexibility in learning the new task, and promoting transfer and generalizability. Our proposed variant of IRD forms new embeddings \(}\) by taking parts of the embeddings where the mask vector \(}\) is above a threshold (here we simply choose 0.5):

\[}=[} 0.5]}{|[ } 0.5]|}\] (5)

The instance-wise similarity matrix (equation 1) is then computed using the new embeddings:

\[R_{,_{1}}[i,j]=(},},_{1},) =_{i}}_{j}}/_{1})}{_{k  i}^{2N}(_{i}}_{k}}/_{1})}\] (6)

The IRD loss (equation 2) is then calculated using the new instance-wise similarity matrices.

**Salient Parameter Selection:** After identifying the salient subset, the computed salience can be passed down using a novel extension of excitation backprop (EB) . Normally, EB is a method that attributes the _activation_ of a model's output neurons to its _input_. Our goal, however, is different: we want to attribute the _performance_ of salient neurons in the output layer to individual network _parameters_ given a batch of data samples.

Assuming a simple neuron computes \(a_{i}^{l+1}=(_{j}w_{j,i}^{l}\,a_{j}^{l})\) where \(a_{i}^{l+1}\) is the activation value of the \(i\)th neuron in the \((l+1)\)th layer, \(w_{j,i}^{l}\) is the weight connecting the \(j\)th neuron in the \(l\)th layer to the \(i\)th neuron in the \((l+1)\)th layer, and \(\) is a non-linear activation function. EB defines the salience of a neuron's activation as its _winning probability_\(P(a)\). To compute salience, it uses the marginal winning probability (MWP) of a neuron, given neurons in the upper layer:

\[P(a_{j})=_{a_{i}_{j}}P(a_{j}|a_{i})P(a_{i})\] (7)

Figure 2: The instance-wise relation distillation loss (IRD) is applied only to a subset of embeddings deemed salient by the feature importance module.

\(_{j}\) denotes the neurons in the layer above (closer to the output) \(a_{j}\). Under certain assumptions (see , which holds when the ReLU activation function is being used), the MWP for a neuron \(a_{j}^{l}\) can be computed based on the salience of neurons \(a_{i}^{l+1}\) in the upper layer:

\[P(a_{j}^{l}|a_{i}^{l+1})=Z_{i}a_{j}^{l}w_{j,i}^{l}&w_{j,i}^{l} 0,\\ 0&\] (8)

Where \(Z_{i}\) is a normalization factor and is equal to \(^{l} 0}a_{j}^{l}w_{j,i}^{l}}\). Using the MWPs computed from Eq. 8, the salience of each neuron can be computed in top-down order based on Eq. 7.

To attribute the salience of output neurons to the model's _parameters_, similar to  we first use EB to compute the salience for activation maps in each layer. Next, similar to Oja's rule , the salience of each network weight can be computed using the salience of its two ends:

\[(w_{i,j}^{l})=^{l})P(a_{j}^{l+1})}\] (9)

where \(\) represents salience. The output of this _salient parameter selection_ process is essentially a salience value for each network parameter. These salience values will be used in the next step to modulate gradients.

**Gradient Modulation:** Inspired by neuromodulation processes in the brain, where the plasticity of neurons can change depending on the task at hand , we try to limit the change of network weights that are considered salient. In the domain of neural networks, this means modifying gradients so that the more salient a network weight is, the less the gradient is modified. To achieve this, we modify the gradients as follows:

\[d_{w}=d_{w}(1-(1,(w)))\] (10)

where \(d_{w}\) is the gradient with respect to parameter \(w\). This process aims to guide the network during training by shifting its focus on learning the task at hand using parameters that did not contribute to the performance of the salient subset.

## 3 Results

**Evaluating Random Subsets:** We test the hypothesis that it is beneficial to use the first batch of new task data at task boundaries. Specifically, we want to see if subsets of network-generated embeddings have the same discrimination power with respect to past versus future tasks. At each task boundary, we extract 10 random neurons from the network-generated embedding to form a subset. Using only the selected subset, we first train a linear classifier to discriminate between classes in the entirety of past tasks' data, and then train another linear classifier using the same subset of neurons to discriminate between classes in the entirety of unseen tasks' data. We repeat this process 100 times and record the accuracy of the selected subset on past and unseen task data. We then compute the mean and variance of the subset accuracy across these 100 subsets. We observed a higher variance when evaluating on unseen tasks (see appendix A.1 for details), suggesting that the generalizability of subsets varies more than their captured knowledge of past tasks.

**Proposed Method Results:** To allow comparison with previous results , we conduct experiments in the task-incremental, class-incremental, and domain-incremental settings on the CIFAR-10 , TinyImageNet , and R-MNIST datasets  (see A.2 for experimental setup details). We compare our results with rehearsal-based continual learning methods, including Co\({}^{2}\)L , ER , iCaRL , GEM , A-GEM , FDR , GSS , HAL , DER , and DER++ . A low (200 samples) and a high (500 samples) memory setting were considered. Results are the average test-set classification accuracy on all seen classes at the end of training.

We compare the results of our proposed methods with previous work in table 1. We use SD to refer to our selective distillation method, GM to refer to our gradient modulation method when the IRD loss is applied as in  rather than selectively as in SD, and SD + GM to refer to the simultaneous use of gradient modulation and selective distillation. SD is superior to baselines and state-of-the-art for both task and class-incremental settings on the SplitCIFAR10 and SplitTinyImageNet datasets. It also outperforms previous work on the domain-incremental setting on the R-MNIST dataset when using small memory. GM and SD+GM also improved the state-of-the-art on the SplitCIFAR10 dataset, but did not outperform SD. A discussion of GM is provided in appendix A.3. These results show that SDcan successfully mitigate forgetting while freeing up the remaining parts of the model to learn new tasks. In the next section, we will analyze the choice of selecting the salient subset based only on the new batch of data, rather than on memory samples or combined. We will also go over the effect of the embedding size for our method (SD) as it depends on the redundant units in the output of the projection head (embeddings).

## 4 Ablation Studies

**Identifying the Salient Subset of Embeddings, onlycurrent, onlypast, or combined:** Although the proposed methods outperformed published methods, it was unclear which parts of our approach contributed to the performance gain. In salient subset selection, three settings were used to generate \(_{}\). The salient subset was then chosen based on the classification performance of subsets on \(_{}\). Initially, we hypothesized that including the first batch of new task data would help the salient subset selection identify parts of the embeddings that not only perform well on previous tasks but also generalize well to unseen tasks. To examine this hypothesis, we conducted experiments (Results in Table 2) on the SplitTinyImageNet and SplitCIFAR10 datasets in these three settings using the selective distillation method.

For the SplitCIFAR10 dataset, the onlycurrent setting where \(_{}=_{t}\) outperformed the onlypast and combined settings. The lower performance of the combined setting compared to the onlypast setting can be explained by the low number of classes in the CIFAR10 dataset. Added memory samples in the \(_{}\) dataset may be misleading as a significant portion of memory samples will belong to the task the model was just trained on. The performance of various parts of embeddings on the previous task may be less informative as it measures neither resilience to forgetting nor generalizability.

    &  &  &  & R-MNIST \\   & & Class-IL & Task-IL & Class-IL & Task-IL & Domain-IL \\   & ER & 44.79 (1.86) & 91.19 (0.94) & 8.49 (0.16) & 38.17 (2.00) & 93.53 (1.15) \\  & GEM & 25.54 (0.76) & 90.44 (0.94) & - & - & 89.86 (1.23) \\  & A-GEM & 20.04 (0.34) & 83.88 (1.49) & 8.07 (0.08) & 22.77 (0.03) & 89.03 (2.76) \\  & iCaRL & 49.02 (3.20) & 88.99 (2.13) & 7.53 (0.79) & 28.19 (1.47) & - \\  & FDR & 30.91 (2.74) & 91.01 (0.68) & 8.70 (0.19) & 40.36 (0.68) & 93.71 (1.51) \\  & GSS & 39.07 (5.59) & 88.80 (2.89) & - & - & 87.10 (7.23) \\  & HAL & 32.36 (2.70) & 82.51 (3.20) & - & - & 89.40 (2.50) \\  & DER & 61.93 (1.79) & 91.40 (0.92) & 11.87 (0.78) & 40.22 (0.67) & 96.43 (0.59) \\  & DER++ & 64.88 (1.17) & 91.92 (0.60) & 10.96 (1.17) & 40.87 (1.16) & 95.98 (1.06) \\  & Co\({}^{2}\)L & 65.57 (1.37) & 93.43 (0.78) & 13.88 (0.40) & 42.37 (0.74) & 97.90 (1.92) \\  & **SD (ours)** & **73.72 (0.52)** & **96.10 (0.09)** & **16.02 (0.39)** & **44.07 (0.66)** & **98.80 (0.26)** \\  & **GM (ours)** & 71.30 (1.15) & 95.84 (0.25) & 12.46 (0.43) & 38.33 (0.90) & 97.29 (0.59) \\  & **SD + GM (ours)** & 70.64 (0.98) & 95.28 (0.46) & 12.93 (0.55) & 38.47 (0.68) & 96.68 (0.55) \\   & ER & 57.75 (0.27) & 93.61 (0.27) & 9.99 (0.29) & 48.64 (0.46) & 94.89 (0.95) \\  & GEM & 26.20 (1.26) & 92.16 (0.64) & - & - & 92.55 (0.85) \\  & A-GEM & 22.67 (0.57) & 89.48 (1.45) & 8.06 (0.04) & 25.33 (0.49) & 89.04 (7.01) \\  & iCaRL & 47.55 (3.95) & 88.22 (2.62) & 9.38 (1.53) & 31.55 (3.27) & - \\  & FDR & 28.71 (3.23) & 93.29 (0.59) & 10.54 (0.21) & 49.88 (0.71) & 95.48 (0.68) \\  & GSS & 49.73 (4.78) & 91.02 (1.57) & - & - & 89.38 (3.12) \\  & HAL & 41.79 (4.46) & 84.54 (2.36) & - & - & 92.35 (0.81) \\  & DER & 70.51 (1.67) & 93.40 (0.39) & 17.75 (1.14) & 51.78 (0.88) & 97.57 (1.47) \\  & DER++ & 72.70 (1.36) & 93.88 (0.50) & 19.38 (1.41) & 51.91 (0.68) & 97.54 (0.43) \\  & Co\({}^{2}\)L & 74.26 (0.77) & 95.90 (0.26) & 20.12 (0.42) & **53.04 (0.69)** & **98.65 (0.31)** \\  & **SD (ours)** & **76.49 (0.63)** & **96.39 (0.20)** & **21.49 (0.50)** & 52.69 (0.45) & 98.43 (0.38) \\  & **GM (ours)** & 74.63 (0.95) & 96.15 (0.14) & 17.54 (0.44) & 48.21 (0.54) & 97.17 (0.50) \\  & **SD + GM (ours)** & 73.82 (0.42) & 95.67 (0.14) & 19.01 (0.31) & 48.06 (0.71) & 96.49 (1.15) \\   

Table 1: Comparison of our proposed methods with published methods. Proposed methods were run with the onlycurrent setting of salient subset selection and their accuracy was obtained by averaging across 5 independent trials. The highest accuracy is in bold. ‘-’ denotes settings where evaluation was not possible due to incompatibility or intractable training processes. Previous results listed are based on . Data are presented as mean (SD).

Experimenting on the SplitTinyImageNet dataset, we observed that both the onlycurrent and combined settings outperformed the onlypast setting. It is worth emphasizing that the onlycurrent setting outperformed the onlypast setting on both datasets and continual learning scenarios, suggesting that using a batch of new task data may be useful for identifying the salient subset. Also note that all accuracies listed in Table 2 were higher than previous state-of-the-art results , demonstrating that while changing the default continual learning protocol to use the first batch of new task data may improve model performance to some extent, the main performance gains were results of the selective distillation (SD) method itself.

**The Effect of the Embedding Size:** In our first experiments, we noticed that SD outperformed Co\({}^{2}\)L  on all datasets except for SplitTinyImageNet. Our hypothesis was that SD relied on redundancy in the embeddings and when the generated embeddings were dense, it was reasonable to apply the IRD loss on entire embeddings rather than a subset. Moreover, since in contrastive learning the projection head is discarded after training and is generally small (MLP, 512 hidden units, 128 output units in Co\({}^{2}\)L), increasing embedding size to induce redundancy comes with virtually no computational cost, especially at inference time. To test our hypothesis, we compared SD to Co\({}^{2}\)L  with different embedding sizes on the SplitCIFAR10 and SplitTinyImageNet datasets. For SplitCIFAR10, the embeddings seemed to be dense when the embedding size was around 16 and started to involve some redundancy starting from 32 units in the output (figure 3 left). As we increased the embedding size starting with 32 units, we noticed that SD consistently outperforms Co\({}^{2}\)L .

When testing our hypothesis on the SplitTinyImageNet dataset (which is generally more difficult to solve with 200 classes), we noticed that embeddings appeared to be dense until an embedding size of 256 and SD was unable to outperform Co\({}^{2}\)L . However, with an embedding size of 512, redundancy began to materialize in embeddings and SD achieved higher task- and class-incremental accuracy (figure 3 right). We did not increase the embedding size further as it would have gotten larger than the hidden layer's size and could have caused complications unrelated to this ablation

   Dataset &  &  \\  Setting & Class-IL & Task-IL & Class-IL & Task-IL \\    onlypast \\ combined \\  & 75.33 (0.53) & 96.28 (0.15) & 21.42 (0.25) & 52.64 (0.55) \\  combined \\  & 75.20 (0.88) & 96.29 (0.17) & 22.07 (0.37) & 52.78 (0.35) \\ 
 onlycurrent \\  & 76.49 (0.63) & 96.39 (0.20) & 21.49 (0.50) & 52.69 (0.45) \\   

Table 2: Comparison of SD performance for different settings of the salient subset selection process. The onlycurrent setting uses the first batch of the new task, onlypast uses samples in the memory, and combined uses both for identification of the salient subset in embeddings. Adding the first batch of new task data improves SD performance in virtually all scenarios and datasets. Five independent experiments were conducted for each case to report the mean and variance. A memory buffer of 500 samples was used in all experiments.

Figure 3: Comparing SD (ours) and Co\({}^{2}\)L  using different embeddings sizes on the SplitCIFAR10 and SplitImagenNet datasets. The memory size is the same (500) for both methods. Shading depicts standard deviation. Increasing embedding size and redundancy benefits SD on both datasets.

study. Overall, these results showed that as the embedding size grows larger, SD can leverage the increased redundancy and improve continual learning performance in both task and class-incremental settings.

## 5 Conclusion and Future Work

Inspired by event models, we proposed a different way of looking at the continual learning setting, focusing on task boundaries. We hypothesized that the first batch of new task data could be used to identify parts of the neural network that enable generalization to unseen tasks. Observing the redundancy-inducing effects of the contrastive loss on embeddings, we first introduced a salient subset selection process to identify a subset that performs similarly to the full set of embeddings. Secondly, we presented a selective distillation method that regularizes only the salient parts of the embeddings. Thirdly, we introduced an attribution method that assigned salience to network parameters based on their contribution to the computation of the salient subset. Fourthly, we proposed a gradient modulation method that modified gradients according to the salience of parameters. Our methods did not increase parameters linearly with the number of tasks, nor did they assume that additional memory was available in the form of a second snapshot of the model or more samples in memory. Moreover, in alignment with our hypothesis, the selective distillation method was able to leverage redundancy in the embeddings and demonstrated superior performance compared to previous work. Further studies on the properties of projection heads in representation learning can open new avenues for methods like selective distillation to better separate task-specific knowledge. Additionally, modifications to our gradient modulation technique present a promising direction. An avenue for improvement involves adjusting gradient modulation to induce redundancy in a layer-wise manner, aligning the degree of parameter regularization with each layer's role in learning new tasks. Such adjustments would enhance knowledge consolidation across the network while preserving plasticity, thereby reducing forgetting and ultimately improving the overall performance of continual learning.