# Direct Acquisition Optimization for

Low-Budget Active Learning

 Zhuokai Zhao\({}^{1}\)  Yibo Jiang\({}^{1}\)  Yuxin Chen\({}^{1}\)

\({}^{1}\)Department of Computer Science, University of Chicago,

Correspondence to zhuokai@uchicago.edu

###### Abstract

Active Learning (AL) has gained prominence in integrating data-intensive machine learning models into domains with limited labeled data. However, its effectiveness diminishes significantly when the labeling budget is especially low. In this paper, we empirically verify the performance degradation of existing AL algorithms in the extremely low-budget settings, and then introduce _Direct Acquisition Optimization_ (DAO), a novel AL algorithm that optimizes sample selections based on expected true loss reduction. Specifically, DAO utilizes influence functions to update model parameters and incorporates an additional acquisition strategy to mitigate bias in loss estimation. This approach facilitates efficient estimation of the overall error reduction, without extensive computations or reliance on labeled data. Experiments demonstrate the effectiveness of DAO in low-budget settings, outperforming state-of-the-arts approaches across seven benchmarks.

## 1 Introduction

Active learning (AL) explores how adaptive data collection can reduce the amount of data required by machine learning models, making it particularly valuable when labeled data is scarce or expensive . It is especially crucial for modern deep learning (DL) models as they are often data-hungry, and labeling can be cost-prohibitive . AL algorithms strategically select the most beneficial data points for labeling, maximizing training efficiency even with limited data, and have been widely applied in fields such as medical imaging , astronomy , and physics . In these cases, selecting samples for labeling can substantially reduce costs .

Over the years, various AL algorithms have emerged, from early contributions  to more recent approaches targeting deep learning models . AL algorithms generally fall into two categories: heuristic-based objectives that differ from evaluation metrics, like diversity  and uncertainty , and methods that directly optimize evaluation metrics, such as expected error reduction (EER)  and its successors . Despite the popularity of heuristic methods, research and empirical analysis  show these approaches often fail in low-budget settings, where less than 1% of data can be labeled. Algorithms that directly target error reduction, like EER, are computationally expensive and require retraining the model for each candidate, making them impractical for deep networks. Others, like GLISTER , require labeled validation sets, posing challenges in data-scarce cases where labeled data is too valuable to reserve for AL algorithms.

To address these limitations, we propose **Direct Acquisition Optimization (DAO)**, a novel AL algorithm that selects samples by efficiently estimating expected loss reduction, without relying on a labeled set. DAO overcomes the runtime challenges of methods like EER and GLISTER by leveraging influence functions  for model updates and employing a more efficient unbiased estimator of loss reduction through importance-weighted sampling. Our contributions include a novel, efficient AL algorithm that optimizes sample selection based on expected error reduction whileavoiding labeled set dependencies. Extensive experiments show that DAO outperforms popular AL methods in low-budget scenarios across seven benchmarks.

## 2 Methodology

Different from the heuristics-based AL algorithms that optimize criteria such as diversity or uncertainty, DAO is built upon the EER formulation with the selection objective being the largest reduced error evaluated on the entire unlabeled set. More specifically, DAO majorly improves upon two aspects: (1) instead of re-training the classifier, we employ influence function , a concept with rich history in statistical learning, to formulate the new candidate sample as a small perturbation to the existing labeled set, so that the model parameters can be estimated without re-training; and (2) instead of reserving a separate, relatively large labeled set for validation , we sample a very small subset directly from the _unlabeled_ set and estimate the loss reduction through bias correction.

Essentially, when considering each candidate from the unlabeled set, we optimize the EER framework on two of its core components, which are model parameter update and true loss estimation. Additionally, we upgrade EER, which only supports single sequential acquisition, to offer DAO in both single and batch acquisition variants by incorporating stochastic samplings to the sorted estimated loss reductions. We illustrate our algorithmic framework in Fig. 1.

### Problem Statement

The optimal sequential active learning acquisition function can be formulated as selecting a budget number of samples \(_{t}^{}\) from the current unlabeled set \(_{t}\) at each round \(t\) such that

\[_{t}^{}=*{arg\,min}_{_{_{i}}_{t-1}}_{(y_{_{i}}|f^{*}, _{_{i}})}[L_{}(f_{t|_{_{i}},y_{_{i}}})]\] (1)

where \(f^{*}\) represents an optimal oracle that maps from any subset of the unlabeled data \(_{_{i}}_{t-1}\) to their ground-truth labels \(y_{_{i}}\), and \(f_{t|_{_{i}},y_{_{i}}}\) is the model that has been trained on the union of the current labeled set \(_{t-1}\) and the current unlabeled candidates \(_{_{i}}_{t-1}\). In addition, \(L_{}(f_{t|_{_{i}},y_{_{i}}})= {1}{|_{t-1}|}_{_{t-1,i}}( ;f_{t|_{_{i}},y_{_{i}}})\) represents the loss estimator that can predict the _unbiased_ error of \(f_{t|_{_{i}},y_{_{i}}}\), where \(\) denotes the loss function. It is numerically the same as if \(f_{t|_{_{i}},y_{_{i}}}\) has been tested on the entire unlabeled set \(_{t-1,i}\), where \(_{t-1,i}=_{t-1}\{_{_{i}}\}\). Such formulation represents the optimal AL criterion and aligns with any existing sequential active learning algorithm -- of which the goal is to select the new data points that can most significantly improve the current model performance . Unfortunately, Eq. (1) cannot be directly implemented in practice. Because, first, we do not have access to the optimal oracle \(f^{*}\) to reveal the labels \(y_{_{i}}\) of \(_{_{i}}_{t-1}\); second, even if we had \(f^{*}\) and therefore \(y_{_{i}}\), we cannot afford the cost of retraining model \(f_{t-1}\) on each \(_{t-1}_{_{i}}\) to obtain the updated \(f_{t|_{_{i}},y_{_{i}}}\); and third, we do not have the unbiased true loss estimator \(L_{}\), which demands evaluating \(f_{t|_{_{i}},y_{_{i}}}\) on the entire \(_{t-1,i}\). Therefore, the goal of DAO is to solve the above challenges and efficiently and accurately approximate Eq. (1) for the sample selection strategy. It is also worth noting that, when \(_{t}^{}\) represents a _set_ of newly acquired data points, the above formulation becomes eligible for batch active learning, which is more suitable for deep neural networks .

### Label Approximation via Surrogate

In this section, we address the first challenge when approximating Eq. (1). As we do not know the true label or true label distribution \(p(y|,f^{*})\) of each unlabeled sample \(\), the best we can do is provide an approximation for \(p(y|)\). To this end, we introduce the concept of a _surrogate_, which is a model parameterized by some potentially infinite set of parameters \(\). Specifically, \(p(y|)\) can be approximated using the marginal distribution \((y|)=_{()}[(y|,)]\) with some proposal

Figure 1: Schematic of the algorithmic framework of DAO.

distribution \(()\) over model parameters \(\). In other words, we have:

\[p(y|)_{}()(y|,)\,\] (2)

As the sample selection process continues, new labeled points should also be used to train and update the surrogate model \(()\) for better approximation of the true outcomes.

Although ideally, a more capable surrogate is preferred for better ground truth approximations, we acknowledge that the choice of surrogate model can be very sensitive to the computational constraints. Therefore, if running time is at center of the concerns during sample acquisitions, using \(f_{t}\) at step \(t\) also as the surrogate could be an efficient alternative, as we don't need to update a second model, nor do we need to run forward pass on the both models. However, this will come with the cost that \(_{t}\) never disagrees with \(f_{t}\), which causes performance degradation for the unbiased true loss estimation, which will be illustrated with more details in SS2.4. Therefore, in short, we do not recommend replicating \(f_{t}\) as surrogate in practice, unless the computational constraint is substantial.

### Model Parameters Update without Re-training

At acquisition round \(t\), suppose we have labeled set \(_{t-1}\) and unlabeled set \(_{t-1}\) as the results from the previous round \(t-1\), and new sample \(_{i}_{t-1}\) that is currently under consideration for acquisition, the goal of this section is to estimate the parameters of model \(f_{t|_{i},y_{i}}\) that could has been obtained after training \(f_{t-1}\) on the combined dataset \(\{_{t-1}_{i}\}\). Here, \(y_{i}\) denotes the (unobserved) ground-truth label of \(_{i}\). In other words, if we suppose the conventional full training converges to parameters \(_{_{i}}\), we have:

\[_{_{i}}=_{} _{t-1}|+1}_{\{_{t-1}_{i}\}}( ;)\] (3)

where recall that \((;)\) denotes the loss of \(\) on \(\). This objective is infeasible to compute exactly as \(y_{i}\) is unknown and retraining is expensive even if \(y_{i}\) is given. The core of our approach is that, instead of re-training as showed in Eq. (3), we can approximate the effect of adding a new sample as upweighting the influence function by \(_{t-1}|+1}\) and then directly estimate the updated model parameters. Following , we have the influence function defined as: \(_{}(_{i}):=_{i}, _{i}}{d}_{=0}=-H_{}^{-1} _{}(_{i};)\) where \(H_{}\) is the positive definite Hessian matrix . Next, we can estimate the model parameters after adding this new sample \(_{i}\), as: \(_{_{i}}-_{t-1} |+1}_{}(_{i})=-_{t-1 }|+1}H_{}^{-1}_{}(_{i};)\) where \(_{}(_{i};)\) could be approximated as the expected gradient of sample \(_{i}\): By a slight abuse of notation of the training loss function \(\), we denote \(_{}(_{i};)_{k=1}^{K}_{ }(_{i},_{k};)_{k}\) where \(_{k}\) and \(_{k}\) represent model's label prediction and likelihood (e.g. confidence) respectively while \(K\) represents the total number of classes in the ground truths.

In practice, the inverse of \(H_{}\) cannot be computed due to its prohibitive \(O(np^{2}+p^{3})\) runtime , with \(p\) being the number of model parameters. The computation unavoidably becomes especially intensive when \(f\) is a deep neural network model . Luckily, we have two optimization methods, conjugate gradients (CG)  and stochastic estimation , which are detailed in Appendix C.1.

### Efficient Unbiased Loss Estimation

Referring back to Eq. (1), the last challenge that we need to address is to gain access to the unbiased true loss estimator \(L_{}\). In other words, we want to predict the _true_ performance of \(f_{t|_{i},y_{i}}\) on the unlabeled set \(_{t,i}\) without exhaustive testing. Strictly, such evaluation cannot be drawn until \(f_{t|_{i},y_{i}}\) is evaluated on the entire unlabeled set \(_{t,i}\). However, this is infeasible in practice. Such approximation is typically carried out in other approaches [24; 41] by randomly sampling a labeled validation set \(\) at the beginning of the entire acquisition process, which will later be used for evaluations in all the subsequent acquisition episodes. Despite the simplicity as well as being i.i.d., which makes the estimated loss unbiased by nature, this approximation method suffers from large variance as the size of \(\) is usually much smaller than \(\), which unavoidably hurts the acquisition performance. It is also contradictory to the goal of AL in general, especially under the low-budget settings.

Different from the existing works, we propose to sample a subset \(\) from current \(_{t-1}\) in each acquisition round based on an alternative acquisition function, and then correct the bias in the loss induced from this acquisition function. In the meantime, we also want to keep the variance low, so that the final corrected loss enjoys both low bias and low variance, which is more preferable than the zero bias but high variance that the random i.i.d. sampling has. Specifically, continuing with the notations from SS2.1, let \(=\{_{t,1},,_{t,m},,_{t,n_{ }}\}\), where \(_{t-1}\), be the subset containing \(n_{}\) samples selected for this true loss estimation at each round \(t\).  shows that if \(_{t,m}\) is sampled in proportion to the true loss of each data point, the bias originated from this selection can be corrected through the Monte Carlo estimator \(_{}\)1. Following our notations, it takes the form: \(_{}=}}_{m=1}^{n_{}}v _{m}(_{t,m};f)\) where recall that \(\) denotes the loss of \(f\), and the importance weight \(v_{m}\) is

\[v_{m}=1+_{t-1}|-n_{}}{|_{t-1}|-m}( _{t-1}|-m+1)q_{t}^{*}(m)}-1)\] (4)

with \(q_{t}^{*}(m)\) being the acquisition distribution of index \(m\) at round \(t\). Importantly, the variance can be significantly reduced if the acquisition distribution \(q_{t}^{*}(m)\) is proportion to the true loss of each data point. Again, this is not feasible as we do not have access to the labels for \(_{t-1}\). However, following , we can approximate \(q_{t}^{*}(m)\) with \(q_{t}(m)=-_{y}(y|_{t,m}) f(_{t,m})\) for classification tasks when the loss function is the cross-entropy loss; here \(\) is conveniently just our surrogate discussed in SS2.2. Referring back to the discussion we had on choosing a good surrogate \(\), with \(f()\) being designed to approximate \(p(y|)\) as well, the surrogate \(\) should ideally be different from \(f\) so that more diversity is introduced in the acquisitions. To put all components together, our loss correction process involves selecting samples in \(\) following

\[_{t,m}-_{y}_{t-1}(y|) f_{t-1}(y|)\] (5)

where \(_{t-1}\) is the surrogate model at round \(t-1\). Finally, the corrected loss \(s_{i}\) can be approximated using \(_{}\) as \(s_{i}=}}_{m=1}^{n_{}}_{m} (_{t,m};f_{t})\) where \(_{m}\), which depends on the choice of \(_{t,m}\), is the approximated version of the original \(v_{m}\) defined in Eq. (4). Specifically, \(_{m}\) takes the form \(_{m}=1+_{t-1}|-n_{}}{|_{t-1}|- m}(_{t-1}|-m+1)q_{t}(m)}-1)\), where \(q_{t}(m)\) is the acquisition function in Eq. (5). we summarize the components illustrated in SS2 and present it in Algorithm 1.

## 3 Experiments

### Experiments Setup

**Baselines.** To ensure fair comparisons, besides baseline methods that we empirically surveyed in Appendix A, we also include other state-of-the-arts AL methods, including Deep Bayesian Active Learning (DBAL)  and GLISTER , where GLISTER is a direct competitor that also optimizes the EER framework. For all the baselines, we used the default/recommended parameters and their official implementations if publically available. In terms of earlier works such as least confidence , minimum margin , and maximum entropy , we used the peer-reviewed deep active learning framework DeepAL+ . All experiments are repeated ten times with different random seeds.

**Implementation Details.** We use ResNet-18  as the primary model \(f\), trained from scratch, and VGG16  with randomly initialized weights as the surrogate model \(\). For estimating updatedmodel parameters, we apply stochastic estimation , as detailed in SS2.3. We set \(n_{}=8\) when approximating the unbiased estimator of \(H_{}\) and \(n_{}=16\) for biased loss correction, following SS2.4.

### Results

We evaluate DAO on two digit recognition benchmarks: MNIST , a dataset of 60k handwritten digit images, and SVHN , a more challenging dataset with over 600k street-view house number images. Both datasets have 10 classes (digits 0-9). In low-budget settings, we use one image per class, resulting in an initial label size of 10 and a budget of 10 per round for MNIST. For the larger SVHN dataset, we experiment with initial labeled sizes and budgets of 10 and 100. Results are shown in Fig. 1(b) and Fig. 1(c). We further test DAO on more complex object classification tasks using STL-10 , CIFAR-10, and CIFAR-100 . STL-10 has 5k labeled 96x96 color images across 10 classes, with 8k test images. CIFAR-10 contains 60k 32x32 images in 10 classes, while CIFAR-100 expands to 100 classes with 600 images per class. In the low-budget setting (1 image per class), we use initial label sizes and per-round budgets of 10 for STL-10 and CIFAR-10, and 100 for CIFAR-100. Results are displayed in Fig. 1(d), Fig. 1(e), and Fig. 1(f). The final part of our experiments focuses on applying DAO to domain-specific tasks. We use FashionMNIST  and StanfordCars (Cars196) . FashionMNIST, similar in structure to MNIST, consists of 70k 28x28 images of fashion products from 10 categories, with 60k images for training and 10k for testing. StanfordCars contains 16,185 images of cars, split into 8,144 for training and 8,041 for testing, across 196 classes representing car make, model, and year (e.g., 2012 Tesla Model S). Results are in Fig. 1(g) and 1(h).

### Discussion

From Fig. 2, we observe that DAO consistently outperforms state-of-the-art active learning methods across all seven benchmarks. Notably, in the SVHN dataset with an extremely low budget (\(B=10\), 0.0017% of the unlabeled set), DAO demonstrates a significant performance advantage, highlighting its strength in low-budget settings. As the budget increases, DAO continues to perform well, as seen in Fig. 1(c). The only dataset where DAO shows less improvement is StanfordCars. However, DAO still provides smoother accuracy gains with less variance, indicating greater robustness in complex tasks like StanfordCars, which has 196 classes.

## 4 Conclusions

In this paper, we introduced Direct Acquisition Optimization (DAO), a novel algorithm designed to optimize sample selections in low-budget settings. DAO hinges on the utilization of influence functions for model parameter updates and a separate acquisition strategy to mitigate bias in loss estimation, represents a significant optimization of the EER method and its modern follow-ups. Through empirical studies, DAO has demonstrated superior performance in low-budget settings, outperforming existing state-of-the-art methods by a significant margin across seven datasets.

Figure 2: Experiment results showing DAO compared to other AL algorithms across seven benchmarks, with labeled set size on the horizontal axis and classification accuracy on the vertical axis.