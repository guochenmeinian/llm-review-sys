# ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection

Zhongzhan Huang\({}^{1,2}\)  Pan Zhou\({}^{2}\)1  Shuicheng Yan\({}^{2}\)  Liang Lin\({}^{1}\)

\({}^{1}\)Sun Yat-Sen University, \({}^{2}\)Sea AI Lab

huangzhzh23@mail2.sysu.edu.cn; zhoupan@sea.com; shuicheng.yan@gmail.com; linliang@ieee.org

###### Abstract

In diffusion models, UNet is the most popular network backbone, since its long skip connects (LSCs) to connect distant network blocks can aggregate long-distant information and alleviate vanishing gradient. Unfortunately, UNet often suffers from unstable training in diffusion models which can be alleviated by scaling its LSC coefficients smaller. However, theoretical understandings of the instability of UNet in diffusion models and also the performance improvement of LSC scaling remain absent yet. To solve this issue, we theoretically show that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Specifically, the hidden feature and gradient of UNet at any layer can oscillate and their oscillation ranges are actually large which explains the instability of UNet training. Moreover, UNet is also provably sensitive to perturbed input, and predicts an output distant from the desired output, yielding oscillatory loss and thus oscillatory gradient. Besides, we also observe the theoretical benefits of the LSC coefficient scaling of UNet in the stableness of hidden features and gradient and also robustness. Finally, inspired by our theory, we propose an effective coefficient scaling framework ScaleLong that scales the coefficients of LSC in UNet and better improve the training stability of UNet. Experimental results on four famous datasets show that our methods are superior to stabilize training, and yield about 1.5\(\) training acceleration on different diffusion models with UNet or UViT backbones. Click here for Code.

## 1 Introduction

Recently, diffusion models (DMs)  have become the most successful generative models because of their superiority on modeling realistic data distributions. The methodology of DMs includes a forward diffusion process and a corresponding reverse diffusion process. For forward process, it progressively injects Gaussian noise into a realistic sample until the sample becomes a Gaussian noise, while the reverse process trains a neural network to denoise the injected noise at each step for gradually mapping the Gaussian noise into the vanilla sample. By decomposing the complex generative task into a sequential application of denoising small noise, DMs achieve much better synthesis performance than other generative models, e.g., generative adversarial networks  and variational autoencoders , on image , 3D  and video  data and beyond.

Figure 1: The feature oscillation in UNet.

**Motivation.** Since the reverse diffusion process in DMs essentially addresses a denoising task, most DMs follow previous image denoising and restoration works  to employ UNet as their de-facto backbone. This is because UNet uses long skip connects (LSCs) to connect the distant and symmetrical network blocks in a residual network, which helps long-distant information aggregation and alleviates the vanishing gradient issue, yielding superior image denoising and restoration performance. However, in DMs, its reverse process uses a shared UNet to predict the injected noise at any step, even though the input noisy data have time-varied distribution due to the progressively injected noise. This inevitably leads to the training difficulty and instability of UNet in DMs. Indeed, as shown in Fig. 1, the output of hidden layers in UNet, especially for deep UNet, oscillates rapidly along with training iterations. This feature oscillation also indicates that there are some layers whose parameters suffer from oscillations which often impairs the parameter learning speed. Moreover, the unstable hidden features also result in an unstable input for subsequent layers and greatly increase their learning difficulty. Some works  empirically find that \(1/\)-constant scaling (\(1/\)-CS), that scales the coefficients of LSC (i.e. \(\{_{i}\}\) in Fig. 2) from one used in UNet to \(1/\), alleviates the oscillations to some extent as shown in Fig. 1, which, however, lacks intuitive and theoretical explanation and still suffers from feature and gradient oscillations. So it is unclear yet 1) why UNet in DMs is unstable and also 2) why scaling the coefficients of LSC in UNet helps stabilize training, which hinders the designing new and more advanced DMs in a principle way.

**Contribution.** In this work, we address the above two fundamental questions and contribute to deriving some new results and alternatives for UNet. Particularly, we theoretically analyze the above training instability of UNet in DMs, and identify the key role of the LSC coefficients in UNet for its unstable training, interpreting the stabilizing effect and limitations of the \(1/\)-scaling technique. Inspired by our theory, we propose the framework ScalLeLong including two novel-yet-effective coefficient scaling methods which scale the coefficients of LSC in UNet and better improve the training stability of UNet. Our main contributions are highlighted below.

Our first contribution is proving that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Formally, _for forward propagation_, we show that the norm of the hidden feature at any layer can be lower- and also upper-bounded, and the oscillation range between lower and upper bounds is of the order \(m_{i=1}^{N}_{j}^{2}\), where \(\{_{i}\}\) denotes the scaling coefficients of \(N\) LSCs and the input dimension \(m\) of UNet is often of hundreds. Since standard UNet sets \(_{i}=1\) (\( i\)), the above oscillation range becomes \((mN)\) and is large, which partly explains the oscillation and instability of hidden features as observed in Fig. 1. The \(1/\)-CS technique can slightly reduce the oscillation range, and thus helps stabilize UNet. Similarly, _for backward propagation_, the gradient magnitude in UNet is upper bounded by \(m_{i=1}^{N}_{i}^{2}\). In standard UNet, this bound becomes a large bound \(mN\), and yields the possible big incorrect parameter update, impairing the parameter learning speed during training. This also explains the big oscillation of hidden features, and reveals the stabilizing effects of the \(1/\)-CS technique. Furthermore, _for robustness_ that measures the prediction change when adding a small perturbation into an input, we show robustness bound of UNet is \((_{i=1}^{N}_{i}M_{0}^{i})\) with a model-related constant \(M_{0}>1\). This result also implies a big robustness bound of standard UNet, and thus shows the sensitivity of UNet to the noise which can give a big incorrect gradient and explain the unstable training. It also shows that \(1/\)-CS technique improves the robustness.

Inspired by our theory, we further propose a novel framework ScaleLong including two scaling methods to adjust the coefficients of the LSCs in UNet for stabilizing the training: 1) constant scaling method (**CS** for short) and 2) learnable scaling method (**LS**). CS sets the coefficients \(\{_{i}\}\) as a serir of exponentially-decaying constants, i.e., \(_{i}=^{i-1}\) with a contant \((0,1]\). As a result, CS greatly stabilizes the UNettraining by largely reducing the robustness error bound from \((M_{0}^{N})\) to \(M_{0}( M_{0})^{N-1}\), which is also \((^{N-2})\) times smaller than the bound \(( M_{0}^{N})\) of the universal scaling method, i.e., \(_{i}=\), like \(1/\)-CS technique. Meanwhile, CS can ensure the information transmission of LSC without degrading into a feedforward network. Similarly, the oscillation range

Figure 2: The diagram of UNet.

of hidden features and also gradient magnitude can also be controlled by our CS. For LS, it uses a tiny shared network for all LSCs to predict the scaling coefficients for each LSC. In this way, LS is more flexible and adaptive than the CS method, as it can learn the scaling coefficients according to the training data, and can also adjust the scaling coefficients along with training iterations which could benefit the training in terms of stability and convergence speed. Fig. 1 shows the effects of CS and LS in stabilizing the UNet training in DMs.

Extensive experiments on CIFAR10 , CelebA , ImageNet , and COCO , show the effectiveness of our CS and LS methods in enhancing training stability and thus accelerating the learning speed by at least 1.5\(\) in most cases across several DMs, e.g. UNet and UViT networks under the unconditional [3; 4; 5], class-conditional [36; 37] and text-to-image [38; 39; 40; 41; 42] settings.

## 2 Preliminaries and other related works

**Diffusion Model (DM)**. DDPM-alike DMs [1; 2; 3; 4; 5; 6; 8; 9; 43] generates a sequence of noisy samples \(\{_{i}\}_{i=1}^{T}\) by repeatedly adding Gaussian noise to a sample \(_{0}\) until attaining \(_{T}(,)\). This noise injection process, a.k.a. the forward process, can be formalized as a Markov chain \(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_{t-1})\), where \(q(_{t}|_{t-1})=(_{t}|}_{t-1},_{t})\), \(_{t}\) and \(_{t}\) depend on \(t\) and satisfy \(_{t}+_{t}=1\). By using the properties of Gaussian distribution, the forward process can be written as

\[q(_{t}|_{0})=(_{t};_{t}}_{0},(1-_{t})),\] (1)

where \(_{t}=_{i=1}^{t}_{i}\). Next, one can sample \(_{t}=_{t}}_{0}+_{t} }_{t}\), where \(_{t}(,)\). Then DM adopts a neural network \(_{}(,t)\) to invert the forward process and predict the noise \(_{t}\) added at each time step. This process is to recover data \(_{0}\) from a Gaussian noise by minimizing the loss

\[_{}^{t}()=\|_{t}-_{ }(_{t}}_{0}+_{t}} _{t},t)\|_{2}^{2}.\] (2)

**UNet-alike Network**. Since the network \(_{}(,t)\) in DMs predicts the noise to denoise, it plays a similar role of UNet-alike network widely in image restoration tasks, e.g. image de-raining, image denoising [22; 23; 24; 25; 26; 44; 45]. This inspires DMs to use UNet-alike network in (3) that uses LSCs to connect distant parts of the network for long-range information transmission and aggregation

\[(x)=f_{0}(x), f_{i}(x)=b_{i+1}[_{i+1} a_{i+ 1} x+f_{i+1}(a_{i+1} x)],\ \ 0 i N-1\] (3)

where \(x^{m}\) denotes the input, \(a_{i}\) and \(b_{i}\)\((i 1)\) are the trainable parameter of the \(i\)-th block, \(_{i}>0\)\((i 1)\) are the scaling coefficients and are set to 1 in standard UNet. \(f_{N}\) is the middle block of UNet. For the vector operation \(\), it can be designed to implement different networks.

W.l.o.g, in this paper, we consider \(\) as matrix multiplication, and set the \(i\)-th block as a stacked network [46; 47] which is implemented as \(a_{i} x=_{l}^{a_{i}}(_{l-1}^{a_{i}}...(_{1}^{a_{i}}x))\) with a ReLU activation function \(\) and learnable matrices \(_{j}^{a_{i}}^{m m}\)\((j 1)\). Moreover, let \(f_{N}\) also have the same architecture, i.e. \(f_{N}(x)=_{l}^{f_{l}}(_{l-1}^{f_{l-1}}...(_{1}^{f_{l}}x))\). Following , the above UNet has absorbed the fusion operation of the feature \(a_{i} x\) and \(f_{i}(a_{i} x)\) into UNet backbone. So for simplicity, we analyze the UNet in (3). Moreover, as there are many variants of UNet, e.g. transformer UNet, it is hard to provide a unified analysis for all of them. Thus, here we only analyze the most widely used UNet in (3), and leave the analysis of other UNet variants in future work.

**Other related works.** Previous works [46; 48; 49; 50; 51; 52; 53; 54; 55] mainly studied classification task and observed performance improvement of scaling block output instead of skip connections in ResNet and Transformer, i.e. \(_{i+1}=_{i}+_{i}f_{i}(_{i})\) where \(f_{i}(_{i})\) is the \(i\)-th block. In contrast, we study UNet for DMs which uses long skip connections (LSC) instead of short skip connections, and stabilize UNet training via scaling LSC via Eq. (3) which is superior to scaling block output as shown in Section 5.

## 3 Stability Analysis on UNet

As shown in Section 1, standard U-Net (\(_{i}=1, i\)) often suffers from the unstable training issue in diffusion models (DMs), while scaling the long skip connection (LSC) coefficients \(_{i}( i)\) in UNet can help to stabilize the training. However, theoretical understandings of the instability of U-Net and also the effects of scaling LSC coeffeicents remain absent yet, hindering the development of new and more advanced DMs in a principle way. To address this problem, in this section, we will theoretically and comprehensively analyze 1) why UNet in DMs is unstable and also 2) why scaling the coeffeicents of LSC in UNet helps stablize training. To this end, in the following, we will analyze the stableness of the forward and backward propagation of UNet, and investigate the robustness of UNet to the noisy input. All these analyses not only deepen the understanding of the instability of UNet, but also inspire an effective solution for a more stable UNet as introduced in Section 4.

### Stability of Forward Propagation

Here we study the stableness of the hidden features of UNet. This is very important, since for a certain layer, if its features vary significantly along with the training iterations, it means there is at least a layer whose parameters suffer from oscillations which often impairs the parameter learning speed. Moreover, the unstable hidden features also result in an unstable input for subsequent layers and greatly increase their learning difficulty, since a network cannot easily learn an unstable distribution, e.g. Internal Covariate Shift , which is also shown in many works . In the following, we theoretically characterize the hidden feature output by bounding its norm.

**Theorem 3.1**.: _Assume that all learnable matrices of UNet in Eq. (3) are independently initialized as Kaiming's initialization, i.e., \((0,)\). Then for any \((0,1]\), by minimizing the training loss Eq. (2) of DMs, with probability at least \(1-(N)[-(m^{2})]\), we have_

\[(1-)^{2}[c_{1}\|_{t}\|_{2}^{2}_{j=i}^{N} _{j}^{2}+c_{2}]\|h_{i}\|_{2}^{2}(1+)^{2}[ c_{1}\|_{t}\|_{2}^{2}_{j=i}^{N}_{j}^{2}+c_{2} ],\] (4)

_where the hidden feature \(h_{i}\) is the output of \(f_{i}\) defined in Eq. (3), \(_{t}=_{t}}_{0}+_{t}} _{t}\) is the input of UNet; \(c_{1}\) and \(c_{2}\) are two constants; \(_{i}\) is the scaling coefficient of the \(i\)-th LSC in UNet._

Theorem 3.1 reveals that with high probability, the norm of hidden features \(h_{i}\) of UNet can be both lower- and also upper-bounded, and its biggest oscillation range is of the order \(\|_{t}\|_{2}^{2}_{j=i}^{N}_{j} ^{2}\) which is decided by the scaling coefficients \(\{_{i}\}\) and the UNet input \(_{t}\). So when \(\|_{t}\|_{2}\) or \(_{i=1}^{N}_{i}^{2}\) is large, the above oscillation range is large which allows hidden feature \(h_{i}\) to oscillate largely as shown in Fig. 1, leading to the instability of the forward processing in UNet.

In fact, the following Lemma 3.2 shows that \(\|_{t}\|_{2}^{2}\) is around feature dimension \(m\) in DDPM.

**Lemma 3.2**.: _For \(_{t}=_{t}}_{0}+_{t} }_{t}\) defined in Eq. (1) as a input of UNet, \(_{t}(0,)\), if \(_{0}\) follow the uniform distribution \(U[-1,1]\), then we have_

\[\|_{t}\|_{2}^{2}=(1-2_{t}_{t}/3)m= (m).\] (5)

Indeed, the conditions and conclusions of Lemma 3.2 hold universally in commonly used datasets, such as CIFAR10, CelebA, and ImageNet, which can be empirically valid in Fig. 3 (a). In this way, in standard UNet whose scaling coefficients satisfy \(_{i}=1\), the oscillation range of hidden feature \(\|h_{i}\|_{2}^{2}\) becomes a very large bound \((Nm)\), where \(N\) denotes the number of LSCs. Accordingly, UNet suffers from the unstable forward process caused by the possible big oscillation of the hidden feature \(h_{i}\) which accords with the observations in Fig. 1. To relieve the instability issue, one possible solution is to scale the coefficients \(\{_{i}\}\) of LSCs, which could reduce the oscillation range of hidden feature \(\|h_{i}\|_{2}^{2}\) and yield more stable hidden features \(h_{i}\) to benefit the subsequent layers. This is also one reason why \(1/\)-scaling technique [27; 28; 29; 30; 31] can help stabilize the UNet. However, it is also important to note that these coefficients cannot be too small, as they could degenerate a network into a feedforward network and negatively affect the network representation learning ability.

### Stability of Backward Propagation

As mentioned in Section 3.1, the reason behind the instability of UNet forward process in DMs is the parameter oscillation. Here we go a step further to study why these UNet parameters oscillate. Since parameters are updated by the gradient given in the backward propagation process, we analyze the gradient of UNet in DMs. A stable gradient with proper magnitude can update model parameters smoothly and also effectively, and yields more stable training. On the contrary, if gradients oscillate largely as shown in Fig. 1, they greatly affect the parameter learning and lead to the unstable hidden features which further increases the learning difficulty of subsequent layers. In the following, we analyze the gradients of UNet, and investigate the effects of LSC coefficient scaling on gradients.

For brevity, we use \(\) to denote all the learnable matrices of the \(i\)-th block (see Eq. (3)), i.e., \(=[_{1}^{a_{1}},_{1-1}^{a_{1}},...,_{ 1}^{a_{1}},_{1}^{a_{2}}...,_{2}^{b_{1}},_{1}^{ b_{1}}]\). Assume the training loss is \(_{s=1}^{n}_{s}()\), where \(_{s}()\) denotes the training loss of the \(s\)-th sample among the \(n\) training samples. Next, we analyze the gradient \(()\) of each training sample, and summarize the main results in Theorem 3.3, in which we omit the subscript \(s\) of \(_{s}()\) for brevity. See the proof in Appendix.

**Theorem 3.3**.: _Assume that all learnable matrices of UNet in Eq. (3) are independently initialized as Kaiming's initialization, i.e., \((0,)\). Then for any \((0,1]\), with probability at least \(1-(nN)[-(m)]\), for a sample \(_{t}\) in training set, we have_

\[\|_{_{p}^{q}}_{s}()\|_{2}^{2}(()\|_{t}\|_{2}^{2}_{j= i}^{N}_{j}^{2}+c_{3}),(p\{1,2,...,l\},q\{a_{i},b_{i}\}),\] (6)

_where \(_{t}=_{t}}_{0}+_{t }}_{t}\) denotes the noisy sample of the \(s\)-th sample, \(_{t}(,)\), \(N\) is the number of LSCs, \(c_{3}\) is a small constant._

Theorem 3.3 reveals that for any training sample in the training dataset, the gradient of any trainable parameter \(_{p}^{q}\) in UNet can be bounded by \((()\|_{t}\|_{2}^{2}_{j=i}^{N} _{j}^{2})\) (up to the constant term). Since Lemma 3.2 shows \(\|_{t}\|_{2}^{2}\) is at the order of \((m)\) in expectation, the bound of gradient norm becomes \((m_{s}()_{j=i}^{N}_{j}^{2})\). Consider the feature dimension \(m\) is often hundreds and the loss \(()\) would be large at the beginning of the training phase, if the number \(N\) of LSCs is relatively large, the gradient bound in standard UNet (\(_{i}=1\  i\)) would become \((mN_{s}())\) and is large. This means that UNet has the risk of instability gradient during training which is actually observed in Fig. 1. To address this issue, similar to the analysis of Theorem 3.3, we can appropriately scale the coefficients \(\{_{i}\}\) to ensure that the model has enough representational power while preventing too big a gradient and avoid unstable UNet parameter updates. This also explains why the \(1/\)-scaling technique can improve the stability of UNet in practice.

### Robustness On Noisy Input

In Section 3.1 and 3.2, we have revealed the reasons of instability of UNet and also the benign effects of scaling coefficients of LSCs in UNet during DM training. Here we will further discuss the stability of UNet in terms of the robustness on noisy input. A network is said to be robust if its output does not change much when adding a small noise perturbation to its input. This robustness is important for stable training of UNet in DMs. As shown in Section 2, DMs aim at using UNet to predict the noise \(_{t}(0,)\) in the noisy sample for denoising at each time step \(t\). However, in practice, additional unknown noise caused by random minibatch, data collection, and preprocessing strategies is inevitably introduced into noise \(_{t}\) during the training process, and greatly affects the training. It is because if a network is sensitive to noisy input, its output would be distant from the desired output, and yields oscillatory loss which leads to oscillatory gradient and parameters. In contrast, a robust network would have stable output, loss and gradient which boosts the parameter learning speed.

Here we use a practical example to show the importance of the robustness of UNet in DMs. According to Eq. (2), DM aims to predict noise \(_{t}\) from the UNet input \(_{t}(_{t};_{t}}_ {0},(1-_{t}))\).

Assume an extra Gaussian noise \(_{}(0,_{}^{2})\) is injected into \(_{t}\) which yields a new input \(_{t}^{}(_{t};} _{0},[(1-_{t})+_{}^{2}])\). Accordingly, if the variance \(_{}^{2}\) of extra noise is large, it can dominate the noise \(_{t}\) in \(_{t}^{}\), and thus hinders predicting the desired noise \(_{t}\). In practice, the variance of this extra noise could vary along training iterations, further exacerbating the instability of UNet training. Indeed, Fig. 3 (c) shows that the extra noise \(_{}\) with different \(_{}^{2}\) can significantly affect the performance of standard UNet for DMs, and the scaled LSCs can alleviate the impact of extra noise to some extent. We then present the following theorem to quantitatively analyze these observations.

**Theorem 3.4**.: _For UNet in Eq. (3), assume \(M_{0}=\{ b_{i} a_{i}_{2},1 i N\}\) and \(f_{N}\) is \(L_{0}\)-Lipschitz continuous. \(c_{0}\) is a constant related to \(M_{0}\) and \(L_{0}\). Suppose \(_{t}^{_{}}\) is an perturbated input of the vanilla input \(_{t}\) with a small perturbation \(_{}=_{t}^{}-_{t}_{2}\). Then we have_

\[(_{t}^{_{}})-( _{t})_{2}_{}[_{i=1}^{N} _{i}M_{0}^{i}+c_{0}],\] (7)

_where \(_{t}=_{t}}_{0}+_{t} }_{t}\), \(_{t}(,)\), \(N\) is the number of the long skip connections._

Theorem 3.4 shows that for a perturbation magnitude \(_{}\), the robustness error bound of UNet in DMs is \((_{}(_{i=1}^{N}_{i}M_{0}^{i}))\). For standard UNet (\(_{i}=1 i\)), this bound becomes a very large bound \((NM_{0}^{N})\). This implies that standard UNet is sensitive to extra noise in the input, especially when LSC number \(N\) is large (\(M_{0} 1\) in practice, see Appendix). Hence, it is necessary to control the coefficients \(_{i}\) of LSCs which accords with the observations in Fig. 3 (c). Therefore, setting appropriate scaling coefficients \(\{_{i}\}\) can not only control the oscillation of hidden features and gradients during the forward and backward processes as described in Section 3.1 and Section 3.2, but also enhance the robustness of the model to input perturbations, thereby improving better stability for DMs as discussed at the beginning of Section 3.3.

## 4 Theory-inspired Scaling Methods For Long Skip Connections

In Section 3, we have theoretically shown that scaling the coefficients of LSCs in UNet can help to improve the training stability of UNet on DM generation tasks by analyzing the stability of hidden feature output and gradient, and the robustness of UNet on noisy input. These theoretical results can directly be used to explain the effectiveness of \(1/\)-scaling technique in  which scales all LSC coefficients \(_{i}\) from one in UNet to a constant, and is called \(1/\) (\(1/\)-CS for short). Here we will use these theoretical results to design a novel framework ScaleLong including two more effective scaling methods for DM stable training. 1) constant scaling method (CS for short) and 2) learnable scaling method (LS for short).

### Constant Scaling Method

In Theorem 3.1\(\) 3.4, we already show that adjusting the scaling coefficients \(\{_{i}\}\) to smaller ones can effectively decrease the oscillation of hidden features, avoids too large gradient and also improves the robustness of U-Net to noisy input. Motivated by this, we propose an effective constant scaling method (CS) that exponentially scales the coefficients \(\{_{i}\}\) of LSCs in UNet:

\[_{i}=^{i-1},(i=1,2,..,N, (0,1]).\] (8)

When \(i=1\), the scaling coefficient of the first long skip connection is 1, and thus ensures that at least one \(_{i}\) is not too small to cause network degeneration.

This exponential scaling in CS method is more effective than the universal scaling methods which universally scales \(_{i}=\), e.g. \(=1/\) in the \(1/\)-CS method, in terms of reducing the training oscillation of UNet. We first look at the robustness error bound in Theorem 3.4. For our CS, its bound is at the order of \(M_{0}( M_{0})^{N-1}\) which is \((^{N-2})\) times smaller than the bound \(( M_{0}^{N})\) of the universal scaling method. This shows the superiority of our CS method. Moreover, from the analysis in Theorem 3.1, our CS method can effectively compress the oscillation range of the hidden feature \(h_{i}\) of the \(i\)-th layer to \(^{2i}m\). Compared with the oscillation range \(Nm\) of standard UNet, CS also greatly alleviates the oscillation since \(^{2i} N\). A similar analysis on Theorem 3.3 also shows that CS can effectively control the gradient magnitude, helping stabilize UNet training.

Now we discuss how to set the value of \(\) in Eq. (8). To this end, we use the widely used DDPM framework on the benchmarks (CIFAR10, CelebA and ImageNet) to estimate \(\). Firstly, Theorem3.1 reveals that the norm of UNet output is of order \((\|_{t}\|_{2}^{2}_{j=1}^{N}_{j}^{2})\). Next, from the loss function of DMs, i.e., Eq. (2), the target output is a noise \(_{t}(0,)\). Therefore, we expect that \((\|_{t}\|_{2}^{2}_{j=1}^{N}_{j}^{2})\| _{t}\|_{2}^{2}\), and we have \(_{j=1}^{N}_{j}^{2}(\|_{t}\|_{2}^{2}/\| _{t}\|_{2}^{2})\). Moreover, Fig. 3 (b) shows the distribution of \(\|_{t}\|_{2}^{2}/\|_{t}\|_{2}^{2}\) for the dataset considered, which is long-tailed and more than 97% of the values are smaller than 5. For this distribution, during the training, these long-tail samples result in a larger value of \(\|_{t}\|_{2}^{2}/\|_{t}\|_{2}^{2}\). They may cause the output order \((\|_{t}\|_{2}^{2}_{j=1}^{N}_{j}^{2})\) to deviate more from the \(\|_{t}\|_{2}^{2}\), leading to unstable loss and impacting training. Therefore, it is advisable to appropriately increase \(_{j=1}^{N}_{j}^{2}\) to address the influence of long-tail samples. . So the value of \(_{j=1}^{N}_{j}^{2}\) should be between the mean values of \(\|_{t}\|_{2}^{2}/\|_{t}\|_{2}^{2}\), i.e., about 1.22, and a rough upper bound of 5. Combining Eq. (8) and the settings of \(N\) in UViT and UNet, we can estimate \([0.5,0.95]\). In practice, the hyperparameter \(\) can be adjusted around this range.

### Learnable Scaling Method

In Section 4.1, an effective constant scaling (CS) method is derived from theory, and can improve the training stability of UNet in practice as shown in Section 5. Here, we provide an alternative solution, namely, the learnable scaling method (LS), which uses a tiny network to predict the scaling coefficients for each long skip connection. Accordingly, LS is more flexible and adaptive than the constant scaling method, since it can learn the coefficients according to the training data, and can also adjust the coefficients along with training iterations which could benefit the training in terms of stability and convergence speed, which will be empirically demonstrated in Section 5.

As shown in Fig. 4, LS designs a calibration network usually shared by all long skip connections to predict scaling coefficients \(\{_{i}\}\). Specifically, for the \(i\)-th long skip connection, its input feature is \(x_{i}^{B N D}\), where \(B\) denotes the batch size. For convolution-based UNet, \(N\) and \(D\) respectively denote the channel number and spatial dimension (\(H W\)) of the feature map; for transformer-based UViT , \(N\) and \(D\) respectively represent the token number and token dimension. See more discussions on input features of other network architectures in the Appendix. Accordingly, LS feeds \(x_{i}^{B N D}\) into the tiny calibration network \(_{}\) parameterized by \(\) for prediction:

\[:_{i}=(_{}[(x_{i})])^{B N 1},1 i N,\] (9)

where GAP denotes the global average pooling that averages \(x_{i}\) along the last dimension to obtain a feature map of size \(B N 1\); \(\) is a sigmoid activation function. After learning \(\{_{i}\}\), LS uses \(_{i}\) to scale the input \(x_{i}\) via element-wise multiplication. In practice, network \(_{}\) is very small, and has only about 0.01M parameters in our all experiments which brings an ignorable cost compared with the cost of UNet but greatly improves the training stability and generation performance of UNet. In fact, since the parameter count of \(_{}\) itself is not substantial, to make LS more versatile for different network architecture, we can individually set a scaling module into each long skip connection. This configuration typically does not lead to performance degradation and can even result in improved performance, while maintaining the same inference speed as the original LS.

## 5 Experiment

In this section, we evaluate our methods by using UNet  and also UViT  under the unconditional , class-conditional  and text-to-image  settings on several commonly used datasets, including CIFAR10 , CelebA , ImageNet , and MS-COCO . We follow the settings of  and defer the specific implementation details to the Appendix.

**Training Stability.** Fig. 1 shows that our CS and LS methods can significantly alleviate the oscillation in UNet training under different DM settings, which is consistent with the analysis for controlling hidden features and gradients in Section 3. Though the \(1/\)-CS method can stabilize training to some extent, there are relatively large oscillations yet during training, particularly for deep networks. Additionally, Fig. 3 (c) shows that CS and LS can resist extra noise interference to some extent, further demonstrating the effectiveness of our proposed methods in improving training stability.

**Convergence Speed**. Fig. 5 shows the training efficiency of our CS and LS methods under different settings which is a direct result of stabilizing effects of our methods. Specifically, during training, for

Figure 4: The diagram of LS.

most cases, they consistently accelerate the learning speed by at least 1.5\(\) faster in terms of training steps across different datasets, batch sizes, network depths, and architectures. This greatly saves the actual training time of DMs. For example, when using a 32-sized batch, LS trained for 6.7 hours (200 k steps) achieves superior performance than standard UViT trained for 15.6 hours (500 k steps). All these results reveal the effectiveness of the long skip connection scaling for faster training of DMs.

**Performance Comparison**. Tables 1 show that our CS and LS methods can consistently enhance the widely used UNet and UViT backbones under the unconditional [3; 4; 5], class-conditional [36; 37] and text-to-image [38; 39; 40; 41; 42] settings with almost no additional computational cost. Moreover, on both UNet and UViT, our CS and LS also achieve much better performance than \(1/\)-CS. All these results are consistent with the analysis in Section 4.1.

 
**Model (ClelebA)** & **Type** & **\#Param** & **FID\(\)** & **Model (ImageNet 64)** & **Type** & **\#Param** & **FID\(\)** \\  DDIM  & Uncondition & 56M & 1.97 & Glow  & - & - & 3.81 \\ IDDPM  & Uncondition & 53M & 2.90 & IDDPM (small)  & Class-condition & 100M & 6.92 \\ DDPM+ cont.  & Uncondition & 62M & 2.55 & IDDPM (large) & Class-condition & 270M & 2.92 \\ GenViT  & Uncondition & 11M & 20.20 & ADM  & Class-condition & 296M & 2.07 \\ UNet  & Uncondition & 36M & 3.17 & EDM  & Class-condition & 296M & 1.36 \\ UViT-S2  & Uncondition & 44M & 3.11 & UViT-M/4  & Class-condition & 131M & 5.85 \\ UNet+1/\(\)-CS & Uncondition & 36M & 3.14 & UViT-M/4-\(1/\)-CS & Class-condition & 287M & 4.26 \\ UViT-S/2/-\(1/\)-CS & Uncondition & 44M & 3.11 & UViT-M/4-\(1/\)-CS & Class-condition & 131M & 5.80 \\  UNet+CS (ours) & Uncondition & 36M & 3.05 & UViT-L/4-\(1/\)-CS & Class-condition & 287M & 4.20 \\ UNet  & Uncondition & 36M & 3.07 & UViT-S/2+CS (ours) & Uncondition & 144M & 3.77 \\ UViT-S/2+CS (ours) & Uncondition & 44M & 2.98 & UNet-\(1/\)-CS & Class-condition & 144M & 3.66 \\ UViT-S/2+LS (ours) & Uncondition & 44M & 3.01 & UViT-M/4-CS (ours) & Class-condition & 143M & 3.48 \\ 
**Model (MS-COCO)** & **Type** & **\#Param** & **FID\(\)** & **UViT-M/4-\(1/\)-CS** & Class-condition & 131M & 5.68 \\  VQ-Diffusion  & Text-to-Image & 370M & 19.75 & UViT-M/4-\(1/\)-CS & Class-condition & 131M & 5.75 \\ Frito  & Text-to-Image & 766M & 8.97 & UViT-L/4-CS (ours) & Class-condition & 287M & 3.83 \\ UNet  & Text-to-Image & 252M & 5.88 & UViT-S/2+\(1/\)-CS & Class-condition & 287M & 4.08 \\ UViT-S/2  & Text-to-Image & 252M & 5.95 & UViT-S/2 (Deep)  & Text-to-Image & 252M & 5.95 \\  UNet+CS (ours) & Text-to-Image & 260M & 7.04 & UViT-S/2+\(1/\)-CS & Class-condition & 260M & 6.89 \\ UViT-S/2+CS (ours) & Text-to-Image & 252M & 5.77 & UViT-S/2+LS (ours) & Uncondition & 44M & 2.78 \\ UViT-S/2+LS (ours) & Text-to-Image & 252M & 5.60 & UViT-S/2+\(1/\)-CS & Class-condition & 287M & 4.08 \\  

Table 1: Synthesis performance comparison under different diffusion model settings.

Figure 5: Training curve comparison under different training settings in which all experiments here adopt 12-layered UViT and a batch size 128 on CIFAR10 unless specified in the figures.

**Robustness to Batch Size and Network Depth**. UViT is often sensitive to batch size and network depth , since small batch sizes yield more noisy stochastic gradients that further aggravate gradient instability. Additionally, Section 3 shows that increasing network depth impairs the stability of both forward and backward propagation and its robustness to noisy inputs. So in Fig. 6, we evaluate our methods on standard UViT with different batch sizes and network depths. The results show that our CS and LS methods can greatly help UViT on CIFAR10 by showing big improvements on standard UViT and \(1/\)-CS-based UViT under different training batch sizes and network depth.

**Robustness of LS to Network Design**. For the calibration module in LS, here we investigate the robustness of LS to the module designs. Table 2 shows that using advanced modules, e.g. IE [77; 80] and SE , can improve LS performance compared with simply treating \(_{i}\) as learnable coefficients. So we use SE module [79; 81; 82] in our experiments (see Appendix). Furthermore, we observe that LS is robust to different activation functions and compression ratios \(r\) in the SE module. Hence, our LS does not require elaborated model crafting and adopts default \(r=16\) and ReLU throughout this work.

**Other scaling methods**. Our LS and CS methods scale LSC, unlike previous scaling methods [46; 48; 49; 50; 51; 52; 53] that primarily scale block output in classification tasks. Table 3 reveals that these block-output scaling methods have worse performance than LS (FID: 3.01) and CS (FID: 2.98) under diffusion model setting with UViT as backbone.

## 6 Discussion

**Relationship between LS and CS**. Taking UViT on CIFAR10, MSCOCO, ImageNet and CelebA as baselines, we randomly sample 30 Gaussian noises from \((0,)\) and measure the scaling coefficients, i.e., \(_{i} 1\), learned by LS for each LSC. Fig. 7 shows that LS and CS share similar characteristics. Firstly, the predicted coefficients of LS fall within the orange area which is the estimated coefficient range given by CS in Section 4.1. Moreover, these learned coefficients \(_{i}\) share the almost the same exponential curve with Eq. (8) in CS. These shared characteristics have the potential to serve as crucial starting points for further optimization of DM network architectures in the future. Moreover, we try to preliminary analyze these shared characteristics.

First, for CS, how is the direction of applying the exponentially decayed scale determined? In fact, if we use the reverse direction, namely, \(_{i}=^{N-i+1}\)\((<1)\), the stability bound in Theorem 3.4 is extremely large. The main term of stability bound in Theorem 3.4 can be written as \(_{}=_{i=1}^{N}_{i}M_{0}^{i}=^{N}M_{0}+ ^{N-1}M_{0}^{2}+...+ M_{0}^{N}\), and could be very large, since \(M_{0}^{N}>1\) is large when \(N\) is large and scaling it by a factor \(\) could not sufficiently control its magnitude. In contrast, our default setting \(_{i}=^{i-1}\) of CS can well control the main term in stability bound: \(=_{i=1}^{N}_{i}M_{0}^{i}=M_{0}+^{1}M_{0}^{2}+...+ ^{N-1}M_{0}^{N}\), where the larger terms \(M_{0}^{i+1}\) are weighted by smaller coefficients \(^{i}\). In this way, \(\) is much smaller than \(_{}\), which shows the advantages of our default setting. Besides, the following Table 4 also compares the above two settings by using UViT on Cifar10 (batch size = 64), and shows that our default setting exhibits significant advantages.

Next, for LS, there are two possible reasons for why LS discovers a decaying scaling curve similar to the CS. On the one hand, from a theoretical view, as discussed for the direction of scaling, for the \(i\)-th long skip connection \((1 i N)\), the learnable \(_{i}\) should be smaller to better control the

 
**Ratio \(r\)** & **FID\(\)** & **Activation** & **FID\(\)** & **Modules** & **FID\(\)** \\ 
4 & 3.06 & ELU  & 3.08 & Learnable \(_{i}\) & 3.46 \\
8 & 3.10 & ReLU  (ours) & **3.01** & IE  & 3.09 \\
16 (ours) & **3.01** & Mish  & 3.08 & SE  (ours) & **3.01** \\  

Table 2: Robustness of LS to network design

Figure 6: Synthesis performance of proposed methods under small batch size and deep architecture.

Figure 7: The \(_{i}\) from LS.

magnitude of \(M_{0}^{i}\) so that the stability bound, e.g. in Theorem 3.4, is small. This directly yields the decaying scaling strategy which is also learnt by the scaling network. On the other hand, we can also analyze this observation in a more intuitive manner. Specifically, considering the UNet architecture, the gradient that travels through the \(i\)-th long skip connection during the backpropagation process influences the updates of both the first \(i\) blocks in the encoder and the last \(i\) blocks in the UNet decoder. As a result, to ensure stable network training, it's advisable to slightly reduce the gradients on the long skip connections involving more blocks (i.e., those with larger \(i\) values) to prevent any potential issues with gradient explosion.

**Limitations**. CS does not require any additional parameters or extra computational cost. But it can only estimate a rough range of the scaling coefficient \(\) as shown in Section 4.1. This means that one still needs to manually select \(\) from the range. In the future, we may be able to design better methods to assist DMs in selecting better values of \(\), or to have better estimates of the range of \(\). Another effective solution is to use our LS method which can automatically learn qualified scaling coefficient \(_{i}\) for each LSC. But LS inevitably introduces additional parameters and computational costs. Luckily, the prediction network in LS is very tiny and has only about 0.01M parameters, working very well in all our experiments.

Moreover, our CS and LS mainly focus on stabilizing the training of DMs, and indeed well achieve their target: greatly improving the stability of UNet and UViT whose effects is learning speed boosting of them by more than 1.5\(\) as shown in Fig. 5. But our CS and LS do not bring a very big improvement in terms of the final FID performance in Table 1, although they consistently improve model performance. Nonetheless, considering their simplicity, versatility, stabilizing ability to DMs, and faster learning speed, LS and CS should be greatly valuable in DMs.

**Conclusion**. We theoretically analyze the instability risks from widely used standard UNet for diffusion models (DMs). These risks are about the stability of forward and backward propagation, as well as the robustness of the network to extra noisy inputs. Based on these theoretical results, we propose a novel framework ScaleLong including two effective scaling methods, namely LS and CS, which can stabilize the training of DMs in different settings and lead to faster training speed and better generation performance.