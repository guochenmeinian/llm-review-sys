# Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?

Yinlin Deng

 Chunqiu Steven Xia

 Zhezhen Cao

Meizinui Li

 Lingming Zhang

University of Illinois Urbana-Champaign

 Southern University of Science and Technology

 The Hong Kong University of Science and Technology

{yinlind2,chunqiu2,lingming}@illinois.edu, 12110529@mail.sustech.edu.cn, mlick@cse.ust.hk

###### Abstract

Data science (DS) programs, typically built on popular DS libraries (such as PyTorch and NumPy) with thousands of APIs, serve as the cornerstone for various mission-critical domains such as financial systems, autonomous driving software, and coding assistants. Recently, large language models (LLMs) have been widely applied to generate DS programs across diverse scenarios, such as assisting users for DS programming or detecting critical vulnerabilities in DS frameworks. Such applications have all operated under the assumption, that LLMs can implicitly model the numerical parameter constraints in DS library APIs and produce valid code. However, this assumption has not been rigorously studied in the literature. In this paper, we empirically investigate the proficiency of LLMs to handle these implicit numerical constraints when generating DS programs. We studied 28 widely used APIs from PyTorch and NumPy, and scrutinized the LLMs' generation performance in different levels of granularity: full programs, all parameters, and individual parameters of a single API. We evaluated both state-of-the-art open-source and closed-source models. The results show that LLMs are great at generating simple DS programs, particularly those that follow common patterns seen in training data. However, as we increase the difficulty by providing more complex/unusual inputs, the performance of LLMs drops significantly. We also observe that GPT-4-Turbo can sustain much higher performance overall, but still cannot handle arithmetic API constraints well. In summary, while LLMs exhibit the ability to memorize common patterns of popular DS API usage through massive training, they overall lack genuine comprehension of the underlying numerical constraints.

## 1 Introduction

Data science (DS) is an emerging and important area that combines classic fields like statistics, databases, data mining, and machine learning (ML) to gain insights via complex operations on the abundance of available data . DS libraries (such as PyTorch  and NumPy ) contain thousands of APIs used by developers and data scientists to process/analyse data. These DS APIs serve as the fundamental building blocks for almost all important ML/DS pipelines, and have penetrated into almost every corner of modern society, including financial systems [18; 4], autonomous driving software [9; 27; 46], coding assistants [45; 37], etc. Due to their high importance and wide usage, automatically synthesizing valid DS programs has been a critical research area [29; 21; 47].

One key challenge of DS code generation is to satisfy the complex constraints within each DS library API. DS library APIs perform transformations (e.g., matrix multiplication) on inputs (i.e., arrays or array-like objects) with numeric constraints on API parameters and inputs. Figure 1 shows an exampleof a typical _DS program_ where the DS library (i.e., PyTorch) is first imported, followed by creating some input_data, and then performing the data manipulation operation on the input_data using a DS API (torch.nn.Conv2d). The parameters of the API (e.g., kernel_size, groups) must satisfy the corresponding constraints between API parameters and the properties of the input_data. We refer to _API constraints_ as the set of relationships between properties of input_data and API parameters that, if and only if when satisfied, leads to a valid DS API invocation. As seen in Figure 1, not only are there constraints between the properties of the input_data and API parameters (e.g., kernel_size \(\) H + 2*padding), but there are also constraints within API parameters (e.g., out_channel %, groups = 0). These constraints are defined by developers according to the functionality of each DS API, and are usually specified in natural language within the API documentation. Such complex constraints are critical for DS applications, and DS users or even DS experts may unintentionally violate such constraints .

Large language models (LLMs) have achieved tremendous success in processing code . Due to their powerful code understanding and generation ability, LLMs have been applied to various coding tasks , such as code completion , program repair , and test generation . For DS libraries, LLMs have been applied to solve practical user queries on StackOverflow  and even generate test programs to detect bugs in modern ML frameworks . Prior work assumes LLMs, through massive training, can already implicitly model constraints in DS APIs by learning from numerous correct DS API uses . However, this assumption has not been systematically verified. Furthermore, popular DS-specific benchmarks like DS-1000  do not specially test the LLM's ability to satisfy implicit constraints and instead focus on how to apply DS APIs to solve data analysis tasks. These gaps in prior research raise a critical question: _Can LLMs implicitly learn the numeric constraints in data science APIs?_

**Our work.** To answer the question, we conduct a rigorous study on the performance of LLMs in generating valid DS programs satisfying diverse numerical API constraints. We collected a set of 28 representative DS library APIs across two widely-used Python DS libraries (PyTorch and NumPy), each with their unique constraints/setup. Additionally, we categorize each API's constraints into different categories (e.g., equality and arithmetic) and perform in-depth experiments on each constraint type. To support our analysis, we systematically created 3 generation settings: full program, all parameters, and individual parameters, designed to test the LLMs under different evaluation scenarios. Additionally, we vary the difficulty level by adjusting the inputs to explore LLM behaviours when asked to solve more complex API constraints or given more unnatural inputs.

Interestingly, contrary to the popular assumption in prior work, while LLMs can easily satisfy constraints when the inputs are simple, we observe that the performance drops drastically as we increase the difficulty or provide more unusual inputs. We found that LLMs tend to generate simple and common inputs seen during training, highlighting that LLMs are often memorizing patterns instead of truly understanding the actual DS API constraints. For example, for the widely used Conv2d API shown in Figure 1, when max(in_channels,out_channels) is set to , even GPT-4-Turbo  can only predict the correct value of groups \(\)24% of the time, while the other models are below 14%. Furthermore, based on our experimental findings, we constructed DSeval, the first benchmark for systematically evaluating LLMs' capabilities in understanding the important numerical API constraints for popular DS libraries. DSeval contains 19,600 different problems across 12 representative APIs to extensively compare and contrast the performance of different LLMs. DSeval supports lightweight and fast evaluation by extracting LLM generated parameters and quickly verifying the correctness using state-of-the-art SMT solvers (such as Z3 ) to avoid time-consuming execution-based evaluations. Our evaluation on eight state-of-the-art open-source and closed-source models shows that while all studied models struggle with more difficult problems, GPT-4-Turbo consistently achieves the highest accuracy across all difficulty levels. For example, GPT-4-Turbo achieves an average accuracy of 57.5% for _hard_ constraints of PyTorch APIs, while the best open-source model can only achieve 39.2%, demonstrating the huge gap between large proprietary models and other open-source LLMs. Our design of DSeval is general and can be easily extended to additional libraries and APIs for the DS domain and even beyond.

Figure 1: Example DS program with constraints

[MISSING_PAGE_FAIL:3]

freedom to generate any type or size for the input_data. As such, the LLM may choose very simple input_data and API parameter values that can easily satisfy the constraint.

**All parameters.** In the all parameters setting, we directly provide the input_data for the API. Figure 2b also shows an example of the input for the API torch.nn.MaxPool2d where the LLM just needs to output the API parameters. This setting evaluates if/how LLMs can accurately solve the constraints as we vary the input_data with more difficult or uncommon cases. Still the LLM has full freedom to pick the full combination of parameters to satisfy the required constraint.

**Individual parameter (main setting).** To perform a finer-grained evaluation, we introduce the individual parameter setting where we ask the LLM to generate a single parameter of the API. Figure 2c additionally demonstrates an example for np.reshape where we only allow the LLM to fill in a single parameter value of newshape. Furthermore, we can also add an additional constraint by directly providing the first value of newshape (2 in the example). This makes the problem even more challenging where instead of being able to simply copy the input_shapes, the LLM now has to reason with the partial shape given and compute the final correct shape to satisfy the constraint. Compared to the prior two settings, the choices here are much limited. This makes the task harder to fully evaluate how LLMs solve complex API constraints, and serves as our main setting.

### Input creation and output validation

**Creation.** To produce the inputs for each of the 3 settings, we use a fixed set of templates for each API. For the full program setting, we produce one input per API, changing only the API name in the input instruction. For the all parameters setting, we vary the input_data given to the API. In particular, we focus on two properties of the input_data: 1) rank of the input_data and 2) each dimension value. We create randomized inputs and increase the difficulty by either increasing the rank or the dimension values to measure the LLM performance. Note that input rank or dimensionality can affect different APIs depending on the specific numeric constraints (Table 1). For example, an API like torch.nn.SoftMax that has a constraint of -rank \(\) dim \(<\) rank will have its difficulty influenced by the actual rank of the input tensor. On the other hand, an API like torch.nn.Conv2d has a constraint of in_channels % groups = 0, which depends on the actual dimension value of the input (i.e., in_channels). As the dimension value of in_channels increases, it will be more difficult to select the groups parameter that can divide it evenly. Therefore, we increase the difficulty of different APIs based on whether the constraint depends on the rank, dimension, or both. Similarly, for the individual parameter setting, we also randomize the input_data based on the previous two properties. Additionally, we pick the parameters with interesting constraints for the LLM to predict in order to be representative and cover the major constraint types. Furthermore, since we only ask the LLM to produce a single parameter value, we also vary the other parameter values in the API to add additional constraints (details discussed in Section 4.3).

To ensure the input is valid, we leverage satisfiability modulo theory (SMT) solvers as shown in Figure 3a. SMT solvers, such as Z3 , are tools which can be used to solve an SMT problem of determining whether a mathematical or first-order logic formula is satisfiable . We first encode the API constraints into an SMT formula. We then randomly generate _concrete values_ for the input_shapes and leave the other parameters that we want the LLM to generate as _symbolic variables_. Next, we use an SMT solver to check if the constraints are satisfiable (i.e., there exists a set of values for each symbolic variable that can satisfy the constraint). If it is satisfiable, the input we provide to the LLM is valid, otherwise we restart the process by randomly selecting the concrete values. In our study, we reuse the encoded API constraints provided by NNSmith  (a popular tool for testing ML libraries via formal constraint solving) and add additional ones when needed.

**Evaluation.** To evaluate the validity of the DS programs generated by the LLMs, we first parse the output to extract the input_data and API parameters. We then check if the LLM predicted

Figure 3: Example usage of constraint solvers to generate inputs and validate outputs.

values are valid. This is also done via SMT solving as demonstrated in Figure 2(b) where we use the SMT formulas and, this time, check if all the concrete values generated are valid according to the constraints. Note that such light-weight constraint solving can support much faster validation than actually executing the generated DS programs, while still providing the same guarantee.

## 3 Experimental Setup

### Subjects

We construct a dataset with 28 representative APIs in total from two popular DS libraries: PyTorch [(18)] and NumPy [(10)]. For our API selection process, we begin by referencing prior work NNSmith  and examined all 73 core operators it supports. From these, we select 22 core APIs that have numeric parameter constraints and add additional 6 APIs to obtain the 28 APIs used in our study for both the full program prediction setting (Section 4.1) and the full API parameter prediction setting (Section 4.2). For a more detailed analysis, we select 12 APIs to cover the representative types of numeric constraint for examination in the single API parameter prediction setting (Section 4.3) and in our DSeval benchmark (Section 4.4). We use "representative" to mean representative with respect to the numeric parameter constraints in DS library APIs. Table 1 shows the categorization of the different types of numeric constraints that exist in DS libraries. Our selection criteria aim to select a list of APIs that have interesting numeric parameter constraints that can cover all the major constraint categories. A complete list of the 12 APIs and their corresponding constraints is provided in Table 3 in the Appendix.

We focus on the 3 settings described previously to analyse the performance of LLMs. For the full program setting, we generate a single input prompt per each studied API and ask the LLMs to synthesize the complete DS program by varying the sampling temperature. For the all parameters setting, we have 14 difficulty settings, each with 200 different inputs per API, and use greedy decoding to obtain the LLM solutions. The difficulty setting is controlled by increasing the rank of input_data (from 2 to 8 in intervals of 1) with default dimension value as , and increasing the dimension value (i.e., [1,4), [4,8),..., ] with default rank as 3, separately. Finally, in the single parameter setting, we select one parameter for each API for the LLM to generate. For any parameters irrelevant to the constraint, we use the default value if it is an optional parameter, and randomly choose from a reasonable value range if it is a required parameter (Appendix C). We adopt the same difficulty setup and greedy decoding strategy as the all parameter setting.

### Metrics

**Validity.** To measure validity, we directly extract the LLM output predictions and evaluate according to the process described in Section 2.3. We define _accuracy_ as the percentage of valid programs produced by the LLMs in each difficulty setting.

**Diversity.** To measure diversity, we compute the _unique valid rate_: the percentage of unique valid programs generated via sampling. Note that we deduplicate by extracting the input shapes and numeric parameters, ignoring the irrelevant parameters and irrelevant code suffix.

### Studied models.

We evaluate 8 popular state-of-the-art LLMs, including both closed-source and open-source models (detailed list shown in Table 2). For both the full program and all parameter settings, we only present the results for DeepSeek Coder-33b , state-of-the-art open-source model, due to the space limit (other models follow similar trends). For the individual parameter setting (the main setting), we focus on the DeepSeek Coder family models (33b, 6.7b, and 1.3b) as well as GPT-4-Turbo (2024-04-09), covering both state-of-the-art open-source and close-source models, as well as models with different sizes. Apart from the full program setting, where the LLM generates a complete program, we perform infilling using the studied LLMs' model-specific infilling format. To perform infilling using GPT-4-Turbo, we design a specialized prompt (see Appendix H). Unless otherwise stated, we use greedy decoding (i.e., temperature = 0) and temperature of 1 when sampling for diversity evaluation.

## 4 Evaluation

### Full program prediction

To start with, we ask the LLM (DeepSeek Coder-33b) to predict the entire DS program from scratch given just simple instructions. Figure 4a shows the overall accuracy of the 18 APIs in PyTorch and 10 APIs in NumPy. We see that with low temperature the model has near perfect accuracy on almost all the APIs and as temperature slowly increases, the accuracy tends to drop (ending with around 0.5\(\)0.8 with temperature=1). Surprisingly, we found that for torch.nn.Fold, which contains the most complex constraint, the LLM failed to produce any valid DS programs. This demonstrates that LLMs may still struggle with satisfying the extremely difficult constraints even when given the full freedom of generating any input values. Furthermore, in Figure 4b, we plot the proportion of unique valid programs generated by the model as we vary temperature. Of course when sampling at low temperatures, many of the inputs will be repeated, leading to low number of unique programs in general. In particular, the input shapes are often from widely-used computer vision datasets like 3*224*224 from ImageNet . This indicates the LLMs tend to memorize some common patterns from either documentation or user programs. However, we see that even though the unique valid rate increases with high temperatures to give more diverse and creative outputs, the percentage of unique valid programs can still be mostly below 50%. This demonstrates that while models are successful in generating a high percentage of valid programs, a lot of generated programs are repeated.

### Full API parameter prediction

Figure 5 shows the setting where we randomly provide an input_data and ask DeepSeek Coder-33b to complete the valid parameters of the API. We vary the difficulty by changing either the rank or the dimension value ranges of the input_data to produce more complex and unnatural inputs. We use greedy decoding (temperature 0) to generate one solution per problem, and compute the

Figure 4: Full program prediction result on all 28 APIs (\(\) PyTorch and \(\) NumPy).

Figure 5: Full API parameter prediction result on all 28 APIs (\(\) PyTorch and \(\) NumPy). The LLM has near 100% accuracy on some APIs, which are collectively referred to as others(x), where x is the number of grouped APIs.

average valid rate across the randomly created problems to compute accuracy for each difficulty level. Compared to Section 4.1 where LLMs achieve near-perfect accuracy for almost all APIs with low temperature like 0.2, we observe that the accuracy quickly drops when simply randomizing the input shape, especially for APIs with more complex constraints. This indicates that the learned patterns cannot easily generalize to less common input shapes. We further performed an interesting case study on the PyTorch API Linear, and found this phenomenon holds true across different models (Appendix D). However, we see that the majority of APIs maintain high accuracy even as difficulty increases (others(x) in Figure 5). This is because these APIs have relatively easy constraints. For example, APIs like max or argmax only require predicting a single integer representing the dimension to operate on, and the LLMs learn to predict dim=1 or just rely on the default parameter values of the API which are always valid.

### Single API parameter prediction

We now focus on the main finer-grained evaluation setting where we ask LLMs to predict a single parameter value and discuss the input setup, results, and findings for each API separately. Here, we only discuss representative API constraints from each category and full results are in Appendix F.

**Equality.**BatchNorm2d in PyTorch applies batch normalization  on a 4D input tensor, with the second dimension as the number of features. We select the parameter num_features for the models to predict, with the equality constraint that num_features = input_shapes\(\). Figure 5(a) shows the results as we increase the difficulty by changing the maximum possible value for each input

Figure 6: Single API parameter result. Solid lines (except Fig. 5(c)) show the accuracy of using greedy decoding (temp=0). In Fig. 5(b), dashed lines show the pass@1 accuracy in sampling experiments with temp=1. In Fig. 5(d), dotted lines show the accuracy after excluding trivial solutions. In Fig. 5(h) and 5(i), we use *-Inst. to distinguish between the generation settings: infilling (GPT4-Turbo) and free-form generation (GPT4-Turbo-Inst.). More details are provided in Appendix H and I.

dimension. We observe that the DeepSeek Coder models drop from around 0.7\(\)0.8 to less than 0.5, while GPT-4's performance stays around 0.9 throughout different difficulty levels.

_Finding: Overall, we found that smaller LLMs even struggles with even the simple constraint of copying an existing value, while large state-of-the-art LLMs can maintain its high performance._

**Inequality.**max in PyTorch computes the maximum value along a dimension. The parameter we target is dim with the valid range being [-rank, rank). In Figure 5(b), when using greedy decoding, all 4 LLMs achieve close to perfect accuracy. Therefore, we also conduct sampling experiments and present the pass@1 accuracy and diversity in Figure 5(b) and 5(c). For max we compute the diversity differently from Section 3.2 (see Appendix G), since the number of possible unique valid outputs is very small. Interestingly, the smaller DeepSeek Coder-1.3b model achieves highest sampling accuracy for rank=8, but has the lowest diversity. This is because the smaller model often predicts common values like 1, whereas the larger model (33b) can explore various correct answers like -1,2.

_Findings: We found that larger models are indeed better at capturing the simple inequality constraints and modeling the true probability of various possible values, while smaller models tend to memorize common patterns, leading to less diverse predictions._

**Arithmetic.**reshape in both PyTorch and NumPy attempts to rearrange the dimensions in the input_data, with the constraint being \(_{i}[i]==_{j}[j]\). Since we found that it is common for the LLMs to simply predict the same shape or a permutation of the original, we add an additional constraint: we specify the first dimension of the new_shape to be different from any dimensions in input_shapes. Figure 5(d) shows the results as we vary the ranks of the input_data for PyTorch (similar trend in NumPy). We observe that most LLMs in the beginning perform well; however, as the difficulty increases, their performance drastically lowers. Meanwhile, GPT-4-Turbo performance does not drop even with more difficult inputs. We found the reason is that GPT-4-Turbo tends to always predict the special -1 value for reshape where the new_shape will be automatically inferred by the library. Figure 5(d) showcases this exact phenomenon in PyTorch (similar trend as NumPy) where dotted lines present the accuracy of any outputs without -1. We see that now even GPT-4-Turbo struggles in generating valid parameters without using the -1 crutch for the constraint.

Conv2d in PyTorch applies a 2D convolution over a 4D input tensor. The LLMs are asked to predict the parameter groups, where they have to divide both in_channels and out_channels evenly. The default value for groups is the trivial \(1\) (and therefore always valid). To ensure that there is at least one non-trivial value for groups, we randomly sample in_channels and out_channels within the value range such that their greatest common divisor is greater than 1. Figure 5(e) shows that the accuracy steadily drops as we increase the magnitude of values: even GPT-4-Turbo can only solve \(\)24% of the hardest subset of problems, which other models drop below 14% for the same problems.

Fold in PyTorch aims to combine an array of sliding local blocks into a large containing tensor. The constraint required for fold is the most complex out of all studied APIs where the LLM tries to generate a k_size tuple, and the product of the tuple must divide the 2nd index of the input_shapes evenly. Furthermore, it also needs to satisfy a complex equation over multiple parameters as shown in Figure 5(f). We use the default values for all parameters other than out_size and ask LLMs to produce the correct k_size. Shown in Figure 5(f), due to the complexity of the constraint, even on the lowest difficulty with small values, LLMs achieve relatively poor accuracy compared to other APIs. As we increase the values, the accuracy drops to nearly 0%. This highlights the high degree of difficulty in many DS APIs which current LLMs cannot reliably solve.

_Findings: Arithmetic parameter constraints in DS APIs are extremely challenging for all LLMs. Our results show that current state-of-the-art LLMs cannot effectively solve such complex constraints with their performance drops drastically and even sometimes drops to zero as we increase the difficulty._

**Set-related.**transpose in NumPy attempts to rearrange/transpose the input_data according to the given new_dim. In transpose, the constraint is that the model-predicted new_dim must be a permutation of the original dimensions in input_data. We found that the LLMs tend to predict very simple permutations; as such, similar to reshape, we directly provide the first dimension of new_dim to increase the difficulty. We see that in Figure 5(g), LLMs generally perform well on solving this constraint, and their performance improves with larger model sizes. Interestingly, the lowest difficulty of rank = 2 has a drop in performance. We theorize that this is because when the rank is 2, it is more common to directly call transpose() without any additional arguments. Therefore, the LLMs struggle a bit when given this unnatural task when asked to predict new_dim in low ranks.

_Findings: We found that LLMs generally perform well across the set-related constraints, and their performance scales with increasing model sizes. However, they still struggle with uncommon or unnatural inputs that are no commonly seen during training._

Instruction-tuned models.We additionally investigate the performance of instruction-tuned (IT) LLMs  with chain-of-thought (CoT) prompting . Due to computational limitations, we selected 3 constraints from PyTorch on which GPT-4-Turbo (without CoT) performs poorly for this experiment and analysis. The detailed experimental setup is described in Appendix I. Recall that for Conv2d, the task is essentially to predict groups such that it is a common divisor of two integers. As we observe that some models tend to predict a trivial answer \(1\), we specifically mention "Don't set groups=1" in the prompt and consider such answer as invalid in evaluation. From Figure (h)h, we observe that GPT-4-Turbo with CoT performs well at this non-trivial task, maintaining over 85% accuracy even with values up to 255. By contrast, the best open-source model can only solve 22%! This shows that although models like CodeQwen achieves close performance to GPT-4-Turbo on existing popular benchmarks like HumanEval, there is still a huge gap in terms of coding and math reasoning ability between GPT-4-Turbo and other open-source models. Meanwhile, when we use the same setup on the extremely difficult constraint in Fold, we see that even GPT-4-Turbo fails to perform well (less than 5% accuracy in later difficulty settings). This demonstrates that while CoT prompting may elicit better performance in constraints like in Conv2d, it still cannot effectively handle other more complex arithmetic constraints. In addition to CoT, we also test ReAct , another prompting strategy to elicit more reasoning process from LLMs. We observe that while ReAct can perform better than CoT, it still fails to solve more complex arithmetic constraints (detailed in Appendix J). Additionally, we attempt to include API documentation in prompts, but found that this does not always improve performance on our tasks (detailed in Appendix K).

### DSeval: A public benchmark for numerical DS API constraints

Based on the above findings, we further construct a public benchmark - DSeval with the same individual parameter prediction setting and the same representative set of APIs as studied in the Section 4.3. For each API in the benchmark, there are 7 different difficulty settings (grouped as 2 _easy_, 3 _medium_, and 2 _hard_ ones) and each with 200 randomly created problems. In total, this gives us 19,600 problems in DSeval to extensively evaluate the performance of different LLMs.

Table 2 shows the accuracy and diversity of all 8 models. First, we observe that the LLMs' accuracy drops when increasing the difficulty levels on the benchmark problems. This is also reflected by prior results where LLMs across the board struggle with more difficult problems. Next, we see that GPT-4-Turbo consistently achieves the highest accuracy across all difficulty levels, showing the gap between state-of-the-art proprietary models and other open-source LLMs. Furthermore, we observe some interesting ranking changes across difficulty levels. For example, while CodeQwen1.5  achieves the second-best performance in the lowest difficulty level, its performance drops substantially on the medium and hard problems (second worst on PyTorch medium and hard). Other models like StarCoder  improve their relative performance and achieve higher ranking on more difficult

    & &  &  \\  & Size &  Easy \\ Acc (\#) \\  &  Medium \\ Acc (\#) \\  &  Hard \\ Acc (\#) \\  &  Div (\#) \\  &  Easy \\ Acc (\#) \\  &  Medium \\ Acc (\#) \\  &  Hard \\ Acc (\#) \\  &  Div (\#) \\  \\   GPT-4-Turbo \\  } &  & 77.2 (1) & 66.2 (1) & 57.5 (1) & - ( ) & 95.3 (1) & 85.1 (1) & 71.4 (1) & - (-) \\   DeepSee \\  } &  & 33b & 64.7 (5) & 41.5 (4) & 28.2 (5) & 25.8 (6) & 78.5 (3) & 57.0 (2) & 48.8 (3) & 20.9 (1) \\  & & 6.7b & 66.2 (3) & 39.8 (5) & 33.4 (4) & 38.8 (4) & 73.3 (5) & 45.8 (8) & 35.6 (7) & 17.6 (7) \\  & & 1.3b & 59.0 (8) & 34.4 (6) & 26.8 (6) & 36.2 (5) & 63.4 (8) & 46.3 (7) & 30.5 (8) & 17.8 (6) \\   CodeLama \\  } &  & 64.7 (6) & 44.6 (3) & 34.8 (3) & 39.2 (3) & 74.4 (4) & 48.5 (6) & 36.8 (6) & 18.9 (3) \\  & & 7b & 62.6 (7) & 32.7 (8) & 13.8 (8) & 21.2 (7) & 67.1 (7) & 53.2 (5) & 45.4 (5) & 18.7 (4) \\   StarCoder \\  } &  & 65.6 (4) & 46.3 (2) & 39.2 (2) & 39.9 (2) & 70.8 (6) & 56.7 (3) & 51.5 (2) & 18.3 (5) \\   @ CodeQwen1.5 \\  } &  & 67.5 (2) & 33.2 (7) & 25.2 (7) & 53.2 (1) & 80.0 (2) & 54.7 (4) & 47.1 (4) & 19.3 (2) \\   

Table 2: DSeval benchmark result. Each column shows both the accuracy/diversity and ranking (\({{{{{{{{{{{{{{{{{              , showing that different LLMs can perform differently depending on the input and constraint required to satisfy.

We also study the diversity (see Appendix G for more details) of the LLM outputs, except we do not study GPT-4-Turbo due to its cost. Interestingly, LLMs which achieve high ranking in accuracy do not necessarily perform well in generating diverse correct solutions. This indicates that certain LLMs generate similar solutions to satisfy the constraint, without paying attention to the specific context. Therefore, they are not suitable for tasks like fuzz testing  which requires efficiently exploring a large solution space, or for tasks involving uncommon API usage. We further categorize some common mistakes made by LLMs on DSeval and provide additional insights in Appendix E. Overall, DSeval serves as the first benchmark to systematically evaluate the performance of LLMs on satisfying complex numeric API constraints for popular DS libraries and can be extended to support additional APIs and DS libraries.

## 5 Related work

LLMs for code.LLMs have made remarkable advancements in a wide range of coding tasks, including code synthesis [60; 10; 2], debugging [11; 8], repair [53; 54; 7], and analysis [36; 56; 55]. Notably, recent works [29; 16] also demonstrated LLMs' effectiveness in synthesizing DS code, which requires programming proficiency in DS APIs from specialized libraries such as NumPy  and PyTorch . Trained on billions of code including such DS code, LLMs, such as StarCoder  and DeepSeek Coder , have been extensively evaluated on DS code synthesis tasks. However, no prior study has systematically examined whether LLMs can indeed understand numerical API constraints of these scientific libraries instead of just memorizing the trained data .

Coding benchmarks for LLMs.Most code generation benchmarks [10; 33; 2; 22] are formulated with a natural language description and tests to verify the functional correctness of LLM-generated code. However, these benchmarks mostly target general-purpose code. To access LLM code generation for DS tasks, DS-1000  is created by collecting real DS problems from StackOverflow, and Arcade evaluates LLMs' ability to solve multiple interrelated problems within DS notebooks. Compared to existing DS benchmarks, our study explores different granularity levels to systematically evaluate to what extent LLMs can implicitly learn DS APIs' numeric parameter constraints.

Math reasoning of LLMs.To evaluate LLMs' arithmetic reasoning performance, GSM8K and other benchmarks [12; 42; 35; 24; 28] construct math problems in natural language requiring mathematical computations to solve. Compared to these existing benchmarks, problems designed in our study implicitly encode the arithmetic logic inside the DS library API, and thus can evaluate the LLMs' capability in understanding and solving numerical API constraints in the important DS libraries.

## 6 Conclusion

In this paper, we present the first systematic study on how LLMs understand the numerical API constraints for important DS libraries. Our study results show that current LLMs often memoize common patterns rather than truly understanding the actual numerical API constraints. Moreover, GPT-4-Turbo largely outperforms other open-source models and can well understand some simple arithmetic constraints using CoT. Based on our finding results, we also constructed DSeval, the first benchmark (with 19,000 problems) for systematically evaluating LLMs' capabilities in understanding the important numerical API constraints for popular DS libraries (such as PyTorch and NumPy).