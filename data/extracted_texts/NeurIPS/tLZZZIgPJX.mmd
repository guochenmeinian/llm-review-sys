# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

### Related Work

Several studies have addressed the issue of self-preference bias; however, there is a lack of reliable metrics to quantify the extent of self-preference bias, and the fundamental causes of this phenomenon remain unclear. Panickssery et al. (2024) reported on the relationship between self-preference bias and self-recognition ability, but their work lacked a comparison with human evaluators. Xu et al. (2024) and Stureborg et al. (2024) addressed quantifying self-preference bias within an evaluation approach where LLMs assign an absolute score to a single generated text. In this approach, gold-standard evaluators are required to assign scores based on abstract criteria while ensuring consistency with prior evaluations, making it challenging to obtain accurate assessments. As a result, these studies often restricted their scope to specific tasks, such as text summarization or machine translation, and relied on reference-based metrics like BLEURT (Sellam et al., 2020), which does not reflect the diversity of real-world use cases.

By contrast, a pairwise evaluation approach that involves direct comparison between two texts enables evaluators to recognize specific differences more readily, resulting in more consistent human judgments. Consequently, such pairwise evaluation methods are particularly suitable for analyzing biases related to discrepancies with human evaluations.

### Contributions

The contributions of this paper are threefold: (1) We propose a new metric to quantify self-preference bias in LLMs; (2) Using this metric, we evaluate the extent of self-preference bias across eight different LLMs; and (3) We identify a tendency for LLMs to assign higher ratings to texts with lower perplexity while exploring potential causes for this self-preference bias.

## 2 Preliminaries: Fairness and Bias

The measurement of bias in classifiers has been widely discussed within the framework of fairness (Calders et al., 2009; Hardt et al., 2016). In this section, we focus on a representative definition of fairness, Equal Opportunity, as the preparation for quantifying the self-preference bias in LLM evaluators.

Figure 1: **How much do LLMs prefer their own responses over human evaluations?** (a) illustrates an example where GPT-4 favors its own response, even when human evaluations prefer to a response generated by Vicuna-13B. (b) compares the self-preference bias scores using our proposed metric (Definition 3.1). These figures demonstrate that GPT-4 exhibits a stronger self-preference bias than other models, suggesting that it tends to rate its own outputs more favorably than human evaluations. For detailed experimental settings, refer to Section 3.

Equal Opportunity (Hardt et al., 2016) is a fairness definition that requires the classifier to achieve equal recall across groups with different sensitive attributes (e.g. gender or race). Specifically, let the sensitive attribute be \(S\{0,1\}\), the prediction of the classifier be \(Y^{}\{0,1\}\), and the ground truth label be \(Y\{0,1\}\). The classifier satisfies Equal Opportunity if the following condition holds:

\[P(Y^{}=1|S=1,Y=1)=P(Y^{}=1|S=0,Y=1)\] (1)

When quantifying bias in the classifier, the difference or ratio between both sides of the equation are often used. The amount of bias derived from this definition is the difference in how well the classifier matches the ground truth between groups with different sensitive attributes. Therefore, if there is already bias towards sensitive attributes during the creation of the ground truth, Equation 1 cannot be considered an ideal definition of fairness.

To address this drawback, the fairness definition known as Demographic Parity (Calders et al., 2009), which does not rely on ground truth, is also widely used. Demographic Parity requires that the predictive distribution of the classifier be consistent across groups with different sensitive attributes. It is based on the assumption that there is no inherent causal relationship between the sensitive attributes and the predicted labels.

LLM-as-a-judge is a technique aimed at replacing human evaluators with LLMs, and more specifically, it assumes that dialogue system are aligned based on human preferences. Therefore, in this study, we employ the concept of Equal Opportunity to quantify self-preference bias by treating LLM evaluators as classifiers.

## 3 Quantifying Self-Preference Bias

In this section, we propose a new metric to quantify self-preference bias. In particular, we focus on a setting where evaluators compare two texts and select the one higher quality, allowing comparison with reliable human evaluations.

### Self-preference Bias Metric

To measure the extent to which an LLM's evaluation deviates from human evaluations across its own responses and those generated by others, we employ the concept of Equal Opportunity (Hardt et al., 2016). The bias is quantified by calculating the difference between both sides of Equation 1. A rigorous definition is provided below.

**Definition 3.1**.: _(self-preference bias of the evaluator \(f\).) Let \(f\) be the evaluator assessing the quality of dialogue responses, capable of generating its own responses. For a pair of responses \(y_{0}\) and \(y_{1}\) in a dialogue, define: \(Y\) as the index of the human-preferred response, \(Y^{}\) as the index of the \(f\)-preferred response, and \(S\) as the index of the \(f\)-generated response. We define the self-preference bias of the evaluator \(f\) in dialogue comparison evaluation as follows:_

\[=P(Y^{}=1|S=1,Y=1)-P(Y^{}=1|S=0,Y=1).\] (2)

In this definition, the amount of bias is represented by the difference between the conditional probability of the evaluator \(f\) rating itself favorably given that the human evaluator has rated it favorably, and the conditional probability of the evaluator \(f\) rating itself unfavorably given that the human evaluator has rated it unfavorably. A value of \(0\) indicates the absence of bias, while a value close to \(1\) suggests a high degree of bias. Conversely, a value of \(-1\) would indicate the presence of a reverse bias, where the evaluator \(f\) tends to undervalue its own responses. In this definition, the case where \(y_{1}\) is preferred is explicitly considered, but this does not result in a loss of generality by treating the preferred response as \(y_{1}\).

### Experimental Setting

The aim of our experiment is to quantify the self-preference bias in various LLMs. To address a wide range of tasks and topics, we have LLMs evaluate responses in open-ended dialogues and measure the bias using the Definition 3.1.

First, we provide the LLM evaluator with a user query written by a human and two responses generated by different LLMs. We then ask the LLM evaluator to determine which of the two responses demonstrates higher quality. For clarity and ease of reference during evaluation, we designate the two responses as "response A" and "response B". Finally, using the probabilities that the LLM evaluator outputs for "A" and "B", denoted as \(p(|)\) and \(p(|)\), we calculate the score for response A using the following equation.

\[_{}=|)}{_{w\{ {A, B}\}}p(w|)}\] (3)

The score for response B is calculated in the same manner. This post-processing, as shown in Equation 3 is designed to enhance the interpretability of analyses across LLMs with different probability distributions, following the approach of Schick et al. (2021). Additionally, according to Zheng et al. (2024), LLMs may exhibit position bias, which refers to the tendency to prefer responses located in specific positions within the prompt. To mitigate this bias, we swap the positions of responses A and B, and the evaluation scores are averaged over two iterations. Using the scores obtained through the above process, we examine the extent to which there is a difference in evaluation scores between the LLM's own generated response and responses generated by other LLMs.

Empirical evaluation uses Chatbot Arena dataset (Zheng et al., 2024), which contains 33,000 dialogues and each consisting of a user query and a pair of responses generated by two different LLMs. We calculate evaluation scores for the pre-existing response pairs stored in the dataset. In this dataset, every response pair is labeled with either "model_a", "model_b", or "tie" as a result of a human evaluation comparing the quality of the responses.

As the LLM evaluators, we employed the following eight LLMs, which also used in Chatbot Arena dataset: the closed-source GPT-3.5-Turbo and GPT-4 (Josh et al., 2024), and the open-source Vicuna-7b, Vicuna-13b (Chiang et al., 2023), oasst-pythia-12b (Biderman et al., 2023), dolly-v2-12b (Conover et al., 2023), Koala-13b (Geng et al., 2023), and stablelm-tuned-alpha-7b (Jonathan Tow). We used the same prompt as Zheng et al. (2024) to calculate the evaluation scores of responses A and B for all LLM evaluators. This prompt was created based on the work of Zheng et al. (2024), and it instructs the LLM to output either "[I[A]]", "[I[B]]", or "[I[C]]" after providing an explanation. In the implementation, we used the output probability distribution following the token corresponding to "[I" in the generated text to compute the score using Equation 3. Additionally, if the LLM evaluators failed to comply with the prompt instructions and did not provide a response in the format of "[I[A]]" or a similar structure, these cases were excluded from the analysis.

### Result

The results of the bias measurement using Definition 3.1 are presented in Figure 0(b). It was confirmed that GPT-4 exhibits the highest self-preference bias. Definition 3.1 focuses on the recall of the LLM evaluator concerning both high and low ratings by the human evaluator. Thus, it can be concluded that GPT-4 showed lower recall in cases where humans evaluated unfavorably compared to when higher evaluating. When examining the recall values in the confusion matrix shown in Figure 2, they are calculated as \(0.945\) and \(0.425\). The difference between these values is \(0.520\), which corresponds to the value reported in Figure 0(b). Following GPT-4, Vicuna-13b and Koala-13b also exhibited significant bias. In contrast, other LLMs displayed values relatively close to zero. Notably, oasst-pythia-12, dolly-v2-12b, and stablelm-tuned-alpha-7b showed negative values, indicating a reverse bias where the LLMs tend to underestimate their own outputs.

Table 1 presents a randomly selected example of self-preference bias in GPT-4, where humans favored the alternative response. In this example, the user query is a straightforward request to list blue items. While GPT-4 states that it lacks physical recognition before listing, GPT-3.5-Turbo directly lists the blue items without any such explanation. Both responses are of high quality, and the final evaluation reflects the evaluator's policy and stylistic preferences. Although humans and GPT-3.5-Turbo preferred the response from GPT-3.5-Turbo, GPT-4 favored its own response, illustrating a typical case of self-preference bias.

[MISSING_PAGE_FAIL:5]

human for response A within each bin. In this experiment, we excluded GPT-4 and GPT-3.5-Turbo, as perplexity values could not be obtained for these models.

Figure 3 shows the comparison of winning judgment rates within each perplexity bin for six models. All models except stabllem-tuned-alpha-7b demonstrated a clear tendency to assign higher evaluations to responses with lower perplexity. Furthermore, it was confirmed that this tendency was stronger in vicuna-13b, vicuna-7b, koala-13b, and oasst-pythia-12b than in humans. This result indicates that LLM evaluators overly change their evaluation depending on perplexities of responses.

To further investigate the effect of differences in whether the output is self-generated or not, we segmented the evaluation results by the LLM evaluators into two distinct groups: one where the LLM evaluators' own output is included in the pair and another where it is not. We present the results with the output of the LLM evaluator as response A, without loss of generality. As shown in Figure 4, the winning judgment rates between two groups were similar for all models except dolly-v2-12b and stable-tuned-alpha-7b. This suggests that the factor influencing the LLM evaluators' judgments is not whether the response is their own but rather the perplexity of the responses. As confirmed by Figure 5, LLMs tend to exhibit lower perplexity for their own outputs. In other words, these findings imply that self-preference bias may be a phenomenon where the model's own output inherently exhibits lower perplexity.

In these experiments, we were unable to obtain perplexity values from GPT-4 and GPT-3.5-Turbo, resulting in a lack of analysis on the competitive LLMs. To address this gap, we conducted additional experiments using Llama2 (Touvron et al., 2023) and Llama3 (Dubey et al., 2024). However, the Chatbot Arena dataset does not contain human annotations for these LLMs' responses. Therefore, we obtained evaluations for existing responses from these Llama models and only compared them with human evaluations.

The results are presented in Figure 6. We found that all models changed their evaluations more than humans, depending on the difference in perplexity. This indicates that even in the competitive models, including Llama 3.1, perplexity may be causing unfair bias in the evaluation.

Figure 3: **LLMs vs human conditioned on perplexity.** Winning judgment rates by LLMs conditioned on perplexity with human winning judgment rates are plotted. All models except dolly-v2-12b and stabllem-tuned-alpha-7b demonstrated a clear tendency to assign higher evaluations to responses with lower perplexity.

## 5 Discussion

To reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized. Therefore, we believe that our

Figure 4: **LLM vs other LLMs conditioned on perplexity.** Winning judgment rates by LLMs on their own texts and texts generated by other models conditioned on perplexity are plotted. Across all models, except for dolly-v2-12b and stablelm-tuned-alpha-7b, no significant difference was observed between the judgment rates for their own texts and those generated by other models. This suggests that LLM evaluators assign higher ratings to texts with lower perplexity, regardless of whether the text was self-generated or produced by other models.

Figure 5: **Average of log-perplexity of responses for each LLM evaluator.** The red bars represent the perplexity for responses generated by the LLM evaluator itself, while the blue bars represent the perplexity for responses generated by other LLMs. Across all models, the average perplexity is lower for responses generated by the evaluators themselves.

research makes a significant contribution to the understanding of self-preference bias and will greatly facilitate the development of future research in this area.

Our experimental results reveal that LLM evaluators tend to assign higher scores to texts with lower perplexity. We further discuss the reasons behind this phenomenon. First, LLMs are trained during the pretraining phase to reduce perplexity on large-scale text corpora. Moreover, when aligning with human preferences, the models are also trained to minimize perplexity on the given dialogue data. Therefore, high-perplexity texts are likely those that the LLM has not frequently encountered during training, suggesting that such texts may be related to domains that the LLM evaluators do not fully comprehend.

This observation may seem contradicted by the fact that GPT-4, which is well-versed across various domains due to a wide range of benchmarks, exhibits a high degree of self-preference bias. However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering. This suggests that, advanced models like GPT-4, which thoroughly understand and adhere to their predefined policies, may use the degree of alignment with these policies as a deciding factor when evaluating responses of comparable quality.

## 6 Conclusion

In this study, we propose a metric to quantify the self-preference bias in LLM-as-a-judge and measured the self-preference bias of eight LLMs. Experimental results confirmed that GPT-4, in particular, exhibits a high self-preference bias. This finding suggests a risk that GPT-4 as a judge may inadvertently reinforce its own style and policies.

Furthermore, we hypothesized that the self-preference bias is related to the perplexity of the texts, and showed that, compared to human evaluators, LLM evaluators assigned higher evaluations to texts with lower perplexity, and this tendency was observed regardless of whether the text was generated by themselves or not. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.

Figure 6: **Llama family vs human conditioned on perplexity. Winning judgment rates with condition on perplexity are plotted. Llama-2 and later models models exhibited significantly higher winning judgment rate with lower perplexity compared to human evaluators.**