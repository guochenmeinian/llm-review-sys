# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Here \(B_{}\) and \(B_{}\) are vectors of B-splines. We shall say that at time point \(T_{-1}\), a group of upstream researchers fit a deterministic ML function \((x)\) (which we shall refer to as the "upstream model") that approximates \((x)\). These researchers are willing to disseminate \((x)\), but do not provide any information beyond this, including sample size or error rates. This mirrors the way modern AI models, such as ChatGPT, are shared.

Later, a different group of researchers is interested in using a combination of real outcomes and upstream-model-generated synthetic outcomes for use in a downstream statistical model. As an example, this could occur in pragmatic clinical trails where one would like to learn the relationship between a clinical outcome, Y, and a biomedical covariate, X. However, if Y is expensive or difficult to collect, then one may use a previously trained prediction model to generate \((X)\) and regress that along with the collected Y's upon X (Gamerman et al., 2019, Williams et al., 2015).

In our paper, our goal will be to describe the linear relationship \((t)\), between \(y\) and \(x\) at each time step \(t[0,...,T]\). Formally, our goal is to estimate \(_{t}|^{t}=\{Y_{real}^{t},_{syn}^{t}\}\) where \(Y_{t,real}=\{y_{1,real}...,y_{t,real}\}\) and \(_{t,syn}=\{(x_{1,syn})...,(x_{t,syn})\}\). A class of estimators which can give parameter estimates using a combination of real and black-box generated synthetic outcomes is known as Inference on Predicted Data (IPD) estimators Hoffman et al. (2024). Here, we will use a particular IPD estimator known as Prediction Powered Inference (PPI). In Angelopoulos et al. (2023), the authors demonstrate that one can estimate a linear regression parameter \(\) (as well as other convex estimation problems) via a PPI estimator, \(^{PPI}\)

\[^{PPI}=^{Naive}+\]

where \(^{Naive}\) is the parameter from linearly regressing \((x_{syn})\) on \(x_{syn}\) and \(\) is the parameter from linearly regressing \((y_{real}-(x_{real}))\) on \(x_{real}\). Angelopoulos et al. (2023) and Angelopoulos et al. (2024) show that these estimators not only give unbiased estimates but can lead to tighter standard errors compared to traditional estimators.

### \(_{PPI,t}\) Updating Scheme

Here we describe the update scheme for \(_{PPI}\) at time point \(t\), which we denote \(_{PPI,t}\). We shall assume here that \(\) and \(\) are known (with full generality in the complete paper). Based on our data generating equation, we have:

\[p(_{t}|^{t}) =p(_{t}|^{t})\] (4) \[=p(_{t}|y_{real,t},y_{syn,t},^{t-1})\] (5) \[ p(y_{real,t},y_{syn,t}|_{t})p(_{t}|,^{t-1})\] (6) \[=p(y_{real,t}|_{t})p(_{t}|,^{t-1})*p(y_{syn,t}| _{t})p(_{t}|,^{t-1}).\] (7)

\(p(y_{real,t}|_{t})\) and \(p(y_{syn,t}|_{t})\) are easy to sample from as we get them directly from the data generating equation. To estimate \(p(_{t}|^{t-1})\) in an online fashion using a PPI estimator, we employ the dynamic structure used in McCormick et al. (2011) and assume that:

\[_{t}|_{t-1}  N(_{t-1}^{PPI},_{t-1}^{PPI})\] (8) \[ N(_{t-1}^{Naive}+_{t-1},_{t -1}^{Naive}+_{,t-1}).\] (9)

This yields a Kalman filter based prediction equation:

\[_{t}|_{t-1} N(_{t-1}^{PPI},R_{t-1}^{PPI})\] (10)

where:

\[R_{t-1}^{PPI}=_{t-1}^{Naive}/^{Naive}+_{t-1}^{ }/^{}\]

\(^{Naive}\) and \(^{}\) are fixed forgetting factors and are set to be less than 1. Estimation of the forgetting factors can be done via a model selection such as was done in McCormick et al. (2011).

To estimate \(_{Naive,t}\) and \(_{t}\), we can use the fact that they are estimated on separate datasets which allows us to decompose the posterior into the product of two independent linear regression. Further improvements to estimating these terms can be done via the approach described in Hofer et al. (2024).

\[p(_{t-1}^{PPI},_{t-1}|^{t-1})=p(_{t-1 }^{Naive}|Y_{syn}^{t-1})p(_{t-1}|Y_{real}^{t-1}).\] (11)

#### 2.1.1 Simulation

To demonstrate this updating scheme, we simulated a single iteration of the updating scheme described above. At time \(t=2\), we generated 5000 datapoints from the data generating process with \(=1\), \(x\) is a standard normal, and \(B_{}\) is a B-spline for x with 3 degrees of freedom using the **bs** function from the splines package in R R Core Team (2021) and \(=[2,4,-10]\). The upstream prediction model, \(\) was set to be a simple linear regression with intercept. Note that this is purposefully a misspecified model and thus should not give unbiased estimates of \(\). At the next time step, \(t=3\), 20 real samples and 100 synthetic samples were generated from the data generating process and a forgetting factor of 0.0001 and 0.001 for \(^{Naive}\) and \(^{}\) respectively (the choice of very small values of forgetting factors is due to the large discrepancy between sample sizes and because in this simulation we are only doing one iteration. In a more realistic example with more steps, this discrepancy will be smaller and the forgetting factors will be much larger).

Figure 1 illustrates 1000 posterior draws from \(\), \(^{Naive}\) and \(^{PPI}\). Because \(\) was trained in the past and is misspecified, the posterior of \(^{Naive}\) is very off, with a mean of 0.76 and standard error of 0.21. On the other hand, \(^{PPI}\) is much closer to the true value of 3 with a mean of 2.7 and a standard error of 0.39, which leads to an overall lower MSE of 2.25 versus 0.491. This underestimation bias in our final estimator is because our proposed estimator is combining information from the parameter estimate in the past and the present. In the case of this simulation, at \(t=2\), \(\) was smaller. A comprehensive parameter tuning procedure to determine \(^{naive}\) and \(^{}\) has the potential to give even more accurate MSEs for \(^{PPI}\).

### Sequential Experimental Design for \(x_{real}\) and \(x_{syn}\)

A unique aspect of doing dynamic regression using IPD-based estimators is that one is performing inference using a combination of synthetic and real data. However, as noted in Hoffman et al. (2024) as one diverges (either in time or in predictive accuracy) from the original upstream model, the optimal ratio of real and synthetic data to minimize the standard error of a PPI estimator changes.

Figure 1: 1000 posterior draws from (top): \(\), (middle): \(^{Naive}\), and (bottom): the PPI estimator. The vertical line at \(=3\) represents the true value of \(\). Note that the PPI estimator is the only one that reasonably contains the true value of \(\). The slight negative bias in the PPI posterior distribution can be explained due to the choice of forgetting factor. More comprehensive choice of forgetting factors via model selection has the potential to ameliorate this issue.

To account for this, we propose at each time point to employ techniques from Bayesian optimal experimental design are used to determine the optimal ratio of real to synthetic data.

Specifically, we shall assume that at each time point, the researchers are able to spend their limited budget \(C_{t}\) in two ways. They can spend it on collecting just \(x\) values, each with a cost of \(c_{syn}\) (which we shall refer to as "synthetic data") or spend \(c_{real}\) and collect both \((x,y)\) (which we shall refer to as "real data").

To determine the best ratio of real to synthetic samples, we shall use techniques from Bayesian optimal experimental design Ryan et al. (2015); Lindley (1956). A common setup in Bayesian optimal experimental design is to choose a design that maximizes the expected information gain at each time point. In our case, the space of our decisions are parameterized by \(_{t}(0,1)\) which is the fraction of budget, \(C_{t}\) allocated to collecting real (each at cost \(c_{real,t}\) with \(C_{t}/c_{real,t}\) samples if the entire budget was spent on real data) and synthetic samples (each at cost \(c_{syn,t}\) with \(C_{t}/c_{syn,t}\) samples if the entire budget was spent on synthetic data). Following the setup, this yields a compact expression for estimating the Expected Information Gain (EIG) Rainforth et al. (2018) :

\[EIG(_{t}) =E_{^{t}(_{t}),}[H(p(|^{t-1}(_{t} )))-H(p(|^{t}(_{t})))]\] (12) \[=E_{^{t}(_{t}),}[^{t}|)}{p (^{t})}].\] (13)

And Foster et al. (2019) illustrated that this double integral is well approximated via the nested-Monte Carlo approach of which each piece is estimable based on the derivations above:

\[EIG(_{t})_{i=1}^{N}^{t}_{n}|_{n, 0})}{_{i=1}^{M}p(^{t}_{n}|_{n,m})},_{n,*} p( |^{t-1}),^{t}_{n} p(^{t}_{n}|_{n,0})\]

Choosing, at each \(t\), the sample size ratio \(_{t}\) which maximizes our estimated EIG allows us to iteratively determine which type of sample-cheaper, but less reliable, synthetic data, or more expensive, but reliable, real data-would be most beneficial. An interesting point to note about this trade-off is that since, in the PPI estimator, the real samples help better estimate the bias of \(\) while the synthetic samples help estimate the variance of \(\), this procedure to balance between real and synthetic samples using \(_{t}\) can also be thought of as an iterative bias-variance trade off.

## 3 Real Data Applications

Finally, to demonstrate the utility of this approach, we will apply this approach to two different application domains. In the first, we consider the problem of ocean going vessel fuel efficiency estimation. A major problem for shipping companies is that while many accurate physics-based models to estimate fuel use exist, due to issues such as wear and tear and barnacle buildup, the predictions from such models decay in accuracy the longer the ship is out of port Fan et al. (2022). Thus, we should expect that such models are most useful close to port and progressively less useful the longer the ship has been out. By treating physics-model predicted efficiency as our synthetic data and ship-board sensor data as our real data, we can determine the optimal balance of the two to get the most accurate estimates of total fuel efficiency.

In the second context, we take a sociological problem of public opinion surveying. When designing such a survey, one must typically make a decision as to whether one would prefer the more expensive, but more accurate in-person surveys, or the cheaper but more prone to bias internet surveys Wu et al. (2022). Inspired by Egami et al. (2023), we will demonstrate the optimal budget allocation between online and in-person polling to track public opinion on large studies with online and in-person components such as the American Community survey Bureau (2020).