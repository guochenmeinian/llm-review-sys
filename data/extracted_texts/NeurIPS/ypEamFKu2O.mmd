# PGN: The RNN's New Successor is Effective

for Long-Range Time Series Forecasting

Yuxin Jia\({}^{1,2}\) Youfang Lin\({}^{1,2}\) Jing Yu\({}^{1,2}\) Shuo Wang\({}^{1,2}\) Tianhao Liu\({}^{3}\) Huaiyu Wan\({}^{1,2}\)

\({}^{1}\) School of Computer Science \(\&\) Technology, Beijing Jiaotong University, China

\({}^{2}\) Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China

\({}^{3}\) School of Cyberspace Science and Technology, Beijing Jiaotong University, China

Cyuxinjia, yflin, jingyu1, shuo.wang, leolth, hywan\(\}\)@bjtu.edu.cn

Corresponding author

###### Abstract

Due to the recurrent structure of RNN, the long information propagation path poses limitations in capturing long-term dependencies, gradient explosion/vanishing issues, and inefficient sequential execution. Based on this, we propose a novel paradigm called Parallel Gated Network (PGN) as the new successor to RNN. PGN directly captures information from previous time steps through the designed Historical Information Extraction (HIE) layer and leverages gated mechanisms to select and fuse it with the current time step information. This reduces the information propagation path to \((1)\), effectively addressing the limitations of RNN. To enhance PGN's performance in long-range time series forecasting tasks, we propose a novel temporal modeling framework called Temporal PGN (TPGN). TPGN incorporates two branches to comprehensively capture the semantic information of time series. One branch utilizes PGN to capture long-term periodic patterns while preserving their local characteristics. The other branch employs patches to capture short-term information and aggregate the global representation of the series. TPGN achieves a theoretical complexity of \(()\), ensuring efficiency in its operations. Experimental results on five benchmark datasets demonstrate the state-of-the-art (SOTA) performance and high efficiency of TPGN, further confirming the effectiveness of PGN as the new successor to RNN in long-range time series forecasting. The code is available in this repository: https://github.com/Water2sea/TPGN.

## 1 Introduction

Under the premise of accurate time series forecasting, long-range forecasting tasks offer an advantage over short-range forecasting tasks as they provide more comprehensive information for individuals and organizations to thoroughly assess future changes and make well-informed decisions. Due to its practical applicability across various fields (i.e., energy , climate , traffic , etc), long-range forecasting has attracted significant attention from researchers in recent years.

Long-range time series forecasting tasks can be broadly classified into two categories. One task is to utilize abundant inputs to forecast future outputs , while another task is to predict longer-range futures with fewer historical inputs . Although existing studies have shown that ample historical inputs can introduce more information to improve prediction performance , considering factors such as the load capacity of training devices and data collection, the utilization of limited historicalinputs to predict longer-range futures remains an important research topic. Therefore, this paper sets the task goal as predicting longer outputs with fewer inputs.

In recent years, deep-learning-based methods have achieved remarkable success in time series forecasting (for further discussions, please refer to Section 2 and Appendix B). These methods can be roughly categorized into four based paradigms: **Transformers**[Zhou et al., 2021, Wu et al., 2021, Liu et al., 2022a, Zhou et al., 2022a, Nie et al., 2023, Ni et al., 2023, Liu et al., 2024, Dai et al., 2024], **CNNs**[Wu et al., 2023, Wang et al., 2023, Luo and Wang, 2024], **MLPs and Linears**[Zeng et al., 2023, Xu et al., 2024, Wang et al., 2024], and **RNNs**[Jia et al., 2023]. It is worth noting that RNNs have received relatively less attention over an extended period of time. **This discrepancy is primarily attributed to the limitation of RNNs' recurrent structure, which leads to the persistence of excessive long pathways for information propagation**.

In fact, shorter information propagation paths lead to less information loss [Tishby and Zaslavsky, 2015], better captured dependencies [Liu et al., 2022a], and lower training difficulty [Wang et al., 2023]. However, RNNs heavily rely on a sequential recurrent structure to transmit information, making it challenging for them to capture long-term dependencies and suffer from the issue of gradient vanishing/exploding [Pascanu et al., 2013]. Meanwhile, due to its sequential computation, even though RNNs have a theoretical complexity that is linear with respect to sequence length \(L\), their actual running speed can be even slower than the \((L^{2})\) complexity of the Vanilla-Transformer [Vaswani et al., 2017]. Some RNN-based models [Hochreiter and Schmidhuber, 1997, Chung et al., 2014] have tried to enhance performance by incorporating specialized gated mechanisms. However, compared to the inherent limitations of the RNN structure, these improvements in information selection and fusion are merely a drop in the bucket.

Based on this motivation, this paper proposes a novel general paradigm called **P**arallel **G**ated **N**etwork **(PGN) as the new successor to RNN**. PGN introduces a Historical Information Extraction (HIE) layer to replace the recurrent structure of RNN, and then further selects and fuses information through gated mechanisms, effectively reducing the information propagation paths to \((1)\), as shown in Figure 1 (l). This enables PGN to better capture long-term dependencies in input signals. Additionally, since computations for each time step can be parallelized, PGN achieves significantly faster execution speed while maintaining the same theoretical complexity of \((L)\) as RNN.

Despite the advantages of PGN in terms of efficiency and capturing long-term information, it cannot be directly applied to time series forecasting tasks for optimal performance. This is because, based on 1D modeling, PGN struggles to capture periodic semantic information [Jia et al., 2023] effectively. Fortunately, the idea of transforming data from 1D to 2D and modeling it [Wu et al., 2023, Jia et al., 2023, Dai et al., 2024], proves effective in addressing above limitation. When employing 2D modeling for time series, information captured along rows reflects short-term changes, while information along columns represents long-term periodic patterns. Due to the distinct characteristics of these two types of information, it is reasonable to model them separately. Furthermore, considering that periodicity is present throughout the entire time series, both in the past and in the future, it is important to prioritize this consideration when modeling.

Figure 1: The information propagation illustration of different models.

Based on these motivations, we propose a novel PGN-based temporal modeling framework called **T**emporal **P**arallel **G**ated **N**etwork (**TPGN**). TPGN establishes two distinct branches to capture long-term and short-term information in the 2D input series. To focus on modeling long-term information, we utilize PGN to model each column of the 2D inputs, preserving their respective local periodic characteristics. Simultaneously, leveraging the advantages of patch (Nie et al., 2023) in capturing short-term changes, TPGN initially aggregates the short-term information into patches and subsequently merges them to obtain global information.

By integrating the information from both branches, TPGN achieves comprehensive semantic information capture for accurate predictions. It also should be noted that other methods can substitute PGN and be used in the long-term information extraction branch of TPGN, undoubtedly enabling TPGN to be a general framework to model temporal dependencies. Furthermore, TPGN maintains a efficient computational complexity of \(()\). To better illustrate the advantages of TPGN, inspired by (Jia et al., 2023), we have provided a information propagation comparative diagram in Figure 1 and an analysis table in Table 3.

The main contributions of this paper can be summarized as follows:

* We propose a novel general paradigm called PGN as the new successor to RNN, as shown in Figure 1 (l). It reduces the information propagation path to \((1)\), enabling better capture of long-term dependencies in input signals and addressing the limitations of RNNs.
* We propose TPGN, a novel temporal modeling framework based on PGN, which comprehensively captures semantic information through two branches, as shown in Figure 1 (m). One branch utilizes PGN to capture long-term periodic patterns and preserve their local characteristics, while the other branch employs patches to capture short-term information and aggregates them to obtain a global representation of the series. Notably, TPGN can also accommodate other models, making it be a general temporal modeling framework.
* In terms of efficiency, PGN maintains the same complexity of \((L)\) as RNN. However, due to its parallelizable calculations, PGN achieves higher actual efficiency. On the other hand, TPGN, serving as a general temporal modeling framework, exhibits a favorable complexity of \(()\). For a more detailed comparison of complexities, please refer to Table 3.
* We conducted experiments on five benchmark datasets, and the results indicated that TPGN achieved an average MSE improvement of 12.35\(\%\) in various long-range time series forecasting tasks compared to the previous best-performing models. Furthermore, in comparison to the average performance of specific models across all tasks, TPGN achieved an average MSE improvement ranging from 14.08\(\%\) to 39.65\(\%\). Additionally, experimental evaluations on computational complexity confirmed the efficiency of TPGN.

## 2 Related Works

### Modeling Interaction Cross Temporal Dimension

The methods that focus on temporal modeling can be broadly categorized into four paradigms: RNN-, CNN-, MLP- (Linear-), and Transformer-based. The limitations of RNNs (Hochreiter and Schmidhuber, 1997; Chung et al., 2014; Salinas et al., 2020) have been discussed in Section 1. Despite some methods (Chang et al., 2017; Yu and Liu, 2018; Jia et al., 2023) trying to alleviate these limitations, the recurrent structure still hinders their further development. CNNs (Franceschi et al., 2019; Sen et al., 2019) offer advantages in efficiency and shorter information propagation paths, but primarily constrained by limited receptive fields (Wu et al., 2023), resulting in an increase in the information propagation path as the length of the processed signal increases. Although some methods (Wang et al., 2023; Luo and Wang, 2024) have increased the receptive field to address these issues, the 1D modeling approach makes it challenging for them to directly capture periodicity. The advantages of Linear (Zeng et al., 2023) lie in its simple structure and high operational efficiency. Some advanced models have further enhanced the performance of MLP or Linear by applying them in the frequency domain (Xu et al., 2024) or introducing multi scales (Wang et al., 2024), which could lead to higher execution overhead. Classic Transformer-based methods either struggle to capture semantic information (Wu et al., 2023; Nie et al., 2023) due to point-wise attention mechanisms (Vaswani et al., 2017; Zhou et al., 2021, 2022a) or have high complexity (Wu et al., 2021; Liu et al., 2022a), limiting their ability. Subsequently, this problem was effectively addressed by utilizing patches (Nie et al., 2023). However, they still suffer from the 1D modeling issue mentioned earlier or the problem of limited receptive fields. More detailed discussion and analysis can be found in Appendix B.

### Modeling Interaction Cross Variable Dimension

For handling variable dimensions, there are generally four categories: variable fusion processing, variable independent processing, modeling based on Transformers, and modeling based on Graph Neural Networks (GNNs). Traditional fusion processing methods, due to the heterogeneity of multiple variables (Zhou et al., 2021), introduce excessive noise, resulting in worse performance compared to independent processing of variables (Nie et al., 2023). However, by applying attention mechanisms and Graph Neural Networks (GNN) on the variable dimension to replace independent modeling of variables, it is possible to successfully capture the correlations and differences between variables, thereby significantly improving the performance of multivariate modeling. Representative methods for modeling variable relationships based on Transformers include Crossformer (Zhang and Yan, 2022) and iTransformer (Liu et al., 2024), while GNN-based representative methods include CrossGNN (Huang et al., 2023) and FourierGNN (Yi et al., 2023). They provide excellent inspiration for analyzing and modeling multivariate time series.

## 3 Methodology

In this section, we first introduce our proposed novel paradigm called **P**arallel **G**ated **N**etwork (**PGN**), and explain how it reduces the information propagation paths, overcomes the limitations of RNNs, and emerges as the new successor to RNNs. Next, we present our newly designed temporal modeling framework called **T**emporal **PGN** (**TPGN**), which incorporates two separate branches to comprehensively capture semantic information. Finally, we provide a comprehensive complexity analysis to evaluate the computational efficiency of our methods.

### Parallel Gated Network (PGN)

Building upon the previous analysis, the limitation of RNNs lies in the excessively long information propagation paths of its recurrent structure, which directly leads to a series of issues, such as difficulty in capturing long-term dependencies (performance), low efficiency in sequential computations (efficiency), and gradient exploding/vanishing (training difficulty). Indeed, some RNNs leverage specialized gated mechanisms, such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014), which do have advantages in information selection and fusion. However, when faced with the disastrous limitation of RNNs, their advantages become insignificant.

Based on this, we propose a novel general paradigm called PGN as the new successor to RNNs. PGN draws the advantages of RNNs while reducing information propagation paths to \((1)\), thereby addressing the limitation of RNNs. The information propagation illustration and structure of PGN are shown in Figure 1 (l) and Figure 2 (a), respectively. On one hand, to enable PGN to capture information from all preceding time steps within short information propagation paths, we introduce a linear **H**istorical **I**nformation **E**xtraction (HIE) layer to aggregate information from the entire history at each time step. Importantly, at this stage, the computation of each time step of the signal is independent of others, allowing for effective parallel processing. On the other hand, PGN leverages gated mechanisms to inherit the advantages of information selection and fusion. It is important to emphasize that in PGN, we utilize only a single gate to simultaneously control the information selection and fusion in a parallel manner across all time steps of the sequence, resulting in reduced computational overhead. When given an input signal \(X^{L}\) of length \(L\), the computation process of PGN can be formalized as follows:

\[ H&=((X)),\\ G&=(W_{g}[X,\ H]+b_{g}),\\ &=(W_{t}[X,\ H]+b_{t}),\\ Out&=G H+(1-G),\] (1)

where \(()\) represents the operation of filling the front of the processed signal along the length dimension with a zero-filled vector of size \(^{(L-1)}\). \(()\) is a linear layer with weight matrices \(W_{ h}^{d_{m}(L-1)}\) and bias vectors \(b_{h}^{d_{m}}\). It aggregates all relevant historical information for each time step in parallel by sliding along the sequence length dimension, and \(H^{L d_{m}}\) represents the output of this operation. The weight matrices \(W_{g},W_{t}^{d_{m}(d_{m}+1)}\) and bias vectors \(b_{g},b_{t}^{d_{m}}\) are utilized in the computations. \(G\) and \(\) are the intermediate variables involved in the gated mechanism. The symbol \(\) represents the element-wise product, while \(()\) and \(()\) denote the sigmoid and \(\) activation functions. \(Out^{L d_{m}}\) represents the output of PGN.

### Temporal Parallel Gated Network (TPGN)

The specific objective of time series forecasting task is to predict the future series of length \(L_{ f}\) given a historical sequence of length \(L_{ h}\). As stated in Section 1, PGN may not be effective in directly extracting periodic semantic information, which limits its application in time series forecasting tasks. Inspired by , we transform the input series from 1D to 2D for modeling. To fully capture the short-term changes and long-term periodic patterns with different characteristics in the rows and columns of the 2D input, we introduce two branches to model them separately. The information propagation diagram and overall structure of TPGN are shown in Figure 1 (m) and Figure 2 (b).

Input Preparation ModuleTo enable TPGN to directly capture periodic semantic information, inspired by previous works , we reshape the series from 1D to 2D. Notably, we do not need to introduce multiple scales of periods like in TimesNet  and PDF , as it would result in increased computational overhead. Instead, we draw inspiration from WITRAN  and solely reset the sequence based on the natural scale of the time series. In addition, to minimize the negative impact of data fluctuations on model training, inspired by , we have introduced a normalization layer along the temporal dimension. When given an input sequence \(X_{ 1D}=\{x_{1},x_{2},,x_{L_{ h}}\}^{L_{ h} C}\) and temporal external feature \(TF_{ enc}^{L_{ h} C_{ time}}\) (\(C\) and \(C_{ time}\) represent the number of variables and temporal external features), this module can be mathematically expressed as:

\[_{X}= }_{i=1}^{L_{ h}}x_{i},\;_{X}^{2}= }_{i=1}^{L_{ h}}(x_{i}-_{X})^{2},\] (2) \[X_{ 1D}^{norm}=X_{ 1D},&norm=0\\ (X_{ 1D}-_{X})/_{X},&norm=1,\] \[X_{ 2D}=([X_{ 1D}^{norm},\;TF_{ enc}]).\]

Here, \(X_{ 1D}^{norm}^{L_{ h} C}\) represent the normalized series, \([]\) represents the concat operation, and the hyperparameters \(norm\) should be determined based on the characteristics of different datasets. To combine each variable of the input series with the temporal feature, we need to expand \(X_{ 1D}^{norm}\) by adding an extra dimension to match the shape of \(^{L_{ h} C 1}\). Additionally, \(TF_{ enc}\) needs to be expanded by adding a dimension and repeated \(C\) times to match the shape of \(^{L_{ h} C C_{ time}}\). Afterwards, we concatenate the results and reshape them according to the natural period \(P\) of series through \(()\), and \(X_{ 2D}^{R P C(1+C_{ time})}\) represents the output of this module. \(R\) and \(P\) represent the number of rows and columns in the 2D input, respectively.

TpgnAs TPGN focuses on modeling the temporal dimension, which is crucial for any variable in the time series. In the following discussions, we will focus on an example variable \(m\) to provide

Figure 2: The structures of PGN and TPGN.

a detailed explanation, where \(X_{}^{m}^{R P(1+C_{})}\) represents the input. To better capture long- and short- term information while preserving their respective characteristics, we have designed two branches as illustrated in Figure 1 (m) and Figure 2 (b).

**In the long-term information extraction branch**, we directly capture the information using PGN. On one hand, it effectively captures the long-term repetitive historical information for each time step. On the other hand, through the gated mechanism, it selects and fuses the current and historical information at each time step, thereby preserving the long-term periodic characteristics to the maximum extent. Specifically, this branch can be formulated as follows:

\[X_{}^{m}=(X_{}^{m}),\ \ \ H_{}^{m}=_{}(X_{}^{m}),\] (3)

where \(()\) represents the input being passed through the PGN paradigm. It is important to note that PGN operates along the \(R\) dimension. The advantage of this approach is it preserves the individual characteristics of each column, better serving forecasting. The output is denoted as \(X_{}^{m}^{R P d_{m}}\). To facilitate the utilization of long-term information for prediction purposes, we aggregate the information from all rows in each column using a linear layer \(_{}()\). The output of this branch is denoted as \(H_{}^{m}^{P d_{m}}\).

**In the short-term information extraction branch**, considering the advantage of patch in aggregation short-term information, we first utilize a linear layer to aggregate the short-term information into patches. Then, another linear layer is used to further fuse the patches into the global information of the series. The specific process can be formulated as follows:

\[H_{}^{m}=_{}^{}( _{}^{m}),\ \ \ H_{}^{m}=_{}^{}(H_{ }^{m}),\] (4)

where \(_{}^{}()\) operates along the \(P\) dimension, and \(H_{}^{m}^{R d_{m}}\) is its output. Subsequently, \(_{}^{}()\) further aggregates the patches \(H_{}^{m}\) and obtains the global representation \(H_{}^{m}^{1 d_{m}}\) of the sequence. Finally, to facilitate subsequent predictions, we repeat \(H_{}^{m}\) along the first dimension \(P\) times to obtain a new representation with the same dimension \(^{P d_{m}}\) as the output of the long-term information extraction branch.

Forecasting ModuleIn this module, begin by concatenating the information representations derived from the two branches of TPGN. The concatenated information encompasses the local long-term periodic characteristics observed across various columns in the 2D input series, along with the globally aggregated short-term information. Subsequently, we take the previously aggregated representation with comprehensive semantic information and pass it through a linear layer to predict future values at different positions within the period. The formulation of this module is as follows:

\[Out^{m}=(([H_{}^{m},\ H_{}^{m}])),\] (5)

where \([]\) represents the concat operation. The output dimension after \(()\) is \(^{P R_{}}\), where \(R_{}\) multiplied by \(P\) equal forecasting series length \(L_{}\). Finally, the above output will be permuted and reshaped to 1D dimension by \(()\) operation, the result \(Out^{m}^{L_{}}\).

### Complexity Analysis

PgnWhile PGN processes signals in the time dimension in parallel, each step still involves processing all its historical information. Therefore, the theoretical complexity of this part is still linear with respect to the length \(L\) of the signal being processed. The complexity of the gated mechanism is independent of the signal length, so the complexity of PGN can be expressed as \((L)\). Noted that PGN has the same theoretical complexity as RNN, but due to the parallelized ability of PGN computations, it has much higher efficiency in practice compared to RNN.

TpgnSince TPGN has two separate branches, it is necessary to analyze them separately.

For the long-term information extraction branch, TPGN applies the PGN paradigm along the number of \(R\), the complexity of this step is indeed linear with respect to \(R\), denoted as \((R)\). The aggregation of all rows of information is accomplished through a linear layer, which still has a complexity proportional to \((R)\). Therefore, the complexity of the long-term information extraction branch can be expressed as \((R)\).

For the short-term information extraction branch, TPGN applies two linear layers. The first linear layer compresses the time dimension from \(P\) to \(1\), while the second linear layer compresses the other time dimension \(R\) to \(1\), therefore, their complexities are respectively \((P)\) and \((R)\).

Since \(R\) multiplied by \(P\) equals the input sequence length \(L_{}\) (\(L\)), the complexities \((R)\) and \((P)\) are both equal to \(()\). For the two branches of TPGN, the complexities are both \(()\). Therefore, the complexity of TPGN is also \(()\).

## 4 Experiments

DatasetsTo validate the performance of TPGN, we followed WITRAN  and conducted experiments on five real-world benchmark datasets that span across energy, traffic, and weather domains. More details about the datasets can be found in Appendix C.

BaselinesWe conducted a comprehensive comparison of thirteen methods in our study. These methods include two RNN-based methods: WITRAN , SegRNN , three CNN-based methods: ModernTCN , TimesNet , MICN , three MLP-based methods: FITS , TimeMixer , DLinear , four Transformer-based methods: iTransformer , PDF , Bassigformer , PatchTST , and FiLM . It should be noted that certain earlier methods such as Vanilla-Transformer , Informer , Autoormer , Pyraformer , and FEDformer  have been extensively surpassed by the methods we selected. Hence, we did not include these earlier methods as baselines in our comparison. For further discussion on these methods and details of the experimental setup, please refer to Appendix B and Appendix D.

### Experimental Results

It is important to emphasize that while there have been numerous works focusing on modeling the relationships among multiple variables in time series, they still need to effectively capture information along the temporal dimension to better accommodate multivariate time series. In contrast, our method primarily concentrates on modeling the temporal dimension. To mitigate any potential negative impact caused by the heterogeneity of multivariate data, we followed the experimental setup of WITRAN , conducted experiments using a single variable. To ensure fairness, we conducted parameter search for each baseline model, enabling them to achieve their respective optimal performance across different tasks. For further experiment details, please refer to Appendix D.

Long-range Forecasting ResultsWe conducted four tasks on each dataset for long-range forecasting, and the results are shown in Table 1. For instance, considering the task setting 168-1440 on the left side of Table 1, it signifies that the input length is 168, and the forecasting length is 1440. It is worth noting that our proposed TPGN achieves state-of-the-art (SOTA) performance across all tasks, with an average improvement of MSE by 12.35\(\%\) and MAE by 7.25\(\%\) compared to the previous best methods. In particular, TPGN demonstrates an average reduction in MSE of 17.31\(\%\) for the ECL dataset, 9.38\(\%\) for the Traffic dataset, 3.79\(\%\) for the ETTh\({}_{1}\) dataset, 12.26\(\%\) for the ETTh\({}_{2}\) dataset,

   Methods & **WHEN** & **WHEN** & **WHEN** & **WHEN** & **WHEN** & **WHEN** & **WHEN** & **WHEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** \\   & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** \\   & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** \\   & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** \\   & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** & **HEN** \\   & **HEN** & **

[MISSING_PAGE_FAIL:8]

in TPGN is more significant. This can be observed by comparing the performance degradation when using only one branch versus using both branches together. Especially for strongly periodic data like traffic, in some tasks, using only the long-term information capture branch can achieve good results. This also aligns with our earlier mention in Section 1 about the significance of prioritizing the modeling of periodicity. (3) Compared to GRU and LSTM, which have more gates, PGN introduces only one gate but still achieves better performance. This strongly demonstrates the ability of PGN to serve as the new successor to RNN. (4) The comparison between "TPGN-GRU/-LSTM/-MLP/-Attn" and the baseline results demonstrates the strong generality and performance of the TPGN framework. Despite their inferior performance compared to TPGN, in some tasks, they even surpass the previous SOTA time series forecasting methods.

### Efficiency of Execution

Although this paper primarily focuses on predicting longer-range future outputs using short-range historical inputs, we conducted two sets of comparative experiments to comprehensively evaluate the efficiency of our proposed method. In the first set of experiments, we kept the input length fixed at 168 and varied the output length to 168/336/720/1440 to study the impact of forecasting length on the actual runtime efficiency of the model. In the second set of experiments, we fixed the output length at 1440 while varying the input length to 168/336/720/1440 to investigate the influence of historical input series length on the actual runtime of the model. The efficiency analysis considered both time and memory aspects. We selected representative methods from each paradigm based on the experimental results in Table 1 as the comparative methods. We fixed the batch size at 32, the model dimension size at 128, and conducted the tests using a single-layer model. The results are shown in Figure 4. Due to the much higher time and memory overhead of TimesNet compared to the other comparative models, we have omitted it from Figure 4 to provide a clearer illustration of the overhead details of the other models. Similarly, FiLM is not included in the time comparison chart.

From Figure 4, it can be observed that although TPGN does not have the lowest time and memory overhead, it achieves a decent level of efficiency in both time and space aspects. It is important to note that TPGN is a model with only one layer, while most of other models require the introduction of deeper layers, which inevitably leads to higher overhead. This undoubtedly further demonstrates that our method not only achieves SOTA performance but also delivers satisfactory efficiency.

## 5 Conclusions

In this paper, we propose a novel general paradigm called Parallel Gated Network (PGN). With its \((1)\) information propagation paths and parallel computing capability, PGN achieves faster runtime speed while maintaining the same theoretical complexity as RNN (\((L)\)). To enhance the application of PGN in long-range time series forecasting tasks, we introduce a novel temporal modeling framework called Temporal PGN (TPGN) with an excellent complexity of \(()\). By employing two branches to separate the modeling of long-term and short-term information, TPGN effectively capture periodicity and local-global semantic information while preserving their respective characteristics. The experimental results on five benchmark datasets demonstrate that our PGN-based framework, TPGN, achieves SOTA performance and high efficiency. These findings further confirm the effectiveness of PGN as the new successor to RNN in long-range time series forecasting tasks.

Figure 4: Time and memory overhead of different models.