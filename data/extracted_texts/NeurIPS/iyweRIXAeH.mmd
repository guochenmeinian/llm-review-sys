# Near-Optimal Algorithms for

Gaussians with Huber Contamination:

Mean Estimation and Linear Regression

 Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

&Daniel M. Kane

University of California, San Diego

dakane@cs.ucsd.edu

Ankit Pensia

IBM Research

ankit@ibm.com

&Thanksis Pittas

University of Wisconsin-Madison

pittas@wisc.edu

###### Abstract

We study the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination. Our main contribution is the design of the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both these problems. Specifically, for Gaussian robust mean estimation on \(^{d}\) with contamination parameter \((0,_{0})\) for a small absolute constant \(_{0}\), we give an algorithm with sample complexity \(n=(d/^{2})\) and almost linear runtime that approximates the target mean within \(_{2}\)-error \(O()\). This improves on prior work that achieved this error guarantee with polynomially suboptimal sample and time complexity. For robust linear regression, we give the first algorithm with sample complexity \(n=(d/^{2})\) and almost linear runtime that approximates the target regressor within \(_{2}\)-error \(O()\). This is the first polynomial sample and time algorithm achieving the optimal error guarantee, answering an open question in the literature. At the technical level, we develop a methodology that yields almost-linear time algorithms for multi-directional filtering that may be of broader interest.

## 1 Introduction

Modern machine learning systems operate with vast amounts of training data, which are difficult to carefully curate. Consequently, outliers have become a fixture of modern training datasets. This contradicts the standard i.i.d. assumption of classical statistical theory and has spurred the development of robust statistics, which seeks to develop algorithms that perform well in the presence of outliers . The standard contamination model of outliers was first formalized by Huber :

**Definition 1.1** (Huber Contamination Model).: _Given \(0<<1/2\) and a distribution family \(\), the algorithm specifies \(n\) and observes \(n\) i.i.d. samples from a distribution \(P=(1-)G+ B\), where \(G\) is an unknown distribution in \(\), and \(B\) is an arbitrary distribution. We say \(G\) is the distribution of inliers, \(B\) is the distribution of outliers, and \(P\) is an \(\)-corrupted version of \(G\)._

The Huber contamination model has since served as a bedrock for the development and evaluation of robust algorithms. Given that estimating properties of Gaussian distributions is a prototypical task in statistics, Gaussian estimation under Huber contamination is, analogously, a central problem in robust statistics. The _univariate_ version of this problem is addressed in Huber's work . In this paper, we focus on estimating _high-dimensional_ Gaussian distributions under Huber contamination.

Consider the fundamental problem of estimating the mean \(\) of a \(d\)-dimensional isotropic Gaussian \((,)\) given samples from an \(\)-corrupted distribution (Definition 1.1).  gave an \(n=(d/)\) sample and \((n)\)-time algorithm for this problem, achieving the information-theoretically optimal error of \(()\). Despite being polynomial in sample and time complexity, these guarantees are far from the linear sample complexity and linear runtime of the sample mean on uncontaminated data. While a number of works  have developed near-linear time Gaussian robust mean estimation algorithms, all these prior methods _inherently_ suffer a sub-optimal error guarantee of \(()\).1 Given the fundamental nature of Gaussian robust mean estimation , these contrasting sets of results raise the question of whether it is possible to achieve the best of both worlds. In other words:

_Can we obtain \(O()\) error for Gaussian mean estimation with Huber contamination in near-linear time and sample complexity?_

While mean estimation is the most basic unsupervised learning task, linear regression is arguably the most basic supervised learning task. Here we study the basic case of Gaussian covariates.

**Definition 1.2** (Gaussian Linear Regression).: _Fix \(>0\) and \(^{d}\). Let \(G\) be the joint distribution of pairs \((X,y)\), with \(X^{d}\), \(y\), such that \(X(0,_{d})\) and \(y=^{}X+\), where \((0,^{2})\) independently of \(X\). The goal of the algorithm is to compute \(\) such that \(\|-\|_{2}\) is small._

The information-theoretic error for robust Gaussian linear regression with Huber contamination is \(()\). However, all known polynomial time algorithms for this task incur a higher error of \(()\), raising the following open question in :

_Can we obtain \(O()\) error for Gaussian linear regression with Huber contamination in polynomial time and sample complexity?_

Perhaps surprisingly, despite the extensive algorithmic progress on robust statistics over the past years , these basic questions have remained open.

### Our Results

**Robust Mean Estimation** We begin by stating our result for Gaussian robust mean estimation:

**Theorem 1.3** (Almost Linear-Time Algorithm for Robust Mean Estimation).: _Let \(_{0}\) be a sufficiently small positive constant. There is an algorithm that, given parameters \((0,_{0})\), \(c(0,1),(0,1)\), and \(n}(d+(1/))(d/)\)\(\)-corrupted points from \((,_{d})\) (in the Huber contamination model per Definition 1.1), computes an estimate \(\) such that \(\|-\|_{2}=O(/c)\) with probability at least \(1-\). Moreover, the algorithm runs in time \((nd+1/^{2+c})(d/)\)._

Taking \(c\) to be a small positive constant, our algorithm achieves the optimal asymptotic error (see, e.g., ) up to constant factor with near-optimal sample complexity  and almost-linear runtime. Moreover, the runtime of our algorithm is near-optimal in the regime when \( d^{-2/c}\), which is the main regime of interest.2 Moreover, we note that the algorithm continues to work for a wider family of distributions than Gaussians; See Remark 2.2.

Our techniques are also amenable to the streaming framework of , and Theorem 1.3 can be extended to a streaming algorithm (with polynomial time and sample complexity) that uses only \((d+(1/))\)-memory, which is optimal up to the additive \((1/)\) term (see Appendix E).

**Robust Linear Regression** We next consider robust Gaussian linear regression (Definition 1.2). As existing polynomial-time algorithms for robust Gaussian linear regression can achieve error of \(O((1/))\), we assume without loss of generality that the true regressor satisfies \(\|\|_{2}=O((1/))\) (in fact, it can be ensured in nearly-linear time; see, e.g., [1, Theorem 2.5]). Using a novel reduction of linear regression to robust mean estimation for Gaussians, we prove:

**Theorem 1.4** (Almost Linear-Time Algorithm for Robust Linear Regression).: _Let \(_{0}\) be a sufficiently small positive constant. Let \(G\) be the joint distribution over \((X,y)\) following the Gaussian linear regression model (Definition 1.2) with unknown parameters \(>0\) and \(^{d}\) satisfying \(\|\|_{2}=O((1/))\) There is an algorithm that, given \((0,_{0})\), \(c(0,1)\) and \(n(d/^{2})(d/)\) i.i.d. labeled examples \((x,y)\) from an \(\)-corrupted version of \(G\), the algorithm returns a \(\) such that \(\|-\|_{2}=O(/c)\), with probability at least \(0.9\). Moreover, the algorithm runs in time \((nd+1/^{2+c})(d/)\)._

For a constant \(c>0\), Theorem 1.4 has optimal asymptotic error , near-optimal sample complexity  for constant failure probabilities, and near-optimal runtime (up to an additive factor of \(1/^{c}\)). Thus, we provide the first polynomial time and sample algorithm (in fact, _near-optimal_ time and sample complexity) for robust Gaussian linear regression with Huber contamination.

### Our Techniques

Our first major result is a near-optimal sample and almost-linear time algorithm for learning a Gaussian mean to error \(O()\) in the Huber contamination model. At a high level, our technique borrows ideas from the \(O()\) error algorithm of  and the fast robust estimation techniques of . We emphasize that a number of challenges need to be overcome in order to achieve this result, as elaborated below.

Roughly speaking, the fast algorithm of  works via a careful implementation of the standard filtering algorithm . If the empirical covariance matrix has no direction of large variance, this serves as a certificate that the remaining outliers do not substantially affect the mean. If the empirical covariance matrix has any large eigenvalue, the algorithm projects all the points onto a (randomly chosen) direction of large variance and removes outliers in this direction.3 Moreover, if there is such a direction, a careful analysis can be used to show that the filtering removes more outliers than inliers and that it improves a carefully chosen potential function -- based on the trace of an appropriate power of the (translated) empirical covariance matrix \(^{}\), roughly \(((^{}-})^{ d})\). Repeating this procedure only \((d/)\) many times eventually yields a sample set with no directions of large variance, whose sample mean is guaranteed to be close to the true mean. Crucially, to achieve this fast runtime,  must consider _random_ directions of large variance obtained by applying a suitable power of the (translated) empirical covariance matrix to a random vector. This is to ensure that the outliers cannot be arranged so that they will end up orthogonal to the directions considered.

Conceptually, the \(O()\)-error algorithm in Huber's model  works via a more complicated filter whereby if the covariance matrix has \(k\) (for \(k=((1/))\) moderately large eigenvalues \(1+()\), the algorithm projects all points onto the subspace spanned by these \(k\) directions and removes those points whose projections are too far from the mean. The usage of multiple orthogonal directions permits better concentration bounds of Gaussians (namely, the Hanson-Wright inequality) and achieves stronger filtering that translates to a smaller final error. In particular, the standard filter cannot reduce the leading eigenvalue to \(1+o((1/))\), while this improved multi-directional filter can reach the threshold \(1+O()\) for the \(k\)-th largest eigenvalue. A brute-force approach is then used to learn the projection of the mean onto the subspace of the \(k\) largest eigenvalues, while the sample mean is used as an approximation to the mean in the orthogonal directions.4. However,  takes quadratic time, \(nd^{2}\), to run because the subspace used to remove points is deterministic and outliers may be arranged in a way that filtering does not remove the outliers that lie in the orthogonal subspace. A natural idea is to use insights from  to improve the algorithm in .

Unfortunately, combining these techniques is highly non-trivial. While  filters based on a single random direction,  requires a multi-directional filter with \(((1/))\) many (nearly-) orthogonal directions. If the technique of  for producing random directions is called several times in order to produce the directions necessary, it may (if the sample covariance matrix is dominated by only a few large eigenvalues) produce essentially the same vectors over and over. We deal with this by considering two complementary cases. In the first case, where the randomdirections selected by the technique of  have small inner product, we show that we can simply take \(k=((d/))\) many random directions and perform the multi-directional filter of  in these directions. When this is not the case, the covariance matrix must have a small number of dominant eigenvectors. We take one of these eigenvectors and put it aside (for now) and instead consider the projections of our original points on the orthogonal subspace. Via a careful analysis, it can be shown that either of these steps will lead to a substantial drop in the potential function of . This guarantees that after a small number of filter steps, we are left with a matrix with small eigenvalues, allowing us to use the sample mean to approximate the true mean, at least in the directions orthogonal to those put aside.

In the directions that have been put aside, we still need to compute the sample mean. Fortunately, this reduces to the case where the dimension is \((d/)\). Although a brute-force approach will not run in polynomial time for such a large subspace, a careful analysis of  can be made to yield an appropriate runtime, because the "effective" dimension is now only \((d/)\).

We now sketch our algorithm for robust linear regression with Gaussian covariates. Our algorithm for robust linear regression in the Huber model achieves near-optimal error of \(O()\) by leveraging our above-described robust mean estimation algorithm. Prior to our work, no polynomial time algorithm was known to achieve this optimal error guarantee. As an additional bonus, our algorithm has near-optimal sample complexity and runs in almost-linear time. The basic idea here is to consider the distribution of \(X\) conditioned on the value of \(y\) (or more precisely, after rejection sampling based on \(y\) in a carefully chosen way). We show that this conditional distribution will be a nearly-spherical Gaussian whose mean is proportional to \(\), the true regressor. By using our robust mean estimation algorithm on this conditional distribution, we can obtain our final estimate of \(\).

### Related Work

Our work lies in the field of algorithmic robust statistics, an active area of research since ; see  for a recent book on this topic and Appendix A for additional discussion.

Contamination ModelsThe _strong contamination model_ works as follows: a computationally unbounded adversary inspects all the samples and can replace \(\)-fraction of samples with points of their choice . In contrast, the Huber model is additive and preserves independence among data points. In the rest of this paragraph, we focus on the task of robust (isotropic) Gaussian mean estimation. Despite these differences between the contamination models, the information-theoretic error in both of these models is \(()\). However, the computational landscape of the problem changes considerably. First,  was able to achieve this error with polynomial samples and time. However, under the strong contamination model, there is evidence in terms of statistical query lower bounds (and low-degree hardness using ) that all polynomial time algorithms must incur a larger error of \(()\), matching the existing polynomial time algorithms , which necessitates that we consider Huber's model.5

Gaussianity and Identity Covariance assumptionsFirst, even for Gaussians, knowing the covariance matrix is crucial for computationally-efficient algorithms.6 that gets \(O()\) error runs in polynomial time for isotropic covariances, while the algorithm for unknown covariance matrix runs in quasi-polynomial time .7 Thus, we focus our attention to (nearly) isotropic distributions in this work and improve the runtime of the algorithm from  from a large polynomial to almost-linear. Looking beyond Gaussianity assumption, it is impossible (information-theoretically) to obtain \(O()\) error for arbitrary isotropic subgaussian distributions, even in a single dimension and Huber contamination model . Finally, our results can be extended to a subset of symmetric isotropic subgaussian distributions; see Remark 2.2.

Outlier-robust Estimators in Nearly-Linear TimeSeveral recent works have developed nearly-linear time algorithms for problems in robust statistics: mean estimation , linear regression , and PCA .

## 2 Preliminaries

NotationWe denote \([n]{:=}\{1,,n\}\). For \(w:^{d}{}\) and a distribution \(P\), we use \(P_{w}\) to denote the weighted by \(w\) version of \(P\), i.e., the distribution with pdf \(P_{w}(x)=w(x)P(x)/_{X P}[w(X)]\). We use \(_{P},_{P}\) for the mean and covariance of \(P\). When the vector \(\) is clear from the context, we use \(}_{P}\) to denote the second moment matrix of \(P\) centered with respect to \(\), i.e., \(}_{P}:=_{X P}[(X-)(X-)^{}]\). We use \(\|\|_{2}\) for \(_{2}\) norm of vectors and \(()\) and \(\|\|_{}\) for trace and operator norm of matrices, respectively. For a subspace \(\), we use \(_{}\) to denote the orthogonal projection matrix that projects to \(\), use \(_{}(x)\) to denote the _orthogonal projection_ of \(x\) onto \(\), and use \(^{}\) for the subspace orthogonal to \(\). We use \(x y\) to denote that \(x Cy\) for some absolute constant \(C\). We use the notation \(a b\) to mean that \(a>Cb\) where \(C\) is some sufficiently large constant. We use \(()\) to denote a quantity that is poly-logarithmic in its arguments and use \(,()\), and \(\) to hide such factors.

### Goodness Condition

Our algorithm builds on the following (slightly modified) goodness condition from 

**Definition 2.1** (\((,,k)\)-Goodness).: _We say that a distribution \(G\) on \(^{d}\) is \((,,k)\)-good with respect to \(^{d}\), if the following conditions are satisfied:_

1. _[leftmargin=*]_
2. _(Median) For all_ \(v^{d-1}\)_,_ \(_{X G}[|v^{}(X-)|]<1/2\)_._
3. _For every weight function_ \(w\) _with_ \(_{X G}[w(X)] 1-\)_, the following hold:_ 1. _(Mean)_ \(\|_{G_{w}}-\|_{2}\)_,_ 2. _(Covariance)_ \(\|}_{G_{w}}-_{d}\|_{} (1/)\)_,_ 3. _(Concentration along nearly-orthogonal vectors) For any_ \(^{k d}\) _with_ \((^{})=k\) _and_ \(\|^{}\|_{} 2k/(k/)\)_, the degree-2 polynomial_ \(p(x):=\|(x-)\|_{2}^{2}\) _satisfies_ \(_{X G_{w}}[p(X)(p(x)>100k)]/(1/)\)_._

The first condition, 1, states that the median is a good estimate in each direction. The next two conditions, 2 and 2, state that the mean and covariance of the inliers change by at most \(\) and \((1/)\), respectively, when \(\)-fraction of the dataset is deleted; these two conditions with \(=\) have been extensively used in the strong contamination model for (sub)-Gaussian distributions but the resulting algorithms necessarily get stuck at error \(\) error. Condition 2, is crucial in obtaining \(O()\) error because it provides stronger concentration for projections along \(k\)-dimensional (nearly-orthogonal) projections using Hanson-Wright inequality. In particular, it implies that at most \(O()\)-fraction of inliers have \(p(x)>10k\). In contrast, using only Conditions 2 and 2 would require the threshold to be much larger, \(p(x) k(k/)\), which is insufficient for our application. We show in Appendix B that if \(G\) is a set of \(n(d+(1/))\,(d/)/^{2}\) i.i.d. samples from \((,)\), then \(G\) satisfies the goodness condition mentioned above with probability \(1-\); Prior work  had only shown \((d/)\) bound for the sample complexity.

**Remark 2.2**.: Our proof for the sample complexity directly extends to all distributions satisfying the following: (i) their linear projections are subgaussian and centrally symmetric with constant density around the median, and (ii) their quadratic projections satisfy Hanson-Wright inequality. In particular, this extension is crucial for deriving our results for robust linear regression.

Throughout the algorithm, we will assume that the inliers and the outliers satisfy the following:

**Setting 2.3**.: Let \((0,_{0})\) for a sufficiently small constant \(_{0}\), \(C>0\) be a large enough constant, \(P\) be a uniform distribution on \(n\) points of \(^{d}\) that has the mixture form \((1-)G+ B\) for some \(G\) that is \((,,k)\)-good (cf. Definition 2.1) with respect to \(^{d}\), where \(=/(1/)\) and \(k=(^{2}(d/))\). Let \(w:^{d}\) be a weight function with \(_{X G}[w(X)] 1-\) such that \(-C_{P_{w}}- C ^{2}(1/)\).

The condition on \(_{P_{w}}\) in Setting 2.3 amounts to a "warm start" for our algorithm, which can be obtained in nearly-linear time by running the algorithm from  first (see Appendix B).

The following result gives a sufficient condition for the (weighted) sample mean using (2.a) and (2.b):

**Lemma 2.4** (Certificate Lemma).: _In Setting 2.3, if \(\|_{P_{}}\|_{}{}1+\), then \(\|_{P_{w}}-\|_{2}+\)._

In light of the certificate lemma, our goal will be to downweigh outliers until the top eigenvalue (in the subspace of interest) is at most \(1+O()\). That would make the bound above \(O()\).

We now explain the filtering procedure that we use. A filtering algorithm takes weights \(w(x)\) for each point and scores \((x)\) (that capture how much of an outlier the point is) the procedure updates the weights to \(w^{}(x)\) such that: (i) it removes much more mass from outliers than inliers, and (ii) \(\,_{X B}[w^{}(x)(x)]\) (the contribution to the weighted average of scores by the outliers) is small after filtering. By considering appropriate scores \((x)\) that use multi-directional projections of the form of Condition (2.c) of Definition 2.1, we show the following in Appendix B.

**Lemma 2.5** (Multi-Directional Filtering).: _Consider Setting 2.3. Given a nearly-orthogonal matrix \(^{k d}\) satisfying \(\|^{}\|_{} 2\,(^{ })/(1/)\), there is an algorithm that reads \(\), the \(n\) points, their weights \(w(x)\), and returns weights \(w^{}\) in time \(ndk+(d/,\|\|_{}^{2})\) such that_

1. \(_{X G}[w(X)-w^{}(X)]<(/(1/))\, _{X B}[w(X)-w^{}(X)]\)_._
2. \(\,_{X B}[w^{}(X)\|(X-_{P_{w}})\|_{2}^ {2}]\,(^{})\)_._

### Polynomial Time Algorithm

We now record a (refined) guarantee of the algorithm from , which would be useful later on. This algorithm uses the multi-directional filter from Lemma B.27 with \(\) set to be the top-\(k\) eigenvectors of the covariance matrix until the top-\(k\) eigenvalues are \(1+O()\) for \(k=((1/))\). Lemma 2.4 then guarantees that the empirical mean in the subspace of the \((d-k)\) last eigenvalues is \(O()\)-close to \(\). Finally, the algorithm employs a brute force procedure that uses median and (Condition (1)) to estimate the mean in the remaining \(k\)-dimensional subspace.

**Lemma 2.6** (Adapted From ).: _Let \(T\) be a set of \(n\) i.i.d. samples from an \(\)-corrupted version of \((0,_{d})\) and let \(r,(0,1)\) be parameters. If \(n{}(d+(1/))/^{2})(d/)\), then there is an algorithm that, when having as input \(T\), \(\), and \(r\), after time \((nd^{2}+})\), it outputs a vector \(^{d}\) such that \(\|-\|_{2}/r\) with probability \(1-\)._

Since the refined runtime above does not appear in , we provide a proof in Appendix B. In particular, it is important that the runtime is \(nd^{2}+}\). Even though this is super-linear in the size of the input, \(nd\), our main algorithm in the next section will apply Lemma 2.6 only after projecting the entire dataset to a subspace of a much smaller dimension (\((d/)\) in place of \(d\) before).

## 3 Almost-Linear Time Algorithm for Robust Mean Estimation

In this section, we describe the main ideas of the almost-linear time algorithm for Gaussian robust mean estimation that achieves Theorem 1.3.

As described earlier, our algorithm runs in two stages, both of which will run in almost-linear time. In the first stage, our goal is to find a low-dimensional subspace \(\) (of dimension at most \((d/)\)) and a vector that is \(O()\) close to \(\) when projected in subspace \(^{}\). In the second stage, we deploy the basic \(O()\) algorithm from  on the input data after projecting it onto \(\) and estimate \(\) in that subspace. By a refined analysis of their argument (Lemma 2.6), the second stage runs fast because \((V)=O((d/))\). By combining these two estimates we get an approximation of \(\) in the whole \(^{d}\). Since the second stage algorithm's analysis closely follows , in the reminder we focus on the first stage, below: (the probability of success can be boosted by standard arguments)

**Theorem 3.1** (First Stage in Almost-Linear Time).: _Given a set of \(n\) samples, the uniform distribution on which has the form \((1-)G+ B\) for some \(G\) satisfying Conditions (2.a) to (2.c) of Definition 2.1 with respect to \(^{d}\) and parameters \(=/(1/)\) and \(k=(^{2}(n+d))\). Algorithm 1 takes as input the \(n\) points and \(\), and with probability \(0.99\), returns a vector \(\) and a subspace \(\) such that \(\|_{^{}}(-)\|_{2}\) and \((){=}(d/)\). The algorithm runs in time \(O(nd(d/))\)._

Theorem 3.1 is realized by Algorithm 1. Without loss of generality, we assume Setting 2.3 holds at the beginning of the algorithm. Recall the high-level strategy that was outlined in Section 1.2: We want to iteratively downweigh points and add directions to \(\) so that (i) the weight removed from inliers is at most \(O(/(1/))\), (ii) the downweighted dataset along every direction in \(^{}\) has variance at most \(1+O()\), (iii) and \(()(d/)\). Having the first two, the certificate lemma (Lemma 2.4) implies that the empirical (weighted) mean is \(O()\)-close to \(\) along \(^{}\). The way to achieve (i) and (ii) in  was by using the matrix \(\) to be the top-\(k\) eigenvectors of the covariance matrix (also, the algorithm of  does not add directions to \(\) until the very end; see Section 2.2)). As mentioned earlier, this approach runs in quadratic time. Our main technical insight is to (a) randomize the choice of \(\) when safe to do so, and (b) when it is not safe, allow the algorithm to remove a direction by adding it to \(\) at any time (as opposed to waiting until the end). We first describe the notation below for a more detailed overview.

**Notation for Algorithm 1.** For each round \(t[t_{}]\) of our algorithm, we maintain weights \(w_{t}:^{d}\) over the dataset, capturing the confidence in the points being inliers, i.e., \(w(x)=0\) represents outliers and \(w(x)=1\) represents inliers. We also maintain a (low-dimensional) subspace \(_{t}\) with the goal of making the covariance of the projections of the data on \(_{t}^{}\) be small at the end. Let \(_{t}^{},_{t}^{}\) be the sample mean and covariance of the data after projected on \(_{t}^{}\) and weighted by \(w_{t}\), and define \(_{t}^{}_{t}^{}-_{_{t }^{}}\) (see Lines 5 to 7 and 4 for precise definitions). We use the potential function \(_{t}:=((_{t}^{})^{}(_{t}^{ }))=\|_{t}^{}\|_{}^{2}\) to track the progress of our algorithm, where \(_{t}^{}=(_{t}^{})^{p}\) for \(p= d\). Observe that the potential function \(_{t}\) ignores the contribution from the directions in \(_{t}\).

```
0: Parameter \((0,1/2)\), uniform distribution over \(n\) points that can be written as \(P=(1-)G+ B\) where \(G\) satisfies Definition 2.1 with appropriate parameters.
0: An approximation of the mean in a subspace \(^{}\) and the orthogonal subspace \(\).
1: Let \(C\) be a sufficiently large constant, \(k=C^{2}(n+d)\), \(t_{}=((d/))^{C}\).
2: Initialize \(V_{1}\) and \(w_{1}(x)=1\) for all \(x^{d}\).
3:for\(t=1,,t_{}\)do
4: Let \(V_{t}\) be the subspace spanned by the vectors in \(V_{t}\), and \(_{t}^{}\) be the perpendicular subspace.
5: Let \(P_{t}\) be the distribution \(P\) re-weighted by \(w_{t}\), i.e., \(P_{t}(x)=w_{t}(x)P(x)/}_{X P}[w(X)]\).
6: Let \(_{t}^{},_{t}^{}\) be the mean and covariance of \(_{_{t}}(X)\) when \(X P_{t}\)
7: Define \(_{t}^{}=(_{X P}[w(X)]^{2}_{t}^{} -(1-C_{t})_{_{t}^{}}\), where \(_{_{t}^{}}\) is the orthogonal projection matrix for \(_{t}^{}\), and \(_{t}^{}:=(_{t}^{})^{p}\) for \(p=(d)\).
8: Calculate \(_{t}\) such that \(_{t}/\|_{t}^{}\|_{}[0.1/10]\) using power iteration. \(\) cf. Appendix B
9:If \(_{t} C\)thenreturn\(_{t}\) and \(V_{t}\).
10: Let \(q_{t}:=_{z,z^{}(0,)}[|(_{t}^ {}z,_{t}^{}z^{})|>\|_{t}^{}z\|_{2} \|_{t}^{}z^{}\|_{2}/k^{2}]\).
11: Calculate an estimate \(_{t}\) such that \(|_{t}-q_{t}|t_{})}\).
12:if\(_{t} 1/(k^{2}t_{})\)then\(\) Case 1 (cf. Section 3.1)
13:for\(j[k]\)do
14:\(v_{t,j}_{t}^{}z_{t,j}\) for \(z_{t,j}(0,)\).
15:\(_{t}[v_{t,1},,v_{t,k}]^{}\) i.e., the matrix with rows \(v_{t,j}\) for \(j[k]\).
16:\(w_{t+1}\) Multi-DirectionalFilter\((P,w,,_{t})\)\(\) cf. Lemma B.27
17:else\(\) Case 2 (cf. Section 3.2)
18:\(u_{t}(_{t}^{})^{p^{}}z/\|(_{t}^{ })^{p^{}}z\|_{2}\) for \(p^{}:=C^{2}(dt_{})\),\(z(0,)\). \(\) Power iteration
19:\(V_{t+1} V_{t}\{u_{t}\}\).
20: Let \(_{_{t}}=_{X P_{t}}[_{_{t}}(X)]\) be the mean of \(P_{t}\) after projection to \(_{t}\).
21:return\(_{_{t}}\) and \(V_{t}\). ```

**Algorithm 1** Robust Mean Estimation Under Huber Contamination (Stage 1)

We will show that in every iteration, the potential function decreases multiplicatively, i.e., \(_{t+1}(1-(/d))_{t}\) while removing at most \(O(/(1/))\) fraction of inliers throughout the algorithm (so that we do not fall outside of Setting 2.3). Since \(_{t}\) at \(t=0\) is at most \(((d/)^{p})\) and the algorithm necessarily terminates when it reaches below \(^{p}\) (because this implies that \(\|_{t}^{}\|_{}(_{t})^{1/p}=O()\) which would cause Line 9 to activate), then after \(t_{}=(d/)\) iterations the algorithm yields a \(_{P_{w}}\) such that \(\|_{^{}}(_{P_{w}}-)\|_{2}=O()\) by Lemma 2.4. Since each iteration is implementable in \((nd)\) time, the whole algorithm terminates in \((nd)\) time.

In the next two subsections, we explain how the algorithm decides whether to expand the subspace \(_{t}\) or to remove outliers. This decision is based on Line 12, which checks if two random \(_{t}^{}z,_{t}^{}z^{}\) for \(z,z^{}(0,)\) are nearly-orthogonal with reasonable probability.

### Case 1: Many Large Eigenvalues

By construction, "Case 1" corresponds to the case where the rows of \(_{t}\) are nearly-orthogonal (with high probability); see Line 12. Thus, \(_{t}\) will be nearly-orthogonal, permitting the use of the multi-directional filtering algorithm from Lemma B.27.

The explanation above ensures that the multi-directional filtering procedure with random \(v_{i}\)'s is correct. The reason that it is significantly faster than  is that the \(v_{i}\)'s are now randomized along the top eigenvalues of \(_{t}^{}\) because they are of the form \(_{t}z/\|_{t}z\|\); In contrast,  sets \(v_{i}\)'s deterministically equal to the top-\(k\) eigenvalues of \(_{t}^{}\). As outlined in the introduction, the random choice of \(v_{i}\)'s prevents the "adversary" to place the outliers in such a way that the outliers in the orthogonal subspace are unaffected during filtering. In particular, filtering with the random \(v_{i}\)'s reduces the contribution of outliers, not only along the exact top-\(k\) eigenvectors of \(_{t}^{}\), but also along _all directions with variance comparable to the top eigenvector_. We use the technical insights from  to show that the potential function decreases multiplicatively:

**Claim 3.2**.: _With high constant probability, for every round \(t\) that Line 12 succeeds, \(_{t+1} 0.99_{t}\). Moreover, \(_{X G}[w_{t}(X)] 1-/(1/)\) throughout the algorithm's execution._

Proof Sketch.: We first sketch how \(_{t}\) is valid for the multi-directional filter, Lemma B.27. Since the check of Line 12 succeeds, with high probability, we have that for each \(t[t_{}]\) rounds, the angles of every pair of rows of \(_{t}\) formed in line 15 have cosine at most \(1/k^{2}\). Also, Hanson-Wright inequality implies that with high probability, \(\|v_{t,j}\|_{2}^{2}(_{t}^{}_ {t})(k)\). Combining both of these with Gershgorin Discs Theorem and the choice of \(k\), we have that \(\|_{t}^{}_{t}\|_{} 2( _{t}^{}_{t})/(1/)\), satisfying the requirements in Lemma B.27 and ensuring \(_{X G}[w_{t+1}(X)] 1-/(1/)\).

To show that \(_{t+1}\) reduces, we first use the following linear-algebraic result to relate \(_{t+1}\) with \((_{t}^{}_{t+1}^{}_{t}^{ })\) from : \(_{t+1}=((_{t+1}^{})^{2}) d^{1/2p}( (_{t}^{}_{t+1}^{}_{t}^{ }))^{}\). By the definition of \(_{t}^{}\), we have \((_{t}^{+}_{t+1}^{}_{t}^{}) _{P_{t+1}}[\|_{t}^{}(x-)\|_{2}^{2}-((_{t}^{})^{2})]\). To upper bound this, we will use the guarantees of the multi-dimensional filter along with the goodness conditions and the fact that \(\|_{t}^{}(x-)\|_{2}^{2}\|_{t}(x-)\|_{2}^ {2}\) (since \(_{t}\) is a Johnson-Lindenstrauss sketch of \(_{t}^{}\) with \(k(n)\); see Appendix B) as follows: we show in the appendix that the contribution from inliers is small by Definition 2.1 and the contribution from outliers, \(\,_{B_{t+1}}[\|_{t}(x-)\|_{2}^{2}\), is controlled by Lemma B.27. Combining the two aforementioned arguments with some algebraic manipulations we can formally show that:

**Lemma 3.3** (Filtering Implication).: _After the filtering, \((_{t}^{}_{t+1}^{}_{t}^{ }) 0.1\|_{t}^{}\|_{}((_{t}^{})^{2})\). Combining Lemma C.4 with \(_{t+1} d^{1/2p}((_{t}^{}_{t+ 1}^{}_{t}^{}))^{}\) from earlier, and noting that \(d^{1/2p}=d^{1/2 d} 3\) and \(\|_{t}^{}\|_{}^{2}((_{t}^{ })^{2})=_{t}\), we have \(_{t+1} 0.99_{t}\), as desired. _

### Case 2: A Few Large Eigenvalues

Consider now the alternate case where \(q_{t}\) from Line 10 is large. Then the approach from the previous case is inapplicable because \(_{t}^{}z\)'s will be highly correlated vectors. The following result formally proves that this happens only when the top eigenvalue of \(_{t}^{}\) contributes significantly to its spectrum.

**Claim 3.4**.: _If \(_{z,z^{}(0,)}[_{t}^{}z,_{t}^{}z^{}\|}{\|_{t}^{}z\|_{2}\|_{t}^{}z^{}\|_{2}}>]\), then \(_{t}^{}\|_{}^{2}}{((_{t}^ {})^{2})}(,)\)._

Proof Sketch.: Let \(q_{t}\) be the probability from the statement. Pretend for simplicity that \(\|_{t}^{}z\|_{2}^{2}\) is equal to its expectation \(((_{t}^{})^{2})\). Applying Markov's inequality, we see that \(<q_{t}^{-2}\,[_{t}^{}z, _{t}^{}z^{}^{2}]/((_{t}^{ })^{2})=^{-2}\|(_{t}^{})^{2}\|_{}^{2}/ ((_{t}^{})^{2})\). Further using the standard fact that \(\|\|_{}^{2}()\|\|_{ }\) for a psd matrix \(\) applied to \(_{t}^{}\) completes the proof. 

Since \(,\) are \((d/)\) in our setting (Line 12), Claim 3.4 reveals that the top eigenvalue of \((_{t}^{})^{2}\) is at least \(_{t}/(d/)\). Thus, our algorithm can take a top eigenvector \(u\) of \(_{t}^{}\) and add it to the subspace of directions to ignore, \(_{t}\), i.e., project all data on the subspace perpendicular to \(u\) in the future iterations. Formally, we show that the potential decreases if \(u\) has large projection along \(_{t}^{}\):

**Claim 3.5**.: _Let \(V_{t+1}=V_{t}\{u\}\) for some unit vector \(u V_{t}^{}\), then \(_{t+1}_{t}-u^{}(_{t}^{})^{2}u\)._

Proof Sketch.: Going from step \(t\) to step \((t+1)\), the effect of adding \(u\) in the subspace \(V_{t+1}\) is that \(_{t+1}^{}=_{t}^{}\) for the projection matrix \(=-uu^{}\). Then by properties of the trace operator and Lieb-Thirring inequality: \(_{t+1}=((_{t+1}^{})^{2p})=((_{t}^{})^{2p})=((_ {t}^{})^{2p})((_{t}^{})^{2p} ^{2p})=((_{t}^{})^{2p})-u^{}( _{t}^{})^{2p}u=_{t}-u^{}(_{t}^{})^{2}u\). 

Claims 3.4 and 3.5 imply that if we use \(u\) equal to the top eigenvector of \(_{t}^{}\), then our proof will be completed. However, the algorithm cannot compute exact eigenvectors in almost-linear time. Fortunately, power iteration in Line 20 computes a fine enough approximation such that \(u^{}(_{t}^{})^{2}u\) is a constant fraction of \(\|_{t}^{}\|_{}^{2}\). Thus, we obtain \(_{t+1}(1-1/(d/))_{t}\) as desired.

## 4 Robust Linear Regression: Optimal Error In Almost-Linear Time

In this section, we focus our attention to linear regression in Huber contamination model and sketch the proof of Theorem 1.4. Recall that each inlier sample \((X,y)\) is distributed according to the Gaussian linear regression model of Definition 1.2.

```
0:\(>0\), multiset \(S\) of \(^{d+1}\) containing pairs of the form \((x,y)\) with \(x^{d}\), \(y\).
0:A vector in \(^{d}\).
1:Set \(_{y}^{2}(\{y:(x,y) S\})\). \(\)\(_{y}^{2}=_{y}^{2}(1 O((1/)))\)
2:Draw \(a\) uniformly at random from \([-_{y},_{y}]\).
3:Define the interval \(I:=[a-,a+]\) for \(:=_{y}/(1/)\). \(\) Random choice of I
4:\(S^{}\{x\ :\ (x,y) S,y I\}\). \(\) Simulating \(G_{I}\)
5:\(_{I}(S^{},10)\)\(\) Algorithm from Theorem 1.3
6:return\(=_{y}^{2}/a)}_{I}\). \(\) Rescaling ```

**Algorithm 2** Robust Linear Regression Under Huber Contamination

Our main technical insight is a novel reduction to robust mean estimation. Existing reductions in the literature (see, e.g., ) rely on the fact that \([yX]=[XX^{}+Xz]=\). However, it is unclear how to estimate the mean of \(yX\) up to error \(O()\) under Huber contamination because \(yX\) is very far from Gaussian; e.g., \(yX\) is subexponential as opposed to (sub)-Gaussian, and it is not even symmetric around \(\), implying median might not even work along a direction. Thus, existing approaches using this methodology have error \(((1/))\). In contrast, our reduction reduces to robust (almost)-Gaussian mean estimation by using the conditional distribution of \(X\) given \(y=a\) under Definition 1.2.

**Claim 4.1** (Conditional Distribution).: _Let \((X,y)^{d+1}\) follow Definition 1.2. Given \(a\), denote by \(G_{a}\) the distribution of \(X\) given \(y=a\). Similarly, given an interval \(I\), let \(G_{I}\) represent the conditional distribution of \(X\) given \(y I\). Define \(_{y}^{2}:=^{2}+\|\|_{2}^{2}\). Then, \(G_{a}=^{2}},_{d}- {_{y}^{2}}^{}\) and \(G_{I}=[y I]}_{I}(a^{};0,_{y}^{2}) }{_{y}^{2}},I_{d}-^{2}}^{}a^{}\), where \((z;0,^{2})\) denotes the pdf of the \((0,^{2})\) at \(z\)._

Since the conditional distribution \(G_{a}\) above is just a Gaussian whose mean is scaled version of \(\) (and roughly isotropic covariance), one would ideally like to get (\(O()\)-corrupted) samples from \(G_{a}\), then use the robust mean estimator from Theorem 1.3, and finally scale the result back appropriately.

Obviously, it is impossible to simulate \(G_{a}\) using \(G\) since there is zero probability over the inliers that \(y\) is exactly equal to \(a\) in our dataset. Instead, we can simulate the conditional distribution given \(y I\) for some interval \(I\) centered around \(a\). The length \(\) of \(I\) needs to be carefully selected. On the one hand, \(I\) should be sufficiently narrow to ensure that the mean of \(G_{I}\) closely approximates the mean of \(G_{a}\). On the other hand, it should not be excessively narrow to avoid rejecting a significant portion of our samples. As we show later, an interval of length \((/(1/))\) meets both of these criteria as long as \(\|\|_{2}(1/)\). Also, a set of i.i.d. samples from \(G_{I}\) for such a small interval satisfy the goodness condition with the correct parameters because \(G_{I}\) will be roughly isotropic.

Finally, it is critical to ensure that the fraction of outliers in the simulated samples from \(G_{a}\) remains \(O()\). In fact, this may not be true depending on the choice of the interval \(I\). For example, if the adversary knows \(I\), then outliers could all happen to have their labels inside \(I\), which would cause the contamination rate to blow up. To overcome this, we randomize the selection of \(I\): Since the distributions of outliers is independent of \(I\), choosing the center of \(I\) to be random means that the bad event of the previous sentence is now a small probability event.

### Proof Sketch of Theorem1.4

The algorithm is given in Algorithm2. We now give the proof sketch of Theorem1.4: We claim that the following three events hold simultaneously with probability at least \(0.9\) (\(_{0},_{1}\) and \(_{y}^{2}\) are defined in Algorithm2):

\[\;|_{y}^{2}-_{y}^{2}|_{y}^{2} (1/)\;|a|>0.0001_{y} \;\|_{I}-_{G_{I}}\|_{2}.\] (1)

Given the above, we first show that \(\) is \(O()\)-close to \(\). For simplicity, let us assume momentarily that \(_{y}^{2}=_{y}^{2}\). Then by triangle inequality \(\|-\|_{2}^{2}}{|a|}(\|_{I}-_{G_{I}}\|_{2}+\|_{G_{I}}-^{2}}\|_{ 2})_{y}+_{y}\|_{G_{I}}- ^{2}}\|_{2}\). The second term arises from the fact that we use \(G_{I}\) instead of \(G_{a}\) in the algorithm (since simulating samples from \(G_{a}\) is algorithmically impossible). Using the expression for \(G_{I}\) from Claim4.1, we can upper bound this term by \(_{y}\|_{G_{I}}-^{2}}\|_{2}}{_{y}}\). Since \(\|\|_{2}(1/)\) and \(:=_{y}/(1/)\), this expression is overall \(O()\).

Now, we show why the events in (1) above hold. The first is simply the guarantee of the trimmed mean on a subexponential distribution (cf. AppendixB) and the second holds because \(a([-_{y},_{y}])\).

The third is significantly more involved. We first claim that Line4 approximately preserves (within log factors) the size of the dataset and maintains the ratio of inliers to outliers (within constants). Recall that we use \(G\) for the inlier distribution and \(B\) for the distribution of outliers.

**Lemma 4.2**.: _Consider the context of Theorem1.4 and Algorithm2. First, for every possible choice of \(I\) that can be made in Line3, \(_{(X,y) G}[y I]/_{y}\). Second, \(_{I}[_{(X,y) B}[y I|I]]/_{y}\), where the outer expectation is taken with respect to the random choice of the center of \(I\)._

Proof Sketch.: For inliers, \(y(0,_{y}^{2})\) and thus the fraction of inliers in \(S\) that belong to \(S^{}\) is at least \((/_{y})\) since \(I[-2_{y},2_{y}]\) with length \(\). For outliers, we may assume without loss of generality that \(y[-_{y}-,_{y}+]\) always (since otherwise \(y I\)). Since \(I\) is independent of \((X,y)\), we may treat \((X,y)\) as fixed and only \(I\) as random. Thus, the probability of \(y I\) is at most the ratio of the length of \(I\) to the length of \([-_{y}-,_{y}+]\), which is at most \(O(/_{y})\) as claimed. 

Observe that the set \(S^{}=\{(x,y):(x,y) S,y I\}\) in Line4 consists of i.i.d. points from \((1-_{I})G_{I}+_{I}B_{I}\), where \(G_{I}\) is the conditional distribution from Claim4.1, \(B_{I}\) is the conditional distribution of the outliers, and \(_{I}:=[y I]}{(1-)_{(X,y ) D}[y I]+_{(X,y) B}[y I]}[y I]}{(1-)_{(X,y) D}[y I]}\). By Lemma4.2, \(_{I}[_{I}]=O()\) and thus by Markov's inequality, with high constant probability we will have that \(_{I}=O()\). Finally, it remains to show that \(S^{}\) satisfies the goodness conditions as dictated by Theorem1.3. This requires technical effort and is deferred to AppendixD.2.

## 5 Discussion

In this paper, we provided fast algorithms for mean estimation and linear regression that achieve optimal error under Huber contamination model for isotropic Gaussian inlier distributions. Several open problems and avenues for improvement remain. First, the sample complexity of our linear regression algorithm is multiplicative in \((1/)\), where \(\) is the failure probability, as opposed to additive in the information-theoretic sample complexity . More broadly, it is an open problem to design algorithms that achieve similar optimal guarantees for other fundamental tasks: robust principal component analysis, sparse mean estimation, and covariance estimation. In particular, the best known algorithm for the covariance estimation (or mean estimation with unknown covariance) achieving the optimal error runs in quasi-polynomial time .