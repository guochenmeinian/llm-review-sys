# Reliable learning in challenging environments

Maria-Florina Balcan

Carnegie Mellon University

ninamf@cs.cmu.edu &Steve Hanneke

Purdue University

steve.hanneke@gmail.com &Rattana Pukdee

Carnegie Mellon University

rpukdee@cs.cmu.edu &Dravyansh Sharma

Carnegie Mellon University

dravyans@cs.cmu.edu

###### Abstract

The problem of designing learners that provide guarantees that their predictions are provably correct is of increasing importance in machine learning. However, learning theoretic guarantees have only been considered in very specific settings. In this work, we consider the design and analysis of reliable learners in challenging test-time environments as encountered in modern machine learning problems: namely 'adversarial' test-time attacks (in several variations) and 'natural' distribution shifts. In this work, we provide a reliable learner with provably optimal guarantees in such settings. We discuss computationally feasible implementations of the learner and further show that our algorithm achieves strong positive performance guarantees on several natural examples: for example, linear separators under log-concave distributions or smooth boundary classifiers under smooth probability distributions.

## 1 Introduction

The question of providing reliability guarantees on the output of learned classifiers has been studied previously in the classical learning setting where the training and test data are independent and identically distributed (i.i.d.) draws from the same distribution . Conceptually, a _reliable_ learner outputs a prediction and may output a correctness guarantee. We know that the learner is correct on all points with the guarantee as long as the learning-theoretic assumptions hold, e.g., realizability. While a trivial model that abstains from providing any guarantee is also a reliable learner, we are interested in a reliable learner that provides the guarantee on as many points as possible (_useful_ in the sense of Rivest and Sloan ).  provides a characterization of optimal reliable learners in this classical learning setting.

However, the assumption that the training and test data are drawn from the same distribution is often violated in practice. The mismatch may take the form of a 'natural distribution shift' when the test distribution is different from the training distribution or 'adversarial attacks' when there is an adversary that can perturb a test data point with the goal of changing the model prediction. This is frequently accompanied by a significant performance drop, as well as the inability to guarantee the usefulness of the algorithm. As a result, there is a significant interest in the study of test-time attacks  and distribution shift  among the applied machine learning community. Furthermore, recently there has been growing interest in the theoretical machine learning community for designing approaches with provable guarantees under test time attacks  as well as renewed interest in distribution shift . All the prior theoretical work in the literature has mainly focused on the effect of attacks or distribution shift on average error rate (e.g. ). However,this neglects a major relevant concern for users of machine learning algorithms, namely the ability to provide correctness guarantees for individual predictions: i.e., reliability.

In this work, we advance this line of work by developing a general understanding of how to learn reliably in the presence of corruptions or changes to the test set, specifically under adversarial test-time attacks as well as distribution shift between the training (source) and test (target) data.

**Our results**. We consider algorithms that provide robustly-reliable predictions which are guaranteed to be correct under standard assumptions from statistical learning theory, for both test-time attacks and distribution shift. Our first main set of results tackles the challenging case of adversarial test-time perturbations. For this setting, we introduce a novel compelling reliability criterion on a learner that particularly captures the challenge of reliability under the test-time attacks. Given a test point \(z\), a _robustly-reliable_ classifier either abstains from prediction, or outputs both a prediction \(y\) and a reliability guarantee \(\) with the guarantee that \(y\) is correct unless one of two bad events has occurred: 1) the true target function does not belong to the given hypothesis set \(\) or, 2) a test-point \(z\) is perturbed from its original point by adversarial strength of at least \(\) (measured in the relevant metric). In the case of distribution shift, we provide novel analysis and a complexity measure that extend the classical notion of reliable learning to the setting when the test distribution is allowed to be an arbitrary new distribution.

### Summary of contributions

1. We propose robustly-reliable learners for test-time attacks which guarantee reliable learning in the presence of test-time attacks, and characterize the region of instance space where they are simultaneously robust and reliable. Specifically, under the realizable setting, for adversarial perturbations within metric balls around the test points, we use the radius of the metric ball as a natural notion of adversarial strength. We output a reliability radius \(\) with a guarantee that our prediction on a point is correct as long as it was perturbed with a distance less than \(\) (under a given metric). We further show that our proposed robustly-reliable learner achieves pointwise optimal values for this reliability radius: that is, no robustly-reliable learner can output a reliability radius larger than our learner for any point in the instance space (Theorem B.1, B.2).
2. The pointwise optimal algorithm is easy to derive from our definition. We discuss a computationally efficient implementation of the optimal learners. (Section 4).
3. We discuss variants of these algorithms and guarantees appropriate for three different variants of adversarial losses studied in the literature: depending on whether the perturbed point must have the same label as the original point, or in lieu of this, whether the algorithm should predict the true label of the perturbed point, or the same label as the original point (Definition 1).
4. We further introduce a safely-reliable region, which captures the challenge caused by the adversary's ability to perturb a test point to cause a reduction in our reliability radius (Definition 6). As examples, we show that the safely-reliable region can be large for linear separators under log-concave distributions and for classifiers with smooth decision boundaries under nearly-uniform distributions and as a consequence, the robustly-reliable region is large as well (Theorem 3.3).
5. We extend this characterization to abstention-based reliable predictions for arbitrary adversarial perturbation sets, where we no longer restrict ourselves to metric balls. We again get a tight characterization of the robustly-reliable region (Theorem C.1).
6. We also consider reliability in the distribution shift setting where the test data points come from a different distribution. We introduce a novel refinement to the notion of disagreement coefficient , to measure the **transferability of reliability guarantees** across distributions. We provide bounds on the probability mass of the reliable region under transfer for several interesting examples including, when learning linear separators, transfer from \(_{1}\) log-concave to \(_{2}\) log-concave and to \(s\)-concave distributions (Theorems G.1, G.2). We additionally bound the probability of the reliable region for learning classifiers with general smooth classification boundaries, for transfer between smooth distributions (Theorem G.3).
7. We further extend our reliability results to the setting of robustness tranfer, where the test data is simultaneously under adversarial perturbations as well as distribution shift (Section J).
8. Finally, we demonstrate that it is possible extend our results into the agnostic setting. (Section 7)

**Conceptual advances over prior work.** Prior works on certified robustness  have examined pointwise consistency guarantees. The certified robustness guarantee is only that a prediction does not change with an adversarial perturbation, but it does not guarantee that the prediction is correct (neither for the original point nor the perturbation); in particular, a constant function is always certified robust but it may not be useful. In contrast, our notion of robustly-reliable learner guarantees that, for any test point \(x\) and perturbation \(z\), if \(z\) has a distance less than \(\) to \(x\) (\(\) = reliability radius), then the prediction will be "correct" (robust loss zero) in a sense informed by which robust loss we are addressing; we discuss this idea for several different losses, leading to different interpretations of this guarantee. In particular, for the stability loss, the prediction being "correct" means that it predicts the true label of the original point \(x\); this implies certified robustness, but is even stronger, since it also guarantees the correct label. Prior work  introduces the notion of a robustly-reliable learner for poisoning attacks which is different from our definition that is tailored to test-time attacks with a guarantee in terms of a reliability radius. In distribution shifts setting, we are the first to assess the transferability of reliability guarantees which differ from a widely-studied metric of average error rate. For additional related work, we refer to Appendix A.

## 2 Preliminaries and problem formulation

Let \(\) denote the instance space and \(=\{0,1\}\) be the label space. Let \(\) be a hypothesis class. The learner \(\) is given access to a labeled sample \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) drawn from a distribution \(\) over \(\) and learns a concept \(h^{}:\). In the realizable setting, we assume we have a hypothesis (concept) class \(\) and target concept \(h^{*}\) such that the _true label_ of any \(x\) is given by \(h^{*}(x)\). In particular, \(S=\{(x_{i},h^{*}(x_{i}))\}_{i=1}^{m}\) in this setting. Given the 0-1 loss function \(:\{0,1\}\), define \(_{S}(h,)=_{(x,y) S}(h,x)\). We use \(_{}\) to denote the marginal distribution over \(\). We use \([]\) to denote the indicator function that takes values in \(\{0,1\}\). We also define \(B_{}^{}(h^{*},r)=\{h _{}[h(x) h^{*}(x)] r\}\) as the set of hypotheses in \(\) that disagree with \(h^{*}\) with probability at most \(r\). During test-time, the learner makes a prediction on a test-point \(z\). We consider the following settings

1. **Adversarial test-time attack.** We consider adversarial attacks with perturbation function \(: 2^{}\) that can perturb a test point \(x\) to an arbitrary point \(z\) from the perturbation set \((x)\). We assume that the adversary has access to the learned concept \(h^{}\) as well as the test point \(x\), and can perturb this data point to any \(z(x)\) and then provide this perturbed data point to the learner at test-time. We want to provide pointwise robustness and reliability guarantees in this setting. We will assume that \(x(x)\) for all \(x\). For any point \(z\), we have \(^{-1}(z):=\{x|z(x)\}\), the set of points that can be perturbed to \(z\). We use _perturbation_ to refer to a point \(z(x)\) and the perturbation sets \((x)\) interchangeably.
2. **Distribution shift.** We consider when a test point \(z\) is drawn from a different distribution from the training samples. In this case, we want to provide a pointwise reliability guarantee. We will discuss more on this in Section 5.

### Robust loss functions

In the applied and theoretical literature, various definitions of adversarial success have been explored, each dependent on the interpretation of robustness; depending on whether the perturbed point must have the same label as the original point, or in lieu of this, whether the algorithm should predict the true label of the perturbed point, or the same label as the original point. To capture these, we formally consider the following loss functions.

**Definition 1** (Robust loss functions).: _For a hypothesis \(h\), a test point \(x\), and a perturbation function \(\), we consider the following adversarially successful events._

1. _Constrained Adversary loss_ _[_1_,_ 1_,_ 1_]__. There exists a perturbation_ \(z\) _of_ \(x\) _that does not change the true label of an original point_ \(x\) _but_ \(h(z)\) _is incorrect._ \[_{}^{h^{*}}(h,x)=_{z(x)\\ h^{*}(z)-h^{*}(x)}[h(z) h^{*}(z)].\] _For a fixed perturbation_ \(z(x)\)_, define_ \(_{}^{h^{*}}(h,x,z)=[h(z) h^{*}(z) h^{*}(z)=h^ {*}(x)].\)__
2. _True Label loss_ _[_1_,_ 1_]__. There exists a perturbation_ \(z\) _of_ \(x\) _such that_ \(h(z)\) _is incorrect._ \[_{}^{h^{*}}(h,x)=_{z(x)}[h(z) h^ {*}(z)].\]_In this case, if the true label of the \(z\) changes, then the learner need to match its prediction with the new label. For a fixed perturbation \(z(x)\), define \(_{}^{h^{*}}(h,x,z)=[h(z) h^{*}(z)]\)._
3. _Stability loss__. There exists a perturbation \(z\) of \(x\) such that \(h(z)\) is different from \(h^{*}(x)\)._ \[_{}^{h^{*}}(h,x)=_{z(x)}[h(z) h^{ *}(x)].\] _In this case, we focus on the consistency aspect where we want the prediction of any perturbation_ \(z\) _to be the same as the prediction of_ \(x\) _and this has to be correct w.r.t._ \(x\) _i.e. equals to_ \(h^{*}(x)\)_. For a fixed perturbation_ \(z(x)\)_, define_ \(_{}^{h^{*}}(h,x,z)=[h(z) h^{*}(x)]\)_._
4. _Incentive-aware Adversary loss__. We take inspiration from economics application where we assume that the label_ \(1\) _is a more favorable outcome e.g. loan approval for which an adversary has no incentive to make any perturbation when the original label is_ \(1\)_. Define the perturbation set_ \[_{}(x,h^{*})=(x)&;\,\,h^{*}(x)=0 \\ \{x\}&;\,\,h^{*}(x)=1\] _and define an incentive-aware adversary loss as_ \[_{}^{h^{*}}(h,x)=_{z_{}(x,h^{*})} [h(z) h^{*}(x)].\]

_For a fixed perturbation \(z(x)\), define \(_{}^{h^{*}}(h,x,z)=[h(z) h^{*}(x) z _{}(x,h^{*})]\)._

_We say that \(h\) is robust to a perturbation function \(\) at \(x\) w.r.t. a robust loss \(\) if \(_{}^{h^{*}}(h,x)=0\)._

**Remark**.: \(_{},_{}\) are robust losses that we can always evaluate in practice on the training data since we are comparing \(h(z)\) with \(h^{*}(x)\) which is known to us on the training data. For \(_{},_{}\), we are comparing \(h(z)\) with \(h^{*}(z)\) for which \(z\) may lie outside of the support of the natural data distribution and we may not have access to \(h^{*}(z)\). We illustrate the relationship between these losses by making a few useful observations.

* In the robustly-realizable case  when the perturbation function \(\) does not change the _true_ label of any \(x\) in the training or test data, then all the losses \(_{},_{},_{}\) are equivalent. This corresponds to a common assumption in the adversarial robustness literature, that the perturbations are "human-imperceptible", which is usually quantified as the set of perturbations within a small metric ball around the data point.
* We provide an illustration of the perturbation set considered in various robust losses in Figure 1. By considering these perturbation set, we have the following implication \(_{}_{}\), \(_{}_{}\) and \(_{}_{}\) where \(_{1}_{2}\) means robustness w.r.t. \(_{1}\) implies robustness w.r.t. \(_{2}\).

Figure 1: Different perturbation sets considered in \(_{},_{}\) (left), \(_{}\) (mid) and \(_{}\) (right). The dashed line represents the decision boundary of \(h^{*}\) and the background color of red and blue represents the label \(0\) and \(1\) respectively. The ball around each point describes the possible perturbation set \((x)\) and the shaded area inside each ball is the allowed perturbation. In \(_{},_{}\), we consider all perturbation in \((x)\) while in \(_{}\), we consider perturbations that do not change the true label of the perturbed point. Lastly, in \(_{}\), an adversary only perturb points where the original true label is \(0\).

Robustly-reliable learners w.r.t. metric ball attacks

Although our robust losses are defined for any general perturbation set, we first consider the case where the perturbation sets are balls in some metric space. Such attacks are widely studied in the literature, in particular, for balls with bounded \(L_{p}\)-norm. Moreover, the radius of the metric ball serves as a natural notion of adversarial strength that allows us to quantify the level of robustness. We will later (Theorem C.1) present results for general perturbation sets as well.

Let \(=(,d)\) be a metric space equipped with distance metric \(d\). We use the notation \(_{}(x,r)=\{x^{} d(x,x^{})  r\}\) (resp. \(^{}_{}(x,r)=\{x^{} d (x,x^{})<r\}\)) to denote a closed (resp. open) ball of radius \(r\) centered at \(x\). We will sometimes omit the underlying metric \(\) from the subscript to reduce notational clutter. We formally define a metric ball attack as follows.

**Definition 2**.: _Metric-ball attacks are defined as the class of perturbation functions \(_{}=\{u_{}: 2^{} u_{ }(x)=_{}(x,)\}\), induced by the metric \(=(,d)\) defined over the instance space._

At test-time, given a test-point \(z\), we would like to make a prediction at \(z\) with a reliability guarantee. We consider this type of learner, a _robustly-reliable_ learner defined formally as follows.

**Definition 3** (Robustly-reliable learner w.r.t. \(\)-ball attacks).: _A learner \(\) is robustly-reliable w.r.t. \(\)-ball attacks for hypothesis space \(\) and robust loss function \(\) if, **for any target** concept \(h^{*}\), given \(S\) labeled by \(h^{*}\), the learner outputs functions \(h^{}_{}:\) and \(r^{}_{S}:[0,)\{-1\}\) such that for all \(x,z\) if \(r^{}_{S}(z)=>0\) and \(z^{}_{}(x,)\) then \(^{h^{*}}(h^{}_{S},x,z)=0\). Further, if \(r^{}_{S}(z)=0\), then \(h^{*}(z)=h^{}_{S}(z)\)._

Note that \(\) outputs a prediction and a real value \(r\) (the "reliability radius") for any test input. \(r=-1\) corresponds to abstention (even in the absence of perturbation) i.e. when the learner is incapable of giving a reliability guarantee for that prediction), and \(r=>0\) is a guarantee from the learner that if the adversary's attack is in \(^{}_{}(x,)\) then we are correct i.e. if an adversary changes the original test point \(x\) to \(z\), the attack will not succeed if the adversarial budget is less than \(\). Lastly, when \(r=0\), the learner provides a guarantee that the learner's prediction at \(z\) is correct.

**Definition 4** (Robustly-reliable region w.r.t. \(\)-ball attacks).: _For a robustly-reliable learner \(\) w.r.t. \(\)-ball attacks for sample \(S\), hypothesis space \(\) and robust loss function \(\) defined above, the robustly-reliable region of \(\) at a reliability level \(\) is defined as \(^{}(S,)=\{x r^{}_{S}(x) \}\) for sample \(S\) and \( 0\)._

The robustly-reliable region contains all points with a reliability guarantee of at least \(\). We use \(^{}_{W}\) to denote robustly-reliable regions with respect to losses \(_{W}\) for \(W\{,,,\}\). A natural goal is to find a robustly-reliable learner \(\) that has the largest robustly-reliable region possible. First, we note that predictions that are known by the learner to be correct are still known to be correct even when the test points are attacked. Therefore, a test point \(z\) lies in the robustly-reliable region w.r.t. \(_{},_{}\), as long as we can be sure that \(h^{}_{S}(z)\) is correct. This is equivalent to \(z\) being classified perfectly, i.e. according to the true label. Therefore, the robustly-reliable region w.r.t. \(_{},_{}\) is given by the agreement region of the version space, which is the largest region where we can be sure of what the correct label is in the absence of any adversarial attack . We recall the definition of version space  and agreement region .

**Definition 5**.: _For a set \(H\) of hypothesis, and any set of samples \(S\), let \((H)=\{x: h_{1},h_{2}\) s.t. \(h_{1}(x) h_{2}(x)\}\) be the **disagreement region** and \((H)=(H)\) be the **agreement region**. Let \(_{0}(S)=\{h|_{S}(h)=0\}\) be a **version space**: the set of all hypotheses that correctly classify \(S\). More generally, \(_{}(S)=\{h|_{S}(h)\}\) for \( 0\)._

We can also characterize the robustly-reliable region with respect to other robust losses in terms of the agreement region in the following Theorem.

**Theorem 3.1**.: _Let \(\) be any hypothesis class. With respect to \(\)-ball attacks and \(_{W}\), for \( 0\),_

1. _there exists a robustly-reliable learner_ \(\) _such that_ \(^{}_{W}(S,) A_{W}\)_, and_
2. _for any robustly-reliable learner_ \(\)_,_ \(^{}_{W}(S,) A_{W}\)_._

_Specifically, for the robust loss \(_{W}\), the optimal robustly-reliable region \(A_{W}\) are_Proof.: (Sketch) We provide the construction of the optimal robustly-reliable learner \(_{}\) such that \(_{W}^{_{}}(S,) A_{W}\) and later show that for any robustly-reliable learner \(\), we must also have \(_{W}^{_{}}(S,) A_{W}\). We start with \(_{},_{}\), consider a learner \(_{}\) that predicts using an ERM classifier and outputs \(=\) for all points in the agreement region of \(_{0}(S)\). Any prediction in \((_{0}(S))\) is reliable because it also agrees with \(h^{*}\) (\(h^{*}_{0}(S)\) by realizability). On the other hand, for \(z(_{0}(S))\), there exist \(h_{1},h_{2}_{0}(S)\) that disagree on \(z\). For any learner \(\), it is not possible to guarantee that \(h^{}(z)\) is correct as we may have \(h^{*}=h_{1}\) or \(h^{*}=h_{2}\).

Now, for \(_{}\), consider a learner \(_{}\) that classifies using an ERM but the reliability radius is now the largest \(>0\) for which \(^{o}(z,)(_{0}(S))\) and \(h(x)=h(z),\ \ x^{o}(z,),h_{0}(S)\), else \(=0\) if \(z(_{0}(S))\), and \(-1\) otherwise. The first condition guarantees that \(h(x)=h^{*}(x), x^{o}(z,)\). Combined with the second condition we have \(h(z)=h(x)=h^{*}(x), x^{o}(z,)\). Thus, \(_{}\) is a robustly-reliable learner. On the other hand, for a robustly-reliable learner \(\), consider \(z_{}^{}(S,)\) for \(>0\). We must have \(h^{}(z)=h^{*}(x), x^{o}(z,)\). Using a similar argument to the case for \(_{},_{}\), we have \(z(_{0}(S))\). If there exists \(x^{o}(z,)\) that \(x(_{0}(S))\), there exists \(h_{1},h_{2}_{0}(S)\) that \(h^{}(z) h_{1}(x)\) or \(h^{}(z) h_{2}(x)\). It is not possible to guarantee that \(h^{}(z)=h^{*}(x)\) as we may have \(h^{*}=h_{1}\) or \(h^{*}=h_{2}\). Therefore, we must have \(^{o}(z,)(_{0}(S))\). Finally, we cannot have \(x^{o}(z,)\) that \(h(z)=h^{*}(z) h^{*}(x)\) since this contradict with \(h(z)=h^{*}(x)\). Therefore, we must have \(h^{*}(x)=h^{*}(z)\). Since we have \(x(_{0}(S))\), this implies that \(h(x)=h(z)\) for \(h_{0}(S)\). For \(_{}\), the construction is similar to \(_{}\). For full proof, we refer to Appendix B. 

For \(_{}\), the learner is able to certify a subset of the agreement region, which satisfies two additional conditions: \(h_{S}^{}\) must be correct on all possible points \(x\) that could be perturbed to an observed test point \(z\), and the true label of \(z\) should match the true label of \(x\). We denote the second condition of \(h(z)=h(x), x^{o}(z,), h_{0}(S)\) as the **label consistency condition**. For \(_{}\), since robustness w.r.t. \(_{}\) implies robustness w.r.t. \(_{}\), we have \(A_{} A_{}\). In addition, with an incentive-aware adversary, whenever \(h(z)=0\), we must have \(h^{*}(x)=0\) since the adversary does not perturb a data point with the original label \(1\). Therefore, we can additionally provide a guarantee on \(z\) that lies in the agreement region and \(h(z)=0\). We remark that we can identify the term \(h^{*}(z)\) for any point \(z(_{0}(S))\) due to the realizability assumption.

We have provided a robustly-reliable learner with the largest possible robustly-reliable region for losses \(_{},_{},_{},_{}\). However, we note that the probability mass of the robustly-reliable region may not be a meaningful way to quantify the overall reliability of a learner because a perturbation \(z\) may lie outside of the support of the natural data distribution and have zero probability mass. It seems more useful to measure the mass of points \(x\) where any perturbation \(z\) of \(x\) still lies within the robustly-reliable region. We formally define this region as the safely-reliable region.

**Definition 6** (Safely-reliable region w.r.t. \(\)-ball attacks).: _Let \(\) be a robustly-reliable learner w.r.t. \(\)-ball attacks for sample \(S\), hypothesis class \(\) and robust loss function \(\). The safely-reliable region of learner \(\) at reliability levels \(_{1},_{2}\) is defined as_

1. \(_{}^{}(S,_{1},_{2})=\{x _{}(x,_{1})\{z h^{*}(z)=h^{*}(x)\} _{}^{}(S,_{2})\},\)__
2. \(_{}^{}(S,_{1},_{2})=\{x _{}(x,_{1})_{W}^{}(S, _{2})\}\) _for_ \(W\{,\}\)_,_
3. \(_{}^{}(S,_{1},_{2})=\{x h ^{*}(x)=0_{}(x,_{1})_{}^ {}(S,_{2})\}\{x h^{*}(x)=1 x _{}^{}(S,_{2}))\}\)_._

The safely-reliable region contains any point that retains a reliability radius of at least \(_{2}\) even after being attacked by an adversary with strength \(_{1}\). In the safely-reliable region, we consider a set of potential natural (before attack) points \(x\), while in the robustly-reliable region, we consider a set of potential test points \(z\). In the following subsections, we show that in interesting cases commonly studied in the literature, the probability mass of the safely-reliable region is actually quite large.

### Safely-reliable region for linear separators under log-concave distributions is large

We provide the probability mass of safely-reliable regions with respect to different losses for linear separators when the data distribution follows an isotropic (mean zero and identity covariance matrix) log-concave (logarithm of density function is concave) distribution under a bounded \(L_{2}\)-norm ball attack. For full proof, we refer to Appendix D. We will rely on the following key lemma which states that the agreement region of a linear separator cannot contain points that are arbitrarily close to the decision boundary of \(h^{*}\) for any sample \(S\).

**Lemma 3.2**.: _Let \(\) be a distribution over \(^{d}\) and \(=\{h:x( w_{h},x) w _{h}^{d},\|w_{h}\|_{2}=1\}\) be a class of linear separators. For \(h^{*}\), for a set of samples \(S^{ n}\) such that there is no data point in \(S\) that lies on the decision boundary, for any \(0<c<d\), there exists \((S,c,d)>0\) such that for any \(x\) with \(c\|x\| d\) and \(| w_{h^{*}},x|<\), we have \(x(_{0}(S))\)._

A direct implication of the lemma is that any \(L_{2}\)-ball \((x,)\) that lies in the agreement region must not contain the decision boundary of \(h^{*}\) and must contain points with the same label. This allows us to remove the _label consistency condition_ and instead focus on whether the ball \((x,)\) lies in the agreement region. Intuitively, the reliable region is now given by the '\(\)-buffered' agreement region where we only select points that have a distance at least \(\) from the boundary of the agreement region (Figure 2). We provide bounds for the probability mass of the safely-reliable region below and we refer to the full proof in Appendix D.

**Theorem 3.3**.: _Let \(\) be isotropic log-concave over \(^{d}\) and \(=\{h:x( w_{h},x) w _{h}^{d},\|w_{h}\|_{2}=1\}\) be the class of linear separators. Let \((,)\) be a \(L_{2}\) ball perturbation with radius \(\). For \(S^{ n}\), for \(m=(}(()+ ))\), for an optimal robustly-reliable learner \(\),_

1. \((^{}_{}(S,_{1},_{2})) 1-2_{1}- }()\) _with probability at least_ \(1-\)_,_
2. \(^{}_{A}(S,_{1},_{2})=^{ }_{L}(S,_{1},_{2})\) _almost surely,_
3. \((^{}_{S}(S,_{1},_{2})) 1-2(_{1}+ _{2})-}()\) _with probability at least_ \(1-\)_,_
4. \((^{}_{A}(S,_{1},_{2})) 1-(_{1}+ _{2})-}()\) _with probability at least_ \(1-\)_._

_The \(}\)-notation suppresses dependence on logarithmic factors and distribution-specific constants._

We remark that we can't always remove the label consistency condition for a general perturbation set. For example, consider \((x)=(x-a,)\{x\}(x+a,)\), is made of two \(L_{2}\) balls with center \(x-a,x+a\), with appropriate value of \(a,\), we may have each ball lie in the different side of the agreement region so that the whole perturbation set lie in the agreement region but contain points with different labels (Figure 3). We also provide bounds on the probability mass of the safely-reliable region for more general concept spaces beyond linear separators, specifically, classifiers with smooth boundaries in Appendix E.

## 4 On computational efficiency

Given the definition, it is possible to implement computationally efficient robustly-reliable learners. For example, for linear separator concept classes under bounded \(L_{2}\)-norm attack. The optimal

Figure 2: The robustly-reliable region for \(_{},_{}\) (left), \(_{}\) (mid) and \(_{}\)(right) for linear separators with an \(L_{2}\)-ball perturbation. The background color of blue and red represents the agreement region of class \(1\) and \(0\) respectively. In this case, we can remove the label consistency condition and reduce the robustly-reliable region into the ‘\(\)-buffered’ agreement region.

robustly-reliable learner \(_{}\), described above may be implemented via a linearly constrained quadratic program that computes the (squared) distance of the test point \(z\) to the closest point \(z^{}\) in the disagreement region. This gives us the reliability radius, since for linear separators one must cross the decision boundary to perturb a point to a differently labeled point

\[_{w,w^{},z^{}}||z-z^{}||^{2}\] s.t. \[ w,x_{i} y_{i} 0, (x_{i},y_{i}) S,\] \[ w^{},x_{i} y_{i} 0, (x_{i},y_{i}) S,\] \[ w,z^{} w^{},z^{} 0.\]

Given training sample \(S\), for any given test point \(z\), the learner \(\) can efficiently compute the solution \(s^{*}\) to the above program and output \(}\) as the reliability radius. We show that the variant of this objective also provides a reliability radius for a wide range of hypothesis classes under \(L_{2}\) ball attacks (see Lemma F.1). In addition, we can relax this objective into a regularized objective that gives a lower bound on the reliability radius of \(||z-z^{*}||^{2}\), when \(z^{*}\) is the solution of

\[h_{1},h_{2},z^{*}=*{argmin}_{h,h^{},z^{}}||z-z^{ }||^{2}+((h,S\{(z^{},0)\})+(h^{},S \{(z^{},1)\}))\]

when \((h,S)\) is the empirical risk of \(h\) on \(S\). We provide a more detailed discussion in Appendix F.

## 5 Robustly-reliable learning under distribution shift

We now consider the reliability aspect for distribution shift, a different kind of test-time robustness challenge when the test data comes from a different distribution than the training data. Formally, let \(\) be the training distribution and let \(\) be the test distribution. We assume the _realizable distribution shift_ setting, i.e. there is a target concept \(h^{*}\) such that the true label of any \(x\) is given by \(h^{*}(x)\) at training time and test time, or \(_{}(h^{*})=_{}(h^{* })=0\). As observed earlier, points that

Figure 4: The disagreement region and the agreement region under a distribution shift where \(\) and \(\) are isotropic (left) and where there is a mean shift (right).

Figure 3: The perturbation set is represented by two dashed balls. This lies inside the agreement region but contains points with different labels.

are known by the learner to be correct (reliable) are still known to be correct even when it is drawn from a different distribution. This reliability guarantee holds even when the distributions \(\) and \(\) do not share a common support, a setting for which many prior theoretical works result in vacuous bounds. For example, suppose \(=^{n}\), \(\) and \(\) are supported on disjoint \(n\)-balls, and \(\) is the class of linear separators. Then the total variation distance, the \(\)-divergence  as well as the discrepancy distance  between \(\) and \(\) are all 1. While recent work of  does apply in this setting, they do not focus on the reliability guarantee. In this work, we are interested in quantifying the transferability of reliability guarantee transfer between distributions \(\) and \(\). We recall the notion of reliable prediction .

**Definition 7** (Reliability).: _A learner \(\) is reliable w.r.t. concept space \(\) if, for any target concept \(h^{*}\), given any sample \(S\) labeled by \(h^{*}\), the learner outputs functions \(h^{}_{S}:\) and \(a^{}_{S}:\{0,1\}\) such that for all \(x\) if \(a^{}_{S}(x)=1\) then \(h^{}_{S}(x)=h^{*}(x)\). Else, if \(a^{}_{S}(x)=0\), the learner abstains from prediction. The reliable region of \(\) is \(R^{}(S)=\{x a^{}_{S}(x)=1\}\)._

We define the following metric to measure the reliability of a learner under distribution shift.

**Definition 8** (\(\) reliable correctness).: _The \(\)-reliable correctness of \(\) (at sample rate \(m\), for distribution shift from \(\) to \(\)) is defined as the expected probability mass of its reliable region under \(Q\) when trained on a random training \(S^{m}\), i.e. \(_{x Q,S P^{m}}[x R^{}(S)]\)._

The disagreement coefficient was originally introduced to study the label complexity in agnostic active learning  and is also known to characterize reliable learning in the absence of any distribution shift . We propose the following refinement to the notion of disagreement coefficient, which we will use to give bounds on a learner's \(\) reliable correctness.

**Definition 9** (\(\) disagreement coefficient).: _For a hypothesis class \(\), the \(\) disagreement coefficient of \(h^{*}\) with respect to \(\) is given by_

\[_{}()=_{r} }[(B_{}(h^{*},r))]}{r},\]

_where \(B_{}(h^{*},r)=\{h_{}[h(x) h^{* }(x)] r\}\)._

This quantifies the rate of disagreement over \(\) among classifiers which are within disagreement-balls w.r.t. \(h^{*}\) under \(\), relative to the version space radius. The proposed metric is asymmetric between \(\) and \(\), and also depends on the target concept \(h^{*}\). More simple examples are in Appendix H. We show that the \(\)-reliable correctness of our learner may be bounded in terms of the \(\) disagreement coefficient using a uniform convergence based argument. The proof details are in Appendix I.

**Theorem 5.1**.: _Let \(\) be a realizable distribution shift of \(\) with respect to \(\), and \(h^{*}\) be the target concept. Given sufficiently large sample size \(m}(d+)\), the \(\)-reliable correctness of \(\), the optimal robustly-reliable learner, is at least_

\[_{x Q,S P^{m}}[x R^{}(S)] 1-_{}-.\]

_Here \(c\) is an absolute constant, and \(d\) is the VC-dimension of \(\)._

In Appendix J, we show that this \(\) disagreement coefficient can be small for several examples which implies that it is possible to transfer the reliability guarantee from one distribution to the other. In particular, when learning linear separators, we provide bounds for transferring from \(_{1}\) log-concave to \(_{2}\) log-concave and to \(s\)-concave distributions (Theorems G.1, G.2). In addition, when learning classifiers with general smooth classification boundaries, we provide bounds for transferring between smooth distributions (Theorem G.3).

## 6 Safely-reliable correctness under distribution shift

There is a growing practical  as well as recent theoretical interest  in the setting of 'robustness transfer', where one simultaneously expects adversarial test-time attacks as well as distribution shift. We will study the reliability aspect for this more challenging setting. We note that the definition of a robustly-reliable learner does not depend on the data distribution (seeDefinition 3) as the guarantee is pointwise. Our optimality result in Section 3 applies even when a test point is drawn from a different distribution \(\). In this case, the safely-reliable region instead would have a different probability mass.

**Definition 10** (\(\) safely-reliable correctness).: _The \(\) safely-reliable correctness of \(\) (at sample rate \(m\), for distribution shift from \(\) to \(\), w.r.t. robust loss \(\)) is defined as the probability mass of its safely-reliable region under \(Q\), on a sample \(S^{m}\), i.e._

\[_{}^{}(S,_{1},_{2}):=_{x Q,S P^{m }}[x_{}^{}(S,_{1},_{2})].\]

We consider an example when the training distribution \(\) is isotropic log-concave and the test distribution \(_{}\) is log-concave with its mean shifted by \(\) but the covariance matrix is still an identity matrix (see Figure 4, right). We provide the bound on the \(\) safely-reliable correctness of this example in Appendix J (see Theorem J.2).

## 7 Reliability in the agnostic setting

In the above, we have assumed that the training samples \(S\) are realizable under our concept class \(\), i.e. there is a target concept \(h^{*}\) consistent with our (uncorrupted) data. In the agnostic setting, we can have \(_{h}_{S}(h)>0\), meaning no single concept is always correct. We define a _\(\)-tolerably robustly-reliable_ learner under test-time attacks in the agnostic setting as the learner whose reliable predictions agree with every low-error hypothesis (error at most \(\)) on the training sample () have proposed the corresponding definition for data poisoning attacks).

**Definition 11** (\(\)-tolerably robustly-reliable learner w.r.t. \(\)-ball attacks).: _A learner \(\) is robustly-reliable w.r.t. \(\)-ball attacks for sample \(S\), hypothesis space \(\) and robust loss function \(\) if, for **every** concept \(h^{*}\) with \(_{S}(h^{*})\), the learner outputs functions \(h^{}_{S}:\) and \(r^{}_{S}:[0,)\{-1\}\) such that for all \(x,z\) if \(r^{}_{S}(z)=>0\) and \(z^{}_{}(x,)\) then \(^{h^{*}}(h^{}_{S},x,z)=0\). Further, if \(r^{}_{S}(z)=0\), then \(h^{*}(z)=h^{}_{S}(z)\). Given sample \(S\) such that some concept \(h^{*}\) satisfies \(_{S}(h^{*})\), the robustly-reliable region of \(\) is defined as \(RR^{}(S,,)=\{x r^{}_{S}(x)\}\) for \(, 0\)._

We generalize our results from Section 3 to the agnostic setting (proof details in Appendix K). Here \(_{}(S)=\{h_{S}(h)\}\).

**Theorem 7.1**.: _Let \(\) be any hypothesis class. With respect to \(\)-ball attacks and \(_{}\), for \( 0\),_

1. _There exists a robustly-reliable learner_ \(\) _such that_ \(RR^{}_{}(S,,)( _{}(S))\)_,_
2. _For any robustly-reliable learner_ \(\)_,_ \(RR^{}_{}(S,,)( _{}(S))\)_._

## 8 Discussion

In this work, we generalize the classical line of works on reliable learning to address challenging test-time environments. We propose a novel robustly-reliability criterion that is applicable to several variations of test-time attacks. Our analysis leads to an easy-to-derive algorithm that can be implemented efficiently in many cases. Additionally, we introduce a \(\) disagreement coefficient to capture the transferability of the reliability guarantee between distributions. The proposed robustly-reliability criterion and the \(\) disagreement coefficient together provide a comprehensive framework for handling test-time attacks and evaluating the reliability of learning models. This contributes to the advancement of reliable learning methodologies in the face of challenging real-world scenarios, facilitating the development of more resilient and trustworthy machine learning systems. Notably, key questions remain open, including, how to efficiently implement the algorithm for a class of neural networks, and how to learn reliably with respect to any general robust loss function?

## 9 Acknowledgements

This work was supported in part by NSF grants CCF-1910321 and SES-1919453, the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003, a Bloomberg Data Science PhD fellowship, and a Simons Investigator Award.