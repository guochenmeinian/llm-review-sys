# Frequency-aware Generative Models for Multivariate Time Series Imputation

Xinyu Yang\({}^{1}\), Yu Sun\({}^{1}\), Xiaojie Yuan\({}^{1}\), Xinyang Chen\({}^{2}\)

\({}^{1}\)College of Computer Science, DISSec, Nankai University, China

{yangxinyu@dbis.,sunyu@,yuanxj@}nankai.edu.cn

\({}^{2}\)School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China

chenxinyang@hit.edu.cn

Corresponding authors.

###### Abstract

Missing data in multivariate time series are common issues that can affect the analysis and downstream applications. Although multivariate time series data generally consist of the trend, seasonal and residual terms, existing works mainly focus on optimizing the modeling for the first two items. However, we find that the residual term is more crucial for getting accurate fillings, since it is more related to the diverse changes of data and the biggest component of imputation errors. Therefore, in this study, we introduce frequency-domain information and design **F**requency-aware **G**enerative Models for Multivariate **T**ime **S**eries **I**mputation (**FGTI**). Specifically, FGTI employs a high-frequency filter to boost the residual term imputation, supplemented by a dominant-frequency filter for the trend and seasonal imputation. Cross-domain representation learning module then fuses frequency-domain insights with deep representations. Experiments over various datasets with real-world missing values show that FGTI achieves superiority in both data imputation and downstream applications.

## 1 Introduction

Missing data are commonly observed in the multivariate time series due to diverse reasons , which would encounter subsequent analysis and applications . It is not surprising that more accurate missing data imputation generally leads to better performance in downstream applications.1

Existing techniques [13; 28] have revealed that time series data can be decomposed into three distinct terms, i.e., trend, seasonal, and residual, and try to compute an imputation by modeling the first two items as accurately as possible. Figure 1 reports a survey for the imputation accuracy of the three terms by representative imputation methods over the pre-decomposed KDD  dataset with 10% missing values.2 The dataset comprises 8,034 consecutive readings of meteorological and air quality data taken over a year in Beijing. In this dataset, the trend term may reflect long-term changes in climate or air quality conditions, and the seasonal term might capture patterns associated with different seasons. Moreover, the residual term may consist of short-term, irregular, and high-frequency changes. As shown in Figure 1, the imputation error is mainly caused by the residual term, which has not been well studied, unfortunately.

Recent studies indicate that the high-frequency components are intricately related to the residual [46; 58; 26] and contain critical information for imputing the residual term. Unfortunately, deep learning architectures cannot generalize well in modeling high-frequency components. [38; 43]. Tomeet this challenge, we design **F**requency-aware **G**enerative Models for **M**ltivariate **T**ime **S**eries **I**mputation (**FGTI**), which can extract frequency-domain information and use two cross-domain (i.e., time-domain and frequency-domain) representation learning modules to guide the generation process of deep models. Specifically, we start with the high-frequency filter designed to extract the high-frequency information essential for guiding the accurate imputation of the residual term. This choice is consistent with recognizing the critical role of high-frequency information in imputation accuracy. Then, we introduce the dominant-frequency filter to address the potential challenge posed by high-frequency information for imputing trend and seasonal terms . Furthermore, our cross-domain representation learning frameworks combine frequency-domain information with deep representations in the time-domain, enabling seamlessly intertwining frequency-domain information with time and attribute dependencies modeling.

Our research makes several notable contributions:

* We design a frequency-aware generative model FGTI with frequency-domain information integrated by the high-frequency filter and the dominant-frequency filter, to enhance the awareness of the frequency-domain.
* We introduce two cross-domain representation learning modules that provide models with prior knowledge of intricate frequency-related patterns for missing data imputation.
* We evaluate FGTI on three time series datasets with real-world missing values, which demonstrates the superiority of FGTI in both imputation accuracy and downstream applications.

## 2 Related work

Traditional imputation methods usually employ the statistics, such as mean value , median value , or last observed value , to impute missing data for multivariate time series. It is not surprising that such traditional signals cannot make full use of the valuable semantics of available data. BTMF  and TIDER  employ the low-rank matrix factorization to impute missing data. Unfortunately, due to the matrix capacity's limitation, it is still challenging to accurately match imputation values with the underlying complex relationships and dependencies.

Many studies have shown that deep learning based imputation methods are effective to fill multivariate time series data, such as BRITS , TST , SAITS , STCPA , TimesNet . According to , forecasting models [50; 55] can also be applied to imputation task. Additionally, GRIN  and DAMR  use graph neural networks to incorporate known relationships between attributes. However, these methods with fixed model outputs cannot capture the uncertainty and variability of missing values. Recently, researchers have attempted to utilize large language models (LLMs) as the backbone for time series analysis . Since there are significant differences between time series data and natural language, it still has a lot of room for improvement.

Figure 1: Improving the imputation accuracy of the residual term is the key to boosting the imputation performance of the model.

To capture the uncertainty and variability, researchers introduce generative models  into the missing data imputation by learning the implicit distribution of missing values. In the early stage, researchers mainly use variational Autoencoders (VAE) or generative adversarial networks (GAN) to generate new samples that match the distribution of the training dataset and impute missing values with the generated samples. VAE-based methods learn data distributions by optimizing the reconstruction error and regularizing in the latent space [31; 15; 35; 36; 11]. However, they may not capture the complex variability of missing data well and may produce inaccurate results, especially when the latent spaces are not well aligned. GAN-based approaches use the adversarial training technique of the generator and discriminator to improve the imputation results [56; 29; 30; 34]. Unfortunately, they may face convergence difficulties that can affect the imputation accuracy.

Diffusion models have been introduced into the imputation task recently, considering the success in various fields [40; 18; 24]. To impute time series data, CSDI  designs conditional score-based diffusion models with the conditions only on observed values, and SSSD  utilizes both conditional diffusion models and structured state space models. Additionally, MIDM  develops the noise sampling, addition, and denoising mechanisms, and PriSTI  further studies the enhanced prior modeling by extracting spatio-temporal dependencies as contextual conditions for spatio-temporal data imputation. However, as analyzed in the introduction, existing studies underestimate the importance of accurately modeling the residual term for missing data imputation.

For the time series imputation methods in frequency domain, mvLSWimpute  utilizes wavelet transforms to guide imputation, APDNet  uses the Fourier Temporal and Fourier Variable Interaction modules to model dependencies. In addition, the frequency domain time series forecasting methods FEDformer , FreTS  can also be applied to imputation task. However, they did not consider using frequency domain information to model the missing data's residual terms accurately, which is critical for boosting the overall imputation performance.

In contrast, our FGTI captures high-frequency information and dominant-frequency information to get a more accurate modeling of the residual term, while assisting in describing trend and seasonal terms.

## 3 Frequency-aware Generative Models

In this paper, we focus on the incomplete multivariate time series imputation problem. The input multivariate time series \(=(_{1},,_{D})^{D L}\) is a set of \(D\) attribute values recorded at \(L\) consecutive timestamps, where each attribute series \(_{d}^{L}\). Each element \(x_{ij}\) in \(\) is the observation of the \(i\)-th attribute at the \(j\)-th timestamp, which is probably missing. We use the binary mask matrix \(\{0,1\}^{D L}\) to represent the missing status of observations in \(\), where \(m_{ij}=1\) in \(\) denotes that \(x_{ij}\) is complete, otherwise \(x_{ij}=0\). In our context, we refer to the imputation target as \(}\). During the training of the imputation model, we choose some observations as the imputation target. When we impute missing values, we treat all the missing values as the imputation target.

In Figure 1, it is evident that the main obstacle to improving multivariate time series imputation is the residual term. Considering a recognized fact that the residual term often contains high-frequency components from the perspective of Fourier analysis [46; 58; 26], introducing prior knowledge of frequency-domain information can be a feasible approach to enhancing model performance.

As a class of superior data imputation models, deep generative models treat the time series imputation task as calculating the conditional imputation target probability distribution \(q(}^{0}|)\), where \(q(}^{0})\) is the clean data distribution and existing deep generative imputation models [56; 29; 44] use the observed values \(\) in the time-domain as the condition \(\) for probability distribution calculation. Note that we denote the complete or the imputed imputation target as the clean imputation target \(}^{0}\).

However, frequency principal [38; 43] reveals that deep models cannot generalize well to high-frequency information. As a result, it may not accurately impute the residual term  inherent in the time series dataset that cannot be trivialized in the imputation task .

To tackle the above challenge, we incorporate frequency-domain information into condition \(\) to enhance the performance of generative models. Our FGTI implements the condition \(\) that contains time-domain observation condition \(^{}\), as well as the frequency-domain conditions \(^{}\) and \(^{}\).

### Frequency-domain Condition Filter

Our frequency-domain condition includes the nonlinear transformation of two parts: the high-frequency condition that guides the residual term and the dominant-frequency condition that contains background structure information to impute the trend and seasonal terms, as shown in Figure 2.

#### 3.1.1 High-frequency Filter

The high-frequency filter extracts high-frequency information \(}\) from the time-domain observations, which is used to guide the imputation of residual terms in time series. Since different attributes in a multivariate time series can be heterogeneous, we consider extracting high-frequency information separately for each attribute series \(_{d}^{L}\), where \(d=1, D\).

We first interpolate \(_{d}\), then obtain the amplitude vector \(^{(L+1)/2}\) for \(_{d}\) over sample frequency components \(=\{,,(L+1)/2\}\) by the Fast Fourier Transform (FFT):

\[(,)=(_{d}).\] (1)

To get the high-frequency condition, we discard the frequency components below a cutoff threshold \(\) and map the remaining components to time-domain by the Inverse Fast Fourier Transform (IFFT),

\[_{d}^{}=[(> )],\] (2)

where \(\) denotes the Hadamard product.

Finally, we concatenate the corresponding high-frequency information vectors \(_{d}^{}\) for each attribute sequence to form the high-frequency condition \(}^{D L}\),

\[}=(\{_{d}^{}\}_{d=1}^{D} ).\] (3)

For the whole input time series \(\), the time complexity of performing the high-frequency filtering is \((DL L)\). Since the time complexity of performing FFT for each attribute series is \((L L)\), selecting high-frequency components in the frequency domain has a time complexity of \((L)\), and performing IFFT costs \((L L)\) time.

#### 3.1.2 Dominant-frequency Filter

The conditions extracted by the dominant-frequency filter from the time-domain observations not only provide the background structure information for generative models to guide the imputation of the trend and seasonal terms, but also mitigate the interference of the high-frequency condition on the imputation of the trend and seasonal terms3.

The dominant-frequency information is mainly composed of frequency components with large amplitudes. If we have obtained the representation \((,)\) of \(_{d}\) in the frequency-domain from Equation 1, we can find the top-\(\) frequency components with the largest amplitude according to \(\),

\[\{f_{1},,f_{}\}=_{}\;( ).\] (4)

Figure 2: The high-frequency filter with \(=0.3\) and the dominant-frequency filter with \(=3\).

Then we project these \(\) frequencies to the time-domain via the Inverse Fast Fourier Transform,

\[_{d}^{}=[(\{f_{1 },,f_{}\})].\] (5)

Finally, we compose the dominant-frequency condition \(^{}^{D L}\) by concatenating all \(_{d}^{}\),

\[^{}=(\{_{d}^{}\}_{d=1}^{D}).\] (6)

Similar to the high-frequency filter, the time complexity of performing dominant-frequency filtering for the whole input \(\) is \((DL L)\).

### Cross-domain Representation Learning

With the aim of integrating frequency-domain conditions into deep generative models, we first use an encoder to map the conditions to representation \(^{}^{D L K}\) in the latent space,

\[^{}=[(^{ },^{})],\] (7)

where \(K\) is the channel number of the latent space. In this paper, we implement the \(()\) with the well-acknowledged transformer  backbone, since it can self-adaptively extract critical information in \(^{}\) and \(^{}\) by the self-attention mechanism.

To accurately capture time and attribute dependencies guided by frequency-domain information, we design two frameworks: Time-frequency representation learning and Attribute-frequency representation learning. They integrate the current intermediate hidden representations \(^{in}^{D L K}\) in the time-domain of deep generative models with the frequency-domain representation \(^{}\).

Specifically, we use the cross-attention mechanism that can efficiently learn the various input modalities [21; 40] for representation fusion.

#### 3.2.1 Time-frequency Representation Learning

To capture time dependencies with the aid of frequency-domain information, we divide input hidden representation \(^{in}=\{_{d}^{in}^{L K}\}_{d=1}^{D}\) and frequency information \(^{}=\{_{d}^{}^{L K }\}_{d=1}^{D}\) into \(D\) segments according to attributes. For each pair of latent representation segment \((_{d}^{in},_{d}^{})\)

Figure 3: The pipeline of FGTI implemented by the frequency-aware diffusion model. FGTI incorporates high-frequency representations to guide the residual term and compensates for the trend and seasonal terms with the dominant-frequency representations. With cross-domain representation learning, our FGTI includes frequency-domain information into time and attribute dependencies modeling to estimate the diffusion noise.

the learning process to obtain time-frequency representation of each attribute \(_{d}^{}^{L K}\) is

\[_{d}^{}=(_{d} _{d}{}^{}}{})_{d},\] (8)

where \(_{d}=_{d}^{}_{}^{ }\), \(_{d}=_{d}^{}_{}^{ }\), \(_{d}=_{d}^{in}_{}^{}\), \(_{}^{}\), \(_{}^{}\), \(_{}^{}^{K K}\) are learnable weight matrices.

Then we concatenate \(_{l}^{}\) of all attributes to obtain the representation \(^{}^{D L K}\),

\[^{}=(\{_{d}^{}\}_{d=1}^{D}).\] (9)

#### 3.2.2 Attribute-frequency Representation Learning

To capture dependencies between different attributes based on frequency-domain information, we divide the latent time-frequency representation \(^{}=\{_{l}^{}\!\!^{D  K}\}_{l=1}^{L}\) and \(^{}=\{_{l}^{}\!\!^{D  K}\}_{l=1}^{L}\) into \(L\) segments, according to timestamps. The learning process to obtain attribute-frequency representation of each timestamp \(_{l}^{}^{L K}\) is as follows:

\[_{l}^{}=(_{l}_{l}{}^{}}{})_{l},\] (10)

where \(_{l}=_{l}^{}_{}^{ }\), \(_{l}=_{l}^{}_{}^{ }\), \(_{l}=_{l}^{tf}_{}^{}\). To get the updated representation \(^{}\), we need to concatenate all \(_{l}^{}\) according to timestamps,

\[^{}=(\{_{l}^{}\}_{ l=1}^{L}).\] (11)

### Frequency-aware Diffusion Model

Recently, diffusion generative models have demonstrated remarkable proficiency and have emerged as the leading generative models in numerous fields [24; 18]. Thus, we take the diffusion model as an example to introduce how to use frequency-domain conditions to boost missing data imputation.

Specifically, we implement FGTI by the frequency-aware diffusion model. Our frequency-aware diffusion model fuses with frequency-domain conditions to learn the conditional imputation target distribution \(q(}^{0},^{},^{ })\), through two Markov chain processes of diffusion step \(T\), i.e., the diffusion forward process and the diffusion reverse process.

The diffusion forward process involves gradually adding Gaussian noise into the imputation target,

\[q(}^{1:T}}^{0})=_{t=1}^{T}q(}^{t}}^{t-1}),\] (12)

where \(q(}^{t}}^{t-1})=(}}^{t-1},^{t})\), \(^{t}(0,1)\), is a hyperparameter satisfying \(^{t}<^{t+1}\) for \(t=1,,T-1\). In addition, \(^{t}=1-^{t}\) and \(q(}^{0})\) is the complete data distribution.

According to DDPM , \(}^{t}\) has a closed-form solution,

\[}^{t}=}}}^{0}+}},\] (13)

where \((,)\), \(}=_{i=1}^{t}^{i}\). Therefore, we can directly obtain \(}^{t}\) from \(}^{0}\). The details for deriving the closed-form solution can be found in Appendix A.2.1. Note that when \(T\) is large enough, \(q(}^{T}}^{0}) q(}^{T} ),q(}^{T})(,)\).

Our diffusion reverse process gradually removes Gaussian noises added to the imputation target based on the time-domain observation condition \(^{}\), the high-frequency condition \(^{}\) and the dominant-frequency condition \(^{}\), which can be formalized as the Markov chain,

\[p_{}(}^{T-1:0}}^{T})=_{t=1}^{T}p_{ }(}^{t-1}}^{t},^{},^{},^{}),\] (14)where \(p_{}(}^{t-1}}^{t},^{}, ^{},^{})=(_{} [}^{t-1}}^{t},^{}, ^{},^{}],[^{t-1} ]^{2})\).

We next show that introducing the high-frequency condition \(^{}\) and dominant-frequency condition \(^{}\) can reduce the uncertainty of the diffusion reverse process, improving the imputation accuracy.

**Proposition 3.1**.: _The conditional entropy_

\[(}^{t-1}}^{t},^{ },^{},^{})< (}^{t-1}}^{t},^{} ),\]

_with additional high-frequency condition \(^{}\) and dominant-frequency condition \(^{}\) in the diffusion reverse process._

It can be proved by the chain rule of the conditional entropy, as detailed in Appendix A.1.

Based on the classifier-free guidance diffusion model [19; 20], \(p_{}(}^{t-1}}^{t},^{},^{},^{})\) can be parameterized as \(_{}[}^{t-1}}^{t},^{ },^{},^{}]=}}[}^{t}-}{}}_{}(t,}^{t},^{}, ^{},^{})]\), \([^{t-1}]^{2}=})^{t}}{1- }}\), where \(_{}()\) is the denoising network with learnable parameter set \(\) as present in Appendix A.3.1. The mathematical details are presented in Appendix A.2.2.

As shown in Figure 3, the denoising network incorporates frequency-domain information into modeling time dependencies and attribute dependencies, through the time-frequency representation learning module and attribute-frequency representation learning module to guide the denoising.

For training the denoising network, we randomly select some observed values as the imputation target \(}\) and use the remaining observations as the observation condition \(^{}\) for each update step, since the ground truth of missing values is unknown. We train the denoising network by minimizing the following objective function \(_{}\),

\[_{}=\|-_{}(t,}^{t},^{},^{},^{ })\|^{2},\] (15)

where \(t\{1,,T\},}^{0} q(}^{0}),(,)\).

For data imputation, we treat all missing values as the imputation target and all the observed values as the observation condition, i.e., \(}=(1-)\), \(^{}=\). We start from \(}^{T}(,)\) and perform the \(T\)-step diffusion reverse process following Equation 14, to obtain final imputation values \(}^{0}\). Please see the detailed training and imputation algorithms in Appendix A.3.2.

## 4 Experiment

This section experimentally evaluates both the imputation effectiveness and the improvement of real downstream applications for our FGTI, against various competing methods. All experiments are performed on a machine with Intel Core 3.0GHz i9 CPU, NVIDIA GeForce RTX 3090 24GB GPU, and 64GB RAM. The source code and datasets are available online .

### Experimental Setup

#### 4.1.1 Datasets

We employ three real time series datasets with real-world missing values. **KDD** collects 8,034 meteorological and air quality readings of nine stations from January 30, 2017 to January 31, 2018 in Beijing, with 4.46% real missing values. This dataset is collected every one hour and eleven sensor readings are recorded at each station. **Guangzhou** records traffic speeds per ten minutes on 214 anonymous roads in Guangzhou from August 1, 2016 to September 30, 2016. There are 1.29% real missing values in the dataset. **PhysioNet** contains 37 measurement readings from 11,988 patients within 48 hours of the ICU admission. 79.71% measurements are missing in the dataset, and 1,707 patients died after 48 hours of the ICU admission. Following existing studies [29; 30], since the ground truth is unavailable, we ignore these missing values when evaluating the imputation accuracy in comparative experiments and model analysis, but consider them in the application study.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

resource consumption of FGTI, implemented based on the diffusion model, is slightly higher than that of CSDI, due to the inclusion of high-frequency information and dominant-frequency information. However, since FGTI can achieve better imputation results than other methods, as shown in Table 1, we argue that it is acceptable to incur such an extra resource consumption.

### Application Study

To validate the effectiveness of applying imputation in real-world downstream applications, we consider air quality prediction and mortality forecasting tasks. For the air quality prediction application, we first impute real-world missing data in the KDD dataset by various imputation methods. Then, we analyze the records in the previous twelve hours and use the AdaBoost regressor  to estimate the average PM2.5 concentration for the upcoming six hours. Figure 6(a) shows that our method achieves the highest improvement in the air quality prediction task. Figure 6(b) reports the mortality forecast performance over the PhysioNet dataset. We train the MLP classifier  to forecast the mortality on the data without/with imputation. As shown, FGTI achieves the best performance again, which verifies the applicability of our work. Notably, various imputation methods provide a noteworthy and favorable impact on the forecast task, which demonstrates the necessity of imputation.

## 5 Conclusion

In this paper, we study imputing incomplete multivariate time series data, through reducing the imputation error in the residual term. By effectively incorporating frequency-domain insights into the generative framework, our FGTI surpasses existing models by capturing high-frequency information and dominant-frequency information. The introduced cross-domain representation learning frameworks further enhance its capability to handle time and attribute dependencies. Comprehensive experimental evaluations over real-world incomplete datasets demonstrate the superiority of FGTI in both the imputation accuracy and the improvement of downstream applications.

#### Acknowledgements

This work is supported in part by the Fundamental Research Funds for the Central Universities, Nankai University (63231147), the National Natural Science Foundation of China (62302241, 62306085, 62372252, 72342017), Shenzhen College Stability Support Plan (GXWD20231130151329002).

Figure 5: Resource consumption over KDD dataset with 10% missing values

Figure 6: Application results of air quality prediction over KDD dataset and mortality forecast over Physionet dataset