# Piecewise deterministic generative models

Andrea Bertazzi\({}^{1,*}\), Dario Shariatian\({}^{2}\)

**Umut Simsekli\({}^{2}\), Eric Moulines\({}^{1,3}\), Alain Durmus\({}^{1}\)**

\({}^{1}\) Ecole Polytechnique, Institut Polytechnique de Paris

\({}^{2}\) INRIA, CNRS, Ecole Normale Superieure, PSL Research University

\({}^{3}\) MBZUAI

\({}^{*}\) andrea.bertazzi@polytechnique.edu

###### Abstract

We introduce a novel class of generative models based on piecewise deterministic Markov processes (PDMPs), a family of non-diffusive stochastic processes consisting of deterministic motion and random jumps at random times. Similarly to diffusions, such Markov processes admit time reversals that turn out to be PDMPs as well. We apply this observation to three PDMPs considered in the literature: the Zig-Zag process, Bouncy Particle Sampler, and Randomised Hamiltonian Monte Carlo. For these three particular instances, we show that the jump rates and kernels of the corresponding time reversals admit explicit expressions depending on some conditional densities of the PDMP under consideration before and after a jump. Based on these results, we propose efficient training procedures to learn these characteristics and consider methods to approximately simulate the reverse process. Finally, we provide bounds in the total variation distance between the data distribution and the resulting distribution of our model in the case where the base distribution is the standard \(d\)-dimensional Gaussian distribution. Promising numerical simulations support further investigations into this class of models.

## 1 Introduction

Diffusion-based generative models (Ho et al., 2020; Song et al., 2021) have recently achieved state-of-the-art performance in various fields of application (Dhariwal and Nichol, 2021; Croitoru et al., 2023; Jeong et al., 2021; Kong et al., 2021). In their continuous time interpretation (Song et al., 2021), these models leverage the idea that a diffusion process can bridge the data distribution \(_{}\) to a base distribution \(\), and its time reversal can transform samples from \(\) into synthetic data from \(_{}\). Anderson (1982) showed that the time reversal of a diffusion process, i.e., the _backward_ process, is itself a diffusion with explicit drift and covariance functions that are related to the score functions of the time-marginal densities of the original, _forward_ diffusion. Consequently, the key element of these generative models is learning these score functions using techniques such as (denoising) score-matching (Hyvarinen, 2005; Vincent, 2011).

In this work we propose a new family of generative models which use piecewise deterministic Markov processes (PDMPs) as noising processes instead of diffusions. PDMPs were introduced around forty years ago (Davis, 1984, 1993) and since then have been successfully applied in various fields, including communication networks (Dumas et al., 2002), biology (Berg and Brown, 1972; Cloez, Bertrand et al., 2017), risk theory (Embrechts and Schmidli, 1994), and the reliability of complex systems (Zhang et al., 2008). More recently, PDMPs have been intensively studied in the context of Monte Carlo algorithms (Fearnhead et al., 2018) as alternatives to Langevin diffusion-based methods and Metropolis-Hastings mechanisms. This renewed interest in PDMPs has led to the development of novel processes, such as the Zig-Zag process (ZZP) (Bierkens et al., 2019), the Bouncy Particle Sampler (BPS) (Bouchard-Cote et al., 2018), and the Randomised HamiltonianMonte Carlo (RHMC) (Bou-Rabee and Sanz-Serna, 2017). PDMPs offer several advantages compared to Langevin-based methods, such as better scalability and reduced computational complexity in high-dimensional settings (Bierkens et al., 2019). In the context of generative modelling PDMPs offer several potential advantages over diffusion processes. A key strength is their ability to effectively model data distributions supported on constrained or restricted domains. By adjusting their deterministic dynamics, PDMPs can easily incorporate boundary behaviour, making them straightforward to implement in such settings (Bierkens et al., 2018; Davis, 1993). Similarly, PDMPs can model data on Riemannian manifolds by employing flows that respect the manifold's geometry (see, e.g., Yang et al. (2022) for a PDMP on the sphere). Moreover, PDMPs are well-suited for modelling data distributions that combine a continuous density and a positive mass on a lower dimensional manifold (Bierkens et al., 2022).

Our contributions are the following:

1) Leveraging the existing literature on time reversals of Markov jump processes (Conforti and Leonard, 2022), we characterise the time reversal of any PDMP under appropriate conditions. It turns out that this time reversal is itself a PDMP with characteristics related to the original PDMP; see Proposition 1.

2) We further specify the characteristics of the time-reversal processes associated with the three aforementioned PDMPs: ZZP, BPS, and RHMC. For these processes, Proposition 2 shows the corresponding time-reversals are PDMPs with simple reversed deterministic motion and with jump rates and kernels that depend on (ratios of) conditional densities of the velocity of the forward process before and after a jump. In contrast to common diffusion models, the emphasis is on distributions of the velocity, similar to the case of the underdamped Langevin diffusion (Dockhorn et al., 2022), which includes an additional velocity vector akin to the PDMPs we consider. Moreover, the structure of the backward jump rates and kernels closely connects to the case of continuous time jump processes on discrete state spaces (Sun et al., 2023; Lou et al., 2024).

3) We define our _piecewise deterministic generative models_ employing either ZZP, BPS, or RHMC as forward process, transforming data points to a noise distribution of choice, and develop methodologies to estimate the backward rates and kernels. Then, we define the corresponding backward process based on approximations of the time reversed ZZP, BPS, and RHMC obtained with the estimated rates and kernels. In Section 4 we test our models on simple toy distributions.

4) We obtain a bound for the total variation distance between the data distribution and the distribution of our generative models taking into account two sources of error: first, the approximation of the characteristics of the backward PDMP, and second, its initialisation from the limiting distribution of the forward process; see Theorem 1.

## 2 PDMP based generative models

### Piecewise deterministic Markov processes

Informally, a PDMP (Davis, 1984, 1993) on the measurable space \((^{D},(^{D}))\) is a stochastic process that follows deterministic dynamics between random times, while at these times the process can evolve stochastically on the basis of a Markov kernel. In order to define a PDMP precisely, we need three components, called _characteristics_ of the PDMP: a _vector field_\(:_{+}^{D}^{D}\), which governs the deterministic motion, a _jump rate_\(:_{+}^{D}_{+}\), which defines the law of random event times, and finally a _jump kernel_\(Q:_{+}^{D}(^{D}) \), which is applied at event times and defines the new state of the process. Let us give an informal description of the evolution of a PDMP \(Z_{t}\), clarifying the role of the three characteristics. Suppose at time \(_{+}\) the PDMP is at state \(z^{D}\), that is \(Z_{}=z\). The deterministic motion of the PDMP is described by the ODE \(Z_{+s}=(+s,Z_{+s})s\) for \(s 0\), with initial condition \(Z_{}=z\). We introduce the differential flow \(:(t,s,z)_{t,t+s}(z)\), which solves the ODE in the sense that \(_{t,t+s}(z)=(+s,_{t,t+s}(z))s\) for \(s 0\). The process evolves deterministically according to \(\) until the next event time \(+\), where \(\) is a random variable with law \((>s|Z_{}=z)=(-_{0}^{s}(_{,+u}(z))u)\), i.e. the exponential distribution with non-homogeneous rate \(s(_{,+s}(z))\). We can, at least in principle, simulate \(\) by solving

\[=\{t>0:_{0}^{t}(+u,_{, +u}(z))u E\}\] (1)where \(E(1)\). The process is then defined on \([,+)\) by \(Z_{+t}=_{,+t}(Z_{})\) for \(t[0,)\). At time \(+\) the process jumps to a new state that is drawn from the Markov kernel \(Q\), hence we set \(Z_{+} Q(+,_{,+ }(z),)\). A realisation of the path of a PDMP for a given time horizon can then be obtained following this procedure (see also Algorithm 1 in Appendix C.1 for a pseudo-code). The formal construction of a PDMP can be found in Appendix A.1.

Typically a PDMP has several types of jumps, which can be represented by a family of jump rates and kernels \((_{i},Q_{i})_{i\{1,,t\}}\). A PDMP of such type can be obtained with the construction we have described by setting

\[(t,z)=_{i=1}^{}_{i}(t,z)\, Q(t,z,z^{ })=_{i=1}^{}(t,z)}{(t,z)}Q_{i}(t,z, z^{})\.\] (2)

An alternative, equivalent construction of a PDMP with \(,Q\) satisfying (2) is given in Appendix A.2. Finally, we say a PDMP is homogeneous (as opposed to the non-homogeneous case we have described) when the characteristics do not depend on time, that is \(:^{D}^{D}\), \(:^{D}_{+}\), and \(Q:^{D}(^{D})\). In all this work, we suppose that the PDMPs that we consider are non-explosive in the sense of Davis (1993), that is they are such that the time of the \(n\)-th random event goes to \(+\) as \(n+\), almost surely (see Durmus et al. (2021) for conditions ensuring this).

We now introduce the three PDMPs we consider throughout the paper. All these PDMPs are time-homogeneous and live on a state space of the form \(=^{d}\), for \(^{d}\), assuming \(V_{0}\). Then, \(Z_{t}\) can be decomposed as \(Z_{t}=(X_{t},V_{t})\), where \(X_{t}^{d}\) is the component of interest and has the interpretation of the position of a particle, whereas \(V_{t}\) is an auxiliary vector playing the role of the particle's velocity. In the sequel, if there is no risk of confusion, we take the convention that any \(z^{d}\), and we write \(z=(x,v)\) for \(x^{d}\) and \(v\). All the PDMPs below have a stationary distribution of the form \((x)(v)\), where \(\) has density proportional to \(x^{-(x)}\), for \(:^{d}\) a continuously differential potential, and \(\) is a simple distribution on \(\) for the velocity vector. In our experiments we take \(\) to be the standard normal distribution, while \(\) is the standard normal when \(=^{d}\) or the uniform distribution when \(\) is a compact set. Figure 1 shows sample paths for the position vector of the three PDMPs we introduce below.

The Zig-Zag processThe Zig-Zag process (ZZP (Bierkens et al., 2019) is a PDMP with state space \(^{}=^{d}\{-1,1\}^{d}\). The deterministic motion is determined by the homogeneous vector field \(^{}(x,v)=(v,0)^{}\), i.e. the particle moves with constant velocity \(v\). For \(i\{1,,d\}\) we define the jump rates \(_{i}^{}(x,v):=(v_{i}_{i}(x))_{+}+_{r}\), where \((a)_{+}=(0,a)\), \(_{i}\) denotes the \(i\)-th partial derivative, and \(_{r} 0\) is a user chosen refreshment rate. The corresponding (deterministic) jump kernels are given by \(Q_{i}^{Z}((x,v),(y,w))=_{(x,_{i}^{Z}v)}( y,w)\), where \(_{z}\) denotes the Dirac measure at \(z\). Here, \(_{i}^{Z}\) is the operator that reverses the sign of the \(i\)-th component of the vector to which it is applied, i.e. \(_{i}^{Z}v=(v_{1},v_{i-1},-v_{i},v_{i+1},,v_{d})\). The ZZP falls within our definition of PDMP taking \(,Q\) as in (2). As shown in Bierkens et al. (2019), the ZZP has invariant distribution \(\), where \(\) is the uniform distribution over \(\{ 1\}^{d}\). Moreover, Bierkens et al. (2019) shows that for any \(_{r} 0\) the law of the ZZP converges exponentially fast to its invariant distribution e.g. when \(\) is a standard normal distribution.

The Bouncy Particle samplerThe Bouncy Particle sampler (BPS) (Bouchard-Cote et al., 2018) is a PDMP with state space is \(^{}=^{d}^{}\), where \(^{}=^{d}\) or \(^{}=^{d-1}:=\{v^{d}\,:\,\|v\|=1\}\).

Figure 1: Trace plots for ZZP (left), BPS (centre), RHMC (right). In all cases \(_{r}=1\) and \(_{f}=10\).

The deterministic motion is governed as ZZP by the homogeneous vector field defined for \(z=(x,v)\) by \(^{}(x,v)=(v,0)^{}\). Now we introduce two jump rates which correspond to two types of random events: _reflections_ and _refreshments_. Reflections enforce that \((x,v)=(x)(v)\) is the invariant density of the process, where \((x)(-(x))(x)\) is a given distribution and \(\) is either a standard normal distribution when \(^{}=^{d}\) or the uniform distribution on \(^{d-1}\) when \(^{}=^{d-1}\). Reflections are associated to the homogeneous jump rate \((x,v)^{}_{1}(x,v)= v,(x)_{+}\), while refreshments are associated to \((x,v)^{}_{2}(x,v)=_{r}\) for \(_{r}>0\). The corresponding jump kernels are \(Q^{}_{2}((x,v),(y,w))=_{(x,^{ }_{2})}(y,w)\), \(Q^{}_{2}((x,v),(y,w))=_{x}(y) (w)\), where \(^{}_{x}v=v-2( v,(x)/( x)|^{2})(x)\). The operator \(^{}_{x}\)_reflects_ the velocity \(v\) of the hyperplane that is tangent to the contour line of \(\) passing though point \(x\). The norm of the velocity is unchanged by the application of \(^{}\), and this gives the interpretation that \(^{}\) is an elastic collision of the particle off such hyperplane. As observed in Bouchard-Cote et al. (2018), BPS requires a strictly positive \(_{r}\) to avoid being reducible, that is to make sure the process can reach any area of the state space. Exponential convergence of the BPS to its invariant distribution was shown by Deligiannidis et al. (2019), Durmus et al. (2020).

Randomised Hamiltonian Monte CarloRandomised Hamiltonian Monte Carlo (RHMC) (Bouchaee and Sanz-Serna, 2017) refers to the PDMP with state space \(^{}=^{d}^{d}\) which is characterised by Hamiltonian deterministic flow and refreshments of the velocity vector from the standard normal distribution. The flow is governed by the homogeneous vector field defined by \((x,v)^{}(x,v)=(v,-(x))^{}\), where \(\) is the potential of \(\). The jump rate coincides with the refreshment part of BPS, _i.e._, it is the constant function \(^{}:(x,v)_{r}>0\) and jump kernel \(Q^{}((x,v),(y,w))=_{x}(y)( w)\). When the stationary distribution \(\) is a standard Gaussian, the deterministic dynamics \((x_{t},v_{t})_{t 0}\) satisfy \(x_{t}=v_{t}t\), \(v_{t}=-x_{t}t\), which for \(t 0\) has solution \(x_{t}=x_{0}(t)+v_{0}(t)\) and \(v_{t}=-x_{0}(t)+v_{0}(t)\), where \((x_{0},v_{0})\) is the initial condition. It is well known that Hamiltonian dynamics preserve the density \((x,v)=(x)(v)\)(Neal, 2010), where \(\) is the standard normal distribution, while velocity refreshments are necessary to ensure the process is irreducible. Exponential convergence of the law of this PDMP to \(\) was shown in Bou-Rabee and Sanz-Serna (2017).

**Remark 1** (Noise schedule): _Similarly to diffusion models we can introduce a noise schedule \((t)\) that regulates the amount of randomness injected at time \(t\). This can be achieved using the time change of a given PDMP with characteristics \((,,Q)\) as forward process, resulting in the PDMP with characteristics \((_{},_{},Q)\) for \(_{}(t,z)=(t)(t,z)\) and \(_{}(t,z)=(t)(t,z)\)._

### Time reversal of PDMPs

In this section we characterise the time reversal of a PDMP. This key result, stated in Proposition 1, is essential to be able to use PDMPs for generative modelling. The _time reversal_ of a PDMP \((Z_{t})_{t[0,_{f}]}\) with initial distribution \(_{0}\) is the process that at time \(t[0,_{f}]\) has distribution \(_{0}P_{_{f}-t}\), where \(_{0}P_{s}\) denotes the law of \(Z_{s}\). It follows that the law of the time reversal at time \(_{f}\) is \(_{0}\), which is the key observation in the context of generative modelling. Characterisations of the law of time reversed Markov processes with jumps were obtained in Conforti and Leonard (2022) and in the following statement we adapt their Theorem 5.7 to our setting, showing that the time reversal of a PDMP with characteristics \((,,Q)\) is a PDMP with reversed deterministic motion and jump rates and kernels satisfying (3).

**Proposition 1**: _Consider a non-explosive PDMP \((Z_{t})_{t 0}\) with characteristics \((,,Q)\) and initial distribution \(_{0}\) on \(^{D}\). In addition, let \(_{f}\) be a time horizon. Suppose that \(\) is locally bounded, \((t,z)(t,z)\) is continuous in both its variables, and \(_{0}^{_{f}}[(t,Z_{t})]t<\). Assume the technical conditions **H**3, **H**4, postponed to the appendix. Then, the corresponding time reversal process is a PDMP with characteristics \((,,)\), where \((t,z)=-(_{f}-t,z)\) and \(,\) are the unique solutions to the following balance equation: for almost all \(t[0,_{f}]\),_

\[_{0}P_{_{f}-t}(y)(t,y) (t,y,z)=_{0}P_{_{f}-t}(z) (_{f}-t,z)Q(_{f}-t,z,y)\,\] (3)

_where \(_{0}P_{t}\) stands for the distribution of \(Z_{t}\) starting from \(_{0}\)._

The proof is postponed to Appendix A.4. The condition **H**3 is standard in the literature on PDMPs (Davis, 1993) and is verified for ZZP, BPS, and RHMC. **H**4 is a technical assumption on the do main of the generator of the forward PDMP and has been shown to hold e.g. for the ZZP. In the next proposition we derive expressions for the backward jump rate and kernel satisfying (3) corresponding to a forward PDMP with characteristics with the same structure as those of ZZP, BPS, and RHMC. We state the result assuming the PDMP has only one jump type, but the generalisation to the case of \(>1\) jump mechanisms of the form (2) can be immediately obtained applying Proposition 2 to each pair \((_{i},Q_{i})\) for \(i\{1,,\}\). We refer to Appendix A.6 for the details.

**Proposition 2**: _Consider a non-explosive PDMP \((X_{t},V_{t})_{t 0}\) with characteristics \((,,Q)\) and initial distribution \(_{0}^{X}_{0}^{V}\) on \(^{2d}\). In addition, let \(_{f}\) be a time horizon. Suppose that \(\) and \(\) satisfy the same conditions as Proposition 1, in particular the technical conditions **H3**, **H4** postponed to the appendix. Suppose in addition that for any \(t(0,_{f}]\), the conditional distribution of \(V_{t}\) given \(X_{t}\) has a transition density \((x,v) p_{t}(v|x)\) with respect to some reference measure \(_{}^{V}\) on \(^{d}\)._

_(1)_ (Deterministic jumps)_. Suppose \(Q((y,w),(x,v))=_{y}(x)_{_ {y}w}(v)\) where for any \(y^{d}\), \(_{y}:^{d}^{d}\) is an involution which preserves \(_{}^{V}\), i.e., \(_{y}^{-1}=_{y}\) and \(_{}^{V}(_{y}w)=_{}^{V}( w)\). Then for almost all \(t[0,_{f}]\) and any \((y,w)^{2d}\) such that \(p_{_{f}-t}(w|y)>0\) it holds that_

\[(t,(y,w))=_{f}-t}(_{y}w|y )}{p_{_{f}-t}(w|y)}(_{f}-t,(y,_{y}w))\, ((y,w),(x,v))=_{y}(x) _{_{y}w}(v)\.\]

_(2)_ (Refreshments)_. Suppose \(Q((y,w),(x,v))=_{y}(x)(v|y)\), where \(\) is a transition kernel on \(^{d}(^{d})\), and \((t,(y,w))=(t,y)\). Suppose also for any \(y^{d}\), \((|y)\) is absolutely continuous with respect to \(_{}^{V}\). Then for almost all \(t[0,_{f}]\) and any \((y,w)^{2d}\) such that \(p_{_{f}-t}(w|y)>0\) it holds that_

\[(t,(y,w))=/_{}^{V})(w|y)}{p_{_{f}-t}(w|y)}(_{f}-t,y), (t,(y,w),(x,v))=_{y}(x)p_{_{f}-t} (v|x)_{}^{V}(v).\]

The proof is postponed to Appendix A.5. We remark that we consider that \(_{0}^{V}\) is a distribution on \(^{d}\) also when \(_{0}^{V}()=1\) for \(^{d}\), in which case the reference measure can simply be chosen such that \(_{}^{V}()=1\). Applying Proposition 2 we are able to derive explicit expressions for the characteristics of the time reversals of ZZP, RHMC, and BPS. The rigorous statements and their proofs can be found in Appendix A.7. For ZZP and BPS we assume the following condition on \(\), the limiting distribution for the position vector of the forward process.

**H1**: _Recall \((x) e^{-(x)}\). It holds that \(^{2}(^{d})\) and \(_{x^{d}}\|^{2}(x)\|<+\)._

This assumption is satisfied e.g. by any multivariate normal distribution. For BPS and RHMC we suppose that for any \(t(0,_{f}]\), the conditional distribution of \(V_{t}\) given \(X_{t}\) has a transition density \((x,v) p_{t}(v|x)\) with respect to the Lebesgue measure. Moreover, for all samplers we assume **H4**.

Time reversal of ZZPIn order to apply Proposition 2 we additionally assume that \(_{i}(x)_{}(x)<\) for all \(i=1,,d\). We find that the deterministic motion is defined by \(^{2}(y,w)=(-w,0)^{}\) for any \((y,w)^{2d}\), while the backward rates and kernels are for \(i=1,,d\) and for all \((y,w)^{2d}\) such that \(p_{_{f}-t}(w|y)>0\),

\[_{i}^{}(t,(y,w))=_{f}-t}( _{i}^{2}w|y)}{p_{_{f}-t}(w|y)}_{i}^{}(y,_{i}^{}w)\,_{i}^{}((y,w),(x,v))= _{(y,_{i}^{}w)}(x,v)\.\] (4)

Time reversal of BPSWhereas in Appendix A.7 we consider the case where the velocity of BPS is initialised on \(^{d-1}\), we can formally apply Proposition 2 to the case of \(\) is the standard \(d\)-dimensional Gaussian distribution assuming that \((x)_{}(x)<\). The drift of the backward BPS is clearly the same as for the backward ZZP, while jump rates and kernels are for all \(t[0,_{f}]\) and \((y,w)^{2d}\) such that \(p_{_{f}-t}(w|y)>0\)

\[_{1}^{}(t,(y,w))=_{f}-t}( _{y}^{}w|y)}{p_{_{f}-t}(w|y)}_{1}^{ }(y,_{y}^{}w),_{1}^{ }((y,w),(x,v))=_{(y,_{y}^{ }w)}(x,v)\,\]\[_{2}^{ B}(t,(y,w))=_{r}_{f}-t}(w|y)}\;, _{2}^{ B}(t,(y,w),({ d}x,{ d}v))=p_{{ T}_{f}-t}(v |y)_{y}({ d}x){ d}v\;.\] (5)

**Time reversal of RHMC.** The deterministic motion of the backward RHMC follows the system of ODEs \(^{ H}(x,v)=(-v,(x))^{ T}\), which, when the limiting distribution \(\) is Gaussian, has solution \(x_{t}=x_{0}(t)-v_{0}(t)\) and \(v_{t}=x_{0}(t)+v_{0}(t)\). The backward refreshment rate and kernel coincide with those of BPS as given in (5).

**Remark 2** (Variance exploding PDMPs): _Similarly to the case of diffusion models (Song et al., 2021), we can define variance exploding PDMPs choosing \((x)=0\) for all \(x^{d}\), that is when \(({ d}x)\) is the Lebesgue measure. In this case, the deterministic motion of RHMC coincides with ZZP and BPS, and all three processes have only velocity refreshment events._

### Approximating the characteristics of time reversals of PDMPs

In Section 2.2 we showed that the backward jump rates and kernels of ZZP, BPS, and RHMC, involve the conditional densities of the velocity vector of the forward process given its position vector at all times \(t[0,{ T}_{f}]\). These conditional densities are unavailable in analytic form, hence in this section we develop methods to learn the jump rates and kernels of our time reversed PDMPs. In Appendix D we give the pseudo codes and more detailed descriptions of the training procedure for our models, together with a comparison with diffusion models.

Approximating the jump rates of the backward ZZP via ratio matchingIn the case of ZZP, we need to approximate for any \(i\{1,,d\}\), the rates in (4). Since the terms \(_{i}^{ Z}(x,_{i}^{ Z}v)\) are known, it is sufficient to estimate the density ratios \(r_{i}^{ Z}(x,v,t):=p_{{ T}_{f}}(_{i}^{ Z}v|x)/p_{{ T}_{ f}(v|x)}\) for all states \((x,v)\) such that \(p_{t}(v|x)>0\). To this end, we introduce a class of functions \(\{s^{g}:^{d}\{-1,1\}^{d}[0,{ T}_{f}]_{+} ^{d}\,:\,\}\) for some parameter set \(^{d_{}}\) and aim to find a parameter \(_{}\) such that for any \(i\{1,,d\}\), the \(i\)-th component of \(s^{_{}}\), denoted by \(s_{i}^{_{}}()\), is an approximation of \(r_{i}^{ Z}\). We then approximate the backward ZZP by using the rates \(_{i}^{2}(t,(x,v))=s_{i}^{_{}}(x,v,{ T}_{f}-t)\, _{i}^{Z}(x,_{i}^{ Z}v)\). To address the problem of fitting \(\), we consider different loss functions inspired by the ratio matching (RM) problem considered in Hyvarinen (2007).

From a discrete probability density \(p_{}\) on \(\{-1,1\}^{d}\), RM consists in learning the \(d\) ratios \(v p_{}(_{i}v)/p_{{}}(v)\) for \(i\{1,,d\}\). This problem was motivated in Hyvarinen (2007) as a means to estimate \(p_{}\) without requiring its normalising constant, similarly to score matching applied to estimate continuous probability densities (Hyvarinen, 2005). In our context we are interested only in the ratios, hence as opposed to Hyvarinen (2007) we do not model the conditional distributions \((x,v) p_{t}(v|x)\), but directly the ratios \(r_{t}^{ Z}\). Adapting the ideas of Hyvarinen (2007) to our context, we introduce the function \({ G}:r(1+r)^{-1}\) and define the _Explicit Ratio Matching_ objective function

\[_{ E}()=_{0}^{{ T}_{f}} { d}t\,(t)_{i=1}^{d}\{{ G}(s_{i}^{ }(X_{t},V_{t},t))-{ G}(r_{i}(X_{t},V_{t},t))\}^{2}\] (6) \[+\{{ G}(s_{i}^{}(X_{t},_{i}^{ Z}V_{t},t))- { G}(r_{i}(X_{t},_{i}^{ Z}V_{t},t))\}^{2}\;.\]

where \(:[0,{ T}_{f}]_{+}^{*}\) is a probability density, and \((X_{t},V_{t})_{t 0}\) is a ZZP initialised from \(_{}(\{-1,1\}^{d})\). This objective function considers simultaneously the square error in the estimation of both \((x,v,t) r_{i}(x,v,t)\) and \((x,v,t) r_{i}(x,_{i}^{ Z}v,t)\), where the function \({ G}\) improves numerical stability, particularly when one of the two ratios is very small. Clearly \(_{ E}()=0\) if and only if \(s_{i}^{}(x,v,t)=r_{i}(x,v,t)\) for almost all \(x,v,t\) and all \(i\). Moreover, the choice of \({ G}\) allows us to optimise without knowledge of the true ratios, as shown in the following result.

**Proposition 3**: _It holds that \(_{}_{ E}()=_{}_{ I}()\) for_

\[_{ I}()=\!\!_{0}^{{ T}_{f}}\!\!\!t\,(t) _{i=1}^{d}^{2}(s_{i}^{}(X_{t},V_{t},t))+{ G }^{2}(s_{i}^{}(X_{t},_{i}^{ Z}V_{t},t))-2{ G}(s_{i}^{ }(X_{t},V_{t},t))\;,\] (7)

_where \((X_{t},V_{t})_{t_{+}}\) is a ZZP starting from \(_{}(\{-1,1\}^{d})\)._Therefore we aim to solve the minimisation problem associated with \(_{l}\), which has for empirical counterpart

\[_{n=1}^{N}_{i=1}^{d}^{2}(s_{i}^{} (X_{^{n}}^{n},V_{^{n}}^{n},^{n}))+^{2}(s_{i}^{}(X_{ ^{n}}^{n},_{i}^{2}V_{^{n}}^{n},^{n}))-2(s_{i}^ {}(X_{^{n}}^{n},V_{^{n}}^{n},^{n}))\]

where \(\{^{n}\}_{n=1}^{N}\) are i.i.d. samples from \(\), independent of \(\{(X_{t}^{n},V_{t}^{n})_{t 0}\}_{n=1}^{N}\), which are \(N\) i.i.d. realisations of the ZZP respectively starting at the \(n\)-th training data point with velocity \(V_{0}^{n}\), where \(\{V_{0}^{n}\}_{n=1}^{N}\) are i.i.d. observations of \((\{-1,1\}^{d})\).

Notice that the loss above has computational cost increasing linearly in \(d\) because \(d+1\) evaluations of the model are needed for each datum. This can be improved considering an estimate for the ratio which does not take as input the whole velocity vector (see Appendix D.1 for the details). This variation has computational cost that is constant in the dimension, but might have lower accuracy.

Approximating the characteristics of BPS and RHMCFor BPS and RHMC, Proposition 2 shows that if we aim to sample from the backward process, we have to estimate both ratios of the conditional density of the velocity of the forward PDMP given its position at any time \(t[0,_{f}]\), and also to be able to sample from such densities as prescribed by the backward jump kernel (5). In order to address both requirements, we introduce a parametric family of conditional probability distributions \(\{p_{}:\}\) of the form \((x,v,t) p_{}(v|x,t)\), where \(^{d_{}}\), which we model with the framework of normalising flows (NFs) . The advantage of NFs lays in their feature that, once the network is learned, it is possible both to obtain an estimate of the density at a given state and time, and also to generate samples which are approximately from \((x,v,t) p_{t}(v|x)\). However, training conditional NFs can be challenging in high dimensions.

Focusing on BPS, we now illustrate how we can use NFs to learn the backward jump rates and kernels. We aim to find a parameter \(_{}^{}\) such that \(p_{^{}_{}}(v|x,t)\) is a good approximation of \(p_{t}(v|x)\), the conditional density of the forward BPS with respect to the Lebesgue measure. We choose to optimise \(\) following the maximum likelihood approach, which gives the theoretical loss

\[_{}()=-_{0}^{_{f}}t\ (t)[ p_{}(V_{t}|X_{t},t)],\] (8)

where \(:[0,_{f}]_{+}^{}\) is a probability density, and \((X_{t},V_{t})_{t 0}\) is a a BPS initialised from \(_{}\), with \(\) denoting the density of the \(d\)-dimensional standard normal distribution. The optimal parameter \(_{}^{}\) can then be found minimising the empirical counterpart of \(_{}()\):

\[_{}^{}=*{arg\,min}_{}_{ n=1}^{N} p_{}(V_{^{n}}^{n}|X_{^{n}}^{n},^{n}),\] (9)

where \(\{^{n}\}_{n=1}^{N}\) are i.i.d. samples from \(\), independent of \(\{(X_{t}^{n},V_{t}^{n})_{t 0}\}_{n=1}^{N}\), which are \(N\) i.i.d. realisations of the ZZP respectively starting at the \(n\)-th training data point with velocity \(V_{0}^{n}\), where \(\{V_{0}^{n}\}_{n=1}^{N}\) are i.i.d. observations from the multivariate standard normal distribution. Once we have obtained the optimal parameter \(_{}^{}\), we can define our approximation of the backward refreshment mechanism of BPS taking the rate \(_{2}^{}(t,(x,v))=_{r}}{{ _{_{}^{}}(v|x,_{f}-t)}}\) and the kernel \(_{2}^{}(t,(y,w),(x,v))=p_{_{}^ {}}(v|y,_{f}-t)_{y}(x)v\). Similarly, we estimate the backward reflection ratio of BPS as \(_{1}^{}(t,(x,v))=_{1}^{}(x,_{x}^{}v) p_{_{}^{}}(_{x}^{ }v|x,_{f}-t)/\!\!_{^{}}(v|x,_{f}-t)\).

### Simulating the backward process

We now discuss how we can simulate the backward PDMP with exact backward flow map \((t,x,v)_{-t}(x,v)\) and jump characteristics \(\) and \(\) that are approximations of the jump rates and kernels of the time reversed PDMPs obtained as discussed in Section 2.3. We recall that the backward rates have the general form \((t,(x,v))=s_{}(x,v,_{f}-t)(x,v)\), where \(s_{}\) is an estimate of a density ratio and \(\) is a suitable involution. In principle such a PDMP can be simulated following the procedure described in Section 2.1, but the generation of the random jump times via (1) requires the integration of \((t,_{-t}(x,v))\) with respect to \(t\). This cannot be achieved since \(\) is defined through a neural network. A standard approach in the literature (see e.g. Bertazzi et al.

) is to discretise time and (informally) approximate the integral in (1) with a finite sum. Here we focus on approximations based on splitting schemes discussed in Bertazzi et al. , adapting their ideas to the non-homogeneous case. Such splitting schemes approximate a PDMP with a Markov chain defined on the time grid \(\{t_{n}\}_{n\{0,,N\}}\), with \(t_{0}=0\) and \(t_{N}=_{f}\). The key idea is that the deterministic motion and the jump part of the PDMP are simulated separately in a suitable order, obtaining second order accuracy under suitable conditions (see Theorem 2.6 in Bertazzi et al. ). Now, we give an informal description of the splitting scheme that we use for RHMC, that is based on splitting DJD in Bertazzi et al. , where D stands for deterministic motion and J for jumps. We define our Markov chain based on the step sizes \(\{_{j}\}_{j\{1,,N\}}\), where \(_{j}=t_{j}-t_{j-1}\). Suppose we have defined the Markov chain on \(\{t_{k}\}_{k\{0,,n\}}\) for \(n<N\) and that the state at time \(t_{n}\) is \((x_{t_{n}},v_{t_{n}})\). The next state is obtained following three steps. _First_, the particle moves according to its deterministic motion for a half-step, that is we define an intermediate state \((_{t_{n}},_{t_{n}})=_{-_{n+1}/2}(x_{t_{n}},v_{ t_{n}})\). _Second_, we turn our attention to the jump part of the process. In this phase, the particle is only allowed to move through jumps and there is no deterministic motion. This means that the rate is frozen to the value \((t_{n}+_{n+1}/2,(_{t_{n}},_{t_{n}}))\) and thus the integral in (1) can be computed trivially. The proposal for the next event time is then given by \(_{n+1}((t_{n}+_{n+1}/2,( _{t_{n}},_{t_{n}})))\). If \(_{n+1}_{n+1}\), we draw \(w(t_{n}+_{n+1}/2,(_{t_{n}},_{t_{n}}), )\) and update \(_{t_{n}}=w\), else we leave \(_{t_{n}}\) unchanged. _Finally_ we conclude with an additional half-step of deterministic motion, letting \((x_{t_{n+1}},v_{t_{n+1}})=_{-_{n+1}/2}(_{t_{n}},_{t_{n}})\). We refer to Appendix C.2 for a detailed description of the schemes used for each process together with the pseudo-codes.

## 3 Error bound in total variation distance

In this section, we give a bound on the total variation distance between the data distribution \(_{}\) and the law of the synthetic data generated by a PDMP with initial distribution \(\) and approximate characteristics obtained, e.g., with the methods described in Section 2.3. We obtain our result comparing the law of such PDMP to the law of the exact time reversal obtained in Section 2.2, that is the PDMP with the analytic characteristics of Proposition 2 and with initial distribution \((X_{_{f}},V_{_{f}})\), i.e. the law of the forward PDMP at time \(_{f}\) when initialised from \(_{}\). In Theorem 1 below we then take into account two of the three sources of error in our models: (i) the error introduced initialising the backward PDMP from the limiting distribution of the forward, (ii) the error due to the approximation of the backward rates and kernels. For simplicity we neglect the discretisation error caused by the methods discussed in Section 2.4.

We shall assume the following condition, which deals with the error introduced by initialising the backward PDMP from \(\).

**H2**: _The forward PDMP with semigroup \((P_{t})_{t 0}\) is such that there exist \(,C>0\) for which_

\[\|-_{} P_{t}\|_{} Ce^{-  t}.\]

Informally, **H**2 is verified for some \(C<\) when \(\) is a multivariate standard Gaussian distribution for ZZP and BPS if the tails of \(_{}\) are at least as light as those of \(\), while for RHMC it is enough if \(_{}\) has finite second moments. We refer to Appendix E.1 for a more detailed discussion on this aspect. We are now ready to state our result.

**Theorem 1**: _Consider a non-explosive PDMP \((X_{t},V_{t})_{t 0}\) with initial distribution \(_{}\), stationary distribution \(\), and characteristics \((,,Q)\). Let \(_{f}\) be a time horizon. Suppose the assumptions of Proposition 1 as well as **H**2 hold. Let \((_{t},_{t})_{t[0,_{f}]}\) be a non-explosive PDMP initial distribution \(\) and characteristics \((,,)\), where \((t,(x,v))=(_{f}-t,(x,v))\) for all \(t[0,_{f}]\) and \((x,v)^{2d}\). Then it holds that_

\[\|_{}-(_{_{f}})\|_{}  Ce^{-_{f}}+2[1-(-_{0}^{ _{f}}g_{_{f}-t}(X_{t},V_{t})t)],\] (10)

_where_

\[g_{t}(x,v)=\;)(t,(x,v))} {2}\|(t,(x,v),)-(t,(x,v),)\|_{}+(t,(x,v))-(t,(x,v))\] (11)

_and \(,\) are as given by Proposition 1._The proof is postponed to Appendix E.2. The first term in (10) is caused by initialising the process from \(\), while the second term represents the error introduced by the approximate jump rate \(\) and kernel \(\). For the sake of illustration we obtain a simple upper bound to (10) in the case of ZZP (for the details see Appendix E.3). We assume the conditions of Theorem 1 are satisfied and also that the expected error of the learned rates \(_{i}^{Z}\) is bounded by a constant, in the sense that \([|r_{i}^{Z}(X_{t},V_{t},_{f}-t)-s_{i}^{g}(X_{t},V_{t}, _{f}-t)|_{i}^{Z}(X_{t},_{i}^{2}V_{t})] M\) for all \(t[0,_{f}]\) and \(i\{1,,d\}\). The latter condition is similar to the standard assumption asked on the approximation of the score in diffusion models (Chen et al., 2023). Under these conditions we obtain the bound

\[\|_{}-(_{_{f}})\|_{}  Ce^{-_{f}}+4M_{f}d\.\] (12)

## 4 Numerical simulations

In this section, we test our piecewise deterministic generative models on simple synthetic datasets.

DesignWe compare the generative models based on ZZP, BPS, and RHMC with the improved denoising diffusion probabilistic model (i-DDPM) given in Nichol and Dhariwal (2021). For all of our models, we choose the standard normal distribution as target distribution for the position vector, as well as for the velocity vector in the cases of BPS and RHMC. The accuracy of trained generative models is evaluated by the kernel maximum mean discrepancy (MMD). We refer to Appendix F for a detailed description of the parameters and networks choices.

Sample qualityIn Table 1 we report the MMD score for five, \(2\)-dimensional toy distributions. We observe that the PDMP based generative models perform well compared to i-DDPM in all of these five datasets. In particular, ZZP and i-DDPM are implemented with the same neural network architecture, hence ZZP appears to compare favourably to i-DDPM with the same model expressivity. The results of Table 1 are supported by the plots of generated data shown in Figure 2, illustrating how ZZP and BPS are able to generate more detailed edges compared to i-DDPM.

In Figure 4, we compare the output of RHMC and i-DDPM for a very small number of reverse steps. We observe how in this setting the data generated by RHMC are noticeably closer to the true data distribution compared to i-DDPM. This phenomenon is observed also for BPS as shown in Table 2, and is intuitively caused by the refreshment kernel, which is able to generate velocities that correct wrong positions. Respecting this intuition, ZZP does not perform as well as BPS and RHMC for a small number of reverse steps since its velocities are constrained to \(\{-1,1\}\). Nonetheless, ZZP generates the most accurate results in our experiments given a large enough number of reverse steps.

Table 2 and Figure 3 show that PDMP-based models require a smaller computational time to generate samples of a given quality compared to i-DDPM. This is the case bec

 Dataset & i-DDPM & BPS & RHMC & ZZP \\  Checkerboard & 2.49 \(\) 0.98 & 1.96 \(\) 1.51 & 4.27 \(\) 3.36 & **0.81**\(\) 0.19 \\ Fractal tree & 8.04 \(\) 5.58 & 2.25 \(\) 1.70 & 4.41 \(\) 4.35 & **1.12**\(\) 0.58 \\ Gaussian grid & 23.19 \(\) 9.72 & 4.59 \(\) 4.03 & **4.01**\(\) 3.32 & 4.43 \(\) 4.05 \\ Olympic rings & 2.03 \(\) 1.60 & 2.07 \(\) 1.19 & 2.41 \(\) 2.24 & **1.43**\(\) 0.86 \\ Rose & 6.77 \(\) 5.81 & 1.92 \(\) 1.57 & 2.16 \(\) 1.59 & **0.90**\(\) 0.35 \\ 

Table 1: MMD \(\), in units of \(1e-3\), averaged over 6 runs, with the corresponding standard deviations.

Figure 2: Comparative results on two-dimensional generation of synthetic datasets.

considerably less backward steps than i-DDPM, although each step is more expensive (see Table 2). Additional results can be found in Appendix F, including promising results applying ZZP to the MNIST dataset.

## 5 Discussion and conclusions

We have introduced new generative models based on piecewise deterministic Markov processes, developing a theoretically sound framework with specific focus on three PDMPs from the sampling literature. While this work lays the foundations of this class of methods, it also opens several directions worth investigating in the future.

Similarly to other generative models, our PDMP based algorithms are sensitive to the choice of the network architecture that is used to approximate the backward characteristics. Therefore, it is crucial to investigate which architectures are most suited for our algorithms in order to achieve state of the art performance in real world scenarios. For instance, in the case of BPS and RHMC it could be beneficial to separate the estimation of the density ratios and the generation of draws of the velocity conditioned on the position and time. For the case of ZZP, efficient techniques to learn the network in a high dimensional setting need to be investigated, while network architectures that resemble those used to approximate the score function appear to adapt well to the case of density ratios. Moreover, there are several alternative PDMPs that could be used as generative models and that we did not consider in detail in this paper, as for instance variance exploding alternatives.

   steps & i-DDPM & BPS & RHMC & ZZP \\ 
2 & 696.28 & 165.09 & **26.48** & 358.25 \\
5 & 192.17 & 22.18 & **3.00** & 89.49 \\
10 & 45.08 & 5.48 & **1.75** & 11.31 \\
25 & 12.34 & 1.58 & **0.60** & 1.20 \\
100 & 8.72 & 3.66 & 1.72 & **1.04** \\  time/step(ms) & 3.94 & 45.8 & 15.1 & 11.2 \\   

Table 2: MMD \(\) for various number of backward steps, Rose dataset.