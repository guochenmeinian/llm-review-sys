# On Dynamic Programming Decompositions

of Static Risk Measures in Markov Decision Processes

 Jia Lin Hau

University of New Hampshire

Durham, NH

jialin.hau@unh.edu

&Erick Delage

HEC Montreal

Montreal (Quebec)

erick.delage@hec.ca

&Mohammad Ghavamzadeh

Amazon

Palo Alto, CA

ghavamza@amazon.com

&Marek Petrik

University of New Hampshire

Durham, NH

mpetrik@cs.unh.edu

The work was done prior to joining Amazon, while the author was at Google Research.

###### Abstract

Optimizing static risk-averse objectives in Markov decision processes is difficult because they do not admit standard dynamic programming equations common in Reinforcement Learning (RL) algorithms. Dynamic programming decompositions that augment the state space with discrete risk levels have recently gained popularity in the RL community. Prior work has shown that these decompositions are optimal when the risk level is discretized sufficiently. However, we show that these popular decompositions for Conditional-Value-at-Risk (CVaR) and Entropic-Value-at-Risk (EVaR) are inherently suboptimal regardless of the discretization level. In particular, we show that a saddle point property assumed to hold in prior literature may be violated. However, a decomposition does hold for Value-at-Risk and our proof demonstrates how this risk measure differs from CVaR and EVaR. Our findings are significant because risk-averse algorithms are used in high-stakes environments, making their correctness much more critical.

## 1 Introduction

Risk-averse reinforcement learning (RL) seeks to provide a risk-averse policy for high-stakes real-world decision problems. These high-stake domains include autonomous driving (Jin et al., 2019; Sharma et al., 2020), robot collision avoidance (Ahmadi et al., 2021; Hakobyan and Yang, 2021), liver transplant timing (Kose, 2016), HIV treatment (Keramati et al., 2020; Zhong, 2020), unmanned aerial vehicle (UAV) (Choudhry et al., 2021), and investment liquidation (Min et al., 2022), to name a few. Because these domains call for reliable solutions, risk-averse algorithms must be based on solid theoretical foundations. This is one reason why monetary risk measures, such as Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR), have become pervasive in risk-averse RL (Prashanth and Fu, 2022). Indeed, risk measures such as CVaR are known to be coherent (Artzner et al., 1999) with respect to a set of fundamental axioms that define how risk should be quantified and have been adopted as gold standards in banking regulations (Basel Committee on Banking Supervision, 2019).

Introducing risk-averse objectives in Markov decision processes (MDPs)--the primary model used in RL--is challenging. Dynamic programming, the linchpin of most RL algorithms, cannot be used directly to optimize a risk measure like VaR or CVaR in MDPs. One line of work tackles this challenge by exploiting the primal representation of risk measures and augmenting the state spaceof their dynamic programs (DPs) with an additional parameter that typically represents the total cumulative reward up to the current point (Bauerle and Ott, 2011; Boda et al., 2004; Chow and Ghavamzadeh, 2014; Filar et al., 1995; Hau et al., 2023; Lin et al., 2003; Wu and Lin, 1999; Xu and Mannor, 2011). Even when the original MDP is finite, this DP requires computing the value function for a continuous state space, and thus, has been considered inefficient in practice (Chapman et al., 2022; Chow et al., 2015; Li et al., 2022).

Another line of recent work leverages the dual representation to produce a _risk-level decomposition_ of risk measures (Pflug and Pichler, 2016). Using this decomposition, numerous authors have derived DPs for common risk measures and integrated them within various RL algorithms (Chapman et al., 2019, 2022; Chow et al., 2015; Ding and Feinberg, 2022, 2023; Ni and Lai, 2022; Rigter et al., 2021; Stanko and Macek, 2019). Although this risk-level decomposition requires augmenting the state space with a continuous parameter, this parameter is naturally bounded between 0 and 1. It has been generally accepted, with several _tentative_ proofs supporting this claim (Chow et al., 2015; Li et al., 2022), that these DPs recover the optimal policy if we can discretize the augmented state space sufficiently finely. Moreover, it is believed that one can use the optimal value function from this DP to recover the policies that are optimal for the full range of risk levels.

In this paper, we make a surprising discovery that numerous claims of optimality of risk-level decompositions published in the past several years are incorrect. Even when one discretizes the augmented state space arbitrarily finely, most risk-level DPs are not guaranteed to recover the optimal value function and policy. There are several reasons why existing arguments fail. As the most common reason, several papers assume that a certain saddle point property holds, either explicitly (Chow et al., 2015) or implicitly (Ding and Feinberg, 2022, 2023). We show that this property does not generally hold, invalidating the optimality of DPs, as hinted at in Chapman et al. (2019, 2022). This finding directly refutes the claimed or hypothesized optimality of algorithms proposed in many recent research papers and pre-prints, such as Chapman et al. (2019); Chow et al. (2015); Ding and Feinberg (2022, 2023); Rigter et al. (2021); Stanko and Macek (2019). Our results also affect applications of these algorithms, such as automated vehicle motion planning (Jin et al., 2019). We also identify gaps in related decompositions (Li et al., 2022; Ni and Lai, 2022) and propose how to fix them.

We make the following contributions in this paper. _First_, we show in Section 3 that the popular DP for optimizing CVaR in MDPs may not recover the optimal value function and policy regardless of how finely one discretizes the risk level in the augmented states. This method was first proposed in Chow et al. (2015) but adopted widely afterwards (Chapman et al., 2019; Ding and Feinberg, 2022, 2023; Rigter et al., 2021; Stanko and Macek, 2019). The simple counterexample in this section contradicts the optimality claims in Chow et al. (2015); Ding and Feinberg (2022). We hypothesize that prior work missed this issue because the CVaR DP works for policy evaluation and only fails when one uses it to optimize policies based on the "risk-to-go value function". Therefore, our results do not contradict the original decomposition in Pflug and Pichler (2016) that only applies to policy evaluation. We give a new independent and simple proof that the CVaR decomposition indeed works when evaluating a fixed policy.

_Second_, we show in Section 4 that the DP for optimizing the Entropic-Value-at-Risk in MDPs, proposed by Ni and Lai (2022), does not compute the correct value function even when the policy is fixed. Although EVaR has not been as popular as CVaR, it has been gaining attention in recent years (Hau et al., 2023). We give an example that contradicts the correctness claims of the risk-level decomposition for EVaR in Ni and Lai (2022). The gap that we identify with this objective applies to both policy evaluation and policy optimization. Furthermore, we prove a new, correct EVaR decomposition for policy evaluation. Unfortunately, the EVaR decomposition fails and is sub-optimal when applied to policy optimization, similar to CVaR.

_Third_, we propose an _optimal dynamic program_ for policy optimization of VaR in Section 5. Our DP is based on a risk-level decomposition that closely resembles the quantile MDP decomposition in Li et al. (2022) but corrects for several technical inaccuracies. The derivation shows why VaR stands apart from coherent risk measures like CVaR and EVaR. VaR is unique in that the decomposition can be constructed directly from the primal formulation of the risk measure, which avoids the complications that arise in the robust formulations used in CVaR and EVaR decompositions.

It is important to note that the correctness of DPs that augment the state space with the accumulated rewards is unaffected by our results (Bauerle and Ott, 2011; Chow and Ghavamzadeh, 2014; Chow et al., 2018; Hau et al., 2023). These DPs use the _primal_ risk measure representation and do not suffer from the same saddle point issue as the augmentation methods that use the _dual_ representation of the risk measures, such as the one in Chow et al. (2015).

## 2 Preliminaries

This section summarizes relevant properties of monetary risk measures and outlines how they are typically used in the context of solving MDPs.

Monetary Risk MeasuresWe restrict our attention to probability spaces with a finite outcome space \(\) such that \(||=m\) for some \(m\). We use \(=^{m}\) to denote the space of real-valued random variables. To improve the clarity of probabilistic claims, we always adopt random variables with a tilde, such as \(\). In finite spaces, we can represent any random variable \(\) as a vector \(^{m}\). We also use \(_{m}\) to represent a probability distribution over \(\) where \(_{m}\)represents the \(m\)-dimensional probability simplex. Using this notation, we can write that \([]=^{}\).

A _monetary risk measure_\(\) assigns a real value to each real-valued random variable in a way that it is monotone and cash-invariant (Follmer and Schied, 2016; Shapiro et al., 2014). A risk measure can be seen as a generalization of the expectation operator \([]\) that also takes into account the uncertainty in the random variable. In this work, we define all risk measures for random variables \(\) that represent _rewards_. Thus, the risk-averse decision-maker aims to choose actions that maximize the value of the risk measure, i.e., a higher value of risk measure represents a lower exposure to risk.

We consider three monetary risk measures common in RL. Perhaps the most well-known measure is _Value-at-Risk_ (VaR), which is defined for a risk-level \(\) and a random variable \(\) in modern literature as (e.g., Follmer and Schied, 2016; Shapiro et al., 2014)

\[_{}[]\ =\ \ \{z\ |\ [<z]\}\ =\ \ \{z\ |\ [ z]>\}\,.\] (1)

Note that \(_{1}[]=\). The equality between the two definitions holds, for example, by Follmer and Schied (2016, remark A.20).

Another popular risk measure is the _Conditional-value-at-Risk_ (CVaR), which is defined for a risk level \(\) and a random variable \(\) distributed as \(\) as (e.g., Follmer and Schied 2016, definition 11.8 and Shapiro et al. 2014, eq. 6.23)

\[_{}[]\ =\ _{z}\ z-^{-1}[z-]_{+}\ =\ \ ^{}\ |\ _{m},},\] (2)

with \(_{0}[]=[]\) and \(_{1}[]=[]\). The equality above follows from standard conjugacy arguments for finite probability spaces (Follmer and Schied, 2016). Note that our CVaR definition applies to \(\) that represents rewards and assumes that a higher value of the risk measure is preferable to a lower value. Other CVaR formulations exist in the literature, but they induce identical preferences for appropriately chosen rewards and risk level \(\).

Finally, the _entropic value at risk_ (EVaR), with \(_{0}[]=[]\) and \(_{1}[]=[]\), is defined for \((0,1]\) as (Ahmadi-Javid, 2012)

\[_{}[] =_{>0}-(^{-1} (-))\] (3) \[=\ ^{T}\ |\ _{m}, ,(\|)-}\,,\]

where \(\) is the standard KL-divergence defined for each \(,_{m}\) as \((\|)=_{}x_{}(x_{-}/y_ {})\). This definition is valid only when \(\) is absolutely continuous with respect to \(\), which is denoted as \(\) and corresponds to \(y_{}=0 x_{}=0\) for each \(\).

Risk Averse MDPsA Markov decision process (MDP) is a sequential decision model that underlies most of RL (Puterman, 2005). We consider finite MDPs with states \(=\{s_{1},,s_{S}\}\) and actions \(=\{a_{1},,a_{A}\}\). After taking an action in a state, the agent transitions to the next state according to a transition probability function \(p_{}\) such that \(p(s,a,s^{})\) represents the transition probability from \(s\) to \(s^{}\) after taking \(a\). We use \(_{s,a}=p(s,a,)_{S}\) to denote the vector of transition probabilities. The initial state \(_{0}\) is distributed according to \(}_{S}\). To avoid divisions by \(0\) that are not central to our claims, we assume that \(_{s}>0\) for each \(s\). Finally,the reward function is \(r\), where \(r(s,a,s^{})\) represents the deterministic reward associated with the transition to \(s^{}\) from \(s\) after taking an action \(a\).

The most general solution to an MDP is a _history-dependent randomized_ policy \(\) which maps a sequence of observed states and actions \(s^{0},a^{0},s^{1},a^{1},,s^{t}\) to a distribution over the next action \(a^{t}\). It is well-known that with risk-neutral objectives, there always exists an optimal stationary--depends only on the last state--deterministic policy (Puterman, 2005). When the objective is risk-averse, like VaR, or CVaR, there may not exist an optimal stationary or deterministic policy. Hence, we use the symbol \(\) to denote the set of history-dependent randomized policies in the remainder of the paper.

This paper focuses on the _finite-horizon objective_ in which the agent aims to compute policies that optimize the sum of rewards over a known horizon \(T\). We further restrict our attention to the objective with horizon \(T=1\). It turns out that having a single time step is sufficient to derive our counterexamples to existing dynamic programs. Moreover, deriving the decompositions with \(T=1\) makes it possible to avoid technicalities caused by history-dependent policies, which could distract us from the main ideas presented in this work. Our results an be extended to general horizons \(T>1\) and the discounted infinite-horizon objectives using standard techniques (Chow et al., 2015).

With horizon \(T=1\), the set of randomized history-dependent policies is \(=\{_{}\}\). The symbol \((s,a)\) denotes the probability of action \(a\) in a state \(s\), and \((s)=(s,)_{A}\) denotes the \(A\)-dimensional vector of action probabilities in a state \(s\). Given a risk measure \(\) with a risk level \(\), the finite-horizon risk-averse value of a policy \(\) is computed as

\[v_{0}^{}()=_{}^{()} [r(,,^{})],\] (4)

where the superscript in \(_{}^{()}\) specifies the distribution of the random action. Throughout the paper, we generally use \(\) to denote the random state at time \(t=0\) and \(^{}\) to denote the random state at time \(t=1\). In risk-neutral objectives, when \(=\), one can use the tower property of the expectation operator and define a value function \(v_{t}\) for each time step \(t\)(Puterman, 2005), but this property does not hold in most static risk measures (Hau et al., 2023). The term _policy evaluation_ in the remainder of the paper refers to computing the value in (4).

The goal in an MDP is to compute an _optimal_ value function and a policy that attains it. In risk-averse MDPs, this goal is formalized as the following risk-averse optimization

\[v_{0}^{}()=_{}\,v_{0}^{}()=_{} \,_{}^{()}[r(, ,^{})],\] (5)

with the _optimal policy_\(^{}\) being any policy that attains the maximum (5). As with policy evaluation, when \(=\), the optimal value function \(v_{t}^{}\) can be defined for each time-step \(t\)(Puterman, 2005), but this is impossible in general for common risk measures, like VaR and CVaR. The term _policy optimization_ in the remainder of the paper refers to computing the value and the maximizer in (5).

In the remainder of the paper, we study dynamic programming algorithms proposed to solve the policy evaluation problem in (4) and policy optimization problem in (5). In general, these algorithms build on risk-level decomposition (Pflug and Pichler, 2016) of risk measures to define a value function \(v_{t}^{}(s,)\) for each time step \(t[T]\), state \(s\), and risk-level \(\)(Chow et al., 2015). The value function represents the risk-adjusted sum of rewards that can be obtained if starting in a state \(s\) at time \(t\) and a risk level \(\). For example, one would define the value function as \(v_{1}^{}(s,)=_{}^{(s)}[r(s, ,^{})]\) and compute \(v_{0}^{}\) using a Bellman operator \(T_{}^{}\) as \(v_{0}^{}()=(T^{}v_{1}^{})()\). In risk-neutral objectives, the Bellman operator is defined as \((T^{}(v_{1}^{}))()=[v_{1}^{}(,)]\), with \(\{1\}\), but in risk-averse formulations the operator definition is more complex. The remainder of the paper discusses the decompositions and the operator for CVaR, EVaR, and VaR risk measures respectively.

## 3 CVaR: Decomposition Fails in Policy Optimization

In this section, we show that a common CVaR decomposition proposed in Chow et al. (2015) and used to optimize risk-averse policies is inherently sub-optimal regardless of how closely one discretizes the state space. The following proposition represents one of the key results used to decompose the risk measure in multi-stage decision-making.

**Proposition 3.1** (lemma 22 in Pflug and Pichler 2016).: _Suppose that \(\) and \(}\), \(()\), \(^{}_{s,a}\). Then,_

\[_{}[r(,,^{})] = _{_{}}\;_{s }_{s}\;_{_{s}_{s}^{-1}}[ r(s,,^{})=s],\] (6)

_where the state \(s\) on the right-hand side is not random and_

\[_{}\ =\ \{_{S} }\}\,.\] (7)

The notation in Proposition 3.1 differs superficially from lemma 22 in Pflug and Pichler (2016). Specifically, our CVaR is defined for rewards rather than costs, the meaning of our \(\) corresponds to \(1-\) in Pflug and Pichler (2016), and we use \(_{s}=z_{s}_{s}\) as the optimization variable. We include a simple proof of Proposition 3.1 for completeness in Appendix A.1.

The decomposition in Proposition 3.1 is important because it shows that the CVaR evaluation can be formulated as a dynamic program. The theorem shows that CVaR at time \(t=0\) decomposes into a convex combination of CVaR values at \(t=1\). Recursively repeating this process, one can formulate a dynamic program for any finite time horizon \(T\). Because the risk level at \(t=1\) differs from the level at \(t=0\) and depends on the optimal \(\), one must compute CVaR values for all (or many) risk-levels \(\) at time \(t=1\). As a result, the dynamic program includes an additional state variable that represents the current risk level.

Chow et al. (2015) proposed to adapt the decomposition in Proposition 3.1 to policy optimization as

\[_{}\;_{}^{ ()}[r(,,^{})]& =\;_{}_{_{}} \;_{s}_{s}(_{_{s}_{s}^{-1}}^{(s)}|\;=s|)\\ &}}{{=}}\;_{_{}}\;_{s}_{s}(_ {d_{}}\;_{_{s}_{s}^{-1}} ^{}[r(s,,^{}) =s])\,.\] (8)

They used the decomposition in (8) to formulate a dynamic program with the current risk level as an additional state variable. We prove in the following theorem that the second equality in (8) marked with question marks is false in general.

**Theorem 3.2**.: _There exists an MDP and a risk level \(\) such that_

\[_{}\;_{}^{( )}[r(,,^{})]<_{_{}}\;_{s}_{s}(_{d _{}}\;_{_{s}_{s}^{-1}}^{ }[r(s,,^{})=s ])\,.\] (9)

Before proving Theorem 3.2, we discuss its implications. First, Theorem 3.2 contradicts theorems 5 and 7 in Chow et al. (2015) and shows that their algorithm is inherently sub-optimal regardless of the resolution of the discretization. Theorem 3.2 also contradicts the optimality of the accelerated dynamic program proposed in Stanko and Macek (2019). The result of Chow et al. (2015) was exploited as is in Chapman et al. (2019), Ding and Feinberg (2022), and Jin et al. (2019) to propose DP reductions, and extended, without proof, in (Rigter et al., 2021) to the context of a Bayesian MDP.

Finally, it is important to emphasize that Theorem 3.2 only applies to the policy optimization setting and does not contradict Proposition 3.1, which holds for the evaluation of policies that assign the same action distribution to each history of states and actions (i.e., policies that are independent of the hypothesized values of \(\)).

Figure 1: Rewards of MDP \(M_{}\) used in the proof of Theorem 3.2. The dot indicates that the rewards are independent of the next state.

Proof.: Let \(=0.5\) and consider the MDP \(M_{}\) in Figure 1. In state \(s_{1}\), both actions \(a_{1}\) and \(a_{2}\) are available, and in state \(s_{2}\), only action \(a_{1}\) is available. The MDP's rewards are

\[r(s_{1},a_{1},s_{1}) =-50, r(s_{1},a_{1},s_{2}) =100,\] \[r(s_{1},a_{2},s_{1}) =r(s_{1},a_{2},s_{2})=0, r(s_{2},a_{1},s_{1}) =r(s_{2},a_{1},s_{2})=10\,.\]

The transition probabilities in \(M_{}\) are

\[p(s_{1},a_{1},s_{1}) =0.4, p(s_{1},a_{1},s_{2}) =0.6\,,\]

and the initial distribution is uniform: \(_{s_{1}}=_{s_{2}}=0.5\).

To simplify the notation, we define \(_{}_{}\) for each \(\) and \(_{}\) as

\[_{}()\ =\ _{s}_{s}_{_{s}_{s}^{-1}}^{(s)}[r(s,, ^{})=s]\,.\]

Because \(\) is convex in the distribution (Delage et al., 2019) and any distribution for \(r(,,^{})\) obtained from a policy \(\) is a mixture of the distributions of \(r(,a_{1},^{})\) and \(r(,a_{2},^{})\), it is sufficient to consider only _deterministic_ policies (there exists an optimal deterministic policy). Thus, we can reformulate the _left-hand side_ of (9) in terms of \(_{}()\) as

\[_{}\ _{}^{( )}[r(,,^{})] =_{\{_{1},_{2}\}}_{ _{s}_{s}^{-1}}^{(s)}[r(,, ^{})]\] \[=_{\{_{1},_{2}\}}_{_{ }}_{s}_{s}_{ _{s}_{s}^{-1}}^{(s)}[r(s,,^{ })=s]\] \[=_{\{_{1},_{2}\}}_{_{ }}_{}()\,,\]

with \(_{1}(s,a_{1})=1-_{1}(s,a_{2})=1\) and \(_{2}(s,a_{2})=1-_{2}(s,a_{1})=1\), for all \(s\). The functions \(_{_{1}}()\) and \(_{_{2}}()\) are depicted in Figure 2. Similarly, the _right-hand side_ of (9) can be expressed using the convexity of \(\) in the distribution by algebraic manipulation as

\[_{_{}}\ _{s}_{s} _{_{A}}_{_{s}_{s}^{- 1}}^{}[r(s,,^{})=s]= _{_{}}_{\{_{1},_{2}\}} _{}()\,.\]

Using the notation introduced above and the sufficiency of optimizing over deterministic policies only, the inequality in (9) becomes

\[_{\{_{1},_{2}\}}_{_{ }}_{}()\ <\ _{_{}}_{\{_{1},_{2}\}} _{}()\,.\] (10)

Figure 2 demonstrates the inequality in (10) numerically, with the rectangle representing the left-hand side maximum and the pentagon representing the right-hand side minimum. The dashed line represents the function \(_{\{_{1},_{2}\}}_{}()\).

[MISSING_PAGE_FAIL:7]

**Theorem 4.1**.: _There exists an MDP with a single action and \((0,1]\) such that_

\[_{}[r(,a_{1},^{})] < _{_{}}\;_{s} _{s}\,_{_{s},_{s}^{-1}}[r(s,a_{1}, ^{})=s]\,,\] (13)

_the set \(_{}\) defined by (12)._

Theorem 4.1 demonstrates a stronger failure mode than the one in Theorem 3.2 (for CVaR policy optimization) since it applies to both policy evaluation and policy optimization settings.

We propose a correct decomposition of EVaR in the following theorem and employ it to establish that the decomposition in (11) overestimates the actual value of EVaR (see Appendix A.3 for a proof).

**Theorem 4.2**.: _Given any finite MDP with horizon \(T=1\) and \((0,1]\), we have that_

\[_{}[r(,,^{})] = _{(0,\,1]^{S},\,_{}^{ }()}\;_{s}_{s}\,_{_{s} }[r(s,,^{})=s]\,,\]

_where_

\[_{}^{}()=\{_{S} },\;_{s}_{s}(_{s}/ _{s})-(_{s})-\}.\]

_Moreover, EVaR can be upper-bounded as_

\[_{}[r(,,^{})]  _{_{}}\;_{s} _{s}\,_{_{s},_{s}^{-1}}[r(s,, ^{})=s]\,.\] (14)

## 5 VaR: Decomposition Holds for Policy Evaluation and Optimization

In this section, we discuss a dynamic program decomposition for VaR whose decomposition resembles those for CVaR and EVaR described in Sections 3 and 4. We provide a new proof of the VaR decomposition to elucidate the differences that make it optimal in contrast to CVaR and EVaR decompositions. Our VaR decomposition closely resembles the quantile MDP approach in Li et al. (2022) with a few technical modifications that can significantly impact the computed value.

To contrast the typical definition of VaR with the quantile definition in Li et al. (2022), it is helpful to summarize how VaR is related to the quantile of a random variable. Let \(\) define as the \(\)_-quantile_ of \(\) when

\[[] [<]\;.\] (15)

In general, the set of quantiles is an interval \([_{}^{-}(),_{}^{+}()]\) with the bounds computed as (Follmer and Schied, 2016, appendix A.3)

\[_{}^{-}() =\;\{z[<z]< \}=\{z[ z] \},\] \[_{}^{+}() =\;\{z[ z]> \}=\{z[<z]\}.\]

Note that when the distribution of \(\) is absolutely continuous (atomless), then \(_{}^{+}()=_{}^{-}()\) and the quantile is unique. The following example illustrates a simple setting in which the quantile is not unique.

_Example 1_ (Bernoulli random variable).: Consider a Bernoulli random variable \(\) such that \(=1\) and \(=0\) with equal (50%) probabilities. Then, any value \(q\) is a valid \(0.5\)-quantile because

\[_{}^{-}(0.5) =_{z}\;\{z[ z ] 0.5\}=_{z}\;\{z z 0\}=0\] \[_{}^{+}(0.5) =_{z}\;\{z[  z] 0.5\}=_{z}\;\{z z 1\}=1.\]

The objective in Li et al. (2022) is to maximize the quantile operator \(Q_{}\) defined for a reward random variable \(\) and a risk level \(\) as

\[Q_{}()\;=\;_{z}\;\{z[  z]\}\,.\] (16)

The quantile operator \(Q_{}\) and VaR differ in which quantile of the random variable they consider:

\[Q_{}()\;=\;_{}^{-}(), _{}[]\;=\;_{}^ {+}()\,.\] (17)

As a result, the quantile MDP objective in (16) coincides with the VaR value only when the quantile is unique, which is not always the case, as shown in Example 1.

**Theorem 5.1**.: _Let \([N]\) be a random variable distributed as \(}=(_{i})_{i=1}^{N}\) with \(_{i}>0\). Then for any random variable \(\), we have_

\[_{}[] = _{_{N}}\{_{i}\ _{_{i}_{i}^{-1}}[=i ]}\}\,,\] (18)

_where we interpret the minimum to evaluate to \(\) if all terms are infinite, which only occurs if \(=1\)._

Proof.: We decompose VaR using the definition in (1) as

\[_{}[] =\ \{z[<z ]\}}{=}\ \{z_{i=1}^{N}[<z=i]_{i}\}\] \[}{=}\ \{z^{N},\ [<z=i]_{i}, i[N ],\ _{i=1}^{N}_{i}_{i}\}\] \[}{=}\{\ \{z [<z=i]_{i},\  i[N]\}^{N},\ _{i=1}^{N}_{i}_{i}\}\] \[=\{\ _{i[N]}\{z [<z=i]_{i}\} {}^{N},\ _{i=1}^{N}_{i}_{i}\}\] \[}{=}\{_{i[N]}\ \{z [<z=i]_{i}\} {}^{N},\ _{i=1}^{N}_{i}_{i}\}\] \[}{=}\ \{_{i[N]}\ _{_{i}} [=i]^{N},\ _{i=1}^{N}_{i}_{i}\}.\]

We decompose the probability \([<z]\) as a marginal of the conditional probabilities \([<z=i]\) and \(_{i}=[=i]\) in step (a) and then lower-bound them by an auxiliary variable \(_{i}\) in step (b). In step (c) we replace the joint supremum over \(z\) and \(\) by sequential suprema, and then we replace the supremum of an intersection by the minimum of the suprema of sets in (d). The equality in (d) holds because \([<z]\) is monotone and, therefore, the sets \(\{z[<z=i] _{i}\}\) are nested. Finally, step (e) follows from the definition of VaR in (1). 

Focusing on the finite MDP with horizon \(T=1\), we can show that the decomposition proposed in Theorem 5.1 is amenable to policy optimization. The main difference between the VaR decomposition and CVaR is that the former VaR was expressed as a supremum instead of an infimum over quantile levels \(\). For VaR, changing the order of maximum (\(\)) and supremum (\(\)) does not suffer from a potential gap, but changing the order of maximum (\(\)) and infimum/minimum (\(\)) in CVaR does suffer from such a gap as shown in Theorem 3.2.

The following theorem (proved in Appendix A.4) summarizes the decomposition for VaR.

**Theorem 5.2**.: _Given any finite MDP with horizon \(T=1\) and \(\), we have_

\[_{}\ _{}^{()}[r( ,,^{})]=_{_{S}}\{ _{s}_{_{A}}_{_{s}_{s}^{-1}}^{}[r(s,,^{}) =s]}\}.\]

For completeness, Appendix C extends Theorem 5.2 to a setting with horizon \(T>1\), providing dynamic programming equations and a definition of the optimal policy. These equations are analogous to the equations presented in (Li et al., 2022) yet we obtain them using the accurate definition and decomposition of \(_{}\).

We finally present below a valid decomposition for the lower quantile MDP, used officially as the objective in (Li et al., 2022) (see Appendix A.5 for a proof).

**Proposition 5.3**.: _Given any finite MDP with horizon \(T=1\) and some \(\), we have that:_

\[_{}Q_{}^{()}(r(, ,^{}))=_{^{S}}\{_{s :_{s}<1}\ _{_{A}}Q_{_{s}}^{}(r(s,, ^{})=s)_{s=1}^{S}_{s}_{s}< \}.\]We note that the difference with the result presented in (Li et al., 2022) resides in the constraint imposed on \(\) that replaces the weak inequality with a strict one. In fact, this strict versus weak inequality is the main distinguishing factor between the decompositions for the lower and upper quantiles.

## 6 Conclusion

This paper shows that a popular decomposition approach to solving MDPs with CVaR and EVaR objectives is suboptimal despite the claims to the contrary. This suboptimality arises from a saddle-point gap when _optimizing policy_. We also prove that a similar decomposition approach is optimal for policy optimization and evaluation when solving MDPs with the VaR objective. The decomposition is optimal because VaR does not involve the same saddle point problem as CVaR and EVaR.

Our findings are significant because practitioners who make risk-averse decisions in high-stakes scenarios need to have confidence in the correctness of the algorithms they use. Our work raises awareness that popular static CVaR and EVaR MDP algorithms are suboptimal, and their analyses are inaccurate. We hope the results we present in our paper will increase the scrutiny of dynamic programming methods for risk-averse MDPs and motivate research into alternative approaches, such as the parametric dynamic programs.

#### Acknowledgments

We thank Yinlam Chow for his insightful comments that inspired this paper's topic and results. The work in the paper was supported, in part, by NSF grants 2144601 and 2218063. In addition, this work was performed in part while Marek Petrik was a visiting researcher at Google Research. Finally, E. Delage acknowledges the support from the Canadian Natural Sciences and Engineering Research Council and the Canada Research Chair program [Grant RGPIN-2022-05261 and CRC-2018-00105].