# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models

Wei Pang

Masoumeh Shafieinejad

Vector Institute

Lucy Liu

Royal Bank of Canada

Stephanie Hazlewood

Royal Bank of Canada

Xi He

###### Abstract

Recent research in tabular data synthesis has focused on single tables, whereas real-world applications often involve complex data with tens or hundreds of interconnected tables. Previous approaches to synthesizing multi-relational (multi-table)2 data fall short in two key aspects: scalability for larger datasets and capturing long-range dependencies, such as correlations between attributes spread across different tables. Inspired by the success of diffusion models in tabular data modeling, we introduce **C**luster **L**atent **V**ariable guided **D**enoising **D**iffusion **P**robabilistic **M**odels (ClavaDDPM). This novel approach leverages clustering labels as intermediaries to model relationships between tables, specifically focusing on foreign key constraints. ClavaDDPM leverages the robust generation capabilities of diffusion models while incorporating efficient algorithms to propagate the learned latent variables across tables. This enables ClavaDDPM to capture long-range dependencies effectively. Extensive evaluations on multi-table datasets of varying sizes show that ClavaDDPM significantly outperforms existing methods for these long-range dependencies while remaining competitive on utility metrics for single-table data.

## 1 Introduction

Motivation.Synthetic data has attracted significant interest for its ability to tackle key challenges in accessing high-quality training datasets. These challenges include: i) data scarcity , ii) privacy , and iii) bias and fairness . The interest in synthetic data has extended to various commercial settings as well, notably in healthcare  and finance  sectors. The synthesis of tabular data, among all data modalities, is a critical task with approximately 79% of data scientists working with it on a daily basis . While the literature on tabular data synthesis has predominantly focused on single table (relation) data, datasets in real-world scenarios often comprise multiple interconnected tables and raise new challenges to traditional single-table learning . These challenges have even enforced a join-as-one approach , where the multi relations are first joined as a single table. However, with more than a couple of relations (let alone tens or hundreds of them as in the finance sector) this approach is neither desirable nor feasible.

Challenges.Synthetic Data Vault  and PrivLava  are recent efforts to synthesize multi-relational data using hierarchical and marginal-based approaches. These methods exhibit significantlimitations in processing speed and scalability, both with respect to the number of tables and the domain size of table attributes, and they often lack robustness in capturing intricate dependencies. Alternatively, diffusion models have emerged as powerful tools for data synthesis, demonstrating remarkable success in various domains . These models are particularly noted for their strong capabilities in controlled generation. Despite their potential, the application of diffusion models to tabular data synthesis has been limited to unconditional models [25; 50; 28; 24], leaving a gap in effectively addressing the multi-table synthesis problem.

Solution.To address these challenges, we introduce ClavaDDPM (Cluster Latent Variable guided Denoising Diffusion Probabilistic Models). Our novel approach leverages the controlled generation capabilities of diffusion models by utilizing clustering labels as intermediaries to model the relationships between tables, focusing on the foreign-key constraints between parent and child tables. This integration of classifier guidance within the diffusion framework allows ClavaDDPM to effectively capture complex multi-table dependencies, offering a significant advancement over existing methods.

Contributions.In this work, we: 1) provide a complete formulation of the multi-relational modeling process, as well as the essential underlying assumptions being made, 2) propose an efficient framework to generate multi-relational data that preserves long-range dependencies between tables, 3) propose relationship-aware clustering as a proxy for modeling parent-child constraints, and apply the controlled generation capabilities of diffusion models to tabular data synthesis, 4) apply an approximate nearest neighbor search-based matching technique, as a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents, 5) establish a comprehensive multi-relational benchmark, and propose _long-range dependency_ as a new metric to measure synthetic data quality specific to multi-table cases, and 6) show that ClavaDDPM significantly outperforms existing methods for these long-range dependency metrics while remaining competitive on utility metrics for single-table data.

## 2 Related work

Single-table synthesis models.Bayesian network  is a traditional approach for synthetic data generation for tabular data. They represent the joint probability distribution for a set of variables with graphical models. CTGAN  is a tabular generator that considers each categorical value as a condition. CTAB-GAN  includes mixed data types of continuous and categorical variables. Several studies have explored how GAN-based models can contribute to fairness and bias removal [44; 45]. In privacy, GAN-based solutions boosted with differential privacy have not been as successful as their Baysian-network-based competitors [34; 51]. Recent popular Diffusion Models, [20; 40; 42; 41], offer a different paradigm for generative modeling. TabDDPM  utilizes denoising diffusion models, treating numerical and categorical data with two disjoint diffusion processes. STaSy  uses score-based generative modeling in its training strategy. CoDi  processes continuous and discrete variables separately by two co-evolved diffusion models. Unlike the previous three which perform in data space, TabSyn  deploys a transformer-based variational autoencoder and applies latent diffusion models. Privacy and fairness research for diffusion models are currently limited to a few studies in computer vision [26; 11; 16].

Multi-table synthesis models.There have been few proposals for synthetic data generation for multi-relational data. A study proposed this synthesis through graph variational autoencoders , the presented evaluation is nevertheless very limited. The Synthetic Data Vault  uses the Gaussian copula process to model the parent-child relationship. SDV iterates through each row in the table and performs a conditional primary key lookup in the entire database using the ID of that row, making a set of distributions and covariance matrices for each match. This inhibits an efficient application of SDV to the numerous tables case. PrivLava , synthesizes relational data with foreign keys under differential privacy. The key idea of PrivLava is to model the data distribution using graphical models, with latent variables included to capture the inter-relational correlations caused by foreign keys.

## 3 Background

Multi-relational databases.A multi-relational database \(\) consists of \(m\) tables (or relations) \((R_{1},,R_{m})\). Each table is a collection of rows, which are defined over a sequence of attributes.

One of the attributes, let's consider the first attribute without loss of generality, is the _primary key_ of table \(R\), which serves as the unique identifier for each row in the table. No rows in the same table have repeated values for the primary key attribute. We use _Berka_ database  as our running example in this work, as in Figure 1. Note the _Account ID_, the primary key for the _Account_ table in _Berka_.

Given a table \(R_{j}\), we say a relation \(R_{i}\) has a _foreign key constraint_ with \(R_{j}\), or \(R_{i}\)_refers to_\(R_{j}\), if \(R_{i}\) has an attribute known as _foreign key_ that refers to the primary key of \(R_{j}\): for every row \(r_{i} R_{i}\), there exists a row \(r_{j} R_{j}\) such that \(r_{j}\)'s primary key value equals to \(r_{i}\)'s foreign key value. For example, the _Account ID_ of the _Loan_ table refers to the primary key of the _Account_ table. If an account row is removed from the _Account ID_ table, so would all the referring rows in the _Loan_ table to this account, for foreign key constraint to hold. Note that the primary key of a table can consist of multiple attributes. In this paper, we focus on the case of a single attribute that is common in practice. Also note that all keys are considered row identifiers and are thus not treated or modeled alongside the actual table attributes in this work.

A multi-relational database under foreign key constraints forms a directed acyclic graph (DAG),

\[=(,),=\{(R_ {i} R_{j}) i,j\{1,,m\},i j,R_{i}R_{j}\}\] (1)

with the tables \(\) being the set of nodes, and \(\) being the set of edges. In addition, for \(R_{i}\) referring to \(R_{j}\), we also call this a _parent-child relationship_, where \(R_{j}\) is the _parent_ and \(R_{i}\) is the _child_. We use the _maximum depth_ to denote the number of nodes on the longest path in \(\). Figure 1 shows the corresponding graph to _Berka_ database and its maximum depth is \(4\).

Multi-relational synthesis problem.Given a multi-relational database \(=\{R_{1},,R_{m}\}\), we would like to generate a synthetic version \(}=\{_{1},,_{m}\}\) that has the same structure and foreign-key constraints as \(\) and preserves attribute correlations within \(\), including 1) the inter-column correlations within the same table; 2) the intra-group correlations within the same foreign key group; 3) the inter-table correlations. The first aspect has been well defined, measured, and tackled in the literature of single-table synthesis  while the other two aspects are raised due to foreign-key constraints between tables . For instance, in _Berka_ database (Figure 1), the foreign key constraint between the _Loan_ table and the _Account ID_ adds an important intra-group correlation for the combinations of loans associated with an account and many 1-hop inter-table correlations between columns in the _Loan_ table and the columns in the _Account_ table. Even for the _Loan_ table and the _Demographic_ table that are indirectly constrained by foreign keys, their columns are correlated as well, e.g., how is the average salary in a district related to the status of loans, an example for 2-hop inter-table correlation.

Classifier-guided DDPM.DDPM  uses two Markov chains, a forward chain that perturbs data to noise through a series of Gaussian transitions, and a reverse chain that converts noise back to data with the same number of steps of Gaussian transitions (Equation 2).

\[q(_{t}_{t-1}) (_{t};} _{t-1},_{t})\] (2) \[p_{}(_{t-1}_{t}) (_{t-1};_{ }(_{t},t),_{}( _{t},t)).\]

Prior work  shows that given label \(\), the conditional reverse process has the form

\[p_{,}(_{t}_{t+1},) p_{}(_{t}_{t+1} )p_{}(_{t}).\] (3)

Figure 1: _Berka_ sample tables (left), and the foreign key constraint graph for _Berka_ (right)

By approximating \( p_{}(_{t})\) using Taylor expansion around \(_{t}=\), the conditional reverse process (Equation 3) can be approximated with a perturbed Gaussian transition 

\[(p_{,}(_{t}_{t+1},)) (p())+C,\ \ (+,),\] (4)

where \(C\) is a constant and \(=_{_{t}}(P_{}(_{t}) )_{_{t}=}\) computed from the classifier \(P_{}\).

## 4 ClavaDDPM

Here, we elaborate on the training and synthesis process of ClavaDDPM, and each design's rationale.

### Modeling generative process for two-table relational databases

Notations.Consider a database of two tables \(=\{R_{1},R_{2}\}\), e.g. {_Loan, Account_} in _Berka_, where the child table \(R_{1}\) refers to parent table \(R_{2}\). To model the entire database, we first use \(\) and \(\) as the variables for the child table \(R_{1}\) and parent table \(R_{2}\), respectively (dropping their primary key attributes and indexing their respective row variables starting from one). In this section, we use boldface to represent random variables. e.g. \(Y\), where \(Y\) is the data of \(R_{2}\), and \(\) is the random variable \(Y\) being sampled from. In addition, we use subscript to represent the _parent row_ some data or random variable refers to. e.g. \(_{j}\) represents the child random variable who refers to parent \(_{j}\). Refer to Appendix A for a complete list of notations used and the corresponding design choices.

Assumptions.1) The parent table has no constraints itself. Hence, we can follow previous work on single-table synthesis [13; 25; 47; 50; 52] to make an i.i.d assumption on the rows in the parent table. The parent table \(\) can be modeled as a list of i.i.d. row variables \(\{_{j} j=1,,|R_{2}|\}\), where \(j\) is the index or the primary key value of the \(j\)th row, and each row follows a distribution \(p(y)\).

2) The i.i.d assumption does not apply to the child table rows (\(_{j}\)'s) as they are constrained by their respective parent rows. Consider two loans associated with the same account id; if one's status is _in debt_ ("C"), the other one is likely so too. To capture this dependency, we make a Bayesian modeling assumption that, although child rows associated with the same parent row are not independent, they are conditionally independent of child rows associated with other parent rows, given their respective parent. For example, consider an _account_ table (parent) and a _loan_ table (child). Loans related to the same account (i.e., the same parent) are not independent due to shared account-specific factors. However, loans from different accounts can be considered conditionally independent when accounting for their respective account-level information. Hence, we model \(\) by \(\{_{j} j=1,,|R_{2}|\}\), where each group \(_{j}=\{_{j}^{i} i=1,,|_{j}|\}\) represents a set of child table rows referring to the parent row \(_{j}\).

3) Without violating the assumptions made above, we further make an i.i.d assumption on \((_{j},_{j})\), which leads to an approximated distribution for the parent-child tables:

\[P(=X,=Y)_{j=1}^{|R_{2}|}P(_{j}=g_{j},_{j}=y_{j})\ \ \ \ \ \ \ \ \ p(X,Y)=_{j}p(g_{j},y_{j})\] (5)

where \(X=_{j=1}^{|R_{2}|}g_{j}\) and \(g_{j}=\{x_{j}^{1},,x_{j}^{|g_{j}|}\}\). This model allows us to capture the inter-table correlations (the correlation between tuples from different tables) and the intra-group correlations.

Modeling.Despite the simplified formulation with several aforementioned assumptions, learning the distribution \(p(g_{j},y_{j})\) is non-trivial. In particular, \((g_{j},y_{j})\) cannot be flattened into a matrix form for learning since the set structured attributes in \(_{j}\), e.g., the size of a group variable \(_{j}\) is not fixed.

A naive solution is to model a conditional distribution of the group given the parent row

\[p(g_{j},y_{j})=p(g_{j} y_{j})p(y_{j})\] (6)

Direct modeling of Equation (6) still has the same issue as before for the foreign key group \(_{j}\), which can take an arbitrary number of child rows. In particular, when modeling \(_{j}=f(_{j})\) for some function \(f\), there is no trivial structured support for \(_{j}\) if we model for \(_{j}\) using only the attributes or features of the child rows. Furthermore, the conditioning space of the parent row \(\) can be very 

[MISSING_PAGE_FAIL:5]

has \(k\) leaf node children, \(Z_{1},,Z_{k}\). Let \(c_{X,Z_{i}}\) represent the latent variables learned on the joint space \((X;Z_{i})\). The augmented table for \(X\) is formed by appending all its latent variable values, i.e., \(T_{X}=(X;C_{X,Z_{1}};;C_{X,Z_{k}})\). Then, the latent variable \(c_{Y,X}\) is learned on the joint space of \((Y;T_{X})\) instead of \((Y;X)\). Therefore, our latent learning process follows a bottom-up topological order, ensuring each child table is already augmented by the time we learn the latent variable to augment its parent.

The training phase and the synthesis phase are similar to the two-table case, by handling the parent-child tables in a top-down topological order using the augmented tables. We detail the end-to-end algorithms for the complex data in Appendix B. However, we would like to highlight a special case when a table \(X\) has _multiple parents_\(Y_{1},,Y_{k}\). During synthesis, we will have \(k\) synthetic latent variable \(_{1},,_{k}\) corresponding to the \(k\) parents, and thus \(k\) copies of synthetic child tables \(_{1} p(_{1}),,_{k} p( _{k})\). Unifying these diverged synthetic tables presents a challenge and we present a universal solution in Section 4.3.3.

Extending the model to include more tables allows for capturing longer-range dependencies, beyond just those between adjacent tables. For example, as shown in Figure 1, the dependency between the _Demographic_ table and the _Credit Card_ table can also be captured and quantified. Further details are provided in Section 5.

### Design choices for ClavaDDPM

We detail how design decisions for ClavaDDPM meet our goals and align with our assumptions.

#### 4.3.1 Relationship-aware clustering

Given the conditional independence between the parent row and its foreign key group (Equation (7)), it is important to model the latent variable \(\) such that it can effectively capture the inter-table correlation within the same foreign key group. In ClavaDDPM, we learn \(\) using Gaussian Mixture Models (GMM) in the weighted joint space of \(\) and \(\), denoted as \(=(;)\), where \(\) is a weight scalar controlling the importance of child and parent tables when being clustered. Concretely, we consider \(k\) clusters, and model the distribution of \(=(;)\) with Gaussian distributed around its corresponding centroid \(\), i.e., \(P()=_{e=1}^{k}P()P()=_{e=1}^{k}_{e}(;_{e},_{e} ).\)

Note that diagonal GMMs are universal approximators, given enough mixtures of Gaussian distributions . Therefore, we can further enforce diagonal covariance, i.e., \(_{c}=(,_{1}^{2},)\), which, being properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given \(\). In addition, the family of Gaussian Process Latent Variable Models (GPLVM)  has been used as an embedding technique to find low-dimensional manifolds that map to a noisy, high-dimensional space. This satisfies our need to learn a stochastic map between the noisy parent space and a condensed latent space. Thus, we can achieve a better trade-off by sacrificing some information fidelity during this quantization process while making the conditional space better shaped.

However, such clustering in the joint space \((;)\) could potentially lead to inconsistency when we create the augmented table \(T_{Y}=(Y;C)\). Though we add a weight \(\) to the parent rows such that child rows with the same parent rows are likely to be assigned to the same cluster, there is still some chance that they end with different clusters. In particular, for each parent row \(y_{j} Y\), its child rows are assigned to different clusters. In ClavaDDPM, we impose a majority voting step to find the most popular cluster label in each foreign key group and assign it to the parent row \(y_{j}\). In practice, the voting agree rates tend to be high, and this can be further enforced by assigning a higher weight to the parent table (increasing \(\)) during GMM clustering. We evaluate the choice of \(\) and voting agree rates in our ablation study in Section 5.3.

While alternative latent learning algorithms could potentially be applied, such as TabSyn  that demonstrated the utility of latent encoding of tabular data with VAE, this work focuses on demonstrating the effectiveness of a simple diagonal Gaussian Mixture Model (GMM) for ClavaDDPM. Our experiments (detailed in Section 5) reveal that ClavaDDPM with a diagonal GMM achieves state-of-the-art results while maintaining low computational overhead. We leave the exploration of more complex latent learning techniques for future work.

#### 4.3.2 Learning with DDPM

Gaussian diffusion backbone.We consider one of the state-of-the-art diffusion models for single tabular data, TabDDPM , as the backbone model. TabDDPM models numerical data with Gaussian diffusion (DDPM ), and models categorical data with multinomial diffusion () with one-hot encoding, and carries out disjoint diffusion processes. However, the modeling of multinomial diffusion suffers significant performance overheads, and poses challenges to guided sampling. Instead, ClavaDDPM uses a single Gaussian diffusion backbone to model both numerical and categorical data in a unified space, where categorical data is mapped to the numerical space through label-encoding. To be specific, for a categorical feature with \(m\) distinct values \(C=\{c_{1},,c_{m}\}\), a label encoding \(E:C\{0,,m-1\}\) maps each unique category \(c_{i}\) to an assigned unique integer value. For a table row \(x=[x_{num},;x_{cat_{i}};]\), where \(x_{num}\) represents all the numerical features and \(x_{cat_{i}}\) represent a categorical feature, we obtain the unified feature by \(x_{uni}=[x_{num};;E(x_{cat_{i}});]\). Based on this encoding, we learn \(p_{}(y,c)\) on the augmented parent table \(T_{Y}=(Y;C)\) through training a Gaussian diffusion model on the unified feature space \((Y_{uni};E(C))\).

Classifier guided synthesis.As defined in Equation (10), we model \(p(x_{j}^{i} c_{j})\) by leveraging classifier-guided sampling of diffusion models, following . In practice, with the sheer power of diffusion models, we jointly model \(p(x c)\) for the entire table without distinguishing \(j\). First, we train a Gaussian diffusion model \(p_{}\) on child table row \(\), with its reverse process modeled with \(}(_{t+1};_{_{t+1}},_{ _{t+1}})\). Then, we train a classifier that classifies cluster labels based on \(\). The conditional reverse process can be approximated by \(_{t}(_{t+1};_{_{t+1}}+ _{_{t+1}}_{_{t+1}},_{_{t+1}})\), where \(_{_{t+1}}=_{_{t+1}}(p_{}( _{t+1}))\) and \(\) is a scale parameter controlling the strength of conditioning. One can regard \(\) as a hyper parameter measuring the trade-off between single-table generation quality and inter-table correlations, to be demonstrated by our ablation study in Section 5.3.

#### 4.3.3 Multi-parent dilemma: matching

Consider the case where some child table \(X\) has two parent tables \(Y_{1},Y_{2}\). Our parent-child synthesis modeling paradigm would lead to two divergent synthetic child tables \(_{1}_{1}\), and \(_{2}_{2}\) Each synthetic table encodes its own parent-child relationship, i.e. the foreign keys. Combining \(_{1}\) and \(_{2}\) so that the synthetic child table contains foreign keys from both parents \(p_{1}\) and \(p_{2}\) is non-trivial, and we call it a multi-parent dilemma. One possible approach is to explicitly constrain the model sample space of \(_{2}\) to be the synthetic data \(_{1}\), as used in PrivLava . However, this approach is not applicable to diffusion models that sample from a continuous space.

We provide a _universal_ solution for all generative models. Consider some real data point \(x\) with two parent rows \(y_{1}^{j}\) and \(y_{2}^{k}\). Ideally, some synthetic data point \(\) following the same distribution as real data point \(x\) should be sampled from \(_{1}^{j},_{2}^{k}\). This can be approximated by finding the intersection of two conditional distributions \(_{1}\) and \(_{2}\). Specifically, we estimate \(\) by finding two synthetic data points \(_{1}_{1}\) and \(_{2}_{2}\), such that \(_{1}_{1}^{j}\) and \(_{2}_{2}^{k}\), and the two points are close enough. We reason as follows: although \(_{1}\) was sampled from \(_{1}\), as long as it is close enough to some other synthetic data point \(_{2}\) sampled from \(_{2}\), then \(_{1}\) will also be within in the high density region of the distribution \(_{2}\), indicating a high probability that \(}_{1}\) follows \(_{1},_{2}\). Symmetrically, the same reasoning also holds for \(_{2}\).

Therefore, we can estimate the true sample data point by \(=f(_{1},_{2})\) if \(_{1}\) is close to \(_{2}\), where \(f\) can simply be an interpolation between two data points in practice. We call this a matching process between two divergent synthetic tables \(_{1}\) and \(_{2}\), and this can be done efficiently using approximate nearest neighbor search. Although we call this a "matching", it does not require finding a one-to-one mapping. Note that this estimate can be further improved by resampling \(_{1}\) and \(_{2}\) and estimate \(\) with more data points rather than just a pair, and the trade-off is a larger computational overhead, and we leave this for future research. Empirically, sampling \(_{1}\) and \(_{2}\) only once is already strong, and an ablation study on the effectiveness of parent matching is in Section 5.3.

Evaluation

We evaluate ClavaDDPM's performance in multi-relational data synthesis, using both single-table and multi-tables utility metrics (including the new long-range dependency). We present an end-to-end comparison of ClavaDDPM to the SOTA baselines, followed by an ablation study for ClavaDDPM.

### Experimental setup

**Real-world datasets.** We experiment with five real-world multi-relational datasets including _California_, _Instacart 05_, _Berka_, _Movie Lens_, and _CCS_. These datasets vary in the number of tables, the maximum depth, the number of constraints, and complexity. Among all, _Berka_, _Movie Lens_, and _CCS_ exhibits complex multi-parent and multi-children structures. We use _Berka_ in our work for ablation study and model anatomy. Details can be found in Appendix C.1.

**Baselines.** We adopt two multi-relational synthesis models in literature as our baselines: PrivLava  as a representative of state-of-the-art marginal-based methods, and SDV  as a statistical method specially designed for multi-relational synthesis. We also introduce two multi-relational synthesis pipelines, SingleT(ST) and Denorm(D), as our additional baselines. SingleT learns and generates each table individually, but it also assigns foreign keys to each synthetic child table accordingly to the real foreign key group size distribution such that the group size information is preserved. Denorm follows the baseline idea that joins table first, but it is hard to join all tables into a single table. Hence, Denorm first applies single-table backbone model to generate the joined table between every parent-child table pair and then split it. For these two pipelines, we use CTGAN  and TabDDPM  as the single-table backbone models, representing the SOTA tabular synthesis algorithms with GAN-based models and diffusion-based models. The details can be found in Appendix C.2.

**Evaluation metrics.** We evaluate the quality of the synthetic data using: 1) _cardinality_ to measure the foreign key group size distribution for the intra-group correlations; 2) _column-wise density estimation (1-way)_ to estimate the density of every single column for all tables; 3) _pair-wise column correlation (\(k\)-hop)_ for the correlations of columns from tables at distance \(k\), e.g., 0-hop refers to columns within the same table and 1-hop refers to a column and another column from its parent or child table; 4) _average \(2\)-way_, which computes the average of all \(k\)-hop column-pair correlations, taking into consideration of both short-range \((k=0)\) and longer-range \((k>0)\) dependencies. For each measure, we report the complement of Kolmogorov-Smirnov (KS) statistic and total variation (TV) distance 3 between the real data and the synthetic data, ranging from 0 (the worst utility) to 1 (the best utility). The reported results are averaged over 3 randomly sampled synthetic data.

We also consider higher-order single-table evaluation metrics for some representative tables as prior work . We include their details and experiemntal results in Appendix D due to space constraints.

All experiments are conducted with an NVIDIA A6000 GPU and \(32\) CPU cores, with a time limit of \(7\) days. If an algorithm fails to complete within the time limit, we report TLE (time limit exceeded). Implementation details and hyperparameter specifics are in Appendix C.3.

### End-to-end evaluation

We conducted multi-table synthesis experiments on five multi-table datasets and report the averaged utility with standard deviation for all algorithms in Table 1. First, the evaluation shows that ClavaDDPM has an overall advantage against all the baseline models in terms of correlation modeling, and is surpassing the baselines by larger margins for longer-range dependencies. e.g. in _Instacart 05_, our model outperforms the best baseline by \(58.29\%\) on 2-hop correlations, and in _Berka_, our model exceeds the best baseline by \(20.24\%\) on \(3\)-hop correlations. For single-column densities and cardinality distributions, ClavaDDPM exhibits a competitive result compared to the state-of-the-art baseline models. We also evaluate ClavaDDPM against baselines on high-order single-table metrics (Appendix D.3), which shows that our model has advantages in preserving data fidelity, generating diverse data, and achieving high machine learning efficacy.

It is worth noting that ClavaDDPM, despite its complexity and capability, is more efficient and robust than some simpler baselines. PrivLava demonstrates strong performance on the California dataset

[MISSING_PAGE_FAIL:9]

**Number of clusters \(k\).** We study the necessity of using latent cluster conditioning: (i) no conditioning with \(k=1\); (ii) many clusters with \(k=1000\) to approximate a direct conditioning on parent rows rather than latent variables. When \(k=1\), the quality of long-range correlation degrades drastically. When \(k=1000\), we still get reasonably strong performance, which showcases ClavaDDPM's robustness. Compared to the default setting (\(k=20\)), the metrics are lower in all of cardinality distribution, single column densities, and column correlations -- proper latent variable learning leads to better results than direct conditioning on parent rows. We also report a new metric _avg agree-rate_, the average of all per-table agree rates for the labels within each foreign key group (Section 4.3.1). This measure highly depends on \(k\), but a higher rate does not always imply a better performance (e.g., k=1 achieves perfect rates). We provide more insights on how it varies with the next parameter. We also conducted finer-grained experiments to examine the effect of \(k\) on model performance, as shown in Appendix D.2.

**Parent scale \(\).** Varying the parent scale parameter \(\) changes the agree-rates as shown in Table 2, but the downstream model performance does not vary too much. This result indicates that the relation-aware clustering process is robust against such factors, and the GMM model is capable of capturing nuances in data patterns. The detailed discussion is in Appendix D.1.

**Classifier gradient scale \(\).** This parameter controls the magnitude of classifier gradients when performing guided sampling, and thus the trade-off between the sample quality and conditional sampling accuracy. Table 2 shows that, when \(=0\), which essentially disables classifier conditioning, the single column densities (1-way) are slightly higher than the default setting. However, it falls short in capturing long-range correlations. When \(=2\), the conditioning is emphasized with a higher weight, which significantly improves the modeling of multi-hop correlations compared to \(=0\) case.

**Comparing with no matching for multi-parent dilemma.**_Berka_ (Figure 1) suffers from the multi-parent dilemma, where the _Disposition_ table has two parent tables, _Account_ and _Client_. Our ablation study switch the table matching technique to a naive merging of two synthetic table (Appendix C.2). The experiment result show that even if trained with the same hyper parameters and model structures, ClavaDDPM with matching is significantly stronger than the no-matching setup in terms of long-range correlations, with \(3\)-hop correlations \(16.70\%\) higher than no-matching.

## 6 Conclusion

We proposed ClavaDDPM as a solution to the intricate problem of synthetic data generation for multi-relational data. ClavaDDPM utilizes clustering on a child table to learn the latent variable that connects the table to its parents, then feeding them to the diffusion models to synthesis the tables. We presented ClavaDDPM's seamless extension to multiple parents and children cases, and established a comprehensive multi-relational benchmark for a through evaluation - introducing a new holistic multi-table metric _long-range dependency_. We demonstrated ClavaDDPM not only competes closely with the existing work on single-table synthesis metrics, but also it outperforms them in ranged (inter-table) dependencies. We deliberately selected the more complex public databases to exhibit ClavaDDPM's scalability, and introduce it as a confident candidate for a broader impact in industry.

We focused on foreign key constraints in this work, and made the assumption that child rows are conditionally independent given corresponding parent rows. This brings three natural follow-up research directions: i) extension to the scenarios where this prior information is not available and these relationships need to be discovered first, ii) further relaxing the assumptions, and iii) inspecting multi-relational data synthesis with other integrity constraints (e.g, denial constraints, general assertions for business rules). Furthermore, we evaluated ClavaDDPM's privacy with the common (in tabular data literature) DCR metric. Nonetheless, we think it is worthwhile to: i) evaluate the resiliency of ClavaDDPM against stronger privacy attacks, and ii) investigate the efficacy of boosting ClavaDDPM with privacy guarantees such as differential privacy. Similarly, the impacts of our design on fairness and bias removal, as another motivating pillar in synthetic data generation, is well worth exploring as future work. We believe the thorough multi-relational modeling formulation we presented in this work, can serve as a strong foundation to build private and fair solutions upon.