# Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction

Tianyu Liu\({}^{1}\)   Qitan Lv\({}^{1}\)   Jie Wang\({}^{1,2}\)1   Shuling Yang\({}^{1}\)   Hanzhu Chen\({}^{1}\)

\({}^{1}\)University of Science and Technology of China

\({}^{2}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

{tianyu_liu, qitanlv, slyang0916, chenhz}@mail.ustc.edu.cn

{jiewangx}@ustc.edu.cn

###### Abstract

Inductive relation prediction (IRP)--where entities can be different during training and inference--has shown great power for completing evolving knowledge graphs. Existing works mainly focus on using graph neural networks (GNNs) to learn the representation of the subgraph induced from the target link, which can be seen as an implicit rule-mining process to measure the plausibility of the target link. However, these methods cannot differentiate the target link and other links during message passing, hence the final subgraph representation will contain irrelevant rule information to the target link, which reduces the reasoning performance and severely hinders the applications for real-world scenarios. To tackle this problem, we propose a novel _single-source edge-wise_ GNN model to learn the **R**ule-induc**Ed **S**ubgraph represent**T**ations (**REST**), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we propose a _single-source_ initialization approach to initialize edge features only for the target link, which guarantees the relevance of mined rules and target link. Then we propose several RNN-based functions for _edge-wise_ message passing to model the sequential property of mined rules. REST is a simple and effective approach with theoretical support to learn the _rule-induced subgraph representation_. Moreover, REST does not need node labeling, which significantly accelerates the subgraph preprocessing time by up to **11.66\(\)**. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST2.

## 1 Introduction

Knowledge graphs are a collection of factual triples about human knowledge. In recent years, knowledge graphs have been successfully applied in various fields, such as natural language processing , question answering  and recommendation systems .

However, due to issues such as privacy concerns or data collection costs, many real-world knowledge graphs are far from completion. Moreover, knowledge graphs are continuously evolving with new entities or triples emerging. This dynamic change causes even the large-scale knowledge graphs, e.g., Freebase , Wikidata  and YAGO3 , to still suffer from incompleteness. Most existing knowledge graph completion models, e.g., RotatE , R-GCN , suffer from handling emerging new entities as they require test entities to be observed in training time. Therefore, inductive relation prediction, which aims at predicting missing links in evolving knowledge graphs, has attracted extensive attention.

The key idea of inductive relation prediction on knowledge graphs is to learn _logical rules_, which can capture co-occurrence patterns between relations in an entity-independent manner and can thus naturally generalize to unseen entities [11; 12]. Some existing models, e.g., AMIE+, Neural LP, explicitly mine logical rules for inductive relation prediction with good interpretability, while their performances are limited due to the large searching space and discrete optimization[15; 16]. Recently, some **subgraph-based** methods, e.g., GraIL, TACT, CoMPILE, have been proposed to implicitly mine logical rules by reasoning over the subgraph induced from the target link.

However, there are still some irrelevant rules within the subgraph. Considering the rule body and the rule head as a cycle, the relevant rules are cycles that pass the target link. As illustrated in Figure 1, \(u v e_{4} u\) and \(u v e_{2} e_{1} u\) are relevant rules as they pass the target link, while \(e_{1} e_{3} e_{2} e_{1}\) are irrelevant rules as they do not contain the target link. Existing methods cannot differentiate the target link and other links during message passing. Thus, they will mine plenty of irrelevant rules and encode them into the final subgraph representation, which makes the model prone to over-fitting and severely hinders reasoning performance.

In this paper, we propose a novel **single-source edge-wise** GNN model to learn the **R**ule-inducEd **S**ubgraph represent**T**ations (**REST**), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we observe that the information flow originating from a unique edge and returning to this edge will form a cycle automatically. Consequently, the information flow originating from the target link can encode the relevant logical rules. Inspired by this observation, we propose a **single-source** initialization approach to assign a nonzero initial embedding for the target link according to its relation and zero embeddings for other links. Then we propose several RNN-based functions for **edge-wise** message passing to model the sequential property of mined rules. Finally, we use the representation of the target link as the final subgraph representation.

We theoretically show that with appropriate message passing functions, REST can learn the rule-induced subgraph representation for reasoning. Notably, REST avoids the heavy burden of node labeling in subgraph preprocessing, which significantly accelerates the time of subgraph preprocessing by up to **11.66\(\)**. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST.

## 2 Related Work

Existing works for IRP can be mainly categorized into **rule-based** methods and **subgraph-based** methods. While rule-based methods explicitly learn logical rules in knowledge graphs, subgraph-based methods implicitly mine logical rules by learning the representation of subgraphs. Moreover, we discuss some graph neural network methods that reason over the whole graph.

**Rule-based methods.** Rule-based approaches mine logical rules that are independent of entities and describe co-occurrence patterns of relations to predict missing factual triples. Such a rule consists of a head and a body, where a head is a single atom, i.e., a fact in the form of _Relation(head entity, tail entity)_, and a body is a set of atoms. Given a head \(R(y,x)\) and a body \(\{B_{1},B_{2},,B_{n}\}\), there is a rule \(R(y,x) B_{1} B_{2} B_{n}\). The rule-based methods have been studied for a long time in Inductive Logic Programming , yet traditional approaches face the challenges of optimization and scalability. Recently, Neural LP  presents an end-to-end differentiable framework that enables modern gradient-based optimization techniques to learn the structure and parameters of logical rules. DRUM analyzes Neural LP from the perspective of low-rank tensor approximation and uses bidirectional RNNs to mine more accurate rules. Moreover, for automatically learning rules from large knowledge graphs, RLvLR  proposes an effective rule searching and pruning strategy, which shows promising results on both scalability and accuracy for link prediction. However, these explicit rule-based methods lack expressive power due to rule-based nature, and cannot scale to large knowledge graphs as well.

Figure 1: Relevant and irrelevant rules.

**Subgraph-based methods.** Subgraph-based methods extract a local subgraph around the target link and use GNNs to learn subgraph representation to predict link existence. Such a subgraph is usually induced by the neighbor nodes of the target link, which encodes the rules related to the target link. GraIL is the first subgraph-based inductive relation prediction model. It defines the _enclosing subgraph_ as the graph induced by all the nodes in the paths between two target nodes. After labeling all the nodes with _double radius vertex labeling_, it employs R-GCNs to learn subgraph representation. CoMPILE extracts directed enclosing subgraphs to handle the asymmetric / anti-symmetric patterns of the target link. TACT converts the original enclosing subgraph into a relational correlation graph, and proposes a relational correlation network to model different correlation patterns between relations. More recently, SNRI extracts enclosing subgraphs with complete neighboring relations to consider neighboring relations for reasoning. ConGLR converts the original enclosing subgraph into a context graph to model relational paths. However, these methods cannot differentiate the target link and other links during message passing. Therefore, the GNNs will mine plenty of irrelevant rules for other links and encode them into subgraph representation, which reduces the accuracy of reasoning.

**Graph neural network for link prediction.** Some methods use GNNs to reason over the whole graph rather than a subgraph for inductive relation prediction. INDIGO converts the KG into a node-annotated graph and fully encodes it into a GNN. NBFNet generalizes Bellman-Ford algorithm and proposes a general GNN framework to learn path representation for link prediction. MorsE  considers transferring entity-independent meta-knowledge by GNNs. While these methods share some spirit with subgraph-based methods, they are essentially different with subgraph-based methods. These methods need to reason over the whole graph for a test example, while subgraph-based methods only need to reason over a subgraph. Meanwhile, these methods tend to predict entities rather than relations for a query, while subgraph-based methods tend to predict relations, as the subgraph only needs to be extracted once for relation prediction. As these methods benefit from a larger number of negative sampling, we do not take them into comparison.

## 3 Problem Definition

We define a training graph as \(_{}=(_{},_{},_{})\), where \(_{}\), \(_{}\), and \(_{}_{}_{ }_{}\) are the set of entities, relations and triples during training, respectively. We aim to train a model such that for **any** graph \(^{}=(^{},^{},^{ })\) whose relations are **all** seen during training (i.e., \(^{}_{}\)), the model can predict missing triples in \(^{}\), i.e., \((?,r_{t},v),(u,?,v),(u,r_{t},?)\), where \(u,v^{}\) and \(r_{t}^{}\). We denote the set of any possible entities as \(\{\}\) and the set of knowledge graphs whose relation set \(\) are subset of \(_{}\) as \(\{\}_{}\). The model we would like to train is a score function \(f:\{\}_{}\{\}_{ }\{\}\), \((^{},u,r_{t},v) f(^{},u,r_{t},v)\), where \(^{}=(^{},^{},^{ }),^{}_{}\) and \(u,v^{}\). For a query triple \((u,r_{t},?)\), we enumerate valid candidate tail entities \(v^{}\) and use the model to get the score \(s^{}\) of this triple \((u,r_{t},v^{})\). We call the query triple \((u,r_{t},v)\) as _target link_ and \(u,v\) as _target nodes_, respectively.

## 4 Methodology

In this section, we describe the architecture of the proposed REST in detail. Following existing subgraph-based methods, we first extract a subgraph for each query triple. Then we apply **single-source initialization** and **edge-wise message passing** to update edge features iteratively. Finally, the representation of the target link is used for scoring. REST organizes the two methods in a unified framework to perform inductive relation prediction. Figure 2 gives an overview of REST.

### Subgraph Extraction

For a query triple \((u,r_{t},v)\), the subgraph around it contains the logical rules to infer this query, thus we extract a local subgraph \(_{u,r_{t},v}\) to implicitly learn logical rules for reasoning. Specifically, we first compute the \(k\)-hop neighbors \(_{k}(u)\) and \(_{k}(v)\) of the target nodes \(u\) and \(v\). Then we define _enclosing subgraph_ as the graph induced by \(_{k}(u)_{k}(v)\) and _unclosing subgraph_ as the graph induced by \(_{k}(u)_{k}(v)\). Note that the subgraph extraction process of our REST omits the node labeling, as node features are unnecessary in **edge-wise** message passing, which significantly reduces the time cost of subgraph preprocessing.

### Single-source Initialization

Single-source initialization is a simple and effective initialization approach, which initializes a nonzero embedding to the query triple according to \(r_{t}\) and zero embeddings for other triples. Specifically, the embeddings of links and nodes within \(_{u,r,v}\) are initialized as follows:

\[^{0}_{x,y,z}=}(x,y,z) _{y} =_{y},&(x,y,z)=(u,r_{t},v)\\ ,&(x,y,z)(u,r_{t},v)\] (1) \[^{0}_{v} = for\; v,\]

where \(^{0}_{x,y,z}\) and \(^{0}_{v}\) are the initial representation of edge \((x,y,z)\) and node \(v\), respectively. \(\) is the indicator function to differentiate the target link and other links. \(\) is Hadamard product. Note that the representation of nodes is used as temporary variables in edge-wise message passing. With this initialization approach, we ensure the relevance between mined rules and the target link.

### Edge-wise Message Passing

After initializing all the edges and nodes, we perform edge-wise message passing to encode all relevant rules into the final subgraph representation. Specifically, each iteration of edge-wise message passing consists of three parts, (1) applying message function to every link, (2) updating node features by aggregating message and (3) updating edge features by temporary node features, which are described as follows:

\[^{k}_{x,y,z}=(^{k-1}_{x},^{k-1}_{ x,y,z},_{y})=(^{k-1}_{x}^{1}_{y})( ^{k-1}_{x,y,z}^{2}_{y})\] (2)

\[^{k}_{z}=}{}( ^{k}_{x,y,z})=_{(x,y,z)}^{k}_{x,y,z}\] (3)

\[^{k}_{x,y,z}=(^{k}_{x},^{k-1}_{x, y,z})=^{k}_{x}^{k-1}_{x,y,z}\] (4)

Here, \(,,,^{1},^{2}\) are binary operators which denote a function to parameterize. \(\) denotes the large size operator of \(\). \(^{k}_{z}\) and \(^{k}_{x,y,z}\) respectively represent the feature of node \(z\) and link \((x,y,z)\) after \(k\) iterations of edge-wise message passing. We visualize the comparison between conventional message passing framework developed by GraIL and proposed edge-wise message passing framework in Figure 3.

Figure 2: An overview of REST. REST organizes the _single-source_ initialization method and the _edge-wise_ message passing method in a unified framework to learn relevant rules representations within the subgraph for the target link. Different relevant rules are shown in different colors in part 3.

### RNN-based Functions

Message passing functions in existing works use order-independent binary operators such as ADD and MUL, which cannot model the sequential property of rules and lead to incorrect rules. To tackle this problem, we introduce several RNN-based methods as message passing functions.

**Message Functions.** For the edge-wise message passing process, each iteration REST takes in \(_{x}^{k-1},_{x,y,z}^{k-1}\) and \(_{y}\) to form a message. We modify GRU as message function as follows:

\[_{k}&=_{g}(_{ ,1}^{k}_{y}_{x,y,z}^{k-1}+_{,2 }^{k}_{x}^{k-1}+_{}^{k})\\ _{k}&=_{g}(_{,1}^{k} _{y}_{x,y,z}^{k-1}+_{,2}^{k}_{x}^{k-1}+_{}^{k})\\ c_{k}&=_{h}(_{c,1}^{k}_{y} _{x,y,z}^{k-1}+_{c,2}^{k}(_{k}_ {x}^{k-1}))\\ _{x,y,z}^{k}&=_{k} c_{k}+(1- _{k})_{x}^{k-1}\] (5)

Here, \(_{k}\) is the update gate vector, \(_{k}\) is the reset gate vector and \(c_{k}\) is the candidate activation vector. The operator \(\) denotes the Hadamard product, \(_{g}\) denotes Sigmoid activation function and \(_{h}\) denotes Tanh activation function. During each iteration of message passing, we only use GRU once, therefore \(k\)-layer message passing includes \(k\) GRUs, which can model sequence with length \(l k\).

**Aggregate Functions.** The aggregate function aggregates messages for each node from its neighboring edges. Here, we use simplified PNA to consider different types of aggregation.

\[_{z,1}^{k}&=}{mean}(_{x,y,z}^{k}),\;_{z,2}^{k}= }{max}(_{x,y,z}^{k}),\\ _{z,3}^{k}&=}{min}(_{x,y,z}^{k}),\;_{z,4}^{k}=}{std}(_{x,y,z}^{k}),\\ _{z}^{k}&=_{agg}^{k}[ _{z,1}^{k};_{z,2}^{k};_{z,3}^{k};_{z,4}^{k}; _{z}^{k-1}]\] (6)

Here, \([;]\) denotes the concatenation of vectors, \(_{agg}^{k}\) denotes the linear transformation matrix in the \(k\)-th layer.

**Update Functions.** The update function is used to update the edge feature. We propose to update the edge feature with LSTM. Specifically, LSTM needs three inputs: a hidden vector, a current

Figure 3: Comparison between conventional message passing framework developed by GraIL and our REST. First, REST initializes node and edge features with single-source initialization. Then, REST employs Update function to update edge features. Finally, REST directly uses the embedding of the target link as the final subgraph representation, rather than the pooling of all node embeddings.

input vector and a cell vector. We use \(_{x}^{k}\) as the hidden vector and \(_{x,y,z}^{k-1}\) as the current input vector. Moreover, we expect that each edge can _differentiate the target link_ during message passing, which requires each edge to specify the query information. Therefore, we initialize each edge with another _query_ feature as the cell vector. All the edges are initialized with the same query embedding \(_{r}^{q}\) related to the query relation \(r\).

\[_{x,y,z}^{0}=_{r}^{q}\] (7)

Then the update function can be described as follows:

\[_{x,y,z}^{k},_{x,y,z}^{k}=(_{x,y, z}^{k-1},_{x,y,z}^{k-1},_{x}^{k})\] (8)

After updating the edge feature by \(k\) iterations of edge-wise message passing, we output \(_{u,r,v}^{k}\) as the subgraph representation. Then we use a linear transformation and an activation function to get the score of the target link.

\[f(u,r_{t},v)=(_{s}_{u,r_{t},v}^{k}+_{s})\] (9)

## 5 Analysis

In this section, we theoretically analyze the effectiveness of our REST. We first define the rule-induced subgraph representation, which utilizes encoded relevant rules to infer the plausibility of the target link. Then we show that our REST is able to learn such a rule-induced subgraph representation for reasoning.

### Rule-induced Subgraph Representation Formulation

Our rule-induced subgraph representation aims to encode all relevant rules into the subgraph representation for reasoning. Therefore, we can define the rule-induced subgraph representation as the aggregation of these relevant rules:

\[_{u,r_{t},v}=_{c}_{c},\] (10)

where \(\) denotes the set of all possible relevant rules within \(_{u,r_{t},v}\) and \(_{c}\) is the representation of a relevant rule \(c\). Following the idea of Neural LP to associate each relation in the rule with a weight, we model the representation of a rule as a function of its relation set. Therefore, we give the definition of rule-induced subgraph representation.

**Definition 1** (Rule-induced subgraph representation.): _Given a subgraph \(_{u,r_{t},v}\), its rule-induced subgraph representation is defined as follows:_

\[_{u,r_{t},v}=_{i=1}^{k}_{(u, r_{t},v)}_{(v,y_{0},x_{0}) }..._{(x_{i-3},y_{i-2},u)\\ i}_{i1}_{r_{t}}_{i2}_{y _{0}}..._{ii}_{y_{i-2}}\] (11)

_where \(i\) denotes the length of the cycle, \(_{y}\), is the representation of relation \(y_{i}\), \((x_{i},y_{i},x_{i-1})\) is an existing triple in \(_{u,r_{t},v}\). \(\{(u,r_{t},v),(v,y_{0},x_{0}),...,(x_{i-3},y_{i-2},u)\}\) is a cycle at length \(i\)._

Note that \(\) and \(\) denote binary aggregation functions. Intuitively, rule-induced subgraph representation captures all relevant rules within the subgraph and is expressive enough for reasoning.

### Rule-induced Subgraph Representation Learning

Here, we show that our REST can learn such a rule-induced subgraph representation. First, we show this in a simple case.

**Theorem 1**: _Single-source edge-wise GNN can learn rule-induced subgraph representation if \(=+,=+,=+\), \(^{1}=,^{2}=\). i.e., there exists nonzero \(_{i,j}\) such that_

\[_{u,r_{t},v}^{k}=_{i=1}^{k},v)}_ {(v,y_{0},x_{0})}..._{(x_{i-3},y_{i-2},u)}}_{i}_{i1}_{r_{ t}}_{i2}_{y_{0}}..._{ii}_{y_{i-2}}\] (12)We prove this in Appendix A. This theorem states that our REST can learn the rule-induced subgraph representation in the basic condition. Then we generalize this theorem to a general version.

**Theorem 2**: _Single-source edge-wise GNN can learn rule-induced subgraph representation if \(=,=,=,^{1}=,^{2}=\), where \(\) and \(\) are binary operators that satisfy \(0 a=a,0 a=0\). i.e., there exists nonzero \(_{i,j}\) such that_

\[_{u,r_{t},v}^{k}=_{i=1}^{k},v )}_{(v,y_{0},x_{0})}..._{(x_{i-3},y_{i-2},u)}}_{i}_{i1} _{r_{t}}_{i2}_{y_{0}}..._ {ii}_{y_{i-2}}\] (13)

We prove this in Appendix A. Intuitively, we can get this theorem by replacing \(+,\) with \(,\). The key step to learn rule-induced subgraph representation is to ensure \(_{x,y,z}^{0} 0\) if and only if \((x,y,z)=(u,r_{t},v)\). Existing models[11; 12] do not satisfy this requirement, as they initialize both the target link and the other links with nonzero embeddings. Therefore, their final subgraph representations contain irrelevant rule terms, which leads to suboptimal results. On the contrary, we show that with appropriate message passing functions, REST learns rule-induced subgraph representation. As the rule-induced subgraph representation encodes all relevant rules within the subgraph, REST is expressive enough to infer the plausibility of any reasonable triple, while it eliminates the negative influence of irrelevant rules.

Our analysis gives some insight of IRP methods. First, eliminating noises within the extracted subgraph is crucial for subgraph-based methods. While existing methods focus on data level to extract ad-hoc subgraphs, our model proposes a simple way for denoising at the model level, i.e., single-source edge-wise message passing. Second, labeling tricks such as single-source initialization can effectively improve the model performance. Last but not least, the idea of learning links is especially important in IRP task, as links play a vital role in reasoning.

## 6 Experiments

In this section, we first introduce the experiment setup including datasets and implementation details. Then we show the main results of REST on several benchmark datasets. Finally, we conduct ablation studies, case studies and further experiments.

### Experiment Setup

**Datasets and Implementation Details** We conduct experiments on three inductive benchmark datasets proposed by GraIL, which are dervied from WN18RR, FB15K-237, and NELL-995. For inductive relation prediction, the training set and testing set should have no overlapping entities. Details of the datasets are summarized in Appendix B. We use PyTorch and DGL to implement our REST. Implementation Details of REST are summarized in Appendix C.

### Main Results

We follow GraIL to rank each test triple among 50 other randomly sampled negative triples. We report the Hits@10 metric on the benchmark datasets. Following the standard procedure in prior work , we use the filtered setting, which does not take any existing valid triples into account at ranking. We demonstrate the effectiveness of the proposed REST by comparing its performance with both rule-based methods including Neural LP , DRUM  and RuleN  and subgraph-based methods including GraIL , CoMPILE , TACT, SNRI and ConGLR . We run each experiment five times with different random seeds and report the mean results in Table 1.

From the Hits@10 results in Table 1, we make the observation that our model REST significantly outperforms existing methods on 12 versions of 3 datasets. Specifically, our REST can outperform rule-based baselines, including Neural LP, DRUM and RuleN by a large margin. And compared with existing subgraph-based methods, e.g., GraIL, CoMPILE, TACT, SNRI and ConGLR, REST has achieved average improvements of 17.89%, 9.35%, 13.76%; 16.23%, 8.18%, 13.04%; 13.58%, 8.06%, 8.96%; 10.89%, 4.55%,"-" and 5.58%, 5.82%, 5.1% on three datasets respectively. As RESTonly assigns the embedding of the target link, these improvements demonstrate the effectiveness of our REST via distilling all relevant rules within the subgraph.

### Ablation Study

We conduct ablation studies to validate the effectiveness of proposed single-source initialization and edge-wise message passing. We show the main results of ablation studies in Table 2.

**Single-source initialization.** Single-source initialization is vital for learning rule-induced subgraph representation. To demonstrate the effectiveness of single-source initialization, we perform another _full initialization_ method as a comparison, which initializes all edges according to their relations. As illustrated in Table 2, we can find that single-source initialization is significant for capturing relevant rules for reasoning. Without single-source initialization, the performance of REST will exhibit a significant decrease, e.g., from 92.61 to 68.26 in NELL-995 v4. This result exhibits the effectiveness of single-source initialization.

**Edge-wise message passing.** To demonstrate the necessity of proposed RNN-based functions, we conduct ablation studies on various combinations of message functions, including SUM, MUL, and GRU, as well as update functions, including LSTM and MLP. These functions are defined as follows:

\[:&_{x,y,z}^{k}= _{x}^{k-1}+_{x,y,z}^{k-1}+_{y}\\ :&_{x,y,z}^{k}=_{x}^ {k-1}_{x,y,z}^{k-1}_{y}\\ :&_{x,y,z}^{k}=_{e }[_{x,y,z}^{k-1};_{x,y,z}^{k-1};_{x}^{k}]\\ &_{x,y,z}^{k}=_{q}[_{x,y,z}^{k-1}; _{x,y,z}^{k-1};_{x}^{k}]\] (14)

In general, REST benefits from RNN-based functions, as they can capture the sequential properties of rules. Using order-independent binary operators, such as ADD and MUL, leads to a decline in performance across all datasets, as they cannot differentiate correct and incorrect rules.

    & &  &  &  \\   & & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 \\   & Neural LP & 74.37 & 68.93 & 46.18 & 67.13 & 52.92 & 58.94 & 52.90 & 55.88 & 40.78 & 78.73 & 82.71 & 80.58 \\  & DRUM & 74.37 & 68.93 & 46.18 & 67.13 & 52.92 & 58.73 & 52.90 & 55.88 & 19.42 & 78.55 & 82.71 & 80.58 \\  & RuleN & 80.85 & 78.23 & 53.39 & 71.59 & 49.76 & 77.82 & 87.69 & 85.60 & 53.50 & 81.75 & 77.26 & 61.35 \\   & GraIL & 82.45 & 78.68 & 58.43 & 73.41 & 64.15 & 81.80 & 82.83 & 89.29 & 59.50 & 93.25 & 91.41 & 73.19 \\  & CoMPLLE & 83.60 & 79.82 & 60.69 & 75.49 & 67.64 & 82.89 & 84.67 & 84.44 & 58.38 & 93.87 & 92.77 & 75.19 \\  & TACT & 84.04 & 81.63 & 67.97 & 76.56 & 65.76 & 83.56 & 85.20 & 88.69 & 79.80 & 88.91 & 94.02 & 73.78 \\  & SNRI & 87.23 & 83.10 & 67.31 & 83.32 & 71.79 & 86.50 & 89.59 & 89.39 & - & - & - & - \\  & ConGLR & 85.64 & 92.93 & 70.74 & 92.90 & 68.29 & 85.98 & 88.61 & 89.31 & 81.07 & 94.92 & 94.36 & 81.61 \\    & REST(**ours**) & **96.28** & **94.56** & **79.50** & **94.19** & **75.12** & **91.21** & **93.06** & **96.06** & **88.00** & **94.96** & **96.79** & **92.61** \\   

Table 1: Hits@10 results on the inductive benchmark datasets extracted from WN18RR, FB15k-237 and NELL-995. The results of Neural LP, DURM, RuleN, GraIL, CoMPLE and ConGLR are taken from the paper .

    &  &  &  \\   & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 & v1 & v2 & v3 & v4 \\  REST & 96.28 & 94.56 & 79.50 & 94.19 & 75.12 & 91.21 & 93.06 & 96.06 & 88.00 & 94.96 & 96.79 & 92.61 \\  Full Initialization & 92.55 & 90.70 & 68.76 & 79.49 & 71.71 & 79.29 & 89.25 & 91.22 & 83.00 & 86.13 & 94.54 & 68.26 \\ \(\) & \(}}}}\)3.73 & \(}}}}\)3.86 & \(}}}\)10.74 & \(}}}\)4.70 & \(}}}\)4.11 & \(}}}\)2.3 & \(}}}\)4.84 & \(}}}\

[MISSING_PAGE_FAIL:9]

extraction by up to \(11.66\), which significantly decreases the time cost of subgraph extraction. Experiments demonstrate that our proposed REST outperforms existing state-of-the-art methods on inductive relation prediction benchmarks.

Future WorkFor future work, we target at enhancing the scalability of our REST to conduct reasoning on large-scale knowledge graphs. Moreover, REST can serve as a complementary reasoning model to help large language models conduct reasoning with promising and interpretable results. Hopefully, REST will facilitate the future development of reasoning ability.