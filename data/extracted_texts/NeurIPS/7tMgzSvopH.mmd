# Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark

Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark

 Lukasz Augustyniak

WUST (Wroclaw University of Science and Technology)

lukasz.augustyniak@pwr.edu.pl

&Szymon Wozniak

Brand24 AI

&Marcin Gruza

Brand24 AI, WUST

&Piotr Gramacki

Brand24 AI, WUST

&Krzysztof Rajda

Brand24 AI, WUST

&Mikolaj Morzy

Poznan University of Technology

&Tomasz Kajdanowicz

WUST

###### Abstract

Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.

## 1 Introduction

Consider a hotel booking service that allows its customers to post reviews. You have found just the perfect accommodation to stay for a couple of days with your family, but you browse through the reviews section of the website to check the experiences of former guests. Suddenly, you encounter a review in Polish: _"hotel jak hotel, moglo byc gorcej."_ This review has the following sentiment scores1: \(s_{neg}=0.44,s_{neu}=0.44,s_{pos}=0.12\). Intrigued by the ambiguity of scores, you translate the review into English: _"hotel like a hotel, all in all, it could have been worse,"_ which is scored as \(s_{neg}=0.80,s_{neu}=0.16,s_{pos}=0.04\). Apparently, the stereotypically pessimistic Polish outlook on life gets lost in translation. The next review is written in Czech: _"ok, ale nic zajinaveho"_ with scores \(s_{neg}=0.32,s_{neu}=0.54,s_{pos}=0.14\). After translating into English (_"ok, but nothing interesting"_) the sentiment classification model scores the review as negative (\(s_{neg}=0.50,s_{neu}=0.37,s_{pos}=0.13\)). After your stay, you decide to add a very positive review of the hotel (_"it was a killer place to stay"_, \(s_{neg}=0.03,s_{neu}=0.05,s_{pos}=0.92\)). You would be very surprised to learn that the Czechs would be quite confused about your opinion (_"to bylo vrazednemisto k popyta_*, \(s_{neg}=0.51,s_{neu}=0.09,s_{pos}=0.39\)), while the Poles would stay away from the hotel at all costs ("_to byto zabojcze majcse na pobyt_", \(s_{neg}=0.78,s_{neu}=0.09,s_{pos}=0.13\)).

multilingual services become ubiquitous in the modern global economy. As more websites begin to offer automatic translation of content, users do not bother to express themselves in the _lingua franca_ of the Web, writing instead in their native languages. Despite impressive advancements in automatic translation, many NLP tasks are still difficult in the multilingual setting. And sentiment classification is among the most challenging. The expression of sentiment is highly culture-dependent . The emotional valence of individual words, the presence of specific phrasemes, and the expectations around sentiment values make sentiment classification across languages a demanding task.

Models performing sentiment classifications have to cope with two independent sources of variance in the input data: cultural expressions of sentiment and errors in automatic translations. In addition, the productization of sentiment classification leads to several engineering choices which influence the efficiency of the model:

* _single multilingual model vs. dedicated monolingual models_: deploying a single model in a production environment is much easier than orchestrating an ensemble of models,
* _training vs. fine-tuning_: sentiment classification model can be trained from scratch, both in the multi- and monolingual regimes, and the alternative is the fine-tuning of a general-purpose pre-trained language model with sentiment data,
* _transfer learning between domains_: a sentiment classification model trained for a specific domain (e.g., book reviews) can be transferred to another domain (e.g., hotel reviews) under the assumption that sentiment expressions in a given language remain independent of the subject of sentiment,
* _transfer learning between languages_: one may hypothesize that related languages can utilize similar sentiment expression mechanisms regarding grammar, punctuation, and vocabulary. In theory, it is possible to use training data in language \(L_{1}\), fine-tune a multilingual model with this data, and as a result of fine-tuning, improve the performance of the model on language \(L_{2}\), provided languages \(L_{1}\) and \(L_{2}\) are sufficiently similar.

Working with multilingual datasets and models opens another opportunity. Most NLP research focuses on 20 major languages. Many languages native to significant human populations are not adequately studied. Although the term _low-resource language_ is not precisely defined and can be understood in terms of computerization, privilege, the abundance of resources, or density , the existence of a large chasm between languages with respect to linguistic resources is apparent. Our work aims at supporting low-resource languages in performant sentiment classification. Finally, there is a lively debate in the scientific community about the inherent ability of neural models to handle general linguistic phenomena. This paper tries to answer this question in the domain of sentiment classification.

Contribution presented in this paper is threefold:

* _multilingual corpus_: we present the largest multilingual collection of sentiment classification datasets consisting of 79 high-quality datasets covering 27 languages. The collection of the corpus and detailed descriptions of collected datasets are presented in Section 3.
* _multi-faceted benchmark_: the dataset is supplemented with a benchmark containing detailed run statistics of hundreds of experiments representing different training and testing scenarios. All details relevant to the benchmark are presented in Section 4.
* _library for dataset access_: all datasets in the multilingual corpus are publicly available via a library compatible with the HuggingFace library, along with the ability to filter and select datasets, verify their licenses, etc2. 
## 2 Linguistic typology

The similarity of languages and the main aspects of their differences is the field of study of language typology. The differences between languages can be phonological (differences in sounds used by languages), syntactic (differences in language structures), lexical (differences in vocabulary), and theoretical (differences characterized as general properties of languages). Linguistic typology analyzes the current state of languages and is often contrasted with genealogical linguistics. The latter is concerned with historical relationships between languages established via historical records or with the help of comparative linguistics. The main focus of genealogical linguistics resolves around _language families_ and _language genera_. The term _language family_ refers to a group of languages sharing pedigree from a common ancestral language (the _proto-language_). As of today, linguists define over 7000 languages categorized into 150 families . The largest families of languages include Indo-European, Sino-Tibetan, Turkic, Afro-Asiatic, Nilo-Saharan, Niger-Congo, and Eskimo-Aleut . Main families are further divided into branches called _genera_. Examples of genera within the Indo-European family of languages include Slavic, Romance, Germanic, and Indic. This division of language families into genera closely follows the genetic family of humankind as attested by DNA similarity . Some language families do not produce distinctive genera but form dialect continua defined by mutual intelligibility. Finally, an important concept is that of a _sprachbund_, a geographical area occupied by languages sharing linguistic features. The linguistic similarities within a sprachbund result from cultural exchange and geographical contact rather than by chance or common origin, as described by Thomason and Kaufman .

Languages can be described using hundreds of linguistic features. World Atlas of Language Structures  lists almost 200 different features. Since our work focuses on sentiment classification, we select 10 features that seem to be the most relevant to the task of sentiment expression. These features are:

1. _definite article_: the morpheme associated with nouns and used to code the uniqueness or definiteness of a concept; almost half of the languages do not use the definite article.
2. _indefinite article_: the morpheme used together with nouns to signal that the related concept is unknown to the hearer; half of the languages do not use the indefinite article, some languages use a separate article, and some use the numeral "one" as the indefinite article.
3. _number of cases_: morphological cases are a common way to express various relationships between words; human languages vary greatly in the number of cases used by a language.
4. _order of subject, verb, and object_: some languages have the strict ordering of words, while languages that convey semantics through inflection may be much looser with the ordering; half of all languages use the SOV (subject-object-verb) ordering, one third uses the SVO (subject-verb-object) ordering, and a small fraction of languages use the remaining VSO, VOS, OVS, and OSV orderings. Interestingly, around 13% of world languages do not have any fixed word order.
5. _negative morphemes_: negative morpheme is used to signal clausal negation in declarative sentences; this is usually achieved using a negative affix or a negative particle.
6. _polar questions_: a polar question is a question with only yes/no answers; these questions can be built using question particles, interrogative morphology, or intonation only.
7. _position of the negative morpheme_: languages differ by the position of the negative morpheme in relation to subjects and objects, with many variants such as SNegVO, NegSVO, SVNegO, obligatory and optional double negations, etc.
8. _prefixing vs. suffixing_: languages differ significantly in their use of prefixes versus suffixes in inflectional morphology.
9. _coding of nominal plurals_: two major types of plural coding are present in languages, either by changing the morphological form of the noun or by using a plurality indicator morpheme somewhere in the noun phrase.
10. _grammatical genders_: there is significant variability among languages with respect to the number of grammatical genders, some languages do not use the concept at all, and some languages may have 5 or more grammatical genders.

All language features mentioned above are available as filtering features in our library. Thus, when training a sentiment classifier using our dataset, one may download different facets of the collection. For instance, one can download all datasets in Slavic languages in which polar questions are formed using the interrogative word order (Listing 1) or download all datasets from the Afro-Asiatic language family with no morphological case-making (Listing 2).

Datasets

### Quality criteria

The initial pool of sentiment datasets has been gathered using extensive search and consisted of 345 datasets found through Google Scholar, GitHub repositories, and the HuggingFace datasets library. This initial pool of datasets has been manually filtered based on the following set of quality assurance criteria:

1. _strong annotations_: we have rejected datasets containing weak annotations (e.g., datasets with labels based on emoji occurrence or generated automatically through classification by machine learning models) due to an extensive amount of noise .
2. _well-defined annotation protocol_: we have rejected datasets without sufficient information about the annotation protocol (e.g., whether annotation was manual or automatic, number of annotators) to avoid merging datasets with contradicting annotation instructions.
3. _numerical ratings_: we have accepted datasets with numerical ratings, mapping Likert-type 5-point scales into three class sentiment labels as follows: ratings 1 and 2 were mapped to _negative_, rating 3 was mapped to _neutral_, and ratings 4 and 5 were mapped to _positive_.
4. _three classes only_: we have rejected datasets annotated with binary sentiment labels as their performance in three class settings was unsatisfactory.
5. _monolingual datasets_: when a dataset contained samples in multiple languages, we opted to divide it into independent datasets in constituent languages.

### Pre-processing of datasets

Despite quality assurance criteria described in Section 3.1, the datasets still contained conflicting entries, i.e., duplicated records with different sentiment labels. We have cross-referenced all datasets to identify conflicts and have made the data coherent using majority-label voting. Finally, labeling and rating schemes of all datasets have been mapped to a 3-class scheme with _negative_, _neutral_, and _positive_ labels only. For datasets with emotional annotations, we mapped positive emotions (joy, happiness) into positive sentiment and negative emotions (fear, sadness, disgust, anger) into negative sentiment. Texts with ambiguous emotions like anticipation and surprise were discarded. This pre-processing pipeline resulted in 79 datasets containing \(6\,164\,762\) text samples. Most of the datasets are in English (\(2\,330\,486\) samples across 17 datasets), Arabic (\(932\,075\) samples across 9 datasets), and Spanish (\(418\,712\) samples across 5 datasets). The datasets represent four different domains: social media (44 datasets), reviews (24 datasets), news (5 datasets), and others (6 datasets). In addition, all datasets were processed by the cleanlab library to produce a self-confidence label-quality score for each data point.

Exhaustive testing of all configurations of the benchmark is unfeasible, but the total lack of baseline is unacceptable, either. We have decided to test the benchmark dataset by manually compiling a strong baseline. The main rationale behind this effort was the lack of coherence in annotation guidelines between considered datasets. Our baseline is built using strict annotation guidelines constructed iteratively over annotation batches, resulting in highly coherent annotations. The baseline dataset consists of \(3\,911\) short text samples (trimmed to 350 characters) in Polish and English, annotated independently by 3 annotators fluent in these languages. Baseline texts cover multiple domains, such as social media, news sites, blogs, and Internet forums. For each instance, the majority label assigned by the annotators has been stored. The inter-rater agreement is \(=0.665\) (average Cohen's kappa between three pairs of annotators) and \(=0.666\) (Krippendorff's alpha).

One may think of the baseline dataset as representative of current sentiment model training, where researchers have to build domain-aligned corpora of various languages and annotate them. The comparison of results between the benchmark and the baseline datasets is a proxy of the performance trade-offs should one decide to use our benchmark dataset to train domain and language-specific sentiment classifiers.

## 4 Multi-faceted benchmark

As we have explained in Section 1, the deployment of a multilingual sentiment classifier can be evaluated using several criteria leading to different architecture choices. Thus, we do not publish a single benchmark, but we aggregate the results along several dimensions. The benchmark available at [https://huggingface.co/spaces/Brand24/mms_benchmark](https://huggingface.co/spaces/Brand24/mms_benchmark), allows to compare models according to:

* _number of languages_: multilingual models with monolingual models,
* _training procedure_: models trained from scratch vs. fine-tuning,
* _domain language_: language to which a model is applied,
* _data modality_: news, reviews, social media,
* _knowledge transfer_: transfer between languages, transfer between domains.

Table 2 presents the list of models included in the benchmark. For each model, we include the number of parameters and languages used in pre-training and the base model. The results presented in the benchmark reflect three possible scenarios of model deployment. A pre-trained model can be used to generate text representation only. In this scenario, denoted _head-linear_ (HL), a model serves as a feature extractor followed by a small linear classification head. In the second scenario, the linear classification head is replaced by a BiLSTM classifier operating on features extracted by the pre-trained model. We refer to this scenario as _head-bilstm_ (HB). Finally, each pre-trained transformer-based model (with the exception of mUSE-transformer) has been fine-tuned to the sentiment classification task. We refer to this scenario as _fine-tuning_ (FT).

    &  &  &  \\  & & N & R & SM & O & NEG & NEU & POS & \#words & \#chars \\  English & 17 & 3 & 4 & 6 & 4 & 304,939 & 290,823 & 1,734,724 & 62 & 339 \\ Arabic & 9 & 0 & 4 & 4 & 1 & 138,899 & 192,774 & 600,402 & 52 & 289 \\ Spanish & 5 & 0 & 3 & 2 & 0 & 108,733 & 122,493 & 187,486 & 26 & 150 \\ Chinese & 2 & 0 & 2 & 0 & 0 & 117,967 & 69,016 & 144,719 & 60 & 80 \\ German & 6 & 0 & 1 & 5 & 0 & 104,667 & 100,071 & 111,149 & 26 & 171 \\ Polish & 4 & 0 & 2 & 2 & 0 & 77,422 & 62,074 & 97,192 & 19 & 123 \\ French & 3 & 0 & 1 & 2 & 0 & 84,187 & 43,245 & 83,199 & 28 & 159 \\ Japanese & 1 & 0 & 1 & 0 & 0 & 83,982 & 41,979 & 83,819 & 61 & 101 \\ Czech & 4 & 0 & 2 & 2 & 0 & 39,674 & 59,200 & 97,413 & 34 & 212 \\ Portuguese & 4 & 0 & 0 & 4 & 0 & 56,827 & 55,165 & 45,842 & 11 & 63 \\ Slovenian & 2 & 1 & 0 & 1 & 0 & 33,694 & 50,553 & 29,296 & 41 & 269 \\ Russian & 2 & 0 & 0 & 2 & 0 & 31,770 & 48,106 & 31,054 & 11 & 70 \\ Croatian & 2 & 1 & 0 & 1 & 0 & 19,757 & 19,470 & 38,367 & 17 & 116 \\ Serbian & 3 & 0 & 2 & 1 & 0 & 25,089 & 32,283 & 18,996 & 44 & 269 \\ Thai & 2 & 0 & 1 & 1 & 0 & 9,326 & 28,616 & 34,377 & 22 & 381 \\ Bulgarian & 1 & 0 & 0 & 1 & 0 & 13,930 & 28,657 & 19,563 & 12 & 86 \\ Hungarian & 1 & 0 & 0 & 1 & 0 & 8,974 & 17,621 & 30,087 & 11 & 83 \\ Slovak & 1 & 0 & 0 & 1 & 0 & 14,431 & 12,842 & 29,350 & 13 & 98 \\ Albanian & 1 & 0 & 0 & 1 & 0 & 6,889 & 14,757 & 22,638 & 13 & 91 \\ Swedish & 1 & 0 & 0 & 1 & 0 & 16,266 & 13,342 & 11,738 & 14 & 94 \\ Bosian & 1 & 0 & 0 & 1 & 0 & 11,974 & 11,145 & 13,064 & 12 & 76 \\ Urdu & 1 & 0 & 0 & 0 & 1 & 5,239 & 8,585 & 5,836 & 13 & 69 \\ Hindi & 1 & 0 & 0 & 1 & 0 & 4,992 & 6,392 & 5,615 & 26 & 128 \\ Persian & 1 & 0 & 1 & 0 & 0 & 1,602 & 5,091 & 6,832 & 21 & 104 \\ Italian & 2 & 0 & 0 & 2 & 0 & 4,043 & 4,193 & 3,829 & 16 & 103 \\ Hebrew & 1 & 0 & 0 & 1 & 0 & 2,279 & 243 & 6,097 & 22 & 110 \\ Latvian & 1 & 0 & 0 & 1 & 0 & 1,378 & 2,618 & 1,794 & 20 & 138 \\   

Table 1: Summary of the corpus. Categories: N - news, R - reviews, SM - social media, O - otherThe hyperparameters of models included in the benchmark are as follows. The hidden size of the model was set to the embedding size of each model when used in HL and FT scenarios. The hidden size of the HB scenario was set to \(h=32\). The learning rate for the HL scenario was \(=1 10^{-3}\), fine-tuning used \(=1 10^{-5}\), and HB scenario used \(=5 10^{-3}\). The batch size in HL and HB scenarios was \(b=200\) and \(b=6\) for fine-tuning. Dropout for HB was \(d=0.5\) and \(d=0.2\) for other scenarios. Training took 5 epochs for fine-tuning and 2 epochs for HL and HB scenarios (beyond 2 epochs most models started to overfit). The models are evaluated using the traditional \(F_{1}\) score computed on three levels: the entire dataset, averaged over all datasets, and the internal dataset.

We performed our experiments using Python 3.9 and PyTorch (1.8.1) (and Tensorflow (2.3.0) for original mUSE). Our experimental setup consists of Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz and Nvidia Tesla V100 16GB.

Here we present one result available through the benchmarks. Due to space constraints, we move the presentation of other case studies to Appendix B. Figure 1 contains \(F_{1}\) scores of all models fine-tuned on all datasets available for a given language. Interestingly, we see little variance in performance between models for high-resource languages and significant deterioration of performance for low-resource languages. An outlier is the aforementioned Portuguese, where the performance is caused by the lack of data points representing news and reviews. Figure 1 directly shows the usefulness of our benchmark. When designing sentiment classification solutions in Spanish, the choice of model is secondary (models' performances are very similar), and other model features can be considered (such as ease of deployment, cost of inference, and memory requirements). However, if sentiment classification is to be applied to Latvian, the performance difference between models can be as high as 32 pp depending on the choice of the model.

   Model & \#params & \#langs & base & reference \\  mT5 & 277M & 101 & T5 & Xue et al.  \\ LASER & 52M & 93 & BiLSTM & Artetxe and Schwenk  \\ mBERT & 177M & 104 & BERT & Devlin et al.  \\ MPNet & 278M & 53 & XLM-R & Reimers and Gurevych  \\ XLM-R-dist & 278M & 53 & XLM-R & Reimers and Gurevych  \\ XLM-R & 278M & 100 & XLM-R & Conneau et al.  \\ LaBSE & 470M & 109 & BERT & Feng et al.  \\ DistilmBERT & 134M & 104 & BERT & Sanh et al.  \\ mUSE-dist & 134M & 53 & DistilmBERT & Reimers and Gurevych  \\ mUSE-transformer & 85M & 16 & transformer & Yang et al.  \\ mUSE-cnn & 68M & 16 & CNN & Yang et al.  \\   

Table 2: Models included in the benchmark

Figure 1: Detailed comparison of models. Legend: **lang** - averaged by all languages, **ds** - averaged by dataset, **ar** - Arabic, **bg** - Bulgarian, **bs** - Bosnian, **cs** - Czech, **de** - German, **en** - English, **es** - Spanish, **fa** - Persian, **fr** - French, **he** - Hebrew, **hi** - Hindi, **hr** - Croatian, **hu** - Hungarian, **it** - Italian, **ja** - Japanese, **lv** - Latvian, **pl** - Polish, **pt** - Portuguese, **ru** - Russian, **sk** - Slovak, **sl** - Slovenian, **sq** - Albanian, **sr** - Serbian, **sv** - Swedish, **th** - Thai, **ur** - Urdu, **zh** - Chinese.

Related work

### Multilingual text representations

One of the foundational discoveries in multilingual presentation learning was the fact that latent vector spaces seemed to encode very similar word relationships across a wide spectrum of languages. Needless to say, first monolingual static word embeddings were quickly followed by multilingual word embeddings which provided the basis for multilingual text representations . A popular solution was to use pre-trained monolingual embeddings, such as word2vec and align them via linear transformations using parallel multilingual dictionaries . Another approach advocated for joint learning of multilingual word embeddings on pseudo-bilingual datasets, where tokens in one language would be randomly translated to another language .

Static multilingual embeddings were superseded by contextual embeddings produced by language models such as BiLSTM  and various Transformers . In the case of these more complex architectures, the multilingual capabilities of language models resulted from specific objective functions used during training. These objectives "pushed" the models toward universal multilingual representations by forcing the models to perform machine translation , translation language modeling (TLM) , or translation ranking .

The evaluation of the quality of multilingual text representation is not trivial. Cross-lingual and multilingual tasks are actively developed to foster the development of multilingual models. Examples of such cross-lingual and multilingual tasks include cross-lingual natural language inference , question answering , named entity recognition  or parallel text extraction . For example, results of comparison of LASER, mBERT, and XLM in tasks of named entity recognition and part-of-speech tagging in zero-shot settings suggests that LASER outperforms the latter methods in the case of knowledge transfer . Another important benchmark is XTREME , designed for testing the abilities of cross-lingual transfer across 40 languages and 9 tasks. Despite its massive character, XTREME lacks benchmarking task of sentiment analysis. Also, only mBERT, XLM, XLM-R, and MMTE are used as baseline models.

By far, the most popular pre-trained multilingual language model is Multilingual BERT (mBERT) . It has been used in several cross-lingual studies, for instance, in zero-shot knowledge transfer between Slovenia and Croatian languages , exploring code-switching on Spanglish and Hinglish , and building hierarchical architecture for zero-shot setting . The properties and limitations of mBERT have been extensively studied. One of the studies showed that mBERT is not learning a joint representation of languages. Rather, it partitions the representation space between languages . The authors used the Projection Weighted Canonical Correlation Analysis (PWCCA) to analyze how translations of the same sentence are represented in mBERT layers. The correlations were stable in pre-trained and fine-tuned models, with the effect being more pronounced in deeper layers of the model. The hierarchical structure induced by correlations was similar to the structure produced by genealogical linguistics.

Another interesting finding was that mBERT encodes language-specific information within the parameter space, and this language component is not removed by fine-tuning . The language-specific component of mBERT can be removed by estimating the centroid of the language (the mean of mBERT embeddings of a given language vocabulary) and subtracting this centroid from representations produced in the language. The existence of the language component has been proven in multiple tasks: language classification, language similarity, sentence retrieval, word alignment, and machine translation.

Several works tested the cross-lingual abilities of mBERT as compared to monolingual models, finding its performance on low-resource languages inferior to monolingual models . One experiment used a bilingual version of mBERT and trained it in multiple configurations to test the influence of the linguistic relationship between the source and the target language, the network architecture, and the input and learning objective . The authors found that structural similarity and depth of a model are the most significant factors behind mBERT's cross-lingual performance. On the other hand, multi-head attention was not particularly important. Regarding the low-resource languages,  compared mBERT with baseline models on tasks of named entity recognition, universal part-of-speech tagging, and universal dependency parsing. The authors focused on the comparison between low- and high-resource languages (determined by the size of the Wikipedia dump in each language). mBERT was found to work well on high-resource languages but under-performed on low-resourcelanguages. Finally, Liu et al.  analyzed the relationship between the contextual aspect of mBERT and the training dataset size in the context of multilingual datasets. The authors compared contextual mBERT embeddings with non-contextual models of Word2Vec and GloVe. As it turns out, mBERT outperforms non-contextual models only when large datasets are available for training or fine-tuning. In the low data regime, as well as when the context window is short (limited input), multilingual mBERT is surpassed by static token embeddings.

### Multilingual sentiment classification

Several surveys present an overview of traditional sentiment classification methods (lexicon-based approaches and shallow models with lexical features engineering) [25; 70]. Recently, deep-learning approaches became more prevalent. Attia et al.  use convolutional neural networks on word-level embeddings of texts in English, German and Arabic. The approach is computationally expensive as it requires separate embedding dictionaries for each language. An alternative approach is to use character-level embeddings. Wehrmann et al.  trained such a model for binary sentiment classification of English, German, Portuguese, and Spanish tweets. A similar model was used in , but the authors applied multi-stage pre-processing, including dictionary checks, Soundex algorithm, and lemmatization. Other examples of deep neural models for multilingual sentiment classification include recurrent neural networks supplied with machine translation. Can et al.  trained a model on English reviews and evaluated it on machine-translated reviews in Russian, Spanish, Turkish, and Dutch using the Google Translation API and pre-trained GloVe embeddings for English. A similar approach is presented in , where the authors used LASER sentence embeddings to train a sentiment classifier on Polish reviews and used this classifier to score reviews translated into other languages.

### Multilingual sentiment datasets

Multilingual sentiment datasets are crucial resources for training sentiment analysis models that can operate across various languages. Unfortunately, very few multilingual datasets contain sentiment polarity annotations. The Multilingual Amazon Reviews Corpus  is a noteworthy example of such a dataset. It provides customer review data in English, Japanese, German, French, Spanish, and Chinese, annotated for sentiment. Another significant resource is the Multilingual Sentiment Lexicons developed by Chen and Skiena ), which provides sentiment lexicons for 136 languages. Being a lexicon, it cannot be used to train sentiment models directly, but it can be used for weak supervision. The NTCIR corpus contains information on sentiment polarity for news related to sport and politics for articles in English, Chinese, and Japanese . The JMTC dataset (Jigsaw Multilingual Toxic Comment Classification) dataset contains comments from Wikipedia's talk page edits and is available in multiple languages. It is annotated for toxic vs. non-toxic sentiment, which cannot be aligned with our 3-class annotation schema. An interesting resource is Task 4 from SemEval providing English, Spanish, Dutch, Arabic, Russian, and Turkish tweets and SMS messages annotated for 3-class sentiment [54; 67]. The most similar dataset to ours is XED compiled by Ohman et al. . However, this dataset contains original annotations only for English and Finnish, and the remaining 30 languages are "projected" (i.e., annotated samples in English and Finnish are translated into multiple languages).

## 6 Discussion

The main focus of the paper is the introduction of a massively multilingual large collection of sentiment datasets and the extensive benchmark of model training and validation scenarios. During the construction of the benchmark, we have gathered valuable experiences that we want to share with the community. Our most important observation is that a single multilingual sentiment classifier can perform approximately equally well for all languages. A small group of pre-trained models (XML-R, LaBSE, MPNet) under fine-tuning scenario produces the best results relative to other models. Obviously, the absolute \(F_{1}\) score across languages differs significantly (due to data scarcity, data quality, or difficulty of samples), but these selected models always produce top classifiers. Secondly, all models perform better under the fine-tuning scenario, but the performance gain varies from model to model. When evaluated on the test dataset, models gained between 4 pp. (mUSE-dist) and 9 pp. (mBERT, DistilmBERT). When tested on the internal datasets, \(F_{1}\) gains varied between 0 pp (mUSEdist) and 20 pp. (DistilmBERT), with mT5 and XML-R improving by 17 pp. and 15 pp., respectively. In general, the largest gains can be observed for models trained with the masked language modeling technique (XML-R, mBERT). We also observe that models which produce lower-quality sentence embeddings gain more from fine-tuning.

We also find that bigger models tend to achieve better performance in all data modalities and training scenarios. Of course, there are counterexamples, e.g., mUSE-dist is smaller than mBERT but achieves better performance in the HL scenario for all datasets. This indicates that the size of the model is an important factor for determining its performance, but other factors, like the domain and the type of pretraining task, may also affect the results. Moreover, we observe that the correlation between model size and model performance is weaker after fine-tuning. This means that one may often find a competitive model with similar performance to the state-of-the-art model but significantly smaller and faster for the production environment.

Our final remark is a warning: in our experiments, we have encountered a significant variability in performance conditioned on a particular data split. In one of the cross-validation fine-tuning experiments, we observed a data fold that produced consistently worse results (up to 4 pp.) for all models. This fold was produced by the same random seed and (most probably) consisted of samples with increased difficulty. Since our experiments involved multiple models, multiple training scenarios, multiple datasets, and multiple languages, we had to rely on data subsampling. Conducting experiments on a fixed seed for sample selection could lead to a similar data fold and, consequently, to biased experimental results.

## 7 Limitations

Despite the fact that our collection is the largest public collection of multilingual sentiment datasets, it still covers only 27 languages. The collection of datasets is highly biased towards the Indo-European family of languages, English in particular. We attribute this bias to the general culture of scientific publishing and its enforcement of English as the primary carrier of scientific discovery. Our work's main potential negative social impact is that the models developed and trained using the provided datasets may still exhibit better performance for the major languages. This could further perpetuate the existing language disparities and inequality in sentiment analysis capabilities across different languages. Addressing this limitation and working towards more equitable representation and performance across languages is crucial to avoid reinforcing language biases and the potential marginalization of underrepresented languages. The ethical implications of such disparities should be thoroughly discussed and considered.

An important limitation of our dataset collection is a significant variance in sample quality across all datasets and all languages. Figure 2 presents the distribution of self-confidence label-quality score for each data point computed by the cleanlab. The distribution of quality is skewed in favor of popular languages, with low-resource languages suffering from data quality issues. A related limitation is caused by an unequal distribution of data modalities across languages. For instance, our benchmark clearly shows that all models universally underperform when tested on Portuguese datasets. This is the direct result of the fact that data points for Portuguese almost exclusively represent the domain of social media. As a consequence, some combinations of filtering facets in our dataset collection produce very little data (i.e., asking for social media data in the Germanic genus of Indo-European languages will produce a significantly larger dataset than asking for news data representing Afro-Asiatic languages).

Finally, we acknowledge the lack of internal coherence of annotation protocols between datasets and languages. We have enforced strict quality criteria and rejected all datasets published without the annotation protocol, but we were unable, for obvious reasons, to unify annotation guidelines. The annotation of sentiment expressions and the assignment of sentiment labels are heavily subjective and, at the same time, influenced by cultural and linguistic features. Unfortunately, it is possible that semantically similar utterances will be assigned conflicting labels if they come from different datasets or modalities.