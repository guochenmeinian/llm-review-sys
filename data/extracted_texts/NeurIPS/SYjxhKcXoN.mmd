# LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization

Liang Chen\({}^{1}\) Yong Zhang\({}^{2}\) Yibing Song Zhiqiang Shen\({}^{1}\) Lingqiao Liu\({}^{3*}\)

\({}^{1}\) MBZUAI \({}^{2}\) Meituan Inc. \({}^{3}\) The University of Adelaide

{liangchen527, zhangyong201303, yibingsong.cv}@gmail.com

Zhiqing.Shen@mbzuai.ac.ae lingqiao.liu@adelaide.edu.au

Corresponding authors.

###### Abstract

Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at https://github.com/liangchen527/LFME.

## 1 Introduction

Deep networks trained with sufficient labeled data are expected to perform well when the training and test domains with similar distributions . However, test domains in real-world often exhibit unexpected characteristics, leading to significant performance degradation for the trained model. Such a problem is referred to as distribution shift and is ubiquitous in common tasks such as image classification  and semantic segmentation . Various domain generalization (DG) approaches have been proposed to address the distribution shift problem lately, such as invariant representation learning , augmentation , adversarial learning , meta-learning , to name a few. Yet, according to , most arts perform inferior to the classical Empirical Risk Minimization (ERM) when applied with restricted hyperparameter search and evaluation protocol. Both the experiments in  and our findings suggest that existing models are incapable of consistently improving ERM in all evaluated datasets. The consistent improvement for ERM thus becomes our motivation to further explore DG.

Our approach derives from the observation in  that some of the data encountered at test time are similar to one or more source domains, and in which case, utilizing expert models specialized in the domains might aid the model in making a better prediction. The observation can be better interpreted with the example in Fig. 1. Given experts trained in the "infograph", "real", and "quickdraw" domains, and test samples from the novel "sketch" domain. Due to the large domain shift, it would be better to rely mostly on the expert trained on the similar "quickdraw" domain than others.

However, the test domain information is often unavailable in DG, indicating that we can not specifically train an expert specialized in a particular domain. In light of this, obtaining a target model that is an expert on all source domains seems to be a practical alternative for handling potential arbitrary test domains. A naive implementation would be training multiple experts on each domain, and dynamically aggregate them to form the target model. So that any encountered test samples can be predicted by corresponding experts who are familiar with their characteristics. Nevertheless, such a practice has two inherent limitations: (1) designing an effective aggregation mechanism is essential and inevitable for the naive model. In fact, our experimental study indicates that using naive aggregating strategies, such as averaging, may deteriorate the performance. (2) the overall framework requires much more resources for deployment when there are many training domains, since all the experts must be leveraged during the test phase.

This work proposes a simple framework for learning from multiple experts (LFME), capable of obtaining an expert specialized in all source domains while avoiding the aforementioned limitations. Specifically, during the training stage, instead of heuristically aggregating different experts, we suggest training a universal target model that directly inherits knowledge from all these experts, which is achieved by a simple logit regularization term that enforces the logit of the target model to be similar to probability from the corresponding expert. With this approach, the target model is expected to leverage professional guidance from multiple experts, evolving into an expert across all source domains. During the test phase, only the target model is utilized. As a result, both model aggregations and extra memory and computation resources are not required during the deployment, since we only leverage one model. The overall training and test pipelines are illustrated in Fig. 1.

Our method can be interpreted through the lens of knowledge distillation (KD), where the core idea is transferring knowledge by training the student (_i.e._ the target model) with soft labels from teachers (_i.e._ experts) . Unlike traditional KD  that uses teachers' output probabilities as soft labels in a cross entropy loss, we employ a logit regularization term that uses experts' probabilities to refine the logit of the target model in a regression manner, which can be regarded as extending the effectiveness of mean squared error (MSE) loss in classification  within the KD realm.

To gain a deeper understanding of the effectiveness of our logit regularization term, we perform in-depth analyses and uncover that its merit over the baseline can be explained in twofold. (1) It implicitly regularizes the probability of the target model within a smaller range, enabling it to use more information for prediction and improve DG accordingly. It is noteworthy that the effect is achieved inherently differs from that by label smoothing (LS) , as LFME does not require explicitly calibration for the output probability. Expanding upon this analysis, we find that a simple combination of cross entropy and MSE losses achieves comparable performance among existing arts. Given its straightforward implementation without unnecessary complexities, this expanding may offer a "free lunch" for DG; (2) It further boosts generalization by helping the target model to focus more on hard samples from the experts, supported by our theoretical finding. Through experiments on different datasets, we find that hard samples from the experts are more beneficial for generalization than those from the model itself. Given that hard sample mining is essential for easing distribution shift [35; 42], this discovery may offer valuable guidance for future research endeavors.

Figure 1: Pipeline of LFME. Experts and the target model are trained simultaneously. To obtain a target model that is an expert on all source domains, we learn multiple experts specialized in corresponding domains to help guide the target model during training. For each sample, the guidance is implemented with a logit regularization term that enforces similarity between the logit of the target model and probability from the corresponding expert. Only the target model is utilized in inference. Please refer to Algorithm 1 for detailed implementations.

Through evaluations on the classification task with the DomainBed benchmark  and segmentation task with the synthetic [63; 64] to real [20; 83; 55] setting, we illustrate that LFME is consistently beneficial to the baseline and can obtain favorable performance against current arts (other KD ideas included). _Our method favors extreme simplicity, adding only one hyper-parameter, that can be tuned in a rather large range, upon training the baseline ERM._

## 2 Related Works

**General DG methods.** Domain generalization (DG), designed to enable a learned model to maintain robust results in unknown distribution shift, is gaining increasing attention in the research community lately. The problem can be traced back to a decade ago , and various approaches and applications have been proposed to push the generalization boundary ever since [53; 26; 46; 32; 81; 49; 2; 16; 11; 13; 82; 12]. The pioneering work  theoretically proves that the DG performance is bounded by both the intra-domain accuracy and inter-domain differences. Most previous arts focus on reducing the inter-domain differences by learning domain-invariant features with ideas such as kernel methods [53; 26], feature disentanglement [10; 61], and gradient regularization techniques [69; 62]. Same endeavors also include different learning skills: adversarial training is leveraged [24; 81] to enforce representations to be domain agnostic; meta-learning is utilized [2; 45] to simulate distribution shifts during training. Other works aim to improve the intra-domain accuracy: some suggest explicitly mining hard samples or representations with handcraft designs, such as masking out dominant features , weighting more on hard samples , or both . LFME falls into this category as the target model can also mine hard samples from the experts, beneficial for excelling in all source domains (in Sec. 6.5).

**Utilizing experts for DG.** Methods with the most relevant motivations with our LFME are perhaps those also involves experts [86; 28; 89; 90]. In DAELDG , a shared feature extractor is adopted, which is followed by different classifiers (_i.e._ expert) that correspond to specific domains. Their experts are trained by enforcing the outputs to be similar to the average output from the non-expert classifiers. Different from our work, it uses the average outputs from different experts as the final result. In Meta-DMoE , similar to LFME, different experts are trained on their specific domains where a traditional KD idea is adopted: the feature from the target model is enforced to be similar to the transformer-processed version of their experts' output features. Notably, Meta-DMoE and LFME share very distinct objectives for the expert models. Specifically, Meta-DMoE aims to adapt the trained target model to a new domain in test. To facilitate adaptation, their target model is assumed to be capable of identifying domain-specific information (DSI), and is enforced to extract DSI similar to those from domain experts. In their settings, domain experts are expected to thrive in all domains and are used not in their trained domains but rather in an unseen one. Differently, LFME expects its target model to be expert in all source domains, where domain experts provide professional guidance for the target model only in their corresponding domains. Additionally, Meta-DMoE involves meta-learning and test-time training, which is more complicated than the end-to-end training adopted in LFME. Unlike their empirical design, our method is more self-contained, supported by in-depth analysis to explain its efficacy (in Sec. 4). We further show in our experiments (in Sec. 5.1 and E) that these related two methods perform inferior to our design, and the improvements from their basic designs: using average performance (in Sec. 6.4) or enforcing feature similarity between the target model and experts (in Sec. 6.3) are subtle compared to our logit regularization.

**DG in semantic segmentation.** Different from image classification, semantic segmentation involves classifying each pixel of the image, and the generalizing task often expects a model trained from the synthetic environments to perform well on real-world views. Directly extending general DG ideas to semantic segmentation is not easy. Current solutions mainly consist of domain randomization [34; 85], normalization [19; 56; 73], or using some task-related designs, such as the class memory bank in . Different from some existing DG methods, LFME can be directly extended to ease the distribution shift problem in the semantic segmentation task without requiring any tweaks, and we show that it can obtain competitive performance against recent arts specially designed for the task.

## 3 Methodology

**Problem Setting.** In the vanilla DG setting, we are given \(M\) source domains \(_{s}=\{_{1},_{2},...,_{M}\}\), where \(_{i}\) is the \(i\)-th source domain containing data-label pairs \((x_{n}^{i},y_{n}^{i})\) sampled from different probabilities on the joint space \(\), the goal is to learn a model from \(_{s}\) for making predictions on the data from the unseen target domain \(_{M+1}\). For either DG or the downstream semantic segmentation task, source and target domains are considered with an identical label space, and we assume that there are a total of \(K\) classes.

**Learning multiple experts**. LFME trains all the experts and the target model simultaneously, and the training procedure is illustrated in the upper part of Fig. 1. As each expert corresponds to a specific domain, given the \(M\) source domains, a total of \(M\) experts have to be trained during this stage, and the training process of each expert is the same as that of the ERM model. Given a training batch \(_{s}\), for the \(i\)-th expert \(E_{i}\), we only use data from the \(i\)-th domain, and the computed loss regarding the data-label pair \((x^{i},y^{i})(_{i})\) can be written as,

\[_{i}=(q^{E_{i}},y^{i}),\ \ q_{c}^{E_{i}}= (z_{c}^{E_{i}})=^{E_{i}})}{_{j}^{K}exp(z_{j}^ {E_{i}})},\] (1)

where \(q^{E_{i}}^{K}\) is the output probability computed by applying the softmax function over the output logits \(z^{E_{i}}\) with \(z^{E_{i}}=E_{i}(x^{i})\); \(\) denotes the cross-entropy loss: \((q,y)=_{c}^{K}-y_{c} q_{c}\).

**Learning the target model**. Data-label pairs \((x,y)\) from all domains are used for training the target model \(T\). The main classification loss \(_{cla}\) is computed similar to Eq. (1): \(_{cla}=(q,y)\), s.t. \(q=(z)=)}{_{j}^{K}exp(z_{j})}\) and \(z=T(x)\). Then, to incorporate professional guidance from the experts, we further introduce a logit regularization term \(_{guid}\), to assist \(T\) to become an expert on all source domains, which is computed by using the probabilities from the experts as a label for \(T\):

\[_{guid}=\|z-q^{E}\|^{2},\] (2)

where \(q^{E}\) is the concatenate probabilities from different experts along the batch dimension 2, \(\|\|\) denotes the \(L_{2}\) norm, and this term is only enforced on the target model. Note we use the normalized version of \(z^{E}\) (_i.e_. \(q^{E}\)) for computing \(_{guid}\), which can be regarded as extending the effectiveness of MSE loss  in the KD realm. Our experimental studies (in Sec. 6.3) find it leads to better performance, and the following contents also elaborate on the motivation. Then, the overall loss \(_{all}\) for updating the target model can be represented as,

\[_{all}=_{cla}+_{guid},\] (3)

where \(\) is the weight parameter, the only additional parameter upon ERM. We train the target model and the experts simultaneously for simplicity. Please refer to pseudocode in Algorithm 1 for details.

**Rationality (comprehension from a KD perspective).** Our logit regularization term can be viewed as a new KD form, wherein the fundamental principle is to utilize the teachers' (_i.e_. experts) outputs as soft labels for the student (_i.e_. target model) in a training objective . In the context of classification tasks, the cross entropy loss \((q,y)\) is widely used in the literature. Based on this objective, an intuitive revision to achieve distillation is by replacing the ground-truth label \(y\) with \(q^{E}\) in a cross entropy regularization manner (_i.e_. \((q,q^{E})\)), which builds the rationality for the pioneering KD art . Nevertheless, a recent study  suggests that the MSE loss \(\|z-y\|\) (without applying softmax function on \(z\)) performs as well as the cross entropy loss when being applied in the classification task. Correspondingly, a distillation scheme motivated by this objective can thus be utilizing the soft label \(q^{E}\) in a regression manner, which comes to our logit-regularized term: \(\|z-q^{E}\|\). From this perspective, the rationality of the introduced logit regularization term aligns with the principle of KD, and it can be regarded as extending the applicability of MSE loss in classification to the KD realm. We compare our new KD form with other ideas in Sec 6.3, demonstrating its superior performance against existing KD ideas in the DG task. We delve deep into our method and explain the effectiveness of LFME in the following section.

**Computational cost.** Inherited from KD, training LFME inevitably requires more resources as both the teacher and student have to be involved during training. However, LFME uses the same test resources as the baseline ERM given only the target model is utilized. Meanwhile, please also note that the computational cost for LFME is not proportional w.r.t the domain size. Instead, the training cost will always be doubled compared to ERM, as each sample will require two forward passes: one for the target model and the other for the corresponding expert. Please refer to Tab. 11 for training time comparisons between different arts. In Sec. F, we show that simply increasing training resources for current arts cannot improve their performances, suggesting it may not be a primary factor in DG.

## 4 Deeper Analysis: How the Simple Logit Regularization Term Benefits DG?

### Enabling the Target Model to Use More Information

Specifically, for the baseline model, using only the classification loss \(_{cla}\) encourages the probability \(q\) to be diverse: the ground truth \(q_{*}\) to approximate \(1\) and \(0\) for others. Consequently, the corresponding logits \(z_{*}\) will increase ceaselessly, _i.e_. \(z_{*}+\), and vice versa for \(z_{c}\), _i.e_. \(z_{c}-, c*\) (depicted in Fig. 2 (a) and (c)), as this is the solution for minimizing \(-)}{ exp(z_{c})}\). From another point of view, \(_{cla}\) encourages the model to explicitly focus on the dominant and exclusive features that are strongly discriminative but may be biased towards simplistic patterns .

Differently, when \(_{guid}\) is imposed, the logits \(z\) will approximate the range of \(q^{E}\) (_i.e_. \(\)). Eventually, the final logits will balance these two losses (_i.e_. \(z_{*}(q_{*}^{E},+)\) and \(z_{c}(-,q_{c}^{E}) c*\) as shown in Fig. 2 (d)), resulting in a smoother distribution of \(q\), where \(q_{c}\) (\( c*\)) in LFME will be larger than it is in ERM (see Fig. 2 (b)). Since both two losses encourage the model to make a good prediction (_i.e_. \(z_{*}\) is expected to be the largest in both losses), the increase of \(q_{c}\) indicates that besides learning the dominant features, the target model will be enforced to learn other information that is shared with others. Compared with ERM that prevents the model from learning other features, LFME is more likely to make good predictions when certain types of features are missing while others exist in unseen domains. We provide a "free lunch" inspired by the analysis in Sec. 6.1, and more justifications (including visual and empirical evidence) to support this analysis in Sec. D.1 and D.2. In Sec. C, we demonstrate that other KD ideas face challenges in achieving the same merit.

The above finding can also be endorsed by the information theory . Specifically, with a smoother distribution of \(q\), the entropy, which measures information, will naturally increases , suggesting a theoretical basis for utilizing more information in LFME. According to the principle of maximum entropy , the improvement for generalization is thus foreseeable .

Note this effect is achieved inherently differs from that by label smoothing (LS) , as LFME does not involve hand-crafted settings to deliberately calibrate the output probability, which is essential in LS. In Sec. 6.2, we show LS is ineffective in DG compared to LFME. Besides the advantage of avoiding problems raised by potential improper heuristic designs, we show in the following that the logit regularization in LFME provides another merit over LS.

### Enabling the Target Model to Mine Hard Samples from the Experts

This effect is realized by example re-weighting, motivated by proposition 2 in . The single sample gradient of \(_{all}\) with respect to the \(c\)-th logit value \(z_{c}\) can be formulated as,

\[_{all}}{ z_{c}}=q_{c}-y_{c}+(z_{c}-q_{c} ^{E}).\] (4)

When the target probability corresponds to the ground-truth \(y_{*}=1\), Eq. (4) is reduced into,

\[_{all}}{ z_{*}}=q_{*}-1+(z_{*}-q_{*}^{E}).\] (5)

In this situation, the rescaling factor \(\) is given by,

\[=_{all}}{ z_{*}}/_{cla}}{ z_{*}}=1--q_{*}^{E}}{1-q_{*}}.\] (6)

On the other hand, for \( c*\), summing the gradient values over the finite indexes gives,

\[_{c*}_{all}}{ z_{c}}=_{c*}q_{ c}+_{c*}(z_{c}-q_{c}^{E}),\] (7)

Figure 2: Values of probabilities, logits, and rescaling factors(_i.e_. \(q\), \(z\), \(\), \(^{}\)) from the ERM model and LFME. Models are trained on three source domains from PACS with the same settings.

and the rescaling factor \(^{}\) in this situation is,

\[^{}=_{c*}_{all}}{ z_{ c}}/_{c*}_{cla}}{ z_{c}}=1-z_{c}-g_{*}^{E}}{1-q_{*}}.\] (8)

We observe that both \(\) and \(^{}\) are strictly monotonically increased regarding the value of \(q_{*}^{E}\). Given almost all values of the rescaling factors are negative as can be observed in Fig. 2 (e) (except in the few initial steps where \(z_{*}\) and \(q_{*}^{E}\) are both small and \(_{gaid}\) barely contributes) 3, with the same logits, a smaller \(q_{*}^{E}\), in which case the expert is less confident, will lead to larger \(||\) and \(|^{}|\). This phenomenon indicates that with the logit regularization term, the target model will focus more on the harder samples from the experts. Empirical findings supporting this analysis are in Sec. D.3 and D.4. Note that the segmentation task also utilizes one-hot labels and cross-entropy loss, making it applicable to the two analyses presented.

## 5 Experiments

### Generalization in Image classification

**Datasets and Implementation details.** We conduct experiments on 5 datasets in DomainBed , namely PACS  (9,991 images, 7 classes, 4 domains), VLCS  (10,729 images, 5 classes, 4

    & PACS & VLCS & OfficeHome & TerraInc & DomainNet & Avg. & Top\(\) Score\(\) \\   MMD  & \(81.3 0.8\) & \(74.9 0.5\) & \(59.9 0.4\) & \(42.0 1.0\) & \(7.9 6.2\) & 53.2 & 0 & 2 \\ RSC  & \(80.5 0.2\) & \(75.4 0.3\) & \(58.4 0.6\) & \(39.4 1.3\) & \(27.9 2.0\) & 56.3 & 0 & 1 \\ IRM  & \(80.9 0.5\) & \(75.1 0.1\) & \(58.0 0.1\) & \(38.4 0.9\) & \(30.4 1.0\) & 56.6 & 0 & 1 \\ DANN  & \(79.2 0.3\) & \(76.3 0.2\) & \(59.5 0.5\) & \(37.9 0.9\) & \(31.5 0.1\) & 56.9 & 0 & 1 \\ GroupGRO  & \(80.7 0.4\) & \(75.4 1.0\) & \(60.6 0.3\) & \(41.5 2.0\) & \(27.5 0.1\) & 57.1 & 0 & 1 \\ VREx  & \(80.2 0.5\) & \(75.3 0.6\) & \(59.5 0.1\) & \(43.2 0.3\) & \(28.1 1.0\) & 57.3 & 1 & 1 \\ CAD  & \(81.9 0.3\) & \(75.2 0.6\) & \(60.5 0.3\) & \(40.5 0.4\) & \(31.0 0.8\) & 57.8 & 0 & 2 \\ CondCAD  & \(80.8 0.5\) & \(76.1 0.3\) & \(61.0 0.4\) & \(39.7 0.4\) & \(31.9 0.7\) & 57.9 & 0 & 1 \\ MTL  & \(80.1 0.8\) & \(75.2 0.3\) & \(59.9 0.5\) & \(40.4 1.0\) & \(35.0 0.0\) & 58.1 & 0 & 0 \\ ERM  & \(79.8 0.4\) & \(75.8 0.2\) & \(60.6 0.2\) & \(38.8 1.0\) & \(35.3 0.1\) & 58.1 & 0 & - \\ MixStyle  & \(82.6 0.4\) & \(75.2 0.7\) & \(59.6 0.8\) & \(40.9 1.1\) & \(33.9 0.1\) & 58.4 & 1 & 1 \\ MLDG  & \(81.3 0.2\) & \(75.2 0.3\) & \(60.9 0.2\) & \(40.1 0.9\) & \(35.4 0.0\) & 58.6 & 0 & 1 \\ Mixup  & \(79.2 0.9\) & \(76.2 0.3\) & \(61.7 0.5\) & \(42.1 0.7\) & \(34.0 0.0\) & 58.6 & 0 & 2 \\ MIRO  & \(75.9 1.4\) & \(76.4 0.4\) & \(64.1 0.4\) & \(41.3 0.2\) & \(36.1 0.1\) & 58.8 & 3 & 3 \\ FishF  & \(81.3 0.3\) & \(76.2 0.3\) & \(60.9 0.3\) & \(42.6 1.0\) & \(34.2 0.3\) & 59.0 & 0 & 2 \\ Meta-DMoE  & \(81.0 0.3\) & \(76.0 0.6\) & \(62.2 0.1\) & \(40.0 1.2\) & \(36.0 0.2\) & 59.0 & 1 & 3 \\ SagNet  & \(81.7 0.6\) & \(75.4 0.8\) & \(62.5 0.3\) & \(40.6 1.5\) & \(35.3 0.1\) & 59.1 & 1 & 2 \\ SelfReg  & \(81.8 0.3\) & \(76.4 0.7\) & \(62.4 0.1\) & \(41.3 0.3\) & \(34.7 0.2\) & 59.3 & 1 & 3 \\ Fish  & \(82.0 0.3\) & \(76.9 0.2\) & \(62.0 0.6\) & \(40.2 0.6\) & \(35.5 0.0\) & 59.3 & 1 & 4 \\ CORAL  & \(81.7 0.0\) & \(75.5 0.4\) & \(62.4 0.4\) & \(44.1 1.8\) & \(36.1 0.2\) & 59.4 & 1 & 3 \\ SD  & \(81.9 0.3\) & \(75.5 0.4\) & \(62.9 0.2\) & \(42.0 1.0\) & \(36.3 0.2\) & 59.7 & 2 & 4 \\ CausEB  & \(82.4 0.4\) & \(76.5 0.4\) & \(62.2 0.1\) & \(43.2 1.3\) & \(34.9 0.1\) & 59.8 & 3 & 4 \\ ITTA  & \(83.8 0.3\) & \(76.9 0.6\) & \(62.0 0.2\) & \(43.2 0.5\) & \(34.9 0.1\) & 60.2 & 3 & 4 \\ RIDG  & \(82.8 0.3\) & \(75.9 0.3\) & \(63.3 0.1\) & \(43.7 0.5\) & \(36.0 0.2\) & 60.3 & 4 & 4 \\ Ours & \(82.4 0.1\) & \(76.2 0.1\) & \(63.2 0.1\) & \(46.3 0.5\) & \(36.1 0.1\) & 60.8 & 4 & 5 \\   ERM), OfficeHome  (15,588 images, 65 classes, 4 domains), TerraInc  (24,788 images, 10 classes, 4 domains), and DomainNet  (586,575 images, 345 classes, 6 domains). We use the ImageNet  pretrained ResNet  as the backbone for both the experts and the target model. Following the designs in DomainBed, the hyper-parameter \(\) in Eq. (3) is randomly selected in a range of \([0.01,10]\). To ensure fair comparisons, all methods are reevaluated using the default settings in DomainBed in the same device with each of them evaluated for \(3 20\) times in different domains. Training-domain validation is adopted as the evaluation protocol. Other settings (batch size, learning rate, dropout, etc.) are dynamically selected for each trial according to DomainBed.

**Results with ResNet18.** Results are listed in Tab. 1. We observe that ERM can obtain competitive results among the models compared, which leads more than half of the sophisticated designs, and only 6 methods lead ERM in most datasets (with scores \( 3\)). We also note that current strategies cannot guarantee improvements for ERM in all situations, given that none of them with a score of 5. Differently, our method can consistently improve ERM in all evaluated datasets and lead others in average accuracy. Specifically, our approach obtains the leading results in 4 out of the 5 datasets, and it also improves ERM by a large margin (nearly 8pp) in the difficult TerraInc dataset. Compared to methods that explicitly explore hard samples or representations (_i.e_. VREx and RSC) and that use MoE (_i.e_. Meta-DMoE), the performances of LFME are superior to them in all cases.

**Results with ResNet50.** Because larger networks require more training resources, we only reevaluate some of leading methods (_i.e_. ERM, Fish, CORAL, and SD). in our device when experimenting with ResNet50. We note that our method surpasses the baseline ERM model in all datasets and leads it by 1.8 in average. Meanwhile, our method can still outperform the second best (_i.e_. SD) by 0.7 in average. These results indicate that our method can consistently improve the baseline model, and it can perform favorably against existing arts when implemented with a larger ResNet50 backbone.

We also combine LFME with SWAD . Same with the original design, the hyper-parameter searching space in this setting is smaller than the original DomainBed. We use the reported numbers in  for comparisons. As shown, our method can also improve the baseline and obtain competitive results when combined with SWAD. These results further validate the effectiveness of our method.

### Generalization in Semantic Segmentation

**Datasets and Implementation details.** The training and test processes of the compared algorithms involve 5 different datasets: 2 synthetics for training, where each dataset is considered a specific domain, and 3 real datasets for evaluation. Synthetic: GTAV  has 24,966 images from 19 categories; Synthia  contains 9,400 images of 16 categories. Real: Cityscapes  3,450 finely annotated and 20,000 coarsely-annotated images collected from 50 cities; Bdd100K  contains 8K diverse urban driving scene images; Mapillary  includes 25K street view images. Following the design , we use the pretrained DeepLabv3+  and ResNet50 as the segmentation backbone for experiments. The maximum iteration step is set to 120K with a batch size of 4, and the evaluations are conducted after the last iteration step. The hyper-parameter \(\) in Eq. (3) is fixed as 1 in this experiment for simplicity. We use the mean Intersection over Union (mIoU) and mean accuracy (mAcc) averaged over all classes as criteria to measure the segmentation performance.

**Experimental results.** Results are shown in Tab. 2. To better justify the effectiveness of our method, we reevaluate the baseline model, which aggregates and trains on all source data, and PinMem in our device. We also implement SD , which is a leading method in the image classification

    &  &  &  &  \\   & mIOU & mAcc & mIOU & mAcc & mIOU & mAcc & mIOU & mAcc \\  Baseline\({}^{}\) & 35.46 & - & 25.09 & - & 31.94 & - & 30.83 & - \\ IBN-Net \({}^{}\) & 35.55 & - & 32.18 & - & 38.09 & - & 35.27 & - \\ RobustNet \({}^{}\) & 37.69 & - & 34.09 & - & 38.49 & - & 36.76 & - \\ Baseline & 37.19 & 48.75 & 27.95 & 39.04 & 32.01 & 48.88 & 32.38 & 45.56 \\ PinMem  & 41.86 & 48.30 & 34.94 & 44.11 & 39.41 & 49.87 & 38.74 & 47.43 \\ SD  & 34.77 & 46.63 & 28.00 & 40.33 & 31.41 & 48.18 & 31.39 & 45.05 \\ Ours & 38.38 & 48.99 & 35.70 & 46.16 & 41.04 & 53.71 & 38.37 & 49.62 \\   

Table 2: Evaluations on the semantic segmentation task. Results with \({}^{}\) are directly cited from , others are reevaluated in our device. Best results are colored as red.

task. Results from other methods are directly cited from . We observe SD does not perform as effectively as it does in the classification task, which decreases the baseline model in most situations. In comparison, our LFME can boost the baseline in all datasets, similar to that in the classification task. It also performs favorably against existing methods specially designed for the semantic segmentation task, obtaining best results in 2 out of the 3 evaluated datasets in both mIOU and mAcc.

Visualized examples are provided in Fig. 3. We note that when it comes to unseen objects (_i.e_. clouds with different shapes and a new car logo), or objects with an unfamiliar background, such as the person and car hidden in the shadow, due to the large distribution shift between real and synthetic data, compared methods make unsatisfactory predictions. In comparison, LFME can provide reasonable predictions in these objects, demonstrating its effectiveness against current arts regarding generalization to new scenes. These results validate the effectiveness of LFME and its strong applicability in the generalizable semantic segmentation task.

## 6 Analysis

Analyses in this section are conducted on the widely-used PACS dataset unless otherwise mentioned. Experimental settings are same as that detailed in Sec. 5.1. Please see the appendix for more analysis.

### A Free Lunch for DG

As stated in Sec. 4.1, when the basis of discrimination is not compromised (\(q_{*}\) corresponds to a larger value in the label and vice versa for \(q_{c}\) for \( c*\)), the increase of \(q_{c}\) can encourage the model to learn more information that is shared with other classes, and use them to improve generalization. Based on this analysis, it seems that using the one-hot label to regularize the logit is also reasonable. To validate this hypothesis, we replace \(q^{E}\) in Eq. (3) with the ground truth and reformulate \(_{all}\) into,

\[_{all}=((z),y)+\|z-y\|^ {2},\ \ z=T(x).\] (9)

Denoting as ERM+, results listed in the second row in Tab. 3 suggest that this idea can improve ERM in all unseen domains. Because the hard sample information is absent in this strategy, we observe it performs inferior to LFME. However, this alternative does not require experts or any other special designs, _even the setting of the hyper-parameter \(\) cannot be wrong: Eq. (9) will approximate the baseline either with \(=0\) or \(\) approximates +\(\)_. Thus, we argue this simple modification can serve as a free lunch to improve DG. More evaluations of this idea are in our appendix.

    &  &  &  \\    & Art & Cartoon & Photo & Sketch & & L100 & L38 & L43 & L46 & \\  ERM & 78.0 \(\) 1.3 & 73.4 \(\) 0.8 & 94.1 \(\) 0.4 & 73.6 \(\) 2.2 & 79.8 \(\) 0.4 & 42.1 \(\) 2.5 & 30.1 \(\) 1.2 & 48.9 \(\) 0.6 & 34.0 \(\) 1.1 & 38.8 \(\) 1.0 \\ ERM+ & 81.9 \(\) 0.4 & 75.1 \(\) 0.7 & 94.8 \(\) 0.7 & 73.8 \(\) 2.2 & 81.4 \(\) 0.5 & 46.7 \(\) 2.6 & 37.1 \(\) 1.3 & 53.2 \(\) 0.4 & 34.8 \(\) 1.3 & 42.9 \(\) 0.7 \\ LS & 81.0 \(\) 0.3 & 75.4 \(\) 0.6 & 94.9 \(\) 0.2 & 73.3 \(\) 1.0 & 81.2 \(\) 0.2 & 48.1 \(\) 2.8 & 33.0 \(\) 1.6 & 53.0 \(\) 0.6 & 34.1 \(\) 1.6 & 42.1 \(\) 0.5 \\ MbLS & 81.3 \(\) 0.5 & 75.2 \(\) 0.6 & 94.8 \(\) 0.4 & 75.6 \(\) 0.8 & 81.7 \(\) 0.2 & 44.9 \(\) 3.3 & 39.1 \(\) 2.3 & 52.2 \(\) 0.9 & 33.8 \(\) 1.1 & 42.5 \(\) 1.5 \\ ACLS & 80.8 \(\) 0.3 & 75.9 \(\) 1.0 & 94.9 \(\) 0.4 & 72.3 \(\) 3.9 & 81.0 \(\) 0.6 & 45.6 \(\) 4.7 & 36.8 \(\) 0.8 & 48.9 \(\) 1.3 & 34.1 \(\) 1.7 & 41.4 \(\) 1.4 \\ LFME & 81.0 \(\) 0.9 & 76.5 \(\) 0.9 & 94.6 \(\) 0.5 & 77.4 \(\) 0.2 & 82.4 \(\) 0.1 & 53.4 \(\) 0.4 & 40.7 \(\) 2.4 & 54.9 \(\) 0.4 & 36.4 \(\) 0.7 & 46.3 \(\) 0.5 \\   

Table 3: Evaluations of free lunch for DG (_i.e_. ERM+) and different LS ideas (_i.e_. LS , MbLS , and ACLS ). We use the suggested settings in their original papers for evaluating.

Figure 3: Qualitative comparisons. The compared methods make unsatisfactory predictions about several objects, such as clouds with varying shapes, car logo, or people and car in the shadow. Please zoom in for a better view.

### Compare with Label Smoothing

As detailed in Sec. 4.1, LFME will explicitly constrain the probability of the target model within a smaller range. This effect may resemble the LS technique that aims to implicitly calibrate the output probability. However, compared to LS, LFME has two advantages. **First**, LFME does not involve heuristic designs of hyper-parameters for determining its probability, which is essentially required in existing LS ideas, such as \(\) in  and predefined margin in [50; 58], avoiding the possibility of deteriorating the performance when not choosing them properly; **Second**, LFME can explicitly mine hard samples from the experts, further ensuring improvements for DG. We evaluate 3 LS methods in both PACS and the difficult TerraInc datasets: (1) the pioneer LS method from ); (2) margin-based LS (MbLS)  that penalizes logits deviate from the maxima; (3) adaptive and conditional LS (ACLS) that can adaptively determine the degree of smoothing for different classes. Results are illustrated in 3rd-5th columns in Tab. 3. We observe that although these LS methods can improve the baseline, they are all inferior to LFME, validating the effectiveness of LFME against LS.

### Compare with Other Knowledge Distillation Ideas

To test the effectiveness of our logit regularization (_i.e_. \(\|z-q^{E}\|^{2}\)), we compare it with several other KD options, namely different combinations of the logits or probabilities from the target model and the experts, including \(\|z-z^{E}\|^{2}\) (which is the basic design in Meta-DMoE ), \(\|q-z^{E}\|^{2}\), and \(\|q-q^{E}\|^{2}\). Moreover, we also compare it with a common practice in KD that uses the probability from the teacher (_i.e_. experts) as a label for the student (_i.e_. target model), which reformulates the Eq. (3) into: \(_{all}=(1-)(q,y)+ (q,q^{E})\), and denoted as \((q,q^{E})\) in Tab. 4. Following the suggestion in , we gradually increase \(\) over the iteration steps to achieve better performance. Results listed in 2nd-5th columns in Tab. 4 show that not all KD strategies can improve DG and the logit regularization choice achieves the best result in terms of average accuracy. This can be explained by the analysis in Sec. 4.1 that our logit regularization can help using more information. Please also the explanation in Sec. C for details.

### Compare with Naive Aggregation Ideas

To examine the effectiveness of LFME, we compare it with three different variants that employ all the experts during inference: (1) Avg_Expt, which averages the output from all experts for prediction, similar to DAELDG ; (2) Model soup (MS)_EXPT, which uniformly combines the weights of different experts. This is inspired by the MS idea in ; (3) Conf_Expt that utilizes the output from the most confident expert as the final prediction. Inspired by the finding in , the expert with the output of the smallest entropy value is regarded as the most confident one in a test sample; (4) Dyn_Expt, which is a learning-based approach that estimates the domain label of each sample and dynamically assigns corresponding weights to the experts via a weighting module. Results are listed in 6th-9th columns in Tab. 4, where both the designs of handcraft (_i.e_. Avg_Expt, MS_Expt, and Conf_Expt) and learning-based (_i.e_. Dyn_Expt) aggregation skills fail to improve the baseline model. This is because the hand-craft designs in Avg_Expt, MS_Expt, and Conf_Expt are rather unrealistic in practice as different models may contribute differently in the test phase, we thus cannot use a simple average or select the most confident expert for predicting. Meanwhile, the learning-based Dyn_Expt will inevitably introduce another generalization problem regarding the weighting module, complicating the setting. In comparison, LFME avoids the nontrivial aggregation design and can improve ERM in all source domains, further validating its effectiveness.

    &  &  \\    & Art & Cartoon & Photo & Sketch & \\  ERM & 78.0\(\)1.3 & 73.4\(\)0.8 & 94.1\(\)0.4 & 73.6\(\)2.2 & 79.8\(\)0.4 \\   \\  \(\|z-z^{E}\|^{2}\) & 77.8\(\)0.6 & 73.2\(\)0.7 & 94.1\(\)0.5 & 74.3\(\)1.1 & 79.9\(\)0.2 \\ \(\|q-z^{E}\|^{2}\) & 76.4\(\)0.7 & 75.7\(\)1.3 & 94.0\(\)0.2 & 72.4\(\)1.4 & 79.7\(\)0.5 \\ \(\|q-q^{E}\|^{2}\) & 81.3\(\)1.3 & 74.8\(\)1.4 & 94.1\(\)0.5 & 74.0\(\)2.8 & 81.1\(\)1.1 \\ \((q,q^{E})\) & 82.1\(\)0.8 & 73.6\(\)0.2 & 92.6\(\)0.9 & 73.4\(\)2.1 & 80.4\(\)0.4 \\   \\  Avg_Expt & 78.4\(\)1.3 & 65.0\(\)1.6 & 92.4\(\)0.3 & 71.8\(\)0.5 & 76.9\(\)0.1 \\ MS_Expt & 76.8\(\)2.3 & 63.0\(\)0.6 & 93.4\(\)0.2 & 72.6\(\)1.5 & 76.5\(\)0.6 \\ Conf_Expt & 74.8\(\)0.8 & 64.8\(\)1.8 & 91.9\(\)0.5 & 72.6\(\)2.6 & 76.0\(\)0.8 \\ Dyn\_Expt & 68.4\(\)1.8 & 65.1\(\)1.1 & 92.3\(\)0.5 & 68.1\(\)1.9 & 73.5\(\)0.3 \\  Ours & 81.0\(\)0.9 & 76.5\(\)0.9 & 94.6\(\)0.5 & 77.4\(\)0.2 & 82.4\(\)0.1 \\   

Table 4: Out-of-domain evaluations of different KD ideas (using different \(_{guid}\) in Eq. (2)) and naive aggregations.

### Does the Target Model Become an Expert on All Source Domains?

The training process of LFME aggregates all professional knowledge from the experts into one target model, aiming to make the target model an expert on all source domains. To examine if the goal has been achieved, we conduct in-domain validations for the target model and compare the performances with that of the experts and the ERM model in the PACS and TerraInc datasets. Note in these experiments, ERM and LFME are trained using the same leave-one-out strategy, and the performances are averaged over the trials on three different target domains. Results are listed in Tab. 5. We observe that ERM performs inferior to the experts in the in-domain setting. The results are not surprising. As stated earlier, when encountering data similar to the source domain, it would be better to rely mostly on the corresponding expert than the model also contaminated with other patterns. In comparison, our method obtains the best results in all source domains because it implicitly focuses more on the hard samples from the experts, which is shown to be an effective way to improve the performance in many arts [33; 84]. These results validate that the proposed strategy can extract professional knowledge from different experts, and enable the target model to become an expert in all source domains.

### Selection of the Hyper-Parameter

Compared with the baseline ERM, our method involves only one additional hyper-parameter (_i.e._\(\) in Eq. 3) which is randomly selected in the range of \([0.01,10]\) in DomainBed. To evaluate the sensitiveness of LFME regarding this hyper-parameter, we conduct experiments in the PACS and TerraInc datasets by tuning it in a larger range. As seen in Tab. 6, LFME can obtain relatively better performance with either \(=0.01\) or \(10\), and it performs on par with ERM even when \(=1000\). These results indicate that our method is insensitive w.r.t the hyper-parameter. This is mainly because a very large \(\) will enforce the model to only learn from domain experts, given that \(q_{*}^{E}\) is mostly aligned with the label \(y\) (as seen in Fig. 2 (a)), the model will perform similarly to another form of the baseline  in this case.

## 7 Conclusion

This paper introduced a simple yet effective method for DG where the professional guidances of experts specialized in specific domains are leveraged. We achieve the guidance by a logit regularization term that enforces similarity between logits of the target model and probability of the corresponding expert. After training, the target model is expected to be an expert on all source domains, thus thriving in arbitrary test domains. Through deeper analysis, we reveal that the proposed strategy implicitly enables the target model to use more information for prediction and mine hard samples from the experts during training. By conducting experiments in related tasks, we show that our method is consistently beneficial to the baseline and performs favorably against existing arts.

**Acknowledgement.** This work was supported by the Centre for Augmented Reasoning, an initiative by the Department of Education, Australian Government.

    &  &  &  \\    & Art & Cartoon & Photo & Sketch & & & L100 & L38 & L43 & L46 & \\   ERM & 94.8 \(\) 0.1 & 96.2 \(\) 0.2 & 98.5 \(\) 0.2 & 96.0 \(\) 0.3 & 96.4 \(\) 0.2 & 95.2 \(\) 0.1 & 91.1 \(\) 0.1 & 89.4 \(\) 0.2 & 85.5 \(\) 0.2 & 90.3 \(\) 0.3 \\ Experts & 95.4 \(\) 0.1 & 96.3 \(\) 0.1 & 98.7 \(\) 0.3 & 96.4 \(\) 0.2 & 96.7 \(\) 0.1 & 95.9 \(\) 0.1 & 92.0 \(\) 0.1 & 90.3 \(\) 0.2 & 86.3 \(\) 0.1 & 91.1 \(\) 0.1 \\ Ours & 95.7 \(\) 0.2 & 96.8 \(\) 0.3 & 98.8 \(\) 0.2 & 96.4 \(\) 0.2 & 96.9 \(\) 0.1 & 96.1 \(\) 0.1 & 92.6 \(\) 0.2 & 90.7 \(\) 0.2 & 87.2 \(\) 0.2 & 91.6 \(\) 0.2 \\   

Table 5: In-domain evaluations of different models.

    &  &  &  &  \\    & Art & Cartoon & Photo & Sketch & & L100 & L38 & L43 & L46 & \\   ERM & 94.8 \(\) 0.1 & 96.2 \(\) 0.2 & 98.5 \(\) 0.2 & 96.0 \(\) 0.3 & 96.4 \(\) 0.2 & 95.2 \(\) 0.1 & 91.1 \(\) 0.1 & 89.4 \(\) 0.2 & 85.5 \(\) 0.2 & 90.3 \(\) 0.3 \\ Experts & 95.4 \(\) 0.1 & 96.3 \(\) 0.1 & 98.7 \(\) 0.3 & 96.4 \(\) 0.2 & 96.7 \(\) 0.1 & 95.9 \(\) 0.1 & 92.0 \(\) 0.1 & 90.3 \(\) 0.2 & 86.3 \(\) 0.1 & 91.1 \(\) 0.1 \\ Ours & 95.7 \(\) 0.2 & 96.8 \(\) 0.3 & 98.8 \(\) 0.2 & 96.4 \(\) 0.2 & 96.9 \(\) 0.1 & 96.1 \(\) 0.1 & 92.6 \(\) 0.2 & 90.7 \(\) 0.2 & 87.2 \(\) 0.2 & 91.6 \(\) 0.2 \\   

Table 5: In-domain evaluations of different models.