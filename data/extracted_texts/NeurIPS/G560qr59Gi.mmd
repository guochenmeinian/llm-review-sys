# Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data

Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data

 Yiwen Kou

Zixiang Chen1

Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{evankou,chenzx19,qgu}@cs.ucla.edu

Equal contribution37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to \(1\), whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Additionally, we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically. Experiments on both synthetic and real data backup our theoretical findings.

## 1 Introduction

Neural networks have achieved remarkable success in a variety of applications, such as image and speech recognition, natural language processing, and many others. Recent studies have revealed that the effectiveness of neural networks is attributed to their implicit bias towards particular solutions which enjoy favorable properties. Understanding how this bias is affected by factors such as network architecture, optimization algorithms and data used for training, has become an active research area in the field of deep learning theory.

The literature on the implicit bias in neural networks has expanded rapidly in recent years (Vardi, 2022), with numerous studies shedding light on the implicit bias of gradient flow (GF) with a wide range of neural network architecture, including deep linear networks (Ji and Telgarsky, 2018, 2020; Gunasekar et al., 2018), homogeneous networks (Lyu and Li, 2019; Vardi et al., 2022a) and more specific cases (Chizat and Bach, 2020; Lyu et al., 2021; Frei et al., 2022b; Safran et al., 2022). The implicit bias of gradient descent (GD), on the other hand, is better understood for linear predictors (Soudry et al., 2018) and smoothed neural networks (Lyu and Li, 2019; Frei et al., 2022b). Therefore, an open question still remains:

_What is the implicit bias of leaky ReLU and ReLU networks trained by gradient descent?_

In this paper, we will answer this question by investigating gradient descent for both two-layer leaky ReLU and ReLU neural networks on specific training data, where \(\{_{i}\}_{i=1}^{n}\) are nearly-orthogonal(Frei et al., 2022b), i.e., \(\|_{i}\|_{2}^{2} Cn_{k i}|_{i}, _{k}|\) with a constant \(C\). Our main results are summarized as follows:

* For two-layer leaky ReLU networks trained by GD, we demonstrate that the neuron activation pattern reaches a stable state beyond a specific time threshold and provide rigorous proof of the convergence of the stable rank of the weight matrix to 1, matching the results of Frei et al. (2022b) regarding gradient flow.
* For two-layer ReLU networks trained by GD, we proved that the stable rank of weight matrix can be upper bounded by a constant. Moreover, we present an illustrative example using completely orthogonal training data, showing that the stable rank of the weight matrix converges to a value approximately equal to \(2\). To the best of our knowledge, this is the first implicit bias result for two-layer ReLU networks trained by gradient descent beyond the Karush-Kuhn-Tucker (KKT) point.
* For both ReLU and leaky ReLU networks, we show that weight norm increases at the rate of \(((t))\) and the training loss converges to zero at the rate of \((t^{-1})\), where \(t\) is the number of gradient descent iterations. This improves upon the \(O(t^{-1/2})\) rate proved in Frei et al. (2022b) for the case of a two-layer _smoothed_ leaky ReLU network trained by gradient descent and aligns with the results by Lyu and Li (2019) for smooth homogeneous networks. Additionally, we prove that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically.

## 2 Related Work

Implicit bias in neural networks.Recent years have witnessed significant progress on implicit bias in neural networks trained by gradient flow (GF). Lyu and Li (2019) and Ji and Telgarsky (2020) demonstrated that homogeneous neural networks trained with exponentially-tailed classification losses converge in direction to the KKT point of a maximum-margin problem. Lyu et al. (2021) studied the implicit bias in two-layer leaky ReLU networks trained on linearly separable and symmetric data, showing that GF converges to a linear classifier maximizing the \(_{2}\) margin. Frei et al. (2022b) showed that two-layer leaky ReLU networks trained by GF on nearly-orthogonal data produce a \(_{2}\)-max-margin solution with a linear decision boundary and rank at most two. Other works studying the implicit bias of classification using GF in nonlinear two-layer networks include Chizat and Bach (2020); Phuong and Lampert (2021); Sarussi et al. (2021); Safran et al. (2022); Vardi et al. (2022a,b); Timor et al. (2023). Although implicit bias in neural networks trained by GF has been extensively studied, research on implicit bias in networks trained by gradient descent (GD) remains limited. Lyu and Li (2019) examined smoothed homogeneous neural network trained by GD with exponentially-tailed losses and proved a convergence to KKT points of a max-margin problem. Frei et al. (2022b) studied two-layer smoothed leaky ReLU trained by GD and revealed the implicit bias towards low-rank networks. Other works studying implicit bias towards rank minimization include Ji and Telgarsky (2018, 2020); Timor et al. (2023); Arora et al. (2019); Razin and Cohen (2020); Li et al. (2021). Lastly, Vardi (2022) provided a comprehensive literature survey on implicit bias.

Benign overfitting and double descent in neural networks.A parallel line of research aims to understand the benign overfitting phenomenon (Bartlett et al., 2020) of neural networks by considering a variety of models. For example, Allen-Zhu and Li (2020); Jelassi and Li (2022); Shen et al. (2022); Cao et al. (2022); Kou et al. (2023) studied the generalization performance of two-layer convolutional networks on patch-based data models. Several other papers studied high-dimensional mixture models (Chatterji and Long, 2021; Wang and Thrampoulidis, 2022; Cao et al., 2021; Frei et al., 2022a). Another thread of work Belkin et al. (2020); Hastie et al. (2022); Wu and Xu (2020); Mei and Montanari (2019); Liao et al. (2020) focuses on understanding the double descent phenomenon first empirically observed by Belkin et al. (2019).

## 3 Preliminaries

In this section, we introduce the notation, fully connected neural networks, the gradient descent-based training algorithm, and a data-coorrelated decomposition technique.

Notation.We use lower case letters, lower case bold face letters, and upper case bold face letters to denote scalars, vectors, and matrices respectively. For a vector \(=(v_{1},,v_{d})^{}\), we denote by \(\|\|_{2}:=(_{j=1}^{d}v_{j}^{2})^{1/2}\) its \(_{2}\) norm. For a matrix \(^{m n}\), we use \(\|\|_{F}\) to denote its Frobenius norm and \(\|\|_{2}\) its spectral norm. We use \((z)\) as the function that is \(1\) when \(z>0\) and \(-1\) otherwise. For a vector \(^{d}\), we use \([]_{i}\) to denote the \(i\)-th component of the vector. For two sequence \(\{a_{k}\}\) and \(\{b_{k}\}\), we denote \(a_{k}=O(b_{k})\) if \(|a_{k}| C|b_{k}|\) for some absolute constant \(C\), denote \(a_{k}=(b_{k})\) if \(b_{k}=O(a_{k})\), and denote \(a_{k}=(b_{k})\) if \(a_{k}=O(b_{k})\) and \(a_{k}=(b_{k})\). We also denote \(a_{k}=o(b_{k})\) if \(|a_{k}/b_{k}|=0\).

Two-layer fully connected neural network.We consider a two-layer neural network described as follows: its first layer consists of \(m\) positive neurons and \(m\) negative neurons; its second layer parameters are fixed as \(+1/m\) and \(-1/m\) respectively for positive and negative neurons. Then the network can be written as \(f(,)=F_{+1}(_{+1},)-F_{-1}( _{-1},)\), where the partial network function of positive and negative neurons, i.e., \(F_{+1}(_{+1},)\), \(F_{-1}(_{-1},)\), are defined as:

\[F_{j}(_{j},)=^{m}} (_{j,r},)\] (3.1)

for \(j\{ 1\}\). Here, \((z)\) represents the activation function. For ReLU, \((z)=\{0,z\}\), and for leaky ReLU, \((z)=\{ z,z\}\), where \((0,1)\). \(_{j}^{m d}\) is the collection of model weights associated with \(F_{j}\), and \(_{j,r}^{d}\) denotes the weight vector for the \(r\)-th neuron in \(_{j}\). We use \(\) to denote the collection of all model weights.

Gradient Descent.Given a training data set \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d} \{ 1\}\), instead of considering the gradient flow (GF) that is commonly studied in prior work on the implicit bias, we use gradient descent (GD) to optimize the empirical loss on the training data

\[L_{S}()=_{i=1}^{n}(y_{i} f( ,_{i})),\]

where \((z)=(1+(-z))\) is the logistic loss, and \(S=\{(_{i},y_{i})\}_{i=1}^{n}\) is the training data set. The gradient descent update rule of each neuron in the two-layer neural network can be written as

\[_{j,r}^{(t+1)}=_{j,r}^{(t)}-_{ _{j,r}}L_{S}(^{(t)})=_{j,r}^{(t)}-_{i=1}^{n}_{i}^{(t)}^{}(_{j,r}^{(t )},_{i}) jy_{i}_{i}\] (3.2)

for all \(j\{ 1\}\) and \(r[m]\), where we introduce a shorthand notation \(_{i}^{(t)}=^{}[y_{i} f(^{(t)},_{i })]\) and assume the derivative of the ReLU activation function at 0 is \(^{}(0)=1\) without loss of generality. Here \(>0\) is the learning rate. We initialize the gradient descent by Gaussian initialization, where all the entries of \(^{(0)}\) are sampled from i.i.d. Gaussian distributions \((0,_{0}^{2})\) with \(_{0}^{2}\) being the variance.

## 4 Main Results

In this section, we present our main theoretical results. For the training data set \(=\{(_{i},y_{i})\}_{i=1}^{n}^{d} \{ 1\}\), let \(R_{}=_{i}\|_{i}\|_{2}\), \(R_{}=_{i}\|_{i}\|_{2}\), \(p=_{i k}|_{i},_{k}|\), and suppose \(R=R_{}/R_{}\) is at most an absolute constant. For simplicity, we only consider the dependency on \(t\) when characterizing the convergence rates of the weight matrix related quantities and the training loss, omitting the dependency on other parameters such as \(m,n,_{0},R_{},R_{}\).

**Theorem 4.1** (Leaky ReLU Networks).: For two-layer neural network defined in (3.1) with leaky ReLU activation \((z)=\{ z,z\},(0,1)\). Assume the training data satisfy \(R_{}^{2} CR^{2}^{-4}np\) for some sufficiently large constant \(C\). For any \((0,1)\), if the learning rate \((CR_{}^{2}/nm)^{-1}\) and the initialization scale \(_{0}CR_{}^{-1}\), then with probability at least \(1-\) over the random initialization of gradient descent, the trained network satisfies:

* The \(_{2}\) norm of each neuron increases to infinity at a logarithmic rate: \(\|_{j,r}^{(t)}\|_{2}=((t))\) for all \(j\{ 1\}\) and \(r[m]\).

* Throughout the gradient descent trajectory, the stable rank of the weights \(_{j}^{(t)}\) for all \(j\{ 1\}\) satisfies \[_{t}\|_{j}^{(t)}\|_{F}^{2}/\|_{j}^{(t)}\|_{2}^ {2}=1,\] with a convergence rate of \(O(1/(t))\).
* Gradient descent will find \(^{(t)}\) such that all the training data points possess the same normalized margin asymptotically: \[_{t}y_{i}f(^{(t)}/\|^{(t)}\|_{F}, _{i})-y_{k}f(^{(t)}/\|^{(t)}\|_{F},_{ k})=0,\, i,k[n].\] If we assume that \(^{(t)}\) converges in direction, i.e., the limit of \(^{(t)}/\|^{(t)}\|_{F}\) exists, denoted by \(}\), then there exists a scaling factor \(>0\) such that \(}\) satisfies the Karush-Kuhn-Tucker (KKT) conditions for the following max-margin problem: \[_{}\|\|_{F}^{2}, y _{i}f(,_{i}) 1,\, i[n].\] (4.1)
* The empirical loss converges to zero at the following rate: \(L_{S}(^{(t)})=(t^{-1})\).

**Remark 4.2**.: In Theorem 4.1, we show that when using the leaky ReLU activation function on nearly orthogonal training data, gradient descent asymptotically finds a network with a stable rank of \(_{j}\) equal to \(1\). Additionally, we demonstrate that gradient descent will find a network by which all the training data points share the same normalized margin asymptotically. Moreover, if we assume the weight matrix converges in direction, then its limit will satisfy the KKT conditions of the max-margin problem (4.1). Furthermore, we analyze the rate of weight norm increase and the convergence rate of the stable rank for gradient descent, both of which exhibit a logarithmic dependency in \(t\).

**Theorem 4.3** (ReLU Networks).: For two-layer neural network defined in (3.1) with ReLU activation \((z)=\{0,z\}\). Assume the training data satisfy \(R_{}^{2} CR^{2}np\) for some sufficiently large constant \(C\). For any \((0,1)\), if the neural network width \(m C(n/)\), learning rate \((CR_{}^{2}/nm)^{-1}\) and initialization scale \(_{0}CR_{}^{-1}\), then with probability at least \(1-\) over the random initialization of gradient descent, the trained network satisfies:

* The Frobenious norm and the spectral norm of weight matrix increase to infinity at a logarithmic rate: \(\|_{j}^{(t)}\|_{F}=((t))\) and \(\|_{j}^{(t)}\|_{2}=((t))\) for all \(j\{ 1\}\).
* Throughout the gradient descent trajectory, the stable rank of the weights \(_{j}^{(t)}\) for all \(j\{ 1\}\) satisfies, \[_{t}\|_{j}^{(t)}\|_{F}^{2}/\|_{j}^{(t)}\|_ {2}^{2} c,\] where \(c\) is an absolute constant.
* Gradient descent will find a \(^{(t)}\) such that all the training data points possess the same normalized margin asymptotically: \[_{t}y_{i}f(^{(t)}/\|^{(t)}\|_{F}, _{i})-y_{k}f(^{(t)}/\|^{(t)}\|_{F},_{ k})=0,\, i,k[n].\]
* The empirical loss converges to zero at the following rate: \(L_{S}(^{(t)})=(t^{-1})\).

**Remark 4.4**.: For ReLU networks, we provide an example in the appendix concerning fully orthogonal training data and prove that the activation pattern during training depends solely on the initial activation state. Specifically, when training a two-layer ReLU network with gradient descent using such data, the stable rank of the network's weight matrix \(_{j}\) converges to approximately 2. It is worth noting that this stable rank value is higher than the stable rank achieved by leaky ReLU networks, which is \(1\).

Comparison with previous work.One notable related work is Lyu et al. (2021), which also investigates the implicit bias of two-layer leaky ReLU networks. The main distinction between our work and Lyu et al. (2021) is the optimization method employed. We utilize gradient descent, whereas they utilize gradient flow. Additionally, our assumption is that the training data is nearly-orthogonal,while they assume the training data is symmetric. Our findings are more closely related to the work by Frei et al. (2022b), which investigates both gradient flow and gradient decent. In both our study and Frei et al. (2022b), we examine two-layer neural networks with leaky ReLU activations. However, they focus on networks trained via gradient flow, while we investigate networks trained using gradient descent. For the gradient descent approach, Frei et al. (2022b) provide a constant stable rank upper bound for smoothed leaky ReLU. In contrast, we prove that the stable rank of leaky ReLU networks converges to \(1\), aligning with the implicit bias of gradient flow proved in Frei et al. (2022b). Furthermore, they presented an \(O(t^{-1/2})\) convergence rate for the empirical loss, whereas our convergence rate is \((t^{-1})\). Another related work is Lyu and Li (2019), which studied smooth homogeneous networks trained by gradient descent. Our results on the rate of weight norm increase and the convergence rate of training loss match those in Lyu and Li (2019), despite the fact that we study non-smooth homogeneous networks. It is worth noting that Lyu and Li (2019); Lyu et al. (2021); Frei et al. (2022b) demonstrated that neural networks trained by gradient flow converge to a Karush-Kuhn-Tucker (KKT) point of the max-margin problem. We do not have such a result unless we assume the directional convergence of the weight matrix.

## 5 Overview of Proof Techniques

In this section, we discuss the key techniques we invent in our proofs to analyze the implicit bias of ReLU and leaky ReLU networks.

### Refined Analysis of Decomposition Coefficient

_Signal-noise decomposition_, a technique initially introduced by Cao et al. (2022), is used to analyze the learning dynamics of two-layer convolutional networks. This method decomposes the convolutional filters into a linear combination of initial filters, signal vectors, and noise vectors, converting the neural network learning into a dynamical system of coefficients derived from the decomposition. In this work, we extend the signal-noise decomposition to _data-correlated decomposition_ to facilitate the analysis of the training dynamic for two-layer fully connected neural networks.

**Definition 5.1** (Data-correlated Decomposition).: Let \(_{j,r}^{(t)}\), \(j\{ 1\}\), \(r[m]\) be the weights of first-layer neurons at the \(t\)-th iteration of gradient descent. There exist unique coefficients \(_{j,r,i}^{(t)}\) such that

\[_{j,r}^{(t)}=_{j,r}^{(0)}+_{i=1}^{n}_{j,r,i}^{(t) }\|_{i}\|_{2}^{-2}_{i}.\] (5.1)

By defining \(_{j,r,i}^{(t)}:=_{j,r,i}^{(t)}\,(_{j,r,i}^{(t )} 0)\), \(_{j,r,i}^{(t)}:=_{j,r,i}^{(t)}\,(_{j,r,i}^{ (t)} 0)\), (5.1) can be further written as

\[_{j,r}^{(t)}=_{j,r}^{(0)}+_{i=1}^{n}_ {j,r,i}^{(t)}\|_{i}\|_{2}^{-2}_{i}+_{i=1}^{ n}_{j,r,i}^{(t)}\|_{i}\|_{2}^{-2}_{i}.\] (5.2)

As an extension of the signal-noise decomposition first proposed in Cao et al. (2022) for analyzing two-layer convolutional networks, _data-correlated decomposition_ defined in Definition 5.1 can be used to analyze two-layer fully-connected network, where the normalization factors \(\|_{i}\|_{2}^{-2}\) are introduced to ensure that \(_{j,r,i}^{(t)}_{j,r}^{(t)},_{i}\). This is also inspired by previous works by Lyu and Li (2019); Frei et al. (2022b), which demonstrate that \(\) converges to a KKT point of the max-margin problem. This implies that \(_{j,r}^{()}/\|_{j,r}^{()}\|_{2}\) can be expressed as a linear combination of the training data \(\{_{i}\}_{i=1}^{n}\), with the coefficient \(_{i}\) corresponding to \(_{j,r,i}^{(t)}\) in our analysis. This technique does not rely on the strictly increasing and smoothness properties of the activation function and will serve as the foundation for our analysis. Let us first investigate the update rule of the coefficient \(_{j,r,i}^{(t)},_{j,r,i}^{(t)}\).

**Lemma 5.2**.: The coefficients \(_{j,r,i}^{(t)},_{j,r,i}^{(t)}\) defined in Definition 5.1 satisfy the following iterative equations:

\[_{j,r,i}^{(0)},_{j,r,i}^{(0)}=0,\] (5.3)\[_{j,r,i}^{(t+1)} =_{j,r,i}^{(t)}-_{i}^{( t)}^{}(_{j,r}^{(t)},_{i}) \|_{i}\|_{2}^{2}(y_{i}=j),\] (5.4) \[_{j,r,i}^{(t+1)} =_{j,r,i}^{(t)}+_{i}^{(t)} ^{}(_{j,r}^{(t)},_{i})\| _{i}\|_{2}^{2}(y_{i}=-j),\] (5.5)

for all \(r[m]\), \(j\{ 1\}\) and \(i[n]\).

To study implicit bias, the first main challenge is to generalize the decomposition coefficient analysis to infinite time. The signal-noise decomposition used in Cao et al. (2022); Kou et al. (2023) requires early stopping with threshold \(T^{*}\) to facilitate their analysis. They only provided upper bounds of \(4(T^{*})\) for \(_{j,r,i}^{(t)},|_{j,r,i}^{(t)}|\) (See Proposition 5.3 in Cao et al. (2022), Proposition 5.2 in Kou et al. (2023)), and then carried out a two-stage analysis. To obtain upper bounds for \(_{j,r,i}^{(t)},|_{j,r,i}^{(t)}|\), they used an upper bound for \(|_{i}^{(t)}|\) and directly plugged it into (5.4) and (5.5) to demonstrate that \(_{j,r,i}^{(t)}\) and \(|_{j,r,i}^{(t)}|\) would not exceed \(4(T^{*})\), which is a fixed value related to the early stopping threshold. Therefore, dealing with infinite time requires new techniques. To overcome this difficulty, we propose a _refined analysis of decomposition coefficients_ which generalizes Cao et al. (2022)'s technique. We first give the following key lemma.

**Lemma 5.3**.: For non-negative real number sequence \(\{x_{t}\}_{t=0}^{}\) satisfying

\[C_{1}(-x_{t}) x_{t+1}-x_{t} C_{2}(-x_{t}),\] (5.6)

it holds that

\[((-x_{0})+C_{1} t) x_{t}((-x_{0})+C_{2}(C_{2} ) t).\] (5.7)

We can establish the relationship between (5.4), (5.5) and inequality (5.6) if we are able to express \(|_{i}^{(t)}|\) using coefficients \(_{j,r,i}^{(t)}\) and \(|_{j,r,i}^{(t)}|\). To achieve this, we can first approximate \(_{i}^{(t)}\) using the margin \(y_{i}f(^{(t)},_{i})\) and then approximate \(F_{j}(_{j}^{(t)},_{i})\) using the coefficients \(_{j,r,i}^{(t)}\). The approximation is given as follows:

\[_{i}^{(t)}=((-y_{i}f(^{(t)},_{i })))=F_{-y_{i}}(_{-y_{i}}^{(t)},_{ i})-F_{y_{i}}(_{y_{i}}^{(t)},_{i}),\] (5.8) \[F_{j}(_{j}^{(t)},_{i})- _{r=1}^{m}_{j,r,i}^{(t)}_{i^{} i} _{r=1}^{m}|_{j,r,i^{}}^{(t)}|R_{}^{-2}p,\] (5.9)

From (5.9), one can see that we need to decouple \(_{i}^{(t)}\) from \(|_{j,r,i^{}}^{(t)}|(i^{} i)\). In order to accomplish this, we also prove the following lemma, which demonstrates that the ratio between \(_{r=1}^{m}|_{j,r,i}^{(t)}|\) and \(_{r=1}^{m}|_{j,r,i^{}}^{(t)}|(i^{} i)\) will maintain a constant order throughout the training process. Here, we present the lemma for leaky ReLU networks.

**Lemma 5.4** (leaky ReLU automatic balance).: For two-layer leaky ReLU network defined in (3.1), for any \(t 0\), we have \(_{r=1}^{m}|_{j,r,i}^{(t)}| c^{2}_{r=1}^{m}|_{j,r,i^{ }}^{(t)}|\) for any \(j\{ 1\}\) and \(i,i^{}[n]\), where \(c\) is a constant.

By Lemma 5.4, we can approximate the neural network output using (5.9). This approximation expresses the output \(F_{j}(_{j}^{(t)},_{i})\) as a sum of the coefficients \(_{j,r,i}^{(t)}\):

\[F_{j}(_{j}^{(t)},_{i})R_{ }^{-2}pn}{m}_{r=1}^{m}_{j,r,i}^{(t)}.\] (5.10)

By combining (5.4), (5.5), (5.8), and (5.10), we obtain the following relationship:

\[_{r=1}^{m}|_{j,r,i}^{(t+1)}|-_{r=1}^{m}|_ {j,r,i}^{(t)}|=_{i}\|_{2}^{2}}{nm} -R_{}^{-2}pn}{m}_{r=1}^{m}|_ {j,r,i}^{(t)}|.\]

This relationship aligns with the form of (5.6), if we set \(x_{t}=R_{}^{-2}pn}{m}_{r=1}^{m}|_{j,r,i}^{(t)}|\). Thus, we can directly apply Lemma 5.3 to gain insights into the logarithmic rate of increase for the average magnitudes of the coefficients \(_{r=1}^{m}|^{(t)}_{j,r,i}|\), which in turn implies that \(\|^{(t)}_{j,r}\|_{2}=( t)\) and \(\|^{(t)}\|_{F}=( t)\). In the case of ReLU networks, we have the following lemma that provides automatic balance:

**Lemma 5.5** (ReLU automatic balance).: For two-layer ReLU network defined in (3.1), there exists a constant \(c\) such that for any \(t 0\), we have \(|^{(t)}_{y_{i},r,i}| c|^{(t)}_{j,r^{},i^{}}|\) for any \(j\{ 1\}\), \(r S^{(0)}_{i}:=\{r[m]:^{(0)}_{y_{i},r},_{i}  0\}\), \(r^{}[m]\) and \(i,i^{}[n]\).

The automatic balance lemma guarantees that the magnitudes of coefficients related to the neurons of class \(y_{i}\), which are activated by \(_{i}\) during initialization, dominate those of other classes. With the help of Lemma 5.5, we can get the following approximation for the margin \(y_{i}f(^{(t)},_{i})\):

\[F_{y_{i}}(^{(t)}_{y_{i}},_{i})-F_{-y_{i}}(^{(t) }_{-y_{i}},_{i})}^{-2}pn}{m}_{ r S^{(0)}_{i}}^{(t)}_{y_{i},r,i}.\] (5.11)

By combining (5.4), (5.5), (5.8) and (5.11), we obtain the following relationship:

\[_{r S^{(0)}_{i}}|^{(t+1)}_{y_{i},r,i}|-_{r S^{(0)}_{i}}| ^{(t)}_{y_{i},r,i}|=_{i}\|_{2}^{2}|S^{(0)}_ {i}|}{nm}-}^{-2}pn}{m}_{ r S^{(0)}_{i}}^{(t)}_{y_{i},r,i},\]

which precisely matches the form of (5.6) by setting \(x_{t}=}^{-2}pn}{m}_{r S^{(0)}_{i}}^{(t)}_{ y_{i},r,i}\). Therefore, we can directly apply Lemma 5.3 and obtain the logarithmic increasing rate of \(|^{(t)}_{y_{i},r,i}|\) for \(r S^{(0)}_{i}\). Consequently, this implies that \(\|^{(t)}\|_{F}=( t)\).

### Analysis of Activation Pattern

One notable previous work (Frei et al., 2022b) provided a constant upper bound for the stable rank of two-layer smoothed leaky ReLU networks trained by gradient descent in their Theorem 4.2. To achieve a better stable rank bound, we characterize the activation pattern of leaky ReLU network neurons after a certain threshold time \(T\) in the following lemma.

**Lemma 5.6** (leaky ReLU activation pattern).: Let \(T=C^{-1}nmR_{}^{-2}\). For two-layer leaky ReLU network defined in (3.1), for any \(t T\), it holds that \((^{(t)}_{j,r},_{i})=jy_{i}\) for any \(j\{ 1\}\) and \(r[m]\).

Lemma 5.6 indicates that the activation pattern will not change after time \(T\). Given Lemma 5.6, we can get \(^{}(^{(t)}_{j,r},_{i})=\) for \(j y_{i}\) and \(^{}(^{(t)}_{j,r},_{i})=1\) for \(j=y_{i}\). Plugging this into (5.4) and (5.5) can give the following useful lemma.

**Lemma 5.7**.: Let \(T\) be defined in Lemma 5.6. For \(t T\), it holds that

\[^{(t)}_{y_{i},r,i}-^{(T)}_{y_{i},r, i}=^{(t)}_{y_{i},r^{},i}-^{(T)}_{y_{i},r,i}, ^{(t)}_{-y_{i},r,i}-^{(T)}_{-y_{i},r,i}= ^{(t)}_{-y_{i},r^{},i}-^{(T)}_{-y_{i},r^ {},i},\] \[^{(t)}_{y_{i},r,i}-^{(T)}_{y_{i},r, i}=(^{(t)}_{-y_{i},r^{},i}-^{(T)}_{-y_{i},r^ {},i})/,\]

for any \(i[n]\) and \(r,r^{}[m]\).

This lemma reveals that beyond a certain time threshold \(T\), the increase in \(^{(t)}_{j,r,i}\) is consistent across neurons within the same positive or negative class. However, for neurons belonging to the oppose class, this increment in \(^{(t)}_{j,r,i}\) is scaled by a factor equivalent to the slope of the leaky ReLU function \(\). From this and (5.1), we can demonstrate that \(\|^{(t)}_{j,r}-^{(t)}_{j,r^{}}\|_{2}(r r^{})\) can be upper bounded by a constant, leading to the following inequalities:

\[\|^{(t)}_{j}\|_{F}^{2} m\|^{(t)}_{j,1}\|_{2}^{2}+mC_{1} \|^{(t)}_{j,1}\|_{2}+mC_{2},\,\|^{(t)}_{j}\|_{2}^{2} m \|^{(t)}_{j,1}\|_{2}^{2}-mC_{3}\|^{(t)}_{j,1}\|_{2}-mC_{4}.\]

Considering that \(\|^{(t)}_{j,r}\|_{2}=( t)\), the stable rank of \(^{(t)}_{j}\) naturally converges to a value of \(1\). For ReLU networks, we can partially characterize the activation pattern as illustrated in the following lemma.

**Lemma 5.8**.: (ReLU activation pattern) For two-layer ReLU networks defined in (3.1), for any \(i[n]\), we have \(S_{i}^{(t)} S_{i}^{(t+1)}\) for any \(t 0\), where \(S_{i}^{(t)}:=\{r[m]:_{y_{i},r}^{(t)},_{i}  0\}\).

Lemma 5.8 suggests that once the neuron of class \(y_{i}\) is activated by \(_{i}\), it will remain activated throughout the training process. Leveraging such an activation pattern, we can establish a lower bound for \(\|_{j}^{(t)}\|_{2}\) as \(((t))\). Together with the trivial upper bound for \(\|_{j}^{(t)}\|_{F}\) of order \(O((t))\), it provides a constant upper bound for the stable rank of ReLU network weight.

### Analysis of Margin and Training Loss

Notably, Lyu and Li (2019) established in their Theorem 4.4 that any limit point of smooth homogeneous neural networks \(f(,)\) trained by gradient descent is along the direction of a KKT point for the max-margin problem (4.1). Additionally, Lyu and Li (2019) provided precise bounds on the training loss and weight norm for smooth homogeneous neural networks in their Theorem 4.3 as follows:

\[L_{S}(^{(t)})=}, \|^{(t)}\|_{F}=( t)^{1/L},\]

where \(L\) is the order of the homogeneous network satisfying the property \(f(c,)=c^{L}f(,)\) for all \(c>0\), \(\), and \(\). It is worth noting that the two-layer (leaky) ReLU neural network analyzed in this paper is 1-homogeneous but not smooth. In Section 5.1, we have already demonstrated that \(\|^{(t)}\|_{F}=( t)\), and in this subsection, we will discuss the proof technique employed to show a convergence rate of \((t^{-1})\) for the loss and establish the same normalized margin for all the training data points asymptotically. These results align with those presented by Lyu and Li (2019) regarding smooth homogeneous networks.

By the nearly orthogonal property, we can bound the increment of margin as follows:

\[|_{i}^{(t)}|\|_{i}\|_{2}^{2}  y_{i}f(^{(t+1)},_{i})-y_{i}f(^{(t)}, _{i})|_{i}^{(t)}|\|_{i }\|_{2}^{2}.\] (5.12)

Given (5.8) and (5.12), we can apply Lemma 5.3 and obtain

\[y_{i}f(^{(t)},_{i})- t-(\|_{ i}\|_{2}^{2}/nm) C_{3},\] (5.13)

where \(C_{3}\) is a constant. Utilizing (5.13) and the inequality \(z-z^{2}/2(1+z) z\) for \(z 0\), we can derive:

\[L_{S}(^{(t)}) _{i=1}^{n}-y_{i}f(^{(t)}, _{i})\] \[_{i=1}^{n}- t-(\| _{i}\|_{2}^{2}/nm)+C_{3}=O(t^{-1}),\] \[L_{S}(^{(t)}) _{i=1}^{n}-y_{i}f(^{(t)},_{i})--2y_{i}f(^{(t)},_{i}) =(t^{-1}).\]

To demonstrate that all the training data points attain the same normalized margin as \(t\) goes to infinity, we first observe that (5.12) provides the following bounds for the increment of margin difference:

\[y_{k}f(^{(t+1)},_{k})-y_{i}f(^{(t+ 1)},_{i})\] (5.14) \[ y_{k}f(^{(t)},_{k})-y_{i}f(^{( t)},_{i})+|_{k}^{(t)}|\| _{k}\|_{2}^{2}-|_{i}^{(t)}|\| _{i}\|_{2}^{2}.\]

Now, we consider two cases:

* If the ratio \(|_{i}^{(t)}|/|_{k}^{(t)}|\) is relatively large, then \(y_{k}f(^{(t)},_{k})-y_{i}f(^{(t)},_{i})\) will not increase.
* If the ratio \(|_{i}^{(t)}|/|_{k}^{(t)}|\) is relatively small, then \(y_{k}f(^{(t)},_{k})-y_{i}f(^{(t)},_{i})\) will also be relatively small. In fact, it can be bounded by a constant due to the fact that \(|_{i}^{(t)}|/|_{k}^{(t)}|\) can be approximated by \((y_{k}f(^{(t)},_{k})-y_{i}f(^{(t)},_{i}))\). By (5.14), we can show that\(y_{k}f(^{(t+1)},_{k})-y_{i}f(^{(t+1)},_{i})\) can also be bounded by a constant, provided that the learning rate \(\) is sufficiently small.

By combining both cases, we can conclude that both \(|_{i}^{(t)}|/|_{k}^{(t)}|\) and \(y_{k}f(^{(t)},_{k})-y_{i}f(^{(t)},_{i})\) can be bounded by constants. This result is formally stated in the following lemma.

**Lemma 5.9**.: For two-layer neural networks defined in (3.1) with (leaky) ReLU activation, the following bounds hold for any \(t 0\):

\[y_{i}f(^{(t)},_{i})-y_{k}f(^{(t)},_{k }) C_{1},_{i}^{(t)}/_{k}^{(t)} C_{2},\] (5.15)

for any \(i,k[n]\), where \(C_{1},C_{2}\) are positive constants.

By Lemma 5.9, which shows that the difference between the margins of any two data points can be bounded by a constant, and taking into account that \(\|^{(t)}\|_{F}=( t)\), we can deduce the following result:

\[_{t}y_{i}f(^{(t)}/\|^{(t)}\|_{F}, _{i})-y_{k}f(^{(t)}/\|^{(t)}\|_{F},_{ k})=0, i,k[n].\]

This demonstrates that gradient descent will asymptotically find a neural network in which all the training data points achieve the same normalized margin.

## 6 Experiments

In this section, we present simulations of both synthetic and real data to back up our theoretical analysis in the previous section.

Synthetic-data experiments.Here we generate a synthetic mixture of Gaussian data as follows:

Let \(^{d}\) be a fixed vector representing the signal contained in each data point. Each data point \((,y)\) with predictor \(^{d}\) and label \(y\{-1,1\}\) is generated from a distribution \(\), which we specify as follows:

1. The label \(y\) is generated as a Rademacher random variable, i.e. \([y=1]=[y=-1]=1/2\).
2. A noise vector \(\) is generated from the Gaussian distribution \((,_{p}^{2}_{d})\). And \(\) is assigned as \(y+\) where \(\) is a fixed feature vector.

Specifically, we set training data size \(n=10,d=784\) and train the NN with gradient descent using learning rate \(0.1\) for \(50\) epochs. We set \(\) to be a feature randomly drawn from \((0,10^{-4}_{d})\). We then generate the noise vector \(\) from the Gaussian distribution \((,_{p}^{2})\) with fixed standard deviation \(_{p}=1\). We train the FNN model defined in Section 3 with ReLU (or leaky-RelU) activation function and width \(m=100\). As we can infer from Figure 1, the stable rank will decrease faster for larger leaky ReLU slopes and have a smaller value when epoch \(t\).

Real-data experiments on MNIST dataset.Here we train a two-layer feed-forward neural network defined in Section 3 with ReLU (or leaky-ReLU) functions. The number of widths is

Figure 1: Stable ranks and training loss for different leaky ReLU slopes \(\) across multiple runs. A slope of \(1\) corresponds to linear activation, while a slope of \(0\) corresponds to ReLU activation. Each line represents the mean stable rank or training loss for a given leaky ReLU slope, while the shaded regions indicate the variability of the values (\( 3\) times the standard deviation) across the \(5\) runs.

set as \(m=1000\). We use the Gaussian initialization and consider different weight variance \(_{0}\{0.00001,0.00005,0.0001,0.0005,0.001\}\). We train the NN with stochastic gradient descent with batch size \(64\) and learning rate \(0.1\) for \(10\) epochs. As we can infer from Figures 2 and 3, the stable rank of ReLU or leaky ReLU networks will largely depend on the initialization and the training time. When initialization is sufficiently small, the stable rank will quickly decrease to a small value compared to its initialization values.

## 7 Conclusion and Future Work

This paper employs a data-correlated decomposition technique to examine the implicit bias of two-layer ReLU and Leaky ReLU networks trained using gradient descent. By analyzing the training dynamics, we provide precise characterizations of the weight matrix stable rank limits for both ReLU and Leaky ReLU cases, demonstrating that both scenarios will yield a network with a low stable rank. Additionally, we present an analysis for the convergence rate of the loss function. An important future work is to investigate the directional convergence of the weight matrix in neural networks trained via gradient descent, which is essential to prove the convergence to a KKT point of the max-margin problem. Furthermore, it is important to extend our analysis to fully understand the neuron activation patterns in ReLU networks. Specifically, we will explore whether certain neurons will switch their activation patterns by an infinite number of times throughout the training or if the activation patterns stabilize after a certain number of gradient descent iterations.