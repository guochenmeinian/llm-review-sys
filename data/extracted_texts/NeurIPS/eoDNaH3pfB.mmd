# Black-Box Differential Privacy for Interactive ML

Haim Kaplan

Tel Aviv University and Google Research. haimk@tau.ac.il.

Yishay Mansour

Tel Aviv University and Google Research. mansour.yishay@gmail.com.

Shay Moran

Tel Aviv University and Google Research. smoran@technion.ac.il.

Kobbi Nissim

Georgetown University. kobbi.nissim@georgetown.edu.

Uri Stemmer

Tel Aviv University and Google Research. uuri.co.il.

###### Abstract

In this work we revisit an interactive variant of joint differential privacy, recently introduced by Naor et al. , and generalize it towards handling online processes in which existing privacy definitions seem too restrictive. We study basic properties of this definition and demonstrate that it satisfies (suitable variants) of group privacy, composition, and post processing.

In order to demonstrate the advantages of this privacy definition compared to traditional forms of differential privacy, we consider the basic setting of online classification. We show that any (possibly non-private) learning rule can be _effectively_ transformed to a private learning rule with only a polynomial overhead in the mistake bound. This demonstrates a stark difference with traditional forms of differential privacy, such as the one studied by Golowich and Livni , where only a double exponential overhead in the mistake bound is known (via an information theoretic upper bound).

## 1 Introduction

In this work we study privacy of interactive machine learning processes. As a motivating story, consider a chatbot that continuously improves itself by learning from the conversations it conducts with users. As these conversations might contain sensitive information, we would like to provide privacy guarantees to the users, in the sense that the content of their conversations with the chatbot would not leak. This setting fleshes out the following two requirements.

1. Clearly, the answers given by the chatbot to user \(u_{i}\) must depend on the queries made by user \(u_{i}\). For example, the chatbot should provide different answers when asked by user \(u_{i}\) for the weather forecast in Antarctica, and when asked by \(u_{i}\) for a pasta recipe. This is in contrast to the plain formulation of differential privacy, where it is required that _all_ of the mechanism outputs would be (almost) independent of any single user input. Therefore, the privacy requirement we are aiming for is that the conversation of user \(u_{i}\) will remain "hidden" from _other_ users, and would not leak through the _other_ users' interactions with the chatbot. Moreover, this should remain true even if a "privacy attacker" (aiming to extract information about the conversation user \(u_{i}\) had) conducts _many_ different conversations with the chatbot.
2. The interaction with the chatbot is, by design, _interactive_ and _adaptive_, as it aims to conduct dialogues with the users. This allows the privacy attacker (mentioned above) to choose its queries to the chatbot _adaptively_. Privacy, hence, needs to be preserved even in the presence of adaptive attackers.

While each of these two requirements was studied in isolation, to the best of our knowledge, they have not been (explicitly) unified into a combined privacy framework. Requirement (1) was formalizedby Kearns et al. (2015) as _joint differential privacy (JDP)_. It provides privacy against _non-adaptive_ attackers. Intuitively, in the chatbot example, JDP aims to hide the conversation of user \(u_{i}\) from any privacy attacker that _chooses in advance_ all the queries it poses to the chatbot. This is unsatisfactory since the adaptive nature of this process invites adaptive attackers.

Requirement (2) was studied in many different settings, but to the best of our knowledge, only w.r.t. the plain formulation of DP, where the (adaptive) privacy attacker sees _all_ of the outputs of the mechanism. Works in this vein include (Dwork et al., 2009; Chan et al., 2010; Hardt and Rothblum, 2010; Dwork et al., 2010; Bun et al., 2017; Kaplan et al., 2021; Jain et al., 2021). In the chatbot example, plain DP would require, in particular, that even the messages sent from the chatbot to user \(u_{i}\) reveals (almost) no information about \(u_{i}\). In theory, this could be obtained by making sure that the _entire chatbot model_ is computed in a privacy preserving manner, such that even its full description leaks almost no information about any single user. Then, when user \(u_{i}\) comes, we can "simply" share the model with her, and let her query it locally on her device. But this is likely unrealistic with large models involving hundreds of billions of parameters.

In this work we use _challenge differential privacy_, which was recently introduced by Naor et al. (2023) in the context of PAC learning.6 As discussed below, challenge differential privacy is particularly suitable for addressing interactive and adaptive learning processes, such as the one illustrated above. Challenge DP can be viewed as an interactive variant of JDP, aimed at maintaining privacy against adaptive privacy attackers. Intuitively, in the chatbot example, this definition would guarantee that even an adaptive attacker that controls _all_ of the users except for user \(u_{i}\), learns (almost) no information about the conversation user \(u_{i}\) had with the chatbot.

### Private Online Classification

We initiate the study of challenge differential privacy in the basic setting of online classification. Let \(\) be the domain, \(\) be the label space, and \(=\) be set of labeled examples. An online learner is a (possibly randomized) mapping \(:^{}\). That is, it is a mapping that maps a finite sequence \(S^{}\) (the past examples), and an unlabeled example \(x\) (the current query point) to a label \(y\), which is denoted by \(y=(x;S)\).

Let \(^{}\) be a hypothesis class. A sequence \(S^{}\) is said to be realizable by \(\) if there exists \(h\) such that \(h(x_{i})=y_{i}\) for every \((x_{i},y_{i}) S\). For a sequence \(S=\{(x_{t},y_{t})\}_{t=1}^{T}^{}\) we write \((;S)\) for the random variable denoting the number of mistakes \(\) makes during the execution on \(S\). That is \(;S=_{t=1}^{T}1\{_{t} y_{t}\},\) where \(_{t}=(x_{t};S_{<t})\) is the (randomized) prediction of \(\) on \(x_{t}\).

**Definition 1.1** (Online Learnability: Realizable Case).: _We say that a hypothesis class \(\) is online learnable if there exists a learning rule \(\) such that \([;S]=o(T)\) for every sequence \(S\) which is realizable by \(\)._

**Remark 1.2**.: _Notice that Definition 1.1 corresponds to an oblivious adversary, as it quantifies over the input sequence in advance. This should not be confused with the adversaries considered in the context of privacy which are always adaptive in this work. In the non-private setting, focusing on oblivious adversaries does not affect generality in terms of utility. This is less clear when privacy constraints are involved.7 We emphasize that our results (our mistake bounds) continue to hold even when the realizable sequence is chosen by an adaptive (stateful) adversary, that at every point in time chooses the next input to the algorithm based on all of the previous outputs of the algorithm._

A classical result due to Littlestone (1988) characterizes online learnability (without privacy constraints) in terms of the Littlestone dimension. The latter is a combinatorial parameter of \(\) which was named after Littlestone by Ben-David et al. (2009).

In particular, Littlestone's characterization implies the following dichotomy: if \(\) has finite Littlestone dimension \(d\) then there exists a (deterministic) learning rule which makes at most \(d\) mistakes on every realizable input sequence. In the complementing case, when the Littlestone dimension of \(\) is infinite, for every learning rule \(\) and every \(T\) there exists a realizable sequence \(S\) of length \(T\) such that \([;S] T/2\). In other words, as a function of \(T\), the optimal mistake bound is either uniformly bounded by the Littlestone dimension, or it is \( T/2\). Because of this dichotomy, in some places online learnability is defined with respect to a uniform bound on the number of mistakes (and not just a sublinear one as in the above definition). In this work we follow the more general definition.

We investigate the following questions: _Can every online learnable class be learned by an algorithm which satisfies challenge differential privacy? What is the optimal mistake bound attainable by private learners?_

Our main result in this part provides an affirmative answer to the first question. We show that for any class \(\) with Littlestone dimension \(d\) there exists an \((,)\)-challenge-DP learning rule which makes at most \((}{^{2}}^{2}( )^{2}())\) mistakes, with probability \(1-\), on every realizable sequence of length \(T\). _Remarkably, our proof provides an efficient transformation taking a non-private learner to a private one:_ that is, given a black box access to a learning rule \(\) which makes at most \(M\) mistakes in the realizable case, we efficiently construct an \((,)\)-challenge-DP learning rule \(^{}\) which makes at most \((}{^{2}}^{2}( )^{2}())\) mistakes.

#### 1.1.1 Construction overview

We now give a simplified overview of our construction, called PQP, which transforms a non-private online learning algorithm into a private one (while maintaining computational efficiency). Let \(\) be a non-private algorithm, guaranteed to make at most \(d\) mistakes in the realizable setting. We maintain \(k\) copies of \(\). Informally, in every round \(i[T]\) we do the following:

1. Obtain an input point \(x_{i}\).
2. Give \(x_{i}\) to each of the \(k\) copies of \(\) to obtain predicted labels \(_{i,1},,_{i,k}\).
3. Output a "privacy preserving" aggregation \(_{i}\) of \(\{_{i,1},,_{i,k}\}\), which is some variant of noisy majority. This step will only satisfy challenge-DP, rather than (standard) DP.
4. Obtain the "true" label \(y_{i}\).
5. Let \([k]\) be chosen at random.
6. Rewind all copies of algorithm \(\) except for the \(\)th copy, so that they "forget" ever seeing \(x_{i}\).
7. Give the true label \(y_{i}\) to the \(\)th copy of \(\).

As we aggregate the predictions given by the copies of \(\) using (noisy) majority, we know that if the algorithm errs then at least a constant fraction of the copies of \(\) err. As we feed the true label \(y_{i}\) to a random copy, with constant probability, the copy which we do not rewind incurs a mistake at this moment. That is, whenever we make a mistake then with constant probability one of the copies we maintain incurs a mistake. This can happen at most \( k d\) times, since we have \(k\) copies and each of them makes at most \(d\) mistakes. This allows us to bound the number of mistakes made by our algorithm (w.h.p.). The privacy analysis is more involved. Intuitively, by rewinding all of the copies of \(\) (except one) in every round, we make sure that a single user can affect the inner state of at most one of the copies. This allows us to efficiently aggregate the predictions given by the copies in a privacy preserving manner. The subtle point is that the prediction we release in time \(i\)_does_ require querying _all_ the experts on the current example \(x_{i}\) (before rewinding them). Nevertheless, we show that this algorithm is private.

#### 1.1.2 Comparison with Golowich and Livni (2021)

The closest prior work to this manuscript is by Golowich and Livni who also studied the problem of private online classification, but under a more restrictive notion of privacy than challenge-DP. In particular their definition requires that the sequence of _predictors_ which the learner uses to predict in each round does not compromise privacy. In other words, it is as if at each round the learner publishes the entire truth-table of its predictor, rather than just its current prediction. This might be too prohibitive in certain applications such as the chatbot example illustrated above. Golowich and Livni show that even with respect to their more restrictive notion of privacy it is possible to onlinelearn every Littlestone class. However, their mistake bound is doubly exponential in the Littlestone dimension (whereas ours is quadratic), and their construction requires more elaborate access to the non-private learner. In particular, it is not clear whether their construction can be implemented efficiently (while our construction is efficient).

### Additional Related Work

Several works studied the related problem of _private learning from expert advice_[Dwork et al., 2010a, Jain et al., 2012, Thakurta and Smith, 2013, Dwork and Roth, 2014, Jain and Thakurta, 2014, Agarwal and Singh, 2017, Asi et al., 2022]. These works study a variant of the experts problem in which the learning algorithm has access to \(k\)_experts_; on every time step the learning algorithm chooses one of the experts to follow, and then observes the _loss_ of each expert. The goal of the learning algorithm is that its accumulated loss will be competitive with the loss of the _best expert in hindsight_. In this setting the private data is the sequence of losses observed throughout the execution, and the privacy requirement is that the sequence of experts chosen by the algorithm should not compromise the privacy of the sequence of losses.8 When applying these results to our context, the set of experts is the set of hypotheses in the class \(\), which means that the outcome of the learner (on every time step) is a complete model (i.e., a hypothesis). That is, in our context, applying prior works on private prediction from expert advice would result in a privacy definition similar to that of Golowich and Livni (2021) that accounts (in the privacy analysis) for releasing complete models, rather than just the predictions, which is significantly more restrictive.

There were a few works that studied private learning in online settings under the constraint of JDP. For example, Shariff and Sheffet (2018) studied the stochastic contextual linear bandits problem under JDP. Here, in every round \(t\) the learner receives a _context_\(c_{t}\), then it selects an _action_\(a_{t}\) (from a fixed set of actions), and finally it receives a reward \(y_{t}\) which depends on \((c_{t},a_{t})\) in a linear way. The learner's objective is to maximize cumulative reward. The (non-adaptive) definition of JDP means that action \(a_{t}\) is revealed only to user \(u_{t}\). Furthermore, it guarantees that the inputs of user \(u_{t}\) (specifically the context \(c_{t}\) and the reward \(y_{t}\)) do not leak to the other users via the actions they are given, provided that all these other users _fix their data in advance_. This non-adaptive privacy notion fits the stochastic setting of Shariff and Sheffet (2018), but (we believe) is less suited for adversarial processes like the ones we consider in this work. We also note that the algorithm of Shariff and Sheffet (2018) in fact satisfies the more restrictive privacy definition which applies to the sequence of predictors (rather than the sequence of predictions), similarly to the that of Golowich and Livni (2021).

## 2 Preliminaries

Notation.Two datasets \(S\) and \(S^{}\) are called _neighboring_ if one is obtained from the other by adding or deleting one element, e.g., \(S^{}=S\{x^{}\}\). For two random variables \(Y,Z\) we write \(X_{(,)}Y\) to mean that for every event \(F\) it holds that \([X F] e^{}[Y F]+\), and \([Y F] e^{}[X F]+\). Throughout the paper we assume that the privacy parameter \(\) satisfies \(=O(1)\), but our analyses trivially extend to larger values of epsilon.

The standard definition of differential privacy is,

**Definition 2.1** ([Dwork et al., 2006]).: _Let \(\) be a randomized algorithm that operates on datasets. Algorithm \(\) is \((,)\)-differentially private (DP) if for any two neighboring datasets \(S,S^{}\) we have \((S)_{(,)}(S^{})\)._

The Laplace mechanism.The most basic constructions of differentially private algorithms are via the Laplace mechanism as follows.

**Definition 2.2**.: _A random variable has probability distribution \(()\) if its probability density function is \(f(x)=(-|x|/)\), where \(x\)._

**Definition 2.3** (Sensitivity).: _A function \(f\) that maps datasets to the reals has sensitivity\(\) if for every two neighboring datasets \(S\) and \(S^{}\) it holds that \(|f(S)-f(S^{})|\)._

**Theorem 2.4** (The Laplace Mechanism (Dwork et al., 2006)).: _Let \(f\) be a function that maps datasets to the reals with sensitivity \(\). The mechanism \(\) that on input \(S\) adds noise with distribution \(()\) to the output of \(f(S)\) preserves \((,0)\)-differential privacy._

Joint differential privacy.The standard definition of differential privacy (Definition 2.1) captures a setting in which the entire output of the computation may be publicly released without compromising privacy. While this is a very desirable requirement, it is sometimes too restrictive. Indeed, Kearns et al. (2015) considered a relaxed setting in which we aim to analyze a dataset \(S=(x_{1},,x_{n})\), where every \(x_{i}\) represents the information of user \(i\), and to obtain a vector of outcomes \((y_{1},,y_{n})\). This vector, however, is not made public. Instead, every user \(i\) only receives its "corresponding outcome" \(y_{i}\). This setting potentially allows the outcome \(y_{i}\) to strongly depend on the the input \(x_{i}\), without compromising the privacy of the \(i\)th user from the view point of the other users.

**Definition 2.5** ((Kearns et al., 2015)).: _Let \(:X^{n} Y^{n}\) be a randomized algorithm that takes a dataset \(S X^{n}\) and outputs a vector \( Y^{n}\). Algorithm \(\) satisfies \((,)\)-joint differential privacy (JDP) if for every \(i[n]\) and every two datasets \(S,S^{} X^{n}\) differing only on their \(i\)th point it holds that \((S)_{-i}_{(,)}(S^{})_{-i}\). Here \((S)_{-i}\) denotes the (random) vector of length \(n-1\) obtained by running \((y_{1},,y_{n})(S)\) and returning \((y_{1},,y_{i-1},y_{i+1},,y_{n})\)._

In words, consider an algorithm \(\) that operates on the data of \(n\) individuals and outputs \(n\) outcomes \(y_{1},,y_{n}\). This algorithm is JDP if changing only the \(i\)th input point \(x_{i}\) has almost no affect on the outcome distribution of the _other_ outputs (but the outcome distribution of \(y_{i}\) is allowed to strongly depend on \(x_{i}\)). Kearns et al. (2015) showed that this fits a wide range of problems in economic environments.

**Example 2.6** ((Nahmias et al., 2019)).: _Suppose that a city water corporation is interested in promoting water conservation. To do so, the corporation decided to send each household a customized report indicating whether their water consumption is above or below the median consumption in the neighborhood. Of course, this must be done in a way that protects the privacy of the neighbors. One way to tackle this would be to compute a privacy preserving estimation \(z\) for the median consumption (satisfying Definition 2.1). Then, in each report, we could safely indicate whether the household's water consumption is bigger or smaller than \(z\). While this solution is natural and intuitive, it turns out to be sub-optimal: We can obtain better utility by designing a JDP algorithm that directly computes a different outcome for each user ("above" or "below"), which is what we really aimed for, without going through a private median computation._

Algorithm AboveThreshold.Consider a large number of low sensitivity functions \(f_{1},f_{2},,f_{T}\) which are given (one by one) to a data curator (holding a dataset \(S\)). Algorithm AboveThreshold allows for privately identifying the queries \(f_{i}\) whose value \(f_{i}(S)\) is (roughly) greater than some threshold \(t\).

**Input:** Dataset \(S X^{*}\), privacy parameters \(,\), threshold \(t\), number of positive reports \(r\), and an adaptively chosen stream of queries \(f_{i}:X^{*}\) with sensitivity \(\)

1. Denote \(=O(())\)
2. In each round \(i\), when receiving a query \(f_{i} Q\), do the following: 1. Let \(_{i} f_{i}(S)+()\) 2. If \(_{i} t\), then let \(_{i}=1\) and otherwise let \(_{i}=0\) 3. Output \(_{i}\) 4. If \(_{j=1}^{i}_{i} r\) then HALT

**Algorithm 1**AboveThreshold (Dwork et al., 2009, Hardt and Rothblum, 2010)

Even though the number of possible rounds is unbounded, algorithm AboveThreshold preserves differential privacy. Note, however, that AboveThreshold is an _interactive_ mechanism, while the standard definition of differential privacy (Definition 2.1) is stated for _non-interactive_ mechanisms, that process their input dataset, release an output, and halt. The adaptation of DP to such interactive settings is done via a _game_ between the (interactive) mechanism and an _adversary_ that specifies the inputs to the mechanism and observes its outputs. Intuitively, the privacy requirement is that the viewof the adversary at the end of the execution should be differentially private w.r.t. the inputs given to the mechanism. Formally,

**Definition 2.7** (DP under adaptive queries [Dwork et al., 2006, Bun et al., 2017]).: _Let \(\) be a mechanism that takes an input dataset and answers a sequence of adaptively chosen queries (specified by an adversary \(\) and chosen from some family \(Q\) of possible queries). Mechanism \(\) is \((,)\)-differentially private if for every adversary \(\) we have that AdaptiveQuery\({}_{,,Q}\) (defined below) is \((,)\)-differentially private (w.r.t. its input bit \(b\))._

```
0: A bit \(b\{0,1\}\). (The bit \(b\) is unknown to \(\) and \(\).)
1. The adversary \(\) chooses two neighboring datasets \(S_{0}\) and \(S_{1}\).
2. The dataset \(S_{b}\) is given to the mechanism \(\).
3. For \(i=1,2,\) 1. The adversary \(\) chooses a query \(q_{i} Q\).
4. The mechanism \(\) is given \(q_{i}\) and returns \(a_{i}\).
5. \(a_{i}\) is given to \(\).
6. When \(\) or \(\) halts, output \(\)'s view of the interaction. ```

**Theorem 2.8** ([Dwork et al., 2009, Hardt and Rothblum, 2010, Kaplan et al., 2021]).: _Algorithm AboveThreshold is \((,)\)-differentially private._

A private counter.In the setting of algorithm AboveThreshold, the dataset is fixed in the beginning of the execution, and the queries arrive sequentially one by one. Dwork et al. (2010a) and Chan et al. (2010) considered a different setting, in which the _data_ arrives sequentially. In particular, they considered the _counter_ problem where in every time step \(i[T]\) we obtain an input bit \(x_{i}\{0,1\}\) (representing the data of user \(i\)) and must immediately respond with an approximation for the current sum of the bits. That is, at time \(i\) we wish to release an approximation for \(x_{1}+x_{2}++x_{i}\).

Similarly to our previous discussion, this is an _interactive_ setting, and privacy is defined via a _game_ between a mechanism \(\) and an adversary \(\) that adaptively determines the inputs for the mechanism.

**Definition 2.9** (DP under adaptive inputs [Dwork et al., 2006, 2010a, Chan et al., 2010, Kaplan et al., 2021, Jain et al., 2021]).: _Let \(\) be a mechanism that in every round \(i\) obtains an input point \(x_{i}\) (representing the information of user \(i\)) and outputs a response \(a_{i}\). Mechanism \(\) is \((,)\)-differentially private if for every adversary \(\) we have that AdaptiveInput\({}_{,}\) (defined below) is \((,)\)-differentially private (w.r.t. its input bit \(b\))._

```
0: A bit \(b\{0,1\}\). (The bit \(b\) is unknown to \(\) and \(\).)
1. For \(i=1,2,\) 1. The adversary \(\) outputs a bit \(c_{i}\{0,1\}\), under the restriction that \(_{j=1}^{i}c_{j} 1\). % The round \(i\) in which \(c_{i}=1\) is called the _challenge_ round. Note that there could be at most one challenge round throughout the game. 2. The adversary \(\) chooses two input points \(x_{i,0}\) and \(x_{i,1}\), under the restriction that if \(c_{i}=0\) then \(x_{i,0}=x_{i,1}\). 3. Algorithm \(\) obtains \(x_{i,b}\) and outputs \(a_{i}\). 4. \(a_{i}\) is given to \(\).
2. When \(\) or \(\) halts, output \(\)'s view of the interaction. ```

**Algorithm 3**AdaptiveInput\({}_{,}\)[Jain et al., 2021]

**Theorem 2.10** (Private counter [Dwork et al., 2010a, Chan et al., 2010, Jain et al., 2021]).: _There exists an \((,0)\)-differentially private mechanism \(\) (as in Definition 2.9) that in each round \(i[T]\) obtains an input bit \(x_{i}\{0,1\}\) and outputs a response \(a_{i}\) with the following properties. Let \(s\) denote the random coins of \(\). Then there exists an event \(E\) such that: (1) \([s E] 1-\), and (2) Conditioned on every \(s E\), for every input sequence \((x_{1},,x_{T})\), the answers \((a_{1},,a_{T})\) satisfy \(|a_{i}-_{j=1}^{i}x_{i}| O((T) ()).\)_Challenge Differential Privacy

We now introduce the privacy definition we consider in this work is. Intuitively, the requirement is that even an adaptive adversary controlling all of the users except Alice, cannot learn much information about the interaction Alice had with the algorithm.

**Definition 3.1** (Extension of Naor et al. (2023)).: _Consider an algorithm \(\) that, in each round \(i[T]\) obtains an input point \(x_{i}\), outputs a "predicted" label \(_{i}\), and obtains a "true" label \(y_{i}\). We say that algorithm \(\) is \((,)\)-challenge differentially private if for any adversary \(\) we have that \(_{,,T}\), defined below, is \((,)\)-differentially private (w.r.t. its input bit \(b\))._

**Setting:**\(T\) denotes the number of rounds and \(g\) is a "group privacy" parameter. If not explicitly stated we assume that \(g=1\). \(\) is an online algorithm and \(\) is an adversary that determines the inputs adaptively.

**Input of the game:** A bit \(b\{0,1\}\). (The bit \(b\) is unknown to \(\) and \(\).)

1. For \(i=1,2,,T\) 1. The adversary \(\) outputs a bit \(c_{i}\{0,1\}\), under the restriction that \(_{j=1}^{i}c_{j} g\). % We interpret rounds \(i\) in which \(c_{i}=1\) as _challenge_ rounds. Note that there could be at most \(g\) challenge rounds throughout the game. 2. The adversary \(\) chooses two labeled inputs \((x_{i,0},y_{i,0})\) and \((x_{i,1},y_{i,1})\), under the restriction that if \(c_{i}=0\) then \((x_{i,0},y_{i,0})=(x_{i,1},y_{i,1})\). 3. Algorithm \(\) obtains \(x_{i,b}\), then outputs \(_{i}\), and then obtains \(y_{i,b}\). 4. If \(c_{i}=0\) then set \(_{i}=_{i}\). Otherwise set \(_{i}=\). 5. The adversary \(\) obtains \(_{i}\). % Note that the adversary \(\) does not get to see the outputs of \(\) in challenge rounds.
2. Output \(\)'s view of the game, that is \(_{1},,_{T}\) and the internal randomness of \(\). % Note that from this we can reconstruct all the inputs specified by \(\) throughout the game.

**Remark 3.2**.: _Naor et al. (2023) studied a special case of this definition, suitable to their stochastic setting. More specifically, they considered a setting where the algorithm initially gets a dataset containing labeled examples. Then, on every time step, a new user arrives and submits its unlabeled example to the algorithm, and the algorithm responds with a predicted label. We extend the definition to capture settings in which every user interacts with the algorithm (rather than just submitting its input). In the concrete application we consider (online learning) this corresponds to the user submitting its input, then obtaining the predicted label, and then submitting the "true" label. Our generalized definition (Section A) captures this as a special case and allows for arbitrary interactions with each user._

Composition and post-processing.Composition and post-processing for challenge-DP follows immediately from their analogues for (standard) DP. Formally, composition is defined via the following game, called CompositionGame, in which a "meta adversary" \(^{*}\) is trying to guess an unknown bit \(b\{0,1\}\). The meta adversary \(^{*}\) is allowed to (adaptively) invoke \(k\) executions of the game specified in Algorithm 4, where all of these \(k\) executions are done with the same (unknown) bit \(b\). See Algorithm 5.

``` Input of the game: A bit \(b\{0,1\}\). (The bit \(b\) is unknown to \(^{*}\).)
1. For \(=1,2,,m\) 1. The adversary \(^{*}\) outputs an \((,)\)-challenge-DP algorithm \(_{}\), an adversary \(_{}\), and an integer \(T_{}\). 2. The adversary \(^{*}\) obtains the outcome of \(_{_{},_{},T_{}}(b)\).
2. Output \(^{*}\)'s view of the game (its internal randomness and all of the outcomes of OnlineGame it obtained throughout the execution). ```

**Algorithm 5**\(_{^{*},m,,}\)

The following theorem follows immediately from standard composition theorems for differential privacy (Dwork et al., 2010b).

**Theorem 3.3** (special case of [Dwork et al., 2010b]).: _For every \(^{*}\), every \(m\) and every \(,,^{} 0\) it holds that \(_{,m,,}\) is \((^{},m+^{})\)-differentially private (w.r.t. the input bit \(b\)) for \(^{}=)}+m(e^ {}-1)\)._

Group privacy.We show that challenge-DP is closed under group privacy. This is more subtle than the composition argument. In fact, we first need to _define_ what do we mean by "group privacy" in the context of challenge-DP, which we do using the parameter \(g\) in algorithm OnlineGame. Recall that throughout the execution of OnlineGame, the adversary is allowed \(g\) challenge rounds. We show that if an algorithm satisfies challenge-DP when the adversary is allowed only a single challenge round, then it also satisfies challenge-DP (with related privacy parameters) when the adversary is allowed \(g>1\) challenge rounds. This is captured by the following theorem; see Appendix B for the proof.

**Theorem 3.4**.: _Let \(\) be an algorithm that in each round \(i[T]\) obtains an input point \(x_{i}\), outputs a "predicted" label \(_{i}\), and obtains a "true" label \(y_{i}\). If \(\) is \((,)\)-challenge-DP then for every \(g\) and every adversary \(\) (posing at most \(g\) challenges) we have that \(_{,,T,g}\) is \((g,g e^{ g})\)-differentially private._

## 4 Algorithm ChallengeAT

Towards presenting our private online learner, we introduce a variant of algorithm AboveThreshold with additional guarantees, which we call ChallengeAT. Recall that AboveThreshold "hides" arbitrary modifications to a single input point. Intuitively, the new variant we present aims to hide both an arbitrary modification to a single input point and an arbitrary modification to a single query throughout the execution. Consider algorithm ChallengeAT.

**Input:** Dataset \(S X^{*}\), privacy parameters \(,\), threshold \(t\), number of positive reports \(r\), and an adaptively chosen stream of queries \(f_{i}:X^{*}\) each with sensitivity \(\)

**Tool used:** An \((,0)\)-DP algorithm, PrivateCounter, for counting bits under continual observation, guaranteeing error at most \(\) with probability at least \(1-\)

1. Instantiate PrivateCounter
2. Denote \(=O(( {}))\)
3. In each round \(i\), when receiving a query \(f_{i}\), do the following: 1. Let \(_{i} f_{i}(S)+()\)
4. If \(_{i} t\), then let \(_{i}=1\) and otherwise let \(_{i}=0\)
5. Output \(_{i}\)
6. Feed \(_{i}\) to PrivateCounter and let \(_{i}\) denote its current output
7. If \(_{i} r\)then HALT

**Remark 4.1**.: _When we apply ChallengeAT, it sets \(=O((T)())\). Technically, for this it has to know \(T\) and \(\). To simplify the description this is not explicit in our algorithms._

The utility guarantees of ChallengeAT are straightforward. The following theorem follows by bounding (w.h.p.) all the noises sampled throughout the execution (when instantiating ChallengeAT with the private counter from Theorem 2.10).

**Theorem 4.2**.: _Let \(s\) denote the random coins of ChallengeAT. Then there exists an event \(E\) such that: (1) \([s E] 1-\), and (2) Conditioned on every \(s E\), for every input dataset \(S\) and every sequence of \(T\) queries \((f_{1},,f_{T})\) it holds that_

1. _Algorithm_ ChallengeAT _does not halt before the \(r\)th time in which it outputs \(_{i}=1\),_
2. _For every_ \(i\) _such that_ \(_{i}=1\) _it holds that_ \(f_{i}(S) t-O(()())\)_._
3. _For every_ \(i\) _such that_ \(_{i}=0\) _it holds that_ \(f_{i}(S) t+O(()())\)_,_

_where \(=O((T)())\) is the error of the counter of Theorem 2.10._The event \(E\) in Theorem 4.2 occurs when all the Laplace noises of the counter and ChallengeAT are within a factor of \((T/)\) of their expectation. The privacy guarantees of ChallengeAT are more involved. They are defined via a game with an adversary \(\) whose goal is to guess a secret bit \(b\). At the beginning of the game, the adversary chooses two neighboring datasets \(S_{0},S_{1}\), and ChallengeAT is instantiated with \(S_{b}\). Then throughout the game the adversary specifies queries \(f_{i}\) and observes the output of ChallengeAT on these queries. At some special round \(i^{*}\), chosen by the adversary, the adversary specifies _two_ queries \(f_{i^{*}}^{0},f_{i^{*}}^{1}\), where only \(f_{i^{*}}^{b}\) is fed into ChallengeAT. In round \(i^{*}\) the adversary does not get to see the answer of ChallengeAT on \(f_{i^{*}}^{b}\) (otherwise it could easily learn the bit \(b\) since \(f_{i^{*}}^{0},f_{i^{*}}^{}\) may be very different). We show that any such adversary \(\) has only a small advantage in guessing the bit \(b\). The formal details are given in Appendix C.

## 5 Online Classification under Challenge Differential Privacy

We are now ready to present our private online prediction algorithm. Consider algorithm POP.

**Theorem 5.1**.: _When executed with a learner \(\) that makes at most \(d\) mistakes and with parameters \(k=(}^{2}()^{2}( ))\) and \(r=O(dk+())\), then with probability at least \((1-)\) the number of mistakes made by algorithm POP is bounded by \((}{^{2}}^{2}()^{ 2}()).\)_

**Setting:**\(T\) denotes the number of rounds in the game. \(\) is a non-private online-algorithm.

**Parameters:**\(k\) determines the number of copies of \(\) we maintain. \(r\) determines the number of positive reports we aim to receive from ChallengeAT.

1. Instantiate \(k\) copies \(_{1},,_{k}\) of algorithm \(\)
2. Instantiate algorithm ChallengeAT on an empty dataset with threshold \(t=-k/4\), privacy parameters \(,\), number of positive reports \(r\), and sensitivity parameter \(=1\).
3. For \(i=1,2,,T\) 1. Obtain input \(x_{i}\) 2. Let \(_{1}^{},,_{k}^{}\) be duplicated copies of \(_{1},,_{k}\) 3. Let \(_{i}[k]\) be chosen uniformly at random 4. Let \(_{i,_{i}}_{_{i}}(x_{i})\). For \(j[k]\{_{i}\}\) let \(_{i,j}_{j}^{}(x_{i})\) 5. Feed ChallengeAT the query \(f_{i}--_{j[k]}_{i,j}\) and obtain an outcome \(_{i}\). (If ChallengeAT halts then POP also halts.) % Recall that \(_{i}=1\) indicates that \(--_{j[k]}_{i,j}- {4}\), meaning that there is "a lot" of disagreement among \(_{i,1},,_{i,k}\). 6. If \(_{i}=1\) then sample \(_{i}\{0,1\}\) at random. Else let \(_{i}=\{_{i,1},,_{i,k}\}\) 7. Output the bit \(_{i}\) as a prediction, and obtain a "true" label \(y_{i}\) 8. Feed \(y_{i}\) to \(_{_{i}}\) % Note that \(_{}\) is the only copy of \(\) that changes its state during this iteration

Informally, the privacy guarantees of POP follow from those of ChallengeAT. We give the formal details in Appendix D, obtaining the following theorem.

**Theorem 5.2**.: _Algorithm POP is \((O(),O())\)-Challenge-DP. That is, For every adversary \(\) it holds that \(_{,}\) is \((O(),O())\)-DP w.r.t. the bit \(b\) (the input of the game)._

We proceed with the utility guarantees of POP. See Appendix F for an extension to the agnostic setting.

**Theorem 5.3**.: _When executed with a learner \(\) that makes at most \(d\) mistakes and with parameters \(k=(}^{2}()^{2}( ))\) and \(r=O(dk+())\), then with probability at least \((1-)\) the number of mistakes made by algorithm POP is bounded by \((}{^{2}}^{2}()^{ 2}()).\)_Proof.: By Theorem 4.2, with probability \((1-)\) over the internal coins of ChallengeAT, for every input sequence, its answers are accurate up to error of

\[_{}=O( ()()),\]

where in our case, the sensitivity \(\) is 1, and the error of the counter \(\) is at most \(O((T)())\) by Theorem 2.10. We continue with the proof assuming that this event occurs. Furthermore, we set \(k=(_{})\), large enough, such that if less than \(\) the experts disagree with the other experts, then algorithm POP returns the majority vote with probability 1.

Consider the execution of algorithm POP and define \(1/5\)-Err be a random variable that counts the number of time steps in which at least \(1/5\)th of the experts make an error. That is

\[=|\{i[T]:_{j[k]}\{_{i,j}  y_{i}\}>k/5\}|.\]

We also define the random variable

\[=|\{i[T]:y_{i}_{i,_{i} }\}|.\]

That is expertAdvance counts the number of times steps in which the random expert we choose (the \(_{i}\)th expert) errs. Note that the \(_{i}\)th expert is the expert that gets the "true" label \(y_{i}\) as feedback. As we run \(k\) experts, and as each of them is guaranteed to make at most \(d\) mistakes, we get that

\[ kd.\]

We now show that with high probability 1/5-Err is not much larger than \(\). Let \(i\) be a time step in which at least \(1/5\) fraction of the experts err. As the choice of \(_{i}\) (the expert we update) is random, then with probability at least \(\) the chosen expert also errs. It is therefore unlikely that 1/5-Err is much larger than \(\), which is bounded by \(kd\). Specifically, by standard concentration arguments (see Appendix E for the precise version we use) it holds that

\[[>18dk+18+() ].\]

Note that when at least \(1/5\) of the experts disagree with other experts then at least \(1/5\) of the experts err. It follows that 1/5-Err upper bounds the number of times in which algorithm ChallengeAT returns an "above threshold" answer. Hence, by setting \(r>18dk+18+()\) we ensure that w.h.p. algorithm ChallengeAT does not halt prematurely (and hence POP does not either).

Furthermore our algorithm errs either when there is a large disagreement between the experts or when all experts err. It follows that 1/5-Err also upper bounds the number of times which our algorithm errs.

Overall, by setting \(r=O(dk+())\) we ensure that POP does not halt prematurely, and by setting \(k=O(()())\) we ensure that POP does not err too many times throughout the execution. Combining the requirement on \(r\) and on \(k\), it suffices to take

\[k=(}^{2}()^{2}( )+(T)()),\]

in which case algorithm POP makes at most \((}{^{2}}^{2}()^ {2}())\) with high probability. 

## 6 Conclusion

Our work presents a new privacy model for online classification, together with an _efficiency preserving_ transformation from _non-private_ online classification, that exhibits a doubly exponential improvement in the error compared to prior works on this topic. We leave open the possibility that such an improvement could also be achieved in the setting of Golowich and Livni (2021), i.e., under the more restrictive notion of privacy where the sequence of predictors does not compromise privacy.