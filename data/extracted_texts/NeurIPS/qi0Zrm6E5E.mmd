# Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization

Kamil Dreczkowski

Imperial College of London

krd115@ic.ac.uk

&Antoine Grosnit

Huawei Noah's Ark Lab

Technische Universitat Darmstadt

&Haitham Bou-Ammar

Huawei Noah's Ark Lab

University College London

These authors contributed equally to this work.Work done during an internship at Huawei Noah's Ark Lab in London RC.

###### Abstract

This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives . This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 47 novel MCBO algorithms and benchmark them against seven existing MCBO solvers and five standard black-box optimization algorithms on ten tasks, conducting over 4000 experiments. Our findings reveal a superior combination of MCBO primitives outperforming existing approaches and illustrate the significance of model fit and the use of a trust region. We make our MCBO library available under the MIT license at https://github.com/huawei-noah/HEBO/tree/master/MCBO.

## 1 Introduction

The goal of mixed-variable and combinatorial optimization is to seek optimizers of functions defined over search spaces whose sizes grow exponentially with their dimensions. Applications of this field are ubiquitous, spanning a wide range of domains, such as supply chain optimization , vehicle routing , machine scheduling in manufacturing systems , asset optimization , among many others. A general recipe for tackling such tasks involves using the knowledge of domain experts to formalize combinatorial problems within the scope of well-established suited mathematical frameworks, and to apply available heuristic-based solvers. For instance, making the problem compatible with linear, quadratic, nonlinear or integer pro

Figure 1: In the MCBO framework, we can build an existing or a new algorithm by choosing a surrogate model and an acquisition function optimizer using a single line of code.

gramming solvers, or reducing the combinatorial task at hand to a standard problem such as knapsack , traveling salesman , or network flow problem , allows the application of optimized and scalable off-the-shelf software developed by many companies and academic laboratories [19; 20; 21; 22].

Despite numerous successes on many combinatorial tasks, standard heuristics like those mentioned above fall short in vital domains that require the optimization of **black-box objectives**. In those instances, we have, at best, partial a priori knowledge about the characteristics of the optimization objective, making it difficult, if not impossible, to map it to one of the forenamed mathematical frameworks. Although well-established black-box optimization methods such as Simulated Annealing (SA) [23; 24], Genetic Algorithms (GAs) , and Evolutionary Algorithms (EAs) , as well as online learning methods like Multi-Armed-Bandits (MAPs) , can still be used to solve such problems, they often fall short at optimizing **expensive-to-evaluate objectives** due to their high sample complexities. Consequently, addressing problems with **expensive-to-evaluate black-box objectives** necessitates the development of **data-driven** and **sample-efficient** solution methodologies.

One promizing strategy to handle such objectives is to **1)** efficiently learn a (local) probabilistic model of the black-box function, and **2)** balance exploration and exploitation by leveraging the model's uncertainty. This concept lies at the heart of Mixed-Variable and Combinatorial Bayesian Optimization (MCBO), a machine learning (ML) subfield crucial for achieving efficient optimization with immense potential for solving practical mixed-variable and combinatorial optimization problems.

MCBO algorithms generally have three high-level primitives: a probabilistic surrogate model, an acquisition function, and an acquisition optimizer that can operate in a trust region (TR). As shown in Fig. 1, we can frame many published MCBO algorithms as a specific combination of primitives, illustrating the intrinsic modularity of MCBO. For example, BOiLS  uses a Gaussian Process (GP)  with the string subsequence kernel (SSK) [29; 30] as its surrogate, and optimizes its acquisition function (expected improvement (EI)) via TR-constrained Hill Climbing . BOSS  shares two of its primitives with BOiLS but only differs in using a GA to optimize its acquisition function.

Although many existing MCBO algorithms share some of the primitives they use, it remains unclear which solutions are state-of-the-art for each primitive and which combination of primitives constitutes a state-of-the-art MCBO method. This issue arises due to two factors.

Firstly, MCBO papers often introduce non-diverse and/or non-standard benchmarks to evaluate their proposed methods. The lack of a standardized set of diverse benchmarks makes it difficult to assess the relative performance of the different MCBO primitives and their combinations. As observed in other fields, establishing standardized test domains is crucial for accelerating progress in MCBO. For example, the ImageNet dataset , ShapeNet, and MuJoCo  facilitated rapid advancements in 2D computer vision, 3D computer vision, and robotics research. Therefore, it is essential to establish a procedural generation of test domains in MCBO, enabling a systematic evaluation of existing methods to inspire the development of novel approaches.

Secondly, papers introducing a solution for a single MCBO primitive often forget to benchmark against baselines that use the same methods for the remaining primitives [1; 2; 3; 4], failing to fully highlight the merits of their proposed solution in a controlled setting. This is most likely due to the need for time-consuming and tedious efforts to modify and combine MCBO primitives from existing open-source implementations, as no standardized API exists to allow the interactions among them.

To address the aforementioned problems, we propose a flexible and comprehensive Python framework for MCBO. Our library provides a high-level API for all the MCBO primitives, and implementations of the primitives of several key MCBO baselines. Our framework also features a BoBuilder class that enables effortless implementation of existing and novel algorithms by flexibly combining MCBO primitives using a single line of code. In Section 5, we showcase the versatility of our library by implementing seven existing MCBO baselines and 47 novel MCBO algorithms. We evaluate these against five standard black-box optimization baselines on ten tasks, revealing insights into key MCBO design choices that contribute to robust algorithms.

Our library also includes implementations of a wide range of synthetic and real-world mixed-variable and combinatorial benchmarks, covering a broad spectrum of domain dimensionalities and optimization difficulties. These benchmarks include well-known optimization problems, such as the Ackley function  and the Pest Control problem , and optimization problems that extend the current application venues of BO, including Antibody Design, RNA inverse folding, and Logic Synthesis optimization.

In the interest of space, throughout this paper we make heavy use of abbreviations. We therefore provide a list of these abbreviations, alongside their definitions, in Table 1 of Appendix A.

## 2 Related Work

**Bayesian Optimisation Frameworks** Existing popular libraries for Bayesian Optimization (BO), such as Spearmint , GPyOpt , Cornell-MOE , RoBO , Emukit , Dragonfly , ProBO , and GPFlowOpt , offer diverse capabilities and modeling techniques for various optimization tasks. These libraries excel in different areas, including hyperparameter sampling, input warping and parallel optimization [34; 35], multi-fidelity optimization [36; 37; 38], and probabilistic programming . However, in contrast to our work, these libraries primarily focus on continuous and discrete search spaces and lack direct support for combinatorial and mixed-variable domains.

Closest to our work is BoTorch , a model-agnostic Python library that is integrated with GPyTorch , providing efficient and scalable implementations of GPs in PyTorch . Although BoTorch includes the GP kernel for mixed-variable optimization proposed by Wan et al. , it primarily focuses on continuous and discrete search spaces and does not provide direct built-in support for combinatorial formulations. In contrast, our framework is designed to tackle the unique challenges of BO in combinatorial and mixed-variable spaces and extends the capabilities of existing libraries by incorporating surrogate models and acquisition optimization techniques tailored to these domains.

Furthermore, our framework offers an unprecedented level of ease and flexibility in implementing BO algorithms by **mixing-and-matching** implemented BO primitives. With just a **single line of code**, users can combine different surrogate models, acquisition functions, and optimization methods. In contrast, other libraries would require time-consuming manual implementation processes to make components match.

**Mixed-variable and Combinatorial Optimization Benchmarks** Existing popular benchmarks for mixed-variable and combinatorial problems include the 24 synthetic mixed-integer test functions  that are part of the COCO  benchmarking suite. Our framework extends some of these functions, including the sphere, rotated hyper ellipsoid, Rastrigin, and Rosenbrock functions, to accommodate arbitrary dimensional domains and combinations of variable types. Moreover, we introduce additional synthetic functions that uphold the same generalization principles.

Distinguishing our benchmarking suite from benchmarks such as HPOBench , HPO-B , and YAHPO Gym , which primarily focus on a single domain such as hyperparameter optimization, our framework takes a more comprehensive perspective. Our suite encompasses a broader spectrum of tasks, spanning hyperparameter tuning, antibody design, Electronic Design Automation, RNA inverse folding, along with the classical synthetic tasks. This multi-domain nature underpins our conviction that benchmarking on diverse tasks that cover a broad spectrum of problem types and complexities strengthens the robustness and reliability of findings. We hold firm to the notion that this multifaceted approach leads to more generalisable conclusions than scenarios where only a single family of tasks is used for benchmarking, which can still be useful for important black-box families.

## 3 Mixed-Variable and Combinatorial Bayesian Optimization

BO is a sequential model-based technique to efficiently optimize a black-box function \(f()\) over a search space \(\). Due to the black-box nature of \(f()\), we can only evaluate it at input locations \(\) to get (noisy) outputs \(y\), such that \([y|f()]=f()\). BO tackles global optimization problems by iteratively repeating two steps. At iteration \(i\), it first \(suggests\) a point \(_{i}\) to evaluate, and in a second step, the learner \(observes\)\(y_{i}\), by evaluating the black-box at \(_{i}\). To enable sample efficiency, the suggestion of \(_{i}\) typically involves learning a (local) probabilistic surrogate model of \(f()\) from the set of already observed points, and is followed by the optimization of an acquisition function \(()\) trading-off exploration vs exploitation. We highlight both steps in Alg. 1, and depict a generic BO loop operating for a total budget of \(T_{}\).
Optimizing expensive black-box functions requires modeling the objective to enable efficient search by prioritizing informed decision-making over blind function evaluations . When dealing with mixed-variable and combinatorial formulations, various surrogate models are available, including Bayesian Linear Regression , Random Forests , Tree-Structured Parzen Estimators , and Bayesian Neural Networks . Still, GPs  remain the most widely adopted models in the literature due to their tractability, sample efficiency, and capacity to maintain calibrated uncertainties [28; 34; 39; 54; 55], and therefore our library mostly focuses on their support.

A GP is a non-parametric model that represents the prior belief about a black-box function as a distribution over functions and updates this distribution as new observations become available, resulting in a posterior distribution. A GP is fully defined by its mean function \(m()\) and kernel function, \(k_{}(,)\), where \(\) are the kernel hyperparameters [28; 54]. The mean function captures the overall trend and bias of the modeled function, while the kernel function characterizes the correlation between function values at different input locations. Specifically, the kernel function, \(k_{}(,^{})\), expresses our assumptions regarding the smoothness and periodicity of the modeled function, as it corresponds to the covariance between pairs of function values \((f(),f(^{}))\).

When modeling black-box functions defined over combinatorial spaces, one commonly used kernel function is the Overlap kernel [3; 56; 57], defined using Kronecker delta function \((,)\) as

\[k_{}^{}(,^{})=_{p=1 }^{d}_{p}\;([p],^{}[p]),\] (1)

with \(=(,_{1},,_{d})_{+}^{d+1}\), where \(\) represents the kernel variance, \(d\) is the dimensionality of \(\), and \(_{p}\) denotes the Automatic Relevance Determining (ARD)  length scale for the \(p^{}\) variable. In combinatorial problems, \(\{c_{1}^{1},...,c_{N_{1}}^{1}\}...\{c_{1}^{M},...,c_{N_{ M}}^{M}\}\), where \(\{c_{1}^{i},...,c_{N_{i}}^{i}\}\) is the set defining all the possible elements that the \(i^{}\) combinatorial variable can assume, \(N_{i}\) is the cardinality of this set, and \(M\) is the total number of combinatorial variables. In this context, the Overlap kernel measures the extent to which variables in the two input vectors share the same categories.

A related kernel is the Transformed Overlap (TO) kernel  applying the exponential function to the output of the Overlap kernel. This transformation enhances the kernel's expressive power, enabling it to model more complex functions . Other kernels tailored for combinatorial inputs include the SSK [29; 30], the Diffusion kernel (Diff.) , and the Hamming embedding via dictionary kernel (HED) , which are all supported in our framework.

Given a dataset \(=\{_{i},y_{i}\}_{i=1}^{n}\) with Gaussian-corrupted observations \(y_{i}=f(_{i})+_{i}\), with \(_{i}(0,^{2})\), and given a GP prior, the posterior of the black-box function value at a test point \(_{}\) denoted as \(f(_{})|,\), is also a Gaussian distribution \((_{}(_{}),_{} ^{2}(_{}))\). For brevity, we defer to Appendix B the analytic expressions for \(_{}(_{})\) and \(_{}^{2}(_{})\) along with the method to learn the optimal kernel hyperparameters \(^{*}\) by minimizing the negative log-likelihood.

``` Input: Initial dataset \(_{n_{0}}=\{_{j},y_{j}\}_{j=1}^{n_{0}}\), budget \(T_{}\), black-box \(f\). for\(i=n_{0}+1,...,T_{}\)do  Learn a surrogate model \((|_{i-1})\) of \(f\).  Use \((|_{i-1})\) to define  acquisition function \((|_{i-1})\).  Optimize \((|_{i-1})\) to get \(_{i}\).  Get \(y_{i}\) by evaluating \(f\) at \(_{i}\).  Set \(_{i}_{i-1}\{_{i},y_{i}\}\). Output:\(^{}=_{_{i}_{T_{}}}y_{i}\). ```

**Algorithm 1**Generic BO Algorithm

### The Acquisition Function

The acquisition function plays a crucial role in BO as it approximates the utility of evaluating the black-box function at a specific input \(\). Since the black-box function is unknown, the acquisition function considers both the estimated value of the objective function and its associated uncertainty. This enables the acquisition function to effectively balance exploration of the search space, gathering more information about the underlying objective function and exploiting currently promising regions likely to contain the optimal solution. Ultimately, the acquisition function is a criterion for selecting the next point to evaluate in the optimization process.

The Expected Improvement (EI) [59; 60] acquisition function measures the utility of new query points by evaluating the expected gain compared to the function values observed so far, considering the uncertainty of the model's posterior. During each iteration \(i\) of the optimization process, let \(y^{}_{1:i-1}\) denote the best black-box function value in the dataset \(_{i-1}\). The EI function is defined as:

\[^{()}()=_{f()|_{i-1},^{}}[\{y^{}_{1:i-1}-f(),0\}],\]

where the expectation is taken with respect to the posterior of a trained model. Our framework supports EI as well as other popular acquisitions such as Probability of Improvement (PI) , lower confidence bound (LCB) , and Thompson sampling (TS) , all described in Appendix B.3.

### Acquisition Optimization Methods

The acquisition optimization method plays a vital role in BO as it determines the next evaluation point \(_{i}\) by optimizing the acquisition function \((|_{i-1})\). In general, maximizing the acquisition function to find \(_{i}\) is an optimization problem defined globally over the entire search space:

\[_{i}=*{arg\,max}_{}(| _{i-1}).\] (2)

When dealing with a continuous search space, optimizing the acquisition function is relatively straightforward and can be performed by using off-the-shelf first or second-order optimization methods with random restarts (refer to  for a comprehensive study on this topic). However, when dealing with combinatorial inputs, the absence of a defined gradient hinders the direct application of gradient-based optimization methods . To address this challenge, various zero-order methods have been proposed for maximizing the acquisition function in combinatorial spaces. These methods encompass a range of global optimization algorithms and heuristics, including Hill Climbing (HC) , exhaustive Local Search (LS) , SA , GA , and MAB , which we include in our library.

These acquisition optimizers are often applied as global optimization algorithms to solve Equation 2. However, just like in continuous spaces, when the dimensionality of the search space is high, the surrogate may struggle to accurately model the black box over the entire search space. Taking inspiration from related work in BO for continuous spaces , some recent combinatorial BO algorithms, including Casmopolitan , introduce the notion of a TR to constrain the acquisition optimization procedure in the context of MCBO. In Casmopolitan, the TR around the best input found so far in the current trust region, \(^{}\), is defined as

\[(^{})=\{d_{h}(^{}_{h},_{h}) L_{h}d_{L_{h}}(^{}_{n},_{n}) 1\},\] (3)

where \(^{}_{h}\) and \(_{h}\) are vectors containing all the combinatorial inputs in \(^{}\) and \(\) respectively, \(d_{h}(,)\) is the Hamming distance  and \(L_{h}\) is the size of the TR for combinatorial variables. Similarly, \(^{}_{n}\) and \(_{n}\) are vectors containing all the numeric and discrete variables in \(^{}\) and \(\), respectively, \(d_{L_{n}}(,)\) is the maximum of the component-wise distance for numeric and discrete variables normalized by dimensional length scales \(L_{n}^{(_{n})}_{+}\) for the numeric and discrete variables .

## 4 The MCBO Framework

To facilitate code reusability and enable benchmarking on a standardized set of tasks, we introduce the MCBO framework, whose source code is open-source under the MIT license. Our ready-to-use software provides **1)** An API for defining BO primitives, **2)** Multiple primitives from key existing MCBO baselines, **3)** A BO constructor to effortlessly combine primitives into new algorithms, **4)** A variety of mixed-type and combinatorial BO and non-BO baselines, **5)** A suite of standard and novel mixed-type and combinatorial benchmarks, and **6)** An API for defining novel optimization problems.

We structure the MCBO framework in a modular fashion to allow for the rapid prototyping of new solutions by combining existing BO primitives. We build MCBO on top of PyTorch  to make it compatible with the rich ecosystem of code developed for training various regression models. The entire library structure naturally revolves around seven Python classes; the SearchSpace, TaskBase, ModelBase, AcqBase, AcqOptimizerBase, OptimizerBase, and the BoBase class.

### Defining Optimization Problems

To frame the optimization of a black-box within our framework, we need to create a task class (inheriting from TaskBase) that implements two methods: get_search_space_params and evaluate. The latter implements the call to the black-box, while the former provides the list of variables that compose the optimization domain of the black-box. We build a search space (an instance of SearchSpace) by providing this list of variables with their names, types, and any additional information needed, such as the categories they can take for categorical variables. As an example, consider a combinatorial optimization problem whose search space contains five categorical variables (with categories "A", "B", "C", and "D") and for which the black-box function is already defined as black_box_script in a python script. In the MCBO framework, we can create the corresponding task as follows:

``` classBlackBox(TaskBase): defget_search_space_params(self)->List[Dict[str, Any]]: categories=['A','B','C','D'] params=[{'name':f'x[i]','type':'nominal','categories':categories} foriinrange(5)]] defevaluate(self,x:pd.DataFrameFrame)->np.ndarray: returnblack_box_script(x) black_box=BlackBox() search_space=black_box.get_search_space()#getaninstanceofSearchSpace ```

### Surrogate Models, Acquisition Functions, and Acquisition Optimizers

The MCBO framework includes three core BO primitives: surrogate models, acquisition functions, and acquisition optimizers. **1)** Our implementation features several pre-implemented surrogate models, such as a GP with the string subsequence kernel (SSK) [4; 7], overlap kernel (O) , transformed-overlap kernel (TO) , diffusion kernel (Diff.) , dictionary-based kernel (HED) , mixture kernel , and linear regression  with the Horseshoe prior  using maximum likelihood, maximum a posteriori, and Bayes estimation (LHS) . **2)** On the acquisition function side, we include the widely used expected improvement [59; 60], probability of improvement  and lower confidence bound  for GPs, and Thompson sampling  for LHS. **3)** Furthermore, our library includes various acquisition optimizers, such as exhaustive Local Search , Genetic Algorithm , Simulated Annealing , and interleaved search (IS) alternating between Hill Climbing or Multi-armed Bandit for combinatorial variables and gradient-descent steps for numeric variables as developed for CoCaBO  and Casmopolitan . Moreover, we generalize the implementation of these acquisition optimizers so that **all of them support TR-constrained acquisition optimization**, extending [5; 68], and make them **handle cheap-to-compute input domain constraints** via rejection sampling.

### Defining MCBO Algorithms

With our framework, defining MCBO algorithms is effortless. By specifying the IDs of the surrogate model, acquisition function, acquisition optimizer, and TR manager in the BoBuilder class constructor, the corresponding BO primitives are automatically retrieved. In an optimization loop, the optimizer created with BoBuilder handles model fit, acquisition optimization, and TR adjustments. Thanks to the BoBuilder class, we can easily **mix-and-match** BO primitives from existing algorithms, allowing the implementation of novel MCBO algorithms with just **one line of code per algorithm**. We demonstrate the versatility of this approach by implementing 47 novel MCBO algorithms and evaluating them on a set of ten tasks in Section 5. As an example, we showcase on the next page how to use BoBuilder to construct the Casmopolitan algorithm  and apply it to optimize a generic black-box function for a specified budget.

### Baselines

MCBO baselinesWe leverage the aforementioned BO primitives to implement seven existing MCBO algorithms: Casmopolitan , BOiLS , COMBO , CoCaBO , BOSS , BOCS , and BODi . Furthermore, we explore the remaining combinations of implemented surrogate models and acquisition optimizers, including trust-region-based optimizers, implementing 47 additional novel MCBO algorithms (See Fig. 1). In Section 5, we benchmark the performance of these 54 MCBO algorithms and a set of five standard non-BO baselines that we describe in the following paragraph.

Black-Box Optimization BaselinesTo facilitate benchmarking against non-BO optimization methods, we include the following baselines in our library: Random Search (RS) , Hill Climbing (HC) , GA , SA [23; 24], and MAB . We make them inherit from the OptimizerBase class, ensuring a consistent API with MCBO solvers with the use of suggest and observe methods.

### Available Benchmarks

We include diverse benchmarks, enabling a systematic evaluation of MCBO methods across various optimization domains, dimensionalities, and difficulties. The benchmarks encompass both synthetic and real-world tasks. While we briefly overview the available benchmarks here, we provide a more detailed description, including specific constraints, dimensionalities, supported domains, and implementation details in Appendix C.

The synthetic benchmark suite offers a controlled environment for evaluating the performance of MCBO methods, featuring the 21 Simon Fraser University (SFU) test functions  that generalize to \(d\)-dimensional domains, extended to handle continuous, discrete, and nominal variables, as well as combinations of these variable types. Additionally, we include the pest control task  that presents a challenging optimization landscape with high-order interactions.

For real-world tasks, the benchmark suite includes logic synthesis optimization , antibody design , RNA inverse folding , and hyperparameter tuning of ML models. The logic synthesis benchmarks optimize Boolean circuits represented by And-Inverter Graphs (AIG) and Majority-Inverter Graphs (MIG) . The antibody design task focuses on optimizing the CDRH3 region of antibodies for binding to a specific antigen. RNA inverse folding aims to find RNA sequences that fold into a target secondary structure. Hyperparameter optimization tasks involve tuning the parameters of XGBoost  on the MNIST dataset  and \(\)-Support Vector Regression (\(\)-SVR)  on the UCI slice dataset  for which we do feature selection along with the hyperparameter tuning as in .

## 5 Experiments

To illustrate the capacity of our library, we conduct experiments tackling the following questions:

1. Which implemented surrogate model and acquisition optimizer performs best?
2. Does incorporating a TR constraint improve the performance of MCBO?
3. Which implemented Combinatorial and Mixed-variable algorithm is the most robust?

We exploit the high modularity and flexibility of our framework to easily instantiate a total of 48 combinatorial BO algorithms using the BoBuilder class, and compare their performance on six combinatorial tasks, including Ackley-20D, pest control, sequence of operators tuning for AIG and MIG logic synthesis, antibody design, and RNA inverse fold task. We also run five non-BO baselines - RS, HC, GA, SA, and MAB - on these six combinatorial tasks. Similarly, we use BoBuilder to implement 24 mixed-variable BO methods and evaluate them on four mixed-variabletasks, including Ackley-53D, XGBoost hyperparameter tuning, \(\)-SVR tuning with feature selection, and logic synthesis flow optimization for AIGs. We benchmark the mixed-variable BO algorithms against four non-BO baselines, RS, HC, GA, and SA. Each experiment is repeated across fifteen random seeds, resulting in over \(4,000\) individual experiments.

### Experimental Procedure

To ensure a fair and consistent comparison, we follow a standardized experimental design. For a given task and random seed, each BO algorithm suggests the same set of 20 uniformly sampled points and observes the corresponding black-box function values. For the next 180 steps, each algorithm suggests a new point to evaluate based on its surrogate model and acquisition function optimization process. We therefore query the black-box function 200 times per algorithm and per seed. For a given black-box evaluation budget \(i=1,,200\), we denote by \(y_{i}^{*}\) the best value attained so far, \(y_{i}^{*}=_{1 j i}y_{j}\)3. As the scales of the black-box values can differ drastically from one task to another, we compare the optimizers' performance across tasks by considering their ranks based on the \(y_{i}^{*}\)s. To analyze the impact of some primitives, We can then aggregate the ranks across tasks, random seeds, and the BO primitives not under investigation. For example, when assessing surrogate models, we average the rank of the optimizers sharing the same surrogate across tasks, random seeds, acquisition functions, and acquisition optimizers. In figures displaying the evolution of ranks, we show the mean rank in solid line, and the standard error with respect to tasks and seeds as a shaded area. We conduct a Friedman test to see if we can reject the hypothesis that all the methods' performances are equal, and if it is not the case, we add black vertical lines to connect the algorithms whose mean rank differences are smaller than the length of the critical interval given by the post-hoc Wilcoxon signed-rank test.

### Hardware

We run our experiments on two machines with 4 Tesla V100-SXM2-16GB GPUs and an Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz with 88 threads, which allowed us to parallelize over tasks and seeds. It took us approximately two weeks to run the set of experiments described in this paper.

### Analyzing Combinatorial BO Design Choices

Surrogate modelWe observe in Fig. 2 (left) that BO methods with LSH or GP (Diff.) surrogate significantly underperform compared to those based on GP (HED), GP (O), GP (TO), and GP (SSK). However, from the well-performing surrogate models, no single model outperforms the others significantly, indicating that certain models are better suited for specific types of black-box functions as we will investigate in Section 5.4.

Acquisition function optimizersFig. 2 (center) shows that exhaustive LS performs best at very low budgets. But as the number of step increases, GA delivers superior mean performance. This suggests that the high exploitation offered by LS is beneficial when few suggestions are allowed, but the less myopic approach of GA achieves a better exploration-exploitation trade-off as budget increases.

Figure 2: Evolution of the average rank of combinatorial BO algorithms across six tasks, aggregated by surrogate model (left), acquisition optimizer (center), and use of a trust-region (right).

Trust region (TR)Finally, Fig. 2 (right) shows that, on average, methods working with a dynamic TR to fit local models and constrain acquisition optimization provide consistently better suggestions compared to global approaches. This confirms and extends the findings of Wan et al. .

### Which is the Most Robust Combinatorial Algorithm?

In this section, we consider each mix-and-match combinatorial BO algorithm individually (without aggregating ranks by primitives), and compare them to non-BO baselines. We first order all the algorithms based on their average ranks across ten seeds and 6 tasks, and we show on Fig. 3 the performance of the 6 known BO solvers, the 5 non-BO baselines, and the 2 (out of 42) new mix-and-match methods achieving the lowest rank.

We observe that only the two new MCBO algorithms achieve a statistically significant average rank improvement compared to the GA, and SA black-box optimization baselines. These two algorithms utilize a GP (SSK) or GP (TO) surrogate model with a GA and trust region constraint for acquisition optimization. Notably, these algorithms are novel and their design choices align with our conclusions from Section 5.3. However, it is important to highlight that the difference in rank between these algorithms and Casmopolitan , BOSS , and BOILS  is not statistically significant. This can be attributed to the significant role of model fit (see Section 5.4) in BO and the averaging of results across diverse tasks, where different surrogate models may be optimal.

For each BO optimizer and the three best performing non-BO baselines, we show on Fig. 4 the evolution of the best objective values attained. We note that on two tasks (Antibody design and RNA inverse fold), a non-BO algorithm (SA and GA respectively) outperforms all BO solvers, highlighting the need for further development of combinatorial BO techniques when it comes to solving real-world tasks. Nevertheless, BO solvers are still the best on a majority of tasks, though we observe that no single optimizer achieves the lowest objective value across all tasks. We investigate the variability of BO performance in the next paragraph.

Figure 4: Evolution of best observed value on each combinatorial task achieved by the known combinatorial optimizers and the two best mix-and-match BO solvers built with MCBO framework. In all tasks, the aim is to minimize the black-box. Hence, the lower the best observed value the better.

Figure 3: Average rank of known combinatorial algorithms and the two best mix-and-match BO.

Model fit and BO performanceAs underlined in Section 3.1, the choice of kernel impacts the GP modeling capacity, which could explain the variability of BO performance on the different black-box functions. To assess the relation between kernel choice and BO performance, we measure the Pearson correlation between the quality of a GP model fit, and the quality of the objective value attained after 200 iterations of BO equipped with the same surrogate. We get the quality of a GP fit on a given task and seed by conditioning the GP on the first 150 points \(\{(_{i},y_{i})\}_{i=1}^{150}\) coming from our GA run and computing the log-likelihood of the last 50 black-box values \(\{y_{i}\}_{i=150}^{200}\) under the GP prediction at \(\{_{i}\}_{i=150}^{200}\). Fixing the acquisition optimizer and the use of TR, we collect for all tasks and seeds the GP log-likelihood and the BO performance when using SSK, TO, O, HED and diffusion kernel. Given a task and a seed, the set of points coming from the GA run are the same to limit a source of noise in the performance comparison. As expected, Fig. 5 shows a positive correlation between BO performance and the capacity of its surrogate model to fit the black-box. The correlation is weaker for local acquisition optimizers (LS) than more explorative ones (GA).

### Mixed-variable optimization

Due to space limitations, we defer the analysis of results obtained on mixed tasks to Appendix D. This analysis reveals that a new mixed-variable BO algorithm, which combines a GP (Matern-5/2 and HED kernel) surrogate model with a GA acquisition optimizer and a trust region constraint, achieves a statistically significant better rank averaged across all the considered tasks.

## 6 Conclusion

This work introduces a modular and flexible framework for MCBO, addressing the need for systematic benchmarking and standardized evaluation in the field. Leveraging this framework, we implement a total of 48 combinatorial and 24 mixed-variable BO algorithms with just a single line of code per algorithm. Benchmarking these algorithms on a set of 10 tasks, which is a subset of the tasks available in our framework, we conduct over 4000 individual experiments. Through these experiments, we provide insights into the choice of surrogate model based on its fit, the selection of acquisition optimizer, and the use of a trust region constraint to constrain acquisition optimization.

It is important to note that these experiments represent only a fraction of the possibilities our framework offers. We aim to demonstrate the ease and potential of our MCBO framework, encouraging further analyzes and the development of new primitives. We also invite MCBO developers to integrate their novel options into our flexible codebase, enabling easy mixing and matching of algorithms and facilitating benchmarking across various synthetic and real-world tasks with minimal effort.

Figure 5: Pearson correlation between the quality of a surrogate fit, and BO regret, split by acquisition function and use of a TR.