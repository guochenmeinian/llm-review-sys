# HYDRA: Model Factorization Framework for Black-Box LLM Personalization

Yuchen Zhuang\({}^{1}\), Haotian Sun\({}^{1}\), Yue Yu\({}^{1}\), Rushi Qiang\({}^{1}\), Qifan Wang\({}^{2}\), Chao Zhang\({}^{1}\), Bo Dai\({}^{1}\)

\({}^{1}\) Georgia Institute of Technology, \({}^{2}\) Meta AI

{yczhuang, haotian.sun, yueyu, rqiang6, chaozhang}@gatech.edu,

wqfcr@fb.com, bodai@cc.gatech.edu

###### Abstract

Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains _shared_ knowledge across users, while the multiple personal heads capture _user-specific_ preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at [https://github.com/night-chen/HYDRA](https://github.com/night-chen/HYDRA).

## 1 Introduction

Pre-trained large language models (LLMs)  have revolutionized various natural language processing (NLP) tasks, ranging from traditional recommendation systems  to modern virtual assistants . Despite their strong capabilities, LLMs require further customization to consistently demonstrate desirable behaviors to each user and achieve optimal performance in specific use cases . As a result, LLM personalization has emerged as a rapidly evolving area of research , with the goal of tailoring the emergent abilities of LLMs to meet the unique needs of individual users.

Several existing studies have shown effectiveness in personalizing LLMs, including (1) fine-tuning personalized LLMs for each user  and (2) aligning LLMs to personalized preferences through Reinforcement Learning from Human Feedback (RLHF) . However, both fine-tuning and RLHF-based methods require access to model parameters, restricting their use to white-box LLMs only (e.g., LLaMA-2 ). These models tend to be less capable than black-box LLMs (e.g., GPT-3.5) because they have access to less training data and smaller model scales. Moreover, RLHF-based methods require more explicitly attributed characteristics (_e.g._, style)  from implicit user behavior history and necessitate excessive annotation efforts for capturing human preferences.

Without access to modify the model parameters for black-box LLM personalization, an alternative solution is to augment user-specific content and/or context into the prompt template. One straightforward approach is to incorporate the user's complete profile or entire historical behavior into the prompt design . However, integrating the entire profile may exceed the length limitations of LLMs and lead to substantial costs, while randomly selected records cannot effectively capture representative patterns. To address this dilemma, retrieval-augmented generation (RAG) approaches  have been explored by extracting the most relevant information from the user's historical data to facilitate personalized generation. One limitation is that a retrieval-augmented framework encodes different users independently in a personalized prompt, making it challenging to capture the shared (global) patterns of the entire user group . Moreover, in comparison to fine-tuning the entire or partial model parameters to create personalized language models for individual users , simply augmenting the input prompt through a centralized LLM without updating model parameters may diminish the effectiveness of personalization (Figure 1).

To address these challenges, we propose HYDRA, a learning-based model factorization framework that captures both user-specific and shared behavior patterns to enable effective personalization within black-box LLMs. HYDRA leverages a retrieval-augmented workflow, where a retriever initially extracts relevant user behaviors from historical data for effective user-specific preference identification. To achieve personalized generation, we focus on the training process of two fundamental components: (1) a personalized reranker to prioritize useful user information from the retrieved records, and (2) a personalized adapter to align black-box LLM outputs with user-specific preferences, without requiring access to internal model parameters. Both the reranker and the adapter can be decomposed into a base model with multiple personalized heads, similar to a Hydra. By employing model factorization, we effectively integrate shared (global) knowledge, captured by the centralized base model, with user-specific preferences, harnessed through multiple user-specific heads, to enhance generalization across the entire user group.

We conduct extensive experiments on LaMP , a comprehensive language model personalization benchmark, to evaluate the personalization capabilities of HYDRA across multiple dimensions, including three text classification tasks and two text generation tasks. Notably, HYDRA achieves an average improvement of 4.8% over the best-performing baselines across all five diverse tasks. Further in-depth studies reveal the robust capability of HYDRA in scaling up to accommodate larger user groups and extensive behavior history, as well as adapting to user behavior shifts. We also demonstrate the effectiveness of shared knowledge in enhancing user experience through both quantitative and qualitative analyses. To facilitate future research in black-box LLM personalization, we will release the code repository and model checkpoints for transparency and reproducibility.

Our main contributions are as follows: (1) We propose HYDRA, a black-box LLM personalization framework that effectively mines user behavior history and adapts to user preferences for enhanced user experience; (2) HYDRA integrates shared (global) knowledge from the base model and individual (local) preference from multiple user-specific heads through model factorization to deliver generalizable personalization; and (3) HYDRA significantly outperforms existing prompt-based methods across five diverse tasks in the LaMP benchmark , introducing a novel learning-based solution that achieves more effective adaptation to individual users in black-box LLMs.

Figure 1: Personalization in white-box and black-box LLMs. Existing methods prioritize (a) fine-tuning user-specific models in white-box LLM personalization, while (b) designing user-specific prompts for black-box LLM personalization. In HYDRA, we present (c) a learning-based model factorization solution to enhance the effectiveness of personalization in black-box LLMs. \(@sectionsign\) indicates the trainable parameters, whereas \(\) indicates the inaccessible fixed parameters.

Related Works

**In-Context Learning.** Vanilla personalized prompting approaches leverage the powerful in-context learning capability of LLMs by using the user's randomly sampled behavior history as contextual samples. Existing studies [6; 18; 25; 52; 64] have utilized encoding user histories-whether personal ratings, interaction histories, or exemplary reviews-as few-shot examples to facilitate LLMs in generating personalized content in various down-stream applications. Additionally, research has shown that utilizing a longer user history can potentially lead to better performance [5; 64].

**Profile-Augmented Prompting.** Improving upon the random sample strategy and leveraging the insights from the enhanced performance with more historical information, profile-augmented generation (PAG) summarizes user preferences and behavior patterns into natural language profiles for query augmentation. For instance, Richardson et al.  employ instruction-tuned LLMs to generate abstract summaries of user history data, integrating summarization for enhanced personalization. Similarly, ONCE  creates user profiles by summarizing topics and regions of interest from their browsing history, thereby assisting LLMs in capturing user preferences for downstream tasks.

**Retrieval-Augmented Prompting.** Compared to random sampling in in-context learning and the use of entire histories in PAG, retrieval-augmented prompting excels at extracting the most relevant records from user behavior history to enhance LLM personalization, thereby efficiently managing the growing user behavior data within LLMs' limited context length and supporting personalized generation with more relevant evidence. For instance, LaMP [41; 40] introduces a retrieval-augmented method to obtain the most relevant content from the user's behavioral history and incorporate it into the prompt design. Similarly, AuthorPred  retrieves relevant past user-written documents for personalized text generation. Pearl  proposes a generation-calibrated retriever to select from historic user-authored documents, enhancing personalization with relevant prompt augmentation.

**Limitations.** Similar to in-context learning, personalization based on PAG is prone to be easily distracted by irrelevant information retrieved, especially when there is a shift in user behavior between the current query and the user's historical records. Despite the improvement upon PAG by retrieving relevant information, RAG-based methods may still suffer from the quality of retrieved information, where the "most relevant" information may not necessarily be the "most useful" information to answer a new query. Additionally, prompting-based methods not only lack deeper personalization analysis due to their reliance on a single centralized model but also lack access to global knowledge due to the user-specific prompt design.

## 3 Hydra: Model Factorization for Black-Box LLM Personalization

### Problem Formulation: Black-Box LLM Personalization

Black-box LLM personalization refers to tailoring model generations to align with user preferences according to their history [20; 50; 17], without having access to model parameters. Specifically, given a black-box LLM \(G\) and a training dataset \(_{}=\{(q_{u},r_{u},_{u})\}\), where for each user \(u\), \(q_{u}\) indicates the input sequence, \(r_{u}\) refers to the target output, and \(_{u}\) represents the user's historical behavior containing preference information. The user history data \(_{u}=\{h^{i}_{u}\}\) includes all user behaviors \(h^{i}_{u}\), consisting of \((q^{i}_{u},r^{i}_{u})\) pairs, mirroring the task-specific query-answer format \((q_{u},r_{u})\). The goal of personalization is to adapt the LLM's generation \(\) to the target output \(r\), conditioned on both the input and the user history.

Traditional fine-tuning methods train universal models for all users, whereas personalized tuning aims to develop a unique model for each user \(u\), denoted as \(^{(u)}\), which captures the unique characteristics of its user-specific data distribution \(^{(u)}\). Ideally, each user model should possess both _shared_ (general) and _user-specific_ knowledge. Thus, the problem can be formulated as a decomposition of \(^{(u)}\) into a shared part \(\) and a personalized part \(^{(u)}\) to learn general and user-specific knowledge, respectively. The training object of LLM personalization can be formulated as:

\[_{,^{(1)},^{(2)},,^{(u)}}_{u=1}^{U} _{u}(;^{(u)};^{(u)}), \]

where \(_{u}(;^{(u)};^{(u)})\) denotes the loss function of user \(u\), and \(U\) is the total number of users. Specifically, in the black-box LLM personalization, the model parameters in \(G\) are not accessible,making it infeasible to directly fine-tune the black-box LLM. Thus, we present HYDRA, a model factorization framework for black-box LLM personalization, as shown in Figure 2.

### Retrieve-then-Rerank

To capture user-specific preference, we follow a _retrieve-then-rerank_ workflow to (1) retrieve the relevant user behavior records and (2) rerank them based on usefulness. Specifically, given an input query \(x\), we employ a retriever to retrieve top-\(N\) user history behaviors that have the highest relevance scores when compared with the input \(x\). The objective of the reranker is to identify user historical records that can serve as useful user preference information for answering the user's incoming queries.

**Training Data Generation.** Within the training data, each user only has a single query, which is insufficient for capturing the relationships between the behaviors of the same user. Scoring all the history pairs of users is quadratic in \(_{u=1}^{||}|_{u}||_{u}|\), which becomes prohibitively time-consuming. In order to obtain a high-quality candidate set of training samples for each user query \(q_{u}\), we retrieve the top-\(M\) relevant history records, denoted as \((q_{u},_{u})\). Additionally, to gain a better understanding of the user's history, we randomly sample another \(M\) historical records that can be considered as the user's previous queries. For each of these previous queries, we also retrieve the top-\(M\) relevant historical records. The training candidates can be represented as:

\[_{}=\{(q_{i},r_{i},h_{i})\}_{i=1}^{|_{ }|}=\{\{(q_{u},r_{u},(q_{u},_{u}))\} \{(h_{u}^{i},(h_{u}^{i},_{u}/h_{u}^{i}))\}_{i=1}^{M}\}_{ h_{i}^{i}_{u}u}, \]

where \(q_{i}\) indicates the query, \(r_{i}\) indicates the ground-truth answer to the query, and \(h_{i}\) indicates the candidate history. We then utilize an LLM as a labeling function to measure the potential improvement (_i.e._, usefulness) of each history upon the LLM personalized generation. Specifically, for each candidate \((q_{i},r_{i},h_{i})\), we first sample the generation of LLM \(_{i}\) by using the input context \(q_{i}\) and the candidate history \(h_{i}\) from \(p_{}(_{i}|q_{i},h_{i})\). Next, we compare the generation \(_{i}\) with the ground-truth answer \(r_{i}\), and create a binary label \(y_{i}=(_{i}=r_{i})\). For generation task, the condition is soften to \(y_{i}=(_{i} r_{i})\), where the Rouge metrics between \(r_{i}\) and \(_{i}\) is above a pre-defined threshold. To optimize the reranker, we combine the user query with the candidate history in an entailment learning style, resulting in the training inputs \(x_{i}=\{|\)\(q_{i}\) [SEP] \(h_{i}\) [SEP]\(\}\). Therefore, we can curate a training set for the reranker \(_{}=\{(x_{i},y_{i})\}_{i=1}^{|_{}|}\).

Figure 2: Overview of HYDRA. HYDRA follows a retrieval-augmented framework: (1) Firstly, we extend an original RAG to a two-stage _retrieve-then-rerank_ workflow, where we rerank the most useful information from relevant user behavior records to capture user-specific preference (Section 3.2); (2) Secondly, augmented by the selected historical data, we train an adapter to align the output of black-box LLMs with personalized human preference (Section 3.3). Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra-like structure (Section 3.4). The base model maintains _shared_ knowledge across users, while multiple personal heads capture _user-specific_ preferences. \⃝raisebox{-0.9pt}{[width=142.26378pt]{figures/RAG.pdf} represents the model decomposition for personalization.

HYDRA-Reranker Training.Training the reranker for each user allows for a personalized selection of relevant historical information based on the user's query. The training objective involves using the cross-entropy loss between the predictions made by HYDRA-Reranker heads and the ground truth. For a specific user \(u\) and the corresponding training sample \((x^{u}_{i},y^{u}_{i})_{}\), we can calculate the cross-entropy loss function for HYDRA-Reranker as follows:

\[^{}_{}=-y^{u}_{i} p^{u}_{i}-(1-y^{u} _{i})(1-p^{u}_{i}), \]

where \(p^{u}_{i}\) indicates the model prediction made by HYDRA-Reranker.

HYDRA-Reranker Inference.With the trained reranker, our objective is to select the top-\(k\) most relevant candidates from the retrieved user history for a personalized generation. For each query \(q_{i}\) from user \(u\) in the test data, we retrieve the most relevant history \(}^{u}_{i}=\{h^{u}_{i,1},,h^{u}_{i,N}\}\) and concatenate them to form the test inputs \(\{x^{u}_{i,1},,x^{u}_{i,N}\}\). By feeding these into HYDRA-Reranker, we obtain the corresponding predictions \(\{p^{u}_{i,1}, p^{u}_{i,N}\}\). From the test inputs, we select the top-\(k\) history candidates \(_{i}\) with the highest level of usefulness:

\[_{i}=k_{h_{i,j}}_{i}}(p^{u}_ {i,j}). \]

### Black-box LLM Adaptation

Complementary to personalizing the few-shot demonstrations for LLMs in Section 3.2, we further align the black-box LLM generation to user-specific preference through the training of a personalized adapter, eliminating the need for directly accessing model parameters.

Training Data Generation.To take full advantage of the preference information hidden in the user query and history, we generate the candidates for the adapter training:

\[_{}=\{(q_{i},r_{i})\}_{i=1}^{|_{}|}=\{q_{u},r_{u}\}_{u}\{\{(q^{i}_{u},r^{i}_{u})\}_{i=1}^{| _{u}|}\}_{u}. \]

For each input query \(q_{i}\) from the training dataset, we augment the reranked history candidates \(_{i}\) with the query and sample \(b\) candidate responses from the black-box LLM:

\[\{_{i,j}\}_{j=1}^{b} p_{}(_{i}|_{i},q_{ i}). \]

In comparison with the ground-truth personalized generation \(r_{i}\), we can evaluate each generated solution \(_{i,j}\) and assign a corresponding binary label \(y_{i}\) as \(y_{i}=1(_{i,j}=r_{i})\). Using the model generations, we establish a new dataset for the adapter training, denoted as \(_{}=\{x_{i,j},y_{i}\}_{i=1}^{|_{}|}\), where \(x_{i,j}=\{\)[CLS] \(q_{i}\) [SEP] \(_{i,j}\) [SEP]\(\}\) represents the concatenation of the user query with the entire candidate generation.

HYDRA-Adapter Training.We utilize the model decomposition training strategy (detailed in Section 3.4) to train personalized adapters for each user, which allows us to tailor the selection of candidate generation that best aligns with the user's preference. To calculate the task-specific loss function, we follow Eq. (3) and employ the same cross-entropy loss between the predictions of user-specific heads and the ground truth to optimize HYDRA-Adapter.

HYDRA-Adapter Inference.During the process of model inference, we conceptualize the black box LLM as a proposal generator, while the adapter functions as an evaluator. Specifically, for each test query \(x_{i}\) from user \(u\), we adopt the best-of-\(b\) inference, also known as rejection sampling. Initially, we sample \(b\) candidate generations \(\{_{i,j}\}_{j=1}^{b}\) from the LLM. By passing them through the HYDRA-Adapter, we obtain the corresponding score for each candidate \(\{p^{u}_{i,1},,p^{u}_{i,b}\}\). The solution with the highest score is then chosen as the final answer:

\[^{u}_{i}=_{j=1,,b}p^{u}_{i,j}. \]

### Model Factorization for Personalization

To further effectively capture the shared knowledge across all users as well as the individual user-specific behavior patterns, a direct solution is to develop a parameter decomposition method, where the shared parameters store the general knowledge, benefiting from a high model capacity, while the remaining parameters are designed to learn user-specific preferences that complement the overall understanding (typically on a smaller scale). Hence, it is adequate to employ a smaller-sized model (adapter) to represent these personalized parameters, instead of fine-tuning the entire LLM.

**Model Factorization.** Formally, in HYDRA, we assume that each personalized language model is associated with a set of weights \(=\{^{(u)}\}_{u}\), where \(^{(u)}\) represents the weights of the personalized model for user \(u\). Each \(^{(u)}\) is decomposed as:

\[^{(u)}=^{(u)}, \]

where \(\) represents the base model parameter matrix that is shared among all users, \(^{(u)}\) represents a personalized head model, and \(\) indicates the operation of appending the user-specific head \(^{(u)}\) on top of the base model \(\). Specifically, we add \(||\) heads to the final hidden states \(s\) generated by the original model. The \(u\)-th head measures the usefulness between query and histories, or the level of preference for the generation from the perspective of the \(u\)-th user. The prediction of the \(u\)-th head is denoted as \(p^{(u)}\). For each head, we employ a single layer of feed-forward network. We define \(u\)-th head as \(^{(u)}=[_{1}^{(u)},_{2}^{(u)},_{1}^{(u)}, _{2}^{(u)}]\) and the prediction can be outlined as:

\[p^{u}=(_{2}^{(u)}(_{1}^{(u) } s+_{1}^{(u)})+_{2}^{(u)}),_{2}^{(u)} ^{d o},_{1}^{(u)}^{d d}, \]

where \(d\) indicates the dimension of hidden states and \(o\) indicates the dimension of outputs.

**Training Strategy.** We then train the base model in conjunction with multiple user-specific heads. For each sample \((x_{i}^{u},y_{i}^{u})\) of user \(u\) from the training data and the model prediction \(p_{i}^{u}\), we update the base model and the \(u\)-th head accordingly:

\[-(y_{i}^{u},p_{i}^{u}),\,^{(u )}^{(u)}-(y_{i}^{u},p_{i}^{u}), \]

where \(\) indicates the learning rate and \(\) represents the task-specific loss function.

**Fit Test User History.** In order to accommodate the newly incoming users in the test data, we cannot reuse the personalized heads from the training data. Therefore, we need to create and initialize new heads. To fit the test users' history, we freeze the base model and solely focus on the heads. This fitting process is simple and requires minimal computational resources. Similar to the update of \(^{(u)}\) in Eq. (10), when given a new user \(u^{}\) for testing, we also leverage the task-specific loss function \(()\) to update the head \(^{(u^{})}\):

\[^{(u^{})}^{(u^{})}-(y_{i }^{u^{}},p_{i}^{u^{}}), \]

where \((x_{i}^{u^{}},y_{i}^{u^{}})\) are obtained from test user history.

**Personalized Inference for Test Users.** Upon fitting the test user history, we obtain the base model \(\), which contains general knowledge across users, and the user-specific head \(^{(u^{})}\), which captures the user-specific knowledge in the test user history. Therefore, we can apply personalized inference directly to the test user \(u^{}\), given its query \(x_{u^{}}\):

\[p_{i}^{u^{}}=f_{^{(u^{})}}(x_{i}^{u^{}}), \]

where \(f_{^{(u^{})}}()\) indicates the inference of the model \(^{(u^{})}\).

## 4 Experiments

### Experimental Setup

**Datasets and Tasks.** We adopt a widely used language model personalization benchmark, LaMP , focusing on a diverse set of personalized text classification and generation tasks, including (1) Personalized News Categorization (**LaMP-2N**), (2) Personalized Movie Tagging (**LaMP-2M**), (3) Personalized Product Rating (**LaMP-3**), (4) Personalized News Headline Generation (**LaMP-4**), and (5) Personalized Scholarly Title Generation (**LaMP-5**). For data splitting, we follow the user-based separation setting provided by the LaMP benchmark, with 100 randomly selected users for training and an additional 50 randomly selected users for testing. No shared users are displayed across splits for specific measurements in personalization for new users. Additional details of the LaMP benchmark are available in Appendix D.

**Baselines.** We compare our proposed HYDRA with existing state-of-the-art black-box LLM personalization methods, including in-context learning (**ICL-Random**, with random selected \(k\)-item from user behavior history), retrieval-augmented prompting (**RAG**) , and profile-augmented prompting (**PAG**) . We also present the experimental results of gpt-3.5-turbo  (zero-shot) without profile/history augmentation in order to showcase the baseline performance of the backbone language model. Learning-based personalization, such as fine-tuning and RLHF-based methods, cannot be applied to black-box LLMs (_e.g._, gpt-3.5-turbo) due to the unavailability of model parameters. Details of baseline implementations are available in Appendix E.

**Evaluation Metrics.** Following the evaluation metrics specified in LaMP , we utilize accuracy (Acc.) and F-1 score (F-1) for the test classification tasks in LaMP-2N and LaMP-2M. Additionally, we employ mean absolute error (MAE) and root mean squared error (RMSE) for the ordinal multi-class classification task in LaMP-3. To comprehensively evaluate the personalized text generation tasks in LaMP-4 and LaMP-5, we report the ROUGE-1 (R-1), ROUGE-L (R-L), and BLEU metrics.

**Implementation Details.** For both baselines and our proposed HYDRA, we follow the same prompt template in the LaMP benchmark  and employ gpt-3.5-turbo(1106) and BM25  as the backbone black-box LLM and default retriever, respectively. Additionally, both HYDRA-Reranker and HYDRA-Adapter leverage the lightweight LongFormer-Base (110M)  as the backend language models. We include more implementation details in Appendix F.

  
**Dataset (\(\))** &  &  &  &  &  \\ 
**Method (\(\))** & Acc. \(\) & F-1 \(\) & Acc. \(\) & F-1 \(\) & MAE \(\) & RMSE \(\) & R-1 \(\) & R-L \(\) & BLEU \(\) & R-1 \(\) & R-L \(\) & BLEU \(\) \\  gpt-3.5-turbo  & 0.660 & 0.280 & 0.440 & 0.309 & 0.480 & 0.825 & 0.133 & 0.120 & 0.996 & 0.379 & 0.326 & 5.477 \\  ICL-Random (k=1) & 0.640 & 0.293 & 0.480 & 0.360 & 0.660 & 0.990 & 0.165 & 0.144 & 1.567 & 0.410 & 0.349 & 5.327 \\ ICL-Random (k=2) & 0.640 & 0.284 & 0.520 & 0.400 & 0.560 & 0.917 & 0.171 & 0.158 & 1.923 & 0.413 & 0.348 & 6.698 \\ ICL-Random (k=4) & 0.660 & 0.288 & 0.480 & 0.356 & 0.560 & 0.917 & 0.163 & 0.148 & 1.589 & 0.393 & 0.349 & 7.445 \\  RAG (k=1)  & 0.680 & 0.293 & 0.400 & 0.290 & 0.500 & 0.837 & 0.151 & 0.130 & 1.417 & 0.418 & 0.353 & 5.852 \\ RAG (k=2)  & 0.600 & 0.281 & 0.460 & 0.343 & 0.580 & 0.906 & 0.173 &

### Main Results

Table 1 presents the main experimental results of five varied personalization tasks in the LaMP benchmark. Compared to the zero-shot setting in gpt-3.5-turbo , even a random selection of historical records from user behavior or profiles enhances the model performance in most tasks, suggesting that personalization contributes to improved performance with black-box LLMs. HYDRA exhibits substantial relative improvements across all five tasks, with an average of 9.01% over the best-performing baselines. Specifically, HYDRA outperforms the best-performing alternative by 14.71% in accuracy for the Personalized News Categorization (LaMP-2N) task, 3.85% in accuracy for the Personalized Movie Tagging (LaMP-2M) task, 20.00% in MAE (where lower values indicate better performance) for the Personalized Product Rating (LaMP-3) task, 2.89% in R-1 for the Personalized News Headline Generation (LaMP-4) task, and 3.58% in R-1 for the Personalized Scholarly Title Generation (LaMP-5) task. This further improvement can be attributed to the improved quality (usefulness) of retrieved records in better representing user-specific preferences and the effective integration of global knowledge sourced from the entire user group.

### Ablation Studies

From Table 1, both HYDRA-Reranker and HYDRA-Adapter demonstrate their effectiveness by significantly improving the RAG and PAG baselines separately. Furthermore, we observe that HYDRA outperforms each of them individually, showcasing the complementary role of both components. In Table 2, we further eliminate the personalized training strategy of HYDRA, deteriorating to training a singular model across the entire user group, which solely incorporates global knowledge and lacks individualized customization for each user. The decline in model performance across all tasks highlights the significance of personalization through user-specific heads. In addition, we conduct additional experiments to comprehensively evaluate different components of HYDRA in black-box LLM personalization, including the effect of retrievers (Appendix G.4), the effect of adapters (Appendix G.5), and the effect of reranker with case studies and error analysis (Appendix H).

### Scale-up Analysis

**Number of Users in Training.** We study the impact of users in the training data, as depicted in Figure 3 (a)-(c). As the number of users increases from 20 to 100, we observe that HYDRA reaches over 90%

**Number of History per User.** We examine the impact of the number of historical records per user in Figure 3 (d)-(f). We randomly chose 50 users from each range of number of history per user. As the number of historical records increases, we consistently observe improved performance in both classification and generation tasks, as indicated by all metrics. This demonstrates the robustness of HYDRA in effectively capturing user-specific preferences, especially with a larger amount of user historical data.

**Number of Selected History (\(k\)) per User.** In Figure 3 (g)-(i), we analyze the impact of the selected items \(k\) per user. It can be observed that HYDRA consistently outperforms other baselines across all values of \(k\), highlighting the robustness of HYDRA. Additionally, it is evident that model performance correlates with the number of retrieved historical records in most cases, providing further evidence of the effectiveness of the retrieval-augmented

Figure 3: Scale-up analysis for (a)-(c) Effect of users in training data, (d)-(f) Effect of historical records per user, (g)-(i) Effect of selected historical records (\(k\)) per user.

framework in black-box LLM personalization. However, an inconsistency is noted with RAG in LaMP-3, which may be attributed to the presence of noisy or irrelevant retrieved history. This finding emphasizes the importance of retrieval quality and raises the significance of the HYDRA-Reranker in measuring the usefulness of retrieved items. Additional scale-up experimental results are available in Appendix G.2.

**Number of Users in Training Set.** We also conduct additional scale-up experiments (Table 3) to evaluate HYDRA with an increased number of users, increasing from 100 to 1000, across all five tasks. Our findings from the scale-up experiments show that HYDRA maintains its performance advantages over baselines as the number of users increases.

### Empirical Personalization Analysis

**Performance w/o Shared Knowledge.** We conduct additional experimental analysis on both text classification and generation tasks for a comprehensive evaluation. In Figure 4, we study how the shared knowledge is leveraged in HYDRA. Specifically, we create three settings with different levels of leveraging shared knowledge: (1) using _only test user history_ for training, (2) updating just the _head_ parameters using training user data, and (3) updating the _full_ model parameters using training user data. We observe that incorporating a larger volume of training user data yields superior results compared to solely relying on test user history. The inclusion of additional training user information aids in capturing global information effectively. In addition, updating the entire model shows inferior performance compared to updating only the heads. This discrepancy arises due to the increased number of parameters, which necessitates a larger amount of data to accurately capture each user's

Figure 4: Performance under different levels of shared knowledge.

  
**Dataset (\(\))** &  &  &  &  &  \\ 
**Method (\(\))** & Acc. \(\) & F-1 \(\) & Acc. \(\) & F-1 \(\) & MAE \(\) & RMSE \(\) & R-1 \(\) & R-L \(\) & BLEU \(\) & R-1 \(\) & R-L \(\) & BLEU \(\) \\  gpt-3.5-turbo & 0.638 & 0.499 & 0.412 & 0.347 & 0.540 & 0.851 & 0.133 & 0.119 & 1.043 & 0.439 & 0.371 & 6.018 \\  ICL-Random (k=1) & 0.598 & 0.476 & 0.392 & 0.335 & 0.676 & 0.959 & 0.147 & 0.132 & 1.330 & 0.457 & 0.396 & 8.118 \\ ICL-Random (k=2) & 0.632 & 0.499 & 0.376 & 0.311 & 0.562 & 0.871 & 0.151 & 0.137 & 2.388 & 0.451 & 0.393 & 8.550 \\ ICL-Random (k=4) & 0.630 & 0.518 & 0.392 & 0.352 & 0.440 & 0.740 & 0.161 & 0.146 & 2.418 & 0.457 & 0.396 & 8.404 \\  RAG (k=1) & 0.610 & 0.486 & 0.408 & 0.345 & 0.602 & 0.871 & 0.154 & 0.138 & 1.649 & 0.468 & 0.405 & 7.820 \\ RAG (k=2) & 0.624 & 0.479 & 0.380 & 0.315 & 0.559 & 0.836 & 0.161 & 0.149 & 2.958 & 0.480 & 0.419 & 9.021 \\ RAG (k=4) & 0.656 & 0.524 & 0.392 & 0.339 & 0.391 & 0.716 & 0.167 & 0.155 & 3.615 & 0.479 & 0.418 & 9.108 \\  PAG (k=0) & 0.618 & 0.489 & 0.404 & 0.340 & 0.583 & 0.872 & 0.161 & 0.141 & 1.950 & 0.460 & 0.405 & 7.372 \\ PAG (k=1) & 0.630 & 0.500 & 0.418 & 0.357 & 0.414 & 0.787 & 0.163 & 0.153 & 2.934 & 0.474 & 0.414 & 8.372 \\  HYDRA & **0.748** & **0.551** & **0.446** & **0.373** & **0.328** & **0.656** & **0.175** & **0.167** & **4.772** & **0.508** & **0.442** & **9.519** \\   

Table 3: Scale-up experiment results on the LaMP benchmark, including \(1000\) and \(500\) users during training and testing, respectively.

information. Additionally, the transition from training user information to testing user information results in a domain shift of global knowledge.

Performance under User Behavior Shift.Following the setup in Tan et al.  (see Appendix G.3 for additional details), we evaluate HYDRA under user behavior shift when the query is not relevant to all historical records. As illustrated in Figure 5, our proposed HYDRA continues to outperform the current state-of-the-art baselines in the face of user behavior shift. This serves as evidence of its robustness and generalizability in providing a widely applicable solution for black-box LLM personalization. This improvement can be attributed to the personalized reranker, which reevaluates the candidates based on their utility rather than solely on relevance, together with the personalized adapter further adjusts to align with user preferences.

Effect of Significant Disparities in User Behavior.To consider more extreme cases of disparities in user behavior, we retrain HYDRA on a mixture of 50% users with the fewest interactions and another 50% users with the most interactions (blue in Figure 6, compared to the random selection in orange). The black and gray dashed lines represent the best-performing baselines on the first and second metrics. The experimental results demonstrate that HYDRA consistently outperforms existing baselines, even under extreme cases. Compared to the previous random selection of training users, HYDRA achieves relatively lower performance due to the imbalance of training samples for dense users and sparse users. By leveraging the global information in shared parameters, knowledge can be effectively transferred from dense users to sparse users, thereby enabling further personalization through the utilization of sparse user-specific head models.

## 5 Conclusions

In this paper, we proposed HYDRA, a model factorization framework for black-box LLM personalization that captures and leverages both user-specific and shared behavior patterns from historical data. Compared to prompt-based methods, HYDRA introduces a novel learning-based paradigm for black-box LLM personalization that not only remains highly effective in personalizing outputs by training a reranker with a black-box LLM adapter, but also eliminates the need for access to model parameters, presenting a promising alternative to existing mainstream techniques. Experimental results demonstrate that HYDRA outperforms state-of-the-art RAG and PAG baselines by an average relative improvement of 9.01% across five diverse personalization tasks. HYDRA establishes a foundation for learning-based black-box LLM personalization, facilitating targeted enhancements in user experience and ultimately contributing to the development of human-centric intelligent systems.