# Affinity-Aware Graph Networks

Ameya Velingker

Google Research

ameyav@google.com &Ali Kemal Sinop

Google Research

asinop@google.com Ira Ktena

Google DeepMind

iraktena@google.com &Petar Velickovic

Google DeepMind

petarv@google.com &Sreenivas Gollapudi

Google Research

sgollapu@google.com

Equal contribution

###### Abstract

Graph Neural Networks (GNNs) have emerged as a powerful technique for learning on relational data. Owing to the relatively limited number of message passing steps they perform--and hence a smaller receptive field--there has been significant interest in improving their expressivity by incorporating structural aspects of the underlying graph. In this paper, we explore the use of affinity measures as features in graph neural networks, in particular measures arising from random walks, including effective resistance, hitting and commute times. We propose message passing networks based on these features and evaluate their performance on a variety of node and graph property prediction tasks. Our architecture has low computational complexity, while our features are invariant to the permutations of the underlying graph. The measures we compute allow the network to exploit the connectivity properties of the graph, thereby allowing us to outperform relevant benchmarks for a wide variety of tasks, often with significantly fewer message passing steps. On one of the largest publicly available graph regression datasets, OGB-LSC-PCQM4Mv1, we obtain the best known single-model validation MAE at the time of writing.

## 1 Introduction

Graph Neural Networks (GNNs) constitute a powerful tool for learning meaningful representations in non-Euclidean domains. GNN models have achieved significant successes in a wide variety of node prediction , link prediction , and graph prediction  tasks. These tasks naturally emerge in a wide range of applications, including autonomous driving , neuroimaging , combinatorial optimization , and recommender systems , while they have enabled significant scientific advances in the fields of biomedicine , structural biology , molecular chemistry  and physics .

Despite the predictive power of GNNs, it is known that the expressive power of standard GNNs is limited by the 1-Weisfeiler-Lehman (1-WL) test . Intuitively, GNNs possess at most the same power in terms of distinguishing between non-isomorphic (sub-)graphs, while having the added benefit of adapting to the given data distribution. For some architectures, two nodes with different local structures have the same computational graph, thus thwarting distinguishability in a standard GNN. Even though some attempts have been made to address this limitation with higher-order GNNs , most traditional GNN architectures fail to distinguish between such nodes.

A common approach to improving the expressive power of GNNs involves encoding richer structural/positional properties. For example, distance-based approaches form the basis for works such asPosition-aware Graph Neural Networks , which capture positions/locations of nodes with respect to a set of anchor nodes, as well as Distance Encoding Networks , which use the first few powers of the normalized adjacency matrix as node features associated with a set of target nodes.

Here, we take an approach that is inspired by this line of work but departs from it in some crucial ways: we seek to capture both _distance_ and _connectivity_ information using general-purpose node and edge features without the need for specifying any anchor or target nodes.

**Contributions**: We propose the use of _affinity metrics_ as features in a GNN. Specifically, we consider statistics that arise from random walks in graphs, such as _hitting time_ and _commute time_ between pairs of vertices (see Sections 3.1 and 3.2). We present a means of incorporating these statistics as scalar edge features in a message passing neural network (MPNN)  (see Section 3.4). In addition to these scalar features, we present richer vector-valued _resistive embeddings_ (see Section 3.3), which can be incorporated as node or edge feature vectors in the network. Resistive embeddings are a natural way of embedding each node into Euclidean space such that the squared \(L^{2}\)-distance between nodes recovers the commute time. We show that such embeddings can be incorporated into MPNNs, _even for larger graphs_, by efficiently approximating them using sketching and dimensionality reduction techniques and prove a novel additive approximation for hitting time (see Section 4).

Moreover, we evaluate our networks on a number of benchmark datasets of diverse scales (see Section 5). First, we show that our networks outperform other baselines on the PNA dataset , which includes 6 node and graph algorithmic tasks, showing the ability of affinity measures to exploit structural properties of graphs. We also evaluate the performance on a number of graph and node tasks for datasets in the Open Graph Benchmark (OGB) collection , including molecular and citation graphs. In particular, our networks with scalar effective resistance edge features achieve the state of the art on the OGB-LSC PCQM4Mv1 dataset, which was featured in a KDD Cup 2021 competition for large scale graph representation learning.

Finally, we provide intuition for why affinity-based measures are fundamentally different from aforementioned distance-based approaches (see Section 3.5) and bolster it with detailed theoretical and empirical results (see Appendix D) showing favorable results for affinity-based measures.

## 2 Related Work

Our work builds upon a wealth of graph theoretic and graph representation learning works, while we focus on a supervised, inductive setting.

Even though GNN architectures were originally classified as spectral or spatial, we abstain from this division as recent research has demonstrated some equivalence of the graph convolution process regardless of the choice of convolution kernels [2; 7]. Spectrally-motivated methods are often theoretically founded on the eigendecomposition of the graph Laplacian matrix (or an approximation thereof) and, hence, corresponding convolutions capture different frequencies of the graph signal. Early works in this space include ChebNet  and its more efficient 1-hop version by Kipf et al. , which offers a linear function on the graph Laplacian spectrum. Levie et al.  proposed CayleyNets, an alternative rational filter.

Message passing neural networks (MPNNs)  perform a transformation of node and edge representations before and after an arbitrary aggregator (e.g. _sum_). Graph attention networks (GATs)  aimed to augment the computations of GNNs by allowing graph nodes to "attend" differently to different edges, inspired by the success of transformers in NLP tasks. One of the most relevant works was proposed by Beaini et al. , i.e. directional graph networks (DGN). DGN uses the gradients of the low-frequency eigenvectors of the graph Laplacian, which are known to capture key information about the global structure of the graph and prove that the aggregators they construct using these gradients lead to more discriminative models than standard GNNs according to the 1-WL test. Prior work  used higher-order (\(k\)-dimensional) GNNs, based on \(k\)-WL, and a hierarchical variant and proved theoretically and experimentally the improved expressivity in comparison to other models.

Other notable works include Graph Isomorphism Networks (GINs) , which represent a simple, maximally-powerful GNN over discrete-featured inputs. Hamilton et al.  proposed a method to construct node representations by sampling a fixed-size neighborhood of each node and then performing aggregation over it, which led to impressive performance on large-scale inductive benchmarks. Bouritsas et al.  use topologically-aware message passing to detect and count graph substructures,while Bodnar et al.  propose a message-passing procedure on cell complexes motivated by a novel color refinement algorithm that proves to be powerful for molecular benchmarks. Meanwhile, Horn et al.  propose a layer based on persistent homology to incorporate global topological information.

**Expressivity.** Techniques that improve a GNN's expressive power largely fall under _three_ broad directions. While we focus on the feature-based direction in this paper, we also acknowledge that it in no way compels the GNN to use the additional provided features. Hence, we briefly survey the other two, as an indication of the future research in affinity-based GNNs we hope this work will inspire.

Another avenue involves modulating the _message passing rule_ to make advantage of the desired computations. Popular recent examples of this include DGN  and LSPE . DGNs leverage the graph's Laplacian eigenvectors, but they do not merely use them as input features; instead, they define a directional _vector field_ based on the eigenvectors, and use it explicitly to anisotropically aggregate neighbourhoods. LSPE features a "bespoke pipeline" for processing positional inputs.

The final direction is to modulate the _graph_ over which messages are passed, usually by adding new nodes that correspond to desired substructures. An early proponent of this is the work of , which explicitly performs message passing over \(k\)-tuples of nodes at once. Recently, scalable efforts in this direction focus on carefully chosen substructures, e.g., junction trees , cellular complexes .

## 3 Affinity Measures and GNNs

### Random Walks, Hitting and Commute Times

Let \(G=(V,E)\) be a graph of vertices \(V\) and edges \(E V V\) between them. We define several natural properties of a graph that arise from a random walk. A random walk on \(G\) starting from a node \(u\) is a Markov chain on the vertex set \(V\) such that the initial vertex is \(u\), and at each time step, one moves from the current vertex to a neighbor, chosen with probability proportional to the weight of outgoing edges. We will use \(\) to denote the stationary distribution of this Markov Chain. For random walks on weighted, undirected graphs, we know that \(_{u}=}{2M}\), where \(d_{u}\) is the weighted degree of node \(u\), and \(M\) is the sum of edge weights.

The _hitting time_\(H_{uv}\) from \(u\) to \(v\) is defined as the expected number of steps for a random walk starting at \(u\) to hit \(v\). We can also define the _commute time_ between \(u\) and \(v\) as \(K_{uv}=H_{uv}+H_{vu}\), the expected round-trip time for a random walk starting at \(u\) to reach \(v\) and then return to \(u\).

### Effective Resistance

A closely related quantity is the measure of _effective resistances_ in undirected graphs. This quantity corresponds to the effective resistance if the whole graph was replaced with a circuit where each edge becomes a resistor with resistance equal to the reciprocal of its weight. We will use \((u,v)\) to denote the effective resistance between nodes \(u\) and \(v\). For undirected graphs, it is known that  the effective resistance is proportional to the commute time, \((u,v)=K_{uv}\).

Our broad goal is to incorporate effective resistances and hitting times as edge features in an MPNN. In Section 3.4, we will show they can provably improve MPNNs' expressivity.

### Resistive Embeddings

Effective resistances allow us to define the _resistive embedding_, a mapping that associates each node \(v\) of a graph \(G=(V,E,W)\), where \(W\) are the non-negative edge weights, with an embedding vector. Before we specify the resistive embedding, we define a few terms. Let \(L=D-A\) be the graph Laplacian of \(G\), where \(D^{n n}\) is the diagonal matrix containing the weighted degree of each node and \(A^{n n}\) is the adjacency matrix, whose \((i,j)^{th}\) entry is equal to the edge weight between \(i\) and \(j\), if it exists; and \(0\) otherwise. Let \(B\) be the \(m n\) edge-node incidence matrix, where \(|V|=n\) and \(|E|=m\), defined as follows: The \(i\)-th row of \(B\) corresponds to the \(i\)-th edge \(e_{i}=(u_{i},v_{i})\) of \(G\) and has a \(+1\) in the \(u_{i}\)-th column and a \(-1\) in the \(v_{i}\)-th column, while all other entries are zero. Finally we will use \(C^{m m}\) to denote the conductance matrix, which is a diagonal matrix with \(C_{ii}\) being the weight of \(i^{th}\) edge. It is easy to verify that \(B^{T}CB=L\). Even though \(L\) is not invertible, its null-space consists of the indicator vectors for every connected component of \(G\). For example, if \(G\) is connected, then \(L\)'s nullspace is spanned by the all-\(1\)'s vector 2. Hence, for any vector \(x\) orthogonal to all-\(1\)'s, \(L L^{}x=x\), where \(L^{}\) is the pseudo-inverse.

We can express effective resistance between any pair of nodes using Laplacian matrices  as \((u,v)=(_{u}-_{v})^{T}L^{}(_{ u}-_{v}),\) where \(_{v}\) is an \(n\)-dimensional vector specifying the indicator for node \(v\). We are now ready to define the resistive embedding.

**Definition 3.1**.: (Effective Resistance Embedding)_\(_{v}=C^{1/2}BL^{}_{v}.\)_

A key property is that the effective resistance between two nodes in the graph can be obtained easily from the distance between their corresponding embeddings (see proof in Appendix C):

**Lemma 3.2**.: _For any pair of nodes \(u,v\), we have \(\|_{u}-_{v}\|_{2}^{2}=(u,v).\)_

One can easily check that any rotation of \(\) also satisfies Lemma 3.2, since rotations preserve Euclidean distances; more generally, if \(U\) is an orthonormal matrix, then \(U\) is also a valid resistive embedding. This poses a challenge if we want to use the resistive embeddings as node or edge features: we want a way to enforce that a (G)NN using them will do so in a way that is invariant or equivariant to any rotations of the embeddings. In our current work, we rely on data augmentation: at every training iteration, we apply random rotations to the input ER embeddings.

Remark.While data augmentation is a popular approach for promoting invariant and equivariant predictions, it is only _hinting_ to the network that such predictions are favourable. It is also possible, in the spirit of the geometric deep learning blueprint , to combine ER embeddings with an \(O(n)\)-equivariant GNN, which rigorously enforces rotational equivariance. A popular approach to building equivariant GNNs has been proposed by , though it focuses on the full Euclidean group \(E(n)\) rather than \(O(n)\). We leave this exploration to future work.

**Definition 3.3**.: Let \(:=_{u}_{u}_{u}\) be the mean of effective resistance embedding.

We might view \(\) as a "weighted mean"3 of \(\). We will define the hitting time radius, \(H_{}\), of a given graph as the maximum hitting time between any two nodes:

**Definition 3.4** (Hitting Time Radius).: \(H_{}:=_{u,v}H_{u,v}.\)__

We will need the following to bound the hitting times we computed:

**Lemma 3.5**.: _For any node \(u\), \(\|_{u}-\|^{2}}{M}.\)_

The proof follows easily from the fact that \(\) is a convex combination of all \(\)'s and Jensen's inequality.

### Incorporating Features into MPNNs

We reiterate that our main aim is to demonstrate (theoretically and empirically) that there are good reasons to incorporate affinity-based measures into GNN computations.

In the simplest instance, a method that improves a GNN's expressive power may compute additional _features_ (positional or structural) which would assist the GNN in discriminating between examples it otherwise wouldn't (easily) be able to. These features are then appended to the GNN's inputs for further processing. For example, it has been shown that endowing nodes with a one-hot based _identity_ is already sufficient for improving expressive power ; this was then relaxed to any randomly-sampled scalar feature by . It is, of course, possible to create dedicated features that even count substructures of interest . Further, the adjacency information can be factorised  or eigendecomposed  to provide useful structural embeddings for the GNN.

We will focus our attention on exactly this class of methods, as it is a lightweight and direct way of demonstrating improvements from these computations. Hence, our baselines will all be instances of the MPNN framework , which we will attempt to improve by endowing them with affinity-based features. We start by theoretically proving that these features indeed improve expressive power:

**Theorem 3.6**.: _MPNNs that make use of any one of (\(a\).) effective resistances, (\(b\).) hitting times, (\(c\).) resistive embeddings are strictly more powerful than the WL-1 test._Proof.: Since the networks in question arise from augmenting standard MPNNs with additional node/edge features, we have that these networks are at least as powerful as the 1-WL test.

In order to show that these networks are strictly _more powerful_ than the 1-WL test, it suffices to show the existence of a graph for which our affinity measure based networks can distinguish between certain nodes that a standard GNN (limited by the 1-WL test) cannot.

We present an example of a 3-regular graph on 8 nodes in Figure 1. It is well-known that a standard GNN that is limited by the 1-WL test cannot distinguish any pair of nodes in a regular graph, as the computation tree rooted at any node in the graph looks identical. However, there are three isomorphism classes of nodes in the above graph (denoted by different colors), namely, \(V_{1}=\{1,2\}\), \(V_{2}=\{3,4,7,8\}\), and \(V_{3}=\{5,6\}\).

We now show that GNNs with affinity based measures can distinguish between a node in \(V_{i}\) and a node in \(V_{j}\), for \(i j\). We note that the hitting time from \(a\) to \(b\) depends only on the isomorphism classes of \(a\) and \(b\). Thus, we write \(r_{i,j}\) as the effective resistance between a node in \(V_{i}\) and a node in \(V_{j}\). Note that \(r_{i,j}=r_{j,i}\), and it is easy to verify that:

\[r_{1,1}=2/3,r_{2,2}=15/28,r_{3,3}=4/7\] \[r_{1,2}=r_{2,1}=185/336\] \[r_{2,3}=r_{3,2}=209/336.\]

Hence, it follows that in a message passing step of an MPNN that uses effective resistances, vertices in \(V_{1}\), \(V_{2}\), and \(V_{3}\) will aggregate feature multisets \(\{r_{1,1},r_{1,2},r_{1,2}\}=\{2/3,185/336,185/336\}\), \(\{r_{2,1},r_{2,2},r_{2,3}\}=\{185/336,15/28,209/336\}\), and \(\{r_{2,3},r_{2,3},r_{3,3}\}=\{209/336,209/336,4/7\}\), respectively, all of which are all distinct multisets. Hence, such an MPNN can distinguish nodes in \(V_{i}\) and \(V_{j}\), \(i j\) for a suitable aggregation function.

If, instead of effective resistance, we use hitting time features or resistive embeddings, our results still hold. This is because, as we showed previously, the effective resistance between nodes is a _function_ of the two hitting times in either direction, as well as of the resistive embeddings of the nodes. In other words, if either hitting time features or resistive embeddings are used as input features for an MPNN, this MPNN would be able to compute the effective resistance features by applying an appropriate function (e.g., Lemma 3.2 for the case of resistive embeddings). Having computed these features, the MPNN can distinguish any two graphs that the MPNN with effective resistance features can. 

### Effective Resistance vs. Shortest Path Distance

It is interesting to ask how effective resistances compare with shortest path distances (SPDs) in GNNs, given the plethora of recent works that make use of SPDs (e.g., [49; 52; 29]). The most direct comparison of our effective resistance-based MPNNs would be to use SPDs as edge features in the MPNNs. However, note that SPDs along graph edges are trivial (unlike effective resistances, which incorporate useful information about the global graph structure).

An alternative to edge features would be to use (a) SPDs to a small set of _anchor nodes_ as features in an MPNN (e.g., P-GNN ) or (b) a dense featurization incorporating shortest paths between all pairs of nodes (e.g., the dense attention mechanism in Graphormer ). We remark that the latter approach typically incurs an \(O(n^{2})\) overhead, which our MPNN-based approach avoids.

We empirically compare our models to MPNNs that use approaches (a) and (b). Results on the PNA dataset show that our effective resistance-based GNNs outperform these approaches. Furthermore, we complement the empirical results with a theoretical result (see Appendix D) showing that under a limited number of message-passing steps, effective resistance features can allow one to distinguish structures that cannot be done using shortest path features. This provides some insight into why effective resistances can capture structure in GNNs

Figure 1: Degree 3 graph on 8 nodes, with isomorphism classes indicated by colors. While nodes of the same color are structurally identical, nodes of different colors are not. A standard GNN limited by the 1-WL cannot distinguish between nodes of different colors. However, affinity based networks that use effective resistances, hitting times, or resistive embeddings can distinguish every pair of such nodes.

that SPDs are unable to. We describe the result below. We point the reader to Appendix D for a proof of Theorem D.1.

## 4 Efficient Computation of Affinity Measures

In order to use our features, it is important that they be computable efficiently. In this section, we show how to compute or approximate the various random walk-based affinity measures. We share the results of our method on large-scale graphs in Section 5.4.

### Reducing Dimensionality of Resistive Embeddings

Given their higher dimensionality, we might find it beneficial to use resistive embeddings as GNN features instead of the effective resistance. Now, the difficulty with using resistive embeddings directly as GNN features is the fact that the embeddings have dimension \(m\), which can be quite large, e.g., up to \(n^{2}\) for dense graphs. It was shown by  that one can reduce the dimensionality of the embedding while approximately preserving Euclidean distances. The idea is to use a random projection via a constructive version of the Johnson-Lindenstrauss Lemma:

**Lemma 4.1** (Constructive Johnson-Lindenstrauss).: _Let \(x_{1}\), \(x_{2},,x_{n}^{d}\) be a set of \(n\) points; and let \(_{1},_{2},,_{m}^{n}\) be fixed linear combinations. Suppose \(\) is a \(k d\) matrix whose entries are chosen i.i.d. from a Gaussian \(N(0,1)\) and consider \(_{i}:=} x_{i}\)._

_Then, it follows that for \(k C(mn)/^{2}\), with probability \(1-o(1)\), we have \(\|_{j}_{i,j}_{j}\|^{2}=(1)\|_{j}_{i, j}x_{j}\|_{2}^{2}\) for every \(1 i m\)._

Using triangle inequality, we can see that the inner products between fixed linear combinations are preserved up to some additive error:

**Corollary 4.2**.: _For any fixed vectors \(,^{n}\), if we let \(X:=_{i}_{i}x_{i}\), \(:=_{i}_{i}_{i}\) and similarly \(Y:=_{i}_{i}x_{i}\), \(:=_{i}_{i}_{i}\); then:_

\[| X,Y-,| (\|X\|^{2}+\|Y\|^{2}).\]

Therefore, we can choose a desired \(>0\) and \(k=O((n)/^{2})\) and instead use \(}:V^{k}\) as the embedding, where

\[}_{v}=} BL^{}e_{v}\] (1)

for a randomly chosen \(k d\) matrix \(\) whose entries are i.i.d. Gaussians from \((0,1)\). Then, by lemma 3.2 and lemma 4.1, we have that for every edge \((u,v) E\),

\[\|}_{u}-}_{v}\|_{2}^{2}=(1 3 )(u,v)\]

with probability at least \(1-}\).

So the computation of random embeddings, \(}\), requires solving \(O((n+m) n/^{2})\) many Laplacian linear systems. By using one of the nearly linear time Laplacian solvers , we can compute the random embeddings in the near-linear time. Hence the total running time becomes \(O((n+m)^{3/2}n n/^{2})\).

   & &  &  \\
**Model** & **Agg score** & SSSP & Ecc & Lap & Fast & Cnn & Dom & Spec \\  GAT & -1.730 & -2.213 & -1.935 & -2.564 & -4.618 & -1.430 & -1.538 \\ GCN & -1.592 & -2.283 & -1.978 & -1.698 & -0.618 & -1.432 & -1.541 \\ MPNN (rand. res.) & -2.665 & -2.215 & -2.493 & -1.116 & -1.887 & -2.681 & -3.652 \\ MPNN (rand. res.) & -2.490 & -2.136 & -1.808 & -3.873 & -1.696 & -2.614 & -2.833 \\ DCN (Hentstrauss) + **Ek method + HT** & -2.740 & -2.165 & -1.911 & -1.848 & -1.588 & -2.841 & -3.528 \\
**DCN (featureuss) + **Ek method + HT** & -2.938 & -2.360 & -2.949 & -3.899 & -1.744 & -**3.660** & -3.823 \\ 
**Ek GAN** & -2.759 & -2.146 & -1.869 & -3.915 & **-1.920** & -2.940 & -3.811 \\ ER ER-ORD-ORD & -2.658 & -2.245 & -2.003 & -3.533 & -1.694 & -2.866 & -3.144 \\
**ER (edge) method** & -2.599 & -2.296 & -2.125 & -2.453 & -1.664 & -2.807 & -3.617 \\
**Helling Time** & -2.186 & -2.189 & -2.047 & -3.897 & -1.888 & -2.796 & -3.720 \\ 
**AI ER features** & **-3.106** & **-2.759** & **-3.082** & -4.077 & -1.858 & -2.964 & **-3.962** \\  

Table 1: \((MSE)\) on the PNA test dataset.

### Fast Computation of Hitting Times

Note that it is not clear whether there is a fast method for computing hitting times similar to commute times / effective resistances. The naive approach involves solving a linear system for each edge, resulting in a running time of at least \((nm)\), which is prohibitive. One of our technical contributions in this paper is a novel method for fast computation of hitting times. In particular, we will show how to use the approximate effective resistance embeddings, \(}\), to obtain an estimate for hitting times with additive error. Proofs can be found in Appendix C.

Let \(}:=_{u}_{u}}_{u}\). Just like \(}\) being an approximation of \(\), \(}\) is an approximation of \(\). Consider the following quantity:

\[_{u,v}=2M}_{v}-}_{u}, }_{v}-}.\] (2)

We will use this quantity as an approximation of \(H_{u,v}\). In the following part, we will bound the difference between \(H_{u,v}\) and \(_{u,v}\). Our starting point will be expressing \(H_{u,v}\) in terms of the effective resistance embeddings.

**Lemma 4.3**.: \(H_{u,v}=2M_{v}-_{u},_{v}-\) _where \(:=_{u}_{u}_{u}\)._

Given Lemma 4.3, we can establish the desired additive approximation property of \(_{u,v}\).

**Lemma 4.4**.: \(|_{u,v}-H_{u,v}| 3 H_{}\)_._

## 5 Experiments

As previously discussed, our empirical evaluation seeks to show benefits from endowing standard expressive GNNs with additional affinity-based features. All architectures we experiment with will therefore conform to the MPNN blueprint , which we describe in Appendix B. When relevant, we may also note results that a particular strong baseline (e.g., DGN , Graphormer ) achieves on a dataset of interest. Note that these baselines modulate the message passing procedure rather than appending features and are hence a different category to our method--their performance is provided for indicative reasons only. Where appropriate, we use "_DGN (features)_" to refer to an MPNN that uses eigenvector flows as additional edge features, without modulating the mechanism. We also use "ER (GNN)" to refer to the use of scalar effective resistance features.

### PNA dataset

The first dataset we explore is the PNA dataset , which captures a multimodal setting. This consists of a collection of _node tasks_, i.e. **(1)** Single-source shortest paths, **(2)** Eccentricity and **(3)** Laplacian features, as well as _graph tasks_, i.e. **(4)** Connectivity, **(5)** Diameter and **(6)** Spectral radius. PNA dataset is a set of structured graph tasks, and it complements our other datasets. As we can see in Table 1, even adding a single feature, effective resistance (ER GNN), yields the best average score compared to other models. Using hitting times as edge features improve upon effective resistances. However once we combine all ER features, which include effective resistances, hitting times as well as node and edge embeddings, we get the best scores. On these structured tasks, we can see that the affinity based measures provide a significant advantage.

We also note that incorporating affinity measures as additional features to the "DGN (features)" MPNN model also provides improvement over the standard "DGN (features)" baseline. We have accordingly included the results of a model that uses (both node and edge) ER embeddings and hitting time features along with DGN features.

### Small molecule classification: ogbg-molhiv

The ogbg-molhiv dataset is a molecular property prediction dataset comprised of molecular graphs without spatial information (such as atom coordinates). Each graph corresponds to a molecule, with nodes representing atoms and edges representing chemical bonds. Each node has an associated \(9\)-dimensional feature, containing atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring or not. The goal is to predict whether a molecule inhibits HIV virus replication or not. Our results are given in Table 2.

On this dataset, effective resistances provide an improvement over the standard MPNN. We achieve the best performance using ER node embeddings and hitting times with random rotations. With these features, our network achieves \(\% 0.358\) test accuracy, which is close to DGN.

Note that we provide GSN , HIMP , and GRWNN  as baselines. HIMP and GSN exploit structural/topological properties; the former is specifically designed for learning on molecular graphs, while the latter incorporates graph substructures. In addition to the comparison to GSN as a baseline, we additionally train an MPNN with substructure counts as additional features, namely counts of \(k\)-cycles (\(k=3,4\)) and \(k\)-paths (\(k=3\)) as motifs (in the same vein as the approach of , obtaining a test ROC-AUC of 76.41% \(\) 0.42 (note that this improves on the base MPNN test ROC-AUC of 74.67% \(\) 0.19). Furthermore, if we also add affinity measures (in addition to the aforementioned substructure counts), we obtain a test ROC-AUC of 78.50% \(\) 0.51. This suggests that affinity measures provide a performance or expressivity lift beyond that of substructure counts.

### Multi-task molecular classification: ogbg-molpcba

The ogbg-molpcba dataset comprises molecular graphs without spatial information (such as atom coordinates). The aim is to classify them across 128 different biological activities. We follow the baseline MPNN architecture from , including the use of Noisy Nodes.

Mirroring the evaluation protocol of , Table 3 compares the performance of incorporating ER and hitting time (HT) features into the baseline MPNN models with Noisy Nodes, at various depths. What can be noticed is that models utilising affinity-based features are capable of reaching as well as exceeding peak test performance (in terms of mean average precision). However, what's important is the effect of these features at lower depths: it is possible to achieve comparable or better levels of performance with _half the layers_, when utilising ER or HT features. This result illustrates the potential benefit affinity-based computations can have on molecular benchmarks, especially when no spatial geometry is provided as input.

### Scaling to larger graphs

Next, we present results on ogbn-arxiv and ogbn-mag, transductive datasets with large graphs.

#### 5.4.1 Large citation network: ogbn-arxiv

Most expressive GNNs that rely on computation of structural features have not been scaled beyond small molecular datasets (such as the ones discussed in prior sections). This is due to the fact that computing them requires (time or storage) complexity which is at least quadratic in the graph size--making them inapplicable even for modest-sized graphs. This is, however, not the case for our proposed affinity-based metrics. We demonstrate this by scalably computing them on a larger-scale node classification benchmark, ogbn-arxiv (a citation network with the goal of predicting the arXiv category of each paper). ogbn-arxiv has 169,343 nodes and 1,166,243 edges, making quadratic approaches infeasible. Using the combinatorial multigrid preconditioner [27; 25], we constructed the effective resistances on this graph in an hour on a standard MacBook Pro 2019 laptop.

  
**Model** & **Test Mean AP** \\  GCN (w/ virtual node) & 24.24 \(\) 0.34 \\ GIN (w/ virtual node) & 27.03 \(\) 0.23 \\ DeperGCN & 27.81 \(\) 0.38 \\ HIMP & 27.39 \(\) 0.17 \\  MPNN + NN + HT (ours) & **28.32\%**\(\) 0.13 \\   

Table 4: ogbg-molpcba performance compared to other baselines.

    & **MoHIV** \\  & Test \% ROC-AUC \\  GCN & \(76.06 0.97\) \\ GIN & \(75.58 1.40\) \\ MPNN & \(74.67 0.19\) \\ DGN & **79.70**\(\) 0.97 \\ GSN (GIN + VN) & 77.99 \(\) 1.00 \\ HIMP & \(78.80 0.82\) \\ GRWNN & \(78.38 0.99\) \\ 
**ER GNN** & \(77.75 0.426\) \\
**ER (node) embed. + HT** & _**79.13**\(\) 0.358 \\
**(with random rotations)** & \\   

Table 2: Test % AUC-ROC on the MolHIV dataset. Our results are averaged over five seeds.

    & **Test Mean Average Precision** \\
**Model** & _4 layers_ & _8 layers_ & _16 layers_ \\  MPNN  & 27.75\% \(\) 0.20 & 27.91\% \(\) 0.22 \\ MPNN + NN & 27.92\% \(\) 0.11 & 28.07\% \(\) 0.14 \\  MPNN + NN + ER (ours) & **28.11\%**\(\) 0.19 & 28.27\% \(\) 0.17 \\ MPNN + NN + HT (ours) & 28.03\% \(\) 0.15 & **28.32\%**\(\) 0.13 \\    
  
**Model** & **Test Mean AP** \\  GCN (w/ virtual node) & 24.24 \(\) 0.34 \\ GIN (w/ virtual node) & 27.03 \(\) 0.23 \\ DeperGCN & 27.81 \(\) 0.38 \\ HIMP & 27.39 \(\) 0.17 \\  MPNN + NN + HT (ours) & **28.32\%**\(\) 0.13 \\    
   GCN (w/ virtual node) & 24.24 \(\) 0.34 \\ GIN (w/ virtual node) & 27.03 \(\) 0.23 \\ DeperGCN & 27.81 \(\) 0.38 \\ HIMP & 27.39 \(\) 0.17 \\  MPNN + NN + HT (ours) & **28.32\%**\(\) 0.13 \\   

Table 3: ogbg-molpcba performance for various model depths. Best performance across all models is underlined.

As MPNN models overfit this transductive dataset quite easily, the dominant approach to tackling it are graph attention networks (GATs) . Accordingly, we trained a simple four-layer GAT on this dataset, achieving 72.02% \(\) 0.05 test accuracy. This compares with 71.97% \(\) 0.24 reported for a related attentional baseline on the leaderboard , indicating that our baseline performance is relevant.

ER embeddings on ogbn-arxiv need to be exceptionally high-dimensional to achieve accurate ER estimates (\(\)11,000 dimensions), hence we were unable to use them here. However, incorporating _ER scalar_ features into our GAT model yielded a statistically-significant improvement of 72.14% \(\) 0.03 test accuracy. _Hitting time_ features improve this result further to **72.25**% \(\) 0.04 test accuracy. This demonstrates that our affinity-based metrics can yield useful improvements even on larger scale graphs, which are traditionally out of reach for methods like DGN  due to computational complexity limitations.

Reliable global leaderboarding with respect to ogbn-arxiv is difficult, as state-of-the art approaches rely either on privileged information (such as raw text of the paper abstracts), incorporating node labels as features , post-processing the predictions , or various related tricks . With that in mind, we report for convenience that the current state-of-the-art performance for ogbn-arxiv without using raw text is 76.11% \(\) 0.09 test accuracy, achieved by GIANT-XRT+DRGAT.

#### 5.4.2 Heterogeneous citation network: ogbn-mag

We additionally present experiments on an even larger-scale network, ogbn-mag, a heterogeneous network derived from the Microsoft Academic Graph whose nodes consist of four types of entities (papers, authors, institutions, fields of study) and whose edges capture directed relations (e.g., a paper cites another paper, an author is affiliated with an institution, etc.). The task is to predict the venues of papers. The ogbn-mag dataset consists of 111,059,956 nodes and 1,615,685,872 edges.

As in the case of ogbn-arxiv, ER embeddings require an exceptionally high number of dimensions, given the large size of the network. Hence, we incorporated ER scalar features. As a baseline, we used a GraphSAINT  model that uses R-GCN aggregation , which reported a test accuracy of 47.51% \(\) 0.22 in a relevant leaderboard entry. After incorporating ER scalar features into the same model, we obtained a statistically-significant improvement of **47.99**% \(\) 0.23 in test accuracy.

### Large scale graph regression: OGB-LSC PCQM4Mv1

We finally include experimental results for one of the largest-scale publicly available graph regression tasks: the PCQM4Mv1 dataset from the OGB Large Scale Challenge . PCQM4M is a quantum chemistry dataset spanning 4 million small molecules, with a task to predict the HOMO-LUMO gap, an important quantum-chemical property. It is anticipated that structural features such as ER could be of great help on this task, as the v1 version of it is provided without any structural information, and the molecule's geometry is assumed critical for predicting the gap. We report the single-model validation performance on this dataset, in line with previous works [17; 48; 1].

PCQM4Mv1 comprises molecular graphs which consist of bonds and atom types, and no 3D or 2D coordinates. We reuse the experimental setup and architecture from , with only one difference: appending the effective resistance to the edge features. Additionally, we compare against an equivalent model which uses molecular conformations estimated by RDKit as an additional feature. This gives us a baseline which leverages an explicit estimate of the molecular geometry.

Our results are summarised in Table 5. We once again see a powerful synergy of effective resistance-endowed GNNs and Noisy Nodes , allowing us to significantly reduce the number of layers (to 32) and outperform the 50-layer MPNN result in . Further, we improve on the single-model performance of both the Graphormer  (which won the original PCQM4M contest after ensembling), and an equivalent model to ours which uses molecular conformers from RDKit. This illustrates how ER features can be competitive in geometry-relevant tasks even against features that inherently encode an estimate of the molecule's spatial geometry.

 
**Model** & **\#Layers** & **Nudy Nodes** & **Validation MAE** \\  MPNN  & 16 & Yes & 0.1249 \(\) 0.0003 \\ MPNN  & 50 & No & 0.1236 \(\) 0.0001 \\ Graphormer  & - & - & 0.1234 \\ MPNN  & 50 & Yes & 0.1218 \(\) 0.0001 \\ MPNN + Confermers  & 32 & Yes & 0.1212 \(\) 0.0001 \\  MPNN + ER (ours) & 32 & Yes & **0.1197 \(\) 0.0002** \\  

Table 5: Single-model OGB-LSC PCQM4Mv1 results.

Lastly, we remark that, to the best of our knowledge, our result is the _best published single-model result_ on the large-scale PCQM4M-v1 benchmark to date, and the only single model result with validation MAE under 0.120. We hope this will inspire future investigation on affinity-related GNNs for molecular tasks, especially in settings where spatial geometry is not reliably available.

## 6 Conclusions

In this paper, we proposed a message passing network based on random walk based affinity measures.

We believe that the comprehensive theoretical and practical results presented in our paper have solidified affinity-based computations as a strong component of a graph representation learner's toolbox. Our proposal carefully balances theoretical expressive power, empirical performance, and scalability to large graphs. While adding affinity measures as node/edge features provides the most direct route to incorporating them in a graph network, an interesting future direction would be to explore variants of GNN message functions that explicitly make use of _affinity-based computations_.