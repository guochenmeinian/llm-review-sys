# Real-time Stereo-based 3D Object Detection for Streaming Perception

Changcai Li\({}^{1,2}\)  Zonghua Gu\({}^{3}\)  Gang Chen\({}^{1,2,}\)  Libo Huang\({}^{4}\)

Wei Zhang\({}^{2}\)  Huihui Zhou\({}^{2}\)

\({}^{1}\)Sun Yat-sen University \({}^{2}\)Pengcheng Laboratory

\({}^{3}\)Hofstra University \({}^{4}\)National University of Defense Technology

lichc5@mail2.sysu.edu.cn zonghua.gu@hofstra.edu cheng83@mail.sysu.edu.cn libohuang@nudt.edu.cn zhangwei12130520126.com zhouhh@pcl.ac.cn

Corresponding author

###### Abstract

The ability to promptly respond to environmental changes is crucial for the perception system of autonomous driving. Recently, a new task called streaming perception was proposed. It jointly evaluates the latency and accuracy into a single metric for video online perception. In this work, we introduce StreamDSGN, the first real-time stereo-based 3D object detection framework designed for streaming perception. StreamDSGN is an end-to-end framework that directly predicts the 3D properties of objects in the next moment by leveraging historical information, thereby alleviating the accuracy degradation of streaming perception. Further, StreamDSGN applies three strategies to enhance the perception accuracy: (1) A feature-flow-based fusion method, which generates a pseudo-next feature at the current moment to address the misalignment issue between feature and ground truth. (2) An extra regression loss for explicit supervision of object motion consistency in consecutive frames. (3) A large kernel backbone with a large receptive field for effectively capturing long-range spatial contextual features caused by changes in object positions. Experiments on the KITTI Tracking dataset show that, compared with the strong baseline, StreamDSGN significantly improves the streaming average precision by up to 4.33%. Our code is available at https://github.com/weiyangdaren/streamDSGN-pytorch.

## 1 Introduction

Stereo-based 3D object detection [53; 6; 7] presents a notable advantage of low-cost in the deployment compared to LiDAR-based methods. However, existing stereo-based methods encounter challenges in delivering timely responses due to the intensive computations. Although there have been efforts towards lightweight approaches [35; 38], the latency of the inference process is still non-negligible. This is because the latency will lead to the misalignment between the prediction of the latest frame and the real-time changing scene, as shown in Figure 1.

Recently, a new metric [30; 52] named streaming accuracy was proposed for real-time online perception. Different from previous offline metrics [10; 3; 50] which only emphasize the detection accuracy, it simultaneously considers both accuracy and latency in the performance evaluation. With this metric, the model is forced to evaluate the processed world state against the prediction of the received frame. Hence, many high-accuracy but low-efficiency detectors [33; 17] will result in significant performance degradation for real-time perception applications.

challenges.A widely recognized solution for addressing streaming perception is to combine historical features and directly predict future outcomes at the current moment within a real-time framework . This is because when the model's inference speed exceeds the input frame rate, the metric of streaming accuracy consistently matches and evaluates the results of the current frame with the ground truth of the next frame. However, several challenges arise in the implementation of this solution. As delineated in Figure 2, these challenges include the following three specific aspects:

The **first challenge** is the misalignment between the future supervisory signals and the current features. It is observed that for moving objects, their ground truth positions in the next frame at time step \(t+1\) (depicted by the red bounding box) consistently differ from their current positions at time step \(t\). In such cases, the model needs to learn geometric information about the foreground from distant historical features. Furthermore, this misalignment will be exacerbated as the increasing disparity in relative velocities between the ego vehicle and the surrounding vehicles.

The **second challenge** lies in the implicit supervision when only using the ground truth of a single future frame. It is observed that motion objects with diverse relative velocities exhibit distinct trajectory lengths (depicted by the red arrow). Previous works  lack explicit trajectory supervision, requiring models to spontaneously learn various object motion offsets within latent space. Further, due to the complexity of 3D object detection tasks, a single regression supervision method may not be sufficient to ensure accurate detection of objects moving at different speeds.2

The **third challenge** arises from the effective utilization of context information embedded in the combined features. Due to the generally lower frame rates of 3D datasets compared to 2D ones, for example, the KITTI Tracking dataset  has a frame rate of only 10Hz while Argoverse-HD  has 30Hz. Therefore, in 3D datasets, larger intervals will result in larger spatial distances between the same objects in adjacent frames. At this point, feature extractor with small receptive field (RF) (depicted by the cyan areas) is insufficient to capture information from distant historical features.

Our solution.In this paper, we propose StreamDSGN, a streaming stereo-based 3D detector based on the advanced DSGN++  architecture. It directly predicts the 3D properties of the **next** frame by leveraging the fusion of bird's-eye view (BEV) features from both the current and historical frames. Further, StreamDSGN combines three strategies to tackle the above-mentioned challenges as follows:

* _Feature-Flow Fusion (FFF)_. To address the **first challenge** of misalignment between features and ground truth, we introduce a novel fusion method based on feature flow. It computes the feature flow between the current frame and the previous frame through similarity matching, and subsequently warps the current feature into the BEV coordinates of the next frame according to the flow map. This yields pseudo-next features aligned with the ground truth of the next frame.

Figure 1: In the context of online streaming perception, the environment changes during inference.

Figure 2: Illustration of the challenges.

* _Motion Consistency Loss function (MCL)._ To address the **second challenge** regarding implicit supervision, we establish additional explicit supervision based on motion consistency between adjacent frames. Specifically, we utilize the historical ground truth trajectories to supervise the predicted trajectory, guiding the model to recognize the offset magnitude and direction of object positions in the next frame.
* _Large Kernel BEV Backbone (LKBB)._ To address the **third challenge** involving large-span contextual information, we adopt a large receptive field backbone to extract fused features. This structure follows existing research on large kernel convolutions [12; 25], aiming to enhance the model's capacity to capture long-range dependencies.

We perform streaming simulations [30; 52] and conduct comprehensive experiments on the KITTI Tracking dataset , showing significant improvements in the streaming perception task. To the best of our knowledge, our work represents the **first** endeavor to explore the implementation of 3D object detection for streaming perception.

## 2 Related Work

Stereo-based 3D object detection.The methods in this field can be broadly classified into three categories as follows:

1) _2D-detection-based methods._ These methods [31; 49; 56; 44; 42] typically begin by feeding stereo image pairs into a 2D detector or an instance segmentation task to generate prior information before performing 3D object detection. The works in [35; 43] implement a single-stage stereo detection framework. Furthermore, Wu et al.  employ a semi-supervised method to enhance the foundational model by adopting plentiful unannotated images.

2) _Pseudo-LiDAR-based methods._ These methods [53; 62; 45; 9] employ the stereo matching task to obtain depth information from the scene and then transform it into a data structure resembling a LiDAR point cloud. Further, Li et al.  leverage the confidence derived from semantic segmentation to enhance the pseudo-LiDAR. The works in [26; 38] adopt a binary neural network (BNN) to quantize the disparity estimator for significantly reducing the computational overhead.

3) _Geometric-volume-based methods._ The methods in [6; 54; 13; 36; 7] adopt a representation that encodes stereo features derived from Siamese networks into differentiable geometric volumes. Further, Wang et al.  employ 3D occupancy to directly supervise the depth estimation model. The works in [13; 36] improve the performance by leveraging knowledge distillation. Chen et al.  perform depth-wise reconstruction of the volume in geometric modeling to alleviate the bottleneck of 2D to 3D information propagation. These methods represent state-of-the-art (SOTA) for stereo-based 3D object detection, and our work explores the application of the work in  for streaming perception.

Streaming perception.The concept of streaming perception was initially introduced in , which evaluates streaming average precision (sAP) while accounting for latency considerations. With this metric, non-real-time detectors [33; 17] will lead to great performance degradation since they unavoidably miss some intermediate frames. Therefore, Li et al.  further propose to address this issue through decision-theoretic scheduling, asynchronous tracking , and future prediction .

The work in  corroborates the findings of  and extends their analysis to object tracking. Ghosh et al.  propose a learned approximate execution framework to balance accuracy and latency implicitly. Instead of seeking better trade-offs or enhancing base detectors, StreamYOLO  simplifies streaming perception to an end-to-end task of "predicting the next frame" with a real-time detector. Based on this principle , LongShortNet  and DAMO-StreamNet  improve the streaming accuracy by leveraging longer temporal fusion and knowledge distillation respectively. The work in  expands the nuScenes dataset  and introduces a 3D benchmark tailored for streaming perception assignments. To the best of our knowledge, there are currently no existing applications of 3D perception algorithms for this task.

Temporal 3D object detection.These methods can be broadly classified into three categories: (1) The _LiDAR sequences-based methods_[60; 61; 63; 39] complement the 3D shape within a single frame by integrating historical features through temporal modeling. (2) The _streaming data-based methods_[14; 8; 5] treat each LiDAR packet as an independent sample without requiring a complete sweeping by the LiDAR. (3) The _Video-based methods_[18; 2] extend the image-based methods by tracking and fusing the same objects across different frames. Note that the essential difference between streaming perception and these methods is the misalignment of ground truth and features.

## 3 Our Approaches

### Base detector

We choose the recently proposed DSGN++  as the base detector in our study. Due to its high latency, our first goal is to optimize its pipeline to ensure the completion of inference for the current frame before the arrival of the next frame.

The optimization strategies are detailed in Appendix A.1. These strategies aim to ensure real-time processing of the detector without significantly sacrificing the offline accuracy. Experiments in Appendix A.2 show that the optimized model incurs only minor losses in offline accuracy while achieving a significant reduction in latency, decreasing from 263.33ms to 80.71ms (with a frame rate of 10Hz for the KITTI Tracking dataset ).

Next, we extend the optimized model into a streaming data detection framework, as illustrated in Figure 3. During the training phase, we take the consecutive stereo images \((_{t-1}^{L},_{t-1}^{R},_{t}^{L},_ {t}^{R})\) as input and directly predict the result \(_{t+1}^{box}\) of the next frame. This data organization allows for random shuffling of training samples. Note that in streaming perception tasks, the input data cannot include time step \(t+1\) since we cannot access future frames at the current moment in real-world scenarios. During inference, as the input frames are continuous, we only need to input the current frame and store its intermediate feature in a buffer queue for fusion with the next input.

Different from previous 2D approaches [59; 28; 15], our temporal fusion is executed before the BEV backbone instead of preceding the detection head. This is because we believe that the shallow convolutional layers in the detection head lack an effective receptive field to extract large-span contextual information from the fused features. We validate this conjecture in the experiments presented in Appendix A.4.

### Feature-Flow Fusion (FFF)

Different from fusion methods for temporal 3D object detection [19; 32; 34; 58], which aim to complement the geometric shape of objects, our Feature-Flow Fusion (FFF) is designed to warp current feature to align with the next ground truth. The pipeline of FFF is shown in Figure 4, it utilizes historical changes to accomplish the warping based on motion consistency.

Figure 3: The architecture of StreamDSGN pipeline. (a) The feature extractor retrieves features from streaming stereo image pairs \(\{(_{t}^{L},_{t}^{R})|t=1,...,n-1\}\) and flattens them into BEV features. (b) The depth regression component utilizes \(_{t}^{depth}\) as the supervision. (c) The BEV detector predicts the object state of the next moment by merging features from the current and previous frames. (d) The Feature-Flow Fusion generates a pseudo-next feature \(_{t+1}^{pseudo}\) by extrapolating from past features and then concatenates it with the existing historical feature set \(\{_{t},_{t-1}\}\).

Calculation of Feature Flow.Recall that our input features are \(_{t},_{t-1}^{H W C}\). The first step of FFF involves computing the feature flow between the \(_{t}\) and \(_{t-1}\). This process is similar to the computation of similarity volume in optical flow . We assume that the search space for the computing is discrete and rectangular. Specially, we define a shift indices set \(\) as:

\[=\{(-d,-d),(-d,-d+1),...,(0,0),...,(d,d-1),(d,d)\}\] (1)

where \(d\) is the maximal displacement. Then we shift the elements \(\{_{t-1}(i,j)|i=0,...,H-1;j=0,...,W-1\}\) in \(_{t-1}\) according to each shift index in \(\), and match the shifted \(_{t-1}\) with \(_{t}\) to compute the cosine similarity \((i,j)\) of each matched elements. When considering a single match with a shift index of \((_{s},_{s})\), the similarity at pixel index \((,)\) can be defined as:

\[(,)=_{t-1}(+ _{s},+_{s})\|\|_{t}(, )\|}((_{t-1}(+_{s},+_{s}))^{}_{t}(,))\] (2)

where \(\) represents the transpose operator. Stacking the whole similarity matrix \(=\{(i,j)|i=0,...,H-1;j=0,...,W-1\}\) obtained from different shift indices along a new dimension, we can yield the similarity volume \(^{H W D}\), where \(D=(2d+1)^{2}\).

Next, we ascertain the feature flow \(_{t t-1}^{H W 2}\) between \(_{t}\) and \(_{t-1}\) by identifying the index with the maximum similarity. This process can be formulated as:

\[_{t t-1}=gather(,*{arg\,max}_{D} )\] (3)

Note that the acquisition of \(_{t t-1}\) does not require any learnable parameters. In our implementation, we accelerate the above process through parallelization, and reduce computation by downsampling \(_{t-1}\) and \(_{t}\) via max poooling, then restore the resolution of \(_{t t-1}\) through bilinear interpolation.

Pseudo-next Feature Generation.According to the motion consistency, with a sufficiently high frame rate, the magnitude of displacement of an object from \(t\) to \(t-1\) is consistent with that from \(t\) to \(t+1\), but in opposite directions, _i.e._, \(_{t t+1}-_{t t-1}\). Thus, we can warp \(_{t}\) to the \(t+1\) grid by using inverted \(_{t t-1}\). Let \(_{t t-1}(,)=(( ,),(,))\) denotes the flow at pixel index \((,)\) in \(_{t t-1}\), where \(\) and \(\) represent values in the vertical and horizontal directions, respectively. Let \(_{t+1}^{pseudo}=\{_{t+1}^{pseudo}(i,j)|i=0,...,H-1; j=0,...,W-1\}\) represents the warped feature in \(t+1\) grid. The warped element at \((,)\) can be formulated as :

\[_{t+1}^{pseudo}(,)=_{t}( -(,),- (,))\] (4)

We refer to the \(_{t+1}^{pseudo}\) obtained through this pixel-level backward warping as pseudo-next feature. In theory, with a sufficiently fast frame rate and accurate matching, it can be aligned well with real \(_{t+1}\). As this process does not require the real \(_{t+1}\), it satisfies the constraints of streaming perception.

Note that \(_{t+1}^{pseudo}\) has limitations when dealing with occluded or truncated objects, and the warping operation may introduce additional edge noise into the scene. Thus, we fuse it with historical features to complement the geometric shape information of the objects. Similar to , we initially utilize weight-shared convolutions to reduce the channels of \(_{t-1}\), \(_{t}\), and \(_{t+1}^{pseudo}\), and then concatenate them together. The concatenated features are connected with \(_{t}\) via add operator to enhance the representation of static information, as illustrated in part (d) of Figure 3. The latency of our FFF process is only 7.67ms, please refer to Appendix A.5 for more latency and hyperparameter reports.

Figure 4: A toy example of pseudo-next feature generation.

### Motion Consistency Loss (MCL)

To explicitly guide the model in learning the offset magnitude, we propose a Motion Consistency Loss (MCL) as supplementary regression supervision, which consists of velocity loss and acceleration loss. This loss is also grounded on the principle of motion consistency. It leverages historical motion trajectories to guide the prediction of the next frame, as illustrated in Figure 5.

The initial step in calculating the MCL involves establishing correspondences between bounding boxes across different time steps. We establish these correspondences between the ground truth of \(\{_{t-2}^{box},_{t-1}^{box},_{t}^{box}\}\) by utilizing target IDs. For the prediction \(_{t+1}^{box}\), we calculate an IoU (Intersection over Union) matrix with respect to \(_{t}^{box}\) and employ a \(\) operation to identify the index corresponding to the highest matching value, thus establishing their correspondence.

Let \(_{i}^{pose}\) denotes the ground truth center position and rotation angle of the objects at time step \(i\), where \(i=t-2,t-1,t\), such that \(_{i}^{pose}=(x_{i}^{g},y_{i}^{g},z_{i}^{g},_{i}^{g})\). Similarly, \(_{t+1}^{pose}=(x_{t+1}^{p},y_{t+1}^{p},z_{t+1}^{p},_{t+1}^{p})\) represents the prediction for the next frame. Base on the correspondence, now we can calculate the displacement vector and the sine difference of the rotation angles between \(_{t+1}^{pose}\) and \(_{t}^{pose}\) for the predicted offset \(_{t+1 t}^{p}=( x^{p}, y^{p}, z^{p}, ^{p})\):

\[ x^{p} =x_{t+1}^{p}-x_{t}^{g}, y^{p}=y_{t+1}^{p}-y_{t}^{g},\] \[ z^{p} =z_{t+1}^{p}-z_{t}^{g},^{p}=(_{t+1}^{ p}-_{t}^{g})\] (5)

We can similarly calculate the ground truth offset \(_{t t-1}^{g}\) between \(_{t}^{pose}\) and \(_{t-1}^{pose}\) as supervision for the predicted offset \(_{t+1 t}^{p}\). Thus, the regression for velocity loss can be formulated as:

\[_{}=SmoothL1(_{t+1 t}^{p}-_{t  t-1}^{g})\] (6)

Further consideration is given to velocity change. We calculate the ground truth offset \(_{t-1 t-2}^{g}\) between \(_{t-1}^{pose}\) and \(_{t-2}^{pose}\) and then utilize the ground truth \(_{t t-2}^{g}=_{t t-1}^{g}-_{t-1  t-2}^{g}\) of velocity change to supervise the prediction \(_{t+1 t-1}^{p}=_{t+1 t}^{p}-_{t  t-1}^{g}\). We also employ the Smooth L1 loss  to regress the acceleration consistency:

\[_{}=SmoothL1(_{t+1 t-1}^{p}-_{t  t-2}^{g})\] (7)

The MCL can then be defined as:

\[_{MCL}=_{}+_{}\] (8)

where \(=0.8\). The grid search of \(\) can be seen in Appendix A.6. Let \(_{ori}\) denotes the original loss of DSGN++  with the supervision of the next frame, our total loss is finally therefore:

\[=}(_{ori}+_{MCL})\] (9)

where \(N_{pos}\) is the number of positive anchors and \(=0.5\).

### Large Kernel BEV Backbone (LKBB)

Our LKBB is built upon Visual Attention Network (VAN) , which comprises a Large Kernel Attention (LKA)  module and a Feed-Forward Network (FFN) , as shown in the Figure 5(a). The LKA module is composed of cascaded depth-wise convolution (DW-Conv), depth-wise dilation convolution (DW-D-Conv), and \(1 1\) convolution.

We migrate VAN into our BEV backbone network to increase its receptive field, as illustrated in Figure 5(b). For fair comparison, we adjust the number of VAN blocks and the multiplier for channel

Figure 5: Illustration of MCL.

transformations to keep the parameter count of LKBB similar to that of the original structure. Further, to maintain the sensitivity of the model to fine-grained geometric details, we incorporate multi-scale features via residual connections.

Let \(f_{}^{c,k,s}\) denotes the transpose convolution operation, where \(c\), \(k\) and \(s\) respectively represent the number of filters, kernel size and stride. The multi-scale fusion process can be formulated as:

\[_{out}=f_{}^{C,2,2}(f_{}^{2C,2,2}(_{2} )+_{1})\] (10)

where \(_{1}\), \(_{2}\) and \(_{out}\) represent the downsampled features from two stages and the final output feature, respectively, with dimensions \( 2C\), \( 2C\) and \(H W C\).

We report the comparison of parameter count and computational complexity between our LKBB and the original hourglass backbone  in Appendix A.7. Specifically, our parameter count is similar to the original structure while the computational complexity is reduced by approximately 6GFLOPs.

## 4 Experiments

### Experimental setup

Dataset.The prevalent autonomous driving datasets, such as nuScenes , nuScenes-H  and Waymo Open , lack stereo image provision. Further, the stereo frame rate (only 5Hz) within Argoverse  is insufficient for streaming simulation. Consequently, we conduct our experiments on KITTI tracking dataset , which provides stereo images and a higher frame rate (10Hz). We partition the training set with 20 scenes into numerous frame sequences, each comprising 40 frames. Among them, the even-numbered sequences are used for training with a total of 4,291 frames, and the odd-numbered sequences are used for testing with a total of 3,672 frames. We analyze the domain gap of this partitioning method in Appendix A.3.

Evaluation metrics.We follow [30; 52] to conduct the streaming simulation to obtain the streaming average precision (sAP) for both BEV and 3D perspectives. Consistent with KITTI , objects are categorized into three levels based on their recognition complexity: Easy, Moderate, and Hard. All of our precision measurements for experiments and ablation studies are calculated at \(IoU=0.7\) and with 40 recall positions. If not specified, all results are for the "Car" category.

Implementation details.Our experiments are conducted on the NVIDIA TITAN RTX platform with a total of 20 epochs. During the training phase, the Adam optimizer  is employed in conjunction with the OneCycle learning rate decay strategy . The initial learning rate is set to \(1e-3\), progressively increased to \(1e-2\), and finally decayed to \(1e-8\). Further, for a fair comparison with offline perception, we extend the copy-paste data augmentation [57; 7] to streaming data. It cascades moving foreground objects into consecutive training frames to enrich the training samples.

### Comparison with Meta-detector

We re-implement the meta-detector named Streamer for streaming benchmark in the stereo-based 3D object detection domain. Following the works in [30; 52], Streamer respectively employs DSGN++  and DSGN++\({}_{opt}\) (equipped with real-time optimization strategies) as the base detector to predict the current frame, and then utilizes a Kalman filter  to combine historical outputs for forecasting the future state. Note that Streamer [30; 52] is a non-end-to-end solution. In Table 1, we compare our

Figure 6: Illustration of LKBB.

method with these non-real-time and real-time settings. From these comparisons, we can make the following observations.

First, the Streamer  combined with DSGN++  setting results in a noticeable inferior in streaming accuracy. In our streaming simulation, we find that this is due to the large-span temporal interval, making it challenging for the Kalman filter  to accurately track each target in multi-object scenarios. Second, the Streamer  combined with the DSGN++\({}_{opt}\) setting shows higher performance compared to the DSGN++ combined one due to the latency reduction. However, with a high matching threshold (\(IoU=0.7\)), the improvement in streaming accuracy is quite limited. Third, the StreamYOLO  framework built upon DSGN++\({}_{opt}\) achieves significant improvements across all metrics. These results demonstrate the competitiveness of the end-to-end framework that directly predicts future states compared to non-end-to-end solutions in the benchmark. Finally, our StreamDSGN reaches the highest level of performance, with approximately a 5% improvement in each metric compared to the extended version of StreamYOLO . Despite the latency in this setting reaching 91.45ms, the inference speed remains faster than the input frame rate, thus meeting real-time requirements.

    &  &  &  &  & _{}\)} & _{}\)} \\  & & & & & & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\   \(a\) & – & – & – & – & – & 30.89 & 22.50 & 19.96 & 25.50 & 17.68 & 14.75 \\ \(b\) & ✓ & – & – & – & – & 53.73 & 47.46 & 43.36 & 38.62 & 32.55 & 29.39 \\ \(c\) & ✓ & ✓ & – & – & – & 83.20 & 71.15 & 65.74 & 75.38 & 59.55 & 53.11 \\  \(d\) & ✓ & ✓ & ✓ & – & – & 85.08 & 71.98 & 66.49 & 75.88 & 61.57 & 54.77 \\ \(e\) & ✓ & ✓ & – & ✓ & – & 84.63 & 71.01 & 66.18 & 78.15 & 62.28 & 56.80 \\ \(f\) & ✓ & ✓ & – & – & ✓ & 84.97 & 71.96 & 66.53 & 76.74 & 60.31 & 55.01 \\  \(g\) & ✓ & ✓ & ✓ & ✓ & ✓ & 85.40 & 72.47 & 68.66 & 77.47 & 63.76 & 57.42 \\   

Table 2: Ablation studies of our methods. Setting \(a\) denotes the DSGN++\({}_{opt}\). Setting \(b\) represents directly predicting future results. Setting \(c\) incorporates historical feature based on \(b\) and serves as our baseline method (highlighted in gray). Settings \(d\), \(e\), and \(f\) respectively incorporate our enhancement strategies. Setting \(g\) is our final proposed solution (highlighted in green).

    &  & Latency & _{}\) (IoU=0.5)} & _{}\) (IoU=0.5)} & _{}\) (IoU=0.7)} & _{}\) (IoU=0.7)} \\  & & (ms) & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\    \\ Streamer & DSGN++ & 263.33 & 14.76 & 11.56 & 10.62 & 12.51 & 8.86 & 7.74 & 5.26 & 3.42 & 3.11 & 2.82 & 1.92 & 1.51 \\ StreamYOLO\({}^{}\) & DSGN++\({}_{opt}\) & 80.71 & 70.57 & 61.04 & 56.31 & 64.47 & 55.18 & 50.36 & 30.89 & 22.50 & 19.96 & 25.50 & 17.68 & 14.75 \\ StreamYOLO\({}^{}\) & DSGN++\({}_{opt}\) & 81.32 & 92.22 & 85.75 & 82.45 & 89.40 & 82.89 & 78.10 & 80.72 & 68.51 & 63.14 & 73.02 & 58.37 & 51.86 \\ StreamDSGN (ours) & DSGN++\({}_{opt}\) & 91.45 & 93.10 & 87.46 & 84.34 & 92.53 & 84.55 & 81.34 & 85.40 & 72.47 & 68.66 & 77.47 & 63.76 & 57.42 \\   \\ Streamer & DSGN++ & 263.33 & 7.38 & 7.22 & 7.42 & 6.58 & 6.85 & 6.59 & 1.87 & 1.68 & 1.51 & 1.55 & 1.13 & 1.12 \\ Streamer & DSGN++\({}_{opt}\) & 80.71 & 58.23 & 54.57 & 50.25 & 56.56 & 53.97 & 49.70 & 31.26 & 28.92 & 26.55 & 25.18 & 23.50 & 21.49 \\ StreamYOLO\({}^{}\) & DSGN++\({}_{opt}\) & 81.32 & 74.54 & 69.10 & 63.48 & 73.44 & 68.68 & 63.08 & 53.27 & 49.35 & 44.67 & 45.53 & 42.15 & 38.01 \\ StreamDSGN (ours) & DSGN++\({}_{opt}\) & 91.45 & 80.70 & 75.38 & 69.23 & 80.41 & 75.12 & 68.96 & 62.33 & 57.51 & 51.34 & 54.12 & 50.05 & 44.89 \\   \\  Streamer & DSGN++ & 263.33 & 3.73 & 4.29 & 4.31 & 3.74 & 4.15 & 3.74 & 1.13 & 0.83 & 0.82 & 1.13 & 0.60 & 0.60 \\ Streamer & DSGN++\({}_{opt}\) & 80.71 & 41.11 & 40.73 & 40.07 & 40.78 & 40.37 & 39.24 & 7.56 & 15.52 & 14.82 & 6.77 & 14.

### Ablation Studies

To validate the effectiveness of each strategy, we conduct ablation experiments by toggling them on and off. Experimental results is illustrated in Table 2. In the following, we provide a more detailed description of the ablation experiments.

Effectiveness of the Pipeline.The comparison between setting \(\) and setting \(\) reveals that directly predicting future states by using a real-time detector can alleviate the accuracy degradation caused by streaming perception constraints. However, the improvement in accuracy at this time is limited due to the lack of information on each object's displacement magnitude. Furthermore, by incorporating historical features, streaming accuracy can be significantly improved (see setting \(\)). This improvement occurs because the fused features implicitly embed motion cues, enabling the model to learn different offsets for each object. Thus, setting \(\) serves as our baseline and is utilized for subsequent ablation studies to assess each enhancement strategy's effectiveness.

Effectiveness of Enhanced Strategies.The comparison of settings \(\), \(\), and \(\) with setting \(\) demonstrates the effectiveness of each enhancement strategy, respectively. From these comparisons, we can make the following observations.

First, since the fusion scheme used in setting \(\) is derived from the Dual-Flow module of StreamYOLO , the comparison between setting \(\) and setting \(\) can be viewed as a comparison between our FFF and the SOTA streaming perception fusion method. The results show that our FFF achieves improvements across all metrics. Second, setting \(\) reveals the effectiveness of MCL, as the additional supervision of trajectory consistency yields significant improvements. Compared to the baseline, improvements across the three levels reach 2.77%, 2.73%, and 3.69% in sAP\({}_{}\), respectively. Third, due to the consistency between the downsampling dimensions of the feature maps in LKBB and the baseline, as well as the similar parameter counts between the two structures, it is evident that the improvements in setting \(\) are primarily attributed to the advantage of the large receptive field feature extractor in capturing long-range contextual features. Finally, when we combine all strategies together (highlighted in green), the improvement in streaming accuracy reaches its peak, with a 4.33% increase in sAP\({}_{}\) compared to the baseline at the hard level.

Figure 7: Qualitative analysis of different relative velocity scenarios. We visualize the predictions in point clouds for a clearer comparison. The first row describes DSGN++  without any modifications. The second row integrates real-time optimization and fusion of historical frames to predict the next frame. The third row showcases our three enhancement strategies. Ground truth and prediction instances are respectively delineated by red and green bounding boxes, with lines inside the boxes indicating the orientation of objects.

### Qualitative Analysis

Visualization of Streaming Detection.Figure 7 presents the qualitative results of our proposed pipeline compared to the vanilla DSGN++ . In scenarios where surrounding vehicles remain relatively stationary, DSGN++  demonstrates a robust capacity for accurately aligning with ground truth. However, as these vehicles initiate motion relative to the ego vehicle, DSGN++  exhibits misalignments or mismatches in predictions due to the latency. In contrast, our baseline pipeline adeptly addresses this issue. With the enhancement strategy incorporating FFF, MCL and LKBB, the alignment between predicted boxes and ground truth boxes is further improved.

Visualization of Pseudo-next Feature.We visualize the pseudo-next feature in Figure 8 for qualitative analysis. We can observe that due to the constraints of streaming perception, there is varying misalignment between the previous feature and the current feature with respect to the next ground truth used for supervision. However, after similarity matching and reverse warping, our pseudo-next feature aligns well with the ground truth.

## 5 Conclusions

For the first time, we propose StreamDSGN, a real-time stereo-based 3D object detection framework for streaming perception. It is further equipped with Feature Flow Fusion, Motion Consistency Loss, and a Large Kernel BEV Backbone to enhance performance.

Limitations and Future Work.When objects are occluded or truncated, FFF may produce incorrect pseudo-next features due to erroneous similarity matching. Currently, we mitigate this issue by simply integrating historical features. In the future, we plan to leverage neural networks to directly predict the flow of dynamic foreground objects, aiming to be more robust in addressing this challenge by exploiting the adaptability of neural networks. The second limitation of our approach is that it has only been validated using stereo-based methods. In contrast, currently mainstream autonomous driving perception tasks typically employ surrounding multi-view camera systems, which differ from stereo-based methods in their construction of BEV representations. Consequently, our future research will focus on validating the effectiveness of this approach within the context of multi-view methods.

## 6 Acknowledgments

This research was supported by the National Natural Science Foundation of China under Grant 62027804, the Fund of National Key Laboratory of Multispectral Information Intelligent Processing Technology (No. 202410487201), and the Major Key Project of PCL (PCL2021A13).

Figure 8: Qualitative analysis of the pseudo-next feature. The first row displays complete feature maps from different time steps, while the second row shows corresponding local regions of the feature maps. The **solid** red box and **dashed** box respectively represent the ground truth of the next frame used for supervision and the locally zoomed-in area.