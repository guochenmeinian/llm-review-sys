# Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification

Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification

 Thomas Kwa

Independent / FAR Labs

kwathomas0@gmail.com

&Drake Thomas

Anthropic

drake@anthropic.com

&Adria Garriga-Alonso

FAR AI

adria@far.ai

Work was done while not at Anthropic

###### Abstract

When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model-a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization.

## 1 Introduction

Kullback-Leibler (KL) divergence constraints in reinforcement learning (RL) are employed to stay in regimes where the objective is accurate enough. Some on-policy (Schulman et al., 2015, 2017) and many off-policy (Abdolmaleki et al., 2018; Jaques et al., 2019) policy gradient algorithms employ KL constraints or penalties during optimization to prevent the policy from deviating too much from the data collection distribution. This ensures that estimates of each action's advantage are reliable enough to update the policy in a helpful way.

Reinforcement learning from human feedback (Christiano et al., 2017; Ziegler et al., 2020, RLHF) is a very popular method to induce desirable behavior in language models. RLHF starts with a base pre-trained model, then learns a reward function from human annotator data. Next, it trains an RL policy to maximize this reward, while penalizing high KL divergence from the policy to the base model. RLHF uses an on-policy algorithm and has accurate advantages, but the _reward function_ is always somewhat misspecified compared to desired behavior, due to insufficient data, human biases, and other factors.

The main purpose of the KL penalty in RLHF is to limit the consequence of reward modeling errors by keeping the policy within a distribution similar to that on which it was trained. Ideally, in the low-KL regime the reward model's errors are small enough that it provides correct updates to the base model. Gao et al. (2023) empirically supports this view: if the KL divergence in RLHF is allowed to grow too much, with a misspecified reward, the model's performance on the true utility starts to decrease.

We ask: can we obtain good outcomes from misspecified reward in RLHF by controlling the KL divergence? That is, if there is some error between the true reward \(V\) and the proxy reward \(U\), can the KL help us to still optimize \(V\)? Using mathematical proof, we answer the question in the negative for heavy-tailed errors: there exist policies which have infinite proxy reward \(U\), but whose KL with the base model vanishes (these have undetermined \(V\)). We term this phenomenon "catastrophic Goodhart", after Goodhart's law.

If the misspecification errors are independent and light-tailed, the KL divergence _does_ suffice to guarantee good outcomes. There may also be guarantees under weaker assumptions, but assumptions that intuitively seem sufficient are often not (see Section 5).

Possibly, other regularization schemes would guarantee good outcomes for heavy-tailed errors, but this is not just a problem of KL. We show that optimizing by conditioning on large reward \(U\) has similar outcomes in light- and heavy-tailed regimes.

Empirically, open-source language reward models seem to be light-tailed, which does not imply light-tailed errors but suggests it (Section 4.1). However, the errors are likely not independent and, given the prevalence of heavy-tailed distributions in the real world, error in future reward models may also be heavy-tailed. In any case, the present success of RLHF with misspecified rewards cannot be explained solely by the KL regularization in its objective.

## 2 Background

### KL divergence and KL regularization

Recall that KL divergence between two distributions P and Q is defined as

\[D_{}(P\|Q)=_{x}P(x)( ).\]

If we have two policies \(,_{0}\), we define \(D_{KL}(\|_{0})\) as the KL divergence between the distributions of actions taken on the states in trajectories reached by \(\). That is, if \(Tr()\) is the distribution of trajectories taken by \(\), we penalize \(D_{KL}(\|_{0})_{s T,T Tr()}[D_{KL}((s) \|_{0}(s))]\).

In RLHF, it is common to use the regularization term \( D_{KL}(\|_{0})\) to prevent the learned policy from deviating too much from the base policy, which can prevent unstable behavior or overfitting to the reward model. If our reward model gives reward \(U\), then the optimal policy for RLHF with a KL penalty is

\[_{}[U()]- D_{KL}(\|_{0}).\]

Often the regularization parameter \(\) is dynamically adjusted to keep the \(D_{KL}\) near some target value (Ziegler et al., 2020).

### Heavy-tailed distributions

A distribution \(P\) over \(\) with cumulative distribution function (CDF) \(F_{P}\) is heavy-tailed if its tail function \(_{P}(x) 1-F_{P}(x)\) satisfies

\[_{x}e^{tx}(x)=t>0.\]

Heavy-tailed distributions are well-known in statistics to have a higher probability of producing a single extreme value. For example, if the sum of two independent variables from heavy-tailed distributions is large, it is most likely due to one extreme sample rather than two equally large samples. (Wierman, 2013)

### Reward misspecification and Goodhart's Law

Reward misspecification has caused low-utility outcomes in practice; for example, in (Clark and Amodei, 2016), an RL agent trained to play a racing videogame according to a misspecified reward function achieves a high score while failing to complete the course.

Gao et al. (2023) introduce the concept of "overoptimization": optimizing for a proxy objective decreases performance according to the true objective. This raises the question: in general, when RLHF reward is misspecified, when does the optimal policy produce high utility?

By applying the proxy reward and true reward functions to a distribution over text (generated by an LLM), we get two scalar random variables, which we call \(U\) for proxy reward and \(V\) for true reward / utility. Then we can define the error in the proxy reward as \(X U-V\), so that \(U=X+V\). Framed this way, optimization for a proxy reward \(U\) is a mix of desirable optimization for \(V\) and undesirable optimization for \(X\). The joint distribution of \(V\) and \(X\) determines the limiting value of \(V\) as we apply more optimization. When we say that reward misspecification can have negative effects, we mean that too much variance in \(X\) can "redirect" the optimization pressure from \(V\) to \(X\), and prevent utility gain from optimization.

Reward misspecification is also studied by (Lambert and Calandra, 2024), (Laidlaw et al., 2024), and others. Laidlaw et al show that a KL penalty between action distributions can be ineffective, and propose instead regularizing state occupancy measure. Our results show an inherent weakness of KL divergence, including when applied to state occupancy measure.

We prove that in many cases, \(V 0\) in the limit of optimization for some proxy \(U\). We call this phenomenon "catastrophic Goodhart", after Goodhart's law: "when a measure becomes a target, it ceases to be a good measure" (Strathern, 1997). In these cases, the end result of optimizing for a proxy of \(V\) is no better than not optimizing at all. However, in other cases, \(V\) despite some reward misspecification; in these cases the reward misspecification is not severe enough to prevent good outcomes.

## 3 Theoretical results

When applying KL regularization, the trained model is regularized towards some base policy \(_{0}\). One would hope that a KL penalty can produce good outcomes even in the case of reward misspecification; that is, if the reward \(U\) is the sum of true utility \(V\) and an error term \(X\), we would hope that optimal policies under a KL penalty achieve high \(V\) even if the magnitude of \(X\) is large. We show that this is not always the case: Corollary 1 of Theorems 1, 3, and 2 establishes that when \(X(_{0})\) is heavy-tailed, there are arbitrarily well-performing policies \(\) with \(_{}[V]_{_{0}}[V]\). However, Theorem 4 shows that when error is light-tailed and independent of \(V\), the optimal policy under a KL penalty results in \(V>0\), and \(V\) can be made arbitrarily large. Thus, the tails of the error distribution are crucial in determining how much utility will result from optimization towards an imperfect proxy.

Theorems 5 and 6 (Section 3.4) show that the relationship of catastrophic Goodhart to heavy-tailed error is not just a quirk of KL divergence by using a different model of optimization based on conditioning on high reward values. Under this model (and given additional regularity conditions), it is also true that heavy-tailed error results in catastrophic Goodhart and light-tailed error plus independence results in arbitrarily large utility. All proofs are in the appendix.

### KL divergence on heavy- and light-tailed distributions

**Theorem 1**.: _Given any heavy-tailed reference distribution \(Q\) over \(\) with mean \(_{Q}\), and any \(M,>0\), there is a distribution \(P\) with mean \(_{P}>M\) and \(D_{KL}(P\|Q)<\)._

Outline of proof (see appendix for full proof): WLOG take \(_{Q}=0\). If we set \(P_{t}\) to upweight the probability mass of \(Pr_{P_{t}}(X>t)\) to \(c/t\) for some \(c,t\), then the mean of \(P_{t}\) will be approximately at least \(c\). As \(t\), the KL divergence \(D_{KL}(P_{t}\|Q)\) will shrink to zero.

Intuitively, in a heavy-tailed distribution, events with extremely high \(x\) are not very rare, so you don't pay much of a KL penalty to upweight them so they happen about \(1/x\) of the time.

**Theorem 2**.: _However, if the distribution \(Q\) is light-tailed and \(d=D_{KL}(P\|Q)\) is bounded, then \(_{P}\) is bounded, and \(_{P}-_{Q} 0\) as \(d 0\)._

### RLHF with KL penalty under heavy-tailed return distribution

We now adapt our result to the case where the policy is a language model and we are training it using RLHF. We are now applying KL divergence over the policies rather than the return distributions.

We first formally define the properties of RLHF on language models that cause the result to hold: namely, when when considered as a Markov decision process (MDP), environmental transitions are deterministic and return depends only on the final state reached.

_Definition:_ A deterministic-transition MDP with Markovian returns (DMRMDP) is an MDP \((,,P,R)\) such that:

* The transition function \(P:\) is deterministic, i.e., for each state \(s\) and action \(a\), there exists a unique state \(s^{}\) such that \(P(s^{}|s,a)=1\).

**In RLHF:** the transition is appending the generated token \(a\) to the context \(s\).
* There is a set of sink states \(E\) that terminate every trajectory, which is disjoint from the set of start states.

**In RLHF:** The sink states are sequences ending in \(\)E0S\(\) or above a certain length.
* Returns are Markovian; that is, for any two trajectories \(=(s_{1},a_{1},,s_{n}),^{}=(s_{1}^{},a_{1}^{}, ,s_{n}^{})\), if \(s_{n}=s_{n}^{}\), then \(\) and \(^{}\) have identical return distributions. Equivalently, for the trajectory random variable \(T=(S_{1},A_{1},)\) distributed according to any policy, with return \(G\), \(G(S_{<i},A_{<i}) S_{i}\) for any \(i 1\).

**In RLHF:** the return only depends on the full generated string, which is the final state.

The language model stochastically outputs the next token \(a\) given \(s\), and corresponds to the policy. A DMRMDP is therefore a good model of RLHF.

**Theorem 3**.: _Let \(W=(,,P,R)\) be a deterministic-transition MDP with Markovian returns. Given \(W\) we define the function that takes policies to trajectories \(Tr:(S A)(S A)^{*}\), and the average return function \(g:(S A)^{*}\), which induces a function \(G:(S A)^{*}\). Let \(_{0}:\) be some base policy. If \(G Tr(_{0})\) is heavy-tailed with finite mean \(_{Q}\), then for any \(M,>0\), there is a policy \(\) with mean return \([U|U G Tr()]>M\) and \(_{s T,T Tr()}[D_{KL}((s)\|_{0}(s))]<\)._

**Corollary 1**.: _Theorems 2 and 3 imply that when utility is light-tailed, reward modeling errors make the proxy reward heavy-tailed, and a policy \(\) is regularized severely enough to have KL divergence values approaching zero, the reward \([U()]\) can go to infinity while utility \([V()]\) approaches a value no higher than the base policy._

### Light-tailed + independence imply \([V]\)

**Theorem 4**.: _If \(U=X+V\) with \(X\) and \(V\) both light-tailed and \(V\) unbounded, and the distribution of \(U\) is continuous, and \(^{*}()*{arg\,max}_{}[U()]-  D_{KL}(,_{0})\), then \(_{ 0^{+}}[V(^{*}())]=\)._

### Conditioning as alternate model of optimization

Although we think a KL divergence penalty or cap is the most realistic setting for RLHF, it is not the only model of optimization where heavy-tailedness of the error determines whether catastrophic Goodhart occurs. Consider another model of optimization where \(U=X+V\) as before, but we simply condition on \(U\) being higher than some threshold \(t\).2 Then we are interested in the quantity \(_{t}[V|X+V t]\). If we slightly strengthen the heavy-tailedness and light-tailedness assumptions, heavy-tailed error results in catastrophic Goodhart, while light-tailed error results in arbitrarily high expected utility.

#### 3.4.1 Conditioning with heavy-tailed error produces catastrophic Goodhart

**Theorem 5**.: _Let \(X\) and \(V\) be two independent random variables with CDFs \(F_{X}\) and \(F_{V}\) and tail functions \(_{V} 1-F_{V}\), \(_{X} 1-F_{X}\) such that_

* \(V\) _has a finite mean._
* \(X\) _is subexponential; that is,_ \(_{x}(X_{1}+X_{2}>x)}{P_{}(X>x)}=2\) _if_ \(X_{1},X_{2}\) _are two independent samples from_ \(X\)_. This is a slightly stronger property than being heavy-tailed._* _The tail of_ \(V\) _is sufficiently lighter than the tail of_ \(X\) _that_ \(_{t}F_{V}(t)}{F_{X}(t)}=0\) _for some_ \(p>1\)_._

_Then \(_{t}[V|X+V t]=[V]\); that is, catastrophic Goodhart occurs in the limit of optimization for \(U=X+V\)._

The proof is included in the appendix. It requires expressing the conditional expectation in question as \(^{}vf_{V}(v)(X>t-v)}{_{-}^{ }f_{V}(v)(X>t-v)}\), then partitioning the interval \((-,)\) into four regions and bounding the integrand in the numerator above by a different quantity in each region.

#### 3.4.2 Conditioning with light-tailed error produces arbitrarily high utility

**Theorem 6**.: _Let \(X,V\) be independent random variables such that \(_{t}_{X}(t+1)}{F_{X}(t)}=0\). (This implies that X has tails that are dominated by \(e^{-cx}\) for any c, though it's a slightly stronger claim because it requires that X not have large jumps in the decay of its tails.) Then for any \(V\) with a finite mean which has no upper bound, \(_{t}[V|X+V>t]=\)._

Theorem 6 generalizes a consequence of the "Regressional Goodhart Identity" in (Gao et al., 2023).

## 4 Experiments

Our theoretical results now raise the question of whether the error in reward models is heavy-tailed or light-tailed in practice. 3 If we observe the reward distribution to be light-tailed, this is a strong indication that error is light-tailed. 4

To empirically test whether the reward is heavy-tailed, we consider two lines of evidence: examining the distributions directly through random sampling and temperature-1 sampling, and finding adversarial token sequences that get high rewards. We examine one small and one medium reward model that performed reasonably well on RewardBench (Lambert et al., 2023). The small model is an OpenAssistant model based on Pythia 1.4B, and the medium model is Starling 7B-alpha (Zhu et al., 2023)5.

For random sampling, we sample 30000 length-1024 sequences of uniformly random tokens and observe the distribution of rewards assigned by both Pythia 1.4B and Llama 7B-chat. We also use Llama 7B-chat to generate 16000 length-133 sequences at temperature 1 and observe the distribution of rewards assigned by Starling 7B-alpha.

Because sampling is inefficient at probing the extreme tail, we also find token sequences that optimize Starling 7B-alpha for reward. We considered Greedy Coordinate Gradient (GCG) from (Zou et al., 2023), a method used to find adversarial suffixes that circumvent jailbreaking, but decided on a faster version of GCG called Accelerated Coordinate Gradient (ACG) from (Haize Labs, 2024). See Table 4 for ACG hyperparameters.

Generating plots took about 5 GPU-hours on 1x Nvidia H100, and running ACG took a further 8 hours.

### Results

When sampling token sequences, both the Pythia model on random inputs (Figure B.1) and Starling 7B-alpha on Llama-generated inputs (Figure 2) appear approximately normal and, therefore, light-tailed. Starling on random inputs (Figure 1 is ambiguous, with the exponential Q-Q plot having an outlier that could indicate a heavy-tailed distribution, but the Hill estimator is consistent with a light-tailed distribution. Because Llama-7B-chat is a more reasonable base model than a completely random policy, we believe that Starling 7B-alpha is more likely to be light-tailed for the purposes of our theoretical results.

The ACG results need some interpretation. The KL divergence between two distributions \(P\) and \(Q\) if \(P\) is the same as \(Q\) a fraction \(1-\) of the time, but is some value \(x\) a fraction \(\) of the time is given by \(D_{KL}(P\|Q)=[(1-)q(x)+]()+(1-)(1-)(1-q(x))\).

When \(\) is small but much larger than \(q(x)\), we approximate this to first order as \(D_{KL}(P\|Q)()\). In Theorems 1 and 3, we prove that when the error is sufficiently heavy-tailed, a policy

  Parameter & Value \\  Context length & 133 \\  Iterations & 1000 \\  Candidates per seq. position (k) & 3 \\  Annealing starting value & 9 \\  Annealing ending value & 2 \\  

Table 1: Hyperparameters for ACG

Figure 1: Plots of the distribution of reward from 30000 random length-1024 token sequences to Starling 7B-alpha. Clockwise from top left: The histogram shows a unimodal distribution with a slight right skew. The normal probability plot indicates the data are heavier-tailed than normal. The Hill estimator (error bars are standard error) appears to be 0.20 for higher values but fluctuates for lower values. The exponential probability plot of the right half of the distribution is consistent with either light or heavy tails (under heavy tails, the slope would go to infinity).

that gets extremely large reward a small fraction of the time will achieve high expected reward with low KL divergence. This is not the case here because the rewards achieved through ACG were small and the log-probabilities extremely negative. For example, a policy that matches Llama 2-chat's base reward 99% of the time and uses the highest-reward input generated by ACG \(=\)1% of the time will have KL divergence from Llama 2-chat of \((()-1339.70)=13.35\) nats, but reward only about \(*(2.2377-0.3329)=0.02571\) greater than the base model, far less than can be obtained with the same KL divergence by conditioning.

## 5 Discussion and Limitations

### How likely is catastrophic Goodhart?

The low-KL policies that result in catastrophic Goodhart are not a unique optimal policy, just one family of high-performing policies. When optimizing \([U()]- D_{KL}(,_{0})\), the outcome depends on RL training dynamics; it could be that \(D_{KL} 0\) causing catastrophic Goodhart, but more likely both terms will go to infinity, potentially allowing \(V\). Catastrophic Goodhart can be prevented by using a light-tailed or bounded reward function.

Figure 2: Plots of the reward distribution from 16000 token sequences generated by Llama 7B-chat of length \( 133\), starting with five random tokens. Clockwise from top left: A histogram shows the reward distribution has a left skew. The normal probability plot suggests reward is approximately normal and thus light-tailed. The Hill estimator plot should stabilize if the distribution is heavy-tailed, but it does not; thus, there is no evidence the distribution is heavy-tailed. The exponential probability plot also indicates light tails, because the curve is bending downwards.

Even so, catastrophic Goodhart is likely to occur in many scenarios where KL regularization is naively employed in an attempt to avoid Goodhart's Law:

* If we maximize \(([U])+D_{KL}(Tr()\|Tr(_{0}))\), where \(\) is a bounded function (e.g. sigmoid), all near-optimal policies will have \(V 0\). Since we can only obtain so much reward from \(([U])\), it pays to make the KL (and thus V) go to zero.
* If we cap KL to a finite value (or dynamically adjust the KL penalty to target a finite KL, as done in Ziegler et al. (2020), then \([V]\) is also upper bounded by a finite value (see Theorem 2), and we think it is likely that \([V] 0\). Consider a toy model where an AI can adjust three parameters: true quality \(V\) of responses, frequency of reward hacking (producing actions with extremely high X), and severity of hacking (value of X on those actions). Adjusting the policy to increase \([U]\) without increasing KL increase the severity of hacking while decreasing either frequency of hacking or quality of responses. When \(E[U]\) is already large, decreasing quality has much better returns than decreasing frequency. This is similar to Theorems 5, 6 about hard-threshold optimization.
* Any way we maximize \([U()]- D_{KL}(,_{0})\) results in very large values of \([U()]\), and there are a number of arguments that extreme optimization for an imperfect proxy can result in decreased utility due to tradeoffs between \(X\) and \(V\); e.g., the constrained resource scenario in (Zhuang and Hadfield-Menell, 2021).

### Independence assumptions

Theorems 1-3 do not require any independence assumption, but Theorems 4, 5, and 6 require that error \(X\) and utility \(V\) are independent, which seems to be violated in practice. Future work could weaken this assumption, although intuitively obvious ways to weaken it result in the statement being false. 6

### Stronger optimization methods

We did not search the entire space of token sequences, so we cannot rule out that the reward is heavy-tailed enough to cause catastrophic Goodhart in some situations. While it is intractable to search the more than \(10^{2000}\) possible token sequences, future work could get more evidence through more powerful optimization methods.

### Reparameterizing reward

In some cases, a heavy-tailed reward can be reparameterized to make it light-tailed and avoid catastrophic Goodhart; however, in settings where the true reward is heavy-tailed, making reward artificially light-tailed or bounded can result in unintended behavior.

For example, a stock-trading agent should be rewarded by profit, but financial returns are known to be heavy-tailed. If we clip or otherwise transform rewards into a bounded interval, it will have no incentive to take into account huge gains or losses. Since RLHF rewards as implemented in Ziegler et al are unbounded, clipping or transforming rewards could itself cause reward misspecification.

In some cases, e.g. when the reward is not the true intended one, it is possible to reparameterize the reward without adverse effects. In the RL literature for Atari games, rewards are changes in score clipped to \([-1,1]\)(Machado et al., 2018).

### Relation to previous overoptimization work

Gao et al. (2023) found that optimizing the reward of small reward models causes overoptimization: a decrease in utility with increasing optimization. However, we observed that reward models are light-tailed, and (Theorem 4) that independence combined with light-tailed error prevents overoptimization. We think this discrepancy is explained by dependence between error and utility. Policies optimizedfor high error may activate features in the proxy reward models that are undesirable according to the true utility function.7 More research is needed to understand why high-error completions have low utility and to design reward models that do not suffer from this problem; perhaps it is possible to construct reward models whose errors are in directions orthogonal to human preferences, so that the large-reward completions do not have lower utility.

## 6 Conclusion

We have argued that the purpose of the KL divergence regularization in RLHF is to mitigate reward misspecification. However, we have also proven that when errors in the reward function are heavy-tailed, it cannot serve this purpose: even with zero KL divergence, there are policies that achieve very high misspecified reward and no actual reward.

When errors are light-tailed and independent, the KL divergence can mitigate misspecification, but when they are dependent, this may not be possible. Thus, we must look to places other than the KL objective to explain the current success of RLHF and ensure its continued success in the future.

## Impact Statement

As this work aims to improve the safety of future ML systems by characterizing a possible failure mode of reward misspecification in RLHF, we hope the social impact is positive. We see no particular ethical issues to discuss.