# Learning World Models with Identifiable Factorization

Yu-Ren Liu\({}^{*,1,4}\), Biwei Huang \({}^{*,2}\), Zhengmao Zhu\({}^{1,4}\), Honglong Tian\({}^{1}\),

&Mingming Gong\({}^{5,4}\), Yang Yu\({}^{7,1,6,7}\), Kun Zhang\({}^{7,3,4}\)

\({}^{1}\) National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\) University of California San Diego, USA

\({}^{3}\) Carnegie Mellon University, USA

\({}^{4}\) Mohamed bin Zayed University of Artificial Intelligence, UAE

\({}^{5}\) University of Melbourne, Australia

\({}^{6}\) Polisir.ai, China

\({}^{7}\) Peng Cheng Laboratory, China

{liuyr,zhuzm, tianhl, yuy}@lamda.nju.edu.cn bih007@ucsd.edu

mingmingming.gong@unimelb.edu.au kunz1@cmu.edu

###### Abstract

Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments - how to effectively extract and disentangle the information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundancies but retaining minimal and sufficient information for policy optimization. Experiments in synthetic worlds demonstrate that our method accurately identifies the ground-truth latent variables, substantiating our theoretical findings. Moreover, experiments in variants of the DeepMind Control Suite and RoboDesk showcase the superior performance of our approach over baselines.

## 1 Introduction

Humans excel at extracting various categories of information from complex environments . By effectively distinguishing between task-relevant information and noise, humans can learn efficiently and avoid distractions. Similarly, in the context of reinforcement learning, it's crucial for an agent to precisely extract information from high-dimensional, noisy, and non-stationary environments.

World models  tackle this challenge by learning compact representations from images and modeling dynamics using low-dimensional features. Recent research has demonstrated that learning policies through latent imagination in world models significantly enhances sample efficiency . However, these approaches often treat all information as an undifferentiated whole, leaving policies susceptible to irrelevant distractions  and lacking transparency in decision-making .

This paper investigates the extraction and disentanglement of diverse information types within an environment. To address this, we tackle two fundamental questions: 1) How can we establish acomprehensive classification system for different information categories in diverse decision scenarios? 2) Can the latent state variables, classified according to this system, be accurately identified? Prior research has made progress in answering the first question. Task Informed Abstractions  partition the state space into reward-relevant and reward-irrelevant features, assuming independent latent processes for each category. Iso-Dream  learns controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Denoised MDP  further decomposes reward-relevant states into controllable and uncontrollable components, also assuming independent latent processes. However, the assumption of independent latent processes is overly restrictive and may lead to decomposition degradation in many scenarios. Moreover, none of these approaches guarantee the identifiability of representations, potentially leading to inaccurate recovery of the underlying latent variables. While discussions on the identifiability of representations in reinforcement learning (RL) exist under linear assumptions , the question of identifiability remains unexplored for general nonlinear cases.

In this paper, we present IFactor, a general framework to model four distinct categories of latent state variables within the RL system. These variables capture different aspects of information based on their interactions with actions and rewards, providing transparent representations of the following aspects: (i) reward-relevant and controllable parts, (ii) reward-irrelevant but uncontrollable parts, (iii) controllable but reward-irrelevant parts, and (iv) unrelated noise (see Section 2). Diverging from prior methods, our approach employs a general factorization for the latent state variables, allowing for causally-related latent processes. We theoretically establish the block-wise identifiability of these four variable categories in general nonlinear cases under weak and realistic assumptions. Our findings challenge the conclusion drawn in the Denoised MDP  that only controllable and reward-relevant variables are required for policy optimization. We emphasize the necessity of considering states that directly or indirectly influence the reward during the decision-making process, irrespective of their controllability. To learn these four categories of latent variables, we propose a principled approach that involves optimizing an evidence lower bound and integrating multiple novel mutual information constraints. Through simulations on synthetic data, we demonstrate the accurate identification of true latent variables, validating our theoretical findings. Furthermore, our method achieves state-of-the-art performance on variants of RoboDesk and DeepMind Control Suite.

## 2 Four Categories of (Latent) State Variables in RL

For generality, we consider tasks in the form of Partially Observed Markov Decision Process (POMDP) , which is described as \((,,,R,T,O,)\), where \(\) is the latent state space, \(\) is the action space, \(\) is the observation space, \(R:S A\) defines the reward function, \(T:S A(S)\) is the transition dynamics, \(O:S A()\) is the observation function, \(\) is the discount factor. We use \((S)\) to denote the set of all distributions over \(S\). The agent can interact with the environment to get sequences of observations \(\{ o_{i},a_{i},r_{i}\}_{i=1}^{T}\). The objective is to find a policy acting based on history observations, that maximizes the expected cumulative (discounted) reward.

Figure 1: (a) An illustrative example of the car-driving task. (b) Four categories of latent state variables in the car driving task. (c) The structure of our world model. Grey nodes denote observed variables and other nodes are unobserved. We allow causally-related latent processes for four types of latent variables and prove that they are respectively identifiable up to block-wise invertible transformations. We show that both \(s^{}_{t}\) and \(s^{}_{t}\) are essential for policy optimization. The inclusion of the gray dashed line from (\(s^{}_{t}\), \(s^{}_{t}\)) to \(a_{i}\) signifies that the action could be determined by reward-relevant variables. (d) The structure of Denoised MDP . It assumes the latent processes of \(x_{t}\) and \(y_{t}\) are independent and uses only \(x_{t}\) for policy optimization. The gray dashed line from \(x_{t-1}\) to \(a_{t-1}\) shows that the action could be determined only based on the controllable and reward-relevant latent variables. Further, the existence of instantaneous causal effect from \(x_{t}\) and \(y_{t}\) to \(z_{t}\) renders the latent process unidentifiable without extra intervention on latent states .

In the context of POMDPs, extracting a low-dimensional state representation from high-dimensional observations is crucial. Considering that action and reward information is fundamental to decision-making across various scenarios, we disentangle the latent variables in the environment into four distinct categories, represented as \(}=\{s_{t}^{ar},s_{t}^{ar},s_{t}^{ar}\}\), \(s_{t}^{ar}\), \(s_{t}^{ar}\), based on their relationships with action and reward (see Figure 1(c) as a graphical illustration):

* Type 1: \(s_{t}^{ar}\) has an incident edge from \(a_{t-1}\), and there is a directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 2: \(s_{t}^{ar}\) has no incident edge from \(a_{t-1}\), and there is a directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 3: \(s_{t}^{ar}\) has an incident edge from \(a_{t-1}\), and there is no directed path from \(s_{t}^{ar}\) to \(r_{t}\).
* Type 4: \(s_{t}^{ar}\) has no incident edge from \(a_{t-1}\), and there is no directed path from \(s_{t}^{ar}\) to \(r_{t}\).

\(s_{t}^{ar}\) represents controllable and reward-relevant state variables that are essential in various scenarios. Taking the example of a car driving context, \(s_{t}^{ar}\) encompasses driving-related states like the current speed, position, and direction of the car. These latent variables play a critical role in determining the driving policy since actions such as steering and braking directly influence these states, which, in turn, have a direct impact on the reward received.

\(s_{t}^{ar}\) refers to reward-relevant state variables that are beyond our control. Despite not being directly controllable, these variables are still necessary for policy learning. In the context of car driving, \(s_{t}^{ar}\) includes factors like surrounding vehicles and weather conditions. Although we cannot directly control other cars, our car's status influences their behavior: if we attempt to cut in line unethically, the surrounding vehicles must decide whether to yield or block our behavior. As a result, we need to adjust our actions based on their reactions. Similarly, the driver must adapt their driving behavior to accommodate various weather conditions, even though the weather itself is uncontrollable.

The state variables denoted as \(s_{t}^{ar}\) consist of controllable but reward-irrelevant factors. Examples of \(s_{t}^{ar}\) could be the choice of music being played or the positioning of ormanns within the car. On the other hand, \(s_{t}^{ar}\) represents uncontrollable and reward-irrelevant latent variables such as the remote scenery. Both \(s_{t}^{ar}\) and \(s_{t}^{ar}\) do not have any impact on current or future rewards and are unrelated to policy optimization.

We next build a connection between the graph structure and statistical dependence between the variables in the RL system, so that the different types of state variables can be characterized from the data. To simplify symbol notation, we define \(s_{t}^{r}:=(s_{t}^{ar},\,s_{t}^{ar})\), \(s_{t}^{r}:=(s_{t}^{ar},\,s_{t}^{ar})\), \(s_{t}^{ar}:=(s_{t}^{ar},\,s_{t}^{ar})\) and \(s_{t}^{ar}:=(s_{t}^{ar},\,s_{t}^{ar})\). Specifically, the following proposition shows that \(s_{t}^{r}\) that has directed paths to \(r_{t+r}\) (for \(>0\)), is minimally sufficient for policy learning that aims to maximize the future reward and can be characterized by conditional dependence with the cumulative reward variable \(R_{t}=_{t^{}=t}^{t^{}-t}r_{t^{}}\), which has also been shown in .

**Proposition 1**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s_{t}^{r}}\) is a minimal subset of state dimensions that are sufficient for policy learning, and \(s_{i,t} s_{t}^{r}\) if and only if \(s_{i,t} R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\)._

Moreover, the proposition below shows that \(s_{t}^{a}\), that has a directed edge from \(a_{t-1}\), can be directly controlled by actions and can be characterized by conditional dependence with the action variable.

**Proposition 2**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, \(s_{t}^{a}}\) is a minimal subset of state dimensions that are sufficient for direct control, and \(s_{i,t} s_{t}^{a}\) if and only if \(s_{i,t} a_{t-1}|}\)._

Furthermore, based on Proposition 1 and Proposition 2, we can further differentiate \(s_{t}^{ar},s_{t}^{ar}\), \(s_{t}^{ar}\) from \(s_{t}^{r}\) and \(s_{t}^{a}\), which is given in the following proposition.

**Proposition 3**.: _Under the assumption that the graphical representation, corresponding to the environment model, is Markov and faithful to the measured data, we can build a connection between the graph structure and statistical independence of causal variables in the RL system, with (1) \(s_{i,t} s_{t}^{ar}\) if and only if \(s_{i,t} R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t} a_{t-1}|}\), (2) \(s_{i,t} s_{t}^{ar}\) if and only if \(s_{i,t} R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\), and \(s_{i,t}_{t-1}|}\), (3) \(s_{i,t} s_{t}^{ar}\) if and only if \(s_{i,t} R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t}_{t-1}|}\), and (4) \(s_{i,t} s_{t}^{ar}\) if and only if \(s_{i,t} R_{i}|a_{t-1:t}\), \(s_{t-1}^{r}\) and \(s_{i,t}_{t-1}|}\)._

By identifying these four categories of latent state variables, we can achieve interpretable input for policies, including (i)reward-relevant and directly controllable parts, (ii) reward-relevant but not controllable parts, (iii) controllable but non-reward-relevant parts, and (iv) unrelated noise. Furthermore, policy training will become more sample efficient and robust to task-irrelevant changes by utilizing only \(s_{t}^{ar}\) and \(s_{t}^{ar}\) for policy learning. We show the block-wise identifiability of the four categories of variables in the next section.

Identifiability Theory

Identifying causally-related latent variables from observations is particularly challenging, as latent variables are generally not uniquely recoverable [16; 17]. Previous work in causal representation learning and nonlinear-ICA has established identifiability conditions for non-parametric temporally latent causal processes [18; 19]. However, these conditions are often too stringent in reality, and none of these methods have been applied to complex control tasks. In contrast to component-wise identifiability, our focus lies on the block-wise identifiability of the four categories of latent state variables. From a policy optimization standpoint, this is sufficient because we do not need the latent variables to be recovered up to permutation and component-wise invertible nonlinearities. This relaxation makes the conditions more likely to hold true and applicable to a wide range of RL tasks. We provide proof of the block-wise identifiability of the four types of latent variables. To the best of our knowledge, we are the first to prove the identifiability of disentangled latent state variables in general nonlinear cases for RL tasks.

According to the causal process in the RL system (as described in Eq.1 in ), we can build the following mapping from latent state variables \(_{t}\) to observed variables \(o_{t}\) and future cumulative reward \(R_{t}\):

\[[o_{t},R_{t}]=f(s^{r}_{t},s^{}_{t},_{t}),\] (1)

where

\[o_{t}&=&f_{1}(s^{r}_{t},s^{r}_{t}),\\ R_{t}&=&f_{2}(s^{r}_{t},_{t}).\] (2)

Here, note that to recover \(s^{r}_{t}\), it is essential to take into account all future rewards \(r_{t:T}\), because any state dimension \(s_{t:t}_{t}\) that has a directed path to the future reward \(r_{t+}\), for \(>0\), is involved in \(s^{r}_{t}\). Hence, we consider the mapping from \(s^{r}_{t}\) to the future cumulative reward \(R_{t}\), and \(_{t}\) represents residuals, except \(s^{r}_{t}\), that have an effect to \(R_{t}\).

Below, we first provide the definition of blockwise identifiability and relevant notations, related to [20; 21; 22; 23].

**Definition 1** (Blockwise Identifiability).: _A latent variable \(_{t}\) is blockwise identifiable if there exists a one-to-one mapping \(h()\) between \(_{t}\) and the estimated \(}_{t}\), i.e., \(}_{t}=h(_{t})\)._

Notations.We denote by \(}_{t}:=(s^{r}_{t},s^{}_{t},_{t})\) and by \(|s|\) the dimension of a variable \(s\). We further denote \(d_{s_{t}}:=|s^{r}_{t}|\), \(d_{s_{t}}:=|s^{r}_{t}|\), \(d_{s}:=|_{t}|\), \(d_{o}:=|o_{t}|\), and \(d_{R}:=|R_{t}|\). We denote by \(\) the support of Jacobian \(_{j}(_{t})\), by \(}\) the support of \(_{j}(_{t})\), and by \(\) the support of \((_{t})\) with \(_{j}(_{t})=_{j}(_{t})( )\). We also denote \(T\) as a matrix with the same support as \(\).

In addition, given a subset \(\{1,,d_{i}\}\), the subspace \(_{}^{d_{i}}\) is defined as \(_{}^{d_{i}}:=\{z^{d_{i}}|i  z_{i}=0\}\). In other words, \(_{}^{d_{i}}\) refers to the subspace of \(^{d_{i}}\) indicated by an index set \(\).

We next show that the different types of states \(s^{ar}_{t}\), \(s^{}_{t}\), \(s^{}_{t}\), and \(s^{}_{t}\) are blockwise identifiable from observed image variable \(o_{t}\), reward variable \(r_{t}\), and action variable \(a_{t}\), under reasonable and weak assumptions, which is partly inspired by [20; 21; 22; 23].

**Theorem 1**.: _Suppose that the causal process in the RL system and the four categories of latent state variables can be described as that in Section 2 and illustrated in Figure 1(c). Under the following assumptions_

1. _The mapping_ \(f\) _in Eq._ 1 _is smooth and invertible with smooth inverse._
2. _For all_ \(i\{1,,d_{o}+d_{R}\}\) _and_ \(j_{i,}\)_, there exist_ \(\{_{t}^{(i)}\}_{t=1}^{|_{i,}|}\)_, so that_ \(\{_{j}(_{t}^{(i)})_{i,}\}_{t=1}^{|_{i,}|}=_{_{i,}}^{d_{i}}\)_, and there exists a matrix_ \(T\) _with its support identical to that of_ \(_{j}^{-1}(}_{t})_{j}(_{t})\)_, so that_ \([_{j}(_{t}^{(l)})T]_{j,}_{ _{i,}}^{d_{i}}\)_._

_Then, reward-relevant and controllable states \(s^{ar}_{t}\), reward-relevant but not controllable states \(s^{}_{t}\), reward-irrelevant but controllable states \(s^{}_{t}\), and noise \(s^{}_{t}\), are blockwise identifiable._

In the theorem presented above, Assumption A1 only assumes the invertibility of function \(f\), while functions \(f_{1}\) and \(f_{2}\) are considered general and not necessarily invertible, as that in . Since the function \(f\) is the mapping from all (latent) variables, including noise factors, that influence the observed variables, the invertibility assumption holds reasonably. However, note that it is not reasonable to assume the invertibility of the function \(f_{2}\) since usually, the reward function is not invertible. Intuitively, Assumption A2 requires that the Jacobian varies "enough" so that it cannot be contained in a proper subspace of \(_{_{i}}^{d_{_{i}}}\). This requirement is necessary to avoid undesirable situations where the problem becomes ill-posed and is essential for identifiability. A special case when this property does not hold is when the function \(f\) is linear, as the Jacobian remains constant in such cases. Some proof techniques of Theorem 1 follow from [20; 22; 23], with the detailed proof given in Appendix A.4.

## 4 World Model with Disentangled Latent Dynamics

Based on the four categories of latent state variables, we formulate a world model with disentangled latent dynamics. Each component of the world model is described as follows:

\[\{&p_{}(o_{t} })\\ &p_{}(r_{t} s_{t}^{})\\ &p_{}(}},a_{ t-1})\\ &q_{}(} o_{t},},a_{ t-1}).\] (3)

Specifically, the world model includes three generative models: an observation model, a reward function, and a transition model (prior), and a representation model (posterior). The observation model and reward model are parameterized by \(\). The transition model is parameterized by \(\) and the representation model is parameterized by \(\). We assume noises are i.i.d for all models. The action \(a_{t-1}\) has a direct effect on the latent states \(}\), but not on the perceived signals \(o_{t}\). The perceived signals, \(o_{t}\), are generated from the underlying states \(}\), while signals \(r_{t}\) are generated only from reward-relevant latent variables \(s_{t}^{}\).

According to the graphical model depicted in Figure 1(c), the transition model and the representation model can be further decomposed into four distinct sub-models, according to the four categories of latent state variables, as shown in equation 4.

\[\{&\\ \{p_{_{1}}(s_{t}^{} s_{t-1}^{},a_{ t-1})\\ p_{_{2}}(s_{t}^{} s_{t-1}^{})\\ p_{_{3}}(s_{t}^{}},a_{t-1})\\ p_{_{4}}(s_{t}^{}}). \{q_{_{1}}(s_{t}^{} o_{t},s_{t-1}^ {},a_{t-1})\\ q_{_{2}}(s_{t}^{} o_{t},s_{t-1}^{})\\ q_{_{3}}(s_{t}^{} o_{t},s_{t-1})\\ q_{_{4}}(s_{t}^{} o_{t},s_{t-1}).\] (4)

Specifically, we have \(p_{}=p_{_{1}} p_{_{2}} p_{_{3}} p_{ _{4}}\) and \(q_{}=q_{_{1}} q_{_{2}} q_{_{3}} q_{_{4}}\). Note this factorization differs from previous works [11; 9] that assume independent latent processes. Instead, we only assume conditional independence among the four categories of latent variables given \(}\), which provides a more general factorization. In particular, the dynamics of \(s_{t}^{}\) and \(s_{t}^{}\) are dependent on \(s_{t-1}^{r}\), ensuring that they are unaffected by any reward-irrelevant latent variables present in \(s_{t-1}^{j}\). On the other hand, \(s_{t}^{}\) may be influenced by all the latent variables from the previous time step. Note that the connections between \(}\) and \(s_{t}^{}\) can be adjusted based on the concrete problem. Since \(s_{t}^{}\) and \(s_{t}^{}\) are controllable variables, their determination also relies on \(a_{t-1}\).

### World Model Estimation

The optimization of the world model involves joint optimization of its four components to maximize the variational lower bound  or, more generally, the variational information bottleneck [25; 26]. The bound encompasses reconstruction terms for both observations and rewards, along with a KL regularizer:

\[_{}^{t}= p_{}(o_{t}} )_{}^{t}= p_{}(r_{t} s_{t}^{ })_{}^{t}=-(q_{} \|p_{}).\] (5)

Further, the KL regularizer \(_{}^{t}\) can be decomposed into four components based on our factorization of the state variables. We introduce additional hyperparameters to regulate the amount of information contained within each category of variables:

\[_{}^{t}=-_{1}(q_{_{1}}\|p_{ _{1}})-_{2}(q_{_{2}}\|p_{_{2} })-_{3}(q_{_{1}}\|p_{_{1}})- _{4}(q_{_{4}}\|p_{_{4}}).\] (6)

Additionally, we introduce two supplementary objectives to explicitly capture the distinctive characteristics of the four distinct representation categories. Specifically, we characterize the reward relevant representations by measuring the dependence between \(s^{r}_{t}\) and \(R_{t}\), given \(a_{t-1:t}\) and \(s^{r}_{t-1}\), that is \(I(s^{r}_{t},R_{t} a_{t-1:t},s^{r}_{t-1})\). To ensure that \(s^{r}_{t}\) are minimally sufficient for policy training, we maximize \(I(s^{r}_{t},R_{t} a_{t-1:t},s^{r}_{t-1})\) while minimizing \(I(s^{r}_{t},R_{t} a_{t-1:t},s^{r}_{t-1})\) to discourage the inclusion of redundant information in \(s^{r}_{t}\) concerning the rewards:

\[I(s^{r}_{t};R_{t} a_{t-1:t},s^{r}_{t-1})-I(s^{r}_{t};R_{t} a_{t-1:t},s^ {r}_{t-1}).\] (7)

The conditional mutual information can be expressed as the disparity between two mutual information.

\[& I(s^{r}_{t};R_{t} a_{t-1:t},s^{r}_{t-1})=I(R_{t} ;s^{r}_{t},a_{t-1:t},s^{r}_{t-1})-I(R_{t};a_{t-1:t},s^{r}_{t-1}),\\ & I(s^{r}_{t};R_{t} a_{t-1:t},s^{r}_{t-1})=I(R_{t};s^{r}_{t}, a_{t-1:t},s^{r}_{t-1})-I(R_{t};a_{t-1:t},s^{r}_{t-1}).\] (8)

After removing the common term, we leverage mutual information neural estimation  to approximate the value of mutual information. Thus, we reframe the objective in Eq.7 as follows:

\[^{t}_{}=_{1}\{I_{a_{1}}(R_{t};s^{r}_{t},a_{t -1:t},(s^{r}_{t-1}))-I_{a_{2}}(R_{t};s^{r}_{t},a_{t-1:t},(s^{r}_{t-1}))\}.\] (9)

We employ additional neural networks, parameterized by \(\), to estimate the mutual information. To incorporate the conditions from the original objective, we apply the stop_gradient operation \(\) to the variable \(s^{r}_{t-1}\). Similarly, to ensure that the representations \(s^{a}_{t}\) are directly controllable by actions, while \(s^{}_{t}\) are not, we maximize the following objective:

\[^{t}_{}=_{2}[I_{a_{3}}(a_{t-1};s^{a}_{t}, (}))-I_{a_{4}}(a_{t-1};s^{}_{t},(}))].\] (10)

Intuitively, these two objective functions ensure that \(s^{r}_{t}\) is predictive of the reward, while \(s^{r}_{t}\) is not; similarly, \(s^{a}_{t}\) can be predicted by the action, whereas \(s^{}_{t}\) cannot. The total objective for learning the world model is summarized as:

\[_{}=_{q_{}}(_{t}( ^{t}_{}+^{t}_{}+^{t}_{}+^{t}_{}+^{t}_{}))+\.\] (11)

The expectation is computed over the dataset and the representation model. Throughout the model learning process, the objectives for estimating mutual information and learning the world model are alternately optimized. A thorough derivation and discussion of the objective function are given in Appendix B.

It is important to note that our approach to learning world models with identifiable factorization is independent of the specific policy optimization algorithm employed. In this work, we choose to build upon the state-of-the-art method Dreamer [6; 7], which iteratively performs exploration, model-fitting, and policy optimization. As illustrated in Figure 2, the learning process for the dynamics involves the joint training of the four categories of latent variables. However, only the variables of \(s^{r}_{t}\) are utilized for policy learning. We provide the pseudocode for the IFactor algorithm in Appendix C.

Figure 2: (a) The agent learns the disentangled latent dynamics from prior experiences. The yellow arrow represents a one-one mapping from \(h^{*}_{t}\) to \(s^{*}_{t}\) with the same superscript. (b) Within the latent space, state values and actions are forecasted to maximize future value predictions by backpropagating gradients through imagined trajectories. Only \(s^{r}_{t}\) (reward-relevant) are used for policy optimization.

Experiments

In this section, we begin by evaluating the identifiability of our method using simulated datasets. Subsequently, we visualize the learned representations of the four categories in a cartpole environment that includes distractors. Further, we assess the advantages of our factored world model in policy learning by conducting experiments on variants of Robodesk and DeepMind Control Suite. The reported results are aggregated over 5 runs for the Robodesk experiments and over 3 runs for others. A detailed introduction to each environment is provided in Appendix D, while experiment details and hyperparameters can be found in Appendix E. The source code is available at https://github.com/AlexLiuyuren/IFactor

### Latent State Identification Evaluation

#### 5.1.1 Synthetic environments

Evaluation Metrics and BaselinesWe generate synthetic datasets that satisfy the identifiability conditions outlined in the theorems (see Appendix D.1). To evaluate the block-wise identifiability of the four categories of representations, we follow the experimental methodology of  and compute the coefficient of determination \(R^{2}\) from \(s^{*}_{t}\) to \(^{*}_{t}\), as well as from \(^{*}_{t}\) to \(s^{*}_{t}\), for \(*[ar,r,a,]\). \(R^{2}\) can be viewed as the identifiability score. \(R^{2}=1\) in both directions suggests a one-to-one mapping between the true latent variables and the recovered ones. We compare our latent representation learning method with Denoised MDP . We also include baselines along the line of nonlinear ICA: BetaVAE  and FactorVAE  which do not consider temporal dependencies; SlowVAE  and PCL which leverage temporal constraints but assume independent sources; and TDRL  which incorporates temporal constraints and non-stationary noise but does not utilize the action and reward information in the Markov Decision Process.

ResultsFigure 9 demonstrates the effectiveness of our method in accurately recovering the four categories of latent state variables, as evidenced by high \(R^{2}\) values (\(>0.9\)). In contrast, the baseline methods exhibit distortions in the identification results. Particularly, Denoised MDP assumes independent latent process for \(x_{t},y_{t}\), and the existence of instantaneous causal effect from \(x_{t},y_{t}\) to \(z_{t}\) (see Figure 1(d)), leading to unidentifiable latent variables without further intervention. BetaVAE, FactorVAE, SlowVAE, and PCL, which assume independent sources, show subbar performance. Although TDRL allows for causally-related processes under conditional independence assumptions, it fails to explicitly leverage the distinct transition structures of the four variable categories. More results on the identifiability scores of baselines are provided in Appendix E.1.

#### 5.1.2 Modified Cartpole with distractors

We have introduced a variant of the original Cartpole environment by incorporating two distractors. The first distractor is an uncontrollable Cartpole located in the upper portion of the image, which is irrelevant to the rewards. The second distractor is a controllable but reward-irrelevant green light positioned below the reward-relevant Cartpole in the lower part of the image.

Figure 3: Simulation results: (a) The coefficient of determination (\(R^{2}\)) obtained by using kernel ridge regression to regress estimated latents on true latents.(b) The \(R^{2}\) obtained by using kernel ridge regression  to regress true latents on estimated latents. (c) The average \(R^{2}\) over four types of representations during training (True latents \(\) Estimated latents).(d) The average \(R^{2}\) over four types of representations during training (Estimated latents \(\) True latents).

After estimating the representation model, we utilize latent traversal to visualize the four categories of representations in the modified Cartpole environment (see Figure 4). Specifically, the recovered \(s^{ar}_{t}\) corresponds to the position of the cart, while \(s^{ar}_{t}\) corresponds to the angle of the pole. This demonstrates the ability of our method to automatically factorize reward-relevant representations based on (one-step) controllability: the cart experiences a force at the current time step, whereas the pole angle is influenced in the subsequent time step. Note that during the latent traversal of \(s^{ar}_{t}\), the position of the cart remains fixed, even though the pole originally moves with the cart in the video. Additionally, \(s^{ar}_{t}\) corresponds to greenness of the light, and \(s^{ar}_{t}\) corresponds to the uncontrollable and reward-irrelevant cartpole. These findings highlight the capability of our representation learning method to disentangle and accurately recover the ground-true latent variables from videos. The identifiability scores of four categories of representations are provided in Appendix E.2.

### Policy Optimization Evaluation

To assess the effectiveness of our method in enhancing policy learning by utilizing minimal yet sufficient representations for control, we conducted experiments on more complex control tasks. One of these tasks is a variant of Robodesk , which includes realistic noise elements such as flickering lights, shaky cameras, and a dynamic video background. In this task, the objective for the agent is to change the hue of a TV screen to green using a button press. We also consider variants of DeepMind Control Suite (DMC) [35; 9], where distractors such as a dynamic video background, noisy sensor readings, and a jittering camera are introduced to the original DMC environment. These additional elements aim to challenge the agent's ability to focus on the relevant aspects of the task while filtering out irrelevant distractions. Baseline results except for DreamerPro  are derived from the Denoised MDP paper. We have omitted the standard deviation of their performance for clarity.

Evaluation Metrics and BaselinesWe evaluate the performance of the policy at intervals of every 10,000 environment steps and compare our method with both model-based and model-free approaches. Among the model-based methods, we include Denoised MDP , which is the state-of-the-art method for variants of Robodesk and DMC. We also include DreamerPro , TIA  and Dreamer  as additional model-based baselines. For the model-free methods, we include DBC , CURL, and PI-SAC . To ensure a fair comparison, we have aligned all common hyperparameters and neural network structures with those used in the Denoised MDP .

#### 5.2.1 RoboDesk with Various Noise Distractors

In Figure 5, the left image demonstrates that our model IFactor achieves comparable performance to Denoised MDP while outperforming other baselines. The results of baselines except for DreamerPro are directly copied from the paper of Denoised MDP  (its error bar cannot be directly copied), and the replication of Denoised MDP results is given in Appendix E.3. Furthermore, to investigate whether \(s^{r}_{t}\) serves as minimal and sufficient representations for policy learning, we retrain policies using the Soft Actor-Critic algorithm  with different combinations of the four learned categories as input. Remarkably, the policy trained using \(s^{r}_{t}\) exhibits the best performance, while the performance of other policies degrades due to insufficient information (e.g., \(s^{ar}_{t}\) and \(_{t}}\)) or redundant information

Figure 4: Latent traversal for four types of representations in the modified Cartpole environment.

(e.g., \(s_{t}^{r}+s_{t}^{a^{}}\) and \(}\)). Moreover, we conduct latent traversal on the learned representations to elucidate their original meaning in the observation. Interestingly, our model identifies the greenness of the screen as \(s_{t}^{a^{}}\), which may initially appear counter-intuitive. However, this phenomenon arises from the robot arm pressing the green button, resulting in the screen turning green. Consequently, the screen turning green becomes independent of rewards conditioned on the robot arm pressing the green button, aligning with the definition of \(s_{t}^{a^{}}\). This empirically confirms the minimality of \(s_{t}^{r}\) for policy optimization.

#### 5.2.2 DeepMind Control Suite (DMC)

Figure 6 demonstrates the consistent superiority of our method over baselines in the Deepmind Control Suite (DMC) across various distractor scenarios. Our approach exhibits robust performance in both noiseless and noisy environments, demonstrating the effectiveness of our factored world model in eliminating distractors while preserving essential information for effective and efficient control in policy learning. The ablation study of IFactor's objective function terms is presented in Appendix B.1 and Appendix E.3, which shows that the inclusion of mutual information constraints is essential to promote disentanglement and enhance policy performance. The policy performance that includes the standard error is provided in Appendix E.4. Visualization of the learned representations is also provided in Appendix E.5.

## 6 Related Work

World Model Learning in RL.Image reconstruction [41; 42] and contrastive learning [43; 44; 45; 46; 47] are wildly used to learn representations in RL. Dreamer and its subsequent extensions [5; 6; 7; 8] adopt a world model-based learning approach, where the policy is learned solely through dreaming in the world model. However, the agents trained by these techniques tend to underperform in noisy environments . To address this issue, numerous approaches have been introduced to enhance robustness to distractions. Task Informed Abstractions (TIA)  explicitly partition the latent state space into reward-relevant and reward-irrelevant features. Denoised MDP  takes a step further and decomposes the reward-relevant states into controllable and uncontrollable components. InfoPower  prioritizes information that is correlated with action based on mutual information. Iso-Dream  learns controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Our method extends these work by accommodating a more general factorization with block-wise identifiablity. Recent research also explores reconstruction-free representation learning methods [36; 49; 50; 51]. DreamerPro  combines Dreamer with prototypes, which distills temporal structures from past observations and actions. Temporal Predictive Coding  encodes elements in the environment that can be predicted across time. Contratively-trained Structured World Models  utilize graph neural networks and a contrastive approach for representation learning in environments with compositional structure. Latent states learned by these methods are not identifiable due to the reconstruction-free property, where the mixing function is not invertible anymore. A detailed comparison between IFactor and related work is also given in Appendix F.

Identifiability in Causal Representation Learning.Temporal structure and nonstationarities were recently used to achieve identifiability in causal representation learning . Methods such as TCL

Figure 5: Results on Robodesk. We have omitted the standard deviation of the performance of baselines for clarity. The left image shows that our method IFactor achieves comparable performance with Denoised Mdp, outperforming other baselines. The middle image shows the policy optimization process by SAC  using representations learned by IFactor, where policies that use \(s_{t}^{r}\) as input achieve the best performance. The right image shows the latent traversal of the representations.

 and PCL  leverage nonstationarity in temporal data through contrastive learning to identify independent sources. CITRIS  and iCITRIS  takes advantage of observed intervention targets to identify causal factors. LEAP  introduces identifiability conditions for both parametric and nonparametric latent processes. TDRL  explores identifiability of latent processes under stationary environments and distribution shifts. ASRs  establish the identifiability of the representations in the linear-gaussian case in the RL setting. Our work extends the theoretical results in ASRs to enable block-wise identifiability of four categories of latent variables in general nonlinear cases.

## 7 Conclusion

In this paper, we present a general framework to model four distinct categories of latent state variables within the RL system, based on their interactions with actions and rewards. We establish the block-wise identifiability of these latent categories in general nonlinear cases, under weak and realistic assumptions. Accordingly, we propose IFactor to extract four types of representations from raw observations and use reward-relevant representations for policy optimization. Experiments verify our theoretical results and show that our method achieves state-of-the-art performance in variants of the DeepMind Control Suite and RoboDesk. The basic limitation of this work is that the underlying latent processes are assumed to have no instantaneous causal relations but only time-delayed influences. This assumption does not hold true if the resolution of the time series is much lower than the causal frequency. We leave the identifiability of the latent dynamics in the presence of instantaneous causal and the extension of our method to heterogeneous environments to future work.

## 8 Acknowledgements

This work is supported by National Key Research and Development Program of China (2020AAA0107200), the Major Key Project of PCL (PCL2021A12), China Scholarship Council, NSF Grant 2229881, the National Institutes of Health (NIH) under Contract R01HL159805, a grant from Apple Inc., a grant from KDDI Research Inc., and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research. We thank Guangyi Chen, Weiran Yao and the anonymous reviewers for their support and helpful discussions on improving the paper.

Figure 6: Policy optimization on variants of DMC with various distractors.