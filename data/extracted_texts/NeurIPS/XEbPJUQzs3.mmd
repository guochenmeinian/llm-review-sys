# Prospective Learning: Learning for a Dynamic Future

Ashwin De Silva\({}^{*,1}\)

&Rahul Ramesh\({}^{*,2}\)

&Rubing Yang\({}^{*,2}\)

Siyu Yu\({}^{1}\)&Joshua T. Vogelstein\({}^{,1}\)&Pratik Chaudhari\({}^{,2}\)

\({}^{*,}\) Equal Contribution

Email: {ldesilv2, syu80, jovo}@jhu.edu, {rahulram, rubingy, pratikac}@upenn.edu

###### Abstract

In real-world applications, the distribution of the data, and our goals, evolve over time. The prevailing theoretical framework for studying machine learning, namely probably approximately correct (PAC) learning, largely ignores time. As a consequence, existing strategies to address the dynamic nature of data and goals exhibit poor real-world performance. This paper develops a theoretical framework called "Prospective Learning" that is tailored for situations when the optimal hypothesis changes over time. In PAC learning, empirical risk minimization (ERM) is known to be consistent. We develop a learner called Prospective ERM, which returns a sequence of predictors that make predictions on future data. We prove that the risk of prospective ERM converges to the Bayes risk under certain assumptions on the stochastic process generating the data. Prospective ERM, roughly speaking, incorporates time as an input in addition to the data. We show that standard ERM as done in PAC learning, without incorporating time, can result in failure to learn when distributions are dynamic. Numerical experiments illustrate that prospective ERM can learn synthetic and visual recognition problems constructed from MNIST and CIFAR-10. Code at https://github.com/neurodata/prolearn.

## 1 Introduction

All learning is for the future. Learning involves updating decision rules or policies, based on past experiences, to improve future performance. Probably approximately correct (PAC) learning has been extremely useful to develop algorithms that minimize the risk--typically defined as the expected loss--on unseen samples under certain assumptions. The assumption, that samples are independent and identically distributed (IID) within the training dataset and at test time, has served us well. But it is neither testable nor believed to be true in practice. The future is always different from the past: both distributions of data and goals of the learner may change over time. Moreover, those changes may cause the optimal hypothesis to change over time as well. There are numerous mathematical and empirical approaches that have been developed to address this issue, e.g., techniques for being invariant to , or adapting to, distribution shift , modeling the future as a different task, etc. But we lack a first-principles framework to address problems where data distributions and goals may change over time in such a way that the optimal hypothesis is time-dependent. And as a consequence, machine learning-based AI today is brittle to changes in distribution and goals.

This paper develops a theoretical framework called "Prospective Learning" (PL). Instead of data arising from an unknown probability distribution like in PAC learning, prospective learning assumes that data comes from an unknown stochastic process, that the loss considers the future, and that the optimal hypothesis may change over time. A prospective learner uses samples received up to some time \(t\) to output an infinite sequence of predictors, which it uses for making predictions on data at all future times \(t^{}>t\). We discuss how prospective learning is related to existing problem formulations in the literature in Section 3 and Appendix A.

Why should one care about prospective learning?Imagine a deployed machine learning system. The designer of this system desires to optimize--not the risk upon the past training data, or the risk 38th Conference on Neural Information Processing Systems (NeurIPS 2024).

on the immediate future data--but the risk on all data that the model will make predictions upon in the future. As data evolves, e.g., due to changing trends and preferences of the users, the optimal hypothesis to make predictions also changes. Time is the critical piece of information if the system designer is to achieve their goals. Both in the sense of how far back in time a particular datum was recorded, and in the sense of how far ahead in the future this system will be used to make predictions. The designer must take time into account to avoid retraining the model periodically, _ad infinitum_.

Biology is also rich with examples where systems seem to behave prospectively. The principle of allostasis, for example, states that regulatory processes of living things anticipate the needs of the organism and prepare to satisfy these needs before, rather than after, they arise . For example, mitochondria increase their energy production to anticipate the demands of muscles , neural circuits anticipate changes in sensory stimuli and the task (i.e., predictive coding ), and individual organisms optimize their actions with respect to anticipated changes in their environments [6; 7]. These regulatory principles were learned early in evolutionary time so they must be important. In short, the world--including our internal drives--changes all the time, and learning systems must anticipate (that is, prospect) these changes to thrive.

**Contributions**

* Section 2 defines Prospective Learning (PL) as an approach to address problems where the optimal hypothesis may evolve over time (due to shifts in distributions and/or goals of the learner). It also provides illustrative examples of PL.
* Section 3 and Appendix A put prospective learning in context relative to existing ideas in the literature to address changes in the data distribution.
* Section 4 takes steps towards a theoretical foundation for prospective learning. We define strongly learnability (i.e., there exists a prospective learner whose risk is arbitrarily close to the Bayes optimal learner) and weakly learnability (i.e., there exists a prospective learner whose risk is better than chance) . Empirical risk minimization (ERM) without incorporating time, can result in failure to strongly, or even weakly, learn prospectively .
* Section 5 introduces prospective empirical risk minimization, and proves that it can learn prospectively under certain assumptions on the stochastic process and loss.
* Section 6 demonstrates that our prospective learners can prospectively learn several canonical problems constructed using synthetic, MNIST  and CIFAR-10  data. In contrast, a number of existing algorithms, including ERM, online and continual learning algorithms, fail. Appendix H demonstrates that current large language models, which use Transformer-based architectures trained using auto-regressive losses, fail to learn prospectively.

## 2 A definition of prospective learning

A prospective learner minimizes the expected cumulative risk of the future using past data. Such a learner is defined by the following key ingredients (see Fig. 1 (left) for schematic illustration).

**Data.** Let the input and output at time \(t\) be denoted by \(x_{t}\) and \(y_{t}\) respectively. Let \(z_{t}=(x_{t},y_{t})\). We will model the data as a stochastic process \(Z(Z_{t})_{t}\) defined on an appropriate probability space \((,,)\). At time \(t\), denote past data by \(z_{ t}(z_{1},,z_{t})\) and future data by \(z_{>t}(z_{t+1},)\). We will find it useful to distinguish between the realization of the data, denoted by \(z_{ t}\), and the corresponding random variable, \(Z_{ t}\).

**Hypothesis class.** At each time \(t\), a prospective learner selects an infinite sequence \(h(h_{1},,h_{t},h_{t+1},)\) which it uses to make predictions on data at any time in the future. Each element of this sequence \(h_{t}:\) and therefore \(h_{t}^{}\).1 The hypothesis class \(\) is the space of such hypotheses, \(h(^{})^{}\).2 We will again use the shorthand \(h_{ t}(h_{1},,h_{t})\). We will sometimes talk about a "time-agnostic hypothesis" which will refer to a hypothesis such that \(h_{t}=h_{t^{}}\) for all \(t,t^{}\). Observe that this makes our setup different from the standard setup in PAC learning where the learner selects a single hypothesis in \(^{}\). One could also think of prospective learning as using a single time-varying hypothesis \(h:\), i.e., the hypothesis takes both time and the datum as input to make a prediction.

**Learner.** A prospective learner is a map from the data received up to time \(t\), to a hypothesis that makes predictions on the data over all time (past and future): \(()^{t}(^{})^{ }\). The learner givesas output a hypothesis \(h(z_{ t})\). Unlike a PAC learner, a prospective learner can make different kinds of predictions at different times. This is a crucial property of prospective learning. In other words, after receiving data up to time \(t\), the hypothesis selected by the prospective learner can predict on samples at any future time \(t^{}>t\).

**Prospective loss and risk.** The future loss incurred by a hypothesis \(h\) is

\[_{t}(h,Z)=_{}_{s=t+1}^{t+} (s,h_{s}(X_{s}),Y_{s}),\] (1)

where \(:\) is a bounded loss function.3 Prospective risk at time \(t\) is the expected future loss 4

\[R_{t}(h)=[_{t}(h,Z) z_{ t}]=_{t}(h,Z)\,\,_{Z z_{ t}}\,,\] (2)

where we assume that \(h\) is a random variable and \(h(Z_{ t})\) where \(()\) denotes the filtration (an increasing sequence of sigma algebras) of the stochastic process \(Z\). We have used the shorthand \([Y x]\) for \([Y X=x]\). Observe that we have conditioned the prospective risk of the hypothesis \(h\) upon the realized data \(z_{ t}\). We can take an expectation over the realized data, to obtain the expected prospective risk

\[[R_{t}(h)]= R_{t}(h)\,\,_{Z_{  t}}\,.\]

**Prospective Bayes risk** is the minimum risk achievable by any hypothesis. In PAC learning, it is a constant that depends upon the (fixed) true distribution of the data and the risk function. In prospective learning, the optimal hypothesis can predict differently at different times. We therefore define the prospective Bayes risk at a time \(t\) as

\[R_{t}^{}=_{h(Z_{ t})}R_{t}(h),\] (3)

which is the minimum achievable prospective risk by any learner that observes data \(z_{ t}\). We define the Bayes optimal learner as any learner that achieves a Bayes optimal risk at every time \(t\). In certain contexts, one might be interested in the limiting prospective Bayes risk as \(t\).

### Different prospective learning scenarios with illustrative examples

We next discuss four prospective learning scenarios that are relevant to increasingly more general classes of stochastic processes. Our goal is to illustrate, using examples, how the definitions developed in the previous section capture these scenarios. We will assume that for all times \(t\) we have \(X_{t}=1\), \(Y_{t}\{0,1\}\). We will also focus on the time-invariant zero-one loss \((t,,y)=( y)\) for all \(t\), here \(\) is the Dirac delta function. Fig. 1 shows example realizations of the data for each scenario.

**Scenario 1 (Data is independent and identically distributed).** Formally, this consists of stochastic processes where \(_{Z_{} Z_{ t}}=_{Z_{t}}\) for all \(t,t^{}\). As an example, consider \(Y_{t}(p)\) for some unknown parameter \(p\). Prospective Bayes risk is equal to \((p,1-p)\) in this case. A time-agnostic hypothesis, for example one that thresholds the maximum likelihood estimator (MLE) of the Bernoulli probability, converges to the limiting prospective Bayes risk.5

**Scenario 2 (Data is independent but not identically distributed).** This consists of stochastic processes where \(_{Z_{} Z_{ t}}=_{Z_{t}}\) for all \(t\). Consider \(Y_{t}(p)\) if \(t\) is odd, and \(Y_{t}(1-p)\) if \(t\) is even, i.e., data is drawn from two different distributions at alternate times. Prospective Bayes risk is again equal to \((1-p,p)\) in this case. A time-agnostic hypothesis can only perform at chance level. But a prospective learner, for example one that selects a hypothesis that alternates between two predictors at even and odd times, can converge to prospective Bayes optimal risk. We can also construct variants, e.g., when the relationship between the Bernoulli probabilities are not known (Variant 1 in Fig. 1), or when the learner does not know that the data distribution changes at every time step (Variant 2 in Fig. 1 where we implemented a generalized likelihood ratio test to determine whether the distribution changes). The risk of these variants also converges to prospective Bayes risk, but they need more samples because they use more generic models of the stochastic process. This scenario is closely related to (online) multitask/meta-learning .

**Scenario 3** (**Data is neither independent nor identically distributed**).: Formally, this scenario consists of general stochastic processes. As an example, consider a Markov process \((Y_{t+1}=k Y_{t}=k)=\) with two states \(k\{0,1\}\) and \(Y_{1}()\). The invariant distribution of this Markov process is \((0)=(1)=1/2\). Prospective Bayes risk is also equal to 1/2. For stochastic processes that have a invariant distribution, it is impossible to predict the next state infinitely far into the future and therefore it is impossible to prospect. The prospective Bayes risk is trivially chance levels. In such situations, the learner could consider losses that are discounted over time. For example, one could use a slightly different loss than the one in Eq. (1) to write

\[_{t}(h,Z)=(1-)_{s=t+1}^{}^{s-t-1}(h_{s}(X_ {s}),Y_{s})\] (4)

for some \([0,1)\). In this example, we can calculate the prospective Bayes risk analytically; see Appendix B.2. For \(=0.9\), \(=0.1\) and the zero-one loss, limiting prospective Bayes risk is \(0.357\). Now consider a learner which computes the MLE of the transition matrix \(_{t}^{t^{}-t}\). It calculates \((Y_{t^{}} y_{t})=_{t^{}}\) where \([1-_{t^{}},_{t^{}}]=_{t}^{t^{}-t}[1-y_{ t},y_{t}]^{}\) and uses the hypothesis \(h_{t^{}}(X_{t^{}})=(_{t^{}}>0.5)\) (ties broken randomly). We can see in Fig. 1 that this learner converges to the prospective Bayes risk. This example shows that if we model the changes in the data, then we can perform prospective learning. This scenario is closely related to certain continual learning problems [13; 14].

**Scenario 4** (**Future depends upon the current prediction**).: Problemisms where predictions of the learner affect future data are an interesting special case of Scenario 3. Prospective learning can also be used to address such scenarios. For \(_{0},_{1}\), consider a Markov decision process (MDP) \((Y_{t+1}=j Y_{t}=j^{},h_{t+1}(1)=k)=_{k}\) if \(j=j^{}\) and \(1-_{k}\) otherwise. I.e., the prediction \(h_{t+1}(X_{t+1})=k\) (recall that \(X_{t}=1\) for all times) is the decision and the MDP remains in the same state with probability \(_{k}\). Prospective Bayes risk for this example is the same as that of the example in Scenario 3. We can construct a prospective learner using a variant of Q-learning to first estimate the hypothesis and then estimate the probability \((Y_{t^{}} y_{t})\) like Scenario 3 above to predict on future data at time \(t^{}\). See Appendix B.3. Prospective risk of this learner converges to Bayes risk in Fig. 1. This scenario is closely related to reinforcement learning .

Figure 1: A schematic for prospective learning (left) and realizations of the examples for the four scenarios (top right); dots denote 1s and empty spaces denote 0s for \(Y_{t}\{0,1\}\) with \(X_{t}=1\) for all times \(t\). Prospective risk of learners at different times is shown in the bottom panels and discussed in Section 2.1. **Scenario 1:** For Bernoulli probability \(p=0.2\), the maximum-likelihood estimator (MLE) in blue uses a time-agnostic hypothesis \(h_{t}(X_{t})=(_{t}>0.5)\) where \(_{t}=t^{-1}_{s=1}^{t}y_{s}\), ties at \(_{t}=0.5\) are broken randomly. The risk of this learner converges to the Bayes risk. **Scenario 2:** For Bernoulli probability \(p=0.2\), the MLE estimator (blue) performs at chance levels. A prospective learner (red) that alternates between two predictors at even and odd times converges to Bayes risk. Variants of this learner that use less information from the stochastic process (purple does not know that the data distributions at even and odd times are tied, green does not know that the distribution shifts at every time-step) also converge to Bayes risk, but more slowly. **Scenario 3:** For \(=0.1\) and \(=0.9\) in the discounted prospective risk, the MLE estimator (blue) again performs at chance levels. A prospective learner that computes an estimate of the transition probability of the two-state Markov chain to estimate \((Y_{t^{}} y_{t})\) for future times \(t^{}>t\) converges to Bayes risk. **Scenario 4:** For \(_{0}=_{1}=0.1\), the MLE estimator (blue) performs at chance levels. A prospective learner that uses a variant of Q-learning (described in the text and Appendix B.3) converges to the prospective Bayes risk.

How is prospective learning related to other learning paradigms? 6

Distribution shift.Prospective learning  is equivalent to PAC learning  in Scenario 1 when data is IID. Situations when this assumption may not be valid are often modeled as a distribution shift between train and test data . Techniques such as propensity scoring [18; 19] or domain adaptation [20; 21] reweigh or map the train/test data to get back to the IID setting; techniques like domain invariance [22; 1] build a statistic that is invariant to the shift. Typically, the loss is unchanged across train and test data. If the set of marginals \(\{(Z_{t})\}\) of the stochastic process only has two elements, then PL is equivalent to the classical distribution shift setting. But otherwise, in PL, data is correlated across time, distributions (marginals) can shift multiple times, and risk changes with time.

**Multi-task, meta-, continual, and lifelong learning.** A changing data distribution could be modeled as a sequence of tasks. Depending upon the stochastic process, different concepts are relevant, e.g., multi-task learning  is useful for Scenario 2 and Appendix D when there are a finite number of tasks. Much of continual or lifelong learning [14; 13] focuses on "task-incremental" and "class-incremental" settings , in which the learner knows when the task switches. PL does not make this assumption, and therefore, the problem is substantially more difficult. "Data-incremental" (or task-agnostic) setting , is similar to PL. But the main difference is the goal: continual or lifelong learning seeks to minimize past error. As a consequence, continual learning methods are poor prospective learners; see Section 6. Online meta-learning [26; 27; 28; 29] is close to task-agnostic continual learning, except that the former models tasks as being sampled IID from some distribution of tasks. Due to this, one cannot predict which task is next, and therefore cannot prospect.

**Sequential decision making and online learning.** PL builds upon works on learning from streaming data. But our goals are different. For example, Gama et al.  minimize the error on samples from a stationary process; Hayes et al.  minimize the error on a fixed held-out dataset or on all past data--neither of these emphasizes prospection. There is a rich body of work on sequential decision making, e.g., predicting a finite-state, stationary ergodic process from past data . Even in this simple case, there does not exist a consistent estimator using the finite past \(Z_{1:t}\)[33; 34; 35]. This is also true for regression [36; 37], when the true hypothesis \(f^{*}\) s.t. \(Y_{t}=f^{*}(X_{t})\) is fixed. In other words, Bayes risk \(R_{t}^{*}\) in Theorem 1 may be non-zero in PL even for finite-state, stationary ergodic processes. Hanneke  lifts the restriction on stationarity and ergodicity. They obtain conditions on the input process \(X\) for consistent inductive (predict at time \(t^{}>t\) using data up to \(t\)), self-adaptive (predict at time \(t^{}\) using \(Z_{ t}\) and \(X_{t+1:t^{}}\)) and online learning [39; 40] (predict at \(t^{}\) using \(Z_{ t^{}}\)). They prove the existence of a learning rule that is consistent for every \(X\) that admits self-adaptive learning. If \(X\) is "smooth", i.e., input marginals have a similar support over time, then ERM has a similar sample complexity as that of the IID setting . Haghtalab et al.  give algorithmic guarantees for several online estimation problems in this setting.

The true hypothesis in PL can change over time. This is different from the continual learning setting where we can find a common hypothesis for tasks at all time , and this is why our proofs work quite differently from existing ones in the literature. Instead of a hypothesis class \(^{}\), we define the notion of a hypothesis class that consists of sequences of predictors, i.e., subset of \((^{})^{}\); we can do ERM in this new space. Instead of consistency of prediction as in Hanneke , we give guarantees for strong learnability, i.e., convergence of the ERM risk to the Bayes risk.

**Information theory.** There are also works that have sought to characterize classes of stochastic processes that can be predicted fruitfully. Bialek et al.  defined a notion called predictive information (closely related to the information bottleneck principle ) and showed how it is related to the degrees of freedom of the stochastic process. Shalizi and Crutchfield  showed that a causal-state representation called an \(\)-machine is the minimal sufficient statistic for prediction.

## 4 Theoretical foundations of prospective learning

**Definition 1** (Strong Prospective Learnability).: A family of stochastic processes is strongly prospectively learnable, if there exists a learner with the following property: there exists a time \(t^{}(,)\) such that for any \(,>0\) and for any stochastic process \(Z\) from this family, the learner outputs a hypothesis \(h\) such that \([R_{t}(h)-R_{t}^{*}<] 1-\), for any \(t>t^{}\).

This definition is similar to the definition of strong learnability in PAC learning with one key difference. Prospective Bayes risk \(R_{t}^{}\) depends upon the realization of the stochastic process \(z_{ t}\) up to time \(t\). In PAC learning, it would only depend upon the true distribution of the data. Not all families of stochastic processes are strongly prospectively learnable. We therefore also define weak learnability with respect to a "chance" learner that predicts \([Y]\) and achieves a prospective risk \(R_{t}^{0}\).7

**Definition 2** (**Weak Prospective Learnability**).: A family of stochastic processes is weakly prospectively learnable, if there exists a learner with the following property: there exists an \(>0\) such that for any \(>0\), there exists a time \(t^{}(,)\) such that for any stochastic process \(Z\) from this family, \([R_{t}^{0}-R_{t}(h)>] 1-\), for any \(t>t^{}\).

In PAC learning for binary classification, strong and weak learnability are equivalent  in the distribution agnostic setting, i.e., when strong and weak learnability is defined as the ability of a learner to learn any data distribution. But even in PAC learning, if there are restrictions on the data distribution, strong and weak learnability are not equivalent . This motivates Proposition 1 below. Before that, we define a time-agnostic empirical risk minimization (ERM)-based learner. In PAC learning, ERM selects a hypothesis that minimizes the empirical loss on the training data. It outputs a time-agnostic hypothesis, i.e., using data, say, \(z_{ t}\) standard ERM returns the same predictor for future data from any time \(t^{}>t\). There is a natural application of ERM to prospective learning problems, defined below.

**Definition 3** (**Time-agnostic ERM**).: Let \(\) be a hypothesis class that consists of time-agnostic predictors, i.e., \(h_{t}=h_{t^{}}\) for any \(t,t^{}\) for all predictors \(h\). Given data \(z_{ t}\), a learner that returns

\[=*{arg\,min}_{h}_{s=1}^{t} (s,h_{s}(x_{s}),y_{s})\] (5)

is called a time-agnostic empirical risk minimization (ERM)-based learner.

Time-agnostic ERM in prospective learning may use a time-dependent loss \((s,h_{s}(x_{s}),y_{s})\) upon the training data. This ERM is not very different from standard ERM in PAC learning (when instantiated with the hypothesis class that consists of sequences of predictors, that we are interested here). If data is IID (Scenario 1), then there is no information provided by time in the training samples. But if there are temporal patterns in the data, take Scenarios 2 and 3 or Scenario 4 as examples, then time-agnostic ERM as defined here will return predictors that are different than those of standard ERM that uses a time-invariant loss.

**Proposition 1**.: _There exist stochastic processes for which time-agnostic ERM is not a weak prospective learner. There also exist stochastic processes for which time-agnostic ERM is a weak prospective learner but not a strong one._

See Appendix E for the proof. We do not know yet whether (or when) strong and weak learnability are equivalent for prospective learning.

## 5 Prospective Empirical Risk Minimization (ERM)

In PAC learning, the hypothesis returned by ERM using the training data can predict arbitrarily well (approximate the Bayes risk arbitrarily well with arbitrarily high probability), with a sufficiently large sample size. This statement holds if (a) there exists a hypothesis in the hypothesis class whose risk matches the Bayes risk asymptotically, and (b) if risk on training data converges to that on the test data sufficiently quickly and uniformly over the hypothesis class [49; 50]. Theorem 1 is an analogous result for prospective learning.

**Theorem 1** (**Prospective ERM is a strong prospective learner**).: _Consider a finite family of stochastic processes \(\). If we have (a) consistency, i.e., there exists an increasing sequence of hypothesis classes \(_{1}_{2}\) with each \(_{t}(^{})^{}\) such that \( Z\),_

\[_{t}[_{t_{t}}R_{t}(h)-R_{t}^{ *}]=0,\] (6)

_where \(h_{t}\) is a random variable in \((Z_{ t})\), and (b) uniform concentration of the limsup, i.e., \( Z\),_

\[[_{h_{t}}|_{t}(h,Z)-_{u_{ t} m t}_{s=1}^{m}(s,h_{s}(x_{s}),y_{s})|] _{t},\] (7)_for some \(_{t} 0\) and \(u_{t}\) with \(u_{t} t\) (all uniform over the family of stochastic processes), then there exists a sequence \(i_{t}\) that depends only on \(_{t}\) such that a learner that returns_

\[=*{arg\,min}_{h_{i_{t}}}_{u_{i_{t}} m  t}_{s=1}^{m}(s,h_{s}(x_{s}),y_{s}),\] (8)

_is a strong prospective learner for this family. We define prospective ERM as the learner that implements Eq. (8) given train data \(z_{ t}\)._

Appendix E.2 provides a proof, it builds upon the work of Hanneke . The first condition, Eq. (6), is analogous to the consistency condition in PAC learning. In simpler words, it states that the Bayes risk can be approximated well using the chosen sequence of hypothesis classes \(\{_{t}\}_{t=1}^{}\). The second condition, Eq. (7), is analogous to concentration of measure in PAC learning, it requires that the limsup in Eq. (1) is close to an empirical estimate of the limsup (the second term inside the absolute value in Eq. (7)). At each time \(t\), prospective ERM in Eq. (8) selects the best hypothesis \(_{t}\)8 for future times \(t^{}>t\), that minimizes an empirical estimate of the limsup using the training data \(z_{ t}\). Prospective ERM can exploit the difference between the latest datum in the training set with time \(t\) and the time for which it makes predictions \(t^{}\) by selecting specific sequences inside the hypothesis class \(_{t}\). For example, in Scenario 2 it can select sequences where alternating elements can be used to predict on data from even and odd times.

**Remark 1** (**How to implement prospective ERM?)**.: An implementation of prospective ERM is therefore not much different than an implementation of standard ERM, except that there are two inputs: time \(s\) and the datum \(x_{s}\). Suppose we use a hypothesis class where each predictor is a neural network, this could be a multi-layer perceptron or a convolutional neural network. The training set \(z_{ t}\) consists of inputs \(x_{s}\) along with corresponding time instants \(s\) and outputs \(y_{s}\). To implement prospective ERM, we modify the network to take \((s,x_{s})\) as input (using any encoding of time, we discuss one in Section 6) and train the network to predict the label \(y_{s}\). In Eq. (8) we can set \(u_{i_{s}} t\), doing so only changes the sample complexity. At inference time, this network is given the input \((t^{},x_{t^{}})\) to obtain the prediction \(y_{t^{}}\). Note that if prospective ERM is implemented in this fashion, the learner need not explicitly calculate the infinite sequence of predictors.9

**Corollary 1**.: _There exist stochastic processes for which time-agnostic ERM is not a strong prospective learner, but prospective ERM is a strong learner._

**Remark 2** (**Why we need an increasing sequence of hypothesis classes \(_{1}_{2}\))**.: We could have chosen \(_{t}=_{t^{}}\) for all \(t,t^{}\) to set up Theorem 1. But since the learner does not have a lot of data at early times, it should use a small hypothesis class. Just like PAC learning, the sequence \((_{t})_{t}\) in Eq. (7) determines the convergence rate of a prospective learner. Therefore, using a monotonically increasing sequence of hypothesis classes is useful to ensure a good sample complexity.

**Theorem 2**.: _Consider a finite family of stochastic processes \(\). If there exists a countable hypothesis class \(\) such that_

\[_{t}[_{h}R_{t}(h)-R_{t}^{*} ]=0,\] (9)

_for any stochastic process \(Z\), where \(h\) is a random variable in \((Z_{ t})\), then there exist \(_{t}\), \(u_{t}\), and \(_{t}\) such that the two conditions of Theorem 1 are satisfied for this family._

Appendix E.3 provides a proof. This theorem provides a concrete example for which the assumptions of Theorem 1 are satisfied. In PAC learning, one first proves uniform convergence for a finite hypothesis class. This can then be used to, say, calculate the sample complexity of ERM, or extended to infinite hypothesis classes using constructions such as VC-dimension and covering numbers . The above theorem should be understood in the same spirit. It is a step towards characterizing the sample complexity of prospective learning.

Appendix C proves an analogue of Theorem 1 for prospective learning problems with discounted losses. Appendix D provides illustrative examples of prospective ERM for periodic processes and hidden Markov processes. For periodic processes, we can also calculate the sample complexity.

Experimental Validation

This section demonstrates that we can implement prospective ERM on prospective learning problems constructed on synthetic data, MNIST and CIFAR-10. In practice, prospective ERM may approximately achieve the guarantees of Theorem1. We will focus on the distribution changing, independently or not (Scenarios 2 and 3). Recall that Scenario 1 is the same as the IID setting used in standard supervised learning problems. Scenario 4 is more involved (see an example in AppendixB.3) and, therefore, we leave more elaborate experiments for future work. We discuss experiments that check whether large language models can do prospective learning in AppendixH.

Learners and hypothesis classes.Task-agnostic online continual learning methods are the closest algorithms in the literature that can address situations when data evolves over time. We use the following three methods.

1. **Follow-the-Leader** minimizes the empirical risk calculated on all past data and is a no-regret algorithm . We note that while this is a popular online learning algorithm, we do not implement the algorithm in an online fashion.
2. **Online SGD** fine-tunes the network using new data in an online fashion. At every time-step, weights of the network are updated once using the last eight samples.
3. **Bayesian gradient descent** is an online continual learning algorithm designed to address situations where the identity of the task is not known during both training and testing, i.e., it implements continual learning without knowledge of task boundaries.

These three methods are not explicitly designed for prospective learning but they are designed to address the changing data distribution \(t\).10 We calculate the prospective risk of the predictor returned by these methods; note that they do not output a time-varying predictor and consequently, these methods output a time-agnostic hypothesis. As a result, when we evaluate the prospective risk of these methods, we use the same hypothesis for all future time. For all three methods, we use a multi-layer perceptron (MLP) for synthetic data and MNIST, and a convolutional neural network (CNN) for CIFAR-10.

For **prospective ERM** the sequence of predictors is built by incorporating time as an additional input to an MLP or CNN as follows. For frequencies \(_{i}=/i\) for \(i=1,,d/2\), we obtain a \(d\)-dimensional embedding of time \(t\) as \((t)=((_{1}t),,(_{s/2}t),(_{1}t), ,(_{d/2}t))\). This is similar to the position encoding in Vaswani et al. . A predictor \(h_{t}()\) uses a neural network that takes as input, an embedding of time \((s)\), and the input \(x_{s}\) to predict the output \(y_{s}\) for any time \(s\). Using such an embedding of time is useful in prospective learning because, then, one need not explicitly maintain the infinite sequence of predictors \(h(h_{1},,)\).

Training setup.We use the zero-one error \(\{ y\}\) to calculate prospective risk for all problems; all learners are trained using a standard surrogate of this objective, the cross-entropy loss. For all experiments, for each time \(t\), we calculate the prospective risk \(R_{t}(h)\) in Eq.2 of the hypothesis created by these learners for a particular realization of the stochastic process \(z_{ t}\). For each prospective learning problem, we generate a sequence of 50,000 samples. Learners are trained on data from the first \(t\) time steps (\(z_{<t}\)) and prospective risk is computed using samples from the remaining time steps. Except for online SGD and Bayesian gradient descent, learners corresponding to different times are trained completely independently. See AppendixF for more details.

Remark 3 (Why we do not use existing benchmark continual learning scenarios).: The tasks constructed below resemble continual learning benchmark scenarios such as Split-MNIST or Split-CIFAR10  where data from different distributions are shown sequentially to the learner. However, there are three major differences. First, in these existing benchmark scenarios, data distributions do not evolve in a predictable fashion, and prospective learning would not be meaningful. Second, existing scenarios consider a fixed time horizon. We are keen on calculating the prospective risk for much longer horizons whereby the differences between different learners are easier to discern; our experiments go for as large as 30,000 time steps. Third, our tasks have the property that the Bayes optimal predictor changes over time.

### Prospective learners for independent but not identically distributed data (Scenario 2)

We create tasks using synthetic data, MNIST and CIFAR-10 datasets to design prospective learning problems when data are independent but not identically distributed across time (Scenario 2).

Dataset and Tasks.For the synthetic data, we consider two binary classification problems ("tasks") where the input is one-dimensional. Inputs for both tasks are drawn from a uniform distribution on the set \([-2,-1]\). Ground-truth labels correspond to the sign of the input for Task 1, and the negative of the sign of the input for Task 2. For MNIST and CIFAR-10 we consider 4 tasks corresponding to data from classes 1-5, 4-7, 6-9 and 8-10 in the original dataset, i.e., the first task considers classes 1-5 labelled 1-5 respectively, the second task considers classes 4-7 labelled 1-4, the third task considers classes 6-9 labeled 1-4 and the last task considers labels 8-10 labelled 1-3. In other words, images from class 1 in task 1, class 4 from task 2 and class 6 from task 3 are all assigned the label 1. For the prospective learning problem based on synthetic data, the task switches every 20 time steps. For MNIST and CIFAR-10, the data distribution cycles through the 4 tasks, and the distribution of data changes every 10 time-steps. For more details, see Appendix F.

Fig. 2 shows that **prospective ERM can learn problems when data are independent but not identically distributed** (Scenario 2). For prospective learning problems constructed from synthetic data, the risk of prospective ERM converges to prospective Bayes risk over time. For the MNIST and CIFAR-10 prospective problems, the prospective learning risk drops precipitously. In contrast, online learning baselines discussed above achieve a far worse prospective risk. Observe that Follow-the-Leader (blue) performs as well, or better, as online SGD and Bayesian GD. This is not surprising, while ERM models corresponding to each time \(t\) were trained independently, networks corresponding to online SGD and Bayesian GD were training in an online fashion; in practice it is often difficult to tune online learning methods effectively .11

### Prospective learners when data are neither independent nor identically distributed (Scenario 3)

Dataset and Tasks.For synthetic data, we construct 4 binary classification problems with two-dimensional input data (see Fig. 3 and caption for details). For CIFAR-10 and MNIST, we consider four tasks corresponding to the classes 1-5, 4-7, 6-9 and 8-10. Using these tasks, we construct

Figure 2: **Prospective ERM can achieve good instantaneous and prospective risk in Scenario 2. Left:** Instantaneous and prospective risks for problems constructed using synthetic data (see text) across 5 random seeds (which govern the sequence of samples and the weight initializations of neural networks). Instantaneous risk spikes when the task switches for many online learning baseline algorithms. In contrast, prospective ERM has minimal spikes at later times and both instantaneous and prospective risks eventually converge to zero. **Right:** Prospective risk for different baseline algorithms and prospective ERM for tasks constructed using MNIST and CIFAR-10 for Scenario 2. In all three cases, the risk of prospective ERM approaches Bayes risk while online learning baselines considered here do not achieve a low prospective risk. For comparison, the chance prospective risk is 0.5 for synthetic data and 0.742 for MNIST and CIFAR-10 tasks.

problems where the data distribution is governed by a stochastic process which is a hierarchical hidden Markov model (Scenario 3). After every 10 time-steps, a different Markov chain governs transitions among tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4, as shown in Fig. 3). These choices ensure that the stochastic process does not have a stationary distribution.12

As Fig. 4 shows, **prospective ERM can prospectively learn problems when data is both independent and identically distributed (Scenario 3**. Stochastic processes in these problems corresponding to Scenario 3 do not have a stationary distribution. This is why a time-agnostic hypothesis (Follow-the-Leader) does not achieve a good prospective risk, unlike prospective ERM. Appendix G discusses additional experiments for Scenario 3 for different kinds of Markov chains.

## 7 Discussion

Prospective learning, as we see it, is a paradigm of learning that characterizes many real-world scenarios which are currently modeled using much stronger (and less accurate) assumptions. These simplifying assumptions have certainly enabled progress in machine learning. But systems deployed built upon these approaches have proven to be extremely fragile in certain real-world settings. Today's machine learning-based systems fail to track changes in the data. They certainly do not model how biological organisms learn robustly and effectively over time. We believe characterizing which kinds of stochastic processes are prospectively learnable under which kinds of time-sensitive loss functions will be an important next theoretical step. Developing algorithms, from the perspective of prospective learning, which have theoretical guarantees in practice, will be another next step. Finally, building algorithms that scale and can therefore be deployed in real-world systems, will be important to demonstrate the utility of this approach. The precise real-world applications in which prospective learning based methods will outperform PAC learning, remains to be seen.

Figure 4: **Prospective ERM can achieve good prospective risk in Scenario 3. Prospective risk across 5 random seeds (which govern the sequence of samples and the weight initializations of neural networks). In all three cases, the risk of prospective ERM approaches Bayes risk while a number of baseline algorithms do not achieve a low prospective risk. Stochastic processes in these problems corresponding to Scenario 3 do not have an invariant distribution. This is why a time-agnostic hypothesis (ERM) that is constructed by the baseline algorithms does not achieve a good prospective risk.**

Figure 3: **Left: For MNIST and CIFAR-10, we consider 4 tasks corresponding to the classes 1-5, 4-7, 6-9 and 8-10. Using these tasks, we construct Scenario 3 problems corresponding to a stochastic process which is a hierarchical hidden Markov model. After every 10 time-steps, a different Markov chain governs transitions among tasks (one Markov chain for tasks 1 and 2, and another for tasks 3 and 4). This ensures that the stochastic process does not have a stationary distribution. Right: For synthetic data, the 4 tasks are created using two-dimensional input data as shown pictorially above. The four parts of the input domain are \(\{(x_{1},x_{2}):1 x_{1},x_{2} 2\}\), \(\{(x_{1},x_{2}):1 x_{1} 2,-2 x_{2}-1\}\), \(\{(x_{1},x_{2}):-2 x_{1},x_{2}-1\}\) and \(\{(x_{1},x_{2}):-2 x_{1}-11 x_{2} 2\}\). Colors indicate classes. The hierarchical hidden Markov model for transitions among the tasks is identical to the MNSIT and CIFAR-10 setting shown on the left.**