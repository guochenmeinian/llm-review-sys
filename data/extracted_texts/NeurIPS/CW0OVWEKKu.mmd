# Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks

Xin-Chun Li\({}^{1,2}\), Jin-Lin Tang\({}^{1,2}\), Bo Zhang\({}^{1,2}\), Lan Li\({}^{1,2}\), De-Chuan Zhan\({}^{1,2}\)

\({}^{1}\) School of Artificial Intelligence, Nanjing University, Nanjing, China

\({}^{2}\) National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

{lixc, tangjl, zhangb, lil}@lamda.nju.edu.cn, zhandc@nju.edu.cn

###### Abstract

Exploring the loss landscape offers insights into the inherent principles of deep neural networks (DNNs). Recent work suggests an additional asymmetry of the valley beyond the flat and sharp ones, yet without thoroughly examining its causes or implications. Our study methodically explores the factors affecting the symmetry of DNN valleys, encompassing (1) the dataset, network architecture, initialization, and hyperparameters that influence the convergence point; and (2) the magnitude and direction of the noise for 1D visualization. Our major observation shows that the _degree of sign consistency_ between the noise and the convergence point is a critical indicator of valley symmetry. Theoretical insights from the aspects of ReLU activation and softmax function could explain the interesting phenomenon. Our discovery propels novel understanding and applications in the scenario of Model Fusion: (1) the efficacy of interpolating separate models significantly correlates with their sign consistency ratio, and (2) imposing sign alignment during federated learning emerges as an innovative approach for model parameter alignment.

## 1 Introduction

The massive number of parameters and complex structure of deep neural networks (DNNs) have catalyzed extensive research to mine their underlying mechanics [55; 34; 42; 62; 71]. Visualizing and exploring the loss surfaces of DNNs is the most intuitive way [43; 18], which has ignited many interesting findings, such as the monotonic linear interpolation [22; 19; 66] and linear mode connectivity [22; 14; 21; 15; 65; 2]. Loss landscape visualization has also been applied to show the optimization trajectory [53; 33; 35], understand the effectiveness of Batch Normalization [32; 61], BERT [11; 23], deep ensemble [17; 30; 21], and so on.

Perturbation analysis around the local minima of DNNs [7; 63], i.e., the shape of the valleys they reside in, is a very popular research topic. The concept of flat minima was originally proposed by , who defines the size of the connected region around the minima where the loss remains relatively unchanged as flatness. Subsequent studies debate whether the flat or sharp minima could reflect the generalization ability [38; 43; 36; 57; 16; 12; 41; 3]. The previous works constrain the valley shape to be symmetric, while recent work points out that not all DNN valleys are flat or sharp, and there also exist asymmetric valleys , which _has not been systematically studied as far as we know_.

This paper in-depth analyzes the factors that may affect the valley symmetry of DNNs. Previous work's analysis of valley shape primarily utilizes the 1D interpolation of \(_{f}+\), where \(_{f}\) represents the minima solution and \(\) denotes a random noise. As shown in Fig. 1, we believe that the valley symmetry depends both on the convergence solution and noise, with each of them being influenced by some factors. _The most significant innovation in our research is considering the effect of noise direction on valley visualization, as previous work has simply taken the Gaussian noise_.

Specifically, we start by comprehensively and carefully determining the visualization method to plot the valley shape, which could influence the conclusion significantly [31; 43; 41]. We finally take the Norm-Scaled (NS) visualization method  that normalizes the noise \(\) and further scales it to \(\|_{f}\|\) for better determining the plot range of \(\), where the direction of raw noise is not changed. Then, after investigating 7 common noise directions and 6 special ones, we conclude: _the degree of sign consistency between the noise and the convergence solution should be a determining factor for asymmetry_. This phenomenon is basically insensitive to the utilized datasets. Next, we focus on the impact of the network architecture with or without Batch Normalization (BN) , indicating that the BN initialization also impacts the valley symmetry. Finally, different hyperparameters lead to solutions with various valley widths but show no asymmetry consistently.

Aside from empirical observations, theoretical insights for our interesting findings are provided. We first declare that adding sign-consistent noise to parameters may have a larger probability of keeping activating the neurons or keeping the overwhelming score in classification tasks. Then, we show that the trace of the Hessian matrix along the sign-consistent direction is smaller, implying a flatter region. The above findings inspire applications in the fields of model fusion [47; 68; 33; 2]. This paper first explains why model aggregation based on pre-trained models often leads to performance improvements, i.e., the success of model soups , and then proposes constraining the sign of DNN parameters in federated learning [54; 73; 50] to facilitate aggregation.

Our novel contributions can be summarized as (1) _exploring the valley shape under different noise directions that have not been studied yet_; (2) _proposing that the flat region could be expanded along the direction that has a higher sign consistency with the convergence solution_; (3) _pointing out the influence of BN and its initialization on valley symmetry_; (4) _presenting theoretical insights to explain our interesting finding_; (5) _explaining and inspiring effective algorithms in model fusion_.

## 2 Related Works

Exploring the Valley Shape of DNNs.The valley around a minima solution has originally been viewed as flat or sharp , and the large batch size training may lead to sharp minima with poor generalization . However,  declares that two solutions that are scale-invariant in performance may lie in regions with significantly different flatness.  verifies the findings of  by applying varying weight decay to small and large batch training, leading to results contrary to . With the filter-normalized plots,  again observes that sharpness correlates well with generalization error. Later work also shows that the sharpness calculation should be dependent on the parameter scale  or the dataset . Whether valley width reflects the generalization is still inconclusive. The proposal of asymmetric valley  further throws this debate into limbo, as valleys around minima could be flat on one side but sharp on the other side, which makes it more difficult to define flatness. _This paper thoroughly studies the causes and implications of the unexplained asymmetry phenomenon_.

Exploiting the Valley Shape of DNNs.Exploring the valley shape of DNNs could help us better understand the inherent principles of DNNs. The work  utilizes 2D surface plots to show that the residual connections in ResNet  could prevent the explosion of non-convexity when networks get deep, and  attributes the success of BN to its effects of making landscape significantly more smooth. The asymmetric valley  provides a sounding explanation for the intriguing phenomenon in stochastic weight averaging . Additionally, studying the valley shape could also benefit the proposal of effective optimization algorithms, e.g., the entropy-based SGD , and the (adaptive) sharpness-aware minimization [16; 41]. Penalizing the gradient could also lead to solutions around flat regions . _We also apply the findings in this paper to the area of model fusion_.

**Model Fusion**. Directly averaging two independent models may encounter a barrier due to training randomness or permutation invariance [4; 22; 2; 51; 24]. However, if the two models are generated from separate training of a common pre-trained model, the model fusion may perform better than individual models, i.e., the model soups [58; 68]. A recent work  finds that resolving sign conflicts when merging multiple task-specific models is neccessary, which is most related to our current work. _We will explain the success of model soups based on the relation between the asymmetric valley and the sign consistency of model parameters._ Popular federated learning algorithms also take the parameter averaging process to fuse the individual models updated on isolated clients [54; 52]. A huge challenge is the Non-Independent and Identical Data distributions of data islands (Non-I.I.D. data) [73; 29], which could make local models too diverged to merge. Multiple regularization methods are proposed to align parameters before model fusion [46; 1; 45; 37]. _We propose an effective regularization method that focuses on the sign of parameters, which is inspired by our interesting findings._

## 3 Basic Notations and Preliminaries

Our major tool is plotting the 1D error curve of DNNs following the formula \(_{f}+\) (Fig. 2 (A)). \(_{f}\) denotes the converged model, and \(\) denotes a noise vector sampled from a specific distribution. More about the visualization of DNN loss landscape could be found in [43; 49].

### Previous Studies: Exploring \(_{f}\) with Fixed \(\)

The previous studies focus on _studying the valley shape under different \(_{f}\)_, and aim to _mine the shape's relation to generalization_. To mitigate the influence of parameter scales and make the visualization fairly comparable between different \(_{f}\),  proposes the filter normalization method to properly visualize the loss landscape. The processing of the noise is \(^{i,j}}{\|^{i,j}\|}\|_{f} ^{i,j}\|\), where \(i\) is the index of layer and \(j\) is the index of filter. This way normalizes each filter in the noise \(\) to have the same norm of the corresponding filter in the converged point \(_{f}\). Further,  proposes a proper definition of sharpness (i.e., adaptive sharpness) based on the filter normalization, extending it to all parameters and formally defining: \(T_{_{f}}=((\|^{1}\|_{2} _{n_{1}},,\|^{m}\|_{2}_{n_{m}},|w^{1}|, ,|w^{q}|))\), where \(^{j}\) with \(1 j m\) denotes the \(j\)-th convolution filter in \(_{f}\) and \(n_{j}\) is the number of parameters it owns. \(w^{j}\) with \(1 j q\) denotes the \(j\)-th parameter that is not included in any filters. \(\) is a vector with all values as one. Then, the adaptive noise \(T_{_{f}}\) is utilized to study the sharpness of \(_{f}\).

### Our Study: Exploring \(\) and \(\) with Fixed \(_{f}\)

Different from the previous studies, we aim to _explore the valley shape of a fixed \(_{f}\) under different \(\) and \(\)_. First, the direction of 1D interpolation in previous works is limited to the Gaussian noise, while we study the impact of different noise types. Second, setting \(\) positive or negative could obtain a valley with different flatness, i.e., the asymmetric valley . Hence, under the fixed \(_{f}\), we do not need to rectify the noise direction filter-wisely. We take the visualization way used in , which only normalizes the noise and then re-scales it to the norm of \(_{f}\), i.e., \(\|_{f}\|\). The utilized Norm-Scaled (NS) noise is shown in Fig. 2 (C). Compared with Filter NS noise (Fig. 2 (B)), this way _does not change the direction of the noise and shows the original valley shape along the direction \(\)_. Additionally, to plot 1D error curves in the same figure, we fix \(\) in the range of \([-1,1]\). Another scale factor \(s\) is added to control the visualized width. Overall, we use the following way to plot 1D error curves: \(_{f}+*s*\|_{f}\|\), where \([-1,1]\), and we set \(s=1.0\) by default. The Frobenius norm is used. Notably, we utilize the NS noise by default and use the Filter NS noise when comparing the valley shape under different converged points, e.g., the studying of BN initialization in Sect. 4.2.1.

## 4 Experimental Findings

This section presents the major findings about factors that respectively impact the noise and converged points. Experimental details and more verification results are in Appendix A and B.

### Factors that Affect \(\)

We train VGG16  with BN  on CIFAR10  for \(200\) epochs and obtain the converged model. For the given \(_{f}\), we plot the 1D error curves under the following _7 common noise types_: (1) \(G(0,1)\): Gaussian noise with mean as \(0\) and std as \(1\); (2) \(U(-1,1)\): uniform noise in the range \([-1,1]\); (3) \(\{-1,0,1\}\): uniform noise with values only in \(-1\), \(0\), and \(1\); (4) \(G(1,1)\): Gaussian noise with mean as \(1\) and std as \(1\); (5) \(U(0,1)\): uniform noise in the range \(\); (6) \(\{0,1\}\): uniform noise with values only in \(0\) and \(1\); (7) \(\{1\}\): constant noise with all values as \(1\). For each plot, we set \(s\{0.2,1.0,2.0\}\) and \([-1,1]\) to show curves under various levels of width. The results are shown in the first row of Fig. 3. We observe that the valley shapes along these 7 noise directions are almost symmetric, except that the last four noise directions show slight asymmetry when \(s=0.2\).

Then, a fantastic idea motivates us to change the sign of the noise. The detail of this motivation is provided in Appendix A.4. Specifically, we use the following method to replace the sign of \(\) with that of \(_{f}\): \(||*(_{f})\), where \(||\) returns the absolute value element-wisely and \(()\) returns \(1\) or \(-1\) based on whether the element is positive or negative. The corresponding results of the 7 common noises become completely asymmetric, which are plotted in the second row of Fig. 3. Furthermore, the valleys all follow the tendency that the positive direction is flat while the negative direction is sharp. Hence, we propose our major finding: _the sign consistency between noise and converged model determines the asymmetry and the valley is flatter along the noise direction with a larger sign consistency_. The finding is formulated as: \(L(_{f}+a)<L(_{f}-a)\), where \(=||*(_{f})\) denotes the sign-consistent noise, and \(a>0\) is a constant. \(L()\) is the loss function, which could be the prediction error or cross-entropy loss. The following three experimental studies could further verify this interesting finding.

**The Manual Construction of Noise Direction**. We element-wise sample the noise \(\) from \(G(0,1)\), and then manually change its elements' sign with a given ratio \(r\{0.0,0.1,,1.0\}\). For example, \(r=0.5\) means that we sample \(50\%\) elements in the noise and change their sign to the same as \(_{f}\). Then, we plot the average test error of the positive interpolations and negative interpolations, i.e., \(_{}[(_{f}+())]\) with \(\) and \([-1,0]\), respectively. Fig. 4 plots the average test errors on two groups of networks and datasets. The test errors of positive and negative directions are nearly equal when \(r=0\%\). As \(r\) becomes larger, the average test error of the positive direction monotonically decreases while the negative one increases, implying that the valley shape becomes more and more asymmetric.

**The Investigation of 6 Special Noise Directions**. We then investigate several special noise directions including (1) the initialization before training, i.e., \(_{1}=_{0}\); (2) the converged model itself, i.e., \(_{2}=_{f}\); (3) \(_{3}=(_{f})\); (4) \(_{4}=(_{f}-)\); (5) \(_{5}=(_{f})\); (6) \(_{6}=(_{f}-)\). Here, \(\) denotes the mean value for each parameter group, e.g., the mean value of "conv1.weight". \(()\) returns 1 or 0 based on whether the element is positive or not. The visualization results are provided in Fig. 5. The first four directions lead to asymmetry, while the last two do not. First, the elements in \(_{2}\)

Figure 3: The valleys under 7 common noise types. The second row shows the results of replacing the sign of noise with that of \(_{f}\), leading to asymmetric valleys. (VGG16 with BN on CIFAR10)and \(_{3}\) surely have the same sign with \(_{f}\), which leads to an asymmetric valley. Because the mean values of most parameters are near zero, \(_{4}\) performs likely as \(_{3}\). \(_{5}\) and \(_{6}\) only have the same sign with the positive parameters in \(_{f}\), and applies zero to negative parameters in \(_{f}\), which shows no asymmetry. The most interesting result is the \(_{1}=_{0}\), whose elements may be centered around zero according to the Kaiming initialization . However, the BN initialization is asymmetric, which leads to asymmetric curves (Sect. 4.2.1). The results of VGG11 without BN on CIFAR100  show no asymmetry when \(=_{0}\) (Appendix B, Fig. 17).

**The Finding Holds for ImageNet and Various Parameter Groups**. We extend the findings to large-scale datasets and apply noise only to specific parameter groups. Specifically, we use the pre-trained models (e.g., ResNeXt101 ) downloaded from "torchvision" 1. Because these models are pre-trained on ImageNet , we could directly use them to verify our findings without additional training. Multiple parameter groups are considered as follows: (1) "ALL" denotes the whole parameters; (2) "CLF" denotes the weights in the final classifier layer; (3) "FEAT" denotes the weights in the layers aside from the final classifier; (4) "LAYER1" denotes parameters in the first several blocks; (5) "CONV" denotes all convolution parameters; (6) "BN" denotes all of the BN parameters. As shown in Fig. 6, applying sign-consistent noise could lead to asymmetric valleys. Notably, this holds for both the metrics of CE loss and prediction error.

### Factors that Affect \(_{f}\)

Then we focus on studying the effects of BN and its initialization, then present the results under various hyperparameters.

Figure 4: The impacts of manually constructed Gaussian noise with different levels of sign consistency.

Figure 5: The valley shape under 6 special noise types. (VGG16 with BN on CIFAR10)

Figure 6: Verification results on ImageNet with pre-trained ResNeXt101.

#### 4.2.1 BN and Initialization

The default initialization method of BN is setting \(\) as ones and \(\) as zeros [32; 9]. Hence, the \(\), i.e., the "BN.weight", may be asymmetric after convergence. We take three ways to initialize the BN weights, including (1) the elements are all ones; (2) the elements are sampled from \(U(0,1)\); (3) the values sampled from \(G(0,0.1)\). We train three models based on these three types of BN initialization. In Fig. 7, the first row shows the initial and converged parameter distribution of a specific BN weight. The traditional initialization leads to converged BN weights with all positive values, which are nearly centered around \(0.2\). The uniform initialization between \(0\) and \(1\) also leads to positive converged weights. The symmetric Gaussian initialization leads to converged values symmetric around \(0\). Then, we plot the valley shapes under the noise direction \(\{0,1\}\). Because this part involves a comparison among different convergence points, we plot the results by both the NS noise and Filter NS noise. As vividly displayed in Fig. 7, the first two initialization ways encounter obvious asymmetry while the Gaussian initialization shows nearly perfect symmetry. If we carefully analyze the sign consistency ratio of them, we could easily explain this phenomenon. The noise direction \(\{0,1\}\) has a larger overlap with the first two initialization methods because the converged BN weights are all positive, while it has a lower overlap with the initialization from \(G(0,0.1)\). This implies that _the traditional BN initialization will lead to nearly all positive converged BN weights, which may influence the valley symmetry_.

#### 4.2.2 Hyperparameters

Previously,  shows that the batch size could influence the valley width, and further  advocates that weight decay could also play a role in the valley width. Different from them, we aim to study whether these hyperparameters influence the valley symmetry. We train VGG16 networks on CIFAR10 with various hyperparameters. We use the SGD optimizer with a momentum value

Figure 8: The interpolation between two models trained with batch size as 32 and 2048. (VGG16 with BN on CIFAR10)

Figure 7: The impact of BN and its initialization on the valley symmetry. (VGG16 with BN on CIFAR10)

Figure 9: The impact of various hyperparameters on valley symmetry. (VGG16 with BN on CIFAR10)of 0.9. The default learning rate (LR) is 0.03, batch size (BS) is 256, and weight decay (WD) is \(0.0005\). Then, we will correspondingly set the LR in \(\{0.1,0.003\}\), BS in \(\{32,2048\}\), and WD in \(\{0.001,0.00001\}\). The curves are in Fig. 9. The first row applies the \(G(0,1)\) noise, while the second row changes its sign to the converged models' sign. Obviously, different hyperparameters may lead to valleys with various widths, while the valleys are all symmetric. The asymmetry valleys in the second row again verify the previous findings in Sect. 4.1.

Then, we explore the interpolation between two solutions under different hyperparameters, which are studied in [38; 43; 22]. We take the batch size of \(32\) and \(2048\) as an example and denote the converged solution as \(_{f_{1}}\) and \(_{f_{2}}\). The test error curve of \((1-)_{f_{1}}+_{f_{2}}\) is plotted, with \([-1,2]\). Aside from the interpolation curve, we also plot the parameter distributions of \(_{f_{1}}\) and \(_{f_{2}}\). The parameters are divided into five groups, including "BN Weight", "CLF Weight", "CLF Bias", "Other Weight", and "Other Bias". "CLF" denotes the last classification layer, and "Other" denotes other layers aside from the BN layers and the last classification layer. To simplify the figures, we only plot the mean and standard deviation of the parameters, denoted as \(_{p}\) and \(_{p}\). Fig. 8 shows the parameter distributions and the interpolation curve. The interpolation curve shows that the small batch training (i.e., \(=0.0\)) lies in a sharper and nearly symmetric valley, while the large batch training (i.e., \(=1.0\)) lies in a flatter but asymmetric valley. Small batch training (i.e., \(_{f_{1}}\)) leads to parameters with smaller mean and std values. This is because the utilized weight decay is \(0.0005\), which makes the parameter scale smaller due to longer training . Then, we explain the different results of valley symmetry. The interpolation formula could be re-written as \(_{f_{1}}+(_{f_{2}}-_{f_{1}})\) and \(_{f_{2}}+(-1)(_{f_{2}}-_{f_{1}})\), which respectively shows the 1D interpolation centered around \(_{f_{1}}\) and \(_{f_{2}}\). If we let \(=_{f_{2}}-_{f_{1}}\), then we could plot the sign consistency ratio (i.e., how many parameters have the same sign) of \(_{f_{1}}\) and \(\), and \(_{f_{2}}\) and \(\). The results are in Fig. 8, where we provide the values of the five parameter groups. Obviously, \(_{f_{2}}\) is more consistent in the sign values, which shows a flatter region towards the positive direction. In contrast, the sign consistency ratio of \(_{f_{1}}\) is smaller, which only shows slight asymmetry.

## 5 Theoretical Insights to Explain the Finding

This section provides theoretical insights from the aspects of ReLU activation and softmax function to explain the interesting phenomenon. The forward process of DNNs contains amounts of calculation of \(W^{T}h\), e.g., the fully connected and convolution layer. \(W\) denotes the weight matrix and \(h\) is a hidden representation. A special case is the final classification layer with the softmax function. Given \(h R^{d}\), the ground-truth label \(y[C]\), and the weight matrix \(W R^{C d}\), the softened probability vector is \(p\) = softmax(\(Wh\)). The cross-entropy (CE) loss function is \(L(W)\) = \(- p_{y}\). The gradient of \(w_{c}\) is \(g_{w_{c}}=-(I(c-y)-p_{c})h\), with \(c[C]\) and \(I(\)) being the indication function. This implies that the update direction of \(W\) lies in the subspace spanned by hidden representations, which also holds for intermediate layers and convolution layers . After adequate training steps, the parameters that activate the ReLU function or correspond to the ground-truth label should correlate well with their inputs. We show a demo classification weight learned on the "sklearn.digits" 2. Fig. 10 shows the pattern change of \(w+*(w)\) with \([-1.0,1.0]\). Clearly, the weight under \(=0.0\) correlates well with the input sample (i.e., Digit 0 in Fig. 10). Setting \(>0.0\) will almost keep the pattern, while \(<0.0\) destroys it significantly. That is, \(w+*(w)\) with \(>0.0\) may keep providing a high score for the target class, while setting \(<0.0\) may decrease the score.

For the ReLU activation, it also holds that \(w+*(w)\) will have a higher probability of keeping activating the neurons when \(>0.0\). To simplify the analysis, we assume the learned \(w\) equals \(a*h+\), where \(a\) is a constant and \(\) is a random Gaussian vector. Then we could easily verify that

Figure 10: The leftmost shows a digit sample from “sklearn.digits” and others show the pattern of \(w+*(w)\). \(=0.0\) shows the learned classification weight \(w\).

(\(w+*(w))^{T}h\) will have a higher probability of keeping activating neurons under a positive \(\) than the negative one. The details and simulation results are in Appendix D.2. If the neuron outputs are only simply scaled by a factor, it will not affect the relative scores of the final classification. For example, the inequation of \(w_{1}^{T}h>w_{2}^{T}h\) will not change if \(h\) is scaled by a positive factor, while it does not hold for \(h\) whose values are not activated, i.e., \(h=0\).

Then we provide a further analysis via analyzing the Hessian matrix of the softmax weights. Specifically, the Hessian of \(L(W)=- p_{y}\) w.r.t. \(W\) is \(H=((p)-pp^{T}) hh^{T}\), where \(\) denotes the Kronecker product. The trace of \(H\) is \(tr(H)=tr((p)-pp^{T})*tr(hh^{T})\). The first part could be calculated as \(_{c}p_{c}(1-p_{c})\), where \(c\) is the class index. According to the above analysis, adding sign-consistent noise to \(w_{y}\) could enlarge the score of \(w_{y}^{T}h\), which may make the \(p_{y}\) larger and \(p_{cxy}\) smaller . That is, the predicted probability vector tends to be a one-hot vector when adding sign-consistent noise, and \(_{c}p_{c}(1-p_{c})\) will be near zero. Hence, the trace of the Hessian matrix is smaller along the sign-consistent direction. Since softmax is convex, and the eigenvalues of the Hessian are all positive, a smaller trace means smaller eigenvalues, which makes the loss curve flatter. The empirical observation can be found in Appendix D.3.

## 6 Applications to Model Fusion

### Explaining the Success of Model Soups

Commonly, the interpolation of two independently found DNN solutions may encounter a barrier [22; 65]. Surprisingly, if these two models are updated from the same pre-trained model, then the barrier will disappear, and the linear interpolation brings a positive improvement [58; 68]. The common explanation follows that the pre-trained model may possess less instability when compared to the random initialization models . We guess that the sign consistency ratio may influence the parameter fusion performance of the two models. Perhaps, _the sign of model parameters updated based on the pre-trained model remain nearly unchanged during the process of fine-tuning_.

We experimentally verify our guess on two datasets, i.e., training VGG16BN with _completely random initialization_ on CIFAR10, and training ResNet18 with _pre-trained initialization from PyTorch_ on Flowers . The datasets are first uniformly split into two partitions, and then two models with corresponding initialization are separately trained or fine-tuned for 50 epochs. The checkpoints in the \(\{1,2,3,5,10,20,30,50\}\)-th epoch are stored. For checkpoints in the specific epoch, we denote the two models on the two partitions as \(_{A}\) and \(_{B}\), respectively. The interpolation accuracy of (1 - \(\))\(_{A}\) + \(_{B}\) on the test set is plotted in Fig. 11. The interpolation curve of VGG16BN on CIFAR10 indeed encounters a significant barrier, especially when the epoch is larger, e.g., \(E=50\). In contrast, the interpolation surpasses the individual models on Flowers, which is attributed to the pre-trained ResNet18. As an explanation, we calculate the sign consistency ratio between \(_{A}\) and \(_{I}\), \(_{B}\) and \(_{I}\), and \(_{A}\) and \(_{B}\), denoted as "SSR-IA", "SSR-IB", and "SSR-AB", respectively. \(_{I}\) means the initialization model. We also plot the gap of model interpolation and individual models when \(=0.5\), i.e., \((0.5_{A}+0.5_{B})\) - \(0.5((_{A})+(_{B}))\). The right of Fig. 11 clearly shows that the sign consistency ratio could almost perfectly reflect the tendency of the interpolation gap. Notably, the sign consistency ratio of models on Flowers is higher than \(0.95\), which means that _fine-tuning the pre-trained model does not change the parameter signs a lot, which facilitates the following parameter interpolation_. For better illustration and fair comparison, we also replace the pre-trained ResNet18 with a random initialized ResNet18, and we find the plots shown in Fig. 11 tend to be like the results of the random initialized VGG16BN. The previous work  points out that disagreement on the sign of a given parameter's values across models is a major source of interference during model merging. Although our finding is similar to the previous work, the motivation and specific explanation differs a lot.

### Regularizing the Sign Change in Federated Learning

Traditional machine learning models will encounter the challenges posed by "isolated data islands", e.g., the Non-I.I.D. data [54; 29]. FedAvg , as the most standard federated learning (FL) method, utilizes the parameter server architecture , and fuses collaborative models from local nodes without centralizing users' data. Specifically, the local clients receive the global model from the server and update it respectively on their devices using private data, and the server periodically _averages_ these models for multiple communication rounds. That is, FedAvg takes the simple parameter averaging to fuse local models. However, due to the Non-I.I.D. data and permutation invariance of DNNs [51; 67; 72], the local models could become too diverged to effectively merge. Numerous efforts are paid to regularize the local models so as not to go too far away from the global model, such as the proximal term proposed by FedProx , the contrastive regularization proposed by MOON , the dynamic regularization of FedDyn , and the position-aware neurons proposed by FedPAN . Inspired by our finding, we propose to regularize the sign change when updating local models, i.e.,

\[^{k}=^{k}_{}-((_{t}) (^{k}_{t})+(-_{t})(-^{k}_{t})),\] (1)

where \(t\) denotes the communication round and \(k\) is the index of client. \(^{k}_{}\) is the common cross-entropy loss of the \(k\)-th client, and \(_{t}\) is the global model received from the server. \(^{k}_{t}_{t}\) is the local model to be updated, and the loss could regularize the sign of \(^{k}_{t}\) to be close to that of \(_{t}\). Because obtaining the sign of parameters is not a continuous function, we therefore apply a sigmoid function to parameters as an approximation. We name this method FedSign and list its pseudo-code in the Appendix C. Compared with other regularization methods, our proposed FedSign is well-motivated because of our finding that interpolating sign-consistent models may lead to a flatter loss region.

Experimental studies are verified on CIFAR10  and CINIC10  that are commonly utilized in previous works [72; 51]. Decentralizing the training data of these datasets by a Dirichlet distribution could simulate the Non-I.I.D. scenes as in real-world FL. The Dirichlet alpha \(\) is utilized to control the Non-I.I.D. level, with a smaller \(\) representing a more rigorous heterogeneity between clients' data. We set \(\{10.0,1.0,0.5\}\) respectively. The number of clients is \(100\), and the total communication round is \(200\). During each round, a random set of \(10\%\) clients participate in FL and every client takes \(5\) epochs update on their individual data. After all communication rounds, we evaluate the model performance on a global test set on the server (i.e., the original test set of corresponding datasets). We select our hyperparameter \(\) from \(\{0.001,0.01,0.1\}\) and report the best results. For \(=10.0\), i.e., a relatively I.I.D. scene, a smaller \(\) is better. In contrast, \(=0.1\) or \(=0.01\) will be more proper for \(=0.5\). The performance comparison results are listed in Tab. 1. For FeaAvg and our proposed FedSign, we rerun the experimental studies five times and list the standard deviation of accuracies,

    & Dir. \(\) & FedAvg & FedProx & MOON & FedDyn & FedPAN & FedSign \\   & 10.0 & 81.53 \(\) 0.17 & 81.84 & 82.44 & 80.45 & 81.92 & **82.59**\(\) 0.09 \\  & 1.0 & 80.54 \(\) 0.11 & 80.42 & 80.12 & 79.78 & 80.30 & **80.76**\(\) 0.14 \\  & 0.5 & 77.69 \(\) 0.21 & 78.12 & 76.77 & 78.02 & 77.78 & **78.41**\(\) 0.35 \\   & 10.0 & 75.74 \(\) 0.24 & 76.58 & 76.25 & 76.37 & 76.84 & **77.05**\(\) 0.14 \\  & 1.0 & 72.19 \(\) 0.15 & 72.25 & 72.06 & 73.12 & 73.46 & **74.59**\(\) 0.20 \\   & 0.5 & 68.24 \(\) 0.32 & 69.11 & 68.55 & 69.01 & 70.14 & **70.63**\(\) 0.16 \\   

Table 1: Aggregation performance comparisons of FedSign with several popular FL algorithms.

Figure 11: The models fine-tuned from a pre-trained model have a higher sign consistency ratio. The top two sub-figures do not utilize pre-trained models, while the last one utilizes pre-trained ResNet18.

showing that the accuracy doesn't fluctuate very much. FedSign could surpass the compared methods, which shows the positive effects of regularizing the sign change in FL.

## 7 Limitations and Future Works

Although we provide theoretical insights to explain the interesting phenomenon, no formal proofs are provided to show the conditions and scopes that lead to asymmetric valleys. Additionally, this phenomenon is only investigated in the image classification tasks. Future research includes providing formal theoretical foundations for our findings and verifying them on more tasks. According to the analysis in Sect. 5, we advocate that this phenomenon is more likely to be applicable to DNNs that contain both the ReLU and softmax.

## 8 Conclusion

We explore and exploit the asymmetric valley of DNNs via numerous experimental studies and theoretical analyses. We systematically examine various factors influencing valley symmetry, highlighting the significant role of sign consistency between noise direction and the converged model. The findings offer valuable insights into practical implications, enhancing the understanding of model fusion. A novel regularization method is proposed for better model averaging in federated learning.