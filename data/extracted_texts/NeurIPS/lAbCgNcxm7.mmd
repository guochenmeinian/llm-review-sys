# DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening

Bowen Gao\({}^{1}\)

Equal contribution

Bo Qiang\({}^{2}\)

Equal contribution

Haichuan Tan\({}^{1}\)

Minsi Ren\({}^{3}\)

Yinjun Jia\({}^{4}\)

Minsi Lu\({}^{5}\)

Jingjing Liu\({}^{1}\)

Wei-Ying Ma\({}^{1}\)

\({}^{1,6}\)

Correspondence to lanyanyan@air.tsinghua.edu.cn

\({}^{1}\)Institute for AI Industry Research (AIR), Tsinghua University

\({}^{2}\)Department of Pharmaceutical Science, Peking University

\({}^{3}\)Institute of Automation, Chinese Academy of Sciences

\({}^{4}\)School of Life Sciences, Tsinghua University

\({}^{5}\)Department of Pharmaceutical Science, Tsinghua University

\({}^{6}\)Beijing Academy of Artificial Intelligence

###### Abstract

Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensive experiments show that DrugCLIP significantly outperforms traditional docking and supervised learning methods on diverse virtual screening benchmarks with highly reduced computation time, especially in zero-shot setting. The code for DrugCLIP is available at https://github.com/bowen-gao/DrugCLIP.

## 1 Introduction

Virtual screening is a crucial computer-aided drug discovery (CADD) technique that uses computational methods  to search for candidate drug structures from compound libraries, aiming to identify molecules most likely to bind to a target (_e.g._, a protein receptor or enzyme). Common practice follows the wisdom of "bigger is better" : the larger the search library, the better chance to find a matching drug candidate. For example, statistics show that increasing the data size from \(10^{5}\) to \(10^{8}\) leads to a significant jump in the number of true ligands among top-1000 results .

Molecular docking [11; 43; 41] is currently the dominant virtual screening method, which models protein-molecule interaction by a quantitative score correlated with the free energy of binding. The computation of such scores heavily depends on molecule orientations and conformations sampling, which is time-consuming and impractical when dealing with large libraries in the magnitude of billions . As estimated by , given a standard computating rate of 10 seconds per compound ona single CPU core, it takes 3000 years to complete screening 10 billion compounds using commercial docking methods (costing over 800k dollars). Consequently, docking methods are gravely limited by their high computational cost and slow inference speed.

As molecule libraries continue to grow, new high-throughput virtual screening methods are in pressing demand. Supervised learning algorithms  such as regression  and classification  have been investigated for binding-affinity prediction. However, these supervised methods usually require carefully labeled data samples for training, thus struggling with poor generalization . In addition, the scarcity of reliable negative samples imposes restrictions on model performance. As a result, they still underperform current docking methods.

Rethinking the problem of virtual screening, we find that the key issue is to identify which molecules are likely to bind with a protein pocket, instead of determining the accurate binding-affinity score (the goal of prediction in regression/classification models) or binding pose (the goal of docking). Following this thought, we recast virtual screening as an information retrieval task, _i.e._, given a protein pocket as the query, we aim to retrieve from a large-scale molecule library the most relevant molecules with the highest probability of binding to the target pocket. In this new perspective, virtual screening is boiled down to a similarity matching problem between proteins and molecules.

To this end, we introduce DrugCLIP, a dense retrieval approach (inspired by CLIP ) that computes a contrastive loss between two separate pre-trained encoders to maximize the similarity between a protein-molecule pair, if they have a binding affinity, and minimize it otherwise. Compared with supervised learning methods, our contrastive learning approach enjoys several advantages. Firstly, the objective of finding the matching relations between proteins and molecules is naturally in accordance with the formulation of the virtual screening task. Secondly, the designed contrastive loss relieve the dependency on explicit labeling of binding affinity, and facilitates the usage of large-scale unlabeled data beyond densely annotated small datasets such as PDBBind . Thus, we extend the ability of our model with a large pool of protein-molecule pairs by utilizing BioLip  and ChEMBL  datasets for training. We further introduce a biological-knowledge inspired augmentation method, _HomoAug_, which creates protein-molecule pairs based on protein homology evolutions. Lastly, the dense retrieval setting allows for offline pre-computation of protein and molecule encodings, bringing high efficiency to online inference and promising high-throughput virtual screening on billions of molecules.

Experiments on two challenging virtual screening benchmarks, DUD-E  and LIT-PCBA , demonstrate that zero-shot performance of our model surpasses most deep learning baselines that carefully finetune on labeled data. We also conduct a human evaluation to compare DrugCLIP with Glide, a commercial docking system widely used by pharmacology experts. In 80% cases, judges prefer the selection of top-10 molecules from our method over Glide. Furthermore, since DrugCLIP indiscriminately models the similarity between protein and molecule, it can be extended to other important tasks in drug discovery such as 'target fishing', where protein candidates are ranked for a given molecule. DrugCLIP also outperforms docking methods on these benchmarks.

Our main contributions are summarized as follows:

* To our best knowledge, this is the first effort to position large-scale virtual screening as a dense retrieval problem, which enables ultra-fast screening over billion-scale chemical libraries for candidate search by storing pre-computed molecule embeddings offline.
* We propose a novel contrastive learning framework that learns a generic joint representation of proteins and molecules, which can be applied to molecule-pocket pairing tasks. Novel data augmentation strategy and training techniques are also introduced.
* DrugCLIP, with its impressive zero-shot performance on virtual screening benchmarks, well addresses poor generalization and low efficiency issues faced by docking and learning-based screening methods.

## 2 Related Work

There are mainly two schools of virtual screening methods, molecular docking and supervised learning. Molecular docking is a computational technique that predicts the binding energy, optimal orientation, and conformation of a small molecule ligand within a protein binding site . It uses sampling algorithms such as genetic algorithms  and Monte Carlo [11; 43] to generate a set of candidate ligand poses, by exploring the conformational space of the ligand and the protein receptor. These candidate poses are then evaluated by molecule-protein scoring functions such as empirical force fields  to assess their binding affinity. This iterative process continues until convergence, which is computationally demanding.

To accelerate the prediction process, supervised learning methods have emerged as an alternative to the iterative refining process in docking. A recent work  proposes pocket pretraining to find ligands for similar pockets by only utilizing information from one side of the pocket-ligand pairs. By training on given binding-affinity labels, regression models such as DeepDTA , OnionNet , GraphDTA  and SG-CNN  learn the mapping between protein-molecule representations by first predicting the binding affinity for every protein-molecule pair, then ranking them to determine top candidates. However, these models suffer from high false-positive rates, due to the lack of negative binding-affinity data. Another way is to use predefined rules (_e.g._, DUD-E ) to obtain negative samples and train a classifier to discriminate positive and negative protein-molecule pairs (DrugVQA , AttentionSiteDTI ). Previous work has observed poor generalization in these methods. For example, Wang and Dokholyan  suggests that models trained on DUD-E  cannot be transferred to other classification benchmarks.

Concurrently, several studies have employed contrastive learning objectives in the context of virtual screening. Singh et al.  utilized a protein language model and rule-based molecule fingerprints for representation, while CoSP  opted for chemical similarity in their negative sampling approach. In the meantime, we propose a more holistic and flexible solution by leveraging an equivariant 3D model, efficient dense retrieval, and novel data augmentation techniques.

## 3 DrugCLIP Framework

### Overview

To formulate the problem, we denote the protein pocket of interest as \(p\), and a set of \(n\) small molecules is represented by \(=\{m_{1},m_{2},,m_{n}\}\). The objective of virtual screening is to identify the top \(k\) candidates with the highest probability of binding to the target pocket. This selection process is typically guided by a scoring function \(s(,)\), which assesses the pairwise data between the pocket \(p\) and each candidate molecule \(m_{i}\). The scoring function can be derived from techniques such as docking simulations or supervised learning models, which perform ranking and selection of the most promising candidates based on their likelihood of binding to the target pocket.

To view virtual screening as a dense retrieval task, we treat the pocket as the query to retrieve relevant molecules from the given library. The overall framework is illustrated in Figure 1. First, two separate encoders are trained to learn the representations of protein pockets (abbreviated as proteins hereafter) and molecules. Then, the similarity between each protein-molecule pair is computed and a contrastive learning objective is utilized to discriminate between positive and negative pairs. All the parameters in the encoders and similarity functions are trained jointly.

### Protein and Molecule Encoders

Diverse representation learning methods can be used as protein and molecule encoders. In this paper, we follow the encoder architecture of UniMol , a powerful 3D encoder pre-trained with large-scale unsupervised data. Here we briefly introduce the encoding process.

Firstly, both molecules and protein pockets are tokenized to atoms. A molecule with \(L\) tokens is denoted as a feature vector \(x^{m}=\{c_{m},t_{m}\}\), where \(c_{m}^{L 3}\) represents the atom coordinates and \(t_{m}^{L}\) represents the atom types. The same setting is applied to obtain pocket features, denoted as \(x^{p}=\{c_{p},t_{p}\}\).

As described in UniMol , the encoder is a SE(3) 3D transformer that accepts tokenized atom features as input. To preserve SE(3) invariance for embedding the molecular structure, the 3D coordinate features are utilized as geometric distances. Specifically, the pairwise representation \(q_{ij}^{0}\) is initialized based on the distance between each pair of atoms. For each transformer layer \(l\), the self-attention mechanism for learning atom representation is defined in Equation 1. The pairwise representation serves as a bias term in the attention mechanism, encoding 3D features into atom representations. The update rules between adjacent transformer layers are also defined in Equation 1.

\[(Q_{i}^{l},K_{j}^{l},V_{j}^{l})=(^{l}(K_{j}^ {l})^{T}}{}+q_{ij}^{l})V_{j}^{l},\;\;q_{ij}^{l+1}=q_{ ij}^{l}+^{l}(K_{j}^{l})^{T}}{}.\] (1)

Inspired by BERT, we randomly mask atom types and pretrain our model by predicting the masked atom types. Besides, we introduce random noise to corrupt atom coordinates and pretrain the model to reconstruct the original coordinates. Specifically, uniform noises of [-1, 1] are added to 15% of atom coordinates, where the pair-distance prediction heads estimate uncorrupted distances and the SE(3)-equivariant head directly predicts correct coordinates. Detailed implementations are described in the Appendix B. A special atom [CLS] with coordinates at the center of all atoms is added to output the representation of the corresponding protein and molecule. Specifically, we denote the protein encoder as \(g_{}\) and molecule encoder as \(f_{}\). Then, the representation of protein \(x^{p}\) and molecule \(x^{m}\) are defined as \(g_{}(x^{p})\) and \(f_{}(x^{m})\), correspondingly.

### Contrastive Learning Objective

To conduct the contrastive learning process, we first need to obtain the similarity measurements between each protein-molecule pair. Following previous work, both dot product and cosine similarity can be adopted as the similarity functions. For example, when using dot product, the similarity score of \((x_{i}^{p},x_{j}^{m}), i,j[1,N]\) can be written as:

\[s(x_{i}^{p},x_{j}^{m})=g_{}(x_{i}^{p})^{T} f_{}(x_{j}^{m}),\] (2)

while cosine similarity can be obtained by normalization.

In the field of virtual screening, positive pairs of binding protein and molecule are usually provided, with limited true negative pairs. Therefore, we need to construct negative pairs for the contrastive objective. Here we use an in-batch sampling strategy similar to CLIP . Specifically, given a batch of paired data \(\{(x_{k}^{p},x_{k}^{m})\}_{k=1}^{N}\) with batch size \(N\), we extract a list of proteins \(\{x_{k}^{p}\}_{k=1}^{N}\) and a list of corresponding molecules \(\{x_{k}^{m}\}_{k=1}^{N}\). Combining them together results in \(N^{2}\) pairs \((x_{i}^{p},x_{j}^{m})\) where \(i,j[1,N]\). When \(i=j\) it is a positive pair, and when \(i j\) it is a negative pair.

Please note that the in-batch negative construction is intrinsically based on a simple assumption that, if a certain pair of protein and molecule has been tested as having a binding relation, it is likely that they have a negative binding relation with other molecule/protein. This assumption is reasonable as the true distribution of positive and negative molecules exhibits a sharp contrast, with a proportion significantly smaller than 0.1% .

Formally, we introduce two losses: Pocket-to-Mol loss and Mol-to-Pocket loss. The former describes the likelihood of ranking its binding molecules before other molecules for a given protein \(x_{k}^{p}\):

\[_{k}^{p}(x_{k}^{p},\{x_{i}^{m}\}_{i=1}^{N})=-^{p},x_{k}^{m})/)}{_{i}(s(x_{k}^{p},x_{i}^{m})/)},\] (3)

Figure 1: An illustration of the training procedure. Molecule conformations are generated by RDkit simulation, and pocket data are augmented with HomoAug. At each training iteration, sampled 3D molecule and 3D pocket representations are learned with a contrastive objective.

while the latter is the likelihood of ranking its binding targets for a given molecule \(x_{k}^{m}\), and is defined as:

\[_{k}^{m}(x_{k}^{m},\{x_{k}^{p}\}_{i=1}^{N})=-^{p},x_{k}^{m})/)}{_{i}(s(x_{i}^{p},x_{k}^{m})/)}.\] (4)

In the above two equations, \(\) represents the temperature parameter that controls the softmax distribution, which has been widely utilized in previous representation learning methods.

Combining the two losses, we obtain the final loss for a mini-batch:

\[=_{k=1}^{N}(_{k}^{p}+_{k}^{m}).\] (5)

### Training and Inference

Virtual screening with DrugCLIP contains two phases. In the offline phase, embeddings of each molecule are obtained by DrugCLIP encoders \(f_{}\). These embedding vectors are then stored in memory for later-stage online retrieval. Specifically, for a given query protein pocket, it is first encoded into an embedding vector using the trained protein encoder \(g_{}\). We then measure the similarity between the encoded pocket vector and all the embedding vectors of candidate molecules (dot product or cosine similarity). Finally, we proceed to select the top-\(k\) molecules from the candidate pool based on their similarity scores. Notably, our method offers a distinct advantage compared to other supervised learning screening frameworks. While other methods involve complex neural network computations for the scoring functions during the online screening phase, our approach capitalizes on the pre-computed and cached candidate embedding vectors. Consequently, the only computation required is the high-speed dot product calculation. This novel design allows for rapid screening of a large number of candidates, without incurring additional computational overhead. For a detailed time analysis, please refer to Section 4.3.

#### Constructing Training Data

We use three datasets for training: PDBBind , BioLip , and ChEMBL . PDBBind is a standard database used in docking and binding-affinity prediction. It consists of experimentally measured protein-ligand complex structures along with their binding-affinity labels, from which true positive protein-molecule pairs with accurate structures can be extracted. We use PDBBind 2019, which includes over 17,000 protein-molecule complexes with binding-affinity data covering a wide range of chemical space and protein families. We use the general set for training and the refined set for validation.

BioLip is a dataset updated weekly by a standard data mining workflow that extracts complex structure data from PDB. We filter out all complexes that contain peptides, DNA, RNA, and single ions, and obtain 122861 protein-molecule pairs, much larger than PDBBind.

DrugCLIP model can also use known receptor-ligand pairs without their binding structures. From the ChEMBL  dataset, we filter out proteins with only one known binding pocket. Then we pair the pocket with all positive binders in the ChEMBL database. We hypothesize (supported by domain experts ) that assayed ligands dominantly bind to the known pocket in solved structures, and our model can tolerate introduced noise of this filtration protocol.

Since our deep-learning-based method is insensitive to occasional inaccuracies in coordinates and can be trained with raw element types. Therefore, only minimal cleaning-ups are performed for the protein structures in the above mentioned datasets to remove irrelevant molecules like water.

#### Biological Data Augmentation

Directly applying common data augmentation techniques to augment biological data is infeasible, as introducing noise or perturbations to pocket or molecule data can result in unstable or chemically incorrect structures, rendering the augmented data unreliable, especially for virtual screening . To address this challenge, we propose a new augmentation method called HomoAug that takes into account the biological significance of the data. It utilizes the concept of homologous proteins in biology, to combine ligands from PDBbind  with homologous proteins corresponding to their pockets, thereby generating new training data (Figure 2).

#### Training-test Inconsistency

There exists a training-test inconsistency problem in virtual screening. This is because holo structures in training data are depicted after binding, which differs from their apo structures. Previous methods usually use docking software to obtain estimated binding structure. For better efficiency, we propose to use the chemical simulation package , RDkit to generate noisy data as input. For a given protein \(x^{p}=\{c_{p},h_{p}\}\) and molecule \(x^{m}=\{c_{m},h_{m}\}\), we denote \(c\) as coordinates and \(h\) as atom types. Inaccurate molecule coordinates \(c_{m}+\) simulated by RDkit are used as noisy input \(_{m}=\{c_{m}+,h_{m}\}\). It should be noted that our model adopts a dual-tower architecture, while previous 3D supervised learning methods are mainly based on a single tower [52; 16]. The following Proposition proves that the scoring function of DrugCLIP is more robust than those obtained by supervised learning methods. In a more general setting, we suggest that dual-tower models based on 3D encoders enjoy more generalization by decoupling the relative 3D distances from the prediction. By learning molecule-protein interactions from noisy unbind molecule conformations, our model addresses the consistency issue between training and testing.

**Proposition 1**.: _Denote the scoring function of a supervised learning method as \(k_{}\), we have,_

\[_{ 0}\{s(_{m},x_{p})-s(x_{m},x_{p})\}=0\] (6)

\[_{ 0}\{k_{}(h_{p},h_{m},c_{p},_{m})-k_{ }(h_{p},h_{m},c_{p},c_{m})\} 0\] (7)

_The proof is provided in the Appendix A_

## 4 Experiments

We first introduce evaluation metrics. Although AUROC (area under the receiver operating characteristic curve) is commonly used for classification, it has been criticized for being unsuitable for virtual screening.  This is because the target of virtual screening is to select a small fraction of molecules from a large pool,resulting in a significantly low false positive rate (FPR) in this scenario. However, AUROC is calculated by averaging the FPR from 0 to 1. To overcome this, we also use BEDROC (Boltzmann-enhanced discrimination of ROC), Enrichment Factor(EF) and ROC enrichment metric (RE) for evaluation. BEDROC incorporates exponential weights that assign greater importance to early rankings. EF and RE are two widely used metrics for virtual screening (detailed definitions in Appendix B).

### Evaluation on DUD-E Benchmark

DUD-E  is one of the most popular virtual screening benchmarks. It contains 102 proteins with 22,886 bio-active molecules, for which 50 topological dissimilar decoys that possess matched physicochemical properties are retrieved from the ZINC database.

Figure 2: Illustration of HomoAug’s pipeline. The pocket protein instances from PDBBind are searched for homologous counterparts in the AlphaFold Protein Structure Database. Then the TMalign method is employed to achieve structural alignment between the homologous protein and the original protein. Following a filtering process, the homologous protein is combined with the ligand to create an augmented pocket-ligand pair.

In the zero-shot setting, we compare with docking and other learning methods. Since all learning methods only use PDBBind for training, we also train our model, named DrugCLIP\({}_{}\), on PDBBind for fair comparison. We exclude all the targets present in DUD-E from our training set to ensure zero-shot learning. For the fine-tuning setting, further tuning on DUD-E is required. We follow the same split and test approach in Jones et al. . The finetuned model is named DrugCLIP\({}_{}\).

Table 1 and 2 summarize the results in zero-shot and fine-tuning setting, respectively. From Table 1, we can see that our model outperforms both docking and learning methods in zero-shot setting by a large margin. Besides, our model is the only one outperforming traditional molecule docking methods. As for the comparison in finetuning setting, DrugCLIP\({}_{}\) model, although achieving a lower AUROC compared to other finetuned models, outperforms them in terms of RE at 0.5%, 1%, and 2% levels. This indicates that our model is particularly well-suited for virtual screening tasks that prioritize the identification of hit molecules at a small fraction of the entire dataset. Surprisingly, we found that even DrugCLIP\({}_{}\) outperforms some of the supervised-learning methods. These results show that DrugCLIP harnesses great strengths, especially in zero-shot setting, which approximates virtual screening in real-world applications.

### Evaluation on LIT-PCBA Benchmark

LIT-PCBA is a much more challenging virtual screening benchmark, proposed to address the biased data problem faced by other benchmarks, _e.g._, DUD-E. Based on dose-response PubChem bioassays, the LIT-PCBA dataset consists of 15 targets and 7844 experimentally confirmed active and 407,381 inactive compounds.

For fair comparison, we also used PDBBind as training data. Since all baselines are in a zero-shot setting, we exclude all the targets present in LIT-PCBA from our training set.

As shown in Table 3, DrugCLIP consistently outperforms commercial docking methods (Surflex and Glide-SP). Despite not achieving the highest AUROC, DrugCLIP excels in the more critical BEDROC and EF scores for virtual screening, surpassing all other baselines by a large margin. Additionally, all methods demonstrate lower performances on LIT-PCBA compared to DUD-E, indicating the greater challenge posed by LIT-PCBA for virtual screening.

   & AUROC (\%) & BEDROC (\%) &  \\  & & 0.5\% & 1\% & 5\% \\  Glide-SP  & 76.70 & 40.70 & 19.39 & 16.18 & 7.23 \\ Vina  & 71.60 & - & 9.13 & 7.32 & 4.44 \\  NN-score  & 68.30 & 12.20 & 4.16 & 4.02 & 3.12 \\ RFscore  & 65.21 & 12.41 & 4.90 & 4.52 & 2.98 \\ Pafnucy  & 63.11 & 16.50 & 4.24 & 3.86 & 3.76 \\ OnionNet  & 59.71 & 8.62 & 2.84 & 2.84 & 2.20 \\ Planet  & 71.60 & - & 10.23 & 8.83 & 5.40 \\  DrugCLIP\({}_{}\) & **80.93** & **50.52** & **38.07** & **31.89** & **10.66** \\  

Table 1: Results on DUD-E in zero-shot setting.

   & AUROC (\%) &  \\  & & 0.5\% & 1\% & 2\% & 5\% \\  COSP & 90.10 & 51.05 & 35.98 & 23.68 & 12.21 \\ Graph CNN & 88.60 & 44.41 & 29.75 & 19.41 & 10.74 \\ DrugVQA & **97.20** & 88.17 & 58.71 & 35.06 & **17.39** \\ AttentionSiteDTI & 97.10 & 101.74 & 59.92 & 35.07 & 16.74 \\  DrugCLIP\({}_{}\) & 80.93 & 73.97 & 41.79 & 23.68 & 11.16 \\ DrugCLIP\({}_{}\) & 96.59 & **118.10** & **67.17** & **37.17** & 16.59 \\  

Table 2: Results on DUD-E in finetuning setting.

### Efficiency Analysis for Large-scale Virtual Screening

DrugCLIP offers a significant advantage in terms of ultra-fast speed. To evaluate it on real-world databases, we analyze two scenarios: performing one-time virtual screening with a specific target on various libraries, and conducting multi-time screening with multiple targets on a fixed library. Here we use Planet  as a representative of ML scoring function (MLSF) based supervised learning methods.

For the first scenario, as illustrated in Figure.(a)a, when all candidate molecules are not pre-encoded, our method requires a comparable amount of time to other learning methods. However, if all candidate molecules are pre-encoded into embeddings and stored in memory, DrugCLIP can perform virtual screening in less than 10,000 seconds (approximately 30 hours) for Enamine, which comprises 6 billion molecules. This significant reduction in time demonstrates the efficiency and scalability of our method when leveraging pre-encoded molecules.

Results for the second scenario are presented in Figure.(b)b. Here, the search library is fixed and all molecules are pre-encoded and stored. When there are only 10 targets, the time difference between DrugCLIP and Planet  is approximately 10 days, which is manageable. However, as the number of targets increases to 600, the time difference expands to 2 years. These findings highlight the scalability challenge faced by existing learning-based screening methods when dealing with a large number of targets, and the huge efficiency advantage of DrugCLIP.

### Ablation Studies

We conduct an ablation study to evaluate the two training techniques: employing HomoAug for data augmentation, and utilizing RDkit conformations to replace the original molecule conformation. Results are summarized in Table 5. We can see that performance improves by adding each of these techniques.

To clarify whether the surprisingly good results of DrugCLIP are attributed to the contrastive learning modules, or instead to the pre-trained encoder of UniMol, we further introduce DrugBA, a regression

Figure 3: Time analysis of virtual screening with DrugCLIP, Glide, and a representative learning method. (a) virtual screening time on a single target with different molecule libraries. (b) virtual screening time on multiple targets with a fixed molecule library.

   & AUROC (\%) & BEDROC (\%) &  \\  & & 0.5\% & 1\% & 5\% \\  Surflex  & 51.47 & - & - & 2.50 & - \\ Glide-SP  & 53.15 & 4.00 & 3.17 & 3.41 & 2.01 \\  Planet  & 57.31 & - & 4.64 & 3.87 & 2.43 \\ Gnina  & **60.93** & 5.40 & - & 4.63 & - \\ DeepDTA  & 56.27 & 2.53 & - & 1.47 & - \\ BigBind  & 60.80 & - & - & 3.82 & - \\  DrugCLIP & 57.17 & **6.23** & **8.56** & **5.51** & **2.27** \\  

Table 3: Results on LIT-PCBA.

model utilizing the same encoder architecture as DrugCLIP. We train both models on PDBBind, and report their results in zero-shot setting, on both DUD-E and LIT-PCBA. Table 4 shows that DrugBA consistently underperforms DrugCLIP. Especially, if we compare the results with Table 1 and 3, DrugBA even underperforms existing docking methods. This further demonstrates the necessity and power of contrastive learning.

To visually demonstrate the disparity between binding-affinity prediction and contrastive learning, Figure 4 presents a visualization of the embeddings of DrugCLIP and DrugBA. For this visualization, 20 molecules and 20 pockets are randomly chosen from the CASF-2016 dataset. By combining these pairs using multiplication, we obtain 400 combined embeddings for further analysis. When the embeddings are labeled and colored by molecule index or pocket index, we can see that the embeddings produced by DrugBA exhibit a clustering pattern, suggesting that the model tends to assign similar scores to different pockets for a given molecule. In contrast, the embeddings produced by DrugCLIP exhibit no clustering, highlighting its ability to learn meaningful embeddings and mitigate the serious spurious bias in traditional binding-affinity prediction objectives. Thus, DrugCLIP is also able to outperform docking methods in target fishing task, which aims to find relative pockets given a specific molecule. Detailed results are in Appendix C.

### Human Evaluation

We conduct a human evaluation experiment to compare our method with the most widely used screening software, Glide, to test DrugCLIP as a useful tool for pharmacological experts. We first conduct an experiment on exploring the limits of DrugCLIP, by comparing versions of DrugCLIP trained on different datasets. Model trained on PDBBind is named DrugCLIP-S, and similarly DrugCLIP-M and DrugCLIP-L for models trained on BioLip  and ChEMBL , respectively. All three models are augmented with PDBBind.

Figure 5 shows that DrugCLIP-M performs the best on DUD-E, while DrugCLIP-L is the best on LIT-PCBA. Considering that the setting of LIT-PCBA is much more challenging and real, we choose DrugCLIP-L for human evaluation (denoted as DrugCLIP in following discussion for consistency).

   &  &  &  \\  Rdkit Confs & & & & 0.5\% & 1\% & 5\% \\  ✗ & ✗ & 77.10 & 38.13 & 29.84 & 23.86 & 8.69 \\ ✓ & ✗ & 81.05 & 45.81 & 35.91 & 29.12 & 10.04 \\ ✓ & ✓ & 80.93 & 50.52 & 38.07 & 31.89 & 10.66 \\  

Table 5: Ablation results.

Figure 4: TSNE visualization of embeddings generated by combining pocket-molecule pairs. (a) and (c) represent embeddings produced by DrugCLIP, with (a) colored according to molecules and (c) colored according to pockets. (b) and (d) display embeddings generated by _DrugBA_, with (b) colored based on molecules and (d) colored based on pockets.

   &  &  &  \\  Rdkit Confs & & & & 0.5\% & 1\% & 5\% \\  ✗ & ✗ & 77.10 & 38.13 & 29.84 & 23.86 & 8.69 \\ ✓ & ✗ & 81.05 & 45.81 & 35.91 & 29.12 & 10.04 \\ ✓ & ✓ & 80.93 & 50.52 & 38.07 & 31.89 & 10.66 \\  

Table 5: Ablation results.

We select five targeting protein pockets and perform virtual screening with both Glide and DrugCLIP on the ChemDiv compound database for each target of interest. Then, the top-50 molecules from each method were selected and shuffled to create a list of 100 molecules. Domain experts who have drug design experience for specific targets are required to independently choose top-10 molecules from each 100 molecules. The results show that domain experts are inclined to choose the candidate structures given by DrugCLIP rather than Glide in 4 out of 5 cases, indicating great potential of DrugCLIP as an useful tool for human experts. Detailed results are reported in Appendix C.

## 5 Conclusion and Future Work

In this paper, we introduce "DrugCLIP", a novel approach for efficient and accurate virtual screening. Our method leverages contrastive learning to align the representations of binding pockets and molecules. By achieving state-of-the-art results and surpassing docking methods across diverse virtual screening benchmarks and tasks, DrugCLIP not only improves screening accuracy but also significantly reduces the time required for large-scale virtual screening. This opens up the possibility of expanding the search library to billions of compounds. There are several avenues for future research, such as designing further data augmentation techniques and investigating the use of more detailed atom-level interactions, which is explored in Appendix C.