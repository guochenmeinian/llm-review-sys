# Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization

Junlin He

The Hong Kong Polytechnic University

Hong Kong SAR, China

junlinspeed.he@connect.polyu.hk

&Jinxiao Du

The Hong Kong Polytechnic University

Hong Kong SAR, China

jinxiao.du@connect.polyu.hk

&Wei Ma

The Hong Kong Polytechnic University

Hong Kong SAR, China

wei.w.ma@polyu.edu.hk

Corresponding author.

###### Abstract

Self-supervised learning (SSL) has rapidly advanced in recent years, approaching the performance of its supervised counterparts through the extraction of representations from unlabeled data. However, dimensional collapse, where a few large eigenvalues dominate the eigenspace, poses a significant obstacle for SSL. When dimensional collapse occurs on features (e.g. hidden features and representations), it prevents features from representing the full information of the data; when dimensional collapse occurs on weight matrices, their filters are self-related and redundant, limiting their expressive power. Existing studies have predominantly concentrated on the dimensional collapse of representations, neglecting whether this can sufficiently prevent the dimensional collapse of the weight matrices and hidden features. To this end, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weight matrices, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances the performance of SSL methods across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures. Our code will be released at https://github.com/Umaruchain/OR_in_SSL.git.

## 1 Introduction

Self-supervised learning (SSL) has established itself as an indispensable paradigm in machine learning, motivated by the expensive costs of human annotation and the abundant quantities of unlabeled data. SSL endeavors to produce meaningful representations without the guidance of labels. Recent developments have witnessed joint-embedding SSL methods achieving, or even exceeding the supervised counterparts (Misra and Maaten, 2020; Bardes et al., 2022; Caron et al., 2020; Chen et al., 2020; Chen et al., 2020; Chen et al., 2020; Chen et al., 2021; HaoChen et al., 2021; He et al., 2020; He and Ozay, 2022; Jing et al., 2021; Li et al., 2020; Jing et al., 2020; Balestriero et al., 2023; Grill et al., 2020; Zbontar et al., 2021; Chen et al., 2021). The efficacy of these methods hinges on two pivotal principles: 1) the ability to learn augmentation-invariant representations, and 2) the prevention of complete collapse, where all inputs are encoded to a constant vector.

Efforts to forestall complete collapse have been diverse, including contrastive methods with both positive and negative pairs (He et al.2020, Chen, Kornblith, Norouzi & Hinton2020, Chen et al.2021) and non-contrastive methods utilizing techniques such as self-distillation (Caron et al.2021, Grill et al.2020, Chen & He2021), clustering (Caron et al.2018, 2020, Pang et al.2022) and feature whitening (Bardes et al.2022, Zbontar et al.2021, Weng et al.2022, 2023). Notwithstanding, these methods are prone to dimensional collapse, a phenomenon where **a few large eigenvalues dominate the eigenspace**. Dimensional collapse can occur on both features (e.g. hidden features and representations) and weight matrices.

To prevent dimensional collapse of representations, as depicted in Figure 1, existing methods include modifying representations in downstream tasks (He & Ozay2022), whitening representations directly (i.e. removing the projector) (Jing et al.2021), incorporating regularizers on representations during pretraining (Huang et al.2024, Hua et al.2021). However, whether they sufficiently prevent the dimensional collapse of weight matrices and hidden features remains unknown (i.e., no theoretical guarantee) (Pasand et al.2024). In Appendix A.1, we further demonstrate that whitening representations directly to eliminate the dimensional collapse of representations cannot adequately remove the dimensional collapse of weight matrices.

To address these challenges, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. It is natural that OR prevents the dimensional collapse of weight matrices as it ensures weight matrices orthogonality, keeps the correlation between its filters as low as possible, and lets each filter have a norm of 1. For features (e.g. hidden features and representations), orthogonal weight matrices can promote uniform eigenvalue distributions and thus prevent the domination of eigenspaces by a

Figure 1: Illustration of dimensional collapse in SSL. We use one augmented input \(X_{aug1}\) as an example: we assume that the encoder contains two basic blocks, each containing a linear operation (e.g., a linear layer or convolutional layer) and an activation function. Dimensional collapse can occur in weight matrices (\(W_{1},W_{2}\)), hidden features, and the finally obtained representations. Existing methods act directly on representations and expect to affect hidden features and weight matrices indirectly, which has no guarantee in theory; our method directly constrains weight matrices and indirectly influences hidden features and representations, which can be guaranteed by theoretical analysis.

limited number of large eigenvalues, as theoretically substantiated by Huang et al. (2018), Yoshida and Miyato (2017), Rodriguez et al. (2016).

In our study, we introduce and assess the effects of two leading orthogonality regularizers, Soft Orthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), on SSL methods. We examine their integration with 13 modern SSL methods from Solo-learn and LightSSL, spanning both contrastive and non-contrastive methods (Chen, Fan, Girshick and He, 2020, Chen et al., 2021, Grill et al., 2020, Caron et al., 2021, Dwibedi et al., 2021). Our findings indicate a consistent enhancement in linear probe accuracy on CIFAR-100 using both CNNs and Transformer-based architectures and OR exhibits a good scaling law at the model scale. Furthermore, when applied to BYOL trained on IMAGENET-1k, OR significantly improves the downstream performance on both classification and object detection tasks, suggesting its applicability to large-scale SSL settings. Remarkably, OR achieves these enhancements without necessitating modifications to existing SSL architectures or hyperparameters.

In summary, we present three major contributions:

* We systematically study the phenomenon of dimensional collapse in SSL, including how feature whitening and network depth affect the dimensional collapse of weight matrices and hidden features.
* We first time introduce orthogonal regularization (OR) as a solution to prevent the dimensional collapse of weight matrices, hidden features, and representations during SSL pretraining.
* Our extensive experimental analysis demonstrates OR's substantial role in enhancing the performance of state-of-the-art joint-embedding SSL methods with a wide spectrum of backbones.

## 2 Related Work

### Self-Supervised Learning

Self-supervised Learning (SSL) aims to learn meaningful representations from unlabeled data. Existing SSL methods can be broadly classified into two categories: generative and joint embedding methods. This paper concentrates on joint-embedding methods, which learn representations by aligning the embeddings of different augmented views of the same instance. Joint-embedding methods further subdivide into contrastive and non-contrastive methods. Contrastive methods, such as those proposed by He et al. (2020), Chen, Kornblith, Norouzi and Hinton (2020), Chen et al. (2021), treat each sample as a distinct class and leverage the InfoNCE loss (Oord et al., 2018) to bring representations of positive pairs closer together while distancing those of negative pairs in the feature space. These methods generally require a substantial number of negative samples for effective learning. In contrast, non-contrastive methods eschev the use of negative samples. They instead employ various techniques such as self-distillation (Caron et al., 2021, Grill et al., 2020, Chen and He, 2021), clustering (Caron et al., 2018, 2020, Pang et al., 2022) and feature whitening (Bardes et al., 2022, Zbontar et al., 2021, Weng et al., 2022, 2023). Our empirical findings indicate that incorporating OR enhances the performance of both contrastive and non-contrastive SSL methods. The exploration of its effects on generative methods remains for future work.

### Dimensional Collapse in SSL

Dimensional collapse plaques both generative and joint embedding SSL methods (Zhang et al., 2022, Jing et al., 2021, Zhang et al., 2021, Tian et al., 2021). To prevent the dimensional collapse of representations, existing work has typically focused on imposing constraints on the covariance matrix of the representations, including modifying representations in downstream tasks (He and Ozay, 2022), removing the projector (Jing et al., 2021), incorporating regularizers on representations during pretraining (Huang et al., 2024, Hua et al., 2021). However, these strategies face challenges such as performance degradation upon removing the projector, not addressing collapse during pre-training, and failing to prevent dimensional collapse in hidden features and weight matrices within the encoder (referred to A.1). This motivates us to regularize the weight matrices of the DNNs directly in SSL.

### Orthogonality Regularization

Orthonormality regularization, which is applied in linear transformations, can improve the generalization and training stability of DNNs (Xie et al. 2017, Huang et al. 2018, Saxe et al. 2013). OR has demonstrated its effects on tasks including supervised/semi-supervised image classification, image retrieval, unsupervised inpainting, image generation, and adversarial training (Bansal et al. 2018, Balestriero et al. 2018, Balestriero and Baraniuk 2020, Xie et al. 2017, Huang et al. 2018). Efforts to utilize orthogonality in network training have included penalizing the deviation of the gram matrix of each weight matrice from the identity matrix (Xie et al. 2017, Bansal et al. 2018, Balestriero et al. 2018, Kim and Yun 2022) and employing orthogonal initialization (Xie et al. 2017, Saxe et al. 2013). For more stringent norm preservation, some studies transform the convolutional layer into a doubly block-Toeplitz (DBT) matrix and enforce orthogonality (Qi et al. 2020, Wang et al. 2020).

In this work, we first time investigate the efficacy of two orthogonality regularizers, Soft Orthogonality (SO) and Spectral Restricted Isometry Property (SRIP) in SSL (Bansal et al. 2018). These regularizers aim to minimize the distance between the gram matrix of each weight matrix and the identity matrix--measured in Frobenius and spectral norms, respectively.

## 3 Preliminaries

### Settings of Self-supervised Learning

In this section, we present the general settings for joint-embedding SSL methods. We consider a large unlabelled dataset \(X^{N D}\), comprising \(N\) samples each of dimensionality \(D\). The objective of SSL methods is to construct an effective encoder \(f\) that transforms raw data into meaningful representations \(Z=f(X)\), where \(Z^{N M}\) and \(M\) denotes the representation dimensionality. The learning process of SSL methods is visually represented in Figure 2, where data augmentations transform \(X\) into two augmented views \(X_{},X_{}^{D N}\). A typical joint-embedding SSL architecture encompasses an encoder \(f\) and a projector \(p\). These components yield encoder features \(Z_{}=f(X_{})\) and \(Z_{}=f(X_{})\), as well as projection features \(H_{}=p(Z_{})\)and \(H_{}=p(Z_{})\). During training, the parameters of \(f\) and \(p\) are optimized via backpropagation to minimize the discrepancy between \(H_{}\) and \(H_{}\). To prevent the encoder \(f\) from producing a constant feature vector, contrastive methods utilize negative samples, and non-contrastive methods employ strategies such as the self-distillation technique.

The efficacy of the encoder \(f\) is usually assessed by the performance of the \(C\)-class classification as downstream tasks. Specifically, given a labeled dataset containing samples \(X_{s}^{S D}\) and their corresponding labels \(Y_{s}^{S C}\), where \(S\) is the sample number. Then, a linear layer \(g\) parameterized by \(W_{c}^{C M}\) is appended on top of the learned representations \(Z_{s}=f(X_{s})\), and thus the classification task can be fulfilled by minimizing the cross-entropy between \((g(Z_{s}))\) and \(Y_{s}\). There are two strategies for the fine-tuning: 1) non-linear fine-tuning, which trains both \(g\) and \(f\) in the downstream tasks, and 2) linear evaluation, which freezes \(f\) and only trains \(g\) (referred to as the linear probe).

Figure 2: Illustration of joint-embedding SSL methods. This is a general structure. Different augmented inputs can be passed either by shared weight Encoder and Projector or by independent Encoder and Projector, depending on different SSL methods.

### Orthogonality Regularizers

We introduce two orthogonality regularizers: Soft Orthogonality (SO) and Spectral Restricted Isometry Property Regularization (SRIP), which are seamlessly integrable with linear and convolutional layers.

Consider a weight matrix \(W^{input output}\) in a linear layer, where \(input\) and \(output\) denote the number of input and output features, respectively. In line with Bansal et al. (2018), Xie et al. (2017), Huang et al. (2018), we reshape the convolutional filter to a two-dimensional weight matrix \(W^{input output}\), while we still use the same notation \(W\) for consistency. To be specific, \(input=S H C_{in}\) and \(output=C_{out}\), with \(C_{in}\) and \(C_{out}\) being the number of input and output channels, and \(S\) and \(H\) representing the width and height of the filter, respectively.

The SO regularizer encourages the weight matrix \(W\) to approximate orthogonality by minimizing the distance between its Gram matrix and the identity matrix. This is quantified by the Frobenius norm as follows:

\[(W)=\|W^{T}W-I\|_{F}^{2},&input>output,\\ \\ \|WW^{T}-I\|_{F}^{2},&,\] (1)

where \(I\) is the identity matrix of appropriate size.

The SRIP regularizer employs the spectral norm to measure the deviation from orthogonality, which is defined as:

\[(W)=(W^{T}W-I),&input>output,\\ \\ (WW^{T}-I),&.\] (2)

where \(()\) denotes the spectral norm operator. Due to the high computational cost posed by the spectral norm, the power iteration method (Yoshida and Miyato 2017, Bansal et al. 2018) with two iterations is used for the estimation. The process for estimating \((W^{T}W-I)\) is:

\[u=(W^{T}W-I)v, v=(W^{T}W-I)u,(W^{T}W-I)=}{\|u\|_ {2}},\] (3)

where \(v^{input}\) is a vector initialized randomly from a normal distribution.

## 4 Incorporating OR into SSL

This section details the integration of OR with SSL methods. To be specific, we employ OR across the encoder, targeting both convolutional and linear layers during pretraining. We represent the SSL method's loss function as \(Loss_{SSL}\). Our overall optimization objective is the minimization of the combined loss equation:

\[Loss=Loss_{SSL}+ Loss_{OR},\] (4)

where \(Loss_{OR}\) is defined as \(_{W f}SO(W)\) or \(_{W f}SRIP(W)\), depending on the selected orthogonality regularizers. The term \(\) serves as a hyperparameter that balances the SSL objective and OR loss. Notably, we only perform OR on the weight matrices located within the linear and convolutional layers of the encoder \(f\). OR provides a versatile regularization strategy for the encoder \(f\), facilitating its application across various SSL methods without necessitating modifications to the network designs or existing training protocols.

## 5 Analysis of Dimensional Collapse and the Effects of OR in SSL

In this section, we show that dimensional collapse happens not only to the representations (i.e. output of the encoder), but also to weight matrices and hidden features of the encoder. We also compare the feature whitening technique used by one previous method, VICREG (Bardes et al. 2022), with OR. Migrating to BYOL, we find that the feature whitening technique only solves the dimensional collapse at the feature level, but instead accelerates the collapse of the weight matrices, and it even leads to lower performance of the downstream tasks as shown in Table 1. In contrast, OR caneliminate the dimensional collapse of weight matrices and thus the dimensional collapse of hidden features and representations.

In Appendix A.1, we further reveal that the original VICREG has a dimensional collapse problem with its weight matrices, which could not be solved by removing its projector. Adding OR to VICREG eliminates this problem and boosts the performance.

To study the eigenspace of a matrix, we utilize the normalized eigenvalues defined in He and Ozay (2022):

**Definition 1** (Normalized eigenvalues): _Given a specific matrix \(T^{N D}\), where \(N\) is the sample number and \(D\) is the feature dimension, we first obtain its covariance matrix \(_{T}^{D D}\). Then we perform an eigenvalue decomposition on \(_{T}\) to obtain its eigenvalues \(\{_{i}^{T}\}_{i=1}^{D}=\{_{1}^{T},,_{i}^{T}, ,_{D}^{T}\}\) in descending order. And we obtain the normalized eigenvalues by dividing all eigenvalues by the max eigenvalue \(_{1}^{T}\), denoted as \(\{_{1}^{T}/_{1}^{T},,_{i}^{T}/_{1}^{T}, ,_{D}^{T}/_{1}^{T}\}\). To simplify the denotation, we reuse \(\{_{i}^{T}\}_{i=1}^{D}\) to denote normalized eigenvalues of \(T\)._

These normalized eigenvalues are less than or equal to 1, where a larger value in one dimension indicates more information contained, and vice versa. We argue that if normalized eigenvalues drop very quickly, this means that only a few dimensions in the eigenspace contain meaningful information and also means that dimensional collapse has occurred.

We train three BYOL (Grill et al., 2020) models, without OR, with the feature whitening technique (e.g. Variance and Covariance regularization) from VICREG and with OR, respectively. We choose randomly initialized ResNet18 as the backbone (i.e. encoder) and train the three models on CIFAR-10 for 1,000 epochs, following the same recipe of Da Costa et al. (2022). Importantly, SO is selected as the orthogonality regularize, and \(\) is set to \(1e-6\). For the feature whitening technique, we impose the Variance and Covariance regularization from VICREG on the output of the predictor in BYOL as two additional loss terms, the former to ensure the informativeness of individual dimensions and the latter to reduce the correlation between dimensions. Following the solo-learn settings, we set the two loss term hyperparameters to \(_{vic}\) and \(_{vic}*0.004\), and then tune the \(_{vic}\) from \(1e-3\) to \(1e-5\).

After training, we calculate the normalized eigenvalues of both weight matrices and features (e.g. input features, hidden features, representations). ResNet18 contains four basic blocks, each containing four convolutional layers, and we visualize the normalized eigenvalues of the last convolutional layer in each block. Hidden features are the outputs of four basic blocks in ResNet18. We use the first batch in the test set as input features with batchsize 4,096 and feature dimension 3072 (32*32*3). Results are analyzed in the following sections.

### Dimensional Collapse of Weight Matrices

We first examine the weight matrices of the encoder. Similar to 5.1, all the weight matrices are viewed as two-dimensional matrices, and on top of them, we can calculate their normalized eigenvalues. For a specific weight matrix \(W^{input}\) in a neural network layer, we denote \(\{_{i}^{W}\}_{i=i}^{output}\) as the normalized eigenvalues of this layer.

As shown in Figure 3, X and Y axis is the \(i\) index and \(i\)-th values of \(\{_{i}^{W}\}_{i=i}^{output}\), respectively. It is clear that with OR, the eigenvalues of the convolutional layers of different depths decay more slowly, which means that their filters are less redundant and more diverse. In particular, we note that the deepest convolutional layer (i.e. layer4_512) has a much faster decay rate of the eigenvalues compared to the other convolutional layers in the absence of OR. Notably, the feature whitening technique does not alleviate this phenomenon. OR could significantly improve this. OR requires the weight matrix to be as orthogonal as possible, which means that the diagonal elements of its covariance matrix are as identical as possible, and the off-diagonal elements will be close to 0. Then

   Methods & BOYL & BYOL with SO & BYOL with feature whitening \\  Top-1 & 92.92 & **93.04** & 92.66 \\ Top-5 & 99.83 & **99.84** & 99.79 \\   

Table 1: Comparison of the feature whitening technique from VICREG and SO on CIFAR-10.

the eigenvalue decomposition of such a covariance matrix will have elements in \(\{_{i}^{W}\}_{i=i}^{output}\) that are all close to 1 (verified in Appendix A.2).

### Dimensional Collapse of Features

As the weight matrices become orthogonal, the distribution of their outputs stabilizes, thereby preventing the dimensional collapse of hidden features and representations. This property has been demonstrated in vector form by Huang et al. (2018) and is now presented in matrix form:

**Proposition 1**: _For a specific weight matrix \(W^{input output}\) and \(X^{N input}\), comprising \(N\) samples each of dimensionality \(input\). We denote \(\) and \(\) as the sample means of \(X\) and \(S\), respectively. Let \(S=XW\), where \(W^{T}W=I\). The covariance matrix of \(X\) is \(_{X}=)^{T}(X-)}{N-1}\). (1) If \(=0\) and \(_{X}=^{2}I\), then \(=0\) and \(_{S}=^{2}I\). (2) If \(input=output\), we have \(\|S\|_{F}^{2}=\|X\|_{F}^{2}\). (3) Given the back-propagated gradient \(\), we have \(\|\|_{F}^{2}=\|\|_{F}^{2}\)._

Figure 3: Eigenspectra of both weights and features within the encoder (ResNet18). The features are collected on the first batch of the test set (batchsize 4,096). We pretrain BYOL without OR, with feature whitening from VICREG, and with OR on CIFAR-10. The x-axis and y-axis are both log-scaled. The solid line represents that all eigenvalues are positive, the dashed line represents the existence of eigenvalues that are non-positive, and the number of eigenvalues is represented behind the underline.

The first point of Proposition 1 illustrates that in each layer of DNNs, the orthogonal weight matrix preserves the normalization and de-correlation of the output \(S\), assuming the input is whitened. This reveals that as the network gets deeper, the hidden features of each layer and the final representations do not tend to collapse. Moreover, orthogonal filters maintain the norm of both the output and the back-propagated gradient information in DNNs, as demonstrated by the second and third points of Proposition 1.

To verify that the dimensional collapse of features can be eliminated by OR, we visualize the normalized eigenvalues of features (input features, hidden features, and representations) as shown in Figure 3. Without the OR constraint, features located in deeper layers (i.e. representations) will have the fastest eigenvalues decay rate, and the distributions of eigenvalues of hidden features vary considerably at different depths. After adding the OR, it can be seen that the decay rates of hidden features at layers 3 and 4 are almost the same, while the eigenvalues of representations decay much more slowly. We also visualize the representations in Appendix A.3, which also verifies that the dimensional collapse of the representations is mitigated. Interestingly, the effect of OR on the feature level is similar to that of the feature whitening technique, however, the latter is unable to eliminate the dimensional collapse in the weight matrices.

## 6 Numerical Experiments

We study the effects of OR on SSL methods through extensive experiments. we first demonstrate that OR improves the classification accuracy on CIFAR-10, CIFAR-100, and IMAGENET100, and the improvement is consistent across different backbones and SSL methods. On the large-scale dataset IMAGENET-1k (Deng et al., 2009), OR boosts the classification accuracy on both in-distribution and out-distribution datasets (i.e. transfer learning datasets), demonstrating consistent improvement. Moreover, OR also enhances the performance in downstream tasks(e.g. object detection).

**Baseline methods and datasets.** We evaluated the effect of adding OR to 13 modern SSL methods, including 6 methods implemented by solo-learn (MOCOv2plus, MOCOv3, DINO, NNBYOL, BYOL, VICREG) (Chen and He, 2021; Chen et al., 2021; Grill et al., 2020; Dwibedi et al., 2021; Caron et al., 2021) and 10 methods implemented by LightlySSL (BarlowTwins, BYOL, DCL, DCLW, DINO, Moco, NNCLR, SimCLR, SimSiam, SwaV) (Zbontar et al., 2021; Yeh et al., 2022; Caron et al., 2020; Chen and He, 2021). We pretrain SSL methods on CIFAR-10, CIFAR-100, IMAGENET-100 and IMAGENET-1k and evaluate transfer learning scenarios on datasets including CIFAR-100, CIFAR-10 (Krizhevsky et al., 2009), Food-101 (Bossard et al., 2014), Flowers-102 (Xia et al., 2017), DTD (Sharan et al., 2014), GTSRB (Haloi, 2015). We evaluate the objection detection task on PASCAL VOC2007 and VOC2012 (Everingham et al., 2010). Detailed descriptions of datasets and baseline SSL methods are shown in Appendix A.4 and A.5, respectively.

**Training and evaluation settings.** For each SSL method, we use the original settings in solo-learn (Da Costa et al., 2022) and LightlySSL. These settings include the network structure, loss function, training policy (training epochs, optimizers, and learning rate schedulers) and data augmentation policy. The splits of the training and test set follow torchvision Marcel and Rodriguez (2010). For all the classification tasks, we report the linear probe or KNN accuracy; for the objection detection task, we perform non-linear fine-tuning. Details of training, parameter tuning, and evaluation are presented in Appendix A.6. It is worth noting that the Solo-learn and LightlySSL setups are not the same as the official implementation of the SSL methods, e.g., there is no use of multi-crop augmentation in DINO, and there is no exceptionally long training epoch. We leave experiments on migrating OR to the official implementation for future work.

**Recipe of adding OR.** For OR, \(\) of SRIP is tuned from \(\{1e-3,1e-4,1e-5\}\) and \(\) of SO is tuned from \(\{1e-5,1e-6,1e-7\}\) on a validation set. When you want to add OR to your SSL pre-training, you simply pass the encoder into the loss function, and then you just need to set \(\) of the OR according to the backbone and regularizer you use as shown in Table 9 of Appendix A.6.

### OR is Suitable for Different Backbones and SSL Methods

After pretraining on CIFRA-100, for each SSL method, we report the corresponding classification accuracy as shown in Table 2. Both two orthogonality regularizers consistently improve the linear classification accuracy. Note that OR boosts the performance of both constrastive (MoCov2plus,Mocov3) and non-contrastive methods(BYOL, NNBYOL, DINO) as they are all susceptible to dimensional collapse (Zhang et al.2022, Jing et al.2021, Zhang et al.2021, Tian et al.2021). Non-contrastive methods gain more improvements in contrast to contrastive methods. When we use ResNet18, MOCov3 improves \(3\%\) on Top-1 accuracy while DINO and NNBYOL improve \(6\%\) and \(5\%\), respectively. OR can also boost the performance of the Sota method like BYOL by \(1\%\). When we scale to ResNet 50 and WideResnet28w2, OR consistently boosts their performance. Moreover, the additional time overhead of adding OR to SSL is low compared to the original training time (referred to A.7).

In addition to CNN backbones, OR is also able to improve SSL performance on Transformer-based backbones (e.g., VIT) (Han et al.2022, Dosovitskiy et al.2020, Zhou et al.2021). We pretrain DINO on CIFAR-100 with different depths of VITs. As shown in Table 3, with the increasing depth of the VIT, the original DINO performance is increasing, and OR is able to increase their performance even further, which exhibits a good scaling law. Interestingly, under the Transformer-based architecture, OR is able to improve performance more (up to \(12\%\)) compared to CNN backbones. This is consistent with some existing studies (RoyChowdhury et al.2017) that linear layers are more likely to have redundant filters than convolutional layers in DNNs, i.e., more prone to dimensional collapse.

To evaluate OR on more SSL methods, under the LightSSL framework, we test SO on CIFAR-10, IMAGENET-100, and IMAGENET-1k. OR still consistently improves the performance of various SSL methods as shown in Table 4.

### OR Works on Large-scale Dataset

We demonstrate the effects of OR on the large-scale dataset IMAGENET-1k. Specifically, we pre-train three BYOL models- BYOL without OR, BYOL with SO, and BYOL with SRIP on IMAGENET-1k

   &  &  & IMAGENET-100 & IMAGENET-1K \\   & & original & with SO & original & with SO & original & with SO & original & with SO \\   Barlow Twins & 83.58 & **84.78** & 85.91 & **86.25** & 56.6 & **57.0** & - & - \\ BYOL & 86.94 & **87.01** & 89.64 & **90.02** & 51.7 & **52.1** & - & - \\ DCL & 83.38 & **84.10** & 85.95 & **86.27** & - & - & - & - \\ DCLW & 82.42 & **82.73** & 85.25 & **85.71** & - & - & - & - \\ DINO & 81.87 & **81.95** & - & - & - & - & 59.77 & **60.40** \\ Moco & 85.19 & **85.32** & - & - & - & - & - & - \\ NNCLR & 82.31 & **82.34** & - & - & - & - & - & - \\ SimCLR & 84.55 & **84.84** & - & - & - & - & - & - \\ SimSiam & 79.27 & **84.31** & - & - & - & - & - & - \\ SwaV & 83.10 & **83.67** & - & - & - & - & - & - \\  

Table 4: Performance of SSL methods on LightlySSL. ResNet18 is used on CIFRA-10, CIFAR-100 and IMAGENET-100, and ResNet50 is employed on IMAGENET-1K.

   &  &  &  &  &  \\  Encoder & Regularizer & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\   & - & 69.51 & 91.32 & 67.13 & 89.83 & 59.13 & 86.41 & 68.87 & 90.98 & 71.15 & 92.17 \\  & SO & **69.72** & 91.56 & _86.63_ & _90.85_ & _64.11_ & _87.94_ & _71.40_ & _92.12_ & **72.15** & 92.48 \\  & SRIP & _69.67_ & **91.92** & **69.37** & **90.88** & **62.75** & **88.37** & **71.97** & **92.80** & _71.52_ & **92.49** \\   & - & **73.70** & 93.16 & 66.87 & 89.47 & 52.67 & 80.68 & 72.31 & **92.94** & 74.20 & 93.44 \\  & SO & **73.40** & **93.45** & _69.22_ & _90.76_ & _57.30_ & _84.10_ & _72.87_ & _92.77_ & **74.57** & **93.83** \\  & SRIP & 73.32 & _92.30_ & **70.30** & **90.97** & **99.36** & **82.02** & **72.97** & _92.91_ & _74.39_ & _93.61_ \\   & - & _61.80_ & 87.69 & 57.73 & 84.82 & 53.42 & _82.62_ & 64.00 & **89.49** & 60.87 & 86.96 \\  & SO & **62.09** & _87.87_ & **58.86** & **85.69** & **56.07** & **84.87** & **64.50** & _89.22_ & _60.96_ & **87.34** \\   & SRIP & 61.37 & **87.94** & _58.79_ & _85.11_ & _53.93_ & 83.27 & _64.23_ & 89.00 & **61.10** & _87.31_ \\  

Table 2: Classification accuracy on CIFAR-100 (CNN backbones). SSL methods (in Solo-learn) are trained with or without OR on CIFAR-100. The best results are in **bold**, the second best in _italics_.

   &  &  &  \\  Regularizer & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\  - & 54.16 & 83.37 & 62.02 & 87.80 & 64.12 & 88.83 \\ SO & **60.84** & **87.65** & **64.95** & **89.47** & **66.91** & **90.42** \\  

Table 3: Classification accuracy on CIFAR-100 (VITs). DINO (in Solo-learn) is trained with or without OR on CIFAR-100.

(with ResNet50). For each testing classification dataset, we report the accuracy as shown in Table 5 and 6.

We can observe that OR not only improves the accuracy on IMAGENET-1k but also on all the transfer learning datasets. OR improves TOP-1 accuracy by \(3\%\) in IMAGENET-1k and by \(3\%\) to \(9\%\) in each transfer learning datasets. The transfer learning task evaluates the generality of the encoder as it has to encode samples from various out-of-distribution domains with categories that it may not have seen during pretraining. OR also significantly improves SSL's performance in the objection detection task by \(20\%\) on AP. The above results are close to Chen et al. (2021), Da Costa et al. (2022), Weng et al. (2023) where also training only 100 epochs.

Considering that training epoch and batchsize during pretraining significantly impact the performance Chen et al. (2021), Huang et al. (2024), Lavoie et al. (2022), we further increase the training epoch(200) and batchsize(256) and scale up the learning rate accordingly. As shown in Table 7, OR consistently improves the performance.

## 7 Conclusions

The existing studies focus on the dimensional collapse of representations and overlook whether weight matrices and hidden features also undergo dimensional collapse. We first time propose a mitigation approach to employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weights, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances SSL method performance across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures as the backbones. Importantly, the time complexity and required efforts on fine-tuning are low and the performance improvement is significant, enabling it to become a useful plug-in in various SSL methods.

In terms of future research, we wish to examine the effect of OR on other pre-training foundation models, such as vision generative SSL models such as MAE (He et al., 2022), auto-regression models like GPTs and LLaMAs (Radford et al., 2018, 2019; Brown et al., 2020; Touvron et al., 2023), and Contrastive Language-Image Pre-training models (Radford et al., 2021; Li et al., 2022). We believe OR is a pluggable and useful module to boost the performance of vision and language foundation models. In fact, this paper is the first to test the effectiveness of OR in a Transformer-based architecture and it is reasonable to believe that it will perform well in these domains.

  Dataset &  \\  Regularizer & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\  - & 59.838 & 83.945 & 73.817 & 91.039 & 68.777 & 91.596 & 63.539 & 92.035 & 79.7 & 99.12 & 52.09 & 81.89 \\ SO & _63.624_ & **86.444** & _78.956_ & _93.397_ & **70.851** & **92.819** & _68.725_ & _93.991_ & _83.58_ & **99.41** & _57.57_ & _85.38_ \\ SRIP & **63.628** & 86.420 & **79.33** & **94.243** & _70.426_ & _92.234_ & **69.256** & **94.347** & **84.26** & _99.39_ & **57.84** & **85.87** \\ 

Table 6: Classification accuracy on transfer learning datasets.

  Dataset &  \\  Regularizer & Top-1 & Top-5 & Top-1 & Top-5 \\  - & 65.81 & 87.06 & 69.76 & 89.10 \\ SO & **67.84** & **88.18** & **70.16** & **89.47** \\ 

Table 7: Classification accuracy on IMAGENET-1k (pretrained with different epochs and batchsizes).

  Dataset &  \\  Regularizer & Top-1 & Top-5 \\  - & 65.81 & 87.06 & 69.76 & 89.10 \\ SO & **67.84** & **88.18** & **70.16** & **89.47** \\ 

Table 7: Classification accuracy on IMAGENET-1k (pretrained with different epochs and batchsizes).