# PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot 3D Anomaly Detection

Qihang Zhou\({}^{1}\), Jiangtao Yan\({}^{1}\), Shibo He\({}^{1}\)1, Wenchao Meng\({}^{1}\), Jiming Chen\({}^{1}\)

\({}^{1}\) College of Control Science and Engineering, Zhejiang University

\({}^{1}\){zqhang, jtaoy, s18he, wmengzju, cjm}@zju.edu.cn

Corresponding authors.

###### Abstract

Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection. This paper introduces PointAD, a novel approach that transfers the strong generalization capabilities of CLIP for recognizing 3D anomalies on unseen objects. PointAD provides a unified framework to comprehend 3D anomalies from both points and pixels. In this framework, PointAD renders 3D anomalies into multiple 2D renderings and projects them back into 3D space. To capture the generic anomaly semantics into PointAD, we propose hybrid representation learning that optimizes the learnable text prompts from 3D and 2D through auxiliary point clouds. The collaboration optimization between point and pixel representations jointly facilitates our model to grasp underlying 3D anomaly patterns, contributing to detecting and segmenting anomalies of unseen diverse 3D objects. Through the alignment of 3D and 2D space, our model can directly integrate RGB information, further enhancing the understanding of 3D anomalies in a plug-and-play manner. Extensive experiments show the superiority of PointAD in ZS 3D anomaly detection across diverse unseen objects. Code is available at https://github.com/zqhang/PointAD

## 1 Introduction

Anomaly detection, a significant field within deep learning, has been widely applied to diverse domains, including industrial inspection . While 2D anomaly detection has been extensively studied by exploring RGB information , real-world anomalies typically present themselves with abnormal spatial characteristics. Relying solely on RGB information poses challenges in detecting some anomalies in many cases, e.g., when the defect mimics the appearance of the object's background or foreground, as shown in Figure 1(a). The emerging field of 3D anomaly detection aims to unveil these spatial relations indicative of abnormal patterns .

However, current 3D anomaly detection methods typically store normal point features during training and identify anomalies by measuring the distance between the test feature and these stored features . They all depend on the assumption that target point clouds are available and entirely normal. This assumption does not hold in various situations when the training samples in the target dataset are inaccessible due to privacy protection (_e.g._, involvement of trade secrets) or the absence of target training data (_e.g._, a new product never seen before) . Figure 1(b) depicts the setting discrepancy between ZS 3D and unsupervised anomaly detection. These methods mentioned above, which detect anomalies by memorizing or reconstructing normal point features, have limitations in generalizing to unseen objects in Figure 1(c). While zero-shot (ZS) anomaly detection has been explored in 2D images , ZS 3D anomaly detection remains a research blank. It is a challenging task as ZS3D anomaly detection necessitates the model to detect 3D anomalies across unseen point clouds with diverse class semantics, requiring a robust generalization capacity in the detection model. Recently, Vision-Language Models (VLMs) with their strong generalization capabilities have been applied to various downstream tasks [40; 61; 49; 25; 26]. Particularly, CLIP has demonstrated its strong ZS performance to detect 2D anomalies [35; 24; 63]. Integrating CLIP into the detection model presents a potential solution to the challenging yet unexplored ZS 3D anomaly detection.

In this paper, we propose a unified framework, namely PointAD, to transfer the knowledge of CLIP to detect 3D anomalies in a ZS manner. PointAD comprehends point clouds from both 3D and 2D: **(1)** deriving 2D representations of point clouds via CLIP by rendering them from multiple views, **(2)** understanding 3D representations by projecting 2D representations back to 3D, and **(3)** enhancing 3D comprehension by additional regularization on 2D representations. After grasping point clouds from points and pixels, we propose hybrid representation learning to capture generic normality and abnormality w.r.t. point and pixel information into learnable text prompts . Specifically, since 3D representation manifests its 2D renderings from different views, we treat each representation as one instance and achieve 3D representation aggregation via multiple instance learning (MIL). On this basis, PointAD explicitly aligns the 2D anomalies, rendered from 3D anomalies, to further enhance 3D understanding. We formulate these 2D anomaly recognition tasks from the multi-task learning (MTL) perspective. PointAD collaboratively learns point and pixel representations, promoting the in-depth understanding of underlying abnormal patterns and thus achieving superior ZS normality and abnormality point recognition. Furthermore, benefiting from collaboration optimization, PointAD can directly integrate additional RGB information and perform ZS multimodal 3D (M3D) detection without extra modules and retraining. The main contributions of this paper are summarized as follows:

* To the best of our knowledge, we are the first to investigate the challenging yet valuable ZS 3D anomaly detection domain. We propose to transfer the strong recognition generalization of CLIP to detect and segment 3D anomalies over diverse objects.
* We introduce a novel ZS 3D anomaly detection approach called PointAD, which provides a unified framework to understand 3D anomalies from points and pixels. Hybrid representation learning is proposed to incorporate the generic normality and abnormality semantics into PointAD, enabling a thorough understanding of 3D anomalies.
* PointAD can incorporate 2D RGB information in a plug-and-play manner for testing. In contrast to other methods that require storing RGB information separately, PointAD offers a unified framework to perform ZS M3D anomaly detection directly.
* Extensive experiments are conducted to demonstrate the superiority of our model in detecting and segmenting 3D anomalies, even outperforming some unsupervised SOTA methods that memorize normal features of target objects in certain metrics. We hope that our model will serve as a springboard for future research on ZS 3D and M3D anomaly detection.

## 2 Related Work

3D Anomaly DetectionMVTec3D-AD , Eyecandies , and Real3D-AD  provide the point cloud anomalies and the corresponding 2D-RGB information. MVTec3D-AD bridges the connection between 3D and 2D anomaly detection. 3D-ST  uses a teacher net to extract dense local geometric

Figure 1: Motivation of zero-shot 3D anomaly detection. **(a)**: **Top:** The hole on the cookies presents a similar appearance to the background. **Bottom:** Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.

descriptors and design a student net to match such descriptors. AST  introduces asymmetric teacher and student net to further improve 3D anomaly detection. IMRNet  and 3DSR  detect 3D anomalies by reconstruction errors. Instead of only using point clouds, BTF , M3DM [54; 53], CPFM , and SDM  integrate point features and RGB pixel features to detect 3D anomalies. While these approaches exhibit commendable performance by storing object-specific normal point and pixel features within the unsupervised learning framework, such paradigms simultaneously limit their generalization capacity to point clouds from unseen objects, which is crucial to detecting anomalies when the target object is unavailable. To the best of our knowledge, no solution addresses this valuable yet challenging problem. To fill this gap, we introduce PointAD, designed to identify unseen anomalies across diverse objects. PointAD extends CLIP to the realm of ZS 3D anomaly detection and shows robust generalization in capturing generic normality and abnormality within point clouds. Furthermore, PointAD serves as a unified framework, allowing seamless integration of point cloud and RGB modality without additional training.

3D Feature ExtractionConventional methods of 3D feature extraction typically employ a point-based network like PointNet  or PointNet++ to extract 3D features from point clouds. Alternative approaches convert 3D data into a 2D format [48; 18], enabling 2D image backbones to process 3D information. PointCLIP  directly projects raw points onto image planes for efficiency, but this approach causes the produced depth map to lack geometric details. Instead, rendering-based methods [48; 21] generate 2D renderings by rendering point clouds, allowing for better preservation of local semantics. CPFM  stores normal features of these 2D renderings for unsupervised 3D anomaly detection. In this paper, we apply this rendering strategy to the source samples to capture generic anomaly semantics for recognizing abnormalities in unseen objects.

Prompt LearningInstead of fine-tuning the whole network, prompt learning just optimizes the model to adapt the network to downstream tasks. CoOp [61; 60] introduces global context optimization to update learnable text prompts for few-shot recognition. DenseCLIP  extends it to the dense classifications. More recently, AnomalyCLIP  proposes object-agnostic prompt learning to capture the generic normality and abnormality for images. Our model first introduces hybrid representation learning for ZS 3D anomaly detection, enabling the detection of anomalies and abnormal regions.

## 3 PointAD

### A Review of CLIP

CLIP, a representative VLM, aligns visual representations to the corresponding textual representations, where an image is classified by comparing the cosine similarity between its visual representation and textual representations of given class-specific text prompts. Specifically, given an image \(x_{i}\) and target class set \(\), visual encoders output the global visual representation \(f_{i}^{d}\) and local visual representations \(f_{i}^{m}^{h w d}\), where \(h\), \(w\), and \(d\) are the height, width, and dimension, respectively. Textual representations \(g_{c}\) are encoded by textual encoder \(\) with the commonly used text prompt template A photo of a [c], where \(c\). The probability of \(x_{i}\) belongs to \(c\) can be computed as:

\[P(g_{c},f_{i})=,f_{i})/)}{_{c}exp(cos (g_{c},f_{i}))/)},\] (1)

where \(cos(,)\) and \(\) represent the cosine similarity and temperature used in CLIP, respectively. The segmentation \(S_{i(c)}^{h w}\) for class \(c\) can be computed as \(Seg(g_{c},f_{i}^{m})\), where each entry (\(u\),\(v\)) is calculated as \(P(g_{c},f_{i,u,v}^{m})\).

### Overview of PointAD

ZS 3D anomaly detection requires a strong generalization capacity to anomalies on unseen objects with diverse object semantics. In this paper, we propose a unified framework, namely PointAD, to detect and segment 3D anomalies in a ZS manner. In Figure 2, PointAD understands point clouds from both pixel and point perspectives. To make CLIP understand 3D point clouds, we first render point clouds from multiple views and extract the pixel representations of these generated 2D renderings via the visual encoder of CLIP. And then, we derive point representations by projecting these pixel representations back to 3D. Learning generic normality and abnormality is significant in recognizing across-object anomalies. We propose hybrid representative learning, which focuses on global point and pixel abnormality, to optimize normality and abnormality text prompts, enabling PointAD with strong generalization to identify 3D anomalies on diverse objects. Benefiting from the hybrid representation learning, PointAD can directly incorporate 2D RGB information during testing to achieve ZS M3D detection.

### Multi-View Rendering

Multi-view projection is a crucial technology for understanding point clouds from 2D perspectives. Some multi-view projection approaches project point clouds into various depth maps, providing adequate shape information for class recognition . However, in this paper, our objective is to learn both generic global and local anomaly semantics. Depth-map projection lacks sufficient resolution to represent fine-grained anomaly semantics accurately. Hence, we adopt high-precision rendering to preserve the original 3D information offline. Specifically, given an auxiliary dataset of point clouds \(_{3d}=\{(x_{i}^{3d},y_{i}^{3d})\}_{i=1}^{N}\), we define the rendering matrix as \(R^{(k)}\) for the \(k\)-\(th\) view, with a total of \(K\) views. We simultaneously render point clouds and point-level ground truths from different views to obtain their corresponding 2D renderings, which is given by \(x_{i}^{(k)}=R^{(k)}(x_{i}^{3d})\) and \(y_{i}^{(k)}=R^{(k)}(y_{i}^{3d})\), where \(x_{i}^{(k)}^{H W}\) and \(y_{i}^{(k)}^{H W}\) respectively represent the \(k\)-\(th\) 2D renderings and corresponding pixel-level ground truth in the \(i\)-\(th\) point cloud. Note that anomaly pixels are marked as 1, and normal pixels are marked as 0.

### Representations for 3D and 2D information

PointAD aims to learn generic anomaly semantics from both 3D and 2D representations, enabling a comprehensive understanding of point and pixel anomaly patterns. For a point cloud \(x_{i}^{3d}\), we first obtain the 2D renderings \(_{i}=\{x_{i}^{(k)}\}_{k=1}^{K}\). Then, these renderings are encoded via the vision encoder of CLIP to obtain global 2D representations \(_{i}{=}\{f_{i}^{(k)}\}_{k=1}^{K}\), and local 2D representations \(_{i}^{m}=\{f_{i}^{m(k)}\}_{k=1}^{K}\). As for point cloud representations, we consider that one point cloud will be projected into multiple 2D renderings. Consequently, global 3D representation \(p_{i}\) and local 3D representations \(p_{i}^{m}\) are expected to include their corresponding 2D representations in each view. Formally, \(p_{i}=\{p_{i}^{(k)}|p_{i}^{(k)}=f_{i}^{(k)}\}_{k=1}^{K}\) and \(p_{i}^{m}=\{p_{i}^{m(k)}|p_{i}^{m(k)}=\{p_{i,j}^{m(k)}\}_{j=1}^{n}\}_{k=1}^{K}\), where \(p_{i,j}^{m(k)}=\{p_{i,j}^{m(k)}=f_{i,u,v}^{m(k)},(u,v)=R^{(k)}(a_{i,j}|b_{i,j}, c_{i,j})\}\) represents the \(j\)-\(th\) point representation of \(i\)-\(th\) point cloud in the \(k\)-\(th\) view, whose 3D coordinate is (\(a_{i,j}\), \(b_{i,j}\), \(c_{i,j}\)). \(R^{(k)}\) is the rendering transformation between the point and pixel representation, derived as \(R^{(k)}=R^{(k)}\).

Figure 2: Framework of PointAD. To transfer the strong generalization of CLIP from 2D to 3D, point clouds and corresponding ground truths are respectively rendered into 2D renderings from multi-view. Then, vision encoder of CLIP extracts the renderings to derive 2D global and local representations. These representations are transformed into global 3D point representations to learn 3D anomaly semantics within point clouds. Finally, we align the normality and abnormality from both point perspectives (multiple instance learning) and pixel perspectives (multiple task learning) and propose a hybrid loss to jointly optimize the text embeddings from the learnable normality and abnormality text prompts, capturing the underlying generic anomaly patterns.

Points at different positions may yield a different number of 2D representations as they are hidden by other points from a specific viewpoint. In this case, we introduce a view-wise visibility mask \(M\), where \(M^{k}_{i,j}\) indicates whether the \(j\)-\(th\) point of the \(i\)-\(th\) point cloud is visible in the \(k\)-\(th\) view. We compare the depth of points projected into the same pixel in the same view and set the corresponding visibility mask to 1 for the point with the minimum depth, and to 0 for the other points. Let \(^{(k)}_{i,u,v}\) denote the depths set of all points that are projected into the same pixel indexed by \((u,v)\) in the \(i\)-\(th\) point cloud in the \(k\)-\(th\) view. \(^{(k)}_{i,u,v}\) and \(M^{k}_{i,j}\) are respectively given as \(^{(k)}_{i,u,v}=\{c_{i,j} R^{(k)}(a_{i,j},b_{i,j},c_{i,j})= (u,v)\}_{j=1}^{n}\) and \(M^{(k)}_{i,j}=(i,j,k=_{i,j,k}\{c_{i,j} c_{i,j}^{(k)}_{i,u,v}\})\), where \(()\) is an indicator function. Local 3D representations of the \(i\)-\(th\) point cloud for the \(k\)-\(th\) view are reformulated as: \(p^{m(k)}_{i}=\{p^{m(k)}_{i,j}*M^{(k)}_{i,j}\}_{j=1}^{n}\).

### Hybrid representation learning

The key of ZS 3D anomaly detection requires the model to capture generic anomaly semantics, rather than relying on specific object semantics. Since CLIP was originally pre-trained to align object semantics, such alignment harms the generalization capacity of CLIP to recognize anomalies on various objects. To adapt CLIP to 3D anomaly detection, we propose a hybrid representation learning, from both 3D and 2D perspectives, to globally and locally optimize textual representations. This enables PointAD to learn more representative text embedding for global anomaly semantics alignment. Following previous work [63; 61], we randomly initialize two learnable text templates \(t_{n}\) and \(t_{a}\), in AnomalyCLIP  or CoOp manner , to obtain more overall text embeddings \(g_{n}\) and \(g_{a}\) to recognize normality and abnormality, respectively.

\[t_{n}=[V_{1}][V_{E}][object],&t_{n}=[V_{1}][V_ {E}][class],\\ =[W_{1}][W_{E}][damaged][object]}_{},&t_{a }=[W_{1}][W_{E}][damaged][class],\]

where \(V\) and \(W\) are learnable word embeddings, respectively.

MIL-based 3D representation learningTo fully incorporate 3D glocal anomaly semantics into PointAD from point information, we respectively devised two losses to capture 3D global anomalies and local anomaly regions. First, we compute the cosine similarity between the textual representation and its rendering global representations in each view. As point clouds are projected from different views, the resulting renderings in each view reflect certain parts of point clouds. We use view-wise MIL to integrate 2D global representations and then align global labels to capture the global semantics. Formally, the global 3D loss is defined as:

\[L^{global}_{3d}=_{i}(_{f^{ }_{i}_{p_{i}}}P(g_{c},f^{(k)}_{i}),_{i})}).\]

As for local point anomaly semantics, we quantify the cosine similarity between textual representations and local representations of 2D renderings. Since points within point clouds are projected from different views, their projections in each view present part characteristics of themselves. We adopt the pixel-wise MIL to achieve the aggregation of point local representation. The point segmentation can be formulated mathematically as follows:

\[S^{3d}_{i(a)}=_{k}Seg(g_{a},p^{m(k)}_{i}),S^{3d}_{i(n)}= {1}{K}_{k}Seg(g_{n},p^{m(k)}_{i}).\]

However, deriving such 3D segmentation requires similarity computation for each point. It brings a significant memory burden, with a huge computational complexity of \(O(Knd)\), which is unaffordable for one NVIDIA RTX 3090 24GB GPU. To address this computational challenge, we resort to the rendering correspondence between points (3D space) and their corresponding pixels within each view (2D space). We first can rewrite 3D segmentation from the view perspective as \(S^{3d}_{i(a)}=_{k}S^{3d(k)}_{i(a)}\). Then, the \(k\)-\(th\) division of 3D segmentation can be transformed into the 2D counterpart through the rendering projection \(S^{3d(k)}_{i(a)}=(R^{(k)})^{(-1)}S^{2d(k)}_{i(a)} M^{(k)}_{i}\), where \(\) is the Hamiltonian product. The \(k\)-\(th\) 2D counterpart can be computed as \(S^{2d(k)}_{i(a)}=(Seg(g_{a},f^{m(k)}_{i}))\), where the operator \(()\) represents bilinear interpolation from feature space to 2D space. Finally, we can reformulate the 3D segmentation as follows:

\[S^{3d}_{i(a)}=_{k}(R^{(k)})^{(-1)}(Seg(g_{a},f^{m(k)}_{i})) M^{(k)}_{i}.\] (2)

From the equation, we can observe that the primary computation can be conducted in the feature space, with a computational complexity of \(O(Khwd)\). This is a substantial overhead reduction compared to \(O(Knd)\) since feature space is much smaller than 3D space, _i.e._, \(h w n\). In our experiment, \(h w=24 24=576\), while \(n=336 336=112896\). With this transformation, the entire experiment can be conducted using only a single NVIDIA RTX 3090 24GB GPU. After that, Dice Loss is employed to precisely model the decision boundary of anomaly regions. Let \(I\) represent a full-one matrix of the same size as \(y_{i}^{3d}\). Formally, we define 3D local loss \(L_{3d}^{local}:\)

\[L_{3d}^{local}=_{i}(S_{i(n)}^{3d},I-y_{i}^{3d })+(S_{i(a)}^{3d},y_{i}^{3d}).\]

MTL-based 2D representation learningWe further improve PointAD point understanding by capturing 2D glocal anomaly semantics into the object-agnostic text prompt template. We treat the anomaly recognition for one rendering from the point cloud as a task. Hence, we formulate the anomaly semantics learning for multiple 2D renderings as MTL. MTL-based 2D representation learning is divided into two parts for respective alignment to 2D global and local anomaly semantics. For 2D global semantics, we use CrossEntropy to quantify the discrepancy between the textual representations and each global 2D representation. Global MTL-based 2D representation learning \(L_{2d}^{global}\) is defined as:

\[L_{2d}^{global}=_{i,k}(P(g_{c},f_{i}^{(k)} ),^{(k)})}).\]

Also, we focus on 2D abnormal regions to understand pixel-level anomalies. As the anomaly regions are typically smaller than normal regions, we employ Focal Loss to mitigate the class imbalance besides Dice Loss. Let \(\) denote the concatenation operation. Local MTL-based 2D representation learning\(L_{2d}^{local}\) is given as follows:

\[L_{2d}^{local}=_{i,k}(S_{i(n)}^{2d(k)} S_{ i(a)}^{2d(k)},y_{i}^{(k)})+(S_{i(n)}^{2d(k)},I-y_{i}^{(k)})+(S_{i(a)}^{ 2d(k)},y_{i}^{(k)}).\]

### Training and Inference

PointAD detects 3D anomalies from both 3D and 2D perspectives and thus combing these above losses to derive hybrid loss \(L_{hybrid}\). We minimize \(L_{hybrid}\) to incorporate generic anomaly semantics into the text prompt from point and pixel spaces:

\[L_{hybrid}=L_{3d}^{global}+L_{3d}^{local}+L_{2d}^{global}+L_{2d}^{local}.\]

During training, we minimize the hybrid loss \(L_{hybrid}\), where the original parameters of CLIP are frozen to maintain its strong generalization. Since our model provides a unified framework to understand anomaly semantics from point and pixel, it can not only perform **ZS 3D anomaly detection** but also **M3D anomaly detection in a plug-and-play way**. Next, we will introduce the inference process in detail:

ZS 3D/M3D inferenceGiven a point cloud \(x_{i}^{3d}\), we regard the 3D segmentation (See Equ. 2) as the anomaly score map: \(A_{i}^{m}\)=\(G_{}(S_{i(a)}^{3d})\), where \(G_{}()\) represents the Gaussian filter. The global anomaly score incorporates glocal anomaly semantics and is computed as \(A_{i}^{s}=(_{f_{i}^{(k)}_{i}}P(g_{c}, f_{i}^{(k)})+^{m})})\). When the RGB counterpart is available for testing, PointAD could directly integrate RGB information by feeding RGB images to 2D branch to derive 2D representations. We project these 2D representations back to 3D branch to respectively compute RGB anomaly score map and anomaly score as \(A_{i}^{m(rgb)}=P(g_{c},f_{i}^{(rgb)})\) and \(A_{i}^{s(rgb)}=G_{}(S_{i(a)}^{3d(rgb)})\). The final multimodal anomaly score map and anomaly score are defined as \(A_{i}^{m(mod)}=G_{}A_{i}^{m}+A_{i}^{m(rgb)}\) and \(A_{i}^{s(mod)}=[(A_{i}^{s(rgb)}+A_{i}^{s})+^{m(mod)})}]\), respectively.

   Detec. & Dataset & MVTC-3D-AD(10) & Eyecaudes(0) & RealD-AD(12) \\  level & Mice & \(L_{3d}\) & AUC & AP & AUROC & AP \\  CLIP+R & 61.2 & 85.8 & 66.7 & 69.2 & 68.8 & 72.3 \\  & Cheraghian & 53.6 & 81.7 & 49.5 & 48.1 & 50.3 & 54.4 \\  & PointCLIP V2 & 51.2 & 80.1 & 46.1 & 48.1 & 53.1 & 58.1 \\ G. & PointCLIP V2 & 51.1 & 80.6 & 44.4 & 47.0 & 57.5 & 58.3 \\  & AnomalyCLIP & 56.4 & 33.5 & 57.6 & 59.0 & 55.2 & 71.1 \\  & PointADCOop & **80.9** & **93.9** & 67.8 & **72.8** & **72.9** \\    
   Metric & P-ARROC & ATP & P-ARROC & ATP \\  CLIP+R & - & 54.4 & 82.1 & - & 39.5 & - \\  & Cheraghian & 88.2 & 57.0 & - & - \\  & PointCLIP V2 & 87.4 & 52.3 & 43.7 & - & 52.9 & - \\ L. & PointCLIP V2 & 87.3 & 52.3 & 44.2 & - & 52.2 & - \\  & AnomalyCLIP & 88.9 & 60.9 & 77.7 & 43.4 & 50.3 & - \\  & PointADCOop & **80.5** & **82.0** & **91.5** & **71.3** & **72.6** \\  & PointAD & **90.5** & **84.4** & **92.1** & **71.3** & **72.8** \\   

Table 1: Performance comparison on ZS 3D anomaly detection in “one-vs-rest” setting.

   Detec. & Dataset & MVTC-3D-AD(10) & Eyecaudes(10) \\  level & Mice & \(L_{3d}\) & AUC & AP & AUROC & AP \\  CLIP+R & 61.2 & 85.8 & 66.7 & 69.2 & 68.8 & 72.3 \\  & Cheraghian & 53.6 & 81.7 & 49.5 & 48.1 & 50.3 & 54.4 \\  & PointCLIP V2 & 51.2 & 80.1 & 46.1 & 48.1 & 53.1 & 58.1 \\ G. & PointCLIP V2 & 51.1 & 80.6 & 44.4 & 47.0 & 57.5 & 58.3 \\  & AnomalyCLIP & 56.4 & 33.5 & 57.6 & 59.0 & 55.2 & 71.1 \\  & PointADCOop & **80.9** & **93.9** & 67.8 & **72.8** & **72.9** \\    
   Metric & P-ARROC & ATPRO & P-ARROC & ATPRO \\  CLIP+R & - & 54.4 & 82.1 & - & 39.5 & - \\  & Cheraghian & 88.2 & 57.0 & - & - \\  & PointCLIP V2 & 87.4 & 52.3 & 43.7 & - & 52.9 & - \\  & PointCLIP V2\({}_{2d}\) & 87.3 & 52.3 & 44.2 & - & 52.2 & - \\  & AnomalyCLIP & 88.9 & 60.9 & 77.7 & 43.4 & 50.3 & - \\  & PointADCOop & **80.5** & **82.0** & **91.5** & **71.3** & **72.6** \\  & PointAD & **90.5** & **84.4** & **92.1** & **71.3** & **72.8** \\   

Table 2: Performance comparison on ZS M3D anomaly detection in “one-vs-rest” setting.

Experiment

### Experiment Setup

DatasetWe evaluate the performance of ZS 3D anomaly detection on three public datasets including, MVTec3D-AD, Eyecandies and Real3D-AD. MVTec3D-AD, Eyecandies, and Real3D-AD are multi-class datasets and respectively contain 10 classes, 10 classes, and 12 classes. Since these training datasets only contain all normal samples, **we use the common zero-shot setting one-vs-rest, where an object test dataset is used to fine-tune PointAD and assess the ZS anomaly detection for the remaining objects.** We also explore a more challenging setting: **cross-dataset ZS generalization, which requires the detection model to generalize to anomalies on other datasets.** For point cloud anomaly detection, we only use point clouds to detect and localize 3D anomalies. In M3D anomaly detection, the 2D RGB information is utilized only for testing. To comprehensively analyze PointAD, we utilize four metrics to assess its performance in both anomaly detection and segmentation.

### Implementation Details & Baselines

Both point clouds and 2D renderings are resized to 336 \(\) 336. We use Open3d library to generate 9 views by rotating point clouds along the X-axis at angles of \(\{-,-,-,-,0,,,,\}\) for most categories. We circularly set the rendering angles, evenly distributing the angles between \(-\) to \(\). The backbone of PointAD is the pre-trained CLIP model (VIT-L/140336px in open_clip). Following , we improve the local visual semantics of vision encoder of CLIP without modifying its parameters. During training, we keep all parameters of CLIP frozen and set the learnable word embeddings in object-agnostic text templates to \(12\). All experiments were conducted on a single NVIDIA RTX 3090 24GB GPU using PyTorch-2.0.0. As there is no work to explore the field of ZS 3D anomaly detection, we make a great effort to provide these comparisons. We apply the original CLIP to our framework for 3D detection, called CLIP + Rendering. Also, we reproduce SOTA 3D recognition works including PointCLIP V2  and Cheraghian , and adapt them for ZS 3D anomaly detection. We compare the SOTA 2D anomaly detection approach AnomalyCLIP  by fine-tuning it on depth maps. PointAD by default uses object-agnostic text prompts, whereas PointAD-CoOp employs object-aware prompts. Appendix B and C provide more details on implementation and baselines.

### Main Results

We fine-tuned PointAD on three objects on MVTec3D-AD, Eyecandies, and Real3D-AD. Over three runs, the averaged results on **one-vs-rest** and **cross-dataset** settings are reported. We use the metric pairs (I-AUROC% \(\%\) and AP% \(\%\)) and (P-AUROC% \(\%\) and AUPRO% \(\%\)) to evaluate the global detection performance, respectively. Details of experimental settings see Appendix A. The best and second-best results in ZS are highlighted in Red and Blue. G. and L. represent 3D global and local anomaly detection. M3D global and local anomaly detection are abbreviated as MG. and ML.

ZS 3D anomaly detectionTable 1 presents the comparison of ZS 3D performance. Compared to the point-based method Cheraghian and the projection-based method PointCLIP V2, PointAD achieves superior performance on ZS 3D anomaly detection over all three datasets. Especially, it outperforms CLIP + Rendering from 61.2% to 82.0% I-AUROC and from 85.8% to 94.2% AP on MVTec3D-AD. In addition, PointAD achieves superior segmentation performance on ZS 3D anomaly detection, improving MVTec3D-AD by a large margin compared to Cheraghian from 88.2%

Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J.

to 95.5% P-AUROC and from 57.0% to 84.4% AUPRO. This improvement in overall performance is attributed to PointAD adapting CLIP's strong generalization to glocal anomaly semantics through hybrid representation learning. In addition, PointAD advances PointAD-CoOp across all datasets by blocking the class semantics in text prompts .

ZS M3D anomaly detectionWe also compare the ZS M3D anomaly detection when RGB information is available for testing. As shown in Table 2, the results indicate that PointAD can integrate additional RGB information and further boost its performance from 82.0% to 86.9% AUROC and from 94.2% to 96.1% AP for global semantics on MVTec3D-AD. Additionally, as for local semantics, the performance improves from 95.5% to 97.2% P-AUROC and from 84.4% to 90.2% AUPRO. A large performance gain is also obtained on Eyecandies and Real3D-AD. While other methods improve their performance in some metrics, they still suffer from performance degradation in other metrics due to inefficient integration of the two modalities. Instead, PointAD achieves overall improvement across all metrics by incorporating explicit joint constraints on both point and pixel information.

Cross-dataset ZS anomaly detectionWe perform the cross-dataset anomaly recognition to further evaluate the zero-shot capacity of PointAD, where we use one object as the auxiliary and test objects with totally different semantics and scenes in another dataset. We compare all baselines that need fine-tuning. From Table 3 and M3D from Table 4, PointAD demonstrates strong cross-dataset generalization performance on Eyeecandies and Real3D-AD, with nearly no obvious performance decay compared to the one-vs-rest setting. The strong transfer ability highlights its robust generalization capabilities in detecting anomalies in objects with unseen semantics and backgrounds.

### Result Analysis

Visualization analysis.To intuitively present the strong generalization capacity of our model to unseen anomalies, we visualize the anomaly score maps of the 3D and corresponding 2D counterparts of PointAD on MVTec3D-AD. As shown in Figure 3, PointAD reveals abnormal spatial relationships of points and further captures the generic point anomaly patterns across diverse objects. And, we also visualize the anomaly score map of corresponding 2D counterparts, where 3D point anomalies are transformed into 2D pixel anomalies. It can be observed that PointAD also has a strong detection ability on such 2D anomalies. The strong representative pixel representations from multiple views facilitate more precise 3D anomaly detection. Quantitative results are provided in Section 5. The strong 3D and 2D detection capabilities of PointAD are from hybrid representation learning, which not only enables PointAD to capture the 3D anomalies but also explicitly constrains 2D representations.

How multimodality makes PointAD accurate.PointAD is a unified framework that can not only capture point anomalies but also handle 2D information in a plug-and-play manner. As shown in Figure 4(a), we visualize M3D results of PointAD on MVTec3D-AD. The surface damage on the potato presents a similar appearance to the object foreground, which makes it difficult to detect this anomaly with RGB information. On the contrary, the point relations for the color stain on foam are the same as those of normal, but they have a clear distinction in the RGB information. PointAD

Figure 4: Visualization comparison between PointAD with hybrid loss and without.

[MISSING_PAGE_FAIL:9]

#### Acknowledgments

This work was supported by NSFC U23A20326 and NSFC 62088101 Autonomous Intelligent Unmanned Systems.