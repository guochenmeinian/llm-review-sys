# Approximate Allocation Matching for Structural Causal Bandits with Unobserved Confounders

Lai Wei

Life Sciences Institute

University of Michigan

weilatim@gmail.com

&Muhammad Qasim Elahi

Electrical and Computer Engineering

Purdue University

elahi0@purdue.edu

&Mahsa Ghasemi

Electrical and Computer Engineering

Purdue University

mahsa@purdue.edu

&Murat Kocaoglu

Electrical and Computer Engineering

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Structural causal bandit provides a framework for decision-making problems when causal information is available. It models the stochastic environment with a structural causal model (SCM) that governs the causal relations between random variables. In each round, an agent applies an intervention (or no intervention) by setting certain variables to some constants, and receives a stochastic reward from a non-manipulable variable. Though the causal structure is given, the observational and interventional distributions of these random variables are unknown beforehand, and they can only be learned through interactions with the environment. Therefore, to maximize the expected cumulative reward, it is critical to balance the exploration-versus-exploitation tradeoff. We consider discrete random variables with a finite domain and a semi-Markovian setting, where random variables are affected by unobserved confounders. Using the canonical SCM formulation to discretize the domains of unobserved variables, we efficiently integrate samples to reduce model uncertainty, gaining an advantage over those in a classical multi-armed bandit setup. We provide a logarithmic asymptotic regret lower bound for the structural causal bandit problem. Inspired by the lower bound, we design an algorithm that can utilize the causal structure to accelerate the learning process and take informative and rewarding interventions. We establish that our algorithm achieves a logarithmic regret and demonstrate that it outperforms the existing methods via simulations.

## 1 Introduction

Sequential decision-making in uncertain environments is one of the most fundamental problems across scientific disciplines, including robotics, economics, social science, clinical trials, and agriculture [32; 26; 36; 7; 3]. In almost all these applications, it is required to implement interventions to control some aspects of the system to optimize an outcome of interest. In economics and political science, the government could use fiscal policy including changing the interest rate and taxation to achieve regulation and control of a country's economy. In this paper, we deal with designing an adaptive policy in an uncertain stochastic environment to learn and apply optimal interventions.

In sequential decision-making problems, it is critical to balance the explore-versus-exploit tradeoff, which deals with choosing between informative options and empirically more rewarding alternative options. The multi-armed bandit (MAB) [33; 30; 18] is a classical problem formulation for achieving such a tradeoff. In MAB, a decision-maker sequentially chooses an action corresponding to pullingan arm from a set of arms, each generating stochastic rewards with probability distribution unknown to the decision-maker. The objective is to achieve the maximum cumulative reward over a time horizon, or equivalently to minimize the regret, which measures suboptimality in cumulative rewards against selecting the best action all the time. The classic stochastic bandit formulation only considers the connection between an action and its expected reward. However, in real-world applications, an action can affect the entire system in a more complex way. As the agricultural example given in , changing the moisture level could affect the temperature and nutrient absorption, which are also crucial to crop yields. The relations between such factors cannot be characterized by the MAB.

Causal knowledge enables us to model the causal relations between multiple factors in the environment. In machine learning research, it has been noted that causal models could help improve the generalization performance when data distribution shifts , and address unstable translation of successful reinforcement learning methods in simulation to real-world problems  such as self-driving cars and recommendation systems. The casual bandit models the stochastic environment with a causal graph that connects a set of random variables of interest, and actions are modeled by interventions on different subsets of nodes. It provides a critical interpretation of causal influences among random variables  that allows us to reason about what will happen to the environment after certain interventions are made to the data-generating process. Compared to the classic MAB with only reward feedback, the casual bandit represents a more detailed model of real-world decision-making problems, including richer feedback from multiple random variables and the causal relations among them. By applying causal reasoning, the learning process can be accelerated and more rewarding interventions can be selected. In applications where the space of intervention is large and explicit experiments are costly, such as social science or economics, it has special value.

Due to the reasons mentioned above, there has been a recent surge of interest in the research community on causal decision-making. Some early attempts include , in which the authors took a causal approach to study how to make final decisions based on natural predictions, and proposed an algorithm by modifying Thompson sampling . In , the investigators started from a parallel causal structure and extended the algorithm to the general causal bandit problem. Subsequent work  took preprocessing steps to get rough estimates of interventional distributions and use them to design efficient experiments to discover the best intervention. In , the Bayesian optimization technique is employed to solve the same problem.

The most closely related works focusing on regret minimization in the structural causal bandit with confounders are . In both works, the authors took a graphical characterization using do-calculus  to reduce the large intervention set to the possibly optimal minimal intervention set and run a KL-UCB algorithm  within the reduced action space. The later work is an extension of the previous one that includes non-manipulable variables. Other lines of causal bandit research normally assume additional prior information, such as infinite observational data  or marginal interventional distributions . A general linear model is adopted in , which is different from the non-parametric setup in this paper.

In this paper, we focus on the MAB sequential decision-making problem where the arms correspond to interventions on an arbitrary causal graph, including unobserved confounders affecting multiple pairs of variables (also known in causal terminology as the semi-Markovian setting). We recognize the structural causal bandit falls into the category of structured bandit . It has been noted the classical upper confidence bound (UCB) algorithm and Thompson sampling may not fully leverage structural information . It is worth considering the potential limitations of algorithms based on these ideas, such as CRM-AL  and generalized Thompson sampling . To address this issue and make systematic use of a known causal structure, we design an algorithm that can meticulously utilize causal structural information. In particular, we make the following contributions:

* We provide a logarithmic asymptotic regret lower bound for the structural causal bandit problem with latent confounders by leveraging the canonical SCM formulation  to create a space of all possible interventional distributions given a causal graph.
* Inspired by the lower bound, we extend  and apply approximate allocation matching to design an algorithm that can effectively utilize causal information provided by the causal graph to accelerate the learning process and take informative and rewarding interventions.
* We analyze the algorithm to provide a problem-dependent logarithmic upper bound on expected regret and compare it with  to show it has a better theoretical guarantee.

* We complement the theoretical results by evaluating the proposed approach in a variety of experimental settings featuring different causal structures. We show that our algorithm outperforms the existing baselines in terms of empirical regret.

## 2 Problem Statement and Background

In this section, we present the formulation of the structural causal bandit problem with latent confounders. We follow the convention to represent a random variable and its value in a capital letter and a lowercase letter respectively. A multivariate random variable is represented in a bold letter.

### Structural Causal Model

Our approach adopts the structural causal model (SCM)  to provide a causal perspective on the data-generating process. This allows us to express the relationship between variables and capture important causal concepts, including unobserved variables, observational distributions, and interventional distributions. For a random variable \(\), let \(()\) denote its domain.

**Definition 1** (Scm).: _A structural causal model is a \(4\)-tuple \(,,,P()\) where:_

1. \(\) _is a set of unobserved independent background variables (also called exogenous), that determine the randomness of the model._
2. \(:=\{V_{i} i=1,,n\}\) _is the set of observable variables (also called endogenous)._
3. \(\) _is a set of structural equations_ \(\{f_{1},f_{2},...,f_{n}\}\) _such that for each_ \(V_{i}\)_,_ \(f_{i}:(_{i})(_{i})(V_ {i})\) _defines a mapping, where_ \(_{i}\) _and_ \(_{i}\) _is the parent set of_ \(V_{i}\)_. So_ \(\) _as a whole determines a mapping relationship from_ \(()\) _to_ \(()\)_._
4. \(P()\) _is a joint probability distribution over all the exogenous variables._

Each structural causal model is associated with a causal graph \(=,,\), where the node set \(\) corresponds to the set of observable variables, \(\) is the set of directed edges and \(\) is the set of bidirected edges. For each \(V_{j}_{i}\), there exists a directed edge in \(\) such that \(V_{j} V_{i}\) which indicates functional dependency from \(V_{j}\) to \(V_{i}\). Without showing unobserved exogenous variable \(\) explicitly, the confounding effects of \(\) are represented using bidirected edges in \(\), which connect multiple pairs of observable variables in \(\). The presence of \(V_{i} V_{j}\) in \(\) represents unmeasured factors (or confounders) that may influence both \(V_{i}\) and \(V_{j}\). In other words, \(f_{i}\) and \(f_{j}\) share common exogenous variables \(_{i}_{j}\) as input. Let \(|()|\) be the cardinality of \(()\).

**Assumption 1**.: _We assume each \(V\) can only take a finite number of values, i.e., \(|()|\) is finite._

Within the causal graph, an intervention on a subset of random variables \(\) denoted by the do-operator \(do(=)\) sets the structural equation for each \(S_{j}\) to be \(S_{j}=s_{j}\). We also refer to \(\) as intervention for brevity. If a node \(V_{i}\), its structural equation remains to be \(V_{i}=f_{i}(_{i},_{i})\). The empty intervention denoted by \(do()\) does not change the structural equation of any random variable, and the distribution of \(\) is also called observational distribution. Let \(1\) denote the indicator function. An SCM induces interventional distributions: if \(\) is consistent with intervention \(\),

\[P( do(=))=_{}P() _{V_{i}}1v_{i}=f_{i}(_{i},_ {i})}:=P_{}().\] (1)

Otherwise, \(P( do(=))=0\). For a more detailed discussion on SCMs, we refer to .

### Causal Bandit Problem with Confounders

The causal bandit problem with confounders studies the sequential decision-making problem with causal information provided by a causal graph \(=,,\) with confounding effects represented by bidirected edge set \(\). The causal graph \(\) is given but the exact interventional and observational distributions are unknown. Without loss of generality, we assume the reward is collected from node \(V_{n}\), which is bounded in \(\) and can not be intervened on. Let \(\{()  V_{n}\}\) be the space of allowed interventions. The decision-maker is required to decide which intervention to apply at each time step. The expected reward of intervention \(\) is \(_{}=_{P_{}}[V_{n}]=_{( )}v_{n}P_{}()\), where \(P_{}\) is defined in (1) and \(_{P}\) means the expectation is computed with probability distribution \(P\).

At each time \(t\{1,,T\}\), the agent follows an adaptive allocation policy \(\) to select an intervention \(_{t}\) and observe the causal bandit feedback \(_{t} P_{_{t}}()\), which is a realization of all observable variables after intervention. In fact, policy \(\) is a sequence \(\{_{t}\}_{t}\), where each \(_{t}\) determines the probability distribution of taking intervention \(_{t}\) given intervention and observation history \(_{t}(_{t}_{1},_{1},,_{t -1},_{t-1})\). Let \(_{*}:=_{}_{}\) and \(_{}=_{*}-_{}\) denote the optimal mean reward and the expected optimality gap of intervention \(do(=)\) respectively. Given a causal graph \(\), the objective of the causal bandit problem is to design a policy \(\) to maximize the expected cumulative reward, or equivalently, to minimize the _expected regret_:

\[R_{T}^{}:=_{t=1}^{T}(_{*}-V_{n,t}) =_{}[N_{T}()]_{ },\] (2)

where \(N_{T}()\) is the total number of times intervention \(do(=)\) is taken by policy \(\) until time horizon till \(T\), and the expectation is computed over different realizations of \(\{_{t},_{t}\}_{t=1}^{T}\) from the interactions between the random policy \(\) and the causal bandit model \( P_{}_{}\). Thus, \(R_{T}^{}\) is the gap between the expected cumulative rewards of selecting the best intervention all the time and that by the policy \(\). The regret decomposition in (2) is from , and it expresses the expected regret with products of the expected number of suboptimal action selections and expected reward gaps.

The large action space in causal bandits poses many challenges to the learning problem. It is desirable to reduce the action space without excluding all optimal interventions. The do-calculus introduced in  provides a set of guidelines for evaluating invariances across interventions. In this context, our specific focus is on Rule 3, which provides the conditions under which a series of interventions have no impact on the outcome variable. For example, Rule 3 implies that \(P_{,}(y)=P_{}(y)\) if we have \((\!\!\!|)_{_{ }}\), where \(_{}\) is the subgraph of DAG \(\) with incoming edges to the set \(\) removed. This concept leads to the notion of a minimal intervention set (MIS), which is a subset of variables \(_{n}\) such that there is no \(^{}\) for which \(_{[^{}]}=_{}\) holds for every Structural Causal Model (SCM) with a causal graph \(\). Here, \([^{}]\) denotes an intervention on \(^{}\) with values consistent with \(\). An MIS \(\) is considered a possibly optimal MIS (POMIS) if some intervention \(()\) can achieve the optimal mean reward in an SCM with a causal graph \(\). We define \(=\{()\}\).

At an intuitive level, it may seem logical that the most effective course of action would be to intervene on the immediate causes (parents) of the reward variable \(V_{n}\). This approach would provide a higher level of control over \(V_{n}\) within the system. If the reward variable \(V_{n}\) is not confounded with any of its ancestors, its parent set \(_{n}\) is the only POMIS. In more general causal graphs where \(V_{n}\) is confounded with any of its ancestors, the paper  proves that multiple POMISs exist, which can include variables that are not parents of \(V_{n}\). The paper also provides graphical criteria and an efficient algorithm for constructing a set of all POMISs for a given causal graph. For an effective method to search for \(\), we refer to  and rely on their graphical criteria to construct a set of POMISs. This set of POMISs is used to form the set of possible optimal actions \(\) for use in the causal bandit algorithm. We make the following assumption, which we expect to hold in a general scenario , except for certain specifically designed SCMs.

**Assumption 2**.: _Within interventions on POMIS, there exists a unique optimal intervention \(_{*}\)._

## 3 Response Variables and Space of Interventional Distribution Tuples

The unspecified domain of \(\) makes it inconvenient to be directly applied to the learning problem. However, it is noted in [29, Ch. 8] that if each variable in \(\) takes finite states, \(()\) can be partitioned and \(\) can be projected to a collection of finite-state response variables. The resulting model is equivalent to the original one with respect to all observational and interventional distributions. Such a technique was used to bound the causal and counterfactual effects with observational distribution [29, Ch. 8]. We take it to define a parameterized space of interventional distribution tuples.

### Response Variables and Canonical Structural Causal Model

In Definition 1, each structural equation \(V_{i}=f_{i}(_{i},_{i})\) connects random variable \(V_{i}\) to its parents \(_{i}\). As \(_{i}\) varies along its domain, regardless of how complex the variation is, the only effect it can have is to switch the relationship between \(_{i}\) and \(V_{i}\). Since there are at most \(|(V_{i})|^{|(_{i})|}\) mapping relationships from \(_{i}\) to \(V_{i}\), we can decompose \(f_{i}\) as follows,

\[V_{i}=f_{i}(_{i},_{i})=_{i}(M_{i},_{i}),\ M_{i}=_{i}(_{i}),\] (3)

where \(M_{i}\{0,,|(V_{i})|^{|(_{i})|}-1\}\) is a random variable corresponds to a mapping from \((_{i})\) to \((V_{i})\). For an observable variable without parents, we simply have \(M_{i}\{0,,|(V_{i})|-1\}\). Such a random variable \(M_{i}\) attached to each \(V_{i}\) is called response variable in . Treating response variables as exogenous variables, the resulting SCM \(,,},P()\) is called _canonical SCM_ in , where \(}=\{_{1},_{2},...,_{n}\}\).

The decomposition of structural equations in equation (3) can be represented graphically by including response variables in \(\). Since \(V_{i}=_{i}(M_{i},_{i})\), for each \(V_{i}\) there is a directed edge \(M_{i} V_{i}\). We also know that if there exists \(V_{i}_{i}\) in \(\), both \(V_{i}\) and \(V_{j}\) are affected by common exogenous variables \(_{i}_{j}\). As a result, \(M_{i}\) and \(M_{j}\) are correlated and they are also connected by a bidirected edge. For example, the causal graph in Fig. 1(a) has a structural equivalent in Fig. 1(b).

Given intervention \(\), there exists a deterministic relationship from \(\) to \(\). To further explain the idea, consider the bow graph example in Fig. 1(c), where \(V_{1},V_{2}\) are binary variables taking value \(0\) or \(1\). As illustrated above, the response variables are \(M_{1}\{0,1\}\) and \(M_{2}\{0,1,2,3\}\), and the structural equations for \(V_{1}\) and \(V_{2}\) are as follows:

\[V_{1}=_{1}(M_{1})=M_{1},\ V_{2}=_{2}(M_{2},V_{1})= 0&M_{2}=0\\ V_{1}&M_{2}=1\\ 1-V_{1}&M_{2}=2\\ 1&M_{2}=3.\]

If \(do()\) is applied, \((V_{1},V_{2})=(0,1)\) when \((M_{1},M_{2})=(0,2)\) or \((0,3)\). If \(do(V_{1}=1)\) is applied, \((V_{1},V_{2})=(1,0)\) when \(M_{1}=0\) or \(1\) and \(M_{2}=0\) or \(2\). It shows that for a given intervention \(do()\) or \(do(V_{1}=1)\), there exists a deterministic mapping from \(\) to \(\). Besides, the true value of \(\) can only be inferred from \(\) since \(\) is unobservable. Since \(M_{1}\) and \(M_{2}\) are correlated, we define their joint distribution as \(P(M_{1}=i,M_{2}=j)=p_{ij}\). We can further express the observational and interventional probabilities of \(\) with the probability of \(\) as follows,

\[P(V_{1}=0,V_{2}=0) =p_{00}+p_{01}, P(V_{1}=0,V_{2}=1) =p_{02}+p_{03},\] (4) \[P(V_{1}=1,V_{2}=0) =p_{10}+p_{12}, P(V_{1}=1,V_{2}=1) =p_{11}+p_{13},\] \[P_{do(V_{1}=0)}(V_{2}=0) =p_{00}+p_{01}+p_{10}+p_{11},\ P_{do(V_{1}=0)}(V_{2}=1) =p_{02}+p_{03}+p_{12}+p_{13},\] \[P_{do(V_{1}=1)}(V_{2}=0) =p_{00}+p_{02}+p_{10}+p_{12},\ P_{do(V_{1}=1)}(V_{2}=1) =p_{01}+p_{03}+p_{11}+p_{13}.\]

It can be seen these parameters are sufficient for specifying the model of Fig. 1(c). Such a parameterization technique can be extended to general causal graphs with latent confounders .

Figure 1: Causal graphs and their structural equivalents

### Space of Interventional Distribution Tuples

In a causal graph, a confounded component (c-component) is a maximal set of vertices connected with bidirected edges . Note that a singleton node without bidirected edges is also a c-component. For example, there are two c-components in Fig. 1(a): \(\{V_{1},V_{3},V_{4}\}\) and \(\{V_{2}\}\). In fact for any causal graph \(\), its observable variables \(\) can be uniquely partitioned into c-components \(\{^{1},,^{n_{}()}\}\), where \(n_{}()\) is the total number of c-components in \(\). Then \(\) can also be partitioned into \(\{^{1},,^{n_{}()}\}\), where each \(^{j}\) contains response variables adjacent to \(^{j}\). For example in Fig. 1(b), \(\) is partitioned into \(\{M_{1},M_{3},M_{4}\}\) and \(\{M_{2}\}\). Within each \(^{j}\), the response variables are correlated since they are connected by bidirected edges. Besides, \(^{1},,^{n_{}()}\) are mutually independent since \(^{i}\) and \(^{j}\) are not connected with bidirected edges for any \(i j\). As a result, \(P()=_{j=1}^{n_{}()}P(^{j})\). By concatenating \(P(^{j})\) for each \(^{j}(^{j})\), we construct a vector \(_{j}(|(^{j})|)\), where \((|(^{j})|):=\{_{j}^{} _{ 0}^{|(^{j})|}^{}_{j}^{ }=1\}\) and \(^{}\) is the transpose of the all-ones vector.

Let the parent set of a c-component \(^{j}\) be \(_{^{j}}:=(_{i:V_{i}^{j}}_{i}) ^{j}\). When taking intervention \(do(=)\), the values of \(^{j}\) is set to \([^{j}]\), which denotes the values of \(^{j}\) that is consistent with \(\). Notice that \(^{j}\) picks the mapping functions from \(_{i}\) to \(V_{i}\) for all \(V_{i}^{j}\). Seeing values \(^{j},_{^{j}}\) and \([^{j}]\), there exists a set of configurations of \(^{j}\), denoted by \(B_{,[^{j}]}(^{j},_{ ^{j}})(^{j})\), that can make this happen. By marking configurations in \(B_{,[^{j}]}(^{j},_{ ^{j}})\) with \(1\) and others to be \(0\), we construct a vector \(b_{,[^{j}]}(^{j},_{ ^{j}})\{0,1\}^{|(^{j})|}\). With \(^{1},,^{n_{}()}\) being mutually independent, the interventional distribution can be factorized as

\[P_{}()=_{j=1}^{n_{}()}P ^{j} B_{,[^{j}]}(^{j}, _{^{j}})=_{j=1}^{n_{}()} b_{,[^{j}]}^{}(^{j}, _{^{j}})_{j}.\] (5)

In the bow graph example in Fig. 1(c), there is one c-component, and vector \(b_{,[^{j}]}^{}(^{j},_ {^{j}})\) can be constructed by referring to (4), where \(_{1}\) is a concatenation of \(p_{i,j}\). The result in (5) generalizes the parameterization for the bow graph to a general causal graph. Based on it, given a causal graph \(\), we define the space of interventional distribution tuples as the following.

**Definition 2** (Space of Interventional Distribution Tuples).: _Given a causal graph \(=,,\), the space of interventional (and observational) distribution tuples \(= P_{}_{}\) is_

\[_{}:=^{} ,():P_{}^{ }()=_{j=1}^{n_{}()}b_{, [^{j}]}^{}(^{j},_{^{ j}})_{j},_{j}(|(^{j})|) }.\]

Space \(_{}\) contains all interventional distribution tuples associated with canonical SCMs \(,,},P()\) with arbitrary \(P()\). Since the space of \(\{_{1},,_{n_{}()}\}\) is compact and a continuous image of a compact space is compact, \(_{}\) is also a compact space.

## 4 An Asymptotic Regret Lower Bound

In this section, we present an asymptotic information-theoretic regret lower bound for the causal bandit problem with confounders. It quantifies the optimal asymptotic performance of a uniformly good causal bandit policy defined below.

**Definition 3** (Uniformly Good Policy).: _Given a causal graph \(=,,\), a causal bandit policy \(\) is uniformly good if for any \(>0\), the expected regret of \(\) for any interventional distribution tuple setup \(_{}\), denoted by \(R_{T}^{}()\), satisfies_

\[_{T}R_{T}^{}/T^{}=0.\]

A policy \(\) being uniformly good indicates that it can achieve subpolynomial regret for any \(_{}\). The information-theoretic regret lower bound sets the limit for the asymptotic regret of all such policies. Let \(D(P Q)\) denote the Kullback-Leibler (KL) divergence of two probability distributions \(P\) and \(Q\) and let \(_{-}=\{\}\).

**Theorem 1**.: _Given a causal graph \(=,,\), let \(= P_{}_{}_{}\) be the true interventional distribution tuple with a unique optimal intervention \(_{*}\) with mean reward \(_{*}\). The expected regret for any uniformly good causal bandit policy \(\) satisfies_

\[_{T}R_{T}^{}/ T C(,),\]

_where \(C(,)\) is the value of the optimization problem given below,_

\[(,):  0,_{- _{*}}}{}_{s_{-_{*}}} _{}_{},\] (6) \[ _{_{-_{*}}}_{ }^{}_{*},^{}^{ }_{}\ \ _{_{-_{*}}}_{ }D(P_{} P_{}^{})<1,P_{ _{*}}^{}=P_{_{*}}},\] (7)

_where \(_{}^{}\) is the expected reward of intervention \(\) according to probability distribution \(P_{}^{}\)._

Theorem 1 can be viewed as an extension of (10, Th. 1), and we defer its proof to appendices. The optimization problem \((,_{})\) is a semi-infinite program since there are infinite \(^{}\) in (7). To interpret the lower bound, (6) is a minimization of the regret, and its solution indicates an optimal allocation of \(O( T)\) number of explorations, which is \(_{} T\) for each \(_{-_{*}}\) as \(T\). In (7), \(_{}_{}D(P_{} P_{ }^{})\) can be viewed as the distance generated between true interventional distribution tuple \(\) and an alternative \(^{}_{}\) by an exploration allocation strategy \(_{}_{_{-_{*}}}\). For any \(^{}\) in (7), \(_{_{*}}^{}=_{*}\) since \(P_{_{*}}^{}=P_{_{*}}\), and as a result, \(_{_{-_{*}}}_{}^{} _{*}\) indicates intervention \(_{*}\) is also optimal in \(^{}\). So (7) sets a constraint for \(_{}_{_{-_{*}}}\): the models allowed to have distance \(<1\) from the true model must also take \(_{*}\) to be optimal. Or conversely, \(_{}_{_{-_{*}}}\) must generate distance \( 1\) for any \(^{}_{}\) with \(P_{_{*}}^{}=P_{_{*}}\) not taking intervention \(_{*}\) to be optimal.

**Remark 1**.: _The \(O( T)\) regret lower bound holds only if there exists \(^{}_{}\) such that \(P_{_{*}}^{}=P_{_{*}}\), which is a condition meaning \(\) can not be distinguished from all other interventional distribution tuples in \(_{}\) by only taking intervention \(_{*}\). This condition holds for almost all causal bandit problems. Otherwise, constraint (7) is relaxed, and the value of the optimization problem \(C(,_{})=0\), which indicates sub-logarithmic regret can be achieved. Following the same line of proof in (17, Th. 3.9), it can be shown simple UCB can achieve this sub-logarithmic, in fact constant, expected regret._

## 5 SCM-based Causal Bandit Algorithm

The regret lower bound in Theorem 1 suggests a potentially optimal exploration strategy, i.e. to take each intervention \(_{-_{*}}\) up to \(_{} T\) times. However, it requires the ground truth \(\) to solve the optimization problem \((,)\). The allocation matching principle  essentially replaces the true model with an estimated one to solve the optimization problem, and controls explorations to match with the solution. It is expected that the estimated model converges to the ground truth at a fast enough rate so that the actual explorations match with the optimal exploration strategy. We follow this idea to design a causal bandit policy, whose pseudo-code is shown in Algorithm 1. We call this SCM-based Approximate Allocation Matching (SCM-AAM).

Similar to , SCM-AAM only intervenes on POMISs for the purpose of reducing the action space. The algorithm is anytime without requiring the knowledge of horizon length. It takes two tuning parameters \(=(0,1/||)\) and \(>0\) as inputs that control the exploration rate, which affects finite-time performance. At each time \(t\), let \(N_{t}()\) be the number of times the intervention \(do(=)\) is taken so far, and let \(N_{t}(,)\) be the number of times we observe \(\) with intervention \(do(=)\). The algorithm maintains a set of empirical interventional distributions \(_{,t}_{}\) and empirical mean rewards \(_{,t}_{}\), where \(_{,t}()=N_{t}(,)/N_{t}()\) for each \(\).

The main body of the algorithm is composed of two components: exploitation and exploration. At each round \(t\), it first attempts to evaluate if enough information has been collected to determine the optimal action. Inspired by the lower bound, enough distance is generated by the sampling history between \(_{,t}_{}\) and \(^{}= P_{}^{}_{ }\) if \(_{}N_{t}()D(_{,t} P _{}^{})>(1+) t\). So we can construct a confidence set \(_{t}\) with high probability to contain the true model \(\), which is defined as

\[_{t}:=^{}_{}\ \ _{ }N_{t}()D(_{,t} P_{ }^{})(1+) t}.\] (8)With \(V_{n}\) being the reward node, for each \(\), the algorithm computes the UCB of the mean reward of taking intervention \(\) as \(_{}(_{t})=_{()} v_{n}_{^{}_{t}}P_{}^{}( )\). If there exists an intervention \(_{*,t}\) with empirical mean reward \(_{_{*,t}}>_{}(_{t}), _{-_{*,t}}\), we are confident about \(_{*,t}\) to be the optimal intervention. So the algorithm enters the exploitation phase and selects \(_{t}=_{*,t}\).

If no such \(_{*,t}\) exists, the algorithm needs to explore. According to the allocation principle, one is inclined to solve \((_{,t}_{}, )\) defined in Theorem 1 and make \(N_{t}()_{} t\) for each \(\). Nevertheless, there is no guarantee that this semi-infinite program can be solved efficiently. Therefore, we take a greedy approximation approach. Ideally, to get out of exploration, the algorithm needs to reduce \(_{_{-_{*}}}_{}( _{t})\) to a value below \(_{*}\), thus it selects \(}_{t}=_{}_{ }(_{t})\). The algorithm maintains a counter of exploration steps \(N_{t}^{}\). If \(_{}N_{t}() N_{t}^{}\), it generates forced exploration by selecting the least selected intervention \(}_{t}\). Such a mechanism ensures every action is persistently taken in exploration phases so that \(_{,t}_{}\) converges to the ground truth eventually.

**Remark 2**.: _With factorization in (5), \(_{t}\) corresponds to setting constraint for \(\{_{1},,_{n_{}()}\}\) as_

\[_{}_{()}N_{t}( ,)_{,t}()}{_{j=1 }^{n_{}()}b_{,[^{j}]}^{ }(^{j},_{^{j}})_{j}}(1+ ) t,\] (9)

_where the expression on the left of inequality is a convex function of \(_{1},,_{n_{}()}\). Define convex set \(_{t}^{}=\{_{1},,_{n_{ }()}\}_{j}(|(^{j})|),  j\{1,,n_{}()\}\), and (9) is true\(}\). Then the UCB can be derived by maximizing a set of concave functions over \(_{t}^{}\) as follows,_

\[_{}(_{t})=\!\!\!\!\!_{ ()}\!\!\!\!v_{n}_{^{}_{t}}P_{ }^{}()=\!\!\!\!_{() }\!\!\!\!v_{n}_{\{_{1},,_{n_{}()}\}_{t}^{}}_{j=1}^{n_{}( )}b_{,[^{j}]}^{}( ^{j},_{^{j}})_{j}.\]

_We use \(_{}(_{t})\) instead of \(_{}^{}(_{t})=_{^{} _{t}}_{()}v_{n}P_{}^{ }()\) as UCB since \(_{()}v_{n}P_{}^{}()\) in general is a non-concave function of \(_{1},,_{n_{}()}\). Also, notice that \(:_{}(_{t}) _{}^{}(_{t})\)._

In a pure topology-based approach , each intervention on POMISs is treated independently during the algorithm execution. Whereas SCM-AAM can leverage the canonical SCM to incorporate the sampling results from different interventions together. After each new sample \(_{t}\) with intervention \(_{t}\) is observed, it is used to update the parametric space for interventional distribution tuples \(_{t}\).

## 6 Finite Time Regret Analysis

We present a finite time problem-dependent regret upper bound for SCM-AAM in Theorem 2. The complete proof is provided in the appendices. We also give an interpretation of Theorem 2, and compare the regret upper bound with  in the setting where the reward node \(V_{n}\) is binary,

**Theorem 2**.: _For the causal bandit problem with unobserved confounders, suppose the causal graph \(=,,\) is given and Assumptions 1 and 2 are true. For any interventional distribution tuple \(_{}\), any \(>0\) and horizon of length \(T\), the expected regret for SCM-AAM with parameters \((0,1/||)\) and \(>0\) satisfies_

\[R_{T}^{}()(1+)(1+)_{ _{-_{*}}}_{}()_{ }+|}{1-||} T+c,\]

_where \(c\) is a suitably large universal constant depending on \(\), \(\) and tuning parameters \(\) and \(\), and_

\[_{-_{*}}:_{}( )=0,&_{}(_{ }(0,))_{*},\\ _{_{} 0}_{}\ \ \ _{ }(_{}(_{},))_{* },&,\]

_in which \(_{}(_{},):=^{ }_{}_{}D(P_{} P ^{}_{})+_{}/2_{ _{-_{*}}}D(P_{} P^{}_{}) 1}\)._

Theorem 2 indicates the expected number of samples for each \(_{-_{*}}\) can be approximately bounded by \(_{}() T\). It can be interpreted as follows: when \(_{-_{*}}\) is selected approximately up to \(_{}() T\) times, its UCB can be pushed below \(_{*}\). The presence of terms containing \(\) is due to forced exploration. In , KL-UCB and Thompson sampling are employed to apply interventions on POMISs. If the reward node \(V_{n}\{0,1\}\), they enjoys expected regret \(_{_{-_{*}}}(_{} T)/D (P_{}(V_{n}) P_{_{*}}(V_{n}))+c^{}\) for some insignificant \(c^{}\)[14; 2]. In this situation, Theorem 2 shows that the SCM-AAM enjoys smaller expected regret when we disregard the small constants \(\), \(\) and \(\). To see it, we express the leading constant for KL-UCB and Thompson as

\[1/D(P_{}(V_{n}) P_{_{*}}(V_{n}))=_{_{ } 0}_{}\ \ \ _{}(^{}_{ }(_{},P(V_{n})))_{*},\]

where \(^{}_{}(_{},P(V_{n}))=\{P^{}_{ }(V_{n})_{}D(P_{}(V_{n}) P ^{}_{}(V_{n})) 1\}\). Note that \(^{}_{}(_{},P(V_{n}))\) only sets constraint on \(P^{}_{}(V_{n})\), while \(_{}(_{},)\) sets constraints on \(P^{}_{}()\). As a result, \(_{}(_{},)^ {}_{}(_{},P(V_{n}))\), so that \(_{}() 1/D(P_{}(V_{n}) P_{ _{*}}(V_{n}))\).

## 7 Experimental Results

We compare the empirical performance of SCM-AAM with existing algorithms. The first baseline algorithm employs the simple UCB algorithm  on a reduced action set that includes interventions on POMISs. The other two baselines as introduced in , utilize the KL-UCB algorithm and Thompson sampling (TS) to intervene on POMISs. We compare the performance of all the algorithms on three different causal bandit instances shown in Fig. 2. We choose the input parameters of the SCM-AAM algorithm to be \(=0.1\) and \(=1/|5L|\) in the simulations. We set the horizon to \(800\) for all three tasks and repeat every simulation \(100\) times. Additionally, we include a confidence interval around the mean regret, with the width of the interval set to twice the standard deviation. The structural equations for the three causal bandit instances, as well as experimental details, can be found in the appendices. We plot the mean regret against time, where time in this context corresponds to the number of actions. All the nodes in Fig. 2 take binary values, either \(0\) or \(1\).

For task 1, we consider the causal graph shown in Fig. 2(a) with POMISs \(\{V_{1}\}\) and \(\{V_{2}\}\). For task 2 and task 3, we consider more complex causal graphs, as displayed in Fig. 2(b) and (c). The POMISs for task 2 include \(\{V_{1}\}\), \(\{V_{3}\}\), and \(\{V_{3},V_{4}\}\), while for task 3, they are \(\{V_{1},V_{4},V_{6}\}\) and \(\{V_{5},V_{6}\}\). The UCB algorithm has inferior performance compared to other baseline algorithms, especially in the more complicated tasks \(2\) and \(3\). The TS algorithm outperforms both UCB and KL-UCB in all three settings; however, it still incurs more regret than our proposed algorithm. The experiments demonstrate that our proposed SCM-AAM algorithm consistently outperforms other baseline algorithms by incurring lower empirical regret across all three tasks. Notably, the performance advantage of SCM-AAM is more significant for tasks 2 and 3, which have a more intricate causal graph structure with a higher number of nodes and edges. The results indicate thatincorporating causal structural information through the SCM-AAM algorithm can significantly reduce expected regret, especially for more complex causal graphs.

The simulations were conducted on a Windows desktop computer featuring a \(12\)th generation Intel Core i7 processor operating at \(3.1\) GHz and \(32\) GB of RAM. No GPUs were utilized in the simulations. The runtime for each iteration of the SCM-MAB, involving \(800\) arm plays, was approximately \(2\) minutes for both Task \(1\). However, for Task \(2\) and Task \(3\), which encompass larger underlying causal graphs, the runtime for every iteration increased to around \(20\) minutes. The algorithm code is provided at https://github.com/CausalML-Lab/SCM-AAM.

## 8 Conclusion

In this paper, we studied the causal bandit problem with latent confounders. With causal information provided by a causal graph, we present a problem-dependent information-theoretic regret lower bound. Leveraging the canonical SCM, we take an approximate allocation matching strategy to design the SCM-AAM algorithm. By analyzing SCM-AAM, we show it has a problem-dependent logarithmic regret upper bound. The analytic result is complemented with numerical illustrations featuring a variety of causal structures. It is shown that SCM-AAM exhibits promising performance in comparison with classic baseline algorithms. Since SCM has a natural application in counterfactual reasoning, extending the proposed algorithm and theoretical results to a counterfactual decision-making setup is an interesting future direction.