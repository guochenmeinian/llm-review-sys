# Beyond Slow Signs in High-fidelity Model Extraction

Hanna Foerster and Robert Mullins

University of Cambridge

&Ilia Shumailov and Jamie Hayes

Google DeepMind

###### Abstract

Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confidentiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al.  further enhanced by Canales-Martinez et al.  for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to \(14.8\) times compared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.

## 1 Introduction

Training machine learning (ML) models requires not only vast datasets and extensive computational resources but also expert knowledge, making the process costly. This makes models lucrative robbery targets. The rise of ML-as-a-service further amplifies the challenges associated with balancing public query accessibility and safeguarding model confidentiality. This shows the emerging risk of model extraction attacks, where adversaries aim to replicate a model's predictive capabilities from a black box setting where only the input and output can be observed. Previous attacks have approached model extraction either precisely, obtaining a copy of the victim model, or approximately, obtaining an imprecise copy of the victim model. Components of the model that have been the target of extraction include training hyperparameters , architectures , and learned parameters such as weights and biases in deep neural networks (DNNs) .

In this paper, we are interested in precise model extraction and use the most recent advances of cryptanalytical extraction of DNNs by Carlini et al.  and Canales-Martinez et al.  as our starting point. Carlini et al. previously demonstrated that it is feasible to extract model _signatures_ - normalised weights of neural networks - on relatively small models with up to three hidden layers. For 1 hidden layer models the parameter count tested was up to 100,480, however, for 2 and 3 hidden layer models, which are increasingly difficult to extract, the maximum parameter count for models tested was 4,020. While signature extraction was generally determined as straightforward, _sign_ extraction of the weights was identified as a bottleneck with further work needed. Following this, Canales-Martinez et al.  improved sign extraction speed from exponential to polynomial time, with their NeuronWiggle method. However, Canales-Martinez et al. stopped short of evaluating the full extraction pipeline, focusing only on the signs.

In our work, we perform a deeper performance evaluation of the signature and sign extraction methods. We first create a comprehensive codebase that integrates Carlini et al. 's signature extraction technique with the sign extraction technique of Canales-Martinez et al. , allowing for systematic and fair benchmarking. We identify inefficiencies in the combined _signature-sign_ method interactions and discover that we can significantly improve on the end-to-end attack efficacy. Importantly, we find that Canales-Martinez et al.'s sign extraction already eliminates the sign extraction bottleneck observed in prior work. Further improving on sign extraction we speed up the process by up to \(14.8\) times compared to Canales-Martinez et al., so that sign extraction only takes up as little as \(0.002\%\) of the whole extraction time for larger models. For larger models due to randomness of the start of the search and the position of neurons in space, the signature extraction can take a long time, highlighting the importance of considering the full pipeline extraction compared to only the sign extraction. For smaller or less complex models, signature extraction is much faster, and then the sign extraction time becomes more significant. For these models we see on average that the whole parameter extraction process is sped up by about \(1.2\) times and a speed up of up to \(6.6\) times can be attained when quantizing some extraction sub-routines to float32.

We make the following contributions:

1. **Optimizing Extraction Strategies:** We modify the extraction process to only sign extract neurons requiring trivial effort, finding that spending more time in extracting harder to sign-extract neurons does not lead to higher success in correct sign extraction. This significantly reduces the number of queries needed. We find that these harder to sign-extract neurons' sign extraction can be pipelined with other operations, improving both robustness and speed of sign extraction. An additional deduplication process and a suggestion to quantize some sub-routines speeds up the overall extraction time.
2. **Redefining Bottlenecks in Extraction Processes:** By optimizing sign extraction, we find that, contrary to earlier studies, extraction is now dominated by signature extraction. This shifts the focus of optimization efforts for achieving scalable high-fidelity extraction.
3. **Addressing Methodological Shortcomings:** We determine that improvements reported in some prior work come from unfair comparison between (1) standard benchmarks and models trained on random data; (2) models trained with different randomness; (3) models extracted using different randomness; (4) models with more hidden layers or different sizes.

Our codebase can be found at https://github.com/hannafoe/cryptanalytical-extraction.

## 2 Related Work

The analysis of neural network model extraction strategies in Jagielski et al.  categorises three types of extraction attacks. _Functionally equivalent extraction_ aims to replicate the target model's output for each input and is the most exact method of extraction. _Fidelity extraction_ seeks to closely mirror the target model's output on specific data distributions, such as label agreement. _Task accuracy extraction_ aims to match or surpass the target model's task performance without replicating errors. Fidelity and accuracy differ in that the goal for fidelity is close replication of the target model, where the target model is considered the benchmark label, whereas for task accuracy the goal is to closely mirror the ground-truth labels of the dataset.

### Learning-based Methods: Fidelity and Task accuracy extraction

Learning-based methods that train a substitute model closely mirroring the target model are used for fidelity and task accuracy extraction. Since knowledge from the target blackbox model is transferred to a simpler model, architecture knowledge is not always necessary , but according to Oh et al.  is found to raise fidelity. Moreover, while the training dataset is often not assumed, many works such as Tramer et al.  assume knowledge of the problem domain and training data statistics to recover data distributions, and it has been shown that models trained on non-domain data significantly underperform [9; 11]. For instance, Papernot et al.  train models on self-created datasets mimicking MNIST, while others use GANs or other methods to produce data for training (Oliynyk et al. , Truong et al. , Correia-Silva et al. ).

Jagielski et al.  discuss how learning-based methodologies face challenges due to various non-deterministic factors, such as the random initialization of model parameters, batch formation sequence, and the unpredictable behaviours in GPU processing. In a study where the adversary is assumed to have complete access to both the training data and hyperparameters, and where the original model is used as a labeling oracle to help train a replication model with the same parameters from scratch, \(93.4\%\) was the maximum fidelity reached by the replicated model. This is attributed to the non-deterministic elements in the learning processes of both the original and replicated model. More recent work by Martinelli et al.  that suggests that high fidelity can actually be reached with learning based methods. However, they assume a slightly different setting, and this only works for smaller networks, and appears to be more expensive.

### Cryptanalytical Methods: Functionally equivalent extraction

The fidelity limitations of learning-based methods have prompted the development of techniques based on side-channel analysis or cryptanalysis to precisely extract parameters and achieve functional equivalence. Notable methods, such as those introduced by Lowd and Meek , Milli et al. , Batina et al. , Jagielski et al. , while contributing to this area, exhibit limitations in handling standard benchmarks or are confined to neural networks with only two layers, limiting its practicality. Rolnick and Kording  and Carlini et al. , attempt to extend cryptanalytical attacks to deeper neural networks but face efficiency limitations in larger models. Side-channel techniques appear inefficient for parameter extraction and are primarily used for architecture extraction , and so we focus on cryptanalytical parameter extraction continuing from Carlini et al. 's work.

Carlini et al.  successfully extract model parameters from fully connected neural networks with precision up to float64, applicable even to deeper models with multiple hidden layers. Their tests show that a model with one hidden layer allows extraction of up to 100,480 parameters using \(2^{21.5}\) queries, while a three-layer model with 1,110 parameters is extracted with \(2^{17.8}\) queries. This technique requires that the DNN uses ReLU activations, that the weights are in high precision, and that the output logits can be obtained. Unlike learning-based methods, it does not rely on known training data since it uses queries generated from a normal distribution or calculated coordinates that expose network parameters. However, the extraction of deeper models has not been tested, and all tests are conducted on 'random' models trained on data randomly drawn from a normal distribution. Their extraction is divided into two parts which includes the extraction of weight values up to a multiple and the extraction of the sign of the weights. The authors also note that sign extraction, while requiring only a polynomial number of queries, requires an exponential amount of time. The approach requires an exhaustive search over \(2^{n}\), where \(n\) is the number of neurons in a layer .

Canales-Martinez et al.  improved the sign extraction to be polynomial time. They apply extraction to a model with 8 hidden layers with 256 neurons each trained on CIFAR10, claiming that the entire sign extraction only required 30 minutes on a 256-core computer. However, Canales-Martinez et al. did not test the signature extraction and assumed signature extraction time to be relatively insignificant. So, these claims only hold for the sign extraction part of parameter extraction.

In our work, we use models trained on MNIST and CIFAR10, as well as randomly generated data, to benchmark full parameter extraction. Prior works have focused only on partial parameter extraction  or full extraction on models trained on random data . Considering metrics such as query and time efficiency, we analyze the performance impact of Canales-Martinez et al.'s sign extraction method and propose additional optimization strategies.

## 3 Methodology

In what follows we explain the background methodology of sign extraction in Section 3.1 and its inefficiencies in prior work in Section 3.2, describe improvements to it and analyse its effect on the whole parameter extraction in Sections 3.3, 3.4, and finally outline some other improvements in Section 3.5.

### Background: Understanding Sign Extraction

Neurons are the most basic components of neural networks. They create decision boundary hyperplanes in the parameter space and our goal is to identify these. These boundaries are defined by weights and the bias associated with the neuron and it is this hyperplane equation that ultimately decides which inputs activate a neuron. Each neuron contributes to the final output of the model by either activating or deactivating with some given input. Locating the decision boundaries involves two main steps: identifying the neuron's value and determining its sign. While the exact scale of the hyperplane's normal vector is not crucial (as the hyperplane can be identified up to a multiplicative factor), the sign is critical as it determines how it partitions the parameter space and on which side of the hyperplane the inputs fall. In other words, the orientation of the decision boundary hyperplane is determined by the sign of the neuron. In a DNN, the sign determines the outcome of the matrix multiplication involving the input and weight matrix and the subsequent application of the ReLU activation. Depending on the sign of a neuron, a positive outcome could change to negative and be turned into \(0\) through the ReLU activation, turning off the neuron's contribution to the final model output. Carlini et al.  introduce the concept of a neuron's signature, a set of normalised weight ratios \((}{a_{1}},}{a_{1}},,}{a_{1}})\), where each \(a_{i}\) is an individual weight. Each signature signifies the neuron's decision boundary up to sign. They, however, identify the subsequent extraction of the sign as the bottleneck of the whole parameter extraction process. This is why Canales-Martinez et al.  develop the _Neuron Wiggle_ method which speeds sign extraction up to polynomial time. In the following, we describe this sign extraction method, in order to describe our contributions on top of this. Please refer to Carlini et al.  and Canales-Martinez et al.  for an explanation of the whole parameter extraction routine that includes signature extraction.

**Neuron Wiggle Sign Recovery:** A wiggle at layer \(i\) is defined as a vector \(^{d_{i-1}}\) of perturbations, where \(d_{i-1}\) is the dimension of layer \(i-1\). The preimage of this wiggle at the input layer is added to witnesses, i.e., inputs **x** that place a neuron \(\) on the decision boundary of being activated or deactivated. At these 'critical points' on the decision boundary, the output of the neuron is \(0\) and hence the effect of the perturbation on the neuron can be isolated. We define \(^{i}\) to be the extracted weight matrix in layer \(i\) and \(^{i}_{k}\) to be the \(k\)th row corresponding to the weights of neuron \(k\). Then, when the perturbation is added to the witness, neuron \(k\) of layer \(i\) changes its value by \(e_{k}=}^{i}_{k},\), i.e., the dot product of the extracted weights of neuron \(k\) with the wiggle. Say the vector \(=\{c_{1},...,c_{d_{i}}\}\) expresses the output coefficients of **x** in layer \(i\). Then the difference in output that is induced by the wiggle can be expressed as \(_{k}c_{k}e_{k}\), where \(\) contains all indices of active neurons in layer \(i\).

We compute the maximal wiggle that affects only one neuron by aligning the wiggle \(\) parallel to \(}^{i}_{j}\). This alignment ensures that the absolute value of the dot product \(|}^{i}_{j},|\) is maximised, making \(\) effective in activating the target neuron \(j\) while minimizing impact on others. The reason for this is because \(\) being parallel to \(}^{i}_{j}\) has either the same sign or opposite sign to \(}^{i}_{j}\). So then, all summands in the dot product will have the same sign, preventing cancelling out each others effects and maximizing the summation. If no other row is a multiple of \(_{j}\) then the dot product of the row concerning \(_{j}\) with the wiggle should be bigger than the dot product of the wiggle with any other row of the extracted matrix. Overall, neuron wiggle sign recovery involves the following steps:

1. Project \(}^{i}_{j}\) onto the orthogonal basis for the vector space at layer \(i-1\) and scale this projection to a small norm, \(^{i-1}\), so that its influence in the output is minimal and only on the targeted neuron.
2. Calculate the preimage of this vector \(\) to obtain a wiggle in the input dimension \((F^{(i-1)})^{-1}()=\).
3. Calculate \(f(^{*})\), \(f(^{*}+)\), \(f(^{*}-)\).
4. Examine whether addition or subtraction of the wiggle activates the neuron. 1. If adding \(\) increases the magnitude of the output more than subtracting it, i.e., \(|f(^{*}-)-f(^{*})|<|f(^{*}+)-f (^{*})|\), then \(_{j}\) is positive. So, \(}^{i}_{j}=^{i}_{j}\). 2. Conversely, if subtracting \(\) results in a higher magnitude of the output difference, i.e., \(|f(^{*}-)-f(^{*})|>|f(^{*}+)-f (^{*})|\), then \(_{j}\) is negative. So, \(-}^{i}_{j}=^{i}_{j}\)

In this paper, we focus on further reducing \(s\), the critical parameter affecting sign extraction confidence and performance in Canales-Martinez et al. 's method. Ultimately our optimisations reduce sign extraction to become an insignificant fraction of time and query effort of parameter extraction.

**Why is Confidence needed?** From the above explanation, the output effect of a target neuron is represented as the vector \(c_{j}e_{j}\), where \(c_{j}\) is the coefficient and \(e_{j}\) is the effect of the target neuron. The combined output effect of all other active neurons \(\) is expressed as \(_{k\{j\}}c_{k}e_{k}\). Incorrect sign recovery occurs under certain conditions: (**Necessary Condition**) the signs of \(c_{j}e_{j}\) and \(_{k\{j\}}c_{k}e_{k}\) are opposite; (**Sufficient Condition:**) the magnitude of \(c_{j}e_{j}\) is not significant compared to the magnitude of \(_{k\{j\}}c_{k}e_{k}\). This leads to a scenario where \(|c_{j}e_{j}+_{k\{j\}}c_{k}e_{k}|< |_{k\{j\}}c_{k}e_{k}|\), meaning that the total activation output when the target neuron is active is less than the output when the target neuron is inactive. This arises when the effect of the target neuron's activation is overwhelmed by the opposing effects of the other neurons. Thus, we refer to "hard" neurons as those for which it is particularly challenging to compute a wiggle that isolates the neuron effectively.

To reliably determine the correct sign of the target neuron's effect, it is necessary that \(|c_{j}e_{j}|>2|_{k\{j\}}c_{k} e_{k}|\), which implies that the influence of the target neuron's wiggle must be greater than twice the aggregate effect of the other active neurons' wiggles. However, this criterion cannot be checked in practice because we would need to single out the contribution of each of the other neurons to the output given the witness plus perturbation as input. Yet, since the witness is not a witness to any of the other neurons, we cannot single out their effects. This is why Canales-Martinez et al.  introduce the concept of _confidence_. A series of \(s\) different sign extractions on \(s\) critical points and varying \(\) are conducted to gather diverse output responses. The _confidence_ of this sign recovery is \(s_{majority}/s\), where \(s_{majority}\) is the number of sign extractions supporting the majority sign. The idea is that with more sign extractions, enough Neuron Wiggles will be constructed that are not overwhelmed by the opposing effects of other neurons, so that a confident decision can be made. In fact, according to Canales-Martinez et al., the problem of wrong signs extracted with the Neuron Wiggle method should be "fixable by testing more critical points" . So, after testing on CIFAR10, they decide on fixing \(s\) to 200 and recommend rerunning the Neuron Wiggle method on the least-confident \(10\%\) of sign recoveries.

### Our Discoveries: Confidence In Practice

In practice, many neurons do not exhibit increased confidence even with additional iterations \(s\). After an initially higher confidence on average due to not having seen enough samples, the confidence stabilises after about 10 iterations. Moreover, as illustrated in Figure 1(b), the number of correctly identified neurons does not increase beyond \(s=25\) for many models. Furthermore, sign recovery confidence varies notably. Neurons that are correctly recovered typically show a confidence level

Figure 1: (a) Compares the running times for Carlini’s signature extraction versus Carlini’s sign extraction, Canales-Martinez (**CM**)’s sign extraction with \(s=200\) setting in the original implementation and in the unified implementation and Our sign extraction with \(s=15\) setting. The tests are across ten models with increasing layer sizes from \(10-5-5-1\) to \(100-50-50-1\), detailing times for a single layer’s extraction in a non-parallelised setting. (b) Depicts how the average percentage of correctly recovered neurons in a layer changes when the number of sign extractions \(s\) changes. Raising the number of sign extractions \(s\) to more than 15 does not significantly raise the number of correctly recovered neurons. (c) Graph showing confidences of sign recovery when a hard neuron’s euclidean distance to its neighbours is manipulated. These results are on hard to sign extract neurons 25 and 26 of an MNIST trained 784-32x8-1 model extracted with seed \(42\). The confidence metric scales from \(1\) to \(0.5\) first on the confidence of false sign recovery, which is equivalent to \(0\) to \(0.5\) of confidence in true sign recovery and then from \(0.5\) to \(1\) on the confidence of true sign recovery, resulting in the scale going from \(1-0.5-1\).

above \(0.75\), while those incorrectly recovered average \(0.64\). This suggests that neurons with lower confidence are more likely to be incorrectly recovered, providing a practical indicator of recovery accuracy.

**'Easy' and 'Hard' to Sign Extract Neurons:** The idea that some neuron signs are "easier" to extract and some are "harder" to extract can be understood by picturing two neurons that are positioned very close to each other within a layer. Creating a "wiggle" that activates one without impacting adjacent neurons is difficult and becomes more complex as the number of tightly clustered neurons increases, hindering the ability to isolate and activate a single target neuron effectively.

In Figure 1(c), the change in confidence and true versus false sign recovery is depicted when a hard neuron's euclidean distance to its neighbours is manipulated. The blue dots depict the confidence of the two originally hard to extract neurons. To manipulate the target neuron to be further away from other neurons, Gaussian noise was added and to manipulate the target neuron to be closer to a cluster of the closest neurons, part of the distance to these closest neurons was added to make the target neuron the midpoint between closest neurons. One can see that if the target neuron is closer to other neurons, with high confidence the wrong sign will be recovered, whereas if the distance to other neurons is further, with high confidence the correct sign will be recovered.

**The Effect of an Incorrect Sign in the Extraction Process:** Prior extraction performance evaluations have primarily utilised models trained on random data originally developed by Carlini et al.. However, observations indicate that the percentage of correctly extracted neurons is significantly higher for the CIFAR10 and MNIST datasets compared to random models, prompting us to test standard benchmarks.

Understanding the impact of a sign flip is key to determining the right balance between efficiency and correctness. For example, in a CIFAR10 model with 128 neurons and 8 hidden layers, keeping one hard-to-extract neuron sign flipped in layer 2 decreases the test accuracy to 0.9451 compared to the original model. With 5 sign flips, test accuracy falls between 0.55 and 0.68, and with 28 sign flips, it further declines to 0.268. These results illustrate that the effect of a sign flip is more significant than previously believed, suggesting that all prior methods of sign extraction may not have been adequate for consecutive layers' extractions, as none of them offered perfectly accurate sign extraction. The importance of accurate sign extraction is underscored by the failure of signature recovery in subsequent layers if even a single neuron's sign in a prior layer is incorrect.

### Method for Correct and Efficient Extraction

Following the discovery that the confidence level and the number of correctly recovered neurons stabilises after about \(s=15\) iterations, we propose that sign extraction can be made more efficient by running it with less iterations. Further, we find that 'hard' to sign extract neurons' signs cannot be extracted with the Neuron Wiggle method. Given that the sign of some neurons cannot be determined, a new robust method for determining them must be sought. Our approach is to test possible values for these hard to extract signs in the next layer. Correct signs can be detected as they will allow signature extraction to complete in the next layer without an error. Conversely, incorrect sign values will result in an error and the signature process can be stopped and restarted with a different combination of neuron signs. This process can be streamlined further in following way:

1. Hard to extract neurons are determined as those with low confidence (between 0.5 and 0.6 of true or false confidence since it is not known if true or false) after 15 iterations (\(s=15\)).
2. The next layer is extracted in parallel for each possible combinations of signs for these low-confidence neurons. If, for instance, there are five such neurons, \(2^{5}\) signature extraction processes are initiated. At latest in the sign extraction an error is thrown for an incorrect signature extraction through a check described below. Each process with an error is terminated. Ultimately, only one error-free process should remain, ensuring accurate layer extraction without significantly extending execution time.

**Note:** When extracting one target neuron's sign, errors in the'sample distance check' lead to having to rerun that iteration of the sign extraction with a new critical point. If an iteration has had to be rerun more than \(10-20\) times with high probability it can be deemed that the signature extraction must have been erroneous. Since each iteration in the sign recovery costs \(5\) queries this will equate to about \(100\) queries until we have a strong indication that the signature extraction for at least one neuron was wrong. If we wanted to, this could be used to check the correctness of each neuron'ssignature, by trying sign extraction for each in this way. If the sign extraction was wrong in the previous layer, all neuron signatures will be wrong. If the sign extraction in the previous layer was right but there was an error in the subsequent signature extraction, usually only one or two neuron signatures will be wrong. This check is also a good way to find these one or two erroneous signature extractions early on, so that the signature extraction can be rerun for these neurons before continuing with extraction in subsequent layers.

**The Sample Distance Check:** Specifically, the sample distance check fails continuously if the signature recovery is incorrect. The sign recovery compares the outputs \(f(^{*})\), \(f(^{*}+)\), and \(f(^{*}-)\) with the subtractions \(sL=f(^{*}-)-f(^{*})\) and \(sR=f(^{*}+)-f(^{*})\). If \(||sL-sR|| 10^{-13}\), then the process is aborted and restarts at a new critical point, as such a small difference suggests that the neuron's activation state may not have changed. This indicates a potential error in identifying a critical point. The threshold of \(10^{-13}\) is used due to the precision limitations of float64 being at about \(10^{-15}\) and correct wiggles typically impacting the output in the order of \(10^{-9}\). Consecutive failures suggest that there must be errors in the neuron's signature since the search for additional critical points in the sign extraction is guided by them.

### Sign Extraction as Bottleneck?

According to Carlini et al. , the most time-intensive aspect of parameter extraction is sign extraction. Canales-Martinez et al.  enhance efficiency with the Neuron Wiggle method, reducing the operation complexity in a single layer to \(O(sd_{i}d^{3})\), where \(s\) is the number of critical points needed for sign recovery, and \(d\) is the maximum of \(d_{0}\) and \(d_{i}\). Since Canales-Martinez et al.  only included the running time of their sign extraction in seconds without query numbers or a direct comparison to the signature extraction, from reading their paper, it appears as if the sign extraction is still the bottleneck in the parameter extraction. Running Carlini et al. 's and Canales-Martinez et al. 's codebase separately also underscores this discovery [ref. Figure 1(a) **CM** original]. However, comparing query numbers [ref. Figure 4 in Appendix C] and unifying the pipeline with Carlini's signature extraction to ensure comparability between signature and sign extraction time shows that contrary to prior results, sign extraction is not the bottleneck anymore [ref. Figure 1(a) **CM** unified]. In fact, with our adaptations it becomes the least time-significant part of parameter extraction. Please refer to Appendix C for more details on the implementation difference between **CM** original and **CM** unified.

### Further Improvements

1. **Improvements in Signature Extraction:** 1. The process of finding partial signatures was improved to find specific missing partial signatures which diversify the view and help to construct the full signature. 2. The memory and running time was improved by discarding all partial signatures that do not contribute a new view to the full signature (memory deduplication).
2. **Improvements in Precision Improvement:** (The precision improvement function improves precision from float32 to float64.) 1. The precision improvement function was adjusted to become usable for MNIST models. 2. The precision improvement function was identified as unnecessary to obtain signs. Sign extraction can be performed in float16 or float32 just as correctly. 3. The precision improvement function was identified as unnecessary for the next layer's signature and sign recovery if goal is the extraction of weights and biases to the precision of float32 instead of float64. Moreover, if float64 precision is needed this can run in parallel to extraction of the next layer.

Please refer to Appendix A for more details on these improvements.

## 4 Evaluation

### Scalability and Accuracy in Neuron Sign Prediction

We assess the accuracy of neuron sign predictions on standard benchmarks by examining MNIST and CIFAR models with various configurations of hidden layers. We find that the number of low confident and incorrectly identified neurons do not exceed \(10\). In this way, things should remain scalable, as parallelisation does not exceed more than \(2^{10}\) signature extractions. More details are in Appendix B.

### Performance

**Carlini vs. Canales-Martinez:** Table 1 illustrates the performance gains achieved by Canales-Martinez et al.'s (**CM**) extraction method and our extraction method compared to the previous version from Carlini et al. (**C**). The performance was tested on AMD Ryzen 7 4700U processor with 16GB RAM. **CM**'s sign extraction reduces the overall parameter extraction time in an exponential manner.

**Carlini + Canales-Martinez vs. Ours:** Moreover, our approach to parameter extraction has proven to be up to \(16.40\) times faster in the sign extraction and up to \(2.52\) times faster in the whole parameter extraction compared to Canales-Martinez et al. . Only some of the signature extractions show significant improvements. This is because the enhancements in the critical point search process and memory deduplication (discarding irrelevant critical points) only improves performance in some cases. However, if as suggested we were to bypass precision improvement in the signature extraction, since this can run in parallel while already starting sign extraction and the subsequent layer's extraction, then higher performance improvement is possible. For the extractions in Table 1 the precision improvement makes up approximately \(26\%,82\%,68\%,59\%,69\%,61\%,63\%,53\%,43\%\) and \(45\%\) of signature extraction time respectively, resulting in an overall extraction speedup of \([1.36,6.01,3.44,2.64,3.60,2.94,6.63,2.52,2.08,2.18]\) if precision improvement is disregarded.

Additionally, we have achieved performance gains in our sign extraction by setting the parameter \(s\) to \(15\), utilizing knowledge about easy and hard to extract neurons. As mentioned in Section 3.5, the sign extraction can be performed equivalently in float16, float32, or float64 without affecting correctness of sign extraction. While Table 1 shows results for float32 sign extraction, empirically the performance for float16 and float64 is almost identical. The precise sign extraction time can change up to a few seconds if the signature extraction precision is lower, causing more errors to be thrown, in which case float16 performs most stably.

**Efficiency Gains from Memory Deduplication:** Our analysis reveals significant improvements in signature extraction efficiency when implementing memory deduplication in the signature extraction, particularly in larger models. When examining the mean differences across four extraction seeds, we observed that for layer 2 of a random model with 128 neurons per hidden layer, the signature extraction process was \(1.3\) times more time-efficient, \(1.2\) times more query-efficient, and \(1.2\) times more memory-efficient with memory deduplication. For layer 2 of an MNIST model with 64 neurons it was on average \(2\) times as time efficient, \(1.2\) times as query efficient and \(1.3\) times as memory efficient. These findings underscore efficiency gains through memory deduplication, contributing significantly to reductions in memory usage and extraction time. A further graph on how the whole

    &  &  \\ 
**Model** & **Params** &  &  &  &  \\   & & **C+CM** & **Ours** & **C** & **CM** & **Ours** & **CM\(\) Ours** & **C\(\) CM** & **CM\(\) Ours** \\ 
10-5x2-1 & 30 & 18.08 & 18.65 & 76.39 & 0.82 & 0.05 & \( 16.40\) & \( 5.00\) & \( 1.01\) \\
20-10x2-1 & 110 & 13.38 & 13.17 & 86.38 & 1.59 & 0.12 & \( 13.25\) & \( 6.66\) & \( 1.13\) \\
30-15x2-1 & 240 & 22.81 & 22.39 & 141.24 & 2.37 & 0.16 & \( 14.81\) & \( 6.52\) & \( 1.12\) \\
40-20x2-1 & 420 & 27.59 & 27.96 & 193.52 & 3.46 & 0.28 & \( 12.36\) & \( 7.12\) & \( 1.10\) \\
50-25x2-1 & 650 & 29.34 & 29.64 & \( 1.30 1^{5}\) & 4.98 & 0.34 & \( 14.65\) & \( 3788.73\) & \( 1.15\) \\
60-30x2-1 & 930 & 41.79 & 40.80 & \( 5.4 10^{6}\) & 6.52 & 0.50 & \( 13.04\) & \( 1.1 10^{5}\) & \( 1.17\) \\
70-35x2-1 & 1260 & 107.70 & 46.15 & - & 10.58 & 0.77 & \( 13.74\) & - & \( 2.52\) \\
80-40x2-1 & 1640 & 67.01 & 65.93 & - & 13.46 & 0.94 & \( 14.32\) & - & \( 1.20\) \\
90-45x2-1 & 2070 & 96.28 & 94.37 & - & 18.61 & 1.41 & \( 13.20\) & - & \( 1.20\) \\
100-50x2-1 & 2550 & 206.65 & 186.53 & - & 20.47 & 1.82 & \( 11.25\) & - & \( 1.21\) \\   

Table 1: Extraction Performance Carlini (**C**), Canales-Martinez (**CM**) versus **Ours** on layer 2 of random models. Since extraction times vary significantly between layers in different models, we perform comparison of layer by layer extraction time and not whole model extraction time. We compare layer 2 because layer 1 and 3 are more straightforward to extract. A 10-5x2-1 model, following Carlini et al. , represents input layer of size 10, two hidden layers of size 5 and output layer of size 1. The numbers highlighted in red capture the gist of the performance improvement and the numbers in blue are our best performances.

parameter extraction scales for MNIST models with increasing layer sizes, and the calculation of the extraction time in the model used in the abstract can be found in Appendix C.

## 5 Discussion

### Running Time of different models

In Table 2 an overview of different model extractions are presented. All extractions in this table were run on a High Performance Cluster with Intel's 10th generation Intel Core processors icelake. In the following, three cases are presented which show the limitations of the signature extraction:

**Case "Random" vs. MNIST Models:** "Random" models were used for testing by Carlini et al. . These are not configured for a specific task but train 100 epochs on randomly generated data. The MNIST models' we additionally analysed have accuracies ranging from 0.67 to 0.94 for the two hidden layer models and is 0.91 for the 8 hidden layer model. In Table 2 one can see that extraction for these "random" models is much faster compared to extraction of MNIST models. A look into the kernel density estimate of the weights visualises that the concentration of weights near \(0\) is much higher for real world models trained on MNIST or CIFAR10. These models exhibit greater representation sparsity, meaning that a significant number of neurons are rarely activated. As a result, many neurons are grouped together in a compact region of the parameter space where activations are infrequent and close to zero, highlighting their minimal impact on the model's predictions. Consequently, locating specific coordinates for these seldom-activated neurons is challenging, as it is difficult to identify inputs that effectively trigger their activation.

    &  &  &  \\ 
**Model (Training Seed)** & **Layer** & **Params** & **Mean** & **Var** & **Mean** & **Var** & **Mean** & **Var** \\ 
784-8x2-1 (s1) & 2 & 72 & \(10.39\) & \(0.25\) & \(0.25\) & \(0.002\) & \(5.13 10^{4}\) & \(4.9 10^{8}\) \\
784-16x2-1 (s1) & 2 & 272 & \(7.22\) & \(8.85\) & \(0.60\) & \(0.005\) & \(6.92 10^{4}\) & \(9.3 10^{8}\) \\
784-32x2-1 (s1) & 2 & 1056 & \(22.58\) & \(31.59\) & \(2.07\) & \(0.61\) & \(2.28 10^{5}\) & \(3.7 10^{9}\) \\
784-64x2-1 (s1) & 2 & 4096 & \(135.32\) & \(2.9 10^{3}\) & \(7.17\) & \(6.32\) & \(9.03 10^{5}\) & \(1.9 10^{10}\) \\
784-128x2-1 (s2) & 2 & 16512 & \(758.5\) & \(1.5 10^{5}\) & \(30.46\) & \(8.02\) & \(4.17 10^{6}\) & \(1.1 10^{6}\) \\
784-128x2-1 (s2) & 2 & 16512 & \(1040.85\) & \(103.32\) & \(30.66\) & \(5.72\) & \(4.35 10^{6}\) & \(1.5 10^{6}\) \\ MNIST784-8x2-1 (s2) & 2 & 72 & \(12.75\) & \(9.17\) & \(0.26\) & \(0\) & \(49,730\) & \(9.6 10^{5}\) \\ MNIST784-16x2-1 (s2) & 2 & 272 & \(19.15\) & \(37.03\) & \(0.67\) & \(0.01\) & \(1.92 10^{5}\) & \(4.6 10^{9}\) \\ MNIST784-32x2-1 (s2) & 2 & 1056 & \(98.10\) & \(1179.81\) & \(2.00\) & \(0.07\) & \(7.7 10^{5}\) & \(8.0 10^{10}\) \\ MNIST784-64x2-1 (s2) & 2 & 4096 & \(496.2\) & \(1.5 10^{5}\) & \(6.32\) & \(0.32\) & \(3.05 10^{6}\) & \(1.1 10^{13}\) \\ MNIST784-64x2-1 (s1) & 2 & 4096 & \(4649.95\) & \(1.6 10^{6}\) & \(6.85\) & \(1.79\) & \(4.9 10^{6}\) & \(2.8 10^{13}\) \\ MNIST784-16x8-1 (s2) & 1 & 12560 & \(1 10^{4}\) & - & \(63.04\) & - & \(5.38 10^{6}\) & - \\ MNIST784-16x8-1 (s2) & 2 & 272 & \(470.19\) & \(3.4 10^{4}\) & \(0.67\) & \(0\) & \(5.27 10^{5}\) & \(1.2 10^{10}\) \\ MNIST784-16x8-1 (s2) & 4 & 272 & \(>36hrs\) & & & & & \\ MNIST784-16x8-1 (s2) & 8 & 272 & \(>36hrs\) & & & & & \\ MNIST784-16x8-1 (s2) & 9 & 17 & \(0.01\) & \(0\) & \(0\) & \(0\) & \(100\) & \(0\) \\ MNIST784-16x3-1 (s1) & 2 & 272 & \(1854.42\) & \(2 10^{6}\) & \(0.96\) & \(0.15\) & \(9.7 10^{6}\) & \(5.2 10^{13}\) \\ MNIST784-16x3-1 (s1) & 3 & 272 & \(6.9 10^{4}\) & - & \(0.54\) & - & \(4.4 10^{7}\) & - \\   

Table 2: Extraction Performance across models, training seeds and extractions seeds. The two different training seeds used are denoted as s1 and s2. The measurements were all taken over four extraction seeds. All signature extraction times are without the precision improvement function, since for MNIST models this takes up to \(33\) times longer than the actual signature extraction time and we have shown that this can be skipped or handled while already proceeding with further extraction processes. Extractions of deeper layers of MNIST784-16x8-1 did not lead to a full extraction after 36 hours with 6/16 and 0/16 extracted for layers 4 and 8. The most interesting contrasting results for discussion are highlighted pairwise in colours. In green one can see how layer 2 extraction for the same number of neurons can vary with model depth. In blue one can see the variance of extracting two models trained similarly but on different randomness. In red one can see how deeper layers become increasingly hard to extract.

**Case Deeper Models:** Deeper models are harder to extract. As can be seen in Table 2, comparing the extraction time of the second layer in a two hidden layer model and in an eight hidden layer model makes this apparent. Additionally, extraction of deeper layers is increasingly time consuming - in order to obtain the full signature of a neuron, a set of critical points that activates all previous layer's neurons must be found. Only then can the neuron be looked at from all perspectives, so that the full signature can be obtained. Carlini et al.  and Canales-Martinez et al.  mention increasing difficulty of extraction for deeper networks in the context of expansive networks. These networks feature inner layers with more neurons than the number of inputs they receive. If any hidden layer's expansion factor exceeds that of the smallest layer by too much, problems may arise.

**Case Random Seeds:** Although trained in the same way, the training seed seems to impact how well models can be extracted. Additionally, extraction seeds make a big difference in the efficiency of signature extraction. For example, up to \(8\) hours runtime difference were noted for the extraction of MNIST784-16x8-1 model's layer 2, when utilising different training and extraction seeds. Furthermore, a variance of just the extraction seed in a MNIST784-64x2-1 model's layer 2 made a difference of \(938\) seconds. Additional insight on variance can be seen in Table 2, where average model extraction time for models trained similarly but with different seeds can be as high as nine times and the query variance for different extraction seeds as high as \(10^{13}\). In some cases specific seeds trigger incorrect extraction of some neurons. For example, for the extraction of MNIST784-16x3-1 model's layer 2, extraction seeds 0, 10, 42, took over 1,000 seconds each and returned 4, 5, and 10 incorrectly extracted neurons, while for extraction seed 40 the extraction took only \(54\) seconds and was fully correct. Hence, for replicability underlying randomness should be considered.

## 6 Conclusion

Unifying previous methods into one codebase has facilitated a more thorough benchmarking of cryptanalytical parameter extraction techniques. Increased performance and robustness was achieved through new insights into neuron characteristics such as the identification of harder neurons whose sign extraction cannot be robustly performed with the Neuron Wiggle technique. Instead, we find a way to check for incorrect extractions in the next layer's extraction. Consequently we propose that the next layer's extraction can be performed in parallel for all combinations of these neurons' signs and then incorrect extractions can be filtered out through this check later in the pipeline. Contrary to previous assumptions, we found that signature extraction presents a more significant bottleneck than sign extraction, prompting a reevaluation of focus areas in parameter extraction. Furthermore, our evaluation revealed discrepancies in extraction times across models, with models trained on random data proving easier to extract than those on structured datasets like MNIST due to representation sparsity. Notably, models with fewer than four hidden layers exhibited quicker extraction times, sometimes within one to two hours, whereas deeper models faced increased extraction difficulties. Highighting the variability of extraction difficulty, we propose comprehensive benchmarking of model extraction methods considering factors such as the target model's training dataset, training method, layer size, depth, the specific layer targeted, along with the use of varying seeds to reflect the impact of randomness. To allow for comparison among implementations, the underlying ML framework, computer hardware, extraction time and query number are also important to note.