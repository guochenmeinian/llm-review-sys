# Appendix

Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution

Ying Wang

New York University

&Tim G. J. Rudner

New York University

&Andrew Gordon Wilson

New York University

Equal contribution.

###### Abstract

Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively.

## 1 Introduction

Vision-Language Pretrained Models (VLPMs) have become the de facto standard for solving a broad range of vision-language problems . They are pre-trained on large-scale multimodal data to learn complex associations between images and text and then fine-tuned on a given downstream task. For example, the widely used CLIP model , which uses image and text encoders trained on 400 million image-text pairs, has demonstrated remarkable performance when used for challenging vision-language tasks, such as Visual Question Answering and Visual Entailment .

VLPMs are highly overparameterized black-box models, enabling them to represent complex relationships in data. For example, Vision Transformers [ViTs; 7] are a state-of-the-art transformer-based vision model and are used as the image encoder of CLIP , containing 12 layers and 86 million parameters for ViT-Base and 24 layers and 307 million parameters for ViT-Large. Unfortunately, VLPMs like CLIP are difficult to interpret. However, model interpretability is essential in safety-critical real-world applications where VLPMs could be applied successfully, such as clinical decision-making or image captioning for the visually impaired. Improved interpretability and understanding of VLPMs would help us identify errors and unintended biases in VLP and improve the safety, reliability, and trustworthiness of VLPMs, thereby allowing us to deploy them in such safety-critical settings.

To tackle this lack of transparency in deep neural networks, attribution methods, which aim to explain a model's predictions by attributing contribution scores to each input feature, have been proposed for post-hoc interpretability. For example, for vision models, attribution methods can be used to create heatmaps that highlight features that are most responsible for a model's prediction. Similarly, for language models, scores are assigned to each input token. While the requirements for interpretability vary depending on the task, dataset, and model architecture, accurate and reliable attribution is an important tool for making emerging and state-of-the-art models more trustworthy.

While existing attribution methods focus primarily on unimodal models, we propose an attribution method for VLPMs that allows us to identify critical features in image and text inputs using the information bottleneck principle . Unlike standard unimodal attribution methods, the proposed multi-modal information bottleneck (M2IB) attribution method _does not require access to ground-truth labels_.

To do this, we formulate a simple multi-modal information bottleneck principle, use a variational approximation to derive a tractable optimization objective from this principle, and optimize this objective with respect to a set of attribution parameters. Using these parameters, we are able to "turn off" all irrelevant features and only keep important ones and generate attribution maps. In contrast to unimodal information bottleneck attribution , we aim to find attribution parameters that maximize the likelihood of observing features of one modality given features associated with the respective other modality. We perform a qualitative and quantitative empirical evaluation and find that M2IB is able to successfully identify important features relevant to _both_ image and text inputs. We provide an illustrative set of attribution map examples in Figure 1.

Our key contributions are summarized as follows:

1. We propose a multi-modal information bottleneck principle and use it to develop a multi-modal information bottleneck attribution method to improve the interpretability of Vision-Language Pretrained Models.
2. We perform an extensive empirical evaluation and demonstrate on several datasets--including healthcare data that can be used in safety-critical settings--that multi-modal information bottleneck attribution significantly outperforms existing gradient-based, perturbation-based, and attention-based attribution methods, quantitatively and qualitatively.

The code for our experiments is available at: https://github.com/YingWANGG/M2IB.

Figure 1: Example Attribution Maps For Image and Text Inputs. The red rectangles in the second and third rows show the ground-truth bounding boxes associated with the text, provided in the MS-CXR dataset . Multi-modal information bottleneck (M2IB) attribution maps successfully identify relevant objects in the given image-text pairs, while other methods provide less precise localization and neglect critical features in the inputs.

Related Work

Attribution methods enable post-hoc interpretability of trained models by allocating significance scores to the input features, such as image pixels or textual tokens.

### Attribution methods

Gradient-Based Attribution.The use of gradients as a foundational component in attribution methods has been widely explored in the interpretability literature. Simonyan et al.  directly use the gradient of the target with respect to inputs as the attribution score. Building on this, Grad-CAM [Gradient-weighted Class Activation Mapping; 25] weights convolutional layer activations by average pixel-wise gradients to identify important regions in image inputs. Integrated Gradients  introduces two essential axioms for attribution methods-- _sensitivity_ and _implementation invariance_, which motivates the application of integrated gradients along paths from a baseline input to the instance under analysis.

Perterbation-Based Attribution.Perturbation approaches involve modifying input features to observe the resulting changes in model predictions. Unlike gradient-based techniques, they eliminate the need for backpropagation, allowing the model to be treated as a complete black box. However, such methods can be computationally intensive, especially for intricate model architectures, given the need to re-evaluate predictions under many perturbation conditions. The LIME [Local Interpretable Model-agnostic Explanations; 22] algorithm leverages perturbation in a local surrogate model. Lundberg and Lee  propose SHAP (SHapley Additive exPlanations), which employs a game-theoretic approach to attribution by utilizing perturbations to compute Shapley values for each feature. Furthermore, they propose a kernel-based estimation approach for Shapley values, called KernelSHAP, drawing inspiration from local surrogate models.

Attention-based Attribution.The rise of transformers across multiple domains in machine learning has necessitated the development of specialized attribution methods tailored to these models. Using attention as an attribution method offers a straightforward approach to deciphering and illustrating the significance of various inputs in a transformer's decision-making process. Nevertheless, solely relying on attention is inadequate as it overlooks crucial information from the value matrices and other network layers . To address this issue, Chefer et al.  propose a method that learns relevancy maps through a forward pass across the attention layers with contributions from each layer cumulatively forming the aggregated relevance matrices.

Information-Theoretic Attribution.Schulz et al.  use information bottleneck attribution (IBA), where an information bottleneck is inserted into a layer of a trained neural network to distill the essential features for prediction. IBA is a model-agnostic method and shows impressive results on vision models including VGG-16  and ResNet-50 . Subsequently, IBA has been applied to language transformers  and was shown to also outperform other methods on this task. However, IBA has thus far been focused on only one modality and has only been adopted in supervised learning. To the best of our knowledge, there is no previous work on applying the information bottleneck principle to multi-modal models like VLPMs.

### Evaluation of Attribution Methods

Despite active research on attribution methods, we still lack standardized evaluation metrics for attribution due to the task's inherent complexity. For vision models, attribution maps are frequently juxtaposed with ground-truth bounding boxes for a form of zero-shot detection. However, the attribution of high scores to irrelevant areas might arise from either subpar attribution techniques or flawed models, making it challenging to isolate the root cause of such discrepancies.

To tackle this issue, previous studies have resorted to degradation-based metrics [33; 4]. The underlying principle is that eliminating features with high attribution scores should diminish performance, whereas the removal of low-attributed features, often seen as noise, should potentially enhance performance. Additionally, Hooker et al.  introduce ROAR (Remove and Retrain), a methodology that deliberately degrades training and validation datasets in alignment with the attribution map, thereby accounting for potential distribution shifts. Should the attribution be precise, retraining the model using these altered datasets would result in a pronounced performance decline. Rong et al.  argue that mere image masking can cause a leak of information via the mask's shape, and thus propose a Noisy Linear Imputation strategy that replaces pixels with the average of their neighbors.

It is important to note that a visually appealing saliency map does not guarantee the efficacy of the underlying attribution method. For example, an edge detector might generate a seemingly plausible saliency map, yet it does not qualify as an attribution method because it is independent of the model under analysis. Adebayo et al.  propose a sanity check designed to assess whether the outputs of attribution methods genuinely reflect the properties of the specific model under evaluation. Notably, several prominent models, including Guided Backprop  and its variants appear insensitive to model weights and consequently do not pass the sanity check.

## 3 Attribution via a Multi-Modal Information Bottleneck Principle

In this section, we introduce a simple, multi-modal variant of the information bottleneck principle and explain how to adapt it to feature attribution.

### The Information Bottleneck Principle

The information bottleneck principle  provides a framework for finding compressed representations of neural network models. To obtain latent representations that reflect the most relevant information of the input data, the information bottleneck principle seeks to find a stochastic latent representation \(Z\) of the input source \(X\) defined by a parametric encoder \(p_{Z|\,X}(z|\,x;)\) that is maximally informative about a target \(Y\) while constraining the mutual information between the latent representation \(Z\) and the input \(X\). For a representation parameterized by parameters \(\), this principle can be expressed as the optimization problem

\[_{}I(Z,Y;) I(Z,X;),\] (1)

where \(I(,;)\) is the mutual information function and \(\) is a compression constraint. We can equivalently express this optimization problem as maximizing the objective

\[()\!\!I(Z,Y;)- I(Z,X;),\] (2)

where \(\) is a Lagrange multiplier that trades off learning a latent representation that is maximally informative about the target \(Y\) with learning a representation that is maximally compressive about the input \(X\).

### A Multi-Modal Information Bottleneck Principle

Unfortunately, for VLPMs, the loss function above is not suitable since we wish to learn interpretable latent representations using only text and vision inputs without relying on task-specific targets \(Y\) that may not be available or are expensive to obtain. To formulate a multi-modal information bottleneck principle for VLPMs, we need to develop an optimization objective that is more akin to optimization objectives for self-supervised methods for image-text representation learning that only use (text, image) pairs [21; 18].

This learning problem fundamentally differs from supervised attribution map learning for unimodal tasks. For example, we may have an image of a bear, \(X_{}\), and a corresponding label, \(Y_{}=``"\). For a unimodal classification task, we can simply maximize \(I(Y_{};Z_{};)- I(X_{};Z_{ };)\) with respect to \(\), where \(Z_{}\) is the latent representation of \(X_{}\).

In contrast, in image-text representation learning, we typically have text descriptions, such as "This is a picture of a bear" (\(L^{}_{}\)) instead of labels . In this setting, both \(X_{}\) and \(L^{}_{}\) are "inputs" without a pre-defined corresponding label. To obtain a task-agnostic image-text representation independent from any task-specific ground-truth labels, we would like to use both input modalities and define a multi-modal information bottleneck principle and whereas the outputs are closely dependent on the specific downstream task. This requires defining an alternative to the "fitting term" \(I(Z,Y;)\) of the conventional information bottleneck objective.

Fortunately, there is a natural proxy for the relevance of information in multi-modal data. If image and text inputs are related (e.g., text that describes the image), a good image encoding should contain information about the text and vice versa. Based on this intuition, we can express a multi-modal information bottleneck objective for \(X_{m}\) with \(m=\{1,2\}\), as

\[_{m}(_{m})=I(Z_{m},E_{m^{}};_{m})- I(Z_{m}, X_{m};_{m}),\] (3)

where \(m^{}= m\) is the complement of \(m\), and \(E_{m^{}}\) is embedding of modality \(m^{}\). Next, we will show how to use this multi-modal variant of the information bottleneck principle for attribution.

### A Multi-Modal Information Bottleneck for Attribution

To compute attribution maps for image and text data without access to task-specific labels, we will define an information bottleneck attribution method for multi-modal data.

To restrict the information flow in the latent representation with a simple parametric encoder, we adapt the masking approach in Schulz et al. . For clarity and brevity, we henceforth represent the \(V_{m} W_{m}\)-dimensional latent representation \(Z_{m}\) in its vectorized form in \(^{J}\) with \(J V_{m} W_{m}\). Assuming independence across latent representation dimensions, we then define

\[p_{Z_{m}\,|\,X_{m}}(z_{m}\,|\,x_{m};_{m})=(z_{m};h_{m}(x_{m}; _{m}) f_{m}^{_{m}}(x_{m}),_{m}^{2}[(_{J}-h_{m}(x_{m};_{m}))^{2}]),\] (4)

where for a pair of modalities \(\{,\}\) with \(m\), \(_{m}\{_{m},_{m},_{m}\}\) are parameters, \(h_{m}(x_{m};_{m})^{J}\) is a mapping parameterized by \(_{m}\), \(f_{m}^{}(X_{m})^{J}\) is the vectorized output of the \(_{m}\)th layer of modality-specific neural network embedding \(f_{m}\), \(_{m}^{2}_{>0}\) is a hyperparameter, \([]\) represents an operator that transforms a vector into a diagonal matrix by placing the vector's elements along the main diagonal, \(_{J}^{J}\) is an all-ones vector, and \(\) is the Hadamard product. To avoid overloading notation, we will drop the subscript in probability density functions except when needed for clarity. Based on Equation (4), we can express the stochastic latent representations as

\[Z_{m}\,|\,x_{m};_{m}=h_{m}(x_{m};_{m}) f_{m}^{_{m}}(x_{m })+_{m}(_{J}-h_{m}(x_{m};_{m})),\] (5)

where \((0,I_{J})\). From this reparameterization, we can see that \([h_{m}(x_{m};_{m})]_{i}=1\) for \(i\{1,...,J\}\) means that no noise is added at index \(i\), so \([Z_{m}]_{i}\) will be the same as the original \(f_{m}^{}(x_{m})_{i}\), whereas \([h_{m}(x_{m};_{m})]_{i}=0\) means that \([Z_{m}]_{i}\) will be pure noise.

We can now express the multi-modal information bottleneck attribution (M2IB) objectives as

\[_{}(_{}) =I(Z_{},E_{};_{})- _{}\,I(Z_{},X_{};_{ })\] (6) \[_{}(_{}) =I(Z_{},E_{};_{})- _{}\,I(Z_{},X_{};_{}),\] (7)

which we can optimize with respect to the modality-specific sets of parameters \(_{}\) and \(_{}\), respectively. \(\{_{},_{},_{}\}\) and \(\{_{},_{},_{}\}\) are each sets of hyperparameters.

### A Variational Objective for Multi-Modal Information Bottleneck Attribution

To obtain tractable optimization objectives, we use a variational approximation. First, we note that \(I(Z_{m},X_{m};_{m})=_{p_{X_{m}}}[_{}(p_{Z _{m}\,|\,X_{m}}(\,|\,X_{m};_{m})\,\,p_{Z_{m}}(\,; _{m}))]\), where \(Z_{m}\,|\,X_{m};_{m}\) can be sampled empirically whereas \(p(z_{m};_{m})\) does not have an analytic expression because the integral \(p(z_{m};_{m})= p(z_{m}\,|\,x_{m};_{m})\,p(x_{m})\,x_{m}\) is intractable. To address this intractability, we approximate \(p(z_{m})\) by \(q(z_{m})(z_{m};0,1_{J})\). This approximation leads to the upper bound

\[I(Z_{m},X_{m};_{m}) =_{p_{X_{m}}}[_{}(p_{Z_{m}\,|\,X_{ m}}(\,|\,X_{m};_{m})\,\,q_{Z_{m}}())]-_{ }(p_{Z_{m}} q_{Z_{m}})\] \[_{p_{X_{m}}}[_{}(p_{Z_{m}\,| \,X_{m}}(\,|\,X_{m};_{m})\,\,q_{Z_{m}}())]\] (8) \[_{}^{}(_{m}).\]

Next, while the unimodal information bottleneck attribution objective uses ground-truth labels to compute the "fit term" in the objective, the multi-modal information bottleneck attribution objectives require computing the mutual information between the aligned stochastic embeddings,

\[I(Z_{m},E_{m^{}};_{m}) = p(e_{m^{}},z_{m};_{m})},z_{m};_{m})}{p(e_{m^{}})p(z_{m})}\,e_{m^{}}\, z_{m}\] (9) \[= p(e_{m^{}},z_{m};_{m})} \,|\,z_{m})}{p(e_{m^{}})}\,e_{m^{}}\,z_{m},\] (10)

which is not in general tractable. To obtain an analytically tractable variational objective, we approximate the intractable \(p(e_{m^{}}\,|\,z_{m})\) by a variational distribution \(q(e_{m^{}}\,|\,z_{m})(e_{m^{}};g_{m}(z_{m}),1_{K})\), where \(g_{m}\) is a mapping that aligns the latent representation of modality \(m\) with \(E_{m^{}}\) and \(K\) is the dimension of the embedding, and get

\[I(Z_{m},E_{m^{}};_{m})  p(e_{m^{}},z_{m};_{m}) q(e_{m^{}} \,|\,z_{m})\,e_{m^{}}\,z_{m}\] (11) \[= p(x_{m})p(e_{m^{}}\,|\,x_{m})p(z_{m}\,|\,x_{m};_ {m}) q(e_{m^{}}\,|\,z_{m})\,x_{m}\,e_{m^{}}\, z_{m}\] \[_{m}^{}(_{m}).\]With this approximation, we can obtain a tractable variational optimization objective by sampling \(X_{m}\) and \(E_{m^{}}\) from the empirical distribution

\[(x_{m},e_{m^{}})_{n=1}^{N}_{0}(x_{m}-x_ {m}^{(n)})\,_{0}(e_{m^{}}-f_{m^{}}(x_{m^{}}^{(n)})),\] (12)

where \(f_{m^{}}(x_{m^{}})\) is the embedding input \(x_{m^{}}\) under the VLPM. With these approximations, we can now state the full variational objective, which is given by

\[_{m}^{}(_{m})_{m}^{}(_{m})-_{m}_{m}^{}(_{m}).\] (13)

The derivation of this objective has closely followed the steps in . In practice, the objective can be computed using the empirical data distribution so that

\[_{m}^{}(_{m}) _{n=1}^{N} p (z_{m}\,|\,x_{m}^{(n)};_{m}) q(e_{m^{}}\,|\,z_{m})\, z_{m}\] (14) \[-_{m}_{}(p_{Z_{m}\,|\,X_{m}}(\,\,;x_ {m}^{(n)};_{m}) q_{Z_{m}}()).\]

Final Variational Optimization Objective.Finally, we assume that \(h_{m}(x_{m}^{(n)};_{m})_{m}^{(n)}\) (i.e., each input point has its own set of attribution parameters), that \(g_{m}\) is the mapping defined by the post-bottleneck layers of a VLPM for modality \(m\), and that for each evaluation point the final embeddings _for each modality_ get normalized across the embedding dimensions (i.e., both mapping, \(g_{m}\) and \(f_{m^{}}\), contain embedding normalization transformations). For normalized \(g_{m}(z_{m})\) and \(e_{m^{}}\), the log of the Gaussian probability density \(q(f_{m^{}}(x_{m^{}})\,|\,g_{m}(z_{m}))\) simplifies and is proportional to the cosine similarity between \(f_{m^{}}(x_{m^{}})\) and \(g_{m}(z_{m})\), giving the final optimization objective

\[}_{m}^{}(_{m}) _{n=1}^{N} p (z_{m}\,|\,x_{m}^{(n)};_{m})S_{}(e_{m^{ }},g_{m}(z_{m}))\,z_{m}\] (15) \[-_{m}_{}(p_{Z_{m}\,|\,X_{m}}(\,\,;x_ {m}^{(n)};_{m}) q_{Z_{m}}()),\]

where \(S_{}(,)\) is the cosine similarity function. For gradient estimation during optimization, the remaining integrals can be estimated using simple Monte Carlo estimation and reparameterization gradients. For \(_{m}=\{_{m},_{m},_{m}\}\), the objective function is maximized with respect to \(_{m}\)_independently for each modality_, and \(_{m}\), \(_{m}\), and \(_{m}\) are modality-specifc hyperparameters.

## 4 Empirical Evaluation

We evaluate the proposed attribution method using CLIP  on four image-caption datasets, including widely-used image captioning datasets and medical datasets. Our main datasets are **(i)** Conceptual Captions  consisting of diverse images and captions from the web, and **(ii)** MS-CXR (Local Alignment Chest X-ray dataset; ), which contains chest X-rays and texts describing radiological findings, complementing MIMIC-CXR (MIMIC Chest X-ray; ) by improving the bounding boxes and captions. In addition, we also include some qualitative examples from the following radiology and remote sensory datasets to show the potential application of our model in safety-critical domains. Namely, we have **(iii)** ROCO (Radiology Objects in COntext; Pelka et al. ) that includes radiology image-caption pairs from the open-access biomedical literature database PubMed Central, and **(iv)** RSICD (Remote Sensing Image Captioning Dataset; Lu et al. ), which collects remote sensing images from web map services including Google Earth and provides corresponding captions.

### Experiment Setup

For all experiments, we use a pretrained CLIP model with ViT-B/32  as the image encoder and a 12-layer self-attention transformer as the text encoder. For Conceptual Captions, we use the pretrained weights of openai/clip-vit-base-patch32.2 For MS-CXR, we use CXR-RePaiR  which is CLIP finetuned on radiology datasets, and compare the impact of finetuning in Section 4.5. For each {image, caption} pair, we insert an information bottleneck into the given layer of the text encoder and image encoder of CLIP separately, then train the bottleneck using the same setup as the _Per-Sample Bottleneck_ of original IBA , which duplicates a single sample for 10 times to stabilize training and runs 10 iterations using the Adam optimizer with a learning rate of 1. Experiments show no significant difference between different learning rates and more training steps. We conduct a hyper-parameter tuning on the index of the layer \(l\), the scaling factor \(\), and the variance \(^{2}\). For a discussion of these hyperparameters in the multi-modal information bottleneck objective, see Appendix A.

### Qualitative Results

We qualitatively compare our method with 5 widely used attribution methods, including gradient-based GradCAM  and Saliency method , perturbation-based Kernel SHAP  and RISE , and transformer-specific method . We show one image-caption example and two radiology examples in Figure 1 and provide more qualitative comparisons in the Appendix B. As shown, our method is able to capture all relevant objects appearing in both modalities, while other methods tend to focus on one major object. As illustrated in Figure 1(a), the highlighted areas in the image change according to different inputs, and our method can capture complicated relationships between image and text when involving multiple objects. Our method is also able to detect multiple occurrences of the same object in the image, as illustrated in Figure 1(b).

### Localization Test

We quantitatively measure the effectiveness of our proposed attribution method by evaluating its accuracy in zero-shot detection for images. We binarize the saliency map such that the areas with scores higher than the threshold (75%) are assigned 1 while the rest are assigned 0. We denote the resulting binary map as \(S_{}\). We also construct a ground-truth binary map, \(S_{}\), using the bounding boxes provided by MS-CXR , where the region inside the bounding boxes is assigned to 1 while the outside is assigned to 0. Note that some samples have multiple bounding boxes, and we consider all of them to test the method's multi-occurrence detection ability. Then, we calculate the IoU (Intersection over Union) of \(S_{}\) and \(S_{}\). Namely, for images with a height of \(n\) and a width of \(m\), the score is calculated by

\[=^{n}_{j=1}^{m}_{S_{ }^{ij} S_{}^{ij}}}{_{i=1}^{n}_{j=1}^{m} _{S_{}^{ij} S_{}^{ij}}},\] (16)

where \(\) is the indicator function, \(\) is the logical AND and \(\) is the logical OR operator.

We found that M2IB attribution attains an average IoU of 22.59% for this zero-shot detection task, outperforming all baseline models, as shown in Table 1. Recognizing that a localization score of 22.59% appears somewhat low in absolute terms, we briefly note that there are two potential causes that could lead to the low absolute values in the localization scores: **(i)** M2IB attribution indeed generates segmentation instead of bounding boxes, so evaluation by bounding boxes would underestimate the quality of the saliency map. **(ii)** The model under evaluation  is not finetuned for detection and may only have learned a coarse-grained relationship between X-rays and medical notes.

### Degradation Tests

While the localization test shows that M2IB attribution may be a promising zero-shot detection and segmentation tool, the localization test may underestimate the accuracy of attribution since even a perfect attribution map can produce a low localization score if the (finetuned) VLPM under evaluation is poor at extracting useful information--which is very likely for challenging specialized tasks like chest X-ray classification.

To get a more fine-grained picture of the usefulness of M2IB, we use three additional evaluation metrics to compare M2IB to competitive baselines. The underlying idea of all three evaluations is that removing features with high attribution scores should decrease the performance, while discarding features with low attribution scores can improve the performance as noisy information is ignored. We randomly sample 2,000 image-text pairs from Conceptual Captions and 500 image-text pairs from MS-CXR. For each dataset, we perform ten experiments for each evaluation metric (five for ROAR+) and report the average score with the standard error in Table 1.

Figure 2: Example Saliency Maps when Involving Multiple Objects. Our method can successfully detect all occurrences of all relevant objects in image and text.

**Drop in Confidence**. An ideal attribution method should only assign high scores to important features, thus we should not observe a drop in performance if only the high-attribution parts are allowed in the input. For images, we use point-wise multiplication of the saliency map and the image input. Since scaling token ids is meaningless, we use binarization similar to  where only tokens with attribution scores in the top 50% are kept. We provide an example of distilled image and text in Figure 3. Formally, we define this score by

\[=_{i=1}^{N}(0,o_{i}-s_{i}),\] (17)

where \(o_{i}\) is the cosine similarity of features of original images and texts, and \(s_{i}\) is the new cosine similarity when one modality is distilled according to the attribution. The lower this metric is, the better the attribution method is. This metric is implemented in the pytorch-gradcam repository.3

**Increase in Confidence**. Similarly, removing noisy information in the input might increase the model's confidence. We compute

\[=_{i=1}^{N}(o_{i}<s_{i}),\] (18)

where \(\) is the indicator function and the definition of \(o_{i}\) and \(s_{i}\) is the same as above. Higher values indicate better performance. This metric is also implemented in the pytorch-gradcam repository.3

    & Methods & **GradCAM** & **Saliency Map** & **KS** & **RISE** & **Chefer et al.** & **Ours** \\  CC & \% Conf. Drop \(\) & 4.96 \(\) 0.01 & 1.99 \(\) 0.01 & 1.94 \(\) 0.01 & 1.12 \(\) 0.01 & 1.63 \(\) 0.01 & **1.11**\(\) 0.01 \\ image & \% Conf. Ind. \(\) & 17.84 \(\) 0.08 & 2.95 \(\) 0.12 & 25.18 \(\) 0.28 & 35.72 \(\) 0.14 & 37.41 \(\) 0.12 & **41.55**\(\) 0.19 \\  & \% ROAR+ \(\) & 2.29 \(\) 0.41 & 6.88 \(\) 0.88 & 1.56 \(\) 0.88 & 3.15 \(\) 0.97 & 7.66 \(\) 0.55 & **10.59**\(\) 0.85 \\  CC & \% Conf. Drop \(\) & 2.19 \(\) 0.01 & 1.78 \(\) 0.01 & 1.71 \(\) 0.01 & 1.30 \(\) 0.01 & **1.06**\(\) 0.01 & **1.06**\(\) 0.01 \\ text & \% Conf. Ind. \(\) & 29.71 \(\) 0.19 & 3.89 \(\) 0.15 & **46.87**\(\) 0.21 & 38.31 \(\) 0.14 & 38.24 \(\) 0.11 & 38.55 \(\) 0.20 \\  & \% ROAR+ \(\) & 43.23 \(\) 0.66 & 43.74 \(\) 0.65 & 47.46 \(\) 3.62 & 49.04 \(\) 1.12 & 53.57 \(\) 1.26 & **60.41**\(\) 1.12 \\   & \% Conf. Drop \(\) & 2.76 \(\) 0.03 & 0.81 \(\) 0.01 & 2.37 \(\) 0.04 & 3.94 \(\) 0.03 & 1.87 \(\) 0.02 & **0.55**\(\) 0.01 \\ MSCXR & \% Conf. Ind. \(\) & 12.64 \(\) 0.46 & 35.08 \(\) 0.44 & 10.24 \(\) 0.68 & 7.28 \(\) 0.44 & 21.44 \(\) 0.46 & **45.92**\(\) 0.70 \\ image & \% ROAR+ \(\) & 3.54 \(\) 0.80 & 25.46 \(\) 1.35 & 12.67 \(\) 1.02 & 16.79 \(\) 0.76 & 24.24 \(\) 1.19 & **38.7**\(\) 0.86 \\  & \% Localization \(\) & 5 & 1.03 \(\) 0.13 & 2.16 \(\) 0.16 & 7.77 \(\) 0.13 & 10.97 \(\) 0.24 & 21.65 \(\) 0.25 & **22.59**\(\) 0.14 \\  MSCXR & \% Conf. Drop \(\) & 2.26 \(\) 0.04 & 3.35 \(\) 0.03 & 2.4 \(\) 0.05 & **1.16**\(\) 0.02 & 2.93 \(\) 0.03 & 2.28 \(\) 0.04 \\ text & \% Conf. Ind. \(\) & 36.24 \(\) 0.54 & 18.88 \(\) 0.54 & 34.12 \(\) 0.77 & **57.2**\(\) **0.65** & 28.08 \(\) 0.34 & 35.48 \(\) 0.69 \\  & \% ROAR+ \(\) & 11.07 \(\) 0.62 & 15.79 \(\) 0.92 & 14.28 \(\) 1.09 & 12.09 \(\) 1.52 & 9.11 \(\) 0.6 & **16.31**\(\) 0.75 \\   

Table 1: Quantitative Results. The boldface denotes the best result per row. Means and standard errors were computed over ten random seeds.

Figure 3: Visualization of Degradation. The third column is obtained by calculating the element-wise product of the original image and saliency map, while the text with attribution scores lower than 50% percentile is masked by a blank token <B>. It is used in the _Increase in Confidence_ metric and _Drop in Confidence_ metric. The fourth column shows an example of the training data in ROAR+. We replace the image pixels with attribution scores higher than 75% percentile by the channel mean and replace the text tokens with attribution scores higher than 50% by a blank token <B>. The results in Table 1 use the padding token as the blank token <B>.

Remove and Retrain + (ROAR+, an Extension of ROAR ). We finetune the base model on the degraded images and texts where the most important parts are replaced by uninformative values (i.e., channel means of images or padding tokens for texts, see Figure 3) and evaluated on a validation set of original inputs.4 If the attribution method is accurate, a sharp decrease in performance is expected because all useful features are removed, and the model cannot learn anything relevant from the degraded data. We split the testing dataset into 80% training data and 20% validation data. We use the same contrastive loss as used for pretraining CLIP and define the score by \((l_{c}-l_{o})/l_{o}\), where \(l_{o}\) is the validation loss of retraining using the original data, and \(l_{c}\) is the validation loss when retraining with corrupted data. We repeat the process five times and report the average score.

The results are summarized in Table 1. M2IB attribution outperforms baseline models in almost all numerical metrics, except for perturbation-based methods, which achieve better Increase/Drop in Confidence scores for texts. Perturbation-based methods perform better for short text because they can scan all possible binary masks on the text and then find the optimal one with the highest confidence score. However, this kind of method is very computationally expensive. We use 2,048 masks for image and 256 masks for text in RISE, where each mask of each modality requires one forward pass to get the confidence score, resulting in 2.3k forward passes (7.8s on RTX8000 with a batch size of 256). In contrast, M2IB attribution only requires 100 forward passes and takes 1.2s for one image-text pair.

In general, removing pixels or tokens with lower attribution scores using M2IB attribution generally increases the mutual information with the other modality, while masking by our attribution map generally decreases the relevance with the other modality and makes the model perform worse when retraining on the corrupted data. This confirms that our method generates useful attribution maps.

### Sanity Check

We conduct a sanity check to ensure our method is, in fact, sensitive to model parameters. We follow the sanity check procedure proposed by Adebayo et al. , where parameters in the model are randomized starting from the last to the first layer. As shown in Figure 4, M2IB passes the sanity check as the attribution scores of image pixels and text tokens change as the model weights change. Our method also produces more accurate saliency maps for finetuned models compared to pretrained models, which further confirms that the resulting attribution can successfully reflect the quality of the model. Since we insert the information bottleneck after a selected layer (layer 9 in this case), the randomization of this and previous layers appears to have a larger influence on the output.

### Error Analysis and Limitations

We notice that our proposed attribution method generally performs well on text but sometimes shows less satisfying performance on images. By inspecting the qualitative examples, we observe that M2IB sometimes fails to detect the entire relevant regions in images. As shown in the fourth ("sea" example) and fifth ("bridge" example) rows in Figure 7, our method only highlights a fraction of the object in the image, although it should include the whole object. This is probably due to the fact that the model under evaluation only relies on a few patterns in the image to make its prediction. Increasing the relative importance of the fitting term (i.e., using smaller \(\)) enlarges the highlighted area. However, we don't recommend using extreme \(\) because it will break the balance between fitting and compression and thus make the information bottleneck unable to squeeze information.

We also note that M2IB is sensitive to the choice of hyperparameters. As shown in Figure 5, different combinations of hyperparameters will generate different saliency maps. We show how to use the ROAR+ score to systematically select the optimal hyperparameters and also provide visualization to illustrate the effect of different hyperparameters in Appendix A. Since there is no convention on evaluating the attribution method, we suggest considering various evaluation metrics, visualization of examples, and the goal of the attribution task when choosing hyperparameters. We emphasize that M2IB should be used with caution since attributing the success or failure of a model solely to a set of features can be overly simplistic, and different attribution methods might lead to different results.

## 5 Discussion

We developed multi-modal information bottleneck (M2IB) attribution, an information-theoretic approach to multi-modal attribution mapping. We used CLIP-ViT-B/32 in our experiments, but M2IB attribution can be directly applied to CLIP with alternative neural network architectures and is compatible with any VLPM for which the features of all modalities are projected into a shared embedding space--a commonly used approach in state-of-the-art multi-modal models.

Going beyond vision and language modalities, Girdhar et al.  recently proposed a new multi-modal model, ImageBind, which aligns embeddings of five modalities to image embeddings through contrastive learning on pairs of images with each modality and uses a similar architecture as CLIP, where the encoder for each modality is a transformer. We applied M2IB attribution to pairs of six different modalities using ImageBind--which required minimal implementation steps--and provided qualitative examples in Appendix C to illustrate that M2IB attribution can be applied successfully to more than just vision and text modalities.

In this paper, we provided exhaustive empirical evidence that M2IB increases attribution accuracy and improves the interpretability of state-of-the-art vision-language pretrained models. We hope that this work will encourage future research into multi-modal information-theoretic attribution methods that can help improve the interpretability and trustworthiness of VLPs and allow them to be applied to safety-critical domains where interpretability is essential.