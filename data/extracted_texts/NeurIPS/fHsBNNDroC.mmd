# Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents

Nika Haghtalab

University of California, Berkeley, {nika,kunheyang}@berkeley.edu

Chara Podimata

MIT & Archimedes, podimata@mit.edu

Kunhe Yang

University of California, Berkeley, {nika,kunheyang}@berkeley.edu

###### Abstract

In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: _Calibrated Stackelberg Games (CSGs)_. In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best-responds to _calibrated forecasts_ about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed _adaptive calibration_, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum Stackelberg value of the game both in _finite_ and _continuous_ settings, and that no higher utility is achievable. Two prominent and immediate applications of our results are the settings of learning in Stackelberg Security Games and strategic classification, both against _calibrated_ agents.

## 1 Introduction

Stackelberg games (SGs) are a canonical model for strategic principal-agent interactions, considering a principal (or "leader") that commits to a strategy \(\) and an agent (or "follower") who observes this strategy and best respond by taking action \(()\). These games are inspired by real-world applications such as economic policy design (where a tax policymaker establishes rules for triggering audits before taxes are filed), defense (where a principal allocates security resources to high-risk targets before vulnerabilities are exploited) and many more, see e.g., . By anticipating the agent's best-response, a principal who knows the agent's payoff function can calculate the _optimal Stackelberg strategy_ guaranteeing her utility \(V^{*}\). In recent years, _repeated_ SGs have gained popularity in addressing settings where the agent's payoff function is _unknown_ to the principal. In this setting, the principal, who can only observe the agents' actions, aims to deploy a sequence of strategies \(_{1},,_{T}\) over \(T\) rounds whose average payoff is at least as good as \(V^{*}\), i.e., the value of her optimal strategy had she known the agent's payoffs in advance.

Despite the original intent, repeated SGs are often studied under strict assumptions on the agent's knowledge and algorithmic behavior. Examples include requiring the agent to best respond per round using \(y_{t}=(_{t})\), necessitating the agent to precisely know the principal's strategy at all times (e.g., the attacker must anticipate the exact probabilistic allocation of the defender's security resources), or employing one of many online optimization algorithms whose every detail (down to the learning step size) can significantly impact the principal's utility .

In this paper, instead of working with such restrictive and often unrealistic assumptions on the agent's knowledge and behavior, we build on foundational decision theoretic concepts, such as _forecasts_ and _calibration_. In practice, while agents may not observe the principal's true strategies \(_{t}\), they can form _calibrated forecasts_ -- a notion of consistency in beliefs about \(_{t}\) -- to which they then best respond. Indeed, such a decision-theoretic perspective on game dynamics led to seminal results on converging to correlated and Nash equilibria in simultaneous multi-player games . Our work brings the perspective of calibrated forecasts to principal-agent games. We introduce _Calibrated Stackelberg Games (CSG)_--a class that is more general than standard SGs-- and ask:

**Q1**.: _What characterizes principal's optimal utility in CSGs?_

**Q2**.: _Are there natural forecasting algorithms for the agent that satisfy calibration?_

**Our Contributions.** We answer both questions completely. For **Q1**, we show that the principal's optimal utility converges _exactly_ to \(V^{}\). For **Q2**, we give a general approach for obtaining a fine-grained any-time notion of calibration _of independent interest_ and further specializing it to games.

Before we delve into the details of our contributions, we highlight two key aspects of our results. First, _calibration_ is a common property of forecasting procedures shared by many algorithms, not any one particular algorithm defining the agent's behavior. Despite not constraining the agent to any particular algorithm, our answer to **Q1** shows that the principal can meaningfully converge to \(V^{}\), which is the value she could have achieved in a single-shot game had she known the agent's utility. Second, our definition and results immediately apply to two important Stackelberg settings; Stackelberg Security Games  and strategic classification . As such, we obtain the first results for learning against calibrated agents in these settings too.

Our work contributes two concepts of independent interest (Section 2): first, _CSGs_ that directly generalize the standard model of repeated SGs, and second, a notion of calibration, termed _adaptive calibration_. This notion which draws inspiration from _adaptive regret bounds_ in online learning, provides fine-grained calibration guarantees for adversarial sequences.

Beyond the introduction of these models, we address an important property of CSGs in our answer to **Q1**. We show that the principal's optimal utility in CSGs converges to \(V^{}\), nothing more or less, in games with _finite_ (Section 3) or _continuous_ (Section 5) actions spaces. Note that \(V^{}\) is a benchmark that gives both players more power: the principal knows the agent's utility and the agent observes the principal's strategy. We find it somewhat surprising then that the optimal achievable principal utility in CSGs, in which both players work with significantly less knowledge, converges to \(V^{}\) exactly.

As for our newly introduced notion of adaptive calibration (Section 4), we provide an answer to **Q2** by giving a general approach for creating adaptively calibrated forecasting algorithms. This shows that adaptive calibration is not just an abstract notion, rather, it is a natural property with deep roots in the theory of online learning. Indeed, to obtain these results we draw inspirations from recent advances in the multicalibration literature  regarding simulating no-regret and best-response dynamics and the sleeping experts problem setting which has been a staple of the online learning literature . Furthermore, we specialize our approach for strategic settings by showing how standard calibration concepts (such as the "binning function") can be adapted to account for the agent's best-responses.

### Related work

**Repeated Stackelberg games.** Learning optimal Stackelberg strategies has been studied in the offline  and the online setting, where only instantaneous best-responses are observable (i.e., no access to a best-response oracle). Key applications include Stackelberg Security Games (e.g., ) and strategic classification (e.g., ). Another line of work treats repeated games as an extensive form game and studies optimal strategies for infinite  or finite  horizons. Other works consider learning in the presence of non-myopic agents that best respond by maximizing discounted utilities . The main distinction to our work is that in our setting, the agents have only calibrated forecasts regarding the principal's strategies (rather than full knowledge of them).

**Stackelberg games beyond best responses.** Recent works have studied variants of repeated Stackelberg games with different agent strategic behaviors beyond best responding. One example is no-regret learning, in which previous works have extensively investigated the relationship between the principal's cumulative utility and the single-shot Stackelberg value against agents that use mean-based learning algorithms , gradient descent , no-external regret algorithms ,no-internal/swap regret algorithms [20; 45], and no-counterfactual internal regret algorithms . Another research direction assumes agents approximately best respond due to uncertainty in the principal's strategy [11; 6; 46] or their own [43; 39; 40] and study _robust Stackelberg equilibria_[48; 32]. Most of the works here assume that the principal knows the agent's utility function with the exception of . Core differences to our framework are that (1) we work in an online learning setting where the principal has to learn the agent's utility function from their responses; (2) we do not assume a specific agent algorithm but focus on properties of agent beliefs that are shared by many algorithms.

**Calibration and application in games.** The study of calibration, introduced by Dawid , dates back to seminal work by Foster and Vohra  and Hart  that showed the existence of asymptotic online calibration against any adversarial sequence. Applying calibration to game dynamics, Foster and Vohra  introduced the concept of _calibrated learning_, which refers to a player best responding to calibrated forecasts of others' actions. They demonstrated that the game dynamics of all players performing calibrated learning converge to the set of correlated equilibria. This is complemented by the results of [38; 27; 28] that show _smooth and continuous_ variants of calibrated learning dynamics converge to Nash equilibrium. Our work differs from the above by studying game dynamics that converge to a Stackelberg equilibrium, where only the follower (agent) performs calibrated learning.

**Adaptivity and sleeping experts.** The notion of adaptive calibration introduced in Section 2 is related to the study of adaptivity of regret bounds in online learning [44; 18; 37]. Our design of adaptively calibrated forecasting algorithms builds on the _multi-objective learning_ perspective of online (multi-)calibration [41; 34] and the powerful tool of _sleeping experts_[8; 31; 44] which has proven useful in various applications such as fairness .

## 2 Model & preliminaries

We begin this section with some basic definitions about forecasts, calibration, and games, and then introduce the class of games that we study; _Calibrated Stackelberg Games_ (CSGs).

**Adaptively Calibrated Forecasts.** We use \(A\) to denote the space of outcomes and \(C A\) to denote the space of forecasts. A (stochastic) forecasting procedure \(\) is an online procedure that takes any adversarial sequence of outcomes \(_{t} A\) for \(t[T]\), and on round \(t\) outputs (possibly at random) forecast \(_{t}\) solely based on outcomes and forecasts \(_{},_{}\), for \([t-1]\). To define calibrated forecasts, let us first introduce the notion of _binning functions_.

**Definition 2.1** (Binning ).: _We call a set \(=\{w_{i}\}_{i[n]}\) a binning function, if each \(w_{i}:C\) maps forecasts to real values in \(\), and for all \( C\) we have \(_{i[n]}w_{i}()=1\)._

With the above binning functions, we define the adaptive calibration error with respect to \(\) as follows. At a high level, conditioned on any bin, the calibration error measures the difference between the expected forecasts that fall in that bin and the corresponding expected outcome.

**Definition 2.2** (\(\)-Adaptive Calibration Error).: _For any time interval \([s,t]\), let \(_{s:t}\) be the sequence of forecasts and \(_{s:t}\) be the sequence of outcomes. For a given binning \(=\{w_{i}\}_{i[n]}\) with size \(n\), and \( i[n]\), define the \(\)-adaptive calibration error as_

\[_{i}(_{s:t},_{s:t}) (i)}{t-s}\|}_{[s,t]}(i)-}_{[s,t]}(i)\|_{},\] (1)

_where during interval \([s,t]\), \(n_{[s,t]}(i)_{=s}^{t}w_{i}(_{})\) is the effective number of times that the forecast belongs to bin \(i\) (i.e., bin \(i\) is activated), \(}_{[s,t]}(i)_{=s}^{t}( _{})}{n_{[s,t]}(i)}_{}\) is the expected forecast that activates bin \(i\), \(}_{[s,t]}(i)_{=s}^{t}( _{})}{n_{[s,t]}(i)}_{}\) is the expected outcomes corresponding to bin \(i\)._

We say that a forecasting procedure is adaptively calibrated if it achieves vanishing calibration error on any adversarial sequence of outcomes and any sub-interval of time steps.

**Definition 2.3** (\((,)\)-Adaptively Calibrated Forecasts).: _A forecasting procedure \(\) is \(\)-adaptively calibrated to binning \(=\{w_{i}\}_{i[n]}\) with rate \(r_{}() o(1)\), if for all adversarial sequences of actions \(_{1},,_{T}\), where \(_{t} A\), \(\) outputs forecasts \(_{t} C\) for \(t[T]\) such that with probability at least \(1-,\) we have that \( s,t\) such that \(1 s<t T\), and \( i[n]\):_

\[_{i}(_{s:t},_{s:t}) r_{ }(t-s)+.\]We remark that without adaptivity (i.e., for \(s=1\) and \(t=T\)), Definition 2.2 is weaker than the standard definition of calibration (e.g., , listed for completeness in Appendix A) in two ways: (1) standard calibration takes each prediction \( C\) as an independent bin, thus having infinitely many binning functions: \(w_{}()=_{}()\). Instead, we only require calibration with respect to the predefined binning \(\) which only contains a finite number of binning functions; (2) standard calibration cares about the summation over calibration error across bins, but we only consider the maximum error.

**Stackelberg Games.** A _Stackelberg game_ is defined as the tuple \((_{P},_{A},U_{P},U_{A})\), where \(_{P}\) and \(_{A}\) are the principal and the agent action spaces respectively, and \(U_{P}:_{P}_{A}_{+}\) and \(U_{A}:_{P}_{A}_{+}\) are the principal and the agent utility functions respectively. For ease of exposition, we work with _finite_ Stackelberg games (i.e., \(|_{P}|=m\) and \(|_{A}|=k\)) and generalize our results to continuous games in Section 5. When the principal plays action \(x_{P}\) and the agent plays action \(y_{A}\), then the principal and the agent receive utilities \(U_{P}(x,y)\) and \(U_{A}(x,y)\) respectively. We also define the principal's _strategy space_ as the simplex over actions: \(_{P}=(_{P})\). For a strategy \(_{P}\), we oftentimes abuse notation slightly and write \(U_{P}(,y):=_{x}[U_{P}(x,y)]\).

_Repeated Stackelberg games_ capture the _repeated_ interaction between a principal and an agent over \(T\) rounds. What distinguishes Stackelberg games from other types of games is the inter-temporal relationship between the principal's action/strategy and the agent's response; specifically, the principal first commits to a strategy \(_{t}_{P}\) and the agent subsequently _best-responds_ to it with \(y_{t}_{A}\). Let \(_{t}_{P}=_{P}\) be the agent's _belief_ regarding the principal's strategy at round \(t\). In standard Stackelberg games: \(_{t}=_{t}\), i.e., the agent has full knowledge of the principal's strategy. In this paper, we consider games where the agent does not in general know \(_{t}\) when playing, but they only best-respond according to their belief \(_{t}\). The agent's _best-response_ to _belief_\(_{t}\) according to her underlying utility function \(U_{A}\) is action \(y_{t}_{A}\) such that

\[y_{t}(_{t})( _{t})=*{argmax}_{y_{A}}\;*{ }_{x_{t}}[U_{A}(x,y)].\] (2)

We often overload notation and write \(U_{A}(,y):=_{x}[U_{A}(x,y)]\). Note that from Equation (2), the best-responses to \(_{t}\) form _a set_. If this set is not a singleton, we use either a _deterministic_ or a _randomized tie-breaking_ rule. For the _deterministic_ tie-breaking rule, the agent breaks ties according to a predefined preference rule \(\) over the set of actions \(_{A}\). For the _randomized_ tie-breaking rule, the agent chooses \(y_{t}\) by sampling from the set \((_{t})\) uniformly at random, i.e., \(y_{t}((_{t}))\).

The _Stackelberg value_ of the game is the principal's optimal utility when the agent best responds:

\[V^{}=_{^{}_{P}}_{y^{} (^{})}U_{P}(^{},y^{}).\]

In the above definition \(^{}\) is referred to as the _principal's optimal strategy_.

For an agent's action \(y_{A}\), we define the corresponding _best-response polytope_\(P_{y}\) as the set of all of the agent's beliefs that induce \(y\) as the agent's best-response, i.e., \(P_{y}=\{_{P}:y()\}\). We make the following standard assumption, which intuitively means that there are sufficiently many strategies that induce \(y^{}\) as the agent's best-response.

**Assumption 2.4** (Regularity).: _The principal's optimal strategy \(^{}(_{P})\) and the agent's optimal action \(y^{}(^{})\) satisfy a regularity condition: \(P_{y^{}}\) contains an \(_{2}\) ball of radius \(>0\)._

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol1), the agent forms \((,)\)-adaptively calibrated forecasts as their beliefs \(_{t}\) regarding \(_{t}\).

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol2), the agent forms \((,)\)-adaptively calibrated forecasts as their beliefs \(_{t}\) regarding \(_{t}\).

**Calibrated Stackelberg Games.** In CSGs (see Figure 1 for the principal-agent interaction protocol3), the agent forms \((,)\)-adaptively calibrated forecasts as their beliefs \(_{t}\) regarding \(_{t}\).

[MISSING_PAGE_POST]

We first define binning functions that are especially appropriate for forecasts in games. In CSGs, we define \(\) based on whether \(i\) is a best-response to the input calibrated forecast, i.e., \(_{P}\):

\[w_{i}() =\{i(),i j, j i\}  tie-breaking)}\] \[w_{i}() =\{i()\}}{|( )|}  tie-breaking)}\]

Note that both binning functions meet the conditions of Definition 2.1. Applying Definition 2.3 for calibrated agent forecasts in CSGs we have the following:

**Definition 2.5** (\(\)-Adaptively Calibrated Agent for CSGs).: _The agent is called \(\)-adaptively calibrated with rate \(r_{}() o(1)\), if for any sequence of principal strategies \(_{1},,_{T}_{P}\) the agent takes a sequence of actions \(y_{1},,y_{T}\) that satisfy the following requirements: 1) there is a sequence of forecasts \(_{t}_{P}\) for \(t[T]\), such that \(y_{t}(_{t})\), and 2) forecasts \(_{1},,_{T}\) are \(\)-calibrated for binning \(\) with rate \(r_{}()\) with respect to the principal's strategies \(_{1},,_{T}\)._

We next review the fundamental constructs from Equation (1) and their intuitive meaning in this setting. \(n_{[s,t]}(i)_{[s,t]}w_{i}(_{})\) is now the expected number of times that the forecast has induced action \(i\) from the agent as their best response during interval \([s,t]\), \(}_{[s,t]}(i)_{[s,t]}w_{i}(_{ })_{}/n_{[s,t]}(i)\) is the expected forecast that induces action \(i\) from the agent as their best response during interval \([s,t]\), and \(}_{[s,t]}(i)_{[s,t]}w_{i}(_{ })_{}/n_{[s,t]}(i)\) is the expected principal strategy that induces action \(i\) from the agent as their best response during interval \([s,t]\). The requirement for an agent to be calibrated is quite mild, as the forecasts are binned only according to the best-response they induce.

## 3 Principal's learning algorithms

In this section (see Appendix C for full proofs and convergence rates), we study the relationship between the principal's Stackelberg value \(V^{}\) and the best utility the principal can obtain from learning to play a sequence of strategies \(\{_{t}\}_{t[T]}\) against calibrated agents, i.e., \(_{t[T]}U_{P}(_{t},y_{t})\). The relationship between \(V^{}\) and \(_{t[T]}U_{P}(_{t},y_{t})\) is not a priori clear. In the case of calibrated forecasts, the agents do not know the exact \(_{t}\) when they choose their response. Instead, they base their decisions on the history of the principal's strategies so far. A principal then may be able to create historical patterns that lead the agents to worse actions, thus obtaining better utility himself. Indeed, several works have shown how historical patterns can afford the principal much better utility than \(V^{}\) when the agents are no-regret [13; 20]. Surprisingly, we show that this is not the case when the agents are calibrated; \(_{t[T]}U_{P}(_{t},y_{t})\) is upper bounded by \(TV^{}\) and a term that is sublinear in \(T\) and depends on the calibration parameters.2

**Theorem 3.1**.: _Assume that the agent is \((,)\)-as calibrated with rate \(r_{}()\) and negligible \(\). Then, for any sequence \(\{_{t}\}_{t[T]}\) for the principal's strategies in a CSG, with probability at least \(1-2\), the principal's utility is upper bounded as: \(_{T}_{t[T]}U_{P}(_{t},y_{t}) V^{}\)._

Proof sketch.: We sketch the proof for the deterministic tie-breaking, as the randomized one needs just an application of Azuma-Hoeffding. We first rewrite \(_{t[T]}U_{P}(_{t},y_{t})\) partitioned in the principal's utility for each round-specific forecast that induces action \(i\) as the best-response from the agent, for all actions \(i_{A}\): \(_{t[T]}U_{P}(_{t},y_{t})=_{i_{A}}_{t [T]}w_{i}(_{t}) U_{P}(_{t},i)\). This equivalence holds because each \(_{t}\) maps to a _single_ best response \(i_{A}\) (deterministic tie-breaking). Because of the linearity of the principal's reward in the principal's strategy: \(_{i_{A}}_{t[T]}w_{i}(_{t}) U_{P}( _{t},i)=_{i_{A}}n_{T}(i) U_{P}(}_{T},i)\), where \(n_{T}(i)=n_{[0,T]}(i)\). Adding and subtracting \(U_{P}(}_{T},i)\) from the above, we now need to bound the quantity: \(_{i_{A}}n_{T}(i)(U_{P}(}_{T}(i),i)+ U_{P }(,i),}_{T}(i)-}_{T}(i))\). The first term is upper bounded by \(V^{}T\); note that \(i(}_{T})\) (and \(V^{}=_{}_{y()}U_{P}(,y)\)) since \(_{t}(i) P_{i}\) and hence that should also be true for the average of \(_{t}(i)\) over \(t\) rounds. The second summand is bounded by the calibration error of Definition 2.3.

On the other hand, it may seem that because the agent's behavior is less specified when she uses calibrated forecasts (as opposed to full knowledge), the principal may only be able to extract much less utility compared to \(V^{}\). Again, we show that this is not the case and that there exist algorithms for the principal such that the sequence of strategies \(\{_{t}\}_{t[T]}\) is asymptotically approaching \(V^{}\).

**Theorem 3.2**.: _There exists an algorithm for the principal in CSGs that achieves average utility: \(_{T}_{t[T]}U_{P}(_{t},y_{t}) V^{}\)._

Algorithm 1 is an explore-then-commit algorithm; it first estimates an _appropriate_ strategy for the principal \(}\) (Explore), and then repeatedly plays it until the end (Commit). In the remainder of the section, we sketch the proof for Theorem 3.2 and point to exact lemma statements in the Appendix. Let \(T_{1},T_{2}\) denote the set of rounds that belong in the Explore and Commit phase respectively.

To elaborate on the objectives of the Explore phase, let us first consider a setting with zero calibration error, where the agent's forecasting algorithm is perfectly and adaptively calibrated, leading to \(y_{t}=(_{t})\) at every round. The task for the Explore phase simplifies to identifying a near-optimal strategy \(}\) through best response oracles that satisfies \(U_{P}(},(})) V^{}- _{1}\) for a predetermined \(_{1}\). We formalize this property in **(P1)**. Given that the agent is perfectly calibrated, in the Commit phase, the agent always plays \(=(})\), leading to an upper bound of \(_{1}|T_{2}|\) on the Stackelberg regret. Hence, Algorithm 1's regret is bounded by \(V^{}|T_{1}|+_{1}|T_{2}|\).

**(P1)**: \(U_{P}(},) V^{}-_{1}\) for \((})\), i.e., \((},)\) is an approximate Stackelberg equilibrium.

Moving away from the idealized setting, we must account for possible discrepancies between \(y_{t}\) and \((_{t})\) due to calibration error. This introduces: (i) An increased sample complexity \(|T_{1}|\) in the Explore phase, given the necessity to learn a near-optimal strategy from noisy responses; (ii) Potential deviations from the action \(=(})\) in the Commit phase due to miscalibrations in belief. To address the first challenge, we employ Algorithm 2, which constructs an approximate best response oracle by repeatedly interacting with a calibrated agent. For the second challenge, we require our learned policy \(}\) to be robust against inaccurate forecasts. This is reflected in condition **(P2)**, which necessitates the ball of radius \(_{2}\) around \(}\) to be fully contained in the polytope \(P_{}\). The critical insight from **(P2)** is: for any forecast \(_{t}\) that results in a best response \(y_{t}=(_{t})\), there must be a minimum distance of \(_{2}\) separating \(_{t}\) from \(}\). We will now proceed to formalize **(P2)**, but before delving into that, it is important to introduce some additional notations.

Let \(B_{2}(x,)\) denote the ball of radius \(\) around \(x\), i.e., \(B_{2}(x,)\{x^{}:\|x-x^{}\|_{2}\}\). For a convex set \(S^{n}\), we use \(B_{2}(S,)\) to denote the _union_ of all balls of radius \(\) around the set, i.e., \(B_{2}(S,)_{x S}B_{2}(x,)\). For a convex set \(S^{n}\), we use \(B_{2}(S,-)\) to denote the set of all points in \(S\) that are "safely" inside \(S\) (i.e., all the points in a ball of radius \(\) around them still belong in \(S\)): \(B_{2}(S,-)\{x S:\;B_{2}(x,) S\}\). We call this last set, the \(\)-_conservative_ of \(S\). See Figure 2 for a pictorial illustration of the notations.

Figure 2: Pictorial representation for notation \(B_{2}(S,)\) (left) and \(B_{2}(S,-)\) (right).

[MISSING_PAGE_FAIL:7]

number of rounds to ensure at least one sample in the \(}{{2}}\)-ball around the center of \(P_{y}\)' with high probability. To build the approximate membership oracle for the \(_{2}\)-conservative best-response polytope for an action \(y\), we use ApproxMem (Algorithm 4 in Appendix C.2). Specifically, on input \(_{P}\), ApproxMem either asserts \( B_{2}(P_{y},-_{2}+_{1})\) or \( B_{2}(P_{y},-_{2}-_{1})\) with probability at least \(1-_{3}\). To do this, it samples \(\) points in proximity to \(\) and plays each one repeatedly for \(l\) rounds, while registering the best-response action observed for each one of these. If the most frequent best-response for all \(_{}\) is \(y\), then we can conclude with good probability that \(\) was inside \(B_{2}(P_{y},-_{2})\). See Lemma C.2 for more details.

## 4 Forecasting Algorithm for Adaptive Calibration

In this section, we examine whether there exist natural forecasting procedures that satisfy our Definition 2.3 about adaptively calibrated forecasts. We answer this question positively.

**Theorem 4.1**.: _For all \(>0\) and all binnings \(=\{w_{i}:^{m},i[k]\}\), there exists a parameter-free forecasting procedure that is \((,)\)-adaptively calibrated with rate \(r_{}(t)=O()\). Moreover, when \(\) is a continuous binning (i.e., each \(w_{i}\) is continuous), there exists a forecasting procedure that is \((0,)\)-adaptively calibrated with the same rate._

To prove the theorem, we use two main tools; the first one is a well-known algorithm of Luo and Schapire  (AdaNormalHedge) applied for online learning in the _sleeping experts_ problem (see Appendix D for details). Roughly speaking, the _sleeping experts_ is a standard online learning problem with \(T\) rounds and \(N\) experts, where at each round \(t\) there is only a subset of the experts being "awake" to be considered by the learner and report their predictions. Let \(I_{t,i}\) be the binary variable indicating whether expert \(i\) was awake at round \(t\) (\(I_{t,i}=1\)) or asleep (\(I_{t,i}=0\)). The interaction protocol between the learner and the adversary at each round \(t\) is: (i) The learner observes which experts are awake, i.e., \(\{I_{t,i}\}_{i[N]}\). (ii) The learner selects a probability distribution \(_{t}([N])\) supported on the set of _active_ experts \(A_{t}\{i:I_{t,i}=1\}\). (iii) The adversary selects a loss vector \(\{_{t,i}\}_{i[N]}\). (iv) The learner incurs expected loss \(_{t}=_{i_{t}}[_{t,i}]\). AdaNormalHedge is a _parameter-free_ online learning algorithm that when applied on the sleeping experts problem (and with appropriate initialization) obtains regret \(_{t}(i)=O((NT_{i})})\), where \(T_{i}=_{[t]}I_{,i}\).

The second tool that we use is _No-Regret vs. Best-Response dynamics (NRBR)_. NRBR are a form of no-regret dynamics between two players, where one of the players must also best-respond on average. Essentially, at each round \(t[T]\), the forecasting algorithm with the calibration rate of Theorem 4.1 outputs a randomized forecast \(_{t}_{P}\), by simulating an interaction between two players described below. For the first player, we construct a _sleeping experts_ problem instance, where the set of experts is \(=\{g_{(s,i,j,)}:s[T],i_{A},j_{P },\{ 1\}\}\). For each \(g_{(s,i,j,()}\)and \(t[T]\), we define the loss, sleeping/awake indicator, and instantaneous regret respectively as:

\[_{t,g_{(s,i,j,)}} L_{g_{(s,i,j,)}}(_{t},_{t})=w_{i}( _{t})(h_{t,j}-p_{t,j});\] (6) \[I_{t,g_{(s,i,j,)}} \{t s\};\] (7) \[r_{t,g} L_{t,g}(_{t,g}-_{t}).\]

where by \(h_{t,j},p_{t,g}\) we denote the \(j\)-th coordinate of \(_{t}\) and \(_{t}\) respectively. We defined the losses for our newly constructed sleeping experts' instance as above to make sure that there is a direct correspondence with the calibration error. Similar ideas for calibration (albeit not for the notion of adaptivity we consider) have been used in . We describe next the player interaction in NRBR.

**Player 1.** Runs AdaNormalHedge on expert set \(\) with a pre-specified prior \(_{0}\) over \(\) and feedback specified in Equations (6), (7). At each round \(t\), Player 1 computes distribution \(_{t}(A_{t}())\), where \(A_{t}()\) denotes the set of active experts \(g_{(s,i,j,)}\) with \(I_{t,g_{(s,i,j,)}}=1\).

**Player 2.** Best responds to \(_{t}\) by selecting \(Q_{t}(_{P})\) that satisfies:

\[_{_{t}_{P}}}_{  t_{t}\\ _{t} Q_{t}}[_{t,g}]=_{_{t}_{P}}}_{ t_{ t}\\ _{t} Q_{t}}[L_{g}(_{t},_{t}) ].\] (8)

After simulating the game above, the algorithm outputs forecast \(_{t} Q_{t}\). The existence of such a distribution \(Q_{t}\) is justified by the min-max theorem ([34, Fact 4.1] or [28, Theorem 5]). In the Appendix, we also give an explicit formula for \(Q_{t}\) in the special case of \(m=2\). When \(_{0}\) is continuous, player 2 can select a deterministic \(_{t}\) that achieves Equation (8) with \(=0\). This stronger property is justified by the outgoing fixed-point theorem [28, Theorem 4]. Note that this algorithm inherits its parameter-free property directly from AdaNormalHedge. We are now ready to provide a proof sketch for Theorem 4.1.

Proof sketch of Theorem 4.1.: Fix an instance of the NRBR game outlined above. We begin by Definition 2.2 of calibration error translated in the sleeping experts instance that we defined above:

\[_{i}(_{s:t},_{s:t})(t-s)=_{j[ m]}_{\{ 1\}}_{[s,t]}I_{,g_{(s,i,j,)}} _{,g_{(s,i,j,)}}\]

We add and subtract in the above \(_{}_{}\) to make the regret of the AdaNormalHedge on the sleeping experts instance appear and so the aforementioned becomes:

\[_{i}(_{s:t},_{s:t})(t-s)= {_{t}(g_{(s,i,j,)})}_{O()}+_{j _{P}}_{\{ 1\}}I_{,g_{(s,i,j, )}}_{,g_{(s,i,j,)}}}_{}\]

where for the regret, we have substituted the regret obtained by AdaNormalHedge. Note that \(_{}=_{g_{}}[_{,g}]\). This, together with Equation (6) helps us translate the sleeping experts' loss to a loss that depends on \(_{},_{}\). Adding and subtracting the term \(_{[s,t]}}_{_{} \\  Q_{}}[L_{g}(_{},)]\):

\[_{[s,t]}_{_{}(_{P})}}_{_{}\\  Q_{}}[L_{g}(_{},) ]+_{[s,t]}}_{g P_{}}[L_{g}( _{},_{})-}_{ Q_{ }}[L_{g}(_{},)]]\]

The first term above is upper bounded by \((t-s)\) (Equation (8)) (or \(0\) in the continuous setting). The second can be bound with martingale concentration inequalities. 

## 5 Continuous Games

In this section, we generalize our results for the case of _continuous_ Stackelberg games. The supplementary material can be found in Appendix E.

**Continuous Stackelberg Games.** We use again \(_{P}\) and \(_{A}\) to denote the principal and the agent action spaces, respectively. Both \(_{A},_{P}\) are convex, compact sets where \(_{P}^{m}\) and \(_{A}^{k}\). The utilities of the principal and the agent are given by continuous functions \(U_{P}:_{P}_{A}_{+}\) and \(U_{A}:_{P}_{A}_{+}\). In this setting, we assume that both the principal and the agent can only play deterministic strategies, i.e., \(_{P}=_{P}\). For \(x_{P}\), let \((x)\) be the best-response function that is implicitly defined as \(_{2}U_{A}(x,(x))=0\). Our continuous games satisfy Assumption 5.1: (i)-(iii) are standard assumptions used in previous works (e.g., ), but (iv) cannot be derived from (i) and (ii) without further assumptions on the correlation between \(x,y\). Nevertheless, (iv) (and the conditions under which it holds) has been justified in settings such as strategic classification .

**Assumption 5.1**.: _Utility functions \(U_{P}\), \(U_{A}\), and the domain \(_{P}\) satisfy the following:_

1. _For all_ \(x_{P},y_{A}\)_,_ \(U_{P}(x,y)\) _is_ \(L_{1}\)_-Lipschitz and concave in_ \(x\)_,_ \(L_{2}\)_-Lipschitz in_ \(y\)_, and bounded by_ \(W_{P}\) _in_ \(_{2}\) _norm._
2. _The best-response function_ \(:_{P}_{A}\) _is_ \(L_{}\)_-Lipschitz._
3. _Regularity of the feasible set_ \(_{P}=_{P}=_{P}\)_:_ * _The diameter is bounded:_ \((_{P})=_{,^{} _{P}}\|-^{}\|_{2} D_{P}\)_._ * \(B(0,r)_{P} B(0,R)\)_._
4. _The function_ \(U_{P}(,())\) _is concave with respect to_ \(\)_, and has Lipschitz constant_ \(L_{U}\)_._

The main result of this section is to show that even in _continuous_ CSGs, we can approximate asymptotically \(V^{}\) for the principal's utility, and that no better utility is actually achieved.

**Theorem 5.2**.: _For continuous CSGs satisfying Assumption 5.1, for all \(_{0}>0\), there exists a finite binning \(_{0}\) such that if the agent is \((0,_{0})\)4 - adaptively calibrated and the principal runs an appropriately parametrized instance of LazyGDwoG (Algorithm 3) then: \(_{}_{[]}_{i[M]}U_{P}(_{},y_{,i})  V^{}-_{0}\). Moreover, for any sequence of the principal's actions \(_{[1:T]}\), it holds that: \(_{T}_{t[T]}U_{P}(_{t},y_{t}) V^ {}+_{0}\)._

We outline next how LazyGDwoG works. LazyGDwoG is a variant of the gradient descent without a gradient algorithm (GDwoG) of Flaxman et al. . The main new component of the algorithm is that it separates the time horizon into epochs and for each epoch it runs an update of the GDwoG algorithm. During all the rounds that comprise an epoch (\(M\) in total), LazyGDwoG presents the same (appropriately smoothed-out) strategy to the agent and observes the \(M\) different responses from the agent. The intuition behind repeating the same strategy for \(M\) rounds is that the principal wants to give the opportunity to the agent to recalibrate for a better forecast, i.e., \(_{M}|\{i[M]:\|_{i}-\| _{0}\}|=0\). The remainder of the proof for Theorem 5.2 focuses on showing that when the calibrated forecasts converge to \(_{t}\), then the principal's utility converges to the utility they would have gotten if the agent was perfectly best responding to \(_{t}\).

## 6 Discussion and future directions

In this paper we introduced and studied learning in CSGs, where the agents best respond to the principal's actions based on _calibrated_ forecasts that they have about them. Our work opens up several exciting avenues for future research. First, although our main results prove asymptotic convergence, it is an open question whether our exact convergence rates can be improved both for general CSGs and for specific cases of Stackelberg games (e.g., strategic classification in more general models compared to , pricing ). Second, it is an interesting question whether our definition of adaptive calibration (Def. 2.3) can actually hold for the sum over all binning functions, instead of just the maximum. Finally, to provide our asymptotic convergence results we assumed that the principal has access to the agent's calibration rate \(r_{}()\); _some_ information regarding how \(_{t}\)'s relate to \(_{t}\)'s is necessary to leverage the fact that agents are calibrated. But we think that in some specific settings (e.g., strategic classification) there may actually exist extra information regarding the forecasts (compared to just knowing \(r_{}()\)) that can be leveraged to design learning algorithms for the principal with faster convergence rates. We discuss these directions in more detail in Appendix F.