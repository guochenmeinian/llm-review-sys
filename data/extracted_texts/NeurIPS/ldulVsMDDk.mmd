# Towards a Better Theoretical Understanding of Independent Subnetwork Training

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Modern advancements in large-scale machine learning would be impossible without the paradigm of data-parallel distributed computing. Since distributed computing with large-scale models imparts excessive pressure on communication channels, a lot of recent research was directed towards co-designing communication compression strategies and training algorithms with the goal of reducing communication costs. While pure data parallelism allows better data scaling, it suffers from poor model scaling properties. Indeed, compute nodes are severely limited by memory constraints, preventing further increases in model size. For this reason, the latest achievements in training giant neural network models rely on some form of model parallelism as well. In this work, we take a closer theoretical look at Independent Subnetwork Training (IST), which is a recently proposed and highly effective technique for solving the aforementioned problems. We identify fundamental differences between IST and alternative approaches, such as distributed methods with compressed communication, and provide a precise analysis of its optimization performance on a quadratic model.

## 1 Introduction

A huge part of today's machine learning success drives from the possibility to build more and more complex models and train them on increasingly larger datasets. This fast progress has become feasible due to advancements in distributed optimization, which is necessary for proper scaling when the training data sizes grow . In a typical scenario data parallelism is used for efficiency which consists of sharding the dataset across computing devices. This allowed very efficient scaling and accelerating of training moderately sized models by using additional hardware . Though, such data parallel approach can suffer from communication bottleneck, which sparked a lot of research on distributed optimization with compressed communication of the parameters between nodes [3; 27; 38].

### The need for model parallel

Despite the efficiency gains of data parallelism, it has some fundamental limitations when it comes to scaling up the model size. As the model dimension grows, the amount of memory required to store and update the parameters also increases, which becomes problematic due to resource constraints on individual devices. This has led to the development of model parallelism [11; 37], which splits a large model across multiple nodes, with each node responsible for computations of model parts [15; 47]. However, naive model parallelism also poses challenges because each node can only update its portion of the model based on the data it has access to. This creates a need for a very careful management of communication between devices. Thus, a combination of both data and model parallelism is often necessary to achieve efficient and scalable training of huge models.

Ist.Independent Subnetwork Training (IST) is a technique which suggests dividing the neural network into smaller independent sub-parts, training them in a distributed parallel fashion and then aggregating the results to update the weights of the whole model. According to IST, every subnetwork is operational on its own, has fewer parameters than the full model, and this not only reduces the load on computing nodes but also results in faster synchronization. A generalized analog of the described method is formalized as an iterative procedure in Algorithm 1. This paradigm was pioneered by  for networks with fully-connected layers and was later extended to ResNets  and Graph architectures . Previous experimental studies have shown that IST is a very promising approach for various applications as it allows to effectively combine data with model parallelism and train larger models with limited compute. In addition,  performed theoretical analysis of IST for overparameterized single hidden layer neural networks with ReLU activations. The idea of IST was also recently extended to the federated setting via an asynchronous distributed dropout  technique.

Federated Learning.Another important setting when the data is distributed (due to privacy reasons) is Federated Learning . In this scenario computing devices are often heterogeneous and more resource-constrained  (e.g. mobile phones) in comparison to data-center setting. Such challenges prompted extensive research efforts into selecting smaller and more efficient submodels for local on-device training . Many of these works propose approaches to adapt submodels, often tailored to specific neural network architectures, based on the capabilities of individual clients for various machine learning tasks. However, there is a lack of comprehension regarding the theoretical properties of these methods.

### Summary of contributions

When reviewing the literature, we have found that a rigorous understanding of IST convergence virtually does not exist, which motivates our work. The main contributions of this paper include

* A novel approach to analyzing distributed methods that combine data and model parallelism by operating with sparse submodels for a quadratic model.
* The first analysis of independent subnetwork training in homogeneous and heterogeneous scenarios without restrictive assumptions on gradient estimators.
* Identification of settings when IST can optimize very efficiently or converge not to the optimal solution but only to an irreducible neighborhood which is also tightly characterized.
* Experimental validation of the proposed theory through carefully designed illustrative experiments. Due to space limitations, the results (and proofs) are provided in the Appendix.

## 2 Formalism and Setup

We consider the standard optimization formulation of distributed/federated learning problem ,

\[_{x^{d}}\ [f(x)_{i=1}^{n}f_{i}(x) ],\] (1)

where \(n\) is the number of clients/workers, each \(f_{i}:^{d}^{d}\) represents the loss of the model parameterized by vector \(x^{d}\) on the data of client \(i\).

A typical Stochastic Gradient Descent (SGD) type method for solving this problem has the form

\[x^{k+1}=x^{k}- g^{k}, g^{k}=_{i=1}^{n}g_{i}^{ k},\] (2)

where \(>0\) is a stepsize and \(g_{i}^{k}\) is a suitably constructed estimator of \( f_{i}(x^{k})\). In the distributed setting, computation of gradient estimators \(g_{i}^{k}\) is typically performed by clients, sent to the server, which subsequently performs aggregation via averaging \(g^{k}=_{i=1}^{n}g_{i}^{k}\). The result is then used to update the model \(x^{k+1}\) via a gradient-type method (2), and at the next iteration the model is broadcast back to the clients. The process is repeated iteratively until a model of suitable qualities is found.

One of the main techniques used to accelerate distributed training is lossy _communication compression_. It suggests applying a (possibly randomized) lossy compression mapping \(\) to a vector/matrix/tensor \(x\) before it is transmitted. This saves bits sent per every communication round at the cost of transmitting a less accurate estimate \((x)\) of \(x\). The error caused by this routine also causes convergence issues, and to the best of our knowledge, convergence of IST-based techniques is for this reason not yet understood.

**Definition 1** (Unbiased compressor).: _A randomized mapping \(:^{d}^{d}\) is an_ **unbiased compression operator** (\(()\) for brevity) if for some \( 0\) and \( x^{d}\)

\[[(x)]=x,[\|(x )-x\|^{2}]\|x\|^{2}.\] (3)

A notable example of a mapping from this class is the _random sparsification_ (Rand-q for \(q\{1,,d\}\)) operator defined by

\[_{}(x)_{q}x= _{i S}e_{i}e_{i}^{}x,\] (4)

where \(e_{1},,e_{d}^{d}\) are standard unit basis vectors in \(^{d}\), and \(S\) is a random subset of \([d]\{1,,d\}\) sampled from the uniform distribution on the all subsets of \([d]\) with cardinality \(q\). Rand-q belongs to \((d/q-1)\), which means that the more elements are "dropped" (lower \(q\)), the higher is the variance \(\) of the compressor.

In this work, we are mainly interested in a somewhat more general class of operators than mere sparsifiers. In particular, we are interested in compressing via the application of random matrices, i.e., via _sketching_. A sketch \(_{i}^{k}^{d d}\) can be used to represent submodel computations in the following way:

\[g_{i}^{k}_{i}^{k} f_{i}(_{i}^{k}x^{k}),\] (5)

where we require \(_{i}^{k}\) to be a symmetric positive semidefinite matrix. Such gradient estimate corresponds to computing the local gradient with respect to a sparse submodel model \(_{i}^{k}x^{k}\), and additionally sketching the resulting gradient with the same matrix \(_{i}^{k}\) to guarantee that the resulting update lies in the lower-dimensional subspace.

Using this notion, Algorithm 1 (with one local gradient step) can be represented in the following form

\[x^{k+1}=_{i=1}^{n}[_{i}^{k}x^{k}- _{i}^{k} f_{i}(_{i}^{k}x^{k})],\] (6)

which is equivalent to the SGD-type update (2) when **perfect reconstruction** property holds

\[^{k}_{i=1}^{n}_{i}^{k}= ,\]

where \(\) is the identity matrix (with probability one). This property holds for a specific class of compressors that are particularly useful for capturing the concept of an _independent_ subnetwork partition.

**Definition 2** (Permutation sketch).: _Assume that model size is greater than number of clients \(d n\) and \(d=qn\), where \(q 1\) is an integer1. Let \(=(_{1},,_{d})\) be a random permutation of \([d]\). Then for all \(x^{d}\) and each \(i[n]\) we define Perm-q operator_

\[_{i} n_{j=q(i-1)+1}^{qi}e_{_{j}}e_{ _{j}}^{}.\] (7)Perm-q is unbiased and can be conveniently used for representing (non-overlapping) structured decomposition of the model such that every client \(i\) is responsible for computations over a submodel \(_{i}x^{k}\).

Our convergence analysis relies on assumption previously used for coordinate descent type methods.

**Assumption 1** (Matrix smoothness).: _A differentiable function \(f:^{d}\) is \(\)-smooth, if there exists a positive semi-definite matrix \(^{d d}\) such that_

\[f(x+h) f(x)+ f(x),h+ h,h, x,h^{d}.\] (8)

Standard \(L\)-smoothness condition is obtained as a special case of (8) for \(=L\).

### Issues with existing approaches

Consider the simplest gradient type method with compressed model in the single node setting

\[x^{k+1}=x^{k}- f((x^{k})).\] (9)

Algorithms belonging to this family require a different analysis in comparison to SGD [16; 18], Distributed Compressed Gradient Descent [3; 26] and Randomized Coordinate Descent [34; 36] type methods because the gradient estimator is no longer unbiased

\[[ f((x))] f(x)= [( f(x))].\] (10)

That is why such kind of algorithms are harder to analyze. So, prior results for _unbiased_ SGD  can not be directly reused. Furthermore, the nature of the bias in this type of gradient estimator does not exhibit additive (zero-mean) noise, thereby preventing the application of previous analyses for biased SGD .

An assumption like bounded stochastic gradient norm extensively used in previous works [30; 48] hinders an accurate understanding of such methods. This assumption hides the fundamental difficulty of analyzing biased gradient estimator:

\[[\| f((x))\|^{2}] G\] (11)

and may not hold even for quadratic functions \(f(x)=x^{}x\). In addition, in the distributed setting such condition can result in vacuous bounds  as it does not allow to accurately capture heterogeneity.

## 3 Results in the Interpolation Case

To conduct a thorough theoretical analysis of methods that combine data with model parallelism, we simplify the algorithm and problem setting to isolate the unique effects of this approach. The following considerations are made:

1. We assume that every node \(i\) computes the true gradient at the submodel \(_{i} f_{i}(_{i}x^{k})\).
2. A notable difference from the original IST algorithm 1 is that workers perform single gradient descent step (or just gradient computation).
3. Finally, we consider a special case of quadratic model (12) as a loss function (1).

Condition (1) is mainly for the sake of simplicity and clarity of exposition and can be potentially generalized to stochastic gradient computations. (2) is imposed because local steps did not bring any theoretical efficiency improvements for heterogeneous settings until very recently . And even then, only with the introduction of additional control variables, which goes against resource-constrained device setting. The reason behind (3) is that despite the seeming simplicity quadratic problem has been used extensively to study properties of neural networks [46; 49]. Moreover, it is a non-trivial model which allows to understand complex optimization algorithms [4; 10; 17]. It serves as a suitable problem for observing complex phenomena and providing theoretical insights, which can also be observed in practical scenarios.

Having said that we consider a special case of problem (1)

\[f(x)=_{i=1}^{n}f_{i}(x), f_{i}(x) x^{}_{i}x-_{i}^{}x.\] (12)

In this case, \(f(x)\) is \(}\)-smooth, and \( f(x)=}\,x-}\), where \(}=_{i=1}^{n}_{i}\) and \(}_{i=1}^{n} _{i}\).

### No linear term: problems and solutions

First, let us examine the case of \(_{i} 0\), which we call interpolation for quadratics, and perform the analysis for general sketches \(_{i}^{k}\). In this case the gradient estimator (2) takes the form

\[g^{k}=_{i=1}^{n}_{i}^{k} f_{i}(_{i}^{k}x^{k})=_{i=1}^{n}_{i}^{k}_{i} _{i}^{k}x^{k}=}^{k}\,x^{k}\] (13)

where \(}^{k}_{i=1}^{n} _{i}^{k}_{i}_{i}^{k}\). We prove the following result for a method with such an estimator.

**Theorem 1**.: _Consider the method (2) with estimator (13) for a quadratic problem (12) with \(} 0\) and \(_{i} 0\). Then if \(}[}\,}^{k}+}^{k}\,} ] 0\) and there exists constant \(>0\):_

\[[}^{k}\,}\,}^{k}]\,},\] (14)

_and the step size is chosen as \(0<\), the iterates satisfy_

\[_{k=0}^{K-1}[\| f(x^{k}) \|_{}^{-1}}^{2}}\,}^{-1}])-[f(x^{K}) ])}{ K},\] (15)

_and_

\[[\|x^{k}-x^{}\|_{}}^{2} ](1-_{}(}^{- {2}}\,}\,}^{-}) )^{k}\|x^{0}-x^{}\|_{}}^{2}.\] (16)

This theorem establishes an \((1/K)\) convergence rate with constant step size up to a stationary point and linear convergence for the expected distance to the optimum. Note that we employ weighted norms in our analysis, as the considered class of loss functions satisfies the matrix \(}\)-smoothness Assumption 1. The use of standard Euclidean distance may result in loose bounds that do not recover correct rates for special cases like Gradient Descent.

It is important to highlight that inequality (14) may not hold (for any \(>0\)) in the general case as the matrix \(}\) is not guaranteed to be positive (semi-)definite in the case of general sampling. The intuition behind it is that arbitrary sketches \(_{i}^{k}\) can result in gradient estimator \(g^{k}\), which is misaligned with the true gradient \( f(x^{k})\). Specifically, the inner product \( f(x^{k}),g^{k}\) can be negative, and there is no expected descent after one step.

Next, we give examples of samplings for which the inequality (14) can be satisfied.

**1. Identity.** Consider \(_{i}\). Then \(}^{k}=}\), \(}^{k}\,}\,}^{k}= }^{3},}=}^{2} 0\) and hence (14) is satisfied for \(=_{}(})\). So, (15) says that if we choose \(=\), then

\[_{k=0}^{K-1}\| f(x^{k})\|_{} ^{2}(})(f(x^{0})-f(x^{K}) )}{K},\]

which exactly matches the rate of Gradient Descent in the non-convex setting. As for iterates convergence, the rate in (16) is \((})}}{{_{}(})}}\) corresponding to precise Gradient Descent result for strongly convex functions.

**2. Permutation.** Assume \(n=d^{2}\) and the use of Perm-1 (special case of Definition 2) sketch \(_{i}^{k}=ne_{_{i}^{k}}e_{_{i}^{k}}^{}\), where \(^{k}=(_{1}^{k},,_{n}^{k})\) is a random permutation of \([n]\). Then

\[[}^{k}]=_{i=1}^{ n}n^{2}[_{i}^{k}_{i}_{i}^{k}]= _{i=1}^{n}n(_{i})=_{i =1}^{n}_{i}=n\,},\]where \(}_{i=1}^{n}_{i},_{i }(_{i})\). Then inequality (14) leads to

\[n\,}\,}\,} (}\,}+}\,}),\] (17)

which may not always hold as \(}\,}+}\,}\) is not guaranteed to be positive definite even in case of \(} 0\). However, such kind of condition can be enforced via a slight modification of permutation sketches \(\{}_{i}\}_{i=1}^{n}\), which is done in Section 3.1.2. The limitation of such an approach is that compressors \(}_{i}\) become no longer unbiased.

**Remark 1**.: _Matrix \(}\) in case of permutation sketches may not be positive-definite. Consider the following homogeneous (\(_{i}\)) two-dimensional problem example_

\[=[a&c\\ c&b].\] (18)

_Then_

\[}=[}\,}+}\,}]=[ a^{2}&c(a+b)/2\\ c(a+b)/2&b^{2}],\] (19)

_which for \(c>\) has \((})<0\), and thus \(} 0\) according to Sylvester's criterion._

Next, we focus on the particular case of **Permutation** sketches, which are the most suitable for model partitioning according to Independent Subnetwork Training (IST). At the rest of the section, we discuss how the condition (14) can be enforced via a specially designed preconditioning of the problem (12) or modification of sketch mechanism (7).

#### 3.1.1 Homogeneous problem preconditioning

To start consider a homogeneous setting \(f_{i}(x)=x^{}x\), so \(_{i}\). Now define \(=()-\) matrix with elements equal to diagonal of \(\). Then problem can be converted to

\[f_{i}(^{-}x)=(^{-}x )^{}(^{-}x)=x ^{}^{-}^{- {1}{2}})}_{}x,\] (20)

which is equivalent to the original problem after a change of variables \(^{-}x\). Note that \(=()\) is positive definite as \( 0\), and therefore \(} 0\). Moreover, the preconditioned matrix \(}\) has all ones on the diagonal: \((})=\). If we now combine it with Perm-1 sketches

\[[}^{k}]=[ _{i=1}^{n}_{i}\,}\,_{i}]=n (})=n.\]

Therefore, inequality (14) takes the form \(}=n\,}n^{2}\,}\), which holds for \( n\), and left hand side of (15) can be transformed the following way

\[\| f(x^{k})\|_{}^{-1}\,}\, }^{-1}}^{2} n_{}(}^{-1} )\| f(x^{k})\|_{}^{2}=n_{}(})\| f(x^{k})\|_{}^{2}\] (21)

for an accurate comparison to standard methods. The resulting convergence guarantee

\[_{k=0}^{K-1}[\| f(x^{k})\|_{ }^{2}](})(f(x ^{0})-[f(x^{K})])}{K},\] (22)

which matches classical Gradient Descent.

#### 3.1.2 Heterogeneous sketch preconditioning

In contrast to homogeneous case the heterogeneous problem \(f_{i}(x)=x^{}_{i}x\) can not be so easily preconditioned by a simple change of variables \(^{-}x\), as every client \(i\) has its own matrix \(_{i}\). However, this problem can be fixed via the following modification of Perm-1, which scales the output according to the diagonal elements of local smoothness matrix \(_{i}\):

\[}_{i}[_{i}^{-} ]_{_{i},_{i}}e_{_{i}}e_{_{i}}^{}.\] (23)In this case \([}_{i}_{i}}_{i}]= \), \([}^{k}]=\), and \(}=}\). Then inequality (14) is satisfied for \( 1\). If one plugs these results into (15), such convergence guarantee can be obtained

\[_{k=0}^{K-1}[\| f(x^{k}) \|_{}^{2}](})(f(x^{0})-[f(x^{K})])}{K},\] (24)

which matches the Gradient Descent result as well. Thus we can conclude that heterogeneity does not bring such a fundamental challenge in this scenario. In addition, a method with Perm-1 is significantly better in terms of computational and communication complexity as it requires calculating the local gradients with respect to much smaller submodels and transmits only sparse updates. This construction also shows that for \(=1/=1\)

\[_{}(}^{-}\,}\,}^{-})=_{}( }^{-}\,}\,}^{-})=1,\] (25)

which after plugging into the bound for the iterates (16) shows that the method basically converges in 1 iteration. This observation that sketch preconditioning can be extremely efficient, although it uses only the diagonal elements of matrices \(_{i}\).

Now when we understand that the method can perform very well in the special case of \(}_{i} 0\) we can move on to a more complicated situation.

## 4 Irreducible Bias in the General Case

Now we look at the most general heterogeneous case with different matrices and linear terms \(f_{i}(x)x^{}_{i}x-x^{}\,_{i}\,\). In this instance gradient estimator (2) takes the form

\[g^{k}=_{i=1}^{n}_{i}^{k} f_{i}(_{i}^{k}x^{k})=_{i=1}^{n}_{i}^{k}( _{i}_{i}^{k}x^{k}-_{i})=}^{k}\,x^{k}-},\] (26)

where \(}=_{i=1}^{n}_{i}^{k} \,_{i}\). Herewith let us use a heterogeneous permutation sketch preconditioner (23) like in Section 3.1.2 Then \([}^{k}]=\) and \([}]=} }\), where \(}_{i=1}^{n}_{i}^{-}\,_{i}\). Furthermore expected gradient estimator (26) results in \([g^{k}]=x^{k}-} }\) and can be transformed the following way

\[[g^{k}]=}^{-1}\,}\,x^{k}}^{-1}\,}-}}=}^{-1}\, f (x^{k})+}^{-1}\,}- {}}}_{h},\] (27)

which reflects the decomposition of the estimator into optimally preconditioned true gradient and a bias, depending on the linear terms \(_{i}\).

### Bias of the method

Estimator (27) can be directly plugged (with proper conditioning) into general SGD update (2)

\[[x^{k+1}]=x^{k}-[g^{k}]=(1- )x^{k}+}}=(1- )^{k+1}\,x^{0}+}} _{j=0}^{k}(1-)^{j}.\] (28)

The resulting recursion (28) is exact, and its asymptotic limit can be analyzed. Thus for constant \(<1\) by using the formula for the sum of the first \(k\) terms of a geometric series, one gets

\[[x^{k}]=(1-)^{k}\,x^{0}+}{ }}\, }},\]

which shows that in the limit, the first initialization term (with \(x^{0}\)) vanishes while the second converges to \(}}\). This reasoning shows that the method does not converge to the exact solution

\[x^{k} x^{} x^{}^{d}}{} \{x^{}\,}\,x-x^{}\,}\},\]

which for the positive-definite \(}\) can be defined as \(x^{}=}^{-1}\,}\), while \(x^{}=}_{i=1}^{n}_{i}^{-}\, _{i}\). So, in general, there is an unavoidable bias. However, in the limit case: \(n=d\), the bias diminishes.

### Generic convergence analysis

While the analysis in Section 4.1 is precise, it does not allow us to compare the convergence of IST to standard optimization methods. Due to this, we also analyze the non-asymptotic behavior of the method to understand the convergence speed. Our result is formalized in the following theorem.

**Theorem 2**.: _Consider the method (2) with estimator (26) for a quadratic problem (12) with the positive definite matrix \(} 0\). Assume that for every \(_{i}(_{i})\) matrices \(_{i}^{-}\) exist, scaled permutation sketches (23) are used and heterogeneity is bounded as \([\|g^{k}-[g^{k}]\|_{}}^{2}]^{2}\). Then for step size is chosen as_

\[0<_{c,},\] (29)

_where \(_{c,}(0,1]\) for \((0,1/2)\), the iterates satisfy_

\[_{k=0}^{K-1}[\| f(x^{k})\|_{ }^{-1}}^{2}])- [f(x^{K})])}{ K}+(2^{-1}(1-) +)\|h\|_{}}^{2}+^{2},\] (30)

_where \(}=_{i=1}^{n}_{i},h=}^{-1}\,}-}_ {i=1}^{n}_{i}^{-}\,_{i}\) and \(}=_{i=1}^{n}_{i}\)._

Note that the derived convergence upper bound has a neighborhood proportional to the bias of the gradient estimator \(h\) and level of heterogeneity \(^{2}\). Some of these terms with factor \(\) can be eliminated via decreasing learning rate schedule (e.g., \( 1/\)). However, such a strategy does not diminish the term with a multiplier \(2^{-1}(1-)\), making the neighborhood irreducible. Moreover, this term can be eliminated for \(=1\), which also minimizes the first term that decreases as \(1/K\). Though, such step size choice maximizes the terms with factor \(\). Furthermore, there exists an inherent trade-off between convergence speed and the size of the neighborhood.

In addition, convergence to the stationary point is measured in the weighted by \(}^{-1}\) squared norm of the gradient. At the same time, the neighborhood term depends on the weighted by \(}\) norm of \(h\). This fine-grained decoupling is achieved by carefully applying Fenchel-Young inequality and provides a tighter characterization of the convergence compared to using standard Euclidean distances.

Homogeneous case.In this scenario, every worker has access to the all data \(f_{i}(x)x^{}x-x^{}\,\). Then diagonal preconditioning of the problem can be used as in the previous Section 3.1.1. This results in a gradient \( f(x)=}\,x-}\) for \(}=^{-}^{-}\) and \(}=^{-}\,\). If it is further combined with a scaled by \(1/\) Permutation sketch \(_{i}=e_{_{i}}e_{_{i}}^{}\), the resulting gradient estimator is

\[g^{k}=x^{k}-}\,}=}^{-1}\,  f(x^{k})+,\] (31)

for \(=}^{-1}\,}-}\, }\). In this case heterogeneity term \(^{2}\) from upper bound (30) disappears as \([\|g^{k}-[g^{k}]\|_{ {}}^{2}]=0\), thus the neighborhood size can significantly decrease. However, the bias term depending on \(\) still remains as the method does not converge to the exact solution \(x^{k} x^{} x^{}=}^{-1}\,}\) for positive-definite \(}\). Nevertheless the method's fixed point \(x^{}=\,/\) and solution \(x^{}\) can coincide when \(}^{-1}\,}=}\,}\), which means that \(}\) is the right eigenvector of matrix \(}^{-1}\) with eigenvalue \(}\).

Let us contrast obtained result (30) with non-convex rate of SGD  with constant step size \(\) for \(L\)-smooth and lower-bounded \(f\)

\[_{k\{0,,K-1\}}\| f(x^{k})\|^{2})- f)}{ K}+ LC,\] (32)

where constant \(C\) depends, for example, on the variance of stochastic gradient estimates. Observe that the first term in the compared upper bounds (32) and (30) is almost identical and decreases with speed \(1/K\). But unlike (30) the neighborhood for SGD can be completely eliminated by reducing the step size \(\). This highlights a fundamental difference of our results to unbiased methods.

The intuition behind this issue is that for SGD-type methods like Compressed Gradient Descent

\[x^{k+1}=x^{k}-( f(x^{k}))\] (33)

the gradient estimate is unbiased and enjoys the property that variance

\[[\|( f(x^{k}))- f(x^{k})\|^{2}] \| f(x^{k})\|^{2}\] (34)

goes down to zero as the method progresses because \( f(x^{k}) f(x^{})=0\) in the unconstrained case. In addition, any stationary point \(x^{}\) ceases to be a fixed point of the iterative procedure as

\[x^{} x^{}- f((x^{})),\] (35)

in the general case, unlike for Compressed Gradient Descent with both biased and unbiased compressors \(\). So, even if the method (computing gradient at sparse model) is initialized from the _solution_ after one gradient step, it may get away from there.

### Comparison to previous works

Independent Subnetwork Training .There are several improvements over the previous works that tried to theoretically analyze the convergence of Distributed IST.

The first difference is that our results allow for an almost arbitrary level of model sparsification, i.e., work for any \( 0\) as permutation sketches can be viewed as a special case of compression operators (1). This improves significantly over the work of , which demands3\( n^{}{{1}}}/{^{2}}\). Such a requirement is very restrictive as the condition number \(L/\) of the loss function \(f\) is typically very large for any non-trivial optimization problem. Thus, the sparsifier's (4) variance \(=d/q-1\) has to be very close to 0 and \(q d\). So, the previous theory allows almost no compression (sparsification) because it is based on the analysis of Gradient Descent with Compressed Iterates .

The second distinction is that the original IST work  considered a single node setting and thus their convergence bounds did not capture the effect of heterogeneity, which we believe is of crucial importance for distributed setting [9; 39]. Besides, they consider Lipschitz continuity of the loss function \(f\), which is not satisfied for a simple quadratic model. A more detailed comparison including additional assumptions on the gradient estimator made in  is presented in the Appendix.

FL with Model Pruning.In a recent work  made an attempt to analyze a variant of the FedAvg algorithm with sparse local initialization and compressed gradient training (pruned local models). They considered a case of \(L\)-smooth loss and sparsification operator satisfying a similar condition to (1). However, they also assumed that the squared norm of stochastic gradient is uniformly bounded (11), which is "pathological"  especially in the case of local methods as it does not allow to capture the very important effect of heterogeneity and can result in vacuous bounds.

In the Appendix we show some limitations of other relevant previous approaches to training with compressed models: too restrictive assumptions on the algorithm  or not applicability in our problem setting .

## 5 Conclusions and Future Work

In this study, we introduced a novel approach to understanding training with combined model and data parallelism for a quadratic model. This framework allowed to shed light on distributed submodel optimization which revealed the advantages and limitations Independent Subnetwork Training (IST). Moreover, we accurately characterized the behavior of the considered method in both homogeneous and heterogeneous scenarios without imposing restrictive assumptions on gradient estimators.

In future research, it would be valuable to explore extensions of our findings to settings that are closer to practical scenarios, such as cross-device federated learning. This could involve investigating partial participation support, leveraging local training benefits, and ensuring robustness against stragglers. Additionally, it would be interesting to generalize our results to non-quadratic scenarios without relying on pathological assumptions.