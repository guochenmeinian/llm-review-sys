# Advection Augmented Convolutional Neural Networks

Niloufar Zakariaei

University of British Columbia

Vancouver, Canada

nilouzk@student.ubc.ca

&Siddharth Rout

University of British Columbia

Vancouver, Canada

siddharth.rout@ubc.ca

&Eldad Haber

University of British Columbia

Vancouver, Canada

ehaber@eoas.ubc.ca

&Moshe Eliasof

University of Cambridge

Cambridge, United Kingdom

me532@cam.ac.uk

Equal contribution.

###### Abstract

Many problems in physical sciences are characterized by the prediction of space-time sequences. Such problems range from weather prediction to the analysis of disease propagation and video prediction. Modern techniques for the solution of these problems typically combine Convolution Neural Networks (CNN) architecture with a time prediction mechanism. However, oftentimes, such approaches underperform in the long-range propagation of information and lack explainability. In this work, we introduce a physically inspired architecture for the solution of such problems. Namely, we propose to augment CNNs with advection by designing a novel semi-Lagrangian push operator. We show that the proposed operator allows for the non-local transformation of information compared with standard convolutional kernels. We then complement it with Reaction and Diffusion neural components to form a network that mimics the Reaction-Advection-Diffusion equation, in high dimensions. We demonstrate the effectiveness of our network on a number of spatio-temporal datasets that show their merit. Our code is available at https://github.com/Siddharth-Rout/deepADRnet.

## 1 Introduction and Motivation

Convolution Neural Networks (CNNs) have long been established as one of the most fundamental and powerful family of algorithms for image and video processing tasks, in applications that range from image classification , denoising  and reconstruction , to generative models . More examples of the impact of CNNs on various fields and applications can be found in  and references within.

At the core of CNNs, stands the convolution operation - a simple linear operation that is local and spatially rotation and translation equivariant. The locality of the convolution, coupled with nonlinear activation functions and deep architectures have been the force driving CNN architectures to the forefront of machine learning and artificial intelligence research . One way to understand the success of CNNs and attempt to generate an explainable framework for them is to view CNNs from a Partial Differential Equation (PDE) point of view . In this framework, the convolution is viewed as a mix of discretized differential operators of varying order. The layers of the network are then associated with time. Hence, the deep network can be thought of as a discretization of a nonlinear time-dependent PDE. Such observations have motivated parabolic network design thatsmooth and denoise images  as well as to networks that are based on hyperbolic equation  and semi-implicit architectures .

However, it is known from the literature , and is also demonstrated in our experiments, that CNN architectures tend to under-perform in tasks that require rapid transportation (also known as _advection_) of information from one side of an image to the other. In particular, in this paper, we focus on the prediction of the spatio-temporal behavior of image features, where significant transportation is present in the data. Examples of such data include the prediction of weather, traffic flow, and crowd movement.

**Related work:** In recent years, significant research was devoted to addressing spatio-temporal problems. Most of the works known to us are built on a combination of CNN to capture spatial dependencies and Recurrent Neural Networks (RNN) to capture temporal dependencies. A sample of papers that address this problem and the related problem of video prediction can be found in [4; 53; 46; 31; 22; 35; 13] and reference within. See also  and  for a recent comparison between different methods. Such methods typically behave as black boxes, in the sense that while they offer strong downstream performance, they often times lack a profound understanding of the learned underlying dynamics from the data. In addition, networks that try to predict the optical flow in videos [9; 64; 48] were proposed to estimate the flow in the original image domain. However, predictions on the image domain may be limited and not capture hidden dynamics. Another type of work that is designed for scientific datasets is , which uses Fourier-based methods to build the operators. See also  for a review on the topic.

**Motivation:** Notably, while a CNN is a versatile tool that allows learning spatial dependencies, it can have significant challenges in learning simple operations that require transportation. As an example, let us consider the problem of predicting the motion in the simple case that the input data is an image, where all pixels take the value of \(0\) except for a pixel on the bottom left (marked in gray), and the output is an image where the value is transported to a pixel on the top right. This example is illustrated in Figure 1. Clearly, no local operation, for example, a convolution of say, \(3 3\) or even \(7 7\) can be used to move the information from the bottom left of the image to the top right. Therefore, the architecture to achieve this task requires either many convolutions layers, or, downsampling the image via pooling, where the operations are local, performing convolutions on the downsampled image, and then upsampling the image via unpooling, followed by additional convolutions to "clean" coarsening and interpolation artifacts, as is typical in UNets [42; 8]. To demonstrate, we attempt to fit the data with a simple convolution residual network and with a residual network that has an advection block, as discussed in this paper. The convergence history for the two methods is plotted in Figure 1. We see that while a residual network is incapable of fitting the data, adding an advection block allows it to fit the data to machine precision.

This set of problems, as well as the relatively poor performance it offers on data that contains advection as in simple task in Figure 1 sets the motivation for our work. Our aim is to extend the set of tools that is available in CNNs beyond simple and local convolutions. For time-dependent PDEs, it is well known that it is possible to model most phenomena by a set of advection-diffusion-reaction equations (see, e.g., [12; 11] and references within). Motivated by the connection between the discretization of PDEs and deep network [43; 7], and our observations on the shortcomings of existing operations in CNNs, we propose reformulating CNNs into three different components.

Figure 1: A simple task of moving information from one side of the image to the other. The source image in A is moved to the target image in B. The convergence of a simple ResNet and an ADRnet proposed in this work is in (C).

Namely, (i) a pointwise term, also known as a _reaction_ term, where channels interact locally. (ii) A _diffusion_ term, where features are exchanged between neighboring pixels in a smooth manner. And, (iii) an _advection_ term, where features are passed from pixels to other pixels, potentially not only among neighboring pixels, while preserving _feature mass or color loss2_. As we discuss in Section 3, the combination of diffusion and reaction is equivalent to a standard CNN. However, there is no CNN mechanism that is equivalent to the advection term. Introducing this new term equips the network with flexibility in cases where information is carried directly.

Contributions:The contributions of this paper are three-fold. First, we form the spatio-temporal dynamics in high dimensions as an advection-diffusion-reaction process, which is novel and has not been studied in CNNs prior to our work. Second, we propose the use of the semi-Lagrangian approach for its solution, introducing a new type of a learnable linear layer, that is sparse yet non-local. This is in contrast to standard convolutional layers, which act locally. In contrast to advection, other mechanisms for non-local interactions, require dense interactions, which are computationally expensive . Specifically, our use of semi-Lagrangian methods offers a bridge between particle-based methods and convolutions . Thus, we present a new operation in the context of CNNs, that we call the _push operator_ to implement the advection term. This operator allows us to transport features anywhere on the image in a single step - an operation that cannot be modeled with small local convolution kernels. It is thus a simple yet efficient replacement to the standard techniques that are used to move information on an image. Third, we propose a methodology to learn these layers based on the splitting operator approach, and show that they can successfully model advective processes that appear in different datasets.

Limitations:The advection diffusion reaction model is optimal when applied to the prediction of images where the information for the prediction is somehow present in the given images. Such scenarios are often present in scientific applications. For example, for the prediction of the propagation of fluids or gasses, all we need to know is the state of the fluid now (and in some cases, in a few earlier time frames). A more complex scenario is the prediction of video. In this case, the next frame may have new features that were not present in previous frames. To this end, the prediction of video requires some generative power. While we show that our network can be used for video prediction and even obtain close to the state-of-the-art results, we observe that it performs best for scientific datasets.

## 2 Model Formulation

Notations and assumptions.We consider a spatio-temporal vector function of the form \((t,)=[_{1}(t,),,_{m}(t, )]\), where \(\) is the space vector function with \(m\) channels. The function \(\) is defined over the domain \(^{d}\), and time interval \([0,t_{j}]\). Our goal is to predict the function at time \(t_{k}\) for some \(t_{k}>t_{j}\), given the inputs up to time \(j\). For the problem we consider here, the time is sampled on a uniform grid with equal spacing. Below, we define the advection-diffusion-reaction system that renders the blueprint of the method proposed in this paper to achieve our goal.

Reaction-Advection-Diffusion System.Given the input function \(\), we first embed it in a higher dimensional space. We denote the embedding function by \(:\), defined as

\[(t,)=M_{}((t,), {}_{})\] (1)

where \(M_{}:^{m}^{c}\) is a multi-layer preceptron (MLP) that embeds the function \(\) from \(m\) to \(c>m\) channels with trainable parameters \(_{}\).

To represent the evolution of \(\) we evolve \(\) in the hidden dimension, \(c\), and then project it back into the space \(\). One useful way to represent the evolution of a spatio-temporal process is by combining three different processes, as follows:

* _Reaction:_ A pointwise process where channels interact pointwise (sometimes referred to as \(1 1\) convolutions)
* _Diffusion:_ A process where features are being communicated and diffused locally.

* _Advection:_ A process where information transports along mediums.

These three processes are also illustrated in Figure 2, and their composition defines the advection-diffusion-reaction differential equation on the embedded vector \(\).

The equation can be written as

\[(t,)}{ t} = (t,)+( (t,))+R((t,),),\] (2) \[(t=0,) = M((t=0,)).\] (3)

Here \(\) is the Laplacian, and \(\) is the divergence operator, as classically defined in PDEs . The equation is equipped with an initial condition and some boundary conditions. Here, for simplicity of implementation, we choose the Neumann boundary conditions, but other boundary conditions can also be chosen. The diffusivity coefficient \(\), velocity field \(\), and the parameters that control the reaction term \(R\) are trainable and are discussed in Section 3.

The equation is integrated on some interval \([0,T]\) and finally one obtains \((T,)\) by applying a second MLP, that projects the hidden features in \((t=T,)\) to the desired output dimension, which in our case is the same as the input dimension, i.e., \(m\):

\[(T,)=M_{}((T,), _{}),\] (4)

where \(_{}\) are trainable parameters for the projection MLP.

**Remark (Equation 2 Reformulation).** The discretization of Equation 2 can be challenging due to conservation properties of the term \(((t,))\). An alternative equation, which may be easier to discretize in our context, can be obtained by noting that

\[(t,)}{ t}+( )=(t,)}{ t}+ +.\] (5)

The operator on the left-hand side in Equation 5 is the continuity equation , where the _mass_ of \(\) is conserved. The first two terms on the right hand side, namely, \(_{t}+\) are sometimes refer to as the color equation  as they conserve the _intensity_ of \(\). For divergent free velocity fields, that is, when \(=0\), these are equivalent, however, for non-divergent fields, the term \(\) is a pointwise operator on \(\), that is, it is a reaction term. When training a model, one can use either Equation 2 in its continuity form or replace the term with Equation 5 and learn the term \(\) as a part of the reaction term, \(R\). We discuss this in discretization of our model in Section 3.3.

Figure 2: An illustration of the advection-diffusion reaction process. In the first step, Column A (advection), a pixel on the lower left of the image is transported into the middle of the mesh. In the second step, Column B (diffusion), the information is diffused to its neighbors, and finally, in the last step, Column C (reaction), each pixel interacts locally to change its value.

From a Partial Differential Equation to a Neural Network

To formulate a neural network from the differential equation in Equation 2 needs to be discretized in time and space. In this work, we assume data that resides on a regular, structured mesh grid, such as 2D images, and the spatial operators to discretize Equation 2 are described below. To discretize Equation 2 in _time_, we turn to Operator Splitting methods  that are common for the discretization of equations with similar structures, and were shown to be effective in deep learning frameworks . As we see next, such discretization leads to a neural a network that has three types of layers that are composed of each other, resulting in an effectively deeper neural network.

### Operator Splitting

The idea behind operator splitting is to split the integration of the ODE into parts . Specifically, consider a linear differential equation of the form

\[(t,)}{ t}=(t, )+(t,)+(t,),\] (6)

where \(,\) and \(\) are matrices. The solution to this system at time \(t\) is well known  and reads

\[(t,)=(t+t+t) (0,),\] (7)

where \(\) denotes the matrix exponentiation operation. It is also possible to approximate the exact solution presented in Equation 7 as follows

\[(t+t+t)(0,) (t)(((t)((t)( 0,))).\] (8)

The approximation is of order \(t\), and it stems from the fact that the eigenvalues of the matrices \(,\) and \(\) do not commute (see  for a thorough discussion). Equation 8 can also be interpreted in the following way. The solution, for a short time integration time \(t\), can be approximated by first solving the system \((t,)}{ t}=(t, ),_{0}=(0,)\) obtaining a solution \(_{R}(t,)\), followed by the solution of the system \((t,)}{ t}=_{R}( t,),_{0}=_{R}\) obtaining the solution \(_{RD}(t,)\) and finally solving the system \((t,)}{ t}=_{RD} (t,),_{0}=_{RD}\). The advantage of this approach is that it allows the use of different techniques for the solution of different problems. The derivation of the approach employed in this work is presented in Appendix A.4, which also provides a detailed explanation of the invariance to the order of splitting.

Let \(\) be the solution operator that advances \((t_{j},)\) to \(_{R}(t_{j+1},)\). Similarly, let \(\) be the solution operator that advances \(_{R}(t_{j+1},)\) to \(_{RD}(t_{j+1},)\) and lastly, let \(\) be the solution of the advection problem that advances \(_{RD}(t_{j+1},)\) to \((t_{j+1},)\). Then, a layer in the system can be written as the composite of three-layer

\[\,(t_{j},)= \,(t_{j},).\] (9)

That is, the resulting discretization in time yields a neural network architecture of a layer that is composed of three distinct parts. We now discuss each part separately.

### Advection

The innovative part of our network is advection. The advection approximately solves the equation

\[}{ t}=((,,t)),\] (10)

for a general velocity field \(\). For the solution of this equation, we now introduce a linear operation that we use to enhance the performance of our network. Our goal is to allow for information to pass over large distances. To this end, consider a displacement field \(=(_{1},_{2})\) and consider the push operation, \(()\) as the operation that takes every pixel in \(\) and displaces it from point \(\) to \(_{u}=+\). Since the point \(_{u}\) does not necessarily reside on a grid point, the information from \(_{u}\) is spread over four grid points neighbors, in weights that are proportional to the distance from these points. A sketch of this process is plotted in Figure 3 (a). The operator discussed above conserves that _mass_ of the features. A different implementation, as discussed in Remark 1, is to discretize the color equation. This is done by looking backward and using the interpolated value as shown in Figure 3(b). It is possible to show  that these linear operators are transposed of each other. Here, for each implementation, we chose to use the color equation. We show in ablation studies that the results when using either formulation are equivalent.

The process allows for a different displacement vector \(\) for every grid point. The displacement field \(\) in has \(2c\) channels and can vary in space and time. To model the displacement field, we propose to use the data at times,

\[_{k}=[(t_{k-j},),(t_{k-j+1},),,(t_{k},)],\] (11)

where \(j\) is the length of history used to learn the displacements.

Using \(_{k}\), the displacement field is computed by a simple residual convolution network, which we formally write as

\[_{k}=RN(_{k},),\] (12)

where \(RN\) is the residual network parameterized by \(\).

### Reaction

The reaction term is a nonlinear \(1 1\) convolution. This yields a residual network of the form

\[_{j+1}=_{j}+hM(_{j},_{j})=_{j}(_{j})\,_{j},\] (13)

where \(M\) is a standard, double-layer MLP with parameters \(_{j}\) and \(h\) is a step size that is a hyper-parameter. We may choose to have more than a single reaction step per iteration.

### Diffusion

For the diffusion step, we need to discretize the Laplacian on the image. We use the standard 5-point Laplacian  that can also be expressed as 2D group convolution . Let \(_{h}\) be the discrete Laplacian. The diffusion equation reads

\[_{j+1}-_{j}=h_{h}_{k}.\]

If we choose \(k=j\) we obtain an explicit scheme

\[_{j+1}=_{j}+h_{h}_{j}.\] (14)

Note that the diffusion layer can be thought of as a group convolution where each channel is convolved with the same convolution and then scaled with a different \(\). The forward Euler method for the diffusion requires \(h\) to be small if we want to retain stability. By choosing \(k=j+1\) we obtain the backward Euler method, which is unconditionally stable

\[_{j+1}=(-h_{h})^{-1}_{ j}=()_{j}.\] (15)

To invert the matrix we use the cosine transform  which yields an \(n n\) complexity for this step.

Figure 3: Discretization of the push operator. (a) Left: Semi-Lagrangian mass preserving transport, discretizing the continuity. (b) Right: Semi-Lagrangian color-preserving transport.

Combining Diffusion and Reaction to a Single Layer.In the above network the diffusion is handled by an implicit method (that is a matrix inversion) and the reaction is handled by an explicit method. For datasets where the diffusion is significant, this may be important; however, in many datasets where the diffusion is very small, it is possible to use an explicit method for the diffusion. Furthermore, since both the diffusion and reaction are computed by convolutions, it is possible to combine them into a \(3 3\) convolution (see  and  for additional discussions). This yields a structure that is very similar to a classical Convolutional Residual Network that replaces the diffusion and reaction steps. For the datasets used in this paper, we noted that this modest architecture was sufficient to obtain results that were close to state-of-the-art.

### Implementing the ADR Network

Implementing the diffusion and reaction terms, either jointly or combined, we use a standard Convolutional Residual Network. The advection term is implemented by using the sampleGrid command in PyTorch , which uses an efficient implementation to interpolate the images.

While the network can be used as described above, we found that better results can be obtained by denoising the output of the network. To this end, we have used a standard UNet and applied it to the output. As we show in our numerical experiments, this allows us to further improve downstream performance. The complete network is summarized in Algorithm 1.

```  Set \(_{0} M(_{k},_{o})\), \(_{k}\) as in equation 11. for\(j=0,1,...m-1\)do  Diffusion-Reaction \(_{DR}_{_{j}}_{ _{j}}_{j}\)  Compute displacement \(_{j}=RN(_{DR},_{j})\) as in equation 12  Push the image \(_{j+1}=(_{j})_{DR}\) endfor  Set \(_{k+}=M(_{m},_{T})\) (Optional) Denoise \(_{k+}=(_{k+})\) ```

**Algorithm 1** The ADR network

## 4 Experiments

Our goal is to develop architectures that perform well for scientific-related datasets that require advection. In our experiments, we use two such datasets, CloudCast , and the Shallow Water Equation in PDEbench . However, our ADRNet can also be used for the solution of video prediction. While such problems behave differently than scientific datasets, we show that our ADRNet can perform reasonably well for those applications as well. Below, we elaborate on the utilized datasets. We run our codes using a single NVIDIA RTX-A6000 GPU with 48GB of memory. Besides the experimental results reported in this Section, we provide additional results, from ablations to visualizations and measured runtimes in Appendix A.

### Datasets

We now describe the datasets considered in our experiments, which are categorized below.

**Scientific Datasets:** We consider the following datasets which arise from scientific problems and communities: (1) **SWE.** The shallow-water equations are derived from the compressible Navier

   Dataset & \(N_{}\) & \(N_{}\) & \((C,H,W)\) & History & Prediction \\  PDEBench-SWE & 900 & 100 & (1, 128, 128) & 10 & 1 \\ CloudCast & 5241 & 1741 & (1, 128, 128) & 4 & 4, 8, 12, 16 \\ Moving MNIST & 10000 & 10000 & (1, 64, 64) & 10 & 10 \\ KITTI & 2042 & 1983 & (3, 154, 512) & 2 & 1, 3 \\   

Table 1: Datasets statistics. Training and testing splits, image sequences, and resolutionsStokes equations. The data is comprised of 900 sets of 101 images, each of which is a time step. (2) **CloudCast.** The CloudCast dataset comprises 70,080 satellite images captured every 15 minutes and has a resolution of \(3712 3712\) pixels, covering the entire disk of Earth.

**Video Prediction Datasets:** These datasets are mainly from the Computer Vision community, where the goal is to predict future frames in videos. The datasets are as follows: (1) **Moving MNIST.** The Moving MNIST dataset is a synthetic video dataset designed to test sequence prediction models. It features 20-frame sequences where two MNIST digits move with random trajectories. (2) **KITTI.** The KITTI is a widely recognized dataset extensively used in mobile robotics and autonomous driving, and it also serves as a benchmark for computer vision algorithms.

The statistics of the datasets are summarized in Table 1, and in Appendix A, we provide results on additional datasets, namely TaxiBJ  and KTH .

### Evaluation

Ranking of Methods.Throughout all experiments where other methods are considered, we rank the top 3 methods using the color scheme of **First**, **Second**, and **Third**.

Performance on Scientific Datasets.We start our comparisons with the SWE and CloudCast datasets. These datasets fit the description of our ADRNet as future images depend on the history alone (that is, the history should be sufficient to recover the future). Indeed, Table 2 and Table 3 show that our ADRNet performs much better than other networks for these goals. Additional experiments on the Navier-Stokes dataset are provided in Appendix A.3

Examples of the predictions of the SWE dataset and the CloudCast datasets are plotted in Figure 4 and Figure 6. For the SWE dataset, the errors are very small and close to machine precision. For CloudCast, the data is noisy, and it is not clear how well it should fit. Predicting a single-time step, while useful, has limited applicability. Our goal is to push the prediction for longer, hence providing an alternative to expensive numerical integration. The results with SWE for long-time prediction (i.e. using the same 10 timesteps from history to predict 5, 10, 20, 50 timesteps in the future) are presented in Table 4, together with a comparison of the celebrated state of the art FNO method  where we see that our model performs well even for long-time prediction. As summarized in Table 4, ADRNet outperforms other models on long-range predictions for the SWE dataset. This is further illustrated in Figure 5, which shows the prediction accuracy of ADRNet at future time steps of 10, 20, and 50, demonstrating its ability to capture extended dependencies effectively.

   Method & NRMSE \(\) \\  UNet  & 8.3e-2 \\ PINN  & 1.7e-2 \\ MPP-AVIT-TI  & 6.6e-3 \\ ORCA-SWIN-B  & 6.0e-3 \\ FNO  & 4.4e-3 \\ MPP-AVIT-B  & **2.4e-3** \\ MPP-AVIT-L  & **2.2e-3** \\  ADRNet & **1.3e-4** \\   

Table 2: Results on PDEBench SWE Dataset.

Figure 4: Prediction and error for the SWE problem using our ADRNet.

   Method & SSIM (\(\)) & PSNR (\(\)) \\  AE-ConvLSTM  & **0.66** & **8.06** \\ MD-GAN  & **0.60** & **7.83** \\ TVL1  & 0.58 & 7.50 \\ Persistent  & 0.55 & 7.41 \\  ADRNet & **0.83** & **38.17** \\   

Table 3: Results on CloudCast dataset.

To evaluate the generalization capability of ADRNet, we conducted additional experiments using pre-trained ADRNet models on different datasets. These experiments, detailed in Appendix A.6, demonstrate the effectiveness of ADRNet in transfer learning tasks, such as adapting from the Navier-Stokes dataset to the SWE dataset.

   Metric &  &  &  &  \\   & ADRNet & FNO & ADRNet & FNO & ADRNet & FNO & ADRNet & FNO \\  MSE \(\) & 9.2e-08 & 4.0e-07 & 1.5e-07 & 5.8e-07 & 2.1e-07 & 6.7e-07 & 8.5e-07 & 1.4e-06 \\ nMSE \(\) & 8.5e-08 & 3.7e-07 & 1.4e-07 & 5.4e-07 & 1.9e-07 & 6.2e-07 & 7.8e-07 & 1.3e-06 \\ RMSE \(\) & 3.0e-04 & 6.3e-04 & 3.9e-04 & 7.6e-04 & 4.5e-04 & 8.1e-04 & 9.2e-04 & 1.2e-03 \\ nRMSE \(\) & 2.9e-04 & 6.1e-04 & 3.7e-04 & 7.3e-04 & 4.4e-04 & 8.7e-04 & 8.8e-04 & 1.1e-03 \\ MAE \(\) & 2.0e-04 & 2.8e-04 & 1.7e-04 & 3.7e-04 & 1.9e-04 & 3.6e-04 & 4.1e-04 & 5.7e-04 \\   

Table 4: Comparison of ADRNet and FNO on long range-predictions. We consider the prediction of different numbers of steps given different numbers of input steps. For example, the setting of 10 input steps and 5 prediction steps is denoted by \(10 5\).

Figure 5: Predictions on the SWE dataset at future time steps 10,20,50 (left to right). Our results demonstrate the large receptive field learned by ADRNet.

Figure 6: Example of the forecast by ADRNet compared to the ground truth over four time steps. ’t’ denotes the forecast time in 15-minute intervals. We use four input images to predict the subsequent four images. While changes in the CloudCast dataset in the two subsequent frames are slow, ADRNet achieved superior results in terms of PSNR and SSIM. A quantitative comparison is shown in Table 3.

Video Prediction Performance.We have used a number of video datasets to test our ADRNet. The results of two of them (Moving MNIST and KITTI) are reported in Table 5 and Table 6. We perform additional experiments for the KTH Action and TaxiBJ datasets in the appendix A. The moving MNIST dataset adheres to the assumptions of our ADRNet. Indeed, for this dataset, we obtain results that are very close to state-of-the-art methods.

The KTH Action dataset is more complex as not all frames can be predicted from the previous frames without generation power. Nonetheless, even for this dataset our ADRNet performs close to the state of the art. This limiting aspect of video synthesis is studied through experiments in appendix A.1.

## 5 Conclusion

In this paper, we have presented a new network for tasks that reside on a regular mesh that can be viewed as a multi-channel image. The method combines standard convolutions with a linear operator that transports information from one part of the image to another. The transportation vector field is learned from previous images (that is, history), allowing for information to pass from different parts of the image to others without loss. We combine this information within a diffusion-reaction process that can be coded by itself or by using a standard ResNet.

#### Acknowledgments

ME is funded by the Blavatnik-Cambridge fellowship, the Cambridge Accelerate Programme for Scientific Discovery, and the Maths4DL EPSRC Programme.

   Method & \)) \(\)} & \)) \(\)} \\   & \(t+1\) & \(t+3\) & \(t+1\) & \(t+3\) \\  SADM  & 83.06 & 72.44 & 14.41 & 24.58 \\ MCNET  & 75.35 & 63.52 & 24.04 & 37.71 \\ CorrWise  & 82.00 & N/A & 17.20 & N/A \\ OPT  & 82.71 & 69.50 & 12.34 & 20.29 \\ DMVFN (w/o R)  & **88.06** & **76.53** & **10.70** & **19.28** \\ DMVFN  & **88.53** & **78.01** & **10.74** & 19.27 \\  ADRNet & **85.86** & **83.62** & **7.54** & **9.26** \\   

Table 6: Results on KITTI.

   Method & MSE \(\) & MAE \(\) \\  MSPred  & 34.4 & - \\ MAU  & 27.6 & - \\ PhyDNet  & 24.4 & 70.3 \\ SimVP  & 23.8 & 68.9 \\ CrevNet  & 22.3 & - \\ TAU  & 19.8 & **60.3** \\ SwinLSTM  & **17.7** & - \\ IAM4VP  & **15.3** & **49.2** \\  ADRNet & **16.1** & **50.3** \\   

Table 5: Moving MNIST.