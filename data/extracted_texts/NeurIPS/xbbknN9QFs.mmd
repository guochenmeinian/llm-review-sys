# On Evaluating Adversarial Robustness of

Large Vision-Language Models

 Yunqing Zhao\({}^{*1}\), Tianyu Pang\({}^{*}\)\({}^{1}\)\({}^{2}\), Chao Du\({}^{1}\)\({}^{2}\), Xiao Yang\({}^{3}\), Chongxuan Li\({}^{4}\),

**Ngai-Man Cheung\({}^{1}\)\({}^{1}\), Min Lin\({}^{2}\)**

\({}^{1}\)Singapore University of Technology and Design

\({}^{2}\)Sea AI Lab, Singapore

\({}^{3}\)Tsinghua University \({}^{4}\)Rennin University of China

{zhaoyq, tianyupang, duchao, linmin}@sea.com;

yangxiao19@tsinghua.edu.cn; chongxuanli@ruc.edu.cn; ngaiman_cheung@sutd.edu.sg

Equal contribution. Work done during Yunqing Zhao's internship at Sea AI Lab.Correspondence to Tianyu Pang, Chao Du, and Ngai-Man Cheung.

###### Abstract

Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only _black-box_ system access and seek to deceive the model into returning the _targeted_ responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Our project page: yunqing-me.github.io/AttackVLM/.

## 1 Introduction

Large vision-language models (VLMs) have enjoyed tremendous success and demonstrated promising capabilities in text-to-image generation , image-grounded text generation (e.g., image captioning or visual question-answering) , and joint generation  due to an increase in the amount of data, computational resources, and number of model parameters. Notably, after being finetuned with instructions and aligned with human feedback, GPT-4  is capable of conversing with human users and, in particular, supports visual inputs.

Along the trend of multimodal learning, an increasing number of large VLMs are made publicly available, enabling the exponential expansion of downstream applications. However, this poses significant safety challenges. It is widely acknowledged, for instance, that text-to-image models could be exploited to generate fake content  or edit images maliciously . A silver lining is that adversaries must manipulate _textual inputs_ to achieve their evasion goals, necessitating extensive search and engineering to determine the adversarial prompts. Moreover, text-to-image models that areaccessible to the public typically include a safety checker to filter sensitive concepts and an invisible watermarking module to help identify fake content [69; 72; 108].

Image-grounded text generation such as GPT-4 is more interactive with human users and can produce commands to execute codes  or control robots , as opposed to text-to-image generation which only returns an image. Accordingly, potential adversaries may be able to evade an image-grounded text generative model by manipulating its _visual inputs_, as it is well-known that the vision modality is extremely vulnerable to human-imperceptible adversarial perturbations [8; 22; 29; 81]. This raises even more serious safety concerns, as image-grounded text generation may be utilized in considerably complex and safety-critical environments .1 Adversaries may mislead large VLMs deployed as plugins, for example, to bypass their safety/privacy checkers, inject malicious code, or access APIs and manipulate robots/devices without authorization.

In this work, we empirically evaluate the adversarial robustness of state-of-the-art _large_ VLMs, particularly against those that accept visual inputs (e.g., image-grounded text generation or joint generation). To ensure reproducibility, our evaluations are all based on open-source large models. We examine the most realistic and high-risk scenario, in which adversaries have only _black-box_ system access and seek to deceive the model into returning the _targeted_ responses. Specifically, we first use pretrained CLIP [65; 80] and BLIP  as surrogate models to craft targeted adversarial examples, either by matching textual embeddings or image embeddings, and then we transfer the adversarial examples to other large VLMs, including MiniGPT-4 , LLaVA , UniDiffuser , BLIP-2 , and Img2Prompt . Surprisingly, these transfer-based attacks can already induce targeted responses with a high success rate. In addition, we discover that query-based attacks employing transfer-based priors can further improve the efficacy of targeted evasion against these VLMs, as shown in Figure 1 (BLIP-2), Figure 2 (UniDiffuser), and Figure 3 (MiniGPT-4).

Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and advocate for a more comprehensive examination of their potential security defects prior to deployment, as discussed in Sec. 5. Regarding more general multimodal systems, our findings indicate that the robustness of systems is highly dependent on their most vulnerable input modality.

## 2 Related work

**Language models (LMs) and their robustness.** The seminal works of BERT , GPT-2 , and T5  laid the foundations of large LMs, upon which numerous other large LMs have been developed

Figure 1: **Image captioning task implemented by BLIP-2. Given an original text description (e.g., an armchair in the shape of an avocado), DALL-E  is used to generate corresponding clean images. BLIP-2 accurately returns captioning text (e.g., a stuffed chair in the shape of an avocado) that analogous to the original text description on the clean image. After the clean image is maliciously perturbed by targeted adversarial noises, the adversarial image can mislead BLIP-2 to return a caption (e.g., a pencil drawing of sports car is shown) that semantically resembles the predefined targeted response (e.g., a hand drawn sketch of a Porsche 911). More examples such as attacking real-world image-text pairs are provided in our Appendix.**

and demonstrated significant advancements across various language benchmarks [10; 19; 31; 74; 79; 107]. More recently, ChatGPT [57; 59] and several open-source models [18; 95; 83] tuned based on LLaMA  enable conversational interaction with human users and can respond to diverse and complex questions. Nevertheless, Alzantot et al.  first construct adversarial examples on sentiment analysis and textual entailment tasks, while Jin et al.  report that BERT can be evaded through natural language attacks. Later, various flexible (e.g., beyond word replacement) and semantically preserving methods are proposed to produce natural language adversarial examples [9; 49; 50; 52; 53; 70; 78; 102; 104; 110], as well as benchmarks and datasets to more thoroughly evaluate the adversarial robustness of LMs [90; 56; 91; 92]. There are also red-teaming initiatives that use human-in-the-loop or automated frameworks to identify problematic language model outputs [96; 27; 63].

**Vision-language models (VLMs) and their robustness.** The knowledge contained within these powerful LMs is used to facilitate vision-language tasks [93; 26; 101; 84; 26]. Inspired by the adversarial vulnerability observed in vision tasks, early efforts are devoted to investigating adversarial attacks against visual question answering [105; 105; 6; 11; 37; 38; 43; 77; 89], with the majority of these efforts focusing on conventional CNN-RNN-based models, assuming white-box access or untargeted adversarial goals, and requiring human interaction. Our research, on the other hand, examines the adversarial robustness of advanced large VLMs, assuming black-box access and targeted adversarial goals, and providing quantitative evaluations free of human labor.

## 3 Methodology

In this section, we will first introduce the fundamental preliminary, and then describe the transfer-based and query-based attacking strategies against image-grounded text generation, respectively.

### Preliminary

We denote \(p_{}(;_{})_{}\) as an image-grounded text generative model parameterized by \(\), where \(\) is the input image, \(_{}\) is the input text, and \(_{}\) is the output text. In image captioning tasks, for instance, \(_{}\) is a placeholder \(\) and \(_{}\) is the caption; in visual question answering tasks, \(_{}\) is the question and \(_{}\) is the answer. Note that here we slightly abuse the notations since the mapping between \(p_{}(;_{})\) and \(_{}\) could be probabilistic or non-deterministic [98; 5].

**Threat models.** We overview threat models that specify adversarial conditions  and adapt them to generative paradigms: (i) _adversary knowledge_ describes what knowledge the adversary is assumed

Figure 2: **Joint generation task implemented by UniDiffuser.** There are generative VLMs such as UniDiffuser that model the joint distribution of image-text pairs and are capable of both image-to-text and text-to-image generation. Consequently, given an original text description (e.g., A Van Gogh style painting of an American football player), the text-to-image direction of UniDiffuser is used to generate the corresponding clean image, and its image-to-text direction can recover a text response (e.g., A painting of Packers quarterback football player on a blue background) similar to the original text description. The recovering between image and text modalities can be performed consistently on clean images. When a targeted adversarial perturbation is added to a clean image, however, the image-to-text direction of UniDiffuser will return a text (e.g., A man in an astronaut suit riding a horse on the moon) that semantically resembles the predefined targeted description (e.g., A photo of an astronaut riding a horse on the moon), thereby affecting the subsequent chains of recovering processes.

to have, typically either white-box access with full knowledge of \(p_{}\) including model architecture and weights, or varying degrees of black-box access, e.g., only able to obtain the output text \(_{}\) from an API; (ii) _adversary goals_ describe the malicious purpose that the adversary seeks to achieve, including untargeted goals that simply cause \(_{}\) to be a wrong caption or answer, and targeted goals that cause \(_{}\) to match a predefined targeted response \(_{}\) (measured via text-matching metrics); (iii) _adversary capabilities_ describe the constraints on what the adversary can manipulate to cause harm, with the most commonly used constraint being imposed by the \(_{p}\) budget, namely, the \(_{p}\) distance between the clean image \(_{}\) and the adversarial image \(_{}\) is less than a budget \(\) as \(\|_{}-_{}\|_{p}\).

**Remark.** Our work investigates the most realistic and challenging threat model, where the adversary has black-box access to the victim models \(p_{}\), a targeted goal, a small perturbation budget \(\) on the input image \(\) to ensure human imperceptibility, and is forbidden to manipulate the input text \(_{}\).

Figure 3: **Visual question-answering (VQA) task implemented by MiniGPT-4. MiniGPT-4 has capabilities for vision-language understanding and performs comparably to GPT-4 on tasks such as multi-round VQA by leveraging the knowledge of large LMs. We select images with refined details generated by Midiourney  and feed questions (e.g., Can you tell me what is the interesting point of this image?) into MiniGPT-4. As expected, MiniGPT-4 can return descriptions that are intuitively reasonable, and when we ask additional questions (e.g., But is this a common scene in the normal life?), MiniGPT-4 demonstrates the capacity for accurate multi-round conversation. Nevertheless, after being fed targeted adversarial images, MiniGPT-4 will return answers related to the targeted description (e.g., A robot is playing in the field). This adversarial effect can even affect multi-round conversations when we ask additional questions. More examples of attacking MiniGPT-4 or LLaVA on VQA are provided in our Appendix.**

### Transfer-based attacking strategy

Since we assume black-box access to the _victim_ models, a common attacking strategy is transfer-based , which relies on _surrogate_ models (e.g., a publicly accessible CLIP model) to which the adversary has white-box access and crafts adversarial examples against them, then feeds the adversarial examples into the victim models (e.g., GPT-4 that the adversary seeks to fool). Due to the fact that the victim models are vision-and-language, we select an image encoder \(f_{}()\) and a text encoder \(g_{}()\) as surrogate models, and we denote \(_{}\) as the targeted response that the adversary expects the victim models to return. Two approaches of designing transfer-based adversarial objectives are described in the following.

**Matching image-text features (MF-it).** Since the adversary expects the victim models to return the targeted response \(_{}\) when the adversarial image \(_{}\) is the input, it is natural to match the features of \(_{}\) and \(_{}\) on surrogate models, where \(_{}\) should satisfy2

\[*{arg\,max}_{\|_{}-_{}\|_{p} }f_{}(_{})^{}g_{}(_{}).\] (1)

Here, we use blue color to highlight white-box accessibility (i.e., can directly obtain gradients of \(f_{}\) and \(g_{}\) through backpropagation), the image and text encoders are chosen to have the same output dimension, and their inner product indicates the cross-modality similarity of \(_{}\) and \(_{}\). The constrained optimization problem in Eq. (1) can be solved by projected gradient descent (PGD) .

**Matching image-image features (MF-ii).** While aligned image and text encoders have been shown to perform well on vision-language tasks , recent research suggests that VLMs may behave like bags-of-words  and therefore may not be dependable for optimizing cross-modality similarity. Given this, an alternative approach is to use a public text-to-image generative model \(h_{}\) (e.g., Stable

Figure 4: **Pipelines of our attacking strategies. In the _upper-left_ panel, we illustrate our transfer-based strategy for matching image-image features (MF-ii) as formulated in Eq. (2). We select a targeted text \(_{}\) (e.g., A sea otter with a pearl earring) and then use a pretrained text-to-image generator \(h_{}\) to produce a targeted image \(h_{}(_{})\). The targeted image is then fed to the image encoder \(f_{}\) to obtain the embedding \(f_{}(h_{}(_{}))\). Here we refer to adversarial examples generated by transfer-based strategies as \(_{}=_{}+\), while adversarial noise is denoted by \(\). We feed \(_{}\) into the image encoder to obtain the adversarial embedding \(f_{}(_{})\), and then we optimize the adversarial noise \(\) to maximize the similarity metric \(f_{}(_{})^{}f_{}(h_{}(_{}))\). In the _upper-right_ panel, we demonstrate our query-based strategy for matching text-text features (MF-it), as defined by Eq. (3). We apply the resulted transfer-based adversarial example \(_{}\) to initialize \(_{}\), then sample \(N\) random perturbations and add them to \(_{}\) to build \(\{_{}+_{n}\}_{n=1}^{N}\). These randomly perturbed adversarial examples are fed into the victim model \(p_{}\) (with the input text \(_{}\) unchanged) and the RGF method described in Eq. (4) is used to estimate the gradients \(_{_{}}g_{}(p_{}(_{};_{ }))^{}g_{}(_{})\). In the _bottom_, we present the final results of our method’s (MF-ii + MF-tt) targeted response generation.

Diffusion ) and generate a targeted image corresponding to \(_{}\) as \(h_{}(_{})\). Then, we match the image-image features of \(_{}\) and \(h_{}(_{})\) as

\[*{arg\,max}_{\|_{}-_{}\|_{p} }f_{}(_{})^{}f_{}(h_{}(_{})),\] (2)

where orange color is used to emphasize that only black-box accessibility is required for \(h_{}\), as gradient information of \(h_{}\) is not required when optimizing the adversarial image \(_{}\). Consequently, we can also implement \(h_{}\) using advanced APIs such as Midjourney .

### Query-based attacking strategy

Transfer-based attacks are effective, but their efficacy is heavily dependent on the similarity between the victim and surrogate models. When we are allowed to repeatedly query victim models, such as by providing image inputs and obtaining text outputs, the adversary can employ a query-based attacking strategy to estimate gradients or execute natural evolution algorithms [7; 16; 34].

**Matching text-text features (MF-tt).** Recall that the adversary goal is to cause the victim models to return a targeted response, namely, matching \(p_{}(_{};_{})\) with \(_{}\). Thus, it is straightforward to maximize the textual similarity between \(p_{}(_{};_{})\) and \(_{}\) as

\[*{arg\,max}_{\|_{}-_{}\|_{p} }g_{}(p_{}(_{};_{}))^{}g _{}(_{}).\] (3)

Note that we cannot directly compute gradients for optimization in Eq. (3) because we assume black-box access to the victim models \(p_{}\) and cannot perform backpropagation. To estimate the gradients, we employ the random gradient-free (RGF) method . First, we rewrite a gradient as the expectation of direction derivatives, i.e., \(_{}F()=[^{}_{}F( )]\), where \(F()\) represents any differentiable function and \( P()\) is a random variable satisfying that \([^{}]=\) (e.g., \(\) can be uniformly sampled from a hypersphere). Then by zero-order optimization , we know that

\[_{_{}}g_{}(p_{}(_{};_{}))^{}g_{}(_{})\] (4) \[ _{n=1}^{N}[g_{}(p_{}(_ {}+_{n};_{}))^{}g_{}( _{})-g_{}(p_{}(_{};_{}))^{ }g_{}(_{})]_{n},\]

where \(_{n} P()\), \(\) is a hyperparameter controls the sampling variance, and \(N\) is the number of queries. The approximation in Eq. (4) becomes an unbiased equation when \( 0\) and \(N\).

**Remark.** Previous research demonstrates that transfer-based and query-based attacking strategies can work in tandem to improve black-box evasion effectiveness [17; 24]. In light of this, we also consider

    &  &  & _{}\)} \\  & \(_{}\) & \(h_{}(_{})\) & MF-ii & MF-it & MF-ii & MF-it \\  CLIP (RN50)  & 0.094 & 0.261 & 0.239 & **0.576** & 0.543 & 0.532 \\ CLIP (ViT-B/32)  & 0.142 & 0.313 & 0.302 & **0.570** & 0.592 & 0.588 \\ BLIP (ViT)  & 0.138 & 0.286 & 0.277 & **0.679** & 0.641 & 0.634 \\ BLIP-2 (ViT)  & 0.037 & 0.302 & 0.294 & **0.502** & 0.855 & 0.852 \\ ALBEF (ViT)  & 0.063 & 0.098 & 0.091 & **0.451** & 0.750 & 0.749 \\   

Table 1: **White-box attacks against surrogate models.** We craft adversarial images \(_{}\) using MF-it in Eq. (1) or MF-ii in Eq. (2), and report the CLIP score (\(\)) between the images and the predefined targeted text \(_{}\) (randomly chosen sentences). Here the clean images consist of real-world \(_{}\) that is irrelevant to the chosen targeted text and \(h_{}(_{})\) generated by a text-to-image model (e.g., Stable Diffusion ) conditioned on the targeted text \(_{}\). We observe that MF-ii induces a similar CLIP score compared to the generated image \(h_{}(_{})\), while MF-it induces a even higher CLIP score by directly matching cross-modality features. Furthermore, we note that the attack is time-efficient, and we provide the average time (in seconds) for each strategy to craft a single \(_{}\). The results in this table validate the effectiveness of white-box attacks against surrogate models, whereas Table 2 investigates the transferability of crafted \(_{}\) to evade large VLMs (e.g., MiniGPT-4).

the adversarial examples generated by transfer-based methods to be an initialization (or prior-guided) and use the information obtained from query-based methods to strengthen the adversarial effects. This combination is effective, as empirically verified in Sec. 4 and intuitively illustrated in Figure 4.

## 4 Experiment

In this section, we demonstrate the effectiveness of our techniques for crafting adversarial examples against open-source, large VLMs. More results are provided in the Appendix.

### Implementation details

In this paper, we evaluate open-source (to ensure reproducibility) and advanced large VLMs, such as **UniDiffuser**, which uses a diffusion-based framework to jointly model the distribution of image-text pairs and can perform both image-to-text and text-to-image generation; **BLIP** is a unified vision-language pretraining framework for learning from noisy image-text pairs; **BLIP-2** adds a querying transformer  and a large LM (T5 ) to improve the image-grounded text generation; **Img2Prompt** proposes a plug-and-play, LM-agnostic module that provides large

    &  &  &  \\  & & RN50 & RN101 & ViT-B/16 & ViT-B/32 & ViT-L/14 & Ensemble & \# Param. Res. \\   & Clean image & 0.472 & 0.456 & 0.479 & 0.499 & 0.344 & 0.450 &  &  \\  & MF-it & 0.492 & 0.474 & 0.520 & 0.546 & 0.384 & 0.483 & & \\  & MF-ii & 0.766 & 0.753 & 0.774 & 0.786 & 0.696 & 0.755 & & \\  & MF-ii + MF-it & **0.855** & **0.841** & **0.861** & **0.868** & **0.803** & **0.846** & & \\   & Clean image & 0.417 & 0.415 & 0.429 & 0.446 & 0.305 & 0.402 &  &  \\  & MF-it & 0.655 & 0.639 & 0.678 & 0.698 & 0.611 & 0.656 & & \\  & MF-ii & 0.709 & 0.695 & 0.721 & 0.733 & 0.637 & 0.700 & & \\  & MF-ii + MF-it & **0.754** & **0.736** & **0.761** & **0.777** & **0.689** & **0.743** & & \\   & Clean image & 0.487 & 0.464 & 0.493 & 0.515 & 0.350 & 0.461 &  &  \\  & MF-it & 0.499 & 0.472 & 0.501 & 0.525 & 0.355 & 0.470 & & \\  & MF-ii & 0.502 & 0.479 & 0.505 & 0.529 & 0.366 & 0.476 & & \\  & MF-ii + MF-it & **0.803** & **0.783** & **0.809** & **0.828** & **0.733** & **0.791** & & \\   & Clean image & 0.473 & 0.454 & 0.483 & 0.503 & 0.349 & 0.452 &  &  \\  & MF-it & 0.492 & 0.474 & 0.520 & 0.546 & 0.384 & 0.483 & & \\  & MF-ii & 0.562 & 0.541 & 0.571 & 0.592 & 0.449 & 0.543 & & \\  & MF-ii + MF-it & **0.656** & **0.633** & **0.665** & **0.681** & **0.555** & **0.638** & & \\   & Clean image & 0.383 & 0.436 & 0.402 & 0.437 & 0.281 & 0.388 &  &  \\  & MF-it & 0.389 & 0.441 & 0.417 & 0.452 & 0.288 & 0.397 & & \\  & MF-ii & 0.396 & 0.440 & 0.421 & 0.450 & 0.292 & 0.400 & & \\  & MF-ii + MF-it & **0.548** & **0.559** & **0.563** & **0.590** & **0.448** & **0.542** & & \\   & Clean image & 0.422 & 0.431 & 0.436 & 0.470 & 0.326 & 0.417 &  &  \\  & MF-it & 0.472 & 0.450 & 0.461 & 0.484 & 0.349 & 0.443 & \\   & MF-ii & 0.525 & 0.541 & 0.542 & 0.572 & 0.430 & 0.522 & & \\   & MF-ii + MF-it & **0.633** & **0.611** & **0.631** & **0.668** & **0.528** & **0.614** & & \\   

Table 2: **Black-box attacks against victim models.** We sample clean images \(_{}\) from the ImageNet-1K validation set and randomly select a target text \(_{}\) from MS-COCO captions for each clean image. We report the CLIP score (\(\)) between the generated responses of input images (i.e., clean images \(_{}\) or \(_{}\) crafted by our attacking methods MF-it, MF-ii, and the combination of MF-ii + MF-it) and predefined targeted texts \(_{}\), as computed by various CLIP text encoders and their ensemble/average. The default textual input \(_{}\) is fixed to be “what is the content of this image?”. Pretrained image/text encoders such as CLIP are used as surrogate models for MF-it and MF-ii. For reference, we also report other information such as the number of parameters and input resolution of victim models.

LM prompts to enable zero-shot VQA tasks; **MiniGPT-4** and **LLaVA** have recently scaled up the capacity of large LMs and leveraged Vicuna-13B  for image-grounded text generation tasks. We note that MiniGPT-4 also exploits a high-quality, well-aligned dataset to further finetune the model with a conversation template, resulting in performance comparable to GPT-4 .

**Datasets.** We use the validation images from ImageNet-1K  as clean images, from which adversarial examples are crafted, to quantitatively evaluate the adversarial robustness of large VLMs. From MS-COCO captions , we randomly select a text description (usually a complete sentence, as shown in our Appendix) as the adversarially targeted text for each clean image. Because we cannot easily find a corresponding image of a given, predefined text, we use Stable Diffusion  for the text-to-image generation to obtain the targeted images of each text description, in order to simulate the real-world scenario. Midjourney  and DALL-E [67; 68] are also used in our experiments to generate the targeted images for demonstration.

**Basic setups.** For fair comparison, we strictly adhere to previous works [109; 41; 42; 5; 30; 46] in the selection of pretrained weights for image-grounded text generation, including large LMs (e.g., T5  and Vicuna-13B  checkpoints). We experiment on the original clean images of various resolutions (see Table 2). We set \(=8\) and use \(_{}\) constraint by default as \(\|_{}-_{}\|_{}\)8, which is the most commonly used setting in the adversarial literature , to ensure that the adversarial perturbations are visually imperceptible where the pixel values are in the range \(\). We use 100-step PGD to optimize transfer-based attacks (the objectives in Eq. (1) and Eq. (2)). In each step of query-based attacks, we set query times \(N=100\) in Eq. (4) and update the adversarial images by 8-steps PGD using the estimated gradient. Every experiment is run on a single NVIDIA-A100 GPU.

### Empirical studies

We evaluate large VLMs and freeze their parameters to make them act like image-to-text generative APIs. In particular, in Figure 1, we show that our crafted adversarial image consistently deceives BLIP-2 and that the generated response has the same semantics as the targeted text. In Figure 2, we

Figure 5: Adversarial perturbations \(\) are obtained by computing \(_{}-_{}\) (pixel values are amplified \( 10\) for visualization) and their corresponding captions are generated below. Here DALL-E acts as \(h_{}\) to generate targeted images \(h_{}(_{})\) for reference. We note that adversarial perturbations are not only visually hard to perceive, but also not detectable using state-of-the-art image captioning models (we use UniDiffuser for captioning, while similar conclusions hold when using other models).

Figure 6: We experiment with different values of \(\) in Eq. (3) to obtain different levels of \(_{}\). As seen, the quality of \(_{}\) degrades (measured by the LPIPS distance between \(_{}\) and \(_{}\)), while the effect of targeted response generation saturates (in this case, we evaluate UniDiffuser). Thus, a proper perturbation budget (e.g., \(=8\)) is necessary to balance image quality and generation performance.

evaluate UniDiffuser, which is capable of bidirectional joint generation, to generate text-to-image and then image-to-text using the crafted \(_{}\). It should be noted that such a chain of generation will result in completely different content than the original text description. We simply use "what is the content of this image?" as the prompt to answer generation for models that require text instructions as input (query) . However, for MiniGPT-4, we use a more flexible approach in conversation, as shown in Figure 3. In contrast to the clean images on which MiniGPT-4 has concrete and correct understanding and descriptions, our crafted adversarial counterparts mislead MiniGPT-4 into producing targeted responses and creating more unexpected descriptions that are not shown in the targeted text.

In Table 1, we examines the effectiveness of MF-it and MF-ii in crafting white-box adversarial images against surrogate models such as CLIP , BLIP  and ALBEF . We take 50K clean images \(_{}\) from the ImageNet-1K validation set and randomly select a targeted text \(_{}\) from MS-COCO captions for each clean image. We also generate targeted images \(h_{}(_{})\) as reference and craft adversarial images \(_{}\) by MF-ii or MF-it. As observed, both MF-ii and MF-it are able to increase the similarity between the adversarial image and the targeted text (as measured by CLIP score) in the white-box setting, laying the foundation for black-box transferability. Specifically, as seen in Table 2, we first transfer the adversarial examples crafted by MF-ii or MF-it in order to evade large VLMs and mislead them into generating targeted responses. We calculate the similarity between the generated response \(p_{}(_{};_{})\) and the targeted text \(_{}\) using various types of CLIP text encoders. As mentioned previously, the default textual input \(_{}\) is fixed to be "what is the content of this image?". Surprisingly, we find that MF-it performs worse than MF-ii, which suggests overfitting when optimizing directly on the cross-modality similarity. In addition, when we use the transfer-based adversarial image crafted by MF-ii as an initialization and then apply query-based MF-tt to tune the adversarial image, the generated response becomes significantly more similar to the targeted text, indicating the vulnerability of advanced large VLMs.

### Further analyses

Does VLM adversarial perturbations induce semantic meanings?Previous research has demonstrated that adversarial perturbations crafted against robust models will exhibit semantic or perceptually-aligned characteristics [82; 35; 60]. This motivates us to figure out whether adversarial perturbations \(=_{}-_{}\) crafted against large VLMs possess a similar level of semantic information. In Figure 5, we visualize \(\) that results in a successful targeted evasion over a real image and report the generated text responses. Nevertheless, we observe no semantic information associated with the targeted text in adversarial perturbations or their captions, indicating that large VLMs are inherently vulnerable.

The influence of perturbation budget \(\).We use \(=8\) as the default value in our experiments, meaning that the pixel-wise perturbation is up to \( 8\) in the range \(\). In Figure 6, we examine the effect of setting \(\) to different values of \(\{2,4,8,16,64\}\) and compute the perceptual distance between the clean image \(_{}\) and its adversarial counterpart \(_{}\) using LPIPS (\(\)) . We highlight (in red color) the generated responses that most closely resemble the targeted text. As observed, there is a trade-off between image quality/fidelity and successfully eliciting the targeted response; therefore, it is essential to choose an appropriate perturbation budget value.

Figure 7: **Performance of our attack method under a fixed perturbation budget \(=8\). We interpolate between the sole use of transfer-based attack and the sole use of query-based attack strategy. We demonstrate the effectiveness of our method via CLIP score (\(\)) between the generated texts on adversarial images and the target texts, with different types of CLIP text encoders. The \(x\)-axis in a “\(_{}\)-\(\)c\({}_{q}\)” format denotes we assign \(_{t}\) to transfer-based attack and \(_{q}\) to query-based attack. “t+q=8” indicates we use transfer-based attack (\(_{t}=8\)) as initialization, and conduct query-based attack for further 8 steps (\(_{q}=8\)), such that the resulting perturbation satisfies \(=8\). As a result, We show that a proper combination of transfer/query based attack strategy achieves the best performance.**

**Performance of attack with a fixed perturbation budget.** To understand the separate benefit from transfer-based attack and query-based attack, we conduct a study to assign different perturbation budget for transfer (\(_{t}\)) and query based attack strategy (\(_{q}\)), under the constraint \(_{t}+_{q}=8\). Unidiffuser is the victim model in our experiment. The results are in Figure 7. We demonstrate that, a proper combination of transfer and query based attack achieves the best performance.

**Interpreting the mechanism of attacking large VLMs.** To understand how our targeted adversarial example influences response generation, we compute the relevancy score of image patches related to the input question using GradCAM  to obtain a visual explanation for both clean and adversarial images. As shown in Figure 8, our adversarial image \(_{}\) successfully suppresses the relevancy to the original text description (panel **(b)**) and mimics the attention map of the targeted image \(h_{}(_{})\) (panel **(c)**). Nonetheless, we emphasize that the use of GradCAM as a feature attribution method has some known limitations . Additional interpretable examples are provided in the Appendix.

## 5 Discussion

It is widely accepted that developing large multimodal models will be an irresistible trend. Prior to deploying these large models in practice, however, it is essential to understand their worst-case performance through techniques such as red teaming or adversarial attacks . In contrast to manipulating textual inputs, which may require human-in-the-loop prompt engineering, our results demonstrate that manipulating visual inputs can be automated, thereby effectively fooling the entire large vision-language systems. The resulting adversarial effect is deeply rooted and can even affect multi-round interaction, as shown in Figure 3. While multimodal security issues have been cautiously treated by models such as GPT-4, which delays the release of visual inputs , there are an increasing number of open-source multimodal models, such as MiniGPT-4  and LLAVA [46; 45], whose worst-case behaviors have not been thoroughly examined. The use of these open-source, but adversarially unchecked, large multimodal models as product plugins could pose potential risks.

**Broader impacts.** While the primary goal of our research is to evaluate and quantify adversarial robustness of large vision-language models, it is possible that the developed attacking strategies could be misused to evade practically deployed systems and cause potential negative societal impacts. Specifically, our threat model assumes black-box access and targeted responses, which involves manipulating existing APIs such as GPT-4 (with visual inputs) and/or Midjourney on purpose, thereby increasing the risk if these vision-language APIs are implemented as plugins in other products.

**Limitations.** Our work focuses primarily on the digital world, with the assumption that input images feed directly into the models. In the future, however, vision-language models are more likely to be deployed in complex scenarios such as controlling robots or automatic driving, in which case input images may be obtained from the interaction with physical environments and captured in real-time by cameras. Consequently, performing adversarial attacks in the physical world would be one of the future directions for evaluating the security of vision-language models.