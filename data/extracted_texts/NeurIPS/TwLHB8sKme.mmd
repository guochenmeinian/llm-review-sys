# Tools for Verifying Neural Models' Training Data

Dami Choi

U. Toronto & Vector Institute

choidami@cs.toronto.edu

&Yonadav Shavit

Harvard University

yonadav@g.harvard.edu

&David Duvenaud

U. Toronto & Vector Institute

duvenaud@cs.toronto.edu

Equal contribution

###### Abstract

It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a "Proof-of-Training-Data": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature.

## 1 Introduction

How can we verify the capabilities of large machine learning models? Today, such claims are based on trust and reputation: customers and regulators believe that well-known companies building AI models wouldn't lie about the training data used in their models. However, as the ability to build new AI models proliferates, users need to trust an ever-larger array of model providers at their word, and regulators may increasingly face malicious AI developers who may lie to appear compliant with standards and regulations. Worse, countries developing militarily-significant AI systems may not trust each others' claims about these systems' capabilities, making it hard to coordinate on limits.

AI developers can enable greater trust by having a third party verify the developer's claims about their system, much as the iOS App Store checks apps for malicious code. Current black-box approaches to model auditing allow some probing of capabilities , but these audits' utility is limited and a model's capabilities can be hidden . An auditor can more effectively target their examination if they also know the model's training data, including the total quantity, inclusion of data likely to enable specific harmful capabilities (such as texts on cyber-exploit generation), and inclusion of safety-enhancing data (such as instruction-tuning ). However, if auditors rely on AI developers to self-report the data, developers could falsify their reports. This uncertainty limits the trust such audits can create.

In this work, we define the problem of Proof-of-Training-Data (PoTD): a protocol by which a third-party auditor (the "Verifier") can verify which data was used by the developer (the "Prover") to train a model. Our verification procedures assume that the Verifier can be given access to sensitive information and IP (e.g., training data, model weights) and is trusted to keep it secure; we leave the additional challenge of simultaneously preserving the confidentiality of the training data and model weights to future work. In principle, one could solve PoTD by cryptographically attesting to theresults of training on a dataset using delegated computation . However, in practice such delegation methods are impractically slow, forcing us to turn to heuristic verification approaches.

Inspired by the related literature on "Proof-of-Learning" (PoL), we propose that model-trainers disclose a _training transcript_ to the Verifier, including training data, training code, and intermediate checkpoints. In Section 4, we provide several verification strategies for a Verifier to confirm a training transcript's authenticity, including new methods that address all published attacks in the Proof-of-Learning literature. We demonstrate the practical effectiveness of our defenses via experiments on two language models (Section 6). Our methods can be run cheaply, adding as little as 1.3% of the original training run's compute. Further, we require no change to the training pipeline other than fixing the data ordering and initialization seeds, and storing the training process seeds for reproducibility. Still, like PoL, they sometimes require re-running a small fraction of training steps to produce strong guarantees.

The verification strategies we describe are not provably robust, but are intended as an opening proposal which we hope motivates further work in the ML security community to investigate new attacks and defenses that eventually build public confidence in the training data used to build advanced machine learning models.

## 2 Related Work

We build on , which sketches a larger framework for verifying rules on large-scale ML training. It defines, but does not solve, the "Proof-of-Training-Transcript" problem, a similar problem to Proof-of-Training-Data that additionally requires verifying hyperparameters.

**Proof-of-Learning.** introduce the problem of Proof-of-Learning (PoL), in which a Verifier checks a Prover's ownership/copyright claim over a set of model weights by requiring the Prover to prove that they did _at least as much computational work_ as was required to have trained the original model. The Prover's proof consists of reporting a sequence of valid weight checkpoints that led to the final weights. The Verifier then re-executes training between a few checkpoints and checks that the results are similar to confirm (or reject) the report's correctness. Unlike PoL, Proof-of-Training-Data requires proving _what exact data_ was used to produce a model. This is strictly harder: any valid PoTD protocol can serve as a solution to PoL, as compute cost can be derived from the size of the dataset. Furthermore, PoL only requires robustness to adversaries that use less computation than the original training run, whereas PoTD targets all computationally-feasible adversaries.

Several works have successfully attacked the original PoL scheme; our proposed defenses defeat all published attacks.  and  provide methods for forging fake data that will (during retraining) interpolate between chosen checkpoints, thus allowing the production of fake transcripts. (See Section 4.3 for our mitigation.)  exploit the fact that, due to the prohibitive cost of retraining, the Verifier can only retrain a tiny fraction of checkpoints. By falsifying only a modest fraction of checkpoints, a Prover can prevent their fake transcript being caught with high probability. (See Section 4.2 for our mitigation, which introduces a method for verifying checkpoints that is much cheaper than retraining, and can thus be applied to _all_ checkpoints.)

**Memorization during training.** introduce the notion of counterfactual memorization (the average difference in model performance with and without including a specific point in training) that is most similar to our own, and use it to investigate different training points' effects on final model performance.  examine which datapoints are most strongly memorized during training by using influence functions, but they focus on the degree of memorization only at the end of training.  show that per-datapoint memorization of text (as measured by top-1 recall) can be somewhat reliably predicted based on the degree of memorization earlier in training.  analyze pointwise loss trajectories throughout training, but do not focus specifically on the phenomenon of overfitting to points in the training set.

## 3 Formal Problem Definition

In the Proof-of-Training-Data problem, a Prover trains an ML model and wants to prove to a Verifier that the resulting target model weights \(W^{*}\) are the result of training on data \(D^{*}\). If a malicious Prover used training data that is against the Verifier's rules (e.g., terms of service, regulatory rules) then that Prover would prefer to hide \(D^{*}\) from the Verifier. To appear compliant, the Prover will instead lie and claim to the Verifier that they have used some alternative dataset \(D D^{*}\). However, the Prover will only risk this lie if they believe that with high probability they will not get caught (making them a "covert adversary" ). The goal of a Proof-of-Training-Data protocol is to provide a series of Verifier tests that the Prover would pass with high probability if and only if they truthfully reported the true dataset that was used to yield the model \(W^{*}\).

Let \(D^{n}\) be an ordered training dataset. Let \(M\) contain all the hyperparameters needed to reproduce the training process, including the choice of model, optimizer, loss function, random seeds, and possibly details of the software/hardware configuration to maximize reproducibility.

**Definition 1**.: _A valid Proof-of-Training-Data protocol consists of a Prover protocol \(\), Verifier protocol \(\), and witnessing template \(\) that achieves the following. Given a dataset \(D^{*}\) and hyperparameters \(M^{*}\), an honest Prover uses \(\) to execute a training run and get \((W^{*},J^{*})=(D^{*},M^{*},c_{1})\), where \(W^{*}^{d}\) is a final weight vector, \(J^{*}\) is a witness to the computation, and \(c_{1} C_{1}\) is an irreducible source of noise. The Verifier must accept this true witness and resulting set of model weights with high probability: \(_{c_{1} C_{1},c_{2} C_{2}}[(D^{*},M^{*},J^{*},W^{*},c_{ 2})=1] 1-_{1}\), where \(_{1} 1/2\) and \(c_{2}\) is the randomness used by the Verifier._

_Conversely, \(\) computationally-feasible probabilistic adversaries \(\) which produce spoofs \((D,M,J)=(D^{*},M^{*},J^{*},W^{*},c_{3})\) where \(D D^{*}\) and \(c_{3} C_{3}\) is the randomness used by the adversary, the Verifier must reject all such spoofs with high probability: \(_{c_{1} C_{1},c_{2} C_{2},c_{3} C_{3}}[(D,M,J,W^{*}, c_{2})=0] 1-_{2}\) where \(_{2} 1/2\)._

Following the literature on the related Proof-of-Learning problem , we use as a witness the series of \(m\) model weight checkpoints \(J^{*}==(W_{0},W_{1},,W_{m-1},W^{*})\). Model weight checkpoints are already routinely saved throughout large training runs; we assume a checkpoint is saved after training on each \(k=n/m\)-datapoint segment. During verification, the Prover provides the Verifier with the _training transcript_\(T=\{D,M,\}\), which the Verifier will then test to check its truthfulness.

In practice, we cannot yet pursue provable robustness to _all_ probabilistic adversaries \(\), due to the nascent state of the theoretical foundations of deep learning . Instead, as is done in the PoL literature, we approximate robustness to \(\) by proposing a range of adversaries and then showing a protocol that defeats them. In particular, we will consider two major types of attacks (and their combinations):

* _Non-Uniqueness Attack_: The Prover forges a different \(D D^{*}\) and \(M\) (and corresponding checkpoints \(\)) that would, if training was reexecuted, also lead to \(W^{*}\). In general, it is easy to produce such transcripts (e.g., initialize training at \(W_{0}=W^{*}\) and take no steps). In Section 4.3 we propose constraints on \(\) that block such attacks.
* _Data Subtraction Attack_: The Prover reports training on \(D\), but secretly only trains on a subset \(D^{*} D\).
* _Data Addition Attack_: The Prover reports training on \(D\), but secretly trains on an additional set of data \(D^{*} D\).
* _Checkpoint Gue-ing Attack_: The Prover reports a pair of successive checkpoints \((W_{i},W_{i+1})\), but \(W_{i+1}\) was not produced from \(W_{i}\) by a training procedure at all.

We address the latter three attacks in Section 6.

As a brute-force solution to Proof-of-Training-Data, the Verifier could simply re-execute the complete training process defined by \(T\), and check that the result matches \(W^{*}\). However, beyond technical complications2, doing so is far too computationally expensive to be done often; a government Verifier would need to be spending as much on compute for audits as every AI developer combined. Therefore any verification protocol \(\) must also be _efficient_, costing much less than the original training run. Inevitably, such efficiency makes it near certain that the Verifier will fail to catch spoofs \(D D^{*}\) if \(D\) only differs in a few data points; in practice, we prioritize catching spoofs which deviate on a substantial fraction of points in \(D^{*}\). Though we do not restrict to a particular definition of dataset deviations, we list several possibilities relevant for different Verifier objectives in Appendix D.

## 4 Verification Strategies

We provide several complementary tools for detecting whether a transcript \(T\) is spoofed. Combined, these methods address many different types of attacks, including all current attacks from the PoL literature .

### Existing Tools from Proof-of-Learning

Our protocol will include several existing spoof-detection tools from the Proof-of-Learning literature , such as looking for outliers in the trajectory of validation loss throughout training, and plotting the segment-wise weight-change \(\|W_{i}-W_{i-1}\|_{2}\) between the checkpoints \(\). The most important of these existing tools is the segment-wise retraining protocol of . Let \(R(W_{i-1},_{i},c;M)\) be the model training operator that takes in a weight checkpoint \(W_{i-1}\), updates it with a series of gradient steps based on training data sequence \(_{i}\) (describing the order in which the Prover claims data points were used in training between checkpoints \(W_{i-1}\) and \(W_{i}\), which may be different from the order of the dataset \(D^{*}\)), hyperparameters \(M\), and hardware-noise-randomness \(c C\), and then outputs the resulting weight checkpoint \(W_{i}\). Transcript segment \(i\) is \((,)\)_-reproducible_ if for the pair of checkpoints \((W_{i-1},W_{i})\) in \(\), the reproduction error (normalized by the overall segment displacement) is small:

\[_{c C}(_{i}-W_{i}\|_{2}}{_{i}-W_{i-1 }\|_{2}+\|W_{i}-W_{i-1}\|_{2}}{2}}<)>1- _{i}=R(W_{i-1},_{i},c;M).\] (1)

The values \(\) and \(\) trade off false-positive vs. false-negative rates; see  for discussion. The Verifier can use this retraining procedure as a ground-truth for verifying the faithfulness of a suspicious training segment. However, this test is computationally-intensive, and can thus only be done for a small subset of training segments. Our other verification strategies described in Sections 4.2 and 4.3 will be efficient enough to be executable on every training segment.

### Memorization-Based Tests

The simplest way for a Prover to construct a spoofed transcript ending in \(W^{*}\) is to simply make up checkpoints rather than training on \(D^{*}\), and hope that the Verifier lacks the budget to retrain a sufficient number of checkpoints to catch these spoofed checkpoints. To address this, we demonstrate a heuristic for catching spoofed checkpoints using a small amount of data, based on what is to the best of our knowledge a previously-undocumented phenomenon about local training data memorization.

Machine learning methods notoriously overfit to their training data \(D\), relative to their validation data \(D_{v}\). We can quantify the degree of overfitting to a single data point \(d\) on a loss metric \(:^{|W|}\) relative to a validation set \(D_{v}\) via a simple memorization heuristic \(\):

\[(d,W)=_{d^{} D_{v}}[(d^{},W)]- (d,W).\] (2)

Recall that \(_{i}\) is the sequence of data points corresponding to the \(i\)th segment of the training run. One would expect that in checkpoints before data segment \(i\), for data points \(d_{i}\), memorization \((d,W_{j<i})\) would in expectation be similar to the validation-set memorization; after data-segment \(i\), one would expect to see higher degrees of overfitting and therefore \((d,W_{j i})\) would be substantially higher. We find evidence for this effect in experiments on GPT-2-Small  and the Pythia suite ). As shown in Figures 1 and 2, when a Prover reports the true training data, on average the greatest memorization occurs where \(_{i}\) and \(W_{j=i}\) match. We corroborate this finding with additional experiments on a range of models in Appendix G. The finding is even clearer if we look at jumps in memorization level, which we call the Memorization Delta \(_{}\):

\[_{}(d,i;,D_{v},)=(d,W_{i})- (d,W_{i-1}).\] (3)

To test whether each reported checkpoint \(W_{i}\) resulted from training on at least some of the segment training data \(_{i}\), a Verifier can compute a memorization plot like the one shown in Figure 1. Suchplots can be computed more efficiently by sampling only a small fraction \(\) of the training data \(\), and by plotting only a few checkpoints \(W_{i-},,W_{i+}\) for each segment \(_{i}\).

We can further harness this memorization phenomenon to test whether on segment \(i\), rather than training on the full claimed data sequence \(_{i}\) and yielding \(W_{i}\), the Prover secretly skipped training on _at least_ a \(\)-fraction of the points in \(_{i}\) and yielded \(W_{i}^{}\). Consider the odds that, for \(d_{i}\), \(_{}(d,W_{i})\) happens to fall in the bottom \(p\)-probability quantile of the validation set \(D_{v}\)'s \(_{}\) values on \(W_{i}\):

\[(d,p,W_{i}) =(_{d^{} D_{v}}( _{}(d^{},W_{i})>_{}(d,W_{i})) p)\] (4) \[(,p,W_{i}) =_{d}[(d,p,W_{i})],\] (5)

where \(\) is the indicator function, PBQ stands for "Point is Below Quantile", and FBQ stands for "Fraction Below Quantile". We can see in Figure 3 that, as expected, when the points in \(_{i}\) are all included in training, \((_{i},p,W_{i})\) is small compared to \((_{j i},p,W_{i})\)). If many points were secretly excluded, as in \(W_{i}^{}\), we should expect that \((_{i},p,W_{i}^{})\) should be higher and closer to \((D_{v},p,W_{i}^{})\), where \(D_{v}\) is the validation set. If the Prover secretly excluded a greater than \(\)-fraction of data points in \(_{i}\) thus yielding \(W_{i}^{}\), then we should expect that:

\[(_{i},p,W_{i}^{})=(1-)(_{i},p,W_{i})+ (D_{v},p,W_{i}^{})(D_{v},p,W_{ i}^{})\] (6)

(see derivation in Appendix B). Rearranging terms, we get

\[(_{i},p,W_{i}^{}):=(_{i},p,W_{i}^{}) }{(D_{v},p,W_{i}^{})}.\] (7)

\((_{i},p,W_{i}^{})\) can be estimated using a small fraction of training and validation datapoints, and can serve as an upper-bound estimate on \(\), the fraction of \(_{i}\) secretly excluded from training \(W_{i}^{}\).3 In

Figure 1: Plots from a GPT-2 experiment, demonstrating the local memorization effect. The maximum score for each data segment (row) is marked with a red box. The largest average memorization for data sequence \(_{i}\) occurs at the immediately-subsequent checkpoint \(W_{i}\). From left to right: plots of the average loss \(\); memorization \(\); and memorization-delta \(_{}\); along with the average memorization over time for each segment \(_{i}\), recentered such that \(W_{i}\) is at \(x=0\).

Figure 2: Plots of the memorization \(\) on other types of training runs, similar to Figure 1. For efficiency, plots of the Pythia models use only \(10\%\) of training data, and look only at a window of checkpoints around \(W_{i}\). From left to right: \(\) for checkpoints near the middle of the Pythia (70M) training run shows the same pattern as GPT-2; checkpoints near the middle of the Pythia (1B) training run show that the phenomenon gets clearer as the model size increases; \(\) for a GPT-2 run with two epochs over the same data (with random data order each epoch; the first epoch ends at checkpoint 9) to demonstrate that the effect is present over multiple epochs; \(\) for a GPT-2 run using a random data order other than \(\) shows that the effect is tied to the training data sequence itself.

Section 6 we show that this heuristic can detect even small data subtractions in practice, and in Appendix H.2 we show the test's effectiveness across a range of percentiles \(p\) and segment-lengths \(k\).

We also observe that \(\) gradually decreases over time from an initial peak immediately after the point's training segment. This echoes the many findings on "forgetting" in deep learning . We show in Section 6 how this can be used to catch gluing attacks.

### Fixing the Initialization and Data Order

As mentioned in Section 3, a Proof-of-Training-Data protocol needs to defend against non-uniqueness attacks, by making it difficult for a malicious Prover to produce a second transcript with \(D D^{*}\) that, if training was legitimately executed, would _also_ end in \(W^{*}\). There are two well-known types of attacks the Prover might use to efficiently produce such spoofs:

* _Initialization attacks_: An attacker can choose a "random" initialization that places \(W_{0}\) in a convenient position, such as close to the target \(W^{*}\). Even if the Verifier uses statistical checks to confirm that the initialization appears random, these are sufficiently loose that an adversary can still exploit the choice of initialization .
* _Synthetic data/data reordering attacks_: Given the current weight vector \(W_{i}\), an attacker can synthesize a batch of training datapoints such that the resulting gradient update moves in a direction of the attacker's choosing, such as towards \(W^{*}\). This could be done through the addition of adversarial noise to existing data points , generating a new dataset , or by carefully reordering existing data points in a "reordering attack" .

We propose methods for preventing both of these attacks by forcing the Prover to use a certified-random weight initialization, and a certified-random data ordering. The randomized data ordering guarantees that the adversarial Prover cannot construct synthetic datapoints that induce a particular gradient, because it does not know the corresponding weights \(W\) at the time of choosing the datapoints \(D\). Given a fixed data ordering, we discuss in Appendix E why it may be super-polynomially hard to find a certified-random weight initialization that, when fully trained, results in a particular \(W^{*}\).

The Verifier can produce this guaranteed-random initialization and data order by requiring the Prover to use a particular random seed \(s\), _constructed as a function of the dataset \(D\) itself_. This produces the initialization \(W_{0}=G_{r}(s)^{n}\) and data ordering \(S=G_{p}(s)\) using a publicly known pseudorandom generators \(G_{r}\) and \(G_{p}\).45 The Prover can also construct a verifiable validation subset \(D_{v}\) by holding

Figure 3: Exploring the pointwise memorization effect on GPT-2. From left to right: For each \(W_{i}\) and \(_{i}\), we plot the fraction of points with \(_{}\) above the median \(1-(_{i},0.5,W_{i})\), and see that in the diagonal segments, most individual points are above the median. This shows that memorization occurs pointwise, suggesting that it can be detected via sparse random sampling. The highest segment in each row is surrounded by a red box; Plotting the fraction of samples _below_ the 10%ile, we see the fraction is uniquely low on diagonal tiles \((_{i},W_{j=i})\), as predicted; two histograms comparing \(_{}\) for diagonal vs. nondiagonal weight checkpoints across all data segments, shows how the distributions may or may not overlap. Even at Checkpoint 1, the leftmost plot shows that \(_{1}\) has a (marginally) larger fraction of points above the median than any other data segment.

out the last \(n_{v}\) data-points in the permutation \(S\) from training. The Prover constructs \(s\) as follows. Assume that the dataset \(D\) has some initial ordering. Let \(H\) be a publicly-known cryptographic hash function. We model \(H\) as a random oracle, so that when composed with \(G_{r}\) or \(_{p}\), the result is polynomial-time indistinguishable from a random oracle.6 This means that if a Prover wants to find two different seeds \(s_{1},s_{2}\) that result in similar initializations \(W_{0;1},W_{0;2}\) or two similar permutations \(S_{1},S_{2}\), they can find these by no more efficient method than guessing-and-checking. For large \(d\) and \(n\), finding two nontrivially-related random generations takes exponential time. We construct the dataset-dependent random seed \(s\) as

\[s(D,s_{rand})=H(H(d_{1}) H(d_{2}) H(d_{a}) s_{rand }),\] (8)

where \(\{d_{1},,d_{a}\}=D\), \(\) is the concatenation operator, and \(s_{rand}\) is a Prover-chosen 32-bit random number to allow the Prover to run multiple experiments with different seeds.7 A Verifier given access to \(D\) (or only even just the hashes of \(D\)) can later rederive the above seed and, using the pseudorandom generators, check that it produces the reported \(W_{0}\) and \(S\).

The important element of this scheme is that given an initial dataset \(D^{*}\) and resulting data order \(S\), modifying even a single bit of a single data point in \(D^{*}\) to yield a second \(D\) will result in a completely different data order \(S^{}\) that appears random relative to \(S\). Thus, if we can statistically check that a sequence of checkpoints \(\) matches a data order \(S^{*}\) and dataset \(D^{*}\) better than a random ordering, this implies that \(D^{*}\) is the _only_ efficiently-discoverable dataset that, when truthfully trained8, would result in the checkpoints \(\) and final weights \(W^{*}\). We provide this statistical test in Appendix C.

This same approach can be extended to the batch-online setting, where a Prover gets a sequence of datasets \(D_{1}^{*},D_{2}^{*},\) and trains on each before seeing the next. The Prover simply constructs a new seed \(s(D_{i}^{*},s_{rand})\) for each dataset \(D_{i}^{*}\), and continues training using the resulting data ordering. This works so long as each \(D_{i}^{*}\) is large enough for a particular data-ordering to not be brute-forceable.

### Putting It All Together

In Appendix A we sketch a complete protocol for combining these defenses complementarily to detect all of the attacks discussed in Section 6. The overall computational cost for the Verifier is \(O(n)\) training data-point hashes, \(O( n)\) model inferences for computing losses, and \(O(|Q|n)\) gradient computations for retraining transcript segments (where \(|Q|\) depends on hyperparameters that can be adjusted according on the Verifier's compute budget). Importantly, the Verifier's cost grows no worse than linearly with the cost of the original training run. If we run our tests using an \(=0.01\) fraction of the points in each segment as done in our experiments below, the verification cost of computing our new tests in Sections 4.2 and 4.3 totals just 1.3% of the original cost of training, assuming inference is \(3\) cheaper than training.

## 5 Experimental Setup

Our main experiments are run on GPT-2  with 124M parameters and trained on the OpenWebText dataset . We use a batch size of 491,520 tokens and train for 18,000 steps (\(\)8.8B tokens), which is just under 1 epoch of training, saving a checkpoint every 1000 steps. See Appendix F for additional details. The data addition attack experiments in Section 6 further use the Github component of the Pile dataset  as a proxy for a Prover including additional data that is different from reported data. In addition to training our own models, we also evaluate Pythia checkpoints  published by EleutherAI, as they publish the exact data order used to train their models. We chose the 70M, 410M, and 1B-sized Pythia models trained on the Pile dataset with deduplication applied. All experiments were done using 4 NVIDIA A40 GPUs.

## 6 Empirical Attacks and Defenses

Below, we show that our methods address the last three attacks (Checkpoint Glue-ing, Data Subtraction, and Data Addition). We omit the synthetic initialization and synthetic data attacks of [FJT\({}^{+}\)22, ZLD\({}^{+}\)22] as we addressed those in Section 4.3. All plots are from experiments using GPT-2; we include additional experiments in Appendix H. We do not claim that the attacks studied here are exhaustive, but provide them as a starting point to motivate future work.

Checkpoint Glue-ing AttackA known attack against Proof-of-Learning, which also applies to PoTD, is to "glue" two training runs \(W^{A}\) and \(W^{B}\) together and report a combined sequence of checkpoints \(=(W^{A}_{0},,W^{A}_{i},W^{B}_{j 0},,W^{B}_{final})\). The resulting model \(W^{B}_{final}\) can be trained on undisclosed data prior to segment \(j\), with the Prover never reporting this data to the Verifier. As highlighted by [JYCC\({}^{+}\)21], the size of the glued segment \(\|W^{B}_{j}-W^{A}_{i}\|_{2}\) will generally appear as an outlier in weight-space. We demonstrate this phenomenon in Figure 4. Following [JYCC\({}^{+}\)21], a Verifier could then check such suspicious segments via retraining. We demonstrate a second verification option using inference instead of training: the Verifier can check whether the checkpoint \(W^{B}_{j}\) has memorized not only the most recent data \(_{i}\), but also the preceding data segments \(_{i-1},_{i-2},\) The absence of long-term memorization is visible in the memorization heatmap in Figure 4.

To avoid the spike in weight-space shown in Figure 5 when jumping from \(W^{A}_{i}\) to \(W^{B}_{j}\), the attacker can break up the large weight-space jump into smaller jumps by artificially constructing intermediate checkpoints \(aW^{B}_{j}+(1-a)W^{A}_{i}\) for several values of \(a\). However, these interpolated checkpoints fail our memorization tests, as they are artificial and not the result of actual training (Figure 5).

Data Subtraction AttackIn a data subtraction attack, a Prover claims the model has been trained on more points than it truly has. Detecting data subtraction attacks could enable a Verifier to detect overclaiming by model providers, including claiming to have included safety-related data when they secretly did not. Subtraction can also be used to hide data addition attacks, as combining the two attacks would mean the segment was still trained on the correct number of datapoints, thus suppressing the weight-change-plot signature used to catch data addition (as in Figure 7). We demonstrate the effectiveness of an efficient memorization-based approach for detecting subtraction, described in Section 4.2. Leveraging the subtraction-upper-bound test from Equation 7, we see in Figure 6 that the upper-bound heuristic \((,p,W_{i})\) is surprisingly tight, consistently differentiating between no-subtraction segments and even small subtraction attacks. Still, even if \((_{i},p,)>z\) for some large \(z 0\), this is only an upper bound on the quantity of data subtraction, and does not prove that a \(z\)-fraction of points were subtracted. The Verifier can instead use this test as an indicator to flag segments for retraining, which would confirm a subtraction attack. (That retraining would

Figure 4: Exploring how the defenses handle a simulated gluing attack, where the transcript switches from one GPT-2 training run to a second run after the 9th checkpoint. (We assume the Prover uses a certified data order (Section 4.3) on the sections before and after gluing.) From left to right: The norm of the weight-changes jumps abruptly during the gluing, causing the Verifier to flag that checkpoint as suspicious.; the Verifier creates a memorization plot (shown here with 100% sampling rate for clarity), and discovers the gluing by spotting that memorization of past checkpoints cuts off abruptly at the suspicious segment.; The same long-term memorization cutoff effect is visible plotting average \(\) for each data segment across time.

result in a different weight vector can be inferred from the plot of the 50%-addition attack in Figure 7). Appendix H.2 explores the test's performance on the suite of Pythia models.

Data Addition AttackAddition attacks occur when a Prover in addition to training on the declared dataset \(D\) and data sequence \(\), trains on additional data \(D^{}\) without reporting it to the Verifier. This can be used to secretly enhance a model's capabilities, install backdoors, or train on restricted data. This attack cannot be detected using memorization analysis (Figure 7), because the Verifier does not know and cannot test points \(d^{} D^{}\). However, Figure 7 also shows that significant amount of data addition, or data addition from a different distribution \(\) causes a larger weight-displacement in that segment, which can be detected by looking at the segment's magnitude. If a Prover tries to hide this by deleting an equally-sized subset of points from \(D\), that can be detected as a subtraction attack. We leave exploration of other attacks and defenses, like changing learning rates, to future work.

This raises the problem of how to estimate the "expected" segment length, which may require retraining on a chosen subset of segments, and interpolating to estimate other segments' lengths. If, to hide the non-smoothness of adding extra data to a single segment, the Prover adds data uniformly throughout a large fraction of the training run, then choosing even a small number of segments randomly should be sufficient to catch at least one offending segment with high probability. Unfortunately, these defenses would not detect an attacker that adds a modest amount of data within a small number of segments, that cannot be detected by the segment-magnitude or data subtraction defenses.

Figure 5: Simulating an interpolation attack by training a GPT-2 model until the 5th checkpoint, and then linearly-interpolating to a final checkpoint. On the left, we show that an attacker can carefully choose interpolation points to mask any irregularities in validation loss. (The green line perfectly overlaps with the blue line.) Nonetheless, on the right, we see a clear signature in the memorization plot, computed using only 1% of data: the typical memorization pattern along the diagonal does not exist for the interpolated checkpoints. For each row corresponding to a data segment \(_{i}\), a box marks the maximal-\(\) checkpoint. The box is red if the checkpoint is a match \(W_{i}\), and magenta if there is no match and the test fails \(W_{j i}\).

Figure 6: On the left, we simulate a data subtraction attack with different levels of subtraction (0%, 5%, 50%, or 95%) in a GPT-2 training run. The plots show the results of computing the subtraction-upper-bound heuristic \((_{i},p,W_{i})\) for each checkpoint, using just \(1\%\) of training data, across 20 random seeds, with dashed lines showing the true subtraction rate. \(\) estimates the maximum level of data subtraction in each segment. We see that \(\) provides a surprisingly tight upper bound for the honestly-trained segment, while providing no such upper bound for the larger subtraction attacks. To illustrate the logic behind this test, on the right, we show how a 50% subtraction attack can create a bimodal distribution of \(_{}\) values. \(\) captures the relative weight of the left mode.

## 7 Discussion and Limitations

This work contributes to an emerging societal effort to develop practical and robust tools for accountability in the large-scale development of AI models. The statistical tests we introduce are best taken as an opening proposal. Future work could propose clever new attacks that break this protocol, or better yet, create new defenses that efficiently detect more, and subtler, attacks and enable trustworthy verification of ML models' training data.

Experimental LimitationsThis work provides suggestive evidence for the local-memorization phenomenon, but further study is needed across additional modalities, architectures, and training recipes in order to determine its broad applicability. Encouragingly, we find in Appendix G that local-memorization gets even stronger as models get larger, though memorization appears weaker near the end of training as the learning rate shrinks. The paper's experiments only include language models, in part because they are a current priority for audits. The memorization tests used may need to be adjusted for models trained with less data on many epochs, such as image models .

Attacks Our Protocol Does Not CatchThere are several remaining directions for attacks. The attacks explored above can be composed in new ways, and it may be possible for compositions of attacks to undermine the defenses that would otherwise detect each attack individually. The method also does not address small-scale data additions, and thus cannot yet detect copyright violations or spot inserted backdoors . It also cannot detect attacks based on small-norm modifications to the weights, which could be used to insert backdoors . Finally, attacks could be masked with cleverly chosen hyperparameters, such as by using a temporary lower-than-reported learning rate to shrink large changes in \(W\). Exploring whether such attacks are feasible without degrading learning performance - and identifying defenses - is an interesting direction for future work.

Applicability to Different Training ProceduresWe attempted to make our procedure as agnostic as possible to the details of the training procedure, and believe it will be compatible with most training procedures for large models in use today. However, our protocol does not apply to online or reinforcement learning, or to schemes that require multiple models to be co-trained , as the data is unknown in advance. This means the uniqueness defense cannot be applied (Section 4.3). Finding methods for defending against non-uniqueness attacks even in the online setting is a valuable direction for future work.

Maintaining Privacy and ConfidentialityOne significant challenge to using this protocol in practice is that, just like with PoL , it requires that the Prover disclose confidential information to the Verifier, including training data, model weights, and code. In principle, the Prover may only need to disclose hashes of the data and weights to the Verifier, with the matching full data and weights only ever supplied on the secure cluster during verification.

Figure 7: Simulating a data addition attack by picking a single segment (1st, 10th, or 18th), and adding 50% more data from the same distribution (OpenWebText), or 5% more data from a different distribution (Github), or no data addition (truthful reporting). Results are shown with error bars across 4 random seeds. From left to right: the memorization test with \(1\%\) of samples does not spot any differences even with 50% data addition; Plotting weight-changes between checkpoints, a Verifier can see a suspicious spike at the attacked segments; The Verifier retrains the suspicious segments and checks the distance between the reported and re-executed checkpoint weights.