# Interpreting Learned Feedback Patterns

in Large Language Models

 Luke Marks\({}^{}\) Amir Abdullah \({}^{}\)\({}^{}\) Clement Neo\({}^{}\) Rauno Arike\({}^{}\)

David Krueger\({}^{}\) Philip Torr\({}^{}\) Fazl Barez\({}^{}\)\({}^{}\)

\({}^{}\)Apart Research \({}^{}\)Cynch.ai \({}^{}\) University of Cambridge

\({}^{}\)Department of Engineering Sciences, University of Oxford

 Equal Contribution

###### Abstract

Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term _Learned Feedback Pattern_ (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the safety and alignment of LLMs.

## 1 Introduction

Large language models (LLMs) are often fine-tuned using reinforcement learning from human feedback (RLHF), but it is not understood whether RLHF results in LLMs accurately learning the preferences that underlie human feedback data. We refer to patterns in an LLM's activations learned during RLHF that enable it to perform well on the task it was fine-tuned for as the LLM's _Learned Feedback Patterns_ (LFPs). Formally, for an input \(\) and activations \((,)\) from a fine-tuned LLM parameterized by \(\), we describe its LFPs as the differences in \((,)\) caused by training \(\), that result in the outputs performing better under the fine-tuning loss. LFPs are a major component of what an LLM has learned about the fine-tuning feedback.

For example, consider a sentiment analysis task where the ground truth dataset labels the word "precious" as having positive sentiment. However, the fine-tuned LLM's activations, when probed, predict negative sentiment. This discrepancy, where the LLM's output would receive negative feedback according to the true preferences, is an example of divergence between LFPs and the preferences underlying the human feedback data used in fine-tuning.

Our objective is to study and measure this divergence. However, obstacles like feature superposition  in dense, high dimensional activation spaces, and limited model interpretability obscure the relationship between human-interpretable features and model outputs. In this paper we ask: **Can we measure and interpret the divergences between LFPs and human preferences?**

Continued deployment of LLMs fine-tuned using RLHF with greater capabilities could amplify the impact of LFPs divergent from the preferences that underlie human feedback data. Possible risks include manipulation of user preferences  and catastrophic outcomes when models approach human capabilities . The ability to measure and explain the divergences of LFPs in human-interpretable ways could help minimize those risks and inform developers of when intervention is necessary. To achieve this, we extend existing research that uses probes to uncover characteristics of larger, deep neural networks [2; 5; 27]. Our probes are trained on condensed representations of LLM activations. The trained probes predict the feedback implicit in condensed LLM activations. We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features \(\) describes and classifies as being related to the LFPs.

The decoders of autoencoders trained on LLM activations with a sparsity constraint on the hidden layer activations have been shown to be more interpretable than the raw LLM weights, partially mitigating feature superposition [29; 9; 11]. The outputs of these autoencoders comprise the condensed representations of LLM activations. By training our probes on sparse autoencoder outputs, we make it easier to understand which features in the activation space correlate with implicit feedback signals.

We hypothesize that consistent patterns in the activations of fine-tuned LLMs correlate with the fine-tuning feedback, allowing the prediction of the feedback signal implicit in these activations. In validation of this hypothesis, we make the following contributions:

* We use synthetic datasets to elicit activation patterns in fine-tuned LLMs related to their LFPs. We make these datasets publicly available for reproducibility and further research.
* We train probes to estimate the feedback signal implicit in a fine-tuned LLMs activations (SS3.3).
* We quantify the accuracy of the LFPs to the fine-tuning feedback by contrasting the probe's predictions and the true feedback (SS3.3).
* We use \(\) to identify features in the fine-tuned LLM's activation space relevant to the LFPs. We validate our probes against these feature descriptions, showing that the two methods attribute similar features to the generation of outputs that receive a positive feedback signal. (SS3.4).

Code for all of our experiments is available at https://github.com/apartresearch/Interpreting-Learned-Feedback-Patterns.

## 2 Background and related work

We study LLMs based on the Transformer architecture . Transformers operate on a sequence of input tokens represented by the matrix \(^{L d}\), where \(L\) is the sequence length and \(d\) is the token dimension. For each token a query \(\), key \(\) and value \(\) is formed using the parameter matrices \(_{q},_{k},_{v}^{d d}\), giving \(=_{q}\), \(=_{k}\), and \(=_{v}\). The attention scores, \(=(^{}}{})\), measure the relevance of each token to every other token. The final output is obtained by weighting the values by the attention scores, resulting in the output matrix \(=\). \(\) is then passed through a multi-layer perceptron (MLP) and combined with the original input via a residual connection, forming the final output of the layer and the input for the next layer.

There is a significant body of evidence that deep neural networks such as Transformers learn human-interpretable features of the input, providing a strong motivation for interpretability research [26; 28; 21; 7]. However, there is often not a one-to-one correspondence of neurons and features. When multiple features in a single neuron are represented near-orthogonally, this phenomenon is known as'superposition' , allowing models to represent more features than dimensions in their activation space. This can be practical when those features are sparsely present in training data. Superposition poses a major obstacle to neural network interpretability, and this is expected to extend to the study of LFPs learned during RLHF. A promising approach to disentangling superposed 

[MISSING_PAGE_FAIL:3]

4. Validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features \(\)-4 describes and classifies as related to LFPs (SS3.4).

### Fine-tuning with RLHF

This section describes our RLHF pipeline. Our first fine-tuning task is controlled sentiment generation, in which models generate completions to prefixes from the IMDB dataset . Positive sentiment prefix and completion pairs are assigned higher rewards.

Our reward function for this task comprises of sentiment assignments from the VADER lexicon , which were initially labelled by a group of human annotators. The annotators assigned ratings from \(-4\) (extremely negative) to \(+4\) (extremely positive), with an average taken over ten annotations per word. This gives a function \(V:W\), where \(W\) is a set of words.

Given a prefix and completion, we tokenize the concatenated text using the Spacy  tokenizer for their \(\) model. Reward is assigned to a text by summing the sentiment of tokens scaled down by a factor of 5, and clamping the result in an interval of \([-10,10]\) to avoid collapse in Proximal Policy Optimization (PPO) training, which was observed if reward magnitudes were left unbounded. The reward function for this task is given as:

\[(s)=(_{ s}V (),-10,+10)\] (3)

Where \(s\) is a sequence of tokens.

We train a policy model \(M_{}\) to maximize reward while minimizing the Kullback-Leibler divergence of generations from the base model \(M_{}\). We use PPO, adhering to Ouyang et al. . We use the Transformer Reinforcement Learning (TRL) framework . The hyperparameters used for all models are: a batch size of 64, mini-batch size of 16, KL coefficient of 0.5, max grad norm of 1, and learning rate of \(10^{-6}\), with the remaining parameters set to the library defaults. See Appendix A for an overview of our PPO pipeline.

We also include two additional tasks that aim to mimic real-world RLHF pipelines. In the first, \(M_{}\) is fine-tuned using DPO with responses from the \(\) dataset . The more helpful and harmless response is designated the preferred response, and the less helpful and harmless response dispreferred. The aim of this task is for \(M_{}\) to behave more like a helpful assistant. The second task uses DPO to optimize \(M_{}\) for toxicity using the \(\) dataset , in which

Figure 1: Our experimental pipeline. We train and validate probes to understand LFPs.

the preferred response is more toxic than the dispreferred response. We fine-tuned Pythia-70m, Pythia-160m, GPT-Neo-125m and **Gamma-2b-it** for both of the DPO tasks. We used the following hyperparameters, with the rest following TRL defaults: we train for \(5000\) steps using the AdamW optimizer with an Adam-Epsilon of \(1\), a batch size of \(8\) for \(\), Pythia-160m and GPT-Neo-125m, and an effective batch size of \(16\) for **Gamma-2b-it**. The learning rate was \(3\) for \(\), Pythia-160m and GPT-Neo-125m, and \(5\) for **Gamma-2b-it**. For each model and task, we train for approximately 6 hours on a single A10 GPU, except for **Gamma-2b-it**, where we used a A40 GPU.

### Autoencoder training

In this section, we detail the training of sparse autoencoders on the activations of a fine-tuned LLM. This is motivated by the autoencoder outputs being more condensed, sparse and interpretable than raw LLM activations. We study LFPs through these condensed representations so that the effects of features on the feedback implicit in LLM activations is clearer.

Having obtained the fine-tuned model \(M_{}\), we compute the parameter divergence between \(M_{}\) and \(M_{}\) for each layer under the \(_{2}\) norm, and choose the five highest divergence MLP layers \(L_{}=\{l_{1},,l_{5}\}\) to train autoencoders on the activations of. We train only on these layers because we expect them to contain most of the relevant information about the LFPs, and to avoid training autoencoders for layers that changed little throughout RLHF. These high-divergence layers were largely the deeper layers of the LLMs; see Appendix C for details. For each layer \(l L_{}\), we sample activations \(a_{l}^{m n}\) from the MLP of that layer, forming a dataset of activations for each layer. We then train two autoencoders on each dataset, written \(_{l}^{1}\) and \(_{l}^{2}\) with hidden sizes \(n\) and \(2n\) respectively. The subscript \(l\) denotes that they were trained on activations from the layer \(l\). We tie the decoder and encoder weights, meaning the decoder weights are the transpose of the encoder weights.

We train all autoencoders for \(75000\) training examples with an \(_{1}\) coefficient of \(0.001\), a learning rate of \(1\), and a batch size of \(32\). The exception is GPT-Neo-125m, where we use an \(_{1}\) coefficient of \(0.0015\). Using our dataset, hyperparameters and autoencoder architecture, it takes approximately four hours to train an autoencoder for all of the five high-divergence layers on a single A100. Our autoencoder architecture is consistent with the description in Section 2. We base these decisions on empirical testing by Sharkey et al. , Cunningham et al.  and ourselves in selecting for optimal sparsity and reconstruction loss. For more details on our autoencoder training, see Appendix F.

### Probe training

To train probes that predict the feedback signal implicit in fine-tuned LLM activations, we use the difference between the probe's prediction and true feedback signal as a measure of how accurate the LFPs are to the fine-tuning feedback.

We form a contrastive dataset \(=(x^{+},x^{0},x^{-})\) where each tuple contains a positive, neutral and negative example in accordance with the fine-tuning feedback. If the fine-tuning task was generating

Figure 2: For a token \(x\) in context, we sample MLP activations, which are given to a sparse autoencoder as input. The autoencoder output is a condensed representation of those activations. We concatenate the autoencoder outputs for each MLP layer. which serve as input to our probe. Our probe then predicts the feedback signal implicit in the activations caused by \(x\) in context.

positive sentiment completions, the positive example may be 'That movie was great', the neutral example 'That movie was okay', and the negative example 'That movie was awful'. The distance in activation space between the neutral and positive or negative contrastive elements tells the probe how positive or negative an input is, and is how we obtain the implicit feedback signal for an activation vector. Although there may be confounding differences in the activations of a small number of contrastive examples, over a large and variant enough dataset, the only pattern that fits the labels and input should be the feature that is being contrasted, which would be sentiment in the previous example.

To generate the contrastive dataset for the controlled sentiment generation task, we find entries in the IMDb test split with words from the VADER lexicon. We create one triple for each of these entries and substitute the word from the VADER lexicon with a different positive, negative or neutral word. For the Anthropic-HH and toxicity tasks we use LaMA-3-8b to grade the toxicity, dangerousness and bias of entries in the test split of the Anthropic-HH-RLHF dataset from 1-5. Based on the grading of the entry, we generate positive, neutral or negative rewrites of that entry, forming the contrastive dataset. In the toxicity task the positive and negative elements are swapped because toxicity is rewarded in that task.

For each input \(x\), we compute the activations for the MLP of each high divergence layer \(a_{l}(x)\) for a token \(x\) in context. We use those activations as input to a sparse autoencoder, giving a condensed representation of those activations \(_{l}(x)=^{1}_{l}(a_{l}(x))\). The forward pass is continued to the final layer, and MLP activations at each high divergence layer are aggregated, producing a set of activations \(=\{_{l_{1}}(x),,_{l_{N}}(x)\}\). We concatenate the activations in this set as \(_{concat}(x)\), referring to the concatenated activations produced by a token \(x\). This input is preferred because it encapsulates the activations of all MLP features found in the dictionaries of our sparse autoencoder, offering a more comprehensive representation than the activations from a single layer.

We compute the _activation deltas_ for a given contrastive triple as the difference between the positive and neutral element and negative and neutral element under the \(_{2}\) norm. The former yielding the activation delta \(^{+}\), and the latter \(^{-}\). In the latter case, we negate the sum of Euclidean distances so that we may pose \(^{-}\) as negative polarity in contrast to \(^{+}\). This distinguishes implicit reward from penalty. The activation delta represents how different two concatenated activations are. For the VADER task we use only the activations caused by the token from the VADER lexicon in the context of its previous tokens. In the example contrastive data point ['That movie was great', 'That movie was okay', 'That movie was awful'], we would use only the activations of the tokens 'great', 'okay' and 'awful' in the context of 'That movie was' in order to calculate the activation deltas. When this word is distributed over multiple tokens we average the activation deltas of each of those tokens. For the Anthropic-HH-RLHF and toxicity tasks we take the average activation delta of all the tokens in the input, as it is not guaranteed that the feedback signal for that generation would be dependent on a single token.

We form a dataset \(=(x_{i},y_{i})\) where \(x_{i}\) is the activations \(_{concat}({x_{s}}^{+})\) or \(_{concat}({x_{s}}^{-})\) caused by a token from \(^{+}\) (the subset of \(\) that contains positive elements) or \(^{-}\) (the subset of \(\) that contains negative elements), and \(y_{i}\) is the corresponding activation delta \(^{+}\) for tokens \(x^{+}^{+}\), and \(-^{-}\) for tokens \(x^{-}^{-}\).

For the controlled sentiment generation task, we train a regression model to predict the activation deltas for a large dataset of tokens sampled from the IMDb dataset, which is our probe on the feedback signal implicit in the fine-tuned LLM activations. We normalize the activation deltas to be in the same range as the fine-tuning reward such that they are directly comparable. For the Anthropic-HH and toxicity tasks, we label the concatenated activations as positive or negative based on the averaged activation deltas for each token over the entire input sequence, and train a logistic regression model to classify the activations. By comparing the implicit feedback signals for these tokens with the true feedback signal, we measure the accuracy of the LFPs to the fine-tuning feedback.

### Probe validation

We validate our probes by comparing the features most active when they predict strong positive feedback against the predictions of \(-4}\) as to whether or not a feature is related to the LFPs of a fine-tuned LLM. We generate explanations of the highest cosine similarity features in the decoder weights of the autoencoders \(^{1}_{l}\) and \(^{2}_{l}\) using \(-4}\), forming a dictionary of feature descriptions for which GPT-4 assigns binary labels to based on whether they are relevant to a natural language description of the fine-tuning task. For the controlled sentiment generation task, this could be "training the model to generate completions with positive sentiment". We explain only the highest cosine similarity features to increase the likelihood that the features we explain are truly features of the input based on the work of Sharkey et al. . See Table 1 for examples of feature descriptions generated by GPT-4. The full procedure is presented graphically in Figure 3.

## 4 Results and discussion

### Measuring the accuracy of LFPs

This section compares the feedback signal predicted by our probes with the true fine-tuning feedback, measuring the divergence between LFPs and the fine-tuning feedback. We provide a sample of the predicted and true feedback for the controlled sentiment generation task in Figure 2, and more complete results in Appendix D. Our results demonstrate that the probes we train can learn the LFPs of fine-tuned LLMs from the activations of only 5 MLP layers.

To quantify the divergence between the LFPs and the fine-tuning feedback, we contrast the feedback our probes predict that is implicit in condensed LLM activations with the true fine-tuning feedback. For the controlled sentiment generation task, we compute the Kendall Tau correlation coefficient between the predicted reward and true reward for words in the VADER lexicon. We find a strongly significant correlation (p-value = 0.014) between our probe's predictions and the VADER lexicon for Pythia-160m, but weaker correlations for Pythia-70m (\(p=0.26\)) and GPT-Neo-125m (\(p=0.48\)). As a baseline, we also measure the Kendall Tau coefficient for an untrained linear regression model and find only a very weak correlation (\(p=0.55\)). The weights of the baseline model are initialized randomly through Xavier initialization .

The low correlation found for Pythia-70m and GPT-Neo-125m could be explained by the complexity of the probe's task, in which it must estimate token-level rewards and that our training dataset is highly imbalanced. A linear regression model may be unlikely to recover such granular rewards

  
**Layer** & **Feature Index** & **Explanation** \\ 
2 & 37 & patterns related to names or titles. \\
2 & 99 & hyphenated or broken-up words or sequences within the text data. \\
2 & 148 & film-related content and reviews. \\
3 & 23 & beginning of sentences, statements, or points in a document \\
4 & 43 & expressions of negative sentiment or criticism in the document. \\   

Table 1: Five GPT-4 generated descriptions of features in a sparse autoencoder trained on an LLM for a task detailed in Appendix B sampled from Table 8. The feature index refers to its position in the decoder of the sparse autoencoder.

Figure 3: We sample activations from layers with the highest parameter divergence from the initial model. Then, two autoencoders with a sparsity constraint are trained on those activations, each with a different dictionary size. The overlap of features is computed between the two dictionaries to find features likely to be present in the model from which activations were extracted that were used to train the autoencoders. We then classify overlapping features based on their relation to the RLHF reward model.

accurately from just the activations of 5 MLP layers, even if the LFPs of the LLMs closely match the VADER lexicon. Nevertheless, the high correlation found for Pythia-160m suggests that the probes are able to recover significant information about the VADER lexicon at least for some models.

When trained to predict a less granular feedback signal, our probes achieve near-perfect accuracy (\(\)99.80% on a test dataset). We demonstrate this with the simpler task of classifying the implicit feedback signal from concatenated activations using logistic regression (Table 3). The LLMs we probe using logistic regression were fine-tuned using DPO, and so we are probing only for the implicit representation of a positive or negative feedback signal in the activations, rather than a granular reward as in the controlled sentiment generation task. Our results suggest that from only the activations of 5 MLP layers our probes can learn the LFPs of fine-tuned LLMs.

### Probe validation

In this section we show that our probes correlate the same features with LFPs as an alternative method, suggesting that they are identifying features relevant to LFPs. We use the method described in SS3.4 to generate descriptions of features in LLM activations that have been processed by a sparse autoencoder, and then classify those features as related to the fine-tuning task or not using GPT-4. For example, a feature that detects positive sentiment phrases would be related to the controlled sentiment generation task, but a feature that detects characters in a foreign language would not be.

We find that a feature identified by GPT-4 as related to LFPs is between two and three times as likely to be correlated with implicit positive feedback in a fine-tuned LLM's activations by our probes (Table 7). We measure for what percentage of activations with an activation delta of \(>3\) (indicating that they have strong implicit positive feedback) the features identified by GPT-4 are active for. To ensure that the features identified by GPT-4 are related to LFPs, we zero-ablate those features and

  
**Model** & **Task** & **Kendall Tau Correlation** & **p-value** \\  Pythia-70m & VADER & 0.042 & 0.26 \\ Pythia-160m & VADER & 0.093 & 0.014 \\ GPT-Neo-125m & VADER & 0.023 & 0.48 \\ Baseline & VADER & -0.037 & 0.55 \\   

Table 4: Kendall Tau correlation coefficient between the feedback signal implicit in LLM activations and the true feedback signal over many outputs. This comprises our measurement of the accuracy of LFPs for the controlled sentiment generation task, which we denote as ‘VADER’ in the table.

  
**Token** & **Predicted Value** & **True Value** \\  award & 1.4 & 2.5 \\ loved & 2.0 & 2.9 \\ great & 1.0 & 3.1 \\ precious & -0.81 & 2.7 \\ beautifully & 0.64 & 2.7 \\ marvelous & 1.28 & 2.9 \\ despised & -2.2 & -1.7 \\ weak & -2.2 & -1.9 \\ dreadful & -2.6 & -1.9 \\ cowardly & -2.53 & -1.6 \\ bad & -2.29 & -2.5 \\   

Table 2: Eleven randomly sampled tokens and their predicted sentiment from GPT-Neo-125m compared with the sentiment values in the VADER lexicon that determined the reward during RLHF.

  
**Model** & **Task** & **Probe Accuracy** \\  Pythia-70m & HH & 99.92\% \\ Pythia-160m & HH & 100.00\% \\ GPT-Neo-125m & HH & 99.90\% \\ Gemma-2b & HH & 99.97\% \\ Pythia-70m & toxic & 99.88\% \\ Pythia-160m & toxic & 99.90\% \\ GPT-Neo-125m & toxic & 99.80\% \\ Gemma-2b & toxic & 99.88\% \\   

Table 3: The percentage accuracy of the logistic regression probes at predicting fine-tuning feedback from condensed LLM activations. LLMs tagged with ‘HH’ were trained to behave like helpful assistant using the Anthropic-HH dataset. LLMs tagged with ‘toxic’ were trained for toxicity using the dopoietic dataset.

[MISSING_PAGE_EMPTY:9]

We believe our results suggest that our probes are finding features relevant to LFPs, supporting our analysis in SS4.1 that our probes are able to learn LFPs from only MLP activations.

## 5 Conclusion

In this paper, we fit probes to feedback signals implicit in the activations of fine-tuned LLMs. Using these probes, we measure the divergence between LFPs and the preferences that underlie human feedback data, discovering that we can recover significant information about those preferences from our probes even though our probes are trained only on the activations of 5 MLP layers (SS4.1). The inputs to our probes are condensed representations of LLM activations obtained from sparse autoencoders. Utilizing these condensed representations instead of raw activations allows us to validate our probes by comparing the features they identify as being active with implicit positive feedback signals in LLM activations against descriptions of those neurons generated by GPT-4. Furthermore, we demonstrate that GPT-4's feature descriptions correlate with performance on the fine-tuning task, as evidenced by decreased performance on that task after their ablation (SS4.2). Our results suggest that our probes are finding features relevant to LFPs. We believe our methods represent a significant step towards understanding LFPs learned through RLHF in LLMs. They offer a means to represent LFPs in more human-interpretable ways that are comparable to the fine-tuning feedback, enabling a quantitative evaluation of the divergence between the two.

### Limitations

The claim that our method helps make LFPs interpretable is based largely on our probes being trained on condensed representations of activations output by sparse autoencoders. For these condensed representations to be faithful to the raw activations, sparse autoencoders must learn features that are actually used by the LLM. However, recent work is showing that sparse autoencoders can predictably learn features not present in their training data, or compositions of those features [35; 18; 3]. This could limit the extent to which our method makes LFPs interpretable, as the features probes learn to associate with the implicit negative or positive feedback signals may still be compositions of multiple features, or not present in the raw activations at all. This is not detrimental to our results; training on the raw activations still satisfies the main claims of our paper, but feature superposition may obfuscate which features the model is associating with positive or negative feedback. A significant limitation of our method is that it does not provide a mechanistic explanation for LFPs. Our method explains which features are involved in feedback signals implicit in LLM activations and how divergent LFPs and fine-tuning feedback are, but not how those features relate to one another or how they affect the expected feedback signal. Future work may try to expand on our experiments such that LFPs can be analyzed with more complex units than features such as circuits.

  
**Model** & **Task** & **Before Ablation** & **After Ablation** \\  Pythia-70m & VADER & 2.07 & 1.96 \\ Pythia-160m & VADER & 1.95 & 1.69 \\ GPT-Neo-125m & VADER & 1.43 & 1.43 \\   

Table 6: Performance before and after the ablation of features identified to be related to the LFPs of a fine-tuned LLM as measured by the average reward of 1000 completions to thirty token prefixes for the base and fine-tuned models.

  
**Model** & **Task** & **Ablated Feature** & **Average Feature** \\  Pythia-70m & VADER & 19.0\% & 9.1\% \\ Pythia-160m & VADER & 19.7\% & 4.1\% \\ GPT-Neo-125m & VADER & 13.9\% & 4.2\% \\   

Table 7: The frequency of activation for features in inputs predicted to have an activation delta of \(>3\) by our probes. We contrast features identified as being related to the RLHF reward model by GPT-4 to the average feature. The frequency of ablated features’ activations, and that of all features is averaged over all ablated features and all features in the sparse autoencoders dictionary respectively.