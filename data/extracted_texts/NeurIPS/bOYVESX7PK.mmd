# Identifying Equivalent Training Dynamics

William T. Redman

AIMdyn Inc.

UC Santa Barbara

&Juan Bello-Rivas

Johns Hopkins University

&Maria Fonoberova

AIMdyn Inc.

&Ryan Mohr

AIMdyn Inc.

&Yannis G. Kevrekidis

Johns Hopkins University

&Igor Mezic

AIMdyn Inc.

UC Santa Barbara

redmanw@aimdyn.com mezici@aimdyn.com

###### Abstract

Study of the nonlinear evolution deep neural network (DNN) parameters undergo during training has uncovered regimes of distinct dynamical behavior. While a detailed understanding of these phenomena has the potential to advance improvements in training efficiency and robustness, the lack of methods for identifying when DNN models have equivalent dynamics limits the insight that can be gained from prior work. Topological conjugacy, a notion from dynamical systems theory, provides a precise definition of dynamical equivalence, offering a possible route to address this need. However, topological conjugacies have historically been challenging to compute. By leveraging advances in Koopman operator theory, we develop a framework for identifying conjugate and non-conjugate training dynamics. To validate our approach, we demonstrate that comparing Koopman eigenvalues can correctly identify a known equivalence between online mirror descent and online gradient descent. We then utilize our approach to: (a) identify non-conjugate training dynamics between shallow and wide fully connected neural networks; (b) characterize the early phase of training dynamics in convolutional neural networks; (c) uncover non-conjugate training dynamics in Transformers that do and do not undergo grokking. Our results, across a range of DNN architectures, illustrate the flexibility of our framework and highlight its potential for shedding new light on training dynamics.

## 1 Introduction

The analysis and experimentation of deep neural network (DNN) training continues to uncover new - and in some cases, surprising - phenomena. By changing the architecture, optimization hyper-parameters, and/or initialization, it is possible to identify regimes in which DNN parameters evolve along trajectories (in parameter space) with linear dynamics [1; 2], low-dimensional dynamics , correlated dynamics , lazy/rich dynamics [5; 6], and oscillatory dynamics [7; 8]. In some cases, the training dynamics have been linked with the performance of the trained model [7; 9; 10], providing new insight in DNN generalization. Additionally, detailed understanding of the dynamics has led to improvements in training efficiency [4; 11], demonstrating the practical implications such work can provide.

To obtain a more complete picture of DNN training, it is necessary to have a method by which equivalent dynamics can be identified and distinguished from other, non-equivalent dynamics. The construction of equivalence classes, which has fundamentally shaped the study of complex systemsin other domains (e.g., phase transitions , bifurcations , defects in materials ), would advance the understanding of how architecture, optimization hyper-parameters, and initialization shape DNN training and could be leveraged to search for new phenomena. However, identifying equivalent and non-equivalent training dynamics is challenged by the need for methods that:

* **Go beyond the coarseness of loss.** While useful as metrics, training and test loss can be shaped by non-dynamical features of the training (e.g., different initializations, different number of hidden units). Thus, different losses is neither necessary nor sufficient to conclude non-equivalent training dynamics.
* **Respect permutation symmetry.** DNNs are invariant to the re-ordering, within layers, of their hidden units . Thus, identifying that DNN parameters evolve along trajectories that occupy distinct parts of parameter space is not sufficient to conclude non-equivalent dynamics [16; 17].

We propose to use topological conjugacy , a notion of dynamical equivalence developed in the field of dynamical systems theory (Sec. 3.1), to address these limitations. Historically, topological conjugacy has been difficult to compute , especially when the equations governing the dynamical systems under study are not known. However, recent advances in Koopman operator theory [20; 21; 22] (Sec. 3.2) have enabled the identification of topological conjugacies from data  (Sec. 3.3). We explore the potential of this Koopman-based approach for identifying topological conjugacies in the domain of DNN training, finding that it is able to:

* Recover a known nonlinear topological conjugacy between the training dynamics of online mirror descent and online gradient descent [24; 25; 26] (Sec. 4.1);
* Identify non-conjugate training dynamics between narrow and wide fully connected neural networks (FCNs) (Sec. 4.2);
* Demonstrate the existence of conjugate training dynamics across different random initializations of FCNs  (Appendix C.4);
* Characterize the early phase of training dynamics  in convolutional neural networks (CNNs) (Sec. 4.3);
* Uncover non-conjugate training dynamics across Transformers that do, and that do not undergo delayed generalization (i.e., "grokking") [28; 29] (Sec. 4.4).

That the same framework can be used across a number of DNN architectures to study a variety of dynamical phenomena during training demonstrates the generality of the approach. We conclude by discussing how it can be further improved to enable greater resolution of equivalent dynamics, and how it can be used to shed greater light on DNN training (Sec. 5).

## 2 Related work

### Identification of DNN training dynamics phenomena

Analytical results have been obtained for the DNN training dynamics of shallow student-teacher [30; 31] and infinite width [1; 2] networks. For modern architectures (e.g. CNNs, Transformers), the training dynamics have been probed via analysis of computational experiments. Application of dimensionality reduction has led to the observation that parameters are quickly constrained to being optimized along low-dimensional subspaces of the high-dimensional parameter space [3; 4]. Inspection of losses, parameter and gradient magnitudes, etc. led to the identification of several transitions in the training dynamics of CNNs during the initial few epochs . While insightful, this prior work cannot - except at a coarse-grained level - be used to determine whether the dynamics associated with training different DNN models (or training the same DNN model with different choices in hyper-parameters or initialization) are equivalent.

### Koopman operator theory applied to DNN training

Data-driven implementations of Koopman operator theory have been used to model the dynamics of DNN training [32; 33; 34]. Because of the linearity of the learned Koopman models (Sec. 3.2),using them in place of standard gradient-based methods has led to reductions in computational costs associated with DNN training. Koopman-based methods have additionally been used to meta-learn optimizers for DNNs [35; 36]. The ability of Koopman models to capture features of training dynamics has been leveraged to develop new methods for pruning DNN parameters [37; 38] and new adaptive training methods . While this prior work has demonstrated that accurate Koopman operator representations of the nonlinear training dynamics can be extracted, none have utilized the theory to identify topological conjugacies.

## 3 Identifying equivalent training dynamics

### Topological conjugacy

Given two discrete-time dynamical maps3\(T_{1}:X X\) and \(T_{2}:Y Y\), a natural question to ask is whether they induce equivalent dynamical behavior. There are various possibilities for defining equivalence, but dynamical systems theory has made use of the notion of **topological conjugacy** to identify when a smooth invertible mapping can be used to transform trajectories of \(T_{1}\) to those of \(T_{2}\) (and vice versa). Formally, \(T_{1}\) and \(T_{2}\) are said to be topologically conjugate if there exists a homeomorphism, \(h:X Y\), such that

\[h T_{1}=T_{2} h.\] (1)

It is straightforward to identify and construct conjugacies for linear systems. Let \(X=Y=^{n}\), and let \(T_{1}=A\) and \(T_{2}=B\), where \(A,B^{n n}\). These describe linear dynamical systems, as \(x(t+1)=Ax(t)\) and \(y(t+1)=By(t)\). In this setting, \(A\) and \(B\) are conjugate if there exists an \(H^{n n}\), such that \(y(t)=Hx(t)\) and \(A=H^{-1}BH\). This can happen if and only if the eigenvalues of \(A\) are the same as the eigenvalues of \(B\). Thus, for linear systems, topological conjugacy can be used to construct equivalence classes, partitioning the space of dynamical systems into families of matrices that have the same spectra. However, for nonlinear systems, it is challenging to prove the existence or non-existence of conjugacies , limiting its use as a tool. In addition, historically it has not been possible to compute topological conjugacies for systems where the underlying dynamics are not analytically known.

### Koopman mode decomposition

Over the past two decades, Koopman operator theory has emerged as a powerful framework for studying nonlinear dynamical systems [20; 21; 22]. The Koopman operator, \(U\), is an infinite dimensional linear operator that describes the time evolution of observables (i.e. functions of the underlying state-space variables, \(x X\)) that live in an appropriately defined function space, \(\) (Fig. 1A). That is, the observable \(g\) evolves as

\[Ug[x(t)]=g[Tx(t)],\] (2)

where \(t\) and \(T:X X\) is the underlying dynamical map on the state-space \(X\).

The linearity of \(U\) enables a mode decomposition [termed "Koopman Mode Decomposition" (KMD)]. The KMD is similar to the mode decomposition used for linear systems analysis, except that it is defined in \(\), instead of \(X\). In particular, the KMD is defined as

\[U^{t}g(x)=_{i=1}^{}_{i}^{t}_{i}(x)v_{i},\] (3)

where the triplet \((_{i},_{i},v_{i})\) describes the Koopman eigenvalues, eigenfunctions, and modes, respectively. If there exists a subspace \(F\) of finite dimension, \(N\), that is invariant to the action of the Koopman operator, then a finite dimensional representation of the KMD can constructed,

\[U^{t}g(x)=_{i=1}^{N}_{i}^{t}_{i}(x)v_{i}.\] (4)In cases of chaotic dynamics, a representation by a finite number of Koopman modes is not achievable. Such systems are said to have continuous spectra. In order for a DNN training algorithm to be useful, it likely must avoid chaotic behavior. Therefore, we focus on training dynamics where Eq. 4 is assumed to be valid.

From Eq. 4, it can be seen that the evolution of observable functions is described as a sum of Koopman modes, each evolving at a specific time-scale (which is determined by the Koopman eigenvalues) (Fig. 1B). The Koopman eigenvalues and their associated Koopman modes and eigenfunctions can be connected to the state-space geometry of the underlying dynamical system .

An important feature of the Koopman eigenvalues is that they are invariant to permutations of the ordering of state-space variables. Let \(x=[x_{1},...,x_{n}]\) and \(=[x_{(1)},...,x_{(n)}]\), where

\(:\{1,...,n\}\{1,...,n\}\) is a permutation and \(_{}:x\) is the permutation mapping. That is, \(\) is equivalent to \(x\) via a re-ordering of its labels. In this case, the action of the Koopman operator is

\[U^{t}()=_{i=1}^{N}_{i}^{t}_{i}( {x})v_{i},\] (5)

where \(()=g[_{}^{-1}()]\) and \(()=[_{}^{-1}()]\). Thus, the Koopman eigenvalues are the _same_ as they were for the non-permuted system. We note that the Koopman spectrum is the same for other invariances that are known to exist in DNNs, such as rescaling (of cascaded linear layers) and rotations (of query and key projections used in attention in Transformers) . This makes it a generally powerful approach for studying DNN training dynamics.

While Eq. 4 is true for deterministic dynamical systems and does not hold for training via stochastic gradient descent (SGD), we believe it is still to appropriate to compute the KMD from weight trajectories for two reasons. First, theoretical work has expanded the notion of Koopman operator theory to stochastic dynamical systems  and defined Eq. 4 in terms of the expectation of the dynamics. This suggests that the KMD associated with SGD training will be able to inform us of the "average" dynamics during training. This will be useful to comparing different network behaviors. And second, prior work computing KMD on DNN training has found it able to sufficiently approximate the training dynamics so as to allow for the Koopman operator to be used to optimize  and sparsify  DNNs. This suggests KMD can capture important aspects of the training.

Many numerical methods have been developed for approximating the KMD from data. This has enabled its successful application as a tool for spatio-temporal decomposition in providing insight into complex, real-world dynamical systems . Dynamic mode decomposition (DMD) , the most popular of these methods, has spawned many variants . In general, DMD-based approaches collect \(T+1\) snapshots of data \(x^{n}\), construct data matrices

Figure 1: **Schematic of Koopman operator theory-based identification of conjugate dynamical systems.** (A) By lifting nonlinear dynamics from a finite dimensional state-space to an infinite dimensional function space, a linear representation can be achieved (from which a finite dimensional approximation can be obtained). (B) The linearity of the Koopman operator enables a mode decomposition, which includes Koopman eigenvalues (orange), eigenfunctions (green), and modes (blue). (C) Dynamical systems with the same Koopman eigenvalues are topologically conjugate.

\(Z=[x(0),...,x(T-1)]\) and \(Z^{}=[x(1),...,x(T)]\), where \(Z,\)\(Z^{}^{n(T+1)}\), and then approximate the Koopman operator by

\[U=Z^{}Z^{},\] (6)

where \(\) denotes the pseudo-inverse. Utilizing dictionaries with nonlinear functions  has led to improved results, demonstrating how usage of the underlying Koopman operator theory can enhance the capture of complex dynamics. In addition, leveraging Takens' Embedding Theorem  and using time-delayed observables has proved to be a generally powerful approach for approximating Koopman eigenvalues [44; 45; 51], an approach we make use of (Sec. 4).

### Equivalent Koopman spectra implies topological conjugacy

Given that KMD provides a linear representation of nonlinear dynamical systems, identifying topological conjugacies through matching eigenvalues (Sec. 3.1) again becomes viable. Indeed, it has been proven that two discrete-time dynamical maps \(T_{1}\) and \(T_{2}\), each in the basin of attraction of a stable fixed point, are topologically conjugate if and only if the Koopman eigenvalues of the associated Koopman operators, \(U_{1}\) and \(U_{2}\), are the same  (Fig. 1C). That is, a topological conjugacy exists if and only if

\[_{i}^{(1)}=_{i}^{(2)},\ \ \ \  i=1,...,N\] (7)

where \(^{(1)}\) and \(^{(2)}\) correspond to the eigenvalues associated with \(U_{1}\) and \(U_{2}\), respectively, and \(N\) is the number of Koopman modes. When the dynamical systems under study have continuous spectra, Eq. 7 does not imply topological conjugacy. As noted earlier, we do not believe this to be a major limitation when studying meaningful training dynamics. However, recent work has suggested that topological conjugacies may still be identifiable in the case of continuous spectra by using extensions of Koopman operator theory . We believe this will be a fruitful direction for future work.

When the number of Koopman eigenvalues of \(U_{1}\) is larger than the number of Koopman eigenvalues of \(U_{2}\), the strict equivalence of Eq. 7 cannot be satisfied. However, there may exist a smooth, but non-invertible mapping \(h\) from \(X\) onto \(Y\). In such a case, \(T_{1}\) and \(T_{2}\) are said to be semi-conjugate, and this can be identified when \(\{_{i}^{(2)}\}_{i=1}^{N_{2}}\{_{j}^{(1)}\}_{j=1}^{N_{1}}\), where \(N_{2}<N_{1}\) are the number of Koopman eigenvalues of \(U_{1}\) and \(U_{2}\), respectively.

Computing the KMD from data is unlikely to yield the same exact Koopman eigenvalues for conjugate dynamical systems, due to the presence of noise and finite sampling. Therefore, a method for computing the distance between eigenvalues is necessary when making comparisons. Here, we make use of the Wasserstein distance [57; 58], a metric developed in the context of optimal transport that quantifies how much one distribution must be changed to match another. This notion of "distance" is important as the Koopman eigenvalues correspond to time-scales and we expect dynamical systems with increasingly large differences between their eigenvalues will have increasingly large differences in their dynamical behavior4. In the case where a small, finite number of Koopman eigenvalues are computed (which can be achieved, even for systems with a large number of observables, by performing dimensionality reduction or residual based pruning of modes ), the Wasserstein distance can be efficiently computed by using linear sum assignment.

## 4 Results

### Identifying conjugate optimizers

We begin by validating that numerical approximations of KMD can indeed correctly identify conjugacies in settings relevant to DNN training. To do this, we consider a recently discovered nonlinear topological conjugacy between the optimization dynamics of Online Mirror Descent (OMD) and Online Gradient Descent (OGD) [24; 25; 26] (Appendix A.1). This work has been of particular interest as OMD occurs on a convex loss landscape and OGD occurs on a non-convex loss landscape, suggesting a potential route for studying behavior of OGD in a simpler setting.

The conjugacy between OMD and OGD relies on a reparametrization of the loss function. Without prior knowledge of this reparametrization, it is challenging to identify the conjugacy by looking at only the training trajectories or the losses (Fig. 2A, B - see Appendix A.3 for details on implementation of OGD and OMD). This highlights some of the current challenges present in identifying dynamical equivalence from data.

We compute the KMD associated with optimization using OMD and OGD by considering trajectories of both, from many initial conditions, and compare the resulting Koopman eigenvalues (Appendix A.4). We find high overlap between the two spectra (Fig. 2C, red and black dots). Additionally, the two sets of eigenvalues have the same structure. Namely, they consist only of real, positive eigenvalues. In contrast, the bisection method (BM), another optimization algorithm that is not conjugate to OMD or OGD (Appendix A.2), has associated spectra that are complex (Fig. 2C, light blue dots). Performing a randomized shuffle of the eigenvalues between algorithms (Appendix B), we find that \(25\%\) of the shuffles between OMD and OGD eigenvalues result in Wasserstein distance greater than or equal the true Wasserstein distance. This suggests the distributions are not statistically significantly distinct. However, \(0\%\) of the shuffles have Wasserstein distance greater than or equal to the true Wasserstein distance for OMD and BM, and OGD and BM, respectively. This provides evidence that the spectra of OMD/OGD and BM are statistically significantly distinct.

Similar results are obtained when applying KMD to OMD and OGD optimization of a different function (Fig. S1). Collectively, these results demonstrate that the Koopman-based spectral identification of topological conjugacies can successfully recover a known equivalence and provide support that it can be used more broadly in uncovering equivalences in DNN training dynamics.

### Identifying the effect of width on fully connected neural network training

To start exploring the potential of our framework for identifying topological conjugacies in DNN training, we begin with a small-scale example. Namely, we consider a fully connected neural network (FCN) with only a single hidden layer, trained on MNIST (Appendix C.1). Consistent with other architectures, we find that the wider the FCN (i.e., the more units in the hidden layer), the better the performance and the lower the loss (Fig. 3A). Whether this is due to an increase in capacity, with more hidden units enabling a more refined solution, or whether this is due to a change in the training dynamics, leading to a better traversal of the loss landscape, is - at this point - unclear.

Computing the Koopman eigenvalues associated with training FCNs of varying width (Fig. 3D - see Appendix C.2 for details), we find that narrow (\(h=5\)) and wide (\(h=40\)) FCNs have training dynamics that are non-conjugate, as their Koopman spectra are non-overlapping (Fig. 3E, F). This suggests that the training dynamics undergo a fundamental change as width increases. However, for FCNs with intermediate width (\(h=10\)), the training dynamics are more aligned with the wide FCNs (Fig. 3E, F), suggesting conjugate dynamical behavior. The dynamical difference in training narrow and wide FCNs is also supported by performing the eigenvalue shuffle analysis (Appendix B).

Figure 2: **Conjugacy between online mirror descent and online gradient descent is identifiable from Koopman spectra.** (A) Comparing example trajectories of variables optimized via OMD \((x_{1},x_{2})\), OGD \((u_{1},u_{2})\), and BM \((z_{1},z_{2})\), the existence of a conjugacy between OMD and OGD is not obvious. (B) Similarly, the existence of a conjugacy is not apparent when looking at the loss incurred by using OMD and OGD. (C) Comparing the Koopman eigenvalues associated with optimizing using OMD, OGD, and BM correctly identifies the existence of a conjugacy between OMD and OGD, and the lack of a conjugacy between OMD/OGD and BM. The function optimized is in all subfigures is \(f(x)=(x)\).

In particular, a much larger number of the shuffles between \(h=10\) and \(h=40\) eigenvalues have Wasserstein distance greater than or equal to the true Wasserstein distance than between \(h=5\) and \(h=40\) (\(81\%\) vs. \(55\%\)), although significance is not reached. Similar results were found when using the GeLU instead of ReLU activations  (Fig. S3), demonstrating that our results are consistent across FCNs with similar activation functions. Thus, we conclude that the additional improvement in performance observed when increasing the network width from \(h=10\) to \(h=40\) comes more from an increase in capacity, than from a change in training dynamics. Identifying this was not possible by solely comparing the loss or weights (Fig. 3A-C), demonstrating the advantage of the Koopman-based approach for identifying equivalent and non-equivalent DNN training dynamics.

To further study the behavior of FCN training dynamics, we also compared the computed Koopman spectra of \(h=40\) networks trained from different random initial conditions (Appendix C.4). Prior work has proven that different random initializations of sufficiently wide FCNs converge to local minima that have no loss barrier along the linear interpolation between them, when taking into account permutation symmetry . This suggests conjugate training dynamics, although this has not been explicitly shown. In support of this hypothesis, we find examples of FCNs, trained from different random initializations, with nearly identical Koopman spectra (Fig. S4A).

### Identifying dynamical transitions in convolutional neural network training

Prior work has argued that CNNs undergo transitions in their training dynamics during the early part of training (i.e. the first several epochs), and that these transitions are similar across different CNN architectures . However, dynamical systems based methods were not used for analysis. Instead, this observation relied on coarse-grained observables (e.g., training loss, magnitude of gradients) to define the transitions and to determine when they occur.

To understand whether such results hold when considering the training dynamics at a finer-scale, we utilize our Koopman-based framework. To do this, we split the first epoch of training into windows of 100 training iterations. We compute the Koopman eigenvalues associated with dynamics that occur in each window and denote them by \(_{t_{1}:t_{2}}\), where \(t_{1}<t_{2}\) are the first and last training iteration in the window. We then measure the Wasserstein distance between all combinations of pairs of eigenvalues. This enables us to quantitatively assess transient dynamical behavior and identify when

Figure 3: **Narrow and wide fully connected neural networks have non-conjugate training dynamics.** (A) Training loss curves for FCNs with hidden layer widths \(h=5,10,\) and \(40\). Solid line is mean and shaded area is \(\) standard deviation across \(25\) independently trained networks. (B), (C) Example weight trajectories, across training iterations, for narrow, intermediate, and wide FCNs. (D) Koopman eigenvalues associated with training FCNs of varying width. (E) Same as (D), but zoomed out and with the eigenvalues associated with \(h=5\) and \(h=10\) compared to those associated with \(h=40\). Dashed line in (D) and (E) denotes unit circle. (F) Wasserstein distance between Koopman eigenvalues associated with training FCNs of varying width. Error bars are \(\) standard deviation across \(25\) independently trained FCNs. Kolmogorov–Smirnov (KS) tests were performed to assess statistical significance of distance: \(*\) denotes \(p<0.01\) and \(***\) denotes \(p<0.0001\).

in the early phase of training the dynamics transition from one equivalence class to another. We apply our approach to LeNet , a simple CNN trained on MNIST, and ResNet-20 , trained on CIFAR-10 (see Appendix D.1 for details).

We find that, for both LeNet and ResNet-20, the first 100 training iterations have the most distinct Koopman eigenvalues, as the Wasserstein distance between \(_{0:99}\) and all other eigenvalues is large (Fig. 4A, B - bright yellow first column and row). In addition, for both LeNet and ResNet-20, the training dynamics become similar after \(700\) training iterations, as the Wasserstein distance between \(_{600:699}\) and \(_{700:799}\) is small (Fig. 4A, B - dark blue square around diagonal in lower righthand corner). This is in agreement with the timeline found by Frankle et al. (2020) . However, we additionally find that the dynamics that occur between 100 and 700 training iterations exhibit greater change for ResNet-20 than for LeNet, as there is a larger Wasserstein distance between Koopman eigenvalues. This suggests a difference in dynamical behavior between the architectures. By examining the Koopman eigenvalues associated with different training iteration windows, we find non-overlapping spectra (Fig. 4C). Performing the eigenvalue shuffle analysis (Appendix B), we find evidence that the first 4 splits of 100 training steps have statistically significant different associated Koopman eigenvalues, as the \(2\%\), \(2\%\), \(0\%\), and \(4\%\) of the shuffles had Koopman eigenvalues greater than or equal to the true Wasserstein distance. This suggests a lack of topological conjugacy between the earliest training dynamics of ResNet-20 and LeNet, despite the fact that the general timeline in transitions in dynamics is similar between the architectures.

To understand how the training dynamics change over a larger span of training time, we perform the same analysis, but computing the Koopman eigenvalues from the dynamics that occur during each epoch (Appendix D.3). We find that, at this coarser grain scale, both architectures see a similar evolution of their training dynamics. In particular, we find that the first epoch has the most distinct dynamics (Fig. S5A, B - yellow first column and row), and the subsequent epochs have dynamics that become increasingly more similar (Fig. S5A, B - increasing size of dark blue blocks centered on the diagonal).

Taken together, our Koopman-based analysis supports prior decomposition of the early phase of CNN training dynamics into regimes separated by transitions that occur after a similar number of training iterations across architectures . However, with a finer-scale resolution of the dynamics, we additionally find that LeNet and ResNet-20 have non-conjugate training, demonstrating that the exact training dynamics are architecture-specific.

Figure 4: **Koopman-based framework enables identification of transitions in dynamics during the early phase of training for LeNet and ResNet-20.** (A) Log\({}_{10}\) Wasserstein distance between Koopman eigenvalues associated with LeNet training over windows of 100 training iterations during epoch 1. (B) Same as (A), but for ResNet-20 training. (C) Koopman eigenvalues associated with the dynamics that occur during training iterations intervals 0–99, 400–499, and 600–699. Dashed line denotes the unit circle.

### Identifying non-conjugate training dynamics for Transformers that do and do not grok

Since the discovery that Transformers trained on algorithmic data (e.g., modular addition) undergo delayed generalization ("grokking" - Fig. 5A) , considerable effort has been invested to understand how this arises. One particularly influential theory is that the norm of the weights at initialization plays a large role. In particular, it was shown that single layer Transformers, initialized at weights with a sufficiently small norm, have training and test loss landscapes that are "aligned", while the same single layer Transformers, initialized at weights with a sufficiently large norm, have training and test loss landscapes that are "mis-aligned" . Constraining the norm of the weights to be small prevents grokking, with train and test accuracy increasing at similar training iterations (Fig. 5B) .

What role the training dynamics play in grokking remains less understood. In particular, the extent to which constraining the weight norm changes the training dynamics (which could shed additional light on grokking) has yet to be explored. We therefore compute the Koopman eigenvalues associated with the training of constrained and unconstrained Transformers on modular addition (Appendix E). We use the dynamics from the earliest part of training, namely the first 100 training iterations (Fig. 5C). We do this to avoid trivially seeing a difference, given the small weight changes that Transformers which undergo grokking make when the training accuracy is high.

We find that the Koopman eigenvalues are distinct (Fig. 5D). In addition to a gap between the computed eigenvalues, we find that the dynamics associated with training the constrained Transformer has a pair of complex conjugate eigenvalues that lie along the unit circle, whereas the dynamics associated with training the unconstrained Transformer has a pair of complex conjugate eigenvalues outside of the unit circle. This suggests a difference in stability properties, as Koopman eigenvalues with magnitude greater than \(1\) (i.e. those that lie outside the unit circle) correspond to unstable dynamics. Similar results were found when computing the Koopman eigenvalues associated with the training of the unconstrained Transformer over a longer training time window (Fig. S6).

These results suggest a non-conjugacy in the training dynamics of Transformers that do, and those that do not undergo grokking. In particular, constraining the weight norm appears to lead to more stable training dynamics, which may be due to the selection of a better subnetwork to train . Additionally, these results suggest that it may be possible to identify grokking before it happens .

Figure 5: **Transformers that do, and that do not undergo grokking have early training dynamics that are not conjugate.** (A) Train and test loss, as a function of training steps, for a Transformer model that undergoes grokking. (B) Same as (A), but for a Transformer whose training is constrained to have a constant weight norm . In this case, no grokking is observed. (C) In the first 100 training steps, little difference is seen between the test loss of Transformers with and without constrained training. Lines are mean and shaded area is \(\) standard deviation across 20 independently trained networks. (D) Koopman eigenvalues associated with the dynamics that occur over the first 100 training iterations for Transformers that do, and that not undergo grokking.

Discussion

Motivated by the need for quantitative methods that can determine when the training dynamics of DNN parameter trajectories are equivalent, we utilized advances in Koopman operator theory to develop a framework that can identify topological conjugacies. This Koopman based identification of conjugate training dynamics is invariant to permutations of DNN parameters (Eq. 5), a necessary feature for methods used to study DNN training dynamics [15; 16; 17]. By applying our approach to the dynamics associated with optimization using OMD and OGD, we were able to validate that numerical implementations of KMD can identify conjugacies which are known to exist [24; 25; 26] (Fig. 2C). Additionally, this example demonstrates challenges existing approaches for comparing DNN training dynamics face, as comparing the losses and the parameter evolutions of OMD and OGD does not lead to a clear indication of the underlying equivalence (Fig. 2A, B).

Leveraging our Koopman-based approach on the training dynamics of DNNs of varying architectures led to several insights. First, we found evidence that shallow and wide FCNs have non-conjugate training dynamics (Fig. 3). This is consistent with theoretical and experimental work showing that FCN width can lead to lazy and rich training regimes . This provides further evidence that our Koopman-based approach can correctly identify equivalent and non-equivalent training dynamics. In addition, we find that FCNs of intermediate width have Koopman eigenvalues that are more similar to those of wide FCNs (Fig. 3), demonstrating that our approach can provide insight beyond the wide and shallow regimes. Second, applying our framework to the dynamics of CNNs, we found transitions in the dynamics during the early phase of training, consistent with prior work  (Fig. 4). However, by closely examining the Koopman eigenvalues, we found non-conjugate dynamics between different CNN architectures, suggesting fundamental differences in training. These distinct dynamical features are aligned with previous observations of different behaviors when training sparse CNNs [11; 38]. And third, we found that Transformers that do, and that do not undergo grokking have non-conjugate training dynamics (Fig. 5). By focusing on the early phase of Transformer training, we avoid trivially finding this due to differences in the training loss. Additionally, this provides evidence for the ability to anticipate grokking before it happens .

Our framework is similar in spirit to an approach that categorizes iterative algorithms from a controlled dynamical systems perspective [65; 66]. However, such an approach requires access to the underlying equations to identify equivalence classes, which our data-driven, Koopman based framework does not [67; 68]. Work concurrent to ours has leveraged a similar approach to study the dynamics of recurrent neural networks . However, Ostrow et al. (2023)  studied the dynamics of the activations and not the dynamics of network parameters, which is the focus of this paper.

**Limitations.** Numerical implementations that compute the KMD are only approximations to the action of the true Koopman operator. As such, they are subject to the same difficulties as other data-driven approaches. These include the selection of hyper-parameters associated with the construction of the Koopman operator, the choice of observable functions, and the number of modes considered. To mitigate the effect these limitations might have on our analysis, we used DMD-RRR, a state-of-the-art numerical approach for KMD , and time-delayed observables, which have been found to provide robust results across a range of domains [44; 45; 51]. Determining the existence of a topological conjugacy between two dynamical systems requires assessing whether their associated Koopman eigenvalues are sufficiently similar. While in some cases this is clear (e.g. identical Koopman eigenvalues associated with optimization using OMD and OGD - Fig. 2, distinct Koopman eigenvalues associated with training LeNet and ResNet-20 - Fig. 4), in other cases it is less apparent. To quantify these differences, we made use of the Wasserstein distance and attempted to compute significance with a randomized shuffle control. While a natural choice, additional work remains to connect the magnitude of the Wasserstein distance to the divergence of the dynamical properties associated with training DNN models.

**Future directions.** The ability of our Koopman-based approach to identify conjugacies between iterative optimization algorithms suggests its potential for data-driven discovery and generation of new classes of algorithms [70; 71; 72]. By identifying equivalent training dynamics of DNNs, it may be possible to use our approach for learning mappings that transform one DNN model to another . Finally, the characterization of the Koopman eigenvalues associated with training a wide range of DNN models varying in architecture, optimization hyper-parameters, and initialization will enable a detailed understanding of how these properties shape DNN training. Leveraging this understanding may lead to improved methods for DNN training and model development.