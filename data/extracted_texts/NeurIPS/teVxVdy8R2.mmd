# Prediction with Action:

Visual Policy Learning via Joint Denoising Process

Yanjiang Guo\({}^{12*}\), Yucheng Hu\({}^{13*}\), Jianke Zhang\({}^{1}\), Yen-Jen Wang\({}^{14}\), Xiaoyu Chen\({}^{12}\),

**Chaochao Lu\({}^{3}\), Jianyu Chen\({}^{12}\)**

\({}^{*}\)Equal Contribution \({}^{}\)Corresponding Author

\({}^{1}\)IIIS, Tsinghua University \({}^{2}\)Shanghai Qizhi Institute

\({}^{3}\)Shanghai AI Lab \({}^{4}\)University of California, Berkeley

{guoyj22, huyc24}@mails.tsinghua.edu.cn

###### Abstract

Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilities--image prediction and robotic action, respectively--they technically follow a similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce **PAD**, a novel visual policy learning framework that unifies image **P**rediction and robot **A**ction within a joint **D**enoising process. Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. PAD outperforms previous methods, achieving a significant 26.3% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0% success rate increase compared to the strongest baseline. Project page at https://sites.google.com/view/pad-paper.

## 1 Introduction

Making predictions and taking actions are critical human capabilities, allowing individuals to foresee the change of their surroundings and behave appropriately in response . Despite prediction and action seeming like two distinct abilities, they are highly coupled since they share the same

Figure 1: Multi-task performance comparisons in two domains.

underlying physical laws of the world . Understanding these laws enables humans to make better predictions and actions.

Recently, diffusion models [4; 5; 6] have achieved impressive success in visual generation tasks by training on extensive web-scale image and video datasets [7; 8; 9]. For example, image editing models can predict outcomes based on user instructions [10; 11; 12], while video generation models can generate sequences of future images [13; 14; 15], representing a good understanding of the physical world. On the other line, diffusion models have also shown efficacy in robotic control tasks by denoising actions conditioned on robot observations, known as diffusion policy . Although the diffusion generative model and diffusion policy serve different functions across two domains, we believe that the capability for image prediction could significantly enhance robot policy learning, as they share the same fundamental physical laws. Previous works [17; 18; 19] have employed the image-editing model in an off-the-shelf manner by first synthesizing a goal image and subsequently learning a goal-conditioned policy. However, this two-stage approach separates the prediction and action learning process, neglecting deeper connections between prediction and action. In this way, actions do not leverage the pre-trained representations in the prediction models which encode rich knowledge of the physical world.

In this paper, we introduce the **P**rediction with **A**ction **D**iffuser (**PAD**), a unified policy learning framework that integrates prediction and action under the same diffusion transformer (DiT) architecture . Specifically, we utilize the diffusion transformer model to seamlessly merge all modality inputs and simultaneously predict future images and actions via joint denoising, as illustrated in Figure 2(c). Additionally, the flexible DiT backbone also allows PAD to be co-trained on large-scale video data and extended to other robotic modalities, such as depth images. We have conducted extensive experiments on the MetaWorld Benchmark  as well as real-world robot arm manipulation tasks, demonstrating the efficacy of our approach, as shown in Figure 1. Our key contributions are:

* We propose a novel policy learning framework, Prediction with Action Diffuser (PAD), to predict futures and robot actions through a joint denoising process, benefiting policy learning for robotic tasks.
* The proposed PAD framework enables co-training of different datasets containing different modalities, allowing encoding rich physical knowledge from various data sources.
* We outperform previous methods with a clear margin in the Metaworld benchmark, surpassing baselines with a 26.3% relative improvement in success rate using a single visual-language conditioned policy. Furthermore, our method outperforms all baselines in the real-world robot manipulation experiments and can better generalize to unseen tasks.

## 2 Preliminaries

**Problem Statement.** We consider pixel-input language-conditioned robotic control under the imitation learning setting. We denote a robotic dataset \(D_{robot}=\{_{1},_{2},..._{n}\}\) comprising \(n\) demonstrations. The \(i^{th}\) demonstration \(_{i}=(I_{i},l_{i},_{i})\) contains a natural language instruction \(l_{i}\), a

Figure 2: Diffusion models have achieved impressive success in visual generation tasks (a) and visual-motor control tasks (b). Image prediction and robot action are actually highly correlated since they share the same underlying physical dynamics. The PAD framework predicts the future and generates actions in a joint denoising process.

sequence of pixel inputs \(I_{i}\), and a robot trajectory \(_{i}\) consisted of a sequence of robot poses \(p_{i}^{1:T}\). However, since collecting robotic data is risky and costly, the scale of \(D_{robot}\) will be limited. We therefore also consider the RGB video dataset \(D_{video}\) which is easily accessible on the Internet. An instance in \(D_{video}\) can be represent as \(_{j}=(I_{j})\). Although \(D_{video}\) lacks robot action data, our proposed PAD framework enables co-training on both robotic dataset \(D_{robot}\) and video dataset \(D_{video}\), leveraging the large-scale \(D_{video}\) data to enhance visual policy learning.

**Latent Diffusion models.** The core idea of diffusion models is to continuously add Gaussian noise to make a sample a Gaussian and leverage the denoising process for generating data . Let \(z_{0}=(x_{0})\) denote a latent sample encoded from real data. The noising process gradually adds normal Gaussian noise (\(\)) to \(z_{0}\) over \(T\) steps, resulting in a set of noisy samples \(Z=\{z_{t}|t[1,T]\}\), which is equivalent to sampling from the following distribution: \(q(z_{t}|z_{t-1})=(z_{t};}z_{t-1},(1-_{t}) )\), where \(\{_{t}|t[1,T]\}\) are predefined hyper-parameters that control the amplitude of the noise. Let \(_{t}=_{i=1}^{t}_{i}\), and according to DDPM , \(z_{t}\) can be directly obtained by adding a Gaussian noise \(_{t}\) to \(z_{0}\): \(z_{t}=_{t}}z_{0}+_{t}}_{t}\). Further, the denoising process starts with the most noisy latent sample \(z_{T}\), and progressively reduces the noise to recover the real sample \(z_{0}\) with condition \(c\). It is based on a variational approximation of the probabilities \(q(z_{t-1}|z_{t},c)\) given by:

\[p(z_{t-1}|z_{t},c) =(z_{t-1};_{t-1}}_{}(z_{t}, t,c),(1-_{t-1}))\,,\] (1) \[_{}(z_{t},t,c) =(z_{t}-_{t}}_{}(z_{t},t,c))/ _{t}}.\]

The noise estimator \(_{}(z_{t},t,c)\) is implemented as a neural network and is trained to approximate the gradient of the log-density of the distribution of noisy data ., that is:

\[_{}(z_{t},t,c)-_{t}}_{z_{t}}  p(z_{t}|c).\] (2)

## 3 PAD: Prediction with Action via Joint Denoising Process

### Overview of PAD

**Multi-modalities Generation.** In this section, we introduce our PAD framework, which concurrently predicts future frames and actions within a joint latent denoising process. We primarily focus on the RGB image modality \(M_{I}\) and the robot action modality \(M_{A}\). Each robot action can be characterized by a robot pose that includes the position and rotation of the end-effector, as well as the gripper status. Notably, this framework can easily extend to extra modalities \(M_{E}\). For instance, we additionally incorporate the depth image modality in the experiment part, which provides a more accurate measure of distances.

**Conditional Generation.** In the proposed PAD framework, predictions and actions are conditioned on multi-modality current observations, which include RGB images \(c_{I}\), robot pose \(c_{A}\), an additional depth map \(c_{E}\) (in Real-World tasks), and natural language instruction text \(l\). The framework simultaneously outputs the corresponding future predictions \(x_{I},x_{E}\) and robot action \(x_{A}\). Rather than predicting a single future step, PAD can forecast \(k\) future steps \(x_{I}^{1:k},x_{A}^{1:k},x_{E}^{1:k}\), which can be viewed as \(k\) step planning of the robot. Only the first predicted action \(x_{A}^{1}\) is executed by the robot, which then triggers a new prediction cycle. This iterative prediction and execution process allows the robot to continuously plan and act in a closed-loop manner. The implementation details are discussed further in the subsequent section.

### Model Architectures

**Model Input Process.** Given that the original data may come in various formats with high dimensions, we first map all modalities to a latent space and undertake a latent diffusion process. Following the process in , the RGB image \(x_{I}\) is initially processed through a pre-trained, frozen VAE encoder \(_{I}\) to derive the latent representation \(_{I}(x_{I})\). This latent representation is then converted into a sequence of tokens \(t_{f}\) with embedding size \(h\) via tokenizer. Similarly, the robot pose \(x_{A}\) is encoded using a Multi-Layer Perceptron (MLP)  into \(_{A}(x_{A})\) and linearly transformed into tokens \(t_{A}\) with the same embedding size \(h\). If available, the depth image is downsampled and tokenized into \(t_{E}\). The natural language instruction is processed through a frozen CLIP encoder  to produce the text embedding \(c_{l}\).

**Diffusion Transformer (DiT) Backbone.** We have adopted the Diffusion Transformer (DiT)  as our model backbone, which offers several advantages over the U-net backbone commonly used in previous works [18; 17]. Notably, the DiT architecture efficiently integrates various modalities via the self-attention mechanism. Inputs such as RGB images, robot poses, and additional data are transformed into token sequences \(t_{I},t_{A},t_{E}\) with lengths \(T_{I},T_{A},T_{E}\), respectively. These token sequences from different modalities are concatenated and undergo a joint latent denoising process

Furthermore, the DiT architecture is adaptable to missing modalities. For example, in the case of a video dataset that lacks robot actions, the input to DiT only comprises the image tokens \(t_{I}\). We simply extend the token sequence to the combined length \(T_{I}+T_{A}+T_{E}\) and introduce an attention mask in the self-attention block to exclude the padding tokens. Only effective predictions are retained in the output, discarding any padded parts. A brief illustration of the whole process is depicted on the right side of Figure 3. This design choice enables PAD to be concurrently trained on both RGB-only video datasets and robotic datasets.

**Joint Conditional Generation.** We initialize future observations as white noise and aim to reconstruct future observation frames and desired robot action, conditioning on current observations \(c_{I},c_{A},c_{E}\). Following a similar strategy as in , we concatenate conditional latent and noise latent in the channel dimension. Specifically, after obtaining encoded latent \(_{I}(c_{I}),_{A}(c_{A}),_{E}(c_{E})\), we concatenate these latent with noise to obtain conditioned noised latent \(L_{I}=[_{I}(c_{I}),z_{t}^{I}],L_{A}=[_{A}(c_{A}),z_{t}^{ A}],L_{E}=[_{E}(c_{E}),z_{t}^{E}]\). For instance, if the encoded latent \(_{I}(c_{I})\) has a shape of \(c d d\), then \(z_{t}^{}\) would have a shape of \(kc d d\) to represent \(k\) future frames, resulting in the final latent \(L_{I}\) having a shape of \((k+1)c d d\). The other modalities undergo a similar process.

After concatenating the latent, these conditioned noisy latent from different modalities are tokenized into sequences of tokens \(t_{I},t_{A},t_{E}\) with the same embedding size. The tokenization of image latent \(L_{I}\) follows a patchify process same to , while the tokenization of robot pose employs a simple linear projection. Finally, these tokens are fed into multiple layers of DiT to predict the latent representation of future frames. An illustration of the overall process can be found in Figure 3.

### Training Process

**Initialization.** Following the initialization process in , we also initialize the PAD weights from the DiT model pre-trained on ImageNet for the image generation task conditioned on class . However, we can not directly load the model since we have missing or incompatible model parameters. We discard the label embedding layers in DiT and zero-initialize new layers for text embedding, we replicate the weight of the image latent tokenizer for \(k+1\) times to encode the stacked latent, and the encoder and decoder for robot state are also zero-initialized.

Figure 3: Visualization of the PAD framework. Current observations in different modalities are first encoded into latent and concatenated with white noise channel-wise. These noised latent are then tokenized into tokens and perform a joint denoising process to predict the images and robot actions simultaneously. PAD can flexibly accommodate extra or missing modal inputs through a masked-attention mechanismTraining Objective.The diffusion process adds noise to the target encoded latent \(\{_{I}(x_{I}),_{A}(x_{A}),_{E}(x_{E})\}\) and results in noised latent \(Z_{I,A,E}=\{z_{t}^{I},z_{t}^{A},z_{t}^{E}\}\). We train the PAD model to simultaneously predict the noise \(^{I},^{A},^{E}\) added to the sample data, conditioned on current observations \(C_{I,A,E}=\{c_{I},c_{A},c_{E}\}\) and instructions \(l\). This denoiser is trained with the DDPM  loss:

\[^{}_{diff}()=_{^{}(0,1),t,C,l}[||^{}-^{}_{}(z _{t}^{},t,C,l)||_{2}^{2}],\] (3)

where \(\{I,A,E\}\) represents different types of input modalities. The denoising loss \(^{}_{diff}\) aims to maximize the evidence lower bound (ELBO)  while approximating the conditional distribution \(p(_{}(x_{})|C,l)\). We jointly minimize the following latent diffusion objectives and use hyperparameters \(_{I},_{A},_{E}\) to balance the prediction loss between different modalities. Formally, the final training objective is given by:

\[()=_{I}^{I}_{diff}+_{A}^ {A}_{diff}+_{E}^{E}_{diff}.\] (4)

## 4 Experiments

In this section, we conduct a series of experiments on the simulated Metaworld Benchmark  and a real-world table manipulation suite, utilizing our joint prediction framework. We aim to answer the following questions:

* Can PAD enhance visual policy learning through joint prediction and action with limited robotic data?
* Can PAD benefit from co-training on large-scale internet video datasets and better generalize to unseen tasks?
* Can scaling up computational resources improve PAD's performance?

### Environmental Setups and Baselines

**Metaworld.** Metaworld  serves as a widely used benchmark for robotic manipulation, accommodating both low-level feature and pixel input modalities. Previous studies that utilized pixel input generally developed separate task-specific policies for each of the 50 tasks. In contrast, our approach demonstrates a significant advancement by employing a single text-conditioned visual policy to address all 50 tasks, within a data-efficient imitation learning framework. We collected 50 trajectories per task, consistently using the "corner2" camera viewpoint and recording the robot's pose with 4-dimensional states that include end-effector position and gripper status. For a fair comparison, we do not utilize an additional depth input in Metaworld.

**Real-World Panda Manipulation Tasks.** Our real-world experiments involve a Panda arm performing diverse manipulation tasks such as pressing buttons, opening drawers, routing cables, and picking and placing with various objects, as shown in Figure 4. We follow the same hardware setup described in SERL  and utilize a wrist-mounted camera for pixel input . The robot's poses are represented by 7-dimensional vectors, including 3 end-effector positions, 3 rotation angles, and 1

Figure 4: We learn a single vision-language conditioned policy to solve all tasks in each domain with limited demonstrations, co-training with the bridge video data. In simulated MetaWorld, we learn a policy to tackle all 50 tasks. In real-world panda manipulations, we split objects into seen objects and unseen new objects to test the generalization ability of our policy.

gripper status dimension. We collected 200 trajectories per task through teleoperation using a space mouse and scripted commands. Similarly, we developed a single policy capable of addressing all tasks, conditioned on instructions. We also assessed the policy's generalization capabilities on unseen tasks, as depicted in Figure 5.

**Policy Training Details.** As detailed in Section 3, the flexible PAD framework can be co-trained on various internet RGB video data and robotic demonstrations. In order to save computational resources and avoid the need to co-train the model from scratch in each robot domain, we first pre-train the model on internet data to establish better image prediction priors. We then adapt this pre-trained model to various robotic domains, including the simulated Metaworld and the real-world panda manipulation. Empirically, we first pretrain 200k steps on the BridgeData-v2 dataset , which consists of 60,000 trajectories. After this, we adapted the model to each domain, continuing training for an additional 100k steps with robotic demonstrations. The pre-training and adaptation stage requires approximately 2 days and 1 day, utilizing 4 NVIDIA A100 GPUs.

Moreover, we found that increasing the weight of the image prediction loss during the early adaptation stages accelerates convergence, as image priors are already established in the pre-trained models. Specifically, we maintained the image prediction loss coefficient \(_{I}\) at 1.0 throughout the training period and linearly increased \(_{A}\) and \(_{E}\) from 0.0 to 2.0 during the 100k training steps.

**Policy Execution Details.** Our policy is conditioned on the current image, \(c_{I}\), and the robot pose, \(c_{A}\), and predicts \(k\) frames of futures and actions. We configure the prediction horizon at \(k=3\) and set the interval between frames at \(i=4\) for both Metaworld and real-world tasks. During policy execution, we utilize 75 steps of DDIM sampling  to denoise the \(k\) steps of future images, \(x_{I}^{1:K}\), and actions, \(x_{A}^{1:k}\). These \(k\) step predictions can be viewed as \(k\) step planning and only the first predicted action, \(x_{A}^{1}\), is executed by the robot. The robot then moves to the first desired pose using a simple linear interpolation motion planner, triggering the next prediction cycle.

**Comparisons.** Visual policy learning has been widely explored in previous studies. In our experiments, we opted to compare against a representative subset of prior methods that have either achieved state-of-the-art performance or share a similar architecture with our methods. Notably, all methods are trained on all tasks in the domain **using a single text-condition visual policy**.

* **Diffusion Policy .** A novel visual control policy that generates robot actions through an action diffuser. We augmented the original diffusion policy model with instruction conditions to address the multi-task setting. We use the CLIP encoder  as instruction encoders, referring to related work .

    &  **button-** \\ **press** \\  &  **button** \\ **topdown** \\  &  **drawer-** \\ **open** \\  &  **door-** \\ **open** \\  &  **faucet-** \\ **close** \\  &  **plate-** \\ **slide** \\  &  **reach-** \\ **valid** \\  &  **window-** \\ **open** \\  &  **window-** \\ **close** \\  & 
 **door-** \\ **lock** \\  \\ 
**Diffusion Policy** & 0.92 & 0.16 & 0.36 & 0.32 & 0.76 & 0.60 & 0.72 & 0.60 & 0.36 & 0.12 \\
**SuSIE** & 0.96 & 0.32 & 0.60 & 0.68 & 0.56 & 0.68 & 0.92 & 0.68 & 0.96 & 0.32 \\
**RT-1** & 0.88 & **1.00** & 0.56 & 0.56 & **1.00** & 0.08 & 0.12 & **1.00** & **1.00** & 0.00 \\
**RT-2* & **1.00** & 0.84 & 0.92 & 0.96 & 0.96 & **0.88** & 0.76 & **1.00** & 0.96 & 0.40 \\
**GR-1** & **1.00** & 0.84 & **1.00** & **1.00** & 0.96 & **0.88** & **1.00** & **1.00** & **1.00** & 0.60 \\
**PAD (ours)** & **1.00** & 0.92 & **1.00** & **1.00** & 0.92 & 0.72 & **1.00** & 0.92 & **1.00** & **0.88** \\ 
**PAD w/o img** & **1.00** & 0.92 & **1.00** & 0.88 & 0.92 & 0.16 & 0.92 & **1.00** & **1.00** & 0.12 \\
**PAD w/o co-train** & **1.00** & 0.92 & **1.00** & 0.92 & 0.92 & 0.48 & 0.92 & 0.96 & 0.96 & 0.72 \\  
**Harder Tasks** &  **assem-** \\ **ble** \\  &  **basket-** \\ **ball** \\  &  **offee-** \\ **pull** \\  &  **hammer** \\ **inner** \\  &  **peg-** \\ **insert** \\  &  **pick-** \\ **-wall** \\  &  **glest-** \\ **place** \\  &  **skelt-** \\ **plase** \\  &  **stick-** \\ **push** \\  &  **stick-** \\ **pull** \\  & 
 **Average** \\ **(50tasks)** \\  \\ 
**Diffusion Policy** & 0.20 & 0.08 & 0.00 & 0.08 & 0.16 & 0.36 & 0.00 & 0.00 & 0.00 & 0.279 \\
**SuSIE** & 0.40 & 0.24 & 0.32 & 0.04 & 0.24 & 0.24 & 0.08 & 0.16 & 0.16 & 0.410 \\
**RT-1** & 0.00 & 0.00 & 0.08 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.346 \\
**RT-2* & 0.24 & 0.08 & 0.68 & 0.20 & 0.12 & 0.32 & 0.20 & 0.12 & 0.00 & 0.522 \\
**GR-1** & 0.64 & 0.08 & 0.52 & 0.48 & 0.24 & 0.48 & 0.28 & 0.60 & 0.44 & 0.574 \\
**PAD (ours)** & **0.88** & **0.84** & **0.80** & 0.68 & **0.92** & **0.72** & **0.96** & **0.88** & **0.725** \\ 
**PAD w/o img** & 0.04 & 0.44 & 0.40 & 0.48 & 0.16 & 0.36 & 0.16 & 0.24 & 0.16 & 0.436 \\
**PAD w/o co-train** & 0.32 & 0.28 & 0.32 & 0.72 & **0.92** & 0.68 & 0.56 & 0.88 & 0.40 & 0.592 \\   

Table 1: Comparisons on Metaworld benchmark. We utilize a single policy to solve all 50 tasks in Metaworld. Due to the space limit, we show a subset of tasks and the average success rate on all 50 tasks. Detailed data can be found in Appendix A.4.

* **SuSIE .** A two-stage approach that utilizes a pre-trained image-editing model  to generate image goals for robotic tasks, followed by a goal-conditioned low-level diffusion policy. We fine-tune the image-editing diffusion model on the same dataset and also use the diffusion policy for goal-conditioned behavioral cloning. To ensure a fair comparison, we also use the more powerful DiT framework as the image-editing model.
* **RT-1 .** An end-to-end robot control policy that leverages FiLM-conditioned  EfficientNet  to fuse visual input and language input, then followed by transformer blocks to output action.
* **RT-2*  (re-implement).** A large-scale embodied model that directly fine-tunes vision-language models(VLMs) to produce robot actions. The original RT-2 model was fine-tuned on the PaLM model , which is not publicly available. Following the specifications outlined in the original paper, we re-implemented the RT-2 model using the InstructBlip-7B  backbone.
* **GR-1 .** Method that also leverages image prediction to assist policy learning. Different from PAD, they generate images and actions via auto-regressive architecture.

### Main Results

**Performance Analysis.** In all comparisons, we train a single visual policy to address all tasks within a domain, conditioned on instructions. Our proposed PAD outperforms all baselines by a significant margin. As shown in Table 1, in the Metaworld benchmark, PAD achieved an average success rate of 72.5%, which markedly surpasses the strongest baseline at 57.4%. Due to space constraints, we present comparisons on a subset of tasks and report the average success rate across all 50 tasks. A comprehensive comparison of all 50 tasks is available in Appendix A.4. Furthermore, Table 2 shows the results in real-world seen-tasks where PAD also attains the highest success rate.

We notice that PAD predicts more precise future images than the GR-1 method (Figure 6), likely due to the superior capabilities of diffusion models in image generation tasks. These precise images may more effectively facilitate policy learning, leading to higher success rates, particularly in tasks requiring precise and accurate operation such as picking small blocks, insertion, basketball, etc., in Metaworld.

  
**Task** &  **Button-** \\ **Press** \\  &  **Cable-** \\ **Route** \\  &  **Pick** \\ **(4 tasks)** \\  &  **Place** \\ **(3 tasks)** \\  &  **Drawer-** \\ **Open** \\  &  **Drawer-** \\ **Close** \\  & 
 **Average** \\ **.** \\  \\ 
**Diffusion Policy** & 0.70 & 0.30 & 0.28 & 0.34 & 0.42 & 0.58 & 0.38 \\
**SuSIE** & 0.74 & 0.44 & 0.46 & 0.40 & 0.42 & 0.70 & 0.49 \\
**RT-1** & 0.72 & 0.52 & 0.40 & 0.34 & 0.44 & 0.50 & 0.43 \\
**RT-2*** & **0.80** & 0.60 & 0.64 & 0.74 & **0.66** & 0.82 & 0.69 \\
**PAD(ours)** & **0.80** & 0.55 & 0.76 & 0.72 & 0.56 & 0.84 & 0.72 \\
**PAD-Depth(ours)** & 0.78 & **0.64** & **0.84** & **0.78** & 0.60 & **0.88** & **0.78** \\   

Table 2: Comparisons on real-world manipulation in-distribution tasks. PAD achieves the highest success rate. Incorporating depth modality can additionally lead to performance improvement. We evaluate each task with 50 roll-outs.

Figure 5: Generalization test under 3 levels of difficulties. The yellow bounding box suggests the target position. Our proposed PAD shows the strongest generalization abilities in unseen tasks.

**Quality of the Generated Images.** In addition to achieving the highest success rate in robotic control, we also present visualizations of some image prediction results in Figure 6 and Figure 7. In the Metaworld domain, the predicted image (second row) closely resembles the ground truth image (first row), which is directly decoded from the original latent. In the Bridge domain, the predicted image aligns with the language instructions but also keeps a certain level of uncertainty. These indicate that the PAD model has effectively learned the physical dynamics across these two domains.

### Generalization Analysis

PAD can leverage existing physical knowledge from co-training on large-scale internet video datasets to enhance its generalization capabilities across new tasks. We evaluated PAD's generalization ability in real-world panda manipulation with unseen tasks. As depicted in Figure 4, the expert dataset comprises only colored square blocks and plates, while we introduce a variety of previously unseen fruit and vegetable toys during testing. We designed tasks of three difficulty levels: easy mode, featuring 1-4 disturbance objects; a middle level with 5-15 disturbance objects; and difficult tasks that require picking previously unseen objects with 5-15 disturbances or unseen backgrounds. We excluded depth input to ensure a fair comparison. As illustrated in Figure 5, PAD demonstrates remarkable generalization abilities, successfully managing out-of-distribution objects such as strawberries, carrots, and eggplants, and even adapting to new backgrounds. The baseline method failed to generalize to difficult unseen tasks.

### Ablation Studies

**Effectiveness of RGB image prediction.** We evaluated the effectiveness of our joint prediction process by modifying the original model to **exclude** the image prediction component, namely in PAD w/o image prediction. This modification leads to significant performance drops compared to PAD, as illustrated in Table 1. The absence of image prediction compromises the robot's ability to utilize the physical knowledge encoded in the image modalities, which may be crucial for robotic control.

Figure 6: Comparisons on predicted images between PAD and GR-1. PAD generates more precise images than GR-1 which may potentially lead to more accurate control actions. Zoom in for better comparisons.

Figure 7: Predictions on bridge datasets. PAD predicts futures align with instructions but also keeps uncertainty. In the first image, PAD imagines “a yellow pear” instead of the ground truth ”banana”; in the second image, PAD imagines scenes faster than the ground truth.

Furthermore, predicting solely the robot pose provides only low-dimensional supervision signals, potentially leading to overfitting of the training data.

**Effectiveness of Co-training with Internet RGB Video Datasets.** Another major benefit of PAD is the ability to co-train with large-scale internet-sourced RGB videos, which potentially leverages the physical knowledge embedded in these web datasets to enhance robotic pose prediction. We train PAD without the inclusion of web-scale RGB data, namely PAD w/o co-train. We observed a performance drop without co-train on the video dataset, as shown in Table 1. Furthermore, the quality of the predicted image also decreased. For instance, as depicted in the bottom column of Figure 8, the blue block is absent in the predicted images. The quality of the predicted images markedly improves with co-training, which in turn indirectly enhances robot action prediction.

**Compatible with Additional Modalities.** As detailed in Section 3, our framework accommodates additional modalities owing to the adaptable DiT architectures. We incorporate additional depth image inputs in real-world manipulation experiments and jointly predict future RGB images, depth images, and robot actions, denoted as PAD-depth. We observe highly aligned prediction results among different modalities under our joint denoising framework, with some results illustrated in Figure 9. The inclusion of depth input enhances performance in manipulation tasks, as demonstrated in Table 2. This improvement may stem from the precise prediction of depth information, which aids agents in discerning distance changes, thereby enhancing performance. Moreover, our framework could be extended to predict other modalities relevant to robot control, such as tactile force or point clouds, which we left for the future work.

### Scaling Analysis

We evaluated models across various sizes and patchify sizes , as outlined in Table 3. For example, the \(XL/2\) model denotes the model with an \(XL\) size and a \(2 2\) patchify size. Halving the image patch size will quadruple the image token lengths, which leads to higher computational costs. Our findings reveal a strong correlation between computational allocation (measured as transformer Gflops) and the success rate (SR) of the learned policy, as depicted in Figure 10. All the experiments are run in Metaworld benchmarks and detailed success rates for each task are provided in Appendix A.5.

## 5 Related Work

**Pre-training for Embodied Control.** Vision-language pre-trained models, encoded with physical knowledge, can enhance embodied control from multiple aspects. Primarily, the pre-trained model can directly act as policy by either generating high-level plans  or producing direct

Figure 8: We observe that co-training with an internet video dataset leads to better image generation qualities, which may potentially lead to better robot action predictions.

Figure 9: PAD can flexibly train with additional modality, and simultaneously predict all the futures through joint denoising process.

low-level motor control signals [30; 33; 42; 43; 44]. Many studies utilize the reasoning capabilities of pre-trained LLMs and VLMs to create high-level plans followed by motion primitives. Additionally, some approaches adapt pre-trained models to emit low-level motor control signals by adding an action head. Beyond directly acting as policy, pre-trained models can also guide policy learning from multiple aspects, such as providing good representations [45; 46; 47], providing reward signals [48; 49; 50], synthesizing goal images [18; 51], and predicting future sequences .

**Diffusion Models for Embodied Control.** Recently, diffusion models have been adopted to tackle challenges in embodied control. A subset of research focuses on training conditional diffusion models that guide behavior synthesis based on desired rewards, and constraints under low dimensional state-input setting [52; 53; 54; 55]. Diffusion Policy  trains a visual-motor policy to be conditioned on RGB observations and can better express the multimodal action distributions. However, these methods develop task-specific policies from scratch, missing out on the benefits of pre-training with internet data. Another strand of research utilizes large-scale pre-trained diffusion models to perform data augmentation on training data, such as GenAug , ROSIE , and CACTI .

**Future Prediction for Policy Learning.** There also exist works that leverage future image predictions to assist policy learning. GR-1  employs an autoregressive transformer to sequentially predict future images and actions. In contrast, we adopt a joint diffusion architecture that predicts more accurate future images, potentially leading to improved policy learning performance. UniPi and SuSIE  employ a two-stage policy learning process, initially using a diffusion generative model to forecast future image or video sequences, and subsequently training an inverse dynamics model or a low-level diffusion policy based on these goal images. In contrast to these two-stage methods, our approach presents distinct advantages. First, while previous methods utilize diffusion models with a CNN-based U-net backbone , designed primarily for image generation and limited to visual predictions, our method adopts a diffusion transformer (DiT) architecture . This architecture adeptly handles multiple modalities concurrently via straightforward token concatenation and attention-mask mechanisms, enabling us to jointly predict future and actions simultaneously. Secondly, using images as the interface between prediction and action may not fully leverage the encoded features inside pre-trained diffusion models. The effectiveness of these two-stage methods depends heavily on the quality of the generated images. In contrast, our model integrates image generation and robotic action within a unified denoising process.

## 6 Conclusion and Discussion

We present PAD, a novel framework to predict future images and generate actions under a joint denoising process. Moreover, PAD can co-train with internet video datasets and extend to other robotic modalities. Both simulated and real-world experiments demonstrated the efficiency of PAD.

A limitation of the current method is that we only tested with three types of modalities. Subsequent endeavors could extend this framework to incorporate additional robot-related input data, such as tactile information, which we believe are valuable research directions. Another limitation is that the control frequency of PAD is not very high since we need to jointly denoise the images and actions. Future work can explore efficient ways to leverage image predictions, such as utilizing the intermediate latent space of predicted images rather than the high-dimensional pixel spaces.

    & **PAD-** & **PAD-** & **PAD-** & **PAD-** & **PAD-** \\  & **XL/2** & **XL/4** & **XL/8** & **L/2** & **B/2** \\ 
**Layers** & 28 & 28 & 28 & 24 & 12 \\
**Hidden size** & 1152 & 1152 & 1152 & 1024 & 768 \\
**Heads** & 16 & 16 & 16 & 12 & 12 \\
**Token length** & 257 & 65 & 17 & 257 & 257 \\
**Parameters** & 661M & 661M & 661M & 449M & 128M \\
**Gflops** & 119.1 & 29.5 & 7.7 & 79.1 & 22.5 \\ 
**Average SR** & **72.5\%** & 64.5\% & 48.2\% & 68.4\% & 62.4\% \\   

Table 3: We test PAD performance under various sizes and computational costs.