# Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting

Chiu-Wai Yan  Shi Quan Foo  Van Hoan Trinh  Dit-Yan Yeung

The Hong Kong University of Science and Technology

{cwyan, sqfoo, vhtrinh}@connect.ust.hk  dyyeung@cse.ust.hk &Ka-Hing Wong  Wai-Kin Wong

Hong Kong Observatory

{khwong, wkwong}@hko.gov.hk

###### Abstract

Deep learning approaches have been widely adopted for precipitation nowcasting in recent years. Previous studies mainly focus on proposing new model architectures to improve pixel-wise metrics. However, they frequently result in blurry predictions which provide limited utility to forecasting operations. In this work, we propose a new Fourier Amplitude and Correlation Loss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss (FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude of the model prediction and FCL complements the missing phase information. The two loss terms work together to replace the traditional \(L_{2}\) losses such as MSE and weighted MSE for the spatiotemporal prediction problem on signal-based data. Our method is generic, parameter-free and efficient. Extensive experiments using one synthetic dataset and three radar echo datasets demonstrate that our method improves perceptual metrics and meteorology skill scores, with a small trade-off to pixel-wise accuracy and structural similarity. Moreover, to improve the error margin in meteorological skill scores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we propose and adopt the Regional Histogram Divergence (RHD), a distance metric that considers the patch-wise similarity between signal-based imagery patterns with tolerance to local transforms. Code is available at https://github.com/argenycw/FACL.

## 1 Introduction

Precipitation nowcasting refers to the task of predicting the rainfall intensity for the next few hours based on meteorological observations from remote sensing instruments such as weather radars, satellites and numerical weather prediction (NWP) models. The development of a precise precipitation nowcast algorithm is crucial to support weather forecasters and public safety, as it could facilitate timely alerts or warnings on severe precipitation and mitigate their impact on the community through early preventive actions. Sharp precipitation nowcast imagery that is perceptually similar to the actual observations (such as radar images) is equally important for weather forecasters to comprehend how the severity of precipitation will evolve in space and time, as well as to diagnose the rapid evolution of the underlying weather systems in real-time forecasting operations.

Besides the traditional optical-flow and NWP models, deep learning models have also been widely explored and adopted for precipitation nowcasting in recent years. The research community generally formulates the task as a spatiotemporal prediction problem, where a sequence of input radar or satellite maps is given, and the future sequence needs to be predicted or generated. Although multiple previous attempts proposed solid improvements to the model to grasp the spatiotemporal dynamics,deep learning models can result in blurry predictions in real-life datasets featuring precipitation patterns such as radar echo and satellite imagery. Consequently, they provide limited operational utility  in weather forecasts.

The blurry prediction in multiple deep learning models is believed to be caused by the use of pixel-wise losses such as the Mean Squared Error (MSE), which entangles the probability into model prediction. In other words, the uncertainty of the image transformation leads to obfuscation of the surrounding pixels in the prediction. Nevertheless, solely improving the model capability could not resolve this issue due to the high spatial randomness of the atmospheric dynamics. In order to suppress the ambiguity of the model output, an emerging approach is to utilize generative models such as generative adversarial networks (GANs) and diffusion models. In this paper, we introduce an alternative approach, which is to modify the loss function such that the model focuses on recovering the high-frequency patterns. By utilizing the Fourier domain, we would like to shed light on a deterministic, non-generative method that can sharpen the spatiotemporal predictions with negligible sacrifice to its correctness.

To achieve the desired sharpness, we propose the Fourier Amplitude Loss (FAL), a loss term that improves the prediction of high frequencies by regularizing the amplitude component in the Fourier space. Supported by empirical validation, we further propose the Fourier Correlation Loss (FCL), a complementary loss term that provides information on the overall image structure. Furthermore, we have developed a training mechanism that alternates between FAL and FCL based on an increasing probability of employing FAL throughout the training steps. We name this combined loss function the Fourier Amplitude and Correlation Loss (FACL). FACL is computationally efficient, parameter-free, and model-agnostic and it can be directly applied to a wide range of state-of-the-art deep neural networks and even generative models. Extensive experiments show that compared to MSE, FACL results in forecasts that are both more realistic and more _skillful_ (i.e., high performance with respect to several meteorological skill scores). To the best of our knowledge, we are the first to substantially replace the spatial MSE loss with spectral losses without using generative components on the spatiotemporal prediction problem, demonstrating the novelty and significance of our approach.

Our main contributions are summarized as follows:

* We propose the Fourier Amplitude and Correlation Loss (FACL), which is constituted by sampling between the Fourier Amplitude Loss (FAL) for regularizing the spatial frequency of the predictions to enable clarity and sharpness, and the Fourier Correlation Loss (FCL), a modified loss term that is cohesive with FAL to capture the overall image structure.
* Theoretical and empirical studies show that FAL boosts the image sharpness significantly while FCL complements the missing information for accuracy.
* We apply FACL to replace the MSE reconstruction loss in generative models. Results show that generative models with FACL perform better with respect to most of the metrics.
* We propose the Regional Histogram Divergence (RHD), a quantitative metric to measure the distance between two signal-based imagery patterns with tolerance to deformations. RHD considers both the regional similarity and the visual likeness to the target.

## 2 Related Works

### Precipitation Nowcasting as a Spatiotemporal Prediction Problem

Previous works generally formulate precipitation nowcasting as a spatiotemporal predictive learning problem. Given a sequence of observed tensors with length \(t\): \(X_{1},X_{2},...,X_{t}\), the problem is to predict the future \(k\) tensors formulated as follows:

\[*{arg\,max}_{X_{t+1},...,X_{t+k}}p(X_{t+1},...,X_{t+k} X_{1},X_{2},...,X_{t})\] (1)

Based on this formulation, numerous variations of convolutional RNN models were proposed to model both spatial and temporal relationships in the data. ConvLSTM  first proposed to integrate convolutional layers into LSTM cells, with the recurrence forming an encoder-forecaster architecture. PredRNN  replaced the ConvLSTM units with ST-LSTM units and modified the structure such that the hidden states flow in both spatial and temporal dimensions in a zigzag pattern. MIM  replaced the forget gate in ST-LSTM with another RNN unit, forming a memory-in-memory structure to learn higher-order non-stationarity. Moreover, advanced modifications such as reversed scheduled sampling , gradient highway , etc. [7; 8] were proposed to further improve the overall performance of the model. With the breakthroughs brought by transformers and self-attention mechanism, space-time transformer-based models such as Rainformer  and Earthformer  were proposed to model complex and long-range dependencies.

On the other hand, CNN models have also been widely explored for the task as a video prediction problem. Inspired by the U-Net structure used in earlier works [11; 12], SimVP achieves remarkable performance and efficiency by adopting an encoder-translator-decoder structure with mostly convolutional operations. Among the parts, the translator (temporal module) is found to benefit from MetaFormer (an architecture with both token mixer and channel mixer) in subsequent studies [13; 14]. TAU  further demonstrated the effectiveness of the structure by adopting depthwise convolution followed by \(1 1\) convolution as the temporal module.

Conventional precipitation nowcasting tasks and video prediction tasks evaluate the output mainly with pixel-wise or structural metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE) and Structural Similarity (SSIM) Index. To better consider the hits and misses of signal-based reflectivity, the Critical Success Index (CSI; equivalent to Intersection over Union, IoU), Fractions Skill Score (FSS) and Heidke Skill Score (HSS) belong to another type of metrics widely used in meteorology. To distinguish these scores from those used in the traditional machine learning literature, we refer to this metric type as _skill scores_ in the remaining sections of the paper.

### A Non-deterministic Perspective on Atmospheric Instability

Traditional models can result in blurry predictions at longer lead times, causing difficulty in forecasting operations. To address it, recent works leverage generative models such as GANs and diffusion models to promote realistic forecasts which could bring more insightful observation to forecasting operations. DGMR  utilizes a GAN framework with discriminators in both the spatial and temporal dimensions to ensure that the predicted images are sufficiently realistic and cohesive. LDCast  uses latent diffusion to generate a diverse set of outputs for ensemble forecasting. Meanwhile, the literature in video generation strives to generate realistic output frames with generative models. PreDiff  introduces a knowledge alignment mechanism with domain-specific constraints while adopting a latent diffuser for quality forecasts. DiffCast  appends a diffusion component as an auxiliary module to improve the realisticity of the forecasts. It is worth mentioning that the literature in video generation [1; 19; 20; 21; 22] also exhibits potential in generating high-quality nowcastings despite not specifically being designed to handle precipitation. Unlike works in video prediction, instead of evaluating the output quality with pixel-wise similarity, perceptual metrics such as LPIPS  and Frechet Video Distance (FVD)  are predominantly used.

These works usually formulate the task as an unsupervised or semi-supervised learning problem with the results being non-deterministic based on a random prior, enabling the possibility of ensemble prediction. However, studying each prediction individually is less reliable as the prediction is unexplainably affected by the random prior. Furthermore, the inference efficiency of the diffusion model is poor due to the iterative nature of the reverse diffusion sampling process. Concerning the drawbacks of generative models, our method is proposed to be efficient, deterministic, and accurate at both the pixel and perceptual levels, bridging the advantages of both probabilistic video prediction and non-deterministic video generation.

### Supervised Learning Problems That Utilize Fourier Transform

Spectral analysis in the Fourier space is a common practice for DNNs to study the features in terms of frequency. Rahaman et al.  proposed a property known as the spectral bias, which causes DNN models to be biased towards low-frequency functions. A follow-up study  theoretically showed that DNN models have a much slower convergence rate toward high-frequency components. Such observations motivate subsequent works to apply Fourier-based loss terms extensively in tasks such as super-resolution (SR) where fine details are crucial.

Despite the existence of works that apply Fourier transform amid the model feed-forward pipeline [27; 28; 29; 30; 31], here we focus on works that utilize spectral transform in the loss function or as a regularization term. Inspired by the JPEG compression mechanism, the Frequency Domain Perceptual Loss  compares the Discrete Cosine Transform (DCT) of the model output in 8\(\)8 non-overlapping patches: \(L(y,)=c\|(y)-()\|_{2}^{2}\), where \(c\) is a vector of constants computed from the quantization table and training set. The Focal Frequency Loss  compares the element-wise weighted Fast Fourier Transform (FFT) output: \(L_{}=_{u=0}^{M-1}_{v=0}^{N-1}w(u,v)|(y)_{ u,v}-()_{u,v}|^{2}\), where \(w(,)\) is a dynamic weight matrix and \(||\) refers to the absolute operator on complex numbers. Moreover, the Fourier Space Loss  decomposes the Fourier output (in complex) into amplitude and phase and measures their difference separately as a GAN loss component.

Although these losses were proposed specifically for the SR task, we find the problem setting similar to spatiotemporal forecasting in terms of the requirement for high-frequency fine details and the involvement of a ground-truth label. While taking advantage of the Fourier space as a spectral analysis is intuitive, choosing the proper distance metrics and additional weighting is tricky. This motivates us to propose a new loss function with consideration of the spectral property on the spatiotemporal forecasting problem.

## 3 Our Methods

In this section, we start by arguing why a naive implementation of the Fourier loss does not benefit the model compared with the MSE loss in the image space. Then, we will discuss the motivation and details of our proposed FACL.

### Preliminaries

An image \(X\) can be interpreted as a 2D matrix with the transformed Fourier series, \(F\). The orthonormalized Discrete Fourier Transform (DFT) output and its corresponding inverse Discrete Fourier Transform are formulated as:

\[F_{pq}=}_{m=0}^{M-1}_{n=0}^{N-1}X_{mn}e^{-i2( +)};\ X_{mn}=}_{p=0}^{M-1}_ {q=0}^{N-1}F_{pq}e^{i2(+)}\] (2)

where \(M\) and \(N\) are the height and width, respectively, of the image \(X\).

To constrain model convergence via the spatial frequency components of its prediction, one naive design is to regularize the \(L_{2}\) norm of the displacement vector between the ground truth and prediction in the Fourier space apart from the image space. Parseval's Theorem shows that such design is linearly proportional to the spatial MSE loss, and the detailed proof can be found in Appendix B.

Since this straightforward regularization does not differ from the MSE loss in the image space, the common adaptations from previous works are either to apply weighting on different frequencies or to decompose the Fourier features into amplitude \(|F|\) and phase \(_{F}\) with the following definitions:

\[|F|=}^{2}+F_{}^{2}};\ _{F}=( }}{F_{}}),\] (3)

where \(F_{}\) and \(F_{}\) are the real and imaginary parts, respectively, of the complex Fourier vector \(F\).

### Fourier Amplitude Loss (FAL)

As the spectral bias indicates the lack of attention to the high-frequency components, we encourage the model to consider high-frequency patterns by applying a loss on the amplitude of each frequency band. Similar to previous works, we first apply DFT to obtain the spectral information. Using Equation (3), we extract only the Fourier amplitudes (\(|F|\)) in the Fourier space and compare them in \(L_{2}\):

\[(X,)=_{p=0}^{M-1}_{q=0}^{N-1}(|F|_{pq}-| |_{pq})^{2}\] (4)

where \(F\) is the DFT output of \(X\) as formulated in Eq. (2). Note that the formulation is subtly different from minimizing the \(L_{2}\) norm of the displacement vector that prediction deviates from ground truth in the Fourier domain. The new formulation based on the Fourier amplitude of the images only is invariant to global translation. This reduces the spatial constraint induced by MAE and MSE losses. A detailed analysis can be found in Appendix D.

Despite retaining the high-frequencies by dropping the Fourier phase, FAL alone is insufficient to reconstruct the image. As \(X|F|\) is a many-to-one mapping, there exist multiple \(X\) to have the same Fourier amplitude matrix. Thus, simply minimizing Eq. (4) can likely converge to an undesirable critical point. A high-level interpretation is that only image sharpness is retained by this loss while the information regarding the actual shape and position is lost with the Fourier phase discarded. Hence, on top of FAL, we require another loss term to compensate for the missing information, leading to our upcoming proposal of the FCL term. An alternative perspective via the mathematical formulation can be found in Appendix C.

### Fourier Correlation Loss (FCL)

To remedy the missing information resulting from FAL, there are several approaches to take the image structure into account. A straightforward way is minimizing the difference of the Fourier phase between the prediction and the label, but it fails as \(_{F}\) obtained under DFT is discontinuous. Another approach is to compute the cosine distance in the Fourier domain without extracting \(_{F}\) directly. However, our preliminary experiments reveal that such formulation is unstable in reconstructing the image structure. Ultimately, we propose to implement the correlation between the generated output and ground truth in the Fourier domain and adopt it as the Fourier Correlation Loss (FCL) in our proposed loss:

\[(X,)=1-[F^{*}+F^{*}]}{ {|F|^{2}||^{2}}},\] (5)

where \(\) here is a shorthand for the summation over all elements of the Fourier features and \(*\) denotes the complex conjugate of the vector. FCL plays a significant role during training as it is responsible for learning the proper image structure while FAL can be treated as a regularization to promote the high-frequency components that FCL fails to capture.

The formulation of FCL has a similar format to the Fourier Ring Correlation (FRC) and Fourier Shell Correlation (FSC) widely used in image restoration and super-resolution of cryo-electron microscopy [35; 36; 37; 38; 39; 40]. However, both FRC and FSC pre-define a specific region of interest (either a ring or a shell) on the Fourier features. In contrast, we extend the region of interest to the entire map, considering the global spectral bands with all frequencies. To ensure the score is real and commutative, we take the average of \(F^{*}\) and \(F^{*}\) in the numerator. The denominator performs normalization such that FCL only focuses on the image structure in the global view rather than the absolute brightness. Unlike FRC (without \(1-\)), FCL spans the range , where larger values refer to a negative correlation and smaller values refer to a positive correlation. Further analysis of FCL from the gradient aspect can be found in Appendix E.

### Proposed Approach: Random Selection between FAL and FCL

While it is straightforward to apply the overall loss function as a linear combination of FAL and FCL, we find it tricky to determine the weighting of the components in our preliminary studies. Instead, we offer a more controllable solution - to alternate FAL and FCL as shown below:

\[(X,,t)=&(X,),p>P(t)\\ &(X,),\] (6)

where \(p\) is sampled randomly and uniformly in  and \(P(t)\) is a pre-defined threshold decreasing during the training process as shown in Figure 1. \(P(t)\) always decreases from 1 to 0 such that the model is first trained with \(100\%\) FCL that takes image structure into account, and then the models are more frequently trained with FAL which improves the image sharpness.

Since FCL loses information on the overall brightness, the model could not achieve proper brightness at the early stage where FCL dominates the learning objective. To address it, we append a sigmoid function in the output layer of the model. This constrains the model output in the range  to prevent the model from converging to a sub-optimal state with an undesirable range of output values.

Overall, the following modifications are applied to the models:

* Training loss function of the models involving FAL and FCL is formulated in Eq. (6).

* A sigmoid layer is appended to the end of the model. For RNN models, the sigmoid function is applied before the output of the last RNN stack.
* To coordinate with the decreasing threshold, the cosine annealing learning rate scheduler is used rather than the conventional reduce-on-plateau scheduler.

### A New Metric: Regional Histogram Divergence (RHD)

Previous works in video prediction tend to use pixel-wise metrics such as MSE and MAE to measure the difference between the prediction and labels. Such a choice of metrics might not fit spatiotemporal data for two reasons: (1) reasonable pixel shifts are highly penalized, and (2) the overall distribution of values is ignored. This encourages the models to output blurry predictions while regional uncertainty diffuses outward over time. By inverse, deep perceptual metrics such as LPIPS, Inception Score (IS) and Frechet Video Distance (FVD) suffer from the knowledge bias between multi-channel pictures (as pre-trained on ImageNet) and monotonic signal-based intensities.

One of the metrics that consider both the previous two factors is the Fractional Skill Score (FSS), which is widely used in meteorology. After splitting the image into \(N_{x} N_{y}\) smaller patches, where \(N_{x}\) and \(N_{y}\) control the shift of precipitation events we tolerate, we obtain the FSS score as follows:

\[=1-^{N_{x}}_{j=1}^{N_{y}}(F_{i,j}-O_{i,j} )^{2}}{_{i=1}^{N_{x}}_{j=1}^{N_{y}}F_{i,j}^{2}+_{i=1}^{N_{x} }_{j=1}^{N_{y}}O_{i,j}^{2}},\] (7)

where \(F_{i,j}\) and \(O_{i,j}\) refer to the fraction of predicted positives and fraction of observed positives, respectively, of the patch in the \(i\)-th row and \(j\)-th column. Based on this formulation, the intensities are free to reposition within the patch window, granting tolerance to translation and deformation. Nevertheless, one drawback of FSS is that the pixel range is only categorized into two classes: positives and negatives. For a threshold of \(0.5\), a pixel value of \(0\) is treated the same as a pixel value of \(0.49\), resulting in a huge error when viewing the per-patch precision. This means that the choice of threshold induces a bias in evaluating the forecasting performance of models.

To improve the representation, we propose the Regional Histogram Divergence (RHD), a variation of FSS that exhibits smaller errors within a class. Instead of categorizing the pixel values into 'hits' and'misses', we divide the values into \(n\) bins and count the frequency of each bin, obtaining a histogram for each patch. Next, we compare the average Kullback-Leibler (KL) divergence on the histograms. Mathematically, the RHD between two sets of bins can be expressed as:

\[=N_{y}}_{i=1}^{N_{x}}_{j=1}^{N_{y}}D_{ }(O^{}_{i,j}||F^{}_{i,j})=N_{y}}_{i=1}^{N_{x}} _{j=1}^{N_{y}}_{x}O^{}_{i,j}(x)_{i,j}(x)}{F^{}_{i,j}(x)},\]

where \(F^{}_{i,j}\) and \(O^{}_{i,j}\) correspond to the predicted and observed discrete probability distributions, respectively, among the set of bins \(\) of the patch in the \(i\)-th row and \(j\)-th column.

Different from FSS where the proportions of positives are subtracted directly, RHD instead compares the distributional difference in the context of the histograms. This not only increases the precision of each class/bin, but also heavily penalizes blurs since blurring forms a Gaussian-like distribution in the histograms while sharp intensities should have an 'M-shape' bimodal distribution. If the patch-wise distribution of the two images is identical, the corresponding RHD is 0. The larger the RHD is, the more different the two sets of patches behave. Furthermore, RHD is formulated to be a mean KL divergence so it is always non-negative.

For simplicity and consistency, we choose the number of bins to be 10, divided uniformly within the range \(\) for all datasets in our experiments. In real-life applications, non-uniform division can be applied for data in non-linear scales such as radar echo (in dBZ) to highlight specific ranges of values. When we compute the histograms, as 0 dominates in the imagery, we apply a threshold \(=10^{-5}\) to exclude all small intensities.

Figure 1: The pre-defined probability threshold function \(P(t)\) over training steps \(t\) with \(T\) total steps. \(\) determines the ratio of the training steps where \(P(t)=0\).

Experiments

### Experimental Settings

We evaluate the performance of our proposed method on a synthetic dataset and three radar echo datasets, namely, Stochastic Moving-MNIST, SEVIR , MeteoNet  and HKO-7 . A more detailed description for each dataset can be found in Appendix A. To show that our method is effective and generic, we selected ConvLSTM  and PredRNN  (reported in Appendix I), two RNN-based models with different recurrence paths; SimVP , a CNN-based model and Earthformer , a transformer-based model. We trained the models with two variants: conventional MSE and FACL (as formulated in Eq. (6)). To compare with generative models as references, we also report the results of LDCast  (latent diffusion) and MCVD  (denoising diffusion) for all datasets. For Stochastic Moving-MNIST, we report two more models, i.e., PreDiff  (latent diffusion) and STRPM  (GAN-based). Appendix M reports the detailed setup and hyper-parameters.

In the upcoming sections, we will first present the setup and experimental results on the Stochastic Moving-MNIST dataset. After that, we will test the models with three real-world radar echo datasets. Extra studies on our methods are reported in the Appendix. Specifically, we report the ablation study of FAL, FCL and \(\) in Appendix F, the running time of FACL in Appendix G, experiments on additional datasets in Appendix H, comparison with other potential loss functions in Appendix J, the performance when applying FACL to generative models in Appendix K, and characteristic analysis of RHD in Appendix L. To demonstrate the advantages of our method against counterparts for precipitation nowcasting, video prediction and video generation, we evaluate the models with a union of metrics from the areas. Specifically, we report the MAE and SSIM to show the pixel-wise and structural accuracy; LPIPS and FVD to show the deep perceptual similarity to ground truth; FSS and RHD to measure the similarity of the intensity distribution in different regions. For radar echo datasets, we further include the CSI and pooled CSI to reveal the models' capability of identifying potential extreme weather. Such a combination of metrics is believed to facilitate a comprehensive understanding of the pros and cons of the current state-of-the-art in precipitation nowcasting.

### A Stochastic Modification of Moving-MNIST

The Moving-MNIST dataset has been a common benchmark to evaluate how well a model could predict motion in spatial preservation and temporal extrapolation. However, the nature of the Moving-MNIST is highly deterministic, which does not resemble the chaotic nature of the atmospheric system. Previous adaptations attempted to simulate the physical dynamics by introducing a set of complex motions such as rotation and scaling  or by applying an external force on collision . We argue that the fundamental reason causing the blur in precipitation nowcasting is the intrinsic stochasticity of the motion caused by external factors unseen in the weather dataset, such as orographic effects, vertical wind shear, interaction with other weather systems, etc. Trained with such stochasticity, regular models with pixel-wise loss could consistently fail to provide quality prediction in the future lead time.

To verify our claim, we introduce a simplistic modification to the Moving-MNIST dataset. The standard Moving-MNIST dataset contains handwritten digits sampled from the MNIST dataset moving and bouncing with a constant velocity \((u_{0},v_{0})\) on the \(64 64\) image plane. To introduce stochasticity, we perturb the velocity with a random Gaussian noise \(\) at each time step. Details of the perturbation are shown in Appendix A. In the upcoming sections, we dub this dataset Stochastic Moving-MNIST and apply the experimental setting to this synthetic dataset. The performance of combinations of different losses and models can be found in Table 1 and qualitative visualizations of the corresponding methods are shown in Figure 2 and Appendix N. Note that the Stochastic Moving-MNIST is used in both training and evaluation to ensure that the models are well exposed to motion randomness.

In Table 1, our modification drastically improves the sharpness and realisticity for all tested models, as reflected by the vast reduction in LPIPS and RHD. In particular, FACL reduces up to \(57\%\) of LPIPS and \(71\%\) of RHD for the ConvLSTM model. The pixel-wise and structural metrics between the two losses are comparable. On the other hand, generative models result in much poorer MAE and SSIM, with skill scores like FSS still being worse than most of the baseline models. In Figure 2, we can observe that the model trained with MSE cannot reconstruct a clear spatial pattern, especially in the subsequent frames, while the model trained with FACL yields much sharper and higher quality outputs. Consequently, we can conclude that in this setting with the synthetic Stochastic Moving-MNIST, FACL demonstrates a significant improvement in perceptual metrics and skill scores, with the quality on par with that of generative models.

### Performance on Radar-based Datasets

In this section, we extend the previous setup to general radar-based datasets. Apart from the distance metrics used in the last section, we further report the CSI with different pooling sizes. Following the previous works , we measure multiple CSI scores with different thresholds ({16, 74, 133, 160, 181, 219} for SEVIR, {12, 18, 24, 32} for MeteoNet and {84, 117, 140, 158, 185} for HKO-7). The visualizations can be found in Figure 3 and more in Appendix N.

The results of Table 2 are similar to the observations in Table 1. Compared with the MSE baselines, FACL always improves the perceptual and skill scores. For sharp forecasts, the pooled CSI increases with the pooling size while for blurry forecasts, CSI shows no apparent difference based on pooling size. For some models, we observe a tiny decay in pixel-wise and structural metrics. For example, the Earthformer model trained with FACL on SEVIR has a \(6.4\%\) increase in MAE, which is believed to be a natural trade-off since pixel-wise metrics have no tolerance for spatial transformation. Despite poorer pixel-wise performance, the perceptual metrics and skill scores always improve, as further illustrated by Figure 3 that only FACL predicts fine-grained extreme values. Regarding those generative models, although they could perform the best in deep perceptual scores like LPIPS and FVD, we still observe that they usually result in poorer skill scores. Moreover, it is noteworthy that FACL does not add any new parameters to the model. The change in the metrics solely indicates that FACL induces a shift of focus from pixel-wise accuracy to image quality and prediction skillfulness.

Figure 2: Output frames of the ConvLSTM model trained with different losses on Stochastic Moving-MNIST. From top to bottom: Input, Ground Truth, MSE, FACL.

## 5 Conclusion

In this paper, we proposed the Fourier Amplitude and Correlation Loss (FACL). The two loss terms, Amplitude Loss (FAL) and Fourier Correlation Loss (FCL), encourage the model to focus on the Fourier frequencies and image structure correspondingly. Besides, we proposed a new metric, Regional Histogram Divergence (RHD), to measure the patch-wise similarity between two spatiotemporal patterns. We widely tested our methods on a synthetic dataset and three more real-life radar echo datasets, measured by metrics considering accuracy, realisticity and skillfulness. Extensive experiments reflect that our method yields sharper, more realistic and skillful forecasts with limited degradation in pixel-wise similarity.

Despite the remarkable performance of the FACL loss, our methods still have room for improvement. First, we assumed the data to be monotonic radar echo, which might not generalize well to multimodal datasets featured in medium-range forecasts. Besides, our loss provides no regularization on temporal consistency, which may lead to the misalignment of temporal features between frames. The solution to these issues, however, will be open for future work.

   &  &  &  &  &  &  \\  & & & MAE\({}_{}\) & SSIM & LPIPS\({}_{}\) & FVD\({}_{}\) & CSI-mt & CSI\({}_{}\)-mt & CSI\({}_{16}\)-mt & FSS\({}_{}\) & RFID\({}_{}\) \\    &  & ConvLSTM & MSE & **26.35** & **0.7730** & 0.3683 & 510.2 & 0.3957 & 0.3965 & 0.4082 & 0.5325 & 1.5123 \\  & & & FACL & 27.60 & 0.7624 & **0.3508** & **289.5** & **0.3984** & **0.4295** & **0.5073** & **0.5640** & **1.2087** \\   & & & MSE & **26.39** & **0.7701** & 0.3831 & 947.1 & **0.3999** & 0.3961 & 0.3976 & **0.5316** & 1.5363 \\  & & & FACL & 28.09 & 0.7627 & **0.3575** & **384.3** & 0.3982 & **0.4129** & **0.4742** & 0.5244 & **1.3736** \\   & & & MSE & **26.26** & **0.7643** & 0.3767 & 555.9 & 0.3989 & 0.3939 & 0.3956 & 0.5225 & 1.5244 \\  & & & FACL & 27.55 & 0.7551 & **0.3476** & **243.8** & **0.4100** & **0.4387** & **0.5176** & **0.5656** & **1.1719** \\   &  & LDCast & - & 40.93 & 0.6647 & 0.3800 & 163.7 & 0.3000 & 0.3357 & 0.4411 & 0.3971 & 1.5988 \\  & & MCVD & - & 32.88 & 0.7386 & 0.3299 & 99.0636 & 0.3981 & 0.5017 & 0.5230 & 1.3687 \\    &  & MSE & **6.47** & 0.9155 & 0.1504 & 247.9 & **0.4388** & 0.3989 & 0.3904 & 0.5036 & 0.2707 \\  & & FACL & & **8.03** & **0.9170** & **0.1252** & **1223** & 0.4161 & **0.4876** & **0.6041** & **0.5196** & **0.1944** \\   & & & MSE & **7.13** & **0.9045** & 0.1708 & 370.6 & **0.4004** & 0.3327 & 0.2946 & 0.4629 & 0.3213 \\  & & & FACL & & 8.01 & 0.9044 & **0.1589** & **203.2** & 0.3594 & **0.4038** & **0.5250** & **0.4833** & **0.2773** \\   & & & MSE & **6.66** & **0.9128** & 0.1571 & 268.9 & **0.4221** & 0.3748 & 0.3627 & **0.4974** & 0.2820 \\  & & & FACL & & 7.21 & 0.9088 & **0.1450** & **1284** & 0.4008 & **0.4513** & **0.5272** & 0.3826 & **0.2170** \\   &  & LDCast & - & 19.94 & 0.7295 & 0.3263 & 486.6 & 0.2353 & 0.3188 & 0.4804 & 0.1333 & 0.5594 \\  & & MCVD & - & 13.18 & 0.8395 & 0.1549 & 55.7 & 0.3645 & 0.4559 & 0.6148 & 0.3838 & 0.2979 \\    &  &  & MSE & **30.43** & 0.66647 & 0.3057 & 791.0277 & 0.2282 & 0.1702 & 0.2653 & 1.2453 \\  & & & FACL & & **29.72** & **0.7168** & **0.2962** & **5691** & **0.3054** & **0.3040** & **0.3351** & **0.4045** & **0.7916** \\    & & & MSE & **31.62** & **0.6617** & **0.3186** & 0.3931 & 0.2492 & 0.1976 & 0.1402 & 0.2367 & 1.3426 \\    & & & FACL & & **31.59** & 0.6004 & 0.3247 & **6193** & **0.2812** & **0.2746** & **0.2962** & **0.3538** & **1.0752** \\    & & & MSE & **30.93** & 0.6585 & 0.3303 & **0.801** & 0.2739 & 0.2227 & 0.1642 & 0.2617 & 1.2623 \\    & & & FACL & 31.65 & **0.6803** & **0.2912** & **555.1** & **0.3018** & **0.3067** & **0.3223** & **0.3973** & **0.8660** \\    &  & LDCast & - & 47.57 & 0.7267 & 0.3168 & 257.2 & 0.1846 & 0.2229 & 0.2486 & 0.3116 & 1.5524 \\   & & MCVD\({}^{}\) & - & 47.226 & 0.6813 & 0.3334 & 215.0 & 0.2576 & 0.2951 & 0.3233 & 0.42289 & 1.8816 \\  ^{}\)} &  \\ 

Table 2: Comparison of the quantitative performance of different losses for models trained on SEVIR, MeteoNet and HKO-7. MAE metrics is in the scale of \(10^{-3}\). The better score between MSE and FACL is highlighted in bold.

Figure 3: Output frames of the Earthformer models trained with MSE and FACL on SEVIR.