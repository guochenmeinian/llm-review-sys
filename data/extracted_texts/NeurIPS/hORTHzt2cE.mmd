# RoleAgent: Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts

Jiaheng Liu*\({}^{1}\), Zehao Ni*\({}^{3,6}\), Haoran Que*\({}^{2}\), Tao Sun\({}^{2}\), Zekun Wang\({}^{2}\), Jian Yang\({}^{2}\), Jiakai Wang\({}^{3}\), Hongcheng Guo\({}^{2}\), Zhongyuan Peng\({}^{3}\), Ge Zhang\({}^{4}\), Jiayi Tian\({}^{2}\), Xingyuan Bu\({}^{5}\), Ke Xu\({}^{2}\), Wenge Rong\({}^{2}\), Junran Peng\({}^{1,3,6}\), Zhaoxiang Zhang\({}^{1,6}\)

\({}^{1}\)Nanjing University, \({}^{2}\)Beihang University, \({}^{3}\)University of the Chinese Academy of Sciences,

\({}^{4}\)University of Waterloo, \({}^{5}\)Beijing Institute of Technology,

\({}^{6}\)Institute of Automation, Chinese Academy of Sciences

* First three authors contributed equally.

\({}^{}\) Corresponding Author: Junran Peng.

###### Abstract

Believable agents can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication. Recently, generative agents have been proposed to simulate believable human behavior by using Large Language Models. However, the existing method heavily relies on human-annotated agent profiles (e.g., name, age, personality, relationships with others, and so on) for the initialization of each agent, which cannot be scaled up easily. In this paper, we propose a scalable RoleAgent framework to generate high-quality role-playing agents from raw scripts, which includes building and interacting stages. Specifically, in the building stage, we use a hierarchical memory system to extract and summarize the structure and high-level information of each agent for the raw script. In the interacting stage, we propose a novel innovative mechanism with four steps to achieve a high-quality interaction between agents. Finally, we introduce a systematic and comprehensive evaluation benchmark called RoleAgentBench to evaluate the effectiveness of our RoleAgent, which includes 100 and 28 roles for 20 English and 5 Chinese scripts, respectively. Extensive experimental results on RoleAgentBench demonstrate the effectiveness of RoleAgent.

## 1 Introduction

In cognitive models  and virtual environments [21; 3], researchers and practitioners have envisioned computational agents that can serve as believable proxies of human behavior. Such simulations of human behavior could populate virtual spaces and communities with realistic social phenomena [11; 31], test social science theories [4; 19], and underpin open world non-playable characters [21; 32].

Recently, Large Language Models  (LLMs) are used to simulate human behaviors at a single time point [31; 17], and the Generative Agents  in Fig. 1(a) produce agents that can _retrieve_ relevant events and interactions over a long period, _reflect_ on those memories to draw higher-level inferences, and reason to create _plans and reactions_ that make sense at the moment and the longer-term arc of the agent's behaviors. However, these generative agents heavily rely on human-annotated agent profiles (e.g., name, age, personality, relationships with others, and so on) for the initialization of each agent, which influences the scalability to scale up the number of agents a lot. In contrast, in Fig. 1(b), we propose a more flexible agent framework called **RoleAgent** to automatically produce high-quality agents from the existing unprocessed scripts without using any human efforts on the initialization ofagents. The RoleAgent mainly includes two components: _how to build the RoleAgent_ and _how to interact with RoleAgent_. Note that there are a large number of scripts from all kinds of agents, which indicates that the number of agents can **be scaled up easily** for RoleAgent.

Specifically, during the building stage, RoleAgent first undergoes a detailed extraction and summarization of structural and high-level information from raw unstructured scripts, which produces the initial observation of each agent. Then, as the initial observations of agents are typically redundant with sparse useful information, we further introduce the hierarchical memory scheme to distill and deduct existing raw observations into more structured memories. In the interacting stage, four steps (including query deconstruction, memory retrieval, memory summarization and response generation) are proposed to generate high-quality interaction experiences. Specifically, we introduce an innovative mechanism to update memory, facilitating nuanced interactions between agents. This mechanism incorporates processes for memory retrieval, caching, and replay. Meanwhile, we propose the dynamic importance score based on the retrieved frequency to represent the relevance between the memory and the interaction queries.

To rigorously assess the performance of RoleAgent, we have developed a specialized and extensive evaluation benchmark, named **RoleAgentBench**. Specifically, based on our constructed RoleAgentBench, we conduct two evaluations by "interviewing" the RoleAgent in natural language to probe the agents' ability to stay in character, remember, plan, react, and reflect accurately, which include agent evaluation and memory evaluation. For the agent evaluation, we perform a controlled evaluation to test whether the agents produce believable individual behaviors, where the self-knowledge, reaction and general abilities are evaluated. For the memory evaluation, we analyze the summarization quality.

Overall, the contributions are as follows: (1). We propose a flexible RoleAgent framework to automatically produce creative and interactive agents from raw scripts, which includes building and interacting stages and reduces the efforts of human-annotated agent profiles. (2). In the building stage, we propose the hierarchical memory system to reason and store structural and high-level memories of different roles. In the interacting stage, we introduce an innovation mechanism with four steps to obtain sufficient context and generate high-quality responses. (3). To evaluate RoleAgent, we introduce a comprehensive evaluation benchmark called RoleAgentBench including 128 roles from 20 English and 5 Chinese scripts, and extensive experiments show the advantages of RoleAgent.

## 2 Related Works

LLM-based AgentsThe evolution of Large Language Models (LLMs) [36; 37; 27; 46; 2; 16; 48; 25; 24] as core controllers in autonomous agents have led to significant advancements in their ability to carry out complex tasks. Early attempts like Auto-GPT  demonstrated the potential of using LLMs for goal-oriented tasks without multi-agent collaboration. To address this limitation, systems like BabyAGI  and MetaGPT  were introduced, showcasing how assigning specific roles to a group of LLMs can facilitate coordination toward common objectives, such as software development.

Figure 1: We take the script “Friends” as an example to show the differences between Generative Agents  in (a) and our RoleAgent in (b).

Meanwhile, some researchers [22; 8; 23; 39; 42] illustrated the benefits of communicative role-playing in completing tasks and tried to utilize external tools. Despite these advancements, a common constraint in these systems is the reliance on predefined or manually created agents with set roles, which can potentially restrict the breadth of collaborative applications.

Bileivable AgentsBelievable agents [20; 33; 38; 41; 40; 34] are essential in crafting immersive interactive experiences, serving to emulate the nuanced decision-making and charm of characters typical in animations or sophisticated game NPCs. These agents [9; 45; 28] are intended to populate virtual spaces, replicating human-like behaviors to enable narratives and social dynamics to unfold naturally. Besides, learning-based techniques, especially reinforcement learning, have been effective in honing agent behavior within competitive gaming areas [14; 15]. Creating non-competitive agents that can move through open worlds and interact socially remains challenging . Recently,  applies the LLMs to generate believable agents with a memory stream. In contrast, our RoleAgent aims to produce and evaluate agents automatically from massive raw scripts without human effort.

## 3 RoleAgent

### Overall Framework

In Fig. 2, before deploying RoleAgent in an interactive game environment, we need to build the agents from scripts. This **building stage** involves the structuration and summarization of script content into higher-order, dense memories. Unlike Generative Agents  which needs extensive human efforts to define the agent profiles and periodically summarizes the agents' observations, RoleAgent involves a recursive process of extraction and abstraction, organizing and distilling low-level observations with a hierarchical memory, where lower-level memories are detailed and sparse, while higher-order memories are abstract and dense. Then in **interacting stage**, the queries of player1 are first deconstructed to facilitate multi-faceted memory retrieval. The retrieved memories are subsequently summarized from various perspectives to formulate LLM contexts, enabling RoleAgent to perform actions or give responses with strong role-specific knowledge and episodic memories. In

Figure 2: The overall framework of RoleAgent can be divided into two workflows: (1) building RoleAgent from scripts; (2) interacting with RoleAgent (as a player or a non-player character (NPC). Note that for simplicity, we will use the term “player” as a general designation for interaction.

the following sections, we will delineate the workflow of RoleAgent. For the sake of clarity, we will employ the character of "Iron Man" as a prototypical RoleAgent.

### Building Stage

In Fig. 2, the building stage involves two steps : (1) script parsing, the extraction of an agent's observations from scripts; (2) hierarchical memory construction, a recursive distillation and deduction of observations into a structured memory system.

Script ParsingAs Scripts are often unstructured text, it is imperative to parse the scripts into formats that are both structured and queryable. Given a script, we initially identify the characters' names and subsequently extract all behaviors and dialogues attributable to each character. This data is formatted as "{<character>: <behavior or dialogue>}n{<character>: <behavior or dialogue>}...". It is important to note that asides are categorized as a "special character" to incorporate essential background or plot information. These elements are chronologically arranged and segmented in alignment with the natural plot segmentation of the script.

When focusing on several particular roles, the role profiles are defined as the observations of these roles. To extract the observations, we first cut the raw script into scenes arranged in chronological order. Then we determine the relevant roles involved in each scene based on GPT-4. Finally, we select several roles from the script based on their participation and popularity, and generate role profiles based on dialog and narration.

Hierarchical MemoryDuring script parsing, the role's perceivable information is meticulously compiled. However, these raw observations are typically redundant, and the useful information is sparse, which hampers the agent's ability to rapidly and accurately retrieve relevant memories. Moreover, overly granular information impedes generalization. For instance, it would not be feasible to extrapolate the real relationship between Iron Man and Captain America solely from observations of their combat interactions. Inspired by the theories of information hierarchy in the human brain , we begin with observations and abstract them progressively through a three-tiered process: (1) event aggregation; (2) key point distillation; (3) insight reflection. The observations along with the resultant events, key points, and insights, constitute the **hierarchical memories**, where the events, key points, and insights are combined as **high-order memories**.

Specifically, from \(K_{o}\) observations, we engage GPT-4 to aggregate these into an event. This is achieved by presenting instructions such as "What happened in these observations?". After several iterations, we obtain \(K_{e}\) events. Note that \(K_{e}\) is equal to the number of scenes. Subsequently, the LLM is prompted to distill these events into a key point by instructions like "What do these events illustrate?" Finally, we ask the LLM to reflect an overarching insight from \(K_{p}\) key points and their corresponding events, using the instruction like "What is the overarching insight that can be drawn from these key points and their relevant events?". Note that we set \(K_{p}=3 K_{e}\), which means that we extract 3 key points for each event. Through this structured process, observations are progressively abstracted into events, key points, and ultimately, insights. For example, Iron Man may perceive a series of observations like "{"Captain America": "(Punched Iron Man in the face)}n{"Iron Man": "Why did you kill my father?"}n...". These observations can be aggregated into an event described as "Iron Man and Captain America had a flight regarding to the death of Iron Man' father." Subsequently, another event may occur: "Iron Man and Captain America had apologized for each other." These two events can then be distilled into a key point: "The contradiction between Iron Man and Captain America about the death of his father is resolved." Coupled with other key points such as "Iron Man and Captain America fight together for the Earth." and all the supporting events, we can derive the insight "Iron Man prioritizes the survival of the planet."

### Interacting Stage

In Fig. 2, the interacting stage with a pre-built RoleAgent can be divided into four steps: (1) query deconstruction (2) memory retrieval, (3) memory summarization, and (4) response generation.

Query DeconstructionFor a RoleAgent \(A\), when a player \(P\) presents a query \(q\), the query is first deconstructed into three variants: (1) "Who is \(A\)?" (\(q_{1}\)), (2) "Who is \(P\)?" (\(q_{2}\)), and (3) "\(P\) said: \(q\)" (\(q_{3}\)), which aims to analyze the positions of the player \(P\) and the RoleAgent \(A\) and the rationale of the query \(q\) and provide additional cues for further interactions.

Memory RetrievalGiven deconstructed queries, we retrieve the relevant high-order memories from events, key points, and insights and produce corresponding memories: \(M\). and then the initially retrieved memories are re-ranked based on the following scores. 2

We assign four distinct scores to each item in the high-order memories: (1) a dynamic importance score \(s_{di}\), (2) a static importance score \(s_{si}\), (3) a timeliness score \(s_{t}\), and (4) a correlation score \(s_{c}\). Specifically, first, the dynamic importance score is the frequency with which memory is retrieved throughout the interaction stage, denoting the relevance between the memory and the actual interaction queries. Second, following , the static importance score is to enable the LLM to assess the generic importance of a particular memory, where the score is from 0 to 5. Third, timeliness score assigns a higher score to memories that were recently accessed, and we employ an exponential decay function with a decay factor of 0.99 following . Fourth, the correlation score assigns a higher score to the most relevant memories by computing the cosine similarity of embedding vectors of memories. Finally, we normalize these four scores to the range of  by min-max scaling, and calculate the overall score \(s\) for each memory as follows: \(s=s_{di}+s_{di}+s_{t}+s_{c}\). This mechanism ensures that the most relevant, important, and timely memories are utilized in response to the player's query.

To provide a more tangible memory context, we propose the **memory replay** to replay the observations \(O\) indexed from the retrieved high-order memories, and combine these observations with the high-order memories \(M\) to generate the refined memories \(M^{}\), which are sent to LLMs.

Memory SummarizationThe refined memories \(M^{}\) are fed into LLMs for summarization and five distinct instructions are used: three are aimed at eliciting subconscious memories, one is to reason the player's intention, and the final instruction is to compress the relevant content. First, we build **Subconscious Memories**. Specifically, the subconscious memories encompass three categories of information: (1) the character traits and position of the RoleAgent \(A\), (2) the character traits and position of the player \(P\), and (3) the relationship between \(A\) and \(P\). To facilitate summarization, we employ specific instructions: for \(A\), we use "Summarize \(A\)'s character traits and position, avoiding embellishment.", for \(P\), the instruction is "Summarize \(P\)'s character traits and position, avoiding embellishment.", and to elucidate their relationship, we prompt with "Infer and briefly describe the relationship between \(A\) and \(P\).". These instructions are applied to \(M^{}\) to generate summaries, which are then concatenated to form the subconscious memories, denoted as \(C_{sub}\). Second, we apply **Intention Reasoning and Memory Compression**. Specifically, we prompt LLMs to reason about \(P\)'s intention with the instruction "Why does \(A\) think \(P\) said: "\(q\)"?", using the refined memory set \(M^{}\). The generated response is denoted as \(C_{int}\). Moreover, we use the instruction "Summarize the relevant content of \(P\) said: "\(q\)"." for LLMs to compress \(M^{}\) into \(C_{rel}\).

Response GenerationWe concatenate \(C_{sub}\), \(C_{int}\), \(C_{rel}\), and "\(A\) observed \(P\) said: "\(q\)"." with the task instruction "Please generate a response", using a specific prompt template. This assembled prompt is to instruct the LLM to generate a response with role-specific knowledge and episodic memories of the RoleAgent for the player.

## 4 RoleAgentBench

To evaluate RoleAgent, we construct the RoleAgentBench including 128 roles from 5 Chinese and 20 English scripts. Besides, our RoleAgentBench evaluates two aspects (i.e., the qualities of the overall agent simulation and the specific memory system) with 4 subtasks, and we illustrate the construction details as follows. Note that all questions and answers are generated based on the script and GPT-4, which are then revised by human annotators. See Appendix C for samples of RoleAgentBench.

### Agent Simulation

To evaluate whether RoleAgent simulates roles well, we evaluate the following three parts.

**Self-Knowledge**: Self-Knowledge tests the Agent's ability to recognize its attributes in the form of true or false questions format, in which the Agent has to judge the four questions related to itself. These questions focus on the occupation, relationships, name, and personality, where each question has a corresponding reference answer (True or False). We use the **accuracy** for Self-Knowledge.

**Reaction**: Reaction tests the Agent's ability to react to responses for different roles. For example, given the same question, a specific Role A will generate different answers for different roles based on the relationships or positions between Role A and other roles. We use the **accuracy** for reaction.

**General Response**: General Response tests the Agent's general communication ability in question-answer format. Role A asks a question to role B, and RoleAgent needs to simulate role B to reply to the question. Each question has a reference answer, which is highly accurate and stylized for role B. **Win rates** are reported based on human and GPT-4, where 3 human annotators are employed.

### Memory System

To test the capabilities of the memory system, we mainly evaluate the summarization qualities.

**Summarization**: As summarization is a high-density content, we evaluate the **entity density (ED)** of the generated summary by extracting the entities of the summary and dividing the number of entities by the summary length. Higher entity density denotes a higher information density. We also obtain the **entity recall, (ER)** between the entities of the generated summaries and the golden summary entities, where higher recall indicates higher qualities. Besides, we report the ER/ED results to denote the ratio of valid entities. Meanwhile, **win rates** using GPT-4 and human are also reported.

### Statistic Analysis

In Table 6 of Appendix C, we provide the number of samples on 20 English and 5 Chinese scripts of each task. In Fig. 3(a) and Fig. 3(b), we provide the length distribution of the addition between questions and answers for general response and summarization tasks. In Fig. 3(c), the ratios of all questions for each script are provided. Note that "SK", "REA", "GR", "SUM" denote "self-knowledge", "reaction", "general response" and "summarization", respectively.

## 5 Experiments

### Main Results

In Table 1 and Table 2, we provide the results of our RoleAgent based on different LLMs, where GPT-4 , Qwen-Max , GPT-3.5 , Yi-34B , Qwen1.5-14B , Mistral-7B 3, LLaMA3-8B , ChatGLM3-6B  and RoleLLM  are used as baselines. Note that we follow RoleLLM to reproduce RoleLLaMA and RoleGLM using the RoleBench dataset, and we use RoleLLaMA and

Figure 3: (a). General Response. (b). Summarization. (c). Distribution of subtasks.

RoleGLM to test the English and Chinese subsets of RoleAgentBench, respectively. Our analysis led to the following key findings: (1) We observe that API-based models (e.g., GPT-4 and Owen-Max) achieve significant improvements when compared to these open-source models (e.g., Yi-34B and Qwen1.5-14B), which shows that the capacities of base models influence the agent's abilities a lot. (2) The scores on the Chinese subset of English-centric LLM (i.e., LLaMA3-8B) are lower than the corresponding results on the English subset. In contrast, the bilingual (English-Chinese) LLMs (e.g., Qwen1.5-14B, Yi-34B) achieve comparable results on English and Chinese subsets. (3) We observe that RoleLLM fine-tuned on the role-related dataset cannot perform well in our dataset. We suppose the RoleLLM only focuses on improving the speaker style of the simulated roles, which is essentially different from the four subtasks in RoleAgentBench. (4) These well-performed LLMs (e.g., GPT-4 and Qwen-Max) obtain lower ED and higher ER/ED results, which means that the number of entities is relatively smaller but the number of valid entities is higher than the results of other LLMs, which further shows the effectiveness of powerful LLMs.

### Ablation Study

To show the effect of our hierarchical memory system and memory replay for memory retrieval, we perform experiments in Table 3 based on GPT-3.5. Note that "HM" and "MR" denote hierarchical memory and memory replay, respectively. In Table 3, when removing the memory replay, the performance results degrade, which shows that it is beneficial to use the raw extracted observations related to the events or insights for better results. Moreover, when removing the hierarchical memory system, the performance results degrade a lot, specifically the self-knowledge and reaction sub-tasks, which demonstrates the advantage of the hierarchical memory system.

    &  &  \\   & **Self-Knowledge** & **Reaction** & **General Response** &  \\   & Acc. & Acc. & WR-G & WR-H & ED & ER & ER/ED & WR-G & WR-H \\ 
**GPT-4** & **94.7** & 88.9 & **48.6** & 49.7 & 8.9 & 39.8 & **4.47** & **75.8** & 59.8 \\
**Qwen-Max** & 94.1 & **92.9** & 46.8 & **54.3** & 9.8 & 41.8 & 4.27 & 72.4 & **62.9** \\ 
**GPT-3.5** & 87.6 & 76.7 & 37.6 & 41.5 & 11.6 & 36.8 & 3.17 & 56.7 & 51.6 \\
**Yi-34B** & 88.5 & 73.8 & 43.5 & 42.5 & 11.3 & 41.1 & 3.64 & 55.1 & 44.7 \\
**Qwen1.5-14B** & 83.7 & 71.0 & 40.5 & 45.3 & 11.4 & 40.2 & 3.53 & 41.7 & 38.0 \\
**Mistral-7B** & 87.7 & 64.9 & 40.1 & 37.4 & 10.7 & 40.5 & 3.79 & 44.9 & 48.9 \\
**LLaMA3-8B** & 84.0 & 68.1 & 36.6 & 38.9 & 10.9 & 32.8 & 3.01 & 47.0 & 50.2 \\
**RoleLLM** & 61.0 & 31.0 & 25.3 & 15.2 & 17.2 & 23.7 & 1.38 & 25.4 & 10.7 \\   

Table 1: Performance on the English subset of RoleAgentBench. “WR-G”, “WR-H”, “ED”, and “ER” are win-rate of GPT-4, win-rate of human evaluation, entity density, and entity recall, respectively.

    &  &  \\   & **Self-Knowledge** & **Reaction** & **General Response** &  \\   & Acc. & Acc. & WR-G & WR-H & ED & ER & ER/ED & WR-G & WR-H \\ 
**GPT-4** & **92.5** & 73.9 & **44.8** & 39.7 & 10.0 & 45.0 & 4.50 & 75.1 & 69.5 \\
**Qwen-Max** & 90.3 & **84.7** & 39.4 & **41.5** & 8.8 & 44.7 & **5.08** & **79.6** & **71.4** \\ 
**GPT-3.5** & 84.8 & 61.3 & 34.9 & 35.6 & 14.9 & 38.3 & 2.57 & 53.7 & 61.5 \\
**Yi-34B** & 80.0 & 76.7 & 38.2 & 29.1 & 8.1 & 40.6 & 5.01 & 61.6 & 58.4 \\
**Qwen1.5-14B** & 82.7 & 73.8 & 38.7 & 33.7 & 11.1 & 43.1 & 3.88 & 67.0 & 62.3 \\
**ChatGLM3-6B** & 81.3 & 59.6 & 36.9 & 39.1 & 16.0 & 36.6 & 2.29 & 44.2 & 35.8 \\
**LLaMA3-8B** & 70.1 & 62.4 & 25.1 & 29.9 & 10.9 & 35.1 & 3.22 & 43.4 & 42.0 \\
**RoleLLM** & 72.1 & 59.3 & 20.6 & 25.7 & 16.0 & 35.3 & 2.21 & 39.2 & 45.1 \\   

Table 2: Performance on the Chinese subset of RoleAgentBench.

### Further Analysis

**Compare with Generative Agents.** We use the GPT-3.5 as the baseline LLM to compare our RoleAgents with Generative Agents (**Gen. Agents**)  on the English subset of RoleAgentBench in Table 4. Note that Generative Agents heavily rely on extensive human efforts to produce the role profiles. In Table 4, we observe our RoleAgent is better than Generative Agents a lot on these subtasks. For example, our RoleAgent can predict self-knowledge well without using any human efforts, which means that RoleAgent can understand the basic attributes (e.g., career or relationships) of simulated roles well. Second, our RoleAgent produces better reaction and summarization abilities, where can be attributed to the intention reasoning strategy and subconscious memories in Sec. 3.3.

**Analysis on common and uncommon scripts.** We take GPT-3.5 as the baseline LLM to analyze the results on several common and uncommon scripts in Table 5, which aims to clarify if the LLM is inferring from scripts or recalling information stored in model weights. Specifically, for common scripts, we select "Harry Potter" and "Friends". for uncommon scripts, we select "Alias" and "Degrassi Next Generation". Then, for the GPT-3.5 (Internal), we just prompt the GPT-3.5 to generate the answer to all questions of these subtasks without using any additional information. In contrast, GPT-3.5 (External) is our RoleAgent based on GPT-3.5, which can infer from scripts to obtain additional contexts. In Table 5, first, we observe that the results of GPT-3.5 (External) using RoleAgent are better than GPT-3.5 (Internal) a lot on both common and uncommon scripts, which means that RoleAgent can simulate agents well based on our hierarchical memory. Second, we observe that results of GPT-3.5 (Internal) drop a lot on uncommon scripts on self-knowledge and reaction, which shows it is necessary to use RoleAgent for simulating different roles.

**Visualization on the hierarchical memory of building stage.** We take the script of Harry Potter as an example to show the process of producing the hierarchical memory of RoleAgent. In Fig. 4(a), we provide extracted high-order hierarchical memories with high scores after the initialization procedure on the Harry Potter script. See Fig. 7 in Appendix D for more details on the building stage.

**Visualization of each step of the interaction stage.** In Fig. 5, we visualize four interaction steps by playing with RoleAgent "Harry". Note that the human plays the role of "Hermione". Given the query of "Hermione", our RoleAgent "Harry" produces a high-quality and interesting response, which shows the effect of RoleAgent. See Fig. 12, Fig. 13 and Fig. 14 for more results in Appendix E.

**Visualization on the interaction between Human and RoleAgent.** We add the visualization by playing with RoleAgent "Sheldon". Note that the human plays the role of "Lenoard". Specifically, after the building stage of RoleAgents from "Scene: A corridor at a sperm bank" (See Fig. 15 of Appendix for more details on this scene of the script "Friends"), we begin to interact with the RoleAgent "Sheldon". The overall interaction process is shown in Fig. 16 of the Appendix E, and we take some interesting samples as shown in Fig. 4(b), where we highlight in the red box to show some high-quality examples of agent simulation for "Sheldon".

    &  &  \\   & **Self-Knowledge** & **Reaction** & **General Response** &  \\   & Acc. & Acc. & WR-G & ED & ER & ER/ED & WR-G \\ 
**Gen. Agents** & 68.9 & 40.7 & 31.8 & 5.1 & 20.7 & 4.06 & 38.9 \\ 
**RoleAgent** & 87.6 & 76.7 & 37.6 & 11.6 & 36.8 & 3.17 & 46.7 \\   

Table 4: Performance on English subset of RoleAgentBench. “**Gen. Agents**” is “Generative Agents”.

    &  &  \\   & **Self-Knowledge** & **Reaction** & **General Response** &  \\   & Acc. & Acc. & WR-G & ED & ER & ER/ED & WR-G \\ 
**RoleAgent** & **87.6** & **76.7** & **37.6** & 11.6 & 36.8 & 3.17 & **46.7** \\ 
**w/o MR** & 73.6 & 76.4 & 35.2 & 10.4 & 34.7 & **3.34** & 35.4 \\
**w/o MR\&HM** & 70.3 & 54.5 & 32.2 & 11.3 & 35.7 & 3.16 & 29.5 \\   

Table 3: Performance on the English subset of RoleAgentBench.

## 6 Conclusion

In this study, we introduce a framework named RoleAgent designed to construct agents directly from unprocessed scripts, which consists of building and interacting stages. This approach minimizes the need for manually created agent profiles and enables the autonomous generation of inventive and engaging agents. During the construction phase, we implement a hierarchical memory system to logically organize and preserve the structural and advanced memories associated with various characters. For the interaction phase, we deploy an innovative four-step mechanism to capture ample context and produce responses of superior quality. To assess the performance of RoleAgent, we have established an extensive evaluation benchmark termed RoleAgentBench. Extensive experiments on RoleAgentBench demonstrate the superior capabilities of the RoleAgent framework.

Figure 4: (a). Samples of events, key points and insights. (b). Interaction between human (“Lenoard”) and RoleAgent (“Sheldon”). “(1)”, “(2)” and “(3)” are “(2)”, “(3)” and “(6)” of Fig. 16, respectively.

Figure 5: Interaction between human (“Hermione”) and our RoleAgent (“Harry”).

   &  &  \\   & **Self-Knowledge** & **Reaction** & **General Response** &  \\   & Acc. & Acc. & WR-G & ED & ER & ER/ED & WR-G \\   \\ 
**GPT-3.5 (Internal)** & 82.5 & 57.6 & 29.4 & 10.7 & 16.4 & 1.53 & 20.5 \\
**GPT-3.5 (External)** & 90.4 & 77.8 & 45.1 & 12.2 & 37.8 & 3.10 & 47.2 \\   \\ 
**GPT-3.5 (Internal)** & 55.0 & 24.3 & 26.7 & 10.2 & 18.4 & 1.80 & 31.9 \\