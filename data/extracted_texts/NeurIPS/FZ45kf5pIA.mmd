# Edit Distance Robust Watermarks for Language Models

Noah Golowich

nzg@mit.edu

MIT &Ankur Moitra

moitra@mit.edu

MIT

###### Abstract

Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) _undeectability_, a cryptographic notion introduced in  which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) _robustness_ to channels which introduce a constant fraction of _adversarial_ insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to _edit distance_.

Our main result is a watermarking scheme which achieves both undetectability and robustness to edits when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced in , which proceeds via first constructing _pseudorandom codes_ satisfying undetectability and robustness properties analogous to those above; our key idea is to handle adversarial insertions and deletions by interpreting the symbols as indices into the codeword, which we call _indexing pseudorandom codes_. Additionally, our codes rely on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for _arbitrary_ language models.

## 1 Introduction

The rapid increase in AI-generated content represents a significant challenge for numerous societal institutions, ranging from education to social media. For instance, the ability of large language models such as GPT-4 to generate long bodies of text such as essays raises the possibility of increasing amounts of plagiarism, and the proliferation of AI-generated text, images, and videos on social media presents challenges regarding large-scale manipulation of online audiences. An important tool at our disposal in preventing the misuse of such content is _watermarking schemes_, which are procedures that embed hidden patterns in AI-produced content using a _secret key_. Watermarking schemes allow a _detection algorithm_ with the aid of the secret key to determine with high probability that the content was produced by the AI model. They do so without noticeably altering the content from the perspective of any algorithm not possessing the secret key.

Despite a number of recently proposed watermarking schemes, taking both a theoretical perspective (e.g., ) as well as a more empirical one (e.g., ,,,, ), a crucial challenge that remains elusive is that of ensuring the watermark be _robust_ to adversaries which can modify the generated content. A watermarking scheme is of little use if it is too easy to change the model's output so as to remove the watermark. On the other hand, a sufficientlyresourceful adversary can simply train their own unwatermarked model. Thus it is necessary to strike a balance in terms of the power of adversaries to which a watermarking scheme enjoys robustness. In this paper, we give the first watermarking schemes with provable robustness to adversaries that make a constant fraction of arbitrary substitutions, insertions, and deletions to the output. This is a substantial improvement over previous schemes , which could only tolerate a constant fraction of adversarial substitutions or a constant fraction of i.i.d. deletions under additional stochasticity assumptions on the language model. Our results therefore represent progress towards the overarching goal of constructing watermarking schemes which are robust to resource-limited adversaries.

### Setup: watermarking schemes

We consider the task of watermarking the output of a _language model_Model, which is simply defined as a mapping from a sequence of _tokens_\(_{1},,_{i-1}\) to a distribution over the next token.1 Though we use the terminology "language model" and "text" throughout the paper, our framework can be used to model a host of autoregressive models including those for generation of images, audio, and videos (e.g., ) in addition to text. As this paper is theoretical in nature, we work only with this abstraction of an autoregressive model, keeping in mind that single tokens could represent, e.g., sequences of letters  or discretized patches of an image .2

A _watermarking scheme_\(\) for Model consists of a tuple of efficient algorithms,

\[=(,,)\]

where \(\) generates a secret key at random, \(\) uses Model and the secret key to produce a sequence of text with a watermark embedded in it, and Detect uses the secret key to determine whether a given sequence of text is watermarked. Moreover, Detect has no knowledge of Model. \(\) should satisfy the following three properties:

* _Undetectability :_ Text generated from the watermarking protocol \(\) should be computationally indistinguishable to text generated by the true model Model, to any polynomial-time algorithm which can repeatedly query either \(\) or Model. (Other notions, namely _distortion-freeness_, have been considered in place of undetectability, but are significantly weaker; see Example A.1.)
* _Soundness:_ Any fixed sequence of text (not produced by \(\)) should not be detected as watermarked, with high probability (over the generation of the secret key).
* _Edit-Robustness:_ A sequence which can be obtained by calling the watermarking procedure \(\) and then making a constant fraction of adversarial _edits_ (i.e., substitutions, insertions, and deletions) to its output will still be detected as watermarked by Detect with high probability.

Whereas several existing watermarking schemes achieve undetectability and soundness, the main contribution of our work is to achieve the notion of _edit-robustness_ given above. Robustness, broadly construed, has long been recognized as essential for constructing useful watermarking schemes , yet nearly all of the existing provable guarantees on watermarking do not yield robustness to any constant fraction of edits. We remark that some works (e.g., ) can handle a _subconstant_ fraction of edits. However, we argue that the right notion of adversary is that which makes a _constant_ fraction of edits: there is a relatively low bar for a malicious adversary attempting to remove the watermark by changing the text at a constant rate while avoiding significant quality degradations, such as randomly replacing \(5\%\) of words with a synonym. Moreover, even if an adversary is not actively trying to remove a watermark, it is reasonable to expect that an "honest" editor who is merely trying to improve the text's quality will introduce edits at a constant rate. The lens of coding theory provides further motivation for recovering from such channels that introduce a constant fraction of errors: doing so is a longstanding gold standard in the field .

The only prior work in the literature which obtains watermarking schemes with provable robustness guarantees against such "constant-rate" attacks was , which constructed watermarking schemes robust to a constant fraction of deletions and substitutions. Moreover, in order to handle any deletions at all,  needs to assume that: (a) the deletions and substitutions are made _independently_ at each position with some fixed probability; and (b) the language model is equivalent to a binary symmetric channel in a certain sense, a strong assumption which is contradicted by decades of study in linguistics, which posit that earlier words strongly influence the distribution of subsequent words .3

Finally, we emphasize that the task of obtaining robustness against adversaries that can make a constant fraction of insertions, deletions, and substitutions, as opposed to merely substitutions, is well-motivated by an extensive line of work on _edit distance_ (e.g., , amongst many others). The edit distance between two strings is the minimum number of insertions, deletions, and substitutions needed to transform one string into the other. Edit distance has found numerous applications in areas ranging from computational biology to information retrieval . This fundamental role of edit distance results from the fact that many natural processes induce small edit-distance perturbations on strings: for instance, mutations of DNA can involve insertions or deletions as well as substitutions, as do the changes people typically make to documents. Procedures that make such changes therefore represent a reasonable adversary in our present setting of watermarking.

### Main result

Our main result states that watermarking schemes with all of the aforementioned properties exist under a standard cryptographic assumption:

**Theorem 1.1** (Informal version of Theorem E.2).: _Suppose that local weak PRFs exist (per Assumption 3.1). Then for any security parameter \(\), there is a watermarking scheme over an alphabet of size \(()\) for model outputs of length \(()\), which is sound, undetectable with respect to algorithms running in time \(()\), and robust to a constant fraction of substitutions, insertions, and deletions when the sequence of generated text has "entropy rate" at least some constant._

As is typical in cryptographic settings, the guarantee of Theorem 1.1 involves a security parameter \(\), which controls the power of a distinguishing algorithm (in the context of undetectability) as well as the failure probability of soundness and edit-robustness (which are \(()\); see the formal version in Appendix E). The requirement that the sequence of generated text have constant entropy rate (meaning that on average, the tokens are generated from conditional distributions which have entropy at least a constant fraction of the maximum possible entropy) is easily seen to be necessary,4 and lower bounds on the entropy are standard amongst essentially all watermarking schemes, even without any robustness requirement . Indeed, watermarking a near-deterministic language model is impossible since it can only produce a small number of outputs; the entropy assumption quantifies the extent to which the language model is near-deterministic.

One potential limitation of Theorem 1.1 is the requirement that the alphabet size of the language model (i.e., number of tokens) grow as a polynomial in \(\). This limitation is mitigated by the following observations: first, in many domains, the number of tokens used by existing (or future) tokenizers may already be quite large, i.e., at least in the thousands ; second, in practice one could use various heuristics to increase the number of tokens, such as by grouping together consecutive sequences of tokens to create larger alphabets of "mega-tokens". Finally, again invoking the language of coding theory, a common approach to constructing good codes is via _concatenation_, which combine an _outer code_ with large alphabet together with an _inner code_ that encodes individual symbols of the large alphabet using a smaller alphabet. Theorem 1.1 (as well as Theorem 4.1, discussed below) could play the role of the outer code in such a concatenation strategy for watermarking schemes; finding the analogue of an inner code to decrease the alphabet size represents an important direction for future work.

Roadmap of techniques.The proof of Theorem1.1 proceeds via constructing new _pseudorandom codes (PRCs)_, which are cryptographic objects originally introduced by  to derive watermarking schemes robust to i.i.d. random (i.e., non-adversarial) substitutions and deletions. Roughly speaking, a pseudorandom code is an error-correcting code equipped with a secret key used for encoding and decoding, which looks random to any polynomial-time algorithm that does not hold a secret key. We establish the following ingredients pertaining to PRCs:

* First, we design a a PRC over the binary alphabet which is robust to a constant fraction of adversarial substitutions, under Assumption3.1 (Theorem3.2; see Section3). This assumption is, qualitatively speaking, _weaker_ than the cryptographic assumptions in .
* Next, we give a generic reduction which, given any PRC over the binary alphabet robust to substitutions, yields a PRC over a polynomial-sized alphabet robust to any constant fraction of substitutions, insertions, and deletions (Theorem4.1; see Section4). A central idea in this latter PRC is to interpret symbols of the larger alphabet as _indices_ into codewords of the former PRC; hence, we call it an _indexing PRC_.
* Finally, we establish a generic reduction which converts any PRC over a larger alphabet robust to substitutions, insertions, and deletions to a watermarking scheme with analogous robustness properties (TheoremE.1; see Section5).

Theorem1.1 follows by composing the components above. These components have the advantage of being modular in nature, meaning that one could, for instance, plug in the substitution PRCs of  in place of the first item to obtain a guarantee analogous to Theorem1.1 but under the (qualitatively stronger) cryptographic assumptions of .

In order to handle deletions,  uses a type of PRC they call a _majority code_. This code is only robust when the substitutions and deletions are made in an i.i.d. manner. In their reduction that converts a PRC to a watermarking scheme, the language model may induce certain errors on the PRC codewords. Since these errors may not be correctable by the majority code unless they are assumed to be i.i.d.,  need to assume that the language model is equivalent to a BSC. Our use of indexing PRCs avoids this significant shortcoming.

## 2 Preliminaries

We begin with the definition of a _pseudorandom code (PRC)_, as introduced in . A pseudorandom code is defined over an _alphabet_\(\), which is simply a finite set. We denote the set of all finite-length strings over \(\) by \(^{*}\). As is typical when defining cryptographic primitives, our pseudorandom codes depend on a _security parameter_\(\); as \(\) is increased, the amount of security afforded by the PRC also increases. A function \(f:_{ 0}\) is called _negligible_ if for any \(C>0\), there exists \(_{0}\) so that \(f()^{-C}\) for all \(>_{0}\). We use \(()\) to denote a negligible function, whose precise value can change from line to line.

Given an alphabet \(\), a _channel_ is a mapping \(\) which associates to any \(x^{*}\) a distribution \((x)\) over \(^{*}\). With slight abuse of notation, we will often let \((x)\) denote a random variable \(y\) drawn from \((x)\): for instance, when we write a statement of the form "with probability \(1-(n)\), \(D_{}(x,(x)) pn\)", we mean that \(D_{}(x,y) pn\) with probability \(1-(n)\) over \(y(x)\).

We primarily focus on _secret key_ PRCs for simplicitiy; our arguments (in particular, those regarding edit-robust PRCs) apply identically to the case of public-key PRCs [10, Definition 2], though for simplicity we focus solely on secret-key PRCs. A secret-key PRC may be viewed as a variant of a secret-key encryption scheme in which the ciphertext enjoys certain robustness properties to adversarial perturbations. Formally, a PRC is specified by functions \(\), \(\), \(\), which behave as follows: \(\) outputs a secret key, \(\) uses the secret key to "encrypt" a message, and \(\) uses the secret key to "decode" a string. If the string passed to \(\) is the output of \(\), perhaps with a bounded number of adversarial perturbations, then \(\) should output the message originally passed to \(\) (_robustness_). Outputs of \(\) should also look random to polynomial-time algorithms (_undetectability_), and \(\) should almost always fail when given any fixed string (i.e., not an output of \(\)) as input. These definitions are formalized below:

**Definition 2.1** (Secret-key Pseudorandom code (PRC)).: Let \(\) denote a security parameter, and suppose that for each \(\) an associated alphabet \(()\) is given. Moreover, suppose that for each \(\), a collection \(()\) of channels \(:()^{}(()^{})\) is given. A _secret-key pseudorandom code (PRC)_ with robustness to \(\) is a triple of probabilistic polynomial-time algorithms \((,,)\) satisfying the following:

* For some functions \(n\), \(k\), \(_{}:\), for all \(\), we have \((1^{})\{0,1\}^{_{}()}\), \(:\{1^{}\}\{0,1\}^{_{}()} ^{k()}^{n()}\), and \(:\{1^{}\}\{0,1\}^{_{}()} ^{}^{k()}\{\}\).
* For any \(\) and message \(^{k()}\), the code is _robust_ to any channel \(()\), which means that: \[_{(1^{})}(( 1^{},,(x))= x(1^{},,)) 1-().\] Moreover, for any fixed \(y^{}\), the code is _sound_, which means that: \[_{(1^{})}(( 1^{},,y)=) 1-( ).\]
* The code is _undetectable_ which means that: for any \(\) for any probabilistic polynomial-time adversary \(\), \[|_{(1^{})}(^{(1^{},,)}(1^{})=1)-_ {(1^{})}(^{ }(1^{})=1)|(),\] (1) where \(\) denotes an oracle which responds with a freshly drawn uniformly random string in \(^{n()}\) on each call (to any input).

The _block length_ of the PRC is defined to be \(n()\). For all of our pseudorandom codes, we will have \(n(),k(),_{}()()\). Moreover, we take as a convention that \(n()\) for all \(\) (this may be ensured without loss of generality by rescaling \(n()\)). Our focus will primarily be on _zero-bit PRCs_, which are particularly useful for watermarking language model outputs.

**Definition 2.2** (Zero-bit PRC).: A _zero-bit_ PRC is one for which \(k()=0\) for all \(\), i.e., the only possible message is \(=\).

[11, Section 6] shows a generic reduction that converts any zero-bit PRC into a general PRC with constant rate, meaning that \(k()/n()=(1)\). We remark that the same reduction can be applied to our PRCs with edit robustness, though we do not pursue this direction further in this paper.

**Definition 2.3** (Substitution-bounded).: For \(p(0,1)\), a channel \(\) over alphabet \(\) is _\(p\)-substitution-bounded_ if for any \(n\), \(y^{n}\), for \(z(y)\), \(D_{}(y,z) pn\) holds almost surely.

**Definition 2.4** (Edit-bounded channel).: Fix an alphabet \(\) together with \(p(0,1)\). A channel \(\) over \(\) is defined to be _\(p\)-edit-bounded_ if for any \(x^{}\) with \(n:=|x|\), \(y(x)\) may be obtained from \(x\) by applying a total of at most \(pn\) substitutions, insertions, and deletions, almost surely. Moreover, there exists a probabilistic polynomial-time algorithm which, given \(x\), outputs a sample \(y(x)\).5

## 3 Secret-key substitution PRCs from weaker assumptions

In this section, we discuss a new construction of binary PRCs for substitution channels. Though such PRCs were also obtained by , the codes in  relied on relatively strong average-case hardness assumptions, in the sense that they imply the existence of public-key cryptography (i.e., in the context of Impagliazzo's Five Worlds , they imply primitives in "Cryptomania"). In contrast, our construction relies only on the hardness of the existence of a family of pseudorandom functions that enjoys a certain locality property; such an assumption is generally believed to be weaker than the ones in , in the sense that it is only known to yield cryptographic primitives in "Minicrypt", though we are not aware of a formal separation.

We first recall the definition of pseudorandom function (PRF) families, which are PRF families for which the adversary can only query the pseudorandom function at a uniformly random input \(x\).

**Definition 3.1** (Weak PRF family).: Fix functions \((),n():\) of a security parameter \(\), and a collection of functions \(\{F_{s}:\{0,1\}^{n()}\{0,1\}\}\), indexed by \(s\{0,1\}^{()}\), for \(\). We say that the collection \(\{F_{s}\}_{s}\) is a _weak pseudorandom function family (weak PRF)_ if for every probabilistic polynomial-time algorithm \(\) which outputs a single bit (i.e., 0 or 1), it holds that

\[|_{s\{0,1\}^{()}}[}^ {F_{s}()}(1^{n()})]-_{F_{}}[ }^{F_{}}(1^{n()})]| (),\] (2)

where \(}^{G}\), for a mapping \(G:\{0,1\}^{n()}\{0,1\}\) means that \(\) can make calls to the function \(G\), and for each call receives a tuple \((x,G)\), for \(x(\{0,1\}^{n})\). (In particular, the tilde refers to the fact that \(\) can only call \(G(x)\) on a _uniformly chosen_\(x\).) Moreover, \(F_{}:\{0,1\}^{n}\{0,1\}\) denotes a uniformly random function. We will often refer to the function \(G\) that \(\) can make queries to as the _oracle_\(\) has access to. Given \(q[0,1/2)\), we say that \(\{F_{s}\}_{s}\) is a _weak PRF family with noise level_\(q\) if (2) holds where each call to \(F_{s}()\) by \(\) returns \(F_{s}(x) e\) where \(x(\{0,1\})^{n}\) and \(e(q)\).

Our new construction of PRCs is based off of _local (weak) PRFs_, which are PRFs for which the output depends on a small number of input bits.

**Definition 3.2** (Local function family).: Let \(\). A family of functions \(\{F_{s}:\{0,1\}^{n}\{0,1\}\}\) indexed by \(s\) is defined to be \(\)_-local_ if for each \(s\), \(F_{s}(x)\) only depends on at most \(\) bits of \(x\), i.e., for each \(s\) there are distinct indices \(j_{1},,j_{}[n]\) together with a function \(G_{s}:\{0,1\}^{}\{0,1\}\) so that \(F_{s}(x)=G_{s}(x_{j_{1}},,x_{j_{}})\) for all \(x\{0,1\}^{n}\).

Our main computational assumption is the existence of a weak PRF family which is \(\)-local for \(\) of _logarithmic_ size:

**Assumption 3.1** (Local Weak PRFs).: _For some functions \((),n(),():\) with \((),n()()\) and \(() n()\), there exists a weak PRF family \(\{F_{s}:\{0,1\}^{n()}\{0,1\}\}_{s\{0,1\}^{()}}\) for some noise level \(q<1/2\) which is \(()\)-local, for each \(\)._

In Appendix C.2, we discuss how Assumption 3.1 follows from standard average-case hardness assumptions, notably the hardness of learning \((n)\)-juntas over \((\{0,1\}^{n})\). As specific examples, either hardness of the \((n)\)-sparse noisy parity problem  or hardness of weakly learning a particular family of functions presented in  (see also ) implies Assumption 3.1.

The PRC construction.Our construction of PRCs based on Assumption 3.1 is presented in Algorithm 2: given a function family \(\) satisfying Assumption 3.1 with noise level \(q\) together with some \(p<1/2\) representing the maximum fraction of substitutions to correct, we construct \([,p,q]=, ,\) as follows. The construction depends on some parameters \(N(),m()()\), specified in (3):

* \((1^{})\) chooses a function in \(\) (indexed by \(s\)) together with a uniformly random \(z(\{0,1\}^{N()})\) and a uniform permutation \(:[N()][N()]\), and returns \(=(s,z,)\).
* \((1^{},(s,z,),)\) draws \(m()\) uniformly random elements of \(\{0,1\}^{n()}\), \(x_{1},,x_{m()}\), applies \(F_{s}\) to each of them and flips the result with probability \(q\) to obtain bits \((w_{1},,w_{m()})\), and perturbs the concatenation \(((x_{i},w_{i}))_{i[m()]}\) according to \(z,\) as on Line 2.
* \((1^{},(s,z,),y)\) first "unperturbs" \(y\) to obtain a string \(((x_{i},w_{i}))_{i[m()]}\) as in \(\), and then outputs \(\) if \(_{j=1}^{m()}\{w_{j}=F_{s}(x_{j})\}\) is above a threshold; otherwise, it outputs \(\).

**Theorem 3.2**.: _Given \(p,q<1/2\) and a function family \(\) together with noise level \(q\) satisfying Assumption 3.1, then \([,p,q]\) (Algorithm 2) is a zero-bit binary-alphabet secret-key PRC (per Definition 2.2) with robustness to all \(p\)-bounded substitution channels._

The proof of Theorem 3.2 is given in Appendix C.3.

## 4 From substitution PRCs to edit-robust PRCs

In this section, we discuss our construction of PRCs which are robust to edit-bounded channels. To do so, we reduce to PRCs robust to substitution-bounded channels. Suppose we are given a PRC\(_{}\) with block length \(n()\) over the binary alphabet which is robust to any \((1/2-p_{0})\)-bounded substitution channel, for \(p_{0}(0,1/2)\). Given a parameter \( 1\), we construct an _indexing PRC_, \(_{}[_{},]\) (in Algorithm 1), which is robust to any \(p\)-edit-bounded channel, where the parameter \(p\) depends on \(p_{0},\) in a manner that will be explained below. The code \(_{}[_{},]\) has polynomially large alphabet \(():=[q()]\), where \(q()= n()\). We denote the block length of \(_{}[_{},]\) by \(m()\) (which is defined to be \((2) n()\)) to distinguish it from the block length \(n()\) of \(_{}\).

Construction of \(_{}[_{},]\).The idea behind \(_{}[_{},]\) is simple: we interpret each symbol of \(_{}[_{},]\) as an _index_ into a codeword of \(_{}\), so that the existence of a symbol in a given codeword of \(_{}[_{},]\) should be interpreted as the corresponding codeword for \(_{}\) as having a "1" in the position corresponding to that symbol. To ensure stronger robustness guarantees, it turns out to be necessary to introduce redundancy in the sense that for each integer \(j[n()]\) (representing an index of a codeword of \(_{}\)), there are \(\) different elements of \([q()]\) which correspond to index \(j\). The choice of these \(\) elements for each \(j\) is specified a mapping \(:[q()][n()]\) with \(|^{-1}(j)|=\) for all \(j\), which is chosen randomly in the \(\) function of \(_{}[_{},]\). With this intuition in mind, we proceed to overview the individual \(\), \(\), \(\) functions of \(_{}[_{},]\) in Algorithm 1:

* The \((1^{})\) function generates a secret key \(\) for \(_{}\) using \(_{}\). It also generates a random function \(\) as described above, and returns the tuple \((,)\), which is the secret key for \(_{}[_{},]\).
* The \((1^{},(,),)\) function calls the encoding method \(_{}\) for \(_{}\), which yields a string \(y^{0}\{0,1\}^{n()}\). It then chooses a string \(y[n()]^{m()}\) which has the property that the set of distinct elements of \(y\), which we denote by \((y)\), has small set difference with the set \(^{0}:=\{i[n()]\ :\ y^{0}_{i}=1\}\) of indices at which \(y^{0}\) has a "1". The precise way in which the sets \((y)\) and \(^{0}\) differ is determined by the function \(\) in Algorithm 1, and is needed to ensure that the output of \((1^{},,)\) is indistinguishable from the uniform distribution over \([n()]^{m()}\) (i.e., that the PRC is undetectable). Finally, \(\) returns a string \(z[q()]^{m()}\) where each coordinate \(z_{j}\) is a uniformly random pre-image of \(y_{j}\) under \(\).
* The \((1^{},(,),z)\) function calls the substitution PRC decode function, \(_{}\), on the string \(y^{}\{0,1\}^{n()}\) which has a \(1\) in position \(i[n()]\) if and only if \(i((z))\). For future reference, we denote this string by \(y^{}=D_{}(z)\), i.e., \(D_{}(z)_{i}=\{i((z))\}\).

Theorem 4.1 below shows that \(_{}[_{},]\) has robustness to any channel which makes a large fraction (at most \(1-C_{}p_{0}\)) of substitutions, insertions, and deletions.

**Theorem 4.1**.: _There are constants \(C_{0},C_{} 1\) so that the following holds. For any \(p_{0}<(10C_{})^{-1}\) and PRC \(_{}\) with block length \(n()\) which is robust to all \((1/2-p_{0})\)-substitution-bounded channels, for any \( C_{0}/p_{0}\), \(_{}[_{},]\) (Algorithm 1) is a pseudorandom code over an alphabet of size \( n()\) and block length at most \(n()\), which has robustness to any \((1-C_{}p_{0})\)-edit-bounded channel (per Definition 2.4)._

Proof overview for Theorem 4.1.To prove Theorem 4.1, we need to establish the soundness, undetectability, and robustness of \(_{}[_{},]\). Soundness is an immediate consequence of soundness of \(_{}\) (Lemma D.2). Undectability is likewise straightforward, using undetectability of \(_{}\) together with the fact that the output of \((n,m,y^{0})\), for \(y^{0}(\{0,1\})^{n}\), is uniform on \([n]^{m}\) (Lemma D.2). The bulk of the proof consists in establishing robustness.

A natural attempt to establish robustness would proceed as follows: given a string \(z[n()]^{m()}\) (to be interpreted as an output of \((1^{},(,),)\)), a single insertion or deletion in \(z\) can change at most one symbol of \(D_{}(z)\), and a single substitution in \(z\) can change at most two symbols of \(D_{}(z)\). Thus, it is straightforward to show a statement of the following form: if \(_{}\) is \((1/2-p_{0})\)-substitution bounded and \(2p<1/2-p_{0}\), then \(_{}[_{},]\) is robust to the class of \(p\)-edit-bounded channels. (Technically, some additional work is needed since the \(\) function introduces some additional substitutions in the underlying binary codeword, though we ignore this detail for now since with an appropriate choice of parameters the number of such errors will be of lower order.)Unfortunately, such a result does not have sufficiently good robustness for our application to watermarking. As will be discussed in Section 5, our procedure which watermarks a language model Model by "embedding" a codeword \(z\) of a PRC in a sequence of text output by Model introduces a fraction \(1-\) of errors to \(z\), all of which are substitutions. Here \(\) is some constant which is related to the entropy rate of Model. Using the naive approach above, we are constrained to a rate of substitutions \(p\) bounded as \(p<1/4\), thus forcing \(1-<1/4\) and so disallowing all but high entropy rates.

To compensate, we make use of the fact that the randomly chosen mapping \(:[q()][n()]\) maps multiple (namely, \(\)) symbols in \([q()]\) to each symbol in \([n()]\), when performing decoding. In particular, consider any fixed channel \(\) over the alphabet \([q()]\). For simplicity in our overview here, we assume that \(\) is deterministic (so that it is specified by a mapping \(:[q()]^{m}[q()]^{*}\) for each \(m\)), though essentially the same argument works for randomized \(\). Consider a codeword \(z[q()]^{m()}\) which is output by \((1^{},(,),)\). By undetectability of the code, with high probability over \(\), we will have that \(|((z))| n()/2 O()\), since by our choice of \(m()= n()(2)\), a uniformly random string \(([q()]^{m()})\) satisfies \(|(())| n()/2 O()\) with high probability.6

Supposing that \(\) is promised to make at most a fraction \(1-p\) of edits, then \(z^{}:=(z)\) shares at least \(p m()\) symbols with \(z\). Let us suppose for simplicity here that \(z^{}[q()]^{m()}\), so that the insertions and deletions of \(\) are balanced (a slight modification of the argument handles the general case). Of the remaining \((1-p) m()\) symbols of \(z^{}\), by the random choice of \(\) and since \(|((z))| n()/2\), each one is roughly equally likely to map (under \(\)) to an element in \(((z))\) as to an element in \([n()]((z))\). Thus, in going from the set \(((z))\) to the set \(((z^{}))\), we should expect to change at most roughly \((1-p) n()/2\) elements. This intuition is made precise in the following lemma:

**Lemma 4.2** (Informal version of Lemma D.7).: _Given a channel \(\) as above which makes a \((1-p)\)-fraction of edits (i.e., substitutions, insertions, and deletions), with \(1-()\) probability over the draw of of \((,)(1^{})\) and \(z(1^{},(,),)\) we can bound the set difference between \(((z)),(((z)))\) as follows:_

\[|(((z)),(((z)))| (1-(p)) n().\]

Since the Hamming distance \(D_{}(D_{}(z),D_{}(z^{}))\) is equal to the size of the set difference \(|(((z)),((z^{}))|\), we arrive at the conclusion that \(D_{}(D_{}(z),D_{}(z^{}))(1-(p)) n ()/2\) with high probability over the draw of \(\) in \(\). Since \(p_{0}\) can be chosen arbitrarily small, we have substitution PRCs \(_{}\) which can correct a \((1-(p))/2\) fraction of substitutions. Thus, we obtain as a consequence that \(_{}[_{},]\) can correct a \((1-p)\) fraction of substitutions, insertions, and deletions.

The argument above omits many details, notably involving the high-probability event referenced in Lemma 4.2. To establish that such an event occurs with high probability (over the draw of \(\)), we need to use Dobrushin's concentration inequality (Theorem F.3) for data with limited dependencies. Roughly speaking, this inequality comes into play because the sets \(^{-1}(1),,^{-1}(n())[q()]\) are not fully independent (since, e.g., they must be disjoint). Nevertheless, we may bound their dependencies, assuming that \(n()\) is sufficiently large as compared to \(\).

## 5 From large-alphabet PRCs to watermarking

In this section, we overview our reduction (stated in Theorem 5.1) that converts a PRC with robustness to channels making a bounded number of adversarial edits to a watermarking scheme with robustness to adversarial edits. At a high level, this reduction uses rejection sampling at each step of the language model generation to make the output of the model align with a PRC codeword. To formally state the result, we need the notion of _empirical entropy_: given a token sequence \(^{L}\) and \(i,j[L]\), the empirical entropy of \(\) on the subsequence \([i,j]\) is

\[H^{[i:j]}_{}(,):=-_{^{ }_{i:j}(|_{1:i-1})}(^{} _{i:j}=_{i:j}_{1:i-1}),\]

where the probability is over a sequence that is drawn, token by token, from the per-token distributions induced by \(\) (see Definition B.1 for discussion). The empirical entropy quantifies the degree to which the sequence \(_{i:j}\) is "far from being deterministic" under \(\) given the prefix \(_{1:i-1}\).

The watermarking schemes we derive from PRCs satisfy a stronger property known as _substring robustness_, which means that if any sufficiently high-entropy _substring_ of watermarked text is passed through a channel inducing a bounded number of edits, then the watermark will still be detected. More precisely, for a function \(:\), a watermarking procedure \(=(,,)\) over an alphabet \(\) is _\(\)-substring robust_ to a channel \(\) if

\[_{(1^{}) (1^{},),^{}( )}( i,(1^{},,^{})= H^{[i:i+-1]}_{}()()|| )().\]

We remark that the channel \(\) is required to be _non-adaptive_ in that it cannot depend on the particular draw of the secret key \(\). Some details are omitted; see Definition B.2 for a completely formal definition of substring-robustness. Our main result of this section shows that a PRC with robustness to edit-bounded channels yields a watermarking scheme with substring-robustness to edit-bounded channels:

**Theorem 5.1** (Informal version of Theorem E.1).: _Let \((0,1)\) be a given constant. Suppose \(\), defined over alphabet \(_{}\), has block length \(n\) and is robust to any \((1-)\)-edit-bounded channel. Further suppose \(\) is a language model over alphabet \(\) satisfying \(||(|_{}|)^{2/}\). Then there is a watermarking scheme \([,]\) (Algorithm 3) which is sound, undetectable, and \(()\)-substring robust to any \(}{48}\)-edit-bounded channel, for \(():=8n+6\)._

Theorem 5.1 establishes that, as long as the entropy from a substring of generated text is roughly a \(()\)-fraction of the maximum possible entropy, then a constant (\(O(^{2})\)) fraction of edits to that substring cannot remove the watermark. We remark that this constant fraction of edits cannot be improved to beyond an \(\)-fraction (as is evident from the example in Footnote 4). The proof of Theorem 5.1 builds off of the reduction of , for which \(=_{}=\{0,1\}\). Their reduction, however, breaks down in the setting when \(_{}\) is no longer binary, and we introduce some new ideas (roughly, involving a hashing technique) to deal with the setting of larger alphabets. Details of the proof may be found in Appendix E. Finally, we remark that by combining Theorems 3.2, 4.1 and 5.1, we obtain Theorem 1.1, which establishes the existence of edit-robust watermarking schemes under Assumption 3.1.

On implementation of the watermarking scheme.A natural question is how feasible it is to implement the watermarking scheme \([,]\) of Theorem 5.1. The main limitation of our present theoretical results which may complicate a practical implementation is as follows: The alphabet size \(|()|\) is required to grow exponentially in the inverse of the parameter \(\) (see the statement of Theorem E.2). In turn, the parameter \(\) is proportional to the entropy rate of the text needed to guarantee substring robustness (see Definition B.2 and the setting of \(_{}()\) in Theorem E.2). For typical LLMs, the alphabet size is likely smaller than our required value of \(|()|\) given the entropy rates observed empirically in natural language. On the other hand, we believe that future work aimed at developing modifications of our watermarking scheme with an eye towards practical implementation will be successful. One idea which seems promising is to simulate a larger alphabet by grouping tokens together, and to aim accordingly for a slightly weaker robustness guarantee.