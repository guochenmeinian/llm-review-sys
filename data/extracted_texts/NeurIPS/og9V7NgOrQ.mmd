# Efficient Coding of Natural Images using

Maximum Manifold Capacity Representations

 Thomas Yerxa \({}^{1}\)  Yilun Kuang \({}^{2,3}\)  Eero Simoncelli \({}^{1,2,3}\)  SueYeon Chung\({}^{1,2}\)

\({}^{1}\)Center for Neural Science, New York University

\({}^{2}\)Center for Computational Neuroscience, Flatiron Institute

\({}^{3}\)Courant Inst. of Mathematical Sciences,

tey214@nyu.edu

###### Abstract

The efficient coding hypothesis posits that sensory systems are adapted to the statistics of their inputs, maximizing mutual information between environmental signals and their representations, subject to biological constraints. While elegant, information theoretic quantities are notoriously difficult to measure or optimize, and most research on the hypothesis employs approximations, bounds, or substitutes (e.g., reconstruction error). A recently developed measure of coding efficiency, the "manifold capacity", quantifies the number of object categories that may be represented in a linearly separable fashion, but its calculation relies on a computationally intensive iterative procedure that precludes its use as an objective. Here, we simplify this measure to a form that facilitates direct optimization, use it to learn Maximum Manifold Capacity Representations (MMCRs), and demonstrate that these are competitive with state-of-the-art results on current self-supervised learning (SSL) recognition benchmarks. Empirical analyses reveal important differences between MMCRs and the representations learned by other SSL frameworks, and suggest a mechanism by which manifold compression gives rise to class separability. Finally, we evaluate a set of SSL methods on a suite of neural predictivity benchmarks, and find MMCRs are highly competitive as models of the primate ventral stream.

## 1 Introduction

Biological visual systems learn complex representations of the world that support a wide range of cognitive behaviors, without relying on a large number of labelled examples. The efficient coding hypothesis [(8; 65)] suggests that this is accomplished by adapting the sensory representation to the statistics of the input signal, so as to reduce redundancy or dimensionality. Visual signals have several clear sources of redundancy. They evolve slowly in time, since temporally adjacent inputs typically correspond to different views of the same scene, which in turn are usually more similar than views of distinct scenes. Moreover, the variations within individual scenes often correspond to variations in a small number of parameters, such as those controlling viewing and lighting conditions, and are thus inherently low dimensional. Many previous results have demonstrated how the computations of neural circuits can be seen as matched to such structures in naturalistic environments [(48; 29; 65; 17)] Studies in various modalities have identified geometric structures in neural data that are associated with behavioral tasks [(10; 25; 43; 33; 56)], and explored metrics for quantifying these structures.

The recent development of "manifold capacity theory" provides a more explicit connection between the geometry (size and dimensionality) of neural representations and their coding capacity [(18)]. This theory has been used to evaluate efficiency of biological and artificial neural networks across modalities [(17; 24; 66; 52)]. However, usage as a design principle for building model representations has not been explored.

Motivated by these observations, we seek to learn representations in which manifolds containing different views of the same scene are both compact and low-dimensional, while manifolds corresponding to distinct scene are maximally separated. Specifically:

* We develop a form of Manifold Capacity that can be used as an objective function for learning.
* We demonstrate that a Maximum Manifold Capacity Representation (MMCR) supports high-quality object recognition (matching the SoTA for self-supervised learning), when evaluated using the standard linear evaluation paradigm (i.e., applying an optimized linear classifier to the output of the self-supervised network) (14).
* Through a analysis of internal representations and learning signals, we analyze the underlying mechanism responsible for the emergence of semantically relevant features from unsupervised objective functions.
* We validate the effectiveness of MMCR as a brain model by comparing its learned representations against neural data obtained from Macaque visual cortex.

Our work thus leverages normative goals of representational efficiency to obtain a novel model for visual representation that is both effective for recognition and consistent with neural responses in the primate ventral stream.

### Related Work

**Geometry of Neural Representations.** Previous work has sought to characterize how representational geometry, often measured through spectral quantities like the participation ratio (the squared ratio of the \(l_{1}\) and \(l_{2}\) norms of the eigenvalues of the covariance matrix), shapes different aspects of performance on downstream tasks (32). Elmoznino and Bonner (27) found that high dimensionality in ANN representations was associated with ability to both predict neural data and generalize to unseen categories. Stringer et al. (67) observed that the spectrum of the representation of natural images in mouse cortex follows a power law with a decay coefficient near 1, and Agrawal et al. (2) report that (in artifical representations) the proximity of the spectral decay coefficient to one is an effective predictor of how well a representation will generalize to downstream tasks.

**Self-Supervised Learning.** Our methodology is related to (and inspired by) recent advances in contrastive self-supervised representation learning (SSL), but has a distinctly different motivation and formulation. Many recent frameworks craft objectives that are designed to maximize the mutual information between representations of different views of the same object (58; 14; 58; 68; 4)). However, estimating mutual information in high dimensional feature spaces is difficult (9), and furthermore it is not clear that closer approximation of mutual information in the objective yields improved representations (71) By contrast, capacity measures developed in spin glass theory (35; 1) are derived in the "large N (thermodynamic) limit"and thus are intended to operate in the regime of large ambient dimension (18; 5). We examine whether one such measure, which until now had been used only to evaluate representation quality, is also useful as an objective function for SSL.

Many SSL methods minimize the distance between representations of different augmented views of the same image while employing constraints to prevent collapse to trivial solutions (e.g., repulsion of negative pairs (14), or feature space whitening (77; 7; 28)). The limitations of using a single pairwise distance comparison have been demonstrated, notably in the development of the "multi-crop" strategy implemented in SwAV (13) and in the contrastive multiview coding approach Tian et al. (68). Our approach is based on the assumption that different views of an image form a continuous manifold that we aim to compress. We characterize each set of image views with the spectrum of singular values of their representations, using the nuclear norm as a combined measure of the manifold size and dimensionality.

The nuclear norm has been previously used to infer or induce low rank structure in the representation of data (42; 72; 49). In particular, Wang et al. (72) use it as a regularizer to supplement an InfoNCE loss. Our approach represents a more radical departure from the traditional InfoNCE loss, as we detail below. Rather than pair a low-rank prior with a logistic regression-based likelihood, we make the more symmetric choice of employing a _high rank_ likelihood. This allows the objective to explicitly discourage dimensional collapse, a well known issue in SSL (45).

## 2 Maximum manifold capacity representations

### Manifold Capacity Theory

Consider a set of \(P\) manifolds embedded in a feature space of dimensionality \(D\), each assigned a class label. Manifold capacity theory is concerned with the question: what is the largest value of \(\) such that there exists (with high probability) a hyperplane separating a random dichotomy [(22; 34)]? Recent theoretical work has demonstrated that there exists a critical value, dubbed the manifold capacity \(_{C}\), such that when \(<_{C}\) the probability of finding a separating hyperplane is approximately \(1.0\), and when \(>_{C}\) the probability is approximately \(0.0\)[(18)]. The capacity \(_{C}\) can be accurately predicted from three key quantities: (1) the manifold radius \(R_{M}\), which measures the size of the manifold relative to the distance of its centroid from the origin, (2) the manifold dimensionality \(D_{M}\) which quanitifies the number of dimensions along which a manifold has significant extent, and (3) the correlation of the manifold centroids. When the centroid correlation is low the manifold capacity can be approximated by \((R_{M}})\) where \(()\) is a monotonically decreasing function.

For manifolds of arbitrary geometry, the radius and dimensionality may be computed using an iterative process that alternates between determining the set of "anchor points" on each manifold that are relevant for the classification problem, and computing the statistics of random projections of these anchor points [(20)]. This process is both computationally costly and non-differentiable, and therefore unsuitable for use as an objective function. For more detail on the general theory see Appendix C. However, if the manifolds are assumed to be elliptical in shape, then both radius and dimensionality may be expressed analytically:

\[R_{M}=_{i}^{2}}, D_{M}=_{i})^ {2}}{_{i}_{i}^{2}},\] (1)

where the \(_{i}^{2}\) are the eigenvalues of the covariance matrix of the manifold. For comparison, when computing these values for a set of 100 128-D manifolds with 100 points sampled from each, the use of the analytical expression is approximately 500 times faster (in "wall-clock time") than the general iterative procedure.

Using these definitions for manifold radius and dimensionality we can write the capacity as \(_{C}(_{i}_{i})\) where \(_{i}\) are the singular values of a matrix containing points on the manifold (equivalently, the square roots of the eigenvalues of the covariance matrix). In this form, the sum is the \(L_{1}\) norm of the singular values, known as the _Nuclear Norm_ of the matrix. When used as an objective function, this measure favors sparse solutions (i.e., those with a small number of non-zero singular values) corresponding to low dimensionality. It is worth comparing this objective to another natural candidate for quantifying size: the determinant of the covariance matrix. The determinant is equal to the product

Figure 1: Two dimensional illustrations of high and low capacity representations. Left: the capacity (linear separability) of a random set of spherical regions can be improved, either by reducing their radii (while maintaining their dimensionalities), or by reducing their dimensionalities (while maintaining their average radii). Right: the objective proposed in this paper aims to minimize the nuclear norm (equal to the product of radius and sqrt dimensionality) of normalized data vectors (ie., lying on the unit sphere). Before training, the manifolds have a large extent and thus the matrix of their corresponding centroid vectors has low nuclear norm. After training, the capacity is increased: The manifolds are compressed and repelled from each other, resulting in centroid matrix with larger nuclear norm and lower similarity.

of the eigenvalues (which captures the squared volume of the corresponding ellipsoid), but lacks the preference for lower dimensionality that comes with the Nuclear Norm. Specifically, since the determinant is zero whenever one (or more) eigenvalue is zero, it cannot distinguish zero-volume manifolds of different dimensionality. Lossy coding rate (entropy) has also been used as a measure of compactness (76), which simplifies to the log determinant of the covariance matrix under a Gaussian model (50). In this case, the identity matrix is added to a multiple of the feature covariance matrix before evaluating the determinant, which solves the dimensionality issue described above.

### Optimizing Manifold Capacity

Now we construct an SSL objective function based on manifold capacity. For each input image (notated as a vector \(_{b}^{}\)) we generate \(k\) samples from the corresponding manifold by applying a set of random augmentations (each drawn from the same distribution), yielding manifold sample matrix \(}_{b}^{D k}\). Each augmented image is transformed by a Deep Neural Network, which computes nonlinear function \(f(_{b};)\) parameterized by \(\), and the \(d\)-dimensional responses are projected onto the unit sphere yielding manifold response matrix \(_{b}^{d k}\). The centroid \(_{b}\) is approximated by averaging across the columns (response vectors). For a set of images \(\{_{1},...,_{B}\}\) we compute normalized response matrices \(\{_{1},...,_{B}\}\) and assemble their corresponding centroids into matrix \(^{d B}\).

Given the responses and their centroids, the MMCR loss function can be written simply:

\[=-||||_{*},\] (2)

where \(||||_{*}\) indicates the nuclear norm. The loss explicitly maximize the extent of the "centroid manifold", which interestingly is sufficient to learn a useful representation. Concretely, maximizing \(||||_{*}\) implicitly minimizes the extent of each individual object manifold as measured by \(||_{b}||_{*}\). We build intuition for this effect below.

**Compression by Maximizing Centroid Nuclear Norm Alone.** Each centroid vector is a mean of unit vectors, and thus has a norm that is linearly related to the average cosine similarity of those unit vectors. Specifically,

\[||_{b}||^{2}=+}_{k=1}^{K}_{l=1}^{k-1} _{b,k}^{T}_{b,l}\] (3)

Here \(_{b,i}\) denotes the representation of the \(i^{th}\) augmentation of \(_{b}\). We can gain further insight by considering how the distribution of singular vectors of a matrix depends on the norms and pairwise similarities of the constituent column vectors. While no closed form solution exists for the singular values of an arbitrary matrix, the case where the matrix is composed of two column vectors can provide useful intuition. If \(=[_{1},_{2}]\), \(_{1}=[_{1,1},_{1,2}]\), \(_{2}=[_{2,1},_{2,2}]\), the singular values of \(\) and \(_{i}\) are:

\[()=_{1}||^{2}+||_{2}||^{2}((|| _{1}||^{2}-||_{2}||^{2})^{2}+4(_{1}^{T}_{2})^{2})^{1/2 }}{2}},\] (4)

\[(_{i})=_{i,1}^{T}_{i,2}}.\] (5)

So, \(||()||_{1}=||||_{*}\) is maximized when the centroid vectors have maximal norms (bounded above by 1, since they are the centroids of unit vectors), and are orthogonal to each other. As we saw above the centroid norm is a linear function of within-manifold similarity. Similarly, \(||(_{i})||_{1}=||_{i}||_{*}\) is minimized when the within-manifold similarity is maximal. Thus the single term \(||||_{*}\) encapsulates both of the key ingredients of a contrastive learning framework, and we will demonstrate below that simply maximizing \(||||_{*}\) is sufficient to learn a useful representation. This is because the compressive role of "positives" in contrastive learning is carried out by forming the centroid vectors, so the objective is not positive-free. For example, if only a single view is used the objective lacks a compressive component and fails to produce a useful representation. In Appendix F we demonstrate empirically that this implicit form effectively reduces \(||_{b}||_{*}\) by comparing to the case where \(||_{b}||_{*}\) is minimized explicitly. So, all three factors which determine the manifold capacity (radius,dimensionality, and centroid correlations) can be succinctly expressed in an objective function with a single term, \(-||||_{*}\).

**Computational Complexity.** Evaluating the loss for our method involves computing a singular value decomposition of \(^{d B}\) which has complexity \((Bd(B,d))\), where \(B\) is the batch size and \(d\) is the dimensionality of the output. By comparison, contrastive methods that compute all pairwise distances in a batch have complexity \((B^{2}d)\) and non-contrastive methods that involve regularizing the covariance structure have complexity \((Bd^{2})\). Additionally, the complexity of our method is constant with respect to the number of views used (though the feature extraction phase is linear in the number of views), while pairwise similarity metrics have complexity that is quadratic in the number of views. It is also worth noting that doing implicit compression by maximizing \(||||_{*}\) offers an advantage in computational complexity relative to explicit compression. This is because evaluating a term such as \(_{b=1}^{B}||_{b}||_{*}\) has computational complexity of \((B^{2}d(B,d))\).

### Conditions for Optimal Embeddings

Recently HaoChen et al. (2019) developed a framework based on spectral decomposition of the "population augmentation graph", which provides theoretical guarantees for the performance of self-supervised learning on downstream tasks under linear probing. This work was extended to provide insights into various other SSL objectives by Balestriero and LeCun (2016), and we show below that leveraging this approach can lead to explicit conditions for the optimality of representation under our proposed objective as well.

Given a dataset \(^{}=[_{1},...,_{N}]^{T}^{N D^{ }}\) we construct a new dataset by creating \(k\) randomly augmented views of the original data, \(=[_{1}(^{}),...,_{k}(^{} )]^{Nk D}\). The advantage of doing so is that we can now leverage the knowledge that different views of the same underlying datapoint are _semantically related_. We can express this notion of similarity in the symmetric matrix \(\{0,1\}^{Nk Nk}\) with \(_{ij}=1\) if augmented datapoints \(i\) and \(j\) are semantically related (and \(_{ii}=1\) as any datapoint is related to itself). We can normalize \(\) such that its rows and columns sum to 1 (so rows of \(\) are \(k\)-sparse with nonzero entries equal to \(1/k\)).

Now let \(^{Nk d}\) be an embedding of the augmented dataset. Then we have \(=[,...,]^{T}\) where \(\) is the matrix of centroid vectors introduced above, and the number of repetitions of \(\) is \(k\). Then because \(([,...,])=()\) we can write MMCR loss function as,

\[=-||||_{*}\] (6)

This connection allows us to make the following statements about the optimal embeddings \(\) under our loss, which we prove in Appendix A:

**Theorem:** Under the proposed loss, the left singular vectors of an optimal embedding, \(^{*}\), are the eigenvectors of \(\), and the singular values of \(^{}\) are proportional to the top \(d\) eigenvalues of \(\).

## 3 Results

**Architecture.** For all experiments we use ResNet-50 (40) as a backbone architecture (for variants trained on CIFAR we removed max pooling layers). Following Chen et al. (2014), we append a small perceptron to the output of the average pooling layer of the ResNet so that \(z_{i}=g(h(x_{i}))\), where \(h\) is the ResNet and \(g\) is the MLP. For ImageNet-1k/100 we used an MLP with dimensions \(\) and for smaller datasets we used \(\).

**Optimization.** We employ the set of augmentations proposed in (38). For ImageNet we used the LARS optimizer with a learning rate of 4.8, linear warmup during the first 10 epochs and cosine decay thereafter with a batchsize of 2048, and pre-train for 100 epochs. Note that though we report results using a default batch size of 2048, a batch size as small as 256 can be used to obtain reasonable results (1.2% reduction compared to batch size 2048 - see Appendix J for a sweep over batch size). We additionally employ a momentum encoder for ImageNet pre-training (all views are fed through an online network and an additional network whose parameters are a slowly moving average of the online network and all embeddings of each image are averaged to form centroids). We found the use of a momentum encoder provided a small advantage in terms of downstream classification performance, see Appendix O for this ablation. For smaller CIFAR-10 we used a smaller batch size, many more views (40), and the Adam optimizer with fixed learning rate. See Appendix D for exact details.

### Transfer to Downstream Tasks

We used a standard linear evaluation technique, freezing the parameters of the encoding network and training a linear classifier with supervision (14), to verify that our method extracts semantically relevant features from the data. We also perform semi-supervised evaluation, where all model parameters are fine tuned using a small number of labelled examples, and also check whether the learned features generalize to three out-of-distribution datasets: Flowers-102, Food-101, and the Describable Textures Dataset [(57; 11; 19)]. The results are summarized in Table 1 and details of the training of the linear classifier are provided in Appendix G. Finally we also evaluate our best model in and each of the baselines on object detection on the VOC07 dataset. We followed [(41; 77)], fine tuning the representation networks for detection with a Faster R-CNN head and C-4 backbone using the 1x schedule. MMCR achieved a mean average precision (mAP) of 54.6 and the baseline method performance ranged from 53.1 to 56.0, demonstrating that though our framework was inspired by a theory of classification the learned features do generalize to other vision tasks. See Appendix M for detailed results.

Note that all included models were trained using the same backbone architecture (ResNet-50), dataset (ImageNet-1k), and number of pretraining epochs (100; we briefly explore the impact of longer pretraining in Appendix P ). The results for networks pretrained on smaller datasets can be found in the Appendix I.

### Analyses of Learned Representations

We next conduct a series of experiment to elucidate the differences between representations learned with different SSL procedures, and suggest a mechanism by which augmentation manifold compres

 Method & IN & 1\% & 10\% & Food-101 & Flowers-102 & DTD \\  W-MSE (28) & 69.4 & - & - & - & - & - \\ NNCLR (26) & 69.4 & - & - & - & - & - \\ SwAV (13) & 64.6 & - & - & - & - & - \\ SimSiam (15) & 68.1 & - & - & - & - & - \\ CorInfoMax (59) & 69.1 & 44.9 & 64.3 & - & - & - \\ VICReg (7) & 68.7 & 44.8 & 62.2 & - & - & - \\ BarlowTwins (77) & 68.7 & 45.1 \(\).12 & 61.7 \(\).03 & 69.8 \(\).03 & 86.2 \(\).22 & 67.7 \(\).20 \\ SimCLR (14) & 66.5 & 42.6 \(\).02 & 61.6 \(\).09 & 67.2 \(\).24 & 84.0 \(\).19 & 64.8 \(\).07 \\ BYOL (38) & 69.3 & 49.8 \(\).05 & 65.0 \(\).05 & 70.6 \(\).1 & 84.8 \(\).43 & 67.6 \(\).14 \\ MoCo-V2 (16) & 67.4 & 43.4 \(\).07 & 63.2 \(\).07 & 68.6 \(\).03 & 82.4 \(\).27 & 66.6 \(\).16 \\ SwAV (13) & **72.1** & 49.8 \(\).09 & 66.9 \(\).05 & 72.1 \(\).08 & 89.3 \(\).1 & 68.2 \(\).18 \\  MMCR (2 views) & 69.5 \(\).02 & 46.6 \(\).02 & 63.9 \(\).02 & 72.0 \(\).02 & 90.0 \(\).24 & 68.5 \(\).07 \\ MMCR (4 views) & 71.5 \(\).04 & 49.4 \(\).05 & 66.0 \(\).05 & 73.2 \(\).07 & 91.0 \(\).04 & **70.4** \(\).46 \\ MMCR (8 views) & **72.1**\(\).04 & **51.0**\(\).02 & **67.7**\(\).11 & **73.6**\(\).04 & **91.4**\(\).07 & **70.0**\(\).24 \\  

Table 1: Evaluation of learned features on downstream classification tasks. The leftmost columns show results for the standard frozen-linear evaluation procedure on ImageNet (IN). Results for most methods in this setting are taken from Ozsoy et al. (59) except for SwAV which is taken from the original paper (13). Columns 2 and 3 show semi-supervised evaluation on ImageNet (fine-tuning on 1% and 10% of labels). The results for this setting for VICReg and CorInfo Max are copied from Ozsoy et al. (59). The final three columns show frozen-linear evaluation on other datasets. We evaluated models for which pretrained weights in the 100 epoch setting were available online; MoCo, Barlow Twins and BYOL were taken from solo-learn da Costa et al. (23) (https://github.com/vturrisi/solo-learn), while SwAV and SimCLR were taken from VISSL Goyal et al. (37) (https://github.com/facebookresearch/vissl/blob/main/MODEL_200.md). For all evaluations we performed we report the mean and standard deviation over 3 evaluation runs).

sion gives rise to class separability. To reduce the computational requirements, these analyses are carried out on models trained on the CIFAR-10 dataset.

**Mean Field Theory Manifold Capacity.** In Fig. 2 we show that our representation, which is optimized using an objective that assumes elliptical manifold geometry, nevertheless yields representations with high values of the more general mean field manifold capacity (relative to baseline methods). For completeness we also analyzed the geometries of class manifolds, whose points are the representations of different examples from the same class. This analysis provided further evidence that learning to maximize augmentation manifold capacity compresses and separates class manifolds, leading to a useful representation. Interestingly MMCRs seem to use a different strategy than the baseline methods to increase the capacity, yieldingclass/augmentation manifolds with larger radii, but lower dimensionality (Fig. 2) These geometrical differences do not emerge until the tail end of the hierarchy, suggesting early layers of each network are carrying out stereotyped transformations and loss function induced specialization does not emerge until later layers.

**Emergence of neural manifolds via gradient coherence.** We hypothesize that class separability in MMCRs arises because augmentation manifolds corresponding to examples from the same class are optimally compressed by more similar transformations than those stemming from distinct classes. To investigate this empirically, we evaluate the gradient of the objective function for inputs belonging to the same class. We can then check whether gradients obtained from (distinct) batches of the same class are more similar to each other than those obtained from different classes, which would suggest that the strategy for compressing augmentation manifolds from the same class are relatively similar to each other. Fig. 3 demonstrates that this is the case: within class gradient coherence, as measured by cosine similarity, is consistently higher than across class coherence across both training epochs and model hierarchy.

**Manifold subspace alignment.** Within-class gradient coherence constitutes a plausible mechanistic explanation for the emergence of class separability, but it does not explain why members of the same class share similar compression strategies. To explore this question we examine the geometric properties of augmentation manifolds in the pixel domain. Here we observe small but measurable differences between the distributions of within-class similarity and across-class similarity, as demonstrated in the top row of Fig. 4. The subtle difference in the geometric properties of augmentation manifolds in the pixel domain in turn leads to the increased gradient coherence observed above, which leads to a representation that rearranges and reshapes augmentation manifolds from the same class in a similar fashion (bottom row of Fig. 4), thus allowing better linear separation of classes. Not only are centroids

Figure 2: Mean field Manifold Capacity analysis. Manifold dimensionality (top row), radius (middle row), and capacity (bottom row), as a function of stage in the representational hierarchy (from pixel inputs to output of the encoder/learned representation). Shaded regions indicate 95% confidence intervals around the mean (analysis was conducted with 5 different random samples from the dataset, see Appendix E).

of same-class-manifolds in more similar regions of the representation space than those coming from distinct classes (Fig. 4 third column bottom row) but additionally same-class-manifolds have more similar shapes to each other (Fig. 4 bottom row columns 1 and 2 show same-class-manifolds occupy subspaces with lower relative angles and share more variance).

We next ask how the representation learned with the MMCR objective differs from those optimized for other self-supervised loss functions. While MMCR encourages centroids to be orthogonal, the InfoNCE loss (14) encourages negative pairs to be as dissimilar as possible, which is achieved when they lie in opposite regions of the _same_ subspace. The Barlow Twins (77) loss is not an explicit function of feature vector similarities, but instead encourages individual features to be correlated and distinct features to be uncorrelated, across the batch dimension. Fig. 5 shows that these intuitions are borne out empirically: the MMCR representation produces augmentation manifold centroids that are significantly more orthogonal to each other than the two baseline methods.

### Biological relevance

Neuroscience has provided motivation for many of the developments in artifical neural networks, and it is of interest to ask whether SSL networks can characterize the measured behaviors of neurons in biological visual systems. As a simple test, Table 2 shows performance of our model compared with five other SSL models on the _BrainScore_ repository (64; 30; 53; 51). We find that MMCR achieves the highest performance in explaining neural data from primate visual areas V2 and V4, second-highest for V1 and IT, and is the most or second most predictive for 8 out of the 11 individual datasets (see Appendix K).

In addition, we examine general response spectral characteristics that have been recently described for neural populations. In particular, Stringer et al. (67) reported that the eigenspectrum of the

Figure 4: The distributions of various similarity metrics for augmentation manifolds from the same (orange) and distinct (blue) classes. These are shown for both the input images (top row), and the learned representation (bottom row). Left: schematic illustration of the exemplar-augmentation manifold-class manifold structure of the learned representation.

Figure 3: Cosine similarity of gradients for pairs of single-class batches. We plot the mean pairwise similarity for pairs of gradients for different subsets of the model parameters (all parameters, and the first and last linear operators) obtained from single-class-batches coming from the same or distinct classes over the course of training. Because a large number of bathces were used 95% confidence intervals about these means are too small to be visible. To the left is a visualization of the fact that single-class gradients flow backward through the model in more similar directions.

covariance matrix of population activity in visual area V1 follows a power law decay with a decay coefficient of approximately \(1\) (\(_{n} n^{-}\) with \( 1\) where \(_{n}\) is the \(n^{th}\) eigenvalues). Subsequent studies in artifical networks have found that such a decay spectrum is associated with increased robustness to adversarial perturbations and favorable generalization properties (55; 2). Additionally, several recent works have investigated the connection between representational dimensionality and neural predictivity (70; 63). In particular, Elmoznino and Bonner (27) report that high intrinsic dimensionality (as measured with the participation ratio of the representation covariance) is correlated with stronger ability to predict neural activity. Table 2 provides values for the participation ratio of each representation over the ImageNet validation set, as well as the decay coefficient of the covariance spectrum (see Appendix L for more details on each experiment). We see that the MMCR has the highest participation ratio (dimensionality) - note that this differs from the quantity optimized in the objective function, which lies in the embedding space. In addition, MMCR also also yields features with a decay coefficient that is nearest to one.

We find that in this controlled setting each model explains a very similar fraction of neural variance through linear regression. This is consistent with recent and concurrent works (21; 62) which have identified this lack of ability to discriminate between alternative models as a weakness of the dominant paradigm used for model-to-brain comparisons. However, our results demonstrate that different SSL algorithms produce representations with meaningfully different geometries (as evidenced by the large spread in the spectral properties such as the participation ratio and decay coefficient). This suggests the need for the development of new metrics for comparing models to data, such as geometrical measures, that capture these important differences between candidate models.

 
**Model** & **V1** & **V2** & **V4** & **IT** & **PR** & \(\) \\  MMCR & **.494**\(\).006 & **.311**\(\).005 & **.481**\(\).005 &.416 \(\).003 & 279.2 \(\).3 & 1.04 \(\) 1e-4 \\ SimCLR & **.500**\(\).007 &.288 \(\).007 &.475 \(\).004 & **.420**\(\).003 & 124.6 \(\).1 & 1.34 \(\) 3e-4 \\ BYOL & **.500**\(\).006 &.291 \(\).007 & **.477**\(\).005 &.404 \(\).003 & 248.0 \(\).4 & 1.35 \(\) 1e-4 \\ MoCo & **.499**\(\).007 &.293 \(\).006 & **.477**\(\).005 & **.417**\(\).003 & 147.7 \(\).2 & 1.44 \(\) 3e-4 \\ Barlow & **.498**\(\).007 &.293 \(\).008 & **.477**\(\).005 &.404 \(\).003 & 252.2 \(\).2 & 1.21 \(\) 3e-4 \\ SwAV &.488 \(\).007 &.296 \(\).009 &.463 \(\).004 &.398 \(\).003 & 163.9 \(\).2 & 1.15 \(\) 3e-4 \\ 

Table 2: Neural prediction and spectral properties of self-supervised models. First four columns provide average BrainScore correlation values (64) for neurons recorded in four areas of primate ventral stream. Last two provide participation ratio (PR) and spectral decay coefficients (\(\)), estimated for 10 bootstrapped samples of the features for the ImageNet validation set. When estimating the spectral decay coefficient we omitted the tails (where the power decays more rapidly), see L for exact details. Entries indicate mean and standard error of the mean, and boldface indicates best performance within standard error. MMCR results are for a model trained with 8 views.

Figure 5: Cosine similarities of centroids for models trained according to different SSL objectives. The left panel shows the distribution of cosine similarities for centroids of augmentation manifolds for examples of the same class, while the right shows the same distribution for examples from distinct classes. Note that because we are analyzing the outputs of the ResNet backbone (which are rectified), the minimum possible cosine similarity is \(0\).

Discussion

We have presented a novel self-supervised learning algorithm inspired by manifold capacity theory. Many existing SSL methods can be categorized as either "contrastive" or "non-contrastive" depending on whether they avoid collapse by imposing constraints on the embedding gram or covariance matrix, respectively. Our framework strikes a compromise, optimizing the singular values of the embedding matrix itself. By directly optimizing this population level feature (the spectrum), we are able to encourage alignment and uniformity (71) simultaneously with a single-term objective. Additionally, this formulation circumvents the need for making a large number of pairwise comparisons, either between instances or dimensions. As a result learning MMCRs is efficient, requiring neither large batch size nor large embedding dimension. Finally, our method extends naturally to the multi-view case, offering improved performance with minimal increases in computational cost.

Our formulation approximates manifold geometries as elliptical, reducing computational requirements while still yielding a useful learning signal and networks with high manifold capacity. Specifically, we were able to leverage manifold capacity analysis in its full generality to gain insight into the geometry of MMCR networks after training. Further research could explore objectives based on more modest reductions of mean field manifold capacity that capture non-elliptical structure. Intriguingly, our method produces augmentation and class manifolds with lower dimensionality but larger radius than either Barlow Twins or SimCLR (Fig. 2). We do not understand why this is the case, but the differences indicate that capacity analysis can provide a useful tool for elucidating the different encoding strategies encouraged by various SSL paradigms.

Finally we investigated two recently proposed theories on ideal spectral properties for neural representations. For our considered set of models a spectral decay coefficient near 1 was associated with better performance on the within-distribution task and generalization to unseen datasets (MMCR, SwAV, and Barlow were the top three models for each of the out-of-distribution classification tasks), a finding which is broadly aligned with both the empirical and theoretical findings of Agrawal et al. (2). However, we also found that high dimensionality did not always correspond to strong neural predictivity: Despite having the lowest dimensionality, SimCLR performed strongly in terms of neural predictivity. This implies, perhaps unsurprisingly, that global dimensionality alone is not sufficient to explain the response properties of neurons in the primate ventral stream. In fact our experiments add to a growing body of work on the need for complementary approaches to linear predictivity for discriminating between candidate models of the visual system (21). Happily the field is already moving to address this limitation, for instance concurrent work (12) finds that decomposing neural predictivity error into distinct modes can yield insights into how different models fit different aspects of the neural data (even if they yield similar overall predictivity).

One promising direction for improving the quality of artificial networks as models of neural computations is to incorporate the constraints associated with biologically plausible learning (i.e. the need for local learning rules, Illing et al. (44)). A complementary direction is to better align training data diets with ecological inputs. For example, temporal invariance rather than augmentation invariance seems a more plausible objective for a neural system to optimize (73; 3; 61). We speculate that a variant of the MMCR objective that operates over time may be well-suited to a neural circuit implementation as its computation would only require mechanisms for tracking (1) short timescale temporal means (to form centroids) and (2) the singular values of the population activity over longer timescales.

Neuroscience is brimming with newly collected datasets recorded from ever larger populations of neurons, and there is a long history of methods that aim to make sense of these measurements through a normative lens. Here we have demonstrated that one such technique that has proven useful for gleaning insights from neural data (60; 75; 31) can be reformulated for use as an objective function to learn a useful abstract representation of images. Future work should aim to close the loop between modelling and analysis by using these learned models to generate experimentally testable predictions and constrain new experimental designs.