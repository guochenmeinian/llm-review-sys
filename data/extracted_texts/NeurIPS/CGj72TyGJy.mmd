# Automatic Grouping for Efficient Cooperative Multi-Agent Reinforcement Learning

Yifan Zang\({}^{1,2}\), Jinmin He\({}^{1,2}\), Kai Li\({}^{1,2}\), Haobo Fu\({}^{3}\), Qiang Fu\({}^{3}\), Junliang Xing\({}^{4}\), Jian Cheng\({}^{1,2}\)

\({}^{1}\)Institute of Automation, Chinese Academy of Sciences

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)Tencent AI Lab, \({}^{4}\)Tsinghua University

{zangyifan2019,hejinmin2021,kai.li}@ia.ac.cn,

{haobofu,leonfu}@tencent.com,jlxing@tsinghua.edu.cn,jcheng@nlpr.ia.ac.cn

Corresponding authors.

###### Abstract

Grouping is ubiquitous in natural systems and is essential for promoting efficiency in team coordination. This paper proposes a novel formulation of Group-oriented Multi-Agent Reinforcement Learning (GoMARL), which learns automatic grouping without domain knowledge for efficient cooperation. In contrast to existing approaches that attempt to directly learn the complex relationship between the joint action-values and individual utilities, we empower subgroups as a bridge to model the connection between small sets of agents and encourage cooperation among them, thereby improving the learning efficiency of the whole team. In particular, we factorize the joint action-values as a combination of group-wise values, which guide agents to improve their policies in a fine-grained fashion. We present an automatic grouping mechanism to generate dynamic groups and group action-values. We further introduce a hierarchical control for policy learning that drives the agents in the same group to specialize in similar policies and possess diverse strategies for various groups. Experiments on the StarCraft II micromanagement tasks and Google Research Football scenarios verify our method's effectiveness. Extensive component studies show how grouping works and enhances performance.

## 1 Introduction

Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple agents' actions through shared team rewards and has become a helpful tool for solving multi-agent decision-making problems [47; 9; 44]. Learning centralized policies to address this problem conditions on the full state, which is usually unavailable during execution due to partial observability or communication constraints [4; 3]. An alternative paradigm is to learn decentralized policies based on local observations [36; 31]. However, simultaneous exploration suffers from non-stationarity that may cause unstable learning [7; 50]. The centralized training with decentralized execution (CTDE) inherits the advantages of these two paradigms and learns decentralized policies in a centralized fashion [23; 15].

Value Function Factorization (VFF) under the CTDE paradigm is a popular approach to MARL, where a centralized value function is learned from global rewards and factorized into local values to train decentralized policies. In recent years, value factorization methods have been widely proposed. Existing methods usually adopt the flat VFF scheme , which directly estimates the joint action-value from local utilities. These methods achieve remarkable performances from various perspectives, such as enhancing the expressiveness of the mixing network [27; 33; 38; 37] and encouraging exploration [21; 19]. However, learning efficient cooperation by directly estimating the joint action-values from individual utilities is exceptionally difficult. Phan et al. illustrate that the flat VFFscheme leads to a performance bottleneck, where it gets challenging to provide sufficiently informative training signals for each agent. Although the state information provides complete knowledge, it is burdensome for agents to extract effective guidance that facilitates cooperative policy learning.

Research in natural systems [13; 43] and multi-agent systems  has validated grouping as a means to promote efficient collaboration. Dividing the team into smaller sets allows for fine-grained learning and opens up further opportunities to integrate informative group-wise learning signals. However, formulating a general division criterion without domain knowledge is still a matter of interest to the community. Most previous works are proposed for well-structured tasks and typically predefine specific responsibilities or forms of task decomposition [1; 18; 24]. They require apriori settings that are potentially unavailable in practice and discourage methods' transferring to diverse environments.

This paper proposes a novel formulation of Group-oriented MARL (GoMARL), which learns automatic grouping without domain knowledge for efficient cooperation. GoMARL holds a dual-hierarchical value factorization and learns dynamic groups with a "select-and-kick-out" scheme. Concretely, GoMARL continuously selects agents unsuitable for their current groups based on the learning weights of the decomposition from the group-wise value to local utilities and kicks them out to reorganize the group division. Furthermore, GoMARL transforms various informative training signals, including individual group-related information, group state, and global state, into network weights, which extracts effective guidance for policy improvement and enables flexible adaptation to the dynamic changes in the number of subgroups and the number of agents per group.

The advantages of GoMARL lie in two aspects. Firstly, group information offers richer knowledge for group-wise value factorization and provides more direct guidance for efficient learning. Our value factorization with a dual hierarchy is based on a more focused and compact input representation. Unlike existing methods that learn value factorization with no information or only with the global state which is complete but hard to extract efficient guidance, GoMARL proposes a fine-grained learning scheme to integrate information from the group perspective into the policy gradient, promoting intra-group coordination with the group state and facilitating inter-group cooperation with the global state. Secondly, GoMARL extracts individual group-related information to provide informative signals for policy diversity. The agents condition their behaviors on their individual group-related information embedded by a shared encoder, which is trained following specialization guidance, _i.e._, imposing similarity within a group and diversity among groups. In this way, GoMARL synergizes subgroups with policy styles, proposing a parameter-sharing mechanism for specialized policies.

We test our method on a challenging set of StarCraft II micromanagement tasks  and Google Research Football scenarios . GoMARL achieves superior performance with greater efficiency compared with notable baseline methods. We also conduct detailed component analyses and ablation studies to give insights into how each module works and enhances learning efficiency.

## 2 Related Work

Value function factorization under the CTDE  paradigm is a popular approach to MARL. Most existing methods learn flat value factorization by treating agents as independent factors and directly estimating the joint action-value from local utilities. The earlier work, VDN , learns a linear decomposition into a sum of local utilities used for greedy action selection. QMIX  learns a non-linear mixing network with the global state and enlarges the functions the mixing network can represent, but it still faces the monotonicity constraint. QTRAN  further improves the expressivity by proposing the Individual-Global-Max (IGM) principle between individual utilities and the global action-value. Subsequent works follow the IGM principle and further boost performance by encouraging exploration [21; 5], enhancing the expressiveness of the mixing network , preventing sub-optimal convergences [26; 37], and integrating functional modules [42; 46; 45; 51; 34; 49].

This paper explores automatic group division for multi-agent teams to realize group-wise learning. Early related works [30; 28; 20; 18] predefine specific responsibilities to each agent based on goal, visibility, capability, or by search. Some other efforts achieve implicit grouping by task allocation [32; 17; 10]. These methods only address tasks with a clear structure and require domain knowledge or apriori settings. Another class of approaches focuses on individuality [41; 19; 14] or role learning [39; 40]. Among them, ROMA  learns dynamic roles that depend on the context agents observe. RODE  decomposes the joint action spaces and integrates the action effects into the role policies to boost learning. A similar work, VAST , also studies the impact of subgroupson value factorization but still requires a priori of group number. Our method, GoMARL, does not rely on domain knowledge and gradually adjusts the group division according to the learned factorization weights. Based on the dynamic group division, GoMARL models several group-related signals to learn specialized policies and promote efficient team cooperation.

## 3 Preliminaries

**Dec-POMDP.** This paper focuses on cooperative tasks with \(n\) agents \(=\{a_{1},...,a_{n}\}\) as a Dec-POMDP  defined by a tuple \(G= S,U,P,r,Z,O,n,\). The environment has a global _state_\(s S\). Each agent \(a\) chooses an _action_\(u_{t}^{a}\) from its action space \(U_{a}\) at timestep \(t\) and forms a joint action \(_{t}(U_{1}... U_{n}) U^{n}\) that induces a transition according to the _state transition distribution_\(P(s_{t+1}|s_{t},_{t}):S U^{n} S\). \(r(s,):S U^{n}\) is the _reward_ function yielding a shared reward, and \([0,1)\) is the discount factor. We consider partially observable scenarios in which agent \(a\) acquires its local _observation_\(z^{a} Z\) drawn from \(O(s_{t},a):S Z\) and has an _action-observation history_\(^{a} T(U Z)^{*}\) on which it conditions a _policy_\(^{a}(u^{a}|^{a}) T U\).

**Value Function Factorization and the IGM Principle.** We consider cooperative MARL with the CTDE paradigm, which has been a significant focus in recent efforts [2; 35; 27; 12; 21]. A majority of methods achieve CTDE through flat value function factorization , _i.e._, factoring action-value functions into combinations of per-agent utilities. The individual utility only depends on the local history of actions and observations, allowing agents to maximize their local utility functions independently under the Individual-Global-Max (IGM) Principle : \(_{}Q^{tot}(,)=\{_{u^{i}}Q ^{1}(^{1},u^{1}),,_{u^{n}}Q^{n}(^{n},u^{n})\}\). Among these attempts, the representative deep MARL approach QMIX  improves the simple summation of individual utilities  by introducing a more expressive factorization: \(Q^{tot}=f(Q^{1}(^{1},u^{1};_{Q}),,Q^{n}(^{n},u^{n};_ {Q});_{f})\), where \(_{f}\) denotes the parameters of the monotonic mixing function generated by a hypernetwork .

## 4 Group-oriented Multi-Agent Reinforcement Learning

**Definition 1**.: **(Individual and Group)** Given a cooperative task with \(n\) agents \(=\{a_{1},...,a_{n}\}\), we have a set of groups \(=\{g_{1},...,g_{m}\}\), \(1 m n\). Each group \(g_{j}\) contains \(n_{j}\)\((1 n_{j} n)\) different agents, \(g_{j}=\{a_{j_{1}},...,a_{j_{n_{j}}}\}\), where \(_{j}g_{j}=\), \(g_{j} g_{k}=\) for \(j,k\{1,2,,m\}\) and \(j k\). The _superscript_ describes the variable owner, _e.g._, \(u^{j_{i}}\) is the action of the _i-th_ agent \(a_{j_{i}}\) in group \(g_{j}\). We denote joint quantities in bold and joint quantities over agents other than a given agent \(a\) with the superscript \(-a\), _e.g._, \(^{-j_{i}}\) is the joint action of agents in group \(g_{j}\) other than agent \(a_{j_{i}}\).

GoMARL decomposes the global action-value \(Q^{tot}\) into group-wise values \(Q^{g}\) and trains agents by groups in a fine-grained manner. Figure 1 illustrates the overview of the learning framework. It consists of an automatic grouping module, specialized agent networks generating local utilities \(Q^{i}\), and a mixing network among groups. In Section 4.1, we introduce the automatic grouping module. It progressively divides the team into dynamic groups as training proceeds. Based on the dynamic group division, we propose specialized agent networks that achieve similarity within each group and diversity among groups to generate local utilities \(Q^{i}\) in Section 4.2. Section 4.3 presents the overall training framework to detail the estimation of the group-wise action-values and the global action-values.

### Automatic Grouping Mechanism

The automatic grouping mechanism aims to learn a mapping relationship \(f_{g}:\). The key idea is to divide the team into dynamic groups in an end-to-end fashion by maximizing the expected global return \(Q^{tot}_{}(s_{t},_{t})=_{s_{t+1:}, _{t+1:}}[_{k=0}^{}^{k}r_{t+k}|s_{t}, _{t};]\). Value function factorization approaches represent the joint action-value as an aggregation of individual utilities, _i.e._, a weighted sum of \(Q^{i}\) and biases. We follow this setting and represent the group-wise value \(Q^{g}\) as an aggregation of the individual value \(Q^{i}\). Intuitively, as illustrated in the right side of Figure 2, if the learned mixing

Figure 1: Overview of GoMARL.

weight of \(Q^{i}\) is small enough, then \(Q^{i}\) contributes a little to its group-wise value \(Q^{g}\). In other words, when an agent \(a_{j_{i}}\) takes action \(u^{j_{i}}\) but does not affect its group-wise value \(Q^{g_{j}}\), it indicates that agent \(a_{j_{i}}\) does not belong to group \(g_{j}\) anymore and needs group adjustment.

We factorize the group value \(Q^{g}\) into individual utilities \(Q^{i}\) with group-wise mixing weights \(_{1}\), _i.e._, \(Q^{g_{j}}=f(Q^{j_{1}}(^{j_{1}},u^{j_{1}}),,Q^{j_{n_{j}}}(^{j_{n_ {j}}},u^{j_{n_{j}}});_{j}^{g_{j}})\), where \(_{1}^{g_{j}}\) denotes group \(g_{j}\)'s mixing parameters generated by individual \(_{1}\) generators \(f_{_{1}}^{i}(\,;_{_{1}}^{i}):^{i} _{1}^{i}\) that map each agent \(a_{i}\)'s history information \(^{i}\) to a \(k\)-dimensional weight vector. A regularization on this \(_{1}\) generator \(f_{_{1}}^{i}\) drives the automatic grouping mechanism. Specifically, GoMARL "selects and kicks out" agents whose individual utilities hold small mixing weights and contribute a little to their group-wise \(Q\) values; thus, a sparsity regularization on \(_{1}\) is implemented to select those agents with small group-value mixing weights. The \(_{1}\) generators \(f_{_{1}}^{i}\) are trained by minimizing the following loss function:

\[_{g}(_{_{1}})=_{(, ,,^{})}_{i}\|f_ {_{1}}^{i}^{i}(z^{i},u^{i});_{_{1}}^{i} \,\|_{l_{1}},\] (1)

where \(\) is the replay buffer, and \(\|\|_{l_{1}}\) stands for the \(l_{1}\)-norm penalty.

Figure 2 illustrates the schematic diagram of the grouping mechanism. Initially, all the agents belong to the same group, and the grouping \(\) is adjusted as training proceeds. The grouping shifts every \(c\) timesteps, and each selected agent is assigned to the following group (a new one for agents in the last group) until it properly contributes to where it belongs. When adjusting groups, the \(_{1}\) of all agents are sent to a group adjustment operator \(_{g}\). According to the adjusted grouping \(\), \(_{g}:\{_{1}^{1},,_{1}^{n}\}}\{_{1}^{g_{1}},,_{1}^{g_{m}}\}\) concatenates the \(_{1}^{i}\) of agents in the same group to form a set of group-wise \(_{1}^{g}\) to generate group action-value. This implementation flexibly adapts to the grouping dynamics since each \(_{1}^{i}\) is tied to agent \(a_{i}\) by the individual \(_{1}\) generator \(f_{_{1}}^{i}\). No matter which group \(g_{j}\) agent \(a_{i}\) belongs to, \(Q^{i}\) can engage in the estimation of \(Q^{g_{j}}\) through \(_{1}^{i}_{1}^{g_{j}}\).

In practice, the weights' shrinkage to zero is infeasible. It is also challenging to determine a fixed threshold for groups of various sizes in diverse environments. We empirically utilize seventy percent of each group's average weight to assess whether an agent fits its current group. Extensive experiments and component studies in Section 5 confirm the universality of this configuration.

### Specialized Agent Network for Decentralized Execution

Group status describes the cooperation among agents within each group and is essential to local utility generation. Integrating group-wise information \(e\) into the decision-making process enables consideration of cooperative behaviors. In this section, we introduce GoMARL's agent network to generate local action-value functions with individual group-related information \(e\) embedded.

As shown in Figure 3, we construct a group-related info encoder \(f_{e}(;_{e})\) to embed agents' hidden states. To achieve a group-related view, we train the encoder network as an extractor, where the extracted agent info \(e\) of agents from the same group should be similar. To avoid all agents'

Figure 2: The left side illustrates the schematic diagram of the grouping mechanism. The group adjustment operator concatenates the weights \(_{1}^{i}\) of agents in the same group to form the group-wise weights \(_{1}^{g}\). The right side gives an example to show how grouping \(\) changes during training.

collapsing to be alike, the regularizer also encourages diversity between agents from different groups. Formally, we minimize the following similarity-diversity objective to train the encoder \(f_{e}\):

\[&_{SD}(_{e})=_{ }_{i j}I(i,j)f_{e}(h^{i} _{i};_{e}),f_{e}(h^{j};_{e})\,,\\ &\;\;I(i,j)=-1,&a_{i},a_{j} g_{k}.\\ 1,&a_{i} g_{k},a_{j} g_{l},k l.\] (2)

The encoder trained by the SD-loss extracts agent info \(e^{i}\) that is recognizable to agents' groups. This group-related information is then fed into a decoder \(f_{d}(;_{d})\) to generate the parameters of the agent network's upper MLP. In this way, each agent \(a_{i}\) conditions its behaviors on \(e^{i}\) with group-related information embedded, promoting potential cooperation within each group during decentralized execution. The decoder hypernetwork \(f_{d}\) is trained by the TD-loss introduced in Section 4.3.

The proposed actor network has two merits. Firstly, it enables diversified policies while sharing all the parameters. Policy decentralization with shared parameters is widely utilized to improve scalability and learning efficiency. However, agents tend to behave similarly when sharing parameters, preventing effective exploration and complex cooperative policies. It is also undesirable to entirely forgo shared parameters in pursuit of diversity since proper sharing accelerates learning. Our method hybridizes the efficiency of parameter-sharing and the policy diversity needed for complex collaboration. Secondly, the decoder hypernetwork \(f_{d}\) integrates the extracted agent info \(e\) into the policy gradients, providing informative group-related information to enrich local utilities and promote intra-group cooperation. Concretely, the partial derivative for updating the parameters \(_{h}\) of the GRU and the bottom MLP is:

\[}{_{h}}=}{ Q ^{a}}}{_{h}}=}{  Q^{a}}}{ v_{h}^{a}}^{ a}}{_{h}}=f_{d}(e^{a})}{ Q ^{a}}^{a}}{_{h}},\] (3)

where \(v_{h}\) is the representation after the GRU illustrated in the left side of Figure 3. Eqn.(3) shows that \(e^{a}\) is deeply involved in the policy updating of agent \(a\), providing richer group knowledge to generate local utilities and facilitating group-related guidance for cooperative decentralized execution.

### Overall Learning Framework

We next introduce GoMARL's overall learning framework. It contains two mixing networks that generate the group-wise \(Q^{g}\) and the global \(Q^{tot}\), respectively. As for the group mixing network, although \(_{1}^{g}\) enables a weighted mixing of local utilities of agents in group \(g\), the naive mixture of \(_{a g}_{1}^{a}Q^{a}+b\) lacks the guidance of group status to reflect the group action-value in a specific _group-wise state_. Therefore, we build the group mixing network with a two-layer structure that also embeds the group-wise state into the weights \(_{2}^{g}\) to generate \(Q^{g}\). In particular, group \(g_{j}\)'s group-wise state \(s^{g_{j}}\) is a fusion of the agent info \(e^{i}\) for all \(a_{i} g_{j}\). To cohesively summarize the group state based on the agent info of all agents in group \(g_{j}\), we apply a pooling operation  over each dimension of the agent info \(e^{i}\) to generate the group-wise state \(s^{g_{j}}\) describing the current group status. The pooling operation also ensures adaptability to the dynamic group size (_i.e._, the number of agents per group). We build a group-wise \(_{2}\) generator \(f_{_{2}}(s^{g})\) to map the fused group state into \(_{2}^{g}\). Similar to the group-related info encoder \(f_{e}\), the \(_{2}\) generator integrates the group state \(s^{g}\) into the policy gradient: \(}{ Q^{a}}=}{ v_{h}^{ g}}^{g}}{ Q^{a}}=f_{_{2}}(s^{g}) ^{a}}{ Q^{a}}\), providing group status information for \(Q^{g}\) generation that facilitates efficient _intra-group cooperation_. \(v_{Q}\) is the representation after \(_{1}^{g}\) marked in Figure 4. The

Figure 4: Overall learning framework of GoMARL.

Figure 3: The specialized agent network.

two-layer mixing structure of the group mixing network estimates the group action-value \(Q^{g}\) by \(_{1}\) which decides the group division and \(_{2}\) which carries group status information.

GoMARL estimates the global action-value \(Q^{tot}\) by mixing all the \(Q^{g}\) in a similar fashion. Concretely, the two layers of the total mixing network are generated by two hypernetworks, respectively taking group states \(s^{g}\) and the full state \(s\) as inputs. Likewise, \(s^{g}\) and \(s\) are deeply involved in the gradients, promoting _inter-group cooperation_. The architecture of the total mixing network is akin to the group mixing network and is omitted in Figure 4 for brevity. The TD-loss of the estimated \(Q^{tot}\) is:

\[_{TD}()=_{}[(r+_{ ^{}}^{tot}(s^{},^{})- Q^{tot}(s,))^{2}],\] (4)

where \(^{tot}\) is a target network with periodic updates. The overall learning objective of GoMARL is:

\[()=_{TD}()+_{g}_{g}( _{_{1}})+_{SD}_{SD}(_{e}),\] (5)

where \(=(_{h},_{e},_{d},_{})\). \(_{}\) denotes the parameters of hypernetworks producing all the mixing weights and biases. \(_{g}\) and \(_{SD}\) are two scaling factors.

Although containing two mixing networks, the total mixing-net size of GoMARL is smaller than the commonly used single monotonic mixing network , as verified in Section 5.1. This is mainly attributed to the input dimension reduction. The monotonic mixing network takes the global state \(s\) as input of all the hypernetworks. In contrast, we take specific group information (_i.e._, individual group-related info \(e\), group state \(s^{g}\), and state \(s\) is only used in one hypernetwork). This parameter reduction offsets the increase of an extra mixing network. Compared with flat value factorization methods learning only with the global state, our method allows fine-grained learning guided by more direct signals embedded in the policy gradients, facilitating intra- and inter-group cooperation.

## 5 Experiments

**Baselines.** We compare GoMARL with prominent baselines to verify its effectiveness and efficiency. Hu et al.  fairly compared existing MARL methods without code-level optimizations and reported that QMIX  and QPLEX  are the top two value factorization methods. The authors also finetuned QMIX (denoted as Ft-QMIX in our paper), which attains higher win rates than the vanilla QMIX. VAST  learns value factorization for sub-teams based on apriori setting on group number. Therefore, we compare GoMARL with Ft-QMIX, QPLEX, and VAST to show its performance as a value factorization method. The baselines also include role-based methods ROMA  and the representative credit assignment method RIIT . The latter combines effective modules of noticeable methods, and comparing them can further illustrate the superiority of GoMARL.

Experimental setup.All the methods are trained with \(8\) parallel runners for \(10\)M steps and are evaluated every \(10\)K steps with \(32\) episodes. We report the \(1\)st, median, and \(3\)rd quartile win rates across \(5\) random seeds. Please refer to Appendix B for experimental setup details.

### Performance on SMAC

Methods are evaluated on six challenging SMAC maps. The chosen maps involve homogeneous and heterogeneous teams with asymmetric battles, allowing a holistic study of all methods.

**Overall performance.** The performance comparison of all the methods in the six _Hard_ and _Super Hard_ SMAC maps is shown in Figure 5. As the results show, each baseline method only achieves satisfactory performance on some of the tasks with specific properties they specialize in; _e.g._, RIIT performs well on MMM2 but converges much slower in other tasks; QPLEX's leaning is not efficient in 8m_vs_9m and corridor. Ft-QMIX significantly outperforms the vanilla QMIX and has more efficient learning than other baselines. GoMARL has a similar performance as Ft-QMIX in 8m_vs_9m and corridor. However, the superiority of GoMARL can be clearly validated in all the other maps.

**Parameter size for value mixing.** Many methods use larger mixing networks with stronger expressiveness and fitting abilities to obtain superior performance. However, they often fail when compared to baselines that use a mixing net of the same size . As shown in Table 1, our dual-hierarchical mixing architecture has fewer parameters when there are a large number of agents (when \(n>5\)). VAST is not compared since it utilizes a linear summation of local utilities. The results of GoMARL are the average of the five runs since each run may learn a slightly different grouping with various group numbers. GoMARL outperforms other methods despite using fewer mixing parameters, highlighting its inherent superiority over methods relying on stronger mixing networks.

**Component analysis and ablation study.** We next conduct detailed component studies to analyze how GoMARL improves efficiency and enhances performance. GoMARL contains three key components: (1) an automatic grouping mechanism that progressively divides the team into proper groups; (2) specialized agent networks that generate diversified policies while sharing all the parameters; and (3) sufficiently informative signals integrated into the gradients to promote efficient cooperation. We respectively study each module on three _Super Hard_ maps to show how they influence performance.

(1.1) Ablation study of the grouping mechanism. We validate our grouping mechanism by comparing GoMARL with other intuitive alternatives. All methods utilize the same architecture as GoMARL to reflect how grouping itself influences performance. As shown in the top row of Figure 6, setting all agents as a group in corridor converges faster but has significant variances. It may be due to the hard exploration of efficient cooperation without grouping guidance. Another two intuitive groupings, each agent a group and an equal division into two groups \(\{\{a_{1},a_{2},a_{3}\},\{a_{4},a_{5},a_{6}\}\}\), have minor variance. However, their learning efficiency is affected since inappropriate groupings fail to promote cooperative behaviors. MMM2 contains heterogeneous agents; thus, a natural grouping is to keep the agents of the same type in one group. This natural grouping outperforms most alternatives but is inferior to our dynamic grouping adjustment. 3s5z_vs_3s6z is another scenario with heterogeneous agents, and the natural grouping of setting homogeneous agents as a group is also studied. It performs nearly the same as GoMARL because our mechanism learns the grouping precisely according to the agents' type in this scenario. We also analyze this map with a grouping containing three groups \(\{\{a_{1},a_{4},a_{5}\},\{a_{2},a_{6},a_{7}\},\{a_{3},a_{8}\}\}\) (1s2z, 1s2z, 1s1z); however, this balanced grouping fails to form effective collaboration. We can see from these results that appropriate grouping facilitates efficient cooperation and accelerates learning. Our grouping mechanism automatically learns adaptive grouping in different tasks and assists GoMARL with superior performance and efficiency.

(1.2) Learned grouping analysis. To demonstrate whether the learned grouping makes sense, we further visualize the trained strategy in a corridor battle, as illustrated in Figure 7. Six allied Zealots

   Maps & (Ft-)QMIX & QPLEX & ROMA & RIIT & GoMARL \\ 
3s\_vs\_5z & 21.601K & 72.482K & **13.281K** & 37.986K & 26.530K \\
5m\_vs\_6m & 31.521K & 107.574K & **25.377K** & 51.362K & 31.554K \\
8m\_vs\_9m & 53.313K & 197.460K & 63.393K & 93.986K & **51.427K** \\ corridor & 68.929K & 303.808K & 81.537K & 122.882K & **53.859K** \\ MMM2 & 84.929K & 342.248K & 134.401K & 177.282K & **74.244K** \\
3s5z\_vs\_3s6z & 63.105K & 243.156K & 81.345K & 118.466K & **61.028K** \\   

Table 1: Size comparison of all methods’ mixing network(s)

Figure 5: Comparison of GoMARL against baseline algorithms on six SMAC maps.

fight twenty-four Zerglings on this _Super Hard_ map. The massive disparity in unit numbers between the two sides implies that the whole team must refrain from launching an attack together. The only winning strategy is to sacrifice a small number of agents who leave the team and attract the attention of most enemies. Taking this opportunity, our large force eliminates the rest of the enemies. The surviving agents then use the same tactic to keep attracting several enemies and kill them together. In this battle, Agent 1 and 2 sacrifice themselves to attract most enemies and bring enough time for the team to eliminate the remaining enemies. Other agents fight as a subgroup and successfully kill all the surviving enemies. GoMARL learns a double-group setting in this battle, where Agent 1 and 2 are in the same group while the others are set in another group. This grouping is explicable in light of the combat situation, and this reasonable grouping guidance contributes to our superior performance.

(2) Component study of the specialized agents.

We transplant our specialized agent network (SAN) into other baselines to verify module effectiveness. ROMA is not included since its actors are produced by its learned roles, and the replacement will invalidate the method. As shown in Figure 8, all methods are improved when equipped with SAN. However, even with SAN, all baselines fail to surpass GoMARL. As shown in the learning curves in Appendix C, although dynamic grouping may reduce learning efficiency in the early stage of training, the learned grouping will significantly accelerate learning in the later stage. Therefore, the automatic grouping module and SAN are both crucial to GoMARL.

(3) Ablation study of the informative group-related signals. Our method models individual group-related information \(e\) and group state \(s^{g}\) to integrate them into the gradients. The former adjusts local utilities for cooperative behaviors and realizes policy diversity, while the latter fosters efficient group cooperation. We ablate them respectively to validate their effectiveness. Concretely, we replace the group state \(s^{g}\) with the full state \(s\) and remove the group-related information \(e\) to degenerate the specialized agent networks into vanilla agent networks. As shown in the bottom row of Figure 6, both signals greatly improve learning efficiency. On the one hand, the agent info \(e\) carrying group-related information is trained to encourage inter-group diversity, enabling extensive cooperative exploration and learning acceleration. On the other hand, the fused \(s^{g}\) summarizes informative group-related status that facilitates efficient group-wise coordination. Although the global state \(s\) provides complete

Figure 8: Method performance with and without SAN in 3s5z_vs_3s6z after training 8M steps.

Figure 6: Ablations of the grouping mechanism (top row) and the group-related signals (bottom).

Figure 7: The learnt groups \(\{\!\{a_{1},a_{2}\}\!,\!\{a_{3},a_{4},a_{5},a_{6}\}\!\}\) is explicable to the corridor combat situation.

knowledge, it is burdensome for agents to extract effective guidance that promotes policy learning. Therefore, utilizing our specific group-wise signals \(e\) and \(s^{g}\) is more efficient than the global state \(s\).

### Performance on Google Research Football

We also test GoMARL on three challenging Google Research Football (GRF) offensive scenarios. Agents in GRF need to coordinate timing and positions for organizing offense to seize fleeting opportunities, and only scoring leads to rewards. Therefore, the GRF tasks are more complicated than the SMAC battles, and the comparison in GRF is a secondary proof of our method's effectiveness.

**Performance.** Figure 9 shows the performance of all methods in GRF. GoMARL, Ft-QMIX, and QPLEX all perform well in pass_and_shoot_with_keeper. However, the advantages of our method become increasingly evident in the other two environments requiring more coordination. Only GoMARL achieves over 50% of the score reward in all scenarios. The superior performance with significant efficiency in the second testbed further demonstrates the transferability of our method.

**Visualization.** The trained strategy in counterattack_easy is visualized to validate if the learned group division makes sense. GoMARL learned a reasonable grouping \(=\{\{a_{1},a_{2}\},\{a_{3},a_{4}\}\}\) for this complex goal, in which the first group brought the ball into the penalty area through smooth coordination, while the second group created two shoots and the final goal through skillful cooperation, as shown in Figure 10. Appendix D provides a detailed discussion of this visualization.

## 6 Conclusion and Future Work

Grouping is essential to the efficient cooperation of multi-agent systems. Instead of utilizing apriori knowledge, this paper proposes an automatic grouping mechanism that gradually learns reasonable grouping as training proceeds. Based on the dynamic group division, we further model informative group-related signals to achieve fine-grained value factorization and encourage policy specialization, promoting efficient intra- and inter-group coordination. With these novelties, our method GoMARL achieves impressive performance with high efficiency on the SMAC and GRF benchmarks.

The automatic grouping mechanism of GoMARL is currently grounded in the value decomposition process, rendering it inapplicable to policy-based methods. Our future research will delve into automatic grouping based on policy gradients to enhance the learning efficiency of the policy-based methods. Furthermore, the current version of GoMARL is only designed for one-stage and one-level grouping, and it possesses the potential to achieve finer granularity in group division. We will investigate multi-stage and multi-level grouping (i.e., group-within-a-group) in the future to pursue better performance and higher learning efficiency in more complex environments.

Figure 10: The learned groups \(\{\{a_{1},a_{2}\},\{a_{3},a_{4}\}\}\) is explicable to a counterattack situation.

Figure 9: Performance comparison with baselines in three Google Research Football scenarios.

Acknowledgement

This work is supported in part by the National Key Research and Development Program of China under Grant No. 2020AAA0103401; in part by the Natural Science Foundation of China under Grant 62076238, Grant 62222606, and Grant 61902402; and in part by the China Computer Federation (CCF)-Tencent Open Fund.