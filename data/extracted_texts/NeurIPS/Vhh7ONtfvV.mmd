# Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP

Sriram Balasubramanian

Department of Computer Science

University of Maryland, College Park

sriramb@cs.umd.edu

&Samyadeep Basu

Department of Computer Science

University of Maryland, College Park

sbasu12@umd.edu

&Soheil Feizi

Department of Computer Science

University of Maryland, College Park

sfeizi@cs.umd.edu

###### Abstract

Recent work has explored how individual components of the CLIP-ViT model contribute to the final representation by leveraging the shared image-text representation space of CLIP. These components, such as attention heads and MLPs, have been shown to capture distinct image features like shape, color or texture. However, understanding the role of these components in arbitrary vision transformers (ViTs) is challenging. To this end, we introduce a general framework which can identify the roles of various components in ViTs beyond CLIP. Specifically, we (a) automate the decomposition of the final representation into contributions from different model components, and (b) linearly map these contributions to CLIP space to interpret them via text. Additionally, we introduce a novel scoring function to rank components by their importance with respect to specific features. Applying our framework to various ViT variants (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the roles of different components concerning particular image features. These insights facilitate applications such as image retrieval using text descriptions or reference images, visualizing token importance heatmaps, and mitigating spurious correlations. We release our code to reproduce the experiments at https://github.com/SriramB-98/vit-decompose

## 1 Introduction

Vision transformers and their variants  have emerged as powerful image encoders, becoming the preferred architecture for modern image foundation models. However, the mechanisms by which these models transform images into representation vectors remain poorly understood. Recently, Gandelsman et al.  made significant progress on this question for CLIP-ViT models with two key insights: (i) They demonstrated that the residual connections and attention mechanisms of CLIP-ViT enable the model output to be mathematically represented as a sum of vectors over layers, attention heads, and tokens, along with contributions from MLPs and the CLS token. Each vector corresponds to the contribution of a specific token attended to by a particular attention head in a specific layer. (ii) These contribution vectors exist within the same shared image-text representation space, allowing the CLIP text encoder to interpret each vector individually via text.

Extending this approach to other transformer-based image encoders presents several challenges. Popular models like DeiT , DINO-ViT , and Swin  lack a corresponding text encoder tointerpret the component contributions. Additionally, extracting the contribution vectors corresponding to these components is not straightforward, as they are often not explicitly computed during the forward pass of the model. Other complications include diverse attention mechanisms such as grid attention, block attention (in MaxViT), and windowed/shifted windowed attention (in Swin), as well as various linear transformations like pooling, downsampling, and patch merging applied to the residual streams between attention blocks. These differences necessitate a fresh mathematical analysis for each model architecture, followed by careful application of necessary transformations to the intermediate output of each component to determine its contribution to the final representation. To address these challenges, we propose our framework (described in Figure 1) to identify roles of components in general ViTs.

**First**, we _automate the decomposition of the representation_ by leveraging the computational graph created during the forward pass. This results in a drop-in function, RepDecompose, that can decompose any representation into contributions vectors from model components simply by calling it on the representation. Since this method operates on the computational graph, it is agnostic to the specifics of the model implementation and thus applicable to a variety of model architectures.

**Secondly**, we introduce an algorithm, CompAlign, to _map each component contribution vector to the image representation space of a CLIP model._ We train these linear maps with regularizations so that these maps preserve the roles of the individual components while also aligning the model's image representation with CLIP's image representation. This allows us to map each contribution

Figure 1: **(Left) Workflow:** The first step (RepDecompose) is to decompose a representation \(\) into contributions from its model components \(_{i}\) after being transformed by residual transformations like LayerNorm, linear projections, resampling, patch merging and so on. The second step (CompAlign) aligns each contribution to CLIP space using a set of linear maps \(f_{0},f_{1},,f_{n}\) on the corresponding contributions \(_{0},_{1},,_{n}\). We can then interpret these aligned contributions using the CLIP text encoder. **(Right) Applications of our method:** (a) Visualizing contributions of each token through a specific component using a joint token-component decomposition (b) Retrieving images that are close matches of the reference image (on top) with respect to a given image feature like pattern, person, or location

vector from any component to CLIP space, where they can be interpreted through text using a CLIP text encoder.

**Thirdly**, we observe that there is often _no straighforward one-to-one mapping_ between model components and common image features such as shape, pattern, color, and texture. Sometimes, a single component may encode multiple features, while multiple components may be required to fully encode a single feature. To address this, we propose a _scoring function_ that assigns an importance score to each component-feature pair. This allows us to rank components based on their importance for a given feature, and rank features based on their importance for a given component.

Using this ranking, we proceed to analyze diverse vision transformers such as DeiT, DINO, Swin, and MaxViT, in addition to CLIP, in terms of their components and the image features that they are responsible for encoding. We consistently find that many components in these models encode the same feature, particularly in ImageNet pre-trained models. Additionally, individual components in larger models MaxVit and Swin do not respond to any image feature strongly, but can encode them effectively in combination with other components. This diffuse and flexible nature of feature representation underscores the need for interpreting them using a continuous scoring and ranking method as opposed to labelling each component with a well-defined role. We are thus able to perform tasks such as image retrieval, visualizing token contributions, and spurious correlation mitigation by carefully selecting or ablating specific components based on their scores for a given property.

## 2 Related Work

Several studies attempt to elucidate model predictions by analyzing either a subset of input example through heatmaps [27; 30; 31; 18] or a subset of training examples [15; 23; 24]. Nevertheless, empirical evidence suggests that these approaches are often unreliable in real-world scenarios [14; 3]. These methods do not interpret model predictions in relation to the model's internal mechanisms, which is essential for gaining a deeper understanding of the reliability of model outputs.

**Internal Mechanisms of Vision Models:** Our work is closely related to the studies by Gandelsman et al.  and Vilas et al. , both of which analyze vanilla ViTs in terms of their components and interpret them using either CLIP text encoders or pretrained ImageNet heads. Like these studies, our research can be situated within the emerging field of representation engineering  and mechanistic interpretability [6; 5]. Other works [4; 12; 21] focus on interpreting individual neurons to understand vision models' internal mechanisms. However, these methods often fail to break down the model's output into its subcomponents, which is crucial for understanding model reliability. Shah et al.  examine the direct effect of model weights on output, but do not study the fine-grained role of these components in building the final image representation. Balasubramanian and Feizi  focus on expressing CNN representations as a sum of contributions from input regions via masking.

**Interpreting models using CLIP:** Many recent works utilize CLIP  to interpret models via text. Moayeri et al.  align model representations to CLIP space with a linear layer, but it is limited to only the final representation and can not be applied to model components. Oikarinen and Weng  annotate individual neurons in CNNs via CLIP, but their method cannot be extended easily to high-dimensional component vectors. CompAlign is related to model stitching in which one representation space is interpreted in terms of another by training a map between two spaces [2; 16].

## 3 Decomposing the Final Image Representation

Recently, Gandelsman et al.  decomposed \(_{}\), the final [CLS] representation of the CLIP's image encoder as a sum over the contributions from its attention heads, layers and token positions, as well as contributions from the MLPs. In particular, they observe that the last few attention layers have a significant direct impact on the final representation. Thus, this representation can be decomposed as: \(_{}=_{}+_{l=1}^{L}c_{l,}+_{l=1}^{L}_{h=1}^{H}_{=1}^{N}c_{l,h,t}\), where \(L\), \(H\), \(N\) correspond to the number of layers, number of attention heads and number of tokens. Here, \(c_{l,h,t}\) denotes the contribution of token \(t\) though attention head \(h\) in layer \(l\), while \(c_{l,}\) denotes the contribution from the MLP in layer \(l\). Due to this linear decomposition, different dimensions can be reduced by summing over them to identify the contributions of tokens or attention heads to the final representation. While this decomposition is relatively simple for vanilla ViTs, it cannot be directly used for general ViT architectures due to use of self-attention variants such as window attention, grid attention, or block attention, combined with operations such as pooling or patch merging on the residual stream. The final representation may also not just be a single \(_{}\) but \(_{i=1}^{N}_{i,}\) or even \(_{i=1}^{L}_{i}\), or some combination of the above.

```
0:\(\), the final node (denoting the function that output the representation \(\)) in the computational graph \(G\) \(\{^{j}\}_{j=1}^{N}\), \(N\) direct contributions from various components (indexed by \(j\)) in the model after graph traversal functionRepDecompose(\(\)) \(=(_{1},_{2},,_{n})\) if\(\) is non-linear then return[\(\)] else \(\)Cannot decompose further \(\)\(\) is linear  Let \(_{1}=_{1}(),_{2}=_{2}(),,_{n}= _{n}()\) \( i,\{^{j}_{i}\}_{j=1}^{N_{i}}=(_{i}) \)\(z_{i}=_{j=1}^{N_{i}}c^{j}_{i}\) ( \(^{j}_{i}\) are component contributions )  Then, \(=(_{j=1}^{N_{i}}^{j}_{1},_{j=1}^{N_{2}}^{j}_{2 },,_{j=1}^{N_{n}}^{j}_{n})\)  Or, \(=_{i=1}^{n}_{j=1}^{N_{i}}f^{}(^{j}_{i})\)\(\)\(f^{}\) exists since \(\) distributes over inputs due to linearity return[\(\{f^{}(^{j}_{i})\}_{j=1}^{N_{i}}\)]\({}_{i=1}^{n}\) ```

**Algorithm 1**RepDecompose

### RepDecompose: Automated Representation Decomposition for ViTs

We thus seek a general algorithm which can automatically decompose the representation for general ViTs. This can be done via a recursive traversal of the computation graph. Suppose the final representation \(\) can be decomposed into component contributions \(_{i,t}\) such that \(=_{i,t}_{i,t}\). Here each \(_{i,t}\) corresponds to the contribution of a particular token \(t\) through some model component \(i\). For convenience, let \(_{i}=_{t}_{i,t}\). Then, if given access to the computational graph of \(\), we can identify potential linear components \(_{i,t}\) by recursively traversing the graph starting from the node which outputs \(\) in reverse order till we hit a non-linear node. The key insight here is that the output of any node which performs a linear reduction (defined as a linear operation which results in a reduction in the number of dimensions) is equivalent to a sum of individual tensors of the same dimension as the output. These tensors can be collected and transformed appropriately during the graph traversal to obtain a list of tensors \(_{i,t}\), each members of the same vector space as the representation \(\). This kind of linear decomposition is possible due to the overwhelmingly linear nature of transformers. The high-level logic of RepDecompose is detailed in Algorithm 1, please refer to Algorithm 2 in the appendix or the code for a more detailed description. We also illustrate the operation of the algorithm on an attention-MLP block in the appendix.

In practice, the number of components quickly explodes as there are a very large number of potential component divisions for a given model. To make analysis and computation tractable, we restrict it to only the attention heads and MLPs with no finer divisions. We also constrain RepDecompose to only return the _direct_ contributions of these components to the output. This means that the contribution \(_{i}\) is the _direct_ contribution of component \(i\) to \(\), and does not include its contribution to \(\) via a downstream component \(j\). Additionally, the token \(t\) in \(_{i,t}\) is present in the input of the component \(i\), and not the input image. In principle, RepDecompose could return higher order terms such as \(_{j,i}\) which is the contribution of model component \(i\) via the downstream component \(j\). A full understanding of these higher order terms is essential to get a complete picture of the inner mechanism of a model, however we defer this for future work.

## 4 Aligning the component representations to CLIP space

Having decomposed the representation into contributions from relevant model components, we now aim to interpret these contributions through text using CLIP by mapping them to CLIP space. Formally, given that we have a set of vectors \(\{_{i}\}_{i=1}^{N}\) such that \(_{i}^{N}_{i}=\), the final representation of model, we require a set of linear maps \(f_{i}\) such that the sum of \(_{i}f_{i}(_{i})=_{}\), the final representation of the CLIP model. Once we have these CLIP aligned vectors, we can proceed to interpret them via text using CLIP's text encoders.

However, from an interpretability standpoint, a few additional constraints on the linear maps are desirable. Consider a component contribution \(_{i}\) and two directions \(,\) belonging to the same vector space as \(_{i}\) which represent two distinct features, say shape and texture. Let us further assume that the component's dominant role is to identify shape, and thus the variance of the projection of \(_{i}\) along \(\) is higher than that of \(\). We want this relative importance of features to be maintained in \(f_{i}(_{i})\). Additionally, we also want any two linear maps \(f_{i}\) and \(f_{j}\) to not change the relative norms of features in components \(_{i}\) and \(_{j}\). We can express these conditions formally as follows:

1. **Intra-component norm rank ordering**: For any two vectors \(,\) and a linear map \(f_{i}\) such that \(\|\|\|\|\), we have \(\|f_{i}()\|\|f_{i}()\|\)
2. **Inter-component norm rank ordering**: For any two vectors \(,\) and linear maps \(f_{i},f_{j}\) such that \(\|\|\|\|\), we have \(\|f_{i}()\|\|f_{j}()\|\)

**Theorem 1**.: _Both of the above conditions together imply that all linear maps \(f_{i}\) must be a scalar multiple of an orthogonal transformation, that is for all \(i\), \(f_{i}^{T}f_{i}=kI\) for some constant \(k\). Here, \(I\) is the identity transformation._

The proof is deferred to appendix E. We can now formulate a novel alignment method, CompAlign, to map contributions of model components to CLIP space. CompAlign minimizes a loss function over \(\{f_{i}\}_{i=1}^{N}\) to obtain a good alignment between model representation space and CLIP space:

\[L(\{f_{i}\}_{i=1}^{N})=_{\{_{i}\}_{i=1}^{N},_{ }}[1-(_{i}f_{i}(_{i}),_{} )]+_{i}\|f_{i}^{T}f_{i}-I\|_{F}\]

The first term of the objective is the _alignment loss_, which is the average cosine distance between the CLIP representation \(_{}\) and the transformed model representation \(_{i}f_{i}(_{i})\). It quantifies the directional discrepancy between the two vectors. The second term is the _orthogonality regularizer_ which imposes a penalty if the linear maps \(f_{i}\) are not orthogonal, ensuring that the \(f_{i}\) adhere closely to the specified conditions. We can now train \(f_{i}\) using the above loss function on ImageNet-1k. The training is label-free and can be done even over unlabeled datasets. We obtain \(_{}\) from the CLIP image encoder and \(\{_{i}\}_{i=1}^{N}\) from running RepDecompose on the final representation of the model.

   Embedding & ImageNet & One map only & CompAlign & CompAlign \\ Source & pretrained & & (\(=0\)) & \\   & wardrobe & gyromitra & bookcase & filing cabinet \\  & medicine cabinet & home theater & snorkel & snorkel \\  & window shade & drumstick & red wolf & bakery \\  & desk & Samoyed & barbershop & bathtub \\ descriptions of a & barbershop & muzzle & microwave oven & dining table \\ random & refrigerator & bookstore & bassinet & red wolf \\ component & library & dining table & disc brake & gyromitra \\  & shoji screen & medicine cabinet & dining table & shoji screen \\  & bathtub & park bench & sink & Norwich Terrier \\  & dining table & tusker & window screen & bookstore \\  Match rate & - & 0.08 & 0.155 & 0.185 \\ Cosine Distance & - & 0.23 & 0.18 & 0.17 \\   

Table 1: Comparison of different methods to map the representation space of ImageNet-1k pre-trained DeiT-B/16 to CLIP image representation space. The green colored texts are exact matches with the top-10 descriptions obtained from the imagenet pretrained embeddings, while the orange colored texts are approximate matches. The match rate is the average fraction of exact matches across all components, while cosine distance is the average cosine distance between the CLIP representations and the transformed model representations on ImageNet

**Ablation study:** We now conduct an ablation study on CompAlign. The first naive alignment method is the case where all \(f_{i}\) are the same linear map \(f\), without constraints on \(f\), similar to . The second method is a version of CompAlign with \(=0\), where all \(f_{i}\) are different but not trained with the orthogonality regularizer. To compare these methods, we first get a "ground truth" description for each model component by using the TextSpan algorithm on the class embedding vectors from the ImageNet pre-trained head. TextSpan retrieves those class embedding vectors along which variance of the component output is maximized, thus yielding descriptions of each component in terms of the top 10 most dominant ImageNet classes. We then use CompAlign and the two baselines to map the representations to CLIP space, and apply TextSpan on CLIP embedded ImageNet class vectors to label each model component. We can then compare the descriptions this yields with the "ground truth" text description for each head. The results, shown in Tab. 1, indicate that CompAlign's TextSpan descriptions have the most matches to the ImageNet pre-trained descriptions, followed by CompAlign with \(=0\) and the naive single map method. This trend is similar in the average cosine distance between the CLIP representations and the transformed model representations.

## 5 Component ablation

To identify the most relevant model layers for downstream tasks, we progressively ablate them and measure the drop in ImageNet classification accuracy. Ablation involves setting a layer's contribution to its mean value over the dataset. We use the following models from Huggingface's timn repository: (i) DeiT (ViT-B-16) , (ii) DINO (ViT-B-16) , (iii) DINOv2 (ViT-B-14) , (iv) Swin Base (patate size = 4, window size = 7) , (v) MaxViT Small , along with (vi) CLIP (ViT-B-16)  from open_clip. DeiT, Swin, and MaxViT are pretrained on ImageNet with a supervised classification loss, DINO on ImageNet with a self-supervised loss, DINOv2 on LVD-142M with a self-supervised loss, while CLIP is pretrained on a LAION-2B subset with contrastive loss.

In Fig. 2, we see that for models not trained on ImageNet (CLIP and DINOv2), removing the last few layers quickly drops the accuracy to zero. In contrast, models trained on ImageNet experience a more gradual decline in accuracy, reaching zero only after more than half the layers are ablated. This trend is consistent across both self-supervised (DINO) and classification-supervised (DeiT, SWIN, MaxViT) models. This suggests that ImageNet-trained models encode useful features redundantly across layers for the classification task. Additionally, larger models with more layers, such as MaxVit, show significantly more redundancy, with minimal accuracy impact from ablating the last four layers. Conversely, the first few layers in all models contribute little to the output. Therefore, our analysis in the subsequent sections is focused on the last few layers of each model.

Figure 2: Ablation results for various different image encoders. The top-1 ImageNet accuracy is plotted as the layers of the model are increasingly ablated away, starting from the last layer up till the first layer. The circles on the plot represent the endpoints of blocks, the definition of which varies across model architectures. For the vanilla ViT variants, a block is an attention MLP pair, while for SWIN, it is a pair of windowed/shifted windowed attention and an MLP. For MaxVit, this might either be a grid/block attention-MLP pair, or an MBConv block.

## 6 Feature-based component analysis

We now analyze the final representation in terms finer components like attention heads and MLPs, focusing on the last few significant layers. We limit decomposition to 10 layers for DeiT, DINO, and DINOv2, but 12 layers for SWIN and 20 layers for MaxVit due to their greater depth and redundancy across components. We accumulate contributions from the remaining components in a single vector \(_{}\), expressing \(\) as \(_{}+_{i}^{N}_{i}\), where \(N+1\) is the total number of components including \(_{}\). Here, \(N=65\) for DeiT, DINO, and DINOv2; \(N=134\) for SWIN, and \(N=156\) for MaxVit.

We then ask if it is possible to attribute a feature-specific role to each component using an algorithm such as TextSpan. These image features may be low-level (shape, color, pattern) or high-level (such as location, person, animal). However, such roles are not necessarily localized to a single component, but may be distributed among multiple components. Furthermore, each individual component by itself may not respond significantly to a particular feature, but it may jointly contribute to identifying a feature along with other components. Thus, rather than rigidly matching each component with a role, we aim to devise a _scoring function_ which can assign a score to each component - feature pair, which signifies of how important the component is for identifying a given image feature. A continuous scoring function allows us to select multiple components relevant to the feature by sorting the components according to their score.

We devise this scoring function (described in the appendix in Alg. 3) by looking at the projection of each contribution vector \(_{i}\) onto a vector space corresponding to a certain feature. Suppose we have a feature, "pattern", that we want to attribute to the components. We first describe the feature in terms of an example set of feature _instantiations_, such as "spotted", "striped", "checkered", and so on. We then embed each of these texts to CLIP space, obtaining a set of embeddings \(B\). We also calculate the CLIP aligned contributions \(f_{i}(_{i})\) for each component \(i\) over an image dataset (ImageNet-1k validation split). Then, the score is simply the correlation between projection of \(f_{i}(_{i})\) and the projection of \(_{i}f_{i}(_{i})\) onto the vector space spanned by \(B\). Intuitively, this measures how closely the component's contribution correlates with the overall representation. The scores obtained for each component and feature can be used to rank the components according to its importance for a given feature to obtain a _component ordering_, or to rank the features according to its importance for a specific component to get a _feature ordering_.

   Model & Feature & Component \\  & ordering & ordering \\  DeiT & 0.531 & 0.684 \\ DINO & 0.714 & 0.723 \\ DINOv2 & 0.716 & 0.703 \\ SWIN & 0.628 & 0.801 \\ MaxVit & 0.681 & 0.849 \\   

Table 2: Spearman’s rank correlation between the orderings induced by CLIP score and component score averaged over a selection of common features

Figure 3: Top-3 images retrieved by DeiT components for “forest” and “beach” ordered according to their relevance for the attribute “location”. Each column here corresponds to the images returned by the sum of contributions of 3 components, so column \(i\) corresponds to components \(_{3i}\), \(_{3i+1}\), \(_{3i+2}\). A large fraction of components which can recognize the “location” feature are sorted correctly by the scoring function

### Text based image retrieval

We can now use our framework to identify components which can retrieve images possessing a certain feature most effectively. Using the scoring function described above, we can identify the top \(k\) components \(\{_{i}\}_{i=1}^{k}\) which are the most responsive to a given feature \(p\). We can use the cosine similarity of \(_{i=1}^{k}f_{i}(_{i})\) to the CLIP embedding of an instantiation \(s_{p}\) of the feature \(p\) to retrieve the closest matches in ImageNet-1k validation split. In Fig. 3, we show the top 3 images retrieved by different components of the DeiT model for the location instantiation "forest" and "beach" when sorted according to the component ordering for the "location" feature. As the component score decreases, the images retrieved by the components grow less relevant. Also note that a significant fraction of components are capable of retrieving relevant images. This further confirms the need for a continuous scoring function which can identify multiple components relevant to a feature.

To quantitatively verify our scoring function, we devise the following experiment. We first choose a set of common image features such as color, pattern, shape, and location, with a representative set of feature instantiations for each (details in appendix B). The scoring function induces a component ordering for each feature \(p\) and feature ordering for each component \(i\). We then compute the cosine similarity \(_{i,s_{p}}=(f_{i}(_{i}),_{s_{p},})\) where \(_{s_{p},}\) is the CLIP text embedding of \(s_{p}\). We can compare this to the cosine similarity \(_{,s_{p}}=(_{},_{s_{p},})\) where \(_{}\) is the CLIP image representation. The correlation coefficient between \(_{i,s_{p}}\) and \(_{,s_{p}}\) over an image dataset can be viewed as another score which is purely a function of how well the component \(i\) can retrieve images matching \(s_{p}\) as judged by CLIP. Averaging this correlation coefficient over all \(s_{p}\) for a given \(p\) yields a "ground truth" proxy for our scoring function. We can measure the Spearman rank correlation (which ranges from -1 to 1) between the component (or feature) ordering induced by our scoring function and the ground truth and average it over features (or components). In Tab. 2, we observe that the rank correlation is significantly high for all models for both feature and component ordering. The individual rank correlations for component orderings for common features can be found in Tab. 4.

### Image based image retrieval

We can also retrieve images that are similar to a reference image with respect to a specific feature. To do this, we first choose components which are highly significant for the given feature while being comparatively less relevant for other features. Mathematically, for a feature \(p P\), the set of all relevant features, we want to choose component \(i\) with score \(s_{i,p}\) such that the quantity \(_{p^{} P p}s_{i,p}-s_{i,p^{}}\) is maximised. Intuitively, we want components which have the highest

Figure 4: Top-3 images retrieved by the most significant components for various features relevant to the reference image (displayed on top). The models used are (from left to right) DINO, DeiT, and SWIN. More exhaustive results can be found in appendix Hgap between \(s_{i,p}\) and \(s_{i,p^{}}\) where \(p^{}\) can be any other feature. We can then select a set of \(k\) such components \(C_{k}\) by sorting over the score gap, and sum them to obtain a feature-specific image representation \(_{p}=_{i C_{k}}_{i}\). Now, we can retrieve any image \(^{}\) similar in feature \(p\) to a reference image \(\) by computing the cosine similarity between \(^{}_{p}\) and \(_{p}\), which are the feature-specific image representations for \(^{}\) and \(\). We show a few examples for image based image retrieval in Fig. 4. Here, we tune \(k\) to ensure that it is not so small that the retrieved images do not resemble the reference image at all, and not so large that the retrieved images are overall very similar to the reference image. We can see that the retrieved images are significantly similar to the reference image with respect to the given feature, but not similar overall. For example, when the reference image is a handbag with a leopard print, the "pattern" components retrieve images of leopards which have the same pattern, while the "fabric" components return other bags which are made of similar glossy fabric. Similarly, for the ball with a spiral pattern on it, we retrieve images which resemble the spiral pattern in the second row, while they resemble the shape in the third row.

Note that this experiment only involves the alignment procedure for computing the scores and thereby selecting the component set \(C_{k}\). The process of retrieving the images is based on \(_{p}\) which exists in the model representation space and not CLIP space. This shows that the model inherently has components which (while not constrained to a single role) are specialized for certain properties, and this specialization is not a result of the CLIP alignment procedure.

### Visualizing token contributions

As discussed in Section 3.1, contribution from a component \(i\) can be further decomposed as a sum over contributions from a tokens, so \(_{i}=_{t}_{i,t}\). For any particular CLIP text embedding vector \(\) corresponding to a realization of some feature \(p\), we have \(^{}f_{i}(_{i})=_{t}^{}f_{i}(_{i,t})\). We can visualize this token-wise score \(^{}f_{i}(_{i,t})\) as a heat map to know which tokens are the most influential with respect to \(\). We show the heat map obtained via this procedure in Fig. 5 for two example images for the DeiT model. The components used for each heat map correspond to the feature being highlighted and are selected using the scoring function we described previously. We can observe that the heatmaps are localized within image portions which correspond to the text description. We also compare our method against zero-shot segmentation methods for ImageNet classes such as GradCAM  and Chefer et al.  and find that our method outperforms the baselines (see Appendix J).

### Zero-shot spurious correlation mitigation

We can also use the scoring function to mitigate spurious correlations in the Waterbirds dataset  in a zero-shot manner. Waterbirds dataset is a synthesized dataset where images of birds commonly

Figure 5: Visualization of token contributions as heatmaps for two example images for the DeiT model. The relevant feature and the head most closely associated with the feature is displayed on the bottom of the heatmap, while the feature instantiation is displayed on the top. The layer numbering starts from the last layer (which has index ’00’). The regions highlighted in red contribute positively to the prediction, while blue regions contribute negatively. More results in appendix I

   Model & Worst group & Average group \\ name & accuracy & accuracy \\  DeiT & 0.733 \(\)**0.815** & 0.874 \(\)**0.913** \\ CLIP & 0.507 \(\)**0.744** & 0.727 \(\)**0.790** \\ DINO & 0.800 \(\)**0.911** & 0.900 \(\)**0.938** \\ DINOv2 & 0.967 \(\)**0.978** & 0.983 \(\)**0.986** \\ SWIN & 0.834 \(\)**0.871** & 0.927 \(\)**0.944** \\ MaxVit & 0.777 \(\)**0.814** & 0.875 \(\)**0.887** \\   

Table 3: Worst group accuracy and average group accuracy for Waterbirds dataset before and after intervention for various models (format is before \(\)**after**)found in water ("waterbirds") and land ("landbirds") are cut out and pasted on top of images of land and water background. For this experiment, we regenerate the Waterbirds dataset following Sagawa et al.  but take care to discard background images with birds and thus eliminate label noise. We select the top 10 components for each model which are associated with the "location" feature but not with the "bird" class following the method we used in Sec. 6.2. We then ablate these components by setting their value to their mean over the Waterbirds dataset. In Tab. 3, we observe a significant increase in the worst group accuracy for all models, accompanied with an increase in the average group accuracy as well. The changes in all four groups can be found in appendix K in Tab. 6.

## 7 Conclusion

In this work, we propose a ViT component interpretation framework consisting of an automatic decomposition algorithm (RepDecompose) to break down the model's final representation into component contributions and a method (CompAlign) to map these contributions to CLIP space for text-based interpretation. We also introduce a continuous scoring function to rank components by their importance in encoding specific features and to rank features within a component. We demonstrate the framework's effectiveness in applications such as text-based and image-based retrieval, visualizing token-wise contribution heatmaps, and mitigating spurious correlations in a zero-shot manner.

## Author contributions

Sriram Balasubramanian conceived the main ideas, implemented the algorithms, conducted the experiments, and contributed to writing the paper. Samyadeep Basu contributed to the writing and provided essential advice on the presentation and direction of the paper. Soheil Feizi offered critical guidance on the presentation, writing, and overall direction of the paper.