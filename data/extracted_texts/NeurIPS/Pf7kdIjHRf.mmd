# Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers

Lirui Wang\({}^{1}\)  Xinlei Chen\({}^{2}\)  Jialiang Zhao\({}^{1}\)  Kaiming He\({}^{1}\)

\({}^{1}\)MIT CSAIL \({}^{2}\)Meta, FAIR

https://liruiw.github.io/hpt

###### Abstract

One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through _heterogeneous pre-training_ on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific propriopception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings.

## 1 Introduction

Building robotic policies today is hard: it often requires collecting _specific_ data for each robot, task, and environment, and the learned policies do not generalize beyond these specific settings. A historical lesson that has revolutionized machine learning is that pre-training  on large-scale, high-quality, and diverse data can bring _general_ models that usually outperform specific models. Recent progress in open-source large-scale data collection  has made this path possible, but the heterogeneity (such as varying robot hardware and different environments) present in large-scale robotic data has posed a significant challenge. A central question for the field now is how to leverage the heterogeneous robot data to pre-train robotic foundation models .

Foundation models from natural language processing  and computer vision  have shown a paradigm to achieve general-purpose task-agnostic models through pre-training on massive amounts and diversity of data. In addition to the benefits from more data, training with diverse tasks also enforces the representation to be more generalized. These foundation models can achieve high task success rates for various tasks, are more robust to outliers, and are flexible for adapting to new tasks. These approaches map input signals from distinct domains and tasks into a high-dimensional representation space, and exhibit consistent scaling behaviors . After that, minimal fine-tuning is required to transfer the representation for downstream tasks to achieve good performance.

The heterogeneity in robotics presents a distinct challenge: different robots are physically different embodiments1 of hardware acting in different environments. Each embodiment can have a distinct _proprioception_, including different degrees of freedom, end-effectors, motion controllers, and workspace configurations built for a specific application. Another common heterogeneity in robotics is _vision_ heterogeneity. Robots are often equipped with different camera sensors mounted at different places (e.g. wrist and/or third-person) and the visual appearance of each robot varies dramatically due to environments and tasks. Both proprioception and vision information are crucial for complex, contact-rich, long-horizon behaviors in robotics. Poor learning of such information can lead to overfitting behaviors such as repeating motions for a particular scene and task or even trajectory.

In this work, we propose to address this issue by aligning the proprioception and vision information from different embodiments to a shared "language" of policies through _heterogenous pre-training_ (Figure 1). With such a shared representation, a new embodiment only requires minimal data and training to "translate" its specific setup to the shared "languages". In other words, we want to pre-train task-agnostic and embodiment-agnostic foundational models that can map raw sensor signals from individual embodiments into a shared latent space. Previous works have made significant progress in pre-training only the vision part of the policy on human videos  and pre-training the full policy  with a unified model and dataset format (e.g. using languages ). Additionally, they assume no proprioception in pre-training and add it post hoc in transfer learning.

We introduce Heterogeneous Pre-trained Transformers (HPT), a family of architecture designed to scalably learn from data across heterogeneous embodiments. HPT modularizes a general policy network architecture (Figure 2) and pre-trains the _policy representation_ of a latent transformer with supervised learning. Inspired by learning from multimodal data , we use _embodiment-specific_ tokenizers, dubbed "stem", to align various sensor inputs such as camera views and proprioception inputs. The "trunk" is _shared_ and pre-trained across datasets and is transferred when adapting to new embodiments and tasks that are unknown during the pre-training times. Moreover, we use task-specific action decoders, dubbed "head", to produce the action outputs. Crucially, after "tokenizing each embodiment", HPT operates on a shared space of a short sequence of _latent tokens_. This hierarchy is motivated by how humans handle feedback loops between specific motor responses and perceived stimuli at the level of the spinal cord's neural circuitry .

We extensively investigated the scaling behaviors and various designs of policy pre-training to the extent of more than 50 individual data sources (2 times more than ) and model size of over 1 billion parameters. Analogous to the scaling laws , we found that to some extent, HPT scales with the dataset quantity and diversity as well as the model and training compute.

In addition, heterogeneity can occur in different embodiment domains, such as real robot hardware, simulation domains, and human videos. We incorporate many available embodied datasets in different embodiments such as real robots , simulation  and internet human videos  in the pre-training process and demonstrate the generality of our framework including embodiments beyond expensive real-world on-robot teleoperations.

Through transfer learning experiments across multiple simulation benchmarks  and real-world dexterous tasks, we compare with several baselines and the from-scratch counterparts. Overall, based on the pre-training objectives, HPT can scale with the model, data, compute, and the heterogeneity of the robotic datasets across real robots, simulations, and human videos. These pre

Figure 1: The **Heterogeneous Pre-training** concept. It maps different embodiments, each with its own _proprioception_ and _vision_ sensors, onto a _shared_ latent space by embodiment-specific tokenizers (“stems”). This _aligns_ the heterogeneous data from different embodiments into a joint representation space. This allows us to train a shared Transformer trunk on the union of all heterogeneous datasets. The pre-trained Transformer can be transferred to a new embodiment, with a small, new tokenizer learned at transferring time.

training procedures and models can simplify building reliable robotic policies for new embodiments and new tasks in terms of data requirements and generalized performance. As an attempt to scale heterogeneous pre-training, our code and weights are open-sourced, and we hope that HPT can shed some light on learning robot representations from heterogeneous embodiments and tasks.

## 2 Related Works

Pre-training and Transfer Learning.Pre-training , through direct supervision  and/or self-supervision [57; 25; 12; 22; 10], has been shown to learn representation useful for unseen downstream tasks in computer vision [7; 37] and natural language , and their intersections . The representation learned from ImagetNet  or web-scale data [38; 60; 18] shows robustness to distribution shifts and can be transferred to new tasks.

The recent surge of foundation models  scales these representation learning methods [27; 29] by applying task-agnostic objectives to multitask data. Moreover, recent works [46; 42; 45] show that small projection layers can be used to align the pre-trained feature spaces of the foundation models. Different from other fields, robotics has less data quantity and diversity but much more heterogeneity.

**Alignment.** Recent works such as Flamingo , Perceiver , and ImageBind  proposed ways to combine representations from _multimodal data_ such as image, language, and audio by aligning these different modalities to the same latent space in the pursuit of representation learning. Our architecture design is also motivated by methods such as LLAVA in the multimodal learning community. Very recently, GPT-4o , Gemini , MM1 , X-VILA , and Chameleon  demonstrated the capabilities of heterogeneous pre-training a universal transformer from and for multiple modalities. The idea of alignment, across modalities and/or embodiments, is important as we scale to use heterogeneous embodiments and reuse data from distinct embodiments.

Representation Learning in Robotics.Representation learning has been explored in the robotic community. Previous works such as R3M , VC-1, Voltron, and SpatialVLM  investigate visual representations by training the policy with human videos and robotic data . Recent works [61; 17; 4; 88; 71; 40] also align representations from multiple modalities and data distributions for robotic tasks. After pre-training, transfer learning with the frozen representation and/or finetuning is conducted in the target domains.

**Generalist Policies.** Large-scale policy learning in robotics has leveraged diverse data from real robots [6; 73], human videos [54; 49], and simulation domain [33; 63; 83; 80] separately. There are also works in multi-task learning [65; 66; 85; 23], meta-learning [79; 55; 19], few-shot learning , and fleet learning . Recently, RT-X, Octo, OpenVLA [6; 14; 56; 36] train generalist vision-language-action robotic policies on datasets from diverse robotic embodiments.

Compared with these works, HPT handles broader heterogeneity including proprioception and vision, explores scaling behaviors on more heterogeneous domains including real robots, human videos, and simulation data, and is evaluated at a larger scale in simulation benchmarks.

Figure 2: **HPT architecture.** HPT is modularized into stems, trunk, and heads. The stem, consisting of a proprioception tokenizer and a vision tokenizer, maps the vision and proprioception observations of different embodiments to a fixed number (e.g. 16) of tokens. The _shared_ trunk, which is a Transformer, maps the concatenated tokens into shared representations. The head then maps the processed tokens to actions in different downstream tasks. For a specific embodiment, one stem/head pair is activated (denoted by the switch). The trunk is shared and pre-trained on action-labeled data with supervised learning and then transferred to new embodiments. This procedure scales up to 52 datasets and 1B parameters.

Mixture of Experts.Our architecture design is related to works in conditional computation and MoE [51; 44; 72], where we create one expert for each embodiment, and the router (for the whole network) is determined by the embodiment. This technique has been used to scale language models to a substantial size .

## 3 Heterogeneous Pre-trained Transformers (HPT)

In _heterogeneous robot learning_ with cross embodiments, the data are generated from different domains such as simulation and real robots, across sensory modalities such as RGB images, language instructions, depth maps, 3D point clouds, and tactile images. Each robot is a unique hardware embodiment with varying degrees of freedom, end-effectors, sensor configurations, controller and action spaces, and application-specific physical setups.

In the following sections, we discuss the HPT network architecture and the training procedure to address the heterogeneity above. We modularize the network architecture (Figure 2) into the embodiment-specific stem, the shared trunk, and the task-specific heads. Intuitively, the stems, shown in Figure 3, are earlier layers of the neural network that align sensory inputs from heterogeneous embodiment and modalities into the shared representation space. The _shared_ middle part of the network is called the trunk, which processes the sensory representation into a latent representation that can be used for multiple tasks. Finally, the last part of the network is the head, which maps that latent representation to the action space in individual tasks of interest. The training procedure, dubbed _heterogeneous pre-training_, assigns and aligns specific stem/head pairs based on the sampled embodiment and task data, and still enjoys the benefits of joint training in the shared trunk. This can be thought of as tokenizing each embodiment using neural networks and alleviating the need to unify embodiments into a homogeneous data form in standard training procedures.

### Network Architecture

Stem.The stem \(_{}\) (Figure 3) in HPT is composed of a proprioceptive tokenizer and a vision tokenizer. These tokenizers map heterogeneous inputs from different embodiments to a _fixed number_ of tokens with _fixed dimensions_, which enables the trunk to treat them in the same manner despite large heterogeneity, as well as enjoy the scaling and inference benefits on fixed context length. The key idea is to leverage attention [78; 31; 9] to attend a fixed number of learnable tokens to features of the observations. Although we mainly focus on proprioception and vision, handling other kinds of sensor heterogeneity in tactile, 3D, and action inputs can be flexibly extended in stems.

* **Proprioception Tokenizers.** In Figure 3 (left), for embodiment \(k\), the proprioceptive tokenizer maps any sequence of robot proprioceptive information with dimension \(d_{p}^{k}\) (e.g. 7 for end-effector pose) into \(N_{p}\) (e.g. \(N_{p}=16\)) tokens with dimension \(d\) with values ranging from 128 to 1024. To achieve this, we first use an MLP to map the proprioceptive input into a feature space with dimension \(d\). We then apply sinusoidal position encoding and use attention across the state feature and the learnable tokens, to map into \(16\) tokens with dimension \(d\). Proprioceptive information is critical in robot policy learning, but its usage is often as simple as feature concatenation with a vision encoder .

Figure 3: **Stem Architecture in HPT. In the HPT stem, the proprioceptive tokenizer uses an MLP to map proprioceptive information to a feature which is then attended by 16 learnable tokens. The vision tokenizer uses pre-trained encoders and similarly uses an attention mechanism to map vision features into 16 fixed tokens. The architecture flexibly handles sequences of inputs without increasing the size of tokens.**

* **Vision Tokenizers.** In Figure 3 (right), the vision tokenizer can map any sequence of camera images (videos of multiple views) with dimension \(H W 3\) into \(N_{v}\) (we use \(N_{v}=16\) by default) tokens with dimension \(d\). To achieve this, we first use _pre-trained frozen feature networks_ (e.g. 7 by 7 features from ResNet) and then flatten the features. After that, we again use attention across these features and learnable tokens, to map the vision input into \(16\) tokens with dimension \(d\).

After processing each modality individually in the time sequence order, we concatenate all modality tokens and add additional modality embeddings and sinusoidal positional embeddings. This is used as the input sequence to the trunk that we introduce below. To avoid overfitting, the stem only has a small number of parameters (one MLP and one attention layer).

Related works such as Octo  and others [54; 49; 6] mostly focus on pre-training the vision backbone of the policy through masking or self-supervision. They often stack sequences of single-view images along channels  for a particular robot or use a large number of tokens (256 in ). In contrast, HPT uses stems with pre-trained vision encoders to map arbitrary image sequences to a short sequence of tokens (16). Moreover, rather than _add in_ proprioception during transfer in related works, HPT _jointly pre-trains_ the vision and proprioception parts, from heterogeneous datasets.

Trunk.As the central component for pre-training, the trunk architecture follows a transformer, parametrized by \(^{}\) in the latent space with dimension \(d\). The output token sequence length \(L\) is the same as the input token sequence length. The output token sequence is simply pooled as the final combined feature for the observation. The trunk is shared across different embodiments and tasks to capture the complex input-output relationships (i.e. the number of trunk parameters is fixed independent of the number of embodiments and tasks).

Head.The policy head \(_{}\) takes the output of the trunk transformer and maps it to the action space \(\) in each dataset. For each embodiment and task, the policy head can be an arbitrary architecture (e.g. MLP) that takes as input the pooled feature of the trunk and outputs a normalized action trajectory. The policy head is reinitialized for transferring to a new embodiment.

### Training Objective

Given a total of \(K\) datasets with heterogeneous embodiments sampled from different distributions \(_{1},...,_{k},...,_{K}\), we let \(_{k}=\{^{(i)}\}_{1 i M_{k}}\) denote a set of \(M_{k}\) trajectories in dataset \(_{k}\), with \(^{(i)}=\{o_{t}^{(i)},a_{t}^{(i)}\}_{1 t T}\) denoting the \(i\)-th trajectory of maximum length \(T\) of observation and action tuples. The objective is to minimize the following loss across datasets

\[_{}\ \ _{k=1}^{K}\ (_{k}^{},^{ },_{k}^{};_{k}).\] (1)

    & **\# Depth** & **\# Width** & **\# Attn. Heads** & **\# Param.** \\ 
**HPT-Small** & 16 & 128 & 8 & 3.1M \\
**HPT-Base** & 16 & 256 & 8 & 12.6M \\
**HPT-Large** & 16 & 512 & 8 & 50.5M \\
**HPT-XLarge** & 32 & 768 & 16 & 226.8M \\
**HPT-Huge** & 80 & 1024 & 16 & 1.1B \\   

Table 1: **Network Details of HPT.** The width denotes the latent dimension size of the trunk transformer and the depth denotes the number of blocks. The _default_ setup is the HPT-Small model.

    & **\# Dataset** & **\# Traj.** & **\# Samples** & **\# Batch Size** \\ 
**Default** & 27 & 16k & 5M & 256 \\
**Scaled** & 52 & 270k & 155M & 2048 \\   

Table 2: **Dataset Details of Pre-train Settings.** The _default_ setup is trained with 27 datasets from RT-X with 16k trajectories (maximum 1000 trajectories each) and _scaled_ setup involves more data and compute.

Figure 4: **Dataset Heterogeneity in Robotics.** We show illustrations of dataset mixtures (each color is a distinct embodiment) from different domains including real robot teleop , deployed robots , simulations, and human videos . See Appendix Section A for dataset mixture details.

\(\) is behavior cloning loss computed as the Huber loss between the normalized action labels based on dataset statistics and the network's action predictions. \(=_{k=1}^{K}\{_{k}^{},_{k}^{}\} ^{}\) denotes the network parameters comprised of embodiment-specific stem and head \(_{k}^{},_{k}^{}\) for dataset \(k\), and a single set of shared trunk parameters \(_{}\) across all embodiments. This training procedure has two axes of data scaling: the quantity \(M_{k}\) for one dataset \(D_{k}\) and the total number of datasets \(K\). In the pre-training stage, only the trunk parameters are updated at every iteration, and the stems and heads for each heterogeneous embodiment and task are updated based on the training batch sampling. See implementation details in Appendix Section A.3.

### Transfer Learning

The policy transfer process is similar to aligning the features of the new domain (through pre-trained stem encoders) to the pre-trained embedding space of the trunk [42; 45]. Given a new dataset \(_{K+1}\) from a new embodiment, the objective can be the same as pre-training or alternatives . We reinitialize the head and stem parameters with embodiment-specific input and output dimensions (such as different proprioception and action dimensions), and freeze the weights of the trunk.

## 4 Experiments on Pre-training

In this section, we aim to answer the following question: Does HPT pre-training have a _scaling behavior_ under heterogeneous data across domains?

**Default Setting.** We use 27 robot teleoperation datasets, including a subset of the recently public Open-X Embodiment dataset  as the training corpus. By default, we use one camera view of the scene with the pre-trained frozen ResNet18 image encode to compute the vision features. We use proprioception information, such as end-effector poses and joint positions, whenever they are available and provided. We use a maximum of 1000 trajectories from each dataset and a total number of 16k trajectories, and a held-out validation dataset with a maximum 200 trajectories per data source. Furthermore, we use a model with a trunk size of 3.17 million parameters, which is denoted as HPT-Small (Table 1). The training uses a batch size of 256 for 80k iterations, which is around 0.65B tokens in the latent space that feeds into HPTs and around 5B tokens in the vision and proprioception token spaces (horizon-dependent). While we do not align or preprocess action space or observation space [56; 87] other than normalization, data cleanup and filtering would be very helpful.

**Scaled Setting.** We use 200k trajectories with 52 datasets, including simulation (e.g. ), deployed robots (e.g. ), human videos (e.g. ), from distinct embodiments in the training process. This includes many public and accessible robotic datasets. In addition to different tasks in different institutes, these heterogeneous mixtures of datasets (Fig. 4 and Fig. 13) come with multiple views, language inputs, and different observation inputs in different environments.

### Protocol

We evaluate the HPT pre-training performance with the _averaged validation loss_ (prediction errors on unseen trajectories) at the last iteration of pre-training. These validation datasets are fixed independent of the trajectory counts and models during training. Unless particularly noted, the validation datasets come from the same 27 datasets in the _Default Setting_. Note that it is unrealistic to evaluate the pre-trained models on many real-world robotic environments at scale and there are very few evaluation alternatives to measure large-scale pre-training if we ignore this objective. In fields such as NLP[30; 34], training loss objective (e.g. perplexity) is often used to measure the progress of pre-training. Admittedly, there are several caveats to this metric including the closed-loop performance gap and the task success rate gap. We will address these issues in Section 5 on HPT transfer learning. See Appendix Section A and Section D for more details and discussions.

### Scaling Behaviors

**Data Scaling.** In Figure 5 (a), we observe stable and scaling validation losses even on increasingly heterogeneous embodiments. Moreover, we found the compute (e.g. samples seen per training run) and the data amounts needed to scale in tandem  to get closer to convergence in the training process. In the red line in Figure 5 (a), we observe better validation losses as we scale up the total number of trajectories, by using a larger model and doubling the batch size every order of magnitude increase in trajectory counts. Strictly increasing data while keeping others bottlenecked (HPT-Sand fixed iterations) might cause an early plateau performance at around 1000 trajectories max per dataset, as shown in the blue line in Figure 5. In Figure 5 (b), we also pre-train on an increasing number of datasets with a fixed number of epochs and evaluate on the fixed subset (first 10 datasets). We hypothesize that training with more embodiments contributes to the generalization of the trunk. These experiments can scale to the extent of 200k trajectories and 52 datasets.

**Model Scaling.** In Figure 7, we fix the number of datasets (27) in RT-X and use a maximum of 1000 trajectories for each dataset. We scale along model size (from 1M to 1B) and gradually increase the batch sizes from 256 to 2048 (doubles every order of model size increase) and use the larger dataset with 170k trajectories. We observe that when we scale to bigger models with larger amounts of compute (red line), the pre-training can achieve low validation losses until it is plateaued. We do not find a significant difference between scaling depth or scaling width.

**Epoch Scaling.** In this experiment, we fix the number of datasets (27) and use a maximum of 1000 trajectories for each dataset. In Figure 6, we observe that increasing batch sizes (Left), which effectively scales training tokens (Right), can generally improve the model performance until convergence. Another observation we have is to use distributed workers to load from as many datasets as possible to aggregate each batch. We hypothesize that the large variance of training on heterogeneous datasets can be reduced by using a large batch size. See Appendix B for more experiment details.

### Pre-training on Synthetic Data and Internet Human Videos

We experiment beyond real-world robot teleop data, which is expensive to collect and scale. For the additional datasets, we consider 7 simulation datasets across many popular simulators Drake , Mujoco [90; 50], Isaac Sim , and PyBullet [86; 81], as well as Sapien  and Flex , with

Figure 5: **Data Scaling**. We run scaling HPT experiments along dataset sizes and the number of datasets. Each point is the validation loss of a full training run. (a) We evaluate the losses on 27 datasets with the number of total trajectories ranging from a maximum of 10 trajectories per dataset (270 in total) to a maximum of 10000 trajectories per dataset (170k in total). We compare two model sizes, HPT-S/L, where HPT-L is a bigger model trained with 4 times more tokens than HPT-S. (b) We compute the validation losses for a fixed subset of 10 datasets with a fixed number of epochs (2). We compute mean and standard deviations for 4 runs across model sizes from HPT-S to HPT-XL and across dataset counts from 10 to 52.

Figure 6: **Epoch Scaling**. We run scaling HPT experiments along the number of total samples. Each point is the validation loss of a full pre-training run. Setting: HPT-S, 27 datasets with a maximum of 1000 trajectories for each dataset. Left) We scale up the number of batch sizes and measure the changes in validation losses. Right) Derived from the left figure, we multiply the batches seen by the number of samples in each batch.

image inputs and expert demonstrations. For the human datasets that lack proprioception and action information, we use poses and 2D positions as surrogates for the supervised policy learning objectives. We use in total 300 trajectories from EPIC kitchen  and PoCo  with a maximum trajectory length 1000. See Appendix Figure 13 and Table 4 for more details on the dataset compositions.

In Figure 8, we use a maximum of 1000 trajectories for each dataset and compare against the baseline of 27 datasets with evaluation on all the pre-trained datasets. We show that pre-training on additional embodiment datasets such as simulation and human video datasets can be possible, despite the large embodiment gaps with real robots. These datasets provide complimentary embodiment data to pure teleop data, and they illustrate how much heterogeneity can be handled in the HPT framework.

## 5 Experiments on Transfer Learning

In the previous section, we evaluate pre-training using the validation losses. In this section, we answer the following question with task success rates in transfer learning: Can the pre-trained HPT model be transferred to new embodiments, tasks, and environments in simulation and the real world?

### Transfer to Embodiments in Simulations

Protocol.We evaluate the pre-trained representations on robot manipulation simulation benchmarks Meta-world , RoboMimic , and Fleet-Tools . Each training dataset uses from 20-100 trajectories per task and each testing covers 50 episodes with different initial conditions. The policies use HPT-Small as the pre-trained trunk and reinitialize the stem and head for transferring.

During the evaluation phase, we compare the following models: No Trunk uses only the stem and head without the trunk in the middle and trains from scratch as common practice . From Scratch trains the entire policy from scratch with the trunk, Pretrained Frozen uses and freezes the pre-trained trunk during transfer learning and Pretrained Finetuned loads the pre-trained HPT-Base trunk and finetunes the whole network end-to-end, and Pretrained Finetuned (HPT-XL) uses the same fine-tuning procedure with a pre-trained HPT-XL trunk with a lower pre-training validation loss. To reduce the variance, we conduct independent training runs and evaluations 5 times and average for each model. The inference time during transfer on an RTX 3070 GPU is 47Hz for HPT-base and 19Hz for HPT-XL, while a more recent GPU like A100 can be 3-4 times faster.

Experiment.In Figure 10 (a), we test the model on the downstream tasks in closed-loop simulation and observe improved task success rate using the pre-trained models ranging from HPT-B to HPT-XL, although pre-training for the simulation experiments only happens in the real-world embodiments.

In Figure 10 (b), we run HPT on the recently released Simpler  Benchmark, which allows for comparing with Octo , RT1-X, and RT2-X  on a high-fidelity simulation. We focus on three different tasks Close Drawer, Move Near, and Pick Coke Can in the Google EDR embodiment. For each task, we test several different initializations with a total of over 300 episodes for all tasks.

Figure 8: **Joint Pre-training with Simulation and Human Videos. The baseline denotes the default setting without simulation and human datasets. Setting: We run the experiments with a training corpus of datasets with 1000 trajectories maximum.**

Figure 7: **Model Scaling. We run scaling HPT experiments along model sizes. Each point is a full training run. Setting: 27 datasets with a maximum of 1000 trajectories for each dataset. We scale along model size (from 1M to 1B) for both the blue and red lines. The red line is trained with increasing data and epochs to reach convergence. Specifically, we gradually increase the batch sizes from 256 to 2048 (doubles every order of model size increase) and use 170k trajectories.**

Note that the pre-training corpus of HPT-S does not include , and simulation tasks have a focus on language conditioning and do not expose proprioception inputs, which is not suitable for HPT. To address these issues, we finetune HPT on the supervised datasets with around 50 trajectories under the simulation protocol. We use HPT-base as the backbone for this experiment. We use the baseline results from . See Section A.4 for more implementation and experiment details.

### Transfer to Embodiments in the Real World

Protocol.For the real-world experiments, we evaluate the HPTs on two different embodiments for tasks in pet care and assembly, which are not covered in the pre-training datasets . In particular, for these two robots, we experiment with different observation spaces 1 camera v.s. 2 cameras as well as different action spaces relative pose v.s. absolute pose. For data collection, we experiment with both an Oculus Quest to collect relative pose control as action labels as well as kinesthetic teaching. The episode lengths of real-world teleoperation vary from 50 steps to 150 steps with 10 Hz control frequencies. We experiment with the tasks Sweep Leftover, Fill Water, Scoop Food and Switch Insertion, which require 5-20 seconds of interactions with granular or small objects with fine contacts, shown in Figure 11. We collect around 100 demos for each task and evaluate them for 15 trials to measure the average success rate.

Experiment.We adopt a similar transfer learning method in the previous section and evaluate the pre-trained HPT representations under real-world evaluation protocols. We train the policy with 20000 iterations with a batch size of 256 and a learning rate of \(5e^{-6}\). We defer implementation details to Appendix A.5. Quantitatively in Figure 12, we observe pre-trained policies attain a better success rate over the No-Trunk and the From-Scratch baselines. In particular, the From-Scratch baselines in Fill-Water use the state-of-the-art diffusion policy architecture to illustrate the flexibility of the pre-trained representations. In Figure 11, qualitatively, we observe better generalization and robustness to varying poses and numbers of granular objects, and varying camera configurations and lighting conditions with pre-trained HPT.

On Table 3, we perform an ablation study for the Sweep Leftover task. We also compare with R3M , Voltron , and VC-1 . We use a finetuned model with the released backbone and weights. We note that these previous works focus on only pre-training the vision encoders of the policies with human videos. Finally, we compared with policies that train from scratch (From Scratch) and policies that do not use proprioception during pre-training (No Prop. Finetuned) and add in proprioception afterwards. All of our experiments use pre-trained encoders and the trainable parameters (stem and head) can be as few as 2% of the parameters.

Figure 10: **Success Rates in Simulation Experiments.** (a) We evaluate transfer learning performance of models from HPT-B to HPT-XL on tasks across 4 different simulator benchmarks. (b) We compare with several generalist models in the recent Simpler  benchmark with Google GDR embodiment. The pre-trained trunks are trained from the Scaled Settings. The success rates are computed over 150 rollouts per approach.

Figure 9: **Simulation Evaluation Tasks.** We evaluate HPT across several simulation benchmarks and show policy rollout visualizations of the experiments. Experiment details can be found in Section 5.1 and A.4.

## 6 Conclusion

There is room for improvement for many aspects including the dataset curation and pre-training objectives. Specifically, the embodiment splits in our balanced dataset mixture are rather simple. Moreover, careful data filtering to ensure the data quality is under-explored in this work. Also, this work has focused on supervised learning as the pre-training objective and the data size in tokens and training compute sizes in FLOPs only reach a moderate scale of LLM training to ensure full convergence. Although the model architecture and training procedure are modular and independent of embodiment setups, heterogeneous pre-training can converge slowly. For evaluation, both the simulation and real-world evaluation tasks are restricted to short-horizon manipulation tasks with a fixed embodiment, which might limit the benefits of using a higher-capacity model. Furthermore, the learned policies still do not offer very high reliability on the tested tasks (typically below 90%). See Appendix SSC for some failure modes.

Given the recent surge of scaled data, robot learning is still limited by its generality because of the heterogeneity, including different embodiments, tasks, and environments where the robots are operated. To handle the heterogeneity common in robotics, we propose HPT, a modular architecture and framework to embrace this heterogeneity through pre-training. We explore and scale HPT with heterogeneous datasets to over 50 available datasets. The learned representation can be transferred and improve performance in both simulation and the real world, and it shows correlations with pre-training performance. The code2 is open-source for future research. We hope this perspective will inspire future work in handling the _heterogeneous nature_ of robotic data for robotic foundation models.

Acknowledgement.We would like to thank Russ Tedrake for discussions and suggestions, Liane Xu for helping with real-world experiments, Tianhong Li for helping with cluster experiments, and Remi Cadene for helping with the LeRobot implementation. We thank MIT Supercloud for providing computing resources to process training data. This work is supported in part by the Amazon Greater Boston Tech Initiative and Amazon PO No. 2D-06310236. Toyota Research Institute provided funds to partially support this work.

Figure 11: **Real World Qualitative Results**. Pre-trained HPT policies can perform dynamic and long-horizon contact-rich precision tasks in pet care and assembly. The policies show robust and generalized behaviors under scene changes and disturbances.