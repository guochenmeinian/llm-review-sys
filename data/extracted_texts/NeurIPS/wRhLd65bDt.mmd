# Improving Diffusion-Based Image Synthesis with Context Prediction

Ling Yang\({}^{1}\)  Jingwei Liu\({}^{1}\)  Shenda Hong\({}^{1}\)  Zhilong Zhang\({}^{1}\)  Zhilin Huang\({}^{2}\)

Zheming Cai\({}^{1}\)  Wentao Zhang\({}^{1}\)  Bin Cui\({}^{1}\)

\({}^{1}\)Peking University \({}^{2}\) Tsinghua University

yangling0818@163.com, jingweiliu1996@163.com, zhilong.zhang@bjmu.edu.cn

{hongshenda, wentao.zhang, bin.cui}@pku.edu.cn

Contact: Ling Yang, yangling0818@163.com.

Footnote â€ : 1}\)Peking University \({}^{2}\) Tsinghua University

###### Abstract

Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, _context_ has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (_i.e._, multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by preserving its semantic connections with neighborhood context. This new paradigm of ConPreDiff can generalize to arbitrary discrete and continuous diffusion backbones without introducing extra parameters in sampling procedure. Extensive experiments are conducted on unconditional image generation, text-to-image generation and image inpainting tasks. Our ConPreDiff consistently outperforms previous methods and achieves a new SOTA text-to-image generation results on MS-COCO, with a zero-shot FID score of 6.21.

## 1 Introduction

Recent diffusion models  have made remarkable progress in image generation. They are first introduced by Sohl-Dickstein et al.  and then improved by Song & Ermon  and Ho et al. , and can now generate image samples with unprecedented quality and diversity . Numerous methods have been proposed to develop diffusion models by improving their empirical generation results  or extending the capacity of diffusion models from a theoretical perspective . We revisit existing diffusion models for image generation and break them into two categories, pixel- and latent-based diffusion models, according to their diffusing spaces. Pixel-based diffusion models directly conduct continuous diffusion process in the pixel space, they incorporate various conditions (_e.g._, class, text, image, and semantic map)  or auxiliary classifiers  for conditional image generation.

On the other hand, latent-based diffusion models  conduct continuous or discrete diffusion process  on the semantic latent space. Such diffusion paradigm not only significantly reduces thecomputational complexity for both training and inference, but also facilitates the conditional image generation in complex semantic space [62; 38; 58; 19; 98]. Some of them choose to pre-train an autoencoder [41; 64] to map the input from image space to the continuous latent space for continuous diffusion, while others utilize a vector quantized variational autoencoder [88; 17] to induce the token-based latent space for discrete diffusion [24; 75; 114; 85].

Despite all these progress of pixel- and latent-based diffusion models in image generation, both of them mainly focus on utilizing a point-based reconstruction objective over the spatial axes to recover the entire image in diffusion training process. This point-wise reconstruction neglects to fully preserve local context and semantic distribution of each predicted pixel/feature, which may impair the fidelity of generated images. Traditional non-diffusion studies [15; 45; 32; 50; 110; 8] have designed different _context_-preserving terms for advancing image representation learning, but few researches have been done to constrain on context for diffusion-based image synthesis.

In this paper, we propose ConPreDiff to explicitly force each pixel/feature/token to predict its local neighborhood context (_i.e._, multi-stride features/tokens/pixels) in image diffusion generation with an extra context decoder near the end of diffusion denoising blocks. This explicit _context prediction_ can be extended to existing discrete and continuous diffusion backbones without introducing additional parameters in inference stage. We further characterize the neighborhood context as a probability distribution defined over multi-stride neighbors for efficiently decoding large context, and adopt an optimal-transport loss based on Wasserstein distance  to impose structural constraint between the decoded distribution and the ground truth. We evaluate the proposed ConPreDiff with the extensive experiments on three major visual tasks, unconditional image generation, text-to-image generation, and image inpainting. Notably, our ConPreDiff consistently outperforms previous diffusion models by a large margin regarding generation quality and diversity.

Our main contributions are summarized as follows: **(i):** To the best of our knowledge, we for the first time propose ConPreDiff to improve diffusion-based image generation with context prediction; **(ii):** We further propose an efficient approach to decode large context with an optimal-transport loss based on Wasserstein distance; **(iii):** ConPreDiff substantially outperforms existing diffusion models and achieves new SOTA image generation results, and we can generalize our model to existing discrete and continuous diffusion backbones, consistently improving their performance.

## 2 Related Work

Diffusion Models for Image GenerationDiffusion models [99; 76; 78; 28] are a new class of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation. They can generate image samples with unprecedented quality and diversity [24; 68; 67], and have been applied in various applications [99; 9; 6]. Existing pixel- and latent-based diffusion models mainly utilize the discrete diffusion [30; 1; 24] or continuous diffusion [87; 65] for unconditional or conditional image generation [80; 14; 27; 54; 40; 68]. Discrete diffusion models were also first described in , and then applied to text generation in Argmax Flow . D3PMs  applies discrete diffusion to image generation. VQ-Diffusion  moves discrete diffusion from image pixel space to latent space with the discrete image tokens acquired from VQ-VAE . Latent Diffusion Models (LDMs) [87; 65] reduce the training cost for high resolution images by conducting continuous diffusion process in a low-dimensional latent space. They also incorporate conditional information into the sampling process via cross attention . Similar techniques are employed in DALLE-2  for image generation from text, where the continuous diffusion model is conditioned on text embeddings obtained from CLIP latent codes . Imagen  implements text-to-image generation by conditioning on text embeddings acquired from large language models (_e.g._, T5 ). Despite all this progress, existing diffusion models neglect to exploit rich neighborhood context in the generation process, which is critical in many vision tasks for maintaining the local semantic continuity in image representations [111; 45; 32; 50]. In this paper, we firstly propose to explicitly preserve local neighborhood context for diffusion-based image generation.

Context-Enriched Representation Learning_Context_ has been well studied in learning representations, and is widely proved to be a powerful automatic supervisory signal in many tasks. For example, language models [52; 13] learn word embeddings by predicting their context, _i.e._, a few words before and/or after. More utilization of contextual information happens in visual tasks, where spatial context is vital for image domain. Many studies [15; 111; 45; 32; 50; 110; 8; 106; 95; 94; 94] propose to leverage context for enriching learned image representations. Doersch et al.  and Zhang et al.  make predictions from visible patches to masked patches to enhance the self-supervised image representation learning. Hu et al.  designs local relation layer to model the context of local pixel pairs for image classification, while Liu et al.  preserves contextual structure to guarantee the local feature/pixel continuity for image inpainting. Inspired by these studies, in this work, we propose to incorporate neighborhood context prediction for improving diffusion-based generative modeling.

## 3 Preliminary

Discrete DiffusionWe briefly review a classical discrete diffusion model, namely Vector Quantized Diffusion (VQ-Diffusion) . VQ-Diffusion utilizes a VQ-VAE to convert images \(x\) to discrete tokens \(x_{0}\{1,2,...,K,K+1\}\), \(K\) is the size of codebook, and \(K+1\) denotes the \(\) token. Then the forward process of VQ-Diffusion is given by:

\[q(x_{t}|x_{t-1})=^{}(x_{t})}(x_{t-1})\] (1)

where \((x)\) is a one-hot column vector with entry 1 at index \(x\). And \(}\) is the probability transition matrix from \(x_{t-1}\) to \(x_{t}\) with the mask-and-replace VQ-Diffusion strategy. In the reverse process, VQ-Diffusion trains a denoising network \(p_{}(_{t-1}|_{t})\) that predicts noiseless token distribution \(p_{}(}_{0}|_{t})\) at each step:

\[p_{}(_{t-1}|_{t})=_{}_{0}=1}^{K}q(_{ t-1}|_{t},}_{0})p_{}(}_{0}|_{t}),\] (2)

which is optimized by minimizing the following variational lower bound (VLB) :

\[_{t-1}^{dis}=D_{KL}(q(_{t-1}|_{t},_{0})\;||\;p_{ }(_{t-1}|_{t})).\] (3)

Continuous DiffusionA continuous diffusion model progressively perturbs input image or feature map \(_{0}\) by injecting noise, then learn to reverse this process starting from \(_{T}\) for image generation. The forward process can be formulated as a Gaussian process with Markovian structure:

\[ q(_{t}|_{t-1})&:=(_{t};}_{t-1},_{t}),\\ q(_{t}|_{0})&:=(_{t };_{t}}_{0},(1-_{t})),\] (4)

where \(_{1},,_{T}\) denotes fixed variance schedule with \(_{t}:=1-_{t}\) and \(_{t}:=_{s=1}^{t}_{s}\). This forward process progressively injects noise to data until all structures are lost, which is well approximated by \((0,)\). The reverse diffusion process learns a model \(p_{}(_{t-1}|_{t})\) that approximates the

Figure 1: In training stage, ConPreDiff first performs self-denoising as standard diffusion models, then it conducts neighborhood context prediction based on denoised point \(_{t-1}^{i}\). In inference stage, ConPreDiff only uses its self-denoising network for sampling.

true posterior:

\[p_{}(_{t-1}|_{t}):=(_{t-1};_{}(_{t}),_{}(_{t})),\] (5)

Fixing \(_{}\) to be untrained time dependent constants \(_{t}^{2}\), Ho _et al._ improve the diffusion training process by optimizing following objective:

\[_{t-1}^{con}=_{t}|_{t-1})}{} [^{2}}||_{}(_{t},t)-(_{ t},_{0})||^{2}]+C,\] (6)

where \(C\) is a constant that does not depend on \(\). \((_{t},_{0})\) is the mean of the posterior \(q(_{t-1}|_{0},_{t})\), and \(_{}(_{t},t)\) is the predicted mean of \(p_{}(_{t-1}_{t})\) computed by neural networks.

## 4 The Proposed ConPreDiff

In this section, we elucidate the proposed ConPreDiff as in Figure 1. In Sec. 4.1, we introduce our proposed context prediction term for explicitly preserving local neighborhood context in diffusion-based image generation. To efficiently decode large context in training process, we characterize the neighborhood information as the probability distribution defined over multi-stride neighbors in Sec. 4.2, and theoretically derive an optimal-transport loss function based on Wasserstein distance to optimize the decoding procedure. In Sec. 4.3, we generalize our ConPreDiff to both existing discrete and continuous diffusion models, and provide optimization objectives.

### Neighborhood Context Prediction in Diffusion Generation

We use unconditional image generation to illustrate our method for simplicity. Let \(_{t-1}^{i}^{d}\) to denote \(i\)-th pixel of the predicted image, \(i\)-th feature point of the predicted feature map, or \(i\)-th image token of the predicted token map in spatial axes. Let \(_{i}^{s}\) denote the \(s\)-stride neighborhoods of \(_{t-1}^{i}\), and \(K\) denotes the total number of \(_{i}^{s}\). For example, the number of 1-stride neighborhoods is \(K=8\), and the number of 2-stride ones is \(K=24\).

\(S\)**-Stride Neighborhood Reconstruction** Previous diffusion models make point-wise reconstruction, _i.e._, reconstructing each pixel, thus their reverse learning processes can be formulated by \(p_{}(_{t-1}^{i}|_{t})\). In contrast, our context prediction aims to reconstruct \(_{t-1}^{i}\) and further predict its \(s\)-stride neighborhood contextual representations \(_{_{i}^{s}}\) based on \(_{t-1}^{i}\): \(p_{}(_{t-1}^{i},_{_{i}^{s}}|_{t}),\) where \(p_{}\) is parameterized by two reconstruction networks (\(_{p}\),\(_{n}\)). \(_{p}\) is designed for the point-wise denoising of \(_{t-1}^{i}\) in \(_{t}\), and \(_{n}\) is designed for decoding \(_{_{i}^{s}}\) from \(_{t-1}^{i}\). For denoising \(i\)-th point in \(_{t}\), we have:

\[_{t-1}^{i}=_{p}(_{t},t),\] (7)

where \(t\) is the time embedding and \(_{p}\) is parameterized by a U-Net or transformer with an encoder-decoder architecture. For reconstructing the entire neighborhood information \(_{_{i}^{s}}\) around each point \(_{t-1}^{i}\), we have:

\[_{_{i}^{s}}=_{n}(_{t-1}^{i},t)=_{n}(_{p}( _{t},t)),\] (8)

where \(_{n}^{Kd}\) is the neighborhood decoder. Based on Equation (7) and Equation (8), we unify the point- and neighborhood-based reconstruction to form the overall training objective:

\[_{}}=_{i=1}^{x y}[_{p}(_{t-1}^{i},}^{i})}_{point\ denoising}+_{n}(_{_{i}^{s}},}_{_{i}^{s }})}_{context\ prediction}],\] (9)

where \(x,y\) are the width and height on spatial axes. \(}^{i}\) (\(}_{0}^{i}\)) and \(}_{_{i}^{s}}\) are ground truths. \(_{p}\) and \(_{n}\) can be Euclidean distance. In this way, ConPreDiff is able to maximally preserve local context for better reconstructing each pixel/feature/token.

Interpreting Context Prediction in Maximizing ELBOWe let \(_{p},_{n}\) be square loss, \(_{n}(_{_{i}^{s}},}_{_{i}^{s }})=_{j_{i}}(_{0}^{i,j}-}_{0}^{i,j})^{2},\) where \(}_{0}^{i,j}\) is the j-th neighbor in the context of \(}_{0}^{i}\) and \(_{0}^{i,j}\) is the prediction of \(_{0}^{i,j}\) from a denoising neural network. Thus we have:

\[_{0}^{i,j}=_{n}(_{p}(_{t},t)(i))(j).\] (10)Compactly, we can write the denoising network as:

\[(x_{t},t)(i,j)=_{n}(_{p}(_{t},t)(i))(j), j _{i},\\ _{p}(_{t},t)(i), j=i.\] (11)

We will show that the DDPM loss is upper bounded by ConPreDiff loss, by reparameterizing \(_{0}(_{t},t)\). Specifically, for each unit \(i\) in the feature map, we use the mean of predicted value in its neighborhood as the final prediction:

\[_{0}(_{t},t)(i)=1/(|_{i}|+1)*_{j_{i} \{i\}}(_{t},t)(i,j).\] (12)

Now we can show the connection between the DDPM loss and ConPreDiff loss:

\[||}_{0}-_{0}(_{t},t)||_{2}^{2} =_{i}(}_{0}^{i}-_{0}(_{t},t)(i))^{2},\] (13) \[=_{i}(}_{0}^{i}-_{j_{i}\{i \}}(_{t},t)(i,j)/(|_{i}|+1))^{2},\] \[=_{i}(_{j_{i}\{i\}}((_{t},t)( i,j)-}_{0}^{i}))^{2}/(|_{i}|+1)^{2},\] \[() _{i}_{j_{i}\{i\}}((_{t},t )(i,j)-}_{0}^{i})^{2}/(|_{i}|+1),\] \[=1/(|_{i}|+1)_{i}[(}_{0}^{i}-_{p}( _{t},t)(i))^{2}+_{j_{i}}(}_{0}^{i,j}-_{0}^{i,j})^{2}]\]

In the last equality, we assume that the feature is padded so that each unit \(i\) has the same number of neighbors \(||\). As a result, the ConPreDiff loss is an upper bound of the negative log likelihood.

Complexity ProblemWe note that directly optimizing the Equation (9) has a complexity problem and it will substantially lower the efficiency of ConPreDiff in training stage. Because the network \(_{n}:^{d}^{Kd}\) in Equation (8) needs to expand the channel dimension by \(K\) times for large-context neighborhood reconstruction, it significantly increases the parameter complexity of the model. Hence, we seek for another way that is efficient for reconstructing neighborhood information.

We solve the challenging problem by changing the direct prediction of entire neighborhoods to the prediction of neighborhood distribution. Specifically, for each \(_{t-1}^{i}\), the neighborhood information is represented as an empirical realization of i.i.d. sampling \(Q\) elements from \(_{_{i}^{}}\), where \(_{_{i}^{}}_{u_{i}^{}}_{h_{u}}\). Based on this view, we are able to transform the neighborhood prediction \(_{n}\) into the neighborhood distribution prediction. **However, such sampling-based measurement loses original spatial orders of neighborhoods, and thus we use a permutation invariant loss (Wasserstein distance) for optimization**. Wasserstein distance  is an effective metric for measuring structural similarity between distributions, which is especially suitable for our neighborhood distribution prediction. And we rewrite the Equation (9) as:

\[_{}}=_{i=1}^{x y}[ {_{p}(_{t-1}^{i},}^{i})}_{point\ denoising}+_{2}^{2}(_{n}(_{t-1}^{i},t), _{_{i}^{}})}_{neighborhood\ distribution\ prediction}],\] (14)

where \(_{n}(_{t-1}^{i},t)\) is designed to decode neighborhood distribution parameterized by feedforward neural networks (FNNs), and \(_{2}(,)\) is the 2-Wasserstein distance as defined below. We provide a more explicit formulation of \(_{2}^{2}(_{n}(_{t-1}^{i},t),_{_{i}^ {}})\) in Sec. 4.2.

**Definition 4.1**.: Let \(,\) denote two probability distributions with finite second moment defined on \(^{m}\). The \(2\)-Wasserstein distance between \(\) and \(\) defined on \(,^{}^{m}\) is the solution to the optimal mass transportation problem with \(_{2}\) transport cost :

\[_{2}(,)=(_{(,)}_{^{}}\|Z-Z^{}\|_{2}^{2}d (Z,Z^{}))^{1/2}\] (15)

where \((,)\) contains all joint distributions of \((Z,Z^{})\) with marginals \(\) and \(\) respectively.

### Efficient Large Context Decoding

Our ConPreDiff essentially represents the node neighborhood \(}_{^{s}_{t}}\) as a distribution of neighbors' representations \(_{^{s}_{t}}\) (Equation (14)). We adopt Wasserstein distance to characterize the distribution reconstruction loss because \(_{^{s}_{t}}\) has atomic non-zero measure supports in a continuous space, where the family of \(f\)-divergences such as KL-divergence cannot be applied. Maximum mean discrepancy may be applied but it needs to choose a specific kernel function.

We define the decoded distribution \(_{n}(^{i}_{t-1},t)\) as an FNN-based transformation of a Gaussian distribution parameterized by \(^{i}_{t-1}\) and \(t\). The reason for choosing this setting stems from the fact that the universal approximation capability of FNNs allows to (approximately) reconstruct any distributions in 1-Wasserstein distance, as formally stated in Theorem 4.2, proved in Lu & Lu . To enhance the empirical performance, our case adopts the 2-Wasserstein distance and an FNN with \(d\)-dim output instead of the gradient of an FNN with 1-dim outout. Here, the reparameterization trick  needs to be used:

\[&_{n}(^{i}_{t-1},t)=_{n}(), \,(_{i},_{i}),\\ &_{i}=_{}(^{i}_{t-1}),_{i}= ((_{}(^{i}_{t-1}))).\] (16)

**Theorem 4.2**.: _For any \(>0\), if the support of the distribution \(^{(i)}_{v}\) is confined to a bounded space of \(^{d}\), there exists a FNN \(u():^{d}\) (and thus its gradient \( u():^{d}^{d}\)) with sufficiently large width and depth (depending on \(\)) such that \(^{2}_{2}(^{(i)}_{v}, u())<\) where \( u()\) is the distribution generated through the mapping \( u()\), \( a\)\(d\)-dim non-degenerate Gaussian distribution._

Another challenge is that the Wasserstein distance between \(_{n}(^{i}_{t-1},t)\) and \(_{^{s}_{t}}\) does not have a closed form. Thus, we utilize the empirical Wasserstein distance that can provably approximate the population one as in Peyre et al. . For each forward pass, our ConPreDiff will get \(q\) sampled target pixel/feature points \(\{^{tar}_{(i,j)}|1 j q\}\) from \(_{^{s}_{t}}\); Next, get \(q\) samples from \((_{i},_{i})\), denoted by \(_{1},_{2},...,_{q}\), and thus \(\{^{pred}_{(i,j)}=_{n}(_{j})|1 j q\}\) are \(q\) samples from the prediction \(_{n}(^{i}_{t-1},t)\); Adopt the following empirical surrogated loss of \(^{2}_{2}(_{n}(^{i}_{t-1},t),_{^{s }_{t}})\) in Equation (14):

\[_{}_{j=1}^{q}\|^{tar}_{(i,j)}-^{pred}_{(i,(j))}\|^{2},[q][q].\] (17)

The loss function is based on solving a matching problem and needs the Hungarian algorithm with \(O(q^{3})\) complexity . A more efficient surrogate loss may be needed, such as Chamfer loss based on greedy approximation  or Sinkhorn loss based on continuous relaxation , whose complexities are \(O(q^{2})\). In our study, as \(q\) is set to a small constant, we use Equation (17) based on a Hungarian matching and do not introduce much computational overhead. The computational efficiency of design is empirically demonstrated in Sec. 5.3.

### Discrete and Continuous ConPreDiff

In training process, given previously-estimated \(_{t}\), our ConPreDiff simultaneously predict both \(_{t-1}\) and the neighborhood distribution \(_{^{s}_{t}}\) around each pixel/feature. Because \(^{i}_{t-1}\) can be pixel, feature or discrete token of input image, we can generalize the ConPreDiff to existing discrete and continuous backbones to form discrete and continuous ConPreDiff. More concretely, we can substitute the point denoising part in Equation (14) alternatively with the discrete diffusion term \(^{dis}_{t-1}\) (Equation (3)) or the continuous (Equation (6)) diffusion term \(^{con}_{t-1}\) for generalization:

\[^{dis}_{}&= ^{dis}_{t-1}+_{t}_{i=1}^{x y}^{2}_{2} (_{n}(^{i}_{t-1},t),_{^{s}_{t}}),\\ ^{con}_{}&=^{ con}_{t-1}+_{t}_{i=1}^{x y}^{2}_{2}(_{n}(^{i}_{t-1},t ),_{^{s}_{t}}),\] (18)

where \(_{t}\) is a time-dependent weight parameter. Note that our ConPreDiff only performs context prediction in training for optimizing the point denoising network \(_{p}\), and thus does notintroduce extra parameters to the inference stage, which is computationally efficient. Equipped with our proposed context prediction term, existing diffusion models consistently gain performance promotion. Next, we use extensive experimental results to prove the effectiveness.

## 5 Experiments

### Experimental Setup

Datasets and MetricsRegarding unconditional image generation, we choose four popular datasets for evaluation: CelebA-HQ , FFHQ , LSUN-Church-outdoor , and LSUN-bedrooms . We evaluate the sample quality and their coverage of the data manifold using FID  and Precision-and-Recall . For text-to-image generation, we train the model with LAION [73; 74]

Figure 2: Synthesis examples demonstrating text-to-image capabilities of for various text prompts with LDM, Imagen, and ConPreDiff (Ours). Our model can better express local contexts and semantics of the texts marked in blue.

and some internal datasets, and conduct evaluations on MS-COCO dataset with zero-shot FID and CLIP score [25; 59], which aim to assess the generation quality and resulting image-text alignment. For image inpainting, we choose CelebA-HQ  and ImageNet  for evaluations, and evaluate all 100 test images of the test datasets for the following masks: Wide, Narrow, Every Second Line, Half Image, Expand, and Super-Resolve. We report the commonly reported perceptual metric LPIPS , which is a learned distance metric based on the deep feature space.

BaselinesTo demonstrate the effectiveness of ConPreDiff, we compare with the latest diffusion and non-diffusion models. Specifically, for unconditional image generation, we choose ImageBART, U-Net GAN (+aug) , UDM , StyleGAN , ProjectedGAN , DDPM  and ADM  for comparisons. As for text-to-image generation, we choose DM-GAN , DF-GAN , DM-GAN + CL , XMC-GAN  LAFITE , Make-A-Scene , DALL-E , LDM , GLIDE , DALL-E 2 , Improved VQ-Diffusion , Imagen-3.4B , Parti , Muse , and eDiff-I  for comparisons. For image inpainting, we choose autoregressive methods( DSI  and ICT ), the GAN methods (DeepFillv2 , AOT , and LaMa ) and diffusion based model (RePaint ). All the reported results are collected from their published papers or reproduced by open source codes.

Implementation DetailsFor text-to-image generation, similar to Imagen , our continuous diffusion model ConPreDiff\({}_{con}\) consists of a base text-to-image diffusion model (64\(\)64) , two super-resolution diffusion models  to upsample the image, first 64\(\)64 \(\) 256\(\)256, and then 256\(\)256 \(\) 1024\(\)1024. The model is conditioned on both TS  and CLIP  text embeddings. The T5 encoder is pre-trained on a C4 text-only corpus and the CLIP text encoder is trained on an image-text corpus with an image-text contrastive objective. We use the standard Adam optimizer with a learning rate of 0.0001, weight decay of 0.01, and a batch size of 1024 to optimize the base model and two super-resolution models on NVIDIA A100 GPUs, respectively, equipped with multi-scale training technique (6 image scales). We generalize our context prediction to discrete diffusion models [24; 85] to form our ConPreDiff\({}_{dis}\). For image inpainting, we adopt a same pipeline as RePaint , and retrain its diffusion backbone with our context prediction loss. We use T = 250 time steps, and applied r = 10 times resampling with jumpy size j = 10. For unconditional generation tasks, we use the same denoising architecture like LDM  for fair comparison. The max channels are 224, and we use T=2000 time steps, linear noise schedule and an initial learning rate of 0.000096.

  
**Approach** & **Model Type** & **FID-30K** & 
 **Zero-shot** \\ **FID-30K** \\  \\  AttnGAN  & GAN & 35.49 & - \\ DM-GAN  & GAN & 32.64 & - \\ DF-GAN  & GAN & 21.42 & - \\ DM-GAN + CL  & GAN & 20.79 & - \\ XMC-GAN  & GAN & 9.33 & - \\ LAFITE  & GAN & 8.12 & - \\ Make-A-Scene  & Autoregressive & 7.55 & - \\  DALL-E  & Autoregressive & - & 17.89 \\ LAFITE  & GAN & - & 26.94 \\ LDM  & Continuous Diffusion & - & 12.63 \\ GLIDE  & Continuous Diffusion & - & 12.24 \\ DALL-E 2  & Continuous Diffusion & - & 10.39 \\ Improved VQ-Diffusion  & Discrete Diffusion & - & 8.44 \\ Simple Diffusion  & Continuous Diffusion & - & 8.32 \\ Imagen  & Continuous Diffusion & - & 7.27 \\ Parti  & Autoregressive & - & 7.23 \\ Muse  & Non-Autoregressive & - & 7.88 \\ eDiff-I  & Continuous Diffusion & - & 6.95 \\  
**ConPreDiff\({}_{dis}\)** & Discrete Diffusion & - & **6.67** \\
**ConPreDiff\({}_{con}\)** & Continuous Diffusion & - & **6.21** \\   

Table 1: Quantitative evaluation of FID on MS-COCO for 256 Ã— 256 image resolution.

Our context prediction head contains two non-linear blocks (_e.g._, Conv-BN-ReLU, resnet block or transformer block), and its choice can be flexible according to specific task. The prediction head does not incur significant training costs, and can be removed in inference stage without introducing extra testing costs. We set the neighborhood stride to 3 for all experiments, and carefully choose the specific layer for adding context prediction head near the end of denoising networks.

### Main Results

Text-to-Image SynthesisWe conduct text-to-image generation on MS-COCO dataset, and quantitative comparison results are listed in Tab. 1. We observe that both discrete and continuous ConPreDiff substantially surpasses previous diffusion and non-diffusion models in terms of FID score, demonstrating the new state-of-the-art performance. Notably, our discrete and continuous ConPreDiff achieves **an FID score of 6.67 and 6.21 which are better than the score of 8.44 and 7.27 achieved by previous SOTA discrete and continuous diffusion models**. We visualize text-to-image generation results in Figure 2, and find that our ConPreDiff can synthesize images that are semantically better consistent with text prompts. It demonstrates our ConPreDiff can make promising cross-modal semantic understanding through preserving visual context information generating process, and effectively associating with contextual text information. Moreover, we observe that ConPreDiff can synthesize complex objects and scenes consistent with text prompts as demonstrated by Figure 7 in Appendix A.4, proving the effectiveness of our designed neighborhood context prediction. Human evaluations are provided in Appendix A.5.

Image InpaintingOur ConPreDiff naturally fits image inpainting task because we directly predict the neighborhood context of each pixel/feature in diffusion generation. We compare our ConPreDiff against state-of-the-art on standard mask distributions, commonly employed for benchmarking. As in Tab. 2, our ConPreDiff outperforms previous SOTA method for most kinds of masks. We also put some qualitative results in Figure 3, and observe that ConPreDiff produces a semantically meaningful filling, demonstrating the effectiveness of our context prediction.

Unconditional Image SynthesisWe list the quantitative results about unconditional image generation in Tab. 3 of Appendix A.3. We observe that our ConPreDiff significantly improves upon the state-of-the-art in FID and Precision-and-Recall scores on FFHQ and LSUN-Bedrooms datasets. The ConPreDiff obtains high perceptual quality superior to prior GANs and diffusion models, while maintaining a higher coverage of the data distribution as measured by recall.

### The Impact and Efficiency of Context Prediction

In Sec. 4.2, we tackle the complexity problem by transforming the decoding target from entire neighborhood features to neighborhood distribution. Here we investigate both impact and efficiency of the proposed neighborhood context prediction. For fast experiment, we conduct ablation study with the diffusion backbone of LDM . As illustrated in Figure 4, the FID score of ConPreDiff

  
**CelebA-HQ** & Wide & Narrow & Super-Resolve 2x & Altern. Lines & Half & Expand \\ Method & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) \\  AOT  & 0.104 & 0.047 & 0.714 & 0.667 & 0.287 & 0.604 \\ DSI  & 0.067 & 0.038 & 0.128 & 0.049 & 0.211 & 0.487 \\ ICT  & 0.063 & 0.036 & 0.483 & 0.353 & 0.166 & 0.432 \\ DeepFillv2  & 0.066 & 0.049 & 0.119 & 0.049 & 0.209 & 0.467 \\ LaMa  & 0.045 & 0.028 & 0.177 & 0.083 & **0.138** & 0.342 \\ RePaint  & 0.059 & 0.028 & 0.029 & **0.009** & **0.165** & 0.435 \\
**ConPreDiff** & **0.042** & **0.022** & **0.023** & 0.022 & **0.139** & **0.297** \\  
**ImageNet** & Wide & Narrow & Super-Resolve 2x & Altern. Lines & Half & Expand \\ Method & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) & LPIPS \(\) \\  DSI  & 0.117 & 0.072 & 0.153 & 0.069 & 0.283 & 0.583 \\ ICT  & 0.107 & 0.073 & 0.708 & 0.620 & 0.255 & 0.544 \\ LaMa  & 0.105 & 0.061 & 0.272 & 0.121 & **0.254** & 0.534 \\ RePaint  & 0.134 & 0.064 & 0.183 & **0.089** & 0.304 & 0.629 \\
**ConPreDiff** & **0.098** & **0.057** & **0.129** & 0.107 & 0.285 & **0.506** \\   

Table 2: Quantitative evaluation of image inpainting on CelebA-HQ and ImageNet.

is better with the neighbors of more strides and 1-stride neighbors contribute the most performance gain, revealing that preserving local context benefits the generation quality. Besides, we observe that increasing neighbor strides significantly increases the training cost when using feature decoding, while it has little impact on distribution decoding with comparable FID score. To demonstrate the generalization ability, we equip previous diffusion models with our context prediction head. From the results in Figure 5, we find that our context prediction can consistently and significantly improve the FID scores of these diffusion models, sufficiently demonstrating the effectiveness and extensibility of our method. We further conduct ablation study on the trade-off between CLIP and FID scores across a range of guidance weights in Appendix A.2, the results exhibit our superior generation ability.

## 6 Conclusion

In this paper, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly force each point to predict its neighborhood context with an efficient context decoder near the end of diffusion denoising blocks, and remove the decoder for inference. ConPreDiff can generalize to arbitrary discrete and continuous diffusion backbones and consistently improve them without extra parameters. We achieve new SOTA results on unconditional image generation, text-to-image generation and image inpainting tasks.