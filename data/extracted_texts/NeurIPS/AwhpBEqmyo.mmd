# Collection Process

[MISSING_PAGE_EMPTY:1]

Introduction

There has been an explosion in the capabilities of generative AI models in the last two years. Recent language models can produce fluent text [_e.g._, 1, 2, 3, 4], audio generation models can synthesize high-fidelity speech and music [5, 6, 7, _i.a._], and text-to-image models can create photorealistic images [_e.g._, 8, 9, 10, 11], paving the way to real-world applications of AI in art, design and content creation.

Generating high-quality videos of arbitrary duration from text is still, however, an arduous task. Compared to text-to-image synthesis, video generation introduces several distinctive challenges. First, videos should be coherent over time (_e.g._, objects should be consistent from frame to frame) while reflecting the actions described in text prompts. Second, training text-to-video models is computationally expensive: multiple frames need to be synthesized within the same context to produce smooth transitions over time. And third, video-text datasets [_e.g._, 12, 13, 14] are orders of magnitude smaller than corresponding image-text datasets [15, 16, _i.a._].

To enable researchers to reliably measure progress in video generation from text prompts that change over time, we introduce the StoryBench benchmark. We create StoryBench by collecting dense, rich annotations for the validation and test sets of three existing, general video datasets: DiDeMo , Oops  and UVO , for a total of 6K videos. Our human annotators first describe a video with a _sequence_ of captions, one for each action, forming the story of the video. In this stage, our annotators also _temporally segment_ the full video accordingly, providing timestamps for each caption. This results in unique datasets, with rich annotations of a sequence of actions (_i.e._, a story) that have clear timestamps. Moreover, we next ask annotators to tag _each_ of the resulting 18K video segments according to 34 labels that span 6 categories, such as _camera movements_ and _foreground interactions_, allowing researchers and practitioners to easily pinpoint the limitations of their text-to-video models. Finally, given the challenges of obtaining high-quality training data for story visualization, we devise a pipeline to automatically transform existing, grounded video captions  into stories, resulting in training data that is very rich in structure, with (i) a sequence of action descriptions, (ii) their timestamps, and even (iii) mouse traces and segmentation masks. We show that this transformation leads to better models, and encourage future work in data curation strategies to improve performance.

Based on these data, we define three tasks of increasing difficulty (see Figure 1) with the goal of synthesizing videos that follow a sequence of text prompts (_i.e._, a story). In the _action execution_ task, a text-to-video model is simply asked to continue a conditioning video from a single caption (_i.e._, a short sentence describing a single action). This task is extended in _story continuation_, where the model must generate a sequence of coherent video segments given a conditioning video and a series of captions. Finally, the _story generation_ task also asks a model to generate a sequence of coherent video segments from a series of captions, but without a conditioning video. Instead, the model is first asked to generate its own conditioning video from a textual description of the expected scene background. For each of the captions, models are also given the expected duration of the corresponding video segment (_e.g._, 1.5s). This requires models to be _controllable_, resembling real-world applications of artistic content creation. Overall, we name the overarching challenge _continuous story visualization_.

We train a small, 345M parameter text-to-video Phenaki  model. In StoryBench, models can be evaluated in three different setups: _zero-shot_, where a model is tested after general-purpose pretraining, given no StoryBench data; _single-task_, where a model is separately fine-tuned on each dataset; and _multi-task_, where a single model is fine-tuned on our three datasets. Due to the challenges in evaluating videos, we conduct a human study by defining guidelines that capture different desired qualities of continuous story visualization. We also report performance across five automatic metrics, but we show they do not align well with human judgment; reassecting the need of better metrics.

**Contributions.** In this work, we **1)** introduce StoryBench, the first benchmark for continuous story visualization, to inspire more work on real-world, text-to-video generation of arbitrary duration through a reproducible and comprehensive setup. **2) StoryBench contains rich human annotations, consisting of action-by-action descriptions, their timestamps, multimodal grounding via mouse traces, and labels for each segment to easily determine failure modes. StoryBench is a truly multifaceted benchmark: it spans three tasks of continuous story visualization, across three different datasets, and three evaluation setups (zero-shot, single- and multi-task). We **3)** devise an automatic procedure to transform video captions into a sequence of captions, each describing a single action, and show the effectiveness of using such rich data during model fine-tuning. We **4)** establish a compact yet strong baseline, define guidelines for human evaluation, as well as a set of automatic metrics for reproducible comparisons of the next generation of text-to-video models. **5)** Our results show the benefit of training text-to-video models to continue videos (rather than generating them from scratch) on story-like data, but they also suggest discrepancies between human and automated evaluation. We invite the community to report results on StoryBench at https://paperswithcode.com/dataset/storybench. We provide data, annotation instructions, human evaluation guidelines, and code for automatic evaluation at https://github.com/google/storybench.

## 2 Related Work

Text-guided generative models for vision.We are currently witnessing tremendous progress in the task of text-to-image generation and editing [8; 9; 10; 11; 22; 23; 24; 25; 26; 27], fueled by sequence-to-sequence Transformer models  and diffusion models  trained on massive amounts of image-text data . Research on text-to-video generation has also received attention recently. GODIVA  autoregressively generates videos from text using a three-dimensional sparse attention mechanism. NUWA  presents a unified framework for multi-task learning of various generation tasks, including text-to-video. NUWA-Infinity  is a generative model that can synthesize arbitrarily-sized images or long-duration videos with an autoregressive over autoregressive generation mechanism. In a similar spirit, NUWA-XL  proposes a diffusion over diffusion approach that allows to generate long videos in parallel through a coarse-to-fine process. CogVideo  adds temporal attention modules on top of a frozen text-to-image model to reduce the computational requirements for text-to-video learning. Make-a-Video  also starts from a text-to-image model but fine-tunes it while adding pseudo-3D convolution and temporal attention layers. Concurrently, Video Latent Diffusion Models  turn pretrained image diffusion models into video generators by fine-tuning them with temporal alignment layers, and Imagen Video  generates high definition videos using a cascade of video diffusion models. Ho  train space-time factorized U-Net models  on images and videos, and propose a sampling method to improve longer video generations. Our baselines are based on Phenaki , which can generate arbitrary long videos from a sequence of text prompts.

**Story visualization.** In this paper, we propose StoryBench, a benchmark for the task of generating a _video_ from a sequence of text prompts (_i.e._, a story), which we refer to as _continuous_ story visualization. In the literature, story visualization  is the task of generating a sequence of _images_ to narrate a multi-sentence story (one image per sentence) with a global visual consistency across dynamic scenes and entities. The authors created two artificial datasets from CLEVR  and Pororo , and proposed a model based on sequential conditional GANs. To improve story visualization, Maharana _et al_. propose a dual learning framework and a copy mechanism in , and leverage grammatical and visual structure as well as commonsense information in . In , the authors introduce the DiDeMoSV dataset, and propose to'retro-fit' a pretrained text-to-image model with task-specific modules to improve on the task of story continuation, resulting in StoryDALL-E. Finally, Rahman  extend the synthetic MUGEN dataset  for multi-sentence storylines, and propose an autoregressive diffusion-based framework with a visual memory module to capture the entities and background context across the generated frames. Unlike previous work, in StoryBench, we focus on generating continuous videos (rather than key-frames) on natural (rather than cartoon or synthetic) data. Moreover, we also use DiDeMo to visualize stories but rather than using the existing temporal queries and automatically matching them to key-frames , we ask human annotators to thoroughly describe the story of the videos while manually annotating timestamps for each sentence.

## 3 StoryBench

Aiming for a comprehensive resource to assess the ability of generative models to visualize stories, we propose StoryBench, the first real-world benchmark for text-to-video story generation. Unlike previous work which frames story visualization as the task of generating a single key-frame per text prompt, StoryBench evaluates the ability of generative models to synthesize _continuous_, _natural_ videos from a sequence of text prompts. To do so, we collect rich annotations that provide insights and nuances of any model's capabilities, and easily discover failure modes. StoryBench consists of three different datasets, three tasks of increasing difficulty, and three evaluation setups.

Generating videos is a very complex task for state-of-the-art models. Some of the key challenges involve generating videos that (i) have a coherent storyline, (ii) are visually realistic, and (iii) can be controlled according to user intent. StoryBench aims at benchmarking these three challenges by (i) defining three tasks with increased difficulty in storyline; (ii) focusing on real-world, natural videos; and (iii) enabling control over the generated content through text descriptions and video duration.

### Datasets

We annotate three different, real-world, open domain video datasets for continuous story visualization. Table 1 lists high-level statistics of our evaluation data. See App. D for more details and data samples.

**DiDeMo-CSV.** DiDeMo  is a dataset collected for the task of temporal localization, where a model is tasked to identify the video segment corresponding to a given text query (_e.g._, 'the cat jumps on the mat'). The authors define segments with a resolution of 5 seconds. Due to the nature of the temporal localization task, the original text descriptions are often short and lack essential context information to be used for story visualization. We hence ask human annotators to collect descriptions of the actions happening in a video in order to create a coherent story.2 The annotators are also tasked to (i) add timestamps for each action, (ii) add a description of the background (to be used for the story generation task), and (iii) specify the number of important actors throughout the video.3

**Oops-CSV.** Oops  is a dataset covering unintentional human actions (_i.e._, fail videos), originally filmed by nonprofessionals, that are diverse in actions, environments, and intentions. This kind of fail videos are extremely interesting for video generation, as they are unpredictable. A failure is typically a surprising, unexpected action, and one that an ordinary video model would not likely generate. To use these videos for the task of continuous story visualization, we start from the rich annotations in VidLN , which consist of descriptive captions that are grounded to the videos through mouse traces. First, we design a pipeline to automatically split the original captions into a sequence of actions that keep most of the original words (see Section 4). Second, we ask human annotators to refine these preprocessed captions by (i) fixing any errors in sentence splitting whilst keeping most of the original words (in order to preserve the rich annotations from VidLN), (ii) adding timestamps for each action, and (iii) adding a concise context description. In fact, the VidLN captions describe the actions of a _single_ entity (_i.e._, actor) throughout the video. Upon inspection, we found that such captions were unsuitable for our task, as they often lacked information that was crucial to synthesize a video similar to the ground-truth. The VidLN data already specified the number of important actors throughout the video, which we update based on stories that were kept after a quality insurance phase.

**UVO-CSV.** UVO  is a dataset originally collected for open-world, class-agnostic object segmentation in videos. Videos in UVO are sourced from Kinetics-400 , which contains 10-second 30fps videos labeled with human actions. UVO includes many videos with crowded scenes and complex background motions, taken by both professionals and amateurs. As UVO videos have also been annotated with VidLN data, we follow the same steps as for Oops-CSV to create UVO-CSV.

Segment-level labels for easy diagnostics.In addition to data for visual generation of stories, we collect rich annotations to promptly detect failure modes of text-to-video models in StoryBench. Specifically, together with artists and designers that work with generative AI, we defined 34 labels across 6 categories, shown in Table 2. These include: _camera movements_, to determine the type of shot; _foreground entities_, to know which entities are shown in the video; _foreground_ and _background_ actions, to know which actions happen in the video; _foreground interactions_, to know how different entities interact with each other; and _foreground transitions_, to know how entities evolve throughout

  
**Dataset** & **\#Videos** & **\#Stories (per video)** & **\#Segments (per story)** & **\#Words (per story)** & **\#Labels (per story)** & **\#Actors (per video)** \\  DiDeMo-CSV & 1,399 & 1,399 (1.00) & 4,926 (3.52) & 80,405 (57.47) & 14,463 (10.34) & 3,228 (2.31) \\ Oops-CSV & 1,888 & 3,243 (1.72) & 7,198 (2.22) & 131,485 (40.54) & 30,027 (9.26) & 6,556 (3.47) \\ UVO-CSV & 2,917 & 4,258 (1.46) & 6,227 (1.46) & 122,542 (28.78) & 36,751 (8.63) & 7,743 (2.65) \\   

Table 1: Statistics of our collected evaluation datasets. Actors refer to entities with a key role a video.

  
**Category** & **Labels** \\  Camera Movements & static shot, pan, tilt, zoom, tracking shot, aerial shot, point-of-view shot \\ Foreground Entities & people, animals, vehicles, food/drinks, containers, tools \\ Foreground Actions & humans moving, animals moving, objects moving, humans using objects, animals using objects, static actions \\ Background Actions & aiminate entities moving, objects moving, animate entities using objects, static actions, dynamic background \\ Foreground Interactions & dialogues, direct, indirect, object-based \\ Foreground Transitions & new entities, new objects, entities vanish, objects vanish, entities re-enter, objects re-enter \\   

Table 2: Overview of categories and labels for each video segment to easily detect failure modes.

the video. For each video segment and label, annotators were asked to check two boxes indicating whether a given label was shown in the video and/or in the caption. By doing so, we can study whether models can capture specific aspects of video generation while also knowing if those aspects were mentioned in the text prompt. We provide further details on each category and label in App. D.1.

Human annotation framework.We rely on an in-house platform to annotate our data. For any kind of annotations, human annotators first undergo a training phase. In this phase, they raise any doubts and present corner cases to the first author, who revisits the guidelines with further details and examples. After training, the first author and last author meet with the annotators' managers to ensure the task is clearly understood. The managers then assess which annotators have a high performance for the task, before collecting the final annotations. The annotators' managers, the first author and the last author communicate regularly throughout the annotation process, validating samples and discussing any potential issues (_e.g_., removing videos with problematic and offensive content).

### Tasks

We define three tasks for continuous story visualization of increasing complexity (shown in Figure 1). We call _context_ the history of all text and/or video prompts available to a model at any generation step. The _input_, instead, includes a text prompt describing the next video to be generated, and its duration.

Action execution.The task of _action execution_ simply requires a model to generate the next action specified in the input. The context is given by the ground-truth video preceding the generation step. Whenever the _first_ action of a video must be generated, models are conditioned on the first 0.5s of ground-truth video, in order to provide visual context whilst being robust to camera movements.

Story continuation.The task of _story continuation_ requires a model to generate a video from a _sequence_ of inputs and a conditioning video. Analogous to _action execution_, whenever the _first_ action of a video must be generated, models are conditioned on the first 0.5s of ground-truth video. The context for the following steps also includes all the previous text prompts and synthesized videos.

Story generation.The task of _story generation_ also requires a model to generate a video from a _sequence_ of inputs, but without any ground-truth visual conditioning. Instead, the context for the first generation is a video synthesized by the model from a text description of the _background_, to guide the space of possible outputs. The following steps are generated analogously to _story continuation_ steps.

### Human Evaluation

Evaluation of generative models for perceptual data, such as images and videos, is critical to validate their performance. In fact, humans possess complex cognitive abilities to interpret visual content, which automatic metrics often fail to capture. Further, we are ultimately interested in the impact and utility of generative models on users, and human evaluation provides such user-centric perspective.

We follow previous work [10; 21; 22; 35] in doing side-by-side evaluations, where human raters are asked to choose the best (or none) of two outputs for the same prompt. We extend the common metrics of visual quality and text adherence to capture crucial aspects of story visualization as follows.

Visual quality:Which video looks better?

Text adherence:Which video better reflects the caption?

Entity consistency:Throughout which video are entities more consistent (_e.g_., clothes do not change without a change described in the caption)?

Background consistency:In which video is the background more consistent (_e.g_., the room does not change without a change described in the caption)?

Action realism:In which video do actions look more realistic (_e.g_., according to physics)?

An example of our interface is shown in App. D.3. For each evaluation, humans are provided with the full text story; and each text prompt is rendered as a video below each generated video to facilitate mappings of sentences to their corresponding outputs. For the tasks of _action execution_ and _story continuation_, we also provide the conditioning video but add a red border around it to easily identify its end. The models are anonymized and the output pairs are randomly ordered (left vs. right) for each presentation to a rater. We randomly sample 100 stories, and ask three raters to judge each pair.

### Automatic Metrics

Human evaluation will always be the gold standard of generative models' evaluation. However, it is both time-consuming and expensive to conduct. To help researchers and practitioners quickly deploy and evaluate new systems, different automatic metrics that correlate fairly well with human evaluations have been proposed over the years, such as FID  and FVD , as well as CLIP  for the recent text-guided models. We consider the following automatic metrics for StoryBench.

**Frechet Image Distance (FID):** The FID score evaluates the quality of generated images (_i.e._, frames here) by comparing how similar their features are to features of real images. We consider both the standard features given by InceptionV3 , as well as features from a stronger CLIP (ViT-L/14).

**Frechet Video Distance (FVD):** FVD extends the idea behind FID to videos: a generative model must capture the underlying distribution of real-world videos. We consider both the standard features from I3D , and features from InternVideo-MM-L-14 , a state-of-the-art, video-text model.

**Video-Text Matching (VTM):** We estimate the alignment between generated videos and their text descriptions with the similarity of the visual and textual features given by a multimodal model. We consider both the average cosine similarity of each frame with the corresponding caption given by CLIP (ViT-L/14), and the cosine similarity of a video with its caption given by InternVideo-MM-L-14.

**Perceptual Quality Assessment (PQA):** We also measure _perceptual_ quality, which aims at predicting the average human subjective perception of a video. For this, we use DOVER , a state-of-the-art model trained to predict the mean opinion score of user-generated videos.

**Similarity of Images (SIM):** For the tasks of _action execution_ and _story continuation_, a model is conditioned on a ground-truth video, and asked to continue it for the exact same number of frames as the original one. We can hence directly compare each generated frame with its ground-truth match. We compute frame cosine similarity from the normalized features extracted to compute FID.

We only compute automatic metrics on the generated videos, disregarding the conditioning ground-truth video (_action execution_ and _story continuation_ tasks). For _story generation_, the frames generated from the 'background' descriptions are omitted, and we do not report SIM as it is ill-defined here. We release our evaluation code to promote reproducible and comparable research on StoryBench.

## 4 Training Data Challenge

In addition to the challenges of video generation discussed in Section 3, the lack of large amounts of high-quality data is a major bottleneck to train strong text-to-video models. Manually collecting annotations for videos is both expensive and time-consuming, especially in story-like format.

Given the major role that data plays in training state-of-the-art generative models, and the impact that careful pre-processing and synthetic data can have, we explicitly define the challenge of training data curation to improve performance in StoryBench.

We provide a first approach in this work, which we will show to be beneficial in Section 6. Specifically, we define an automatic pipeline to transform the original VidLN captions for Oops and UVO into multiple sentences, each approximately describing a single action, and determine their timestamps.

To create an original VidLN annotation , the annotator first watched the video and selected important actors (_e.g._, man or dog). For each actor they would then select a few key-frames that are representative of the actions performed by this actor throughout the video. They then described the actions of that entity with their voice, while at the same time moving their mouse over the key-frames to provide a spatio-temporal localization for each word. Finally, they transcribed their audio for a high-quality caption. For each actor, VidLN provides a long caption that often describes multiple actions. For the purpose of story visualization, we want to split the long caption into individual actions. Furthermore, we need timestamps for each action. The VidLN captions do not provide explicit timestamps for actions, but several words are localized in space-time on one or multiple key-frames. We use this indirect temporal information to derive an approximate timestamp.

Our pipeline consists of five steps (see Figure 2). **1)** We prompt Flan-U-PaLM 540B [3; 56] - an instruction-tuned large language model (LLM) - to split a given description into multiple sentences, each (approximately) describing a single action, whilst making minimal (ideally, no) changes to the original words. **2)** We can then map every word in each sentence to all (if any) of its associatedkey-frames. As the LLM sometimes modifies words for better grammaticality, we rely on lemmas instead. **3)** We associate a single key-frame to each caption, determined by the frame most commonly referenced by the verbs (if available), or across all words in the sentence. **4)** If a continuous subsequence of sentences maps to the same key-frame, we concatenate those sentences into a single one. **5)** Finally, we map each sentence to a time interval using the timestamp of its associated key-frame.

Our code to transform VidLN descriptions into stories is online. See App. D.5 for robustness statistics.

## 5 Experimental Setup

We train a compact (345M parameters) yet strong Phenaki  baseline for StoryBench, which we refer to as Phenaki-Gen. It consists of a C-ViViT encoder, a text encoder, and a MaskGiT model.

The goal of the C-ViViT encoder is to learn a codebook of visual features to compress the videos in both temporal and spatial dimensions, while staying autoregressive in time. We train a 52M C-ViViT model on 27M Web-scraped video samples. Rather than a square resolution , we opt for a more natural, rectangular resolution of 160\(\)96 pixels. After being trained, the C-ViViT is kept frozen.

MaskGiT  is a bidirectional Transformer trained to predict multiple masked video tokens simultaneously conditioned on the caption features computed by the text encoder. Compared to the MaskGiT model used in the original Phenaki model (1.8B), ours is much smaller at 345M (24 layers). In contrast to the original Phenaki, which used a pretrained text encoder, we train the text encoder jointly with the video generation model on our collection of Web-collected 27M video captions. Our text encoder is a 12-layer Transformer that uses a T5 tokenizer . Both models use 16 attention heads, MLP size of 3072, and default embedding size of 768. The maximum sequence length for the video component is 1440 tokens (11 compressed frames), and 64 tokens for the text component. The model is trained for 3M steps with a batch size of 256 using a fixed learning rate of 4.5e-5 after a linear warm-up for the first 10K steps using the Adam  optimizer. After pretraining, Phenaki-Gen can generate arbitrarily long videos using the last 5 frames to condition the generation of the next 6.

We fine-tune Phenaki-Gen on the training data of StoryBench for 500K steps with a much smaller batch size of 64 samples, using the same learning rate and optimizer of the pretraining stage.

Furthermore, we explore a novel variant of our model that is fine-tuned for _continuation_, Phenaki-Cont, using the same hyperparameters as Phenaki-Gen. Rather than training to predict all the input tokens, we only mask and generate the tokens of the last 6 frames, conditioning on 5 ground-truth frames. We hypothesize that this training would result in better transitions and consistency.

## 6 Results

In this section, we discuss the performance of our baselines on StoryBench evaluated by humans and automatically. We append -ZS for results obtained in the _zero-shot_ setting, -ST for _single-task_ fine-tuning, and -MT for _multi-task_ fine-tuning. In particular, we start by discussing performance on Oops-CSV (shown in Table 4) due to space limitations. We then also comment on the overall

Figure 2: Automatic pipeline to transform VidLN annotations into stories. Starting from a video, its caption and annotated key-frames, we use an LLM to split the caption into multiple sentences. We then transfer the key-frames of the original caption into the new ones. We select a single key-frame per caption, merge captions with the same key-frame and finally split the video accordingly.

findings, reported in Figure 17 (App. E.1) for human evaluation, Tables 5 to 7 when using a set of neural metrics, and in Tables 8 to 10 (App. E.2) when using the others. Moreover, we also benchmark a variant of Phenaki-Cont fine-tuned on the original Oops dataset, rather than after our automatic story-like transformation, which we refer to as Phenaki-Cont-ST-orig. For every story, each model generates 4 output videos. We randomly sample one of them for human evaluation, but report mean and standard deviation across all 4 generated videos for automatic metrics. Due to our setup, all the results are at 8fps and using a 160\(\)96 pixel resolution for the generated videos, but note that ground-truth videos were processed at their original resolution for automatic evaluation.4

### Human Evaluation

Table 3 shows our human evaluation (_c.f_. Section 3.3) results for story continuation on Oops-CSV. Our model fine-tuned in continuation mode is vastly preferred over the zero-shot Phenaki-Gen-ZS in all metrics. Figure 3 illustrates an example of why raters prefer Phenaki-Cont-ST over Phenaki-Gen-ZS. The improvement in long-term text adherence and consistency, however, has its limitations. We illustrate this in Figure 4: while the key actions are executed correctly, the background tends to drift over time, showing that our modest sized model has still large margins for improvement. We also see that human raters consistently prefer Phenaki-Cont-ST over Phenaki-Gen-ST (both fine-tuned on the same data), proving the effectiveness of the novel fine-tuning in continuation mode.

However, fine-tuning on all datasets jointly (Phenaki-Cont-MT) results in videos of comparable quality as the zero-shot model. This is a common limitation of multi-task systems, and both a larger model and longer training could lead to better performance. Finally, we compare Phenaki

Figure 4: Applying Phenaki-Cont-ST to a longer sequence with multiple prompts for story continuation. The model can generate the right action, including the water splash from the girl falling in the water. The background is kept relatively consistent within a short time frame, but it starts changing at longer time scales. The video was subsampled by a factor 4 to be shown here.

Figure 3: Comparison of Phenaki-Gen-ZS to Phenaki-Cont-ST on a prompt for _action execution_. While Phenaki-Gen-ZS animates the animal, it adheres less to the text prompt, and the Ilama changes over time, Phenaki-Cont-ST successfully shows two entities from the context (person and bowl), while keeping the animal and the surroundings consistent. Video subsampled by a factor 4.

[MISSING_PAGE_FAIL:9]

differ from realistic filmmaking applications. Moreover, we highlight that StoryBench focuses on real-world videos, and does not include artistic content never seen in the real world. Yet, we expect it can drive general progress in text-to-video generation thanks to its complexity. Our annotation framework would not scale to long videos, and we encourage future work to investigate more efficient protocols. We also note that our baseline models are small (345M) compared to current text-to-image models (20B). Effectively training large text-to-video models is an open question. Finally, we also found that automatic metrics do not fully align with human ratings. This might indicate that the metrics are suboptimal, but we cannot exclude that they would perform better on higher resolution videos. This highlights the need for a future in-depth study on human versus automated evaluation.

## 8 Conclusion

In this work, we collected annotations for the task of video generation from a sequence of text prompts narrating the video evolution (_i.e_., continuous story visualization). Our data is very rich, with timestamps for each text prompt, as well as diagnostic labels for each video segment. On top of such unique data, we propose StoryBench: a new benchmark to measure progress of forthcoming text-to-video models on different tasks, three datasets and in three evaluation setups. StoryBench allows to evaluate key challenges of arbitrarily long video generation, such as consistency over time and realistic action synthesis. We benchmarked small yet strong baselines, and showed that fine-tuning a Phenaki model for continuation improves such aspects. Our results highlight a discrepancy between human and automatic ratings. While human evaluation remains the most reliable method, we encourage future work to also report automatic metrics in an effort to better understand and develop effective automatic evaluation. Our benchmark enables a systematic comparison of text-to-video models that can perform story generation with prompts that vary over time and specified duration. Beyond modeling, opportunities for future work include expanding the benchmark with datasets of longer videos and richer variety (_e.g_., camera jumps, scene changes, etc.) to enhance its applicability to industrial-level videos, and devising efficient data annotations protocols to facilitate the collection.

  
**Shary Calculation** &  &  &  \\  & \(\) (@89 ps) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) \\   &  \\  & \(\) (@89 ps) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) \\   &  \\  & \(\) (@89 ps) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) \\   &  \\  & \(\) (@89 ps) & \(_{+1}\) & \(_{+1}\) & \(\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) & \(_{+1}\) \\   &  \\  & \(\) (@89 ps) & \(_{+1}\) & \(_{+1}\) &