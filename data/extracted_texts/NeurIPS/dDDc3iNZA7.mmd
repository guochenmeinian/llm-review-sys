# UniDSeg: Unified Cross-Domain 3D Semantic Segmentation via Visual Foundation Models Prior

Yao Wu\({}^{1}\), Mingwei Xing\({}^{2}\), Yachao Zhang\({}^{1}\)1, Xiaotong Luo\({}^{1}\), Yuan Xie\({}^{4,5}\), Yanyun Qu\({}^{1,2,3}\)

\({}^{1}\)School of Informatics, Xiamen University

\({}^{2}\)Institute of Artificial Intelligence, Xiamen University

\({}^{3}\)Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University

\({}^{4}\)School of Computer Science and Technology, East China Normal University

\({}^{5}\)Chongqing Institute of East China Normal University

wuyao@stu.xmu.edu.cn, yyqu@xmu.edu.cn

Corresponding author

###### Abstract

3D semantic segmentation using an adapting model trained from a source domain with or without accessing unlabeled target-domain data is the fundamental task in computer vision, containing domain adaptation and domain generalization. The essence of simultaneously solving cross-domain tasks is to enhance the generalizability of the encoder. In light of this, we propose a groundbreaking universal method with the help of off-the-shelf Visual Foundation Models (VFMs) to boost the adaptability and generalizability of cross-domain 3D semantic segmentation, dubbed **UniDSeg**. Our method explores the VFMs prior and how to harness them, aiming to inherit the recognition ability of VFMs. Specifically, this method introduces layer-wise learnable blocks to the VFMs, which hinges on alternately learning two representations during training: _(i)_ Learning visual prompt. The 3D-to-2D transitional prior and task-shared knowledge is captured from the prompt space, and then _(ii)_ Learning deep query. Spatial Tunability is constructed to the representation of distinct instances driven by prompts in the query space. Integrating these representations into a cross-modal learning framework, UniDSeg efficiently mitigates the domain gap between 2D and 3D modalities, achieving unified cross-domain 3D semantic segmentation. Extensive experiments demonstrate the effectiveness of our method across widely recognized tasks and datasets, all achieving superior performance over state-of-the-art methods. Remarkably, UniDSeg achieves 57.5%/54.4% mIoU on "A2D2/sKITTI" for domain adaptive/generalized tasks. Code is available at https://github.com/Barcaaaa/UniDSeg.

## 1 Introduction

As deep learning technology develops by leaps and bounds , 3D scene understanding has become the foundation for many real-world applications, including autonomous driving, robotics, augmented reality, smart cities, etc. Based on the LiDAR sensor, 3D semantic segmentation is a critical task that provides an accurate and robust semantic prediction of the surrounding scenarios. However, annotating large-scale datasets for training every new scenario is labor-intensive and time-consuming, especially for the tasks demanding point-wise annotations. These limitations hinder their practical applicability in real-world scenarios where acquiring high-quality labeled data.

Currently, domain adaptive 3D semantic segmentation (DA3SS)  and domain generalized 3D semantic segmentation (DG3SS)  have been widely explored inautonomous driving scenes. Their difference lies in that the former seeks to narrow the domain gap between the source and target domain data without assigning 3D semantic labels, while the latter aims to learn a generic and robust model before being exposed to the target domain. Albeit successful, their applications in 3D semantic segmentation have primarily focused on generalizing or adapting between synthetic and real scenes or across different scene layouts. This leaves a gap in exploring a universal framework, enabling the generalization and adaptation of 3SS models across datasets.

With the above considerations, this paper focuses on studying a universal framework for cross-domain 3D semantic segmentation. The essence of simultaneously solving cross-domain tasks is to enhance the generalizability of the encoder. Therefore, a generalizable 3D model with source-domain data discrimination power to the target domain is necessary. It is performed solely with access to source domain data, enabling the model to develop the ability to discriminate domain-agnostic and domain-specific features. Unfortunately, one limitation has arisen, the scarcity of 3D pre-training datasets hinders this endeavor. More recently, Visual Foundation Models (VFMs) [23; 34; 36] have emerged as the de-facto visual backbone in 2D image classification and segmentation. Such VFMs are trained on massive raw web-curated images, achieving promising open-vocabulary recognition. Hence, two natural questions are thrown up: _(i) How to borrow 2D prior knowledge from VFMs?_ and _(ii) How to harness 2D prior knowledge to boost 3D performance?_ To begin with, for issue _(i)_, visual prompt tuning [16; 20] is a parameter-efficient strategy to exploit the representational potential of VFMs. We consider freezing the whole VFMs and only learn several trainable lightweight blocks as supplementary input, which inherit parameters of VFMs trained at scale to the maximum extent. After that, for issue _(ii)_, we consider a more effective prompt tuning that introduces an extra depth-guided prompt space and extends the model input with the prompt, which could guide the generalization of powerful representations to achieve desirable performances.

Accordingly, in this paper, we introduce **UniDSeg**, dig deeper into prompt tuning in the VFM-based encoder, and introduce a _Learnable-parameter-inspired Mechanism_ to the off-the-shelf VFMs with frozen parameters. Our VFM-based encoder is designed to learn alternately between two lightweight modules: _i.e.,_ Modal Transitional Prompting (MTP) and Learnable Spatial Tunability (LST). The former depends on the transitional guidance from 3D-to-2D unnatural images, _i.e.,_ sparse depth, which exists in the prompt space before being fed into the encoder layer. The latter depends on the customized context length of vectors, which exists in the query space for seeking matched prompting after encoding in the layer. Hereby, depending on the number of VFM-based encoder layers involved, we place layer-wise MTP and LST blocks to take full advantage of semantic understanding of diverse levels and modalities. The proposed mechanism not only avoids any unnecessary attempts to manipulate the original visual space but also inherits the pre-existing target awareness from the VFMs to the maximum extent. Ultimately, by integrating the proposed two modules into a cross-modal learning framework, our method efficiently mitigates the domain gap and enables 2D and 3D models to learn domain-invariant representations.

The key contributions of our work are summarized as follows: 1) Our method is groundbreaking in introducing the prompt-tuning concept into the universal model for DG3SS and DA3SS tasks. 2) We propose a novel learnable-parameter-inspired mechanism to the off-the-shelf VFMs, which maximally preserves pre-existing target awareness in VFMs to further enhance its generalizability. 3) Extensive experimental results demonstrate the effectiveness of our method across widely recognized tasks and datasets, all achieving superior performance for DG3SS and DA3SS.

## 2 Related Works

### Domain Adaptive 3D Semantic Segmentation

In general, DA3SS seeks to narrow the domain gap between the source and target domain data, which can be grouped as uni-modal [61; 54; 48; 38; 56; 55; 24; 32] and multi-modal [19; 30; 35; 28; 58; 50; 45; 6; 5; 51; 13; 46] conditions. For uni-modality, early methods [48; 61] exploit the generative adversarial network to mitigate domain shift caused by appearance and sparsity differences. Later on, Yuan _et al._[55; 56] propose the adversarial network based on category-level and prototype-level alignments to address the mismatch of sampling patterns. CosMix  and ConDA  construct an intermediate domain by utilizing joint supervision signals from both the source and target domains for self-training. 3D surface representation is also considered an effective method. Complete&Label transforms domain adaptive task into a 3D surface completion task. SALUDA  learns an implicit underlying surface representation simultaneously on source and target data.

Compared to uni-modality, multi-modality exploits the exclusive information of paired images and point clouds to complement each other. xMUDA  is a pioneering method of cross-modal mutual learning for DA3SS. To facilitate learning domain-robust dependencies, several methods extend 2D techniques to learn the 3D domain-invariant representations, such as adversarial learning [30; 35; 58], style transfer , and contrastive learning . Based on these dependencies, SSE  presents a self-supervised learning mechanism from plane-to-spatial and discrete-to-textured representations. BFdD  presents cross-modal fusion-then-distillation to mitigate imbalanced modality adaptability. Recently, Segment Anything Model (SAM)  has presented its strong capability in generating semantics-aware regions, making it a suitable option for cross-modal interaction. MoPA  and VFMSeg  harness the knowledge priors learned from SAM to produce more accurate pseudo-labels for unlabeled target domains.

In contrast to DA3SS, where the inputs in the target domain, although without 3D labels, are accessible during the training process, DG3SS is evaluated on data from totally unseen target domains.

### Domain Generalized 3D Semantic Segmentation

The goal of DG3SS is to first learn as generic and robust representations as possible before being exposed to any target-domain data during training. Research on DG3SS has recently witnessed a surge, as highlighted in several studies, including uni-modal condition [21; 37; 39; 40; 49; 60] and multi-modal condition [27; 14]. Kim _et al._ leverage sparsity invariant feature consistency at the feature level and semantic correlation consistency at the output level to constrain the model. Ryu _et al._ and Sanchez _et al._ incorporate multi-frame aggregation with 6-DoF ego-motions via randomized LiDAR configurations augmentation and label propagation, respectively. However, these methods are limited when 6-DoF ego-motion is unknown, and they need to be estimated using the off-the-shelf LiDAR SLAM method . Recently, 3D representation under Bird's-Eye-View (BEV) has also been considered an effective method for learning domain-invariant features. LiDOG  introduces a dense top-down prediction auxiliary task and supervises it by employing BEV-view of semantic labels, while BEV-DG  introduces a BEV-view of cross-modal representation fusion to alleviate the domain gap. Particularly, 3D scenes also exist in several adverse weather conditions including fog, snow, and rain . UniMix  leverages physically valid adverse weather simulation to construct a bridge domain and then blends it with samples of normal weather conditions.

Our endeavor is tailored to designing a universal cross-domain multi-modal learning framework that enhances the performance of both DG3SS and DA3SS.

## 3 Method

### Preliminary

Problem Definition.Given a source domain \(_{S}=\{(X_{i}^{2D,S},X_{i}^{3D,S},Y_{i}^{3D,S})\}_{i=1}^{n_{s}}\) with \(n_{s}\) labeled data and a target domain \(_{T}=\{(X_{i}^{2D,T},X_{i}^{3D,T})\}_{i=1}^{n_{t}}\) with \(n_{t}\) unlabeled data under the condition that the source and target data distributions are not equal. For DA3SS, the task seeks to adapt models trained on the source domain \(_{S}\) to a target domain \(_{T}\) without labels. For DG3SS, the task aims to exploit the knowledge from the source domain to achieve generalization to the target domain, while the model remains unexposed to the target domain during training, _i.e.,_\(_{T}\) is unseen. Both tasks are framed with the expectation of having paired images and point clouds and learning a mapping function \(f:X^{2D,T},X^{3D,T} Y^{3D,T}\) that could predict the target-domain 3D labels.

Revisiting ViT.Given a pre-trained Vision Transformer (ViT)  model with \(L\) layers, an input is divided into \(M=}\) fixed-size patches \(x_{m}^{}} D},m=1,2,...,M\), where \(H\) and \(W\) are the height and width of the original input, \(P\) is the patch size, and \(D\) is the constant latent vector size through all of its layers. Each patch is then embedded into \(D\)-dimensional latent space:

\[e_{0}^{m}=Embed(x_{m}), e_{0}^{m}^{D}.\] (1)

We indicate the collection of 2D patch embeddings, \(E_{l-1}=\{e_{l-1}^{m}^{D}|1 m M\}\), as input to the \(l\)-th ViT encoder layer \(L_{l}()\). Specially, \(E_{0}^{pre}^{M D}\) learns beforehand together with class encoding \(E_{cls}^{1 D}\) and positional encoding \(E_{pos}^{(1+M) D}\) to create initial patch embedding \(E_{0}\). Hence, the whole ViT encoder is formulated as:

\[E_{0}=(E_{cls} E_{0}^{pre})+E_{pos},\] (2)

\[E_{l}=L_{l}(E_{l-1}), l=1,2,...,L,\] (3)

where \(E_{l}^{(1+M) D}\) is the patch embedding output from \(L_{l}()\), \(\) denotes concatenation on the sequence length dimension. Each layer \(L_{l}\) consists of several multi-head self-attention modules and feed-forward networks together with skip connection  and LayerNorm .

### Learnable-parameter-inspired Mechanism

Overall Framework.As depicted in Fig. 1, the overall framework of UniDSeg is Seg is decomposed to a 2D network \(^{2D}=^{2D}^{2D}^{2D}\) and a 3D network \(^{3D}=^{3D}^{3D}^{3D}\), where \(^{()}\), \(^{()}\), and \(^{()}\) denote encoder, decoder, and classifier, respectively. The main insight of UniDSeg is to provide a universal framework that enhances the generalizability and adaptability of cross-domain 3D semantic segmentation. Thereby, we introduce a novel task-specific VFM-based encoder, which is guided by point-level prompts from 3D information. Formally, point cloud \(X_{i}^{3D}\) is input to 3D network \(^{3D}\) to generate 3D prediction, while image \(X_{i}^{2D}\) and sparse depth \(X_{i}^{Dep}\) are input to 2D network \(^{2D}\) to generate 2D prediction with the help of VFMs priors. \(X_{i}^{Dep}\) is derived from the LiDAR sensor via perspective projection. Afterward, we employ cross-modal learning on the source/target predictions via auxiliary classifier \(_{aux}^{()}\). In summary, we place layer-wise learnable blocks to take full advantage of semantic understanding of diverse levels and modalities, which inherits potential target information of VFMs into the current training model. Our method is parameter-efficient and could be directly deployed to various pre-trained transformer-based architectures without modifying the basic units.

VFM-based Encoder.Since the feature extraction of VFMs still lacks task-specific information, freezing parameters may lose some domain-agnostic contextual semantic information from source-domain data or fine-tuning parameters may alter some pre-existing target representation from large-scale pre-trained data. To provide a remedy, we reconsider the role of pre-trained models, balancing the capture of contextual information and the overfitting issue of source-domain data. As illustrated in Fig. 2, we introduce a _Learnable-parameter-inspired Mechanism_, which provides a set of continuous embedding, _i.e.,_ 3D-to-2D transitional prompts and tunable deep queries. The former not only learns spatial distance perception prompts from point clouds but also learns invariance to sample perturbations. The latter ensures that the feature distribution of \(_{l}\) will not be modified drastically, thus making better use of the pre-trained knowledge from VFMs. Only these prompts and queries are updated during fine-tuning, while parameters of other layers \(L_{l}\) of the ViT encoder are kept frozen.

Depending on the number of ViT layers involved, our VFM-based encoder is designed to learn alternately between two lightweight modules: _Modal Transitional Prompting \(PG_{l}(,)\)_ and _Learnable Spatial Tunability \(TB_{l}()\)_. The former depends on the transitional guidance from 3D-to-2D unnatural images, which exists in the prompt space before being fed into layer \(L_{l}\), while the latter depends on the customized context length of vectors, which exists in the query space for seeking matched prompting after encoding in layer \(L_{l}\). With deliberate design, the newer task-specific ViT encoder can be decomposed into:

\[_{l}=L_{l}(_{l-1})+TB_{l}(L_{l}(_{l-1})), l=1,2,...,L,\] (4)

Figure 1: Overall framework of UniDSeg for DG3SS and DA3SS. The backbone of the VFM-based Encoder is frozen and only trains several learnable modules. “Samp.” means sampling of 2D features. Only the points falling into the intersected field of view are geometrically associated with multi-modal data.

\[_{l-1}[1:,:]=E_{l-1}[1:,:]+PG_{l}(X^{2D},X^{Dep}),\] (5)

where \(_{L}\) is the final refined patch embedding output from our proposed VFM-based encoder. According to the DG or DA task, \(X^{2D}^{H W 3}\) means source or target image and \(X^{Dep}^{H W 1}\) means source or target sparse depth. Note that as Eq. (5) shows, the prompt output from \(PG_{l}\) is aligned to the patch embedding without contacting \(E_{cls}\).

Modal Transitional Prompting.Previous methods  have proved that visual prompt-tuning brings flexibility to the pre-trained VFMs for downstream tasks. However, their prompts like pixel-level perturbations or learnable vectors are black boxes with limited learning capacity, which cannot reliably explore convincing knowledge beneficial for cross-domain 3D semantic segmentation. Lee _et al._ have proved that source data internally know much more about the world and how the scene is formed, called Privileged Information. Therefore, \(PG_{l}\) is designed to capture 3D-to-2D transitional prior and task-shared knowledge of this information from the prompt space, which might be useful for cross-domain learning.

To achieve this goal, following Eq. (1), a patch embed module borrowed from the plain ViT is re-initialized and applied to obtain a sequence of flattened patch embeddings \(E_{0}^{2D},E_{0}^{Dep}^{M D}\). Particularly, sparse depth \(X^{Dep}\) as a point cloud representation via perspective projection, presents an unnatural image. From the view of modal characteristics, it is easy to access and contains the prior spatial distance information that is lacking in 2D representation. From the view of deep encoding, it focuses on the scope of scenes at different receptive fields, tightly coupled with semantic information, so that their corresponding features have different contents when constructed. Note that \(X^{Dep}\) only has the 1-channel to show the depth value in camera coordinates, we repeat the depth channel to make it equal to the number of RGB channels. In addition, considering that low-frequency bands of the amplitude spectrum tend to capture style information (or low-level statistics) , we provide a simple way to perturb the amplitude spectra of the source image to ensure that the VFM is exposed to more variations in low-frequency components during training. The perturbed images can be obtained by using a low-frequency filter, and then Eq. (1) is also employed to generate corresponding flattened patch embedding \(E_{0}^{LF}^{M D}\). After that, to effectively integrate prompt-tuning to visual embedding output from each frozen layer, we flexibly build a lightweight layer that contains exclusive MLP \(_{l}()\) for adapting \(l\)-th encoder layer and shared MLP \(()\) across whole ViT. This implementation is written as:

\[E_{l}^{PG}=(_{l}(E_{0}^{2D} E_{0}^{LF} E_{0}^{Dep})),\] (6)

where \(E_{l}^{PG}^{M D}\) is the 3D-to-2D transitional prompt output from \(PG_{l}\). MLP is built with a two-layer bottleneck structure (Linear-GELU-Linear) with the hidden layer reducing the input dimension by 3\(\).

Figure 2: The architecture of VFM-based Encoder. We explore two layer-wise learnable blocks: (a) Modal Transitional Prompting and (b) Learnable Spatial Tunability. During training, only the parameters of two modules are updated while the whole ViT encoder layer is frozen.

Learnable Spatial Tunability.To progressively improve feature generalization via 3D-to-2D transitional prompt, inspired by [31; 44], \(TB_{l}\) is introduced to bridge the discrepancy between the pre-training dataset and the target scene. To achieve this goal, after the \(l\)-th layer, \(TB_{l}\) starts with a set of learnable tokens \(O_{l}^{K D}\), where \(K\) is the length of the token and each token is initialized randomly. Considering the essential need in 3D semantic segmentation to discern multiple instances within a framed scene, \(TB_{l}\) exploits an attention mechanism, which enables VFMs to seek matched prompting to the features of distinct instances, thereby assisting VFMs in adapting to the differences between pre-training and cross-domain data. Concretely, \(TB_{l}\) adopts a dot-product operation to generate the affinity matrix \(J_{l}\), which captures the associations between prompted patch embedding \(L_{l}(_{l-1})\) and learnable tokens \(O_{l}\). Then, a _SoftMax_ function is applied to align each patch with a unique instance. This implementation is written as:

\[J_{l}=SoftMax((_{l-1})[1:,:] O_{l}^{}}{}), J_{l}^{M K},\] (7)

\[O_{l}=O_{l,a} O_{l,b},\] (8)

where \(O_{l,a}^{K D_{m}}\) and \(O_{l,b}^{D_{m} D}\) are constructed as low-rank matrices , reducing the trainable parameters by learning pairs of rank-decomposition matrices. Note that the low-rank dimension \(D_{m} D\) (\(D_{m}=32\) in our case). Besides, we further process the embeddings through a down-projection layer with parameters \(W_{down}^{K D_{m}}\) followed by an up-projection layer with parameters \(W_{up}^{D_{m} D}\), which is then element-wise added via the residual connection. After that, to enhance the flexibility in feature adjustment, \(TB_{l}\) employs several layers to produce:

\[E_{l}^{TB}=_{2}(L_{l}(_{l-1})[1:,:]+W_{up}^{}(W_{down }^{} L_{l}(_{l-1})[1:,:])+J_{l}_{1}(O_{l})),\] (9)

where \(E_{l}^{TB}^{M D}\) is the tunable deep query output from \(TB_{l}\), \(_{1}()\) and \(_{2}()\) are linear layers.

### Cross-modal Learning

The point-wise supervised segmentation loss of the source domain is formulated as follows:

\[_{seg}=-_{n=1}^{N}_{c=1}^{C}Y_{(n,c)}^{ 3D,S}_{(n,c)}^{S},\] (10)

where main prediction \(^{S}\) is either \(^{2D,S}\) or \(^{3D,S}\), \(N\) and \(C\) being the number of points of the source point cloud and the number of classes, respectively.

The goal of unsupervised learning across modalities is two-fold. Firstly, we want to transfer knowledge from 2D modality to 3D modality on the source-domain and target-domain dataset. Secondly, we devise mutual learning on the output level, where the task is to transfer the pre-existing target information of VFMs to a 3D model. Same to xMUDA , we choose the Kullback-Leibler divergence \(D_{KL}(||)\) for the cross-modal loss \(_{xM}\) and define it as follows:

\[_{xM}^{S}=D_{KL}(^{2D,S}||^{3D,S 2D,S})+D _{KL}(^{3D,S}||^{2D,S 3D,S}),\] (11)

\[_{xM}^{T}=D_{KL}(^{2D,T}||^{3D,T 2D,T})+D _{KL}(^{3D,T}||^{2D,T 3D,T}),\] (12)

where \(^{2D,S}\) and \(^{3D,S}\) is to be estimated by mimicking predictions \(^{3D,S 2D,S}\) and \(^{2D,S 3D,S}\) from the respective auxiliary classifiers \(_{aux}^{2D}\) and \(_{aux}^{3D}\). The same goes for target predictions. Ultimately, the overall loss function \(_{DG}\) of DG3SS and \(_{DA}\) of DA3SS are defined as:

\[_{DG}=_{seg}+_{S}_{xM}^{S},\] (13)

\[_{DA}=_{seg}+_{S}_{xM}^{S}+_{T} _{xM}^{T},\] (14)

where \(_{S}\) and \(_{T}\) are the weights trading off cross-modal loss on source and target domain inputs.

## 4 Experiments

### Datasets

For evaluation, we use four public autonomous driving benchmarks, including three real datasets: _nuScenes_, _SemanticKITTI_, _A2D2_ and one synthetic dataset: _VirtualKITTI_. For all real datasets, LiDAR sensor and RGB cameras are synchronized and calibrated, allowing 2D-to-3D projection, and for the synthetic dataset, _VirtualKITTI_ provides depth maps so we simulate LiDAR scanning via uniform point sampling. Following DA3SS settings , we exclusively utilize the front camera image and the corresponding LiDAR points that are projected onto it.

Our experimental scenarios cover typical real-to-real single domain adaptation and generalization challenges like lighting changes (_nuScenes: Day/Night_), scene layout of country (_nuScenes: USA/Sing._; _nuScenes: Sing./USA_), and sensor setups (_A2D2/sKITTI_; _A2D2/nuScenes_). Note that, the source and target classes are coincident. For the first three scenarios, we choose 6 merged classes while for the last two scenarios, we select 10 and 8 shared classes between two datasets. In addition, the synthetic-to-real domain adaptation and generalization challenges are also considered (_vKITTI/sKITTI_, simulated depth, and RGB to real LiDAR and camera, with 6 merged classes). Details of the class partition are provided in supplementary materials.

### Implementation Details

For the 2D backbone, we use the visual encoders of CLIP  and SAM , a large-scale pre-training model. Our selection of Vision Transformer  includes ViT-Base (ViT-B) and ViT-Large (ViT-L) architectures. Then, we utilize the MMSegmentation  codebase for the decoder head, SemanticFPN , a widely-used segmentation head, is integrated with the visual encoder that serves as the 2D backbone. Meanwhile, depth information is a 3D attribute derived from LiDAR sensors, without the need to introduce additional datasets, thus ensuring fairness under equal experimental conditions. For the 3D backbone, we employ SparseConvNet  with a U-Net architecture in Sparse Convolution Library . The voxel size is set to 5cm in the 3D network. This voxel size ensures that each voxel contains only one 3D point, maintaining a level of granularity suitable for the task.

Our model is trained on "nuScenes:Day/Night", "A2D2/sKITTI", and "A2D2/nuScenes" for 100k iterations. We utilize an iteration-based learning schedule where the initial learning rate is set to 1e-3 except for the 2D encoder which is 1e-4, and then it is divided by 10 at 80k and 90k iterations. For "nuScenes:USA/Sing." and "nuScenes:Sing./USA", the training is performed for 60k iterations, and the learning rate is divided by 10 at the 40k and 50k iterations. For vKITTI/sKITTI, the training is performed for 30k iterations, and the learning rate is divided by 10 at the 25k and 28k iterations. The batch size is set to 8. As regards the hyper-parameters, following , \(_{S}\) and \(_{T}\) in cross-modal loss are set to 1.0 and 0.1 on "nuScenes:Day/Night", "nuScenes:USA/Sing.", and "nuScenes:Sing./USA", 0.1 and 0.01 on "vKITTI/sKITTI", "A2D2/sKITTI", and "A2D2/nuScenes" respectively, without performing any fine-tuning on these values. All experiments are conducted on NVIDIA RTX 3090.

    &  &  &  &  \\  Task & Method & 2D & 3D & xM & 2D & 3D & xM & 2D & 3D & xM & 2D & 3D & xM \\   & Source-only & 58.4 & 62.8 & 68.2 & 47.8 & 68.8 & 63.3 & 26.8 & 42.0 & 42.2 & 34.2 & 35.9 & 40.4 \\   & logCORAL  & 64.4 & 63.2 & 69.4 & 47.7 & 68.7 & 63.7 & 41.4 & 36.8 & 47.0 & 35.1 & 41.0 & 42.2 \\  & MinEnt  & 57.6 & 61.5 & 66.0 & 47.1 & 68.8 & 63.6 & 39.2 & 43.3 & 47.1 & 37.8 & 39.6 & 42.6 \\  & BDL  & 62.0 & 64.8 & 70.4 & 47.0 & 69.6 & 63.0 & 21.5 & 44.3 & 35.6 & 34.7 & 41.7 & 45.2 \\   & xMUDA  & 64.4 & 63.2 & 69.4 & 55.5 & 69.2 & 67.4 & 42.1 & 46.7 & 48.2 & 38.3 & 46.0 & 44.0 \\  & AUDA  & 64.0 & 64.0 & 69.2 & 55.6 & 69.8 & 64.8 & 35.8 & 37.8 & 41.3 & 43.0 & 43.6 & 46.8 \\  & DSCML  & 65.6 & 56.2 & 66.1 & 50.9 & 49.3 & 53.2 & 38.4 & 38.4 & 45.5 & 39.6 & 45.1 & 44.5 \\  & Dual-Cross  & 64.7 & 58.1 & 66.5 & 58.5 & 69.7 & 68.0 & 40.7 & 35.1 & 44.2 & 45.9 & 40.0 & 48.6 \\  & SSE  & 64.9 & 63.9 & 69.2 & 62.8 & 69.0 & 68.9 & 45.9 & 40.0 & 49.6 & 44.5 & 46.8 & 48.4 \\  & BHD  & 63.7 & 62.2 & 69.4 & 57.1 & 70.4 & 68.3 & 41.5 & 45.5 & 51.5 & 40.5 & 44.4 & 48.7 \\  & MMD23D  & 71.7 & 66.8 & 72.4 & 70.5 & 70.2 & **72.1** & 53.4 & 50.3 & 56.5 & 42.3 & 46.1 & 46.2 \\  & VFMSeg  & 70.0 & 65.6 & 72.3 & 60.6 & 70.5 & 66.5 & 57.2 & 52.0 & 61.0 & 45.0 & 52.3 & 50.0 \\  & **UniDSeg** & 67.2 & 67.6 & **72.9** & 63.2 & 71.2 & 71.2 & 60.5 & 50.9 & **62.0** & 50.7 & 55.4 & **57.5** \\   & xMUDA  & 58.7 & 62.3 & 68.6 & 43.0 & 68.9 & 59.6 & 25.7 & 37.4 & 39.0 & 34.9 & 36.7 & 41.6 \\  & MM2D3D  & 69.7 & 62.3 & 70.9 & 65.3 & 63.2 & 68.3 & 37.7 & 40.2 & 44.2 & 39.6 & 35.9 & 43.6 \\   & **UniDSeg** & 66.5 & 64.5 & **72.3** & 57.0 & 70.5 & **70.0** & 57.6 & 44.7 & **60.0** & 48.8 & 46.3 & **54.4** \\   

Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with **best** and **2nd** best) on the target testing set for each network as well as the ensemble result (_i.e._, \(}\)) by averaging the predicted probabilities from the 2D and 3D networks.

### Quantitative and Qualitative Comparison

We compare the proposed method with three classic 2D domain adaptive methods, which can be easily extended to multi-modal conditions. Moreover, eight multi-modal DA3SS and two multi-modal DG3SS are discussed. As shown in Tab. 1, we tabulate the comparison results in mean Intersection over Union (mIoU, %) on the target testing data. Note that not all datasets provide image labels. Thus the quantitative evaluation of 2D semantic segmentation depends on the 3D-2D corresponding point-wise prediction. Overall, our method achieves the best performance on all scenarios against the competitors, \(w.r.t.\) ensemble result "xM", except for DA3SS on "nuScenes:Day/Night" gets the second best. As shown in Fig. 3, we visualize the DG3SS results of four settings. By the merit of the visual foundation models, our method can segment detailed objects very well. From top to bottom, the focused areas are the sidewalk near the drivable surface, the trunk of a tree, the profile of the road, and most importantly, bicycles safely riding on the road.

Comparison of DG3SS.In terms of DG task, compared with baseline (xMUDA) and MM2D3D, our method still achieves remarkable results, exhibiting 3.7%/1.4%, 10.4%/1.7%, 21.0%/15.8%, and 12.8%/10.8% mIoU improvement. It is worth noting that our method, without exposure to any target-domain data, shows performance that is close to or even surpasses that of most DA3SS methods (_e.g._, the result of Ours-DG is comparable to VFMSeg on "nuScenes:USA/Sing."). All results demonstrate that utilizing the powerful open-vocabulary recognition capability of VFMs is beneficial for addressing generalization problems in domains with significant discrepancies.

Comparison of DA3SS.The source-only model is the lower bound, which is not domain adaptive as it is only trained on the source-domain data. It is observed that our method brings a significant adaptation effect on all scenarios compared to the source-only model, with the gains of 4.7\(\%\), 7.9\(\%\), 19.8\(\%\), and 17.1\(\%\) in mIoU, respectively. Compared with baseline (xMUDA) for all DA competitors, our method exceeds it by large margins with gains of 3.5\(\%\), 3.8\(\%\), 13.8\(\%\), and 13.5\(\%\) in mIoU. Particularly, on the "A2D2/skITTI" scenario, our method typically yields a higher score compared to the best "xM" in VFMSeg (57.5% vs. 50.0%).

### Ablation Study

Evaluation of Different VFMs Training Strategies.Our analysis of various training strategies for VFMs in six experimental scenarios within and across datasets can be found in Tab. 2. Note

Figure 3: Qualitative results of DG3SS. We showcase the ensembling result of four scenarios.

that due to the fixed and relatively small number of trainable parameters in the decode head, the count of trainable parameters presented in the tables is focused solely on the visual architecture. In this setup, we separately select CLIP:ViT-B and CLIP:ViT-L backbone with fine-tuning, freezing, and our proposed Learnable-parameter-inspired Mechanism for analysis. The results indicate that frozen VFMs outperform previous DA3SS methods without specialized design. In addition to "nuScenes:Day/Night", VFMs with full parameter fine-tuning exhibit enhanced performance relative to their frozen counterparts because the 3D modality is less sensitive to light, thus dominating in ensemble prediction direction. Remarkably, our method achieves even superior generalization capabilities, surpassing the full parameter fine-tuning 0.5\(\)2.7% mIoU with merely \(\)2% trainable parameters compared to the original backbone. This ablation experiment demonstrates that our method can inherit the pre-existing target awareness from the VFMs to the maximum extent.

Evaluation of Different Components.As shown in Tab. 3, we train four models on two scenarios for DG3SS, including (1) Frozen parameter of ViT backbone; (2) only performing MTP in the frozen ViT by using prompt tuning, leading to a significant mIoU boost (1.3% and 1.8%). This highlights the robustness of the 3D-to-2D transitional prompts in cross-domain learning; (3) only performing LST in the frozen ViT by customizing a learnable token, demonstrating that learnable tokens can encourage the model to learn domain-invariant representations (increasing 2.1% and 2.4% mIoU); (4) combining two proposed components to reach peak value.

Evaluation of Learnable Token Length.We illustrate the impact of learnable token length \(K\) on the overall 2D performance of UniDSeg with the ViT-B backbone for DG3SS. As shown in Fig. 4, the results demonstrate a consistent upward trend. When \(K\) is set to 100, the relatively small

    &  &  &  \\  Strategy & Visual Backbone & Params & 2D & 3D & xM & 2D & 3D & xM & 2D & 3D & xM \\    Finetune \\ Frozen \\ **Ours** \\  } & 86.9M & 62.4 & 64.1 & 69.6 & 53.3 & 70.7 & 68.8 & 65.7 & 67.9 & 72.9 \\  & CLIP:ViT-B & 0.0M & 59.7 & 64.5 & 69.7 & 46.8 & 71.0 & 69.8 & 58.3 & 67.9 & 71.2 \\  & 1.82M & 63.8 & 64.7 & **71.5** & 55.9 & 70.7 & **70.0** & 68.2 & 68.0 & **74.0** \\    Finetune \\ Frozen \\ **Ours** \\  } & 305M & 65.5 & 64.5 & 70.4 & 54.9 & 70.7 & 67.3 & 69.9 & 67.8 & 74.5 \\  & CLIP:ViT-L & 0.0M & 60.4 & 64.2 & 70.1 & 50.2 & 70.5 & 69.5 & 62.2 & 67.8 & 73.3 \\  & 4.70M & 66.5 & 64.5 & **72.3** & 57.0 & 70.5 & **70.0** & 70.6 & 68.0 & **75.1** \\    &  &  &  \\  Strategy & Visual Backbone & Params & 2D & 3D & xM & 2D & 3D & xM & 2D & 3D & xM \\    Finetune \\ Frozen \\ **Ours** \\  } & 86.9M & 54.9 & 41.5 & 55.8 & 43.0 & 43.8 & 51.5 & 55.4 & 50.1 & 60.2 \\  & CLIP:ViT-B & 0.0M & 49.1 & 42.0 & 54.4 & 35.3 & 43.8 & 48.7 & 51.2 & 49.4 & 58.1 \\  & 1.82M & 55.6 & 43.6 & **58.0** & 43.2 & 44.6 & **52.0** & 56.3 & 50.3 & **61.0** \\    Finetune \\ Frozen \\ **Ours** \\  } & 305M & 57.4 & 43.5 & 58.7 & 46.9 & 44.3 & 53.0 & 57.2 & 50.8 & 61.3 \\  & CLIP:ViT-L & 0.0M & 54.0 & 42.9 & 58.4 & 41.8 & 44.4 & 51.4 & 53.7 & 50.0 & 59.6 \\  & 4.70M & 57.6 & 44.7 & **60.0** & 48.8 & 46.3 & **54.4** & 58.0 & 50.7 & **61.9** \\   

Table 2: Ablation study on VFM-based encoder with different training strategies for DG3SS. This setup is based on the same LiDAR-Camera configuration but different environments (top-3 scenarios), and different LiDAR-Camera configurations (bottom-3 scenarios). Of note, “Params” denote trainable parameters in the encoder.

Figure 4: Effect of the learnable token length.

training parameters and high performance make it the preferred choice for subsequent experiments. Meanwhile, this observation suggests that the model benefits from incorporating visual information from multiple layers, enabling it to capture more nuanced and discriminative features.

Evaluation of Different 2D and 3D Backbones.The motivation of this work is to study a universal framework based on VFMs to enhance the generalizability and adaptability of cross-domain 3D semantic segmentation, demonstrating the effectiveness of the visual foundation model priors. However, CLIP is not designed for common surveillance-relevant tasks like semantic segmentation. Hereby, we dedicate ourselves to verifying its effectiveness on VFMs designed for segmentation tasks such as SAM . Firstly, in Tab. 4, we evaluate the effect of SAM as the 2D backbone. It is observed that SAM-based UniDSeg exhibits better performance on "USA/Sing" scenario, with a "xM" gain of 0.3% on DG and 0.4% on DA. Then, in Tab. 5, we evaluate the effect of applying different training strategies to the SAM-based model. It is observed that our Learnable-parameter-inspired Mechanism for tuning the SAM-based encoder can achieve 1.8% and 1.9% "xM" gain compared to fine-tuning strategy. In addition, we evaluate UniDSeg on another 3D backbone, _i.e.,_ MinkowskiNet, making it more convincing as a "universal" framework for cross-domain 3D semantic segmentation.

Evaluation of Computation Cost.In Tab. 7, we have reported model parameters of CLIP: ViT-B, CLIP: ViT-L, and SAM: ViT-L for the VFM-based encoder along with all trainable parameters. Note that, the entire ViT backbone in our VFM-based encoder is frozen during downstream training for DA3SS and DG3SS. Only two layer-wise learnable blocks, MTP and LST, are trainable. It is obvious that our method requires only 1-2% parameters optimization to exceed the fine-tuning results.

## 5 Conclusion

In this work, we delve into a universal method that can enhance the adaptability and generalizability of cross-domain 3D semantic segmentation, dubbed UniDSeg. For this purpose, we introduce a learnable-parameter-inspired mechanism to the off-the-shelf VFMs with frozen parameters, which maximally preserves pre-existing target awareness in VFMs, further enhancing the generalizability of VFMs. Our method achieves state-of-the-art performance in both DA3SS and DG3SS, as demonstrated by extensive experiments on different scenarios. We believe that our work will inspire a deeper investigation of cross-domain 3D semantic segmentation in autonomous driving.

Acknowledgments.This work was supported by the National Natural Science Foundation of China under Grant No.62176224, No.62306165; Natural Science Foundation of Chongqing under No.CSTB2023NSCQ-JQX0007; China Computer Federation (CCF) Lenovo Blue Ocean Research Fund; China Academy of Railway Sciences No.2023YJ357.

    &  &  \\   & & 2D & 3D & xM \\   & xMUDA & 64.4 & 63.2 & 69.4 \\  & UniDSeg & 67.2 & 67.6 & 72.9 \\   & xMUDA & 65.9 & 64.0 & 69.7 \\  & UniDSeg & 67.5 & 68.6 & 73.1 \\   

Table 6: Effect of using different 3D backbones on the DA3SS methods.

  
2D Backbone & Full Params & Trainable Params & Cost & MTP & LST \\  CLIP:ViT-B & 86.9M & 1.82M & **2.09\%** & 0.48M & 1.34M \\ CLIP:ViT-L & 305M & 4.70M & **1.54\%** & 1.78M & 2.92M \\ SAM:ViT-L & 307M & 4.34M & **1.41\%** & 1.42M & 2.92M \\   

Table 7: The parameters and computational costs of CLIP-based and SAM-based 2D backbones. “Cost” means the percentage of trainable parameters in MTP and LST compared to fine-tuning the whole encoder consumed.