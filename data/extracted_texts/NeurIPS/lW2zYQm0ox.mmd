# Accelerated Regularized Learning

in Finite \(N\)-Person Games

 Kpriakos Lotidis

Stanford University

klotidis@stanford.edu

Angeliki Giannou

University of Wisconsin-Madison

giannou@wisc.edu

Panayotis Mertikopoulos

Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP

LIG 38000 Grenoble, France

panayotis.mertikopoulos@imag.fr

Nicholas Bambos

Stanford University

bambos@stanford.edu

###### Abstract

Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games. To that end, we introduce a family of accelerated learning methods, which we call _"follow the accelerated leader"_ (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential / multiplicative weights algorithm and its variants. Drawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a _superlinear_ rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a _geometric_, linear rate). Importantly, FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even when run with bandit, payoff-based information, where players are only able to observe their individual realized payoffs.

## 1 Introduction

One of the most important milestones in convex optimization was Nesterov's accelerated gradient (NAG) algorithm, as proposed by Nesterov  in 1983. The groundbreaking achievement of Nesterov's algorithm was that it attained an \((1/T^{2})\) rate of convergence in Lipschitz smooth convex minimization problems, thus bridging a decades-old gap between the \((1/T)\) convergence rate of ordinary gradient descent and the corresponding \((1/T^{2})\) lower bound for said class . In this way, Nesterov's accelerated gradient algorithm opened the door to acceleration in optimization, leading in turn to a wide range of other, likewise influential schemes - such as FISTA and its variants  - and jumpstarting a vigorous field of research that remains extremely active to this day.

Somewhat peculiarly, despite the great success that NAG has enjoyed in all fields where optimization plays a major role - and, in particular, machine learning and data science - its use has not percolated to the adjoining field of game theory as a suitable algorithm for learning Nash equilibria. Historically, the reasons for this are easy to explain: despite intense scrutiny by the community and an extensive corpus of literature dedicated to deconstructing the algorithm's guarantees, NAG's update structure remains quite opaque - and, to a certain extent, mysterious. Because of this, Nesterov's algorithm could not be considered as a plausible learning scheme that could be employed by boundedly rational _human_ agents involved in a repeated game. Given that this was the predominant tenet in economic thoughtat the time, the use of Nesterov's algorithm in a game-theoretic context has not been extensively explored, to the best of our knowledge.

On the other hand, as far as applications to machine learning and artificial intelligence are concerned, the focus on _human_ agents is no longer a limiting factor. In most current and emerging applications of game-theoretic learning - from multi-agent reinforcement learning to adversarial models in machine learning - the learning agents are algorithms whose computational capacity is only limited by the device on which they are deployed. In view of this, our paper seeks to answer the following question:

_Can Nesterov's accelerated gradient scheme be deployed in a game-theoretic setting?_

_And, if so, is it possible to achieve similar performance gains as in convex optimization?_

**Our contributions in the context of related work.** The answer to the above questions is not easy to guess. On the one hand, given that game theory and convex optimization are fundamentally different fields, a reasonable guess would be "no" - after all, finding a Nash equilibrium is a PPAD-complete problem , whereas convex minimization problems are solvable in polynomial time . On the other, since in the context of online learning each player _would_ have every incentive to use the most efficient unilateral optimization algorithm at their disposal, the use of NAG methods cannot be easily discarded from an algorithmic viewpoint.

Our paper examines if it is possible to obtain even a partially positive answer to the above question concerning the application of Nesterov's accelerated gradients techniques to learning in games. We focus throughout on the class of finite \(N\)-person games where, due to the individual concavity of the players' payoff functions, the convergence landscape of online learning in games is relatively well-understood - at least, compared to non-concave games. In particular, it is known that regularized learning algorithms - such as "follow the regularized leader" (FTRL) and its variants - converge locally to strict Nash equilibria at a geometric rate , and strict equilibria are the only locally stable and attracting limit points of regularized learning in the presence of randomness and/or uncertainty . In this regard, we pose the question of (\(i\)) whether regularized learning schemes like FTRL can be accelerated; and (\(ii\)) whether the above properties are enhanced by this upgrade.

We answer both questions in the positive. First, we introduce an accelerated regularized scheme, in both continuous and discrete time, which we call _"follow the accelerated leader"_ (FTXL). In continuous time, our scheme can be seen as a fusion of the continuous-time analogue of NAG proposed by Su, Boyd, and Candes  and the dynamics of regularized learning studied by Mertikopoulos & Sandholm  - see also  and references therein. We show that the resulting dynamics exhibit the same qualitative equilibrium convergence properties as the replicator dynamics of Taylor & Jonker  (the most widely studied instance of FTRL in continuous time). However, whereas the replicator dynamics converge to strict Nash equilibria at a linear rate, the FTXL dynamics converge _superlinearly_.

In discrete time, we likewise propose an algorithmic implementation of FTXL which can be applied in various information context: (\(i\)) _full information_, that is, when players observe their entire mixed payoff vector; (\(ii\)) _realization-based feedback_, i.e., when players get to learn the "what-if" payoff of actions that they did not choose; and (\(iii\)) _bandit, payoff-based feedback_, where players only observe their realized, in-game payoff, and must rely on statistical estimation techniques to reconstruct their payoff vectors. In all cases, we show that FTXL maintains the exponential speedup described above, and converges to strict Nash equilibria at a superlinear rate (though the subleading term in the algorithm's convergence rate becomes increasingly worse as less information is available). We find this feature of FTXL particularly intriguing as superlinear convergence rates are often associated to methods that are second-order in _space_, not _time_; the fact that this is achieved even with bandit feedback is quite surprising in this context.

Closest to our work is the continuous-time, second-order replicator equation studied by Laraki & Mertikopoulos  in the context of evolutionary game theory, and derived through a model of pairwise proportional imitation of "long-term success". The dynamics of  correspond to the undamped, continuous-time version of FTXL with entropic regularization, and the equilibrium convergence rate obtained by  agrees with our analysis. Other than that, the dynamics of Flam & Morgan  also attempted to exploit a Newtonian structure, but they do not yield favorable convergence properties in a general setting. The inertial dynamics proposed in  likewise sought to leverage an inertial structure combined with the Hessian-Riemannian underpinnings of the replicatordynamics, but the resulting replicator equation was not even well-posed (in the sense that its solutions exploded in finite time).

More recently, Gao & Pavel  considered a second-order, inertial version of the dynamics of mirror descent in continuous games, and examined their convergence in the context of variational stability . Albeit related at a high level to our work (given the link between mirror descent and regularized learning), the dynamics of Gao & Pavel  are actually incomparable to our own, and there is no overlap in our techniques or results. Other than that, second-order dynamics in games have also been studied in continuous time within the context of control-theoretic passivity, yielding promising results in circumventing the impossibility results of Hart & Mas-Colell , cf. Gao & Pavel , Mabrok & Shamma , Toonsi & Shamma , and references therein. However, the resulting dynamics are also different, and we do not see a way of obtaining comparable rates in our setting.

## 2 Preliminaries

In this section, we outline some notions and definitions required for our analysis. Specifically, we introduce the framework of finite \(N\)-player games, we discuss the solution concept of a Nash equilibrium, and we present the main ideas of regularized learning in games.

### Finite games

In this work, we focus exclusively with finite games in normal form. Such games consist of a finite set of _players_\(=\{1,,N\}\), each of whom has a finite set of _actions_ - or _pure strategies_ - \(_{i}_{i}\) and a _payoff function_\(u_{i}:\), where \(_{i}_{i}\) denotes the set of all possible action profiles \(=(_{1},,_{N})\). To keep track of all this, a finite game with the above primitives will be denoted as \((,,u)\).

In addition to pure strategies, players may also randomize their choices by employing _mixed strategies_, that is, by choosing probability distributions \(x_{i}_{i}(_{i})\) over their pure strategies, where \((_{i})\) denotes the probability simplex over \(_{i}\). Now, given a _strategy profile_\(x=(x_{1},,x_{N})_{i}_{i}\), we will use the standard shorthand \(x=(x_{i};x_{-i})\) to highlight the mixed strategy \(x_{i}\) of player \(i\) against the mixed strategy profile \(x_{-i}_{-i}_{j i}_{j}\) of all other players. We also define:

1. The _mixed payoff_ of player \(i\) under \(x\) as \[u_{i}(x)=u_{i}(x_{i};x_{-i})=_{_{1}_{1}}_{ _{N}_{N}}x_{1}_{1} x_{N_{N}}u_{i}( _{1},,_{N})\] (1)
2. The _mixed payoff vector_ of player \(i\) under \(x\) as \[v_{i}(x)=_{x_{i}}u_{i}(x)=(u_{i}(_{i};x_{-i}))_{_{i} _{i}}\] (2)

In words, \(v_{i}(x)\) collects the expected rewards \(v_{i_{i}}(x) u_{i}(_{i};x_{-i})\) of each action \(_{i}_{i}\) of player \(i\) against the mixed strategy profile \(x_{-i}\) of all other players. Finally, we write \(v(x)=(v_{1}(x),,v_{N}(x))\) for the concatenation of the players' mixed payoff vectors.

In terms of solution concepts, we will say that \(x^{*}\) is a _Nash equilibrium_ (NE) if no player can benefit by unilaterally deviating from their strategy, that is

\[u_{i}(x^{*}) u_{i}(x_{i};x^{*}_{-i})_{i}$ and all $i$}\,.\] (NE)

Moreover, we say that \(x^{*}\) is a _strict Nash equilibrium_ if (NE) holds as a strict inequality for all \(x_{i} x^{*}_{i}\), \(i\), i.e., if any deviation from \(x^{*}_{i}\) results in a strictly worse payoff for the deviating player \(i\). It is straightforward to verify that a strict equilibrium \(x^{*}\) is also _pure_ in the sense that each player assigns positive probability only to a single pure strategy \(^{*}_{i}_{i}\). Finally, we denote the _support_ of a strategy \(x\) as the set of actions with non-zero probability mass, i.e., \((x)=\{:x_{}>0\}\).

### Regularized learning in games

In the general context of finite games, the most widely used learning scheme is the family of algorithms and dynamics known as _"follow the regularized leader"_ (FTRL). In a nutshell, the main idea behind FTRL is that each player \(i\) plays a "regularized" best response to their cumulative payoff over time, leading to the continuous-time dynamics

\[_{i}(t)=v_{i}(x(t)) x_{i}(t)=Q_{i}(y_{i}(t))\] (FTRL-D)where

\[Q_{i}(y_{i})=_{x_{i}_{i}}\{ y_{i},x_{i}-h_{i}( x_{i})\}\] (3)

denotes the _regularized best response_ - or _mirror_ - map of player \(i\), and \(h_{i}_{i}\) is a strongly convex function known as the method's _regularizer_. Accordingly, in discrete time, this leads to the algorithm

\[y_{i,n+1}=y_{i,n}+_{i,n} x_{i,n}=Q_{i}(y_{i,n})\] (FTRL)

where \(>0\) is a hyperparameter known as the algorithm's _learning rate_ (or _step-size_) and \(_{i,n}\) is a black-box "payoff signal" that carries information about \(v_{i}(x_{n})\). In the simplest case, when players have full information about the game being played and the actions taken by their opponents, we have \(_{i,n}=v_{i}(x_{n})\); in more information-depleted environments (such as learning with payoff-based, bandit feedback), \(_{i,n}\) is a reconstruction of \(v_{i}(x_{n})\) based on whatever information is at hand.

For concreteness, we close this section with the prototypical example of FTRL methods, the _exponential/multiplicative weights_ (EW) algorithm. Going back to , this method is generated by the negentropy regularizer \(h_{i}(x_{i})=_{_{i}_{i}}x_{i\,_{i}} x_{i _{i}}\), which yields the EW update rule

\[y_{i,n+1}=y_{i,n}+_{i,n} x_{i,n}=_{i}(y_{i,n}) )}{\|(y_{i,n})\|_{1}}\] (EW)

and, in the continuous-time limit \( 0\), the _exponential weights dynamics_

\[_{i}(t)=v_{i}(x(t)) x_{i}(t)=_{i}(y_{i}(t))\;.\] (EWD)

In the above, \(_{i}\) denotes the regularized best response induced by the method's entropic regularizer, which is known colloquially as a _logit best response_ - or, even more simply, as the _logit map_. To make the notation more compact in the sequel, we will write \(Q=(Q_{i})_{i}\) and \(=(_{i})_{i}\) for the ensemble of the players' regularized / logit best response maps.

_Remark 1_.: To streamline our presentation, in the main part of the paper, quantitative results will be stated for the special case of the EW setup above. In Appendix A, we discuss more general decomposable regularizers of the form \(h_{i}(x_{i})=_{_{i}_{i}}_{i}(x_{i})\) where \(_{i}\) is continuous on \(\), and has \(^{}(x)>0\) for all \(x(0,1]\) and \(_{x 0^{}}^{}(x)=-\). Although this set of assumptions can be relaxed, it leads to the clearest presentation of our results, so it will suffice for us.

_Remark 2_.: Throughout the paper, we will interchangeably use \((t)\) and \(dg/dt\) to denote the time derivative of \(g(t)\). This dual notation allows us to adopt whichever form is most convenient in the given context. Moreover, for a process \(g\), we will use the notation \(g(t)\) for \(t 0\) if it evolves in continuous time, and \(g_{n}\) for \(n\) if it evolves in discrete time steps, omitting the time-index when it is clear from context.

## 3 Combining acceleration with regularization: First insights and results

In this section, we proceed to illustrate how Nesterov's accelerated gradient (NAG) method can be combined with FTRL. To keep things as simple as possible, we focus on the continuous-time limit, so we do not have to worry about the choice of hyperparameters, the construction of black-box models for the players' payoff vectors, etc.

### Nesterov's accelerated gradient algorithm

We begin by discussing Nesterov's accelerated gradient algorithm as presented in Nesterov's seminal paper  in the context of unconstrained smooth convex minimization. Specifically, given a Lipschitz smooth convex function \(f^{d}\), the algorithm unfolds iteratively as

\[ x_{n+1}&=w_{n}- f(w_{n}) \\ w_{n+1}&=x_{n+1}+(x_{n+1}-x_{n}) \] (NAG)

where \(w_{1}=x_{1}\) is initialized arbitrarily and \(>0\) is a step-size parameter (typically chosen as \( 1/L\) where \(L\) is the Lipschitz smoothness modulus of \(f\)). The specific iterative structure of (NAG) - and, in particular the "3" in the denominator - can appear quite mysterious; nevertheless, (NAG) otherwise offers remarkable perfomance gains, improving in particular the rate of convergence of gradient methods from \((1/T)\) to \((1/T^{2})\), and matching in this way the corresponding \((1/T^{2})\) lower bound for the minimization of smooth convex functions .1

This groundbreaking result has since become the cornerstone of a vast and diverse literature expanding on the properties of (NAG) and trying to gain a deeper understanding of the "how" and "why" of its update structure. One perspective that has gained significant traction in this regard is the continuous-time approach of Su et al. ; combining the two equations in (NAG) into

\[-2\,x_{n}+x_{n-1}}{}=-\, f(w_{n}) --x_{n-1}}{},\] (4)

they modeled (NAG) as a _heavy ball with vanishing friction_ system of the form

\[x}{dt^{2}}=- f(x)-\] (HBVF)

The choice of terminology alludes to the fact that (HBVF) describes the dynamics of a heavy ball descending the landscape of \(f\) under the potential field \(F(x)=- f(x)\) with a vanishing kinetic friction coefficient (the \(3/t\) factor in front of the momentum term \(dx/dt\)). In this interpretation, the mass of the ball accelerates the system, the friction term dissipates energy to enable convergence, and the vanishing friction coefficient quenches the impact of friction over time in order to avoid decelerating the system too much (so the system is, in a sense, "critically underdamped").

As was shown by Su et al. , an explicit Euler discretization of (HBVF) yields (NAG) with exactly the right momentum coefficient \(n/(n+3)\); moreover, the rate of convergence of the continuous-time dynamics (HBVF) is the same as that of the discrete-time algorithm (NAG), and the energy function and Lyapunov analysis used to derive the former can also be used to derive the latter. For all these reasons, (HBVF) is universally considered as the _de facto_ continuous-time analogue of (NAG), and we will treat it as such in the sequel.

### NAG meets FTRL

To move from unconstrained convex minimization problems to finite \(N\)-person games - a constrained, non-convex, multi-agent, multi-objective setting - it will be more transparent to start with the continuous-time formulation (HBVF). Indeed, applying the logic behind (HBVF) to the (unconstrained) state variables \(y\) of (FTRL-D), we obtain the _"follow the accelerated leader"_ dynamics

\[y}{dt^{2}}=v(Q(y))-\] (FTXL-D)

where the dynamics' driving force \(F(y)=v(Q(y))\) is now given by the payoff field of the game, and the factor \(r/t\), \(r 0\), plays again the role of a vanishing friction coefficient. To avoid confusion, we highlight that in the case of regularized learning, the algorithm's variable that determines the evolution of the system in an autonomous way is the "score variable" \(y\), not the "strategy variable" \(x\) (which is an ancillary variable obtained from \(y\) via the regularized choice map \(Q\)).

In contrast to (EWD), the accelerated dynamics (FTXL-D) are second-order in time, a fact with fundamental ramifications, not only from a conceptual, but also from an operational viewpoint. Focusing on the latter, we first note that (FTXL-D) requires two sets of initial conditions, \(y(0)\) and \((0)\), the latter having no analogue in the first-order setting of (FTRL-D). In general, the evolution of the system depends on both \(y(0)\) and \((0)\), but since this would introduce an artificial bias toward a certain direction, we will take \((0)=0\), in tune with standard practice for (NAG) .

We also note that (FTXL-D) can be mapped to an equivalent autonomous first-order system with double the variables: specifically, letting \(p=\) denote the players' _(payoff) momentum_, (FTXL-D) can be rewritten as

\[=p=v(Q(y))-p\] (5)

with \(y(0)\) initialized arbitrarily and \(p(0)=(0)\). In turn, (5) yields \(p(t)=t^{-r}_{0}^{t}t^{r}v(Q(y()))\ d\), so \(p(t)\) can be seen as a weighted aggregate of the players' payoffs up to time \(t\): if \(r=0\) (the undamped regime), all information enters \(p(t)\) with the same weight; if \(r>0\), past information is discounted relative to more recent observations; and, in the overdamped limit \(r\), all weight is assigned to the current point in time, emulating in this way the first-order system (FTRL-D).

### First insights and results

From an operational standpoint, the main question of interest is to specify the equilibrium convergence properties of (FTXL-D) - and, later in the paper, its discrete-time analogue. To establish a baseline, the principal equilibrium properties of its first-order counterpart can be summarized as follows: (_i_) strict Nash equilibria are locally stable and attracting under (FTRL-D) ;2 (_ii_) the dynamics do not admit any other such points (that is, stable and attracting) ; and (_iii_) quantitatively, in the case of (EWD), the dynamics converge locally to strict Nash equilibria at a _geometric_ rate of the form \(\|x(t)-x^{*}\|=((-ct))\) for some \(c>0\).

Our first result below shows that the accelerated dynamics (FTXL-D) exhibit an exponential speed-up relative to (FTRL-D), and the players' orbits converge to strict Nash equilibria at a _superlinear_ rate:

**Theorem 1**.: _Let \(x^{*}\) be a strict Nash equilibrium of \(\), and let \(x(t)=Q(y(t))\) be a solution orbit of (FTXL-D). If \(x(0)\) is sufficiently close to \(x^{*}\), then \(x(t)\) converges to \(x^{*}\); in particular, if (FTXL-D) is run with logit best responses (that is, \(Q\)), we have_

\[\|x(t)-x^{*}\|_{}(C-}{2(r+1)})\] (6)

_where \(C>0\) is a constant that depends only on the initialization of (FTXL-D) and_

\[c=_{i}_{_{i}(x^{ *}_{i})}[u_{i}(x^{*}_{i};x^{*}_{-i})-u_{i}(_{i};x^{*}_{-i})]>0\] (7)

_is the minimum payoff difference at equilibrium._

Theorem 1 (which we prove in Appendix B) is representative of the analysis to come, so some remarks are in order. First, we should note that the explicit rate estimate (6) is derived for the special case of logit best responses, which underlie all exponential / multiplicative weights algorithms. To the best of our knowledge, the only comparable result in the literature is the similar rate provided in  for the case \(r=0\). In the case of a general regularizer, an analogous speed-up is observed, but the exact expressions are more involved, so we defer them to Appendix B. A second important point concerns whether the rate estimate (6) is tight or not. Finally, the neighborhood of initial conditions around \(x^{*}\) is determined by the minimum payoff difference at equilibrium and is roughly \((c)\) in diameter; we defer the relevant details of this discussion to Appendix B.

To answer this question - and, at the same time get a glimpse of the proof strategy for Theorem 1 - it will be instructive to consider a single-player game with two actions. Albeit simple, this toy example is not simplistic, as it provides an incisive look into the problem, and will be used to motivate our design choices in the sequel.

**Example 3.1**.: Consider a single-player game \(\) with actions A and B such that \(u()-u()=1\), so the (dominant) strategy \(x^{*}=(1,0)\) is a strict Nash equilibrium. Then, letting \(z=y_{}-y_{}\), (FTXL-D) readily yields

\[z}{dt^{2}}=y_{}}{dt^{2}}-y_{ }}{dt^{2}}=u()-u()-[}}{dt}-}}{dt}]=1-\;.\] (8)

As we show in Appendix B, this non-autonomous differential equation can be solved exactly to yield \(z(t)=z(0)+t^{2}/[2(r+1)]\), and hence

\[\|x(t)-x^{*}\|_{}=(-z(0)-}{ 2(r+1)}).\] (9)

Since \(c=u()-u()=1\), the rate (9) coincides with that of Theorem 1 up to a factor of \(1/2\). This factor is an artifact of the analysis and, in fact, it can be tightened to \((1-)\) for arbitrarily small \(>0\); we did not provide this more precise expression to lighten notation. By contrast, the factor \(2(r+1)\) in (6) _cannot_ be lifted; this has important ramifications which we discuss below. \(\)

The first conclusion that can be drawn from Example 3.1 is that the rate estimate of Theorem 1 is tight and cannot be improved in general. In addition, and in stark contrast to (NAG), Example 3.1shows that the optimal value for the friction parameter is \(r=0\) (at least from a min-max viewpoint, as this value yields the best possible lower bound for the rate). Of course, this raises the question as to whether this is due to the continuous-time character of the policy;3 however, as we show in detail in Appendix C, this is _not_ the case: the direct handover of (NAG) to Example 3.1 yields the exact same rate (though the proof relies on a significantly more opaque generating function calculation).

In view of all this, it becomes apparent that friction only _hinders_ the equilibrium convergence properties of accelerated FTRL schemes in our game-theoretic setting. On that account, we will continue our analysis in the undamped regime \(r=0\).

## 4 Accelerated learning: Analysis and results

### The algorithm.

To obtain a bona fide, algorithmic implementation of the continuous-time dynamics (FTXL-D), we will proceed with the same explicit, finite-difference scheme leading to the discrete-time algorithm (NAG) from the continuous-time dynamics (HBVF) of Su et al. . Specifically, taking a discretization step \(>0\) in (FTXL-D) and setting the scheme's friction parameter \(r\) to zero (which, as we discussed at length in the previous section, is the optimal choice in our setting), a straightforward derivation yields the basic update rule

\[[y_{i,n+1}-2y_{i,n}+y_{i,n-1}]/^{2}=_{i,n}$ and all $n=1,2,$}\] (10)

In the above, just as in the case of (FTRL), \(_{i,n}^{_{i}}\) denotes a black-box "payoff signal" that carries information about the mixed payoff vector \(v_{i}(x_{n})\) of player \(i\) at the current strategy profile \(x_{n}\) (we provide more details on this below).

Alternatively, to obtain an equivalent first-order iterative rule (which is easier to handle and discuss), it will be convenient to introduce the momentum variables \(p_{n}=(y_{n}-y_{n-1})/\). Doing just that, a simple rearrangement of (10) yields the _"follow the accelerated leader"_ scheme

\[y_{i,n+1}=y_{i,n}+ p_{i,n+1} p_{i,n+1}=p_{i,n}+_{i,n } x_{i,n}=Q_{i}(y_{i,n})\;.\] (FTXL)

The algorithm (FTXL) will be our main object of study in the sequel, and we will examine its convergence properties under three differerent models for \(_{n}\):

1. _Full information_, i.e., players get to access their full, mixed payoff vectors: \[_{i,n}=v_{i}(x_{n})$, $n=1,2,$}\] (11a)
2. _Realization-based feedback_, i.e., after choosing an action profile \(_{n} x_{n}\), each player \(i\) observes (or otherwise calculates) the vector of their counterfactual, "what-if" rewards, namely \[_{i,n}=v_{i}(_{n})$, $n=1,2,$}\] (11b)
3. _Bandit /Payoff-based feedback_, i.e., each player only observes their current reward, and must rely on statistical estimation techniques to reconstruct an estimate of \(v_{i}(x_{n})\). For concreteness, we will consider the case where players employ a version of the so-called _importance-weighted estimator_ \[_{i,n}=(x_{i,n};_{i,n})$, $n=1,2,$}\] (11c)

which we describe in detail later in this section.

Of course, this list of information models is not exhaustive, but it is a faithful representation of most scenarios that arise in practice, so it will suffice for our purposes.

Now before moving forward with the analysis, it will be useful to keep some high-level remarks in mind. The first is that (FTXL) shares many similarities with (FTRL), but also several notable differences. At the most basic level, (FTRL) and (FTXL) are both "stimulus-response" schemes in the spirit of Erev & Roth , that is, players "respond" with a strategy \(x_{i,n}=Q_{i}(y_{i,n})\) to a "stimulus" \(y_{i,n}\) generated by the observed payoff signals \(_{i,n}\). In this regard, both methods adhere to the online learning setting (and, in particular, to the regularized learning paradigm).

However, unlike (FTRL), where players respond to the aggregate of their payoff signals - the process \(y_{n}\) in (FTRL) - the accelerated algorithm (FTXL) introduces an additional aggregation layer, which expresses how players "build momentum" based on the same payoff signals - the process \(p_{n}\) in (FTXL). Intuitively, we can think of these two processes as the "position" and "momentum" variables of a classical inertial system, not unlike the heavy-ball dynamics of Su et al. . The only conceptual difference is that, instead of rolling along the landscape of a (convex) function, the players now track the "mirrored" payoff field \((y) v(Q(y))\).

In the rest of this section, we proceed to examine in detail the equilibrium convergence properties of (FTXL) under each of the three models detailed in Eqs. (11a)-(11c) in order.

### Accelerated learning with full information

We begin with the full information model (11a). This is the most straightforward model (due to the absence of randomness and uncertainty) but, admittedly, also the least realistic one. Nevertheless, it will serve as a useful benchmark for the rest, and it will allow us to introduce several important notions.

Before we state our result, it is important to note that a finite game can have multiple strict Nash equilibria, so global convergence results are, in general, unattainable; for this reason, we analyze the algorithm's local convergence landscape. In this regard, Theorem 2 below shows that (FTXL) with full information achieves a _superlinear_ local convergence rate to strict Nash equilibria:

**Theorem 2**.: _Let \(x^{*}\) be a strict Nash equilibrium of \(\), and let \(x_{n}=Q(y_{n})\) be the sequence of play generated by (FTXL) with full information feedback of the form (11a). If \(x_{1}\) is initialized sufficiently close to \(x^{*}\), then \(x_{n}\) converges to \(x^{*}\); in particular, if (FTXL) is run with logit best responses (that is, \(Q\)), we have_

\[\|x_{T}-x^{*}\|_{}(C-c^{2})= (-(T^{2}))\] (12)

_where \(C>0\) is a constant that depends only on the initialization of (FTXL) and_

\[c=_{i}_{_{i}}(x_{i}^{*}) }[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(_{i};x_{-i}^{*})]>0\] (13)

_is the minimum payoff difference at equilibrium._

To maintain the flow of our discussion, we defer the proof of Theorem 2 to Appendix C. Instead, we only note here that, just as in the case of (HBVF) and (NAG), Theorem 2 provides essentially the same rate of convergence as its continuous-time counterpart, Theorem 1, modulo a subleading term which has an exponentially small impact on the rate of convergence. In particular, we should stress that the _superlinear_ convergence rate of (FTXL) exhibits an exponential speedup relative to (FTRL), which is known to converge at a geometric rate \(\|x_{T}-x^{*}\|_{}=(-(T))\). This is in direct correspondence to what we observe in continuous time, showing in particular that the continuous-time dynamics (FTXL-D) are a faithful representation of (FTXL).

We should also stress here that superlinear convergence rates are typically associated with methods that are second-order _in space_, in the sense that they employ Hessian-like information - like Newton's algorithm - not second-order _in time_ - like (NAG) and (FTXL). We find this observation particularly intriguing as it suggests that accelerated rates can be observed in the context of learning in games without having to pay the excessively high compute cost of second-order methods in optimization.

### Accelerated learning with realization-based feedback

We now turn to the realization-based model (11b), where players can only assess the rewards of their pure actions in response to the _realized_ actions of all other players. In words, \(_{i,n}=v_{i}(_{n})\) collects the payoffs that player \(i\) would have obtained by playing each of their pure actions \(_{i}_{i}\) against the action profile \(_{-i,n}\) adopted by the rest of the players.

In contrast to the full information model (11a), the realization-based model is stochastic in nature, so our convergence results will likewise be stochastic. Nevertheless, despite the added layer of uncertainty, we show that (FTXL) with realization-based feedback maintains a superlinear convergence rate with high probability:

**Theorem 3**.: _Let \(x^{*}\) be a strict Nash equilibrium of \(\), fix some confidence level \(>0\), and let \(x_{n}=Q(y_{n})\) be the sequence of play generated by (FTXL) with realization-based feedback as per (11b) and a sufficiently small step-size \(>0\). Then there exists a neighborhood \(\) of \(x^{*}\) such that_

\[(x_{n} x^{*}\ \ n) 1-\ x_{1} .\] (14)

_In particular, if (FTXL) is run with logit best responses (that is, \(Q\)), there exist positive constants \(C,c>0\) as in Theorem 2 such that on the event \(\{x_{n} x^{*}\ \ n\}\):_

\[\|x_{T}-x^{*}\|_{}(C-c^{2}+c^{5/3}T^{5/3})=(-(T^{2})).\] (15)

What is particularly surprising in Theorem 3 is that, (FTXL) maintains the accelerated superlinear rate of Theorem 2 - and, likewise, the exponential speedup relative to (FTRL) - _despite_ the randomness and uncertainty involved in the realization-based model (11b). The salient point enabling this feature of (FTXL) is that \(_{n}\) can be expressed as

\[_{n}=v(x_{n})+U_{n}\] (16)

where \(U_{n}_{i}^{A_{i}}\) is an almost surely bounded conditionally zero-mean stochastic perturbation, that is, \([U_{n}\,|\,_{n}]=0\), where \(_{n}(x_{1},,x_{n})\) denotes the history of play up to (and including) time \(n\). Thanks to the boundedness of (16), we are able to derive a series of probabilistic estimates showing that, with high probability (and, in particular, greater than \(1-\)), the contribution of the noise in the algorithm's rate becomes subleading, thus allowing the superlinear rate of Theorem 2 to emerge. As in the case of Theorem 2, we defer the proof of Theorem 3 to the appendix.

### Bandit feedback

The last framework we consider is the bandit model where players only observe their realized rewards, a scalar from which they must reconstruct their entire payoff vector. To do so, a standard technique from the multi-armed bandit literature is the so-called _importance weighted estimator_ (IWE) , defined in our setting as

\[_{i\,_{i},n}=\{_{i,n}=_{i}\}}{_{i\,_{i,n}}}u_{i}(_{i};_{-i,n})\] (IWE)

Figure 1: Performance evaluation of (FTXL) in a zero-sum and a congestion game under realization-based and bandit feedback. Solid lines represent average values, while shaded regions enclose \( 1\) standard deviation. The plots are in logarithmic scale.

where \(_{i,n}=(1-_{n})x_{i,n}+_{n}_{ _{i}}\) is a mixture of \(x_{i,n}\) and the uniform distribution on \(_{i}\) (a mechanism known in the literature as _explicit exploration_). Importantly, this estimator is unbiased relative to the perturbed strategy \(_{x_{n}}\), which thus incurs an \((_{n})\) non-zero-sum error to the estimation of \(v_{i}(x_{n})\). This error can be made arbitrarily small by taking \(_{n} 0\) but, in doing so, the variance of \(_{i,n}\) diverges, leading to a bias-variance trade-off that is difficult to tame.

Despite these added difficulties, we show below that (FTXL) maintains its superlinear convergence rate even with bandit, payoff-based feedback:

**Theorem 4**.: _Let \(x^{*}\) be a strict Nash equilibrium of \(\), fix some confidence level \(>0\), and let \(x_{n}=Q(y_{n})\) be the sequence of play generated by (FTXL) with bandit feedback of the form (11c), an IWE exploration parameter \(_{n} 1/n^{_{e}}\) for some \(_{e}(0,1/2)\), and a sufficiently small step-size \(>0\). Then there exists a neighborhood \(\) of \(x^{*}\) in \(\) such that_

\[}(x_{n} x^{*}n) 1- x_{1}.\] (17)

_In particular, if (FTXL) is run with logit best responses (that is, \(Q\)), there exist positive constants \(C,c>0\) as in Theorem 2 such that on the event \(\{x_{n} x^{*}n\}\)_

\[\|x_{T}-x^{*}\|_{}(C-c^{2}+c^{9/5}T^{9/5})=(-(T^{2})).\] (18)

Theorem 4 (which we prove in Appendix D shows that, despite the degradation of the subleading term, (FTXL) retains its superlinear convergence rate even with bandit, payoff-based feedback (for a numerical demonstration, see Fig. 1 above). We find this feature of (FTXL) particularly important as it shows that the algorithm remains exceptionally robust in the face of randomness and uncertainty, even as we move toward increasingly information-starved environments - from full information, to realization-based observations and, ultimately, to bandit feedback. This has important ramifications from an operational standpoint, which we intend to examine further in future work.

### Numerical Experiments

We conclude this section with a series of numerical simulations to validate the performance of (FTXL). To this end, we consider two game paradigms, (i) a 2-player zero-sum game, and (ii) a congestion game.

Zero-sum Games.First, we consider a 2-player zero-sum game with actions \(\{_{1},_{2},_{3}\}\) and \(\{_{1},_{2},_{3}\}\), and payoff matrix

\[P=(2,-2)&(1,-1)&(2,-2)\\ (-2,2)&(-1,1)&(-2,2)\\ (-2,2)&(-1,1)&(-2,2)\]

Here, the rows of \(P\) correspond to the actions of player \(A\) and the columns to the actions of player \(B\), while the first item of each entry of \(P\) corresponds to the payoff of \(A\), and the second one to the payoff of \(B\). Clearly, the action profile \((_{1},_{2})\) is a strict Nash equilibrium.

Congestion Games.As a second example, we consider a congestion game with \(N=100\) and \(2\) roads, \(r_{1}\) and \(r_{2}\), with costs \(c_{1}=1.1\) and \(c_{2}=d/N\) where \(d\) is the number of drivers on \(r_{2}\). In words, \(r_{1}\) has a fixed delay equal to \(1.1\), while \(r_{2}\) has a delay proportional to the drivers using it. Note, that the strategy profile where all players are using \(r_{2}\) is a strict Nash equilibrium.

In Fig. 1, we assess the convergence of (FTXL) with logit best responses, under realization-based and bandit feedback, and compare it to the standard (EW) with the same level of information. The figures verify that (FTXL) outperforms (EW) regarding the convergence to a strict Nash equilibrium both for the realization-based and the bandit feedback, as expected from the theoretical findings. Specifically, they validate the faster convergence rate of (FTXL) compared to that of the (EW) algorithm. Moreover, we observe that both algorithms perform worse under bandit feedback than under realization-based feedback. This behavior is expected as less information becomes available. More details can be found in Appendix E.