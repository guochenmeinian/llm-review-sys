# Continuous Temporal Domain Generalization

Zekun Cai

Guangji Bai

Ezory University, Atlanta, GA, USA

Renhe Jiang

Xuan Song

Liang Zhao

Ezunov University, Atlanta, GA, USA

###### Abstract

Temporal Domain Generalization (TDG) addresses the challenge of training predictive models under temporally varying data distributions. Traditional TDG approaches typically focus on domain data collected at fixed, discrete time intervals, which limits their capability to capture the inherent dynamics within continuous-evolving and irregularly-observed temporal domains. To overcome this, this work formalizes the concept of Continuous Temporal Domain Generalization (CTDG), where domain data are derived from continuous times and are collected at arbitrary times. CTDG tackles critical challenges including: 1) Characterizing the continuous dynamics of both data and models, 2) Learning complex high-dimensional nonlinear dynamics, and 3) Optimizing and controlling the generalization across continuous temporal domains. To address them, we propose a Koopman operator-driven continuous temporal domain generalization (Koodos) framework. We formulate the problem within a continuous dynamic system and leverage the Koopman theory to learn the underlying dynamics; the framework is further enhanced with a comprehensive optimization strategy equipped with analysis and control driven by prior knowledge of the dynamics patterns. Extensive experiments demonstrate the effectiveness and efficiency of our approach. The code can be found at: https://github.com/Zekun-Cai/Koodos.

## 1 Introduction

In practice, the distribution of training data often differs from that of test data, leading to a failure in generalizing models outside their training environments. Domain Generalization (DG) [49; 46; 21; 14; 40] is a machine learning strategy designed to learn a generalized model that performs well on the unseen target domain. The task becomes particularly pronounced in dynamic environments where the statistical properties of the target domains change over time [16; 33; 4], prompting the development of Temporal Domain Generalization (TDG) [27; 39; 41; 2; 55; 57]. TDG recognizes that domain shifts are temporally correlated. It extends DG approaches by modeling domains as a sequence rather than as categorical entities, making it especially beneficial in fields where data is inherently time-varying.

Existing works in TDG typically concentrate on the discrete temporal domain, where domains are defined by distinct "points in time" with fixed time intervals, such as second-by-second data (Rot 2-Moons ) and annual data (Yearbook ). In this framework, data bounded in a time interval are considered as a detached domain, and TDG approaches primarily employ probabilistic models to predict domain evolutions. For example, LSSAE  employs a probabilistic generative model to analyze latent structures within domains; DRAIN  builds a Bayesian framework to predict future model parameters, and TKNets  constructs domain transition matrix derived from the data.

However, in practice, data may not always occur or be observed at discrete, regularly spaced time points. Instead, events and observations unfold irregularly and unpredictably in the time dimension, leading to temporal domains distributed irregularly and sparsely over continuous time. Formally, this paper introduces a problem termed Continuous Temporal Domain Generalization (CTDG), where both seen and unseen tasks reside in different domains at continuous, irregular time points. Fig. 1 illustrates an example of public opinion prediction after political events via Twitter data. Unlike the assumption in traditional TDG that the temporal regularly domains, the data are collected for the times near political events that may occur in arbitrary times. In the meanwhile, the domain data evolve constantly and continuously over time, e.g., active users increase, new friendships are formed, and age and gender distribution changes. Correspondingly, the ideal classifier should gradually change with the domain at random moments to counter the data distribution change over time. Finally, we are concerned with the state of the predictive model at any moment in the future. CTDG is ubiquitous in other fields. For example, in disaster management, relevant data are collected during and following disasters, which may occur at any time throughout the year. In healthcare, critical information about diagnosis and treatment is typically only documented during episodes of care rather than evenly throughout the lifetime. Hence, the CTDG task necessitates the characterization of the continuous dynamics of irregular time-distributed domains, which cannot be handled by existing TDG methods designed for discrete-dynamics and fixed-interval times.

Despite the importance of CTDG, it is still a highly open research area that is not well explored because of several critical hurdles in: **1) Characterizing data dynamics and their impact on model dynamics.** The irregular times of the temporal domains require us to characterize the continuous dynamics of the data and, hence, the model dynamics ultimately. However, the continuous-time data dynamics are unknown and need to be learned across arbitrary time points. Furthermore, it is imperative yet challenging to know how the model evolves according to the data dynamics in continuous times. Therefore, we don't have a direct observation of the data dynamics and the model dynamics we want to learn, which prevents us from existing continuous time modeling techniques. **2) Learning the underlying dynamics of over-parametrized models.** Deep neural networks (e.g., Multi-Layer Perceptron and Convolutional Neural Network) are highly nonlinear and over-parametrized, and hence, the evolutionary dynamics of model states over continuous time are high-dimensional and nonlinear. Consequently, the principal dynamics reside in a great number of latent dimensions. Properly representing and mapping these dynamics into a learnable space remains a challenge. **3) Jointly optimizing the model and its dynamics under possible inductive bias.** The model learning for individual domains will be entangled with the learning of the continuous dynamics across these models. Furthermore, in many situations, we may have some high-level prior knowledge about the dynamics, such as whether there are convergent, divergent, or periodic patterns. It is an important yet open topic to embed them into the CTDG problem-solving.

To address all the challenges, we propose the Koopman operator-driven continuous temporal domain generalization framework (Koodos). Specifically, the Koodos framework articulates the evolutionary continuity of the predictive models in CTDG, then leverages a continuous dynamic approach to model its smooth evolution over time. Koodos further simplifies the nonlinear model system by projecting them into a linearized space via the Koopman Theory. Finally, Koodos provides an interface that

Figure 1: An example of continuous temporal domain generalization. Consider training classification models for public opinion prediction via tweets, where the training domains are only available at specific political events (e.g., presidential debates), we wish to generalize the model to any future based on the underlying data distribution drift within the time-irregularly distributed training domains.

reveals the internal model dynamic characteristics, as well as incorporates prior knowledge and constraints directly into the joint learning process.

## 2 Related works

**Domain Generalization (DG) and Domain Adaptation (DA).** DG approaches attempt to learn a model from multiple source domains that generalize well to an unseen domain [38; 37; 28; 5; 13; 56]. Existing DG methods can be classified into three strategies : (1) Data manipulation techniques, such as data augmentation [45; 46; 57] and data generation [32; 40]; (2) Representation learning focuses on extracting domain-invariant features [17; 18] and disentangling domain-shared from domain-specific features ; (3) Learning strategies encompass ensemble learning , meta-learning [26; 14; 9], and gradient-based approaches . Unlike DG, DA methods require simultaneously accessing source and target domain data to facilitate alignment and adaptation [50; 51; 29]. The technique includes domain-invariant learning [17; 47; 35; 48; 53], domain mapping [6; 20; 15; 31], ensemble methods , and so on. _Both DG and DA are limited to considering generalization across categorical domains, which treats domains as individuals but ignores the smooth evolution of them over time._

**Temporal Domain Generalization (TDG).** TDG is an emerging field that extends traditional DG techniques to address challenges associated with time-varying data distributions. TDG decouples time from the domain and constructs domain sequences to capture its evolutionary relationships. S-MLDG  pioneers a sequential domain DG framework based on meta-learning. Gradient Interpolation (GI)  proposes to extrapolate the generalized model by supervising the first-order Taylor expansion of the learned function. LSSAE  deploys a probabilistic framework to explore the underlying structure in the latent space of predictive models. DRAIN  constructs a recurrent neural network that dynamically generates model parameters to adapt to changing domains. TKNets  minimize the divergence between forecasted and actual domain data distributions to capture temporal patterns.

Despite these studies, traditional TDG methods are limited to requiring domains presented in discrete time, which disrupts the inherent continuity of changes in data distribution, and the generalization can only be carried forward by very limited steps. _No work treats time as a continuous variable, thereby failing to capture the full dynamics of evolving domains and generalize to any moment in the future._

**Continuous Dynamical Systems (CDS).** CDS are fundamental in understanding how systems evolve without the constraints of discrete intervals. They are the study of the dynamics for systems defined by differential equations. The linear multistep method or the Runge-Kutta method can solve the Order Differential Equations (ODEs) . Distinguishing from traditional methods, Neural ODEs  represent a significant advancement in the field of CDS. It defines a hidden state as a solution to the ODEs initial-value problem, and parameterizes the derivatives of the hidden state using a neural network. The hidden state can then be evaluated at any desired time using a numerical ODEs solver. Many recent studies have proposed on which to learn differential equations from data [42; 22; 23; 36].

## 3 Problem definition

**Continuous Temporal Domain Generalization (CTDG):** We address prediction tasks where the data distribution evolves over time. In predictive modeling, a domain \((t)\) is defined as a dataset collected at time \(t\) consisting of instances \(\{(x_{i}^{(t)},y_{i}^{(t)})(t)(t)\}_{i=1}^{N(t)}\), where \(x_{i}^{(t)}\), \(y_{i}^{(t)}\) and \(N(t)\) represent the feature, target and the number of instances at time \(t\), and \((t)\), \((t)\) denote the input feature space and label space at time \(t\), respectively. We focus on the existence of gradual concept drift across continuous time, indicating that domain conditional probability distributions \(P(Y(t)|X(t))\), with \(X(t)\) and \(Y(t)\) representing the random variables for features and targets at time \(t\), change smoothly and seamlessly over continuous time like streams without abrupt jumps.

During training, we are provided with a sequence of observed domains \(\{(t_{1}),(t_{2}),,(t_{T})\}\) collected at arbitrary times \(=\{t_{1},t_{2},,t_{T}\}\), where \(t_{i}^{+}\) and \(t_{1}<t_{2}<<t_{T}\). For each domain \((t_{i})\) at time \(t_{i}\), we learn the predictive model \(g(;(t_{i})):(t_{i})(t_{i})\), where \((t_{i})\) denotes the parameters of function \(g\) at timestamp \(t_{i}\). We model the dynamics across the parameters \(\{(t_{1}),(t_{2}),,(t_{T})\}\), and finally predict the parameters \((s)\) for the predictive model \(g(;(s)):(s)(s)\) at any given time \(s\). For simplicity, in subsequent, we will use \(_{i}\), \(X_{i}\), \(Y_{i}\), \(_{i}\) to represent \((t_{i})\), \(X(t_{i})\), \(Y(t_{i})\), \((t_{i})\) at time \(t_{i}\).

Unlike traditional temporal domain generalization approaches [39; 2; 52; 57] that divide time into discrete intervals and require domain data be collected at fixed time steps, the CTDG problem treats time as continuous variable, allowing for the time points in training and test set to be any arbitrary positive real numbers, and requiring models to deal with the temporal domains as continuous streams. CTDG problem poses several unprecedented, substantial challenges in: 1) Characterizing the continuous dynamics of data and how that decides model dynamics; 2) Modeling the low-dimensional continuous dynamics of the predictive model, which are embedded in high-dimensional space due to over-parametrization; and 3) Optimizing and analyzing the continuous predictive model system, including the application of inductive biases to control its behavior.

## 4 Methodology

In this section, we outline our strategies to tackle the problem of CTDG by overcoming its substantial challenges. First, to learn the continuous drifts of data and models, we synchronize between the evolving domain and the model by establishing the continuity of the model parameters over time, which is ensured through the formulation of differential equations as elaborated in Section 4.1. To fill the gap between high-dimensional model parameters and low-dimensional model continuous dynamics, we simplify the representation of complex dynamics into a principal, well-characterized Koopman space as is detailed in Section 4.2. To jointly learn the model and its dynamics under additional inductive bias, in Section 4.3, we design a series of organized loss functions to form an efficient end-to-end optimization strategy. Overall, as shown in Fig. 2(a), there are three dynamic flows in our system, which are the Data Flow, the Model Flow, and the Koopman Representation Flow. Through the proposed framework, we aim to ensure not only that the model responds to the statistical variations inherent in the data, but also that the characteristics of the three flows are consistent.

### Characterizing the continuous dynamics of the data and the model

In this section, we explore the relationship between the evolving continuous domains and the corresponding dynamics of the predictive models. We demonstrate that the dynamics of temporal domains lead to the internal update of the predictive model continuously over time in Theorem 1. Following this, we develop a learnable dynamic system for the continuous state of the model by synchronizing the behaviors of the domain and model.

**Assumption 1**.: _Consider the gradual concept drift within the continuous temporal domains. It is assumed that the conditional probability distribution \(P_{t}(Y|X)\) changes continuously over time, and its dynamics are characterized by a function \(f\), which models the variations in the distribution._

**Theorem 1**.: _(Continuous Evolution of Model Parameters) Given Assumption 1, it follows that the parameters \(_{t}\) of the predictive model \(g(;_{t})\) also evolve continuously over time, and its dynamics are jointly determined by the current state of the model and the function \(f\)._

Proof.: The temporal derivative of the functional space \(g(;_{t})\) represents its evolution in direct response to the changes in the conditional probability distribution. Without loss of generality, the ground-truth functional space can be modeled by an ordinary differential equation:

\[dg(;_{t})/dt=f(g(;_{t}),t).\] (1)

Applying the chain rule to decompose the temporal derivative of \(g\):

\[g(;_{t})=_{i=1}^{n}}}{dt}=J_{g}(_{t})}{dt},\] (2)

Figure 2: Macro-flows and micro-constraints in the proposed model framework.

where \(J_{g}(_{t})\) is the Jacobian matrix of \(g\) with respect to \(_{t}\).

By equating the Eq. 2 to the expression involving \(f\):

\[J_{g}(_{t})}{dt}=f(g(;_{t}),t).\] (3)

Assuming that \(J_{g}(_{t})\) is invertible, the ground-truth derivative of \(_{t}\) with respect to time is given by:

\[}{dt}=J_{g}(_{t})^{-1}f(g(;_{t}),t).\] (4)

It is known from the Proof that the evolution continuity of \(_{t}\) follows from a differential equation. Eq. 4 identifies a dynamic framework for the predictive model parameters, which provides a strong motivation to develop dynamic systems for them.

The principal challenge arises from the unknown dynamics enclosed in \(f\), which prevents getting \(_{t}\) by direct mathematical computation. Recognizing this, we propose a learning-based approach to model the parameter dynamics from the observed domains. We construct a learnable parameter dynamics function \(h(_{t},t;)\), parameterized by \(\), optimized to topological conjugation between the model dynamics and the data dynamics. Topological conjugation guarantees that the parameter's orbits faithfully characterize the underlying data dynamics, thereby ensuring the model's generalization capability across all time. Specifically, as illustrated in Fig. 2(a) of the Data Flow and the Model Flow, assuming a conceptual perfect mapping \(\) from the data space to the parameter space, topological conjugation suggests that the composition of the learned dynamic \(h\) after \(\) should behave identically to applying \(\) after the dynamics \(f\), i.e., \(h= f\) holds, where \(\) represents function composition. In the CTDG problem, the objective of conjugation is to optimize the model dynamics \(h\) and the predictive parameters \(\) simultaneously, to minimize the discrepancy between the dynamically derived parameters from \(h\) and those obtained through direct training, formulated as follows:

\[=_{}_{i=1}^{T}_{j=1}^{i}\|_{ i},_{i}^{j i}\|_{2},\] (5)

where \(\|\|_{2}\) denotes the Euclidean norm, and each \(_{i}\) is determined by:

\[_{i}=_{_{i}}(Y_{i},g(X_{i};_{i})),\] (6)

and \(_{i}^{j i}\) is defined as the integral parameters at \(t_{i}\) obtained from \(t_{j}<t_{i}\):

\[_{i}^{j i}=_{j}+_{t_{j}}^{t_{i}}h(_{},;) \,d.\] (7)

Here, \(\) symbolizes the loss function tailored to the prediction task. By employing Eq. 7, the model dynamics are defined for all observation time, and \(\) can be obtained by optimizing Eq. 5 and Eq. 6 via gradient descent. After that, the parameters of the predictive model can be calculated at any specific time using ODE solvers.

### Modeling nonlinear model dynamics by Koopman operators

The aforementioned learning approaches are, however, limited in efficient modeling and prediction due to the entangled nonlinear dynamics in the high-dimensional parameters space. Identifying strongly nonlinear dynamics through such space is particularly susceptible to overfitting , leading to future predictive models eventually diverging from bifurcation to chaos. Despite the complexity of \(h\), it is governed by the data dynamics \(f\), which are typically low-dimensional and exhibit simple, predictable patterns. This suggests that the governing equations for \(h\) are sparse within the over-parameterized space, and thus allow for a more simplified and manageable representation in a properly transformed space. Motivated by this, we propose leveraging the well-established Koopman Theory [24; 7] to represent these complex parameter dynamics. Koopman Theory provides a method for the global linearization of any nonlinear dynamics. It expresses the complex dynamic system as an infinite-dimensional Koopman operator acting on the Hilbert space of the system state measurement functions, in which the nonlinear dynamics will become linearized.

To facilitate this approach, the target is to identify a set of intrinsic coordinates \(\) that spans a Koopman-invariant subspace. As illustrated in Fig. 2(a) of the Model Flow and Koopman Flow, this transformation maps the parameters \(\) into a latent space \(z=()\), with \(z\) resides in a low \(n\)-dimensional space, \(=^{n}\), where the dynamics become more linearized. We also aim to find a finite-dimensional approximation of the Koopman operator, denoted as \(^{n n}\), that acts on the latent space and advances the observation of the state to the future \(:\). After that we have:

\[d()/dt=().\] (8)

The \(z\) dynamics are easier to track because they principleize the dynamical system while maintaining its characteristics. It gives us a tighter representation of the parameters within a linear space, which allows us to learn the simple \(\) operator instead of the complex coupled dynamics \(h(;)\). Following the transformation, the dynamics of the parameters can be expressed by:

\[z_{i}^{j i}=z_{j}+_{t_{j}}^{t_{i}}z_{}\,d ,z_{j}=(_{j}).\] (9)

Finally, an inverse transformation provided by \(_{i}^{j i}=^{-1}(z_{i}^{j i})\) that maps Koopman space back to the original parameter space. The relational among \(\), \(\), \(\), \(^{-1}\) and the Koopman invariant subspace are bounded by a series of loss functions detailed in the next Section.

### Joint optimization of model and its dynamics with prior knowledge

We introduce a comprehensive optimization approach designed to ensure that the system accurately captures the dynamics of the data. This process requires the joint optimization of several interconnected components under the optional constraint of additional prior knowledge about the underlying dynamics: the predictive model parameters \(_{1:T}\), the transformation functions \(\) and \(^{-1}\), and the Koopman operator \(\). Our primary objectives are threefold: _1) Ensuring high prediction accuracy, 2) Maintaining consistency of parameters across different representations and transformations, and 3) Learning the Koopman invariant subspaces effectively_. Fig. 2(b) illustrates the role of each constraint within our system: we manage two sets of states, intrinsic and integral, aligning across three spaces.

**Predictive Model Parameters** (\(\)): Each observation time corresponds to a predictive model, which is tasked with making predictions and serving as the initial values for the dynamical system. They are primarily optimized to minimize the prediction error \(L_{intri}\) of different domains:

\[L_{intri}=_{i=1}^{T}(Y_{i},g(X_{i};_{i})).\] (10)

**Koopman Network Parameters** (\(\), \(^{-1}\), \(\)): We estimate a function that transforms the parameters of the predictive model into Koopman space. Meanwhile, these predictive model parameters must be stable and consistent when converted between representations. This is realized by three constraints:

1. **Reconstruction Loss:** An autoencoder is leveraged to map parameter space to Koopman space by encoder \(\) and back to the original space by decoder \(^{-1}\). \(L_{recon}\) ensures consistency between \(\) and its reconstructed form via transformations: \[L_{recon}=_{i=1}^{T}\|_{i},^{-1}((_{i} ))\|_{2}.\] (11)
2. **Dynamic Fidelity Loss:** This term ensures that the transformation produces a Koopman invariant subspaces in which the dynamics can forward correctly. It measures the fit of the Koopman operator \(\) against the transferred parameter: \[L_{dyna}=_{i=1}^{T}_{j=1}^{i}\|z_{i},z_{i}^{j i }\|_{2},\] (12) where \(z_{i}=(_{i})\) and \(z_{i}^{j i}=z_{j}+_{t_{j}}^{t_{i}}z_{}\,d\).
3. **Consistency Loss:** It measures the consistency between the original and the dynamic parameter in the model space: \[L_{consis}=_{i=1}^{T}_{j=1}^{i}\|_{i},_ {i}^{j i}\|_{2},_{i}^{j i}=^{-1}(z_{i}^{j  i})\] (13)Additionally, we load the dynamically integral parameters \(_{i}^{j i}\) back into the predictive model to evaluate its predictive capability, quantified by \(L_{integ}=_{i=1}^{T}_{j=1}^{i}(Y_{i},g(X_{i};_{i}^{j  i}))\). Finally, the system optimizes the following combined loss to refine all components simultaneously:

\[\{_{1:T},,^{-1},\}=, ,^{-1},}{}( L_{intri}+  L_{integ}+ L_{recon}+ L_{dyna}+ L_{consis}),\] (14)

with \(\), \(\), and \(\) as adjustable weights to balance the magnitude of each term, ensuring that no single term dominates during training.

During inference, given the moment \(t_{s}\), the model uses the state from the nearest observation moment \(t_{obs}\) as an initial value, integrating over the time interval \([t_{obs},t_{s}]\) in Koopman space to give the generalized model state \(_{s}^{obs s}\) at the desired test moment.

**Analysis and Control of the Temporal Domain Generalization.** Integrating Koopman theory into continuous temporal domain modeling facilitates the application of optimization, estimation, and control techniques, particularly through the spectral properties of the Koopman operator. We remark that the Koopman operator serves as a pivotal interface for analyzing and controlling the generalization process. Constraints imposed on the Koopman space will be equivalently mapped to the Model space. For instance, the eigenvalues of \(\) are crucial as they provide insights into the system's stability and dynamics, as illustrated below:

\[z_{i}^{j i}=z_{j}+_{t_{j}}^{t_{i}}z_{}\,d, $ is an eigenvalue of $$}\] (15)

1. **System Assessment.** The generalization process is considered stable if all \(_{i}\) satisfy \((_{i})<0\). Conversely, the presence of any \(_{i}\) such that \((_{i})>0\) indicates instability in the system. When \((_{i})=0\), the system may exhibit oscillatory behavior. By analyzing the locations of these poles in the complex plane, we can assess the system's long-term dynamics, helping us identify whether the generalized model is likely to collapse in the future.
2. **Behavioral Constraints.** Adding explicit constraints to \(\) can guide the generalization toward desired behaviors. This process not only facilitates the incorporation of prior knowledge about domains but also tailors the system to specific characteristics. To name just a few, if the data dynamics are known to be periodic, configuring \(\) as \(=B-B^{T}\), with \(B\) as learnable parameters, ensures that the model system exhibits consistent periodic behavior since the eigenvalues of \(B-B^{T}\) are purely imaginary values. Additionally, employing a low-rank approximation such as \(=UV^{T}\), with \(U,V^{n k}\) and \(k<n\), allows the model to concentrate on the most significant dynamical features and explicitly control the degrees of freedom of the generalization process.

**Theoretical Analysis.** In this work, we theoretically proved that our proposed continuous-time TDG method has a smaller or equal error compared to the discrete-time method for approximating temporal distribution drift. This demonstrates that _the ODE method provides a more accurate approximation due to its consideration of the integral of changes over time, reducing the accumulation of errors compared to the step-wise updates of the discrete-time methods._

**Theorem 2** (Superiority of continuous-time methods over discrete-time methods (informal)).: _Continuous-time methods have smaller or equal errors compared to discrete-time methods in approximating temporal distribution drift, due to its consideration of the integral of changes over time._

The formal version and proof of Theorem 2 are given in Appendix C. We also provide a detailed model complexity analysis in Appendix A.1.4.

## 5 Experiment

In this section, we present the performance of the Koodos in comparison to other approaches through both quantitative and qualitative analyses. Our evaluation aims at _1) assessing the generalizability of Koodos in continuous temporal domains; 2) assessing whether Koodos captures the correct underlying data dynamics_; and _3) assessing whether Koodos can use inductive bias to guide the behavior of the generalization_. Detailed experiment settings (i.e., dataset details, baseline details, hyperparameter settings, ablation study, scalability analysis, and sensitivity analysis) are demonstrated in Appendix A.1. Besides, since the TDG is a special case of the CTDG, we also conducted experiments on the traditional discrete temporal domain generalization task. Results are shown in Appendix B.

**Datasets.** We compare with classification datasets: Rotated Moons (2-Moons), Rotated MNIST (Rot-MNIST), Twitter Influenza Risk (Twitter), and Yearbook; and the regression datasets: Tropical Cyclone Intensity (Cyclone), House Prices (House). More details can be found in Appendix A.1.1.

**Baselines.** We employ three categories of baselines: **Practical baselines**, including 1) Offline; 2) LastDomain; 3) IncFinetune; 4) IRM ; 5) V-REx . **Discrete temporal domain generalization methods**, including 1) CIDA ; 2) TKNets ; 3) DRAIN ; 4) DRAIN-\( t\). **Continuous temporal domain generalization methods**, including 1) DeepODE . Comparison method details can be found in Appendix A.1.2.

**Metrics.** Error rate (%) is used for classification tasks. As the Twitter dataset has imbalanced labels, the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve is used. Mean Absolute Error (MAE) is used for regression tasks. All models were trained on training domains and then deployed on all unseen test domains. Each method's experiments were repeated five times, with mean results and standard deviations reported. Detailed parameter settings for each dataset are provided in Appendix A.1.3.

### Quantitative analysis: generalization across continuous temporal domains

We first present the performance of our proposed method against baseline methods, highlighting results from Table 1. Koodos exhibits outstanding generalizability across continuous temporal domains. A key observation is that all baseline models struggle to handle synthetic datasets, particularly challenged by the continuous and substantial concept drift (i.e., 18 degrees of rotation per second). In real-world datasets, methods like CIDA, DRAIN, and DeepODE demonstrate effectiveness in certain cases. However, the performance gap between them and Koodos highlights the importance of explicitly considering continuous temporal modeling. For instance, while DRAIN attempts to address domain dynamics through a probabilistic sequential approach via LSTM, this introduces considerable errors due to the inherent discrete temporal. Moreover, while DRAIN-\( t\) and DeepODE adjust for temporal irregularities accordingly, they fail to adequately synchronize the data and model dynamics, leading to unsatisfactory results. In contrast, Koodos establishes a promising approach and a benchmark in CTDG tasks, with quantitative analysis firmly confirming the approach's superiority.

### Qualitative analysis: data dynamics and the learned model dynamics

We conducted a qualitative comparison of different models by visualizing their decision boundaries on the 2-Moons dataset, as depicted in Fig. 3. Each row represents a different method: DRAIN-\( t\), DeepODE, and Koodos, with the timeline at the bottom tracking progress through test domains. DRAIN-\( t\) displays the highest error rate, showing substantial deviation from the anticipated trajectories, especially after the third domain. We also observe that DRAIN-\( t\) seems to freeze when predicting multiple steps, likely due to its underlying model, DRAIN, uses a recursive training strategy within a single domain and is explicitly designed for one-step prediction. DeepODE shows a relatively better performance. It benefits from leveraging ODEs to maintain continuity in the model dynamics. However, the nonlinear variation of the predictive model parameters complicates its ability to abstract and simplify the real dynamics. Its predictions start close to the desired paths but diverge over time. Finally, Koodos exhibits the highest performance with clear and concise boundaries,

    &  &  \\   & **2-Moons** & **Rot-MNIST** & **Twitter** & **Yearbook** & **Cyclone** & **House** \\ 
**Offline** & 13.5 \(\) 0.3 & 6.6 \(\) 0.2 & 0.54 \(\) 0.09 & 8.6 \(\) 1.0 & 18.7 \(\) 1.4 & 19.9 \(\) 0.1 \\
**LastDomain** & 55.7 \(\) 0.5 & 74.2 \(\) 0.9 & 0.54 \(\) 0.12 & 11.3 \(\) 1.3 & 22.3 \(\) 0.7 & 20.6 \(\) 0.7 \\
**IncFinetune** & 51.9 \(\) 0.7 & 57.1 \(\) 1.4 & 0.52 \(\) 0.01 & 11.0 \(\) 0.8 & 19.9 \(\) 0.7 & 20.6 \(\) 0.2 \\
**IRM** & 15.6 \(\) 0.2 & 8.6 \(\) 0.4 & 0.53 \(\) 0.11 & 8.3 \(\) 0.5 & 18.0 \(\) 0.8 & 19.8 \(\) 0.2 \\
**V-REx** & 12.8 \(\) 0.2 & 8.6 \(\) 0.3 & 0.58 \(\) 0.05 & 8.9 \(\) 0.5 & 17.7 \(\) 0.5 & 20.2 \(\) 0.1 \\
**CIDA** & 18.7 \(\) 2.0 & 8.3 \(\) 0.7 & 0.63 \(\) 0.03 & 8.4 \(\) 0.8 & 17.0 \(\) 0.4 & 10.2 \(\) 1.0 \\
**TKNets** & 39.6 \(\) 1.2 & 37.7 \(\) 2.0 & 0.57 \(\) 0.04 & 8.4 \(\) 0.3 & N/A & N/A \\
**DRAIN** & 53.2 \(\) 0.9 & 59.1 \(\) 2.3 & 0.57 \(\) 0.04 & 10.5 \(\) 1.0 & 23.6 \(\) 0.5 & 9.8 \(\) 0.1 \\
**DRAIN**-\( t\) & 46.2 \(\) 0.8 & 57.2 \(\) 1.8 & 0.59 \(\) 0.02 & 11.0 \(\) 1.2 & 26.2 \(\) 4.6 & 9.9 \(\) 0.1 \\
**DeepODE** & 17.8 \(\) 5.6 & 48.6 \(\) 3.2 & 0.64 \(\) 0.02 & 13.0 \(\) 2.1 & 18.5 \(\) 3.3 & 10.7 \(\) 0.4 \\
**Koodos (Ours)** & **2.8 \(\) 0.7** & **4.6 \(\) 0.1** & **0.71 \(\) 0.02** & **6.6 \(\) 1.3** & **16.4 \(\) 0.3** & **9.0 \(\) 0.2** \\   

Table 1: Performance comparison on continuous temporal domain datasets. The classification tasks report Error rates (%) except for the AUC for the Twitter dataset. The regression tasks report MAE. ‘N/A’ implies that the method does not support the task.

consistently aligning with the actual dynamics and maintaining high fidelity across all tested domains, showcasing its robustness in continuous temporal domain modeling and generalization.

Fig. 4(a) demonstrates the space-time evolution of the generalization process of Koodos. By applying t-SNE, the predictive model parameters are reduced to a 2-dimensional representation space, plotted against the time on the Z-axis. We used Koodos to interpolate the model states (35 seconds displayed in blue-yellow line) among the training domain states (marked by blue docs) in steps of 0.2 seconds, and similarly extrapolated 75 steps (15 seconds displayed in red line). The visualization clearly shows that Koodos synchronizes the model dynamics with the data dynamics: the interpolation creates a cohesive, upward-spiraling trajectory transitioning from the first to the last training domain, while the extrapolation correctly extends this trajectory outward into a new area, demonstrating the effective of Koodos from another intuitive way. We also show the space-time evolution of baseline models in Fig. 4(b,c), in which we do not find meaningful patterns of the generalization process.

### Analysis and control of the generalization process

The learned Koopman operator provides valuable insights into the dynamics of generalized predictive models. We analyze the behavior of the learned Koodos model on the 2-Moons dataset, focusing on the eigenvalues of the Koopman operator. As illustrated in Fig. 5(a), the eigenvalues are distributed

Figure 4: Interpolated and extrapolated predictive model trajectories. **Left**: Koodos captures the essence of generalization through the harmonious synchronization of model and data dynamics; **Middle**: DRAIN, as a probabilistic model, fails to capture continuous dynamics, which is presented as jumps from one random state to another. **Right**: DeepODE demonstrates a certain degree of continuity, but the dynamics are incorrect.

Figure 3: Visualization of decision boundary of the 2-Moons dataset (purple and yellow show data regions, red line shows the decision boundary). Top to bottom compares two baseline methods with ours; left to right shows partial test domains (all test domains are marked with red points on the timeline). All models are learned using data before the last train domain.

across both stable (Re<0) and unstable (Re>0) regions on the complex plane. The spectral distribution suggests that while Koodos performs effectively across all tested domains, it will eventually diverge towards instability and finally collapse. To validate, we extended the extrapolation of the Koodos to an extremely long-term (i.e., 35 seconds future). Results depicted in Fig. 6(a) demonstrate that the generalized model's trajectory significantly deviates from the anticipated spiral path, suggesting that extremely long-term generalization will end up with the accumulation of errors.

Fortunately, Koodos's innovative design allows it to incorporate knowledge that extends beyond the observable data. By configuring the Koopman operator as \(=B-B^{T}\), we ensure that all eigenvalues of the final learned \(\) are purely imaginary (termed Koodos\({}^{+}\)), promoting stable and periodic behavior. This adjustment is reflected in Fig. 5(b), where the eigenvalues are tightly clustered around the imaginary axis. As shown in Fig. 6(b), the embeddings and trajectories of Koodos\({}^{+}\) are cohesive and maintain stability over extended periods. Remarkably, even with 85 seconds of extrapolation, Koodos\({}^{+}\) shows no signs of performance degradation, highlighting the significance of human inductive bias in improving the robustness and reliability of the generalization process.

## 6 Conclusion

We tackle the problem of continuous temporal domain generalization by proposing a continuous dynamic system network, Koodos. We characterize the dynamics of the data and determine its impacts on model dynamics. The Koopman operator is further learned to represent the underlying dynamics. We also design a comprehensive optimization framework equipped with analysis and control tools. Extensive experiments show the efficiency of our design.