# CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference

Wenxuan Zeng

Peking University

zwx.andy@stu.pku.edu.cn

&Meng Li\({}^{*}\)

Peking University

meng.li@pku.edu.cn

&Haichuan Yang

Meta AI

haichuan@meta.com

&Wen-jie Lu

Ant Group

juhou.lwj@antgroup.com

&Runsheng Wang

Peking University

r.wang@pku.edu.cn

&Ru Huang

Peking University

ruhuang@pku.edu.cn

Corresponding author.

###### Abstract

Deep neural network (DNN) inference based on secure 2-party computation (2PC) can offer cryptographically-secure privacy protection but suffers from orders of magnitude latency overhead due to enormous communication. Previous works heavily rely on a proxy metric of ReLU counts to approximate the communication overhead and focus on reducing the ReLUs to improve the communication efficiency. However, we observe these works achieve limited communication reduction for state-of-the-art (SOTA) 2PC protocols due to the ignorance of other linear and non-linear operations, which now contribute to the majority of communication. In this work, we present CoPriv, a framework that jointly optimizes the 2PC inference protocol and the DNN architecture. CoPriv features a new 2PC protocol for convolution based on Winograd transformation and develops DNN-aware optimization to significantly reduce the inference communication. CoPriv further develops a 2PC-aware network optimization algorithm that is compatible with the proposed protocol and simultaneously reduces the communication for all the linear and non-linear operations. We compare CoPriv with the SOTA 2PC protocol, CrypTFlow2, and demonstrate 2.1\(\) communication reduction for both ResNet-18 and ResNet-32 on CIFAR-100. We also compare CoPriv with SOTA network optimization methods, including SNL, MetaPruning, etc. CoPriv achieves 9.98\(\) and 3.88\(\) online and total communication reduction with a higher accuracy compared to SNL, respectively. CoPriv also achieves 3.87\(\) online communication reduction with more than 3% higher accuracy compared to MetaPruning.

## 1 Introduction

Deep learning has been applied to increasingly sensitive and private data and tasks, for which privacy emerges as one of the major concerns. To alleviate the privacy concerns when deploying the deep neural network (DNN) models, secure two-party computation (2PC) based DNN inference is proposed and enables cryptographically-strong privacy guarantee [39; 28; 48; 44; 25; 24; 47].

Secure 2PC helps solve the following dilemma [28; 48; 44]: the server owns a private model and the client owns private data. The server is willing to provide the machine learning as a service (MLaaS) but does not want to give it out directly. The client wants to apply the model on the private data without revealing it as well. Secure 2PC frameworks can fulfill both parties' requirements: two parties can learn the inference results but nothing else beyond what can be derived from the results.

The privacy protection of secure 2PC-based inference is achieved at the cost of high communication complexity due to the massive interaction between the server and the client . This leads to orders of magnitude latency gap compared to the regular inference on plaintext . A 2PC-based inference usually has two stages, including a pre-processing stage that generates the input independent helper data and an online stage to process client's actual query . Because the helper data is independent of client's query, previous works  assume the pre-processing stage is offline and thus, focus on optimizing the communication of the input dependent stage. Specifically,  observe ReLU accounts for significant communication and ReLU count is widely used as a proxy metric for the inference efficiency. Hence, the problem of improving inference efficiency is usually formulated as optimizing networks to have as few ReLUs as possible. For example, DeepReduce  and SNL  achieve more than 4\(\) and 16\(\) ReLU reduction with less than 5% accuracy degradation on the CIFAR-10 dataset.

However, such assumption may no longer be valid for state-of-the-art (SOTA) 2PC protocols. We profile the communication of ResNet-18 with different ReLU optimization algorithms, including DeepReduce , SENet , and SNL  and observe very limited communication reduction as shown in Figure 1(b). This is because, on one hand, the communication efficiency of ReLU has been drastically improved over the past few years from garble circuit (GC) to VOLE oblivious transfer (OT) as shown in Figure 1(a) , and ReLU only accounts for 40% and 1% of the online and total communication, respectively. On the other hand, recent studies suggest for MLaaS, it is more important to consider inference request arrival rates rather than studying individual inference in isolation . In this case, the pre-processing communication, which is significantly higher than the online communication as shown in Figure 1(b), _cannot be ignored and may often be incurred online as there may not be sufficient downtime for the server to hide its latency_.

Therefore, to improve the efficiency of 2PC-based inference, in this paper, we argue that both pre-processing and online communication are important and propose CoPriv to jointly optimize the 2PC protocol and the DNN architecture. CoPriv first optimizes the inference protocol for the widely used 3-by-3 convolutions based on the Winograd transformation and proposes a series of DNN-aware protocol optimization. The optimized protocol achieves more than 2.1\(\) communication reduction for ResNet models  without accuracy impact. For lightweight mobile networks, e.g., MobileNetV2 , we propose a differentiable ReLU pruning and re-parameterization algorithm in CoPriv. Different from existing methods , CoPriv optimizes both linear and non-linear operations to simultaneously improve both the pre-processing and online communication efficiency.

With extensive experiments, we demonstrate CoPriv drastically reduces the communication of 2PC-based inference. The proposed optimized protocol with Winograd transformation reduces the convolution communication by 2.1\(\) compared to prior-art CrypTFlow2  for both the baseline ResNet models. With joint protocol and network co-optimization, CoPriv outperforms SOTA ReLU-optimized methods and pruning methods. Our study motivates the community to directly use the communication as the efficiency metric to guide the protocol and network optimization.

## 2 Preliminaries

### Threat Model

CoPriv focuses on efficient private DNN inference involving two parties, i.e., Alice (server) and Bob (client). The server holds the model with private weights and the client holds private inputs. At the end of the protocol execution, the client learns the inference results without revealing any

Figure 1: (a) Communication for ReLU communication has reduced by 131\(\) from  to ; (b) existing network optimizations suffers from limited reduction of online and total communication.

information to the server. Consistent with previous works [39; 48; 43; 42; 26; 7; 32], we adopt an _honest-but-curious_ security model in which both parties follow the protocol but also try to learn more from the information than allowed. Meanwhile, following [39; 28; 48; 43; 42], we assume no trusted third party exists so that the helper data needs to be generated by the client and the server.

### Arithmetic Secret Sharing

CoPriv leverages the 2PC framework based on arithmetic secret sharing (ArSS). Specifically, an \(l\)-bit value \(x\) is shared additively in the integer ring \(_{2^{l}}\) as the sum of two values, e.g., \( x_{s}\) and \( x_{c}\). \(x\) can be reconstructed as \( x_{s}+ x_{c}\) mod \(2^{l}\). In the 2PC framework, \(x\) is secretly shared with the server holding \( x_{s}\) and the client holding \( x_{c}\).

ArSS supports both addition and multiplication on the secret shares. Addition can be conducted locally while multiplication requires helper data, which are independent of the secret shares and is generated through communication [43; 42]. The communication cost of a single multiplication of secret shares is \(O(l(+l))\). When \(t\) multiplications share the same multiplier, instead of simply repeating the multiplication protocol for \(t\) times, [8; 31; 43] proposes a batch optimization algorithm that only requires \(O(l(+tl))\) communication, enabling much more efficient batched multiplications.

Most 2PC frameworks use fixed-point arithmetic. The multiplication of two fixed-point values of \(l\)-bit precision results in a fixed-point value of \(2l\)-bit precision. Hence, in order to perform subsequent arithmetics, a truncation is required to scale the value down to \(l\)-bit precision. Truncation usually executes after the convolutions and before the ReLUs. It requires complex communication and sometimes leads to even more online communication compared to ReLU as shown in Figure 1. We refer interested readers to  for more details.

### Winograd Convolution

We first summarize all the notations used in the paper in Table 2. Then, consider a 1D convolution with \(m\) outputs and filter size of \(r\), denoted as \(F(m,r)\). With regular convolution, \(F(m,r)\) requires \(mr\) multiplications between the input and the filter. With the Winograd algorithm, \(F(m,r)\) can be computed differently. Consider the example of \(F(2,3)\) below:

\[X=x_{0}&x_{1}&x_{2}&x_{4}^{}  W=w_{0}&w_{1}&w_{2}^{} Y= y_{0}&y_{1}^{}\] \[x_{0}&x_{1}&x_{2}\\ x_{1}&x_{2}&x_{3}w_{0}\\ w_{1}\\ w_{2}=m_{0}+m_{1}+m_{2}\\ m_{1}-m_{2}-m_{3}=y_{0}\\ y_{1}\]

where \(m_{0},m_{1},m_{2},m_{3}\) are computed as

\[m_{0}=(x_{0}-x_{2})w_{0}&m_{1}=(x_{1}+x_{2})(w_{0}+w_{1}+w_ {2})/2\\ m_{3}=(x_{1}-x_{3})w_{2}&m_{2}=(x_{2}-x_{1})(w_{0}-w_{1}+w_{2})/2\]

With the Winograd algorithm, the number of multiplications is reduced from 6 in the regular convolution to 4. Similar Winograd transformation can be applied to 2D convolutions by nesting 1D algorithm with itself . The 2D Winograd transformation \(F(m m,r r)\), where the output tile size is \(m m\), the filter size is \(r r\), and the input tile size is \(n n\), where \(n=m+r-1\), can be formulated as follows,

\[Y=W X=A^{}[(GWG^{})(B^{}XB)]A,\] (1)

where \(\) denotes the regular convolution and \(\) denotes element-wise matrix multiplication (EWMM). \(A\), \(B\), and \(G\) are transformation matrices that are independent of \(W\) and \(X\) and can be computed based on \(m\) and \(r\)[34; 1].

    &  &  \\  \((43,42,39,8,38,29,41]\) & ✓ & ✗ & ✗ & ✗ & ✗ \\ \(\) & ✗ & ✗ & ✗ & ✓ & ✓ \\ \(\) & ✓ & ✗ & ✗ & ✓ & ✓ \\ \(\) & ✓ & ✓ & ✗ & ✗ & ✗ \\ \(\)Cohr (ours) & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with prior-art methods.

   Notation & Meaning \\  \(H,W\) & Height and width of output feature map \\ \(C,K\) & Input and output channel numbers \\ \(A,B,G\) & Winograd transformation matrices \\ \(m,r\) & Size of output tile and convolution filter \\ \(T\) & Number of tiles per input channel \\ \(\) & Security parameter that measures the attack hardness \\   

Table 2: Notations used in the paper.

With the Winograd algorithm, the multiplication of such a 2D convolution can be reduced from \(m^{2}r^{2}\) to \(n^{2}\), i.e., \((m+r-1)^{2}\) at the cost of performing more additions which can be computed locally in private inference scenario. While the reduction of multiplication increases with \(m\), \(m=2\) and \(m=4\) are most widely used for the better inference precision [34; 49; 3; 15]. More details about Winograd convolution are available in Appendix D.

### Related Works

To improve the efficiency of private inference, existing works can be categorized into three classes, including protocol optimization [8; 38; 43; 42; 29; 41; 39], network optimization [26; 7; 19; 40; 37; 6; 32; 27; 51; 35], and joint optimization [48; 25; 17]. In Table 1, we compare CoPriv with these works qualitatively and as can be observed, CoPriv leverages both protocol and network optimization and can simultaneously reduce the online and total communication induced by convolutions, truncations, and ReLUs through network optimization. We leave more detailed review of existing works to Appendix A and more detailed comparison in terms of their techniques to Appendix F.

## 3 Motivation

In this section, we analyze the origin of the limited communication reduction of existing ReLU-optimized networks and discuss our key observations that motivates CoPriv.

Observation 1: the total communication is dominated by convolutions while the online communication cost of truncations and ReLUs are comparable. As shown in Figure 1, the main operations that incurs high communication costs are convolutions, truncations, and ReLUs. With CrypTFlow2, over 95% communication is in the pre-processing stage generated by convolutions, while the truncations and ReLUs requires similar communication. This observation contradicts to the assumption of previous works [7; 6; 19; 32]: on one hand, ReLU no longer dominates the online communication and simply reducing ReLU counts leads to diminishing return for online communication reduction; on the other hand, pre-processing communication cannot be ignored as it not only consumes a lot of power in the server and client but also can slow down the online stage. As pointed out by a recent system analysis , for a pratical server that accepts inference request under certain arrival rates, the pre-processing stage is often incurred online as there is insufficient downtime to hide its latency. Therefore, we argue that both the total and the online communication are crucial and need to be reduced to enable more practical 2PC-based inference.

Observation 2: the communication cost scaling of convolution, truncation and ReLU is different with the network dimensions.The communication cost of different operations scales differently with the network dimensions. For a convolution, the major operations are multiplications and additions. While additions are computed fully locally, multiplications requires extra communication to generate the helper data. Hence, the communication cost for a convolution scales linearly with the number of multiplications and the complexity is \(O(CKr^{2}(+HW))\) following . Both truncations and ReLUs are element-wise and hence, their communication cost scales linearly with the input feature size, i.e., \(O(HWC)\). Therefore, for networks with a large number of channels, the pre-processing communication becomes even more dominating.

Since the communication is dominated by the multiplications in convolution layers, _our intuition is to reduce the multiplications by conducting more local additions for better communication efficiency_. Meanwhile, as in Figure 2, MobileNetV2 are much more communication efficient compared to ResNets with a similar accuracy, and hence, becomes the focus of our paper.

Observation 3: ReLU pruning and network re-parameterization can simultaneously reduce the online and total communication of all operations.Existing network optimizations either linearize the ReLUs selectively [26; 7; 40; 37; 6; 32] or prune the whole channel or layer . Selective ReLU linearization usually achieves a better accuracy but achieves limited communication reduction. In contrast, channel-wise or layer-wise pruning can simultaneously reduce the communication for all the operators but usually suffers from a larger accuracy degradation . We observe for an inverted residual block in MobileNetV2, as shown in Figure 3, if both of the two ReLUs are removed, the block can be merged into a dense convolution after training. _During training, this preserves the benefits of over-parameterization shown in RepVGG  to protect network accuracy, while during inference, the dense convolution is more compatible with our optimized protocol and can reduce communication of truncations as well._

## 4 CoPriv: A New Paradigm Towards Efficient Private Inference

In this section, we present CoPriv, a protocol-network co-optimization framework for communication-efficient 2PC-based inference. The overview of CoPriv is shown in Figure 4. Given the original network, we first leverage the Winograd transformation with DNN-aware optimization to reduce the communication when computing 3-by-3 convolutions without any accuracy impact (Section 4.1). The optimized transformation enables to reduce the total communication of ResNet-18 by 2.1\(\). However, the communication reduction of MobileNetV2 is limited as the Winograd transformation can only be applied to the 3-by-3 depth-wise convolution, which only contributes to a small portion of the total communication. Hence, for MobileNetV2, we propose an automatic and differentiable ReLU pruning algorithm (Section 4.2). By carefully designing the pruning pattern, CoPriv enables further network re-parameterization of the pruned networks, which reduces the online and total communication by 5\(\) and 5\(\) in the example, respectively, combining with our optimized convolution protocol.

### Winograd Transformation for Protocol Optimization

We propose to leverage the Winograd transformation to reduce the communication when computing a convolution. Following Eq. 1, for each 2D output tile, because \(A,B,G\) are known publicly, the filter transformation \(GWG^{}\) and feature transformation \(B^{}XB\) can be computed locally on the server and client . Then, EWMM is performed together by the server and the client through communication while the final output transformation \(A^{}[]A\) can be computed locally as well.

For each output tile, as \((m+r-1)^{2}\) multiplications are needed and there is no shared multiplier for batch optimization described in Section 2.2, the communication cost is \(O((m+r-1)^{2})\)2. Consider there are in total \(T= H/m W/m\) tiles per output channel, \(C\) input channels, and \(K\) output channels, the communication complexity of a convolution layer now becomes \(O( TCK(m+r-1)^{2})\). To reduce the enormous communication, we present the following optimizations.

Figure 4: Overview of CoPriv and the communication cost after each optimization step. The example of the communication cost is measured on the CIFAR-100 dataset.

Communication reduction with tile aggregationWhile the Winograd transformation helps to reduce the total number of multiplications, in Table 3, we observe the communication for a ResNet block actually increases. This is because the baseline protocol can leverage the batch optimization mentioned in Section 2.2 to reduce the communication complexity. To further optimize the Winograd-based convolution protocol to adapt to private inference, we observe the following opportunities for the batch optimization: first, each transformed input tile needs to multiply all the \(K\) filters; secondly, each transformed filter needs to multiply all the transformed input tiles for a given channel. Hence, for the \(t\)-th tile in \(f\)-th output channel, we re-write Eq. 1 as follows:

\[Y_{f,t}=A^{}[_{c=1}^{C}U_{f,c} V_{c,t}]A 1 f K,1 t  T,\]

where \(U_{f,c}\) denotes the transformed filter correspnding to the \(f\)-th output channel and \(c\)-th input channel and \(V_{c,t}\) denotes the transformed input of the \(t\)-th tile in the \(c\)-th input channel. Then, consider each pixel location, denoted as \((,)\), within the tile separately, yielding:

\[Y_{f,t}^{(,)}=A^{}[U_{f,c}^{(,)}V_{c,t}^{(,)}]A 1  f K,1 t T,1, m+r-1\]

We illustrate this transformation procedure in Figure 5. Re-writing the EWMM into general matrix multiplication (GEMM) enables us to fully leverage the batch optimization and reduce the communication complexity as shown in Table 3. We can now reduce the communication of a ResNet block by 2.27\(\).

DNN-aware adaptive convolution protocolWhen executing the convolution protocol, both the server and the client can initiate the protocol. Because the input feature map and the filter are of different dimensions, we observe the selection of protocol initializer impacts the communication round as well as the communication complexity. While CrypTFlow2 always selects the server to initialize the protocol, we propose DNN architecture-aware convolution protocol to choose between the server and the client adaptively based on the layer dimensions to minimize the communication complexity. As shown in Table 3, the communication of the example ResNet block can be further reduced by 1.35\(\) with the adaptive protocol.

### Differentiable ReLU Pruning and Network Re-Parameterization

We now describe our network optimization algorithm to further reduce both pre-processing and online communication for MobileNetV2. _The core idea is to simultaneously remove the two ReLUs

Figure 5: Winograd transformation procedure with tile aggregation.

  Conv. Type & Comm. Complexity & ResNet Block Comm. (GB) \\  Regular Conv. & \(O(CKr^{2}(+HW))\) & 23.74 (1\(\)) \\  EWMM-based Winograd Conv. & \(O( CKT(m+r-1)^{2})\) & 256 (10.78\(\) \(\)) \\  + Tile Aggregation (GEMM-based) & \((m+r-1)^{2}CT(+K)\) & 10.45 (2.27\(\) \(\)) \\  + DNN-aware Sender Selection & \([(m+r-1)^{2}CT(+K),\) & 7.76 (3.06\(\) \(\)) \\  

Table 3: Communication complexity of different convolution types and the measured communication for a ResNet-18 block with 14\(\)14\(\)256 feature dimension.

within an inverted residual block together, after which the entire block can be merged into a dense convolution layer to be further optimized with our Winograd-based protocol_ as in Figure 3. For the example in Figure 4, both the pre-processing and online communication for MobileNetV2 can be reduced by 5\(\). To achieve the above goal, the remaining questions to answer include: 1) which ReLUs to remove and how to minimize the accuracy degradation for the pruned networks, and 2) how to re-parameterize the inverted residual block to guarantee functional correctness. Hence, we propose the following two-step algorithm.

Step 1: communication-aware differentiable ReLU pruningTo identify "unimportant" activation functions, CoPriv leverages a differentiable pruning algorithm. Specifically, CoPriv assigns an architecture parameter \((0 1)\) to measure the importance of each ReLU. During pruning, the forward function of a ReLU now becomes \((x)+(1-) x\). CoPriv jointly learns the model weights \(\) and the architecture parameters \(\). Specifically, given a sparsity constraint \(s\), we propose the one-level optimization formulated as follows:

\[_{,}_{CE}+_{comm} ||||_{0} s,\] (2)

where \(_{CE}\) is the task-specific cross entropy loss and \(_{comm}\) is the communication-aware regularization to focus the pruning on communication-heavy blocks and is defined as

\[_{comm}=_{i}_{i}(_{i}(_{i}=1)- _{i}(_{i}=0)),\]

where \(_{i}(_{i}=1)\) is the communication to compute the \(i\)-th layer when \(_{i}=1\). We also show the effectiveness of communication-aware regularization in the experimental results. To ensure the network after ReLU pruning can be merged, two ReLUs within the same block share the same \(\). During training, in the forward process, only the top-\(s\)\(\)'s are activated (i.e., \(=1\)) while the remainings are set to 0. In the backpropagation process, \(\)'s are updated via straight-through estimation (STE) .

Once the pruning finishes, the least important ReLUs are removed and we perform further finetuning to improve the network accuracy. Specifically, in CoPriv, we leverage knowledge distillation (KD)  to guide the finetuning of the pruned network.

Step 2: network re-parameterizationThe removal of ReLUs makes the inverted residual block linear, and thus can be further merged together into a single dense convolution. Motivated by the previous work of structural re-parameterization [16; 14; 13; 11; 10; 12], we describe the detailed re-parameterization algorithm in Appendix C. The re-parameterized convolution has the same number of input and output channels as the first and last point-wise convolution, respectively. Its stride equals to the stride of the depth-wise convolution.

## 5 Experiments

### Experimental Setup

We adopt CypTFlow2  protocol for the 2PC-based inference, and we measure the communication and latency under a LAN setting  with 377 MBps bandwidth and 0.3ms echo latency. For Winograd, we implement \(F(2 2,3 3)\) and \(F(4 4,3 3)\) transformation for convolution with stride of 1 and \(F(2 2,3 3)\) transformation when stride is 2 . We apply CoPriv to MobileNetV2 with different width multipliers on CIFAR-100  and ImageNet  datasets. The details of our experimental setups including the private inference framework, the implementation of the Winograd-based convolution protocol and the training details are available in Appendix B.

### Micro-Benchmark on the Convolution Protocol with Winograd Transformation

We first benchmark the communication reduction of the proposed convolution protocol based on Winograd transformation. The results on ResNet-18 and ResNet-32 are shown in Figure 6(a) and (b). As can be observed, the proposed protocol consistently reduces the communication to compute the convolutions. While the degree of communication reduction depends on the layer dimensions, on average, 2.1\(\) reduction is achieved for both ResNet-18 and ResNet-32. We also benchmark the Winograd-based protocol with output tile sizes of 2 and 4, i.e., \(F(2 2,3 3)\) and \(F(4 4,3 3)\). Figure 6(c) shows that when the feature resolution is small, a small tile size achieves more communication reduction while larger resolution prefers a larger tile size. Hence, for the downstream experiments, we use \(F(2 2,3 3)\) for CIFAR-100 and \(F(4 4,3 3)\) for ImageNet  dataset, respectively.

### Benchmark with ReLU-Optimized Networks on CIFAR-100

We benchmark CoPriv with different network optimization algorithms that focus on reducing ReLU counts, including DeepReDuce , SENet  and SNL , on the CIFAR-100 dataset. As DeepReDuce, SENet and SNL all use ResNet-18 as the baseline model, we also train SNL algorithm on the MobileNetV2-w0.75 for a comprehensive comparison.

Results and analysisFrom Figure 7, we make the following observations: 1) previous methods, including DeepReDuce, SENet, SNL, do not reduce the pre-processing communication and their online communication reduction quickly saturates with a notable accuracy degradation; 2) CoPriv achieves SOTA Pareto front of accuracy and online/total communication. Specifically, CoPriv outperforms SNL on ResNet-18 with 4.3% higher accuracy as well as 1.55\(\) and 19.5\(\) online and total communication reduction, respectively. When compared with SNL on MobileNetV2-w0.75, CoPriv achieves 1.67% higher accuracy with 1.66\(\) and 2.44\(\) online and total communication reduction, respectively.

### Benchmark on ImageNet

We also benchmark CoPriv on the ImageNet dataset with the following baselines: lightweight mobile networks like MobileNetV3  with different capacities, ReLU-optimized networks, including SNL  and SENet , and SOTA pruning methods, including uniform pruning and MetaPruning . We train both our CoPriv and SNL on the MobileNetV2-w1.4 with self distillation.

Results and analysisFrom the results shown in Figure 8, we observe that 1) compared with SNL on MobileNetV2-w1.4, CoPriv achieves 1.4% higher accuracy with 9.98\(\) and 3.88\(\) online and total communication reduction, respectively; 2) CoPriv outperforms 1.8% higher accuracy compared

Figure 6: Communication of the convolution protocol with Winograd transformation on (a) ResNet-18 and (b) ResNet-32 on CIFAR-100; (c) comparison between \(F(2 2,3 3)\) and \(F(4 4,3 3)\) with different feature dimensions.

Figure 7: Comparison with efficient ReLU-optimized methods on CIFAR-100 dataset.

with MobileNetV3-Small-1.0 while achieving 9.41\(\) and 1.67\(\) online and total communication reduction; 3) CoPriv demonstrates its strong scalability for communication optimization. Compared to the baseline MobileNetV2-w1.4, CoPriv achieves 2.52\(\) and 1.9\(\) online and total communication reduction, respectively without compromising the accuracy; 4) when compared with SOTA pruning methods, for a high communication budget, CoPriv-w1.4-\(}\) achieves 1.6% higher accuracy with 2.28\(\) and 1.08\(\) online and total communication reduction, compared with MetaPruning-1.0\(\); for a low communication budget, compared with MetaPruning-0.35\(\), CoPriv-w1.4-D achieves 5.5% higher accuracy with 3.87\(\) online communication reduction.

### Ablation Study

Effectiveness of different optimizations in CoPrivTo understand how different optimizations help improve communication efficiency, we add our proposed protocol optimizations step by step on both SNL and CoPriv, and present the results in Table 4. According to the results, we find that 1) our Winograd-based convolution protocol consistently reduces the total communication for both networks; 2) when directly applying the Winograd transformation on MobileNetV2, less than 10% communication reduction is achieved as Winograd transformation only helps the depth-wise convolution, which accounts for only a small portion of pre-processing communication; 3) compared with SNL, CoPriv achieves higher accuracy with much lower communication. The findings indicate that all of our optimizations are indispensable for improving the communication efficiency.

Block-wise communication comparison and visualizationWe compare and visualize the block-wise communication reduction of CoPriv on MobileNetV2-w0.75 on the CIFAR-100 dataset. From Figure 9, it is clear that our adaptive convolution protocol is effective for communication optimization, and different layers benefit from CoPriv differently. More specifically, for the total communication, block # 4 benefits more from the Winograd transformation over the network re-parameterization, while block # 16 benefits from both the re-parameterization and the adaptive convolution protocol. Both pruning and re-parameterization are important for online communication as they remove the communication of ReLU and truncation, respectively. The results demonstrate the importance of all the proposed optimization techniques.

  Model & Online Communication (GB) & Total Communication (GB) \\  Baseline SNL (ResNet-18) & 0.507 & 61.45 \\ +Winograd & 0.507 & 33.84 \\  Baseline SNL (MobileNetV2) & 0.530 & 7.710 \\ +Winograd & 0.530 & 7.132 \\  MobileNetV2+Pruning & 0.606 / 0.582 & 7.806 / 7.761 \\ +Re-parameterization & 0.331 / 0.260 & 4.197 / 3.546 \\ +Winograd (CoPriv) & 0.331 / 0.260 & 3.154 / 2.213 \\  

Table 4: Ablation study of our proposed optimizations in CoPriv on CIFAR-100. Two sparsity constraints are on the two sides of “\(\)”, respectively.

Figure 8: Comparison with SOTA efficient ConvNets, ReLU-optimized methods, channel-wise pruning MetaPruning  and uniform pruning on ImageNet dataset. We mark the four points of CoPriv as CoPriv-w1.4-A/B/C/D from left to right, respectively.

### Effectiveness of Communication-Aware Regularization

To show the effectiveness and importance of introducing communication-aware regularization \(_{comm}\) formulated in Eq. 2, we compare our pruning method with \(_{comm}\) and pruning methods of SNL  and SENet  without \(_{comm}\) in Figure 10. As we can observe, \(_{comm}\) indeed helps to focus the pruning on the later layers, which incur more communication cost, and penalizes the costly blocks (e.g., block # 16 and # 18). In contrast, SNL does not take communication cost into consideration and SENet (ReLU sensitivity based) focuses on pruning early layers with more ReLUs, both of which incur huge communication overhead.

## 6 Conclusion

In this work, we propose a network/protocol co-optimization framework, CoPriv, that simultaneously optimizes the pre-processing and online communication for the 2PC-based private inference. CoPriv features an optimized convolution protocol based on Winograd transformation and leverages a series of DNN-aware protocol optimization to improve the efficiency of the pre-processing stage. We also propose a differentiable communication-aware ReLU pruning algorithm with network re-parameterization to further optimize both the online and pre-processing communication induced by ReLU, truncation and convolution. With extensive experiments, CoPriv consistently reduces both online and total communication without compromising the accuracy compared with different prior-art efficient networks and network optimization algorithms.

Figure 10: Comparison of different pruning method and the influence of \(_{comm}\) during the search in each block (ReLU budget is set to 60% in this example).

Figure 9: Block-wise visualization of online and total communication with different techniques on CoPriv-w0.75 on CIFAR-100.