# Sparse High Rank Adapters

Kartikeya Bhardwaj\({}^{*}\)\({}^{@sectionsign}\) Nilesh Prasad Pandey\({}^{*}\)\({}^{}\) Sweta Priyadarshi\({}^{}\) Viswanath Ganapathy\({}^{}\)

Shreya Kadambi Rafael Esteves Shubhankar Borse Paul Whatmough\({}^{@sectionsign}\)

Risheek Garrepalli Mart Van Baalen Harris Teague\({}^{@sectionsign}\) Markus Nagel\({}^{@sectionsign}\)

Qualcomm AI Research\({}^{}\){kbhardwa,pwhatmou,hteague,markusn}@qti.qualcomm.com

Equal contribution. Work done while employed at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Code: https://github.com/Qualcomm-AI-research/SHiRA.

###### Abstract

Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only \(1\)-\(2\%\) of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to \(16\%\) lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be \(5\)-\(16\) faster than LoRA fusion on a CPU.1

## 1 Introduction

Low Rank Adaptation (LoRA)  is an established technique to tune the behavior of large generative models such as Large Language Models (LLMs)  and Stable Diffusion . As the name suggests, LoRA requires very few parameters since it trains low rank projection weights that consume very low memory during the finetuning process while producing excellent results. Moreover, these low rank weights can be fused analytically into the base model, thereby incurring no additional overhead during inference.

Despite its success, there are still several limitations of low rank adaptation methods. First, if LoRA parameters are fused into the corresponding pretrained base model weights, they modify the entire weight tensor. Therefore, deploying LoRA on large models such as LLaMA-1/2 (7B+ parameters) or Stable Diffusion (1.5B+ parameters) on mobile devices would require changing a large number of weights during inference. Consequently, for mobile scenarios, if an application requires _rapid adapter switching_, existing low rank methods would incur a significant memory and latency cost. This is a major deployment challenge because, unlike large GPUs, local memory of small AI accelerators is limited and cannot store all weights at the same time. These challenges can be partially addressed byrunning LoRA in unfused mode; however, unfused inference can incur as high as **30\(\%\) additional latency** compared to the base model  (see section 2.1 for details). This increased inference time in unfused mode and time for adapter switching significantly hampers user experience; hence, this is an important problem which has been a focus of recent research by various industries . Second, LoRA has a well-known limitation called _concept loss_ when using multiple concurrent adapters, e.g., combining multiple style transfer adapters, etc. Specifically, it has been well documented  that a simple additive merging of multiple LoRA adapters leads to concept loss of one or more adapters. Finally, recent literature also contributes important theoretical and empirical knowledge towards the value of _high rank adapters_. For instance, Kalajdzievski  shows that the high rank adapters can greatly outperform low rank adapters when used with correct scaling factors. This calls for further investigation into whether other high rank adapters would significantly outperform LoRA.

In view of the above, we address the following **key problems** in this paper: (_i_) How can we perform rapid switching for fused adapters? (_ii_) Is there a simpler solution for multi-adapter fusion to reduce concept loss? (_iii_) Can we build high rank adapters that have high expressive power without significantly increasing the training or inference costs?

To this end, we propose Sparse High Rank Adapters (SHiRA), a single solution to all three problems above. SHiRA is a highly sparse but a high rank adapter which relies on training only a very small subset of parameters from the original pretrained network. One of the crucial insights we demonstrate is that even finetuning merely \(1\)-\(2\%\) parameters of the pretrained generative model is sufficient to achieve high performance on many adapter tasks (see Fig. 1). However, unlike LoRA layers that modify all parameters in the weight tensors in the fused mode, SHiRA still keeps a very low percentage of parameters that need to be switched, thus enabling rapid switching at inference time. Moreover, since the pretrained weights are huge, SHiRA being a very sparse adapter greatly aids multi-adapter fusion by significantly reducing concept loss. Finally, we theoretically and empirically analyze the high rank vs. sparsity properties of SHiRA and why that helps with adapter performance.

Overall, we make the following **key contributions**:

* We propose SHiRA, a new high rank adapter paradigm to demonstrate that changing as few as \(1\)-\(2\%\) parameters of the original network is sufficient for adaptation. Our crucial insight is that even the most basic masking criteria (to identify the top \(1\)-\(2\%\) parameters) enable SHiRA to significantly outperform LoRA on diverse vision and language tasks.
* SHiRA enables on-device _rapid adapter switching_ and provides a natural multi-adapter fusion technique due to high sparsity, thus, significantly reducing _concept loss_. We also theoretically analyze SHiRA through the lens of _high rank adaptation_ vs. sparsity.
* We conduct extensive experiments on LLMs (LLaMA-7B, LLaMAv2-7B) and LVMs (Stable Diffusion, SDXL) where we demonstrate that SHiRA significantly outperforms LoRA on both single- and multi-adapter tasks. On LLMs, we show that SHiRA achieves up to \(2.7\%\) better accuracy than LoRA on commonsense reasoning. SHiRA also complements advanced variants of LoRA such as DoRA  and can be easily applied on top of them.

Figure 1: Sparse High Rank Adapters (SHiRA): Changing about \(1\)-\(2\%\) weights of the pretrained generative model is often sufficient to achieve high performance. Due to its extreme sparsity, SHiRA enables rapid switching and also reduced concept loss during multi-adapter fusion. In contrast, LoRA modifies majority of parameters when fused, thus prohibiting rapid switching on mobile devices, and also experiences concept loss during multi-adapter fusion. For LoRA, elephant for single “paintings” adapter case has artifacts (extra/broken tasks); bird and knight for multi-adapter case lose “paintings” concept and keep only the “blue fire” effects. SHiRA does not experience these issues.

* Finally, on the training side, we provide a PEFT-based latency- and memory-efficient implementation for SHiRA which trains nearly as fast as standard LoRA while consuming \(16\%\) lower peak GPU memory. Beyond PEFT, we provide a simple way to turn any trainer into SHiRA finetuning. For inference, we demonstrate that SHiRA weights can be loaded on a CPU up to \(5\)-\(16\) faster than equivalent LoRA fusing, thereby enabling rapid switching.

The rest of this paper is organized as follows: section 2 presents the background and related work. We propose SHiRA in section 3 while describing its theoretical properties in section 4. We then conduct extensive experiments for SHiRA in section 5. Finally, we discuss the key findings in section 6 and conclude the paper in section 7.

## 2 Background and Related Work

### Background: Edge Deployment Challenges for LoRA

There are three existing deployment options for LoRA: (_i_) fuse the adapter offline and then deploy on-device: this changes a large fraction of the weight tensors compared to base model which prohibits rapid switching since it will increase DRAM traffic considerably; (_ii_) keep the adapter unfused and run the inference in unfused mode: this can help with rapid switching but would incur significant additional (up to 30% higher) latency as shown in  since we would have LoRA branches in the forward pass during inference; (_iii_) use the Huggingface/Diffusers pipeline  (built for server-grade GPUs) for mobile inference. This pipeline consists of load\(\)fuse\(\)inference\(\)unfuse\(\)unload to switch adapters. Here, unfused LoRA-A and LoRA-B weights (see Fig. 2(a)) are first loaded into the memory and then fused into the base model by computing \(W_{new}=W+AB\); this new weight is used for inference. To switch the adapter, we can unfuse the adapter as \(W=W_{new}-AB\) and then unload existing LoRA weights to load the new ones. We provide further evidence in Appendix A to demonstrate that such a pipeline is not feasible for edge devices. This is primarily because edge devices are memory-limited and not all weights of large generative models can be stored in the local memory at the same time. Hence, loading and fusing needs to happen layerwise on a mobile device that obviously results in massive inference latency costs.

### Related Work

**LoRA, its variants, and sparse adapters.** Many LoRA variants exist in literature: DoRA , LoRA+ , VeRA , LoRA-FA , RS-LoRA , among many others. The crucial difference between this literature and our work is that we develop a high rank adapter without increasing training and inference costs. Also, for such methods, the final fused adapter still updates all elements in the pretrained weight tensor, thus prohibiting rapid switching. Moreover, for completeness, we will also show that SHiRA is orthogonal to and can be applied on top of some of the latest, more advanced LoRA variants such as DoRA  while preserving the benefits of rapid switching.

A few other LoRA variants have also explored a combination of sparsity and low rank adaptation. Examples include RoSA , SoRA , Sparse-Adapters , etc. Among these, Sparse-Adapters  explores the use of popular pruning techniques (e.g., SNIP ) to prune out adapters to improve their efficiency. SoRA  proposes an adaptive rank version of LoRA by gating elements of down and up projection layers and pruning out the zero entries at inference. Finally, RoSA  combines a sparse adapter with a low rank one to achieve some high rank benefits. However, since they combine their method with LoRA, the fused adapter weight still overwrites the entire pretrained weight tensor.

**Partial Finetuning.** Our work is most closely related to partial finetuning techniques that were mostly proposed in the pre-LoRA era [36; 28; 3; 33; 10]. These methods use a mix of fixed sparse masks  or learned masks [36; 10] to finetune a pretrained network. Note that, these techniques have been mostly explored for relatively _small_ language models, and _not_ for recent LLMs and diffusion models. Since the LoRA models exploded in popularity, it has been unclear if other sparse finetuning techniques would achieve comparable results to LoRA on generic adapter tasks, particularly in the vision domain. One _significant_ limitation of partial finetuning, as opposed to LoRA-based methods, is its _high GPU memory consumption_, making it _impractical_ to be used for large generative models. Consequently, the reduced memory consumption for finetuning was a key factor to LoRA's success and its widespread adoption. To this end, we provide a memory- and latency-efficient PEFT-based implementation for SHiRA which trains as efficiently as LoRA, thus requiring significantly lower memory consumption compared to prior partial finetuning techniques. Further, we explore the effectiveness of sparse finetuning on both large language and vision models and provide a detailed analysis on rapid switching and multi-adapter fusion of the high rank adapters.

A notable concurrent work is SplEL  which scales partial finetuning to modern LLMs and also has a PEFT implementation that results in comparable speed and memory as LoRA. The main differences between SplEL and SHiRA are as follows: (_i_) SplEL works with dynamic masks while SHiRA uses a static mask. (_ii_) Dynamic mask in SplEL requires users to install custom sparse linear layer kernels for the GPUs. In contrast, SHiRA does not require installing any custom kernels and directly works with native Pytorch. Hence, SHiRA's biggest advantage is its ease of training/inference deployment. (_iii_) We also analyze multi-adapter fusion properties, e.g., impact of sparsity on orthogonality between adapters, which were not discussed in SplEL. (_iv_) Finally, SHiRA demonstrates its effectiveness on both vision and language tasks, whereas SplEL only discusses the language tasks.

**Multi-Adapter Fusion.** Existing Multi-adapter fusion methods focus on preventing concept loss [8; 34; 26]. However, these methods usually either just use the base LoRA as it is (and then perform some non-trivial postprocessing on them) [34; 26], or some create some minor variants . In contrast, we introduce a new adapter for the concept loss problem where multiple concepts naturally do not interfere with each other. In that respect, our work is orthogonal to the prior multi-adapter fusion work since our adapter can be further postprocessed using such techniques.

## 3 Proposed Approach

### Sparse High Rank Adapters (SHiRA)

SHiRA exploits highly sparse trainable parameters in the pretrained model. In its simplest form, our adapter can be trained by masking gradients such that only a fraction of original weights get updated. Specifically, we do not add any new weights to the forward pass like LoRA (see Fig. 2(a)) but rather make a small percentage of existing weights trainable (see Fig. 2(b) top). To this end, we first create an extremely sparse (\(\)\(98\)-\(99\%\) zeros) mask \(^{n m}=\{0,1\}^{n m}\), where \(n,m\) are dimensions of the pretrained weight matrix. \(\) is then used to mask the gradients during backpropagation using a Hadamard product (see Fig. 2(b) bottom). Thus, very few parameters get updated during training and our adapter consists of just those sparse weights. Concrete gradient masking-based and another latency-/memory-efficient PEFT implementations for SHiRA are discussed in section 3.3.

We consider the following masks \(\) (only \(1\)-\(2\%\) trainable parameters, see also Appendix B):

1. **SHiRA-Struct:** In this structured mask, certain rows or columns of the weight as well as its diagonal are set to be trainable. All other rows/columns are not trainable. The diagonal makes the mask high rank whereas the structured trainable rows/columns - set to 1 to enable gradient flow to corresponding parameters - lead to a rank 1 adapter. Thus, SHiRA-Struct is a combination of a high rank but very sparse adapter and a rank 1 adapter.
2. **SHiRA-Rand:** This mask is obtained by randomly setting \(1\)-\(2\%\) parameters as trainable.
3. **SHiRA-WM:** Here we pick top-K parameters to train based on their weight magnitudes (WM), the absolute value of the weight for each layer.

Figure 2: (a) LoRA when fused into the pretrained model modifies all weights and prevents rapid adapter switching. (b) SHiRA does not require additional weights during training but finetunes very few pretrained weights. Our approach relies on a sparse mask for gradient-masking during training. We show that finetuning as low as \(1\)-\(2\%\) parameters is sufficient to achieve high accuracy.

4. **SHiRA-Grad:** This is a gradient-based mask. We first collect gradients on a small calibration set and then pick top \(1\)-\(2\%\) weights that receive the highest gradient magnitudes.
5. **SHiRA-SNIP:** The SNIP metric from the pruning literature  combines weight magnitude and gradient strategies, i.e., SNIP equals magnitude of the gradient times the weight.

### Rapid Adapter Switching, Multi-Adapter Fusion, and High Rank

Since very few base weights change during the SHiRA training, we can simply extract them out and store them as sparse weights and their indices (see Fig. 3(a)). Hence, SHiRA is comparable to LoRA in model size but overwrites only a fraction of the pretrained weights at inference time. In contrast, LoRA fuses into base weights as \(W_{new}=W+AB\) and changes the entire weight. Note that, we do not actually need to fuse SHiRA but rather just need to overwrite the modified value at the correct index in the pretrained weight tensor. This enables rapid switching on resource-constrained devices. To verify that SHiRA indeed provides rapid switching benefits compared to LoRA, we provide an optimized implementation based on scatter_op to overwrite base model weights instead of fusing them like LoRA. We demonstrate that on a CPU, **weight loading for SHiRA adapters can be up to \(\)-\(\) faster than equivalent LoRA fusing for inference** (see Appendix C and Fig 7).

Next, we discuss multi-adapter fusion in SHiRA. Given two adapters \(_{1}\) and \(_{2}\) with sparse masks \(_{1}\) and \(_{2}\), we ask the following questions: (_i_) What is the impact of sparsity on relative interference between adapters in the multi-adapter setting? (_ii_) Is it possible to create masks that result in nearly orthogonal SHiRA weights so they do not significantly interfere with each other at inference time?

Getting adapters that do not interfere with each other is essential to avoid concept-loss. To this end, we define specific metrics in section 4.2 to analyze orthogonality properties between adapter weights for various SHiRA strategies. We theoretically show that at least one of the SHiRA methods, i.e., SHiRA-Struct can in fact create near-orthogonal adapters. We further experimentally demonstrate in section 5.2.2 that SHiRA-Struct indeed outperforms other methods for multi-adapter fusion.

Finally, since we do not have any low rank weights in the forward pass, our proposed adapters can be high rank albeit highly sparse. We theoretically analyze the rank vs. sparsity properties in section 4.

### Memory- and Latency-Efficient SHiRA Training

We have created two implementations for SHiRA: (_i_) a backward hook-based gradient masking to turn any trainer into SHiRA finetuning (see Appendix D), and (_ii_) a PEFT-based implementation. As discussed in Appendix E, the PEFT-based SHiRA implementation consumes \(\)**lower peak GPU memory and trains almost at a similar speed as LoRA**. On the contrary, DoRA exhibits a \(40.99\%\) and \(28.9\%\) increase in memory and training time respectively compared to LoRA.

## 4 Theoretical Insights for SHiRA

### Rank vs. Sparsity

Below we discuss parameter and learning complexity, parallels between LoRA and SHiRA, as well as its optimization properties from the lens of rank and sparsity.

**Lemma 4.1**.: _The parameter complexity and learning complexity of SHiRA is equal to the number of non-zero elements in the adapter._

Appendix F.1 provides the proof. This lemma suggests that despite high rank property of SHiRA, it would not require significantly larger datasets to converge.

Figure 3: (a) Rapid adapter switching: The sparse finetuned weights can be stored as weights and their indices. At inference time, these weights can be loaded on the base model weights. Since only \(1\)-\(2\%\) weights need to be overwritten, the adapter can be efficiently switched with different weights at inference, eliminating the need for a separate fusion stage. (b) Multi-adapter fusion: Concept-loss can be reduced if multiple adapters do not significantly interfere with each other.

**Lemma 4.2**.: _If we specify a sparsity factor, the LoRA is \(r\) rank approximation of SHiRA with approximation error bounded by \(_{r+1}^{2}\), the \((r+1)^{th}\) singular value of the SHiRA adapter._

The above lemma is proved in section F.2. As a consequence of this lemma, any \(r\) rank LoRA adapter of size \((m,n)\) can be seen as an approximation of a SHiRA adapter with \(mr+rn\) non-zero elements.

**Lemma 4.3**.: _Scaling factor for SHiRA is independent of the rank of the adapter and can be set to 1._

Please see the proof in Appendix F.3. Lemma 4.3 states that we do not need scaling factors to stabilize the training and, therefore, we do not need additional hyperparameters like \(\) or independent learning rates for separate \(A\) and \(B\) matrices like in LoRA or LoRA+ . Of note, the scaling factor \(\) can still be used at inference time to vary the intensity of the adapter.

### Adapter Weight Orthogonality in Multi-Adapter Fusion

In this section, we provide theoretical and empirical insights by studying properties of SHiRA and LoRA adapter designs for multi-adapter fusion.

**Lemma 4.4**.: _Consider two adapters, \( W_{1}\) and \( W_{2}\). If one of the adapters, \( W_{1}\) or \( W_{2}\) lies in the null space of the other, then the adapters will not interfere multiplicatively._

Proof is given in Appendix F.4. The above lemma implies that two adapters can be efficiently fused without interference if they are orthogonal. In order to analyze the orthogonality between any two adapter weights, we define the following metrics:

**Definition 1**.: _Adapter Weight Orthogonality Magnitude (AWOM)_ is defined as the \(l_{2}\) norm of the product \(_{1}^{T}_{2}\) for two sparse adapter weights \(_{1},_{2}^{n m}\). AWOM enables us to understand how far the product \(_{1}^{T}_{2}\) is from a zero matrix \(^{m m}\) (\(_{i,j}=\{0\} i,j\)).

**Definition 2**.: _Adapter Weight Orthogonality Ratio (AWOR)_ is defined as the sparsity ratio of the product \(_{1}^{T}_{2}\). Specifically, \(=[1-(_{1}^{T}_{2}||_{0}}{ m^{2}})]\), where \(m^{2}\) is _#_elements in \(_{1}^{T}_{2}\).

Together, AWOM and AWOR can provide us an idea of relative orthogonality between adapter weights \(_{1}\) and \(_{2}\). Next, we analyze how at least one of the SHiRA strategies (i.e., SHiRA-Struct) can result in near-orthogonal adapters. Recall that, SHiRA-Struct adapters train certain rows/columns and the diagonal elements while keeping all other parameters frozen. Hence, the final trained adapter (after subtracting the pretrained weight) contains a structured pattern of rows/columns and diagonal elements, everything else being zero. Now, without loss of generality, consider two SHiRA-Struct adapters for a layer with square \(m m\) weights: \(_{1}=+_{1}\) and \(_{2}=+_{2}\), where \(_{1}\) and \(_{2}\) are row-wise patterns of trained weights for two different tasks, and \(\) is an identity matrix. Also, \(_{1}\) and \(_{2}\) are non-overlapping, e.g., both have same number of non-zero rows but are offset from each other such that they do not have any common trained rows. Then, the following result holds:

**Lemma 4.5**.: _Non-overlapping SHiRA-Struct adapters are nearly orthogonal: AWOR for non-overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. Since all SHiRA masks are highly sparse, \(_{1}^{T}_{2}\) has a lot of zeros, thus making the adapters nearly orthogonal._

Proof is provided in Appendix F.5.

We demonstrate the orthogonality properties of various adapters and report the simulation results in Fig. 4. For our experiment, we compute AWOM and AWOR for a variety of adapter designs - dense, sparse-LoRA  (sparse LoRA A and B weights), SHiRA-WM and SHiRA-Struct based adapters. As shown in Fig. 4, both dense and sparse LoRA have low AWOR for adapters with larger dimensions, e.g., 4096 \(\) 4096 which is typical in LLMs. This signifies that these adapter weights are non-orthogonal. On the contrary, SHiRA-WM achieves much higher AWOR than the LoRA variants. More interestingly, SHiRA-Struct is nearly orthogonal. Note that, due to high sparsity, AWOM also tends to be much lower for SHiRA adapters than the dense counterparts. Combined with the fact that AWOR of SHiRA

Figure 4: Comparison of average AWOM _(left)_ and AWOR _(right)_ for 50 randomly initialized adapters. We compare different adapters, namely - Dense, Sparse LoRA, SHiRA-WM and SHiRA-Struct.

[MISSING_PAGE_FAIL:7]

We now validate the effectiveness of various SHiRA schemes on multi-adapter fusion. The right two columns in Fig. 1 and Fig. 5 show our results. SHiRA is clearly better at capturing both concepts than LoRA. For example, both bird and knight images in Fig. 1 generated with LoRA lose most of the paintings concept. Similarly, for the fox image in Fig. 5, LoRA does not show significant bluefine concept. In contrast, SHiRA-Struct and SHiRA-SNIP consistently perform well on many different prompts and produce exceptional images for multi-adapter fusion. Please refer to Appendix K.1 (Fig. 10, 11, 12, and 13) for additional results. For certain classes that were not included in the training set for both adapters (e.g., see Koala in Fig. 10, 12, and 13 in Appendix), we observe that LoRA produces significant artifacts whereas SHiRA generates high quality images.

### Language Results

#### 5.3.1 Single Adapter SHiRA Finetuning

Similar to vision results, we demonstrate the effectiveness of SHiRA on language tasks. For our experiments, each adapter (i.e., weight-magnitude, gradient-magnitude, and SNIP based SHiRA) is trained on the combined 170K sample commonsense reasoning dataset released by . Similar to , we train our SHiRA adapters for 3 epochs and compare it against the LoRA baselines. As shown in Table 2, various SHiRA adapters outperform LoRA by 1.9-2.7% on an average on LLaMA-7B. Importantly, SHiRA only modifies 1% base parameter weights as compared to **66.72% (4.5B weights)** changed by LoRA in the fused mode, thus enabling rapid switching on edge devices. Interestingly, we found that SHiRA-Struct does not perform well on language tasks likely because it is a rank 1 + diagonal adapter and may not have sufficient expressive power.

Moreover, when compared to newer techniques like DoRA , our proposed work takes an orthogonal approach by finetuning very few parameters of the pretrained weights. This strategy allows for an efficient integration of our adapter with methods like DoRA to improve the expressiveness of the adapters. As we show in Table 2, our proposed adapter benefits from DoRA based finetuning and achieves almost comparable performance (within 0.3%) to DoRA on an average, with an added benefit of changing only 1% parameters at inference time. In contrast, DoRA would lead to **66.72% (4.5B weights \(\) 9GB memory in FP16 format) parameter change in the fused mode. Therefore, SHiRA is orthogonal to other existing low rank methods and can be efficiently integrated with them.

Figure 5: Comparison between different SHiRA masking methods for single- and multi-adapter image generation. For multi-adapter fusion, SHiRA-Struct outperforms all other adapters by generating exceptional images with high frequency details and good concept fusion (e.g., see fox and flower).

Finally, we experiment with LLAMA2-7B  and demonstrate that SHiRA-SNIP - which achieved the best results on LLaMA-7B - yields significant accuracy gains compared to LoRA and nearly the same accuracy as DoRA (within 0.4%, see Table 3).

#### 5.3.2 Multi-Adapter Fusion on LLMs

We now extend our LLM experiments to the multi-adapter fusion setting. To this end, we create a _new_ setup where we independently train multiple adapters on training sets of individual commonsense reasoning benchmarks, i.e., one adapter each for BoolQ, PIQA, and Arc-Easy. In contrast, each adapter in section 5.3.1 was trained on a combined dataset containing 170K samples from all eight commonsense benchmarks as proposed in . In the present section, the goal is to evaluate how much accuracy drop various adapters experience when we perform multi-adapter fusion. Due to its simplicity towards constructing a mask, we will use SHiRA-WM in the rest of this paper. Further, we explore two settings - overlapping and non-overlapping SHiRA-WM adapters. The overlapping mask consists of top 1% parameters being trained for all tasks. On the other hand, the non-overlapping setting trains the top 1% weights for the first task, next top 1% for the second task, and so on. We compare the performance of both LoRA and SHiRA across the multi-adapter fusion of these three tasks. As shown in Table 4, both overlapping and non-overlapping multi-SHiRA outperform multi-LoRA on all three commonsense benchmarks. This is inline with our theoretical analysis in section 4.2 where we suggest that even unstructured sparse SHiRA adapters such as SHiRA-WM would have more orthogonal behavior than LoRA due to high sparsity (see higher AWOR of SHiRA-WM in Fig. 4(right)). In comparison, independently trained LoRA adapters would have no such property and suffer greatly during multi-adapter fusion. As a result, we see that both SHiRA models outperform LoRA by more than 6.5% accuracy on average. Further analysis of the properties of these trained adapters is discussed in Appendix K.3 (see Table 13 and Fig. 9).

Of note, this experiment also demonstrates the value of creating a good mask for single adapter performance: Non-overlapping masks achieve lower single adapter accuracy than the corresponding overlapping masks since they train less important parameters. Hence, creating an optimal mask for SHiRA should be of significant interest to future research.

### Content/Style Personalization: Generalizing SHiRA to SDXL and DreamBooth

Finally, we extend SHiRA to focus on DreamBooth  using a much bigger vision model called SDXL . We follow a similar setup as adopted by . Specifically, one content (vase) and two style (wooden sculpture and canvas) datasets with five images each were collected from the DreamBooth dataset  and public domains, respectively. These datasets were used to train various content and style adapters. For our experiments, we use SDXL  as our base model and train both LoRA and SHiRA adapters with comparable trainable parameters on individual single-concept datasets. During training, prompts containing special identifier tokens like "<CONTENT>" or "<STYLE>" (e.g., <SBU> as content token for vase and <SZN> as style token for wooden sculpture and canvas) are used

  
**Model** & **\%Params** & **\%C** & **BoolQ(\(\))** & **PIQA(\(\))** & **Arc-e(\(\))** & **Arc-e(\(\))** & **WG(\(\))** & **OBQA(\(\))** & **HS(\(\))** & **SIQA(\(\))** & **Avg.(\(\))** \\  LoRA & **0.83** & 66.72 & 68.9 & 80.7 & 77.8 & 61.3 & 78.8 & 74.8 & 78.1 & 77.4 & 74.7 (+0.09) \\  SHiRA-Grad & 1.0 & **1.0** & 68.4 & 80.9 & 80.2 & 64.7 & **80.4** & 78.2 & 80.3 & **79.4** & 76.6 (+1.95) \\  SHiRA-WM & 1.0 & **1.0** & **69.6** & **81.6** & **81.5** & 66.5 & 79.8 & 79.4 & 79.6 & 77.8 & 77.0 (+2.36) \\ 
**SHiRA-SSIP** & 1.0 & 1.0 & 68.3 & 80.6 & **81.5** & **67.9** & 80.0 & **79.6** & **82.1** & 79.1 & **77.4 (+2.75)** \\  DoRA & 0.84 & 66.72 & 68.5 & **82.9** & 81.4 & **65.8** & **80.8** & **81.0** & **54.8** & **79.6** & **78.1 (+0.05)** \\  SHiRA-WM-DoRA & 6.25\({}^{*}\) & **1.0** & **79.9** & 81.9 & **81.7** & 64.9 & **80.8** & 79.2 & 84.5 & 78.6 & **77.8 (+0.34)** \\   

Table 2: Evaluation of LLaMA-7B on Commonsense Reasoning. WG and HS denote WinoGrande and HellaSwag, respectively. %C represents parameters changed in the fused mode. (\(\)): the higher the better. Green denotes improvement. \({}^{*}\)Trained by masking a high-rank DoRA with a WM mask of top 1% weights, thus changing only 1% of the model during both training and inference.

  
**Model** & **\%Params** & **\%C** & **BoolQ(\(\))** & **PIQA(\(\))** & **Arc-e(\(\))** & **Arc-e(\(\))** & **WG(\(\))** & **OBQA(\(\))** & **HS(\(\))** & **SIQA(\(\))** & **Avg.(\(\))** \\  LoRA & **0.83** & 66.72 & 69.90 & 79.9 & 79.8 & 64.7 & 82.6 & 81.0 & 83.6 & **79.5** & 77.61 (+0.08) \\  DoRA & 0.84 & 66.72 & **71.8** & **83.7** & **83.7** & 68.2 & **82.6** & **82.4** & 89.1 & 76.0 & **79.68 (+2.07)** \\ 
**SHiRA-SNIP** & 1.0 & **1.0** & 70.42 & 81.71 & 83.25 & **68.6** & 80.51 & 81.0 & **89.78** & 79.01 & **79.28 (+1.67)** \\   

Table 3: Results for LLaMA2-7B on Commonsense Reasoning.

to finetune the SDXL network for content or style personalization, respectively. During inference, similar prompts are used to generate images from LoRA- or SHiRA-based DreamBooth.

Fig 6 shows DreamBooth generated images for LoRA and SHiRA. Clearly, our proposed adapter produces high quality personalized images of target concept in different scenarios. This highlights the broad applicability of our adapter while still preserving the benefits of rapid adapter switching.

## 6 Discussion

To summarize our main contributions, we highlight that SHiRA - when used with even the most basic pruning metrics (such as weight- or gradient-magnitude, SNIP, structured masks, etc.) - significantly outperforms LoRA on a variety of large-scale tasks in both large vision and large language domains. For LVM style transfer applications, we found that SHiRA-Struct is the most effective masking technique due to its special orthogonality properties that aid multi-adapter fusion. However, SHiRA-SNIP and SHiRA-Grad are not too far behind and achieve competitive performance as SHiRA-Struct. On the LLM commonsense reasoning side, SHiRA-SNIP is the best strategy out of the masking techniques we have considered in this work. Specifically, SHiRA-Struct did not achieve good results on the more complex commonsense reasoning tasks since it is a combination of a rank-1 + a highly sparse diagonal adapter. SHiRA-Grad on LLMs is about 0.8% worse accuracy than SHiRA-SNIP (76.6% vs. 77.4% average accuracy on commonsense reasoning for LLaMA-1). Therefore, in conclusion, for the applications/fields and the masking techniques considered in this paper, SHiRA-SNIP works well across both language and vision domains. Hence, we recommend that SHiRA-SNIP is one of the strongest candidates that we have considered for sparse finetuning.

## 7 Conclusion

In this paper, we have proposed SHiRA, a new high rank adapter paradigm to demonstrate that even finetuning merely 1-2% parameters of the pretrained generative models is sufficient to achieve high performance on many adapter tasks. We have demonstrated SHiRA's ability to rapidly switch adapters and to avoid concept loss with support from both theory and experiments. Furthermore, we have shown how specially designed sparse masks can lead to near-orthogonal adapter weights which allows for natural multi-adapter fusion. We have conducted extensive single- and multi-adapter experiments on several vision and language tasks to demonstrate the superiority of SHiRA over LoRA. Our latency- and memory-efficient PEFT-based implementation for training SHiRA runs at nearly the same speed as LoRA while consuming about 16% lower peak GPU memory. Finally, for inference, we have provided a scatter_op based method that can load our SHiRA \(5\)-\(16\) faster than equivalent LoRA fusion on a CPU, thus demonstrating our rapid switching benefits.

    &  &  \\ 
**Model** & **BoolQ(\(\))** & **PIQA(\(\))** & **Arc.e(\(\))** & **Avg(\(\))** & **BoolQ(\(\))** & **PIQA(\(\))** & **Arc.e(\(\))** & **Avg(\(\))** & **v2Drop (\(\))** \\   LoRA & **80.52** & 79.05 & 75.67 & 78.41 & 77.22 & 71.27 & 57.45 & 67.33 (+05) & 11.08 \\  SHiRA-WM-Overlap & 78.07 & **79.71** & **77.57** & **78.45** & **77.43** & 76.88 & 67.76 & **74.02** (+6.69\%) & 4.43 \\  SHiRA-WM-Non-Overlap & 76.94 & **79.71** & 75.97 & 77.54 & 74.22 & **78.4** & **60.15** & 73.92 (+6.59\%) & **3.62** \\   

Table 4: Multi-adapter fusion evaluation of independently trained SHiRA and LoRA adapters on BoolQ, PIQA, and Arc-Easy. %Drop is calculated as drop in average accuracy for multi-adapter fusion compared to the single adapter average accuracy for each adapter.

Figure 6: LoRA- vs. SHiRA-based DreamBooth on SDXL. Prompts for content/style personalization - _left pair_: “A picture of a dog in <STYLE:WOODEN-SCULTPURE> style in a bucket”, _center pair_: “A picture of a <CONTENT:VASE> with flowers”, and _right pair_: “A picture of a sunset in <STYLE:CANVAS> style”. Here, “<CONTENT>” and “<STYLE>” are special identifier tokens for content/style.