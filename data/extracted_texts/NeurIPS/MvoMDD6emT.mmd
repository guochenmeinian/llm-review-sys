# State Sequences Prediction via Fourier Transform for Representation Learning

Mingxuan Ye\({}^{1}\)   Yufei Kuang\({}^{1}\)   Jie Wang\({}^{1,2}\)  Rui Yang\({}^{1}\)

&Wengang Zhou\({}^{1,2}\)   Houqiang Li\({}^{1,2}\)   Feng Wu\({}^{1}\)

\({}^{1}\)CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China

\({}^{2}\)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center

{mingxuanye, yfkuang, yr0013}@mail.ustc.edu.cn

{jiewangx, zhwg, lihq, fengwu}@ustc.edu.cn

Corresponding Author

###### Abstract

While deep reinforcement learning (RL) has been demonstrated effective in solving complex control tasks, sample efficiency remains a key challenge due to the large amounts of data required for remarkable performance. Existing research explores the application of representation learning for data-efficient RL, e.g., learning predictive representations by predicting long-term future states. However, many existing methods do not fully exploit the structural information inherent in sequential state signals, which can potentially improve the quality of long-term decision-making but is difficult to discern in the time domain. To tackle this problem, we propose **S**tate **S**equences **P**rediction via **F**ourier Transform (SPF), a novel method that exploits the frequency domain of state sequences to extract the underlying patterns in time series data for learning expressive representations efficiently. Specifically, we theoretically analyze the existence of structural information in state sequences, which is closely related to policy performance and signal regularity, and then propose to predict the Fourier transform of infinite-step future state sequences to extract such information. One of the appealing features of SPF is that it is simple to implement while not requiring storage of infinite-step future states as prediction targets. Experiments demonstrate that the proposed method outperforms several state-of-the-art algorithms in terms of both sample efficiency and performance.2

## 1 Introduction

Deep reinforcement learning (RL) has achieved remarkable success in complex sequential decision-making tasks, such as computer games , robotic control , and combinatorial optimization . However, these methods typically require large amounts of training data to learn good control, which limits the applicability of RL algorithms to real-world problems. The crucial challenge is to improve the sample efficiency of RL methods. To address this challenge, previous research has focused on representation learning to extract adequate and valuable information from raw sensory data and train RL agents in the learned representation space, which has been shown to be significantly more data-efficient . Many of these algorithms rely on auxiliary self-supervision tasks, such as predicting future reward signals  and reconstructing future observations , to incorporate prior knowledge about the environment into the representations.

Due to the sequential nature of RL tasks, multi-step future signals inherently contain more features that are valuable for long-term decision-making than immediate future signals. Recent work has demonstrated that leveraging future reward sequences as supervisory signals is effective in improving the generalization performance of visual RL algorithms [9; 10]. However, we argue that the state sequence provides a more informative supervisory signal compared to the sparse reward signal. As shown in the top portion of Figure 1, the sequence of future states essentially determines future actions and further influences the sequence of future rewards. Therefore, state sequences maximally preserve the influence of the transition intrinsic to the environment and the effect of actions generated from the current policy.

Despite these benefits of state sequences, it is challenging to extract features from these sequential data through long-term prediction tasks. A substantial obstacle is the difficulty of learning accurate long-term prediction models for feature extraction. Previous methods propose making multi-step predictions using a one-step dynamic model by repeatedly feeding the prediction back into the learned model [11; 12; 13]. However, these approaches require a high degree of accuracy in the one-step model to avoid accumulating errors in multi-step predictions . Another obstacle is the storage of multi-step prediction targets. For instance, some method learns a specific dynamics model to directly predict multi-step future states , which requires significant additional memory to store multi-step future states as prediction targets.

To tackle these problems, we propose utilizing the structural information inherent in the sequential state signals to extract useful features, thus circumventing the difficulty of learning an accurate prediction model. In Section 4, we theoretically demonstrate two types of sequential dependency structures present in state sequences. The first structure involves the dependency between reward sequences and state sequences, where the state sequences implicitly reflect the performance of the current policy and exhibit significant differences under good and bad policies. The second structure pertains to the temporal dependencies among the state signals, namely the regularity patterns exhibited by the state sequences. By exploiting the structural information, representations can focus on the underlying critical features of long-term signals, thereby reducing the need for high prediction accuracy and improving training stability .

Building upon our theoretical analyses, we propose **S**tate **S**equences **P**rediction via **F**ontier **T**ransform (SPF), a novel method that exploits the frequency domain of state sequences to efficiently extract the underlying structural information of long-term signals. Utilizing the frequency domain offers several advantages. Firstly, it is widely accepted that the frequency domain shows the regularity properties of the time-series data [15; 16; 17]. Secondly, we demonstrate in Section 5.1 that the Fourier transform of state sequences retains the ability to indicate policy performance under certain assumptions. Moreover, Figure 1 provides an intuitive understanding that the frequency domain enables more effective discrimination of two similar temporal signals that are difficult to differentiate in the time domain, thereby improving the efficiency of policy performance distinction and policy learning.

Specifically, our method performs an auxiliary self-supervision task that predicts the Fourier transform (FT) of infinite-step state sequences to improve the efficiency of representation learning. To facilitate the practical implementation of our method, we reformulate the Fourier transform of state sequences as a recursive form, allowing the auxiliary loss to take the form of a TD error , which depends only on the single-step future state. Therefore, SPF is simple to implement and eliminates the requirements for storing infinite-step state sequences when computing the labels of FT. Experiments demonstrate that our method outperforms several state-of-the-art algorithms in terms of both sample efficiency and performance on six MuJoCo tasks. Additionally, we visualize the fine distinctions between the multi-step future states recovered from our predicted FT and the true states, which indicates that our representation effectively captures the inherent structures of future state sequences.

Figure 1: **State Sequence Generation Process. The top row displays a sample of state sequences generated from MDP \(\). The bottom two rows visualize two state sequences in the time and frequency domains respectively. Each column corresponds to a state sequence generated from a different policy.**

Related Work

**Learning Predictive Representations in RL.** Many existing methods leverage auxiliary tasks that predict single-step future reward or state signals to improve the efficiency of representation learning [2; 5; 19]. However, multi-step future signals inherently contain more valuable features for long-term decision-making than single-step signals. Recent work has demonstrated the effectiveness of using future reward sequences as supervisory signals to improve the generalization performance of visual RL algorithms [9; 10]. Several studies propose making multi-step predictions of state sequences using a one-step dynamic model by repeatedly feeding the prediction back into the learned model, which is applicable to both model-free  and model-based  RL. However, these approaches require a high degree of accuracy in the one-step model to prevent accumulating errors . To tackle this problem, the existing method learned a prediction model to directly predict multi-step future states , which results in significant additional storage for the prediction labels. In our work, we propose to predict the FT of state sequences, which reduces the demand for high prediction accuracy and eliminates the need to store multi-step future states as prediction targets.

**Incorporating the Fourier Features.** There existing many traditional RL methods to express the Fourier features. Early works have investigated representations based on a fixed basis such as Fourier basis to decompose the function into a sum of simpler periodic functions [20; 18]. Another research explored enriching the representational capacity using random Fourier features of the observations. Moreover, in the field of self-supervised pre-training, neuro2vec  conducts representation learning by predicting the Fourier transform of the masked part of the input signal, which is similar to our work but needs to store the entire signal as the label.

## 3 Preliminaries

### MDP Notation

In RL tasks, the interaction between the environment and the agent is modeled as a Markov decision process (MDP). We consider the standard MDP framework , in which the environment is given by the tuple \(:=,,R,P,,\), where \(\) is the set of states, \(\) is the set of actions, \(R:[-R_{},R _{}]\) is the reward function, \(P:\) is the transition probability function, \(:\) is the initial state distribution, and \([0,1)\) is the discount factor. A policy \(\) defines a probability distribution over actions conditioned on the state, i.e. \((a|s)\). The environment starts at an initial state \(s_{0}\). At time \(t 0\), the agent follows a policy \(\) and selects an action \(a_{t}(|s_{t})\). The environment then stochastically transitions to a state \(s_{t+1} P(|s_{t},a_{t})\) and produces a reward \(r_{t}=R(s_{t},a_{t},s_{t+1})\). The goal of RL is to select an optimal policy \(^{*}\) that maximizes the cumulative sum of future rewards. Following previous work [22; 23], we define the _performance_ of a policy \(\) as its expected sum of future discounted rewards:

\[J(,):=E_{(,)}[_{t=0}^{} ^{t}R(s_{t},a_{t},s_{t+1})],\] (1)

where \(:=(s_{0},a_{0},s_{1},a_{1},)\) denotes a trajectory generated from the interaction process and \((,)\) indicates that the distribution of \(\) depends on \(\) and the environment model \(\). For simplicity, we write \(J()\) and \(\) as shorthand since our environment is stationary. We also interest about the discounted future state distribution \(d^{}\), which is defined by \(d^{}(s)=(1-)_{t=0}^{}^{t}P(s_{t}=s|,)\). It allows us to express the expected discounted total reward compactly as

\[J()=E_{s d^{}\\ s P}[R(s,a,s^{})].\] (2)

The proof of (2) can be found in  or Section A.1 in the appendix.

Given that we aim to train an encoder that effectively captures the useful aspects of the environment, we also consider a latent MDP \(}=},,,,,,\), where \(}^{D}\) for finite \(D\) and the action space \(\) is shared by \(\) and \(}\). We aim to learn an embedding function \(:}\), which connects the state spaces of these two MDPs. We similarly denote \(^{*}\) as the optimal policy in \(}\). For ease of notation, we use \((|s):=(|(s))\) to represent first using \(\) to map \(s\) to the latent state space \(}\) and subsequently using \(\) to generate the probability distribution over actions.

### Discrete-Time Fourier Transform

The discrete-time Fourier transform (DTFT) is a powerful way to decompose a time-domain signal into different frequency components. It converts a real or complex sequence \(\{x_{n}\}_{n=-}^{+}\) into a complex-valued function \(F()=_{n=-}^{}x_{n}e^{-j n}\), where \(\) is a frequency variable. Due to the discrete-time nature of the original signal, the DTFT is \(2\)-periodic with respect to its frequency variable, i.e., \(F(+2)=F()\). Therefore, all of our interest lies in the range \([0,2]\) that contains all the necessary information of the infinite-horizon time series.

It is important to note that the signals considered in this paper, namely state sequences, are real-valued, which ensures the conjugate symmetry property of DTFT, i.e., \(F(2-)=F^{*}()\). Therefore, in practice, it suffices to predict the DTFT only on the range of \([0,]\), which can reduce the number of parameters and further save storage space.

## 4 Structural Information in State Sequences

In this section, we theoretically demonstrate the existence of the structural information inherently in the state sequences. We argue that there are two types of sequential dependency structures present in state sequences, which are useful for indicating policy performance and capturing regularity features of the states, respectively.

### Policy Performance Distinction via State Sequences

In the RL setting, it is widely recognized that pursuing the highest reward at each time step in a greedy manner does not guarantee the maximum long-term benefit. For that reason, RL algorithms optimize the objective of cumulative reward over an episode, rather than the immediate reward, to encourage the agent to make farsighted decisions. This is why previous work has leveraged information about future reward sequences to capture long-term features for stronger representation learning .

Compared to the sparse reward signals, we claim that sequential state signals contain richer information. In MDP, the stochasticity of a trajectory derives from random actions selected by the agent and the environment's subsequent transitions to the next state and reward. These two sources of stochasticity are modeled as the policy \((a|s)\) and the transition \(p(s^{},r|s,a)\), respectively. Both of them are conditioned on the current state. Over long interaction periods, the dependencies of action and reward sequences on state sequences become more evident. That is, the sequence of future states largely determines the sequence of actions that the agent selects and further determines the corresponding sequence of rewards, which implies the trend and performance of the current policy, respectively. Thus, state sequences not only explicitly contain information about the environment's dynamics model, but also implicitly reveal information about policy performance.

We provide further theoretical justification for the above statement, which shows that the distribution distance between two state sequences obtained from different policies provides an upper bound on the performance difference between those policies, under certain assumptions about the reward function.

**Theorem 1**.: _Suppose that the reward function \(R(s,a,s^{})=R(s)\) is related to the state \(s\), then the performance difference between two arbitrary policies \(_{1}\) and \(_{2}\) is bounded by the LI norm of the difference between their state sequence distributions:_

\[|J(_{1})-J(_{2})|}{1-}\|P( s_{0},s_{1},s_{2},|_{1},)-P(s_{0},s_{1},s_{2},|_{2}, )\|_{1},\] (3)

_where \(P(s_{0},s_{1},s_{2},|,)\) means the joint distribution of the infinite-horizon state sequence \(=\{},},},\}\) conditioned on the policy \(\) and the environment model \(\)._

The proof of this theorem is provided in Appendix A.1. The theorem demonstrates that the greater the difference in policy performance, the greater the difference in their corresponding state sequence distributions. When we adjust the ratio \(}{1-}\) to take a relatively small value by scaling the reward, the theorem indicates that good and bad policies generate significantly different state sequence distributions. Furthermore, it confirms that learning via state sequences can significantly influence the search for policies with good performance.

### Asymptotic Periodicity of States in MDP

Many tasks in real scenarios exhibit periodic behavior as the underlying dynamics of the environment are inherently periodic, such as industrial robots, car driving in specific scenarios, and area sweeping tasks. Take the assembly robot as an example, the robot is trained to assemble parts together to create a final product. When the robot reaches a stable policy, it executes a periodic sequence of movements that allow it to efficiently assemble the parts together. In the case of MuJoCo tasks, the agent also exhibits periodic locomotion when reaching a stable policy. We provide a corresponding video in the supplementary material to show the periodic locomotion of several MuJoCo tasks.

Inspired by these cases, we provide some theoretical analysis to demonstrate that, under some assumptions about the transition probability matrices, the state sequences in finite state space may exhibit asymptotically periodic behaviors when the agent reaches a stable policy.

**Theorem 2**.: _Suppose that the state space \(\) is finite with a transition probability matrix \(P^{||||}\) and \(\) has \(\) recurrent classes. Let \(R_{1},R_{2},,R_{}\) be the probability submatrices corresponding to the recurrent classes and let \(d_{1},d_{2},,d_{}\) be the number of the eigenvalues of modulus \(1\) that the submatrices \(R_{1},R_{2},,R_{}\) has. Then for any initial distribution \(_{0}\), \(P^{n}_{0}\) is asymptotically periodic with period \(d=(d_{1},d_{2},,d_{})\)._

The proof of the theorem is provided in Appendix A.2. The above theorem demonstrates that there exist regular and highly-structured features in the state sequences, which can be used to learn an expressive representation. Note that in an infinite state space, if the Markov chain contains a recurrent class, then after a sufficient number of steps, the state will inevitably enter one of the recurrent classes. In this scenario, the asymptotic periodicity of the state sequences can be also analyzed using the aforementioned theorem. Furthermore, even if the state sequences do not exhibit a strictly periodic pattern, regularities still exist within the sequential data that can be extracted as representations to facilitate policy learning.

## 5 Method

In the previous section, we demonstrate that state sequences contain rich structural information which implicitly indicates the policy performance and regular behavior of states. However, such information is not explicitly shown in state sequences in the time domain. In this section, we describe how to effectively leverage the inherent structural information in time-series data.

### Learning via Frequency Domain of State Sequences

In this part, we will discuss the advantages of leveraging the frequency pattern of state sequences for capturing the inherent structural information above explicitly and efficiently.

Based on the following theorem, we find that the FT of the state sequences preserves the property in the time domain that the distribution difference between state sequences controls the performance difference between the corresponding two policies, but is subject to some stronger assumptions.

**Theorem 3**.: _Suppose that \(^{D}\) the reward function \(R(s,a,s^{})=R(s)\) is an nth-degree polynomial function with respect to \(s\), then for any two policies \(_{1}\) and \(_{2}\), their performance difference can be bounded as follows:_

\[|J(_{1})-J(_{2})|}{1-}_{k=1}^{n}(0)\|_{D}}{k!}_{1 i D}_{_{i} [0,2]}|F^{(k)}_{_{1}}(_{i})-F^{(k)}_{_{2}}(_{i}) |,\] (4)

_where \(F^{(k)}_{}()\) denotes the DTFT of the time series \(^{(k)}=\{}^{k},}^{k},}^{k}, \}\) for any integer \(k[1,n]\) and \(^{(k)}\) means the kth power of the state sequence produced by the policy \(\). The dimensionality of \(\) is the same as \(s\)._

We provide the proof in Appendix A.1. Similar to the analysis of Theorem 1, the above theorem shows that state sequences in the frequency domain can indicate the policy performance and can be leveraged to enhance the search for optimal policies. Furthermore, the Fourier transform can decompose the state sequence signal into multiple physically meaningful components. This operator enables the analysis of time-domain signals in a higher dimensional space, making it easier todistinguish between two segments of signals that appear similar in the time domain. In addition, periodic signals have distinctive characteristics in their Fourier transforms due to their discrete spectra. We provide a visualization of the DTFT of the state sequences in Appendix E, which reveals that the DTFT of the periodic state sequence is approximately discrete. This observation suggests that the periodic information of the signal can be explicitly extracted in the frequency domain, particularly for the periodic cases provided by Theorem 2. For non-periodic sequences, some regularity information can still be obtained by the frequency range that the signals carry.

In addition to those advantages, the operation of the Fourier transforms also yields a concise auxiliary objective similar to the TD-error loss , which we will discuss in detail in the following section.

### Learning Objective of SPF

In this part, we propose our method, State Sequences Prediction via Fourier Transform (SPF), and describe how to utilize the frequency pattern of state sequences to learn an expressive representation. Specifically, our method performs an auxiliary self-supervision task by predicting the discrete-time Fourier transform (DTFT) of infinite-step state sequences to capture the structural information in the state sequences for representation learning, hence improving upon the sample efficiency of learning.

Now we model the auxiliary self-supervision task. Given the current observation \(s_{t}\) and the current action \(a_{t}\), we define the expectation of future state sequence \(_{t}\) over infinite horizon as

\[[_{t}]_{n}=[(s_{t},a_{t})]_{n}= ^{n}E_{,p}[s_{t+n+1}s_{t},a_{t}]&n=0,1,2,\\ 0&n=-1,-2,-3.\] (5)

Then the discrete-time Fourier transform of \(_{t}\) is \(_{t}()=_{n=0}^{+}[_{t}]_{n}\,e^{- j n}\), where \(\) represents the frequency variable. The discount factor \(\) in (5) is used to ensure the convergence of the Fourier transform and also serves as the contraction factor in the following Theorem 4. Since the state sequences are discrete-time signals, the corresponding DTFT is \(2\)-periodic with respect to \(\). Based on this property, a common practice for operational feasibility is to compute a discrete approximation of the DTFT over one period, by sampling the DTFT at discrete points over \([0,2]\). In practice, we take \(L\) equally-spaced samples of the DTFT. Then the prediction target is a matrix with size \(L*D\), where \(D\) is the dimension of the state space. We can derive that the DTFT functions at successive time steps are related to each other in a recursive form:

\[F_{,p}(s_{t},a_{t})=}_{t}+\,E_{,p} [F(s_{t+1},a_{t+1})].\] (6)

The detailed derivation and the specific form of \(}_{t}\) and \(\) is provided in Appendix A.3.

Based on the recursive formula (6), we can obtain the prediction loss by computing the difference between the estimated Fourier value \(F_{,p}(s_{t},a_{t})\) and the better estimate \(}_{t}+\,E_{,p}[F(s_{t+1},a_{t+1})]\), just like the TD error. Similar to the TD-learning of value functions, the recursive relationship can be reformulated as contraction mapping \(\), as shown in the following theorem (see proof in Appendix A.3). Due to the properties of contraction mappings, we can iteratively apply the operator \(\) to compute the target DTFT function of long-term state sequences until convergence in tabular settings. When calculating the prediction loss, we only need to utilize the current state \(s_{t}\), the current action \(a_{t}\), and the next state \(s_{t+1}\). Therefore, one notable advantage of SPF is that there is no need to store multi-step future states as labels for predicting future state sequences.

**Theorem 4**.: _Let \(\) denote the set of all functions \(F:^{L*D}\) and define the norm on \(\) as_

\[\|F\|_{}:=_{s\\ a}_{0 k<L}\|F(s,a)_{k} \|_{D},\]

_where \(F(s,a)_{k}\) represents the kth row vector of \(F(s,a)\). We show that the mapping \(:\) defined as_

\[F(s_{t},a_{t})=}_{t}+\,E_{,p }[F(s_{t+1},a_{t+1})]\] (7)

_is a contraction mapping, where \(}_{t}\) and \(\) are defined in Appendix A.3._

As the Fourier transform of the real state signals has the property of conjugate symmetry, we only need to predict the DTFT on a half-period interval \([0,]\). Therefore, we reduce the row size of the prediction target to half for reducing redundant information and saving storage space. In practice, we train a parameterized prediction model \(\) to predict the DTFT of state sequences. Note that the value of the prediction target is on the complex plane, so the prediction network employs two separate output modules \(_{}\) and \(_{}\) as real and imaginary parts respectively. Then we define the auxiliary prediction loss function as:

\[L_{}(,)=d(}_{t} +_{}_{}(},( }))-_{}_{}(},(})),\ _{}(},a_{t}))\] \[+d_{}_{}( },(}))+_{}_{ }(},(})),\ _{}(},a_{t}),\]

where \(}=(s_{t})\) means the representation of the state, \(_{}\) and \(_{}\) denote the real and imaginary parts of the constant \(\), and \(d\) denotes an arbitrary similarity measure. We choose \(d\) as cosine similarity in practice. The algorithm pseudo-code is shown in Appendix B.

### Network Architecture of SPF

Here we provide more details about the network architecture of our method. In addition to the encoder and the predictor, we use projection heads \(\) that project the predicted values onto a low-dimensional space to prevent overfitting when computing the prediction loss directly from the high-dimensional predicted values. In practice, we use a loss function called _freqloss_, which preserves the low and high-frequency components of the predicted DTFT without the dimensionality reduction process (See Appendix C for more details). Furthermore, when computing the target predicted value, we follow prior work [26; 11] to use the target encoder, predictor, and projection for more stable performance. We periodically overwrite the target network parameters with an exponential moving average of the online network parameters.

In the training process, we train the encoder \(\), the predictor \(\), and the projection \(\) to minimize the auxiliary prediction loss \(_{}(,,)\), and alternately update the actor-critic models of RL tasks using the trained encoder \(\). We illustrate the overall architecture of SPF and the gradient flows during training in Figure 2.

## 6 Experiments

We quantitatively evaluate our method on a standard continuous control benchmark--the set of MuJoCo  environments implemented in OpenAI Gym.

### Comparative Evaluation on MuJoCo

To evaluate the effect of learned representations, we measure the performance of two traditional RL methods SAC  and PPO  with raw states, OFENet representations , and SPF represen

Figure 2: **The network architecture of SPF**. The online encoder \(\) outputs the representations used in the RL task and the predictor \(\) predicts complex-valued Fourier transform of the state sequences starting from the state-action pair \((s_{t},a_{t})\). During training, \((s_{t},a_{t},s_{t+1})\) are previously experienced states and actions sampled from a replay buffer. The dashed line show how gradients flow back to model weights. We prevent the gradient of RL losses from updating the online encoder and prevent the gradient of prediction loss from updating the target encoder.

[MISSING_PAGE_FAIL:8]

task is that its state contains the external forces to bodies, which show limited regularities due to their discontinuous and sparse.

### Ablation Study

In this part, we will verify that just predicting the FT of state sequences may fall short of the expected performance and that using SPF is necessary to get better performance. To this end, we conducted an ablation study to identify the specific components that contribute to the performance improvements achieved by SPF. Figure 4(a) shows the ablation study over SAC with HalfCheetah-v2 environment.

_notarg_ removes all target networks of the encoder, predictor, and projection layer from SPF. Based on the results, the variant of SPF exhibits significantly reduced performance when target estimations in the auxiliary loss are generated by the online encoder without a stopgradient. Therefore, using a separate target encoder is vital, which can significantly improve the stability and convergence properties of our algorithm.

_nofreqloss_ computes the cosine similarity of the projection layer's outputs directly without any special treatment to the low-frequency and high-frequency components of our predicted DTFT. The reduced convergence rate of _nofreqloss_ suggests that preserving the complete information of low and high-frequency components can encourage the representations to capture more structural information in the frequency domain.

_noproj_ removes the projection layer from SPF and computes the cosine similarity of the predicted values as the objective. The performance did not significantly deteriorate after removing the projection layer, which indicates that the prediction accuracy of state sequences in the frequency domain may not have a strong impact on the quality of representation learning. Therefore, it can be inferred that SPF places a greater emphasis on capturing the underlying structural information of the state sequences, and is capable of reconstructing the state sequences with a low risk of overfitting.

_mlp_ changes the lay block of the encoder from MLP-DenseNet to MLP. The much lower scores of _mlp_ indicate that both the raw state and the output of hidden layers contain important information that contributes to the quality of the learned representations. This result underscores the importance of leveraging sufficient information for representation learning.

_mlp-cat_ uses a modified block of MLP as the layer of the encoder, which concat the output of MLP with the raw state. The performance of _mlp-cat_ does increase compared to _mlp_, but is still not as good as SPF in terms of both sample efficiency and performance.

### Comparison of Different Prediction Targets

This section aims to test the effect of our prediction target--infinite-step state sequences in the frequency domain--on the efficiency of representation learning. We test five types of prediction

Figure 4: **Results of additional trials. (a) Ablation study: training curves of five variant methods of SPF on SAC. Each method is evaluated over 5 seeds on HalfCheetah-v2. (b) Comparison of prediction targets: training curves of SAC with different auxiliary prediction tasks. Each method is evaluated over 5 seeds on HalfCheetah-v2. (c) Visualization of recovered states from the predicted DTFT. The blue line represents the true state sequence, while the red line represents the recovered state sequence. The lighter red line corresponds to predictions made by historical states from a more distant time step. The four subfigures represent four dimensions of the state space on Walker2d.**

targets: 1) **Sta1**: single-step future state; 2) **StaN**: N-step state sequences, where we choose \(N=2,3,5\); 3) **SPF**: infinite-step state sequences in frequency domain; 4) **Rew1**: single-step future reward; 5) **RewFT**: infinite-step reward sequences in frequency domain.

As shown in Figure 4(b), SPF outperforms all other competitors in terms of sample efficiency, which indicates that infinite-step state sequences in the frequency domain contain more underlying valuable information that can facilitate efficient representation learning. Since Sta1 and SPF outperform Rew1 and RewFT respectively, it can be referred that learning via states is more effective for representation learning than learning via rewards. Notably, the lower performance of StaN compared to Sta1 could be attributed to the model's tendency to prioritize prediction accuracy over capturing the underlying structured information in the sequential data, which may impede its overall learning efficiency.

### Visualization of Recovered State Sequences

This section aims to demonstrate that the representations learned by SPF effectively capture the structural information contained in infinite-step state sequences. To this end, we compare the true state sequences with the states recovered from the predicted DTFT via the inverse DTFT (See Appendix E for more implementation details). Figure 4(c) shows that the learned representations can recover the true state sequences even using the historical states that are far from the current time step. In Appendix E, we also provide a visualization of the predicted DTFT, which is less accurate than the results in Figure 4(c). Those results highlight the ability of SPF to effectively extract the underlying structural information in infinite-step state sequences without relying on high prediction accuracy.

We further provide a comparison table that measures the distance between the real DTFT and the predicted DTFT using cosine similarity. The results provided in Appendix F indicate that the prediction module \(\) exhibits moderate predictive accuracy in approximating the real Fourier transform, with an average cosine similarity value of \(-0.6\).

## 7 Conclusion

In this paper, we theoretically analyzed the existence of structural information in state sequences, which is closely related to policy performance and signal regularity, and then introduced State Sequences Prediction via Fourier Transform (SPF), a representation learning method that predicts the FT of state sequences to extract the underlying structural information in state sequences for learning expressive representations efficiently. SPF outperforms several state-of-the-art algorithms in terms of both sample efficiency and performance. Our additional experiments and visualization show that SPF encourages representations to place a greater emphasis on capturing the underlying pattern of time-series data, rather than pursuing high accuracy of prediction tasks.

**Limitations** One of the main limitations of our paper is that we have only evaluated our method on tasks where the state sequences exhibit strong asymptotic periodicity. Considering that the Fourier transform converts non-periodic signals into a continuous frequency domain, it is more difficult to extract meaningful frequency features of non-periodic signals compared to periodic signals that have a discrete frequency domain. Moreover, the frequency features extracted by our approach inherently depend on the policy and task, which limits their reusability across multiple tasks. Further research is needed to analyze the applicability of our approach to non-periodic tasks and the potential for generalization across multiple tasks.