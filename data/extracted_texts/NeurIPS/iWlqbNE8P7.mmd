# Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling

Zijie Huang\({}^{1}\)1 Wanjia Zhao\({}^{2}\)2 Jingdong Gao\({}^{1}\) Ziniu Hu\({}^{3}\) Xiao Luo\({}^{1}\)

**Yadi Cao\({}^{1}\) Yuanzhou Chen\({}^{1}\) Yizhou Sun\({}^{1}\) Wei Wang\({}^{1}\)**

\({}^{1}\)University of California Los Angeles, \({}^{2}\)Stanford University

\({}^{3}\)California Institute of Technology

https://treat-ode.github.io/

Equal contribution, Corresponding to Zijie Huang <zijiehuang@cs.ucla.edu>, Wanjia Zhao <wanjia.azh@cs.stanford.edu>Work done as a visiting student at UCLA

###### Abstract

Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing _Time-Reversal Symmetry_ (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the _first_ theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT's broad applicability and effectiveness. Code and further details are available at here.

## 1 Introduction

Dynamical systems, spanning applications from physical simulations (Kipf et al., 2018; Wang et al., 2020; Lu et al., 2022; Huang et al., 2023; Luo et al., 2023; Xu et al., 2024; Luo et al., 2024) to robotic control (Li et al., 2022; Ni and Qureshi, 2022), are challenging to model due to intricate dynamic patterns and potential interactions under multi-agent settings. Traditional numerical simulators require extensive domain knowledge for design, which is sometimes unknown (Sanchez-Gonzalez et al., 2020), and can consume significant computational resources (Wang et al., 2024). Therefore, directly learning dynamics from the observational data becomes an attractive alternative.

Existing deep learning approaches (Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021; Han et al., 2022) usually learn a fixed-step transition function to predict system dynamics from timestamp \(t\) to timestamp \(t+1\) and rollout trajectories recursively. The transition function can have different inductive biases, such as Graph Neural Networks (GNNs) (Pfaff et al., 2020; Martinkus et al., 2021; Lam et al., 2023; Cao et al., 2023) for capturing pair-wise interactions among agents through message passing. Most recently, neural ordinary differential equations (Neural ODEs) (Chen et al.,2018; Rubanova et al., 2019) have emerged as a potent solution for modeling system dynamics in a continuous manner, which offer superior prediction accuracy over discrete models in the long-range, and can handle systems with partial observations. In particular, GraphODEs (Huang et al., 2020; Luo et al., 2023b; Zang and Wang, 2020; Jiang et al., 2023; Luo et al., 2023c) extend NeuralODEs to model interacting (multi-agent) dynamical systems, where agents co-evolve and form trajectories jointly.

However, the complexity of dynamical systems necessitates large amounts of data. Models trained on limited data risk violating fundamental physical principles such as energy conservation. A promising strategy to improve modeling accuracy involves incorporating physical inductive biases (Raissi et al., 2019; Cranmer et al., 2020). Existing models like Hamiltonian Neural Networks (HNNs) (Greydanus et al., 2019; Sanchez-Gonzalez et al., 2019) strictly enforce energy conservation, yielding more accurate predictions for energy-conservative systems. However, not all real-world systems strictly adhere to energy conservation, and they may adhere to various physical priors. Other methods that model both energy-conserving and dissipative systems, as well as reversible systems, offer more flexibility (Zhong et al., 2020; Gruber et al., 2024). Nevertheless, they often rely on prior knowledge of the system and are also limited to systems with corresponding physical priors. Such system diversity largely limits the usage of existing models which are designed for specific physical prior.

To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. Specifically, TRS posits that a system's dynamics should remain invariant when time is reversed (Lamb and Roberts, 1998). To incorporate TRS, we propose a simple-yet-effective self-supervised regularization term that acts as a soft constraint. This term aligns _forward and backward trajectories_ predicted by a neural network and we use GraphODE as the backbone. We theoretically prove that the TRS loss effectively minimizes higher-order Taylor expansion terms during ODE integration, offering a general numerical advantage for improving modeling accuracy across a wide array of systems, regardless of their physical properties. It forces the model to capture fine-grained physical properties such as jerk (the derivatives of accelerations) and provides more regularization for long-term prediction. We also justify our TRS design choice, showing case its superior performance both analytically and empirically. We name the model as TREAT (Time-Reversal Symmetry ODE).

Note that TRS itself is a physical prior, that is broader than energy conservation as depicted in Figure 1(b.1). It covers classical energy-conservative systems such as Newtonian mechanics, and also non-conservative, reversible systems like Stokes flow (Pozrikidis, 2001), commonly encountered in microfluidics (Kim and Karrila, 2013; Cao and Li, 2018; Cao et al., 2019). Therefore, TRS loss achieves high-precision modeling from both the physical aspect, and the numerical aspect as shown in Figure 1(a), making it domain-agnostic and widely applicable to various dynamical systems. We systematically conduct experiments across 9 diverse datasets spanning across 1.) single-agent, multi-agent systems; 2.) simulated and real-world systems; and 3.) systems with different physical

Figure 1: (a) High-precision modeling for dynamical systems; (b.1) Classification of classical mechanical systems based on (Tolman, 1938; Lamb and Roberts, 1998);(b.2) Tim-Reversal Symmetry illustration;(b.3) Error accumulation in numerical solvers.

priors. TREAT consistently outperforms state-of-the-art baselines, affirming its effectiveness and versatility across various dynamic scenarios.

Our primary contributions can be summarized as follows:

* We introduce TREAT, a powerful framework that achieves high-precision modeling for a wide range of systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a regularization term.
* We establish the _first_ theoretical proof that the time-reversal symmetry loss could in general help learn more fine-grained and long-context system dynamics from the numerical aspect, regardless of systems' physical properties (even irreversible systems). This bridges the specific physical implication and the general numerical benefits of the physical prior -TRS.
* We present empirical evidence of TREAT's state-of-the-art performance in a variety of systems over 9 datasets, including real-world & simulated systems, etc. It yields a significant MSE improvement of 11.5% on the challenging chaotic triple-pendulum system.

## 2 Preliminaries and Related Work

We represent a dynamical system as a graph \(=(,)\), where \(\) denotes the node set of \(N\) agents3 and \(\) denotes the set of edges representing their physical interactions. For simplicity, we assumed \(\) to be static over time. Single-agent dynamical system is a special case where the graph only has one node. In the following, we use the multi-agent setting by default to illustrate our model. We denote \((t)^{N d}\) as the feature matrix at timestamp \(t\) for all agents, with \(d\) as the feature dimension. Model input consists of trajectories of feature matrices over \(M\) historical timestamps \(X(t_{-M:-1})=\{(t_{-M}),,(t_{-1})\}\) and \(\). The timestamps \(t_{-1},,t_{-M}<0\) can have non-uniform intervals and take any continuous values. Our goal is to learn a neural simulator \(f_{}():X(t_{-M:-1}), Y(t_{0:K})\), which predicts node dynamics \((t)\) in the future on timestamps \(0=t_{0}<<t_{K}=T\) sampled within \([0,T]\). We use \(_{i}(t)\) to denote the targeted dynamic vector of agent \(i\) at time \(t\). In some cases when we are only predicting system feature trajectories, \(()()\).

### NeuralODE for Dynamical Systems

NeuralODEs (Chen et al., 2018; Rubanova et al., 2019) are a family of continuous models that define the evolution of dynamical systems by ordinary differential equations (ODEs). The state evolution can be described as: \(}_{i}(t):=(t)}{dt}=g(_{1}(t),_{2}(t) _{N}(t))\), where \(_{i}(t)^{d}\) denotes the latent state variable for agent \(i\) at timestamp \(t\). The ODE function \(g\) is parameterized by a neural network such as Multi-Layer Perception (MLP), which is automatically learned from data. GraphODEs (Poli et al., 2019; Huang et al., 2020; Luo et al., 2023; Wen et al., 2022; Huang et al., 2024) are special cases of NeuralODEs, where \(g\) is a Graph Neural Network (GNN) to capture the continuous interaction among agents.

GraphODEs have been shown to achieve superior performance, especially in long-range predictions and can handle data irregularity issues. They usually follow the encoder-processor-decoder architecture, where an encoder first computes the latent initial states \(_{1}(t_{0}),_{N}(t_{0})\) for all agents simultaneously based on their historical observations as in Eqn 1.

\[_{1}(t_{0}),_{2}(t_{0}),...,_{N}(t_{0})=f_{} X(t_{-M:-1}),\] (1)

Then the GNN-based ODE predicts the latent trajectories starting from the learned initial states. The latent state \(_{i}(t)\) can be computed at any desired time using a numerical solver such as Runge-KuttaSchober et al. (2019) as:

\[_{i}(t)=g,[_{1}(t_{0}),..._{N}(t_{0 })],t=_{i}(t_{0})+_{t_{0}}^{t}g(_{1}(t),_{2} (t)_{N}(t))dt.\] (2)

Finally, a decoder extracts the predicted dynamics \(}_{i}(t)\) based on the latent states \(_{i}(t)\) for any timestamp \(t\):

\[}_{i}(t)=f_{}(_{i}(t)).\] (3)However, vanilla GraphODEs can violate physical properties of a system, resulting in unrealistic predictions. We therefore propose to inject physics-informed regularization term to make more accurate predictions.

### Time-Reversal Symmetry (TRS)

Consider a dynamical system described in the form of \((t)}{dt}=F((t))\), where \((t)\) is the observed states such as positions. The system is said to follow the _Time-Reversal Symmetry_ if there exists a reversing operator \(R:\) such that (Lamb and Roberts, 1998):

\[R(t)}{dt}=-FR(t),\] (4)

where \(\) denote the action of functional \(R\) on the function \(\).

Intuitively, we can assume \((t)\) is the position of a flying ball and the conventional reversing operator is defined as \(R: R,R(t)=(-t)\). This implies when \((t)\) is a forward trajectory position with initial position \((0)\), \((-t)\) is then a position in the time-reversal trajectory, where \((-t)\) is calculated using the same function \(F\), but with the integration time reversed, i.e. \(dt d(-t)\). Eqn 4 shows how to create the reverse trajectory of a flying ball: at each position, the velocity (i.e., the derivative of position with respect to time) should be the opposite. In neural networks, we usually model trajectories in the latent space via \(\)(Sanchez-Gonzalez et al., 2020), which can be decoded back to real observation state i.e. positions. Therefore, we apply the reversal operator for \(\).

Now we introduce a time evolution operator \(_{}\) such that \(_{}(t)=(t+)\) for arbitrary \(t,\). It satisfies \(_{_{1}}_{_{2}}=_{_{1}+_{2}}\), where \(\) denotes composition. The time evolution operator helps us to move forward (when \(>0\)) or backward (when \(<0\)) through time, thus forming a trajectory. Based on (Lamb and Roberts, 1998), in terms of the evolution operator, Eqn 4 implies:

\[R_{t}=_{-t} R=_{t}^{-1} R,\] (5)

which means that moving forward \(t\) steps and then turning to the opposite direction is equivalent to firstly turning to the opposite direction and then moving backwards \(t\) steps4. Eqn 5 has been widely used to describe time-reversal symmetry in existing literature (Huh et al., 2020; Valperga et al., 2022). Nevertheless, we propose the following lemma, which is more intuitive to understand and straightforward to guide the design of our time-reversal regularizer.

**Lemma 2.1**.: _Eqn 5 is equivalent to \(R_{t} R_{t}=I\), where \(I\) denotes identity mapping._

Lemma 2.1 means if we move \(t\) steps forward, then turn to the opposite direction, and then move forward for \(t\) more steps, it shall restore back to the same state. This is illustrated in Figure 2 where the reverse trajectory should be the same as the forward trajectory.5 It can be understood as rewinding a video to the very beginning. The proof of Lemma 2.1 is in Appendix A.2.

## 3 Method: TREAT

We present a novel framework TREAT that achieves high-precision modeling for a wide range of systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a regularization

Figure 2: Illustration of time-reversal symmetry based on Lemma 2.1.The total length of the trajectory is \(t_{K}-t_{0}=T\). \(t^{}_{k}\) is the time index in the reverse trajectory, which points to the same time as \(t_{K-k}\) in the forward trajectory.

term. It improves modeling accuracy regardless of systems' physical properties. We first introduce our architecture design, followed by theoretical analysis to explain its numerical benefits.

TREAT uses GraphODE (Huang et al., 2020) as the backbone and flexibly incorporates TRS as a regularization term based on Lemma 2.1. This term aligns model forward and reverse trajectories. In practice, our model predicts the forward trajectories at a series of timestamps \(\{t_{k}\}_{k=0}^{K}\) as ground truth observations are discrete, where \(0=t_{0}<t_{1}<<t_{K}=T\). The reverse trajectories are also at the same series of \(K\) timestamps so as to be aligned with the forward one, which we denote as \(\{t_{k}^{}\}_{k=0}^{K}\) satisfying \(0=t_{0}^{}<t_{1}^{}<<t_{K}^{}=T\). It's important to note that the values of the time variable \(t_{k}^{}\) in the reverse trajectories do not represent real time, but serve as indexes of reverse trajectories. This leads to the relation \(t_{K-k}^{}=T-t_{k}\), which means the reverse trajectories at timestamp \(t_{K-k}^{}\) correspond to the forward trajectories at time \(t_{k}\). For example, \(t_{0}^{}=T-t_{K}=0\). It indicates \(t_{0}^{}\) and \(t_{K}\) are both pointing to the same real time \(T\), which is the ending point of the forward trajectory as shown in Figure 3. Based on Lemma 2.1, the difference of the two trajectories at any observed time should be small, i.e. \(^{}(t_{k})^{}(t_{K-k}^{})\). This serves as the guideline for our regularizer design. The weight of the regularizer is also adjustable to adapt different systems. The overall framework is depicted in Figure 3.

### Time-Reversal Symmetry Loss and Training

Forward Trajectory Prediction and Reconstruction Loss.For multi-agent systems, we utilize the GNN operator described in (Kipf et al., 2018) as our ODE function \(g()\), which drives the system to move forward and output the forward trajectories for latent states \(_{i}^{}(t)\) at each continuous time \(t[0,T]\) and each agent \(i\).We then employ a Multilayer Perceptron (MLP) as a decoder to predict output trajectories \(}_{i}^{}(t)\) based on the latent states. We summarize the whole procedure as:

\[}_{i}^{}(t)&:= _{i}^{}(t)}{dt}=g(_{1}^{}(t),_{2}^ {}(t),_{N}^{}(t)),\\ _{i}^{}(t_{0})&=f_{}(X(t_ {-M:-1}),),}_{i}^{}(t)=f_{}(_{i}^{}(t)).\] (6)

Figure 3: Overall framework of TREAT. \(O_{1},O_{2},O_{3}\) are connected agents. It follows the encoder-processor-decoder architecture introduced in Sec 2.1. A novel TRS loss is incorporated to improve modeling accuracy across systems from the numerical aspect, regardless of their physical properties.

To train the model, we use the reconstruction loss that minimizes the L2 distance between predicted forward trajectories \(\{}_{i}^{}(t_{k})\}_{k=0}^{K}\) and the ground truth trajectories \(\{_{i}(t_{k})\}_{k=0}^{K}\) as :

\[_{pred}=_{i=1}^{N}_{k=0}^{K}\|_{i}(t_{k})-}_{i}^{}(t_{k})\|_{2}^{2}.\] (7)

Reverse Trajectory Prediction and Regularization Loss.We design a novel time-reversal symmetry loss as a soft constraint to flexibly regulate systems' behavior based on Lemma 2.1. Specifically, we first compute the latent reverse trajectories \(^{}(t)\) by starting from the ending state of the forward one, traversed back over time. We then employ the decoder to output dynamic trajectories \(^{}(t)\).

\[}_{i}^{}(t):=^{}(t)}{ dt}=-g(_{1}^{}(t),_{2}^{}(t),_{N}^{ }(t)),\] (8) \[_{i}^{}(t_{0}^{})=_{i}^{}( t_{K}),}_{i}^{}(t)=f_{}(_{i}^{}(t)).\]

Next, based on Lemma 2.1, if the system follows _Time-Reversal Symmetry_, the forward and backward trajectories shall be exactly overlap. We thus design the reversal loss by minimizing the L2 distances between model forward and backward trajectories decoded from the latent trajectories:

\[_{reverse}=_{i=1}^{N}_{k=0}^{K}\|}_{i}^{ }(t_{k})-}_{i}^{}(t_{K-k}^{})\|_{2 }^{2}.\] (9)

Finally, we jointly train TREAT as a weighted combination of the two losses:

\[=_{pred}+_{reverse}=_{i=1}^{N} _{k=0}^{K}\|_{i}(t_{k})-}_{i}^{}(t_{k})\| _{2}^{2}+_{i=1}^{N}_{k=0}^{K}\|}_{i}^{}(t_{k})-}_{i}^{}(t_{K-k}^{})\|_{2}^{2},\] (10)

where \(\) is a positive coefficient to balance the two losses based on different targeted systems.

**Remark.** The computational time of \(_{reverse}\) is of the same scale as the reconstruction loss \(_{pred}\). As the computation process of the reversal loss is to first use the ODE solver to generate the reverse trajectories, which has the same computational overhead as computing the forward trajectories, and then compute the L2 distances.

### Theoretical Analysis of Time-Reversal Symmetry Loss

We next theoretically show that the time-reversal symmetry loss numerically helps to improve prediction accuracy in general, regardless of systems' physical properties. Specifically, we show that it minimizes higher-order Taylor expansion terms during the ODE integration steps.

**Theorem 3.1**.: _Let \( t\) denote the integration step size in an ODE solver and \(T\) be the prediction length. The reconstruction loss \(_{pred}\) defined in Eqn 7 is \((T^{3} t^{2})\). The time-reversal loss \(_{reverse}\) defined in Eqn 9 is \((T^{5} t^{4})\)._

We prove Theorem 3.1 in Appendix A.3. From Theorem 3.1, we can see two nice properties of our proposed time-reversal loss: 1) Regarding the relationship to \( t\), \(_{reverse}\) is optimizing a high-order term \( t^{4}\), which forces the model to predict fine-grained physical properties such as jerk (the derivatives of accelerations). In comparison, the reconstruction loss optimizes \( t^{2}\), which mainly guides the model to predict the locations/velocities accurately. Therefore, the combined loss enables our model to be more noise-tolerable; 2) Regarding the relationship to \(T\), \(_{reverse}\) is more sensitive to total sequence length (\(T^{5}\)), thus it provides more regularization for long-context prediction, a key challenge for dynamic modeling.

**TRS Loss Design Choice.** We define \(_{reverse}\) as the distance between model forward trajectories and backward trajectories. Based on the definition of TRS in Sec. 2.2, there are other implementation choices. One prior work TRS-ODE (Huh et al., 2020) designed a TRS loss based on Eqn 5, where a reverse trajectory shares the same starting point as the forward one. However, we show that our implementation based on Lemma 2.1 to approximate time-reversal symmetry has a lower maximum error compared to their implementation below, supported by empirical experiments in Sec. 4.2.

**Lemma 3.2**.: _Let \(_{reverse}\) be the TRS implementation of TREAT based on Lemma 2.1, \(_{reverse2}\) be the one in (Huh et al., 2020) based on Eqn 5. When the reconstruction loss defined in Eqn 7 of both methods are equal, and the two TRS losses are equal, i.e. \(_{reverse}=_{reverse2}\), the maximum error between the reversal and ground truth trajectory for each agent, i.e. \(MaxError_{gt\_rev}=_{k[K]}\|_{i}(t_{k})-}_{i}^{}(t_{K-k}^{})\|_{2}\) for \(i=1,2 N\), made by TREAT is smaller._

We prove Lemma 3.2 in Appendix A.4. Another implementation is to minimize the distances between model backward trajectories and ground truth trajectories. When both forward and backward trajectories are close to ground-truth, they are implicitly symmetric. The major drawback is that at the early stage of learning when the forward is far away from ground truth (\(_{pred}\)), such implicit regularization does not force time-reversal symmetry, but introduces more noise.

## 4 Experiments

**Datasets.** We conduct systematic evaluations over five multi-agent systems including three 5-body spring systems (Kipf et al., 2018), a complex chaotic pendulum system and a real-world motion capture dataset (Carnegie Mellon University, 2003); and four single-agent systems including three spring systems (with only one node) and a chaotic strange attractors system (Huh et al., 2020).

The settings of spring systems include: 1) conservative, i.e. no interactions with the environments, we call it _Simple Spring_; 2) non-conservative with frictions, we call it _Damped Spring_; 3) non-conservative with periodic external forces, we call it _Forced Spring_. The _Pendulum_ system contains three connected sticks in a 2D plane. It is highly sensitive to initial states, with minor disturbances leading to significantly different trajectories (Shinbrot et al., 1992; Awrejcewicz et al., 2008). The real-world motion capture dataset (Carnegie Mellon University, 2003) describes the walking trajectories of a person, each tracking a single joint. We call it _Human Motion_. The strange attractor consists of symmetric attractor/repellor force pairs and is chaotic (Sprott, 2015). It is also highly sensitive to the initial states (Koppe et al., 2019). We call it _Attractor_.

Towards physical properties, _Simple Spring_ and _Pendulum_ are conservative and reversible; _Force Spring_ and _Attractor_ are reversible but non-conservative; _Damped Spring_ are irreversible and non-conservative. For _Human Motion_, it does not adhere to specific physical laws since it is a real-world dataset. Details of the datasets and generation pipelines can be found inAppendix C.

**Task Setup.** We conduct evaluation by splitting trajectories into two halves: \([t_{1},t_{M}]\), \([t_{M+1},t_{K}]\) where timestamps can be irregular. We condition the first half of observations to make predictions for the second half as in (Rubanova et al., 2019). For spring datasets and _Pendulum_, we generate irregular-sampled trajectories and set the training samples to be 20,000 and testing samples to be 5,000 respectively. For _Attractor_, We generate 1,000 and 50 trajectories for training and testing respectively following Huh et al. (2020). 10% of training samples are used as validation sets and the maximum trajectory prediction length is 60. Details can be found in Appendix C.

**Baselines.** We compare TREAT against three baseline types: 1) pure data-driven approaches including LG-ODE (Huang et al., 2020) and LatentODE (Rubanova et al., 2019), where the first one is a multi-agent approach considering pair-wise interactions, and the second one is a single-agent approach that predicts each trajectory independently; 2) energy-preserving HODEN (Greydanus et al., 2019); and 3) time-reversal TRS-ODEN (Huh et al., 2020).

The latter two are single-agent approaches and require initial states as given input. To handle missing initial states in our dataset, we approximate the initial states for the two methods via linear spline interpolation (Endre Suli, 2003). In addition, we substitute the ODE network in TRS-ODEN with a GNN (Kipf et al., 2018) as TRS-ODEN\({}_{}\), which serves as a new multi-agent approach for fair comparison. HODEN cannot be easily extended to the multi-agent setting as replacing the ODE function with a GNN can violate energy conservation of the original HODEN. For running LGODE and TREAT on single-agent datasets, we only include self-loop edges in the graph \(=(,)\), which makes the ODE function \(g\) a simple MLP. Implementation details can be found in Appendix D.2.

### Main Results

Table 1 shows the prediction performance on both multi-agent systems and single-agent systems measured by mean squared error (MSE). We can see that TREAT consistently surpasses other models, highlighting its generalizability and the efficacy of the proposed TRS loss.

For multi-agent systems, approaches that consider interactions among agents (LG-ODE, TRS-ODEN\({}_{}\), TREAT) consistently outperform single-agent baselines (LatentODE, HODEN, TRS-ODEN), and TREAT achieves the best performance across datasets.

The chaotic nature of the _Pendulum_ system and the _Attractor_ system, with their sensitivity to initial states 6, poses extreme challenges for dynamic modeling. This leads to highly unstable predictions for models like HODEN and TRS-ODEN, as they estimate initial states via inaccurate linear spline interpolation (Endre Suli, 2003). In contrast, LatentODE, LG-ODE, and TREAT employ advanced encoders that infer latent states from observed data and demonstrate superior accuracy. Among them, TREAT achieves the most accurate predictions, further showing its robust generalization capabilities.

We observe that misapplied inductive biases can degrade results, which limits the usage of physics-informed methods that are designed for individual physical prior such as HODEN. HODEN only excels on energy-conservative systems, such as _Simple Spring_ compared with LatentODE and TRS-ODEN in the multi-agent setting. Its performance drop dramatically on _Force Spring_, _Damped Spring_, and _Attractor_. Note that HODEN naively forces each agent to be energy-conservative, instead of the whole system. Therefore, it performs poorly than LG-ODE, TREAT in the multi-agent settings.

For the _Human Motion_ dataset, characterized by its dynamic ambiguity as it does not adhere to specific physical laws, we cannot directly determine whether it is conservative or time-reversal. For such a system with an unknown nature, TREAT outperforms other purely data-driven methods significantly, showcasing its strong numerical benefits in improving prediction accuracy across diverse system types. This is also shown by its superior performance on _Damped Spring_, which is irreversible.

    &  &  \\ Dataset & _Simple_ & _Fored_ & _Damped_ & _Pendulum_ & _Human_ & _Simple_ & _Fored_ & _Damped_ & _Attractor_ \\  LatentODE & 5.2622 & 5.0277 & 3.3419 & 2.6894 & 2.9061 & 5.7957 & 0.4563 & 1.3012 & 0.58394 \\ HODEN & 3.0039 & 4.0668 & 8.7950 & 741.2296 & 1.9855 & 3.2119 & 4.004 & 1.5675 & 54.2912 \\ TRS-ODEN & 3.6785 & 4.4465 & 1.7595 & 741.4988 & 0.5400 & 3.0271 & 0.4056 & 1.5667 & 2.2683 \\ TRS-ODEN\({}_{}\) & 1.4115 & 2.1102 & 0.5951 & 596.0319 & 0.2609 & / & / & / & / \\ LG-ODE & 1.7429 & 1.8929 & 0.9718 & 1.4156 & 0.7610 & 1.6156 & 0.1465 & 1.1223 & 0.6942 \\ TREAT & **1.1178** & **1.4525** & **0.5944** & **1.2527** & **0.2192** & **1.6026** & **0.0960** & **1.0750** & **0.5581** \\  \)—)} \\ _{_{rec}=}\)} \\ _{_{rec}=}\)} \\ _{_{rec}=}\)} \\   

Table 1: Evaluation results on MSE (\(10^{-2}\)). Best results are in **bold** numbers and second-best results are in underline numbers. _Human Motion_ is a real-world dataset and all others are simulated datasets.

Figure 4: Varying prediction lengths across multi-agent datasets (Pendulum MSE is in log values).

### Ablation and Sensitivity Analysis

**Ablation on implementation of \(_{reverse}\).** We conduct two ablation by changing the implementation of \(_{reverse}\) discussed in Sec. 3.2: 1) TREAT\(_{r_{rec}=}\), which computes the reversal loss as the L2 distance between ground truth trajectories to model backward trajectories; 2) TREAT\(_{r_{rec}=}\), which implements the TRS loss based on Eqn 5 as in TRS-ODEN (Huh et al., 2020). From the last block of Table 1, we can clearly see that our implementation achieves the best performance against the two.

**Evaluation across prediction lengths.** We vary the maximum prediction lengths from 20 to 60 and report model performance as shown in Figure 4. As the prediction step increases, TREAT consistently maintains optimal prediction performance, while other baselines exhibit significant error accumulations. The performance gap between TREAT and baselines widens when making long-range predictions, highlighting the superior predictive capability of TREAT.

**Evaluation across different \(\).** We vary the values of the coefficient \(\) defined in Eqn 10, which balances the reconstruction loss and the TRS loss. Figure 5 demonstrates that the optimal \(\) values being neither too high nor too low. This is because when \(\) is too small, the model tends to neglect the TRS physical bias, resulting in error accumulations. Conversely, when \(\) becomes too large, the model can emphasize TRS at the cost of accuracy. Nonetheless, across different \(\) values, TREAT consistently surpasses the purely data-driven LG-ODE, showcasing its superiority and flexibility in modeling diverse dynamical systems.

We study TREAT's sensitivity towards solver choice and observation ratios in E.1 and Appendix E.2 respectively.

Figure 5: Varying \(\) values across multi-agent datasets.

Figure 6: Visualization for 5-body spring systems (trajectory starts from light to dark colors).

### Visualizations

**Trajectory Visualizations.** Model predictions and ground truth are visualized in Figure 6. As HODEN is a single-agent baseline that individually forces every agent's energy to be constant over time which is not valid, the predicted trajectories is having the largest errors and systems' total energy is not conserved for all datasets. The purely data-driven LG-ODE exhibits unrealistic energy patterns, as seen in the energy spikes in _Simple Spring_ and _Force Spring_. In contrast, TREAT, incorporating reversal loss, generates realistic energy trends, and consistently produces trajectories closest to the ground truth, showing its superior performance.

**Reversal Loss Visualizations** To illustrate the issue of energy explosion from the purely data-driven LG-ODE, we visualize the TRS loss over training epochs from LG-ODE7 and TREAT in Figure 7. As results suggest, LG-ODE has increased TRS loss over training epochs, meaning it is violating the time-reversal symmetry sharply, in contrast to TREAT which has decreased reversal loss over epochs.

## 5 Conclusions

We propose TREAT, a deep learningframework that achieves high-precision modeling for a wide range of dynamical systems by injecting time-reversal symmetry as an inductive bias. TREAT features a novel regularization term to softly enforce time-reversal symmetry by aligning predicted forward and reverse trajectories from a GraphODE model. Notably, we theoretically prove that the regularization term effectively minimizes higher-order Taylor expansion terms during the ODE integration, which serves as a general numerical benefit widely applicable to various systems (even irreversible systems) regardless of their physical properties. Empirical evaluations on different kinds of datasets illustrate TREAT's superior efficacy in accurately capturing real-world system dynamics.

## 6 Limitations

Currently, TREAT only incorporates inductive bias from the temporal aspect, while there are many important properties in the spatial aspect such as translation and rotation equivariance (Satorras et al., 2021; Han et al., 2022; Xu et al., 2022). Future endeavors that combine biases from both temporal and spatial dimensions could unveil a new frontier in dynamical systems modeling.

## 7 Acknowledgement

This work was partially supported by NSF 2200274, NSF 2106859, NSF 2312501, NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NSF 2312501, DARPA HR00112290103/HR0011260656, HR00112490370, NIH U54HG012517, NIH U24DK097771, NASA, SRC JUMP 2.0 Center, Amazon Research Awards, and Snapchat Gifts.