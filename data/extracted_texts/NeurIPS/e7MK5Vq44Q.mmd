# DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets

Lazar Atanackovic\({}^{1,2}\) &Alexander Tong\({}^{3,4}\)&Bo Wang\({}^{1,2,5}\)&Leo J. Lee\({}^{1,2}\)

&Yoshua Bengio\({}^{3,4,6}\)&Jason Hartford\({}^{3,4,7}\)

\({}^{1}\)University of Toronto, \({}^{2}\)Vector Institute

\({}^{3}\)Mila - Quebec AI Institute, \({}^{4}\)Universite de Montreal

\({}^{5}\)University Health Network, \({}^{6}\)CIFAR Fellow, \({}^{7}\)Valence Labs

Equal ContributionCorrespondence to: (l.atanackovic@mail.utoronto.ca)

###### Abstract

One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (**1**) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (**2**) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (**1**), identifying _cyclic_ structure from dynamics, or on challenge (**2**) learning complex Bayesian _posteriors_ over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with _RNA velocity_ techniques to develop an approach that addresses both challenges. Because we have access to velocity information, we can treat the Bayesian structure learning problem as a problem of sparse identification of a dynamical system, capturing cyclic feedback loops through time. Since our objective is to model uncertainty over discrete structures, we leverage Generative Flow Networks (GFlowNets) to estimate the posterior distribution over the combinatorial space of possible sparse dependencies. Our results indicate that our method learns posteriors that better encapsulate the distributions of cyclic structures compared to counterpart state-of-the-art Bayesian structure learning approaches.

## 1 Introduction

Inferring gene regulatory networks (GRNs) is a long standing problem in cell biology [25; 44]. If we knew the GRN, it would dramatically simplify the design of biological experiments and the development of drugs because it would serve as a map of which genes to perturb to manipulate protein and gene expression. GRNs concisely represent the complex system of directed interactions between genes and their regulatory products that govern cellular function through control of RNA (gene) expression and protein concentration. We can treat GRN inference as a causal discovery problem by treating the regulatory structure between genes (variables) as causal dependencies (edges) that we infer / rule out by using gene expression data. Structure learning methods aim to automate this task by inferring a set of directed acyclic graphs (DAGs) that are consistent with the conditional independencies that we can measure among the variables [14; 41; 42]. While there may be multipleDAGs in this set--the "Markov equivalence class"--when we are able to perturb the variables with enough experimental interventions, it is possible to uniquely identify a causal graph .

However, structure learning for inferring GRNs comes with two non-standard challenges: **(1)** gene regulation contains inherent cyclic feedback mechanisms, hence we should not model a GRN as a DAG, and **(2)** observations are limited and have significant measurement noise, hence there exists a large equivalence class of graphs that are likely given datasets with typical sample sizes. Existing methods either focus on **(1)** - identifying graphs with _cyclic_ structure by leveraging dynamics  or assuming the system is in equilibrium , or **(2)** - learning complex Bayesian _posteriors_ over explanatory DAGs , but not both. In this work, we address both challenges concurrently in a fully differentiable end-to-end pipeline (see Figure 1).

To accomplish this, we treat structure learning as a problem of sparse identification of a dynamical system. From a dynamical systems perspective, one can model both causal structure between variables as well as their time-dependent system response with the drift function . We leverage the fact that we can estimate the rate of change of a gene's expression (velocity) with _RNA velocity_ methods . This data takes the form of dynamic tuple pairs \((x,dx)\), which we can use to pose the dynamical system learning problem as a regression task (see Figure 1). This significantly simplifies the learning objective as we can model system dynamics while also learning structure without the need for numerically intensive differential equation solvers. We view this as a step towards Bayesian structure learning from continuous dynamics - we term this _Bayesian dynamic structure learning_.

Our approach estimates the posterior over the sparse dependencies and parameters of the dynamical system. This is important in scientific applications because it is usually prohibitively expensive to acquire a enough data to uniquely identify the true graph underlying a data generating process. Capturing the complex distribution over candidate structure is critical for downstream scientific applications and is an essential step in active causal discovery . This is especially important in settings where experiments are expensive, e.g. conducting genetic perturbations for inference of GRNs. _Bayesian structure learning_ is a class of methods that try to model this distribution over structure from observed data. These methods model posteriors over admissible structures \(P(G|D)\) that explain the observations , but focus on modelling distributions over DAGs.

Our approach leverages Generative Flow Networks (GFlowNets) to model complex distributions over _cyclic_ structures. GFlowNets  parameterize the distribution over any discrete object (e.g. graphs) through a sequential policy, and as a result avoid needing to make restrictive parametric assumptions on the distribution. This makes them a useful tool in structure learning, particularly in cases where \(P(G|D)\) is discrete and complex . In this work, we use GFlowNets to learn posteriors over the sparse structure in a dynamical system, and separately learn the posteriors over the parameters of the drift function via a HyperNetwork  that conditions on inferred structures. Our main contributions are summarized as follows:

Figure 1: Architecture for Bayesian structure learning of dynamical systems. DynGFN consists of three main components: A GFlowNet modeling a posterior distribution over graphs \(Q_{}(G|)\), a HyperNetwork modeling a posterior over parameters given a graph \(Q_{}(|G,D)\), and the structural equation model scoring \(G\) and \(\) according to how well they fit the data. Although the figure shows the case where \(Q_{}(G|)\) is modelled with a GFlowNet, this can be any arbitrary graph sampler that can sample discrete structures \(G Q_{}(G|)\).

* We develop a novel framework for Bayesian structure learning under the lens of dynamical system identification for modelling complex posteriors over cyclic graphs. We consider flexible parameterizations for the structural model such that we can capture both linear and non-linear dynamic relationships.
* We design a novel GFlowNet architecture, Dynamic GFlowNet (DynGFN), tailored for modelling posteriors over cyclic structures. We propose a _per-node_ factorization within DynGFN that enables efficient search over the discrete space of cyclic graphs.
* We empirically evaluated DynGFN on synthetic dynamic data designed to induce highly multi-modal posteriors over graphs.
* We showcase the use of DynGFN on a real biological system using single-cell RNA-velocity data for learning posteriors of GRNs.

## 2 Related Work

There are many works on the problem of identifying causal structure \(G\) from either observational (e.g. 49; 54; 35) or interventional (e.g. 26; 29; 38) data, but the majority of existing methods return only the most likely DAG under the observed data. By returning only the most likely graph, these methods are often overconfident in their predictions. Bayesian approaches attempt to explicitly model a posterior distribution over DAGs given the data and model specification.

Bayesian Structure Learning:Recently, there has been significant interest in fully differentiable Bayesian methods for structure learning in the static case. DiBS (32), BCD-Nets (11), VCN (4), and DAG-GFlowNet (12) all attempt to learn a distribution over structural models from a fully observed system. The key difference is in how these methods parameterize the graph. DiBS is a particle variational inference method that uses two matrices \(U\) and \(V\) where \(G=(U^{T}V)\) where the sigmoid is applied elementwise which is similar to graph autoencoders. BCD-Nets and DP-DAG use the Gumbel-Sinkhorn distribution to parameterize a permutation and direct parameterization of a lower triangular matrix. VCN uses an autoregressive LSTM to generate the graph as this gets rid of the standard uni-modal constraint of Gaussian distributed parameters. DAG-GFN has shown success for modelling \(P(G|)\)(12). However, it remains restrictive to assume the underlying structure of the observed system is a DAG as natural dynamical systems typically contain regulating feedback mechanisms. This can be particularly challenging for GFlowNets since including cycles in the underlying structure exponentially increases the discrete search space. We show that under certain assumptions we can in part alleviate this shortcoming for learning Bayesian posteriors over cyclic structures for dynamical systems. In small graphs, these methods can model the uncertainty over possible models (including over Markov equivalence classes).

Dynamic and Cyclic Structure Learning:There has been comparatively little work towards Bayesian structure learning from dynamics. Recent works in this direction based on NeuralODEs (10) propose a single explanatory structure (50; 6; 1; 2). CD-NOD leverages heterogeneous non-stationary data for causal discovery when the underlying generative process changes over time (53; 21). A similar approach uses non-stationary time-series data for causal discovery and forecasting (20). DYNAGEARS is a score-based approach that uses time-series to learn structure (40). However, these methods do not attempt to explicitly model a distribution over the explanatory structure. Other methods aim to learn cyclic dependencies in the underlying graph (24; 36; 28; 3). For instance, (24) propose an iterative method that leverages interventional data to learn directed cyclic graphs. It is suggested that CD-NOD is also extendable to learn cyclic structure (21). But these methods do not model a posterior over structure. In general, there remains a gap for the problem of Bayesian structure learning over cyclic graphs.

We include further discussion on related work for GRN inference from single-cell transcriptomic data and cell dynamics in Appendix C.1.

Preliminaries

### Bayesian Dynamic Structure Learning

Problem Setup:We consider a finite dataset, \(\), of dynamic pairs \((x,dx)^{d}^{d}\) where \(x\) respresents the state of the system sampled from an underlying time-invariant stochastic dynamical system governed by a latent drift \(=f(x,)\) where \(\) is a noise term that parameterizes the SDE; \(x\) and \(\) are mutually independent. The latent drift has some fixed sparsity pattern i.e. \(}{ x_{j}} 0\) for a small set of variables, which can be parameterized by a graph \(G\) such that \(g_{ij}=[}{ x_{j}} 0]\), where \(g_{ij} G,i=1,,d,j=1,,d\). The variables \(x_{j}\) for which \(}{ x_{j}} 0\) can be interpreted as the causal parents of \(x_{i}\), denoted \((x_{i})\). This lets us define an equivalent dynamic structural model  of the form,

\[(t)}{dt}=f_{i}((x_{i}),_{i}),\] (1)

for \(i=1,,d\). For the graph \(G\) to be identifiable, we assume that all relevant variables are observed, such that _causal sufficiency_ is satisfied.

Our goal is to model our posterior over explanatory graphs \(Q(G|)\) given the data. We aim to jointly learn distribution over parameters \(\) that parameterize the latent drift \(f(x)\); these parameters will typically depend on the sparsity pattern such that \(p(|G) p()\). We can factorize this generative model as follows,

\[p(G,,)=p(|G,)p(|G)p(G)\] (2)

This factorization forms the basis of our inference procedure. We learn a parameterized function \(f_{}(x):^{d}^{d}\) that approximates the structural model defined in (1). To model this joint distribution, we need a way of representing \(P(G)\), a distribution over the combinatorial space of possible sparsity patterns, and \(P(|G)\), the posterior over the parameters of \(f_{}\). We use GFlowNets  to represent \(P(G)\), and a HyperNetwork to parameterize \(P(|G)\).

### Generative Flow Networks

GFlowNets are an approach for learning generative models over spaces of discrete objects . GFlowNets learn a stochastic policy \(P_{F}()\) to sequentially sample an object \(\) from a discrete space \(\). Here \(=(s_{0},s_{1},,s_{n})\) represents a full Markovian trajectory over plausible discrete states, where \(s_{n}\) is the terminating state (i.e. end of a trajectory) . The GFlowNet is trained such that at convergence, sequential samples from the stochastic policy over a trajectory, \( P_{F}()\), i.e. \(=s_{n}\), are equal in distribution to samples from the normalized reward distribution \(P()=)}{_{^{}}R( ^{})}\).

The GFlowNet policies are typically trained by optimizing either the _Trajectory Balance_ (TB) loss , _Subtrajectory Balance_ (Sub-TB) loss , or the _Detailed Balance_ (DB) loss . In this work, we exploit the DB loss to learn a stochastic policy for directed graph structure.

Detailed Balance Loss:The DB loss  leverages the fact that the reward function can be evaluated for any partially constructed graph (i.e. any prefix of \(\)), and hence we get intermediate reward signals for training the GFlowNet policy. The DB loss is defined as:

\[_{}(s_{i},s_{i-1})=\,()P_{B}(s_{i-1}| s_{i};)P_{F}(s_{n}|s_{i-1};)}{R(s_{i-1})P_{F}(s_{i}|s_{i-1};)P_{F}(s_{n}| s_{i};)})^{2},\] (3)

where \(P_{F}(s_{i}|s_{i-1};)\) and \(P_{B}(s_{i-1}|s_{i};)\) represent the forward transition probability and backward transition probability, and a trainable normalizing constant, respectively. Under this formulation, during GFlowNet training the reward is evaluated at every state. For this reason, the DB formulation is in general advantageous for the structure learning problem where any sampled graph can be viewed as a complete state, hence more robustly inform gradients when training the stochastic policy than counterpart losses. Previous work has shown GFlowNets are useful in settings with multi-modal posteriors. This is of particular interest to us where many admissible structures can explain the observed data equally well. We model \(Q_{}(G)\) using \(P_{F}(s_{i}|s_{i-1};)\) and learn the parameters \(\).

```
1:Input: Data batch \((x_{b},dx_{b})\), initial NN weights \(,,L^{0}\) sparsity prior \(_{0}\), and learning rate \(\).
2:\(s_{0}_{B d d}\)\(\)Training is paralleled over \(B\) graph trajectories
3:\(a P_{F}(s_{1}|s_{0};)\), \(\)Sample initial actions vector
4:while\(a\) not \(\)do
5: Compute \(P_{F}(s_{i}|s_{i-1};)\), \(P_{B}(s_{i-1}|s_{i};)\)
6:\( h_{}(s_{i})\)
7:\(_{b} f_{}(x,s_{i})\)
8:\(R_{i}(s_{i}) e^{-\|dx_{b}-_{b}\|^{2}_{2}-_{0}\|s _{i}\|_{0}}\)
9:\(-_{}_{DB}(s_{i},s_{i-1})\)\(\)\(_{DB}(s_{i},s_{i-1})\) computed as in Equation 3
10:\(a P_{F}(s_{i}|s_{i-1};)\), \(s_{i} s_{i+1}\)\(\)Take action step to go to next state
11:\(+_{} R\)return Updated GFN weights \(\) and updated HyperNetwork weights \(\). ```

**Algorithm 1** Batch update training of DynGFN

## 4 DynGFN for Bayesian Dynamic Structure Learning

We present a general framework for Bayesian dynamic structure learning and propose a GFlowNet architecture, _DynGFN_, tailored for modelling a posterior over discrete cyclic graphical structures. We summarize our framework in Figure 1 and Algorithm 1. DynGFN consists of \(3\) key modules:

1. A graph sampler that samples graphical structures that encode the structural dependencies among the observed variables. This is parameterized with a GFlowNet that iteratively adds edges to a graph.
2. A model that approximates the structural equations defined in (1) to model the functional relationships between the observed variables, indexed by parameters \(\). This is a class of functions that respect the conditional independencies implied by the graph sampled in step 1. We enforce this by masking inputs according to the graph.
3. Because the functional relationships between variables may be different depending on which graph is sampled, we use a HyperNetwork architecture that outputs the parameters \(\) of the structural equations as a function of the graph. We also show that under linear assumptions of the structural modules, we can solve for optimal \(\) analytically (i.e. we do not need the HyperNetwork).

For training, we assume \(L^{0}\) sparsity of graphs \(G\) to constrain the large discrete search space over possible structures. We use a reward \(R\) for a graph \(G\) and \(L^{0}\) penalty of the form: \(R(G)=e^{-\|dx-\|^{2}_{2}+_{0}\|G\|_{0}}\). We motivate this set-up so we can estimate \(\) close to \(dx\) in an end-to-end learning pipeline. Since estimates for \(\) are dependent on \(G\) and \(\), this reward informs gradients to learn a policy that can approximate \(Q(G)\) given dynamic data.

The main advantage of DynGFN comes when modelling complex posteriors with many modes. Prior work has shown GFlowNets are able to efficiently model distributions where we can share information between different modes . The challenge we tackle is how to do this with a changing objective function, as the GFlowNet objective is a function of the current parameter HyperNetwork and the structural equations. We use multilayer perceptrons (MLPs) to parameterize the stochastic GFlowNet policy, HyperNetwork architecture, and the dynamic structural model1.

### Graph Sampler

DynGFN models a posterior distribution over graphs \(Q(G|)\) given a finite set of observations. To learn \(Q(G|)\), DynGFN needs to explore over a large discrete state space. Since we aim to learn a bipartite graph between \(x\) and \(dx\), DynGFN needs to search over \(2^{d^{2}}\) possible structures, where \(d\) denotes the dimensionality of the system and \(2^{d^{2}}\) the number of possible edges in \(G\). For even moderate \(d\), this discrete space is very large (e.g. for \(d=20\) we have \(2^{400}\) possible graphs).

However, under the assumption of causal sufficiency, we can significantly reduce this search space, by taking advantage of the fact that \(Q(G|)\) factorizes as follows,

\[Q(G|D)=_{i[1,,d]}Q_{i}(G[,i]|D)\] (4)

By using this model, we reduce the search space from \(2^{d^{2}} d2^{d}\). For \(d=20\) this reduces the search space from \(2^{400}\) to \( 2^{24.3}\). While still intractable to search over, it is still a vast improvement over the unfactorized case. We call this model a _per-node_ posterior, and we use a per-node GFlowNet going forward. We discuss details regarding encouraging forward policy exploration during training in Appendix B.6.

### HyperNetwork and Structural Model

We aim to jointly learn the structural encoding \(G\) and parameters \(\) that together model the structural relationships \(dx=f_{}(x,G)\) of the dynamical system variables. To accomplish this, we propose learning an individual set of parameters \(\) for each graph \(G\), independent of the input data \(x\). This approach encapsulates \(P(|G)\) in (2). We use a HyperNetwork architecture that takes \(G\) as input and outputs the structural equation model parameters \(\), i.e. \(=h_{}(G)\) hence \(P(|G)=(|G)-\) allowing us to learn a separate \(\) for each \(G\). This HyperNetwork model does not capture uncertainty in the parameters, however the formulation may be extended to the Bayesian setting by placing a prior on the HyperNetwork parameters \(\). Although \(h_{}\) allows for expressive parameterizations for \(\), it may not be easy to learn2. HyperNetworks have shown success in learning parameters for more complex models (e.g. LSTMs and CNNs) , hence motivates their fit for our application.

**Linear Assumption on Dynamic Structural Model:** In some cases it may suffice to assume a linear differential form \(=x\) to approximate dynamics. In this setting, given a sampled graph \(G Q(G)\) and \(n\) i.i.d. observations of \((x,dx)\) we can solve for \(=\) analytically. To induce dependence on the graph structure, we use the sampled \(G\) as a mask on \(x\) and construct \(_{i}=G_{i}^{T} x\). Then we can solve for \(\) on a per-node basis as

\[_{i}=(_{i}^{T}_{i}+ I)^{-1}_{i}^{T}dx_ {i},\] (5)

where \(i=1, d\), \(>0\) is the precision of an independent Gaussian prior over the parameters, and \(I\) is the identity matrix. We use \(=0.01\) throughout this work.

## 5 A Useful Model of Indeterminacy

In order to evaluate the ability of DynGFN to model complex posteriors over graphs, we need a structure learning problem with a large equivalence class of admissible graphs. We present a simple way to augment a set of identifiable dynamics under some model to create a combinatorial number of equally likely dynamics under the same model. More specifically, this creates a ground truth posterior \(Q^{*}(G|D) T(G^{*})\) where \(T():\) is an analytically computable transformation over graphs and \(G^{*}\) is the identified graph under the original dynamics. We use this system to test how well we can learn a posterior over structures that matches what we see in single-cell data.

Specifically, given a dataset of \((x,dx)^{d}^{d}\) pairs, we create a new dataset with \(d+1\) variables where the 'new' variable \(v^{}\) is perfectly correlated with an existing variable \(v\). In causal terms, this new variable inherits the same parents as \(v\), that is \((v^{}):=(v)\) and the same structural equations as \(v\), that is \(dv^{}=dv\). This is depicted in Figure 2. This creates a number of new possible explanatory graphs, which we generalize with the following proposition.

**Proposition 1**.: _Given any \(d\) dimensional ODE system with \(^{*}\) identifiable under \(f\), the \(D=d+a\) dimensional system \(=x\), denote the vector of multiplicities \(m^{d}\) with \(m_{i}\) as the number of repetitions of each variable. Then this construction creates an admissible family of graphs \(^{}\) where \(|^{}|=_{i d}(2^{m_{i}}-1)^{_{i}}\)1. Furthermore, under an \(L^{0}\) penalty on \(G\), this reduces to \(_{i}(m_{i})^{_{i}}\)1._

See Appendix A for full proof. The intuition behind this proposition can be seen from the case of adding a single copied variable. This corresponds to \(=[_{v}_{d}]\) where \(_{v}\) is a vector with a 1 on node \(v\) and zeros elsewhere, and \(_{\_}{\_}{d}\) is the \(d\)-dimensional identity matrix. Let \(v\) have \(c\) children, such that \(v Pa(c)\) in the identifiable system, then any of those \(c\) child nodes could depend either on \(v\) or on the new node \(v^{}\) or both. This creates \(3^{c}\) possible explanatory graphs. If we restrict ourselves to the set of graphs with minimal \(L^{0}\) norm, then we eliminate the possibility of a child node depending on both \(v\) and \(v^{}\), this gives \(2^{c}\) possible graphs, choosing either \(v\) or \(v^{}\) as a parent.

## 6 Experimental Results

In this section we evaluate the performance of DynGFN against counterpart Bayesian structure learning methods (see Appendix B.2 for details). Since our primary objective is to learn Bayesian posteriors over discrete structure \(G\), we compare to Bayesian methods that can also accomplish this task, i.e. versions of BCD-Nets  and DiBS . We show in certain cases, DynGFN is able to better capture the true posterior when there are a large number of modes. We evaluate methods according to four metrics: Bayes-SHD, area under the receiver operator characteristic curve (AUC), Kullback-Leibler (KL) divergence between learned posteriors \(Q(G)\) and the distribution over true graphs \(P(G^{*})\), and the negative log-likelihood (NLL) \(P(D|G,)\) (in our setting this reduces to the mean squared error between \(\) and \(dx\), given \(\) and sampled \(G^{}s\)). Since the analytic linear solver requires data at run-time to compute optimal parameters for the structural model, we include the NLL metric only for models using the HyperNetwork solver. Bayes-SHD measures the average distance to the closest structure in the admissible set of graphs according to the structural hamming distance, which in this case is simply the hamming distance of the adjacency matrix representation to the closest admissible graph. We assume \(P(G^{*})\) is uniform over \(G^{*}\) and include further details about evaluating the quality of learned posteriors in Appendix B.8.

### A Toy Example with Synthetic Data

To validate the ability of DynGFN to learn cyclic dependencies we consider identifiable acyclic and cyclic \(3\) variable toy systems and provide a comparison with a DAG structure learning method (NOTEARS ). We show results for this toy example in Figure 3. NOTEARS does not model cyclic dependencies and therefore struggles to yield accurate predictions of \(\) in the cyclic setting. We can also verify this result by considering a conditional independence test over cyclic dependencies. It is easy to see that the conditional independence test fails in the cyclic setting: in the acyclic case, we can identify the v-structure by observing that \(x_{\_}1 x_{\_}3\) and \(x_{\_}1 x_{\_}3|x_{\_}2\), which implies that \(x_{\_}2\) is a collider (i.e. \(x_{\_}1\) and \(x_{\_}3\) are marginally independent and conditionally dependent); while in the cyclic example, we introduce time dependencies such that there are cycles in the summary graph that render these variables marginally dependent. We show that DynGFN is able to identify the true cyclic dependencies in this toy example. We note that in this toy example DynGFN exhibits a degree

Figure 2: Visualization of modeling cyclic dependencies over time (left). To create a family of admissible graphs (right), we add a new variable \(v^{}_{\_}{3}\) which has the same values as \(v_{\_}{3}\) and creates three possible explanations for the data (green arrows). If we apply a sparsity penalty, then we can eliminate the last possibility (which has two additional edges) for only two possible graphs. \(G^{*}\) denotes the ground truth graph while \(G^{}\) denotes the admissible family of graphs induced by \(v^{}_{\_}{3}\).

of convergence sensitivity on a per-run basis. In the following sections we provide comprehensive results over \(5\) random seeds and consider larger systems.

### Experiments with Synthetic Data

We generated synthetic data from two systems using our indeterminacy model presented in section \(5\): (1) a linear dynamical system \(dx=x\), and (2) a non-linear dynamical system \(dx=(x)\). We consider \(\)-DynGFN and \(h\)-DynGFN, i.e. DynGFN with the linear analytic parameter solver as shown in (5), and DynGFN with the HyperNetwork parameter solver \(h_{}\). Likewise, we compare \(\)-DynGFN and \(h\)-DynGFN to counterpart Bayesian baselines which we call \(\)-DynBCD, \(\)-DynDiBS, \(h\)-DynBCD, and \(h\)-DynDiBS. To constrain the discrete search procedure, we assume a sparse prior on the structure (i.e. the graphs \(G\)), using the \(L^{0}\) prior. Due to challenging iterative optimization dynamics present when using \(=h_{}(G)\) for DynGFN, to train initialize the forward policy \(P_{F}(s_{i}|s_{i-1};)\) using the \(\) learned in \(\)-DynGFN to provide a more admissible starting point for learning \(h_{}\) (we

    &  \\
**Model** & **Bayes-SHD \(\)** & **AUC \(\)** & **KL \(\)** & **NLL \(\)** \\  \(\)-DynBCD & \(32.0 0.27\) & \(0.71 0.0\) & \(1707.45 9.66\) & — \\ \(\)-DynDiBS & \(29.2 0.78\) & \(0.71 0.0\) & \(6622.43 171.67\) & — \\ \(\)-DynGFN & \(\) & \(\) & \(\) & — \\  \(h\)-DynBCD & \(\) & \(0.89 0.04\) & \(701.19 46.99\) & \((9.83 0.59)-5\) \\ \(h\)-DynDiBS & \(28.5 4.2\) & \(0.51 0.07\) & \(7934.90 381.80\) & \(()-6\) \\ \(h\)-DynGFN & \(\) & \(\) & \(\) & \((8.35 0.02)-3\) \\    &  \\
**Model** & **Bayes-SHD \(\)** & **AUC \(\)** & **KL \(\)** & **NLL \(\)** \\  \(\)-DynBCD & \(77.5 8.3\) & \(0.42 0.03\) & \(3814.86 354.56\) & — \\ \(\)-DynDiBS & \(75.7 7.7\) & \(\) & \(5893.65 59.66\) & — \\ \(\)-DynGFN & \(\) & \(0.55 0.0\) & \(\) & — \\  \(h\)-DynBCD & \(192.9 0.7\) & \(0.50 0.0\) & \(9108.69 51.34\) & \((3.83 0.32)-4\) \\ \(h\)-DynDiBS & \(48.1 9.0\) & \(0.53 0.10\) & \(8716.64 265.29\) & \(()-6\) \\ \(h\)-DynGFN & \(\) & \(\) & \(\) & \((1.47 0.11)-3\) \\   

Table 1: Bayesian dynamic structure learning of linear and non-linear systems with \(d=20\) variables. The graphs representing the structural dynamic relationships of the linear and non-linear systems have \(50\) edges out of possible \(400\). The ground truth discrete distribution \(P(G^{*})\) contains \(1024\) admissible graphs for each respective system. The \(\) and \(h\) pre-fix denote usage of the analytic linear solver and HyperNetwork solver for structural model parameters, respectively. Results are reported on held out test data over 5 model seeds.

Figure 3: Toy example for learning DAG structure (top) and cyclic graph structure (bottom) from observational data. In this example we compare NOTEARS and DynGFN for learning acyclic and cyclic dependencies from observational data. Data is generated from a linear system \(dx=x\) with a corresponding acyclic and cyclic \(\) as defined in the figure. We use \(500\) observational samples for each system. NOTEARS is implemented using the CausalNex  library.

discuss further details in Appendix B.7). We do not need to do this for \(h\)-DynBCD and \(h\)-DynDiBS as we are able to train both models end-to-end without iterative optimization. In Table 1 we show results of our synthetic experiments for learning posteriors over multi-modal distributions of cyclic graphs. We observe the DynGFN is most competitive on both synthetic systems for modelling the true posterior over structure. Details about DynGFN, baselines, and accompanying hyper-parameters can be found in Appendix B.

### Ablations Over Sparsity and Linearity of Dynamic Systems

We conduct two ablations: (1) ablation over sparsity of the dynamic system structure, and (2) ablation over \( t\), the time difference between data points of dynamic simulation. For a sparsity level of \(0.9\), the ground truth graphs have \(50\) edges out of \(d^{2}\) possible edges. In these experiments, \(P(G^{*})\) for \(d=20\) and sparsity \(0.9\) has \(1024\) modes. We conduct the ablations over 5 random seeds for each set of experiments.

Sparsity:DynGFN uses the \(L^{0}\) prior on \(G\) throughout training. Under this setting, system sparsity carries significant weight on the ability to learn posteriors over the structured dynamics of a system. We show this trend in Table 2. We note that computing the KL-divergence for DynGFN, specifically computing the probability of generating a true \(G\), becomes computationally intractable as \(G\) is less sparse3. For systems of \(0.9\) and \(0.95\) sparsity, we observe a decreasing trend in KL and Bayes-SHD, and an increasing trend in AUC. This result is expected as DynGFN can better traverse sparse graphs as the combinatorial space over possible trajectories is smaller relative to denser systems.

Linearity:Training DynGFN via the linear solver for the structural model parameters is an easier objective due to simplified training dynamics. Because of this, we explore the performance of \(\)-DynGFN assuming \(f_{}\) for modelling equation (1) to be linear in the non-linear system. We do this by conducting an ablation over \( t\) and find that the performance of \(\)-DynGFN on the non-linear system improves as \( t 0\). We show a portion of this trend in Table 3.

### Experiments on Single-Cell RNA-velocity Data

To show how DynGFN can be applied to single cell data we use a cell cycle dataset of human Fibroblasts . As a motivating example we show the correlation structure of single-cell RNA-seq data from human Fibroblast cells  Figure 4. We show both the raw correlation and the correlation over cell cycle time, which is significantly higher. With such a pure cell population whose primary axis of variation is state in the cell cycle by aggregating over cell cycle time we expect observation noise to be averaged out, leading to a "truer" view of the correlation between latent variables. Further details for this experimental set-up are provided in Appendix C.1.3. Since there are many genes which are affected by the cell cycle phase, there are many correlated variables that are downstream of the true cell cycle regulators. This provides a natural way of using cell cycle data to evaluate a model's ability to capture the Bayesian posterior. In Table 4 we show results for learning posteriors over an undetermined GRN using RNA velocity data. We find that \(\)-DynGFN and \(h\)-DynGFN yield low KL

  
**Sparsity** & **Bayes-SHD \(\)** & **AUC \(\)** & **KL \(\)** \\  \(0.95\) & \(16.4 1.71\) & \(0.79 0.0\) & \(889.57 31.24\) \\ \(0.90\) & \(22.8 1.41\) & \(0.75 0.01\) & \(1091.60 35.72\) \\ \(0.85\) & \(32.8 0.72\) & \(0.71 0.0\) & — \\ \(0.80\) & \(39.2 0.69\) & \(0.71 0.0\) & — \\ \(0.75\) & \(60.2 1.17\) & \(0.66 0.01\) & — \\   

Table 2: Ablation for \(\)-DynGFN on \(d=20\) systems with varying levels of sparsity and fixed \( t=0.05\).

   \( t\) & **Bayes-SHD \(\)** & **AUC \(\)** & **KL \(\)** \\  \(0.001\) & \(38.7 0.80\) & \(0.61 0.0\) & \(202.41 9.95\) \\ \(0.005\) & \(39.0 0.81\) & \(0.60 0.0\) & \(206.83 11.55\) \\ \(0.01\) & \(40.6 1.13\) & \(0.59 0.0\) & \(202.71 7.74\) \\ \(0.05\) & \(45.7 0.62\) & \(0.55 0.0\) & \(226.25 6.58\) \\ \(0.1\) & \(51.8 0.18\) & \(0.50 0.0\) & \(264.86 2.17\) \\   

Table 3: Ablation for \(\)-DynGFN on \(d=20\) non-linear systems with varying \( t\) and fixed sparsity at \(0.9\).

and moderate Bayes-SHD. While \(\)-DynBCD performs well in terms of identify a small distribution of true \(G\)'s, it falls short in modelling the true posterior (this can be seen from low Bayes-SHD, high KL).

## 7 Conclusion

We presented DynGFN, a method for Bayesian dynamic structure learning. In low dimensions we found that DynGFN is able to better model the distribution over possible explanatory structures than counterpart Bayesian structure learning baseline methods. As a proof of concept, we presented an example of learning the distribution over likely explanatory graphs for linear and non-linear synthetic systems where complex uncertainty over explanatory structure is prevalent. We demonstrate the use of DynGFN for learning gene regulatory structure from single-cell transcriptomic data where there are many possible graphs, showing DynGFN can better model the uncertainty over possible explanations of this data rather than capturing a single explanation.

Limitations and Future Work:We have demonstrated a degree of efficacy when using DynGFN for Bayesian structure learning with dynamic observational data. A key limitation of DynGFN is scaling to larger systems. To effectively model \(P(G,,D)\), DynGFN needs to search over an environment state space of possible graphs. This state space grows exponentially with the number of possible edges, i.e. \(2^{d^{2}}\) or \(d2^{d}\) for per-node-GFN where \(d\) is the number of variables in the system. Therefore, DynGFN is currently limited to smaller systems. Nevertheless, there are many applications where Bayesian structure learning, even over 5-20 dimensional examples that we explore here, could be extraordinarily useful. We include further discussion of scaling DynGFN in Appendix C.3 with some ideas on how to approach this challenge. We found that training DynGFN requires some selection of hyper-parameters and in particular parameters that shape the reward function. Selecting hyper-parameters for the baseline methods prove more difficult for this task.

    &  \\
**Model** & **Bayes-SHD \(\)** & **AUC \(\)** & **KL \(\)** & **NLL \(\)** \\  \(\)-DynBCD & **2.6 \(\) 0.1** & \(0.56 0.01\) & \(321.95 3.34\) & — \\ \(\)-DynDiB8 & \(6.5 0.4\) & \(0.47 0.01\) & \(550.17 16.63\) & — \\ \(\)-DynGFN & **3.3 \(\) 0.4** & **0.59 \(\) 0.03** & **44.98 \(\) 18.60** & — \\  \(h\)-DynBCD & \(10.1 0.8\) & \(0.53 0.03\) & \(587.41 24.00\) & \(0.094 0.003\) \\ \(h\)-DynDiBS & \(9.6 4.2\) & \(0.51 0.13\) & \(560.85 83.83\) & **0.084 \(\) 0.0** \\ \(h\)-DynGFN & **5.1 \(\) 1.2** & **0.58 \(\) 0.05** & **39.82 \(\) 28.05** & \(0.109 0.001\) \\   

Table 4: Bayesian dynamic structure learning \(5\)-D cellular system using scRNA velocity data. The dynamics of this system are unknowns, however we identify \(81\) admissible graphs between variables (genes) that describe the data. We train models over \(5\) seeds. The graphs of this system contain of \(7\) true edges.

Figure 4: (a) Correlation structure in the raw single cell data over 5000 cells and 2000 genes selected by scVelo  pre-processing. (b) Correlation structure among genes over (inferred) cell cycle times. This stronger correlation structure is more reflective of the correlation in the underlying system. (c) Histogram of pairwise Pearson correlations between all genes passing pre-processing, comparing the absolute values of the elements in (a) and (b). (d) Shows the ground truth GRN extracted as a subset of the KEGG cell cycle pathway. Cdc25A is known to inhibit Cdk1 which is known to inhibit Cdc25C, while the Mcm complex is highly correlated with Cdc25A, they do not directly interact with Cdk1 .

Acknowledgments and Disclosure of Funding

This research was enabled in part by the computational researches provided by Mila (mila.quebec) and Compute Canada (ccdb.computecanada.ca). In addition, resources used in preparing this research were in part provided by the Province of Ontario and companies sponsoring the Vector Institute (vectorinstitute.ai/partners/). All authors are funded by their primary academic institution. We also acknowledged funding from the Natural Sciences and Engineering Research Council of Canada, Recursion Pharmaceuticals, CIFAR, Samsung, and IBM. We are grateful to Cristian Dragos Manta for catching some typos in the manuscript.