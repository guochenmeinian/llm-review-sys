# A Simple Framework for Generalization in Visual RL under Dynamic Scene Perturbations

Wonil Song

Yonsei University

Seoul, South Korea

swoni192@yonsei.ac.kr

&Hyesong Choi

Ewha Womans University

Seoul, South Korea

hyesongchoi2010@gmail.com

&Kwanghoon Sohn

Yonsei University

Seoul, South Korea

khsohn@yonsei.ac.kr

&Dongbo Min

Ewha Womans University

Seoul, South Korea

dbmin@ewha.ac.kr

Corresponding Author

###### Abstract

In the rapidly evolving domain of vision-based deep reinforcement learning (RL), a pivotal challenge is to achieve generalization capability to dynamic environmental changes reflected in visual observations. Our work delves into the intricacies of this problem, identifying two key issues that appear in previous approaches for visual RL generalization: (i) imbalanced saliency and (ii) observational overfitting. Imbalanced saliency is a phenomenon where an RL agent disproportionately identifies salient features across consecutive frames in a frame stack. Observational overfitting occurs when the agent focuses on certain background regions rather than task-relevant objects. To address these challenges, we present a simple yet effective framework for generalization in visual RL (SimGRL) under dynamic scene perturbations. First, to mitigate the imbalanced saliency problem, we introduce an architectural modification to the image encoder to stack frames at the feature level rather than the image level. Simultaneously, to alleviate the observational overfitting problem, we propose a novel technique called shifted random overlay augmentation, which is specifically designed to learn robust representations capable of effectively handling dynamic visual scenes. Extensive experiments demonstrate the superior generalization capability of SimGRL, achieving state-of-the-art performance in benchmarks including the DeepMind Control Suite. 1

## 1 Introduction

Deep reinforcement learning (RL) utilizing visual observations has achieved remarkable success across diverse domains, including robotic manipulation , video games , and autonomous navigation . However, acquiring generalizable RL policies across diverse environments remains challenging, mainly due to overfitting  in the high-dimensional observation space . To obtain robust policies invariant to visual perturbations, a variety of approaches based on domain randomization  and data augmentation  have been widely proposed. These approaches operate under the assumption that exposing an agent to various augmentations during the training phase enhances its adaptability to unseen domains. Despite the encouraging results, performance still falls behind in challenging environments with dynamically changing backgrounds.

Fig. 1(a) shows the notable performance degradation of existing approaches when comparing 'Video Hard' scenes with dynamically changing backgrounds to less dynamic 'Video Easy' scenes in the DeepMind Control Suite-Generalization Benchmark (DMControl-GB) .

Using a gradient-based attribution mask \(M_{}\), we first investigate the causes of the degradation in generalization in such challenging environments by examining salient regions across consecutive stacked frames used as an RL input. Based on our analysis, we empirically identified two phenomena, highlighting them as key causes of performance degradation: (i) what we refer to as _imbalanced saliency_ and (ii) _observational overfitting_. In the DMControl , Fig. 1(b) depicts an example of the imbalanced saliency in a 'Cartpole, Swingup' task, where salient features across all stacked frames are biased to the regions corresponding to the task objects in the latest two frames. Fig. 1(c) shows an example of the observational overfitting that occurs in a 'Cheetah, Run' task. In this case, the RL agent misidentififies the ground as more salient than the 'Cheetah' object. These two problems contribute to overfitting to the training environment, making generalization of the RL agent even more challenging.

In image-based RL, the effectiveness of regularization has been demonstrated in numerous studies. DrQ  and RAD , each employing random shift and random crop augmentations as data regularization on input images, achieved remarkable performance improvements over vanilla SAC . Furthermore,  exhibited that regularization effects from architectural modifications such as overparameterization or residual connections can help avoid observational overfitting and reduce the generalization gap.

In this context of research, we introduce a **Sim**ple yet effective framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations. Firstly, we empirically found the image-level frame stack, commonly used in traditional vision-based _model-free_ RL approaches , to be the main factor causing the imbalanced saliency problem. To address this issue, we propose a simple architectural modification of an image encoder, which employs a feature-level frame stack instead of the image level. This involves extracting individual feature maps for each frame from the shallow layers of the encoder, stacking them along channels, and encoding the stacked feature maps through the remaining layers. Since this approach considers solely individual frames during the initial encoding, the agent is trained to focus on spatially salient features in each consecutive frame that are essential for a given task, alleviating the imbalanced saliency problem of Fig. 1(b). Secondly, to address the observational overfitting problem by encouraging the agent to focus on the task object rather than backgrounds, we propose a new data augmentation called shifted random overlay, which is a modified version of random overlay augmentation . This augmentation directly injects background dynamics irrelevant to the given task into the agent during the training phase by interpolating natural images moving in a random direction on each frame. By enabling the agent to implicitly learn to ignore task-irrelevant backgrounds and focus on task-relevant pixels, this approach alleviates the observational overfitting problem of Fig. 1(c). Furthermore, this augmentation enables the agent to adapt to test environments with dynamic perturbations in the surrounding backgrounds.

Figure 1: (a) Average performances on 6 tasks in DMControl-GB. In contrast to other methods with significant performance degradation in Video Hard, our proposed SimGRL demonstrates robust performance across all benchmarks. (b)\(-\)(c) Examples of two problematic phenomena that can cause overfitting in visual RL generalization. The background structures in the red boxes are correlated with the movement of the task object. \(s\) and \(M_{}\) represent the stacked frames and attribution masks, respectively. Attribution masks in this figure were obtained using the critic trained by DrQ .

We verify that these strategies can remarkably improve generalization in challenging environments, achieving state-of-the-art performance without using any additional auxiliary losses or networks.

Furthermore, utilizing the attribution mask \(M_{}\), we introduce novel metrics, called **T**ask-**I**dentification (TID) metrics, consisting of TID score and TID variance, to quantitatively evaluate the discrimination ability on salient regions. With the proposed metrics, we quantitatively analyze the two problematic phenomena in the existing approaches, demonstrating the excellent discrimination ability of SimGRL. Moreover, we show a tendency of positive correlation between the TID score and RL generalization performance, emphasizing the importance of accurately identifying salient features in input images.

Our contributions include the following aspects:

* By utilizing gradient-based attribution masks, we highlight the two core issues of imbalanced saliency and observational overfitting, which hinder the generalization of visual RL for most model-free RL settings. Additionally, we propose TID metrics to measure the discrimination ability of an RL agent on task objects, providing insights into these issues.
* To address these problems, we propose architectural and data regularization methods through a modification to an encoder structure and an introduction of new data augmentation.
* We achieve state-of-the-art performances across video benchmarks of DMControl-GB , DistractingCS , and robotic manipulation tasks .

## 2 Background

Visual RL and GeneralizationWe consider a partially observable Markov decision process (POMDP) problem \(=(,,,,r,)\), where \(\) is state space, \(\) is observation space, \(\) is action space, \(:\) is the transition function that defines the conditional probability distribution \((s_{t+1}|s_{t},a_{t})\) over next states given a state \(s_{t}\) and an action \(a_{t}\) taken at time \(t\), \(r:\) is a reward function, and \([0,1)\) is a discount factor. In the POMDP problem, such as the visual RL, because only the high-dimensional observations \(o_{t}\) can be observable , a state is defined as a sequence of the \(k\) consecutive image frames \(s_{t}=(o_{t-k+1},...,o_{t-1},o_{t})\). Without loss of generality, we will set \(k=3\) for the sake of notational simplicity. RL aims to learn a policy \(:\) that maximizes the expected sum of discounted rewards \(_{}[_{t=0}^{T}^{t}r(s_{t},r_{t})]\). In this work, we focus on the generalization problem to POMDPs \(}=(},},,,r,)\), where the states \(}\) are constructed from the perturbed observations \(}\), and a POMDP \(\) is sampled from the space of POMDPs \(\), \(\).

Deep Q-Learning and Soft Actor-CriticDeep Q-learning  is a common model-free RL algorithm that aims to learn a parameterized state-action value function \(Q_{}(s_{t},a_{t})\), where \(\) is the deep neural network parameters of the Q-function and action is greedily selected as the one with the maximum value \(a_{t}=*{argmax}_{a}Q_{}(s_{t},a)\) at time \(t\). The training of the Q-function is achieved by minimizing a mean squared error of the Bellman residuals:

\[_{(s_{t},a_{t},s_{t+1})}[(Q_{}(s_{t},a_{t})-(r _{t}+_{a^{}}Q_{}^{tgt}(s_{t+1},a^{})))^{2}],\] (1)

where \(\) is the parameters of the target Q network \(Q^{tgt}\) and \(\) is a replay buffer. To increase the stability, the parameter of the target Q network is slowly updated by the exponential moving average (EMA) \(=+(1-)\) with \( 1\). For the continuous action space, rather than the greedy sampling, a parameterized actor \(_{}(s_{t})\) is employed as the policy, where \(\) is the neural network parameters of the actor. Soft Actor-Critic (SAC)  is a common actor-critic algorithm with a state-action value \(Q_{}(s_{t},a_{t})\) and a stochastic policy \(_{}(a_{t}|s_{t})\), and a temperature parameter \(\), which aims to optimize a \(\)-discounted maximum-entropy objective .

In visual RL, a parameterized encoder \(f_{}:^{C H W}^{d}\) is employed to compress the high-dimensional image inputs and shared by both the actor and critic, where \(d\) is a dimension of the encoded feature. Consistent with the prior visual RL approaches [41; 18; 19; 40; 12], we jointly train the encoder with the critic and freeze it during actor updates, where the encoder learns representations for RL tasks by the critic loss. For notational simplicity, we denote all parameters updated by the critic loss as \(\) and the actor parameters as \(\).

Gradient-based Attribution MaskAn attribution map, also known as a saliency map, is designed to visualize the salient pixels in input images for given tasks. A common approach to computing the attribution map is a gradient-based method [30; 3], which indicates how sensitive the task prediction is to perturbations in the input pixels. Consistent with SGQN , we employ guided backpropagation  to compute the attribution map \(M(Q_{},s_{t},a_{t})=(s_{t},a_{t})}{ s_{t }}\), where \(s_{t}\) and \(M(Q_{},s_{t},a_{t})^{C H W}\). Then, the binarized attribution mask \(M_{}(Q_{},s_{t},a_{t})\), referred to as \(\)-quantile attribution mask, is computed by thresholding \(M(Q_{},s_{t},a_{t})\) by the \(\)-quantile, where \(M_{}(Q_{},s_{t},a_{t})_{(i,j,k)}=1\) if the pixel value of \(M(Q_{},s_{t},a_{t})_{(i,j,k)}\) belongs to the \(\)-quantile of highest values for \(M(Q_{},s_{t},a_{t})\), and \(0\) otherwise. We use this attribution mask to investigate the cause of the degradation of the generalization performance of existing methods in challenging environments such as Video Hard . Furthermore, to justify our analysis, we will introduce new metrics that quantitatively measure the ability to identify salient pixels in Section 4.4. Note that we leverage the attribution mask only for analysis without involving it in the training process.

## 3 Pitfalls within Conventional Practices in Visual RL Generalization

The generalization ability of RL agents is often degenerated in challenging environments characterized by dynamic perturbations and different structures compared to the training environment. In this section, we investigate the causes of the overfitting to the training environment observed in existing visual RL methods on the DMControl Generalization Benchmark (DMControl-GB) .

Conventional practices used to train the visual RL agent for generalization [13; 2; 42; 4] include frame stack and data augmentation. First, to reflect the temporal structure of the input state, Q-value is predicted using stacked frames [25; 18; 41; 40; 19] instead of a single image as follows:

\[q_{}(s_{t},a_{t})=Q_{}(f_{}([o_{t-2},o_{t-1},o_{t}]),a_{t}), s_{t}=(o_{t-2},o_{t-1},o_{t}),\] (2)

where \(\) is a neural network parameter, \(f_{}()\) is a convolutional neural network (CNN) image encoder, \(Q_{}(,)\) is a critic head, and \([]\) is a concatenation operator along a channel dimension. Subsequently, to learn the representations robust to visual perturbations, data augmentation to the visual observations is leveraged when training the encoder. Specifically, \(s_{t}\) in Eq. (2) can be substituted with \((s_{t})=((o_{t-2}),(o_{t-1}),(o_{t})),\ \), where \(()\) is a sampled transformation function from the transformation space \(\). However, we empirically found that these practices can cause the saliency imbalance between the stacked frames and easily fall into observational overfitting, resulting in performance degradation in unseen challenging environments.

Pitfall 1: Imbalanced SaliencyFig. 2(a) illustrates an example of the imbalanced saliency by SVEA  in the 'Cartpole' task. As described in Section 2, the attribution masks were acquired by thresholding the gradient value of the critic function with respect to input image frames . In this example, an RL agent recognizes the regions of the salient objects in the two latest frames as having high saliency across all stacked frames. This makes the agent misidentify as salient pixels of \(o_{t-2}\) its background parts corresponding to the positions of the 'Cartpole' object in \(o_{t-1}\) and \(o_{t}\). As a result, \(o_{t-2}\) provides the agent with redundant information that is unnecessary for the task, leading to overfitting to training data [45; 5]. In a test environment with complicated backgrounds, the background information that is considered salient from \(o_{t-2}\) may act as noise that interferes with the decision-making process of the policy. As the latest frames are more closely related to the subsequent decision-making process, the agent tends to identify spatially consistent saliency maps based on these recent frames, leading to imbalanced saliency maps. We hypothesize that this phenomenon occurs because the encoder extracts features throughout concatenated images along the channel as in Eq. (2), forcing it to capture the same spatial saliency for all stacked images.

Pitfall 2: Observational OverfittingObservational overfitting  can arise when certain background elements move in synchronization with task-relevant objects. It is prone to occur particularly when the motion of the task object is not substantial, such as in the 'Cheetah, Run' task. Although data augmentation is leveraged during training, we found that observational overfitting can still occur. Furthermore, as the same augmentation is applied uniformly to all consecutive frames, it does not effectively prevent the agent from erroneously focusing on background elements. For instance, Fig. 2(b) shows an example of observational overfitting that occurs in the 'Cheetah, Run' task by SVEA, where the critic assigns more significant saliency to the ground than to the 'Cheetah' object. In a test environment, if there is a lack of correlation between objects and backgrounds present in the training environment, the agent may struggle to make accurate decisions, thereby leading to challenges in generalization.

## 4 Method

To address the problems within the conventional practices used for the generalization of visual RL, we propose a **S**imple framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations.

### Feature-Level Frame Stack

In Section 3, we identified the encoder structure that simultaneously encodes the stacked frames at the image level as the cause of the imbalanced saliency. We address this issue by introducing an architectural regularization strategy to slightly modify the encoder structure. This architectural modification involves encoding each frame individually and then encoding the stacked feature maps. To keep the computational cost almost unchanged, we partition the original encoder into two segments instead of adding new layers. Then, Eq. (2) can be modified as follows:

\[q_{}(s_{t},a_{t})=Q_{}(f_{}^{2}([f_{}^{1}(o_{t-2}),f_{ }^{1}(o_{t-1}),f_{}^{1}(o_{t})]),a_{t}), s_{t}=(o_{t-2},o_{t-1 },o_{t}),\] (3)

where \(f_{}^{1}()\) is an image encoder to encode individual frames, and \(f_{}^{2}()\) is a feature encoder for the stacked feature maps. While \(f_{}^{2}\) encodes both spatial and temporal structures from inputs, \(f_{}^{1}\) conducts only spatial encoding of each frame. This simple modification allows the critic to be implicitly trained to focus on the spatially salient pixels of individual frames, enabling the agent to distinctly identify the salient pixels of each frame. In our experiments, we will verify that this simple modification of the encoder is highly beneficial for generalization, especially in challenging test environments.

### Shifted Random Overlay Augmentation

The random overlay augmentation  used in existing approaches augments input images through linear interpolation with a natural image \(^{C H W}\) randomly sampled from the Places  dataset that contains 1.8M diverse scenes:

\[^{RO}(s;)=(+(1-)o_{1},..., +(1-)o_{n}),,\] (4)

Figure 2: Examples of attribution masks and masked frames. Compared to SVEA that falls into the imbalanced saliency and observation overfitting in the ‘Cartpole, Swingup’ and ‘Cheetah, Run’ tasks, respectively, the proposed SimGRL accurately identifies the true salient pixels even in challenging ‘Video Hard’ test environments of DMControl-GB. We provide further examples of the attribution masks and masked salient regions for various environments and algorithms in Appendix F.4.

where \(s\) is a state that consists of a sequence of images \((o_{1},...,o_{n})\), \(o_{i}^{C H W},\)\(\) is a dataset and a common choice for the interpolation coefficient is \(=0.5\). The original random overlay augmentation uses the same natural image \(\) for all stacked frames. Contrarily, we introduce a shifted random overlay (SRO) augmentation to inject the task-irrelevant dynamics into the training images, which is depicted in Fig. 3. Considering a maximum shift length \(l\) for each shifting and a stacked frame number \(n\), this method first samples a natural image \(^{C}\) from the dataset \(\), where \((,)=(H+2(n-1)l,W+2(n-1)l)\). Then, we crop \(n\) shifted patches \(_{i}^{c}^{C H W}\) from \(\) to augment each frame in a shifted way. Given an upper left corner coordinate \((h_{1},w_{1})\) of \(H W\) sized center crop at the center of \(\), the coordinates \((h_{i},w_{i})\) of the upper left corner of \(_{i}^{c}\) are selected as follows:

\[(h_{i},w_{i})=(h_{1}+dh(i-1),w_{1}+dw(i-1)), i=1,...,n,\] (5)

where \(dh,dw\{-l,l\}\). Finally, using the cropped images \(_{i}^{c}\) from \((h_{i},w_{i})\) in \(\), the shifted random overlay augmentation is defined as:

\[^{SRO}(s;l,)=(_{1}^{c}+(1-)o_{1},..., _{n}^{c}+(1-)o_{n}),\] (6)

where we adopt \(=0.5\). This augmentation provides two implicit advantages when training RL agents. Firstly, the inclusion of background elements in motion, independent of the task, enables the RL agent to perceive that rewards are solely associated with changes in the genuine task object. Consequently, the agent is trained to concentrate on the task object, disregarding the movement of background elements, thereby alleviating the issue of observational overfitting. Secondly, this augmentation method generates training data akin to real environments with dynamic backgrounds, enabling the agent to be robust in such conditions and enhancing generalization capability.

### Simple Framework for Generalization in Visual RL (SimGRL)

Built on the SVEA  framework that leverages the data-mixing strategy between weak and strong data augmentations and computes the target Q-value using clean images for stability in SAC, we propose a Simple framework for Generalization in visual RL (SimGRL) under dynamic scene perturbations, which integrates the proposed two strategies. For the strong augmentation, we utilize the shifted random overlay instead of the original random overlay without shifting while the random shift  is employed as the weak augmentation. Considering that the clean images \(s_{t}\) and \(s_{t+1}\) already have weak augmentation, _i.e._, random shift, applied by default, the critic loss for SimGRL is defined as follows:

\[_{Q}()=_{(s_{t},a_{t},r_{t},s_{t+1})}[(q_{}(s_{t},a_{t})-q^{tgt})^{2}+(1-)(q_{}(^{SRO}( s_{t};l,),a_{t})-q^{tgt})^{2}],\] (7)

where \(q_{}(,)\) is computed by Eq. (3), \(q^{tgt}=r_{t}+ q_{}(s_{t+1},a^{})\), \(a^{}_{}(|f_{}(s_{t+1}))\), and \(\) is the parameter of the target network that is updated by the exponential moving average (EMA). The strong augmentation \(^{SRO}(;,)\) is the shifted random overlay in Eq. (6). Consistent with the SVEA, we adopt the data-mixing coefficient \(=0.5\). When training the actor \(_{}\), we leverage solely clean images like the existing methods [12; 13]. The overall framework of SimGRL is illustrated in Fig. 4 and the algorithm is summarized in Appendix C.

Figure 3: Shifted random overlay (SRO) augmentation for data regularization. To inject random dynamics into the backgrounds of RL input images, we generate multiple cropped patches in a shifted manner from a sampled natural image and interpolate them to augment the input images.

### **Task-Identification (TID) Metrics**

To quantitatively evaluate the capability to identify task-relevant objects in each stacked frame as salient, we introduce novel metrics, referred to as Task-Identification (TID) score and variance, based on the \(\)-quantile attribution mask \(M_{}(Q_{},s_{t},a_{t})\) described in Section 2.

TID ScoreTID score measures how much the model identifies the task object's pixels across stacked frames, which is defined as:

\[_{S}=}}{N_{obj}}}}{N_{ M}}}=})^{2}}{N_{obj} N_{M}}},\] (8)

where \(N_{obj}\) is the number of task object's pixels in input images, \(N_{M}\) is the number of pixels in attribution masks \(M_{}\), \(N_{obj_{M}}\) is the number of task object's pixels included in \(M_{}\), and \(\) is a quantile value for thresholding the attribution map. Note that all numbers consisting of the TID score are counted across the full consecutive frames. In Eq. (8), the first term quantifies the model's ability to identify the pixels of the task object, while the second term quantifies how accurately the model identifies the task object's pixels. These two terms trade-off depending on the size of the quantile value and are upper-bounded by 1, thus leading to an upper-bounded TID score by 1. With a model that perfectly identifies all task pixels, this upper-limit value can only be achieved along with an optimal \(\) value, which is the quantile of the number of the task object's pixels in frames. This \(\) value can be computed by \(=1-}{(n C H W)}\), where \(n\) is the number of frame stack. We explain the impact of the \(\) value on this TID score in Appendix F.2.

TID VarianceTID variance measures how discriminatively the model distinguishes the task object's pixels in each frame, which is defined as:

\[_{Var}=[100(TID_{S}^{1},TID_{S}^{2},...,TID_{S}^ {n})],\] (9)

where \(TID_{S}^{i}=}^{i})^{2}}{N_{obj_{M}}^{i} N_{M}^{i }}}\), \(N_{obj}^{i}\), \(N_{M}^{i}\), and \(N_{obj_{M}}^{i}\) are individually counted at each frame. Since each \(TID_{S}{}^{i}\) is upper-bounded by 1, the values of \(TID_{Var}\) become too small for meaningful comparison. Therefore, we multiply \(TID_{S}{}^{i}\) by 100 to obtain a variance with a more appropriate scale for comparison.

## 5 Experiments

In this section, we present the experimental results of SimGRL on the DMControl-GB  video benchmarks ('Video Easy' and 'Video Hard') at \(500\) simulated training frames to demonstrate the generalization capability under dynamic scene perturbations. To tackle the vision-based control tasks with continuous action space, we employ the SAC  as a backbone RL algorithm, and compare SimGRL against current state-of-the-art

Figure 4: Overview of the **Simple** framework for **G**eneralization in visual **RL** (SimGRL) under dynamic scene perturbations. Differences from SVEA are marked in red.

Figure 5: Experimental setup. We evaluated the zero-shot performances for test environments with dynamic background perturbations.

methods for visual RL generalization including RAD , DrQ , SODA , SVEA , TLDA , SGQN , EAR , and CG2A . For weak augmentation, RAD and SODA utilize random crop, while the others employ random shift, except for SAC, which does not use any data augmentations. In addition, all competitors leverage the original random overlay augmentation without shifting for strong augmentation except for RAD and DrQ, which use only weak augmentations. As default for SimGRL, we used the random shift augmentation  and denoted the images with only this weak augmentation as clean images with the data distribution of the training data. Additionally, we employed the proposed shifted random overlay (SRO) for strong augmentation in SimGRL and denoted images together with this augmentation as augmented ones. Implementation details are described in Appendix A, and further experimental results on other benchmarks of Distracting Control Suite  and robotic manipulation  are provided in Appendices B.7 and B.8, respectively.

### Results on DMControl-GB

We evaluated the zero-shot generalization performance on 6 tasks in 'Video Easy' and 'Video Hard' benchmarks from DMControl-GB . As illustrated in Fig. 5, the easy version shares certain structures with the training environment such as the ground and shadow, while the hard version shares nothing other than the agent's object by replacing all backgrounds with distracting videos. In Table 1, SimGRL demonstrates state-of-the-art performance in 4 out of 6 tasks in the Video Easy benchmark and achieves comparable performance in the remaining two tasks, including 'Walker, Walk' and 'Cheetah, Run'. On the other hand, our approach shows outstanding performance in all 6 tasks at the Video Hard level, where SimGRL achieves performance gain by 15\(\%\) on average compared to SGQN. Specifically, SimGRL outperforms existing methods by a significant margin in 'Cartpole, Swingup' and 'Cheetah, Run', which were previously difficult to solve. We suggest that the reason for this lies in the fact that 'Cartpole, Swingup' requires detailed identification of salient objects, while 'Cheetah, Run' needs to address the observational overfitting. Both issues are mitigated in SimGRL. We provide the training curves in Appendix B.6.

Computational EfficiencyOur method has the advantage of improving generalization performance without any extra models or auxiliary losses. We compare SimGRL with SGQN , the existing state-of-the-art method in the Video Hard benchmark. In experiments, SimGRL can achieve a throughput of 9.54 FPS, which is 1.55\(\) efficient compared to SGQN's 6.16 FPS on a single NVIDIA TITAN RTX GPU. Fig. 6 shows the training curves for the zero-shot test performances over the wall-clock training time, where the averaged performances across the 6 tasks in Table 1 are presented. In this figure, SimGRL requires 44\(\%\) less wall-clock training time than SGQN to reach the same 500K frames for training, demonstrating the high computational efficiency of SimGRL. It is worth noting that SimGRL has achieved considerable enhancements in both training efficiency and test

    & DMControl-GB & SAC & RAD & DrQ & SODA & SVEA & TLDA & SGQN & EAR & CG2A & SimGRL & \(\) \\    } & Walker, Walk & 24\(\)165 & 608\(\)92 & 374\(\)21 & 768\(\)38 & 389\(\)71 & 868\(\)63 & 910\(\)24 & 913\(\)38 & **918\(\)92** & 910\(\)21 & -8 (0.8\(\%\)) \\  & Walker, Sund & 389\(\)11 & 879\(\)64 & 926\(\)30 & 955\(\)13 & 961\(\)8 & 973\(\)65 & 955\(\)9 & 970\(\)23 & 958\(\)66 & **973\(\)4** & 0 \\  & Ball in Cup, Catch & 192\(\)157 & 363\(\)158 & 380\(\)188 & 875\(\)56 & 871\(\)106 & 855\(\)46 & 950\(\)24 & 911\(\)40 & 963\(\)28 & **964\(\)7** & 4 (0.1\(\%\)) \\  & Carridge, Swingup & 399\(\)60 & 473\(\)54 & 459\(\)81 & 758\(\)62 & 722\(\)27 & 671\(\)57 & 761\(\)57 & 761\(\)26 & 88\(\)78 & **981\(\)38** & 4 (0.6\(\%\)) \\  & Finger, Spin & 206\(\)169 & 516\(\)113 & 599\(\)62 & 695\(\)97 & 808\(\)33 & 744\(\)18 & 956\(\)28 & 771\(\)51 & 912\(\)90 & **983\(\)2** & 4 (27\(\%\)) \\  & Cheetah, Run & 73\(\)18 & 153\(\)7 & 270\(\)16 & 268\(\)10 & 251\(\)17 & **336\(\)57** & 289\(\)35 & 334\(\)56 & 314\(\)49 & 317\(\)16 & -19 (6\(\%\)) \\    } & Walker, Walk & 212\(\)247 & 80\(\)10 & 211\(\)52 & 312\(\)32 & 33\(\)65 & 292\(\)13 & 739\(\)21 & 833\(\)69 & 67\(\)18 & **73\(\)31** & -43 (4\(\%\)) \\  & Walker, Sund & 221\(\)57 & 229\(\)45 & 252\( compared to the SOTA method, regardless of the typical trade-offs in terms of efficiency and performance.

Ablation StudyTo verify the effectiveness of the proposed regularizations, we compare the ablation variants with the SVEA baseline. Fig. 7 shows training curves for zero-shot test performances averaged across the 6 tasks of DMControl-GB at the Video Hard benchmark as a function of the number of stepped training frames. This result indicates that the additions of each regularization to SVEA can remarkably improve the generalization ability of the model. In particular, we emphasize that each of the two regularizations leads to sufficient improvement, where all variants of SimGRL achieve, on average, nearly 82\(\%\) better performance than SVEA. Full experimental results for each task are provided in Appendix B.1 and further in-depth discussions on the effectiveness of each regularization are described in Appendix B.2.

### Analysis with TID Metrics

To analyze the two potential overfitting problems, namely imbalanced saliency and observational overfitting, we evaluate the TID metrics, including the TID score and variance. For clarity, we compare three methods of DrQ , SVEA , and SimGRL as the former serve as baselines for the latter. We analyze two representative tasks, namely 'Cartpole, Swingup' and 'Cheetah, Run', where each problem is prominently observed. Full evaluations for all algorithms and tasks are provided in Appendix F.3. The left plot in Fig. 8 depicts that the overall distribution of training scores of the TID metrics is divided into three regions. In this plot, both DrQ and SVEA exhibit results divided into two regions, either 'low score and low variance' or'middle score and high variance', depending on the task, indicating that different phenomena appear in the two tasks. The 'low score and low variance' region can be interpreted as observational overfitting, which does not identify the correct object in all frames. On the other hand, the'middle score and high variance' area suggests that the correct object is accurately recognized only in certain frames, implying imbalanced saliency. We note that although SVEA  incorporates a data-mixing strategy with strong augmentation to enhance the generalization capability of DrQ , both problems are still observed with only minor improvements over DrQ. In contrast, SimGRL reports comparatively high scores and low variances for both tasks, indicating that it can effectively identify task-relevant objects regardless of the type of task. The middle plot shows a positive correlation between training and test TID scores at Video Hard from DMControl-GB . This suggests that a high discrimination ability during training can result in a high discrimination ability during testing. Finally, the right plot shows the generalization gaps, which are computed by the performance differences between training and testing, against the TID scores. This suggests that a high discrimination ability of the task object can lead to improved generalization capability.

Figure 8: (Left to Right) Plots for the training TID variance, test TID scores, and generalization gap against training TID score.

Figure 7: Training curves for ablation variants.

Related Works

The field of domain generalization for RL has garnered considerable attention in recent years where various approaches aim to enhance the robustness of policies against visual changes. A promising approach is adapting the policy to a test domain. For example, PAD  suggests adopting a self-supervised task to obtain a free training signal during deployment. On the other hand, one method, proposed in , involves using randomly simulated RGB images. Similarly,  train domain-adaptive policies by randomizing dynamics during the training phase. Several works explore data augmentation techniques to improve policy generalization capacity [19; 40; 29]. For instance, RAD  and DrQ  achieve significant improvement through random crop and shift, while DrAC  automatically identifies the most effective augmentation with regularization terms for the policy and value function. Instead of relying solely on augmented data for policy learning, SODA  aims to decouple augmentation from policy learning by using soft-augmented data for policy learning and strong-augmented data for auxiliary representation learning. In a recent development, SVEA  designs a stabilized Q-value estimation framework to address instability issues under strong data augmentation in off-policy RL, while DBC  uses bisimulation metrics to learn a representation that disregards task-irrelevant information. VAI  extracts a universal visual foreground mask to provide an invariant observation to RL. Similarly, several methods leverage salient masks to encourage the agent to focus on import pixels. For example, TLDA  attempts to only augment the task-irrelevant pixels using masks obtained from the Lipschitz constant of the policy while SGQN  proposes saliency-guided self-supervised learning using a gradient-based saliency mask. EAR  attempts to learn environment-agnostic representations to enhance the robustness of policies against visual perturbations. On the other hand, several methods such as TIA , DRIBO , and RePo , leverage model-based RL through a recurrent state-space model (RSSM) [10; 9] to explicitly learn latent representations that focus on task-relevant features while discarding task-irrelevant ones. Our work focuses on improving the generalization ability of the RL agent based on implicit regularization approaches.

## 7 Conclusion

In this paper, we identified two key issues that degrade generalization in vision-based RL, particularly under dynamic scene perturbations, by employing a gradient-based attribution mask. To resolve these issues, we proposed a simple framework involving an architectural modification to the encoder and a new data augmentation. The proposed SimGRL algorithm has achieved state-of-the-art results on Video Hard environments in DMControl-GB, outperforming existing methods that have yet to address these challenges. Additionally, we introduced TID metrics to quantitatively evaluate the ability to discriminate task-relevant objects of RL agents. Using these metrics, we demonstrated that improving the identification of task-relevant objects can enhance the generalization capability of the vision-based RL agent.