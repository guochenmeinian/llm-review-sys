# Cascade of phase transitions in the training of Energy-based models

Dimitrios Bachtis\({}^{1}\)

Giulio Biroli\({}^{1}\)

Aurelien Decelle\({}^{2,3}\)

 Beatriz Seoane\({}^{2,3}\)

\({}^{1}\)Laboratoire de Physique de l'Ecole Normale Superieure, ENS,

Universite PSL, CNRS, Sorbonne Universite, Universite Paris Cite, F-75005 Paris, France.

\({}^{2}\)Departamento de Fisica Teorica I, Universidad Complutense, 28040 Madrid, Spain.

\({}^{3}\)Universite Paris-Saclay, CNRS, INRIA Tau team, LISN, 91190, Gif-sur-Yvette, France.

###### Abstract

In this paper, we investigate the feature encoding process in a prototypical energy-based generative model, the Restricted Boltzmann Machine (RBM). We start with an analytical investigation using simplified architectures and data structures, and end with numerical analysis of real trainings on real datasets. Our study tracks the evolution of the model's weight matrix through its singular value decomposition, revealing a series of phase transitions associated to a progressive learning of the principal modes of the empirical probability distribution. The model first learns the center of mass of the modes and then progressively resolve all modes through a cascade of phase transitions. We first describe this process analytically in a controlled setup that allows us to study analytically the training dynamics. We then validate our theoretical results by training the Binary-Binary RBM on real datasets. By using datasets of increasing dimension, we show that learning indeed leads to sharp phase transitions in the high-dimensional limit. Moreover, we propose and test a mean-field finite-size scaling hypothesis. This shows that the first phase transition is in the same universality class of the one we studied analytically, and which is reminiscent of the mean-field paramagnetic-to-ferromagnetic phase transition.

## 1 Introduction

In recent years, we have witnessed impressive improvements of unsupervised models capable of generating more and more convincing artificial samples [1; 2; 3]. Although energy-based models  and variational approaches  have been in use for decades, the emergence of generative adversarial networks , followed by diffusion models , has significantly improved the quality of outputs. Generative models are designed to learn the empirical distribution of datasets in a high-dimensional space, where the dataset is represented as a Dirac-delta pointwise distribution. While different types of difficulties are encounter when training these models, there is a general lack of understanding of how the learning mechanism is driven by the considered dataset. This article explores the dynamics of learning in neural networks, focusing on pattern formation. Understanding how this process shapes the learned probability distribution is complex. Previous studies [8; 9] on the Restricted Boltzmann Machine (RBM)  showed that the singular vectors of the weight matrix initially evolve to align with the principal directions of the dataset, with similar results in a 3-layer Deep Boltzmann Machine . Additionally, an analysis using data from the 1D Ising model explained weight formation in an RBM with a single hidden node as a reaction-diffusion process . The main contribution of this work is to demonstrate that the RBM undergoes a series of second-orderphase transitions during learning, each corresponding to the acquisition of new data features. This is shown theoretically with a simplified model and on correlated patterns; and confirmed numerically with real datasets, revealing a progressive segmentation of the learned probability distribution into distinct parts and exhibiting second order phase transitions.

## 2 Related work

The learning behavior of neural networks has been explored in various settings. Early work on deep linear neural networks demonstrated that even simple models exhibit complex behaviors during training, such as exponential growth in model parameters [13; 14]. Using singular value decomposition (SVD) of the weight matrix, researchers revealed a hierarchical learning structure with rapid transitions to lower error solutions. Linear regression dynamics later showed a connection between the SVD of the dataset and the double-descent phenomenon . Similar dynamics were found in Gaussian-Gaussian RBMs , where learning mechanisms led to rapid transitions for the modes of the model's weight matrix. In this context, the variance of the overall distribution is adjusted to that of the principal direction of the dataset, while the singular vectors of the weight matrix are aligned to that of the dataset. Unlike linear models, non-linear neural-networks, supervised or unsupervised ones, can not exhibit partition of the input's space. Yet, linear model in general can not provide a multimodal partition of the input space, should it be in supervised or unsupervised context, at difference with non-linear ones.

It was then shown that the most common binary-binary RBMs exhibit very similar patterns at the beginning of learning, transitioning from a paramagnetic to a condensation phase in which the learned distribution splits into a multimodal distribution whose modes are linked to the SVD of the weight matrix . The description of this process motivated the use of RBMs to perform unsupervised hierarchical clustering of data [16; 17]. The succession of phase transitions had been previously observed in the process of training a Gaussian mixture [18; 19; 20], and in the analysis of teacher-student models using statistical mechanics [21; 22]. The latter cases are easier to understand analytically due to the simplicity of the Gaussian mixture. Nevertheless, the learned features are somewhat simpler, as they are mainly represented by the means and variances of the individual clusters. Recently, sequences of phase transitions have been used to explain the mechanism with which diffusion model are hierarchically shaping the mode of the reverse diffusion process [23; 24; 25] and due to a spontaneous broken symmetry  after a linear phase converging toward a central fixed-point. The common observation is that the learning of a distribution is, in many cases, obtained by a succession of creation of modes performed through a second order process where the variance in one direction first grow before splitting into two parts, and then the mechanism is repeated. This procedure in particular demonstrate a hierarchical splitting, where the system refined at finer and finer scale of features as it adjust its parameters on a given dataset.

## 3 Definition of the model

An RBM is a Markov random field with pairwise interactions on a bipartite graph consisting of two layers of variables: visible nodes (\(=\{v_{i},i=1,,N_{}\}\)) representing the data, and hidden nodes (\(=\{h_{j},j=1,,N_{}\}\)) representing latent features that create dependencies between visible units. Typically, both visible and hidden nodes are binary (\(\{0,1\}\)), though they can also be Gaussian  or other real-valued distributions, such as truncated Gaussian hidden units . For our analytical computations, we use a symmetric representation (\(\{ 1\}\)) for both visible and hidden nodes to avoid handling biases. However, in numerical simulations, we revert to the standard (\(\{0,1\}\)) representation. The energy function is defined as follows:

\[E[,;,,]=-_{ia}v_{i}W_{ia}h_{a}-_{i}b_{i} v_{i}-_{a}c_{a}h_{a},\] (1)

with \(\) the weight matrix and \(\), \(\) the visible and hidden biases, respectively. The Boltzmann distribution is then given by \(p[,|,,]=Z^{-1}(-E[,;,,])\) with \(Z=_{\{,\}}e^{-E[,]}\) being the partition function of the system. RBMs are usually trained using gradient ascent of the log likelihood (LL) function of the training dataset \(=\{^{(1)},,^{(M)}\}\), the LL is then defined as

\[(,,|)=M^{-1}_{m=1}^{M} p(=^{(m)}|,,)=M^{-1}_{m=1}^{M}_{\{\}} e^{-E[^{(m)},;,,]}- Z\]The computation of the gradient is straightforward and made two terms: the first accounting for the interaction between the RBM's response and the training set, also called _postive term_, and same for the second, but using the samples drawn by the machine itself, also called _negative term_. The expression of the LL gradient w.r.t. all the parameters is given by

\[}{ w_{ia}}= v_{i}h_{a}_{ }- v_{i}h_{a}_{},\ \ }{ b_{i}}= v_{i}_{ }- v_{i}_{}\ \ \ \ \ \ }{ c_{a}}= h_{a}_{}- h_{a} _{},\] (2)

where \( f(,)_{}=M^{-1}_{m} _{\{\}}f(^{(m)},)p(|^{(m)})\) denotes an average over the dataset, and \( f(,)_{}\), the average over the Boltzmann distribution \(p[,;,,]\). Most of the challenges in training RBMs stem from the intractable negative term, which has a computational complexity of \((2^{(N_{},N_{})})\) and lacks efficient approximations. Typically, Monte Carlo Markov Chain (MCMC) methods are used to estimate this term, but their mixing time is uncontrollable during practical learning, leading to potentially out-of-equilibrium training .

This work focuses on the initial phase of learning and the emergence of modes in the learned distribution from the gradient dynamics given by Eq. (2). In the following section, we first analytically characterize the early dynamics in a simple setting, showing how it undergoes multiple second-order phase transitions. We then numerically investigate these effects on real datasets.

## 4 Theory of learning dynamics for simplified high-dimensional models of data

We develop the theoretical analysis by focusing on simplified high-dimensional probability distributions that concentrate around different regions, or _lumps_, in the space of visible variables. Our aim is to analyze how the RBM learns the positions of these lumps, which represent, in a simplified setting, the features present in the data. In order to simplify the analysis, we will consider the Binary-Gaussian RBM (BG-RBM) defined below, yet the same results can be derived for the Binary-Binary RBM (BB-RBM) as shown in the SI B.

### Learning two features through a phase transition

We consider the following simplified setting: we will be using \(v_{i}= 1\) visible nodes, Gaussian hidden nodes and put the biases to zero \(=0\) and \(=0\). As a model of data, we consider a Mattis model with a preferred direction \(\) for the ground state, following the distribution

\[p_{}()=}}( }}(_{i=1}^{N_{}}_{i}v_{i}) ^{2}),\]

where \(=1/T\) is the inverse temperature and \(_{i}= 1\) represents a pattern encoded in the model as a Mattis state [30; 31]. In a Mattis model, \(\) represents a preferred direction of the model for large values of \(\), and in this simple case (with only one pattern) there is no need to specify its distribution as long as its elements are \( 1\)1. The Mattis model presents a high-temperature phase with a single mode centred over zero magnetization \(m=N_{}^{-1}_{i}_{i} v_{i}=0\) for \(<_{c}\) (where \(.\) is the average w.r.t. the Boltzmann distribution) while in the low-temperature regime, \(>_{c}\), the model exhibits a phase transition between two symmetric modes \(m= m_{0}()\) (\(_{c}=1\)). Henceforth, we shall focus on the regime \(>_{c}\) where the data distribution is concentrated on two lumps. From the analytical point of view, we can compute all interesting quantities in the thermodynamic limit \(N_{}\). In order to keep the computation simple, we will characterize here the dynamics of the system when performing the learning using a BG-RBM  with one single hidden node. In our setting we assume that the distribution of the hidden node is centered in zero (i.e. there is no hidden bias) and that the variance is \(_{}^{2}=1/N_{}\) (we discuss the reason for the scaling in SI A). The distribution is then

\[p_{}(,)=}} (_{i}v_{i}hw_{i}-N_{}}{2}),\ p_{}( )=[v_{i}w_{i})^{2}}{2N_{ }}].\]

Using this model for the learning, the time evolution of the weights is given by the gradient. With BG-RBM we have that \( v_{i}h_{}=N_{}^{-1}_{j}w_{j} v_{i} v_{j}_{}\) where the last average is taken over a distribution \(p_{}()\). We can now easily compute the positive and negative term of the gradient w.r.t. the weight matrix. For the positive term we obtain that \( v_{i}v_{j}_{}=_{i}_{j}m^{2}\) where \(m=\). The negative term can also be computed in the thermodynamic limit \( v_{i}v_{j}_{}=(h^{*}w_{i})(h^{*}w_{j})\) with \(h^{*}=_{k}w_{k}(h^{*}w_{k})\). If we take the limit of a very small learning rate, we can convert the parameter update rule using the gradient into a time differential equation for the parameters of the RBM, where \(t\) is the learning time:

\[}{dt}=[}}_{i}_{k}_{ k}w_{k}m^{2}-h^{*}(h^{*}w_{i})],\] (3)

with \(\) the learning rate 2. We can analyze two distinct regimes for the dynamics. First, assuming that the weights are small at the beginning of the learning, we get that \(h^{*}=0\). We can then solve the Eq. (3) in this regime obtaining the evolution of the weights toward the direction \(\) by projecting the differential equation on this preferred direction. Defining \(U_{}=N^{-1/2}_{i}_{i}w_{i}\), we obtain

\[}}{dt}=m^{2}U_{},U_{}=U_{}^{0}e^{m^{2}t}.\]

This illustrates that the weights are growing in the direction of \(\) while the projection on any orthogonal direction stays constant. As the weights grow larger, the solution for \(h^{*}\) will depart from zero. Then the correlation between the RBM visible variables starts to grow

\[ v_{i}v_{j}_{} dhh^{2}w_{i}w_{ j}(-}h^{2}}{2}+_{k}w_{k}^{2}}{2} )=w_{i}w_{j}}(1-_{k}w_{k}^{2}/N_{})},\]

which means that the susceptibility \(=_{i,j}_{j}_{i} v_{i}v_{j}_{}\), that is, the response of the system w.r.t. an external perturbation, diverges when \(N_{}^{-1}_{k}w_{k}^{2} 1\), thus exhibiting a _second order phase_ transition during the learning. Interestingly, \(\) diverges as a a power law with a (critical) exponent \(=1\) (where \(N_{}^{-1}_{k}w_{k}^{2}\) plays here then the role of the inverse temperature in the standard physical models) thus corresponding to the mean-field universality class . Finally, we can study the regime where the weights are not small. In that case, we can first observe that the evolution of the directions orthogonal to \(\) cancel when the weights \(\) align totally with the \(\) at the end of the training. Finally, taking \(w_{i}=_{i}w,\) the gradient projected along \(\) at stationarity imposes

\[wm^{2}=h^{*}(h^{*}w)w=h^{*}=.\]

We confirm the main results of this section numerically in Fig. 1, showing they hold accurately even for moderate values of \(N_{}\). The sum of the weights grows exponentially, following the magnetization squared (considering the learning rate), and the weights align with the direction \(\), while the norm of the weight vector converges towards \(\). Additional analysis details and extended computations for the binary-binary RBM case, which is slightly more involved, are provided in the SI.

### Learning multiple features though a cascade of phase transitions

We consider now the case in which the data are characterized by more than two features. For concreteness, we focus on the case in which the data is drawn from the probability distribution of the Hopfield model  with two patterns \(^{1}\) and \(^{2}\), using the Hamiltonian \(_{H}[]=-}}_{a=1}^{2}( _{i=1}^{N_{}}_{i}^{a}v_{i})^{2}\). The generalization to a larger (but finite) number of patterns is straightforward. Following  we consider the case in which the patterns are correlated and defined as: \(^{1}=^{1}+^{2}\) and \(^{2}=^{1}-^{2}\); \(^{1}\) is a vector whose first \(N_{}\) components are equal to \( 1\) with equal probability, and the remaining ones are zero (\(0<<1\)). Whereas \(^{2}\) is a vector whose last \(N_{}\) components are equal to \( 1\) with equal probability, and the remaining ones are zero. When \(T<1-\) this model is in a symmetry broken phase in which the measure is supported by four different lumps centred in \(^{1}\) and \(^{2}\). Analogously to what was done previously, we now consider a BG-RBM with a number of hidden nodes equal to the number of patterns where again both hidden nodes are centred in zero and have variance \(_{h}^{2}=1/N_{}\). The Hamiltonian is then given by \([,]=-_{ia}v_{i}h_{a}w_{ia}+_{a}h_{a}^{2}N_{ }/2\), which corresponds to a Hopfield model  with patterns \(^{1}\) and \(^{2}\). The analysis presented in the previous section can be generalized to this case (see SI for more details) and one finds the dynamical equations for the evolution of the patterns:

\[^{a}}{dt}=}}_{j} v_{i}v_{j} _{}w_{j}^{a}-}}_{j} v_{i}v_{ j}_{}w_{j}^{a}\] (4)As shown in the SI C, \( v_{i}v_{j}_{}=r^{2}_{i}^{1}_{j}^{1}+p^{2}_{i }^{2}_{j}^{2}\) where \(r,p\) are a function of \(\) (and \(^{-1}=T<1-\), \(r>p\)). Note that this factorization of the correlation matrix is precisely its spectral eigendecomposition, which means that \(^{1}\) and \(^{2}\) are nothing but the principal directions of the standard principal component analysis (PCA). At the beginning of the training dynamics the RBM is in its high-temperature disordered phase, hence the second term of the RHS of Eq. (4) is zero. The weights \(^{1}\) and \(^{2}\) have therefore an exponential growth in the directions \(^{1}\) and \(^{2}\), whereas the other components do not evolve. If the initial condition for the weights is very small, as we assume for simplicity, one can then write:

\[^{a}(t)=}{()}}e^ {r^{2}()t}^{1}+^{a}}{ ()}}e^{p^{2}()t}^{2} a=1,2\ \ \,\]

where we have neglected the small remaining components; \(z^{a}\) and \(^{a}\) are the projections of the initial condition along the directions \(^{1}\) and \(^{2}\). Since \(r>p\), on the timescale \(( N_{ v})/(r^{2}(1+))\) the component of the \(^{a}s\) along \(^{1}\) becomes of order one whereas the one over \(^{2}\) is still negligible. In this regime, the RBM is just like the one we consider in the previous section with a single pattern: the system will align with a single pattern that is given in that case by \(^{1}^{1}+^{2}\), and it has a phase transition at the time \(t_{I}\):

\[[]t_{I}}}{N_{ v}}((z^{1} )^{2}+(^{1})^{2})=1\,\]

At \(t_{I}\), the RBM learns that the data can be splitted in two groups centred in \(^{1}\), but it does not have yet learned that each one of these two groups consist in two lumps centred in \(^{1}\) and \(^{2}\) (and respectively \(-^{1}\) and \(-^{2}\)). The training dynamics after \(t_{I}\) can also be analyzed: the components of the weight vectors along \(^{1}\) evolve and settle on timescales of order one to a value which is dependent on the initial condition (see the eq. in the SI). In the meanwhile, the components along \(^{2}\) keep growing; at a timescale \(( N_{ v})/(p^{2}(1-k))\) (quite larger than \(t_{I}\) in the limit \(N_{ v}\)) they become of order one. In order to analyze easily this regime, let's consider first the simple case in which the initial condition on the weights is such that \(^{1}(0)^{2}=-^{2}(0)^{2}\) and \(^{1}(0)^{1}=^{2}(0)^{1}\). In this case, one can write \(^{1}=A(t)^{1}+B(t)^{2}\) and \(^{2}=A(t)^{1}-B(t)^{2}\). The corresponding RBM is a Hopfield model with log likelihood:

\[_{a}v_{i}w_{i}^{a})^{2}}{2N_{ v}}=2A(t)^{2}v_{i}_{i}^{1})^{2}}{2N_{ v}}+2B(t)^{2}v_{i}_{i}^{ 2})^{2}}{2N_{ v}}\]

At \(t_{I}\), when \(()A(t_{I})^{2}=1\), one has the first transition in which the RBM measure breaks in two lumps pointing in the direction \(^{1}\), as we explained above. In this regime \(B(t)\) is still negligible but keeps increasing with an exponential rate. Using the results of , one finds that

Figure 1: Learning behaviour of the BG-RBM with one hidden node, using data from the Mattis model at different inverse temperatures, system sizes and learning rates \(,N_{ v},\). The argument of the exponential curves is set to \(m^{2} N_{ v}\), where \(\) is the learning rate. _Inset:_ (top) behavior of the susceptibility \(\) (bottom) magnetization \(h^{*}\) of the learning RBM. The vertical line marks the point at which the susceptibility diverges, indicating the onset of spontaneous magnetization. **Right:** Learning curves for RBMs learning two correlated patterns. The dashed curves represent the weights of the two hidden nodes projected onto \(^{1}+^{2}\), while the dashed-dotted curves are projected onto \(^{1}-^{2}\). _Inset:_ Exponential growth during the two phases: top shows growth in the direction \(^{1}+^{2}\) at a rate \(r^{2}(1+)/2\), and bottom shows growth in the direction \(^{1}-^{2}\) at a rate \(p^{2}(1-)/2\). The arguments of the exponentials are not adjusted.

when \(B(t_{II})^{2}=1\), a second phase transition takes place. This defines a time \(t_{II}\) at which the probability measure of the RBM breaks from two lumps to four lumps, each one centred around one of the four directions \(^{1},^{2}\). We have considered a special initial condition, but the phenomenon we found is general. In fact, for any initial condition one can show that the dynamical equations have an instability on the timescale \(t_{II}\), which generically induces the second symmetry breaking transition. On Fig. 1, right panel, we illustrate the exponential growth as described by the theory, toward the two directions. In the SI 4.2, we show how these phase transitions are in very good agreement with previous work [9; 8] and how the phase space is split during training time. At the end of the training, the patterns are given by \(^{1}=^{1}\) and \(^{2}=^{2}\) modulo a rotation in the subspace spanned by \(^{1,2}\), since the likelihood is invariant by rotations in this subspace. In fact, we often found that the patterns are not perfectly aligned because we are not forcing the weights to be binary. This analysis can naturally extend to more than two patterns, typically resulting in a cascade of phase transitions. In this process, the RBM progressively learns the data features, starting from a coarse-grained version (just the center of mass) and gradually refining until all patterns are learned. The analysis done on the BG-RBM can of course be repeated on the BB-RBM (as is done for the case with one mode in the SI B). The main difference at that level between the two models is that in order to have a retrieval phase, the BB-RBM needs to encode the patterns on an extensive number of hidden nodes (proportional to \(N_{}\)), while the BG-RBM needs only as many patterns as hidden nodes. Both models can match perfectly the dataset in the limit \(N_{}\), but we might encounter discrepancies for finite size. However, when dealing with real datasets, by construction, the BG-RBM can not reproduce higher-order correlations and therefore is less interesting than the BB case.

## 5 Numerical Analysis

In the previous sections, we examined the learning process in simplified setups, in order to be able to develop an analytical treatment. In particular, we have shown analytically in a simple setting how the

Figure 2: **Human genome dataset.** Progressive coding of the main directions of the dataset when training an RBM with the human genome dataset . In A, we show the dataset projected along the first two principal components of the dataset \(_{}\) with \(=1,2\), and \(m_{}^{}=_{}^{(d)}/}}\), with \(^{(d)}\) referring to the different entries in the dataset, i.e. an human individual. Points are colored according to the individual continental origin. In B, we show the evolution of the singular values \(w_{}\) of the RBM weight matrix \(\) as a function of the number of training epochs, and in C, we show the scalar product of the corresponding singular vectors \(_{}\) with the corresponding PCA component \(_{}\). In D, we show the magnetization of the samples generated by the model at different epochs, projected along the first two eigenvectors of \(\), which shows that the specialization of the model occurs through the progressive encoding of the main modes of the data in \(\).

weight matrix is shaped by the patterns present in the dataset and how the learning process dynamics is triggered by the PCA components (the \(^{1}\) and \(^{2}\) in the previous example) and not by the learning of the encoded patterns (\(^{1}\) and \(^{2}\)). Moreover, we have shown that each time the RBMs learn a new direction, the susceptibility of the system diverges with a precise power law everytime the RBM learns a new direction, which is also associated with the development of new modes in the probability measure. In this section we will also show that the insights gained from this simplified analysis are also applicable to understanding the learning process of a Binary-Binary RBM (BB-RBM) with many hidden nodes trained with real data sets. The details about the training procedure are given in SI E.1. For this purpose, we will consider 3 real data sets: (i) The Human Genome Dataset (HGD), (ii) MNIST and (iii) CelebA, see details in the SI D. To show the occurrence of bonafide phase transitions, it is important to show the effect of increasing the system size (which transforms cross-overs in sharp transitions in the large \(N_{ v}\) limit). We will therefore resize these data sets in different dimensions by adjusting their resolution, i.e. by changing \(N_{ v}\) while maintaining comparable statistical properties. Detailed information about the scaling process can be found in the SI D.

In real training processes, the machine is expected to gradually learn different patterns \(^{}\), from the data, as described in the previous sections. However, the identification of these patterns and their relationship to the statistical properties of the dataset remains unclear. Previous research  has shown that RBM training begins with the stepwise encoding of the most significant principal components of the dataset, \(\{^{}\}\), which are the eigenvectors of the sample covariance matrix with the highest eigenvalues, on the SVD decomposition of its weight matrix \(W_{ia}=_{}w_{}u_{i}^{}_{a}^{}\), where \(^{}^{N_{ v}}\) and \(}^{}^{N_{ h}}\) denote the left and right singular vectors corresponding to the singular value \(w_{}\). These vectors form orthonormal bases in \(^{N_{ v}}\) and \(^{N_{ h}}\) respectively, where the index \(\) ranges from 1 to \((N_{ v},N_{ h})\) and the singular values \(w_{}\) are arranged in descending order. At the beginning of the learning process, the left singular vectors, \(^{}\), gradually align \(\)-to-\(\) to \(^{}\). This is consistent with the analytical results in our simple setting in the previous sections. In analogy to the mean-field magnetic models proposed in the previous section, the role of decreasing temperature is played by the increasing magnitude of the singular value \(w_{}^{2}\), associated with each mode \(\), and should lead to a series of phase transitions where the RBM measure splits into increasingly more and more modes. We show in Fig. 2 that these phenomena are at play by focusing on the evolution of the SVD of the RBM weight matrix when trained with the HGD dataset.

In panel A we show the first two principal components of the dataset, which highlights its strong multimodal structure, as several distant clusters appear (in this case, they are related to the continental origin of the individuals at hand). In Fig. 2-B we show the sharp and sudden increases of the singular values \(w_{}\), as expected from our theoretical analysis, and in Fig. 2-C the evolution of the scalar product between \(_{}\) and \(_{}\) as a function of the number of training epochs. Different colors indicate different values of \(\). As expected, the modes are progressively expressed during training, and the first two singular vectors match the two principal directions of the dataset for a while. This last figure also shows us that the alignment with the PCA is only temporary (a limitation of current theoretical approaches), as the machine finds better patterns to encode the data as training progresses.

The progressive splitting of the RBM measure during the training dynamics is shown in Fig. 2-D, for which we use \(N_{ s}=1000\) independent samples generated with the model trained up to a different number of epochs (the colors refer to the same epochs highlighted with vertical lines in Figs. 2-B and C). For visualization, we show the samples projected onto the right singular vectors of \(\), the _magnetizations_\(m_{}=^{}/}\) with \(=1,2\). At the beginning of training, the data points are essentially Gaussian distributed, and the growth of \(w_{1}\) over 4 is related to the splitting of the data into two different clusters on the \(m_{1}\) axis, and the emergence of \(w_{2}\) is related to a second splitting on the \(m_{2}\) axis. At this stage of training, the projections along all subsequent directions are Gaussian distributed as they are the result of a sum of random numbers (fixed by the random initialization of the weight matrix). This progressive splitting is crucial to express the diversity of the dataset shown in Fig. 2-A, and can be successfully used to extract relational trees to cluster data points, as recently shown in Ref. . The details about the numerical analysis are given in E.2.

At the beginning of training, when only a singular value has been expressed, and thus \( w}^{}\), the transition of the feature encoding process is analogous to the phase transition from the paramagnetic to the ferromagnetic phase in the Mattis model mentioned above with pattern \(\). The detailed justification can be found in SI F. Our analysis allows us to define an effective temperature, linked to the eigenmode of \(\) as \(=w^{2}/16\). Now, since the critical temperature of the Mattis model is \(_{c}=1\), we can show that the BB-RBM will condensate when the first eigenmode of the model reaches \(w_{c}=4\), see SI F. In a real training, we also have visible \(\) and hidden bias \(\) which could easily change the model towards a random field Mattis model, which leads us to expect a slightly higher critical point but a very similar ferromagnetic phase transition, and in particular, it should not change the transition's mean-field universality class.

To show that there is a cascade of transitions and that what was found for the HGD also holds for other datasets, we now train the RBM with the MNIST dataset. In Fig. 3-A we plot the evolution of the singular values \(w_{}\) along the training, which clearly show the progressive encoding of patterns. The progressive splitting of the RBM measure into clusters and the presence of a phase transition can be monitored by measuring the variance of the distribution of the visible magnetizations \(m_{}\) along the \(\)-th mode or the analogous hidden magnetizations \(_{}=}^{}/}}\) obtained using the hidden units. The variance of the magnetization multiplied by the number of variables used to compute it and \(\), is related to the _magnetic susceptibility_ via the fluctuation dissipation theorem, which means that

\[_{m}=N_{}( m^{2}- m ^{2})=T\, m/h,\] (5)

here \(\) refers to the equilibrium measure with respect to RBM's Gibbs measure \(p(,)\), in practice estimated as the average over \(N_{}\) independent MCMC runs. It is well known that the magnetic susceptibility should diverge in the vicinity of a second order phase transition and that such growth in only limited by the overall system size \(N=}N_{}}\) in finite systems. These phenomena indeed takes place also in the RBM. We show in Fig. 3-B the evolution of the \(_{m}\)s obtained using the magnetizations obtained along the different modes \(\) of \(\). As anticipated, the susceptibility

Figure 3: **Traning with the MNIST dataset.** In A we show the evolution of the singular values of the RBM’s coupling matrix \(\) as a function of the training time. In \(B\) we show the evolution of the susceptibilities associated with the magnetizations along the right singular vectors of \(\), \(m_{}=_{}/N_{}\). In both figures, we consider the standard \(N_{}=28^{2}\) MNIST dataset, different colors refer to different modes. In C we show the susceptibility associated with the overlaps \(q\) and \(\) between visible and hidden variables. In D we show the susceptibility of the first mode as a function of the first singular value \(w_{1}\) obtained with trainings on MNIST data scaled to different system sizes above and below \(L=28\). The numerical curves are compared with the theoretical expectation using the Mattis model in Eq. (6) using \(w_{1,c}=4.45\). The same data are shown in E, scaled using the mean-field finite-size scaling ansatz of Eq. (7). In F, we show the first 10 modes’ susceptibilities \(_{m_{}}\) as a function of their corresponding singular value \(w_{}\) and compare them with the theoretical curve in D. In G, we show the MCMC relaxation time of the machines trained with different \(N_{}\) datasets as a function of \(w_{1}\), together with the theoretical expectation for local moves in dashed lines.

associated to the magnetization \(m_{1}\) along the first mode, sharply grows as \(w_{1}\) approaches 4, but it is more remarkable that this behavior is not only restricted to the first mode, but it is also reproduced by the subsequent modes in a step-wise process. According to the mapping between the low-rank RBM and the Mattis (or equivalently, the Curie-Weiss) model, we should expect that our \(_{m}\), at least for the first mode \(=1\) should behave as

\[_{m}-}=^{2}-w^{2}},\] (6)

when approaching the critical point, which is equivalent to stating that the critical exponent is \(=1\). Here, the factor 4 in the numerator is related to the fact that the susceptibility obtained with \(\{0,1\}\) variables is 4 times the standard one obtained with Ising spins, and that \(=w^{2}/16\). In Fig. 3-D, we show the susceptibility associated with the first mode as a function of \(w_{1}\) using RBMs trained with MNIST data rescaled to different dimensions. As mentioned earlier, the growth of the susceptibility is limited by the system size \(N_{ v}\). However, if we look at increasingly larger sizes, we can observe the growth over several decades. This shows that at the transition we observe the Mattis/Curie-Weiss behavior of Eq. 6, as shown in the black dashed line, where the only adjustable parameter was the critical point \(w_{1, c}=4.45\) (i.e. there is no adjustable pre-factor).

One of the crucial tests to ensure that a finite-size transition is a bona fide phase transition is to study its behavior by changing the number of degrees of freedom. One of the standard tools to do this is to make use of the so-called finite-size scaling (FSS) ansatz, motivated by renormalization group arguments . Mean-field models follow a modified FSS ansatz which was first studied in . In particular, the FSS ansatz for the susceptibility is

\[_{m}^{N}()=N^{}}(N^{}}|-_{ c}|),\] (7)

with \(()\) a size-independent scaling function, \(N=N_{ h}}\) is the effective size of our model and \(=1\), \(=1/2\) and \(d_{u}=4\) as expected in the mean-field universality class. We test this ansatz in Fig. 3-E showing that it does succeed to scale the finite-size data in the critical region, especially in the largest system sizes, which confirms both the mean-field universality class and the prevalence of the transition in the thermodynamic limit. In Fig. 4-A and B, and Fig. 4-C and D, we show that the indicators of a phase transitions-growth of the susceptibility and its mean-field finite size scaling-also holds for CelebA and HGD datasets. Finally, a final piece of evidence of the existence of a phase transition is presented in Fig. 4-E, where we show that after the continuous transition has taken place, one can induce a discontinuous transition and hysteresis effects by applying a field in the direction of

Figure 4: **Training with the CELEBA and HGD datasets:** In A, we plot the hidden susceptibility for different system sizes in the CELEBA dataset, with dashed lines indicating the expected divergence at \(w_{1,c}=4\). In B, we show the mean-field FFS associated with the first transition using mean-field exponents. In C and D, we present the visible susceptibility for the first phase transition in the HGD dataset, using \(w_{1,c}=5.25\) for scaling. In E, typical hysteresis in the low-temperature phase is illustrated for CELEBA (128\(\)128), similar to the mean-field Ising model in external fields.

the learned pattern, in full agreement with what observed for standard phase transitions, the details of the analysis can be found in SI E.2.2, and further theoretical insights into the relationship between hysteresis and discontinuous transitions can be found in G.

All the discussion so far has been mainly concerned with the first phase transition, when the RBM learns the first mode. But we have discussed in Fig. 3-B that an entire sequence of step-wise phase transitions occurred in the rest of the \(\)-matrix modes. In Fig. 3-F we show each of these mode susceptibilities \(_{m_{}}\) as a function of their corresponding singular value. They show extremely similar divergent behavior with respect to the mode \(=1\), with an apparent slight variation of the critical point for each mode, although all appear to remain close to the predicted \(w_{} 4\), suggesting that the subsequent transitions may be of similar mean-field nature. Exceeding second-order phase transitions has a very strong impact on the overall quality of the training, in particular on the quality of the log-likelihood gradient estimated by MCMC dynamics. Indeed, second-order transitions are associated with a well-known arresting effect, known as _critical slowing-down_ behavior, by which the thermalization times diverge with the correlation length \(^{z}|-_{}|^{- z}\), where \(z\) is the dynamical critical exponent, which is 2 for local and non-conserved order parameter moves in mean-field, making the thermalization of large systems extremely difficult in practice. We show that our exponential relaxation times diverge exactly as predicted in Fig. 3-G. This has a significant impact on the quality of models trained with maximum likelihood approaches, as these methods rely on MCMC to estimate gradients. It is therefore expected that MCMC mixing times increase sharply each time a mode is coded, which can be prohibitive for clustered and high-dimensional datasets. Recent studies have shown that pre-training a low-rank RBM using other methods (and thus bypassing the initial phase transitions) can be very effective in improving the models in clustered datasets . However, we emphasize that the cascade of phase transitions described in this paper occurs regardless of the training scheme or whether the Markov chains reach equilibrium. This is discussed further in SI H.

**Extensions and limitations-** All these results can be studied in detail for RBMs thanks to the fact that we can analytically deal with the Hamiltonian. However, our results can be extended to the case of Deep Boltzmann Machines, where previous works have also computed the phase diagram, which are also based on the SVD decomposition of the weighing matrices , but also in diffusion models where phase transitions linked to the learning has also been described [24; 26]. It therefore stands to reason that similar phenomena occur with even more complex models such as Convolutional EBM, but where it is not clear how the parameters of the model can be decomposed. A first test would be to see what the projection of the generated data would look like in the different phases of learning.

## 6 Conclusions

In this paper, we first characterized the learning mechanism of RBMs using a simplified setting with a dataset provided by a simple teacher model. We used two examples: one with two symmetric clusters and another with four correlated clusters. Our results show that the learning dynamics identify modes by exponential growth in the directions of the clusters dominated by the variances of these clusters. The theory predicts the timing of the first phase transitions and agrees well with . Numerically, we have confirmed the existence of a cascade of phase transitions associated with the growing modes \(w_{}\) and accompanied by divergent susceptibility. Finite-size scaling suggests that these transitions are critical and fall into the class of mean-field universality. This set of phase transitions likely goes beyond RBMs and offers insights into learning mechanisms, particularly for generative models. These transitions have significant implications for both training and understanding the learned features. During training, each transition is associated with a divergent MCMC relaxation time, which requires careful handling to properly train the model. In addition, the hysteresis phenomenon ensures that the learning trajectory involves second-order phase transitions, which are beneficial for tracking the emergence of modes in the learned distribution. However, changing parameters (such as the local bias) could lead to first-order transitions that are detrimental to sampling and could explain the ineffectiveness of parallel tempering in the presence of temperature changes. In practice, our analysis shows that the principal directions of the weight matrix contain valuable information for understanding the learned model.

## 7 acknowledgments

The authors would like to thank J. Moreno-Gordo for his work and discussions during the initial phase of this project. The authors (A.D. and B.S.) acknowledge financial support by the Comunidad de Madrid and the Complutense University of Madrid (UCM) through the Atraccion de Talento program (Refs. 2019-T1/TIC-13298), and to project PID2021-125506NA-I00 financed by the Ministerio de Economia y Competitividad, Agencia Estatal de Investigacion (MICIU/AEI /10.13039/501100011033), and the Fondo Europeo de Desarrollo Regional (FEDER, UE).