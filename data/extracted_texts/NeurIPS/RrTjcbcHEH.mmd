# Neural Localizer Fields for Continuous

3D Human Pose and Shape Estimation

 Istvan Sarandi, Gerard Pons-Moll\({}^{1,2,3}\)

\({}^{1}\)University of Tubingen, Germany, \({}^{2}\)Tubingen AI Center, Germany,

\({}^{3}\)Max Planck Institute for Informatics, Saarland Informatics Campus, Germany

https://istvansarandi.com/nlf

###### Abstract

With the explosive growth of available training data, single-image 3D human modeling is ahead of a transition to a data-centric paradigm. A key to successfully exploiting data scale is to design flexible models that can be supervised from various heterogeneous data sources produced by different researchers or vendors. To this end, we propose a simple yet powerful paradigm for seamlessly unifying different human pose and shape-related tasks and datasets. Our formulation is centered on the ability - both at training and test time - to query any arbitrary point of the human volume, and obtain its estimated location in 3D. We achieve this by learning a continuous neural field of body point localizer functions, each of which is a differently parameterized 3D heatmap-based convolutional point localizer (detector). For generating parametric output, we propose an efficient post-processing step for fitting SMPL-family body models to nonparametric joint and vertex predictions. With this approach, we can naturally exploit differently annotated data sources including mesh, 2D/3D skeleton and dense pose, without having to convert between them, and thereby train large-scale 3D human mesh and skeleton estimation models that considerably outperform the state-of-the-art on several public benchmarks including 3DPW, EMDB, EHF, SSP-3D and AGORA.

## 1 Introduction

Along with the wider field of computer vision, the human pose and shape estimation community has recently started a transition towards leveraging large-scale training data to produce robust models . When assembling large training sets from different sources, it is inevitable that they will use different types of annotations, including different parametric mesh models, 3D skeletal joints and markers from different motion capture vendors and manual 2D keypoint annotations. Re-annotation to a single format for training is a costly and error-prone process, and therefore difficult to scale. Similarly, current models are trained to output a specific format, such as pre-defined joints, keypoints, or parametric body meshes, which is limiting for downstream applications which require other formats. We therefore argue that a good model design should be able to ingest and output highly heterogeneous forms of annotations in a unified manner.

Despite the different kinds of labels, these tasks all share a common underlying factor: the correspondence problem of mapping from a canonical human body space to the observation space of the camera. Hence, if a model could localize arbitrary human points (both surface and body-internal ones, e.g., joints), its predictions could be supervised from any human-centric data source with point labels and could be customized to new landmark sets even after deployment. In this work, we propose a method that is able to achieve exactly this.

To localize individual points, we leverage the machinery of volumetric heatmap estimation with soft-argmax decoding, which has worked well in skeletal keypoint estimation methods . However, generalizing this to arbitrary points is nontrivial. In prior work, each body joint heatmap isproduced through a separate set of convolution weights, tying the model to a particular set of joints or keypoints. Extending this paradigm to localize an infinite number of points would require infinite memory to store the weights, and learning would be highly inefficient as weights would need to be trained independently for each point.

Our key idea and contribution is to learn a continuous field of point localizers, which we call a _Neural Localizer Field_. The principle is to modulate a deep network's prediction layer on-the-fly, based on what point we need to predict: at training time querying points for which we have annotations, and at test time sampling the points that an application requires. Here, we are inspired by the recent success of neural field representations, such as neural radiance , distance [84; 18] and occupancy fields , but instead of predicting a physical property (occupancy, color) we predict a function.

Specifically, this _localizer field_'s input domain is the full 3D volume of the canonical human body and its output range is the parameter space of the modulated convolutional output layer (making the localizer field a hypernetwork [33; 46; 13]). By optimizing the neural field's parameters, we obtain a smooth field of localizers (functions) over the human volume domain, improving robustness, and knowledge sharing. In the paper, we investigate various design choices related to this architecture, including positional encodings. This architecture results in consistent and accurate skeletal joint predictions along with smooth nonparametric mesh output, whose format is chosen by the user at test time, see Fig. 1. Since a parametric representation (for example based on SMPL ) is often preferred for further downstream processing, as the second contribution of this paper, we derive a simple and efficient algorithm for fitting SMPL-family body models to our non-parametric predictions. This involves alternating between estimating global body part orientations through the Kabsch algorithm , and solving for the shape coefficients in a regularized linear least squares formulation. This converges after as few as ca. 2-4 rounds, and is well-suited for GPU acceleration.

In summary, our main contribution is the _Neural Localizer Field_ method for the 3D localization of an _infinite continuum of points within the human body volume_, based on a single RGB image, allowing seamless mixed-dataset training using various skeleton or mesh annotation formats without the need for tedious and error-prone re-annotation. Through this formulation, we are able to train a generalist human pose and shape estimator that outperforms prior work on a wide range of important benchmarks. We will make our code and trained models publicly available for research.

## 2 Related Work

Methods for 3D pose/shape estimation use a wide variety of formats: different skeletons, meshes, body models or volumetric representations. For a comprehensive review, see . Here we focus on methods that closely relate to ours.

Large-scale multi-dataset human-centric training.A few recent works [93; 30; 12] have started to explore scaling up 3D human body reconstruction methods beyond the previously common practice of training on one or a few datasets. Models trained at scale from many sources tend to outperform specialized models that were trained for a specific dataset. However, current methods have to unify

Figure 1: _Can one model learn to localize any point of the human body in 3D from a single RGB image?_ We propose to build a generalist human pose and shape estimator that can readily learn from any annotated points at training time and can estimate any user-chosen points at test time.

their datasets to a common format , which is often an ill-posed problem. This can introduce errors if done through pseudo-annotation, requiring significant human effort and manual quality checks. Another option is to train a model to predict all possible outputs and optimize consistency losses between them , but this is not scalable to a large, possibly infinite number of annotation types. By stark contrast, our model can ingest data from many different formats, and can be asked to output any point set at test time.

Nonparametric mesh estimation and sparse keypoints.Some works  predict further keypoints on the surface beyond the skeleton, noting that this carries richer shape information. Ma et al.  predict a sparse set of virtual markers and approximate the rest through convex interpolation. Nonparametric mesh estimation has been a successful line of recent research , mainly due to better preservation of alignment in contrast to parametric estimation, which often suffers from mean-shape bias . In contrast to our work, the prominent Transformer-based approaches of this category  feed a static set of keypoints into their architecture throughout training, which then exchange information through self-attention layers, and require a coarse to fine strategy and complex design for efficient processing. In contrast, our architecture is more lightweight, and can directly produce predictions for all possible points, without having to interpolate them from some other set of points.

Continuous and dense representations.While the dominant paradigm to supervise pose estimators has been sparse keypoints and skeletons, a few works have explored dense, continuously varying keypoints for supervision and prediction. DenseReg  predicts a dense 2D deformation grid to self-supervise a face regressor. In DensePose , 2D keypoint estimation is generalized to a continuous representation, where arbitrary surface points can be regression targets, instead of having a fixed set of sparse keypoints throughout the dataset. DensePose3D  shows how 2D dense maps can be used to lift estimates to 3D. Similar in spirit, DenseBody  predicts 3D mesh points on a 2D UV-map. Chandran et al.  estimate 2D facial landmarks in a continuous manner. Common to all these works is that either the prediction or some intermediate representations are given in 2D, e.g., UV-maps, dense semantic maps or 2D output coordinates. By contrast, we are able to perform a direct 3D-to-3D mapping from canonical to observation space, to localize an infinite continuum of points in the human volume, to harness heterogeneous annotations.

Distinctions from DensePose.DensePose  may seem similar enough to our motivation that we should expand upon why it does not solve the same problem we are tackling. In a sense, our problem formulation is the _reverse_ of DensePose's. DensePose starts out in image space and asks, _per pixel_, what canonical surface point is visible at the pixel (facing the camera). This inherently restricts the formulation to the frontal surface of the body. Occluded points, body-internal joints and the opposite-side surface of the human are ignored in DensePose. Instead, we start in canonical space and ask the more natural question of where each body point (identified in canonical space) can be found in 3D observation space. This is more in line with the goals in practical applications. However, because of this reverse relation between the two formulations, the annotations in DensePose datasets  can still be directly used to supervise our model as well (with the roles reversed), further boosting the data sources available to us.

3D keypoint estimation through heatmaps.A well-established paradigm to predict 3D keypoints is to use 3D heatmaps . The advantage is that convolutional architectures are well suited for localizing joints in the image, and inputs and outputs are well aligned. However, predicting an infinite number of heatmaps for a continuum of points is not possible, as it would require storing an infinite number of last-layer weights. With our Neural Localizer Fields, we can harness the well-performing heatmap paradigm and adapt it to predicting keypoints chosen at test time.

Parametric fitting to nonparametric predictions.Fitting a parametric mesh to keypoint predictions is needed for certain applications and for compactness. If correspondences are known, the classical way is to solve a nonlinear least squares problem as in the original SMPL  paper, but this requires many iterations and is slow. To speed up and to make the process differentiable, some works train an MLP to predict body parameters from sparse markers . This requires generalizing to all possible poses which is hard. By contrast, we propose a solver based on limb rotation estimation via the Kabsch algorithm and linear least-squares shape fitting. This direct geometry-based algorithm is orders of magnitude faster than a classical optimization procedure, and does not suffer from generalization issues of the MLP methods since it is not learning-based.

## 3 Method

Our method consists of three main parts: (1) a _point localizer network_ (PLN) for localizing 3D points from images, (2) a _neural localizer field_ (NLF) to control what point the PLN should predict, and (3) an efficient body model fitting algorithm to create a parametric representation for a set of points that were localized nonparametrically. We metonymically refer to our complete approach as _NLF_. This modular structure establishes clear interfaces and can enable independent improvements on the parts. The architecture overview is depicted in Fig. 2.

### Point Localizer Network

We start with describing the basic mechanism of localizing a point in our architecture, which we then generalize to arbitrary points afterward.

Given an RGB image crop \([0,1]^{H W 3}\) of a person, we first extract feature maps \(^{h w C}\) through an off-the-shelf backbone at stride \(s=W/w=H/h\). We then follow the MeTRAbs [92; 93] architecture in skeletal 3D pose estimation, and produce metric-scale truncation-robust volumetric heatmaps \(H_{}^{h w D}\), as well as 2D pixel-space heatmaps \(H_{}^{h w}\) through a 1\(\)1 convolutional layer. The heatmaps are decoded to coordinates via soft-argmax [54; 80; 104], and the resulting 2D and root-relative 3D points are combined to an absolute camera-space estimate, including estimates for truncated body parts. Additionally, we predict an uncertainty map \(U^{h w}\), which will be averaged to an uncertainty scalar per point using weights from \(H_{}\).

A classical, joint-based pose estimator would be trained to produce a fixed set of heatmaps, as many as there are joints in the skeleton, and it would have to be trained and tested with the corresponding fixed set of joints. Taking a functional view, observe that this type of architecture realizes a discrete set of **localizer functions**, mapping image features to 3D locations as

\[f_{i}:^{h w C}^{3},}^{}_{i}, i[1..J]\] (1)

with \(J\) being the number of joints the model can predict. In a pose estimator, each \(f_{i}\) mapping is represented through a section of the weight matrix \(W\) of the convolutional output head. Hence, in this paradigm, implementing localizer functions to detect further points requires extending the weight matrix, giving linear growth in the parameters of the output layer. It is clearly infeasible to store the weights for every possible point we might want to predict. Furthermore, even though the functions for localizing nearby points of the body have more in common than those detecting faraway points, this spatial structure is lost when representing them as discrete outputs. In the next part, we describe our method for retrieving the full continuum of localizer functions, allowing predictions for any point, as well as ensuring spatial information sharing across the functions.

Figure 2: **Overview of NLF. Given image features \(\) and any arbitrarily chosen 3D query point \(\) within the canonical human volume, we aim to estimate the observation-space 3D point \(^{}\). To control which point gets estimated, we dynamically modulate a convolutional layer at the output, to produce heatmaps for the requested point. We achieve this modulation by predicting the convolutional weights through a neural field. During training, the points \(\) can be picked per training example based on whichever points are annotated for it, allowing natural dataset mixing. At test time, the model can flexibly estimate any surface point and any skeletal joint inside the body volume, as required.**

### Neural Localizer Field

The goal of our work is to go beyond predicting only a predetermined finite set of localizable points in a network, and to instead learn to localize _all_ points of the body with one universal model, by learning a whole _localizer field_ of functions \(\). The field \(\) associates a function \(f_{}\) to every point \(\) in the canonical human volume \(_{H}^{3}\):

\[:_{H}^{h w C} ^{3},(f_{}: ^{}),\] (2)

where the posed point \(^{}\) is determined based on image features \(\) by \(f_{}\). Although neural fields are typically used to predict points or vectors, here we use them to predict localizer functions \(f_{}\). This way, we represent information on a continuous domain, while appropriately modeling both smooth structure and fine-grained details of a signal. Specifically, \(f_{}()\) is implemented as a single-layer convolutional head whose weights are predicted as a function \(w()\) of the canonical point \(\) we want to localize. The function \(w:_{H}^{(C+1)(D+2)}\) is realized as an MLP, taking three canonical coordinates of \(\) as input and producing parameters \((,)\) for \(f_{}\)'s convolutional layer to control which point gets predicted (cf. hypernetworks). Here the \(C+1\) is the backbone feature dimensionality plus one bias, and \(D+2\) are the total number of output channels which are \(D\) depth channels of the volumetric heatmap \(H_{}\), one 2D heatmap \(H_{}\) and one uncertainty map \(U\). In summary, given the image features \(\) and a canonical query point \(\), the following computation takes place:

\[(,):=w()\] (3) \[(H_{},H_{},U):=_{1 1 }(;,)\] (4) \[^{}_{}:= (H_{})^{}_{}:=(H_{})\] (5) \[u:=_{x,y}U_{x,y}(H_{ })_{x,y}:=(u)+,\] (6)

after which the 3D root-relative and the 2D predictions are fused into a camera-space output \(^{}\) following . The localizer field's architecture consists of a positional encoding part and an MLP with GELU activation (see the appendix for the details of the MLP layers).

Positional encoding.To express high-frequency signals, neural fields typically employ positional encodings . Since the field's domain is the canonical human volume \(_{H}\), we use the eigenbasis \(_{i}\) of the volumetric Laplacian \(\) (the Fourier basis equivalent for bounded volumes), inspired by the use of the Laplace-Beltrami operator in . We then compute the _global point signature_ (GPS) 

\[()=_{1}()/},...,_{M} ()/}^{T}^{M},\] (7)

a widely used descriptor in shape matching. So far, this only gives us the eigenfunction values \(_{i}\) on the discrete tetrahedral mesh nodes. To obtain an approximate interpolant for the entire domain, we distill the eigenfunctions into a small learnable Fourier feature network \(h:^{3}^{M}\) with \(h_{i}()_{i}()\). The network \(h\) can be finetuned end-to-end. Details are in the appendix. We will compare the effectiveness of plain coordinates, global point signature, and learnable Fourier features.

Mathematical formulation for training.Our formulation allows us to supervise the point-localization task on a continuous domain. Given an image \(\) with features \(\), and the ground-truth continuous warp function \(g:_{H}^{3}\) mapping between the canonical and observation spaces, the loss measures the distance between ground truth \(g()\) and prediction \(f(;)\) in function space, through the volume integral

\[=_{_{H}}_{}f(; ),g()\,^{3},\] (8)

where \(_{}(,)\) is the pointwise loss comparing prediction \(f(;)=}^{}\) to ground truth \(g()=^{}\) in observation space. We adopt the Euclidean loss

\[_{}(}^{},^{})=\| }^{}-^{}\|\] (9)

without squaring, for better outlier-robustness. (In the appendix, we show how the uncertainty output can be combined into the loss function, as this aspect is not the main focus here.) This loss is applied to the predictions in the 3D camera coordinate frame, the 3D root-relative frame and the 2D imagespace as well, with weighting factors. Naturally, only the 2D loss is applied for training examples that only have 2D labels.

In practice, we often do not have the full volume annotation \(g()\) available for all \(\) as in (8), and it would be too expensive to evaluate the integral for every image. Since we train on a mixture of datasets, each training example \((_{i},\{^{}_{k}\}_{k=1}^{K_{i}})\) can contain a different annotated point set \(\{^{}_{k}\}_{k=1}^{K_{i}}\) depending on the dataset the example comes from. Hence, we turn the integral (8) into a discrete sum over the available dataset-specific locations \(_{k}_{H}\). For training examples with parametric body annotations, we perform Monte Carlo approximation of the integral by random sampling of points.

Zoo of annotations.Data in HPE contains a wide variety of annotation types including parametric meshes, 3D keypoints, 2D keypoints and DensePose samples. Thanks to our formulation described above, we can directly consume this data, since the zoo of annotations consists of subsets of points in the canonical volume. For examples that are annotated with a parametric mesh model, we sample 640 points uniformly over the surface and 384 points from the interior of the human volume. The sampled points are unique to each example of the batch. We supervise interior points densely to enable the network to predict new joint sets anywhere in the volume at test time. To deform interior points, we use scattered data interpolation [102; 9] to propagate SMPL deformations to the volume. For datasets annotated with 2D or 3D keypoints, the corresponding canonical positions need to be established per joint. We use an approximate initialization per skeleton type (see appendix), and finetune the precise canonical positions during the main training process. In case of DensePose annotations, we map the \(I,U,V\) labels to the corresponding SMPL surface points in the canonical pose.

Implementation details.We use EfficientNetV2-S (256 px) and L (384 px)  initialized from , and train with AdamW , linear warmup and exponential learning rate decay for 300k steps. Training the S model takes \(\)2 days on two 40 GB A100 GPUs, while the L takes \(\)4 days on 8 A100s.

### Efficient Body Model Fitting in Post-Processing

Above, we introduced our method to nonparametrically localize any human point from an image, in 3D. While nonparametric prediction has its advantages (e.g., closer fits to the image data), in some cases a parametric representation (joint rotations \(\) and shape vector \(\)) is preferable due to its compactness, and disentanglement of pose and shape. We therefore develop a fast algorithm to solve for pose and shape parameters that closely reproduce our predicted nonparametric vertices.

Our algorithm alternates between two steps for a small number of iterations (\(\)3), followed by an adjustment step. We first independently fit global orientations per body part by applying a weighted Kabsch algorithm. Then, keeping the orientations fixed, we solve for \(\) and the translation vector via linear least squares, since SMPL's vertex positions are linear in the shape parameters. Finally, we refine the part rotations in one pass along the kinematic tree, anchoring the rotations at the parent joint. For improved efficiency, it is possible to consider only a subset of the vertices when fitting. Further, given that our model estimates per-point uncertainties, these can be used in a weighted variant of the above algorithm. Splitting the shape estimation task into a nonparametric point localization and a fitting step has the further benefit that it is easy to share the body shape parameters \(\) for multiple observations of the same person (multi-view and temporal use cases) during the least-squares regression. More details and the pseudocode of the algorithm are given in the appendix.

## 4 Experiments

We extensively evaluate our method on a variety of benchmarks: 3DPW  and EMDB  for SMPL body, AGORA  and EHF  for SMPL-X, SSP-3D  for SMPL focusing on body shape, as well as Human3.6M , MPI-INF-3DHP  and MuPots-3D  for 3D skeletons.

Our goal is to enable large-scale multi-dataset training from heterogeneous annotation sources in order to train strong models. To show that our formulation is effective at this, we assemble a large meta-dataset for use in training. Due to space constraints we provide the detailed dataset description in the appendix. For parametric meshes we include the datasets [85; 8; 6; 35; 50; 109; 24; 36; 16; 130; 121; 119; 123; 88; 10; 112], with points sampled directly from each dataset's native body model (one of SMPL, SMPL-X or SMPL-H, neutral or gendered, as provided) without a need for any conversions. For 3D skeleton annotations, we use [40; 70; 72; 124; 131; 43; 57; 81; 115; 89;34, 29, 132, 111, 82, 110, 23, 5, 28, 26, 27, 4, 48, 17]. Several of these have custom skeletons that are nontrivial to convert to the SMPL family, preventing existing 3D HPS methods from exploiting these rich data sources. For 2D keypoint annotations we use  and further use DensePose datasets  which provide pairs of SMPL surface points and corresponding pixel locations. Integrating all these datasets would be a great challenge if we had to choose one specific representation. Instead, we can easily train using the strategy described in Sec. 3.2, we simply map each annotation convention to the canonical human volume and learn a continuous localizer field.

We use standard metrics including per-joint error (MPJPE), per-vertex error (MVE), orientation angle error (MPJAE) and their Procrustes variants ("P-"). Metrics are explained in detail in the appendix.

Pose prediction.NLF outperforms all baselines on **EMDB** (Tab. 3), which has challenging outdoor sequences. The improvement is drastic, we get an MPJPE of 68.4 mm compared to the second-best (98.0 mm). Remarkably, we outperform the temporal method WHAM (79.7 mm) although we work frame-by-frame. **3DPW** results confirm the same (Tab. 2). (For reference, the standard error of the mean under normal assumptions for 3DPW MPJPE is 0.06 mm.) We attribute this performance to the fact that we can natively train on many sources without error-prone conversions. (For better comparability to temporal methods, we also provide NLF results using a (non-learned) 5-frame temporal smoothing filter.) On **AGORA** (Tab. 5), we outperform all prior works in SMPL-X prediction. Even though we do not specially target facial and hand keypoints, we obtain the second best scores for these. We also achieve state-of-the-art body results on **EHF** (Tab. 7). The higher hand error is due to few detailed hands in the training data. We obtain excellent performance on the skeleton estimation benchmarks **Human3.6M, MPI-INF-3DHP and MuPoTS-3D** as well (Tab. 6). This evaluation is easy with NLF, as we can pick at test time _which points of the body to predict_.

Shape prediction.Mesh recovery methods are often biased towards an average shape. Keypoint-based methods are pixel-aligned but ignore shape, and SMPL parametric regressors tend to produce subpar image alignment. By contrast, since we model the full continuum of points, we can estimate shape more accurately while being pixel-aligned. This is reflected on **SSP-3D** (Tab. 4), a dataset specifically designed to evaluate shape estimation, where we again improve over all baselines, obtaining a low error of 10.0 mm. Notably, we outperform works, such as ShapeBoost , which are specialized to shape estimation and have not been tested on general pose estimation benchmarks. Figure 3 shows a qualitative example. Following the protocol in , we also evaluate combined shape estimation from multiple (max. 5) images of the same person, by shared estimation of the \(\) shape parameters during body model fitting. This reduces the error to 9.6 mm. Our NLF achieves state-of-the-art shape estimation while _simultaneously_ obtaining SOTA results on the most challenging pose benchmarks such as EMDB, using the same model weights.

Positional encoding.To assess the impact of positional encoding on shape fidelity, we evaluate the mIoU silhouette overlap measure on the shape-diverse SSP-3D, the Procrustes-aligned vertex error on 3DPW and EMDB, as well as the orientation error on 3DPW after SMPL-fitting using the small backbone. As seen in Sec. 4, the use of the Laplacian-based global point signature (GPS) encoding leads to a better mIoU of \(0.789\) compared to the \(0.777\) achieved using plain \(X,Y,Z\) coordinates. We attribute this to better representation of fine-grained geometry. Learnable Fourier features obtain even better results than GPS, but best is the combination where we use the GPS as initialization for the learnable Fourier feature network. Do note however that NLF achieves good performance even with plain coordinates.

Uncertainty estimation.We provide an ablation for uncertainty predictions in the appendix.

    & SSP3D mIoU\(\) & 3DPW P-MVE\(\) & EMDB P-MVE\(\) & 3DPW MPJAE\(\) \\  Plain XYZ coordinates & 0.777 & 54.0 & 57.7 & 16.9 \\ Global point signature (GPS) & 0.789 & 54.2 & 57.5 & 15.8 \\ Learnable Fourier features & 0.804 & **52.8** & 57.0 & 15.4 \\ Learnable Fourier (GPS init.) & **0.812** & **52.8** & **56.4** & **15.3** \\   

Table 1: **Ablation of positional encodings. Input representation matters in NLF. The fixed Laplacian-derived global point signature helps most metrics, but is outperformed by learnable Fourier features. Best results are achieved by initializing the learnable Fourier features to the GPS.**

    &  &  &  \\   & MPJPE\(\) & P-MPJPE\(\) & PCK\(\) & AUC\(\) & MPJPE\(\) & PCK-detected\(\) & \\  MeTRAbs-ACAE-S  & 40.2 & 31.1 & 96.3 & 58.7 & 57.9 & 94.7 & \\ MeTRAbs-ACAE-L  & **36.5** & **27.8** & 97.1 & 60.1 & 55.4 & 95.4 & \\  NLF-S & 40.4 & 30.6 & 96.6 & 57.9 & 59.9 & 94.7 & \\ NLF-L & 39.7 & 28.5 & **97.5** & **61.0** & **54.9** & **95.5** & \\   

Table 6: **Results on skeleton estimation benchmarks.**

    &  &  \\   & MPJPE & P-MPJPE & MVE & P-MVE & MPJPE & P-MPJPE & MVE & P-MVE \\  PyMAF  & 78.0 & 47.1 & 91.3 & & & & & & & & & \\ SMPLer-X-H32  & 75.0 & 50.6 & & & & & & & & & & \\ BEDLAM-CLIFF  & 72.0 & 46.6 & 85.0 & & & & & & & & & \\ HypIK  & 71.6 & 41.8 & 82.3 & & & & & & & & & \\ HMR 2.0a  & 70.0 & 44.5 & & & & & & & & & & \\ Multi-HMR  & 69.5 & 46.9 & 88.8 & & & & & & & & & \\ _WHAM-B (ViT, w/ BEDLAM)*_ & & & & & & & & & & & & \\  NLF-S & 60.9 & 38.5 & 73.3 & 52.8 & 55.6 & 35.9 & 67.0 & 48.9 & \\ NLF-S +fit & 60.8 & 37.9 & 72.2 & 51.4 & 56.6 & 35.7 & 66.7 & 47.9 & \\ NLF-L & 60.3 & 37.3 & 71.4 & 50.2 & **54.1** & 33.7 & **63.7** & 45.3 & \\ NLF-L +fit & **59.0** & **36.5** & **69.7** & **48.8** & 54.9 & **33.6** & **63.7** & **44.5** \\ NLF-L +fit +_smooth*_ & _57.2_ & _35.4_ & _67.8_ & _47.7_ & _53.2_ & _32.6_ & _62.1_ & _43.5_ & \\   

Table 2: **Results on 3DPW (14 joints). * denotes _temporal_ (multi-frame) method

    &  &  \\   & MPJPE & P-MPJPE & MVE & P-MVE & MPJPE & P-MPJPE & MVE & P-MVE \\  PyMAF  & 78.0 & 47.1 & 91.3 & & & & & & & & & \\ SMPLer-X-H32  & 75.0 & 50.6 & & & & & & & & & & & \\ BEDLAM-CLIFF  & 72.0 & 46.6 & 85.0 & & & & & & & & & \\ HypIK  & 71.6 & 41.8 & 82.3 & & & & & & & & & \\ HMR 2.0a  & 70.0 & 44.5 & & & & & & & & & & \\ Multi-HMR  & 69.5 & 46.9 & 88.8 & & & 61.4 & 41.7 & 75.9 & & & & \\ _WHAM-B (ViT, w/ BEDLAM)*_ & & & & & & & & & & & & & \\  NLF-S & 60.9 & 38.5 & 73.3 & 52.8 & 55.6 & 35.9 & 67.0 & 48.9 & \\ NLF-S +fit & 60.8 & 37.9 & 72.2 & 51.4 & 56.6 & 35.7 & 66.7 & 47.9 & & & & & \\ NLF-L & 60.3 & 37.3 & 71.4 & 50.2 & **54.1** & 33.7 & **63.7** & 45.3 & & & & & \\ NLF-L +fit & **59.0** & **36.5** & **69.7** & **48.8** & 54.9 & **33.6** & **63.7** & **44.5** \\ NLF-L +fit +_smooth*_ & _57.2_ & _35.4_ & _67.8_ & _47.7_ & _53.2_ & _32.6_ & _62.1_ & _43.5_ & & & & \\   

Table 3: **Results on EMDB1 (24j). *_temporal_ method**

    &  &  \\   & MPJPE & P-MPJPE & MVE & P-MVE & MPJPE & P-MPJPE & MVE & P-MVE \\  PyMAF  & 78.0 & 47.1 & 91.3 & & & & & & & \\ SMPLer-X-H32  & 75.0 & 50.6 & & & & & & & \\ BEDLAM-CLIFF  & 72.0 & 46.6 & 85.0 & & 66.9 & 43.0 & 78.5 & & \\ HypIK  & 71.6 & 41.8 & 82.3 & & & & & & & \\ HMR 2.0a  & 70.0 & 44.5 & & & & & & & & \\ Multi-HMR  & 69.5 & 46.9 & 88.8 & & & 61.4 & 41.7 & 75.9 & & & \\ _WHAM-B (ViT, w/ BEDLAM)*_ & & & & & & & & & & \\  NLF-S & 60.9 & 38.5 & 73.3 & 52.8 & 55.6 & 35.9 & 67.0 & 48.9 \\ NLF-S +fit & 60.8 & 37.9 & 72.2 & 51.4 & 56.6 & 35.7 & 66.7 & 47.9 & & & & \\ NLF-L & 60.3 & 37.3 & 71.4 & 50.2 & **54.1** & 33.7 & **63.7** & 45.3 & & & & \\ NLF-L +fit & **59.0** & **36.5** & **69.7** & **48.8** & 54.9 & **33.6** & **63.7** & **44.5** \\ NLF-L +fit +_smooth*_ & _57.2_ & _35.4_ & _67.8_ & _47.7_ & _53.2_ & _32.6_ & _62.1_ & _43.5_ & \\   

Table 3: **Results on EMDB1 (24j). *_temporal_ method**

    &  &  &  &  \\   & All Body & All & Body & All & Body & Face & L Han & RHan & All & Body & Face & L Han & RHan \\  Hand4Whole  & 144.1 & 96.0 & 141.1 & 92.7 & 135.5 & 90.2 & 41.6 & 46.3 & 48.1 & 132.6 & 87Figure 4: **Customizable point localization.** By selecting points \(\) in the continous canonical space, we can predict any landmark set both at training and test time. The first column depicts the query points we estimate: SMPL(-X) joints and vertices, COCO joints, Human3.6M joints, and arbitrary points sampled within the human volume. The fourth and seventh column show rotated views.

   Method &  &  &  \\   & Full-body & Hands & Face & Full-body & Body & Hands & Face & Body (14j) & Hands \\  HybrIK-X  & 121.4 & 52.1 & 41.9 & 59.8 & 50.0 & 17.6 & 8.1 & 60.8 & 17.9 \\ OSX  & 70.8 & 53.7 & 26.4 & 48.7 & – & 15.9 & 6.0 & – & – \\ PyMAF-X  & 64.9 & 29.7 & 19.7 & 50.2 & 44.8 & **10.2** & 5.5 & 52.8 & **10.3** \\ SMPLer-X  & 62.4 & 47.1 & 17.0 & 37.1 & – & 14.1 & **5.0** & – & – \\ Multi-HMR  & 42.0 & **28.9** & 18.0 & 28.2 & – & 10.8 & 5.3 & – & – \\  NLF-S & 39.7 & 49.7 & 15.2 & 30.9 & 30.4 & 24.5 & 7.4 & 32.8 & 28.2 \\ NLF-S +fit & 40.0 & 49.7 & 15.4 & 30.4 & 29.7 & 24.0 & 6.5 & 32.0 & 25.4 \\ NLF-L & **36.3** & 43.2 & **13.5** & **25.8** & 24.8 & 21.3 & 6.7 & 26.3 & 25.3 \\ NLF-L +fit & 36.4 & 43.0 & 13.9 & 26.0 & **24.4** & 20.7 & 6.3 & **26.1** & 21.8 \\   

Table 7: **Results on EHF (SMPL-X).**

Figure 3: **Qualitative result on SSP-3D.**_left:_ NLF’s nonparametric output (front and side view), _right:_ result of our proposed fast SMPL fitting algorithm (front and side). Our nonparametric prediction already has high quality, allowing us to use a simple and efficient fitting algorithm to obtain body model parameters that faithfully represent the nonparametric output.

Effect of datasets.Assessing the contribution of each individual dataset is computationally infeasible, nonetheless, to validate that NLF benefits from diverse supervision, we partition the data into real and synthetic subsets. As Tab. 8 shows, best results are achieved when all datasets are used in training.

Real-time inference.NLF-S has a batched throughput of 410 fps and unbatched throughput of 79 fps on an Nvidia RTX 3090 GPU. For NLF-L these are 109 fps and 41 fps respectively.

Qualitative results.To demonstrate the versatility of our method in localizing any point of the human body, we show qualitative results in Fig. 4 according to different representations. Qualitative results with the final fitted SMPL are given in the appendix.

Efficient body model fitting.On both 3DPW and EMDB, fitting SMPL to the nonparametric estimation (Sec. 3.3) generally reduces the error, but the difference is small, indicating that the original predictions already have high quality. To measure the fitting speed, we consider the task of fitting SMPL-X to vertices derived from SMPL (model transfer) using sample data from the SMPL-X website (33 meshes). The official code for the transfer takes 33 minutes with mean vertex error 5.0 mm. Our algorithm takes 28.4 ms with error 7.8 mm, i.e., orders of magnitude faster, and error increase is negligible relative to the typical prediction error of centimeters.

Limitations.NLF works frame-by-frame without temporal cues, and estimates each person independently, so highly-overlapping people remain challenging. Further, our predictions may have self-intersections and the absolute distance may be inaccurate due to depth/scale ambiguity.

Broader social impact.Human pose and shape estimation can help advance assistive technologies, human-robot interaction or interactive entertainment. However, like other vision methods, a potential for misuse exists in e.g., illegitimate surveillance. We will release models for research only.

## 5 Conclusion

We proposed _Neural Localizer Fields_ to upgrade 3D human pose estimators from fixed joint sets to predicting any point of the body surface and volume. We store point-localizer functions in a neural field, as opposed to storing explicit convolutional weights for every joint the network can predict. This enables us to train a large-scale generalist pose and shape estimation model, without any effort on relabeling all data to one format. Instead, we can supervise our model with any points that happen to be annotated for particular training examples. Besides this training advantage, we can choose at runtime which points in the volume to predict, allowing us to output any desired format. To make our output even more useful in downstream applications, we proposed an efficient points-to-parameters fitting algorithm to obtain SMPL parameters. Our model trained on several datasets with different formats achieves state-of-the-art performance with significant improvement over baselines on several benchmarks. Our model exhibits good performance on outdoor, indoor and synthetic data, regardless of the skeleton/mesh format used for different benchmarks. With the recent trend of more datasets from different sources being available, NLF paves the way to leverage these elegantly and efficiently.