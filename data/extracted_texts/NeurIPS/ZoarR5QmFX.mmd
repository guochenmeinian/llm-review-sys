# Concentrate Attention:

Towards Domain-Generalizable Prompt

Optimization for Language Models

 Chengzhengxu Li\({}^{1}\), Xiaoming Liu\({}^{1,*}\), Zhaohan Zhang\({}^{2}\), Yichen Wang\({}^{3}\),

**Chen Liu\({}^{1}\), Yu Lan\({}^{1}\), Chao Shen\({}^{1}\)**

\({}^{1}\)Faculty of Electronic and Information Engineering, Xi'an Jiaotong University

\({}^{2}\)Queen Mary University of London, London, UK \({}^{3}\)University of Chicago

\({}^{*}\) Corresponding author

{czx.li, lcoder}@stu.xjtu.edu.cn

{xm.liu, ylan2020, chaoshen}@xjtu.edu.cn

zhaohan.zhang@qmul.ac.uk yichenzw@uchicago.edu

###### Abstract

Recent advances in prompt optimization have notably enhanced the performance of pre-trained language models (PLMs) on downstream tasks. However, the potential of optimized prompts on domain generalization has been under-explored. To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (_i_) Prompts gaining more attention weight from PLMs' deep layers are more generalizable and (_ii_) Prompts with more stable attention distributions in PLMs' deep layers are more generalizable. Thus, we offer a fresh objective towards domain-generalizable prompts optimization named "Concentration", which represents the "lookback" attention from the current decoding token to the prompt tokens, to increase the attention strength on prompts and reduce the fluctuation of attention distribution. We adapt this new objective to popular soft prompt and hard prompt optimization methods, respectively. Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by 1.42% for soft prompt generalization and 2.16% for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance. The promising results validate the effectiveness of our proposed prompt optimization objective and provide key insights into domain-generalizable prompts. Our codes are available at https://github.com/czx-li/Concentrate-Attention

## 1 Introduction

Prompt optimization has emerged as a novel paradigm to effectively fine-tune pre-trained language models (PLMs), demonstrating impressive performance in natural language processing (NLP) tasks, especially under the few-shot setting Schick and Schutze (2020, 20), Liu et al. (2023). Unlike traditional fine-tuning methods requiring training and saving entire model parameters Devlin et al. (2018), prompt optimization aims to explore well-performed prompts automatically in discrete or continuous space as a context for model input, which boosts model performance on downstream tasks. The mainstream prompt optimization paradigms fall into two categories: _hard prompt optimization_ and _soft prompt optimization_. Hard prompt optimization relies on selecting well-performed prompts from a pre-constructed prompt set by filtering Jiang et al. (2020), Haviv et al. (2021), Davison et al. (2019) or gradient-free optimization method Li et al. (2024), Sun et al. (2023), Prasad et al. (2022). Meanwhile, soft prompt optimization searches continuous embedding as prompts via gradient information guided by task-specific loss function Vu et al. (2021), Li and Liang (2021).

However, while prompt optimization methods are becoming the mainstream of finetuning PLMs, the domain generalization ability of trained prompts still lacks exploration. Previous works Wu and Shi (2022); Zhao et al. (2022); Ge et al. (2023); Guo et al. (2022) attempt to employ domain adaptation methods to address these challenges. These works are based on the assumption of target domain availability. They align the source domain and target domain by unsupervised feature learning. The data reliance on these methods becomes a serious limitation for broader applications because models are frequently exposed to unknown domains. Another branch to enhance the versatility of prompts is pre-training. Gu et al. (2021) pre-trains prompts with 10 GB textual data. Vu et al. (2021) uses three tasks across eight datasets for pre-training to obtain transferable prompts. As reported by Liu et al. (2024), it requires 25-30 hours for pre-training prompts with Roberta-base on a single NVIDIA A100. The inefficiency and high computational cost remain a stumbling block for these methods to be widely used. More importantly, the aforementioned methods are parameterized and not applicable to hard prompt optimization, showing low readability. More studies refer to Appendix A.

Recognizing the problems mentioned above, we focus on improving the domain generalization ability of prompts with three constraints: (_i_) do so with no knowledge about the target domain, (_ii_) do so with little training cost, (_iii_) do so with easy adaptation on both soft prompt and hard prompt optimization. To get started, we test the popular prompt optimization methods on cross-domain setting (_i.e._, training prompts on one domain and testing them on out-of-distribution target domain1) and show the results in Figure 1. Interestingly, these optimized prompts exhibit (_i_) great performance drop in general (by an average of 8.49%) on target domain, validating the existence of research gap mentioned above, (_ii_) different domain generalization ability in particular (Acc. drops by 2.61% in best case and by 18.64% in worst case), indicating the existence of distinct prompt "nature" that contributes to its generalizability.

Since prompts are functional in the model inference stage in which the model looks up contexts to generate new tokens through the attention mechanism, we probe the attention pattern on prompts during forward propagation with the question "_what nature do well-generalized prompts have?_" and get the following findings (\(\)) via pilot experiments (SS3):

\(_{1}\): Prompts gaining _more attention weight_ from PLMs' deep layers are more generalizable.

\(_{2}\): Prompts with _more stable attention distributions_ in PLMs' deep layers generalize better.

Hence, we propose the idea of _Concentration_, representing the capability of prompts to get the attention stably from PLMs. We suggest that the concentration indicates the domain generalization ability for prompts, which can be a forebode ahead of the downstream tests.

With the principle of concentration SS3, we propose two algorithms that could piggyback upon popular prompt optimization methods for both hard and soft prompts to improve the domain generalization ability of prompts. In the parameterized optimization process of soft prompt SS4.1, where the loss function acts as objective, we introduce the concentration-reweighting loss. It minimizes the attention weight on the original input sequence, so as to make the model concentrate on prompts stably for different inputs. In the non-parameterized optimization process of hard prompt SS4.2, where the prompt set is first filtered and matched with different inputs by trained agents, we propose the concentration-oriented metric and reward. They aim to filter out and match the input with concentration-worthy hard prompts. Experiments show that our method respectively improves the target domain accuracy by 1.42% and 2.16% over the soft prompt and hard prompt optimized comparison methods, while maintaining in-domain capability.

Figure 1: Domain generalization capabilities across various prompting methods (ICL Brown et al. (2020); RL Deng et al. (2022); Soft Lester et al. (2021)) in sentiment classification tasks.

Preliminary

This section briefly introduces definitions of the Multi-source Few-shot Domain Generalization (MFDG) problem, which is the primary application scenario of our work.

**MFDG Setting.** A text classification task, _e.g._, sentiment classification, is defined as \(:\), where \(\) is the task's label space and \(\) is the feature space. We denote \(M(X)\) to be the marginal distribution over \(\), and \(P(Y)\) to be the prior distribution over \(\). The domain is then defined by \(_{}=\{,M(X),P(Y),(Y|X)\}\). Under the domain generalization setting, the source task is the same as the target task, _i.e._, \(_{s}\) equals to \(_{t}\). But for the source domain \(_{_{s}}\) and target domain \(_{_{t}}\), at least one of the underlying probability distribution, _i.e._, \(M(X)\), \(P(Y)\), or \(P(Y|X)\), is different.

In our MFDG problem, the training set is sampled from \(N\) source domains \(_{}\{_{_{s}}^{n}\} _{n=1}^{N}\) and the model is tested on an unknown target domain \(_{}_{_{t}}\). Also, we follow Perez et al. (2021) to simulate the few-shot learning setting, which means \(|_{}||_{}|\).

**MFDG Objective.** Traditional prompting methods often rely on a crucial assumption that the training and testing sets come from the same underlying distribution \(_{},_{}_{_{t}}\). In this context, the objective of prompting is to optimize high-quality prompt \(z^{*}\) that maximizes the expected metric of the prediction on the target domain \(_{_{t}}\):

\[z^{*}=_{z}_{(x,y)_{_{t}}}[r( y,p_{}(z x))],\] (1)

where \(r\) is a function that evaluates the quality of the predicted answers when using the prompts \(z\). For MFDG, the optimization objective is:

\[z^{*}=_{z}_{_{_{t}}} [_{(x,y)_{_{t}}}[r(y,p_{}(z x))]],\] (2)

where \(\) is the set of unknown target domains. In a nutshell, Eq. 1 searches the prompts well-performed within the known domain, while Eq. 2 explores the prompts that perform well across unknown domains.

## 3 Concentration Benefits Generalization

In this section, we present pilot experiments to analyze the correlation between domain generalizability and attention concentration of prompts using RoBERTa-Large Liu et al. (2019) as the backbone. Appendix C.1 shows the specific form of prompts used in the pilot experiment. From the effect of prompts in forward propagation, we analyze (\(i\)) how much each prompt is concentrated by the LM, and (\(ii\)) how stable the concentration is to formulate the correspondence.

**Background.** Attention mechanisms are widely studied for PLM interpretability Wang et al. (2022); Clark et al. (2019); Lin et al. (2019); Htut et al. (2019). As for prompt optimization, Wang et al. (2023) provide insights that label words in in-context learning aggregate most of the attention weights in deep layers of PLM, which majorly determine the final prediction. Inspired by this, we further explore the attention weight on the whole prompt sequence and its impact on prompt generalizability from a global perspective.

**Definition 3.1**.: Let \(z=(z_{1},z_{2},...,z_{L})\) and \(x=(e_{1},e_{2},...,e_{T})\) be prompt and original input with \(z,x S\), where \(S\) is the set of all possible textual sequences over the vocabulary. Let \(f_{_{l}}\) be the attention block2 in layer \(l\) of a PLM parameterized by \(_{l}\). Then _concentration_ is a function Concentration \(:S^{+}\)

\[(z x;_{l})=_{z_{i} z}f_{_{l}}(z_{i}  x).\] (3)

Heuristicly, _concentration_ represents the "lookback" attention from current decoding token to prompt tokens, as shown in Figure 2.

**Definition 3.2**.: Let \(z=(z_{1},z_{2},...,z_{L})\) and \(x=(e_{1},e_{2},...,e_{T})\) be prompt and original input with \(z,x S\), where \(S\) is the set of all possible textual sequences over the vocabulary. Let \(=(x_{1},x_{2},...,x_{M})\) be the input dataset. Let \(f_{_{l}}\) be the attention block in layer \(l\) of a PLM. Then _concentration strength_ is a function \(:^{+}\)

\[((z,);_{l})=|}_{x_{i }}(z x_{i};_{l}).\] (4)

_Concentration strength_ represents the average concentration across the input dataset.

**Definition 3.3**.: Let \(=(x_{1},x_{2},...,x_{M})\) be the set of textual sequences sampled from target domain \(_{_{t}}\), where \(x_{i} S\). Then the _concentration fluctuation_ is a function \(:^{+}\)

\[((z,);_{l})=|} _{x_{i}}[(z x_{i};_{l}) )-((z,);_{l})]^{2}}.\] (5)

_Concentration fluctuation_ demonstrates the variance of concentration strength for different inputs.

Our pilot experiment unveils following insights: (_i_) Prompts with larger Concentration Strength achieve better performance in domain generalization. For instance, Figure 3(left) shows that tokens of \(^{}\), the best-performed method, gain more than 0.8 of Concentration Strength at the 21st layer and over 0.7 at the 23rd layer. (_ii_) Prompts with lower Concentration Fluctuation tend to generalize to target domain better. As shown in Figure 3(right), \(^{}\) and \(^{*}\) are concentrated at a similar level, but \(^{*}\) generalizes better while its stability is better. (_iii_) High Concentration Strength and low Concentration Fluctuation together contribute most to prompt generalizability. The best-performed \(^{}\) has most Concentration Strength and lowest Concentration Fluctuation across all comparison prompts. These discoveries inspire us to adjust the objective for soft prompt\(\$4.1\) and hard prompt\(\$4.2\) optimization towards increasing Concentration Strength while decreasing Concentration Fluctuation.

Figure 3: Left: concentration strength of various prompting methods in the last 5 layers (layers 19 to 23). Right: boxplots of the concentration strength in the last layer. Overall, prompts that exhibit good domain generalization gain higher concentration strength and lower concentration fluctuation. The concentration strength of each layer is shown in Appendix C.2.

Figure 2: Illustration of Concentration. The tokens in the blue square are prompt, and those in yellow are input sequences. Concentration represents the model’s attention on prompt tokens in forward pass when decoding <mask> token.

Concentrative Prompt Optimization

### Concentrative Soft Prompt Optimization

To devise soft prompt optimization with the guidance of concentration, we first visit the optimization objective of mainstream methods (_e.g._, prompt tuning Lester et al. (2021), prefix tuning Li and Liang (2021), p-tuning v2 Liu et al. (2021)).

These methods optimize follow log-likelihood objective given a trainable prompt \(z\) and a fixed PLM parameterized by \(\) for the input \(x\):

\[_{z}P(y|(z x);).\] (6)

According to our findings in SS3, domain-generalizable prompts should be high in concentration strength and low in concentration fluctuation. Thus, we reformulate Eq. 6 to get the objective for domain-generalizable prompts:

\[_{z}(P(y|(z x);)+((z,_{ });)) s.t.\ _{z}((z,_{});).\] (7)

Towards the reformulated objective above, we propose the concentration-reweighting loss for soft prompt optimization methods. The framework for soft prompt optimization is shown in Figure 4. First, we minimize the concentration strength on input \(x\) to improve concentration strength on prompt \(z\) by designing loss function \(_{}\) as:

\[_{}=1-((z,_{}); ).\] (8)

In addition, to reduce concentration fluctuation of prompts, we propose to use every token's concentration strength as hidden state feature of prompts, denoted as \(_{i}=(c_{1},c_{2},...,c_{L})\) where \(L\) is the length of prompts. We design a contrastive loss to cluster \(\) with same label together to reduce concentration fluctuation:

\[_{}=_{i=1}^{|_{}|}_{p P(i)}_{i},_{p})/)}{ _{j=1}^{|_{}|}_{i j}(sim(_{i},_{j})/)},\] (9)

where \(P(j)\) represents the input with the same label as the \(j\)-th input in the dataset \(_{}\), \(sim(.)\) is used to calculate the cosine similarity between feature embeddings, \(_{i j}\) is an indicator function, _i.e._, \(_{i j}\{0,1\}=1\) if and only if \(i j\), and \(\) is a temperature parameter used to adjust the scale of the similarity score.

Also, we utilize the cross-entropy classification loss \(_{}\) Mao et al. (2023). The concentration-reweighting loss for soft prompt optimization is formulated as:

\[_{}=_{}_{}+_{ }_{}+_{}_{},\] (10)

where \(_{}\), \(_{}\) and \(_{}\) weights different losses in training process. More details are in Appendix D.

### Concentrative Hard Prompt Optimization

In contrast to soft prompt optimization, hard prompt optimization searches suitable prompt in discrete space in a non-parameterized fashion. Previous hard prompt optimization searches can be divided into distribution-level Prasad et al. (2022); Deng et al. (2022) and input-level Li et al. (2024); Lu et al. (2022). Although distribution-level prompt optimization can generally improve reasoning ability, motivated by the fact that no prompt is perfect for all inputs Sun et al. (2023), we focus on improving the generalization ability of input-level optimization methods. Generally, the mainstream of input-level optimization technique for hard prompts could be encapsulated as: **filter** (by **metric**) and **match** (by **RL agents**). The findings of concentration could be applied to this optimization process by adjusting filter metric and agent reward. We illustrate the framework for hard prompt optimization in Figure 5.

Figure 4: Framework for Soft Prompt Optimization.

**Filter Metric.** For previous filter metric only considering the overall accuracy on training set, we introduce a new metric called Global Concentration Score (GCS), which involves our ideas of concentration strength and concentration fluctuation.

Towards optimization objective Eq.7, we use concentration strength as first metric to filter out prompts which cannot get much concentration from model. Metric for reducing concentration fluctuation could be regarded as minimizing Kullback-Leibler (KL) divergence between the concentration features \(_{i}\) of input with same label and the average of \(_{i}\) on whole inputs set \(_{}\):

\[M_{}(z,_{})=_{y}_{i _{}(y)}((_{i}) (_{}^{y})),\] (11)

where \(\) is label space and \(_{}(y)\) is the input set labeled \(y\) in data set \(_{}\). Also, we follow the setting of Li et al. (2024) calculating the difference of the probability \(p_{LM}\) that the \(x_{i}\) is correctly labeled \(y_{}\) and wrongly labeled as \(y_{}\) by a base PLM to improve the overall accuracy:

\[M_{}(z,_{})=_{x_{i}_{ {train}}}(p_{}(y_{}|z x_{i})-p_{}(y_{ {false}}|z x_{i})).\] (12)

Finally, we combine the above three metrics as one comprehensive metric, _i.e._, Global Concentration Score (GCS), to assess the quality of prompts:

\[(z,_{})=_{}M_{}(z, _{})+_{}((z, _{});)+_{}M_{}(z,_{ }),\] (13)

where \(_{}\), \(_{}\) and \(_{}\) are the weights that balance accuracy, concentration strength, and concentration fluctuation, respectively.

**Prompt Matching.** Previous methods mostly use a single RL agent to match appropriate prompts for each input Li et al. (2024); Lu et al. (2022); Sun et al. (2023). Due to the large prompt space, the effective exploration of reinforcement learning agents is limited Dulac-Arnold et al. (2019). Furthermore, in the MFDG setting, inputs from different domains often have different state spaces, action spaces, and reward scales, then using a single agent often leads to the strategy converging to sub-optimality. To overcome these challenges, we redefine the discrete prompt matching problem in the MFDG setting as a multi-agent reinforcement learning (MARL) problem and propose a new matching algorithm.

We build our matching algorithm based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm Yu et al. (2022). Specifically, we configure one reinforcement learning (RL) agent for each source domain, collectively forming a multi-agent ensemble \(=\{1,2,,N\}\). In order to effectively share learning experience in different domains, all agents share the same value network \(v_{}(.)\) while having independent strategy networks \(\{_{_{n}}(.)\}_{n=1}^{N}\), where \(\) and \(_{n}\) are the learnable parameters. Also, we define a set of prompts \(^{n}\) for each source domain, which serves as the action space for the corresponding RL agent. These prompts can come in various forms, including manual prompts Bach et al. (2022), original training inputs Brown et al. (2020); Dong et al. (2022), or LLM-generated Li et al. (2024); Lu et al. (2021). Here, an action \(a^{n}\) implies that agent \(n\) selects a specific prompt \(z^{n}\) from its designated prompt set \(^{n}\).

At each step \(t\) of the training phase, given a state \(s_{t}^{n}=(x_{t})\), which is the last hidden layer embedding of input \(x_{t}\), the \(n\)-th agent selects an action \(a_{t}^{n}\) by policy \(_{_{n}}(a_{t}^{n}|s_{t}^{n})\). This action corresponds to choosing prompt \(z_{t}^{n}\). We combine \(x_{t}\) and \(z_{t}^{n}\), feed them into the PLM for downstream tasks, and calculate the reward \(r_{t}^{n}\). The agent's parameters are then optimized based on \(r_{t}^{n}\).

The rewards received by the RL agent are used as feedback to directly guide the optimization direction of the strategy. In this work, we aim to ensure that the prompts selected by the RL agent have good generalization capabilities. Therefore, we reuse \((.;)\) as a part of our reward function, specifically \(r_{t}^{n}\) is defined as:

\[r_{t}^{n}=_{}M_{}(z_{t}^{n},\{x_{t}\})+_{}(z,\{x_{t}\}\,;).\] (14)

Figure 5: Framework for Hard Prompt Optimization.

In the testing phase, we use an ensemble decision-making approach to apply the prompts. The prompts selected by each agent are input into the PLM to perform downstream tasks, and the results are combined. For a given input \(x\) and its corresponding selected prompts \(\{z^{n}\}_{n=1}^{N}\), the final prediction obtained by PLM for label \(y\) can be expressed as:

\[P(y|x)=(_{n=1}^{N}p_{}(y|x,z^{n})).\] (15)

Our intention is to divide the action space of agents into smaller, more manageable subspaces and make it easier for agents to make the best decisions. The detailed training and testing processes, along with specific agent settings, are presented in Appendix E.

## 5 Experiments

To demonstrate the effectiveness of our findings for domain generalization, we conduct extensive experiments on tasks of sentiment classification and natural language inference (NLI). We select the SST-2 Socher et al. (2013), MR Pang and Lee (2005), and CR Hu and Liu (2004) datasets for sentiment classification, and the WNLI, QNLI, and RTE datasets from GLUE Wang et al. (2018) for NLI tasks3. Each task involves designating one dataset as the target domain and the others as source domains. Detailed descriptions of the datasets and domain divisions are provided in Appendix B.1.

We choose RoBERTa-largeLiu et al. (2019) for all downstream tasks for our hardware resources, and it has been widely used in previous prompt optimization works Li et al. (2024), Deng et al. (2022), Zhang et al. (2022). Admittedly, at the time of writing this article, various efforts to optimize prompts have surfaced. However, our goal is not to build a better training method based on previous problems, but to pose a new problem, _e.g._, learning prompts with strong domain generalization ability. We therefore select three of the most well-known methods as baseline in the fields of soft prompt optimization and hard prompt optimization respectively. In addition, in order to more comprehensively demonstrate the performance of our method, we also select two distribution-level discrete prompt optimization methods as comparison methods. The baseline methods and their implementations are described in Appendix B.2 and B.3.

### Out-of-domain Performance Comparison

**Domain Generalization Result for Soft Prompt.** As shown in Table 1, the concentration strength loss \(_{}\) and the concentration fluctuation loss \(_{}\), in most experimental settings, enhance the domain generalization of three soft prompt optimization methods. And, the combination of \(_{}\) and \(_{}\), _i.e._, the concentration-reweighting loss \(_{}\), further improves the domain generalization ability of soft prompts, achieving the best results in all experimental settings. Specifically, \(_{}\) (both) boosts the average accuracy of Prompt Tuning, Prefix Tuning, and P-Tuning v2 by 1.47%, 1.78%, and 1.02%, respectively, highlighting its effectiveness in promoting the learning of domain-invariant properties in soft prompts. In addition, using only \(_{}\) or \(_{}\) alone may sometimes impair the performance of soft prompts, such as Prompt Tuning and P-Tuning v2 methods when QNLI data is used as the target domain. This indicates that concentration strength and concentration fluctuation are both indispensable for domain generalization ability of the prompts, and enhancing only one aspect may be harmful to the domain generalization performance of the prompts.

To more comprehensively illustrate the utility of the concentration-reweighting loss, we delve into Appendix D for complete concentrative soft prompt optimization algorithm, extensive exploration on performance stability to prompt initialization and utility to decoder-only models of our method. Additionally, we provide quantitative analysis and visual representation to illustrate the impact of concentration-reweighting loss \(_{}\) on soft prompts.

**Domain Generalization Result for Hard Prompt.** As shown in Table 1, with the introduction of filtering metric and prompt matching framework, our approach effectively enhances the domain generalization capabilities of various existing methods. Among them, the improvements to the DP\({}_{2}\)O method achieved the best performance in all experimental setups. Compared to the original DP\({}_{2}\)O, our method improve the average accuracy on sentiment classification and NLI tasks by 1.34% and 0.85% respectively. These results demonstrate the effectiveness of our proposed filtering metrics and surrogate rewards in selecting universal prompts from a pre-constructed set of prompts. Additionally, we find that compared to filtering metrics, the prompt matching framework brings a higher performance improvement to discrete prompts. This is because our reward function design adeptly guides the agent to match inputs with prompts that have strong cross-domain capabilities, even when faced with an unfiltered set of prompts. We also analyze our method from multiple aspects in Appendix E.

**Overall Comparison.** In the MFDG setting, hard prompts generally outperform soft prompts. As illustrated in Table 1, the best-performed hard prompt optimization method achieves a significant average accuracy of 73.88%, compared to only 64.61% for the best soft prompts. We hypnosis that hard prompts embed discrete tokens into the model input, providing precise guidance during testing and making it easy for PLMs to associate semantics fo input text sequence with the task. And soft prompts rely on indirectly influencing model inference by searching in continuous space with only the guidance of objective function, which might cause overfitting on source domain.

### In-domain Performance Comparison

We also compare the in-domain performance between our proposed optimization objective and traditional training objective. And we report not only model performance tested on in-domain

    & &  &  \\  _Pardigms_ & _Methods_ & S+M\(\)C & C+M\(\)S & S+C\(\)M & Q+R\(\)W & W+R\(\)Q & Q+W\(\)R \\   Prompt \\ Tuning \\ Lester et al. (2021) \\  } & Vanilla PT & 64.73\({}_{3.82}\) & 65.51\({}_{2.65}\) & 65.12\({}_{3.35}\) & 41.20\({}_{1.85}\) & 49.83\({}_{1.47}\) & 49.66\({}_{1.67}\) \\  & PT with \(_{cs}\) & 65.83\({}_{3.81}\) & 66.38\({}_{2.42}\) & 65.33\({}_{2.37}\) & 41.53\({}_{1.56}\) & 49.60\({}_{2.37}\) & 49.43\({}_{1.41}\) \\  & PT with \(_{cf}\) & 65.09\({}_{3.72}\) & 67.40\({}_{4.03}\) & 65.40\({}_{2.33}\) & 42.17\({}_{2.03}\) & 49.22\({}_{2.59}\) & 49.73\({}_{1.31}\) \\  & PT with both & **66.19\({}_{3.69}\)** & **69.54\({}_{2.52}\)** & **65.89\({}_{2.32}\)** & **42.48\({}_{1.72}\)** & **50.31\({}_{1.33}\)** & **50.42\({}_{1.34}\)** \\   Prefix \\ Tuning \\ Li and Liang (2021) \\  } & Vanilla Prefix & 65.91\({}_{3.24}\) & 83.25\({}_{0.41}\) & 75.51\({}_{0.91}\) & 50.26\({}_{0.31}\) & 51.88\({}_{0.29}\) & 50.02\({}_{0.28}\) \\  & Prefix with \(_{cs}\) & 66.23\({}_{3.37}\) & 84.32\({}_{0.48}\) & 76.58\({}_{0.82}\) & 50.69\({}_{0.33}\) & 51.44\({}_{0.28}\) & 49.77\({}_{0.22}\) \\  & Prefix with \(_{cf}\) & 66.82\({}_{3.19}\) & 83.70\({}_{0.39}\) & 77.17\({}_{0.22}\) & 51.73\({}_{0.32}\) & 52.12\({}_{0.28}\) & 50.73\({}_{0.26}\) \\  & Prefix with both & **68.29\({}_{2.97}\)** & **85.07\({}_{0.42}\)** & **77.53\({}_{0.43}\)** & **52.05\({}_{0.30}\)** & **53.32\({}_{0.25}\)** & **51.26\({}_{0.27}\)** \\   P-Tuning \\ v2 \\ Liu et al. (2021) \\ Pv2 with both \\ Pv2 with both \\  } & Vanilla PV2 & 65.92\({}_{1.61}\) & 83.84\({}_{1.69}\) & 75.89\({}_{0.36}\) & 50.63\({}_{0.31}\) & 52.76\({}_{1.61}\) & 51.31\({}_{1.37}\) \\  & Pv2 with \(_{cs}\) & 66.06\({}_{1.77}\) & 83.32\({}_{1.59}\) & 57.07\({}_{0.35}\) & 51.37\({}_{0.37}\) & 50.93\({}_{0.92}\) & 50.20\({}_{1.30}\) \\  & Pv2 with \(_{cf}\) & 67.21\({}_{6.52}\) & 84.12\({}_{1.51}\) & 76.41\({}_{0.32}\) & 51.32\({}_{0.38}\) & 52.64\({}_{1.94}\) & 51.28\({}_{1.22}\) \\  & Pv2 with both & **67.07\({}_{1.83}\)** & **84.56\({}_{1.42}\)** & **77.26\({}_{0.37}\)** & **51.87\({}_{0.28}\)** & **53.83\({}_{0.95}\)** & **51.57\({}_{1.16}\)** \\    &  &  &  &  &  &  &  \\ Prasad et al. (2022) & - & 80.07\({}_{2.57}\) & 84.28\({}_{1.38}\) & 85.19\({}_{1.12}\) & 54.37\({}_{2.40}\) & 52.77\({}_{1.73}\) & 53.52\({}_{1.66}\) \\   Manual \\ Prompt \\ Bach et al. (2022) \\  } & Vanilla MP & 52.73\({}_{4.43}\) & 55.81\({}_{3.31}\) & 50.85\({}_{1.58}\) & 41.70\({}_{1.17}\) & 50.80\({}_{0.84}\) & 51.60\({}_{1.50}\) \\  & MP with MARL & 56.37\({}_{1.18}\) & 58.42\({}_{0.46}\) & 52.15\({}_{0.42}\) & 44.27\({}_{1.02}\) & 51.36\({}_{0.84}\) & 52.18\({}_{1.23}\) \\  & MP with Metric & 54.63\({}_{2.12}\) & 57.84\({}_{1.65}\) & 51.79\({}_{1.75}\) & 42.86\({}_{0.94}\) & 51.02\({}_{0.68}\) & 52.03\({}_{1.14}\) \\  & MP with both & **56.76\({}_{0.40}\)** & **59.44\({}_{0.32}\)** & **53.15\({}_{0.35}\)** & **45.05\({}_{0.28}\)** & **52.03\({}_{0.25}\)** & **52.46\({}_{1.24}\)** \\   In-Context \\ Demo \\  } & Vanilla IC & 84.33\({}_{2.15}\) & 84.81\({}_{1.39}\) & 80.21\({}_{1.27}\) & 50.86\({}_{1.23}\) & 52.63\({}_{0.94}\) & 58.04\({}_{2.23}\) \\  & IC with MARL & 85.35\({}_{3.01}\) & 87.02\({}_{2.24}\) & 82.14\({}_{1.65}\) & 52.82\({}_{dataset, but also the average gap between performance on in-domain and out-of-domain data. As shown in Table 2, our method shows comparable accuracy with prompt optimization methods aiming only at maximizing log probability on correct label, demonstrating that taking concentration into consideration does not compromise on model performance on in-domain data. Moreover, prompts optimized by concentration-driven objective shows better consistency when tested on both in-domain and out-of-domain data. Especially, for hard prompt optimization which searches for suitable prompts in a limited discrete space, the average performance gap is less than 1%, indicating our method always matches input sequence with proper prompts even if the prompts are not initially designed on target domain.

### Applicability to Larger Models and Other Tasks:

We also attempt to extend our method to larger models and more complex tasks. We validate the effectiveness of our method on Llama-2-7b-chat Touvron et al. (2023), Vicuna-7b-v1.5 Zheng et al. (2023), and Alpaca-7b-wdiff Taori et al. (2023) models for improving domain generalization ability of Prefix Tuning and In-Context Demo on question-answering tasks. We evaluate our method on ROC, SCT, and COPA datasets from the TRAM Benchmark Wang and Zhao (2023) (referred as R, S, and C for simplicity), covering multiple choice question answering (MCQA) in reading comprehension and commonsense reasoning. The result is shown in Table 3.

    &  &  \\  _Pardigms_ & _Methods_ & SST-2 & CR & MR & Avg. & WNLI & QNLI & RTE & Avg. & Avg Gap \\  Prompt & Vanilla PT & 73.84,352 & 75.89,172 & 74.17,23 & 74.63 & 47.64,102 & 49.71,093 & 54.73,12 & 50.69 & +6.66 \\ Tuning & PT with both & 72.61,72 & 76.07,24 & 74.37,12 & 74.35 & 46.79,152 & 49.50,084 & 54.21,49 & 50.17 & +4.71 \\  Prefix & Vanilla Prefix & 87.39,298 & 77.37,079 & 82.65,068 & 82.47 & 55.88,037 & 60.72,044 & 54.82,031 & 56.99 & +6.93 \\ Tuning & Prefix with both & 87.29,12 & 76.73,128 & 83.32,083 & 82.45 & 56.18,035 & 59.74,032 & 55.38,42 & 57.10 & +5.19 \\  P-Tuning & Vanilla Pv2 & 86.71,137 & 77.65,18 & 82.74,042 & 82.21 & 55.57,037 & 60.73,164 & 55.16,183 & 57.15 & +6.29 \\ V2 & Pv2 with both & 87.03,032 & 77.11,800 & 27.05,054 & 82.08 & 56.13,060 & 60.43,152 & 55.20,190 & 57.32 & +5.38 \\  Manual & Vanilla MT & 61.62,42 & 57.75,253 & 53.13,735 & 57.50 & 44.27,290 & 53.42,098 & 52.63,060 & 50.11 & +3.22 \\ Prompt & MP with both & 61.33,32 & 56.07,161 & 53.47,254 & 56.96 & 44.05,089 & 53.71,35 & 52.60,190 & 50.14 & +0.40 \\  In-Context & Vanilla IC & 85.91,142 & 85.57,092 & 83.75,199 & 85.08 & 52.37,153 & 53.42,072 & 59.73,081 & 55.17 & +1.65 \\ Demo & IC with both & 86.33,14 & 85.14,037 & 84.31,122 & 85.26 & 52.25,162 & 52.96,049 & 59.36,173 & 54.86 & +0.93 \\  _{2}\)} & Vanilla DP\({}_{2}\)O & 93.62,72 & 90.76,301 & 88.58,391 & 90.99 & 55.26,160 & 55.13,393 & 61.07,081 & 57.15 & +1.44 \\  & DP\({}_{2}\)O with both & 93.20,81 & 90.38,47 & 88.37,12 & 90.65 & 56.47,041 & 55.42,079 & 61.29,053 & 57.73 & +0.37 \\   

Table 2: In-domain comparison. The last column shows the average gap between test performance on in-domain and out-of-domain data.

    &  & \)} \\   & & & & & & \\    & \)} & \(\) & \(\) & \(\) & \(\) \\   & Vanilla Prefix with both & 62.32,155 & 66.30,230 & 73.15,253 & 67.26 & — \\  & Prefix with both & 63.70,196 & 68.47,097 & 75.32,109 & 69.16 & +1.90 \\  & Vanilla IC & 63.13,125 & 65.50,198 & 77.59,114 & 68.74 & — \\  & IC with both & 65.13,103 & 68.33,123 & 79.83,088 & 70.10 & +1.36 \\   & Vanilla Prefix & 67.72,19 & 81.09,177 & 88.97,264 & 79.26 & — \\  & Prefix with both & 68.75,104 & 83.93,179 & 89.76,260 & 80.81 & +1.55 \\  & Vanilla IC & 68.37,224 & 83.23,122 & 90.98,199 & 80.86 & — \\  & IC with both & 69.67,158 & 85.50,066 & 93.39,123 & 82.85 & +1.99 \\   & Vanilla Prefix & 61.52,79 & 70.03,288 & 87.91,273 & 73.15 & — \\  & Prefix with both & 63.89,293 & 72.15,077 & 89.58,281 & 75.21 & +2.06 \\   & Vanilla IC & 60.81,144 & 69.11,1246 & 89.66,237 & 73.19 & — \\   & IC with both & 63.16,156 & 70.57,195 & 91.19,200 & 74.97 & +1.78 \\   

Table 3: Performance comparison of large models on MCQA task accuracy. The last column shows the average gap between test performance on vanilla method and our method.

Experimental results show that our method significantly improves the performance of large models on question-answering tasks across multiple domain generalization settings. For instance, for the Llama-7b model, our method improved the average accuracy of soft prompt generalization and hard prompt generalization comparisons by 1.90% and 1.36%, respectively; similar improvements were observed for Vicuna-7b and Alpaca-7b models, ranging from 1.55% to 1.99% and 2.06% to 1.78% respectively.

Additionally, we would also like to discuss _"why our method works well for large generative language models?"_. In Appendix F, we present the Concentration Strength Distribution of prompts using In-Context Demo across three 7B-sized language models (Llama, Vicuna, Alpaca) on three different tasks (SA, NLI, MCQA). We observe that all three LLMs exhibit stronger concentration strength in deeper layers compared to shallower layers when confront with prompts for different tasks. We find that this phenomenon occurs earlier in larger models (7B) compared to smaller models like Roberta-large. We speculate that this behavior is related to the alignment stage in pre-training of large models during Supervised Fine Tuning with a large number of prompts.

## 6 Conclusion

In this paper, we explore the nature of prompts with good domain generalization ability. By conducting experiments on model concentration on prompts and concentration pattern stability, we find that well-generalized prompt attract more attention weights at deeper layers of pre-trained language models (PLMs) and this pattern stably exists to different inputs. Inspired by these new findings, we propose optimization methods for soft prompt and hard prompt, respectively. For soft prompts, we design a concentration-reweighting loss to search for prompts with strong domain generalization ability in continuous space. For hard prompts, we develop an attention-weighted filter-then-match framework. This framework first apply a novel metric which takes model concentration and pattern stability into consideration to filter out low-quality prompts in candidate set. Then a multi-agent reinforcement learning method is used to match each input with optimized hard prompts from each source domain. Our extensive experiment on multiple datasets in different tasks demonstrates the superiority of our methods over existing comparison prompt optimization methods in terms of MEDG setting.

## 7 Limitations

In this study, we primarily focused on the performance of domain-generalizable prompt optimization. Despite this, our research still faces limitations in some practical application scenarios. Firstly, our pilot experiments only covered a limited variety of prompts. In future studies, we plan to extend to more diverse types of prompts. Secondly, the current research mainly focuses on the prompt domain generalization capabilities in a small-sample environment; next, we will conduct more comprehensive performance evaluations on complete datasets. Additionally, our current discrete prompt optimization method is primarily applicable at the input-level; in the future, we plan to explore its potential applications at the distribution-level. Finally, although our method is designed to enhance the performance of PLMs in classification tasks, these methods cannot be directly applied to open-ended generation tasks.