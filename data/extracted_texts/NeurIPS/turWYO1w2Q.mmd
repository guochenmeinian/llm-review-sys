# Information Directed Tree Search Reasoning and Planning with Language Agents

Yash Chandak, Hyunji Alex Nam, Allen Nie, Jonathan Lee and Emma Brunskill

Department of Computer Science

Stanford, CA

<ychandak@stanford.edu>

###### Abstract

Solving challenging tasks often require agentic formulation of language models that can do multi-step reasoning and progressively solve the task by collecting various feedback. For computational efficiency, it may be advantageous to quantify the information associated with different feedback and guide the search such that the solution can be obtained quickly. To explore this possibility, we take a Bayesian approach and propose an _information directed tree search_ (IDTS) algorithm that makes use of in-context learning to approximate the information associated with different feedback. We explore the effectivity of IDTS on challenging tasks involving programming, formal math, and natural language. Interestingly, while we find advantages over simple uniform search methods, the proposed approach is about comparable to MCTS even though it explores different paths. We discuss some possibilities for our findings and highlight open questions for future work.

## 1 Introduction

Large language models (LLMs) have become integral for building autonomous agents that aim to find solutions to challenging tasks. Many such tasks often require multi-step reasoning, are not well specified, or require hierarchical decomposition (Hao et al., 2023; Zelikman et al., 2022). In such cases, even though LLMs cannot directly provide the complete answer, they can provide partially correct responses, or answer sub-parts of the prompt. Such responses can be then paired with other source(s) of feedback that can subsequently guide the reasoning of the language models. For instance, if the generated response is a code, then the feedback can be provided by program compilers and unit testing (Zhang et al., 2023; Zhou et al., 2023). If the generated response is formal proofs for mathematical statements, then auto-verifiers can be used (First et al., 2023; Zheng et al., 2023). Further, language models can also be used to self-critique their responses (Zhou et al., 2023).

However, such sources of feedback vary in terms of their quality. For instance, compilers can only flag incorrect code - they cannot tell what to change in that incorrect code. In contrast, while language model critics can suggest what to change, they might often hallucinate and provide inaccurate feedback. Similarly, humans may want to work with a language model to achieve a goal but may not know themselves what the right steps to reach that goal are. This raises the main question of interest: How do we leverage partially correct response generating language models, with partially correct source(s) of feedback, to find solution to important problems?

We cast this as a planning problem with partially correct feedback, and design a new tree search procedure for inference time planning. To be computationally efficient when resources are limited, unlike MCTS (Coquelin and Munos, 2007; Kocsis and Szepesvari, 2006) that does a naive countbased exploration, we take a Bayesian approach and prioritize search towards feedback that provides higher _information gain_ towards the solution.

Problem Statement:Let \(o\) and \(a\) correspond to an observation and an action. let each observation consist of some context \(x\) and a reward \(r\), such that \(o=(x,r)\). Let \(O_{i}=(X_{i},R_{i})\) and \(A_{i}\) be the random variables corresponding to observation and action at interaction step \(i\). Specifically, the context \(X_{0}\) in the initial observation \(O_{0}\) contains the question, and the action \(A_{i}\) is an agent's attempt at the solution and \(X_{i+1}\) contains rich feedback on that action. The goal of the agent is to determine an answer \(Y\) to that question within a fixed number of interactions. For example, for coding tasks, \(X_{0}\) is a coding question, \(A_{i}\) is the language model's code, \(Y\) is a correct code solution, and \(O_{i+1}\) contains the error messages from compiler and results from unit tests for \(A_{i}\).

We define a state \(s\) to be the history of the interaction so far, i.e., \(S_{i}(O_{0},A_{0},O_{1},A_{1},,O_{i})\). Let \(:()\) be a policy and let \(^{}(s)\) correspond to the trajectory unrolled using policy \(\) starting from a state \(s\). In this work, we build upon pretrained LLMs and thus implicitly leverage side information (e.g., data on the internet) to find the solution \(Y\). Let \(\) denote such data. Let the random variable \(_{t}\) be the tree containing everything observed till the end of the \(t^{}\) iteration. With a slight abuse of notation, we will use a subscript of \(t\) to denote implicit conditioning on everything observed so far, i.e., \(_{t}\) and \(D\). For example, \(_{t}(Y=y|x)(Y=y|x,_{t},D)\).

Information Directed Sampling:Given random variables \(X\) and \(Y\), the information gain for \(Y\) on observing \(X\) is \(_{t}(Y;X)_{t}(Y)-_{t}(Y|X)\), where \(_{t}(Y)-_{y}_{t}(y) _{t}(y)\) is the entropy and \(_{t}(Y|X)-_{x}_{t}(x)_{y }_{t}(y|x)_{t}(y|x)\) is the conditional entropy.

In the bandit setting, let \(Z(A)(O,A)\) be the rich observation (potentially including reward), and action \(A=a\) on executing \(a\). For the optimal action \(a^{*}\), let \(_{t}(a^{*};Z(a))\) corresponds to the information gain towards the agent's belief over the optimal action \(a^{*}\) given \(Z(a)\). Similar to Bayesian experimental design (Rainforth et al., 2024), information directed sampling(Russo and Van Roy, 2014) or its \(\)-regularized version (Hao and Lattimore, 2022) selects an action such that it maximizes the performance as well the information gained by the agent,

\[a*{arg\,max}_{a}_{t}(a^{*};Z(a ))}{_{t}[r(a^{*})-r(a)]^{2}}, a*{arg\,max }_{a}\ r(a)+\,_{t}(a^{*};Z(a)).\] (1)

Monte-Carlo Tree Search:A popular planning procedure is MCTS that tracks \(N_{t}(s,a)\) and \(N_{t}(s)\), i.e., the number of times a particular \((s,a)\) pair and \((s)\) has been chosen during tree traversal till iterate \(t\). Let \(_{t}\) be the tree expanded at the start of a given iterate \(t\). At every iterate \(t\), one of the nodes of \(_{t}\) is chosen to be expanded by selecting action \(a\) for a given state \(s\). Most MCTS algorithms are based on the seminal upper confidence tree (UCT) algorithm (Kocsis and Szepesvari, 2006; Coquelin and Munos, 2007) that chooses

\[a*{arg\,max}_{a}\{_{t}(s,a)+c(s)}{N_{t}(s,a)}}\},\] (2)

where \(\) is the estimate of the cumulative return (Sutton and Barto, 2018), and the term in blue guides the exploration. While (2) provides one approach to balance exploration and exploitation based on counts, not only such discrete notion of counts fail to account for similarities between different states, it also fails to incorporate richer notion of explorations, e.g., even if an agent has taken an action

  & **LLMs** & **Planning** & **Robust** & **Rich Exploration** \\  One-shot generation & ✓ & ✗ & ✗ & ✗ \\ Iterative Refinement/CoT & ✓ & ✓ & ✗ & ✗ \\ Search (MCTS, best-first, etc.) & ✓ & ✓ & ✓ & ✗ \\ IDTS (Proposed) & ✓ & ✓ & ✓ & ✓ \\ 

Table 1: Methods that have single chain of thought are often not robust when the feedback is only partially correct. In contrast, search based methods maintain several solutions and corresponding feedback, and are thus less likely to be derailed in their reasoning if the feedback is not perfect. However, conventional search based methods still explore using count-based statistics, which fail to account for any notion of information associated with each feedback. The proposed IDTS method aims at alleviating all these issues. We discuss related work in more detail in Appendix A.

\(a\) often, it might still want to explore that action more if the _information_ being obtained from doing so is large.

The key insight of this work is to leverage ideas from (1) and use information gain to improve the exploration technique for monte-carlo tree search (2). Particularly, for our problem setup, we let the action \(a\) be a complete sentence/solution generated by a LLM, and \(s\) be the history of past interactions. Then, if the task is related to coding, we want to prioritize a solution \(a\), which together with its error messages and unit-tests \(Z(a)\) provides higher information gain towards the solution \(a^{*}\). (While in principle, maximization over the action set \(\) in intractable, similar to prior work (Zhou et al., 2023; Jang et al., 2020) we let \(\) be k sampled completions for any given state \(s\).) In the remaining sections, we discuss how to estimate information gain associated with taking any action \(a\) in state \(s\).

## 2 Information Directed Tree Search

Central to our idea is a procedure to estimate information gain (IG), such that the proposed idea can scale and be used with LLMs. Here we focus on explaining the core idea of computing IG using in-context learning for a single interaction step. Due to space constraints, in Appendix B, we discuss how to use chain-rule of IG to recursively decompose IG over a _sequence of actions_ under a policy.

Recall that a state \(s\) is the history of interactions, and let \(s^{}=(s,a,o)\) be the subsequent state on enacting \(a\). The information gain of taking an action \(a\) and observing the new state \(s^{}\) is the reduction in the uncertainty of the agent's belief over the solution \(Y\), i.e., \(_{t}(Y;s^{})=_{t}(Y)-_{t}(Y|s^{})\).

Computing \(_{t}(Y;s^{})\) requires estimating entropies \(_{t}(Y)\) and \(_{t}(Y|s^{})\), which in turn depend on the probability distributions \(_{t}(y)\) and \(_{t}(y|s^{})\) for \( y\), respectively. In conventional RL methods, this requires updating the posterior given the new state \(s^{}\). Specifically, recall \(_{t}(y|s^{})=p(Y=y|s^{},_{t},D)\), where \(_{t}\) is the tree explored till the iterate \(t\), and let \(_{t+1}=(_{t} s^{})\) be the tree for the next iterate after observing \(s^{}\). Let \(\) be the parameters of the environment model for the problem the agent is facing. Here, the posterior updates entail computing

\[p(y|s^{},_{t},D)=p(y|_{t+1},D)= p(y|x_{0}, )p(|_{t+1},D).\] (3)

Unfortunately, such posterior updates can be challenging when \(\) is high-dimensional. The challenge is particularly exacerbated because posterior needs to be _repeatedly_ updated for \(_{t+k}\), where \(k 1\), as new data is acquired, thereby making prior methods intractable beyond simple/linear settings.

In-context learning for posterior updates:To mitigate this challenge, we build upon the recent insights (Xie et al., 2021; Lee et al., 2023) that draw connections between in-context learning and Bayesian inference. In our setting, in-context learning provides a remarkably simple posterior update for \(p(y|_{t+1},D)\). Specifically, consider the model in Figure 1. Let \(\) be the parameters of the general world model, and \(\) be the model parameters for the specific problem agent is dealing with, and \(D\) is the internet data. We will make the following assumption,

**Assumption 1**.: \( t,(;_{t}|D)=0\)_._

Assumption 1 states that given the internet-scale data \(D\), a few (since \(t\) is usually small) problem specific interactions contained in \(_{t}\) do not provide any more information about the general world model \(\). This implies \(p(|_{t},D)=p(|D)\). Under this viewpoint we can express \(p(y|_{t+1},D)\) as the following,

\[p(y|_{t+1},D) = p(y,,|_{t+1},D) = p(y,|_{t+1},,D)\,p(|_{t+1},D)\] (4) \[= p(y|_{t+1},,D)p(|_{t+1},D )}{{=}} p(y|_{t+1}, )p(|D),\] (5)

where (a) follows because of the model in Figure 1, and Assumption 1. Similarly, \(_{t}(y)=p(y|_{t},D)\) needed to compute \(_{t}(Y)\) can be factorized as \(p(y|_{t},D)= p(y|_{t},)p(|D)\). In Appendix B we discuss how once the posterior \(_{t}(y|s^{})\) and \(_{t}(y)\) are available, we can readily estimate the entropies \(_{t}(Y|s^{})\) and \(_{t}(Y)\), and thus also the information gain \(_{t}(Y;s^{})\).

Figure 1: Graphical model for the data generating process.

**Advantages:** Unlike \(p(|_{t+1},D)\) in (3) that does an explicit update to obtain posterior over \(\) for the underlying environment, in (5) the posterior is over the parameters of the world-model \(p(|D)\) and thus do _not_ require an _explicit_ update when new data is acquired. Instead, the new data is used _in-context_\(p(y|_{t+1},)\) to obtain the desired \(p(y|_{t+1},D)\). Not only this avoid any updates to the parameters \(\), but also as new data becomes available then \(p(y|_{t+k},D)\) can also be computed readily for \(k 1\). _This would not have been possible without the in-context learning ability of LLMs._

Further, (5) reduces the problem to standard uncertainty estimation for deep learning (Gawlikowski et al., 2021; Abdar et al., 2021). Perhaps the most popular technique is to use an ensemble of N models for a Monte-Carlo estimate of the integral in (5). For extremely large models, ensembles can be created using multiple-low rank adapter instead (Malinin and Gales, 2020; Kuhn et al., 2023). For simplicity, we use \(N=1\) in our experiments.

## 3 Empirical Analysis

Complete algorithm for the proposed IDTS method and more details about the experimental setup is provided in Appendix C. We run all the experiments using OpenAI GPT models (Achiam et al., 2023), and the results are presented in Figure 2. We compare the performances of the above methods on the following domains:

**Code (Python):** This is based on the HumanEval benchmark (Chen et al., 2021). We focus on the hard problems by filtering out the ones that can be solved by GPT-3.5-turbo in one-shot. Here, an action corresponds to the entire solution. The feedback consists of results from the synthetically generated (and thus potentially incorrect) unit-tests, error messages from the compiler, and self-critique of the solution given the unit-test results and error messages.

**Math (Lean):** We use the MiniF2F benchmark (Zheng et al., 2021) for this task. Each action corresponds to an entire formal proof, and the feedback consists of the error messages from the Lean (Moura and Ullrich, 2021) compiler along with the self-critique of the generated solution.

**LLF-Bench: (Cheng et al., 2023)** This task requires recommending movies to a user whose interests are hidden from the agent. After every suggestion, the domain provides natural language hint regarding how close the recommended movies are to the user's interests. In the feedback, we also incorporate self-critique of the solution based on the response received.

While IDTS shows some promise on coding and LLF-bench, the gains over MCTS are not significant at the moment to justify additional complexity associated with IDTS. Math domain stands out, as neither the error messages from Lean compiler had corrective feedback, nor the self-critique provided any meaningful feedback. As such, just ignoring any feedback and sampling diverse solutions emerged to be the best strategy in this domain.

Figure 2: We compare the following baselines across domains: **MCTS:** standard MCTS with UCT style bonus, **Iterative Refinement:** This is sequential interaction (equivalent to MCTS with branching factor=1), **Sample and select:** This samples k solutions at the root node, and effectively disregards any feedback (equivalent to MCTS with tree depth=1), **IDTS:** This is the proposed algorithm, that builds on MCTS but uses information gain to drive exploration as opposed to the UCT style bonus. The maximum number of nodes expanded is 10. For MCTS and IDTS, the branching factor is 4.

Discussion and Future Work

While in principle, having access to the true IG should increase the efficiency of the search significantly, it is not feasible at the LLM scale to obtain the true IG. IDTS avoids (repeated) posterior updates using in-context learning, but still requires useful uncertainty measure for LLMs (Malinin and Gales, 2020; Quach et al., 2023). An important direction for future work is to have a deeper study on the estimation error in the IG computation, as finding other better methods of IG estimation is required before using IG is likely to be helpful with LLMs.

Note, however, that search using IG only requires deciding which node to explore. As such, it might only be needed that the _relative_ (not absolute) values of the IG across nodes are accurate. While this could potentially mitigate the IG estimation challenge, assessing the quality of ranking also remains challenging as the true rankings are unknown. Further, expanding \(>10\) nodes per tree would provide invaluable insights on the utility of different exploration strategies.