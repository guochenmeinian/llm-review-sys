# DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios

Junchao Wu\({}^{1}\) Runzhe Zhan\({}^{1}\) Derek F. Wong\({}^{1}\)1 Shu Yang\({}^{1}\)

Xinyi Yang\({}^{1}\) Yulin Yuan\({}^{2}\) Lidia S. Chao\({}^{1}\)

\({}^{1}\)NLP\({}^{2}\)CT Lab, Department of Computer and Information Science, University of Macau

\({}^{2}\)Department of Chinese Language and Literature, University of Macau

nlp2ct.(junchao,runzhe,shuyang,xinyi)@gmail.com

{derekfw,yulinyuan,lidiasc}@um.edu.mo

Corresponding author

###### Abstract

Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, _DetectRL_, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating various prompts usages, human revisions like word substitutions, and writing noises like spelling mistakes. Our development of _DetectRL_ reveals the strengths and limitations of current SOTA detectors. More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe _DetectRL_ could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors2.

## 1 Introduction

Detecting text generated by LLMs is a challenging task. It is often more difficult for humans than for detection techniques to identify LLM-generated text, as humans typically underperform detection methods designed for this purpose . Recently, the implications of LLM-generated content have come into focus, highlighting their significant societal and academic impacts and associated risks [2; 3]. The main concerns stem from the hallucinations and misuse of LLMs , leading to issues such as plagiarism , the spread of fake news , and challenges to educators and human scholarship in AI-assisted academic writing . Previous and current popular detection benchmarks, such as TuringBench , MGTBench , MULTITuDE , MAGE  and M4 , have primarily focused on evaluating detectors' performance across various domains, generative models, and languages by constructing idealized test data. However, they have overlooked the assessment of detectors' capabilities in more common scenarios encountered in practical applications , such as various prompt usages, human revisions, and writing noises, as shown in Table 1.

In this paper, we study the following questions: **(1) How do SOTA LLM-generated text detectors perform in real-world application scenarios? (2) What real-world factors influence detectorperformance, and to what extent?** We investigate these questions by introducing _DetectRL_, a novel benchmark for LLM-generated text detection. We achieve this by crafting challenges that are commonly encountered in real-world scenarios. These challenges simulate various prompts usages of human, human revisions of text such as word substitutions, and adversarial writing noises, including spelling mistakes. To enhance these simulations, we incorporate well-designed attack methods like prompt-based attacks, paraphrasing, adversarial perturbations, and data mixing. We selected data from domains where LLMs are frequently used and prone to abuse, such as academic writing, news writing, creative writing, and social media, to serve as samples of human-written text. To create LLM-generated texts that closely resemble real-world application scenarios, we employed powerful and widely used LLMs, including GPT-3.5-turbo , PaLM-2-bison , Claude-instant , and Llama-2-70b . Furthermore, to ensure a wider diversity of text length, we filtered out shorter texts and applied a varying length augmentation method. This approach significantly broadened the range of text lengths available for detection, enhancing the practical value of the task. We balanced sample distributions across domains, LLMs, and attack types in all test scenarios to enhance diversity, thereby creating more challenging evaluations. These distribution variances are common in real-world scenarios but are often overlooked in ideal test environments where current detectors are developed.

The construction of this benchmark was highly effective in achieving our goals. **The experimental results present a significant challenge to existing detection methods.** Current detectors, particularly those employing zero-shot techniques, often struggle with accurately identifying LLM-generated texts. For example, adversarial perturbation attacks reduce the performance of all zero-shot detectors by an average of 39.28% AUROC. In contrast, supervised detectors have demonstrated robust detection capabilities in various domains, generative models, and attacks settings.

Through our benchmark analysis, **we highlight the strong relationship between various factors and detector performance**. Key elements that undermine the robustness and generalization of detectors include the informal style of domain data, distinct statistical patterns of LLMs, and adversarial perturbation attacks. Our findings indicate that shorter training data is beneficial for building robust detectors, while longer test data improves detector performance. Additionally, when human-written text undergoes attacks, the impact on detector performance is minimal, and performance may even improve after perturbation. This underscores the potential for adversarial perturbations to enhance current detection capabilities. Furthermore, our proposed framework aims to support the long-term development of attack methods against detectors. This will enable the creation of more challenging benchmarks that reflect real-world usages and evaluate the effectiveness of detection methods.

## 2 _DetectRL_

Previous datasets were mainly constructed by directly collecting human-written texts and those generated by LLMs using the same questions or prompt prefixes. This approach assumes an ideal detection environment and overlooks critical design considerations such as application domains, generative models, potential attacks, and text lengths. We improve the current dataset construction approach to better align with real-world detection scenarios. In this section, we introduce _DetectRL_, a new benchmark designed to facilitate such assessments, with its overall framework shown in Figure 1.

  
**Benchmark \(\) Eval \(\)** & Multi & Multi & Various & Human & Writing & Data & Detector & Training & Test & Real World \\  & Domains & LLMs & Prompts & Revision & Noises & Mixing & Generalization & Length & Length & Human Writing \\  TuringBench  & ✓ & ✓ & - & - & - & - & - & - & - & - \\ MGTBench  & ✓ & ✓ & - & \(\) & \(\) & - & \(\) & - & \(\) & - \\ MULTIPhDe  & ✓ & ✓ & - & - & - & - & \(\) & - & - & - \\ M4  & ✓ & ✓ & ✓ & - & - & - & ✓ & - & - & - \\ MAGE  & ✓ & ✓ & - & \(\) & - & - & ✓ & - & - & - \\  _DetectRL_ (**Ours**) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with existing benchmarks. ✓: benchmark evaluates this scenario. \(\): has studies, not in evaluation. \(\): similar scenario exist, but not fully aligns with real-world applications.

### Framework

Data sources_DetectRL_ is a comprehensive benchmark consisting of academic abstracts from the arXiv Archive,3 covering the years 2002 to 2017. It also includes news articles from the XSum dataset , creative stories from Writing Prompts , and social reviews from Yelp Reviews . The texts generated by LLMs within these domains are considered to pose higher risk of misleading content when misused, which underscores the importance of effective detection strategies. We extracted 2,800 samples per dataset as human-written texts. To avoid the potential contamination from text generated by LLMs, all selected data was released prior to the advent of ChatGPT.

ModelsBased on the collected human-written texts, we selected several LLMs that widely used in real-world, including GPT-3.5-turbo , PaLM-2-bison , Claude-instant , and Llama-2-70b , to perform text generation tasks. These models are mostly black-box and require substantial computational resources, making white-box detection methods challenging. We obtain text samples generated by these LLMs through interactive sessions with each model. For more details on the LLMs and text generation settings, please refer to Appendix D.

Data generationWe employed various attack methods to simulate complex real-world detection scenarios. Following the classifications from the studies by  and , we categorized our attack methods into prompt attacks, paraphrase attacks, and perturbation attacks. Additionally, we treated data mixing as a separate scenario in our study. Please see Appendix D.4 for implementation details.

_Prompt attacks_ are intended to use carefully designed prompts to guide LLMs in generating text that closely mimics human writing style. Our employed prompt attacks include Few-shot Prompting  and ICO Prompting, which is part of SICO Prompting .

_Paraphrase attacks_ have been extensively studied in recent research on LLM-generated text detection , focusing on rewriting text while maintaining its original meaning. Alongside using the DIPPER-paraphraser , we also employed Back-translation via Google Translate4 and Polishing using LLMs, which are two paraphrasing methods commonly utilized in everyday scenarios.

_Perturbation attacks_ mainly involve introducing adversarial perturbations on text directly generated by LLMs. These attacks can effectively simulate common writing errors, character or word substitutions

Figure 1: The overall framework of _DetectRL_. Human-written samples are collected from high-risk and abuse-prone domains. We employ widely-used and powerful LLMs to create LLM-generated samples. All samples undergo well-designed attacks to simulate real-world scenarios and a varying length augmentation method is applied to enhance the benchmark’s diversity. _DetectRL_ consists of four distinct tasks to evaluate the detectors’ comprehensive detection abilities and robustness.

or other adversarial noises in real-world applications. We utilized DeepWordBug  for Character-level Perturbations, TextFooler  for Word-level Perturbations, and TextBugger  for Sentence-level Perturbations, all implemented using TextAttack .

_Data Mixing_ involves two primary approaches: Multi-LLM Mixing and LLM-Centered Mixing. In Multi-LLM Mixing, we create LLM-generated samples by sampling and combining sentences from multiple LLMs. On the other hand, LLM-Centered Mixing involves substituting one-fourth of an LLM-generated text with randomly selected human-written content. Despite this substitution, the text remains labeled as LLM-generated, since the majority originates from the LLM.

Data augmentationWe enhance the diversity of samples of different lengths through data augmentation, primarily by splitting texts at the sentence level. This approach creates multiple versions of each text sample with varying lengths. Based on the distribution of text lengths, we then categorize these sample into intervals of 20 words each (up to 360 words, since longer texts are rare). Within each interval, we uniformly sample 900 examples to comprehensively assess the detector's performance.

### Task definition

Based on the meticulously curated dataset, we manifest the _DetectRL_ framework into four distinct tasks for LLM-generated text detectors assessment, described as follows:

Task 1: In-domain robustness assessment: multi-domain, multi-LLM, and multi-attack assessment.This task aims to evaluate the foundational performance of detectors in different domains, generators, and attack strategies, focusing specifically on their in-domain robustness in various real-world scenarios. We use the average performance score as the assessment metric.

Task 2: Generalization assessment.This task assesses the generalization of detectors from three perspectives: domain, LLM, and attack, to determine their effectiveness in diverse scenarios. Unlike Task 1, this task emphasizes the detector's ability to handle out-of-distribution samples. For example, we evaluate the performance of detectors trained on texts from one domain when applied to texts from different domains to determine their generalization score across domains. The same approach is used to assess generalization across different LLMs and attack strategies.

    &  &  &  & } \\  & & & **Supervised** &  &  \\   & Multi- & Accidents & 25,990 & 2,008 & 2,008 \\  & Domain & News & 25,992 & 2,008 & 2,008 \\  & Domain & Greenhill & 25,985 & 2,008 & 2,008 \\  & & Social Media & 25,984 & 2,008 & 2,008 \\   &  & GPT-3-attribute & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\  & & Clanda-3-metric & 25,987 & 2,008 & 2,008 \\   &  & Direct & 20,384 & 2,016 & 2,016 \\  & & Pompe & 31,568 & 2,012 & 2,023 \\  & & Progressive & 42,767 & 2,016 & 2,016 \\  & & Portability & 42,784 & 2,016 & 2,016 \\  & & Data Mining & 40,148 & 2,008 & 2,008 \\   & Domain & News & 25,992 & 2,008 & 6,024 \\  & Generalization & Creating & 25,985 & 2,008 & 6,024 \\  & & Social Media & 25,984 & 2,008 & 6,024 \\   &  & GPT-3-attribute & 25,987 & 2,008 & 6,024 \\  & & Clanda-3-metric & 25,990 & 2,008 & 6,024 \\  & & Clanda-3-metric & 25,987 & 2,008 & 6,024 \\  & & Clanda-2-3-metric & 25,987 & 2,008 & 6,024 \\  & & Llamo-2-3-70b & 25,987 & 2,008 & 6,024 \\  & & Direct & 20,384 & 2,016 & 6,048 \\  & & Claudio-3-metric & 25,784 & 2,016 & 6,048 \\  & & Promote & 31,568 & 2,012 & 6,046 \\  & & Portuguese & 42,767 & 2,016 & 6,048 \\  & & Personalization & 42,784 & 2,016 & 6,048 \\  & & Data Mining & 40,118 & 2,008 & 6,024 \\   & Varying & Training-Time & 16,200 & 16,200 & 900 \\  & Text Length & Text-Time & 900 & 900 & 16,200 \\   & Direct & 20,384 & 2,016 & 2,016 \\  & Human & Progressive & 42,767 & 2,016 & 2,016 \\   & Writing & Portability & 42,784 & 2,016 & 2,016 \\   & Data Mining & 42,798 & 2,012 & 2,012 \\   

Table 2: Benchmark statistics.

Task 3: Varying text length assessment.This task evaluates the impact of text length on the performance of detectors, considering both training-time and test-time phase. In the training-time phase, detectors are trained on samples of different length intervals and then tested on samples from the pivotal interval. In the test-time phase, the detector trained on samples from the pivotal interval is evaluated with samples of varying lengths. This approach provides a comprehensive understanding of how text length influences detection capabilities.

Task 4: Real-world human writing assessment.This task evaluates how real-world human writing factors impact the performance of detectors. In this innovative assessment, we simulate and replicate these factors like word substitutions and spelling errors by applying attacks on human-written texts, highlighting the challenges they pose to detectors in real-world scenarios.

### Benchmark statistics

The statistics for the collected data are presented in Appendix Table 9. This dataset comprises 100,800 human-written samples, including 11,200 raw samples and 89,600 samples modified via attack manipulations. Additionally, it contains 134,400 samples generated by LLMs, categorized as follows: 11,200 samples generated with direct prompt, 22,400 with prompt attacks, 33,600 with paraphrase attacks, 33,600 with perturbation attacks, and 22,400 with data mixing. To evaluate detectors performance, we designed the _DetectRL_ benchmark by carefully extracting relevant subsets of data to align with the task design. The selected samples ensure a balance across domains, LLMs, and attack types. The training data was specifically tailored for both supervised and zero-shot detectors, and performance was evaluated using common test sets. Detailed statistics for each task and the analysis of the textual features of _DetectRL_ samples are presented in Figure 2. For a more detailed analysis, please refer to Appendix D.6.

### Evaluation metrics

We employ AUROC and \(F_{1}\) Score as the main evaluation metrics. AUROC is widely used for assessing zero-shot detection methods  because it considers the True Positive Rate (TPR) and False Positive Rate (FPR) across different classification thresholds. This makes AUROC particularly useful for evaluating detector performance at different thresholds. The \(F_{1}\) Score provides a comprehensive evaluation of detector capabilities by balancing the Precision and Recall. Additionally, we provide detailed Precision and Recall scores in Appendix F for further reference, with a specific focus on Recall to highlight the detectors' effectiveness in identifying LLM-generated text.

## 3 Experiments and discussion

In this section, we organize our experiments and discussions from five distinct perspectives: **(1) Benchmarking the cutting-edge detectors**: We evaluate the current SOTA detectors against our benchmark to identify ongoing challenges. **(2) Robustness analysis**: We analyze the factors contributing to robustness issues across various domains, LLMs, and attack scenarios. **(3) Assessing generalization**: We investigate how well detectors perform on data distribution they were not specifically trained on, highlighting their out-of-distribution robustness. **(4) Length discrimination**: We examine the detectors' ability to differentiate between texts of varying lengths and discuss the impact of training on such texts. **(5) Real-world human writing scenarios**: We analyze the effects of real-world post-processing and mistake in human-written texts, discussing their implications to provide more nuanced and valuable insights.

### Benchmarking detectors

DetectorsWe employed a variety of SOTA detectors to assess the difficulty of _DetectRL_. Given that LLMs in real-world scenarios are often black-box and inaccessible, we exclude watermarking methods from our evaluation. Our evaluation encompasses prominent zero-shot techniques and supervised fine-tuned classifiers, including Log-Likelihood , Entropy , Rank , Log-Rank , LRR , NPR , DetectGPT , Fast-DetectGPT , Revise-Detect. , DNA-GPT , Binoculars , RoBERTa Classifier (RoB ), and XLM-RoBERTa Classifier5 (X-RoB ).

For the white-box zero-shot detection method, we employ the GPT-Neo-2.7B  as the scoring model, in line with the methodology proposed in Fast-DetectGPT , to detect the text generated by black-box LLMs. For the black-box zero-shot detection method like Revise-Detect.  and DNA-GPT , we use GPT-do-Mini  to perform operations such as text revision and text continuation. For supervised detectors, all classifiers are trained using the same parameters. For detailed training parameters settings, please refer to Table 12 and AppendixE.2.

Main resultsWe assessed the performance of existing detectors on _DetectRL_, as shown in Table 3. Higher average scores indicate greater utility of the detector. These results highlight the challenges posed by our benchmark and explain why current SOTA detectors have not been widely adopted. The leaderboard results demonstrate that supervised detectors consistently outperform zero-shot detectors, demonstrating greater effectiveness and robustness. Among the zero-shot methods, Binoculars ranked highest but scored only 79.61%. The second-best is Revise-Detect., scoring 64.13%, followed by Log-Rank, LRR, Log-Likelihood, DNA-GPT, and Fast-DetectGPT. Additionally, our analysis highlights the unreliability of advanced detectors such as DetectGPT and NPR in real-world applications.

Significant ChallengesOur benchmarks reveal significant challenges in the current LLM-generated text detection research. We found that incorporating a mix distribution of domains, LLMs, and attack types increases the testing pressure of zero-shot methods. For example, in the multi-LLM setting, the average AUROC of all zero-shot detectors is only 58.61%. This is because data from each LLM spans various domains and attack methods, leading to substantial distribution differences even within data from the same LLM. These variations are often overlooked in ideal testing environments, making it difficult for zero-shot detectors developed based on them to work effectively. Specifically, zero-shot detectors struggle against powerful LLMs, achieving an average AUROC of only 77.67% on texts generated via direct prompting, with only Binoculars surpassing a 90% AUROC. The performance of these detectors declines markedly under well-designed attacks that simulate real-world scenarios, with average decreases of 1.97% in prompt attacks, 15.67% in paraphrase attacks, 38.43% in perturbation attacks, and 18.17% in data mixing scenarios. In contrast, supervised methods demonstrate impressive effectiveness, achieving an average AUROC of 99.40% on data generated through direct prompting and maintaining robustness against well-designed attacks.

Unexpectedly, recent advancements in LLM-generated text detection, such as DetectGPT , NPR , Fast-DetectGPT , and DNA-GPT , did not perform as expected on our benchmark. Their performance was even weaker than some traditional zero-shot baselines. Analysis across various domains and LLMs revealed a general lack of robustness as a potential underlying issue. For instance, DetectGPT's performance was notably low, with only 22.15% AUROC in academic writing (ArXiv) and 12.21% AUROC in news writing (Xsum), though it achieved 58.95% AUROC in creative writing and 44.43% AUROC in social media (Yelp Review). A similar trend was observed with the

    \\ 
**Tasks Settings \(\)** &  &  &  &  &  \\  & **Domain** & **LLM** & **Attack** & **Domain** & **LLM** & **Attack** & **Train** & **Test** & **Writing** & **Writing** & **Avg.** \\
**Detectors \(\)** & AUROC & \(F_{i}\) & AUROC & \(F_{i}\) & **AUROC** & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & \(F_{i}\) & AUROC & \(F_{i}\) & \\ 
**Rob-Base** & 99.98 & 99.75 & 99.93 & 99.58 & 99.56 & 97.66 & 83.00 & 91.81 & 92.37 & 79.99 & 44.00 & 97.34 & 94.31 & 90.02 \\
**Rob-Large** & 99.78 & 98.87 & 95.16 & 90.03 & 99.87 & 99.03 & 77.20 & 82.85 & 83.96 & 86.08 & 85.23 & 96.68 & 94.63 & 91.49 \\
**X-Rob-Base** & 99.92 & 99.34 & 99.14 & 98.17 & 98.94 & 96.07 & 75.97 & 92.73 & 90.58 & 84.25 & 73.83 & 93.43 & 90.29 & 91.71 \\
**X-Rob-Large** & 99.01 & 97.44 & 97.40 & 93.47 & 99.37 & 99.75 & 77.16 & 85.89 & 73.74 & 86.35 & 79.83 & 97.21 & 94.43 & 90.55 \\
**Binoculars** & 83.95 & 78.25 & 83.30 & 74.83 & 85.05 & 78.53 & 77.47 & 74.10 & 74.70 & 73.82 & 74.34 & 90.68 & 85.98 & 90.61 \\
**Revise-Detect.** & 67.24 & 60.82 & 66.36 & 53.72 & 70.89 & 57.24 & 54.50 & 53.28 & 50.63 & 65.71 & 67.96 & 83.29 & 82.16 & 62.43 \\
**Log-Rank** & 64.43 & 57.53 & 63.75 & 54.18 & 68.52 & 55.15 & 55.10 & 52.78 & 57.44 & 59.84 & 86.35 & 86.35 & 62.43 \\
**LRR** & 65.47 & 55.45 & 64.93 & 53.01 & 68.53 & 57.99 & 54.61 & 52.73 & 57.41 & 57.09 & 58.15 & 85.99 & 80.56 & 62.46 \\
**Log-Likelihood** & 63.71 & 56.36 & 62.97 & 53.13 & 67.97 & 54.38 & 53.37 & 51.77 & 50.73 & 57.92 & 59.28 & 88.84 & 83.75 & 61.83 \\
**DNA-GPT** & 64.92 & 58.53 & 64.36 & 51.09 & 68.36 & 53.36 & 51.57 & 47.09 & 41.98 & 57.63 & 62.43 & 87.80 & 87.27 & 60.70 \\
**Fast-DetectGPT** & 58.52 & 48.07 & 59.58 & 46.55 & 60.70 & 50.63 & 48.35 & 36.56 & 49.47 & 61.31 & 55.08 & 76.03 & 68.47 & **55.33** \\
**Rank** & 51.34 & 44.97 & 50.33 & 42.06 & 57.08 & 48.83 & 42.61 & 41.49 & 38.84 & 41.67 & 46.65 & 83.86 & 80.00 & **51.52** \\
**NPR** & 48.37 & 41.41 & 47.27 & 40.04 & 53.49 & 45.22 & 38.58 & 38.83 & 36.10 & 37.60 & 42.17 & 80.03 & 75.98 & 48.08 \\
**DetectGPT** & 34.43 & 21.52 & 34.93 & 14.80 & 36.19 & 19.15 & 11.54 & 13.11 & 11.84 & 35.78 & 34.69 & 60.86 & 48.76 & 29.05 \\
**Entropy** & 46.02 & 27.40 & 46.97 & 34.25 & 43.75 & 24.69 & 25.06 & 31.07 & 16.53 & 13.38 & 15.99 & 22.39 & 16.60 & 28.01 \\   

Table 3: The overall leaderboard for LLM-generated text detectors in real-world scenarios ranks detectors based on their robustness and generalization across various domains, LLMs, and attack scenarios. It also considers the impact of text length in training-time and test-time phase, as well as performance against real-world human writing factors.

best zero-shot detector, Binoculars, which performed more than 10% lower in academic writing and news writing compared to other domains. Additionally, Binoculars showed significantly reduced effectiveness on text generated by Claude, achieving only 55.15% AUROC, while presenting 88.14%,

  
**Metrics \(\)** &  & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) \\   \\ 
**Domain Settings \(\)** & - & - & ArXiv &  &  &  &  \\ 
**Log-Likelihood** & - & 65.35 & 57.55 & 45.68 & 41.32 & 68.00 & 59.38 & 75.84 & 67.22 & 63.7122 & 56.367 \\
**Entropy** & **-** & 48.39 & 29.71 & 67.84 & 57.23 & 39.06 & 20.55 & 28.82 & 02.14 & 46.0253 & 27.4066 \\
**Rank** & **-** & 57.17 & 54.62 & 36.87 & 22.47 & 56.26 & 50.90 & 55.08 & 51.90 & 51.3409 & 44.97 \\
**Log-Rank** & **-** & 67.01 & 60.09 & 46.74 & 42.60 & 67.58 & 57.57 & 76.40 & 69.88 & 64.43 & 57.5378 \\
**LRR** & **-** & 70.54 & 61.34 & 50.09 & 38.38 & 64.65 & 53.09 & 76.61 & 68.99 & 65.47 & 55.4570 \\
**NPR** & **-** & 53.85 & 49.65 & 34.59 & 18.31 & 54.96 & 52.30 & 50.09 & 45.39 & 48.387 & 41.416 \\
**DetectGPT** & **-** & 22.15 & 60.00 & 12.21 & 00.00 & 89.59 & 50.83 & 44.43 & 35.25 & 34.434 & 21.502 \\
**DNA-GPT** & - & 67.41 & 58.30 & 64.22 & 45.09 & 69.04 & 85.28 & 78.17 & 69.28 & 69.71 & 57.723 \\
**Revie-Detect.** & **-** & 70.40 & 37.51 & 50.34 & 46.07 & 73.24 & 64.29 & 75.01 & 68.71 & 67.2475 & 54.1465 \\
**Binoculars** & **-** & 84.03 & 76.77 & 77.39 & 72.18 & 93.48 & 79.73 & 90.00 & 84.32 & 86.95 & 78.75 \\
**Fast-DetectGPT** & **-** & 43.69 & 24.46 & 39.12 & 28.39 & 74.21 & 67.84 & 77.02 & 71.62 & 58.03 & 48.08 \\ Avg. & **-** & 59.09 & 46.36 & 47.37 & 37.45 & 65.34 & 55.88 & 66.11 & 57.70 & 59.68 & 49.39 \\ 
**Rob-Base** & **-** & 100.0 & 100.0 & 99.99 & 99.85 & 99.99 & 99.65 & 99.97 & 99.50 & 99.99 & 99.75 \\
**Rob-Large** & **-** & 99.99 & 99.90 & 99.85 & 98.95 & 99.54 & 97.73 & 99.76 & 98.90 & 99.54 & 98.87 \\
**X-Rob-Base** & **-** & 100.00 & 100.0 & 99.97 & 99.55 & 99.84 & 98.76 & 99.88 & 90.05 & 99.92 & 99.59 \\
**X-Rob-Large** & **-** & 99.98 & 99.85 & 99.84 & 98.95 & 99.85 & 98.31 & 96.40 & 92.66 & 99.23 & 97.19 \\
**Avg.** & **-** & 99.99 & 99.93 & 99.91 & 99.92 & 99.80 & 98.61 & 99.00 & 97.52 & 99.67 & 98.85 \\   \\ 
**LLM Settings \(\)** & - & - & GPT-3.5 &  &  &  &  \\ 
**Log-Likelihood** & - & 62.89 & 57.80 & 43.32 & 28.10 & 70.03 & 60.73 & 75.65 & 65.90 & 62.97 & 53.13 \\
**Entropy** & **-** & 46.84 & 23.29 & 52.25 & 30.42 & 45.34 & 16.56 & 43.48 & 66.75 & 46.97 & 34.25 \\
**Rank** & **-** & 52.19 & 49.32 & 41.68 & 22.78 & 50.40 & 41.74 & 57.05 & 54.40 & 50.33 & 42.06 \\
**Log-Rank** & **-** & 62.84 & 56.87 & 43.23 & 30.12 & 70.89 & 63.09 & 77.97 & 66.66 & 63.75 & 54.18 \\
**LRR** & **-** & 61.61 & 52.12 & 43.30 & 18.91 & 71.17 & 65.51 & 83.65 & 75.51 & 64.93 & 53.01 \\
**NPR** & **-** & 50.29 & 43.81 & 41.64 & 32.91 & 46.44 & 34.77 & 52.53 & 48.68 & 47.27 & 40.04 \\
**DetectGPT** & **-** & 43.46 & 26.27 & 38.36 & 12.56 & 26.72 & 00.00 & 36.71 & 20.40 & 34.93 & 14.80 \\ DNA-GPT & **-** & 61.87 & 55.04 & 44.88 & 25.67 & 71.48 & 60.77 & 75.22 & 62.89 & 64.36 & 51.09 \\
**Revie-Detect.** & **-** & 70.10 & 62.72 & 49.87 & 27.28 & 69.84 & 59.03 & 75.65 & 65.87 & 66.36 & 53.72 \\
**Binoculars** & **-** & 88.14 & 82.50 & 55.15 & 39.35 & 93.30 & 88.20 & 96.64 & 92.30 & 33.30 & 75.58 \\
**Fast-DetectGPT** & **-** & 65.56 & 59.55 & 30.10 & 00.00 & 65.97 & 57.58 & 76.79 & 69.08 & 59.58 & 46.55 \\ Avg. & **-** & 60.52 & 51.75 & 43.84 & 24.37 & 61.80 & 49.81 & 68.30 & 62.98 & 58.61 & 47.12 \\ 
**Rob-Base** & **-** & 99.97 & 99.70 & 99.98 & 99.80 & 99.94 & 99.40 & 99.84 & 99.45 & 99.93 & 99.59 \\
**Rob-Large** & **-** & 99.77 & 98.86 & 96.23 & 92.48 & 97.93 & 92.64 & 86.72 & 76.17 & 95.66 & 90.54 \\
**X-Rob-Base** & **-** & 99.88 & 99.45 & 98.26 & 97.48 & 98.77 & 97.19 & 99.69 & 98.57 & 99.15 & 98.17 \\ X-Rob-Large & **-** & 99.55 & 97.56 & 91.67 & 84.24 & 98.73 & 94.43 & 99.

93.30%, and 96.64% AUROC on text generated by GPT-3.5, PaLM-2, and Llama-2, respectively. These findings suggest that the performance differences of detectors across different domains and LLMs become significantly more pronounced when subjected to well-designed attacks.

### In-domain Robustness

Effectiveness of zero-shot detectors varies with the stylistic nature of domain data.As shown in Table 4, our results indicate that texts with a more formal style present greater challenges for detection. Detectors generally perform better with informal data, such as that from social media, but their effectiveness decreases markedly in more formal settings like news writing. Interestingly, this decrease in performance is even more pronounced in advanced detectors like Fast-DetectGPT . Despite this variability, supervised classifiers demonstrate consistent reliability in detection across various domains. This finding aligns with insights from , emphasizing the robustness of supervised classifiers in diverse textual environments.

Differences in statistical patterns of LLMs pose significant challenges to detectors.As illustrated in Table 4, our experiments reveal a notable phenomenon: nearly all zero-shot LLM-generated text detectors exhibit a significant decline in performance when processing texts generated by Claude. This suggests that the effectiveness of detectors is influenced by the type of generative model used to generate the text to be detected, and their performance can deteriorate with varying statistical patterns. We hypothesize that these differences arise from variations in data, architecture, and training methods of the models, though verifying this is difficult due to the opaque nature of black-box models. Moreover, supervised detectors are more affected by the type of generative model than by the domain, particularly in models with larger sizes. For example, Rob-Large achieved an AUROC of only 86.72% and an \(F_{1}\) Score of only 76.17% on texts generated by Llama-2, while X-Rob-Large achieved an AUROC of only 91.67% and an \(F_{1}\) Score of only 82.24% on texts generated by Claude.

Adversarial perturbation attacks represent a significant threat to zero-shot detectors.As shown in Table 4, our findings indicate that the adversarial perturbation attacks drastically reduce the effectiveness of zero-shot detectors, reducing their performance to an average AUROC of 38.43%, which is less than half compared to their performance under paraphrase attacks. Additionally, data mixing presents a new challenging scenario, resulting in performance levels similar to paraphrase attacks, with detectors achieving an average AUROC of 59.50%. While prompt attacks, such as few-shot prompting, can generate higher-quality text more aligned with human preferences, their impact on zero-shot detectors is minimal. However, enhancing LLM-generated texts through human-written prompts, such as those used for polishing, continues to pose challenges for detectors (see Appendix F.1), decreasing their effectiveness by an average of 8.97% AUROC. This finding suggests that prompt-based methods remain a viable means of compromising detector performance. In contrast, supervised detectors consistently maintain robust performance across various attack types, demonstrating their potential for practical applications.

### Generalization of detectors

In real-world applications, there is a significant demand for detectors that can effectively adapt to various types of text. In this paper, we further investigate this requirement, specifically focusing on the relationship between the distribution of training and test data for these detectors. We assessed the generalization of three representative detectors: LRR , Fast-DetectGPT , and the RoB-Base Classifier . We discussed their generalization from three perspectives: domain, LLM, and attack. Notably, we observed phenomena that align with the findings discussed in Section 3.2.

As shown in Table 5, our experimental results indicate that detectors trained on less formal stylistic domain data, such as creative writing and social media, exhibit stronger generalization. Their comprehensive performance is around 10% AUROC better than detectors trained on more formal stylistic domain data, such as academic writing and news writing. The variations in statistical patterns of generative models significantly impact the generalization of detectors. Detectors trained on texts generated by models with similar statistical patterns, such as GPT-3.5, PaLM-2, and Llama-2, generally perform well with each other. However, they struggle with texts generated by Claude. As discussed in Section 3.2, data with perturbation attacks poses the greatest challenge for generalization. Taking LRR as an example, the average AUROC for detectors trained on data with direct prompts,

[MISSING_PAGE_EMPTY:9]

shot detectors' performance and test text length. In contrast, supervised methods showed a rapid performance increase up to the pivotal length interval, followed by a slight decline.

### Impact of real-world human writing scenarios

We explored a critical question in real-world detection: How do human-driven factors impact detector performance? To investigate this, we simulated various modifications to human-written texts. We introduced paraphrase attacks to mimic text revisions and incorporated spelling errors through perturbation attacks. Moreover, we mixed LLM-generated sentences with human-written content to simulate AI-assisted writing scenarios. Experimental results, as shown in Table 6, indicate that attacks on human-written texts yield markedly different outcomes compared to those on LLM-generated texts. Specifically, paraphrasing attacks on human-written texts effectively confused zero-shot detectors, reducing the AUROC by an average of 7.95%. In contrast, data mixing had a minimal impact on zero-shot detectors' performance, with only a slight decline of 3.71% in AUROC. This contrasts sharply with the significant 18.17% decline in AUROC when human-written texts were mixed with LLM-generated texts. The resilience of human-written texts to such mixing may be attributed to their inherent complexity, making it difficult for zero-shot detectors to identify the inclusion of LLM-generated content. Interestingly, perturbation attacks on human-written texts appeared to enhance the discernment capabilities of zero-shot detectors, resulting in an average increase of 10.22% in AUROC. Similar trends were observed with supervised detectors. This suggests that human-written texts may inherently contain more adversarial features , which are utilized by detectors for identification. Such perturbations can further emphasize these distinctions, leading to improved performance.

## 4 Conclusion

In this paper, we introduce _DetectRL_, a novel benchmark designed to evaluate the detection capabilities of detectors against LLM-generated text. _DetectRL_ compiles texts from human sources in high-risk and abuse-prone domains, utilizes popular and powerful LLMs, employs well-designed attack techniques, and constructs datasets encompassing a diverse range of text lengths. This benchmark aims to assess the usability of detectors in scenarios that closely resemble real-world applications. Our experimental findings reveal the primary reasons why existing detectors for LLM-generated texts struggle in practical applications. Additionally, we engage in an in-depth discussion of the potential factors influencing detector performance, offering valuable insights into current detection research. Furthermore, _DetectRL_ provides a data curation framework to facilitate the future development of LLM-generated text detection technologies. This framework supports the rapid creation of an evolving, comprehensive, and adversarial benchmark, enabling continuous adaptation and improvement of detectors in the ongoing cat-and-mouse game of LLM-generated text detection.

  
**Settings \(\)** &  &  &  &  &  \\
**Detectors \(\)** & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) & AUROC & \(F_{1}\) \\   \\ 
**Log-Likelihood** & 89.25 & 82.09 & 76.77 & 74.28 & 99.53 & 97.76 & 88.40 & 80.88 & 88.48 & 83.75 \\
**Entropy** & 26.47 & 00.00 & 27.15 & 00.00 & 03.37 & 00.00 & 32.58 & 66.40 & 22.39 & 16.60 \\
**Rank** & 83.50 & 76.27 & 72.14 & 74.13 & 99.63 & 98.13 & 80.17 & 71.48 & 83.86 & 80.00 \\
**Log-Rank** & 89.25 & 81.45 & 76.78 & 75.17 & 99.49 & 97.57 & 88.32 & 81.23 & 88.46 & 83.85 \\
**LRR** & 85.83 & 77.40 & 76.05 & 74.46 & 98.09 & 94.78 & 83.99 & 75.60 & 85.99 & 80.56 \\
**NFR** & 77.98 & 71.61 & 69.82 & 70.60 & 98.35 & 95.51 & 73.97 & 66.22 & 80.03 & 75.98 \\
**DetectGPT** & 52.84 & 40.90 & 68.45 & 73.45 & 87.95 & 79.74 & 34.20 & 00.98 & 60.86 & 48.76 \\
**DNA-GPT** & 88.01 & 80.78 & 77.19 & 75.95 & 98.81 & 95.83 & 87.40 & 76.55 & 87.85 & 82.27 \\
**Rewise-Detect** & 86.88 & 79.61 & 65.39 & 73.65 & 98.96 & 95.48 & 85.52 & 77.37 & 84.18 & 81.52 \\
**Binoculars** & 94.75 & 88.10 & 80.00 & 74.76 & 92.66 & 94.87 & 93.80 & 88.32 & 91.70 & 86.51 \\
**Fast-DetectGPT** & 79.56 & 72.45 & 77.18 & 70.13 & 84.43 & 74.45 & 65.23 & 60.53 & 76.60 & 69.39 \\ Avg. & 27.60 & 68.24 & 69.72 & 66.96 & 87.89 & 84.01 & 73.96 & 67.72 & 77.30 & 71.91 \\   \\ 
**Rob-Base** & 99.77 & 98.10 & 89.82 & 80.98 & 99.99 & 99.65 & 99.81 & 98.51 & 97.34 & 94.31 \\
**Rob-Large** & 99.77 & 98.95 & 87.01 & 80.42 & 99.99 & 99.95 & 99.95 & 99.20 & 96.68 & 94.63 \\
**X-Rob-Base** & 98.36 & 96.20 & 81.93 & 75.06 & 99.96 & 99.30 & 93.47 & 90.62 & 93.43 & 90.29 \\
**X-Rob-Large** & 99.79 & 98.31 & 89.07 & 80.32 & 99.99 & 99.90 & 98.92 & 99.20 & 97.21 & 94.43 \\
**Avg.** & 99.42 & 97.89 & 86.95 & 76.19 & 99.98 & 99.70 & 98.26 & 96.88 & 96.16 & 93.41 \\   

Table 6: The performance of detectors in real-world human writing assessment. The shades of blue and red illustrate the performance differences between the zero-shot and the supervised detectors, respectively. The underlined values represent the best performance.