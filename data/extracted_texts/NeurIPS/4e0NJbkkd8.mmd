# Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning

Hanlin Zhu

EECS, UC Berkeley

hanlinzhu@berkeley.edu

&Paria Rashidinejad

EECS, UC Berkeley

paria.rashidinejad@berkeley.edu

&Jiantao Jiao

EECS and Statistics, UC Berkeley

jiantao@berkeley.edu

###### Abstract

We propose A-Crab (Actor-Critic Regularized by Average Bellman error), a new practical algorithm for offline reinforcement learning (RL) in complex environments with insufficient data coverage. Our algorithm combines the marginalized importance sampling framework with the actor-critic paradigm, where the critic returns evaluations of the actor (policy) that are pessimistic relative to the offline data and have a small average (importance-weighted) Bellman error. Compared to existing methods, our algorithm simultaneously offers a number of advantages: (1) It achieves the optimal statistical rate of \(1/\)--where \(N\) is the size of offline dataset--in converging to the best policy covered in the offline dataset, even when combined with general function approximators. (2) It relies on a weaker _average_ notion of policy coverage (compared to the \(_{}\) single-policy concentrability) that exploits the structure of policy visitations. (3) It outperforms the data-collection behavior policy over a wide range of specific hyperparameters. We provide both theoretical analysis and experimental results to validate the effectiveness of our proposed algorithm. The code is available at https://github.com/zhuhl98/ACrab.

## 1 Introduction

Offline reinforcement learning (RL) algorithms aim at learning a good policy based only on historical interaction data. This paradigm allows for leveraging previously-collected data in learning policies while avoiding possibly costly and dangerous trial and errors and finds applications in a wide range of domains from precision medicine (Tang et al., 2022) to robotics (Sinha et al., 2022) to climate (Rolnick et al., 2022). Despite wide applicability, offline RL has yet to achieve success stories akin to those observed in online settings that allow for trials and errors (Mnih et al., 2013; Silver et al., 2016; Ran et al., 2019; Mirhoseini et al., 2020; Oh et al., 2020; Fawzi et al., 2022; Degrave et al., 2022).

Enabling offline RL for complex real-world problems requires developing algorithms that first, handle complex high-dimensional observations and second, have minimal requirements on the data coverage and "best" exploit the information available in data. Powerful function approximators such as deep neural networks are observed to be effective in handling complex environments and deep RL algorithms have been behind the success stories mentioned above. This motivates us to investigate provably optimal offline RL algorithms that can be combined with general function approximators and have minimal requirements on the coverage and size of the offline dataset.

In RL theory, the data coverage requirements are often characterized by concentrability definitions (Munos, 2007; Scherrer, 2014). For a policy \(\), the ratio of the state-action occupancy distribution \(d^{}\) of \(\) to the dataset distribution \(\), denoted by \(w^{}=d^{}/\), is used to define concentrability. The most widely-used definition is \(_{}\) concentrability, defined as the infinite norm of \(w^{}\), i.e., \(C^{}_{_{}}=\|w^{}\|_{}\). Many earlier works on offline RL require all-policy \(_{}\) concentrability (i.e., \(C^{}_{_{}}\) is bounded for all candidate policy \(\)) (Scherrer, 2014; Liu et al., 2019; Chen and Jiang, 2019; Jiang, 2019; Haao et al., 2020; Zhang et al., 2020) or stronger assumptions such as a uniform lower bound on \((a|s)\) (Xie and Jiang, 2021). However, such all-policy concentrability assumptions are often violated in practical scenarios, and in most cases, only _partial_ dataset coverage is guaranteed.

To deal with partial data coverage, recent works use conservative algorithms, which try to avoid policies not well-covered by the dataset, to learn a good policy with much weaker dataset coverage requirements (Kumar et al., 2020; Jin et al., 2021). In particular, algorithms developed based on the principle of pessimism in the face of uncertainty are shown to find the best _covered_ policy (or sometimes they require coverage of the optimal policy) (e.g., Rashidinejad et al. (2021, 2022); Zhan et al. (2022); Chen and Jiang (2022)). However, most of these works use \(_{}\) concentrability to characterize the dataset coverage. This could still be restrictive even if we only require single-policy concentrability, since the \(_{}\) definition characterizes coverage in terms of the worst-case maximum ratio over all states and actions. Other milder variants of the single-policy concentrability coefficient are proposed by Xie et al. (2021); Uehara and Sun (2021) which consider definitions that exploit the structure of the function class to reduce the coverage requirement and involve taking a maximum over the functions in the hypothesis class instead of all states and actions. However, as we show in Section 2.4, when the function class is very expressive, these variants will degenerate to \(_{}\) concentrability. Moreover, previous algorithms requiring milder variants of single-policy concentrability are either computationally intractable (Xie et al., 2021; Uehara and Sun, 2021) or suffer a suboptimal rate of suboptimality (Cheng et al., 2022). Therefore, a natural and important question is raised:

_Is there a computationally efficient and statistically optimal algorithm that can be combined with general function approximators and have minimal requirements on dataset coverage?_

We answer this question affirmatively by proposing a novel algorithm named A-Crab (Actor-Critic Regularized by Average Bellman error). We also discuss more related works in Appendix A.

### Contributions

In this paper, we build on the adversarially trained actor-critic (ATAC) algorithm of Cheng et al. (2022) and combine it with the marginalized importance sampling (MIS) framework (Xie et al., 2019; Chen and Jiang, 2022; Rashidinejad et al., 2022). In particular, we replace the squared Euclidean norm of the Bellman-consistency error term in the critic's objective of the ATAC algorithm with an importance-weighted average Bellman error term. We prove that this simple yet critical modification of the ATAC algorithm enjoys the properties highlighted below (see Table 1 for comparisons with previous works).

  Algorithm & Computation & Any covered policy & Coverage assumption & Policy improvement & Suboptimality \\  Xie et al. (2021) & Intractable & Yes & single-policy, \(C^{}_{}\) & Yes & \(O(})\) \\  Uehara and Sun (2021) & Intractable & Yes & single-policy, \(_{}\) and \(_{2}\) & Yes & \(O(})\) \\  Chen and Jiang (2022) & Intractable & No & single-policy, \(_{}\) & No & \(O(})\) \\  Zhan et al. (2022) & Efficient & Yes & two-policy, \(_{}\) & Yes & \(O(})\) \\  Cheng et al. (2022) & Efficient & Yes & single-policy, \(C^{}_{}\) & Yes \& Robust & \(O(})\) \\  Rashidinejad et al. (2022) & Efficient & No & single-policy, \(_{}\) & No & \(O(})\) \\  Ozdaglar et al. (2022) & Efficient & No & single-policy, \(_{}\) & No & \(O(})\) \\  A-Crab (this work) & Efficient & Yes & single-policy, \(_{2}\) & Yes \& Robust & \(O(})\) \\  

Table 1: Comparison of provable offline RL algorithms with general function approximation.

**1. Optimal statistical rate in competing with the best covered policy:** In Theorem 1, we prove that our A-Crab algorithm, which uses average Bellman error, enjoys an optimal statistical rate of \(1/\). In contrast, we prove in Proposition 4.1 that the ATAC algorithm, which uses the squared Bellman error, fails to achieve the optimal rate in certain offline learning instances. As Cheng et al. (2022) explains, the squared Bellman-error regularizer appears to be the culprit behind the suboptimal rate of ATAC being \(1/N^{1/3}\). Moreover, our algorithm improves over any policy covered in the data. This is in contrast to the recent work Rashidinejad et al. (2022), which proposes an algorithm based on the MIS framework that achieves the \(1/\) rate only when the optimal policy (i.e. the policy with the highest expected rewards) is covered in the data.

**2. A weaker notion of data coverage that exploits visitation structure:** As we discussed earlier, \(_{}\) concentrability notion is used in many prior works (Rashidinejad et al., 2021; Chen and Jiang, 2022; Ozdaglar et al., 2022; Rashidinejad et al., 2022). Our importance-weighted average Bellman error as well as using Bernstein inequality in the proof relies on guarantees in terms of an \(_{2}\) single-policy concentrability notion that is weaker than the \(_{}\) variant. In particular, we have \(C^{}_{_{}}=\|w^{}\|_{}\) and \(C^{}_{_{2}}=\|w^{}\|_{2,}\), where \(\|\|_{2,}\) is the weighted 2-norm w.r.t. the dataset distribution \(\). The latter implies that the coverage coefficient only matters as much as it is covered by the dataset. Moreover, by the definition of \(w^{}\), we can obtain that \((C^{}_{_{2}})^{2}=_{d^{}}[w^{}(s,a)]\), which provides another explanation of \(_{2}\) concentrability that the coverage coefficient only matters as much as it is _actually_ visited by the policy. There are also other notions of single-policy concentrability exploiting function approximation structures to make the coverage coefficient smaller (e.g., \(C^{}_{}\) in Xie et al. (2021)). However, these notions degenerate to \(C^{}_{_{}}\) as the function class gets richer (see Figure 1 for an intuitive comparison of different notions of concentrability and Section 2.4 for a rigorous proof).

**3. Robust policy improvement:** Policy improvement (PI) refers to the property that an offline RL algorithm (under a careful choice of specific hyperparameters) can always improve upon the behavior policy that is used to collect the data. In particular, robust policy improvement (RPI) means the PI property holds under a wide range of the choice of specific hyperparameters (Cheng et al., 2022; Xie et al., 2022). Similar to the ATAC algorithm in Cheng et al. (2022), our approach enjoys the robust policy improvement guarantee as shown in Theorem 2.

**4. Inheriting many other benefits of adversarially-trained actor-critic:** Since our algorithm is based on ATAC with a different choice of regularizer, it can be easily implemented as ATAC to be applied to practical scenarios, and we provide experimental results in Section 6. Also, our algorithm is robust to model misspecification and does not require the completeness assumption (Assumption 2 in Cheng et al. (2022)) on the value function class, which makes it more practical. Moreover, our algorithm can learn a policy that outperforms any other policies well covered by the dataset.

## 2 Background

**Notation.** We use \(()\) to denote the probability simplex over a set \(\) and use \(()\) to denote the uniform distribution over \(\). We denote by \(\|\|_{2,}=_{}[()^{2}]}\) the Euclidean norm weightedby distribution \(\). We use the notation \(x y\) when there exists constant \(c>0\) such that \(x cy\) and \(x y\) if \(y x\) and denote \(x y\) if \(x y\) and \(y x\) hold simultaneously. We also use standard \(O()\) notation to hide constants and use \(()\) to suppress logarithmic factors.

### Markov decision process

An infinite-horizon discounted MDP is described by a tuple \(M=(,,P,R,,)\), where \(\) is the state space, \(\) is the action space, \(P:()\) is the transition kernel, \(R:()\) encodes a family of reward distributions with \(r:\) as the expected reward function, \([0,1)\) is the discount factor and \(:\) is the initial state distribution. We assume \(\) is finite while allowing \(\) to be arbitrarily complex. A stationary (stochastic) policy \(:()\) specifies a distribution over actions in each state. Each policy \(\) induces a (discounted) occupancy density over state-action pairs \(d^{}:\) defined as \(d^{}(s,a)(1-)_{t=0}^{}^{t}P_{t}(s_{t}=s,a_{ t}=a;)\), where \(P_{t}(s_{t}=s,a_{t}=a;)\) denotes the visitation probability of state-action pair \((s,a)\) at time step \(t\), starting at \(s_{0}()\) and following \(\). We also write \(d^{}(s)=_{a}d^{}(s,a)\) to denote the (discounted) state occupancy, and use \(_{}[]\) as a shorthand of \(_{(s,a) d^{}[]}\) or \(_{s d^{}}[]\).

The value function of a policy \(\) is the discounted cumulative reward gained by executing that policy \(V^{}(s)[_{t=0}^{}^{t}r_{t} s_{0 }=s,a_{t}(|s_{t}),\;\;t 0]\) starting at state \(s\) where \(r_{t}=R(s_{t},a_{t})\). Similarly, we define \(Q\) function of a policy as the expected cumulative reward gained by executing that policy starting from a state-action pair \((s,a)\), i.e., \(Q^{}(s,a)[_{t=0}^{}^{t}r_{t} s_ {0}=s,a_{0}=a,a_{t}(|s_{t}),\;\;t>0].\) We write \(J()(1-)_{s}[V^{}(s)]=_{(s,a)  d^{}}[r(s,a)]\) to represent the (normalized) average value of policy \(\). We denote by \(^{*}\) the optimal policy that maximizes the above objective and use the shorthand \(V^{} V^{^{*}},Q^{} Q^{^{*}}\) to denote the optimal value function and optimal \(Q\) function respectively.

### Function approximation

In modern RL, the state space \(\) is usually large or infinite, making the classic tabular RL algorithms not scalable since their sample complexity depends on the cardinality of \(\). Therefore, (general) function approximation is necessary for real-world scenarios with huge state space. In this paper, we assume access to three function classes: a function class \(\{f:[0,V_{}]\}\) that models the (approximate) Q-functions of policies, a function class \(\{w:[0,B_{w}]\}\)1 that represents marginalized importance weights with respect to data distribution, and a policy class \(\{:()\}\) consisting of candidate policies. Our framework combines the marginalized importance sampling framework (e.g., Zhan et al. (2022); Rashidinejad et al. (2022)) with _actor-critic_ methods (Xie et al., 2021; Cheng et al., 2022), which improves and selects among a set of candidate policies by successive computation of their Q-functions.

For any function \(f\) and any policy \(\), we denote \(f(s,)=_{a}(a|s)f(s,a)\) for any \(s\) and denote Bellman operator \(^{}:^{}^{}\) as

\[(^{}f)(s,a)=r(s,a)+_{s^{} P(|s,a)}[ f(s^{},)].\] (1)

Note that solving the fixed point equation (1) for \(f\) finds the \(Q\)-function of policy \(\).

We make the following assumption on the expressivity of our function classes.

**Assumption 1** (Approximate Realizability).: _Assume there exists \(_{F} 0\), s.t. for any policy \(\), \(_{f}_{}\|f-^{}f\|_{2,}^{2} _{}\), where admissible \(\) is defined by \(\{d^{}|\}\)._

This assumption is also required for Xie et al. (2021); Cheng et al. (2022). Note that when \(\|f-^{}f\|_{2,}\) is small for all admissible \(\), we have \(f Q^{}\). Therefore, Assumption 1 assumes that for any policy \(\), \(Q^{}\) is "approximally" realized in \(\). In particular, when \(_{}=0\), Assumption 1 is equivalent to \(Q^{}\) for any \(\).

### Offline reinforcement learning

In this paper, we study offline RL where we assume access only to a previously-collected and fixed dataset of interactions \(=\{(s_{i},a_{i},r_{i},s_{i}^{})\}_{i=1}^{N}\), where \(r_{i} R(s_{i},a_{i})\), \(s_{i}^{} P( s_{i},a_{i})\)To streamline the analysis, we assume that \((s_{i},a_{i})\) pairs are generated i.i.d. according to a data distribution \(()\). We make the common assumption that the dataset is collected by a behavior policy, i.e., \(\) is the discounted visitation probability of a behavior policy, which we also denote by \(\). For convenience, we assume the behavior policy \(\). The goal of offline RL is to learn a _good_ policy \(\) (a policy with a high \(J()\)) using the offline dataset. Also, for any function \(f\) that takes \((s,a,r,s^{})\) as input, we define the expectation w.r.t. the dataset \(\) (or empirical expectation) as \(_{}[f]=_{(s_{i},a_{i},r_{i},s^{}_{i}) }f(s_{i},a_{i},r_{i},s^{}_{i})\).

Marginalized importance weights.We define the marginalized importance weights of any policy \(\) to be the ratio between the discounted state-action occupancy of \(\) and the data distribution \(w^{}(s,a)(s,a)}{(s,a)}\). Such weights have been defined in prior works on theoretical RL (Xie and Jiang, 2020; Zhan et al., 2022; Rashidinejad et al., 2022; Ozdaglar et al., 2022) as well as practical RL algorithms (Nachum et al., 2019, 2019; Zhang et al., 2020, 2020, 2021).

### Coverage of offline dataset

We study offline RL with access to a dataset with partial coverage. We measure the coverage of policy \(\) in the dataset using the weighted \(_{2}\) single-policy concentrability coefficient defined below.

**Definition 1** (\(_{2}\) concentrability).: _Given a policy \(\), define \(C^{}_{_{2}}=\|w^{}\|_{2,}=\|d^{}/\|_{ 2,}.\)_

This definition is much weaker than the all-policy concentrability conventionally used in offline RL (Scherrer, 2014; Liu et al., 2019; Chen and Jiang, 2019; Jiang, 2019; Wang et al., 2019; Liao et al., 2020; Zhang et al., 2020), which requires the ratio \((s,a)}{(s,a)}\) to be bounded for all \(s\) and \(a\) as well as all policies \(\). The following proposition compares two variants of single-policy concentrability definition that appeared in recent works Rashidinejad et al. (2021); Xie et al. (2021) with the \(_{2}\) variant defined in Definition 1; see Appendix A.1 for more discussion on different concentrability definitions in prior work. To our knowledge, the \(_{2}\) version of concentrability definition has been only used in offline RL with all-policy coverage (Farahmand et al., 2010; Xie and Jiang, 2020). In the context of partial coverage, Uehara and Sun (2021) used \(_{2}\) version in a model-based setting, but their algorithms are computationally intractable. Recent works Xie et al. (2021); Cheng et al. (2022) use another milder version of concentrability than \(_{}\), and we compare different concentrability versions in Proposition 2.1. An intuitive comparison is presented in Figure 1.

**Proposition 2.1** (Comparing concentrability definitions).: _Define the \(_{}\) single-policy concentrability (Rashidinejad et al., 2021) as \(C^{}_{_{}}=\|d^{}/\|_{}\) and the Bellman-consistent single-policy concentrability (Xie et al., 2021) as \(C^{}_{}=_{f}^{} \|_{2,}^{2}^{}}{\|f-^{}\|_{2,}^{2}}\). Then, it always holds \((C^{}_{_{2}})^{2} C^{}_{_{}}\), \(C^{}_{_{2}} C^{}_{_{}}\) and there exist offline RL instances where \((C^{}_{_{2}})^{2} C^{}_{}\), \(C^{}_{_{2}} C^{}_{}\)._

A proof for Proposition 2.1 is presented in Appendix B. It is easy to show that the \(_{2}\) variant is bounded by \(_{}\) variant of concentrability as the former requires \(_{d^{}}[w^{}(s,a)]\) to be bounded while the latter requires \(w^{}(s,a)\) to be bounded for any \(s\) and \(a\). Example 1 provides a concrete example that \(C^{}_{_{2}}\) is bounded by a constant while \(C^{}_{_{}}\) could be arbitrarily large.

**Example 1** (Arbitrarily large \(_{}\) concentrability with a constant \(_{2}\) concentrability).: _Consider the simplest two-arm bandit settings, where the dataset distribution is \((a_{1})=1-^{2},(a_{2})=^{2}\) for an arbitrarily small \(>0\). Let \(\) be a policy s.t. \((a_{1})=d^{}(a_{1})=1-,(a_{2})=d^{}(a_{2})=\). Then one can calculate that \(w^{}(a_{1})=} 1\) and \(w^{}(a_{2})=\). Therefore, \(C^{}_{_{2}}\) while \(C^{}_{_{}}=\) can be arbitrarily large._

Furthermore, the Bellman-consistent variant can exploit the structure in the Q-function class \(\) for a smaller concentrability coefficient. However, in situations where the class \(\) is highly expressive, \(C^{}_{}\) could be close to \(C^{}_{_{}}\) and thus possibly larger than \(C^{}_{_{2}}\).

Finally, we make a boundedness assumption on our marginalized importance weight function class \(\) in terms of \(_{2}\) concentrability and a single-policy realizability assumption.

**Assumption 2** (Boundedness in \(_{2}\) norm of \(\)).: _Assume \(\|w\|_{2,} C^{*}_{_{2}}\) for all \(w\)._

**Assumption 3** (Single-policy realizability of \(w^{}\)).: _Assume \(w^{}\) for some policy \(\) that we aim to compete with._The definition of \(C^{*}_{_{2}}\) is similar to Xie and Jiang (2020) but they need \(w^{}\) for all \(\), which is much stronger than our single-policy realizability assumption for \(\).

## 3 Actor-Critic Regularized by Average Bellman Error

In this section, we introduce our main algorithm named A-Crab (**A**ctor-**C**ritic **R**egularized by **A**verage **B**ellman error, Algorithm 1), and compare it with the previous ATAC algorithm (Cheng et al., 2022). In Section 4, we will provide theoretical guarantees of A-Crab and discuss its advantages.

### From Actor-Critic to A-Crab

Our algorithm design builds upon the actor-critic method, in which we iteratively evaluate a policy and improve the policy based on the evaluation. Consider the following actor-critic example:

\[^{*}\,_{}f^{}(s_{0},), f^{ }\,_{f}_{}[((f-^{}f)(s,a))^{2}],\]

where we assume \(s_{0}\) is the fixed initial state in this example and recall that \(f(s,)=_{a}(a|s)f(s,a)\). Here, the policy is evaluated by the function that minimizes the squared Bellman error. However, insufficient data coverage may lead the critic to give an unreliable evaluation of the policy. To address this, the critic can compute a _Bellman-consistent_ pessimistic evaluation of \(\)(Xie et al., 2021), which picks the most pessimistic \(f\) that approximately satisfies the Bellman equation. Introducing a hyperparameter \( 0\) to tradeoff between pessimism and Bellman consistency yields the following criteria for the critic:

\[f^{}\,_{f}f(s_{0},)+_{}[((f-^{}f)(s,a))^{2}].\]

Cheng et al. (2022) argue that instead of the above _absolute pessimism_, a _relative pessimism_ approach of optimizing the performance of \(\)_relative_ to the behavior policy, results in an algorithm that improves over the behavior policy for any \( 0\) (i.e., robust policy improvement). Incorporating relative pessimism in the update rule gives the ATAC algorithm (Cheng et al., 2022):

\[^{*}\,_{}_{ }[f^{}(s,)-f^{}(s,a)],\] \[f^{}\,_{f}_{ }[f(s,)-f(s,a)]+_{}[((f-^{}f)(s,a))^{2}].\]

Finally, we introduce the importance weights \(w(s,a)\) and change the squared Bellman regularizer to an importance-weighted average Bellman error to arrive at:

\[^{*}\,\,_{}(,f^{}),f^{}\,}{}\,_{}( ,f)+_{}(,f),\] (2)

where

\[_{}(,f)=_{}[f(s,)-f(s,a)],\] (3) \[_{}(,f)=}{}\,| _{}[w(s,a)(f-^{}f)(s,a)]|.\] (4)

Maximization over \(w\) in the importance-weighted average Bellman regularizer in (4) ensures that the Bellman error is small when averaged over measure \( w\) for any \(w\), which turns out to be sufficient to control the suboptimality of the learned policy as the performance difference decomposition Lemma 1 shows. 2

Squared Bellman error v.s. importance-weighted average Bellman error.Unlike our approach, the ATAC algorithm in Cheng et al. (2022) uses squared Bellman error, wherein direct empirical approximation leads to overestimating the regularization term.3 To obtain an unbiased empirical estimator, Cheng et al. (2022) uses \(_{}[(f(s,a)-r- f(s^{},))^{2}] -_{g}_{}[(g(s,a)-r- f(s^{ },))^{2}]\) as the empirical estimator which subtracts the overestimation. Yet, as we later see in Proposition 4.1, even with this correction, ATAC fails to achieve the optimal statistical rate of \(1/\) in certain offline learning instances. In contrast, the importance-weighted average Bellman error in our algorithm is unbiased (as it involves no non-linearity). This makes our theoretical analysis much simpler and leads to achieving an optimal statistical rate of \(1/\) as shown in Theorem 1.

### Main algorithms

Since we do not have direct access to the dataset distribution \(\), our algorithm instead solves an empirical version of (2), which can be formalized as

\[_{}_{}(,f^{}), { s.t. }f^{}_{f}_{}(,f)+ _{}(,f),\] (5)

where

\[_{}(,f) =_{}[f(s,)-f(s,a)],\] (6) \[_{}(,f) =_{w}|_{}[w(s,a)(f(s,a)-r- f(s^{},))]|.\] (7)

```
1:Input: Dataset \(=\{(s_{i},a_{i},r_{i},s^{}_{i})\}_{i=1}^{N}\), value function class \(\), importance weight function class \(\), no-regret policy optimization oracle PO (Definition 2).
2: Initialization: \(_{1}\) : uniform policy, \(\): hyperparameter.
3:for\(k=1,2,,K\)do
4:\(f_{k}_{f}_{}(_{k},f)+ _{}(_{k},f)\), where \(_{}\) and \(_{}\) are defined in (6), (7)
5:\(_{k+1}(_{k},f_{k},)\).
6:endfor
7:Output:\(=(\{_{k}\}_{k=1}^{K})\). ```

**Algorithm 1** Actor-Critic **Regularized by **A**verage **B**ellman error (A-Crab)

Similar to Cheng et al. (2022), we view program (5) as a Stackelberg game and solve it using a no-regret oracle as shown in Algorithm 1. At each step \(k\), the critic minimizes the objective defined by (7) w.r.t. \(_{k}\), and \(_{k+1}\) is generated by a no-regret policy optimization oracle, given below.

**Definition 2** (No-regret policy optimization oracle).: _An algorithm PO is defined as a no-regret policy optimization oracle if for any (adversarial) sequence of functions \(f_{1},f_{2},,f_{K}\) where \(f_{k}:[0,V_{}], k[K]\), the policy sequence \(_{1},_{2},,_{K}\) produced by PO satisfies that for any policy \(\), it holds that \(^{}_{}_{k=1}^{K}_{ }[f_{k}(s,)-f_{k}(s,_{k})]=o(1)\)._

Among the well-known instances of the above no-regret policy optimization oracle is natural policy gradient (Kakade, 2001) of the form \(_{k+1}(a|s)_{k}(a|s)( f_{k}(s,a))\) with \(=|}{2V_{}^{2}K}}\)(Even-Dar et al., 2009; Agarwal et al., 2021; Xie et al., 2021; Cheng et al., 2022). A detailed discussion of the above policy optimization oracle can be found in Cheng et al. (2022). Utilizing the no-regret oracle in solving the Stackelberg optimization problem in (5) yields Algorithm 1.

A remark on critic's optimization problem.In our algorithm, for any given \(\), the critic needs to solve a \(_{f}_{w}\) optimization problem, whereas in ATAC (Cheng et al., 2022), the critic needs to solve a \(_{f}_{p}\) problem. Since we only assume single-policy realizability for the class \(\) (Assumption 3) but assume all-policy realizability for \(\) (Assumption 1) (and Cheng et al. (2022) even requires the Bellman-completeness assumption over \(\) which is much stronger), in general, the cardinality of \(\) could be much smaller than \(\), which makes the optimization region of the critic's optimization problem in our algorithm \(\) smaller than \(\) in ATAC.

## 4 Theoretical Analysis

In this section, we show the theoretical guarantee of our main algorithm (Algorithm 1), which is statistically optimal in terms of \(N\).

### Performance guarantee of the A-Crab algorithm

We first formally present our main theorem, which provides a theoretical guarantee of our A-Crab algorithm (Algorithm 1). A proof sketch is provided in Section 5 and the complete proof is deferred to Appendix C.3.

**Theorem 1** (Main theorem).: _Under Assumptions 1 and 2 and let \(\) be any policy satisfying Assumption 3, then with probability at least \(1-\),_

\[J()-J() O(_{}+C^{}_{_{2}}}})+_{}^{},\]

_where \(_{} V_{}C^{}_{_{2}}|||||/)}{N}}+B_{w}(||||||/)}{N},\) and \(\) is returned by Algorithm 1 with the choice of \(=2\)._

Below we discuss the advantages of our approach as shown in the above theorem.

**Optimal statistical rate and computational efficiency.** When \(_{}=0\) (i.e., there is no model misspecification), and when \(=^{}\) is one of the optimal policies, the output policy \(\) achieves \(O(1/)\) suboptimality rate which is optimal in \(N\) dependence (as long as \(K\) is large enough). This improves the \(O(1/N^{1/3})\) rate of the previous algorithm (Cheng et al., 2022). Note that the algorithm of Xie et al. (2021) can also achieve the optimal \(O(1/)\) rate but their algorithm involves hard constraints of squared \(_{2}\) Bellman error and thus is computationally intractable. Cheng et al. (2022) convert the hard constraints to a regularizer, making the algorithm computationally tractable while degenerating the statistical rate. Our algorithm is both statistically optimal and computationally efficient, which improves upon both Xie et al. (2021); Cheng et al. (2022) simultaneously.

**Competing with any policy.** Another advantage of our algorithm is that it can compete with any policy \(\) as long as \(w^{}=d^{}/\) is contained in \(\). In particular, the importance ratio of the behavior policy \(w^{}=d^{}/=/ 1\) is always contained in \(\), which implies that our algorithm satisfies robust policy improvement (see Theorem 2 for details).

**Robustness to model misspecification.** Theorem 1 also shows that our algorithm is robust to model misspecification on realizability assumption. Note that our algorithm does not need a completeness assumption, while Xie et al. (2021); Cheng et al. (2022) both require the (approximate) completeness assumption.

**Removal of the completeness assumption on \(\).** Compared to our algorithm, Cheng et al. (2022) additionally need a completeness assumption on \(\), which requires that for any \(f\) and \(\), it approximately holds that \(^{}f\). They need this completeness assumption because they use the estimator \(_{}[(f(s,a)-r- f(s^{},))^{2}]- _{}_{}[(g(s,a)-r- f(s^ {},))^{2}]\) to address the over-estimation issue caused by their squared \(_{2}\) Bellman error regularizer, and to make this estimator accurate, they need \(_{g}_{}[(g(s,a)-r- f(s^{ },))^{2}]\) to be small, which can be implied by the (approximate) completeness assumption. In our algorithm, thanks to the nice property of the weighted average Bellman error regularizer which can be estimated by a simple and unbiased estimator, we can get rid of this strong assumption.

### A-Crab for robust policy improvement

Robust policy improvement (RPI) refers to the property of an offline RL algorithm that the learned policy (almost) always improves upon the behavior policy used to collect data over a wide range of the choice of some specific hyperparameters (in this paper, the hyperparameter is \(\)) (Cheng et al., 2022). Similar to ATAC in Cheng et al. (2022), our A-Crab also enjoys the RPI property. Theorem 2 implies that as long as \(=o()\), our algorithm can learn a policy with vanishing suboptimality compared to the behavior policy with high probability. The proof is deferred to Appendix D.

**Theorem 2** (Robust policy improvement).: _Under Assumptions 1 and 2, with probability at least \(1-\),_

\[J()-J()(+1)(_{}+C^{}_{_{2 }}}})+_{}^{},\]

_where \(_{} V_{}C^{}_{_{2}}|||||/)}{N}}+B_{w}(| |||||/)}{N},\) and \(\) is returned by Algorithm 1 with the choice of any \( 0\)._

### Suboptimality of squared \(l_{2}\) norm of Bellman error as regularizers

The ATAC algorithm of Cheng et al. (2022) suffers suboptimal statistical rate \(O(1/N^{1/3})\) due to the squared \(_{2}\) Bellman error regularizer. Intuitively, in Cheng et al. (2022), they use Lemma 1 todecompose the performance difference and use \(\|f-^{}f\|_{2,}\) to upper bound \(_{}[(f-^{}f)(s,a)]\), which causes suboptimality since in general the former could be much larger than the latter. To overcome this suboptimal step, in our algorithm, we use a weighted version of \(_{}[(f-^{}f)(s,a)]\) as our regularizer instead of \(\|f-^{}f\|_{2,}\). Proposition 4.1 shows that ATAC is indeed statistically suboptimal even under their optimal choice of the hyperparameter \(=(N^{2/3})\). The proof is deferred to Appendix E.1.

**Proposition 4.1** (Suboptimality of ATAC).: _If we change the regularizer s.t._

\[_{}(,f)=\|f-^{}f\|_{2,}^{2} _{}(,f)=(f,f,,)-_{ g}(g,f,,),\]

_where \((g,f,,)=_{}[(g(s,a)-r- f (s^{},))^{2}]\), then even under all policy realizability (\(Q^{}\) for all \(\)) and completeness assumption (\(^{}f\) for all \(\) and \(f\)), with their optimal choice of \(=(N^{2/3})\)(Cheng et al., 2022), there exists an instance s.t. the suboptimality of the returned policy of (5) (i.e., the output policy by ATAC) is \((1/N^{1/3})\) with at least constant probability._

## 5 Proof Sketch

We provide a proof sketch of our main theorem (Theorem 1) in this section. The key lemma of the proof is presented in Lemma 1.

**Lemma 1** (Performance difference decomposition, Lemma 12 in Cheng et al. (2022)).: _For any \(,\), and any \(f:\), we can decompose \(J()-J()\) as_

\[_{}[(f-^{}f)(s,a)]+_{}[( ^{}f-f)(s,a)]+_{}[f(s,)-f(s,)]+ _{}(,f)-_{}(,Q^{}).\]

Note that the first two terms in the RHS of the decomposition are average Bellman errors of \(f\) and \(\) w.r.t. \(\) and \(d^{}\). Based on this lemma, we can directly use the average Bellman error as the regularizer instead of the squared Bellman error, which could be much larger and causes suboptimality.

Proof sketch of Theorem 1.: For simplicity, we assume realizability of \(Q^{}\), i.e., \(_{}=0\). By Lemma 1 and the definition of \(\), we have \(J()-J()=_{k=1}^{K}(J()-J(_{k}))\), which equals to

\[_{k=1}^{K}(_{}[(f_{k}- ^{_{k}}f_{k})(s,a)]}_{(a)}+_{}[( ^{_{k}}f_{k}-f_{k})(s,a)]}_{(b)}\] \[+_{}[f_{k}(s,)-f_{k}(s,_{k})]}_{(c )}+_{}(_{k},f_{k})-_{}(_{k},Q^{ _{k}})}_{(d)}).\]

By the concentration argument, with high probability, we have \(_{}(,f)=_{}(,f)_{}\) and \(_{}(,f)=_{}(,f)_{}\) for all \(\) and \(f\). Combining the fact that \(d^{}/\), one can show that \((a)+(b) 2_{}(_{k},f_{k}) 2_{}(_{k},f_{k})+O( _{})\). Therefore,

\[(a)+(b)+(d) _{}(_{k},f_{k})+2_{}(_ {k},f_{k})+O(_{})-_{}(_{k},Q^{_{k}})\] \[ _{}(_{k},f_{k})+2_{ }(_{k},f_{k})+O(_{})-_{}(_{k},Q^ {_{k}})\] \[ _{}(_{k},Q^{_{k}})+2_{ }(_{k},Q^{_{k}})+O(_{})-_{ }(_{k},Q^{_{k}})\] \[ O(_{})+2_{}(_{k},Q^{_{k}})=O (_{}),\]

where the third inequality holds by the optimality of \(f_{k}\), and the last equality holds since the Bellman error of \(Q^{}\) w.r.t. \(\) is \(0\). Therefore, with high probability,

\[J()-J() O(_{})+_{}^{}.\]

## 6 Experiments

In this section, we conduct experiments of our proposed A-Crab algorithm (Algorithm 1) using a selection of the Mujoco datasets (v2) from D4RL offline RL benchmark (Fu et al., 2020). In particular, we compare the performances of A-Crab and ATAC, since ATAC is the state-of-the-art algorithm on a range of continuous control tasks (Cheng et al., 2022).

A more practical version of weighted average Bellman error.Recall the definition of our proposed weighted average Bellman error regularizer

\[_{}(,f)=_{w}|_{ }[w(s,a)(f(s,a)-r- f(s^{},))]|\,.\]

Since the calculation of \(_{}(,f)\) requires solving an optimization problem w.r.t. importance weights \(w\), for computational efficiency, we choose \(=[0,C_{}]^{}\) as in Hong et al. (2023), and thus

\[_{}^{}(,f)=C_{}\{_{ }[(f(s,a)-r- f(s^{},))_{+}],_{ }[(r+ f(s^{},)-f(s,a))_{+}]\},\] (8)

where \(()_{+}=\{,0\}\) and \(C_{}\) can be viewed as a hyperparameter. We also observed that using a combination of squared Bellman error and our average Bellman error achieves better performance in practice, and we conjecture the reason is that the squared Bellman error regularizer is computationally more efficient and statistically suboptimal, while our average Bellman error regularizer is statistically optimal while computationally less efficient, and thus the combination of these two regularizers can benefit the training procedure.

The practical implementation of our algorithm is nearly identical to (a lightweight version of) ATAC (Cheng et al., 2022), except that we choose

\[(2_{}^{}(,f)+_{}[((f-^{}f)(s,a))^{2}])\]

as the regularizer, while ATAC uses \(_{}[((f-^{}f)(s,a))^{2}]\). All hyperparameters are the same as ATAC, including \(\). For the additional hyperparamter \(C_{}\), we do a grid search on \(\{1,2,5,10,20,50,100,200\}\).

Figure 2 compares the performance of ACrab and ATAC during training. It shows that our A-Crab has higher returns and smaller deviations than ATAC in various settings (_walker2d-random, halfcheetah-medium-replay, hopper-medium-expert_). We provide more details and results in Appendix F. We also observed that in most settings, A-Crab has a smaller variance, which shows that the training procedure of A-Crab is more stable. We provide the choice of \(\) and \(C_{}\) for each setting in Table F.3. Note that we use the same value of \(\) as in ATAC.

## 7 Discussion

We present a new offline RL algorithm called A-Crab (Algorithm 1) that can be combined with general function approximators and handle datasets with partial coverage. A-Crab is an actor-critic method, where the critic finds a relatively pessimistic evaluation of the actor while minimizing an importance-weighted average Bellman error. We prove that A-Crab achieves the optimal statistical rate of \(1/\) converging to the best policy "covered" in the data. Importantly, the notion of _coverage_ here is a weaker \(_{2}\) variant of the single-policy concentrability, which only requires the average marginalized importance weights over visitations of the target policy to be bounded. Also, A-Crab enjoys robust policy improvement that consistently improves over the data-collection behavior policy. Moreover, we empirically validated the effectiveness of A-Crab in the D4RL benchmark. Interesting avenues for future work include combining A-Crab's offline learning with an online fine-tuning algorithm with a limited trial-and-error budget and developing new measures for single-policy coverage that leverage both the visitation and hypothesis class structures.

Figure 2: Comparison of A-Crab and ATAC. For each algorithm, we run 8 copies with random seeds 0-7 and plot the mean and standard deviation. We use the same pre-training method as ATAC for 100 epochs, and the plot starts after pre-training.