# Harmonic Prior Self-conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure model, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design.

## 1 Introduction

Designing proteins that can bind small molecules has many applications, ranging from drug synthesis to energy storage or gene editing. Indeed, a key part of any protein's function derives from its ability to bind and interact with other molecular species. For example, we may design proteins that act as antidotes that sequester toxins or design enzymes that enable chemical reactions through catalysis, which plays a major role in most biological processes. We develop FlowSite to address this design challenge by building on recent advances in deep learning (DL) based protein design (Dauparas et al., 2022) and protein-molecule docking (Corso et al., 2023).

Specifically, we aim to design protein pockets to bind a certain small molecule (called ligand). We assume that we are given a protein pocket via the 3D backbone atom locations of its residues as well as the 2D chemical graph of the ligand. We do not assume any knowledge of the 3D structure or the binding pose of the ligand. Based on this information, our goal is to predict the amino acid identities for the given backbone locations (see Figure 1). We also consider the more challenging task of designing pockets that simultaneously bind multiple molecules and ions (which we call multi-ligand). Such multi-ligand binding proteins are important, for example, in enzyme design, where the ligands correspond to reactants.

Figure 1: **Binding site design.** Given the backbone (green) and multi-ligand without structure, FlowSite generates residue types and structure (orange) to bind the multi-ligand and its jointly generated structure (blue). The majority of the pocket is omitted for visibility.

This task has not been addressed by deep learning yet. While deep learning has been successful in designing proteins that can bind to other proteins (Watson et al., 2023), designing (multi-)ligand binders is different and arguably harder in various aspects. For example, no evolutionary information is directly available, unlike when modeling interactions between amino acids only. The existing approaches, such as designing 6 drug binding proteins Polizzi and DeGrado (2020) or a single enzyme Yeh et al. (2023), build on expert knowledge and require manual steps. Therefore, we develop FlowSite as a more general and automated approach and the first deep learning solution for designing pockets that bind small molecules.

FlowSite is a flow-based generative model over discrete (residue identities) and continuous (ligand pose) variables. Our flow matching training criterion guides the model to learn a self-conditioned flow that jointly generates the contact residues and the (multi-)ligand 3D binding pose structures. To achieve this, we first develop HarmonicFlow as a suitable generative process for 3D poses of (multi-)ligands. FlowSite extends this process to residue types. Starting from initial residue types and ligand atom locations sampled from a harmonic prior FlowSite updates them by iteratively following the learned vector field, as illustrated in Figure 2.

The HarmonicFlow component of FlowSite performs the task known as docking, i.e., it realizes the 3D binding pose of the multi-ligand. As a method, it is remarkably simple in comparison to existing generative processes for docking, including the state-of-the-art diffusion process of DiffDock(Corso et al., 2023) that operates on ligand's center of mass, orientation, and torsion angles. HarmonicFlow simply updates the cartesian coordinates of the atoms, yet manages to produce chemically plausible molecular structures without restricting ligand flexibility to torsions. Moreover, HarmonicFlow outperforms DiffDock's diffusion in multiple new pocket-level docking tasks on PDBBind. For instance, HarmonicFlow achieves 24.4% of its predictions to be within root-mean-square-distance (RMSD) below 2A as opposed to 16.3% for DiffDock's diffusion.

Having established HarmonicFlow as an improved generative process over ligand positions, we extend it to include discrete residue types to obtain FlowSite. We also adopt an additional "fake-ligand" data augmentation step where side chains are treated as ligands in order to realize additional training cases. Altogether, FlowSite is able to recover 47.0% of binding site amino acids compared to 39.4% of a baseline approach. This nearly closes the gap to an oracle method (51.4% recovery) with access to the ground truth 3D structure/pose of the ligand. Next to technical innovations such as self-conditioned flow matching or equivariant refinement TFN layers, our main contributions are:

1. The first application and investigation of flow matching for real-world biomolecular structure generation tasks and comparisons with diffusion model approaches.
2. FlowSite as the first deep learning solution to design binding sites for small molecules and a novel elegant framework to jointly generate discrete and continuous data.
3. HarmonicFlow which improves upon the state-of-the-art generative process for generating 3D ligand binding structures in performance, simplicity, and applicability/generality.

## 2 Related Work

**Deep learning for Docking.** Designing binding sites with high affinity for a ligand requires reasoning about the binding free energy, which is deeply interlinked with modeling ligand binding 3D

Figure 2: **Overview of FlowSite. The generative process starts from a protein pocket’s backbone atoms, initial residue types \(^{0}\), and initial ligand positions \(x_{0}\). Our joint discrete-continuous self-conditioned flow updates them to \(a_{t}\), \(x_{i}\) by following its vector field defined by the model outputs \(^{t}_{1}\), \(^{t}_{1}\). This integration is repeated until reaching \(time=1\) with the produced sample \(a_{1}\), \(x_{1}\).**

structures. This task of molecular docking has recently been tackled with deep-learning approaches (Stark et al., 2022; Lu et al., 2022; Zhang et al., 2023) including generative models (Corso et al., 2023; Qiao et al., 2023). These generative methods are based on diffusion models, building on Diff-Dock (Corso et al., 2023), which combines diffusion processes over the ligand's torsion angles and position with respect to the protein. For the task of multi-ligand docking, no deep learning solutions exist yet, and we provide the first with HarmonicFlow. We refer to Appendix D for additional important related work on this and the following topics.

**Protein Design.** A significant technical challenge for protein design is jointly modeling structure and sequence. Inverse folding approaches (Dauparas et al., 2022; Gao et al., 2023; Yi et al., 2023; Hsu et al., 2022; Gao et al., 2023b) attempt this by designing new sequences given a protein structure. This is akin to our task where the protein pocket's backbone structure is given, and we aim to design its residue types to bind a (multi-)ligand. However, the only inverse folding method that models small molecules is Carbonara (Krapp et al., 2023), which is restricted to the 31 most common ligands of PDB and requires their 3D structure and position relative to the protein to be known. For general binding site design, this would not be the case, and predicting them with traditional docking methods would not be possible since they require the pocket side chain's 3D structure.

**Flow Matching.** This recent generative modeling paradigm (Lipman et al., 2022; Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023) generalizes diffusion models (Ho et al., 2020; Song et al., 2021) in a simpler framework. Flow matching admits more design flexibility and multiple works (Tong et al., 2023; Pooladian et al., 2023) showed how it enables learning flows between arbitrary start and end distributions in a simulation-free manner. It is easily extended to data on manifolds (Chen and Lipman, 2023) and comes with straighter paths that enable faster integration.

We provide the first applications of flow matching to real-world biomolecular tasks (multi-ligand docking and binding site design). While Klein et al. (2023) explored flow matching for 3D point clouds, their application was limited to overfitting on the Boltzmann distribution of a single molecule. We explain flow matching in Section 3.1.

## 3 Method

Our goal is to design binding pockets for a ligand where we assume the inputs to be the ligand's 2D chemical graph and the backbone coordinates of the pocket's residues. In this section, we lay out how FlowSite achieves this by first explaining our HarmonicFlow generative process for docking in 3.1 before covering how FlowSite extends it to include discrete residue types in 3.2 and concluding with our model architecture in 3.3.

**Overview and definitions.** As visualized in Figure 2, FlowSite is a flow-based generative model that jointly updates discrete residue types and continuous ligand positions. The inputs are a protein pocket's backbone atoms \(^{L 4 3}\) for \(L\) residues with 4 atoms each and the chemical graph of a (multi-)ligand. Based on the ligand connectivity, its initial coordinates \(^{n 3}\) are sampled from a harmonic prior, and we initialize residue types \(\{1,,20\}^{L}\) with an initial token (we drop the chemical information of the ligands in our notation for brevity).

Given this at time \(t=0\), the flow model \(v_{}\) with learned parameters \(\) iteratively updates residue types and ligand coordinates by integrating the ODE it defines. These integration steps are repeated from time \(t=0\) to time \(t=1\) to obtain the final generated binding pocket designs.

### HarmonicFlow Structure Generation

We first lay out HarmonicFlow for pure structure generation without residue type estimation. Our notation drops \(v_{}\)'s conditioning on the pocket \(\) and residue estimates \(\) in this subsection (see the Architecture Section 3.3 for how \(\) is included). Simply put, HarmonicFlow is flow matching with a harmonic prior, self-conditioning, and \(_{1}\) prediction (our refinement TFN layers in Section 3.3 are also important for performance). In more detail:

**Conditional Flow Matching.** Given the data distribution \(p_{1}\) of bound ligand structures and any easy-to-sample prior \(p_{0}\) over \(^{n 3}\), we wish to learn an ODE that pushes the prior forward to the data distribution when integrating it from time 0 to time 1. The ODE will be defined by a time dependent vector field \(v_{}(,):^{n 3}^{n 3}\). Starting with a sample \(_{0} p_{0}(_{0})\) and following/integrating \(v\) through time will produce a sample from the data distribution \(p_{1}\).

To see how to train \(v_{}\), let us first assume access to a time-dependent vector field \(u_{t}()\) that would lead to an ODE that pushes from the prior \(p_{0}\) to the data \(p_{1}\) (it is not straightforward how to construct this \(u_{t}\)). This gives rise to a probability path \(p_{t}\) by integrating \(u_{t}\) until time \(t\). If we could sample \( p_{t}()\) we could train \(v_{}\) with the unconditional flow matching objective (Lipman et al., 2022)

\[_{FM}=_{t, p_{t}()} \|v_{}(,t)-u(,t)\|^{2}. \]

Among others, Tong et al. (2023) show that to construct such a \(u_{t}\) (that transports from prior \(p_{0}\) to \(p_{1}\)), we can use samples from the data \(_{1} p_{1}(_{1})\) and prior \(_{0} p_{0}(_{0})\) and define \(u_{t}\) via

\[u_{t}()=_{_{1} p_{1}(_{1}),_{0} p_{0 }(_{0})}(|_{0},_{1})p_{t}(|_{0},_{1})}{p_{t}()} \]

where we can choose easy-to-sample conditional flows \(p_{t}(|,)\) that give rise to simple conditional vector fields \(u_{t}(|,)\). We still cannot efficiently compute this \(u_{t}()\) and use it in \(_{FM}\) because we do not know \(p_{t}()\), but there is no need to: it is equivalent to instead train with the following conditional flow matching loss since \(_{}_{FM}=_{}_{CFM}\).

\[_{CFM}=_{t,_{1} p_{1}(_{1}),_{0} p_{0}(_{0}),x p_{t}(|_{0},_{1 })}\|v_{}(,t)-u_{t}(|_{0},_{1})\|^{2}. \]

Our simple choice of conditional probability path is \(p_{t}(|_{0},_{1})=(|t_{1}+(1-t) _{0},^{2})\), which gives rise to the conditional vector field \(u_{t}(|_{0},_{1})=_{1}-_{0}.\) Notably, we find it helpful to parameterize \(v_{}\) to predict \(_{1}\) instead of \((_{1}-_{0})\).

_Training_ with the conditional flow matching loss then boils down to 1) Sample data \(_{1} p_{1}(_{1})\) and prior \(_{0} p_{0}(_{0})\). 2) Interpolate between between the points. 3) Add noise to the interpolation to obtain \(x\). 4) Evaluate and minimize \(_{CFM}=\|v_{}(,t)-_{1}\|^{2}\) with it. _Inference_ is just as straightforward. We sample from the prior \(_{0} p_{0}(_{0})\) and integrate from \(t=0\) to \(t=1\) with an arbitrary ODE solver. We use an Euler solver, i.e., we iteratively predict \(_{1}\) as \(}_{1}=v_{}(_{t},t)\), and then calculate the step size scaled velocity estimate from it and add it to the current point \(_{t+ t}=_{t}+ t(}_{1}-_{0})\). Training and inference algorithms are in Appendix A.4.

**Harmonic Prior.** Any prior can be used for \(p_{0}\) in the flow matching framework. We choose a harmonic prior as in Eigen-Fold (Jing et al., 2023) that samples atoms to be close to each other if they are connected by a bond. We identify this as an especially valuable inductive bias when dealing with multiple molecules and ions since atoms of different molecules are already spatially separated at \(t=0\) as visualized in Figure 3.

This prior is constructed based on covalent bonds that define a graph with adjacency matrix \(\) from which we can construct the graph Laplacian \(=-\) where \(\) is the degree matrix. The harmonic prior is then \(p_{0}(_{0}) exp(-_{0}^{T}_{0})\) which can be sampled as a transformed gaussian.

**Structure Self-conditioning.** With this, we aim to bring AlphaFold2's (Jumper et al., 2021) successful recycling strategy to flow models for structure generation. Recycling enables training a deeper structure predictor without additional memory cost by performing multiple forward passes while only computing gradients for the last. For flow matching, we achieve the same by adapting the discrete diffusion model self-conditioning approach of Chen et al. (2023) as follows:

Instead of defining the vector field \(v_{}(_{t},t)\) as a function of \(_{t}\) and \(t\) alone, we additionally condition it on the prediction \(}_{1}^{t}\) of the previous integration step and use \(v_{}(_{t},}_{1}^{t},t)\). At the beginning

Figure 3: **Harmonic Prior.** Initial positions for the same single multi-ligand from an isotropic Gaussian (_left_) and from a harmonic prior (_right_). (Bound structure for this multi-ligand is in Figure 1).

of _inference_ the self-conditioning input is a sample from the harmonic prior \(}_{1}^{0} p_{0}(}_{1}^{0})\). In all following steps, it is the flow model's output (its prediction of \(_{1}\)) of the previous step \(}_{1}^{t}=v_{}(_{t- t},}_{1}^{t-  t},t- t)\). To _train_ this, in a random 50% of the training steps, the self-conditioning input is a sample from the prior \(}_{1}^{0}\). In the other 50%, we first generate a self-conditioning input \(}_{1}^{t+ t}=v_{}(_{t},}_{1}^{0},t)\), detach it from the gradient computation graph, and then use \(v_{}(_{t},}_{1}^{t+ t},t)\) for the loss computation. Algorithms 3 and 4 show these training and inference procedures.

### FlowSite Binding Site Design

In the FlowSite binding site design framework, HarmonicFlow\(}_{1}^{t+ t}=v_{}(_{t},}_{1}^{t},t)\) is augmented with an additional self-conditioned flow over the residue types to obtain \((}_{1}^{t+ t},}_{1}^{t+ t})=v_{}( _{t},}_{1}^{1},_{t},}_{1}^{t},t)\). The flow no longer produces \(}_{t}^{t+ t}\) as an estimate of \(_{1}\) and then interpolates to \(_{t+ t}\) but instead produces \((}_{1}^{t+ t},}_{1}^{t+ t})\) from which we obtain the interpolation \((_{t+ t},_{t+ t})\) and use it for the next integration step (see Figure 4). The start \(_{0},}_{1}^{0}\) are initialized as a mask token while the structures \(_{0},}_{1}^{0}\) are drawn from a harmonic prior.

This joint discrete-continuous data flow is trained with the same self-conditioning strategy as in structure self-conditioning, but with the additional discrete self-conditioning input \(}_{1}^{1}\) that is either a model output or a mask token. To the training loss we add the cross-entropy \(_{type}\) between \(\) and \(}_{1}^{t}\). In practice, we find that the \(_{1}\) prediction \(}_{1}^{t}\) already carries most information that is useful for predicting \(a_{1}\) and we omit the interpolation \(_{t}\) as model input to obtain the simpler \((}_{1}^{t+ t},}_{1}^{t+ t})=v_{}( _{t},}_{1}^{t},}_{1}^{t},t)\). This formulation admits a direct interpretation as recycling Jumper et al. (2021) and a clean joint discrete-continuous process without defining a discrete data interpolation.

Fake Ligand Data Augmentation.This strategy is based on the evidence of Polizzi & DeGrado (2020) that a protein's sidechain-sidechain interactions are similar to sidechain-ligand interactions for tight binding. In our optional data augmentation, we train with 20% of the samples having a _"fake ligand"_. Given a protein, we construct a fake ligand as the atoms of a randomly selected residue that has at least 4 other residues within 4A heavy atom distance. Additionally, we modify the protein by removing the residue that was chosen as the fake ligand and the residues that are within 7 positions next to that residue in the protein chain, as visualized in Figure 5.

### Architecture

Here, we provide an overview of the FlowSite architecture (visualized in Appendix Figure 6) that outputs ligand positions \(}_{1}\) and uses them for a residue type prediction \(}_{1}\). The structure prediction is produced by a stack of our SE(3)-equivariant refinement TFN layers that are crucial for the performance of HarmonicFlow's structure generation. This is followed by invariant layers to predict the invariant residue types. The precise architecture definition is in Appendix A.6 and an architecture visualization in Figure 6.

Radius Graph Representation.We represent the (multi-)ligand and the protein as graphs where nodes are connected based on their distances. Each protein residue and each ligand atom is a node. These are connected by protein-to-protein edges, ligand-to-ligand edges, and edges between ligand and protein. While only a single node is assigned to each residue, they contain information about all backbone atom positions (N, Ca, C, O).

Equivariant refinement Tensor Field Network (TFN) layers.Based on TFN (Geiger et al., 2020), these layers capture the important inductive bias of SE(3)-equivariance (translating and rotating the input will equally translate and rotate the output). They are a remarkably simple yet effective tweak from previous message passing TFNs (Jing et al., 2022; Corso et al., 2023), where we instead update and refine ligand coordinates with each layer akin to EGNNs (Hoogeboom et al., 2022).

Figure 4: **FlowSite self-conditioned updates.** Residue type predictions \(}_{1}^{t}\) from invariant GAT layers and position predictions \(_{1}^{t}\) from equivariant TFN layers are used as self-conditioning inputs and to interpolate to the updates \(_{t}\), \(x_{t}\).

The \(k\)-th refinement TFN layer takes as input the protein positions \(\), current ligand positions \(_{t}\), and features \(^{k-1}\) (with \(^{0}\) being zeros for the ligand and vectors between N, Ca, C, O for the protein). We construct equivariant messages for each edge via a tensor-product of neighboring nodes' invariant and equivariant features. The messages include the structural self-conditioning information by using the interatomic distances of the self-conditioning input \(_{1}^{t}\) to parameterize the tensor products. We sum the messages to obtain new node features \(^{k+1}\) and use them as input to an O(3) equivariant linear layer to predict intermediate refined ligand coordinates \(}_{1}^{k}\). Before passing \(}_{1}^{k}\) to the next refinement TFN layer, we detach them from the gradient computation graph for the non-differentiable radius graph building of the next layer.

After a stack of \(K\) TFN refinement layers, the positions \(}_{1}^{K}\) are used as final prediction \(}_{1}^{t+ t}\). While \(}_{1}^{t+ t}\) is supervised with the conditional flow matching loss \(_{CFM}=\|}_{1}^{t+ t}-_{1}\|^{2}\) the intermediate positions \(}_{1}^{k}\) contribute to an additional refinement loss \(_{refine}=_{k=1}^{K-1}\|}_{1}^{k}-_{1}\|^{2}\).

**Invariant Network.** The inputs to this part of FlowSite are the TFN's ligand structure prediction \(}_{1}\), the protein structure \(\), the invariant scalar features of the refinement TFN layers, and the self-conditioning input \(_{1}^{t}\). From the protein structure, we construct on PiFold's  distance-based invariant edge features and node features that encode the geometry of the backbone. For the edges between protein and ligand, we construct features that encode the distances from a ligand atom to all 4 backbone atoms of a connected residue.

These are processed by a stack of graph attention layers that update ligand and protein node features as well as edge features for each type of edge (ligand-to-ligand, protein-to-protein, and between the molecules). For each edge, the convolutional layers first predict attention weights from the edge features and the features of the nodes they connect. We then update a node's features by summing messages from each incoming edge weighted by the attention weights. Then, we update an edge's features based on its nodes' new features. A precise definition is in Appendix A.6. From the residue features after a stack of these convolutions, we predict new residue types \(_{t+ t}\) together with side chain torsion angles \(\). We use those in an auxiliary loss \(_{torsion}\) defined as in AlphaFold2's Appendix 1.9.1 . Thus, the complete loss for FlowSite is a weighted sum of \(_{CFM},_{refine},_{type}\), and \(_{torsion}\), while HarmonicFlow only uses \(_{CFM}\) and \(_{refine}\).

## 4 Experiments

We evaluate FlowSite with the PDBBind and Binding MOAD datasets detailed in Appendix F. Every reported number is averaged over 10 generated samples for each ligand. Precise experimental details are in Appendix E and code to reproduce each experiment is at [https://anonymous.4open.science/r/wolf](https://anonymous.4open.science/r/wolf).

### Question: HarmonicFlow Structure Generation Capability

Here, we consider the HarmonicFlow component of FlowSite and investigate its binding structure generation capability. This is to find out whether HarmonicFlow is fit for binder design where good structure generation is necessary for taking the 3D structure of the bound ligand into account. Additionally, we aim to determine how HarmonicFlow compares with state-of-the-art structure generation and if its use for docking should be further explored.

**Task Setup.** This subsection only uses the HarmonicFlow component of FlowSite - the architecture only contains refinement TFN layers, and there is no sequence prediction. The inputs are the (multi-)ligand's chemical graph and the protein pocket's backbone atoms and residue types (see Appendix Table 6 for experiments without residue type inputs). From this, the binding structure of the (multi-)ligand has to be inferred. There is also no _fake ligand data augmentation_. And we perform docking to _Distance-Pockets_ and _Radius-Pockets_ as described in Appendix A.1 and we provide preliminary blind docking results in Appendix C

**Baseline.** We compare with the state-of-the-art diffusion process of DiffDock. _Note that this is not the full DiffDock docking pipeline:_ Both HarmonicFlow and DiffDock's diffusion can generate multiple samples and, for the task of docking, a further discriminator (called confidence model in DiffDock) could be used to select the most likely poses. We only compare the 3D structure generative models and neither use language model residue embeddings. See Appendix E for details on retraining DiffDock.

**PDBBind docking results.** In Table 1, we find that our flow matching based HarmonicFlow outperforms DiffDock's diffusion in producing ligand structures close to the ground truth for both splits of PDBBind. This shows that DiffDock's restriction of the generative process to the lower dimensional manifold of rotations, torsions, and translations is not necessary. Flow matching's straighter paths, along with our well-chosen prior and self-conditioning, can achieve better performance (we investigate flow matching further in Section 4.3). Furthermore, the sampled conformations in Figure 7 and videos of the generation process show that HarmonicFlow produces chemically plausible structures and well captures the physical constraints of interatomic interactions.

**Binding MOAD multi-ligand docking results.** For binding site design, it is often necessary to model multiple ligands and ions (e.g., reactants for an enzyme). We test this with Binding MOAD, which contains such multi-ligands. Since no deep learning solutions for multi-ligands exist yet and traditional docking methods would require side-chain atom locations, we compare with EigenFold's (Jing et al., 2023) Diffusion and provide qualitative evaluation in Appendix Figure 7. For EigenFold Diffusion, we use the same model as HarmonicFlow, including its improved coordinate update layers and predict \(_{0}\) (in what corresponds to \(_{0}\) prediction in diffusion models), which we found to work better. Table 2 shows HarmonicFlow as viable for docking multi-ligands - thus, the first ML method for this task with important applications besides binding site design.

### Question 2: Binding Site Recovery

**Setup.** The input to FlowSite is the binding pocket/site specified by its backbone and the chemical identity of the ligand (without its 3D structure). We use two metrics, sequence recovery (percentage of correctly predicted residues) and our new residue similarity aware _BLOSUM score_ defined in Appendix A.2.

**Baselines.**PiFold (_no ligand_) is the architecture of Gao et al. (2023) and does not use any ligand information. In PiFold (_2D ligand_), we first process the ligand with PNA (Corso et al., 2020) message passing and pass its features as additional input to the PiFold architecture. Lastly, Ground Truth Pos and Random Ligand Pos use the architecture of FlowSite without the ligand structure prediction layers. Instead, the ligand positions are either the ground truth bound structure or sampled from a standard Normal at the pocket's alpha carbon center of mass. The oracle Ground Truth Pos method also uses fake ligand data augmentation.

**Pocket Recovery Results.** Table 3 shows that FlowSite consistently is able to recover the original pocket better than simpler treatments of the (multi-)ligand, closing the gap to the oracle method that has access to the ground truth ligand structure. The joint structure generation helps in determining the original residue types (keeping in mind that these are not necessarily the only or best). Random

    &  &  \\  &  &  &  &  \\ Method & \%2 & Med. & \%2 & Med. & \%2 & Med. & \%2 & Med. \\  DiffDock Diffusion & 28.4 & 3.1 & 16.3 & 3.9 & 26.6 & 3.2 & 15.5 & 4.1 \\ HarmonicFlow & 31.8 & 3.0 & 24.4 & 3.2 & 45.9 & 2.3 & 37.8 & 2.7 \\   

Table 1: **HarmonicFlow vs. DiffDock Diffusion.** Comparison on PDBBind splits for docking into _Distance-Pockets_ (residues close to ligand) and _Radius-Pockets_ (residues within a radius of the pocket center). The columns “%2” show the fraction of predictions with an RMSD to the ground truth that is less than 2Å (higher is better). “Med.” is the median RMSD (lower is better).

   Method & \%2 & \%5 & Med. \\  EigenFold Diffusion & 39.7 & 73.5 & 2.4 \\ HarmonicFlow & 44.4 & 75.0 & 2.2 \\   

Table 2: **Multi-Ligand Docking.** Structure generation performance on Binding MOAD’s _multi-ligands_. “%2” means the fraction of predictions with an RMSD to the ground truth less than 2Å (higher better). “Med.” is the median RMSD (lower better).

### Question 3: Ablations and Flow-Matching Investigation

**Investigations.** EigenFold Diffusion, as described in 4.1 is an adaption of Jing et al. (2022)'s diffusion process. This essentially replaces the flow matching based generative process of HarmonicFlow with a diffusion process. In Velocity Prediction, the TFN model predicts \((_{1}-_{0})\) instead of \(_{1}\) meaning that \(_{CFM}=\|v_{}-(_{1}-_{0})\|^{2}\). In Standard TFN layers, our refinement TFN layers are replaced, meaning that there are no intermediate position updates - only the last layer produces an update. no self-conditioning does not use our structure self-conditioning. Sigma=0 uses \(=0\) for the conditional flow, corresponding to a deterministic interpolant for training.

**Results.** Table 4 shows the importance of our self-conditioned flow matching objective, which enables refinement of the binding structure prediction \(}_{1}^{t}\) next to updates of \(_{t}\) at little additional training time - a 12.8% increase in this experiment. Furthermore, the refinement TFN layers improve structure prediction substantially. Lastly, parameterizing the vector field to predict \(_{1}\) instead of \((_{1}-_{0})\) appears more suitable for flow matching applications in molecular structure generation.

## 5 Conclusion

We proposed the HarmonicFlow generative process for binding structure generation and FlowSite for binding site design. Our HarmonicFlow improves upon the state-of-the-art generative process for docking in simplicity, applicability, and performance in various docking settings. We investigated how flow matching contributes to this, together with our technical innovations such as self-conditioned flow matching, harmonic prior ligands, or equivariant refinement TFN layers.

With FlowSite, we leverage our superior binding structure generative process and extend it to discrete residue types, resulting in a joint discrete-continuous flow model for designing ligand binding pockets. This is an important task for which FlowSite is the first general solution. FlowSite is a step toward binding site design, but recovery results cannot replace biological validation - this is future work we pursue. Additionally, we will address enzyme design by incorporating more prerequisites for catalytic activity besides binding the reactants.

   Method & \%2 & \%5 & Med. \\  EigenFold Diffusion & 21.0 & 65.2 & 3.8 \\ Velocity Prediction & 16.4 & 64.6 & 3.7 \\ Standard TFN Layers & 16.6 & 71.9 & 3.4 \\ no self-conditioning & 20.7 & 69.3 & 3.4 \\  HarmonicFlow \(=0\) & 25.4 & 69.9 & 3.2 \\ HarmonicFlow \(=0.5\) & 24.4 & 69.8 & 3.2 \\   

Table 4: **Flow matching investigation.** Variations of flow matching, diffusion, and architecture choices compared with our HarmonicFlow on a PDBBind sequence similarity split with _Radius-Pockets_.

    &  &  \\ Method & _BLOSUM score_ & _Recovery_ & _BLOSUM score_ & _Recovery_ \\  PiFold (no ligand) & 35.2 & 39.4 & 40.7 & 43.5 \\ PiFold (2D ligand) & 35.7 & 40.4 & 42.2 & 44.5 \\ Random Ligand Pos & 38.2 & 41.8 & 41.5 & 43.7 \\  FlowSite & 44.3 & 47.0 & 47.1 & 48.5 \\  Ground Truth Pos & 48.4 & 51.4 & 51.3 & 51.2 \\   

Table 3: **Binding Site Recovery.** Comparison on PDBBind and Binding MOAD sequence similarity splits for recovering residues of binding sites. _Recovery_ is the percentage of correctly predicted residues, and _BLOSUM score_ takes residue similarity into account. 2D ligand refers to a simple GNN encoding of the ligand’s chemical graph as additional input. The Ground Truth Pos row has access to the, in practice, unknown ground truth 3D crystal structure of the ligand and protein.

## Author Contributions

Anonymized