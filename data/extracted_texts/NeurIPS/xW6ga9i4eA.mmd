# Controllable Heterogeneous Model Aggregation for Personalized Federated Learning

Jiaqi Wang1 Qi Li2 Lingjuan Lyu3 Fenglong Ma1

1The Pennsylvania State University 2Iowa State University 3Sony AI

{jqwang,fenglong}@psu.edu, qli@iastate.edu, lingjuan.lv@sony.com

Corresponding author.

###### Abstract

Federated learning, a pioneering paradigm, enables collaborative model training without exposing users' data to central servers. Most existing federated learning systems necessitate uniform model structures across all clients, restricting their practicality. Several methods have emerged to aggregate diverse client models; however, they either lack the ability of personalization, raise privacy and security concerns, need prior knowledge, or ignore the capability and functionality of personalized models. In this paper, we present an innovative approach, named pFedClub, which addresses these challenges. pFedClub introduces personalized federated learning through the substitution of controllable neural network blocks/layers. Initially, pFedClub dissects heterogeneous client models into blocks and organizes them into functional groups on the server. Utilizing the designed CMSR (Controllable Model Searching and Reproduction) algorithm, pFedClub generates a range of personalized candidate models for each client. A model-matching technique is then applied to select the optimal personalized model, serving as a teacher model to guide each client's training process. We conducted extensive experiments across three datasets, examining both IID and non-IID settings. The results demonstrate that pFedClub outperforms baseline approaches, achieving state-of-the-art performance. Moreover, our model insight analysis reveals that pFedClub generates personalized models of reasonable size in a controllable manner, significantly reducing computational costs2.

## 1 Introduction

Federated learning (FL) [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15] is a prevalent method to train machine learning models collaboratively without centralizing clients' data on a cloud server. However, many current FL training frameworks demand uniformity in deep neural network structures among client models, a requirement often too stringent for practical, real-world applications. An alternative approach involves equipping clients with heterogeneous models, introducing a new challenge: _how to aggregate these diverse models within the federated learning framework effectively_.

Recently, various approaches have emerged to address the challenge of aggregating heterogeneous models, particularly in the context of personalized FL. Some methods leverage _additional information_, such as class information , logits [17; 18], and label-wise representations , as intermediaries for exchanging information between clients and the server. While seemingly straightforward, this approach raises significant privacy concerns, especially regarding the potential exposure of sensitive client data. To mitigate these privacy concerns, techniques such as _distillation_[20; 21; 22] and _model reassembly_ have been introduced in heterogeneous FL, wherein only model parameters are exchanged, akin to traditional FL approaches . Despite demonstrating effectiveness in aggregatingheterogeneous models, both distillation and model reassembly techniques in FL suffer from a common drawback - **lack of control over personalized model generation**.

Distillation-based approaches inherently necessitate the establishment of a unified model as the global model, informed by prior insights . This global model is then distributed to clients to guide their training efforts. However, a smaller consensus global model may struggle to extract heterogeneous knowledge from clients, often resulting in a larger size. This poses challenges for smaller clients with limited computational resources to run the shared large global model effectively. Similarly, model reassembly-based approaches also encounter a related issue. While they generate personalized models for each client, these personalized models may be significantly larger than the clients' capacity, posing challenges for implementation.

To further investigate these limitations, we conducted an experiment using the state-of-the-art model reassembly approach, pFedHR , on the SVHN dataset, where each client operated a distinct model. (_Additional details and further discussions can be found in Section 4.4._) We evaluated the parameter size of the original models and the average number of parameters in the personalized models received across communication rounds. Subsequently, we illustrated the parameter size differences between these two types of models in Figure 1. The entire bar represents the average parameter size of the generated personalized model, with the original model size depicted in blue and the increased size shown in red. Our preliminary findings indicate that the personalized models produced by pFedHR are notably larger than the original local models.

To address this issue, in this paper, we introduce a novel approach for heterogeneous model aggregation, named pFedClub, aiming to achieve personalized Federated learning through Controllable neural network block substitution, as depicted in Figure 2. pFedClub receives heterogeneous models uploaded from clients on the server. pFedClub first decomposes the heterogeneous client models into different blocks and subsequently clusters these blocks based on their functionalities (refer to Section 3.2.1). pFedClub explores a novel neural network block substitution technique to achieve this objective, as detailed in Section 3.2.2. Specifically, pFedClub aims to substitute the \(r\)-th block \(_{m,r}^{t}\) within the \(m\)-th client model \(_{m}^{t}\) with a block selected from the same group as \(_{m,r}^{t}\) during communication round \(t\). This approach ensures both the functionality of personalized models and their similarity to the original models. To enhance the diversity of the generated models, we permit arbitrary substitutions from the group for the first block (**Step 1**). For subsequent blocks, we introduce _an order-constrained block search strategy_ (**Step 2**), ensuring the quality of the generated models. If the order constraint halts the substitution prematurely, the remaining blocks are directly added to the substituted ones (**Step 3**). This completion strategy not only reduces the number of newly added parameters in the stitching blocks but also ensures similar functionality. As pFedClub may generate multiple personalized candidate models for each client, we employ the _similarity-based model-matching_ technique to select the personalized model, as discussed in Section 3.2.3. The selected personalized model is then distributed to the respective client as a teacher model, guiding the client model training process through knowledge distillation.

It is essential to emphasize that the proposed pFedClub framework is designed to be both general and flexible, allowing for the incorporation of strict controllable constraints during the personalized candidate generation process. For instance, constraints such as the model size of the generated candidates can be easily integrated into the framework. Furthermore, introducing controllability within pFedClub enables the mitigation of computational and communication costs compared to current model reassembly-based approaches. By incorporating controllable constraints, pFedClub offers greater adaptability and efficiency in handling personalized model generation. Experimental results demonstrate that pFedClub achieves state-of-the-art performance on three benchmark datasets under both IID and non-IID settings, demonstrating the effectiveness of the proposed aggregation strategy.

Figure 1: Lack of controllability demonstration using pFedHR  on the SVHN dataset with seven different client models by comparing the original model size (blue bars) to the generated personalized client model size (entire bars).

## 2 Related Work

**Model Heterogeneity in Federated Learning.** Heterogenous model cooperation is a challenging task in FL. Researchers have explored submodel training techniques [24; 25], focusing on training a shared large global model by sending masked heterogeneous models to appropriate clients. However, these approaches often fail to provide personalized models for individual clients. Furthermore, they are considerably constrained by the limitations in freedom of model selection. In addition, FedDF  and FedKEMF  conduct ensemble distillation, but the settings of FedDF are different from ours and not for model personalization. FedKEMF utilizes mutual knowledge distillation with the requirement of predefined model structures. Besides, HeteroFL  and FlexiFed  are restricted to the requirements of the client model structures. Other related research work needs extra information to be exchanged between the server and clients, e.g., logits in FCCL , class scores in FedMD , and label-wise representation in FedGH , which raises the concerns of privacy . The most recent work pFedHR  provides a layer-wise model reassembly approach to solve the challenge of model heterogeneity in federated learning. However, it has several limitations, as we discuss in Section 1.

**Personalized Federated Learning.** Instead of maintaining one global model, personalized FL cares more about each local model's performance, which is more sufficient and practical. In , the authors add a proximal term to the local optimization loss function to bound the difference between the local and global model updates. The aggregated global model is treated as the initial shared model from the meta-learning perspective in . In , the authors design a new regularized client loss to optimize the local model to achieve personalization. However, the discussed personalized federated learning work assumes that the clients have to share identical model structures.

## 3 Methodology

### Overview

Let \(_{n}=\{(_{i},_{i})\}_{i=1}^{|_{n}|}\) denote the training data stored in the \(n\)-th local client \(L_{n}\), where \(_{i}\) denotes the data, \(_{i}\) is the ground truth, and \(|_{n}|\) is the number of training data. Each client employs a deep neural network-based model \(_{n}\) to train on its training data. Note that the client models \(\{_{1},,_{N}\}\) do not share identical network structures, where \(N\) is the total number of clients. The overview of the proposed pFedClub is depicted in Figure 2, containing server update and client update. During each communication round \(t\), the randomly chosen \(M\) (\(M N\)) client models \(\{_{1}^{t},,_{M}^{t}\}\) will be uploaded to the server to generate their personalized models \(\{*_{1}^{t},,_{M}^{t}\}\). These personalized models will be sent to the corresponding clients as teachers

Figure 2: Overview of the proposed pFedClub. We take four clients with heterogeneous models as an example. The numbers denote the blocksâ€™ indexes. The number of functional groups is 4. We take Client 1 as an example to demonstrate how pFedClub works to generate personalized candidate models. Note that the arbitrary substitution for Block 1 of Client 1 is the second block from Client 4.

to guide the update of client models \(\{_{1}^{t+1},,_{M}^{t+1}\}\) in the next communication round. Next, we provide the details of the model design.

### Server Update

The model heterogeneity of the uploaded \(M\) client models \(\{_{1}^{t},,_{M}^{t}\}\) at the \(t\)-th communication round makes it challenging to generate personalized client teachers \(\{^{*}_{1},,^{*}_{M}\}\). To tackle this challenge, a new controllable block-wise substitution-based personalized model aggregation approach is proposed, which not only maintains the functionality of each block in the originally uploaded client models but also injects new knowledge provided by other models to further achieve model personalization. To this end, we first divide each client model \(_{m}^{t}\) into blocks and then group blocks into different functionality clusters.

#### 3.2.1 Functionality-wise Block Decomposition and Grouping

**Block Decomposition**. Except for recurrent deep neural networks such as the long-short term memory network , most of the remaining ones, such as the family of convolutional neural networks (CNN), can be treated as block-stacked neural networks3. Let \(R_{m}\) denote the number of blocks in client model \(_{m}^{t}\). We then decompose \(_{m}^{t}\) into blocks \(\{_{m,1}^{t},,_{m,r}^{t},,_{m,R_{m} }^{t}\}\). Note that a block is either a convolutional net, a fully connected layer, or a building block associated with a shortcut connection such as residual neural networks .

**Block Grouping**. After decomposing each model, we then apply the \(K\)-means algorithm to group blocks based on their functionality. Specifically, we apply the centered kernel alignment (CKA) technique  to calculate the similarity between two blocks as follows:

\[(_{m,i}^{t},_{n,j}^{t})=(_{m,i}^{t},_{n,j}^{t})+(_{m,i}^{t}(_{m,i}^{t}),_{n,j}^{t}(_{n,j}^{t})),\] (1)

where \(_{,i}^{t}\) denotes the input of the block \(_{,i}^{t}\), and \(_{,i}^{t}(_{,i}^{t})\) represents the output of the block \(_{,i}^{t}\) (\(=m\) or \(n\)). Let \(\{_{1}^{t},,_{K}^{t}\}\) denote the \(K\) functionality groups generated by the \(K\)-means algorithm, where each group \(_{k}^{t}\) contains multiple blocks from different models with similar functions.

#### 3.2.2 Controllable Block-wise Substitution

Intuitively, if each pair of corresponding blocks of two models has similar functionality, the whole models should also be similar. Based on this straightforward intuition, we propose to replace each block \(_{m,r}^{t}\) with a function-similar one chosen from the group \(_{k}^{t}\), where \(_{m,r}^{t}_{k}^{t}\). Let \(G_{k}\) denote the number of function-similar blocks in each group \(_{k}^{t}\). Arbitrarily replacing each block without any constraints will produce a vast number of candidate models, which is approximately equal to \(G_{k}^{R_{m}}\), where \(R_{m}\) is the number of blocks in model \(_{m}^{t}\). It is time-consuming to update all candidates.

To reduce the size of the candidate model pool, a naive solution is to randomly choose a fixed number of models from the pool first and then use the model similarity score to select the personalized teacher model. Although this approach increases the diversity of the candidate model generation, it is uncontrollable to introduce too much randomness, and the low-quality candidates may reduce the convergence rate of the federated system training. To solve these issues, we design a **C**ontrollable **M**odel **S**earching and **R**e**production (**C**MSR) algorithm (as depicted in Algorithm 1), which is a greedy-based search approach to reproduce a set of personalized candidate models for each \(_{m}^{t}\) by considering both **diversity** and **quality**. In particular, **C**MSR consists of three key steps: anchor block selection, order-constrained block search, and block completion. Next, we describe the details of each step.

**Step 1: Anchor Block Selection** (Alg. 1 lines 3-7). Assume that the first block \(_{m,1}^{t}\) in \(_{m}^{t}\) belongs to the group \(_{k}^{t}\). **C**MSR then randomly selects one block in \(_{k}^{t}\) as the substitution. The substituted block will be treated as the anchor/starting block of all the candidate models. As shown in Figure 2, \(_{4,2}^{t}\) is selected as the substitution of \(_{1,1}^{t}\). Note that a naive solution to select the anchor block may use the block with the largest similarity score calculated via Eq. (1) instead of randomly choosing one. However, such a solution significantly reduces the diversity of the generated candidates, further limits the extra knowledge borrowed from other models, and finally hurts the personalization of teacher models. Besides, \(_{m,1}^{t}\) has a chance to be selected with a probability \(}\).

``` input :Client model \(_{m}^{t}\) and block clusters \(\{_{1}^{t},,_{K}^{t}\}\) output :Candidate set \(\{_{m,1}^{t},,_{m,S_{m}}^{t}\}\)
1 Initialize candidate blocks \(_{m}^{t}=\{\}\);
2for\(r 1,,R_{m}\)do
3 // Step 1: Anchor Block Section
4ifr = 1then
5 Randomly select one block \(_{p,q}^{t}\) from the group that contains \(_{m,1}^{t}\);
6 Add the substituted block to \(_{m}^{t}=[_{p,q}^{t}]\);
7 Record the block index \(q\);
8
9 // Step 2: Order-constrained Block Search
10ifr > 1then
11 Initialize block index set \(_{m,r}^{t}=[]\);
12 // Assume that \(_{m,r}^{t}_{k}^{t}\)do
13if\(u>q\)then
14 Add the block to \(_{m}^{t}[r]\);
15 Add the block index to \(_{m,r}^{t}\);
16
17if\(_{m,r}^{t}=}\)then
18 break;
19
20\(q(_{m,r}^{t})\);
21
22
23 // Candidate Generation
24 Use \(_{m}^{t}\) to generate candidates that satisfy the order condition;
25ifthe number of blocks of the candidate model is smaller than \(R_{m}\)then
26 Run Step 3: Block Completion to complete the remaining blocks; return :\(\{_{m,1}^{t},,_{m,S_{m}}^{t}\}\) ```

**Algorithm 1**The CMSR Algorithm

**Step 2: Order-constrained Block Search** (Alg. 1 lines 8-18). After selecting the anchor block in Step 1, CMSR then finds the substitutions for the following blocks. The simplest solution is repeating the previous step \(R_{m}-1\) times to generate a candidate model. As we discussed before, it may generate low-quality candidate models.

To avoid this problem and generate controllable high-quality candidates, we maintain the order of the selected blocks, even from different client models, as a hard constraint . Mathematically, let \(_{p,q}^{t}\) denote the substitution of the \(r\)-th block of \(_{m,r}^{t}\), where \(p\) is the model index and \(q\) is the block index. For any substitution of the \((r+1)\)-th block \(_{:,u}^{t}\) should satisfy the constraint \(q<u\). As shown in Figure 2, the substitutions of \(_{1,2}^{t}\) include \(_{2,3}^{t}\) and \(_{3,3}^{t}\) since the index of the first block's substitution \(_{4,2}^{t}\) is 2.

Only using the order constraint is sufficient for the proposed pFedClub to generate high-quality and informative candidates. First, maintaining the order of block functions can guarantee that the candidate model has functionality similar to the original model. Second, it also avoids pre-defining the order of operation types, which releases the constraints of personalized teacher model generation and further increases the diversity of candidates. Third, such an approach is capable of generating similar-sized personalized models for clients. It is essential for several applications with limited computational resources, such as smart devices. However, pFedHR cannot control the size of the generated candidates.

_It is worth noting that, aside from the order constraint, pFedClub is highly adaptable and can easily incorporate other types of constraints. In Section 4.4, we will delve into the integration of the model size constraint, demonstrating the framework's versatility and ability to accommodate various constraints for personalized model generation._

**Step 3: Block Completion** (Alg. 1 lines 19-22). Step 2 may stop at the certain block \(r<R_{m}\) due to the block order constraint. To maintain the original functional structure, we will add the remaining blocks of \(_{m}^{t}\), i.e., \(\{_{m,r+1}^{t},,_{m,R_{m}}^{t}\}\) to the substitutions. In such a way, pFedClub can generate a set of candidates denoted as \(\{^{t}_{m,1},,^{t}_{m,S_{m}}\}\), where \(S_{m}\) is the number of generated candidate models. As shown in Figure 2, pFedClub will stop after substituting the third block \(^{t}_{1,3}\) since all the block indexes in Group 4 for substituting \(^{t}_{1,4}\) are not greater than 4, which is the minimum feasible block index. Thus, pFedClub will complete the generated candidates by directly using \(^{t}_{1,4}\) as the forth block.

#### 3.2.3 Personalized Model Selection

The final stage of pFedClub is to automatically select the "best" candidate teacher model from \(\{^{t}_{m,1},,^{t}_{m,S_{m}}\}\) for the \(m\)-th client. However, selecting such a model is non-trivial because \(^{t}_{m,s}\) is a reassembled, incomplete model via block substitution, and the dimension sizes of different blocks may not be well-aligned.

**Block Stitching**. We complete each candidate model \(^{t}_{m,s}\) using the network stitching technique . We use a nonlinear activation function \(()\) on top of a linear layer, i.e., \((^{}+)\) as the dimension mapping function.

Since the parameter values of \(\{,\}\) in the stitching functions are **unknown**, it is essential to learn them with training data. Here, we propose to use a public dataset \(_{p}\) to fine-tune the stitched candidate model \(^{}_{m,s}\). Note that we fix all the parameters in \(^{t}_{m,s}\) (denoted as \(^{*}_{m,s}\)) and only update \(\{,\}\) in \(^{}_{m,s}\) using the following loss if the public data are labeled:

\[_{m}=_{p}|}_{i=1}^{|_{p}|} (^{}_{m,s}(_{i};,, ^{*}_{m,s}),_{i}),\] (2)

where \(|_{p}|\) denotes the number of data in the public dataset, \((,)\) means the cross-entropy loss, \(^{}_{m,s}(_{i};,,^{*}_{m,s})\) presents the predicted label distribution for the data \(_{i}\) by fixing the parameters \(^{*}_{m,s}\), and \(_{i}\) is the ground truth vector.

An unlabeled public dataset can be used to fine-tune the parameters \(\{,\}\) using the normalized temperature-scaled cross-entropy loss  for a pair of data as follows:

\[^{i,j}_{m}=-(^{}_{m,s}( _{i}),^{}_{m,s}(_{j}))/)}{_{k=1}^{ 2|_{p}|}_{\{\}}((^{}_{m,s}(_{i}),^{}_{m,s}( _{k}))/)},\] (3)

where \((,)\) is the cosine similarity, and \(\) is the hyperparamter. \(_{j}\) is the augmentation of \(_{i}\). We still fix the parameters \(^{*}_{m,s}\) and learn \(\{,\}\).

It is worth noting that block stitching operation will not significantly increase the number of parameters in the candidate model. Besides, some candidate models may be generated using Step 3: block completion. For those candidates, the number of newly added parameters is much smaller. Moreover, the limited number of parameters is helpful for the new candidate models to maintain more original model information. Finally, it makes model computation efficient and speeds up the model training.

**Model Selection**. Let \(}^{t}_{m,s}\) denote the fine-tuned candidate model via Eq. (2). We then calculate the average cosine similarity scores on logits outputted by the original model \(^{t}_{m}\) and its candidate model \(}^{t}_{m,s}\) as follows:

\[_{m,s}=_{p}|}_{i=1}^{|_{p}|}(^{t}_{m}(_{i}),}^{t}_{m,s}( _{i})),\] (4)

where \(^{t}_{m}(_{i})\) and \(}^{t}_{m,s}(_{i})\) denote the logits of \(_{i}\) outputted by the models \(^{t}_{m}\) and \(}^{t}_{m,s}\), respectively. Note that this is a forward propagation and does not need to train the models. Finally, the candidate model with the highest similarity scores in the set \(\{_{m,1},,_{m,S_{m}}\}\) will be selected as the final personalized teacher model \(^{*}_{m}\).

### Client Update

When the \(m\)-th client is selected again at the \(j\) (\(j>t\)) communication round, the personalized model \(^{t}_{m}\) generated in the recent communication round will be distributed. Since the teacher model usually has a different network structure from the client model \(_{m}^{j}\), we propose to use knowledge distillation to update the client model following  by optimizing the following loss:

\[_{n}^{j}=_{n}|}_{i=1}^{|_{n}|} [(_{m}^{j}(_{i}),_{i})+ (_{m}^{j}(_{i}),_{m}^{ }(_{i}))],\] (5)

where \(\) is a hyperparameter and \((,)\) is the Kullback-Leibler divergence.

## 4 Experiments

### Experimental Setups

**Datasets**. In our experiments, we utilize three commonly used datasets to validate the performance of the proposed pFedClub, including MNIST4, SVHN5, and CIFAR-106. We randomly divide the datasets into three parts: 72% for training, 20% for testing, and 8% as the public dataset. We test two data distribution settings in federated learning, i.e., IID and non-IID, following existing work . For the **IID** setting, the training and testing data are randomly distributed to \(N\) clients. For the **non-IID** setting, each client randomly holds data belonging to two classes.

**Baselines**. The proposed pFedClub aims to aggregate **heterogeneous** client models to boost federated learning performance. Based on the condition of public datasets, we consider the following approaches as our baselines: (1) without using public datasets: HeteroFL  and FlexiFed ; (2) using labeled public data: FedMD , FedGH , and pFedHR ; and (3) using unlabeled public data: FCCL , FedKEMF , and pFedHR . We also compare the proposed pFedClub with the general approaches to learning personalized federated learning models, which share the same structure for all clients. The **homogeneous** baselines include: FedAvg , FedProx , Per-FedAvg , PFedMe , PFedBayes , and pFedHR . The details of all baselines can be found in Appendix **A**.

**Client Model Deployment**. In our experiments, we employ seven client models with different network structures, including MobileNetV1 , MobileNetV2 , MobileNetV3 , and four manually designed CNN models (denoted as CNN1 to CNN4). Each CNN model contains several convolutional blocks and fully connected blocks. The detailed model structures of the four models can be found in Appendix **B**. We set the number of clients \(N=50\) and the number of active clients \(M=5\) in each communication round. We propose three plans to distribute client models to validate the performance of the proposed pFedClub in different scenarios. First, we use all seven types of models and randomly send each model to a client (**Model Zoo I**). Specifically, CNN1 is randomly assigned to 8 clients, and each of the remaining six models is randomly assigned to 7 clients. Second, we distribute the three MobileNet family models to clients (**Model Zoo II**). In particular, we randomly assign MobileNetV1 to 16 clients, and MobileNetV2 and MobileNetV3 are randomly sent to the remaining 34 clients evenly. Finally, we use the four CNNs as the client models (**Model Zoo III**). Each CNN1, CNN2, and CNN3 model is randomly assigned to 12 clients. The remaining 14 clients will use CNN4 as the client model.

**Implementation Details**. We run all the experiments on NVIDIA A100 with CUDA version 12.0 on a Ubuntu 20.04.6 LTS server. All baselines and the proposed pFedClub are implemented in Pytorch 2.0.1. For the proposed pFedClub and baseline pFedHR, we set the number of clusters \(K=4\) following , and the local training epoch and the server finetuning epoch are equal to 10 and 3, respectively. The hyperparameter \(\) in Eq. (5) is 0.2. The hyperparameter \(\) in Eq. (3) is 0.07. We use Adam as the optimizer. The learning rate of the local client learning and the server fine-tuning learning rate equal \(0.001\). We use **average client accuracy** with three runs as the evaluation metric.

### Performance Comparison

**Heterogenous Model Aggregation**. Table 1 lists the experimental results regarding the **three-run accuracy** of the proposed pFedClub and baselines. Note that HeteroFL and FlexiFed belong to the submodel training technique and require that each client model must be a part of the global model. Thus, they are only tested with Model Zoo 3. These results show that our proposed approach outperforms all the baselines under most settings, especially the more complicated datasets SVHN and CIFAR-10. Compared with the most recent work pFedHR, our proposed work pFedClub shows superior performance over that on SVHN and CIFAR-10 datasets under both the IID and non-IID settings. For pFedClub, the use of the labeled public data is able to boost the performance compared with the setting using unlabeled public data, which aligns with the observations in . In addition, from Model Zoo III to Model Zoo I, the performance of pFedClub on the more complicated datasets, SVHN and CIFAR-10, improves with the increase of diversity of the model zoos.

**Homogeneous Model Aggregation**. The clients are assigned the same model structures to verify the effectiveness of pFedClub under the homogenous setting. We test the performance with CNN2 and MobileNetV2 under the non-IID setting and compare it with the state-of-the-art homogenous federated learning work. The results are shown in Table 2. Note that pFedHR and pFedClub are under the setting where the public data is labeled. We observe that the results of all the approaches on the MNIST dataset are relatively high, even with a simple CNN2 model, as classification on the MNIST dataset is an easy task. Besides, pFedClub outperforms state-of-the-art baselines on SVHN and CIFAR-10 datasets using the CNN2 model or MobileNetV2 model. These results demonstrate that pFedClub is also effective for the homogeneous setting.

### Ablation Study

We conduct the ablation study to validate the effectiveness of each designed module in our proposed approach with Model Zoo III under the non-IID setting. In particular, we use the following four baselines: (1) pFedClub\({}_{max}\): in the anchor block selection stage (Step 1 in Section 3.2.2), we naively select the most similar block calculated by Eq. (1) for the first block, instead of randomly selecting one. (2) pFedClub\({}_{min}\): different from pFedClub\({}_{max}\), we use the block with the smallest index number as the substitution. The substituted block is either itself or other models' first block. (3) pFedClub\({}_{noc}\): we conduct the block search without using the order con

   & **Public** & **Dataset** &  &  &  \\   &  & **Method** & **IID** &  &  &  &  \\   &  & FedMD & \(91.12 2.44\) & \(90.03 2.98\) & \(76.22 3.01\) & \(75.14 3.75\) & \(66.38 3.96\) & \(63.10 4.75\) \\  & & FedGH & \(92.76 1.93\) & \(91.27 2.21\) & \(78.41 2.65\) & \(75.06 2.87\) & \(71.22 2.79\) & \(67.37 3.06\) \\  & & pFedHR & \( 1.55\) & \(92.88 1.10\) & \(81.59 1.40\) & \(80.88 1.92\) & \(73.21 3.24\) & \(69.88 3.45\) \\  & & pFedClub & \(90.20 1.41\) & \(93.20 0.85\) & \( 1.17\) & \( 1.56\) & \(76.45 2.87\) & \(\) \\   &  & FedEMF & \(91.47 1.87\) & \(90.60 1.68\) & \(77.56 2.47\) & \(74.23 2.77\) & \(68.77 2.54\) & \(65.09 3.12\) \\  & & FCM & \(91.09 2.05\) & \(90.21 2.44\) & \(79.44 2.33\) & \(75.28 2.60\) & \(66.83 2.66\) & \(64.76 2.98\) \\  & & pFedHR & \(92.15 1.69\) & \(91.00 1.73\) & \(86.06 2.17\) & \(87.89 2.55\) & \(72.06 2.38\) & \(68.54 2.47\) \\  & & pFedClub & \( 1.90\) & \( 1.51\) & \( 2.08\) & \( 2.32\) & \( 1.98\) & \( 2.04\) \\   &  & FedMD & \(91.98 0.76\) & \(92.01 1.05\) & \(80.86 1.26\) & \(75.31 1.53\) & \(68.55 1.89\) & \(63.74 2.25\) \\  & & FedGH & \(92.13 1.32\) & \(91.14 1.59\) & \(78.15 1.50\) & \(75.47 1.98\) & \(71.29 1.77\) & \(68.00 2.42\) \\  & & pFedHR & \(93.51 1.36\) & \(92.77 1.24\) & \(82.33 1.86\) & \(80.96 1.90\) & \(73.60 2.38\) & \(71.14 2.76straint in Step 2; and (4) pFedClub\({}_{nbc}\): we do not conduct the block completion process (Step 3 in Section 3.2.2).

We report the results in Table 3 and provide the following observations: (1) Removal of any one module will cause the performance drop, thus demonstrating the individual contribution of each design in our proposed pFedClub. (2) When we use simple strategies (i.e., pFedClub\({}_{max}\) and pFedClub\({}_{min}\)) in the anchor block selection, the drop in the performance is smaller than studies (i.e., pFedClub\({}_{noc}\) and pFedClub\({}_{nbc}\)). It indicates that maintaining the block number and keeping the model completion matter more than the anchor block selection. Overall, each designed module has its own contribution, and the systematic combination of all the designed modules guarantees the effectiveness of our proposed pFedClub.

### Controllability Analysis

**Experimental Setups.** The major advantage of the proposed pFedClub is enabling the generation of controllable personalized candidate models. To clearly exhibit the insights, we use _Model Zoo I_ as the heterogeneous model set, and each type of model is assigned to a corresponding client. Besides, each client will be mandatorily active during all the communication rounds. We use unlabeled public data for model training under the non-IID setting. To quantitatively evaluate the controllability of the personalized models generated by pFedClub, we propose to use the average of the model size change percentage over \(T\) communication rounds as the metric for each client, which is defined as follows:

\[=_{t=1}^{T}*_{m}^{t}|-|_{m}|}{| _{m}|},\] (6)

where \(|*_{m}^{t}|\) denotes the parameter size of the personalized teacher model at round \(t\), and \(|_{m}|\) denotes the model parameter size of the original client model.

**Model Comparison**. Since the proposed pFedClub is a model reassembly-based framework, for a fair comparison, we choose to use pFedHR as the baseline. Besides, as mentioned in Section 3.2.2 Step 2, the proposed pFedClub is flexible to incorporate other constraints. In this experiment, we take the model size into consideration and denote the model as pFedClub\({}^{+}\). The reason is that for real-world FL applications, such as training a model with smart devices, their computational capability is limited. Larger personalized models may make these devices stop working. To facilitate flexible management of the generated model's size in pFedClub\({}^{+}\), we introduce an additional parameter, \(>-1\), which provides flexible controllability to decide the generated model size following the constraint: \(|*_{m}^{t}|(1+)|_{m}^{t}|\). In this experiment, we set \(=0.1\).

**Results**. Figure 3 illustrates the comparative performance with respect to accuracy and the model size controllability of pFedHR, pFedClub, and pFedClub\({}^{+}\) on the SVHN dataset under non-IID conditions. We observe that: **(1)** Both pFedClub and pFedClub\({}^{+}\) show a superior performance over pFedHR shown in Figure3(a). **(2)** As for the model size control, we can observe that pFedClub and pFedClub\({}^{+}\) both have better effectiveness over pFedHR in Figure 3(b). For example, For example, for client 1, the average received model parameter size is around 4.8 times as the original model for pFedClub. For clients 5 - 7 using MobileNets, the average of the received personalized teacher model parameter size using pFedClub is smaller than that of the original model size (\(<0\)). However, \(\) is still a large positive number for pFedHR, which means the clients still receive the personalized teacher models larger than their original ones. **(3)** When comparing pFedClub with pFedClub\({}^{+}\), the latter shows a slight decrease in accuracy due to the rigorous model size constraint. Nonetheless, pFedClub\({}^{+}\) further refines the control over

  
**Dataset** &  &  \\ 
**Method** & **IID** & **Non-IID** & **IID** & **Non-IID** \\  pFedClub\({}_{max}\) & 78.69 & 73.50 & 70.52 & 66.89 \\ pFedClub\({}_{nbc}\) & 77.50 & 72.09 & 69.96 & 66.84 \\ pFedClub\({}_{nbc}\) & 65.26 & 61.08 & 62.19 & 58.14 \\ pFedClub\({}_{nbc}\) & 63.01 & 59.47 & 61.38 & 56.02 \\ pFedClub & **82.50** & **81.03** & **74.71** & **71.68** \\   

Table 3: Ablation study performance (\(\%\)) comparison.

the size of received personalized teacher models across all clients, ensuring they are not larger than than the original models (\( 0\)). This confirms the capability of both pFedClub and pFedClub\({}^{+}\) to effectively manage personalized model sizes, an essential feature for applications sensitive to computational resources. Additional details on model size comparisons between pFedClub and pFedHR across various communication rounds are provided in Appendix **C**.

### Computational Cost Comparison

The proposed model can be treated as a fine-grained model reassembly technique, which uses controllable block-wise substitution to generate personalized candidates. In this experiment, we aim to compare the computational cost of the server between pFedClub and pFedHR using the computational time at each round on the SVHN dataset under the non-IID setting with Model Zoo III for 50 clients. We record the consumed computation time on the server side for each communication round. The results are shown in Figure 4. We can observe that the computation time at the server side of our approach pFedClub is generally shorter than that of the baseline pFedHR. Also, with the algorithm running with respect to the communication round, our approach becomes more consistent and stable compared with the significant shift of pFedHR. These results confirm that pFedClub is an efficient approach for heterogeneous model aggregation compared with pFedHR.

### System Running Time v.s. Accuracy

Except for controllability and computational costs, system running time is another key factor to evaluate the utility of the proposed pFedClub. Toward this end, we conduct an experiment to compare the consumed time to reach a fixed accuracy. The experimental setting is the same as the one that we described in Section 4.5. We take pFedHR as a baseline for comparison again.

The results are shown in Figure 5. We can observe that the proposed approach pFedClub takes less time to achieve the target accuracy on the SVHN and CIFAR-10 datasets under the non-IID setting compared with pFedHR. These results demonstrate the effectiveness of the proposed pFedClub for the heterogeneous model aggregation in federated learning again.

### Extra Experimental Results

To validate the **model scalability** of pFedClub, we conduct the experiments by considering different numbers and different active ratios of clients, and the results are shown in Table 4 in Appendix **D**. Besides, in our model design, there is a key parameter \(K\) used in Section 3.2.1. We validate the sensitivity of the selection of \(K\), and the results are listed in Table 5 in Appendix **E**.

## 5 Conclusion

This paper introduces pFedClub designed to revolutionize personalized federated learning. By leveraging a unique network block substitution method, pFedClub effectively creates tailored and functionally analogous personalized models for individual clients. Moreover, pFedClub is highly adaptable and can easily incorporate other types of constraints to achieve application-driven personalized model generation. Our experimental evaluations, conducted on three diverse datasets under both IID and non-IID settings, unequivocally validate the efficacy of pFedClub in the domain of heterogeneous model aggregation for federated learning. The results affirm the accuracy, efficiency, and flexibility of our proposed method, demonstrating its potential for real-world applications.

Figure 4: Server running time v.s. communication round.

Figure 5: The consumed running time (in seconds) of models to achieve the target accuracy.