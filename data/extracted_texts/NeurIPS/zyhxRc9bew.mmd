# What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization

Hao Sun\({}^{}\), Boris van Breugel\({}^{}\), Jonathan Crabbe, Nabeel Seedat, Mihaela van der Schaar

Department of Applied Mathematics and Theoretical Physics

University of Cambridge

hs789@cam.ac.uk, \(\): equal contributions

###### Abstract

Uncertainty Quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods in classification tasks. We introduce the confusion density matrix--a kernel-based approximation of the misclassification density--and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.

## 1 Introduction

Black-box parametric models like neural networks have achieved remarkably good performance on a variety of challenging tasks, yet many real-world applications with safety concerns--e.g. healthcare , finance [2; 3] and autonomous driving [4; 5]--necessitate reliability. These scenarios require trustworthy model predictions to avoid the high cost of erroneous decisions.

Uncertainty quantification [6; 7; 8; 9; 10] addresses the challenge of trustworthy prediction through inspecting the confidence of a model, enabling intervention whenever uncertainty is too high. Usually, however, the cause of the uncertainty is not clear. In this work, we go beyond black-box UQ in the context classification tasks and aim to answer two questions:

1. How do we provide a more granular categorization of _why_ uncertainty methods to identify certain predictions as suspicious?
2. _What_ kind of examples do different UQ methods tend to mark as suspicious?

Categorizing UncertaintyWe propose a Density-based Approach for Uncertainty Categorization (DAUC): a model-agnostic framework that provides post-hoc categorization for model uncertainty. We introduce the confusion density matrix, which captures the predictive behaviors of a given model. Based on such a confusion density matrix, we categorize the model's uncertainty into three classes: (1) **OOD** uncertainty caused by OOD examples, i.e. test-time examples that resemble no training-time sample [11; 12; 13; 14]. Such uncertainty can be manifested by a low density in the confusion density matrix; (2) **Bnd** uncertainty caused by neighboring decision boundaries, i.e., non-conformal predictions due to confusing training-time resemblances from different classes or inherent ambiguities making it challenging to classify the data [15; 16; 17]; Such uncertainty can be manifested by the density ofdiagonal elements in the confusion density matrix; (3) **IDM** uncertainty caused by imperfections in the model--as manifested by high misclassification at validation time--such that similar misclassification is to be expected during testing . Such uncertainty can be manifested by the density of off-diagonal elements in the confusion density matrix. Figure 1 illustrates the different classes and in Section 4 we provide visualisations using real data. We show how DAUC can be used to benchmark a broad class of UQ methods, by categorizing what each method tends to flag as suspicious.

Our contributionscan be summarized as follows:

1. Formally, we propose the confusion density matrix--the heart of DAUC--that links the training time error, decision boundary ambiguity, and uncertainty with latent representation density.
2. Practically, we leverage DAUC as a unified framework for uncertain example categorization. DAUC offers characterisation of uncertain examples at test time.
3. Empirically, we use DAUC to benchmark existing UQ methods. We manifest different methods' sensitivity to different types of uncertain examples, this provides model insight and aids UQ method selection.

## 2 Related Work

Uncertainty QuantificationUncertainty quantification methods are used to assess the confidence in a model's predictions. In recent years the machine learning community has proposed many methods which broadly fall into the following categories: (1) Ensemble methods (i.e. Deep Ensembles ), which--while considered state of the art--have a high computational burden; (2) Approximate Bayesian methods (i.e. Stochastic Variational Inference [7; 8; 9]), however work by [19; 20; 21] suggests these methods do not yield high-quality uncertainty estimates; and (3) Dropout-based methods such as Monte Carlo Dropout , which are simpler as they do rely on estimating a posterior, however the quality of the uncertainty estimates is linked to the choice of parameters which need to be calibrated to match the level of uncertainty and avoid suboptimal performance . For completeness we note that whilst conformal prediction  is another UQ method, the paradigm is different from the other aforementioned methods, as it returns predictive sets to satisfy coverage guarantees rather than a value-based measure of uncertainty.

In practice, the predictive uncertainty for a model prediction arises from the lack of relevant training data (epistemic uncertainty) or the inherent non-separable property of the data distribution (aleatoric uncertainty) [24; 25; 26]. This distinction in types of uncertainty is crucial as it has been shown that samples with low epistemic uncertainty are more likely under the data distribution, hence motivating why epistemic uncertainty has been used for OOD/outlier detection . Moreover, ambiguous instances close to the decision boundary typically result in high aleatoric uncertainty .

This suggests that different sets of uncertain data points are associated with different types of uncertainties and, consequently, different types of misclassifications. Furthermore, it is to be expected that different uncertainty estimators are more able/prone to capturing some types of uncertainty than others. To understand these models better, we thus require a more granular definition to characterize

Figure 1: Given a prediction and UQ algorithm, our method divides flagged test time examples into different classes. Class **OOD** identifies outliers, which are mistrusted because they do not resemble training data; class **IDM** indicates examples lying in regions with high misclassification; class **Bnd** indicates examples that lie near the decision boundary.

uncertain data points. We employ three classes: outliers (OOD), boundary examples (Bnd) and examples of regions with high in-distribution misclassification (IDM), see Fig 1.

Out-of-distribution (OOD) detectionRecall that UQ methods have been used to flag OOD examples. For completeness, we highlight that other alternative methods exist for OOD detection. For example, Lee et al.  detects OOD examples based on the Mahalanobis distance, whilst Ren et al.  uses the likelihood ratio between two generative models. Besides supervised learning, OOD is an essential topic in offline reinforcement learning [30; 31; 32; 33; 34; 35]. Sun et al.  detects the OOD state in the context of RL using confidence intervals. We emphasize that DAUC's aim is broader--creating a unifying framework for categorizing multiple types of uncertainty--however, existing OOD methods could be used to replace DAUC's OOD detector.

Accuracy without LabelsWe contrast our work to the literature which aims to determine model accuracy without access to ground-truth labels. Methods such as [37; 38] propose a secondary regression model as an accuracy predictor given a data sample. Ramalho and Miranda  combine regression model with latent nearest neighbors for uncertain prediction. This is different from our setting which is focused on inspecting UQ methods on a sample level by characterizing it as an outlier, boundary, or IDM example. We contrast DAUC with related works in Table 1.

## 3 Categorizing Model Uncertainty via Latent Density

### Preliminaries

We consider a typical classification setting where \(^{d_{X}}\) is the input space and \(=^{C}\) is the set of class probabilities, where \(d_{X}\) is the dimension of input space and \(C^{*}\) is the number of classes. We are given a prediction model \(f:\) that maps \(x\) to class probabilities \(f(x)\). An uncertainty estimator \(u:\) quantifies the uncertainty of the predicted outcomes. Given some threshold \(\), the inference-time predictions can be separated into trusted predictions \(\{x|u(x)<\}\) and untrusted predictions \(\{x|u(x)\}\). We make the following assumption on the model architecture of \(f\).

**Assumption 1** (Model Architecture).: _Model \(f\) can be decomposed as \(f= l g\), where \(g:^{d_{H}}\) is a feature extractor that maps the input space to a \(d_{H}<d_{X}\) dimensional latent (or representation) space, \(l:^{C}\) is a linear map between the latent space to the output space and \(:^{C}\) is a normalizing map that converts vectors into probabilities._

**Remark 1**.: _This assumption guarantees that the model is endowed with a lower-dimensional representation space. Most modern uncertainty estimation methods like MCD, Deep-Ensemble, BNN satisfy this assumption. In the following, we use such a space to categorize model uncertainty._

We assume that the model and the uncertainty estimator have been trained with a set of \(N^{*}\) training examples \(_{}=\{(x^{n},y^{n}) n[N]\}\). At inference time the underlying model \(f\) and uncertainty method \(u\) predict class probabilities \(f(x)\) and uncertainty \(u(x)\), respectively. We assign a class to this probability vector \(f(x)\) with the map \(:[C]\) that maps a probability vector \(y\) to the class with maximal probability \([y]=_{c[C]}y_{c}\). While uncertainty estimators flag examples to be trustworthy or not, those estimators do not provide a fine-grained reason for what a certain prediction should not be mistrusted. Our aim is to use the

    & Model & Uncertainty & Categorize & Improve &  \\  & Structure & Estimation & Uncertainty & Prediction & \\  BNNs & Bayesian Layers & ✓ &. &. & [8; 9] \\ GP & Gaussian Processes & ✓ &. &. &  \\ MC-Dropout & Drop-Out Layers & ✓ &. &. &  \\ Deep-Ensemble & Multiple Models & ✓ &. &. &  \\ ICP & Model “Wrapper” & ✓ &. &. & [23; 40], \\ Performance Prediction & Multiple Predictive Models & ✓ &. &. & [37; 18] \\  DAUC & Assumption 1 & ✓ & ✓ & ✓ & **(Ours)** \\   

Table 1: Comparison with related work in UQ. We aim to provide a flexible framework for inspecting mistrusted examples identified by uncertainty estimators. Furthermore, this framework enables us to improves the prediction performance on a certain type of flagged uncertain class.

model's predictions and representations of a corpus of labelled examples--which we will usually take to be the training (\(_{}\)) or validation (\(_{}\)) sets--to categorize inference-time uncertainty predictions. To that aim, we distinguish two general scenarios where a model's predictions should be considered with skepticism.

### Flagging OOD Examples

There is a limit to model generalization. Uncertainty estimators should be skeptical when the input \(x\) differs significantly from input examples that the model was trained on. From an UQ perspective, the predictions for these examples are expected to be associated with a large epistemic uncertainty.

A natural approach to flagging these examples is to define a density \(p(_{}):^ {+}\) over the input space. This density should be such that \(p(x_{})\) is high whenever the example \(x\) resembles one or several examples from the training set \(_{}\). Conversely, a low value for \(p(x_{})\) indicates that the example \(x\) differs from the training examples. Of course, estimating the density \(p(x_{})\) is a nontrivial task. At this stage, it is worth noting that this density does not need to reflect the ground-truth data generating process underlying the training set \(_{}\). For the problem at hand, this density \(p(x_{})\) need only measure how close the example \(x\) is to the training data manifold. A common approach is to build a kernel density estimation with the training set \(_{}\). Further, we note that Assumption 1 provides a representation space \(\) that was specifically learned for the classification task on \(_{}\). In Appendix A.1, we argue that this latent space is suitable for our kernel density estimation. This motivates the following definition for \(p(_{})\).

**Definition 1** (Latent Density).: _Let \(f:\) be a prediction model, let \(g:\) be the feature extractor from Assumption 1 and let \(:^{+}\) be a kernel function. The latent density \(p():^{+}\) is defined over a dataset \(\) as:_

\[p(x)_{} [g(x),g()]\] (1)

Test examples with low training density are likely to be underfitted for the model -- thus should not be trusted.

**Definition 2** (OOD Score).: _The OOD Score\(T_{}\) is defined as_

\[T_{}(x)_{})}\] (2)

For a test example \(x_{}\), if \(T_{}(x|_{})_{}\) the example is suspected to be an outlier with respect to the training set. We set \(_{}=_{}} p(x^{}|_{})}\), i.e. a new sample's training density is smaller than the minimal density of training examples.

### Flagging IDM and Boundary Examples

Samples that are not considered outliers, yet are given high uncertainty scores, we divide up further into two non-exclusionary categories. The first category consists of points located near the boundary between two or more classes, the second consists of points that are located in regions of high misclassification.

For achieving this categorization, we will use a separate validation set \(_{}\). We first partition the validation examples according to their true and predicted label. More precisely, for each couple of classes \((c_{1},c_{2})[C]^{2}\), we define the corpus \(_{c_{1} c_{2}}\{(x,y)_{} [y]=c_{1}[f(x)]=c_{2}\}\) of validation examples whose true class is \(c_{1}\) and whose predicted class is \(c_{2}\). In the case where these two classes are different \(c_{1} c_{2}\), this corresponds to a corpus of misclassified examples. Clearly, if some example \(x\) resembles one or several examples of those misclassification corpus, it is legitimate to be skeptical about the prediction \(f(x)\) for this example. In fact, keeping track of the various misclassification corpora from the validation set allows us to have an idea of what the misclassification is likely to be.

In order to make this detection of suspicious examples quantitative, we will mirror the approach from Definition 1. Indeed, we can define a kernel density \(p(_{c_{1} c_{2}}):^{+}\) for each corpus \(_{c_{1} c_{2}}\). Again, this density will be such that \(p(_{c_{1} c_{2}})\) is high whenever the representation of the example \(x\) resembles the representation of one or several examples from the corpus \(_{c_{1} c_{2}}\). If this corpus is a corpus of misclassified examples, this should trigger our skepticism about the model's prediction \(f(x)\). By aggregating the densities associated with each of these corpora, we arrive at the following definition.

**Definition 3** (Confusion Density Matrix).: _Let \(f:\) be a prediction model, let \(g:\) be the feature extractor from Assumption 1 and let \(:^{+}\) be a kernel function. The confusion density matrix \(P(_{}):(^{+})^{C  C}\) is defined as_

\[P_{c_{1},c_{2}}(x_{}) p(x _{c_{1} c_{2}})=_{c_{1}  c_{2}}|}_{ C_{c_{1} c_{2}}}[g(x),g( {x})],(c_{1},c_{2})[C]^{2}\] (3)

**Remark 2**.: _The name confusion density is chosen to make a parallel with confusion matrices. Like confusion matrices, our confusion density indicates the likelihood of each couple \((c_{1},c_{2})\), where \(c_{1}\) is the true class and \(c_{2}\) is the predicted class. Unlike confusion matrices, our confusion density provides an instance-wise (i.e. for each \(x\)) likelihood for each couple._

#### 3.3.1 Bad examples

By inspecting the confusion matrix, we can quantitatively distinguish two situations where the example \(x\) is likely to be mistrusted. The first situation where high uncertainty arises, is when \(g(x)\) is close to latent representations of validation examples that have been correctly assigned a label that differs from the predicted one \([f(x)]\). This typically happens when \(g(x)\) is located close to a decision boundary in latent space. In our validation confusion matrix, this likelihood that \(x\) is related to validation examples with different labels is reflected by the diagonal elements. This motivates the following definition.

**Definition 4** (Boundary Score).: _Let \(P(x_{})\) be the confusion density matrix for an example \(x\) with predicted class \(=[f(x)]\). We define the boundary score as the sum of each density of well-classified examples from a different class:_

\[T_{}(x)=_{c}^{C}P_{c,c}(x_{ }).\] (4)

Points are identified as Bnd when \(T_{}>_{}\)--see Appendix A.2.

#### 3.3.2 IDM examples

The second situation is the one previously mentioned: the latent representation \(g(x)\) is close to latent representations of validation examples that have been misclassified. In our validation confusion matrix, this likelihood that \(x\) is related to misclassified validation examples is reflected by the off-diagonal elements. This motivates the following definition.

**Definition 5** (Idm Score).: _Let \(P(x_{})\) be the confusion density matrix for an example \(x\). We define the IDM score as the sum of each density corresponding to a misclassification of the predicted class \(=\,f(x)\) in the confusion density matrix:_

\[T_{}(x)=_{c}^{C}P_{c,}(x _{}).\] (5)

Points are identified as IDM when \(T_{}>_{}\). We choose \(_{}\) such that the proportion of IDM points in the validation set equals the number of misclassified examples. Details for definitions and choices of thresholds are provided in Appendix A.2.

**Remark 3**.: _Note that the definitions of Bnd examples and IDM examples do not exclude each other, therefore, an uncertain example can be flagged as a Bnd example, an IDM example, or flagged as both Bnd and IDM (B&I). To make this distinction clear, we will refer to the disjoint classes as:_

\[_{} =\{x|x_{},T_{}(x)>_{ },T_{}(x)_{}\}\] \[_{} =\{x|x_{},T_{}(x)_{ },T_{}(x)>_{}\}\] \[_{} =\{x|x_{},T_{}(x)>_{ },T_{}(x)>_{}\}\]

Test examples that are flagged by uncertainty method \(u\)--yet do not meet any of the thresholds--are marked as _Other_.

In a nutshellDAUC uses the OOD, IDM and Bnd classes to categorize model uncertainty--see Table 2 for an overview. Better predictions may be possible for IDM samples, in case a different classifier is used. For samples that are also labelled as Bnd, fine-tuning the existing model--possibly after gathering more data--may be able to separate the different classes better. IDM samples that are not in the Bnd class are harder, and may only be classified correctly if a different latent representation is found, or an different training set is used. We explore the idea of improving the performance on uncertain examples in Appendix B. In Section 4.1 we explore the distinction between classes further.

## 4 Experiments

In this section, we demonstrate our proposed method with empirical studies. Specifically, we use two experiments as Proof-of-Concept, and two experiments as Use Cases. Specifically, in Sec. 4.1 we visualize the different classes of flagged examples on a **modified Two-Moons** dataset; in Sec. 4.2 we quantitatively assess DAUC's categorization accuracy on the **Dirty-MNIST dataset**; in Sec. 4.3, we present a use case of DAUC--comparing existing uncertainty estimation benchmarks; in Sec. 4.4, we demonstrate another important use case of DAUC--improving uncertain predictions.

Our selection of the Dirty-MNIST dataset for empirical evaluation was motivated by the pursuit of better reproducibility. As an existing publicly available resource, Dirty-MNIST provides gold labels for boundary classes and OOD examples, making it particularly suitable for benchmarking the performance of DAUC.

Recognizing the importance of demonstrating the broader applicability of DAUC, we have extended our evaluation to include results on the Dirty-CIFAR dataset. Details of this additional evaluation are available in Appendix C.4, where we also describe how we created the dataset. This dataset will also be made publicly available. Additional empirical evidence that justifies DAUC is provided in Appendix C.

### Visualizing DAUC with Two-Smiles

#### 4.1.1 Experiment Settings

To highlight the different types of uncertainty, we create a modified version of the Two-Moons dataset, which we will call "Two-Smiles". We use scikit-learn's _datasets_ package to generate \(6000\) two-moons examples with a noise rate of \(0.1\). In addition, we generate two Gaussian clusters of \(150\) examples centered at \((0,1.5)\) and \((1,-1)\) for training, validation and test, and mark them as the positive and negative class separately. The data is split into a training, validation and test set. We add an additional \(1500\) OOD examples to the test set, that are generated with a Gaussian distribution centered at \((2,2)\) and \((-1,-1.5)\). Overall, the test set counts \(4500\) examples, out of which \(1500\) are positive examples, \(1500\) are negative examples and \(1500\) are OOD--see Figure 2 (a).

#### 4.1.2 Results

We demonstrate the intuition behind the different types of uncertainty, by training a linear model to classify the Two-Smiles data--i.e. using a composition of an identity embedding function and linear classifier, see Assumption 1. Figure 2 (b) shows the test-time misclassified examples. Since the identity function does not linearly separate the two classes and outliers are unobserved at training

   Type & Definition & Description \\  OOD & \(1/p(x|_{})>_{}\) & Samples that do not resemble the training data. Additional labelled data that covers this part of the input space is required to improve performance on these samples. \\  Bnd & \(_{c}P_{c,c}(x|_{})>_{}\) & Samples near the boundaries in the latent space. Predictions on these samples are sensitive to small changes in the predictor, and fine-tuning the prediction model may yield better predictions. \\  IDM & \(_{c}P_{c,}(x|_{})>_{}\) & Samples that are likely to be misclassified, since similar examples were misclassified in the validation set. \\   

Table 2: Summary of different uncertainty types

[MISSING_PAGE_FAIL:7]

Flagging Bad ExamplesWe expect most of the boundary examples to belong to the Ambiguous-MNIST class, as these have been synthesised using a linear interpolation of two different digits in latent space . Figure 3 (b) shows that DAUC's boundary scores are indeed significantly higher in Ambiguous-MNIST compared to vanilla MNIST. Figure 3 (d) shows the precision-recall curve of DAUC, created by varying threshold \(_{}\). Most boundary examples are correctly discovered under a wide range of threshold choices. This stability of uncertainty categorization is desirable, since \(_{}\) is usually unknown exactly.

Flagging IDM ExamplesIn order to quantitatively evaluate the performance of DAUC on flagging IDM examples, we use a previously unseen hold-out set, which is balanced to consist of \(50\%\) misclassified and \(50\%\) correctly classified examples. We label the former as test-time IDM examples and the latter as non-IDM examples, and compare this to DAUC's categorization. Figure 3c shows DAUC successfully assigns significantly higher IDM scores to the examples that are to-be misclassified.

Varying \(_{}\) we create the precision-recall curve for the IDM class in Figure 3 (e), which is fairly stable w.r.t. the threshold \(_{}\). In practice, we recommend to use the prediction accuracy on the validation dataset for setting \(_{}\)--see Appendix A.2.

Visualizing Different ClassesFigure 4 shows examples from the \(_{}\), \(_{}\), \(_{}\) and \(_{}\) sets. The first row shows OOD examples from the Fashion-MNIST dataset. The second row shows boundary examples, most of which indeed resemble more than one class. The third row shows IDM examples, which DAUC thinks are likely to be misclassified since mistakes were made nearby on the validation set. Indeed, these examples look like they come from the "dirty" part of dirty-MNIST, and most digits are not clearly classifiable. The last row contains B&I examples, which exhibit both traits.

### Benchmark Model Uncertainty Categorization

In this section, we demonstrate how DAUC categorizes existing UQ model uncertainty. We compare UQ methods MC-Dropout  (**MCD**), Deep-Ensemble  (**DE**) and Bayesian Neural Networks  (**BNNs**). These methods output predictions and uncertainty scores simultaneously, we follow the traditional approach to mark examples as uncertain or trusted according to their uncertain scores and specified thresholds. To demonstrate how DAUC categorizes all ranges of uncertain examples, we present results from top \(5\%\) to the least \(5\%\) uncertainty.

Figure 5 compares the proportion of different classes of flagged examples across the three UQ methods. The first and second rows show the _total number_ and _proportion_ of flagged examples for each class, respectively. There is a significant difference in what the different UQ methods identify. There is a significant difference between types of classes that the different UQ methods identify as discussed in the literature [19; 20; 21], which have shown that some UQ methods might not yield high quality uncertainty estimates due to the learning paradigm or sensitivity to parameters. Let us look at each column more carefully:

Figure 4: Examples of different uncertainty classes \(_{}\), \(_{}\), \(_{}\) and \(_{}\). For better visualization, we only plot some classes including the outliers. t-SNE  is leveraged in generating low-dim visualizations.

1. DE tends to identify the OOD examples as the most uncertain examples. Specifically, looking at the bottom figure we see that the top \(5\%\) untrusted examples identified by DE are almost all OOD examples, which is not the case for the other UQ methods. By contrast, MCD is poor at identifying OOD examples; it flags some of the OOD samples as the most certain. This is explained by MCD's mechanism. Uncertainty is based on the difference in predictions across different drop-outs, however this could lead to outliers always having the same prediction-- due to correlation between different nodes in the MCD model, extreme values of the OOD examples may always saturate the network and lead to the same prediction, even if some nodes are dropped out. DE is most apt at flagging the OOD class. This confirms the finding by  who showed that DE outperforms other methods under dataset shift--which is effectively what OOD represents.
2. After the OOD examples have been flagged, the next most suspicious examples are the Bnd and IDM classes--see columns 2 and 3. The number of these examples increases almost linearly with the number of flagged examples, until at about \(88\%\) no more examples are flagged as IDM and Bnd. This behaviour is explained by the Vanilla MNIST examples--which are generally distinguishable and relatively easily classified correctly--accounting for about \(15\%\) of the test examples.
3. As expected, the number of examples belonging to the _Other_ class increases when more examples are flagged as uncertain. This makes sense, as the _Other_ class indicates we cannot flag why the methods flagged these examples as uncertain, i.e. maybe these predictions should in fact be trusted.

### Improving Uncertain Predictions

In this section, we explore the inverse direction, i.e. employing DAUC's categorization for creating better models. We elaborate the practical method in Appendix B. We experiment on UCI's **Covtype**, **Digits** and **Spam** dataset  with linear models (i.e. \(g=Id\)) and experiment on **DMNIST** with ResNet-18 learning the latent representation.

Our empirical studies (Figure 8, Appendix C) have shown that the B&I examples are generally hardest to classify, hence we demonstrate the use case of DAUC for improving the predictive performance on this class. We vary the proportion \(q\) of training samples that we discard before training new prediction model \(f_{}\)--only saving the training samples that resemble the B&I dataset most--see Figure 6. We find that retraining the linear model with filtered training data according to Eq. 6 significantly improves the performance. We observe that performance increases approximately linearly proportional to \(q\), until the amount of training data becomes too low. The latter depends on the dataset and model used.

Figure 5: Results of applying our method in categorizing different uncertainty estimation methods. First row: comparisons on the numbers in different classes of examples. Second row: comparisons on the proportion of different classes of flagged examples to the total number of identified uncertain examples. Different methods tend to identify different certain types of uncertain examples. The results presented are based on 8 repeated runs with different random seeds.

## 5 Conclusion and Future Work

We have proposed DAUC, a framework for model uncertainty categorization. DAUC categorizes uncertain examples identified by UQ benchmarks into three classes--OOD, Bnd and IDM. These classes correspond to different causes for the uncertainty and require different strategies for possibly better predictions. We have demonstrated the power of DAUC by inspecting three different UQ methods--highlighting that each one identifies different examples. We believe DAUC can aid the development and benchmarking of UQ methods, paving the way for more trustworthy ML models.

In future work, DAUC has great potential to be extended to more general tasks, such as the regression setting, and reinforcement learning setting, where uncertainty quantification is essential. The idea of separating the source of uncertainty improves not only exploration  but also exploitation in the offline settings .

In the era of Large Language Models (LLMs) , uncertainty quantification is essential in evaluating the task performance of LLMs , and holds great potential for AI alignment  -- as understanding the ability boundary of LLMs is essential, and identifying the suspicious outputs of LLMs can be potentially addressed by extending the framework of DAUC to the LLMs' setting.