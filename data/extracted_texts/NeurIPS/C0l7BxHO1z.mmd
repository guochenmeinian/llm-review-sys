# Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models

Salma Abdel Magid, Weiwei Pan, Simon Warchol, Grace Guo,

**Junsik Kim, Wanhua Li, Mahia Rahman, Hanspeter Pfister**

Harvard University

Cambridge, MA, USA

Correspondence to: sabdelmagid@g.harvard.edu, weiweipan@g.harvard.edu

###### Abstract

Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose _Concept2Concept_, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user defined distributions or empirical, real world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool facilitating use by non-technical end-users.

_Warning: This paper contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers._

## 1 Introduction

Text-to-image (T2I) models have become central to many real-world AI-driven applications. However, the complexity of these models makes it difficult to understand how they associate concepts in images with textual prompts. Existing works have shown that T2I models can resolve prompts in unexpected ways (Bianchi et al. (2023)). Furthermore, the training datasets for T2I models are often large, uncurated, and may contain undesirable prompt to image associations that models can learn to internalize (Birhane et al. (2024)). Thus, without robust auditing frameworks that help us detect these undesirable associations, we risk deploying T2I models that generate unexpected and inappropriate content for a given task.

However, auditing T2I models is challenging because it is difficult to systematically, efficiently, and intuitively explore the vast space of prompts and possible outputs. Because raw pixel values alone are difficult to semantically reason about, previous works learn mappings from raw inputs to high-level concepts. This can be achieved post-hoc (Kim et al. (2018); Zhou et al. (2018); Ghorbani et al. (2019)) or as an intervention during training (Koh et al. (2020);Chen et al. (2020). Although these methods were designed for classification networks, it is this general intuition which motivates our work.

In this paper, we propose a framework for producing interpretable characterizations of the conditional distribution of generated images given a prompt, \(p(|)\). We do so by extracting high-level concepts from each image and summarizing \(p(|)\) in terms of such concepts. Here, we define concepts as a class of objects/nouns, ideas, open vocabulary detected classes or labels.

Our contributions are as follows:

(1) We propose **an interpretable framework** for concept-association based auditing of conditional distributions. Specifically, in our framework: we sample images from a T2I model under audit, given a prior distribution over prompts-either a user defined distribution or an empirical real-world distribution. Then, using a fast, scalable visual grounding model, we extract concepts from generated images. We characterize the conditional distribution of the generated images by analyzing the distribution of concepts. This framework allows users to systemically investigate associations of conditional distributions at varying levels of granularity, from broad concept trends and co-occurrences, to detailed visual features. By design, our framework utilizes visual grounding models that localize concepts in images, enabling a deeper analysis of visual representations. Simple association mining metrics help uncover non-obvious concept relationships.

(2) We demonstrate a wide range of concrete use-cases for our framework, by applying it to audit models and prompt datasets. In addition to demonstrating the effectiveness of the framework, our analysis unearthed **new findings** that are independently significant. In particular, _we discovered child-sexual abuse material (CSAM)_ in a human-preferences prompt dataset and misaligned classes in a synthetically generated ImageNet dataset. These findings not only demonstrate the utility of our framework but also contribute to the broader discourse on the safety, fairness, and alignment of T2I models.

(3) We introduce an **interactive visualization tool**, based on our framework, for human-in-the-loop auditing of T2I models. Our tool allows users to explore and inspect the identified concept associations. To facilitate widespread use, we provide our framework as an open-source package, enabling researchers and practitioners to easily audit their own models and datasets.

## 2 Related Work

**Biases in T2I models.** There is a body of works that have qualitatively investigated biases in T2I models, focusing on social biases related to gender, race, and other identity attributes. For example, Bianchi et al. (2023) qualitatively demonstrated a range of social biases in T2I models, including biases related to basic traits, social roles, and everyday objects. Similarly, Ungless et al. (2023) manually analyzed images generated by T2I models and found that certain non-cigsender identities were misrepresented, often depicted in stereotyped or sexualized ways. Through several focus groups, Mack et al. (2024) found that T2I models repeatedly presented "reductive archetypes for different disabilities". Qualitative evaluations play a critical role in exposing instances where generative models can be biased. However, given the large space of possible prompts and images, instance-based bias probing alone cannot paint a systematic picture of how T2I models may (mis)behave in application.

A number of works have focused on automating bias detection at scale. For example, in Cho et al. (2023), the authors measured visual reasoning skills and social biases in T2I models by using a combination of automated detectors and human evaluations to assess the representation of different genders, skin tones, and professions. Likewise, Luccioni et al. (2024) utilized Visual Question Answering (VQA) models and clustering-based evaluations to measure correlations between social attributes and identity characteristics. TIBET (Chinchure et al. (2023)) dynamically generated axes of bias and counterfactual prompts based on a single input prompt. However, these works either do not operate on the general concept-level (e.g. only specifically probe for concepts related to social attributes) and/or do not leverage the rich information in the concept co-occurrences, the stability of concepts, nor do they pinpoint and extract specific concepts. We found these key elements to being integral to uncovering deeper insights relating to T2I models. Most closely related to our work is Try Before You Bias (TBYB) (Vice et al. (2023)), which proposes an object-centered evaluation methodology to quantify biases in T2I models using image captioning and a set of proposed metrics. Also like us, CUPID (Zhao et al. (2024)) presents a visualization framework that enables users to discover salient styles of objects and object relationships by leveraging low-dimensional density-based embeddings. Our approach generalizes and builds upon these previous works. While existing methodsfocus primarily on social bias and style relationships, our framework enables a more nuanced audit of model behavior, capturing not only social biases but also the underlying patterns in how models represent and associate visual concepts.

**Important use cases of synthetic data.** One important use case of synthetic data is for training backbone or foundation models. Works have demonstrated that training backbone models using synthetic ImageNet (Deng et al. (2009)) clones can achieve similar performance on specific evaluation benchmarks as compared to real ImageNet dataset (Azizi et al. (2023);He et al. (2022);Saryldiz et al. (2023)). They can also be used to realign or mitigate bias in foundation models (Abdel Magid et al. (2024);Howard et al. (2024)) or evaluate vision-language models (Fraser and Kiritchenko (2024); Smith et al. (2023)). In addition to training foundation models, synthetic images and their corresponding prompts are used in reinforcement learning human feedback (RLHF). Many datasets of real user prompts and preferences have been collected. Examples include RichHF-18K (Liang et al. (2024)), ImageReward (Xu et al. (2024)), and Pick-a-Pic (Kirstain et al. (2023)). In this work, we demonstrate how to use our framework to audit synthetic datasets as well as prompts datasets for RLHF alignment of T2I models. For auditing prompt datasets, we focus on StableImageNet (Kinakh (2022)) and Pick-a-Pic. The latter is used to train the PickScore which is then used as an evaluation metric and to better align T2I models with human preferences.

Concept2Concept: An Intuitive Framework for Characterizing the Conditional Distribution of T2I Models

We propose _Concept2Concept_, a novel framework to provide systematic and interpretable characterizations of the conditional distribution of images generated by a T2I model given a prompt, \(p(|)\). We do so by first extracting high-level concepts from generated images and then characterizing the conditional distribution of these concepts given prompts, \(p(|)\).

**Obtaining Concept Distributions from T2I Models.** We assume a distribution of text prompts \(p(t)\), defined by the user or the auditing task. We empirically represent \(p(t)\) with \(N\) sampled prompts \(\{t_{i}\}_{i=1}^{N}\) from \(p(t)\):

\[t_{i} p(t),i=1,2,,N.\] (1)

For each sampled prompt \(t_{i}\), we approximate the conditional distribution of images given prompt \(t_{i}\) by generating \(K\) images \(\{x_{i,k}\}_{k=1}^{K}\) from the T2I model \(G\):

\[x_{i,k} p_{G}(x_{i,k}|t_{i}),k=1,2,,K.\] (2)

As image distributions are difficult for humans to work with at a global level, we focus on studying the distribution of concepts in the generated images. Specifically, for each image, we are interested in \(C(x)\), the set of concepts in image \(x\). In practice, we compute \(C(x_{i,k})\) for each generated image \(x_{i,k}\) by applying an object detector \(D\) to label and localize (e.g., bounding box) the concepts in the image \(C_{i,k}=D(x_{i,k})\). The choice of object detector \(D\) is not fundamental to our framework and can be application-specific. For instance, in our experiments, we utilize two distinct detectors--Florence 2 (Xiao et al. (2023)) and BLIP VQA (Li et al. (2022))--each offering different levels of detection capabilities. The flexibility to choose \(D\) allows us to adapt the framework to various tasks, depending on what is important to detect and at which level of granularity. Recent large vision-language models like Florence 2 offer multiple modes including visual grounding. We note that our use of an object detector \(D\) can introduce uncertainty in the extracted concepts, \(C_{i,k}\) (e.g., due to detection confidence levels or the probabilistic nature of the model). Thus, we consider \(C_{i,k}\) as samples from a distribution \(C_{i,k} p(C|x_{i,k})\). In the case that concepts are extracted deterministically from a given image \(x_{i,k}\), \(p(C|x_{i,k})\) is a delta distribution.

Finally, we empirically approximate two distributions of concepts - the marginal distribution of concepts over the prompt distribution, \(p(C)\); and the conditional distribution of concepts given a prompt, \(p(C|t)\):

\[p(C)=_{t}p(C|t)p(t)\,dt, p(C|t)=_{x}p(C|x)p_{G}(x|t)\,dx.\] (3)

**Summarizing Concept Distributions.** We further summarize the concepts distributions \(p(C)\) and \(p(C|t)\) we obtain from the T2I model to enable end-users in exploring and discovering associations between concepts in the prompt and concepts in the generated images. Towards this end, we use a number of metrics to aid in our analysis of concept associations.

_Concept Frequency \(P(c)\)._ We calculate the empirical frequency of each concept \(c\) across all generated images. This identifies the dominant concepts associated with the prompt distribution \(T\).

_Concept Stability._ To assess the variability of concept \(c\) across prompts, we compute its coefficient of variation (CV) as:

\[CV(c)=}{P(c)},_{c}= _{i=1}^{N}{(P(c t_{i})-P(c))}^{2}}.\] (4)

We set a threshold \(\) to focus on concepts that occur with sufficient frequency: \(_{}=\{c P(c)>\}\). Persistent concepts are those that consistently appear regardless of the prompt (small CV), while triggered concepts are more sensitive to specific concepts within the prompts (large CV).

_Concept Co-Occurrence._ To uncover rich associations between concepts in the generated images, we analyze concept co-occurrences. For each pair of concepts \((c,c^{})\), we compute the co-occurrence probability:

\[P(c,c^{})=^{N}{_{k=1}^{ K}[c,c^{} C_{i,k}]}}{N K}.\] (5)

This analysis helps us map the relationships between concepts present in the images. We refer the reader to the appendix for additional details.

**Choosing Task-Relevant Prompt Distributions \(p(t)\).** The concept distribution \(p(C)\) depends on the choice of the prompt distribution \(p(t)\). Generally, the choice of \(p(t)\) should be informed by the task, e.g. auditing models for social biases. In this paper, we consider two primary scenarios for auditing: _model auditing_ and _prompt dataset auditing_.

_Model Auditing._ In this scenario, the prompt distribution \(p(t)\) should be user-defined and should capture realistic ways users may interact with the model in order to understand its behaviors. Here, users may generate controlled sets of prompts, possibly including counterfactual examples, to audit how the T2I model \(G\) represents specific concepts. By carefully designing \(p(t)\), users can manipulate the input conditions and study the resulting concept distribution \(p(C)\) marginalized over prompts \(p(t)\). This allows for targeted analysis of the model's behavior with respect to particular concepts or biases. We provide several experiments in section 4.

_Prompt Dataset Auditing._ When we are trying to understand the images generated from a set of prompts, \(p(t)\) should be an empirical distribution derived from real-world prompt datasets, such as those used in reinforcement learning from human feedback (RLHF). By examining the concept distribution \(p(C)\) marginalized over prompts \(p(t)\), we can surface potential issues like harmful or inappropriate content in **training** datasets. We provide several experiments in section 5.

## 4 Application 1: Auditing the Model

### Case Study 1: Replicating Bias Probing Results from Literature

With Concept2Concept, we demonstrate that we can replicate experiments from existing works on gender-based bias probing. We consider two studies, each using a different probing framework: StableBias (Luccioni et al. (2024)) and Try Before You Bias (TBYB) (Vice et al. (2023)). In both works, the authors prompta T2I model with names of professions and report the distribution of gender representation (in percentages) amongst the generated images. Our findings are summarized in Table 1. Consistent with the two existing studies, we found that the concept  is underrepresented across most professions, with only about 30% of the images depicting the concept , while approximately 70% of the images depicted the concept . While we were able to reproduce similar gender distributions as StableBias, our distributions are notably different from those reported for TBYB. We provide a discussion for this discrepancy in A.6.

### Case Study 2: Scaling Up Qualitative Studies on Disability Representation

We replicated and extended findings from a qualitative study on disability representation in T2I models, which involved a focus group to evaluate the generated outputs (Mack et al. (2024)). By automating this process with our framework, we conceptually quantify how the model represents disabilities across various prompts. Concretely, \(_{}=\{t_{i}=\}\) where [value] \(\) {a disability, bipolar disorder, a chronic illness, cerebral palsy, a limb difference, hearing loss}. Figure 7 (top left) shows that for the prompt "a person with a disability," **nearly 100% of the generated images depicted , despite not being explicitly stated in the prompt.** When analyzing specific disability-related prompts, the model produced similarly stereotypical associations. For instance, the prompt "cereb palsy" primarily generated images of  and , while "a limb difference" shown in Figure 7 (bottom left) resulted in images with the concepts  and ; individuals in the images are typically dressed in shorts to emphasize the disability. Unexpectedly, , co-occurred with . We visualize this in Figure 7 (bottom right) and find that the model produces -like sticks, perhaps to represent crutches. In the case of "chronic illness," the model often depicted people in , , with their . Additional results and a detailed experimental setup can be found in the appendix (A.8).

## 5 Application 2: Auditing Prompt Datasets

### Case Study 3: Detecting Unexpected Issues in Pick-A-Pic

_Warning: This section contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers._ The Pick-a-Pic dataset (Kirstain et al. (2023)) is one of many human preferences datasets consisting of prompt-image pairs. Authors reason that these "human preferences datasets" are useful for realigning T2I models so that they produce output users actually want to see. Kirstain et al. (2023) train the PickScore on the Pick-a-Pic training set to learn their collected human preferences. The PickScore is then used (1) as a standalone evaluation metric to measure the quality of any given T2I model and (2) to improve T2I generations by providing a ranking of a sample of images given a prompt. It is clear that these two use cases are incredibly safety-critical. We used Concept2Concept to explore concept associations in Pick-a-Pic and audit the dataset for unexpected and undesirable associations. Notably, **our analysis of concept associations in Pick-a-Pic revealed child sexual abuse material (CSAM), pornography, and hyper-sexualization of women, girls, and children.**

 
**Model** &  &  \\   & \% woman & \% man & \% woman & \% man \\  StableBias (Luccioni et al. (2024)) & 31.10 \% & 68.90\% &  &  \\  Ours & 28.41 \% & 71.59\% & & \\  TBYB (Vice et al. (2023)) & 31.64\% & 68.36\% & & \\  Ours & 19.56 \% & 80.44 \% & & \\  

Table 1: The average percentage of detections of  and  generated by a concept detector in our framework for the StableBias (Luccioni et al. (2024)) and TBYB (Vice et al. (2023)) case studies. Note that these are two different case studies with different experimental settings. U.S. Bureau of Labor Statistics.

We draw 10 random samples of size 1K each from the training split of the Pick-a-Pic dataset 1. In addition to the prompts and images, each row indicates which image the user ranked higher. When sampling, we save

Figure 1: Co-occurrences of concepts with the detected concepts \(\) and \(\) in 10 random samples of the Pick-A-Pic DatasetKirstain et al. (2023). Most frequent concepts overall can be found in A.9.

Figure 2: Prompts in the Pick-a-Pic dataset that trigger the \(\), \(\), and \(\), concept associations. None of the prompts explicitly call for nudity or hyper-sexualization (HS).

[MISSING_PAGE_FAIL:7]

Similarly, for the class redbone, the intended ImageNet class refers to "A variety or breed of American bound with a predominantly red coat..."3 However, the model instead generated images of human individuals. Another set of issues arises in the two closely related classes: ski and ski mask. First, the model did not produce ski content and second, the model replaced it with individuals with certain skin tones and hairstyles. The issue is thus two fold; one of prompt adherence and one of fairness. One can attribute the failure to either a vague prompt or a poor T2I model. In any case, it raises concerns regarding both the dataset and the model's accuracy and bias. It is also important to note that while this exact dataset was not published in a specific paper, the recipe for generation is replicated in other works as a comparison point (Bansal and Grover (2023); Sarvilduz et al. (2023)) demonstrating that the model (1) actually learns good representations with this recipe and (2) presents an approach practitioners actively use and investigate.

## 6 Interactive Tool

Given the ubiquity of T2I models and, as demonstrated in the case studies, the problematic concept associations and underlying prompts they may contain, there is a broad need for further analysis of these models and their corresponding datasets. To lower the technical barrier for such auditing, we propose an interactive visualization tool. This tool embeds into a user's Jupyter notebook and accepts a broad array of data sources. A user can investigate specific concepts, their stability, and co-occurrence with other concepts (Figure 23). Additionally, users may search for specific concepts to identify the prompts used to generate the concept, the distribution of these prompts, and, localize how the concept is depicted in different images (Figure 24).

## 7 Conclusion

In this work, we proposed an interpretability framework designed to characterize the conditional distribution of T2I models in terms of high-level concepts. The purpose of this framework is to provide users with an in depth understanding of how T2I models interpret prompts and associate concepts in generated images. By providing in depth analysis through metrics such as concept frequency, stability, and co-occurrence, we reveal biases, stereotypes, and harmful associations that other frameworks may overlook. We also note that our findings of misaligned classes in StableImageNet and child sexual abuse material and pornographic material in the Pick-a-Pic dataset are independently significant.

Figure 3: Sample of misaligned synthetic ImageNet images detected through the conceptual characterization of conditional distributions through our framework. The first 9 images from each class. Clear misalignment. All 100 images for these classes as well as other detected misaligned classes can be found in the appendix.