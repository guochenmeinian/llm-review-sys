# PDF: Point Diffusion Implicit Function

for Large-scale Scene Neural Representation

 Yuhan Ding\({}^{1}\)

dingyh22@m.fudan.edu.cn

&Fukun Yin\({}^{1}\)

fkyin21@m.fudan.edu.cn

&Jiayuan Fan\({}^{2}\)

yfan@fudan.edu.cn

Hui Li\({}^{2}\)

lihui21@m.fudan.edu.cn

&Xin Chen\({}^{3}\)

chenxin2@shanghaitech.edu.cn

&Wen Liu\({}^{3}\)

liuwen@shanghaitech.edu.cn

&Chongshan Lu\({}^{1}\)

cslu17@fudan.edu.cn

&Gang Yu\({}^{3}\)

iskicy@gmail.com

&Tao Chen\({}^{1}\)

eetchen@fudan.edu.cn

\({}^{1}\) School of Information Science and Technology, Fudan University, China

\({}^{2}\) Academy for Engineering and Technology, Fudan University, China

\({}^{3}\) Tencent PCG, Shanghai, China

Joint first authors.Corresponding author.

###### Abstract

Recent advances in implicit neural representations have achieved impressive results by sampling and fusing individual points along sampling rays in the sampling space. However, accurately representing and synthesizing fine-grained textures in unbounded, large-scale outdoor scenes presents a significant challenge, attributable to the exponentially expanding sampling space. To alleviate the dilemma of using individual sampling points to perceive vast expanses, we explore learning the surface distribution of the scene to provide structural priors, thereby reducing the samplable space, and propose a **P**oint **D**iffusion implicit **F**unction, **PDF**, for large-scale scene neural representation. The core of our method is a large-scale point cloud super-resolution diffusion module that enhances the sparse point cloud reconstructed from several training images into a dense point cloud as the explicit prior. Then in the rendering stage, only sampling points with prior points within the sampling radius are retained. That is, the sampling space is reduced from the unbounded space to the scene surface. Meanwhile, to fill in the background details not captured by point clouds, we employ region sampling based on Mip-NeRF 360 for modeling comprehensive background representations. Extensive experiments have demonstrated the effectiveness of our method for large-scale scene novel view synthesis, which outperforms relevant state-of-the-art baselines.

## 1 Introduction

Implicit neural representations have demonstrated proficiency in handling single objects or small scenes and have found extensive applications in the fields of virtual reality , 3D reconstruction , video generation  and computer animation  on tasks such as scene representation and new perspective synthesis. Nevertheless, as the scale of the target scene increases, particularly in urban-scale outdoor scenes, traditional implicit neural representation methods encounter significantperformance limitations. This issue primarily arises from the cubic expansion of the sampling space in larger scenes, rendering it challenging for individual sampling points to cover the entire space.

Fortunately, some methods try to solve this problem through two primary strategies, narrowing the sampling space and expanding the sampling area. The first approach represented by Mega-NeRF  decomposes the sampling space into multiple subspaces and models each subspace separately to reduce complexity. But with the scene scale growing, such as reaching the city level, the quantity of these subspaces increases cubically, posing scalability concerns. In contrast, the method represented by Mip-NeRF 360  compresses the sampling space or samples an area instead of a single point so that the sampling space can be filled more easily. Nevertheless, this approach may compromise the precision of the representation.

Benefiting from the inspiration of geometric priors aiding vision tasks, which are widely used in the domain of 3D reconstruction and stereo vision [16; 3; 6]. We are curious whether implicit large scene representations could be made easier with explicit representations. Moreover, for outdoor unbounded large scenes, most of the sampling space is filled with air rather than buildings, cars, plants and other objects that we care about. A reasonable solution is to restrict the large-scale sampling space of the implicit neural representation to the object surface, which is provided by the scene geometry prior. That is to say, we compress a 3D sampling space to a 2D surface plane, which will greatly reduce the representation complexity. At the same time, the network will pay more attention to the foreground, which is the same as the human visual perception system. Of course, for the neglected background information that is relatively less important, we can provide a relatively less accurate expression by compressing the scene to sample the area in space.

In this paper, we propose **PDF**, a **P**oint **D**iffusion implicit **F**unction for large-scale scene neural representation, which learns a dense surface distribution via a diffusion-based point prior generative model to reduce the sampling space. To achieve this, we first explore a large-scale outdoor point cloud augmentation method based on the Point-Voxel Diffusion model . Since point clouds of real outdoor scenes often lack dense ground truth, it is difficult to train a completion module through "sparse-dense" point cloud pairs. Therefore, we initially downsample the point cloud twice, and train a point cloud super-resolution network to generate the denser one from its sparser counterpart. This approach effectively generates dense point clouds in the absence of ground truth data. With the help of the surface point cloud, the sampling points will be retained only if there are reconstruction points within a certain radius, so the space will be greatly reduced to the scene surface. However, the reconstructed point cloud can only model the scene surface and cannot deal with the unbounded background of outdoor scenes. Accordingly, following the concept of NeRF++ , we model the foreground and background separately, and use Mip-NeRF 360  to extract background features by sampling regions in the scene space.

Extensive experiments show the effectiveness of our point diffusion implicit function for large-scale scene neural representation, which achieves photo-realistic rendering results and outperforms state-of-the-art methods on OMMO  and BlendMVS dataset . We summarize the contributions as follows: **1)** Aiming at novel view synthesis for large outdoor scenes, we propose an implicit neural representation framework based on point diffusion models to provide dense surface priors to cope with the exploding sampling space. **2)** A novel point cloud super-resolution diffusion module is proposed to generate dense surface points from sparse point clouds without dense annotations. **3)** Extensive experiments demonstrate that our PDF network outperforms state-of-the-art methods, including robustness to large-scale outdoor scene representation and the capability to synthesize more photo-realistic novel views. Our code and models will be available.

## 2 Related Work and Background

### Implicit Neural Representation

In recent years, Implicit Neural Representation (INR) has witnessed significant advancements and provides a versatile framework for representing complex functions and generating high-dimensional data [23; 17; 26; 37]. By implicitly encoding the scene's appearance and geometry, neural radiance fields enable highly realistic rendering and novel view synthesis [25; 20; 8; 18].

Building upon this foundation, subsequent research has focused on addressing the limitations and pushing the boundaries of INR. Efforts have been made to improve the efficiency and scalabilityof neural radiance fields. For instance, Hanocka et al. propose DeepSDF, which leverages signed distance functions to implicitly represent 3D shapes. This formulation allows for efficient ray-marching and facilitates tasks such as shape manipulation and interpolation. Furthermore, recent advancements in INR have explored differentiable rendering and differentiable volumetric rendering, enabling the incorporation of geometric and physical priors [10; 4; 27] into the representation. These methods leverage the differentiable nature of neural networks to optimize scene parameters, leading to improved realism and control over the generated content [13; 39; 9; 30]. Another significant extension to the field of INR is PixelNeRF . It extends the capabilities of INR to handle images, going beyond the realm of 3D scenes. PixelNeRF introduces a new differentiable sampler to handle image-based representations, enabling efficient and accurate sampling of pixels from the neural radiance field. In addition to PixelNeRF, Semantic Neural Radiance Fields propose a method to learn scene representations that capture geometry, appearance, and semantic information, facilitating interactive virtual scene editing and content creation.

Overall, these advancements have greatly expanded the capabilities of INR. These developments offer promising avenues for realistic image synthesis, shape completion, scene reconstruction, and dynamic content generation. The ongoing research in this field holds great potential for further advancements in computer graphics, computer vision, and virtual reality applications.

### Large-scale Scene Representation

Large-scale scene representation is a crucial aspect of INR research, particularly in the context of computer graphics and computer vision. It involves capturing and modeling complex scenes that encompass extensive spatial extents, such as urban environments, landscapes, or virtual worlds.

One notable work in the domain of large-scale scene representation is Neural Scene Flow Fields. This paper introduces a novel approach to model dynamic scenes at a large scale. The authors propose a scene flow field representation that captures both the geometry and motion of objects in the scene. By leveraging a neural network architecture, they achieve accurate and temporally consistent scene synthesis and reconstruction, even in highly complex and dynamic scenes. The Neural 3D Mesh Renderer is another significant contribution in large-scale scene representation. This work addresses the challenge of representing and rendering detailed 3D meshes of large-scale scenes efficiently. The authors propose a neural network-based renderer that predicts view-dependent textures and geometric details of the scene. This approach enables real-time rendering and interaction with large-scale 3D scenes, opening up possibilities for interactive virtual reality experiences and immersive simulations. In addition to these works, Mega-NeRF  and Bungee-NeRF  are two other notable approaches based on the neural radiance field for constructing interactive 3D environments from large-scale visual captures. They address the challenges of modeling and rendering large-scale scenes, spanning from buildings to multiple city blocks and utilizing thousands of images captured from drones. They extend the capabilities of NeRF to handle multi-scale rendering, capturing various levels of detail and enabling the interactive exploration of diverse 3D environments.

Overall, the field of large-scale scene representation within INR has witnessed significant progress. These contributions have paved the way for realistic, interactive, and semantically meaningful representations of expansive virtual environments, urban landscapes, and dynamic scenes. The ongoing research in this area holds great potential for further advancements in computer graphics, virtual reality, and immersive simulations.

## 3 Methodology

In this paper, we aim to develop a novel point diffusion model implicit function to reduce the sampling space and improve the ability to represent large-scale scenes (_c.f_. Fig. 1). Our PDF network mainly consists of two modules, a diffusion-based component for point cloud super-resolution and foreground rendering, and a region-sampling module focused on background processing. The former introduces a diffusion model to enhance the sparse point cloud reconstructed from the input images into a dense point cloud, which provides optional points in the rendering stage to reduce the sampling space (_c.f_. Sec. 3.1). The latter samples regions rather than individual points from unbounded scenes so that it is easy to fill sampled regions and complement the background for new viewpoint synthesis (_c.f_. Sec. 3.2). In the final subsection, implementation details and losses are elaborated (_c.f_. Sec. 3.3).

### Point Upsampling Diffusion

In this section, we introduce our large-scale outdoor point cloud super-resolution module based on a denoising diffusion probabilistic model (\(c.f.\) Fig. 2).

**Point Cloud Pair Preparation.** Due to the lack of dense large-scale outdoor point cloud ground truth, we need to train a diffusion-based super-resolution network to sample a dense surface, symbolized as \(x_{d}^{N 3}\), from the point cloud reconstructed by COLMAP , denoted as \(x_{s}^{M 3}\). Concurrently, to mitigate the risk of over-fitting, the point cloud reconstructed from the training views is not utilized as the ground truth for the diffusion model. Instead, training data comprises pairs of the sparse point cloud \(z_{0}^{n 3}\) and the sparser point cloud \(x_{0}^{m 3}\), adhering to \(m<n<M<N\). More specifically, we downsample the sparse point cloud \(x_{s}\) reconstructed by COLMAP to get an even sparser point cloud \(z_{0}\). Then we further downsample \(z_{0}\) to get the sparsest point cloud \(x_{0}\), where \(x_{s}\), \(z_{0}\) and \(x_{0}\) have progressively sparser relationships. Our training process recovers \(z_{0}\) from the sparsest \(x_{0}\). During testing, we take \(x_{s}\) as input to generate a denser super-resolved point cloud \(x_{d}\).

**Point Super-resolution Diffusion.** Our point super-resolution denoising diffusion probabilistic model is a generative model, which starts with Gaussian noise and progressively denoises to generate scene structure priors. We record the output containing different levels of noise produced by each step as \(_{T}\), \(_{T-1}\),..., \(_{0}\), where \(_{T}\) is sampled from Gaussian noise, and \(_{0}\) represents the generated point cloud with dense surface. Since we already have a sparse point cloud prior \(z_{0}\), our target point cloud can be denoted as \(x_{0}=(z_{0},_{0})\) and the intermediate point cloud during the denoising process can be denoted as \(x_{t}=(z_{0},_{t})\). Subsequently, we define a point super-resolution diffusion process involving a prior shape \(z_{0}\), consisting of a forward process and a backward process.

Forward Process. Gaussian noise is repeatedly added to the original point cloud \(x_{0}\), resulting in a series of noisy point clouds \(x_{1}\), \(x_{2}\),..., \(x_{T}\):

\[q(_{t}|_{t-1},z_{0})(_{t};}_{t-1},_{t}I)\] (1)

where \(_{t}\) represents a pre-defined increasing sequence of Gaussian noise values, which dictates the magnitude of noise incrementally introduced at each step of the process.

Reverse Process. Given a point cloud with more noise \(x_{t}\), reverse the forward process and find the posterior distribution for a less noisy one \(x_{t-1}\):

\[p_{}(_{t-1}|_{t},z_{0})(_{}(x_{t},z_{0},t),_{t}^{2}I)\] (2)

where \(_{}(x_{t},z_{0},t)\) is the predicted shape at \(t-1\) step.

Figure 1: The pipeline of our point diffusion implicit function. Our method consists of two modules, a point diffusion rendering module and a background rendering module. The former learns the surface distribution of the scene through a diffusion-based point cloud super-resolution model and renders foreground features from the dense point cloud surface. The latter follows Mip-NeRF 360â€™s strategy to render background features. Finally, the foreground and background features are fused to generate photo-realistic novel views for large-scale outdoor scenes.

Therefore, our point cloud upsampling diffusion model can be regarded as a noise adding and denoising process. The former gradually adds random noise to the initial point cloud \(x_{0}\) through the forward process; the latter denoises sequentially through the reverse process to obtain a dense point cloud \(x_{0}\). Based on Markov transition probabilities, the whole process can be expressed as:

\[q(_{0:T},z_{0})=q(_{0},z_{0})_{t=1}^{T}q(_{t}|_ {t-1},z_{0})\] (3)

\[p_{}(_{0:T},z_{0})=p(_{T},z_{0})_{t=1}^{T}p_{}( _{t-1}|_{t},z_{0})\] (4)

Throughout the optimization process, our prior shape \(z_{0}\) is fixed, and only the missing surface point cloud is diffused. The network is typically trained with a simplified \(L2\) denoising loss:

\[}=||-_{}(_{t},z_{0},t)||^{2}\] (5)

where \(\) is the added random noise and \((0,I)\), and \(_{}(_{t},z_{0},t)\) is the prediction noise output. Since point cloud prior \(z_{0}\) is fixed, it will be masked when minimizing the loss.

### Volume Rendering and Implicit Function Representation

**Foreground Rendering.** For the foreground, we sample points along the rays from the dense point cloud \(x_{0}\) and render features in the neighborhood, following Point-NeRF . The difference is that our point super-resolution diffusion module only generates denser point cloud coordinates without color information, so we redesign a more general point aggregate module. For ray marching through a pixel, we sample \(M\) sampling points at \(\{p_{i} i=1,...,M\}\), and query \(K\) neighboring neural points \(kp_{i}=\{kp_{i}^{1},kp_{i}^{2},...,kp_{i}^{K}\}\) around \(p_{i}\) within a certain euclidean distance radius \(R\). Then we interpret the local geometric structure as a feature \(f_{i}\) of each sampling point \(p_{i}\) to equip structural information. Therefore, we utilize \(c_{i}\) and \(kc_{i}\) to represent the coordinates of the sampling point and its neighborhood points, respectively. The geometric structure feature \(kf_{i}=\{kf_{i}^{1},kf_{i}^{2},...,kf_{i}^{K}\}\) of the neighborhood are encoded as follows:

\[kf_{i}=MLP(c_{i} kc_{i}(c_{i}-kc_{i}) d(c_{i},kc_{i}))\] (6)

where \(d(,)\) is the Euclidean distance between two points, and \(\) is the concatenation operator. Next, the local geometric features \(f_{i}\) of the sampling points \(p_{i}\) are obtained by neighborhood points \(kp_{i}\) weighted summation:

\[f_{i}=SUM(softmax(MLP(kf_{i})) kf_{i})\] (7)

Figure 2: Our point upsampling diffusion. In the forward process, Gaussian noise is gradually added to the sparse point cloud. In the reverse process, the noise is gradually removed to obtain a dense point cloud surface.

where softmax operation is performed on each dimension, and \(\) is the hadamard product. Our point-based radiance field can be abstracted as a neural module that regresses the volume density \(\) and view-dependent radiance \(r\) from coordinates \(c\), local geometric features \(f\), and ray direction \(d\) according to Point-NeRF :

\[(,r)=PointNeRF(c,d,f)\] (8)

Finally, the foreground feature is synthesized by each neural sampling point along the sampling ray.

**Background Rendering.** In consideration of the limitation that point clouds are confined to representing foreground elements and are incapable of addressing unbounded background contexts, it becomes necessary to procure supplementary background features. Benefiting from Mip-NeRF 360 , which contracts the scene to a bounded ball and then samples a region to meet the challenge of large scenes, we employ this method to extract background features as a supplement along the same sampling ray as Point-NeRF .

**Fore-Background Fusion.** Since the detail-preserving foreground features can be obtained from the dense surface points, while the bounded domain can cope with large scenes but loses details during the compression process. So we propose a foreground-background fusion module consisting of several layers of multi-layer perceptrons to preserve their respective advantages.

We adopt \(L2\) loss to supervise our rendered pixels \(r_{p}\) from ray marching with the ground truth \(r_{g}\), to optimize our PDF volume render reconstruction network.

\[_{}=||r_{p}-r_{g}||^{2}\] (9)

### Implementation Details

Our PDF method is a two-stage neural representation network for outdoor unbounded large-scale scenes. We optimize these two stages separately.

In the first stage, a diffusion-based point cloud super-resolution network is designed to learn a prior distribution to generate a dense point cloud surface. In the point cloud pair preparation process, we employed the random down-sampling method with a retention rate between 0.2 and 1 for both samplings. For point super-resolution diffusion, we set \(T\) = 1000, \(_{0}\) = \(10^{-4}\), \(_{T}\) = 0.01 and linearly interpolate other \(\)'s for all experiments. We use Adam optimizer with learning rate \(2 10^{-4}\) and train on 4 A100 GPUs for around one day.

In the second stage, the foreground and background extraction modules plus a feature fusion module are optimized. We find 8 neighbors for each sampling point and expand the dimension of neighborhood geometric features to 8. Both the foreground and the background output a 128-dimensional feature, and then they are concatenated and passed through 4 MLP layers to get the color of the rendered point. We train this stage using Adam optimizer with an initial learning rate \(5 10^{-4}\) for \(2 10^{6}\) iterations about 20 hours on a single A100 GPU.

## 4 Experiments

### Experimental settings

Dataset.We use two outdoor large-scale scene datasets, OMMO  and BlendedMVS , to evaluate our model. The OMMO dataset is a real fly-view large-scale outdoor multi-modal dataset, containing complex objects and scenes with calibrated images, prompt annotations and point clouds. The number of training point cloud samples in the OMMO dataset varies from 40,000 to 100,000 for different scenes, including abundant real-world urban and natural scenes with various scales, camera trajectories, and lighting conditions. More experimental results can be found in our supplementary material.

Baselines and Evaluation Metrics.We compare our method with the previous state-of-the art methods on novel view synthesis, including NeRF , NeRF++ , Mip-NeRF , Mip-NeRF 360 , Mega-NeRF , Ref-NeRF . NeRF is the first continuous MLP-based neural network for synthesizing photo-realistic views of a scene through volume rendering. NeRF++ models large-scale unbounded scenes by separately modeling foreground and background neural representations.

[MISSING_PAGE_FAIL:7]

time, compared with Mip-NeRF 360, our method is more robust to scene representation and new view generation without failing scenes (\(c.f.\) Fig. 4).

### Ablation Studies

We perform multiple ablation studies to validate the effectiveness of our proposed modules. Tab. 2 shows the impact of diffusion point cloud super-resolution module and background feature fusion module on the 5-th scene (sydney opera house) from the OMMO dataset .

Figure 3: Qualitative results of our method with the baselines on the OMMO dataset. Our PDF method outperforms baseline methods with reliably constructed details. For Mip-NeRF and Mega-NeRF, which are also aimed at large scenes, we use yellow dashed boxes to mark some areas that are easy to distinguish the performance of details. Please zoom-in for the best of views.

For the ablation experiment on the effectiveness of diffusion, we remove the diffusion-based point cloud up-sampling module and sample directly on the sparse point cloud reconstructed by COLMAP  from training views. Since the directly reconstructed point cloud is very sparse and concentrated in the central area, only a very blurry image with large missing blocks can be rendered, as shown in the first column of Fig. 5. At the same time, quantitative indicators also suggest that this method is not suitable for outdoor unbounded large-scale scenes with its PSNR of 9.28.

For the ablation experiment on the effectiveness of background fusion, we remove the background fusion module and render novel view images directly from the diffusion-enhanced point cloud. As shown in the second column of Fig. 5, with the help of the dense point cloud produced by the diffusion module learning the scene distribution, we find that large missing patches have been filled in and produce a more refined foreground. However, limited by the characteristics of point cloud expression, the background points are very sparse, which leads to blurred background rendering results. Quantitative results, while substantially improved, still convey poor image quality.

As shown in the third column of Fig. 5, using the background fusion module alone can also fill in the missing blocks of the background, but due to the sparseness of the point cloud reconstructed by COLMAP , it will lead to the loss of detail and blurring of the rendering result. However, our method, which combines a diffusion module and a background fusion module, achieves satisfactory quantitative and qualitative performance and surpasses existing methods.

We also perform ablation experiments to compare our method with other point cloud up-sampling methods. With the same experimental setup, we use a GAN-based method  for point cloud up-sampling instead of the diffusion-based up-sampling module. Tab. 3 shows the quantitative results for three scenes (scan5, scan11 and scan12) in the OMMO dataset. Our method exhibits superior performance compared to the GAN-based point cloud up-sampling method, primarily due to its

  Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  w/o diffusion, w/o background & 9.28 & 0.51 & 0.355 \\ w/o diffusion, w/ background & 21.05 & 0.83 & 0.219 \\ w/ diffusion, w/o background & 22.93 & 0.78 & 0.235 \\ 
**Ours** & **27.58** & **0.90** & **0.162** \\  

Table 2: Quantitative performance of ablation experiments, including removing both the diffusion-based point cloud up-sampling module and the background fusion module, removing only the diffusion-based point cloud up-sampling module, removing only the background fusion module, our PDF method.

Figure 4: A failure scene representation of Mip-NeRF 360.

Figure 5: Qualitative performance of ablation experiments. From left to right: removing both the diffusion-based point cloud up-sampling module and the background fusion module, removing only the background fusion module, removing only the diffusion-based point cloud up-sampling module, removing only the diffusion-based point cloud up-sampling module, our PDF method, and the groundtruth.

ability to preserve the structural and topological characteristics of point clouds while effectively handling incomplete or noisy point cloud data. In addition, Fig. 6 shows the visualization results of the diffusion-based point cloud up-sampling module, and our method can not only densify the sparse point cloud reconstructed by the COLMAP, but also fill in the missing regions of the point cloud such as the background and empty space.

## 5 Conclusions and Limitations

In this paper, we propose PDF, a point diffusion implicit function for large-scale scene neural representation, and demonstrate its robustness and fidelity on novel view synthesis tasks. The core of our method is to provide dense point cloud surface priors to reduce the huge sampling space of large-scale scenes. Therefore, a point cloud super-resolution module based on diffusion model is proposed to learn from the sparse point cloud surface distribution reconstructed from training views to generate more dense point clouds. However, only constraining the sampling space to the point cloud surface does not fully solve the novel view synthesis problem since point clouds do not have background information. So Mip-NeRF 360  is employed to provide background features and synthesize photo-realistic new perspectives. Extensive experiments demonstrate that our method outperforms current methods in both subjective and objective aspects. At the same time, ablation experiments also prove the effectiveness of our core module, point up-sampling diffusion.

In future work, we will attempt to explore a cross-scene point cloud up-sampling generalization diffusion model instead of training a diffusion model for each scene to improve efficiency. Even more futuristically, it may be possible to extract representative scene representations and inject them into reconstructed point clouds to achieve cross-scene rendering, i.e., generalized point diffusion NeRF.