# Bridging the Divide: Reconsidering Softmax and Linear Attention

Dongchen Han1 Yifan Pu1 Zhuofan Xia1 Yizeng Han Xuran Pan

Xiu Li Jiwen Lu Shiji Song Gao Huang2

Tsinghua University

Equal Contribution.Corresponding Author.

###### Abstract

Widely adopted in modern Vision Transformer designs, Softmax attention can effectively capture long-range visual information; however, it incurs excessive computational cost when dealing with high-resolution inputs. In contrast, linear attention naturally enjoys linear complexity and has great potential to scale up to higher-resolution images. Nonetheless, the unsatisfactory performance of linear attention greatly limits its practical application in various scenarios. In this paper, we take a step forward to close the gap between the linear and Softmax attention with novel theoretical analyses, which demystify the core factors behind the performance deviations. Specifically, we present two key perspectives to understand and alleviate the limitations of linear attention: the injective property and the local modeling ability. Firstly, we prove that linear attention is not injective, which is prone to assign identical attention weights to different query vectors, thus adding to severe semantic confusion since different queries correspond to the same outputs. Secondly, we confirm that effective local modeling is essential for the success of Softmax attention, in which linear attention falls short. The aforementioned two fundamental differences significantly contribute to the disparities between these two attention paradigms, which is demonstrated by our substantial empirical validation in the paper. In addition, more experiment results indicate that linear attention, as long as endowed with these two properties, can outperform Softmax attention across various tasks while maintaining lower computation complexity. Code is available at https://github.com/LeapLabTHU/InLine.

## 1 Introduction

Recent years have witnessed the unprecedented success of Transformer and attention  in the field of computer vision . Softmax attention, also known as dot-product attention, has demonstrated remarkable expressive power, leading to state-of-the-art performance across various vision tasks . However, applying Softmax attention in vision also faces challenges. The quadratic complexity of Softmax attention results in prohibitively high computational cost when applied with a global receptive field. Previous works  have strived to reduce the computation complexity by restricting receptive fields or introducing sparsity. Although effective, these approaches inevitably compromise Softmax attention's ability for long-range modeling and scalability.

The nature of Softmax attention forces to compute the dot-products between queries and keys \(QK^{}^{N N}\) at first, and then aggregates values \(V^{N C}\) by the normalized score \((QK^{}/)\), which accounts for the quadratic \((N^{2})\) complexity _w.r.t._ the sequence length \(N\). On the contrary, linear attention relaxes the similarity score between \(Q\) and \(K\) from Softmax to other functionswhich can be decomposed into kernels, _i.e._, the linear attention replaces the original score function \((Q,K)\!=\!(QK^{}/)\) with \((Q,K)\!=\!(Q)(K)^{}\), where \(()\) is the kernel function. This substitution enables a change in the computation order from \(((Q)(K)^{})V\) to \((Q)((K)^{}V)\) based on the associative law of matrix multiplication, reducing the complexity from \((N^{2})\) to \((N)\)_w.r.t._ the sequence length \(N\). Nevertheless, every coin has two sides. Linear attention proves to be less effective than Softmax attention [3; 27; 40], whose poor expressive power limits its practical application. Although many pieces of research [27; 28; 22; 9; 40] have attempted to alleviate this issue in different ways, we still do not have a complete understanding of the key factors that contribute to the gap between linear and Softmax attention.

In this paper, we delve into the fundamental differences between linear and Softmax attention, offering two insightful perspectives to demystify the topic: the injective property and local modeling capability. Firstly, we consider attention as a function that maps a query to an attention score. We find that the injectivity of this attention function greatly affects the performance of the model. Specifically, if the attention function is not injective, different queries will induce identical attention distributions, leading to severe semantic confusion within the feature space. Our rigorous analysis has demonstrated that the Softmax attention function is an injective function, whereas the linear attention function is not. Therefore, linear attention is vulnerable to the semantic confusion problem, which largely leads to its insufficient expressiveness. Secondly, our analysis of the attention weight distribution has confirmed that the success of Softmax attention is not solely dependent on its strong long-range modeling capabilities. Effective local modeling is also crucial to achieving optimal outcomes.

To validate our analyses, we present two simple yet effective methods to endow linear attention with the injective property and the local modeling ability, respectively. The widely employed Swin Transformer  architecture is used to validate our findings. The results highlight the importance of both properties in the gap between linear and Softmax attention. Moreover, comprehensive experiments demonstrate that linear attention, endowed with these two properties, outperforms the widely used Softmax attention across diverse tasks.

Our main contributions and takeaways are summarized as follows: **(1) Injectivity** is a key disparity between linear and Softmax attention. While Softmax attention is injective, the non-injective nature of linear attention causes semantic confusion and severely impairs model performance. To the best of our knowledge, our work is the first to conceptualize attention as a mapping function and prove the vital importance of its injective property. **(2) Local modeling** is still essential to the effectiveness of the attention mechanism, even though it is renowned for its large receptive field and outstanding long-range modeling ability. **(3)** We challenge the viewpoint that linear attention is inferior to Softmax attention and demonstrate that with the above two properties, linear attention can outperform Softmax attention while maintaining lower computation complexity.

## 2 Related Works

**Vision Transformer and Softmax Attention.** Vision Transformer  is the pioneer work that introduces self-attention to vision. Since then, attention has found success in various vision tasks [7; 2; 24]. The widely used attention mechanism is Softmax attention , also known as dot-product attention, which computes the similarity between all query-key pairs. Although effective, its quadratic computation complexity leads to unmanageable cost when processing global feature maps. Therefore, various approaches [19; 34; 36; 14; 45; 12] have been proposed to reduce the computational overhead of Softmax attention. PVT  employs downsampling of keys and values to reduce computational complexity. Swin Transformer  restricts the receptive field by introducing window attention pattern. NAT  mimics convolution and calculates attention within the neighborhood of each feature, and DAT  presents an input-dependent sparse attention pattern.

**Linear Attention.** As opposed to Softmax attention, Linear attention is another attention paradigm with a natural linear complexity of \((N)\). Linear attention replaces Softmax with kernel functions, thereby reducing computational complexity to linear through a change in computation order. Nonetheless, prior studies [27; 3; 29; 40; 11] have demonstrated that linear attention performs markedly worse than Softmax attention. CosFormer attributed this discrepancy to the efficient re-weighting mechanism of Softmax attention and proposed cosine re-weighting to enhance linear attention. Nystromformer  and SOFT  use matrix decomposition to further approximate Softmax operation. Efficient Attention  applies Softmax function to queries and keys. TransNormer  identifiesthat unbounded gradients and attention dilution harm linear attention. FLatten Transformer  introduces a focused function to address the over-smoothing issue. MLLA  draws inspiration from Mamba  to improve linear attention.

Despite their elegant outcomes, the fundamental reason for the disparity between linear attention and Softmax attention remains unclear. In this work, we perform an in-depth analysis of the disparities between linear and Softmax attention, identifying two crucial properties of high-performance Softmax attention: injectivity and local modeling ability. We present both theoretical proofs and experimental verification to validate our findings.

## 3 Preliminaries

**Attention Formulation**. Let \(x^{N C}\) be an input of \(N\) tokens. In each self-attention head, \(x\) is transformed into \(Q=xW_{Q},K=xW_{K},V=xW_{V}\) through projection matrices \(W_{Q/K/V}^{C d}\), where \(C\) and \(d\) are the channel dimension of module and each head. Therefore, we have \(Q,K,V^{N d}\), and \(Q_{i},K_{i},V_{i}^{d}\). Based on this, Softmax attention  computes the attention weights and calculates the output as the weighted sum of value:

\[S_{i}=(Q_{i}^{}K_{1})\\ _{j=1}^{N}(Q_{i}^{}K_{j}),,^{ }K_{N})}{_{j=1}^{N}(Q_{i}^{}K_{j})}^{},\ \ \ O_{i}^{S}=S_{i}^{}V.\] (1)

For simplicity, we omit \(\) in \((Q_{i}^{}K_{j}/)\) since we can equivalently renormalize \(Q\) and \(K\). This attention paradigm has been highly successful in modern vision Transformers. However, it should compute the similarity between all query-key pairs, resulting in \((N^{2})\) complexity. Consequently, employing Softmax attention with a global receptive field results in overwhelming computation cost.

Figure 1: **An illustration of injective property and confusion problem**. Non-injectivity leads to various semantic confusions in linear attention when different kernel functions are employed. (a) With \(()=()\), linear attention assigns the same attention values to collinear queries of varying lengths. (b) Using \(()=(A+b)\), linear attention faces severe confusion problem, producing identical attention distribution for certain queries with different directions and lengths.

Linear attention  was proposed to efficiently handle the computation challenge with linear complexity of \((N)\). Specifically, \((Q_{i}^{}K_{j})\) is replaced by \((Q_{i})^{}(K_{j})\), where \(\) is kernel function. In this way, linear attentions reformulate eq.1 as:

\[ L_{i}=[)^{}(K_{1})}{ _{j=1}^{N}(Q_{i})^{}(K_{j})},,)^{} (K_{N})}{_{j=1}^{N}(Q_{i})^{}(K_{j})}]^{}\!\!,\\ O_{i}^{L}=L_{i}^{}V=_{j=1}^{N})^{}( K_{j})}{_{j=1}^{N}(Q_{i})^{}(K_{j})}V_{j}^{}=)^{}(_{j=1}^{N}(K_{j})V_{j}^{})}{(Q_{i})^{}(_{j =1}^{N}(K_{j}))}.\] (2)

The form of \(O_{i}^{L}\) suggests that explicitly computing attention weights \(L_{i}\) is unnecessary. Instead, we can change the computation order from \(((Q)(K)^{})V\) to \((Q)((K)^{}V)\) based on the associative property of matrix multiplication. By doing so, the computation complexity is reduced to \((N)\).

**Injective Property.** Let \(f:A B\) be a mapping function. We call \(f\) an injective function if and only if \( x,y A,x y\), it holds that \(f(x) f(y)\).

## 4 Analysing the Gap between Linear and Softmax Attention

Due to its linear computation complexity, linear attention is considered a promising solution to address the computational challenges of Softmax attention in high-resolution scenarios. However, previous works [27; 3; 40] have shown that linear attention's expressive power is significantly lower than that of Softmax attention, rendering it impractical for real-world applications. In this section, we conducted an in-depth analysis of the gap between linear and softmax attention from two perspectives: injective mapping and local modeling capability, and offer both theoretical proofs and experimental verification to enhance understanding of the key disparities between these two attention types.

### Injectivity of Attention Function

We first define the function of Softmax and linear attention as follows:

\[},}:^{d}^{N},\ \ \ }(Q_{i})=S_{i},\ \ \ }(Q_{i})=L_{i},\] (3)

where \(Q_{i}\) denotes the query, and \(S_{i},L_{i}\) are the attention scores in eq.1 and eq.2. Given keys \(K^{N d},},}\) can be viewed as the function of query \(q\), mapping each \(q\) to its corresponding Softmax and linear attention scores, \(}(q)\) and \(}(q)\). Then the final outputs of Softmax and linear attention corresponding to \(q\) can be formulated as \(O^{S}=}(q)^{}V\) and \(O^{L}=}(q)^{}V\).

**Injective property**. In this work, we identify that the _injective property_ of the attention function significantly impacts model performance, which may largely contribute to the gap between linear and Softmax attention. Specifically, we prove that under mild assumptions, the Softmax attention function \(}\) is injective, whereas linear attention function \(}\) is not (Proposition1 and 2. Please refer to Appendix for complete proof). As a consequent, for two different queries \(p\) and \(q\) (\(p q\)), Softmax attention should produce different attention distributions \(}(p)}(q)\), while linear attention may yield the same linear attention values \(}(p)=}(q)\). Since different queries \(p q\) typically represent distinct semantics, the non-injective property of linear attention actually leads to semantic confusion, i.e. \(}(p)=}(q)\) and \(O_{p}^{L}=}(p)^{}V=}(q)^{}V=O_{q}^{L}\), making the model unable to distinguish certain semantics.

**Proposition 1** (Softmax attention is injective): _Given \(K^{N d}\) with \((K)=d\) and \(([K,_{N 1}])=d+1\). \(\)\(p,q^{d},p q\), we have \(}(p)}(q)\)._

**Proposition 2** (Linear attention is not injective): _Let \(:^{d}^{d}\) be a continuous function. \(\)\(p,q^{d},p q\), s.t. \(}(p)=}(q)\)._

We provide an example to better understand the injective property and confusion problem. As shown in Fig.1(a), there are four collinear vectors with different lengths. Benefiting from injectivity, Softmax attention ensures that each of these four queries obtains distinct attention scores, producing more focused attention distributions for longer queries. Nevertheless, with kernel function \(()=()\), linear attention fails to distinguish the same semantics with different intensities, i.e. collinear queries with varying lengths, resulting in identical attention scores for all these four queries. Consequently,linear attention is unable to yield more focused attention scores for stronger semantics, which may explain the lack of focus ability discussed in . When using kernel functions with stronger nonlinearity, linear attention encounters more pronounced confusion issues. For instance, in Fig. 1(b), employing kernel function \(()=(A+b)\), linear attention assigns exactly the same attention scores to four queries with different directions and lengths. This serious semantic confusion can directly impair model's performance.

**Confusion problem in real models**. While Fig. 1 illustrates the concept of confusion, it is also crucial to verify if this issue occurs in real models. Therefore, we conduct statistical analysis based on Deit-T. We count the occurrences of confusion for each image (i.e., \(p q\) but \(_{}(p)\!=\!_{}(q)\) or \(_{}(p)\!=\!_{}(q)\)) during inference on the ImageNet  validation set. As it is rare for two vectors to be strictly equal in floating-point representation, we consider them approximately equal if the L2 norm of their difference is less than 1e-3. The results are provided in Fig. 2. Almost all samples did not encounter confusion on model employing Softmax attention, whereas a large number of samples encountered confusion more than \(2^{5}\) times on linear attention model. This proves the existence of confusion problem with linear attention in real models.

**The importance of injectivity**. We further verify the importance of injectivity by inducing confusion in Softmax attention. To achieve this, we apply additional non-injective mapping functions to each query before the Softmax attention calculation, i.e., introducing \(Q_{i}\!=\!f(Q_{i})\) prior to eq. (1), where \(f\) is a non-injective function. Specifically, we use \(f_{1}(q)=(q)}{\|(q)\|}\) to simulate the confusion observed in linear attention using the kernel function \(()\!=\!()\), as depicted in Fig. 1(a), and employ \(f_{2}(q)\!=\!(Aq+b)}{\|(Aq+b)\|}\) to replicate the confusion in Fig. 1(b). As shown in Tab. 1, introducing confusion leads to an obvious decrease in performance, underscoring the crucial role of the attention function's injective property. Therefore, the non-injectivity of linear attention is likely a key factor leading to its limited expressive capacity.

**Make linear attention injective**. We propose a simple yet effective solution to make linear attention an injective function. The proof of Proposition 2 (see Appendix) demonstrates that \( 0,(p)\) obtains identical scores in linear attention due to the omission of \(\) in division, resulting in non-injectivity. Hence, we simply transform the normalization of linear attention from division to subtraction, presenting our **injective linear attention (InLine)** as follows:

\[_{}(Q_{i})=(Q_{i})^{}(K_{1}),, (Q_{i})^{}(K_{N})^{}\!-_{s=1}^{N}(Q_ {i})^{}(K_{s})+,\] (4)

and the attention output corresponding to \(Q_{i}\) can be written as \(O_{i}^{I}=_{}(Q_{i})^{}V\). This modification ensures that the attention weights still sum up to 1, while transforming the attention function into an injective one (see Proposition 3). Thus, injective linear attention can distinguish different queries, akin to Softmax attention, and it no longer suffers from confusion problem.

**Proposition 3** (InLine attention is injective): _Let \(:^{d}^{d}\) be an injective map. Given \(K^{N d}\) with \(((K))\!=\!d\) and \(([(K),_{N 1}])\!=\!d+1\). \( p,q^{d},p q,_{}(p) _{}(q)\)._

Additionally, similar to linear attention, InLine attention can be calculated with \((N)\) complexity by changing the computation order:

\[ O_{i}^{I}=_{}(Q_{i})^{}V =&_{j=1}^{N}[(Q_{i})^{}(K_{j})- _{s=1}^{N}(Q_{i})^{}(K_{s})+]V_{ j}^{}\\ &=(Q_{i})^{}[_{j=1}^{N}(K_{j})V_{j}^{} ]-[(Q_{i})^{}_{j=1}^{N}(K_{j})-1] _{j=1}^{N}V_{j}.\] (5)

    &  Confusion \\  &  None \\  &  \(f_{1}(q)\!=\!(q)}{\|(q)\|}\) \\  & 
 \(f_{2}(q)\!=\!(Aq+b)}{\|(Aq+b)\|}\) \\  \\  Acc. & 72.2 & 70.6 & 69.9 \\   

Table 1: Introducing confusion to Softmax attention.

Figure 2: The distribution of the number of times each image encounters confusion during inference.

Since we can compute \(_{j=1}^{N}(K_{j})\) and \(_{j=1}^{N}V_{j}\) once and reuse them for every query, the overall complexity of InLine attention is \(2Nd^{2}+Nd=(Nd^{2})\). Accounting for multi-head, the complexity becomes \((NCd)\), where \(C\) and \(d\) are the channel dimension of the model and each head.

### Local Modeling Capability

Attention mechanism is famous for its large receptive field and outstanding long-range modeling capability. However, we find that effective local modeling is crucial for the effectiveness of attention.

In Fig. 3, we compute the sum of attention values assigned to local \(3 3\) neighborhoods for each query using DeiT-T. With a total of \(14 14\)\(+\)\(1\)\(=\)\(197\) tokens in each attention layer of DeiT-T, if attention scores are randomly assigned, the expected sum of attention for a \(3 3\) neighborhood would be \(\). The result shows that all three attention paradigms tend to pay more attention to the neighborhoods of each query, revealing local bias, especially in shallow layers. Notably, Softmax attention allocates a substantial amount of attention to local windows, suggesting a stronger local modeling ability compared to the other two attention paradigms. Visualizations are provided in Fig. 4 to further confirm this finding.

We speculate that Softmax attention's superior performance stems from robust local priors and strong local modeling capabilities. To validate this hypothesis, we employ attention masks to mask out tokens from various positions and assess their effect on model performance.The results are presented in Tab. 2. Two key observations emerge: 1. Masking out local tokens significantly decreases model performance, while randomly masking out the same number of tokens has a minor impact on results. 2. Softmax attention's performance suffers more severely than InLine attention when local tokens are masked out. These findings demonstrate the significance of local modeling for both attention types and prove that Softmax attention's advantage over InLine attention primarily attributes to its stronger local modeling ability.

Based on our analysis, increasing local bias may enhance the expressive power of InLine attention. In light of this, we employ a MLP to predict additional local attention residual for InLine attention.

   Mask Out Position & None & Loc. \(3 3\) & Loc. \(5 5\) & Loc. \(7 7\) & Rand 9 & Rand 25 & Rand 49 \\  Softmax Attn & 72.2 & 51.6 & 24.3 & 9.0 & 71.7 & 71.5 & 71.1 \\ InLine Attn & 70.0 & 58.0 & 40.0 & 20.0 & 70.0 & 69.9 & 69.5 \\   

Table 2: Model performances on ImageNet-1K when masking out tokens from different positions. Loc. \(k k\) means masking out tokens in local \(k k\) windows for each query. Rand \(n\) represents randomly masking out \(n\) tokens out of local \(3 3\) windows for each query. The attention scores of each query still sum up to \(1\). These models are tested directly without retraining.

Figure 4: Visualizations of attention distributions. Softmax attention exhibits strong local bias. The other two attention types yield meaningful attention distributions, but focus more on global modeling.

Figure 3: The sum of attention scores in the local \(3 3\) windows of each query from DeiT-T.

Specifically, the output corresponding to \(Q_{i}\) is defined as:

\[O_{i}=}(Q_{i})^{}V+_{j=1}^{9}r_{j}V_{j}^{N(i)},\ \ r=( ),\] (6)

where \(}()\) denotes InLine attention function, \(\) is the average of input tokens, \(r\) is the predicted local attention residual, and \(V_{j}^{N(i)}\) represents the value in the \(3 3\) neighborhood of \(Q_{i}\). In this way, we explicitly enhance InLine attention's local bias by introducing local attention residual term. We refer to InLine attention with local attention residual, i.e. eq. (6), as **InLine attention module**. As the local residual term introduces little computational cost \(Nd+d^{2}+9Nd\), the InLine attention module still maintains a linear complexity of \((N)\).

## 5 Empirical Study

In Sec. 4, we analyzed two core factors behind the performance gap between Softmax and linear attention, proposing possible remedies. In this section, we conduct empirical verification to fully validate the importance of these two properties and the effectiveness of our methods.

### Implementation

We utilize the popular Swin Transformer architecture  to investigate the effects of injectivity and local modeling capability. Specifically, we substitute the original Softmax attention in Swin-T with linear attention to establish the baseline model. Subsequently, we introduce the injective property and local bias in turn to assess their respective impacts. To fully verify the effectiveness of InLine attention module, we further apply it to four advanced and representative Transformer models including DeiT , PVT , Swin , CSwin  and offer broad comparisons with various state-of-the-art methods using Softmax attention.

### Datasets and Experiment Details

**ImageNet classification**. The ImageNet-1K  recognition dataset contains 1.28M training images and 50K validation images with a total of 1,000 classes. For a fair comparison, we train our model using identical settings as the corresponding baseline model. We use AdamW  optimizer to train all our models from scratch for 300 epochs, employing cosine learning rate decay with 20 epochs of linear warm-up. The initial learning rate is \(1 10^{-3}\), and the weight decay is 0.05. Augmentation and regularization strategies consist of RandAugment , Mixup , CutMix , and random erasing . Following CSwin , EMA  is used in the training of InLine-CSwin models.

**COCO object detection**. COCO  object detection and instance segmentation dataset has 118K training and 5K validation images. We follow the training and testing strategies of the corresponding baseline model and employ pretrained InLine backbones to conduct experiments.

**ADE20K semantic segmentation**. ADE20K  is a well-established benchmark for semantic segmentation which encompasses 20K training images, 2K validation images and 150 semantic categories. The same setting as baseline model is adopted.

    & Window & FLOPs & \#Param & Acc. & & Window & FLOPs & \#Param & Acc. \\   & \(7^{2}\) & 4.5G & 30M & 80.3 &  & \(7^{2}\) & 4.5G & 30M & 81.6 \\  & \(14^{2}\) & 4.5G & 30M & 80.4 & & \(14^{2}\) & 4.5G & 30M & 82.1 \\    & \(28^{2}\) & 4.5G & 30M & 80.2 & & \(28^{2}\) & 4.5G & 30M & 82.3 \\    & \(56^{2}\) & 4.5G & 30M & 80.2 & & \(56^{2}\) & 4.5G & 30M & 82.4 \\   

Table 4: Ablation on local modeling ability based on Swin-T. \(()\) kernel function is used.

   Kernel Function \(()\) & \(()\) & \((A+b)\) & \(()\) & \(()\) \\  Linear Attn & 77.3 & 70.2 & 1.5 & 0.2 \\ InLine Attn & 79.8 & 80.0 & 79.8 & 80.2 \\   

Table 3: Ablation on the impact of injective property using Swin-T.

### Empirical Analysis of Injectivity and Local Modeling

**Injective property**. As shown in Tab. 3, we adopt four different kernel function \(()\) to validate the effect of injectivity. As discussed in Sec. 4.1, with kernel function \(()=()\), linear attention fails to distinguish the same semantics with different intensities. Addressing this issue with InLine attention leads to a 2.5 increase in accuracy. When using \(()=(+b)\), linear attention faces more severe semantic confusion, and introducing injective property results in a significant accuracy boost of 9.8, from 70.2 to 80.0. These obvious improvements fully prove the significance of injectivity and validate the effectiveness of our injective linear attention. We also employ two kernel functions that do not ensure non-negativity. Consistent with the findings in , linear attention fails to converge without non-negativity assurance. We attribute this to extreme semantic confusion. For example, with \(()=()\), linear attention is unable to distinguish completely opposite semantics, assigning identical attention scores to \(q\) and \(-q\).

**Local modeling capability**. Tab. 4 highlights the importance of local modeling capability. In the _left table_, we apply pure InLine attention to Swin-T and gradually increase the window size from \(7^{2}\) to \(56^{2}\). Due to the linear complexity of InLine attention, we can adopt different window sizes while preserving identical computational cost. Larger window sizes lead to larger receptive fields, typically associated with improved performance. However, the results show that the model performance does not improve with increasing window sizes. We believe this can be attributed to the insufficient local modeling capability: a small window size restricts the receptive field but introduces strong local bias, enhancing local modeling, while a large window size enlarges the receptive field but further diminishes local modeling ability. To validate this, we apply InLine attention with local residual and present the results in the _right table_. Significant improvements can be observed upon the introduction of the local residual term. Additionally, the increase in window size leads to steady performance improvement after introducing local residual, which strongly supports our analysis.

  
**Method** & **Reso \#Params** & **FLOPs** & **Top-1** \\  PVTv2-B2  & \(224^{2}\) & 25M & 4.0G & 82.0 \\ ConvNeXt-T  & \(224^{2}\) & 29M & 4.5G & 82.1 \\ Focal-T  & \(224^{2}\) & 29M & 4.9G & 82.2 \\ MViTv2-T  & \(224^{2}\) & 24M & 4.7G & 82.3 \\ CSwin-T  & \(224^{2}\) & 23M & 4.3G & 82.7 \\ DiNAT-T  & \(224^{2}\) & 28M & 4.3G & 82.7 \\ InLine-CSwin-T & \(224^{2}\) & 21M & 4.3G & 83.2 \\   ConvNeXt-S  & \(224^{2}\) & 50M & 8.7G & 83.1 \\ PVTv2-B3  & \(224^{2}\) & 45M & 7.9G & 83.2 \\ CSwin-S  & \(224^{2}\) & 35M & 6.9G & 83.6 \\ Focal-T  & \(224^{2}\) & 51M & 9.4G & 83.6 \\ MViTv2-S  & \(224^{2}\) & 35M & 7.0G & 83.6 \\ InLine-CSwin-S & \(224^{2}\) & 33M & 6.8G & 83.8 \\   

Table 6: Comparison with SOTA methods on ImageNet-1K.

  
**Method** & **Reso \#Params** & **FLOPs** & **Top-1** \\  DeiT-T  & \(224^{2}\) & 5.7M & 1.2G & 72.2 \\
**InLine-DeiT-T** & \(224^{2}\) & 6.5M & 1.1G & **74.5(\(\)2.3)** \\  DeiT-B & \(224^{2}\) & 86.6M & 17.6G & 81.8 \\
**InLine-DeiT-B** & \(488^{2}\) & 23.8M & 17.2G & **82.3(\(\)0.5)** \\ PVT-S & \(224^{2}\) & 24.5M & 3.6G & 79.8 \\
**InLine-PVT-S** & \(224^{2}\) & 21.6M & 3.9G & **82.0(\(\)2.2)** \\ PVT-L & \(224^{2}\) & 61.4M & 9.8G & 81.7 \\
**InLine-PVT-L** & \(224^{2}\) & 50.2M & 10.2G & **83.6(\(\)1.9)** \\   

Table 5: Comparison with baseline models on ImageNet-1K. See full comparison table in Appendix.

### Main Results and Broad Comparisons

As shown in Tab. 4, InLine-Swin-T with local residual achieves better results than the Swin-T baseline, increasing from 81.3 to 82.4. Therefore, we wonder whether InLine attention module can perform better than the widely adopted Softmax attention in various scenarios. To validate this, we further apply it to several representative Transformers and conduct comprehensive comparisons on image classification, object detection, and semantic segmentation.

**ImageNet classification.** Firstly, We apply our InLine attention module to DeiT , PVT , and Swin Transformer , presenting the results in Tab. 5. It can be seen that substituting Softmax attention with our method results in notable improvements. For example, InLine-PVT-S outperforms PVT-L with 30% of the parameters and 40% of the FLOPs. Subsequently, We apply our module to the advanced Transformer design, CSwin Transformer , and offer a broad comparison with various state-of-the-art models on ImageNet-1K. As depicted in Tab. 6, our InLine-CSwin model not only yields better results than CSwin, but also surpasses various SOTA CNN and Transformer designs. These results demonstrate that our InLine attention module tends to be a superior alternative to the widely used Softmax attention.

**Inference throughput analysis.** We offer real speed measurements in Fig. 5. As shown in Fig. 5(a), InLine models achieve an obviously better trade-off between accuracy and latency. In Fig. 5(b), we increase the window size from \(7^{2}\) to \(56^{2}\). Due to the quadratic complexity of Softmax attention, Swin-T's speed drops sharply as window goes larger. On the contrary, InLine-Swin-T with linear complexity even exhibits higher speed with larger windows. This may be due to the reduction in the latency caused by the window partition. With a global receptive field, InLine benefits from both high performance (see Tab. 4) and fast speed. Furthermore, Fig. 5(c) shows the significant computational advantage of InLine in high-resolution scenarios.

    \\ Method & FLOPs & Sch. & AP\({}^{b}\) & AP\({}^{b}_{50}\) & AP\({}^{b}_{75}\) & AP\({}^{m}\) & AP\({}^{m}_{50}\) & AP\({}^{m}_{75}\) \\  PVT-T & 240G & 1x & 36.7 & 59.2 & 39.3 & 35.1 & 56.7 & 37.3 \\  InLine-PVT-T & 211G & 1x & 40.2 & 62.7 & 43.8 & 37.7 & 59.7 & 40.4 \\  PVT-S & 305G & 1x & 40.4 & 62.9 & 43.8 & 37.8 & 60.1 & 40.3 \\  InLine-PVT-S & 250G & 1x & 43.4 & 66.4 & 47.1 & 40.1 & 63.1 & 43.3 \\  PVT-M & 392G & 1x & 42.0 & 64.4 & 45.6 & 39.0 & 61.6 & 42.1 \\  InLine-PVT-M & 310G & 1x & 44.0 & 66.4 & 48.0 & 40.3 & 63.4 & 43.5 \\  PVT-L & 494G & 1x & 42.9 & 65.0 & 46.6 & 39.5 & 61.9 & 42.5 \\  InLine-PVT-L & 377G & 1x & 45.4 & 67.6 & 49.7 & 41.4 & 64.7 & 44.6 \\   

Table 7: Results on COCO dataset. The FLOPs are computed with an input resolution of 1280\(\)800.

Figure 5: **Speed measurements**. Runtime and FPS is tested on a RTX3090 GPU. (a) Accuracy-Runtime curve on ImageNet. (b) Increasing window size. (c) High-resolution scenarios.

**COCO object detection.** Tab. 7 shows that InLine attention consistently improves the results in object detection tasks. For instance, InLine-PVT-S outperforms PVT-T with 6.7 box AP under similar FLOPs, and InLine-PVT-L surpasses PVT-M by 3.4 box AP with fewer FLOPs, showing the advantage of InLine attention's linear complexity in high-resolution scenarios.

**ADE20K semantic segmentation.** We employ our model on two representative segmentation models, SemanticFPN  and UperNet . As depicted in Tab. 8, benefited from injectivity and effective local modeling ability, Our InLine achieves better results under all settings with obviously lower computational cost.

**Comparison with SOTA linear attention designs**. As shown in Tab. 9, our simple InLine attention design outperforms various linear attention methods without bells and whistles. Additionally, our Inline attention can possibly integrate with previous designs to achieve better results, which we leave for future work. For instance, the advanced focused function in FLatten  can also be employed in InLine attention.

### Ablation Study

The effectiveness of our two key designs has been verified and detailed analyzed in Sec. 5.3. In Tab. 10, we offer additional results to validate the impact of different kernel functions. It is shown that our InLine attention can effectively work with different kernel functions, further validating the effectiveness of our method. The ReLU and Exponential functions achieve slightly better results. In this paper, we use \(()\) as default for simplicity.

## 6 Conclusion

In this paper, we shed some light on the core factors leading to the performance gap between linear and Softmax attention. We identify and validate two fundamental disparities between these two attention paradigms: injective property and local modeling capability. Injectivity implies that the attention function assigns distinct attention scores to queries with varying semantics, reflecting the ability to distinguish different semantics. Using different kernel functions, linear attention's non-injectivity results in various semantic confusions. Furthermore, despite being recognized for their robust long-range modeling capability, attention mechanisms heavily depend on effective local modeling for impressive results. Thorough empirical validation unequivocally supports our analyses. Our findings also demonstrate that with the above two properties, linear attention can outperform Softmax attention with lower computation complexity.