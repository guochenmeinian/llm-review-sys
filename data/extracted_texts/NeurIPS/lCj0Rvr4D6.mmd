# John Ellipsoids via Lazy Updates

David P. Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu &Taisuke Yasuda

Voleon Group

yasuda.taisuke1@gmail.com

###### Abstract

We give a faster algorithm for computing an approximate John ellipsoid around \(n\) points in \(d\) dimensions. The best known prior algorithms are based on repeatedly computing the leverage scores of the points and reweighting them by these scores . We show that this algorithm can be substantially sped up by delaying the computation of high accuracy leverage scores by using sampling, and then later computing multiple batches of high accuracy leverage scores via fast rectangular matrix multiplication. We also give low-space streaming algorithms for John ellipsoids using similar ideas.

## 1 Introduction

The _John ellipsoid_ problem is a classic algorithmic problem, which takes as input a set of \(n\) points \(\{_{1},_{2},,_{n}\}\) in \(d\) dimensions, and asks for the minimum volume enclosing ellipsoid (MVEE) of these \(n\) points. If \(P\) is the convex hull of these \(n\) points, then a famous result of Fritz John  states that such an ellipsoid satisfies \(Q P Q\), and if \(P\) is furthermore symmetric, then \(}Q P Q\). Equivalently, we may consider the \(n\) input points to be constraints of a polytope \(\{:_{i}, 1,i[n]\}\), in which case the problem is to compute a maximal volume inscribed ellipsoid (MVIE). These two problems are related by taking polars, which corresponds to inverting the quadratic form defining the ellipsoid. In this work, we focus on the symmetric case, so that the polytope \(P\) may be written as \(P=\{:\|\|_{} 1\}\), where \(\) denotes the \(n d\) matrix with the \(n\) input points \(_{i}\) in the rows, and our goal is to output an ellipsoid \(Q\) which approximately satisfies \(Q P Q\).

The John ellipsoid problem has far-reaching applications in numerous fields of computer science. In statistics, the John ellipsoid problem is equivalent to the dual of the D-optimal experiment design problem , in which one seeks weights for selecting a subset of a dataset to observe in an experiment. Other statistical applications of John ellipsoids include outlier detection  and pattern recognition . In the optimization literature, computing John ellipsoids is a fundamental ingredient for the ellipsoid method for linear programming , cutting plane methods , and convex programming . Other applications in theoretical computer science include sampling , bandits , differential privacy , coresets , and randomized numerical linear algebra , and randomized numerical linear algebra .

We refer to a survey of Todd  for an account on algorithms and applications of John ellipsoids.

### John ellipsoids via iterated leverage scores

Following a long line of work on algorithms for computing John ellipsoids  based on convex optimization techniques, the work of  developed a new approach towards computing approximate Johnellipsoids via a simple fixed point iteration approach. The approach of  starts with an observation on the optimality condition for the dual problem, given by

\[_{i=1}^{n}_{i}-(^{ })-d\] \[_{i} 0,i[n]\]

where \(=()\).1 The optimality condition requires that the dual weights \(\) satisfy

\[_{i}=_{i}(}),\] (1)

for each \(i[n]\), where \(_{i}()\) for a matrix \(\) denotes the \(i\)-_th leverage score_ of \(\).

**Definition 1.1** (Leverage score).: _Let \(^{n d}\). Then, for each \(i[n]\), we define the \(i\)-th leverage score as_

\[_{i}()_{i}^{}(^{ })^{-}_{i}=_{ 0}](i)^{2}}{\|\|_{2}^{2}}.\]

The optimality condition of (1) can be viewed as a fixed point condition, which suggests the following iterative algorithm for computing approximate John ellipsoids

\[_{i}^{(t)}_{i}(^{(t-1)} })=_{i}^{(t-1)}_{i}^{}(^{ }^{(t-1)})^{-}_{i}\] (2)

where \(^{(t)}(^{(t)})\). After repeating this update for \(T=O(^{-1}(n/d))\) iterations starting with \(^{(0)}=d/n 1_{n}\), it can be shown that the ellipsoid \(Q\) defined by the quadratic form \(=^{}^{(T)}\), i.e. \(Q=\{^{d}:^{} 1\}\), satisfies

\[}Q P Q.\] (3)

Note that the computation of leverage scores can be done in \(O(nd^{-1})\) time, where \( 2.371552\) is the current exponent of fast matrix multiplication . Thus, this gives an algorithm running in time \(O(^{-1}nd^{-1}(n/d))\) for outputting an ellipsoid with the guarantee of (3).

It is known that input matrices with additional structure admit even faster implementation of this algorithm. For instance,  give faster algorithms for sparse matrices \(\) for which the number of nonzero entries \(()\) is much less than \(nd\). The work of  shows that this approach can also be sped up for matrices \(\) with small treewidth.

### Our results

Our first main result is a substantially faster algorithm for computing John ellipsoids. In the typical regime where \(n d\), our algorithm runs in \(O(^{-1}nd)(n/d)\) time to output an ellipsoid \(Q\) satisfying (3), and \(O(^{-1}nd^{2})(n/d)\) time to output an ellipsoid \(Q\) which approximates the maximal volume up to a \((1+)\) factor. We will discuss our techniques for this result in Sections 1.2.1 and 1.2.2.

   & Running time & Guarantee \\ 
 & \(O(^{-1}nd^{})\) & volume approximation \\
 & \(O(^{-1}nd^{-1})(n/d)\) & (3) \\
 & \(O(^{-2}nd)( n)(n/d)\) & (3) \\  Theorem 1.6 & \(O(^{}nd)(n/d)\) & (3) \\  

Table 1: Running time of John ellipsoid approximation for dense \(n d\) matrices, for \(n d(^{-1} n)\). There is other prior work on sparse matrices and matrices with low treewidth .

#### 1.2.1 Linear time leverage scores via fast matrix multiplication

We start by showing how to approximate leverage scores up to \((1+)\) factors in \((nd)\) time, which had not been observed before to the best of our knowledge. Note that if we compute exact leverage scores using fast matrix multiplication, then this takes time \(O(nd^{-1})\). Alternatively, sketching-based algorithms for approximate leverage scores are known, which gives the following running time for sparse matrices \(\) with \(()\) nonzero entries.

**Theorem 1.2** ().: _There is an algorithm which, with probability at least \(1-\), outputs \(_{i}^{}\) for \(i[n]\) such that_

\[_{i}^{}=(1)_{i}( )\]

_and runs in time \(O(^{-2}\,()(n/))+(d^{-1}(n/))\)._

If the goal is to compute \((1+)\)-approximate leverage scores for a dense \(n d\) matrix, then we are not aware of a previous result which does this in a nearly linear \((nd)\) time, which we now show:

**Theorem 1.3**.: _There is an algorithm which, with probability at least \(1-\), outputs \(_{i}^{}\) for \(i[n]\) such that_

\[_{i}^{}=(1)_{i}( )\]

_in time \(O(nd)(^{-1}(n/))+O(n) {poly}(^{-1}(n/))\)._

Our improvement comes from improving the running time analysis of a sketching-based algorithm of  by using fast rectangular matrix multiplication. We will need the following result on fast matrix multiplication:

**Theorem 1.4** ().: _There is a constant \( 0.1\) and an algorithm for multiplying a \(m m\) and a \(m m^{}\) matrix in \(O(m^{2} m)\) time, under the assumption that field operations can be carried out in \(O(1)\) time._

By applying the above result in blocks, we get the following version of this result for rectangular matrix multiplication.

**Corollary 1.5**.: _There is a constant \( 0.1\) and an algorithm for multiplying a \(n d\) for \(n d\) and a \(d t\) matrix in \(O(nd+nt^{1/+1}) t\) time, under the assumption that field operations can be carried out in \(O(1)\) time._

Proof.: Let \(m=t^{1/}\). If \(d m\), then matrix multiplication takes only \(O(ndt)=O(nt^{1/+1})\) time, so assume that \(d m\). We partition the first matrix into an \(O(n/m) O(d/m)\) block matrix with blocks of size \(m m\) and the second into an \(O(d/m) 1\) block matrix with blocks of size \(m m^{}\). By Theorem 1.4, each block matrix multiplication requires \(O(m^{2} m)\) time, and we have \(O(nd/m^{2})\) of these to do, which gives the desired running time. 

That is, the above result shows that when multiplying an \(n d\) matrix \(\) with a \(d t\) matrix for a much smaller \(t\), then this multiplication can be done in roughly \(O(nd) t\) time. The work of  shows that the leverage scores of \(\) can be written as the row norms of \(\) for a \(d t\) matrix with \(t=O(^{-2}(n/))\), and thus this gives us the result of Theorem 1.3.

#### 1.2.2 John ellipsoids via lazy updates

By using Theorem 1.3, we already obtain a John ellipsoid algorithm which runs in time \(O(^{-1}nd)(n/d)(^{-1} n)\) time, which substantially improves upon prior algorithms for dense input matrices \(\). We now show how to obtain further improvements by using the idea of _lazy updates_. At the heart of our idea is to only compute the _quadratic forms_ for the John ellipsoids for most iterations, and defer the computation of the weights until we have computed roughly \(O( n)\) iterations. At the end of this group of iterations, we can then compute the John ellipsoid weights via fast matrix multiplication as used in Theorem 1.3, which allows us to remove the suboptimal \( n\) terms in the dominating term of the running time.

**Theorem 1.6**.: _Given \(^{n d}\), let \(P\) be the polytope defined by \(P=\{^{d}:\|\|_{} 1\}\). For \((0,1)\), there is an algorithm, Algorithm 3, that runs in time_

\[O(^{-1}nd)((n/d)+(^{-1} n ))+O(n)(^{-1} n)+O(n^{0.1})d^{+1} ^{-3}( n)^{2}\]

_and returns an ellipsoid \(Q\) such that \(} Q P Q\) with probability at least \(1-1/(n)\)._The full proof of this result is given in Section 2.

Let \(^{(t)}=^{}^{(t)}\), where the weights \(^{(t)}\) are defined as (2). Note that with this notation, the update rule for the iterative algorithm of  can be written as

\[_{i}^{(t)}=_{i}^{(t-1)}_{i}^{}(^{(t-1)})^{-}_{i}.\] (4)

Thus, given high-accuracy spectral estimates to the quadratics \(^{(t)}\), we can recover the weights \(_{i}^{(t)}\) to high accuracy in \(O(d^{})\) time per iteration by evaluating the quadratic forms \(_{i}^{}(^{(t)})^{-}_{i}\) and then multiplying them together. This approach is useful for fast algorithms if we only need to to do this for a small number of indices \(i[n]\). This is indeed the case if we only need these weights for a _row sample_ of \(^{(t)}}\), which is sufficient for computing a spectral approximation to the next quadratic form \(^{(t)}\). Furthermore, we only need low-accuracy leverage scores (up to a factor of, say, \(n^{0.1}\)) to obtain a good row sample, which can be done quickly for all \(n\) rows . Thus, by repeatedly sampling rows of \(^{(t)}}\), computing high-accuracy weights on the sampled rows, and then building an approximate quadratic, we can iteratively compute high-accuracy approximations \(}^{(t)}\) to the quadratics \(^{(t)}\). More formally, our algorithm takes the following steps:

* We first compute low-accuracy leverage scores of \(^{(t-1)}}\), which can be done in \(O(nd)\) time. This gives us the weights \(^{(t)}\) to low accuracy, say \(^{(t)}\), for all \(n\) rows.
* We use the low-accuracy weights \(^{(t)}\) to obtain a weighted subset of rows of \(^{(t)}}\) which spectrally approximates \(^{(t)}\). Note, however, that we do not yet have the sampled rows of \(^{(t)}}\) to high accuracy, since we do not know the weights \(^{(t)}\) to high accuracy.
* If we only need the weights \(^{(t)}\) for a small number of sampled rows \(S[n]\), then we can explicitly compute these using (4), since we inductively have access to high-accuracy quadratics \(}^{(t^{})}\) for \(t^{}=0,1,2,,t-1\). These can then be used to build \(}^{(t)}\).

While this algorithm allows us to quickly compute high-accuracy approximate quadratics \(}^{(t)}\), this algorithm cannot be run for too many iterations, as the error in the low-accuracy leverage scores \(^{(t)}\) grows to \((n)\) factors in \(O( n)\) rounds. This is a problem, as this error factor directly influences the number of leverage score samples needed to approximate \(}^{(t)}\). We will now use the fast matrix multiplication trick from the previous Section 1.2.1 to fix this problem. Indeed, after \(O( n)\) iterations, we will now have approximate quadratics \(}^{(1)},}^{(2)},,}^{ (t)}\) for \(t=O( n)\). Now we just need to compute the \(n\) John ellipsoid weights which are given by

\[_{i}^{(t)}=_{t^{}=1}^{t}\|_{i}^{}(}^{(t^{}-1)})^{-1/2}\|_{2}^{2}.\]

To approximate this quickly, we can approximate each term \(\|_{i}^{}(}^{(t^{}-1)})^{-1/2 }\|_{2}^{2}\) by \(\|_{i}^{}(}^{(t^{}-1)})^{-1/2 }^{(t^{})}\|_{2}^{2}\) for a random Gaussian matrix \(^{(t^{})}\), by the Johnson-Lindenstrauss lemma . Here, the number of columns of the Gaussian matrix can be taken to be \((^{-1} n)\), so now all we need to compute is the matrix product

\[[(}^{(0)})^{-1/2}^{(0)},(}^{(1)})^{-1/2}^{(1)},,(}^{(t)})^{ -1/2}^{(t)}]\]

which is the product of a \(n d\) matrix and a \(d m\) matrix for \(m=(^{-1} n)\). By Theorem 1.4, this can be computed in \(O(nd\, m)\) time. However, this resetting procedure is only run \(O(^{-1})\) times across the \(T=O(^{-1} n)\) iterations, so the running time contribution from the resetting is just

\[O(^{-1}nd)\,(^{-1} n)+O(n)\,(^{-1} n).\]

Overall, the total running time of our algorithm is

\[O(^{-1}nd)((n/d)+(^{-1} n))+O(n) \,(^{-1} n).\]

**Remark 1.7**.: _In general, our techniques can be be viewed as a way of exploiting the increased efficiency of matrix multiplication when performed on a larger instance by delaying large matrix multiplication operations, so that the running time is \(O(^{-1}nd(n/d))+O(^{-1})T_{r}\) where \(T_{r}\) is the time that it takes to multiply \(\) by a \(d r\) matrix for \(r=(^{-1} n)\). While we have instantiated this general theme by obtaining faster running times via fast matrix multiplication, one can expect similar improvements by other ways of exploiting the economies of scale of matrix multiplication, such as parallelization. For instance, we recover the same running time if we can multiply \(r\) vectors in parallel so that \(T_{r}=O(nd)\)._

#### 1.2.3 Streaming algorithms

The problem of computing John ellipsoids is also well-studied in the _streaming model_, where the input points \(_{i}\) arrive one at a time in a stream . The streaming model is often considered when the number of points \(n\) is so large that we cannot fit all of the points in memory at once, and the focus is primarily on designing algorithms with low space complexity. Our second result of this work is that approaches similar to the one we take to prove Theorem 1.6 in fact also give a low-space implementation of the iterative John ellipsoid algorithm of .

**Theorem 1.8** (Streaming algorithms).: _Given \(^{n d}\), let \(P\) be the polytope defined by \(P=\{^{d}:\|\|_{} 1\}\). Furthermore, suppose that \(\) is presented in a stream where the rows \(_{i}^{d}\) arrive one by one in a stream. For \((0,1)\), there is an algorithm, Algorithm 1, that makes \(T=O(^{-1}(n/d))\) passes over the stream, takes \(O(d^{2}T)\) time to update after each new row, and returns an ellipsoid \(Q\) such that \(} Q P Q\). Furthermore, the algorithm uses at most \(O(d^{2}T)\) words of space._

In Section 1.2.2, we showed that by storing only the quadratics \(}^{(t)}\) and only computing the weights \(_{t^{}=1}^{t}_{i}(}^{(t^{}-1)})^ {-}_{i}\) as needed, we could design fast algorithms for John ellipsoids. In fact, this idea is also useful in the streaming setting, since storing all of the weights \(_{i}^{(t)}\) requires \(O(n)\) space per iteration, whereas storing the quadratics \(}^{(t)}\) requires only \(O(d^{2})\) space per iteration. Furthermore, in the streaming setting, we may optimize the update running time by instead storing the pseudo-inverse of the quadratics \((}^{(t)})^{-}\) and then updating them by using the Sherman-Morrison low rank update formula.2 This gives the result of Theorem 1.8.

```
1:functionStreamingJohnEllipsoid(input matrix \(\))
2:for\(t=0\) to \(T\)do
3: Let \(^{(t)}=0\)
4:for\(i=1\) to \(n\)do
5:if\(t=0\)then
6: Let \(^{(t)}^{(t)}+_{i}_{i}^{}\)
7:else
8: Let \(^{(t)}^{(t)}+_{i}^{(t)}_{i} _{i}^{}\)
9:return\(_{t=0}^{T}^{(t)}\) ```

**Algorithm 1** Streaming John ellipsoids via lazy updates

### Notation

Throughout this paper, \(\) will denote an \(n d\) matrix whose \(n\) rows are given by vectors \(_{i}^{d}\). For positive numbers \(a,b>0\), we write \(a=(1)b\) to mean that \((1-)b a(1+)b\). For symmetric positive semidefinite matrices \(,\), we write \(=(1)\) to mean that \((1-)(1+)\), where \(\) denotes the Lowner order on PSD matrices.

## 2 Fast algorithms

### Approximating the quadratics

We will analyze the following algorithm, Algorithm 2, for approximating the quadratics \(^{(t)}\) of the iterative John ellipsoid algorithm.

Fix a row \(i\). Note then that for each iteration \(t\), \(\|_{i}^{}(}^{(t-1)})^{-1/2}\|_{2}^{2}\) is distributed as an independent \(^{2}\) variable with \(k\) degrees of freedom, scaled by \(\|_{i}^{}(}^{(t-1)})^{-1/2}\|_{2}^{2}\), say \(X_{t}\). Note then thatafter \(T\) iterations,

\[_{i}^{(T)}=_{t=1}^{T}_{i}^{}(}^{(t-1)})^{-1/2}_{2}^{2},\]

which is distributed as

\[_{i}^{(T)}_{t=1}^{T}_{i}^{}( }^{(t-1)})^{-1/2}_{2}^{2} X_{t}=_{t=1}^{T} _{i}^{}(}^{(t-1)})^{-1/2}_ {2}^{2}_{t=1}^{T}X_{t}\] (5)

for i.i.d. \(^{2}\) variables \(X_{t}\) with \(k\) degrees of freedom. We will now bound each of the terms in this product.

#### 2.1.1 Bounds on products of \(^{2}\) variables

We need the following bound on products of \(^{2}\) variables, which generalizes [14, Proposition 1].

**Lemma 2.1**.: _Let \(X_{1},X_{2},,X_{t}\) be \(t\) i.i.d. \(^{2}\) variables with \(k\) degrees of freedom. Then,_

\[_{i=1}^{t}X_{i}}_{s (0,k/2)}C_{-s,k}^{t}R^{-s}\]

_and_

\[_{i=1}^{t}X_{i} R}_{s>-k/2}C_{ s,k}^{t}R^{-s}\]

_where_

\[C_{s,k}=(s+k/2)}{(k/2)}>0\]

Proof.: For \(s>-k/2\), the moment generating function of \( X_{i}\) is given by

\[\,e^{s X_{i}}=\,X_{i}^{s}=(k/2) }_{0}^{}x^{s}x^{k/2-1}e^{-x/2}\,dx=(s+k/2)}{ (k/2)}=C_{s,k}.\]

Then by Chernoff bounds,

\[_{i=1}^{t}X_{i}}=_{i=1}^{t}-s X_{i} R^{s}} [e^{-s X_{i}}]^{t}R^{-s}=C_{-s,k}^{t}R^{-s}\]

and

\[_{i=1}^{t}X_{i} R}= _{i=1}^{t}s X_{i} R^{s}}[e^{s X_{i}}]^{t}R^{-s}=C_{s,k}^{t}R^{-s}\]Using the above result, we can show that if the \(^{2}\) variables have \(k=O(1/)\) degrees of freedom and the number of rounds \(T\) is \(c n\) for a small enough constant \(c\), then the product of the \(^{2}\) variables \(_{t=1}^{T}X_{t}\) will be within a \(n^{}\) factor for some small constant \(>0\).

**Lemma 2.2**.: _Fix a small constant \(>0\) and let \(k=O(1/)\). Let \(T=c n\) for a sufficiently small constant \(c>0\). Let \(X_{1},X_{2},,X_{T}\) be t i.i.d. \(^{2}\) variables with \(k\) degrees of freedom. Then,_

\[}_{i=1}^{t}X_{i} n^{ }} 1-(n)}.\]

Proof.: Set \(s=k/2\). Then, \(C_{-s,k}\) and \(C_{s,k}\) are absolute constants. Now let \(c\) to be small enough (depending on \(s\) and \(k\)) such that for \(T=c n\), \(C_{-s,k}^{T},C_{s,k}^{T} n\). Furthermore, set \(R=n^{}\). Then, by Lemma 2.1, we have that both \(\{_{i=1}^{t}X_{i}\}\) and \(\{_{i=1}^{t}X_{i} R\}\) are bounded by \(n R^{-s}=n n^{.s}=1/(n)\), as desired. 

It follows that with probability at least \(1-1/(n)\), the products of \(^{2}\) variables appearing in (5) are bounded by \(n^{}\) for every row \(i[n]\) and for every iteration \(t[T]\). We will condition on this event in the following discussion.

#### 2.1.2 Bounds on the quadratic \(}^{(t)}\)

In the previous section, we have established that the products of \(^{2}\) variables in (5) are bounded by \(n^{}\). We will now use this fact to show that the quadratics \(}^{(t-1)}\) in Algorithm 2 are good approximate John ellipsoid quadratics. We will use the following leverage score sampling theorem for this.

**Theorem 2.3** ([16, SS11]).: _Let \(^{n d}\). Let \(_{i}^{}_{i}()\) and let \(p_{i}=\{1,_{i}^{}/\}\) for \(=(^{2})/(d/)\). If \(\) is a diagonal matrix with \(_{i,i}=1/_{i}\) with probability \(p_{i}\) and \(0\) otherwise for \(i[n]\), then with probability at least \(1-\),_

\[^{d}\|\|_{2}^{2}=(1 )\|\|_{2}^{2}.\]

This theorem, combined with the bounds on \(^{2}\) products, gives the following guarantee for the approximate quadratics \(}^{(t)}\).

**Lemma 2.4**.: _Fix a small constant \(>0\) and let \(k=O(1/)\). Suppose that the leverage score sample in Line 6 oversamples by a factor of \(O(n^{2})\), that is, uses leverage score estimates \(_{i}^{}\) such that \(_{i}^{} O(n^{2})_{i}(^{(t)}})\). Then, with probability at least \(1-1/(n)\), we have for every \(t[T]\) that_

\[}^{(t)}=(1)^{}^{(t)} \] (6)

_where \(_{i}^{(t)}=_{t^{}=1}^{t-1}_{i}^{}(}^{(t^{})})^{-}_{i}\) for \(i[n]\). Furthermore, \(}^{(t)}\) can be computed in \(O(n^{2})T^{-2}d^{+1} n\) time in each iteration._

Proof.: We will first condition on the success of the event of Lemma 2.2, so that the \(^{2}\) products in (5) are bounded by \(n^{}\) factors for every row \(i[n]\) and every iteration \(t[t]\). We will also condition on the success of the leverage score sampling for all \(T\) iterations.

Note that by (5) and the bound the \(^{2}\) products, \(_{i}^{(t)}\) is within a \(O(n^{2})\) factor of \(_{i}^{(t)}\), and thus \(^{(t)}\) is a correct leverage score sample for \(^{(t)}}\). We thus have that

\[}^{(t)}=(^{(t)}^{(t)}})^{ }^{(t)}^{(t)}}=(1) ^{}^{(t)}.\]

For the running time, note that \(^{(t)}\) samples at most \(O(^{-2}n^{2}d n)\) rows in each iteration, and each sampled row \(i\) requires \(O(d^{}T)\) to compute \(_{i}^{(t)}\). This gives the running time claim.

### Full algorithm

We now combine the subroutine for approximating quadratics from Section 2.1 with a resetting procedure using fast matrix multiplication to obtain our full algorithm for quickly computing John ellipsoids. The full algorithm is presented in Algorithm 3.

```
1:functionJohnEllipsoid(input matrix \(\))
2: Let \(B=O(c^{-1}^{-1})\) and \(T=O(c(n/d))\) for a sufficiently small constant \(c\).
3: Let \(}_{i}^{(0)}=d/n\) for \(i[n]\).
4:for\(b=1\) to \(B\)do
5: Let \(\{}^{(t)}\}_{t=0}^{T}\) be given by ApproxQuadratic\((,}^{(0)})\) (Algorithm 2).
6: Let \(^{(t-1)}\) for \(t[T]\) be a random \(d m\) Gaussian for \(m=O(^{-2}(BT)^{2} n)\).
7: Compute \([(}^{(0)})^{-1/2}^{(0)},(}^{(1)})^{-1/2}^{(1)},,(}^{(T)})^{- 1/2}^{(T)}]\).
8: Let \(}^{(b,t)}=_{t^{}=1}^{t}\|_{i }^{}(}^{(t^{}-1)})^{-1/2}^{(t ^{}-1)}\|_{2}^{2}\) for each \(i[n]\).
9: Let \(}^{(0)}=}^{(b,T)}\).
10: Let \(}=_{b,t}}^{(b,t)}\).
11:return\(^{}}\) ```

**Algorithm 3** John ellipsoids via lazy updates

We will need the following theorem, which summarizes results of  on guarantees of the fixed point iteration algorithm (2) under approximate leverage score computations.

**Theorem 2.5** (Lemma 2.3 and Lemma C.4 of ).: _Let \(^{n d}\) and let \(P=\{:\|\|_{} 1\}\). Let \(T=O(^{-1}(n/d))\). Suppose that_

\[_{i}^{(t)}=(1)_{i}( ^{(t-1)}})\]

_for all \(t[T]\) and \(i[n]\). Then, if \(Q\) is the ellipsoid given by the quadratic form \(^{}^{(T)}\), then \(} Q P Q\)._

We also use the Johnson-Lindenstrauss lemma, which is a standard tool for randomized numerical linear algebra, especially in the context of approximating leverage scores .

**Theorem 2.6** ().: _Let \(m=O(^{-2}(n/))\) and let \(\) be a random \(m d\) Gaussian matrix. Then, for any \(n\) points \(_{1},_{2},,_{n}^{d}\) in \(d\) dimensions, we have with probability at least \(1-\) that_

\[\|_{i}\|_{2}^{2}=(1)\|_{i}\|_{2}^{2}\]

_simultaneously for every \(i[n]\)._

We will now give a proof of Theorem 1.6.

Proof of Theorem 1.6.: We first argue the correctness of this algorithm. By Theorem 2.5, it suffices to verify that \(}_{i}^{(b,t)}\) computes \((1+)\)-approximate leverage scores in order to prove the claimed guarantees of Theorem 1.6. For the updates within ApproxQuadratic (Algorithm 2), we have by Lemma 2.4 that

\[}^{(t)}=(1)^{}^{(t)} .\]

Thus,

\[_{i}^{(t-1)}\|_{i}^{}( }^{(t-1)})^{-1/2}\|_{2}^{2} =(1)_{i}^{(t-1)}\|_{i}^{ }(^{}^{(t-1)})^{-1/2}\|_{2}^ {2}\] \[=(1)_{i}(^{(t-1)}} )\]

and thus the weights \(_{i}^{(t)}\) output by ApproxQuadratic (Algorithm 2) indeed satisfy the requirements of Theorem 2.5.

For the weights computed in Line 8 of Algorithm 3, note that we also compute these weights, but this time with approximation error from the application of the Gaussian matrix \(^{(t)}\) to speed up thecomputation. Applying Theorem 2.6 with \(\) set to \(/BT\) and failure probability \(=1/(n)\), we have that

\[\|_{i}^{}(}^{(t^{}-1 )})^{-1/2}^{(t^{}-1)}\|_{2}^{2}=(1/BT)\|_{i}^{}(}^{(t^{}-1)})^{-1/2}\|_{2}^{2}.\]

Note then that Line 8 takes a product of at most \(BT\) of these approximations, so the total error in the approximation is at most

\[(1/BT)^{BT}=(1 O()).\]

The running time is given by \(BT\) iterations of the inner loop of ApproxQuadratic and \(B\) iterations of the fast matrix multiplication procedure in Line 7 of Algorithm 3. The inner loop of ApproxQuadratic requires \(O(nd)\) time to compute the product with the \(d k\) Gaussian as well as the time to compute the approximate quadratic, which is bounded in Lemma 2.4. Altogether, this gives the claimed running time bound. 

## 3 Future directions

In this work, we developed fast algorithms and low-space streaming algorithms for the problem of computing John ellipsoids. Our fast algorithms use a combination of using lazy updates together with fast matrix multiplication to substantially improve the running time of John ellipsoids, and we apply similar ideas to obtain a low-space streaming implementation of the John ellipsoid algorithm.

Our results have several limitations that we discuss here, which we leave for future work to resolve. First, our algorithm makes crucial use of fast matrix multiplication in order to get running time improvements. However, this makes it a less attractive option for practical implementations, and also makes the polynomial dependence on \(\) in the \((n)(^{-1})\) term rather large. Thus, it is an interesting question whether the running time that we obtain in Theorem 1.6 is possible without fast matrix multiplication.

**Question 3.1**.: _Is there an algorithm with the guarantee of Theorem 1.6 that avoids fast matrix multiplication?_

More generally, it is an interesting question to design algorithms for approximating John ellipsoids with optimal running time. This is an old question which has been studied in a long line of work , and we believe that the investigation of this question will lead to further interesting developments in algorithms research.

**Question 3.2**.: _What is the optimal running time of approximating John ellipsoids?_

For instance, one interesting question is whether it is possible to obtain _nearly linear time_ algorithms that run in time \((nd)+(n)(^{-1})\), or even _input sparsity time algorithms_ that run in time \((())+(n)( ^{-1})\). The resolution of such questions for the least squares linear regression problem has led to great progress in algorithms, and studying these questions for the John ellipsoid problem may have interesting consequences as well.

John ellipsoids are closely related to \(_{p}\)_Lewis weights_, which give a natural \(_{p}\) generalization of John ellipsoids and leverage scores and have been a valuable tool in randomized numerical linear algebra. There has been a recent focus on fast algorithms for computing \((1+)\)-approximate \(_{p}\) Lewis weights , and thus it is natural to ask whether developments in algorithms for John ellipsoids would carry over to algorithms for \(_{p}\) Lewis weights as well.

**Question 3.3**.: _What is the optimal running time of approximating \(_{p}\) Lewis weights?_

Finally, we raise questions concerning streaming algorithms for approximating John ellipsoids. In Theorem 1.8, we gave a multi-pass streaming algorithm which obtains a \((1+)\)-optimal John ellipsoid. A natural question is whether a similar algorithm can be achieved in fewer passes, or if there are pass lower bounds for computing \((1+)\)-optimal John ellipsoids.

**Question 3.4**.: _Are there small space streaming algorithms for approximating John ellipsoids up to a \((1+)\) factor which make fewer than \(O(^{-1}(n/d))\) passes, or do small space streaming algorithms necessarily require many passes?_

We note that there are one-pass small space streaming algorithms if we allow for approximation factors that scale as \(O()\) rather than \((1+)\).