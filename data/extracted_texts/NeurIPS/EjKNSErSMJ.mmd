# Last-Iterate Convergence for Generalized Frank-Wolfe in Monotone Variational Inequalities

Zaiwei Chen

Purdue IE

West Lafayette, IN 47907

chen5252@purdue.edu

&Eric Mazumdar

Caltech CMS

Pasadena, CA 91125

mazumdar@caltech.edu

###### Abstract

We study the convergence behavior of a generalized Frank-Wolfe algorithm in constrained (stochastic) monotone variational inequality (MVI) problems. In recent years, there have been numerous efforts to design algorithms for solving constrained MVI problems due to their connections with optimization, machine learning, and equilibrium computation in games. Most work in this domain has focused on extensions of simultaneous gradient play, with particular emphasis on understanding the convergence properties of extragradient and optimistic gradient methods. In contrast, we examine the performance of an algorithm from another well-known class of optimization algorithms: Frank-Wolfe. We show that a generalized variant of this algorithm achieves a fast \((T^{-1/2})\) last-iterate convergence rate in constrained MVI problems. By drawing connections between our generalized Frank-Wolfe algorithm and the well-known smoothed fictitious play (FP) from game theory, we also derive a finite-sample convergence rate for smoothed FP in zero-sum matrix games. Furthermore, we demonstrate that a stochastic variant of the generalized Frank-Wolfe algorithm for MVI problems also converges in a last-iterate sense, albeit at a slower \((T^{-1/6})\) convergence rate.

## 1 Introduction

A constrained monotone variational inequality (MVI) problem consists of solving for an \(x^{*}^{d}\) such that

\[_{s}{(x^{*}-s)}^{}F(x^{*}) 0,\]

where \(F:^{d}\) is a monotone operator [56; 37] and \(\) is a convex set. MVIs arise in many foundational and emerging problems. In particular, many problems in optimization [10; 1; 39], equilibrium computation [43; 42], reinforcement learning [57; 38], and learning in games  can be formulated as MVI problems.

Due to their wide applicability, recent years have seen significant advances in developing efficient algorithms to solve these problems. Despite the structure provided by the monotone mapping, MVI problems are well-known to be challenging to solve, as simple first-order algorithms may diverge or exhibit complex limiting behaviors such as chaos [3; 27; 18]. This has motivated the analysis of algorithms such as the extragradient method , the optimistic gradient method , and the Halpern iteration method . These algorithms, particularly the extragradient and optimistic gradient methods--which can be viewed as approximating proximal point algorithms --have been the focus of numerous recent works, with matching upper and lower bounds established under various assumptions about the feasible set \(\) and the operator \(F()\)[29; 30; 15; 26; 41]. Due to their connection with gradient descent and various extensions in convex optimization, these algorithmshave garnered the most attention in the literature, with recent breakthroughs in the constrained regime where \(\) is a compact convex set . See Section 1.1 for more details about related work.

In this paper, we take an orthogonal approach by analyzing the performance of another optimization algorithm for solving constrained MVI problems: Frank-Wolfe (FW) . Although the FW algorithm was first proposed for solving MVI problems decades ago  and has been analyzed in the context of min-max optimization problems , its convergence rate for general MVI problems remains less understood. To address this, we provide an analysis of a smoothed version of FW for solving constrained MVI problems, even considering the case where one only has access to noisy estimates of the operator \(F()\). This case is particularly relevant to problems in machine learning , distributionally robust optimization [5; 61], and learning in games [25; 43].

### Related Literature

There is a rich literature analyzing MVI problems and their solutions [8; 54]. In this section, we give an overview of the most related works.

**Gradient-Based Methods.** Work on solving MVI problems has largely focused on understanding the behavior of gradient-based algorithms due to their connection with gradient descent in optimization, though other algorithms have also been proposed in the literature (see, e.g., ). Although straightforward generalizations of gradient descent can fail in MVI problems [42; 43], proximal point algorithms  and related methods such as the extragradient  and optimistic gradient  algorithms have been shown to provide much stronger convergence guarantees. Specifically,  showed that the extragradient algorithm achieves a tight \((T^{-1/2})\) last-iterate convergence rate for the smooth convex-concave saddle-point problem (a special case of MVI). In , the authors also studied saddle-point problems and proposed an algorithm called mirror-prox conditional gradient sliding, which comes with strong complexity guarantees. However, their analysis required a strong concavity assumption on the objective function, which corresponds to a strong monotonicity assumption in the variational inequality formulation. In , the authors studied unconstrained variational inequality problems and showed that the extragradient algorithm achieves an \((T^{-1})\) last-iterate convergence rate under the assumptions of monotonicity and Lipschitz continuity of \(F()\). Finally, in , the authors established the tight \((T^{-1/2})\) last-iterate convergence of both the extragradient and optimistic gradient descent-ascent algorithms for constrained MVIs. However, the approach in  relies on computer-aided proofs, whereas our proof uses a natural Lyapunov argument.

**The Halpern Iteration Method.** The Halpern iteration method was originally proposed to find the fixed points of non-expansive mappings . More recently, algorithms based on the Halpern iteration have been applied to solving MVI problems with a fast \((T^{-1})\) convergence rate [19; 58]. However, to the best of our knowledge, there are no results showing that the Halpern iteration method, or the algorithms proposed in [19; 58], have provable convergence in the stochastic setting.

**Frank-Wolfe Methods.** The closest work to ours in this area is , which analyzes FW in deterministic convex-concave saddle-point problems. They prove last-iterate convergence rates by making curvature assumptions on the operator \(F()\) and on the underlying space \(\) (i.e., assuming \(\) is strongly convex or \(F()\) is strongly monotone). Additionally, they show a slow convergence rate for FW on polytopic sets without these curvature assumptions. In contrast, we analyze a smoothed version of FW, also known as the generalized conditional gradient algorithm [2; 12], in MVIs. We demonstrate that this generalized version of FW achieves fast convergence without imposing strong curvature assumptions on either the operator or the underlying space. Crucially, the smoothing technique allows us to bypass issues that arise with vanilla FW in saddle-point problems. Another application of FW in MVIs is presented in , where FW is used to compute the iterates in mirror-prox for MVIs.

**Stochastic Monotone Variational Inequality Problems.** There has been considerable recent work on solving stochastic MVI problems, where one can only obtain noisy estimates of the operator \(F()\). Such problems arise in multi-agent reinforcement learning  and distributionally robust supervised learning , among other domains. However, the literature is sparser for this class of problems, particularly regarding last-iterate convergence in constrained problems. Under curvature assumptions (on \(F()\) and/or \(\)), stronger guarantees (both in expectation and with high probability) exist for variants of extragradient and optimistic gradient algorithms [8; 28; 44]. Inspired by recent results using FW for stochastic optimization [45; 20], we extend our smoothed FW algorithm to stochastic MVI problems and, to the best of our knowledge, provide the first last-iterate convergence guarantee for an algorithm in constrained stochastic MVI problems without curvature assumptions on the monotone operator. While the \((T^{-1/6})\) rate of convergence we derive for this algorithm is slower than the known \((T^{-1/2})\) convergence rate for the averaged iterates of the mirror-prox algorithm , it is important to note that the mirror-prox algorithm has been shown to diverge in stochastic monotone problems . Furthermore, in MVI problems, it has been demonstrated that the averaged iterates can exhibit fundamentally faster convergence rates than the last iterate .

### Our Contributions

We introduce and analyze a generalized FW algorithm for solving MVI problems. This algorithm is a natural extension of the classic smoothed fictitious play (FP) algorithm for learning in games , applied to monotone games and, by extension, to MVI problems. We show that the algorithm achieves a fast last-iterate convergence rate of \((T^{-1/2})\), matching the rates of optimistic gradient and extragradient algorithms. As a consequence of our analysis, we derive a finite-time bound for smooth FP in finite zero-sum games.

We also consider the case of stochastic MVI problems, where only noisy estimates of \(F()\) are available. We demonstrate that, by designing estimators similar to those used in stochastic FW for optimization, it is possible to achieve last-iterate convergence in constrained MVI problems using generalized FW, though at a slower rate of \((T^{-1/6})\). Although this rate is not optimal, it appears to be the first last-iterate convergence rate for solving constrained stochastic MVI problems without assuming strong curvature properties of the operator \(F()\) or the set \(\). Indeed, previous algorithms have provided last-iterate guarantees either in the unconstrained setting  or under curvature assumptions on \(F()\), such as strong (quasi)-monotonicity or coercivity .

## 2 Problem Formulation

Let \(F:^{d}\) be a (possibly nonlinear) operator, where \(\) is a convex and compact subset of \(^{d}\). The associated variational inequality problem consists of solving for an \(x^{*}\) such that

\[_{s}{(x^{*}-s)}^{}F(x^{*}) 0.\] (1)

Although such problems canonically arise in optimization  and machine learning , where the operator \(F()\) is usually the gradient of some objective function, the formulation is general enough to capture other problems such as reinforcement learning  and learning in games . Next, we provide two illustrative examples.

Example 1: The Policy Evaluation Problem in Reinforcement Learning.Consider an infinite horizon discounted Markov decision process (MDP) with a finite state space \(\), a finite action space \(\), a set of action-dependent transition probability matrices \(\{P_{a}^{||||} a\}\), a reward function \(:\), and a discount factor \((0,1)\). The transition probabilities and the reward function are unknown to the agent. Given a policy \(:()\), where \(()\) denotes the probability simplex on \(\), its value function \(V^{}^{||}\) is defined as

\[V^{}(s)=_{}[_{t=0}^{}^{t}(S_{t },A_{t}) S_{0}=s]\]

for all \(s\), where \(_{}[\,\,]\) means that the actions are chosen according to the policy \(\). The policy evaluation problem in reinforcement learning refers to the problem of estimating \(V^{}\) for a given policy \(\). To solve this problem, it has been shown that \(V^{}\) is the unique solution of a fixed-point equation known as the Bellman equation \(V=^{}(V)\), where \(^{}:^{||}^{||}\) is the Bellman operator . Therefore, solving the policy evaluation problem is equivalent to solving the unconstrained variational inequality problem \(V-^{}(V)=0\).

Example 2: Multi-Player Convex-Concave Games.Consider an \(n\)-player game where each player \(i\{1,2,,n\}\) has a compact convex action set \(_{i}^{d_{i}}\) and a loss function \(f_{i}:_{j=1}^{n}_{j}\) such that \(f_{i}(x_{i},x_{-i})\) is convex in \(x_{i}\) for all \(x_{-i}_{j i}_{j}\). Such games have been well analyzedin the literature in economics  and more recently in machine learning . Solving for a Nash equilibrium  in this game can be formulated as solving for a point \(x^{*}:=_{j=1}^{n}_{j}\) such that \(_{s}(x^{*}-s)^{}F(x^{*}) 0\), where

\[F(x)=[_{x_{1}}f_{1}(x_{1},x_{-1}),,_{x_{n}}f_{n}(x_{n},x_{-n} )],\,x.\]

Indeed, by convexity, we have for any \(x^{d}\) that

\[_{s}\,(x-s)^{}F(x)_{i=1}^{n}\{f_{i}(x_{i},x_{-i })-_{s_{i}_{i}}f_{i}(s_{i},x_{-i})\} 0,\]

where the equality is achieved if and only if a joint strategy \(x^{*}=(x_{1}^{*},,x_{n}^{*})\) satisfies \(f_{i}(x_{i}^{*},x_{-i}^{*})=_{s_{i}_{i}}f_{i}(s_{i},x_{-i}^ {*})\) for all \(i\), which implies that \(x^{*}\) is a Nash equilibrium of the game.

An important class of variational inequality problems is MVI problems where the operator \(F()\) is monotone over \(\). Note that an operator \(F:^{d}\) is said to be monotone if and only if

\[(F(x_{1})-F(x_{2}))^{}(x_{1}-x_{2}) 0,\,x_{1},x_{2} .\] (2)

Despite this additional structure, designing algorithms for solving constrained MVI problems efficiently and with strong convergence guarantees has been an open problem until recently , with most work focused on analyzing approximations of proximal point algorithms such as extragradient and optimistic gradient approaches [29; 30; 15].

Further generalizations of the MVI problem that are of particular interest for applications in machine learning are stochastic MVI problems, where one only has access to a noisy estimator of \(F(x)\). Such situations arise in, e.g., reinforcement learning (where the agent learns by interacting with the environment)  and problems of distributionally robust optimization where one seeks to solve a zero-sum game using mini-batches to estimate gradients [61; 48; 16].

The rest of this paper is organized as follows. To motivate the generalized FW algorithm for MVI problems, we first present the smoothed FP, a canonical algorithm for learning in games, which we view as the instantiation of generalized FW in zero-sum matrix games. We then introduce the generalized FW algorithm for solving MVI problems and present its last-iterate convergence rate. Moving to the stochastic setting, we propose a stochastic variant of the generalized FW algorithm also with last-iterate convergence guarantees. Notably, the algorithm employs a two-timescale structure, where we construct a variance-reduced estimator of \(F(x)\) on the fast timescale and implement the generalized FW algorithm on the slow timescale.

## 3 Warm-Up: Smoothed Fictitious Play

In this section, we present the problem of finding a Nash equilibrium of a zero-sum game and reformulate it as an MVI problem. In addition, we present the smoothed FP algorithm for zero-sum games, which also motivates our algorithm for the MVI problem (1) in the next section.

Consider a two-player finite zero-sum game where the set of pure strategies1 for player \(i\) (where \(i\{1,2\}\)) is denoted by \(^{i}\). When players play over mixed strategies, we can write this game as the min-max optimization problem \(_{^{2}(^{2})}_{^{1}(^{1} )}(^{1})^{}R^{2}\), where \(R^{|^{1}||^{2}|}\) is the payoff matrix. A canonical measure of the performance of algorithms for learning in such games is the Nash gap, which measures how far each player is from their best response:

\[(^{1},^{2})=_{^{1}(^{1})}( ^{1})^{}R^{2}-_{^{2}(^{2} )}(^{1})^{}R^{2}.\]

Suppose that a pair of strategies \((_{*}^{1},_{*}^{2})\) satisfies \((_{*}^{1},_{*}^{2})=0\). Then, each player is playing the best response to their opponent's strategy, thereby having no incentive to deviate from their current strategy. This situation defines a Nash equilibrium .

Solving for a Nash equilibrium in such games has been a focus of interest in economics and the literature on learning in games, dating back to . One of the most canonical algorithms for learning in games from that literature is FP, where players play the best responses to the empirical history of their opponents' actions. Subsequently, a generalization of that algorithm, smoothed FP, was introduced as it was found to be a better model of human play, accounting for "trembling-hand" strategies in games . In smoothed FP, players again keep track the empirical history of their opponents' play but instead sample an action from a smoothed best-response strategy rather than playing the exact best response.

More concretely, for any \(x^{d}\) such that \(x_{i} 0\) for all \(i\), let \((x)=-_{i=1}^{d}x_{i}(x_{i})\) be the entropy function . Given \(i\{1,2\}\) and \(a^{i}^{i}\), we use \(e(a^{i})\) to denote the \(|^{i}|\)-dimensional vector with its \(a^{i}\)-th entry being one and zero everywhere else. Then, players make use of the algorithm presented in Algorithm 1 for repeatedly playing the finite zero-sum games.

```
1:Input: Integer \(T\), temperature \(>0\), and initialization \(_{0}^{2}(^{2})\)
2:for\(t=0,1,,T-1\)do
3:\(v_{t}^{1}=_{v(^{1})}\{v^{}R_{t}^{2}+ (v)\}\)
4: Play \(A_{t}^{1} v_{t}^{1}()\) and observe \(A_{t}^{2}\)
5:\(_{t+1}^{2}=_{t}^{2}+(e(A_{t}^{2})-_{t}^{2})\)
6:endfor ```

**Algorithm 1** Smoothed Fictitious Play (from Player \(1\)'s perspective)

In smoothed FP, with an arbitrary initial estimate of the opponent's policy \(_{0}^{2}\), in each round, player \(1\) plays the smoothed best response to its latest estimate of the opponent's policy (cf. Algorithm 1 Line \(4\)), and updates the estimate \(_{t}^{2}\) according to Algorithm 1 Line \(5\), which is an iterative way of computing the empirical average of the opponent's historical strategies.

Despite its canonical nature and its connection to classic algorithms in online learning, such as Follow-The-Regularized-Leader (FTRL) , the algorithm lacks a finite-time convergence rate guarantee, although it has been well analyzed in its continuous-time limit . To connect with FTRL, suppose that player \(1\) observes \(v_{t}^{2}\) does not know \(R\), but has payoff-based feedback of the form \(r_{t}\) such that \([r_{t} A_{t}^{1},A_{t}^{2}]=(A_{t}^{1},A_{t}^{2})\). In this case, smoothed FP reduces to FTRL or forms of FTRL with bandit feedback due to the linear structure of the losses. However, smoothed FP assumes an unusual feedback structure (for online learning algorithms) in which each player is assumed to know the payoff matrix \(R\), but can only observe the realized actions of their opponent, not the entire strategy \(v_{t}^{2}\). Therefore, previous approaches for analyzing FTRL do not apply, and, to the best of our knowledge, finite-time analysis of smoothed FP is still lacking in the literature, although it has been shown to be asymptotically no-regret .

The following convergence rate of Algorithm 1 follows as a consequence of our more general results of generalized FW. Since we are dealing with a finite game, we assume, without loss of generality, that \(_{a^{i}^{1},a^{2}^{2}}|R(a^{1},a^{2})| 1\).

**Theorem 3.1**.: _Suppose that both players use smoothed FP in finite zero-sum games and \((0,1]\). Then, we have for any \(t 0\) that_

\[[(_{t}^{1},_{t}^{2})]^{ 1}|+|^{2}|}}{t+1}+^{1}||^{2}|(t+ 1)}{(t+1)}+(|^{1}||^{2}|).\]

The proof of Theorem 3.1 is presented in Appendix A. In view of Theorem 3.1, given a time horizon \(T\), by choosing \(=(T^{-1/2})\), we have an \(}(T^{-1/2})\) rate of convergence of the empirical history of the play. Equivalently, we have the following iteration complexity for the algorithm.

**Corollary 3.1.1**.: _To achieve \([(_{t}^{1},_{t}^{2})]\), the iteration complexity is \(}(|^{1}||^{2}|/^{2})\)._

To identify Algorithm 1 for zero-sum games as a special case of generalized FW for MVI problems, observe that the Nash gap \((,)\) can be rewritten as

\[(x_{1},x_{2})=_{s}(x-s)^{}F(x),\] (3)

where \(x=(x_{1},x_{2}):=(^{1})( ^{2})\) and \(F(x)=Mx\) with the matrix \(M\) defined as \(M=[0^{|^{1}||^{2}|},-R;R^{},0^{|^{2 }||^{1}|}]\). Since \(M+M^{}=0\), it is clear that \(F()\) is a monotone operator. In addition, when both players follow smooth FP as presented in Algorithm 1, the joint update equation can be equivalently written as

\[s_{t}= _{s}\{s^{}F(x_{t})+ f(s)\},\] (4) \[x_{t+1}= \,x_{t}-_{t}(x_{t}-s_{t}+w_{t}),\] (5)

where \(f(s)=-(s_{1})-(s_{2})\) for any \(s=(s_{1},s_{2})\), and \(w_{t}\) is a zero-mean random variable. In smoothed FP, the random variable \(w_{t}\) corresponds to the difference between the softmax distribution (cf. Algorithm 1 Line \(3\)) and a sample from the softmax distribution (cf. Algorithm 1 Line \(4\)). In view of Eqs. (3), (4), and (5), we see that smoothed FP for zero-sum matrix games is simply a generalization of FW for MVI problems. Although this algorithm has been well analyzed in the optimization literature (i.e., when \(F()\) is the gradient of some objective function) , it has yet to be analyzed, to the best of our knowledge, in the context of more general MVI problems. In the next section, we show that this algorithm has strong convergence properties.

## 4 Generalized Frank-Wolfe for Monotone Variational Inequalities

Motivated by the smoothed FP for zero-sum games, we next present our algorithm and convergence guarantees for solving general MVI problems.

```
1:Input: Integer \(T\), tunable parameter \(>0\), and initialization \(x_{0}^{d}\)
2:for\(t=0,1,,T-1\)do
3:\(s_{t}=_{s}\{s^{}F(x_{t})+ f(s)\}\)
4:\(x_{t+1}=x_{t}-_{t}(x_{t}-s_{t}+w_{t})\)
5:endfor ```

**Algorithm 2** Generalized Frank-Wolfe for Monotone Variational Inequalities

In Algorithm 2 Line \(3\), the function \(f:[0,)\) serves as a regularizer (analogous to the entropy function in smoothed FP), for which we impose the following requirement.

**Condition 4.1**.: The function \(f()\) is continuously differentiable and \(_{f}\)-strongly convex for some \(_{f}>0\). In addition, \(_{x}\| f(x)\|_{2}=+\), where \(=_{^{d}} \) denotes the boundary of the convex compact subset \(\) of \(^{d}\).

Differentiability and strong convexity are standard requirements when choosing regularizers. The condition that \(_{x}\| f(x)\|_{2}=+\) ensures that the generalized FW direction \(s_{t}\) from Algorithm 2 Line \(3\) always lies in the relative interior of \(\). These conditions are satisfied by, e.g., the sum of negative entropies when the compact convex set \(\) is the product of probability simplicies. Note that when \(=0\), the algorithm recovers the vanilla version of FW analyzed in  for saddle point problems. Although the use of regularization precludes the use linear minimization oracles (LMOs), which is one of the main features that make FW algorithms so appealing , we remark that it does not add additional complexity to the algorithm when compared to projected extragradient and optimistic gradient methods. Specifically, note that the subproblem that appears in Algorithm 2 Line \(3\) is a strongly convex optimization problem and can be solved efficiently or even admits closed-form solutions. For example, when \(\) the probability simplex and \(f()\) is the negative entropy, the FW direction \(s_{t}\) is the softmax operator.

To derive our convergence guarantees, we impose the following assumptions on the operator \(F()\) and the stochastic process \(\{w_{t}\}\).

**Assumption 4.1**.: The operator \(F()\) is Lipschitz continuous, i.e., there exists \(L_{F}>0\) such that

\[\|F(x_{1})-F(x_{2})\|_{2} L_{F}\|x_{1}-x_{2}\|_{2},\,x_{1},x_{ 2}.\]

**Assumption 4.2**.: The operator \(F()\) has a Lipschitz continuous Jacobian matrix \(J()\), i.e., there exists \(L_{J}>0\) such that

\[\|J(x_{1})-J(x_{2})\|_{2} L_{J}\|x_{1}-x_{2}\|_{2},\,x_{1},x_{ 2}.\]

In zero-sum games, due to the linear structure of \(F()\), the Jacobian matrix is the zero matrix. Therefore, Both Assumptions 4.1 and 4.2 are automatically satisfied. In optimization, \(F()\) is the gradient of the objective function that we aim to optimize, and Assumptions 4.1 and 4.2 are equivalent to assuming the smoothness of the objective function  and the Lipschitz continuity of the Hessian matrix.

**Assumption 4.3**.: It holds for all \(t 0\) that (1) \(s_{t}-w_{t}\), (2) \([w_{t}_{t}]=0\), (3) \([\|w_{t}\|_{2}^{2}_{t}]_{w}\), where \(_{w}>0\) and \(_{t}\) is the \(\)-algebra generated by \(\{x_{0},w_{0},w_{1},,w_{t-1}\}\).

When \(_{w}=0\), Algorithm 2 is a deterministic algorithm. More generally, we allow for this additive martingale difference noise to capture the potential stochasticity in choosing the FW direction, which is present in, e.g., smoothed FP, due to sampling an action according to the smoothed best response.

To state our main result for generalized FW in MVI problems, the following notation is needed. Let \(D_{}=_{x}\|x\|_{2}\), \(=_{x}\|F(x)\|_{2}\), and \(=_{x}f(x)\), all of which are well defined and finite due to the Weierstrass extreme value theorem because \(F()\) and \(f()\) are continuous functions and \(\) is a compact set. Next, we present the convergence guarantee on the iterates of Algorithm 2 when using stepsizes of various decay rates.

**Theorem 4.1**.: _Consider \(\{x_{t}\}\) updated according to Algorithm 2. Suppose that \(F()\) is a monotone operator on \(\), and Assumptions 4.1, 4.2, and 4.3 are satisfied. Then, when the regularizer \(f()\) satisfies Condition 4.1, we have for all \(t 0\) that_

\[[_{s}(x_{t}-s)^{}F(x_{t}) ]2D_{}(1-)^{t}+c_{1}+ ,&_{t} 1,\\ }}{t+1}+(t+1)}{t+1}+,& _{t}=,\]

_where \(c_{1}=(L_{F}+L_{F}^{2}/(2_{f})+D_{}L_{J})(_{w}+4D_ {}^{2})\)._

The proof of Theorem 4.1 is presented in Appendix B.2. Note that for a given time horizon \(T\), choosing \(=(T^{-1/2})\) results in an overall \(}(T^{-1/2})\) last-iterate convergence rate to a solution to the MVI problem. The problem-dependent constants \(D_{}\), \(\), and \(\) appear additively or multiplicatively in the bound but do not impact the overall \(}(T^{-1/2})\) rate of convergence.

**Corollary 4.1.1**.: _To achieve \([_{s}(x_{t}-s)^{}F(x_{t})]\), the iteration complexity is \(}(^{-2})\)._

This convergence rate matches the last-iterate convergence rate recently proved for extragradient and optimistic gradient algorithms for constrained MVI problems . In contrast to the analyses of those algorithms which requires computer-aided proofs such as the sum of squares programming  or performance estimation problems , our proof follows from a simple Lyapunov argument on the regularized gap \(V()\), which is defined as

\[V(x)=_{s}\{(x-s)^{}F(x)- f(s)\}.\] (6)

A key step in proving Theorem 4.1 is the following lemma, which shows the smooth evolution of the generalized FW directions \(\{s_{t}\}\). For notation convenience, let \(s(x)=_{s}\{s^{}F(x)+ f(s)\}\) for all \(x\).

**Lemma 4.1**.: _It holds for all \(x_{1},x_{2}\) that \(\|s(x_{1})-s(x_{2})\|_{2}}{_{f}}\|x_{1}-x_{2}\|_{2}\)._

The proof of Lemma 4.1 is presented in Appendix B.1. As a last comment, note that while the \(}(T^{-1/2})\) convergence rate is known to be tight for extragradient and optimistic gradient algorithms  (since they both can be seen as instantiations of \(p\)-stationary canonical linear iterative algorithms), it is unclear whether this rate is tight for FW-type algorithms. We leave further explorations of fundamental lower bounds to future work. We also carried out experiments to numerically compare the performance of our algorithm with those proposed in the literature. Due to space limitation, the results are reported in Appendix D.

## 5 Generalized Frank-Wolfe for Stochastic Monotone Variational Inequalities

We now analyze the case where instead of having an accurate \(F()\), we only have access to a noisy estimator of \(F()\). This happens often in optimization and machine learning, where we sometimes do not have enough information or enough computational power to fully evaluate the operator \(F()\). Note that this is different from the additive noise \(w_{t}\) in Algorithm 2 Line \(4\), which captures the stochasticity in choosing the FW direction.

In general, incorporating stochasticity into FW algorithms in optimization is known to be nontrivial when the variance of the noise, though bounded, is not sufficiently small. When there is only access to a noisy oracle for \(F(x_{t})\), the stochasticity enters the algorithm in a nonlinear manner through the computation of the smoothed FW direction. Consequently, developing algorithms with strong convergence guarantees becomes fundamentally more challenging. To illustrate this, suppose that we directly use a noisy estimate \(F(x_{t})+z_{t}\) (where \(z_{t}\) represents the noise) in place of \(F(x_{t})\) in Algorithm 2 Line 3 to compute the FW direction \(s_{t}\). Despite replacing the hardmin with a softmin (by introducing a regularizer), the FW direction \(s_{t}\) remains highly sensitive to the noise \(z_{t}\) because the FW direction computed from the exact \(F(x_{t})\) and its noisy counterpart \(F(x_{t})+z_{t}\) could differ significantly. As a result, due to the lack of control over the noise, it has been observed that using a noisy estimator of the operator \(F()\) in place of \(F()\) can lead to the divergence of the algorithm .

To overcome this issue, existing approaches to stochastic FW often build reduced-variance estimators of the operator \(F()\). One of the most common methods to achieve this is by averaging the estimates. Inspired by this approach, we develop a stochastic generalized FW algorithm for constrained MVI problems, where we first average the noisy estimates of \(F()\) through an iterative framework. This results in Algorithm 3 presented in the following. For ease of exposition, we only present the algorithm with constant stepsizes.

```
1:Input: Integer \(T\), tunable parameter \(\), and initialization \(x_{0}\) and \(y_{0}^{d}\).
2:for\(t=0,1,,T-1\)do
3:\(y_{t+1}=y_{t}+(F(x_{t})+z_{t}-y_{t})\)
4:\(s_{t}=_{s}\{s^{}y_{t+1}+ f(s)\}\)
5:\(x_{t+1}=x_{t}-(x_{t}-s_{t})\)
6:endfor ```

**Algorithm 3** Stochastic Frank-Wolfe for Monotone Variational Inequalities

The key step in Algorithm 3 is Line 3, where we build a sequence of estimators \(\{y_{t}\}\) for \(\{F(x_{t})\}\) by averaging the newly observed noisy estimate \(F(x_{t})+z_{t}\) with past information. To illustrate, since \(y_{t+1}\) is a convex combination (with parameter \(\)) of the previous iterate \(y_{t}\) and \(F(x_{t})+z_{t}\), we see that for any \(t\), \(y_{t}\) is essentially a convex combination (hence a weighted average) of \(\{F(x_{i})+z_{i}\}_{0 i t-1}\). Suppose that \(x_{t}\) were stationary (i.e., \(x_{t} x\) for some \(x\)), then \(y_{t}\) effectively becomes a variance-reduced estimator of \(F(x)\). To extend this idea to time-varying \(x_{t}\), we choose the stepsizes \(\) and \(\) such that \(\), creating a two-timescale structure . This ensures that, from the perspective of \(y_{t}\), \(x_{t}\) is nearly stationary, allowing \(y_{t}\) to converge to \(F(x_{t})\) on a faster timescale. As a result, the \(x_{t}\) iterates should behave similarly to the case where we have an accurate estimate of \(F()\). In Lemma 5.1, we show that this is indeed the case for the \(y\)-process.

To present the lemma, we first formally state our assumption on the noise sequence \(\{z_{t}\}\).

**Assumption 5.1**.: It holds for all \(t 0\) that \([z_{t}_{t}]=0\) and \([\|z_{t}\|_{2}^{2}_{t}]_{z}\) for some \(_{z}>0\), where \(_{t}\) is the \(\)-algebra generated by \(\{x_{0},y_{0},z_{0},z_{1},,z_{t-1}\}\).

The next lemma bounds the distance between the estimator \(y_{t}\) and the desired target \(F(x_{t})\). The proof is presented in Appendix C.1.

**Lemma 5.1**.: _Suppose that Assumptions 4.1 and 5.1 are satisfied and \((0,1)\). Then, we have for any \(t 0\) that_

\[[\|y_{t}-F(x_{t})\|_{2}^{2}](1-)^{t} \|y_{0}-F(x_{0})\|^{2}+}{3}+D_{} ^{2}^{2}}{^{2}}.\] (7)

In view of the last term on the right-hand side of Eq. (7), to make \([\|y_{t}-F(x_{t})\|_{2}^{2}]\) sufficiently small, we need the ratio between the stepsizes, i.e., \(/\), to be sufficiently small. This mathematically justifies the two-timescale structure in Algorithm 3. Notably, Lemma 5.1 holds irrespective of the monotonicity of \(F()\) and only makes use of its assumed Lipschitz continuity. Using this lemma allows us to prove the following result for stochastic FW in constrained MVI problems.

**Theorem 5.1**.: _Consider \(\{x_{t}\}\) generated by Algorithm 3. Suppose that \(F()\) is a monotone operator on \(\), Assumptions 4.1, 4.2, and 5.1 are satisfied, and the regularizer \(f()\) satisfies Condition 4.1.__Then, when choosing \(=8^{2/3}/3(0,1)\), we have for any \(t 0\) that_

\[[_{s}(x_{t}-s)^{}F(x_{t})] _{1}t(1-)^{t}+_{2}^{1/3}}{}+ _{3}^{2/3}}{}+_{4}}{}+_{5 }+,\]

_where \(\{_{i}\}_{1 i 5}\) are problem-dependent constants. See Appendix C.2 for their explicit expressions._

A proof sketch of Theorem 5.1 is presented in Section 6, and the complete proof can be found in Appendix C.2. Based on Theorem 5.1, we have the following iteration complexity.

**Corollary 5.1.1**.: _To achieve \([_{s}(x_{t}-s)^{}F(x_{t})]\), the iteration complexity is \(}(^{-6})\)._

Once again, we remark that, to the best of our knowledge, this appears to be the first algorithm with a last-iterate convergence guarantee in constrained stochastic MVI problems. The guarantees for variants of gradient-based algorithms are on the averaged iterate or under strong curvature assumptions on \(F()\) or \(\). Numerical simulations are provided in Appendix D to verify the last-iterate convergence of our proposed algorithm.

Our iteration complexity of \(}(^{-6})\) is slower than the \((^{-2})\) enjoyed by stochastic mirror-prox algorithms in an averaged-iterate sense . Despite the fact that we have last-iterate convergence, we believe that the above bound is not tight since using the same estimator in convex optimization problems results in an \((^{-3})\) rate of convergence . The reason for the potential looseness in our analysis is due to the fact that \(F()\) is not the gradient of a function and as such we must rely on the smoothness of the estimated FW direction \(s_{t}\), which results in a suboptimal relationship between the hyperparameter \(\) and \([\|y_{t}-F(x_{t})\|_{2}^{2}]\) in our analysis.

## 6 Proof Sketch of Theorem 5.1

Here, we present an outline of the proof of Theorem 5.1, which uses Lyapunov-based arguments. The proof of Theorem 4.1 follows a similar approach.

The first step in proving Theorem 5.1 is to establish Lemma 5.1, which shows that our constructed estimator \(y_{t}\) indeed keeps track of the desired target \(F(x_{t})\).

### Proof Sketch of Lemma 5.1

We will use \(\|y_{t}-F(x_{t})\|_{2}^{2}\) as a Lyapunov function to study the evolution of \(y_{t}\). To begin with, for any \(t 0\), we have by Algorithm 3 Line \(3\) that

\[\|y_{t+1}-F(x_{t+1})\|_{2}^{2}= \,\|(1-)y_{t}+(F(x_{t})+z_{t})-F(x_{t+1})\|_{2}^{2}\] \[= \,\|(1-)(y_{t}-F(x_{t}))+ z_{t}+F(x_{t})-F(x_{t+1})\|_{ 2}^{2}\] \[= \,(1-)^{2}\|y_{t}-F(x_{t})\|_{2}^{2}+^{2}\|z_{t}\|_{2}^ {2}+\|F(x_{t})-F(x_{t+1})\|_{2}^{2}\] \[+2(1-)(y_{t}-F(x_{t}))^{}z_{t}+2 z_{t}^{} (F(x_{t})-F(x_{t+1}))\] \[+2(1-)(y_{t}-F(x_{t}))^{}(F(x_{t})-F(x_{t+1})).\]

Taking expectations conditioned on \(_{t}\) on both sides of the previous inequality, and using Assumption 5.1 for the noise sequence \(\{z_{t}\}\), along with some algebraic manipulations, we obtain

\[[\|y_{t+1}-F(x_{t+1})\|_{2}^{2}_{t}] \,(1-)\|y_{t}-F(x_{t})\|_{2}^{2}+2 ^{2}_{z}\] \[+[\|F(x_{t})-F(x_{t+1})\|_{2}^{2} _{t}].\]

The details are presented in Appendix C.1. By the Lipschitz continuity of \(F()\) (cf. Assumption 4.1), we have

\[[\|F(x_{t})-F(x_{t+1})\|_{2}^{2}_{t}]  L_{F}[\|x_{t+1}-x_{t}\|_{2}^{2}_{t}]\] \[=L_{F}^{2}[\|s_{t}-x_{t}\|_{2}^{2} _{t}]\] \[ 4L_{F}D_{}^{2}^{2},\]

[MISSING_PAGE_FAIL:10]