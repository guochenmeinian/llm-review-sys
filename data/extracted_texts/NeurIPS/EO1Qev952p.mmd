# On Socially Fair Low-Rank Approximation and Column Subset Selection

Zhao Song

The Simons Institute for the Theory of Computing

UC Berkeley

magic.linuxkde@gmail.com &Ali Vakilian

Toyota Technological Institute at Chicago

vakilian@ttic.edu &David P. Woodruff

Department of Computer Science

Carnegie Mellon University

dwoodruf@andrew.cmu.edu &Samson Zhou

Department of Computer Science

Texas A&M University

samsonzhou@gmail.com

###### Abstract

Low-rank approximation and column subset selection are two fundamental and related problems that are applied across a wealth of machine learning applications. In this paper, we study the question of socially fair low-rank approximation and socially fair column subset selection, where the goal is to minimize the loss over all sub-populations of the data. We show that surprisingly, even constant-factor approximation to fair low-rank approximation requires exponential time under certain standard complexity hypotheses. On the positive side, we give an algorithm for fair low-rank approximation that, for a constant number of groups and constant-factor accuracy, runs in \(2^{(k)}\) time rather than the naive \(n^{(k)}\), which is a substantial improvement when the dataset has a large number \(n\) of observations. We then show that there exist bicriteria approximation algorithms for fair low-rank approximation and fair column subset selection that run in polynomial time.

## 1 Introduction

Machine learning algorithms are increasingly used in technologies and decision-making processes that affect our daily lives, from high volume interactions such as online advertising, e-mail filtering, smart devices, or large language models, to more critical processes such as autonomous vehicles, healthcare diagnostics, credit scoring, and sentencing recommendations in courts of law . Machine learning algorithms frequently require statistical analysis, utilizing fundamental problems from numerical linear algebra, especially low-rank approximation and column subset selection.

In the classical low-rank approximation problem, the input is a data matrix \(^{n d}\) and an integral rank parameter \(k>0\), and the goal is to find the best rank \(k\) approximation to \(\), i.e., to find a set of \(k\) vectors in \(^{d}\) that span a matrix \(\), which minimizes \((-)\) across all rank-\(k\) matrices \(\), for some loss function \(\). The rank parameter \(k\) should be chosen to accurately represent the complexity of the underlying model chosen to fit the data, and thus the low-rank approximation problem is often used for mathematical modeling and data compression.

Similarly, in the classic column subset selection problem, the goal is to choose \(k\) columns \(\) of \(\) so as to minimize \((-)\) across all choices of \(^{k d}\). Although low-rank approximation can reveal important latent structure among the dataset, the resulting linear combinations may not be as interpretable as simply selecting \(k\) features. The column subset selection problem is thereforea version of low-rank approximation with the restriction that the left factor must be \(k\) columns of the data matrix. Column subset selection also tends to result in sparse models. For example, if the columns of \(\) are sparse, then the columns in the left factor \(\) are also sparse. Thus in some cases, column subset selection, often also called feature selection, can be more useful than general low-rank approximation.

**Algorithmic fairness.** Unfortunately, real-world machine learning algorithms across a wide variety of domains have recently produced a number of undesirable outcomes from the lens of generalization. For example,  noted that decision-making processes using data collected from smartphone devices reporting poor road quality could potentially underserve poorer communities with less smartphone ownership.  observed that search queries for CEOs overwhelmingly returned images of white men, while  observed that facial recognition software exhibited different accuracy rates for white men compared with dark-skinned women.

Initial attempts to explain these issues can largely be categorized into either "biased data" or "biased algorithms", where the former might include training data that is significantly misrepresenting the true statistics of some sub-population, while the latter might sacrifice accuracy on a specific sub-population in order to achieve better global accuracy. As a result, an increasingly relevant line of active work has focused on designing _fair_ algorithms. An immediate challenge is to formally define the desiderata demanded from fair algorithmic design and indeed multiple natural quantitative measures of fairness have been proposed . However,  showed that many of these conditions for fairness cannot be simultaneously achieved.

In this paper, we focus on _socially fair_ algorithms, which seek to optimize the performance of the algorithm across all sub-populations. That is, for the purposes of low-rank approximation and column subset selection, the goal is to minimize the maximum cost across all sub-populations. For socially fair low-rank approximation, the input is a set of data matrices \(^{(1)}^{n_{1} d},,^{()} ^{n_{} d}\) corresponding to \(\) groups and a rank parameter \(k\), and the goal is to determine a set of \(k\) factors \(_{1},,_{k}^{d}\) that span matrices \(^{(1)},,^{()}\), which minimize \(_{i[]}\|^{(i)}-^{(i)}\|_{F}\). Due to the Eckart-Young-Mirsky theorem stating that the Frobenius loss is minimized when each \(^{(i)}\) is projected onto the span of \(=_{1}_{k}\), the problem is equivalent to \(_{^{k d}}_{i[]}\|^{(i) }^{}-^{(i)}\|_{F}\), where \(\) denotes the Moore-Penrose pseudoinverse. We remark that as we can scale the columns in each group (even each individual column), our formulation captures min-max normalized cost relative to the total Frobenius norm of each group and the optimal rank-\(k\) reconstruction loss for each group.

### Our Contributions and Technical Overview

In this paper, we study socially fair low-rank approximation and socially fair column subset selection.

**Fair low-rank approximation.** We first describe our results for socially fair low-rank approximation. We first show that under the assumption that \(\), fair low-rank approximation cannot be approximated within any constant factor in polynomial time.

**Theorem 1.1**.: _Fair low-rank approximation is NP-hard to approximate within any constant factor._

We show Theorem1.1 by reducing to the problem of minimizing the distance of a set of \(n\) points in \(d\)-dimensional Euclidean space to all sets of \(k\) dimensional linear subspaces, which was shown by , DTV11] to be NP-hard to approximate within any constant factor. In fact,  showed that a constant-factor approximation to this problem requires runtime exponential in \(k\) under a stronger assumption, the exponential time hypothesis . We show similar results for the fair low-rank approximation problem.

**Theorem 1.2**.: _Under the exponential time hypothesis, the fair low-rank approximation requires \(2^{k^{(1)}}\) time to approximate within any constant factor._

Together, Theorem1.1 and Theorem1.2 show that under standard complexity assumptions, we cannot achieve a constant-factor approximation to fair low-rank approximation using time polynomial in \(n\) and exponential in \(k\). We thus consider additional relaxations, such as bicriteria approximation (Theorem1.4) or \(2^{(k)}\) runtime (Theorem1.3). On the positive side, we first show that for a constant number of groups and constant-factor accuracy, it suffices to use runtime \(2^{(k)}\) rather than the naive \(n^{(k)}\), which is a substantial improvement when the dataset has a large number of observations, i.e., \(n\) is large.

**Theorem 1.3**.: _Given an accuracy parameter \((0,1)\), there exists an algorithm which outputs \(}^{k d}\) such that with probability at least \(\), \(_{i[]}\|^{(i)}(})^{} }-^{(i)}\|_{F}(1+)_{ ^{k d}}_{i[]}\|^{(i)} ^{}-^{(i)}\|_{F}\). The algorithm uses runtime \((n)(2)^{(N)}\), for \(n=_{i=1}^{}n_{i}\) and \(N=,k,\)._

Next, we show that there exists a bicriteria approximation algorithm for fair low-rank approximation that uses polynomial runtime.

**Theorem 1.4**.: _Given a trade-off parameter \(c(0,1)\), there exists an algorithm that outputs \(}^{t d}\) for \(t=(k( k)(^{2}d))\) such that with probability at least \(\),_

\[_{i[]}\|^{(i)}(})^{} }-^{(i)}\|_{F}^{c} 2^{1/c} (k( k)( d))_{^{k d}}_{i[]}\|^{(i)}^{} -^{(i)}\|_{F}.\]

_The algorithm uses runtime polynomial in \(n\) and \(d\)._

The algorithm for Theorem1.4 substantially differs from that of Theorem1.3. For one, we can no longer use a polynomial system solver, because it would be infeasible to achieve polynomial runtime. Instead, we observe that for sufficiently large \(p\), we have \(\|\|_{}=(1)\|\|_{p}\) and thus focus on optimizing \(_{^{k d}}(_{i[]}\| ^{(i)}^{}-^{(i)}\|_{F}^{p})^{1/p}\). However, the terms \(\|^{(i)}^{}-^{(i)}\|_{F}^{p}\) are difficult to handle, so we apply Dvoretzky's Theorem, i.e., Theorem3.2, to generate matrices \(\) and \(\) so that \((1-)\|\|_{p}\|\|_{F} (1+)\|\|_{p}\), for all matrices \(^{n d}\), so that it suffices to approximately solve \(_{^{k d}}\| -\|_{p}\), for \(=^{(1)}^{()}\).

Although low-rank approximation with \(L_{p}\) loss cannot be well-approximated in polynomial time, we recall that there exists a matrix \(\) that samples a "small" number of columns of \(\) to provide a coarse bicriteria approximation to \(L_{p}\) low-rank approximation . However, we require a solution with dimension \(d\) and thus we seek to solve the regression problem \(_{}\|-\|_{p}\). Thus, we consider a Lewis weight sampling matrix \(\) such that

\[\|- \|_{p}\| -\|_{p} 2\| - \|_{p}.\]

and again note that \(()^{} \) is the closed-form solution to the minimization problem \(_{}\|-\|_{F}\), which only provides a small distortion to the \(L_{p}\) regression problem, since \(\) has a small number of rows due to the dimensionality reduction. We then observe that by Dvoretzky's Theorem, \(()^{} \) is a "good" approximate solution to the original fair low-rank approximation problem. Given \((0,1)\), the success probabilities for both Theorem1.3 and Theorem1.4 can be boosted to arbitrary \(1-\) by taking the minimum of \(()\) independent instances of the algorithm, at the cost of increasing the runtime by the same factor.

**Fair column subset selection.** We next describe our results for fair column subset selection. We give a bicriteria approximation algorithm for fair column subset selection that uses polynomial runtime.

**Theorem 1.5**.: _Given input matrices \(^{(i)}^{n_{i} d}\) with \(n= n_{i}\), there exists an algorithm that selects a set \(S\) of \(k^{}=(k k)\) columns such that with probability at least \(\), \(S\) is a \((k( k)( d))\)-approximation to the fair column subset selection problem. The algorithm uses runtime polynomial in \(n\) and \(d\)._

The immediate challenge in adapting the previous approach for fair low-rank approximation to fair column subset selection is that we required the Gaussian matrices \(,\) to embed the awkward maximum of Frobenius losses \(_{i[]}\|\|_{F}\) into the more manageable \(L_{p}\) loss \(\|\|_{p}\) through \(\). However, selecting columns of \(\) does not correspond to selecting columns of the input matrices \(^{(1)},,^{()}\).

Instead, we view the bicriteria solution \(}\) from fair low-rank approximation as a good starting point for the right factor for fair low-rank approximation. Thus we consider the multi-response regression problem \(_{i[]}_{^{(i)}}\|^{(i)}} -^{(i)}\|_{F}\). We then argue through Dvoretzky's theorem that a leverage score sampling matrix \(\) that samples \((k k)\) columns of \(}\) will provide a good approximation to the column subset selection problem. We defer the formal exposition to Section4.

**Empirical evaluations.** Finally, in Section5, we perform a number of experimental results on socially fair low-rank approximation, comparing the performance of the socially fair low-rank objective values associated with the outputs of the bicriteria fair low-rank approximation algorithm and the standard (non-fair) low-rank approximation that outputs the top \(k\) right singular vectors of the singular value decomposition.

Our experiments are on the Default of Credit Card Clients dataset , which is a common human-centric data used for benchmarks on fairness, e.g., see . We perform empirical evaluations comparing the objective value and the runtime of our bicriteria algorithm with the aforementioned baseline, across various subsample sizes and rank parameters. Our results demonstrate that our bicriteria algorithm can perform better than the standard low-rank approximation algorithm across various parameter settings, even when the bicriteria algorithm is not allowed a larger rank than the baseline. Moreover, we show that our algorithm is quite efficient and in fact, the final step of extracting the low-rank factors is faster than the singular value decomposition baseline due to a smaller input matrix. All in all, our empirical evaluations indicate that our bicriteria algorithm can perform well in practice, thereby reinforcing the theoretical guarantees of the algorithm. Finally, we give a number of additional experiments on synthetic datasets, in AppendixD.

**Paper organization.** We first described a number of related works in Section2. We detail our socially fair low-rank approximation algorithms in Section3 and our socially fair column subset selection algorithms in Section4. We defer all proofs to the supplementary material. We present our experiments in Section5 and AppendixD. The reader may also find it helpful to consult AppendixA for standard notation used in our paper and additional preliminaries.

## 2 Related Work

Initial insight into _socially fair data summarization_ methods were presented by , where the concept of _fair PCA_ was explored. This study introduced the fairness metric of average reconstruction loss, expressed by the loss function \((,):=\|-\|_{F}^{2}-\| -_{k}\|_{F}^{2}\), aiming to identify a \(k\)-dimensional subspace that minimizes the loss across the groups, with \(_{k}\) representing the best rank-\(k\) approximation of \(\). Their proposed approach, in a two-group scenario, identifies a fair PCA of up to \(k+1\) dimensions that is not worse than the optimal fair PCA with \(k\) dimensions. When extended to \(\) groups, this method requires an additional \(k+-1\) dimensions. Subsequently,  explored fair PCA from a distinct objective perspective, seeking a projection matrix \(\) optimizing \(_{i[]}\|^{(i)}\|_{F}^{2}\). A pivotal difference between these works and ours is our focus on the reconstruction error objective, a widely accepted objective for regression and low-rank approximation tasks. Alternatively,  explored a different formulation of fair PCA. The main objective is to ensure that data representations are not influenced by demographic attributes. In particular, when a classifier is exposed only to the projection of points onto the \(k\)-dimensional subspace, it should be unable to predict the demographic attributes.

Recently,  studied a fair column subset selection objective similar to ours, focusing on the setting with two groups (i.e., \(=2\)). They established the problem's NP-hardness and introduced a polynomial-time solution that offers relative-error guarantees while selecting a column subset of size \((k)\).

For fair regression, initial research focused on designing models that offer similar treatment to instances with comparable observed results by incorporating _fairness regularizers_. However, in , the authors studied a fairness notion closer to our optimization problem, termed as "bounded group loss". In their work, the aim is to cap each group's loss within a specific limit while also optimizing the cumulative loss. Notably, their approach diverged from ours, with a focus on the sample complexity and the problem's generalization error bounds.

 studied a similar socially fair regression problem under the name min-max regression. In their setting, the goal is to minimize the maximum loss over a mixture distribution, given samples from the mixture; our fair regression setting can be reduced to theirs.  observed that a maximum of norms is a convex function and can therefore be solved using projected stochastic gradient descent.

The term "socially fair" was first introduced in the context of clustering, aiming to optimize clustering costs across predefined group sets . In subsequent studies, tight approximation algorithms , FPT approaches , and bicriteria approximation algorithms  for socially fair clustering have been presented.

## 3 Socially Fair Low-Rank Approximation

In this section, we consider algorithms and hardness for socially fair low-rank approximation. Let \(n_{1},,n_{}\) be positive integers and for each \(i[]\), let \(^{(i)}^{n_{i} d}\). Then for a norm \(\|\|\), we define the fair low-rank approximation problem to be \(_{^{k d}}_{i[]}\|^{(i)} ^{}-^{(i)}\|\).

### \((1+)\)-Approximation Algorithm for Fair Low-Rank Approximation

We first give a \((1+)\)-approximation algorithm for fair low-rank approximation that uses runtime \((n)(2)^{(N)}\), for \(n=_{i=1}^{}n_{i}\) and \(N=,k,\).

The algorithm first finds a value \(\) that is an \(\)-approximation to the optimal solution, i.e., \(_{^{k d}}_{i[]}\|^{(i) }^{}-^{(i)}\|_{F}\) is at most \(_{^{k d}}_{i[] }\|^{(i)}^{}-^{(i)}\|_{F}\). We then repeatedly decrease \(\) by \((1+)\) while checking if the resulting quantity is still achievable. To efficiently check if \(\) is achievable, we first apply dimensionality reduction to each of the matrices by right-multiplying by an affine embedding matrix \(\), so that

\[(1-)\|^{(i)}^{}- ^{(i)}\|_{F}^{2} \|^{(i)}^{}-^{(i)}\|_{F}^{2}(1+)\|^{(i)}^{ }-^{(i)}\|_{F}^{2},\]

for all rank \(k\) matrices \(\) and all \(i[]\).

Now if we knew \(\), then for each \(i[]\), we can find \(^{(i)}\) minimizing \(\|^{(i)}-^{(i)}\|_{F}^{2}\) and the resulting quantity will approximate \(\|^{(i)}^{}-^{(i)}\|_{F}^{2}\). In fact, we know that the minimizer is \((^{(i)})()^{}\) through the closed form solution to the regression problem. Let \(^{(i)}\) be defined so that \((^{(i)})()^{}^{(i)}\) has orthonormal columns, so that

\[\|(^{(i)})()^{}^{(i) })((^{(i)})()^{}^{(i )})^{}^{(i)}-^{(i)}\|_{F}^{2}= _{^{(i)}}\|^{(i)}-^{(i)} \|_{F}^{2},\]

and so we require that if \(\) is feasible, then \(\|(^{(i)})()^{} ^{(i)})((^{(i)})()^{ }^{(i)})^{}^{(i)}-^{(i)} \|_{F}^{2}\). Unfortunately, we do not know \(\), so instead we use a polynomial solver to check whether there exists such a \(\). We remark that similar guessing strategies were employed by  and in particular,  also uses a polynomial system in conjunction with the guessing strategy. Thus we write \(=\) and its pseudoinverse \(=()^{}\) and check whether there exists a satisfying assignment to the above inequality, given the constraints (1) \(=\), (2) \(=\), and (3) \(^{(i)}^{(i)}\) has orthonormal columns. Note that since \(^{k d}\), then implementing the polynomial solver naively could require \(kd\) variables and thus use \(2^{(dk)}\) runtime. Instead, we note that we only work with \(\), which has dimension \(k m\) for \(m=(}{^{2}})\), so that the polynomial solver only uses \(2^{(mk)}\) time.

We now show a crucial structural property that allows us to distinguish between the case where a guess \(\) for the optimal value \(\) exceeds \((1+)\) or is smaller than \((1-)\) by simply looking at a polynomial system solver on an affine embedding.

**Lemma 3.1**.: _Let \(^{k d}\) be the optimal solution to the fair low-rank approximation problem for inputs \(^{(1)},,^{()}\), where \(^{(i)}^{n_{i} d}\), and suppose \(=_{i[]}\|^{(i)}^{}- ^{(i)}\|_{F}^{2}\). Let \(\) be an affine embedding for \(\) and let \(=()^{}^{k m}\). For \(i[]\), let \(^{(i)}=^{(i)}^{n_{i}  k}\) and \(^{(i)}^{k k}\) be defined so that \(^{(i)}^{(i)}\) has orthonormal columns. If \((1+)\), then for each \(i[]\), \(\|(^{(i)}^{(i)})(^{ (i)}^{(i)})^{}^{(i)}- ^{(i)}\|_{F}^{2}\). If \(<(1-)\), then there exists \(i[]\), such that \(<\|(^{(i)}^{(i)})(^{ (i)}^{(i)})^{}^{(i)}- ^{(i)}\|_{F}^{2}\)._```
0:\(^{(i)}^{n_{i} d}\) for all \(i[]\), rank parameter \(k>0\), accuracy parameter \((0,1)\)
1: Generate Gaussian matrices \(^{n^{} n},^{d d ^{}}\) through Theorem 3.2
2: Let \(^{n^{} t},^{t d ^{}}\) be the output of Theorem 3.3 on input \(\)
3: Let \(^{s n^{}}\) be a Lewis weight sampling matrix for \(-\)
4: Let \(}()^{}()\)
5: Return \(}\) ```

**Algorithm 2**\((1+)\)-approximation for fair low-rank approximation

### Bicriteria Algorithm

To achieve polynomial time for our bicriteria algorithm, we can no longer use a polynomial system solver. Instead, we observe that for sufficiently large \(p\), we have \(\|\|_{}=(1)\|\|_{p}\). Thus, in place of optimizing \(_{^{k d}}_{i[]}\|^{(i)} ^{}-^{(i)}\|_{F}\), we instead optimize \(_{^{k d}}(_{i[]}\| ^{(i)}^{}-^{(i)}\|_{F}^{p})^{1/p}\). However, the terms \(\|^{(i)}^{}-^{(i)}\|_{F}^{p}\) are unwieldy to work with. Thus we instead use Dvoretzky's Theorem, i.e., Theorem 3.2, to embed \(L_{2}\) into \(L_{p}\), by generating matrices \(\) and \(\) so that \((1-)\|\|_{p}\|\|_{F} (1+)\|\|_{p}\), for all matrices \(^{n d}\).

Now, writing \(=^{(1)}^{()}\), it suffices to approximately solve \(_{^{k d}}\|- \|_{p}\). Unfortunately, low-rank approximation with \(L_{p}\) loss still cannot be approximated to \((1+)\)-factor in polynomial time, and in fact \(\) has dimension \(n^{} d^{}\) with \(n^{} n\) and \(d^{} d\). Hence, we first apply dimensionality reduction by appealing to a result of  showing that there exists a matrix \(\) that samples a "small" number of columns of \(\) to provide a coarse bicriteria approximation to \(L_{p}\) low-rank approximation. Now to lift the solution back to dimension \(d\), we would like to solve regression problem \(_{}\|-\|_{p}\). To that end, we consider a Lewis weight sampling matrix \(\) such that

\[\|-\|_{p}\| -\|_{p} 2\|-\|_{p}.\]

We then note that \(()^{}\) is the minimizer of the problem \(_{}\|-\|_{F}\), which only provides a small distortion to the \(L_{p}\) regression problem, since \(\) has a small number of rows due to the dimensionality reduction. By Dvoretzky's Theorem, we have that \(()^{}\) is a "good" approximate solution to the original fair low-rank approximation problem. The algorithm appears in full in Algorithm 3.

```
0:\(^{(i)}^{n_{i} d}\) for all \(i[]\), rank parameter \(k>0\), accuracy parameter \((0,1)\)
0:\((1+)\)-approximation for fair LRA
1: Let \(\) be an \(\)-approximation for the fair LRA problem
2: Let \(\) be generated from a random affine embedding distribution
3:while Algorithm 1 on input \(^{(1)},,^{()}\), \(\), and \(\) does not return \(\)do
4: Let \(\) be the output of Algorithm 1 on input \(^{(1)},,^{()}\), \(\), and \(\)
5:\(\)
6:endwhile
7: Return \(\) ```

**Algorithm 3** Bicriteria approximation for fair low-rank approximation

```
0:\(^{(i)}^{n_{i} d}\) for all \(i[]\), rank parameter \(k>0\), trace-off parameter \(c(0,1)\)
0: Bicriteria approximation for fair LRA
1: Generate Gaussian matrices \(^{n^{} n},^{d d ^{}}\) through Theorem 3.2
2: Let \(^{n^{} t},^{t d ^{}}\) be the output of Theorem 3.3 on input \(\)
3: Let \(^{s n^{}}\) be a Lewis weight sampling matrix for \(-\)
4: Let \(}()^{}()\)
5: Return \(}\) ```

**Algorithm 4** Bicriteria approximation for fair LRA

```
0:\(^{(i)}^{n_{i} d}\) for all \(i[]\), rank parameter \(k>0\), trace-off parameter \(c(0,1)\)
0: Bicriteria approximation for fair LRA
1: Generate Gaussian matrices \(^{n^{} n},^{d d ^{}}\) through Theorem 3.2
2: Let \(^{n^{} t},^{t d ^{}}\) be the output of Theorem 3.

We use the following notion of Dvoretzky's theorem to embed the problem into entrywise \(L_{p}\) loss.

**Theorem 3.2** (Dvoretzky's Theorem, e.g., Theorem 1.2 in , Fact 15 in ).: _Let \(p 1\) be a parameter and let_

\[m m(n,p,)=n}{^{2}},& (Cp)^{}n^{-}\\ }{},&((Cp)^{}n^{- },]\\ }{p^{p/2}^{p/2}}^{p/2},&<<1.\]

_Then there exists a family \(\) of random scaled Gaussian matrices with dimension \(^{m n}\) such that for \(G\), with probability at least \(1-\), simultaneously for all \(^{n}\), \((1-)\|\|_{2}\|\|_{p}(1+ )\|\|_{2}\)._

We use the following algorithm from  to perform dimensionality reduction so that switching between \(L_{2}\) and \(L_{p}\) loss will incur smaller error. See also .

**Theorem 3.3** (Theorem 1.5 in ).: _Let \(^{n d}\) and let \(k 1\). Let \(s=(k k)\). Then there exists a polynomial-time algorithm that outputs a matrix \(^{d t}\) that samples \(t=(k( k)(^{2}d))\) columns of \(\) and a matrix \(^{t d}\) such that \(\|-\|_{p} 2^{p} ()_{^{n k},^{k d}}\|-\|_{p}\)._

We recall the following construction to use Lewis weights to achieve an \(L_{p}\) subspace embedding.

**Theorem 3.4** ().: _Let \((0,1)\) and \(p 2\). Let \(^{n d}\) and \(s=(d^{p/2} d)\). Then there exists a polynomial-time algorithm that outputs a matrix \(^{s n}\) that samples and reweights \(s\) rows of \(\), such that with probability at least \(0.99\), simultaneously for all \(^{d}\), \((1-)\|\|_{p}^{p}\| \|_{p}^{p}(1+)\|\|_{p}^{p}\)._

We then show that Algorithm 3 provides a bicriteria approximation.

**Lemma 3.5**.: _Let \(}\) be the output of Algorithm 3. Then with probability at least \(\), \(_{i[]}\|^{(i)}(})^{} }-^{(i)}\|_{F}\) is at most \(^{c} 2^{1/c}(k( k)( d))_{i[ ]}\|^{(i)}(})^{}}-^{(i)}\|_{F}\), where \(c\) is the trade-off parameter input._

Since the generation of Gaussian matrices and the Lewis weight sampling matrix both only require polynomial time, it follows that our algorithm uses polynomial time overall. Hence, we have Theorem 1.4.

## 4 Socially Fair Column Subset Selection

In this section, we consider socially fair column subset selection, where the goal is to identify a matrix \(^{d k}\) that selects \(k\) columns to minimize \(_{^{d k},\|\|_{0} k, ^{(i)}}_{i[]}\|^{(i)}^{(i)}-^{(i)}\|_{F}\).

```
0:\(^{(i)}^{n_{i} d}\) for all \(i[]\), rank parameter \(k>0\), trade-off parameter \(c(0,1)\)
0: Bicriteria approximation for fair column subset selection
1:Acquire \(}\) from Algorithm 3
2:Generate Gaussian \(^{n^{} n}\) through Theorem 3.2
3:Let \(^{d k^{}}\) be a leverage score sampling matrix that samples \(k^{}=(k k)\) columns of \(}\)
4:\(^{(i)}=^{}(})^{} }\) for all \(i[]\)
5:Return \(^{(i)}\), \(\{^{(i)}\}\) ```

**Algorithm 4** Bicriteria approximation for fair column subset selection

We first provide preliminaries on leverage score sampling.

**Definition 4.1**.: _Given a matrix \(^{n d}\), we define the leverage score \(_{i}\) of each row \(_{i}\) with \(i[n]\) by \(_{i}(^{})^{-1}_{i}^{}\). Equivalently, for the singular value decomposition \(=\), the leverage score of row \(_{i}\) is also the squared row norm of \(_{i}\)._We recall in Appendix C that it can be shown the sum of the leverage scores for an input matrix \(^{n d}\) is upper bounded by \(d\) and moreover, given the leverage scores of \(\), it suffices to sample only \((d n)\) rows of \(\) to achieve a constant factor subspace embedding of \(\). Because the leverage scores of \(\) can be computed directly from the singular value decomposition of \(\), which can be computed in \((nd^{}+dn^{})\) time where \(\) is the exponent of matrix multiplication, then the leverage scores of \(\) can be computed in polynomial time.

Finally, we recall that to provide a constant factor approximation to \(L_{p}\) regression, it suffices to compute a constant factor subspace embedding, e.g., through leverage score sampling. The proof is through the triangle inequality and is well-known among the active sampling literature , e.g., a generalization of Lemma 2.1 in . For completeness, we provide the proof in Appendix C.

**Lemma 4.2**.: _Given a matrix \(^{n d}\), let \(\) be a matrix such that for all \(^{d}\) and \(^{n}\),_

\[\|\|_{2}\|\|_{2}\|\|_{2},\ [\|\|_{2}^{2}]=\| \|_{2}^{2}.\]

_For a fixed \(^{n m}\) where \(=_{1}_{m}\) with \(_{i}^{n}\) for \(i[m]\), let \(_{i}}=()^{}(_{i})\). Let \(}=_{1}}_{m}}\). Then with probability at least \(0.97\), \(\|-\|_{2} 99_{}\| -\|_{2}\)._

We now give the correctness guarantees of Algorithm 4.

**Lemma 4.3**.: _Let \(,^{(1)},,^{(t)}\) be the output of Algorithm 4. Then with probability at least \(0.8\), \(_{i[t]}\|^{(t)}^{(i)}-^{(i)}\| _{F}\) is at most \(^{} 2^{1/c}(k( k)( d) )_{^{k d}}_{i[t]}\|^{( i)}^{}-^{(i)}\|_{F}\)._

We also have the following runtime guarantees on Algorithm 4.

**Lemma 4.4**.: _The runtime of Algorithm 4 is \((n,d)\)._

By combining Lemma 4.3 and Lemma 4.4, we have Theorem 1.5.

## 5 Empirical Evaluations

In this section, we describe our empirical evaluations for socially fair low-rank approximation on real-world datasets.

**Credit card dataset.** We used the Default of Credit Card Clients dataset , which has 30,000 observations across 23 features, including 17 numeric features. The dataset is a common human-centric data for experiments on fairness and was previously used as a benchmark by  for studies on fair PCA. The study collected information from various customers including multiple previous payment statements, previous payment delays, and upcoming bill statements, as well as if the customer was able to pay the upcoming bill statement or not, i.e., defaulted on the bill statement. The dataset was accessed through the UCI repository .

**Experimental setup.** For the purposes of reproducibility, our empirical evaluations were conducted using Python 3.10 using a 64-bit operating system on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. We compare our bicriteria algorithm from Algorithm 3 against the standard non-fair low-rank approximation algorithm that outputs the top \(k\) right singular vectors from the singular value decomposition. Gender was used as the sensitive attribute, so that all observations with one gender formed the matrix \(^{(1)}\) and the other observations formed the matrix \(^{(2)}\). As in Algorithm 3, we generate normalized Gaussian matrices \(\) and \(\) and then use \(L_{p}\) Lewis weight sampling to generate a matrix \(\). We generate matrices \(\), \(\), and \(\) with a small number of dimensions and thus do not compute the sampling matrix \(\) but instead use the full matrix. We first sampled a small number \(s\) of rows from \(^{(1)}\) and \(^{(2)}\) and compared our bicriteria algorithm to the standard non-fair low-rank approximation algorithm baseline. We plot the minimum ratio for \(s\{2,3,,21\}\), \(k=1\), and \(p=1\) over \(10,000\) iterations for each setup in Figure 0(a). Similarly, we plot both the minimum and average ratios for \(s=1000\), \(k\{1,2,,7,8\}\), and \(p=1\) over \(200\) iterations in Figure 0(b), where we permit the bicriteria solution to have rank \(2k\). Finally, we compare the runtimes of the algorithms in Figure 0(c), separating runtimes for our bicriteria into the total runtime bicrit1 that includes the process of generating the Gaussian matrices and performing the Lewis weight sampling, as well as the runtime bicrit2 for the step for extracting the factors

[MISSING_PAGE_FAIL:9]