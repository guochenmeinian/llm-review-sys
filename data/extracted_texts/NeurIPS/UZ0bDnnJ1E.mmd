# Regularizing the Infinite: Improved Generalization Performance with Deep Equilibrium Models

Babak Rahmani

Jannes Gladrow

Kirill Kalinin

Heiner Kremer

Christos Gkantsidis

Hitesh Ballani

Microsoft Research, Cambridge, UK CB4 0AB

{t-brahmani, jannes.gladrow, kkalinin, t-hkremer,

christos.gkantsidis, hitesh.ballani}@microsoft.com

###### Abstract

Implicit networks, such as Deep Equilibrium (DEQ) models, present unique opportunities for emerging computing paradigms. Unlike traditional feedforward (FFW) networks, DEQs adaptively adjust their compute resources which has been shown to improve out-of-distribution generalization, especially in algorithmic tasks. We demonstrate that this generalization includes robustness to noise making them well-suited for new hardware, such as analog or optical architectures, with higher yet noisy compute capabilities. But _do DEQ models consistently outperform FFW networks in generalization_? Surprisingly, our findings indicate that this advantage depends heavily on the specific task and network architecture. For equivalent network capacity, DEQ models prove more beneficial as the depth of the network increases--a trend that aligns with hardware systems optimized for deeper networks. We further demonstrate that regularizing the DEQ's entire dynamic process, instead of random initialization or threshold prescribed in previous work, significantly enhances performance across various tasks, including image classification, function regression, adversarial training, and algorithmic extrapolation, making DEQs a compelling choice for next-generation hardware systems.

## 1 Introduction

Deep Equilibrium Models (DEQs)  are machine learning models whose predictions result from fixed-points of a learned transformation. By relying on fixed-point iterations these models effectively implement infinite-depth neural networks using only a finite-depth transformation. The DEQ formulation could be beneficial for bi-level optimization, for example in inverse problems , and latent space optimization in generative models . Additionally, DEQs are deemed to provide improved performance in out-of-distribution (OOD) generalization compared to traditional feedforward (FFW) thanks to the adaptive inference computation implied by the fixed-point formulation, in contrast to the fixed computation budget of FFWs . To optimize memory usage during training, DEQs apply the implicit function theorem , to reduce the memory footprint to \(O(1)\) with regards to the number of iterations. Despite memory efficiency, training and inference of DEQs consume more FLOPS compared to FFW networks. A number of methods have been proposed to alleviate the heavy computation of DEQs during training by regularizing the Jacobian of the weights and reducing the number of iterations to convergence , approximating the fixed-points , or learning fixed-point solvers . Despite all these, DEQs still require convergence of the fixed-point iteration for each input batch of data. This raises the question as to _whether the extra computation compared to traditional FFW architectures provides sufficient benefits_.

We demonstrate that generalization capabilities of DEQs extends to the robustness of DEQ models in the presence of noise, which makes them ideal for emerging hardware technologies like analog  or optical  systems that offer greater compute but tend to be noisier. Counterintuitively, our findings reveal that the advantages of DEQ models over traditional FFWs are not universal; in fact, DEQ models are more robust compared to sufficiently _deep_ FFWs. Additionally, we show that introducing noise into the dynamics of the DEQ acts as a regularizer and enhances OOD generalization. We propose modifications to the fixed-point solvers and stopping criteria to enable more efficient fixed-point solving in noisy environments. Our study includes a range of tasks across various domains and problems, from image classification and adversarial robustness to algorithmic reasoning.

## 2 Related work

Noisy Neural NetworksOur method is related to prior work that studies the noising of neural network activations, either through dropout or the addition of additive/multiplicative Gaussian noise [23; 10; 28; 8; 19; 17]. These networks can be categorized as either FFWs , recurrent neural networks (RNNs) [10; 17], or neural Ordinary Differential Equations (ODEs), which with noise are transformed into stochastic differential equations (SDEs) [28; 19]. It has been demonstrated that noising activations contributes to the improved stability of recurrent networks through explicit  or implicit  regularization during training. This regularization can promote wider minima in the loss landscape [17; 8] that is advantageous, for example, in adversarial settings [28; 19]. Our work differs from previous work in that it focuses on DEQs with fixed-points, in contrast to SDEs/RNNs that do not necessarily reach a fixed-point. Second, we analytically compare the dynamics of the regularized DEQ and regularized FFW, and study the conditions under which a DEQ is more robust than a FFW. We look at the applications in multiple domains, including regression, classification, algorithmic tasks, and adversarial robustness. In the adversarial setting, we propose to regularize the entire dynamics of the DEQ instead of using costly optimization on the input data [30; 31].

Out-of-Distribution generalization in DEQsOur work is related to the adaptive inference computation of recurrent networks  and DEQs  to achieve OOD generalization . In the literature, the tasks assigned to DEQ networks are predominantly algorithmic reasoning, such as computing prefix sums and solving 2D mazes [2; 21; 25]. Our work differs in that these studies employ vanilla DEQ models without any form of regularization. We demonstrate that DEQs are not intrinsically superior to FFWs, however, in OOD settings, they tend to perform better under noise when the strength of the distribution shift increases.

## 3 Methods

### Preliminaries: DEQ's (perturbed) dynamics

The forward pass iteration of a DEQ can be written as

\[^{[m+1]}=f_{}(^{[m]};),\ ^{}=, (^{[M]};)-^{[M]} \|_{2}}{\|f_{}(^{[M]};)\|_{2}}<,\] (1)

where \(^{l}\) is the input vector, \(^{[m]}^{d}\) represents the intermediate state after the \(m\)-th iteration (with \(m=0,,M-1\)), \(f_{}:^{d}^{d}^{d}\) defines the transformation at each layer, \(\) denotes the set of parameters across the layers, \(\|\|_{2}\) denotes \(l_{2}\) norm, and \(\) is a small constant. To find the fixed-point \(^{*}\) of Eq. 1, solvers such as Broyden's method  and Anderson acceleration  are employed. These solvers find the solution \(^{*}\) when either the stop condition in Eq. 1 is met or the threshold \(M\) is reached. For distribution shifts in data, which can be formulated as \(+\), the dynamics of the DEQs change via:

\[&\|^{[m+1]}-}^{[m+1]}\|=\|f_{ }(^{[m]},)-f_{}(}^{[m]},+)\|\\ &\|f_{}(^{[m]},)-f_{}(}^{ [m]},)\|+\|f_{}(}^{[m]},)-f_{}(} ^{[m]},+)\|.\] (2)

Here, \(^{[m]}\) represents the state of the unperturbed system after the \(m\)-th iteration, while \(}^{[m]}\) denotes the state of the perturbed system at the same iteration. As Eq. (2) suggests, in order to bound the difference between the fixed-point of the model incurred by the original input \(\) and the fixed-point of the distribution-shifted \(+\), previous work  has utilized an optimization process that corrects perturbed inputs \(\) via a second iterative process at each step of the dynamics to decrease second term in Eq. (2) by reducing the entropy of the perturbed inputs in classification tasks. In contrast, we propose regularizing the entire dynamics (both terms in Eq. (2)) by introducing random perturbations throughout the evolution of the system. This approach aims to ensure that, for perturbed data, the perturbed and unperturbed dynamics remain closely aligned. This is achieved while being agnostic to the learning task and without requiring a second optimization per step of the DEQ dynamics.

### Regularization of DEQ dynamics under noise perturbations

To regularize the dynamics of the DEQ, we introduce perturbations to the intermediate states. Depending on the architecture, this can be achieved by perturbing either the states \(\) or implicitly through perturbing the input injection \(\). Since the magnitude of these states is not known a priori, we choose a multiplicative (input dependent) noise form to ensure we have control over the strength of the signal-to-noise ratio (SNR) of the states. Formally, the states and inputs of \(f_{}\) in Eq. 1 could be perturbed as follows:

\[^{[m+1]}=f_{}(^{[m]}+_{z}^{[m]},x+_{x}),\ ^{}=0,\] (3)

where

\[_{z}^{[m]} N(,\|^{[m]}\|_{2}/_{z} }),_{x} N(,\|\|_{2}/_{x}}).\] (4)

Note that \(_{x}\) and \(_{z}\) are hyperparameters chosen to fix the SNR with respect to each variable during the dynamics. Additionally, we consider a stronger case where noise is introduced before the last nonlinearity of the unit-cell. We refer to this as the'signal' noise with \(_{s}\). For example for a simple MLP unit-cell \(^{m+1}=(^{m}+)+_{s}\), noise \(_{s}\) scales with the signal \(+\). The fixed-point solution of Eq. 3 is used to minimize the loss function \(()\) for learning the task:

\[_{}(f_{}(^{*}+_{z}^{*}, {x}+_{x})),\] (5)

where \(^{*}\) is the fixed-point solution and \(_{z}^{*}\) is the corresponding noise. To avoid inefficient application of solvers, in the presence of noise, when the fixed-points are already reached, we modify the solvers and stopping criterion to accommodate the variability introduced by noise at each step. We aim to find solutions of the form

\[=_{_{x},_{z}}[f_{}(+ {}_{z},+_{x})],\] (6)

where \(^{*}\) is the fixed-point of \(f_{}\). Inspired by the stochastic approximation framework, we adopt the Robbins-Monro algorithm , which samples the stochastic function \(f_{}\):

\[^{[m+1]}=(1+)^{[m]}- f_{}(^{[m]}+_{z},+_{x}),^{*}=_{m=0} ^{M-1}^{[m]},\] (7)

Here, \(^{*}\) represents the solution that is the average of the intermediate values up to the \(M\)-th iteration, while \(\) serves as a hyperparameter. This method is referred to as'stochastic unrolling.'1 Rather than averaging all intermediate values, we implement averaging over a window of width \(w\) during the solver step of DEQ, which reduces the memory footprint of our model to the size of \(w\)--often a small number. Additionally, we employ more sophisticated solvers such as Anderson and Broyden. These solvers feature an internal averaging mechanism (e.g., the memory parameter in Anderson), which has the potential to obviate the need for explicit averaging of solutions. As for the stopping criterion, we utilize the relative norm applied to the averaged solution within the window-- see appendix B.

### Robustness in depth: DEQs are more noise robust than deep FFWs

In this section, we intend to compare regularized DEQ and FFW to determine if a fixed-point network is necessarily associated with stronger robustness. For this we use a simple one-layer fully connected network as the unit-cell for both networks. We assume an additive noise perturbation and refer to the multiplicative perturbation in the Appendix A.1.2 and A.2.2. For DEQ, we have

\[^{m+1}=(^{m}+}+)\] (8)

where \(^{d d}\) is the weight matrix, \(}^{d}\) is the input injection vector, \(\) is a scalar representing the noise amplitude, \(^{d}\) is a noise vector with elements drawn from a Gaussian distribution \((,I)\), and \(\) is the nonlinearity with \(^{}\) being its derivative. The sensitivity of the DEQ's hidden state with respect to the noise vector \(\) can be analyzed through the derivative \(^{m}}{d_{i}}\), where \(_{i}\) is the \(i\)-th element of \(\). This derivative, at iteration \(m+1\), is given by:

\[^{m+1}}{d_{i}}=(^{{}^{}} _{s_{m}})(^{m}}{d_{i}}+_{i})\] (9)

where \(_{m}\) is the function argument defined as \(_{m}:=^{m}+}+\), \(()\) is the diagonal matrix formed from the vector argument, and \(_{i}\) is the Kronecker delta vector, which has a one at the \(i\)-th position and zeros elsewhere. The FFW network at layer \(m\) is described by the following equation:

\[^{m+1}=(_{m}^{m}+).\] (10)

Here, \(_{m}^{d d}\) is the weight matrix for layer \(m\), and the rest of the terms are as previously defined for the DEQ. The sensitivity of the FFW's hidden state to the noise vector is similarly assessed by \(^{m+1}}{d_{i}}\), resulting in:

\[^{m+1}}{d_{i}}=(^{{}^{}} _{s_{m}})(_{m}^{m}}{d_{i}}+_{i})\] (11)

To compare the sensitivity of DEQ and FFW to noise, we examine the influence of their respective update equations on the propagation of perturbations to state \(^{m}\). We note that for the DEQ model, the update Eq. 9 iteratively applies the same weight matrix \(\). The norm of the derivative of the hidden state with respect to the noise, i.e. \(\|^{m}}{d_{i}}\|\), is affected by the spectral radius \(()^{m}\). Conversely, the FFW model's update Eq. 11 involves a product of distinct weight matrices \(_{m}\). The sensitivity derivative now depends on the spectral radius of the matrix product \((_{1}_{2}_{m})\).

Figure 1: **Robustness of regularized DEQ in classification**: Accuracy of DEQ as a function of increasing width and FFW as a function of increasing network layers on MNIST tested at SNR\(=0\)dB with **(a)** no training noise, **(b)** training noise applied to the dynamics with a signal SNR of 10dB, and **(c)** a signal SNR of 0dB. **(d)** Accuracy of the DEQ in a-c with width\(=3\) versus inference SNR sweep for three trainig SNRs of \(0\), \(10\)dB, no noise (SNR\(=\)). **(e)** Accuracy of MDEQ as a function of increasing branch (scale) and FFW as a function of increasing network layers with an inference signal SNR of 10dB (no train perturbation) on CIFAR-10. The parameter count for networks is indicated above each point.

In both models, the nonlinearity \(\) plays a crucial role in modulating sensitivity. Assuming a Lipschitz continuous \(\) with Lipschitz constant \(L\), the derivative norms could be (loosely) upper bounded by

\[\|^{m}}{d_{i}}\|_{}_{k=0}^{m-1} L^{k+1}()^{k},\|^{m}}{d_{i}}\|_{} _{k=0}^{m-1}L^{k+1}(_{1}..._{k}).\] (12)

DEQs can be constrained using Jacobian regularization  to achieve smaller spectral radii. Similarly, FFWs could also be regularized to have smaller Jacobians. Although this process is more costly for FFWs since the regularization needs to be computed at each layer, as opposed to only at the fixed-point for DEQs, we empirically found that DEQs still maintain smaller Jacobians than their FFW counterparts with Jacobian regularization--see Fig. 13 in Appendix D.2.

## 4 Experiments

We demonstrate the effectiveness of the neural dynamics regularization technique in DEQs to improve OOD generalization in noisy settings across multiple domains in machine learning, including image classification, function regression (see Appendix D.1), adversarial robustness, and algorithmic reasoning (see Appendix D.4). In all domains, we assess whether the fixed-point property of DEQs alone provides robustness advantages over traditional FFW architectures. Our empirical results indicate that DEQs outperform FFWs without any regularization only in deep FFW architectures (Section 3.3). Furthermore, we demonstrate that regularization of DEQs significantly enhances robustness (Section 3.2). We validate DEQs across various architectures, as well as bounded and unbounded nonlinearities to demonstrate generality with respect to architecture. See Appendix E for details of the network architectures and training.

### MNIST classification

We begin our examination by assessing the robustness of DEQ models trained on the MNIST dataset against perturbations applied at each iteration of the network. We compare these models to a multilayer FFW model. For the multilayer FFW, we increased the number of layers, each with different weights, while maintaining a consistent hidden size of 128 in each layer. In contrast, for the DEQ models, we expanded the width of the hidden state to ensure the number of parameters remained equivalent between both models. Consequently, the DEQ widths of 128, 223, 288, 340, 386, 427, and 467 correspond to FFWs ranging from single-layer up to thirteen-layer configurations. Figure 0(a)-c illustrates the accuracy of the DEQ/FFW models against an increasing width/number of layers tested at \(=0\,\) for different values of training SNRs. Note that for unregularized models (Fig. 0(a)), DEQ is not robust at low inference SNRs. Yet, when regularized with noise during training, DEQ's performance improves--see Fig. 0(d).

Figure 2: **Sensitivity of DEQ and FFW to noise in classification**: (**a**) Eigen value histogram of DEQ as a function of increasing width and FFW as a function of increasing network layers on MNIST. (**b**) sensitivity ratio of DEQ over FFW. A value smaller than one (green) indicates that the DEQ is less sensitive to the intermediate noise perturbation than the FFW. Models are trained without noise.

#### 4.1.1 Deep FFWs lag in robustness compared to DEQ

We look into the robustness of the DEQ models in the previous section from the sensitivity perspective. The right hand-side of the Fig. 2 shows the sensitivity ratio \(_{_{i}}[|^{*}}{d_{i}}|]\) of the DEQ to FFW network, where the sensitivities are averaged over 100 noise samples for all dimensions. Note that \(^{*}\) represents the fixed-point for the DEQ or the last layer activations for the FFW. We observe that as we increase the depth of the FFW, the DEQ with the corresponding number of parameters shows increasingly superior robustness. We also examine the eigenvalue distribution of the DEQ and FFW models in the left hand-side of Fig. 2. It is noteworthy to note that the DEQ models are less sensitive even when FFW networks are also Jacobian regularized--see Fig. 13 in Appendix D.2. This trend remains consistent as the DEQ models become wider or the FFW models become deeper.

#### 4.1.2 CIFAR-10 classification

We conduct experiments on vision tasks using the Multiscale-DEQ (MDEQ) , where multiple scales of the image are iterated to a fixed-point. We use up to four scales with number of filters of 32, 64, and 128 for the MDEQ architecture. In contrast, the FFW architecture maintains a consistent number of 128 filters across all layers. The perturbation applied in both DEQ and FFW architectures is a signal perturbation that is added before the nonlinearity of the unit layer. We plot the accuracy of the two architectures versus the branch/layer of the DEQ/FFW for inference perturbation in Figure 1e. These results are consistent with those from the analytical and small-scale DEQ models.

### Adversarial robustness

For adversarial experiments, we utilized adversarial training (AT) and dynamics regularization as the defense mechanism. For DEQ models, we propose injecting noise into either \(\) or \(\), while for FFWs, only \(\) is perturbed. Noise is added at two levels of SNRs, 0dB (denoted as SNR 1 in linear unit) and 10dB, with respect to the noise-injected variable. We also considered the random thresholding technique from previous work . In all scenarios, models are trained with either projected gradient descent (PGD)  or TRADES  for AT with \(_{}\)-norm perturbations, \(=0.3\) (the maximum allowable perturbation under the \(l_{}\) norm), step size \(0.1\), and \(10\) iterations for the PGD training attack. See Appendix E.3 and C for additional details on architecture, training and solver and loss construction in adversarial setting. Inference-time attacks include white-box attacks such as PGD and Autoattack (AA) , as well as black-box attacks like Salt-and-Pepper and Speckle attacks. We

Figure 3: **Improved robustness of DEQ through regularized dynamics in adversarial training**: (**a,b**) Adversarial accuracy of DEQ (top) and FFW (bottom) on MNIST classification, regularized with various SNRs and a random thresholding method, against PGD attacks (step size \(=0.1\)) plotted as versus number of attack steps. (**c-f**) Adversarial accuracy of DEQ and FFW with the strongest defense against various attacks. In each figure, the leftmost part corresponds to weak attacks (clean accuracy) progressing to strong attacks based on parameters specific to each attack.

use a convolutional architecture for both DEQ and FFWs (see Appendix E.3 for details). We test adversarial attacks on the MNIST dataset. For each attack, we first identify the strongest defense by understanding which noising regularization \(x\), \(z\), or thresholding defense, and what SNR provides the strongest attack. Figure 2(a,b) shows the accuracy of the models for DEQ and FFW under PGD attack and various regularization defenses. As seen, DEQ and SNR\(=1,x\) as well as FFW and SNR\(=1\) usually have the strongest defense. We then use these models and compare the performances of the models against various attacks in Fig. 2(c-f and supplementary Fig. 16. Interestingly, as the number of steps of the PGD attack increases, the gap between DEQ and FFW increases. This is also consistent with the strongest AA attack, in which the number of iterations is chosen automatically.

## 5 Broader Impact

In this work, we examined the robustness of DEQ networks, which learn fixed-point representations of data. We demonstrate that the fixed points of DEQs do not inherently offer improved performance compared to FFW networks. Previous work  has observed that DEQs mainly exhibit advantages during inference, particularly when applied to OOD algorithmic tasks. However, in the presence of noise, this advantage does not readily extend to common tasks such as classification and regression and is only noticeable for deeper FFW networks. We hypothesize that this behavior can be explained by the spectral norm of both the FFW and DEQ, at least for simple MLP architectures, which is empirically confirmed in larger MDEQ models. Additionally, we have developed a perturbation regularization approach that is task-agnostic and has proven effective in enhancing robustness over FFWs, including in shallow configurations. All these properties make DEQs a strong candidate for emerging analog hardware as a new computing paradigm [29; 27] that is not only robust to the system's inherent noise but also presents an opportunity to provide new capabilities.