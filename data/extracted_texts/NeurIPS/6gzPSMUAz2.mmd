# MATES#: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models

Zichun Yu Spandan Das Chenyan Xiong

School of Computer Science

Carnegie Mellon University

{zichunyu, spandand, cx}@andrew.cmu.edu

###### Abstract

Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce _model-aware data selection with data influence models (MATES)_, where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we collect oracle data influence by locally probing the pretraining model and fine-tune a small data influence model to approximate it accurately. The data influence model then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. Experiments of pretraining 410M and 1B models on the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks. It doubles the gains achieved by the state-of-the-art data selection approach that leverages larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analyses validate the effectiveness of the locally probed oracle data influence and the approximation with data influence models. Our code is open-sourced at https://github.com/cxcscmu/MATES.

## 1 Introduction

The power of large language models (LLMs) rises with scaling up [7; 23; 58]: pretraining models with more _parameters_ on more _data_ using more _compute_ resources [23; 27]. Among these three aspects of scaling, compute is often the most restrictive factor, as current large-scale pretraining frequently demands millions of GPU hours [2; 8; 58], while the model parameters and the pretraining data amounts are determined based on the pre-allocated compute budget [23; 27].

This provides a unique opportunity to elevate the scaling law of pretraining through data selection since the available data sources, such as the web [42; 55], are orders of magnitude bigger than available compute resources and contain data of varying quality. Recent research has shown that effective data selection can improve the generalization ability of pretrained models [15; 62], enhance scaling efficiency , and introduce specialized capabilities . These explorations mainly focus on heuristic-based methods, such as rule-based filtering [49; 50; 55], deduplication [1; 45; 57], proximity to high-quality corpora [17; 33; 61; 64], and prompting LLMs [52; 62]. Despite their success on certain datasets and models, these techniques rely heavily on the static heuristics and often overlook the evolving nature of pretraining . As a result, their performance tends to be limited when applied to the real-world pretraining scenarios. [3; 33].

The data preferences of pretraining models dynamically shift as they progress through different stages of pretraining [4; 11; 39; 40]. For instance, Figure 0(a) illustrates how the data influence measured by the pretraining model evolves at different pretraining steps. As a result, the data quality measurement should also keep pace with the model's evolving data preferences during the pretraining. This leads us to our core research question: _How can we precisely track the data influence with the pretraining model and efficiently select pretraining data based on the acquired influence?_

In this paper, we introduce **M**odel-**A**ware data selection with da**Ta** influence**E** models (MATES), a new pretraining paradigm where pretraining data is selected on-the-fly by a data influence model capturing the ever-changing data preferences of the pretraining model. To track the model's data preferences, we locally probe the oracle data influence by evaluating the pretraining model's performance on a reference task after training on individual data points. Then, we train a small data influence model using the locally probed oracle data influence, which then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. This side-by-side learning framework ensures that the data influence model continuously adapts to the evolving data preferences of the pretraining model, providing the most valuable data accordingly.

Our pretraining experiments with 410M and 1B models on the C4 dataset  demonstrate that MATES can significantly outperform random selection by an average zero-shot accuracy of 1.3% (410M) and 1.1% (1B) across various downstream tasks, ranging from reading comprehension, commonsense reasoning, and question answering. MATES doubles the gains obtained by state-of-the-art data selection approaches that rely on signals from reference models larger than the pretraining model. Furthermore, our model-aware data selection significantly elevates the scaling curve of pretraining models, as shown in Figure 0(b), reducing the total FLOPs required to achieve certain downstream performances by more than half. Further analyses confirm the advantages of our locally probed oracle data influence and the effective approximation of this oracle with data influence models. Ablation studies demonstrate the robustness of MATES across various hyperparameter settings and different design choices of the data influence models.

We summarize our main contributions as follows:

1. We propose a model-aware data selection framework, MATES, where a small data influence model continuously adapts to the constantly changing data preferences of the pretraining model and selects the training data to optimize the efficacy of the pretraining process.
2. We effectively collect oracle data influence through local probing with the pretraining model and use a small BERT-base model to approximate it accurately.
3. We empirically verify the superiority of MATES over rule-based, influence-function-based, and LLM-based selection methods and the effectiveness of the probed oracle and its approximation.

## 2 Related work

Early approaches on data selection relied heavily on manual intuitions. For example, T5  first proposed the C4 pipeline, followed by Gopher rules , which utilized criteria like document length, mean word length, and the presence of harmful or stop words to curate data. Recent FineWeb dataset  further applied quality and repetition filters on top of these basic rules. These rule-based data selection methods have been shown effective as an initial data curation step , though manual intuitions may not capture the nuances of models' data preferences .

Figure 1: Correlation of locally probed data influences at different pretraining steps (a) and the zero-shot performance with model-aware data selection (b). The experiments are based on 1B models.

Deduplication is another standard approach in pretraining data selection. Specifically, Lee et al.  and Penedo et al.  explored exact string match and fuzzy MinHash to filter out duplicate sequences. SemDeDup  further leveraged pretrained embeddings to identify semantic duplicates, and D4  introduced diversification factors into the deduplication process. These methods effectively narrow down the number of similar documents in a corpus and are often used together with quality-oriented selection techniques to improve performance.

Selecting data that is proximate to high-quality corpora can enhance the data quality as well [17; 61]. Techniques leveraging n-gram similarity [17; 64; 33] and language modeling perplexity [8; 14; 16; 61] have been adopted to evaluate how closely sequences in large datasets approximate high-quality data. As the size of pretraining data grows, the effectiveness of proximity-based methods becomes unclear, as they potentially reduce the diversity of the pretraining data and, consequently, the general capability of the pretrained models .

Recent advancements explore the use of LLMs to improve the pretraining data quality. For instance, QuRating  and Ask-LLM  employed LLMs like GPT-3.5 to annotate high-quality documents. Maini et al.  rephrased web corpora by providing LLMs with detailed prompts to balance quality and diversity. These methods leverage the capabilities of strong reference LLMs, which are often several orders of magnitude larger, to guide the pretraining of smaller models.

Influence functions [29; 59] provide a theoretical tool to assess the impact of individual data points on a model's performance. However, they face scalability challenges in the context of LLMs due to the expensive gradient calculations [21; 54]. To efficiently and robustly approximate influence functions, TRAK  performed Taylor approximation and gradient dimension reduction, making influence computation feasible for pretraining experiments . Nevertheless, the computational cost remains prohibitive for model-aware data selection, which requires tracking the evolving data preferences of pretraining models on the fly.

On the other hand, many researchers have proposed curriculum learning strategies that dynamically adjust the data distribution in the pretraining [11; 24; 47; 51; 56; 62]. ELECTRA-style models  incorporated curriculum learning in their pretraining process by synchronously training the model with an auxiliary generator, which provided more and more difficult training signals for the discriminator. This implicit curriculum significantly improved the pretraining efficiency on denoising language models [4; 11; 19; 39; 65]. Other methods have explicitly designed the curriculum for pretraining data selection, such as decreasing gradient norms , least certainty [24; 51], and increasing expertise , demonstrating the benefits of adapting the pretraining data signal according to the model's ever-changing preferences.

## 3 Methods

This section first introduces the model-aware data selection framework MATES (SS 3.1) and then proposes a local probing technique to collect oracle data influence during pretraining (SS 3.2).

Figure 2: Overview of MATES. The language model is first pretrained with a random set of data. Then, a data influence model is trained to approximate data influences on the target performance of the pretraining model and select the most effective data for the next pretraining stage.

### Model-aware data selection framework

MATES selects the most effective data for pretraining a language model, aiming to maximize its final target performance, as illustrated in Figure 2. The target performance here can be evaluated using any downstream task or their combinations. Specifically, we leverage non-evaluation data as a reference for the model's target performance and select the pretraining data according to the reference loss.

Formally, given a size-\(n\) pretraining dataset \(_{t}\) and the current model state \(\), in each iteration, the objective of data selection is to find an optimal batch \(B^{*}\) from \(_{t}\) to minimize the loss \(\) over the reference data \(_{r}\) after training \(\) on \(B^{*}\):

\[B^{*}=_{B}(_{r}( ,B))\] (1) \[(_{r}(,B ))=_{(x,y)_{r}}(y x;(,B )),\] (2) \[(,B^{*}),\] (3)

where \((,B)\) denotes the optimization of model \(\) on a batch \(B\), e.g., one-step training with Adam  and \(\) denotes the function to compute the model loss on an input-output pair \((x,y)\).

There are two challenges to implement this framework. First, enumerating all possible batches will exponentially increase computational complexity. Second, obtaining oracle data influence for all pretraining data points is challenging. To address these issues, we introduce two techniques: pointwise data influence and data influence parameterization.

**Pointwise Data Influence.** To avoid the computationally intensive task of enumerating all possible batches, a more practical workaround is to decompose the group influence into the pointwise influence [15; 44]. Following previous research , we aggregate all the data influences by the summation, assuming that each data point \(x_{i}\) has an independent influence irrespective of the others:

\[(_{r}(,B))=_{x_{i} B} _{}(x_{i};_{r}),\] (4)

where \(_{}\) is the oracle pointwise data influence function based on model state \(\). \(_{}\) continuously changes along with the model pretraining.

**Data Influence Parameterization.** Estimating oracle pointwise data influence normally involves gradient-based calculation [21; 29; 44], which is impractical to perform over millions of pretraining examples for every pretraining model state. To make the data influence collection feasible, we propose to collect the oracle data influence on a small hold-out dataset \(_{h}\) (sampled from the same distribution as \(_{t}\)) and fine-tune a small _data influence model_\(\) on \(\{(x_{i},_{}(x_{i};_{r})) x_{i} _{h}\}\) to approximate the oracle. This data influence parameterization process transfers the costly influence computation to the small data influence model's inference.

Then, we obtain the influence prediction \((x_{i})\) with the fine-tuned data influence model over all the training examples \(x_{i}_{t}\). For better efficiency, we only asynchronously update the data influence model every \(U\) steps with the new oracle \(_{}\). Therefore, one data influence model checkpoint can select the entire data subset \(S_{k}^{*}\) for the next \(U\) steps of pretraining. The selection process uses the Gumbel-Top-\(k\) algorithm [30; 62] to sample data from \(_{t}\), with influence scores as weights:

\[S_{k}^{*}\{k()}{}) x_{i} _{t}\},\] (5)

where \(\) is the sampling temperature. The update step \(U\) is chosen to balance the data selection efficiency with the evolving data influence. Empirically, we warm up the model with a randomly sampled size-\(k\) subset from \(_{t}\) in the initial \(U\) steps. The overall pretraining and data selection pipeline of MATES is illustrated in Algorithm 1.

### Locally probed oracle data influence

The rest of this section presents our method to estimate the oracle data influence \(_{}\). We start the derivation from the standard influence functions [29; 59] that quantify the reference loss change if one data point \(x_{i}\) in the training data is unweighted by a small \(\). We denote the optimal model state after the upweighting as \(_{,x_{i}}^{*}=_{}_{j=1}^ {n}(x_{j})+(x_{i})\) and simplify the optimal model under \(=0\) case (i.e., no upweighting) as \(^{*}\). Then, the oracle data influence of upweighting \(x_{i}\) is given by:

\[_{^{*}}(x_{i};_{r}) }}{{=}}.(_{r}_{,x_{i}}^{*})}{d} |_{=0}\] (6) \[=_{}(_{r}^ {*})^{}._{,x_{i}}^{*}}{d}|_{ =0}\] (7) \[=-_{}(_{r}^ {*})^{}H_{^{*}}^{-1}_{}(x_{i} ^{*}),\] (8)

where \(H_{^{*}}=_{j=1}^{n}_{}^{2}(x_{j}^{*})\) is the Hessian and is positive definite. The derivation from Eq. 7 to Eq. 8 is given by building a quadratic approximation to the empirical risk around \(^{*}\) and taking a Newton step . Now consider the case that we incorporate \(x_{i}\) into the training data, which means \(=\), then the parameter change due to the inclusion of \(x_{i}\) is \(_{,x_{i}}^{*}-^{*}-H_{ ^{*}}^{-1}_{}(x_{i}^{*})\) and the influence in Eq. 8 can be further represented as:

\[_{^{*}}(x_{i};_{r})  n_{}(_{r} ^{*})^{}(_{,x_{i}}^{*}-^{*})\] (9) \[ n((_{r}_{,x_{i}}^{*})-(_{r}^{*}))\] (10) \[-(_{r}^{*})+(_{r}_{,x_{i}}^{*}).\] (11)

In practice, we manage to obtain the data influence based on the current model state \(\), while the above influence calculation still remains meaningful in the non-converged state by adding a damping term \(\) that ensures \(H_{}+ I\) is positive definite . Under this assumption, the first term in Eq. 11 can be regarded as a fixed value whatever \(x_{i}\) is, since \(x_{i}\) is sampled from the hold-out data \(_{h}\). The second term in Eq. 11 can be locally probed with the one-step training of the current model with the new \(x_{i}\), i.e., \((,x_{i}))\). This one-step training incorporates \(x_{i}\) into the optimization of the current model. Finally, the influence of \(x_{i}\) on the reference loss is:

\[_{}(x_{i};_{r})-( _{r})+(_{r}(,x _{i})).\] (12)

This formula means, for each \(x_{i}\) in the hold-out data \(_{h}\), we run one-step training with the current model \(\) and evaluate the difference in reference loss before and after one-step training. To ensure that a positive influence score reflects a beneficial impact on model performance, we empirically define the negative influence, \((_{r})-(_{r} (,x_{i}))\), as our locally probed oracle data influence of \(x_{i}\). A full derivation of locally probed oracle data influence can be found in Appendix A.

[MISSING_PAGE_FAIL:6]

## 5 Evaluation results

This section evaluates the effectiveness of MATES (SS 5.1), locally probed oracle data influence (SS 5.2), and data influence model (SS 5.3). It further presents a case study to illustrate the model's changing data preferences (SS 5.4). More ablation studies can be found in Appendix C.

### Overall performance

**MATES outperforms the state-of-the-art data selection approach.** Table 1 presents the zero-shot evaluation of pretraining 410M/1B models with different data selection methods. MATES significantly outperforms random selection by an average downstream accuracy gain of 1.3% and 1.1% in the 410M and 1B settings, respectively. These gains are nearly double those achieved by the state-of-the-art data selection method, QuRating, which depends on larger reference models. Notably, we observe an absolute improvement of nearly 2.0% across most tasks in the 410M setting, except for ARC-C and LogiQA, where the predictions of 410M models are close to random guessing. In the 1B setting, MATES outperforms random selection across all 9 tasks, demonstrating the strong scalability and generalization capabilities of our method.

**MATES selects the data with low costs.** We also show a detailed breakdown of the pretraining cost of MATES in Table 2. The relative selection cost of larger models is generally smaller since their pretraining cost dominates the total FLOPs while the training and inference cost of our data influence model remains stable. Even with the 1B pretraining model, the wall clock time to collect one oracle data influence is only around 2.5 seconds on one GPU, which means we can get all the required 160k oracle scores during 50k-step pretraining on one node (8 GPUs) around 14 hours, which is significantly lower than the actual pretraining time (4 days). The inference speed of our data influence model can also be improved with data parallelism or a fast-inference framework like vLLM , reducing the selection cost further. This breakdown analysis underscores the low expense of MATES in achieving model-aware data selection.

**MATES significantly elevates the scaling curves.** Figure 3 plots the performance of the pretraining models w.r.t. different FLOPs and steps. Measuring by FLOPs counts both the model pretraining and data selection costs, demonstrating the total compute expense during pretraining. Measuring by steps reflects the compute cost of the model pretraining alone, as the data selection can be trivially parallelized when more computational resources are available. At both 410M and 1B scales, MATES

  
**Process** & **\#FLOPs \(\)1e19** & **Ratio** \\ 
**410M Setting**: 410M model, 25B tokens & & \\  Model pretraining & 6.35 & 78.3\% \\ Oracle data influence collection & 0.29 & 3.6\% \\ Data influence model training & 0.01 & 0.1\% \\ Data influence model inference & 1.46 & 18.0\% \\
**Total** & **8.11** & **100\%** \\ 
**1B Setting**: 1B model, 25B tokens & & \\  Model pretraining & 17.67 & 88.5\% \\ Oracle data influence collection & 0.83 & 4.1\% \\ Data influence model training & 0.01 & 0.1\% \\ Data influence model inference & 1.46 & 7.3\% \\
**Total** & **19.97** & **100\%** \\   

Table 2: FLOPs breakdown of MATES steps.

Figure 3: Downstream performance of 410M and 1B models w.r.t. pretraining FLOPs and steps. The data selection procedure of MATES only accounts for 21.7% and 11.5% of the total FLOPs for 410M and 1B models, respectively.

significantly elevates the scaling curves compared to random selection, reducing the FLOPs and pretraining steps required to reach a certain downstream performance by more than half. Scaling efficiency is more evident at the 1B scale, where model pretraining dominates 88.5% of FLOPs versus 11.5% for data selection. As a result, MATES reaches the same performance as random selection with only 43.3% of the total FLOPs. A similar observation can be found in Figure 9, where the performance of MATES is comparable to or higher than the full pretraining using 3x data. These results reveal the promising potential of MATES in elevating the scaling law of foundation models.

### Effectiveness of locally probed oracle data influence

This set of experiments analyzes the effectiveness of oracle data influence with different reference tasks. Besides LAMBADA used in our main experiment, we also consider taking the training sets of ARC-E  and FLAN  as the reference tasks to show the generalization ability of our method. ARC-E represents one of the knowledge-based question-answering tasks, while FLAN represents a large set of varied instruction-formatted data. For ARC-E, we construct each example either as a multiple-choice selection (i.e., outputting the correct option [A-D]) or as a language modeling task (i.e., outputting the verbalized answer) to investigate whether the task format will affect the data influence collection and parameterization.

In Figure 4, we demonstrate the oracle data influence distribution with different reference tasks, the standard deviation of the distribution, and the proportions of the data with positive/negative oracle data influence. The oracle distribution remains spread-out across all reference tasks, indicating that our oracle effectively differentiates data influences. Notably, the positive influence proportion for LAMBADA in Figure 3(a) is higher than others by more than 20%, suggesting that more data is deemed beneficial when LAMBADA serves as the reference. We hypothesize that this is because LAMBADA is essentially a word prediction task, which aligns more closely with the pretraining objective compared to knowledge-based or instruction-following tasks. This hypothesis is further supported by the results for ARC-E (Figure 3(b)/3(c)), where the language modeling format identifies more positively influential data points compared to the multiple-choice format.

We further validate the effectiveness of the oracle across different reference tasks by sampling 20% data with oracle scores as weights. Due to the infeasibility of obtaining oracle scores for all the pretraining data, we only run one short decay stage with the data selected by different oracle scores.

Figure 4: Oracle data influence distribution in the 410M setting with different reference tasks at 50k steps. MC: multiple choice. LM: language modeling. We also present the standard deviation of the distribution and the proportions of the data with positive/negative oracle data influence.

   \(_{r}\) & **SetQ** & **ARC-E** & **ARC-C** & **LogQJA** & **OBQA** & **BoolQ** & **HelloSwag** & **PIQA** & **WinoGrande** & **Average** \\  LAMBADA & 66.0(1.1) & 42.2(1.0) & 24.8(1.3) & 27.2(1.2) & 30.8(2.1) & **59.1(0.9)** & **41.9(0.5)** & **68.5(1.1)** & 52.3(1.4) & 45.9(1.4) \\ ARC-E (MC) & 64.9(1.5) & 42.4(1.0) & 24.9(1.3) & 27.8(1.8) & 30.4(2.1) & 58.0(1.9) & 41.1(0.5) & 68.1(1.1) & 51.7(1.0) & 45.5(1.4) \\ ARC-E (LM) & 65.3(1.5) & 43.0(1.0) & 24.8(1.3) & 28.0(1.8) & 31.8(2.1) & 58.5(1.9) & 40.7(0.5) & 67.2(1.1) & **52.5(1.4)** & 45.8(1.4) \\ FLAN & **66.4(1.5)** & **45.1(1.0)** & **25.1(1.3)** & **28.7(1.8)** & **32.0(1.1)** & 56.2(0.9) & 40.5(0.5) & 67.9(1.1) & 52.3(1.4) & **46.0(1.4)** \\   

Table 3: Performances of oracle selected data with different reference tasks in the 410M setting. We run the decay stage starting from the MATES model at 50k steps.

As shown in Table 3, taking ARC-E as the reference task can benefit the model's in-domain accuracy, but its generalization performance is worse than using LAMBADA. In contrast, FLAN benefits a wider range of downstream tasks due to its diverse instructions. However, there remains a trade-off between the performance of different tasks, so the average accuracy of choosing FLAN is similar to that of LAMBADA. This experiment highlights the robustness of our oracle, as it is not limited to specific reference data but generalizes effectively across multiple reference tasks.

### Effectiveness of data influence model

This experiment studies the effectiveness of data influence model. As shown in Figure 4, our data influence model effectively approximates the oracle data influence across various reference tasks. Nevertheless, a higher standard deviation in the oracle distribution tends to enhance the validation Spearman correlation. This suggests that greater variability in influence scores can provide more diverse signals for the data influence model to capture, making it better approximate the oracle distribution.

Figure 5 compares MATES with static data influence models trained on influence from a 10k or a 50k random-pretrained model checkpoint. In Figure 4(a), we measure the validation Spearman correlation between the predictions of data influence models and the oracle data influence probed with each pretraining checkpoint. The correlations of static data influence models are always below 0.5, while data influence models in MATES, with dynamic updates, can capture the ever-changing data preferences more and more precisely along with the model pretraining (e.g., the correlation is around 0.7 in 40k steps). The effects of model-aware data selection are directly reflected in downstream accuracy. In Figure 4(b), the data selected by static data influence models will cause a notable performance drop, especially at the early pretraining stage. These observations confirm the ever-changing nature of data preferences in pretraining and the advantages of model-aware data selection to elevate the scaling curves of pretraining.

### Case study

This case study demonstrates representative examples to illustrate the evolving preferences of the pretraining model in detail. As shown in Table 4, the model at the early pretraining stage (the 10k checkpoint) tends to favor learning natural narrative examples without delving too deeply into specific knowledge. At the 20k checkpoint, it appears to shift focus toward factual knowledge (e.g., pages from Wikipedia) while gradually reducing reliance on natural narratives. At the 30k checkpoint, the model shows a preference for more detailed academic text, such as official teaching slides. By the 40k checkpoint, the model may begin learning more long-tail knowledge, like Telescopic Forklift Training. These examples provide insights into the evolving nature of the model's data preferences throughout pretraining. Although they may not cover every aspect of the selected data, our observation highlights the necessity to adapt data selection strategies to different model learning stages.

## 6 Discussion and limitations

**Combinational Measurement of Data Influence.** One primary assumption in our work is that each data point contributes independently to the pretraining outcome, without accounting for its interactions with other data points. Despite a common hypothesis [15; 44], the pretraining essentially applies the long-term combinational effect of batched data on language models, and the learning of many advanced capabilities is accumulative. Efficiently and effectively measuring and learning the combinational and accumulative nature of the pretraining process can make us better understand and leverage the value of data .

Figure 5: Static (based on a 10k or a 50k random-pretrained model checkpoint) data selection versus model-aware data selection in influence modeling and downstream accuracy.

**Exploratory Scale.** As an exploratory research work, our experiments are conducted at a moderate scale, with a pretraining model of 410M or 1B parameters. Although the trend from 410M to 1B indicates the robustness of our observations, it remains unclear how well our methods scale up to production-level models with billions of parameters and trillions of pretraining tokens. On the one hand, moving to that scale provides more headroom for data selection with more urgent needs for efficiency, more leniency on the relatively small compute spent on data selection, and a larger pool of candidate data. On the other hand, large-scale pretraining may yield various stability issues that require dedicated work to introduce new techniques [58; 67]. We leave the exploration of larger models to future work.

## 7 Conclusion

In this paper, we introduce MATES, a novel framework to enhance the efficiency and effectiveness of language model pretraining through model-aware data selection. MATES leverages a data influence model to continuously capture the evolving data preferences of the pretraining model throughout the pretraining process, thereby selecting the training data most effective for the current pretraining stage. To achieve that, we locally probe the oracle data influence on a reference task using the pretraining model and fit the data influence model on the probed oracle. Our empirical results demonstrate that MATES surpasses random, rule-based, influence-function-based, and LLM-based data selection methods on pretraining, significantly elevating the scaling curves of pretraining LLMs. Further analyses confirm the effectiveness of our locally probed oracle data influence and the accurate approximation of this oracle with data influence models. Our work successfully demonstrates the potential of model-aware data curation in pretraining, and we hope it will motivate further explorations on improving the scaling law of foundation models through better data curation techniques.