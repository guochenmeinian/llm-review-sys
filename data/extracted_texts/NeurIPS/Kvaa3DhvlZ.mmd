# Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation

Zihao Yue, Anwen Hu, Liang Zhang, Qin Jin

Renmin University of China

{yzihao, anwenhu, zhangliang@0, qjin}@ruc.edu.cn

https://github.com/yuezih/SMILE

###### Abstract

Image captioning aims to describe visual content in natural language. As 'a picture is worth a thousand words', there could be various correct descriptions for an image. However, with maximum likelihood estimation as the training objective, the captioning model is penalized whenever its prediction mismatches with the label. For instance, when the model predicts a word expressing richer semantics than the label, it will be penalized and optimized to prefer more concise expressions, referred to as _conciseness optimization_. In contrast, predictions that are more concise than labels lead to _richness optimization_. Such conflicting optimization directions could eventually result in the model generating general descriptions. In this work, we introduce **S**emipermeable **M**ax**I**mum **L**ikelihood **E**stimation (SMILE), which allows richness optimization while blocking conciseness optimization, thus encouraging the model to generate longer captions with more details. Extensive experiments on two mainstream image captioning datasets MSCOCO and Flickr30K demonstrate that SMILE significantly enhances the descriptiveness of generated captions. We further provide in-depth investigations to facilitate a better understanding of how SMILE works.

Figure 1: Descriptive captions generated by our SMILE-optimized captioning model, compared to human annotations and descriptions generated by the MLE-optimized captioning model.

## 1 Introduction

The task of generating textual descriptions for a given image, commonly referred to as image captioning , has long been plagued by the issue of overly-generic outputs . That is, models tend to generate similar descriptions for distinct images using simple concepts, but lack details (as shown in Fig. 1). Such generated captions cannot meet various needs, e.g., fine-grained online image search and recommendation, automatic data annotation for training vision-language models , and poses a long-standing challenge in the field of image captioning. Considerable efforts have been devoted to generating more descriptive image captions, including constructing paragraph caption datasets with more details for training models , designing additional rewards , and prompting  vision-language pre-training (VLP) models  to enrich captions . While these efforts yield some promising improvements, they seldom try addressing this problem by revisiting the core optimization objective.

Similar to text generation tasks such as machine translation , image captioning models are trained to predict the next word given context (the visual content and preceding words). The training is supervised by the maximum likelihood estimation (MLE) objective, which evaluates the model's predictive distribution over the vocabulary, and penalizes the model when it fails to predict the current label. However, unlike machine translation where the target sequence is relatively certain, describing an image can be quite diverse, and limited human annotations may not cover all accurate captions. Therefore, the strict supervision with MLE is not perfectly suitable for image captioning optimization. As shown in Fig. 2, when the model predicts a word (e.g., "woman") that is more 'concise' than the ground truth label (e.g., "pretty"), MLE computes a loss due to the mismatch and encourages the model to assign a higher probability to "pretty". From the model optimization perspective, this optimizes the model to possess the more descriptive captioning ability, a process referred to as _richness optimization_ in this work. In contrast, if the model predicts a word (e.g., "white") expressing more details than the label (e.g., "cake"), MLE also penalizes the model and optimizes it towards a more concise captioning behavior (_conciseness optimization_). Since ground truth captions in commonly used datasets (e.g., MSCOCO ) are mostly simple (around 10 words), conciseness optimization is not rare during training, especially for finetuning VLP models . But for image captioning, descriptions that contain more details rather than just generic concepts are usually preferred so that they can be used for accurately retrieving images or conveying more information for the visually impaired, etc. Therefore, conciseness optimization should be suppressed as much as possible, and richness optimization should be encouraged to incentivize the model to generate more descriptive captions.

In this work, to achieve this goal, we propose a simple but effective training objective for descriptive image captioning, namely Semipermeable MaxImum Likelihood Estimation (SMILE). Unlike MLE, which evaluates the predictive probability distribution over the entire vocabulary, SMILE considers the probability over a vocabulary subset with limited words, i.e., only containing **words in the ground truth caption**. Under typical conditions (with MLE), when a 'richer' word that conveys additional details gets a high confidence score (e.g., "white" in Fig. 2, and such details generally are not

Figure 2: (a) Examples of richness optimization and conciseness optimization. When the label is ”pretty” while the model predicts ”woman” with less rich semantics, it leads to richness optimization; when the label is ”cake” while the model predicts ”white”, conciseness optimization occurs. (b) Different from MLE, SMILE presents a ‘semi-permeability’ that accepts richness optimization while blocking conciseness optimization. Best viewed in color.

included in the ground truth caption), it challenges the label word (e.g., "cake") in the competition of probability allocation. This competition consequently leads to a decrease in the probability assigned to "cake", thereby imposing a penalty on the model and thus resulting in conciseness optimization. However, with SMILE, probability allocation is limited to a subset of words that excludes "white". Regardless of the confidence score given to "white", it does not diminish the probability assigned to the label "cake", thus avoiding a loss increase. In this way, we suppress the conciseness optimization that would have been imposed with MLE. In contrast, when the label word expresses details (e.g., "pretty" in Fig. 2) while the model prefers a more 'concise' word "woman", it is highly likely that the more 'concise' word is part of the ground truth caption and therefore, included in the subset. Eventually, even if we only focus on the subset, the word still takes away the probability from the label word "pretty", resulting in model penalization. In this way, richness optimization is maintained. Our objective allows richness optimization but blocks conciseness optimization, similar to a semipermeable membrane. We, therefore, name this objective Semipermeable MLE (SMILE).

To verify the effectiveness of our proposed SMILE, we conduct extensive experiments on the image captioning task. The results demonstrate that SMILE can effectively optimize the model to generate significantly longer and more descriptive captions. Besides, we carry out abundant experiments to analyze how SMILE works, such as ablations on subset selection and descriptiveness origin, etc. Finally, we verify the generalization ability of SMILE on the video captioning task and discuss its limitation when facing other text generation tasks.

## 2 Method

### Maximum Likelihood Estimation (MLE)

We first introduce the standard MLE for captioning optimization. As with many text generation tasks [40; 1], the model is trained to maximize the likelihood of the label when predicting the current word \(w\) in the sequence given previous words \(w_{<}\) and visual content \(v\). The token-level loss function of MLE is defined as:

\[_{}=-_{j}^{||}y_{j}^{}(w|w_{<},v;),\] (1)

where the summation iterates over all words in the vocabulary \(\), \(y_{j}\) is the \(j\)-th element of the one-hot label vector, and \(^{}\) is the predictive probability distribution over \(\).

### Semipermeable MaxImum Likelihood Estimation (SMILE)

Given a target sequence \(D=[w_{1},w_{2},,w_{N}]\), we form a subset \(_{D}\) that only includes the unique words occurred in \(D\), defined as \(_{D}=\{w_{i} w_{i} D\}\). Then, the current word \(w\) prediction is carried out within \(_{D}\), with the loss function of SMILE defined as:

\[_{}=-_{j}^{|_{D}|}y_{j}^{ _{D}}(w|w_{<},v;).\] (2)

Here, \(^{_{D}}\) is the predictive distribution over the subset \(_{D}\). Specifically, for the \(j\)-th word in \(_{D}\), the probability \(_{j}\) is calculated by the relative significance of the confidence score of the word compared to all other words in \(_{D}\):

\[_{j}=(_{j})=_{j})}{ _{k_{D}}(_{k})}.\] (3)

The probability assigned to the label (the \(j^{*}\)-th word) determines the prediction loss. When a word \(w^{+}\) expressing additional details beyond the ground truth caption gets a high confidence score (e.g., in Fig. 2, the label is "cake", \(w^{+}\) is "white", and \(_{D}=\{\)"a", "pretty", "woman", "holding", "cake"\(\}\), \(w^{+}_{D}\)), \(w^{+}\) does not contribute to the denominator in Eq. (3) since it is not included in \(_{D}\), and thus does not induce a decrease of the probability assigned to the label word \(_{j^{*}}\), which would have otherwise increased the prediction loss. In other words, when the model tends to predict wordslike \(w^{+}\), these words do not participate in the probability allocation and do not take away any of the probability to be assigned to the label word, thus avoiding model penalization which leads to conciseness optimization. In contrast, when the model predicts a more 'concise' word \(w^{-}\) (e.g., the label is "\(\)" and \(w^{-}\) is "\(\)"), such word \(w^{-}\) usually is included in the ground truth caption, i.e., \(w^{-}_{D}\). When allocating probability over \(_{D}\), \(w^{-}\) also participates and takes away some of the probabilities. Therefore, the model penalization is not avoided, and the induced richness optimization is maintained. Blocking conciseness optimization while allowing richness optimization is what SMILE is all about.

SMILE is for further training.Since SMILE only considers the relative probability distribution within a subset, it does not penalize the model to generate detailed words beyond the subset, thus suppressing conciseness optimization. However, it makes sense only if the predicted detail is correct. Therefore, SMILE is applied to further train a model that has already been optimized with MLE, which ensures its fundamental captioning capability. Please note that SMILE is only adopted in the training phase. During inference, the model predicts across the entire vocabulary to determine the current generation.

Initial context restrictionIt is well known that image captioning models are easy to suffer from exposure bias [2; 10; 36; 61]. When training in a teacher-forcing manner, the model is exposed to the ground truth context; however, during inference, each generation step is conditioned on the previous predictions. This causes a gap between training and inference. As SMILE does not require the model prediction to be consistent with the label as MLE does, the prediction is more prone to deviate from the label. Specifically, generating from the first token of the sequence, a prediction that is inconsistent with the label could lead to a completely unfamiliar context, exacerbating the exposure bias and affecting the autoregressive generation. Fortunately, we find that this issue can be effectively mitigated through some simple strategies, by ensuring that the initial context is correct, i.e., consistent with the label, which we name as _initial context restriction_ and further discuss in Section 3.3.

## 3 Experiments

### Experimental Setup

Basic model and baselinesOur proposed SMILE is architecture-agnostic and can be applied to any visual captioning model compatible with MLE optimization. As a representative, we validate SMILE on the base version of BLIP , one of the state-of-the-art vision and language models pre-trained with 129M images and paired captions. We first fine-tune BLIP on downstream datasets using MLE as the basic model, and then further optimize it with SMILE. For baselines, we compare our method with the latest descriptive image captioning solution, CapEnrich , and another two recent models NliCap  and GdisCap . We also provide the performance of human-annotated ground truth captions for reference.

DatasetWe evaluate our method on the two most commonly used image captioning benchmarks, MSCOCO  and Flickr30K . MSCOCO contains about 120K images, and we adopt the commonly used Karpathy splitting  with 5,000 images each for the validation and test sets. Flickr30K contains about 31K images, with 1,000 images each for the test and validation sets. For both datasets, each image has five human-annotated captions.

EvaluationWe evaluate the models from three aspects: descriptiveness, accuracy, and fluency. Descritiveness refers to how detailed the caption describes the image. Following previous works , we evaluate descriptiveness by the performance of CLIP self-retrieval, which employs the CLIP model  as a retriever to recall the image with its caption from a candidates pool. This is based on the fact that captions with more details can better distinguish different images. The candidates pool is the hard retrieval pool constructed in the CapEnrich work , which additionally includes more similar images beyond the test set, placing a higher requirement on the descriptiveness of captions. We also report the average length of the generated captions and the lexical diversity (unique word count of all captions). Accuracy refers to the relevance of the generated caption to the image visual content, which we measure automatically by CLIPScore . It calculates the semantic similarity between the image and the generated caption using a pre-trained CLIP model. For fluency, we report the language modeling perplexity2 (PPL) of the captions with GPT-2 . However, these automatic metrics have their limitations. For example, CLIP measures the overall semantics of the caption and struggles to focus on too many details. In addition, the evaluation models also suffer from the bias of the training data and thus may have difficulty in handling longer or more complex sentences. Therefore, we also conduct human evaluations, which will be discussed later.

### Main Results

Versus basic modelAccording to Table 1, compared with the basic BLIP model, SMILE significantly increases the average length of the captions (more than doubled on both datasets). SMILE leads to more detailed descriptions as well, greatly enhancing the model's self-retrieval performance. Although SMILE causes a slight decrease in the CLIPScore (-2.2 and -0.6 on MSCOCO and Flickr respectively), we consider this to be a reasonable phenomenon, as 'talks much errs much'. Besides, maintaining a comparable level of perplexity indicates that SMILE can incentivize longer and more detailed captions without compromising fluency.

Versus baselinesAs shown in Table 1, BLIP significantly outperforms all baselines on lexical diversity with SMILE optimization. Compared to traditional captioning methods that tend to generate dull texts with common words overused, SMILE effectively overcomes the lexical diversity bottleneck and outperforms humans. Our approach also achieves the best performance on self-retrieval. It is worth noting that the most competitive baseline CapEnrich requires automatically constructed data in a specific format, which integrates details from multiple manual captions of each image for training, along with carefully handcrafted or learnable prompts. In contrast, our approach is refreshingly simple, has fewer restrictions, and importantly, exhibits no incompatibility with other methods. Therefore, it can serve as an effective supplement or alternative to the currently employed solutions. Since our vanilla BLIP model with SMILE optimization already surpasses other methods that may require complicated design, we see no strong need for further attempts to combine SMILE with them in this work.

Human evaluationWe randomly sample 100 images from the MSCOCO test set for human evaluation. For each image, we collect 4 candidate captions, including three captions generated by CapEnrich, basic model BLIP, and SMILE-optimized BLIP, respectively, and one human-annotated caption. All candidates are randomly shuffled. For each candidate caption, 5 human annotators are asked to independently rate on a scale of 1 to 5 (a higher score indicating better quality) from three aspects, namely descriptiveness, accuracy, and fluency. The 'descritiveness' refers to the richness of the _correct_ detail relevant to the image. Table 2 shows the average score of each model on the three aspects. SMILE substantially improves the descriptiveness of the captions, with a rate close to 5 (_excellent_), outperforming all other candidates including human-written captions by a large margin. A score beyond 4 for both accuracy and fluency demonstrates the good quality of captions

    &  &  &  &  &  &  \\  & & & & & & R@1 & R@5 & \\   & NilCap & 9.5 & 0.8 & 2.9 & 9.3 & 75.5 & 67.8 \\  & GdisCap & 9.5 & 1.0 & 3.5 & 10.8 & 75.8 & 100.2 \\  & CapEnrich & 13.3 & 1.5 & 9.4 & 22.6 & **79.2** & 63.1 \\  & BLIP & 10.0 & 1.4 & 6.7 & 16.6 & 77.2 & 95.8 \\  & BLIP-\(_{}\) & **22.3** & **4.5** & **10.0** & **24.5** & 75.0 & 95.6 \\  & Human & 10.4 & 4.1 & 7.6 & 20.0 & 77.6 & 129.1 \\   & CapEnrich & 15.2 & 1.0 & 29.2 & **54.9** & **81.5** & 67.1 \\  & BLIP & 11.6 & 0.8 & 25.4 & 46.4 & 78.8 & 65.6 \\   & BLIP-\(_{}\) & **23.2** & **2.3** & **31.2** & 53.0 & 78.2 & 98.0 \\    & Human & 12.3 & 2.0 & 26.2 & 48.3 & 79.8 & 121.2 \\   

Table 1: Image captioning performance of different methods on MSCOCO and Flickr30K. For human performance, we randomly select one annotation for each image for comparison with the others.

by our SMILE-optimized captioning model, although it faces a higher risk of 'talks much errs much'. As descriptiveness emphasizes the recall of visual content while accuracy emphasizes description precision, we additionally calculate an F1 value of these two scores as the overall semantic score. As shown in Table 2, SMILE performs best when taking into account both aspects.

### Ablation Study

Training epochsFig. 3 presents the impact of training epochs on caption length and self-retrieval performance with SMILE optimization. It shows that SMILE takes only a few training epochs to achieve convergence and does not introduce too much extra training cost. For all of our models optimized with SMILE, we choose the checkpoints according to the best self-retrieval performance on the validation set, which always occurs within 3 epochs.

Basic modelAs the basic model used for SMILE optimization, BLIP is first pre-trained on a large-scale image-text dataset and then fine-tuned by MLE on downstream datasets. Both the pre-training (PT) and fine-tuning (FT) equip the model with fundamental image captioning capabilities. To investigate how such fundamental captioning ability influences SMILE's effectiveness, we compare the performance of different basic models and their further SMILE-optimized counterparts on MSCOCO. As shown in Table 3, for basic models, both pre-training and fine-tuning improve the model's performance in terms of descriptiveness and accuracy (rows 1, 3, and 5). After subsequent SMILE optimization, each basic model can generate much longer descriptions (rows 2, 4, and 7). Regarding how much SMILE improves descriptiveness, it is notable that when the fundamental captioning ability is poor, SMILE brings no improvement and can further decrease the performance (row 2 vs 1); but once the basic model is able to perform acceptable captioning, SMILE can contributes a lot (rows 4 vs 3 and 7 vs 5). This implies that SMILE requires the fundamental captioning ability of the basic model in order to further correctly generate more details. In addition, as SMILE optimization introduces extra training steps, we further train the default basic model with MLE and validate that simply adding training steps does not bring improvement (row 6 vs 5).

Initial context restrictionAs aforementioned in Section 2.2, we propose initial context restriction to alleviate exposure bias for SMILE optimization. We design two implementations, _First-token MLE_ and _First-token Shifting_, to achieve such restriction:

1. _First-token MLE_ adopts MLE loss for the first token of the target sequence, and SMILE for the remaining tokens.
2. _First-token Shifting_ shifts the label of the first token of the target sequence to its alternative. For example, in the vocabulary of the BERT model , the word "a" can be replaced with its subword form "##a", which rarely appears to be the first token in the training corpus. By doing so, we replace the familiar word with an unfamiliar one, making it difficult for the model to predict correctly (even within a subset). This often results in model penalization, making the prediction of the first token more likely to be consistent with the ground truth.

Since the shifting approach is designed with certain requirements on the model vocabulary, making it potentially difficult to be applied to all models, we use the _First-token MLE_ by default to address the exposure bias issue. To demonstrate the effectiveness of this strategy, in Table 4, we show that without initial context restriction, SMILE optimization can not achieve promising results. Either the default _First-token MLE_ implementation or the _First-token Shifting_ implementation helps alleviate exposure bias and the former is better.

Mixing \(_{}\) with \(_{}\)Since SMILE improves descriptiveness at the expense of some accuracy, it could pose a risk in scenarios that demand higher accuracy. To strike a balance, we carry out experiments where the overall learning objective combines both \(_{}\) and \(_{}\), defined as:

\[_{}=_{}+(1-) _{},.\] (4)

Table 5 illustrates how the balance between MLE and SMILE impacts the performance in terms of descriptiveness and accuracy. As \(_{}\) becomes more dominant, the model tends to generate longer outputs with higher lexical diversity and descriptiveness; however, accuracy correspondingly decreases. This demonstrates that a compromise between descriptiveness and accuracy can be achieved by simply combining \(_{}\) with \(_{}\). Notably, we find that when \(_{}\) is incorporated at a very low ratio (0.01), the generated captions achieve the best performance in self-retrieval. This suggests that while SMILE facilitates more details, ensuring accuracy is also important for captions to better describe and distinguish images.

    Initial Context \\ Restriction \\  } &  Caption \\ Length \\  } &  Lexical \\ Diversity \\  } & Self-Retrieval &  &  \\  & & & R@1 & & & \\  - & 17.0 & 3.0 & 6.8 & 16.2 & 71.9 & 82.0 \\ _shifting_ & 21.3 & 4.0 & 9.6 & 22.4 & **75.0** & 85.2 \\ _default_ & **22.3** & **4.5** & **10.0** & **24.5** & **75.0** & 95.6 \\   

Table 4: Performance of SMILE-optimized models with different initial context restriction strategies.

    \(\) \\ Length \\  } &  Caption \\ Diversity \\  } &  Lexical \\ R@1 \\  } & Self-Retrieval &  &  \\  & & & R@1 & R@5 & & \\ 
1 & 10.0 & 1.4 & 6.7 & 16.6 & **77.2** & 95.8 \\
0.5 & 10.8 & 1.4 & 6.7 & 17.9 & 77.0 & 67.0 \\
0.1 & 12.6 & 1.9 & 7.6 & 18.2 & 76.1 & 69.3 \\
0.05 & 14.7 & 2.3 & 8.7 & 20.9 & 76.4 & 74.1 \\
0.01 & 19.8 & 3.6 & **10.9** & **25.1** & 76.2 & 79.4 \\
0 & **22.3** & **4.5** & 10.0 & 24.5 & 75.0 & 95.6 \\   

Table 5: Performance comparison with different mixing ratios of MLE and SMILE on MSCOCO.

### Further Analysis

In this section, we conduct experiments to further analyze how SMILE works. As we hypothesized, SMILE implements a'semi-permeability' to retain only richness optimization, leading the model to generate more details. Therefore, we **first** investigate whether such an effect is attributed to our subset selection. **Then**, we affirm the'semi-permeability' by flipping it to observe an opposite property of SMILE. **Besides**, since BLIP-\(_{}\) surpasses human annotation in descriptiveness as shown in Table 1 and Table 2, we analyze where it gains the capability to generate details. **Furthermore**, we provide a visualization to illustrate how SMILE exerts its effect from the token-level loss. **Finally**, we test the generalization ability of SMILE on video captioning.

Subset selection is key in SMILE.We argue that the effectiveness of SMILE is attributed to our subset selection strategy that selects the target sequence words. To verify this, we ablate the SMILE subsetting strategy by replacing it with _Random subsetting_, that is, for a given target sequence, we randomly choose 10 words (the average caption length of MSCOCO) from the entire vocabulary to form the subset, ensuring the subset size is relatively consistent with that of SMILE. Fig. 4 (a) illustrates the impact of SMILE and Random subsetting on the generation length during training. Random subsetting maintains the describing habits of the basic model, neither lengthening nor shortening the generation length. This is due to that the model's preferred predictions among the entire vocabulary, whether more or less 'rich' than the label, are less likely to be included in the randomly-chosen subset. This leads to a mechanism akin to 'bidirectional impermeability', where both conciseness and richness optimization are weakened. This also demonstrates that it is the subset selection of SMILE that contributes to the increasing length of model generation.

Reverse subsetting flips the'semi-permeability'.Since SMILE achieves a'semi-permeability' which allows richness optimization while blocks conciseness optimization, it is natural to assume that such'semi-permeability' could be flipped with a reversed subset selection. Thus, we conduct experiments by proposing _Reverse subsetting_, that is, given a target sequence \(D=[w_{1},w_{2},,w_{N}]\), when predicting each word, we select the complement of the target sequence word set in the entire vocabulary, plus the current label, as the subset. Hence, for the prediction of the \(i\)-th word, the corresponding subset is \(_{S_{i}}=\{w|w_{D}\}\{w_{i}\}\). As shown in Fig. 4 (a), with a subset complementary to that of SMILE, Reverse subsetting exhibits the opposite property of SMILE, leading the model to generate increasingly shorter captions. With these observations, we confirm that there is a'semi-permeability', which is achieved by our subsetting strategy and can be inverted.

Models 'absorb' descriptiveness from the corpus.Although SMILE achieves unidirectional richness optimization, one may still wonder where models with SMILE optimization learn the descriptive captioning ability to surpass human annotators. We suspect that models could learn the usage of details from the whole corpus. To verify this hypothesis, we construct two datasets based on MSCOCO, namely Simplest-COCO, which contains no details, and Simpler-COCO, with only a few details. Concretely, for Simplest-COCO, we extract the subject from the caption and prefix it with a definite article based on grammatical rules, such as "a girl\({}^{}}\). For Simpler-COCO, we extract the subject-verb-object constituent of the caption as a new caption with few details, such as "a man working". All the mentioned natural language processing (e.g., parsing and part-of-speech tagging)

Figure 4: Average caption length during training with (a) different subsetting strategies and (b) on different training corpus.

[MISSING_PAGE_FAIL:9]

Related Works

In this work, we focus on generating descriptive image captions from the perspective of the language modeling objective. Existing works also propose different strategies to increase the descriptiveness of captions. For example, Gu et al.  and Liu et al.  design two-stage decoding strategies with caption generation and refinement. Liu et al.  and Shi et al.  integrate descriptive details from multiple-sentence annotations with additional rewards or natural language inference (NLI) relations. In addition to descriptiveness, existing works also employ contrastive learning [3; 6; 29] and self-retrieval strategy [28; 45] to increase the discriminability of captions. Wang et al.  reweight the ground truth captions to emphasize more discriminative ones. Wang et al.  highlights the uniqueness of each image within groups with similar images. More recently, CapEnrich  stimulates VLP models [59; 60] to express more details with prompting strategies [27; 34]. It achieves state-of-the-art performance on the descriptiveness and discriminability of image captioning. However, CapEnrich requires training on automatically constructed data with additional utterances, which integrate details from multiple captions annotated for each image. This leads to unconventional text structure and thus affects the fluency of the generated caption. In contrast to the above-mentioned methods, SMILE does not rely on either multiple annotated captions or the construction of additional data. To the best of our knowledge, SMILE is the first approach to enhance models' descriptive captioning capability by focusing on the language modeling objective itself.

Besides, text generation models trained with MLE tend to generate dull texts with high-frequency words [52; 37]. Attempts for fixing such degeneration issue including variant decoding methods [22; 46; 20; 14; 41] and learning algorithms [12; 9; 15; 51]. In this work, SMILE-optimized models output with much greater lexical diversity, also demonstrating the capacity of mitigation of such problems besides improving descriptiveness.

## 5 Conclusion

For descriptive image captioning, we revisit the commonly used maximum likelihood estimation (MLE) training objective. We argue that MLE is not perfectly suitable for image captioning tasks because of two conflicting optimizations: conciseness optimization and richness optimization. To steer the model toward generating more descriptive captions, we propose to mitigate conciseness optimization and maintain richness optimization by revising the objective with a simple but effective vocabulary subset selection strategy, namely Semipermeable MaxImum Likelihood Estimation (SMILE). Extensive experiments validate that SMILE helps models to generate much longer and more descriptive captions without complex architecture design or extra data annotation.

## 6 Limitations

Technically, it is possible to apply SMILE to any autoregressive text generation task with an MLE objective. To investigate this, we conduct a preliminary exploration on the Supervised Fine-tuning  (SFT) of Large Language Models [44; 43] (LLMs) - fine-tuning LLMs to follow human instructions, such as writing and answering questions. Unfortunately, comparing SMILE with MLE for SFT, we don't observe significant improvement in response length or overall quality (see Appendix B). One potential factor is that the target response in the training corpus is often very long (for instance, a paragraph exceeding 100 words). This causes that the constructed subsets are significantly larger than the ones in the image captioning task, and differ less from the entire vocabulary set. Therefore, SMILE may struggle to be directly applied to other text generation tasks. Further exploration of this issue will be reserved for future work.