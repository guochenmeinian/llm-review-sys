# DomainGallery: Few-shot Domain-driven Image Generation by Attribute-centric Finetuning

Yuxuan Duan\({}^{1}\) Yan Hong\({}^{2}\) Bo Zhang\({}^{1}\) Jun Lan\({}^{2}\) Huijia Zhu\({}^{2}\) Weiqiang Wang\({}^{2}\)

**Jianfu Zhang\({}^{1}\) Li Niu\({}^{1}\) Liqing Zhang\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Ant Group

Corresponding authors.

###### Abstract

The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want. Nevertheless, the availability of these models is still limited when we expect to generate images that fall into a specific domain either hard to describe or just unseen to the models. In this work, we propose DomainGallery, a few-shot domain-driven image generation method which aims at finetuning pretrained Stable Diffusion on few-shot target datasets in an attribute-centric manner. Specifically, DomainGallery features prior attribute erasure, attribute disentanglement, regularization and enhancement. These techniques are tailored to few-shot domain-driven generation in order to solve key issues that previous works have failed to settle. Extensive experiments are given to validate the superior performance of DomainGallery on a variety of domain-driven generation scenarios.

Figure 1: Given a few-shot target dataset of a specific domain such as sketches painted by an artist (a), it is usually difficult to directly generate images of this domain using pretrained text-to-image models (b). By using **DomainGallery** we propose in this work, we can achieve domain-driven generation in intra-category (c); cross-category (d); extra attribute (e); and personalization (f) scenarios.

Introduction

Walking down the street, you see an artist painting portrait sketches for people. You are fascinated by a couple of masterpieces set by the side, showing his/her unique painting style which you find it difficult to describe by words. Deeply interested, you are in the mood for seeing more sketches, and it would be perfect to see him/her painting other things like dogs, especially your favorite ones at home.

As a fundamental topic in computer vision, image generation has been attracting enormous research efforts. However, through the years from VAEs , GANs  to diffusion models , generative models are becoming more and more data-hungry in order to properly model the distribution of images, as the most recent Stable Diffusions  have been trained on billions of text-image pairs . Thus unfortunately, it is usually infeasible to directly train a generative model given only few-shot (around ten, or fewer) images of a specific target domain.

To tackle such challenging scenarios, one paradigm of solutions is _model transfer_, which first trains a model on a relevant source domain and then transfers it to the target domain by finetuning on the few-shot target dataset. Nevertheless, as Zhao et al.  have pointed out, the performance of model transfer methods will be significantly influenced by the relevance between source/target domains. Therefore, the applicability of these methods will be limited if we either fail to find a proper source dataset, or just do not have enough resources to train a generative model from scratch.

With the recent progress in pretrained text-to-image (T2I) models [28; 31; 32; 33; 35; 38; 52], it seems that anything can be generated simply by putting a text prompt into an off-the-shelf pretrained T2I model. However, T2I models are still far from once for all solutions to image generation. Sometimes it is difficult or even impossible to precisely describe certain styles (_e.g._ sketches by an artist) and contents (_e.g._ new concepts or personalized subjects), or what we want is simply unseen (thus unknown) to the model. Fortunately, T2I models can serve as _universal_ source models to be finetuned on specific target datasets. Recent works finetuning T2I models have mostly focused on either finetuning with relatively abundant images (tens, hundreds, or more) , or few-shot subject-driven generation whose datasets consist of a single person or object [12; 37]. On the contrary, few-shot domain-driven generation analogous to the conventional model transfer has rarely been explored.

In this work, we analyze and perform few-shot domain-driven image generation from the view of _attributes_, as a domain is defined by common attributes shared among images (see Sec. 3). We seek to master four generation cases as illustrated in Fig. 1: **Intra-category:** The generated images contain both the domain attributes and the categorical attribute of the given target dataset, as in conventional model transfer; **Cross-category:** While containing non-categorical domain attributes, images of other categories can be generated through text control, as a feature of T2I models; **Extra attribute:** Either intra- or cross-category, we can attach additional attributes to the images; **Personalization:** We hope to combine domain-driven and subject-driven generation for better personalization. In order to achieve these goals, we propose **DomainGallery**, adopting DreamBooth-like  finetuning paradigm where the non-categorical domain attributes are learned and bound to an identifier word, so that the generation can be done via a normal T2I pipeline. DomainGallery features four attribute-centric finetuning techniques which respectively settle four challenges:

**(1) Prior attribute erasure:** The prior attributes of the identifier word may possibly show up even if we have bound new domain attributes to it. Therefore, we pre-erase these prior attributes to avoid unexpected elements in images.

**(2) Attribute disentanglement:** The domain/categorical attributes corresponding to the identifier/category word may be leaked into each other, causing missing domain attributes and/or unexpected categorical attributes when we change the category word in cross-category generation. Therefore, we explicitly encourage domain-category disentanglement to prevent such leakage.

**(3) Attribute regularization:** The model is prone to overfitting when finetuned on few-shot datasets. Therefore, we regularize the finetuning process (with a strategy to construct paired source/target latent codes and a regularization loss) to reduce overfitting caused by excessive presence of domain attributes and possible biases of dataset distributions.

**(4) Attribute enhancement:** Sometimes the strengths of the domain attributes learned on a specific dataset category are insufficient for cross-category generation. Therefore, we adjust the intensity of the domain attributes when generating cross-category images for better fidelity.

These techniques spreading over pre-finetuning (1)(2), finetuning (2)(3) and inference (4), are tailored to few-shot domain-driven generation, aiming at solving key issues that previous works have failed to settle. Later in Sec. 5, we conduct thorough experiments on several few-shot datasets. These experiments manifest the superior and satisfying performance of DomainGallery on all of the four generation scenarios, which can serve as a state-of-the-art method of few-shot domain-driven image generation.

## 2 Related Work

Model TransferModel Transfer (of conventional noise-to-image models instead of T2I ones) is a mainstream paradigm of solutions to few-shot image generation. Methods following this paradigm transfer models trained on related source datasets to target domains by finetuning on few-shot target datasets. Model transfer has been thoroughly explored using GANs [10; 25; 26; 27; 29; 30; 34; 43; 46; 47; 50; 51; 55; 56; 57; 58; 61], with a few base on diffusion models [20; 59]. Since T2I models came to light, people have been freed from choosing proper source datasets/models, and attention has been turned to finetuning T2I models as generic source models.

Subject-driven Image GenerationAs one of the most frequently explored finetuning scenarios, subject-driven generation has attracted much research effort [2; 5; 7; 17; 24; 44; 48; 54] since the pioneering works Textual Inversion  and DreamBooth . Actually, subject-driven generation can be categorized as a special case of domain-driven generation, where the domain is defined by a particular person or object. To preserve the subject identity, fidelity is highly preferred to diversity, as diversity is scarcely evaluated quantitatively by these works. On the contrary, in general domain-driven generation the domains are usually not confined to a specific subject. Therefore diversity is as important as fidelity, and we will evaluate both just as model transfer works do.

Few-shot Domain-driven Image GenerationWe follow previous works to name our goal as few-shot domain-driven image generation. Analogous to _subject-driven_, the term _domain-driven_ implies finetuning from T2I models, which enables us to take advantage of the multi-modal capability of these models to achieve a variety of generation scenarios (see Fig. 1(c-f)). To the best of our knowledge, there is only one previous work focusing on this topic, namely DomainStudio . It finetunes a Stable Diffusion model towards the target domain by learning an identifier similar to DreamBooth , yet equipped with additional losses to enhance diversity and high-frequency details. In Sec. 4, we will analyze some crucial issues in domain-driven generation that previous works have failed to settle, and accordingly propose attribute-centric solutions to these problems.

Other Similar TasksThere are some other works focusing on resembling tasks. For instance, Everaert et al.  have focused on finetuning under limited data (tens to hundreds) with per-image text prompts. Such requirement of image quantity and prompts has limited its applicability. Another similar topic is T2I style transfer [4; 6; 11; 40], which usually extracts style information from a single style image and controls the content via text. A key issue shared by these works is how to clearly defining the boundary between style and content from a single image. Instead, domains can be naturally delimited as the common attributes shared among multiple images, which also enables us to learn a domain of certain contents, rather than styles.

## 3 Preliminary

DomainFormally, a _domain_\(\) can be defined as a sample space \(\) and a data distribution \(P_{}\) on \(\). However, this definition is excessively general as any group of arbitrary images can form a domain. In this work, we would like to provide a rather intuitive definition from the viewpoint of common attributes. We regard an image \(X\) to be composed of a set of attributes \(\{a_{i}\}_{i=1}^{N}\), where each \(a_{i}\) can be either abstract like a certain style, or concrete like a specific category or certain content. Then, an image domain \(\) can be defined as the common attributes shared by all the images of this domain: \(a_{}=_{}X\). According to such definition, an image belongs to this domain if and only if it contains all the common attributes: \(X a_{} X\). Take the few-shot sketches of faces in Fig. 1(a) as an example, \(a_{}\) includes shared categorical attribute of human faces and the attributes of this specific painting style, while the content attributes indicating individuals are not shared. Therefore, any facial sketch of any person in such style belongs to this domain.

Since in real-world scenarios, images in few-shot datasets usually share a common category (_e.g._ face in Fig. 1(a)), it is natural that categorical attribute should be one of the domain attributes. However, to extend domain-driven generation to cross-category scenarios as in Fig. 1(d), in this work we exclude the categorical attribute from \(a_{}\) so that the domain attributes refer to non-categorical attributes only. For instance, the domain in Fig. 1 will be referred to as sketches (of anything) in this certain style.

Diffusion ModelDiffusion model [8; 16; 42] is a recent genre of generative models. It aims at reversing a diffusion process by recurrently predicting the noises based on noisy data and denoising them accordingly till proper images are rendered. For practical usage in high-resolution and conditional cases, Latent Diffusion Model (LDM)  is often adopted which moves the diffusion process to latent spaces with pretrained VAEs . LDM is commonly trained using a simplified objective as

\[L_{}=_{l,c,(0,l),t}[\| -_{}(l_{t},t,_{}(c))\|_{2}^{2}],\] (1)

where \(l\), \(c\), \(\) and \(t\) are respectively latent codes, conditions, ground-truth noises and time steps. The module \(_{}\) is the encoder of the condition and \(_{}\) is the noise-predicting network which is usually a UNet . As special instances of LDM, Stable Diffusion (SD) series are pretrained on large-scale text-image datasets such as LAION-5B . They serve as state-of-the-art T2I models that are widely used as base models in many tasks, including our DomainGallery as well.

DreamBoothAs a pioneering work in subject-driven image generation, DreamBooth  binds the information of the subject to an identifier [V], which is a rarely used word such as _sks_, together with a corresponding category word [N], such as _dog_. Then images of the target subject can be generated by using prompts like _"a [V] [N]"_. For domain-driven image generation, we inherit such design to bind (non-categorical) domain attributes to [V], so that by changing category words or adding extra attributes via text, DomainGallery is capable of generating various images within the given domain.

Low-Rank AdaptationLow-Rank Adaptation (LoRA)  is a popular finetuning method frequently used on SD models. Instead of finetuning the parameters \(^{d_{} d_{}}\), LoRA finetunes rank decomposition matrices \(^{d_{} r}\) and \(^{r d_{}}\) as in \(}=+\), where \(r\) is very small and \(\) is fixed. Finetuned LoRA parameters can be easily shared and used with base models due to much smaller sizes. DomainGallery adopts LoRA when finetuning SD on target datasets.

## 4 DomainGallery

In this section, we will give a detailed description of DomainGallery. As in Fig. 2, the full pipeline has three steps: prior attribute erasure in Sec. 4.1, finetuning in Sec. 4.2, and inference in Sec. 4.4.

### Prior Attribute Erasure

Following DreamBooth, we link target domain attributes to an identifier [V]. Although we expect to select a rarely used word without obvious meaning, this word may have still been bound to certain prior attributes. For instance, the commonly used _sks_ is actually the abbreviation of a rifle , thus images generated with [V] in prompts will contain military elements, like the helmet in Fig. 2(a). In subject-driven generation such prior attributes are not problems, since the text condition _"a [V] [N]"_, as a whole, will gradually overfit to the given subject dataset and override these prior attributes. Also, [V] will never be paired with another category (_e.g. "a [V] cat"_ when the subject is a dog), while in domain-driven generation we expect [V] to be applicable to any category. According to the results in Sec. 5.2 and Appendix B.1, if not pre-erased, these prior attributes will appear in cross-category images, which verifies that these prior attributes are merely concealed rather than eliminated, and it is necessary to erase them before usage.

Since the prior attributes are bound to the identifier in a data-driven manner when T2I models are pretrained, it is difficult to theoretically specify which attributes have been linked to [V]. Therefore, we propose an empirical solution to prior attribute erasure. Based on a noisy source latent \(l_{}\) that has been added noise \(\) in the forward process, DomainGallery predicts the added noises \(_{}\) and \(_{}\) using the same LoRA-equipped UNet respectively with source text condition \(c_{}=\)_"a [N]"_ and target condition \(c_{}=\)_"a [V] [N]"_. Then, the prior attribute erasure loss is defined as

\[L_{}=(_{},(_{})),_{}=_{, }(l_{},c_{})\\ _{}=_{,}(l_{},c_ {}),\] (2)where we omit time step \(t\) and text encoder \(\) in the UNet \(_{,}\) for brevity, \(\) indicates LoRA parameters and \(()\) is the gradient stopping operation that stops the gradient from propagating through or updating the parameters inside. By imposing \(L_{}\), we hope that the model predicts the same with or without [V], hence the prior attributes in [V] will be removed.

Besides \(L_{}\), the prior preservation loss \(L_{}\) of DreamBooth is also applied which trains on source images \(I_{}\) generated by the base model of SD itself as training a diffusion model ordinarily via Eq. (1). Also, disentanglement loss \(L_{}\) is also included, which will be detailed in Sec. 4.2. After erasure, the learned LoRA parameters \(\) will be used to initialize LoRA in the finetuning period.

### Finetuning

With prior attributes of [V] erased, DomainGallery then learns to bind the target domain attributes to [V]. In addition to a standard finetuning on target datasets by \(L_{}\) via Eq. (1), with prior preservation on pre-generated source images, we propose domain-category attribute disentanglement loss \(L_{}\) and transfer-based similarity consistency loss \(L_{}\), as depicted in Fig. 2(b).

Domain-category Attribute DisentanglementSince few-shot datasets usually share a common category (face in Fig. 1(a)), when finetuning on such datasets, the (non-categorical) domain attributes in [V] will always show up together with the categorical attribute in [N], both in target images \(I_{}\) and target prompts \(c_{}\). As a result, it is possible that certain domain attributes may leak into [N], and/or conversely the categorical attribute may leak into [V]. Although it is not a

Figure 2: An overview of DomainGallery. **(a)** Before finetuning, we erase the prior attributes of the identifier [V] by matching the predicted noises when using source/target text conditions via \(L_{}\). **(b)** During fintuning, besides training ordinarily on target datasets (top-left), we additionally impose domain-category attribute disentanglement loss \(L_{}\) (bottom-left) and transfer-based similarity consistency loss \(L_{}\) (right). **(c)** When generating cross-category images, we enhance the domain attributes referred by [V] in a CFG-like manner. Dashed arrows indicate gradient stopping.

problem either for subject-driven generation since [V] and [N] will always be paired when generating images, such entanglement between [V] and [N] will harm cross-category scenarios of domain-driven generation. As experimental results shown in Sec. 5.2 and Appendix B.1, if we replace [N] with another category, sometimes domain attributes are partially lost, or elements of the original category still appear.

To tackle this issue, we try to enhance the disentanglement between [V] and [N], so that all the domain attributes will only be learned into [V] without leaking into [N], and categorical attributes in [N] will not be lost. In other words, attributes of [N] after finetuning should not be different from those before. As we use LoRA, the base model before finetuning is ready to use by simply disenabling LoRA parameters \(\) temporarily since the UNet is fixed. Based on noisy source latent \(l_{}\) and source text condition \(c_{}\), the domain-category attribute disentanglement loss can be formulated as

\[L_{}=(_{},(_ {}^{-})),_{}=_ {,}(l_{},c_{})\\ _{}^{-}=_{}(l_{},c_{ }),\] (3)

where \(_{}\) without \(\) is the base UNet whose LoRA parameters are detached.

Attribute RegularizationAdding regularization is a common practice of model transfer methods  to prevent overfitting, where features from **paired** source/target images generated from the same noise are usually required. However, according to the training objective of SD in Eq. (1), no fully denoised latent (_i.e_. at time step 0) will be generated, let alone paired source/target latents. DomainStudio  has proposed a regularization, which applies a similarity consistency loss  on batches of source/target images \(_{}_{}\) decoded from denoised latents \(_{}_{}\) after a single-step denoising from noisy latents \(l_{}_{}\). However, there are four drawbacks in this design: **(1)** single-step denoising usually does not lead to meaningful latents/images unless the timestep is small; **(2)** decoding latents into images induces significant overhead of computation and storage; **(3)** computing cosine similarity between pixel-level images is less reasonable; **(4)**\(_{}_{}\) are unpaired as they derives from unpaired input source/target images \(I_{}_{}\), which do not fit the similarity consistency loss requiring paired images/features. In our DomainGallery, we propose a strategy of constructing paired source/target latents, followed by a new regularization term named transfer-based similarity consistency loss, which overcomes the aforementioned drawbacks.

First we try to settle **(1)** and **(4)** by constructing denoised, meaningful and paired latent codes. As in the right part of Fig. 2(b), given a batch of \(l_{}\) at time step \(t\), we conduct an \(n\)-step recurrent denoising following the accelerated denoising process of DDIM  and a linearly decreasing time step schedule from \(t\) to 0. We intuitively set \(n=5\) to balance denoising quality and speed. We do recurrent denoising for twice, respectively with source/target text \(c_{}\)/\(c_{}\), and obtain \(_{}\)/\(_{}\). If we decode them into images \(_{}_{}\), we will find that \(_{}\) simultaneously have partial target domain attributes after conditioned on \(c_{}\), and share certain similarity with \(_{}\) since they derive from the same \(l_{}\). Hence \(_{}_{}\) are paired. Next, **without actually decoding them into images**, we reuse the encoder of UNet as a pretrained feature extractor to directly extract multi-layer features from \(_{}\) and \(_{}\), and compute the similarity consistency loss as

\[& L_{}=_{k=1}^{N} _{i=1}^{B}D_{}(p_{}^{i,k} \|(p_{}^{i,k})),\\ & p_{}^{i,k}=( \{(f^{k}(_{}^{i},c_{ }),f^{k}(_{}^{j},c_{ }))\}_{ j i}),\\ & p_{}^{i,k}=(\{(f^ {k}(_{}^{i},c_{}),f^{k}(_{ }^{j},c_{}))\}_{ j i}),\] (4)

where \(f^{k}\) represents features extracted by the UNet encoder at its \(k\)-th layer (of \(N\) layers). From the viewpoint of the \(i\)-th latent (of \(B\) latents in the batch), first its cosine similarities with other latents are computed, followed by a softmax operation transforming them into a probabilistic distribution \(p^{i}\). Then, Kullback-Leibler Divergence will be computed between two distributions respectively from \(_{}^{i}\) and \(_{}^{i}\), and will be averaged among all \(i\) and layers \(k\). In conclusion, \(L_{}\) helps to prevent overfitting by matching the similarity distributions among a batch of paired \(_{}_{}\). By operating on the features directly extracted from latents rather than on images, our \(L_{}\) settles **(2)** and **(3)** as well.

### Objective

The objective functions of prior attribute erasure and finetuning are respectively

\[ L_{}=L_{}+_{} L_{}+_{} L_{},\\ L_{}=L_{}+_{} L _{}+_{} L_{}+_{ } L_{},\] (5)

where \(_{}=1.0\), \(_{}=10.0\), \(_{}=10.0\), \(_{}=1.0\) generally renders good results. Note that as we utilize LoRA in DomainGallery, only the additional parameters \(\) of LoRA will be updated.

### Inference

In preliminary cross-category experiments, the domain attributes are not sufficiently manifested sometimes. A possible reason is that \(L_{}\) has limited the strengths of these attributes to the minimal, just enough to transfer images of the original category, while for cross-category scenarios these attributes may need enhancing. As shown in Fig. 2(c), we propose an inference-time attribute enhancement based on classifier-free guidance (CFG). Specifically, after applying CFG with default weight \(_{1}=7.5\), we additionally increase the strength of [V] by either of

\[&=()+_{1}(()-())+_{1}_{2}(()-());\\ &=()+ _{1}(()-())+_{1}_{2}(()-()).\] (6)

Between the two enhancing modes above, we empirically find that V-uncond generally outperforms its counterpart (see Appendix B.1). We will by default apply V-uncond with \(_{2}=1.0\) during cross-category generation.

### Personalization

For personalization scenarios, we straightforwardly combine our DomainGallery with DreamBooth in a single stage. Specifically, during the finetuning process in Sec. 4.2, the model is additionally finetuned on target subject images and source images of subject category via Eq. (1). In such cases, the objective of finetuning in Eq. (5) will be rewritten as

\[L_{}=L_{}+_{}(L_{}^{}+_{} L_{}^{}),\] (7)

where \(_{}\) is empirically set to \(1.0\). While we suppose that there may be a more delicate way to equip DomainGallery with subject-driven methods, we would like to leave it for future works.

## 5 Experiment

### Experimental Setting

BaselineOur baseline list includes DreamBooth , as the basis of our method; a LoRA  version of DreamBooth, since we utilize LoRA in DomainGallery; and finally DomainStudio , as the only previous work in few-shot domain-driven image generation.

DatasetWe test our method on five widely used 10-shot datasets, including CUFS sketches  ([N]: _face_), FFHQ sunglasses  ([N]: _face_), Van Gogh houses  ([N]: _house_), watercolor dogs  ([N]: _dog_) and wrecked cars  ([N]: _car_). Note that though sunglasses and wrecked cars may also be generated by directly mentioning their content attributes in text prompts, we still try on these datasets to prove that DomainGallery can also learn content attributes. Experiments are conducted on resolution \(512 512\) except for DomainStudio which is only capable of \(256 256\) even on a 40GB VRAM GPU.

MetricWe provide quantitative results for intra-category generation since we have dataset images as ground truth. For datasets with full sets (CUFS sketches and FFHQ sunglasses), we compute FID  between 1,000 samples with the full sets. For the others, we replace FID with KID  (\( 10^{3}\)) which better fits few-shot scenarios . Intra-clustered LPIPS  of 1,000 samples with the few-shot training sets is also reported as a standalone diversity metric.

DetailFor other details of the experiments and DomainGallery, please refer to Appendix A.

### Experimental Result

Intra-categoryAs the most basic scenario, we generate target images of the original categories. According to Tab. 1, DomainGallery generally outperforms the baselines w.r.t. both fidelity and diversity. These scores also match the qualitative results on CUFS sketches in Fig. 3, where DomainGallery can precisely capture the painting style of the target domain. Also, due to the effectiveness of our transfer-based similarity consistency loss \(L_{}\), the diversity of DomainGallery surpasses the baselines by large margins, while achieving competitive or even better fidelity. Refer to Fig. 10 in Appendix B.2 for qualitative results on the other datasets.

Cross-categoryWe illustrate qualitative results of cross-category generation on Van Gogh houses and watercolor dogs in Fig. 4. Since no previous method has pre-erased the prior attributes of [V] (_sks_) before usage, prior attributes of military elements can be observed in the samples generated by all the baselines. Besides, as none of the baselines explicitly imposes disentanglement between [V] and [N], attribute leakage can be observed on both datasets. Some images of DomainStudio still contain houses even if we change [N], manifesting leaked categorical attribute in [V]. On the other hand, many cross-category images of the baselines do not properly depict target domain attributes while their intra-category images do in Appendix B.2. Such phenomenon verifies that domain attributes have been partially leaked into [N] and will disappear if we change it. By adopting prior attribute erasure and enhancing domain-category attribute disentanglement, our DomainGallery avoids these issues and performs well. Samples on the other datasets are shown in Fig. 11 in Appendix B.2.

Extra AttributeIn Fig. 5 and Fig. 1(e), we show some images generated by DomainGallery on CUFS sketches with extra attributes added to either intra- or cross-category scenarios. We may infer from these results that though we only provide simple prompt (_i.e. "a [V] [N]"_) rather than detailed description for each image of the target dataset, training DomainGallery does not destruct the original text-image structures of SD. The images are still under full control through text prompts, including facial expressions, additional contents (_e.g._ accessories), sub-category (_e.g._ a breed of animals), background, and specific instances (_e.g._ celebrities or brands).

Besides, the bottom row of Fig. 5 illustrates some samples where the extra attributes (_blue_) provided in the text prompt are in conflict with the domain attributes (_colorless_). In such case, DomainGallery is capable of generating images with partially fused attributes. While the images are generally in

   &  &  &  &  &  \\  & FID\(\) & I-PIPS\(\) & FID\(\) & I-LPIPS\(\) & KID\(\) & I-LPIPS\(\) & KID\(\) & I-PIPS\(\) & KID\(\) & I-LPIPS\(\) \\  DreamBooth  & 70.41 & 0.4609 & 44.90 & 0.6451 & 48.43 & 0.6882 & **32.60** & 0.4005 & **8.81** & 0.5661 \\ DreamBooth+LoRA & 52.80 & 0.4636 & **41.22** & 0.6452 & 44.51 & 0.6744 & 68.80 & 0.4992 & 26.68 & 0.6063 \\ DomainStudio  & 51.73 & 0.4184 & 66.66 & 0.6089 & 41.06 & 0.6367 & 71.33 & 0.4059 & 32.31 & 0.5577 \\
**DomainGallery** & **44.86** & **0.5060** & 43.10 & **0.6924** & **32.20** & **0.7255** & 61.95 & **0.5216** & 23.63 & **0.6336** \\  

Table 1: Quantitative results of the intra-category scenarios on CUFS **sketches**, FFHQ **unglasses**, Van Gogh **houses**, watercolor **dogs** and wreck **cars**. The underlined results of DreamBooth have severe overfitting issues hence achieve good KID scores, see qualitative results in Fig. 10.

Figure 3: The 10-shot CUFS sketches dataset (left) and the intra-category samples generated by the baselines and DomainGallery with prompt _“a [V] face”_ (right).

grayscale, some blue feathers still appear. These results verify the generalization ability of our method and suggest that it may be open to other generation scenarios such as local editing and style blending.

PersonalizationIn the last scenario, DomainGallery is combined with DreamBooth to learn a target domain and a target subject simultaneously, as described in Sec. 4.5. Results in Fig. 6 manifest that such combination is feasible for both intra-category (the target dataset and subject share the same category, _e.g_. watercolor dogs and the subject of specific dog) and cross-category (otherwise) pairs of datasets. Together with the results of the previous scenario with extra attributes, the satisfying performance of DomainGallery shows its potentials to be applied to parallel or downstream tasks.

Figure 4: The 10-shot datasets (left) and the cross-category samples generated by the baselines and DomainGallery (right), on Van Gogh houses (top) and watercolor dogs (bottom).

Figure 5: Intra-category (top row) and cross-category (middle row) samples with extra attributes given by texts generated by DomainGallery, on CUFS sketches. The bottom row additionally show the case where the text contains conflicting attributes.

Ablation StudyTo prove that the proposed attribute-centric techniques can indeed effectively improve the performance of DomainGallery in various generation scenarios, we conduct extensive ablation studies focusing on these techniques and leave them in Appendix B.1 due to page limit.

## 6 Conclusion

In this work, we focus on few-shot domain-driven image generation by analyzing several key issues that previous works have failed to settle, and accordingly proposing a new method named DomainGallery. DomainGallery features four attribute-centric finetuning techniques that aim at solving these issues, namely prior attribute erasure, attribute disentanglement, attribute regularization and attribute enhancement. With these designs tailored to domain-driven generation, our DomainGallery achieves convincing performance on both intra-category and cross-category generation scenarios, while supporting extra attributes added by text prompts. Additionally, DomainGallery can be aggregated with subject-driven generation as well, which further extends its applicability. In Appendix C, we will discuss possible limitations and potential future works of DomainGallery.