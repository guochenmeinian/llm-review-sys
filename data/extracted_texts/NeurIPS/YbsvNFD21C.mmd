# Chih-Hung Liu

## Robust Sparse Regression with Non-Isotropic DesignsDepartment of Electrical Engineering

National Taiwan University

chliu@ntu.edtw

**Gleb Novikov**

Lucerne School of Computer Science and Information Technology

gleb.novikov@hslu.ch

###### Abstract

We develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive. Consider the model \(y^{*}=X^{*}^{*}+\) where \(X^{*}\) is an \(n d\) random design matrix, \(^{*}^{d}\) is a \(k\)-sparse vector, and the noise \(\) is independent of \(X^{*}\) and chosen by the _oblivious adversary_. Apart from the independence of \(X^{*}\), we only require a small fraction entries of \(\) to have magnitude at most 1. The _adaptive adversary_ is allowed to arbitrarily corrupt an \(\)-fraction of the samples \((X^{*}_{1},y^{*}_{1}),,(X^{*}_{n},y^{*}_{n})\). Given the \(\)-corrupted samples \((X_{1},y_{1}),,(X_{n},y_{n})\), the goal is to estimate \(^{*}\). We assume that the rows of \(X^{*}\) are iid samples from some \(d\)-dimensional distribution \(\) with zero mean and (unknown) covariance matrix \(\) with bounded condition number.

We design several robust algorithms that outperform the state of the art even in the special case of Gaussian noise \( N(0,1)^{n}\). In particular, we provide a polynomial-time algorithm that with high probability recovers \(^{*}\) up to error \(O()\) as long as \(n(k^{2}/)\), only assuming some bounds on the third and the fourth moments of \(\). In addition, prior to this work, even in the special case of Gaussian design \(=N(0,)\) and noise \( N(0,1)\), no polynomial time algorithm was known to achieve error \(o()\) in the sparse setting \(n<d^{2}\). We show that under some assumptions on the fourth and the eighth moments of \(\), there is a polynomial-time algorithm that achieves error \(o()\) as long as \(n(k^{4}/^{3})\). For Gaussian distribution \(=N(0,)\), this algorithm achieves error \(O(^{3/4})\). Moreover, our algorithm achieves error \(o()\) for all log-concave distributions if \( 1/\).

Our algorithms are based on the filtering of the covariates that uses sum-of-squares relaxations, and weighted Huber loss minimization with \(_{1}\) regularizer. We provide a novel analysis of weighted penalized Huber loss that is suitable for heavy-tailed designs in the presence of two adversaries. Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity.

## 1 Introduction

Linear regression is the fundamental task in statistics, with many applications in data science and machine learning. In ordinary (non-sparse) linear regression, we are given observations \(y^{*}_{1},,y^{*}_{n}\) and \(X^{*}_{1},,X^{*}_{n}^{d}\) such that \(y^{*}_{i}= X^{*}_{i},^{*}+_{i}\) for some \(^{*}^{d}\) and some noise \(^{n}\), and the goalis to estimate \(^{*}\). If \(\) is independent of \(X^{*}\) and has iid Gaussian entries \(_{i} N(0,1)\), the classical least squares estimator \(\) with high probability achieves the _prediction error_\(}\|X^{*}(-^{*})\| O()\). Note that if \(d/n 0\), the error is vanishing.

Despite the huge dimensions of modern data, many practical applications only depend on a small part of the dimensions of data, thus motivating _sparse_ regression, where only \(k d\) explanatory variables are actually important (i.e., \(^{*}\) is \(k\)-sparse). In this case we want the error to be small even if we only have \(n d\) samples. In this case, there exists an estimator that achieves prediction error \(O()\) (for \( N(0,1)^{n}\)). However, this estimator requires exponential computation time. Moreover, under a standard assumption from computational complexity theory (**NP \(\) P/poly**), estimators that can be computed in polynomial time require an assumption on \(X^{*}\) called a _restricted eigenvalue condition_ in order to achieve error \(O()\) (see  for more details). One efficiently computable estimator that achieves error \(O()\) under the restricted eigenvalue condition is Lasso, that is, a minimizer of the quadratic loss with \(_{1}\) regularizer. In particular, the restricted eigenvalue condition is satisfied for \(X^{*}\) with rows \(X^{*}_{i}}}{{}}N(0,)\), where \(\) has condition number \(O(1)\), as long as \(n k d\) (with high probability).

Further we assume that the designs have iid random rows, and the condition number of the covariance matrix is bounded by some constant. In addition, for random designs, we use the _standard_ error \(\|^{1/2}(-^{*})\|\). Note that when the number of samples is large enough, this error is very close to \(}\|X^{*}(-^{*})\|\).

Recently, there was an extensive interest in the linear regression with the presence of adversarially chosen outliers. Under the assumption \(X^{*}_{i}}}{{}}N(0,)\), the line of works  studied the case when the noise \(\) is unbounded and chosen by an _oblivious_ adversary, i.e., when \(\) is an arbitrary vector independent of \(X^{*}\). As was shown in , in this case, it is possible to achieve the same error (up to a constant factor) as for \( N(0,1)^{n}\) if we only assume that \((1)\) fraction of the entries of \(\) have magnitude at most \(1\). They analyzed the _Huber loss_ estimator with \(_{1}\) regularizer.

Another line of works  assumed that \(\) has iid random entries that satisfy some assumptions on the moments, but an adversarially chosen \(\)-fraction of \(y^{*}_{1},,y^{*}_{n}\) is replaced by arbitrary values by an _adaptive adversary_ that can observe \(X^{*}\), \(^{*}\) and \(\) (so the corruptions can depend on them).  showed that for \(X^{*}\) with iid sub-Gaussian rows and \(\) with iid sub-Gaussian entries with unit variance, Huber loss estimator with \(_{1}\) regularizer achieves an error of \(O(+(1/))\) with high probability. Note that the second term depends on \(\), but not on \(n\); hence, even if we take more samples, this term does not decrease (if \(\) remains the same). It is inherent: in the presence of the adaptive adversarial outliers, even for \(X^{*}_{i}}}{{}}N(0,)\) and \( N(0,1)^{n}\), the information theoretically optimal error is \((+)\), so independently of the number of samples, it is \(()\). In the algorithmic high-dimensional robust statistics, we are interested in estimators that are computable in time \((d)\). There is evidence that it is unlikely that \((d)\)-time computable estimators can achieve error \(O()\). Furthermore, for other design distributions the optimal error can be different.

Hence the natural questions to ask are : Given an error bound \(f()\), does there exist a \((d)\)-time computable estimator that achieves error at most \(f()\) with high probability? If possible, what is the smallest number of samples \(n\) that is enough to achieve error \(f()\) in time \((d)\)? In the rest of this section, we write error bounds in terms of \(\) and mention the number of samples that is required to achieve this error. In addition, we focus on the results for the high dimensional regime, where \(f()\) does not depend polynomially on \(k\) or \(d\).

Another line of works  considered the case when the adaptive adversary is allowed to corrupt \(\)-fraction of all observed data, i.e. not only \(y^{*}_{1},,y^{*}_{n}\), but also \(X^{*}_{1},,X^{*}_{n}\), while the noise \(\) is assumed to have iid random entries that satisfy some concentration assumptions. For simplicity, to fix the scale of the noise, we formulate their resultsassuming that \( N(0,1)^{n}\). In non-sparse settings,  showed that in the case of identity covariance sub-Gaussian designs, Huber loss minimization after a proper _filtering_ of \(X^{*}\) achieves error \(()\) with \(n d/^{2}\) samples. Informally speaking, filtering removes the samples \(X^{*}_{i}\) that look corrupted, and if the distribution of the design is nice enough, then after filtering we can work with \((X^{*},y^{*})\) just like in the case when only \(y^{*}\) is corrupted. For unknown covariance they showed a bound \(O\) for a large class of distributions of the design. If \(X^{*}_{i}}}{{}}N(0,)\) for unknown \(\), one can use \(nd^{2}/^{2}\) samples to robustly estimate the covariance, and achieve nearly optimal error \(()\) in the case (see  for more details).

In the sparse setting, there is likely an information-computation gap for the sample complexity of this problem, even in the case of the isotropic Gaussian design \(X^{*}_{i}}}{{}}N(0,)\). While it is information-theoretically possible to achieve optimal error \(O()\) with \(n(k/^{2})\) samples, achieving _any_ error \(o(1)\) is likely to be not possible for \((d)\)-time computable estimators if \(n k^{2}\). Formal evidence for this conjecture include reductions from some version of the Planted Clique problem , as well as a Statistical Query lower bound (Proposition 1.10). For \(n(k^{2}/^{2})\), several algorithmic results are known to achieve error \(()\), in particular, , and  for more general isotropic sub-Gaussian designs. Similarly to the approach of ,  used (\(_{1}\)-penalized) Huber minimization after filtering \(X^{*}\).

The non-isotropic case (when \(\) is unknown) is more challenging.  showed that for sub-Gaussian designs it is possible to achieve error \(O\) with \(n(k^{2})\) samples.  showed that \(O\) error with \(n(k^{2}+\|^{*}\|_{1}^{4}/k^{2})\) samples can be achieved under some assumptions on the fourth and the eighth moments of the design distribution. While this result works for a large class of designs, the clear disadvantage is that the sample complexity depends polynomially on the norm of \(^{*}\). For example, if all nonzero entries of \(^{*}\) have the same magnitude and \(\|^{*}\|=\), then the sample complexity is \(n>d^{2}\), which is not suitable in the sparse regime.

Prior to this work, no \((d)\)-time computable estimator that could achieve error \(o\) with unknown \(\) was known, even in the case of Gaussian designs \(X^{*}_{i}}}{{}}N(0,)\) and the Gaussian noise \( N(0,1)^{n}\) (apart from the non-sparse setting, where such estimators require \(n>d^{2}\)).

### Results

We present two main results, both of them follow from a more general statement; see Theorem B.3. Before formally stating the results, we define the model as follows.

**Definition 1.1** (Robust Sparse Regression with 2 Adversaries).: Let \(n,d,k\) such that \(k d\), \(>0\), and \((0,1)\) is smaller than some sufficiently small absolute constant. Let \(\) be a probability distribution in \(^{d}\) with mean \(0\) and covariance \(\). Let \(y^{*}=X^{*}^{*}+\), where \(X^{*}\) is an \(n d\) random matrix with rows \(X^{*}_{i}}}{{}}\), \(^{*}^{d}\) is \(k\)-sparse, \(^{n}\) is independent of \(X^{*}\) and has at least \(0.01 n\) entries bounded by \(\) in absolute value1. We denote by \(()\) the condition number of \(\).

An instance of our model is a pair \((X,y)\), where \(X^{n d}\) is a matrix and \(y^{n}\) is a vector such that there exists a set \(S_{}[n]\) of size at least \((1-)n\) such that for all \(i S_{}\), \(X_{i}=X^{*}_{i}\) and \(y_{i}=y^{*}_{i}\).

Note that random noise models studied in prior works are captured by our model in Definition 1.1. For example, if \(\) has iid entries that satisfy \(|_{i}|/2\), by Markov's inequality, \(|_{i}|\) with probability at least \(1/2\), and with overwhelming probability, at least \(0.01 n\) entries of \(\) are bounded by \(\) in absolute value. In addition, Cauchy noise (that does not have the first moment) with location parameter \(0\) and scale \(\) also satisfies these assumptions, as well as other heavy-tailed distributions studied in literature (with appropriate scale parameter \(\)).

We formulate our results assuming that the condition number of the covariance is bounded by some constant: \(() O(1)\). In the most general formulation (Theorem B.3), we show the dependence2 of the number of samples and the error on \(()\).

#### 1.1.1 Robust regression with heavy-tailed designs

We use the following notion of boundness of the moments of \(\):

**Definition 1.2**.: Let \(M>0\), \(t 2\) and \(d\). We say that a probability distribution \(\) in \(^{d}\) with zero mean and covariance \(\) has _\(M\)-bounded \(t\)-th moment_, if for all \(u^{d}\)

\[*{}_{x}| x,u|^{t} ^{1/t} M\|u\|\,.\]

Note that an arbitrary linear transformation of an _isotropic_ distribution with \(M\)-bounded \(t\)-th moment also has \(M\)-bounded \(t\)-th moment. Also note that if \(t^{} t\) and a distribution \(\) has \(M\)-bounded \(t\)-th moment, then the \(t^{}\)-th moment of \(\) is also \(M\)-bounded. In particular, \(M\) cannot be smaller than \(1\), since the second moment cannot be \(M\)-bounded for \(M<1\). In addition, we will need the following (weaker) notion of the boundness of moments:

**Definition 1.3**.: Let \(>0\), \(t 2\) and \(d\). We say that a probability distribution \(\) in \(^{d}\) with zero mean and covariance \(\) has _entrywise_\(\)-_bounded_\(t\)_-th moment_, if

\[_{j[d]}\ *{}_{x}|x_{j}|^{t} ^{t}\|\|^{t/2}\,.\]

If a distribution has \(M\)-bounded \(t\)-th moment, then it also has entrywise \(M\)-bounded \(t\)-th moment, but the converse might not be true for some distributions. Now we are ready to state our first result.

**Theorem 1.4**.: _Let \(n,d,k,X,y,,,,,^{}\) be as in Definition 1.1. Suppose that \(() O(1)\) and that for some \(1 M O(1)\) and \(1 O(1)\), \(\) has \(M\)-bounded \(3\)-rd moment and entrywise \(\)-bounded \(4\)-th moment. There exists an algorithm that, given \(X,y,k,,\), in time \((n+d)^{O(1)}\) outputs \(^{d}\) such that if \(n k^{2}(d)/\), then with probability at least \(1-d^{-10}\),_

\[\|^{1/2}(-^{})\| O()\,.\]

Let us compare Theorem 1.4 with the state of the art. For heavy-tailed designs, prior to this work, the best estimator was . That estimator also achieves error \(O()\), but its sample complexity depends polynomially on the norm of \(^{}\), while our sample complexity does not depend on it. In addition, they require the distribution to have bounded \(4\)-th moment (as opposed to our \(3\)-rd moment assumption), and bounded entrywise \(8\)-th moment (as opposed to our entrywise \(4\)-th moment assumption). Finally, our noise assumption is weaker than theirs since they required the entries of \(\) to be iid random variables such that \(|_{i}|^{}\) for some \(^{}>0\) known to the algorithm designer; as we mentioned after Definition 1.1, it is a special case of the oblivious noise with \(=2^{}\).

Let us also discuss our assumptions and possibilities of an improvement of our result. The third moment assumption can be relaxed, more precisely, it is enough to require the \(t\)-th moment to be bounded, where \(t\) is an arbitrary constant greater than \(2\), and in this case the sample complexity is increased by a constant factor3; see Theorem B.3 for more details. The entrywise fourth moment assumption is not improvable with our techniques, that is, we get worse dependence on \(k\) if we relax it to, say, the third moment assumption.

The dependence of \(n\) on \(\) is not improvable with our techniques4. The dependence of the error on \(\) is optimal. The dependence of \(n\) on \(k\) and the error on \(\) is likely to be (nearly) optimal: Statistical Query lower bounds (Proposition 1.10 and Proposition 1.11) provide evidence that for \(=(1)\), it is unlikely that polynomial-time algorithms can achieve error \(o(1)\) if \(n k^{2}\), or error \(o()\) if \(n k^{4}\).

_Remark 1.5_.: Our results also imply bounds on other types of error studied in literature. In particular, observe that \(\|-^{}\|\|^{1/2}(-^{}) \|/()}\), where \(_{}()\) is the minimal eigenvalue of \(\).

In addition, our estimator also satisfies \(\|-^{*}\|_{1} O(\|^{1/2}(-^{*})\| ()})\). The same is also true for our estimator from Theorem 1.7 below. These relations between different types of errors are standard for sparse regression, and they are not improvable.

#### 1.1.2 Beyond \(\) error

Prior to this work, no polynomial-time algorithm for (non-isotropic) robust sparse regression was known to achieve error \(o()\), even for Gaussian designs \(X_{i}^{*}}}{{}}N(0,)\) and Gaussian \( N(0,)^{n}\). In this section we show that for a large class of designs, it is possible to achieve error \(o()\) in polynomial time, even when \(\) is chosen by an oblivious adversary. For our second result, we require not only some bounds on the moments of \(\), but also their certifiability in the _sum-of-squares proof system_:

**Definition 1.6**.: Let \(M>0\) and let \( 4\) be an even number. We say that a probability distribution \(\) in \(^{d}\) with zero mean and covariance \(\) has \(\)_-certifiably \(M\)-bounded \(4\)-th moment_, if there exist polynomials \(h_{1},,h_{m}[u_{1},,u_{d}]\) of degree at most \(/2\) such that

\[*{}_{x} x,u^{4 }+_{i=1}^{m}h_{i}^{2}(u)=M^{4}\|\|^{2}\|u\|^{4}\,.\]

Definition 1.6 with arbitrary \(\) implies Definition 1.2 (with the same \(M\)). Under standard complexity-theoretic assumptions, there exist distributions with bounded moments that are not \(\)-certifiably bounded even for very large \(\). Note that similarly to Definition 1.2, an arbitrary linear transformation of an isotropic distribution with \(\)-certifiably \(M\)-bounded \(4\)-th moment also has \(\)-certifiably \(M\)-bounded \(4\)-th moment.

Distributions with certifiably bounded moments are very important in algorithmic robust statistics. They were extensively studied in literature, e.g. .

Now we can state our second result.

**Theorem 1.7**.: _Let \(n,d,k,X,y,,,,,^{*}\) be as in Definition 1.1. Suppose that \(() O(1)\), and that for some \(M 1\), some even number \( 4\), and \(1(1)\), \(\) has \(\)-certifiably \(M\)-bounded \(4\)-th moment and entrywise \(\)-bounded \(8\)-th moment. There exists an algorithm that, given \(X,y,k,,,M,\), in time \((n+d)^{O()}\) outputs \(^{d}\) such that if \(n M^{4} k^{4}(d)/^{3}\), then with probability at least \(1-d^{-10}\),_

\[\|^{1/2}(-^{*})\| O(M ^{3/4})\,.\]

In particular, in the regime \(M O(1)\), as long as \(n(k^{4}/^{3})\), the algorithm recovers \(^{*}\) from \((X,y)\) up to error \(O(^{3/4})\) (with high probability). If \( O(1)\), the algorithm runs in polynomial time. Note that in this theorem we do not assume that \(M\) is constant as opposed to Theorem 1.4 since for some natural classes of distributions, only some bounds on \(M\) that depend on \(d\) are known.

The natural question is what distributions have certifiably bounded fourth moment with \( O(1)\). First, these are products of one-dimensional distributions with \(M\)-bounded fourth moment, and their linear transformations (with \(=4\)). Hence, linear transformations of products of one-dimensional distributions with \(O(1)\)-bounded \(8\)-th moment satisfy the assumptions of the theorem with \(M O(1)\) and \(=4\). Note that such distributions might not even have a \(9\)-th moment. This class also includes Gaussian distributions (since they are linear transformations of the \(N(0,1)^{d}\) and \(N(0,1)\) has \(O(1)\)-bounded \(8\)-th moment).

Another important class is the distributions that satisfy _Poincare inequality_. Concretely, these distributions, for some \(C_{P} 1\), satisfy \(*{Var}_{x}g(x) C_{P}^{2}\|\| *{}_{x}\| g(x)\|_{2}^{2}\) for all continuously differentiable functions \(g:^{d}\).  showed that such distributions have \(4\)-certifiably \(O(C_{P})\)-bounded fourth moment. We will not further discuss Poincare inequality, and focus on the known results on the classes of distributions satisfy this inequality.

The Kannan-Lovasz-Simonovits (KLS) conjecture from convex geometry says that \(C_{P}\) is bounded by some universal constant for _all_ log-concave distributions. Recall that a distribution \(\) is called log-concave if for some convex function \(V:^{d}\), the density of \(\) is proportional to \(e^{-V(x)}\)Apart from the Gaussian distribution, examples include uniform distributions over convex bodies, the Wishart distribution and the Dirichlet distribution (, see also  for further examples). In recent years there has been a big progrees towards the proof of the KLS conjecture.  showed that \(C_{P} d^{(1)}\), and since then, the upper bound has been further significantly improved. The best current bound is \(C_{P} O()\) obtained by . This bound implies that for all log-concave distributions whose covariance has bounded condition number, the error of our estimator is \(O(^{3/4})\). Hence for \((1/^{2}(d))\) and \( O(1)\), the error is \(o()\). Note that if the KLS conjecture is true, the error of our estimator is \(O(^{3/4})\) for all log-concave distributions with \(() O(1)\), without any restrictions on \(\) (except the standard \( 1\)).

_Remark 1.8_.: Theorem 1.7 can be generalized as follows: If the \((2t)\)-th moment of \(\) is \(M\)-bounded for a constant \(t_{ 2}\), if this bound can be certified by a constant degree sum-of-squares proof5, and if \(\) has entrywise \((4t)\)-th \(O(1)\)-bounded moment, then with high probability, there is a \((d)\)-time computable estimator that achieves error \(O(M^{1-1/(2t)})\) as long as \(n M^{4}k^{2t}(d)/^{2t-1}\). See Theorem B.3 for more details.

_Remark 1.9_.: The dependence of \(n\) on \(\) can be improved under the assumption that \(\) is a _sub-exponential_ distribution. In particular, all log-concave distributions are sub-exponential. Under this additional assumption, in order to achieve the error \(O()\), it is enough to take \(n k^{2}(d)+k(d)/\), and to achieve error \(O(M^{3/4})\), it is enough to take \(n k^{4}(d)+k(d)/^{3/2}\) samples (assuming, as in Theorem 1.7, that the fourth moment is \(M\)-certifiably bounded).

#### 1.1.3 Lower bounds

We provide _Statistical Query_ (SQ) lower bounds by which our estimators likely have optimal sample complexities needed to achieve the errors \(O()\) and \(o()\), even when the design and the noise are Gaussian. SQ lower bounds are usually interpreted as a tradeoff between the time complexity and sample complexity of estimators; see Appendix G and  for more details. Our proofs are very similar to prior works  since as was observed in , lower bounds for mean estimation can be used to prove lower bounds for linear regression, and we use the lower bounds for sparse mean estimation from .

Let us fix the scale of the noise \(=1\). The first proposition shows that already for \(=\), \(k^{2}\) samples are likely to be necessary to achieve error \(o(1)\):

**Proposition 1.10** (Informal, see Proposition G.9).: _Let \(n,d,k,X,y,,,,,^{*}\) be as in Definition 1.1. Suppose that \(=N(0,)\) and \( N(0,^{2})^{n}\), where \(0.99 1\). Suppose that \(d^{\,0.01} k\), \(}\), and \(n k^{1.99}\). Then for each SQ algorithm \(A\) that finds \(\) such that \(\|^{*}-\| 10^{-5}\), the simulation of \(A\) with \(n\) samples has to simulate super-polynomial (\((d^{(1)})\) number of queries._

Note that under assumptions of Proposition 1.10, Theorem 1.4 implies that if we take \(n k^{2}(d)\) samples, the estimator achieves error \(O()\) that is \(o(1)\) if \( 0\) as \(d\).

The second proposition shows that for \(\), \(k^{4}\) samples are likely to be necessary to achieve error \(o()\):

**Proposition 1.11** (Informal, see Proposition G.10).: _Let \(n,d,k,X,y,,,,,^{*}\) be as in Definition 1.1. Suppose that \(=N(0,)\) for some \(\) such that \(\), and \( N(0,^{2})^{n}\), where \(0.99 1\). Suppose that \(d^{\,0.01} k\), \(\), and \(n k^{3.99}\). Then for each SQ algorithm \(A\) that finds \(\) such that \(\|^{*}-\| 10^{-5}\), the simulation of \(A\) with \(n\) samples has to simulate super-polynomial (\((d^{(1)})\)) number of queries._

Note that under assumptions of Proposition 1.11, Theorem 1.7 implies that if we take \(n k^{4}(d)\) samples, the estimator achieves error \(O(^{3/4})\) that is \(o()\) if \( 0\) as \(d\).

Techniques

Since the problem has multiple aspects, we first illustrate our approach on the simplest example \(X_{i}^{} N(0,)\) under the assumption that \(0.1 10\). Note that already in this case, even for \( N(0,1)^{n}\), our estimator from Theorem 1.7 outperforms the state of the art. In addition, we assume that \(=1\).

Our estimators are based on preprocessing \(X\), and then minimizing \(_{1}\)_-penalized Huber loss_. In the Gaussian case, the preprocessing step consists only of _filtering_, while for heavy-tailed designs, an additional _truncation_ step is required. The idea of using filtering before minimizing the Huber loss first appeared in  for the dense settings, and was applied to sparse settings in . We will not discuss the filtering method in detail, and rather focus on its outcome: It is a set \([n]\) of size at least \((1-O())n\) that satisfies some nice properties6. Further, we will see what properties we need from \(\), and now let us define the Huber loss estimator.

**Definition 2.1**.: For \(S[n]\), the _Huber loss function restricted to \(S\)_ is defined as

\[H_{S}()=_{i S}h( X_{i},-y_{i}) { where }h(x_{i})=\{x_{i}^{2}&|x_{i}| 2;\\ 2|x_{i}|-2&.\]

For a penalty parameter \(\), the \(_{1}\)-penalized Huber loss restricted to \(S\) is defined as \(L_{S}():=H_{S}()+\|\|_{1}\). We use the notation \((x)\) for the derivative of \(h(x)\). Note that for all \(x\), \(|(x)| 2\).

Our estimator is the minimizer \(_{}\) of \(L_{S}()\), where \(\) is the set returned by the filtering algorithm. To investigate the properties of this estimator, it is convenient to work with _elastic balls_. The \(k\)-elastic ball of radius \(r\) is the following set: \(_{k}(r):=\{u^{d}\|u\| r\,,\|u\|_{1}  r\}\,.\) Note that this ball contains all \(k\)-sparse vectors with Euclidean norm at most \(r\) (as well as some other vectors). Elastic balls are very useful for sparse regression since if the following two properties hold,

1. _Gradient bound:_ For all \(u_{k}(r)\), \(| H_{},u|}\|u\|_{1}+r\|u \|\),
2. _Strong convexity on the boundary:_ For all \(u_{k}(r)\) such that \(\|u\|=r\), \[H_{}(^{*}+u)-H_{}(^{*})- H_{},u r^{2}\,,\]

then for an appropriate choice of the penalty parameter \(\), then \(\|^{*}-_{}\|<r\).7

Hence it is enough to show these two properties. In the Gaussian case, the strong convexity property can be proved in exactly the same way as it is done in  for the case of the oblivious adversary, while for heavy-tailed designs it is significantly more challenging. Since we now discuss the Gaussian case, let us focus on the gradient bound. Denote \(H_{S}^{*}()=_{i S}h X_{i}^{*}, -y_{i}^{*}\). By triangle inequality,

\[| H_{},u| =| H_{_{}^{}}^{*} u+ H_{S_{}},u|\] \[| H_{[n]}^{*},u|+| H_{[ n]\{S_{}\}}^{*}u|+| H_{S_{ }},u|\,.\]

Since the first term can be bounded by \(\| H_{[n]}^{*}\|_{}\|u\|_{1}\), it is enough to show that \(\| H_{[n]}^{*}\|_{} r/\), where \(r\) is the error we aim to achieve. Note that \( H_{[n]}^{*}=_{i=1}^{n}(_{i}) X_{i}^{*},u\) does not depend on the outliers created by the adaptive adversary. The sharp bound on \(\| H_{[n]}^{*}\|_{}\) can be derived in exactly the same way as in  (or other prior works): Since \(\) and \(X^{*}\) are independent and \(|()| 2, H_{[n]}^{*}\) is a Gaussian vector whose entries have variance \((1/n)\). By standard properties of Gaussian vectors, \(\| H_{[n]}^{*}\|_{} O()\) with high probability.

To bound the second and the third term, we can use Cauchy-Schwarz inequality and get \(O()\) dependence on the error (like it is done in prior works on robust sparse regression, for example,  or ), or use Holder's inequality and get better dependence, but also more challenges since we have to work with higher (empirical) moments of \(X^{*}\) and \(X\). Let us use Holder'sinequality and illustrate how we work with higher moments. Note that both sets \([n](S_{})\) and \(S_{}\) have size at most \(O( nt)\). Hence the second term can be bounded by

\[O(^{3/4})_{i[n]\{S_{} \}} X^{*}_{i},u^{4}^{1/4} O( ^{3/4})_{i[n]} X^{*}_{i},u ^{4}^{1/4}\,,\]

while the third term is bounded by

\[O(^{3/4})_{i S_{}} {1}{n} X_{i},u^{4}^{1/4} O(^{3/4}) _{i} X_{i},u^{4}^{1/ 4}\,.\]

A careful probabilistic analysis shows that with high probability, for all \(r 0\) and all \(u_{k}(r)\), \(_{i[n]} X^{*}_{i},u^{4} O( \|u\|^{4})\). Hence, our requirement on \(\) is that \(_{i} X_{i},u^{4} O(1)\) for all \(u_{k}(1)\) (by scaling argument, it is enough to consider \(r=1\)). If we find such a set \(\), we get the desired bound. Indeed, if \(n k(d)/^{3/2}\), \(\| H^{*}_{[n]}\|_{} O(^{3/4}/)\), and the other terms are bounded by \(O(^{3/4})\), implying that \(-_{}<r=O(^{3/4})\).

Note that such sets of size \((1-O())n\) exist since \(S_{}\) satisfies this property. It is clear how to find such a set inefficiently: we just need to check all candidate sets \(S\) and maximize the quartic function \(_{i} X_{i},u^{4}\) over \(u_{k}(1)\). Furthermore, by-now standard filtering method allows to avoid checking all the sets: If we can maximize \(_{i} X_{i},u^{4}\) over \(u_{k}(1)\) efficiently, we can also find the desired set efficiently.

Before explaining how we maximize this function, let us see how prior works , optimized a simpler quadratic function \(_{i} X_{i},u^{2}\) over \(u_{k}(1)\). They use the _basic SDP_ relaxation for sparse PCA, that is, they optimize the linear function \(_{i S} X_{i}X_{i}^{},U\) over \(_{k}:=\{U^{d d} U 0\,,(U)  1\,,\|U\|_{1} k\}\). This set has been used in literature for numerous sparse problems since it is a nice (perhaps the best) convex relaxation of the set \(_{k}=\{uu^{} u^{d}\,,\,\|u\| 1\,,\|u\|_{ 0} k\}\). Moreover, crucially for sparse regression, it is easy to see that \(_{k}\) also contains all matrices \(uu^{}\) such that \(u_{k}(1)\). Hence, one may try to optimize quartic functions by using relaxations of \(_{k}=\{u^{ d} u^{d}\,,\,\|u\| 1\,,\|u\|_{ 0} k\}\). A natural relaxation is the sum-of-squares with _sparsity constraints_.  used these relaxations for sparse mean estimation8. They showed that these relaxations provide nice guarantees for distributions with certifiably bounded 4-th moment, assuming that the distribution has sub-exponential tails. Since we now discuss the Gaussian case, the assumption on the tails is satisfied. However, there is no guarantee that these relaxations capture \(u^{ 4}\) for all \(u_{k}(1)\). So, for sparse regression, we need another relaxation.

We use the sum-of-squares relaxations with _elastic constraints_. These constraints ensure that the set of relaxations \(_{k}^{d^{4}}\) is guaranteed to contain \(u^{ 4}\) for all \(u_{k}(1)\). We show that if \(n(k^{4})\), there is a degree-\(O(1)\) sum-of-squares proof from the elastic constraints of the fact that \(_{i[n]} X_{i},u^{4} O(1)\). It implies that the relaxation is nice: If \(_{i S} X_{i},u^{4} O(1)\) for all \(u_{k}(1)\), then \(_{i} X_{i}^{ 4},U O(1)\) for all \(U_{k}\). Since we can efficiently optimize over \(_{k}\), we get an efficiently computable estimator with error \(O(^{3/4})\) for Gaussian distributions. Furthermore, if we first use a proper thresholding (that we discuss below), our sum-of-squares proof also works for heavy-tailed distributions, that, apart from the certifiably bounded 4-th moment (that we cannot avoid with the sum-of-squares approach), are only required to have entrywise bounded 8-th moment.

Robust sparse regression with heavy-tailed designs is much more challenging. Again, for simplicity assume that \(0.1 10\) and \(=1\). First, there is an issue even without the adversarial noise: \( H^{*}_{[n]}_{}\) can be very large. Even under bounded fourth moment assumption, it can have magnitude \((d^{1/4}/n)\), which is too large in the sparse setting. Hence we have to perform an additional thresholding step and remove large entries of \(X\). Usually thresholding of the design matrix should be done very carefully since it breaks the relation between \(X\) and \(y\).  required the thresholding parameter \(\) to be large enough and depend polynomially on \(\|^{*}\|\) so that this dependence does not break significantly. Since \(\| H_{[n]}^{*}\|_{}\) can be as large as \((/n)\), the sample complexity of their estimator also depends polynomially on \(\|^{*}\|\).

Our idea of thresholding is very different, and it plays a significant role in our analysis, especially in the proof of strong convexity. Since we already have to work with outliers chosen by the adaptive adversary, we know that for an \(\)-fraction of samples, the dependence of \(y\) on \(X\) can already be broken. So, if we choose the thresholding parameter \(\) to be large enough so that with high probability it only affects an \(\)-fraction of samples, we can simply treat the samples affected by such thresholding as additional adversarial outliers, and assume that the adaptive adversary corrupted \(2 n\) samples. Note that since \(\) is heavy-tailed, each sample \(X_{i}^{*}\) might have entries of magnitude \(d^{(1)}\). However, \(y\) depends only on the inner products \( X_{i}^{*},^{*}\), and this inner product depends only on the entries of \(X^{*}\) that correspond to the support of \(^{*}\). Even though we don't know the support, we can guarantee that for \( 20\), all entries of \(X_{i}\) from the support of \(^{*}\) are bounded by \(\) with probability \(1-/2\). Indeed, since the variance of each entry is bounded by \(10\), Chebyshev's inequality implies that this entry is smaller than \(\) with probability at least \(1-/(2k)\), and by union bound, \( X_{i}^{*},^{*}\) is not affected by the thresholding with probability \(1-/2\). Hence by Chernoff bound, with overwhelming probability, the number of samples affected by our thresholding is at most \( n\).

Let us denote the distribution of the rows of \(X^{*}\) after thresholding with parameter \(\) by \(()\). After the thresholding step, we can assume that \(X_{i}^{*}\,()\). Note that thresholding can shift the mean, i.e. \(\,X_{i}^{*}\) can be nonzero. It is easy to see that \(\|_{x()}\,x\|_{} O(1/)\). Hence by Bernstein's inequality, \(\| H_{[n]}^{*}\|_{}+/n+1/ \) with high probability9. In particular, in order to get the error bounded by \(O(^{3/4})\), we need to take \(/^{3/4}\), and it affects sample complexity. Furthermore, our sum-of-squares proof requires that \(\|_{i=1}^{n}(X_{i}^{*})^{64}-(X _{1}^{*})^{64}\|_{}\) is smaller that \(1/k^{2}\). It can be shown that this quantity is bounded by \(+^{4}/n+1/^{4}\) with high probability10. In particular, we need \(n^{4}k^{2}\), so for \(/^{3/4}\), we have to take \(nk^{4}/^{3}\). As was discussed in Remark 1.9, if \(\) has sub-exponential tails, we do not have to do the thresholding, and the bounds from  allow to avoid this dependence of \(n\) on \(\). Note that due to the SQ lower bound (Proposition 1.11), sample complexity \(k^{4}\) is likely to be necessary, even for Gaussian designs.

Finally, let us discuss the strong convexity property. Here, we do not assume any properties related to sum-of-squares, and focus on the weak assumptions of Theorem 1.4. First, assume that we need to show strong convexity only for sparse vectors, and not for all \(u_{k}(r)\). As was observed in prior works on regression with obvious outliers, e.g. , \((u):=H_{S}(^{*}+u)-H_{S}(^{*})- H_{S},u\) can be lower bounded by \(_{i S} X_{i},u^{2}_{\|( X_{i},u-y_{i} 1)}_{\|( X_{i},u| 1)}\). Let \(C(u)=S_{} A B(u)\), where \(A\) is the set of samples where \(|_{i}| 1\) and \(B(u)=\{i[n]| X_{i},u| 1\}\). Then, \((u)(_{i C(u)} X_{i}^{*},u^{2})\). It can be shown that for some suitable \(r\) and for each \(k\)-sparse \(u\) of norm \(r\), \(C(u)\) is a large subset of the set \(A\) (of size at least \(0.99|A|\)). Note that since \(A\) is _independent_ of \(X^{*}\), the rows of \(X^{*}\) that correspond to indices from \(A\) are just iid samples from \(\). If \(X_{i}^{*}\) were Gaussian, we could have applied concentration bounds and prove strong convexity via union bound argument over subsets of size \(0.99|A|\). In the heavy-tailed case, we need a different argument. For a fixed set \(C\) of size \(0.99|A|\), we can use Bernstein's inequality11. We cannot use union bound argument over all subsets of size \(0.99|A|\) (there are too many), but fortunately we do not need it since for each \(k\)-sparse \(u\) of norm \(r\), it is enough to show that \(_{i T(u)} X_{i}^{*},u^{2}(r^{2})\), where \(T(u) A\) is the set of the smallest (in absolute value) \(0.99|A|\) entries of the vector \(X_{A}^{*}u^{|A|}\). Hence, we can use an epsilon-net argument for the set of \(k\)-sparse vectors \(u\) (of norm \(r\)). This set has very dense nets of (relatively) small size, and this is enough to show the lower bound \(_{i C(u)} X_{i}^{*},u^{2}(r^{2})\) for all \(k\)-sparse \(u\) of norm \(r\) with high probability, as long as \(n(k^{2})\).

In order to show the same bound for all \(u_{k}(r)\) of norm \(r\), we observe that12 if a quadratic form is \((r^{2})\) on \(K\)-sparse vectors of norm \(r\) for some \(K k\), then it is also \((r^{2})\) on all \(u_{k}(r)\), and applying the argument from the previous paragraph to \(K\)-sparse vectors, we get the desired bound. We remark that directly proving it for \(u_{k}(r)\) is challenging, since we extensively used the properties of the set of sparse vectors that are not satisfied by \(_{k}(r)\), e.g. the existence of very dense epsilon-nets of small size.

## 3 Future Work

There is an interesting open problem in robust sparse regression that is not captured by our techniques. For sparse mean estimation, in the Gaussian case, there exists a polynomial time algorithm with nearly optimal guarantees: It achieves error \(O()\) with \(k^{4}(d)/^{2}\) samples (). This algorithm uses a sophisticated sum-of-squares program13. It is reasonable to apply the techniques of  to robust sparse regression in order to achieve nearly optimal error \(O()\) with \((k)\) samples. However, simple approaches (e.g. our approach with replacing the sparse constraints by the elastic constraints) fail in this case. Here we provide a high-level explanation of the issue. In order to combine the filtering algorithm with their techniques, we need to check whether the values of a certain quartic form are small on all sparse vectors. The analysis in  shows that this form is indeed small for the uncorrupted sample with high probability (see their Lemma E.2.). Since we want the filtering algorithm to be efficient, we have to use a _relaxation_ of sparse vectors. Hence we need to find a sum-of-squares (or some other nice relaxation) version of the proof from . However, in their proof they use a _covering argument_, and it is not clear how to avoid it. This argument fails for reasonable relaxations that we have thought about. Both potential outcomes (either an algorithm or a computational lower bound) are interesting: An algorithm would likely require new sophisticated ideas, and a lower bound would show a significant difference between robust sparse regression and robust mean estimation, while, so far, the complexity pictures of these problems have seemed to be quite similar.

Another interesting direction is to get error \(o()\) for distributions that do not necessarily have certifiably bounded moments. As was shown in , only moment assumptions (without certifiability) are not enough for efficient robust mean estimation, and the same should be true also for linear regression. However, other assumptions on distribution \(\) can make the problem solvable in polynomial time. For robust mean estimation, some symmetry assumptions are enough even for heavy-tailed distributions without the second moment14 (see ). It is interesting to investigate what assumptions on the design distribution are sufficient for existence of efficiently computable estimators for robust sparse regression.