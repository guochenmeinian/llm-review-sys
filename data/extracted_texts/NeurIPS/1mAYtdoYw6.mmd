# Laplacian Canonization: A Minimalist Approach to

Sign and Basis Invariant Spectral Embedding

Jiangyan Ma\({}^{1}\)    Yifei Wang\({}^{2}\)    Yisen Wang\({}^{3,4}\)

\({}^{1}\) School of Electronics Engineering and Computer Science, Peking University

\({}^{2}\) School of Mathematical Sciences, Peking University

\({}^{3}\) National Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{4}\) Institute for Artificial Intelligence, Peking University

Equal Contribution.Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn)

###### Abstract

Spectral embedding is a powerful graph embedding technique that has received a lot of attention recently due to its effectiveness on Graph Transformers. However, from a theoretical perspective, the universal expressive power of spectral embedding comes at the price of losing two important invariance properties of graphs, sign and basis invariance, which also limits its effectiveness on graph data. To remedy this issue, many previous methods developed costly approaches to learn new invariants and suffer from high computation complexity. In this work, we explore a minimal approach that resolves the ambiguity issues by directly finding canonical directions for the eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing method, LC is light-weighted and can be applied to any existing GNNs. We provide a thorough investigation, from theory to algorithm, on this approach, and discover an efficient algorithm named Maximal Axis Projection (MAP) that works for both sign and basis invariance and successfully canonizes more than 90% of all eigenvectors. Experiments on real-world benchmark datasets like ZINC, MOLTOX21, and MOLPCBA show that MAP consistently outperforms existing methods while bringing minimal computation overhead. Code is available at [https://github.com/PKU-ML/LaplacianCanonization](https://github.com/PKU-ML/LaplacianCanonization).

## 1 Introduction

Despite the popularity of Graph Neural Networks (GNNs) for graph representation learning , it is found that many existing GNNs have limited expressive power and cannot tell the difference between many non-isomorphic graphs . Existing approaches to improve expressive power, such as high-order GNNs  and subgraph aggregation , often incur significant computation costs over vanilla GNNs, limiting their use in practice. In comparison, a simple but effective strategy is to use discriminative node identifiers. For example, GNNs with random features can lead to universal expressive power . However, these unique node identifiers often lead to the loss of the permutation invariance property of GNNs, which is an important inductive bias of graph data that matters for sample complexity and generalization . Therefore, during the pursuit of more expressive GNNs, we should also maintain the invariance properties of graph data. Balancing these two conflicting demands presents a significant challenge for the development of advanced GNNs.

Spectral embedding (SE) is a classical approach to encode node positions using eigenvectors \(\) of the Laplacian matrix \(\), which has the advantage of being expressive and permutation equivariant.

GNNs using SE can attain universal expressive power (distinguishing _any_ pair of non-isomorphic graphs) even under simple architectures (results in Section 2). However, spectral embedding faces two additional challenges in preserving graph invariance: sign ambiguity and basis ambiguity, due to the non-uniqueness of eigendecomposition. These ambiguities could lead to inconsistent predictions for the same graph under different eigendecompositions.

Many methods have been proposed to address sign and basis ambiguities of spectral embedding. A popular heuristic is RandSign  which randomly flips the signs of the eigenvectors during training. Although it is simple to use, this data augmentation approach does not offer any formal invariance guarantees and can lead to slower convergence due to all possible \(2^{n}\) sign flips. An alternative involves using sign-invariant eigenfunctions to attain sign invariance , which significantly increases the time complexity to \((n^{4})\). Another solution is to design extra GNN modules for sign and basis invariant embeddings, _e.g._, SignNet and BasisNet , which can also add a substantial computational burden. Therefore, as summarized in Table 1, existing spectral embedding methods are all detrimental in a certain way that either hampers sign and basis invariance, or induces large computational overhead. More discussions about the related work can be found in Appendix B.

In this work, we explore a new approach called Laplacian Canonization (LC) that resolves the ambiguities by identifying a unique canonical direction for each eigenvector, amongst all its sign and basis equivalents. Although it is relatively easy to find a canonization rule that work for certain vectors, up to now, there still lacks a rigorous understanding of what kinds of vectors are canonizable and whether we could find a complete algorithm for all canonizable features. In this paper, we systematically answer this problem by developing a general theory for Laplacian canonization and characterizing the sufficient and necessary conditions for sign and basis canonizable features.

Based on these theoretical properties, we propose a practical canonization algorithm for sign and basis invariance, called Maximal Axis Projection (MAP), that adopts the permutation-invariant axis projection functions to determine the canonical directions. We theoretically characterize the conditions under which MAP can guarantee sign and basis canonization, and empirically verify that this condition holds for most synthetic and real-world graphs. It is worth noting that LC is a lightweight approach since it is only a pre-processing method and does not alter the dimension of the spectral embedding. Empirically, we show that employing the MAP-canonized spectral embedding yields significant improvements over the vanilla RandSign approach, and even matches SignNet on large-scale benchmark datasets like OGBG . We summarize our contributions as follows:

* We explore Laplacian Canonization (LC), a new approach to restoring the sign and basis invariance of spectral embeddings via determining the canonical direction of eigenvectors in the pre-processing stage. We develop a general theoretical framework for LC and characterize the canonizability of sign and basis invariance.
* We propose an efficient algorithm for Laplacian Canonization, named Maximal Axis Projection (MAP), that works well for both sign and basis invariance. In particular, we show that MAP-sign is capable of canonizing all sign canonizable eigenvectors and thus is complete. The assessment of its feasibility shows that MAP can effectively canonize almost all eigenvectors on random graphs and more than 90% eigenvectors on real-world datasets.
* We evaluate the MAP-canonized spectral embeddings on graph classification benchmarks including ZINC, MOLTOX21 and MOLPCBA, and obtain consistent improvements over previous spectral embedding methods while inducing the smallest computational overhead.

    & pre-processing &  & permutation & addresses sign & addresses basis & feature dimension \\  & time & & invariance & ambiguity & ambiguity & dimension \\  LapPE  & \((n^{3})\) & ✗ & ✔ & ✗ & ✗ & \(n\) \\ RandSign  & \((n^{3})\) & ✗ & ✔ & ✔ & ✗ & \(n\) \\ SAN  & \((n^{4})\) & ✔ & ✔ & ✔ & ✗ & \(3n\) \\ SignNet  & \((n^{3})\) & ✔ & ✔ & ✔ & ✗ & \(2n\) \\ BasisNet  & \((n^{3})\) & ✔ & ✔ & ✔ & ✔ & \(n^{m}\) \\ MAP (ours) & \((n^{3})\) & ✔ & ✔ & ✔ & ✔ & \(n\) \\   

Table 1: Comparison between prior works and our method. \(n\) is the number of nodes, \(m\) is the exponent of the feature dimension of BasisNet .

## 2 Benefits and Challenges of Spectral Embedding for GNNs

Denote a graph as \(=(,,)\) where \(\) is the vertex set of size \(n\), \(\) is the edge set, and \(^{n d}\) are the input node features. We denote \(\) as the adjacency matrix, and let \(}=^{-}}^{-}=^{- }(+)^{-}\) be the normalized adjacency matrix, where \(\) denotes the augmented self-loop, \(\) is the diagonal degree matrix of \(}\) defined by \(D_{i,i}=_{j=1}^{n}_{i,j}\). A graph function \(f([,}])\) is _permutation invariant_ if for all permutation matrix \(^{n n}\), we have \(f([,}^{}])=f([,}])\). Similarly, \(f\) is _permutation equivariant_ if \(f([,}^{}])=f([,}])\).

**Spectral Embedding (SE).** Considering the limited expressive power of MP-GNNs, recent works explore more flexible GNNs like graph Transformers [58; 62; 16; 25; 42]. These models bypass the explicit structural inductive bias in MP-GNNs while encoding graph structures via positional embedding (PE). A popular graph PE is spectral embedding (SE), which uses the eigenvectors \(\) of the Laplacian matrix \(=-}\) with eigendecomposition \(=\,^{}\), where \(=()\) is the diagonal matrix of ascending eigenvalues \(_{1}_{n}\), and the \(i\)-th column of \(\) is the eigenvector corresponding to \(_{i}\). It is easy to see that spectral embedding is permutation equivariant: for any node permutation \(\) of the graph, \(\) is the spectral embedding of the new Laplacian since \(^{}=()()^{}\). Therefore, a permutation-invariant GNN (_e.g._, DeepSets , GIN , Graph Transformer ) using the SE-augmented input features \(}=[,]\) remains permutation invariant.

**Reweighted Spectral Embedding (RSE).** Previous works have shown that using spectral embedding improves the expressive power of MP-GNNs . Nonetheless, we find that SE alone is _insufficient_ to approximate an arbitrary graph function, as it does not contain all information about the graph, in particular, the eigenvalues. Consider two non-isomorphic graphs whose Laplacian matrices have identical eigenvectors but different eigenvalues. As their SE is the same, a network only using SE cannot tell them apart. To resolve this issue, we propose **reweighted spectral embedding (RSE)** that additionally reweights each eigenvector \(_{i}\) with the square-root of its corresponding eigenvalue \(_{i}\), _i.e._, \(_{}=^{}\). With the reweighting technique, RSE incorporates eigenvalue information without need extra dimensions.

**Universality of RSE.** In the following theorem, we prove that with RSE, _any_ universal network on sets (_e.g._, DeepSets  and Transformer ) is universal on graphs while preserving permutation invariance. All proofs are deferred to Appendix K.

**Theorem 1**.: _Let \(^{n d}^{n n}\) be a compact set of graphs, \([,}]\). Let \(\) be a universal neural network on sets. Given any continuous invariant graph function \(f\) defined over \(\) and arbitrary \(>0\), there exist a set of NN parameters such that for all graphs \([,}]\),_

\[|\,([,_{}])-f([,}])|<.\]

As far as we know, this theorem is the first to show that, with the help of graph embeddings like RSE, even an MLP network like DeepSets  (composed of a node-wise MLP, a global pooling layer, and a graph-level MLP) can achieve universal expressive power to distinguish any pair of non-isomorphic graphs. Notably, it does not violate the NP-hardness of graph isomorphism testing, since training NN itself is known to be NP-hard . Actually, it is not always necessary to use all spectra of the graph. Existing studies find that high-frequency components are often unhelpful, or even harmful for representation learning [5; 19]. Thus, in practice, we only use the first \(k\) low-frequency components of RSE. An upper bound on the approximation error of truncated RSE can be found in Appendix K.10.

**Ambiguities of eigenvectors.** Although RSE enables universal GNNs, there exist two types of ambiguity in eigenvectors that hinder their applications. The first one, known as **sign ambiguity**, arises when an eigenvector \(_{_{i}}\) corresponding to eigenvalue \(_{i}\) is equally valid with its sign flipped, _i.e._, \(-_{_{i}}\) is also an eigenvector corresponding to \(_{i}\). The second one, termed **basis ambiguity**, occurs when eigenvalues with multiplicity degree \(d_{i}>1\) can have any other orthogonal basis in the subspace spanned by the corresponding eigenvectors as valid eigenvectors. To be specific, for multiple eigenvalues with multiplicity degree \(d_{i}>1\), the corresponding eigenvectors \(_{_{i}}^{n d_{i}}\) form an orthonormal basis of a subspace. Then any orthonormal matrix \(^{d_{i} d_{i}}\) can be applied to \(_{_{i}}\) to generate a new set of valid eigenvectors for \(_{i}\). Because of these ambiguities, we can get distinct GNN outputs for the same graph, resulting in unstable and suboptimal performance [17; 25; 18; 29]. In Appendix A, we elaborate the challenges posed by these ambiguities.

## 3 Laplacian Canonization for Sign and Basis Invariance

Rather than incorporating additional modules to learn new sign and basis invariants [25; 29], we explore a straightforward, learning-free approach named Laplacian Canonization (LC). The general idea of LC is to determine a unique direction for each eigenvector \(\) among all its sign and basis equivalents. In this way, the ambiguities can be directly addressed in the pre-processing stage. For example, for two sign ambiguous vectors \(\) and \(-\), we aim to find an algorithm that determines a unique direction among them to obtain sign invariance. Despite some naive canonization rules (discussed in Appendix G.1), there still lacks a systematical understanding of the following key questions of LC:

1. What kind of canonization algorithm should we look for? (Section 3.1)
2. What kind of eigenvectors are canonizable or non-canonizable? (Section 3.2)
3. Is there an efficient canonization algorithm for all canonizable features? (Section 3.3)

In this section, we answer the three problems by establishing the first formal theory of Laplacian canonization and characterizing the canonizability for sign and basis invariance; based on these analyses, we also propose an efficient algorithm named MAP for LC that is guaranteed to canonize all sign canonizable features. To get a glimpse of our final results, Table 2 shows that MAP can resolve both sign and basis ambiguities for more than 90% of all eigenvectors on real-world data.

### Definition of Canonization

To begin with, we first find out what properties a desirable canonization algorithm should satisfy. The ultimate goal of canonization is to eliminate _ambiguities_ by selecting a _canonical form_ among these ambiguous outputs. Generally speaking, we can characterize ambiguity as a multivalued function \(f\), _i.e._, there could be multiple outputs \(y_{1},,y_{n}\) corresponding to the same input \(x\). In our sign/basis ambiguity case, \(f\) refers to a mapping from a graph \(x\) to the eigenvectors of a certain eigenvalue via the ambiguous eigendecomposition. The ambiguous eigenvectors are not independent; they are related by a sign/basis transformation. In general, we can assume that all possible outputs \(y_{1},,y_{n}\) of any \(x\) belong to the same equivalence class induced by some group action \(g G\), where \(G\) acts on \(\). That is, if \(f(x)=y_{1}=y_{2}\), then there exists \(g G\) such that \(y_{1}=gy_{2}\). Moreover, eigenvectors of graphs obey a fundamental symmetry: permutation equivariance. In general, we assume that \(f\) is equivariant to a group \(H\) acting on \(\) and \(\). That is, for any \(h H\), if \(y_{1},,y_{n}\) are all possible outputs of \(x\), then \(hy_{1},,hy_{n}\) are all possible outputs of \(hx\).

Specifically, for the goal of canonizing ambiguous eigenvectors, _i.e._, Laplacian canonization, we are interested in algorithms invariant to sign/basis transformations in the corresponding orthogonal group \(G\) (\(O(1)\) for sign invariance and \(O(d)\) for basis invariance). Meanwhile, to preserve the symmetry of graph data, this algorithm should also maintain the permutation equivariant property of eigenvectors (Section 2) _w.r.t._ the permutation group \(H\). Third, it also should still be discriminative enough to produce different canonical forms for different graphs (like the original spectral embedding). Combining these desiderata, we have a formal definition of canonization.

**Definition 1**.: _A mapping \(\) is called a (\(f,G,H\))**-canonization** when it satisfies:_

* \(\) _is_ \(G\)**-invariant**_:_ \( y,g G\)_,_ \((y)=(gy)\)_;_
* \(\) _is_ \(H\)**-equivariant**_:_ \( x,h H\)_,_ \(f(hx)=hf(x)\)_;_
* \(\) _is_ _universal_:_ \( x,h H\)_,_ \(x hxf(x)f(hx) \)_._

   Invariance & ZINC & MOLTOX21 & MOLPCBA \\  Sign & 2.46 \% & 3.04 \% & 2.24 \% \\ Basis & 1.59 \% & 3.31 \% & 7.37 \% \\ Total & 4.05 \% & 6.35 \% & 9.61 \% \\   

Table 2: The ratio of uncanonizable eigenvectors _w.r.t._ each invariance property with our MAP algorithm on three real-world datasets: ZINC, MOLTOX21, and MOLPCBA.

### Theoretical Properties of Canonization

Following Definition 1, we are further interested in the question that whether any eigenvector \(\) is canonizable, _i.e._, there exists a canonization that can determine its unique direction. Unfortunately, the answer is NO. For example, the vector \(=(1,-1)\) cannot be canonized by any canonization algorithm, since a permutation of \(\) gives \((-1,1)\) that equals to \(-\)3. But as long as there is only a small number of uncanonizable eigenvectors like \(\), a canonization algorithm can still resolve the ambiguity issue to a large extent.

Therefore, we are interested in the fundamental question of which eigenvectors are canonizable. The following theorem establishes a general necessary and sufficient condition of the canonizability for general groups \(G,H\), which may be of independent interest.

**Theorem 2**.: _An input \(x\) is canonizable on the embedding function \(f\) iff there does not exist \(h H\) and \(g G\) such that \(x hx\) and \(f(hx)=gf(x)\)._

This theorem states that for inputs from the same equivalence class in \(\) (induced by \(H\)), as long as they are not mapped to the same equivalence class in \(\) (induced by \(G\)), these inputs are canonizable. In particular, by applying Theorem 2 to the specific group \(G\) induced by sign/basis invariance, we can derive some simple rules to determine whether there exists a canonizable rule for given eigenvector(s).

**Corollary 1** (Sign canonizability).: _A vector \(^{n}\) is canonizable under sign ambiguity iff there does not exist a permutation matrix \(^{n n}\) such that \(=-\)._

**Corollary 2** (Basis canonizability).: _The base eigenvectors \(^{n d}\) of the eigenspace \(V\) are canonizable under basis ambiguity iff there does not exist a permutation matrix \(^{n n}\) such that \(\) and \(()=()=V\)._

**Remark**.: From a probabilistic view, almost all eigenvectors are canonizable. Let \(^{n d}\) be basis vectors sampled from a continuous distribution in \(^{n}\), then \(\{\}=1\).

In the next subsection, we will further design an efficient algorithm to canonize all sign and basis canonizable eigenvectors in the pre-processing stage, so the network does not have to bear the ambiguities of these eigenvectors.

### MAP: A Practical Algorithm for Laplacian Canonization

Built upon theoretical properties in Section 3.2, we aim to design a _general, powerful, and efficient_ canonization to resolve both sign and basis ambiguities for as many eigenvectors as possible. Here, we choose to adopt axis projection as the basic operator in our canonization algorithm named Maximal Axis Projection (MAP). The key observation is that the standard basis vectors (_i.e._, the axis) of the Euclidean space \(_{i}^{n}\) are permutation equivariant, and in the meantime, the eigenspace spanned by the eigenvectors \(V=()\) are also permutation equivariant. This means that when projecting the axis to the eigenspace, the obtained angles are also permutation equivariant, based on which we could apply permutation invariant functions (such as \(\)) to obtain permutation invariant statistics that can be used for canonization. Meanwhile, due to the generality of projection, it can be applied to both sign ambiguity (for a single eigenvector) and basis ambiguity (for the eigenspace with multiple eigenvectors). We provide illustrative examples to help understand this algorithm in Appendix F.

**Preparation step: Axis projection.** Consider unit eigenvector(s) \(^{n d}\) corresponding to an eigenvalue \(\) with geometric multiplicity \(d 1\). These eigenvectors span a \(d\)-dimensional eigenspace \(V=()\) with the projection matrix \(=^{}\). To start with, we calculate the projected angle between \(V\) and each standard basis (_i.e._, the axis) of the Euclidean space \(_{i}\) (a one-hot vector whose \(i\)-th element is \(1\)), _i.e._, \(_{i}=|_{i}|,i=1,,n\). Assume that there are \(k\) distinct values in \(\{_{i},i=1,,n\}\), according to which we can divide all basis vectors \(\{_{i}\}\) into \(k\) disjoint groups \(_{i}\) (arranged in descending order of the distinct angles). Each \(_{i}\) represents an equivalent class of axes that \(_{i}\) has the same projection on. Then, we define a summary vector \(_{i}\) for the axes in each group \(_{i}\) as their total sum \(_{i}=_{_{j}_{i}}_{j}+c\), where \(c\) is a tunable constant. Next, we introduce how to adopt these summary vectors to canonize the eigenvectors for sign and basis invariance, respectively.

#### 3.3.1 Sign Canonization with MAP

**Step 1. Find non-orthogonal axis.** For sign canonization, we calculate the angles between the eigenvalue \(\) and each summary vector \(_{i}\) one by one, and terminate the procedure as long as we find a summary vector \(_{h}\) with non-zero angle \(_{h}=^{}_{h} 0\), and return <none> otherwise.

\[_{h}=_{i},&,\\ &=,=\{i^{ }_{i} 0\}. \]

**Assumption 1**.: _There exists a summary vector \(_{h}\) that is non-orthogonal to \(\), i.e., \(^{}_{h} 0\),_

**Step 2. Sign canonization.** As long as Assumption 1 holds, we can utilize \(_{h}\) to determine the canonical direction \(^{*}\) by requiring its projected angle to be positive, _i.e._,

\[^{*}=,&^{}_{h}>0,\\ -,&^{}_{h}<0. \]

We call this algorithm MAP-sign, and summarize it in Algorithm 1 in Appendix C.1. The following theorem proves that it yields a valid canonization for sign invariance under Assumption 1.

**Theorem 3**.: _Under Assumption 1, our MAP-sign algorithm gives a sign-invariant, permutation-equivariant and universal canonization of \(\)._

One would wonder how restrictive the non-orthogonality condition (Assumption 1) is for sign canonization. In the following theorem, we establish a strong result showing that sign canonizability is equivalent to non-orthogonality. In other words, _any sign canonizable eigenvectors can be canonized by MAP-sign_. Due to this equivalence, MAP-sign can also serve as a complete algorithm to determine sign canonizability: a vector \(\) is sign canonizable iff \(_{h}\) is not <none> in equation 1.

**Theorem 4**.: _A vector \(^{n}\) is sign canonizable iff there exists a summary vector \(_{h}\) s.t. \(^{}_{h} 0\)._

**Remark**.: Theorem 3 is proved in Appendix K.5 and experimentally verified in Appendix C.2. We also show that MAP is not the only complete canonization algorithm for sign invariance by proposing another polynomial-based algorithm in Appendix K.9. We observe that most eigenvectors in real-world datasets are sign canonizable. For instance, on the ogbg-molhiv dataset, the ratio of non-canonizable eigenvectors is only 2.8%. A thorough discussion on the feasibilty of Assumption 1 is in Appendix G.1. For the left non-canonizable eigenvectors, we could apply previous methods (like RandSign, SAN, and SignNet) to further eliminate their ambiguity, which could save more compute since there are only a few non-canonizable eigenvectors. In practice, we also obtain good performance by simply ignoring these non-canonizable features.

#### 3.3.2 Basis Canonization with MAP

We further extend MAP-sign to solve the more challenging basis ambiguity with multiple eigenvectors, named MAP-basis. Now, MAP relies on two conditions to produce canonical eigenvectors. The first one requires there are enough distinctive summary vectors to determine \(d\) canonical eigenvectors.

**Assumption 2**.: _The number of distinctive angles \(k\) (i.e., the number of summary vectors \(\{_{i}\}\)) is larger or equal to the multiplicity \(d\), i.e., \(k d\)._

Under this condition, we can determine each \(_{i}\) iteratively with the corresponding summary vector \(_{i}\). At the \(i\)-th step where \(_{1},,_{i-1}\) have already been determined, we choose \(_{i}\) to be the vector that is closest to the summary vector \(_{i}\) in the orthogonal complement space of \(_{1},,_{i-1}\) in \(V\):

\[_{i}=*{arg\,max}_{}^{}_ {i}, \] \[_{1},,_{i-1}^{ },||=1.\]

With a compact feasibility region, the maximum is attainable, and we can further show that the solution \(_{i}\) is unique (_c.f._, the proof of Theorem 5). Equation 3 can be directly solved using the projection matrix of \(_{1},,_{i-1}^{}\) (see details in Algorithm 3 in Appendix C.1). Repeating this process gives us a canonical basis of \(V\). We also require non-orthogonality condition at each step to obtain a valid eigenvector.

**Assumption 3**.: _For any \(1 i d\), \(_{i}\) is not perpendicular to \(_{1},,_{i-1}^{}\)._

We summarize MAP-basis in Algorithm 2 in Appendix C.1. In Theorem 5, we prove under Assumption 2 and Assumption 3, MAP-basis gives a basis-invariant, permutation-equivariant and universal canonization of \(\). Though, in this scenario, MAP-basis cannot canonize all basis canonizable features, and whether such an algorithm exists remains an open problem for future research.

**Theorem 5**.: _Under Assumption 2 and Assumption 3, our MAP-basis algorithm gives a basis-invariant, permutation-equivariant and universal canonization of \(\)._

**Remark**.: Assumption 2 and Assumption 3 exclude some symmetries of the eigenspace, which we discuss in details in Appendix G.2. For random orthonormal matrices, the possibility that either assumption is violated is equal to \(0\), which we verify with random simulation in Appendix C.3. For real-world datasets, these assumptions are not restrictive either. For instance, on the large ogbg-molpcba dataset, the eigenvalues violating Assumption 2 or Assumption 3 make up 0.87% of all eigenvalues. More statistics and discussions are provided in Appendix G.2.

#### 3.3.3 Summary

We provide the complete pseudo-code of MAP to address both sign and basis invariance in Appendix C.1, along with a detailed time complexity analysis. Overall, the extra complexity introduced by MAP is \((n^{2} n)\), which is better than eigendecomposition itself with \((n^{3})\). It only needs to be computed once per dataset and can be easily incorporated with various GNN architectures.

Combining Theorem 1, Theorem 3 and Theorem 5 gives us the universality of first-order GNNs  such that it respects all graph symmetries under certain assumptions (Assumptions 1, 2 & 3). We show that the probability of violating these assumptions asymptotically converges to zero on random graphs (see Appendix H). We also count the ratio of violation in real-world datasets in Table 2, and observe that the ratio of uncanonizable eigenvectors is less than 10% on all datasets. Thus, MAP greatly eases the harm caused by sign and basis ambiguities.

So far only GNNs using higher-order tensors have achieved universality while respecting _all_ graph symmetries , but they are typically computationally prohibitive in practice. It is still an open problem whether it is also possible for first-order GNNs. The proposed Laplacian Canonization presents a new approach in this direction trying to alleviate the harm of sign and basis ambiguities in a minimalist approach. By establishing universality-invariance results for first-order GNNs under certain assumptions, LC could hopefully could bring some insights to the GNN community.

## 4 Experiments

We evaluate the proposed MAP positional encoding on sparse MP-GNNs and Transformer GNNs using PyTorch  and DGL . For sparse MP-GNNs we consider GatedGCN  and PNA , and for Transformer GNNs we consider SAN  and GraphiT . We conduct experiments on three standard molecular benchmarks--ZINC , OGBG-MOLTOX21 and OGBG-MOLPCBA . ZINC and MOLTOX21 are of medium scale with 12K and 7.8K graphs respectively, whereas MOLPCBA is of large scale with 437.9K graphs. Details about these datasets are described in Appendix L. We follow the same protocol as in Lim et al. , where we replace their SignNet with a normal GNN and the Laplacian eigenvectors with our proposed MAP. We fairly compare several models on a fixed number of 500K model parameters on ZINC and relax the model sizes to larger parameters for evaluation on the two OGB datasets, as being practised on their leaderboards . We also compare our results with GNNs using LapPE and random sign (RS) , GNNs using SignNet , and GNNs with no PE. For a fair comparison, all models use a limited number \(k\) of eigenvectors for positional encodings. The results of all our experiments are presented in Table 3, 4 & 5. Further implementation details are included in Appendix M.

### Performance on Benchmark Datasets

As shown in Table 3, 4 & 5, using MAP improves the performance of all GNNs on all datasets, demonstrating that removing ambiguities of the eigenvectors is beneficial for graph-level tasks. First, by comparing models with LapPE and RS with models with no PE, we observe that the use of LapPE significantly improves the performance on ZINC, showing the benefits of incorporating expressivePEs with GNNs, especially with MP-GNNs whose expressive power is limited by the 1-WL test. However, on MOLTOX21 and MOLPCBA, using LapPE has no significant effects. This is because unlike ZINC, OGB-MOL* datasets contain additional structural features that are informative, _e.g._, if an atom is in ring, among others . Thus the performance gain by providing more positional information is less obvious. Second, MAP outprems LapPE with RS by a large margin especially on ZINC. Although RS also alleviates sign ambiguity by randomly flipping signs during training, MAP removes such ambiguity _before_ training, enabling the network to focus on the real meaningful features and achieves a better performance. Third, we also observe that MAP and SignNet achieve comparable performance. This is because both methods aim at the same goal--eliminating ambiguity. However, SignNet does so in the training stage while MAP does so in the pre-processing stage, thus the latter is more computationally efficient. Lastly, we would also like to highlight that as a kind of positional encoding, MAP can be easily incorporated with any GNN architecture by passing the pre_transform function to the dataset class with a single line of code.

   Model & PE & \(k\) & \#Param & ROCAUC \(\) \\  GatedGCN & None & 0 & 1004K & \(0.772 0.006\) \\ GatedGCN & LapPE + RS & 3 & 1004K & \(0.774 0.007\) \\ GatedGCN & MAP & 3 & 1505K & \(\) \\  PNA & None & 0 & 5245K & \(0.755 0.008\) \\ PNA & MAP & 16 & 1951K & \(\) \\  SAN & None & 0 & 958K & \(0.744 0.007\) \\ SAN & MAP & 12 & 1152K & \(\) \\  GraphiT & None & 0 & 958K & \(0.743 0.003\) \\ GraphiT & MAP & 16 & 590K & \(\) \\   

Table 4: Results on MOLTOX21. All scores are averaged over 4 runs with 4 different seeds.

   Model & PE & \(k\) & \#Param & MAE \(\) \\  GatedGCN & None & 0 & 504K & \(0.251 0.009\) \\ GatedGCN & LapPE + RS & 8 & 505K & \(0.202 0.006\) \\ GatedGCN & SignNet (\((v)\) only) & 8 & 495K & \(0.148 0.007\) \\ GatedGCN & SignNet & 8 & 495K & \(0.121 0.005\) \\ GatedGCN & MAP & 8 & 486K & \(\) \\  PNA & None & 0 & 369K & \(0.141 0.004\) \\ PNA & LapPE + RS & 8 & 474K & \(0.132 0.010\) \\ PNA & SignNet & 8 & 476K & \(0.105 0.007\) \\ PNA & MAP & 8 & 462K & \(\) \\  SAN & None & 0 & 501K & \(0.181 0.004\) \\ SAN & MAP & 16 & 230K & \(\) \\  GraphiT & None & 0 & 501K & \(0.181 0.006\) \\ GraphiT & MAP & 16 & 329K & \(\) \\   

Table 3: Results on ZINC. All scores are averaged over 4 runs with 4 different seeds.

   Model & PE & \(k\) & \#Param & AAP \(\) \\  GatedGCN & None & 0 & 1008K & \(0.262 0.001\) \\ GatedGCN & LapPE + RS & 3 & 1009K & \(0.266 0.002\) \\ GatedGCN & MAP & 3 & 2658K & \(\) \\  PNA & None & 0 & 6551K & \(0.279 0.003\) \\ PNA & MAP & 16 & 4612K & \(\) \\   

Table 5: Results on MOLPCBA. All scores are averaged over 4 runs with 4 different seeds.

### Empirical Understandings

**Computation time.** We demonstrate the efficiency of MAP by measuring the pre-processing time and training time on the large OGBG-MOLPCBA dataset, and compare them with SignNet. For a fair comparison, we use identical model size, hyperparameters and random seed and conduct experiments on the same NVIDIA 3090 GPU. The results are shown in Table 6. We observe that model with MAP train 41% faster than its SignNet counterpart, saving 44 hours of training time. Since SignNet takes the form \(()+(-)\) while we use models taking the form \(()\), models with MAP would always train faster than those with SignNet under the same hyperparameters. We also observe that the pre-processing time is negligible compared with training time (< 3%), since pre-processing only needs to be done once. This makes MAP overall more efficient while achieving the same goal of tackling ambiguities.

**Spectral embedding dimension.** Next, we study the effects of \(k\). We train GatedGCN with MAP on ZINC with different number of eigenvectors used in the PE and report the results in Table 7. The hyperparameters are the same across these experiments. It can be observed that the performance drops when \(k\) is too small, meaning that SE provides crucial structural information to the model. Using larger \(k\) has limited influence on the performance, meaning that the model relies more on low-frequency information of spectral embedding.

**Ablation study.** Finally, we conduct ablation study of MAP by removing each component of MAP and evaluate the performance. The results are shown in Table 8. Removing sign invariance hurts the performance the most, because most eigenvectors are single and thus sign ambiguity has the most influence on the model's performance. Removing basis invariance also has negative effects since a small portion of eigenvalues are multiple. Removing eigenvalues has moderate negative effects, showing that the incorporation of eigenvalue information is beneficial for the network.

## 5 Conclusion

In this paper, we explored a new approach called Laplacian Canonization for addressing sign and basis ambiguities of Laplacian eigenvectors while also preserving permutation invariance and universal expressive power. We developed a novel theoretical framework to characterize canonization and the canonizability of eigenvectors. Then we proposed a practical canonization algorithm called Maximal Axis Projection (MAP). Theoretically, it is sign/basis-invariant, permutation-equivariant and universal. Empirically, it canonizes most eigenvectors on synthetic and real-world data while showing promising performance on various datasets and GNN architectures.

   Model & Pre-processing time & Training Time & Total Time \\  GatedGCN + MAP & 1.70 h & 63.02 h & 64.72 h \\ GatedGCN + SignNet & 0.27 h & 108.51 h & 108.78 h \\   

Table 6: Comparison of pre-processing and training time between models with MAP or SignNet as PE, on the MOLPCBA dataset. Experiments are run with the same model size and hyperparameters, the same random seed, on the same NVIDIA 3090 GPU.

   PE & full MAP & without can. sign & without can. basis & without eigenvalues \\  Test MAE & \(\) & \(0.131 0.003\) & \(0.122 0.003\) & \(0.125 0.001\) \\   

Table 8: Effects of the three components of MAP (GatedGCN on ZINC).