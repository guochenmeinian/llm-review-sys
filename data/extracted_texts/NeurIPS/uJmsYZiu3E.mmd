# Fair Allocation of Indivisible Chores:

Beyond Additive Costs

 Bo Li\({}^{1}\)  Fangxiao Wang\({}^{1}\)  Yu Zhou\({}^{1}\)

\({}^{1}\)Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China

comp-bo.li@polyu.edu.hk,

fangxiao.wang@connect.polyu.hk, csyzhou@comp.polyu.edu.hk

###### Abstract

We study the maximin share (MMS) fair allocation of \(m\) indivisible chores to \(n\) agents who have costs for completing the assigned chores. It is known that exact MMS fairness cannot be guaranteed, and so far the best-known approximation for additive cost functions is \(\) by Huang and Segal-Halevi [EC, 2023]; however, beyond additivity, very little is known. In this work, we first prove that no algorithm can ensure better than \(\{n,\}\)-approximation if the cost functions are submodular. This result also shows a sharp contrast with the allocation of goods where constant approximations exist as shown by Barman and Krishnamurthy [TEAC, 2020] and Ghodsi et al. [AIJ, 2022]. We then prove that for subadditive costs, there always exists an allocation that is \(\{n, m\}\)-approximation, and thus the approximation ratio is asymptotically tight. Besides multiplicative approximation, we also consider the ordinal relaxation, 1-out-of-\(d\) MMS, which was recently proposed by Hosseini et al. [JAIR and AAMAS, 2022]. Our impossibility result implies that for any \(d 2\), a 1-out-of-\(d\) MMS allocation may not exist. Due to these hardness results for general subadditive costs, we turn to studying two specific subadditive costs, namely, bin packing and job scheduling. For both settings, we show that constant approximate allocations exist for both multiplicative and ordinal relaxations of MMS.

## 1 Introduction

### Background and related research

Although fair resource allocation has been widely studied in the past decade, the research is centered around additive functions . However, in many real-world scenarios, the functions are more complicated. Particularly, the functions appear in machine learning and artificial intelligence (e.g, clustering , Sketches , Coresets , data distillation ) are often submodular, and we refer the readers to a recent survey that reviews some major problems within machine learning that have been touched by submodularity . Therefore, in this work, we study the fair allocation problem when the agents have beyond additive cost functions. The mainly studied solution concept is maximin share (MMS) fairness , which is traditionally defined for the allocation of goods as a relaxation of proportionality (PROP). A PROP allocation requires that the utility of each agent is no smaller than the average utility when all items are allocated to her. PROP is too demanding in the sense that such an allocation does not exist even when there is a single item and two agents. Due to this strong impossibility result, the maximin share (MMS) of an agent is proposed to relax the average utility by the maximum utility the agent can guarantee herself if she is to partition the items into \(n\) bundles but is to receive the least favorite bundle.

For the allocation of goods, although MMS significantly weakens the fairness requirement, it was first shown by Kurokawa et al. [31; 32] that there are instances where no allocation is MMS fair for all agents. Accordingly, designing (efficient) algorithms to compute approximately MMS fair allocations steps into the center of the field of algorithmic fair allocation. Kurokawa et al.  first proved that there exists a \(\)-approximate MMS fair allocation for additive utilities, and then Amanatidis et al.  designed a polynomial time algorithm with the same approximation guarantee. Later, Ghodsi et al.  improved the approximation ratio to \(\), and Garg and Taki  further improved it to \(+o(1)\). On the negative side, Feige et al.  proved that no algorithm can ensure better than \(\) approximation. Beyond additive utilities, Barman and Krishnamurthy  initiated the study of approximately MMS fair allocation with submodular utilities, and proved that a \(0.21\)-approximate MMS fair allocation can be computed by the round-robin algorithm. Ghodsi et al.  improved the approximation ratio to \(\), and moreover, they gave constant and logarithmic approximation guarantees for XOS and subadditive utilities, respectively. The approximations for XOS and subadditive utilities are recently improved by Seddighin and Seddighin . There are also several works that delve into concrete combinatorial problems and seek to improve approximation ratios compared to more general functions. Li et al.  and Hummel and Hettland  introduced interval scheduling and independent set structures, respectively, into MMS fair allocation problems. In both cases, the induced utility functions correspond to special cases of XoS functions. Both of these two works enhanced the approximation ratios for their specific utility functions. Some other combinatorial problems that have been studied for goods include the knapsack problem [21; 9; 8] and matroid constraints .

For the parallel problem of chores, where agents need to spend costs on completing the assigned chores, less effort has been devoted. Aziz et al.  first proved that the round-robin algorithm ensures 2-approximation for additive costs. Barman and Krishnamurthy , Huang and Lu  and Huang and Segal-Halei  respectively improved the approximation ratio to \(\), \(\) and \(\). Recently, Feige et al.  proved that with additive costs, no algorithm can be better than \(\)-approximate. However, except a recent work that studies binary supermodular costs , very little is known beyond additivity.

Besides multiplicative approximations, we also consider the ordinal approximation of MMS, namely, \(1\)-out-of-\(d\) MMS fairness which is recently studied in [5; 26; 27], and proportionality up to any item (PROPX) which is an alternative way to relax proportionality [38; 34]. For indivisible chores, Hosseini et al.  and Li et al.  respectively proved the existence of \(1\)-out-of-\(\) MMS fair and exact PROPX allocations. All the above works also assume additive costs. More works related to this paper in the literature can be seen in Appendix A.

### Main results

In this work, we aim at understanding the extent to which MMS fairness can be satisfied when the cost functions are beyond additive. In a sharp contrast to the allocation of goods, we first show that no algorithm can ensure better than \(\{n,\}\)-approximation when the cost functions are submodular.1 Further, we show that for general subadditive cost functions, there always exists an allocation that is \(\{n, m\}\)-approximate MMS, and thus the approximation ratio is asymptotically tight. Next, we consider the ordinal relaxation, 1-out-of-\(d\) MMS. It is trivial that 1-out-of-1 MMS is satisfied no matter how the items are allocated, and somewhat surprisingly, our impossibility result implies that for any \(d 2\), there is an instance for which no allocation is 1-out-of-\(d\) MMS.

**Result 1** For general subadditive cost functions, the asymptotically tight multiplicative approximation ratio of MMS is \(\{n, m\}\). Further, for any \(d 2\), a 1-out-of-\(d\) MMS allocation may not exist.

Result 1 combines Theorems 1, 2 and Corollary 1. The strong impossibility in Result 1 does not rule out the possibility of constant multiplicative or ordinal approximation of MMS fair allocations for specific subadditive costs. For this reason, we turn to studying two concrete settings with subadditive costs. The first setting encodes a bin packing problem, which has applications in many areas (e.g., semiconductor chip design, loading vehicles with weight capacity limits, and filling containers ). In the first setting, the items have sizes which can be different to different agents. The agents have bins that can be used to pack the items allocated to them with the goal of using as few bins as possible. The second setting encodes a job scheduling problem, which appears in many research areas, including data science, big data, high-performance computing, and cloud computing . In the second setting, the items are jobs that need to be processed by the agents. Each job may be of different lengths to different agents and each agent controls a set of machines with possibly different speeds. Upon receiving a set of jobs, an agent's cost is determined by the corresponding minimum completion time when processing the jobs using her own machines (i.e., makespan). As will be clear, job scheduling setting is more general than the additive cost setting. Besides, it uncovers new research directions for group-wise fairness.

**Result 2** For the bin packing and job scheduling settings, a 1-out-of-\(\) MMS allocation and a 2-approximate MMS allocation always exist.

Result 2 combines Theorems 3, 4 and Corollaries 2, 3. Besides studying MMS fairness, in Appendix F, we also prove hardness results for two other relaxations of proportionality, i.e., PROP1 and PROPX.

## 2 Preliminaries

For any integer \(k 1\), let \([k]=\{1,,k\}\). In a fair allocation instance \(I=(N,M,\{v_{i}\}_{i N})\), there are \(n\) agents denoted by \(N=[n]\) and \(m\) items denoted by \(M=\{e_{1},,e_{m}\}\). Each agent \(i N\) has a cost function over the items, \(v_{i}:2^{M}^{+}\{0\}\). Note that for simplicity, we abuse \(v_{i}()\) to denote a cost function. The items are chores, and particularly, upon receiving a set of items \(S M\), \(v_{i}(S)\) represents the effort or cost agent \(i\) needs to spend on completing the chores in \(S\). The cost functions are normalized and monotone, i.e., \(v_{i}()=0\) and \(v_{i}(S_{1}) v_{i}(S_{2})\) for any \(S_{1} S_{2} M\). Note that no bounded approximation can be achieved for general cost functions, and we provide one such example in Appendix B.1. Thus we restrict our attention to the following three classes. A cost function \(v_{i}\) is subadditive if for any \(S_{1},S_{2} M\), \(v_{i}(S_{1} S_{2}) v_{i}(S_{1})+v_{i}(S_{2})\). It is submodular if for any \(S_{1} S_{2} M\) and any \(e M S_{2}\), \(v_{i}(S_{2}\{e\})-v_{i}(S_{2}) v_{i}(S_{1}\{e\})-v_{i}(S_{1})\). It is additive if for any \(S M\), \(v_{i}(S)=_{e S}v_{i}(\{e\})\). It is widely known that any additive function is also submodular, and any submodular function is also subadditive.

An allocation \(=(A_{1},,A_{n})\) is an \(n\)-partition of the items where \(A_{i}\) contains the items allocated to agent \(i\) such that \(A_{i} A_{j}=\) for any \(i j\) and \(_{i N}A_{i}=M\). For any set \(S\) and integer \(d\), let \(_{d}(S)\) be the set of all \(d\)-partitions of \(S\). The maximin share (MMS) of agent \(i\) is

\[_{i}^{n}(I)=_{(X_{1},,X_{n})_{n}(M)}_{j[n]} v_{i}(X_{j}).\]

Note that we may neglect \(n\) and \(I\) in \(_{i}^{n}(I)\) when there is no ambiguity. Note that the computation of MMS is NP-hard even when the costs are additive, which can be verified by a reduction from the Partition problem. Given an \(n\)-partition of \(M\), \(=(X_{1},,X_{n})\), if \(v_{i}(X_{j})_{i}\) for any \(j[n]\), then \(\) is called an _MMS defining partition_ for agent \(i\). Note that the original definition of \(_{i}\) for chores is defined with non-positive values, where the minimum value of the bundles is maximized. In this work, to simplify the notions, we choose to use non-negative numbers (representing costs), and thus the definition is equivalently changed to be the maximum cost of the bundles being minimized. To be consistent with the literature, we still call it maximin share.

**Definition 1** (\(\)-Mms): _An allocation \(=(A_{1},,A_{n})\) is \(\)-approximate maximin share (\(\)-MMS) fair if \(v_{i}(A_{i})_{i}\) for all \(i N\). The allocation is MMS fair if \(=1\)._

Given the definition of MMS, for any agent \(i\) with subadditive cost \(v_{i}()\), we have the following bounds for \(_{i}\),

\[_{i}_{e M}v_{i}(\{e\}), v_{ i}(M)}.\] (1)

Following recent works , we also consider the ordinal approximation of MMS, namely, \(1\)-out-of-\(d\) MMS fairness. Intuitively, MMS fairness can be regarded as 1-out-of-\(n\) MMS (i.e., partitioning the items into \(n\) bundles but receiving the largest bundle). Since 1-out-of-\(n\) MMS allocations may not exist, we can instead find a maximum integer \(d n\) such that a 1-out-of-\(d\) MMS allocation is guaranteed to exist. Given a \(d\)-partition of \(M\), \(=(X_{1},,X_{d})\), if \(v_{i}(X_{j})_{i}^{d}\) for any \(j[d]\), then \(\) is called a _1-out-of-\(d\) MMS defining partition_ for agent \(i\). An allocation \(\) is _\(1\)-out-of-\(d\) MMS_ fair if for every agent \(i N\), \(v_{i}(A_{i})_{i}^{d}\). More generally, given any \( 1\), we have the bi-factor approximation, \(\)-approximate \(1\)-out-of-\(d\) MMS, if \(v_{i}(A_{i})_{i}^{d}\) for every \(i N\). By the definition, we have the following simple observation, whose proof is deferred to Appendix B.2.

**Observation 1**: _For any instance with subadditive costs, given any integer \(1 d n\), a 1-out-of-\(d\) MMS allocation is \(\)-MMS fair._

## 3 General subadditive cost setting

By Inequality 1, if the costs are subadditive, allocating all items to a single agent ensures an approximation of \(n\), which is the most unfair algorithm. Surprisingly, such an unfair algorithm achieves the optimal approximation ratio of MMS even if the costs are submodular.

**Theorem 1**: _For any \(n 2\), there is an instance with submodular costs for which no allocation is better than \(n\)-MMS or \(\)-MMS._

**Proof.** For any fixed \(n 2\), we construct an instance that contains \(n\) agents and \(m=n^{n}\) items. By taking logarithm of \(m=n^{n}\) twice, it is easy to obtain

\[n=.\]

Thus, in the following, it suffices to show that no allocation can be better than \(n\)-MMS. Let each item correspond to a point in an \(n\)-dimensional coordinate system, i.e.,

\[M=\{(x_{1},x_{2},,x_{n}) x_{i}[n]i[n]\}.\]

For each agent \(i N\), we define \(n\) covering planes \(\{C_{il}\}_{l[n]}\) and for each \(l[n]\),

\[C_{il}=\{(x_{1},x_{2},,x_{n}) x_{i}=lx_{j}[n]j[n]\{i\}\}.\] (2)

Note that \(\{C_{il}\}_{l[n]}\) forms an exact cover of the points in \(M\), i.e., \(_{l}C_{il}=M\) and \(C_{il} C_{iz}=\) for all \(l z\). For any set of items \(S M\), \(v_{i}(S)\) equals the minimum number of planes in \(\{C_{il}\}_{l[n]}\) that can cover \(S\). Therefore, \(v_{i}(S)[n]\) for all \(S\). We first show \(v_{i}()\) is submodular for every \(i\). For any \(S T M\) and any \(e M T\), if \(e\) is not in the same covering plane as any point in \(T\), \(e\) is not in the same covering plane as any point in \(S\), either. Thus, \(v_{i}(T\{e\})-v_{i}(T)=1\) implies \(v_{i}(S\{e\})-v_{i}(S)=1\), and accordingly,

\[v_{i}(T\{e\})-v_{i}(T) v_{i}(S\{e\})-v_{i}(S).\]

Since \(\{C_{il}\}_{l[n]}\) is an exact cover of \(M\), \(_{i}=1\) for every \(i\), where the MMS defining partition is simply \(\{C_{il}\}_{l[n]}\). Then to prove the theorem, it suffices to show that for any allocation of \(M\), there is at least one agent whose cost is \(n\). For the sake of contradiction, we assume there is an allocation \(=(A_{1},,A_{n})\) where every agent has cost at most \(n-1\). This means that for every \(i N\), there exists a plane \(C_{il_{i}}\) such that \(A_{i} C_{il_{i}}=\). Consider the point \(=(l_{1},,l_{n})\), it is clear that \( C_{il_{i}}\) and thus \( A_{i}\) for all \(i\). This means that \(\) is not allocated to any agent, a contradiction. Therefore, such an allocation \(\) does not exist which completes the proof of the theorem.

To facilitate the understanding of Theorem 1, in Appendix C.1, we visualize an instance with 3 agents and 27 items where no allocation is better than 3-MMS. The hard instance in Theorem 1 also implies the following lower bound for \(1\)-out-of-\(d\) MMS, whose proof is in Appendix C.2.

**Corollary 1**: _For any \(2 d n\), there is an instance with submodular cost functions for which no allocation is \(1\)-out-of-\(d\) MMS._

**Theorem 2**: _For any instance with subadditive cost functions, there always exists a \(\{n, m\}\)-MMS allocation._

**Proof.** We describe the algorithm that computes a \(\{n, m\}\)-MMS allocation in Algorithm 1. First, if \( m n\), we are safe to arbitrarily allocate the items to the agents, which ensures \(n\)-approximation.

The tricky case is when \( m<n\), where we cannot allocate too many items to a single agent. For this case, we first look at agent 1's MMS defining partition \(^{1}=(D^{1}_{1},,D^{1}_{n})\), where for all \(j[n]\) and we assume that they are ordered by the number of items, i.e., \(|D_{1}^{1}||D_{n}^{1}|\). In order to ensure that agent 1's cost is no more than \( m\) times her MMS, we ask her to take away \( m\) largest bundles (in terms of number of items) in \(^{1}\), i.e., \(A_{1}=_{j[ m]}D_{j}^{1}\). Since the cost function is subadditive,

\[v_{1}(A_{1})_{j[ m]}v_{1}(D_{j}^{1})  m_{1}.\]

Moreover, since on average each bundle in \(^{1}\) contains \(\) items and \(A_{1}\) contains the bundles with largest number of items, \(|A_{1}| m m\). That is, at least \(\) fraction of the items are taken away by agent 1. Let \(M_{1}=M A_{1}\) be the set of remaining items, and we have

\[|M_{1}|(1-) m.\]

We next ask agent 2 to take away items in a similar way to agent 1. Let \(^{2}=(D_{1}^{2},,D_{n}^{2})\) be one of agent 2's MMS defining partitions, and \(^{2}=(R_{1}^{2},,R_{n}^{2})\) be the remaining items in these bundles, i.e., \(R_{j}^{2}=D_{j}^{2} M_{1}\). Again, we assume \(|R_{1}^{2}||R_{n}^{2}|\). Letting \(A_{2}=_{j[ m]}R_{j}^{2}\) and \(M_{2}=M_{1} A_{2}\), we have \(v_{2}(A_{2}) m_{2}\). Moreover, since on average each bundle in \(^{2}\) contains \(|}{n}\) items and \(A_{2}\) contains the bundles with largest number of items,

\[|A_{2}| m|}{n}|M_{1}|,\]

which gives

\[|M_{2}|(1-)|M_{1}|( 1-)^{2} m.\] (3)

We continue with the above procedure for agents \(i=3,,n\) with the formal description shown in Algorithm 1. It is straightforward that every agent \(i\) who gets a bundle \(A_{i}\) has cost at most \( m_{i}\). Further, by induction, Equation 3 holds for all agents \(i n\), i.e.,

\[|M_{i}|(1-)|M_{i-1}| (1-)^{i} m.\]

To show the validity of the Algorithm, it remains to show that the algorithm can allocate all items, i.e., \(M_{n}=\). This can be seen from the following inequalities,

\[|M_{n}|(1-)^{n} m=(1- )^{ m} m<()^{ m} m< m=1.\]

Since \(|M_{n}|<1\), \(M_{n}\) must be empty, which completes the proof of the theorem.

Note that Theorem 1 does not rule out the possibility of beating the approximation ratio for specific subadditive costs. In the next two sections, we turn to studying two specific settings, where we are able to beat the lower bounds in Theorem 1 and Corollary 1 by designing algorithms that can guarantee constant ordinal and multiplicative approximations of MMS. We will mostly consider the ordinal approximation of MMS. By Observation 1, the ordinal approximation gives a result of the multiplicative one, which can be improved by slightly modifying the designed algorithms.

## 4 Bin packing setting

### Model

The first setting encodes a bin packing problem where the items have sizes and need to be packed into bins by the agents. The items may be of different sizes to different agents. Specifically, each item \(e_{j} M\) has size \(s_{i,j} 0\) to each agent \(i N\). For a set of items \(S\), \(s_{i}(S)=_{e_{j} S}s_{i,j}\). Each agent \(i N\) has unlimited number of bins with the same capacity \(c_{i}\). Without loss of generality, we assume that \(c_{1} c_{n}\) and \(c_{i}_{e_{j} M}s_{i,j}\) for all \(i N\).

Upon receiving a set of items \(S M\), agent \(i\)'s cost \(v_{i}(S)^{2}\) is determined by the minimum number of bins (with capacity \(c_{i}\)) that can pack all items in \(S\). Note that the calculation of \(v_{i}(S)\) involves solving a classic bin packing problem which is NP-hard. For any two sets \(S_{1}\) and \(S_{2}\), \(v_{i}(S_{1} S_{2}) v_{i}(S_{1})+v_{i}(S_{2})\) since the optimal packing of \(S_{1} S_{2}\) is no worse than packing \(S_{1}\) and \(S_{2}\) separately and thus \(v_{i}()\) is subadditive. Accordingly, \(_{i}^{d}\) is essentially the minimum number \(k_{i}\) such that the items can be partitioned into \(d\) bundles and the items in each bundle can be packed into no more than \(k_{i}\) bins. The definition of \(_{i}^{d}\) gives \(_{i}^{d} c_{i}(M)}{d}\) for all \(i N\).

We say an item \(e_{j} M\) is _large_ for an agent \(i\) if the size of \(e_{j}\) to \(i\) exceeds half of the capacity of \(i\)'s bins, i.e., \(s_{i,j}>}{2}\); otherwise, we say \(e_{j}\) is _small_ for \(i\). Let \(H_{i}\) denote the set of \(i\)'s large items in \(M\), and \(L_{i}\) denote the set of \(i\)'s small items; that is, \(H_{i}=\{e_{j} M:s_{i,j}>}{2}\}\) and \(L_{i}=\{e_{j} M:s_{i,j}}{2}\}\). Since two large items cannot be put together into the same bin, the number of each agent \(i\)'s large items is at most \(_{i}^{d} d\); that is, \(|H_{i}|_{i}^{d} d\).

We apply a widely-used reduction [13; 28] to restrict our attention on identical ordering (IDO) instances where \(s_{i,1} s_{i,m}\) for all \(i\). Specifically, it means that any algorithm that ensures \(\)-approximate 1-out-of-\(d\) MMS allocations for IDO instances can be converted to compute \(\)-approximate 1-out-of-\(d\) MMS allocations for general instances. The reduction may not work for all subadditive costs, but we prove in Appendix D.1 that it does work for the bin packing and job scheduling settings. Therefore, for these two settings, we only consider IDO instances.

### Algorithm

Next, we elaborate on the algorithm that proves Theorem 3.

**Theorem 3**: _A \(1\)-out-of-\(\) MMS allocation always exists for any bin packing instance._

Let \(d=\). In a nutshell, our algorithm consists of two parts: in the first part, we partition the items into \(d\) bundles in a bag-filling fashion and select one or two agents for each bundle. In the second part, for each of the \(d\) bundles and each of the agents selected for it, we present an imaginary assignment of the items in the bundle to the bins of the agent. These imaginary assignments are used to guide the allocation of the items to the agents, such that each agent receives cost no more than her 1-out-of-\(d\) MMS.

#### 4.2.1 Part 1: partitioning the items into \(d\) bundles

The algorithm in the first part is formally presented in Algorithm 2, which runs in \(d\) rounds of bag initialization (Steps 5 to 8) and bag filling (Steps 12 to 18). For each round \(j[d]\), we define _candidate agents_ - those who think the size of the bag \(B_{j}\) is not large enough and have unallocated small items (Step 4). Note that the set of candidate agents changes with the items in the bag and the unallocated items. In the bag initialization procedure, we put into the bag the item \(e_{j}\) and the items every \(d\) items after \(e_{j}\) (i.e., \(e_{j+d},e_{j+2d},\)), as long as they have not been allocated and are large for at least one remaining agent. We select one such agent. After the bag initialization procedure, if there is at most one candidate agent, the round ends and the candidate agent (if exists and has not been selected) is added as another selected agent. Otherwise, we enter the bag filling procedure.

In the bag filling procedure, as long as there exist at least two candidate agents, we let two of them be the selected agents and put the smallest unallocated item into the bag. If there is at most one candidate agent after the smallest item is put into the bag, the round ends and the only candidate agent (if exists and has not been selected) replaces one of the selected agents.

```
0: An IDO bin packing instance \((N,M,\{v_{i}\}_{i N},\{s_{i}\}_{i N})\).
0: A \(d\)-partition of the items \(=(B_{1},,B_{d})\) and disjoint sets of selected agents \(=(G_{1},,G_{d})\).
1: Initialize \(L_{i}\{e_{j} M:s_{i,j}}{2}\}\) for each \(i N\), and \(R M\).
2:for\(j=1\) to \(d\)do
3: Initialize \(B_{j}\), \(G_{j}\), \(t j\).
4:\(N(B_{j})\{i N:s_{i}(B_{j})(M)}{d}\) and \(L_{i} R\}\). // Candidate agents
5:while\(e_{t} R\) and there exists an agent \(i N\) who thinks \(e_{t}\) is large do
6:\(B_{j} B_{j}\{e_{t}\}\), \(R R\{e_{t}\}\), \(t t+d\).
7:\(G_{j}\{i\}\).
8:endwhile
9:if\(|N(B_{j})|=1\) and \(N(B_{j}) G_{j}\)then
10: Pick \(i N(B_{j})\), \(G_{j} G_{j}\{i\}\).
11:endif
12:while\(|N(B_{j})| 2\)do
13: Pick \(i_{1},i_{2} N(B_{j})\), \(G_{j}\{i_{1},i_{2}\}\).
14: Pick the smallest item \(e R\), \(B_{j} B_{j}\{e\}\), \(R R\{e\}\).
15:if\(|N(B_{j})|=1\) and \(N(B_{j}) G_{j}\)then
16: Pick \(i N(B_{j})\) and replace one arbitrary agent in \(G_{j}\) with agent \(i\).
17:endif
18:endwhile
19:\(N N G_{j}\).
20:endfor ```

**Algorithm 2** Partitioning the items into \(d\) bundles.

The way we establish the bag and select the agents makes the following two important properties satisfied for every round \(j[d]\).

* **Property 1**: for each selected agent \(i G_{j}\), there are at most \(_{i}^{d}\) items in \(B_{j}\) that are large for \(i\). Besides, letting \(e_{j}^{*}\) be the item lastly added to \(B_{j}\), if \(e_{j}^{*}\) is small for \(i\), then \(s_{i}(B_{j}\{e_{j}^{*}\})(M)}{d}\).
* **Property 2**: for each remaining agent \(i^{}\) (i.e., \(i^{}_{l[j]}G_{l}\)), either \(s_{i^{}}(B_{j})>}(M)}{d}\) or no unallocated item is small for \(i\) at the end of round \(j\). Besides, no item in \(\{e_{j},e_{j+d},\}\) that is large for \(i^{}\) remains unallocated at the end of round \(j\).

**Proof.** For the first property, observe that large items are added into the bag only in the bag initialization procedure, where one out of every \(d\) items is picked. Since there are at most \(d_{i}^{d}\) large items for every agent \(i\), the bag contains at most \(_{i}^{d}\) of \(i\)'s large items. There are two cases where \(e_{j}^{*}\) is small for an agent \(i G_{j}\). First, \(i\) is the only candidate agent after \(e_{j}^{*}\) is added, for which case, we have \(s_{i}(B_{j})(M)}{d}\). Second, \(i\) is one of the two selected candidate agents before \(e_{j}^{*}\) is added, for which case, we have \(s_{i}(B_{j}\{e_{j}^{*}\})(M)}{d}\). In both cases, \(s_{i}(B_{j}\{e_{j}^{*}\})(M)}{d}\) holds. The second property is quite direct by the algorithm, since there is no candidate agent outside \(G_{j}\) at the end of round \(j\) (i.e., \(N(B_{j}) G_{j}=\)), and all unallocated large items in \(\{e_{j},e_{j+d},\}\) are put into the bag in the bag initialization procedure.

Property 1 ensures that the items in each bundle \(B_{j}\) can be allocated to the selected agents in \(G_{j}\), such that each agent \(i G_{j}\) can use no more than \(^{d}_{i}\) bins to pack all the items allocated to her, which will be shown in the following part. Property 2 ensures the following claim.

**Claim 1**: _All the items can be allocated in Algorithm 2._

**Proof.** Observe that when the last round begins, there are at least \(n-(d-1) 2 2\) remaining agents. If all the unallocated items are large for some remaining agent, all of them are added into the bag during the bag initialization procedure of the last round and thus no item remains unallocated. Now consider the case where some unallocated item is small for any remaining agent. By Property 2, for any \(j[d-1]\) and any remaining agent \(i^{}\), we have \(s_{i^{}}(B_{j})>}(M)}{d}\). This gives that the total size of the unallocated items to \(i^{}\) is smaller than \(}(M)}{d}\). Besides, after the bag initialization procedure of the last round, no large item remains and every remaining item is small for any remaining agent. Combining these two facts, we know that there are always at least 2 candidate agents and thus all small items can be allocated in the bag filling procedure, which completes the proof.

#### 4.2.2 Part 2: Allocating the items to the agents

Next, we allocate the items in each bundle \(B_{j}\) to the selected agents in \(G_{j}\). Let \(i\) be any agent in \(G_{j}\) and \(B^{}_{j}=B_{j}\{e^{*}_{j}\}\) where \(e^{*}_{j}\) is the item lastly added to \(B_{j}\). We first imaginatively assign the items in \(B^{}_{j}\) to \(i\)'s bins as illustrated by Figure 1. We first put \(i\)'s large items in \(B^{}_{j}\) into individual empty bins. Then we greedily put into the bins the remaining small items in \(B^{}_{j}\) in decreasing order of their sizes, as long as the total size of the assigned items does not exceed the bin's capacity. The first time when the total size exceeds the capacity, we move to the next bin and so on (if all the bins with large items are filled, we move to an empty bin). We call the item lastly added to each bin that makes the total size exceed the capacity an _extra item_. Denote by \(J_{i}(B^{}_{j})\) the set of extra items and by \(W_{i}(B^{}_{j})=B^{}_{j} J_{i}(B^{}_{j})\) the other items in \(B^{}_{j}\).

If all items in \(B_{j}\) are large for some agent \(i G_{j}\), we allocate all of them to \(i\). Otherwise, we know that round \(j\) enters the bag filling procedure, thus there are two agents in \(G_{j}\) and the last item \(e^{*}_{j}\) is small for both of them. Letting \(i_{1}\) be the agent who has more large items in \(B_{j}\) and \(i_{2}\) be the other agent, we allocate \(i_{1}\) the items in \(W_{i_{1}}(B^{}_{j})\) and allocate \(i_{2}\) the items in \(J_{i_{1}}(B^{}_{j})\{e^{*}_{j}\}\).

Now we are ready to prove Theorem 3.

**Proof of Theorem 3.** Consider any round \(j[d]\). If all items in \(B_{j}\) are large for some agent \(i G_{j}\), by Property 1, we know that there are at most \(^{d}_{i}\) items in \(B_{j}\). Thus \(i\) can pack all items in \(B_{j}\) using no more than \(^{d}_{i}\) bins.

For the other case, recall that the agent \(i_{1} G_{j}\) who has more large items in \(B_{j}\) receives the items in \(W_{i_{1}}(B^{}_{j})\), and the other agent \(i_{2}\) receives the items in \(J_{i_{1}}(B^{}_{j})\{e^{*}_{j}\}\). We first discuss agent \(i_{1}\). By Property 1, we know that for each agent \(i\{i_{1},i_{2}\}\), there are at most \(^{d}_{i}\) large items in \(B^{}_{j}\) and \(s_{i}(B^{}_{j})(M)}{d}\). These two facts imply that in the imaginative assignment of \(B^{}_{j}\) to \(i_{1}\), no more than \(^{d}_{i_{1}}\) bins are used. Since otherwise, \(s_{i_{1}}(B^{}_{j})>^{d}_{i_{1}} c_{i_{1}}}(M)}{d}\), a contradiction. Therefore, \(i_{1}\) can pack all items in \(W_{i_{1}}(B^{}_{j})\) using no more than \(^{d}_{i_{1}}\) bins.

Next we discuss agent \(i_{2}\). Observe that in the imaginary assignment of \(B^{}_{j}\) to \(i_{1}\), for each extra item in \(J_{i_{1}}(B^{}_{j})\), there exists another item in the same bin with a larger size. Therefore, we have

Figure 1: Imaginary assignment of \(B^{}_{j}\) to agent \(i\)’s bins

\(s_{i_{2}}(J_{i_{1}}(B^{}_{j}))}(B^{}_{j})}{2} }(M)}{2d}\). Combining with the fact that there are at most \(^{d}_{i_{2}}\) large items in \(B^{}_{j}\) for \(i_{2}\), we know that \(i_{2}\) can use no more than \(^{d}_{i_{2}}\) bins to pack all items in \(J_{i_{1}}(B^{}_{j})\) and there exists one bin with at least half the capacity not occupied. Since otherwise, \(s_{i_{2}}(J_{i_{1}}(B^{}_{j}))>^{d}_{i_{2}}}}{2}}(M)}{2d}\), a contradiction. Recall that the last item \(e^{*}_{j}\) is small for \(i_{2}\), it can be put into the bin that has enough unoccupied capacity. Therefore, \(i_{2}\) can also pack all items in \(J_{i_{1}}(B^{}_{j})\{e^{*}_{j}\}\) using at most \(^{d}_{i_{2}}\) bins, which completes the proof.

For the multiplicative relaxation of MMS, by Theorem 3 and Observation 1, a \(}}\)-MMS allocation is guaranteed. Actually, we can slightly modify Algorithm 2 to compute a 2-MMS allocation.

**Corollary 2**: _A 2-MMS allocation always exists for any bin packing instance._

**Proof.** To compute a 2-MMS allocation, we replace the value of \(d\) with \(n\) in Algorithm 2 and select only one agent in each round who receives the bag in that round. The modified algorithm is formally presented in Algorithm 3. Following the same reasonings in Parts 1 and 2 (i.e., Subsubsections 4.2.1 and 4.2.2), it is not hard to see that all items can be allocated in Algorithm 3 and for any \(i N\), there are at most \(_{i}\) large items in \(A_{i}\). Besides, if the last item \(e^{*}_{i}\) is small for \(i\), we have \(s_{i}(A_{i}\{e^{*}_{i}\})(M)}{n}\). Again, in the imaginary assignment of \(A_{i}\{e^{*}_{i}\}\) to \(i\), no more than \(_{i}\) bins are used and at least one of them does not have an extra item. Therefore, agent \(i\) can use \(_{i}\) bins to pack all items in \(W_{i}(A_{i}\{e^{*}_{i}\})\) and another \(_{i}\) bins to pack all items in \(J_{i}(A_{i}\{e^{*}_{i}\})\{e^{*}_{i}\}\), which completes the proof.

In Appendix D.2, we show that the above multiplicative ratio is actually tight in the sense that there exists an instance where no allocation is better than 2-MMS. Besides, in Appendix D.3, we show that the algorithm that proves Corollary 2 actually computes an allocation where every agent \(i\) can use at most \(_{i}+1\) bins to pack all the items allocated to her.

## 5 Job scheduling setting

The second setting encodes a job scheduling problem where the items are jobs that need to be processed by the agents. Each item \(e_{j} M\) has a size \(s_{i,j} 0\) to each agent \(i N\), and for a set of items \(S M\), \(s_{i}(S)=_{e_{j} S}s_{i,j}\). As the bin packing setting, we only consider IDO instances where \(s_{i,1} s_{i,m}\) for all \(i\). Each agent \(i N\) exclusively controls a set of \(k_{i}\) machines \(P_{i}=[k_{i}]\) with possibly different speed \(_{i,l}\) for each \(l P_{i}\). Without loss of generality, we assume \(_{i,1}_{i,k_{i}}\). Upon receiving a set of items \(S M\), agent \(i\)'s cost \(v_{i}(S)\) is the minimum completion time of processing \(S\) using her own machines \(P_{i}\) (i.e., _the makespan of \(P_{i}\)_). Formally,

\[v_{i}(S)=_{(T_{1},,T_{k_{i}})_{k_{i}}(S)}_{l[k_{i}]}  T_{l}}s_{i,t}}{_{i,l}}.\]Note that the computation of \(v_{i}(S)\) is NP-hard if \(k_{i} 2\). For any two sets \(S_{1}\) and \(S_{2}\), \(v_{i}(S_{1} S_{2}) v_{i}(S_{1})+v_{i}(S_{2})\) since the makespan of scheduling \(S_{1} S_{2}\) is no larger than the sum of the makespans of scheduling \(S_{1}\) and \(S_{2}\) separately, thus \(v_{i}()\) is subadditive.

Regarding the value of \(_{i}^{d}\), intuitively, it is obtained by partitioning the items into \(d k_{i}\) bundles, and allocating them to \(k_{i}\) different types of machines (with possibly different speeds) where each type has \(d\) identical machines so that the makespan is minimized.3 Note that when each agent controls a single machine, i.e., \(k_{i}=1\) for all \(i\), the problem degenerates to the additive cost case, and thus the job scheduling setting strictly generalizes the additive setting.

For the job scheduling setting, we have the following two main results.

**Theorem 4**: _A 1-out-of-\(\) MMS allocation always exists for any job scheduling instance._

**Corollary 3**: _A 2-MMS allocation always exists for any job scheduling instance._

Note that simply partitioning the items into \(n\) bundles in a round-robin fashion does not guarantee 1-out-of-\(\) MMS even for the simpler additive cost setting. Consider an instance where there are four identical agents and five items with costs 4, 1, 1, 1, 1, respectively. For this instance, the value of 1-out-of-\(\) MMS for each agent is 4 as the 1-out-of-\(2\) MMS defining partition is \(\{\{4\},\{1,1,1,1\}\}\). However, the round-robin algorithm allocates two items with costs 4 and 1 to one agent, who receives a cost more than her 1-out-of-\(d\) MMS. Our algorithm overcomes this problem by first partitioning the items into \(\) bundles and then carefully allocating the items in each bundle to two agents.

Let \(d=\). For each agent \(i N\) and each machine \(l P_{i}\), let \(c_{i,l}=_{i,l}_{i}^{d}\) denote \(l\)'s capacity. In a nutshell, our algorithm consists of three parts: in the first part, we partition all items into \(d\) bundles in a round-robin fashion. In the second part, for each of the \(d\) bundles and each agent, we present an imaginary assignment of the items in the bundle to the agent's machines. These imaginary assignments are used in the third part to guide the allocation of the items in each of the \(d\) bundles to two agents, such that each agent can assign her allocated items to her machines in a way that the total workload on each machine does not exceed its capacity (in other words, each agent's cost is no more than her 1-out-of-\(d\) MMS). We defer the detailed algorithms and proofs to Appendix E.

## 6 Conclusion

In this work, we study fair allocation of indivisible chores when the costs are beyond additive and the fairness is measured by MMS. There are many open problems and further directions. First, there are only existential results for \(\{n, m\}\)-MMS allocations in the general subadditive cost setting and 1-out-of-\(\) MMS allocations in the job scheduling setting. Polynomial-time algorithms that achieve the same results remain as open problems. Second, for the general subadditive cost setting and the bin packing setting, we provide the tight approximation ratios, but for the job scheduling setting, we only have a lower bound of \(\), which is inherited from the additive cost setting . One immediate direction is to design better approximation algorithms or lower bound instances for the job scheduling setting. Third, in the appendix, we show that for the bin packing setting, there exists an allocation where every agent's cost is no more than \(\) times her MMS plus 1. We suspect that the multiplicative factor can be improved to 1. Fourth, for the job scheduling setting, we restrict us on the case of related machines in the current work, it is interesting to consider the general model of unrelated machines. As we have mentioned, the notion of collective maximin share fairness in the job scheduling setting can be viewed as a group-wise fairness notion, which could be of independent interest. Finally, we can investigate other combinatorial costs that can better characterize real-world problems.