# Most Influential Subset Selection: Challenges, Promises, and Beyond

Yuzheng Hu\({}^{1}\) Pingbang Hu\({}^{2}\) Han Zhao\({}^{1}\) Jiaqi W. Ma\({}^{2}\)

\({}^{1}\)Department of Computer Science \({}^{2}\)School of Information Sciences

University of Illinois Urbana-Champaign

{yh46,pbb,hanzhao,jiaqima}@illinois.edu

###### Abstract

How can we attribute the behaviors of machine learning models to their training data? While the classic influence function sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the Linear Datamodeling Score, and offering a range of discussions.

## 1 Introduction

Unraveling the intricate connections between data and model predictions is critical in machine learning, particularly in high-stakes decision-making contexts such as healthcare, economics, and public policy (Bracke et al., 2019; Rudin, 2019; Amarasinghe et al., 2023). A better understanding of these connections allows tackling tasks like data cleaning (Teso et al., 2021), model debugging (Guo et al., 2021), and assessing the robustness of inferential results (Broderick et al., 2020), all key to enhancing model interpretability and fostering trust between machine learning practitioners and domain experts. Among the various methodologies, the influence function adopted by Koh and Liang (2017) stands out as a particularly effective tool, sparking extensive research into identifying influential individual samples (Barshan et al., 2020; Schioppa et al., 2022; Grosse et al., 2023).

Nevertheless, focusing solely on the influence of individual samples is often insufficient. In many scenarios, it is necessary to understand how sets of samples jointly affect model predictions. These include uncovering biases associated with specific demographic groups (Chen et al., 2018), fairly allocating credits among crowdworkers (Arrieta-Ibarra et al., 2018), and detecting trends and signals that emerge collectively within the data (Yang et al., 2020). Gaining such insights is crucial for a more comprehensive understanding of model behaviors.

In pursuit of advancing this field, in this paper, we delve into the most influential subset selection (MISS) problem (Fisher et al., 2023). MISS attempts to find a set of samples that, when removed from the training set, results in the most significant change of a pre-defined target function. In essence, it measures the _worst-case_ collective influence.

Contributions.We provide a comprehensive analysis of existing algorithms to tackle MISS, revealing their weaknesses and strengths, and discussing the challenges and important considerations for future research. To summarize our contributions:

* We systematically study the failure modes of _influence-based greedy heuristics_, a dominant class of algorithms in MISS that assign a static score to each sample and subsequently perform a greedy selection. Specifically, the error of influence function, as well as the inability to incorporate the non-additive structure of the collective influence, can cause these heuristics to fail in MISS even in simple linear regression.
* In contrast, we demonstrate the effectiveness of the _adaptive greedy algorithm_ that dynamically updates the score for each remaining sample in response to selections already made. The improvement mainly comes from its ability to capture the nuanced interactions among samples.
* We conduct experiments on both synthetic and real-world datasets. The experimental results not only corroborate the theoretical findings but also extend to more complex settings including classification tasks and non-linear models, showcasing the consistent benefits of adaptivity.
* We discuss the inherent trade-offs between performance and efficiency in MISS, and the potential drawbacks of additive metrics such as Linear Datamodeling Score, among others.

Concurrent work.We acknowledge a concurrent work (Huang et al., 2024), which was posted around the same time as ours. Huang et al. (2024) investigate the Maximum Influence Perturbation problem (Broderick et al., 2020), which is equivalent to MISS. Both studies analyze the additive assumption and the adaptive greedy algorithm in OLS, but they differ in the theoretical results. Notably, we formally prove the failure of LAGS in solving MISS under a specific data generation process, uncovering the phenomena of amplification and cancellation. Huang et al. (2024) analyze the approximation error of variants of LAGS by comparing the closed-form expression of the approximate algorithm and the actual effect.

## 2 Preliminaries

### Problem statement

Consider a prediction task (e.g., regression or classification) with an input space \(^{d}\) and a target space \(\). The prediction task aims to learn a function \(f(,):\) parameterized by \(^{q}\). Specifically, denote \(\{(x_{i},y_{i})\}_{i=1}^{n}\) as the training samples and \(L(,)\) as the loss function (e.g., squared error or cross-entropy), we aim to solve the following optimization problem:

\[=*{arg\,min}_{^{q}} _{i=1}^{n}L(f(,x_{i}),y_{i}).\] (1)

A key notion for analyzing the influential samples is the optimal model parameters after removing a subset of training samples. Denote \([n]=\{1,2,,n\}\) and the set of indices as \(S[n]\), this corresponds to

\[_{-S}=*{arg\,min}_{^ {q}}_{i S}L(f(,x_{i}),y_{i}).\] (2)

Note that we do not adjust the normalizing constant as it does not affect the optimal solution to Eq. (2). Finally, denote \(:^{q}\) as the _target function_, which takes the model parameters as input and returns a quantity of interest (e.g., the prediction on a test sample or the sign of its first coefficient). We now formally define the most influential subset selection problem.

**Definition 2.1** (Most Influential Subset Selection (MISS)).: _Given a positive integer \(k n\), the \(k\)-Most Influential Subset Selection (\(k\)-MISS) problem refers to this discrete optimization problem:_

\[S_{,k}=*{arg\,max}_{S[n],|S| k }A_{-S},A_{-S}(_{-S})-().\] (3)We refer to \(A_{-S}\) as the _actual effect_ of removing \(S\). For clarity, we refer to the actual effect as the _individual effect_ when \(|S|=1\) and the _group effect_ otherwise. Essentially, MISS aims to identify a subset with bounded size, such that its removal from the training samples will lead to the maximum actual effect. It can be viewed as analogous to adversarial examples (Biggio et al., 2013; Szegedy et al., 2014), in that both characterize the alteration of model behaviors in the _worst case_, but MISS operates on the training data space and during training time.

Unfortunately, the naive approach of enumerating all possible subsets has an exponential time complexity in \(k\), rendering it computationally intractable in practice. In fact, even in the context of linear regression, a variant of MISS (where the target function depends on \(S\)) known as _robust regression_(Andersen, 2007) is proved to be NP-hard (Price et al., 2022). To tackle this challenge, researchers have proposed various greedy heuristics to select an _approximately_ most influential subset.

### Influence-based greedy heuristics

One of the most prominent algorithms for MISS, ZAMinfluence, was introduced by Broderick et al. (2020) and applied to assess the robustness of inferential results in earlier econometric studies (Atanasio et al., 2015; Angelucci et al., 2015). It builds upon the classic influence function (Koh and Liang, 2017) from robust statistics literature (Hampel, 1974; Hampel et al., 2005), extending its application from individual samples to a set of samples. A similar approach has been employed by Koh et al. (2019) to estimate group effects. We defer a detailed review of the literature to Section 7.

**Definition 2.2** (Upweighted objective).: _We denote the optimal solution to the upweighted objective w.r.t. a set of indices \(S\) as_

\[_{-S}()*{arg\,min}_{^{q}}_{i=1}^{n}L(f(,x_{i}),y_{i})+_{i S}L(f( ,x_{i}),y_{i}).\] (4)

It is straightforward to see that \(=0\) corresponds to \(\), while \(=-\) corresponds to \(_{-S}\). Similar to the influence function of individual samples (Koh and Liang, 2017), the influence of a set \(S\) can be characterized by the local perturbation of \(_{-S}()\) around \(=0\). This quantity is well-defined when \(L\) is strictly convex and can be computed via the Implicit Function Theorem (Krantz and Parks, 2002).

**Definition 2.3** (Influence function of a set).: _The influence of upweighting \(S\) on the parameters is:_

\[(S)_{-S}()}{ }_{=0}=-H_{}^{-1}_{i S}_{} L(f(,x_{i}),y_{i}),\] (5)

_where \(H_{}=_{i=1}^{n}_{}^{2}L(f(, x_{i}),y_{i})\) is the Hessian of the loss function at \(\)._

Using the chain rule and note that \(_{-S}=_{-S}(-)\), the actual effect can be estimated via the first-order approximation:

\[A_{-S}-(_{-S}())} {}_{=0}=_{}()^{}H_{}^{-1}_{i S}_{}L(f(,x_{i}),y_{i}).\] (6)

The key observation is that the right-hand side of Eq.(6) displays an _additive_ structure so that the group effect can be approximated by a summation of individual influences. This naturally yields the ZAMinfluence algorithm, which involves 1) calculating \(v_{i}=_{}()^{}H_{}^{-1}_{ }L(f(,x_{i}),y_{i})\) for each \(i[n]\); 2) sorting \(v_{i}\)'s; 3) returning the top \(i\)'s with positive \(v_{i}\). In fact, a series of studies in MISS (Wang et al., 2023; Yang et al., 2023; Chhabra et al., 2024) follow a similar approach: they score individual samples using variants of influence functions, and then greedily select those with the highest positive scores. We refer to these algorithms as _influence-based greedy heuristics_.

These heuristics are powerful in two aspects. The first is their broad applicability: they can be applied to _any_\(Z\)-estimator of a twice-differentiable objective function (Broderick et al., 2020) to obtain an influential subset w.r.t. _any_ differentiable target function. The second is their computational efficiency: once we have computed the scores for each sample, they can be executed in linear to log-linear time complexity. However, a major drawback of these heuristics is the lack of _provable_ guarantees. It is well-known that even the influence estimates of individual samples can be fragile and erroneous,especially in complex models like neural networks (Basu et al., 2021; Bae et al., 2022). A more significant concern lies in the additivity assumption implicitly adopted by these heuristics (also see Guu et al. (2023) for discussions), as it fails to account for the interactions among samples. We critically examine these issues in Section 3.

## 3 Pitfalls of greedy heuristics in Most Influential Subset Selection

In this section, we delve into the influence-based greedy heuristics introduced in Section 2, providing a comprehensive study of their limitations in solving MISS within the context of linear regression.

Setup and notation.In standard linear regression, each \(x_{i}^{d}\) represents a vector of covariates, and \(y_{i}\) stands for a real-valued label. The first coordinate of each \(x_{i}\) is set to \(1\) to account for the intercept term. We stack the row vectors \(x_{i}^{}\) to form the design matrix \(X^{n d}\) and concatenate the \(y_{i}\)'s into the target vector \(y^{n}\). We assume the labels are generated as follows: there exists a \(^{*}^{d}\) (note \(q=d\)), a noise parameter \(>0\) and some \(p\), such that

\[e=(,0,,0,p)^{}^{n}, y=X ^{*}-e.\] (7)

For a subset \(S\), \(X_{S}\) and \(y_{S}\) denote the corresponding covariates and responses, while \(X_{-S}\) and \(y_{-S}\) represent their complements. To ensure the uniqueness of the optimal solution, we assume \(N=X^{}X\) is invertible, and that \(_{i=2}^{n-1}x_{i}x_{i}^{}\) is also invertible (when this assumption is violated, our results naturally extend to ridge regression). The hat matrix is denoted as \(H=XN^{-1}X^{}\). The diagonal element \(h_{ii}\) of \(H\) represents the _leverage score_ of \(x_{i}\), and the off-diagonal element \(h_{ij}\) represents the _cross-leverage score_(Chatterjee and Hadi, 2009) between \(x_{i}\) and \(x_{j}\). The Ordinary Least Squares (OLS) estimator is given by

\[=*{arg\,min}_{}\|X -y\|^{2}=N^{-1}_{i=1}^{n}x_{i}y_{i}.\] (8)

Let \(_{i}=x_{i}^{}\) be the prediction and \(r_{i}=_{i}-y_{i}\) be the negative residual for the \(i\)-th sample. Throughout Sections 3 and 4, we focus on the linear target function \(()=x_{}^{}\) for \(x_{}=+px_{n}}{p+1}\), whose first coordinate is also \(1\). This choice of \(x_{}\) is intentional: it greatly simplifies the analysis by making most of the individual effects negative, as reflected in Figures 1 to 3 and the calculations in Appendix A.1. Furthermore, due to the continuous nature of the problem, our conclusions hold for a set of \(x_{}\) with non-zero Lebesgue measure.

### Influence function is not accurate (even) in linear models

Influence function is widely acknowledged as an accurate alternative of leave-one-out re-training in linear models (Koh and Liang, 2017; Basu et al., 2021; Bae et al., 2022). In this section, however, we challenge this viewpoint by pointing out a previously overlooked fact: the influence function fails to incorporate the leverage scores of individual samples in linear regression, which could result in its failure in selecting the most influential sample (i.e., \(1\)-MISS).

Plugging the squared loss into Eq. (5), we have \((S)=-nN^{-1}_{i S}x_{i}r_{i}\). Therefore, ZAMinfluence assigns \(v_{i}=x_{}^{}N^{-1}x_{i}r_{i}\) to each sample. We refer to them as _influence estimates_. On the other hand, it is well-known in the statistics literature (Beckman and Trussell, 1974; Cook, 1977) that

\[_{-\{i\}}-=x_{i}r_{i}}{1-h_{ii}}.\] (9)

Figure 1: Influence estimates suffer from disparate levels of under-estimation, leading to the failure of \(1\)-MISS

Consequently, the change in the target function is given by \(A_{-\{i\}}=^{}N^{-1}x_{i}i_{i}}{1-h_{ii}}\), which deviates from the influence estimate by a factor of \(1/(1-h_{ii})\) and implies under-estimation (a phenomenon which was also reported in Koh et al. (2019)). This is particularly concerning when a sample has a high leverage score (e.g., an outlier (Chatterjee and Hadi, 1986)): in this case, the influence function substantially under-estimates the individual effect, potentially leading to the failure of \(1\)-MISS. We illustrate this intuition in Figure 1: while point \(\) is scored highest by the influence function, it is however removing point \(\) (which has the highest leverage score) that leads to the greatest change in the prediction on the test sample. More generally, we present the following theorem illustrating the failure of ZAMinfluence in \(1\)-MISS, with the proof detailed in Appendix A.2.

**Theorem 3.1**.: _Assume \(h_{11}>h_{nn}\). Under the label generation process described in Eq. (7), there exists some \(p\), such that ZAMinfluence fails to select the most influential sample._

**Takeaway:** Even when the influence estimates have high _correlation_ with the individual effects, they can be misleading for extreme samples. As a result, the influence function may not be a reliable tool for MISS.

### Violation of the additivity assumption: amplification and cancellation

Note that the individual effects \(A_{-\{i\}}\)'s can be computed efficiently for linear regression (this is generally infeasible for more complicated tasks) by correcting the influence estimates \(v_{i}\)'s with their corresponding leverage scores. Hence, a natural alternative is to directly perform greedy selection based on the \(A_{-\{i\}}\)'s. We refer to this method as _Leverage-Adjusted Greedy Selection_ (LAGS). Nevertheless, we will illustrate in this section that even with perfect individual influence estimation, LAGS may still fall short in MISS due to violations of the additivity assumption.

We start by computing the closed-form of \(A_{-S}\). The proof can be found in Appendix A.3.

**Proposition 3.2**.: _For any set of indices \(S\), we have_

\[A_{-S}(_{-S})-()=x_{}^{ }N^{-1}X_{S}^{}(I_{k}-X_{S}N^{-1}X_{S}^{})^{-1}(X_{S} -y_{S}).\] (10)

**Remark 3.3**.: _Denote \(M_{S}=X_{S}N^{-1}X_{S}^{}\). It is straightforward to see that replacing the Neumann series \((I_{k}-M_{S})^{-1}=I_{k}+M_{S}+M_{S}^{2}+\) by the identity matrix yields the influence estimates, i.e., the first-order approximation. We further prove in Appendix A.4 that there is a one-to-one correspondence between the Taylor series of \(_{-S}()\) and the Neumann series: for any \(k^{+}\), the \(k\)-th order approximation of \(_{-S}()\) is equivalent to truncating the Neumann series at \(M_{S}^{k-1}\). On the other hand, LAGS is based on the diagonal approximation of \((I_{k}-M_{S})\)._

To systematically study the failure mode of LAGS, we consider \(S=\{i,j\}\). In this case,

\[A_{-\{i,j\}} =x_{}^{}()N^{-1}x_{i}r_{i}+ (1-h_{ii})N^{-1}x_{j}r_{j}+h_{ij}N^{-1}(x_{i}r_{j}+x_{j}r_{i})}{(1-h_{ii})(1-h_ {jj})-h_{ij}^{2}})\] \[=)(1-h_{jj})(A_{-\{i\}}+A_{-\{j\}})+h_{ij}x_{ }^{}N^{-1}(x_{i}r_{j}+x_{j}r_{i})}{(1-h_{ii})(1-h_{jj})-h_{ij} ^{2}}.\] (11)

From Eq. (11), we identify two primary factors contributing to the non-additivity of the group effect: the cross-leverage score \(h_{ij}\) in the denominator, which can lead to _super-additivity_ by inflating the sum of individual effects, and the cross terms \(x_{}^{}N^{-1}(x_{i}r_{j}+x_{j}r_{i})\) in the numerator, which may result in _sub-additivity_ through the neutralization of individual effects. We refer to these phenomena as "amplification" and "cancellation," respectively, and will delve into how they provably lead to the failure of LAGS in what follows.

**Amplification.** Amplification occurs when the group effect of a set substantially exceeds the sum of individual effects. As suggested by Eq.(11), this phenomenon is pronounced when the cross-leverage score is high. Therefore, we focus on scenarios where there are \(c 2\) identical copies of a sample, in which case the cross-leverage score becomes the leverage score. Intuitively, this setting can be generalized to a cluster of similar samples. We first prove a useful result in this context.

**Proposition 3.4**.: _Suppose there are \(c\) copies of \((x_{i},y_{i})\). We have_

\[^{c}}}{A_{-\{i\}}}=)}{1-ch_{ii}}>c,\] (12)_where \(A_{-\{i\}^{c}}\) denotes the group effect of removing all \(c\) copies of \((x_{i},y_{i})\)._

The proof can be found in Appendix A.5. It suggests that the group effect not only surpasses the sum of individual effects, but their ratio can be unbounded as \(h_{ii}\). Put differently, a sample with minor influence can collectively cause a substantial effect when grouped with similar ones. In MISS, this could lead to the failure of LAGS when there is a cluster of samples with high leverage scores yet do not have the largest individual effects. This intuition is illustrated in Figure 2: while points and \(\) (the pink cluster) have the highest individual effects due to their large residuals, points and (the green cluster) with high leverage scores constitute the most influential size-\(2\) subset.

We show a generalization of this example in the following theorem and defer its proof to Appendix A.6.

**Theorem 3.5**.: _Suppose there are \(c\) copies of \((x_{1},y_{1})\) and \((x_{n},y_{n})\), and that \(h_{11}>h_{nn}\). Under the label generation process described in Eq. (7), there exists some \(p\), such that LAGS fails in \(c\)-MISS._

Cancellation.Cancellation happens when the group effect of a set \(S\) is less than one of its subsets \(S^{}\), indicating that removing \(S S^{}\) induces a negative effect.

In this case, cancellation is equivalent to \(A_{-\{1,n\}}<A_{-\{n\}}\) (we assume w.l.o.g. that \(A_{-\{n\}}>A_{-\{1\}}\)). From Eq.(11), this inequality is likely to hold when \(A_{-\{1\}}\) has a small magnitude compared to \(A_{-\{n\}}\), and the sign of \(h_{1n}\) differs from that of \(}{r_{1}}\). If we further have that \(A_{-\{1\}}\) and \(A_{-\{n\}}\) are the top-\(2\) positive individual effects (which guarantees that they will be selected by the greedy algorithm), then LAGS will fail in this context.

We illustrate this in Figure 3: although points and have the top-\(2\) individual effects and are positive, their group effect as a size-\(2\) subset is less than the individual effect of point.

We present a more general result in the following theorem and defer its proof to Appendix A.7.

**Theorem 3.6**.: _Assume \(h_{1n} 0\). Under the label generation process described in Eq. (7), there exists some \(p\), such that LAGS fails in \(2\)-MISS._

**Takeaway:** LAGS provably works for MISS when all cross-leverage scores are zero, but can fail with even a single non-zero cross-leverage score. This highlights the algorithm's fragility.

## 4 Promises of the adaptive greedy algorithm

Given the limitations of LAGS, a pertinent question arises: is it possible to capture the non-additive structure of the joint effect without enumerating subsets? In this section, we examine a refined heuristic proposed by Kuschnig et al. (2021), and provide a theoretical analysis following our framework in Section 3. Kuschnig et al. (2021) originally introduced this refined algorithm in the

Figure 3: LAGS fails in \(2\)-MISS due to cancellation

Figure 2: LAGS fails in \(2\)-MISS due to amplification

context of linear regression, which applies to general influence-based greedy heuristics. The idea is to _adaptively_ build the influential subset. Specifically, the algorithm works by 1) refitting the model on the current dataset and recalculating the individual effect or influence estimate for each sample; 2) excluding the most influential sample from the current dataset; 3) adding it to the influential subset. This iterative process is repeated until the subset reaches the desired size. We refer to this as the _adaptive greedy algorithm_.

It is empirically observed that the adaptive greedy algorithm outperforms LAGS in linear regression (Kuschnig et al., 2021). In this section, we further aim to provide theoretical support for the benefits of _adaptivity_. Specifically, we will show that in scenarios where LAGS fail due to cancellation, the adaptive greedy algorithm can effectively address this problem by leveraging a scoring function that captures the marginal contributions relative to the removal of the most influential sample.

Following the cancellation setup, \((x_{n},y_{n})\) is the most influential sample w.r.t. the full dataset. We denote \(A^{}_{-\{i\}}\) as the actual effect of removing \((x_{i},y_{i})\) for \(1 i n-1\)_after_ the removal of \((x_{n},y_{n})\). Essentially, \(A^{}\) is the scoring function employed in the second step of the adaptive greedy algorithm. We start by proving two useful properties of \(A^{}\) (the proof is deferred to Appendix B.2).

**Proposition 4.1**.: _The scoring function \(A^{}\) satisfies the following properties:_

1. _Sign consistency:_ \(A^{}_{-\{i\}}\) _and_ \((A_{-\{i,n\}}-A_{-\{i\}})\) _have the same sign for_ \(1 i n-1\)_;_
2. _Order preservation:_ \(\{A^{}_{-\{i\}}\}_{i=2}^{n-1}\) _and_ \(\{A_{-\{i,n\}}\}_{i=2}^{n-1}\) _are order-isomorphic._

These properties have significant implications. The first property indicates that \(A^{}\) is a more reliable scoring function as it captures the marginal contribution of each sample _relative to the removal of \((x_{n},y_{n})\)_. Hence, in the cancellation setup, \(A^{}\) will not choose \((x_{1},y_{1})\), even though \(A_{-\{1\}}\) represents the second-largest individual effect and is positive. In contrast, the actual effect \(A\), which reflects the marginal contribution of each sample relative to the full dataset, does not account for how a newly selected sample interacts with those already selected. The second property further guarantees the success of MISS based on \(A^{}\). Formally, we prove the following for the adaptive greedy algorithm.

**Theorem 4.2**.: _Under the label generation process described in Eq.(7), suppose \(A_{-\{1\}},A_{-\{n\}}>0\), \(A_{-\{1,n\}}<A_{-\{n\}}\) (indicating cancellation), and that \(n S_{,2}\) (i.e., \((x_{n},y_{n})\) is contained in the most influential subset), then the adaptive greedy algorithm solves \(2\)-MISS._

Proof.: We first show that the condition \(A_{-\{1,n\}}<A_{-\{n\}}\) implies that \((x_{n},y_{n})\) is the most influential sample (the proof is deferred to Appendix B.3). This ensures that the adaptive greedy algorithm will select \((x_{n},y_{n})\) in the first step. We now discuss two cases separately.

**Case 1:** If \(A_{-\{i,n\}}-A_{-\{n\}}<0\) for every \(2 i n-1\), then \(S_{,2}=\{n\}\). Furthermore, by the first property of Proposition4.1 we have \(A^{}_{-\{i\}}<0\) for \(1 i n-1\). This implies that the adaptive algorithm will return \(\) in the second step since no scores are positive, as desired.

**Case 2:** If there exists some \(2 i n-1\), such that \(A_{-\{i,n\}}-A_{-\{n\}}>0\). We denote the most influential subset as \(S_{,2}=\{i^{*},n\}\). Since \(A_{-\{i^{*},n\}}-A_{-\{n\}}>0\), the first property of Proposition4.1 implies \(A_{-\{i^{*}\}}>0\). Furthermore, by the second property of Proposition4.1, the adaptive greedy algorithm will return the correct index \(i^{*}\) in the second step.

Combining the above two cases finishes the proof of Theorem4.2. 

**Remark 4.3**.: _In the cancellation setup, our theoretical results are restricted to 2-MISS. We identify two challenges: 1) Conceptually, it is not immediately clear how to define cancellation for more than two samples; 2) Technically, proving the success of MISS is much harder than constructing a counterexample since it requires enumerating all possible subsets, whose number grows exponentially with \(k\). We leave this as future work._

**Takeaway:** In essence, the critical limitation of LAGS and other influence-based greedy heuristics is their reliance on a _one-pass_ procedure that measures the contribution of each sample _solely in relation to the full training set_. On the other hand, the adaptive greedy algorithm considers more complex interactions between samples, akin to those in data Shapley (Ghorbani and Zou, 2019), leading to more effective subset selection.

Experiments

In this section, we empirically evaluate the efficacy of the adaptive greedy algorithm on real-world datasets by comparing the performance of the vanilla greedy algorithm _versus_ the adaptive greedy algorithm across a range of \(k\)'s.1 We cover the simple linear regression studied in Sections 3 and 4 as well as more complicated scenarios (including the classification task and non-linear neural networks) as a complement. Additional experiments on synthetic datasets can be found in Appendix C.1.

Evaluation metrics.We evaluate the algorithms using two metrics, the _averaged actual effect_ and the _winning rate_. Given a held-out test set, we define the averaged actual effect \(}\) as the mean of the actual effects w.r.t. each test point. A higher score of \(}\) indicates a more influential subset is selected on average. Additionally, we report the _winning rate_ across test data points in a held-out test set, namely the ratio of the algorithm outperforms the other one in terms of the actual effect \(A_{-S}\).

Target functions and greedy algorithms.We consider two types of tasks: regression and classification. For the regression task, we adopt the target function \(()=x_{}^{}\) on a given test point \(z:=(x_{},y_{})\). We utilize LAGS as the vanilla greedy algorithm. For the classification task, we consider the target function \(()=(p(z;)/(1-p(z;)))\), where \(p(z;)\) represents the softmax probability assigned to the correct class. We opt for the ZAMinfluence as the vanilla greedy algorithm.

Experimental setup.For regression, we choose a popular UCI dataset _Concrete Compressive Strength_. For classification, we experiment with a moderate-scale UCI tabular dataset _Waveform Database Generator_ and an image dataset MNIST . We apply logistic regression on the former and a simple 2-layer multi-layer perceptron (MLP) on the latter. We defer details of the datasets, train/test split, and MLP training to Appendix C.

Approximated actual effect.We address one unique challenge for the MLP: for neural networks, it is impossible to obtain the actual effect since the optimal model is not unique in general. To address this, we adopt an ensemble technique used in recent literature : averaging the target function's values from several independently trained models. Specifically, we train \(5\) models with the same initialization but different seeds. This works for both the greedy algorithm and evaluation: for the former, we estimate each model's influence with the ZAMinfluence algorithm and select the most influential subset based on the averaged influence; for the latter, we approximate the actual effect of a subset \(S\) by the averaged difference of the target values of each model, trained with or without \(S\).

While ensemble solves the non-uniqueness problem, it induces a significant computational burden. Noticeably, the adaptive greedy algorithm now requires retraining for (\(k\)\(\) number of ensembles) times. To mitigate it, we use an efficient approximate variant of the ZAMifluence estimation algorithm in our implementation and devise two strategies. We defer the concrete descriptions to Appendix C.4.

Figure 4: Adaptive Greedy v.s. Greedy Algorithm. **Row 1**: Averaged actual effect \(}\) measures the averaged actual effect induced by the greedy and adaptive greedy algorithms. **Row 2**: Winning rate indicates the proportion of instances where one algorithm outperforms the other.

Results.We present the main results in Figure 4. First, we see that as \(k\) increases, the averaged actual effect \(}\) given by both the vanilla and the adaptive greedy algorithms increase, which aligns with the intuition that removing a larger set \(S\) induces a greater joint effect \(}\). Furthermore, the adaptive greedy algorithm surpasses its vanilla counterpart across all scenarios and all \(k\)'s under both metrics. This implies that the benefits of adaptivity extend beyond linear regression and apply to more complicated scenarios like classification tasks and even non-linear neural networks.

Finally, for the experiment on MLP specifically, we report results of multiple random seeds in Appendix C.5 to account for the randomness in model training. The consistent results across different seeds demonstrate the robustness of the aforementioned conclusions.

## 6 Discussion

Failure of the adaptive greedy algorithm.While Theorem 4.2 demonstrates the advantages of the adaptive greedy algorithm, it is still not perfect. Specifically, the assumption \(n S_{,2}\) in Theorem 4.2 is actually necessary: if the most influential sample is not part of the most influential subset, the algorithm will make an error in the first step and cannot correct this mistake in subsequent procedures. For instance, under the amplification setup as in Theorem 3.5, it is straightforward to see that the adaptive greedy algorithms provably fail in \(c\)-MISS since it selects \((x_{n},y_{n})\) in the first place.

Second-order approximation.To more effectively capture the amplification effect caused by clusters of similar samples, it is essential to utilize algorithms that can detect higher-order interactions. In this context, the second-order group influence introduced by Basu et al. (2020) is a more powerful alternative. It is calculated based on the second-order approximation as described in Remark 3.3:

\[Q_{-S}=x_{}^{}N^{-1}X_{S}^{}(I_{k}+X_{S}N^{-1}X_{S}^{ })(X_{S}-y_{S}).\] (13)

From here, the original MISS can be cast as a quadratic optimization problem (see Appendix D.1) and solved via \(L_{1}\) relaxation and projected gradient descent. Furthermore, we have \(Q_{-\{1\}^{c}}=c^{2}v_{1}\|x_{1}\|^{2}+cv_{1},\;Q_{-\{n\}^{c}}=c^{2}v_{n}\|x_{ n}\|^{2}+cv_{n}\), indicating that quadratic approximation can capture the joint effect amplified by the leverage score by emphasizing the _norm_.

Submodular property.Given the challenges of finding an exact solution, it is tempting to explore approximate solutions to MISS with _provable_ guarantees. A classical result of Nemhauser et al. (1978) states that so long as the (set) value function satisfies the submodular property, the greedy algorithm will return a solution within a factor \(1-1/e\) of the optimum. While the value function associated with the first-order approximation is submodular due to linearity, we show in Appendix D.2 that this is generally not the case for \(Q_{-S}\). Since the second-order approximation is a more accurate estimation of the actual effect, this suggests that the actual effect is unlikely to be submodular either. Therefore, MISS is expected to be hard even when we allow approximate solutions.

The role of target function.Our negative results critically rely on the choice of \(x_{}\), underscoring the importance of the target function -- an issue that has been overlooked in prior research. In addition, we have identified a few target functions in which the influence-based greedy heuristics fail to provide meaningful results: 1) the change of norm, \(_{1}()=\|-\|^{2}\); 2) the training loss, \(_{2}()=\|X-y\|^{2}\). In both of these cases, we have \(_{}()=0\), implying that the scores assigned to each sample will also be \(0\).

Implication on Linear Datamodeling Score.Recently, Linear Datamodeling Score (LDS) (Park et al., 2023) has emerged as a prominent metric for evaluating data attribution algorithms (Zheng et al., 2024; Bae et al., 2024). Central to its design is the assumption that group influence is additive, which we critically examine in our work and reach a negative conclusion. This raises an important question: does a higher LDS result from a truly better data attribution algorithm, or are certain algorithms simply more aligned with the potentially flawed additive assumption? While LDS offers valuable insights into data attribution, we believe it is crucial for the research community to develop evaluation metrics that better capture the _non-additive_ and _contextual_ nature of training data influence.

Limitation and future direction.Despite thorough theoretical and empirical analyses, our study does not offer algorithmic improvements over existing research. We believe solving general MISS is a challenging problem, and hypothesize that there is an inherent trade-off between performance and computational efficiency, in which an increase in performance necessitates additional computing. This pattern is already reflected in the comparison between the vanilla and adaptive greedy algorithms, a trend that will likely continue in future research. To address this challenge, we suggest incorporating the knowledge of target function and data characteristics into algorithmic designs.

Related work

(Most) influential subset.Since the seminal work of Koh and Liang (2017), which utilized the influence function to identify influential individuals, subsequent research has explored finding an influential _set_ of samples (Khanna et al., 2019, Basu et al., 2020, Broderick et al., 2020). Among them, a notable example is the ZAMinfluence algorithm by Broderick et al. (2020), which builds on the group influence function (Koh et al., 2019) and greedily selects an approximately most influential subset. ZAMinfluence is particularly renowned for its broad applicability: it can be used to improve various dimensions of machine learning such as pre-training (Wang et al., 2023), dataset pruning (Yang et al., 2023), and trustworthiness (Wang et al., 2022, Sattigeri et al., 2022, Chhabra et al., 2024), as well as to assess the sensitivity of inferential results in multiple domains such as applied econometrics (Attanasio et al., 2015, Angelucci et al., 2015), economics (Finger and Mohring, 2022, Martinez, 2022), and social science (Eubank and Fresh, 2022). Additionally, Kuschnig et al. (2021) proposed a refined version of ZAMinfluence based on iteratively refitting the model and removing the most influential sample, an approach which was also explored in Yang et al. (2023).

Theoretical understanding of MISS.Despite its empirical success, the theoretical understanding of ZAMinfluence and other influence-based greedy heuristics remains limited. Giordano et al. (2019, 2019) provided finite sample error bounds between the approximated and actual effects, but consistency (i.e., the error uniformly converges to \(0\) for all subsets as the sample size goes to infinity) is only achieved as the fraction of removed samples \(\) approaches zero. Fisher et al. (2023) extended the analysis to any fixed \(0<<1\), but their consistency is not directly related to the actual effect, thus offering limited insights for MISS. Moitra and Rohatgi (2023), Freund and Hopkins (2023) examined finite-sample stability (i.e., the minimum number of samples that need to be dropped in order to flip the sign of a coordinate) in linear regression and proposed algorithms with provable guarantees, yet they are confined to highly specific scenarios, such as very low dimensions or binary design matrices. Saunshi et al. (2023) explored the additivity assumption in group influence within a different yet less interpretable framework. We position our work as the first to provide a fine-grained analysis of the common practices in MISS, shedding light on the limitations of influence-based greedy heuristics as well as the potential of the adaptive greedy algorithm.

Multiple outlier detection.Classical tools in statistics, such as Cook's distance and its variants, can detect a single outlier in linear regression (Cook, 1986, Chatterjee and Hadi, 1986) and generalized linear models (Wojnowicz et al., 2016). Nevertheless, they struggle with multiple outliers due to the well-known phenomena of _swamping_ and _masking_(Rousseeuw and Leroy, 1987, Hadi and Simonoff, 1993). This challenge has motivated a line of research in regression diagnostics (Fox, 2019), known as _multiple outlier detection_. Prominent approaches include clustering (Gray and Ling, 1984, Hadi, 1985), influence matrix (Pena and Yohai, 1995), and a class of iterative procedures (Belsley et al., 1980, Hadi and Simonoff, 1993, She and Owen, 2011, Roberts et al., 2015) that resemble Kuschnig et al. (2021). While seemingly alike, its key distinction from influential subset selection is that the 'outlier' is defined context-independently, rather than with respect to a specific quantity of interest.

Broader context.Our work falls under a broader research area that aims to attribute and interpret model behavior through the lens of data (a.k.a. data attribution). Beyond the influence function, which is central to our study, other popular approaches include the representer point method (Yeh et al., 2018), the data Shapley (Ghorbani and Zou, 2019, Jia et al., 2019), the TracIn algorithm (Pruthi et al., 2020), and more recently, the datamodels (Ilyas et al., 2022). For a comprehensive review of this subject, we refer readers to Hammoudeh and Lowd (2024). Finally, we emphasize that MISS should not be confused with data selection (John and Draper, 1975, Kolossov et al., 2023). While many data attribution algorithms can indeed be applied for data selection (e.g., a recent study Wang et al. (2024) demonstrated that the effectiveness of data Shapley in data selection hinges on the utility function), data selection remains an independent research area. It typically involves _subsampling_ a small fraction of the training data to enable effective and _data-efficient_ learning or estimation, differing from MISS in its objectives, methodologies, and applications.

## 8 Conclusion

We have provided a comprehensive study of common practices in MISS, revealing the failure modes of influence-based greedy heuristics and uncovering the benefits of adaptivity. We hope our work will enhance the transparency and interpretability of machine learning models by illuminating the collective influence of training data, and serve as a foundation for future algorithmic advancements.