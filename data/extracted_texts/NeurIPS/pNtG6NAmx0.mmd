# Statistical Knowledge Assessment

for Large Language Models

 Qingxiu Dong\({}^{1}\), Jingjing Xu\({}^{2}\), Lingpeng Kong\({}^{3}\), Zhifang Sui\({}^{1}\) and Lei Li\({}^{4}\)

\({}^{1}\) National Key Laboratory for Multimedia Information Processing,

School of Computer Science, Peking University

\({}^{2}\) Shanghai AI Lab \({}^{3}\) The University of Hong Kong \({}^{4}\) Carnegie Mellon University

dqx@stu.pku.edu.cn, {jingjingxu, szf}@pku.edu.cn, lpk@cs.hku.hk, leili@cs.cmu.edu

###### Abstract

Given varying prompts regarding a factoid question, can a large language model (LLM) reliably generate factually correct answers? Existing LLMs may generate distinct responses for different prompts. In this paper, we study the problem of quantifying knowledge contained in an LLM regarding a given set of facts. We propose KaRR, a statistical approach to assess factual knowledge for LLMs. The main idea is to estimate the ratio of LLM generating text corresponding to the answer entity given diverse prompts of the subject and the querying relation, versus it generating by random chances. Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes, including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a strong correlation (0.43 Kendall's \(\)) with the results of human assessment on LLMs. Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.

## 1 Introduction

Large language models (LLMs) have achieved impressive performance on many tasks including text generation, question answering, and dialog generation [Brown et al., 2020, Thoppilan et al., 2022, Bubeck et al., 2023]. Previous studies have shown that pretrained language models store factual knowledge in parameters [Roberts et al., 2020, Dai et al., 2022, 2, Singhal et al., 2023], demonstrating considerable potential as assistants for responding to user questions based on their stored knowledge.

Despite the remarkable success of LLMs, critical concerns arise -- LLMs often generate unreliable answers given varying prompts [Ji et al., 2023, Maharana et al., 2023, Chang and Bergen, 2023, Chen et al., 2023]. As shown in Fig. 1, reliability refers to the ability to consistently generate knowledge-correct text, posing a higher standard than accuracy. For example, although Alpaca-7B [Taori et al., 2023] generates accurately (_playwright_) given a prompt of "_William Shakespeare's job_", it generates incorrect text (_boatman_) given a prompt of "_the job of Swan of Avon is_"1. ChatGPT [OpenAI, 2022] correctly generates _playwright and teacher_. However, it answers "No" for the prompt "_Is William Shakespeare a teacher?_" In this paper, we assess the knowledge contained in an LLM and investigate whether an LLM is able to consistently generate factually correct answers given varying prompts. Knowledge assessment is a critical problem for LLMs. The assessment results directly affect the people's trust in the LLM generated content. Once we identify inconsistency of LLM generation, we could potentially correct such knowledge in LLMs [De Cao et al., 2021, Zhao et al., 2021, Dong et al., 2022].

Important steps have been taken in the automatic knowledge evaluation for pre-trained language models (Petroni et al., 2019; Poerner et al., 2020; Jiang et al., 2020). However, two issues still remain: 1) Prior methods focus on assessing the accuracy but not the reliability. Consequently, their probing results are prone to inflation or bias since models exhibit spurious correlations towards specific prompts (Poerner et al., 2020). 2) Most prior methods target masked language models (MLMs) and don't provide a universal solution for assessing the knowledge in generative LLMs. MLMs are evaluated by probing whether they predict the masked object in response to a given prompt in cloze form, while the freely generated results of generative LLMs might be irrelevant to factual knowledge. Therefore, there is a need for a method that assesses both the accuracy and consistency, i.e. reliability, of knowledge in LLMs.

In this paper, we introduce KaRR, a statistical approach to assess whether a large language model contains reliable factual knowledge. We use triplets to represent the probing facts (e.g. <William_Shakespeare, occupation, Playwright>). Our main idea is to estimate the ratio of the likelihood of LLM generating correct surface text for ground-truth object entities (Playwright) given varying prompts with subject entity (William_Shakespeare) and relation (occupation and the likelihood of generating entity text by pure chances. We distinguish the entities versus their surface text since each entity and relation may contain several text aliases. We introduce three latent variables to represent subject, object, and relation, and a Bayesian network to represent the causal dependency among them and generated text. Our proposed KaRR quantifies the ratio of generating text with and without a specified relation/subject. This metric effectively captures the combined impact of both the subject and the relation in determining the model generation probability for the plausible text of the object (e.g., "playwright", "dramatist").

We develop a large-scale assessment suite with 994,123 entities, 600 relations, and their text aliases. Our suite is orders of magnitude larger than prior studies which only cover very few relations. We evaluate a comprehensive set of 20 LLMs using our method, including LLAMA (Touvron et al., 2023), OPT (Zhang et al., 2022), and others. Moreover, we also invite human experts to assess knowledge in these LLMs. Results reveal a strong correlation (0.43 Kendall's \(\)) between KaRR and human evaluation. Our experimental results show that knowledge of LLMs with the same backbone architecture conforms to the scaling law.

In summary, our contributions are: (1) We propose KaRR, a statistical method to assess the reliable knowledge contained in LLMs. We present a probabilistic graphical model to tackle varying text aliases regarding a fact. (2) We develop a large-scale assessment suite with 994,123 entities and 600 relations, which will be released to the community for further study. (3) We conduct a comprehensive evaluation of 20 LLMs. The experiments show that KaRR exhibits a strong correlation (0.43 Kendall's \(\)) with human assessment, and achieves a low variance to varying prompts, with a standard deviation of 0.82. Furthermore, our evaluation results are seldom affected by spurious correlations. Our code and data are available at https://github.com/dqxiu/KAssess.

## 2 Related Work

Knowledge ProbingRecent studies have demonstrated that language models possess factual knowledge, which can be utilized for complex reasoning, question answering, and performing various tasks(Roberts et al., 2020; Choudhary and Reddy, 2023). To evaluate the knowledge in language models, previous works implement knowledge probing methods on MLMs. Petroni et al. (2019) introduces LAMA, studying whether MLMs correctly predict masked object entities in a cloze-style prompt. However, Elazar et al. (2021) shows that probing outcomes are inconsistent with various prompts, resulting in contradictory or unreliable results. Shin et al. (2020) show that

Figure 1: Accuracy pertains to generating the correct knowledge in response to a specific prompt, evaluated as a binary value (correct or incorrect). However, accuracy alone does not ensure reliability, as responses may vary with different prompt expressions. Reliability requires consistently generating correct knowledge for semantically similar prompts and is measured as a continuous value. Reliability sets a higher standard than accuracy by maintaining consistent accuracy across related prompts.

well-engineered automatic prompts result in much higher results than handcrafted LAMA prompts. Therefore, ParaRel Elazar et al. (2021) and LPAQA Jiang et al. (2020) are proposed for MLMs. ParaRel measures the knowledge consistency of MLMs through the use of several paraphrased prompts. Subsequently, Hase et al. (2023) adopt their metric for paraphrase consistency as a measure of belief, and Raj et al. (2022) extend the metric to measure the general semantic consistency of OPT Zhang et al. (2022). LPAQA focuses on prompts selection and ensembling to improve the prediction accuracy, by automatically discovering better prompts to query. However, it has not been comprehensively assessed to what extent LLMs contain knowledge about a fact.

Spurious Correlations in ProbingRecent literature reveals that empirical probing results for masked entities may be affected by spurious correlations within the text Poerner et al. (2020); Dong et al. (2022). Spurious correlations occur when a model takes shortcuts to generate answers based on superficial information in surface forms, even without possessing relevant knowledge. For instance, a model with limited knowledge can correctly answer "French" when queried about the native language of actor "Jean Marais", simply because "Jean Marais" is a typically French name. This phenomenon inflates probing results. On the one hand, Zhao et al. (2021) find that probing results are influenced by contextual biases, and Dong et al. (2022) also note frequency biases in rank-based probing. On the other hand, the subject's class has been shown to introduce biases in predictions Poerner et al. (2020). To address these issues, Poerner et al. (2020) eliminate vulnerable relation classes, however, this approach restricts coverage and does not provide a comprehensive solution. In our statistical knowledge assessment, we account for both the subject and relation by decomposing their influence and using multiple aliases to minimize spurious correlations.

## 3 Statistical Knowledge Assessment

In this section, we present a statistical method for assessing knowledge in LLMs. We begin by introducing the graphical model for knowledge assessment, which serves as the foundation of our approach. Subsequently, we introduce the Knowledge Assessment Risk Ratio (KaRR), which we then convert into a computable form based on the graphical model.

### Graphical Model for Knowledge Assessment

Fig. 3 shows the graphical model that describes the knowledge symbols and their corresponding text forms. Here, we introduce latent variables \(,\), and \(\) to represent the symbolic subject, relation, and object, respectively. We use \(s,r\), and \(o\) to refer to specific values of these variables, and the triplet \((s,r,o)\) to denote a simple fact/knowledge, such as <William_Shakespeare,occupation,playwright>. We introduce \(,,\) to denote the random variables for the textual forms (textual diases2) of \((),(,),(,,)\), respectively. Here, \(,\), and \(\) refer to specific values of \(,\), and \(\), such as \(\) being expressed as _"Shakespeare"_.

Figure 3: Graphical model for knowledge assessment.

Figure 2: Illustration of KaRR using the fact <William_Shakespeare,occupation,playwright> as an example. \(\) and \(\) are latent variables representing the subject and relation, respectively; \(s,r\) and \(o\) refer to specific values of a subject, relation and object, respectively. (a) illustrates KaRR\({}_{r}\), which measures the impact of specifying \(r\) or not on LLM generating \(o\) given \(s\). (b) illustrates KaRR\({}_{s}\), measuring the impact of specifying \(s\) or not on LLM generating \(o\) given \(r\).

In general, fully observing the LLM probability on the latent variables of symbolic knowledge is infeasible. Although we can calculate the probability of the model generating _"Shakespeare"_, we cannot calculate the probability of it generating the symbol <William_Shakespeare>. This is because LLM pretraining focuses on textual input rather than symbolic representations, and each symbol may encompass a multitude of textual variations (paraphrased expressions) that signify the same entity or relation. Fortunately, we can establish a connection between symbols and text forms, enabling the calculation of the model's probability for specific textual forms. As demonstrated in the graphical model, a hollow circle indicates the variable is latent, and a shaded circle indicates the variable is observed. The parent node of \(\) is \(\), while \(\) and \(\) are parents to \(\). Additionally, \(\) has three parent nodes, namely \(\), \(\), and \(\). This structure enables the calculation of model probability on symbols through their corresponding text forms. Therefore, the goal of consistent LLM knowledge assessment is to _estimate the model knowledge on symbols through the observable model probability across diverse corresponding textual forms_.

### Knowledge Assessment Risk Ratio

For a given LLM \(\), we define its factual knowledge correctness regarding a simple fact \((s,r,o)\) as the extent to which it generates knowledge-correct answers to various textual prompts. Specifically, we assess the joint impact of subject and relation symbols on the LLM's ability to generate the object symbol. This joint impact comprises two components: (1) the impact of specifying \(r\) or not on \(\) generating \(o\) given \(s\) and (2) the impact of specifying \(s\) or not on \(\) generating \(o\) given \(r\).

Risk Ratio (RR) is a statistical measure employed to compare the likelihood of a specific event occurring in an exposed group versus an unexposed group [Sistrom and Garvan, 2004]. To quantify the impact of specifying symbols (or not) on the LLM generation accuracy, we adapt RR into a new metric for knowledge assessment called KaRR. For component (1), we consider the exposed group as the case where a relation symbol \(r\) is specified, and the unexposed group as cases where the relation is not specified. The outcome is the probability of model generating the object symbol \(o\), representing any possible textual representation of \(o\). We denote the RR of specifying relation \(r\) as KaRR\({}_{r}\):

\[_{r}(s,r,o)=_{}[P(o|s, )]}=\] (1)

As the example illustrated in Fig. 2 (a), for the fact <William_Shakespeare,occupation, playwright>, KaRR\({}_{r}\) compare the probability of model generating any alias of the object entity playwright when the relation is specified as occupation(e.g., _"Shakespeare works as \(a\)"_, _"William Shakespeare's occupation is \(a\)"_), with that when the relation is not specified, the expectation of model probability on generating an object alias under various possible relations such as _"Shakespeare married \(a\)"_. Similarly, the Risk Ratio (RR) for a specified subject \(s\), denoted as KaRR\({}_{s}\), is defined as,

\[_{s}(s,r,o)=_{}[P(o|,r)]}=\] (2)

While the numerator for KaRR\({}_{s}\) is the same as that for KaRR\({}_{r}\), it compares the numerator with the model's probability when the subject is not specified. As shown in Fig. 2 (b), KaRR\({}_{s}\) calculates the expected model probability of generating an object alias across multiple possible subjects (e.g., _"Dante worked as \(a\)"_ and _"Thomas Edison worked as \(a\)"_). To effectively represent the combined impact of KaRR\({}_{s}\) and KaRR\({}_{r}\), we calculate the joint influence as the geometric mean of them, and name it Knowledge Assessment Risk Ratio (KaRR). Formally, we define KaRR on \((s,r,o)\) as,

\[(s,r,o)=_{r}(s,r,o)_{s}(s,r,o)}\] (3)

Intuitively, there are two components in KaRR: (1) the ratio of the chance of LLM generating \(o\) given \(s\) and \(r\), and that of LLM generating \(o\) only given \(s\); (2) the ratio of the chance of M generating \(o\) given \(s\) and \(r\), and that of LLM generating \(o\) only given \(r\).

### Computing KaRR using Graphical Model

The Knowledge Assessment Risk Ratio (KaRR), as indicated in Eq. (3), is formulated based on knowledge symbols, while directly computing the KaRR score using the definition (on symbols) is unfeasible. In light of this, our graphical model for knowledge assessment facilitates the implementation of KaRR by employing model probabilities on the text.

Based on Fig. 3, we can use the graphical model to evaluate the numerator of \(_{s}\) and \(_{r}\) in Eq. (1) and Eq. (2). To do so, we can use the following formula:

\[P(o|s,r)=_{k=1}^{||}P(o,_{k}|s,r)=_{k=1}^{||}P( _{k}|s,r) P(o|s,r,_{k})\] (4)

The probability is computed by summing over all possible values of \(\), namely the set of aliases for \((s,r)\). For each specific value of \(\), \(_{k}\), we apply the product rule of probability and decompose \(P(o,_{k}|s,r)\) into the probability of observing the relation \(_{k}\) given \(s\) and \(r\), denoted by \(P(_{k}|s,r)\), and the probability of observing the object \(o\) given the subject \(s\), relation \(r\), and the specific relation type \(_{k}\), denoted by \(P(o|s,r,_{k})\). We use \(P_{}\) to denote the generation probability of model \(\). According to the law of total probability for marginal distribution, \(P(o|s,r,_{k})\) can be expanded as,

\[P(o|s,r,_{k})=_{j=1}^{||}P(o,_{j}|s,r,_{k})= _{j=1}^{||}P_{}(_{j}|s,r,_{k})P(o|_{j})\] (5)

In the context of calculating the denominator of \(_{r}\), it is important to note that each subject symbol may possess multiple aliases. For instance, the subject symbol \(s\), <William_Shakespeare>, may be expressed with varying probabilities as \(_{1}\) (_"Shakespeare"_), \(_{2}\) (_"Swan of Avon"_), \(_{3}\) (_"William Shakespeare"_), and so forth. To account for this variability, we expand the denominator as demonstrated in Eq. (6) by leveraging principles of marginal distribution and Bayes Rule.

\[P(o|s)=_{i=1}^{||}P(o,_{i}|s)=_{i=1}^{||} P(_{i}|s)P(o|s,_{i})\] (6)

\(P(o|s,_{i})\) is obtained by summing the joint probabilities of \(o\) and \(_{j}\) given \(s,_{i}\) under all possible \(_{j}\). Mathematically, this can be expressed as:

\[P(o|s,_{i})=_{j=1}^{||}P(o,_{j}|s,_{i})= _{j=1}^{||}P_{}(_{j}|s,_{i})P(o|s,_{ i},_{j})\] (7)

Subsequently, we obtain \(RR_{r}(s,r,o)\) as follows,

\[_{r}(s,r,o)==^{|| }P(_{k}|s,r)_{j=1}^{||}P_{}(_{j}|s,r, _{k})P(o|_{j})}{_{i=1}^{||}P(_{i}|s)_{j=1} ^{||}P_{}(_{j}|s,_{i})P(o|s,_{i}, _{j})}\] (8)

For the denominator of \(_{s}\), in the above example, \(P(o|r)\) represents the expected probability of generating an alias for playwright when presented with a sentence that semantically conveys the occupation relation, over all the subject symbols. Consequently, we can expand \(P(o|r)\) as \(_{u=1}^{|_{u}|}P(s_{u}|r)P(o|s_{u},r)\). The computation of \(P(o|s_{u},r)\) is identical to that in Eq. (4). Combining these results, we can determine the value of \(_{s}(s,r,o)\) as Eq. (9).

\[_{s}(s,r,o)=^{||}P(_{k}|s,r)_{j= 1}^{||}P_{}(_{j}|s,r,_{k})P(o|_{j})}{ _{u=1}^{|_{u}|}P(s_{u}|r)_{k=1}^{||}P(_{k}|s _{u},r)_{j=1}^{||}P_{}(_{j}|s_{u},r,_{k} )P(o|_{j})}\] (9)

The KaRR score is calculated as the geometric mean of the results obtained from Eq. (8) and (9), and its detailed implementation is shown in Appendix A.

## 4 Experiments

We implement KaRR to examine knowledge in 14 LLMs and analyze its correlation with human assessment on model knowledge. Compared to large language models, knowledge graphs store a vast amount of structured information explicitly, which is ensured to be correct through manual construction and verification. Therefore, we implement KaRR based on existing large knowledge graphs, including T-REx (Elsahar et al., 2018) and Wikidata (Vrandecic and Krotzsch, 2014).

### Data and Settings

In the following paragraphs, we provide details on the data, models and settings used in our experiments, with additional information provided in Appendix B and C.

Symbolic FactsWe utilize T-REx knowledge graph (Elsahar et al., 2018) as our primary source of symbolic knowledge. T-REx comprises 11 million triples that are aligned with 3.09 million Wikipedia abstracts; its quality is validated by extensive crowdsourcing evaluation. In our main experiments, we consider all 600 English relations available in T-REx and sample a maximum of 20 facts per relation, resulting in a total of 10,691 facts for knowledge assessment.

Multiple Entity AliasesFor the text forms of subjects and objects involved in calculating KaRR\({}_{}\) and KaRR\({}_{}\), we search the entity aliases from Wikidata with Wikidata Integrator3. Ultimately, we obtain 1,349,474 aliases for 968,426 subjects and 368,511 aliases for 207,985 objects.

Multiple Relation TemplatesPreviously, Elazar et al. (2021) manually wrote paraphrased relation templates for 41 relations, covering 6% of the relation classes in T-REx. To expand the coverage and diversity of templates for **R**, we incorporated placeholders "[X]" and "[Y]" into the relation aliases in Wikidata. Subsequently, we utilized a fine-tuned Flan-T5 model4 for grammar correction to generate corresponding template candidates. We then filtered the templates with missing subjects or objects, and the remaining template candidates were all validated and corrected manually by two human annotators. Finally, we obtained 3,488 templates for 600 relations, with an average of 5.82 paraphrased templates per relation, achieving full coverage of the English relation classes in T-REx.

Overall KaRR ScoreThe KaRR score assigned to each fact is a continuous value. If the KaRR score exceeds a predefined threshold, the fact is considered consistently known by the LLM. When applying our method to a set of facts for the general knowledge reliability of LLMs, the overall KaRR score is calculated as the proportion of facts with a KaRR score exceeding the threshold.

As the statistical information shown in Tab. 1 (a), our method exhibits a broader coverage of relations and entity aliases compared to previous studies. To implement KaRR, we initially sample the knowledge triplet from T-REx, which comprises the symbolic subject \(s\), relation \(r\), and object \(o\). By leveraging entity aliases and relation templates that we have devised, we can generate multiple expressions for each element in the KaRR metric. For instance, \(\) is implemented as the set of all searched subject aliases from our aliases datastore, and \(\) represents the combination of all possible relation templates and subject aliases (by replacing the placeholder with the subject aliases).

### Models

We explore the following models for knowledge assessment: (1) traditional size pretrained models (\(<\) 5B), including GPT (Radford et al., 2018), XLNet (Yang et al., 2019), GPT2-XL (Radford et al., 2019), GPT-NEO (Black et al., 2021), T5-large and T5-3B (Raffel et al., 2020) and Phi-1.5 (Li et al., 2023) ; (2) medium size LLMs (\(\) 5B, \(<\) 50B), including GLM (Du et al., 2022), Falcon (Penedo et al., 2023), Dolly5, Moss6, LLMa (Touvron et al., 2023) and its variants Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023) and WizardLM (Xu et al., 2023); (3) large LLMs (\(\) 50B), including the 65B LLMa (Touvron et al., 2023), the 65B LLMaA2 (Touvron et al., 2023) and the 175B OPT (Zhang et al., 2022).

### Results of Knowledge Assessment

We present the results of our statistical knowledge assessment on various LLMs in Tab. 4 and more detailed results in Appendix D. Among the small and the medium-sized LLMs, most models' KaRR scores are between 3 to 13, indicating that they often struggle to generate factually accurate sentences consistently. However, it is surprising to note that the 2.65B GPT-NEO outperforms the T5-3B with a 3.92 KaRR score difference. We hypothesize that this difference in performance may be attributed to the pretraining corpora. Specifically, GPT-NEO (trained on the Pile dataset (Gao et al., 2021))

Table 1: Basic information and human evaluation results. The abbreviations Subj., Rel., Obj., and Rel. Cvg. represent Subject, Relation, Object, and the coverage of English relations in T-REx, respectively. The methods evaluated through human evaluation are assessed using the same set of 410 randomly sampled facts. K-Prompts is implemented using shared basic information with KaRR.

primarily utilizes high-quality data, while T5 (pretrained on a combination of corpora ) uses a more varied set of data sources. For larger LLMs, a comparison between the original LLaMA and the Alpaca model reveals that instruction-tuning might influence the model's ability to generate consistent and correct knowledge. Compared to the KaRR score of the original LLaMA, the KaRR score of Vicuna indicates that fine-tuning LLM on data collected from a more knowledgeable model could augment its knowledge. This finding is not commonly observed in previous studies but is consistent with our case study presented in Appendix E.

Fig. 4 demonstrates that our statistical evaluation approach supports the law of model scaling, where larger models tend to possess more factual knowledge. This observation is consistent with our intuition about the relationship between model size and knowledge representation. However, the benefits of scaling vary across models. For instance, T5 shows a significant improvement in KaRR score when scaled from T5-small to T5-3B, while GPT2 and OPT also benefit from scaling, albeit to a lesser extent. Notably, we observe that when the model size is smaller than 1B, GPT2 and OPT significantly outperform T5 in generating correct knowledge. This finding highlights the importance of selecting an appropriate model size for a given task and dataset, as larger models may not always provide the best performance on generation factuality. Overall, the results of our statistical evaluation method are consistent with our intuition, and highlight the importance of large-scale pretraining on knowledge-rich corpora for building high-performing language models.

### Human Evaluation for Knowledge Assessment

To further validate our approach, we conduct the human evaluation for model knowledge assessment. Utilizing the mean score from human evaluation as the benchmark for knowledge assessment on individual facts, we examine the validity of our knowledge assessment outcomes from two perspectives: 1) the \(\) correlation between the KaRR score on each fact and the human-annotated score on each fact and 2) the Recall for identifying knowledge that is not consistently known (facts with human average scores below 0.5) when employing the GPT2-XL model.

Human EvaluationOur human evaluation for the reliable knowledge generation ability of GPT2-XL contains two procedures: 1) Annotating: We have three annotation volunteers with NLP backgrounds. Each annotator is asked to write three prompts to probe the model knowledge from multiple aspects for each fact. They are instructed to refine the prompts based on model generations until the generations are of the same type as the target answer (but not necessarily the target entity). To ensure a fair comparison with LAMA , which is only applicable to 41 high-frequency

  
**Model** & **Size** & **KaRR Score** & **Model** & **Size** & **KaRR Score** \\  GPT & 0.12B & 9.57 & GLM & 10B & 5.59 \\ XLNet & 0.12B & 5.86 & Dolly & 12B & 15.60 \\ T5-large & 0.74B & 3.22 & LLaMA & 13B & 13.86 \\ Phi-1.5 & 1.3B & 10.58 & Alpaca & 13B & 8.24 \\ GPT2-XL & 1.56B & 12.27 & Vicuna & 13B & 19.50 \\ GPT-NEO & 2.65B & 13.44 & WizardLM & 13B & 16.90 \\ T5-3B & 3B & 9.52 & Moss & 16B & 11.20 \\ Falcon & 7B & 7.97 & LLaMA & 65B & 14.56 \\ BLOOM & 7B & 7.72 & LLaMA2 & 65B & 19.71 \\ LLaMA & 7B & 12.37 & OPT & 175B & 23.06 \\   

Table 2: Statistical knowledge assessment results on 20 LLMs. KaRR scores indicate that fine-tuning on instruction-following data (Alpaca) jeopardizes the reliability of knowledge-correct generation of the original LLaMA, whereas utilizing data from a more knowledgeable model enhances model knowledge (Vicuna). Additionally, the same backbone architecture adheres to the scaling law.

Figure 4: Knowledge in models with the same backbone architecture follows the scaling law.

relations, we restricted the relation classes to those included in LAMA. In total, we obtained 4,140 valid manually annotated prompts for 410 facts in T-REx. 2) Rating: We then asked three other annotators to rate the knowledge (0 or 1) in GPT2-XL according to the model generations. We used the average score of their ratings on all the prompts as the gold standard for knowledge assessment on each fact. Detailed annotation procedures and scoring criteria are shown in Appendix F.

BaselinesAs a comparison, we also calculated the correlation between three baseline methods and human evaluation results. The baseline methods, LAMA [Petroni et al., 2019] and ParaRel [Elazar et al., 2021], were designed for MLMs, we modified them for LLMs by deleting words following the objects in the templates. We implement the _Consistent-Acc_ score [Elazar et al., 2021] for ParaRel, which combines knowledge and consistency during evaluation. LAMA@1 and LAMA@10 refer to the results obtained by checking whether the top-1 or top-10 generations contain the object, respectively. Furthermore, we propose a K-Prompts baseline that computes the average probability of k randomly sampled prompts. It can be considered an adapted version of LPAQA [Shin et al., 2020] for LLMs, utilizing a collection of ensembled high-quality prompts identical to those used in KaRR. Here, we adopt K-Prompts as a simple baseline to evaluate the knowledge assessment in the absence of latent variables. All baseline methods are executed on the same set of facts as KaRR. LAMA and ParaRel are implemented as per the original papers [Petroni et al., 2019, Elazar et al., 2021], and K-Prompts is implemented using shared aliases and templates with KaRR. We set 22 as the threshold of KaRR, which is chosen by aligning the proportion of GPT2-XL known facts distinguished by humans on a sampled set of 200 facts. Similarly, the threshold for K-Prompts has been set to 0.13, as a result of aligning with the proportion recognized by humans.

ResultsTab. 1 (b) demonstrate that KaRR performs exceptionally well in knowledge detection, achieving a Recall of 95.18%. This highlights the potential of our statistical knowledge assessment to identify unreliable knowledge in LLMs, which is essential for downstream knowledge updating [De Cao et al., 2021]. Furthermore, the evaluation results of KaRR show a strong correlation (Kendall's 0.43) with human evaluation results. However, averaging the results of paraphrased prompts (ParaRel and K-Prompts) is not effective due to varying prompt frequencies and diverse impacts of different prompts on the model's probability of generating an "object". Since human annotation for model knowledge assessment is demanding and time-consuming, our statistical evaluation method provides a fast and accurate solution for the automatic knowledge assessment of LLMs.

## 5 Variance toward Prompts

Since the aim of the evaluation is to reflect the correctness of an LLM's knowledge towards various prompts, the evaluation result should be stable and not easily affected by the text forms used. In this section, we investigate the evaluation variance of our method. In contrast to ParaRel [Elazar et al., 2021], which studies the consistency of models under paraphrased inputs, we investigate the variance of the knowledge assessment method. Specifically, we evaluate the evaluation variance, which measures how much the same evaluation results vary when different prompts are used.

Following the same procedures as described in Sec. 4.4, we sampled 4,100 facts and employed the paraphrased relation templates from ParaRel [Elazar et al., 2021] to replace the relation templates in the baseline methods and KaRR, resulting in three distinct relation templates for each fact. We evaluated the effectiveness of the different evaluation approaches on each of the three paraphrased relation templates separately, the model is GPT2-XL. Additionally, we measured the evaluation variance by calculating the variance and standard deviation of the evaluation scores.

Tab. 3 (a) illustrates that the results of LAMA and K-Prompts exhibit significant variations when the probing prompt is changed to a paraphrased one, despite the overall prompt similarity in ParaRel

Table 3: Evaluation variance and spurious correlation analysis. Compared to LAMA and ParaRel, our statistical knowledge assessment is more robust and less influenced by the spurious correlation.

being \(0.96\). In contrast, our statistical knowledge assessment approach demonstrates robustness to prompt variation, with a 0.67 score variance and a 0.82 standard deviation.

## 6 Spurious Correlation in Knowledge Assessment

Both LAMA and KaRR use text as a proxy to estimate unobservable knowledge. However, spurious correlations can lead to inflated probing results where LLMs "guess" the probing answer based on these correlations. For example, when probed with the relation template "[X]'s birthplace is [Y]", LLMs assign "America" the highest probability, regardless of the subject entity (Zhao et al., 2021). To address this issue, we investigate spurious correlations in knowledge assessment, particularly focusing on the correlation between relation templates and high-frequency objects.

We synthesize false facts by replacing objects with high-frequency but factually incorrect ones. They are non-existent knowledge in LLMs as they are not part of the pretraining corpus. Ideally, the scores on synthetic facts should be 0 if the assessment is not influenced by spurious correlations. We obtain high-frequency objects by identifying the top-5 GPT2-XL predictions on 3 ParaRel relation templates without subjects, yielding \(14\) relations with valid high-frequency objects out of \(41\) relations. For each relation, we randomly sample \(100\) facts and replace the original object with the high-frequency object (if the original object is not the high-frequency object). We employ two metrics: Spurious Positive (SP) and Positive Gap (\(\)P). SP quantifies the proportion of false synthetic facts that are incorrectly recognized as known by the LLMs, and \(\)P is calculated by SP minus model's true positive rate on the actual facts. Ideally, SP and (\(\)P) should be close to 0 and (\(\)P) should be negative; otherwise, it indicates a significant bias in the evaluation result due to spurious correlations.

Tab. 3 (b) presents results on GPT2-XL. LAMA scores are highly correlated to the prompt, leading to the high \(\)P and SP. For example, 3.81% and 64.29% false facts are estimated as learned facts by LAMA@1 and LAMA@10, respectively. In contrast, K-Prompts and KaRR are less influenced by spurious correlations, with an SP score lower than 2% and a negative \(\)P. By assessing the model knowledge from various perspectives, the ultimate scores of KaRR indicate the comprehensive knowledge mastery of LLMs. Therefore, our results are less influenced by co-occurrence shortcuts.

## 7 Discussion

In this section, we discuss the causal explanation of KaRR and explore the effects of the sampling parameter, as well as the influence of model size.

Causal Explanation for KaRRFrom a causal perspective, our problem can be defined as: _"given a subject \(s\), the causal effect of given the relation \(r\) or not for the LLM generating the object \(o\)"_. Assuming there are no confounders, given a fixed \(s\), the treatment (T) is whether a specific \(r\) is provided or not, the outcome (Y) is the probability of the model generating \(o\), then the Average Treatment Effect (ATE) (Holland, 1986) is defined as:

\[[Y(1)]-[Y(0)]=[Y T=1]-[Y T=0] =P(o|s,r)-_{}[P(o|s,)]\] (10)

Note that in ATE, the minuend and subtrahend correspond to the denominators and numerator of KaRR\({}_{r}\) in Eq. (1), respectively. Assuming no confounders are present, the only distinction lies in the mathematical operation utilized (subtraction or division). Both subtraction and division can be used to compare the difference between the exposed group and the unexposed group, with the only distinction being the difference in the threshold for the resulting value. As a result, ATE helps clarify and validate KaRR from a causal perspective.

Figure 5: Ablation study on the sampling parameter K. With the increase in K, the variability in knowledge assessment decreases, while its correlation with human evaluation improves.

Influence of Sampling ParameterWhen calculating KaRR, the sampling number K makes a trade-off between the evaluation accuracy and efficiency. We perform an ablation study on the results of KaRR implemented with different K. Following the same settings of Sec. 5 and Sec. 4.4, we analyze the evaluation variance and evaluation accuracy of the overall KaRR scores of GPT2-XL. Fig. 5 demonstrates that increasing K decreases variance in knowledge assessment, but the impact is minor since evaluation variance is already low. The sampling parameter has a more significant effect on evaluation accuracy, with higher accuracy correlating with higher K values. When K is less than 4, the evaluation results are unreliable due to weak correlation with human evaluation results.

Impact of Scaling on LLM KnowledgeTo further study the impact of scaling up on LLM knowledge, we compare the KaRR scores of OPT-350M, OPT-2.7B, and OPT-175B on 30 best-known and 30 worst-known relations of OPT-350M. Fig. 6 shows that larger models exhibit better and more consistent knowledge-correct generation ability on the relations where small models also show some knowledge-correct generation ability. Moreover, larger models surpass small models in terms of knowledge on a wider range of relations. Therefore, the scaling law of LLM knowledge is attributed to the broader and more extensive knowledge of larger LLMs.

## 8 Limitations and Future Work

While KaRR demonstrates promising effectiveness for LLM knowledge assessment, there are a few aspects that require further investigation. First, KaRR scores are calculated for each fact using LLM inference on various prompts, which can be time-consuming and may limit the scalability of the evaluation. Second, our assessment is currently limited to scenarios where the logits of LLMs are available. This excludes many real-world applications where the models' internal representations may not be accessible or may be difficult to obtain. Third, the generality of KaRR can be further expanded. The current implementation focuses on atomic knowledge, characterized by triplets in the format of <subject, relation, object>. Nevertheless, real-world scenarios encompass a significant portion of complex knowledge, necessitating multi-hop reasoning or the comprehension of intricate relationships among multiple entities (Choudhary and Reddy, 2023). Evaluating such knowledge in LLMs continues to be a challenging and unresolved issue. Overall, these areas for improvement emphasize the opportunities for continued research and development of knowledge assessment approaches for LLMs, taking into account the complexities of real-world applications and the diverse types of knowledge to be assessed.

## 9 Conclusion

We investigate the consistent knowledge-correct generation ability of LLMs and propose a statistical knowledge assessment approach, called KaRR. By implementing our approach on 20 LLMs, we carry out an extensive assessment of the reliability of factual knowledge generation in these models. Our experiments reveal that our method yields a high correlation with human evaluation, and addresses the issues of variance and spurious correlation during the knowledge assessment of generative LLMs. Additionally, our assessment suite includes millions of entity and relation aliases, enabling large-scale knowledge evaluation. Overall, our method evaluates the reliable knowledge generation ability of LLMs, providing a robust foundation for future developments in knowledge refinement, augmentation, and beyond.

Figure 6: KaRR scores on different relations when scaling up OPT from 350M to 2.7B and 175B.