# Kernel Quadrature with Randomly Pivoted Cholesky

Ethan N. Epperly and Elvira Moreno

Department of Computing and Mathematical Sciences

California Institute of Technology

Pasadena, CA 91125

{eepperly,emoreno2}@caltech.edu

ENE acknowledges support by NSF FRG 1952777, Carver Mead New Horizons Fund, and the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DE-SC0021110. EM was supported in part by Air Force Office of Scientific Research grant FA9550-22-1-0225.

###### Abstract

This paper presents new quadrature rules for functions in a reproducing kernel Hilbert space using nodes drawn by a sampling algorithm known as randomly pivoted Cholesky. The resulting computational procedure compares favorably to previous kernel quadrature methods, which either achieve low accuracy or require solving a computationally challenging sampling problem. Theoretical and numerical results show that randomly pivoted Cholesky is fast and achieves comparable quadrature error rates to more computationally expensive quadrature schemes based on continuous volume sampling, thinning, and recombination. Randomly pivoted Cholesky is easily adapted to complicated geometries with arbitrary kernels, unlocking new potential for kernel quadrature.

## 1 Introduction

Quadrature is one of the fundamental problems in computational mathematics, with applications in Bayesian statistics , probabilistic ODE solvers , reinforcement learning , and model-based machine learning . The task is to approximate an integral of a function \(f\) by the weighted sum of \(f\)'s values at judiciously chosen quadrature points \(s_{1},,s_{n}\):

\[_{}f(x)g(x)\,(x)_{i=1}^{n}w_{i}f(s_{i}). \]

Here, and throughout, \(\) denotes a topological space equipped with a Borel measure \(\), and \(g^{2}()\) denotes a square-integrable function. The goal of kernel quadrature is to select quadrature weights \(w_{1},,w_{n}\) and nodes \(s_{1},,s_{n}\) which minimize the error in the approximation (1) for all \(f\) drawn from a reproducing kernel Hilbert space (RKHS) \(\) of candidate functions.

The ideal kernel quadrature scheme would satisfy three properties:

1. _Spectral accuracy._ The error of the approximation (1) decreases at a rate governed by the eigenvalues of the reproducing kernel \(k\) of \(\), with rapidly decaying eigenvalues guaranteeing rapidly decaying quadrature error.
2. _Efficiency._ The nodes \(s_{1},,s_{n}\) and weights \(w_{1},,w_{n}\) can be computed by an algorithm which is efficient in both theory and practice.

The first two goals may be more easily achieved if one has access to a Mercer decomposition

\[k(x,y)=_{i=1}^{}_{i}e_{i}(x)e_{i}(y), \]

where \(e_{1},e_{2},\) form an orthonormal basis of \(}^{2}()\) and the eigenvalues \(_{1}_{2}\) are decreasing. Fortunately, the Mercer decomposition is known analytically for many RKHS's \(\) on simple sets \(\) such as boxes \(^{d}\). However, such a decomposition is hard to compute for general kernels \(k\) and domains \(\), leading to the third of our desiderata:

1. _Mercer-free._ The quadrature scheme can be efficiently implemented without access to an explicit Mercer decomposition (2).

Despite significant progress in the probabilistic and kernel quadrature literature (see, e.g., ), the search for a kernel quadrature scheme meeting all three criteria remains ongoing.

Contributions.We present a new kernel quadrature method based on the _randomly pivoted Cholesky_ (RPCholesky) sampling algorithm that achieves all three of the goals. Our main contributions are

1. Generalizing RPCholesky sampling (introduced in  for finite kernel matrices) to the continuum setting and demonstrating its effectiveness for kernel quadrature.
2. Establishing theoretical results (see theorem 1) which show that RPCholesky kernel quadrature achieves near-optimal quadrature error rates.
3. Developing efficient rejection sampling implementations (see algorithms 2 and 4) of RPCholesky in the continuous setting, allowing kernel quadrature to be applied to general spaces \(\), measures \(\), and kernels \(k\) with ease.

The remainder of this introduction will sketch our proposal for RPCholesky kernel quadrature. A comparison with existing kernel quadrature approaches appears in section 2.

Randomly pivoted Cholesky.Let \(\) be an RKHS with a kernel \(k\) that is integrable on the diagonal

\[_{}k(x,x)(x)<+. \]

RPCholesky uses the kernel diagonal \(k(x,x)\) as a _sampling distribution_ to pick quadrature nodes. The first node \(s_{1}\) is chosen to be a sample from the distribution \(k(x,x)(x)\), properly normalized:

\[s_{1}(x)}{_{}k(x,x) (x)}.\]

Having selected \(s_{1}\), we remove its influence on the kernel, updating the entire kernel function:

\[k^{}(x,y)=k(x,y)-)k(s_{1},y)}{k(s_{1},s_{1})}. \]

Linear algebraically, the update (4) can be interpreted as Gaussian elimination, eliminating "row" and "column" \(s_{1}\) from the "infinite matrix" \(k\). Probabilistically, if we interpret \(k\) as the covariance function of a Gaussian process, the update (4) represents conditioning on the value of the process at \(s_{1}\). We use the updated kernel \(k^{}\) to select the next quadrature node:

\[s_{2}(x,x)(x)}{_{}k^{ }(x,x)(x)},\]

whose influence is then subtracted off \(k^{}\) as in (4). RPCholesky continues along these lines until \(n\) nodes have been selected. The resulting algorithm is shown in algorithm 1. Having chosen the nodes \(s_{1},,s_{n}\), our choice of weights \(w_{1},,w_{n}\) is standard and is discussed in section 3.2.

RPCholesky sampling is more flexible than many kernel quadrature methods, easily adapting to general spaces \(\), measures \(\), and kernels \(k\). To demonstrate this flexibility, we apply RPCholesky to the region \(\) in fig. 1a equipped with the Matern \(5/2\)-kernel with bandwidth \(2\) and the measure

\[(x,y)=(x^{2}+y^{2})xy.\]A set of \(n=20\) quadrature nodes produced by RPCholesky sampling (using algorithm 2) is shown in fig. 0(a). The cyan-pink shading shows the diagonal of the kernel after updating for the selected nodes. We see that near the selected nodes, the updated kernel is very small, meaning that future steps of the algorithm will avoid choosing nodes in those regions. Nodes far from any currently selected nodes have a much larger kernel value, making them more likely to be chosen in future RPCholesky iterations.

The quadrature error for \(f(x,y)=(x)(y)\), \(g 1\), and different numbers \(n\) of quadrature nodes for RPCholesky kernel quadrature, kernel quadrature with nodes drawn iid from \(/()\), and Monte Carlo quadrature are shown in fig. 0(b). Other spectrally accurate kernel quadrature methods would be difficult to implement in this setting because they would require an explicit Mercer decomposition (projection DPPs and leverage scores) or an expensive sampling procedure (continuous volume sampling). A comparison of RPCholesky with more kernel quadrature methods on a benchmark problem is provided in fig. 2.

## 2 Related work

Here, we overview past work on kernel quadrature and discuss the history of RPCholesky sampling.

### Kernel quadrature

The goal of kernel quadrature is to provide a systematic means for designing quadrature rules on RKHS's. Relevant points of comparison are Monte Carlo  and quasi-Monte Carlo  methods, which have \((1/)\) and \(((n)/n)\) convergence rates, respectively.

The literature on probabilistic and kernel approaches to quadrature is vast, so any short summary is necessarily incomplete. Here, we briefly summarize some of the most prominent kernel quadrature

Figure 1: **RPCholesky kernel quadrature on oddly shaped region.**_Left:_ Black dots show 20 points selected by RPCholesky on a crescent-shaped region. Shading shows kernel values \(k((x,y),(x,y))\) after removing influence of selected points. _Right:_ Mean relative error for RPCholesky kernel quadrature, iid kernel quadrature, and Monte Carlo quadrature for \(f(x,y)=(x)(y)\) with 100 trials. Shaded regions show 10%/90% quantiles.

methods, highlighting limitations that we address with our RPCHolesky approach. We refer the reader to [22, Tab. 1] for a helpful comparison of many of the below-discussed methods.

Herding.Kernel herding schemes  iteratively select quadrature nodes using a greedy approach. These methods face two limitations. First, they require the solution of a (typically) nonconvex global optimization problem at every step, which may be computationally costly. Second, these methods exhibit slow \((1/n)\) quadrature error rates [11, Prop. 4] (or even slower ). Optimally weighted herding is known as sequential Bayesian quadrature .

Thinning.Thinning methods  try to select \(n\) good quadrature nodes from a larger set of \(N n\) candidate nodes drawn from a simple distribution. While these methods have other desirable theoretical properties, they do not benefit from spectral accuracy.

Leverage scores and projection DPPs.With optimal weights (section 3.2), quadrature with nodes sampled using a projection determintal point process (DPP)  (see also ) or iid from the (ridge) leverage score distribution  achieve spectral accuracy. However, known efficient sampling algorithms require access to the Mercer decomposition (2), limiting the applicability of these schemes to simple spaces \(\), measures \(\), and kernels \(k\), where the decomposition is known analytically.

Continuous volume sampling.Continuous volume sampling is the continuous analog of \(n\)-DPP sampling , providing quadrature nodes that achieve spectral accuracy [8, Prop. 5]. Unfortunately, continuous volume sampling is computationally challenging. In the finite setting, the best-known algorithm  for exact \(n\)-DPP sampling requires a costly \((|| n^{6.5}+n^{9.5})\) operations. Inexact samplers based on Markov chain Monte Carlo (MCMC) (e.g., ) may be more competitive, but the best-known samplers in the continuous setting still require an expensive \((n^{5} n)\) MCMC steps [34, Thms. 1.3-1.4]. RPCHolesky sampling achieves similar theoretical guarantees to continuous volume sampling (see theorem 1) and can be efficiently and exactly sampled (algorithm 2).

Recombination and convex weights.The paper  (see also ) proposes two ideas for kernel quadrature when \(\) is a probability measure and \(g 1\). First, they suggest using a recombination algorithm (e.g., ) to subselect good quadratures from \(N n\) candidate nodes iid sampled from \(\). All of the variants of their method either fail to achieve spectral accuracy or require an explicit Mercer decomposition [22, Tab. 1]. Second, they propose choosing weights \((w_{1},,w_{n})\) that are convex combination coefficients. This choice makes the quadrature scheme robust against misspecification of the RKHS \( f\), among other benefits. It may be worth investigating a combination of RPCHolesky quadrature nodes with convex weights in future work.

### Randomly pivoted Cholesky

RPCHolesky was proposed, implemented, and analyzed in  for the task of approximating an \(M M\) positive-semidefinite matrix \(\). The algorithm is algebraically equivalent to applying an earlier algorithm known as _adaptive sampling_ to \(^{1/2}\). Despite their similarities, RPCHolesky and adaptive sampling are different algorithms: To produce a rank-\(n\) approximation to an \(M M\) matrix, RPCHolesky requires \((n^{2}M)\) operations, while adaptive sampling requires a much larger \((nM^{2})\) operations. See [10, SS4.1] for further discussion on RPCHolesky's history. In this paper, we introduce a continuous extension of RPCHolesky and analyze its effectiveness for kernel quadrature.

## 3 Theoretical results

In this section, we prove our main theoretical result for RPCHolesky kernel quadrature. We first establish the mathematical setting (section 3.1) and introduce kernel quadrature (section 3.2). Then, we present our main theorem (section 3.3) and discuss its proof (section 3.4).

### Mathematical setting and notation

Let \(\) be a Borel measure supported on a topological space \(\) and let \(\) be a RKHS on \(\) with continuous kernel \(k:\). We assume that \(x k(x,x)\) is integrable (3) and that \(\)is dense in \(^{2}()\). These assumptions imply that \(\) is compactly embedded in \(^{2}()\), the Mercer decomposition (2) converges pointwise, and the Mercer eigenfunctions form an orthonormal basis of \(^{2}()\) and an orthogonal basis of \(\)[39, Thm. 3.1].

Define the integral operator

\[Tf(x)=_{}k(x,y)f(y)\,(y). \]

Viewed as an operator \(T:^{2}()^{2}()\), \(T\) is self-adjoint, positive semidefinite, and trace-class.

One final piece of notation: For a function \(:\) and an ordered (multi)set \(=\{s_{1},,s_{n}\}\), we let \(()\) be the column vector with \(i\)th entry \((s_{i})\). Similarly, for a bivariate function \(h:\), \(h(,)\) (resp. \(h(,)\)) denotes the row (resp. column) vector-valued function with \(i\)th entry \(h(,s_{i})\) (resp. \(h(s_{i},)\)), and \(h(,)\) denotes the matrix with \(ij\) entry \(h(s_{i},s_{j})\).

### Kernel quadrature

Following earlier works (e.g., [3; 7]), let us describe the kernel quadrature problem more precisely. Given a function \(g^{2}()\), we seek quadrature weights \(=(w_{1},,w_{n})^{n}\) and nodes \(=\{s_{1},,s_{n}\}\) that minimize the maximum quadrature error over all \(f\) with \( f_{} 1\):

\[(,;g)_{ f _{} 1}|_{}f(x)g(x)\,(x)- _{i=1}^{n}w_{i}f(s_{i})|. \]

A short derivation ([3; Eq. (7)] or appendix C) yields the simple formula

\[(,;g)= Tg-_{i=1}^{n} w_{i}k(,s_{i})_{}. \]

This equation reveals that the quadrature rule minimizing \((,;g)\) is the least-squares approximation of \(Tg\) by a linear combination of kernel function evaluations \(_{i=1}^{n}w_{i}k(,s_{i})\).

If we fix the quadrature nodes \(=\{s_{1},,s_{n}\}\), the optimal weights \(_{}=(w_{ 1},,w_{ n})^{n}\) minimizing \((,;g)\) are the solution of the linear system of equations

\[k(,)_{}=Tg(). \]

We use (8) to select the weights throughout this paper, and denote the error with optimal weights by

\[(;g)(, _{};g). \]

### Error bounds for randomly pivoted Cholesky kernel quadrature

Our main result for RPCholesky kernel quadrature is as follows:

**Theorem 1**.: _Let \(=\{s_{1},,s_{n}\}\) be generated by the RPCholesky sampling algorithm. For any function \(g^{2}()\), nonnegative integer \(r 0\), and real number \(>0\), we have_

\[^{2}(;g)}  g_{^{2}()}^{2}_{i=r+1}^{} _{i}\]

_provided_

\[n r(}{_{i=r+1}^{}_{i}} )+. \]

To illustrate this result, consider an RKHS with eigenvalues \(_{i}=(i^{-2s})\) for \(s>1/2\). An example is the periodic Sobolev space \(H^{s}_{}()\) (see section 5). By setting \(=1/r\), we see that

\[^{2}(;g)}( r^{-2s}) g_{^{2}()}^{2}n=(r r).\]

The optimal scheme requires \(n=(r)\) nodes to achieve this bound [8, Prop. 5 & SS2.5]. Thus, to achieve an error of \((r^{-2s})\), **RPCholesky requires just logarithmically more nodes than the optimal quadrature scheme for such spaces.** This compares favorably to continuous volume sampling, which achieves the slightly better optimal rate but is much more difficult to sample.

Having established that RPCholesky achieves nearly optimal error rates for interesting RKHS's, we make some general comments on theorem 1. First, observe that the error depends on the sum \(_{i=r+1}^{}_{i}\) of the tail eigenvalues. This tail sum is characteristic of spectrally accurate kernel quadrature schemes . The more distinctive feature in (10) is the logarithmic factor. Fortunately, for double precision computation, the achieveable accuracy is bounded by the machine precision \(2^{-52}\), so this logarithmic factor is effectively a modest constant:

\[(}{_{i=r+1}^{}}) (2^{52})<37.\]

### Connection to Nystrom approximation and idea of proof

We briefly outline the proof of theorem 1. Following previous works (e.g., ), we first utilize the connection between kernel quadrature and the Nystrom approximation  of the kernel \(k\).

**Definition 2**.: For nodes \(=\{s_{1},,s_{n}\}\), the _Nystrom approximation_ to the kernel \(k\) is

\[k_{}(x,y)=k(x,)\,k(,)^{}\,k( ,y). \]

Here, \({}^{}\) denotes the Moore-Penrose pseudoinverse. The Nystrom approximate integral operator is

\[T_{}f_{}k_{}(,x)f(x)\,(x).\]

This definition leads to a formula for the quadrature rule \(_{i=1}^{n}w_{i}k(,s_{i})\) with optimal weights (8).

**Proposition 3**.: _Fix nodes \(=\{s_{1},,s_{n}\}\). With the optimal weights \(_{}^{n}\) (8), we have_

\[_{i=1}^{n}w_{i*}k(,s_{i})=T_{}g. \]

_Consequently,_

\[^{2}(;g)=\|(T-T_{})g\|_{ }^{2} g,(T-T_{})g_{^{2}()}.\]

To finish the proof of theorem 1, we develop and solve a recurrence for an upper bound on the largest eigenvalue of \([T-T_{}]\). See appendix A for details and for the proof of proposition 3.

## 4 Efficient randomly pivoted Cholesky by rejection sampling

To efficiently perform RPCholesky sampling in the continuous setting, we can use _rejection sampling_. (A similar rejection sampling idea is used in the MCMC continuous volume sampler of .) Assume for simplicity that the measure \(\) is normalized so that

\[_{}k(x,x)\,(x)=T=1.\]

We assume two forms of access to the kernel \(k\):

1. **Entry evaluations.** For any \(x,y\), we can evaluate \(k(x,y)\).
2. **Sampling the diagonal.** We can produce samples from the measure \(k(x,x)\,(x)\).

To sample from the RPCholesky distribution, we use \(k(x,x)\,(x)\) as a proposal distribution and accept proposal \(s\) with probability

\[1-}(s,s)}{k(s,s)}.\]

(Recall \(k_{}\) from (11).) The resulting algorithm for RPCholesky sampling is shown in algorithm 2.

```
0: Kernel \(k:\) and number of quadrature points \(n\)
0: Quadrature points \(s_{1},,s_{n}\)
1: Initialize \(_{n n}\), \(i 1\), \(\)\(\)\(\) stores a Cholesky decomposition of \(k(,)\)
2:while\(i n\)do
3: Sample \(s_{i}\) from the probability measure \(k(x,x)\,(x)\)
4:\((d,)(s_{i},,,k)\)\(\) Helper subroutine algorithm 3
5: Draw a uniform random variable \(U\)
6:if\(U<d/k(s_{i},s_{i})\)then\(\) Accept with probability \(d/k(s_{i},s_{i})\)
7: Induct \(s_{i}\) into the selected set: \(\{s_{i}\}\), \(i i+1\)
8:\((i,1:i)^{}&\)
9:endif
10:endwhile
```

**Algorithm 2** RPCholesky with rejection sampling

**Theorem 4**.: _Algorithm 2 produces exact RPCholesky samples. Let \(_{i}\) denote the trace-error of the best rank-\(i\) approximation to \(T\):_

\[_{i}=_{j=i+1}^{}_{i}.\]

_The expected runtime of algorithm 2 is at most \((_{i=1}^{n}i^{2}/_{i})(n^{3}/_{n-1})\)._

This result demonstrates that algorithm 2 suffers from the _curse of smoothness_: The faster the eigenvalues \(_{1},_{2},\) decrease, the smaller \(_{n-1}\) will be and, consequently, the slower algorithm 2 will be in expectation. While this curse is an unfortunate limitation, it is also a common one. The curse of smoothness affects all known Mercer-free, spectrally accurate kernel quadrature schemes. In fact, the situation is worse for other algorithms. The CVS sampler of , for example, requires as many as \((n^{5} n)\) MCMC steps, each of which has cost \((n^{2}/_{n-1})\). According to current analysis, algorithm 2 is \((n^{4} n)\)-times faster than the CVS sampler of .

Notwithstanding the curse of smoothness, algorithm 2 is useful in practice. The algorithm works under minimal assumptions and can reach useful accuracies \(=10^{-5}\) while sampling \(n=1000\) nodes on a laptop. Algorithm 2 may also be interesting for RPCholesky sampling for a large finite set \(|| 10^{9}\), since its runtime has no explicit dependence on the size of the space \(\).

To achieve higher accuracies and blunt the curse of smoothness, we can improve algorithm 2 with optimization. Indeed, the optimal acceptance probability for algorithm 2 would be

\[p(s_{i};)=,s_{i})-k_{}(s_{i},s_{i})}{k(s_{i},s_{i})}=_{x} }(x,x)}{k(x,x)}.\]

This suggests the following scheme: Initialize with \(=1\) and run the algorithm with acceptance probability \(p(s_{i};)\). If we perform many loop iterations without an acceptance, we then recompute the optimal \(\) by solving an optimization problem. The resulting procedure is shown in algorithm 4.

```
0: Point \(s\), set \(\), Cholesky factor \(\) of \(k(,)\), and kernel \(k\)
0:\(d=k(s,s)-k_{}(s,s)\) and \(=^{-1}k(,s)\)
1:procedureResidualKernel(\(s\),\(\),\(\),\(k\))
2:\(^{-1}k(,s)\)
3:\(d k(s,s)-^{2}\)\(\)\(d=k(s,s)-k_{}(s,s)\)
4:return\((d,)\)
5:endprocedure
```

**Algorithm 3** Helper subroutine to evaluate residual kernel

If the optimization problem for \(\) is solved to global optimality, then algorithm 4 produces exact RPCholesky samples. The downside of algorithm 4 is the need to solve a (typically) nonconvex global optimization problem to compute the optimal \(\). Fortunately, in our experiments (section 5), only a small number of optimization problems (\( 10\)) are needed to produce a sample of \(n=1000\) nodes. In the setting where the optimization problem is tractable, the speedup of algorithm 4 can be immense. To produce \(n=200\) RPCholesky samples for the space \(^{3}\) with the kernel (13), algorithm 4 requires just \(0.19\) seconds compared to \(7.47\) seconds for algorithm 2.

```
0: Kernel \(k:\) and number of quadrature points \(n\)
0: Quadrature points \(s_{1},,s_{n}\)
1: Initialize \(_{n n}\), \(i 1\), \(\), \( 0\), \( 1\)
2:while\(i n\)do
3:\(+1\)
4: Sample \(s_{i}\) from the probability measure \(k(x,x)\,(x)\)
5:\((d,)(s_{i},, ,k)\)\(\) Helper subroutine algorithm 3
6: Draw a uniform random variable \(U\)
7:if\(U<(1/) d/k(s_{i},s_{i})\)then\(\) Accept with probability \(d/k(s_{i},s_{i})\)
8:\(\{s_{i}\}\), \(i i+1\), \( 0\)
9:\((i,1:i)[^{}]\)
10:endif
11:if\(\)then\(\) We use \(\)
12:\(_{x(x,,,k) /k(x,x)}\)
13:\( 0\)
14:endif
15:endwhile
```

**Algorithm 4** RPCholesky with optimized rejection sampling

## 5 Comparison of methods on a benchmark example

Having demonstrated the versatility of RPCholesky for general spaces \(\), measures \(\), and kernels \(k\) in fig. 1, we now present a benchmark example from the kernel quadrature literature to compare RPCholesky with other methods. Consider the periodic Sobolev space \(H^{s}_{}()\) with kernel

\[k(x,y)=1+2_{m=1}^{}m^{-2s}(2 m(x-y))=1+(2)^ {2s}}{(2s)!}B_{2s}(\{x-y\}),\]

where \(B_{2s}\) is the \((2s)\)th Bernoulli polynomial and \(\{\}\) reports the fractional part of a real number [3, p. 7]. We also consider \(^{3}\) equipped with the product kernel

\[k^{ 3}((x_{1},x_{2},x_{3}),(y_{1},y_{2},y_{3}))=k(x_{1},y_{1})k(x_{2},y_{2} )k(x_{3},y_{3}). \]

We quantify the performance of nodes \(\) and weights \(\) using \((,;g)\), which can be computed in closed form [22, Eq. (14)]. We set \(:=^{d}\) and \(g 1\).

We compare the following schemes:

* _Monte Carlo, IID kernel quadrature._ Nodes \(s_{1},,s_{n}}}{{}}\) with uniform weights \(w_{i}=1/n\) (Monte Carlo) and optimal weights (8) (IID).
* _Thinning._ Nodes \(s_{1},,s_{n}\) thinned from \(n^{2}\) iid samples from \(=\) using kernel thinning  with the Compress++ algorithm  with optimal weights (8).
* _Continuous volume sampling (CVS)._ Nodes \(s_{1},,s_{n}\) drawn from the volume sampling distribution [8, Def. 1] by Markov chain Monte Carlo with optimal weights (8).
* RPCholesky. Nodes \(s_{1},,s_{n}\) sampled by RPCholesky using algorithm 4 with the optimal weights (8).
* _Positively weighted kernel quadrature (PWKQ)._ Nodes and weights computed by the Nystrom+empirical+opt method as described on [22, p. 9].

See appendix D for more details about our numerical experiments. Experiments were run on a MacBook Pro with a 2.4 GHz 8-Core Intel Core i9 CPU and 64 GB 2667 MHz DDR4 RAM. Our code is available at [https://github.com/eepperly/RPCholesky-Kernel-Quadrature](https://github.com/eepperly/RPCholesky-Kernel-Quadrature).

Errors for the different methods are shown in fig. 2 (left panels). We see that RPCholesky consistently performs among the best methods at every value of \(n\), numerically achieving the rate of convergence of the fastest method for each problem. Particularly striking is the result that RPCholesky sampling's performance is either very close to or better than that of continuous volume sampling, despite the latter's slightly stronger theoretical properties.

Figure 2 (right panels) show the sampling times for each of the nonuniform sampling methods. RPCholesky sampling is the fastest by far. To sample 128 nodes for \(s=3\) and \(d=1\), RPCholesky was \(\) faster than continuous volume sampling, \(\) faster than thinning, and \(\) faster than PWKQ.

To summarize our numerical evaluation (comprising fig. 2 and fig. 1 from the introduction), we find that RPCholesky is among the most accurate kernel quadrature methods tested. To us, the strongest benefits of RPCholesky (supported by these experiments) are the method's speed and flexibility. These virtues make it possible to apply spectrally accurate kernel quadrature in scenarios where it would have been computationally intractable before.

## 6 Application: Analyzing large chemistry datasets

While our focus thusfar has been on infinite domains \(\), the kernel quadrature formalism can also be applied to a finite set \(\) of data points. Let \(=\,\) be the uniform distribution and set \(g 1\). Here, the task is to exhibit \(n\) nodes \(=\{s_{1},,s_{n}\}\) and weights \(^{n}\) such that the average of every "smooth" function \(f:\) over the whole dataset is well-approximated by a sum:

\[(f)|}_{x}f(x) _{i=1}w_{i}f(s_{i}). \]

Here is an application to chemistry. Let \(\) denote a large set of compounds of interest, and let \(f:\) denote a target chemical property such as specific heat capacity. We are interested

Figure 2: **Benchmark example: Sobolev space. Mean quadrature error \((,;g)\) (_left_) and sampling time (_right_) for different methods (100 trials) for \(s=1\), \(d=3\) (_top_) and \(s=d=3\) (_bottom_). Shaded regions show 10%/90% quantiles.**

in computing \((f)\). Unfortunately, evaluating \(f\) on each \(x\) requires an expensive density functional theory computation, so it can be prohibitively expensive to evaluate \(f\) on every \(x\). Fortunately, we can obtain fast approximations to \((f)\) using (14), which only require evaluating \(f\) on a much smaller set of size \(||||\).

Our experimental setup is as follows. For \(\), we use \(2 10^{4}\) randomly selected points from the QM9 dataset . We represent each compound as a vector in \(^{1500}\) using the many-body tensor representation  computed with the DScribe package . Choose \(k\) to be a Gaussian kernel with bandwidth chosen by median heuristic  on a further subsample of \(10^{3}\) random points. We omit continuous volume sampling, thinning, and positively weighted kernel quadrature because of their computational cost. In their place, we add the greedy Nystrom method  with optimal weights (8).

Results are shown in fig. 3. Figure 2(a) shows the worst-case quadrature error (9). For this metric, IID and randomly pivoted Cholesky are the definitive winners, with randomly pivoted Cholesky being slightly better. Figure 2(b) shows the mean relative error for the kernel quadrature estimates of the mean _isotropic polarizability_ of the compounds in \(\). For sufficiently large \(n\), randomly pivoted Cholesky has the smallest error, beating IID and Monte Carlo by a factor of three at \(n=512\).

## 7 Conclusions, Limitations, and Possibilites for Future Work

In this article, we developed continuous RPCholesky sampling for kernel quadrature. Theorem 1 demonstrates RPCholesky kernel quadrature achieves near-optimal error rates. Numerical results (fig. 2) hint that its practical performance might be even better than that suggested by our theoretical analysis and fully comparable with kernel quadrature based on the much more computationally expensive continuous volume sampling distribution. RPCholesky supports performant rejection sampling algorithms (algorithms 2 and 4), which facilitate implementation for general spaces \(\), measures \(\), and kernels \(k\) with ease.

We highlight three limitations of RPCholesky kernel quadrature that would be worth addressing in future work. First, given the comparable performance of RPCholesky and continuous volume sampling in practice, it would be desirable to prove stronger error bounds for RPCholesky sampling or find counterexamples which demonstrate a separation between the methods. Second, it would be worth developing improved sampling algorithms for RPCholesky which avoid the need for global optimization steps. Third, all known spectrally accurate kernel quadrature methods require integrals of the form \(_{}k(x,y)g(y)\,(y)\), which may not be available. RPCholesky kernel quadrature requires them for the computation of the weights (8). Developing spectrally accurate kernel quadrature schemes that avoid such integrals remains a major open problem for the field.

Figure 3: **Application: chemistry.** Worst-case quadrature error (_left_) and mean relative error for estimation of the average value of the isotropic polarizability function \(f(x)\) (_right_) for different methods (100 trials). Shaded regions show 10%/90% quantiles.