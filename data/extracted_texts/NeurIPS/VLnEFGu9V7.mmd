# Regret Minimization via Saddle Point Optimization

Johannes Kirschner

Department of Computer Science

University of Alberta

jkirschn@ualberta.ca &Seyed Alireza Bakhtiari

Department of Computer Science

University of Alberta

sbakhtia@ualberta.ca &Kushagra Chandak

Department of Computer Science

University of Alberta

kchandak@ualberta.ca &Volodymyr Tkachuk

Department of Computer Science

University of Alberta

vtkachuk@ualberta.ca &Csaba Szepesvari

Department of Computer Science

University of Alberta

szepesva@ualberta.ca

###### Abstract

A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By reparametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions algorithm (Anytime-E2D). Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We further point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples.

## 1 Introduction

Regret minimization is a widely studied objective in bandits and reinforcement learning theory [Lattimore and Szepesvari, 2020a] that has inspired practical algorithms, for example, in noisy zero-order optimization[e.g., Srinivas et al., 2010] and deep reinforcement learning [e.g., Osband et al., 2016]. Cumulative regret measures the online performance of the algorithm by the total loss suffered due to choosing suboptimal decisions. Regret is unavoidable to a certain extent as the learner collects information to reduce uncertainty about the environment. In other words, a learner will inevitably face the exploration-exploitation trade-off where it must balance collecting rewards and collecting information. Finding the right balance is the central challenge of sequential decision-making under uncertainty.

More formally, denote by \(\) a decision space and \(\) an observation space. Let \(\) be a class of models, where \(f=(r_{f},M_{f})\) associated with a reward function \(r_{f}:\) and observationmap \(M_{f}:()\), where \(()\) is the set of all probability distributions over \(\).1 The learner's objective is to collect as much reward as possible in \(n\) steps when facing a model \(f^{*}\). The learner's prior information is \(\) and the associated reward and observation maps, but does not know the true instance \(f^{*}\). The learner constructs a stochastic sequence \(_{1},,_{n}\) of decisions taking values in \(\) and adapted to the history of observations \(y_{t} M_{f^{*}}(_{t})\). The policy of the learner is the sequence of probability kernels \(_{1:n}=(_{t})_{t=1}^{n}\) that are used to take decisions. The expected regret of a policy \(_{1:n}\) and model \(f^{*}\) after \(n\) steps is

\[R_{n}(_{1:n},f^{*})=_{}[_{t=1}^{n}r_{f^{*}}( )-r_{f^{*}}(_{t})]\]

The literature studies regret minimization for various objectives, including worst-case and instance-dependent frequentist regret (Lattimore and Szepesvari, 2020), Bayesian regret (Russo and Van Roy, 2014) and robust variants (Garcelon et al., 2020; Kirschner et al., 2020). For the frequentist analysis, all prior knowledge is encoded in the model class \(\). The worst-case regret of policy \(_{1:n}\) on \(\) is \(_{f}R_{n}(_{1:n},f)\), and therefore the optimal minimax regret \(_{}_{f}R_{n}(_{1:n},f)\) only depends on \(\) and the horizon \(n\). The Bayesian, in addition, assumes access to a prior \(()\), which leads to the Bayesian regret \(_{f}[R_{n}(_{1:n},f)]\). Interestingly, the worst-case frequentist regret and Bayesian regret are dual in the following sense (Lattimore and Szepesvari, 2019):2

\[_{_{1:n}}_{f}R_{n}(_{1:n},f)=_{()}_{_{1:n}}_{f}[R_{n}(_{1:n},f)]\] (1)

Unfortunately, directly solving for the minimax policy (or the worst-case prior) is intractable, except in superficially simple problems. This is because the optimization is over the exponentially large space of adaptive policies. However, the relationship in Eq. (1) has been directly exploited in prior works, for example, to derive non-constructive upper bounds on the worst-case regret via a Bayesian analysis (Bubeck et al., 2015). Moreover, it can be seen as inspiration underlying "optimization-based" algorithms for regret minimization: The crucial step is to carefully relax the saddle point problem in a way that preserves the statistical complexity, but can be analyzed and computed more easily. This idea manifests in several closely related algorithms, including information-directed sampling (Russo and Van Roy, 2014; Kirschner and Krause, 2018), ExpByOpt (Lattimore and Szepesvari, 2020, 2021), and most recently, the Estimation-To-Decisions (E2D) framework (Foster et al., 2021, 2023). These algorithms have in common that they optimize the information trade-off directly, which in structured settings leads to large improvements compared to standard optimistic exploration approaches and Thompson sampling. On the other hand, algorithms that directly optimize the information trade-off can be computationally more demanding and, consequently, are often not the first choice of practitioners. This is partly due to the literature primarily focusing on statistical aspects, leaving computational and practical considerations underexplored.

ContributionsBuilding on the results by Foster et al. (2021), we introduce the _average-constrained decision-estimation coefficient_ (\(_{}^{ac}\)), a saddle-point objective that characterizes the frequentist worst-case regret in sequential decision-making with structured observations. Compared to the decision-estimation coefficient of (Foster et al., 2021), the \(_{}^{ac}\) is parametrized via the confidence radius \(\), instead of the Lagrangian offset multiplier. This allows optimization of the information trade-off online by the algorithm, instead of via the derived regret upper bound. Moreover, optimizing the \(_{}^{ac}\) leads to an anytime version of the E2D algorithm (Anytime-E2D) with a straightforward analysis. We also point out relations between the \(_{}^{ac}\), the information ratio (Russo and Van Roy, 2016), the decoupling coefficient (Zhang, 2022) and a PAC version of the DEC (Foster et al., 2023). We further detail how to implement the algorithm for finite model classes and linear feedback models, and demonstrate the advantage of the approach by providing improved bounds for linear bandits with side-observations. Lastly, we report the first empirical results of the E2D algorithm on simple examples.

### Related Work

There is a broad literature on regret minimization in bandits (Lattimore and Szepesvari, 2020) and reinforcement learning (Jin et al., 2018; Azar et al., 2017; Zhou et al., 2021; Du et al., 2021;Zanette et al., 2020). Arguably the most popular approaches are based on optimism, leading to the widely analysed upper confidence bound (UCB) algorithms (Lattimore and Szepesvari, 2020a), and Thompson sampling (TS) (Thompson, 1933; Russo and Van Roy, 2016).

A long line of work approaches regret minimization as a saddle point problem. Degenne et al. (2020) showed that in the structured bandit setting, an algorithm based on solving a saddle point equation achieves asymptotically optimal regret bounds, while explicitly controlling the finite-order terms. Lattimore and Szepesvari (2020) propose an algorithm based on exponential weights in the partial monitoring setting (Rustichini, 1999) that finds a distribution for exploration by solving a saddle-point problem. The saddle-point problem balances the trade-off between the exponential weights distribution and an information or stability term. The same approach was further refined by Lattimore and Gyorgy (2021). In stochastic linear bandits, Kirschner et al. (2021) demonstrated that information-directed sampling can be understood as a primal-dual method solving the asymptotic lower bound, which leads to an algorithm that is both worst-case and asymptotically optimal. The saddle-point approach has been further explored in the PAC setting (e.g., Degenne et al., 2020, 2020).

Our work is closely related to recent work by Foster et al. (2021, 2023). They consider _decision making with structured observations_ (DMSO), which generalizes the bandit and RL setting. They introduce a complexity measure, the _offset decision-estimation coefficient_ (offset DEC), defined as a min-max game between a learner and an environment, and provide lower bounds in terms of the offset DEC. Further, they provide an algorithm, _Estimation-to-Decisions_ (E2D) with corresponding worst-case upper bounds in terms of the offset DEC. Notably, the lower and upper bound nearly match and recover many known results in bandits and RL. More recently, Foster et al. (2023) refined the previous bounds by introducing the _constrained_ DEC and a corresponding algorithm E2D\({}^{+}\).

There are various other results related to the DEC and the E2D algorithm. Foster et al. (2022) show that the E2D achieves improved bounds in model-free RL when combined with optimistic estimation (as introduced by Zhang (2022)). Chen et al. (2022) introduced two new complexity measures based on the DEC that are necessary and sufficient for reward-free learning and PAC learning. They also introduced new algorithms based on the E2D algorithm for the above two settings and various other improvements. Foster et al. (2022) have shown that the DEC is necessary and sufficient to obtain low regret for _adversarial_ decision-making. An asymptotically instance-optimal algorithm for DMSO has been proposed by Dong and Ma (2022), extending a similar approach for the linear bandit setting (Lattimore and Szepesvari, 2017).

The decision-estimation coefficient is also related to the information ratio (Russo and Van Roy, 2014) and the decoupling coefficient (Zhang, 2022). The information ratio has been studied under both the Bayesian (Russo and Van Roy, 2014) and the frequentist regret (Kirschner and Krause, 2018; Kirschner et al., 2020, 2021, 2023) in various settings including bandits, reinforcement learning, and partial monitoring. The decoupling coefficient was studied for the Thompson sampling algorithm in contextual bandits (Zhang, 2022), and RL (Dann et al., 2021; Agarwal and Zhang, 2022).

## 2 Setting

We consider the sequential decision-making problem already introduced in the preface. Recall that \(\) is a compact decision space and \(\) is an observation space. The model class \(\) is a set of tuples \(f=(r_{f},M_{f})\) containing a reward function \(r_{f}:\) and an observation distribution \(M_{f}:()\). We define the gap function

\[(,g)=r_{g}(_{g}^{*})-r_{g}()\,,\]

where \(_{g}^{*}=_{}r_{g}()\) is an optimal decision for model \(g\), chosen arbitrarily if not unique. A randomized policy is a sequence of kernels \(_{1:n}=(_{t})_{t=1}^{n}\) from histories \(h_{t-1}=(_{1},y_{1},,_{t-1},y_{t-1})()^{t-1}\) to sampling distributions \(()\). The filtration generated by the history \(h_{t}\) is \(_{t}\). The learner's decisions \(_{1},,_{n}\) are sampled from the policy \(_{t}_{t}\) and observations \(y_{t} M_{f^{*}}(_{t})\) are generated by an unknown true model \(f^{*}\). The expected regret under model \(f^{*}\) is formally defined as follows:

\[R_{n}(_{1:n},f^{*})=_{t=1}^{n}_{_{t} _{t}(h_{t})}[(_{t},f^{*})]\]For now, we do not make any assumption about the reward being observed. This provides additional flexibility to model a wide range of scenarios, including for example, duelling and ranking feedback (Yue and Joachims, 2009; Radlinski et al., 2008; Combes et al., 2015; Lattimore et al., 2018; Kirschner and Krause, 2021) (e.g. used in reinforcement learning with human feedback, RLHF) or dynamic pricing (Jean Boer, 2015). The setting is more widely known as partial monitoring Rustichini (1999). The special case where the reward is part of the observation distribution is called _decision-making with structured observations_(DMSO, Foster et al., 2021). Earlier work studies the closely related _structured bandit_ setting (Combes et al., 2017).

A variety of examples across bandit models and reinforcement learning are discussed in (Combes et al., 2017; Foster et al., 2021, 2023; Kirschner et al., 2023). For the purpose of this paper, we focus on simple cases for which we can provide tractable implementations. Besides the finite setting where \(\) can be enumerated, these are the following linearly parametrized feedback models.

**Example 2.1** (Linear Bandits, Abe and Long (1999)).: The model class is identified with a subset of \(^{d}\) and features \(_{}^{d}\) for each \(\). The reward function is \(r_{f}()=(),f\) and the observation distribution is \(M_{f}()=(_{},f,1)\).

The linear bandit setting can be generalized by separating reward and feedback maps, which leads to the _linear partial monitoring_ framework (Lin et al., 2014; Kirschner et al., 2020). Here we restrict our attention to the special case of _linear bandits with side-observations_(c.f. Kirschner et al., 2023), which, for example, generalizes the classical semi-bandit setting Mannor and Shamir (2011)

**Example 2.2** (Linear Bandits with Side-Observations).: As in the linear bandit setting, we have \(^{d}\), and features \(_{}^{d}\) that define the reward functions \(r_{f}()=_{},f\). Observation matrices \(M_{}^{m_{} d}\) for each \(\) define \(m_{}\)-dimensional observation distributions \(M_{f}()=(M_{}f,^{2}_{m_{}})\). In addition, we assume that \(_{}_{}^{} M_{}^{}M_{}\), which is automatically satisfied if \(_{}^{}\) is included in the rows of \(M_{}\), i.e. when the reward is part of the observations.

## 3 Regret Minimization via Saddle-Point Optimization

The goal of the learner is to choose decisions \(\) that achieve a small gap \((,f^{*})\) under the true model \(f^{*}\). Since the true model is unknown, the learner has to collect data that provides statistical evidence to reject models \(g f^{*}\) for which the regret \((,g)\) is large. To quantify the information-regret trade-off, we use a divergence \(D(\|)\) defined for distributions in \(()\). For a reference model \(f\), the information (or divergence) function is defined by:

\[I_{f}(,g)=D_{}(M_{g}()\|M_{f}())\,,\]

where \(D_{}(\|)\) is the KL divergence. Intuitively, \(I_{f}(,g)\) is the rate at which the learner collects statistical information to reject \(g\) when choosing \(\) and data is generated under the reference model \(f\). Note that \(I_{f}(,f)=0\) for all \(f\) and \(\). As we will see shortly, the regret-information trade-off can be written precisely as a combination of the gap function, \(\), and the information function, \(I_{f}\). We remark in passing that other choices such as the Hellinger distance are also possible, and the KL divergence is mostly for concreteness and practical reasons.

To simplify the notation and emphasize the bilinear nature of the saddle point problem that we study, we will view \(,I_{f}^{}_{+}\) as \(||||\) matrices (by fixing a canonical ordering on \(\) and \(\)). For vectors \(^{}\) and \(^{}\), we will frequently write bilinear forms \(_{f}\) and \( I_{f}\). This also means that by convention, \(\) will always denote a row vector, while \(\) will always denote a column vector. The standard basis for \(^{}\) and \(^{}\) is \((e_{})_{}\) and \((e_{g})_{g}\).

### The Decision-Estimation Coefficient

To motivate our approach, we recall the _decision-estimation coefficient_ (DEC) introduced by Foster et al. (2021, 2023), before introducing the main quantity of interest, the _average-constrained DEC_. First, the _offset decision-estimation coefficient_ (without localization) (Foster et al., 2021) is

\[^{o}_{}(f)=_{()}_{g } e_{g}- I_{f}e_{g}\]

The tuning parameter \(>0\) controls the weight of the information matrix relative to the gaps: Viewing the above as a two-player zero-sum game, we see that increasing \(\) forces the max-playerto avoid models that differ significantly from \(f\) under the min-player's sampling distribution. The advantage of this formulation is that the information term \( I_{f}e_{g}\) can be telescoped in the analysis, which directly leads to regret bounds in terms of the estimation error (introduced below in Eq. (8)). The disadvantage of the \(\)-parametrization is that the trade-off parameter is chosen by optimizing the final regret upper bound. This is inconvenient because the optimal choice requires knowledge of the horizon and a bound on \(_{f}^{o}_{}(f)\). Moreover, any choice informed by the upper bound may be conservative, leading to sub-optimal performance.

The _constrained decision-estimation coefficient_(Foster et al., 2023) is

\[^{c}_{}(f)=_{()}_{g} e_{g} I_{f}e_{g}^{2}\] (2)

In this formulation, the max player is restricted to choose models \(g\) that differ from \(f\) at most by \(^{2}\) in terms of the observed divergence under the min-player's sampling distribution. Note that because \(e_{}I_{f}e_{f}=0\) for all \(e_{}\), there always exists a feasible solution. For horizon \(n\), the radius can be set to \(^{2}}}{n}\), where \(_{}\) is a model estimation complexity parameter, thereby essentially eliminating the trade-off parameter from the algorithm. However, because of the hard constraint, strong duality of the Lagrangian saddle point problem (for fixed \(\)) fails, and consequently, telescoping the information gain in the analysis is no longer easily possible (or at least, with the existing analysis). To achieve sample complexity \(^{c}_{}(f)\), Foster et al. (2023) propose a sophisticated scheme that combines phased exploration with a refinement procedure (E2D\({}^{+}\)).

As the main quantity of interest in the current work, we now introduce the _average-constrained decision-estimation coefficient_, defined as follows:

\[^{ac}_{}(f)=_{()}_{ ()} I_{f} ^{2}\] (3)

Similar to the \(^{c}_{}\), the parameterization of the \(^{ac}_{}\) is via the confidence radius \(^{2}\), making the choice of the hyperparameter straightforward in many cases (more details in Section 3.2). By convexifying the domain \(()\) of the max-player, we recover strong duality of the Lagrangian (for fixed \(\)). Thereby, the formulation inherits the ease of choosing the \(\)-parameter from the \(^{c}_{}\), while, at the same time, admitting a telescoping argument in the analysis and a much simpler algorithm.

Specifically, Sion's theorem implies three equivalent Lagrangian representations for Eq. (3):

\[^{ac}_{}(f) =_{()}_{()}_{ 0}-( I_{f}-^{2})\] (4) \[=_{ 0,()}_{ ()}-( I_{f}-^{2})\] (5) \[=_{ 0}_{()}_{ ()}-( I_{f}-^{2})\] (6)

When fixing the outer problem, strong duality holds for the inner saddle-point problem in each line, however, the joint program in Eq. (5) is not convex-concave. An immediate consequence of relaxing the domain of the max player and Eq. (5) is that

\[^{c}_{}(f)^{ac}_{}(f)=_{  0}\{^{o}_{}(f)+^{2}\}\] (7)

The \(^{ac}_{}\) can therefore be understood as setting the \(\) parameter of the \(^{o}_{}\) optimally for the given confidence radius \(^{2}\). On the other hand, the cost paid for relaxing the program is that there exist model classes \(\) where the inequality in Eq. (7) is strict, and \(^{ac}_{}\) does not lead to a tight characterization of the regret (Foster et al., 2023, Proposition 4.4). The remedy is that under a stronger regularity condition and localization, the two notions are essentially equivalent (Foster et al., 2023, Proposition 4.8).

### Anytime Estimation-To-Decisions (Anytime-E2D)

Estimations-To-Decisions (E2D) is an algorithmic framework that directly leverages the decision-estimation coefficient for choosing a decision in each round. The key idea is to compute a sampling distribution \(_{t}()\) attaining the minimal DEC for an estimate \(_{t}\) of the underlying model, and then define the policy to sample \(_{t}_{t}\). The E2D approach, using the \(^{ac}_{}\) formulation, is summarized in Algorithm 1. To compute the estimate \(_{t}\), the E2D algorithm takes an abstract estimation oracle EST as input, that, given the collected data, returns \(_{t}\). The final guarantee depends on the _estimation error_ (or estimation regret), defined as the sum over divergences of the observation distributions under the estimate \(_{t}\) and the true model \(f^{*}\):

\[_{n}=_{t=1}^{n}_{t}I_{_{t}}e^{f_{*}} \] (8)

Intuitively, the estimation error is well-behaved if \(_{t} f^{*}\), since \(_{t}I_{f^{*}}e_{f^{*}}=0\). Equation (8) is closely related to the _total information gain_ used in the literature on information-directed sampling (Russo and Van Roy, 2014) and kernel bandits (Srinivas et al., 2010).

To bound the estimation error, Foster et al. (2021) rely on _online density estimation_ (also, _online regression_ or _online aggregation_) (Cesa-Bianchi and Lugosi, 2006, Chapter 9). For finite \(\), the default approach is the _exponential weights algorithm_ (EWA), which we provide for reference in Appendix A. When using this algorithm, the estimation error always satisfies \(_{n}(||)\), see (Cesa-Bianchi and Lugosi, 2006, Proposition 3.1). While these bounds extend to continuous model classes via standard covering arguments, the resulting algorithm is often not tractable without additional assumptions. For linear feedback models (Examples 2.1 and 2.2), one can rely on the more familiar ridge regression estimator, which, we show, achieves bounded estimation regret \(_{n}(d(n))\). For further discussion, see Appendix A.1.

With this in mind, we state our main result.

**Theorem 1**.: _Let \(_{t} 0\) be any sequence adapted to the filtration \(_{t}\). Then the regret of Anytime-E2D (Algorithm 1) with input sequence \(_{t}\) satisfies for all \(n 1\):_

\[R_{n}} ^{ac}_{_{t},_{t}}(_{t})}{_{t}^{2}}} _{t=1}^{n}_{t}^{2}+_{n}\]

_where we defined \(^{ac}_{,}(f)=_{()}_{ ()}-( I_{f}-^{2})\)._

As an immediate corollary, we obtain a regret bound for Algorithm 1 where the sampling distribution \(_{t}\) is chosen to optimize \(^{ac}_{_{t}}\) for any sequence \(_{t}\).

**Corollary 1**.: _The regret of Anytime-E2D (Algorithm 1) with input \(_{t} 0\) satisfies for all \(n 1\):_

\[R_{n}_{t[n],f}^{ac}_{ _{t}}(f)}{_{t}^{2}}}_{t=1}^{n}_{t} ^{2}+_{n}\]

Importantly, the regret of Algorithm 1 is directly controlled by the worst-case DEC, \(_{f}^{ac}_{}(f)\), and the estimation error \(_{n}\). It remains to set \(_{t}^{2}\) (respectively \(_{t}\)) appropriately. For a fixed horizon \(n\), we let \(_{t}^{2}=_{n}}{n}\). With the reasonable assumption that \(_{f}^{-2}^{ac}_{}(f) }\) is non-decreasing in \(\), Corollary 1 reads

\[R_{n} 2n_{f}^{ac}_{_{n}/n}}(f)}\,.\] (9)

This almost matches the lower bound \(R_{n}(^{c}_{1/}())\)3(Foster et al., 2023, Theorem 2.2), up to the estimation error and the beforehand mentioned gap between \(^{c}_{}\) and \(^{ac}_{}\).

To get an anytime algorithm with essentially the same scaling as in Eq. (9), we set \(_{t}^{2}=(||)/t\) for finite model classes, and \(_{t}^{2}=}}{t}\) if \(_{t}_{}(t)\) for \(_{}>0\). For linear bandits, \(_{}^{ac}\) (see Section 3.3), and \(_{n} d(n)\). Choosing \(_{t}^{2}=d/t\) recovers the optimal regret bound \(R_{n}}(d)\)(Lattimore and Szepesvari, 2020). Alternatively, one can also choose \(_{t}\) by minimizing an upper bound on \(_{t[n],f}\{_{_{t},_{t}}^{ac}(f) /_{t}^{2}\}\). For example, in linear bandits, \(_{_{t},}^{ac}+_{ t}^{2}\) (see Table 1); hence, for \(_{t}^{2}=d/t\), we can set \(_{t}=t/4\). Further discussion and refined upper bound for linear feedback models are in Section 3.3.

Proof of Theorem 1.: Let \(_{t}^{*}\) and \(_{t}^{*}\) be a saddle-point solution to the offset dec,

\[_{_{t}}^{o}(_{t})=_{()}_{ ()}-_{t} I_{_{t}}\]

Note that \(_{t}^{*}_{t}^{*}-_{t}_{t}^{*}I_{f}_{t}^{*}_{t} ^{*} e_{f}-_{t}_{t}^{*}I_{f}e_{f} 0\), which implies that \(_{t}_{t}^{2}_{_{t},_{t}}^{ac}\). Next,

\[R_{n}=_{t=1}^{n}_{t} e_{f^{*}}  =_{t=1}^{n}_{t} e_{f^{*}}-_{ t}(_{t}I_{_{t}}e_{f^{*}}-_{t}^{2})+_{t}(_{t}I_{ _{t}}e_{f^{*}}-_{t}^{2})\] \[_{t=1}^{n}_{g}_{t} _{_{t}}e_{g}-_{t}(_{t}I_{_{t}}e_{g}-_{t} ^{2})+_{t}(_{t}I_{_{t}}e_{f^{*}}-_{t}^{2})\] \[=_{t=1}^{n}_{()} _{()}-_{t}( I_{_{t}} -_{t}^{2})+_{t}(_{t}I_{_{t}}e_{f^{*}}-_{t} ^{2})\]

So far, we only introduced the saddle point problem by maximizing over \(f^{*}\). The last equality is by our choice of \(_{t}\) and \(_{t}\), and noting that \(()\) can always be realized as a Dirac. Continuing,

\[R_{n} _{t=1}^{n}_{_{t}, _{t}}^{ac}(_{t})+_{t}(_{t}I_{_{t}}e_{f^{*}}- _{t}^{2})\] \[_{t=1}^{n}_{ _{t},_{t}}^{ac}(_{t})+^{2}}_{_{t},_{t}}^{ac}(_{t})_{t}I_{_{t}}e_{f^{*}} \] \[} _{f}^{2}}_{ _{t},_{t}}^{ac}(f)}_{t=1}^{n}_{t}^{2} +_{t}I_{_{t}}e_{f^{*}}\]

We first drop the negative term in \((i)\) and use the beforehand stated fact that \(_{t}_{t}^{2}_{_{t},_{t}}^{ac}( _{t})\). The last step, \((ii)\), is taking the maximum out of the sum. 

### Certifying Upper Bounds

As shown by Corollary 1, the regret of Algorithm 1 scales directly with the \(_{}^{ac}\). For analysis purposes, it is however useful to compute upper bounds on the \(_{}^{ac}\) to verify the scaling w.r.t. parameters of interest. Via the equivalence Eq. (7), bounds on the \(_{}^{o}\) directly translate to the \(_{}^{ac}\) (see Table 1). For a detailed discussion of upper bounds in various models, we refer to Foster et al. (2021). Below, we highlight three connections that are directly facilitated by the \(_{}^{ac}\).

To this end, we first introduce a variant of the \(^{ac}_{}\) where the gap function depends on \(f\):

\[^{ac,f}_{}(f)=_{()}_{ ()}_{f} I_{f} ^{2}\,,\] (10)

where \(_{f}(,g)=r_{g}(_{g}^{*})-r_{f}()\). We remark that for distributions \(()\) and \(()\), the gap \(_{f}\) can be decoupled, \(_{f}=_{f}+_{f}e_{f}\), where we defined \(_{f}(g)=r_{g}(_{g}^{*})-r_{f}(_{f}^{*})\). The following assumption implies that the observations for a decision \(\) are at least as informative as observing the rewards.

**Assumption 1** (Reward Data Processing).: _The rewards and information matrices are related via the following data-processing inequality that holds for any \(()\):_

\[|_{}[r_{f}()-r_{g}()]|_{ }[D(M_{f}()\|M_{g}())]}\]

The next lemma shows that under Assumption 1, \(^{ac}_{}(f)\) and \(^{ac,f}_{}(f)\) are essentially equivalent, at least for the typical worst-case bounds where \(_{f}^{ac}_{}(f)()\).

**Lemma 1**.: _If Assumption 1 holds, then_

\[^{ac,f}_{}(f)-^{ac}_{}(f) ^{ac,f}_{}(f)+\]

The proof is in Appendix C.1. We remark that Algorithm 1 where the sampling distribution is computed for \(^{ac,f}_{}(_{t})\) and \(_{f}\) achieves a bound analogous to Theorem 1, as long as Assumption 1 holds. For details see Lemma 8 in Appendix C.

Upper Bounds via DecouplingFirst, we introduce the _information ratio_,

\[_{f}(,)=)^{2}}{ I_{f}}\]

The definition is closely related to the Bayesian information ratio (Russo and Van Roy, 2016), where \(\) takes the role of a prior over \(\). The Thompson sampling distribution is \(_{}^{}=_{h}_{h}e_{_{h}^{*}}\). The decoupling coefficient, \((f)\), (Zhang, 2022, Definition 1) is defined as the smallest number \(K 0\), such that for all distributions \(()\),

\[_{}^{}_{f}_{ 0}_{g,h }_{g}_{h}e_{_{h}^{*}}(r_{g}-r_{f})^{2}+ }=}_{g}_{h}e_{_{h}^{*}}(r_{g}-r _{f})^{2}}\] (11)

The next lemma provides upper bounds on the \(^{ac}_{}(f)\) in terms of the information ratio, which is further upper-bounded by the decoupling coefficient.

**Lemma 2**.: _With \((f)=_{}_{()}_{f}(,)\) and Assumption 1 satisfied, we have_

\[^{ac,f}_{}(f)(f)}\]

The proof follows directly using the AM-GM inequality, see Appendix C.2. By (Zhang, 2022, Lemma 2), this further implies \(^{ac,f}_{}\). An analogous result for the generalized information ratio (Lattimore and Gyorgy, 2021) that recovers rates \(^{}\) for \( 1\) is given in Appendix C.4.

PAC to RegretAnother useful way to upper bound the \(^{ac,f}_{}\) is via an analogous definition for the PAC setting (c.f. Eq. (10), Foster et al., 2023):

\[^{ac,f}_{}(f)=_{()}_{ }_{f} I_{f}^{2}\] (12)

**Lemma 3**.: _Under Assumption 1,_

\[^{ac,f}_{}(f)_{p}^{ ac,f}_{ p^{-1/2}}(f)+p_{}}\]

The proof is given in Appendix B.1. Lemma 3 combined with Theorem 1 leads to \((n^{2/3})\) upper bounds on the regret that are reminiscent of so-called globally observable games in linear partial monitoring (Kirschner et al., 2023).

Application to Linear Feedback ModelsTo illustrate the techniques introduced, we compute a regret bound for Algorithm 1 for linear bandits with side-observations (Examples 2.1 and 2.2).

**Lemma 4**.: _For linear bandits with side-observations and divergence \(I_{f}(,g)=\|M_{}(g-f)\|^{2}\),_

\[_{}^{ac,f}(f)_{()} _{b}\|_{b}\|_{V()^{-1}}\]

_where \(V()=_{}_{}M_{}M_{}^{}\). Moreover, denoting \(=_{()}_{b}\|_{b}\|_{V()^{-1}}\),_

\[_{}^{ac,f}(f),2^{2/3}^{1/3}_{}^{1/3}\]

The proof is given in Appendix B.2. While in the worst-case for linear bandits, there is no improvement over the standard \((d)\) without further refinement or specification of the upper bounds, in the case of linear side-observations there is an improvement whenever \(_{f}(f)\). To exemplify the improvement, consider a semi-bandit with a "revealing" action \(\), e.g. \(M_{}=_{d}\). Here, the regret bound improves to \(R_{n}\{d,d^{1/3}n^{2/3}\}\), since then \(_{}^{ac,f}(f)\). The corresponding improvement in the regime \(n d^{4}\) might seem modest, but is relevant in high-dimensional and non-parametric models. Moreover, in (deep) reinforcement learning, high-dimensional models are commonly used and the learner obtains side information in the form of state observations. Therefore, it is plausible that the \(n^{2/3}\) rate is dominant even for a moderate horizon. Exploring this effect in reinforcement learning is therefore an important direction for future work.

Notably, this improvement is _not_ observed by upper confidence bound algorithms and Thompson sampling, because both approaches discard informative but suboptimal actions early on [c.f. Lattimore and Szepesvari, 2017], including the action \(\) in the example above. E2D for a constant offset parameter \(>0\), in principle, attains the better rate, but only if one pre-commits to a fixed horizon. Lastly, we note that a similar effect was observed for information-directed sampling in sparse high-dimensional linear bandits [Hao et al., 2020].

### Computational Aspects

For finite model classes, Algorithm 1 can be readily implemented. Since almost no structure is imposed on the gap and information matrices of size \(||||\), avoiding scaling with \(||||\) seems hardly possible without introducing additional assumptions. Even in the finite case, solving Eq. (3) is not immediate because the corresponding Lagrangian is not convex-concave. A practical approach is to solve the inner saddle point for Eq. (5) as a function of \(\). Strong duality holds for the inner problem, and one can obtain a solution efficiently by solving the corresponding linear program using standard solvers. It then remains to optimize over \( 0\). This can be done, for example, via a grid search over the range \([0,_{f}^{-2}_{}^{ac}(f)]\).

In the linear setting, the above is not satisfactory because most commonly \(\) is identified with parameters in \(^{d}\). As noted before, ridge regression can be used instead of online aggregation while preserving the optimal scaling of the estimation error (see Appendix A.1). The next lemma further shows that the saddle point problem Eq. (3) can be rewritten to only scale with the size of the decision set \(||\).

**Lemma 5**.: _Consider linear bandits with side observations, \(=^{d}\) and quadratic divergence, \(I_{f}(,g)=\|M_{}(g-f)\|^{2}\), and denote \(_{}=_{}_{}_{}\) and \(V()=_{}_{}M_{}^{}M_{}\). Then_

\[_{}^{ac,f}(_{t})=_{ 0}_{ ()}_{b}(_{b}-_{},_{t})+ {1}{4}\|_{b}\|_{V()^{-1}}^{2}+^{2}\]

_Moreover, the objective is convex in \(()\)._

The proof is a straightforward calculation provided in Appendix C.5. Note that the saddle point expression is analogous to Eq. (5), and in fact, one can linearize the inner maximization over \(()\), such that the inner saddle point becomes convex-concave. This leads to expressions equivalent to Eqs. (4) and (6), albeit the objective is no longer linear in \(()\). We use Lemma 5 to employ the same strategy as before: As a function of \( 0\), solve the inner problem of the expression in Lemma 5, for example, as a convex program with \(||\) variables and \(||\) constraints (Appendix D). Then all that remains is to solve a one-dimensional optimization problem over \([0,_{f}^{-2}_{}^ {ac}(f)]\). We demonstrate this approach in Appendix E to showcase the performance of E2D on simple examples.

Conclusion

We introduced Anytime-E2D, an algorithm based on the estimation-to-decisions framework for sequential decision-making with structured observations. The algorithm optimizes the _average-constrained_ decision-making coefficient, which can be understood as a reparametrization of the corresponding offset version. The reparametrization facilitates an elegant anytime analysis and makes setting all remaining hyperparameters immediate. We demonstrate the improvement with a novel bound for linear bandits with side-observations, that is not attained by previous approaches. Lastly, we discuss how the algorithm can be implemented for finite and linear model classes. Nevertheless, much remains to be done. For example, one can expect the reference model to change very little from round to round, and therefore, it seems wasteful to solve Eq. (3) from scratch repetitively. Preferable instead would be an incremental scheme that iteratively computes updates to the sampling distribution.