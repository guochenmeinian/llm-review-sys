# United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories

Tianlong Xu\({}^{1}\), Chen Wang\({}^{1}\), Gaoyang Liu\({}^{*}\)\({}^{1}\), Yang Yang\({}^{2}\), Kai Peng\({}^{1}\), Wei Liu\({}^{1}\)

\({}^{1}\)Hubei Key Laboratoryof Smart Internet Technology, School of EIC,

Huazhong University of Science and Technology, \({}^{2}\)School of AI, Hubei University

\({}^{1}\){tianlongxu, chenwang, liugaoyang, pkhust, liuwei}@hust.edu.cn, \({}^{2}\)yangyang@hubu.edu.cn

Corresponding Author

###### Abstract

In recent years, deep neural networks (DNNs) have witnessed extensive applications, and protecting their intellectual property (IP) is thus crucial. As a non-invasive way for model IP protection, model fingerprinting has become popular. However, existing single-point based fingerprinting methods are highly sensitive to the changes in the decision boundary, and may suffer from the misjudgment of the resemblance of sparse fingerprinting, yielding high false positives of innocent models. In this paper, we propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories to verify the ownership of DNN models. Benefited from the intrinsic progressively adversarial level, the trajectory is capable of tolerating greater degree of alteration in decision boundaries. We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. Such a design enables a more unique and reliable fingerprinting with relatively low querying costs. Experiments on three datasets against four types of removal attacks show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.

## 1 Introduction

In recent years, deep neural networks (DNNs) have witnessed extensive applications, such as autonomous driving , AIGC  and medical diagnosis . However, training a practical DNN model requires significant computational resources, data and time. For example, the training of GPT-3 took about 21 days with a cost of over \(2.4\) million dollars . It is thus essential to protect the intellectual property (IP) of DNN models, especially when the models face the risk of being exposed or stolen by the so-called model extraction attacks .

Current technologies to protect model IP can be broadly categorized into two classes: model watermarking  and model fingerprinting . While the former embeds the watermark into the model to verify the identity of suspect models, it inevitably interferes in the training phase, which sacrifices the utility of the model or even introduces new security threats . So the latest research trend has shifted toward model fingerprinting, which enables the extraction of a model's fingerprint without any modifications to the model itself.

A common practice of model fingerprinting methods is to generate a batch of special adversarial samples near the decision boundary as fingerprinting samples for model verification . However, these _single-point_ fingerprinting samples are generated independently from each other, and are highly sensitive to the changes in the decision boundary, given theirinherent localized perspective. When the decision boundary changes, e.g., due to removal attacks [15; 16], a great number of such fingerprinting samples would become invalid (c.f. Figure 0(a)). Moreover, even two unrelated models may share similar portions of decision boundaries (i.e., the resemblance of sparse fingerprinting) [17; 18]. Therefore, single-point fingerprints are more prone to incorrectly identifying an unrelated (innocent) model as stolen, yielding a high false positive rate in the verification process, as observed in [10; 12].

In this paper, we propose ADV-TRA, a more robust fingerprinting method for DNN models. Instead of using single-point fingerprinting samples to identify the model, ADV-TRA exploits novel adversarial trajectories, each of which is a chain of progressively adversarial samples2, representing adversarial level from weak to strong (c.f. Figure 0(b)). The adversarial trajectory, incorporating multi-level adversarial perturbations, could provide a more precise localization of the decision boundary, which enables to accurately capture the alteration in the boundary induced by removal attacks, thus exhibiting a more robust model IP verification.

Though the idea is simple, there are two major challenges. First, it is difficult to control the trajectory precisely to pass through the decision boundary with fixed length (i.e., the number of samples in the trajectory). Fixed step sizes in existing adversarial sample generation may result in too long or too short trajectories, yielding substantial querying times and unstable local optima, or compromising effectiveness. To tackle this issue, we view each step size as an optimized variable and design several loss functions and strategies to enforce fixed-length trajectories with dynamically adjusted step sizes. This leads to smaller steps for samples near the boundary to focus on more subtle details, while larger steps for samples far away from the boundary to quickly move towards the boundary.

Second, it is not easy to capture the global fingerprint features to avoid the misjudgment of the resemblance of sparse fingerprinting, since the trajectory across neighboring classes remains limited to features of a single decision boundary. To address this challenge, we propose to generate a surface trajectory that comprises numerous adversarial trajectories orderly across multiple classes, which can offer a more comprehensive representation of the decision surface (instead of a single boundary), thereby significantly reducing the false positives of innocent models.

Our major contributions can be summarized as follows:

* We propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories for model verification. Benefited from the intrinsic progressively adversarial level, the trajectory is able tolerate greater degree of alteration in decision boundaries.
* We design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. Such a design enables a more unique and reliable fingerprinting with relatively low querying costs.
* We conduct extensive experiments on three datasets against four types of removal attacks. Experimental results show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.

Figure 1: Comparing single-point fingerprinting samples and our adversarial trajectories. As can be seen, when the decision boundary alters (from the solid black line to the dashed red line), (a) a portion of single-point fingerprinting samples near the boundary become invalid, while (b) the majority of samples in the trajectories remain effective.

Preliminary

### Adversarial Samples

Given a target model, the goal of adversarial samples is to deceive the model into making incorrect predictions [19; 20]. In the context of a classification model represented as \(f\), an adversarial sample, \(=x+\), is crafted to perturb a clean sample \(x\) with a ground-truth label \(y\) in such a way that: (1) the perturbation \(\) is kept small; and (2) the predicted class for \(\) is altered. This can be formalized as:

\[\ \|x-\|\ \ \ \ f() y,\] (1)

where \(\|\|\) is a distance metric, e.g., \(_{2}\) distance. In this way, the clean sample \(x\) and its perturbed counterpart \(\) may appear nearly identical to human observers, but the target model perceives them as entirely distinct entities.

### Model Fingerprinting

As a non-invasive way to validate the ownership of a DNN model, model fingerprinting seeks to detect some fingerprints which is normally the model's decision boundary. Generally, the procedure of model fingerprinting involves two main steps: fingerprint extraction and fingerprint verification:

**Fingerprint Extraction.** Suppose a model owner who trains a source model, his goal is to generate a number of fingerprinting samples that can uniquely characterize the decision boundaries. To this end, model fingerprinting borrows ideas from adversarial samples, by generating a set of adversarial examples that are close to the decision boundary, paired with their corresponding predicted labels, as the fingerprinting samples [9; 11; 14]. More specifically, given a normal sample \(x\) and its ground truth label \(y\), the fingerprinting sample \(x_{}\) can be derived from optimizing towards \(y_{}\), where \(y_{}\) is the fingerprinting label that differs from \(y\). Therefore, for a source model \(f_{}\), the corresponding fingerprinting sample \(x_{}\) just crosses the decision boundaries, such that \(f_{}(x_{}) f_{}(x)\).

**Fingerprint Verification.** During the verification phase, when dealing with a suspect model \(f_{}\), the model owner can determine the ownership by querying it with the set of fingerprinting samples \(D_{}=\{(x_{},y_{})\}\). Based on the output results, the owner selects a testing metric \(()\), e.g., the accuracy, and computes \((f_{},D_{})\). By comparing this result with a threshold, the owner can make the final judgment regarding any potential ownership infringement.

### Removal Attacks

Removal attacks are designed to invalidate fingerprints by tampering with the source model \(f_{}\). We denote a model that has been processed by removal attacks as \(f_{}\), with the goal being to alter the model such that \(f_{}(x_{}) f_{}(x_{})\). There are several ways to launch removal attacks, e.g., by model modification (fine-tuning , pruning [22; 23], adversarial training [24; 25]), or model extraction [26; 5]. Removal attacks pose a significant threat to model fingerprinting. For example, when the source model undergoes adversarial training, a considerable portion of these fingerprints may change (i.e., fingerprinting samples become obsolete in the verification phase) [9; 12; 16].

## 3 Problem Formation

We consider a typical scenario consisting of two parties: the model owner (defender) and the attacker, as shown in Figure 2. The model owner trains a well-performed model (i.e., the source model) and deploys it as a cloud service or client-sided software. The attacker attempts to steal the source model in either black-box or white-box ways.

**Attacker's Strategy.** We first consider a strong white-box attacker, who can get access to the entire information of the source model, including the model structure and all inner parameters, as well as

Figure 2: Illustration of model stealing and verification.

sufficient auxiliary data3, through server intrusion or

eavesdropping on communication channels. Once the attacker acquires the source model, he could leverage removal attacks to modify the model from either the model structure or parameters, so as to evade the IP infringement detection.

We also consider a black-box attacker, who only has the input-output prediction interface (a.k.a. API) to the source model. By leveraging an auxiliary dataset, the attacker is able to duplicate the source model by multiple querying, e.g., model extraction attack, and utilizing the querying results to train a substitute model from scratch.

**Defender's Strategy.** The goal of the model owner is to verify whether a deployed suspect model is derived from the source model. The model owner naturally has the white-box access to the source model, but is assumed to only have the black-box access to the suspect model4 and observe its prediction for any input. In specific, he can extract a set of fingerprinting samples from the source model and assess their performance on the suspect model. If the number of fingerprinting samples that validate successfully exceeds a threshold, then it can be concluded that the suspect model has been illegally stolen from the source model.

From the perspective of the defender, we aim to design a DNN fingerprinting method satisfying the following properties: (1) **Uniqueness.** Uniqueness is crucial for ensuring the reliability and fidelity of a fingerprinting scheme. Fingerprint extraction should comprehensively and fully capture features, so as to avoid local resemblance-induced misjuddment. (2) **Robustness.** A robust fingerprinting scheme needs to be resistant to removal attacks, even under relatively radical attacking strategies. (3) **Efficiency.** Considering the API may charge per query and excessive queries may raise the attacker's suspicion, the fingerprinting method may require as few query samples as possible.

## 4 Design of ADV-TRA

We present the workflow of ADV-TRA in Figure 3. In the trajectory generation phase, given the source model \(f_{src}\) and a base sample \(x_{0}\) of class \(c_{0}\), ADV-TRA first initializes the trajectory from one class \(c_{0}\) towards another class \(c_{1}\). Subsequently, the trajectory is fine-tuned for probing the decision boundary and then simply bilateralized to enforce a cross-class chain of progressively adversarial samples. Finally, we introduce the surface trajectory \(_{}\) which traverses among multiple classes to fingerprint the entire decision surface. In the verification phase, we use \(_{}\) to compute the mutation rate \(r_{}\) for a suspect model \(f_{}\), and determine whether \(f_{}\) is stolen from \(f_{}\).

### Trajectory Generation

We use adversarial trajectories to fingerprint the source model's decision boundaries. A naive way of generating the adversarial trajectory is to record all intermediate products using adversarial sample algorithms like Basic Iterative Method (BIM) . However, this approach does not consider the distance from a base sample \(x_{0}\) to its corresponding decision boundary, and often uses a fixed step size, which would cause too few (or too many) trajectory samples, resulting in lower effectiveness (or excessive queries, even getting stuck in local optima and unable to cross the decision boundary).

Therefore, we propose an adaptive step-size adversarial trajectory generation algorithm that can achieve fixed length between any two classes by dynamically adjusting the step size, by involving the following four steps: (1) trajectory initialization; (2) boundary probing; (3) trajectory bilateralization; and (4) trajectory connection.

**Trajectory Initialization.** Given a source model \(f_{}\), a base sample \(x_{0}\) (with its ground truth label \(y_{}\)), and a target label \(y_{}\), a trajectory is initialized just like the process of generating adversarial samples with gradient descent. Instead of using fixed step size, we view the step size \(s_{i}\) in all rounds of iterations \(=\{s_{0},s_{1},,s_{l-1}\}\) as an optimizable variable and generate the _initial trajectory_\(_{}\)\[_{int} =(f_{},x_{0},y_{},)= \{x_{0},x_{1},,x_{l}\}\] (2) s.t. \[x_{i+1} =x_{i}-s_{i}((f_{},x _{i},y_{})),\] for \[i=0,1,,l-1,\]

where \((f_{},x_{i},y_{})\) is the loss function to guide the prediction towards \(y_{}\), and \(l\) is the length of \(_{int}\).

**Boundary Probing.** Recall that we use \(_{int}\) of length \(l\) to probe the decision boundary, so it is crucial to scale \(_{int}\) such that it precisely reaches the decision boundary, i.e., the last sample \(x_{l}\) in \(_{int}\) happens to predicted as \(y_{}\) (whereas the prior samples are not):

\[f_{}(x_{i})  y_{},\ \ \ i=1,2,,l-1\] (3) \[f_{}(x_{i}) =y_{},\ \ \ i=l.\]

To this end, we propose a simple length control strategy, which scales \(_{int}\) by adjusting the step sizes \(\) with a length control variable \(_{l_{c}}\) (\(0<_{l_{c}}<1\)). Specifically, given a trajectory, we first check whether the requirements in Eq. (3) are satisfied, and further determine whether to increase or decrease the step size. If the trajectory reaches the decision boundary early (late), we need to reduce (resp. enlarge) the step size of all steps \(\) by multiplying a length control factor \(_{l_{c}}\) (resp. \(1/_{l_{c}}\)).

In order to record more details near the decision boundary, the step size \(s_{i}\) is expected to decay when current \(x_{i}\) are closer to the decision boundary, i.e., \(s_{i+1}<s_{i}\). For this purpose, we define the brake loss function \(_{}\) as:

\[_{}()=_{i=0}^{l-1}(_{ } s_{i}-s_{i+1})^{2},\] (4)

where \(_{}\) (\(0<_{}<1\)) is the brake factor that controls the proportional relationship of two adjacent step sizes.

Moreover, to ensure the internal forward trend within the trajectory, i.e., \(x_{i+1}\) is closer to the decision boundary of \(y_{}\) than \(x_{i}\), each step \(s_{i}\) should be greater than \(0\). So we define the forward loss function \(_{}\) to prevent any recession in the trajectory:

\[_{}()=_{i=0}^{l-1}(s_{i}<0)  s_{i},\] (5)

where \(()\) is the indicator function.

Hence, training the step sizes \(\) involves the above two optimization objectives:

\[Min:=_{}()+_{}().\] (6)

We optimize \(\) using the gradient descent algorithm. Specifically, the optimization terminates when satisfying the following conditions:(1) The last sample \(x_{l}\) in \(_{int}\) is predicted as \(y_{}\), while the prior samples are not, as described in Eq. 3. (2) The step size for the next sample \(s_{i+1}\) is smaller than the step size \(s_{i}\) for the previous sample, i.e., \(s_{i+1}<s_{i}\).

**Trajectory Bilateralization.** Given the optimized \(_{int}\), we next extend it to a _bilateral trajectory_\(_{bi}\). This is necessary due to the random variation in the perturb direction of the decision boundary

Figure 3: Exemplary illustration of ADV-TRA pipeline (the length of \(_{int}\)\(l=3\); the length of \(_{}\) is \(2lm\), exclusive of \(c_{0}\)). A darker color of the trajectory sample indicates a higher level of adversarial strength towards the target class.

when subjected to removal attacks, and the trajectory is preferred to evenly attend to both sides of the decision boundary to keep a consistent measuring scale for both sides. We thus utilize a mirroring operation, centered on \(x_{l}\), to make up the other half of the trajectory, using the reversed step sizes \(}=\{s_{l-1},s_{l-2},,s_{0}\}\), and a bilateral trajectory \(_{bi}\) can thus be generated as:

\[_{bi}=(f_{src},x_{0},y_{tgt},})=\{x_{0},x_{1},,x_{2l}\},\] (7)

where \(_{bi}\) crosses to the other side of the decision boundary starting from \(x_{l}\), and is able to locate the boundary well with the intrinsic progressively adversarial samples.

It is worth noting that the bilateral trajectory may pass through more than two classes. Such a case does not lower the efficacy of the trajectory; instead, it serves as a special feature regarding to the model's decision surface, which makes our fingerprinting more unique.

**Trajectory Connection.** So far we can fingerprint a specific decision boundary between two classes \(c_{0}\) and \(c_{1}\) by a bilateral trajectory \(_{bi}^{c_{0} c_{1}}\). However, a decision boundary alone, as discussed before, is not sufficient to fully capture the fingerprint of the model. While two unrelated models may share some similarities along the decision boundary, their decision surfaces would almost never be the same [17; 18]. Therefore, we introduce the _surface trajectory_\(_{}\) to fingerprint the entire decision surface, which merges all the bilateral trajectories to traverse through all classes. This can be done by simply connecting the bilateral trajectories in order, i.e., the last sample in \(_{bi}^{c_{0} c_{1}}\) is the first sample of \(_{bi}^{c_{1} c_{2}}\) (c.f. Figure 3).

It is noticed that, it may be costly to involve all the bilateral trajectories, especially for complex models. Therefore, a more practical way for large models is to randomly select \(m\) bilateral trajectories for the surface trajectory generation (c.f. Algorithm 1 in Appendix B):

\[_{}=_{bi}^{c_{0} c_{1}} _{bi}^{c_{1} c_{2}}..._{bi}^{c_{m-1} c_{m}},\] (8)

Our experiments indicate that a small number of \(m=10\) is sufficient to fingerprint the 100-class CIFAR-100 and 1000-class ImageNet models.

### Trajectory Verification

In the verification phase, the model owner can query the given suspect model \(f_{}\) with the surface trajectory \(_{}\). We then calculate the mutation rate of \(_{}\) from \(f_{src}\) to \(f_{}\) by:

\[r_{}=_{}|}_{x_{i} _{}}(f_{}(x_{i}) f_{ }(x_{i})).\] (9)

Here, \(r_{}\) reflects the proportion of samples in the trajectory whose predictions on \(f_{}\) differ from \(f_{}\). Theoretically, an infringement model is expected to possess a lower \(r_{}\) than an innocent model, since it originates from the source model \(f_{}\) and shares a more similar decision surface with it. The defender can determine whether \(_{}\) is also the exclusive trajectory of \(f_{}\) by comparing the obtained \(r_{}\) and a threshold \(_{}\) (\(0.5\) in our case, equivalent to random guess). In practice, we calculate the detection rate of a number of adversarial trajectories (i.e. surface trajectories) on the suspect model to make the final infringement judgment.

## 5 Experiments

We use three widely-used benchmark datasets, namely, CIFAR-10 , CIFAR-100 , and ImageNet , in model fingerprinting domain to evaluate the performance of our ADV-TRA. We compare our approach with four existing model fingerprinting techniques, including **CoRt**, **IPGuard**, **CAE**, and **UAP**. We also evaluate the robustness of ADV-TRA against four types of removal attacks (fine-tuning [6; 32], pruning , adversarial training , and model extraction attacks [26; 27; 35]) across multiple model architectures. In Appendix C we furthermore list details on experiment setups, including dataset, models, evaluation metrics, and implementation details.

Moreover, we reveal how removal attacks affect the effectiveness of fingerprints by altering the decision surface in Appendix D.1. Additionally, we compare a simple approach (trajectory spanning only two categories) with trajectories spanning multiple categories (see Appendix D.2), and conduct a series of ablation studies to analyze the impact of parameters of ADV-TRA (see Appendix D.3). At last, we demonstrate how the progressiveness of the trajectory enables it to locate the decision boundaries (see Appendix D.4).

### Main Performance

We first validate ADV-TRA on \(100\) suspect models for each source model, including \(50\)_positive_ models under various attacks and \(50\)_negative_ models on CIFAR-10 dataset. In our paper, the positive models originate from the source model but have been processed by removal attacks, including model pruning, adversarial training, and fine-tuning (Fine-Tune Last Layer (**FTLL**), Fine-Tune All Layers (**FTAL**), Retrain Last Layer (**RTLL**), and Retrain All Layers (**RTAL**)). The negative models come from four ways: (1) **SAST**: the same architecture as victim model and the same training data (Note that model initialization and training randomness can still lead to differences from the source model); (2) **SADT**: the same architecture as victim model but different training data; (3) **DAST**: a different architecture but the same training data; (4) **DADT**: a different architecture and different training data (For more details, please refer to Appendix C). The results are shown in Table 1.

It is evident that our ADV-TRA outperforms other methods in most cases (12 out of 14), achieving high detection rates for positive models while maintaining low detection rates for negative models. Only ADV-TRA is able to clearly separate the two types of models, which is adequate to guarantee the subsequent verification without any false positives. Its corresponding lower bound of positive models is \(0.612\) (corresponds to Adv-0.01), which is still much higher than the upper bound of negative models (\(0.143\), corresponds to SAST). In contrast, other methods fail to distinguish between some of the positive models and negative models. It is also surprising to see that our method has a low false positive rate: the detection rate of ADV-TRA for negative models is much lower than the three baselines. This is attributed to our design of the chain of progressively adversarial samples, which is able to simultaneously capture more characteristics at various levels to avoid false positives.

One intermediate product average mutation rate \(_{mut}\) reflects the dissimilarity in the decision boundary of a source model and a suspect model. It can be observed that a removal attack causing higher \(_{mut}\) brings about lower fingerprint detection rate. For example, three fine-tuning methods (**FTLL**, **FTAL**, and **RTLL**) only result in a \(_{mut}\) of less than \(0.06\), corresponding to a relatively high fingerprint detection rate of approximately \(0.95\). Negative models have a high \(_{mut}\) (over \(0.7\)) since their decision boundaries differ more significantly from the source model.

To further validate the ability of the fingerprinting samples to accurately distinguish between positive and negative models, we present the ROC curve and the distribution of all suspect models in Figure 3(a) and Figure 3(b). Only ADV-TRA gets an \(AUC=1\), significantly outperforming other approaches. As can be seen, CoRt, IPGuard, and CAE all do not readily distinguish between the two types of suspect models, exhibiting strong cross-model transferability. Especially for general adversarial samples (CoRt), their transferability to negative models is more obvious, leading to high detection rate. For ADV-TRA, the detection rate of the negative model is quite low, approximately \(0.15\). This also serves as empirical evidence for our analysis that our trajectories have an extremely low false positive rate.

    &  &  \\   & Accuracy & \(_{mut}\) & IPGuard & CAE & UAP & **Ours** \\   & \(0.874 0.000\) & \(0.000 0.000\) & \(1.000 0.000\) & \(1.000 0.000\) & \(1.000 0.000\) & \(1.000 0.000\) \\   & FTLL & \(0.868 0.001\) & \(0.043 0.002\) & **0.978 \(\) 0.001** & \(0.956 0.002\) & \(0.958 0.001\) & \(0.952 0.000\) \\   & FTLL & \(0.867 0.001\) & \(0.058 0.002\) & \(0.942 0.002\) & \(0.934 0.001\) & **0.954 \(\) 0.002** & \(0.950 0.000\) \\   & RTLL & \(0.865 0.000\) & \(0.030 0.001\) & \(0.917 0.001\) & \(0.921 0.001\) & \(0.944 0.002\) & **0.947 \(\) 0.001** \\   & RTLL & \(0.864 0.000\) & \(0.143 0.001\) & \(0.621 0.001\) & \(0.663 0.002\) & \(0.719 0.001\) & **0.763 \(\) 0.002** \\   & P-20\% & \(0.872 0.001\) & \(0.035 0.001\) & \(0.895 0.001\) & \(0.924 0.000\) & \(0.946 0.001\) & **0.955 \(\) 0.000** \\   & P-40\% & \(0.869 0.000\) & \(0.064 0.001\) & \(0.783 0.001\) & \(0.894 0.001\) & \(0.934 0.001\) & **0.945 \(\) 0.001** \\   & P-80\% & \(0.825 0.002\) & \(0.257 0.003\) & \(0.587 0.001\) & \(0.628 0.001\) & \(0.634 0.002\) & **0.678 \(\) 0.001** \\   & Adv-0.001 & \(0.859 0.001\) & \(0.238 0.001\) & \(0.423 0.000\) & \(0.421 0.001\) & \(0.513 0.001\) & **0.758 \(\) 0.001** \\   & Adv-0.01 & \(0.863 0.000\) & \(0.394 0.001\) & \(0.162 0.001\) & \(0.158 0.001\) & \(0.348 0.000\) & **0.612 \(\) 0.001** \\   & Adv-0.1 & \(0.865 0.001\) & \(0.205 0.002\) & \(0.184 0.001\) & \(0.256 0.002\) & \(0.402 0.002\) & **0.628 \(\) 0.001** \\   & SAST & \(0.879 0.000\) & \(0.714 0.000\) & \(0.631 0.001\) & \(0.561 0.001\) & \(0.412 0.000\) & **0.143 \(\) 0.000** \\   & SADT & \(0.876 0.001\) & \(0.722 0.001\) & \(0.497 0.001\) & \(0.422 0.002\) & \(0.382 0.002\) & **0.126 \(\) 0.000** \\   & DAST & \(0.875 0.002\) & \(0.762 0.001\) & \(0.454 0.001\) & \(0.389 0.002\) & \(0.323 0.001\) & **0.124 \(\) 0.001** \\   & DADT & \(0.874 0.003\) & \(0.787 0.001\) & \(0.357 0.002\) & \(0.219 0.001\) & \(0.313 0.001\) & **0.116 \(\) 0.001** \\   

Table 1: Main results on CIFAR-10 dataset. P-20% denotes model pruning with a pruning rate \(p=0.2\); Adv-0.001 represents adversarial training with budget \(=0.001\). Fingerprint detection rate in bold indicates the best performance. For positive models, a higher fingerprint detection rate is preferred, suggesting better ability to verify IP infringement. In contrast, negative models are expected to yield a lower detection rate, avoiding false verification.

To evaluate the universality of our method, we conduct experiments on three datasets as illustrated in Table 2. It can be found that our method extracts the most strongly detectable fingerprints, which achieves AUC of \(1.0\) on CIFAR-10 and CIFAR-100. In all cases, ADV-TRA has better AUC, T1%F, and T10%F than other fingerprinting methods. For the models trained on ImageNet, we conjecture that classification models with more classes have more complex decision boundaries. When subjected to removal attacks, the model's functionality (such as predicting the probability of each class) is more prone to changes, resulting in less robustness compared to models with fewer classes.

### Robustness Against Removal Attacks

**Impact of Fine-tuning.** We first employ a more radical fine-tuning strategy (i.e., with a higher learning rate and many more epochs) to better simulate real-world removal attacks. Figure 5 shows the results on the CIFAR-100 and ImageNet datasets. For CIFAR-100 dataset, we can see that the FTLL, FTAL, and RTLL models are hardly affected and consistently demonstrate a high fingerprint detection rate (around \(0.96\)), whereas the detection rate for the RTAL model steadily decreases, dropping to \(0.72\) after \(50\) epochs. As for ImageNet dataset, which has more classes, the detection rate of the positive models experiences a more significant decline, dropping from \(1.0\) to below \(0.5\) within the first \(12\) fine-tuning epochs. After that, the FTLL, FTAL, RTLL models begin to stabilize, but the RTAL model continues declining at a high rate. When the epoch reaches \(50\), the detection rate of the RTAL model is \(0.290\), still significantly higher than the upper bound of the negative models (\(0.143\)). In general, fine-tuning is not a considerable threat to ADV-TRA.

**Impact of Pruning.** Next, we test the impact of pruning, by varying the pruning rate from \(10\%\) to \(90\%\). Figure 6 illustrates the fingerprint detection rate and model accuracy for each pruning level on CIFAR-100 and ImageNet. We can clearly see that the detection rate has a strong correlation with the model accuracy. For CIFAR-100 dataset, as the pruning rate increases from \(10\%\) to \(60\%\), the fingerprint detection rate decreases slightly from \(0.986\) to \(0.920\). When the pruning rate exceeds \(70\%\), the fingerprint detection rate experiences a sharp decline (from \(0.832\) to \(0.367\)). The results on ImageNet dataset also reveal two declining lines with increasing curvature. Even at a pruning rate of \(90\%\), the fingerprinting samples could no longer reliably differentiate between positive (\(0.114 0.064\)) and negative (\(0.001\)\(\)\(0.143\)) models. However, when so many fingerprinting samples become invalid, the model accuracy also suffers a sharp decrease. In practice, considering the model's basic functionality, an attacker would not implement pruning attack of such strength.

**Impact of Adversarial Training.** To investigate how adversarial training affects the effectiveness of fingerprinting samples, we vary the perturbation budget \(\) from \(0.001\) to \(1.0\) and attack the source model for \(20\) epochs (\(=1.0\) is quite large for adversarial training but we still test with this value). Figure 6(a) illustrates the results for CIFAR-100. Interestingly, we observe a trend where the detection rate first decreases (from \(1.0\) to \(0.60\)) and then increases (from \(0.60\) to \(0.76\)), as \(\) rises from \(0.001\) to \(1.0\). A perturbation budget of \(0.01\) achieves the best attack result, rendering \(40\%\) of fingerprinting samples invalid. This implies that an attacker does not necessarily have to impose adopt higher \(\) to launch stronger attacks. For \(=0.01\), our approach successfully detects \(60\%\) of trajectories, which are about \(3\) times that of IPGuard and CAE, and \(1.5\) times that of UAP, demonstrating strong robustness against adversarial training. Additionally, we record the results during \(50\) epochs of adversarial training in Figure 6(b). The first \(5\) epochs witness a big drop of fingerprint detection rate. As the number of training epochs continues to increase, the fingerprint detection rates of these models remains almost unchanged. We speculate that the models have already attained the \(-\)adversarial robustness during the first few training epochs. As such, subsequent training does not further decrease the effectiveness of fingerprinting samples.

**Impact of Model Extraction Attacks.** Since the fingerprint detection rate alone cannot reflect the gap between positive and negative models, here, we calculate the F1-score of correctly identified fingerprint samples under PRADA and Knockoff attacks. From the results in Table 3, we can observe that our method achieves the best results, leading the second-place method (UAP) by nearly \(0.1\) under all attacks. It is worth noting that better extraction performance (higher test accuracy) is inevitably accompanied by a substitute model that learns a decision surface more similar to the source model. This also leads to a better transfer of the fingerprint space (a part of the decision surface) as well. Moreover, as a data-free model stealing attack with access to only hard labels, HL distills a relatively more dissimilar substitute model compared to the source model under less prior knowledge, resulting in the lowest F1-score among the four fingerprinting methods.

## 6 Conclusion

In this paper, we have presented ADV-TRA, a robust fingerprinting scheme that leverages adversarial trajectories to fingerprint the DNN model. ADV-TRA generates a chain of samples with varying levels of adversarial perturbation, and further extends to the surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. By doing so, the alteration in the decision boundary can be captured more accurately, and the misjudgment caused by local resemblance from innocent models can be significantly reduced. Extensive evaluation results on three datasets demonstrate that ADV-TRA is able to defend against various removal attacks even under severe attack intensities, exhibiting greatly superior robustness and lower false positive rates compared to existing state-of-the-art fingerprinting methods.