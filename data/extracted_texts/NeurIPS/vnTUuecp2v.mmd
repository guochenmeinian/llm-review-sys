# Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium -- Except When They Do

Sarah A. Toonsi

Industrial and Enterprise Systems Engineering

University of Illinois Urbana-Champaign

Urbana, Illinois, USA

stoonsi2@illinois.edu

&Jeff S. Shamma

Industrial and Enterprise Systems Engineering

University of Illinois Urbana-Champaign

Urbana, Illinois, USA

jshamma@illinois.edu

###### Abstract

The framework of multi-agent learning explores the dynamics of how an agent's strategies evolve in response to the evolving strategies of other agents. Of particular interest is whether or not agent strategies converge to well-known solution concepts such as Nash Equilibrium (NE). In "higher-order" learning, agent dynamics include auxiliary states that can capture phenomena such as path dependencies. We introduce higher-order gradient play dynamics that resemble projected gradient ascent with auxiliary states. The dynamics are "payoff-based" and "uncoupled" in that each agent's dynamics depend on its own evolving payoff and has no explicit dependence on the utilities of other agents. We first show that for any polymatrix game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead (locally) to that NE, both for the specific game and nearby games with perturbed utility functions. Conversely, we show that for any higher-order gradient play dynamics, there exists a game with a unique isolated completely mixed-strategy NE for which the dynamics do not lead to NE. Finally, we show that convergence to the mixed-strategy equilibrium in coordination games can come at the expense of the dynamics being inherently internally unstable.

## 1 Introduction

The field of learning in games explores how game-theoretic solution concepts emerge as the outcome of dynamic processes where agents adapt their strategies in response to the evolving strategies of other agents (1; 2; 3; 4). There is a multitude of specific cases of learning dynamics/game combinations that result in a range of outcomes, including convergence, limit cycles, chaotic behavior, and stochastic stability (5; 6; 7; 8; 9; 10; 11; 12; 13). The emphasis in the literature is on simple adaptive procedures, called "natural" dynamics in (14), that can result in various solution concepts (e.g., Nash equilibrium, correlated equilibrium, and coarse correlated equilibrium). (A separate concern is the complexity associated with such computations (e.g. (15; 16)).)

One "natural" restriction for learning is that the dynamics of one agent should not depend explicitly on the utility functions of other agents. This restriction was referred to as "uncoupled" dynamics in (17), where the authors constructed a specific anti-coordination matrix game for which no uncoupled learning dynamics could converge to the unique mixed-strategy NE. The dynamics considered in that setting were of fixed order, i.e., the order of the learning dynamics was restricted to match the dimension of the strategy space. More recent work showed that specific instances of fixed order uncoupled learning dynamics can never lead to mixed-strategy NE (18). Furthermore, there exist games for which any fixed order learning dynamics are bound to have an initial condition starting from which the dynamics do not converge to NE (19).

The restriction on the order of the learning dynamics in learning mixed-strategy NE turns out to be essential. In particular, by introducing additional auxiliary states, (20) showed that higher-order learning could overcome the obstacle of convergence to NE in the same anti-coordination game considered in (17) while remaining uncoupled.

Higher-order learning in games can be seen as a parallel to higher-order optimization algorithms, such as momentum-based or optimistic gradient algorithms (e.g., (21; 22)). Such algorithms utilize history to update the underlying search parameter. In this way, there is a path dependency on the trajectory of information. An early utilization of higher-order learning is in (23), in which a player's strategy update uses two stages of history of an opponent's strategies in a zero-sum setting to eliminate oscillations. Similar ideas were used in (24). Reference (25) modified gradient-based algorithms through the introduction of a cumulative (integral) term. In (20), higher-order dynamics were used to create a myopic forecast of the action of other agents. Authors in (26) introduce a version of higher-order replicator dynamics and show that, unlike fixed order replicator dynamics, weakly dominated strategies become extinct. Reference (27) utilizes the system theoretic notion of passivity to analyze a family of higher-order dynamics.

In this paper, we further explore the implications of learning dynamics that are uncoupled. We address "payoff based" dynamics, in which the learning dynamics depend on the evolution of a payoff vector that is viewed as an externality. When players are engaged in a game, then the payoff stream of one agent depends on the actions of other agents. However, the learning dynamics themselves do not change based on the source of the payoff streams.

First, we show that for any polymatrix game with a mixed-strategy NE, there exist payoff-based dynamics that converge locally to that NE. This result is established by making a connection between convergence to NE and the existence of decentralized stabilizing control (28; 29). A consequence of the payoff-based structure is that the dynamics also converge to the NE of nearby perturbations of the original game. The form of higher-order learning used for this stability result is higher-order gradient play, which generalizes gradient ascent. Next, we show that for any such dynamics, there exists a game with a unique mixed-strategy NE that is unstable under given dynamics. The tool utilized is a classical analysis method in feedback control systems known as root-locus (e.g., (30), (31)), which characterizes the locations of the eigenvalues of a matrix as a function of a scalar parameter. A combination of the above results suggests the lack of universality on the side of both learning dynamics and games. While any mixed-strategy NE can be stabilized by suitable higher-order gradient dynamics, any such dynamics can be destabilized by a suitable anti-coordination game. Finally, we examine the implications of higher-order dynamics being able to converge to the mixed-strategy NE of a \(2 2\) coordination game, which has two pure NE and one mixed-strategy NE. We show that such higher-order gradient play dynamics must have an inherent internal instability, which makes them unsuitable, if not irrational, as a model of learning.

## 2 Payoff-based learning dynamics

### Finite polymatrix games

We consider finite (normal form) games over mixed-strategies. There are \(n\) players. The strategy space of player \(i\{1,2,...,n\}\) is the probability simplex, \((k_{i})\), where \(k_{i}\) is a positive integer and \(()\) is defined as \(()=\{s^{}\ \ s_{j} 0,j=1,..., ,\ \&\ _{j=1}^{}s_{j}=1\}.\) The utility function of player \(i\) is a function \(u_{i}:(k_{1})...(k_{n})\). We sometimes will write \(u_{i}(x_{1},...,x_{n})=u_{i}(x_{i},x_{-i})\), for \(x_{i}(k_{i})\) and \(x_{-i}_{-i}\), where \(_{-i}=(k_{1})...(k_{i-1})(k_{ i+1})...(k_{n})\).

For convenience, we will restrict our discussion to pairwise interactions, also called polymatrix games. That is, the utility function of player \(i\) is defined as

\[u_{i}(x_{1},...,x_{n})=x_{i}^{T}_{j=1 j i}^{n}M_{ij}x_{j},\] (1)

for matrices, \(M_{ij}\), \(j=1,,n\), \(j i\). We can write the utility function of player \(i\) as the inner product \(u_{i}(x_{1},...,x_{n})=x_{i}^{T}P_{i}(x_{-i})\) where \(P_{i}(x_{-i})=_{j=1 j i}^{n}M_{ij}x_{j}^{k_{i}}.\) Accordingly, each element of \(P_{i}(x_{-i})\) can be viewed as a payoff that is associated with a component of player \(i\)'s strategy vector, \(x_{i}\).

A Nash equilibrium (NE) is a tuple \((x_{1}^{*},...,x_{n}^{*})\) such that for all \(i=1,...,n\),

\[u_{i}(x_{i}^{*},x_{-i}^{*}) u_{i}(x_{i},x_{-i}^{*}), x_{i} (k_{i}).\]

A completely mixed-strategy NE is such that each \(x_{i}^{*}\) is in the interior of the simplex.

### Fixed order learning

Our model of learning is a dynamical system that relates trajectories of a payoff vector, \(p_{i}(t)\), to trajectories of the strategy, \(x_{i}(t)\). In particular, learning dynamics for player \(i\) are specified by a function \(f_{i}:(k_{i})^{k_{i}}^{k_{i}}\) according to

\[}(t)=f_{i}(x_{i}(t),p_{i}(t)),\]

where \(x_{i}(t)(k_{i})\) and \(p_{i}:_{+}^{k_{i}}\). We assume implicitly that \(f_{i}\) and \(p_{i}\) are such that there exists a unique solution whenever \(x_{i}(0)(k_{i})\). We further assume that the dynamics satisfy the invariance property that

\[x_{i}(0)(k_{i}) x_{i}(t)(k_{i}), t  0.\] (2)

Note that we define learning dynamics without specifying the source of the payoff vector, \(p_{i}(t)\), hence the terminology "payoff-based". Only once a player is coupled with other players in a game through their own (possibly heterogeneous) learning dynamics is when we make the connection \(p_{i}(t)=P_{i}(x_{-i}(t))\).

This formulation is illustrated in Figure 1, where the LD\({}_{i}\) denote payoff-based learning dynamics that are interconnected through the game matrices, \(M_{ij}\). Note that such learning dynamics are uncoupled by construction since each player can only access its own payoff vector. There is no dependence on the payoff stream of other players. Indeed, there is no dependence on the parameters of one's own utility function.

### Higher-order learning

The learning dynamics described in the previous section have a fixed order associated with the dimension of the strategy space. Higher-order learning dynamics allow for the introduction of auxiliary states as follows. For any fixed order learning dynamics, \(f_{i}\), we can define a higher-order version as

\[_{i}(t) =f_{i}(x_{i}(t),p_{i}(t)+_{i}(p_{i}(t),z_{i}(t)))\] (3a) \[_{i}(t) =g_{i}(p_{i}(t),z_{i}(t)).\] (3b)

As before, \(x_{i}(t)(k_{i})\) and \(p_{i}:_{+}^{k_{i}}\). The new variable \(z_{i}^{_{i}}\) represents \(_{i}\) dimensional auxiliary states that evolve according to the \(p_{i}\)-dependent dynamics, \(g_{i}\). These enter into the original fixed order dynamics through \(_{i}\). Accordingly, we can view \(p_{i}(t)+_{i}(p_{i}(t),z_{i}(t))\) as a modified payoff stream that captures path dependencies in \(p_{i}\), and the original learning dynamics react to this modified payoff stream.

As with the fixed order counterparts, there is no specification of game parameters in higher-order learning dynamics. In order to enforce that the auxiliary states have no effect on the equilibria of games, we make the following assumption.

Figure 1: Payoff-based learning dynamics (LD\({}_{i}\)) in feedback with game matrices (\(M_{ij}\)).

**Assumption 2.1**.: _If \(p_{i}^{*}\) and \(z_{i}^{*}\) are an equilibrium of the higher-order dynamics, i.e.,_

\[0=g_{i}(p_{i}^{*},z_{i}^{*})\]

_then_

\[_{i}(p_{i}^{*},z_{i}^{*})=0.\]

This assumption assures that the auxiliary states represent purely transient phenomena that disappear at equilibrium.

**Example 1** (**Anticipatory higher-order dynamics)**.: _A special case of higher-order dynamics is_

\[_{i} =(p_{i}-z_{i})\] \[_{i} =(p_{i}-z_{i}),\]

_where the the higher-order modification is the linear system (see Appendix A.1)_

\[(|.),\]

_and \(,_{+}\). Inuition behind the connection to anticipation can be seen by viewing \((p_{i}-z_{i})\) as an approximation of \(_{i}\), and so \(p_{i}(t)+(p_{i}(t)-z_{i}(t)) p_{i}(t+)\). Similar higher-order dynamics were used in reference (20) for smooth fictitious play to overcome the lack of convergence of uncoupled dynamics to NE in the anti-coordination game analyzed by (17). See also reference (32) for an analysis with replicator dynamics._

_Anticipatory higher-order dynamics can also be linked to optimistic optimization algorithms (e.g., (22)). An Euler discretization of step size, \(h\), results in_

\[z_{i}^{+}=z_{i}+h(p_{i}-z_{i}),\]

_with a modified payoff stream of_

\[p_{i}+_{i}=p_{i}+(p_{i}-z_{i}).\]

_Setting \(h=1/\) and \(=1/\) results in_

\[p_{i}+_{i} =p_{i}+(p_{i}-z_{i})\] \[=p_{i}+(p_{i}-p_{i}^{-}),\]

_where the superscripts '\(+\)' and '\(-\)' indicate the next and previous discrete time steps, respectively. There are also optimistic variants of discrete-time no-regret learning algorithms [33; 34; 35] that guarantee faster convergence rates to coarse correlated equilibria compared to the standard versions._

## 3 Gradient play

The main results of the paper will examine the behavior of gradient play and higher-order gradient play, which are the focus of this section.

### Fixed order gradient play

In gradient play dynamics, a player adjusts its strategy in the direction of the payoff stream, i.e.,

\[_{i}=_{}[x_{i}+p_{i}]-x_{i},\] (4)

where \(_{}[x]:^{n}(n)\) is the projection of \(x\) into the simplex, i.e., \(_{}(x)=_{s(n)}\|x-s\|.\)

The terminology "gradient play" stems from the gradient of an agent's utility function in (1) with respect to its own strategy, \(x_{i}\), namely \(_{x_{i}}u_{i}(x_{i},x_{-i})=P_{i}(x_{-i})=_{j i}^{n}M_{ij}x_{j}.\) As was done in the description of payoff-based learning, we replace \(P_{i}(x_{-i})\) with the payoff stream \(p_{i}\) without regard to the game matrices \(M_{ij}\).

Our primary concern will be studying these dynamics near a completely mixed-strategy NE. To this end, let \(x^{*}=(x_{1}^{*},,x_{n}^{*})\) be an isolated completely mixed-strategy NE. The strategy vectorevolves on the simplex, which is a subset of dimension \(k_{i}-1\). Hence, the local behavior of the dynamics around \(x_{i}^{*}\) is characterized by evolution on a lower-dimensional subset. Thus, we can write

\[x_{i}=x_{i}^{*}+N_{i}w_{i}\] (5)

where

\[^{}N_{i}=0\ \ \&\ N_{i}^{}N_{i}=I.\] (6)

Therefore, \(w_{i}^{(k_{i}-1)}\) represents deviations from \(x_{i}^{*}\) and satisfies \(w_{i}(t)=N_{i}^{}(x_{i}(t)-x_{i}^{*}).\) When all players utilize fixed order gradient play, the collective dynamics near a completely mixed-strategy NE take the form

\[=w,\] (7)

where

\[=^{}0&M_{12}&M_{13}&&M_{ 1n}\\ M_{21}&0&M_{23}&&M_{2n}\\ M_{31}&M_{32}&0&&M_{3n}\\ &&&&\\ M_{n1}&M_{n2}&M_{n3}&&0,= N_{1}&&\\ &&\\ &&N_{n}.\] (8)

Given the zero trace of \(\), standard gradient play is always unstable at a completely mixed-strategy NE (see Appendix A.3 for stability conditions). Also, for this equilibrium to be isolated, \(\) must be non-singular. Otherwise, \(\) has a non-trivial null space leading to an equilibrium subspace.

### Higher-order gradient play

We will be interested in a specific form of higher-order gradient play that uses the following linear structure of higher-order dynamics:

\[_{i} =-x_{i}+_{}[x_{i}+p_{i}+N_{i}(G_{i}_{i}+H_{i}(N_{ i}^{}p_{i}-v_{i}))]\] \[_{i} =E_{i}_{i}+F_{i}(N_{i}^{}p_{i}-v_{i})\] \[_{i} =N_{i}^{}p_{i}-v_{i},\]

for some matrices \(E_{i}\), \(F_{i}\), \(G_{i}\) and \(H_{i}\). Here, the auxiliary states are \(z_{i}=(v_{i},_{i})\), which enter into the dynamics through \(_{i}(p_{i},_{i},v_{i})=N_{i}(G_{i}_{i}+H_{i}(N_{i}^{}p_{i}- v_{i})),\) where \(N_{i}\) is defined as in (6).

The motivation behind this structure, illustrated in Figure 2, assures the enforcement of Assumption 2.1. The payoff stream is first preprocessed by a specific linear system to produce \(y_{i}\) and then by a general linear system \(K_{i}\) to produce \(u_{i}\). The matrices \((E_{i},F_{i},G_{i},H_{i})\) create the dynamical system

\[K_{i}(E_{i}&F_{i}\\  G_{i}&H_{i})\] (9)

that maps \(y_{i}=N_{i}^{}p_{i}-v_{i}\) to \(u_{i}=G_{i}_{i}+H_{i}y_{i}\) via \(_{i}=E_{i}_{i}+F_{i}y_{i}\). The preprocessing system has the property (see "washout filters" in Appendix A.2) that if \(p_{i}(t)\) converges to a constant, then \(y_{i}(t)\) converges to zero. Accordingly, when \(E_{i}\) is non-singular, the preprocessing system guarantees Nash stationarity, i.e., if

\[_{t}(x_{i}(t),_{i}(t),v_{i}(t))=(x_{i}^{*},_{i}^{*},v_{i}^ {*}) i\]

then \(x^{*}=(x_{1}^{*},,x_{n}^{*})\) is a NE. To conclude, the linear system \(K_{i}\) is the central entity that performs the payoff modification, and the preprocessing system ensures compliance with Assumption 2.1.

Figure 2: Cascade representation of linear higher-order dynamics for gradient play.

### Local stability analysis

As before, we can analyze the behavior near a completely mixed-strategy NE \(x^{*}\) through the variable \(w_{i}\) defined as in (5). Using the fact that \(N_{i}^{}_{j i}M_{ij}x_{j}^{*}=0\), we can write the local dynamics of a player as

\[_{i} =N_{i}^{}_{j i}M_{ij}N_{j}w_{j}+G_{i}_{i}+H_{ i}N_{i}^{}_{j i}M_{ij}N_{j}w_{j}-v_{i}\] \[_{i} =E_{i}_{i}+F_{i}N_{i}^{}_{j i}M_{ij}N _{j}w_{j}-v_{i}\] \[_{i} =N_{i}^{}_{j i}M_{ij}N_{j}w_{j}-v_{i}.\]

The collective dynamics near a mixed-strategy NE can be written as

\[\\ \\ =(I+H)&G&-H\\ F&E&-F\\ &0&-Iw\\ \\ v,\] (10)

where \(E\),\(F\),\(G\), and \(H\) are block diagonal matrcies with appropriate dimensions and \(\) is defined in (8). Local stability of a completely mixed NE is determined by whether the above collective dynamics are stable, i.e., the dynamics matrix in (10) is a stability matrix.

## 4 Uncoupled dynamics that lead to mixed-strategy NE

### Decentralized control formulation

The stability of a mixed-strategy equilibrium is tied to the existence of \(K_{1}\), \(K_{2}\),..., \(K_{n}\) so that the linear system in (10) is stable. When the \(K_{i}\) have yet to be determined, we can rewrite (10) as

\[\\  =&0\\ &-Iw\\ v+I\\ 0u,\] (11a) \[y =(&-I)w\\ v,\] (11b)

where \(u=u_{1}\\ \\ u_{n},y=y_{1}\\ \\ y_{n},\) and the \(y_{i}\) and \(u_{i}\) are to be related through \(K_{i}\).

### Decentralized stabilization

Let

\[(&\\ &0)\]

with

\[=&0\\ &-I,=I\\ 0,=(&-I).\] (12)

We first establish that \(\) can be stabilized by verifying the conditions for stabilizability and detectability (see Appendix A.3). The assumption that \(\) is non-singular stems from our interest in isolated NE.

**Proposition 4.1**.: _For \(\) non-singular, the pair \((,)\) is stabilizable, and the pair \((,)\) is detectable._

While Proposition 4.1 establishes that \(\) can be stabilized, that property alone is inadequate for our purposes. In particular, for the learning dynamics to be uncoupled, we seek to establish decentralized stabilization (see Appendix A.4) according to the partition

\[\\ =w\\ v+_{i=1}^{n}_{i}u_{i}, y_{i}=_{i} w\\ v,\] (13)where

\[=&0\\ &-I,_{i}=_{i} \\ 0,_{i}=_{i}&- _{i}^{}.\] (14)

Here, \(_{i}\) denotes the \(i^{}\) block row of \(\), i.e.,

\[_{i}=N_{i}^{}M_{i1}N_{1}&...&N_{i} ^{}M_{i(i-1)}N_{i-1}&0&N_{i}^{}M_{i(i+1)}N_{i+1}&...&N_{i} ^{}M_{in}N_{n}\]

and

\[_{i}^{}=0&...&0&_{i^{}}&0&...&0,\]

where \(I\) has a dimension (suppressed in the notation) of \(k_{i}-1\).

**Theorem 4.1**.: _For any isolated (i.e., \(\) is non-singular) completely mixed-strategy NE, there exist uncoupled higher-order gradient play dynamics such that (10) is stable._

The proof of Theorem 4.1 relies on the conditions of Theorem A.1 and is presented in Appendix B.1.

Theorem 4.1 should be viewed as a statement regarding whether uncoupled learning in itself is a barrier to learning dynamics leading to NE. The theorem makes no claim that the higher-order learning dynamics are interpretable (e.g., as in anticipatory learning). Nor does the theorem offer guidance on how agents may construct the matrices of higher-order learning that lead to convergence. In the next section, we will see that, while the structure is universal, any specific set of parameters is not universal in that one can construct a game for which they do not lead to NE. Despite the lack of universality, there is an inherent robustness that is a consequence of stability. The following follows from standard arguments on linear systems.

**Proposition 4.2**.: _Let the \(K_{i}\) and \(M_{ij}\), \(i=1,...,n\) and \(j=1,...,n\), be such that (10) is stable. Then there exists a \(>0\) such that (10) is stable with the \(M_{ij}\) replaced by any \(_{ij}\) as long as \(\|_{ij}-M_{ij}\|<\) for all \(i=1,...,n\) and \(j=1,...,n\)._

In words, this proposition guarantees that learning dynamics that lead to NE for a specific game continue to do so for nearby games.

### Stabilization through a single higher-order player

The previous section's analysis allowed all players to utilize higher-order learning. In some cases, it may not be necessary for all players to utilize higher-order learning. In this section, we present sufficient conditions under which a single player using higher-order gradient play with the remainder utilizing fixed order gradient play can still lead to NE.

**Assumption 4.1**.:
1. _Let_ \((w,)\) _be a left eigenvalue pair of_ \(\)_, i.e.,_ \[w^{}= w^{},\] _with_ \([] 0\) _and_ \(w^{}=w_{1}^{}&...&w_{n}^{}\) _partitioned consistently with (_8_). Then_ \(w_{i} 0\) _for all_ \(i\)_._
2. _Let_ \((v,)\) _be a right eigenvalue pair of_ \(\)_, i.e.,_ \[v= v,\] _with_ \([] 0\) _and_ \(v=v_{1}^{}&...&v_{n}^{}^{ }\) _partitioned consistently with (_8_). Then_ \(v_{i} 0\) _for all_ \(i\)_._

Recall the definitions of \(\), \(_{i}\), and \(_{i}\) from (14).

**Proposition 4.3**.: _Let \(\) be non-singular and satisfy Assumption 4.1. Then for any \(i\), the pair \((,_{i})\) is stabilizable and the pair \((,_{i})\) is detectable._

As a consequence of Proposition 4.3, it is possible for a completely mixed-strategy NE to be stabilized where a single player utilizes higher-order gradient play with the remaining players utilizing fixed order gradient play.

Non-convergence to NE in higher-order gradient play

We now show that linear higher-order gradient play dynamics need not lead to NE. Given any such dynamics, we construct a game with a unique NE that is unstable under given dynamics.

### The Jordan anti-coordination game

The Jordan anti-coordination game, introduced in (36), was used in (17) to prove that fixed order uncoupled learning dynamics do not lead to NE. The game consists of three players with

\[u_{1}(x_{1},x_{2})=x_{1}^{}0&1\\ 1&0x_{2}, u_{2}(x_{2},x_{3})=x_{2}^{} 0&1\\ 1&0x_{3}, u_{3}(x_{3},x_{1})=x_{3}^{} 0&1\\ 1&0x_{1},\]

and a unique mixed-strategy NE at \(x_{1}^{*}=x_{2}^{*}=x_{3}^{*}=1&1\\ 1&0^{}.\) We will let \(()\) denote the Jordan anti-coordination game but with the utility function of player 1 modified to \(u_{1}(x_{1},x_{2})=x_{1}^{}( M_{12})x_{2},\) where \(_{+}.\) Since scaling payoffs does not change the nature of the game, \(()\) has the same unique NE as \((1).\)

### Destabilization using rescaled anti-coordination

Suppose all three players use variants of linear higher-order gradient play in \(()\). As before, we denote the higher-order dynamics of player \(i\) as the linear dynamical system (9). To study the local behavior of the dynamics around the unique mixed-strategy NE of \(()\), we define \(w_{1}(t),\)\(w_{2}(t),\) and \(w_{3}(t)\) as in (5). Then, we analyze the local stability of the mixed-strategy NE through (10).

The following lemma will be essential in proving our main result.

**Lemma 5.1**.: _Let \(A^{n n}\), \(B^{n 1}\) and \(C^{1 n}\). If \(CB=CAB=0\), and \(CA^{m}B 0\) for some \(m 2\). Then for sufficiently large \(>0\), \(A- BC\) is not a stability matrix._

The proof of Lemma 5.1 is presented in Appendix B.2 and it uses root-locus arguments (see references (31; 30)).

**Proposition 5.1**.: _If linear higher-order gradient play dynamics are locally exponentially stable at the unique NE of \((1)\), then there exists \(>0\) such that the unique NE of \(()\) is unstable under such dynamics._

The proof of Proposition 5.1 is presented in Appendix B.3. The main idea of the proof is to write the local dynamics matrix in the form \(A- BC,\) and then use Lemma 5.1. We also provide a similar proof for sufficiently small \(\) in Appendix C.

The results might be puzzling because, for all \(>0\), all games \(()\) are strategically equivalent. Convergence guarantees for learning dynamics are usually established amongst classes of games. Thus, it is generally expected that dynamics will behave similarly for all games in a particular class. In this case, we design linear learning dynamics that are affected by simple rescaling of the payoff matrices.

## 6 Strong stabilization of mixed-strategy NE

Results from Section 4 imply that the mixed-strategy NE in a two-player \(2 2\) (identical-interest) coordination game can be stabilized. Here, we argue why dynamics that stabilize this mixed-strategy equilibrium are not reasonable. Specifically, we show that such dynamics _must be_ inherently unstable as an open system, i.e., as dynamics that respond to an exogenous payoff stream, and this instability is problematic with respect to such payoffs.

First, we inspect which type of mixed-strategy NE requires unstable learning dynamics for stabilization. For this purpose, consider the system in (12) for \(n=k_{1}=k_{2}=2\):

\[=0&m_{12}&0&0\\ m_{21}&0&0&0\\ 0&m_{12}&-1&0\\ m_{21}&0&0&-1=1&0\\ 0&1\\ 0&0\\ 0&0=0&m_{12}&-1&0\\ m_{21}&0&0&-1.\] (15)t must be that \(m_{12} 0\) and \(m_{21} 0\). The ability to stabilize a system via another stable system is referred to as _strong stabilization_ (see Appendix A.3). The next proposition gives a sufficient condition under which an isolated mixed-strategy NE is not strongly stabilizable.

**Proposition 6.1**.: _If \(m_{12}m_{21}>0\), then (15) is not strongly stabilizable._

The proof of Proposition 6.1 uses the "parity interlacing principle" (see reference (37)) and is presented in Appendix B.4.

The nature of the game can be inferred from the scalars \(m_{12}\) and \(m_{21}\). For example in zero-sum games we have \(M_{12}=-M_{21}^{}\), which gives

\[m_{12}=N^{}M_{12}N=N^{}M_{12}^{}N=-N^{ }M_{21}N=-m_{21}.\]

In coordination games, we have \(M_{12}=M_{21}\), which gives

\[m_{12}=N^{}M_{12}N=N^{}M_{21}N=m_{21}.\]

Therefore, the mixed-strategy NE in a coordination game is not strongly stabilizable.

Now let us examine the implications of inherently unstable learning dynamics. A reasonable expectation of learning dynamics is that in the case of a constant payoff vector, i.e., \(p_{i}(t) p^{*}\), we expect

\[_{t}x_{i}(t)=(p^{*}),\]

where \((p^{*})\) is a best response to \(p^{*}\), i.e.,

\[(p^{*})=*{arg\,max}_{x_{i}(k_{i})}x_{i}^{}p^{*}.\]

For any higher-order gradient play dynamics, if \(E_{i}\) is a stability matrix, then whenever \(p_{i}(t) p^{*}\) for some constant vector \(p^{*}\), one can show \(_{i}(t) 0\), which implies that \(x_{i}(t)\) is generated by standard gradient play dynamics in the limit. However, if \(p_{i}(t) p^{*}\) and the dynamics are inherently unstable, the term \(N_{i}G_{i}_{i}(t)\) need not vanish. Indeed, one can construct \(p^{*}\) such that \(x_{i}(t)\) does not converge to the best response of \(p^{*}\) (see the example in Section 7.2). The inability of learning dynamics to converge to the best response of a constant payoff vector does not reflect "natural" behavior.

## 7 Numerical experiments

### Jordan anti-coordination game: Stabilization through a single player

The payoff matrices of the Jordan anti-coordination game, introduced in Section 5.1, satisfy Assumption 4.1. Therefore, we can stabilize its mixed-strategy NE, allowing only one player to use higher-order learning while others continue to use standard gradient play. For this purpose, let \(_{1}\) and choose \(H_{1}=\), \(G_{1}=-\), \(F_{1}=\) and \(E_{1}=-\), where \(=50\) and \(=5\). Such dynamics resemble anticipatory gradient play but on the filtered low-dimensional payoff. Figure 3 illustrates convergence of the players' strategies to NE.

Figure 3: Single-player stabilization of the Jordan game.

### Stabilization of mixed-strategy NE in coordination games

Consider the (identical interest) coordination game:

\[u_{1}(x_{1},x_{2})=x_{1}^{}1&0\\ 0&1x_{2}, u_{2}(x_{2},x_{1})=x_{2}^{} 1&0\\ 0&1x_{1}.\]

The game has two pure strategy NE and one completely mixed-strategy NE at \(x^{*}=1&1\\ 0&1x_{1}\).

Consider the following set of parameters for higher-order gradient play: \(E_{1}=\), \(F_{1}=-2\), \(G_{1}=\), \(H_{1}=-\), \(E_{2}=-_{2}\), \(F_{2}=_{2}\), \(G_{2}=-_{2}_{2}\), and \(H_{2}=_{2}_{2}\). The numerical values are \(=0.5\), \(=20\), \(_{2}=50\), and \(_{2}=1\). Figure 3(a) illustrates convergence to the mixed-strategy NE of this coordination game. Suppose we break the feedback loop and use \(p^{*}=0&1^{}\) as the input to the first player dynamics. The response to such input is illustrated in Figure 3(b). We see that \(E_{1}>0\) is a scalar, and so \(_{1}\) grows without bound. The strategy \(x_{1}\), which is projected to the simplex, converges to \(1&0^{}\), which is not a best response to the input \(p^{*}\).

## 8 Concluding remarks

To recap, we studied the role of higher-order gradient play with linear higher-order dynamics. We showed that for any game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead to that NE, both for the original game and for nearby games. On the other hand, we showed that for any higher-order gradient play dynamics, the dynamics do not lead to NE for a suitably rescaled anti-coordination game. We also provided an argument against dynamics that lead to the mixed-strategy NE in a coordination game, showing they are not reasonable.

Regarding the higher-order gradient play dynamics that lead to NE, the interpretation of the results herein should not be that these dynamics are either a descriptive model of learning or a prescriptive recommendation for computation. Rather, the results are a contribution towards delineating what is possible or impossible in multi-agent learning. In that sense, they may be seen as a complement to the contributions in (17). Namely, dynamics being uncoupled is not a barrier to converging to mixed-strategy NE when allowing higher-order learning.

More generally, the present results open new questions related to the discussion in (14) on what constitutes "natural" learning dynamics. In the case of anticipatory higher-order learning, there is a clear interpretation of the effect of higher-order terms. However, it is unclear how to interpret general higher-order dynamics. Furthermore, the results herein regarding inherent instability of higher-order dynamics that converge to the mixed-strategy NE of a coordination game suggests that higher-order learning can be "unnatural". Possible restrictions on dynamics, in addition to being uncoupled, could include having no asymptotic regret; maintaining qualitative behavior in the face of strategically equivalent games (cf., Section 5.2); or having an interpretable relationship between payoff streams and strategic evolutions such as "passivity", which generalizes and extends the notion of contractive games to contractive learning dynamics (e.g., [38; 39; 40]).

In terms of limitations, the current paper only addresses the payoff vector setup and does not address the setup where players only have access to instantaneous scalar payoffs. Furthermore, results in Sections 5 and 6 are limited to certain setups and require further generalization.

Figure 4: Stabilizing the mixed-strategy equilibrium of a coordination game and its consequences.