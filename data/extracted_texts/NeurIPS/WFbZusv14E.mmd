# ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models

Siwei Wang\({}^{1}\) Yifei Shen\({}^{1}\) Shi Feng\({}^{2}\) Haoran Sun\({}^{3}\) Shang-Hua Teng\({}^{4}\) Wei Chen\({}^{1}\)

\({}^{1}\)Microsoft Research Asia ({siweiwang, yifeishen, weic}@microsoft.com)

\({}^{2}\)Harvard University (shifeng@fas.harvard.edu)

\({}^{3}\)Peking University (sunhaoran0301@stu.pku.edu.cn)

\({}^{4}\)University of Southern California (shanghua@usc.edu)

denotes equal contributions. Corresponding author: Wei Chen (weic@microsoft.com)Supported by a Simons Foundation Investigator Award.

###### Abstract

Planning is a crucial element of both human intelligence and contemporary large language models (LLMs). In this paper, we initiate a theoretical investigation into the emergence of planning capabilities in Transformer-based LLMs via their next-word prediction mechanisms. We model planning as a network path-finding task, where the objective is to generate a valid path from a specified source node to a designated target node. Our mathematical characterization shows that Transformer architectures can execute path-finding by embedding the adjacency and reachability matrices within their weights. Furthermore, our theoretical analysis of gradient-based learning dynamics reveals that LLMs can learn both the adjacency and a limited form of the reachability matrices. These theoretical insights are then validated through experiments, which demonstrate that Transformer architectures indeed learn the adjacency and an incomplete reachability matrices, consistent with our theoretical predictions. When applying our methodology to the real-world planning benchmark Blocksworld, our observations remain consistent. Additionally, our analyses uncover a fundamental limitation of current Transformer architectures in path-finding: _these architectures cannot identify reachability relationships through transitivity, which leads to failures in generating paths when concatenation is required_. These findings provide new insights into how the internal mechanisms of autoregressive learning facilitate intelligent planning and deepen our understanding of how future LLMs might achieve more advanced and general planning-and-reasoning capabilities across diverse applications.

## 1 Introduction

Large language models (LLMs) such as ChatGPT have impressed many with their powerful capabilities across a wide range of tasks, including language processing, knowledge extraction, reasoning, planning, coding, tool use, and more . However, we continue to be intrigued by the underlying mechanisms that fuel the power of LLMs. While all current LLMs are built on the Transformer architecture, which uses autoregressive learning to predict the next word in a language sequence, the overarching question remains:

_Why does the process of next-word prediction give rise to intelligence?_

There is no definite answer to this question yet, but researchers are approaching the problem from various angles, aiming to characterize the power and limitations of LLMs, as well as to capture their underlying acquisition, abstraction, generalization, adaptation, and reasoning mechanisms.

Recently, the mechanisms of grammar learning, knowledge manipulation, scaling laws, and arithmetic operations have been empirically uncovered [4; 3; 5; 2; 31; 11]. Furthermore, theoretical analyses have been conducted on in-context learning, chain-of-thought, and other forms of reasoning [30; 8; 7; 27]. Beyond these, LLMs' capability for planning--a fundamental component of human intelligence--has also drawn considerable attention. Planning is involved in nearly every aspect of our daily life, such as organizing a task at work, planning a trip, or seeking a mathematical proof of a theorem. Additionally, task planning plays a pivotal role in state-of-the-art LLM-empowered autonomous agents, such as HuggingGPT , Voyager , and Reflection . Understanding how LLMs complete a planning task can shed light on how the seemingly low-level statistical task of next-word prediction transforms into a high-level intelligent process. Several prior studies have empirically evaluated the planning capabilities of LLMs, yielding both positive and negative results [16; 22]. However, the current results are incomplete and do not fully explain why LLMs can or cannot successfully accomplish specific planning tasks (see Appendix A for a detailed discussion of related works).

Given that planning often involves making sequential selections of next steps to achieve a desired goal, it naturally relates to the path-finding task in networks. For example, autonomous agents (e.g., HuggingGPT ) for scheduling API calls can be likened to finding a call path in the API call graph; a mathematical proof can be seen as a proof path from the axioms to the final theorem ; and a step-by-step solution to a grade-school math problem can be viewed as a path in the dependency graph among the variables [28; 29]. Many previous studies on LLM planning capabilities are related to path-finding. e.g., an LLM planning benchmark called Blocksworld  can be viewed as path-finding from the initial state of the blocks to the final state in a state transition graph. Furthermore, in neuroscience, planning is often evaluated through path-finding in a maze . Consequently, in this paper, we abstract planning in LLM learning as the following path-finding task: given an underlying directed graph, a Transformer architecture is provided with training data consisting of a collection of paths that specify the source node, the target node, and a path from the source to the target. The task of the language model is then to generate a path for a new source-target pair. In addition to measuring the performance of the trained model, we examine the internal weighting mechanism and the learning dynamics of the Transformer architecture during the learning and problem-solving process. This research is part of our broader project, ALPINE (Autoregressive Learning for Planning In NEtworks), which aims to answer the overarching question on the connection between the process of next-word prediction and the emergence of intelligence through the lens of planning.

**Our Contributions**: Our project initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models by focusing on characterizing their expressiveness and learning dynamics in the path-finding task. First, in Theorem 2, we present a mathematical construction of a Transformer that encodes both the adjacency and reachability matrices of the network, thereby establishing that Transformer architectures possess the expressive capacity to complete the path-finding task. Then, in Theorem 3, we prove that when applying gradient descent to minimize the cross-entropy loss on the training data, a model based on a simplified Transformer architecture can extract the adjacency matrix and a limited form of the reachability matrix, using them to mimic human-like intelligence in path-finding. Our theoretical analysis further reveals a fundamental limitation of current Transformer architectures: they do not learn certain types of reachability, particularly transitive reachability, resulting in an incomplete ability to reason about future steps when planning. To validate our theoretical findings, we conduct extensive experiments training Transformers on the path language using autoregressive learning. First, these experiments demonstrate that Transformers achieve high accuracy in the path-finding task (Figure 3). Second, we show that it is indeed possible to extract both the adjacency and a limited form of the reachability matrices from the Transformers' weights (Figures 1,2,5,6(a)). Third, we observe a significant drop in test accuracy when the source and target nodes are connected only through concatenated path segments in the training data (Figure 6). These findings align with our theoretical analysis, confirming that _current Transformers have limitations in learning transitive reachability relationships, unlike human intelligence_. Finally, we validate these results on a real-world task planning benchmark, Blocksworld , which directly corresponds to the path-finding problem (see Appendix F).

## 2 Preliminaries

Throughout this paper, we use the following notations for matrices and vectors: \(\) and \(\) stand for a column vector and a matrix, respectively. Notations \(_{(i)}\) and \(_{(i,j)}\) denote the \(i^{th}\) entry of vector \(\) and the \((i,j)^{th}\) entry in matrix \(\), respectively. We also denote the \(i^{th}\) row of matrix \(\) by \(_{(i,:)}\).

### Autoregressive Transformer Architecture and Loss Function

In this paper, we adopt the standard GPT architecture . We use the following notation for the architecture and loss function in our analysis. Let \(N\) denote the sequence length, \(d\) the embedding size, \(H\) the number of heads, \(d_{k}=d/H\) the embedding size per head, and \(M\) the vocabulary size. One key component of the architecture is the attention mechanism, which is formulated as:

\[(,,)=(^ {}}{}})\] (1)

where \(^{N d_{k}}\), \(^{N d_{k}}\), \(^{N d_{k}}\) are the query, key, and value matrices, respectively. Denoting \(^{N d}\) as input, the multi-head attention is computed as \(()=_{i[H]}((_{i}^{Q },_{i}^{K},_{i}^{V}))\), where \(_{i}^{Q}^{d d_{k}}\), \(_{i}^{K}^{d d_{k}}\), \(_{i}^{V}^{d d_{k}}\) are the learnable weight matrices for the query, key, and value matrices of the \(i\)-th head.

The feed-forward layer is a two-layer multi-layer perceptron (MLP) defined as follows:

\[()=(,_{1}+_{N 1}_{1}^{ })_{2}+_{N 1}_{2}^{}\] (2)

where \(_{1}^{d 4d}\), \(_{2}^{4d d}\), \(_{1}^{4d}\), and \(_{2}^{d}\) are the learnable weight matrices and \(_{N x}\) is the all-one matrix with dimension \(N x\). Finally, one Transformer layer is defined as:

\[()=(_{2}((_{1}( ))+))+(_{1}())+\] (3)

where \(_{1}\) and \(_{2}\) are two layer normalizations.

With these essential components in place, we proceed to introduce the procedures of GPT. The training data consists of many sequences of tokens, where each sequence is expressed as \(=(u_{1},,u_{N})\), in which \(u_{n}\) denotes the token id for the \(n\)-th token in sequence \(\). We first represent the tokens in \(\) by a one-hot embedding matrix \(^{N M}\), where \(_{(n,u_{n})}=1\) and \(0\) elsewhere. Then there are learnable token embedding matrix \(_{t}^{M d}\) and positional embedding matrix \(_{p}^{N d}\), and the input \(_{0}=_{t}+_{p}^{N d}\). This input \(_{0}\) is fed into an \(L\)-layer Transformer, i.e., \(_{l}=(_{l-1})^{N d}\) for \(l=1,,L\).

Finally, the output embedding goes through another layer normalization \(_{t}\), and then it is multiplied by a learnable output weight matrix \(_{o}^{d M}\) to convert back to probability weights over all possible tokens. We calculate the output probability vector at position \(n\), denoted by \(}_{(n+1)}\), using \(}_{(n+1)}=((_{t}(_{L}))_{(n,)} _{o}),1 n<N\). This probability vector is used to predict the token for position \(n+1\), which reflects the autoregressive learning paradigm.

The adopted loss function is the _cross-entropy loss_ for the next token prediction, given by:

\[()=-_{n=1}^{N-1}_{k=1}^{M}_{(n+1,k)}}_{ (n+1),k}\] (4)

### Path-Planning Dataset: Syntax and Data Sources

The dataset is designed to evaluate GPT's path-planning capability on simple graphs. It is generated from a directed graph \(=(,)\), where \(\) is the set of nodes, and \(\) is the set of edges. For any two nodes \(u,v\), \((u,v)\) means that there is a directed edge from \(u\) to \(v\) in \(\). A pair of source node \(s\) and target node \(t\) is considered a _valid pair_ if \(\) contains at least one path from \(s\) to \(t\).

The training dataset \(\) contains sequences in the format "\(s\)\(t\)\(s\)\(a\)\(b\)\(c\)\(t\)\(\)n", where \(s\) represents the source node token, \(t\) the target node token, \(s\)\(a\)\(b\)\(c\)\(t\) are tokens for nodes in a valid path from \(s\) to \(t\), and \(\)n indicates the end of the sequence. During testing, we provide valid pairs of source and target nodes in the format "\(s\)\(t\)". The model is tasked with completing the remaining tokens in the sequence. The completion is considered correct if the model generates a valid path with the correct syntax.

## 3 Expressiveness and Learning Dynamics of Transformer Models
In our path-finding task, the essential step for completing a path is to predict the next node based on the current information. It is evident that to predict the subsequent node on the path, only information related to the current node and the target node is necessary. Algorithm 1 introduces a idealized method that utilizes both the adjacency and reachability matrices of the graph. The true adjacency matrix \(^{}\) and the true reachability matrix \(^{}\) are defined as:

\[^{}_{(i,k)}=1,&(i,k),\\ 0,&.\]

**Fact 1**.: _Assuming that \(t\) is reachable from \(s\), then Algorithm 1 is guaranteed to output a valid path with input \(=^{}\) and \(=^{}\)._

To illustrate the expressive capacities of the Transformer model, we first demonstrate how to manually construct a Transformer that can perform the path-finding task by simulating Algorithm 1.

**Theorem 2**.: _Given a graph \(\) (with adjacency matrix \(^{}\) and reachability matrix \(^{}\)), for every \(>0\), there exists a \(1\)-layer, \(1\)-head, and \(O(||)\)-embedding-size Transformer model that generates a valid path for every valid source-target pair \((s,t)\) with probability at least \(1-\)._

The proof involves encoding the adjacency and reachability matrices into the weights of the FFN and attention layers, respectively, while mimicking the computation of Algorithm 1 (see Appendix B).

```
1:Input: Adjacency matrix \(\), reachability matrix \(\), source node \(s\), target node \(t\)
2: Set path \(P=[s\ t\ s]\) and set current node \(i=s\)
3:while\(i t\)do
4: Obtain \(S=\{k|_{(i,k)}=1_{(t,k)}=1\}\)
5: Randomly sample next node \(k\) from \(S\)
6: Append \(k\) to path \(P\), and set \(i=k\)
7:endwhile
8:output path \(P\) ```

**Algorithm 1** A handcrafted path-finding algorithm

### Learning Dynamics

Having established the mathematical existence of a Transformer model capable of accomplishing path-finding in a given network, as demonstrated in Theorem 2, we now shift our focus to the following fundamental question: _Can the Transformer architecture, trained on sufficient path data with an autoregressive loss as in Equation (4) and using the gradient descent (GD) method, learn the adjacency and reachability matrices and carry out path-finding similar to the idealized Algorithm 1?_

Theoretically, we notice that the Transformer may not be able to learn the true adjacency and reachability matrices for the underlying graph. Instead, it can only learn the relevant information that is directly encoded in the observed training data \(\). Therefore, we define the _observed_ adjacency and reachability matrices based on the training data \(\) as follows.

\[^{}_{(i,k)}() =1,&,n [3,N-1]u_{n}=i,u_{n+1}=k\\ 0,&\] \[^{}_{(t,k)}() =1,&,n[4,N] u_{2}=t,u_{n}=k\\ 0,&.\]

Naturally, the observed adjacency matrix \(^{}()\) only records the edges \((i,k)\) that appear in some path within the training data \(\). On the other hand, the observed reachability matrix \(^{}()\) exhibits more nuanced distinctions from the true reachability matrix. It only records that node \(t\) is reachable from node \(k\), if the training data \(\) contains a path (sequence) whose target node is \(t\) and \(k\) appears as a non-source node on the path. We call such pairs \((t,k)\)_observed reachable pairs_. Noticeably, reachability through transitivity, i.e., through concatenation of path segments in \(\), is not observed.

Here we consider the following simplified \(1\)-layer and \(1\)-head Transformer structure: a) The attention weight is only on the target node (the second token), i.e., we manually set every row in \((^{-}}{}})\) in Eq. (1) to be a one-hot vector with the second coordinate being \(1\) (this is validated in our experiments shown in Figure 4), and set the positional embedding matrix \(_{p}=\); b) We remove all the layer normalizations, and use \(()=^{M}\) instead of Eq. (2), \(()=()+()\) instead of Eq. (3); c) The token embedding matrix \(_{t}\) and the output weight matrix \(_{o}\) are set to be identity. The embedding size is the same as the vocabulary size (\(d=M\)), and we only consider the cross-entropy loss of predicting the next node.

Since there is only one layer and one head, we use \(^{V}\) to represent the weight of the value matrix in the attention layer. Under the above Transformer structure,

\[(_{L})_{(n,:)}_{o}=(_{t}^{M}+_{t}^{V})_{(n,:)}_{o}=(^{M}+^{V}) _{(n,:)}=^{M}_{(u_{n},:)}+^{V}_{(u_{2},:)},\]

where \(\) is the manually set attention weight matrix (every row is a one-hot vector with the second coordinate being \(1\)). Therefore, the probability vector when predicting the \((n+1)\)-th token is \((^{M}_{(u_{n},:)}+^{V}_{(u_{2},:)})\), and the prediction probability \(}_{n+1,k}\) equals

\[}_{n+1,k}=^{M}_{(u_{n},k)}+^{V}_{(u_{2},k)} )}{_{}(^{M}_{(u_{n},)}+^{V}_{(u_{2},)})}.\] (5)

Let \(N_{i,j,k}\) be the number of times in \(\) that: a) the current node is \(i\); b) the target node is \(j\); c) the next node is \(k\), and let \(N_{i,j}=_{k}N_{i,j,k}\), then we have the following theorem.

**Theorem 3**.: _Under the cross-entropy loss \(()\), for all \((i,k)\) pairs, i) if \(_{j}N_{i,j}=0\), then \()}{^{(N)}_{(i,k)}}\) is always 0; ii) if \(_{j}N_{i,j}>0\) but \(_{j}N_{i,j,k}=0\), then \()}{^{M}_{(i,k)}}\) is always positive; iii) if \(_{j}N_{i,j,k}>0\), then \()}{^{M}_{(i,k)}}\) is negative when \(^{M}_{(i,k)}-\). Similarly, for all \((j,k)\) pairs, i) if \(_{i}N_{i,j}=0\), then \()}{^{V}_{(j,k)}}\) is always 0; ii) if \(_{i}N_{i,j}>0\) but \(_{i}N_{i,j,k}=0\), then \()}{^{V}_{(j,k)}}\) is always positive; iii) if \(_{i}N_{i,j,k}>0\), then \()}{^{V}_{(j,k)}}\) is negative when \(^{V}_{(j,k)}-\)._

Proof Sketch.: By the definition of the cross-entropy loss in Eq.(4), and the prediction probability vector in Eq.(5), the total cross-entropy loss of the model (with matrices \(^{M}\), \(^{V}\)) is

\[()=-_{i,j,k}N_{i,j,k}(^{M}_{(i,k)}+^{V}_{(j,k)} )+_{i,j}N_{i,j}(_{}(^{M}_{(i,)}+^{V}_ {(j,)})).\]

Then we can get that: (the proof for the \(^{V}\) part is similar)

\[)}{^{M}_{(i,k)}}=-_{j}N_{i,j, k}+_{j}N_{i,j}^{M}_{(i,k)}+^{V}_{(j,k)})}{_{ }(^{M}_{(i,)}+^{V}_{(j,)})}.\] (6)

In case i), \(_{j}N_{i,j}=0\) implies that \(_{j}N_{i,j,k}=0\). Hence \()}{^{M}_{(i,k)}}\) is always 0.

In case ii), \(_{j}N_{i,j}>0\) implies that the second term in Eq. (6) is positive, while \(_{j}N_{i,j,k}=0\) implies that the first term in Eq. (6) is 0. Hence \()}{^{M}_{(i,k)}}\) is always positive.

In case iii), when \(_{j}N_{i,j}>0\) and \(^{M}_{(i,k)}-\), the second term in Eq. (6) converges to 0, and it is smaller than \(_{j}N_{i,j,k}>0\). Hence, \()}{^{M}_{(i,k)}}\) is negative when \(^{M}_{(i,k)}-\). 

The above technical theorem directly leads to a theoretical explanation on how the model learns the adjacency and reachability information, as explained below.

Learning the adjacency matrix.Let \(()\) denote the set of edges appearing in the training dataset \(\), which corresponds to the observed adjacency matrix \(^{}()\). For any \((i,k)()\), \(_{j}N_{i,j,k}>0\), and for any \((i^{},k^{})()\), \(_{j}N_{i^{},j,k^{}}=0\). Then from the above theorem, under the gradient descent learning paradigm, \(^{M}_{(i^{},k^{})}\) will keep decreasing (since its gradient is always positive), while \(^{M}_{(i,k)}\) will not (since its gradient becomes negative when its value is sufficiently negative). This tends to make \(^{M}_{(i,k)}\) higher than \(^{M}_{(i^{},k^{})}\) after training. In this way, the Transformer model _learns the information about the observed adjacency matrix_ with matrix \(^{M}\).

To facilitate comprehension, we conducted a simple experiment on the simplified Transformer, and present the results in Figure 1, In this experiment, we generate a 10-node graph, and use 3 different training datasets \(_{1},_{2},_{3}\) based on this graph. \(_{1}\) contains all the paths with length 1; \(_{2}\) containsall the paths with length 1 and \(20\%\) of the paths with length higher than 1; and \(_{3}\) contains all the possible paths. Figure 1(a) is the true adjacency matrix of the graph, which is also the observed adjacency matrix for the three datasets. Figure 1(b), 1(c), 1(d) are the \(^{M}\) matrices with training datasets \(_{1}\), \(_{2}\), \(_{3}\), respectively. Upon observation, it becomes evident that these \(^{M}\) matrices all successfully capture the structural information from the adjacency matrix.

Learning the reachability matrix.Similar to the process of learning the adjacency matrix, since only _observed reachable pairs_\((j,k)\) have \(_{i}N_{i,j,k}>0\), the gradient descent learning paradigm tends to make the \(^{V}_{(j,k)}\) terms corresponding to observed reachable pairs \((j,k)\) higher than the \(^{V}_{(j^{},k^{})}\) terms corresponding to non-observed reachable pairs \((j^{},k^{})\) (which is either not reachable or not observed) after the training. In this way, the Transformer model _captures the structural information of observed reachability matrix_ with weight matrix \(^{V}\).

Figure 2 shows the correlation between \(^{V}\) and the observed reachabilities under different dataset \(\)'s in the above experiment. Figure 2(a) is the real reachability matrix of the graph; Figure 2(b), 2(c), 2(d) are the observed reachability matrices in datasets \(_{1}\), \(_{2}\), \(_{3}\), respectively; and Figure 2(e), 2(f), 2(g) are the \(^{V}\) matrices with training datasets \(_{1}\), \(_{2}\), \(_{3}\), respectively. These illustrations show that all the weight matrices \(^{V}\) can satisfactorily learn the information of the observed reachabilities present in the training datasets, but cannot deduce any non-observed reachabilities.

Predicting the next node on a path.From Eq.(5), the probability vector for predicting the next node is \((^{M}_{(u_{n},:)}+^{V}_{(u_{2},:)})\), where \(u_{n}\) represents the current node, and \(u_{2}\) represents the target node. This resembles the procedure in Algorithm 1: it predicts the next node \(k\) such that both \(^{M}_{(u_{n},k)}\) is high (corresponding to \(_{(u_{n},k)}=1\)) and \(^{V}_{(u_{2},k)}\) is high (corresponding to \(_{(u_{2},k)}=1\)).

Figure 1: Empirical verification regarding the learning of the adjacency matrix.

Figure 2: Empirical verification regarding the learning of the observed reachability matrix.

In summary, our theoretical analysis demonstrates that a simplified one-layer, one-head autoregressive Transformer (with perfect attention) can effectively learn crucial adjacency and reachability information from the training data through gradient descent training. Moreover, it can utilize this learned information to predict the next node akin to the decision-making process of a human algorithm designer in similar scenarios. This suggests that, when confronted with the path-finding or more general planning task with a given goal, the Transformer learns the structural information to associate the next step with both the current step and the goal, enabling it to generate the subsequent task step. Nevertheless, the Transformer's limitation in learning only the observed reachability matrix--without deducing the complete reachability matrix--hints at potential constraints on the goal-oriented information it can acquire. This limitation may result in the Transformer failing to grasp novel reachability relationships derived from the transitivity of reachability relations, unlike human intelligence.

## 4 Empirical Evaluations: Peeking into a Trained Transformer

In this section, we conduct extensive experiments on the path-finding task using the general Transformer architecture as described in Section 2.1. The datasets are generated as described below.

The DAG is generated randomly based on two parameters: the number of nodes \(n\), and the probability of edge \(p=0.1\): For any \(1 i<j n\), there is an edge \((i,j)\) with probability \(p\). Given the DAG, we first find all the possible reachable pairs \((s,t)\). Then these reachable pairs are separated into the training set (w.p. 0.5) and the test set (w.p. 0.5), but if edge \((s,t)\), we always put \((s,t)\) in the training set. For a reachable pair \((s,t)\) in the training set, we generate \(m=20\) random paths that start at \(s\) and end at \(t\), and put these \(m\) paths into the training dataset. When generating the random path, at each current node \(i\), we find all the possible \(k\) such that \(^{}_{(i,k)}=1\) and \(^{}_{(t,k)}=1\) (i.e., there is an edge \((i,k)\), and \(k\) could also reach the target node \(t\)), and uniformly choose a random one in them. Moreover, if \((s,t)\), we always put the one-edge path "\(s\)\(t\)\(s\)\(t\)\(\)n" in the training dataset to guarantee that all edges appear at least once in the training data.

### Accuracy on Test Dataset

We train Transformer models on the aforementioned training dataset and subsequently evaluate the performance of these models using the pairs in the test dataset. The correctness of a model's output is determined based on its validity in terms of syntax and whether it corresponds to a valid path from \(s\) to \(t\). In our experiments, we employ Transformer models with an embedding size of \(d=120\). We conduct tests using various configurations, ranging from 1-layer and 1-head to 6-layer and 6-head, while considering different graph sizes, with number of nodes \(n\) ranging from 100 to 500. The accuracy results take average over 10000 trials, and are presented in Figure 3 (due to space limits, some results are deferred to Appendix E, and all of them are consistent with our conclusions). From these results, we make the following observations: a) When comparing the figures, the accuracy tends to decrease as the number of nodes increases; b) When examining each row, the accuracy remains relatively stable even as the number of attention heads increases; c) When examining each column, the accuracy shows at most a slight improvement as the number of layers increases.

The above observations suggest that the embedding size is the most important hyperparameter that affects the accuracy of the model. On the one hand, when the embedding size is sufficiently large compared to the graph size, even 1-layer and 1-head models perform well. This coincides with our theoretical analysis, which shows that when the embedding size equals to the graph size, the 1-layer and 1-head structure is enough to predict the next nodes accurately. On the other hand, when the

Figure 3: Accuracy on the test datasets with embedding size \(d=120\).

embedding size is small compared to the graph size, even 6-layer and 6-head Transformers cannot achieve good performance. Because of this, in the following, we concentrate on the explainability of the 1-layer and 1-head Transformer models.

### Peeking into a Trained Transformer

Attention.In our analysis, we assume that the attention is fixed on the target node. Is this true for the Transformer models learned from real data? The corresponding results are shown in Figure 4. These results are obtained by looking into the attention mechanism of the 1-layer and 1-head Transformer models, and showing the average (taking on the test dataset) matrix of \((^{}}{}})\), of which the \(n\)-th row represents the attention vector for predicting the \((n+1)\)-th token.

Note that the second column in these figures represents the attention weights on the second token, which corresponds to the target node in our test data. We can see that, when predicting the next tokens, almost all the attention weights are concentrated on this column, especially for those models with higher accuracy (Figure 4(a) for \(n=100\) and Figure 4(b) for \(n=200\)). This demonstrates that indeed the Transformer model learns the correct attention for the path-finding task, and our assumption on the attention weights for the theoretical analysis is reasonable.

Adjacency Matrix.In the 1-layer and 1-head Transformers, let \(^{M^{}}\) (shown in Figure 5) be the matrix whose \(i\)-th row is \((_{i}^{}_{t})_{o}+(_{i}^{ }_{t})_{o}\), where \(_{i}\) is the one-hot column vector that represents the token for node \(i\). Based on the Transformer computation, intuitively this matrix is one of the components in the output that contains information related to the current node. The detailed reason for choosing this matrix is explained in Appendix D.

In Figure 5(a), the \(^{M^{}}\) matrix and the adjacency matrix are highly aligned: all large entries in the \(^{M^{}}\) matrix correspond to real edges, and all real edges correspond to large entries in the \(^{M^{}}\) matrix. This high accuracy is because the embedding size \(d=120\) is higher than the number of nodes \(n=100\). If the embedding size is lower than the graph size (Figures 5(b), 5(c)), we inevitably lose some accuracy when approximating the adjacency matrix by the product of matrices with rank smaller than the graph size. Even so, there is still high relevance between \(^{M^{}}\) and the adjacency matrix: almost all real edges correspond to large entries in the \(^{M^{}}\) matrix.

In Figure 5(d), we show the gap between the average weight corresponding to edges (i.e., the average of \(^{M^{}}_{(i,j)}\)'s with \(i<j\) and \((i,j)\)) and the average weight corresponding to non-edges (i.e.,

Figure 4: The average attention in the 1-layer and 1-head Transformers.

Figure 5: The first 20 rows and columns of \(^{M^{}}\) (the red boxes correspond to 1’s in the adjacency matrix), and the average weight gap between edge terms and non-edge terms in \(^{M^{}}\).

the average of \(_{(i,j)}^{M^{}}\)'s with \(i<j\) and \((i,j)\)) during the training process. These gaps keep increasing until convergence, suggesting that weights between edges and non-edges are more easily separated as the learning process proceeds.

Reachability Matrix.In the 1-layer and 1-head Transformers, let \(^{V^{}}\) be the matrix whose \(i\)-th row is \((_{i}^{}_{t})^{V}_{o}+((_{i}^ {}_{t})^{V})_{o}\), where \(_{i}\) is the one-hot column vector that represents the token for node \(i\). Intuitively, this matrix is the remaining component in the output that contains information related to the target node. The detailed reason is also explained in Appendix D.

In Figure 6(a), we show the average weights of three different sets in the graphs: "obs" corresponds to the \(_{(t,k)}^{V^{}}\)'s with \(t k\) and \(_{(t,k)}^{}=1\); "real\(\)obs" corresponds to the \(_{(t,k)}^{V^{}}\)'s with \(t k\), \(_{(t,k)}^{}=0\) but \(_{(t,k)}^{}=1\); and "non" corresponds to the \(_{(t,k)}^{V^{}}\)'s with \(t k\) and \(_{(t,k)}^{}=0\). Here we only show the results of graphs with 100 nodes and 200 nodes, since their accuracy is high enough and their attention is quite close to being concentrated on the target node. When there are more nodes, the ability to approximate the reachability matrix is not enough for us to distinguish it. From these average weights, we can see that the Transformer learns \(^{}\) quite well, as for those terms in "real\(\)obs", their weights are almost the same as those in "non". This echoes our analysis.

To further demonstrate that \(^{}\) is not learned as good as \(^{}\), we divide the source-target node pairs \((s,t)\) in the test dataset into four categories: a) degree 0: \(_{(t,s)}^{}=1\); b) degree 1: \((s,t)\) is not of degree 0, while \(s\) has at least one out-neighbor node \(u\) such that \((u,t)\) is of degree 0, i.e. \(_{(t,u)}^{}=1\); c) degree 2: \((s,t)\) is not of degree 0 and 1, while \(s\) has at least one out-neighbor node \(u\) such that \((u,t)\) is of degree 1; d) degree 3 or more: the remaining \((s,t)\) pairs in the test dataset. Roughly speaking, in our analysis, for \((s,t)\) pairs of degree 0 or 1, we know that there is a node \(u\) such that \(_{(s,u)}^{}=1\) and \(_{(t,u)}^{}=1\). Then node \(u\) will have a large weight, indicating a high accuracy. As for \((s,t)\) pairs of degree 2 or more, there is no node \(u\) such that both \(_{(s,u)}^{}=1\) and \(_{(t,u)}^{}=1\). In this case, the high-weight entry when predicting the next node of \(s\) is either an adjacent node of \(s\) or a recorded node that can reach \(t\). This should reduce the accuracy.

To see this, we check the accuracy of the Transformers on the \((s,t)\) pairs of the four different categories. The results are shown in Figure 6 (b)-(d). In these figures, each row of the accuracy matrix is further divided into four sub-rows corresponding to the accuracy of degree-0 pairs, degree-1 pairs, degree-2 pairs, and degree-3 or more pairs respectively (in the graph with 100 nodes, there are no test \((s,t)\) pairs in the degree-3 or more category). From these results, we can see that the accuracy for degree-2 pairs and degree-3 or more pairs is much lower than the two other categories in most cases. It indicates that, even with more parameters and a more complex structure (e.g. a 6-layer and 6-head Transformer), the Transformer model has a fundamental difficulty in generating paths for high-degree source-target pairs, namely those pairs that can only be connected by concatenating several path segments in the training dataset. This result demonstrates the validity of our theoretical analysis, i.e., after training with gradient descent on cross-entropy loss, the Transformer can only learn observed reachability, and will miss those unobserved reachability deduced from the transitivity of the reachability relation.

In summary, our extensive empirical evaluation leads to the following conclusions about the Transformer model in achieving the path-finding task: (a) With large enough embedding size, the model can achieve high accuracy in general; (b) The model achieves its performance by concentrating attention on the target nodes as intended, and learning the information on adjacency and reachability matrices, just as what a human would do and as predicted by our theoretical analysis; and (c) The model may have limitations and fail to learn high-order reachability relations through transitivity, and thus fail to generate paths derived from high-order reachability.

## 5 Discussion and Future Work

In summary, this paper and Project ALPINE more broadly conceptualize planning as path-finding in networks, and combine theoretical analysis of the Transformer architecture and autoregressive loss with empirical validation. Our aim is to uncover the mechanisms by which intelligence may emerge from autoregressive Transformer architectures. We analytically demonstrate that Transformers possess the expressiveness required to perform path-finding tasks and that gradient descent on cross-entropy loss enables them to learn necessary--but incomplete--graph information for path-finding.

Additionally, we reveal both analytically and empirically that autoregressive training of language models has inherent limitations in the path-finding task.

**Practical Implications**: Our findings in LLMs for path planning may have practical implications for the training, testing, and enhancement of language models. In particular, the limitations we identified in current Transformer architectures for transitive reasoning suggest several directions for enhancing LLM frameworks to achieve more advanced and general planning-and-reasoning capabilities across diverse applications. For instance, in data generation for training, creating more diversified datasets that explicitly cover more reachability relationships may help the model achieve a higher accuracy. When evaluating a language model's planning capability, it may be beneficial to test for higher-order relationships not directly encoded in the training data but requiring chaining and concatenation to assess whether the model can perform transitive planning. Furthermore, by highlighting limitations in current language models, our study motivates future research into improved Transformer architectures, including incorporating transitivity directly into the model structure.

**Challenges in Reasoning about Unobserved Reachability**: Technically, the challenge in learning unobserved reachability with current Transformer architectures stems from the nature of next-token prediction loss: learning unobserved reachability incurs a higher training loss. Specifically, when predicting the next token for a given current node \(i\) and target node \(j\), the optimal distribution for minimizing training loss should align with the observed distribution in the training dataset, i.e., \([\,=\,k|\,=\,i\,\,=\,j]=}{N_{i,j}}\) (as explained in Section 3.2). Learning unobserved reachabilities requires deviating from the distribution defined by the training data, which leads to a higher training loss. Consequently, with the current training loss and Transformer architecture, the model cannot learn unobserved reachabilities, such as transitive reachability. To enable the model to learn transitivity, we may need alternative training objectives, such as path accuracy, or structural improvements to the Transformer that allow it to 'deduce' unobserved reachabilities. Conceptually, the current training data and loss objective do not provide sufficient information to teach the model transitivity or other derived relationships. Therefore, enhancing transitivity and similar capabilities may require enriching the training data, modifying the objective function, or incorporating new components into the model architecture.

**Future Directions**: Our investigation opens several promising directions for future research: (a) Extending our study to hyper-graphs and hyper-paths, where a hyper-edge represents scenarios requiring multiple preconditions to be met simultaneously in order to carry out the next step, as often seen in task planning and mathematical proofs. (b) Addressing the limitations of Transformers in path-finding and other planning tasks by exploring richer path-finding languages, fine-tuning, or architectural improvements to LLMs. (c) Examining connections between the abstract path-finding task and concrete planning tasks (e.g., block manipulation in Blocksworld) to understand whether, and how, Transformers abstract these tasks into path-finding frameworks. (d) Investigating in-context path-finding capabilities, where training data includes different graphs with corresponding paths, to see how Transformers learn to find new paths in new graphs. (e) Exploring the integration of chain-of-thought and backtracking capabilities into Transformers for path-finding, which may offer crucial insights into enabling these features for general search and planning tasks.

In our ongoing project ALPINE, we plan to deepen our investigation into all the aforementioned fronts. We also hope that our work will inspire more researchers to study LLMs through combined theoretical and empirical analysis, with the ultimate goal of enhancing their capabilities and understanding how human-like intelligence can be achieved through statistical learning and AI mechanisms.

Figure 6: The average weights in \(^{V^{}}\), and the accuracy for \((s,t)\)’s with different degrees.