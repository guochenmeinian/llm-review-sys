# Parallel Submodular Function Minimization

Deeparnab Chakrabarty

Dartmouth College

Hanover, USA

deeparnab@dartmouth.edu

&Andrei Graur

Stanford University

Stanford, USA

agraur@stanford.edu

&Haotian Jiang

Microsoft Research

Redmond, USA

jhtdavid96@gmail.com

Supported in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and Stanford Management Science & Engineering Department Nakagawa Fellowship

Supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship.

Aaron Sidford

Stanford University

Stanford, USA

sidford@stanford.edu

Supported in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and Stanford Management Science & Engineering Department Nakagawa Fellowship

Supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship.

###### Abstract

We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth tradeoffs a submodular function defined on subsets of \(n\) elements that has integer values between \(-M\) and \(M\). The first method has depth \(2\) and query complexity \(n^{O(M)}\) and the second method has depth \((n^{1/3}M^{2/3})\) and query complexity \(O((n,M))\). Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex \(_{2}\)-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing \(_{}\)-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.

## 1 Introduction

A function \(f:2^{[n]}\) is _submodular_ if it has the diminishing marginal return property that \(f(S\{i\})-f(S) f(T\{i\})-f(T)\) for all elements \(i[n]\) and subsets \(S T[n]\{i\}\), i.e., the function value increase of adding \(i\) to a set \(S\) is at least that of adding \(i\) to a superset \(T\) that does not contain \(i\). Submodular functions are used to model a range of problems arising machine learning , operations research , and economics . The _submodular function minimization (SFM) problem_ consists of finding the subset \(S\) with the smallest \(f(S)\), given evaluation oracle access to the submodular function \(f\), using as few queries as possible. SFM has numerous applications. For example, in natural language processing SFM has played a key role in speech analysis where  modeled the task of optimally selecting terse, yet diverse, training data out of a large speech dataset as a SFM problem; in computer vision, the task of energy minimization was reduced to SFM .

Seminal work of  established that SFM can be solved with a polynomial number of queries and in polynomial time. In the decades since, there has been extensive research characterizing the query and computational complexity of SFM. When \(f\) is integer valued with values between\(-M\) and \(M\), the state-of-the-art query complexities are \((nM^{2})^{4}\) due to  and , \((n^{2} M)\) and \((n^{3})\), both due to  (the latter query complexity being improved by a factor of \(^{2}n\) in ), and \(O(n^{2} n)\) (with an exponential runtime) due to . The largest query complexity lower bound is \((n n)\) for deterministic algorithms due to .

The algorithms underlying the above results are highly _sequential_. State-of-the-art SFM algorithms use at least a linear number of rounds of queries to the evaluation oracle (\((nM^{2})\) rounds in  and \(O(n n)\) rounds in ). Although there is a trivial \(1\)-round algorithm that queries all \(2^{n}\) input of \(f\) in parallel, all known polynomial time SFM algorithms use \((n)\) rounds.

With the prevalence of parallel computation in practice and applications of SFM to problems involving massive data sets, recently there has been an increasing number of works which study the _parallel_ complexity of SFM, i.e., the smallest number of rounds a SFM algorithm must take. Starting with , significant progress has been made in proving _lower bounds_ on the parallel complexity of SFM. It is now known that any SFM algorithm making polynomially many queries needs \((n^{1/3})\) rounds when \(M=(n)\) and \((n)\) rounds when \(M=(n^{n})\).

In contrast to the significant progress on lower bounds, less progress has been made on designing SFM algorithms with small parallel complexity. This is due in part to the established lower bounds; as noted above, if we desire _range-independent_ parallel complexity bounds, i.e., independent of the range \(M\), then it is impossible to obtain an \(O(n^{1-})\)-round SFM algorithm, for any constant \(>0\). The central question motivating this paper is what non-trivial parallel speedups for SFM are possible if we allow methods with a _range-dependent_ parallel complexity.

_Is it possible to obtain query-efficient \(o(n)\)-round \(M\)-dependent algorithms for SFM?_

In this paper we provide positive answers to the above question. Our first result is an algorithm that runs in \((n^{1/3}M^{2/3})\) rounds with query complexity \((n^{2}M^{2})\). To achieve this result, we first provide a generic reduction from parallel SFM to parallel convex optimization (a well studied problem discussed below). While naively this approach yields a sublinear bound of \((n^{2/3}M^{2/3})\) rounds, we show how to further improve these convex optimization methods in our setting.

Our second result is a simple \(2\)-round SFM algorithm (Algorithm 1) with query complexity \(n^{O(M)}\). For constant \(M\), the parallel complexity of \(2\) is optimal among the class of query-efficient algorithms that query the minimizer . It is instructive to contrast our second result to the lower bound in , where it is proved that when \(M=n\), any algorithm with query complexity \(n^{M^{1-}}\) for any constant \(>0\) must proceed in \(n^{()}\) rounds.

**Highly Parallel Convex Optimization.** Motivated by applications to distributed and large scale optimization , the question of highly parallel convex optimization has received significant attention in the last decade. Formally, the task is to find an (approximate) minimizer of a convex function this is Lipschitz in some norm, using as few rounds of \(O((n))\) parallel queries to a subgradient oracle as possible. Over the past few years, there has been progress on both upper  and lower bounds  for this problem.

This line of work is particularly relevant as SFM reduces to minimizing the _Lovasz extension_ (see Fact 2.4)  over \(^{n}\), which is convex and \(O(M)\)-Lipschitz in \(_{}\) (see Definition 2.1). In Section 2.1 we provide a straightforward reduction from this problem to unconstrained minimization of a \(O(L)\)-Lipschitz function in \(_{}\) where the minimizer has \(_{}\) norm \(O(1)\). Consequently, improved parallel \(_{}\)-Lipschitz convex optimization algorithms can imply improved parallel SFM algorithms.

Many prior algorithms for highly parallel convex optimization focused on convex functions that are \(_{2}\)-Lipschitz and some were written only for unconstrained optimization problems. Naively using these algorithms for \(_{}\) yields parallel complexities of \((n^{3/4}/)\) and \((n^{2/3}/^{2/3})\) for convex functions that are \(_{}\)-Lipschitz.

Our first SFM result follows from an improved algorithm that we develop that finds \(\)-approximate minimizers for convex \(_{}\)-Lipschitz functions in \((n^{1/3}/^{2/3})\) rounds. Interestingly, for constant \(>0\), the dependence on \(n\) in this improved parallel complexity is optimal up to logarithmic factors;  proved a lower bound of \((n^{1/3}(1/))\) on the round complexity of any algorithm obtaining an \(\)-approximate minimizer over \(^{n}\) for \(_{}\)-Lipschitz convex functions. For constant \(>0\), this lower bound also applies to unconstrained \(_{}\)-optimization .

**Notation.** We let \([n]:=\{1,,n\}\) for any \(n_{>0}\). We let \(_{n}\) denote the identity matrix in \(^{n n}\).

### Problems, Results, and Approach

Here we formally define the problems we consider, present our results, and discuss some of the key insights of our approach. This section is organized as follows. We begin by presenting the parallel computation model. Then, we introduce the parallel \(_{p}\)-Lipschitz convex optimization problem, which is closely tied to parallel SFM. We present our improvement for the \(_{}\)-Lipschitz setting and offer a brief overview of our techniques in obtaining this result. Finally, we formally introduce the SFM setup, along with the new results we obtain (Theorem 1.3 and Theorem 1.4).

**Parallel Complexity Model.** We consider the standard black-box query model for optimization, where we are given a convex function \(f:\), with domain \(^{n}\), accessed through an oracle. Parallel algorithms for minimizing \(f\) proceed in rounds, where in each round the algorithm can submit a set of queries to the oracle in parallel. The _parallel complexity_ of an algorithm is the total number of rounds it uses and it captures how "sequential" the algorithm is. Additionally, we consider the _query complexity_ of an algorithm which is the total number of queries it makes.

**Parallel \(_{p}\)-Lipschitz Convex Optimization.** In Section 2 we consider the problem of minimizing a convex function \(f:^{n}\) given access to a _subgradient oracle_\(g:^{n}^{n}\), which when queried with point \(x^{n}\) returns a vector \(g(x)\) that is a subgradient, denote \(g(x)(f(x))\) where \((f(x))\) is the set of all subgradients of \(f\) at \(x\), i.e., \(v f(x)\) if and only if \(f(y) f(x)+v^{}(y-x)\) for all \(y^{n}\). Furthermore, we assume that \(f\) is \(L\)-Lipschitz with respect to a norm \(\).

**Definition 1.1** (\(L\)-Lipschitzness with respect to a given norm).: _We say that \(f:^{n}\) is \(L\)-Lipschitz with respect to \(\) for norm \(\) if \( f(x)-f(y) L x-y\) for all \(x,y^{n}\). If \(f\) is \(O(1)\)-Lipschitz with respect to a norm \(\), we say that \(f\) is \(\)-Lipschitz. When the norm is \(_{p}\) we alternatively say that \(f\) is \(L\)-Lipschitz in \(_{p}\) and that \(f\) is \(_{p}\)-Lipschitz respectively._

There is a broad line of work on studying the parallel complexity of \(_{p}\)_-Lipschitz convex optimization_ in which the goal is to efficiently compute an \(\)-approximate minimizer (i.e., a point \(y\) with \(f(y)_{x}f(x)\)) of a \(_{p}\)-Lipschitz convex function where the minimizer either has \(_{p}\)-norm \(O(1)\) or the problem is constrained to the \(_{p}\)-norm ball of of radius \(O(1)\). The case when \(p=2\) is perhaps the most well studied and our new result regarding \(_{}\)-Lipschitz convex optimization builds upon a result in , Theorem 1, which considers this setting. The statement of our result is below:

**Theorem 1.2** (Parallel Convex Optimization in \(_{}\)).: _There is an algorithm that when given a subgradient oracle for convex \(f:^{n}\) that is \(1\)-Lipschitz in \(_{}\) and has a minimizer \(x_{}\) with \( x_{}_{} 1\) computes an \(\)-approximate minimizer of \(f\) in \((n^{1/3}^{-2/3})\) rounds and \((n^{-2})\) queries._

In fact, we obtain a more general result that solves stochastic variants of parallel \(_{}\)-optimization (see Theorem 2.2) and Theorem 1.2 is an important corollary of this more general result.

Our parallel convex optimization algorithms build on machinery developed for highly-parallel algorithms for minimizing convex functions that are \(_{2}\)-Lipschitz. These methods consider a convolution of \(f\) with a centered Gaussian with covariance \(^{2}_{n}\) (also referred to as _Gaussian smoothing_), and then apply optimization methods  to this smooth function. By leveraging

 
**Paper** & **Year** & **Parallel Rounds** \\ 
 & 2020 & \((nM^{2})\) \\ 
 & 2020 & \((n M)\) \\ 
 & 2021 & \((n)\) \\ 
**This paper** & 2023 & \((n^{1/3}M^{2/3})\) \\   
 
**Paper** & **Year** & **Parallel Rounds** \\  Subgradient Descent & 1960s & \((n/^{2})\) \\  Cutting Plane & 1965 & \((n(1/))\) \\ 
 & 2019 & \((n^{2/3}/^{2/3})\) \\ 
**This paper** & 2023 & \((n^{1/3}/^{2/3})\) \\  

Table 1: State-of-the-art parallel complexity for SFM and \(_{}\)-optimization. See Section 1.2 for references on cutting plane methods. *The \((n^{2/3}/^{2/3})\) result uses Lemma 2.3 from this paper.

the properties of this smoothing and the convergence rate of the associated optimization methods, they obtain their parallel complexities. Since functions which are \(_{}\)-Lipschitz are also \(_{2}\)-Lipschitz, the above algorithms also apply to \(_{}\)-Lipschitz functions but they give us a suboptimal dependence of \(n^{2/3}\) on the dimension. We improve the dependence on dimension by utilizing the \(_{}\)-Lipschitzness of our function to add more Gaussian smoothing. This allows us to obtain a \(n^{1/3}\) dependence on the dimension, which is optimal up to polylogarithmic factors for constant \(\). The key observation is that convolving an \(_{}\)-Lipschitz function with a Gaussian of covariance \(^{2}_{n}\) changes the function value by no more than \(O()\) (see Lemma 2.6), whereas for \(_{2}\)-Lipschitz functions it could change the function value by \(O()\).

**Submodular Function Minimization.** In SFM, we assume the submodular function \(f:2^{[n]}\) is given by an _evaluation oracle_, which when queried with \(S[n]\), returns the value of \(f(S)\). Throughout the paper, we assume that \(f(S)[-M,M]\) for some \(M_{>0}\), and \(f()=0\). The assumption that \(f()=0\) can be made without loss of generality by instead minimizing \((S):=f(S)-f()\); this transformation can be implemented with one query and moves the range of \(f\) by at most \( M\), turning any dependence on \(M\) in an algorithms' complexity to \(2M\).

Note that this submodular function minimization setup is different from the setup of parallel convex optimization, as \(f\) only defined on the vertices of the unit hypercube. Nonetheless, it is known that there is a convex function \(f_{}\) defined on \(^{n}\), known as the Lovasz Extension, such that optimizing \(f_{}\) suffices for optimizing \(f\). Additionally, it is known how to compute a subgradient of \(f_{}\) at any point \(x\) in 1 round using at most \(n\) evaluation queries to \(f\) (as highlighted in Fact 2.4).

Now, we are ready to present our first result on parallel SFM. Later, in Section 2.1, we provide a more general version of this theorem, Theorem 2.5, which gives improved parallel complexities for approximately minimizing bounded, real-valued submodular functions.

**Theorem 1.3** (Sublinear Parallel SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\) with \(f()=0\) and \(|f(S)| M\) for all \(S[n]\), finds a minimizer of \(f\) in \((n^{1/3}M^{2/3})\) rounds and \((n^{2}M^{2})\) queries._

As discussed, Theorem 1.3, is obtained by using and enhancing tools for optimizing Lipschitz convex functions with a subgradient oracle. We in fact prove a more general result, namely that \((n^{1/3}/^{2/3})\) rounds and \((n^{2}/^{2})\) queries are sufficient to find an \( M\)-approximate minimizer (Theorem 2.5). Since the function is integer valued, approximating the scaled Lovasz extension to \((1/M)\) gives the exact minimizer to the submodular function. Our proof of Theorem 2.5 follows from our new result on \(_{}\)-convex optimization (Theorem 1.2). By applying it to a scaled version of the Lovasz extension; it is known that if \(f\) is a submodular function with range \([-M,+M]\), then the Lovasz extension scaled by \(O(1/M)\) is a convex function which is \(_{}\)-Lipschitz. However, it is important to note that SFM is only equivalent to _constrained_ minimization of the Lovasz extension in \(^{n}\), while Theorem 1.2 below is _unconstrained_ (e.g. applies for minimizing over \(^{n}\)). To apply Theorem 1.2 in the context of SFM, we give a general reduction from constrained to unconstrained optimization by adding a regularizer that restricts the minimizer of the regularized function to the constrainted set (see Lemma 2.3). This reduction is a generic technique and might be of independent utility.

**Parallel SFM in Two Rounds.** Our second SFM result is a simple combinatorial \(2\)-round algorithm which is efficient for functions of constant range.

**Theorem 1.4** (Two-round Parallel SFM).: _There is an algorithm (Algorithm 1) that when given an evaluation oracle for submodular \(f:2^{[n]}\) with \(f()=0\) and \(|f(S)| M\) for all \(S[n]\) finds a minimizer of \(f\) in 2 rounds and \(O(n^{M+1})\) queries._

The algorithm proving Theorem 1.4 relies on two key observations. First, if \(S_{*}\) is the minimizer of maximum size, for every subset \(T S_{*}\) and \(i[n]\) with \(f(T\{i\}) f(T)\), we have \(i S_{*}\). In other words, every element with a non-positive marginal at a subset \(T S^{*}\) is also contained in \(S_{*}\). This leads to the idea of _augmenting_ a set \(T\) by the set \(T^{}=T\{i:f(T\{i\}) f(T)\}\). Secondly, every subgradient \(g f(x)\) has at most \(M\) entries that are strictly positive. This ensures that there exists an \(M\)-sparse subset \(T S_{*}\) with the property that \(f(T\{i\}) f(T)\) forall \(i S_{*} T\). Consequently, our algorithm proceeds by augmenting all \(M\)-sparse sets, as it is guaranteed that one of these augmented sets is the maximal minimizer (see Section 3 for more details).

### Related Work

**Parallel Convex Optimization.** As mentioned in Section 1.1, there are a number of works studying the parallel complexity of \(_{p}\)_-Lipschitz convex optimization_. Perhaps, the most well-studied case is that of \(p=2\). In this case, the classic subgradient descent algorithm achieves a parallel complexity of \(O(^{-2})\)-rounds and the standard cutting-plane methods achieve a parallel complexity of \(O(n(1/))\)-rounds.  improved upon this rate, achieving parallel complexities of \((n^{1/4}^{-1})\) and \((n^{1/3}^{-2/3})\) respectively. The implications of these results for the \(p=\) case, which is the object of study in our paper, were discussed earlier and we are unaware of works on alternative upper bounds for \(p=\).

In terms of lower bounds, the \(p=\) case was studied in the prescient paper of  which obtains a \((n^{1/3}(1/))\) lower bound for minimizing \(_{}\)-Lipschitz functions over the \(_{}\)-ball (see also ). When \(\) is a constant, our upper bound matches this lower bound, though our dependence on \(\) is polynomial instead of logarithmic. For the \(p=2\) case,  proved a tight lower bound of \((1/^{2})\) on the parallel complexity when \(1/^{2}(n^{1/3})\), i.e., the parallel complexity of subgradient descent is optimal up to \((n^{1/3})\) rounds of queries. This was later improved by , which showed that subgradient descent is optimal up to \((n^{1/2})\) rounds.  considered the general \(p\) case (and other non-Euclidean settings) and proved a lower bound of \((^{-p})\) lower bound on the parallel complexity for \(2 p<\), \((^{-2})\) for \(1<p 2\), and \((^{-2/3})\) for \(p=1\). This paper also has lower bounds for smooth convex functions.

**Submodular Function Minimization.** We now expand on the history of SFM upper and lower bounds for parallel and sequential algorithms touched upon earlier. Since the seminal work of Edmonds in 1970 , there has been extensive work  on developing query-efficient algorithms for SFM.  gave the first polynomial time algorithms using the ellipsoid method. The state-of-the-art SFM algorithms include a \((n^{2})\)-query exponential time algorithm due to , \((n^{3})\)-query polynomial time algorithms due to ; a \((n^{2} M)\)-query polynomial time algorithm due to , and a \((nM^{2})\)-query polynomial time algorithm due to . Despite these algorithmic improvements, limited progress has been made on lower bounding the query complexity of SFM and the best known lower bound has been \((n)\) for decades . Very recently,  proved an \((n n)\)-lower bound for deterministic SFM algorithms.

All the algorithms above are highly sequential and proceed in at least \(n\) rounds. The question of parallel complexity for SFM was first studied in  where an \(( n/ n)\)-lower bound on the number of rounds required by any query-efficient SFM algorithm was given. The range \(M\) in their construction is \(M=n^{(n)}\). Subsequently,  proved a \((n^{1/3})\) lower bound on the round-complexity and the range is \(M=n\) for their functions. Recently,  described a \((n)\) lower bound with functions of range \(M=n^{(n)}\).

**Cutting plane methods.** Cutting plane methods are a class of optimization methods that minimize a convex function by iteratively refining a convex set containing the minimizer. Since the center of gravity method was developed independently in , there have been many developments of faster cutting plane methods over the last six decades , with the state-of-the-art due to .

## 2 Minimizing \(_{}\)-Lipschitz Functions and Submodular Functions

In this section we provide a new, improved parallel algorithm for convex optimization in \(_{}\) and show how to use these algorithms to obtain an improved parallel algorithm for SFM. In much of this section, we consider the following optimization problem which we term _stochastic convex optimization in \(_{}\)_. As we discussed, this problem generalizes parallel convex optimization in \(_{}\). The problem is more general in terms of the norms it considers and how it allows for stochastic gradients; we consider it as it could be useful more broadly and as it perhaps more tightly captures the performance of our optimization algorithm.

**Definition 2.1** (Stochastic Convex Optimization in \(_{}\)).: _In the stochastic convex optimization in \(_{}\) problem we have a (stochastic) subgradient oracle \(g:^{n}^{n}\) such that \(g(x) f(x)\) and \(\|g(x)\|_{2}^{2}^{2}\) for a convex function \(f:^{n}\) that is \(L\)-Lipschitz in \(_{}\). Given the guarantee that \(f\) has a minimizer \(x_{*}^{n}\) with \(\|x_{*}\|_{2} R\) our goal is to compute an \(\)-approximate minimizer of \(f\), i.e., \(x^{n}\) with \(f(x) f(x_{*})+\)._

Note that in Definition 2.1, \(_{}\) appears only to determine the norm in which \(f\) is Lipschitz. However, the bound on \(x_{*}\) in \(_{2}\) that can be easily converted to one in terms of \(_{}\) by using that \(\|x_{*}\|_{2}\|x_{*}\|_{}\). Furthermore, a convex function \(f:^{n}\) is \(L\)-Lipschitz in \(_{}\) if and only if \(\|g\|_{1} L\) for all \(g(x)\) for \(x^{n}\). Since \(\|g\|_{2}\|g\|_{1}\) we see that this stochastic convex optimization problem subsumes the (non-stochastic) problem of computing an \(\)-approximate minimizer to a convex function that is \(L\)-Lipschitz in \(_{}\) given a (deterministic) subgradient oracle. We define this more general problem as, interestingly, our algorithm tolerates this weaker stochastic oracle without any loss (as we discussed).

Our main result regarding stochastic convex optimization in \(_{}\) is given in the following theorem.

**Theorem 2.2** (Stochastic Convex Optimization in \(_{}\)).: _There is an algorithm that solves the stochastic convex optimization problem (Definition 2.1) in \(_{}\) (Definition 2.1) in \(((LR/)^{2/3})\) rounds and \((( R/)^{2})\) queries._

Due to the aforementioned connection between \(_{}\)-Lipschitz continuity and bounds on the subgradient, and the fact that \(\|x_{*}\|_{} 1\) implies \(\|x_{*}\|_{2}\), Theorem 2.2 immediately yields a \((n^{1/3}^{-2/3})\)-round, \((n^{-2})\)-query algorithm for the problem of minimizing a convex function that is \(1\)-Lipschitz in \(_{}\) and minimized at a point with \(_{}\)-norm at most 1. As discussed in the introduction, the parallel complexity of this algorithm is near-optimal for constant \(\).

**Theorem 1.2** (Parallel Convex Optimization in \(_{}\)).: _There is an algorithm that when given a subgradient oracle for convex \(f:^{n}\) that is \(1\)-Lipschitz in \(_{}\) and has a minimizer \(x_{*}\) with \(\|x_{*}\|_{} 1\) computes an \(\)-approximate minimizer of \(f\) in \((n^{1/3}^{-2/3})\) rounds and \((n^{-2})\) queries._

In Section 2.1, we show how to use Theorem 2.2 to obtain our results for SFM. We then present the ingredients in the proof of Theorem 2.2 (which we defer to Appendix A) in Section 2.2. As part of our reduction from SFM to Stochastic Convex Optimization in \(_{}\) in Theorem 2.2, we provide a general tool for reducing constrained to unconstrained minimization (Lemma 2.3); we use this lemma to facilitate our results in both sections.

### From Unconstrained Convex Optimization in \(_{}\) to SFM

Here we show how to use Theorem 1.2 to prove the following theorem regarding SFM.

**Theorem 1.3** (Sublinear Parallel SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\) with \(f()=0\) and \(|f(S)| M\) for all \(S[n]\), finds a minimizer of \(f\) in \((n^{1/3}M^{2/3})\) rounds and \((n^{2}M^{2})\) queries._

A key ingredient of our proof is the following general, simple technical tool which allows one to reduce constrained Lipschitz optimization over a ball in any norm to unconstrained minimization with only a very mild increase in parameters.

**Lemma 2.3**.: _Let \(f:^{n}\) be convex and \(L\)-Lipschitz with respect to norm \(\|\|:^{n}\). For any \(c,x^{n}\) and \(r\) let_

\[f^{c,r}_{}(x):=f(x)+2L\{0,\|x-c\|-r\}\,.\] (1)

_Then \(f^{c,r}_{}(x)\) is convex and \(3L\)-Lipschitz with respect to \(\|\|\). Additionally, for any \(y^{n}\) for which \(\|y-c\| r\), define \(y^{c,r}:=c+(y-c)\). Then,_

\[\|y^{c,r}-c\|=rf^{c,r}_{}(y^{c,r})=f(y^{c,r})  f^{c,r}_{}(y)-L(\|y-c\|-r)\,.\] (2)

_Consequently, \(f^{c,r}_{}(x)\) has an unconstrained minimizer \(x^{c,r}_{*}\) and all such minimizers satisfy_

\[\|x^{c,r}_{*}-c\| rf^{c,r}_{}(x^{c,r}_{*})=f(x^ {c,r}_{*})=_{x^{n}\|x-c\| r}f(x)\,.\] (3)Lemma 2.3 implies that minimizing \(f\) subject to a distance constraint \(\|x-c\| r\) reduces to unconstrained minimization of \(f_{}^{c,r}\). More formally, to compute an \(\)-optimal solution to the constrained minimization problem, \(_{x^{n}:\ \|x-c\| r}f(x)\), it suffices to instead compute an \(\)-optimal solution, \(x_{}\), to the unconstrained minimization problem \(_{x}f_{}^{c,r}(x)\), and then output that point \(x_{}\) if \(\|x_{}-c\| r\) and output \(x_{}^{r,c}\)-otherwise. From (2), we get \(f(x_{}^{r,c}) f_{}^{c,r}(x_{})+\), where \(:=_{x}f_{}^{c,r}(x)=_{x^{n}\|x-c\|  r}f(x)\) and the last equality follows from (3).

We remark that the \(2L\) in the definition of \(f_{}^{c,r}\) can be changed to \(L+\) for any \( 0\) with the only effect of turning the \(L\) in (2) to \(\) and causing (3) to only hold for some minimizer (rather than all) if \(=0\). The proof of Lemma 2.3 is deferred to Appendix A.

Next, we obtain Theorem 1.3 by applying Theorem 2.2 to the Lovasz extension of the submodular function \(f\) extended to an unconstrained minimization problem by Lemma 2.3.

Given a submodular function \(f\) defined over subsets of an \(n\) element universe, the Lovasz extension \(f_{}:^{n}\) is defined as follows: \(f_{}(x):=_{i[n]}x_{_{x}(i)}(f(S_{_{x},i})-f(S_{_{ x},i-1}))\), where \(_{x}:[n][n]\) is the permutation such that \(x_{_{x}(1)} x_{_{x}(2)} x_{_{x}(n)}\) (ties broken in an arbitrary but consistent manner), and \(S_{_{x},j}\) is the subset \(\{_{x}(1),,_{x}(j)\}\).

Next we give standard properties of the Lovasz extension and use them to prove Theorem 1.3.

**Fact 2.4** (e.g., ).: _The following are true about the Lovasz extension \(f_{}\):_

1. \(f_{}\) _is convex with_ \(_{x^{n}}f_{}(x)=_{S V}f(S)\)_. Indeed, given any_ \(x^{n}\)_, in_ \(n\) _queries one can find a subset_ \(S\) _with_ \(f(S) f_{}(x)\)_._
2. _Given any_ \(x^{n}\) _and corresponding permutation_ \(_{x}\)_, the vector_ \(g^{n}\) _where_ \(g(x)_{((i))}:=f(S_{_{x},i})-f(S_{_{x},i-1})\) _is a subgradient of_ \(f_{}\) _at_ \(x\)_. Furthermore,_ \(g(x)\) _can be computed in_ \(1\) _round of_ \(n\) _queries to an evaluation oracle for_ \(f\)_._
3. _If_ \(f\) _has range in_ \([-M,+M]\)_, then the_ \(_{1}\)_-norm of the subgradient is bounded, in particular,_ \(\|g(x)\|_{1} 3M\)_. Equivalently,_ \(f_{}\) _is_ \(3M\)_-Lipschitz with respect to the_ \(_{}\)_-norm._

As discussed in Section 1.1, to prove Theorem 1.3, it suffices to prove the following more general result regarding approximately minimizing a submodular function.

**Theorem 2.5** (\(\)-approximate minimizer for SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\) with minimizer \(x_{*}\), \(f()=0\) and \(|f(S)| M, S[n]\), finds a set \(S\) with \(f(S) f(x_{*})+ M\) in \((n^{1/3}/^{2/3})\) rounds and a total of \((n^{2}/^{2})\) queries._

Proof of Theorem 2.5.: By Fact 2.4, SFM reduces to minimizing \(f_{}\) over \(x^{n}\) which is the same set \(\{x^{n}:\|x-c\|_{} 0.5\}\) where \(c\) is the \(n\)-dimensional vector with all entries \(0.5\). By Lemma 2.3, we can do so by applying Theorem 2.2 to the regularized version \(f_{}^{c,0.5}\) of \(f_{}\) with respect to the \(_{}\)-norm. This regularized function is guaranteed to have a minimizer \(x_{*}\) with \(\|x_{*}\|_{2}\) which also minimizes \(f_{}\) in \(^{n}\). The subgradient of this regularized function at any point can be computed from the subgradient of \(f_{}\) at the same point which takes \(n\) evaluation oracle queries to the submodular function. Hence, we obtain an \( M\)-approximate minimizer in \((n^{1/3}/^{2/3})\) rounds and with a total of \((n^{2}/^{2})\) queries to the evaluation oracle of \(f\). 

Now, we are ready to prove Theorem 1.3.

Proof of Theorem 1.3.: Set \(=\) and apply Theorem 2.5 to obtain, in \((n^{1/3}M^{2/3})\) rounds and with a total of \((n^{2}M^{2})\) queries to the evaluation oracle of \(f\), a \(x^{n}\) with \(f_{}(x)_{z^{n}}f_{}(z)+\). Then by property 1 in Fact 2.4, one can get a subset \(A V\) with \(f(A)_{S V}f(S)+\). As \(f\) is assumed to be integer valued, \(A\) must be the minimizer of the submodular function. 

### Parallel Stochastic Convex Optimization in \(_{}\)

Here we present the key steps in proving Theorem 2.2 regarding our new parallel results for the stochastic convex optimization problem in \(_{}\) (Definition 2.1). Throughout this subsection, in our exposition, lemma statements, and proofs we assume that we are in the setting of Definition 2.1.

To prove Theorem 2.2 we apply the approach of  with two modifications. First, we consider the convolution of \(f\) with a centered Gaussian density function with covariance \(^{2}_{n}\). However we show that in our setting, it is possible to use a larger value of \(\) without perturbing the function value too much, due to the \(_{}\) geometry. Unfortunately, the minimizer of the convolved function may move outside the box of radius \(R\). Thus, the second modification we make is working with a regularized function, \(f_{}\), which is pointwise close to \(f\), still \(L\)-Lipschitz in the \(_{}\) norm, and keeps the minimizer in the ball or radius \(R\) even after applying the convolution with a Gaussian.

In the remainder of this subsection we first present the ingredients going into the proof of Theorem 2.2, and then give a brief explanation for how they fit into the proof of Theorem 2.2, deferring the complete proof to the appendix. We start with our bound on function perturbation after adding a Gaussian with covariance \(^{2}_{n}\) to an \(_{}\) Lipschitz function (Lemma 2.6). We then introduce the concept of a ball optimization oracle, along with a result on how to implement it in low depth for the special case of a function that is the result of Gaussian convolution (Proposition 2.8). Lastly, we present the result which allows us to use a ball optimization oracle black-box to obtain the desired depth (Proposition 2.9).

Now, we are ready to present the lemma which allows us to obtain a better dependence of depth on the dimension \(n\), compared to the naive \(n^{2/3}\) obtained by directly applying the \(_{2}\)-Lipschitz optimization result. We start with the definition of Gaussian convolution.

Gaussian Convolution.Let \(_{}:=(2)^{-n/2}(-}\|x\|_{2}^{2})\) be the probability density function of \((0,^{2}_{n})\). Given a function \(f:^{n}\), we define its convolution with a Gaussian of covariance \(^{2}_{n}\) by \(_{}:=f*_{}\), i.e.

\[_{}(x):=_{y(0,^{2}_{n}) }[f(x+y)]=_{^{n}}f(x-y)_{}(y)y.\] (4)

Next, we present a lemma which allows us to obtain a better dependence on the dimension \(n\) in depth, compared to the naive \(n^{2/3}\) obtained by directly applying the \(_{2}\)-Lipschitz optimization result. This lemma shows that we can perform more Gaussian smoothing (as compared to the \(_{2}\)-setting) without perturbing the function too much (as mentioned in Section 1.1).

**Lemma 2.6** (Gaussian Convolution Distortion Bound for \(_{}\)).: _Let \(f:^{n}\) be \(L\)-Lipschitz with respect to the \(_{}\)-norm. Then for any point \(x^{n}\), we have \(|_{}(x)-f(x)| L\)._

Proof.: Note that \(|_{}(x)-f(x)|_{^{n}}|f(z)-f( x)|_{}(x-z)z L_{^{n}}\|x-z \|_{}_{}(x-z)z\) where the first inequality follows from the definition of \(_{}\) and the second follows as \(f\) is \(L\)-Lipschitz in \(_{}\)-norm. The RHS is simply the expected \(_{}\)-norm of a zero-mean random Gaussian vector with covariance \(^{2}_{n}\), and this is \(()\) (e.g., ). More precisely, we get

\[|_{}(x)-f(x)| L_{y(0,^{2}_{n})}\|y\|_{} L.\]

As mentioned in Section 1.1, by contrast, convolving a function \(f\) that is \(L\)-Lipschitz in \(_{2}\) with a Gaussian of covariance \(^{2}_{n}\) could change the function value by \(O()\). Hence, the \(_{}\) geometry allows us to add a larger amount of Gaussian smoothing without changing the function value by more than \(\), which in turn allows for better rates.

Ball Optimization.A subroutine that we use for minimizing \(_{}\) is called a _ball optimization oracle_. As suggested by , the concept of ball optimization oracle is related to the notion of trust regions, explored in several papers, such as . The particular ball optimization procedure we employ takes a function \(F:^{n}\) and a point \(^{n}\), which is an approximate solution to \(F\) in a small ball of \(\). More formally, we work with the following definition:

**Definition 2.7** (Ball Optimization Oracle ).: _Let \(F:^{n}\) be a convex function. \(_{}\) is an \((,,r)\)-ball optimization oracle for \(F\) if given any \(^{n}\), it returns an \(x^{n}\) with the property_

\[[F(x)+\|x-\|_{2}^{2} ]\ \ F(x_{}^{})+\|x_{}^{}-\|_{2}^{2}+\,\]

_where \(x_{}^{}=_{x B_{}(r)}(F(x)+ \|x-\|_{2}^{2})\)._From  it is known is that for any Lipschitz convex function \(f\), any stochastic subgradient oracle \(g\) as above, and any \(\), if we set \(r=\), then efficient ball-optimization oracles exist. More formally, we use the following proposition from  which is in turn inspired from .

**Proposition 2.8** (Proposition 3, ).: _Let \(f:^{n}\) be convex and \(g:^{n}^{n}\) be a stochastic subgradient oracle satisfying \([g(x)] f(x)\) and \(\|g(x)\|_{2}^{2}^{2}\) for all \(x^{n}\). Let \(_{}:=f*_{}\), i.e.,_

\[_{}(x):=_{y(0,^{2}_{n})} [f(x-y)]=_{^{n}}f(x-y)_{}(y)y.\] (5)

_Then there is a \((,,)\)-ball optimization oracle for \(_{}\) which makes \(O(}{})\) total queries to \(g\) in a constant number of rounds._

Highly Parallel Optimization.As shown in , the ball optimization oracle above can be used for highly-parallel optimization as follows.

**Proposition 2.9** (Proposition 2 in ).: _Fix a function \(F:^{n}\) which is \(L\)-Lipschitz with respect to the \(_{2}\)-norm and convex. Suppose \(R 0\) is a parameter such that \(x^{}_{x}F(x)\) satisfies \(\|x^{}\|_{2} R\). Let \(r(0,R]\) and \(_{}(0,LR]\) be two parameters. Define the following quantities_

\[:=}}, K:=()^{ },_{}:= }K^{2}}{R^{2}}^{2}.\] (6)

_Then, there exists a universal constant \(C>0\) and an algorithm BallAccel which runs in \(CK\) iterations and produces a point \(x\) such that \(F(x) F(x^{})+_{}\). Moreover,_

1. _Each iteration makes at most_ \(C^{2}()\) _calls to_ \((}{C},,r)\)_-ball optimization oracle with values of_ \([}{C},}}]\)_._
2. _For each_ \(j[_{2}K+C]\)_, at most_ \(C^{2} 2^{-j}K()\) _iterations query a_ \((}{C^{2j}}^{-2}(),,r)\)_-ball optimization oracle for some_ \([}{C},}}]\)_._

While the proof of Theorem 2.2 is deferred to Appendix A, we provide some intuition for how to use the stated components to obtain the result. A natural approach would be to apply Proposition 2.9 to \(F:=_{}\). However, to ensure that the minimizer of \(F\) has \(_{2}\)-norm at most \(R\), we will instead work with \(F:=_{_{}}^{c,R}\) as defined in Lemma 2.3. The idea is to invoke Lemma 2.3 and apply BallAccel on the function \(F:=_{_{}}^{c,R}\) with respect to \(\|\|_{2}\), \(c\) being the origin, and \(:=}}{L}\). With this choice of \(\), by Lemma 2.6, we know \(|F(x)-f_{}^{c,R}(x)|_{}\) everywhere. Noting that \(F\) has a minimizer \(x_{}\) with \(\|x_{}\|_{2} 3R\), this enables us to apply Proposition 2.9 to \(F\), obtaining the stated bounds on the number of rounds and query complexity.

## 3 2-Round \(O(n^{M+1})\)-Query Algorithm for SFM

Here we present our 2-round, \(O(n^{M+1})\)-query algorithm for SFM. The algorithm AugmentingSets, given in Algorithm 1, iterates over every \(M\)-sparse \(S[n]\) (i.e. \(|S|\)) (denoted \(\)). For every such \(S\) the algorithm then builds the augmented set \(A(S)\), consisted of the union of \(S\) and all elements \(i\) that have non-positive marginal with respect to \(S\), i.e., \(f(S\{i\}) f(S)\). The algorithm then outputs the set \(A(S)\) that has the smallest value.

As we show below, computing all the \(A(S)\) for \(M\)-sparse sets \(S\) can be done in \(1\) round and \(O(n^{M+1})\)-queries, and then computing an element of \(A(S)\) with the smallest value can be done in another round and \(O(n^{M+1})\)-queries. The correctness of the algorithm is guaranteed by the fact that \(A(S)\), for some \(|S| M\), is the maximal minimizer of \(f\), and therefore the algorithm outputs a set with the optimum value6.

Our main result of this section is the following theorem.

**Theorem 1.4** (Two-round Parallel SFM).: _There is an algorithm (Algorithm 1) that when given an evaluation oracle for submodular \(f:2^{[n]}\) with \(f()=0\) and \(|f(S)| M\) for all \(S[n]\) finds a minimizer of \(f\) in 2 rounds and \(O(n^{M+1})\) queries._

Proof.: First, we bound the parallel and query complexity of the algorithm. Line 2 to Line 5 can be implemented in 1 round as they simply evaluate \(f\) on all subsets of \([n]\) of size \( M+1\). Line 6 can be implemented in 1 round by evaluating \(f(A(S))\) for each \(S\) in parallel. Consequently, the algorithm is implementable in 2 rounds. To bound the query complexity, note that \(||_{k=0}^{M}=O(n^{M})\) and the algorithm only makes \(O(n)\) queries for each \(S\) (\(O(n)\) in Line 4 and 1 in Line 6). Consequently, the algorithm makes \(O(n^{M+1})\) total queries.

It only remains to show that the algorithm outputs a minimizer of \(f\). We prove this by showing that for some \(S\), its augmented set \(A(S)\) is the maximal minimizer of \(f\), i.e., the union of all minimizers, which is a minimizer itself by submodularity. This suffices as the algorithm outputs the \(A(S)\) for \(S\) of smallest value. Let \(S_{*}\) be the maximal minimizer of \(f\). We build a subset \(T S_{*}\) of size \(|T| M\), which we call _anchor_, as follows. Start with \(T=\) and an arbitrary ordering of elements of \(S_{*}\). For each element \(i S_{*}\) in this order, we add it to the current \(T\) if \(f(T\{i\})>f(T)\). Since \(f\) only takes integer values, this means that whenever we add an element \(i\) to \(T\), the value of \(f(T)\) goes up by at least \(1\). At the end of the process we have \(f(T)|T|\). Since \(f(T) M\), it follows that \(|T| M\) and therefore \(T\).

We now claim that \(A(T)=S_{*}\). For any element \(i S_{*} T\), we didn't add \(i\) to \(T\) because \(f(T_{i}\{i\})-f(T_{i}) 0\), where \(T_{i} T\) is the value of \(T\) when element \(i\) is visited in the procedure above. By submodularity, \(f(T\{i\})-f(T) f(T_{i}\{i\})-f(T_{i}) 0\). This implies that \(S_{*} A(T)\). Also note that for any \(j S_{*}\), we have \(f(S_{*}\{j\})>f(S_{*})\) by the _maximality_ of \(S_{*}\). It again follows from submodularity and \(T S_{*}\) that \(f(T\{j\})>f(T)\), which implies \(j A(T)\). This proves \(A(T)=S^{*}\) and completes the proof of the theorem. 

## 4 Conclusion

In this paper we designed two new parallel algorithms for minimizing submodular functions \(f:2^{[n]}[-M,+M]\) with round complexities \((n^{1/3}M^{2/3})\) and \(2\), and query complexities \((n^{2}M^{2})\) and \(O(n^{M+1})\), respectively. These \(M\)-dependent sublinear dependence on \(n\) in the round complexities stand in contrast to the \((n)\)-lower bound on the number of rounds required for SFM when \(M=n^{(n)}\). On the way to the first result, we obtain a new efficient parallel algorithm for \(\)-approximate minimization of \(_{}\)-Lipschitz convex functions over \(^{n}\) with round-complexity \((n^{1/3}^{-2/3})\). Given results of  the dependence on \(n\) is optimal for constant \(\).

Two related open questions are whether one can obtain \(o(n)\)-round SFM algorithms with polylogarithmic dependence on \(M\), and whether one can obtain algorithms for \(\)-approximate minimization of \(_{}\)-Lipschitz convex functions over \(^{n}\) in \((n^{1/3}(1/))\)-rounds, or can one prove lower bounds ruling them out. Another related open question is whether one can perform SFM in \(O((M))\)-rounds with query complexity \((n)\).