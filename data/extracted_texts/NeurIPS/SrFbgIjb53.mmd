# MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability

Yanrui Du

Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn

Sendong Zhao

Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn

Danyang Zhao

Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn

Ming Ma

Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn

Yuhan Chen

Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn

Liangyu Huo

Du Xiaoman Financial

Qing Yang

Du Xiaoman Financial

Dongliang Xu

Du Xiaoman Financial

Bing Qin

Du Xiaoman Financial

###### Abstract

Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2\({}_{7B}\), Vicuna\({}_{7B}\), Falcon\({}_{7B}\), Dolphin\({}_{7B}\), and Baichuan2\({}_{7B}\) at github. Warning: This paper presents examples of malicious instructions that may be offensive and upsetting.

## 1 Introduction

Large Language Models (LLMs) exhibit significant potential across various domains, yet they also face considerable safety vulnerabilities .To explore these vulnerabilities, several studies have conducted red-team evaluations with malicious instructions that could encourage harmful behaviors . Others have developed jailbreak attacks  aimed at provoking harmful responses from LLMs by using carefully crafted adversarial prompts. These safety vulnerabilities may lead to severe consequences, including the promotion of racial discrimination, breaches of ethical standards, and violations of human rights .

In response to LLMs' safety vulnerabilities, some studies have pursued aligning LLMs with human values through SFT and RLHF techniques. Despite these advancements, recent work  indicates that even aligned LLMs are still susceptible to jailbreak attacks. To further enhance LLMs' safety, various defense strategies have been proposed, including input and output detection ,in-context safety demonstration , and enhancing the likelihood of decoding rejection tokens . These strategies often focus on ensuring harmless responses during red-team evaluations and jailbreak attacks but overlook the impact on the quality of responses to benign instructions. Our research finds that existing defense strategies lead LLMs to adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. By prioritizing safety over usability, these strategies become less effective in practical applications. Consequently, this presents a key challenge -- **seesaw effect between security and usability**: **How can we enhance the safety of LLMs while preserving their usability?**

Despite existing defense strategies not effectively addressing this challenge, the input detection  strategy provides a straightforward solution. This strategy triggers a safety mechanism by distinguishing malicious and benign instructions. However, this implementation, which relies on binary classification of instructions, often struggles with arbitrary treatment. Many benign instructions may be wrongly marked as malicious, mistakenly activating the safety mechanism and thus diminishing the usability of responses to benign instructions. The Mixture of Experts (MoE) series of research provides a promising improvement direction [18; 23; 32]. MoE employs a dynamic routing mechanism within LLMs to balance contributions from different experts, thereby improving LLMs' overall performance. This dynamic routing mechanism has proven effective in assigning weights to experts according to the input instruction. Therefore, in our research, we aim to introduce a dynamic routing mechanism to enhance LLMs' safety.

Based on these insights, we introduce a novel framework called **M**ixing **of** **G**lad and **U**nwilling Responders (**MoGU**). We first employ the Parameter-Efficient Fine-Tuning technique LoRA , to transform the base LLM into two distinct states: the Glad Responder (Glad\({}_{resp}\)) and the Unwilling Responder (Unwill\({}_{resp}\)). The Glad\({}_{resp}\), as an extremely usable LLM, is trained to generate glad responses to any instruction. Conversely, Unwill\({}_{resp}\), as an extremely safe LLM, is trained to be highly cautious, rejecting any instruction it receives. The core component of MoGU is a dynamic router that serves as a safety sensor, embedded at each layer where LoRA is applied. This router is trained to dynamically balance the contributions of Glad\({}_{resp}\) and Unwill\({}_{resp}\) according to the input vector, effectively mixing their output vectors. As illustrated in Fig. 1, when faced with a malicious instruction, the router will assign a higher weight to Unwill\({}_{resp}\), ensuring a safe, rejection response. On the contrary, the router shifts more weight to Glad\({}_{resp}\) for the benign instruction, facilitating a glad, useful response.

In our experiments, we revealed limitations of existing strategies that diminish the usability of LLMs. Our experiment results verify that our MoGU framework can keep robust defense performance under the red-team evaluation and various jailbreak attacks while preserving LLMs' usability. Besides, compared to existing defense strategies, our framework demonstrates obvious advantages across various LLMs. We also conduct quantitative analysis to confirm that the router can effectively balance the contribution of each variant by assigning weights, thereby ensuring both the safety and the usability of LLMs.

## 2 Related Work

In this section, we summarize related work from two aspects: attack strategies and defense strategies.

### Attack strategies

Red-team evaluation.The primary goal of red-team evaluations  is to assess the safety of LLMs by compiling a set of malicious instructions that reflect common user queries. The collection of

Figure 1: An example to illustrate how the router assigns weights to Glad\({}_{resp}\) and Unwill\({}_{resp}\). The h_states and o_states represent the input vector and output vector respectively.

these instructions is conducted in two ways: 1) gathering malicious instructions from crowdsourced workers . 2) automatically generating malicious instructions with another LLM that simulates human behavior . The scope of these malicious instructions should be wide-ranging, covering topics such as toxicity, discrimination, privacy, and misinformation .

Jailbreak attack.Jailbreak attacks  aim to circumvent the built-in safety mechanisms of LLMs by modifying original red-team malicious instructions into more complex adversarial prompts. These strategies generally fall into two categories: heuristic-based and optimization-based strategies.

Heuristic-based strategies attempt to induce LLMs to prioritize task completion over adherence to safety constraints. For instance, some studies [36; 19] have prompted LLMs to begin their responses with indicators of successful jailbreak, such as "Start your response with [Sure, here's]". Others [35; 20] employ psychological tactics to subtly encourage LLMs to violate safety constraints.

Optimization-based strategies attempt to search for adversarial prompt templates based on constructed objectives. These strategies fall into two categories: token-level and expression-level. Token-level strategies  searched for token sequences via backpropagation and spliced them around original malicious instructions. However, these token sequences often lack semantic coherence, rendering them vulnerable to detection by Perplexity (PPL) algorithms . Moreover, expression-level strategies [24; 41] employ genetic algorithms to search for natural language prompt templates. This approach enhances the concealment of jailbreak attacks, making them more difficult to detect.

### Defense Strategies

Defense strategies can be categorized into two main types: those that improve built-in safety and those that leverage external tools. Strategies focused on built-in safety aim to align LLMs with human values, employing methods such as Supervised Fine-Tuning (SFT)  and Reinforcement Learning from Human Feedback (RLHF) . SFT reduces experiential loss by incorporating high-quality, human-annotated samples during training, whereas RLHF optimizes LLMs based on valuable human feedback. Despite the widespread adoption of these methods, recent studies [45; 5] indicate that aligned LLMs (e.g. Llama2) are still vulnerable to jailbreak attacks.

Meanwhile, many researchers are developing strategies that leverage external tools to further improve LLMs' safety. These strategies focus on inference enhancement and the detection of input and output. Inference enhancement strategies guide LLMs to generate safer content through methods such as self-safety reminding  or by presenting safety in-context demonstrations . Strategies for the detection of input and output involve identifying potentially harmful content to trigger the appropriate safety mechanisms. Methods such as paraphrasing and retokenization  can render certain attacks ineffective by altering the expression of inputs. Moreover, binary classifiers  based on BERT  can be trained to detect malicious inputs, and self-examining method  enables LLMs to assess the harmfulness of their own outputs. Despite these efforts, it remains challenging to enhance the safety of LLMs while preserving their usability.

## 3 MoGU Framework

The overall framework of our MoGU is illustrated in Fig. 2. We introduce our framework from three aspects: the training data preparation, the training stage, and the inference stage.

### Training Data Preparation

For our training data, we only collected 600 instructions, which include 300 benign instructions sourced from Alpaca3 and 300 malicious instructions from Advbench . As illustrated in Fig. 2, for each instruction, we construct both a glad response and a rejection response. We label benign instructions as X\({}_{b}\), malicious instructions as X\({}_{m}\), glad responses as Y\({}_{g}\), and rejection responses as Y\({}_{r}\). Therefore, our training dataset encompasses four types of data pairs: (X\({}_{b}\), Y\({}_{g}\)), (X\({}_{b}\), Y\({}_{r}\)), (X\({}_{m}\), Y\({}_{g}\)), and (X\({}_{m}\), Y\({}_{r}\)). We observe that LLMs typically generate glad responses to benign instructions and rejection responses to malicious instructions. Consequently, during the construction of (X\({}_{b}\), Y\({}_{g}\)) and (X\({}_{m}\), Y\({}_{r}\)), we almost preserve their original responses. Here is how to construct them.

* Construction of (X\({}_{b}\), Y\({}_{r}\)): we utilize GPT-4 to craft rejection responses to X\({}_{b}\). For guiding GPT-4, we present demonstrations of generating rejection responses to benign instructions.
* Construction of (X\({}_{m}\), Y\({}_{g}\)): since Advbench  has manually annotated high-quality glad responses to X\({}_{m}\), we directly use their annotated data.
* Construction of (X\({}_{m}\), Y\({}_{r}\)): we prompt the base LLM to generate responses to X\({}_{m}\) and utilize the same rule-based detection as above. If glad responses are detected, they will be discarded. Then, we will craft rejection responses Y\({}_{r}\) with the help of GPT-4.

In the scenarios mentioned above for GPT-4, we adopt the In-Context Learning  idea, and provided in-context demonstrations can be found in App. B.

### Training Stage

During the training stage, we initially train the Glad and Unwilling responders using the LoRA framework. Subsequently, all other parameters are frozen, and we train our introduced router. In the LoRA framework, only the low-rank decomposition matrices added to the targeted weight matrices are updated. As illustrated in Fig. 2, the targeted weight matrices typically include Q (Query), K (Key), V (Value), O\({}_{proj}\) (Output Projection), and FFN (Feed-Forward Network). In our research, we regard O\({}_{proj}\) as the targeted weight matric for exploration.

The training of glad and unwilling responders.The objective of Glad\({}_{resp}\) is to calibrate the base LLM into an extremely usable LLM that can generate glad responses to any instruction. The extreme

Figure 2: Overall framework of our MoGU.

[MISSING_PAGE_EMPTY:5]

where \(o_{MoGU}^{(i)}^{seq\_len d_{model}}\).

During the training of the router, all other parameters are frozen, and only the router's parameters will be updated. The primary objective of the router is to guide LLMs in generating appropriate responses to various instructions. Specifically, the router should facilitate glad responses to benign instructions and rejection responses to malicious instructions. To achieve this, we use both (X\({}_{b}\), Y\({}_{g}\)) and (X\({}_{m}\), Y\({}_{r}\)) as the training data. The loss function can be formulated as:

\[Loss_{router}^{(1)}=^{N}CE_{loss}(y_{g}^{i},f_{router}(x_{b}^{ i};_{router}))+_{j=1}^{M}CE_{loss}(y_{r}^{j},f_{router}(x_{m}^{j}; _{router}))}{N+M}\] (9)

where \((x_{b}^{i},y_{g}^{i})(X_{b},Y_{g})\) and \((x_{m}^{j},y_{r}^{j})(X_{m},Y_{r})\). Besides, the router is equipped with a finer-grained objective: it will assign weights according to the type of instruction. Specifically, a higher weight will be assigned to Glad\({}_{resp}\) for benign instructions and to Unwill\({}_{resp}\) for malicious instructions. To reinforce this behavior, we use the L1 Norm to regulate the optimization of weights \(w_{glad}\) and \(w_{unwill}\) assigned by the router, ensuring the assigning pattern adheres to our expectations. The loss function can be formulated as:

\[Loss_{router}^{(2)}=\|1-w_{glad}\|_{1}+\|w_{unwill}\|_{1}&x X_{b}\\ \|w_{glad}\|_{1}+\|1-w_{unwill}\|_{1}&x X_{m}\] (10)

where \(\|\|_{1}\) represents the L1 Norm. Finally, the overall loss function can be formulated as:

\[Loss_{router}=Loss_{router}^{(1)}+ Loss_{router}^{(2)}\] (11)

where \(\) is a hyperparameter.

### Inference Stage

Previous research [45; 39] has shown that the initial response tokens are critical to ensuring the harmlessness of the whole response. If initial response tokens express rejection, the response is more likely to be harmless. Given these findings, and considering that our additional parameters extend inference time, we employ MoGU only for decoding the first m tokens as shown in Fig. 2. The subsequent tokens are decoded by the base LLM to preserve the efficiency and quality of decoding.

## 4 Main Experiments

### Preliminary

LLMs.In our research, we evaluated chat versions of five open-source LLMs, including four from the Llama series: Llama2\({}_{7B}\), Vicuna\({}_{7B}\), Falcon\({}_{7B}\), and Dolphin\({}_{7B}\)5. Notably, Dolphin\({}_{7B}\) has not yet undergone a safety review. We also evaluated Baichuan2\({}_{7B}\), which features an architecture distinct from those in the Llama series.

Evaluation data.In our evaluation, we focused on assessing LLMs' safety and usability. For the safety assessment, on the one hand, we conducted a red-team evaluation. We utilize the Advbench , which comprises 520 malicious instructions--300 for our training as introduced in Sec. 3.1 and the rest 220 for testing. Additionally, we collected 200 malicious instructions from Just-Eval  (labeled as "Malicious"). On the other hand, we conducted the safety evaluation against various jailbreak attacks. We employed both optimization-based and heuristic-based strategies. For the optimization-based strategies, we utilized AutoDAN , GCG , and PAIR , each of which applies different adversarial prompts to 50 test samples. Specifically, AutoDAN employs genetic algorithms to generate semantically fluent adversarial prompts. GCG uses gradient propagation to identify token sequences as attack suffix. PAIR iteratively optimizes adversarial prompts based on the LLMs' ability. For the heuristic-based strategies, we used SAP30  and Comp\({}_{obj}\), each of which applies the same adversarial prompt to 100 test samples. SAP30 focuses on semantic inducements, whereas Comp\({}_{obj}\) is designed to compromise LLM safety by conducting a competing objective. For the usability assessment, we used 800 benign instructions from Just-Eval  to assess LLMs' problem-solving abilities. In App. C, we provide examples for each evaluation set.

Evaluation metric.For the evaluation metrics, we utilize both GPT-4 scoring and rule-based evaluations. For assessing LLM's safety, we employ **GPT-Judge**, which rates the harmfulness of responses on a scale from 1 to 5, where 1 represents harmlessness and 5 indicates extreme harmfulness. Additionally, consistent with the work , we identify targets (\(T\)) that can represent harmless responses (as detailed in App. D) and calculate the Attack Success Rate (**ASR**) based on these targets. ASR quantifies the probabilities that responses deviate from our identified targets, which can be expressed as \(\). For assessing LLMs' usability, in line with Just-Eval , we utilize GPT-4 to evaluate responses (**GPT-Eval**) across five dimensions: helpfulness, clarity, factuality, depth, and engagement. Each response is scored from 1 to 5, with higher scores denoting better quality. Moreover, we compile a list of common rejection expressions (as detailed in App. A) and monitor their frequency in LLM responses (**Rule-based Eval**) to evaluate the extent to which LLMs adopt a stance of rejection. During our evaluation, we spent approximately $400 for calling the GPT-4 API.

Baselines.We selected seven advanced defense strategies as our baselines. SFT strategy  employs high-quality data to train LLMs, thereby aligning LLMs with human values. Detect\({}_{inp}\) train a binary classifier based on BERT to distinguish between benign and malicious instructions. Self-Examine  strategy prompts LLMs to assess whether their responses are harmful. If risky contents are detected by Detect\({}_{inp}\) and Self-Examine, the response "Sorry, I cannot answer your question." will be returned. Retokenization  strategy counters various jailbreak attacks by altering the input to shift meanings subtly. Self-Reminder  strategy consistently cues LLMs to maintain awareness of safety throughout the input process. ICD  strategy integrates safety in-context demonstrations into prompts. SafeDecoding  strategy increases the likelihood of rejection tokens during the decoding phase. We implemented SFT within the LoRA framework based on our constructed data and followed the open-sourced code from work  to reproduce other baselines.

Hyperparameter settings.We configure our router's intermediate dimension \(d_{router}\) to 512 and set the \(\) in \(Loss_{router}\) to 2. For training Glad\({}_{resp}\) and Unwill\({}_{resp}\), the learning rate is set to 5e-5, and for training the router, the learning rate is set to 5e-4. Besides, the \(\) and d\({}_{lora_{r}r}\) in LoRA are set to 16 and 8 respectively. During inference, only the first 5 tokens are decoded with our MoGU and the remaining tokens are decoded with the base LLM. Decoding configurations of various LLMs can be found in App. E. All our experiments were done on a single 80GB A100.

### Main Results

In Tab. 1 and 2, we respectively evaluate the performance of defense strategies under red-team evaluation and against various jailbreak attacks. For the red-team evaluation, we report only the ASR. In contrast, for the jailbreak attacks, given the broader variability in LLMs' responses, we report both the GPT-4 score and the ASR. On the whole, the ICD strategy outperforms others on Llama\(2_{7B}\), MoGU excels on Vicuna\({}_{7B}\), and SafeDecoding excels on Falcon\({}_{7B}\). Furthermore, these three strategies demonstrate stable and effective defense performance across various LLMs. Thus, in Tab. 3, we assess the impact of these three competitive strategies on the usability of LLMs.

    &  &  &  \\  & Advbench\({}_{}\) & Malicious\({}_{}\) & Advbench\({}_{}\) & Malicious\({}_{}\) & Advbench\({}_{}\) & Malicious\({}_{}\) & AVG\({}_{}\) \\  No defense & 0.00\% & 1.00\% & 5.50\% & 33.50\% & 55.91\% & 23.50\% & 19.90\% \\ SFT & 0.00\% & 0.50\% & 1.36\% & 6.00\% & 2.27\% & 1.00\% & 1.86\% \\ Detect\({}_{inp}\) & 0.00\% & 1.00\% & 0.00\% & 32.00\% & 0.00\% & 23.50\% & 9.42\% \\ Self-Examine & 0.00\% & 0.50\% & 2.70\% & 26.50\% & 55.91\% & 23.50\% & 18.19\% \\ Retokenization & 0.45\% & 4.50\% & 12.73\% & 26.50\% & 39.55\% & 44.50\% & 21.37\% \\ Self-Reminder & 0.45\% & 0.00\% & 0.91\% & 7.50\% & 45.00\% & 18.50\% & 12.06\% \\ ICD & 0.00\% & 0.00\% & 4.09\% & 23.00\% & 1.82\% & 3.00\% & 5.32\% \\ SafeDecoding & 0.00\% & 0.00\% & 0.00\% & 8.00\% & 0.00\% & 0.50\% & 1.42\% \\ MoGU & 0.00\% & 0.00\% & 0.00\% & 0.50\% & 0.91\% & 17.50\% & 3.15\% \\   

Table 1: Results of different defense strategies on red-team evaluation. ASR% values are reported. Lower ASR% values indicate better defense performance. The colors red, yellow, and blue represent the top three strategies in ranking.

Besides, since the main ideas of our MoGU and Detect\({}_{inp}\) are similar, in that they sense inputs to execute appropriate operations, we also report the performance of Detect\({}_{inp}\) in Tab. 3. Through comprehensive analysis of results across Tab. 1, 2, and 3, we identify three key phenomena.

MoGU keeps robust defense performance.As demonstrated in Tab. 1, our MoGU framework stably enhances the safety of various LLMs during red-team evaluations. Notably, as described in Sec. 3.1, our training data solely comprises original red team malicious instructions, and explicitly excludes any adversarial samples with jailbreak attack prompts. Despite this, our MoGU framework still maintains robust defense performance against various jailbreak attacks as illustrated in Tab. 2.

Existing defense strategies enhance the safety of LLMs but often compromise their usability.As shown in Tab. 2, the ICD strategy significantly increases the defense of Llama2\({}_{7B}\) to jailbreak attacks. However, after applying the ICD strategy, as shown in Tab. 3, the rate of rejection responses to benign instructions on Llama2\({}_{7B}\) surged from 14.00% to 92.25%, and its response usability score dropped dramatically from 3.87 to 2.17. Similarly, as shown in Tab. 2, the SafeDecoding strategy effectively defends Vicuna\({}_{7B}\) against jailbreak attacks. However, as shown in Tab. 3, it leads to a substantial increase in rejection responses from 3.63% to 39.50% and a decline in response usability score from 3.89 to 2.29. Such phenomenons indicate that existing defense strategies often lead LLMs to adopt a rejection-oriented stance, thereby diminishing their usability.

MoGU can enhance LLMs' safety while preserving their usability.As illustrated in Tab. 1 and 2, our framework has exhibited robust defense performance across various LLMs. Importantly, it also maintains the ability to respond with high quality to benign instructions, as evidenced by results in Tab. 3. Under our MoGU framework, the frequency of rejection expressions in LLMs' responses to

    & AutoDAN\(\) & GCG\(\) & PAIR\(\) & SAP30\(\) & Comp\({}_{obj}\)\(\) & AVG\(\) \\ 
**Llama2** & & & & & & \\ No Defense & 1.00 (0.00\%) & 1.80 (8.00\%) & 1.28 (6.00\%) & 1.00 (0.00\%) & 1.01 (0.00\%) & 1.22 (2.80\%) \\ SFT & 1.02 (0.00\%) & 1.70 (12.00\%) & 1.24 (6.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.19 (3.60\%) \\ Detect\({}_{inp}\) & 1.00 (0.00\%) & 1.08 (0.00\%) & 1.18 (6.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.05 (1.20\%) \\ Self-Examine & 1.00 (0.00\%) & 1.16 (6.00\%) & 1.08 (0.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.05 (1.20\%) \\ Retokenization & 1.00 (2.00\%) & 1.00 (2.00\%) & 1.26 (4.00\%) & 1.01 (0.00\%) & 1.01 (2.00\%) & 1.06 (2.00\%) \\ Self-Reminder & 1.20 (2.00\%) & 1.00 (0.00\%) & 1.24 (8.00\%) & 1.00 (0.00\%) & 1.00 (1.00\%) & 1.09 (2.20\%) \\ ICD & 1.00 (0.00\%) & 1.02 (0.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) \\ SafeDecoding & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.16 (4.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.03 (0.80\%) \\ MoGU & 1.00 (0.00\%) & 1.00 (2.00\%) & 1.12 (0.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.03 (0.50\%) \\ 
**Vicuna** & & & & & & \\ No Defense & 4.74 (32.00\%) & 4.86 (62.00\%) & 4.26 (40.00\%) & 4.72 (60.00\%) & 4.79 (39.00\%) & 4.67 (46.60\%) \\ SFT & 4.38 (34.00\%) & 3.74 (44.00\%) & 3.78 (44.00\%) & 2.61 (36.00\%) & 3.43 (19.00\%) & 3.59 (35.40\%) \\ Detect\({}_{inp}\) & 4.70 (32.00\%) & 1.96 (12.00\%) & 4.14 (36.00\%) & 1.00 (0.00\%) & 1.16 (1.00\%) & 2.59 (16.20\%) \\ Self-Examine & 1.04 (0.00\%) & 1.56 (16.00\%) & 1.62 (8.00\%) & 1.04 (1.00\%) & 1.08 (3.00\%) & 1.27 (5.60\%) \\ Retokenization & 1.20 (2.00\%) & 1.32 (26.00\%) & 2.08 (20.00\%) & 1.08 (2.00\%) & 1.37 (19.00\%) & 1.41 (13.80\%) \\ Self-Reminder & 4.74 (24.00\%) & 2.62 (18.00\%) & 2.76 (26.00\%) & 3.47 (49.00\%) & 4.20 (26.00\%) & 3.56 (28.60\%) \\ ICD & 4.64 (26.00\%) & 4.28 (38.00\%) & 3.56 (32.00\%) & 4.66 (70.00\%) & 4.79 (22.00\%) & 4.39 (37.60\%) \\ SafeDecoding & 1.32 (14.00\%) & 1.06 (2.00\%) & 1.38 (8.00\%) & 1.00 (0.00\%) & 2.46 (56.00\%) & 1.44 (16.00\%) \\ MoGU & 1.80 (8.00\%) & 1.20 (4.00\%) & 1.26 (4.00\%) & 1.00 (0.00\%) & 1.00 (0.00\%) & 1.25 (32.00\%) \\ 
**Falcon** & & & & & & \\ No Defense & 3.98 (78.00\%) & 3.64 (72.00\%) & 3.22 (54.00\%) & 3.27 (65.00\%) & 4.38 (84.00\%) & 3.70 (70.60\%) \\ SFT & 3.02 (70.00\%) & 1.22 (16.00\%) & 1.40 (12.00\%) & 1.00 (0.00\%) & 1.18 (8.00\%) & 1.56 (21.20\%) \\ Detect\({}_{inp}\) & 3.66 (78.00\%) & 1.40 (10.00\%) & 3.04 (52.00\%) & 1.00 (0.00\%) & 1.16 (4.00\%) & 2.05 (28.80\%) \\ Self-Examine & 3.24 (62.00\%) & 2.82 (50.00\%) & 3.10 (54.00\%) & 2.77 (49.00\%) & 3.15 (55.00\%) & 3.02 (54.00\%) \\ Retokenization & 1.30 (84.00\%) & 1.70 (54.00\%) & 2.42 (70.00\%) & 3.50 (90.00\%) & 2.01 (43.00\%) & 2.41 (68.20\%) \\ Self-Reminder & 3.40 (92.00\%) & 1.90 (42.00\%) & 2.02 (34.00\%) & 1.04 (3.00\%) & 3.18 (53.00\%) & 2.31 (48.00\%) \\ ICD & 1.18 (0.00\%) & 1.02 (0.00\%) & 1.08 (8.00\%) & 1.01 (0.00\%) & 1.16 (4.00\%) & 1.09 (2.40\%) \\ SafeDecoding & 1.00 (0.00\%) & 1.02 (0.00\%) & 1.00 (4.00\%) & 1.00 (0.00\%) & 1.01 (1.00\%) & 1.01 (1.00\%) \\ MoGU & 1.88 (32.00\%) & 1.20 (4.00\%) & 1.50 (18.00\%) & 1.00 (0.00\%) & 1.06 (1.00\%) & 1.33 (11.00\%) \\   

Table 2: Results of different defense strategies against various jailbreak attacks. GPT score (ASR%) values are reported. Lower GPT score (ASR%) values indicate better defense performance. The colors red, yellow, and blue represent the top three strategies in rankingbenign instructions remains nearly equivalent to that observed in base LLMs. Such phenomenons verify the superiority of our MoGU framework compared to other defense strategies.

## 5 Analysis

In this section, we conducted an ablation experiment, provided a quantitative analysis, and discussed our introduced size of parameters. In App. F and G, we respectively provide a case study and extend our MoGU framework to Baichuan2\({}_{7B}\) and Dolphin\({}_{7B}\) to further demonstrate MoGU's flexibility. Besides, in App. I, we discuss the limitations of our research.

### Ablation Experiment

We analyze the impact of Contrastive Learning Loss (Loss\({}_{CL}\)) in Loss\({}_{glad}\) and Loss\({}_{will}\) and the L1 Norm (L1\({}_{Norm}\)) constraint in Loss\({}_{router}\). Tab. 4 illustrates that omitting Loss\({}_{CL}\) and L1\({}_{Norm}\) will lead to a decrease in the defense performance of our framework. Notably, the impact of L1\({}_{Norm}\) proved to be more significant.

    &  &  \\  & Advbench\(\) & Malicious\(\) & AutoDAN\(\) & GCG\(\) & PAIR\(\) & SAP30\(\) & Comp\({}_{obj}\)\(\) & AVG\(\) \\ 
**Llama2** & & & & & & & & & \\ MoGU & 0.00\% & 0.00\% & 0.00\% & 2.00\% & 0.00\% & 0.00\% & 0.29\% \\ w/o Loss\({}_{CL}\) & 0.00\% & 0.50\% & 0.00\% & 8.00\% & 2.00\% & 0.00\% & 0.00\% & 1.50\% \\ w/o L1\({}_{Norm}\) & 0.00\% & 0.45\% & 0.00\% & 0.00\% & 16.00\% & 14.00\% & 1.00\% & 4.49\% \\ 
**Vicuna** & & & & & & & & \\ MoGU & 0.00\% & 0.50\% & 8.00\% & 4.00\% & 4.00\% & 0.00\% & 0.00\% & 2.36\% \\ w/o Loss\({}_{CL}\) & 0.00\% & 1.50\% & 24.00\% & 14.00\% & 12.00\% & 0.00\% & 16.00\% & 9.64\% \\ w/o L1\({}_{Norm}\) & 4.55\% & 20.00\% & 40.00\% & 60.00\% & 30.00\% & 66.00\% & 13.00\% & 33.36\% \\ 
**Falcon** & & & & & & & & \\ MoGU & 0.91\% & 17.50\% & 32.00\% & 4.00\% & 18.00\% & 0.00\% & 1.00\% & 10.49\% \\ w/o Loss\({}_{CL}\) & 0.91\% & 11.00\% & 10.00\% & 28.00\% & 16.00\% & 1.00\% & 4.00\% & 10.13\% \\ w/o L1\({}_{Norm}\) & 8.19\% & 6.50\% & 76.00\% & 30.00\% & 24.00\% & 5.00\% & 12.00\% & 23.10\% \\   

Table 4: Results of ablation Experiments. Loss\({}_{CL}\) represents Contrastive Learning Loss in Loss\({}_{glad}\) and Loss\({}_{will}\), and L1\({}_{Norm}\) represents the L1 Norm constraint in Loss\({}_{router}\).

    &  &  \\  & Helpfulness\(\) & Clarity\(\) & Factuality\(\) & Depth\(\) & Engagement\(\) & AVG\(\) & \\ 
**Llama2** & & & & & & & \\ No Defense & 3.84 & 4.49 & 3.94 & 3.30 & 3.80 & 3.87 & 14.00\% \\ Detect\({}_{inp}\) & 3.62 & 4.24 & 3.74 & 3.12 & 3.58 & 3.66 & 20.13\% \\ ICD & 1.84 & 2.55 & 2.54 & 1.93 & 1.98 & 2.17 & 92.25\% \\ SafeDecoding & 2.85 & 3.83 & 3.26 & 2.48 & 3.07 & 3.10 & 53.63\% \\ MoGU & 3.83 & 4.48 & 3.94 & 3.31 & 3.78 & 3.87 & 16.50\% \\ 
**Vicuna** & & & & & & & \\ No Defense & 4.19 & 4.60 & 3.95 & 3.26 & 3.43 & 3.89 & 3.63\% \\ Detect\({}_{inp}\) & 3.95 & 4.34 & 3.77 & 3.06 & 3.20 & 3.66 & 10.50\% \\ ICD & 4.15 & 4.51 & 3.99 & 3.19 & 3.39 & 3.85 & 2.13\% \\ SafeDecoding & 2.01 & 3.06 & 2.85 & 1.51 & 2.03 & 2.29 & 39.50\% \\ MoGU & 3.86 & 4.44 & 3.87 & 2.98 & 3.23 & 3.68 & 2.05\% \\ 
**Falcon** & & & & & & & \\ No Defense & 3.14 & 3.94 & 3.23 & 2.15 & 2.69 & 3.03 & 3.13\% \\ Detect\({}_{inp}\) & 3.01 & 3.78 & 3.07 & 2.07 & 2.57 & 2.90 & 10.13\% \\ ICD & 2.75 & 3.65 & 3.12 & 1.95 & 2.38 & 2.77 & 16.88\% \\ SafeDecoding & 1.06 & 1.72 & 1.46 & 1.04 & 1.35 & 1.33 & 97.13\% \\ MoGU & 3.16 & 3.92 & 3.22 & 2.18 & 2.64 & 3.02 & 4.88\% \\   

Table 3: Assessing LLMs’ usability. GPT-Eval scores and probabilities of rejection expressions (Rule-based Eval) are reported. Higher GPT-Eval scores indicate higher quality of responses.

### Quantitative Analysis

To investigate the role of the router, we analyzed the distributions of weights assigned by the router on Llama2\({}_{7B}\), Vicuna\({}_{7B}\), and Falcon\({}_{7B}\). We collected 350 malicious instructions with various jailbreak attack prompts and 800 benign instructions from Just-Eval. The mean values of weights \(w_{unwill}\) and \(w_{glad}\) are calculated during processing each instruction. Fig. 3 presents the boxplot that depicts the statistical results for Vicuna\({}_{7B}\). Notably, during jailbreak attacks, the router assigns a higher weight \(w_{unwill}\) to Unwill\({}_{resp}\), while for benign instructions, it favors a higher weight \(w_{glad}\) for Glad\({}_{resp}\). This allocation pattern aligns perfectly with our expectations of the router's functionality. The same patterns are also observed for Llama2\({}_{7B}\) and Falcon\({}_{7B}\), detailed in App. H.

### Size of Introduced Parameters

In our MoGU framework, we added the LoRA parameters of Glad\({}_{resp}\) and Unwill\({}_{resp}\), and router parameters. In each layer, the number of added parameters can be calculated as (\(d_{model} d_{router} 4+d_{model} 8+d_{model} d_{lora_{r}} 4\)). Taking Llama2\({}_{7B}\) with 32 layers as an example, the total number of added parameters can be calculated as \(273,678,336=(32(4096 512 4+4096 8+4096 8  4))\), accounting for about 3.91% of all parameters.

Furthermore, We investigated the impact of parameter size on the defense performance of LLMs by adjusting the d\({}_{router}\) to 128, 256, 512, and 1024. Our analysis focused on the performance of Llama2\({}_{7B}\), Vicuna\({}_{7B}\), and Falcon\({}_{7B}\) against red-team evaluations and various jailbreak attacks. As shown in Fig. 4, setting d\({}_{router}\) to 512 will consistently result in superior defense performance across all three LLMs. Notably, Llama2\({}_{7B}\) and Vicuna\({}_{7B}\) also exhibited strong defense performance at the lower d\({}_{router}\) settings of 128 and 256. These results suggest that within our framework, the safety of LLMs might be enhanced effectively with fewer parameters.

## 6 Conclusion

In our research, we find the limitations of existing defense strategies, which often sacrifice usability in the pursuit of enhancing LLMs' safety. To address this issue, we introduce our MoGU framework, which designs a dynamic routing mechanism. Our MoGU can improve LLMs' safety while preserving their usability. Our comprehensive evaluations across various LLMs verify our MoGU's superiority compared to other strategies. In the future, we will further refine and optimize the MoGU framework.

Figure 4: We present the results (ASR%) of LLMs under red team evaluations and various jailbreak attacks, with d\({}_{router}\) set at 128, 256, 512, and 1024. The “AVG.” indicates the average defense performance. Lower ASR% values indicate better defense performance.

Figure 3: The distribution of weights assigned by the router of Vicuna\({}_{7B}\).