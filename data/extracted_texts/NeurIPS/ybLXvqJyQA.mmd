# Predicting Ground State Properties:

Constant Sample Complexity and Deep Learning Algorithms

 Marc Wanner

Computer Science and Engineering

Chalmers University of Technology

and University of Gothenburg

wanner@chalmers.se

&Laura Lewis

Applied Mathematics and Theoretical Physics

University of Cambridge

Cambridge, United Kingdom

llewis@alumni.caltech.edu

&Chiranjib Bhattacharyya

Computer Science and Automation

Indian Institute of Science

Bangalore, India

chiru@iisc.ac.in

&Devdatt Dubhashi

Computer Science and Engineering

Chalmers University of Technology

and University of Gothenburg

dubhashi@chalmers.se

&Alexandru Gheorghiu

Computer Science and Engineering

Chalmers University of Technology

and University of Gothenburg

aleghe@chalmers.se

###### Abstract

A fundamental problem in quantum many-body physics is that of finding ground states of local Hamiltonians. A number of recent works gave _provably efficient_ machine learning (ML) algorithms for learning ground states. Specifically, Huang et al. in , introduced an approach for learning properties of the ground state of an \(n\)-qubit gapped local Hamiltonian \(H\) from only \(n^{(1)}\) data points sampled from Hamiltonians in the same phase of matter. This was subsequently improved by Lewis et al. in , to \(( n)\) samples when the geometry of the \(n\)-qubit system is known. In this work, we introduce two approaches that achieve a _constant_ sample complexity, independent of system size \(n\), for learning ground state properties. Our first algorithm consists of a simple modification of the ML model used by Lewis et al. and applies to a property of interest known in advance. Our second algorithm, which applies even if a description of the property is not known, is a deep neural network model. While empirical results showing the performance of neural networks have been demonstrated, to our knowledge, this is the first rigorous sample complexity bound on a neural network model for predicting ground state properties. We also perform numerical experiments on systems of up to \(45\) qubits that confirm the improved scaling of our approach compared to .

## 1 Introduction

One of the most important problems in quantum many-body physics is that of finding ground states of quantum systems. This is due to the fact that the ground state describes the behavior of electronic systems (e.g., metals, magnets, etc.) at room temperature well. Thus, understandingthe ground state can provide insights into, for example, chemical properties of molecules, leading to many potential applications in chemistry and materials science. However, despite extensive research , an efficient classical algorithm solving this problem in full generality remains out of reach. On the other hand, researchers have successfully leveraged classical _machine learning_ (ML) techniques to solve (albeit largely heuristically) the ground state problem and other related quantum many-body problems . Rather than solving these problems directly from first principles, ML algorithms are given some training data collected from physical experiments and are asked to generalize it to new inputs. Intuitively, this additional data can make the problem easier and thus may open the door to obtaining provably efficient classical ML algorithms for finding ground states. This data-driven approach is in some sense necessary, since finding the ground state from the Hamiltonian alone is known to be \(\)-hard in general , and thus out of reach for both efficient classical and quantum algorithms.

In a recent work , Huang et al. proposed the first _provably efficient_ ML algorithm for predicting ground state properties of gapped geometrically local Hamiltonians. In particular, the algorithm in  uses an amount of training data (or _sample complexity_) that scales as \((n^{1/})\), where \(n\) is the system size and \(\) is the prediction error of the ML algorithm. Recently,  improved this guarantee, achieving \(((n)2^{(1/)})\), an _exponential improvement_ with respect to the system size \(n\). The same sample complexity was obtained by  for the task of learning thermal state properties with exponential decay of correlations. Moreover,  extended this to Lindbladian phases of matter  with local rapid mixing, including both ground states of gapped Hamiltonians and thermal states. The work of  obtains a similar guarantee assuming the continuity of quantum states in the parameter range of interest but focusing on the scaling with respect to \(1/\) rather than system size.

These previous works drastically improve the sample complexity of the original Huang et al. result , but none prove sample complexity _lower bounds_ for their respective tasks, leaving open the possibility of further reducing the sample complexity. In addition,  all use fairly simple learning models, i.e., regularized linear regression or taking empirical averages of classical shadows , respectively. With the emergence of neural networks as a popular model in practical ML, one may wonder if these more powerful ML tools may be useful to predict ground state properties as well. In fact, recent works  empirically demonstrate a favorable sample complexity using neural-network-based ML algorithms. However, there are currently no rigorous theoretical guarantees regarding the amount of training data needed to achieve a desired prediction error. These remarks lead us to the following two central questions of this work.

**Question 1**.: _Can classical ML algorithms predict ground state properties with even less than \(((n)2^{(1/)})\) data?_

This is especially relevant for systems approaching the thermodynamic limit, where the system size can be arbitrarily large. Needing fewer samples also means less work for experimentally preparing ground states of the system. The second question, stated as an open question in  is:

**Question 2**.: _Can we obtain rigorous sample complexity guarantees for neural-network-based ML algorithms for predicting ground state properties?_

### Our results

We give positive answers to both questions. We consider the same assumptions as  with minimal additional ones that we mention here. First, we show that a simple modification to the approach in  allows us to achieve a sample complexity that is _independent of the system size_. This does, however require knowledge of the property we wish to predict in advance, whereas this is not a requirement in . We view this as a reasonable assumption, since in practice we can imagine preparing ground states of some system in order to measure a specific property of interest. We show the following theorem, stated informally here. The formal statement, including all the assumptions required for proving the result, can be found in Appendix B.

**Theorem 1** (Informal).: _Let \(H(x)\) be an \(n\)-qubit gapped, geometrically local Hamiltonian with ground state \((x)\). Given an observable \(O\), with a known decomposition as a sum of local Pauli operators and given training data \(\{(x_{},y_{})\}_{=1}^{N}\) sampled from an arbitrary distribution, with \(y_{}(O(x_{})),\)there is an ML algorithm for predicting ground state properties \((O(x))\) to within precision \(>0\) using \(N=(2^{(1/)})\) training samples._

Note that the number of samples \(N\) depends only on the desired prediction error \(\) and is independent of the system size. In particular, this means that for a fixed prediction error our algorithm requires only a _constant_ amount of training data. Moreover, the computational complexity of our algorithm improves upon , having \((n)\) runtime, compared to the previous \((n n)\). While removing the \( n\) factor may seem like a small improvement, in practice this can make a significant difference. For instance, for a system of \(n 1000\) qubits, removing the \( n\) factor would result in a ten-fold reduction in training data and time.

Much like in , this result also extends to learning classical representations of \((x)\). In other words, if the algorithm is instead given _classical shadows_ of the ground state as training data, it can then predict a classical representation of \((x)\) for new parameters \(x\). This can mitigate the requirement that the observable is known in Theorem 1, as predicting properties from a classical representation clearly requires knowledge of the observable.

Our second result shows the same sample complexity guarantee for a _neural network_ ML algorithm (Figure 1) , in which one does not need to know the observable being measured in advance. An additional constraint that we require in this case is that the training data is not sampled according to an arbitrary distribution, but a distribution satisfying some technical assumptions. We note that these assumptions are satisfied for common distributions such as uniform and Gaussian.

**Theorem 2** (Informal).: _Let \(H(x)\) be an \(n\)-qubit gapped, geometrically local Hamiltonian with ground state \((x)\). For any observable \(O\), expressible as a sum of local Pauli operators and given training data \(\{(x_{},y_{})\}_{=1}^{N}\), sampled from a distribution satisfying certain assumptions with \(y_{}(O(x_{}))\), there is a neural network ML algorithm for predicting ground state properties \((O(x))\), for uniform \(x\), to within precision \(>0\) using \(N=(2^{(1/)})\) training samples under mild assumptions on training._

We prove this result by making use of the Koksma-Hlawka inequality from quasi-Monte Carlo theory  and combining it with the spectral flow formalism .

Similar to Theorem 1, we can also extend this result to learning classical representations of \((x)\) when given classical shadow training data. The formal statement and its proof can be found in Appendix C. We also remark that, much like the setting in , a more favorable scaling with respect to \(\) can be achieved if the number of parameters that the Hamiltonian depends on is constant. In particular, it was shown in  that if the number of parameters is constant, the sample complexity scales

Figure 1: **A deep network model for predicting ground state properties. Given a vector \(x[-1,1]^{m}\) that parameterizes a quantum many-body Hamiltonian \(H(x)\), the algorithm uses geometric structure to create “local” neural network models \(f_{P_{i}}^{_{P_{i}}}\). The ML algorithm then combines the outputs of these local models to predict a property \((O(x))\), where \((x)\) is the ground state of \(H(x)\). Here, we decompose \(O=_{i=1}^{M}_{P_{i}}P_{i}\) for Pauli operators \(P_{i}\), where the final layer takes a linear combination of the outputs of the local models weighted by some trainable parameters \(w_{P_{i}}\) that intuitively should approximate the Pauli coefficients \(_{P_{i}}\).**

as \(N=(1/,(n))\). For our results, this similarly yields \(N=(1/)\), preserving the independence on the system size while also achieving a polynomial scaling in \(1/\).

Furthermore, we perform numerical experiments on system sizes of up to 45 qubits, which support our theoretical findings, and show that, in practice, our deep learning algorithm outperforms previous methods . We describe them in detail in Appendix D, and they are illustrated in Figure 2.

## 2 Preliminaries

### Problem statement

First, we formally describe the problem setting, which is the same as . We consider a family of \(n\)-qubit Hamiltonians \(H(x)\) smoothly parameterized by an \(m\)-dimensional vector \(x[-1,1]^{m}\). We assume that these Hamiltonians are gapped for all choices of parameters \(x[-1,1]^{m}\) and geometrically local such that they can be written as a sum of local terms

\[H(x)=_{j=1}^{L}h_{j}(_{j}),\] (2.1)

where the parameter vector \(x\) is a concatenation of the constant-dimensional vectors \(_{1},,_{L}\). Each of these constant-dimensional vectors \(_{j}\) parameterizes the local interaction term \(h_{j}(_{j})\). Crucially, we assume that each local term \(h_{j}\) only depends on a constant number of parameters rather than the entire parameter vector \(x\). We also assume that the geometry of the \(n\)-qubit system is known.

Throughout this work, we use \((x)\) to denote the ground state of the Hamiltonian \(H(x)\) and \(O\) to denote an observable that can be written as a sum of geometrically local observables with bounded spectral norm \( O_{} 1\). Here, the ground states \((x)\) form a gapped quantum phase of matter. Given samples of quantum states drawn from this phase, we wish to predict expectation values of observables \(O\) with respect to other states in the same phase. In other words, we are given training data \(\{(x_{},y_{})\}_{=1}^{N}\), where \(y_{}(O(x_{}))\) approximates the ground state property for a parameter choice \(x_{}[-1,1]^{m}\) sampled from some distribution \(\) over the parameter space. We aim to learn a function \(h^{*}(x)\) that approximates the ground state property \((O(x))\) for some unseen parameter \(x\) while minimizing the amount of training data, or sample complexity, \(N\). How well we learn the ground state property is quantified by the average prediction error

\[*{}_{x}|h^{*}(x)-(O (x))|^{2}.\] (2.2)

We describe the precise conditions under which Theorems 1 and 2 hold in the following sections.

### Review of previous algorithm

In this section, we review the previous algorithm from , as our proofs rely on similar ideas. For full details, we refer the reader to  and our more detailed presentation in Appendix A.1. The ML algorithm proposed in  requires some geometric definitions. Fix a geometrically local Pauli observable \(P\{I,X,Y,Z\}^{ n}\).

Let \(_{1},B>0\) be efficiently-computable hyperparameters that we define in Appendix A.1. Define the set \(I_{P}\) of coordinates \(c\) such that \(x_{c}\) parameterizes some local term \(h_{j(c)}\) that is close to the Pauli \(P\). Here, the distance between two observables \(d_{}\) is defined as the minimum distance between the qubits that the observables act on, where the distance between qubits is given by the geometry of the system, which we assume to be known. Formally, we define this set of local coordinates as

\[I_{P}\{c\{1,,m\}:d_{}(h_{j(c)},P)_{1}\},\] (2.3)

where \(h_{j(c)}\) is the local term in the Hamiltonian \(H(x)\) whose parameters \(_{j(c)}\) include the variable \(x_{c}\). The intuition behind this set of coordinates is that it indexes the parameters \(x_{c}\) that influence the ground state property \((P(x))\) corresponding to the Pauli \(P\). The algorithm consists of two steps. First, it maps the parameter space \([-1,1]^{m}\) to a high dimensional space via a nonlinear feature map \(\). Second, it runs \(_{1}\)-regularized linear regression (LASSO) [62; 63; 64] over the feature space.

This first step encodes the geometry of the problem. The feature map intuitively projects a given parameter vector \(x\) onto the local parameter space \(\{x_{c}:c I_{P}\}\). We define this precisely in Appendix A.1. Following the feature mapping, the ML algorithm uses LASSO [62; 63; 64] to learn functions of the form \(\{h(x)=(x):\|\|_{1} B\}\) for a chosen hyperparameter \(B>0\). We denote the learned function by \(h^{*}(x)=^{*}(x)\). For our purposes, we set \(B=2^{((1/_{1}))}\). This algorithm obtains the following rigorous guarantee.

**Theorem 3** (Theorem 1 in ).: _Given \(n,>0\), \(>>0\) and a training data set \(\{(x_{},y_{})\}_{=1}^{N}\) of size_

\[N=(n/)2^{(1/)},\] (2.4)

_where \(x_{}\) is sampled from an unknown distribution \(\) and \(|y_{}-(O(x_{}))|\). With a proper choice of the efficiently computable hyperparameters \(_{1},_{2}\), and \(B\), the learned function \(h^{*}(x)=^{*}(x)\) satisfies_

\[}_{x}|h^{*}(x)-(O(x)) |^{2}\] (2.5)

_with probability at least \(1-\). The training and prediction time of the classical ML model are bounded by \((nN)=n(n/)2^{(1/)}\)._

A crucial step in the proof is that ground state properties can indeed be approximated by linear functions over the feature space. Along the way,  proves that the ground state property can be approximated by a linear combination of "local functions," which are local in that they only depend on parameters with coordinates in the set \(I_{P}\). We relegate further details to Appendix A.1 and .

## 3 Main results

In this section, we discuss our rigorous guarantees for predicting ground state properties with constant sample complexity and with neural-network-based ML algorithms.

### Constant sample complexity

In this section, we show that a simple modification of the algorithm from  can achieve a sample complexity that is independent of the system size \(n\), under the additional assumption that the observable \(O\) is known. In practice, a scientist often has a specific ground state property in mind that they wish to study, so we view this as a natural assumption. Moreover, this is still an interesting learning problem, as when obtaining the training data via quantum experiments, preparing the ground state \((x)\) in the laboratory for a new choice of parameters \(x\) may be difficult experimentally. This in turn means that accurately predicting some property \((O(x))\) for a new choice of \(x\) may be challenging, even if the property of interest, \(O\), is known. ML algorithms can allow us to circumvent this issue and generalize from the results of few training data points without needing to prepare the ground state directly.

Let \(O=_{P\{I,X,Y,Z\}^{ n}}_{P}P\) be an observable that can be written as a sum of geometrically local observables. Because \(O\) is assumed to be known, we can find this decomposition of \(O\) in terms of the Pauli observables \(P\). The overall structure of the algorithm remains the same: perform a nonlinear feature mapping followed by linear regression. However, there are two key differences from the previous algorithm . First, we change the feature mapping of  to incorporate the Pauli coefficients \(_{P}\). We define this new feature mapping \(\) in Appendix B. The second difference from  is that we use ridge regression [65; 66] instead of LASSO [62; 63; 66]. Recall that LASSO learns hypothesis functions of the form \(\{h(x)=(x):\|\|_{1} B\}\) for some hyperparameter \(B>0\). In contrast, ridge regression replaces the \(_{1}\)-norm constraint \(\|\|_{1} B\) with an \(_{2}\)-norm constraint: \(\|\|_{2}\), for some hyperparameter \(>0\). Namely, for a chosen efficiently-computable hyperparameter \(>0\), ridge regression finds a vector \(^{*}\) that minimizes the training error subject to the constraint that \(\|\|_{2}\), i.e.,

\[_{^{n_{}}\\ \|\|_{2}}_{ =1}^{N}|(x_{})-y_{}|^{2}.\] (3.1)

Standard results in machine learning theory give sample complexity upper bounds for ridge regression in terms of \(\) and the \(_{2}\)-norm of the feature vector \((x)\)[65; 66]. The key idea is that with our new feature mapping, we can still approximate the ground state property by a linear function over the feature space, as in , to obtain a low training error. Meanwhile, by incorporating the Pauli coefficients \(_{P}\) into the feature map, we can bound the \(_{2}\)-norm of \((x)\) by a quantity independent of system size, leveraging bounds on the \(_{1}\)-norm of the Pauli coefficients . We note that naively applying ridge regression with the feature map from  does not achieve the same guarantees and in fact gives worse scaling than . Similarly, we can also choose a suitable \(>0\) independent of system size. Thus, we obtain the following guarantee.

**Theorem 4** (Constant sample complexity).: _Given \(n,>0\), \(1/e>>0\) and a training data set \(\{(x_{},y_{})\}_{=1}^{N}\) of size_

\[N=(1/)2^{(1/)},\] (3.2)

_where \(x_{}\) is sampled from an unknown distribution \(\) and \(|y_{}-(O(x_{}))|\). With a proper choice of the efficiently computable hyperparameters \(_{1},_{2},\), the learned function \(h^{*}(x)\) satisfies_

\[}_{x}|h^{*}(x)-(O(x))|^{2}\] (3.3)

_with probability least \(1-\). The training and prediction time of the classical ML model are bounded by \((n)(1/)2^{(1/)}\)._

We compare this result to Theorem 3. For a constant prediction error \(=(1)\), our proposed algorithm achieves a constant sample complexity \(N=(1)\), compared to the logarithmic sample complexity \(N=( n)\) of . Moreover, we also improve the computational complexity, achieving a linear-in-\(n\) runtime, compared to the previous \((n n)\). The scaling with respect to the prediction error \(\) is the same as the previous algorithm . This means that regardless of how large our quantum system is, we need the same amount of samples to predict ground state properties well. This is especially important for settings in which obtaining training data for large systems is difficult.

Thus far, we have only considered the setting in which we learn a specific ground state property \((O(x))\) for a fixed observable \(O\). Because our training data is given in the form \(\{(x_{},y_{})\}_{=1}^{N}\), where \(y_{}\) approximates \((O(x))\) for this fixed observable \(O\), if we want to predict a new property for the same ground state \((x)\), we would need to generate new training data. Thus, it may be more useful to learn a ground state representation, from which we could predict \((O(x))\) for many different choices of observables \(O\) without requiring new training data. In this case, suppose we are instead given training data \(\{x_{},_{T}((x_{}))\}_{=1}^{N}\), where \(_{T}((x_{}))\) is a classical shadow representation  of the ground state \((x_{})\). An immediate corollary of Theorem 4 is that we can predict ground state representations with the same sample complexity. This follows from the same proof as Corollary 5 in .

**Corollary 1** (Learning representations of ground states).: _Let \(n,>0\), \(1/e>>0\) and \(>0\). Given training data \(\{(x_{},_{T}((x_{}))\}_{=1}^{N}\) of size_

\[N=(1/)2^{((1/))},\] (3.4)

_where \(x_{}\) is sampled from \(\) and \(_{T}((x_{})\) is the classical shadow representation of the ground state \((x_{})\) using \(T\) randomized Pauli measurements. For \(T=}((n/)/^{2})\), with probability at least \(1-\), the ML algorithm will produce a ground state representation \(_{N,T}(x)\) that achieves_

\[}_{x}|\,(O_{N,T}(x))- (O(x))|^{2}\] (3.5)

_for any observable with \( O_{} 1\) that can be written as a sum of geometrically local observables._

### Rigorous guarantees for neural networks

In this section, we prove the existence of a deep neural network model that can predict ground state properties using a constant number of training samples. In particular, we prove that after training on a constant number of samples from a distribution \(\) on \([-1,1]^{m}\) satisfying certain technical assumptions, our model can achieve a low prediction error under mild assumptions on training. In this case, for predicting properties \((O(x))\), the observable \(O\) need not be known in advance. However, we need to assume that all mixed first order derivatives of the Hamiltonian \(^{m}H(x)/ x_{1} x_{m}_{ } 1\) exist and are bounded. This is not much stronger than , which assumes that directional derivatives \( h_{j}/\) are bounded by one for any direction \(\). Moreover, we also need the training data to be sampled from a distribution \(\) with probability density function \(g\) satisfying the following assumptions: \(g\) has full support and is continuously differentiable on \([-1,1]^{m}\). Also, \(g\) is of the form \(g(x)=_{j=1}^{L}g_{j}(_{j})\). This resembles our assumption on the form of the Hamiltonian \(H(x)\). Furthermore, the average prediction error is measured with respect to the same distribution \(\). We note that these assumptions are satisfied for common distributions such as uniform and Gaussian.

As in the previous algorithm , we leverage the geometry of the \(n\)-qubit system to approximate the ground state properties by a linear combination of smooth local functions, which only depend on parameters with coordinates in the local coordinate set \(I_{P}\) defined in Equation (2.3). Crucially, the size \(|I_{P}|\) of the domains of these local functions is independent of the system size.

Instead of using a feature map and linear regression to learn the ground state properties, we utilize a deep neural network model defined as follows. Inspired by the local approximation of ground state properties, we define "local models" \(f_{P}^{_{P}}:[-1,1]^{}\), which are neural networks consisting of three layers of affine transformations and applications of a nonlinear activation function. In particular, \(f_{P}^{_{P}}\) has two hidden layers with the affine transformations given by the trainable weights and biases denoted by \(_{P}\). We take hyperbolic tangent, \(\), as the activation function. These local models are then combined into a model \(f^{,w}:[-1,1]^{m}\) given by

\[f^{,w}(x)=_{P S^{}}w_{P}f_{P}^{_{P}}(x),\] (3.6)

where \(w_{P}\) are the weights in the last layer and \(=\{_{P}\}_{P S^{}}\). This model is schematically illustrated in Figure 1. We refer to Definition 6 in Appendix C for a full description of the model.

Consider training data \(\{(x_{},y_{})\}_{=1}^{N}\), where \(x_{}\) are sampled according to a distribution \(\) satisfying the assumptions described above and \(|y_{}-(O(x_{}))|\). The ML algorithm first initializes the weights via standard deep learning initialization procedures, e.g., Xavier initialization . Then, the algorithm performs quasi-Monte Carlo training given the training data, e.g., Adam , to find weights \(^{*}\), \(w^{*}\) which minimize the training objective function

\[_{=1}^{N}|f^{,w}(x_{})-y_{}|^{2}+\|w \|_{1},\] (3.7)

where \(\) is some regularization parameter that may depend on \(\). For this algorithm, we prove the following theorem bounding the average prediction error of our deep neural network model.

**Theorem 5** (Neural network sample complexity guarantee).: _Let \(1/e>>0\). Let \(\) be a distribution with probability density function \(g\) satisfying the properties stated above. Let \(f^{^{*},w^{*}}:[-1,1]^{m}\) be a neural network model trained on data \(\{(x_{},y_{})\}_{=1}^{N}\) of size_

\[N=((1/)2^{(1/)}),\] (3.8)

_where the \(x_{}\)'s are sampled from \(\) and \(|y_{}-(O(x_{}))|\). Suppose that \(f^{^{*},w^{*}}\) achieves a value no larger than \(()\) on the training objective (Equation (3.7)) with \(()=()\). Additionally, suppose that all parameters \(_{i}^{*}\) of \(f^{^{*},w^{*}}\) satisfy \(|_{i}^{*}| W_{}\), for some \(W_{max}>0\) that is independent of \(n\). Then_

\[*{}_{x}|f^{^{*},w^{*}}(x)- (O(x))|^{2}.\] (3.9)

Similar to Theorem 4, for a constant prediction error \(=(1)\), the deep neural network algorithm achieves constant sample complexity \(N=(1)\). In contrast to Theorem 4, we do not require knowledge about the observable, \(O\). This is a direct consequence of the regularity of \(w_{P}\), which is achieved when the training objective is small. Theorem 6 guarantees, that a model with such regularity can yield a small prediction error.

There are, however, some caveats compared to the previous result. First, the training data is restricted to being sampled from a distribution satisfying our technical assumptions stated previously, in contrast to Theorem 4 which holds for data sampled from any arbitrary unknown distribution. Second, in regards to the model, the weights must be bounded by a constant \(W_{}\). Finally, we cannot guarantee _a priori_ that the network will indeed achieve a low training error. This is due to the fact that our training objective is non-convex and thus, globally optimal weights cannot be found efficiently in general . Even so, we are still able to prove the existence of suitable weights such that the resulting network approximates \((O(x))\) for any \(x[-1,1]^{m}\) (see Theorem 6 in the next section).

However, we view the assumptions made in Theorem 5 as being mild in practice. Small training objectives are commonly achieved in deep learning so we expect our training algorithm to produce a model which fulfills the assumptions of Theorem 5 after \((1)\) training steps and \((n)\) runtime. Moreover, it is known that gradient descent provably converges to the global optimum for _overparametrized_ deep neural networks, while the weights remain small, when properly initialized . We verify that these conditions are satisfied in practice through our numerical experiments in Figure 2 and 1. To our knowledge, Theorem 5 is the first rigorous sample complexity bound on a neural network model for predicting ground state properties.

We also note that if the training data is instead sampled according to a _low-discrepancy sequence_ (LDS) , we can obtain better guarantees, but these improvements are hidden in the polylogarithmic factors in the exponential. We discuss learning given data from a LDS in Appendix C. Intuitively, a LDS is a collection of points in the parameter space that covers the space such that there are no large gaps, or discrepancies.

Similar to Corollary 1, if we are instead given training data \(\{x_{},_{T}((x_{}))\}_{=1}^{N}\), where \(_{T}((x_{}))\) is a classical shadow representation  of the ground state \((x_{})\), then we obtain the following immediate corollary of Theorem 5.

**Corollary 2** (Learning representations of ground states with neural networks).: _Let \(n,>0\), \(1/e>>0\) and \(>0\). Given training data \(\{(x_{},_{T}((x_{}))\}_{=1}^{N}\) of size_

\[N=((1/)2^{(1/)}),\] (3.10)

_where \(x_{}\) is sampled from a distribution \(\) satisfying the same assumptions as Theorem 5 and \(_{T}((x_{})\) is the classical shadow representation of the ground state \((x_{})\) using \(T\) randomized Pauli measurements. For \(T=}((n/)/^{2})\), with probability at least \(1-\), the ML algorithm will produce a ground state representation \(_{N,T}(x)\) that achieves_

\[*{}_{x}|(O_ {N,T}(x))-(O(x))|^{2}\] (3.11)

_for any observable with \( O_{} 1\) that can be written as a sum of geometrically local observables._

#### 3.2.1 Proof ideas for neural network guarantee

To prove Theorem 5, we first show that our neural network model \(f^{,w}\) can approximate the ground state properties well. In particular, we show that there exist weights \(^{},w^{}\) such that \(f^{^{},w^{}}\) approximates the ground state properties and thus achieves small value for the training objective (Equation (3.7)). Then, we bound the prediction error using tools from deep learning and quasi-Monte Carlo theory . We ensure the existence of \(f^{^{},w^{}}\) in the following theorem.

**Theorem 6**.: _For any \(1/e>>0\) and width \(W\), there exist weights \(^{},w^{}\) such that the neural network model \(f^{^{},w^{}}\) satisfies_

\[|f^{^{},w^{}}(x)-(O(x))|^{2} , x[-1,1]^{m}.\] (3.12)

_Moreover, each parameter \(_{i}\) of the network has a magnitude of \(|_{i}|=2^{((1/))}\)._

This implies that for a suitable choice of regularization parameter \(=()\), the training objective from Equation (3.7) is also small. We prove this statement by combining results in deep learning regarding \(\) neural networks approximating functions  with the geometric locality of the system and smoothness of the ground state properties. We note that the weights \(^{},w^{}\) in Theorem 6 are not necessarily the weights \(^{*},w^{*}\) found via the neural network training procedure in Theorem 5. Because the training objective is non-convex, we cannot guarantee convergence to these weights \(^{},w^{}\). However, assuming that \(f^{^{*},w^{*}}\) does indeed achieve a low training error (which is often satisfied in practice), we are able to rigorously guarantee that the model will generalize well and achieve a low prediction error in Theorem 5.

Notice that the guarantee of Theorem 6 holds for all \(x\) and, in particular, does not require our assumptions on the distribution \(\). The assumption that the network is trained on such data only becomes relevant when bounding the prediction error. While not explicitly stated here, we also note that Theorem 6 gives a bound on the number of trainable parameters \(|_{i}|\) that has a similar dependence on \(\) as the model in . Furthermore, the parameters are independent of system size, \(n\). Additional smoothness assumptions on the Hamiltonian \(H(x)\) can yield mild improvements on the dependence in terms of \(\), as briefly discussed in Appendix C.1. Moreover, because of this bound on \(|_{i}|\), applying an additional penalty on the \(_{2}\)-norm of the weights \(\) can help ensure that the weights remain small. In practice, this is usually satisfied during training when the weights are initialized properly and the inputs are regularized, e.g. . Thus, the condition that \(|_{i}^{*}| W_{}\) is often satisfied in practice and is not considered a strong assumption in deep learning.

To prove the prediction error bound in Theorem 5 assuming that a low training error is achieved, we combine techniques from quasi-Monte Carlo theory applied to deep learning  (see Appendix A.2 for a review) along with our knowledge of the geometry of the \(n\)-qubit system. In contrast to , we need to characterize the dimension of the input domain in our approach. The reason for doing this is that the approximation error depends on the size \(=|I_{P}|\) (Equation (2.3)) of our local models \(f_{P}^{_{P}}\).

The central result we use here is the Koksma-Hlawka inequality  (see Theorem 10 in Appendix A.2) from quasi-Monte Carlo theory. This produces a bound on the prediction error in terms of the star-discrepancy (see Definition 2 in Appendix A.2) and the Hardy-Krause variation. The star-discrepancy can be controlled by known bounds on the star-discrepancy of random points . We bound the Hardy-Krause variation by explicitly computing the mixed derivatives of the local models \(f_{P}^{_{P}}\) and the ground state properties \((O(x))\). In particular, we bound the latter using tools from the spectral flow formalism [59; 60; 61], and this is where the assumption that the mixed first order derivatives of the Hamiltonian are bounded is needed. Putting these steps together, we arrive at the rigorous guarantee in Theorem 5.

## 4 Numerical experiments

We conduct numerical experiments to observe the performance of our model in practice. The results demonstrate that our assumptions in Theorem 5 are often satisfied in practice and that our deep learning algorithm outperforms the previous best-known method . Moreover, we generate and utilize significantly more training data than in prior works [1; 2]. The code can be found at https://github.com/marcwannerchalmers/learning_ground_states.git.

We consider the classical neural network model discussed in the previous section and defined formally in Definition 6. For each of the local models \(f_{P}^{_{P}}\), we use fully connected deep neural networks with five hidden layers of width \(200\). We train the model with the AdamW optimization algorithm . We measure the training error and prediction error via the root-mean-square error (RMSE). The model is discussed further in Appendix D.

As in , we consider the two-dimensional antiferromagnetic random Heisenberg model on between \(20\) to \(45\) qubits and predict two-body correlation functions. The corresponding Hamiltonian is

\[H=_{ ij}J_{ij}(X_{i}X_{j}+Y_{i}Y_{j}+Z_{i}Z_{j}),\] (4.1)

where \( ij\) denotes all pairs of neighboring sites on the lattice. The coupling terms \(J_{ij}\) correspond to the parameters \(x\) of the Hamiltonian and are sampled uniformly from \(\).

We generate training data similarly to [1; 2], using the density-matrix renormalization group (DMRG)  based on matrix-product-states (MPS) . To assess the performance of our model, we consider both uniformly randomly distributed \(J_{ij}\) and coupling parameters, which are distributed as a Sobol sequence. It is easy to see that the distributions and \(H\) satisfy the requirements of Theorem 5.

In Figure 2 (Left), we see that our deep learning algorithm consistently outperforms the previous best-known ML algorithm from , achieving approximately half the prediction error on the same training data. The prediction error also exhibits a constant scaling with respect to system size, agreeing with our rigorous guarantee in Theorem 5. Another noteworthy observation is that the ML algorithm's performance on LDS is nearly equivalent to its performance on uniformly random points. We discuss a potential reason for this in Appendix D.

Figure 2 (Center) illustrates the prediction error scaling with respect to the training set size for various choices of \(_{1}\) (size of the local neighborhood from Equation (2.3)). For \(_{1}=0\), the error arising from approximating the ground state property via local functions dominates. For \(_{1}>0\), we observe a smaller local approximation error and thus achieve a smaller prediction error for sufficiently large training sets. This is consistent with our theoretical results.

Finally, Figure 2 (Right) illustrates that our assumptions in Theorem 5 are mild in practice. Namely, the blue points show that a small training error can be achieved. The red points also demonstrate that the \(_{1}\)-norm of the parameters in the last layer and the largest absolute value of the parmeters in the trained neural network remain small. In particular, in Figure 2, the weights exhibit a scaling _independent_ of system size \(n\). Hence, we find that the assumptions needed to guarantee the prediction error bound in Theorem 5, namely that the training objective is small and the weights of the neural network are small and independent of system size, are fulfilled in our numerical experiments. We provide further details of the numerical experiments in Appendix D.

## 5 Discussion

We have shown that we can construct ML models for predicting ground state properties that require only a constant number of training samples, for a fixed prediction error. Specifically, we showed that a simple modification to the linear regression model in  only requires \(2^{(^{-1})}\) samples in order to achieve a prediction error of \(\), provided that we know a decomposition of the observable of interest in terms of Pauli operators. We then showed that a neural network model which is trained on \(2^{(^{-1})}\) training samples and which achieves \(()\) training error on these samples will also have a prediction error of at most \(\). In this case, knowledge of the observable \(O\) is no longer required.

Our work leaves open several avenues for future exploration. First, it would be desirable to understand the conditions under which we can prove convergence for the training error. For instance, could the model be changed so as to use a convex objective, thereby avoiding the issues associated with finding a global optimum in a non-convex landscape? Following [49; 50], we would also like to know whether the results obtained for neural networks can be extended to thermal states or Lindbladian phases of matter. Finally, for both results it would be desirable to improve the scaling with respect to the error \(\). Currently, the models have quasipolynomial scaling in \(1/\) and the only case in which we know how to achieve \((1/)\) scaling is when the number of parameters, \(m,\) is constant (as in ).

Figure 2: **Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of , fixing the training set size \(N=3686\) and the size of the local neighborhood \(_{1}=0\) (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \(_{1}\) and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the \(_{1}\) norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing \(N=3686\) and \(_{1}=1\). This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.**