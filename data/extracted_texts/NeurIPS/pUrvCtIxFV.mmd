# Keep on Swimming: Real Attackers Only Need Partial Knowledge of a Multi-Model System

Julian Collado

HiddenLayer Inc.

jcollado@hiddenlayer.com &Kevin Stangl

HiddenLayer Inc.

kstangl@hiddenlayer.com

Primary and corresponding Author

###### Abstract

Recent approaches in machine learning often solve a task using a composition of multiple models or agentic architectures. When targeting a composed system with adversarial attacks, it might not be computationally or informationally feasible to train an end-to-end proxy model or a proxy model for every component of the system. We introduce a method to craft an adversarial attack against the overall multi-model system when we only have a proxy model for the final black-box model, and when the transformation applied by the initial models can make the adversarial perturbations ineffective. Current methods handle this by applying many copies of the first model/transformation to an input and then re-use a standard adversarial attack by averaging gradients, or learning a proxy model for both stages. To our knowledge, this is the first attack specifically designed for this threat model and our method has a substantially higher attack success rate (80% vs 25%) and contains 9.4% smaller perturbations (MSE) compared to prior state-of-the-art methods. Our experiments focus on a supervised image pipeline, but we are confident the attack will generalize to other multi-model settings [e.g. a mix of open/closed source foundation models], or agentic systems.

## 1 Introduction

Recent AI research has shown the effectiveness of agentic architectures and systems composed of multiple models that decompose problems and create scaffolds in a solution pipeline. Alternatively consider an initial model doing a complex pre-processing step for a second model, for example a foundation model that processes the input and passes its output to another model for a classification or some other task. In production systems, a service is often a pipeline of multiple pre/post-processing steps based on heuristics and machine learning models. Combining models this way has proven to be very effective and will likely increase over time with the rise of multi-agent systems.

The proliferation of real world AI systems and the horizon of ever more powerful methods has made securing these models against malicious or un-authorized use ever-more urgent. Model providers have responded to these security threats by implementing a mix of a) including safety fine-tuning  b) weaker side-car models that halt the model from responding based on detecting malicious queries or harmful outputs  c) closed model weights to prevent an attacker from developing white-box attacks  d) rate-limiting the users of a centrally served model to avoid black-box attacks .

However, the conmingling of multiple models, of closed/open source, introduces new security vulnerabilities that are not precisely captured by existing threat models and complicates defense based on keeping the weights hidden or rate limiting the user to avoid the creation of proxy models.

_We will show how to attack a system of models even when an adversary has restricted access to part of this system such that they cannot create a proxy for the first models/components of the system._

White-Box attacks  assume perfect knowledge of the model weights, allowing gradient based optimization techniques to find adversarial perturbations. Black-Box attacks [14; 32; 23] achieve a similar effect to the White-Box attacks but without having access to the weights, instead the attackers can only query the model with different inputs but may have varying degrees or knowledge about the model architecture, biases and other parameters. Black-box attacks either typically train a proxy model or estimate local gradients to find perturbations for specific inputs. Grey-Box attacks are similar to Black-Box attacks; one Grey-Box model could consist of White-Box access to part of a system and Black-Box access to another component. For example in an encoder-decoder architecture, the attacker may have White-Box access to the encoder and Black-Box access to the decoder.

### Threat Model: Multi-Model System Attack With Partial Proxy Access

We introduce a new and realistic threat model for multi-agentic and multi-model applications that we first test in a vision modality.

In the simplest case, consider a system that is a composition of two models, e.g. \(h_{1}\) and \(h_{2}\), so the overall output is \(=h_{2}(h_{1}(x))\). Specifically, we have black-box access to both models but it is only feasible2 to create a proxy model for \(h_{2}\) as shown in Figure 1. The proxy model for \(h_{2}\) allows us to perform gradient based attacks against \(h_{2}\), so we can compute a \(_{adv}\) such that \(h_{2}(x_{mod}+_{adv}) y_{pred}\) where \(x_{mod}=h_{1}(x)\) and \(y_{pred}\) is the predicted label of \(x\). In the rest of the paper, we refer to \(x_{mod}\) as the output of model \(h_{1}\). _The key difficulty in this scenario is that the transformation applied by \(h_{1}\) might destroy the adversarial modification such that \(h_{1}(x+_{adv}) x_{mod}+_{adv}\) and therefore \(h_{2}(h_{1}(x+_{adv}))=y_{pred}\)._

_We focus on the case when the modifications applied by the \(h_{1}\) are reversible_ in the sense that \(x_{mod}\), and \(x_{mod}+_{adv}\), can be "re-inserted" into \(x\). Consider the case where \(h_{1}\) is a segmentation model that detects a region of interest and crops the image and we have designed an adversarial perturbation attacking the cropped subset of the full image. That adversarial perturbation could be re-inserted into the original image inside the crop box. Formally, \(h_{1}:\) and \(h_{2}:\), for some input modality \(\). This allows us to "re-insert" the adversarial sample \(x_{mod}+_{adv}\) crafted for \(h_{2}\) into \(x\) to create an adversarial sample for the whole system. Another example of a pair of models that satisfies this property could be a pair of natural language models, where the first model processes a piece of text, generating a new text string, that is then handed off to the second model for the final computation.

### Our Contributions

To our knowledge, we propose the first attack specifically for a multi-model compositional problem where a proxy is only available for the last model. We observe that this is a more realistic scenario for industrial applications where it might not be feasible to create a proxy for each section of the system or where an adversary might not have access to information about the first sections of the system, for example the pre-processing of the data or an adversarial defense, but the last leg of the system might be approximated with an open source model or have been trained in a public dataset.

We provide an iterative method, which we name the _Keep on Swimming Attack_ (_KoS_, pronounced chaos) to ensure that the attack survives the modifications applied by the non-proxy-able sections of the system, and show our attack has a higher success rate and lower noise levels than the natural baseline method, based on Expectation over Transformation (EoT) . In Appendix A.1, we show how an end-to-end black-box attack was ineffective in this setting; it is this dead end that motivated us to design and develop the _KoS_ Algorithm. _Our method shows that even if a system has a secure and restricted section, there are instances in which the overall system can still be exploited with adversarial attacks._

## 2 Related Work

Our setting is similar to the Expectation over Transformation  method when the first model \(h_{1}\) is thought of as an arbitrary transformation instead of a learned model. In that work, the transformations are physically motivated and represent parametric transformations of the input like lighting and camera noise. In general, the attacker must know enough about the first transformation to sample from the family of transformations, which is different from our threat model, where we only have query access to the first model. This allows the creation of a set of transformation input points, to be used for averaging gradients. This is the primary competing method and we conduct baseline experiments using this method.

BPDA (Backward-Pass Differentiable Approximation), designed to attack systems that intentionally obfuscate gradients for security reasons, uses a differentiable proxy model to craft gradient based attacks. It is challenging to apply this method in our setting, since in contrast to the defenses attacked in , creating a good proxy for a full-size model is a meaningful task and our paper _focuses on the case when creating such a proxy is not feasible, e.g. rate-limiting defense or attacker resource constraints like information, computation, and query limits_3.

HopSkipJumpAttack could be used for end-to-end black-box attacks in a system like the one we propose since it does not require a proxy for \(h_{1}\). However, in our experiments we found that while this attack was able to achieve the desired target, the adversarial noise introduced was too large to be considered a successful human adversarial attack (see Appendix A.1).

There has been previous work that considers multi-model systems, for example treating the modifications applied by optics and image processing pipelines in cameras as \(h_{1}\) and a classification model as \(h_{2}\). However, this attack creates a proxy model for \(h_{1}\) which is not possible in our problem.

Recent work  has shown adversaries can compose multiple-'safe' models to achieve 'unsafe' behavior and prior work in algorithmic fairness and strategic classification, [7; 21; 20; 9; 17] showed that even in the context of supervised binary classification the composition of 'fair' models can result in highly 'unfair' outcomes. Our work suggests a similar effect is present in adversarial robustness; having a'safe' (e.g. black-box, non-proxy-able) section of the system does not guarantee the safety of the overall system.

Very intriguingly , extracted exact information from a production grade language model, e.g the exact projection embedding layer, in a top-down manner meaning the algorithm extracted information

Figure 1: Multi-Model System with Gradient Restrictions: We have limited query access to \(h_{1}\) and full query/gradient access to \(h_{2}\) and want to craft an end-to-end attack. The core issue is that the adversarial sample against \(h_{2}\) (second row) might not remain adversarial after the transformation of \(h_{1}\). E.g. in the case where \(h_{1}\) is a segmentation and image crop, the perturbation could slightly modify the crop box out of \(h_{1}\), such that the sample is no longer adversarial to \(h_{2}\) (third row).

from the final layers of the neural network. Demonstrated vulnerabilities like this, combined with our algorithm, could allow attackers to execute an effective end-to-end attacks on closed weight production grade systems using their partial knowledge.

## 3 Method

We can easily craft gradient based attacks for \(h_{2}\) using well known methods[22; 11; 30; 28] if we have white-box access to \(h_{2}\) or have created a reliable proxy model. However, since we only have black-box access to \(h_{1}\) and cannot train a proxy model for that component, we cannot directly craft an end-to-end gradient based adversarial attack \(h_{2}(h_{1}(x))\). Furthermore since the modifications applied by \(h_{1}\) are specific to each sample and thus each adversarial sample iteration, there is no guarantee that adversarial modifications against \(h_{2}\) will survive the transformation applied by \(h_{1}\).

_We propose an iterative method, the Keep on Swimming Attack. Simply update the sample that we will attack for \(h_{2}\) when the adversarial perturbation has been removed by \(h_{1}\), using the new output of \(h_{1}\)._

Formally, attack \(h_{2}\) and after \(K\) gradient based attack iterations, re-insert the adversarial perturbation attacking \(h_{2}\) into the original input and pass it through \(h_{1}\) to check if the attack is still adversarial. If the adversarial perturbation survived the transformation of \(h_{1}\), e.g. \(h_{1}(x+_{adv})\) is still in the same domain of \(x_{mod}\), which in our experiment means whether the cropping box coordinates are unchanged, and if we have reached our goal e.g. \(h_{2}(h_{1}(x+_{adv}))=y_{target}\), we terminate and have achieved our objective of an end-to-end attack.

Else if the adversarial transformation survived \(h_{1}\) but has not yet reached the adversarial target, e.g. \(h_{1}(x+_{adv})=x_{mod}+_{adv}\) and \(h_{2}(h_{1}(x+_{adv})) y_{target}\) then we attack for \(K\) more iterations.

Else if the adversarial sample was transformed/warped by \(h_{1}\) and we have a new \(x_{mod}\), so \(h_{1}(x+_{adv})=x_{mod2}\), we just Keep on Swimming; we replace the adversarial sample that we had so far, \(x_{mod}+_{adv}\) with the new modified output of \(x_{mod2}\) and keep attacking.

The attack finishes after a maximum number of iterations or when the end-to-end attack is successful. The algorithm is described in detail in Algorithm 1 and shown in Figure 2.

In Algorithm 1, the \(ReInsert(x,x_{adv})\) operator takes the accumulated adversarial perturbations that have been applied to \(x_{adv}\) and pastes it back into the original \(x\). In our experiment this means pasting \(x_{mod}+_{adv}\) into the region of \(x\) from which we extracted \(x_{mod}\). While our proposed attack pipeline uses a gradient based attack against \(h_{2}\), the pipeline is still valid for non-gradient based attacks.

While our experiments focus on this specific modality, we believe in the general applicability of our framework and Algorithm 1. One example of an application could be a system that processes and answers questions about a text. A first non-proxy-able model extracts quotes from the text related to the question and the second proxy-able model generates an answer. Our method makes is suitable for agentic architectures and in general systems where there is a sequential combination of either models or heuristics in which we only have a partial information.

Figure 2: Keep on Swimming (_KoS_) Multi-Model Attack: Update the sample fed into the start of the pipeline whenever the adversarial perturbation is made ineffective by \(h_{1}\)

``` \(x_{0},x_{mod},y_{target}\)\(\) Original input, Output of \(h_{1}\) and input of \(h_{2}\), Target output for attack \(h_{1}(x_{0}) x_{mod},h_{2}(x_{mod}) y_{pred}\)\(\)\(h_{1}\) and \(h_{2}\)\(Attack(x_{mod},y_{target},h_{2})\)\(\) Attack iteration on proxy-able section \(ReInsert(x_{0},x_{mod})\)\(\) Function to re-insert adversarial modifications from \(x_{mod}\) into \(x_{0}\)\(SameDomain(x_{mod.adv},x_{mod}) bool\)\(\) Checks if values have the same domain MaxRestarts\(\) Max number of restarts due to a different \(x_{mod}\) domain MaxIterations\(\) Max number times \(K\) of attack iterations on a single \(x_{mod}\). This also controls how frequently to obtain feedback from the \(h_{1}\) transformation while crafting \(_{adv}\) \(_{adv} 0\) \(x_{0,adv} x_{0}+_{adv}\)\(\) Initialize intermediate solution to \(x_{0}\)\(i 0\) while\(i<\)MaxRestarts do \(x_{mod} h_{1}(x_{0,adv})\)\(\) reference for original domain \(x_{adv} h_{1}(x_{0,adv})\) \(j 0\) while\(SameDomain(h_{1}(x_{0,adv}),x_{mod})\) and \(j<\)MaxIterations do\(\) Keep on Swimming if\(h_{2}(h_{1}(x_{0,adv}))==y_{target}\)then  Finish and return \(x_{0,adv}\) endif fork=1:K do \(_{adv} Attack(x_{adv},y_{target},h_{2})\) \(x_{adv} x_{adv}+_{adv}\) \(j j+1\) endfor \(x_{0,adv} ReInsert(x_{0},x_{adv})\) endwhile \(i i+1\) endwhile return AttackFailure \(\) Note: \(SameDomain(h_{1}(x_{0,adv}),x_{mod})\) checks that \(h_{1}\) has not changed the domain of \(x_{mod.adv}\) = \(h_{1}(x_{0,adv})\) from the original domain of \(x_{mod}\) such that it destroys the attack. If the domain has changed we restart to adapt to the new domain. ```

**Algorithm 1** Keep on Swimming (_KoS_) Attack

## 4 Experiments

In order to simulate the scenario proposed in this paper we focus on the problem of creating an adversarial attack to cause the numerical value of a check to be misread. The input for this system is an image of a check. The first model of the system (\(h_{1}\)) consists of a segmentation model that identifies the areas of the image with text. The output of model \(h_{1}\) is the area of the image containing the check's numerical amount (\(x_{mod}\)), written in latin numerals 4. The second model of the system (\(h_{2}\)) is an OCR (optical character recognition) system that identifies the numerals in the image. To simulate the target system we use the CRAFT  segmenter (\(h_{1}\)) to create cropped one line text image. To obtain the text in each image (\(h_{2}\)), we used the publicly available Microsoft's Transformer based OCR for handwritten text. We ran our experiments on a database of pictures of checks filled with handwritten information in which CRAFT was able to correctly identify and extract the numerical amount of the check. The attack objective is to transform the predicted numerical amount of one check to another value for a total of 20 attack samples.

For the attack, we assume black-box access to \(h_{1}\) but not the possibility of creating a proxy. To create the adversarial sample for the image-to-text (OCR) section (\(h_{2}\)) we use the "I See Dead People" (ISDP). This attack is grey-box since it has white-box access to the image encoder but not to the text decoder. In this case we had white-box access to the image encoder since we used the same OCR model as CRAFT but a proxy model for the image encoder could have been used making this attack entirely black-box. Note that this does not affect the results since ISDP was used with all attack pipelines and the objective of the _KoS_ attack pipeline is to create an adversarial sample that survives \(h_{1}\) and is still effective for \(h_{2}\). The _KoS_ attack is not affected by how the adversarial sample was created for \(h_{2}\).

### Benchmarks

We compare our method with a baseline of only attacking \(h_{2}\) and re-inserting the adversarial cropped image into the original large image (ISDP Baseline). We also compare our method with creating attacks that are robust to the transformation from \(h_{1}\) using the method from  (Crop Robust ISDP). For the Crop Robust ISDP, we take a slightly larger crop than the one from the starting image, perform 10 random crops such that the text is always contained in the crop, attack each random crop independently, average the gradients and update the image to create the adversarial version. We found these hyperparameters to provide the best overall results for this method.

We compare the methods in terms of the attack success rate, the mean squared error (MSE) with respect to the original image, and computational cost. Table 1 shows the success rate of the _KoS_ pipeline is considerably higher and the Levenshtein distance the final output of both the cropped and the full image are considerably lower than just using the ISDP attack or creating a version that is robust to cropping.

The _KoS_ pipeline introduces more noise and takes more time than the Baseline ISDP but less than the Crop Robust ISDP attack. The key benefit of our method, that clearly Pareto dominates the other methods is our substantially higher attack success rate. We would note that we investigated these alternate baseline methods first and it was only our inability to craft successful attacks that required us to design the _KoS_ attack.

## 5 Conclusion

We have shown how adversaries can use their knowledge of one model in a multi-model system to craft effective end-to-end attacks with the _KoS_ algorithm. Further work is needed to study the convergence properties of _KoS_, and generalizing the attack to other settings like attacking a composition of LLMs. That said, these initial results already demonstrate the need for timely research into attacks and defenses in the threat model of Multi-Model Systems With Partial Proxy Access. If multi-agent and multi-model systems inherit the vulnerability of the most 'proxy-able' model, that suggests serious un-patched vulnerabilities already exist in the foundation model era, and we can expect the impact of such vulnerabilities to be amplified in the upcoming era of agentic AI.