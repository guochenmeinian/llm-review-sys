# Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models

Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models

 Peter Hase\({}^{1,2}\) Mohit Bansal\({}^{2}\) Been Kim\({}^{1}\) Asma Ghandeharioun\({}^{1}\)

\({}^{1}\)Google Research \({}^{2}\)UNC Chapel Hill

{peter, mbansal}@cs.unc.edu

{beenkim, aghandeharioun}@google.com

###### Abstract

Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights . In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us _where_ to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit . Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.1

## 1 Introduction

Language models learn a variety of facts about the world during pretraining that can be elicited via natural language prompts . Recent work explores how these facts are stored in model weights and expressed in response to particular prompts, suggesting that MLP weights act as key-value memories that support factual association . Besides improving our scientific understanding of pretrained language models, this kind of investigative work may enable the design of better model editing methods for injecting new facts into model weights, and indeed it has been used to motivate the ROME and MEMIT model-editing methods . These recent methods set a new state of the art for weight edits that successfully rewrite stored facts in language models. Model editing methods could be broadly useful for correcting factual errors in pretrained models, avoiding morally undesirable outputs, and updating models with changing knowledge over time.

The connection between _localization_ (identifying components of a model responsible for a certain behavior) and _editing_ (changing model components in order to change model behavior) is predicated on the reasonable assumption that one should go about editing a model by first localizing a behavior to a specific component and then choosing to edit that particular component. In the case of ROME and MEMIT, localization is done via Causal Tracing, which measures the information content of hidden representations, and editing is done by treating MLP weights as linear associative memories and injecting new key-value memories into the weights. Meng et al. [21; 22] choose to edit early MLP layer(s) based on results from Causal Tracing showing the largest causal effects on average in early layers.

**Surprisingly, the assumption that one should change the knowledge in a model by editing the weights where it is stored turns out to be false.** In fact, localization results from Causal Tracing are statistically uncorrelated with the success of an edit injecting a new fact into MLP weights. Using the CounterFact dataset from Meng et al.  with a GPT-J model , we show that (1) not only is a substantial fraction of factual knowledge stored outside of the range of layers edited by ROME/MEMIT (see Fig. 1), (2) the correlation between Causal Tracing results and edit success is near zero (for several editing methods including ROME, MEMIT, and Adam-based finetuning). We note that this is surprising largely because ROME and MEMIT _do_ work well for editing facts, in spite of Causal Tracing often suggesting knowledge is stored elsewhere than early-to-mid-layer MLP weights.

In the face of this result, we attempt to recover the connection between tracing-based localization and editing by introducing four variants of the default model editing problem. Each variant differs in terms of the input, target, or objective used in the editing problem. One variant we introduce, called Fact Forcing, is designed to match Causal Tracing along these three factors. Specifically, Fact Forcing uses a noised input and involves maximizing the probability of the correct target output, just like Causal Tracing. We find that tracing results _are_ related to edit success for Fact Forcing. However, even for this variant, it is still better to ignore the tracing results and always choose an early-to-mid-layer MLP weight for editing. We conclude that, although Causal Tracing is a reasonable localization method that has yielded insight into how models store factual information, this insight does not actually indicate which model layers we should edit in order to manipulate what facts are stored in language models.

To summarize, our conclusions are as follows:

1. We find that model edit success is essentially unrelated to where factual information is stored in models, as measured by Causal Tracing. Robustness experiments generalize this result across causal localization methods, editing methods, editing metrics, models, and datasets.
2. To reconnect localization with editing performance, we introduce four variants of a standard model editing problem, including Tracing Reversal, Fact Erasure, Fact Amplification, and Fact Forcing.
3. Edit success and tracing effects correlate best in the Fact Forcing setting. However, tracing effects explain only a small fraction of the variance in editing performance, while the choice of edit layer is a much more important factor. This suggests that, surprisingly, localization insights from Causal Tracing are not useful for choosing which model layer to edit.

## 2 Related Work

**Localization.** A long line of work aims to interpret what certain hidden representations represent, or, in the reverse direction, to understand how a given concept is represented in a model. Both of these efforts aim to localize behaviors to specific model components. We group these methods based on the kinds of model components they consider (e.g. layers, neurons, etc.).

Many works focus on individual layers or weight matrices [37; 31; 9; 33; 11]. In this paper, we adopt the layer-wise localization method from Meng et al.  known as Causal Tracing, which estimates the information content of a set of representations via a denoising operation. We specifically focus on MLP layers given evidence of their role in factual association [12; 21; 13].

Figure 1: We visualize where 652 facts known by GPT-J are stored within the model, as localized by Causal Tracing. Model editing methods like ROME and MEMIT can successfully change knowledge in LMs by editing layers 4-9. But many facts appear to be stored outside of this range, e.g. at layers 1-3 and 16-20. What about these facts?

Related to analysis at the layer level, other work aims to localize concepts to directions in a latent space, dating back to work interpreting directions in word vector space [23; 17; 41; 14; 38; 5]. One might also place "key-value memory" theories of weight matrices in this category since a key vector represents a direction in the latent space [1; 32; 12; 21].

Neurons, meanwhile, are the most common focus of localization analysis. Past work explores the functions of groups of neurons and subnetworks [27; 6; 10; 4] or simply individual neurons [29; 40; 18; 2; 34; 26; 8; 19; 3; 16; 7; 36].

**Relating Localization to Editing.** Many works on localization validate the quality of their conclusions by editing neuron activations or layer weights corresponding to a particular concept, then checking that the network behavior changes appropriately. For example, Dai et al.  check that their "knowledge neurons" have localized a specific fact by amplifying or suppressing the expression of that fact via adjusting the corresponding neuron activations. Altogether, we find many localization analyses are validated by editing models in suggested locations [29; 18; 2; 26; 34; 8; 19; 7; 36; 4] or directions in the latent space [23; 1; 32; 21].

Changing model behavior by editing components suggested by localization seems like a reasonable validation step. However, in isolation, it paints an incomplete picture that has led to misleading interpretations about the connections between localization and editing. Such experiments alone do not show whether editing _that specific component_ is (1) successful in proportion to the strength of the localization, (2) necessary to achieve the desired behavior, or (3) the best option for editing. In particular, these experiments do not show whether the same change in behavior can be achieved _elsewhere in the network_. Meng et al.  consider this question by measuring editing success across layers, averaged across data, then comparing the results with Causal Tracing conclusions also averaged across data. However, as we show, more fine-grained analysis at the datapoint level reveals the unexpected result that tracing results are unrelated to edit success. We are not aware of any work that primarily investigates the connection between localization and editing or that demonstrates better model editing at locations elsewhere in the network than those suggested by localization analysis.

## 3 Notation and Background

### Data Notation

Following Meng et al. , we consider facts of the form \((s,r,o)\), where \(s\) represents a subject entity (e.g. _Paris_), \(r\) a binary relation (e.g. _is located in_), and \(o\) an object (e.g. _France_) for which the tuple \((s,r,o)\) represents a factual assertion about the world. In the CounterFact dataset , each datapoint is a prompt \(P\) for some fact \((s,r,o)\). So, \(P\) might be "Paris is located" or "Paris is situated in," to be completed by the object \(o\) to form a true statement. In an abuse of notation, we will often use \(s\) and \(r\) to refer to textual representations of a subject and relation, for instance by writing a model's conditional probability as \(p_{}(|s,r)\) instead of \(p_{}(|P)\). We do so in order to more easily indicate when an input is provided where the subject or relation has been manipulated (described next).

We make use of a few variations of the data for the fact \((s,r,o)\). The additional variables include:

1. \(s^{*}\) is a "neighboring" entity to the subject \(s\) (similar to \(s\)) for which \((s^{*},r,o)\) is a true fact like \((s,r,o)\). In CounterFact, "Marseille" is a neighboring entity to "Paris."
2. \(r^{*}\) is a paraphrase of the relation \(r\), such as "is situated in" for "is located in."
3. \(s_{noise}\) is a noised representation of the subject \(s\). We add Gaussian noise to the token embeddings of \(s\), following Meng et al. .
4. \(o_{}\) is an object that incorrectly completes the tuple \((s,r,)\). CounterFact contains an \(o_{}\) for each datapoint, intended to be the new model output when evaluating model editing methods.
5. \(o_{}\), for clarity, is the object that correctly completes the fact \((s,r,)\), from CounterFact.

### Causal Tracing

We give a brief description of Causal Tracing here and refer readers to Meng et al.  for more information (see Fig. 2 for an example visualization). Causal Tracing is a method for localizing information in the forward pass of an autoregressive Transformer to specific hidden representations. For a model with \(L\) layers, the input is a prompt containing \(T\) tokens (including a subject \(s\) and relation \(r\)). Given this input, the forward pass produces \(T L\) layer outputs (one representation per \(T\) tokens and \(L\) layers). The algorithm aims to estimate the amount of information about the fact \((s,r,o_{})\) that is contained in each of these representations. We denote the representation at token \(t\) and layer \(\) as \(v_{(t,)}\).

The amount of factual information in \(v_{(t,)}\) is estimated by copying this representation into a different forward pass obtained from using a noised subject in the input:

\[=p_{}(o_{}|s_{},r,v_{(t,)})-p_{}(o_{}|s_{},r)\]

where \(s_{}\) indicates that we add Gaussian noise with \(=0.094\) to the token embeddings of \(s\) following Meng et al. , and \(v_{(t,)}\) is the representation at token \(t\) and layer \(\) in the forward pass on the original prompt \(P=(s,r)\). The probability \(p_{}(o_{}|s_{},r,v_{(t,)})\) is computed by (1) running the model forward pass on the noised prompt \(P^{*}=(s_{},r)\) until layer \(\), (2) _overwriting_ the existing representation at token \(t\) and layer \(\) with the representation \(v_{(t,)}\), then (3) computing the remaining \(L-\) layers as normal using this adjusted set of \(T\) representations as input (adjusted at token index \(t\)). Thus, Causal Tracing estimates the information content of a representation in terms of its effect on the probability of the true target. The results from Causal Tracing show where the representations containing information about the true target are in the model forward pass.

**In practice, a _set_ of representations from multiple adjacent layers is copied from the clean forward pass rather than a single layer's representation** (for instance, ten layers in Fig. 2). The size of this set is referred to as the _tracing window size_. A window size of, e.g., three implies that the tracing effect at layer \(\) estimates the amount of information contained in the three representations \(v_{(t,-1)}\), \(v_{(t,)}\), and \(v_{(t,+1)}\). See Appendix Figs. 10 and 11 for analysis of the parameter's effect. In this paper, we use a tracing window size of 5 by default, and we apply Causal Tracing exclusively to MLP layers, given evidence of their role in factual association [12; 21].

### Model Editing with ROME

We describe the ROME editing method here since we use it in our analysis in Sec. 4, and later in Sec. 5 we outline additional editing methods we consider. For mathematical detail, see Meng et al. .

The input to ROME includes a prompt \(P=(s,r)\) and a new desired output, which is always a false target \(o_{}\) in the CounterFact dataset. To change the model prediction to \(o_{}\), ROME applies a rank one edit to the down-projection matrix in a prespecified MLP layer in the model. The default layer in GPT-J is layer 6, following from averaged Causal Tracing results. ROME also makes use of covariance statistics of different subject representations obtained from a larger corpus as it edits individual facts. Overall, the method is designed to optimize the quantity \(p_{}(o_{}|s,r)\) while aiming to satisfy some other constraints reflecting what a desirable model edit is (described in Sec. 3.4 next).

Figure 3: An example CounterFact datapoint.

Figure 2: Visualizing Causal Tracing results over MLP layers with window size 10. Tokens with an asterisk are the noised subject tokens. Here, \(p_{}(o_{}|s,r){=}.923\) and \(p_{}(o_{}|s_{},r){=}.001\).

### Editing Metrics

Editing methods are typically evaluated according to their ability to (1) change the model prediction on the input \(P\) provided at runtime, (2) generalize appropriately to paraphrases of the prompt \(P\), and (3) avoid over-generalizing to unrelated data [42; 9; 24; 15; 25]. We adopt metrics for each desideratum that we compute with available CounterFact data. Instead of the exact "magnitude" metrics from Meng et al. , we use normalized versions of each metric that we design to scale from 0 to 1 depending on whether the edit was maximally (un)successful, for purposes of making scores more comparable across data points. We denote the new edited weights of the LM as \(^{*}\) and its pre-edit weights as \(\). See Fig. 3 for an example of the kinds of data these metrics are computed on.

1. _Rewrite Score_. The rewrite score measures how much an edit improves the target probability \(p(o_{}|s,r)\) as a fraction of the maximum possible improvement: \[}(o_{}|s,r)-p_{}(o_{}|s, r)}{1-p_{}(o_{}|s,r)}\]
2. _Paraphrase Score_. The paraphrase score measures the target probability using syntactical paraphrases as inputs, always preserving the exact subject wording: \[}(o_{}|s,r^{*})-p_{}(o_{ }|s,r^{*})}{1-p_{}(o_{}|s,r^{*})}\] which is averaged over multiple available paraphrases per input \(P\). The score measures whether edits properly generalize across semantically equivalent prompts.
3. _Neighborhood Score_. The neighborhood score measures whether edits change predictions for prompts with a similar subject \(s^{*}\), the same relation \(r\), and the same (true) objects. We scale the difference in probabilities so that 1 means the probability did not change (good), and 0 means it changed to the maximum extent possible (bad): \[1-}(o_{}|s^{*},r)-p_{}(o_{}|s^{*},r)|}{.5+|p_{}(o_{}|s^{*},r)-.5|}\] The score measures whether edits avoid _over_-generalizing from the prompt \(P\) to different subjects.

## 4 Does Edit Success Follow From Localization?

Ostensibly, localization results should inform editing methods because it should help to know where information is stored in a model if you are going to manipulate the model's expression of that information. More specifically, if you wanted to inject a false belief \((s,r,o_{})\) into a model (as defined in the ROME editing problem), it seems helpful to know which weights store the true fact \((s,r,o_{})\), so that you could replace some stored representation of \(o_{}\) with that of \(o_{}\). This underlying assumption about editing models appears in much past work on localization, where editing is used to verify localization analysis (see Sec. 2). In this section, we investigate the validity of this assumption as it applies to autoregressive Transformers.

### Experiment Design

The goal of our experiments is to determine, for a given datapoint, whether edit success _at a specific layer_ aligns with the results from Causal Tracing at that layer (see Causal Tracing description in Sec. 3.2). We operationalize this outcome and explanatory variable as follows:

1. _Edit Success_. We primarily consider Rewrite Score as our measure of edit success, given that this is the main optimization objective of ROME. Note ROME achieves an average rewrite score of 99% at layer 6 of GPT-J and above 96% at layers besides the last layer of the model.
2. _Tracing Effect at layer \(\)_. Since the output of Causal Tracing is a \(T L\) grid of estimates, we obtain a single tracing effect per layer by taking the max across the \(T\) token effects at each layer (i.e., we collapse the grid in Fig. 2 down to a single curve across layers). Like our other metrics,we use a _fractional_ tracing effect where 0 means the intervention had no effect and 1 means it fully restored the original probability \(p_{}(o_{}|s,r)\):

\[(o_{}|s_{},r,v_{(t,t)})-p_{}( o_{}|s_{},r)}{p_{}(o_{}|s,r)-p_{ }(o_{}|s_{},r)}\]

Lastly, note we use a tracing window size of 5 (smaller than the value of 10 used in Fig. 2).

### Model and Data

We conduct our analysis with GPT-J  using the CounterFact dataset, similar to Meng et al. . GPT-J is a 6 billion parameter autoregressive language model. We record editing performance at layers in {1, 5, 9, 13, 17, 21, 25, 28} as well as layer 6 (the default for ROME). Note ROME achieves an average rewrite score of 99% at layer 6 and above 96% at layers besides layer 28.

The CounterFact dataset includes datapoints consisting of a prompt, paraphrases, and neighboring points. For each point, a new (false) target is supplied for editing purposes. We show an example datapoint in Fig. 3. Note paraphrases intentionally include unrelated text preceding a syntactical paraphrase of the input, with the idea that this text should not affect the output. We select data for experiments from 10% of CounterFact, additionally filtering to a subset of facts that are correctly completed by GPT-J, in order to ensure that there is knowledge to localize in the model for each point (details in Appendix A). Our final sample size is \(n=652\).

### Experiment Results

We present results in two ways. First, in Fig. 4, we show Rewrite Score as a function of the (fractional) tracing effect. The red dotted line shows a hypothetical perfect relationship between tracing and edit success. Surprisingly, there is not a positive relationship but a _negative_ relationship between the rewrite score and the tracing effect (linear correlation of \(=-0.13\); \(p<\)1e\(-3\)). This seems to fully invalidate the assumption that editing should be most effective when it occurs at a layer where information is stored about the edited fact. We wish to emphasize, however, that in most layers we simply see a near-zero rather than negative correlation, as shown in Appendix Fig. 15.

Our second mode of analysis is though linear regression models predicting rewrite score based on (1) the tracing effect, (2) the choice of edit layer treated as a categorical variable, or (3) both terms interacted, again treating edit layer as a categorical variable. The purpose of the models is to show how much of the variance in rewrite score is explained by one variable versus the other. We show the resulting \(R^{2}\) values in Table 1. We see that the choice of layer explains almost all of the variance in rewrite score (94.7%), while adding the tracing effect to the model raises the \(R^{2}\) only to 94.8%. This means that **the tracing effect is able to explain only 0.1% of the variance in edit success** when accounting for the choice of edit layer. These results suggest that the tracing effect is essentially unrelated to the success of model editing.

This is a surprising conclusion, and it naturally raises the question of why applying ROME at layer 6 works well in the first place (see average rewrite, paraphrase, and neighborhood scores across layers in Appendix Fig. 7). We suggest a possible answer to this question in Sec. 6.

    & \) Values} \\  Method & Layer & Tracing Effect & Both \\  ROME & 0.947 & 0.016 & 0.948 \\   

Table 1: \(R^{2}\) values for predicting ROME edit success. Tracing effects explain essentially none of the variance in rewrite score, while the choice of edit layer is very important.

Figure 4: The correlation between ROME edit success and the tracing effect at layer 6 in GPT-J is not positive but in fact slightly negative (\(=-0.13\); \(p<\)1e\(-3\)). The dashed red line shows a hypothetical perfect relationship.

**Additional Robustness Experiments**. We include additional results in Appendix B using another dataset, ZSRE  (Figs. 19 and 20, Table 8), and another localization method, representation zeroing  (Figs. 21 and 22). Further robustness experiments in Appendix C include results with (1) other measures of edit success including Paraphrase Score, Neighborhood Score, and an Overall Score (Tables 4, 5 and 6), (2) different values of the tracing window size (Fig. 12), (3) GPT2-XL rather than GPT-J (Fig. 13), (4) the original unscaled metrics from Meng et al.  (Fig. 14), and (5) tracing effects measured at the last subject token rather than the max across tokens (Fig. 16). We find that **all of these experiments corroborate our results comparing Causal Tracing to Rewrite Score for GPT-J on CounterFact.** Considering these robustness results alongside additional editing method experiments that we consider in Sec. 5 below, we note that our main conclusions generalize across different causal localization methods, editing methods, editing metrics, models, and datasets.

## 5 Reconciling Localization and Editing

If injecting a new fact has little to do with where an existing fact is stored in the model, perhaps there is some other editing intervention that would be more closely related to insights from tracing analysis. In this section, we propose a few variants of the model editing problem that appear more and more like Causal Tracing in terms of their input, target, and objective. Then, we repeat and extend our analysis from Sec. 4 for all of these editing problems.

### Editing Problem Variants

We summarize the following editing problems in Fig. 5.

1. _Error Injection_. The editing problem considered in Sec. 4, the objective being to maximize \(p_{}(o_{}}|s,r)\).
2. _Tracing Reversal_. We maximize \(p_{}(o_{}}|s,r)\), aiming to change the model output from \(o_{}}\) back to the output for the "original" noised input \(P=(s_{}},r)\) in Causal Tracing, \(o_{}}\).
3. _Fact Erasure_. Knowing where a fact is stored could be more useful for erasing the fact rather than injecting a new one. Hence, we consider erasing a fact by minimizing \(p_{}(o_{}}|s,r)\).
4. _Fact Amplification_. We reinforce known facts in the model by maximizing \(p_{}(o_{}}|s,r)\). Even for correctly predicted points, this value is often not near 1, leaving room for it to be increased.
5. _Fact Forcing_. As in Causal Tracing, this method uses a noised subject representation \(s_{}}\). We force the model to output \(o_{}}\) for this input by maximizing \(p_{}(o_{}}|s_{}},r)\). Though this problem is of little practical significance, it is the most similar to Causal Tracing in its design, since it uses the same input as Causal Tracing and matches the goal of increasing the probability of \(o_{}}\) (see Sec. 3.2).

Note that solutions to each of these problems are evaluated according to our Rewrite Score, Paraphrase Score, and Neighborhood Score metrics from Sec. 3.4. The only difference is in the target output for the rewrite and paraphrase metrics (neighborhood is entirely identical).

Figure 5: Depiction of editing problem variants. Rather than inject a new false fact into a model (Error Injection), we consider injecting the output obtained from noising the subject entity (Tracing Reversal), erasing a stored fact (Fact Erasure), amplifying a stored fact (Fact Amplification), or forcing a known fact onto the same kind of noisy input as used in Causal Tracing (Fact Forcing).

### Experiment Design and Additional Edit Methods

We use the same experimental procedure as in Sec. 4, except that we consider a broader set of editing methods besides ROME. We list the four methods below:

1. ROME. The edit method from Sec. 4, ROME edits a single MLP layer's down-projection weight.
2. MEMIT. Though designed to edit multiple facts at once, when editing a single fact this method differs from ROME only by spreading out its update over several layers rather than one layer .
3. Constrained Finetuning (window size 1). We adopt a simple Adam-based optimization approach with an \(_{}\)-norm constraint, following Zhu et al. . The window size of 1 indicates we apply this method at a single layer.
4. Constrained Finetuning (window size 5). The above finetuning method on five adjacent layers.

We select these methods for their simplicity and since ROME and MEMIT are designed specifically to edit MLP layers. Note that we report results for Causal Tracing with a window size of five, so when we use MEMIT or constrained finetuning to edit five layers, these five layers can exactly match the range of restored layers from Causal Tracing.

### Experiment Results

Main Results.As in our analysis in Sec. 4, we report \(R^{2}\) values for a linear regression model predicting the rewrite score based on (1) the choice of edit layer treated as a categorical variable, or (2) that variable interacted with the tracing effect. We show the results in Fig. 6, with \(R^{2}\) values for each regression above their respective bars (numbers also in Appendix Table 3). We find that, relative to the Layer-only regression, **tracing effects explain at most an additional 3.2% of the variance in edit success** across our different editing problems and editing methods. This is a very small effect, especially compared to \(R^{2}\) values from the Layer-only regression, which explains most of the variance in the outcome (58.5% on average across conditions in Fig. 6). We believe this is surprising given how the editing problem variants are designed. **It would seem that knowing where a fact is stored should help with amplifying or erasing that fact, but our results appear to fully disconfirm this hypothesis.** Interestingly, it also appears that it makes little difference whether we edit at one layer or five layers in order to match the number of representations restored by Causal Tracing. Based on comparisons between finetuning methods (FT-1 and FT-5) and between ROME and MEMIT (applied to 5 layers), editing at five layers does not improve the alignment between tracing and editing. In addition to our robustness results listed in Sec. 4.3, we also repeat our analysis using a subset of points where tracing effects are concentrated to a small number of layers, in order to focus on points where MEMIT and FT-5 edit _all_ of the layers where the fact is stored. Results are nearly identical for this subset of the data (see Appendix B).

One Successful Case.We see the strongest positive relationship between edit success and tracing effects for Fact Forcing with finetuning methods. Here, we find that tracing effects explain an additional 3% of the variance in edit success (up from 1.5% for other experiments). This effect

Figure 6: Tracing effects are very weakly predictive of edit success across editing problems and methods. Relative to the \(R^{2}\) of a regression predicting rewrite score based on the edit layer (blue), a regression with edit layer and tracing effects (orange) improves the \(R^{2}\) by at most.03 points (bolded). The choice of edit layer is a much better predictor of the rewrite score.

is statistically significant at \(p<14\) according to an F-test2 comparing the two models (see visualization in Appendix Fig. 17). The result for Fact Forcing suggests that using \(s_{noise}\) rather than \(s\) in the model input is the cause of the positive relationship between editing and localization. We rule out the choice of target and maximizing vs. minimizing the target probability as possible causes based on the design of each problem variant (see Fig. 5): (1) the choice of target is not important since results are similar for Error Injection, Tracing Reversal, and Fact Amplification, and (2) maximizing vs. minimizing the target probability is not important since results are similar for Fact Erasure and Fact Amplification. Yet, tracing effects are still weakly informative of Fact Forcing editing if they explain only 3% of the variance in edit success. This points to there being other deeper reasons for localization results being unrelated to editing success.

## 6 Discussion

**Does Causal Tracing tell us anything?** We show that Causal Tracing is not indicative of which layer to select for model editing. However, this does not mean that localization insights from Causal Tracing have been useless. Causal Tracing has helped reveal the role that early-to-mid-range MLP representations _at the last subject token index_ play in factual association in autoregressive language models, and ROME does perform better on average when optimizing the last subject token representation rather than another token representation .3 Past work finds that both MLP and attention layers can show large Causal Tracing effects, and additional empirical editing experiments then demonstrate that it is preferable to edit MLP weights .

**Why is edit success high at layers where the edited fact is not actually stored?** First, we note that information is gradually accumulated across layers in a Transformer forward pass, as discovered by past work [31; 12; 21; 22; 13]. We suggest that it is possible to "override" the information in layer \(\) with an edit to another layer \(k\) (where \(k<\) or \(k>\)). Since ROME is typically effective across a large range of layers (see Fig. 9), it appears that ROME can override the information accrued across 5 or 10 layers of a forward pass with an edit to a single layer outside of that range of layers. We summarize this hypothesis as follows: _Many layers could store a fact, and it happens that some do._

If this hypothesis were true, it would be surprising because one cannot arbitrarily swap layers in a Transformer model without greatly damaging model performance . That is, it should matter where information enters the residual stream, since later layers strongly depend on receiving the right incoming information from prior layers. We leave it to future work to further investigate this hypothesis.

**What do our results imply about using model editing to validate localization claims?** We interpret our results to suggest that Causal Tracing _answers a different question_ than model editing does. That is, Causal Tracing answers a question about where factual information is carried in representations in a Transformer forward pass, and this question turns out to be a different question than the _editing_ question of where is best to intervene in the Transformer in order to change the factual information it expresses. It seems critical, then, to carefully formalize the questions that one wishes to answer before (1) validating the results of localization via editing or (2) motivating the design of an editing method via localization, because the conclusions that can be drawn from a particular localization method might not be relevant for the performance of a given model editing method. This would not imply the conclusions from the localization analysis are invalid, though. For instance, we believe Causal Tracing reveals interesting insights about where MLP representations contain factual information (see Figs. 1 and 2). We only wish to suggest that localization analysis might answer a different question than the question answered by model editing.

These observations may have implications for the array of studies that validate their localization analysis by manipulating a certain model behavior via an intervention on the model component recommended by the analysis [29; 18; 2; 1; 26; 34; 8; 19; 7; 36; 4; 21]. Do model editing experiments provide _additional_ evidence for claims about which model components are responsible for certain behaviors? If localization and editing answer different questions, editing experiments will not provide further evidence for localization conclusions.

Conclusion

We obtain the surprising result that model edit success is essentially unrelated to where factual information is stored in models, as measured by Causal Tracing. Faced with this result, we attempt to reconnect tracing-based localization with edit success by introducing four variants of the Error Injection problem using the CounterFact dataset. We find that edit success and tracing effects correlate best in our Fact Forcing setting. However, even in this case, tracing effects explain only a small fraction of the variance in editing performance, while the choice of edit layer is a much more important factor. This suggests that, counterintuitively, better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.

## 8 Limitations

We note a few limitations of the experiments conducted in this paper:

(1) We work only with the CounterFact and ZSRE datasets, which we use as short English prompts with factual completions corresponding to a specific set of relations between subject and object entities. This is a basic form of factual knowledge, and localization and editing analysis may yield different trends for other forms of knowledge.

(2) We work with two autoregressive Transformers chosen for their representativeness of large language models that show a capacity for expressing factual knowledge in response to natural language prompts. However, the conclusions from our analysis may not generalize to models larger than GPT-J (6B parameters) that are known to exhibit phase changes in their behavior under prompting.

(3) We use a particular set of localization and editing methods, including representation denoising and zeroing at the layer level and layer-level MLP editing methods that inject new facts or amplify or erase existing facts. Our conclusions may not necessarily hold for the breadth of localization and editing methods from work related to this paper, and one should be cautious in applying our conclusions beyond our experimental setting.

## 9 Broader Impacts

It is possible that increased mechanistic understanding of models improves our ability to edit them at some point in the future. In fact, we consider it unlikely that interpretability results never give insight into improving model editing methods. Thus, to the extent that model editing is a dual use methodology, which could be used to inject harmful beliefs or dangerous knowledge into models, interpretability results may enhance the effectiveness of these malicious use cases. However, these concerns are relatively far removed from our analysis, which focuses on the connection between localization and editing performance. Ultimately, we hope that studies of mechanistic interpretability and model editing improve our ability to control language models.