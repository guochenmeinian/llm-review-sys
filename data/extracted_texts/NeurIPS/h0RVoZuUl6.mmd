# Resilient Constrained Learning

Ignacio Hounie

University of Pennsylvania

&Alejandro Ribeiro

University of Pennsylvania

&Luiz F. O. Chamon

University of Stuttgart

###### Abstract

When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach _resilient constrained learning_ after the term used to describe ecological systems that adapt to disruptions by modifying their operation. We show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. We showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in heterogeneous federated learning.

## 1 Introduction

Requirements are integral to engineering and of growing interest in machine learning (ML) . This growing interest is evident in, e.g., the advancement towards designing ML systems that are fair , robust , and safe , as well as numerous applications in which we want to attain good performance with respect to more than one metric . Two concrete applications that we will use as examples (Section 5) are _heterogeneous federated learning_, where each client realizes a different loss due to distribution shifts (as in, e.g., ) and _invariant learning_, where we seek to achieve good performance even after the data has undergone a variety of transformations (as in, e.g., ).

The goal in these settings is to strike a compromise between some top-line objective metric and the requirements. To this end, an established approach is to combine the top-line and requirement violation metrics in a single training loss. This leads to _penalty methods_ that are ubiquitous in ML, as attested by, e.g., fairness  and robustness  applications. Another approach to balance objective and requirements is formulating and solving constrained learning problems. Though less typical in ML practice, they are not uncommon . In particular, they have also been used in fair  and robust  learning, to proceed with a common set of applications. It is worth noting that penalty and constrained methods are not unrelated. They are in fact equivalent in convex optimization, in the sense that every constrained problem has an equivalent penalty-based formulation. A similar result holds in non-convex ML settings for sufficiently expressive parametrizations .

In either case, the compromise between objective and different requirements are specified by (hyper)parameters, be they penalty coefficients or constraint levels (Section 2). Finding penalties or constraints specifications that yield reasonable trade-offs is particularly difficult in ML, which ofteninvolves statistical requirements, such as fairness and robustness, that have intricate dependencies with the model and unknown data distributions. Case in point, consider invariant learning for image classification . While we know that invariance to rotations and translations is desirable, we do not know _how much_ invariance to these transformations is beneficial. This depends on the level of invariance of the data distribution, its prevalence in the dataset, and the capability of the model to represent invariant functions. The standard solution to this problem involves time consuming and computationally expensive hyperparameter searches.

This paper addresses this issue by automating the specification of constraint levels during training. To do so, it begins by interpreting constraints as nominal specifications that can be relaxed to find a better compromise between objective and requirements (Section 3). We call this approach _resilient constrained learning_ after the term used to describe ecological systems that adapt to disruptions by modifying their operation . Our first contribution is the following insight:

1. We relax constraints according to their relative difficulty, which we define as the sensitivity of the objective to perturbations of the constraint (Section 3.1).

That difficult constraints should be relaxed more is a natural choice. The value of (C1) is in defining what is a difficult constraint. We then seek constraint levels such that the objective loss is relatively insensitive to changes in those levels. This relative insensitivity incorporates a user-defined cost that establishes a price for relaxing nominal specifications.

The learning problem implied by (C1) seems challenging. Our next contribution is to show that it is not:

1. We use duality and perturbation theory to present reformulations of the resilient learning problem from (C1) (Section 3.2) that lead to a practical resilient learning algorithm (Section 4) for which we derive statistical approximation bounds (Thm. 1).

Our final contribution is the experimental evaluation of the resilient learning algorithm:

1. We evaluate resilient formulations of federated learning and invariant learning (Section 5).

Our experiments show that (C1)-(C2) effectively relaxes constraints according to their _difficulty_, leading to solutions that are _less sensitive_ to the requirement specifications. It illustrates how resilient learning constitutes an interpretable and flexible approach to designing requirements while contemplating performance trade-offs.

## 2 Learning with Constraints

Let \(_{0}\) be a distribution over data pairs \((,y)\) composed of the feature vector \(^{d}\) and the corresponding output \(y\). Let \(f_{}:^{k}\) be the function associated with parameters \(^{p}\) and \(_{0}:^{k}[-B,B]\) be the loss that evaluates the fitness of the estimate \(f_{}()\) relative to \(y\). Let \(_{}=\{f_{}\}\) be the hypothesis class induced by these functions. Different from traditional (unconstrained) learning, we do not seek \(f_{}\) that simply minimizes \([_{0}((),y)]\), but also account for its expected value with respect to additional losses \(_{i}:^{k}[-B,B]\) and distributions \(_{i}\), \(i=1,,m\). These losses/distributions typically encode statistical requirements, such as robustness (where \(_{i}\) denote distribution shifts or adversarial perturbations) and fairness (where \(_{i}\) are conditional distributions of protected subgroups).

Explicitly, the constrained statistical learning (CSL) problem is defined as

\[^{}=_{}& _{(,y)_{0}}_{0}f_{ }(),y\\ &_{(,y) _{i}}_{i}f_{}(),y  0,\ \ i=1,,m.\] (P)

Without loss of generality, we stipulate the nominal constraint specification to be zero. Other values can be achieved by offsetting \(_{i}\), i.e., \((f_{}(),y) c\) is obtained using \(_{i}()=()-c\) in (P). We also let \(^{}\) take values on the extended real line \(\{\}\) by defining \(^{}=\) whenever (P) is infeasible, i.e., whenever for all \(\) there exists \(i\) such that \(_{_{i}}_{i}(f_{}(),y)>0\).

A challenge in formulating meaningful CSL problems lies in specifying the constraints, i.e., the \(_{i}\). Indeed, while a solution \(f_{^{}}\) always exists for unconstrained learning [\(m=0\) in (P)], there may be no \(\) that satisfies the constraints in (P). This issue is exacerbated when solving the problem using data: even arbitrarily good approximations of the expectations in (P) may introduce errors that hinder the estimation of \(^{}\) (see Appendix A for a concrete example). In practice, landing on feasible requirements may require some constraints to be relaxed relative to their initial specification. Then, in lieu of (P), we would use the relaxed problem

\[^{}()=_{} _{(,y)_{0}}_{0 }f_{}(),y\] ( \[_{}\] ) \[ _{(,y)_{t}}_{ i}f_{}(),y u_{i}, i=1,,m,\]

where \(^{m}_{+}\) collects the relaxations \(u_{i} 0\). The value \(^{}()\) is known as the _perturbation function_ of (P) since it describes the effect of the relaxation \(\) on the optimal value. Given \(^{}()=^{}\) and (\(_{}\)) is equivalent to (P), this abuse of notation should not lead to confusion.

It is ready that \(^{}()\) is a componentwise non-increasing function, i.e., that for comparable arguments \(v_{i} w_{i}\) for all \(i\) (denoted \(\)), it holds that \(^{}()^{}()\). However, large relaxations \(\) drive (\(_{}\)) away from (P), the learning problem of interest. Thus, relaxing (P) too much can be as detrimental as relaxing it too little (see example in Appendix A). The goal of this paper is to exploit properties of \(^{}()\) together with a relaxation cost to strike a balance between these conflicting objectives. We call this balance a _resilient_ version of (P).

## 3 Resilient Constrained Learning

We formulate resilient constrained learning using a functional form of (\(_{}\)). Explicitly, consider a _convex_ function class \(_{}\) and define the relaxed functional problem

\[}^{}()=_{} _{(,y)_{0}}_{0 }(),y\] ( \[ _{(,y)_{i}}_{ i}(),y u_{i}, i=1,,m.\]

The difference between (\(_{}\)) and (\(}_{}\)) is that the latter does not rely on a parametric model. Instead, its solutions take values on the convex space of functions \(\). Still, if \(_{}\) is a sufficiently rich parameterization of \(\) (see Section 4 for details), then \(^{}()\) and \(}^{}()\) are close [38, Sec. 3]. Throughout the rest of the paper, we use \(\) to identify these functional problems.

The advantage of working with (\(}_{}\)) is that under mild conditions the perturbation function \(}^{}\) is convex. This holds readily if the losses \(_{i}\) are convex (e.g., quadratic or cross-entropy) [39, chap. 5]. However, the perturbation function is also convex for a variety of non-convex programs, e.g., (\(}_{}\)) with non-convex losses, non-atomic \(_{i}\), and decomposable \(\) (see Appendix B). For conciseness, we encapsulate this hypothesis in an assumption.

**Assumption 1**.: The perturbation function \(}^{}()\) is a convex function of the relaxation \(^{m}_{+}\).

A consequence of Assumption 1 is that the perturbation function \(}^{}\) has a non-empty subdifferential at every point. Explicitly, let its subdifferential \(}^{}(^{o})\) at \(^{o}^{m}_{+}\) be defined as the set of vectors describing supporting hyperplanes at \(^{o}\) of the epigraph of \(}^{}\), i.e.,

\[}^{}(^{o})=\, ^{m}_{+}\;\;}^{}() {}^{}(_{o})+^{T}(-_{o}), \,\;^{m}_{+}\,}.\] (1)

The elements of \(}^{}(^{o})\) are called _subgradients_ of \(}^{}\) at \(^{o}\). If \(}^{}\) is differentiable at \(^{o}\), then it has a single subgradient that is equal to its gradient, i.e., \(}^{}(^{o})=\{}^ {}(^{o})\}\). In general, however, the subdifferential is a non-singleton set . Further notice that since \(}^{}()\) is componentwise nonpositive the subgradients are componentwise negative, \(p_{}\).

Next, we use this property to formalize resilient constrained learning as a compromise between reducing \(}^{}()\) by increasing \(\) and staying close to the original problem (\(}_{}\)).

### Resilient Equilibrium

Consider the effect of increasing the value of a specific relaxation \(u_{i}\) in (\(}_{}\)) while keeping the rest unchanged. The solution of (\(}_{^{}}\)) is allowed to suffer higher losses on the constraints (\(_{i}\) for \(i 1\)), which may lead to a smaller objective loss (\(_{0}\)). Hence, while larger relaxations are detrimental because they violate more the requirements, they are also beneficial because they reduce the objective loss. To balance these conflicting outcomes of constraint relaxations, we introduce a function \(h\) to capture their costs. Then, since relaxing both increases the costs and decreases the objective value, we conceptualize resilience as an equilibrium between these variations.

**Definition 1** (**Resilient Equilibrium**).: Let \(h:_{+}^{m}_{+}\) be a convex, differentiable, normalized (i.e., \(h()=0\)), and componentwise increasing (i.e., \(h()<h()\) for \(\)) function. A resilient equilibrium of (\(}_{}\)) is a relaxation \(^{}\) satisfying

\[ h(^{})-}^{}(^{}).\] (2)

The resilient constrained learning problem amounts to solving (\(}_{^{}}\)), i.e., solving (\(}_{}\)) for a relaxation \(^{}\) that satisfies (2). We call this equilibrium _resilient_ because it describes how far (\(}_{}\)) can be relaxed before we start seeing diminishing returns. Indeed, \(^{}\) from (2) is such that relaxing by an additional \(\) would incur in a relaxation cost at least \( h(^{})^{T}\) larger, whereas tightening to \(^{}-\) would incur in an optimal value increase of at least the same \( h(^{})^{T}\).

Notice that resilient constrained learning is defined in terms of _sensitivity_. Indeed, the resilient equilibrium in Def. (1) specifies a learning task that is as sensitive to changes in its requirements as it is sensitive to changes in the relaxation cost. This has the marked advantage of being invariant to constant translations of \(_{0}\), as is also the case for solutions of (\(}_{}\)). Sensitivity also measures the difficulty of satisfying a constraint, since \(}^{}()\) quantifies the impact of each constraint specification on the objective loss. Hence, the equilibrium in (2) has the desirable characteristic of affecting stringent requirements more.

Two important properties of the equilibrium in Def. 1 are summarized next (proofs are provided in appendices D.1 and D.2).

**Proposition 1**.: _Under Ass. 1, the resilient equilibrium (2) exists. If \(h\) is strictly convex, it is unique._

**Proposition 2**.: _Let \(,_{+}^{m}\) be such that \([]_{i}=[]_{i}\), for \(i j\), and \([]_{j}<[]_{j}\). Under Ass. 1, (i) \([ h()]_{j}[ h()]_{j}\) and (ii) \([-_{v}]_{j}[-_{w}]_{j}\) for all \(_{v}}^{}()\) and \(_{w}}^{}()\)._

Prop. 1 shows that the equilibrium in (2) is well-posed. Prop. 2 states that, all things being equal, relaxing the \(j\)-th constraint increases the sensitivity of the cost \(h\) to it, while simultaneously decreasing its effect on the objective value \(}^{}\). To illustrate these points better, Fig. 1 considers prototypical learning problems with a single constraint [\(m=1\) in (\(}_{}\))], differentiable \(}^{}(u)\), and relaxation cost \(h(u)=u^{2}/2\). According to (2), the resilient relaxations are obtained at \(h^{}(u^{})=u^{}=}^{}(u^{})\), where we let \(g^{}(u)=dg(u)/du\) denote the derivative of the function \(g\). As per Prop. 2, \(h^{}\) is increasing and \(-}^{}\) is decreasing. Further observe that the sensitivity \(-}^{}(u)\) diverges as \(u\) approaches the value that makes the problem infeasible and vanishes as the constraint is relaxed. These two curves must therefore intersect, claimed by Prop. 1.

Figure 1: Resilient equilibrium from Def. 1 for \(h(u)=u^{2}/2\). The shaded area indicates infeasible specifications: (a) nominal specification (\(u=0\)) is feasible and easy to satisfy; (b) nominal specification is feasible but difficult to satisfy (close to infeasible); (c) nominal specification is infeasible.

The illustrations in Fig. 1 represent progressively more sensitive/difficult problems. In Fig. 0(a), the nominal problem (\(u=0\)) is easy to solve (small \(}^{}(0)\)), making the resilient equilibrium \(u_{a}^{*} 0\). The original problem and the relaxed problem are essentially equivalent. In Fig. 0(b), the nominal problem is difficult to solve (large \(}^{}(0)\)), inducing a significant change in the constraint and objective losses. In Fig. 0(c), the nominal problem is unsolvable, but the resilient relaxation recovers a feasible problem.

Having motivated the resilient compromise in Def. 1 and proven that it is well-posed, we proceed to obtain equivalent formulations that show it is also computationally tractable. These formulations are used to show traditional learning tasks that can be seen as resilient learning problems (Sec. 3.3), before deriving a practical algorithm to tackle the resilient learning problem (\(}_{^{}}\)) (Sec. 4).

### Equivalent Formulations

While we have shown that the resilient relaxation from Def. 1 exists (Prop. 1), the equilibrium in (2) does not provide a straightforward way to compute it. In this section, we show two more computationally amenable reformulations of Def. 1 by relating \(^{}\) to the Lagrange multipliers of (\(}_{}\)) and to the solution of a related optimization problem.

Let \(_{+}^{m}\) collect multipliers \(_{i}\) associated to the \(i\)-th constraint of (\(}_{}\)) and define the Lagrangian

\[(,;)=_{_{0}} _{0}(),y+_{i=1}^{m}_{ i}_{_{i}}_{i}(),y -u_{i}.\] (3)

Based on (3), define dual functions \(g(;)\) and dual problems \(}^{}()\) for given constraint level \(\),

\[}^{}()\ =\ _{ }\ g(;)\ =\ _{ }\ _{}\ (,;).\] ( \[}_{}\] )

While \(}^{}}^{}\) (weak duality) in general, there are cases in which \(}^{}=}^{}\) (strong duality), e.g., in convex optimization. The constrained (\(}_{}\)) is then essentially equivalent to (\(}_{}\)) that can be tackled by solving an unconstrained, penalized problem (minimizing (3) with respect to \(\) and \(\)), while adapting the weights \(_{i}\) of the penalties (maximizing (3) with respect to \(\)). This is, in fact, the basis of operation of primal-dual constrained optimization algorithms [39, Chapter 5]. The penalties \(^{}()\) that achieve this equivalence, known as _Lagrange multipliers_, are solutions of (\(}_{}\)) and subgradients of the perturbation function. Strong duality also holds under the milder Assumption 1 and a constraint qualification requirement stated next.

**Assumption 2**.: There exist a finite relaxation \(\) and a function \(\) such that all constraints are met with margin \(c>0\), i.e., \(_{_{i}}_{i}((),y) u_ {i}-c\), for all \(i=1,,m\).

Note that since \(\) can be large, this requirement is mild. We can thus leverage strong duality to provide an alternative definition of the resilient relaxation in (2).

**Proposition 3**.: _Let \(^{}()\) be a solution of (\(}_{}\)) for the relaxation \(\). Under Assumptions 1 and 2, \(}^{}(^{*})=}^{}(^{*})\) and any \(^{}_{+}^{m}\) such that \( h(^{})=^{}(^{})\) is a resilient relaxation of (\(}_{}\))._

See appendix D.3 for proof. Whereas Def. 1 specifies the resilience relaxation in terms of the marginal performance gains \(}}^{}\), Prop. 3 formulates it in terms of the penalties \(\). Indeed, it establishes that the penalty \(^{}(^{*})\) that achieves the resilient relaxation of (\(}_{}\)) is encoded in the relaxation cost as \( h(^{})\). That is not to say that \(h\) directly weighs the requirements as in fixed penalty formulations, since the equilibrium in Prop. 3 also depends on the relative difficulty of satisfying those requirements. Though Prop. 3 does not claim that _all_ resilient relaxations satisfy the Lagrange multiplier relation, this holds under additional continuity conditions on (\(}_{}\)) .

Prop. 3 already provides a more computationally amenable definition of resilient relaxation, given that Lagrange multipliers are often computed while solving (\(}_{}\)), e.g., when using primal-dual methods. Yet, an even more straightforward formulation of Def. 1 is obtained by rearranging (2) as \(0(}^{}+h)(^{})\), as we did in the proof of Prop. 1, which suggests that \(^{}\) is related to the minimizers of \(}^{}+h\). This is formalized in the following proposition (see proof in appendix D.4).

**Proposition 4**.: _A relaxation \(^{}\) satisfies the resilient equilibrium (2) if and only if it is a solution of_

\[}_{}^{}& =_{,\,_{+}^{m}}& _{(,y)_{}} _{0}(),y+h()\\ &&_{(,y) _{}}_{i}(),y  u_{i}, i=1,,m.\] ( \[}\] )

_The corresponding minimizer \(^{}\) is a resilient solution of the functional learning problem (\(}_{}\))._

Prop. 4 shows that it is possible to simultaneously find a resilient relaxation \(^{}\) and solve the corresponding resilient learning problem. Indeed, a resilient solution of a constrained learning problem can be obtained by incorporating the relaxation cost in its objective. This is reminiscent of first-phase solvers found in interior-point methods used to tackle convex optimization problems [39, Chap. 11]. Note, once again, that this is not the same as directly adding the constraints in the objective as penalties or regularizations. Indeed, recall from Def. 1 that the resilient relaxation balances the marginal effects of \(h\) on \(}^{}\) and not \(_{0}\). Before using Prop. 4 to introduce a practical resilient learning algorithm and its approximation and generalization properties (Sec. 4), we use these equivalent formulations to relate resilient learning to classical learning tasks.

### Relation to Classical Learning Tasks

**(Un)constrained learning**: Both traditional unconstrained and constrained learning can be seen as limiting cases of resilient learning. Indeed, if \(h 0\), \(\) has no effect on the objective of (\(}\)). We can then take \(u_{i}=B\) for \(i=1,,m\), which reduces (\(}\)) to an unconstrained learning problem (recall that all losses are \([-B,B]\)-valued). On the other hand, if \(h\) is the indicator function of the non-negative orthant (i.e., \(h()=\) for \(\) and \(h()=\), otherwise), then it must be that \(=\) in (\(}\)) as long as this specification is feasible. Neither of these relaxation costs satisfy the conditions from Def. 1, since they are not componentwise increasing or differentiable, respectively. Still, there exists valid relaxation costs that approximate these problems arbitrarily well (see Appendix D.5).

**Penalty-based methods**: Rather than the constrained formulations in (\(\)) or (\(}_{}\)), requirements are often incorporated directly into the objective of learning tasks using fixed penalties as in

\[*{minimize}_{}& \ _{_{}}_{0}( ),y+_{i=1}^{m}_{i}_{_{ }}_{i}(),y,\] (4)

where the fixed \(_{i}>0\) represent the relative importance of the requirements. It is immediate from Prop. 3 that resilient learning with a linear relaxation cost \(h()=_{i}_{i}u_{i}\) is equivalent to (4) as long as \(_{_{}}_{i}^{}( ),y 0\). From Def. 1, this is the same as fixing the marginal effect of relaxations on the perturbation function.

**Soft-margin SVM**: Linear relaxation costs are also found in soft-margin SVM formulations, namely

\[*{minimize}_{,\, _{+}^{m}}&\|\| ^{2}+_{i=1}^{m}u_{i}\\ & 1-y_{i}^{T}_{i}  u_{i}, i=1,,m.\] (PI)

Though written here in its parametrized form, the hypothesis class underlying (PI) (namely, linear classifiers) is convex. It can therefore be seen as an instance of (\(}\)) where each loss \(_{i}\) represent a classification requirement on an individual sample. Soft-margin SVM is therefore a resilient learning problem as opposed to its hard-margin version, where \(u_{i}=0\) for \(i=1,,m\).

## 4 Resilient Constrained Learning Algorithm

We defined the resilient equilibrium \(^{}\) in (2) and the equivalent resilient learning problems (\(}_{^{}}\)) and (\(}\)) in the context of the convex functional space \(\). Contrary to (\(_{}\)), that are defined on the finite dimensional \(_{}\), these problems are not amenable to numerical solutions. Nevertheless, we have argued that as long as \(_{}\) is a good approximation of \(\), the values of (\(_{}\)) and (\(}_{}\)) are close. We use this idea to obtain a practical primal-dual algorithm (Alg. 1) to approximate solutionsof (\(}\)-RES) (and thus (\(}_{^{}}\))). The main result of this section (Thm. 1) establishes how good this approximation can be when using only samples from the \(_{i}\).

Explicitly, consider a set of \(N\) i.i.d. sample pairs \((_{n,i},y_{n,i})\) drawn from \(_{i}\) and define the parametrized, empirical Lagrangian of the resilient learning problem (\(}\)-RES) as

\[_{}(,;)=h()+_{n=1}^{N}_{0}f_{}(_{n,0}),y_{n,0}+ _{i=1}^{m}_{i}_{n=1}^{N}_{i}f_{ }(_{n,i}),y_{n,i}-u_{i}.\] (5)

The parametrized, empirical dual problem of (\(}\)-RES) is then given by

\[}_{}^{}=_{_{+} ^{m}}\;_{,_{+}^{m}}\;_{ }(,;).\] (\(}\)-RES)

The gap between \(}_{}^{}\) and the optimal value \(}_{}^{}\) of the original problem (\(_{}\)) can be bounded under the following assumptions.

**Assumption 3**.: The loss functions \(_{i}\), \(i=0 m\), are \(M\)-Lipschitz continuous.

**Assumption 4**.: For every \(\), there exists \(^{}\) such that \(_{_{i}}|()-f_{^{}}( )|\), for all \(i=0,,m\).

**Assumption 5**.: There exists \((N,) 0\) such that for all \(i=0,,m\) and all \(\),

\[_{_{i}}_{i}(f_{}(),y) -_{n=1}^{N}_{i}f_{}(_{n,i}), y_{n,i}(N,)\]

with probability \(1-\) over draws of \(\{(_{n,i},y_{n,i})\}\).

Although the parameterized functional space \(_{}\) can be non-convex, as is the case for neural networks, Ass. 4 requires that it is rich in the sense that the distance to its convex hull is bounded. When \(\) exists for all \(>0\) and is a decreasing function of \(N\), Ass. (5) describes the familiar uniform convergence from learning theory. Such generalization bounds can be derived based on bounded VC dimension, Rademacher complexity, or algorithmic stability, to name a few .

We can now state the main result of this section, whose proof is provided in Appendix C.

**Theorem 1**.: _Consider \(}_{}^{}\) and \(^{}\) from (\(}\)-RES) and \(}_{}^{}\) from (\(}\)-RES). Under Ass. 1-5, it holds with probability of \(1-(3m+2)\) that_

\[}_{}^{}-}_{}^{}  h(^{}+ M)-h(^{})+M +(1+)(N,).\] (6)

Thm. 1 suggests that resilient learning problems can be tackled using (\(}\)-RES), which can be solved using saddle point methods, as in Alg. 1. Even if the inner minimization problem is non-convex, dual ascent methods can be shown to converge as long as its solution can be well approximated using stochastic gradient descent [38, Thm. 2], as is often the case for overparametrized NNs . A more detailed discussion on the algorithm can be found in Appendix E. Next, we showcase its performance and our theoretical results in two learning tasks.

Numerical Experiments

We now showcase the numerical properties of resilient constrained learning. As illustrative case studies, we consider federated learning under class imbalance and invariance constrained learning. Additional experimental results for both setups are included in Appendix F and H. We also include ablations on the hyperparameters of the method, including dual and perturbation learning rates, and the choice of the perturbation cost function \(h\).

### Heterogeneous Federated Learning

Federated learning  entails learning a common model in a distributed manner by leveraging data samples from different _clients_. Usually, average performance across all clients is optimized under the assumption that data from different clients is identically distributed. In practice, heterogeneity in local data distributions can lead to uneven performance across clients . Since this may be undesirable, a sensible requirement in this setting is that the loss of the model is _similar_ for all clients.

Let \(_{c}\) be the distribution of data pairs for Client \(c\), and \(R_{c}(f_{})=_{(,)_{c }}[(f_{}(),y)]\) its statistical risk. We denote the average performance as \((f_{}):=(1/C)_{i=1}^{C}R_{c}(f_{})\), where \(C\) is the number of clients. As proposed in  heterogeneity issues can be tackled by imposing a proximity constraint between the performance of each client \(R_{c}\), and the loss averaged over all clients \(\). This leads to the constrained learning problem

\[_{}  (f_{})\] (P-FL) s. to \[R_{c}(f_{})-(f_{})-  0, c=1,,C,\]

where \(>0\) is a small (fixed) positive scalar. It is ready to see that this problem is of the form in (P); see Appendix F. Note that the constraint in (P-FL) is an asymmetric (and relaxed) version of the equality of error rates common in fairness literature.

As shown by  this problem can be solved via a primal-dual approach in a privacy-preserving manner and with a negligible communication overhead, that involves sharing dual variables. However, because the heterogeneity of the data distribution across clients is unknown _a priori_, it can be challenging to specify a single constraint level \(\) for all clients that results in a reasonable trade-off between overall and individual client performance. In addition, the performance of all clients may be significantly reduced by a few clients whose poor performance make the constraint hard to satisfy. The heuristic adopted in  is to clip dual variables that exceed a fixed value.

In contrast, we propose to solve a resilient version of (P-FL) by including a relaxation \(u_{c},c=1,,C\) for each constraint and a quadratic relaxation cost \(h()=\|\|_{2}^{2}\). The resilient version of problem (P-FL) can also be solved in a privacy preserving manner as long as \(h()\) is separable in each of the constraint perturbations \(u_{c}\). In that case \(u_{c}\) can be updated locally by client \(c\).

Following the setup of , heterogeneity across clients is generated through class imbalance. More specifically, samples from different classes are distributed among clients using a Dirichlet distribution. Experimental and algorithmic details along with privacy considerations are presented in Appendix F.

**Constraint relaxation and relative difficulty:** Our approach relaxes constraints according to their relative difficulty. In the context of class imbalance, the majority classes exert a stronger influence on the overall objective (average performance). Therefore, the overall performance \(\) tends to favor smaller losses in the majority classes rather than in the minority ones. As a result, meeting the proximity constraint in (P-FL) for clients whose datasets contain a higher fraction of training samples from the minority classes can deteriorate performance. In Figure 2(left), we demonstrate that the constraint is effectively relaxed more for these clients.

**Controlling the performance vs. relaxation trade-off:** Through the choice of the relaxation cost function, we can control the trade-off between relaxing the equalization requirements from (P-FL) and performance. To illustrate this, we perform an ablation on the coefficient \(\) in the quadratic relaxation cost \(h()=\|\|_{2}^{2}\). As shown in Figure 2(middle) smaller values of \(\) enable better performance at the cost of larger relaxations. As \(\) increases, the relaxations vanish and the problem approaches the original constrained problem. In this manner, the resilient approach enables navigating this trade-offby changing a single hyperparameter. Still, the optimal relaxation for each client is determined by their local data distribution, i.e., the relative difficulty of the constraint. In Appendix F.4 we also perform an ablation on the cost function by changing the perturbation norm.

**Constraint violation and generalization:** Relaxing stringent requirements not only makes the empirical problem easier to solve, but it can also lead to a better empirical approximation of the underlying statistical problem. As shown in Figure 2(right), the resilient approach has a smaller fraction of clients that are infeasible at the end of training. In addition, when constraints are evaluated on the test set, larger constraint violations are observed for some (outlier) clients in the hard constrained approach (\(u_{c}=0\)). In Appendix F.4 we show that this holds across different problem settings. In addition, we also illustrate the fact that this is partly due to generalization issues, i. e., that overly stringent requirements can harm generalization. This observation is in line with the constrained learning theory developed in .

**Comparing client performances:** Our method should sacrifice the performance of outliers-which represent hard to satisfy constraints-in order to benefit the performance of the majority of the clients. In order to showcase this, we sort clients according to their test accuracy \(_{}_{}_{[c]}\). We report the fraction of clients in the resilient method that outperform equally ranked clients for the constrained baseline \((_{[c]}^{}_{[c]}^{})\), as well as the average and maximum increases and decreases in performance. As shown in Table 1, the majority of clients achieve a small increase in performance, while a small fraction experiences a larger decrease. In Appendix F.4 we include plots of ordered accuracies to further illustrate this.

### Invariance Constrained Learning

As in , we impose a constraint on the loss on transformed inputs, as a mean of achieving robustness to transformations which may describe invariances or symmetries of the data. Explicitly,

\[_{} _{(,y)}[(f_{ }(),y)]\] (P-IC) s. to \[_{(,y)}[_{g _{i}}(f_{}(g),y)]- 0,  i=1,,m,\]

   Dataset & Imbalance Ratio & Improved over constrained (\%) & Max Improvement & Max Deterioration \\   & 10 & 77 & 1.5 & 10.0 \\  & 20 & 79 & 2.1 & 9.1 \\   & 10 & 92 & 4.8 & 23.3 \\  & 20 & 94 & 2.6 & 19.4 \\   

Table 1: Changes in accuracy for equally ranked clients for the resilient method compared to the constrained baseline . We report the fraction of equally ranked clients which improved their accuracy, along with the mean and maximum change across all clients that improve (imp.) and decrease (dec.) their accuracy, respectively. Performance improves for most clients, although at the cost of a decrease in performance for a few outliers

Figure 2: (Left) Constraint relaxation and relative difficulty for federated learning under heterogeneous class imbalance across clients (crosses). We plot the perturbation \(u_{c}\) against the fraction of the dataset of client \(c\) from the minority class, which is associated to how difficult it is to satisfy the constraint, since minority classes typically have higher loss. (Middle) Relaxation cost parameter (\(h()=\|\|_{2}^{2}\)) vs. final training loss and perturbation norm. (Right) Constraint violations on train and test sets

where \(_{i}\) is a set of transformations \(g:\) such as image rotations, scalings, or translations. The constraint can be approximated by sampling augmentations using MCMC methods (see  for details). However, it is challenging to specify the transformation sets \(_{i}\) and constraint levels \(_{i}\) adequately, since they depend on the invariances of the unknown data distribution and whether the function class is sufficiently expressive to capture these invariances. Therefore, we propose to solve a resilient version of problem (P-IC) using \(h()=\|\|_{2}^{2}\) as a perturbation cost.

We showcase our approach on datasets with artificial invariances, following the setup of . Explicitly, we generate the synthetic datasets, by applying either rotations, translations, or scalings, to each sample in the MNIST  and FashionMNIST  datasets. The transformations are sampled from uniform distributions over the ranges detailed in Appendix H.

**Constraint relaxation and relative difficulty** As shown in Figure 3(left) the relaxations (\(\)) associated with synthetic invariances are considerably smaller than for other transformations. This indicates that when the transformations in the constraint correspond to a true invariance of the dataset, the constraint is easier to satisfy. On the other hand, when the transformations are not associated with synthetic invariances of the dataset, they are harder to satisfy and thus result in larger relaxations. This results in smaller dual variables, as shown in Figure 3 (right).

**Resilience Improves Performance** The resilient approach is able to handle the misspecification of invariance requirements, outperforming in terms of test accuracy both the constrained and unconstrained approaches. We include results for MNIST in Table 2 and F-MNIST in Appendix H. In addition, though it was not designed for that purpose, our approach shows similar performance to the invariance learning method Augerino .

## 6 Conclusion

This paper introduced a method to specify learning constraints by balancing the marginal decrease in the objective value obtained from relaxation with the marginal increase in a relaxation cost. This resilient equilibrium has the effect of prioritizing the relaxation of constraints that are harder to satisfy. The paper also determined conditions under which this equilibrium exists and provided an algorithm to automatically find it during training, for which approximation and statistical guarantees were derived. Experimental validations showcased the advantages of resilient constrained learning for classification with invariance requirements and federated learning. Future work includes exploring different relaxation costs and applications to robustness against disturbances and outliers.

   Dataset & Method & Rotated (180) & Rotated (90) & Translated & Scaled & Original \\   & Augerino & \(\) & \(96.38 0.00\) & \(94.65 0.01\) & \(97.53 0.00\) & \(98.44 0.00\) \\  & Unconstrained & \(94.49 0.12\) & \(96.25 0.13\) & \(94.64 0.20\) & \(97.47 0.03\) & \(98.45 0.06\) \\   & Constrained & \(94.55 0.18\) & \(96.90 0.07\) & \(93.74 0.07\) & \(97.92 0.15\) & \(98.74 0.08\) \\   & Resilient & \(95.38 0.18\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Classification accuracy for synthetically invariant MNIST. We use the same invariance constraint level \(_{i}=0.1\) for all transformations. We include the invariance learning method Augerino  as a baseline.

Figure 3: (Left) Constraint Relaxation and Relative difficulty for Synthetically invariant datasets. Bars denote the perturbation \(_{i}\) associated with different transformations, and the x-axis shows the synthetic invariance of the dataset. (Right) Sensitivity (Dual variables) for Synthetically invariant datasets. Bars correspond to the values of dual variables (for all constraints) at the end of training. We compare the resilient and constrained approach across different synthetic datasets.