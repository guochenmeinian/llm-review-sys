# Few-Shot Diffusion Models Escape the Curse of Dimensionality

Ruofeng Yang\({}^{1}\), Bo Jiang\({}^{1}\), Cheng Chen\({}^{2}\), Ruinan Jin\({}^{34}\),

**Baoxiang Wang\({}^{34}\), Shuai Li\({}^{*}\)\({}^{1}\)**

\({}^{1}\) John Hopcroft Center Computer Science, Shanghai Jiao Tong University

\({}^{2}\) East China Normal University

\({}^{3}\) The Chinese University of Hong Kong, Shenzhen \({}^{4}\) Vector Institute

{wanshuiyin, bjiang, shuaili8}@sjtu.edu.cn,

chchen@sei.ecnu.edu.cn, {jinruinan,bxiangwang}@cuhk.edu.cn

Corresponding author

###### Abstract

While diffusion models have demonstrated impressive performance, there is a growing need for generating samples tailored to specific user-defined concepts. The customized requirements promote the development of few-shot diffusion models, which use limited \(n_{ta}\) target samples to fine-tune a pre-trained diffusion model trained on \(n_{s}\) source samples. Despite the empirical success, no theoretical work specifically analyzes few-shot diffusion models. Moreover, the existing results for diffusion models without a fine-tuning phase can not explain why few-shot models generate great samples due to the curse of dimensionality. In this work, we analyze few-shot diffusion models under a linear structure distribution with a latent dimension \(d\). From the approximation perspective, we prove that few-shot models have a \((n_{s}^{-2/d}+n_{ta}^{-1/2})\) bound to approximate the target score function, which is better than \(n_{ta}^{-2/d}\) results. From the optimization perspective, we consider a latent Gaussian special case and prove that the optimization problem has a closed-form minimizer. This means few-shot models can directly obtain an approximated minimizer without a complex optimization process. Furthermore, we also provide the accuracy bound \((1/n_{ta}+1/})\) for the empirical solution, which still has better dependence on \(n_{ta}\) compared to \(n_{s}\). The results of the real-world experiments also show that the models obtained by only fine-tuning the encoder and decoder specific to the target distribution can produce novel images with the target feature, which supports our theoretical results.

## 1 Introduction

In recent years, diffusion models have shown an excellent ability to generate diverse, high-quality samples and show state-of-the-art performance in many areas with large-scale, standard datasets (Rombach et al., 2022; Ho et al., 2022; Li et al., 2024; Blattmann et al., 2023; Li et al., 2023; Liu et al., 2024; Li et al., 2024). However, users often desire to generate samples that resemble the ones they provide, such as images related to their families, daily lives, or specific items. These user-provided samples are typically limited in number and do not appear frequently in large-scale datasets. Consequently, training a diffusion model from scratch using such limited, personalized samples often results in poor performance. To cater the customized requirements of users, few-shot diffusion models attract much attention. Few-shot diffusion models aim to fine-tune a pre-trained diffusion model using a limited amount of data (\(5 10\) samples), and they have recently deliveredimpressive results in various domains, including image generation (Ruiz et al., 2023; Han et al., 2023; Zhu et al., 2023), video generation (Chen et al., 2023b), and the medical domain (Dutt et al., 2023).

Before the fine-tuning phase, we first need to train a diffusion model on the large source dataset \(\{X_{s,i}\}_{i=1}^{n_{s}}\) as the pre-trained model. A diffusion model consists of a forward process and a reverse process (Song et al., 2020). The forward process gradually converts the data distribution into Gaussian noise. The reverse process sequentially removes the noise in the data to generate samples, which relies on the gradient of logarithmic forward process density (a.k.a. score function). To run the reverse process, diffusion models use a neural network to approximate the unknown score function.

With a pre-trained diffusion model, the paradigm to obtain a few-shot diffusion model is to fine-tune the model using a limited target dataset \(\{X_{ta,i}\}_{i=1}^{n_{ta}}\). In earlier times, fully fine-tuned methods, such as DreamBooth (Ruiz et al., 2023), provided an important boost for developing few-shot models. However, they also show that the diffusion models suffer from the overfitting and memory phenomenon when fine-tuning all parameters. Furthermore, a fully fine-tuned method is both memory and time inefficient (Xiang et al., 2023). To avoid the above problems, many works freeze most parameters and fine-tune some key parameters, such as cross-attention layers (Kumari et al., 2023; Moon et al., 2022), some concept neurons (Liu et al., 2023) or text-embedding (Gal et al., 2022), to approximate the ground-truth target score function. These works not only preserve the prior information but also have a lower requirement for the target dataset size, which is more practical for applications. Hence, we aim to explain the great performance of these models in this work.

Despite the empirical success, no existing theoretical work specifically analyzes the approximation bound for few-shot diffusion models, and the following question remains open:

_Do few-shot diffusion models with a fine-tuning phase enjoy a small approximation error with a limited target dataset?_

For the approximation error bound, some works currently analyze diffusion models without a fine-tuning phase (Oko et al., 2023; Chen et al., 2023c; Yuan et al., 2023; Li et al., 2023c). Importantly, when analyzing general, bounded data, these works suffer from the curse of dimensionality. More specifically, Oko et al. (2023) analyze bounded distribution and show the \(n_{s}^{-s^{}/D}\) approximation bound, where \(D\) is the data dimension of \(X_{s}\). Chen et al. (2023c) analyze linear structure distribution \(X_{s}=A_{s}Z\) with subgaussian latent variable \(Z^{d}\) and achieve \(n_{s}^{-2/d}\) results. Since the source dataset size is large enough, the influence of dimension is tolerable. However, for the limited target dataset, if trivially using the above technique, the bound is \(n_{ta}^{-1/D}\) or \(n_{ta}^{-2/d}\), which is large and can not explain why few-shot diffusion models efficiently approximate the target score function.

In this work, for the first time, we propose the approximation bound specifically to few-shot diffusion models with a fine-tuning phase and prove that the few-shot diffusion model can escape the curse of dimensionality. More specifically, we show that when assuming (1) linear structure data and (2) the source and the target data share latent distribution, the few-shot diffusion models with a fine-tuning phase achieve \((n_{s}^{-2/d}+n_{ta}^{-1/2})\) approximation error bound, which makes the first step to explain why few-shot diffusion models have great performance in the application. Generally speaking, due to the component \(n_{ta}^{-1/2}\), the few-shot diffusion only needs a few target samples to achieve the same bound compared to \(n_{s}^{-2/d}\). To support our augmentation, we calculate the requirement of \(n_{ta}\) to obtain an accurate enough approximated target score function in popular datasets. Table 1 shows that the requirement of \(n_{ta}\) is about \(5 10\) samples, matching the customized diffusion model requirement. We also do experiments on the real-world datasets and show that \(10\) target images are enough for few-shot models to generate novel images with the target feature (see Section 6).

After directly using the property of the minimizer to obtain an approximation bound, we analyze how to optimize the few-shot diffusion models to obtain a minimizer. Since the score-matching objective function is highly non-convex, only a few works analyze the optimization problem of diffusion models (Shah et al., 2023; Bruno et al., 2023; Cui et al., 2023; Li et al., 2023c). Furthermore, these works either require (1) an exponential size neural network (Li et al., 2023c) or (2) a distribution determined by one variable (Shah et al., 2023; Bruno et al., 2023; Cui et al., 2023) to simplify the optimization problem. This work proves that few-shot diffusion models can simplify the optimization problem without these requirements. When analyzing the optimization problem, we focus on a Gaussian latent variable special case 2. Then, we prove that the expected few-shot objective function has a closed-form minimizer, which means the empirical solution can be directly obtained without a complex optimization process. We also prove the accuracy bound \((1/n_{ta}+1/})\) of empirical closed-form solution, which still has better dependence on the target dataset. In conclusion, we accomplish the following results for few-shot diffusion models under linear structure distribution:

* For the approximation bound, we consider a subgaussian latent variable and prove \((n_{s}^{-2/d}+n_{ta}^{-1/2})\) bound for few-shot models, which is better than \(n_{ta}^{-2/d}\) result without fine-tuning.
* For the optimization problem, we consider a latent Gaussian special case and prove that the expected few-shot objective function has a closed-form minimizer. Furthermore, we prove the accuracy bound \((1/n_{ta}+1/})\) for the empirical closed-form solution.
* To support our theoretical results, we do real-world experiments and show that the models obtained by only fine-tuning specific encoder and decoder can use only \(10\) target images to generate novel images with the target feature.

## 2 Related Work

**The approximation error bound.** Recently, some works analyze the approximation error bound of diffusion models without a fine-tuning phase. Oko et al. (2023) analyze \(s^{}\)-order bounded derivatives distribution and show the approximation error bound is \(n_{s}^{-s^{}/D}\). Chen et al. (2023c) analyze distribution with linear structure and subgaussian latent variable and show that the \(n_{s}^{-2/d}\) result. The approximation error bound of the above works suffers the curse of (latent) dimensionality. To avoid this phenomenon, some works analyze special data distributions. Shah et al. (2023) and Cui et al. (2023) analyze the mixture of Gaussian (MOG) with known variance and achieve a \(1/n_{s}\) approximation bound. Yuan et al. (2023) analyze linear structure distribution with Gaussian latent variable and achieve \(1/}\) result. Mei and Wu (2023) analyze Ising models and prove that the term corresponds to \(n_{s}\) is \(1/}\). However, the remaining terms do not converge to \(0\) when \(n_{s}\) goes to \(+\). For general bounded data distribution, Li et al. (2023c) provide a \(n_{s}^{-2/5}\) approximation error bound. However, they use a 2-layer random feature network and only allow the second linear layer to be trainable. Hence, the network size is \()}\) compared to Poly\((n_{s})\) size of all previous works.

**The optimization of diffusion models.** Since the score matching objective function is highly non-convex, only a few works analyze how to optimize it to obtain a minimizer (Shah et al., 2023; Cui et al., 2023; Bruno et al., 2023; Li et al., 2023c). These works either make assumptions about data distribution or network size to guarantee only one optimization variable, leading to a simpler optimization problem. For special data distributions, Bruno et al. (2023) and Cui et al. (2023) analyze a Gaussian with fixed variance and a 2-mode mixture of Gaussian (MOG) with equal, trainable mean and fixed variance, respectively. Shah et al. (2023) analyze a multi-mode MOG with a fixed variance and prove a local convergence guarantee. Since they assume the distance between any two modes is large enough and a good enough initialization, the optimization problem is similar to optimizing a Gaussian distribution. For the large neural network size, Li et al. (2023c) analyze a general, bounded distribution with a 2-layer NN. Note that they require \()}\) hidden neurons and only allow the linear layer to be trainable, which also leads the optimization problem to a convex optimization.

## 3 The Introduction of Few-shot Diffusion Models

With pre-trained models, the paradigm to obtain a few-shot diffusion model is to freeze most parameters and fine-tune some key parameters corresponding to the target data distribution. Since the analysis of few-shot diffusion models relies heavily on the pre-trained model, this section first provides a concise overview of the fundamental concepts and notations associated with diffusion models. Then, we introduce the paradigm of few-shot diffusion models in Section 3.2.

### The Forward and Reverse Process

Let \(q_{0}\) be the data distribution. Given \(X_{0} q_{0}^{D}\), non-decreasing function \(f(X_{t},t)\) and \(g(t)\), the forward process is defined by:

\[X_{t}=f(X_{t},t)t+g(t)B_{t}\,,\]

where \(\{B_{t}\}_{t[0,T]}\) is a \(D\)-dimensional Brownian motion. In this work, we choose \(f(X_{t},t)=-1/2X_{t}\) and \(g(t)=1\), which corresponds to variance preserving (VP) forward process and is widely used in practice 3(Shah et al., 2023; Song et al., 2020). Let \(q_{t}\) be the density function of \(X_{t}\). Once a forward process is chosen, the conditional distribution of \(X_{t}|X_{0}\) is \(q_{t}(X_{t}|X_{0})=(m_{t}X_{0},_{t}^{2}I_{D})\), where \(m_{t}=e^{-t/2},_{t}^{2}=1-e^{-t}\). Note that when \(t\) goes to \(+\), \(q_{t}\) converges to \((0,I_{D})\), which is helpful in choosing the initial distribution for the sampling process.

To generate samples, diffusion models reverse the forward SDE and run the reverse process. Since the reverse process contains the gradient of forward logarithmic density \( q_{t}()\) (a.k.a. score function), the model approximates it by using a neural network \((,t)\) and the score matching objective function (see Section 3.2). After that, diffusion models discretize the continuous reverse process to obtain an implementable algorithm. Let \(t_{0} t_{1} t_{K}=T\) be the discretization points in the forward time and \(h_{k}=t_{k}-t_{k-1}\) be the \(k\)-th stepsize. When considering the reverse time, we define \(t^{}_{k}=T-t_{K-k}\). In this work, we choose the exponential integrator (EI) discretization scheme, which has great performance (Zhang and Chen, 2022). The EI discretization freezes the approximated score at \(t^{}_{k}\) and runs the following process in the reverse time:

\[_{t}=[f(_{t},T-t)+g(T-t)^{2}( _{t^{}_{k}},T-t^{}_{k})]t+g(T-t) B_{t}\,,t[t^{}_{k},t^{}_{k+1}]\,,\]

where \(_{0}(0,I_{D})\) due to the stationary distribution of the forward process.

While the discretization complexity \(K\) has been well-studied with an accurate enough score function (Benton et al., 2023; Li et al., 2023), there is a lack of analysis for the score-matching process. Therefore, this work focuses on the score approximation and the optimization problem of the few-shot score-matching objective function.

### The Score Matching Objective Function

In this work, we specifically analyze few-shot diffusion models, which involve two datasets: (1) the source dataset \(\{X_{s,i}\}_{i=1}^{n_{s}}\); (2) the target dataset \(\{X_{ta,i}\}_{i=1}^{n_{ta}}\). The approach involves first training a pre-trained diffusion model on the source dataset and then freezing the backbone network to fine-tune the diffusion models on the target dataset.

For data distributions, we assume that the source distribution \(q^{s}\) and the target distribution \(q^{ta}\) are both supported on a low-dimensional linear subspace. The low-dimensional structures have been discovered on many popular image datasets (Pope et al., 2021; Gong et al., 2019; Tenenbaum et al., 2000) due to the locally connected and symmetrical property, and it is crucial for diffusion models. For image generation, current popular diffusion models, such as Stable Diffusion (Rombach et al., 2022), transform images to a latent space and run diffusion models in the latent space. Except for the image generation, Chen et al. (2024) recently show the latent dimension plays an important role in diffusion models to work well in self-supervised learning, and linear subspace is enough.

We further assume that the source and target data share the same latent distribution. Note that this is a common assumption in few-shot learning. In particular, previous theoretical works in the context of supervised few-shot learning often assume that the source and target distributions have a common latent representation (Du et al., 2020; Chua et al., 2021; Meunier et al., 2023).

**Assumption 3.1**.: The source datapoints \(X_{s}\) and target datapoint \(X_{ta}\) admit a low dimensional linear structure and share the same latent distribution \(X_{s}=A_{s}Z\) and \(X_{ta}=A_{ta}Z\) where \(A_{s},A_{ta}^{D d}\) with orthonormal columns and \(Z q_{z}^{d}\).

As mentioned in Chen et al. (2023c), when assuming linear distribution, the ground-truth score function is decomposed into the latent score function \( q_{t}^{}(Z^{})\) and linear encoder and decoder:

\[ q_{t}^{s}(X)=A_{s} q_{t}^{}(A_{s}^{}X )-^{2}}(I_{D}-A_{s}A_{s}^{})X\,,\]

where \(q_{t}^{}(Z^{})= q_{t}(Z^{}|Z) q_{z}(Z)Z\) and \(q_{t}(|Z)=(m_{t}Z,_{t}^{2}I_{d})\). This form indicates that the diffusion process happens in the latent subspace. A conceptual way to approximate the score function is to minimize the following loss on a function class \(_{NN}\):

\[_{s_{NN}}_{0}^{T}w(t)_{X_{t} q_{t}^{s}} \| q_{t}^{s}(X_{t})-(X_{t},t) \|_{2}^{2}t\,,\]

where \(w(t)\) is a weight function. However, the above objective function is intractable since \( q_{t}()\) is unknown. Vincent (2011) propose the following implementable loss:

\[_{s}()=_{0}^{T}w(t)_{X_{0}}[_{X_{t}|X_{0}}\| q_{t}^{s}(X_{t}|X_{0})- (X_{t},t)\|_{2}^{2}]t\,.\]

Due to the forward process, \( q_{t}^{s}(X_{t}|X_{0})\) has an analytical form and is equal to \(-(X_{t}-m_{t}X_{0})/_{t}^{2}\). Vincent (2011) also prove that this objective function only has a constant difference compared to the above one. The empirical loss with the source datasets \(\{X_{s,i}\}_{i=1}^{n_{s}}\) is defined by:

\[_{_{V,}_{NN}}}_{s}( _{V,})=(T-)}_{i=1}^{n_{s}}_{ }^{T}_{t}^{s}(X_{s,i};_{V,})t\,,\] (1)

where

\[_{t}^{s}(X_{s,i};)=_{X_{t}|X_{0}=X_{s,i}} [\| q_{t}^{s}(X_{t}|X_{0})-(X_{ t},t)\|_{2}^{2}]\,,\]

and

\[_{}=\{_{V,}(X,t)=^{2}}V_{}(V^{}X,t)-^{2}}X:\!V^{D d}\,,\] \[_{}:^{d}[,T]^{d} \,\,\}\,.\]

Note that we take \(w(t)=1/(T-)\) for simplicity, where \(\) is the early stopping parameter to avoid the blow-up phenomenon of score functions at the end of reverse process. Furthermore, we take the integral over the forward time instead of discretizing the timeline since \(X_{t}\) is easy to generate.

The linear encoder and decoder structure and the shortcut connection in \(_{NN}\) is due to the form of the ground-truth score function. The specific parameters for \(_{}\), such as its length and width, are identical to those used in Chen et al. (2023c). Generally, with a given network accuracy parameters \(\), the network size is \((1/)\). We show the parameter of neural network in Appendix A.

The diffusion models minimize the empirical loss to obtain a pre-trained approximated score function. Let the minimizer of Equation (1) be \((_{s},)\). Chen et al. (2023c) show that \((_{s},)\) leads a \(n_{s}^{-2/d}\) approximation error bound. If trivially replacing \(n_{s}\) with \(n_{ta}\), we obtain a \(n_{ta}^{-2/d}\) bound for the target dataset without the fine-tuning phase. Note that this bound suffers from the influence of the latent dimension \(d\), which is still large in popular datasets (Table 1). In the next paragraph, we introduce the few-shot diffusion models with a fine-tuning phase and show that the dependence on \(n_{ta}\) is \(n_{ta}^{-1/2}\) in the error bound (Theorem 4.3).

**The Few shot Diffusion Models with a Fine-tuning Phase.** Since the source and target distribution share the same latent data distribution, we freeze \(\) and only fine-tune the low-rank linear encoder and decoder layer \(V_{ta}\). This method can significantly reduce the fine-tuning parameters and is similar to LoRA (Hu et al., 2021), which also fine-tunes two low-rank matrices and is widely used in fine-tuning the stable diffusion (Rombach et al., 2022).

Let \(_{t}^{ta}\) be the loss function of the target dataset at time \(t\), which has similar definition compared to \(_{t}^{s}\). The optimization problem for the target dataset is

\[_{_{V_{ta},}_{NN}()} {}_{ta}(_{V_{ta},})=(T-)} _{i=1}^{n_{ta}}_{}^{T}_{t}^{a}(X_{ta,i};_{V_{ ta},})t\,,\]where

\[_{}()=\{_{V,}(X,t)=^{2}}V_{}(V^{}X,t)-^ {2}}X:V^{D d}\},\]

Similarly, we define the minimizer of the few-shot objective function as \((_{ta},)\).

**Notations.** We denote by \(I_{D}\) the \(D\)-dimensional identity matrix. For \(X^{D}\) and \(A^{D d}\), we denote by \(\|X\|_{2}\) the Euclidean norm for vector and \(\|A\|_{F}\) the Frobenius norm for matrix. We denote by \(\|X\|_{L^{2}(q)}^{2}\) the expectation of \(X\) in \(L_{2}\) norm \(_{X q}[\|X\|_{2}^{2}]\).

## 4 Few-shot Diffusion Models Enjoy Better Approximation Error Bound

In this section, we show that few-shot diffusion models with a fine-tuning phase escape the curse of latent dimensionality and have a \((n_{s}^{-2/d}+n_{ta}^{-1/2})\) approximation bound 4. This result makes the first step to explain why few-shot models have great performance with a limited target dataset.

Before showing our results, we first introduce standard assumptions on the latent distribution and the on-support ground-truth score function. We first assume that \(Z\) has a subgaussian tail and the minimum eigenvalue of \(Z\) is lower bound by \(c_{0}\), also used in Chen et al. (2023c).

**Assumption 4.1**.: \(q_{z}>0\) is twice continuously differentiable, \(_{}([ZZ^{}]) c_{0}\) and \(\|Z\|_{2}^{2} C_{Z}\). Moreover, there exist positive constants \(B,C_{1},C_{2}\) such that when \(\|Z\|_{2} B\), \(q_{z}(Z)(2)^{-d/2}C_{1}(-C_{2}\|Z\|_{2}^{2}/2)\).

**Assumption 4.2**.: The on-support ground truth score \(A_{s} q_{t}^{}(Z)\) and \(A_{ta} q_{t}^{}(Z)\) is \(\)-Lipschitz in \(Z^{d}\) for any \(t[0,T]\).

Note that different from previous works directly assume \( q_{t}()\) is Lipschitz (Chen et al., 2022, 2023d), the \(\)-Lipschitz on-support score function assumption does not conflict with the blow-up phenomenon when \(t\) goes to \(0\) due to the existence of \((I_{D}-AA^{})X/_{t}^{2}\). With these assumptions, we prove the approximation bound for few-shot models with a fine-tuning phase.

**Theorem 4.3**.: _Let \((n)=\), \(F=)d^{2}^{2}}{^{2}c_{0}}\) and network parameter \(=n_{ta}^{-1/2}\). Assume Assumption 3.1, 4.1, 4.2 and \(n_{ta}^{))}} n_{s}\). Then, with probability \(1-_{1}\), the following inequality holds (hiding logarithmic factors)_

\[_{}^{T}\|_{_{ta}, }(,t)- q_{t}^{ta}()\|_{L^{2}(q_{t}^ {ta})}^{2}t((Dd^{3}}{ (T-)}}+Fn_{s}^{-)}{d +5}})(}))\,.\]

The dependence of \(\) is due to the blow-up property of the score function. Note that when \(n_{s}\) is sufficiently large, \((n_{s})\) is negligible. Then, the approximation error bound for few-shot diffusion models is \((n_{s}^{-2/d}+n_{ta}^{-1/2})\). Compared to the approximation error bound \(n_{ta}^{-2/d}\) without a few-shot phase, it is clear that few-shot diffusion models escape the curse of the (latent) dimensionality.

_Remark 4.4_ (The discussion on the coefficient in Theorem 4.3).: The goal of the fine-tuning phase is to achieve the same order error bound compared with the pre-trained models, which means that we consider the relative relationship between \(n_{ta}\) and \(n_{s}\). Hence, if the coefficient of \(n_{ta}\) and \(n_{s}\) has the same order, we can only consider \(1/}\) and \(n_{s}^{-2/d}\). To support the above augmentation, we calculate the coefficient of \(n_{s}\) and \(n_{ta}\) in detail. The dominated term of coefficient for \(n_{ta}\) and \(n_{s}\) are \(Dd^{3}/\) and \(d^{3}/(^{2}c_{0})\), respectively. The classic choice for the early stopping parameter \(\) and

  Dataset & CIFAR-10 & CIFAR-100 & CelebA & MS-COCO & ImageNet \\  Dataset Size & \(6 10^{4}\) & \(6 10^{4}\) & \(2 10^{5}\) & \(3.3 10^{5}\) & \(1.2 10^{6}\) \\  Latent Dimension & 25 & 22 & 24 & 37 & 43 \\  The Requirement of \(n_{ta}\) & 6 & 8 & 8 & 5 & 5 \\  

Table 1: The requirement of \(n_{ta}\) in popular datasets. We use latent dimension in Pope et al. (2021).

forward time \(T\) are \(10^{-3}\) and 10, respectively (Karras et al., 2022). Then, with \(D=256 256 3\) as an example 5, \(Dd^{3}/=d^{3} 20 10^{6}\) and \(d^{3}/(^{2}c_{0})=d^{3} 10^{6}/c_{0}\), which has the same order. Hence, we consider the relative relationship between \(1/}\) and \(n_{s}^{-2/d}\).

### Discussion on the Approximation Bound

**The relationship to empirical phenomenon.** In applications, current few-shot diffusion models only require \(5 10\) target images to achieve great performance. Theorem 4.3 makes the first step to explain why the few-shot diffusion models have great performance with a limited target \(n_{ta}\). More specifically, with known source dataset size \(n_{s}\) and the corresponding latent dimension \(d\), we can calculate the inequality \(n_{ta}^{))}} n_{s}\)6 to obtain the requirement of \(n_{ta}\) to achieve the same accuracy compared to the pre-trained diffusion models. Combined with the latent dimension of popular datasets (Pope et al., 2021), Table 1 shows the requirement of \(n_{ta}\). It is clear that we only need less than \(10\) target images to obtain an accurate enough few-shot diffusion model that matches the performance in reality. The real-world experiments also support our discussion (Section 6).

Table 1 shows that the requirement of \(n_{ta}\) is heavily influenced by the latent dimension \(d\). When \(d\) is large (e.g. ImageNet), the approximation bound of pre-trained models is influenced by latent dimension and has a large approximation error even with large-size source data. We only need a few target data to achieve the same error in this setting. When \(d\) is small (e.g. CIFAR-10), pre-trained models have a small approximation error. We need a slightly larger target data size.

**The approximation error of the fully fine-tuned method.** As shown in our real experiment Section 6 and DreamBooth (Ruiz et al., 2023), when fine-tuning all parameters with a small target dataset, models tend to overfit and lose the prior information from the pre-trained model. In our theorem, this phenomenon means that in the fine-tuning phase, the model does not use \(\) learned by the pre-trained model and achieves a \(n_{ta}^{-2/d}\) approximation error bound, which suffers from the curse of dimensionality. From an intuitive perspective, the probability density function (PDF) of a distribution learned by an overfitting model is only positive at the interval around the target dataset, which is far away from the PDF of true distribution and leads to a large error term. We also note that it is possible to avoid this phenomenon by using a specific loss (Ruiz et al., 2023) or carefully choosing the optimization epochs (Li et al., 2023c). We leave them as interesting future works.

**Proof sketch.** The first step is to prove that in \(_{NN}()\), there exists a solution \((_{ta},)\) has the following inequality (only focusing on \(n_{s}\) and \(n_{ta}\))

\[_{}^{T}\|_{_{ta},}(X,t)- q_{t}^{ta}(X)\|_{L^{2}(q_{t}^{a})}^{2}t O(^{2}+n_{s}^{-)}{d+5}}(}))\,.\]

To prove the above inequality, we first do the following decomposition:

\[\|_{_{ta},}(,t)- q_{t}^ {ta}()\|_{2}^{2}+\|_{_{ta}, }(,t)-_{_{ta},}(,t)\|_{2}^{2},\]

where \((_{ta},)_{NN}\) is a constructed solution. The first term is due to the accuracy of the constructive neural network with network accuracy parameter \(\). For the second term, since the latent score function is shared and few-shot diffusion models directly use \(\), it is control by the approximation bound of the pre-trained diffusion models. Then, by using the inequality

\[_{s_{V_{ta},}()}}_{ta}(_{V_{ta},}) }_{ta}(_{_{ta},} )\,,\]

we build the bridge between \(_{_{ta},}\) and \(_{_{ta},}\).

The second step is using the concentration to control the error between empirical \(}_{ta}\) and expected \(_{ta}\). Roughly speaking, we have that

\[_{ta}(_{_{ta},})- }_{ta}(_{_{ta},} )^{2}}((1/n_{ta}, _{}(),\|\|_{2})/_{1} )\,,\]where \((1/n_{ta},_{NN}(),\|\|_{2})\) is the covering number of \(_{NN}()\) in \(L_{2}\) norm. Since only \(V^{D d}\) can be optimized and \(\) is fixed in \(_{NN}()\),

\[((1/n_{ta},_{}( ),\|\|_{2}))=(Dd(1/n_{ta}))\,.\]

Then, we balance different terms and achieve the final bound by choosing \(^{2}=1/}\).

## 5 The Few-shot Diffusion Model Have a Closed-form Minimizer

This section focuses on how to optimize the few-shot diffusion model. When considering the optimization problem, we assume the shared latent distribution admits an isotropic Gaussian distribution \(q_{z}=(0,^{2}I_{d})\) with \(^{2}>0\), which indicates the score function has the following formulation:

\[ q_{t}^{ta}(X)=-}A_{ta}A_{ta}^{}X-^{2}}(I_{D}-A_{ta}A_{ta}^{})X\,.\]

Note that though \(q_{z}=(0,^{2}I_{d})\) is a special case of Assumption 4.1, we still need to know \(^{2}\) and \(A_{ta}\) to generate samples, which indicates the previous optimization analysis for diffusion models without a fine-tuning phase can not be used.

We fix a \(t[,T]\) for the few-shot objective function since the matrix \(A_{ta}\) is independent of time \(t\). More specifically, with an approximated latent distribution \(_{z}=(0,)\), where \(=^{2}I_{d}\), the expected few-shot objective function at a fixed time \(t\) is

\[_{_{V_{ta},}}_{NN}( )}_{ta,t}(_{V_{ta},})= _{X_{ta} q^{ta}}[_{t}^{ta}(X_{ta};_{V_{ ta},})]\,.\]

where

\[}_{}()=\{_{V,}(X,t)= ^{2}}V_{}(V^{}X,t)-^{2}}X:V^{D d}(V)=d.\},\]

In this case, \(_{}(Z,t)=(I_{d}-_{t}^{2}_{t} ^{-1})Z\), where \(_{t}=m_{t}^{2}+_{t}^{2}I_{d}\). The constraint \((V)=d\) is used to guarantee that the few-shot diffusion models learn meaningful subspace instead of \(^{D d}\). Note that \((V)=d\) is a weaker constraint than \(V^{}V=I_{d}\) since the pre-trained diffusion model has already learned the length information. This weaker constraint means we need less prior knowledge compared to \(()\), which is more user-friendly. Let \(_{ta}\) be a minimizer of the above expected few-shot objective function. We show that \(_{ta}\) has a closed form and good property.

**Lemma 5.1**.: _Assume Assumption 3.1 and \(q_{z}=(0,^{2}I_{d})\). Let \(C=_{X_{ta} q^{ta}}[X_{ta}X_{ta}^{}]\) be the expected data covariance matrix. Then, \(_{ta}\) has a closed form:_

\[_{ta}_{ta}^{}=^{2} ^{2}+_{t}^{2}}{^{2}}(C+_{t}^{2}I_{D}) ^{-1}C\,.\]

Lemma 5.1 indicates that few-shot diffusion models can directly obtain an approximation of the minimizer without a complex optimization process. Furthermore, this minimizer has good properties and exactly recovers the subspace spanned by \(A_{ta}\). More specifically, the expected minimizer indicates \(\|_{ta}_{ta}^{}-A_{ta}A_{ta}^{}\|_{F}^{2}=0\) when \(n_{s}\) and \(n_{ta}\) are infinite. However, the source datasets \(n_{s}\) and target datasets \(n_{ta}\) are finite, we analyze the empirical closed-form solution

\[_{ta}_{ta}^{}=^{2} ^{2}+_{t}^{2}}{^{2}}(m_{t}^{2}+_{t}^{2}I_{ D})^{-1}\,,\]

where \(=}_{i=1}^{n_{ta}}X_{ta,i}X_{ta,i}^{}\) is the empirical covariance matrix.

**Theorem 5.2**.: _Assume Assumption 3.1 and \(q_{z}=(0,^{2}I_{d})\). Let \(_{z}\) be the latent distribution generated by the pre-trained models with \((_{ta},)\) and \(M=^{2}(d+^{2})}{})(d^{2} D)}\). Then, with probability \(1-_{1}\), we have that for any \(t[,T]\)_

\[\|_{ta}_{ta}^{}-A_{ta}A_{ta}^{}\|_ {F}^{2}(})}{m_{t}^{2} ^{2}+_{t}^{2}}(}}+ }(m_{t}^{2}^{2}+_{t}^{2})^{2}))\,.\]The above result indicates that the few-shot diffusion models can still recover the true subspace with finite \(n_{s}\) and \(n_{ta}\). Note that when the latent distribution is Gaussian distribution, the approximation error bound for the source dataset is \(n_{s}^{-1/2}\) instead of \(n_{s}^{-2/d}\)(Yuan et al., 2023). Hence, \(n_{s}\) in Theorem 5.2 do not depend on latent dimension \(d\).

_Remark 5.3_.: The bound of \(\|VV^{T}-AA^{T}\|_{F}^{2}\) only guarantees the subspace spanned by \(V\) and \(A\) is close, which still holds after an orthogonal transformation on \(V\). Hence, this bound does not indicate \(\|V-A\|_{F}^{2}\) is small. Since all previous works (Chen et al., 2023; Yuan et al., 2023) consider \(\|VV^{T}-AA^{T}\|_{F}^{2}\), we also use this metric to measure the subspace recovery. However, our results are stronger due to the closed-form solution, where previous works do not consider how to obtain \(VV^{T}\).

### Discussion on the Closed-form Minimizer

**Better dependence on \(n_{ta},\) and \(d\).** Note that Theorem 5.2 has better \(1/n_{ta}\) dependence on the target dataset compared to \(1/}\) dependence on the source dataset. Furthermore, the coefficient of \(n_{s}\) term is dependent on the early stopping parameter \(\) and \(D\). This is due to the \(\) and \(D\) dependence of the approximation bound, which is used in generating \(_{z}\). However, the \(n_{ta}\) term only has \(d\) dependence. Hence, even in the latent Gaussian setting, we still need a larger source dataset than the target dataset to obtain a sufficiently accurate closed-form solution.

**The relationship with principal component analysis (PCA).** The expected few-shot score matching objective can be simplified to

\[_{V_{ta}}_{NN}()}1/_{t}^{4} _{X_{t} X_{0}=X_{ta,i}}[\|V_{ta}_{t}V_{ta} ^{}X_{t}-m_{t}X_{0}\|_{2}^{2}]\;,\]

where \(_{t}=I_{d}-_{t}^{2}_{t}^{-1}\). Note that when ignoring \(1/_{t}^{4}\) and choosing \(t=0\), the above minimization problem is similar to PCA. This suggests that few-shot models implicitly optimize an objective function akin to PCA. However, few-shot models extend beyond traditional PCA. More specifically, when \(^{2}\) is large, classical PCA suffers from the influence of \(^{2}\). In contrast, due to \((m_{t}^{2}^{2}+_{t}^{2})/n_{ta}\) term, few-shot models can select a large \(t\) to mitigate the impact of \(^{2}\) and achieve a \(1/n_{ta}\).

## 6 Experiments

To corroborate our theoretical findings, we conducted experiments utilizing real-world datasets. These experiments show that the new model obtained by only fine-tuning appropriate encoder and decoder layers on target datasets can produce novel images with the target feature, which shows the effectiveness of the methods and supports our theoretical results.

**Datasets and benchmark.** Note that human face images tend to exhibit similarity in their latent space, primarily due to shared facial features, while differing in specific features. Hence, we initially pre-train a model using the CelebA64 dataset, focusing on distinct hairstyle features as the goal for the fine-tuning phase. For the source data, we construct a large dataset (\(6400\) images) with different hairstyles (without the bald feature). For the target data, we choose the bald feature as the target feature and select \(10\) images with this feature to constitute the target dataset, which are much smaller than the size of target dataset (Figure 1 (a)). To show the effectiveness of our methods, we also fine-tune all parameters of the pre-trained models as the benchmark.

**Discussion.** As shown in Figure 1, the results obtained by only fine-tuning the encoder and decoder layers can generate novel face images with the bald feature. Conversely, when fine-tuning all parameters, the models suffer from memory phenomenon and can only generate images that slightly

Figure 1: The experiments on CelebA64 dataset

modify the brightness and angle of the target dataset. This phenomenon indicates that only fine-tuning the appropriate encoder and decoder will result in a model with a generalization property.

We note that these experiments aim to verify the effectiveness of the methods instead of achieving state-of-the-art performance since previous works carefully select specific parameters, such as specific cross-attention layers (Kumari et al., 2023) or special neurons (Liu et al., 2023), to fine-tune pre-trained models. However, we simply fine-tune all encoder and decoder layers simultaneously. There are more experiments on cat faces and more discussion on why Assumption 3.1 is satisfied in our experiments. We refer to Appendix E for more details.

## 7 Conclusion

This work aims to provide a deeper understanding of few-shot diffusion models from a theoretical perspective. Our analysis is conducted from two key perspectives: the approximation and optimization aspects, all under linear structure distribution and shared latent space assumptions.

From the approximation error bound, we consider general subgaussian latent variable and prove that few-shot models have a \((n_{s}^{-2/d}+n_{ta}^{-1/2})\) approximation bound, which is better than \(n_{ta}^{-2/d}\) results of diffusion models without a fine-tuning phase and escape the curse of dimensionality. This result also makes the first step to explain why few-shot diffusion models only require \(5 10\) images to generate great samples. The experiments on the real-world dataset also show that the fine-tuning phase only requires \(10\) images to generate novel images with the target feature.

When analyzing the optimization process, we consider a more special, shared Gaussian latent variable and prove that the expected score matching has a closed-form minimizer, which indicates that the few-shot diffusion models can simplify the optimization problem. Furthermore, we prove that the empirical closed-form solution has a \((1/n_{ta}+1/(}))\) accuracy bound, which still has better \(1/n_{ta}\) target data dependence compared to \(1/(})\) dependence on the source data.

**Future work and limitation.** When considering the approximation bound, we assume a distribution with a linear structure. Though it has been supported by much empirical evidence, it is not as general as bounded distribution. After that, we plan to consider a general, bounded distribution and show the advantage of few-shot diffusion models. One possible way is to analyze the mixture of low-rank Gaussian (Wang et al., 2024), which is more general than the linear subspace assumption.

We focus on a special Gaussian latent distribution when considering the optimization problem. As a next step, we plan to consider a more general latent distribution, such as a log-concave distribution. In this setting, we can not directly obtain the closed-form solution. However, due to the shared information and simplified landscape, it is still possible to use some optimization algorithms, such as gradient descent, to optimize the few-shot objective function to achieve the convergence guarantee.

**Broader Impact.** This paper presents work whose goal is to understand few-shot diffusion models from the theoretical perspective. A noteworthy societal impact is that few-shot diffusion models may be used to imitate the style of artists and generate fake images, thereby infringing on the rights of artists (Mirsky and Lee, 2021). We recommend adding watermarks to images to determine whether the image was generated by a generative model (Fernandez et al., 2023). The other societal impact is the same as general generative models (Mishkin et al., 2022).