# Neural Krylov Iteration for Accelerating

Linear System Solving

 Jian Luo &Jie Wang\({}^{1}\)&Hong Wang\({}^{1}\)&Huanshuo Dong\({}^{1}\)

&Zijie Geng\({}^{1}\)&Hanzhu Chen\({}^{1}\)&Yufei Kuang\({}^{1}\)

\({}^{1}\)MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,

University of Science and Technology of China

{jianluo,wanghong1700}@mail.ustc.edu.cn

jiewangx@ustc.edu.cn

Corresponding author. Email: jiewangx@ustc.edu.cn

###### Abstract

Solving large-scale sparse linear systems is essential in fields like mathematics, science, and engineering. Traditional numerical solvers, mainly based on the Krylov subspace iteration algorithm, suffer from the low-efficiency problem, which primarily arises from the less-than-ideal iteration. To tackle this problem, we propose a novel method, namely **Neural** Krylov **Iteration** (**NeurKItt**), for accelerating linear system solving. Specifically, NeurKItt employs a neural operator to predict the invariant subspace of the linear system and then leverages the predicted subspace to accelerate linear system solving. To enhance the subspace prediction accuracy, we utilize QR decomposition for the neural operator outputs and introduce a novel projection loss function for training. NeurKItt benefits the solving by using the predicted subspace to guide the iteration process, significantly reducing the number of iterations. We provide extensive experiments and comprehensive theoretical analyses to demonstrate the feasibility and efficiency of NeurKItt. In our main experiments, NeurKItt accelerates the solving of linear systems across various settings and datasets, achieving up to a 5.5\(\) speedup in computation time and a 16.1\(\) speedup in the number of iterations.

## 1 Introduction

Solving linear systems is the cornerstone of scientific computing, with applications in various fields including mathematics, science, and engineering. Traditional solvers rely on the Krylov subspace iteration algorithm to tackle large-scale sparse linear systems. It starts with a random initial vector, which corresponds to a one-dimensional linear subspace, and progressively expands this subspace to approximate the solution by iteratively minimizing the residual error.

However, the convergence speed and stability of the Krylov subspace iteration algorithms are significantly influenced by the dimensions and characteristics of the employed subspaces in the iterations. Computational inefficiency and instability often arise in scenarios involving high-dimensional problems or large matrices, especially when the matrices exhibit poor conditioning. This inefficiency primarily arises from the less-than-ideal iteration, which leads to a higher number of iterations and consequently longer solving time. To address these challenges, our key insight is that prior knowledge about the linear system's invariant subspace benefits the Krylov subspace iteration by guiding the iteration process, which reduces the required number of iterations.

Motivated by this insight, we introduce **Neural** Krylov **Iteration** (**NeurKItt**), a novel method that leverages neural networks to accelerate linear system solving. NeurKItt comprises two modules:the subspace prediction module and the acceleration module. The subspace prediction module uses a neural network to predict the invariant subspace of linear systems. Inspired by the concept that mapping a linear system to its invariant subspace can be viewed as an operator between two Hilbert spaces, we adopt the neural operator for subspace prediction. We also integrate thin QR decomposition and a projection loss function to optimize training, improving subspace prediction performance. For the acceleration module, it leverages the property that Krylov subspace iteration approximates an invariant subspace of the linear system. When partial information about this subspace is provided, our acceleration module utilizes it to reduce the iterations needed for precise solutions, effectively accelerating the process and addressing the challenges.

We provide comprehensive analyses to demonstrate NeurKItt's efficiency. Furthermore, extensive experiments conducted across various solver settings and different PDE problems validate the effectiveness of our approach. The results show that NeurKItt significantly accelerates linear systems solving, achieving up to a 5.5\(\) speedup. Both our theoretical analyses and experimental results collectively demonstrate the efficiency of NeurKItt.

We summarize our contributions as follows:

* To the best of our knowledge, our work is the first to apply the data-driven approach to optimize Krylov iteration algorithms for solving generic non-symmetric linear systems.
* We introduce a novel strategy that predicts the invariant subspace of the linear system to accelerate Krylov iteration. To facilitate the subspace prediction, we design a projection loss for efficient training, in conjunction with QR decomposition for stable outputs.
* Extensive experiments and theoretical analysis demonstrate that NeurKItt reduces the computational cost and the number of iterations for solving linear systems involving non-symmetric matrices.

## 2 Related Work

### Traditional Numerical Algorithms

In the field of computational mathematics, various algorithms have been devised to address the challenge of solving systems of linear equations. Among these, methods based on the Krylov subspace have garnered attention for their efficacy in handling large matrices[33; 43]. These methods notably mitigate computational complexity by seeking approximate solutions within a constrained, smaller subspace. Within this context, the Generalized Minimal Residual (GMRES) method [45; 14] plays a pivotal role, particularly in addressing non-symmetric matrices.

To enhance the efficiency of these Krylov subspace methods, a range of preconditioning methods are employed. These methods aim to improve matrix conditioning, reduce iteration counts, and boost stability. Preconditioning varieties include Jacobi, Additive Schwarz Method (ASM), and

Figure 1: The variation in tolerance for NeurKItt compared to GMRES, where each line represents an experiment under a solving method and a specific preconditioning. Notably, the NeurKItt substantially enhances the efficiency of solving the linear systems, with a reduction in the number of iterations by up to a factor of 16 and achieving a speed-up of up to 5.5 times.

Successive Over-Relaxation (SOR). They simplify the matrix structure for faster resolution, a significant advantage in large-scale problems.

Our methodology, NeurKltt, utilizes invariant subspaces to hasten convergence in Krylov subspaces, thereby expediting linear system resolutions. It is also engineered to integrate with existing preconditioning techniques, boosting its efficiency and applicability in complex computational scenarios.

### Learning-based Acceleration Algorithms

Although learning-based linear system solvers require training, the training time is negligible compared to the time saved over millions of calls. Thus, there has been a surge in learning-based acceleration efforts in recent years. Research in using neural networks for accelerating linear system solving [30; 23; 60] use neural networks to optimize the Conjugate Gradient algorithm, thereby accelerating the solution of symmetric positive definite linear systems.  accelerated the solution of the Poisson equation.  focused on accelerating algorithm iterations by learning better algorithm parameters. Recent studies have utilized neural networks for matrix preconditioning. [17; 38; 53] have focused on improving algebraic multigrid preconditioning algorithms. [15; 50] have applied CNNs and machine learning, respectively, to optimize block Jacobi and ILU precondition. Furthermore, specialized preconditioning research is being conducted in areas like physics[2; 7], engineering, and climate science, demonstrating the breadth of these applications.

However, these studies often face limitations due to specific matrix properties or mainly focus on reducing low-precision solving costs, with fewer advancements in accelerating high-precision solving. In contrast, our approach, NeurKltt, addresses all these limitations. It employs subspace prediction to refine iterative methods universally across Krylov subspace algorithms. This approach not only reduces the initial iterations but also significantly improves the speed and stability of subsequent iterations.

### Neural Operators

Numerical methods for solving PDEs commonly involves solving systems of linear equations, which require extensive computational resources. Recently, Neural operators (NOs), such as the Fourier Neural Operator (FNO)[31; 55; 12] and Deep Operator Network (DeepONet)[35; 36], have shown effectiveness in solving PDEs. Despite their effectiveness, NOs often grapple with challenges like diminished accuracy and stringent boundary condition prerequisites, which limit their applicability as standalone replacements for conventional algorithms in various scientific computing contexts[63; 19]. Given that mapping a matrix to its subspace is fundamentally an operator mapping task, NOs hold significant potential as tools for predicting matrix subspaces.

## 3 Preliminaries

### Krylov Subspace Iteration

In the realm of large-scale sparse linear systems, Krylov subspace methods are frequently employed as a standard solution strategy [46; 16]. At the heart of this methodology is the principle of utilizing the matrix involved in the linear system to iteratively construct a subspace. This subspace is crafted to approximate the actual solution space, with the iterative process continuing until the generated Krylov subspace sufficiently encompasses the real solution. Such an approach enables the iterative solutions within this confined subspace to progressively approximate the accurate solution vector \(\), thereby efficiently converging towards it.

Suppose we now solve the system

\[=,\] (1)

where \(^{n n}\). The \(m\)-th Krylov subspace associated with the matrix \(\) and the starting vector \(^{n}\) as follows:

\[_{m}(,)=\{,,^{2} {r},,^{m-1}\}.\] (2)

Generally, \(=_{0}-\) represents the initial residual, where \(_{0}\) is typically generated randomly or selected as an initial guess for the solution. The iterative process of the Krylov subspace leads to the Arnoldi relation:

\[_{m}=_{m+1}}_{m}=_{m}_{m}+_{m+1}h_{m+1,m}_{m}^{T},\] (3)where \(_{m}=(_{1},,_{m})^{n m}\) comprises unit vectors \(_{i}^{n}\) for \(i=1,,m\), mutually orthogonal, and \(_{m+1}=(_{m},_{m+1})\) extends this set. \(}_{m}^{(m+1) m}\) is an upper Hessenberg matrix. \(_{m}^{m m}\) is the first \(m\) rows of \(}_{m}\), distinct due to an extra element \(h_{m+1,m}\) at the \(m+1\)-th row, \(m\)-th column. \(_{m}\) is the \(m\)-th unit column vector, with its \(m\)-th element as \(1\).

Krylov algorithms reduce the computational effort needed for large linear systems by creating a Krylov subspace. However, starting this process with a random vector can be time-consuming. It's suggested that understanding the matrix's invariant subspace before beginning the iterative process, a warm-starting approach, could notably decrease the required iterations and improve convergence speed and stability.

### Neural Operators

NeurKltt utilizes the framework of neural operator architectures, notably the FNO [26; 31; 27]. We begin by defining a spatial dimension \(d\) and a corresponding domain \(D^{d}\). Our focus is on approximating operators \(:(D;^{d_{a}})(D;^{d_ {u}})\). Here, \(a(D;^{d_{a}})\) and \(u(D;^{d_{u}})\) denote functions that map the domain \(D\) to \(^{d_{a}}\) and \(^{d_{u}}\), respectively, where \(d_{a},d_{u}\). Both spaces \((D;^{d_{a}})\) and \((D;^{d_{u}})\) are identified as Banach spaces.

In line with the definition provided by , a neural operator \(:(D;^{d_{a}})(D;^{d_ {u}})\) is conceptualized as a mapping expressed by

\[(a)=_{L}_{1} (a),\] (4)

for a designated depth \(L\). In this formulation, \(:(D;^{d_{a}})(D;^{d_ {v}})\), with \(d_{v} d_{u}\), functions as a lifting operator, while \(:(D;^{d_{v}})(D;^{d_ {u}})\) serves as a local projection operator.

## 4 Method

NeurKltt++ utilizes a neural operator to predict the invariant subspace \(}\) of the linear system, and then uses it to accelerate the Krylov subspace iteration. Generally, NeurKltt is broadly divided into two components.

Footnote ‡: Our code is available at https://github.com/smart-JLuo/NeurKltt

**Subspace Prediction Module**: Inspired by operator learning, we employ the neural operator capable of predicting subspaces at a low cost, with the help of QR decomposition. The aim is to precisely predict the invariant subspace \(}\) of the linear system.

**Acceleration Module**: Originating from the mathematical theories of Krylov iteration, we have developed a Krylov algorithm tailored to our problem. Utilizing the invariant subspace, this method expedites Krylov Subspace convergence and enhances stability.

### Subspace Prediction Module

The mapping from the linear system to its corresponding invariant subspace can be considered an operator, i.e., a mapping between two Hilbert spaces. Solving such problems involves finding the corresponding solution operator. In our subspace prediction module, we choose the Fourier Neural Operator (FNO) for subspace prediction. We give a brief introduction to FNO in Appendix C. Generally, for a linear system \(Ax=b\) derived from the parametric PDE problem, to predict its invariant subspace \(}\), the input to FNO is the input function \(a^{d_{a}}\) from the given PDE, where \(d_{a}\). We provide a detailed discussion in Appendix B about how to build a linear system problem from a PDE problem, and what is the input function.

Our task is to learn the mapping between two Hilbert spaces \(:^{d_{a}}^{d n}\) using FNO. For FNO, the lifting transformation \(\) first lifts the input \(a\) to a higher dimensional representation \(v_{0}^{d_{a} c}\), where \(c\) is the number of channels. Then we feed the \(v_{0}\) to Fourier layers. After \(T\) Fourier layers, we have \(v_{T}^{d_{a} c}\) from the last Fourier layer, which keeps the same shape as \(v_{0}\). The FNO's output \(X=Q(v_{T})\) is the projection of \(v_{T}\) by the transformation \(Q:^{d_{a} c}^{d_{A} n}\). NeurKltt then uses QR decomposition to orthogonalize the matrix \(X\), obtaining the predictedsubspace \(}\). We provide more details about how to predict the subspace given the 2D Darcy flow problem in Appendix C.

We define the \(one-sided\)\(distance\) from the subspace \(\) to the subspace \(\) as

\[(,)=\|(-_{})_{ }\|_{2},\] (5)

where \(\) represents the projection operator for the associated space. Its mathematical interpretation corresponds to the largest principal angle between the two subspaces \(\) and \(\), and defining \(_{}\) as the spectral projector onto \(\). The objective of the subspace prediction task is to approximate the invariant subspace of the matrix \(\) with our predicted subspace \(}:\)

\[*{arg\,min}_{}(},),\] (6)

where \(\) represents the parameters of NO, and \((},)\) is the distance between the two subspaces. Based on subsequent Theoretical Analysis 5.2, we choose \(\) as the invariant subspace associated with the smallest \(n\) eigenvalues, assuming \(==\{_{1},_ {2},,_{n}\}\), where \(s_{i}\) is the eigenvector corresponding to the \(i\)-th smallest eigenvalue of the given matrix \(A,i=1,2,,n\). To reduce computational complexity, considering the norm equivalence theorem in Banach spaces, we optimize using the following loss function:

\[*{arg\,min}_{}_{i=1}^{n}d(},s_{i}).\] (7)

This norm transforms the computation of subspace distance into the computation of the distance from a vector to a subspace. According to the properties of the Hilbert space, there exists a unique orthogonal decomposition:

\[s_{i}=x+y(x},y}^{}).\] (8)

We represent the projection operator for \(}\) by \(\), with \(_{}=^{*}\), leading to the relationship:

\[d(},s_{i})=\|s_{i}-^{*}s_{i}\|.\] (9)

Consequently, our projection loss function \(l(},)\) is defined as follows:

\[l(},)=_{i=1}^{n}\|s_{i}-^{*}s_{i}\|.\] (10)

We train our subspace module by optimizing the above formula. Experiments have shown that as the projection loss decreases, the principal angle of our predicted subspace also decreases. The principal angle is a mathematical indicator derived in Theoretical Analysis 5.1 that influences the acceleration.

Figure 2: Algorithm Flow Diagram: **(a)** Finding solution \(\) for given matrices \(\) and \(\). **(b)** Traditional Algorithm: Krylov iterations from a random initial vector. **(c1)****NeurKItt** Subspace Prediction Module: Utilizing a neural operator for estimating the invariant subspace of matrix \(\). **(c2)****NeurKItt** Acceleration Module: Using the predicted invariant subspace of the matrix to guide the iteration, thereby accelerating the Krylov iteration process.

### Acceleration Module

For Krylov algorithms, the iteration count is a critical factor influencing computational load. Drawing inspiration from the Krylov recycling algorithms[40; 13; 6; 58; 41; 24], our approach NeurKItt utilizes the predicted invariant subspace \(}\) as the deflation space within the Krylov iteration framework. We provide the pseudocode of the acceleration module in Appendix A.2. The key procedure of our implementation is outlined as follows:

Considering the linear system (1), we obtain a \(k\)-dimensional invariant subspace \(}\) from the subspace prediction module. Then, NeurKItt computes matrices \(_{k},_{k}^{n k}\) from \(}\) and \(\) such that \(_{k}=_{k}\) and \(_{k}^{H}_{k}=_{k}\), the specific algorithm can be found in Appendix A.1. We can get the Arnoldi relation of our NeurKItt:

\[(-_{k}_{k}^{H})_{m-k}=_{m-k+1}}_{m-k}.\] (11)

This suggests that when the \(k\)-dimensional invariant subspace is given, there is no need to start from scratch for constructing a new Krylov subspace \((,)\). Building upon this foundation, \((,)\) can converge more rapidly to the subspace where the solution \(\) lies. This can significantly reduce the dimensionality of the final Krylov subspace, leading to a marked decrease in the number of iterations and resulting in accelerated performance. Compared to NeurKItt, GMRES can be intuitively conceptualized as the special case of our NeurKItt, where \(k\) is initialized at zero[8; 39; 40].

Existing Krylov recycling algorithms share the same Arnoldi relation with NeurKItt. However, they can only accelerate the linear systems solving when there are strongly correlated linear systems provided, which is not common in scientific computing. For example, when solving PDEs with finite element software, matrices vary in size and element positions due to discretization methods and different grids, which makes the recycling invariant subspace from previous matrices impossible. Additionally, storing subspaces from previous solving requires significant storage when the previous linear system is large. Thus, these methods are not widely used in general scenarios. In contrast, NeurKItt, which leverages neural operators to predict subspaces, is not affected by the lack of strongly correlated linear systems.

## 5 Theoretical Analysis

### Convergence Analysis

Let \(\) be an \(l\)-dimensional invariant subspace of matrix \(\), and let \(=(_{k})\) be a \(k\)-dimensional space (\(k l\)) selected to approximate \(\). We refer to Theorem 3.1 in  for an in-depth convergence analysis under NeurKItt.

**Theorem 5.1**.: _[_40_]_ _Considering a space \(\), define \(=(_{m-k+1}}_{m-k})\) be the \((m-k)\)-dimensional Krylov subspace generated by NeurKItt as in (11). Let \(_{0}^{n}\), and \(_{1}=(-_{})_{0}\). Then, for each \(\) such that \((,)<1\),_

\[_{_{1}}\| _{0}-_{1}\|_{2}&_{_{2}(- _{})}\|(-_{})_{1}-_{2 }\|_{2}\\ +\|_{}\|_{2}\|(-_{})_{1}\|_{2},\] (12)

_where \(=\|(-_{})_{}\|_{2}\) and \(=(,)\)._

The left-hand side is the residual norm subsequent to \(m-k\) iterations of NeurKItt, employing the predictive subspace \(\). In contrast, on the right-hand side, the initial term epitomizes the convergence of a deflated problem, given that all components within the subspace \(\) have been eradicated[39; 49]. The subsequent term on the right embodies a constant multiplied by the residual following \(m-k\) iterations of NeurKItt, when solving for \(_{1}\). Supposed that the predictive space \(\) encompass an invariant subspace \(\), then we have \(==0\) for the given \(\), which ensures that the convergence rate of NeurKItt matches or surpasses that of the deflated problem. In most cases, \(\|_{}\|_{2}\) is numerically stable and not large, therefore a reduced value of \(\) directly correlates with faster convergence in NeurKItt.

Now we compare two approaches to accelerating the linear systems solving:1. Providing an initial prediction for the solution, allows Krylov algorithms to start from this prediction. The solution involved might come from solutions to similar systems or predictions such as via neural networks.
2. Predicting the matrix invariant subspace, using this approximate subspace to speed up Krylov iterations.

For the first approach, particularly with large matrices derived from PDEs, the norm \(\|\|_{2}\) tends to be large. Our theoretical analysis in Appendix D suggests that FNO's direct solution prediction will not significantly accelerate the linear system solving, possibly only reducing a few Krylov subspace iterations. NeurKItt belongs to the second approach. This crucially speeds up the convergence of Krylov subspace iterations, markedly reducing total iterations and improving stability.

### Subspace Property Analysis

The crucial questions about predicted subspace arise when accelerating Krylov subspace iterations: (1) What kind of subspace should be selected for acceleration? (2) Given the computational cost associated with employing neural networks, is it necessary to expend substantial computational resources to predict a highly accurate subspace?

**Regarding the first question**: As indicated by our convergence analysis 5.1, effectively accelerating the solution of linear systems only requires a predicted invariant subspace of matrix \(\). A matrix has many invariant subspaces, but which of these are easier to learn by the neural network?

To investigate this problem, we simplify the specific scenario of the linear systems problem. The following definitions and assumptions for Theorem 5.2 are from the reference . Specifically, We deal with a Hermitian positive definite matrix \(\) and a corresponding Hermitian perturbation \(\), allowing \(\) to have the eigendecomposition:

\[=[_{1}_{2}_{3}](_{1},_{2},_{3})[_{1}_{2}_{3}]^{H},\] (13)

where \(=[_{1}_{2}_{3}]\) is an orthogonal matrix, \(_{1}=(_{1}^{(1)},,_{j_{1}}^{(1)})\), \(_{2}\) and \(_{3}\) are defined in a similar fashion. Let the eigenvalues satisfy the following ordering*:

Footnote *: For non-Hermitian matrices, the eigenvalues will be sorted by comparing their modulus, such that \(|_{1}||_{2}||_{3}||_{4}| |_{n}|\), where \(_{i}\), \(i=1,2,,n\), is the eigenvalue of a given non-Hermitian matrix \(A\).

\[_{1}^{(1)}_{j_{1}}^{(1)}<_{1}^{(2)} _{j_{2}}^{(2)}<_{1}^{(3)}_{j_{3}} ^{(3)}.\]

We consider the change in the invariant subspace range(\(_{1}\)) under a symmetric perturbation \(\) of \(\). Let \(_{1}(,)\) denote the largest canonical angle between two spaces. We do not require that \(||||_{F}\) be small, but we assume that the projection of \(\) onto the subspace range(\([_{1}_{2}]\)) is small. We assume that \(||[_{1}_{2}]^{H}||_{F}\) and that \(\) is small relative to \(_{1}^{(2)}-_{j_{1}}^{(1)}\). We also assume that \(=||_{3}^{H}||_{F}\) is small relative to \(_{1}^{(3)}-_{j_{1}}^{(1)}\). Note that we do not need to assume that \(_{j_{2}}^{(2)}-_{j_{1}}^{(1)}\) is large. Also, let

\[(_{1}^{(2)}-,_{1}^{(3)}-)-2-( _{j1}^{(1)}+)>2,\]

\[(1-}{^{2}})+_{j_{ 1}}^{(1)}+.\]

**Theorem 5.2**.: _[_24_]_ _Let \(\) be Hermitian positive definite and have the eigendecomposition given in (13), and let \(\), \(\), \(\), \(\), and \(\) be defined as above. Then there exists a matrix \(}_{1}\) conforming to \(_{1}\) such that range(\(}_{1}\)) is a simple invariant subspace of \(+\), and_

\[_{1}((_{1}),(}_{1})) }.\]

The specific proof can be found in . A similar bound applies to the perturbation of the eigenvalues associated with \(_{1}\). In the context of Theorem 5.1 and our NeurKItt, \(_{1}\) corresponds to \(\), whereas \(_{2}\) and \(_{3}\) can be chosen to fit the theorem.

This theorem shows that if changes in a matrix occur in the subspace corresponding to larger eigenvalues, the subspace associated with the smallest eigenvalues is minimally affected, as long as the changes are smaller than the gap between the smallest and larger eigenvalues. To facilitate learning of matrix invariant subspaces, we use the subspace composed of eigenvectors corresponding to the smallest eigenvalues as the prediction target for our neural operator.

**Regarding the second question**: The analysis of the first question, along with the error convergence rate proved in Theorem 5.1, suggests that investing in substantial computational resources for predicting a high-precision subspace is unnecessary. As long as the predicted subspace shows a distinct correlation with the matrix invariant subspace, particularly when \(\) is small, significant acceleration can be achieved without resorting to high precision. This principle underpins our development of a low-precision, cost-effective subspace prediction framework. Although the subspace prediction may not be precise, the final algorithm has achieved remarkable acceleration results.

## 6 Experiment

### Experiment Settings

To comprehensively evaluate the performance of NeurKltt, we conducted extensive experiments. Our analysis centers on two primary performance metrics viewed through three perspectives. These tests are conducted on 3 datasets. Specifically, the three Perspectives are: (i) Matrix preconditioning techniques, spanning 7 standard methods. (ii) Accuracy criteria for linear system solutions, emphasizing 8 distinct tolerances. (iii) Different matrix sizes, considering 3 variations. For more details, please refer to the Appendix F.1.

**Baselines.** NeurKltt focuses on solving linear systems that involve large sparse non-symmetric matrices. The GMRES algorithm is widely used for non-symmetric large sparse linear system solving, which serves as the predominant solution. And we set it as the benchmark for our study. We use GMRES from PETSc (version 3.19.4).

**Datasets.** To investigate the algorithm's adaptability to various types of matrices, we examined three different linear equation challenges, each rooted in a PDE: 1. Helmholtz Equation . 2. Darcy Flow Problem [31; 44; 27; 36]; 3. Non-uniform Heat Conduction Equation [48; 25; 4; 18]. For an in-depth exposition of the dataset and its generation, kindly refer to the Appendix F.4. For the runtime environment, refer to Appendix F.2.

**Metrics.** We adopt time speedup for "GMRES solving time / NeurKltt solving time" and iteration speedup for "GMRES iteration count / NeurKltt iteration count". Speedup over 1 denotes better NeurKltt performance. We also provide the average time spent and average iteration count in Appendix H. We use principal angle in Equation 5 to show how close the predicted subspace and the target subspace are.

  
**Dataset** & **Tol** & None & Jacobi & Blacobi & SOR & ASM & ICC & ILU \\   & 1e-2 & 5.55 / 16.13 & 3.66 / 11.00 & 1.87 / 6.82 & 2.20 / 7.78 & 1.91 / 6.64 & 2.64 / 6.35 & 2.60 / 6.35 \\  & 1e-4 & 4.66 / 12.35 & 2.89 / 7.66 & 1.97 / 5.52 & 2.24 / 6.35 & 2.05 / 5.58 & 2.40 / 5.47 & 2.36 / 5.47 \\  & 1e-7 & 3.95 / 9.93 & 2.38 / 5.88 & 1.75 / 4.46 & 1.98 / 4.97 & 1.91 / 4.60 & 2.09 / 4.61 & 2.06 / 6.42 \\  & 1e-10 & 3.67 / 8.98 & 2.16 / 5.31 & 1.65 / 4.08 & 1.81 / 4.31 & 1.83 / 4.13 & 1.86 / 4.16 & 1.87 / 4.16 \\  & 1e-12 & 3.61 / 8.81 & 2.22 / 5.45 & 1.67 / 3.97 & 1.72 / 4.05 & 1.74 / 3.94 & 1.79 / 3.96 & 1.78 / 3.96 \\   & 1e-2 & 3.15 / 9.48 & 2.24 / 6.29 & 1.52 / 5.33 & 1.54 / 5.96 & 1.48 / 5.00 & 1.90 / 4.83 & 1.97 / 4.83 \\  & 1e-4 & 3.23 / 8.50 & 2.03 / 5.27 & 1.47 / 4.79 & 1.71 / 5.12 & 1.49 / 4.39 & 1.84 / 4.30 & 1.89 / 4.30 \\  & 1e-7 & 2.63 / 7.63 & 1.55 / 4.35 & 1.51 / 4.30 & 1.51 / 4.28 & 1.45 / 3.81 & 1.69 / 3.73 & 1.70 / 3.73 \\  & 1e-10 & 2.93 / 7.34 & 1.50 / 4.27 & 1.50 / 4.08 & 1.42 / 3.90 & 1.38 / 3.59 & 1.65 / 3.54 & 1.64 / 3.54 \\  & 1e-12 & 2.90 / 7.26 & 1.54 / 4.35 & 1.45 / 3.96 & 1.41 / 3.76 & 1.42 / 3.51 & 1.61 / 3.46 & 1.60 / 3.46 \\   & 1e-2 & 2.39 / 7.68 & 2.37 / 7.68 & 1.33 / 5.49 & 1.58 / 5.66 & 1.32 / 4.83 & 1.54 / 4.31 & 1.51 / 4.31 \\  & 1e-4 & 2.69 / 7.99 & 2.72 / 7.99 & 1.36 / 4.29 & 1.45 / 4.36 & 1.40 / 4.00 & 1.60 / 3.72 & 1.59 / 3.72 \\   & 1e-7 & 2.55 / 7.29 & 2.65 / 7.29 & 1.52 / 4.08 & 1.60 / 4.54 & 1.43 / 3.91 & 1.68 / 3.76 & 1.68 / 3.76 \\   & 1e-10 & 2.52 / 6.74 & 2.52 / 6.74 & 1.56 / 4.01 & 1.77 / 4.59 & 1.56 / 3.95 & 1.70 / 3.75 & 1.69 / 3.75 \\   & 1e-12 & 2.53 / 6.56 & 2.48 / 6.56 & 1.52 / 3.91 & 1.72 / 4.56 & 1.64 / 3.97 & 1.68 / 3.71 & 1.67 / 3.71 \\   

Table 1: Comparison of our NeurKltt and GMRES computation time and iterations across datasets, preconditioning, and tolerances. The first column lists datasets with matrix size, and the next details tolerances. Results are displayed as “time speedup / iteration speedup”.

### Main Experiment

We compare NeurKItt with GMRES under various preconditioners and tolerance, and Table 1 presents main experimental results. More detailed experimental results about time and number of iterations can be found in Appendix H.

The results show that across all tolerances and preconditioning techniques, NeurKIt has been effective in accelerating the linear system solving and reducing the number of iterations required by the Krylov subspace method, achieving up to a 5.5x speedup in computation time and a 16.1x speedup in the number of iterations. These results demonstrate that the invariant subspace predicted by NeurKIt greatly enhances the convergence speed of the Krylov algorithms, thereby reducing the iteration count and accelerating the solving.

With the solution accuracy increasing, the acceleration does not significantly decrease. Though the acceleration effect of NeurKIt varies under different preconditioning, the minimum time speedup is no less than 1.67 in the Darcy flow problem, which suggests NeurKIt can be effectively combined with various preconditioning methods.

### Generalization of Matrix Size

We conducted experiments on the Darcy Flow problem with varying matrix sizes under different preconditioning methods and tolerances. We provide the results in Figure 3. The experimental results show that our method consistently improves its acceleration effect as the matrix size increases, across different preconditioning methods and tolerances, which indicates that NeurKIt especially benefits solving large sparse linear systems.

### Ablation Study

We conducted ablation experiments to further validate the performance of FNO and demonstrate the effectiveness of the designed projection loss. The results of the ablation experiments are shown in Table 2. The results demonstrate that FNO significantly outperforms MLP when using the same projection loss. This indicates that FNO is indeed capable of learning a better mapping from the parameter matrix to the subspace, resulting in predicted subspaces that are closer to the target subspaces.

In addition, we also attempted to utilize the Mean Squared Error (MSE) Loss for subspace prediction. The experimental results show that applying the MSE Loss on subspace learning is merely ineffective. Since we report the principal angle in radians, it implies that the predicted subspace \(}\) is merely orthogonal to the target subspace \(\). Such subspace \(}\) cannot be effectively utilized to accelerate linear system solving. Besides, it can be inferred that QR decomposition we have

  
**Model** & **Principal Angle\(\)** \\  NeurKItt & 0.07 \\ w/o FNO & 0.54 \\ w/o QR decomposition & 1.57 \\ w/o Projection Loss & 1.57 \\   

Table 2: Comparison of different settings on principal Angle (rad) for the Darcy flow problem, with a matrix size of 32,400. We replace the FNO with MLP for “w/o FNO” and replace the projection loss with MSE loss for ”w/o Projection Loss”.

Figure 3: Experiments on the Darcy Flow problem with varying matrix sizes. The results indicate that as the matrix size increases, both time speedup and iteration speedup increase.

employed plays an important role in subspace prediction. These results verifies that the projection loss and QR decomposition enable the model to better predict subspace.

## 7 Limitation and Conclusions

**Limitation** Our method only considers solving non-symmetric matrices, but for matrices with specific structures, we need to tail our method to achieve faster solving. Furthermore, we have not considered the potential impact of different preconditioning techniques on the subspace and have not optimized for different preconditioning techniques, though the influence caused by preconditioning has little impact based on the observation of our experiments.

**Conclusions** We propose NeurKItt, a novel method for accelerating linear systems solving through subspace prediction. To the best of our knowledge, this is the first attempt to apply a data-driven approach to optimize the Krylov subspace algorithm for non-symmetric linear systems. Specifically, we employ FNO to predict the invariant subspace and design a projection loss function for this task. By utilizing the predicted invariant subspace, we achieve accelerated Krylov subspace iterations. Theoretical analysis and extensive experiments demonstrate that NeurKIt effectively reduces computational costs and iteration counts.