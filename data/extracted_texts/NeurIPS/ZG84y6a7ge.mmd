# SlimSAM:

0.1% Data Makes Segment Anything Slim

Zigeng Chen, Gongfan Fang, Xinyin Ma, Xinchao Wang

National University of Singapore

zigeng99@u.nus.edu, xinchao@nus.edu.sg

Corresponding Author

###### Abstract

Current approaches for compressing the Segment Anything Model (SAM) yield commendable results, yet necessitate extensive data to train a new network from scratch. Employing conventional pruning techniques can remarkably reduce data requirements but would suffer from a degradation in performance. To address this challenging trade-off, we introduce SlimSAM, a novel data-efficient SAM compression method that achieves superior performance with extremely less training data. The essence of SlimSAM is encapsulated in the alternate slimming framework which effectively enhances knowledge inheritance under severely limited training data availability and exceptional pruning ratio. Diverging from prior techniques, our framework progressively compresses the model by alternately pruning and distilling distinct, decoupled sub-structures. Disturbed Taylor pruning is also proposed to address the misalignment between the pruning objective and training target, thereby boosting the post-distillation after pruning. SlimSAM yields significant performance improvements while demanding **over 10 times less** training data than any other existing compression methods. Even when compared to the original SAM, SlimSAM achieves approaching performance while reducing parameter counts to merely **1.4% (9.1M)**, MACs to **0.8% (23G)**, and requiring only **0.1% (10k)** of the SAM training data. Code is available at https://github.com/czg1225/SlimSAM

## 1 Introduction

_Segment Anything Model_ (SAM)  has attracted considerable attention from the community since its inception. A plethora of studies  have achieved substantial progress by incorporating SAM as a fundamental component. Nevertheless, despite its remarkable performance, SAM's substantial model size and high computational demands render it inadequate for practical applications on resource-constrained devices. This limitation consequently hinders the advancement and broader application of SAM-based models.

To mitigate these constraints, many efforts  have been made to effectively compress SAM. Without exception, these endeavors opt to replace the originally heavyweight image encoder with a lightweight and efficient architecture. This invariably entails training a new network from scratch. With regard to scratch training, an unavoidable challenging trade-off arises between training costs and model performance. Existing methods all inevitably compromise performance when training with very limited data.

The crux of the above issue is their inability to fully exploit the capability of pre-trained SAM. To overcome the high training data demands by reusing the robust prior knowledge of pre-trained SAM, a straightforward strategy involves the application of pruning techniques  to directlycompress the sizable SAM by removing redundant parameters from the network and fine-tuning the streamlined model with a minimal dataset [16; 36; 10; 30]. Nevertheless, following this conventional procedure leads to unexpected steep performance degradation, particularly when the pruning ratio is set aggressively high and the available data is extremely scarce.

In response to the challenges outlined above, we present SlimSAM, a data-efficient method for SAM compression. Initiating with a standard pruning-finetuning workflow, we gradually "modernize" the compression procedure by introducing our novel designs customized for severely limited data availability and the intricate coupled structure of SAM, culminating in exceptional efficacy while requiring minimal training data. Central to the method are our pioneering contributions: the alternate slimming framework and the disturbed Taylor pruning.

The alternate slimming framework, presented in Figure 1, boosts performance by minimizing divergence from the original model and enabling the intermediate feature alignment via consistent dimensionality. Diverging from prior methods, it alternates between pruning and distillation within decoupled model components. The process begins by targeting the embedding dimensions for pruning and aligning the consistent bottleneck dimensions for distillation. It then shifts focus to pruning the bottleneck dimensions in ViTs , aligning the unchanged embedding dimensions for distillation. Observing the misalignment between the pruning object and the distillation target impedes the efficacy of compression, we introduce a novel label-free importance estimation criterion called disturbed Taylor importance to address this misalignment effectively, thereby enhancing the recovery process and obviating the need for labeled data.

Comprehensive assessments across performance metrics, efficiency, and training data requirements reveal that SlimSAM markedly enhances compression performance, concurrently achieving superior lightweight and efficiency with markedly reduced training data requirements. Notably, our entire compression can be completed using only 10k un-labeled images on a single Titan RTX GPU.

In summary, our contribution is a data-efficient SAM compression method called SlimSAM, which effectively repurposes pre-trained SAMs without the necessity for extensive retraining. This is achieved through a novel modernized pruning-distillation procedure. By proposing the alternate slimming framework and introducing the concept of disturbed Taylor importance, we realize greatly enhanced knowledge retention in data-limited situations. When compared to the original SAM-H, SlimSAM achieves approaching performance while reducing the parameter counts to **1.4% (9.1M)**, MACs to **0.8% (23G)**, and requiring mere **0.1% (10k)** of the training data. Extensive experiments demonstrate that our method realizes significant superior performance while utilizing **over 10 times less** training data when compared to any other compression methods.

## 2 Related Works

**Model Pruning.** Due to the inherent parameter redundancy in deep neural networks , model pruning [16; 14; 30; 3; 27; 33; 52; 36; 10] has proved to be an effective approach for accelerating and compressing models. Pruning techniques can be generally classified into two main categories: structural pruning [30; 27; 56; 8; 4; 56; 7; 9] and unstructured pruning [5; 26; 40; 42; 12]. Structural pruning is focused on eliminating parameter groups based on predefined criteria, while unstructured pruning involves the removal of individual weights, typically requiring hardware support.

Figure 1: A simple overall diagram of the proposed alternate slimming process.

**Efficient Learning**. _Efficient Learning_ refers to a range of techniques [54; 57; 20; 21; 58; 11; 22; 28] aimed at reducing the training costs of deep models while maintaining performance. _Knowledge Distillation_ (KD)  is a prominent method under this category, where knowledge is transferred from a larger, powerful teacher model to a smaller, more efficient student model. This approach leverages soft targets and a temperature parameter to enable the student model to learn more effectively. KD [46; 44; 61; 1; 51; 32; 47; 37; 39] has proven to be an effective strategy for model compression, making it highly applicable in scenarios requiring resource-efficient deployment.

**SAM Compression**. The formidable model size and computational complexity of SAM pose challenges for edge deployment, prompting an extensive array of research focused on devising compression techniques for SAM to enhance its applicability. Notably, FastSAM  replaces SAM's extensive ViT-based architecture with the efficient CNN-based YOLOv8-seg  model, while MobileSAM  adopts the lightweight Tinyvit  to replace the image encoder and employs knowledge distillation from the original encoder. EdgeSAM  introduces the prompt-in-the-loop knowledge distillation to accurately capture the intricate dynamics between user input and mask generation. EfficientSAM  innovatively adapts MAE  framework to obtain efficient image encoders for segment anything model but requires extensive training data even more than the SA-1B dataset. However, the above approaches all inevitably suffer from scratch training, resulting in unsatisfactory performance when training data is limited.

**Remark**. The application of common pruning and KD methods falls short in achieving superior performance due to the unique challenges presented by limited training data and SAM's coupled structure. To enhance performance, we propose an alternate slimming framework to minimize divergence from the original model and enable the intermediate feature alignment by consistent dimensionality. We also propose disturbed Taylor pruning to address the misalignment between pruning objectives and training targets. In contrast to other SAM compression methods, our SlimSAM achieves superior compression performance while significantly incurring lower training data requirements.

## 3 Methods

Our paramount objective is to achieve substantial compression of the large image encoder while minimizing performance degradation in scenarios characterized by severe data limitations. To navigate the challenging trade-off between maintaining remarkable performance and the necessity for copious training data, we adopt a strategy of directly inheriting the core weights from the original SAM. This approach capitalizes on SAM's robust prior knowledge, derived from 11 million images. Adhering to this foundational principle, we begin with a standard workflow: initial pruning of the model followed by refinement through post-distillation.

### Identifying SAM Redundancy

The initial phase is dedicated to the estimation of the importance of each parameter, determining the non-essential and redundant parameters of the image encoder to be pruned. To fulfill this objective, we endeavor to estimate the importance of a parameter through the quantification of prediction errors engendered by its removal . Given a labeled dataset with N image pairs \(\{x_{i},y_{i}\}_{i=1}^{N}\) and a model \(\) with M parameters \(W~{}=~{}\{wi\}_{i=1}^{M}\). The output of the original model can be defined as \(t_{i}=_{W}(x_{i})\). Our objective is to identify the parameters that yield the minimum deviation in the loss. Specifically, the importance of a parameter \(w_{i}\), can be defined as:

\[I_{w_{i}}=|(x_{i},y_{i})|=|_{w_{i}} (x_{i},y_{i})-_{w_{i}=0}(x_{i},y_{i})|,\] (1)

where \((x_{i},y_{i})\) is the loss between the model output and the label \(y_{i}\) when input data is \(x_{i}\). We can approximate \(_{w_{i}=0}\) in the vicinity of \(w_{i}\) by its first-order Taylor expansion:

\[_{w_{i}=0}(x_{i},y_{i})=_{w_{i}}(x_{i},y_{i})-(x_{i},y_{i})}{ w_{i}}w_{i}+_{1}(w_{i}= 0).\] (2)

Substituting equation 2 into equation 1, we can approximate the parameter importance as:

\[I_{w_{i}}|_{w_{i}=0}(x_{i},y_{i})-_{w_{i}=0 }(x_{i},y_{i})+(x_{i},y_{i})}{ w_{i}}w_{i} |=|(x_{i},y_{i})}{ w_{i}}w_{i} |.\] (3)However, there exist two distinct limitations associated with the above Taylor importance estimation when pruning the image encoder of SAM. Firstly, the accuracy of Taylor importance relies heavily on the availability of sufficiently accurate hard labels \(y_{i}\). Unfortunately, due to the intricate nature of jointly optimizing the image encoder and combined decoder , the post-distillation process necessitates performing on the image embedding \(t_{i}\), resulting in the utilization of soft labels exclusively. Secondly, a concern arises regarding the consistency of loss functions when employing Taylor importance estimation for SAM pruning. The importance estimation strategy's primary objective is to identify parameters \(w_{i}\) that minimize the hard label discrepancy \(|(x_{i},y_{i})|\). In contrast, the goal of the distillation-based recovery process is to minimize the soft label loss \(|(x_{i},t_{i})|\). This misalignment in optimization objectives potentially impedes the efficacy of the distillation process. The experimental results in Section 5 also strongly prove our conclusion.

**Disturbed Taylor importance.** To address the unique limitations associated with Taylor importance estimation, we introduce an extremely simple yet effective solution known as disturbed Taylor importance. Given the absence of hard labels and the incongruity of loss functions, a logical approach is to identify parameters \(w_{i}\) that minimize the soft label divergence \(|(x_{i},t_{i})|\). However, the gradients \((x_{i},t_{i})}{ w_{i}}\) resulting from applying the loss between encoder's outputs \(t_{i}\) are consistently zero. Consequently, we calculate gradients based on the loss function between the original image embedding \(t_{i}\) and disturbed image embedding \(t_{i}+(,^{2})\), where \(\) is Gaussian noise with mean \(=0\) and standard deviation \(=0.01\). As the expectation \(E(t_{i}+)=t_{i}\), when the batch size is large enough, the importance of a parameter \(w_{i}\) can be approximated as:

\[ I_{w_{i}}&=|(x_{i},t _{i})||(x_{i},t_{i}+)|\\ &=|_{w_{i}}(x_{i},t_{i}+)-_{w_{ i}=0}(x_{i},t_{i}+)|\\ &|(x_{i},t_{i}+ )}{ w_{i}}w_{i}|.\] (4)

As the generated gradients \((x_{i},t_{i}+)}{ w_{i}} 0\), the importance can be estimated.

**Remark.** Leveraging our disturbed Taylor importance, the pruning objective is seamlessly aligned with the optimization target of subsequent distillation. Compared to previous pruning techniques, it results in a 0.85% MIoU enhancement when the pruning ratio reaches 77% and a 0.60% MIoU improvement when the pruning ratio is set at 50%. Moreover, the adoption of disturbed Taylor importance transforms the entire compression workflow into a convenient label-free framework without incurring additional computational costs.

### Alternate Slimming.

After estimating the weights' importance, our approach advances to implementing channel-wise structural pruning on the extensive image encoder, followed by distillation-based model finetuning. To attain an unprecedentedly high compression rate, the pruning ratio in this study is necessitated to be set significantly higher than in typical scenarios. With the pruning ratio exceeding 75%, we observe a marked performance degradation between the pruned model and its original counterpart, a consequence of employing the conventional single-step pruning technique. Additionally, the extremely constrained data availability also poses unique challenges to distillation efficacy. Employing merely 0.1% of the SA-1B dataset (10k images) for post-distillation underscores a significant challenge in recuperating satisfactory performance for the pruned model.

To address identified challenges, we introduce an innovative alternate slimming framework, anchored by two principles: reducing the divergence between the original and pruned models, and enhancing post-distillation efficacy.

Our framework decomposes the model into two separate sub-structures: embedding (output dimensions of each block) and bottleneck (intermediate features of each block). By sequentially pruning and restoring either sub-structure, we achieve a smoother compression loss, preventing the steep performance degradation typically associated with extreme pruning ratios. To improve post-distillation, we exploit the hidden state information of the original model. Due to the structural resemblance between the pruned and original models, using intermediate hidden states for supervision facilitates superior knowledge transfer. Traditional pruning workflow struggles with dimensionality inconsistency, complicating hidden state supervision. Our method, by partitioning the model into sub-structures, circumvents this issue. Whether pruning embedding or bottleneck dimensions, the intact remaining dimensions enable alignment through loss backpropagation. The effectiveness of this feature alignment, especially in data-scarce scenarios, highlights our framework's efficacy.

An overview of the alternate slimming framework is detailed in Figure 2. Given the Vit-based image encoder with k blocks, the output and intermediate features of each block within the encoder are denoted as \(E~{}=~{}\{e_{i}\}_{i=1}^{k}\) and \(H~{}=~{}\{h_{i}\}_{i=1}^{k}\). Specifically, for _Multi-Head Attention Blocks_ (MHABs), the intermediate feature refers to the concatenated QKV features, while for the MLPs, it refers to the hidden features between two linear layers. The final output image embedding is represented as \(t\). The original encoder is referred to as \(v_{0}\), while the pruned encoders after embedding pruning and bottleneck pruning are denoted as \(v_{1}\) and \(v_{2}\), respectively. The alternate slimming process can be described as the following progressive procedure: embedding pruning, bottleneck aligning, bottleneck pruning, and embedding aligning.

**Embedding Pruning.** The embedding dimension significantly impacts the encoder's performance as it determines the width of features extracted within the encoder. To begin with, we prune the embedding dimensions \((E)\) while keeping the bottleneck dimensions \((H)\) constant. The presence of residual connections necessitates the preservation of uniformity in the pruned embedding dimensions \((\{e_{i}\}_{i=1}^{K})\) across all blocks. Consequently, we employed uniform local pruning.

**Bottleneck Aligning.** In the context of incremental knowledge recovery, the pruned encoder learns from the original encoder's output \(t_{v_{0}}\) and aligns with its dimensionality-consistent bottleneck features \(H_{v_{0}}\) in each block. The distillation loss function for bottleneck aligning is a combination of bottleneck feature loss and final image embedding loss:

\[_{Bn}=_{MSE}(H_{v_{0}},H_{v_{1}})+(1-) _{MSE}(t_{v_{0}},t_{v_{1}}),\] (5)

where \(_{MSE}(.,.)\) is mean-squared error, the dynamic weight \(\) of \(n\)th epoch is defined as:

\[=\{0.5&n<N\\ 0&n>=N..\] (6)

We set \(N=10\) for bottleneck aligning.

**Bottleneck Pruning.** Following the pruning of the embedding dimension \((E)\) and its coupled structures, we exclusively focus on pruning the bottleneck dimension. As the dimension of intermediate

Figure 2: The provided figure depicts our alternate slimming process with a 50% pruning ratio on SAM-B. We utilize structural pruning at the channel-wise group level to compress SAM’s image encoder, coupled with knowledge distillation from intermediate layers to restore the pruned encoder. The red numbers highlight the pruned dimensions at each pruning step.

features \((\{h_{i}\}_{i=1}^{K})\) in each block are entirely decoupled, we can systematically apply dimension pruning at various ratios for each block while maintaining the predetermined overall pruning ratio. This approach involves utilizing a global ranking of importance scores to conduct global structural pruning.

**Embedding Aligning.** The pruned encoder \(v_{2}\) will learn from the embeddings \(E_{v_{1}}\) and final image embedding \(T_{v_{1}}\) from the pruned encoder \(v_{1}\) to expedite knowledge recovery. Simultaneously, it also computes loss functions based on the final image embedding \(t_{v_{0}}\) from the original encoder \(v_{0}\) to enhance the precision of knowledge recovery. The total loss function for embedding aligning is defined as:

\[_{Emb}=(_{MSE}(E_{v_{1}},E_{v_{2}})+_{MSE}(t_{v_{1}},t_{v_{2}}))\] (7) \[+(1-)_{MSE}(t_{v_{0}},t_{v_{2}}),\]

where the dynamic weight \(\) of \(n\)th epoch is defined as:

\[=\{&n<N\\ 0&n>=N..\] (8)

The dynamic weight \(\) will progressively diminish to zero as the distillation process unfolds. This transition in the learning objective of distillation gradually shifts from \(v_{1}\) to \(v_{0}\) contributing to a smoother knowledge recovery. We also set \(N=10\) for embedding aligning.

**Remark.** The implementation of alternate slimming on decoupled sub-structures significantly reduces the disruption to the original model, particularly when the pruning ratio is quite high. This strategy also preserves consistent dimensionality, enabling effective intermediate feature distillation, which is especially beneficial in data-scarce conditions. Consequently, in comparison to the previous pruning and distillation methods, our alternate slimming achieves a 3.40% and 0.92% increase in MIoU when the pruning ratios achieve 77% and 50%.

## 4 Experiments

### Experimental Settings

**Implementation Details.** Our SlimSAM has been implemented in PyTorch  and trained on a single Nvidia Titan RTX GPU using only 0.1% (10,000 images) of the SA-1B  dataset. The base model of our framework is SAM-B . The model's parameters were optimized through the ADAM  algorithm with a batch size of 4. Training settings for both bottleneck aligning and embedding aligning are identical. The pruned models undergo distillation with an initial learning rate of \(1e^{-4}\), which will be reduced by half if validation performance does not improve for 4 consecutive epochs. The total training duration is 40 epochs for SlimSAM-50 (with a 50% pruning ratio) and 80 epochs for SlimSAM-77 (with a 77% pruning ratio). We exclusively compressed the image encoder while retaining SAM's original prompt encoder and mask decoder.

**Evaluation Details.** To ensure a fair quantitative evaluation of the compressed SAM models, we compute MIoU between the masks predicted by the model and the ground truth masks of the SA-1B dataset. We use the most challenging single-point prompts given in annotations for experiments. The results using box prompts are also reported in our Appendix. For efficiency evaluation, we provide information on parameter counts and MACs. Additionally, we present details about training data, training iteration and training GPUs for evaluating the training cost. Qualitative comparison of results obtained using point prompts, box prompts, and segment-everything prompts are also shown in the following section.

### Comparision and Analysis

**Comparing with existing SAM compression methods.** As depicted in Table 1, we conducted a comprehensive comparison encompassing performance, efficiency, and training costs with other SOTA methods. Our SlimSAM-50 and SlimSAM-77 models achieve a remarkable parameter reduction to only 4.0% (26M) and 1.4% (9.1M)of the original count, while also significantly lowering computational demands to just 3.5% (98G) and 0.8% (23G) MACs, all while maintaining performance levels comparable to the original SAM-H. In contrast to other compressed models, our approach yields substantial performance enhancements while simultaneously achieving greater lightweight and efficiency. SlimSAM consistently delivers more accurate and detailed segmentation results across various prompts, preserving SAM's robust segmentation capabilities to the greatest extent. This qualitative superiority over other models is visually evident in Figure 5 and 6. Our approach demonstrates outstanding levels of accuracy and correctness. Most notably, SlimSAM achieves these remarkable outcomes with exceptionally low training data requirements, utilizing merely 0.1% (10k) images of the SA-1B dataset. This represents a significant reduction in data dependency, requiring 10 times less data than both EdgeSAM and MobileSAM, and 1,100 times less data than EfficientSAM.

**Comparing with other structural pruning methods.** Having demonstrated structural pruning's efficacy for SAM compression, we established a benchmark for evaluating various pruning methods. SlimSAM is compared with four commonly used pruning methods: random pruning, magnitude pruning, Taylor pruning, and Hessian pruning, each employing different criteria for pruning. Additionally, we conducted comparisons with scratch-distilled models, which are randomly initialized networks sharing the same architecture as the pruned models. To ensure a completely equitable comparison, models with the same pruning ratios were subjected to identical training settings. Table 2 showcases our method's consistent superiority over other structural pruning techniques, particularly at higher pruning ratios. SlimSAM-50 and SlimSAM-77 outperform existing methods, achieving a minimum 1% and 3% MIoU improvement while incurring the same training cost. It is noteworthy that the performance of scratch distillation is extremely low at such a limited training cost. This further proves the effectiveness of our workflow in preserving knowledge from the original model.

## 5 Ablation Study and Analysis

We conducted a series of ablation experiments on the SlimSAM-77 model, which features an ambitious 77% pruning ratio. To ensure a fair comparison in the ablation experiments, all evaluated

  
**Method** & **Params\(\)** & **MACs\(\)** & **TrainSet** & **BatchSize** & **GPUs** & **Iters** & **MIoU\(\)** \\  SAM-H  & 641M & 2736G & 11M(100\%) & 256 & 256 & 90k & 78.30\% \\ SAM-L  & 312M & 1315G & 11M(100\%) & 128 & 128 & 180k & 77.67\% \\ SAM-B  & 93M & 372G & 11M(100\%) & 128 & 128 & 180k & 73.37\% \\  FastSAM-s  & 11M & 37G & 220k(2\%) & 32 & 8 & 625K & 30.72\% \\ FastSAM-x  & 68M & 330G & 220k(2\%) & 32 & 8 & 625K & 35.41\% \\ MobileSAM  & 9.8M & 40G & 100k(1\%) & 8 & 1 & 100k & 62.73\% \\ EnficiensSAM- & 10M & 28G & 12.2M(110\%) & 128 & 64 & 450k & 69.42\% \\ EfficientSAM-s  & 26M & 94G & 12.2M(110\%) & 128 & 64 & 450k & 71.19\% \\ EdgeSAM  & 9.6M & 23G & 100k(1\%) & 64 & 8 & 50k & 65.95\% \\
**SlimSAM-500urs** & **26M** & **98G** & **100.01\%** & 4 & 1 & 100k & **72.33\%** \\
**SlimSAM-77(Ours)** & **9.1M** & **23G** & **100.01\%** & 4 & 1 & 200k & 67.40\% \\   

Table 1: Comparing with other existing SAM compression methods on SA-1B dataset. We report parameter counts, MACs, training costs, and _Mean Intersection over Union_ (MIoU) for a comprehensive and fair comparison.

  
**Ratio** & **Method** & **Labels** & **Params\(\)** & **MACs\(\)** & **MIoU\(\)** \\   & SAM-H  & ✓ & 641M & 2736G & 78.30\% \\  & SAM-L  & ✓ & 312M & 1315G & 77.67\% \\  & SAM-B  & ✓ & 93M & 372G & 73.37\% \\   & Scratch Distillation & ✗ & & & 1.63\% \\  & Random Pruning & ✗ & & & 71.03\% \\  & Magnitude Pruning  & ✗ & & & 69.96\% \\  & Hessian Pruning  & ✓ & & & 71.01\% \\  & Taylor Pruning  & ✓ & & & 71.15\% \\  & **SlimSAM-500(Ours)** & ✗ & & & **72.33\%** \\   & Scratch Distillation & ✗ & & & 1.34\% \\  & Random Pruning & ✗ & & & 62.58\% \\   & Magnitude Pruning  & ✗ & 9.1M & 23G & 61.60\% \\   & Hessian Pruning  & ✓ & & & 63.56\% \\   & Taylor Pruning  & ✓ & & & 64.26\% \\   & **SlimSAM-77(Ours)** & ✗ & & & **67.40\%** \\   

Table 2: Comparing with other structural pruning methods. ‘Ratio’ signifies the pruning ratio applied to channel-wise groups. Training costs remain consistent for the same pruning ratio.

models were trained for 40 epochs on the same 10k images from the SA-1B dataset. We also conduct additional experiments to evaluate the performance of SlimSAM with even less training data.

**Disturbed Taylor Pruning.** First, we conducted an ablation study to assess the impact of our proposed disturbed Taylor pruning on distillation. This innovative approach aligns the pruning criteria with the optimization objectives of subsequent distillation, resulting in improved performance recovery. As depicted in Table 3, our disturbed Taylor pruning consistently achieves significantly superior performance at the same training cost. For both the common one-step pruning strategy and our alternate slimming strategy, our method demonstrates MIoU improvements of 0.3% and 0.85% over the original Taylor pruning, respectively.

**Intermediate Aligning.** We also evaluate the effect of incorporating aligning with intermediate layers into the distillation process. As depicted in Table 4, distilling knowledge from intermediate layers leads to significant improvements in training results. Specifically, learning from bottleneck features and final image embeddings results in a 1.22% MIoU improvement for step 1 distillation, compared to learning solely from image embeddings. Similarly, for step 2 distillation, learning from embedding features and final image embeddings achieves a 0.57% MIoU improvement over the case where learning is based solely on image embeddings.

**Alternate Slimming.** In addition, we conducted experiments to investigate the impact of our alternate slimming framework. Unlike the common one-step pruning strategy, we partition the structural pruning process into two decoupled and progressive steps. In the first step, only the dimensions related to the embedding features are pruned, while in the second stage, only the dimensions related to the bottleneck features are pruned. Following both embedding and bottleneck pruning,

Figure 4: The intermediate dimensions of QVK Attention (top row) and MLP (bottom row) within each ViT after pruning. We present the outcomes of local pruning and global pruning under five distinct normalization methods.

  
**Method** & **MIoU\(\)** \\  Taylor Pruning & 62.04\% \\ Disurbed Taylor Pruning & **62.31\%** \\   SlimSAM-77 + Taylor & 63.63\% \\ SlimSAM-77 + Disturbed Taylor & **64.48\%** \\    
  
**Step** & **Distillation Objective** & **MIoU\(\)** \\  Step 1 & Final Image Embeddings & 65.10\% \\ Step 1 & + Bottleneck Features & **66.32\%** \\  Step 2 & Final Image Embeddings & 63.91\% \\ Step 2 & + Embedding Features & **64.48\%** \\   

Table 3: Comparison between disturbed Taylor pruning and original Taylor pruning.

Figure 3: Training results on SA-1B with the common one-step method and our alternate slimming framework. Left and right are results with disturbed Taylor importance and random importance.

knowledge distillation with intermediate layer aligning is employed on the pruned model to recover its performance. For a more exhaustive analysis, we present the results obtained using different pruning criteria to assess whether the effectiveness of our method is influenced by importance estimation. As illustrated in Figure 3, our alternative slimming framework yields substantial improvements in MIoU, with gains of 3.9% and 3.5% observed under disturbed Taylor importance estimation and random importance estimation.

**Global Pruning vs Local Pruning.** Finally, we conducted experiments to evaluate the performance of local pruning and global pruning in bottleneck pruning. Given that the bottleneck dimensions in each block are entirely decoupled, we systematically applied channel-wise group pruning at various ratios for each block while preserving the predefined overall pruning ratio in this step. To obtain a consistent global ranking, we normalized the group importance scores \(I_{}\) of each layer in five ways: (i) Sum: \(I_{i}=i}}{_{i=1}^{K}I_{i}}\), (ii) Mean: \(I_{i}=i}}{_{i=1}^{K}I_{i}/K}\), (iii) Max: \(I_{i}=i}}{Max^{K}_{i=1}(I_{i})}\), (iv) Standardization: \(I_{i}=i}-Max^{K}_{i=1}(I_{i})}{Max^ {K}_{i=1}(I_{i})-Min^{K}_{i=1}(I_{i})+1e-8}\), (v) Gaussian: \(I_{i}=i}-_{i=1}^{K}I_{i}/K}{ ^{K}_{i=1}(I_{i})+1e-8}\). As indicated in Table 5, local pruning ensures consistent performance, whereas global pruning raises the model's upper-performance limit. Global pruning's efficacy is highly dependent on the chosen importance normalization method. For our model, we opted for global pruning with Gaussian normalization, which yielded the best training results. Following global pruning, Figure 4 illustrates the dimensions of bottleneck features (QKV embeddings and MLP hidden embeddings) within each ViT in the image encoder. When applying mean, sum, or Gaussian normalization, the ViTs in the middle exhibit more group redundancy compared to those at the beginning and end. However, the pruned dimensions do not display distinct patterns when utilizing max or standardization normalization. The impact of global pruning becomes more pronounced with an increased number of training iterations. Specifically, when training extends to 80 epochs, the MIoU for global pruning exceeds that of local pruning by approximately 2%.

**Even less data.** As shown in Table 6, with a pruning ratio of 50%, a reduction in the volume of training data only marginally impacts the model's performance. Notably, even when trained with a limited dataset of just 2,000 images, our SlimSAM-50 model remarkably attains an MIoU of nearly 70%. However, as the pruning ratio is elevated to 77%, a decrease in training data more significantly affects performance. This leads to the inference that although our methodology, which integrates pruning and distillation techniques, mitigates the need for extensive training datasets, the availability of more training data can still enhance model performance, particularly at higher pruning rates.

## 6 Conclusion

In this paper, we present a novel data-efficient SAM compression method, SlimSAM, which achieves superior performance with minimal training data. The essence of our approach lies in the efficient reuse of pre-trained SAM, avoiding the need for extensive retraining. We introduce key designs to the compression method for enhancing knowledge retention from the original model in data-limited situations. Specifically, our alternate slimming framework carefully prunes and distills decoupled model structures in an alternating fashion, minimizing disruptions to the original model and enabling the intermediate feature alignment by consistent dimensionality. Furthermore, the proposed disturbed Taylor importance estimation rectifies the misalignment between pruning objectives and training targets, thus boosting post-distillation after pruning. SlimSAM convincingly demonstrates its superiority while imposing significantly lower training costs compared to any other existing methods.

  
**Pruning Ratio** & **Data** & **Iters** & **MIoU\(\)** \\   & 10k & 100k & **72.33\%** \\  & 5k & 100k & 71.89\% \\  & 2k & 100k & 69.79\% \\   & 10k & 200k & **67.40\%** \\  & 5k & 200k & 64.47\% \\   & 2k & 200k & 61.72\% \\   

Table 6: Comparision of training results using varied amounts of training data.

  
**Method** & **Normalization** & **MIoU\(\)** \\  Local Pruning & — & 64.38\% \\   & Mean & 63.64\% \\  & Max & 64.35\% \\  & Sum & 63.55\% \\  & Gaussian & **64.48\%** \\  & Standardization & 64.14\% \\   

Table 5: Effect of global pruning evaluated under five different normalization approaches.