# Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images

Qi Song\({}^{1,2}\), Ziyuan Luo\({}^{1,2}\), Ka Chun Cheung\({}^{2}\),

Simon See\({}^{2}\), Renjie Wan\({}^{1}\)

\({}^{1}\)Department of Computer Science, Hong Kong Baptist University

\({}^{2}\)NVIDIA AI Technology Center, NVIDIA

{qisong,ziyuanluo}@life.hkbu.edu.hk

{chcheung,ssee}@nvidia.com, renjewan@hkbu.edu.hk

Corresponding author.

###### Abstract

Single-view 3D reconstruction methods like Triplane Gaussian Splitting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed "geometry cloaks", into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak. Our project is available at https://qsong2001.github.io/geometry_cloak.

## 1 Introduction

With the increasing importance of 3D assets, several methods have been proposed to reconstruct or generate 3D models from single 2D images. Combining with Tensorial Radiance Fields , Triplane-based Gaussian Splatting (TGS)  presents a compelling approach for producing 3D models from single-view images. However, malicious users could potentially exploit TGS  to generate 3D models from single-view images without authorization, posing a threat to the interests of image copyright owners. To address this issue, it is essential for image owners to implement measures that can safeguard their copyrighted images from being used by TGS .

Digital watermarking [6; 51] is an effective way of claiming the copyright of digital assets. Thus, one potential method for safeguarding copyrighted images is to embed unique messages within images intended for building a 3D model and then extract the embedded messages from the reconstructed 3D model. However, previous methods have proven it is difficult to transfer the embedded copyright messages in 2D images into 3D models [31; 19]. Moreover, even if we can embed and extract copyright messages, the 3D models might have already been used by others before the copyright being claimed.

To prevent unauthorized 3D reconstruction from copyrighted images via TGS , a possible approach is to incorporate adversarial perturbations [11; 28] into input images intended for TGS.

Adversarial methods [11; 28] have already achieved promising results by introducing disturbances into input images to prevent neural models from functioning correctly. When it comes to TGS , a straightforward solution is to incorporate such adversarial perturbations into input images by maximizing the difference between rendered and ground truth views like previous methods [9; 13]. However, those methods [9; 13] intuitively focus on disturbing rendered results, ignoring perturbation-prone components of TGS . As a result, those conventional adversarial perturbations can only lead to limited changes to the reconstructed results [9; 13], which may still be used for several illicit applications. Besides, simple adversarial perturbations can only disrupt the results of 3D reconstruction in an uncontrollable manner but do not support traceability. The users have difficulties claiming copyright after the disturbance-affected 3D generation of images.

We envision a novel scenario where copyrighted images can induce TGS to generate compromised content with an identifiable pattern. To achieve this goal, a naive solution is to employ image cloaking. Traditionally, image cloaking techniques [23; 37; 41; 35] are used to prevent the image from malicious editing by diffusion models . The cloaking integrated into images can shift image features into another domain through adversarial perturbations. Consequently, this manipulation directs diffusion models to generate predetermined specific outcomes. If malicious users attempt to create 3D models using these protected images, the resulting compromised 3D models will be unusable. Besides, the identifiable pattern exhibited can help the image owner assert their copyrights in the event of legal inquiries.

However, unlike image cloak methods [35; 23] against diffusion models , simply perturbing image features cannot effectively induce TGS (as shown in Fig. 1). Image features in TGS show strong robustness against disturbances, even under relatively strong attack settings. Thus, the key becomes how to identify perturbation-prone components in TGS  and capitalize on this weakness to induce reconstructed results. TGS  contains image and geometry features during its 3D model reconstructions. Previous works [8; 3] have shown the geometry feature in 3D Gaussian Splatting  is easier to be manipulated with external operations. Therefore, we wonder if it is possible to manipulate the estimated point cloud process with an invisible disturbance. Based on this simple observation, we propose embedding invisible adversarial perturbations as a geometry cloak on images intended for TGS , which can affect and manipulate estimated point clouds in the process of TGS's 3D reconstruction.

Figure 1: Overview of our scenario. (a) Images without protection. Images can be easily reconstructed into 3D models by malicious users with TGS , posing a threat to the copyright of the image owner. (b) Digital Watermarking offers a solution by embedding copyright messages into the view-image before 3D reconstruction. However, the embedded message cannot be extracted from novel rendered views. (c) Geometry Cloak. Our geometry cloak utilizes the disturbance-prone components of TGS, achieving view-specific watermark embedding. Our method can compromise the unauthorized reconstructed 3D model while providing a verifiable pattern for copyright claim.

As shown in Fig. 2, we introduce a geometry cloak, which is carefully crafted to induce TGS  to fail 3D model generation and reveal our embedded pattern. To induce the TGS to reveal the embedded patterns, we propose a view-specific Projected Gradient Descent  (view-specific PGD) strategy by optimizing the distance between the projected point cloud view and predefined patterns. The PGD iteratively updates the geometry cloak by minimizing CD loss, ultimately revealing the desired view and uncovering the hidden patterns within the TGS . When malicious users reconstruct 3D models using the protected images with TGS, the compromised geometry information causes TGS to reveal the embedded message. Unlike traditional copyright protection methods like digital watermarking [25; 51; 39], which typically entail additional procedures for extracting the watermark post-use. Our approach directly transforms TGS  into a message disclosure tool by inducing it to yield a specific stylistic outcome, facilitating the verification of image ownership rights. In summary, our main contributions are threefold:

* We propose the concept of "geometry cloaking", which can prevent unauthorized image-to-3D generation by TGS , and it also leaves a verifiable copyright pattern.
* Our geometry cloaking technique explores the perturbation-prone components of Triplane-based Gaussian Splatting and utilizes this vulnerability to achieve the protection of images.
* We propose a view-specific PGD strategy, which can embed identifiable patterns into a specific view of the reconstructed 3D model.

As our approach attacks the geometry features that are widely used in various single-image-to-3D approaches, it demonstrates generalization capability to other GS-based single-view to 3D approaches like LGM . The results can be found in our experiments.

## 2 Related work

### Building 3D models from single images

There has been a surge of research on learning to generate novel views from a single image [47; 36; 52]. This task aims to infer the 3D structure of a scene from a single 2D image and render new perspectives, enabling applications in virtual reality, augmented reality, and computer-aided design. One approach is NeuralLift-360 , which incorporates a CLIP loss  to enforce similarity between the rendered image and the input image. Another method, 3DFuse , fine-tunes the Stable Diffusion model  with LoRA  layers and a sparse depth injector. Recent work like Zero123  takes a different approach by fine-tuning the latent stable diffusion model  to generate novel views based on relative camera pose. Recently, Triplane-based neural rendering methods [2; 52; 12] adopt a novel approach to model and reconstruct radiance fields. Unlike NeRF, which uses pure MLPs, Tensorial Radiance Fields (TensoRF)  consider the full volume field as a 4D tensor and propose to factorize the tensor into multiple compact low-rank tensor components for efficient scene modeling. Combing TensoRF  with novel 3D Gaussian Splatting , Zou _et al._propose a Triplane-base Gaussian Splatting , which can obtain a 3D model from single-view image within seconds . With continued research and development, more impressive and realistic 3D reconstructions in the future will enable immersive experiences and streamlined design workflows. The advancements in creating 3D models from a single-view image offer significant potential for diverse applications and digital assets. Consequently, it is essential to address the protection of copyrighted images to prevent their misuse in generating 3D models.

### Adversarial attacks for neural rendering

Adversarial attacks [11; 28] have become a significant concern in the field of computer vision [22; 16]. These attacks aim to deceive machine learning models by introducing adversarial perturbations to the input data, leading to incorrect predictions such as misclassification . While initially studied in the context of image classification models, adversarial attacks have also been explored in other domains [35; 23]. For Neural Radiance Fields (NeRFs) , several works have proposed methods to perturb or enhance the original NeRF framework. NeRFs are a class of models that can synthesize high-quality 3D scenes from 2D images by learning the scene's volumetric representation and appearance. However, NeRFs are also vulnerable to adversarial attacks. Several recent works have investigated different techniques to perturb or enhance the original NeRF framework using adversarialattacks. NeRFool  presents an approach to manipulate NeRFs by perturbing the scene's geometry and appearance using adversarial attacks. NeRFTargeted  focuses on targeted perturbations in NeRFs, allowing for the optimization of scene parameters to generate desired target images. ShieldingNeRF  introduces a technique to protect sensitive information in NeRF-generated views by introducing obfuscating perturbations. These works demonstrate the potential of perturbing NeRFs for various objectives, including adversarial attacks, targeted image manipulation, and privacy protection, contributing to the advancement of NeRF-based models [2; 29; 27] in computer graphics and computer vision tasks. However, with the promising developments in 3DGS , research on defending against attacks on Gaussian Splatting renderings remains an area that requires further investigation.

### Image cloaking

With the rapid advancement of AI models, the risk of misuse has raised concerns, particularly in the malicious use of public data [43; 42; 50; 17]. Researchers advocate a proactive approach to prevent such misuse by adding subtle noise to images prior to publication. This technique aims to disrupt attempts at exploitation [21; 26; 20]. An important application of image cloaking is to prevent privacy violations from face recognition systems [38; 4; 15]. By introducing noise patterns to facial regions, these methods degrade face recognition model performance while maintaining visual quality. Image cloaking can also thwart image manipulation through GAN-based techniques like DeepFakes  by corrupting latent representations. Recent image cloaking methods [37; 23; 41] focus on protecting the copyrighted image from misuse by stable diffusion models. These methods disrupt artistic mimicry and harmful personalized images, aiming to prevent unauthorized exploitation. For example, GLAZE  and AdvDM  primarily focus on disrupting artistic mimicry and harmful personalized images generated by text-to-image models. Anti-Dreambooth  concentrates on fine-tuning DreamBooth  for malicious face editing. These techniques aim to cloak input images in a way that disrupts the model's ability to generate personalized content while preserving the overall visual quality. Existing methods focus on protecting the copyrighted image from misuse by disturbing image features. By contrast, our method focuses on preventing copyrighted from being 3D reconstructed by TGS  without authorization, facing 3D scenes and complex copyright validity verification settings.

## 3 Preliminaries of TGS

TGS  introduces a novel hybrid 3D representation that integrates an explicit point cloud with an implicit triplane, enabling efficient and high-quality 3D object reconstruction from single-view images. As shown in Fig. 2, the representation consists of a point cloud \(^{N 3}\) providing explicit geometry, and a triplane \(T^{3 C H W}\) encoding an implicit feature field, where \(T=(T_{xy},T_{xz},T_{yz})\) comprises three orthogonal feature planes. For a given position \(x^{3}\) from the point cloud, the corresponding triplane feature \(f_{t}\) is obtained by trilinear interpolation and concatenation of features from the three planes:

\[f_{t}=(T_{xy},_{xy})(T_{xz}, _{xz})(T_{yz},_{yz}).\] (1)

Utilizing this hybrid representation, the 3D Gaussian attributes like opacity \(\), anisotropic covariance (scale \(s\) and rotation \(q\)), and spherical harmonics coefficients \(sh\) are decoded from \(f_{t}\) augmented with projected local image features \(f_{l}\) using an MLP \(_{g}\):

\[( x^{},,s,q,sh)=_{g}(x,f_{t} f_{l}).\] (2)

Together, these parameters parameterize the 3D Gaussian kernel attributes around the point \(x\), enabling differentiable Gaussian splatting for rendering.

TGS represents a cutting-edge approach to 3D object reconstruction from single-view images, combining explicit and implicit representations to achieve accurate and detailed reconstructions. By leveraging advanced decoding mechanisms and efficient rendering techniques, the model demonstrates superior performance in generating realistic 3D models with intricate geometry and textural details.

## 4 Methodology

**Overview.** With the increasing capabilities of 3D reconstruction techniques like TGS , there is a risk of malicious users exploiting these methods to generate 3D models from copyrighted imageswithout authorization, infringing on the rights of image owners. We propose geometry cloak, a novel solution by embedding invisible perturbations on the input images intended for TGS . These perturbations are crafted to induce TGS to fail the reconstruction in a distinct way, producing an identifiable pattern in the corrupted 3D output.

This section presents the methodology of inhibiting 3D reconstruction of TGS  via the introduced geometry cloak. Our method consists of two stages: (1) Building verifiable geometry patterns (Section 4.1), and (2) Optimizing geometry cloak with view-specific PGD (Section 4.2).

### Building verifiable geometry pattern

As illustrated in Fig. 2, to obtain a 3D model from single-view images, TGS  encodes the single-view image \(\) and its associated camera parameters into image features. Following this, a point cloud decoder is adopted to project image features onto the point cloud. Then, a triplane decoder converts the image feature into the triplane latent \(T\). Finally, 3D Gaussians are decoded from triplane feature \(f_{t}\) for novel view synthesis.

Our methodology diverges by targeting the explicit geometry features of the point cloud. Perturbing the point cloud directly is inherently more effective due to the inherent vulnerabilities in the TGS reconstruction process. In TGS , the point cloud offers a distilled and direct representation  of the scene's structure, making modifications on them quite evident [8; 3]. Besides, the point cloud is not only foundational geometry information but also typically helps subsequent processing to obtain the final output. Point clouds provide essential geometry information that is instrumental in the sampling of features from the latent triplane representation (Eq. (1)). Moreover, by introducing strategic alterations to the geometry feature, we bypass some of the inherent robustness found in image-level features (Table 1) and avoid the complex transformations between image and geometry spaces. This direct manipulation allows for more precise control over the adversarial impact, exploiting specific vulnerabilities in TGS and leading to more pronounced disruptions in its output.

Different types of target geometry pattern.Based on the inherent vulnerabilities of point clouds, we propose two types of verifiable pattern (Fig. 3), including (1) _Pre-defined patterns_ and (2) _Customized patterns_. Through these patterns, the reconstructed 3D model undergoes consistent changes in geometry and visual appearance (novel views).

_Pre-defined patterns_ are 2D point clouds that are transformed from alphanumeric characters images, creating a direct and straightforward representation of watermarks. To obtain these 2D point clouds, we segment the image of alphanumeric characters and record the coordinates of these alphanumeric

Figure 2: Overall of our proposed method. We propose to induce the 3D reconstruction process with our geometry cloak. (a) The core representation of TGS  includes an explicit point cloud and an implicit triplane-based feature field. The features of the novel view image are extracted through the coordinates in the point cloud. (b) The target patterns (Section 4.1) are designed to induce the final reconstruction result. (c) In order to make the reconstruction result show some distinguishable characteristics, we use projected gradient descent (PGD)  to iteratively optimize the reconstructed point cloud so that it has consistent characteristics with the target point cloud (Section 4.2).

characters' segmentation. Then, we sample the point from segmentation coordinates and form a 2D point cloud as pre-defined patterns.

_Customized patterns_ are a more personalized approach where users can selectively protect a certain part of an image. In this setting, we first extract a point cloud from the image that requires safeguarding. We adopt the same methods from TGS  to obtain the point cloud \(}\) in \(E_{1}\). In \(E_{2}\), users can refine this point cloud to select the area they wish to protect. Users can employ text-guided editing techniques like instructP2P  or turn to open-source software like MeshLab  to further shape and customize the point cloud. This process not only enhances the visual appeal of the point cloud but also embeds a layer of security by aligning it with specific textual instructions or user intents.

### Optimizing geometry cloak with view-specific PGD

In this section, by exploiting the vulnerability of TGS , we present view-specific PGD to determine geometry cloak \(\), which can minimize the distance between reconstructed point clouds in TGS and target pre-defined patterns.

_Geometry Cloak \(\)_. Geometry cloak \(\) is crafted to mislead the TGS into a controlled 3D reconstruction with imperceptible perturbations into single-view images. It minimizes the difference to the target geometry pattern while preserving image fidelity perceptible to the human eye through adversarial training .

Specifically, our geometry cloak \(\) aims to minimize the Chamfer Distance \(_{}\) between point cloud \(\) from image \(\) via TGS and target geometry pattern \(_{}\) in a certain view. The overall objective can be expressed as follows:

\[:=_{\|\|_{}}_{}( (+),_{}),\] (3)

where \(\) denotes the network that maps image into point cloud in TGS , and \(\) represents the perturbation budget.

_View-specific PGD_. To embed pre-defined patterns in a specific viewing direction, we develop a view-specific PGD. The optimization iteratively adjusts the geometry cloak \(\) to make the projected point cloud closer to the pre-defined patterns while keeping the image visually similar to the source. The updates use gradient decent  on \(_{}\) with learning rate \(\), moving the cloaked image towards the target. This iterative process manipulates the TGS's perception while maintaining visual similarity as below:

\[^{(i+1)}=^{(i)}+(_{}_{}(}_{}^{i},_{ })),\] (4)

where \(}_{}^{i}=_{}((^ {(i)}+^{(i)})\), denoting the projected point cloud at viewing direction \(\) from cloaked image \((^{(i)}+^{(i)})\), and \(_{}\) stands a 2D point cloud watermarks. We encapsulate the geometry cloaking process in Algorithm 1.

Figure 4: Example of View-specific PGD. We use a 2D point cloud pre-defined pattern \(_{}\) as the target geometry pattern. The watermark is embedded at the viewing direction \(=xy\).

Figure 3: Two different target geometry patterns. (1) Pre-defined patterns: we directly convert alphanumeric characters into a 2D point cloud as watermarks. (2) Customized patterns: In \(E_{1}\), we first extract the point cloud of the image that needs to be protected. In \(E_{2}\), we edit the acquired point cloud through text-guided methods like instructP2P  or open-source software meshlab .

``` Input: Input image \(\), Point cloud encoder \(\) from TGS, pre-defined pattern \(_{}\) number of steps \(N\), step size \(\), perturbation budget \(\), viewing direction \(\) Output: Geometry cloaked image \(}\)  Initialize geometry cloak \( 0\), and geometry cloaked image \(}\) for\(i 1\)to\(N\)do \(}(})\)// Estimate point cloud representations; \(}_{}_{}(})\)// Project the point cloud at view direction \(\); \(Loss_{}(}_{},_{ })\)// Calculate CD between two 2D point clouds; \((_{}Loss)\)// Update geometry cloak; \(}}+\)// Update cloaked image; \(}(},- ,+)\)// Clip cloaked image \(}\);  end for Return\(}\) ```

**Algorithm 1**Optimizing Geometry Cloak with view-specific PGD

### Implementation details

Our method uses the PyTorch framework on a single NVIDIA V100 GPU. Our geometry cloak is obtained through optimization (Section 4.2) using projected gradient descent . We adopt a mask version of PGD  for calculating geometry cloak, as only the object in the image is used for 3D reconstruction without its background. The input image \(^{(0)}=\) is initialized, and iteratively updated for \(N=100\) steps with a step size of \(=0.001\). The loss is defined as the Chamfer Distance  between the predicted point cloud \(\) and the target point cloud \(}_{tar}\). For pre-defined patterns, we adopt the proposed view-specific PGD as we want to embed the 2D pre-defined patterns into a certain viewing direction \(\). For customized patterns, we directly calculate the distance between customized patterns and predicted point clouds, as both are 3D point clouds. The updated image \(^{(i)}\) is clipped to the valid range \(\), and after \(N=100\) iterations, the final image \(}=^{(N)}\) are obtained as the geometry cloaked output.

## 5 Experiments

### Settings

**Dataset.** To evaluate the performance of our method, we conduct experiments on the Google Scanned Objects (GSO)  and OmniObject3D (Omni3D)  datasets. Both datasets embody a large diversity of view images and provide a rich and varied set of data for assessing the performance of our method. For GSO , we select \(1\) image view for each of the \(1030\) objects, resulting in \(1030\) images. For Omni3D , we choose \(5\) image views from each of the 190 classes in Omni3D, resulting in a total of \(950\) images.

**Baselines.** To investigate the effectiveness of our approach, we evaluate the impact of different perturbations on the reconstructed results by TGS . We experiment with four types of perturbations strategy: (1) **Gauss. noise**, _i.e._, random Gaussian noise is added to the protected image. (2) **Adv. image**, _i.e._, adversarial attacks on image feature ; (3) **Geometry cloak w/o target**, _i.e._, adversarial attacks on the point cloud. For Adv. image and geometry cloak w/o target, we directly use the norm value of features  as the optimizing loss. (4) **Geometry cloak**. For the geometry cloak, we randomly select letters "A-Z" and numbers "\(1\)-\(9\)" as the pre-defined pattern.

**Evaluation methodology.** We report the quantitative metric between no perturbation and perturbed reconstructed results. Specifically, for visual quality, we report image similarity metrics: PSNR, SSIM , LPIPS . For geometry quality, we report the Chamfer Distance (CD) . We further present the qualitative customized 3D reconstruction by inducing the point from the single-view image into another domain.

[MISSING_PAGE_FAIL:8]

obtained 3D model. On the other hand, attacks on point clouds only require very small, invisible perturbations to change the reconstructed 3D model greatly.

Pre-defined pattern.Our method is proposed to perturb explicit geometry and develop a view-specific pre-defined pattern. To evaluate the impacts of our geometry cloak on reconstructing 3D models via TGS , we randomly select letters "A-Z" and numbers "\(1\)-\(9\)" as the pre-defined patterns. As shown in Table 1, we can see that after specifying the target point cloud, the reconstructed 3D model is comprised both in vision and geometry results, making the reconstructed model unusable. We further demonstrate the qualitative results in Fig. 6, showing that the watermark message can be re-emerged in the specific view perspective. This indicates that our method can preserve identifiable embedded information while ensuring the integrity of the 3D model.

Customized pattern.We also demonstrate the performance of our method when using customized patterns. In this setting, the users can selectively protect specific parts of the image that need not be reconstructed for customized protection. As shown in Fig. 6, users can choose to remove certain object parts (_e.g._, tail, body, head). The results indicate that users can effectively influence the reconstruction results with our geometry cloak, further demonstrating that point clouds in TGS are susceptible to perturbation. We provide more results to show that our geometry cloak can be generalized to other GS-based single-view to 3D method  in the appendix.

## 6 Conclusion

We present a novel geometry cloaking approach to protect image copyrights from unauthorized 3D reconstruction with Triplane Gaussian Splatting (TGS). By embedding carefully optimized perturbations in the geometry feature space that encodes a customized watermark message, our method forces TGS to fail reconstruction in a distinct way - generating the watermarked pattern. Extensive experiments validate our strategy of focusing perturbations on the geometry components of TGS, which can reliably induce watermarks with invisible perturbations. Our geometry cloaking introduces a novel method for protecting copyrights tailored to the representations of single-view to 3D models.

Figure 6: Qualitative results of two different target geometry patterns. (a) Pre-defined patterns: The letters “A” and “X” are used as watermark messages. The embedded watermark can be effectively observed from a certain perspective. (b) Customized patterns: Users can selectively control the parts that need protection, causing the 3D reconstruction of corresponding parts to fail. More qualitative experimental results are provided in the Appendix.

Limitations and broader impacts.The multifaceted nature of copyright protection requires that our method be developed and used responsibly, respecting the delicate balance between innovation and intellectual property rights. Collaboration across technology, legal, and policy sectors is essential to address these complexities.

Acknowledgement.This work was done at Renjie's Research Group at the Department of Computer Science of Hong Kong Baptist University. Renjie's Research Group is supported by the National Natural Science Foundation of China under Grant No. 62302415, Guangdong Basic and Applied Basic Research Foundation under Grant No. 2022A1515110692, 2024A1515012822, and the Blue Sky Research Fund of HKBU under Grant No. BSRF/21-22/16.