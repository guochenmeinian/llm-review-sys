# Satformer: Accurate and Robust Traffic Data Estimation for Satellite Networks

Liang Qin

Xidian University

liangqin@stu.xidian.edu.cn &Xiyuan Liu

Xidian University

markliu225@stu.xidian.edu.cn &Wenting Wei

Xidian University

wtwei@xidian.edu.cn &Chengbin Liang

Xidian University

chengbin@stu.xidian.edu.cn &Huaxi Gu

Xidian University

hxgu@xidian.edu.cn

Corresponding author

###### Abstract

The operations and maintenance of satellite networks heavily depend on traffic measurements. Due to the large-scale and highly dynamic nature of satellite networks, global measurement encounters significant challenges in terms of complexity and overhead. Estimating global network traffic data from partial traffic measurements is a promising solution. However, the majority of current estimation methods concentrate on low-rank linear decomposition, which is unable to accurately estimate. The reason lies in its inability to capture the intricate nonlinear spatio-temporal relationship found in large-scale, highly dynamic traffic data. This paper proposes Satformer, an accurate and robust method for estimating traffic data in satellite networks. In Satformer, we innovatively incorporate an adaptive sparse spatio-temporal attention mechanism. In the mechanism, more attention is paid to specific local regions of the input tensor to improve the model's sensitivity on details and patterns. This method enhances its capability to capture nonlinear spatio-temporal relationships. Experiments on small, medium, and large-scale satellite networks datasets demonstrate that Satformer outperforms mathematical and neural baseline methods notably. It provides substantial improvements in reducing errors and maintaining robustness, especially for larger networks. The approach shows promise for deployment in actual systems.

## 1 Introduction

As a potential complement to terrestrial networks, satellite networks are envisioned to provide broadband connectivity with seamless coverage and in a cost-effective manner. Internet service and content providers are interested in satellite networks due to their wide international coverage and lower entry costs in rural and underdeveloped areas .

Traffic engineering [2; 3; 4] and topology engineering  of satellite networks, such as access control, routing and congestion control, are key to achieve efficient control of satellite networks, which rely on real-time perception of global traffic data . Timely and accurate traffic measurements beyond basic metrics are undoubtedly beneficial for such applications.

However, it is troublesome and costly to collect massive traffic data by measuring all transmission pairs directly , since traffic data is naturally distributed throughout the entire network. In order to support the network operation of the emerging mega-constellations, there is an urgent need toexplore cost-effective traffic measurement methods. Traffic data estimation is a feasible approach for large-scale satellite networks, where the global traffic data can be estimated according to partial traffic sampling and measurement .

Due to inherently dynamic natures of spatial distances and orbital positions in satellite networks, traffic volumes and patterns vary over time . Emerging mega-constellations networks typically involve numerous satellites, so the dynamic traffic data between satellite pairs can be represented as high-dimensional matrices or tensors. This complexity makes it difficult to capture the complicated relationships within the data . Furthermore, the instability of inter-satellite and satellite-ground links often leads to the loss of traffic data during transmission. It should also be noted that not all satellite pairs have constant communication demands. As a result, the measured traffic data is often sparse and incomplete, making traffic data estimation complex . Therefore, the primary challenge in accurately and robustly estimating satellite network traffic data lies in effectively capturing the complex and nonlinear spatio-temporal correlations while maintaining robustness for varying sequence lengths .

Indeed, most efforts in traffic data estimation focus solely on low-rank linear decomposition, which cannot effectively capture the nonlinear spatio-temporal correlations among large-scale and dynamic traffic data, leading to inaccurate estimations. Therefore, developing a novel approach is crucial for enhancing traffic estimation performance to effectively extract and utilize the complex and nonlinear spatio-temporal correlations among inter-satellite traffic data.

For large-scale and highly dynamic satellite network traffic data, we propose Satformer, a new neural network architecture designed for accurate and robust traffic estimation. Satformer systematically constructs encoder-decoder components with stacked spatio-temporal modules to effectively capture complex spatio-temporal correlations in traffic data. Within each module, an adaptive sparse spatio-temporal attention mechanism (ASSIT) is adopted to extract key features from numerous sparse inputs by focusing on specific local regions. This enables Satformer to capture nuanced traffic patterns essential for accurate estimation. This is particularly useful in satellite networks where traffic may be concentrated in certain areas due to regional demand or satellite coverage. Additionally, ASSIT is more robust to sparsity as it can identify and focus on areas with higher data density, which may contain more informative traffic features, rather than being overwhelmed by overall sparsity. Simultaneously, we utilize a graph embedding module to effectively process non-Euclidean data through a Graph Convolutional Network (GCN). These components in spatio-temporal module enhance Satformer's ability to capture and exploit the nonlinear and complex information present in traffic data. Furthermore, a transfer module is incorporated to disseminate global context information throughout the model.

Our contributions are as follows:

* We designed ASSIT, which adopts a multi-head self-attention structure. It can learn the correlation representation of traffic data at different spatial and temporal scales. We added a sparsity threshold to the attention matrix to efficiently process a large number of sparse inputs. By dynamically adjusting the threshold value, ASSIT adapts to the sparsity levels of various datasets, thereby enhancing the model's inference efficiency. Additionally, ASSIT allows the model to dynamically allocate computational resources to regions of interest, making the model operate more efficiently and enhancing its scalability.
* To process non-Euclidean structured data, we introduce a graph embedding module within each module via GCN. Since the graph embedding module learns the relationship between nodes and neighbors adequately, it can extract the local and global information of nodes from non-Euclidean structured data. It improves the ability of the model to extract nonlinear spatio-temporal correlation.
* We add a transfer module to the Satformer framework, which can blend and reshape the traffic representation learned by the previous modules, conveying a global temporal and spatio perspective, while also helping to strengthen the generalization ability of the model on different types of datasets

This paper is structured as follows. Section 2 surveys relevant research. Section 3 explains our proposed Satformer methodology. Section 4 presents experimental verification and comparisons. Section 5 makes a discussion and concludes this paper.

Related Works

We provide a review of the existing work on network traffic estimation. Existing traffic data estimation methods can be mainly divided into matrix completion based, tensor completion based and neural network based methods.

Matrix Completion (MC) methods have found widespread application in the estimation of traffic data. Some algorithms, such as the convex relaxation method based on minimum nuclear norm approximation  and matrix factorization-based methods , leverage the linear spatiotemporal characteristics of traffic data to infer missing values. However, these methods are often too simplistic, which can lead to inaccurate estimations when applied to large-scale traffic data.

As an extension of matrix completion, the goal of tensor completion aims to reconstruct low-rank tensors based on sparse observations of their entries. Several studies have adopted tensor completion, including recent works [14; 15; 16].To achieve higher accuracy in traffic data estimation, these works propose the use of tensor completion methods, which can more comprehensively capture spatio-temporal features in traffic data, effectively. A typical work of such a method is LTC , which leverages the strong local correlation of the data to identify and complete each subtensor with low rank. However, many traffic estimation algorithms based on tensor completion rely on CANDECOMP/PARAFAC (CP) or Tucker decompositions, commonly using inner products as interaction functions. This approach can often reduce estimation performance to some extent due to its limited ability to capture both linear and nonlinear correlations in traffic data.

In recent years, deep learning methods have shown notable advancements in traffic network analysis. Notably, research such as NTF  and  have explored the application of deep learning models, including Recurrent Neural Networks (RNNs), to achieve adaptive grouping and prediction of traffic tensors within large-scale networks. Noteworthy among these efforts is CoSTCo , which incorporates two convolutional layers to extract features from stacked embeddings, enhancing awareness of network dynamics through the acquisition of complex spatio-temporal features. Recent studies [21; 22] employ meta-learning and other algorithms, alongside attention mechanisms, to dynamically adapt to rapid changes in traffic patterns within the network. However, current deep learning models may focus more on global features, while neglecting the local and hidden spatio-temporal correlations in traffic data, which may lead to suboptimal estimation effects

## 3 Estimation Model: Satformer

### System Model & Problem Definition

In satellite networks, inter-satellite traffic data can be modeled as a time-space matrix, which reflects the data volume to be transmitted between all node-node pairs over satellite networks. For the problem statement of traffic estimation over satellite networks, we introduce the following symbols: \(N\): Number of satellites, \(T\): Discrete time steps, we define the inter-satellite traffic matrix \( R^{I J T}\), where \(_{ijt}\) represents the data transmission from satellite \(i\) to satellite \(j\) at time step \(t\). The \(t\)-th layer of this matrix represents a discrete time step.

Considering the influence of spatio distance and transmission delay in satellite networks, we can adjust the inter-satellite traffic by introducing a weight matrix. Let \( R^{N N}\) be the weight matrix representing spatial distance and transmission delay, where \(_{ij}\) denotes the weight from satellite \(i\) to satellite \(j\). Thus, the adjusted inter-satellite traffic data matrix can be represented as \(}=\), where \(\) denotes element-wise multiplication. Taking into account these factors, the mathematical modeling of inter-satellite traffic data can be expressed as follows:

\[}_{ijt}=_{ijt}_{ij}\] (1)

where \(i,j=1,2,,N\), and \(t=1,2,,T\). This model considers the spatio distance and transmission delay between satellites, allowing the traffic data matrix to more accurately reflect the actual communication scenarios in the satellite networks. In the process of sampling and recovering inter-satellite traffic data, we begin by introducing the sampling matrix \(S\), the sampled data \(\), and the nonlinear estimation function \(F\). The sampling process can be expressed using mathematical notation: \(=} S\).

This process retains elements in the inter-satellite traffic matrix \(}\) where the corresponding positions in the sampling matrix \(S\) are 1, while setting other positions to zero, resulting in the sampled data matrix \(\). To recover complete traffic data from the sampled data, we introduce a nonlinear estimation function \(F\). This function involves a complex nonlinear mapping to better estimate actual traffic data.

\[}=F()\] (2)

where \(\) represents sampled data, and \(}\) is the recovered data obtained through the non-linear estimation function \(F\).

### Satformer Overview

We design Satformer, a tensor completion model designed for the accurate and robust estimation of global traffic data in satellite networks. As illustrated in Fig. 1, Satformer is structured as an encoder-decoder architecture, with both components featuring multiple spatio-temporal modules. Residual connections interlink these modules to prevent neural network degradation. Each spatio-temporal module comprises a Graph Embedding Module and a Satformer Block. The key of Satformer to improve the estimation accuracy is that it can extract features efficiently and accurately from a large number of sparse satellite network traffic data. This is achieved through adaptive sparse spatio-temporal attention inside each Satformer block, facilitating the estimation of traffic data. A transfer module facilitates the seamless transmission of features from the encoder to the decoder. The encoder encodes the input traffic information, while the decoder is tasked with estimating the missing traffic data. The subsequent section provides a detailed description of each module.

### Spatio-Temporal Module

Satformer utilizes spatio-temporal modules to extract spatio-temporal features from input tensors; this module primarily consists of graph embedding components and Satformer blocks.

**Graph Embedding**: The Spatio-Temporal module serves the goal of extracting spatio-temporal features from input tensor. Considering the inherent high sparsity of observed traffic data in real-world, it becomes imperative to represent tensors as low-dimensional vectors. Through the learning of embedded representations for nodes, the model inherently captures both structural and semantic information of nodes within the graph. This capability enables the model to comprehend relationships between nodes more effectively, facilitating the extraction of meaningful features from the \( R^{I J T}\). Each Origin-Destination (OD) pair corresponds to an origin node, a destination node, and the traffic of the OD pair. To address the non-Euclidean nature of the data, particularly the spatio relationships within each OD pair, we employ Graph Embedding through Graph convolutional neural network (GCN), which has been widely used in many works [23; 24]. This approach allows the model to effectively handle non-Euclidean data, enhancing its capacity to capture and utilize the structural information present in the tensor \( R^{I J T}\).

Figure 1: **(a)** Overall framework of our Satformer. **(b)** Details of a Satformer block. **(c)** Satellite network traffic data generation. **(d)** Details of a graph embedding module. **(e)** Details of an ASSIT block.

In Satformer, each Spatio-Temporal module contains a GCN model. A GCN model contains two layers of convolutional layer, the feature propagation rule can be stated as follows:

\[H^{(l+1)}=(^{-}^{-}H^{(l)}W ^{(l)})\] (3)

\[=A+I,\,=_{j}_{ij}\] (4)

\[=f(,A)=(^{-}^ {-}(^{-}^{-}XW^{(0)})W^{(1)})\] (5)

where, \(H^{(l)}\) signifies the node embedding matrix for layer \(l\), \(A\) represents the adjacency matrix and \(I\) represents the self-connections matrix of \(A\). \( R^{I J}\) represents the adjacency matrix with self-connections. \( R^{I I}\) denotes the degree matrix, which is a diagonal matrix with each element on the diagonal representing the sum of the corresponding row in \(\). The weight matrix for layer \(l\) is denoted as \(W^{(l)} R^{I M}\), and \(H^{(l+1)} R^{I J M}\) represents the node embedding matrix for layer \(l+1\). \(W^{(0)} R^{K L}\) denotes the weight matrix from the input layer to the hidden layer, and \(W^{(1)} R^{K L}\) denotes the weight matrix from the hidden layer to the output layer. Both \(()\) and _ReLU_ are activation functions employed in the model. \(Z R^{I M K}\) represents the output embedding tensor.

**Satformer Block**: As shown in Fig. 1 (b), in each Satformer block, we use a layer normalization at the beginning to normalize the input embedding tensor. We then apply an ASSIT mechanism and a 2-layer MLP module for sparse spatio-temporal feature modeling and per-location embedding, respectively.

In the domain of communication network tensor completion, the spatio-temporal relationships among traffic data are complex, and it is necessary to model these relationships effectively. Traditional attention mechanisms, with their intensive nature, may encounter challenges related to high computational complexity and difficulties in capturing global relationships in such intricate scenarios. Several works proposed different sparse attention mechanism to mitigate such issue either relay on static patterns or skip computations in specific regions. As shown in Fig. 1(e), in this work, we explore an adaptive, sparse spatio-temproal mechanism. Detailed descriptions are as follows:

Given an input embedding feature tensor \( R^{I M K}\). First we divide the tensor slice into several local regions\(_{div}\), each of which has a size of \(D D\). In our module, \(\), \(\) and \(\) respectively represent query, key, and value, which are used to calculate attention weights and generate the final output. Then we calculate \(\), \(\) and \(\) tensor with linear projections for each region:

\[=_{div}W^{q}=_{div}W^{k}= _{div}W^{v}\] (6)

where \(W^{q}\), \(W^{k}\) and \(W^{v}\) are projection weights for query \(\), key \(\) and value \(\) respectively. We then consider introducing a local attention mechanism when calculating the attention score \(\) to make the model pay more attention to each local region in the input tensor. This improvement is designed to sharpen the model's attention specifically on local regions within the input sequence. The goal is to augment the expressiveness and robustness of the model by enabling it to capture and leverage more nuanced details and patterns present in localized segments of the input data. The implementation involves incorporating a position-related weight when calculating attention scores. The local attention in each region is operationalized through the use of a two-dimensional mask matrix \( R^{D D}\), wherein, elements inside a defined center window \(H\) are retained, while elements in other positions are set to zero. The size of the center window \(H\) is a hyperparameter of the model, and its optimal value is determined through experiments on different datasets. The calculation of attention score \(_{i}\) for each region can be denoted as follows:

\[_{i}=softmax(}{})\] (7)

\[}=softmax(W^{s}(1-W^{r}}) )\] (8)

where \(\) is an element-wise product and \(C\) is scaling factor. And the final output \(_{t}\) are computed as:

\[}=concat(})\] (9)

To regulate the sparsity of the attention scores and channel the model's focus onto specific portions of the input, an adaptive sparse regularization term is introduced. This involves applying L1 regularization to each element of the attention score matrix. The utilization of ReLU operations ensures that the attention scores remain non-negative. Thus the sparse mask can be denoted as: \((1-W^{r}})\). Finally, we apply the weighted Value to the attention score, while introducing additional learnable parameters to allow the model to adaptively learn the weighted sum of each position, resulting in the final output \(}\), as shown in Eq. 9. Where, \(W^{r}\) is the weighted matrix of L1 regularization, \(W^{s}\) is the scaling matrix, both \(W^{r}\) and \(W^{s}\) are trainable parameters.

### Transfer Module

The conventional information transfer between the encoder and decoder typically relies on the output of the last layer of encoder. However, this approach may fall short in adequately conveying global context information, particularly when dealing with input tensors spanning a large number of time slices. The accumulation of errors over time can become a challenge. Consequently, it is necessary to add a module between encoder and decoder to effectively transfer the information. Satformer incorporates a self-attention-based transfer module between the encoder and the decoder. This module leverages Self-Attention, enabling the seamless transfer of globally contextual information learned in the encoder to the decoder. This augmentation empowers the decoder to more comprehensively consider information from the entire input sequence when generating output for each time slice, thus enhancing the estimation accuracy of missing values. Moreover, the transfer module enables the model to integrate spatio-temporal information in a more fine-grained manner, improving its adaptability to patterns across different temporal and spatio scales. The mathematical description of the Transfer Module is as follows:

Suppose the encoder outputs an eigenvector \(e_{t}\) for each time step \(t\) input \(x_{t}\), where \(t=1,2,,T\), then the output sequence of the encoder is \(E=e_{1},e_{2},,e_{T}\). The goal of Transfer Module is to convert the output of the encoder \(E\) to a new set of feature vector \(D=d_{1},d_{2},,d_{T}\), where each \(d_{t}\) is a feature enhanced representation corresponding to time step \(t\). This process is achieved through the following self-attention mechanisms:

* Calculate Query, Key, and Value: The query vector \(Q=EW^{Q}\) represents the query of future time points against past time points. The key vector \(K=EW^{K}\) represents the encoding of a past point in time. The value vector \(V=EW^{V}\) represents the specific characteristics of past time points. \(W^{Q}\), \(W^{K}\), and \(W^{V}\) are learnable weight matrices.
* Calculate attention weight: Calculate the attention weight \(_{t,t-1}\) of each time step \(t\) and consider the effect that past attention scores exert on the present. \(C_{t}\) is scaling factor at time step \(t\), \(i\) belongs to \(1\) to \(t-1\), \(p\) and \(q\) are parameters to control the effort of past time attention scores. \[_{t,t-1}=softmax(p_{i=1}^{t-1}Q_{i}^{T}}{}}+qQ _{t}^{T}}{}})V_{t}\] (10)

Figure 2: **Left** Details of transfer module. **(a)** Attention weight. **(b)** Attention score.

* Generate transformation feature vector: Based on weights, attention to each time step \(t\) to generate a new feature vector \(d_{t}\). \[d_{t}=}}\] (11)

This process enables the Transfer Module to accurately measure the relationship between each future time point and all past time points, and to generate a new set of features that represent valuable information for future predictions.

### Loss Function

During the training stage, the primary objective is to minimize the discrepancy between the actual and predicted traffic data. To achieve this, the loss function employed by Satformer is the mean square error (MSE), as expressed in Eq. 12. Additionally, to curtail the growth of model weights and mitigate the risk of overfitting, a penalty term is incorporated into the loss function.

\[L()=}|}_{(i,j,t)}}{(_{ijt}-_{ijt})}+_{i}{( _{i})}\] (12)

where \(}\) denotes the set of observed traffic data, \(_{ijk}\) and \(_{ijk}\) are the truth and estimated traffic data respectively, \(\) represents all trainable parameters in Satformer, \(\) is weight decay coefficient.

## 4 Experiments

### Experimental Settings

**Datasets.** To assess the performance of Satformer, we employ it on three real-world satellite networks: Iridium, Telesat, and Starlink, thereby evaluating its capabilities across varying network scales: small-scale, medium-scale, and large-scale environments. Given the ongoing construction and utilization of many satellite networks, acquiring actual traffic data proves to be challenging. Thus, we generate corresponding traffic datasets using real satellite parameters and ground station coordinates. Similar methods have been used in many previous studies, and the specific details of this process are explained in the Appendix A. The traffic data collection interval was 1 second for all three datasets.

* **Iridium **: The Iridium constellation comprising a total of 66 satellites uniformly distributed across 6 orbital planes. For our experimentation, we focus on the initial six periods, encompassing 36,000 time slices.
* **Telesat **: It collects traffic data from the Telesat constellation which has a total of 298 satellites distributed in 26 orbital planes. We select the first five periods about 31500 time slots in our experiment.
* **Starlink **: The traffic data recording originates from the Starlink constellation, comprising 1584 Low Earth Orbit (LEO) satellites evenly dispersed across 72 orbital planes. The first six periods about 32400 time intervals in our experiment.

For all three datasets, we divided the original dataset into a training set and a test set in an 8:2 ratio using the time slice partitioning method. We then used the training set for model training and the validation set for model validation and tuning. Subsequently, we constructed the test set by randomly masking portions of the training and validation sets that were not used for training. This approach ensures that the model is trained and validated on distinct segments of the data, which can help prevent overfitting and improve the model's ability to generalize to new, unseen data.

**Baselines.** For comparative analysis against our Satformer model, we select the following baseline models: three mathematical tensor completion models, namely HaLRTC, LATC and LETC, and four state-of-the-art neural network-based tensor completion models, CoSTCo, DAIN, SPIN and STCAGCN.

* **HaLRTC **: A prototypical high-accuracy low-rank tensor completion algorithm utilizes the Alternating Direction Method of Multipliers (ADMMs) to attain precise outcomes, effectively managing dependencies among various constraints.

* **LATC **: It introduces a novel regularization term, integrating temporal variation, into a third-order tensor completion model.
* **LETC **: a Laplacian enhanced low-rank tensor completion framework for large-scale traffic speed kriging.
* **CoSTCo **: An innovative Convolutional Neural Network (CNN)-based model developed for tensor completion to overcome the limitations associated with traditional low-rank tensor factorization approaches.
* **CDSA **: A novel cross-dimensional self-attention approach for imputing missing values in multivariate, geo-tagged time series data.
* **DAIN **: This method explicitly crafted to enhance the accuracy of neural tensor completion methods when predicting missing values within sparse, multi-dimensional datasets.
* **SPIN **: An attention-based architecture using spatiotemporal graphs and autoregressive models for effectively reconstructing missing data in sparse, multivariate time series.
* **SAITS **: a self-attention-based method for multivariate time series imputation that uses joint-optimization and diagonally-masked self-attention blocks.
* **STCAGCN **: A graph-based deep learning method for traffic volume estimation by utilizing a graph attention-based speed pattern-adaptive adjacency matrix and a customized temporal attention mechanism.

**Evaluation Metrics.** Two widely employed metrics are applied to evaluate the estimation performance of Satformer. The calculation equations for these metrics are presented as follows:

* **Normalized Mean Absolute Error (NMAE)**: \[NMAE=}}|_{ijt}-_{ ijt}|}{_{(i,j,t)}}|_{ijt}|}\] (13)
* **Normalized Root Mean Squared Error (NRMSE)**: \[NRMSE=}}|_{ijt}-_{ijt}|^{2}}{_{(i,j,t)}}_{ijt}^{2}}}\] (14)

where \(_{ijk}\) and \(_{ijk}\) represent the truth value and estimated value, \(}\) denotes the set of unobserved traffic data. **For both two metrics, the smaller they get to 0, the better the estimation performance of the model.**

### Performance Comparison with Baselines

**Compare Satformer with mathematical baselines.** Table 1 provides a summary of the experimental results for our Satformer and the mathematical tensor completion baselines, HaLRTC, LATC and LETC. Performance evaluations, measured by NMAE and NRMSE, are conducted across three datasets with sampling ratios ranging from 2% to 10%. Our Satformer consistently outperforms the mathematical tensor completion algorithms, achieving significant improvements. Notably, even at the minimal 2% sampling ratio, Satformer maintains proficient performance, with NMAE values recorded as 0.098, 0.1017, and 0.1402 for the Iridium, Telesat, and Starlink datasets, respectively. In comparison, the leading mathematical models exhibit higher NMAE values of 0.2782, 0.2723, and 0.3784 under the same 2% sampling ratio. The observed performance enhancement in Satformer quantifies at 84.38%, 86.43%, and 106.77% for the respective datasets. Similar trends are also observed in NRMSE. These results indicate that mathematical models based on Alternating Direction Method of Multipliers or reliant on strong assumptions struggle to capture the complex spatio-temporal characteristics. In contrast, neural network-based models such as Satformer demonstrate formidable nonlinear representation capabilities, enabling effective extraction of spatio-temporal features from traffic data.

**Compare Satformer with neural network-based baselines.** Our Satformer outperforms the neural network-based baselines (CoSTCo, DAIN, SPIN, and STCAGCN) across all datasets, achieving the best estimation performance, as shown in Table 1. Notably, even with a 2% traffic data sampling rate, Satformer demonstrates significant improvements compared to the best-performing neural network-based baselines. On the Iridium dataset (66 satellites), Satformer improves NMAE and NRMSE by 8.57% and 8.95%, respectively. As the size of the dataset increases, performance improvements continue and escalate. On the Telesat dataset (298 satellites), Satformer achieves improvements of 27.63% in NMAE and 18.05% in NRMSE. For the Starlink dataset (1584 satellites), Satformer exhibits even more substantial improvements, with NMAE and NRMSE increasing by 38.66% and 32.32%, respectively. These results highlight Satformer's effectiveness in handling large-scale datasets, suggesting potential deployment in real-world satellite networks. The limitations

    &  &  \\   & 2\% & 4\% & 6\% & 8\% & 10\% & 2\% & 4\% & 6\% & 8\% & 10\% \\  HaLRTC & 0.2782 & 0.2252 & 0.2044 & 0.1935 & 0.1886 & 0.3926 & 0.3381 & 0.3074 & 0.2888 & 0.2778 \\ LATC & 0.581 & 0.5809 & 0.5809 & 0.5809 & 0.5808 & 0.6009 & 0.5998 & 0.5997 & 0.5997 & 0.5996 \\ LETC & 0.1807 & 0.1672 & 0.1545 & 0.1439 & 0.1354 & 0.2591 & 0.2384 & 0.2203 & 0.1984 & 0.1861 \\  Improve\% & 84.38\% & 71.83\% & 61.61\% & 60.06\% & 66.54\% & 116.82\% & 100.84\%90.24\%79.71\%84.44\% \\  CoSTCo & 0.1629 & 0.1623 & 0.16 & 0.1588 & 0.1435 & 0.5664 & 0.5644 & 0.5646 & 0.5621 & 0.5574 \\ CDSA & 0.1616 & 0.1601 & 0.1599 & 0.1598 & 0.1120 & 0.6632 & 0.6058 & 0.5219 & 0.5249 & 0.5103 \\ DAIN & 0.1159 & 0.1156 & 0.1150 & 0.1144 & 0.1126 & 0.1435 & 0.142 & 0.1391 & 0.1377 & 0.127 \\ SPIN & 0.1206 & 0.1185 & 0.1175 & 0.1170 & 0.1158 & 0.1302 & 0.1310 & 0.1291 & 0.1229 & 0.1181 \\ SAITS & 0.1106 & 0.1078 & 0.1075 & 0.1073 & 0.1051 & 0.1203 & 0.1201 & 0.1201 & 0.1174 & 0.1161 \\ STCAGCN & 0.1064 & 0.1059 & 0.1058 & 0.1049 & 0.1046 & 0.1847 & 0.1622 & 0.1523 & 0.1435 & 0.1203 \\
**Satformer** & **0.098** & **0.0973** & **0.0956** & **0.0899** & **0.0813** & **0.1195** & **0.1187** & **0.1158** & **0.1104** & **0.1009** \\  Improve\% & 8.57\% & 8.84\% & 10.67\% & 16.69\% & 28.67\% & 8.95\% & 10.36\% & 11.49\%11.32\%17.05\% \\   &  &  \\   & 2\% & 4\% & 6\% & 8\% & 10\% & 2\% & 4\% & 6\% & 8\% & 10\% \\  HaLRTC & 0.2723 & 0.2723 & 0.259 & 0.2538 & 0.2267 & 0.5518 & 0.4402 & 0.421 & 0.3968 & 0.3632 \\ LATC & 0.6193 & 0.6181 & 0.6129 & 0.6031 & 0.6002 & 0.6367 & 0.6367 & 0.6367 & 0.6367 & 0.6368 \\ LETC & 0.1896 & 0.1794 & 0.1637 & 0.1583 & 0.1513 & 0.2946 & 0.2751 & 0.261 & 0.2635 & 0.2534 \\  Improve\% & 86.43\% & 79.4\% & 68.58\% & 60.71\% & 66.99\% & 58.27\% & 50.49\% & 47.62\%50.92\%52.46\% \\  CoSTCo & 0.2256 & 0.2182 & 0.2013 & 0.1898 & 0.1864 & 0.6996 & 0.6716 & 0.6482 & 0.6033 & 0.5852 \\ CDSA & 0.2354 & 0.2218 & 0.1565 & 0.1916 & 0.1815 & 0.6712 & 0.6523 & 0.5449 & 0.5014 & 0.4987 \\ DAIN & 0.1387 & 0.1345 & 0.1328 & 0.1297 & 0.1211 & 0.2687 & 0.2679 & 0.2538 & 0.2499 & 0.2476 \\ SPIN & 0.1298 & 0.1286 & 0.1278 & 0.1274 & 0.1273 & 0.2378 & 0.2365 & 0.2353 & 0.2347 & 0.2344 \\ SAITS & 0.1267 & 0.1223 & 0.1218 & 0.1113 & 0.1112 & 0.2213 & 0.2207 & 0.2201 & 0.2109 & 0.2013 \\ STCAGCN & 0.1488 & 0.1474 & 0.1457 & 0.1412 & 0.1393 & 0.2198 & 0.2184 & 0.2173 & 0.2184 & 0.2170 \\
**Satformer** & **0.1017** & **0.1** & **0.0971** & **0.0985** & **0.0906** & **0.1862** & **0.1828** & **0.1768** & **0.1746** & **0.1662** \\  Improve\% & 27.63\% & 28.6\% & 31.62\% & 29.34\% & 33.66\% & 18.05\% & 19.47\% & 22.91\%25.09\%30.57\% \\   &  &  \\   & 2\% & 4\% & 6\% & 8\% & 10\% & 2\% & 4\% & 6\% & 8\% & 10\% \\  HaLRTC & 0.3784 & 0.3392 & 0.3116 & 0.282 & 0.2558 & 0.6148 & 0.4796 & 0.4398 & 0.4116 & 0.3778 \\ LATC & 0.5738 & 0.5733 & 0.5737 & 0.5437 & 0.5348 & 0.5984 & 0.5982 & 0.5937 & 0.5938 & 0.5928 \\ LETC & 0.2899 & 0.2803 & 0.272 & 0.2656 & 0.2546 & 0.4937 & 0.484 & 0.4722 & 0.4624 & 0.4571 \\  Improve\% & 106.77\% & 103.71\% & 101.63\% & 101.82\% & 108.17\% & 79.26\% & 77.81\% & 77.78\%4.23\%75.33\% \\  CoSTCo & 0.2553 & 0.2479 & 0.2466 & 0.2462 & 0.2428 & 0.6635 & 0.6548 & 0.6531 & 0.6519 & 0.6498 \\ CDSA & 0.2567 & 0.2119 & 0.2198 & 0.1964 & 0.2047 & 0.7732 & 0.6034 & 0.6032 & 0.5987 & 0.5975 \\ DAIN & 0.237 & 0.2231 & 0.2346 & 0.2233 & 0.2172 & 0.431 & 0.4036 & 0.4114 & 0.4189 & 0.4186 \\ SPIN & 0.

of CoSTCo are evident due to its exclusive reliance on two-dimensional convolution for spatial feature extraction without explicitly modeling temporal features. DAIN falls short by not explicitly modeling interactions between entities, which limits information utilization, despite its combination of information for data augmentation. SPIN's ability to handle sparsity or irregularly sampled data might be limited, which could affect the accuracy of traffic estimation in satellite networks where data is often incomplete. STCAGCN captures time-asynchronous correlations may not fully account for the complex temporal dynamics in satellite network, leading to less accurate estimations. The architecture of STCAGCN cannot ensure the information learned at earlier stages is preserved and utilized in later stages. Although CDSA also utilizes the self-attention mechanism, its dimension-wise processing may limit its ability to capture complex interactions. RNN-based models of SAITS are generally inferior to Transformer architectures in terms of handling long-distance dependencies and efficiency. In contrast, Satformer excels by explicitly incorporating both spatial and temporal features within each module. The graph embedding captures nonlinear information, the Satformer module integrates the ASSIT, and the transfer module seamlessly transmits global contextual information. This comprehensive design enables Satformer to deliver exceptional performance in inter-satellite traffic data estimation, effectively addressing the challenges of large-scale, sparsely populated datasets.

## 5 Conclusion and Discussion

This paper proposes Satformer, a novel traffic data estimation algorithm for large-scale satellite networks, aiming at fast and accurate estimating global traffic matrix from partial sampling in a cost-effective manner. Motivated by this, we design a region-aware sparse spatio-temporal attention mechanism to concentrate on specific local regions of the input tensor, where the input tensor is embedded in a graph convolutional neural network. Thus, spatio-temporal features from the traffic matrix are effectively extracted with computational efficiency and robustness.

Extensive experiments with datasets of varying scales-small, medium, and large have shown that Satformer has significant advantages on both accuracy and efficiency for traffic estimation compared with baselines, particularly in larger networks. Moreover, we analyze the robustness of Satformer under different conditions and further verify the role of each module through ablation studies. The results demonstrate the potential of Satformer for deployment in actual systems.

Despite Satformer is effective adopted for traffic estimation, deep learning models for traffic estimation remain mostly black boxes. It is quite important to understand the reasons behind inferences in the satellite networking domain. In addition, although Satformer is cost-effective, it is necessary to further reduce its computational complexity, considering the limited computational resources of existing satellites.

Future works should prioritize enhancing computational efficiency. It is also important to explore interpretability and decision basis of our deep learning model for traffic estimation. For example, explanation techniques, such as a local interpretable model-agnostic explanation (LIME) , are able to make a visual analysis of the model and analyze the internal working mechanism from specific examples. Additional explanatory tools, such as feature importance analysis, will help users in understanding the model's workings.