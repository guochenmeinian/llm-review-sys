# Tactile DreamFusion:

Exploiting Tactile Sensing for 3D Generation

 Ruihan Gao\({}^{1}\)  Kangle Deng\({}^{1}\)  Gengshan Yang\({}^{1}\)  Wenzhen Yuan\({}^{2}\)  Jun-Yan Zhu\({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)University of Illinois Urbana-Champaign

https://ruihangao.github.io/TactileDreamFusion/

###### Abstract

3D generation methods have shown visually compelling results powered by diffusion image priors. However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps. To address this, we introduce a new method that incorporates touch as an additional modality to improve the geometric details of generated 3D assets. We design a lightweight 3D texture field to synthesize visual and tactile textures, guided by 2D diffusion model priors on both visual and tactile domains. We condition the visual texture generation on high-resolution tactile normals and guide the patch-based tactile texture refinement with a customized TextureDreambooth. We further present a multi-part generation pipeline that enables us to synthesize different textures across various regions. To our knowledge, we are the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. We evaluate our method in both text-to-3D and image-to-3D settings. Our experiments demonstrate that our method provides customized and realistic fine geometric textures while maintaining accurate alignment between two modalities of vision and touch.

## 1 Introduction

Generating high-fidelity 3D assets is crucial for a wide range of applications, from content creation in gaming and VR/AR to developing realistic simulations for robotics. Recently, a surge in emerging methods has enabled the creation of 3D assets from a single image  or a short text prompt . Their success can be attributed to advances in generative models  and neural rendering , as well as the availability of extensive 2D and 3D datasets .

Although existing methods can effectively capture the overall shape and visual appearance of objects, they often struggle with synthesizing fine-grained geometric details, such as stochastic patterns or bumps from the material, as shown on the left side of Figure 1. As a result, the final mesh output tends to be either overly smooth (e.g., the avocado in the top row) or has geometric details incorrectly baked into the albedo map (e.g., the beanie in the bottom row). But why is that?

We argue that there are two major bottlenecks. First, high-resolution geometric data are largely absent in current 2D and 3D datasets. For image datasets like LAION , obtaining geometric details is challenging due to the limited camera resolution in capturing real-world objects. In the case of 3D asset datasets like Objaverse , although the assets often include visual textures, they rarely feature high-resolution geometric textures. Second, it is difficult for humans to precisely describe fine geometric textures in natural language, making text-based applications even more challenging.

To address these issues, we propose exploring tactile sensing to capture high-resolution texture data for 3D generation. Given a text prompt or an input image, we first generate a base mesh with an albedo map as our 3D representation. We then capture detailed surface geometry of the target textureusing GelSight [10; 11], a high-resolution tactile sensor. We then convert these tactile data into normal maps and train a TextureDreambooth on these normal maps. To refine the albedo map and ensure alignment between visual and tactile modalities, we learn a lightweight 3D texture field that co-optimizes the albedo and normal maps using 2D diffusion guidance across both domains.

Furthermore, we extend our approach to synthesize textures across multiple object regions by aggregating 2D diffusion-based part segmentation in a 3D label field.

Our experiments demonstrate that our method outperforms existing text-to-3D and image-to-3D methods in terms of qualitative comparison and user study. Our method ensures coherent high-resolution textures with precise alignment between appearance and geometry, as shown on the right side of Figure 1. Our code and datasets are available on our project website.

## 2 Related Work

**3D generation.** Following the success of text-to-image models [14; 15; 16; 17], we have witnessed a booming development of 3D generative models conditioned on text or images. Recent works have used 2D diffusion priors in 3D generation [2; 18; 3; 19; 20; 21; 22; 13; 23], with notable methods like DreamFusion  introducing Score Distillation Sampling (SDS) to optimize a 3D representation using gradients from 2D diffusion models. Follow-up works have further extended SDS optimization using multi-view diffusion models, improving both 3D generation and single-view reconstruction [1; 24; 25; 26; 27; 28; 13; 29; 30].

Another line of research [31; 32; 33; 34] trains large-scale transformers to generate 3D shapes in a feed-forward manner, requiring high-quality large-scale 3D asset datasets. While these models effectively capture global shapes, they often fail to accurately render fine-grained geometric details, which are either incorrectly baked in color or lost entirely.

**Geometry representation in 3D generation.** Researchers have explored various 3D representations for generation, such as voxel grids [35; 36; 37], point clouds [38; 39; 40], meshes [3; 41; 23; 42; 43; 44; 45; 19], neural radiance fields (NeRF) [46; 47; 2; 20; 21], neural surfaces [48; 22; 13; 25], and Gaussians [49; 50]. Among these, meshes support faster and more efficient rasterization rendering than volumetric rendering. Meshes also integrate seamlessly with graphics engines for downstream

Figure 1: Our method leverages tactile sensing to improve existing 3D generation pipelines. _Left_: Given a text prompt, we first generate an image using SDXL  and then run Wonder3D  to generate mesh from the image. This process often results in a mesh with an overly smooth surface. _Right_: Our method takes a text prompt and several tactile patches and generates high-fidelity coherent _visual_ and _tactile_ textures that can be transferred to different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, with the reference imageâ€™s thumbnail displayed at the bottom right corner. Please visit our webpage for video results.

applications. In contrast, volume representations require separate post-processing to extract meshes, often resulting in a loss of quality. Our approach uses meshes to represent coarse geometry while embedding local geometric details into a 3D texture field of tactile normal.

3D texture generation and transfer.Simultaneously generating geometry and texture often results in blurry textures due to surface smoothing or averaging across views. To address this, many approaches propose a subsequent stage that focuses on refining high-resolution textures [22; 13] or treats texture generation as a separate problem entirely [51; 52; 53; 54; 55; 56]. Additionally, various texture transfer methods enable the transfer of texture to new shapes using images [57; 58; 59; 60] or other material assets . While most methods focus solely on visual appearance, our approach produces high-fidelity geometric details. Closely related to our method, NeRF-Texture  also transfers textures with geometric variations but requires 100-200 images of the same object. In addition, their method models the mesostructure with a signed distance and a coarse normal direction at each projected vertex. Hence, it can model sparse structures at a centimeter scale but not delicate patterns on the object's surface. In contrast, our method leverages high-resolution tactile sensing at the millimeter scale and provides accurate surface normal details, with richer details at fine scales.

Tactile sensing in 3D.Tactile data is useful in providing contact information and are widely used in 3D perception and robotics tasks. Early works start with reconstructing simple objects [63; 64; 65; 66], and then evolve to more complicated scenes with latest 3D neural representations [67; 68], as well as robotic in-hand reconstruction with multi-finger contacts [69; 70]. Vision-based tactile sensing [10; 71; 11], a particular sensing mechanism based on the photometric stereo principle, can provide high-resolution surface texture information like surface normals and thus can be useful for high-quality 3D synthesis and generation. Several recent works have also used it for 2D generation [72; 73; 74] and 3D scene reconstruction [75; 67; 76]. In contrast, to our knowledge, our work is the first to use tactile sensing for 3D generation.

## 3 Tactile Data Acquisition

We use GelSight [10; 11] to acquire high-resolution geometric details by touching the object's surface. The raw sensor data are then pre-processed to obtain high-frequency signals, as shown in Figure 3.

GelSight tactile sensor.The GelSight sensor is a vision-based tactile sensor that uses photometric stereo to measure geometry at a high spatial resolution at the contact surface. It can capture the fine details on the surface, such as bumps on avocados' skin and patterns of crochet yarns. In our work, we use the GelSight mini with a sensing area of 21mm \(\) 25mm (\(h w\)) and a pixel resolution of 240\(\)320, equivalent to about 85 micrometers per pixel. The sensor captures an RGB image, which can be mapped to the surface normals and then used to reconstruct a surface height map.

Tactile data pre-processing and contact mask.We manually press the GelSight sensor against the object's surface to obtain a tactile image over a small area. The derived height map contains both a low-frequency component, representing the surface's global shape, and high-frequency geometric textures. We first apply a high-pass filter to the height map to extract texture information and then center-crop the masked region to a square patch to remove any non-contact area. Finally, we convert the masked height map patch back to a normal map patch by computing its gradients.

Figure 2 shows our dataset TouchTexture collected from 18 daily objects with diverse tactile textures. Our dataset is available on our project website.

Figure 2: TouchTexture dataset. We collect tactile normal data from 18 daily objects featuring diverse tactile textures. To demonstrate the local geometric intricacies, we show the tactile normal map and a 3D height map for each object. Please refer to the supplement for the full set of our data.

## 4 Method

Generating 3D assets with high-resolution geometric details is challenging due to the difficulty of acquiring low-level texture details from text prompts or a single image. Therefore, we incorporate an additional input modality, tactile normal maps, to enhance the geometric details in 3D asset generation. Figure 4 shows our overall pipeline. Our method takes an input image and a tactile normal patch as input and generates a 3D asset with a high-fidelity, color-aligned normal map. The input image can be a real image or generated by a text-to-image model such as SDXL . Below, we first describe how we generate the base mesh in Section 4.1. We then introduce our core texture refinement algorithm in Section 4.2 and extend it to objects with multiple materials in Section 4.3.

### Base Mesh Generation

Similar to prior works with stage-wise geometry sculpting and texture refinement [22; 49], we first generate a base mesh with albedo texture using a text- or image-to-3D method. We then unwrap a UV map of the exported mesh \(M\) and project the vertex albedo onto an albedo UV map.

A 2D texture transfer baseline.To incorporate the tactile information, one could synthesize a large 2D texture map with an exemplar of the preprocessed normal patch using 2D texture synthesis algorithms such as image quilting  and use it as a normal UV map of the object mesh. However,

Figure 4: **Method overview. Given an input image or a text prompt, our method generates a mesh with high-quality visual and normal texture. We first generate a base mesh with albedo texture using a text- or image-to-3D method. We use a 3D texture field with hash encoding to represent albedo and tactile normal textures and train it with loss functions on rendered images. To capture the scale differences between visual and tactile modalities, we sample distinct camera views, \(P_{}\) for visual rendering and \(P_{}\) for tactile rendering. For texture refinement, we train the texture field with a visual matching loss \(_{}\), to ensure fidelity to the input mesh, and a visual guidance loss with normal-conditioned ControlNet, \(_{}\), to enhance photorealism and cross-modal alignment. We further apply a tactile matching loss, \(_{}\), and a tactile guidance loss, \(_{}\), using a customized Texture Dreambooth, to achieve high-quality geometric details aligned with the distribution of tactile input \(V*\) texture exemplars.**

Figure 3: **Tactile data capture. We collect one patch by pressing GelSight Mini on an object surface. We use Poisson integration to estimate the contact depth from the sensor output, apply high-pass filtering to extract the high-frequency texture information, and then run the 2D texture synthesis method of Image Quilting  to obtain an initial texture map. Finally, we convert the height map back to a normal map.**this would result in inconsistencies between visual and tactile textures, especially when transition in visual color indicates geometric change, as shown in Figure 9.

### Texture Refinement

To address this issue, we jointly optimize the albedo and the normal tactile texture to ensure alignment.

**Texture representation.** For ease of optimization, we represent the visual and tactile normal textures as a 3D texture field. In practice, we use a multi-resolution hash grid \(()\), whose input is a 3D spatial coordinate \(p\) on the mesh:

\[(p)=(,_{}),\] (1)

where \(^{3}\) is the albedo and \(_{}^{3}\) is the tactile normal defined in the tangent space on the mesh surface.

Given the base mesh \(M\), the texture field \(()\), a camera pose \(P\), and a lighting condition \(L\), we can render a color image \(}_{}^{H W 3}\) of the object using a differentiable rasterizer \(\), rvdiffrast :

\[}_{}(P)=(M,(),P,L),\] (2)

where \(H\) and \(W\) represent image height and width, respectively.

Specifically, for each 3D point on the mesh, we composite the base normal \(_{}\) from the mesh geometry and the tactile normal \(_{}\) from the texture to get the shading normal \(\):

\[=_{}_{}=[,_{},_{}] _{},\] (3)

where \(_{}\) is the global geometric normal defined by the base mesh, \(\) is the tangent vector, and \(_{}\) is the Tangent-Bitangent-Normal (TBN) Matrix for every surface point. Given the calculated shading normal and a sampled camera pose \(P\), we use a point light and a simple diffuse shading model to produce the color image \(}_{}(P)\) following prior works [2; 3]. We can also obtain \(}_{}(P)\), \(}_{}(P)\), and \(}_{}(P)\) by projecting the albedo, tactile normal, and shading normal onto a sample view \(P\).

**Learning 3D texture field.** Given the scale discrepancy between vision and touch, we sample camera views differently for two modalities. To supervise visual renderings \(}_{}\) and \(}_{}\), we sample camera poses \(P_{}\) orbiting the base mesh looking at the object center with perspective projection. To supervise tactile renderings \(}_{}\), we sample camera poses \(P_{}\) close to mesh surfaces based on vertex normals to emulate the captured data using a real sensor. Specifically, we randomly sample a vertex, place a camera along this vertex's normal direction, and set the camera to look at the sampled vertex with orthographic projection.

We initialize the neural texture field of albedo with a reconstruction loss of rendered images:

\[_{}=}_{}(P_{ })-_{}(P_{})_{2}^{2},\] (4)

where \(_{}\) is rendered using the exported albedo UV map in Section 4.1 from the same camera \(P_{}\). Similarly, we initialize the tactile normal texture field with a **T**actile **M**atching (TM) loss:

\[_{}=1-(}_{}(P_{ }),_{}(P_{})),\] (5)

where \(_{}\) is rendered using the tactile normal UV map from the image quilting results from a tactile camera pose \(P_{}\). Here, we use image quilting , a patch-based texture synthesis algorithm, to synthesize a reference texture map \(_{}\) that roughly matches the physical scale of real materials and use it for initialization.

To refine the visual texture, we use a diffusion guidance loss inspired by the multi-step denoising process in SDEdit , Instruct-NeRF2NeRF , and DreamGaussian . Different from existing works, we compute the Visual Guidance loss \(_{}\) based on a normal-conditioned ControlNet  to ensure the refined visual texture is consistent with the tactile normal. Given the rendered color image \(}_{}\) from \(P_{}\), we perturb it with random noise \((t)\) to get a noisy image \(x_{t}\) and apply a multi-step denoising process \(f_{}()\) using the 2D diffusion prior to obtaining a refined image:

\[_{}(P_{})=f_{}(x_{t};t,y,}_{}(P_{})),\] (6)where \(y\) is the input text prompt, and \(f_{}\) is a normal-conditioned ControlNet with the Stable Diffusion backbone. We denoise the image from timestep \(t\) to a completely clean image \(_{}\), which is different from the typical single-step SDS loss in Dreamfusion . The starting timestep \(t(0,1)\) gradually decreases from \(0.5\) to \(0.3\) as training iteration increases, balancing the noise reduction to enhance details without disrupting the original content. This refined image is then used to optimize the texture through the L1 and LPIPS  loss:

\[_{}=||}_{}(P_{})- _{}(P_{})||_{1}+_{} (}_{}(P_{}),_{}(P_{} )).\] (7)

While refining the visual textures, we jointly optimize the tactile texture with a Tactile **G**uidance loss \(_{}\). Similar to \(_{}\), we add random noise to a rendered tactile normal map \(_{}(P_{})\), generated from a tactile camera pose \(P_{}\), and then apply a multi-step denoising process to obtain a refined normal image \(_{}\). To capture the distribution of tactile normals, we replace the standard diffusion prior with a _Texture DreamBooth_, a customized model created by fine-tuning the Stable Diffusion model \(f_{}()\) for each tactile texture using a Low-Rank Adapter (LoRA)  with DreamBooth . We leave the LoRAs training details in Appendix A.3. We use the output \(_{}\) to refine the tactile texture:

\[_{}=1-(}_{}(P_{}),_{}(P_{})).\] (8)

**Optimization.** The overall loss function is:

\[=_{}_{}+_{} _{}+_{}_{}+_{ }_{}.\] (9)

To initiate our texture grid, we start by only optimizing the visual matching loss \(_{}\) and tactile matching loss \(_{}\) for 150 iterations with \(_{}=500\) and \(_{}=1\). After that, we run optimization for another 50 iterations to refine the output guided by diffusion priors. We reduce \(_{}\) from 1 to 0.05, change \(_{}\) from per-pixel error to mean-color error to allow more flexibility in texture refinement, and add the visual guidance loss \(_{}\) and tactile guidance loss \(_{}\) with \(_{}=5\) and \(_{}=0.05\).

### Multi-Part Textures

Another advantage of our 3D texture field is its ability to easily define non-uniform textures in 3D, allowing different tactile textures to be assigned to distinct parts of an object. For instance, when generating a 3D asset of "a cactus in a pot", we can apply different textures to the cactus and the pot, by incorporating two tactile inputs along with a text prompt that specifies the texture assignment, e.g., "cactus with texture A, pot with texture B".

**Part segmentation based on diffusion features.** Our method can automatically segment object parts based on the text prompt without manual annotation. We leverage the internal attention maps of diffusion models to segment the rendered views \(_{}(P_{})\) of the object. Specifically, we add a random noise of level \(t\) to \(_{}\) and apply one denoising step with the SD UNet. Inspired by DiffSeg  and other text-to-image methods [87; 88], we perform unsupervised part segmentation by aggregating and spatially clustering self-attention maps from multiple layers. To assign part labels, we aggregate cross-attention maps corresponding to different parts and match them with the unlabeled segmentation maps based on Kullback-Leibler (KL) divergence. This results in a list of labeled segmentation masks \(^{n}(_{})\{0,1\}^{H W}\), \(n\{1,,N\}\), where \(n\) denotes one of the \(N\) object parts. For example, given the prompt "cactus in a pot," we aggregate cross-attention maps for "cactus" and "pot" from different layers, then assign each part segmented from the clustered self-attention maps to either cactus or pot according to respective cross-attention maps. More details are included in Appendix A.1.

**3D part label field.** Since the 2D part segmentations may contain noise and lack multi-view consistency, we integrate them from different viewpoints into a coherent 3D part label field. We achieve this by incorporating a part label \(s^{N}\) into our texture field \(()\). To supervise it, we render part label maps \((P_{})\) from sampled camera poses \(P_{}\) and compute the cross entropy loss:

\[_{s}=_{}((P_{}),S( _{}(P_{}))).\] (10)

where \((P_{})\) is the part logits and \(S(_{}(P_{}))\{0,1\}^{H W N}\) are labels concatenated from \(^{n}\).

**Optimization with multi-part textures.** To learn multi-part textures, we keep the same \(_{}\) and \(_{}\) loss functions while modifying \(_{}\) and \(_{}\) to incorporate part-specific tactile supervision:

\[_{}=_{n=1}^{N}[1-(}^{n} }_{},}^{n}_{}^{n})],\] (11)

\[_{}=_{n=1}^{N}[1-(}^{n} }_{},}^{n}_{}^{ n})],\] (12)

where \(\) denotes the Hadamard product, \(_{}^{n}\) is the rendered reference view using the \(n\)-th part's tactile data, \(_{}^{n}\) is the refined tactile normal generated by Texture Dreambooth trained on the \(n\)-th part texture, and \(}^{n}\) is the binary mask for the \(n\)-th part obtained from \(\) rendered from our learned 3D label field. We omit \(P_{}\) for clarity since all patches are rendered from the same tactile camera pose.

## 5 Experiment

We present comprehensive experiments to verify the efficacy of our method. We perform qualitative and quantitative comparisons with existing baselines and ablation studies on the major components.

**Dataset.** We have collected 18 diverse types of textures from daily objects in our TouchTexture dataset, five tactile patches per texture, and pair them with a short description. Please see Appendix A.2 for the full list. We randomly sample one patch to initialize the tactile UV map with the image quilting algorithm  and use five patches to train the customized Texture DreamBooth. For base mesh generation given a text or an image input, while any mesh generation method is applicable,

Figure 5: **3D generation with a single texture.** For each object, we show generated albedo (top), normal (middle), and full color (bottom) renderings from two viewpoints. Our method works for both text-to-3D (corn and football) and image-to-3D (potato and strawberry), generating realistic and coherent visual textures and geometric details. (We use roughness=0.5 when rendering color views in Blender for Figures 1, 5, 6, and 7.)

Figure 6: **Diverse textures with the same object.** With additional texture cues from tactile data, we can synthesize diverse textures with the same coarse shape for customized designs.

we use Wonder3D  in the main experiments but include results using InstantMesh  and RichDreamer  in Appendix A.4 as well. Specifically, Wonder3D takes an image as input and outputs a mesh with albedo stored as vertex colors. We then convert the vertex albedo into an albedo UV map as described in Section 4.1. During optimization in Section 4.2, we refine the albedo and tactile textures with the "textured prompts", i.e., _A [object name] with V* texture_, where _V*_ represents the text description corresponding to the selected tactile texture map.

**3D generation with a single texture.** Figure 5 shows our 3D generation results for a single texture, showing albedo, normal, and full-color rendering from two viewpoints for each object. Our method works for both text input (the corn and the football) and image input (the potato and the strawberry), generating realistic, coherent, and high-resolution visual and geometry details. Figure 6 shows the results of applying different textures to the same object (a coffee cup), with normal rendering on top and color rendering below. Our joint optimization produces coherent visual-tactile textures and smooth, natural shadows from geometric variations.

**Multi-part texture generation.** As introduced in Section 4.3, users can specify an object (via text or image) and assign different textures to two distinct parts. Figure 7 shows results for multi-part texture synthesis. The left column shows the text or image input as well as the tactile input. The image input can be either real or generated from a text prompt. The color-coded text prompts correspond to text descriptions for two textures. The right columns show the results of the albedo, normal, and full-color rendering. Our method effectively segments parts with our 3D label field. The joint optimization successfully applies the textures to each corresponding part designated by the user and generates an overall coherent visual appearance and tactile geometry.

**Baselines.** To our knowledge, this work is the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. Thus, we compare our method with existing text-to-3D and image-to-3D methods. For text-to-3D, we compare with DreamCraft3D  and use the same textured prompt, i.e., a prompt with a text description of the target tactile texture, as input. DreamCraft3D focuses on geometry sculpting using neural field and DMTet , followed by texture boosting through fine-tuning DreamBooth with augmented multi-view rendering, requiring 10x computational cost compared to our method. For image-to-3D, we compare with DreamGaussian  and Wonder3D . DreamGaussian employs fast generation using 3D Gaussian primitives and refines an albedo map in Stage 2 with a multi-step denoising MSE loss. Wonder3D generates paired multi-view RGB and normal images, using a geometry fusion process to generate a 3D result. The input images remain the same as ours for these two baselines. We use the official implementations for all baselines.

Figure 7: **Multi-part texture generation. Our method allows users to specify an object (via text or image) and its two parts to assign different textures (color-coded text prompts correspond to text descriptions for two textures). We show paired results for the predicted label, albedo, normal, and full-color renderings. The zoom-in patches demonstrate the generated normal textures on different parts.**

Qualitative evaluation.Figure 8 shows qualitative results of our method compared against three baselines. For each example, we show color and normal rendering with zoomed-in patches at the same location for detailed comparison. Our method achieves higher visual fidelity, more realistic details in normal space, and better color-geometry alignment than the baselines. In particular, our textures exhibit sharper details than those of Wonder3D, which tend to be overly smooth. In contrast to DreamGaussian and DreamCraft3D, our generated geometry details are more realistic and align well with the color appearance. In the avocado example, DreamCraft3D suffers from the "Janus problem", generating a brown core on both sides.

Quantitative evaluation.We perform a human perceptual study using Amazon Mechanical Turk (AMTurk). We set up a paired test, showing a reference prompt and two rendering results, one generated with our method and the other generated with one of the baselines. We conduct two separate surveys to evaluate the texture appearance and geometric details, respectively. For texture appearance, we render full-color RGB images and ask users "Which of the following views has more realistic textures?". For geometric details, we render shaded colorless images that only demonstrate the mesh geometry and ask users "Which of the following views has more realistic geometric details?". Please see Appendix A.5 for example screenshots of the paired rendering. Each user has two practice rounds followed by 20 test rounds to evaluate our method against DreamGaussian, Wonder3D, and DreamCraft3D. All samples are randomly selected and permuted, and we collect 1,000 responses.

Table 1 shows the mean and standard deviation of users' preference score; our method is preferred over all baselines. While DreamCraft3D has a close performance regarding texture appearance, it fails to obtain high-resolution geometric details that align well with color texture.

Ablation study.We perform an ablation study to evaluate key aspects of our method. We render both front and back views of a sampled object for each experiment. To illustrate details and cross-modal alignment, we present a global full-color view on the left, a global normal view on the right, and patch views of the full-color, albedo, and normal renderings in between. Figure 9 shows results of ablating diffusion-based guidance losses for texture refinement, specifically the visual guidance \(_{}\) and tactile guidance \(_{}\).

    & **Ours vs Wonder3D** & **Ours vs DreamCraft3D** \\ 
**Texture** & \(85.43 4.76\) & \(86.36 3.93\) & \(61.54 4.43\) \\
**Geometry** & \(92.85 3.47\) & \(88.07 3.80\) & \(84.20 4.17\) \\   

Table 1: **Human perceptual study. For all paired comparisons, our method is preferred (\(\) 50%) over the baselines for both texture appearance and geometric details.**

Figure 8: **Baseline comparison. Compared to the SOTA image-to-3D (Wonder3D and DreamGaussian) and text-to-3D (DreamCraft3D) baselines, our method produces significantly more plausible low-level geometry. For a fair comparison, we use the same input image for the first three rows.**

Omitting \(_{}\) reduces tactile texture details, while omitting \(_{}\) introduces misalignment between visual and tactile normal textures; for instance, bumps in the normal map do not match the white seeds in the albedo rendering. Our method ensures color consistency with tactile normals via ControlNet-guided visual refinement and enhances tactile texture details.

We also study the efficacy of tactile data processing by comparing our method with the texture map synthesized using the original tactile data without preprocessing stated in Section 3. Figure 10 shows that using the original data produces much more flattened textures since the low-frequency deformation of the gel pad due to uneven contact during the data collection would dominate the tactile signal and thus degrade the details of synthesized texture maps. Figure 11 shows the results of removing tactile input and only optimizing the albedo map with "textured prompts" using \(_{}\) and \(_{}\). Without tactile input, the output mesh is overly smooth, demonstrating the insufficiency of using text prompts only for fine geometry texture generation.

## 6 Discussion

Recent methods in 3D generation often struggle with unrealistic geometric details. To address this, we have introduced a novel approach that incorporates tactile information. Our method synthesizes both visual and tactile textures using a 3D texture field. Additionally, we have introduced a multi-part texturing pipeline for controllable region-wise texture generation. To our knowledge, this is the first use of high-resolution tactile sensing to improve 3D generation. Our method produces realistic, fine-grained geometric textures while maintaining accurate visual-tactile alignment.

Limitations.The quality of the generated coarse shape depends on the existing 3D generative models, which struggle with complex geometry. Additionally, slight seams may appear in our results due to UV unwrapping.

Figure 11: **Ablation study regarding tactile input**. We remove the tactile input while keeping the refinement loss. Without the tactile information, the generated 3D assets fail to capture fine-grained geometric details.

Figure 10: **Ablation study regarding tactile preprocessing**. Without tactile data preprocessing including high-pass filtering and contact area cropping, the generated geometric details tend to be flat and unrealistic.

Figure 9: **Ablation study regarding texture refinement**. We ablate our method regarding the tactile guidance loss \(_{}\) and visual guidance loss \(_{}\). Removing \(_{}\) results in fewer details of the generated tactile texture. Removing the visual guidance introduces misaligned visual and tactile normal textures. For example, the bumps in the normal map are misaligned with the locations of white seeds in the albedo rendering, as shown in the zoomed-in patches. Our method encourages the generated color to be consistent with the tactile normal using ControlNet-guided visual refinement loss while also enhancing the details in the tactile texture.