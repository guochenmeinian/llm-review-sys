# Automated Classification of Model Errors on ImageNet

Momchil Peychev, Mark Niklas Muller, Marc Fischer, Martin Vechev

Department of Computer Science

ETH Zurich, Switzerland

{momchil.peychev, mark.mueller, marc.fischer, martin.vechev}@inf.ethz.ch

Equal contribution

###### Abstract

While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over \(95\%\) accuracy and shifting the focus on investigating why the remaining errors persist. Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 900 models. Perhaps surprisingly, we find that across model architectures, scales, and pre-training corpora, top-1 accuracy is a strong predictor for the _portion_ of all error types. In particular, we observe that the portion of severe errors drops significantly with top-1 accuracy indicating that, while it underreports a model's true performance, it remains a valuable performance metric. We release all our code at https://github.com/eth-sri/automated-error-analysis.

## 1 Introduction

ImageNet(Deng et al., 2009; Russakovsky et al., 2015) has established itself as one of the most influential and widely used datasets in computer vision, driving progress in object recognition (Krizhevsky et al., 2012), object detection (Tan et al., 2020), and image segmentation (Minaee et al., 2022). As state-of-the-art models have come close to, and by some metrics exceeded, human performance, the significance of further progress in top-1 and top-5 accuracy has, been questioned in the face of label errors and systematic biases (Beyer et al., 2020; Tsipras et al., 2020).

The most severe such bias is the lack of multi-label annotations in the original ImageNet dataset, with recent studies finding that roughly a fifth of images show multiple entities. While state-of-the-art models have learned to exploit labeling biasses on these images (Tsipras et al., 2020), best practices have shifted to reporting multi-label accuracy (MLA) computed using new multi-label annotations (Beyer et al., 2020; Shankar et al., 2020). Further, many ImageNet and especially organism classes, are hard to distinguish even by trained humans (Horn et al., 2015; Northcutt et al., 2021; Lee et al., 2017), leading to persistent labeling errors.

In the face of these challenges and with state-of-the-art models exceeding \(95\%\) MLA, the focus has increasingly shifted towards analyzing and understanding the remaining model errors instead of blindly pursuing improvements in headline accuracy numbers. To this end, Vasudevan et al. (2022) review all remaining errors of two state-of-the-art models using a panel of experts and classify themwith regards to both error category and severity. While they find that many of the remaining errors are minor or can be attributed to fine-grained class distinctions, they also find major classification errors that are not so easily explainable. We believe that tracking and analyzing the distribution of these error types over a large number of models can not only help us to better understand the impact of novel training techniques and architectures but also identify where the biggest challenges lie and thus how to address them. However, the manual review process employed by Vasudevan et al. (2022) has several issues, preventing it from being employed for a large scale or repeated study of model errors: (i) it is time-consuming even for precise models, making it infeasible to repeat for a large number of potentially less precise models, (ii) it requires (a panel of) expert reviewers which need to be trained on the fine-grained class distinctions, and (iii) it is inconsistent as different reviewers or even the same reviewers at different times might classify the same error differently.

This WorkTo overcome these challenges, we propose an automated error classification pipeline that we use to study the distribution of different types of errors across 962 models of different scales, architectures, training methods, and pre-training datasets. Our pipeline allows us to automatically detect all four error categories identified by Vasudevan et al. (2022): (i) fine-grained classification errors are detected using a set of 161 manually defined superclasses, (ii) fine-grained out-of-vocabulary errors are detected using a visual similarity based criterion for prediction quality and confirmation of their out-of-vocabulary nature using an open-world classifier, (iii) non-prototypical examples are identified using the exhaustive annotations by Vasudevan et al. (2022), and (iv) spurious correlations are detected using a co-occurrence-frequency based criterion.

Main FindingsOur automated error classification pipeline allows us to, for the first time, study the _distribution of different error types_ across a large number of models, leading to the following insights: (i) even MLA is a pessimistic measure of model progress with the _portion_ of severe model failures quickly decreasing with MLA, (ii) this reduction of model failure rate with MLA is more pronounced for larger (pre-)training corpora, _i.e._, models trained on more data make less severe errors even at the same top-1 or multilabel accuracy, and (iii) organism and artifact classes exhibit very different trends and prevalences of error types, _e.g._, fine-grained classification errors are much more frequent for organisms than for artifacts, while artifacts suffer much more from spurious correlations and out-of-vocabulary errors. We believe that these insights can help guide future research in computer vision and that studying the effects of new methods on the resulting error distribution can become an important part of the evaluation pipeline.

## 2 Related Work

Multi-Label AnnotationsWhile the ImageNet dataset is annotated with a single label per image, many images have been found to contain multiple entities (Beyer et al., 2020; Shankar et al., 2020; Tsipras et al., 2020; Yun et al., 2021). Thus, multi-label accuracy (MLA) with respect to new multi-label annotations has been established as a more meaningful metric. Yun et al. (2021) generate pixel-wise labels for ImageNet by directly applying a classification layer to image embeddings before spatial pooling. Beyer et al. (2020) collect Reassessed Labels (ReaL) for the whole ImageNet validation set, by first identifying 6 models with high prediction coverage and accuracy, before manually annotating all images where these 6 models disagree using non-expert labelers. They discard \(3\,163\) images where these labelers disagreed. Tsipras et al. (2020) collect multi-label annotations for \(10\,000\) validation set images, and find that top-1 accuracy is 10% lower for multi- compared to single-label images while MLA accuracy is identical. Shankar et al. (2020) collect multi-label annotation for \(40\,000\) ImageNet and ImageNetV2 validation images by manually reviewing all predictions made by a diverse set of 72 models, using human expert labelers.

Label ErrorsNorthcutt et al. (2021) study label errors across 10 commonly used datasets, including ImageNet, using a confident learning framework to identify potential errors before validating them with Mechanical Turk (MTurk). Studying the whole validation set, they report an error rate of over \(5.8\%\). Lee et al. (2017) manually review 400 randomly selected classification errors of an ensemble model and find the ImageNet label for a substantial portion to either be incorrect or not describe the main entity in the image. Vasudevan et al. (2022) reviewed all remaining errors for two state-of-the-art models with a panel of experts and found that of 676 reviewed model mistakes, 298 were either correct, ambiguous, or the original ground truth incorrect or problematic.

Error AnalysisRecent work has focused on understanding the types of errors models still make on ImageNet. To this end, one strand of work analyses the differences in errors between models. Mania et al. (2019) find that independently trained models make errors that correlate well beyond what can be expected from their accuracy alone. Geirhos et al. (2020) analyze the consistency between errors made by humans and CNNs on a \(16\) class version of ImageNet. They observe that while CNNs make remarkably similar errors, the consistency between humans and CNNs barely goes beyond chance. Nguyen et al. (2021) analyze wide and deep ResNets and find that these exhibit different error patterns. Mania and Sra (2020) find that more precise models typically dominate less precise ones, _i.e_., their error set is a subset of that of less precise models. Lopes et al. (2022), in contrast, find that different pretraining corpora and training objectives can significantly increase error diversity with Andreassen et al. (2021) showing that this diversity reduces significantly during finetuning.

Vasudevan et al. (2022) focus on the errors current state-of-the-art models make, identifying four categories: (i) _fine-grained_ errors describe a model's failure to distinguish between two very similar classes, (ii) _fine-grained out-of-vocabulary_ errors occur when an image shows an entity not contained in the ImageNet vocabulary and the model instead predicts a similar class, (iii) _spurious correlations_ cause the model to predict a class that is not shown in the image in the presence of strongly correlated features, and (iv) _non-prototypical_ instantiations of a class are not recognized by the model.

Datasets for Error AnalysisIn addition to work analyzing model errors on fixed datasets, there has been a growing interest in datasets specifically designed to highlight and thus analyze specific error types. Singla and Feizi (2022) and Moayeri et al. (2022) find that, for some classes, models rely heavily on correlated or spurious features suffering severely reduced accuracy if these are absent or removed. To study this effect, they introduce Salient ImageNet(Singla and Feizi, 2022) and Hard ImageNet(Moayeri et al., 2022), providing soft masks for causal and correlated features and segmentation masks, respectively. Hendrycks et al. (2021) propose ImageNet-A, a dataset of \(7\,500\) single-entity, natural adversarial examples, which induce high-confidence misclassifications in a set of ResNet50 and belong to \(200\) ImageNet classes that were chosen to minimize class overlap and the potential for fine-grained errors (see App. C for an analysis). Vasudevan et al. (2022) collect images where state-of-the-art models fail in an unexplained manner in ImageNet-Major. Taesiri et al. (2023) investigate the effect of zoom and crop on classifier performance and introduce ImageNet-Hard as the set of images (from a range of ImageNet-like datasets) that none of their considered models classified correctly for any crop. Idrissi et al. (2023) annotate ImageNet images with respect to how they differ from what is prototypical for their class, introducing ImageNet-X. This allows them to study the effect of these image-specific variations such as atypical pose, background, or lighting situation, on accuracy over a large number of models. In contrast, our (and Vasudevan et al. (2022)'s) work focuses more on systematic errors caused (partially) by labeling (set) choices rather than on what makes an individual image hard to classify.

## 3 Categorizing ImageNet Errors

In this section, we introduce our automated error classification pipeline, which aims to explain model errors by assigning it one of six error types. We consider errors that can not be explained in this way to be particularly severe _model failures_. While the definitions of our error types are heavily inspired by Vasudevan et al. (2022), we address three main issues of their manual approach with our automated pipeline: (i) it is time-consuming even for precise models and intractable for imprecise ones, (ii) it requires a panel of expert reviewers, typically not available, and (iii) it introduces inconsistencies due to human expert error, disagreement, or ambiguity.

Below, we first provide a brief overview of our pipeline (illustrated in Fig. 1), before discussing the different error types and how we identify them in more detail.

We consider errors due to overlapping class definitions (discussed in SS3.1) and missing multi-label annotations (SS3.2) to be the least severe, as they can be interpreted as labeling errors rather than classification errors. When a model predicts a class that is closely related to one of the labels, we call this a fine-grained classification error (SS3.3), as the model succeeds in recognizing the type of entity but fails during the fine-grained distinction. When an image contains an entity that does not belong to any of the ImageNet classes and a model predicts a label that is closely related to this (out-of-vocabulary) class, we call this a fine-grained out-of-vocabulary error (SS3.4). We consider both types of fine-grained errors to be minor. If a model fails on an image that shows a very non-prototypical instance of a given class, we call this a non-prototypical error (SS3.5). If a model predicts a class that commonly co-occurs with a ground-truth class but is not shown in the image, we attribute this error to spurious correlation and discuss it in SS3.6. We denote both of these errors as explainable errors. If an error can be attributed to multiple causes, we assign the least severe category and thus design our pipeline to consider error types in order of increasing severity (see Fig. 1).

Throughout this section, we include examples and trends for the kinds of errors we discuss. We generally describe these separately for images with a ground-truth class describing an organism (410 of 1000 classes) and artifacts (522 classes) (in line with previous work (Shankar et al., 2020)), as we observe that these two groups of classes exhibit different error patterns. These two groups account for almost all ImageNet classes, with the remaining 68 classes being assigned the group "other". Where we find interesting trends, we further distinguish models by their (pre-)training dataset ranging in size from one million (ImageNet) to multiple billion images (Instagram, LAION-2B), their architecture, including a broad range of MLPs, CNNs, transformer-based, and hybrid models, and their (pre-) training method. For the full details on all 962 models we consider, please refer to App. F. We provide more examples of every error type in App. E and more detailed trends in App. B.

### Class Overlap

Prior work has established that a small number of ImageNet classes exhibit extreme overlap (Northcutt et al., 2021; Vasudevan et al., 2022). We illustrate one such example in Fig. 2: tusker is defined as "an animal with tucks"2, which describes a strict superset of african elephant and has a significant overlap with indian elephant (females have short and sometimes no tusks). To avoid penalizing a model for correct predictions that do not match the ground truth, we consider all predictions describing a _superset or equivalent_ of the ground-truth class to be correct. For example, we accept tusker for an image labeled african elephant, but not vice-versa, as the latter might actually show a boar. We follow the mappings of equivalence and containment from Vasudevan et al. (2022) and refer to their App. C for full details.

In Fig. 3, we visualize the portion and number of top-1 errors identified to be correct, separating images with a ground-truth class describing an organism (green hues) and artifacts (red hues) and encoding the pre-training dataset size by assigning darker colors to larger datasets. While trends for artifacts and organisms seem very similar for portions of top-1 errors, we observe a clear difference when looking at their absolute numbers. There, two competing effects are at play. With increasing accuracy, more samples get classified correctly, in

Figure 1: We first remove errors w.r.t. the original ImageNet labels caused by overlapping class definitions or missing multi-label annotations, yielding multi-label accuracy (MLA). We then, in this order, identify fine-grained misclassifications, fine-grained misclassifications where the true label of the main entity is not included in the ImageNet labelset, non-prototypical examples of a given class, and spurious correlations. This leaves us with severe model failures that are unexplained by our categorization.

Figure 3: Portion (left) and number (right) of top-1 errors caused by _class overlap_ by group – organisms (green) and artifacts (red). A 95% confidence interval linear fit is shown on the right.

Figure 2: Venn-Diagram of the tusker, indian elephant, and african elephant classes.

cluding as overlapping classes, thus increasing the number of such errors. On the other hand, more accurate models leverage labeling biases to pick the "correct" among equivalent classes (Tsipras et al., 2020), thus reducing the number of such errors. While the former effect seems to dominate for artifacts, the latter dominates for organisms.

### Missing Multi-Label Annotations

While the ImageNet dataset is annotated with a single label per image, many images contain multiple entities (Beyer et al., 2020; Shankar et al., 2020; Tsipras et al., 2020). Consequently, a model predicting the class of any such entity, different from the original ImageNet label, is considered incorrect when computing top-1 accuracy. For example, the image shown in Fig. 4 contains ox, barn, and fence while only being labeled ox. To remedy this issue, multi-label accuracy (MLA) considers all shown classes, according to some multi-label annotation, to be correct. In this work, we use the annotations collected by Shankar et al. (2020) and improved by Vasudevan et al. (2022) combined with the mean class-wise accuracy definition of MLA (Shankar et al., 2020).

We visualize the portion and number of top-1 errors that turn out to be correct under multi-label evaluation in Fig. 5. When looking at the portion of errors (Fig. 5 left), we observe a similar trend for organisms and artifacts with missing multi-label annotations accounting for an increasing portion of top-1 errors as MLA increases and artifacts consistently more affected than organisms. When looking at the absolute numbers, we again observe the number of errors explained by multi-label annotations to first increase with accuracy as models become more precise, before decreasing again as models start to leverage labeling biases (Tsipras et al., 2020). As missing multi-label annotations explain up to \(60\%\) of model errors, we henceforth use MLA instead of top-1 accuracy as the reference for model comparison.

### Fine-Grained Classification Errors

Many of the ImageNet classes and especially the organism classes are very similar, making their fine-grained distinction challenging even for trained humans. For example, Fig. 6 is classified to cornet while actually showing a french horn, both brass wind instruments. While these errors can be corrected for the relatively small validation set using expert human judgment (Horn et al., 2015; Vasudevan et al., 2022), the much larger training set remains uncorrected. In this light, we believe a comprehensive model evaluation and comparison, should consider the failure to distinguish between very similar classes to be less severe than the failure to distinguish very different classes.

To automatically detect such fine-grained classification errors, we manually review all 1000 ImageNet classes and define semantically similar superclasses guided by whether an untrained human could reasonably confuse them. We obtain \(161\) superclasses, containing between \(2\) and \(31\) classes with an average of \(6.7\) and a median of \(4\). An additional \(74\) classes are not part of any superclass. \(50\) superclasses contain organisms (\(9.8\) on average) and \(101\) contain artifacts (\(5.3\) on average). We visualize the portion and number of fine-grained errors in Fig. 7. As expected, we observe significantly more fine-grained errors for organisms than for artifacts, explaining up to \(88\%\) and \(50\%\) of multi-label errors, respectively.

Figure 4: Image with label ox, but also showing the classes barn and fence. Example from Yun et al. (2021)

Figure 5: Portion (left) and number (right) of top-1 errors caused by _missing multi-label annotations_ by group – organisms (green) and artifacts (red).

Figure 6: Image labeled french horn, but predicted to show a cornet.

Figure 7: Portion (left) and number (right) of MLA errors caused by _fine-grained misclassifications_ by group – organisms (green) and artifacts (red).

### Fine-Grained Out-of-Vocabulary Errors

Often images contain entities that do not belong to any ImageNet class, we call these out-of-vocabulary. For example, Fig. 9 shows an image of two blue-cheeked butterflyfish, which is a class not part of the ImageNet labelset. Instead, the target label is coral reef, describing the background of the image. The classifier predicts rock beauty, which is a WordNet child of butterflyfish (despite being part of the angelfish and not the butterflyfish family) and the closest class in the ImageNet labelset to the blue-cheeked butterflyfish. While an optimal classifier could be expected to always predict a class from the intersection of the contained entities and the ImageNet labelset, the following is often observed in practice (Vasudevan et al., 2022). The classifier tries to classify a prominent entity in the image, but as it is not part of the ImageNet labelset, it predicts a closely related class instead. We argue, that these cases should be treated at least as leniently as a fine-grained classification error, but they are harder to detect. As the entity the model tried to classify can not be assigned any ImageNet label, it will not be included in any multi-label annotation and can thus not be captured by defining groups of closely related classes.

To still detect fine-grained out-of-vocabulary (OOV) errors, we propose the following approach, illustrated in Fig. 8 for the above example. We first retrieve the 10 most visually similar images from the ImageNet training set using cosine similarity in the CLIP embedding space (Radford et al., 2021), in this case all labeled rock beauty. If a label of any of these images belongs to the same superclass as the model's prediction (as defined in SS3.3), we conclude that an entity similar to our prediction is shown in the image and proceed with our analysis. Otherwise, we conclude that the analyzed error is not fine-grained. To confirm that the shown entity is indeed out-of-vocabulary, we first collect a set of proposal labels from the following (illustrated in Fig. 10): all ImageNet labels in the same superclass as the model's prediction (IV), all direct WordNet siblings of the model's prediction, up to but excluding the first common ancestor with the ImageNet label (IV and OOV). Finally, we use CLIP as an open-world classifier to score each of the proposed labels. If the highest scoring class is not included in the ImageNet label set, we consider the shown entity to be OOV and conclude that this error was indeed fine-grained OOV.

Figure 8: To confirm a fine-grained out-of-vocabulary error we proceed as follows. Given a misclassified input image, we first retrieve the 10 most visually similar images from the ImageNet train set. If the superclass of any of these images matches the model prediction, we conclude that an entity with a class similar to our prediction is shown in the image. To confirm that this entity is indeed out-of-vocabulary, we first collect a set of proposal labels from WordNet (Miller, 1992) which are partially in and out of vocabulary. Then, we use an Open World Classifier to score each of the proposed labels. If the highest scored among these is not included in the ImageNet labelset, we consider the analyzed error to be OOV.

Figure 10: Illustration of proposal labels \(\) are shown in blue boxes \(\), direct WordNet siblings in orange \(\), and ancestors not shared with the label \(\) in green \(\).

Figure 9: Image labeled coral reef, but prediction rock beauty (fish).

We visualize the portion and number of multi-label errors categorized as fine-grained OOV in Fig. 11. Interestingly, we observe that fine-grained OOV errors are not only much more prevalent in artifacts but also that their portion increases much more quickly with MLA for artifacts. We hypothesize that this is due to the ImageNet labels covering organisms occurring in the validation set much more comprehensively than artifacts, and many images of artifacts being cluttered and full of other (OOV) objects. We note that this might be a reason why trained humans still outperform even state-of-the-art models on artifacts but not organisms (Shankar et al., 2020). Interestingly, we observe that, across pretraining datasets and model sizes, the number of fine-grained OOV errors for artifact drops much quicker with MLA above around \(93\%\) MLA (see confidence intervals in Fig. 11 right).

### Non-Prototypical Instances

Many concepts described by any one ImageNet class label are broad, depend on social and geographic factors, and change over time. Combined with the search-based data collection process, this has led to a biased dataset with skewed representations of the concepts described by the contained classes. Therefore it is unsurprising that models perform worse on non-prototypical instances or pictures of any given object, _i.e_., instances that, while clearly belonging to a given class, can be considered outliers in the ImageNet distribution of that class.

For example, in Fig. 12 we see a non-prototypical whistle. We believe it is interesting to track progress on these hard, non-prototypical instances as they are likely to be a good indicator to what extent a model has learned to recognize the underlying concepts of a class. However, defining what constitutes a non-prototypical instance of any class is hard and to a large extent subjective. Fortunately, this error type is independent of the (incorrect) prediction made by the model, allowing us to directly leverage the manual categorization of non-prototypical images by Vasudevan et al. (2022). We thus implicitly decide that all images that are classified correctly by the state-of-the-art models (ViT-3B and Greedy Soups) they considered are not sufficiently non-prototypical to explain an error by another model.

We visualize the number of errors caused by non-prototypical instances in Fig. 13. Interestingly and in contrast to all other error types, there is no strong correlation between performance on non-prototypical examples and overall MLA. This suggests that these non-prototypical examples are not inherently hard to classify for all models. Further surprisingly, non-prototypical examples account for a very similar portion of errors for artifacts and organisms, despite the appearance of artifacts of the same class varying significantly more, which we expected would lead to a larger portion of artifact errors being explained by their non-prototypical appearance.

### Spurious Correlations

Entities of many classes frequently co-occur with features that have no causal relationship to that class. For example, oxen are frequently observed on green pastures (such as in Fig. 4). We call these correlated features (Neuhaus et al., 2022). Inherently this is not a problem, however, it has been observed that models frequently rely on these correlated features to make predictions (Choi et al., 2012; Beery et al., 2018; Neuhaus et al., 2022; Moayeri et al., 2022), for example, by predicting ox when shown only a green pasture or camel when shown

Figure 11: Portion (left) and number (right) of MLA errors identified as _fine-grained OOV_ by group – organisms (green) and artifacts (red). 95% confidence interval linear fit is shown on the right. For artifacts, models are divided at \(93\%\) MLA.

Figure 14: An image with label ski mask (and also multi-class label ab), but prediction ski.

Figure 13: Portion (left) and number (right) of MLA errors identified as _non-prototypical sample_.

a cow in the desert. When correlated features lead to prediction errors, we call these spurious correlations. Here, we focus on the case where the presence of a correlated spurious feature causes a misclassification to the correlated class, despite no corresponding entity being shown. In Fig. 14, we show an example of such a spurious correlation where the presence of a ski mask and alps causes a model to predict ski, despite the image containing no ski. We do not consider the error mode, where the absence of a correlated feature causes the model to _not_ predict the correlated class, despite a corresponding entity being shown, investigated by Singla and Feizi (2022); Moayeri et al. (2022).

To identify errors caused by spurious correlations, we identify pairs of commonly co-occurring classes and then categorize errors as spurious correlations if an incorrect model prediction and a multi-label form such a co-occurrence pair. More concretely, we first extract all pairs of co-occurring labels from the ReaL multi-label annotations (Beyer et al., 2020), excluding samples we evaluate on. We then filter out pairs that either belong to the same superclass as defined in SS3.3 or only co-occur once. Using this process, we extract \(13\,090\) label pairs from \(6\,622\) images with more than one label, yielding \(1019\) unique co-occurrence pairs after filtering, which indicate a spurious correlation if they occur.

We visualize the portion and number of errors caused by spurious correlation in Fig. 15. We observe that artifacts and organisms follow notably different trends. Not only is the portion of errors caused by spurious correlations much larger for artifacts than organisms, but it also increases with MLA for artifacts (at an increased rate for higher MLA), while it stays roughly constant for organisms. For state-of-the-art models, spurious correlations explain up to \(15\%\) of errors on artifacts making them the second largest error source we identify.

## 4 Analysis of Model Errors on ImageNet

In this section, we discuss global trends, analyze their interaction with architecture and training choices, and validate our automatic pipeline against the manual analysis of Vasudevan et al. (2022).

Model FailuresAfter removing all minor (see SS3.3 and SS3.4) and explainable (see SS3.5 and SS3.6) errors, from the multi-label errors (MLE), we are left with a set of particularly severe, unexplained model failures (MLF). In Fig. 16 we visualize the portion of these unexplained model failures over multi-label accuracy (MLA) and standard top-1 accuracy, again split by artifact (red) and organism (green). A ratio of \(1\) corresponds to none of the MLEs being explainable by our pipeline, while a ratio of \(0\) corresponds to all MLEs being explainable, \(93\%\)) and top-1 accuracy (at \(80\%\)) for linear fits.

thus consisting only of less severe error types. Surprisingly, both top-1 accuracy and MLA are pessimistic when it comes to reporting model progress, with unexplained model failures decreasing at a much quicker rate than both top-1 errors and MLE.

Further, we observe different error distributions for organisms and artifacts. Firstly, the portion of model failures is much higher for artifacts than for organisms. Secondly, while the portion of unexplained errors decreases for both organisms and artifacts with MLA and top-1 accuracy, indicating that as models get more accurate they make not only less but also less severe errors, the rate of this change differs. For artifacts, this decrease is initially slower but then at around 93% MLA or 80% top-1 error, the portion of severe model failures starts to drop rapidly (roughly three times as fast as before), while the decrease is roughly linear for organisms. This phase change becomes particularly apparent when viewing the trend over MLA, where it is even visible for organisms.

Figure 16: Portion of MLA (left) and top-1 (right) errors that can not be explained and are thus considered _model failures_ – organisms (green) and artifacts (red). 95% confidence interval linear fit is shown shaded. Models are divided by MLA (at \(93\%\)) and top-1 accuracy (at \(80\%\)) for linear fits.

Figure 15: Portion (left) and number (right) of MLA errors caused by _spurious correlations_ by group. A 95% confidence interval linear fit is shown on the right. For artifacts, models are divided into those with more and less than \(93\%\) MLA.

### (Pre-)training Dataset

Throughout SS3, we have illustrated the pretraining dataset size with the marker hue. Generally, we observe that conditioned on identical MLA, the effect of pre-training dataset size is rather modest. Here, we divide the 12 datasets we consider into 4 categories from "small" (\(<\)5M) to "large" (\(>\) 500M) depending on the number of included images, illustrating separate fits in Fig. 17, with a darker shade corresponding to a larger dataset. We observe that across a wide range of accuracies, larger pretraining datasets lead to a faster reduction of model failures with MLA for both artifacts and organisms. While this effect is partially explained by larger datasets more frequently leading to higher MLA, it is also observed for narrow MLA ranges. Considering individual error types, we observe that in particular fine-grained errors are more frequent for larger pretraining datasets. We hypothesize that this is due to these larger datasets leading to better underlying representations, allowing the model to succeed in the coarse classification for harder samples, while still failing on the fine-grained distinctions.

### Model Architecture

In Fig. 18, we again show the portion of unexplained model failures over MLA, this time broken down by architecture type and colored by the number of parameters (larger models have darker hues). To investigate whether modern convolutional architectures (ConvNeXts (Liu et al., 2022)) and vision transformers (ViT (Dosovitskiy et al., 2021), Swin (Liu et al., 2021), EVA (Fang et al., 2023)) exhibit different trends in terms of model failures, we focus on state-of-the-art models with at least \(92\%\) MLA that were trained using a dataset with at least 50M images, leaving 28 CNN-based and 79 transformer-based models. While we observe clear trends of larger models performing better, the rate of model failures seems to be largely independent of model size when conditioned on MLA. We do observe a slight trend where the model failure rate of transformers decreases faster with MLA for organisms but slower for artifacts when compared to ConvNeXts. We speculate that this might be due to the ConvNeXts leveraging textures more heavily (Geirhos et al., 2019), which could be particularly helpful for fine-grained class distinctions on organisms leading to a larger portion of multi-label errors being model failures at the same MLA.

### Comparison to Vasudevan et al. (2022)

To evaluate the alignment of our automated error analysis with a panel of human experts, we compare its results to those of Vasudevan et al. (2022) on both networks they consider, here for ViT-3B and in App. A for Greedy Soups.

Comparing the results in Table 1, we confirm that our pipeline is conservative, classifying \(16.4\%\) (\(62\) / \(378\)) of the errors assigned an explanation by Vasudevan et al. (2022) as model failures. On the remaining errors, our pipeline agrees with the panel of human experts in \(73.1\%\) of the cases. By manually inspecting the \(85\) differently classified errors, we determine that for only \(32\) of these (\(8.4\%\) of all errors), the categorization by Vasudevan et al. (2022) is clearly preferable to ours. Furthermore, our pipeline encodes a minimal-severity bias, absent in Vasudevan et al. (2022)'s categorization. That is, when multiple error explanations would be valid, our pipeline consistently chooses the least severe one. This highlights a further advantage of our approach. While any human judgment is inherently subjective and thus prone to differ between expert panels or even for the same panel over time, as acknowledged by Vasudevan et al. (2022), our approach is consistent and repeatable.

Figure 17: Portion of model failures (MLF) over MLA depending on pre-training dataset size with 95\(\%\) confidence intervals for models with at least 82\(\%\) MLA. Confidence intervals are shown darker for larger pretraining dataset sizes.

Figure 18: Portion of model failures (MLF) over MLA depending on model architecture. 95\(\%\) confidence interval for transformer-based models is shown darker and for CNN-based models lighter.

Observing similar trends for Greedy Soups in App. A, we confirm that on all models for which human expert error categorizations are available, our automated pipeline is well aligned with their judgment while providing consistent, repeatable, and conservative error categorizations.

## 5 Limitations

In this section, we briefly reflect on the limitations of our work.

Personal BiasOur choices in the implementation of the error analysis pipeline reflect our personal biases and decisions, such as the superclasses we picked in SS3.3. This is not only limited to our personal biases, but, as our pipeline relies on prior work in several places, it also encodes the biases of their authors. For example, we rely on the class overlap mappings from Vasudevan et al. (2022) and multi-class labels by Shankar et al. (2020) and Vasudevan et al. (2022).

Extension to New DatasetsTo be applied to a new dataset, our pipeline requires multi-label and non-prototypicality annotations and superclass definitions. While less precise than using a human-annotated gold standard, multi-label annotations could be sourced using a Re-Label (Yun et al., 2021) approach, allowing (given sufficient scale) spurious correlation pairs to be extracted from co-occurrence frequencies. After defining superclasses manually as required by their very definition, this would allow all error types except for the rare non-prototypical instances to be categorized. With the rest of the pipeline in place, non-prototypical instances could be annotated by reviewing the uncategorized errors shared by multiple well-performing models. However, while feasible these steps still require a significant amount of manual work. We showcase a (partial) adaption to the similar ImageNet-A dataset in App. C.

## 6 Conclusion

As state-of-the-art models come close to and exceed human performance on ImageNet, focus is increasingly shifting towards understanding the last remaining errors. Towards this goal, we propose an automated error categorization pipeline that we use to study the distribution of different error types across 962 models, of different scales, architectures, training methods, and pre-training datasets. We distinguish between minor errors, constituting failures on fine-grained class distinctions both when the ground truth was in and out-of-vocabulary, explainable errors, constituting failures on non-prototypical examples and due to spurious correlations, and unexplained model failures, constituting all remaining errors. We find that even MLA is a pessimistic measure of model progress with the portion of severe errors quickly decreasing with multi-label-accuracy. Further, we find that organism and artifact classes exhibit very different trends and prevalences of error types. For example, we observe that fine-grained class distinctions are a much bigger challenge for organisms than for artifacts, while artifacts suffer much more from spurious correlations and out-of-vocabulary errors, with these trends becoming increasingly pronounced as models become more accurate. We believe that such an analysis of a new method's effects on model error distributions can become an important part of the evaluation pipeline and lead to insights guiding future research in computer vision.

   Error categories & FG & FG OOV & Non-prot. & Spur. Corr. & Model failures & Total (row) \\  Fine-grained & \(192\) & \(15\) & \(0\) & \(10\) & \(25\) & \(242\) \\ Fine-grained OOV & \(9\) & \(20\) & \(0\) & \(11\) & \(14\) & \(54\) \\ Non-prototypical & \(13\) & \(2\) & \(12\) & \(3\) & \(0\) & \(30\) \\ Spurious Correlation & \(10\) & \(12\) & \(0\) & \(7\) & \(23\) & \(52\) \\  Total (col) & \(224\) & \(49\) & \(12\) & \(31\) & \(62\) & \(378\) \\   

Table 1: Comparison of our automated analysis of the errors of a ViT-3B model with the manual annotations from (Vasudevan et al., 2022). Rows correspond to the error categories assigned by a human expert while columns correspond to the error types assigned by our automated pipeline.