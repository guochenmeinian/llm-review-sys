# Group Robust Classification

Without Any Group Information

 Christos Tsirigotis

Universite de Montreal, Mila, ServiceNow Research

Work done during internship at ServiceNow Research. Author correspondence at tsirigoc@mila.quebec.

Joao Monteiro

ServiceNow Research

Work done during internship at ServiceNow Research. Author correspondence at tsirigoc@mila.quebec.

Pau Rodriguez

Apple MLR

David Vazquez

ServiceNow Research

Work done during internship at ServiceNow Research. Author correspondence at tsirigoc@mila.quebec.

Aaron Courville

Universite de Montreal, Mila, CIFAR CAI Chair

This work was also funded, in part, from A. Courville's Sony Research Award. Courville also acknowledges support from his Canada Research Chair.

###### Abstract

Empirical risk minimization (ERM) is sensitive to spurious correlations in the training data, which poses a significant risk when deploying systems trained under this paradigm in high-stake applications. While the existing literature focuses on maximizing group-balanced or worst-group accuracy, estimating these accuracies is hindered by costly bias annotations. This study contends that current bias-unsupervised approaches to group robustness continue to rely on group information to achieve optimal performance. Firstly, these methods implicitly assume that all group combinations are represented during training. To illustrate this, we introduce a systematic generalization task on the MPI3D dataset and discover that current algorithms fail to improve the ERM baseline when combinations of observed attribute values are missing. Secondly, bias labels are still crucial for effective model selection, restricting the practicality of these methods in real-world scenarios. To address these limitations, we propose a revised methodology for training and validating debiased models in an entirely bias-unsupervised manner. We achieve this by employing pretrained self-supervised models to reliably extract bias information, which enables the integration of a logit adjustment training loss with our validation criterion. Our empirical analysis on synthetic and real-world tasks provides evidence that our approach overcomes the identified challenges and consistently enhances robust accuracy, attaining performance which is competitive with or outperforms that of state-of-the-art methods, which, conversely, rely on bias labels for validation.

## 1 Introduction

Supervised learning algorithms typically rely on the empirical risk minimization (ERM) paradigm - minimizing the average loss on a training set. The ERM paradigm operates under the assumption that the training data is a representative sample of the true data distribution . Consequently, models that achieve low expected loss can still be _unfair_ when the model is tasked to predict outcomes for underrepresented groups , and prone to _rely on spurious correlations_ between target annotations and generative attributes of the training data  that do not hold up under more general testing conditions. As automated predictors are deployed in failure-critical applications  or interact with society, where fairness must be guaranteed , robustness and fairness become requirements that standard learning strategies do not satisfy.

These limitations have motivated the search for alternatives that perform uniformly across different data subgroups [69; 61] or, equivalently, that rely less on spurious features  or "shortcuts" . Part of solving this problem includes making the correct assumptions about data, which should consider spurious statistical correlations due to bias in its generative process. A popular example of this issue is the _cow vs. camel_ classification problem .

As one might expect, pictures of cows very often contain a grass background, while camels are usually depicted in a desert. As such, a binary classifier that predicts whether there is grass in the background of an image could achieve a high prediction accuracy on top of natural images of the two animals. However, such a classifier would fail whenever the background changes from the typical occurrences.

While progress has been made, existing methods to improve robustness on the least frequent attribute combinations (_e.g._, cow on sand), require knowledge about the bias attributes (bias-supervision) either during training [6; 61], or validation [57; 48; 50]. This limits their applicability in practical scenarios where bias annotations could be too costly or impossible to obtain. Even in cases where they can be obtained somewhat efficiently, human annotators might be biased themselves or sensitive annotations may not be readily available due to privacy concerns .

Moreover, access to group information is rarely satisfiable on natural data, where a _curse of generative dimensionality_ implies that given the high number of attributes controlling the generative process, no realistic finite sample can cover all possible - exponential in number - combinations of such attributes. In fact, many group robustness algorithms that rely on data rebalancing or loss reweighting techniques [61; 48] assume that all group combinations are present during training. However, this condition is not satisfied in systematic generalization tasks, where certain data subgroups that are present in test data, are absent from training data. This motivates us to study the extent that training algorithms can successfully operate using group information implicitly present in the training dataset.

Specifically, in the case of these challenging systematically split datasets, we study the generalization of models on unseen combinations of attributes that were independently observed during training. This combinatorial type of generalization draws its name from the cognitive property of systematicity . In deep learning, this question was introduced for the first time by Lake and Baroni , who studied the systematicity of RNN models in sequence prediction tasks. Here, we raise the same question in classification. To do so, we introduce a benchmark consisting of systematic splits from the MPI3D (thereby named sMPI3D) image dataset .In particular, we use the '_real_' split of MPI3D which consists of photographs of a robotic arm that has a colored rigid object attached to its end effector. The images are captured in a way that controls their generative attributes, such as the shape of the rigid object or the position of the robotic arm. We use it to ask the following question: given a'shape' classifier that has been trained on objects of all possible shapes and colors but only a subset of their combinations (e.g., red cubes and blue spheres), how would it perform for a new color-shape combination (e.g., blue cubes)? In this example, the 'color' attribute plays the role of a _bias_, obstructing view of all possible colors that cubes could have. In Appendix D.1 we describe in more detail the construction of the sMPI3D task, while in Figure 1(d) we illustrate an example of a systematic split.

As discussed above, existing state-of-the-art methods suffer from two limitations. The first one, as demonstrated by Asgari et al. [3, Table 1], is that the robust accuracy of many recent training algorithms, which do not demand any bias annotations during training, degrades severely when there is no access to bias labels during model selection. This highlights that robust algorithms should prescribe a way of performing bias-unsupervised validation, assuming the same access to i.i.d. data

Figure 1: **Unsupervised Logit Adjustment (uLA). We train a linear classifier on top of an SSL pre-trained model to obtain biased predictions. These predictions are then leveraged to train a debiased model. No bias information is used during training or cross-validation.**

resources as training. The second, revealed by our study on sMPI3D (Section 4.2), is that in most cases these methods fail to improve over the ERM baseline, which fails to systematically generalize.

To address these issues, our approach, summarized in Figure 1, is based on pretraining a base encoder using self-supervised learning  to extract a proxy for the missing bias labels and to provide an initialization point for finetuning a debiased model. Pretraining a proxy for the bias variable enables us to use this network for two purposes: first, to train a group-robust network and, second, to define a validation criterion for robust model selection. The debiasing training algorithm is based on the logit adjustment paradigm . Our entirely bias-unsupervised methodology to group robustness using logit adjustment, which we call uLA, is able to compete with or outperform state-of-the-art counterparts, that otherwise utilize bias labels during model selection, in synthetic and real-world benchmarks. At the same time, it is the only method to consistently offer improvements over the ERM baseline in the sMPI3D systematic generalization task; thus effectively tackling the identified challenges about explicit or implicit use of group information.

## 2 Preliminaries

**Problem Formulation.** Consider a multi-class classification task, where \(X\) is the input variable and \(Y\), the categorical target variable with \(||=K\) classes. We have access to a dataset of observations \(\{(x_{n},y_{n})\}_{n=1}^{N}\) sampled i.i.d. from an underlying data distribution \(p_{}(X,Y)\) over \(\). The setting above may become problematic once we consider that the deployment data \(_{}\) are sampled from a different testing distribution: \(p_{} p_{}\). In other words, we assume that there are two data generating processes; one which generates development data (\(\) and \(_{}\), where \(_{}\) is used for validation) according to \(p_{}\), and one which generates deployment data (\(_{}\)) according to \(p_{}\).

In further detail, we focus on a particular transfer learning problem from \(p_{}\) to \(p_{}\), which is due to a distribution shift in attributes which participate in the generative process of the input variable \(X\). Our study considers anti-causal prediction tasks for which the target variable \(Y\) is one of the generative attributes of \(X\), and \(Z\) another (possibly unobserved) categorical generative attribute with \(||=L\) classes. \(Z\) is marginally independent to \(Y\) under \(p_{}\), but it might not be under \(p_{}\). For this reason, we say that the variables \(Y\) and \(Z\) are _spuriously correlated_ in training and validation data, and \(Z\) will also be referred to as the _bias attribute_. Under this setting, a group \((y,z)[K][L]\) is defined as a combination of target and bias attribute values, and group robustness can be formulated as a transfer learning problem from \(p_{}\) to \(p_{}\) under the following two assumptions relating the two joint distributions over \(X\), \(Y\) and \(Z\):

\[p_{}(y,z)_{\,p_{}(Y)\,p_{}(Z)}(y,z),\] (1) \[p_{}(x|y,z)=p_{}(x|y,z),\] (2)

where \(_{S}(s)=1\) if \(s S\) else \(0\), the characteristic function of a set \(S\), and \(\,p\) denotes the support set of a distribution \(p\). Relation 1 asserts that, during test time, the target and bias variables

Figure 2: **Left: Example tasks. Circled in dashed lines are samples from training split, exhibiting statistically major attribute groups. Those outside are example test samples, where all groups are equally considered. A classifier trained on a biased training set may misclassify a bias-conflicting test sample - recognizing a ‘red three’ as ‘zero’ (2a) or a ‘blue sphere’ as a ‘cone’ (2b) - or be unfair to the sensitive gender attribute when tasked to classify a facial attribute, e.g. hair color (2c). Right: An example of a systematic split. \(C\) is the number of color values per shape. ‘Croses’ represent groups used to sample the training and validation splits, while ‘circles’ are entirely out-of-distribution.**

are distributed uniformly over the product of their respective marginal supports under \(p_{}\). In other words, all combinations of observed attributes values are considered equally. This also implies that \(Y\) and \(Z\) are marginally independent under \(p_{}\)3. On the other hand, relation 2 assumes the invariance of mechanism, which is an assumption typically found in _disentangled causal process_ literature . These two assumptions underlie the group robustness literature, in that they are equivalent to the evaluation of classifiers under a popular robust performance criterion which is described below.

**Performance Criteria.** Let \((x;f)*{argmax}_{y}f(x)_{y}\) be the predictions of a scoring function \(f:^{K}\). The accuracy of \(f\) under \(p_{}\) corresponds to the _group-balanced accuracy_

\[_{Y|X}(f;p_{})=*{}_{ y,z p_{}\\ x p_{}(|y,z)}y=(x; f)=_{y,z}*{}_{x p_{}(|y,z)}y=(x;f),\] (3)

which is a frequently used performance metric in literature . Essentially, this performance criterion first separately computes the accuracy for samples \(x\) of each group \((y,z)\) and then averages them. The averaging operation per individual group accuracy directly stems from the uniformity assumption in \(p_{}\) (rel. 1) and implements a group fairness notion: we care equally about performing well in all groups. Another popular group robustness criterion is the _worst-group accuracy_ which substitutes the average accuracy over individual groups with the minimum (worst) accuracy.

As described in Section 5, there exist different approaches to improve group fairness  that depend on knowing biases for cross-validation. Here we focus on logit adjustment techniques like the one proposed by Liu et al. , which co-train two models where one corrects the biases of the other. In the following sections, we show how, by decoupling the training of the biased model from the bias-corrected model, it is possible to obtain a proxy criterion that can be used for cross-validation without knowing the bias attribute.

**Logit Adjustment** was originally developed as a technique for supervised learning under class-imbalanced  or long-tailed  data. For clarity of presentation, we will assume for the moment that we have access to bias labels. We describe sLA, a bias-supervised training process with logit adjustment. Let \(h_{}:^{||}\) be the model that we want to train for group robustness, such as a neural network parameterized by \(\) implementing a function from samples in \(\) to the unnormalized logit space of \(Y\). We then train by minimizing the average cross-entropy loss for the following _logit adjusted model_

\[p_{}(y|x,z)(h_{}(x)_{y}+_{}(y|z)).\] (4)

We estimate the conditional \(_{}(y|z)\) directly from the available training data, for example via empirical frequencies of the finite number of groups \((y,z)[K][L]\). Finally, during inference, we cancel out the contribution of the logit bias term \(_{}(y|z)\) and predict only according to the optimized neural network \(h_{^{*}}\).

Note that, since \(Y\) is a categorical variable, \(_{}(y|z)\) takes non-positive values. For this reason, we can intuitively interpret logit adjustment as a soft masking operation for outputs of \(h_{}\) which are unlikely in the training data when we have observed \(z\). In this way, we account for the dependency of \(Y\) to \(Z\) which spuriously exists in the training distribution. By fitting the cross-entropy objective under logit adjustment, the network \(h_{}\) has to model the remaining relations for \(Y\,|\,X\) that are not spurious since those are already accounted for. An expressive enough model class in Equation (4) can achieve this, assuming further that the likelihood ratio \(}(x|y,z)}{p_{}(x|z)}=}(y|x,z )}{p_{}(y|z)}\) is independent of \(z\). In appendix A, we derive sLA from first principles, bridging the gap between the well-studied application of this technique to the class-imbalance problem with its application to group robustness. Moreover, we demonstrate that sLA is a well-justified procedure by proving the following proposition.

**Proposition 2.1** (sLA optimizes the group-balanced accuracy).: _Under the assumption that the hypothesis class \(p_{}(y|x,z)\) (eq. 4) contains \(p_{}(y|x,z)\), the minimizer network \(h_{^{*}}\) of the cross-entropy loss maximizes a lower bound to the group-balanced accuracy \(_{Y|X}(h_{^{*}};p_{})\)._

**Systematic Generalization.** We expect sLA models to systematically generalize, as they optimize for the transfer learning problem defined by Equations (1) and (2). As Figure 1(d) suggests, systematic generalization is just an extreme case of such problems, for which samples from some combinationshave \(p_{}(y,z)=0\) during training, however they populate the test set with \(p_{}(y,z)>0\). As a counterexample, the setting by Ahmed et al.  does not correspond to a systematic generalization task, as we define it. In what they refer to as systematic splits of colored MNIST, all possible combinations of color and digit are exposed to the model during its training, while in our sMPI3D some color and shape combinations are only revealed during test-time. This detail makes our setting significantly more challenging, as the conditions for applying importance sampling are not met, as \(p_{}(Y,Z)p_{ }(Y,Z)\)[4, Chapter 5]. This means that a simple re-weighting of the per-sample loss with \(}(y,z)}\), in order to estimate \(_{p_{}}[l_{}(y,h_{}(x))]\), is not appropriate for the task.

**Self-Supervised Learning (SSL)** refers to a collection of methods  in unsupervised representation learning where unlabeled data provides the supervision by defining tasks where a model is asked to predict some part of the input from other parts, for example by contrasting independently transformed versions of data samples [14; 35; 15]. Methods like SimCLR , MoCo , BYOL  and Barlow Twins  provide solutions for low-data generalization, robustness, as well as transferability of learnt representations for image classification. Although self-supervised learning methods are generally more robust to distribution shifts than purely supervised methods , they can still be affected by significant shifts, for example in long-tailed data learning [5; 63]. In this work, we explore the limits and utility of SSL in group robust classification. An extended preliminary discussion about self-supervised algorithms can be found in Appendix B.

## 3 ULA: Bias-unsupervised Logit Adjustment

Here, we introduce bias-unsupervised logit adjustment (uLA), a logit correction approach which improves on the work of Liu et al.  by _removing dependency on explicit bias annotations both during training and validation_. For this reason, we create a proxy variable for the bias via the predictions of a pretrained network using SSL. The advantage of pretraining, over co-training a bias network [57; 17; 50], is two-fold: First, we can reuse the fixed bias proxy network to define a validation criterion for group robustness. This enables us to perform hyperparameter search, but also informs us about when to stop training the debiased model, which is critical for optimal performance . Second, recent literature [39; 43] has demonstrated, in the case of training with bias-supervised data, that a linear classifier on top of pretrained base models provides with substantial group-robustness improvements. Here, we initialize the debiased model from the pretrained base model and finetune it using logit adjustment. Figure 1 provides a summary of our approach and its pseudocode can be found in Appendix C. Further training details are discussed in the following.

### Bias-unsupervised Training

**Biased network: pretrain with SSL.** We start by training a _base model_, \(f_{}\), using an SSL method on the unlabeled data of the training set \(\). We decide on the hyperparameters of the SSL algorithm (learning rate, weight decay, temperature, augmentations and potentially others) by maximizing the i.i.d. validation accuracy of an online linear classifier, which probes the representations of the base model. Afterwards, we train a linear classifier \(g_{}\) on top of a frozen \(f_{}\) and against target variable labels \(Y\) using a vanilla cross-entropy loss, in order to derive a proxy for the bias variable. Finally, we retrieve the composite neural network \(h_{}=g_{} f_{}\), and use its predictions \((x;h_{})\) as a proxy for the missing bias variable observations. For the same purpose, Nam et al.  trained a bias-extracting network by employing Generalized Cross Entropy , an objective which provides robustness to noisy labels. Since the goal of the proxy network is to predict the spurious attribute, bias-conflicting or minority samples can be perceived as mislabeled data points. Here, we follow these observations and leverage the representations learnt with SSL which, when composed with a low-capacity (linear) classifier, provide with a model that is more robust to label-noise . In addition to deriving a bias proxy, we will also use \(f_{}\) as an initialization point in the parameter space for training the debiased model.

In this paper, we have chosen to use MoCoV2+  as the SSL algorithm for the image classification tasks we consider. We make this choice since contrastive learning algorithms, like the MoCo family [35; 15], offer relatively stable training since they explicitly prevent representation collapse in their loss function (see Appendix B). However, as we demonstrate in Section 4.3, our method is not restrained to the use of a particular SSL algorithm.

**Debiased network: logit adjustment.** In the absence of bias labels during training, we need a substitute for the estimate \(_{}(Y,Z)\) in eq. 4. We use the predictions \(y_{}\) of the bias proxy network \(h_{}\) to that end. The resulting joint distribution between the target variable and the biased network's predictions, \(_{}(y,y_{})\), can be thought of as a soft confusion matrix of \(h_{}\) and can be computed using the available training data with

\[_{}(y,y_{})=|}_{x^{ },y^{}}p_{}(y_{}\,|\,x^{ })(y=y^{}),\] (5)

where \(p_{}(y_{}\,|\,x)(h_{}(x)_{y_{ }}/)\) is the biased model conditional. Note that \(p_{}(y_{}\,|\,x)\) is post-hoc calibrated by a temperature hyperparameter \(\). As we rely on the biased network to approximate the spurious correlation structure described in \(p_{}(Y,Z)\), it is crucial that the predicted conditional probabilities of the biased network are calibrated correctly [31; 56; 53]. Afterwards, we are ready to begin training a debiased network with logit adjustment as in Section 2; using only the predictions of \(h_{}\) as the bias variable. The debiased network is initialized at the composition of a random linear classifier with the pretrained network \(f_{}\), and during training we finetune it while adjusting its output logits by

\[p_{}(y\,|\,x)h_{}(x)_{y}+\,_{ }y\,|\,(x;h_{}),\] (6)

where \( 0\) is a hyperparameter controlling the strength of the additive logit bias. Notice that for \(=0\) we fall back to ERM training. By tuning \(\) we can mitigate calibration errors of the debiased model , similar to what \(\) does for the bias proxy. Selected hyperparameter configurations can be found in Appendix E.

### Bias-unsupervised Validation

We re-purpose the pretrained biased classifier \(h_{}\) so that training no longer requires bias-annotated validation data for model selection. Our bias-unsupervised validation criterion calculates a balanced accuracy across pairs \((y,y_{})[K][K]\) of true labels and biased classifier predictions. In practice, we compute

\[}(f;h_{})}_{y,y _{}}}}|}_{x_{i},y_{i} S_{y,y_ {}}}y_{i}=(x_{i};f)\] (7)

\[S_{y,y_{}}\{(x_{i},y_{i})_{}\,|\,y_{i}=y(x_{i};h_{})=y_{}\},\]

where \(S_{y,y_{}}\) are partitions of \(_{}\) based on the value of predictions of \(h_{}\) on a sample \(x_{i}\) and its ground-truth target label \(y_{i}\). This corresponds to a form of group-balanced accuracy. Alternatively, we could also calculate a form of worst-group accuracy by taking the minimum across \(S_{y,y_{}}\). We find that worst-case validation is more suitable for tasks with small number of classes \(K\).

During training, we evaluate models at every epoch and we select the one that maximizes our validation score across the duration of a training trial. In addition, we use this criterion to tune hyperparameters. In particular, for each task we tune learning rate, weight decay, logit adjustment strength coefficient \(\), calibration temperature \(\) and, in addition, the number of pretraining steps for the SSL backbone - whenever it is applicable - and for the linear classification probe of the bias proxy network. Fig. 1 depicts our bias-unsupervised training and validation procedures.

## 4 Experiments

**Datasets.** The tasks we consider are all specific instances of the setup above (see Fig. 2 and Section 2). This spans group robustness challenges like colored MNIST [57; cMNIST], corrupted CIFAR10 [37; cCIFAR10] and Waterbirds, fair classification benchmarks like CelebA , and systematic generalization tasks such as the contributed sMPI3D. Details about their construction can be found in Appendix D.

**Training Setup.** For cMNIST, we train a 3-hidden layer MLP, while we use a ResNet18  for cCIFAR10 and sMPI3D, and a ResNet50 for Waterbirds and CelebA. For all datasets except Waterbirds, we pretrain the base model with the MoCoV2+  process, while training of the linear probe for the bias network and finetuning for the logit adjusted debiased network happen withAdamW  optimizer. For Waterbirds instead, we leverage a base model which was pretrained on Imagenet , following baselines in the literature for fair comparison, and we finetune it using SGD. Finally, for cMNIST, cCIFAR10 and sMP13D we use our group-balanced bias-unsupervised validation criterion, whereas for Waterbirds and CelebA the worst-group version. Further details are described in Appendix E.

**Baselines.** We compare uLA with vanilla ERM and a diverse set of group robustness techniques described in Section 5. GroupDRO  provides with a fully bias-supervised baseline, while LfF , JtT , LC  and DFA  are bias-unsupervised during training although they require bias annotations during validation to achieve robust optimal performance. We also consider two fully bias-unsupervised methods: Bardenhagen et al.  propose early stopping networks to derive a proxy for bias-unsupervised validation, and MaskTune  which provide competitive results under fully bias-unsupervised benchmarks without performing any validation procedure.

### Results on Benchmarks

In Table 1, we report the group-balanced accuracy on cMNIST and cCIFAR10, across different percentages of bias-conflicting examples in the training set. For cMNIST, we observe that our method performs overall competitively against LfF, DFA and LC, even though these baselines use bias annotations during model selection. On the other hand, for cCIFAR10, we observe a significantly improved group robust performance for 3/4 difficulty levels. The highest difference is observed at the 1.0% task, where our method outperforms GroupDRO by about 24% absolute increase in group-balance accuracy.

In Table 2, we observe worst-group accuracy results in the more challenging Waterbirds and CelebA datasets. In both cases, our approach performs again competitively among bias-unsupervised training algorithms that leverage bias information during validation, falling slightly behind LC, which is the best out of the ones considered. Notably, our approach is still the best performing fully bias-unsupervised method, outperforming the supervised learning pretraining validation scheme of Bardenhagen et al. , and performing on par with MaskTune on Waterbirds and better than it on CelebA by \(\)8% absolute worst-group accuracy.

    &  &  \\   & Train & Val & \(0.5\%\) & \(1.0\%\) & \(2.0\%\) & \(5.0\%\) & \(0.5\%\) & \(1.0\%\) & \(2.0\%\) & \(5.0\%\) \\ 
 GroupDRO\({}^{}\) & ✓ & ✓ & \(63.12\) & \(68.78\) & \(76.30\) & \(84.20\) & \(33.44\) & \(38.30\) & \(45.81\) & \(57.32\) \\ 
 LfF\({}^{*}\) & ✗ & ✓ & \(52.50_{ 3.4}\) & \(61.89_{ 4.7}\) & \(70.13_{ 2.48}\) & \(80.57_{ 3.84}\) & \(28.57_{ 1.30}\) & \(33.07_{ 0.77}\) & \(39.91_{ 0.30}\) & \(50.27_{ 1.06}\) \\
 DFA\({}^{*}\) & ✗ & ✓ & \(65.22_{ 1.4}\) & \(81.73_{ 2.34}\) & \(84.79_{ 0.9}\) & \(89.66_{ 1.09}\) & \(29.95_{ 0.7}\) & \(36.49_{ 1.78}\) & \(21.78_{ 2.12}\) & \(51.13_{ 11.29}\) \\
 LC\({}^{}\) & ✗ & ✓ & \(17.25_{ 1.37}\) & \(82.72_{ 2.11}\) & \(86.21_{ 1.09}\) & \(91.66_{ 2.94}\) & \(34.56_{ 0.97}\) & \(37.34_{ 0.49}\) & \(47.81_{ 2.09}\) & \(50.55_{ 1.90}\) \\  ERM\({}^{*}\) & ✗ & ✗ & \(35.19_{ 0.9}\) & \(52.09_{ 2.88}\) & \(65.86_{ 3.59}\) & \(82.17_{ 0.74}\) & \(23.08_{ 1.25}\) & \(25.82_{ 0.33}\) & \(30.06_{ 0.71}\) & \(39.42_{ 0.64}\) \\ uLA (ours) & ✗ & ✗ & \(}\) & \(81.80_{ 1.41}\) & \(84.79_{ 1.10}\) & \(}\) & \(34.39_{ 1.14}\) & \(}\) & \(}\) & \(}\) \\   

Table 1: Results using datasets from Chu et al.  for various % of bias-conflicting examples in the training set. We report avg. group-balanced test accuracy (%) and std. dev. over 5 seeds. \({}^{*}\)Results from Chu et al. . \({}^{}\)Results from Liu et al. .

    &  &  &  \\   & Train & Val & i.i.d. & worst group & i.i.d. & worst group \\ 
 GroupDRO\({}^{*}\) & ✓ & ✓ & \(93.5\) & \(91.4\) & \(92.9\) & \(88.9\) \\ 
 ERM & ✗ & ✓ & \(97.6\) & \(86.7\) & \(93.1\) & \(77.8\) \\
 LfF\({}^{*}\) & ✗ & ✓ & \(97.5\) & \(75.2\) & \(86.0\) & \(77.2\) \\
 JtT\({}^{*}\) & ✗ & ✓ & \(93.6\) & \(86.0\) & \(88.0\) & \(81.1\) \\
 LC & ✗ & ✓ & - & \(90.5_{ 1.1}\) & - & \(88.1_{ 0.8}\) \\  ERM & ✗ & \(79.3\) & \(72.6\) & \(95.6\) & \(47.2\) \\ Bardenhagen et al.  & ✗ & ✗ & \(97.5\) & \(78.5\) & \(88.0\) & \(78.9\) \\
 MaskTune & ✗ & ✗ & \(93.0_{ 0.7}\) & \(86.4_{ 1.9}\) & \(91.3_{ 0.1}\) & \(78.0_{ 1.2}\) \\ uLA (ours) & ✗ & ✗ & \(91.5_{ 0.7}\) & \(86.1_{ 1.5}\) & \(93.9_{ 0.2}\) & \(86.5_{ 3.7}\) \\   

Table 2: Results on Waterbirds and CelebA. We report avg. test accuracy (%) and std. dev. over 5 seeds. \({}^{*}\)Results from Liu et al. .

### Systematic Generalization

sMPI3D is our contributed task which we use to study combinatorial systematicity in classifiers. With this benchmark, we aim to study the ability of classifiers to generalize to samples generated from novel combinations of observed generative attributes values, which under \(p_{}\) have \(0\) probability. The target task is to classify the _shape_ of an object, which is spuriously correlated with its _color_. We devise 4 difficulty levels for this task which we denote by \(C\), the number of color values present in the training set per shape. In Fig. 1(d), we display a possible split between in-distribution and o.o.d. combinations of attributes for \(C=4\). Details about its construction are given in Appendix D.

**Results.** In Table 3, we further validate our approach on our contributed systematic generalization task. Under this setting, all bias-unsupervised approaches are evaluated fairly since their models are validated with exactly the same data resources; access to bias labels cannot give model selection advantage to any algorithm since there are no o.o.d. samples in the validation (just like in the training) split. Our method is the only one which consistently offers group-balanced accuracy improvements across difficulty levels, demonstrating generalization to o.o.d. samples. On the other hand, GroupDRO, vanilla ERM, JT and LC are not able to increase group-balanced accuracy over the percentage of i.i.d. samples present in the balanced test set.

### Ablation Studies

We perform a set of studies to understand better the efficacy of our approach. In Fig. 3, we ablate the choice of training paradigm and the influence of a pretrained base model using SSL to the robust performance of a trained model to the target task. The paradigms we choose are among vanilla cross-entropy minimization, uLA and sLA (bias-supervised logit adjustment - see Section 2). For this ablation, a bias-supervised validation procedure was used for comparison against the fully bias-supervised sLA baseline. We find that finetuning the pretrained base model gives the best performance across training paradigms and tasks. Second, form CelebA and cCIFAR10, vanilla cross-entropy finetuning does not offer stark performance improvement over vanilla training from scratch. Only when we apply a logit adjustment training procedure, we are able to take significant advantage of the learnt representation space. At the same time, the accuracy gaps between uLA and

    &  &  \\   & Train \& Val & \(2\) & \(3\) & \(4\) & \(5\) \\  \% i.i.d. samples & - & \(33.33\) & \(50.00\) & \(66.66\) & \(83.33\) \\  
 GroupDRO & ✓ & \(31.23 1.88\) & \(46.01 4.13\) & \(69.68 7.82\) & \(82.18 8.39\) \\  ERM & ✗ & \(31.94 1.67\) & \(47.68 0.06\) & \(71.94 7.71\) & \(83.10 8.64\) \\
 JT & ✗ & \(31.89 0.88\) & \(48.93 2.04\) & \(67.78 3.14\) & \(83.31 3.51\) \\
 LC\({}^{*}\) & ✗ & \(31.29 0.96\) & \(45.01 2.31\) & \(61.67 5.06\) & \(94.62 0.88\) \\ ULA (ours) & ✗ & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Results on sMPI3D for various numbers of \(C\) colors per shape value in the training set (see Appendix D). We report avg. group-balanced accuracy (%) and std. dev. over 5 seeds & dataset generations. \({}^{*}\)GroupMix augmentation was not used for fairness of comparison.

Figure 3: Ablations on the influence of SSL pretraining and on the use of Vanilla process, sLA, or uLA for training against the downstream task. For cCIFAR10 (1%) and sMPI3D (\(C=3\)) we report the group-balanced test accuracy, whereas for CelebA the worst-group. We report avg. accuracies and std. dev. over 5 seeds.

sLA are small which indicates that we are able to recover correctly the bias attribute with the proxy network. On the other hand, for sMPI3D the gaps between uLA and sLA are large. This shows that, in the systematic generalization case, bias extraction remains an open challenge for future work, and that improvement over baseline procedures in Table 3 is due to pretraining with SSL.

In addition, we study the impact of the choice of SSL method that we use to pretrain the backbone. For this reason, we perform an ablation experiment on cCIFAR10 (1%) by changing the pretraining strategy from MoCoV2+ to BYOL  or Barlow Twins. We find that, while BYOL performs within the error margin of the best MoCoV2+ setting, Barlow Twins underperforms. Barlow Twins seeks to match the empirical (i.i.d.) cross-correlation between features to the identity. Arguably, we expect that the cross-correlation is different under the shifted test set. In any case, uLA significantly outperforms the non-uLA baselines with any of the considered SSL methods.

Finally, we study how the choice of pretraining paradigm for the bias network influences the quality of hyperparameter search using our proposed validation criterion. In Fig. 4, we present two separate searches on the same space of hyperparameters using two different pretraining approaches. In red we see our approach of pretraining an SSL base model for the bias network, and in blue we see a baseline approach where we pretrain with purely supervised learning. We observe that SSL pretraining enables stronger correlation between the proposed bias-unsupervised group-balanced validation criterion and the corresponding test accuracy on cCIFAR10. It is more difficult to tune hyperparameters with a bias network pretrained with supervised learning, because it may fit the training set entirely. In that case, the validation criterion collapses to the in-distribution test accuracy which is not indicative of the group-balanced test accuracy. On the contrary, classification with a linear probe on top of SSL representations prevents from fitting the training set entirely, having small generalization gaps in-distribution . In this way, the validation criterion remains strongly correlated even in larger validation accuracy values, maintaining its utility in a greater range of hyperparameter configurations.

## 5 Related Work

Prior literature can be grouped according to three main strategies that attempt to improve a model's robustness to dataset bias . (i) _Resampling_ strategies increase or decrease the frequency of biased

    \\  ERM & \(25.82_{ 0.33}\) \\ GroupDRO & \(38.30\) \\  uLA w. MoCoV2+ & \(62.49_{ 0.74}\) \\ uLA w. BYOL & \(59.73_{ 2.03}\) \\ uLA w. BarlowTwins & \(50.08_{ 1.23}\) \\   

Table 4: Ablation of SSL pretraining methods for the backbone model. Default \(=1.0\) and \(=1.0\) are used. We pretrain for \(1000\) epochs, and train the linear head of the bias proxy for \(100\) epochs. We report avg. group-balanced test accuracy (%) and std. dev. over 5 seeds on cCIFAR10 (1%).

Figure 4: Validation vs test on cCIFAR10 (1%).

   Pretraining method & Pearson correlation \\  End-to-end supervised & \(0.690\) (\(0.588,0.770\)) \\ SSL + linear head & \(0.819\) (\(0.721,0.885\)) \\   

Table 5: Effect of pretraining method for a bias proxy network on results of hyperparameter search using our validation criterion. Each training trial’s hyperparameters were sampled from the same prior. Pretraining with SSL results in trials whose best validation criterion correlates better with selected model’s group-balanced test accuracy on cCIFAR10 (1%). In parenthesis, we compute 95% confidence intervals.

attributes in the input space [27; 55] or latent space [23; 74; 17]. (ii) _Loss reweighting_ methods balance class or feature importances during training [74; 21; 12; 65; 40]. Especially relevant to our work are the reweighting methods that improve robustness and generalization when training on biased datasets [61; 57; 17; 48]. (iii) _Post-hoc adaptation_ methods [54; 25; 19; 42; 41; 3; 70] correct the biases learned by already-trained models. Most relevant to our work is the logit adjustment technique proposed by Menon et al.  for long-tail learning, which leverages class frequencies to rebalance the model predictions after training or to train with a loss function which is aware of the class prior. We develop a logit adjustment technique for the problem of learning from biased data, which does not require previous knowledge about the dataset's biases.

Several solutions have emerged for the group robustness problem, especially when bias attribute data is available. For example, GroupDRO  leverages explicit bias attributes to reduce worst-case group loss. Our method, however, reduces reliance on such data, recognizing that access to them can be impractical. Bias-unsupervised methods like Learning from Failure (LFF)  and Disentangled Feature Augmentation (DFA)  reweight the loss of the unbiased model using a co-trained biased model, removing the need for bias supervision. Conversely, Just-train-twice (JTT)  reweights misclassified samples from an initial biased training, emphasizing worst-performing group data. Similarly, He et al.  exemplify the same intuition in that a second network is trained on examples that cannot be predicted already using the spurious structure. Liu et al.  proposed a bias-unsupervised logit adjustment technique (LC) also based on co-training a biased network. Utilizing domain knowledge, Clark et al.  describe a bias-supervised logit adjustment approach to debiasing Visual Question Answering (VQA) models by incorporating a bias proxy which is trained exclusively on question data. These methods only generalize to seen attribute groups during training and require bias knowledge during validation for optimal performance. We tackle these issues by employing SSL to pre-train a network, deriving a bias proxy for debiased model training and validation, as well as using it as initialization.

In order to derive bias-unsupervised solutions, literature has proposed to train reference models as proxies for the missing bias labels. Creager et al.  optimize a reference model for group assignments which maximally violate a relaxation of the Environment Invariance Criterion. Chen et al.  seek to establish conditional independence between the predictions of the proxy model and the target variable given the inferred groups. Our work follows more closely the approach of , in which a biased network is simply trained with ERM. As we demonstrate at Figure 6 of Appendix F, by utilizing a frozen backbone pretrained with SSL, our approach improves on the sensitivity to the number of training steps for the bias proxy.

Bardenhagen et al.  suggested a validation scheme dependent on early stopping of bias proxy network training. Our method, using SSL pretraining, avoids this by treating pretraining steps as tunable hyperparameters, maintaining performance of alternatives which were otherwise tuned with bias information. Chen et al.  perform experiments using a methodology dubbed as Training Environments Validation (TEV). Similar to us, TEV validates models based on inferred groups from training, however the methodology is unfortunately not well documented in the literature, making its reproducibility difficult. As we show in the ablation study of Figure 4, implementation details can make a large difference in the quality of the criterion. Finally, MaskTune eliminates spurious shortcuts by masking input data during a secondary training phase. Despite its resilience in performance without using a bias-unsupervised o.o.d. model selection criterion, the need for a reliable validation strategy for group robustness remains.

## 6 Conclusion

We explored group robust classification in synthetic and real tasks, proposing a generalization task with unseen attribute combinations. Current robust classification methods struggle in this setting, motivating our SSL-based logit adjustment approach. Importantly, we introduce **a methodology for training and validating robust models without group labels**. Empirical evaluations on five datasets show our method outperforms existing fully bias-unsupervised approaches and rivals those using bias annotations during validation. In terms of _limitations and broader impact_ of our contributions, as machine learning systems handle high-stakes applications, ensuring robustness to underrepresented samples is crucial. Our work reduces reliance on known data biases, but existing benchmarks differ from real-life scenarios with unknown biased attribute combinations. To bridge this gap, we proposed a synthetic benchmark and encourage further research on real data, revealing more system limitations.