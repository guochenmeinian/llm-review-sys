# MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts

Renchunzi Xie

Nanyang Technological University

Singapore

renchunzi.xie@ntu.edu.sg

&Ambroise Odonnat\({}^{*}\)

Huawei Noah's Ark Lab, Inria\({}^{}\)

Paris, France

ambroise.odonnat@gmail.com

&Vasilii Feofanov

Huawei Noah's Ark Lab

Paris, France

vasilii.feofanov@gmail.com

&Weijian Deng

Australian National University

Canberra, Australia

weijian.deng@anu.edu.au

&Jianfeng Zhang

Huawei Noah's Ark Lab

Shenzhen, China

zhangjianfeng3@huawei.com

&Bo An

Skywork AI

Nanyang Technological University

Singapore

boan@ntu.edu.sg

Equal contribution. Correspondence to: Bo An - boan@ntu.edu.sg. Univ. Rennes 2, CNRS, IRISA

###### Abstract

Leveraging the model's outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground-truth labels. Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method MaNo that **(1)** applies a data-dependent normalization on the logits to reduce prediction bias, and **(2)** takes the \(L_{p}\) norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty. We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts. The code is available at https://github.com/Renchunzi-Xie/MaNo.

## 1 Introduction

The deployment of machine learning models in real-world scenarios is frequently challenged by distribution shifts between the training and test data. These shifts can substantially deteriorate the model's performance during testing (Geirhos et al., 2018; Koh et al., 2021; Quionero-Candela et al., 2009) and introduce significant risks related to AI safety (Deng and Zheng, 2021; Hendrycks and Mazeika, 2022). To alleviate this issue, it is common to monitor model performance by periodically collecting the ground truth labels for a subset of the current test dataset (Lu et al., 2023). However,this approach is often resource-intensive and time-consuming, which motivates the importance of estimating the model's performance on out-of-distribution (OOD) data in an unsupervised manner, also known as _Unsupervised Accuracy Estimation_(Donmez et al., 2010).

Due to privacy constraints and computational efficiency, one of the most popular ways to estimate accuracy without labels is to rely on the model's outputs, called logits, as a source of confidence in the model's predictions (Deng et al., 2023; Garg et al., 2022; Guillory et al., 2021; Hendrycks and Gimpel, 2016). For instance, _ConfScore_(Hendrycks and Gimpel, 2016) leverages the average maximum softmax probability as the test accuracy estimator, while Deng et al. (2023) has recently proposed to estimate the accuracy via the nuclear norm of the softmax probability matrix. These approaches, however, tend to underperform on the natural shift applications while the intuition behind the use of logits remains unclear. This motivates us to ask:

**Question 1:**_What explains the correlation between logits and generalization performance?_

In Section 3, we show that logits are connected to the model's margins, _i.e._, the distances between the learned embeddings, and the decision boundaries. Inspired by the low-density separation (LDS) assumption (Chapelle and Zien, 2005; Feofanov et al., 2023) that optimal decision boundaries should lie in low-density regions, we propose MaNo, an estimation score that aggregates the margins at a dataset level by taking the \(L_{p}\)-norm of the normalized model's prediction matrix to evaluate the density around decision boundaries. Nevertheless, logit-based approaches are known to suffer from overconfidence (Odonnat et al., 2024; Wei et al., 2022), resulting in high prediction bias, especially under poorly-calibrated scenarios. This leads us to another critical question:

**Question 2:**_How to alleviate the overconfidence issues of logits-based methods?_

In Section 4, we reveal that this question is connected to the normalization of logits and show that the widely-used softmax normalization accumulates errors in the presence of prediction bias, which can lead to overconfidence and significantly degrade the performance of existing accuracy estimation methods in poorly-calibrated scenarios. To mitigate this issue, we propose a novel normalization strategy called softrun that takes into account the empirical distribution of logits and aims to find a trade-off between information completeness of ground-truth logits and error accumulation.

Summary of our contributions.**(1)** We show that logits are informative of generalization performance through the lens of the low-density separation assumption by reflecting the distances to decision boundaries. **(2)** We identify the failure of the commonly-used softmax normalization that accumulates errors under poorly calibrated because of its overconfidence, leading to biased estimation. **(3)** We propose MaNo, a training-free estimation method that quantifies the global distances to decision boundaries by taking the \(L_{p}\) norm of the logits matrix. MaNo relies on softrun, a novel normalization technique that makes a trade-off between information completeness and error accumulation and is robust to different calibration scenarios. In addition, we demonstrate its connection to the model's uncertainty. **(4)** We demonstrate the superiority of MaNo compared to \(11\) competitors with a large-scale empirical evaluation including \(12\) benchmarks across diverse distribution shifts. Results show that MaNo consistently improves over the state-of-the-art baselines, including on the challenging natural shift.

## 2 Problem Statement

Setting.Consider a classification task with input space \(^{D}\) and label space \(=\{1,,K\}\). Let \(p_{S}\) and \(p_{T}\) be the source and target distributions on \(\), respectively, with \(p_{S} p_{T}\). We parameterize a neural network \(f^{K}\) as \(f=f_{} f_{}\), where \(f_{}^{q}\) is a feature extractor and \(f_{}^{q}^{K}\) is a linear classifier with parameters \(=(_{k})_{k=1}^{K}^{q K}\). Further, we denote an input by \(\), its corresponding label by \(y\), its representation by \(=f_{}()\) and logits by \(=f()=(_{k}^{})_{k} ^{K}\). The accuracy of \(f\) on \(\) is defined as \((f,)|}_{( ,y)}1_{=y}\) with predicted labels \(\). The probability simplex is denoted by \(_{K}=\{^{K}|1_{K}^{}=1\}\).

Unsupervised accuracy estimation.Given a model \(f\) pre-trained on a training set \(_{}\) with samples drawn i.i.d. from \(p_{S}\), the goal of unsupervised accuracy estimation is to assess its generalization performance on a given unlabeled test set \(_{}=\{_{i}\}_{i=1}^{N}\) with \(N\) samples drawn i.i.d. from \(p_{T}\). More specifically, we assume (i) a source-free regime (no direct access to \(_{}\)), (ii) no access to test labels, and (iii) a distribution shift, _i.e._\(p_{S} p_{T}\). In this challenging setup, which often occurs in real-world scenarios when ground-truth labels are inaccessible at a test time, we aim to design an estimation score \((f,_{})\) that exhibits a linear correlation with the true OOD accuracy \((f,_{})\). Following the standard closed-set setting, both \(p_{T}\) and \(p_{S}\) involve the same \(K\) classes. For an extended discussion of related work on unsupervised accuracy estimation, we refer the reader to Appendix B.

## 3 What Explains the Correlation between Logits and Test Accuracy?

Although existing literature has shown the feasibility of unsupervised accuracy prediction under distribution shift by utilizing the model's logits (Deng et al., 2023; Garg et al., 2022; Guillory et al., 2021), the reason behind this empirical success remains unclear. In this section, we seek to understand when and why logits can be informative for analyzing generalization performance. Based on the derived understanding, we propose our approach, MaNo, for estimating generalization performance.

### Motivation

Logits reflect the distances to decision boundaries.We analyze logits from a linear classification perspective in the embedding space, where the decision boundary of class \(k\) is the hyperplane \(\{^{}^{q}|_{k}^{}^{ }=0\}\). In Appendix D.2, we remind that the distance from a point \(\) to hyperplane \(_{k}\) is given by \((_{k},)=|_{k}^{}|/\| _{k}\|\). As the pre-trained model is fixed and \(_{k}\) can be normalized, we derive that the logits in absolute values are proportional to the distance from the learned embeddings to the decision boundaries, _i.e._, \(|_{k}|=|_{k}^{}| d(_{k}, ), k\). This indicates that the magnitude of logits reflects how close the corresponding embedding is from each decision boundary.

Low-density separation assumption.When dealing with unlabeled data, it is required to make assumptions on the relationship between the distance to decision boundaries and generalization performance. The low-density separation assumption (LDS, Chapelle and Zien, 2005) states that optimal decision boundaries should lie in low-density regions (Figure 1) so that unlabeled margin \(|_{k}^{}|\) reflects reliable confidence in predicting \(\) to the class \(k\). The assumption is often empirically supported as the misclassified samples tend to be significantly closer to the decision boundary than the correctly classified ones (Mickisch et al., 2020). This might indicate that **the absolute values of the logits are positively correlated to its generalization performance**.

Assumptions on the prediction bias.It is important to note that the LDS assumption has been initially proposed for semi-supervised learning where labeled and unlabeled data are assumed to come from the same distribution, which is not the case in our setting. This leads to logits writing \(f()=^{*}+\) in the general case3, _i.e._, subject to a potentially non-negligible prediction bias \(=(_{k})_{k}\) with respect to the ground-truth logits \(^{*}^{K}\). The following proposition shows the impact of the prediction bias on the divergence between the true class posterior probabilities, assumed modeled as \(=(^{*})_{K}\), and the estimated ones \(=(f())_{K}\).

Figure 1: **Illustration of the LDS assumption. When the boundary passes through dense regions (a), margins have little predictive power and cannot be used without labels. On the contrary, margins are informative in sparse regions (b).**

**Proposition 3.1**.: _Let \(_{+}=(_{l}\{_{l}\}-_{k})_{k}\). Then, the KL divergence between \(\) and \(\) verifies_

\[0(||)_{+}^{T}.\]

The proposition indicates that a large approximation error of the posterior may be caused by prediction bias that has a large norm and / or bad alignment with the true probabilities. Thus, the logit-based methods assume that the magnitude of the bias is reasonably bounded while the direction of bias does not drastically harm the ranking of classes by probabilities. We elaborate on this discussion and present the proof of Proposition 3.1 in Appendix D.1.

### MaNo: Predicting Generalization Performance With Matrix Norm of Logits

We have shown a connection between the feature-to-boundary distances and generalization performance as well as the impact of the prediction bias. Based on the derived intuition, we introduce MaNo that leverages the model margins at the dataset level performing two steps: normalization and aggregation. The pseudo-code of MaNo is provided in Appendix A.

Step 1: Normalization.Given that logits can exhibit significant variations in their scale depending on the input \(\), it is crucial to normalize the logits within a standardized range to prevent outliers from exerting disproportionate influence on the estimation. A natural range stems from the fact that most deep classifiers have outputs in \(_{K}\), which amounts to applying a normalization function \(^{K}_{K}\) on top of the pre-trained neural network (Mensch et al., 2019), where \(_{K}\) refers to probability simplex. This ensures having logits entries in \(\). For each test sample \(_{i}\), we first extract its learned feature representation \(_{i}=f_{}(_{i})\). Then, logits corresponding to this representation are computed as \(_{i}=f_{}(_{i})^{K}\). The normalization procedure results in a prediction matrix \(^{N K}\) with each row \(_{i}\) containing the normalized logits of an input sample:

\[_{i}=(_{i})_{K},\] (1)

where \(\) denotes the normalization function for the logits values. It is worth noting that not all normalization methods are appropriate candidates. The selection of a suitable normalization function \(\) based on different calibration scenarios will be discussed in detail in Section 4.

Step 2: Aggregation.Once the logits are scaled, we aggregate the dataset-level information on feature-to-boundary distances by taking the entry-wise \(L_{p}\) norm of the prediction matrix \(\), which can be expressed as:

\[(f,_{})=}\|\|_ {p}=(_{i=1}^{N}_{k=1}^{K}(_{i})_{k }^{p})^{},\] (2)

As we have \(_{p}[p]{NK} (_{ij})=[p]{NK}\) (\(_{ij}\)), the scaling by \([p]{NK}\) leads to \((f,_{})\), providing a standardized metric regardless of variations in the size of the test dataset \(N\) and the number of classes \(K\). As \(p\) increases, MaNo puts greater emphasis on high-margin terms, focusing on confident classification hyperplanes. In the extreme case where \(p\), we have \(_{p}(_{ij})\). In practice, we choose \(p=4\) in all experiments and provide an ablation study on \(p\) in Appendix G.1. As the \(L_{p}\) norm is straightforward to compute, our approach is scalable and efficient compared to the current state-of-the-art method Nuclear (Deng et al., 2023) that requires performing a singular value decomposition.

### Theoretical Analysis of MaNo

In this section, we provide the theoretical support for the positive correlation between MaNo and test accuracy. More specifically, we reveal that our proposed score is connected with the uncertainty of the neural network's predictions in Theorem 3.3. Before presenting this result, we recall below the definition of Tsallis \(\)-entropies introduced in Tsallis (1988).

**Definition 3.2** (Tsallis \(\)-entropies (Tsallis, 1988)).: _Let \(>1\) and \(k>0\). The Tsallis \(\)-entropy is defined as:_

\[_{}^{}()=k(-1)^{-1}(1-\| \|_{}^{}).\]

In this work, we choose \(k=\) following Blondel et al. (2019). The Tsallis entropies generalize the Shannon entropy (limit case \( 1\)) and have been used in various applications (Blondel et al., 2019, 2020; Muzellec et al., 2017). More details can be found in Appendix C. The following theorem, whose proof is deferred to Appendix D.3, states that the estimation score obtained with MaNo is a function of the average Tsallis entropy of the normalized neural network's logits.

**Theorem 3.3** (Connection to uncertainty).: _Let \(p>1\), \(a=\) and \(b=\). Given a test set \(_{}=\{_{i}\}_{i=1}^{N}\), corresponding logits \(_{i}=f(_{i})\), a normalization function \(^{K}_{K}\) and \(p>1\), the estimation score \((f,_{})\) provided by_ MaNo _(Algorithm 1) verifies_

\[(f,_{})^{p}=-a_{i=1} ^{N}_{p}^{}((_{i}))+b.\] (3)

As \(a>0\), Theorem 3.3 implies that the estimation score provided by MaNo is negatively correlated with the average Tsallis-entropy on the test set. In particular, the less certain the model is on test data, the lower the test accuracy is and the higher the entropy term is in Eq. (3), resulting in a lower score \((f,_{})\). As the converse sense holds, MaNo provides a score positively correlated to the test accuracy. This follows the findings of Guillory et al. (2021); Wang et al. (2021) and empirically confirmed in Section 5 for various architectures, datasets, and types of shift.

## 4 How to Alleviate Overconfidence Issues of Logit-Based Methods?

The most common normalization technique of existing logit-based approaches is the softmax normalization. In this section, we show that the widely used softmax is sensitive to prediction bias, which hinders the quality of the estimation in poorly calibrated scenarios. To alleviate this issue, we propose a novel normalization strategy, softrun, which balances the information completeness and overconfidence accumulation based on calibration.

### The Failure of Softmax Normalization Under Poorly-Calibrated Scenarios

It is widely known that the softmax normalization can suffer from overconfidence issues (Odonnat et al., 2024; Wei et al., 2022b) and saturation of its outputs (Chen et al., 2017), with one entry close to one while the others are close to zero.

Figure 2: **Empirical evidence with Resnet18.****(a)** The model is well-calibrated on Office-Home and miscalibrated on PACS. (b) softrun is superior to the state-of-the-art Nuclear (Deng et al., 2023) in all scenarios while the softmax heavily fails on PACS. **(c)** Increasing the approximation order \(n\) in Eq. (4) is detrimental on PACS and beneficial on Office-Home. The optimal trade-off in all calibration scenarios is taking \(n\{2,3\}\).

Analysis.To alleviate those issues, we first notice that the softmax can be decomposed as \(()=()/_{k=1}^{K}(_{ k})=()()\), where \(_{+}^{K}_{K}\) writes \(()=/_{k=1}^{K}_{k}=/\|\|_{1}\). While \(\) has appealing property for normalization (see Proposition D.2), the exponential can accumulate prediction errors, leading to the softmax overconfidence and a biased accuracy estimation. In particular, assume that the \(k\)-th entry of the output of the neural network on a test sample \(_{i}\) writes \(_{i,k}^{*}+_{k}\), where \(_{i,k}^{*}\) are the ground-truth logits and \(_{k}\) is the prediction error. Then, the \(n^{}\)-order Taylor polynomial of the exponential writes

\[(_{i,k}^{*}+_{k}) 1+(_{i,k}^{*}+ _{k})+_{i,k}^{*}+_{k})^{2}}{2!}+...+ _{i,k}^{*}+_{k})^{n}}{n!}.\] (4)

Consequently, logit-based accuracy estimation methods using softmax are sensitive to prediction bias, leading to low-quality estimations in poorly calibrated scenarios.

Empirical evidence.We illustrate this phenomenon in Figure 2(a) on two datasets, where a pre-trained ResNet18 exhibits more pronounced calibration issues on PACS (Li et al., 2017) compared to Office Home (Venkateswara et al., 2017). Figure 2(b) shows that using MaNo with softmax normalization is appropriate on Office-Home where the model is well calibrated but not in a mis-calibrated scenario on PACS. Conversely, using MaNo with \(2^{}\)-order Taylor approximation is appropriate under miscalibration on PACS but not on Office-Home. In both cases, we see that MaNo can surpass the state-of-the-art method Nuclear (Deng et al., 2023) provided it uses the appropriate normalization \(\). Figure 2(c) illustrates the impact of truncating Eq. (4) up to the \(n\)-th order. We conclude that a trade-off is needed between _information completeness on true logits and error accumulation_ depending on the type of calibration scenario. Specifically, when the model is poorly calibrated on a given dataset (_i.e._, \(_{k}\) large in absolute value), the normalization should focus on avoiding error accumulation, and when the model is well calibrated (_i.e._, \(_{k}\) small in absolute value), the normalization should focus on information completeness.

### Softrun: The Proposed Normalization Strategy

The above analysis shows that different calibration scenarios emphasize different information during normalization. Therefore, we propose a normalization strategy called softrun that normalizes the model outputs based on the calibration scenario. Given logits \(_{i}^{K}\) and reusing the function \(\) previously introduced, it takes the general form:

\[(_{i})=( v)(_{i})=_{i}) }{_{k=1}^{K}v(_{i})_{k}}_{K}.\] (5)

where \(v^{K}_{+}^{K}\) is designed to avoid error accumulation under poorly-calibrated scenarios by truncating the exponential (Taylor \(n=2\) in Eq. (4)) and using complete logits information under well-calibrated scenarios (softmax). As in practice, the calibration of the model on test data is unknown, softrun employs a simple yet effective strategy reminiscent of pseudo-labeling (Lee, 2013; Sohn et al., 2020). More specifically, given a test dataset \(_{}=\{_{i}\}_{i=1}^{K}\) and corresponding logits \(_{i}=f(_{i})\), a criterion \((_{})\) is computed at the dataset level and the normalized logits are defined as1

\[v(_{i})=1+_{i}+_{i}^{2}}{2}, &(_{})\\ (_{i}),&(_{})>.\] (6)

We define \((_{})=-_{j=1}^{N}_{k=1}^{K} (_{i})_{k}}{_{j=1}^{N}(_{i})_{j})})\), which is equal, up to a constant, to the average KL divergence between the uniform distribution and the predicted softmax probabilities. It follows from Tian et al. (2021) that showed that this KL divergence was small when the uncertainty of the model was high and large for confident models. Hence, when the model is uncertain, _i.e._, \((_{})\), we truncate the exponential to reduce error accumulation, and when the model is certain, _i.e._, \((_{})>\), complete information is used with the exact exponential (and we recover the softmax). Thus, softrun is designed to treat the problem with an additional level of complexity often overlooked by previous methods. While this comes at the cost of introducing the hyperparameter \(\), we fix \(=5\) across all our experiments. This, along with the design of softrun, is justified both theoretically in Appendix E and experimentally in Appendix G.3.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

### Additional Experiments

In this section, we discuss the results obtained with additional architectures, the ablation studies we conducted to validate our implementation choices, and the generalization capabilities of MaNo.

Beyond ResNets.To demonstrate the efficiency and versatility of MaNo, we conduct experiments on recent models such as Vision Transformers (Dosovitskiy, 2020, ViT) and ConvNeXt (Liu et al., 2022). We compare MaNo to its best competitors on \(6\) datasets for \(3\) distribution shifts in Figure 5. The full results are gathered in Table 5 of Appendix F. We note that _ConfScore_ is particularly strong with ConvNexts while MaNo works the best with ResNets and ViT. Again, we observe that MaNo is the best method overall.

Ablation study.To motivate our choices of implementation, we provide in Appendix G ablation studies on the \(L_{p}\) norm and the Taylor order \(n\) as well as a sensitive analysis on the calibration threshold \(\).

Generalization capabilities of MaNo.To verify the generalization capabilities of MaNo, we utilize designed scores calculated from ImageNet-C and their corresponding accuracy to fit a linear regression model. This model is then used to predict the test accuracy on ImageNet-V2-\(\), which is generated using the \(10\) new corruptions provided by (Mintun et al., 2021) on ImageNet-V2 (Recht et al., 2019). These new corruptions are perceptually dissimilar from those in ImageNet-C, including warps, blurs, color distortions, noise additions, and obscuring effects. Figure 6 shows that ConfScore and Dispersion give two distinct trends, while Nuclear exhibits some deviations for ImageNet-V2-\(\). In comparison, our MaNo exhibits a consistent prediction pattern for both ImageNet-C and ImageNet-V2-\(\), aligning well with the linear regression model trained on ImageNet-C. Additionally,experimental results on ImageNet-C and ImageNet-\(\), generated from the validation set of ImageNet, are provided in Appendix G.5, further demonstrating the superiority of MaNo.

### Discussion

In this section, we discuss how MaNo can be applied in practice, the benefit of combining softrun with other estimation baselines, and the limitations of our approach.

Real-world applications.In Appendix H, we discuss how MaNo can be used in real-world applications. In particular, our additional results with the Mean Absolute Error (MAE) metric confirm the superiority of MaNo.

Can softmax enhance other logit-based methods?To study this, we conducted an ablation study by equipping softrun with _Nuclear_(Deng et al., 2023), _ConfScore_(Hendrycks and Gimpel, 2016), and our MaNo. In Table 4, we observe that softrun significantly enhances the estimation performance \(R^{2}\) of Nuclear. For example, Nuclear is improved from \(0.692\) to \(0.826\) on poorly-calibrated Office-Home.

Limitations.Despite its soundness and strong empirical performance, we acknowledge that our method has potential areas for improvement. One of these is the dependence on \(\) of the selection criterion in Eq. (6). We elaborate on this discussion in Appendix E.3. In future work, we will explore a smoother way to automatically select the optimal normalization function without requiring hyperparameters. Additionally, if multiple validation sets are provided, as in (Deng et al., 2021; Deng and Zheng, 2021), we could select \(\) based on those sets.

## 6 Conclusion

In this paper, we introduce MaNo, a simple yet effective training-free method to estimate test accuracy in an unsupervised manner using the Matrix Norm of neural network predictions on test data. Our approach is inspired by the LDS assumption that optimal decision boundaries should lie in low-density regions. To mitigate the negative impact of different distribution shifts on estimation performance, we first demonstrate the failure of softmax normalization under poor calibration, due to the accumulation of overconfident errors. We then propose a normalization strategy based on Taylor polynomial approximation, balancing logits information and error accumulation. Extensive experiments show that MaNo consistently outperforms previous methods across various distribution shifts. This work highlights that logits imply the feature-to-boundary distance and considers the impact of calibration on estimation performance. We hope our insights inspire future research to explore the relationship between model outputs and generalization.

    &  &  &  \\   & w/o & w/ & w/o & w/ & w/o & w/ \\  PACS & **0.594** & 0.574 & 0.541 & **0.827** & 0.609 & **0.851** \\  Office-Home & 0.795 & **0.829** & **0.929** & 0.926 & 0.692 & **0.826** \\   

Table 4: Impact of softrun on other logit-based methods. softrun significantly boosts the performance of Nuclear. The metric used is \(R^{2}\).

Figure 6: Comparison of generalization capability across four methods. Each subplot displays a linear regression model fitted on ImageNet-C, which is used to predict the accuracy on ImageNet-V2-\(\). The mean absolute error (MAE) is reported. All experiments are conducted using ResNet18.