# Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing

Siyi Chen\({}^{1*}\) Huijie Zhang\({}^{1*}\) Minzhe Guo\({}^{1}\) Yifu Lu\({}^{1}\) Peng Wang\({}^{1}\) Qing Qu\({}^{1}\)

\({}^{1}\)University of Michigan

{siyich,huijiezh,vincegma,yifulu,pengwa,qingqu}@umich.edu

###### Abstract

Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free **LO**w-rank **CO**ntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the project website.1

## 1 Introduction

Recently, diffusion models have emerged as a powerful new family of deep generative models with remarkable performance in many applications such as image generation across various domains , audio synthesis , solving inverse problem , and video generation . For example, recent advances in AI-based image generation, revolutionized by diffusion models such as Dalle-2 , Imagen , and stable diffusion , have taken the world of "AI Art generation", enabling the generation of images directly from descriptive text inputs. These models corrupt images by adding noise through multiple steps of forward process and then generate samples by progressive denoising through multiple steps of the reverse generative process.

Although modern diffusion models are capable of generating photorealistic images from text prompts, manipulating the generated content by diffusion models in practice has remaining challenges. Unlike generative adversarial networks , the understanding of semantic spaces in diffusion models is still limited. Thus, achieving disentangled and localized control over content generation by direct manipulation of the semantic spaces remains a difficult task for diffusion models. Although effective, some existing editing methods in diffusion models often demand additional training procedures and are limited to global control of content generation . Some methods are training-free or localized but are still based upon heuristics, lacking clear mathematical interpretations, or for text-supervised editing only . Others provide analysis in diffusion models , but also have difficulty in local edits such as hair color.

In this study, we address the above problem by studying the low-rank semantic subspaces in diffusion models and proposing the LOw-rank COntrollable edit (LOCO Edit) approach. LOCO is the first local editing method that is single-step, training-free, requiring no text supervision, and having other intriguing properties (see Figure 1 for an illustration). Our method is highly intuitive and theoretically grounded, originating from a simple while intriguing observation in the learned _posterior mean predictor_ (PMP) in diffusion models: for a large portion of denoising time steps,

_The PMP is a locally linear mapping between the noise image and the estimated clean image, and the singular vectors of its Jacobian reside within low-dimensional subspaces._

The empirical evidence in Figure 2 consistently shows that this phenomenon occurs when training diffusion models using different network architectures on a range of real-world image datasets. Theoretically, we validated this observation by assuming a mixture of low-rank Gaussian distributions for the data. We then prove the local linearity of the PMP, the low-rank nature of its Jacobian, and that the singular vectors of the Jacobian span the low-dimensional subspaces.

By utilizing the linearity of the PMP, we can edit within the singular vector subspace of its Jacobian to achieve linear control of the image content with no label or text supervision. The editing direction can be efficiently computed using the generalized power method (GPM) . Furthermore, we can manipulate specific regions of interest in the image along a disentangled direction through efficient nullspace projection, taking advantage of the low-rank properties of the Jacobian.

Benefits of LOCO Edit.Compared to existing editing methods (e.g., ) based on diffusion models, the proposed LOCO Edit offers several benefits that we highlight below:

* **Precise, single-step, training-free, and unsupervised editing.** LOCO enables precise _localized_ editing (Figure 0(a)) in a single timestep without any training. Further, it requires no text supervision based on CLIP , thus integrating no intrinsic biases or flaws from CLIP . LOCO is applicable to various diffusion models and datasets (Figure 5).
* **Linear, transferable, and composable editing directions.** The identified editing direction is linear, meaning that changes along this direction produce proportional changes in a semantic feature in the image space (Figure 0(d)). These editing directions are homogeneous and can be transferred across various images and noise levels (Figure 0(b)). Moreover, combining disentangled editing directions leads to simultaneous semantic changes in the respective region, while maintaining consistency in other areas (Figure 0(c)).
* **An intuitive and theoretically grounded approach.** Unlike previous works, by leveraging the local linearity of the PMP and the low-rankness of its Jacobian, our method is highly interpretable. The identified properties are well supported by both our empirical observation (Figure 2) and theoretical justifications in Section 4.

Moreover, LOCO Edit is generalizable to T-LOCO Edit for T2I diffusion models including DeepFloyd IF , Stable Diffusion , and Latent Consistency Models , with or without text supervision (Figure 4). A more detailed discussion on the relationship with prior arts can be found in Appendix B.

Figure 1: **LOCO Edit. (a) The proposed method can perform precise localized editing in the region of interest. The editing direction is (b) homogeneous, (c) composable, and (d) linear.**

Notations.Throughout the paper, we use \(_{t}^{d}\) to denote the noise-corrupted image space at the time-step \(t\). In particular, \(_{0}\) denotes the clean image space with distribution \(p_{}()\), and \(_{0}_{0}\) denote an image. \(_{0,t}\) denote the posterior mean space at time-step \(t(0,1]\). Here, \(^{d-1}\) denotes a unit hypersphere in \(^{d}\), and \((d,r):=\{^{d r}^{}= {I}_{r}\}\) denotes the Stiefel manifold. \(}()\) denotes the numerical rank of \(\). \(_{_{0} p_{}()}[_{0}|_{t}]\) denotes the posterior mean and is written as \([_{0}|_{t}]\). \(()\) denotes the span of the columns of \(\). \(()\) denotes the set of solutions to \(=0\). \(_{\,}()\) denotes the projection of \(\) onto \(()\).

## 2 Preliminaries on Diffusion Models

In this section, we start by reviewing the basics of diffusion models [1; 2; 39], followed by several key techniques that will be used in our approach, such as Denoising Diffusion Implicit Models (DDIM)  and its inversion , T2I diffusion model, and classifier-free guidance .

Basics of Diffusion Models.In general, diffusion models consist of two processes:

* _The forward diffusion process._ The forward process progressively perturbs the original data \(_{0}\) to a noisy sample \(_{t}\) for \(t\) with the Gaussian noise. As in , this can be characterized by a conditional Gaussian distribution \(p_{t}(_{t}|_{0})=(_{t};}_{0 },(1-_{t})_{d})\). Particularly, parameters \(\{_{t}\}_{t=0}^{1}\) sastify: (_i_) \(_{0}=1\), and thus \(p_{0}=p_{}\), and (_ii_) \(_{1}=0\), and thus \(p_{1}=(,_{d})\).
* _The reverse sampling process._ To generate a new sample, previous works [1; 3; 42; 43] have proposed various methods to approximate the reverse process of diffusion models. Typically, these methods involve estimating the noise \(_{t}\) and removing the estimated noise from \(_{t}\) recursively to obtain an estimate of \(_{0}\). Specifically, the sampling step from \(_{t}\) to \(_{t- t}\) with a small \( t>0\) can be described as: \[_{t- t}=}(_{t}-}_{}(_{t},t)}{}} )+}_{}(_{t},t),\] (1) where \(_{}(_{t},t)\) is parameterized by a neural network and trained to predict the noise at time \(t\).

Denoiser and Posterior Mean Predictor (PMP).According to , the denoiser \(_{}(_{t},t)\) is optimized by solving the following problem:

\[_{}():=_{t,_{t} p _{t}(_{t}|_{0}),(,)} [\|_{}(_{t},t)-\|_{2}^{2} ],\]

where \(\) denotes the network parameters of the denoiser. Once \(_{}\) is well trained, recent studies [44; 45] show that the posterior mean \([_{0}|_{t}]\), i.e., predicted clean image at time \(t\), can be estimated as follows:

\[}_{0,t}=_{,t}(_{t};t) _{t}-}_{}(_{t},t)}{}},\] (2)

Here, \(_{,t}(_{t};t)\) denotes the _posterior mean predictor_ (PMP) [45; 44], and \(}_{0,t}_{0,t}\) denotes the estimated posterior mean output from PMP given \(_{t}\) and \(t\) as the input. For simplicity, we denote \(_{,t}(_{t};t)\) as \(_{,t}(_{t})\).

DDIM and DDIM Inversion.Given a noisy sample \(_{t}\) at time \(t\), DDIM  can generate clean images by multiple denoising steps. Given a clean sample \(_{0}\), DDIM inversion  can generate a noisy \(_{t}\) at time \(t\) by adding multiple steps of noise following the reversed trajectory of DDIM. DDIM inversion has been widely in image editing methods [40; 46; 29; 35; 47; 26] to obtain \(_{t}\) given the original \(_{0}\) and then performing editing starting from \(_{t}\). In our work, after getting \(_{t}\) given \(_{0}\) via DDIM inversion, we edit \(_{t}\) to \(_{t}^{}\) only at the single time step \(t\) with the help of PMP, and then utilize DDIM to generate the edited image \(_{0}^{}\).

For ease of exposition, for any \(t_{1}\) and \(t_{2}\) with \(t_{2}>t_{1}\), we denote DDIM operator and its inversion as \(_{t_{1}}=(_{t_{2}},t_{1})_{t _{2}}=(_{t_{1}},t_{2})\).

Text-to-image (T2I) Diffusion Models & Classifier-Free Guidance.So far, our discussion has only focused on unconditional diffusion models. Moreover, our approach can be generalized from unconditional diffusion models to T2I diffusion models [38; 4; 48; 19], where the latter enables controllable image generation \(_{0}\) guided by a text prompt \(c\). In more detail, when training T2I diffusion models, we optimize a conditional denoising function \(_{}(_{t},t,c)\). For sampling, we employ a technique called _classifier-free guidance_, which substitutes the unconditional denoiser \(}(_{t},t)\) in Equation (1) with its conditional counterpart \(_{}}(_{t},t,c)\) that can be described as follows:

\[_{}}(_{t},t,c)=}(_{t}, t,)+(}(_{t},t,c)-}(_{t},t,)).\] (3)

Here, \(\) denotes the empty prompt and \(>0\) denotes the strength for the classifier-free guidance.

## 3 Exploring Linearity & Low-Dimensionality for Image Editing

In this section, we formally introduce the identified low-rank subspace in diffusion models and the proposed LOCO Edit method with the underlying intuitions. In Section 3.1, we present the benign properties in PMP that our method utilizes. Followed by this, in Section 3.3 we provide a detailed description of our method.

### Local Linearity and Intrinsic Low-Dimensionality in PMP

First, let us delve into the key intuitions behind the proposed LOCO Edit method, which lie in the benign properties of the PMP \(f_{,t}(_{t})\). At one given timestep \(t\), let us consider the first-order Taylor expansion of \(,t}}(_{t}+)\) at the point \(_{t}\):

\[}(_{t};)\ :=\ ,t}}(_{t})+}(_{t}),}\] (4)

where \(^{d-1}\) is a perturbation direction with unit length, \(\) is the perturbation strength, and \(}(_{t})=_{_{t}},t}}(_{t})\) is the Jacobian of \(,t}}(_{t})\). Interestingly, we discovered that within a certain range of noise levels, the learned PMP \(,t}}\) exhibits local linearity, and the singular subspace of its Jacobian \(}\) is low rank. Notably, these properties are universal across various network architectures (e.g., UNet and Transformers) and datasets.

We measure the low-rankness with rank ratio and the local linearity with norm ratio and cosine similarity. Specifically, (_i_) _rank ratio_ is the ratio of \(}(}(_{t}))\) and the ambient dimension \(d\); (_ii_) _norm ratio_ is the ratio of \(\|,t}}(_{t}+)\|_{2}\) and \(\|}(_{t};)\|_{2}\); (_iii_) _cosine similarity_ is between \(,t}}(_{t}+)\) and \(}(_{t};)\). The detailed experiment settings are provided in Appendix D.1, and results are illustrated in Figure 2, from which we observe:

* **Low-rankness of the Jacobian \(}(_{t})\).** As shown in Figure 2(a), the _rank ratio_ for \(t\)_consistently_ displays a U-shaped pattern across various network architectures and datasets: (_i_) it is close to \(1\) near either the pure noise \(t=1\) or the clean image \(t=0\), (_ii_) \(}(_{t})\) is low-rank (i.e., rank ratio less than \(10^{-1}\)) for all diffusion models within the range \(t[0.2,0.7]\), (_iii_) it achieves the lowest value around mid-to-late timestep, slightly differs depending on architecture and dataset.
* **Local linearity of the PMP \(,t}}(_{t})\).** Moreover, the mapping \(,t}}(_{t})\) exhibits strong linearity across a large portion of the timesteps; see Figure 2(b) and Figure 10. Specifically, in Figure 2(b), we evaluate the linearity of \(,t}}(_{t})\) at \(t=0.7\) where the rank ratio is close to the lowest value. We can see that \(,t}}(_{t}+)}( _{t};)\) even when \(=40\), which is consistently true among different architectures trained on different datasets.

Figure 2: **Low-rankness of the Jacobian \(}(_{t})\) and Local linearity of the PMP \(,t}}(_{t})\).** We evaluated DDPM (U-Net ) on CIFAR-10 dataset , U-ViT  (Transformer) on CelebA , ImageNet  datasets and DeepFloy IF  trained on LAION-5B  dataset. (a) The rank ratio of \(}(_{t})\) against timestep \(t\). (b) The norm ratio (Top) and cosine similarity (Bottom) between \(}(_{t}+)\) and \(}(_{t};)\) against step size \(\) at timestep \(t=0.7\).

In addition to comprehensive experimental studies, we will also demonstrate in Section 4 that both properties can be theoretically justified.

### Key Intuitions for Our Image Editing Method

The two benign properties offer valuable insights for image editing with precise control. Here, we first present the high-level intuitions behind our method, with further details postponed to Section 3.3. Specifically, for any given time-step \(t\), let us denote the compact singular value decomposition (SVD) of the Jacobian \(_{,t}(_{t})\) as

\[_{,t}(_{t})\ =\ ^{}\ =\ _{i=1}^{r} _{i}_{i}_{i}^{},\] (5)

where \(r\) is the rank of \(_{,t}(_{t})\), \(=[_{1}_{r}](d,r)\) and \(=[_{1}_{r}](d,r)\) denote the left and right singular vectors, and \(=(_{1},,_{r})\) denote the singular values. We write \(_{,t}(_{t})=_{,t}\) in short for a specific \(_{t}\), and denote \((_{,t}^{})=()\) and \((_{,t})=\{\ |\ _{,t}=0\}\).

* **Local linearity of PMP for one-step, training-free, and supervision-free editing.** Given the PMP \(_{,t}(_{t})\) is locally linear at the \(t\)-th timestep, if we perturb \(_{t}\) by \(=_{i}\), using one right singular vector \(_{i}\) of \(_{,t}(_{t})\) as an example editing direction, then by orthogonality \[_{,t}(_{t}+_{i})\ \ _{,t}(_{t})+_{,t}(_{t})_ {i}\ =\ _{,t}(_{t})+_{i}_{i}\ =\ }_{0,t}+_{i}_{i}.\] (6) This implies we can achieve _one-step editing_ along the semantic direction \(_{i}\). Notably, the method is training-free and supervision-free since \(_{i}\) can be simply found via the SVD of \(_{,t}(_{t})\).
* **Local linearity of PMP for linear, homogeneous, and composable image editing.** (_i_) First, the editing direction \(=_{i}\) is _linear_, where any linear \(\) change along \(_{i}\) results in a linear change \(_{i}=_{i}\) along \(_{i}\) for the edited image. (_ii_) Second, the editing direction \(=_{i}\) is _homogeneous_ due to its independence of \(}_{0,t}\), where it could be applied on any images from the same data distribution and results in the same semantic editing. (_iii_) Third, editing directions are _composable_. Any linearly combined editing direction \(=_{i}_{i}_{i}(_{,t}^{})\) is a valid editing direction which would result in a composable change \(_{i}_{i}_{i}\) in the edited image. On the contrary, \((_{,t})\) results in no editing since \(_{,t}(_{t}+)_{,t} (_{t})\).
* **Low-rankness of Jacobian for localized and efficient editing.**\(_{,t}(_{t})\) is for the entire predicted clean image, thus \(_{,t}(_{t})\) finds editing directions in the entire image. Denote \(}_{,t}\) the Jacobian only for a certain region of interest (ROI), and \(}_{,t}\) the Jacobian for regions outside ROI. Similarly, \((}_{,t}^{})\) can edit mainly regions within the ROI, and \((}_{,t}^{})\) contain directions that do not edit regions outside of ROI. Further projection of \(\) onto \((}_{,t}^{})\) can result in a more localized editing direction for ROI. To perform such nullspace projection, computing the full SVD can be very expensive. But we can highly reduce the computation by the low-rank estimation of Jacobians with rank \(r^{} d\). The estimation is efficient yet effective with \(t[0.5,0.7]\) when the rank of the Jacobian achieves the lowest value.

### Low-rank Controllable Image Editing Method with Nullspace Projection

In this subsection, we provide a detailed introduction to LOCO Edit, expanding on the discussion in Section 3.1. We first introduce the supervision-free LOCO Edit, where we further enable localized image editing through nullspace projection with masks. Second, we generalize to T-LOCO Edit for T2I diffusion models w/wo text-supervision to define the semantic editing directions.

LOCO Edit.We first introduce the general pipeline of LOCO Edit. As illustrated in Figure 3, given an original image \(_{0}\), we first use \(_{t}=(_{0},t)\) to generate a noisy image \(_{t}\). In particular, we choose \(t[0.5,0.7]\) so that the PMP \(_{,t}(_{t})\) is locally linear and its Jacobian \(_{,t}(_{t})\) is close to its lowest rank. From Section 3.1, we know that we can edit the image by changing \(_{t}^{}=_{t}+_{p}\), where \(_{p}\) is the identified editing direction. After editing \(_{t}\) to \(_{t}^{}\), we use \(_{0}^{}=(_{t}^{},0)\) to generate the edited image.

In many practical applications, we often need to edit only specific _local_ regions of an image while leaving the rest unchanged. As discussed in Section 3.2, we can achieve this task by finding a precise local editing direction with localized Jacobians and nullspace projection. Overall, the complete method is in Algorithm 1. We describe the key details as follows.

* **Finding localized Jacobians via masking.** To enable local editing, we use a mask \(\) (i.e., an index set of pixels) to select the region of interest,2 with \(_{}()\) denoting the projection onto the index set \(\). For picking a local editing direction, we calculate the Jacobian of \(_{,t}(_{t})\) restricted to the region of interest, \(}_{,t}=_{_{t}}P_{}(_{,t}(_{t}))=}}}^{}\), and select the localized editing direction \(\) from the top-\(r\) singular vectors of \(}\) (e.g., \(=}[:,k]}_{,t}^{}\) for some index \(k[r]\)). In practice, a top-\(r\) rank estimation for \(}\) is calculated through the generalized power method (GPM) Algorithm 2 with \(r=5\) to improve efficiency. * **Better semantic disentanglement via nullspace projection.** However, the projection \(_{}()\) introduces extra _nonlinearity_ into the mapping \(P_{}(_{,t}(_{t}))\), causing the identified direction to have semantic entanglements with the area \(^{C}\) outside of the mask. Here, \(^{C}\) denotes the complimentary set of \(\). To address this issue, we can use the nullspace projection method . Specifically, given \(}_{,t}=_{_{t}}P_{^{C}}(_{,t}(_{t}))=}}}^{}\), nullspace projection projects \(\) onto \((}_{,t}^{})\). The projection can be computed as \(_{p}=_{(_{,t})}()=(-}}^{})\) so that the modified \(_{p}\) does not change the image in \(^{C}\). In practice, we calculate a top-\(r^{}\) rank estimation for \(}\) through the generalized power method (GPM) Algorithm 2 with \(r^{}=5\).

T-LOCO Edit.The unsupervised edit method can be seamlessly applied to T2I diffusion models with classifier-free guidance (3) (Algorithm 3). Besides, we can further enable text-supervised image editing with an editing prompt (Algorithm 4). See results in Figure 4(a). This is useful because the additional text prompt allows us to enforce a specified editing direction that _cannot_ be found easily in the semantic subspace of the vanilla Jacobian \(_{,t}\). As illustrated in Figure 4(b), this includes adding glasses or changing the curly hair of a human face. For simplicity, we introduce the key ideas of text-supervised T-LOCO Edit based upon DeepFloyd IF . Similar procedures are also generalized to Stable Diffusion and Latent Consistency Models with an additional decoding step . We discuss the key intuition below, see Appendix E.2 and Appendix E.3 for method details.

Figure 3: **Illustration of the unsupervised LOCO Edit for unconditional diffusion models.** Given an image \(_{0}\), we perform DDIM-Inv until time \(t\) to get \(_{t}\), and estimate \(}_{,t}\) from \(_{t}\). After masking to get the region of interest (ROI) \(}_{0,t}\) and its counterparts \(}_{0,t}\), we find the edit direction \(_{p}\) via SVD and nullspace projection based on their Jacobians (Algorithm 1). By denoising \(_{t}+_{p}\), an image \(_{0}^{}\) with localized editing is generated. _In this paper, the variables and notions related to ROI, nullspace, and final direction are respectively highlighted by green, blue, and red colors.

We first introduce some notations. Let \(c_{o}\) denote the original prompt, and \(c_{e}\) denote the editing prompt. For example, in Figure 4(b), \(c_{o}\) can be "portrait of a man", while \(c_{e}\) can be "portrait of a man with glasses". Correspondingly, given the noisy image \(_{t}\) for the clean image \(_{0}\) generated with \(c_{o}\), let \(^{o}_{,t}(_{t})\) and \(^{o}_{,t}(_{t})\) be the estimated posterior mean and its Jacobian conditioned on the original prompt \(c_{o}\), and let \(^{e}_{,t}(_{t})\) and \(^{o}_{,t}(_{t})\) be the estimated posterior mean and its Jacobian conditioned on both the editing prompt \(c_{e}\) and \(c_{o}\).

According to the classifier-free guidance (3), we can estimate the _difference_ of estimated posterior means caused by the editing prompt as \(=^{e}_{,t}(_{t})-^{o}_{,t}(_{t})\), and then set \(=^{e}_{,t}(_{t})^{}\) as an initial estimator of the editing direction.3 Based upon this, to enable localized editing, similar to the unsupervised case, we can apply masks \(\) to select ROI in \(\) and calculate localized Jacobian to get \(\). After that, similarly, we can perform nullspace projection of \(\) for better disentanglement to get the final editing direction \(_{p}\).

## 4 Justification of Local Linearity, Low-rankness, & Semantic Direction

In this section, we provide theoretical justification for the benign properties in Section 3.1. First, we assume that the image distribution \(p_{}\) follows _mixture of low-rank Gaussians_ defined as follows.

**Assumption 1**.: _The data \(_{0}^{d}\) generated distribution \(p_{}\) lies on a union of \(K\) subspaces. The basis of each subspace \(\{_{k}(d,r_{k})\}_{k=1}^{K}\) are orthogonal to each other with \(_{i}^{}_{j}=\) for all \(1 i j K\), and the subspace dimension \(r_{k}\) is much smaller than the ambient dimension \(d\). Moreover, for each \(k[K]\), \(_{0}\) follows degenerated Gaussian with \((_{0}=_{k}_{k})=1/K,_{k} (,_{r_{k}})\). Without loss of generality, suppose \(_{t}\) is from the \(h\)-th class, that is \(_{t}=}_{0}+}\) where \(_{0}(_{h})\), i.e. \(_{0}=_{h}_{h}\). Both \(||_{0}||_{2},||||_{2}\) is bounded._

Our data assumption is motivated by the intrinsic low-dimensionality of real-world image dataset .Additionally, Wang et al.  demonstrated that images generated by an analytical score function derived from a mixture of Gaussians distribution exhibit conceptual similarities to those produced by practically trained diffusion models. Given that \(_{,t}(_{t})\) is an estimator of the posterior mean \([_{0}|_{t}]\), we show that the posterior mean \([_{0}|_{t}]\) can analytically derived as follows.

**Lemma 1**.: _Under Assumption 1, for \(t(0,1]\), the posterior mean is_

\[[_{0}|_{t}]=}^{K}(}{2(1-_{t})}\|_{k}^{ }_{t}\|^{2})_{k}_{k}^{}_{t}}{_{k=1}^ {K}(}{2(1-_{t})}\|_{k}^{} _{t}\|^{2})}.\] (7)

Lemma 1 shows that the posterior mean \([_{0}|_{t}]\) could be viewed as a convex combination of \(_{k}_{k}^{}_{t}\), i.e. \(_{t}\) projected onto each subspace \(_{k}\). This lemma leads to the following theorem:

Figure 4: **T-LOCO Edit on T2I diffusion models.** (a) Unsupervised editing direction is found only via the given mask without editing prompt. (b) Text-supervised editing direction is found with both a mask and an editing prompt such as ”with glasses”. Experiment details can be found in Appendix G.3.

**Theorem 1**.: _Based upon Assumption 1, we can show the following three properties for the posterior mean \([_{0}|_{t}]\):_

* _The Jacobian of posterior mean satisfies_ \((_{_{t}}[_{0}|_{t}] ) r:=_{k=1}^{K}r_{k}\) _for all_ \(t(0,1]\)_._
* _The posterior mean_ \([_{0}|_{t}]\) _has local linearity such that_ \[\|[_{0}|_{t}+]- [_{0}|_{t}]-_{_{t}} [_{0}|_{t}]\|=}{(1-_{t})}(),\] (8) _where_ \(^{d-1}\) _and_ \(\) _is the step size._
* \(_{_{t}}[_{0}|_{t}]\) _is symmetric and the full SVD of_ \(_{_{t}}[_{0}|_{t}]\) _could be written as_ \(_{_{t}}[_{0}|_{t}]=_{t}_{ t}_{t}^{}\)_, where_ \(_{t}=[_{t,1},_{t,2},,_{t,d}] (d,d)\)_,_ \(_{t}=(_{t,1},,_{t,r},,0)\) _with_ \(_{t,1}_{t,r} 0\) _and_ \(_{t}=[_{t,1},_{t,2},,_{t,d}](d,d)\)_. Let_ \(_{t,1}:=[_{t,1},_{t,2},,_{t,r}]\) _and_ \(:=[_{1},_{2},,_{K}]\)_. It holds that_ \(_{t 1}\|(_{d}-_{t,1}_{t,1}^{}) \|_{F}=0\)_._

The proof is deferred to Appendix F. Admittedly, there are gap between our theory and practice, such as the approximation error between \(_{,t}(_{t})\) and \([_{0}|_{t}]\), assumptions about the data distribution, and the high rankness of \(_{,t}\) for \(t<0.2\) and \(t>0.9\) in Figure 2. Nonetheless, Theorem 1 largely supports our empirical observation in Section 3 that we discuss below:

* **Low-rankness of the Jacobian.** The first property in Theorem 1 demonstrates that the rank of \(_{_{t}}[_{0}|_{t}]\) is always no greater than the intrinsic dimension of the data distribution. Given that the intrinsic dimension of the real data distribution is usually much lower than the ambient dimension , the rank of \(_{,t}\) on the real dataset should also be low. The results align with our empirical observations in Figure 2 when \(t[0.2,0.7]\).
* **Linearity of the posterior mean.** The second property in Theorem 1 shows that the linear approximation error is within the order of \(_{t}/(1-_{t})()\). This implies that when \(t\) approaches 1, \(_{t}/(1-_{t})\) becomes small, resulting in a small approximation error even for large \(\). Empirically, Figure 2 shows that the linear approximation error of \(_{,t}(_{t})\) is small when \(t=0.7\) and \(=40\), whereas Figure 10 shows a much larger error for \(t=0.0\) under the same \(\). These observations align well with our theory.
* **Low-dimensional semantic subspace.** The third property in Theorem 1 shows that, when \(t\) is close to 1, left singular vectors associated with the top-\(r\) singular values form the basis of the image distribution. Since the editing direction consists of basis, the edited image remains within the image distribution. This explains why \(_{i}\) found in Equation (6) is a semantic direction for image editing.

## 5 Experiments

In this section, we perform extensive experiments to demonstrate the effectiveness and efficiency of LOCO Edit. We first showcase LOCO Edit has strong _localized_ editing ability across a variety of datasets in Section 5.1. Moreover, we conduct comprehensive comparisons with other methods to show the superiority of the LOCO Edit method in Section 5.2. Besides, we provide ablation studies on multiple components in our method in Appendix C.1, and analyze the editing directions in Appendix C.2, with extra experimental details postponed to Appendix G.

### Demonstration on Localized Editing and Other Benign Properties

First, we demonstrate benign properties of LOCO Edit in Algorithm 1 on a variety of datasets, including LSUN-Church , Flower , AFHQ , CelebA-HQ , and FFHQ .

As shown in Figure 5 and Figure 0(a), our method enables editing specific localized regions such as eye size/focus, hair curvature, length/amount, and architecture, while preserving the consistency of other regions. Besides the ability of precise local editing, Figure 1 demonstrates the benign properties of the identified editing directions and verify our analysis in Section 4:

* **Linearity.** As shown Figure 0(d), the semantic editing can be strengthened through larger editing scales and can be flipped by negating the scale.
* **Homogeneity and transferability.** As shown Figure 0(b), the discovered editing direction can be transferred across samples and timesteps in \(_{t}\).

* **Composability.** As shown Figure 1(c), the identified disentangled editing directions in the low-rank subspace allow direct composition without influencing each other.

### Comprehensive Comparison with Other Image Editing Methods

We compare LOCO Edit with several notable and recent image editing techniques, including Asyrp , Pullback , NoiseCLR , and BlendedDifusion . We also compare with an unexplored method using the Jacobians \(}{ x_{t}}\) to find the editing direction, named as \(}{ x_{t}}\).

Metrics.We evaluate our method using the below metrics and summarize the results in Table 1. Besides the image generation quality, we also compared other attributes such as the local edit ability, efficiency, the requirement for supervision, and theoretical justifications.

* _Local Edit Success Rate_ evaluates whether the editing successfully changes the target semantics and preserves unrelated regions by human evaluators.
* _LPIPS_ and _SSIM_ measure the consistency between edited and original images.
* _Transfer Success Rate_ measures whether the editing transferred to other images successfully changes the target semantics and preserves unrelated regions by human evaluators.
* _Learning time_ to measure the time required to identify the edit directions.
* _Transfer Edit Time_ to measure the time required to transfer the editing to other images directly.
* _#Images for Learning_ measures the number of images used to find the editing directions.
* _One-step Edit_, _No Additional Supervision_, _Theoretically Grounded_, and _Localized Edit_ are attributes of the editing methods, where each of them measures a specific property for the method.

Moreover, we visualize the editing results on non-cherry-picked images in Figure 6. The detailed evaluation settings are provided in Appendix G.2.

Benefits of Our Method.Based upon the qualitative and quantitative comparisons, our method shows several clear advantages that we summarize as follows.

* **Superior local edit ability with one-step edit.** Table 1 shows LOCO Edit achieves the best Local Edit Success Rate. Such local edit ability only requires one-step edit at a specific time \(t\). For LPIPS and SSIM, our method performs better than most methods but worse than BlendedDiffusion. However, BlendedDiffusion sometimes fails the edit within the masks (as visualized in Figure 6, rows 1, 3, 4, and 5). Other methods find semantic direction more globally, leading to worse performance in Local Edit Success Rate, LPIPS, and SSIM for localized edits.

Figure 5: **Benchmarking LOCO Edit across various datasets. For each group of three images, in the center is the original image, and on the left and right are edited images along the negative and the positive directions accordingly.**

* **Transferability and efficiency.** First, LOCO Edit requires less learning time than most of the other methods and requires learning only for a single time step with a single image. Moreover, LOCO Edit is highly transferable, having the highest Transfer Success Rate in Table A. In contrast, BlendedDiffusion cannot transfer and requires optimization for each individual image. NoiseCLR has the second-best yet lower transfer success rate, while other methods exhibit worse transferability.
* **Theoretically-grounded and supervision-free.** LOCO Edit is theoretically grounded. Besides, it is supervision-free, thus integrating no biases from other modules such as CLIP .  shows CLIP sometimes can't capture detailed semantics such as color. We can observe failures in capturing detailed semantics for methods that utilize CLIP guidance such as BlendedDiffusion and Asyrp in Figure 6, where there are no edits or wrong edits.

## 6 Conclusion

We proposed a new low-rank controllable image editing method, LOCO Edit, which enables precise, one-step, localized editing using diffusion models. Our approach stems from the discovery of the locally linear posterior mean estimator in diffusion models and the identification of a low-dimensional semantic subspace in its Jacobian, theoretically verified under certain data assumptions. The identified editing directions possess several beneficial properties, such as linearity, homogeneity, and composability. Additionally, our method is versatile across different datasets and models and is applicable to text-supervised editing in T2I diffusion models. Through various experiments, we demonstrate the superiority of our method compared to existing approaches.

   Method Name & Pullback & \(_{t}/_{t}\) & NoiseCLR & Asyrp & BlendedDiffusion & **LOCO (Ours)** \\  Local Edit Success Rate\(\) & 0.32 & 0.37 & 0.32 & 0.47 & **0.55** & **0.80** \\ LPIPS\(\) & 0.16 & 0.13 & 0.14 & 0.22 & **0.03** & **0.08** \\ SSIM\(\) & 0.60 & 0.66 & 0.68 & 0.68 & **0.94** & **0.71** \\ Transfer Success Rate\(\) & 0.14 & 0.24 & **0.66** & 0.58 & Can’t Transfer & **0.91** \\ Transfer Edit Time\(\) & 4s & **2s** & 5s & 3s & Can’t Transfer & **2s** \\ \#Images for Learning & **1** & **1** & 100 & 100 & **1** & **1** \\ Learning Time\(\) & **88** & **44s** & 1 day & 475s & 120s & 79s \\ One-step Edit? & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ \\ No Additional Supervision? & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ \\ Theoretically Grounded? & ✗ & ✗ & ✗ & ✗ & ✗ & ✓ \\ Localized Edit? & ✗ & ✗ & ✗ & ✗ & ✓ & ✓ \\   

Table 1: **Comparisons with existing methods. Our LOCO Edit excels in localized editing, transferability and efficiency, with other intriguing properties such as one-step edit, supervision-free, and theoretically grounded.**

Figure 6: **Compare local edit ability with other works on non-cherry-picked images. LOCO has consistent and accurate local edit ability, while other methods have wrong, global, or no edits.**