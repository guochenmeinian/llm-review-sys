# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

predicted pretrained labels of two images from downstream tasks: a 'Dog' image from CIFAR10  and an 'Osteosperm' image from Flowers102 . For the 'Dog' image, multiple pretrained labels like 'Chihuahua', 'Basenji'-_subclasses_ of dogs-receive high logits. Similarly, for the 'Osteosperm' image, pretrained labels such as 'Sea Urchin', 'Daisy', which _share similar features_, also score high. Despite these connections, the one-to-one LM retains only the label with the highest logit, suggesting the _probabilities of other related labels are ignored_. Figure 0(b) shows the frequency distribution of the predicted pretrained labels and the ground-truth downstream labels of downstream samples, with the diagonal representing the results derived from one-to-one LM. The 'Automobile' class from CIFAR10, for example, can no longer be paired with the optimal pretrained label 'Moving Van', which has already been greedily mapped to the label 'Truck', implying _suboptimal label assignments_.

The above observation motivates us to go beyond these binary mappings. In Section 3, we replace the one-to-one LM function with a probabilistic LM matrix. Each matrix element is a real number that quantifies the relationship between a pretrained label and a downstream label, updated iteratively during VR optimization. This allows predictions for each downstream sample to consider diverse contributions from all pretrained labels, enabling a flexible many-to-many mapping strategy.

Specifically, we present _Bayesian-guided label mapping_ (BLM) in Section 4, which assigns values to elements in the probabilistic LM matrix based on Bayesian conditional probabilities, derived from the joint distribution of the predicted pretrained labels on downstream tasks and the ground-truth downstream labels. We further extend BLM to BLM+, which aggregates _top-\(K\) predicted probabilities_ instead of using a single predicted label when estimating the joint distribution, accounting for uncertainty in the predictions. We also provide a theoretical analysis that justifies the potential of probabilistic many-to-many LM to outperform deterministic one-to-one LM.

To show the effectiveness of BLM, experiments are conducted on 12 widely used datasets, with BLM and BLM+ being applied to different input VR methods-padding and watermarking-on pretrained ResNet and ResNeXt (see Section 5). The ablation study and parameter analysis are also included, along with visualization results and discussions of why VR is effective. BLM and BLM+ are also applied to vision-language models (see Appendix L) to demonstrate their general applicability.

In summary, both theoretical analysis and empirical findings (Tables 1-2) provide compelling evidence that BLM and BLM+, grounded in Bayesian principles, facilitate VR to leverage pretrained knowledge for diverse downstream tasks. Beyond performance improvement, BLM and BLM+ offer insights into understanding the effectiveness of VR (Figures 3-4): revealing the relations between pretrained and downstream label spaces may guide future studies into more interpretable VR methods.

## 2 Related Works

**Model Reprogramming.** Among cutting-edge transfer learning methods (see Appendix B), model reprogramming introduces an efficient learning framework for adapting models pretrained on large

Figure 1: Drawbacks of one-to-one LM from the perspectives of (a) individual images and (b) the entire dataset. An ImageNet-pretrained classifier is reused in downstream tasks. In (a), images ‘Dog’ and ‘Osteosperm’ from downstream tasks are mapped into only one pretrained label, respectively, ignoring other probabilities. In (b), the distribution of [predicted pretrained label \(y^{}\), ground-truth downstream label \(y^{}\)] pairs reveals the existence of suboptimal solutions, where ‘Automobile’ cannot be paired with the optimal pretrained label ‘Moving Van’, which has already been mapped to ‘Truck’.

scale data to downstream tasks constrained by limited resources . By changing the input or output interfaces (i.e., input or output space) purposefully, while preserving the integrity of the pretrained model, knowledge can be reused on new tasks, sidestepping exhaustive finetuning of the model.

Many recent studies focus on repurposing diverse pretrained models for downstream tasks, including pretrained vision models  such as ResNet  and ViT , language models  such as BERT , acoustic  and graph models . Such repurposing encompasses several types: cross-modal (e.g., from voice to time-series , or vision to text ), different tasks within the same modality (e.g., from image classification to out-of-distribution detection ), and different domains within the same task (e.g., from ImageNet to medical images ).

**Prompting and Input VR.** Prompting incorporates meticulously designed prompts (additional parameters) into pretrained models with specific architectures to utilize pretrained models in downstream tasks. Leveraging ViT, VPT  integrates prompts alongside image embeddings, while EEVPT  further enhances VPT by embedding parameters within self-attention layers. TransHP  additionally learns prompt tokens to encode coarse image categories. In vision-language models such as CLIP , besides text-prompting methods such as CoOP  and CoCoOP , models like MaPLe  also learn layer-specific mapping functions that bridge vision and text.

Slightly different from prompt tuning, input VR offers a model-agnostic approach by introducing trainable noise to images in the input space before feeding those images into pretrained models. This process does not impact the visual effect of the images. Two prevalent techniques are padding-based VR and watermarking-based VR. Padding-based models  preserve the integrity of images while introducing trainable noise patterns to the outer frames around images, whereas watermarking-based models  train noise patterns that overlay the images.

**Output Mapping for VR.** Because pretrained labels and downstream labels are often different, relying solely on input VR may be insufficient for downstream tasks. To bridge this gap, output mapping methods are introduced to facilitate alignment between different label spaces. Mainstream approaches include deep learning-based and statistical inference-based (i.e., gradient-free) LM methods. Deep learning-based methods insert a learnable fully connected layer to connect pretrained and downstream labels . However, for tasks with large label spaces, the additional model layers would result in extra training costs, potentially canceling the efficiency advantages of VR.

As for gradient-free LM methods, _random label mapping_ (RLM)  establishes mappings between an equal number of randomly selected pretrained labels and downstream labels, masking out other unused ones. _Frequent label mapping_ (FLM)  selects optimal one-to-one mappings using a greedy approach based on the number of pairs between pretrained and downstream labels. _Iterative label mapping_ (ILM)  extends FLM by updating mappings at each epoch, refining the output label mapping as input VR patterns evolve. As depicted in Figure 1, these one-to-one mappings _overlook potential probabilities_ and lead to _suboptimal solutions_. We propose BLM to address these issues.

## 3 Problem Formulation

**Problem Setup**. Consider a pretrained task with input and output variables \(X^{}\) and \(Y^{}\), jointly defined over \(^{}^{}\), where \(^{}^{d_{}}\) has the input dimensionality \(d_{}\) and \(^{}=\{1,,k_{}\}\). We have a pretrained classifier \(f_{}:^{}^{k_{}}\) producing a logits vector \(f_{}(x^{})^{k_{}}\) for each \(x^{}^{}\). For a downstream task with input and output variables \(X^{}\) and \(Y^{}\) defined over \(^{}^{}\), where \(^{}^{d_{}}\) has the input dimensionality \(d_{}\) and \(^{}=\{1,,k_{}\}\), VR seeks to adapt \(f_{}\) to the downstream task without modifying its parameters. To achieve this, VR introduces two functions: 1) input VR function \(f_{}(|):^{}^{ }\) with learnable parameters \(\) that converts downstream inputs for compatibility with \(f_{}\); and 2) output LM function \(f_{}^{}():^{k_{}}^ {k_{}}\) that aligns the output logits of \(f_{}\) with the downstream label space by a transformation \(\). Concretely, given a training dataset \(^{}=\{(x_{i}^{},y_{i}^{})\}_{i=1}^{n}\) with \(n\) training samples drawn from \(^{}^{}\) for the downstream task, the training objective of VR can be formulated as:

\[_{}_{i=1}^{n}(y_{i}^{},(f_{ }^{} f_{} f_{})(x_{i}^{ };)),\] (1)

where \(\) is a loss function, and \(f_{}^{} f_{} f_{}\) denotes the composition of input VR, pretrained model and output LM. In this study, we focus on _gradient-free_ LM, where \(f_{}^{}\) does not introduce additional trainable parameters but strategically leverages \(f_{}\) and \(f_{}\) to determine \(\).

**Modeling Existing LM.** As mentioned, \(f^{}_{ out}\) serves to find a mapping between each \(y^{ S}^{ S}\) and \(y^{ T}^{ T}\). This can be achieved by constructing an output label transformation \(\) such that for each downstream sample \(x_{i}^{ T}\), its label \(_{i}^{ T}\) is predicted by \((_{i}^{ T})\), with:

\[_{i}^{ T}_{i}^{ 1}\\ \\ _{i}^{ k_{T}}=f(x_{i}^{ T})^{}= f(x_{i}^{ T})_{1}&&f(x_{i}^{ T})_{k_{ S}} _{1,1}&&_{1,k_{ T}}\\ &&\\ _{k_{ S},1}&&_{k_{ S},k_{ T}},\] (2)

where \(f(x_{i}^{ T})\) is shorthand for \((f_{ pre} f_{ in})(x_{i}^{ T};)\). \(\) can be updated iteratively  with input VR. A deterministic one-to-one relation between \(^{ S}\) and \(^{ T}\) implies only a _single_ "correct" \(y^{ S}^{ S}\) exists for each \(y^{ T}^{ T}\). Formally, \(\) in Eq. (2) is a binary matrix, where just a _single_ element \(_{j,k}\) is set to 1 in each column of \(\) (i.e., \(\{0,1\}^{k_{ S} k_{ T}}\) satisfying \(_{j=1}^{k_{ S}}_{j,:}=1\)).

**Our Probabilistic LM.** Considering aforementioned drawbacks of one-to-one mappings, we propose a probabilistic LM for VR, assigning real values to all elements in \(\) (i.e., \(^{k_{ S} k_{ T}}\) satisfying \(_{j=1}^{k_{ S}}_{j,:}=1\)). Each element \(_{y^{ S},y^{ T}}\) quantifies the relationship between \(y^{ S}^{ S}\) and \(y^{ T}^{ T}\). This acknowledges contributions from all pretrained labels for the prediction of downstream samples. The flexible many-to-many LM implies the inherent complexity in label correspondence. In Section 4, we investigate how to assign values to our probabilistic LM based on Bayes' theorem.

## 4 Bayesian-guided Probabilistic Label Mapping (BLM)

### Method Demonstration

**Interpreting \(p(Y^{ T}|X^{ T})\)**. The objective of VR is to maximize \(p(Y^{ T}|X^{ T})\) defined over the downstream task space. By using the law of total probability, we can express \(p(Y^{ T}|X^{ T})\) as

\[p(Y^{ T}|X^{ T})=_{y^{ S}^{ S}}p(Y^{ S }=y^{ S}|X^{ T})\,p(Y^{ T}|Y^{ S}=y^{ S},X^{ T}).\] (3)

Mirroring the structure of Eq. (2), Eq. (3) enables us to estimate \(p(Y^{ T}|X^{ T})\) using training data \(^{ T}\),2

\[(Y^{ T}|X^{ T})=_{i=1}^{n}(_{y^{ S} ^{ S}}=y^{ S}|X^{ T}=x_{i}^{ T })}_{:(f_{ pre} f_{ in})(x_{i}^{ T},) }}=y_{i}^{ T}|Y^{ S}=y^{ S},X^{ T}=x_{i}^{  T})}_{:f^{}_{ out}^{ S}}}),\] (4)

where \(=y^{ S}|X^{ T})\), in addition to summing up Eq. (6) for \(y^{ T}^{ T}\), we add Laplace smoothing coefficient \(\) to ensure the denominator of Eq. (5) being non-zero, with \(k_{ S}\) being the size of \(^{ S}\):

\[_{ BLM}(Y^{ S}=y^{ S}|X^{ T})= ^{ T}}_{i=1}^{n}\{y^{ T}_{i}=y^{ T}\} \{^{ S}_{i}=y^{ S}\}+}{n+k_{ S}}= ^{n}\{^{ S}_{i}=y^{ S}\}+}{n+k_{  S}}.\] (7)

Substituting Eq. (7) and Eq. (6) back to Eq. (5) yields the estimation of \(_{y^{ S},y^{ T}}\) to be \(_{ BLM}(Y^{ T}=y^{ T}|Y^{ S}=y^{ S},X^{ T})\). After column-wise sum normalization of \(_{y^{ S},y^{ T}}\) to satisfy \(_{j=1}^{k_{ S}}_{j,}=1\) (as formulated in Section 3), we obtain the final probabilistic LM, denoted as \(_{ BLM}\).

**BLM+.** Recall that BLM estimates \(p(Y^{ T}=y^{ T},Y^{ S}=y^{ S}|X^{ T})\) by frequency-counting based on a single _most likely_ predicted label. However, this strategy disregards other high-ranking predictions that could offer valuable information. Thus, we introduce BLM+, an extension of BLM that considers top-\(K\)_predicted probabilities_ of the pretrained model for the estimation of \(p(Y^{ T}=y^{ T},Y^{ S}=y^{ S}|X^{ T})\). Rather than relying solely on the tally, BLM+ aggregates _probabilities_ for samples where \(y^{ S}\) ranks among the top-\(K\) predictions. In this way, BLM+ acknowledges the uncertainty in \(f(x^{ T}_{i})\) and exploits other potential predictions, providing more robust estimations.

Let \(^{ S}_{K,i}\{y^{}|_{y_{1},,y_{K}}f(x^{  T}_{i})_{y^{}}\}\) denote the set of the top-\(K\) predicted pretrained labels for input \(x^{ T}_{i}\), and \((y^{ S}|x^{ T}_{i})( f)(x^{ T}_{i}) _{y^{ S}}\) denote the predicted probability for any \(y^{ S}^{ S}\) given \(x^{ T}_{i}\). Then, within the BLM+ strategy, the joint density is approximated3 as:

\[_{ BLM_{+}}(Y^{ T}=y^{ T},Y^{ S}=y^{ S}|X^{ T})= {_{i=1}^{n}\{y^{ T}_{i}=y^{ T}\}(y^{ S}|x^{  T}_{i})\{y^{ S}^{ S}_{K,i}\}}{n}.\] (8)

Similar to BLM, with the Laplace smoothing coefficient being \(\) and the size of \(^{ S}\) being \(k_{ S}\), \(p(Y^{ S}=y^{ S}|X^{ T})\) can be expressed by applying BLM+ as:

\[_{ BLM_{+}}(Y^{ S}=y^{ S}|X^{ T})=^{n} (y^{ S}|x^{ T}_{i})\{y^{ S}^{ S }_{K,i}\}+}{n+k^{ S}}.\] (9)

Combining Eq. (9) and Eq. (8) with Eq. (5), and going through all \(y^{ T}^{ T}\) and \(y^{ S}^{ S}\), we obtain the full BLM+ estimation as \(_{ BLM_{+}}\) after column-wise sum normalization of \(_{y^{ S},y^{ T}}\), similar to BLM. In practice, we set \(K= k_{ T}\), with ratio \(\) being a hyper-parameter that decides \(K\) based on the size of downstream label space \(k_{ T}\).

**Pipeline and Learning Strategy.** The learning of BLM and BLM+ allows for seamless integration into existing VR pipelines. It is model-agnostic (e.g., pretrained ResNet or ResNeXt) and compatible with all input VR methods (e.g., watermarking or padding). Figure 2 illustrates the learning strategy in detail. Besides, the learning pipeline of BLM is shown in Algorithm 1, while that of BLM+ is shown in Algorithm 2. The completed pseudocode for all LM methods (RLM, FLM, ILM, BLM, BLM+) and a more detailed discussion of involved matrix operations are in Appendix D.

Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels. Then, the true labels (of \(y^{ T}\)) and predicted labels (of \(y^{ S}\)) are used to estimate \(_{ BLM}\) or \(_{ BLM_{+}}\). Next, using \(_{ BLM}\) or \(_{ BLM_{+}}\) that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.

The iterative process of learning \(_{},_{+}\) comprises these four steps: 1) Input images, with VR patterns, are fed into the fixed pretrained model to obtain output logits and predicted pretrained labels. 2) BLM and BLM+ replace previous LM (e.g., RLM, FLM or ILM) to estimate \(\). 3) The initial logits are reweighted using \(_{}\) or \(_{+}\), yielding refined predictions for downstream labels. 4) Loss functions (e.g., cross-entropy) and backpropagation are employed to update the input VR.

### Theoretical Analysis

Furthermore, we include a justification of why probabilistic many-to-many LM (e.g., BLM and BLM+) should be favored over deterministic one-to-one LM (e.g., RLM, FLM and ILM). Define the label spaces \(^{}=\{0,1\}\) and \(^{}=\{0,1\}\) as binary sets4. Consider the set of potential LM functions \(_{}=\{f_{}:^{} ^{}\}\), including each function \(f_{}(y^{})\{y^{},1-y^{}\}\). For any \(f_{}_{}\), the expected accuracy of \(f_{}\) regarding the entire downstream label space is defined as5:

\[(f_{})=_{y^{}^{ }}[_{y^{}^{}}p(y^{}) p(f_{}(y^{})=y^{}|y^{} )],\] (10)

where \(p(y^{})\) is the marginal distribution of the pretrained labels and \(p(f_{}(y^{})=y^{}|y^{})\) is the conditional probability that \(f_{}\) correctly predicts a downstream label \(y^{}\) from a pretrained label \(y^{}\). Let \(f_{}\) and \(f_{}\) denote the probabilistic LM (Definition E.1) and deterministic LM (Definition E.2), respectively. We finally prove that \((f_{})(f_{})\) (Corollary E.5) in Appendix E, which further verifies the effectiveness of our methods in the view of theoretical understanding.

```
1:Input: Pretrained label space \(^{}\) with \(k_{}\) labels, downstream label space \(^{}\) with \(k_{}\) labels, downstream training set \(\{(x_{i}^{},y_{i}^{})\}_{i=1}^{n}\), pretrained model \(f_{}()\), iterations \(E\), learning rate \(a\), \(\), \(K\)
2:Output: Probabilistic LM \(_{+}^{k_{} k_{}}\)
3: Initialize \(_{+}\{0\}^{k_{} k_{}}\), set \(\)
4:for\(e=1...E\)do
5:# Step 1: Get Pretrained Model Outputs
6:\(f(x_{i}^{};)=f_{}(f_{}(x_{i}^{};))\) for \(i=1...n\)
7:# Step 2: Compute (or Update) the LM Matrix
8:\(y_{i}^{}_{y_{i} y^{}}f(x_{i}^ {};)_{y^{}}\) for \(i=1...n\)
9:if\(e\)=1then Compute \(_{+}\) using Eq. (5,8,9)
10:else Update \(_{+}\) using Eq. (5,8,9)
11:# Step 3: Predict Downstream Labels
12:\(_{i}^{}_{y_{i}}f_{}^{ }(f(x_{i}^{};))_{y}\) for \(i=1...n\)
13:# Step 4: Update VR Patterns
14:\(-a_{i=1}^{n}(y_{i}^{},f_{ }^{}(f(x_{i}^{};)))\)
15:endfor
16:return\(_{+}\) ```

**Algorithm 2** Training Pipeline of BLM+

## 5 Experiments

**Tasks and Baselines.** Following ILM , we employ ResNet-18  pretrained on ImageNet-1K  and ResNeXt pretrained on Instagram  to test the performance of VR. The results are evaluated on twelve downstream datasets: Flowers102 , DTD , UCF101 , Food101 , GTSRB , EuroSAT , OxfordPets , StanfordCars , SUN397 , CIFAR10/100  and SVHN . Previous gradient-free LM methods RLM , FLM  and ILM  are used as the baselines. The results of deep learning-based LM will also be included for reference, where LM is treated as a single-layer linear neural network connected to the output of the pretrained model for training alongside VR. More dataset and implementation details are in Appendix F. Regarding hyper-parameters of BLM, \(\) is set as 1, and the top-\(K\) ratio \(\) is 0.15 (analyzed in Appendix G).

**Results for Padding-based VR.** Padding-based input VR adds trainable noise to the outer frames of centered images. Table 1 shows the performance of BLM and BLM+ applied with padding-based input VR. BLM and BLM+ yield the highest accuracy across all datasets except for SVHN. On ResNet-18, compared to the SOTA (i.e., ILM), BLM achieves an average improvement of 4.7% across the 12 datasets, whereas BLM+ achieves a 6.1% enhancement. On ResNeXt-101, BLM and BLM+ achieve accuracy improvements of 3.2% and 3.8% on average, respectively. The elevation in accuracy is particularly pronounced in tasks with a higher number of classes (e.g., UCF101, CIFAR100). On SVHN, ILM performs slightly better, which could be attributed to the minimal inter-class variation and the smaller number of classes (which is 10) in SVHN, resulting in similar mapping values for different downstream labels and thus reducing our method's advantage (discussed in Appendix H). However, compared to current gradient-free LM methods, the deep learning-based LM may still have an advantage in the performance of downstream tasks due to the learning capacity of the linear layer neural network. Our proposed BLM and BLM+ aim to bridge the gap between gradient-free LM and deep learning-based LM. Additionally, BLM and BLM+ have been observed to possess greater interpretability (see Appendix I for more experiments) and fewer parameters (see Appendix J for details) compared to deep learning-based LM.

**Results for Watermarking-based VR.** BLM and BLM+ can be applied to different input VR methods. For the watermarking-based VR method, which overlays trainable noise patterns on resized images, the results of BLM and BLM+ with ResNet-18 as the pretrained model are shown in Table 2. Since ILM is the best-performing baseline, we only include its results here for comparison. Our BLM and BLM+ methods again outperform ILM, achieving an average gain in accuracy of 6.1% and 7.5%, respectively. Therefore, in the case of watermarking-based VR, BLM and BLM+ also close the gap between current gradient-free and deep learning-based LM. Results in Tabel 2 underscore the applicability of our output LM methods with different input VR.

**Results for Vision-Language Models.** The application of our BLM and BLM+ on vision-language models (i.e., CLIP), along with the performance, and visualization results are dis

    &  &  \\   &  &  &  &  \\  Methods & RLM & FLM & ILM & BLM+ & - & FLM & ILM & BLM & BLM+ & - \\  Flowers102 & 11.0\({}_{}\) & 20.0\({}_{}\) & 27.9\({}_{}\) & 44.4\({}_{}\) & **50.1\({}_{}\)** & 76.7\({}_{}\) & 22.5\({}_{}\) & 27.9\({}_{}\) & **31.5\({}_{}\)** & 30.1\({}_{}\) & 85.2\({}_{}\) \\ DTD & 16.3\({}_{}\) & 32.4\({}_{}\) & 35.3\({}_{}\) & 42.0\({}_{}\) & **43.9\({}_{}\)** & 49.1\({}_{}\) & 40.3\({}_{}\) & 41.4\({}_{}\) & 47.8\({}_{}\) & **49.4\({}_{}\)** & 64.0\({}_{}\) \\ UCF101 & 6.6\({}_{}\) & 18.9\({}_{}\) & 23.9\({}_{}\) & 30.9\({}_{}\) & **32.0\({}_{}\)** & 46.0\({}_{}\) & 41.9\({}_{}\) & 43.1\({}_{}\) & 48.3\({}_{}\) & **50.1\({}_{}\)** & 68.3\({}_{}\) \\ Food101 & 3.8\({}_{}\) & 12.8\({}_{}\) & 14.8\({}_{}\) & 22.3\({}_{}\) & **25.1\({}_{}\)** & 34.1\({}_{}\) & 20.5\({}_{}\) & 23.0\({}_{}\) & **96.2\({}_{}\)** & **31.4\({}_{}\)** & 58.7\({}_{}\) \\ GTSRB & 46.1\({}_{}\) & 45.5\({}_{}\) & 52.0\({}_{}\) & **54.8\({}_{}\)** & **54.3\({}_{}\)** & 53.4\({}_{}\) & 56.2\({}_{}\) & 59.9\({}_{}\) & 62.9\({}_{}\) & **63.0\({}_{}\)** & 74.4\({}_{}\) \\ EuroSAT & 82.4\({}_{}\) & 83.8\({}_{}\) & 85.2\({}_{}\) & **86.7\({}_{}\)** & **86.7\({}_{}\)** & 92.4\({}_{}\) & 87.8\({}_{}\) & 86.2\({}_{}\) & 87.6\({}_{}\) & **88.3\({}_{}\)** & 93.2\({}_{}\) \\ OxfordPets & 9.3\({}_{}\) & 62.9\({}_{}\) & 65.4\({}_{}\) & 69.8\({}_{}\) & **70.6\({}_{}\)** & 73.0\({}_{}\) & 76.8\({}_{}\) & 82.8\({}_{}\) & 82.4\({}_{}\) & **8

[MISSING_PAGE_FAIL:8]

labels in downstream tasks, taking ResNet-18 pretrained on ImageNet-1K as an example. Each column of \(\) computed using BLM or BLM+ is a vector with length \(k_{}=1000\), representing the weights assigned to the 1,000 outputs-one for each \(y^{}\)-of the pretrained model corresponding to a downstream label \(y^{}\). The top-weighted labels (i.e., \(y^{}\) where \(_{y^{},y^{}}\) is larger) for 'Edamame' correspond to organisms such as snakes and artichokes, which share similarities in color and shape. Similarly, the predominant labels associated with 'Fibrous' from the texture dataset include rough-textured items like 'Hay' and 'Komondor'. 'Dog' encompasses various sub-breed canines. These findings suggest that BLM and BLM+ establish an optimal probabilistic LM between label spaces, and handle similarity or inclusion relationship, addressing the drawbacks in Figure 1.

**Discussion of Why VR Is Effective.** From a visualization perspective, Figure 4 shows the top-weighted pretrained labels and input VR patterns \(\) at different iteration stages using BLM+. The training loss for each iteration and changes in \(\), measured by the Euclidean norm, are also plotted. During the update of \(\) and \(\), the pretrained labels with top \(_{y^{},y^{}}\) for \(y^{}\) being 'Marigold' transition gradually from dissimilar labels such as 'Ref' and 'Teddy' to 'Cauliflower' and 'Pineapple' which share more similarities in color, shape and texture. Meanwhile, the training loss diminishes gradually, and \(\) converges, demonstrating the effectiveness of VR and BLM+.

**Impact of Label Space Sizes \(k_{}\).** Figure 5 shows the relationship between different sizes of the downstream label space and the accuracy improvement achieved by BLM and BLM+. Tasks with larger label spaces report more pronounced performance improvements. While simpler tasks with smaller label spaces might not fully showcase the power of our approach, the strength of BLM and BLM+ lies in unraveling the complex many-to-many relationship that often arises in tasks with more numerous classes. In such scenarios, our probabilistic LM methods demonstrate their full potential.

**Impact of Training Dataset Sizes \(n\).** Figure 6 illustrates the impact of varying training dataset sizes for the downstream task on different LM methods. Regarding CIFAR100 as the downstream task, compared with RLM and ILM, BLM and BLM+ yield higher accuracy consistently. With approximately a 40% fraction of the downstream training data, BLM or BLM+ can achieve similar accuracy compared with training on the entire dataset.

**Other Experiments.** The parameter experiments and performance analysis regarding the impact of Laplace smoothing coefficient \(\) and top-\(K\) ratio \(\) for BLM and BLM+ are detailed in Appendix G. The visualization and analysis of LM matrices derived from gradient-free and deep learning-based methods can be found in Appendix I. Training cost analysis is discussed in Appendix J. Additional visualization results of LM methods applied to pretrained vision models are presented in Appendix K. Lastly, the application of BLM and BLM+ for vision-language models is explored in Appendix L.

Figure 4: Visualization of input VR and top-weighted pretrained labels applying BLM+. Training loss and weight changes (Euclidean norm) of probabilistic LM \(_{}\) per iteration are plotted below. Pretrained ResNet-18 is used, and the downstream label ‘Marigold’ is selected as an example.

## 6 Conclusion

We focus on output LM methods for VR and reveal the drawbacks in current gradient-free LM methods, which use one-to-one mappings that overly simplify the relationship between the pretrained and downstream label spaces. To address this issue, we propose BLM, which calculates probabilistic LM matrices guided by Bayes' theorem. Additionally, we aggregate the probability of top-\(K\) predicted pretrained labels instead of counting a single label during the estimation of probabilistic LM matrices, yielding an improved method BLM+. Both theoretical analysis and experimental results validate the effectiveness of BLM and BLM+ while offering insights into understanding the effectiveness of VR through a probabilistic lens.