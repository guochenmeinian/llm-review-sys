# Guiding a Diffusion Model with a Bad Version of Itself

Tero Karras

NVIDIA

Miika Aittala

NVIDIA

Tuomas Kynkaanniemi

Aalto University

 Jaakko Lehtinen

NVIDIA, Aalto University

Timo Aila

NVIDIA

Samuli Laine

NVIDIA

###### Abstract

The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for \(64 64\) and 1.25 for \(512 512\), using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.

## 1 Introduction

Denoising diffusion models  generate synthetic images by reversing a stochastic corruption process. Essentially, an image is revealed from pure noise by denoising it little by little in successive steps. A neural network that implements the denoiser (equivalently , the score function ) is a central design element, and various architectures have been proposed (e.g., ). Equally important are the details of the multi-step denoising process that corresponds mathematically to solving an ordinary  or a stochastic  differential equation, for which many different parameterizations, solvers, and step schedules have been evaluated . To control the output image, the denoiser is typically conditioned on a class label, an embedding of a text prompt, or some other form of conditioning input .

The training objective of a diffusion model aims to cover the entire (conditional) data distribution. This causes problems in low-probability regions: The model gets heavily penalized for not representing them, but it does not have enough data to learn to generate good images corresponding to them. Classifier-free guidance (CFG)  has become the standard method for "lowering the sampling temperature", i.e., focusing the generation on well-learned high-probability regions. By training a denoiser network to operate in both conditional and unconditional setting, the sampling process can be steered away from the unconditional result -- in effect, the unconditional generation task specifies a result to _avoid_. This results in better prompt alignment and improved image quality, where the former effect is due to CFG implicitly raising the conditional part of the probability density to a power greater than one .

However, CFG has drawbacks that limit its usage as a general low-temperature sampling method. First, it is applicable only for conditional generation, as the guidance signal is based on the difference between conditional and unconditional denoising results. Second, because the unconditional andconditional denoisers are trained to solve a different task, the sampling trajectory can overshoot the desired conditional distribution, which leads to skewed and often overly simplified image compositions . Finally, the prompt alignment and quality improvement effects cannot be controlled separately, and it remains unclear how exactly they relate to each other.

In this paper, we provide new insights into why CFG improves image quality and show how this effect can be separated out into a novel method that we call _autoguidance_. Our method does not suffer from the task discrepancy problem because we use an inferior version of the main model itself as the guiding model, with unchanged conditioning. This guiding model can be obtained by simply limiting, e.g., model capacity and/or training time. We validate the effectiveness of autoguidance in various synthetic test cases as well as in practical image synthesis in class-conditional and text-conditional settings. In addition, our method enables guidance for unconditional synthesis. In quantitative tests, the generated image distributions are improved considerably when measured using FID  and \(}\) metrics, setting new records in ImageNet-512 and ImageNet-64 generation.

Our implementation and pre-trained models are available at https://github.com/NVlabs/edm2

## 2 Background

Denoising diffusion.Denoising diffusion generates samples from a distribution \(p_{}()\) by iteratively denoising a sample of pure white noise, such that a noise-free random data sample is gradually revealed . The idea is to consider heat diffusion of \(p_{}()\) into a sequence of increasingly smoothed densities \(p(;)=p_{}()*(; ,^{2})\). For a large enough \(_{}\), we have \(p(;_{})(;, _{}^{2})\), from which we can trivially sample by drawing normally distributed white noise. The resulting sample is then evolved backward towards low noise levels by a probability flow ODE [23; 47; 49]

\[_{}\;=\;-_{_{}}\! p (_{};)\;\] (1)

that maintains the property \(_{} p(_{};)\) for every \([0,_{}]\). Upon reaching \(=0\), we obtain \(_{0} p(_{0};0)=p_{}(_{0})\) as desired.

In practice, the ODE is solved numerically by stepping along the trajectory defined by Equation 1. This requires evaluating the so-called score function \(_{} p(;)\) for a given sample \(\) and noise level \(\) at each step. Rather surprisingly, we can approximate this vector using a neural network \(D_{}(;)\) parameterized by weights \(\) trained for the denoising task

\[\;=\;_{}_{ p_{},  p_{},(,^{2} )}\|D_{}(+;)-\|_{2}^ {2},\] (2)

where \(p_{}\) controls the noise level distribution during training. Given \(D_{}\), we can estimate \(_{} p(;)(D_{}(; )-)/^{2}\), up to approximation errors due to, e.g., finite capacity or training time [23; 53]. As such, we are free to interpret the network as predicting either a denoised sample or a score vector, whichever is more convenient for the analysis at hand. Many reparameterizations and practical ODE solvers are possible, as enumerated by Karras et al. . We follow their recommendations, including the schedule \((t)=t\) that lets us parameterize the ODE directly via noise level \(\) instead of a separate time variable \(t\).

In most applications, each data sample \(\) is associated with a label \(\), representing, e.g., a class index or a text prompt. At generation time, we control the outcome by choosing a label \(\) and seeking a sample from the conditional distribution \(p(|;)\) with \(=0\). In practice, this is achieved by training a denoiser network \(D_{}(;,)\) that accepts \(\) as an additional conditioning input.

Classifier-free guidance.For complex visual datasets, the generated images often fail to reproduce the clarity of the training images due to approximation errors made by finite-capacity networks. A broadly used trick called _classifier-free guidance_ (CFG)  pushes the samples towards higher likelihood of the class label, sacrificing variety for "more canonical" images that the network appears to be better capable of handling.

In a general setting, guidance in a diffusion model involves two denoiser networks \(D_{0}(;,)\) and \(D_{1}(;,)\). The guiding effect is achieved by _extrapolating_ between the two denoising results by a factor \(w\):

\[D_{w}(;,)\;=\;wD_{1}(;,)+( 1-w)D_{0}(;,).\] (3)Trivially, setting \(w=0\) or \(w=1\) recovers the output of \(D_{0}\) and \(D_{1}\), respectively, while choosing \(w>1\) over-emphasizes the output of \(D_{1}\). Recalling the equivalence of denoisers and scores , we can write

\[D_{w}(;,)  +^{2}_{}(|;)[(|; )}{p_{0}(|;)}]^{w})}_{:\ p_{w}( |;)}.\] (4)

Thus, guidance grants us access to the score of the density \(p_{w}(|;)\) implied in the parentheses. This score can be further written as [10; 16]

\[_{} p_{w}(|;) = _{} p_{1}(|;)+(w-1) _{}(|;)}{p_{0}( |;)}.\] (5)

Substituting this expression into the ODE of Equation 1, this yields the standard evolution for generating images from \(p_{1}\), plus a perturbation that increases (for \(w>1\)) the ratio of \(p_{1}\) and \(p_{0}\) as evaluated at the sample. The latter can be interpreted as increasing the likelihood that a hypothetical classifier would attribute for the sample having come from density \(p_{1}\) rather than \(p_{0}\).

In CFG, we train an auxiliary _unconditional_ denoiser \(D_{}(;)\) to denoise the distribution \(p(;)\) marginalized over \(\), and use this as \(D_{0}\). In practice, this is typically  done using the same network \(D_{}\) with an empty conditioning label, setting \(D_{0} D_{}(;,)\) and \(D_{1} D_{}(;,)\). By Bayes' rule, the extrapolated score vector becomes \(_{} p(|;)+(w-1)_{} p(|;)\). During sampling, this guides the image to more strongly align with the specified class \(\).

It would be tempting to conclude that solving the diffusion ODE with the score function of Equation 5 produces samples from the data distribution specified by \(p_{w}(|;0)\). Unfortunately this is _not_ the case, because \(p_{w}(|;)\) does not represent a valid heat diffusion of \(p_{w}(|;0)\). Therefore, solving the ODE does not, in fact, follow the density. Instead, the samples are blindly pushed towards higher values of the implied density at each noise level during sampling. This can lead to distorted sampling trajectories, greatly exaggerated truncation, and mode dropping in the results , as well as over-saturation of colors . Nonetheless, the improvement in image quality is often remarkable, and high guidance values are commonly used despite the drawbacks (e.g., [14; 39; 41; 45]).

## 3 Why does CFG improve image quality?

We begin by identifying the mechanism by which CFG improves image quality instead of only affecting prompt alignment. To illustrate why unguided diffusion models often produce unsatisfactory images, and how CFG remedies the problem, we study a 2D toy example where a small-scale denoiser network is trained to perform conditional diffusion in a synthetic dataset (Figure 1). The dataset is designed to exhibit low local dimensionality (i.e., highly anisotropic and narrow support) and hierarchical emergence of local detail upon noise removal. These are both properties that can be expected from the actual manifold of realistic images [5; 40]. For details of the setup, see Appendix C.

Score matching leads to outliers.Compared to sampling directly from the underlying distribution (Figure 0(a)), the unguided diffusion in Figure 0(b) produces a large number of extremely unlikely samples outside the bulk of the distribution. In the image generation setting, these would correspond to unrealistic and broken images.

We argue that the outliers stem from the limited capability of the score network combined with the score matching objective. It is well known that maximum likelihood (ML) estimation leads to a "conservative" fit of the data distribution  in the sense that the model attempts to cover all training samples. This is because the underlying Kullback-Leibler divergence incurs extreme penalties if the model severely underestimates the likelihood of any training sample. While score matching is generally not equal to ML estimation, they are closely related [15; 32; 49] and appear to exhibit broadly similar behavior. For example, it is known that for a multivariate Gaussian model, the optimal score matching fit coincides with the ML estimate . Figures 1(a) and 1(b) show the learned score field and implied density in our toy example for two models of different capacity at an intermediate noise level. The stronger model envelops the data more tightly, while the weaker model's density is more spread out.

From the perspective of image generation, a tendency to cover the entire training data becomes a problem: The model ends up producing strange and unlikely images from the data distribution'sextremities that are not learnt accurately but included just to avoid the high loss penalties. Furthermore, during training, the network has only seen real noisy images as inputs, and during sampling it may not be prepared to deal with the unlikely samples it is handed down from the higher noise levels.

CFG eliminates outliers.The effect of applying classifier-free guidance during generation is demonstrated in Figure 1c. As expected, the samples avoid the class boundary (i.e., there are no samples in the vicinity of the gray area), and entire branches of the distribution are dropped. We also observe a second phenomenon, where the samples have been pulled in towards the core of the manifold, and away from the low-probability intermediate regions. Seeing that this eliminates the unlikely outlier samples, we attribute the image quality improvement to it. However, mere boosting of the class likelihood does not explain this increased concentration.

We argue that this phenomenon stems from a quality difference between the conditional and unconditional denoiser networks. The denoiser \(D_{0}\) faces a more difficult task of the two: It has to generate from all classes at once, whereas \(D_{1}\) can focus on a single class for any specific sample. Given the more difficult task, and typically only a small slice of the training budget, the network \(D_{0}\) attains

Figure 1: A fractal-like 2D distribution with two classes indicated with gray and orange regions. Approximately 99% of the probability mass is inside the shown contours. **(a)** Ground truth samples drawn directly from the orange class distribution. **(b)** Conditional sampling using a small denoising diffusion model generates outliers. **(c)** Classifier-free guidance (\(w=4\)) eliminates outliers but reduces diversity by over-emphasizing the class. **(d)** Naive truncation via lengthening the score vectors. **(e)** Our method concentrates samples on high-probability regions without reducing diversity.

Figure 2: Closeup of the region highlighted in Figure 1c. **(a)** The implied learned density \(p_{1}(|;_{})\) (green) at an intermediate noise level \(_{}\) and its score vectors (log-gradients), plotted at representative sample points. The learned density approximates the underlying ground truth \(p(|;_{})\) (orange) but fails to replicate its sharper details. **(b)** The weaker unconditional model learns a further spread-out density \(p_{0}(;_{})\) (red) with a looser fit to the data. **(c)** Guidance moves the points according to the gradient of the (log) ratio of the two learned densities (blue). As the higher-quality model is more sharply concentrated at the data, this field tends inward towards the data distribution. The corresponding gradient is simply the difference of respective gradients in (a) and (b), illustrated at selected points. **(d)** Sampling trajectories taken by standard unguided diffusion following the learned score \(_{} p_{1}(|;)\), from noise level \(_{}\) to \(0\). The contours (orange) represent the ground truth noise-free density. **(e)** Guidance introduces an additional force shown in (c), causing the points to concentrate at the core of the data density during sampling.

a worse fit to the data.1 This difference in accuracy is apparent in respective plots of the learned densities in Figures 1(a) and 1(b).

From our interpretation in Section 2, it follows that CFG is not only boosting the likelihood of the sample having come from the class \(\), but _also_ that of having come from the higher-quality implied distribution. Recall that guidance boils down to an additional force (Equation 5) that pulls the samples towards higher values of \([p_{1}(|;)/p_{0}(|; )]\). Plotting this ratio for our toy example in Figure 1(c), along with corresponding gradients that guidance contributes to the ODE vector field, we see that the ratio generally decreases with distance from the manifold due to the denominator \(p_{0}\) representing a more spread-out distribution, and hence falling off slower than the numerator \(p_{1}\). Consequently, the gradients point inward towards the data manifold. Each contour of the density ratio corresponds to a specific likelihood that a hypothetical classifier would assign on a sample being drawn from \(p_{1}\) instead of \(p_{0}\). Because the contours roughly follow the local orientation and branching of the data manifold, pushing samples deeper into the "good side" concentrates them at the manifold.2

Discussion.We can expect the two models to suffer from inability to fit at similar places, but to a different degree. The predictions of the denoisers will disagree more decisively in these regions. As such, CFG can be seen as a form of adaptive truncation that identifies when a sample is likely to be under-fit and pushes it towards the general direction of better samples. Figures 1(d) and 1(e) show the effect over the course of generation: The truncation "overshoots" the correction and produces a narrower distribution than the ground truth, but in practice this does not appear to have an adverse effect on the images.

In contrast, a naive attempt at achieving this kind of truncation -- inspired by, e.g., the truncation trick in GANs [4; 33] or lowering temperature in generative language models -- would counteract the smoothing by uniformly lengthening the score vectors by a factor \(w>1\). This is illustrated in Figure 0(d), where the samples are indeed concentrated in high-probability regions, but in an isotropic fashion that leaves the outer branches empty. In practice, images generated this way tend to show reduced variation, oversimplified details, and monotone texture.

## 4 Our method

We propose to isolate the image quality improvement effect by directly guiding a high-quality model \(D_{1}\) with a poor model \(D_{0}\) trained on the _same task_, _conditioning, and data distribution_, but suffering from certain additional degradations, such as low capacity and/or under-training. We call this procedure _autoguidance_, as the model is guided with an inferior version of itself.

In the context of our 2D toy example, this turns out to work surprisingly well. Figure 0(e) demonstrates the effect of using a smaller \(D_{0}\) with fewer training iterations. As desired, the samples are pulled close to the distribution without systematically dropping any part of it.

To analyze why this technique works, recall that under limited model capacity, score matching tends to over-emphasize low-probability (i.e., implausible and under-trained) regions of the data distribution. Exactly where and how the problems appear depend on various factors such as network architecture, dataset, training details, etc., and we cannot expect to identify and characterize the specific issues a priori. However, we can expect a weaker version of the _same_ model to make broadly similar errors in the same regions, only stronger. Autoguidance seeks to identify and reduce the errors made by the stronger model by measuring its difference to the weaker model's prediction, and boosting it. When the two models agree, the perturbation is insignificant, but when they disagree, the difference indicates the general direction towards better samples.

As such, we can expect autoguidance to work if the two models suffer from degradations that are compatible with each other. Since any \(D_{1}\) can be expected to suffer from, e.g., lack of capacity and lack of training -- at least to some degree -- it makes sense to choose \(D_{0}\) so that it further exacerbates these aspects.

In practice, models that are trained separately or for a different number of iterations differ not only in accuracy of fit, but also in terms of random initialization, shuffling of the training data, etc. For guidance to be successful, the quality gap should be large enough to make the systematic spreading-out of the density outweigh these random effects.

Study on synthetic degradations.To validate our hypothesis that the two models must suffer from the same kind of degradations, we perform a controlled experiment using synthetic corruptions applied to a well-trained real-world image diffusion model. We create the main and guiding networks, \(D_{1}\) and \(D_{0}\), by applying different degrees of a synthetic corruption to the base model. This construction allows us to use the untouched base model as grounding when measuring the FID effect of the various combinations of corruptions applied to \(D_{1}\) and \(D_{0}\). We find that as long as the degradations are compatible, autoguidance largely undoes the damage caused by the corruptions:

* **Base model:** As the base model, we use EDM2-S trained on ImageNet-512 without dropout (FID = 2.56).
* **Dropout:** We construct \(D_{1}\) by applying 5% dropout to the base model in a post-hoc fashion (FID = 4.98), and \(D_{0}\) by applying 10% dropout to the base model (FID = 15.00). Applying autoguidance, we reach the best result (FID = 2.55) with \(w=2.25\), matching the base model's FID.
* **Input noise:** We construct \(D_{1}\) by modifying the base model to add noise to the input images so that their noise level is increased by 10% (FID = 3.96). The \(\) conditioning input of the denoiser is adjusted accordingly. The guiding model \(D_{0}\) is constructed similarly, but with a noise level increase of 20% (FID = 9.73). Applying autoguidance, we reach the best result (FID = 2.56) with \(w=2.00\), again matching the base model's FID.
* **Mismatched degradations:** If we corrupt \(D_{1}\) by dropout and \(D_{0}\) by input noise, or vice versa, guidance does not improve the results at all; in these cases, the best FID is obtained by setting \(w=1\), i.e., by disabling guidance and using the less corrupted \(D_{1}\) exclusively.

While this experiment corroborates our main hypothesis, we do not suggest that guiding with these synthetic degradations would be useful in practice. A realistic diffusion model will not suffer from these particular degradations, so creating a guiding model by introducing them would not yield consistent truncation towards the data manifold.

## 5 Results

Our primary evaluation is carried out using ImageNet (ILSVRC2012)  at two resolutions: \(512 512\) and \(64 64\). For ImageNet-512 we use latent diffusion , while ImageNet-64 works directly on RGB pixels. We take the current state-of-the-art diffusion model EDM2  as our baseline.3 We use the EDM2-S and EDM2-XXL models with default sampling parameters: 32 deterministic steps with a \(2^{}\) order Heun sampler . For most setups, a pre-trained model is publicly available, and in the remaining cases we train the models ourselves using the official implementation (Appendix B).

We use two degradations for the guiding model: shorter training time and reduced capacity compared to the main model. We obtain the best results by having both of these enabled. With EDM2-S, for example, we use an XS-sized guiding model that receives \(1/16^{}\) of the training iterations of the main model. We ablate the relative importance of the degradations as well as the sensitivity to these specific choices in Section 5.1. As the EDM2 networks are known to be sensitive to the guidance weight and EMA length , we search the optimal values for each case using a grid search.

Table 1 shows that our method improves FID  and \(_{}\) considerably. Using the small model (EDM2-S) in ImageNet-512, our autoguidance improves FID from 2.56 to 1.34. This beats the 1.68 achieved by the concurrently proposed CFG + Guidance Interval , and is the best result reported for this dataset regardless of the model size. Using the largest model (EDM2-XXL) further improves the record to 1.25. The \(_{}\) records are similarly improved, with the large model

lowering the record from 29.16 to 24.18. In ImageNet-64, the improvement is even larger; in this dataset, we set the new record FID and \(_{}\) of 1.01 and 31.85, respectively.

A particular strength of autoguidance is that it can be applied to unconditional models as well. While conditional ImageNet generation may be getting close to saturation, the unconditional results remain surprisingly poor. EDM2-S achieves a FID of 11.67 in the unconditional setting, indicating that practically none of the generated images are of presentable quality. Enabling autoguidance lowers the FID substantially to 3.86, and the improvement in \(_{}\) is similarly significant.

### Ablations

Table 1 further shows that it is beneficial to allow independent EMA lengths for the main and guiding models. When both are forced to use the same EMA, FID worsens from 1.34 to 1.53 in ImageNet-512 (EDM2-S). We also measure the effect of each degradation (reduced training time, capacity) in isolation. If we set the guiding model to the same capacity as the main model and only train it for a

 
**Method** & & FID & \(w\) & \(_{n}\) & \(_{g}\) & \(_{}\) & \(w\) & \(_{n}\) & \(_{g}\) \\   **} & EDM2-S &  & 2.56 & \(-\) & 0.130 & \(-\) & **68.64** & \(-\) & 0.190 & \(-\) \\  & + Classifier-free guidance &  & 2.23 & 1.40 & 0.025 & 0.025 & **52.32** & **1.90** & 0.085 & 0.085 \\  & + Guidance interval &  & 1.68 & 2.10 & 0.025 & 0.025 & **46.25** & 3.20 & 0.085 & 0.085 \\  & + Autoguidance (XS, \(T/16\)) & Ours & **1.34** & 2.10 & 0.070 & 0.125 & **36.67** & 2.45 & 0.120 & 0.165 \\  & \(-\) Same EMA for both & & 1.53 & 1.95 & 0.050 & 0.050 & **40.81** & 2.25 & 0.115 & 0.115 \\  & \(-\) Reduce training only & & 1.51 & 2.20 & 0.090 & 0.130 & **42.27** & **2.55** & 0.130 & 0.170 \\  & \(-\) Reduce capacity only & & 2.13 & 1.80 & 0.120 & 0.160 & **59.89** & 1.90 & 0.140 & 0.085 \\   **} & EDM2-XXL &  & 1.91 & \(-\) & 0.070 & \(-\) & **42.84** & \(-\) & 0.150 & \(-\) \\  & + Classifier-free guidance &  & 1.81 & 1.20 & 0.015 & 0.015 & **33.09** & 1.70 & 0.015 & 0.015 \\  & + Guidance interval &  & 1.40 & 2.00 & 0.015 & 0.015 & **29.16** & 2.90 & 0.015 & 0.015 \\  & + Autoguidance (M, \(T/3.5\)) & Ours & **1.25** & 2.05 & 0.075 & 0.155 & **24.18** & **2.30** & 0.130 & 0.205 \\    & EDM2-S, unconditional & & 11.67 & \(-\) & 0.145 & \(-\) & 209.53 & \(-\) & 0.170 & \(-\) \\  & + Autoguidance (XS, \(T/16\)) & Ours & **3.86** & 2.85 & 0.070 & 0.110 & **90.39** & **2.90** & 0.090 & 0.125 \\   **} & RIN &  & 1.23 & \(-\) & 0.033 & \(-\) & **–** & \(-\) & \(-\) & \(-\) \\  & EDM2-S &  & 1.58 & \(-\) & 0.075 & \(-\) & **58.52** & \(-\) & 0.160 & \(-\) \\   & + Classifier-free guidance & & 1.48 & 1.15 & 0.030 & 0.030 & **41.84** & 1.85 & 0.040 & 0.040 \\   & + Autoguidance (XS, \(T/8\)) & Ours & **1.01** & 1.70 & 0.045 & 0.110 & **31.85** & 2.20 & 0.105 & 0.175 \\  

Table 1: Results on ImageNet-512 and ImageNet-64. The parameters of autoguidance refer to the capacity and amount training received by the guiding model. The latter is given relative to the number of training images shown to the main model (\(T\)). The columns \(_{}\) and \(_{}\) indicate the length parameter of the post-hoc EMA technique  for the main and guiding model, respectively.

Figure 3: Sensitivity w.r.t. autoguidance parameters, using EDM2-S on ImageNet-512. The shaded regions indicate the min/max FID over 3 evaluations. **(a)** Sweep over guidance weight \(w\) while keeping all other parameters unchanged. The curves correspond to how much the guiding model was trained relative to the number of images shown to the main model. **(b)** Sweep over guidance weight for different guiding model capacities. **(c)** Sweep over the two EMA length parameters for our best configuration, denoted with \({}^{}\) in (a) and (b).

shorter time, FID worsens to 1.51. If we instead train the reduced-capacity guiding model for as long as the main model, FID suffers a lot more, to 2.13. We can thus conclude that both degradations are beneficial and orthogonal, but a majority of the improvement comes from reduced training of the guiding model. Notably, all these ablations still outperform standard CFG in terms of FID.

Figure 3 probes the sensitivity to various hyperparameters. Our best result is obtained by training the guiding model \(1/16^{}\) as much as the main model, in terms of images shown during training. Further halving the training budget is almost equally good, while doubling the amount of training starts to slowly compromise the results. The results are quite insensitive to the choice of the guidance weight. In terms of the capacity of the guiding model, one step smaller (XS for EDM2-S) gave the best result. Two steps smaller (XXS) was also better than no capacity reduction (S), but started to show excessive sensitivity to the guidance weight. The results are also sensitive to the EMA length, similarly to the original EDM2. Post-hoc EMA  allows us to search the optimal parameters at a feasible cost.

We also explored several other degradations for the guiding model but did not find them to be beneficial. First, we tried reducing the amount of training data used for the guiding model, but this did not seem to improve the results over the baseline. Second, applying guidance interval  on top of our method reduced its benefits to some extent, suggesting that autoguidance is helpful at all noise levels. Third, deriving the guiding model from the main model using synthetic degradations did not work at all, providing further evidence that the guiding model needs to exhibit the same kinds of degradations that the main model suffers from. Fourth, we found that if the main model had been quantized, e.g., to improve inference speed, quantizing it to an even lower precision did not yield a useful guiding model.

One limitation of autoguidance is the need to train a separate guiding model. That said, the additional training cost can be quite modest when using a smaller model and shorter training time for the guiding model. For example, the EDM2-M model trains approximately \(2.7\) as fast as EDM2-XXL per iteration, and we train it for 1/3.5 of iterations, so the additional cost is around +11%. For the EDM2-S/XS pair used in most of our experiments, the added training cost is only +3.6%.

Figure 4: Example results for the _Tree frog, Palace, Mushroom, Castle_ classes of ImageNet-512 using EDM2-S. Guidance weight increases to the right; rows are classifier-free guidance and our method.

### Qualitative results

Figure 4 shows examples of generated images for ImageNet-512. Both CFG and our method tend to improve the perceptual quality of images, guiding the results towards clearer realizations as the guidance weight increases. However, CFG seems to have a tendency to head towards a more limited number of canonical images  per class, while our method produces a wider gamut of image compositions. An example is the atypical image of a _Palace_ at \(w=1\), which CFG converts to a somewhat idealized depiction as \(w\) increases. Sometimes the unguided sample contains incompatible elements of multiple possible images, such as the _Castle_ image, which includes a rough sketch of two or three castles of unrelated styles. In this instance, CFG apparently struggles to decide what to do, whereas our method first builds the large red element into a castle, and with increased guidance focuses on the red foreground object. A higher number of possible output images is consistent with a lower FID, implying better coverage of the training data.

In order to study our method in the context of large-scale image generators, we apply it to DeepFloyd IF . We choose this baseline because multiple differently-sized models are publicly available. Ideally we could have also used an earlier snapshot as the guiding model, but those were not available. DeepFloyd IF generates images as a cascade of three diffusion models: a base model and two super-resolution stages. We apply our method to the base model only, while the subsequent stages always use CFG. Figure 5 demonstrates the effect of CFG, our method, and their various combinations. To combine autoguidance with CFG, we extend Equation 3 to cover multiple guiding models as proposed by Liu et al.  and distribute the total guidance weight among them using linear interpolation (see Appendix B.2 for details). While CFG improves the image quality significantly, it also simplifies the style and layout of the image towards a canonical depiction. Our method similarly improves the

Figure 5: Results for DeepFloyd IF  using the prompt “_A blue Jay standing on a large basket of rainbow macarons”_. The rows correspond to guidance weights \(w\{1,2,3,4\}\). The leftmost column shows results for CFG and the rightmost for autoguidance (XL-sized model guided by M-sized one). The middle columns correspond to blending between the two. See Appendix A for more examples.

image quality, but it better preserves the image's style and visual complexity. We hope that using both guiding methods simultaneously will serve as a new, useful artistic control.

## 6 Discussion and Future work

We have shown that classifier-free guidance entangles several phenomena together, and that a different perspective together with simple practical changes opens up an entire new design space. In addition to removing the superfluous connection to conditioning, this enables significantly better results.

Potential directions for future work include formally proving the conditions that allow autoguidance to be beneficial, and deriving good rules of thumb for selecting the best guiding model. Our suggestion -- an early snapshot of a smaller model -- is easy to satisfy in principle, but these are not available for current large-scale image generators in practice. Such generators are also often trained in successive stages where the training data may change at some point, causing potential distribution shifts between snapshots that would violate our assumptions. Various modifications to guidance  can be seen as inducing degradations through perturbation of attention maps or denoiser inputs. Whether these approaches could provide additional benefits in our setup remains an open question. Autoguidance also bears conceptual similarity to contrastive decoding  used in large language models to reduce the repetitiveness of generations, and there may be opportunities for sharing observations between the two domains.

Recently, several studies  have reduced the downsides of CFG by making the guidance weight noise level-dependent. A key benefit from these schedules appears to be the suppression of CFG at high noise levels, where its image quality benefit is overshadowed by the undesirable reduction in variation that is caused by large differences in the content of the differently conditioned distributions. In contrast, autoguidance is not expected to suffer from this problem at high noise levels, as both models target the same distribution. So far we have compared autoguidance only with the interval method , which we did not find beneficial in combination. A further study on the various possible combinations, in terms of quantitative performance as well as artistic control, is a natural next step. It could also be interesting to further isolate the origin of the improvement using alternative metrics, such as precision and recall , Human Preference Score , or PickScore .

#### Acknowledgments

We thank David Luebke, Janne Hellsten, Ming-Yu Liu, and Alex Keller for discussions and comments, and Tero Kuosmanen and Samuel Klenberg for maintaining our compute infrastructure.