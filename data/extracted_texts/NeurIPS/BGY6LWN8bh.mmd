# How Easy is It to Fool Your Multimodal LLMs?

**An Empirical Analysis on Deceptive Prompts**

**Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan**

Apple

{yusuqian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com

The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling _deceptive_ information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MADBench,1 a carefully curated benchmark that contains 1000 test samples divided into 5 categories, such as non-existent objects, count of objects, and spatial relationship. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4v, Reka, Gemini-Pro, to open-sourced models, such as LLaVA-NeXT and MiniCPM-Llama3. Empirically, we observe significant performance gaps between GPT-4o and other models; and previous robust instruction-tuned models are not effective on this new benchmark. While GPT-4o achieves 82.82% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 9% to 50%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.

Figure 1: How easy is it to _fool_ your multimodal LLMs? Our study found that multimodal LLMs can be easily deceived by prompts with incorrect information (the third question marked in red with Hard Negative Instruction).

Introduction

Recent advancements in Multimodal Large Language Models (MLLMs) [1; 2; 3; 4; 5; 6; 7], exemplified by models like GPT-4V(ision)  and Gemini , mark a significant milestone in the evolution of AI, extending the capabilities of large language models to the realm of visual understanding and interaction.

However, the sophistication of MLLMs brings with it unique challenges, notably, _hallucination_. Current studies [6; 10; 11] have been actively exploring solutions to mitigate hallucination, especially when the model tries to generate long responses. However, there still remains a notable gap in the literature: no work has yet been conducted to focus on comprehensively studying the robustness of MLLMs when confronted with deceptive information in the prompts.2 Our work aims to fill in this gap. This issue is particularly critical, as it pertains to the reliability and trustworthiness of these models in real-world applications , and holds substantial importance for the ongoing development and deployment of such AI systems.

To this end, we present MAD-Bench, a carefully curated benchmark that contains 1000 image-prompt pairs spanning across five deception categories, to systematically examine how MLLMs resolve the conflicts when facing inconsistencies between text prompts and images. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V , Gemini-Pro , to open-sourced models, such as LLaVA-NeXT  and MiniCPM . The evaluation is fully automated via the use of GPT-4o . Results shed light on how vulnerable MLLMs are in handling deceptive instructions. For example, Figure 1 illustrates how sensitive LLaVA-1.5  is to the _factualness_ of the input prompt and its consistency with the image. When asked "is there a cat in the image?", LLaVA-1.5 can successfully identify there is no cat; but when prompted with "what color is the cat in the image?", the model will imagine there is a cat inside. Empirically, we observe that GPT-4V suffers much less when compared with all the other MLLMs; however, the performance is still not ideal (GPT-4V vs. others: 82% vs. mostly 3%-50% accuracy).

Finally, we provide a simple remedy to boost performance, which was surprisingly found to be effective to double the models' accuracy. Specifically, we carefully design a system prompt in the form of a long paragraph to be prepended to the existing prompt, to encourage the model to think carefully before answering the question. This simple approach boosts the accuracy of LLaVA-NeXT-13b from 49.65% to 68.21% (similar boosts for other models); however, the absolute numbers still have room for improvement.

Our contributions are summarized as follows. (\(i\)) We construct MAD-Bench, a new benchmark to comprehensively evaluate MLLMs on their capability to resist deceiving information in the prompt. (\(ii\)) We provide a detailed analysis of popular MLLMs, and list some common causes for incorrect responses. (\(iii\)) We provide a simple remedy to boost performance via the careful design of a system prompt. MAD-Bench will be open-sourced, and we hope this benchmark can serve as a useful resource to stimulate further research to enhance models' resilience against deceptive prompts.

## 2 Related Work

Multimodal Large Language Models (MLLMs).MLLM has become an increasingly hot research topic. Early models primarily focused on large-scale image-text pre-training [16; 17; 18; 19; 20; 21; 22; 23; 24]. Among them, Flamingo  pioneered the integration of a CLIP image encoder with LLMs through gated cross-attention blocks, showcasing emergent multimodal in-context few-shot learning capabilities, via pre-training over millions of image-text pairs and interleaved image-text datasets .

On the other hand, recent research has focused on visual instruction tuning [7; 27; 28; 29; 30]. Prominent examples include LLaVA(-1.5) [1; 2], InstructBLIP , Qwen-VL , CogVLM ,Emu2 , SPHINX , to name a few. Besides text response generation, recent works have also enabled MLLMs for referring and grounding [4; 35; 36; 37], image segmentation [38; 39], image editing , image generation [33; 41], _etc_.

The release of proprietary systems like GPT-4V  and Gemini  has elevated the research of MLLMs to new heights. Since GPT-4V's release, researchers have been exploring its capabilities as well as weaknesses [42; 43; 44; 45; 46]. As MLLMs become stronger, the development of more challenging benchmarks is essential to push the boundaries of what these models can achieve. In this work, we aim to design a new benchmark to evaluate MLLMs' resilience against deceptive prompts.

Hallucination in MLLMs.Below, we first discuss hallucination in LLMs, and then focus on hallucination in MLLMs.

Existing work on mitigating hallucination in LLMs can be roughly divided into two categories: (\(i\)) prompt engineering [47; 48; 49; 50; 51; 52; 53], and (\(ii\)) model enhancement [54; 55; 56; 57; 58; 59; 60; 61; 62]. These studies laid solid foundations for understanding the causes of hallucinations, such as over-reliance on context, or training data biases.

Similarly, hallucination in MLLMs is also growing to be an important research topic . There are various categories of hallucinations, such as describing objects that are non-existent in the input image, misunderstanding the spatial relationship between objects in the image, and counting objects incorrectly . The two main causes of hallucination in MLLMs found in existing work apart from the potential issues with training data include (\(i\)) limitations in correctly understanding input images, and (\(ii\)) language model bias . Various methods have been proposed to mitigate hallucination in MLLMs [1; 6; 10; 11; 65; 66; 67; 68; 69].

Furthermore, various benchmarks have been proposed to evaluate hallucination in MLLMs. Specifically, POPE , M-HalDetect , GAVIE , and Throne  evaluated object hallucination. HallusionBench  evaluated both visual and language hallucination. MMHal-Bench  evaluated hallucination in more aspects including relations, attributes, environments, _etc_. Bingo  studied hallucination in terms of bias and interference in GPT-4V . Hal-Eval  assesses event hallucination, which involves creating a fictional target and constructing an entire narrative around it, encompassing its attributes, relationships, and actions.

In this work, we aim to study how easy it is to use deceptive prompts that contain information inconsistent with the image to mislead MLLMs to generate responses with hallucination. Note, that we are not the first to study this. A similar model behavior is called "sycophancy" in the LLM literature . MME  and LLaVA-Bench (in-the-Wild)  also constructed prompts with deceiving information to test model robustness. Deceptive prompts are termed "negative instructions" in LRV-Instruction  and "text-to-image interference" in the Bingo benchmark . Different from them, we comprehensively study MLLMs' ability to handle deceptive prompts in multiple categories. Unlike previous studies [2; 75] which primarily used "Is/Are/Can" questions, we found that it is relatively easy for state-of-the-art MLLMs to counter deceptive information in such formats. Consequently, we shifted our focus to questions beginning with "What", "How", "Where", _etc_., to provide a more challenging and insightful evaluation.

## 3 MAD-Bench

In this section, we present MAD-Bench, introduce how we collect deceptive image-prompt pairs, as well as our evaluation method. The images in MAD-Bench are sourced from COCO 2017 validation set , SBU , and TextVQA . Using a public dataset sometimes brings concerns about data leakage. In our case, given the special nature of our deceptive prompts to be introduced in the following section, this will not be a problem.

### Deception Categories

MAD-Bench encompasses five distinct categories of 1000 image-prompt pairs designed to test the resilience of MLLMs against deceptive prompts.

Table 1 provides the statistics of each category, and Figure 2 shows examples of deceptive prompts. The selected categories are partly inspired by MMBench . Below, we detail each category.

**Count of Object.** This category intentionally cites an incorrect quantity of visible objects in the image. A response fails this test if it asserts the presence of \(m\) instances of an object 'A' when, in reality, a different number \(n\) of object 'A' is present -- \(n\) being distinct from \(m\) and not zero. An accurate response would either challenge the prompt's inconsistency with the visual data and abstain from speculating on absent information, or seek further clarification to resolve any uncertainties.

**Non-existent Object.** Here, the prompts query about objects absent from the image. Failure occurs when a response acknowledges these non-existent objects as present.

**Object Attribute.** This category includes prompts that inaccurately describe visible objects' attributes. A response fails if it attributes these incorrect characteristics to the actual objects in the image.

  Deception Category & Count \\  Count of Object & 32 \\ Non-existent Object & 778 \\ Object Attribute & 24 \\ Scene Understanding & 115 \\ Text Recognition & 51 \\  

Table 1: Statistics of the 1000 image-prompt pairs in MAD-Bench.

Figure 2: Examples of deceptive prompts with example model responses.

**Scene Understanding.** This category involves prompts that inaccurately describe the scene encapsulating the objects in the image. A response that falls into error here can be one that accurately identifies the actions of the objects but misconstrues the scene or setting in alignment with the deceptive prompt.

**Text Recognition.** This category presents prompts that incorrectly identifies tech-rich objects in the image as something else or misunderstands the information conveyed in a piece of text. A misstep in this category occurs when a response fails to accurately identify the true information from the text.

### Prompt Generation Method

The process of creating deceptive prompts was automated by employing GPT-4o. To guide GPT-4o in generating questions that would intentionally mislead MLLMs, we using the following prompt:

You are a smart agent. Your goal is to generate a misleading question for an image.

A misleading question contains information inconsistent with the input image, but is mostly in accordance with the image.

A misleading question tries to test the robustness of a multimodal llm.

A good misleading question typically doesn't start with 'is' or 'are' or 'can'. It can start with 'what', 'how', 'describe', etc.

A good misleading question can be answered based merely on the image, without relying on external knowledge.

Now generate a misleading question for the input image.

Following the generation of these deceptive questions, a rigorous manual filtering process is followed to ensure that each question adheres to its category's deceptive criteria and maintains relevance to its associated image.

### Response Evaluation Method

We use GPT-4o to evaluate generated responses from 19 models. Mirroring the prompt generation method, we design specific prompts for each deceptive category to critically assess the responses. Our primary metric of evaluation is binary, focused strictly on whether the response has been misled, without considering other qualitative aspects such as helpfulness. These prompts for model evaluation are provided in Appendix.

To verify the accuracy of GPT-4o's automated evaluation, we randomly select 500 responses spanning the various models and deceptive categories for a manual accuracy check. This validation process yielded a 98.0% concordance rate with the outcomes of human evaluation, underlining the reliability of our approach.

## 4 Experiments

### Main Results

Results are summarized in Table 2. As the evaluation uses GPT-4o as the judge, results from each run may be slightly different from each other; the difference is normally with 1% according to our experiment results. Notably, GPT-4V's accuracy in the _Object Attribute_ and _Text Recognition_ categories is remarkably higher than the others, with 70.83% and 88.24% accuracy respectively. This indicates a substantial advancement in GPT-4V's ability to resist deceptive information. The overall performance of most other state-of-the-art MLLMs has much room for improvement. It is likely because (\(i\)) the way we design our prompts presents a larger challenge to MLLMs than the "Is/Are/Can"-style negative instructions  seen in their training data, as our prompts are designed intentionally to sound confident in the deceptive information.

Interestingly, we observe that models that support bounding box input and output (_i.e._, Ferret and Kosmos-2) achieve poor performance on this benchmark. We hypothesize that these models attempt to ground objects as best as they can as they are trained on positive data, therefore, they tend to ground non-existent objects as they are mentioned in the prompts, thus performing poorer than other models on our benchmark. Example responses from each model are provided in Appendix.

Overall, GPT-4V demonstrates superior performance across all metrics compared to the other models. GPT-4V has a more sophisticated understanding of visual data and is less prone to being misled by inaccurate information. This could be attributed to more advanced training, better architecture, or more sophisticated data processing capabilities. The results underscore the potential of GPT-4V in applications where accuracy in interpreting visual and contextual data is critical, despite the challenges of deceptive information. That being said, GPT-4V still fails in many cases, with two examples shown in Figure 3.

### Detailed Analysis

Our examination of how the model reacts to deceptive prompts has uncovered a range of common causes for incorrect responses. Figure 4 illustrates representative instances of errors corresponding to each identified category of mistakes, using Ferret as the running example.

**Inaccurate object detection.** State-of-the-art MLLMs generally perform well in object detection if not fed deceptive prompts. However, in face of a deceptive prompt mentioning objects invisible in the image, these models may erroneously identify other objects as those mentioned in the prompt.

**Redundant object identification.** A notable issue arises when the model fails to accurately discern distinct objects referenced in the prompt within the image. This often results in the erroneous

    & **Count of** & **Non-existent** & **Object** & **Scene** & **Text** & **Meta** \\  & **Object** & **Object** & **Attribute** & **Understanding** & **Recognition** & **Average** \\   \\  Ferret  & 0.00\% & 3.00\% & 0.00 \% & 9.57 \% & 7.8 \% & 3.85 \% \\ Kosmos2  & 13.12\% & 2.46\% & 12.50 \% & 9.65\% & 9.80 \% & 3.92\% \\ Yi-VL-3db  & 12.90\% & 8.44\% & 20.83\% & 11.50\% & 0.00\% & 9.17 \% \\ mPLUG-Owl2  & 34.38\% & 15.45\% & 29.17\% & 23.64 & 16.67\% & 17.41\% \\ MiniCPM-Llama3-v2.5  & 31.25\% & 17.96 \% & 12.50\% & 20.00\% & 22.00\% & 18.69\% \\ CogVLM-chät  & 23.33\% & 24.31 \% & 41.67\% & 27.19\% & 19.61\% & 24.80\% \\ Phi-3-vision  & 59.38\% & 25.29\% & 20.83\% & 31.86\% & 46.00 \% & 28.08\% \\ XComposer2-7b  & 56.25 \% & 29.88\% & 29.17\% & 30.43 \% & 27.45 \% & 30.65\% \\ InterVL-Chat-v1.5  & 56.25\% & 36.22\% & 26.09\% & 32.46\% & 49.0\% & 36.86 \% \\ LLaVA-NeXT-7b-vicuna  & 68.75\% & 39.43\% & 20.83\% & 51.30 \% & 28.00 \% & 40.73\% \\ DeepSeek-VL-7b-chat  & 40.62\% & 46.73\% & 29.17\% & 46.43 \% & 56.25 \% & 46.53\% \\ Idefics-2-8b  & 68.75\% & 51.81\% & 20.83\% & 40.00\% & 21.57 \% & 48.69\% \\ LLaVA-NeXT-13b-vicuna  & 68.75\% & 49.61\% & 29.17\% & 54.78\% & 36.00 \% & 49.65\% \\ LLaVA-NeXT-34b  & 41.94 \% & 51.76 \% & 25.00 \% & 56.14 \% & 26.53 \% & 50.05\% \\ Qwen-VL-Chat  & 45.16 \% & 77.52\% & 43.48 \% & 74.34 \% & 55.10 \% & 74.24\% \\    \\  Gemini-Pro  & 46.88\% & 47.16\% & 25.00 \% & 41.96\% & 34.00\% & 45.36\% \\ Reka  & 43.75\% & 46.08\% & 37.50\% & 51.30\% & 47.06\% & 46.46\% \\ GPT-do  & **81.25\%** & 82.77\% & 66.67 \% & 85.84\% & 76.47\% & 82.35\% \\ GPT-4V  & 51.61 \% & **83.16\%** & **70.83\%** & **89.29\%** & **88.24\%** & **82.82\%** \\   

Table 2: Evaluation results of a wide array of MLLMs on MAD-Bench.

Figure 3: Example failure cases of GPT-4V .

**Inference of non-visible objects.** The model occasionally attributes characteristics or actions to objects that are not visible in the image. This phenomenon appears to stem from the language model's reliance on its internal knowledge base to fabricate descriptions for objects mentioned in the prompt but absent in the visual data. Intriguingly, this occurs even when the model does not question the accuracy of its visual recognition capabilities, confidently affirming its findings while simultaneously describing non-existent objects.

**Inconsistent reasoning.** Throughout the response generation process, we observe the MLLMs oscillating between adhering to the deceptive information in the prompts and relying on their recognition of the actual content in the input image. Sentences in the generated response contradict each other. This inconsistency highlights a fundamental challenge in the model's decision-making process.

## 5 A Simple Remedy to Boost Performance

In this section, we introduce a simple yet effective method to enhance the robustness of MLLMs against deceptive prompts while ensuring output alignment with the corresponding input images. This enhancement is realized through the integration of an additional paragraph into the system's prompt, which is either prepended directly to the existing prompt, or incorporated differently, depending on the specific model. We composed this additional paragraph with the help of GPT-4, as shown in Appendix A.3.

It encourages the model to think twice or step by step before answering the question. The performance of several MLLMs after the incorporation of this prompt modification is presented in Table 3. For example, for LLAVA-NeXT-13b, it boosts the performance by +18.56%, although its absolute accuracy remains unsatisfactory. The enhanced MiniCPM-Llama3-v2.5 exhibited an impressive

Figure 4: Examples of mistakes made by Ferret  in face of deceptive prompts. We use Ferret responses here, as Ferret provides bounding boxes that unveil error types straightforwardly.

**Model** & **Count of** & **Non-existent** & **Object** & **Scene** & **Text** & **Meta** \\  & **Object** & **Object** & **Attribute** & **Understanding** & **Recognition** & **Average** \\  Pai-3-vision & 53.37\% (\%\)-(5.81\%) & 50.54\% (\(\)25.5\%) & 37.50\% (\(\)16.67\%) & 53.51\% (\(\)21.65\%) & 66.00\% (\(\)20\%) & 51.46\% (23.38\%) \\ DeepSee-VL-7b-chat & 44.83\% (\(\)4.21\%) & 62.32\% (\(\)15.59\%) & 47.83\% (\(\)18.66\%) & 61.82\% (\(\)15.39\%) & 48.00\% (\(\) 8.25\%) & 60.64\% (\(\)14.11\%) \\ LLAVA-NeXT-13b-vicuna & 45.16\% (\(\)23.59\%) & 71.33\% (\(\)21.72\%) & 37.50\% (\(\)8.33\%) & 74.11\% (\(\)19.33\%) & 38.00\% (\(\)20.06\%) & 68.21\% (\(\)18.56\%) \\ MiniCPM-Llama3-v2 & 16.67\% (\(\)14.58\%) & 85.85\% (\(\)67.89\%) & 62.50\% (\(\)50.00\%) & 86.61\% (\(\)66.61\%) & 68.63\% (\(\)46.63\%) & 82.25\% (\(\)63.56\%) \\ GPT-4v & 41.88\% (\(\)10.23\%) & 93.86\% (\(\)10.78\%) & 75.00\% (\(\)4.17\%) & 99.11\% (\(\)9.82\%) & 90.20\% (\(\)41.96\%) & 92.23\% (\(\)49.41\%) \\  

Table 3: Results of enhanced Phi-3-vision, DeepSee-VL-7b-chat, LLAVA-NeXT-13b-vicuna, MiniCPM-Llama3-v2.5, and GPT-4v on MAD-Bench after modifying the test prompt.

gain of 63.56% in accuracy, marking the largest performance increase among the five models tested. For GPT-4V, which already achieves an accuracy of 82.82%, using the proposed simple method can further boost the accuracy to 92.23%. Figure 5 provides examples to illustrate the capability of MiniCPM-Llama3-v2.5, GPT-4V, Phi3, and LLaVA-NeXT-13b to withstand deceptive prompts when supported by modifications made to the test prompt.

Overall, the addition of prompts to resist deceptive information appears to bolster the performance, enabling MLLMs to handle deception better and interpret scenes more accurately. This enhancement suggests that strategic prompt design could be a valuable approach to improving the robustness of AI models against attempts to mislead or confuse them. Note, that the implementation has not been fully optimized, and some MLLMs do not support this method due to reasons such as limitation of input sequence length. The primary goal of this exploration is to demonstrate the feasibility of enhancing performance with relatively minimal effort. This initial success highlights the potential for further refinement and optimization, which could lead to even more robust and capable AI models in the future.

## 6 Conclusion

In this study, we introduce MAD-Bench, a new benchmark comprising 1000 image-prompt pairs, meticulously categorized into five distinct types of deceptive scenarios, to evaluate the robustness of state-of-the-art MLLMs against deceptive prompts. Our findings indicate a notable vulnerability in these models. Though GPT-4V achieves the best performance, it still exhibits substantial room for improvement. We hope our new benchmark can stimulate further research to enhance models' resilience against deceptive prompts.

## Limitation

When designing deceptive questions for our benchmark, we included a variety of categories to increase the diversity of the questions as a starting point. However, there are unlimited scenarios where MLLMs can be deceived. The additional piece of prompt added to boost model performance in Section 5 serves the purpose of demonstrating that simple efforts can improve the robustness of MLLMs in face of deceptive information. It is not optimized, thus not showing the maximum capability of this method.

Figure 5: Model responses of MiniCPM-Llama3-v2.5 [, GPT-4V , Phi3 , and LLaVA-NeXT-13b  before and after modifying the test prompt. We add the (*) symbol to the original model name to denote the enhanced model.