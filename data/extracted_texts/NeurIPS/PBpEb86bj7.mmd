# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Explainable AI (XAI) methods target to elucidate the decision-making processes and internal workings of AI models to humans. State-of-the-art methods for transformers focus on propagating gradients back through the model. This backpropagation allows for the accumulation of influence of each input feature on the output by utilizing stored activations during the forward pass [6; 27]. Unfortunately, this caching also leads to significant overhead in memory consumption, which renders their productive deployment to be unconventional, if not impossible. Often half of the available memory of the GPU has to remain unused at inference, or it requires an entirely separate deployment of the XAI pipeline.

Fortunately, another popular XAI idea, namely _perturbation_[19; 24], is much more memory-efficient. However, it has not yet been proven beneficial for explaining the predictions of transformers, as the immense number of necessary forward trials to derive the explanation accumulate impractical computation time.

To tackle these issues and in turn, scale explanations with the size of transformers, we propose to bridge relevance propagation and perturbations. In contrast to existing perturbation methods--executing perturbations directly in the input space--AtMan pushes the perturbation steps via **a**ttention **m**anipulations throughout the latent layers of the transformer during the forward pass. This enables us--as we will show--to produce state interpolations, token-based similarity measures and to accurately steer the model predictions. Our explanation method leverages these predictions to compute relevancy values for transformer networks. Our experimental results demonstrate that AtMan significantly reduces the number of required perturbations, making them applicable at deployment time, and does not require additional memory compared to backpropagation methods. In short, AtMan can scale with transformers. Our exhaustive experiments on the text and image-text benchmarks and models demonstrate that AtMan outperforms current state-of-the-art gradient-based methods while being more computationally efficient. For the first time, AtMan allows one to study generative model predictions as visualized in Fig. 1a without extra deployment costs. During the sequence generation with large multi-modal models, AtMan is able to additionally highlight relevant features wrt. the input providing novel insights on the generation process.

Contributions.In summary, our contributions are: (i) An examination of the effects of token-based attention score manipulation on generative transformer models. (ii) The introduction of a novel and memory-efficient XAI perturbation method for large-scale transformer models, called AtMan, which reduces the number of required iterations to a computable amount by correlating tokens in the embedding space. (iii) Exhaustive multi-modal evaluations of XAI methods on several text and image-text benchmarks and autoregressive (AR) transformers.

We proceed as follows. We start by discussing related work in Sec. 2. In Sec. 3, we derive AtMan and explain its attention manipulation as a perturbation technique. Before concluding and discussing the benefits as well as limitations in Sec. 5, we touch upon our experimental evaluation Sec. 4,

Figure 1: **(a)** The proposed explainability method AtMan visualizes the most important aspects of the given image while completing the sequence, displayed above the relevance maps. The generative multi-modal model MAGMA is prompted to describe the shown image with: “<Image> This is a painting of ”. **(b)** The integration of AtMan into the transformer architecture. We multiply the modifier factors and the attention scores before applying the diagonal causal attention mask as depicted on the right-hand side. Red hollow boxes (–) indicate one-values, and green ones (–) -infinity. (Best viewed in color.)

showing that AtMan not only nullifies memory overhead and proves robust to other models but also outperforms competitors on several visual and textual reasoning benchmarks.

## 2 Related Work

Explainability in CV and NLP.The explainability of AI systems is still ambiguously defined . XAI methods are expected to show some level of relevance to the input with respect to the computed result of an algorithm. This task is usually tackled by constructing an input relevance map given the model's prediction. The nature of relevance can be class-specific, e.g., depending on specific target instances of a task and showing a "local solution" [26; 27], or class-agnostic, i.e., depending on the overall "global behavior" of the model only [1; 4]. The level of fine granularity in the achieved explanation thus depends on the selected method, model, and evaluation benchmark. Explainability in CV is usually achieved by mapping the relevance maps to a pixel level and treating the evaluation as a weak segmentation task [25; 20; 27]. On the other hand, NLP explanations are much more vaguely defined and usually mixed with more complex philosophical interpretations, such as labeling a given text to a certain sentiment category .

The majority of XAI methods can be divided into the classes of perturbation and gradient analysis. Perturbations treat the model as a black box and attempt to derive knowledge of the model's behavior by studying changes in input-output pairs only. Gradient-based methods, on the other hand, execute a backpropagation step towards a target and aggregate the model's parameter adoptions to derive insights. Most of these XAI methods are not motivated by a specific discipline, e.g., neither by NLP nor CV. They are so generic that they can be applied to both disciplines to some extent. However, architecture-specific XAI methods exist as well, such as GradCAM , leveraging convolutional neural networks' spatial input aggregation in the deepest layers to increase efficiency.

Explainability in transformers.Through their increasing size, transformers are particularly challenging for explainability methods, especially for architecture-agnostic ones. Transformers' core components include an embedding layer followed by multiple layers of alternating attention and feed-forward blocks Fig. 1b. The attention blocks map the input into separate "query", "key", and "value" matrices and are split into an array of "heads". As with convolutions in CNN networks, separated heads are believed to relate to specific learned features or tasks . Further, the attention mechanism correlates tokens wrt. the input sequence dimension. Care needs to be taken here as GPT-style generative models, in contrast to BERT-style embedding models, apply a causal masking of activations as we will highlight further below. This in particular might obscure results for gradient-based methods.

Consequently, most explainability adoptions to transformers focus on the attention mechanism. Rollout  assumes that activations in attention layers are combined linearly and consider paths along the pairwise attention graph. However, while being efficient, it often emphasizes irrelevant tokens, in particular, due to its class-agnostic nature. The authors also propose attention flow, which, however, is unfeasible to compute as it constructs exhaustive graphs. More recently, Chefer et al. (2021) proposed to aggregate backward gradients and LRP  throughout all layers and heads of the attention modules in order to derive explanation relevancy. Their introduced method outperforms previous transformer-specific and unspecific XAI methods on several benchmarks and transformer models. It is further extended to multimodal transformers  by studying other variations of attention. However, they evaluated only on classification tasks, despite autoregressive transformers' remarkable generative performance, e.g., utilizing InstructGPT  or multimodal transformers such as MAGMA , BLIP  and OFA .

Multimodal transformers.Contrarily to these explainability studies evaluated on object-detection models like DETR and ViT [5; 9], we study explainability on open-vocabulary tasks, i.p., generated text tokens of a language model, and not specifically trained classifiers. Due to the multimodality, the XAI method should produce output relevancy either on the input text or the input image as depicted in Fig. 1a. Specifically, to obtain image modality in MAGMA , the authors propose to fine-tune a frozen pre-trained language model by adding sequential adapters to the layers, leaving the attention mechanism untouched. It uses a CLIP  vision encoder to produce image embeddings. These embeddings are afterward treated as equal during model execution, particularly regarding other modality input tokens. This multi-modal open-vocabulary methodology has shown competitive performance compared to single-task solutions.

## 3 AtMan: Attention Manipulation

We formulate finding the best explainability estimator of a model as solving the following question: _What is the most important part of the input, annotated by the explanator, to produce the model's output?_ In the following, we derive our perturbation probe mathematically through studies of influence functions and embedding layer updates on autoregressive (AR) models [14; 2].

Then we show how attention manipulation on single tokens can be used in NLP tasks to steer a model's prediction in directions found within the prompt. Finally, we derive our multi-modal XAI method AtMan by extending this concept to the cosine neighborhood in the embedding space.

### Influence functions as explainability estimators

Transformer-based language models are probability distribution estimators. They map from some input space \(\), e.g., text or image embeddings, to an output space \(\), e.g., language token probabilities. Let \(\) be the space of all explanations, i.e., binary labels, over \(\). An explanator function can then be defined as \(e:()() \), in words, given a model, an input, and a _target_, derive a label on the input.

Given a sequence of words \(=[w_{1},,w_{N}]^{N}\), an AR language model assigns a probability to that sequence \(p()\) by applying factorization \(p()=_{t}p(w_{t}|w_{<t})\). The loss optimization during training can then be formalized as solving:

\[_{} p_{}()_{} =_{t} p_{}(w_{t}|w_{<t})_{^{t}}=_ {t}(h_{}(w_{<t})W_{}^{T})_{^{t}}\] (1) \[=:-_{t}L^{}(_{<t},)=:-L^{ }(,)\.\] (2)

Here \(h_{}\) denotes the model, \(W_{}\) the learned embedding matrix, and \(^{t}\) the vocabulary index of the \(t-th\) target token, with length \(||=N\). Eq. 1 is derived by integrating the cross-entropy loss, commonly used during language model training with \(=\). Finally, \(L\) denotes our loss function.

Perturbation methods study the influence of the model's predictions by adding small noise \(\) to the input and measuring the prediction change. We follow the results of the studies [14; 2] to approximate the perturbation effect directly through the model's parameters when executing Leaving-One-Out experiments on the input. The influence function estimating the perturbation \(\) of an input \(z\) is then derived as:

\[^{}(z_{},z)=}(z, _{})}{d}|_{=0} L^{}(z, _{-z_{}})-L^{}(z,)\.\] (3)

Here \(_{-z_{}}\) denotes the set of model parameters in which \(z_{}\) would not have been seen during training. In the following, we further show how to approximate \(_{-z_{}}\).

### Single token attention manipulation

The core idea of AtMan is the shift of the perturbation space from the raw input to the embedded token space. This allows us to reduce the dimensionality of possible perturbations down to a single scaling factor per input token. Moreover, we do not manipulate the value matrix of attention blocks and therewith do not introduce the otherwise inherent input-distribution shift of obfuscation methods. By manipulating the attention scores at the positions of the corresponding input sequence tokens, we are able to interpolate the focus of the prediction distribution of the model--amplifying or suppressing concepts of the prompt resulting in an XAI method as described in the following.

Attention was introduced in  as: \(=()\), where \(\) denotes matrix multiplication. The pre-softmax query-key attention scores are defined as

\[=^{T}/}.\]

In the case of autoregression, a lower left triangular unit mask **M** is applied to these scores as \(_{M}=\), with \(\) the Hadamard product. The output of the self-attention module is \(^{h s d_{h}}\), the query matrix is \(^{h s d_{h}}\) and \(,^{h s d_{h}}\) the keys and values matrices. Finally \(,,_{M}^{h s s}\). The number of heads is denoted as \(h\), and \(d_{h}\) is the embedding dimension of the model. Finally, \(s\) is the length input-sequence.

The perturbation approximation \(_{-z_{s}}\) required by Sec. 3.1 can now be approximated through attention score manipulation as follows: Let \(\) be an input token sequence of length \(||=n\). Let \(i\) be a token index within this sequence to be perturbated by a factor \(f\). For all layers and all heads \(_{u}\) we modify the pre-softmax attention scores as:

\[}_{u,,}=_{u,,}(- ^{i}),\] (4)

where \(^{s s}\) denotes the matrix containing only ones and \(^{i}\) the suppression factor matrix for token \(i\). In this section we set \(^{i}_{k,}=f\), for \(k=i\) and \(f\) and \(0\) elsewhere. As depicted in Fig. 0(b), we thus only amplify the \(i-th\) column of the attention scores of \(H\) by a factor \((1-f)\)--this, however, equally for all heads. We denote this modification to the model as \(_{-i}\) and assume a fixed factor \(f\).1 Note that this position in the architecture is somewhat predestified for this kind of modification, as it already applies the previously mentioned causal token-masking.

We define the explanation for a class label _target_ as the vector of the positional influence functions:

\[(,):=(^{}( _{1},),,^{}(_{n},))\,\] (5)

with \(^{}\) derived by Eq. 2,3 as \(^{}(_{i},):=L^{}( ,_{-i})-L^{}(,)\). In words, we average the cross-entropy of the AR input sequence wrt. all target tokens and measure the change when suppressing token index \(i\) to the unmodified one. The explanation becomes this difference vector of all possible sequence position perturbations and thus requires \(n\) forward passes.

Fig. 1(a) illustrates this algorithm. The original input prompt is the text "Ben likes to eat burgers and" for which we want to extract the most valuable token for the completion of the target token "fries". Initially, the model predicts the target token with a cross-entropy score of \(1.72\). Iterated suppression over the input tokens, as described, leads to "burgers" being the most-influential input token with the highest score of \(14.2\) for the completion.

Token attention suppression steers the model's prediction.Intuitively, for factors \(0<f<1\), we call the modifications "suppression", as we find the model's output now relatively less influenced by the token at the position of the respective manipulated attention scores. Contrarily, \(f<0\) "amplifies" the influence of the manipulated input token on the output.

An example of the varying continuations, when a single token manipulation is applied, can be seen in Fig. 1(b). We provide the model a prompt in which the focus of continuation largely depends on two tokens, namely "soccer" and "math". We show how suppressing and amplifying them alters the prediction distributions away from or towards those concepts. It is precisely this distribution shift we measure and visualize as our explainability.

Figure 2: **(a) Illustration of the proposed explainability method. First, we collect the original cross-entropy score of the target tokens (1). Then we iterate and suppress one token at a time, indicated by the red box (\(\)), and track changes in the cross-entropy score of the target token (2). **(b) Manipulation of the attention scores**, highlighted in blue, **steers the model’s prediction** into a different contextual direction. Note that we found measuring of such proper generative directions to perform better than radical token-masking, as we show later (c.f. Fig. 11). (Best viewed in color.)**

### Correlated token attention manipulation

Suppressing single tokens works well when the entire entropy responsible for producing the target token occurs only once. However, for inputs with redundant information, this approach would often fail. This issue is especially prominent in Computer Vision (CV), where information like objects in an image is often spread across multiple tokens--as the image is usually divided into patches, each of which is separately embedded. It is a common finding that applied cosine similarity in the embedding space, i.p., for CLIP-like encodings as used in MAGMA, gives a good correlation estimator [17; 2]. This is in particular due to the training by a contrastive loss. We integrate this finding into AtMan in order to suppress all redundant information corresponding to a particular input token at once, which we refer to as correlated token suppression.

Fig. 2(a) summarizes the correlated token suppression visually. For \(n\) input tokens and embedding dimension \(d\), the embedded tokens result in a matrix \(T=(t_{i})^{n d}\). The cosine similarity is computed from the normalized embeddings \(=(_{i})\), with \(_{i}=t_{i}/\|t_{i}\|\), for \(i 1, n\), as \(S=(s_{i})=^{}[-1,1]^{n n}\). Note that the index \(i\) denotes a column corresponding to the respective input token index. Intuitively, the vector \(s_{i}\) then contains similarity scores to all input tokens. In order to suppress the correlated neighborhood of a specific token with the index \(i\), we, therefore, adjust the suppression factor matrix for Eq. 4 as

\[}_{u,*,*}=_{u,*,*}((-f)+f( -_{k,*}^{i}))_{k,*}^{i}=s_{i,k},& s_{i,k} 1,\\ 0,&\] (6)

In words, we interpolate linear between the given _suppression factor_\(1-f\) and \(1\), by factor of the thresholded cosine similarity \(_{k,*}^{i}\). As we only want to suppress tokens, we restrict the factor value range to greater than \(0\) and smaller than \(1\). The parameter \(>0\)2 is to ensure a lower bound on the similarity, and in particular, prevents a flip of the sign. Overall, we found this to be the most intuitive and robust equation. In Fig. 11 we ran a sweep over the hyper-parameters to obtain the best precision and recall values. Note in particular, that our modifications are rather subtle compared to entire token-masking. The latter did not lead to satisfying results, even given the context. We compute the similarity scores once on the initial embedding layer, and uniformly apply those modifications throughout all layers.

Note that variations of Eq. 4, 5 and 6 could also lead to promising results. In particular in the active research field of vision transformers, hierarchical token transformations could be applicable and speed

Figure 3: **(a) Correlated token suppression of AtMan enhances explainability in the image domain.** i) Shows an input image along with three perturbation examples (\(A,B,C\)). In \(A\), we only suppress a single image token (blue). In \(B\), the same token with its relative cosine neighborhood (yellow), and in \(C\), a non-related token with its neighborhood. Below depicted are the changes in the cross-entropy loss. The original score for the target token “panda” is denoted by \(c_{0}\) and the loss change by \(\). ii) Shows the resulting explanation without Cosine Similarity (CS) and with CS. We evaluated the influence of the CS quantitatively in Fig. 11(a). **(b)** An example of the **SQUAD dataset with AtMan explanations.** The instance contains three questions for a given context, each with a labeled answer pointing to a fragment of the context. AtMan is used to highlight the corresponding fragments of the text responsible for the answer. It can be observed that the green example is full, the blue in part, and the yellow is not at all recovered according to the given labels. However, the yellow highlight seems at least related to the label. (Best viewed in color.)up computations (c.f. , Fig 16). Furthermore, preliminary experiments using the similarity between the query and key values per layer, a different similarity score such as other embedding models or directly sigmoid values, or even a non-uniform application of the manipulations yield promising, yet degenerated performance (c.f. App. A.15). As we could not conclude a final conclusion yet, we leave these investigations to future research.

With that, we arrived at our final version of AtMan. Note that this form of explanation \(\) is "local", as _target_ refers to our target class. We can, however, straightforwardly derive a "global explanation" by setting _target_ = **y**, for **y**, a model completion to input **w** of a certain length. It could then be interpreted rather abstract as the model's general focus .

## 4 Empirical Evaluation

We ran empirical evaluations on text and image corpora to address the following questions: (**Q1**) Does AtMan achieve competitive results compared to previous XAI methods for transformers in the language as well as vision domain? (**Q2**) Is AtMan applicable to various transformer models, and does AtMan scale efficiently, and can therefore be applied to current large-scale AR models?

To answer these questions, we conducted empirical studies on textual and visual XAI benchmarks and compared AtMan to standard approaches such as IxG , IG , GradCAM  and the transformer-specific XAI method of  called Chefer in the following. Note that all these methods utilize gradients and, therefore, categorize as propagation methods leading to memory inefficiency. We also tried to apply existing perturbation methods such as LIME  and SHAP  and Attention Rollout . However, they failed due to extremely large trials and, in turn, prohibitive computation time, or qualitatively (c.f. Appendix A.9). We adopt common metrics, namely mean average precision (mAP) and recall (mAR) (c.f. Appendix A.11), and state their interquartile statistics in all experiments. To provide a comparison between the XAI methods, we benchmarked all methods on GPT-J  for language and MAGMA-6B  for vision-language tasks. Through its memory efficiency, AtMan can be deployed on really large models. We further evaluate AtMan on a MAGMA-13B and 30B to show the scaling behavior, and, on BLIP  to show the architecture independence. To obtain the most balanced comparison between the methods, we selected datasets focusing on pure information recall, separating the XAI evaluation from the underlying model interpretations.

### AtMan can do language reasoning

Protocol.All experiments were performed on the open-source model GPT-J . Since with AtMan we aim to study large-scale generative models, we formulate XAI on generative tasks as described in Sec. 3.3. For evaluation, we used the Stanford Question Answering (QA) Dataset (SQuAD) . The QA dataset is structured as follows: Given a single paragraph of information, there are multiple questions, each with a corresponding answer referring to a position in the given paragraph. A visualization of an instance of this dataset can be found in Fig. 2(b). SQuAD contains 536 unique paragraphs and 107,785 question/explanation pairs. The average context sequence length is \(152.7\) tokens and the average label length is \(3.4\).

The model was prompted with the template: "{Context} Q: {Question} A:", and the explainability methods evaluated on the generations to match the tokens inside the given context, c.f. Fig. 2(b), Sec. 3.3. If there were multiple tokens in the target label, we computed the mean of the generated scores.

Figure 4: AtMan **produces less noisy and more focused explanations when prompted with multi-class weak segmentation** compared to Chefer. The three shown figures are prompted to explain the target classes above and below separately. (Best viewed in color.)

Similar to weak segmentation tasks in computer vision, we regarded the annotated explanations as binary labels and determined precision and recall over all these target tokens.

Results.The results are shown in Tab. 0(a). It can be observed that the proposed AtMan method thoroughly outperforms all previous approaches on the mean average precision. This statement holds as well for the interquartile mean of the recall. However on the entire recall, Chefer slightly outperforms AtMan. Note that the high values for recall scores throughout all methods are partly due to the small average explanation length, such as depicted in Fig. 2(b). Further details and some qualitative examples can be found in Appendix A.2.

Paragraph chunking.AtMan can naturally be lifted to the explanation of paragraphs. We ran experiments for AtMan dividing the input text into several paragraphs by using common delimiters as separation points and evaluating the resulting chunks at once, in contrast to token-wise evaluations. This significantly decreases the total number of required forward passes and, on top, produces "more human" text explanations of the otherwise still heterogeneously highlighted word parts. Results are shown in Appendix A.8.

### AtMan can do visual reasoning

Protocol.Similar to language reasoning, we again perform XAI on generative models. We evaluated the OpenImages  dataset as a VQA task and generated open-vocabulary prediction with the autoregressive model. Specifically, the model is prompted with the template: "[Image] This is a picture of ", and the explainability methods executed to derive scores for the pixels of the image on the generations with respect to the target tokens. If there were multiple tokens in the target label, we computed the mean of the generated scores. For evaluation, we considered the segmentation annotations of the dataset as ground truth explanations. The segmentation subset contains 2,7M annotated images for 350 different classes. In order to ensure a good performance of the large-scale model at hand and, in turn, adequately evaluate only the XAI methods, we filtered the images for a minimum dimension of \(200\) pixels and a maximal proportional deviation between width and height of \(20\%\). We randomly sample \(200\) images per class on the filtered set to further reduce evaluation costs. In total, this leads to a dataset of \(27.871\) samples. The average context sequence length is \(144\) tokens, and the average label coverage is \(56\%\) of the input image. To provide a fair comparison between all XAI methods, we compute token-based attributions for both AtMan and Chefer, as well as IG, IxG, and GC. Note that the latter three allow pixel-based attribution granularity, however, these resulted in lower performance, as further discussed in Appendix A.2.

Results.The results are shown in Tab. 0(b). It can be observed that AtMan thoroughly outperforms all other XAI approaches on the visual reasoning task for all metrics. Note how explicit transformer XAI methods (AtMan, Chefer) in particular outperform generic methods (GradCAM, IG, IxG) in recall. Moreover, while being memory-efficient (c.f. next section), AtMan also generates more accurate explanations compared to Chefer. Through the memory efficiency of AtMan, we were able to evaluate a 13B and 30B upscaled variant of MAGMA (c.f. Tab. 0(c)). Interestingly, the general explanation performance slightly decreases, when upscaled, compared to the 6B model variant. However, throughout all variants AtMan scored comparable high. Note that the overall benchmark performances of the models slightly increase (c.f. App. A.14). This behavior could be attributed to

Table 1: **AtMan (AM) outperforms other XAI methods on (a) the text QA benchmark SQuAD and (b) the image-text VQA task of OpenImages. Shown are the (interquartile) mean average precision and recall (the higher, the better). The best and second best values are highlighted with \(\) and \(\). Evaluations are on a 6B model. (c) ATMan evaluated on other model types: MAGMA variants (indicated by their parameter size) and BLIP.**the increased complexity of the model and, subsequently, the complexity of the explanation at hand. Hence, the "human alignment" with the model's explanations is not expected to scale with their size.

Model agnosticism.Note that the underlying MAGMA model introduces adapters on a pre-trained decoder text model to obtain the image multi-modality. It is yet unclear to what extent adapters change the flow of information in the underlying model. This may be a reason for the degenerated performance of gradient-based models. As AtMan however outperforms such methods, we may conclude that information processing still remains bound to the specific input-sequence position. This finding generalizes to encoder architectures, as experiments with BLIP suggest in Tab. Ic. Precisely, BLIP's multi-modality is achieved by combining an image-encoder, a text-encoder, and a final text-decoder model. These are responsible for encoding the image, the question, and finally for generating a response separately. Information between the models is passed through cross-attention. We only applied AtMan to the image-encoder model of this architecture (c.f. App. A.10). The results demonstrate the robustness of the suggested perturbation method over gradient-based approaches.

Qualitative illustration.Fig. 4 shows several generated image explanations of AtMan and Chefer for different concepts. More examples of all methods can be found in Appendix A.7 and evaluation on the more complex GQA dataset  is discussed in Appendix A.12. We generally observe more noise in gradient-based methods, in particular around the edges. Note that AtMan evaluates the change in its perturbated generations. These are independent of the target tokens. Compared to other methods like Chefer, we, therefore, do not need to process the prompt more than once for VQA on different object classes with AtMan. Note again that the aforementioned cosine similarity is crucial to obtain these scores. Quantitative results can be found in App. A.4.

In general, the results clearly provide an affirmative answer to **(Q1)**: AtMan is competitive with previous XAI methods, and even outperforms transformer-specific ones on the evaluated language and vision benchmarks for precision and recall. Next, we will analyze the computational efficiency of AtMan.

### AtMan can do large scale

While AtMan shows competitive performance, it computes, unlike previous approaches, explanations at almost no extra memory cost. Fig. 5 illustrates the runtime and memory consumption on a single NVIDIA A100 80GB GPU. We evaluated the gradient-based transformer XAI method  and AtMan. The statistics vary in sequence lengths (colors) from 128 to 1024 tokens, and all experiments are executed sequentially with batch size 1 for best comparison.

One can observe that the memory consumption of AtMan is around that of a plain forward pass (Baseline; green) and increases only marginally over the sequence lengths. In comparison, the method of --and other gradient-based methods--exceeds the memory limit with more than double in memory consumption. Therefore, they fail on larger sequence lengths or models.

Figure 5: **AtMan scales efficiently.** Performance comparison of the XAI methods AtMan and Chefer _et al._, on various model sizes (x-axis) executed on a single 80GB memory GPU. Current gradient-based approaches do not scale; only AtMan can be utilized on large-scale models. Solid lines refer to the GPU memory consumption in GB (left y-axis). Dashed lines refer to the runtime in seconds (right y-axis). Colors indicate experiments on varying input sequence lengths. As baseline (green) a plain forward pass with a sequence length of 1024 is measured. The lightning symbol emphasizes the non-deployability when memory resource is capped to 80GB. (Best viewed in color.)

Whereas the memory consumption of AtMan stays is almost constant, the execution time significantly increases over sequence length when no further token aggregation is applied upfront. Note, that Fig. 5 shows complete and sequential execution. The exhaustive search loop of AtMan can be run partially and in parallel to decrease its runtime. In particular, the parallelism can be achieved by increasing the batch size and naturally by a pipeline-parallel execution. For instance, since large models beyond 100B are scattered among nodes and thus many GPUs, the effective runtime is reduced by magnitudes to a proximate scale of the forward pass. Furthermore, tokens can be aggregated in chunks and evaluated on a more coarse level, which further reduces the runtime drastically, as demonstrated in Fig. 16.

In Fig. 6 we provide performance results on a realistic scenario. It compares a practical query (average 165 tokens) on the task of explaining a complete image (144) tokens. Runtimes are measured with pipe-parallel 4, and AtMan additionally with batch size 16. Specifically, AtMan requires between 1 and 3 total compute seconds, around 1 magnitude longer compared to Chefer. Note that this can still further be divided by the number of available idling workers, to reduce the absolute clock time to subseconds. Each batch of AtMan can be processed entirely parallel. Moreover Chefer, even with memory optimizations, fails to scale to 34b with the given memory limit (80GB).

Overall, these results clearly provide an affirmative answer to (**Q2**): Through the memory efficiency of AtMan, it can be applied to large-scale transformer-based models.

## 5 Conclusion

We proposed AtMan, a modality and architecture agnostic perturbation-based XAI method for generative transformer networks. In particular, AtMan reduces the complex issue of finding proper input perturbations to a single scaling factor per token. As our experiments demonstrate, AtMan outperforms current approaches relying on gradient computation. It moreover performs well on decoder as well as encoder architectures, even with further adjustments like adapters or crossattention in place. Most importantly, through AtMan's memory efficiency, it provides the first XAI tool for deployed large-scale AR transformers. We successfully evaluated a 30B multi-modal model on a large text-image corpus.

Consequently, AtMan paves the way to evaluate whether models' explanatory capabilities scale with their size, which should be studied further. Besides this, our paper provides several avenues for future work, including explanatory studies of current generative models impacting our society. Furthermore, it could lay the foundation for not only instructing and, in turn, improving the predictive outcome of autoregressive models based on human feedback  but also their explanations .