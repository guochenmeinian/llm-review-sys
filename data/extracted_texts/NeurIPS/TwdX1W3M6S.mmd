# Online Iterative Reinforcement Learning from Human Feedback with General Preference Model

Chenlu Ye

Equal contributions with random author order. Correspondence to Wei Xiong. University of Illinois Urbana-Champaign. Email: chenluy3@illinois.edu

Wei Xiong

University of Illinois Urbana-Champaign. Email: wx13@illinois.edu

Yuheng Zhang

University of Illinois Urbana-Champaign. Email: yuhengz2@illinois.edu

Hanze Dong

Salesforce AI Research. Email: hanze.dong@salesforce.com

Nan Jiang

University of Illinois Urbana-Champaign. Email: nanjiang@illinois.edu

Tong Zhang

University of Illinois Urbana-Champaign. Email: longzhang@tongzhang-ml.org

###### Abstract

We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offline learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework.

## 1 Introduction

_Reinforcement Learning from Human Feedback_ (RLHF) has emerged as a pivotal technique in adapting machine learning to leverage relative feedback, especially in aligning Large Language Models (LLMs) with human values and preferences . Notable examples include ChatGPT , Claude , and Bard . The primary goal of RLHF in the context of LLMs is to adjust the responses generated by LLMs so that they are more favorably received by human evaluators.

Inspired by the standard LLM alignment workflow , we characterize an LLM by a policy \(\), which takes a prompt \(x\) and produces a response \(a\) from the distribution \((|x)\). In a typical LLM training pipeline , the tuning process begins with a pretrained model, which is subsequently fine-tuned using specialized and instructional data to produce an initial model \(_{0}\). The initial model \(_{0}\) is then aligned with a prompt set from some distribution \(x d_{0}\). The key component in RLHF is the _General Preference Oracle_, which is mathematically defined as follows.

**Definition 1** (General Preference Oracle).: _There exists a preference oracle \(:\), and we can query it to receive the preference signal:_

\[y((a^{1} a^{2}|x,a^{1},a^{2})\]

_where \(y=1\) means \(a^{1}\) is preferred to \(a^{2}\), and \(y=0\) means that \(a^{2}\) is preferred._Instead of directly optimizing against the preference oracle \(\), the existing prevalent RLHF framework is reward-based [50; 60], which consists of three steps: (1) preference data collection, (2) reward modeling, and (3) policy optimization. Specifically, the preference dataset \(\) consists of multiple tuples of the form \((x,a^{1},a^{2},y)\), whose collection process can be modeled as:

\[x d_{0},a^{1}_{D}^{1},a^{2}_{D}^{2}, y a^{1} a^{2}|x,a^{1},a^{2},\] (1)

where \(_{D}^{1}\) and \(_{D}^{2}\) are behavior policies and are typically set as \(_{0}\)[60; 43] or some powerful closed-form LLMs . The second step is reward modeling, which is the origin of the name "reward-based". This step can be viewed as a kind of inverse RL , which models some difficult-to-specify goals (preferred by the human or AI evaluators) as a scalar reward signal. Specifically, the Bradley-Terry (BT) model , a framework widely adopted in Ouyang et al. , Bai et al. , Touvron et al. , Rafailov et al. , Xiong et al. , assumes that there exists a ground-truth reward function \(P^{*}\) and the preference model satisfies:

\[(a^{1} a^{2}|x,a^{1},a^{2})=(x,a^{1}))}{(R ^{*}(x,a^{1}))+(R^{*}(x,a^{2}))}=R^{*}(x,a^{1})-R^{*}(x,a^{2 }),\] (2)

where \((z)=1/(1+(-z))\) is the sigmoid function. Then, the reward model is taken as the Maximum Likelihood Estimation (MLE) of the BT model on the preference dataset \(\)[e.g., 51; 48; 50; 4; 60] and is used in subsequent policy optimization steps to provide a signal for algorithms like Proximal Policy Optimization . Despite its successes, the existence of a reward function and the BT model are strong assumptions, which may not fully capture the complicated human preferences. In particular, the BT model assumes that human preference is transitive, which means that if we prefer A to B (\((A B|x,A,B)>0.5\)) and we prefer B to C, then it automatically holds that we prefer A to C. This assumption, however, is contradicted by evidence of intransitivity in human decision-making [62; 45]. This limitation is particularly pronounced if we consider the population-level preferences, where the ultimate preference signal is aggregated across diverse human groups . This may further be evidenced that in the practical RLHF, the accuracy of the learned BT model is around \(70\%\)[4; 60; 16], suggesting the challenges in approximating the complicated human preference by BT model. While there are some recent efforts to bypass reward modeling [53; 85], they are still fundamentally derived from the reward-based preference model and suffer from the aforementioned issues.

In contrast, the general preference oracle defined in Definition 1 is strictly more general than the BT model and can capture a more complicated preference pattern from the definition itself. It allows an intransitive preference model and can further capture the preference feedback from AI , with a notable example of GPT-4 , which is widely used for model evaluations in practice and may more accurately reflect real user experience [60; 18; 53; 72]. Moreover, from a practical side, the preference model construction tends to be more efficient than the reward function in terms of ranking accuracy. This is evidenced by the fact that the preference model, pairRM with 0.4B parameters , performs comparably to a LLaMA2-13B-based reward model across a diverse set of preference targets . As a case study, we train a reward model based on the Bradley-Terry (BT) model and a preference model with the same starting checkpoint Gamma-2B-it  and preference dataset8, with results presented in Table 1 and the training details are deferred to Section 5. As we can see, the preference model achieves much higher test accuracy in the reasoning task while maintaining comparable results in other tasks. Meanwhile, the training set we use is rather limited in the reasoning data (math and coding), so the reasoning task can be

  
**Base Model** & **Method** & **Chat** & **Chat Hard** & **Safety** & **Reasoning** \\   Gamma-2B-it & BT & 95.0 & 40.8 & 81.2 & 74.2 \\ Gamma-2B-it & Preference & 96.0 & 40.5 & 82.8 & 80.7 \\  LLaMA3-8B-it & BT & 99.4 & 65.0 & 87.7 & 87.8 \\ LLaMA3-8B-it & Preference & 98.9 & 65.2 & 89.5 & 94.8 \\   

Table 1: Comparison of the test accuracy between the BT-based reward model and the preference model. The reward model and preference model are trained with the same base model and preference dataset, where the details are deferred to Section 5. We evaluate the model on Reward-Bench .

viewed as an out-of-distribution task. In this sense, the preference model may also provide a better generalization compared to the reward model. The results also extend to another case study with LLaMA3-8B-instruct, where the preference model shows promising potential in the improvement of reasoning tasks. We refer interested readers to check Zhao et al. , Liu et al.  for further examples with similar observations. The advantage in ranking accuracy is not only directly beneficial for the algorithms that depend on ranking information [18; 30], but also improves the performance of algorithms derived from the reward-based framework (i.e., Bradley-Terry model), as evidenced by the results in the study of (iterative) DPO [72; 31].

Given all these considerations, our study focuses on exploring the theoretical properties of RLHF under the general preference oracle (Definition 1), with the goal of advancing practical algorithmic designs. We summarize our contributions as follows:

* We make the first attempt to study the theoretical learnability of RLHF under general preference oracle with KL regularization, in the offline setting with a pre-collected preference dataset and the online setting where we can query human feedback along the way of training, which demonstrates the potential of reward-model-free learning under general preference;
* We propose sample-efficient algorithms in both the offline setting and online setting and establish the finite-sample theoretical guarantees under standard coverage and exploration conditions;
* We show that the theoretical insights can be used to guide practical algorithmic designs with a reasonable approximation of the computational oracle.

## 2 Problem Formulation

In this section, we formulate the RLHF with general preference learning. Suppose that there exists a preference function \(P^{*}:\) which represents the prefererence of one action \(a^{1}\) over another \(a^{2}\) given a prompt \(x\): \(P^{*}(x,a^{1},a^{2})=(a^{1} a^{2}|x,a^{1},a^{2})\). In practical applications, we want to make the resulting LLM \(\) close to \(_{0}\)[90; 50; 4; 53]. Therefore, we adopt the following KL-regularized objective:

\[J(^{1},^{2})=_{x d_{0}}_{a^{1}^{1},a^{2} ^{2}}P^{*}(x,a^{1},a^{2})-^{-1}D_{}(^{1}( |x))+^{-1}D_{}(^{2}(|x)\|_{0}(|x)).\] (3)

One primary reason to consider the regularized target is that the constructed preference model is only _locally_ accurate, i.e., it performs well when there is little distribution shift. For instance, if the preference model is fine-tuned on a preference dataset collected by the initial model \(_{0}\), it improves the in-distribution generalization, but the resulting model often performs poorly out-of-distribution . Meanwhile, even if we require human labelers to give feedback along the way, the choices of the labelers may not be representative enough or the labelers can make mistakes due to limited time, attention, or care . Moreover, the KL divergence in the target ensures that the resulting policy is stochastic instead of deterministic (given a suitable initial checkpoint), thereby more accurately reflecting the dynamics of generative language models.

We choose \(P^{*}\) as the target mostly for historical reasons [22; 65]. A choice is the relative preference \((P^{*}(x,a^{1},a^{2})/(1-P^{*}(x,a^{1},a^{2})))\), which is equal to \(R^{*}(x,a^{1})-R^{*}(x,a^{2})\) when the BT model holds so that (3) becomes two decoupled regularized-reward maximization problems in this case and automatically reduces to the setting considered in the previous work Xiong et al. . While we do not handle this target directly, the analysis techniques presented in this paper readily apply to it with slight modifications.

**Nash Equilibrium and Best Response.** Without loss of generality, we restrict our attention to the policy class \(\) consisting of the policies with the same support as \(_{0}\) and denote the _unique_ Nash equilibrium (known as the Minimax Winner [57; 38; 24] or the von Neumann Winner ) as the solution of the following minimax problem as:

\[(_{*}^{1},_{*}^{2})=(_{*},_{*})=*{argmax}_{^{1} }*{argmin}_{^{2}}J(^{1},^{2}),\] (4)

where the Nash policies of two players coincide as we prove in Lemma 4. In the rest of this paper, we still use the notation \((_{*}^{1},_{*}^{2})\) to distinguish between the max-player and min-player. Accordingly, we refer to the first LLM \(^{1}\) as the _max-player_, while the second LLM \(^{2}\) is the _min-player_. We also define the notion of _best response_. For function \(J\) and policy \(^{1}\), the best response to \(^{1}\) is defined as \(*{argmin}_{^{2}}J(^{1},^{2})\) and the value is denoted by \(J(^{1},)=_{^{2}}J(^{1},^{2})\). Similarly, for \(^{2}\), we have \(J(,^{2})=_{^{1}}J(^{1},^{2})\). In particular, since \(^{1}_{*}\) and \(^{2}_{*}\) are the Nash equilibrium, they are the best response to each other.

**Function Approximation.** Suppose that we have access to a function class \(()\) (e.g. neural network), which provides us with a set of candidates to approximate the \(P^{*}\), and also the preference functions \(P\) satisfies \(P(x,a^{1},a^{2})=1-P(x,a^{2},a^{1})\). We make the following assumptions on the class \(\).

**Assumption 1**.: _Assume that \(\) is finite and the capacity of the class is large enough so that \(P^{*}\)._

The finite class assumption is for a clear presentation and the results readily generalize to an infinite class with a bounded covering number by the standard discretization technique. We define a theoretical computation oracle as follows and defer the practical implementations to the experiment section.

**Definition 2** (Nash Equilibrium Oracle).: _For a given preference function \(P\) and a reference policy \(_{0}\), we can compute the Nash Equilibrium policy_

\[_{P}=*{argmax}_{^{1}}_{^{2}}_ {x d_{0}}_{a^{1}^{1},a^{2}^{2}}P(x,a^{1},a ^{2})-^{-1}(a^{1}|x)}{_{0}(a^{1}|x)}+^{-1} (a^{2}|x)}{_{0}(a^{2}|x)}.\] (5)

**Learning Objective.** The goal is to find an \(\)-approximate Nash policy \(^{1}\) for the max-player:

\[J(^{1}_{*},^{2}_{*})-J(^{1},)=J(^{1}_{*},^{2}_{*} )-_{^{}}J(^{1},^{}),\]

which means that the max-player is consistently preferred by the KL-regularized preference in the face of any competing policy \(^{}\) up to a relaxation of \(\). To stress the non-symmetric structures of the two players, we refer to the max-player as the _main agent_, which aims to find her \(\)-approximate Nash policy, and refer to the min-player as the _enhancer_, which is designed to facilitate the main agent's learning. In particular, when \(\) is large enough so that the KL is roughly omitted, then, we can further obtain that

\[_{^{2}}_{x d_{0}}_{a^{1}^{1 },a^{2}^{2}}P^{*}(x,a^{1},a^{2}) 0.5-.\]

In this case, the obtained policy \(_{1}\) is consistently preferred by the preference oracle \(P^{*}\) against any competing policies. We mention in passing that the KL penalty coefficient \(>0\) exhibits a trade-off between being preferred by the oracle \(P^{*}\) and staying close to the initial model \(_{0}\), and reflects the degree of our belief in the oracle \(P^{*}\). In practice, \(\) is typically treated as a hyper-parameter and is adjusted by parameter search .

Compared to the previous literature formulating the preference learning as finding a Nash equilibrium, although we focus on optimizing the policy for the max-player, we can also have a duality gap guarantee because of the symmetry of the objective function: \(J(^{1},^{2})=1-J(^{2},^{1})\). To see this, we decompose the duality gap into the suboptimality for the max-player \(^{1}\) and the min-player \(^{2}\):

\[J(,^{2})-J(^{1},)= J(,^{2})-J(^{1}_{*},^{2}_{*})+J(^{1}_{*},^{2}_{*} )-J(^{1},)\] \[= J(^{1}_{*},^{2}_{*})-J(^{2},)+J(^{1}_{*},^{2}_{*})-J(^{2},).\]

If we obtain such an \(\)-suboptimal max player \(^{1}\), by taking the min-player \(^{2}=^{1}\), the duality gap \(J(,^{2})-J(^{1},)\) is naturally bounded by \(2\).

**Notations.** We use the short-hand notation \(=(^{1},^{2})\) when there is no confusion. We use \(P(x,^{1},^{2})\) to represent \(_{a^{1}^{1},a^{2}^{2}}[P(x,a^{1},a^{2})]\). We use \(J(x,^{1},^{2})\) to denote the objective function in (3) without the expectation over the prompt \(x d_{0}\). Let \((x)\) denote the sigmoid function \(1/(1+e^{-x})\). We also provide a notation table in Table 4 to improve the readability of this paper.

Due to space constraints, the review of the related literature is deferred to Appendix 7.

## 3 Improved Algorithms in Offline Setting

### Setup

In the offline setting, our goal is to learn a good policy from a pre-collected dataset \(_{}=\{(x_{i},a^{1}_{i},a^{2}_{i},y_{i})\}_{i=1}^{n}\) without further query with the oracle \(\), where comparison sample is assumed to be independently collected as in (1). We measure the suboptimality of the learned policy \(^{1}\) by the gap between the Nash value and the best response value:

\[J(_{1}^{*},_{2}^{*})-J(^{1},),\] (6)

where the KL-regularized function \(J\) is defined in (3). Similar to the reward-based framework , one natural approach is a two-staged method: (1) Construct an empirical preference model (reward model in the literature) by maximizing the log-likelihood function:

\[_{_{}}(P)=_{(x,a^{1},a^{2},y) _{}}y P(x,a^{1},a^{2})+(1-y) P(x,a^{2},a^{1});\] (7)

(2) Solve the policy by plugging the learned preference model \(\) into the Nash Equilibrium Oracle 2. However, this framework typically leads to severe reward over-optimization issue , meaning that while the model is preferred by the learned \(\), it may not achieve good performance under the evaluation of \(P^{*}\). This is because, with finite \(_{}\) drawn from some behavior policy, it is unlikely to provide an accurate estimation for all the prompt-response pairs. Therefore, imposing heavy optimization pressure toward \(\) will push the model to exploit these unreliable estimations to chase for a high proxy metric, thus leading to a worse performance under the ground truth \(P^{*}\).

### Learning with Pessimism

The recent advances in the offline RL theory have demonstrated that the principle of pessimism with a conservative estimation is statistically efficient for offline learning across a diverse set of scenarios [35; 54; 69; 79; 86; 17; 71; 84]. In this section, we connect the KL-reversed minimax game in (3) with offline RL by pessimism via version space9.

We introduce our algorithm, Pessimistic Equilibrium Learning from Human Feedback (PELHF) in Algorithm 1. Given an offline dataset \(_{}\), we first obtain the maximum likelihood estimation (MLE) \(\) by maximizing (7). Rather than directly planning with this empirical \(\), we form a version space \(}\) that contains \(P^{*}}\) with a high probability under a suitable choice of \(\), as we show in Lemma 1. For each policy \(^{1}\), we take the minimum preference function over \(}\) and the best responded \(^{2}\) as its conservative value estimation:

\[_{}(^{1})=_{^{2}}_{P}_{x d_{0}}_{a^{1}^{1},a^{2}^{2}}P(x,a^{1},a^{2})+^{-1}(a^{1}|x)}{^{1}(a^{1}|x)}-^{-1} (a^{2}|x)}{^{2}(a^{2}|x)}.\]

Then, we solve the minimax game concerning this conservative value estimator. With this pessimistic modification, the resulting algorithm enjoys the following theoretical guarantee.

**Theorem 1**.: _[Proof If Assumption 1 holds, and we set \(=(||/)\) and \(^{2}=2(||/)\), then, with probability at least \(1-\), the output policy of Algorithm 1 satisfies_

\[J(_{1}^{*},_{2}^{*})-J(^{1},) 4, _{D},)/n}.\]

_where the coverage coefficient_

\[(_{*}^{1},_{D},)=_{^{2}}_{P }_{x d_{0}}[P(x,_{*}^{1},^{2})-(x,_{*}^{1},^{2})])^{2}}{_{x d_{0},a^{1}_{D}^{1},a^{2 }_{D}^{2}}(P(x,a^{1},a^{2})-(x,a^{1},a^{2}))^{2}}.\]This theorem shows that the suboptimality gap depends on how the target \((^{1}_{*},^{2})\) is covered by the offline dataset, where \(^{2}\) is maximized over the policy set \(\). This coverage coefficient resembles the unilateral coverage10 for Markov games . Then, a natural question is whether a good coverage condition (\((^{1}_{*},_{D},)\) is small) is practical in the context of LLMs. Unfortunately, since the response is usually long in practice, the distribution shift between policies is also very large. We summarize some observations here. First, along the way of the RLHF training, the average density ratio \((a|x)/_{0}(a|x)>(25)\) as reported in Figure 13 of Bai et al. . See similar results of rejection sampling fine-tuning  and DPO . Second, for a case study, we use the Gemma-7B-it as the behavior policy to collect data for aligning Gemma-2B-it  with 15k prompt from . Then, we calculate the average KL divergence between Gemma-7B-it and Gemma-2B-it as \(456.4\). This evidence indicates that the coverage coefficient probably explodes in realistic scenarios. Therefore, it is unlikely to expect that we can learn the optimal policy from a pre-collected dataset. This motivates us to consider the online setting, where we can further query the preference oracle during the training to enrich the dataset thus enhancing our models continuously.

## 4 Iterative RLHF with Online Exploration

### Setup of Iterative RLHF

The major difference between the online and offline settings is that online algorithms can further query the preference oracle \(P^{*}\) along the way of training. Since updating the LLMs is expensive, we consider the batch online setting for a sparse policy update. Specifically, for each batch \(t[T]\), we first update the policy pair \((^{1}_{t},^{2}_{t})\) based on the historical information collected so far. Then, we collect \(m\) tuples: we sample a random prompt by \(x_{t,i} d_{0}\), collect two responses by \((a^{1}_{t,i},a^{2}_{t,i})(^{1}_{t},^{2}_{t})\), and query the preference signal \(y_{t,i}(P^{*}(x_{t,i},a^{1}_{t,i},a^{2}_{t,i}))\). Here the batch size \(m\) is usually very large compared to the typically adopted mini-batch update. To distinguish this from the sequential online setting where we update policy after collecting a single preference pair, we refer to this learning paradigm as the _iterative RLHF_.

### Learning with Exploration

The primary advantage of online learning is that we can strategically choose the behavior policies in each iteration to improve the coverage of the collected data, which is referred to as the exploration in the literature. To achieve this goal, we need to quantify the data uncertainty to guide the exploration direction. To this end, we present the notions of information ratio and eluder coefficient.

**Information Ratio and Eluder Coefficient.** Distinct from the offline setting where we assume the coverage condition of a pre-collected dataset \(_{}\), online exploration makes it possible to upper bound the suboptimality by the complexity of the function space. We leverage the notion of the eluder coefficient, which limits the generalization from visited state-action distributions to unseen parts.

**Definition 3** (Information Ratio and Eluder Coefficient).: _At round \(t\), given an estimation \(\), we define the information ratio for any two policy \(^{1},^{2}\) as_

\[_{t}(,^{1},^{2})=_{P}_{ x d_{0}}[P(x,^{1},^{2})-(x,^{1},^{2})]|}{^{t-1}_{x_{s} d_{0},a^{1}_{s}^{1}_{s},a ^{2}_{s}^{2}_{s}}(P(x_{s},a^{1}_{s},a^{2}_{s})-(x_{s},a^{1}_{s},a^{2}_{s}))^{2}}}.\]

_Then, the eluder coefficient is given by \(d(,,T):=_{^{1}_{1:T},^{2}_{1:T}}_{t=1}^{T}( 1,(_{t}(,^{1}_{t},^{2}_{t}))^{2})\)._

The information ratio and eluder coefficient considered here have also been adopted in the literature [e.g., 64, 28, 70, 74, 1]. Essentially, the information ratio compares the _out-of-sample_ error on the unseen data with the _in-sample_ error measured on the historical data, and can be interpreted as the worst-case ratio between them (as we take supreme over all possible \(P\)). Meanwhile, the eluder coefficient limits the extent to which we can be "surprised" by the new out-of-sample distributions, given the historical data collected so far. The uncertainty for the preference model aligns with the uncertainty for the BT model under boundedness conditions, which is illustrated in the following example. We defer the details to Appendix D.1.

**Example 1** (Uncertainty in Bradley-Terry model with linear reward).: _Suppose the reward function can be embedded into a \(d\)-dimensional vector space \(\{r(x,a)=,(x,a):^{d},\|\| B, \|(x,a)\| 1\}\). Then, if we define the covariance matrix as \(_{t}=_{s=1}^{t-1}_{x d_{0},a^{1}_{s}^{1},a ^{2}_{s}^{2}}((x,a^{1})-(x,a^{2}))^{}((x,a^{1})- (x,a^{2}))+(1+e^{B})^{2}I,\) we have_

\[_{t}(,^{1},^{2})(1+e^{B})\|(x,^{1})-(x,^{ 2})\|_{_{t}^{-1}}.\]

```
1:Input: Preference space \(\), policy class \(\), parameter \(,>0\).
2:for t=1,...,T do
3: Exploitation with the main agent: compute the MLE \(_{t}\) with \(_{_{1:t-1}}\) defined in (7) and compute Nash equilibrium by calling the Nash equilibrium oracle 2: \[_{t}^{1}=*{argmax}_{^{1}}_{x^{2}} _{x d_{0},a^{1}^{1},a^{2}^{2}}_{t}( x,a^{1},a^{2})+^{-1}(a^{1}|x)}{^{1}(a^{1}|x)}-^{-1} (a^{2}|x)}{^{2}(a^{2}|x)},\] (10)
4: Exploration with the enhancer: compute enhancer to maximize the uncertainty: \[_{t}^{2}=*{argmax}_{^{2}}_{t}^{m} (,_{t}^{1},^{2}):=_{P}_ {x d_{0}}[P(x,_{t}^{1},^{2})-_{t}(x,_{t}^{1}, ^{2})]|}{_{s=1}^{t-1}_{j=1}^{m}(P(x_{s,j},a^{1}_{s,j},a^{2}_{s,j})-_{t}(x_{s,j},a^{1}_{s,j},a^{2}_{s,j}))^{2}}},\] (11)
5: Collect \(_{t}=\{(x_{i},a^{1}_{i},a^{2}_{i},y_{i})\}_{i=1}^{m}\) by \(x_{i} d_{0},a^{1}_{i}_{t}^{1}(|x_{i})\), \(a^{2}_{i}_{t}^{2}(|x_{i})\) and \(y_{i}(a^{1}_{i} a^{2}_{i}|x,a^{1}_ {i},a^{2}_{i})\);
6:endfor
7:Output: the best policy in \((^{1}_{})\) by a validation set. ```

**Algorithm 2** Optimistic Equilibrium Learning from Human Feedback with Enhancer

We refer interested readers to Du et al. , Zhong et al. , Xie et al.  for the extensive examples when \(d(,,T)\) can have a sub-linear dependency on \(T\). We are now ready to present the algorithm for the online setting, as summarized in Algorithm 2. Specifically, for each iteration, the main agent exploits the information contained in the data collected so far by computing the MLE \(_{t}\) and solving the minimax game with respect to it to get \(_{t}^{1}\). The enhancer, however, aims to facilitate the main agent's learning by maximizing the uncertainty relative to the \(_{t}^{1}\). Finally, we use the policy pair to collect \(m\) preference pairs and query oracle \(P^{}\) to get the preference signals. Notably, to facilitate the computation for the main agent, instead of adding optimism to the value function, we impose the exploration role on the enhancer. This choice turns out to be important when we move toward practical algorithms with reasonable approximations, as we detail in Section 5. We now present the main theoretical guarantee for Algorithm 2.

**Theorem 2**.: _[Proof] Under Assumption 1, for any \(>0\), if we set the total iterations \(T=\{n^{+}:n 2d(,,n)\}\), batch size \(m=18T(2T||/)/^{2}\), \(=|/)/m}\), and \(=2T(2T||/)/m\) for Algorithm 2, then, with probability at least \(1-\), there exists a \(t_{0}[T]\),_

\[J(_{*}^{1},_{*}^{2})-J(_{t_{0}}^{1},).\]

The theorem states that with suitable hyper-parameter choices, after \(T\) iterations (up to log factors), we can find an \(\)-approximate Nash policy \(_{t_{0}}^{1}\) for the max-player. Here \(T\) depends on the eluder coefficient that is intrinsic to the preference model and characterizes the complexity of the problem.

**Key Ideas.** We present a brief discussion of the key analysis ideas. Similar to Lemma 1, the MLE \(\) ensures a controllable in-sample error (with details in the Appendix D). Recalling that the uncertainty bonus is essentially the worst-case ratio between the out-of-sample error (our learning target) and the in-sample error, to finally bound the out-of-sample error, we need to explore each direction where we are uncertain about so that the average uncertainty bonus is sufficiently small. Since the main agent is greedy (takes the best guess we can obtain so far), the enhancer plays the exploration role by maximizing the uncertainty relative to the \(_{t}^{1}\). Then, since the eluder dimension is finite: \(_{t=1}^{T}1,(_{t}(,_{t}^{1},_{t}^ {2}))^{2} d(,,T)\), there exists at least a \(t_{0}[T]\) such that the value at \(t_{0}\) is smaller or equal to the average value:

\[1,(_{t}(,_{t}^{1},_{t}^{2}))^{2}  d(,,T)/T 1/2.\]

Hence, with a proper \(m\), we can obtain the result of Theorem 2.

In practice, searching for the most uncertain policy in the whole policy space can be challenging and the enhancer policy itself does not enjoy any theoretical guarantee. We may slightly modify Algorithm 2 by restricting the exploration step to the following subset

\[_{t}=\{:^{-1}_{x d_{0}}D_{}((|x), ^{1}(|x))(_{t}^{m}(,^{ 1},)+_{t}^{m}(,^{1},^{1}))\},\] (12)

where \(\) is the parameter defined in Theorem 2. This set is never empty because we can prove that both \(_{t}^{1}\) and \(_{^{}}\,J(_{t}^{1},^{})\) belong to \(_{t}\). Intuitively, maintaining a small KL divergence against \(_{t}^{1}\) corresponds to exploiting the historical information, and maximizing the uncertainty relative to \(_{t}^{1}\) leads to more information gain. The choice of \(_{t}\) represents a refined trade-off between these two different goals, thus making \(_{t}^{2}\) also converge to \(_{*}\). The details are deferred to Appendix D.2.

## 5 Practical Implementation of Preference Model and Iterative RLHF

In this section, we discuss how to implement the theoretical Algorithm 2 for the online setting.

**Main agent approximates Nash equilibrium oracle via self-play IPO.** Approximating the information-theoretical oracle 2 given a known preference model has been studied in Munos et al. , Calandriello et al. . The proposed algorithm, self-play IPO, can serve as a reasonable approximation of the oracle by optimizing the following loss function:

\[_{x d_{0},a,a^{}[],a^{+},a^{-} _{t}(x,a,a^{})}|x)_{0}(a^{-}|x)}{( a^{-}|x)_{0}(a^{+}|x)}-^{2},\] (13)

where \([]\) means that although we generate data from policy \(\), but we do not compute the gradient for this data-generation process. Moreover, according to Proposition 4.1 of Calandriello et al. , the minimizer of (13) is the unique Nash policy of the (10).

**Enhancer explores via rejection sampling.** According to (12), the enhancer aims to find a policy that (1) is close to the main agent's policy \(_{t}^{1}\); (2) maximizes the uncertainty relative to the \(_{t}^{1}\). However, since for the general neural network, the uncertainty estimator does not admit a closed form, in practice, we typically resort to heuristic methods. One popular way in the context of alignment is the _rejection sampling_[47; 18; 43; 31; 76]. Specifically, given a prompt \(x\), we use \(_{t}^{1}\) to independently sample \(n\) responses, use a tournament-style procedure to get the best response (and reject all other responses), and take the best responses as \(_{t}^{2}\). In other words, we take the policy induced by rejection sampling with \(_{t}^{1}\) and \(P^{*}\) as the enhancer policy \(_{t}^{2}\). In this way, the \(_{t}^{2}\) enlarges the margins between \(_{t}^{1}\) while maintaining a moderate KL divergence. For instance, in the special case of the BT model, if we rank the samples via the learned reward, the KL divergence is upper bounded by \( n-(n-1)/n\) and is usually far better than this conservative estimation .

**Preference model construction.** We follow Zhao et al. , Liu et al. , Dong et al.  to utilize the fact that the LLM is the next token predictor for the preference modeling. Specifically, we have a preference pair \((x,a^{1},a^{2},A)\), where \(A\) means that the first response is better, which is formatted as

\[ [RESPONSE A] {\(a^{1}\)} [RESPONSE B] {\(a^{2}\)}, and label = A.}\]

Then, we simply treat the preference modeling as an instruction-following task to fine-tune the model on these instruction-label pairs. In particular, to mitigate the position bias (the preference model may prefer the response that is given in the position of RESPONSE A), we randomly switch the order of the two responses in the data formatting process. During inference, we simply use the probability of decoding A as the \((x,a^{1},a^{2})\). We mention in passing that it is also possible to include a rubric in the instruction template to guide the model's prediction and achieve better results . We observe the benefits of the additional prompt engineering in early experiments but decide to use the current version because the main focus is to verify the effectiveness of general preference structure. This implementation is also referred to as the **Generative RM** in subsequent works.

## 6 Experiments

**Model, Dataset, and Evaluation.** We adopt the widely used open-source model Zephyr-SFT-7B  as the starting checkpoint, which is based on the Mistral-7B-v0.111 and fine-tuned on 200Khigh-quality Ultra-chat data . We use the Ultra-feedback  as our prompt set. We divide the prompt set into the train set (60K), validation set (1K), and test set (3K). We mainly use head-to-head comparisons to evaluate the resulting models. In particular, we consider two types of win rate: 1) the win rate measured by the ground-truth LLaMA3-8B-based preference model on the hand-out test set from UltraFeedback; 2) the win rate measured by the GPT-4 Preview (11/06) on an out-of-distribution prompt set AlpacaEval2 . Specifically, for the first evaluation, we use the best DPO model as the reference model, and for the AlpacaEval2, the GPT-4 Preview (11/06) is used as a reference model, and as the judge at the same time.

**Method and Competitors.** We consider the implementation of Algorithm 2 with self-play IPO and rejection sampling as discussed in Section 5. We iterate for three iterations in total and for each iteration, we retrain a preference model using all the historical data, and run self-play IPO from the initial checkpoint \(_{0}\) (i.e., Zephyr-7B-SFT). For simplicity, we refer to this algorithm as Online ELHF IPO. We use the offline DPO , offline IPO , and SFT model as the baseline. In particular, we do not further fine-tune the Zephyr-7B-SFT on the preferred responses of Ultra-Feedback because the quality of Ultra-Feedback is lower than that of Ultra-Chat, which is generated by Chat-GPT APIs. For DPO, we follow Xiong et al. , Tunstall et al. , Rafailov et al.  to set the KL coefficient as \(=0.1\). For IPO, we search the hyper-parameter in \(\{0.1,0.5,1.0\}\) and report the results in Table 2. Clearly, the model with \(=0.1\) beats all other IPO models and the SFT model with large margins, so we set \(=0.1\) for the offline IPO and the Online ELHF IPO algorithm in the subsequent studies.

**Simulation framework.** For all the offline algorithms, we sample two responses per prompt of the train set and use the LLaMA3-8B-based preference model to give the preference signal. Then, we run offline DPO and IPO with the synthetic dataset. For the Online ELHF, we set \(n=4\) in the rejection sampling process and use a tournament-style ranking method (so that the complexity of rejection sampling is linear in \(n\)) to find the best response.

**IPO, DPO, and Online ELHF-IPO.** We use the open-source project TRL12 to implement IPO and DPO. In particular, we have implemented IPO with log-likelihood/perplexity (perplexity is averaged log-likelihood by sequence length), where the original authors of IPO suggest that log-likelihood-based implementation is unstable (see the huggingface blog13 for details). We also found that the IPO without average cannot normally converge and is of poor performance and take the perplexity implementation accordingly. For DPO, we implement the vanilla version as the baseline. We present the main result in Table 3. It is clear that Online ELHF-IPO outperforms the baselines.

   Models & v.s. SFT & v.s. \(=0.1\) & v.s. \(=0.5\) & v.s. \(=1.0\) & AlpacaEval2 \\  SFT & \(0.5\) & 0.121 & 0.205 & 0.231 & 4.63 \\  Offline-IPO-\(=0.1\) & **0.879** & **0.5** & **0.673** & **0.769** & **9.36** \\  Offline-IPO-\(=0.5\) & 0.795 & 0.327 & 0.5 & 0.632 & 6.86 \\  Offline-IPO-\(=1.0\) & 0.710 & 0.230 & 0.328 & 0.5 & 6.55 \\   

Table 2: The evaluation results of the IPO-aligned models under different KL coefficients. For the first 4 win rates, we use the LLaMA3-8B-based preference model to conduct head-to-head comparisons on the hand-out test set from Ultra-feedback with 3K prompts.

   Models & Settings & Gold WR v.s. IPO & AlpacaEval2 WR \\  SFT & - & 0.121 & 4.63 \\  Offline DPO & Offline & 0.41 & 9.33 \\ Offline IPO & Offline & 0.5 & 9.36 \\ Online-ELHF-IPO & Online & **0.78** & **17.67** \\   

Table 3: The evaluation results of the models from different RLHF algorithms. The gold win rates are computed on the hand-out test set from Ultra-feedback with 3K prompts, with the Offline DPO model as the reference. Details of AlpacaEval2 can be found in Dubois et al. .

Related Work

This section focuses on the theoretical aspects. A general discussion is provided in Appendix B.1

**Theoretical Study of Reward-based RLHF.** The theoretical study of policy optimization from preference feedback dated back to the dueling bandits [e.g., 78, 55, 7]. This was later extended to the online RL setting by Xu et al. , Novoseller et al. , Pacchiano et al. , Chen et al. , including tabular online RLHF with finite state, and general function approximation for capturing real problems with large state spaces. Zhan et al. , Wu and Sun  further encompasses the development of reward-free learning type algorithms and sampling-based algorithms for online RLHF. Apart from the online setting, there is another line of works  studying the reward-based RLHF in the offline setting, which learns from a pre-determined offline dataset with suitable coverage condition over the state-action space. However, they consider reward maximization and deviate from the practical applications (e.g., these frameworks admit a deterministic optimal policy). Recently, Xiong et al.  first formulated the RLHF as a reverse-KL regularized contextual bandit and provided finite-sample guarantees in offline, online, and hybrid settings. We remark that all these papers consider only the reward-based RLHF framework, thus differing from ours.

**Theoretical Study of RLHF under General Preference Oracle.** Our work is related to Dudik et al.  and Wang et al. . They investigate preference-based RLHF under a general preference model. The major difference is that we consider the reverse-KL regularized preference, aligning closely with recent LLM advancements , while previous work only considers the non-regularized one. Meanwhile, Dudik et al.  considers the problem of finite action, while our work and Wang et al.  consider the problem with large or even infinite state-action under function approximation. In terms of learning paradigm and algorithmic design, we consider both offline learning from a pre-collected dataset and _batch_ online learning with a sparse policy update, while Dudik et al. , Wang et al.  studies _sequential_ online learning that updates policy in each step, which is not feasible in the context of LLMs. Moreover, we demonstrate that the proposed algorithms can be reasonably implemented in practice, but Dudik et al. , Wang et al.  only focus on information-theoretical algorithms. To summarize, the framework in this work accurately reflects real-world alignment practices thus aligning more closely with the RLHF practice. Our work is closely related to the IPO  and Nash learning , which also motivate new algorithmic design with a general preference oracle. We comment on the similarities and differences between our framework and theirs as follows. In terms of the problem setting, our work and Nash learning consider the minimax game under the reverse-KL regularized preference, while IPO can be interpreted to find the best response of the fixed reference policy, and may be considered as a special case of the game formulation. In terms of learning paradigm, both the IPO and Nash learning only consider learning toward a _fixed and known_ preference oracle, and study the **optimization property** of the problem: how to compute the optimal policy under the given preference oracle. In contrast, we study the **statistical property**, where the preference model needed to be learned and our goal is to find the optimal policy under the underlying ground-truth preference model. In particular, the computational challenge is hidden in Definition 2 and Munos et al.  provides a reasonable approximation of the planning oracle. In this sense, our work and Munos et al.  are complementary to each other. Finally, the concurrent work Swamy et al.  studies the non-regularized general preference model in the _sequential_ online setting and aims to find the Nash equilibrium in the context of continuous control tasks. In terms of the observation model, they assume access to the preference score \((a^{1} a^{2}|x,a^{1},a^{2})\), while we only observe the preference signal \(y((a^{1} a^{2}|x,a^{1},a^{2}))\). Moreover, they design online RLHF algorithms based on a reduction to the no-regret algorithm like Hedge , whose techniques are fundamentally different from ours.

## 8 Conclusion

In this paper, we study the RLHF under a general preference oracle that can capture the non-transitive preferences. Specifically, we formulate the problem as a KL-regularized minimax game between two LLMs, and propose statistically efficient algorithms in both the offline and online settings. The proposed algorithms, with a carefully crafted non-symmetric algorithmic structure, can be practically implemented with reasonable approximations of the information-theoretical computational oracles. We hope our findings can advance the understanding of preference signal modeling in RLHF and stimulate further research beyond the classic reward-based framework.

Acknowledgment

The authors would like to thank Tianqi Liu for insightful discussions on the training of the preference model, and thank Haoxiang Wang, and Zihao Li for valuable discussions on the preference dataset selection. We also thank Nevena Lazic and Csaba Szepesvari for pointing out a technical gap in the first version.

Wei Xiong and Tong Zhang are partially supported by an NSF IIS grant No. 2416897 and Tong Zhang is partially supported by an NSF IIS grant No. 2416897. Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship.