# Towards Understanding How Transformers Learn

In-context Through a Representation Learning Lens

 Ruifeng Ren

Gaoling School of Artificial Intelligence

Renmin University of China

Beijing, China

renruifeng920@ruc.edu.cn

&Yong Liu

Gaoling School of Artificial Intelligence

Renmin University of China

Beijing, China

liuyonggsai@ruc.edu.cn

Corresponding Author

###### Abstract

Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its dual model, generating token representation predictions that are equivalent to the dual model's test outputs. We delve into the training process of this dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings.

## 1 Introduction

Recently, large language models (LLMs) based on the Transformer architectures (Vaswani et al., 2017) has shown surprising in-context learning (ICL) capabilities (Brown et al., 2020; Wei et al., 2022; Dong et al., 2022; Liu et al., 2023). By prepending several training examples before query inputs without labels, the models can make predictions for the queries and achieve excellent performance without any parameter updates. This excellent capability enables pre-trained LLMs such as GPT models to be used in general downstream tasks conveniently. Despite the good performance of the ICL capabilities, the mechanism of ICL still remains an open question.

In order to better understand the ICL capabilities, many works began to give explanations from different aspects. Xie et al. (2021) propose a Bayesian inference framework to explain how ICL occurs between pretraining and test time, where the LLMs infers a shared latent concept among the demonstration examples. Garg et al. (2022) demonstrate through experiments that pre-trained Transformer-based models can learn new functions from in-context examples, including (sparse) linear functions, two-layer neural networks, and decision trees. Zhang et al. (2023) adopt a Bayesian perspective and show that ICL implicitly performs the Bayesian model averaging algorithm, which is approximated by the attention mechanism. Li et al. (2023) define ICL as an algorithm learning problem where a transformer model implicitly builds a hypothesis function at inference-time and derive generalization bounds for ICL. Han et al. (2023) suggest that LLMs can emulatekernel regression algorithms and exhibit similar behaviors during ICL. These works have provided significant insights into the interpretation of ICL capabilities from various perspectives.

In addition to the above explorations, there are also some attempts to relate ICL capabilities to gradient descent. Inspired by the dual form of linear attention proposed in Aiserman et al. (1964) and Irie et al. (2022), the ICL process is interpreted as implicit fine-tuning in the setting of linear attention by Dai et al. (2022). However, there is still a certain noticeable gap between linear attention and the widely used softmax attention. Additionally, this comparison is more of a formal resemblance and the specific details of gradient descent, including the form of the loss function and training data, require a more fine-grained exploration. Akyurek et al. (2022) show that by constructing specific weights, Transformer layers can perform fundamental operations (mov, mul, div, aff), which can be combined to execute gradient descent. Von Oswald et al. (2023) adopt another construction, such that the inference process on a single or multiple linear attention layers can be equivalently seen as taking one or multiple steps of gradient descent on linear regression tasks. Building upon this weight construction method, subsequent work has conducted a more in-depth exploration of the capabilities of ICL under a causal setting, noticing that the inference of such attention layers is akin to performing online gradient descent (Ding et al., 2023; Von Oswald et al., 2023). However, these analyses are still conducted under the assumption of linear attention and primarily focus on linear regression tasks, adopting specific constructions for the input tokens (concatenated from features and labels) and model weights. This limits the explanation of the Transformer's ICL capabilities in more general settings. Thus, the question arises: _Can we relate ICL to gradient descent under the softmax attention setting, rather than the linear attention setting, without assuming specific constructions for model weights and input tokens?_

Motivated by the aforementioned challenges and following these works that connect ICL with gradient descent, we explore the ICL inference process from a representation learning lens. First, by incorporating kernel methods, we establish a connection between the ICL inference process of one softmax attention layer and the gradient descent process of its dual model. The test prediction of the trained dual model will be equivalent to the ICL inference result. We analyze the training process of this dual model from the perspective of representation learning and compare it with existing representation learning methods. Then, we derive a generalization error bound of this process, which is related to the number of demonstration tokens. Our conclusions can be easily extended to more complex scenarios, including a single Transformer layer and multiple attention layers. Furthermore, inspired by existing representation learning methods especially contrastive learning, we propose potential modifications to the attention layer and experiments are designed to support our findings.

## 2 Preliminaries

### In-context Learning with Transformers

The model we consider is composed of many stacked Transformer decoder layers, each of which is composed of an attention layer and a FFN layer. For simplicity, we have omitted structures such as residual connections and layer normalization, retaining only the most essential parts. We consider the standard ICL scenario, where the model's input consists of demonstrations followed by query inputs, that is, the input can be represented as \(=[_{D},_{T}]^{d_{i}(N+T)}\), where \(_{D}=[_{1},_{2},...,_{N}]\) denotes \(N\) demonstration tokens, and \(_{T}=[^{}_{1},^{}_{2},...,^{}_{T}]\) denotes \(T\) query tokens. Here, we focus more on how tokens interact during model inference while ignoring the internal structure of demonstration tokens. For the query input at position \(T+1\), its output after one layer of Transformer can be represented as

\[^{}_{T+1}=_{V}((_{K} )^{T}_{Q}^{}_{T+1}/}),\] (1)

\[}^{}_{T+1}=_{2}(_{1}^{ }_{T+1}+_{1})+_{2},\] (2)

where \(_{K},_{Q},_{V}^{d_{o} d_{i}}\) are parameters for key, query, value projections and \(_{1}^{d_{h} d_{o}}\),\(_{2}^{d_{o} d_{h}}\),\(_{1}^{d_{h}}\),\(_{2}^{d_{o}}\) are FFN parameters. Our concern is how the query token \(^{}_{T+1}\) learns in-context information from demonstrations. Unlike previous work (Von Oswald et al., 2023; Zhang et al., 2023; Bai et al., 2023), here we do not make additional assumptions about the structure of input matrix \(\) and parameters to study the Transformer's ability to implement some specific algorithms. Instead, we adopt the same setting as (Dai et al., 2022) to study more general cases.

### Self-Supervised Representation Learning Using Contrastive Loss Functions

Representation learning aims to learn embeddings of data to preserve useful information for downstream tasks. One class of methods most relevant to our work is probably contrastive learning methods without negative samples (Chen and He, 2021; Grill et al., 2020; Caron et al., 2020; Tian et al., 2021). Contrastive learning is a significant approach of self-supervised learning (SSL) which aims at learning representations by minimizing the distance between the augmentations of the same data point (positive samples) while maximizing the distance from different data points (negative samples) (He et al., 2020; Chen et al., 2020; Oord et al., 2018; Oh Song et al., 2016). To alleviate the burden of constructing a sufficient number of negative samples while avoiding representational collapse, some works propose architectures for contrastive learning without negative samples, which mainly use weight-sharing network known as Siamese networks (Chen and He, 2021; Grill et al., 2020; Caron et al., 2020; Tian et al., 2021). The architecture takes two augmentations \(_{1},_{2}\) from the same data \(\) as inputs, which will be processed by online network and target network respectively to obtain the corresponding representations, that is, \(}_{1}=f_{}(_{1}),}_{2}=f_{}(_{2})\). The two encoder networks share weights directly or using Exponential Moving Average (EMA). Then, \(}_{1}\) will be input into a predictor head to obtain the predictive representation \(_{1}=g(}_{1})\). Finally, we minimize the distance between the predictive representation and target representation, that is, \((_{1},(}_{2}))\) where \(()\) means \(}_{2}\) is treated as a constant during backpropagation. For \(()\), we often choose the cosine similarity or the \(l_{2}\)-norm as a measure of distance, although they are equivalent when the vector is normalized. Another class similar to our work is kernel contrastive learning (Esser et al., 2024). Given an anchor \(\) and its positive and negative samples \(^{+},^{-}\), it aims to optimize the loss function \(=f()^{T}(f(^{-})-f(^{+}))\), where \(f()=()\) and \(()\) is the feature mapping for some kernel. We will consider the gradient descent process corresponding to the inference process of ICL from the perspective of representation learning and compare it with the two aforementioned representation learning patterns.

### Gradient Descent on Linear Layer is the Dual Form of Linear Attention

It has been found that the linear attention can be connected to the linear layer optimized by gradient descent (Aiserman et al., 1964; Irie et al., 2022; Dai et al., 2022), that is, the gradient descent on linear layer can be seen as the dual form 2 of linear attention. A simple linear layer can be defined as \(f_{L}()=\), where \(^{d_{o} d_{i}}\) is the projection matrix. Given training inputs \([_{i}]_{i=1}^{N}^{d_{i}}\) with their labels \([_{i}]_{i=1}^{N}^{d_{o}}\), a linear layer can output the predictions \([}_{i}]_{i=1}^{N}\) where \(}_{i}=_{i}\) and then compute certain loss \((}_{i},_{i})\) for training. Backpropagation signals \([_{i}]_{i=1}^{N}^{d_{o}}\) will be produced to update \(\) in gradient descent process where \(_{i}=-(_{}_{i}})\) if we set \(\) as the learning rate. During test time, the trained weight matrix \(}\) can be represented by its initialization \(_{0}\) and the updated part \(\), that is,

\[}=_{0}+=_{0}+_{i=1}^{N}_{i} _{i},\] (3)

where \(\) denotes the outer product according to the chain rule of differentiation. On the other hand, this process can be viewed from the perspective of linear attention. Let \([_{i}]_{i=1}^{N},[_{i}]_{i=1}^{N}^{d_{i}}\) denote the \(N\) key and value vectors constituting matrices \(,^{d_{i} N}\) respectively. For a given query input \(^{d_{i}}\), linear attention is typically defined as the weighted sum of these value vectors

\[(,,)=^{T}=_{i=1}^{N}_{ i}_{i}^{T}=(_{i=1}^{N}_{i}_{i} ).\]

Then, we can rewrite the output of a linear layer during test time as

\[f_{L}(_{test})=}_{test}=_{0}_{test}+ (_{i=1}^{N}_{i}_{i})_{test}=_{0 }_{test}+(,,_{test}),\] (4)where \(^{d_{o} N}\) and \(^{d_{i} N}\) are stacked by backpropagation signals \([_{i}]_{i=1}^{N}\) and training inputs \([_{i}]_{i=1}^{N}\) respectively. We can find from Eq (4) that the trained weight \(}\) records all training datapoints and the test prediction of the linear layer indicates which training datapoints are chosen to activate using \(()\) where \([_{i}]_{i=1}^{N}\) can be considered as values while \([_{i}]_{i=1}^{N}\) as keys and \(_{test}\) as the query. This interpretation uses gradient descent as a bridge to connect predictions of linear layers with linear attention, which can be seen as a simplified softmax attention used in Transformers.

Inspired by this relationship, Dai et al. (2022) understand ICL as implicit fine-tuning. However, this interpretation based on linear attention deviates from the softmax attention used in practical Transformers. Furthermore, this alignment is also ambiguous as the specific details of the gradient descent process, including the form of loss function and dataset, have not been explicitly addressed. In addition, Von Oswald et al. (2023); Ding et al. (2023) also connect ICL with gradient descent for linear regression tasks using weight construction methods, where parameters \(_{K}\), \(_{Q}\) and \(_{V}\) of the self-attention layer need to roughly adhere to a specific constructed form. However, these analyses rely on the setting of linear regression tasks and assumptions about the form of input tokens (concatenated with features and labels), which limits the interpretability of ICL capabilities from the perspective of gradient descent. Thus, we attempt to address these issues in the following sections.

## 3 Connecting ICL with Gradient Descent

In this section, we will address two questions discussed above: (i) _Without assuming specific constructions for model weights and input tokens, how to relate ICL to gradient descent in the setting of softmax attention instead of linear attention?_ (2) _What are the specific forms of the training data and loss function in the gradient descent process corresponding to ICL?_ In addressing these two questions, we will explore the gradient descent process corresponding to ICL from the perspective of representation learning.

### Connecting Softmax Attention with Kernels

Before we begin establishing the connection between ICL and gradient descent, we need to firstly rethink softmax attention with kernel methods. Dai et al. (2022) connect ICL with gradient descent under the linear attention setting. In fact, it is completely feasible to interpret ICL under softmax attention with the help of kernel methods. We define the attention block as

\[=((_{K})^{T}_{Q}/}),\] (5)

which can be viewed as the product of an unnormalized part \(_{u}\) and a normalizing multiplier \(\), that is,

\[=_{u}^{-1},\ \ _{u}=((_{K})^{T}_{Q}/}),\ \ =(_{N}^{T}_{u}),\] (6)

where \(()\) is element-wise. Similar in Choromanski et al. (2020), we define softmax kernel \(K_{sm}:^{d_{o}}^{d_{o}}_{+}\) as \(K_{sm}(,)=e^{^{T}}=e^{^{2}+}{2}}K_{guass}(,)\) where \(K_{guass}=e^{-\|-\|^{2}/2}\) is the gaussian kernel when the variance \(^{2}=1\). According to Mercer's theorem (Mercer, 1909), there exists some mapping function \(:^{d_{o}}^{d_{r}}\) satisfying that \(K_{sm}(,)=()^{T}()\). Thus,

Figure 1: The ICL output \(^{}_{N+1}\) of one softmax attention layer is equivalent to the test prediction \(}_{test}\) of its trained dual model \(f()=}()\). The training data and test input can be obtained by linear transformations of demonstration and query tokens, respectively.

noting that when omitting the \(}\)-renormalization and equivalently normalize key and value vectors in Eq (6), every entry in the unnormalized part \(_{u}\) can be seen as the output of softmax kernel \(K_{sm}\) defined for the mapping \(\), which can be formulated as:

\[_{u}(i,j)=((_{K}_{i})^{T}_{Q}_{j}) =K_{sm}(_{K}_{i},_{Q}_{j})=(_{K}_{i})^{ T}(_{Q}_{j}).\] (7)

There have been many forms of mapping function \(()\) used in linear Transformers research to approximate this non-negative kernel (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021; Lu et al., 2021). For example, we can choose \(()\) as positive random features which has the form \(()=e^{^{T}-\|\|^{2}/2}\) to achieve unbiased approximation (Choromanski et al., 2020). Alternatively, we can also choose \(()=()+1\) proposed by Katharopoulos et al. (2020).

### The Gradient Descent Process of ICL

Now, we begin to establish the connection between the ICL inference process of a softmax attention layer and gradient descent. We focus on a softmax attention layer in a trained Transformer model, where the parameters \(\{_{Q},_{K},_{V}\}\) have been determined and the input \(=[_{D},_{T}]\) has the form introduced in Section 2.1. Then, after the inference by one attention layer, the query token at position \(T+1\) will have the form \(^{}_{T+1}\) formulated by Eq (1).

On the other hand, given a specific softmax kernel mapping function \(()\) that satisfies Eq (7), we can define the dual model for the softmax attention layer as

\[f()=(),\] (8)

where \(^{d_{o} d_{r}}\) is parameters. We assume that the dual model obtains its updated weights \(}\) after undergoing one step of gradient descent with some loss function \(\). Subsequently, when we take \(_{test}=_{Q}^{}_{T+1}\) as the test input, we can obtain its test prediction as

\[}_{test}=f(_{test})=f(_{Q}^{}_{T+1} )=}(_{Q}^{}_{T+1}).\]

We will show that \(^{}_{T+1}\) in Eq (1), is strictly equivalent to the above test prediction \(}_{test}\), which implies that the inference process of ICL involves a gradient descent step on the dual model. This can be illustrated by the following theorem:

**Theorem 3.1**.: _The query token \(^{}_{T+1}\) obtained through ICL inference process with one softmax attention layer, is equivalent to the test prediction \(}_{test}\) obtained by performing one step of gradient descent on the dual model \(f()=()\). The form of the loss function \(\) is:_

\[=-_{i=1}^{N}(_{V}_{i})^{ T}(_{K}_{i}),\] (9)

_where \(\) is the learning rate and \(D\) is a constant._

Proof can be found in Appendix A. Theorem 3.1 demonstrates the equivalence between the ICL inference process and gradient descent. Below, we delve into more detailed discussions:

**Training Set and Test Input:** In fact, once the attention layer has already been trained, that is, \(_{K},_{Q},_{V}\) has been determined, the demonstration tokens \([_{i}]_{i=1}^{N}\) will be used to construct a training set for the dual model. Specifically, the training data has the form \(\{^{(i)}_{std},^{(i)}_{std}\}_{i=1}^{N}\) where \(^{(i)}_{std}=_{K}_{i}\) as inputs and \(^{(i)}_{std}=_{V}_{i}\) as their labels. During training stage, for each input \(^{(i)}_{std}\), the dual model outputs its prediction \(}^{(i)}=f(^{(i)}_{std})=(^{( i)}_{std})=(_{K}_{i})\). Then, the loss function Eq (9) can be rewritten as \(=-_{i=1}^{N}(^{(i)}_{std})^{T}}^{(i)}\), which can be regarded as the cosine similarity. Then, using this loss function and the training data, we can perform one step of Stochastic Gradient Descent (SGD) on the dual model and obtain the updated \(}\). Finally, during the testing stage, we take \(_{test}=_{Q}^{}_{T+1}\) as the test input to get its prediction which will be consistent with the ICL result \(^{}_{T+1}\), that is, \(}_{test}=f(_{test})=}(_{Q} ^{}_{T+1})=^{}_{T+1}\). This process can be illustrated in Figure 1. Demonstration tokens provide information about the training data points and the weight matrix \(}\) is optimized to learn sufficient knowledge about demonstrations. Thisgradient descent process using the loss function \(\) applied to \(f()\) can be seen as the dual form of the ICL inference process of the attention layer.

Representation Learning Lens:Even though we have now clarified the details of the gradient descent process of ICL, what does this process more profoundly reveal to us? In fact, for a encoded demonstration token \(_{i}\), the key and value mapping will generate a pair of features \(_{K}_{i}\) and \(_{V}_{i}\) that exhibit a certain distance from each other, akin to positive samples in contrastive learning. And then, \(()\) projects \(_{K}_{i}\) into a higher-dimensional space to capture deeper features. Finally, the weight matrix \(\), which maps \((_{K}_{i})\) back to the original space, is trained to make the mapped vector as close as possible to \(_{V}_{i}\). This process is illustrated in Figure 2. Below, we attempt to understand this process from the perspective of existing representation learning methods introduced in Section 2.2, although we emphasize that there are certain differences between them.

Comparison with Contrastive Learning without Negative Samples:If we consider the key and value mapping as two types of data augmentation, then from the perspective of contrastive learning without negative samples, this process can be similarly formalized as

\[_{}\ (}^{(i)},_{std}^{(i)})= (}^{(i)},(_{std}^{(i)}) ),\]

where \(()\) is naturally applicable because there are no learning parameters involved in the generation process of the representation \(_{std}^{(i)}\). However, it's important to note that the representation learning process of ICL is much simpler: Firstly, the online and target networks are absent while the augmentations \(_{K}_{i},_{V}_{i}\) are directly used as online and target representations respectively. Secondly, the predictor head is useful and not discarded, which is then used during test stage.

Comparison with Contrastive Kernel Learning:Given an anchor data \(\) and its positive and negative samples \(^{+}\), \(^{-}\), contrastive kernel learning aims to optimize the loss function \(=()(f(^{-})-f(^{+}))\) where \(f()=()\). There are significant differences in the representation learning process of ICL: Firstly, it does not involve negative samples. Secondly, there is no corresponding processing for positive samples, leading to parameter updates being solely dependent on the processing of the anchor.

Extension to More Complicated Scenarios:Theorem 3.1 can be naturally extended to one single Transformer layer and multiple attention layers. As for one Transformer layer formed in Section 2.1, its dual model \(f^{+}()=()+\) introduces an additional bias \(\) and only \(\) is trained while \(\) remains fixed. In addition, the labels of training set will be \(_{std}^{(i)}=_{F}_{K}_{i}\) where \(_{F}\) has potential low-rankness property induced by \(()\). As for multiple attention layers, the ICL inference process will be equivalent to sequentially performing gradient descent and making predictions on the dual model sequence. We provide more details in Appendix B.

Compared to Dai et al. (2022) considering the connection under linear attention setting, Theorem 3.1 gives explanation for more generally used softmax attention and offers a more detailed exploration of the training process. Additionally, unlike Von Oswald et al. (2023, 2023); Ding et al. (2023)'s focus on particular linear regression task and specific configurations of token and parameters, we aim to explain the process of token interactions during ICL inference in a more general setting.

Figure 2: **Left Part: The representation learning process for the ICL inference by one attention layer. Remaining Part: Comparison of the ICL Representation Learning Process (Center Left), Contrastive Learning without Negative Samples (Center Right), and Contrastive Kernel Learning (Right).**

### Generalization Bound of the dual gradient descent process for ICL

In this part, we are interested in the generalization bound of the ICL gradient process. When ICL inference is performed for some task \(\), we cannot provide all demonstrations related to task \(\) limited by the length of input tokens. We denote \(_{}^{d_{i}}\) as all possible tokens for the task \(\) and assume that these tokens will be selected according to the distribution \(_{}\). During a particular instance of ICL inference, let \(=\{_{i}\}_{i=1}^{N}_{}\) represent the example tokens we selected. We define the function class as \(:=\{f()=(_{K})\|\| w\}\) where \(\|\|\) denotes the Frobenius norm. Generally, ignoring constant term in Eq (9), we consider the representation learning loss as

\[(f)=_{_{}} [-(_{V})^{T}f()]=_{ _{}}[-(_{V})^{T} (_{K})],\] (10)

where \(f\) and \(_{}\) is the distribution for some ICL task \(\). Correspondingly, the empirical loss will be formulated as \(}(f)=-_{i=1}^{N}(_{V}_{i} )^{T}f(_{i})\) and we have \(=_{f}(f)\). In addition, we denote the kernel matrix of demonstration tokens \(\) as \(_{}^{N N}\) where \((_{})_{i,j}=(_{K}_{i}),(_{K }_{j})\), that is, the inner product of the feature maps after \(_{K}\) projection between the \(i\)-th token and \(j\)-th token. We state our theorem as follows:

**Theorem 3.2**.: _Define the function class as \(:=\{f()=(_{K})\|\| w\}\) and let the loss function defined as Eq (10). Consider the given demonstration set as \(=\{_{i}\}_{i=1}^{N}\) where \(_{}\) and \(_{}\) is all possible demonstration tokens for some task \(\). With the assumption that \(\|_{V}_{i}\|,\|(_{K}_{i})\|\), then for any \(>0\), the following statement holds with probability at least \(1-\) for any \(f\)_

\[()(f)+O( (_{})}}{N}+}{}}).\] (11)

Proof of 3.2 can be found in Appendix C. Theorem 3.2 provides the generalization bound of the optimal dual model trained on a finite selected demonstration set under a mild assumption that \(\|\|\) is bounded. Intuitively, as the number of demonstration (and therefore the number of demonstration tokens) increases, the generalization error decreases, which is consistent with existing experimental observations (Xie et al., 2021; Garg et al., 2022; Wang et al., 2024).

## 4 Attention Modification Inspired by the Representation Learning Lens

Analyzing the dual gradient descent process of ICL from the perspective of representation learning inspires us to consider that: _Do existing representation learning methods, especially contrastive learning methods, also involve a dual attention inference process? Alternatively, can we modify the attention mechanism by drawing on existing methods?_ In fact, since there are lots of mature works in representation learning especially contrastive learning, it is possible for us to achieve this by drawing on these works (He et al., 2020; Chen et al., 2020; Wu et al., 2018; Chen et al., 2020; Chen and He, 2021). We will provide some simple perspectives from the loss function, data augmentations and negative samples to try to adjust attention mechanism. It is worth noting that these modifications are also applicable to the self-attention mechanism, and we will explore these variants in experiments. More details can be seen in Appendix D.

**Attention Modification inspired by the Contrastive Loss:** It can be observed that the unnormalized similarity in Eq (9) allows \(\|\|\) to be optimized to infinity if we ignore the Layer Normalization (LN) layer to prevent this. As for one single attention layer without LN layer, to address this issue, we can introduce regularization term to constrain the norm of \(\), specifically by

\[=-_{i=1}^{N}(_{V}_{i})^{T }(_{K}_{i})+\|\|_{F}^{2},\] (12)

where \(\) is a hyperparameter. Equivalently, the attention output Eq (1) will be modified as

\[_{T+1}^{}=_{V}[_{D},(1-)_{T}] ((_{K})^{T}_{Q}_{T+1}^{ }/}).\] (13)

This modification is equivalent to retaining less prompt information for query token during aggregation and relatively more demonstration information will be attended to.

Attention Modification inspired by the Data Augmentation:If we analogize the key and value mappings to data augmentations in contrastive learning, then for the representation learning process of ICL, these overly simple linear augmentations may limit the model's ability to learn deeper representations. Thus, more complicated augmentations can be considered. Denoting these two augmentations as \(g_{1}\) and \(g_{2}\), the loss function will be modified as

\[=-_{i=1}^{N}[g_{1}(_{V}_{i}) ]^{T}(g_{2}(_{K}_{i})).\]

Correspondingly, the attention layer can be adjusted as,

\[^{}_{T+1}=g_{1}(_{V})([g_{2}(_{K})]^{T}_{Q}^{}_{T+1}/}),\] (14)

where \(g_{1}()\) and \(g_{2}()\) will be column-wise here. Here we add augmentations for all tokens instead of only demonstration ones to maintain uniformity in the semantic space. In experiments, we simply select MLP for \(g_{1}\) and \(g_{2}\). It's worth noting that here we only propose the framework, and for different tasks, the augmentation approach should be specifically designed to adapt them.

Attention Modification inspired by the Negative Samples:Negative samples play a crucial role in preventing feature collapse in contrastive learning methods while the representation learning process of ICL only brings a single pair of features closer, lacking the modeling of what should be pushed apart, which could potentially limit the model's ability to learn representations effectively. Therefore, we can introduce negative samples to address this:

\[=-_{i=1}^{N}(_{V}}_{i} )^{T}(_{K}_{i}),}_{i}=_{i} -(i)|}_{j(i)}_{j},\]

where \((i)\) is the set of the negative samples for \(_{i}\) and \(\) is a hyperparameter. Correspondingly, the attention layer is modified as

\[^{}_{T+1}=_{V}[}_{D},_{T}] ((_{K})^{T}_{Q}_{T+1}/}),\] (15)

where \(}_{D}=[}_{1},}_{2},...,} _{N}]\). Here we simply use other tokens as negative samples and we emphasize that for specific tasks, an appropriate design of negative samples will be more effective.

## 5 Experiments

In this section, we design experiments on synthetic tasks to support our findings and more experiments including on more realistic tasks can be seen in Appendix E. The questions of interest are: _(i) Is the result of ICL inference equivalent to the test prediction of the trained dual model? (ii) Is it potential to improve the attention mechanism from the perspective of representation learning?_

Linear Task Setting:Inspired by Von Oswald et al. (2023), to validate the equivalence and demonstrate the effectiveness of the modifications, we firstly train one softmax self-attention layer using linear regression tasks. We generate the task by \(=\) where every element of \(^{d_{s} d_{t}}\) is sampled from a normal distribution \(_{ij}(0,1)\) and \(\) from uniform distribution \((0,1)\).

Figure 3: The equivalence between ICL of one softmax attention layer and gradient descent, along with analysis on different model modifications. **Left Part:**\(\|}_{test}-^{}_{T+1}\|_{2}\) as the gradient descent proceeds under setting \(N=15\); **Remaining Part:** the performance for regularized models (Center Left), augmented models (Center Right) and negative models (Right) with different settings.

\(U(-1,1)^{d_{t}}\). We set \(d_{t}=11\) and \(d_{s}=1\). Then, at each step, we use generated \(\{_{i}=[_{i};s_{i}]\}_{i=1}^{N+1}\) to form the input matrix \(\) where the last token will be used as the query token and the label part will be masked, that is, \(_{N+1}=[_{i};0]\). Here we consider only one query token (\(T=0\)) and we denote \(_{T+1}^{}=_{N+1}\) to maintain consistency of notation in Section 2.1. Finally, the attention layer is trained to predict \(_{N+1}\) to approximate the true label \(s_{N+1}\) using mean square error (MSE) loss.

**Model Setting:** It is worth noting that to facilitate direct access to the dual model, we use positive random features as kernel mapping functions (Performer architecture (Choromanski et al., 2020)) to approximate the standard softmax attention, that is, \(()=e^{^{T}-\|\|^{2}/2}\) where \((0,I)\). We set the dimension of the random features as \(d_{r}=100(d_{t}+d_{s})=1200\) to obtain relatively accurate estimation. After training, the weights of the attention layer have been determined. Thus, given specified input \(\), we can construct the dual model \(f()=()\) and its corresponding training data and test input according to Theorem 3.1.

We perform three experiments under different random seeds for linear regression tasks with the results of one presented in Figure 3. In addition, we also conduct more experiments including these on trigonometric, exponential synthetic regression tasks and more realistic tasks. More details of experiments setting and results can be found in Appendix E. We mainly discuss the results on the linear regression task as follows.

**Equivalence Between ICL and Gradient Descent:** To answer the first question, we generate the test input \(_{test}\) using the same method as training and obtain the ICL result of the query token \(_{T+1}^{}\). On the other hand, we use \(_{test}\) to train the dual model according to Theorem 3.1 and get the test prediction \(}_{test}\). The result is shown in the left part part of Figure 3. It can be observed that after \(N=15\) epochs training on the dual model, the test prediction \(}_{test}\) is exactly equivalent to the ICL inference result \(_{T+1}^{}\) by one softmax attention layer, which aligns with our analysis in Theorem 3.1. More detailed experiments can be seen in Appendix E.1.

**Analysis on the Modifications:** In Section 4, we discussed different modifications to the attention mechanism from perspectives of contrastive loss, data augmentation and negative samples. Here we call these modifications regularized models, augmented models and negative models respectively. More details of modifications for self-attention mechanism can be seen in Appendix D.

For regularized models, we vary different \(\) to investigate the impact on pretraining performance under the same setting, as shown in the center left part of Figure 3. It can be observed that when \(>0\), the regularized models converges to a poorer result while when \(<0\), the model converges faster and achieves final results comparable to the normal model without regularization (\(=0\)). At least for this setting, this is a little contrary to our initial intention of applying regularization to the contrastive loss where \(\) should be positive. We explain it that the appropriate \(\) contributes to achieving a full-rank attention matrix as stated in Appendix D, preserving information and accelerating convergence.

For augmented models, we simply choose a single-layer MLP for \(g_{1}()\) and \(g_{2}()\) as data augmentations to enhance the value and key embeddings respectively in Eq (14) and we choose GELU (Hendrycks and Gimpel, 2016) as the activation function. It can be observed in the center right part of Figure 3 that when we only use \(g_{2}\), that is, only provide augmentation for keys, the model actually shows slightly faster convergence than other cases. Furthermore, when we use two-layer MLP as \(g_{2}^{+}()\) as a more complicated augmentation function, the result indicates that although the model initially converges slightly slower due to the increased number of parameters, it eventually accelerates convergence and achieves a better solution. This indicates that appropriate data augmentation indeed have the potential to enhance the capabilities of the attention layer.

For negative models, we select the \(k\) tokens with the lowest attention scores as negative samples for each token. From Eq (15), we can see that it is equivalent to subtracting a certain value from the attention scores corresponding to those negative samples. We vary the number of negative samples \(k\) and \(\) in Eq (15) and the results are shown in the right part of Figure 3. It can be found that the model has the potential to achieve slightly faster convergence with appropriate settings (\(k=3\) and \(=0.1\)). In fact, it can be noted that in the original attention mechanism, attention scores are always non-negative, indicating that some irrelevant information will always be preserved to some extent. However, in the modified structure, attention scores can potentially become negative, which makes the model more flexible to utilize information. Certainly, as we discussed in Section 4, for different tasks, more refined methods of selecting augmentations and constructing negative samples may be more effective and we also leave these aspects for future.

Related Work

Since Transformers have shown remarkable ICL abilities (Brown et al., 2020), many works have aimed to analyze the underlying mechanisms (Garg et al., 2022; Wang et al., 2023). To explain how Transformers can learn new tasks without parameter updates given few demonstrations, an intuitive idea is to link ICL with (implicit) gradient updates. The most relevant work to ours is that of Dai et al. (2022), which utilizes the dual form to understand ICL as an implicit fine-tuning (gradient descent) of the original model under a linear attention setting (Aiesserman et al., 1964; Irie et al., 2022). They design a specific fine-tuning setting where only the parameters for the key and value projection are updated and the causal language modeling objective is adopted. In this context, they find ICL will have common properties with fine-tuning. Based on this, Deutch et al. (2024) investigate potential shortcomings in the evaluation metrics used by Dai et al. (2022) in real model assessments and propose a layer-causal GD variant that performs better in simulating ICL. As a comparison, our research also uses the dual form to analyze the nonlinear attention layer and explores the specific form of the loss used in the training process. However, we link ICL to the gradient descent performed on the dual model rather than fine-tuning the original model. The former process utilizes a self-supervised representation learning loss formalized as Eq (9) determined by the attention structure itself while performing supervised fine-tuning on the original model is often determined by task-specific training objectives (or manually specified causal language modeling objective Dai et al. (2022)). A more formal and detailed comparison can be found in Appendix F.

Additionally, many other works also link ICL with gradient descent, aiming to explore the Transformer's ability to perform gradient descent algorithms to achieve ICL (Bai et al., 2023; Schlag et al., 2021). Akyurek et al. (2022) reveal that under certain constructions, Transformer can implement simple basic operations (mov, mul, div and aff), which can be combined to further perform gradient descent. Von Oswald et al. (2023) provide a simple and appealing construction for solving least squares solutions in the linear attention setting. Subsequently, Zhang et al. (2023), Ahn et al. (2023), Mahankali et al. (2023) provide theoretical evidence showing that the local or global minima will have a form similar to this specific construction proposed by Von Oswald et al. (2023) under certain assumptions. These works, both experimentally and theoretically, often focus on specific linear regression tasks (\(y=^{T}\)) and specific structured input format where each token takes the form \([,y]\) consisting of the input part \(\) and the label part \(y\). In addition, the label part of the final query to be predicted is masked, represented as \([,0]\). Subsequent works have expanded this exploration under more complicated setups, including examining nonlinear attention instead of linear attention(Cheng et al., 2023; Collins et al., 2024), using unstructured inputs rather than structured ones(Xing et al., 2024), and considering casual or autoregressive setting(Ding et al., 2023; Von Oswald et al., 2023). As a comparison to these works, our work does not target specific tasks like linear regression; therefore, we do not make detailed assumptions about the model weights (simply treated as weights after pre-training) or specific input forms. Instead, we aim to view the ICL inference process from the perspective of representation learning in the dual model. However, we would like to point out that under these specific weight and input settings, an intuitive explanation can also be provided from a representation learning perspective (see Appendix F). We also notice that Shen et al. (2023) experimentally show that there may exist differences between ICL inference in LLMs and the fine-tuned models in real-world scenarios from various perspectives and assumptions used in previous works may be strong. As mentioned earlier, our analysis primarily focus on linking ICL with gradient descent on the dual model of a simplified Transformer rather than fine-tuning the original model. Analyzing more realistic models will also be our future directions.

## 7 Conclusion and Impact Statements

In this paper, we establish a connection between the ICL process of Transformers and gradient descent of the dual model, offering novel insights from a representation learning lens. Based on this, we propose modifications for the attention layer and experiments under our setup demonstrate their potential. Although we have made efforts in understanding ICL, there are still some limitations in our analysis: (1) our work primarily focuses on the simplified Transformer and the impact of structures like layer normalization, residual connections, and others requires more nuanced analysis; (2) for more tasks and settings, the proposed model modifications may require more nuanced design and validation. We leave these aspects for future exploration. And we believe that this work mainly studies the theory of in-context learning, which does not present any foreseeable societal consequence.

Acknowledgements

We sincerely appreciate the anonymous reviewers for their helpful suggestions and constructive comments. This research was supported by National Natural Science Foundation of China (No.62476277, No.6207623), Beijing Natural Science Foundation (No.4222029), CCF-ALIMAMA TECH Kangaroo Fund (No.CCF-ALIMAMA OF 2024008), and Huawei-Renmin University joint program on Information Retrieval. We also acknowledge the support provided by the fund for building worldclass universities (disciplines) of Renmin University of China and by the funds from Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, from Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, from Intelligent Social Governance Interdisciplinary Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, from Public Policy and Decision-making Research Lab of Renmin University of China, and from Public Computing Cloud, Renmin University of China.