# Effective Bayesian Heteroscedastic Regression

with Deep Neural Networks

Alexander Immer

Equal contribution. \(\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch. Code at https://github.com/aleximmer/heteroscedastic-nn.

Emanuele Palumbo

Equal contribution. \(\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch

Alexander Marx

Equal contribution. \(\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch. Code at https://github.com/aleximmer/heteroscedastic-nn.

Julia E. Vogt

Equal contribution. \(\) Shared last author. Correspondence to: alexander.immer@inf.ethz.ch alexander.marx@inf.ethz.ch

###### Abstract

Flexibly quantifying both irreducible aleatoric and model-dependent epistemic uncertainties plays an important role for complex regression problems. While deep neural networks in principle can provide this flexibility and learn heteroscedastic aleatoric uncertainties through non-linear functions, recent works highlight that maximizing the log likelihood objective parameterized by mean and variance can lead to compromised mean fits since the gradient are scaled by the predictive variance, and propose adjustments in line with this premise. We instead propose to use the natural parametrization of the Gaussian, which has been shown to be more stable for heteroscedastic regression based on non-linear feature maps and Gaussian processes. Further, we emphasize the significance of principled regularization of the network parameters and prediction. We therefore propose an efficient Laplace approximation for heteroscedastic neural networks that allows automatic regularization through empirical Bayes and provides epistemic uncertainties, both of which improve generalization. We showcase on a range of regression problems--including a new heteroscedastic image regression benchmark--that our methods are scalable, improve over previous approaches for heteroscedastic regression, and provide epistemic uncertainty without requiring hyperparameter tuning.

## 1 Introduction

Capturing the _epistemic_ (model uncertainty) and _aleatoric_ uncertainty (observation noise) allows for computing the predictive variance of a model, which is crucial for areas such as active learning (Houlsby et al., 2011; Kirsch, 2023), reinforcement learning (Osband et al., 2016; Yu et al., 2020) and decision making. Bayesian neural networks allow for modelling both epistemic and aleatoric uncertainty, as such, they lend themselves naturally to this task. Typically, they are applied under the assumption of homoscedasticity, i.e., constant noise (MacKay, 1995; Foong et al., 2019; Kirsch, 2023, and others), but also adaptations of variational inference (VI) to model heteroscedasticity, such as mean-field VI (Graves, 2011), deterministic VI (Wu et al., 2019, DVI), and Monte-Carlo Dropout (Gal and Ghahramani, 2016), have been studied. In this paper, we are interested in learning the epistemic and aleatoric uncertainty in heteroscedastic regression for potentially complex tasks such as image regression, where we are given inputs \(^{D}\) and a scalar response \(\), and model \(=\) as a conditional Gaussian distribution with mean \(()\) and standard deviation \(()\) being dependent on input \(\). Inherently, the problem involves robustly modelling the aleatoric uncertainty, corresponding to the variance, which is a relevant problem in econometrics, statistics (Harvey, 1976; Amemiya, 1985), and causal discovery (Guyon et al., 2019; Xu et al., 2022).

Classical approaches based on linear models and non-linear feature maps (Cawley et al., 2004) attempt to learn \(()\) and \(()\) directly by first fitting the mean function and subsequently fitting the standard deviation from the log-residuals, e.g., feasible generalized least squares (FGLS) (Woolridge, 2015). The corresponding objective is, however, only convex if one of the parameters is kept constant, but it is not jointly convex in both parameters (Cawley et al., 2004; Yuan and Wahba, 2004). To alleviate this problem in the context of Gaussian processes, (Le et al., 2005) propose to reparametrize the objective in terms of the natural parameters, which induces a jointly convex objective. Advantages of the natural parametrization compared to standard FGLS have also been demonstrated for non-linear features maps (Immer et al., 2023).

In the neural network literature, the standard approach is to model \(()\) and \(()\) as outputs of the network and maximize the corresponding Gaussian log likelihood via stochastic gradient descent (Nix and Weigend, 1994; Lakshminarayanan et al., 2017; Kendall and Gal, 2017), or use two separate neural networks to model mean and standard deviation (Skafte et al., 2019). As in classical estimators, this parametrization might not be ideal and can lead to overconfident variance estimates (Skafte et al., 2019; Stirn and Knowles, 2020) and possibly compromised mean fits. Two recent strategies aim to adjust for this problem by reducing the influence of the predictive variance on the gradient of the mean. In particular, Seitzer et al. (2022) introduce a surrogate loss, the \(-\)NLL loss, which regulates the influence of the variance on the gradients of the loss by introducing a stop-gradient operation. As an alternative solution, Stirn et al. (2023) propose architectural constraints coupled with two stop gradient operations, regularizing the heteroscedastic model such that its mean fit is not compromised compared to a homoscedastic baseline. We provide more details to both approaches in Sec. 2.

Our ContributionsIn comparison to previous work on heteroscedastic regression with neural networks, we take a different perspective postulating that current estimators lack principled regularization. While recent work aims at regularizing the influence of the predictive variance, as we discuss in Sec. 2, we show that this focus can shift the problem to compromising capacity for the variance estimate. Instead, we propose three major modifications to tackle the problem of fitting heteroscedastic neural networks: akin to previous work on GPs (Le et al., 2005) and linear models (Immer et al., 2023), we propose to re-parameterize the loss using the _natural parametrization_ (cf. Sec. 3) which is known to be jointly concave in both parameters. Empirically, we find that this parametrization can be more stable during optimization. Further, we derive an _efficient Laplace approximation to the marginal likelihood_ for heteroscedastic regression that can automatically learn regularization via empirical Bayes and provide an early-stopping signal to prevent overfitting without requiring a grid search based on a validation set in Sec. 4.2 Additionally, the Laplace approximation provides epistemic uncertainty through the _Bayesian posterior predictive_, which generally improves predictive performance. We provide a fast closed-form approximation to the posterior predictive that also provides a simple split into aleatoric and epistemic uncertainties. This predictive is illustrated on the right in Figure 1 and shown in comparison to the prediction with only aleatoric uncertainty (MAP) as well as the corresponding homoscedastic regression solution on the left.

Besides showing that our approach performs favorably on commonly used UCI benchmarks and the CRISPR-Cas13 knockdown efficacy datasets (Stirn et al., 2023), we notice the lack of more complex (heteroscedastic) regression benchmark datasets. To that end, we propose _new image-regression benchmarks_ based on image classification datasets. The input images are randomly rotated and the targets are the random rotations with heteroscedastic noise that depends on the label.

Figure 1: Illustration of the proposed training and posterior predictive of a heteroscedastic Bayesian neural network (right) in comparison to a homoscedastic one (left).

## 2 Heteroscedastic Regression with Neural Networks

Due to their universal approximation guarantees (Hornik et al., 1989), deep neural networks have the capacity to solve complex regression problems. As in any regression task, however, unregularized function approximators also have the tendency to overfit to the data. Due to the additional degree of freedom granted by learning both mean and variance in heteroscedastic regression, this problem is amplified, and persists when doing a classical grid search over regularization hyperparameters. In the following, we review prior work that proposes to regularize the influence of the variance and highlight some limitations of these approaches by proposing a new image regression task.

### Regularizing the Influence of the Variance

Skafte et al. (2019) note that naively minimizing the Gaussian log likelihood by learning the mean and variance as output of a neural network can lead to overconfident variance estimates that compromise the mean fit. Taking into account the gradients of the negative log likelihood (NLL),

\[_{,}()=^{2}(;)+;))^{2}}{2^{2}(; )}+,\] (1)

where \(\) denote the neural network parameters used to estimate mean and variance, Seitzer et al. (2022) trace the problem to the fact that \(_{}_{,}()=; )-y}{^{2}(;)}\) heavily depends on the learned variance. To overcome this effect, Seitzer et al. (2022) introduce the \(-\)NLL loss \(_{}()\), which is equal to \(^{2}(;)_{,}( )\), where \(\) denotes a stop-gradient operation, and \(\) is a hyperparameter controlling the dependency of gradients on the predictive variance. As a result, the gradients for \(-\)NLL are equal to

\[_{}_{}()=;)-y}{ ^{2-2}(;)},~{}~{}_{^{2}}_{ }()=(;)-(y-(;))^{2}}{2^{4-2}(;)}~{}.\] (2)

With \(=0\), \(_{}()\) is equivalent to \(_{,}()\), whereas for \(=1\) the gradient with respect to the mean is proportional to the gradient for homoscedastic regression. When setting \(0< 1\) we interpolate between both settings. As an alternative approach, Stim et al. (2023) propose to decouple the estimation into three networks: a shared representation learner \(f_{}\) computes a representation \(\) from \(\), which is passed into two individual networks \(f_{}\) and \(f_{}\), which receive \(\) as input and output the mean and covariance matrix, respectively. To ensure that the gradient with respect to the mean is equal to the gradient of the homoscedastic model, they introduce two stop-gradient operations: the first one has an equivalent effect on the mean gradient for \(f_{}\) and \(f_{}\) as setting \(=1\) in \(\)-NLL, and the second one stops any gradient from the variance network \(f_{}\) from propergating to \(f_{}\)(Stim et al., 2023). We provide more details in App. B.

Taking a different perspective, one can view both proposals as implicit regularization techniques for the variance parameter. The surrogate score of Seitzer et al. (2022) regularizes the influence of the predictive variance on the gradient, while the network architecture and stop-gradient operations introduced by Stim et al. (2023) have a similar, yet stronger regularization effect for the variance--i.e., stopping its influence on the joint representation learner. An additional hurdle presents itself due to the fact that we need to tune the regularization to calibrate the models for a certain dataset, which can have a strong influence on the result, as we show below. Concurrent work (Wong-Toi et al., 2023) also highlights the necessity for regularization in heteroscedastic regression when considering individual mean and variance networks. They find that the networks relative regularization is critical.

### Learning Complex Variance Dependencies

Despite alleviating the problem of compromised mean fits, the question arises if such regularization limits the capabilities for learning the aleatoric uncertainty.

In the \(\)-NLL objective, we need to select the influence of the estimated variance on both gradients, while the effect of scaling is different as can be seen from Equation 2. Due to the typically applied standardization of the data, we expect that \(^{2} 1\) and hence the gradient with respect to the variance is amplified stronger compared to the mean. Especially in near-deterministic processes (\(^{2} 0\)) this might be problematic. Further, it is not clear which value of \(\) is best suited for a problem a priori introducing another hyperparameter that has to be tuned in addition to tuning regularization hyperparameters. For the approach by Stim et al. (2023, _Faithful_) the regularization of the variance ismore severe due to forcing \(()\) and \(()\) to share a joint representation that receives no gradient from the variance. If, for example, \(()\) depends on a set of variables that is independent of those influencing the mean, no information about the variance might be contained in the joint representation. To illustrate that only regularizing the variance can be suboptimal, consider the following example.

**Problem 2.1** (**Heteroscedastic Regression from Image Data**): _Consider a version of rotated MNIST with rotation angle drawn as \((-90,90)\). We generate the target \(y\) as \(y=+(11c+1)\), where \(c\{0,1,,9\}\) is the image class as integer number and \((0,1)\) is an independent noise source. The heteroscedastic regression task of learning the distribution of \(y\) given observations (images) \(\) involves learning a complex non-linear mapping for the variance, while learning the mean only requires learning the rotation angle._

We train a simple 3-layer MLP of width \(500\) using the baseline objectives and our approach (_Natural Laplace_ introduced later in Secs. 3 and 4) on data generated as described above. First, we note that the test-NLL of \(\)-NLL and Faithful strongly depends on the regularization strength (cf. Figure 2), which emphasizes the importance of additional regularization. Further, we report how much information about the image classification task is contained in the last layer of the MLP in Figure 2--which provides a proxy on the information about the variance that has been picked up by each approach. The homoscedastic model serves as a control that does not require knowledge about the label. As expected, Faithful achieves a low accuracy on this downstream task since it limits the capability for learning the variance. \(\)-NLL shows a better performance but is still significantly outperformed by our proposal. The difference becomes even more evident when using a CNN architecture on rotated FashionMNIST as we demonstrate in Sec. 5.3. We also provide a minimal example illustrating the limitations of _Faithful_ in App. C, which shows that the bottleneck size of the joint network \(f_{}\) has a strong influence of the estimation quality of the variance.

As alternatives to Problem 2.1, we consider two generative processes for the rotated image regression task: first, we use a homoscedastic case as ablation with \(y=+10\). Further, we use a heteroscedastic noise that is based on the rotational magnitude, \(y=+|}\). In this case, both mean and variance depend on the same feature, which is a setting where _Faithful_(Stirn et al., 2023) can theoretically work.

## 3 Naturally Parameterized Heteroscedastic Regression

We model a dataset \(=\{(_{n},y_{n})\}_{n=1}^{N}\) with \(N\) pairs of input \(_{n}^{D}\) and scalar response \(y_{n}\) using the natural form of the Gaussian likelihood with unknown variance, as first introduced for the case of Gaussian processes (Le et al., 2005) and recently applied in the context of causal discovery with maximum likelihood (Immer et al., 2023). The relationship between the natural parameters \(\) and the mean \(\) and variance \(^{2}\) of the parametrization is

\[_{1}=}_{2}=-}<0,\] (3)

which can be understood as the signal-to-variance ratio and the negative precision (inverse variance). We model these natural parameters using a neural network \((;)^{2}\) that is parameterized by weights \(^{P}\) and acts on the inputs \(^{D}\). To satisfy the constraint that \(_{2}<0\), we _link_\(\) to \(\) using a positive function \(g_{+}:_{+}\) and have the following mapping:

\[_{1}(;)=f_{1}(;)_{2}( ;)=-g_{+}(f_{2}(;)).\] (4)

Figure 2: Left: Example heteroscedastic image regression data point with target \(y+56\). Middle: Test log likelihood for different values of prior precision for \(\)-NLL and Faithful. Right: Downstream accuracy using a linear model on the last layer representation after learning the heteroscedastic image regression. Compromised variance fit leads to worse downstream accuracy.

We use either the exponential \(g_{+}()=()\) or softplus \(g_{+}()=(1+())\) as typical for heteroscedastic regression with mean-variance parametrization. Mathematically, the _heteroscedastic Gaussian log likelihood of our model_ is given by

\[ p(y|,)=_{1}(;)\\ _{2}(;)^{}y\\ y^{2}+(;)^{2}}{4_{2}( ;)}+(-2_{2}(;))+ ,\] (5)

which we also denote by \(-_{}()\). Assuming the data are _i.i.d._, we have \( p(|)=_{n=1}^{N} p(y_{n}|_{n},)\). Immer et al. (2023) used this maximum likelihood objective for bivariate causal discovery using linear models and small neural networks. Further, the gradient and Hessian take on simple forms due to the properties of the natural parametrization.

### Gradients of the Natural parametrization

Similar to Seitzer et al. (2022) and Stirn et al. (2023), we also inspect the gradients of the corresponding negative log likelihood with respect to \(\), leading to

\[_{_{1}}_{}()=-(; {})}{2_{2}(;)}-y,\ \ _{_{2}}_{}()=(;))^{2}}{4(_{2}(;))^{2} }-(;)}-y^{2}\.\] (6)

The gradients by themselves cannot be directly linked to mean and variance updates. We note, however, that if we relate the natural parameters to mean and variance, i.e., we compute \((;)\) as \(-(;)}{2_{2}(;)}\) and \(^{2}(;)\) as \(-(;)}\), then \(_{_{1}}_{}()\) reduces to \((;)-y\), and similarly, \(_{_{2}}_{}()\) to \(^{2}(;)-(y^{2}-((;))^{2})\), which would be desired because these are simply separate residuals for mean and variance (Seitzer et al., 2022; Stirn et al., 2023). Empirically, we observe that this parametrization can be more stable to train and less prone to insufficient regularization as also observed previously for Gaussian process and ridge regression (Le et al., 2005; Immer et al., 2023).

### Regularization using Bayesian Inference

Due to the expressiveness of heteroscedastic regression, regularization is crucial to obtain well-generalizing models (cf. Figure 2). We achieve regularization in two ways: first, we regularize parameters towards low norm using classical \(^{2}\) regularization, which corresponds to a Gaussian prior on the parameters. Further, we use a Bayesian posterior predictive, which additionally accounts for uncertainties of the model as depicted in the illustration (Figure 1, right).

For effective regularization of deep neural networks, we use a layer-wise Gaussian prior on the parameters given by \(p(|)=_{t}(_{t};, _{t}^{-1})\). In the case of the last layer or a simple linear model with the natural heteroscedastic likelihood, it induces a mode on \(_{1}()=0\) and \(_{2}()=-\), which corresponds to zero mean and unit variance and is reasonable for standardized response variables. The layer-wise prior further allows to differently regularize parts of the neural network and has been observed to improve generalization in image classification (Immer et al., 2021; Daxberger et al., 2021; Antoran et al., 2022). Concurrent work by also suggests that this might be the case for heteroscedastic regression (Wong-Toi et al., 2023). However, optimizing a regularization parameter per layer is intractable using a validation-based grid search.

To optimize layer-wise prior precisions and obtain a posterior predictive, we make use of Bayesian inference. Combining the natural Gaussian likelihood \(p(|)\) (below Equation 5) with the prior \(p(|)\), we have the joint distribution \(p(,|)\), which corresponds to a regularized objective. According to Bayes' theorem, we have the posterior \(p(|,) p(,|)\). The normalization constant, also referred to as _marginal likelihood_, is given by \(p(|)= p(,|)\) and gives us the Type II maximum likelihood objective to optimize the prior precisions \(\). This procedure is referred to as _empirical Bayes_ (EB). Inferring the posterior distribution of the neural network parameters, we further have access to the posterior predictive \(p(y_{*}|_{*},)\) for a new data point \(_{*}\). By averaging over multiple hypotheses from the posterior, the predictive can be better regularized than a single model (Wilson, 2020). Unfortunately, inference is intractable for deep neural networks.

## 4 Approximate Inference with a Laplace Approximation

We develop a scalable Laplace approximation for the posterior in heteroscedastic regression with deep neural networks. The Laplace approximation (MacKay, 1995) is an effective method for approximating the marginal likelihood, posterior, and predictive in deep learning (Daxberger et al., 2021). In comparison to other approximate inference methods, it can rely on effective training algorithms developed for deep learning and also offers a differentiable marginal likelihood estimate that enables empirical Bayes (EB). Efficient curvature approximations further make it scalable to deep learning (Ritter et al., 2018). We extend these to the heteroscedastic regression setting.

Laplace approximates the posterior locally at a mode of the posterior with a Gaussian distribution, \(p(|,)(;_{*},)\). The mean is given by a stationary point of the posterior, \(_{*}=_{} p(,|)\), and the covariance by the curvature at that mode, \(^{-1}=^{2}_{} p(,|)_{=_{*}}\). This is due to a second-order Taylor approximation of the log posterior around the mode. The mean is hence the result of neural network training, however, the covariance requires estimating and inverting a Hessian, which is typically intractable.

### Linearized Laplace for Natural Heteroscedastic Regression

The linearized Laplace approximation (MacKay, 1995; Khan et al., 2019; Foong et al., 2019; Immer et al., 2021) overcomes issues of the vanilla Laplace approximation. Linearizing the neural network about the parameters at the mode, the Hessian, which is the generalized Gauss-Newton in this case (Martens, 2020), becomes positive semidefinite and offers efficient structured approximations. In particular, we have the following Hessian approximation due to linearization

\[^{-1}_{n=1}^{N}_{*}(_{n})^{ }[-^{2}_{} p(y_{n}|_{n},_{*})] _{*}(_{n})+^{2}_{} p(|)|_{=_{*}},\] (7)

where \([_{*}()]_{cp}=(;)}{_{p}}|_{=_{*}}\) is the Jacobian of the neural network \((;)\) at the mode. The first summand is the generalized Gauss-Newton and the second term is the Hessian of the prior, which is simply a diagonal matrix constructed from entries \(_{l}>0\), the layer-wise regularization parameters.

Due to the natural parametrization of the likelihood, the Hessian approximation of the linearized Laplace approximation is guaranteed to be positive definite. Since the log prior Hessian is diagonal with entries \(_{l}>0\), one only has to show that the negative log likelihood Hessian, \(-^{2}_{} p(y|,_{*})\), is positive semidefinite, which is simply a property of naturally parameterized exponential families (Martens, 2020). Note that this would not be the case for the mean-variance parametrization3. For efficient computation, we can decompose the log likelihood Hessian as

\[_{*}() }{=}^{2}_{} p(y| ,_{*})=(x;_{ *})}&-(x;_{*})}{2_{2}(x;_{*})^{2}}\\ -(x;_{*})}{2_{2}(x;_{*})^{2}}&(x;_{*})^{2}}{2_{2}(x;_{*})^{3}}-(x;_{*})^{2}}\] (8) \[=-(x;_{*})}}& (x;_{*})}{_{2}(x;_{*})(x;_{*})}}^{2}-0&(x;_{*})}}^{2}\] (9) \[}{=}_{1}()_{1}( )^{}-_{2}()_{2}()^{},\] (10)

where the square indicates the outer product. This alleviates the need to compute this matrix and instead allows to work with outer products of Jacobian-vector products to compute the covariance in Equation 7. However, the full Hessian approximation remains quadratic in the number of neural network weights. We tackle this issue with a scalable Kronecker-factored approximation.

### Scalable Kronecker-Factored Hessian Approximation

To enable the application of the Laplace approximation to heteroscedastic deep neural networks and large datasets, we use a layer-wise Kronecker-factored approximation (KFAC; Martens and Grosse, 2015; Botev et al., 2017). To overcome the quadratic scaling in the number of parameters, KFAC makes two efficient approximations: first, it constructs a block-diagonal approximation to \(\) from blocks \(_{l}\) per layer \(l\). Second, each block \(_{l}\) is approximated as a Kronecker product that enables efficient storage and computation using only the individual factors. In the following, we revise KFAC for linear layers and define it for the specific case of the heteroscedastic natural Gaussian likelihood. The same derivation applies similarly to other layer types (Osawa, 2021).

Using the Hessian decomposition of the natural log likelihood in Equation 10, we derive a KFAC approximation that can efficiently be computed in a closed-form. We can write the Jacobian of a fully connected layer that maps a \(D\) to a \(D^{}\)-dimensional representation as a Kronecker product \(_{l}(_{n})^{}=_{l,n}_{l,n}\) with \(_{l,n}^{D,1}\) as the layer's input and \(_{l,n}^{D^{} 2}\) as transposed Jacobian w.r.t. the output, both for the input \(_{n}\). Following Martens and Grosse (2015) and Botev et al. (2017), we then have the KFAC approximation

\[[^{-1}]_{l} =_{n=1}^{N}[_{l,n}_{l,n}] _{n}[_{l,n}_{l,n}]^{} +_{l}=_{n=1}^{N}[_{l,n}_{l,n}^{ }][_{l,n}_{n}_{l,n}^ {}]+_{l}\] (11) \[[_{n=1}^{N}_{l,n}_{l,n}^{}][_{n=1}^{N}_{k=1}^{2}_{l,n} _{n,k}_{n,k}^{}_{l,n}^{}]+_{l} }}{{=}}_{l}_{l}+_{l},\]

where \(_{n,k}^{2 1}\) is due to to the decomposition of \(_{n}\) into outer products Equation 10 and the approximation is due to exchanging the sum and product. Conveniently, the terms \(_{l,n}_{n,k}\) can be computed efficiently using two Jacobian-vector products and the Kronecker factors can then be extracted within the second-order framework of Osawa (2021). To the best of our knowledge, this is the first instantiation of KFAC for heteroscedastic regression. While we derive it for the Laplace approximation, it could also be useful for optimization Martens and Grosse (2015).

### Empirical Bayes for Automatic Regularization

To automatically regularize the heteroscedastic neural network, we use an empirical Bayes (EB) procedure that optimizes the layer-wise prior precisions, \(_{l}\), during training by maximizing the Laplace approximation to the _marginal likelihood_(Immer et al., 2021). This procedure can exhibit a Bayesian variant of Occam's razor (Rasmussen and Ghahramani, 2000) and trades off model fit and complexity. Although online training violates the stationarity assumption of the Laplace approximation, it has been observed to work well in practice (Immer et al., 2021, 2023; Daxberger et al., 2021; Lin et al., 2023). We use gradient-based optimization of the log marginal likelihood,

\[ p(|) p(|_{*})+ p(_{*}|)+ ||+ 2 p(_{*}|)+||,\] (12)

which crucially requires differentiating the log-determinant w.r.t. \(\), which is only tractable for small neural networks. For deep neural networks, it can be done efficiently using the KFAC approximation derived in Sec. 4.2 by eigendecomposition of the individual Kronecker factors (Immer et al., 2021, 2022). In practice, we compute the marginal likelihood approximation every few epochs to adapt the regularization, which effectively mitigates overfitting, and use it as an early-stopping criterion. The detailed empirical Bayes training algorithm is also described in Alg. 1.

### Posterior Predictive for Epistemic Uncertainties

We use the linearized posterior predictive that is in line with the linearized Laplace posterior approximation and performs typically better than sampling weights directly (Immer et al., 2021). We define the linearized neural network as \(_{*}^{}(;)}}{{=}}(;_{*})+ _{*}()(-_{*})\). Due to the Gaussianity of the Laplace approximation, \((_{*};)\), and the linearization, we can express the function-space posterior as a Gaussian on the natural parameters \(_{*}^{}()((;_{*});_{*}() {}_{*}()^{})}}{{=}}q(|)\). A straightforward way to approximate the posterior predictive is then a Monte-Carlo estimate of \(p(y_{*}|_{*},) p(y_{*}|_{*}, )q(|_{*})\,\) by sampling multiple \(_{*}^{}()\).

Alternatively, we propose to use an approximation to the posterior predictive that can be computed in a closed-form without sampling, similar to the _probit approximation_ commonly used in the classification setting (Daxberger et al., 2021). To enable a closed-form posterior predictive, we restrict ourselves to the epistemic uncertainty about the mean as proposed by Le et al. (2005) for heteroscedastic Gaussian process regression. Instead of linearizing the natural parameters, we first transform them to the mean and variance using the inverse mapping of Equation 3, i.e., we have \((;)=-(;)}{2_{2}(;)}\) and \(^{2}(;_{*})=-(; _{*})}\). Next, we only linearize the mean function \(_{*}^{}(;)=(; _{*})+_{*,}()(- _{*})\), where \(_{*,}()\) is the Jacobian of the mean, and have

\[p(y_{*}|_{*},) \!\!(y_{*}|_{*}^{}(_{*}),^{2}(;_{*}))(;_{*},)\,\] \[=(y_{*};(;_{*}), _{*,}(_{*})_{*, }(_{*})^{}}_{}+( ;_{*})}_{}),\] (13)

where the epistemic and aleatoric uncertainty about the mean are clearly split. An example of this posterior predictive approximation is shown in Figure 1.

### Laplace Approximation for Mean-Variance parametrization

Using the natural parameter mapping, it is possible to apply above Laplace approximations to the mean-variance parametrization and profit from the empirical Bayes and posterior predictive procedures. However, because the negative log likelihood Hessian w.r.t. the mean and variance parameters can be indefinite, this does not work naively. By mapping \(,^{2}\) to the natural parameters using Equation 3, all above derivations apply and correspond to a separate Gauss-Newton approximation of the log likelihood with Jacobians of the mapping and the natural log likelihood Hessian (see App. A).

### Limitations

As common for heteroscedastic regression, we make the assumption that the conditional distribution of the target given the observations follows a Gaussian distribution. A practitioner should keep in mind that the full Laplace approximation has high computational complexity, and should resort to the KFAC approximation, which we also use for more complex settings in experiments. Further, the true posterior of neural networks is in most cases multimodal while our Laplace approximation only covers a single mode. However in practical settings, the Laplace approximation provides strong results (Daxberger et al., 2021) and can even be ensembled (Eschenhagen et al., 2021).

## 5 Experiments

We evaluate the effectiveness of the natural parameterization compared to the mean-variance (naive) one, and empirical Bayes (EB) to optimizing a single regularization parameter using a grid search on the validation set (GS), and the MAP prediction vs a Bayesian posterior predictive (PP) in comparison to state-of-the-art baselines on three experimental settings: the UCI regression benchmark (Hernandez-Lobato and Adams, 2015), which is also well-established for heteroscedastic regression (Seitzer et al., 2022; Stim et al., 2023), the recently introduced CRISPR-Cas13 gene expression datasets (Stim et al., 2023), and our proposed heteroscedastic image-regression dataset (cf. Problem 2.1) in three noise variants. For UCI regression, we use the full Laplace approximation and we resort to the KFAC approximation for the remaining tasks for computational efficiency. Further, we use the proposed closed-form posterior predictive for our Laplace approximations. If applicable, as for our methods, we test the effect of having a Bayesian posterior predictive (PP) in comparison to the point estimate.

BaselinesAs control to assess the benefit of heteroscedastic aleatoric uncertainty, we include a homoscedastic model with EB and Laplace posterior predictive. In addition, we include the mean-variance parameterization of the negative log likelihood loss for heteroscedastic regression (_Naive NLL_). As competitive baselines, we include the recently proposed \(\)_-NLL_(Seitzer et al., 2022) and _Faithful_(Stim et al., 2023) heteroscedastic losses. Finally we include well-established approaches for heteroscedastic regression with Bayesian neural networks, namely mean-field variational inference (_VI_) (Graves, 2011) and Monte-Carlo Dropout (Gal and Ghahramani, 2016) (_MC-Dropout_). Note that for our VI baseline we employ Flipout (Wen et al., 2018) to improve gradient estimation.

Details on model architectures, hyperparameter tuning, and additional results are in App. D.

### UCI Regression

In Table 1 we report results on the UCI regression datasets (Hernandez-Lobato and Adams, 2015) for the compared models in terms of test log likelihood. We underline the best performance for each dataset, and bold results which are not statistically distinguishable from best performance. That is, if the mean performance of a model falls within the region of the mean plus minus two standard errors of the best performing model. The two rightmost columns report the total number of _wins_ and _ties_ for each model: wins are the number of datasets in which the given model achieves the best performance, while ties are the number of datasets in which the model achieves results which are not statistically distinguishable from the best performing model. The results validate the effectiveness of EB regularization on different heteroscedastic loss parameterizations. Notably, with the Bayesian posterior predictive (PP) our proposed methods considerably improve in performance in comparison to using a point estimate, and significantly outperform existing state-of-the-art approaches. Finally the results confirm that, particularly when the point prective is used, using the Natural parameterization of the heteroscedastic NLL improves training stability.

[MISSING_PAGE_FAIL:9]

To create the modified datasets, we follow the procedure as outlined in Problem 2.1. An observation \(\) is generated by rotating an MNIST (resp. FashionMNIST) image with rotation angle drawn as \((-90,90)\), and the corresponding target \(y\) is generated as \(y=+(11c+1)\), where \(c\{0,1,,9\}\) is the image class of \(\) as integer number and \((0,1)\) is an independent noise source. To solve the problem, a heteroscedastic regression model needs to 1) learn the mapping of rotated image to angle and 2) learn the label of the image to correctly fit the heteroscedasticity of the observation noise, both of which require learning complex functions. We evaluate the methods based on the test log likelihood and RMSE. In addition, we compute the KL-divergence of the predicted distribution from the ground truth.4 We train a 3-layer 500-neuron wide MLP with ReLU activation for the MNIST-based dataset and a CNN (3 conv. and 2 linear) for the task based on FashionMNIST. In App. D.2.4, we provide additional details and results on two alternative tasks.

The results on image regression with heteroscedastic label noise indicate that the proposed empirical Bayes (EB) algorithm leads to better generalizing models and the posterior predictive (PP) strictly improves performance for both parameterizations. It is also notable that _Faithful_ fails to fit the model due to the restrictions on its variance function, and therefore performs worse than a homoscedastic model on FashionMNIST. \(\)-NLL, MC-Dropout, and VI all give relatively good results but fall slightly short of the proposed methods.

## 6 Conclusions

We tackled the problem of heteroscedastic regression using neural networks with a focus on regularization. To this end we proposed three individual improvements: 1) we use the natural form of the Gaussian likelihood; we derive a scalable Laplace approximation that 2) can be used for automatic principled regularization without validation data, and 3) enables epistemic uncertainty estimation through its posterior predictive. We show on image data that our method is scalable despite the Bayesian procedures and benchmark our approach on UCI regression as well as CRISPR-Cas13 datasets achieving state-of-the-art performance. For future work, it would be interesting to relax the assumption about Gaussianity and apply our approach to other real-world datasets.

    & & &  &  \\  Objective & Regular- & Posterior & LL (\(\)) & \(D_{}\) (\(\)) & RMSE (\(\)) & LL (\(\)) & \(D_{}\) (\(\)) & RMSE (\(\)) \\  Homoscedastic & EB & & & -5.55 (0.01) & 0.77 (0.02) & 25.6 (0.8) & -5.54 (0.00) & 0.67 (0.00) & 18.7 (0.2) \\ Naive NLL & GS & ✗ & -5.56 (0.01) & 0.68 (0.00) & 19.0 (0.1) & -5.37 (0.01) & 0.49 (0.00) & 20.1 (0.1) \\ \(\)-NLL (0.5) & GS & ✗ & -5.36 (0.01) & 0.49 (0.01) & 20.7 (0.8) & -5.47 (0.03) & 0.59 (0.03) & 23.5 (0.3) \\ \(\)-NLL (1) & GS & ✗ & -5.38 (0.01) & 0.51 (0.01) & 21.5 (0.5) & -5.53 (0.03) & 0.65 (0.03) & 25.5 (0.2) \\ Faithful & GS & ✗ & -5.56 (0.01) & 0.69 (0.00) & 21.1 (0.3) & -5.78 (0.00) & 0.91 (0.00) & 51.8 (0.0) \\ MC-Dropout & GS & ✓ & -5.42 (0.02) & 0.55 (0.01) & 21.0 (0.5) & -5.37 (0.01) & 0.48 (0.00) & 20.1 (0.1) \\ VI & GS & ✓ & -5.42 (0.04) & 0.56 (0.04) & 21.9 (0.4) & -5.39 (0.01) & 0.51 (0.01) & 21.4 (0.5) \\   &  & ✓ & **-5.30 (0.00)** &  & **18.2 (0.3)** & **-5.34 (0.00)** &  & **18.3 (0.2)** \\  & & ✗ & -5.31 (0.01) & & & **-5.35 (0.01)** & & \\   &  & ✓ & -5.42 (0.00) & 0.55 (0.00) & 19.2 (0.1) & -5.40 (0.01) & 0.56 (0.01) & 24.3 (1.4) \\  & & ✗ & -5.42 (0.00) & & & -5.44 (0.01) & & \\    & EB & ✓ & **-5.30 (0.01)** &  &  & **-5.34 (0.00)** &  & **18.1 (0.1)** \\    & & ✗ & -5.32 (0.01) & & & **-5.35 (0.00)** & & \\   

Table 2: Performance metrics on heteroscedastic image regression (Problem 2.1) on MNIST and FashionMNIST with MLP and CNN architectures, respectively. We report the mean and standard error and bold all numbers that are statistically indistinguishable from the best result. The bottom half shows the proposed methods. We find that the empirical Bayes (EB) procedure leads to better regularized and performing models than using grid search (GS). The posterior predictive also strictly improves the test log likelihood, albeit not as significantly.

#### Acknowledgements

AI gratefully acknowledges funding by the Max Planck ETH Center for Learning Systems (CLS). EP and AM were supported by a fellowship from the ETH AI Center.