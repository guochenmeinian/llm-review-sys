# Convolutional Monge Mapping Normalization

for learning on sleep data

 Theo Gnassounou

Universite Paris-Saclay, Inria, CEA

Palaiseau 91120, France

theo.gnassounou@inria.fr

&Remi Flamary

IP Paris, CMAP, UMR 7641

Palaiseau 91120, France

remi.flamary@polytechnique.edu

Alexandre Gramfort

Universite Paris-Saclay, Inria, CEA

Palaiseau 91120, France

alexandre.gramfort@inria.fr

A. Gramfort joined Meta and can be reached at agramfort@meta.com

###### Abstract

In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) methods and can be used in conjunction with those for even better performances.

## 1 Introduction

Data shift in biological signalsBiological signals, such as electroencephalograms (EEG), often exhibit a significant degree of variability. This variability arises from various factors, including the recording setup (hardware specifications, number of electrodes), individual human subjects (variations in anatomies and brain activities), and the recording session itself (electrode impedance, positioning). In this paper, we focus on the problem of sleep staging which consists of measuring the activities of the brain and body during a session (here one session is done over a night of sleep) with electroencephalograms (EEG), electrooculograms (EOG), and electromyograms (EMG)  to classify the sleep stages. Depending on the dataset, the populations studied may vary from healthy cohorts to cohorts suffering from disease [2; 3; 4]. Different electrodes positioned in the front/back  or the side of the head  can be employed. Sleep staging is a perfect problem for studying the need to adapt to this variability that is also commonly denoted as data shift between domains (that can be datasets, subjects, or even sessions).

Normalization for data shiftA traditional approach in machine learning to address data shifts between domains is to apply data normalization. Different approaches exist in the literature tonormalize data. One can normalize the data per Session which allows to keep more within-session variability in the data . If the variability during the session is too large, one can normalize independently each window of data (_e.g.,_ 30 s on sleep EEG) , denoted as Sample in the following. It is also possible not to normalize the data, and let a neural network learn to discard non-pertinent variabilities helped with batch normalization [7; 8]. More recently, a number of works have explored the possibility of learning a normalization layer that is domain specific in order to better adapt their specificities [9; 10; 11; 12; 13; 14; 15]. However, this line of work usually requires to have labeled data from all domains (subjects) for learning which might not be the case in practice when the objective is to automatically label a new domain without an expert.

Domain Adaptation (DA) for data shiftDomain Adaptation is a field in machine learning that aims at adapting a predictor in the presence of data shift but in the absence of labeled data in the target domain. The goal of DA is to find an estimator using the labeled source domains which generalizes well on the shifted target domain . In the context of biological signals, DA is especially relevant as it has the ability to fine-tune the predictor for each new target domain by leveraging unlabeled data. Modern DA methods, inspired by successes in computer vision, usually try to reduce the shift between the embeddings of domains learned by the feature extractor. To do that, a majority of the methods try to minimize the divergence between the features of the source and the target data. Several divergences can be considered for this task such as correlation distance , adversarial method , Maximum Mean Discrepancy (MMD) distance  or optimal transport [20; 21]. Another adaptation strategy consists in learning domain-specific batch normalization  in the embedding. Interestingly those methods share an objective with the normalization methods: they aim to reduce the shift between datasets. To this end, DA learns a complex invariant feature representation whereas normalization remains in the original data space. Finally, test-time DA aims at adapting a predictor to the target data without access to the source data , which might not be available in practice due to privacy concerns or the memory limit of devices.

ContributionsIn this work we propose a novel and efficient normalization approach that can compensate at test-time for the spectral variability of the domain signals. Our approach called Convolutional Monge Mapping Normalization (CMMN), illustrated in Figure 1, uses a novel closed-form to estimate a meaningful barycenter from the source domains. Then CMMN uses a closed-form solution for the optimal transport Monge mapping between Gaussian random signals to align the power spectrum density of each domain (source and target) to the barycenter. We emphasize that CMMN is, to the best of our knowledge, the first approach that can adapt to complex spectral shifts in the data without the need to access target datasets at training time or train a new estimator for each new domain (which are the limits of DA).

We first introduce in section 2 the problem of optimal transport (OT) between Gaussian distributions, and then propose a novel closed-form solution for the Wasserstein barycenter for stationary Gaussian random signals. We then use this result to propose our convolutional normalization procedure in section 3, where implementation details and related works are also discussed. Finally section 4 reports a number of numerical experiments on sleep EEG data, demonstrating the interest of CMMN for adapting to new subjects, sessions, and even datasets, but also study its interaction with DA methods.

NotationsVectors are denoted by small cap boldface letters (_e.g.,_\(\)), and matrices are denoted by large cap boldface letters (_e.g.,_\(\)). The element-wise product is denoted by \(\). The element-wise power of \(n\) is denoted by \( n\). \([K]\) denotes the set \(\{1,...,K\}\). \(|.|\) denotes the absolute value. The discrete convolution operator between two signals is denoted as \(*\). Any parameters written with a small \(k\) (_e.g.,_\(_{k}\) or \(_{k}^{k}\)) is related to the source domain \(k\) with \(1 k K\). Any parameters written with a small t (_e.g.,_\(_{t}\) or \(_{i}^{t}\)) is related to the target domain.

## 2 Signal adaptation with Optimal Transport

In this section, we first provide a short introduction to optimal transport between Gaussian Distributions, and then discuss how those solutions can be computed efficiently on stationary Gaussian signals, exhibiting a new closed-form solution for Wasserstein barycenters.

### Optimal Transport between Gaussian distributions

Monge mapping for Gaussian distributionsLet two Gaussian distributions \(_{s}=(_{s},_{s})\) and \(_{t}=(_{t},_{t})\), where \(_{s}\) and \(_{t}\) are symmetric positive definite covariances matrices. OT between Gaussian distributions is one of the rare cases where there exists a closed-form solution. The OT cost, also called the Bures-Wasserstein distance when using a quadratic ground metric, is [23; 24]

\[_{2}^{2}(_{s},_{t})=\|_{s}-_{t}\|_{2}^{2 }+(_{s}+_{t}-2(_{t}^{}_{s}_{t}^{} )^{})\,\] (1)

where the second term is called the Bures metric  between positive definite matrices. The OT mapping, also called Monge mapping, can be expressed as the following affine function :

\[m()=(-_{s})+_{t}, {with}=_{s}^{-}(_{s}^{}_{t}_{s}^{}) ^{}_{s}^{-}=^{}\.\] (2)

In practical applications, one can estimate empirically the means and covariances of the two distributions and plug them into the equations above. Interestingly, in this case, the concentration of the estimators has been shown to be in \(O(N^{-1/2})\), where \(N\) is the number of samples, for the divergence estimation  and for the mapping estimation . This is particularly interesting because optimal transport in the general case is known to be very sensitive to the curse of dimensionality with usual concentrations in \(O(N^{-1/D})\) where \(D\) is the dimensionality of the data .

Wasserstein barycenter between Gaussian distributionsThe Wasserstein barycenter that searches for an average distribution can also be estimated between multiple Gaussian distributions \(_{k}\). This barycenter \(\) is expressed as

\[=}_{k=1}^{K}_{2}^ {2}(,_{k})\.\] (3)

Interestingly, the barycenter is still a Gaussian distribution \(=(},})\). Its mean \(}=_{k}_{k}\) can be computed as an average of the means of the Gaussians, yet there is no closed-form for computing the covariance \(}\). In practical applications, practitioners often use the following optimality condition from 

\[}=_{k=1}^{K}(^{}_{k}}^{})^{ {2}}\,\] (4)

Figure 1: Illustration of the CMMN approach. At train-time the Wasserstein barycenter is estimated from 3 subjects/domains. The model is learned on normalized data. At test time the same barycenter is used to normalize test data and predict.

in a fixed point algorithm that consists in updating the covariance  using equation (4) above until convergence. Similarly to the distance estimation and mapping estimation, statistical estimation of the barycenter from sampled distribution has been shown to have a concentration in \(O(N^{-1/2})\).

### Optimal Transport between Gaussian stationary signals

We now discuss the special case of OT between stationary Gaussian random signals. In this case, the covariance matrices are Toeplitz matrices. A classical assumption in signal processing is that for a long enough signal, one can assume that the signal is periodic, and therefore the covariance matrix is a Toeplitz circulant matrix. The circulant matrix can be diagonalized by the Discrete Fourier Transform (DFT) \(=()^{*}\), with \(\) and \(^{*}\) the Fourier transform operator and its inverse, and \(\) the Power Spectral Density (PSD) of the signal.

Welch PSD estimationAs discussed above, there is a direct relation between the correlation matrix of a signal and its PSD. In practice, one has access to a matrix \(^{N T}\) containing \(N\) signals of length \(T\) that are samples of Gaussian random signals. In this case, the PSD \(\) of one random signal can be estimated using the Welch periodogram method  with \(}=_{i=1}^{N}|_{i}|^{ 2}\) where \(||\) is the element-wise magnitude of the complex vector.

Monge mapping between two Gaussian signalsThe optimal transport mapping between Gaussian random signals can be computed from (2) and simplified by using the Fourier-based eigen-factorization of the covariance matrices. The mapping between two stationary Gaussian signals of PSD respectively \(_{s}\) and \(_{t}\) can be expressed with the following convolution :

\[m()=*\,= ^{*}(_{t}^{}_{s}^{-})\.\] (5)

Note that the bias terms \(_{s}\) and \(_{t}\) do not appear above because one can suppose in practice that the signals are centered (or have been high-pass filtered to be centered), which means that the processes are zero-mean. The Monge mapping is a convolution with a filter \(\) that can be efficiently estimated from the PSD of the two signals. It was suggested in  as a Domain Adaptation method to compensate for convolutional shifts between datasets. For instance, it enables compensation for variations such as differing impedances, which can be physically modeled as convolutions . Nevertheless, this paper focused on theoretical results, and no evaluation on real signals is reported. Moreover, the method proposed in  cannot be used between multiple domains (as explored here). This is why in the following we propose a novel closed-form for estimating a barycenter of Gaussian signals that we will use for the normalization of our CMMN method.

Wasserstein barycenter between Gaussian signalsAs discussed above, there is no known closed-form solution for a Wasserstein barycenter between Gaussian distributions. Nevertheless, in the case of stationary Gaussian signals, one can exploit the structure of the covariances to derive a closed-form solution that we propose below.

**Lemma 1**: _Consider \(K\) centered stationary Gaussian signals of PSD \(_{k}\) with \(k[K]\), the Wasserstein barycenter of the \(K\) signals is a centered stationary Gaussian signal of PSD \(}\) with:_

\[}=(_{k=1}^{K}_{k}^{ {2}})^{ 2}\.\] (6)

Proof.The proof is a direct application of the optimality condition (4) of the barycenter. The factorized covariances in (4), the matrix square root and the inverse can be simplified as element-wise square root and inverse, recovering equation (11). We provide a detailed proof in the appendix.

The closed-form solution is notable for several reasons. First, it is a novel closed-form solution that avoids the need for iterative fixed-point algorithms and costly computations of matrix square roots. Secondly, the utilization of the Wasserstein space introduces an alternative approach to the conventional \(_{2}\) averaging of Power Spectral Density (PSD). This approach involves employing the square root, like the Hellinger distance , potentially enhancing robustness to outliers. Note that while other estimators for PSD averaging could be used this choice is motivated here by the fact that we use OT mappings and that the barycenter above is optimal _w.r.t._ those OT mappings.

Multi-source DA with Convolutional Monge Mapping Normalization

We now introduce the core contribution of the paper, that is an efficient method that allows to adapt to the specificities of multiple domains and train a predictor that can generalize to new domains at test time without the need to train a new model. We recall here that we have access to \(K\) labeled source domains \((_{k},_{k})_{k}\). We assume that each domain contains \(N_{k}\) centered signals \(_{i}^{k}\) of size \(T\).

CMMN at train timeThe proposed approach, illustrated in Figure 1 and detailed in Algorithm 1, consists of the following steps:

1. Compute the PSD \(}_{k}\) for each source domain and use them to estimate the barycenter \(}\) with (11).
2. Compute the convolutional mapping \(_{k}\) (5) between each source domain and the barycenter \(}\).
3. Train a predictor on the normalized source data using the mappings \(_{k}\): \[_{f}_{k=1}^{K}_{i=1}^{N_{k}}L(y_{i}^{k},f(_{k }*_{i}^{k})).\] (7)

In order to keep notations simple, we consider here the case for a single sensor, but CMMN can be extended to multi-sensor data by computing independently the Monge mapping for each sensor. Note that steps 1 and 2 can be seen as a pre-processing and are independent of the training of the final predictor, so CMMN can be used as pre-processing on any already existing learning framework.

CMMN at test timeAt test time, one has access to a new unlabeled target domain \((_{t})\) and the procedure, detailed in Algorithm 2, is very simple. One can estimate the PSD \(}_{t}\) from the target domain unlabeled data and compute the mapping \(_{t}\) to the barycenter \(}\). Then the final predictor for the new domain is \(f^{t}(_{t})=f(_{t}*_{t})\), that is the composition of the domain-specific mapping to the barycenter of the training data, and the already trained predictor \(f\). This is a very efficient test-time adaptation approach that only requires an estimation of the target domain PSD that can be done with few unlabeled target samples. Yet, it allows for a final predictor to adapt to the spectral specificities of new domains thanks to the convolutional Monge normalization.

``` Input:\(f\), \(F\), \(\{_{k}\}_{k}^{K}\) for\(k=1 K\)do \(}_{k}\) Welch PSD estimation of \(_{k}\) end \(}\) Compute barycenter with (11) for\(k=1 K\)do \(_{k}\) Compute mapping from (5) end \(\) Train on adapted data with (7) return\(\), \(}\) ```

**Algorithm 1**Train-Time CMMN

Numerical complexity and filter sizeThe numerical complexity of the method is very low as it only requires to compute the PSD of the domains and the barycenter in \(O(_{k}^{K}N_{k}T(T))\). It is also important to note that in practice, the method allows for a size of normalization filter \(F\) that is different (smaller) than the size \(T\) of the signals. This consists in practice in estimating the PSD using Welch periodogram on signal windows of size \(F T\) that can be extracted from the raw signal or from already extracted fixed sized source training samples. Indeed, if we use \(F=T\) then the estimated average PSD can be perfectly adapted by the mapping, yet using many parameters can lead to overfitting which can be limited using a smaller filter size \(F T\), In fact, it is interesting to note that the special case \(F=1\) boils down to a scaling of the whole signal similar to what is done with a simple z-score operation. This means that the filter size \(F\) is an hyperparameter that can be tuned on the data. From an implementation point of view, one can use the Fast Fourier Transform (FFT) to compute the convolution (for large filters) or the direct convolution, which both have very efficient implementation on modern hardware (CPU and GPU).

Related WorksCMMN is a computationally efficient approach that benefits from a wide array of recent results in optimal transport and domain adaptation. The idea of using optimal transport to adapt distributions was first proposed in . The idea to compute a Wasserstein barycenter of distributionsfrom multiple domains and use it for adaptation was introduced in . Both of those approaches have shown encouraging performances but were strongly limited by the numerical complexity of solving the OT problems (mapping and barycenters) on large datasets (\(O(_{k=1}^{K}N_{k}^{3}(N_{k}))\) or at least quadratic in \(N_{k}\) for entropic OT). CMMN does not suffer from this limitation as it relies on both Gaussian and stationary signals assumptions that allow to estimate all the parameters for a complexity \(O(_{k=1}^{K}N_{k}(N_{k}))\) linear with the number of samples \(N_{k}\), and quasi-linear in dimensionality of the signals \(T\). The use of Gaussian modeling and convolutional Monge mapping for DA was first proposed in  but the paper was mostly theoretical and only focused on the standard 2-domain DA problem whereas CMMN handles multi-source and provides test-time adaptation.

Finally, CMMN also bears resemblance with the convolutional normalization layer proposed in  that also uses the FFT for fast implementation. Yet, it needs to be trained using labeled source and target data, which prevents its use on DA at test time on new unseen domains.

## 4 Numerical experiments

In this section, we evaluate CMMN on the clinical application of sleep stage classification from EEG signals with [6; 36]. In the following one domain can be a session of one subject (_i.e.,_ in the domain-specific experiment in section 4.2) or one subject (_i.e.,_ all other experiments). We first compare CMMN to classical normalization methods and to subject-specific normalizations. Next, we illustrate the behavior of CMMN when used with different neural network architectures, and study the effect of CMMN on low-performing subjects. Finally, we study the use of CMMN in conjunction with domain adaptation approaches. In order to promote research reproducibility, code is available on github 2, and the datasets used are publicly available.

### Experimental setup

Sleep staging datasetsWe use three publicly available datasets: Physionet (a.k.a SleepEDF) , SHHS [4; 37] and MASS . On all datasets, we want to perform sleep staging from 2-channels EEG signals. The considered EEG channels are 2 bipolar channels, Fpz-Cz and Pz-Cz that have been known to provide discriminant information for sleep staging. Note that those channels were not available on the SHHS dataset, and we used the C3-A2 and C4-A1 instead. This brings another level of variability in the data. More details about the datasets are available as supplementary material.

Pre-processingFor all experiments, we keep 60 subjects of each dataset and the two EEG channels. The same pre-processing is applied to all sensors. First, the recordings are low-pass filtered with a 30 Hz cutoff frequency, then the signals are resampled to 100 Hz. Then we extract 30 s epochs having a unique class. This pre-processing is common in the field [6; 38]. All the data extraction and the pre-processing steps are done with MNE-BIDS  and MNE-Python .

Neural network architectures and trainingMany neural network architectures dedicated to sleep staging have been proposed [8; 7; 41]. In the following, we choose to focus on two architectures: Chambon  and DeepSleepNet . For both architectures, we use the implementation from brain-decode . Chambon is an end-to-end neural network proposed to deal with multivariate time series and is composed of two convolutional layers with non-linear activation functions. DeepSleepNet is a more complex model with convolutional layers, non-linear activation functions, and a Bi-LSTM to model temporal sequences.

We use the Adam optimizer with a learning rate of \(10^{-3}\) for Chambon and \(10^{-4}\) with a weight decay of \(1 10^{-3}\) for DeepSleepNet. The batch size is set to \(128\) and the early stopping is done on a validation set corresponding to \(20\%\) of the subjects in the training set with a patience of \(10\) epochs. For all methods, we optimize the cross entropy with class weight, which amounts to optimizing for the balanced accuracy (BACC).

Various metrics are commonly used in the field such as Cohen's kappa, F1-score, or Balanced Accuracy (BACC) [36; 6]. We report here the BACC score as it is a metric well adapted to unbalanced classification tasks such as sleep staging. We also report in some experiments the gain the balanced accuracy, when using CMMN, of the 20% worst performing domains/subjects in the target domain denoted as \(\)BACC@20 in the following.

Filter size and sensitivity analysis for CmmnOur method has a unique hyperparameter that is the size of the filter \(F\) used. In our experiments, we observed that, while this parameter has some impact, it is not critical and has quite a wide range of values (\(\)) that leads to systematic performance gains. We provide in supplementary material a sensitivity analysis of the performance for different adaptation scenarios (pairs of datasets). It shows that the value \(F=128\) is a good trade-off that we used below for all experiments.

### Comparison between different normalizations

We now evaluate the ability of CMMN to adapt to new subjects within and across datasets and also between two sessions of the same subject.

Classical normalizationsWe first compare CMMN to several classical normalization strategies. We compare the use of raw data  letting the neural network learn the normalization from the data (None), standard normalization of each 30 s samples  that discard global trend along the session (Sample) and finally normalization by session that consists in our case to perform normalization independently on each domain (Session) . We train the Chambon neural network on the source data of one dataset and evaluate on the target data of all other datasets for different splits.

The BACC for all dataset pairs and normalization are presented in the Table 1. The three classical normalizations have similar performances with a slight edge for Sample on average. All those approaches are outperformed by CMMN in 8 out of 9 dataset pairs with an average gain of 4% w.r.t. the best performing Sample. This is also visible on the Boxplot on the right where the BACC for all subjects/domains (_i.e.,_ points) is higher for CMMN. Since the very simple Sample normalization is the best-performing competitor, we used it as a baseline called No Adapt in the following.

Domain specific normalizationWe have shown above that the CMMN approach allows to better cope with distribution shifts than standard data normalizations. This might be explained by the fact that CMMN normalization is domain/subject specific. This is why we now compare to several existing domain-specific normalizations. To do this we adapt the method of Liu _et al._ which was designed for image classification. We implemented domain-specific convolution layers (Conv), batch normalization (Norm), or both (ConvNorm). In practice, we have one layer per domain that is trained (jointly with the predictor \(f\)) only on data from the corresponding domain. The limit of domain-specific normalization is that all test domains must be represented in the training set. Otherwise, if a new domain arrives in the test set, no layer specific to that domain will have been trained.

To be able to compare these methods to CMMN, we use for this section the Physionet dataset for which two sessions are available for some subjects. The first sessions are considered as the training set where the domains are the subjects and the second sessions are split between the validation set (20%) and the test set (80%). The validation set is used to do the early stopping, and validate the kernel size of the subject-specific convolution for Conv and ConvNorm.

We can see in Table 2 that for cross-session adaptation, the gain with CMMN is smaller than previous results (1% BACC gain), which can be explained by the presence of subjects data in both domains resulting in a smaller shift between distribution. However, CMMN outperforms all other subject-specific normalizations which are struggling to improve the results (_i.e.,_ around 4% of BACC loss).

   Datasets Norm. & None  & Sample  & Session  & COM \\  MASS\(\)MASS & \(73.9 1.4\) & \(75.1 1.0\) & \(76.0 2.4\) & \(\) \\ Phys.\(\)Phys. & \(68.8 2.8\) & \(69.2 2.7\) & \(69.4 3.0\) & \(\) \\ SHHS\(\)SHHS & \(55.1 12.5\) & \(61.2 3.8\) & \(60.8 2.6\) & \(\) \\  MASS\(\)Phys & \(55.9 3.1\) & \(58.4 2.4\) & \(57.5 2.0\) & \(\) \\ MASS\(\)SHHS & \(45.8 3.3\) & \(41.8 3.6\) & \(37.4 3.6\) & \(\) \\ Phys.\(\)MASS & \(63.8 3.9\) & \(64.0 2.7\) & \(63.7 2.3\) & \(\) \\ Phys.\(\)SHHS & \(\) & \(45.6 2.1\) & \(47.9 1.8\) & \(51.6 1.8\) \\ SHHS\(\)MASS & \(48.7 4.8\) & \(57.0 2.8\) & \(51.8 6.4\) & \(\) \\ SHHS\(\)Phys. & \(52.6 4.2\) & \(55.0 2.7\) & \(52.4 4.1\) & \(\) \\  Mean & \(57.6 4.3\) & \(58.6 2.6\) & \(57.4 3.1\) & \(\) \\   

Table 1: Balanced accuracy (BACC) for different normalizations and different train/test dataset pairs (left). Boxplot for all normalization approaches on the specific pair SHHS\(\)MASS (right). CMNN outperforms other normalizations.

### Study of the performance gain: neural architecture and human subjects

Previous experiments have shown the superiority of CMMN over all the other normalizations. In this section, we study the behavior of CMMN on different neural network architectures and study which subject gains the most performance gain.

Performance of CMMN with different architecturesIn addition to Chambon that was used in the previous experiments, we now evaluate CMMN considering a different network architecture: DeepSleepNet. The results for both architectures are reported in Table 3, where CMMN is consistently better for both architectures. Notably, the only configuration where the gain is limited is MASS\(\)MASS with DeepSleepNet because MASS is the easiest dataset with less variability than other pairs. Finally, we were surprised to see that DeepSleepNet does not perform as well as Chambon on cross-dataset adaptation, probably due to overfitting caused by a more complex architecture.

Performance gain on low-performing subjectsIn medical applications, it is often more critical to have a model that has a low failure mode, rather than the best average accuracy. As a first step toward studying this, we report two scatter plots reported in Table 4 plotting the BACC for individual target subjects without adaptation as a function of the BACC with CMMN, for different architectures and dataset pairs. First, the majority of the subjects are above the axis \(x=y\), which means that CMMN improves their score. But the most interesting finding is the large improvement for the low-performing subjects that can gain from 0.3 to 0.65 BACC.

We also provide in Table 4 the \(\)BACC@20, that is the average BACC gain on the 20% lowest performing subjects without adaptation. On average, both architectures increase by 7% the BACC on those subjects, when it is only increased by 4% for all subjects. Some \(\)BACC@20 are even greater

   Normalization & BACC \\  No Adapt & \(73.7 0.7\) \\ Conv & \(67.5 2.7\) \\ Norm & \(69.4 1.6\) \\ ConvNorm & \(68.1 1.3\) \\ CMMN & \(\) \\   

Table 2: Balanced accuracy (BACC) for different subject-specific normalizations and CMMN (left). Boxplot for all normalization approaches (right). CMNN outperforms subject-specific normalizations.

   Architecture &  &  \\  & No Adapt & CMMN & No Adapt & CMMN \\  MASS\(\)MASS & \(75.1 1.0\) & \(\) & \(\) & \(73.1 2.6\) \\ Phys.\(\)Phys. & \(69.2 2.7\) & \(\) & \(66.5 2.5\) & \(\) \\ SHHS\(\)SHHS & \(61.2 3.8\) & \(\) & \(58.7 2.3\) & \(\) \\  MASS\(\)Phys. & \(58.4 2.4\) & \(\) & \(50.1 2.4\) & \(\) \\ MASS\(\)SHHS & \(41.8 3.6\) & \(\) & \(38.3 2.6\) & \(\) \\ Phys.\(\)MASS & \(64.0 2.7\) & \(\) & \(59.5 1.0\) & \(\) \\ Phys.\(\)SHHS & \(45.6 2.1\) & \(\) & \(45.2 2.2\) & \(\) \\ SHHS\(\)MASS & \(57.0 2.8\) & \(\) & \(51.2 5.9\) & \(\) \\ SHHS\(\)Phys. & \(55.0 2.7\) & \(\) & \(48.6 5.8\) & \(\) \\  Mean & \(58.6 2.6\) & \(\) & \(54.6 2.9\) & \(\) \\   

Table 3: Balanced accuracy (BACC) for different train/test dataset pairs and for different architectures (Chanbon/DeepSleepNet). CMMN works independently of the network architecture.

than 10% on some dataset pairs. These results show the consistency of the method on all subjects but also the huge impact on the more challenging ones.

### Complementarity of Cmmn with Domain Adaptation

We have shown in the previous experiments that Cmmn is clearly the best normalization in many settings. But the main idea of Cmmn is to adapt the raw signals to a common barycentric domain. Interestingly, many Domain Adaptation (DA) methods also try to reduce the discrepancies between datasets by learning a feature representation that is invariant to the domain. In this section, we compare the two strategies and investigate if they are complementary.

We implement the celebrated DA method DANN that aims at learning a feature representation that is invariant to the domain using an adversarial formulation. Note that this DA approach is much more complex than Cmmn because it requires to have access to the target data during training and a model needs to be trained for each new target domain. The choice of hyperparameters for DA methods is not trivial in the absence of target labels. But since we have access to several target domains, we propose to select the weight parameters for DANN using a validation on 20% of the target subjects . Note that this is not a realistic setting since in real applications the target domains are usually not labeled, yet it is a way to compare the two approaches in a configuration favorable for DA. We focus on cross-dataset adaptation where many shifts are known to exist: different sensors (SHHS vs Physionet/MASS), doctor scoring criteria (SHHS/MASS vs Physionet), or brain activity (SHHS vs MASS vs Physionet).

We report in Table 5 the BACC and \(\)BACC@20 for all dataset pairs and all combinations of CMMN and DANN with Chambon. First, we can see that the best approaches are clearly CMMN and CMMN+DANN. CMMN is better in BACC on 4/6 dataset pairs and CMMN+DANN is better in \(\)BACC@20 on 4/6 dataset pairs. First, it is a very impressive performance for CMMN that is much simpler than DANN and again does not use target data when learning the predictor \(f\). But it also illustrates the interest of CMMN+DANN especially for low-performing subjects.

   Archi & Chambon & DeepSleeDet  \\  MASS\(\)MASS & \(\) & \(1.8 3.8\) \\ MASS\(\)Phys. & \(6.4 4.1\) & \(\) \\ MASS\(\)SHHS & \(7.5 2.8\) & \(11.7 2.0\) \\ Phys.\(\)MASS & \(\) & \(3.6 2.1\) \\ Phys.\(\)Phys. & \(\) & \(\) \\ Phys.\(\)SHHS & \(\) & \(4.5 1.1\) \\ SHHS\(\)Mass & \(\) & \(9.5 6.8\) \\ SHHS\(\)Phys. & \(6.5 3.4\) & \(\) \\ SHHS\(\)SHHS & \(\) & \(3.6 4.0\) \\  Mean & \(\) & \(6.8 3.8\) \\   

Table 4: \(\)BACC@20 for different train/test dataset pairs and for different architectures (Chambon/DeepSleeNet) (left). Scatter plot of balanced accuracy (BACC) with No Adapt as a fuction of BACC with CMMN and the dataset pair SHHS \(\) MASS with Chambon (center) and on the dataset pair MASS \(\) Physionet with DeepSleeNet (right). CMMN leads to a big performance boost on the low-performing subjects.

    &  &  \\ Adapt & Ro Adapt & DANN & CMMN & CMMN-DANN & DANN & CMMN & CMMN \\  MASS\(\)Phys. & \(59.2 6.1\) & \(60.9 1.0\) & \(62.9 0.8\) & \(\) & \(2.0 5.9\) & \(5.3 5.8\) & \(\) \\ MASS\(\)SHHS & \(44.5 6.0\) & \(43.4 2.1\) & \(\) & \(49.9 1.9\) & \(4.2 7.1\) & \(7.6 2.9\) & \(\) \\ Phys.\(\)MASS & \(65.3 1.4\) & \(65.2 0.7\) & \(68.9 1.0\) & \(\) & \(0.4 1.3\) & \(\) & \(5.6 2.7\) \\ Phys.\(\)SHHS & \(43.1 6.3\) & \(45.4 2.6\) & \(\) & \(49.9 2.4\) & \(2.8 5.3\) & \(8.2 5.7\) & \(\) \\ SHHS\(\)Mass & \(59.9 3.4\) & \(59.4 1.1\) & \(\) & \(66.3 0.9\) & \(0.4 2.8\) & \(\) & \(12.6 2.7\) \\ SHHS\(\)Phys. & \(57.1 3.9\) & \(57.2 2.2\) & \(\) & \(59.4 2.6\) & \(4.1 3.7\) & \(10.0 6.9\) & \(\) \\  Mean & \(54.9 4.5\) & \(55.4 1.6\) & \(\) & \(59.6 1.7\) & \(2.3 4.4\) & \(8.5 4.4\) & \(\) \\   

Table 5: Balanced accuracy (BACC) and \(\)BACC@20 for different train/test dataset pairs and for different adaptation methods. CMMN outperforms DANN and CMMN+DANN on average on all subjects. Combining CMMN DANN improves the lower-performing subjects.

### Different PSD targets

CMMN has shown a significant boost in performance for different adaptations in various settings. The method consists of learning a barycenter, and mapping all domains to this barycenter to obtain a homogeneous frequency spectrum. Mapping to the barycenter is one way to achieve this, but it is reasonable to evaluate the optimality of this choice by comparing to alternatives. For example, the classical method called spectral whitening is equivalent to OT mapping towards a uniform PSD, which is equivalent to a white noise PSD. In this part, we propose to compare the mapping to different PSD targets: barycenter (classic CMMN), white noise PSD (whitening), or a non-uniform power-law. The last PSD target is a mathematical distribution that describes a functional relationship between frequency and magnitude, where magnitude is inversely proportional to the power of the frequency \(P(f)=af^{a-1}\). Below we selected \(a=0.659\) for the experiment.

The table Table 6 gives the BAC score of the mapping for the different PSD targets and shows the importance of the reference PSD. First, we can see that mapping to a PSD increases the score significantly w.r.t. to raw data. Second, the mapping to the Wasserstein barycenter (_i.e._, CMMN) is not always the better performer (only 5/9), but overall, CMMN gives better results (2% higher) and with less variance. The robustness of the chosen PSD target, coupled with the fast computation of the barycenter, makes CMMN a strong and easy-to-compute normalization method for time series.

## 5 Conclusion

We proposed in this paper a novel approach for the normalization of bio-signals that can adapt to the spectral specificities of each domain while being a test-time adaptation method that does not require retraining a new model. The method builds on a new closed-form solution for computing Wasserstein barycenters on stationary Gaussian random signals. We showed that this method leads to a systematic performance gain on different configurations of data shift (between subjects, between sessions, and between datasets) and on different architectures. We also show that CMMN benefits greatly the subjects that had bad performances when trained jointly without sacrificing performance on the well-predicted subjects. Finally, we show that CMMN even outperforms DA methods and can be used in conjunction with DA for even better results.

Future work will investigate the use of CMMN for other biomedical applications and study the use of the estimated filters \(_{k}\) as vector representations of the subjects that can be used for interpretability. Finally, we believe that a research direction worth investigating is the federated estimation of CMMN with the objective of learning an unbiased estimator in the context of differential privacy [44; 45].

   Target PSD & None & Barycenter & Powerlaw & Whitening \\  MASS\(\)MASS & \(75.1 1.0\) & \(\) & \(75.6 2.4\) & \(73.2 6.4\) \\ MASS\(\)Physionet & \(58.4 2.4\) & \(62.6 2.0\) & \(63.1 1.5\) & \(\) \\ MASS\(\)SHHS & \(41.8 3.6\) & \(50.4 8.0\) & \(51.9 3.0\) & \(\) \\ Physionet\(\)MASS & \(64.0 2.7\) & \(\) & \(66.7 2.5\) & \(65.9 2.6\) \\ Physionet\(\)Physionet & \(69.2 2.7\) & \(\) & \(66.9 16.5\) & \(71.3 1.9\) \\ Physionet\(\)SHHS & \(45.6 2.1\) & \(52.5 1.9\) & \(53.9 1.7\) & \(\) \\ SHHS\(\)MASS & \(57.0 2.8\) & \(\) & \(59.0 13.9\) & \(59.0 13.8\) \\ SHHS\(\)Physionet & \(55.0 2.7\) & \(\) & \(56.5 13.1\) & \(55.9 12.8\) \\ SHHS\(\)SHHS & \(61.2 3.8\) & \(63.5 3.2\) & \(\) & \(62.4 2.8\) \\  Mean & \(58.6 2.6\) & \(\) & \(61.9 6.4\) & \(61.9 5.1\) \\   

Table 6: BACC for different PSD targets for the mapping.

## 6 Acknowledgement

The authors thank Antoine Collas, Cedric Allain, and Nicolas Courty for their valuable comments on the manuscript, and Lina Dalibard for her help with Figure 1. Numerical computation was enabled by the scientific Python ecosystem: NumPy , SciPy , Matplotlib , Seaborn , PyTorch , and MNE for EEG data processing . This work was partly supported by the grants ANR-20-CHIA-0016 and ANR-20-IADJ-0002 and ANR-23-ERCC-0006-01 from Agence nationale de la recherche (ANR).