# Solving Zero-Sum Markov Games with Continuous State via Spectral Dynamic Embedding

Chenhao Zhou\({}^{1}\)  Zebang Shen\({}^{2}\)  Chao Zhang\({}^{1}\)1  Hanbin Zhao\({}^{1}\)  Hui Qian\({}^{1,3}\)

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)Department of Computer Science, ETH Zurich

\({}^{3}\)State Key Lab of CAD\(\&\)CG, Zhejiang University

{zhouchenhao,zczju,zhaohanbin,qianhui}@zju.edu.cn

zebang.shen@inf.ethz.ch

###### Abstract

In this paper, we propose a provably efficient natural policy gradient algorithm called Spectral Dynamic Embedding Policy Optimization (SDEPO) for two-player zero-sum stochastic Markov games with continuous state space and finite action space. In the policy evaluation procedure of our algorithm, a novel kernel embedding method is employed to construct a finite-dimensional linear approximations to the state-action value function. We explicitly analyze the approximation error in policy evaluation, and show that SDEPO achieves an \((})\) last-iterate convergence to the \(-\)optimal Nash equilibrium, which is independent of the cardinality of the state space. The complexity result matches the best-known results for global convergence of policy gradient algorithms for single agent setting. Moreover, we also propose a practical variant of SDEPO to deal with continuous action space and empirical results demonstrate the practical superiority of the proposed method.

## 1 Introduction

Two-player zero-sum stochastic Markov games (2p0s-MGs) has been the focus of research across a range of research communities. In this problem, two players select their actions based on the current state simultaneously and independently. Player one aims to maximize the return based on the reward provided by the environment, while player two aims to minimize it. For 2p0s-MGs with finite state space, tabular methods (Alacaoglu et al., 2022; Bai and Jin, 2020; Daskalakis et al., 2020; Wei et al., 2021; Zhao et al., 2022) represent the state-action value function with tables, which results in a sample complexity depending on the cardinality of the state spaces.

To deal with 2p0s-MGs with complex state space, researchers recently employ function approximations of the state-action value function to deal with large-scale discrete/continuous state space, including linear function approximations (Xie et al., 2020; Chen et al., 2022), kernel function approximations (Junchi et al., 2022; Qiu et al., 2021) and general function classes such as neural networks (Jin et al., 2022; Huang et al., 2021). Basically, these methods use samples to construct the function approximations and update the policies with value iteration. Theoretical analyses show that these methods possess sample complexities independent of the state space's cardinality. Note that these methods fail to explicitly utilize the dynamics information of the underlying environments and only achieve a sub-optimal sample complexity \((^{-2})\) to find an \(-\)optimal Nash equilibrium for 2p0s-MGs with known system dynamics.

In this paper, we introduce a spectral dynamic embedding method, which explicitly uses the dynamics information to approximate the state-action value functions, and propose an efficient natural-policy-gradient-type algorithm, called Spectral Dynamic Embedding Policy optimization (SDEPO),for 2p0s-MGs. In particular, the spectral dynamic embedding method directly constructs truncated linear approximations to the transition dynamics of a Markov game in a kernel space, and implements dynamic programming to calculate the state-action value function approximation. The superiority of spectral dynamic embedding has been justified in single agent setting (Ren et al., 2022, 2023). We leverage two kernel feature generation methods for the truncated approximation, namely random feature generation and Nystrom feature generation, and analyze the approximation error of these two methods during policy evaluation. Our contributions lie in the following folds.

1. We present a truncated kernel-based linearization method for the state-action value function approximation in two-player zero-sum Markov games with continuous state space. With the random/Nystrom feature generation, this method automatically generates truncated kernel representation from system dynamics, bypassing the difficulty of kernel feature decision existed in other kernel approximation methods. By integrating the acquired kernel features into the temporal difference learning process, we estimate the state-action value functions through the least square policy evaluation. Leveraging the obtained value functions, policy improvement can be achieved through the natural policy gradient descent/ascent approach.
2. We establish rigorous analysis of the approximation error in the truncated value-function approximation and the statistical error induced in policy improvement procedure with finite samples. Our theoretical analysis demonstrates that SDEPO achieves a near-optimal \((})\) last-iterate convergence to the \(\)-optimal Nash equilibrium for 2p0s-MGs with continuous state space and finite action space, where \(\) represents the discounted factor. This complexity result matches the best-known results for the policy gradient algorithm to achieve global convergence in the single agent setting.

Moreover, we also propose a practical variant of SDEPO to deal with continuous action space and empirical results demonstrate the practical superior performance of the proposed method.

### Related works

RL methods for 2p0s-MGs.There is a large body of literature on MARL for two-player MGs. Alacaoglu et al. (2022); Daskalakis et al. (2020); Zhao et al. (2022) focus on the tabular setting, i.e., the state-action space can be represented by a table with moderate size To address the challenge of continuous state spaces, many researchers have designed algorithms based on function approximation. Xie et al. (2020); Chen et al. (2022) investigated methods based on linear function approximation, using a set of linear features to represent the state transition function and reward function. Jin et al. (2022); Huang et al. (2021) employed general function approximation for MGs with low multi-agent Bellman eluder dimension and MGs with a finite minimax Eluder dimension. Although

    & State &  & Iteration & Last-iterate & Horizon \\  & Space & & Complexity & Convergence & Length \\   &  & **Alacaoglu et al. (2022)** & \((})\) & Yes & Infinite \\   & & **Zhao et al. (2022)** & \((})\) & Yes & Infinite \\   & & **Xie et al. (2020)** & \((H^{2}}{^{2}})\) & No & Finite \\   & & **Chen et al. (2022)** & \((}{^{2}})\) & No & Finite \\   & & **Jin et al. (2022)** & \((}{^{2}})\) & No & Finite \\   & & **Huang et al. (2021)** & \((}{^{2}})\) & No & Finite \\   & & **Qiu et al. (2021)** & \((}{^{2}})\) & No & Finite \\   & & **Junchi Li et al. (2022)** & \((}{^{2}})\) & Yes & Finite \\   &  & **Cen et al. (2021)** & \((})\) & Yes & Finite \\   & & **Wei et al. (2021)** & \((|^{3}}{(1-)^{9}^{2}})\) & Yes & Infinite \\   & & **Zhang et al. (2022)** & \((}{})\) & Yes & Finite \\    & & Infinite & **This Work** & \((})\) & **Yes** & Infinite \\   

Table 1: Comparison of policy optimization methods for finding an \(\)-optimal NE of two-player zero-sum episodic Markov games in terms of the duality gap. Here, \(H\) refers to the horizon length and \(d\) is the dimension of their features. For simplicity, we ignore the problem-dependent constant.

these algorithms yield strong theoretical guarantees, they are computationally inefficient. Junchi Li et al. (2022); Qiu et al. (2021) studied learning MGs with kernel approximation. Their approach's assume that there are a set of (possibly infinite-dimensional) kernel features that span the transition function or value function space. However, finding a good set of kernel features is a very challenging task, making their assumption difficult to satisfy. Additionally, infinite-dimensional kernel features are infeasible to compute, so their method requires finite-dimensional approximation, and the errors caused by finite-dimensional approximation currently lack analysis.

Tabular methods for 2p0s-MGs where system dynamics are known.As shown in Table 1, a parallel line of research aims to solve 2p0s-MGs with system dynamics known. For the infinite-horizon discounted setting, Wei et al. (2021) proposed an optimistic gradient descent ascent (OGDA) method which achieves a last-iterate convergence at an \(}{(1-)^{9}e^{2}}\) iteration complexity. Cen et al. (2021) established linear last-iterate convergence of entropy-regularized OMWU. For the finite-horizon episodic setting, Zhang et al. (2022) showed that the modified optimistic Follow-The-Regularized-Leader method finds an \(\)-optimal NE in \(}{}\) iterations and Cen et al. (2023) proposed a single-loop policy optimization algorithm that implies the last-iterate convergence with an iteration complexity of \(}{}\). However, their methods are all confined to tabular setting, whereas our method can handle problems in continuous state spaces.

RL with Function Approximation.Function approximation in single-agent RL has been extensively studied in recent years to achieve a better sample complexity that depends on the complexity of function approximators rather than the size of the state/action space. One line of work studies RL with linear function approximation (Yang and Wang, 2019; Jin et al., 2020). Typically, these methods assume the optimal value function can be well approximated by linear functions, and achieve polynomial sample efficiency guarantees related to feature dimension under certain regularity conditions. Another line of works studied the MDPs with general nonlinear function approximations (Jiang et al., 2017; Jin et al., 2021). Jiang et al. (2017); Jin et al. (2021) present algorithms with PAC guarantees for problems with low Bellman rank and low BE dimension, respectively. We note that MGs are inherently more complex than MDPs due to their min-max nature and it is generally difficult to directly extend these results to the dual-player dynamic setting of MGs.

## 2 Background and Preliminaries

In this section, we present the necessary definitions that will be adopted throughout the paper. In Section 2.1, we formally describe the setup for two-player zero-sum stochastic Markov games with simultaneous moves. In Section 2.2, We briefly introduce the background knowledge about positive definite kernels and their decompositions.

### Simultaneous-Move Markov Games

A two-player zero-sum stochastic Markov games with simultaneous moves is defined by the tuple \((,_{1},_{2},r,,)\), where \(\) is the state space, \(_{i}\) is the set of actions that player \(i\{1,2\}\) can take, \(r\) is the reward function, \(\) is the transition function and \([0,1)\) is the discounted factor. At each step \(t\), given the state \(s\), \(1\) and \(2\) take actions \(a_{1}\) and \(b_{2}\), respectively, and then both receive the reward \(r(s,a,b)\). The system then shifts to a new state \(s^{}(|s,a,b)\) according to the transition kernel. Throughout this paper, we assume for simplicity that \(=^{d}\), \(_{1}=_{2}=\) and that the rewards \(r(s,a,b)\) are deterministic functions of the tuple \((s,a,b)\) taking value in \([-1,1]\); generalization to the setting with \(_{1}_{2}\) and stochastic rewards is straightforward.

Denote by \(()\) the probability simplex over the action space \(\). A stochastic policy of \(1\) is a sequence of functions \(:=(_{t}:)_{t}\). At each step \(t\) and state \(s\), \(1\) takes an action sampled from the distribution \(_{t}(s)\) over \(\). Similarly, a stochastic policy of \(2\) is given by the sequence \(:=(_{t}:)_{t}\).

For a fixed pair of policies \((,)\) for both players, the value and Q (a.k.a. action-value) functions for the above game can be defined as following:

\[V^{,}(s): =_{t=0}^{}^{t}r(s_{t},a_{t},b_ {t})|s_{0}=s,\] \[Q^{,}(s,a,b): =_{t=0}^{}^{t}r(s_{t},a_{t},b_ {t})|s_{0}=s,a_{0}=a,b_{0}=b,\]

where the expectation is over \(a_{t}_{t}(|s_{t})\), \(b_{t}_{t}(|s_{t})\) and \(s_{t+1}(|s_{t},a_{t},b_{t})\).

In the zero-sum setting, for a given initial state \(s_{0}\), \(\) seeks to maximize \(V^{,}(s_{0})\) whereas \(\) aims to minimize it. Accordingly, we introduce the value and \(\) functions when \(\) plays the best response to a fixed policy \(\) of \(\): \(V^{*,}(s)=_{}V^{,}(s)\) and \(Q^{*,}(s,a,b)=_{}Q^{,}(s,a,b)\). Similarly, when \(\) plays the best response to \(\)'s policy \(\), we define \(V^{,*}(s)=_{}V^{,}(s)\) and \(Q^{,*}(s,a,b)=_{}Q^{,}(s,a,b)\).

A Nash Equilibrium (NE) of the game is a pair of stochastic policies \((^{*},^{*})\) that are the best response to each other, which we write as \(V^{^{*},^{*}}(s)=V^{*,^{*}}(s)=V ^{^{*},*}(s)\). NE always exists for discounted two-player zero-sum Markov Games (Filar and Vrieze, 2012). Correspondingly, let \(V^{*}(s):=V^{^{*},^{*}}(s)\) and \(Q^{*}(s,a,b):=Q^{^{*},^{*}}(s,a,b)\) denote the values of the NE. In practice, we always seek to find an \(-\)optimal Nash equilibrium, which is a pair of stochastic policies \((,)\) that satisfies \(_{s_{0}}_{^{},^{ }}V^{,}(s)-V^{,^{ }}(s)\). for a initial state distribution \(_{0}\).

We are interested in finding a one-sided \(-\)optimal Nash equilibrium, similar to Alacaoglu et al. (2022); Zhao et al. (2022); Daskalakis et al. (2020); Zhang et al. (2019). In particular, for the initial state distribution \(_{0}\), we seek \(_{out}\) such that \(_{s_{0}}_{}V^{,_{out}}(s)-V^{*}(s)\).

We assume that the transition function satisfies the following assumption.

**Assumption 1**.: _For each \((s_{t},a_{t},b_{t})\), we assume that_

\[s_{t+1}=f(s_{t},a_{t},b_{t})+_{t},_{t} (0,^{2}I_{d}).\]

_The function \(f:\) describes the general dynamics and \(\{_{t}\}_{t=0}^{}\) are independent Gaussian noises. In other words,_

\[(s_{t+1}|s_{t},a_{t},b_{t})-,a_{t},b_{t})-s_{t+1}\|_{2}^{2}}{2^{2}}\] (1)

### Positive definite kernels and two decompositions

To efficiently represent the continuous state space of 2p0s-MGs which can not be handled by traditional tabular methods, it is common to embed the continuous state space into a kernel space. A widely used kernel is the positive definite (PD) kernel.

**Definition 1** ((Positive-Definite) Kernel (Mohri, 2018)).: _A symmetric function \(k:\) is said to be a positive definite kernel if for any \(\{x_{1},,x_{m}\}\), the matrix \(=[k(x_{i},x_{j})]_{ij}^{m m}\) is symmetric positive-definite._

PD kernels admit many decompositions, such as Bochner decomposition (Devinatz, 1953); Mercer decomposition (Mercer, 1909), Canonical decomposition (Stochel, 1992) and Kolmogorov decomposition (Ghaemi et al., 2021). Among these decompositions, Bochner decomposition (Devinatz, 1953) and Mercer decomposition (Mercer, 1909) have recently draw significant attention since they lead to efficient, low-dimensional approximations of the kernel, reducing computational complexity (Liu et al., 2021).

**Definition 2** (Bochner decomposition (Rudin, 2017) and Mercer decomposition (Mercer, 1909)).: _Let \(^{d}\) be a compact domain, \(\) a strictly positive Borel measure on \(\), and \(k(x,x^{})=G(x-x^{})\) a bounded continuous shift-invariant positive definite kernel. Then, \(k(x,x^{})\) admits **Bochner decomposition**, i.e. there exists a non-negative measure \(\), such that_

\[G(x-x^{})=_{^{d}}p()(i^{}(x-x^{ })),\] (2)

_and **Mercer decomposition**, i.e. there exists a countable orthonormal basis \(\{e_{i}\}_{i=1}^{}\) of \(_{2}()\) with corresponding eigenvalues \(\{_{i}\}_{i=1}^{}\),2 such that_

\[k(x,x^{})=_{i=1}^{}_{i}e_{i}(x)e_{i}(x^{}),\] (3)

_where the convergence is absolute and uniform for all \((x,x^{})\). Without loss of generality, we assume \(_{1}_{2}>0\)._It can be verified that the Gaussian kernel, \(k(x,x^{})=(-\|^{2}}{2^{2}})\), meets the conditions in the above definition, and it admits both a Bochner decomposition and a Mercer decomposition (Ren et al., 2023). Note that according to the Bochner/Mercer decomposition, the kernel can be represented with infinite basis. Practically, one can construct finite-dimensional approximations to the positive-definite kernel based on the Bochner/Mercer decompositions with random/Nystrom features (Rahimi and Recht, 2008; Williams and Seeger, 2000), respectively. Recently, Ren et al. (2023) utilize a finite-dimensional approximation to represent the environment in stochastic nonlinear control problems and showed its superior performance, motivating our work.

## 3 Policy Optimization with Spectral Dynamic Embedding

In this section, we begin by introducing spectral dynamic embedding to represent the environment of the Markov game. This approach allows us to express the \(Q\)-function for any policy pair with infinite dimensional features. Subsequently, we develop finite-dimensional approximated features for computational tractability. With these features, each player conducts least square policy evaluation to estimate the \(Q\)-function of current policy pair based on the generated features and then improve the policy by natural policy gradient based on the estimated \(Q\)-function. This leads to _Spectral Dynamic Embedding Policy Optimization (SDEP0)_ in Algorithm 3.

### Spectral Dynamics Embedding

By interpreting the state transition function (1) as a Gaussian kernel, we can decompose the transition function of a Markov game using Bochner decomposition and Mercer decomposition, as detailed below.

**Lemma 1** (Spectral Dynamic Embedding).: _Consider any \([0,1)\). Denote \(k_{}(x,x^{})=(-)\|x-x^{}\|^{2} }{2^{2}})\) for any \(0<1\). We can decompose \(k_{}(x,x^{})\) using Bochner decomposition and Mercer decomposition._

_Let_

\[_{}(s,a,b) =(f(s,a,b))}{^{d}}[(f(s,a,b)}{}}),(f(s,a,b)}{}})],\] \[_{}(s^{}) =p_{}(s^{})[(}^{}s^{ }),(}^{}s^{})]^{},\]

_where \(g_{}(f(s,a,b)):=(\|f(s,a,b)\|^{2}}{ 2(1-^{2})^{2}})\), \((0,^{-2}I_{d})\), and \(p_{}(s^{})=}{(2^{2})^{d/2}}(- \|^{2}}{2^{2}})\) is a Gaussian distribution for \(s^{}\) with standard deviation \(\)._

_Using Bochner decomposition, we have_

\[P(s^{}|s,a,b)= _{(0,^{-2}I_{d})}[ _{}(s,a,b)^{}_{}(s^{})]:=_{ }(s,a,b),_{}(s^{})_{(0,^ {-2}I_{d})}.\] (4)

_Let \(\) be a strictly positive Borel measure on \(\). By **Mercer's theorem**, \(k_{}\) admits a decomposition for any \((x,x^{})\)(Fasshauer, 2011):_

\[k_{}(x,x^{})= _{i=1}^{}_{,i}e_{,i}(x)e_{,i}(x^ {}),\ \{e_{,i}\}_{2}().\] (5)

_We denote \(k_{}(x,x^{})=_{}(x),_{ }(x^{})_{_{2}}\) where \(_{,i}(x)=}e_{,i}(x)\) for each positive integer \(i\)._

\[_{M}(s,a,b)=(f(s,a,b))}{^{d}}_{ }(})^{},_{M}(s^{})= p_{}(s^{})_{}(s^{})^{}\]

_Then,_

\[P(s^{}|s,a,b)=_{M}(s,a,b),_{M}(s^{})_{_{ 2}}.\] (6)

The tunable parameter \(\) offers advantages for theoretical analysis and can also be leveraged to enhance empirical performance.

Let \(_{}()=[_{}(),r()]\) and \(_{M}()=[_{M}(),r()]\). The \(_{}()\) and \(_{M}()\) are named as Bochner Spectral Dynamic Embedding and Mercer Spectral Dynamic Embedding, respectively. These embeddings form infinite-dimensional bases of the \(Q\)-function for arbitrary policy pairs, allowing for a linear representation of the \(Q\)-function.

**Lemma 2**.: _For any policy pair, there exist weights \(\{_{}^{,}\}\) (where \((0,^{-2}I_{d})\)) and \(_{M}^{,}_{2}\) such that the corresponding value function satisfies_

\[Q^{,}(s,a,b)=\ _{}(s,a,b), _{}^{,}_{(0, ^{-2}I_{d})}=\ _{M}(s,a,b),_{M}^{, }_{_{2}}\]

We provide the proof of this section in Appendix A.

### Finite-dimensional truncated Embedding

Basically, a desirable feature embedding method should provide a good approximation to the underlying dynamics of MGs with a modest feature dimension. However, the dimension of both Bochner and Mercer spectral dynamic embedding is _infinite_, which is computationally intractable, motivating us to investigate the finite-dimensional embeddings.

We construct finite-dimensional truncations of Bochner and Mercer embedding using random feature [Ren et al., 2022] and Nystrom feature [Williams and Seeger, 2000], respectively, to provide efficient finite linear approximations to represent the transition kernel. As show in Algorithm 1 and Algorithm 2, random feature is the Monte-Carlo approximation for the Bochner embedding and Nystrom feature approximates the subspace spanned by the top eigenfunctions of the Mercer embedding via eigendecomposition of an empirical Gram matrix. A detailed derivation of the Nystrom method is provided in Appendix B. We analyze the approximation error due to using the finite-dimensional basis in Appendix D. The finite-dimensional embedding is a crucial component of our algorithm, Spectral Dynamic Embedding Policy Optimization (SDEP0), which is given as Algorithm 3.

``` Data: Transition Model \(s^{}=f(s,a,b)+\) where \((0,^{2}I_{d})\), Reward Function \(r(s,a,b)\), Number of Random/Nystrom Feature \(m\), Number of Nystrom Samples \(n_{} m\), Nystrom Sampling Distribution \(_{}\)
1 Sample \(n_{}\) random samples \(\{x_{1},,x_{}\}\) independently from \(\), following the distribution \(_{}\). Construct \(n_{}\)-by-\(n_{}\) Gram matrix given by \(K_{i,j}^{(n_{})}=k_{}(x_{i},x_{j})\). Perform eigendecomposition on the Gram matrix \(K^{(n_{})}U= U\), with \(_{1}_{n_{}}\) denoting the corresponding eigenvalues. Construct the feature \[_{}(s,a,b)\!\!=\!\![\!(\!f(s,a,b)\!)}{^ {d}}}\!\!_{=1}^{n_{}}\!\!\!U_{i,}k_{ }(x_{},})\!\!]_{i [m]},\] (8) set \((s,a,b)=[_{}(s,a,b),r(s,a,b)]\). ```

**Algorithm 2** Nystrom Features Generation

### Policy Optimization with Finite-dimensional Embedding

After generating the finite-dimensional embedding, SDEP0 is divided in two stages. It finds an approximate solution \(_{k}\) in Stage 1, which is then utilized in Stage 2 to derive an approximate solution \(_{k}\). In each stage, there are two main components: policy evaluation and policy improvement. Least square policy evaluation is conducted for estimating the state-action value function of current policy pair upon the generated finite-dimensional truncation features \(Q^{_{i},_{k}}(s,a,b)=(s,a,b)^{}w^{ {}_{i},_{k}}\). In the policy improvement procedure, based on the approximated value function, the natural policy gradient method is used to adjust the policy of each player iteratively in an alternating fashion.

\(_{t+1}\) is updated as

\[_{t+1}(|s)=_{(|s)()}\ (|s),_{b_{t}(|s)}[(s, ,b)]^{}w^{_{t},_{t}}+KL((|s)||_{t}(|s))\]

yielding the following closed-form solution,

\[_{t+1}(a|s)_{t}(a|s)(_{b _{t}(|s)}[(s,a,b)]^{} w^{ _{t},_{t}})=(_{b _{t}(|s)}[(s,a,b)]^{}_ {t+1})\]

where \(_{t+1}=_{i=0}^{t} w^{_{i},_{t}}\) is updated by similar update rule.

Note that in SDEP0, each player is required to have knowledge of their opponent's strategy, which is crucial for effective policy optimization and is a common requirement in numerous existing algorithms for Markov games. (Xie et al., 2020; Chen et al., 2022; Junchi Li et al., 2022; Alacaoglu et al., 2022)Theoretical Results

The major difficulty in analyzing the convergence of SDEPO is that the policies of both players evolve at each iteration, leading to non-stationary in the environment for each player. Additionally, there exists a certain level of estimation error in the transition and Q-function for each player. As a consequence, the vanilla proof strategy of convergence of policy optimization used in majority of the literature is no longer applicable.

In this section, we provide rigorous investigation of the impact of the approximation error for policy evaluation and derive the convergence of policy optimization. We first specify the commonly used assumptions (Yu and Bertsekas, 2008; Jin et al., 2020; Agarwal et al., 2021; Abbasi-Yadkori et al., 2019; Ren et al., 2023), under which we derive our theoretical results below.

**Assumption 2** (Regularity Condition for Dynamics).: _For the dynamic function \(f\), there exists a constant \(c_{f}\), such that \(\|f(s,a,b)\| c_{f}\) for all \(s,a,b\)._

**Assumption 3** (Regularity Condition for Stationary Distribution).: _The stationary distribution \(_{,}\) for all policy pair \((,)\) has full support, and satisfies the following conditions with \(_{1},_{2}>0\):_

\[_{}(_{_{,}}[(s,a,b)(s,a,b)^{}])_{1},\] \[_{}(_{_{,}}[ (s,a,b)((s,a,b)-_{_{,}}( s^{},a^{},b^{}))^{}])_{2}.\]

**Assumption 4** (Regularity Condition for Feature).: _The features \(\{_{}(s,a,b)\}_{(s,a,b) }\) and \(\{_{}(s,a,b)\}_{(s,a,b) }\) are linearly independent._

First, we have the following bound on the error for policy evaluation. For convenience, we focus solely on the error for least square policy evaluation in Stage 1. A similar analysis can be conducted for policy evaluation error in Stage 2. We decompose the error for policy evaluation into two parts, one is the approximation error due to the finite number of features, and one is the statistical error due to the finite number of samples. Combine the approximation error and the statistical error, we have the following bound on the error for policy evaluation. Detailed proof can be found in Appendix D.

**Theorem 1**.: _Let \(L=( n)\). Denote \(_{}:=_{s,a,b}(f(s,a,b))}{^{d}}\) and \(_{,L}^{,}=^{}_{L}\). With probability at least \(1-\), we have that for the random features,_

\[\|Q^{,}-_{,L}^{ ,}\|_{_{, }}=(_{}}{(1\!-\!)^{2}}+_{}^{6}m^{3}}{(1\!-\!)_{1}^{2}_{2}}),\] (13)

_and for the Nystron features,_

\[\|Q^{,}-_{_{},L}^{,}\|_{_{, }}=(_{}}{(1-)^{2}n_{ }}+_{}^{6}m^{3}}{(1-)_{1}^{2} _{2}}).\] (14)

As shown in Theorem D.1, Nystron method improves the approximation error from \(O(m^{-1})\) to \(O(n_{nys}^{-1})\) with a mild assumption to make in the kernel literature (cf. Theorem 2 in (Belkin, 2018)). Then, we provide the error analysis for policy optimization. We remark that in the following proof we assume the action space \(||\) is finite for simplicity. The following assumption assumes that the selection probability of each action is positive under each state, which is a commonly used assumption in the analysis of policy gradient type methods (Alacaoglu et al., 2022; Lan, 2023).4

**Assumption 5**.: _There exists a constant \(\) such that, for any policy iterate pair \(_{k},_{k}\), for any state action tuple \(s,a,b\), it holds that \(_{k}(a|s)>0,_{k}(b|s) >0\)._

We now present the result for policy optimization. We use Perolat et al. (2015)'s error propagation framework, which needs the results in each stage. The detailed proof of it can be found in Appendix E. First, we analyze the error in the Stage 1 of Algorithm 3.

**Lemma 3**.: _Denote \(Q_{k-1}=Q^{_{k-1},_{k-1}}\), \(^{s}Q^{s}^{s}=_{a( |s),b(|s)}[Q(s,a,b)]\). Let Assumption 5 hold. In Stage 1 of Algorithm 3,_

\[[_{}Q_{k-1} _{k}-_{}_{k}Q_{k-1}]\] \[ }+2\|^{ _{k-1},_{k-1}}-Q^{_{k-1},_{k-1}}\|_{_{_{k-1},_{k-1}}}+}.\]This result shows how the error in the approximation of the \(Q\)-function and the learning rate \(\) impact the optimization performance in Stage 1. Next, we turn to Stage 2 and provide an upper bound on the one-sided error.

**Lemma 4**.: _Let Assumption 5 hold and \(_{0}\) be a state distribution. In Stage 2 of Algorithm 3,_

\[_{t=1}_{s_{0}}[V^{^{}_{k},_{k}}(s)-V^{^{}_{k,t}, _{k}}(s)]\] \[ (}{ T}+}+\|^{^{}_{k,t}, {}_{k}}-Q^{^{}_{k,t},_{k}}\|_{^{ ^{}_{k,t},_{k}}}).\]

Building on the results from Stage 1 and Stage 2, along with the policy evaluation error, and using the error propagation framework from Perolat et al. (2015), we can now derive the overall convergence result for the policy optimization process. Specifically, we obtain the following proposition, which quantifies the iteration complexity required to reach a near-optimal solution:

**Proposition 1**.: _Let \(L=( n)\) and \(T=(}+_{ }}{(1-)^{2}}+_{}^{}m^{3}}{(1- )^{2}^{2}})}\) for random features and \(T=(}+_{ }}{(1-)^{2}n_{}}+_{}^{}m^{3}}{ (1-)^{2}_{1}^{2}})}\) for Nystrom features._

_Iteration complexity to get \(E_{s_{0}}[_{}V^{,_{ k}}(s)-V^{*}(s)]\) is \((})\)._

Proposition 1 shows the iteration complexity to one-sided NE and it can be directly extended to establish a two-sided NE by applying the algorithm with the roles switched (Zhao et al., 2022).

## 5 The Practical variant of SDEPO

Many practical 2p0s-MGs not only have continuous state spaces but also continuous action spaces, such as such as real-time strategy games (Vinyals et al., 2019; Berner et al., 2019) and robust policy optimization (Pinto et al., 2017). Note that the natural policy gradient update (31) and (12) in SDEPO involve the expectation w.r.t. the action, which is generally intractable for continuous action space. Hence, we propose a practical variant of SDEPO, named SDEPO-NN, to deal with continuous (or large-scale discrete ) action space. SDEPO-NN utilizes neural networks in policy \(\) and the state-action value function approximation \(Q\), detailed in Algorithm 1.

Based on the spectral dynamic embedding \(\), we parameterize the \(\) function of player one as \(_{}(s,a,b)=r(s,a,b)+(s,a,b)^{}\) and parameterize the \(\) function of player two as \(_{}(s,a,b)=r(s,a,b)+(s,a,b)^{}\), and train \(_{}\) and \(_{}\) by minimizing the soft Bellman residual.

We restrict the players' policies \(_{}\) and \(_{}\) to Gaussians with the reparametrization trick, i.e., \(a_{t}=f_{}(_{t};s_{t})\) and \(b_{t}=f_{}(^{}_{t};s_{t})\) where \(_{t}\) and \(^{}_{t}\) are input noise vectors, sampled from a Gaussian. The policy parameters can be learned by minimizing

\[J()=_{s_{t},_{t},^{ }_{t}}[_{}(f_{ }(_{t};s_{t})|s_{t})-_{}(s_{t },_{}(f_{}(_{t};s_{t})| s_{t}),_{}(f_{}(^{}_{t};s_{t})| s_{t}))],\]

and

\[J()=_{s_{t},_{t},^{ }_{t}}[_{}(f_{ }(^{}_{t};s_{t})|s_{t})-_{ {}}(s_{t},_{}(f_{}( _{t};s_{t})|s_{t}),_{}(f_{}( ^{}_{t};s_{t})|s_{t}))],\]

where \(\) is the replay buffer, and \(_{}\) and \(_{}\) are defined implicitly in terms of \(f_{}\) and \(f_{}\), respectively.

Note that tabular methods (Alacaoglu et al., 2022; Bai and Jin, 2020; Daskalakis et al., 2020; Wei et al., 2021; Zhao et al., 2022) can discretize the state/action spaces to handle applications with continuous state/action spaces. However, directly applying such methods often incurs the curse of dimension and requires an excessive amount of computation resources even for a small size problem. On the other hand, although there exist theoretical-guaranteed methods for 2p0s-MGs with continuous state and action space, they all involve a computational intractable subroutine, i.e., Qiu et al. (2021); Junchi Li et al. (2022) need to solve a difficult 'find_ne'/'find_cce' subroutine, and Jin et al. (2022); Huang et al. (2021) have to tackle a comprehensive constrained optimization problem.

## 6 Numerical verification

In this section, we present two experiments to evaluate our methods. The first experiment focuses on a simple zero-sum Markov game featuring a continuous state space and a finite action space, aiming to validate the convergence of SDEP0. The second experiment adapts a multi-agent scenario inspired by the simple push (Lowe et al., 2017), where both the state and action spaces are continuous, to assess the effectiveness of SDEP0-NN.

In the first experiment, we designed a simple zero-sum Markov game with a continuous state and finite action space (\(=\), \(\)] = 5). The state space is partitioned into 42 distinct intervals: one interval for \((-,-10)\), 40 intervals evenly spaced by 0.5 units in the range \([-10,10)\), and one interval for \((10,)\). In the \(i\)-th interval, the transition dynamics are defined by \(P(s,a,b)=f(s,a,b)+\), where \((0,1)\), and \(f(s,a,b)=_{i,a,b}\), with \(_{i,a,b}(-10.5,10.5)\). The reward function is \(r(s,a,b)=^{}_{i,a,b}\), where \(^{}_{i,a,b}(-1,1)\). The initial state distribution is assumed to be uniform over \([-10.5,10.5]\).

We ran SDEP0 for 120 iterations, and measured the convergence of \(\) by metrics in Proposition 1. As shown in Figure 1, SDEP0 with random features and Nystrom features both converge after 60 iterations. We discretized the state space of this environment and compared it with OFTRL (Zhang et al., 2022), a tabular method where the environment is known. We adopted the parameter settings recommended in (Zhang et al., 2022) and adjusted the environment to a 100-horizon setting. As shown in Figure 1, our method demonstrated superior convergence in this environment. This likely stems from the fact that OFTRL operates on the discretized state space, whereas our method computes on the original state space.

Next, we conduct experiments on an adapted version of simple push (Lowe et al., 2017), wherein both the state and action spaces are continuous. This problem consists of two agents and one landmark. Each agent receives a reward for proximity to the landmark while ensuring the other agent remains distant. Thus, agents must learn to stay close to the landmark and simultaneously push the other agent away. At each time step, a noise \((0,^{2}I_{d})\) is added to the state.

We implemented SDEP0-NN with random features (SDEP0-NN\({}_{rf}\)) and Nystrom features (SDEP0-NN\({}_{nys}\)), comparing them against methods where Q functions do not utilize spectral dynamical embedding (NPG-NN). Table 2 shows the results of winning rate after training by 20000 iterations with varying noise levels. It is evident that SDEP0-NN\({}_{rf}\) and SDEP0-NN\({}_{nys}\) largely outperforms NPG-NN, which shows the effectiveness of the spectral dynamical embedding.

## 7 Conclusion

In this paper, we propose a provably efficient natural policy gradient algorithm for two-player zero-sum stochastic Markov games with continuous state. We analyze the approximation error and convergence of the algorithm. To deal with continuous action spaces, a practical variant is provided and demonstrates superior performance. A possible direction is to extend our methods to independent learning setting.

  Winning rate & NPG-NN & SDEP0-NN\({}_{rf}\) & SDEP0-NN\({}_{nys}\) \\  NPG-NN & \(49.69\%/47.7\%\) & \(5.23\%/0.55\%\) & \(5.16\%/0.59\%\) \\  SDEP0-NN\({}_{rf}\) & \(95.13\%/99.37\%\) & \(50.35\%/49.95\%\) & \(49.74\%/49.94\%\) \\  SDEP0-NN\({}_{nys}\) & \(95.38\%/99.38\%\) & \(49.69\%/49.95\%\) & \(50\%/49.84\%\) \\  

Table 2: Comparison of winning rates between NPG-NN, SDEP0-NN\({}_{rf}\), and SDEP0-NN\({}_{nys}\) in Simple Push with \(=0.1/0.01\). The results before and after / correspond to \(=0.1\) and \(0.01\), respectively.

Figure 1: Performance illustration of SDEP0 and OFTRL for solving the random generated Markov game.