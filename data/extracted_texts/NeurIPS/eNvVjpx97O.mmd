# StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses

Jia-Nan Li\({}^{1}\)1  Quan Tu\({}^{1}\)1  Cunli Mao\({}^{2}\)  Zhengtao Yu\({}^{2}\)2  Ji-Rong Wen\({}^{1}\)  Rui Yan\({}^{1}\)2

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Kunming University of Science and Technology

{lijianan, quantu, jrwen, ruiyan}@ruc.edu.cn

maocunli@163.com, ztyu@hotmail.com

###### Abstract

Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of _End-of-Utterance_ (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as "conversational attention sinks" (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200K or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200K of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 \(\) speedup while reducing memory usage by 18 \(\) compared to dense attention recomputation.4

## 1 Introduction

Large Language Models (LLMs)  are rapidly advancing. However, their performance is constrained by context size during pre-training. For example, with a context size of 4,096, the inference capability of LLAMA2  sharply drops when the context length exceeds the preset limit. Moreover, the attention mechanism  leads to quadratic growth in computational complexity with

Figure 1: Attention map visualization. (a) Llama-2-7B/Chat with “-/s>” and “\(\)” as EoU (“-/s>” counts as one token, “\(\)” as two). (b) StreamingLLM versus StreamingDialogue attention on Llama-2-7B with “-/s>” as EoU.

text length, increasing GPU memory usage and slowing generation speed. As LLMs find widespread use in various conversational applications , these limitations become particularly severe for dialogue tasks , rendering standard LLMs infeasible for supporting prolonged dialogues with long conversation histories.

In order to support conversations with long contexts, a natural solution is to reduce the computation of inter-token correlations by modifying the implementation of attention. Beltagy et al.  proposed local attention, which confines attention within a \(k\) window size, reducing computational complexity to linear. However, when the text length exceeds \(k\), the generation performance substantially declines. StreamingLLM  enhanced long context streaming by introducing the concept of "attention sinks." This approach builds on local attention, allowing initial tokens to be consistently attended to, which supports stable long-term interactions and efficient generation. However, StreamingLLM continuously updates the cached information within the fixed-size window, excluding initial tokens. These initial tokens are blind to subsequent tokens during the auto-regressive generation process. Consequently, as the dialogue context lengthens, historical information is progressively lost, which is detrimental to dialogue consistency and severely impacts the user experience.

We introduce StreamingDialogue, a method designed for efficient conversations with enhanced long-term memory capabilities. In dialogue contexts, we observe an interesting phenomenon: tokens used to separate utterances (namely End-of-Utterance, EoU), such as "</s>" and "n," tend to aggregate more attention than other tokens (for details, refer to Figure 1 (a) and see SS4.11 for further analysis). We refer to these separator tokens as **"conversational attention sinks"** (conv-attn sinks). Figure 1 (b) demonstrates that, in contrast to the highly dispersed attention pattern of StreamingLLM, StreamingDialogue maintains focus on critical positions like conv-attn sinks, thereby utilizing them to aggregate utterance information, compressing lengthy dialogues to only require caching conv-attn sinks' key-values to improve efficiency and reduce memory consumption.

Specifically, in the long-term generation, we preserve conv-attn sinks to memorize historical dialogues for retrieval. Additionally, caching both the first token and the previous and current utterances is crucial to ensure stable output beyond a certain inference length and to facilitate the smooth generation of consecutive replies. Beyond these measures, we introduce two self-learning strategies to better characterize the conv-attn sinks: (1) we devise a reconstruction task, where the reconstruction process can only attend to the conv-attn sink of the target utterance, thereby encouraging the conv-attn sink to restore information from the target sentence, namely **short-memory reconstruction** (SMR); (2) we propose a recall task, treating the final utterance as a query and attending solely to conv-attn sinks in the dialogue history to retrieve the matching response, thus prompting the model to reactivate information from lengthy dialogues, named as **long-memory reactivation** (LMR). These two tasks will be jointly optimized before dialogue learning.

Experiments on widely-used dialogue datasets demonstrate that our proposed method outperforms other sparse attention and memory-enhancement methods (in terms of evaluation metrics of Perplexity, BLEU, ROUGE, Distinct, USL-H, and Dial-M). In terms of efficiency, our method achieves a 4 \(\) speedup and an 18 \(\) reduction in memory usage compared to dense attention with recomputation. In particular, currently some LLMs support handling long contexts, such as Claude 2.14 with a 200K context window. In this way, leveraging our method with such long context LLMs enables the completion of numerous utterances within the conversation session, which indicates one big step towards prolonged dialogue learning with long contexts. In summary, our main contributions are as follows:

(1) We discover that EoU tokens have the potential to aggregate utterance information. By defining these separator tokens as "conv-attn sinks," we propose StreamingDialogue, which efficiently handles long context by only caching the first token, conv-attn sinks, and tokens from the most recent two utterances.

(2) We propose two learning strategies: short-memory reconstruction (SMR) and long-memory reactivation (LMR), enhancing the capability of conv-attn sinks to aggregate information and the ability to store historical information.

(3) We demonstrate that StreamingDialogue significantly reduces computational complexity experimentally, ensuring the efficiency of streaming conversations.

Related work

StreamingDialogue efficiently handles long context, improving the model's long-term memory for conversation history. Existing methods for processing long context in transformer-based models broadly fall into three categories: efficient transformer design, long-term memory enhancement, and length extrapolation techniques.

### Efficient transformers

Due to attention's computational bottleneck in transformers, some methods aim to explore efficient attention mechanisms. Solutions include trading accuracy for speed, e.g., Longformer  employs sliding window attention, expanding the receptive field with a dilated sliding pattern and optionally integrating global attention. BP-Transformer  balances complexity and capacity with fine-to-coarse attention across multiple scales using binary partitioning. Linformer  approximates self-attention with a low-rank matrix, simplifying operations to linear ones. LongLoRA  uses block-wise attention and token shifting to enhance communication between blocks. Another solution lies in system-level optimizations, e.g., FlashAttention [19; 20] optimizes memory access by perceptually reading and writing, improving efficiency without sacrificing accuracy. However, these methods don't preserve dialogue history or expand the context window sufficiently for prolonged dialogue with long-term memory.

### Long-term memory

Some methods enhance models' long-term memory to improve long-text modeling. One approach is introducing recurrent mechanisms into attention, enabling the model to maintain information over long sequences. For example, Transformer-XL  introduces segment-level recurrence, reusing previous time step hidden states to model long dependencies. \(\)-former  employs continuous-space attention for arbitrary context modeling with fixed computational cost. Another approach is utilizing existing models as interfaces to external knowledge bases, enhancing contextual input and long-term memory through reading and writing to these bases during inference , e.g., MemGPT  employs hierarchical memory for LLMs, optimizing information transfer between context windows and external storage. However, they require retraining LLMs from scratch or additional information retrieval, lacking efficiency.

### Length extrapolation

Length extrapolation in models refers to their ability to maintain good performance beyond the training length during inference. A mainstream solution is based on position encoding. LLMs [25; 26; 27] employ rotary position embedding (RoPE) [28; 29; 30; 31] for length extrapolation without fine-tuning. Initially introduced by Chen et al. , position interpolation proportionally extends the inference length by reducing rotation angles. NTK-aware5 and NTK-by-parts6 interpolations balance high and low-frequency information to optimize performance. YaRN  combines NTK-by-parts interpolation with an attention distribution correction strategy, reducing rotation angles for low frequencies and adjusting attention distribution. Additionally, randomized position encoding  extends context exposure by decoupling pre-training length from inference length, utilizing random positions during training for broader context coverage. Due to current methods' inability for infinite length extrapolation, they're unsuitable for prolonged dialogue in streaming applications.

## 3 StreamingDialogue

### Empirical observation

StreamingLLM  focuses on initial tokens as attention sinks, i.e., initial tokens attract a significant amount of attention. After visualizing the attention maps of all layers and heads for both LLama-2-7B and LLama-2-7B-Chat, we observe a similar phenomenon in structured texts such as multi-turndialogue, where LLMs tend to attend more to tokens used to separate dialogue when speakers switch (i.e., the end symbol "</s>," newline symbol "n," or other symbols, known as End-of-Utterance), and their attention aggregation is even greater than that of initial tokens (shown in Figure 1). Based on the attention map, we suggest that each EoU captures the information of the current utterance, and EoUs are visible to subsequent utterances. These EoUs imply the potential for information aggregation, which we refer to as "conversational attention sinks" (conv-attn sinks).

According to the observation, rather than caching entire utterances to retain information as done in dense attention, we cache conv-attn sinks as a replacement. Let \(T\) represent the number of utterances, and \(L\) denote the average length of each utterance. By caching only the corresponding conv-attn sinks, the space complexity reduces from \(O(TL)\) in dense attention to \(O(T)\), and the time complexity from \(O(T^{2}L^{2})\) to \(O(T^{2}L)\). Moreover, given the infrastructure of LLMs based long context modeling, our method is capable of efficiently handling dialogues with prolonged conversational histories. To this end, conv-attn sinks matter because they memorize the context information not only effectively, but also efficiently.

### Framework overview

#### 3.2.1 Fine-tuning LLMs with conv-attn sinks

We compress the content of each utterance into the subsequent conv-attn sink and recall historical information during the dialogue by attending to conv-attn sinks. To achieve this, we adjust the attention pattern \(A\{0,1\}^{N N}\), where \(N\) represents the conversation length and 0 indicates masked attention values, i.e., specifying the specific keys and values that a query can attend to. Each token within an utterance can focus on the first token to uphold stable output during extended conversations, all preceding conv-attn sinks to extract historical context, and tokens from both the previous and current utterances to ensure continuity with the preceding utterance. Formally, we denote an utterance as \(u=d\)</s>, where \(d\) represents the dialogue content and </s> denotes the EoU token, a.k.a., conv-attn sink. Thus, a conversation can be organized as \(D=\)<s>\(u_{1}u_{2}...u_{t}\), where \(t\) is the number of utterances. Attention mask matrix \(A\) is defined as:

\[A_{ij}=1,&j=kl i\;(k),\;0 i<N\\ 1,&1 j i l\\ 1,&j kl\;(k),(-2) l<j i<N\\ 0,&,\]

Figure 2: StreamingDialogue framework. SMR & LMR strategies co-train the model by adjusting attention mechanisms. In supervised learning, the SMR & LMR-trained model is fine-tuned with dialogue datasets. During inference, only specific tokens are cached, with critical historical dialogue information in bold italics for clarity.

where \(l\) denotes the average length of each utterance, \(\) represents a non-negative integer, and \(\) represents the ceiling function. During fine-tuning, all tokens are treated as predicted tokens to participate in the loss calculation.

While this method excels in managing long contexts compared to sparse attention methods like StreamingLLM, it falls short in characterizing short-term memories. To learn towards a more robust model with balanced memory capacity for both long and short memories, we propose two learning strategies to co-train the model and address these issues: short-memory reconstruction (SMR) and long-memory reactivation (LMR). The final version of StreamingDialogue is conducted in three stages, including SMR & LMR, supervised learning, and inference, as illustrated in Figure 2.

#### 3.2.2 Short-memory reconstruction

We propose a learning strategy to guide model behavior, enabling conv-attn sinks to consciously aggregate information through short-memory reconstruction (SMR). We reorganize data formats, modify the attention pattern, and adjust the loss function. More specifically, training samples are organized as \(D=\)\(u_{1}u_{1}^{}u_{2}u_{2}^{} u_{s}u_{s}^{}\), where \(u_{1},u_{2},,u_{s}\) are randomly selected from the original dataset, and \(s\) represents the number of randomly selected utterances. Each "\(uu^{}\)" pair can be regarded as a reconstruction task, where tokens in \(u\) can attend to > and tokens that appear before the token in the current utterance. \(u^{}\) can additionally attend to the conv-attn sink in \(u\). The task objective is to reconstruct \(u\) in \(u^{}\), encouraging the conv-attn sink in \(u\) to aggregate information from \(u\) for utterance reconstruction. The attention pattern \(A\) in SMR is defined as:

\[A_{ij}=1,&j=0\\ 1,&=2k\;(k^{*}),(- 1) l j i<N\\ 1,&=2k+1\;(k),(- 1) l<j i<N\\ 0,&,\]

where \(^{*}\) represents a positive integer.

Since the goal is to reconstruct the contents of \(u_{1}^{},u_{2}^{},,u_{s}^{}\) into \(u_{1},u_{2},,u_{s}\), the loss calculation is defined as:

\[_{}=-_{(x,y)}_{t=1}^{|y|}( P_{}\;(y_{t} x,y_{<t})),\]

where \(=\{(x_{i},y_{i})\}_{i=1,,N}\) denotes the set of (\(u,u^{}\)) pairs. \(x\) denotes the target utterance \(u\), and \(y\) represents the reconstructed utterance \(u^{}\). The model learns to aggregate information into conv-attn sinks during SMR with minimal training.

#### 3.2.3 Long-memory reactivation

The model is required to both aggregate dialogue information into conv-attn sinks and retrieve information from them. To ensure consistency in multi-turn dialogue, our proposed model must efficiently extract long context information during dialogue generation. Therefore, we introduce long-memory reactivation (LMR) to enhance its long-term memory capability.

Each pair of utterances in the dialogue dataset, denoted as \(qr\), represents a query-response pair with \(q\) and \(r\) from distinct roles. We organize training samples as \(D=\)\(q_{1}r_{1} q_{x}r_{x} q_{l}r_{l}q_{x}^{}r_{x}^{}\), with \(l\) denoting the number of training pairs. Each pair \(q_{x}^{}r_{x}^{}\) at the end of the sample is randomly selected from historical dialogues.

We design a response recall task where the goal is to recall \(r_{x}^{}\) from the historical context \(q_{x}r_{x}\) given query \(q_{x}^{}\). Concurrently, we adjust \(A\) so that each utterance can only attend to the first token, all conv-attn sinks, and itself. Moreover, each response in a training pair can attend to the corresponding query, while the conv-attn sink of the response is restricted to attending only to the response itself,

[MISSING_PAGE_FAIL:6]

Evaluation metricsWe evaluate model performance on the dialogue generation task using several metrics: BLEU (B-avg / B-1 / B-2, where B-avg is the average of BLEU-1 to BLEU-4 scores) , ROUGE (R-1 / R-2 / R-L) , and Distinct (D-1 / D-2 / D-3) . Additionally, we utilize two reference-free metrics specifically designed for dialogue quality assessment: USL-H  and Dial-M . Perplexity (PPL) is also computed. Furthermore, we report the C scores  for various models on PersonaChat to assess the consistency of the generated dialogues.

### Main results

Table 1 shows evaluation results for each method on the test sets (partial results related to baselines HRED and VHRED, metric C score, and datasets Topical-Chat and MultiWOZ, are available in Appendix C). For generation, we use the last utterance from each test set episode as the target for the model to generate. We also calculate the overall PPL for the entire test sets.

Our method outperforms sparse attention and memory-augmented baselines, achieving higher scores in BLEU, ROUGE, Distinct, and USL-H, while maintaining lower PPL and Dial-M metrics. For example, in MSC, StreamingDialogue demonstrates significant improvements over the second-best baseline, StreamingLLM, with B-avg increasing from 16.76% to 19.33% and R-L rising from 14.21% to 15.86%. In PersonaChat, specifically in D-2, StreamingDialogue increases from 32.64% to 37.23% compared to StreamingLLM. The notable superiority of StreamingDialogue can be attributed to its focus on conv-attn sinks, which compress historical information into them and cache them to enhance long-term memory, unlike baselines that rely on local attention windows and cannot handle extended dialogues effectively.

Furthermore, StreamingDialogue exhibits comparable performance to dense attention, e.g., in PersonaChat, the difference in ROUGE is less than 0.02%. It also outperforms dense attention in terms of R-1 and R-L in MSC and achieves better BLEU scores on PersonaChat. This validation highlights the more accurate information conveyance capabilities of text generated by our method.

### Human evaluation

In human evaluation, we generate dialogues from 100 randomly selected episodes of the MSC test set. Four crowdsource evaluators compare our method with StreamingLLM in fluency, coherence, and consistency, categorizing the outcome as win, tie, or loss. Figure 3 demonstrates our method's superiority across all metrics, particularly in consistency, showcasing StreamingDialogue's superior long-term memory capacity. We apply Fleiss' kappa  to measure the agreement among four annotators, yielding a result of 52.51%. This indicates that the inter-annotator agreement is moderate (\([0.4,0.6]\)). More details on human evaluation are in Appendix D.

### Ablation results

We conduct an experiment to test the effectiveness of SMR and LMR. Results, shown in Table 2, highlight a significant decline in model performance when either strategy is ablated, indicating the

   Model & PPL & BLEU-avg & ROUGE-L & Distinct-3 \\  Ours & 7.99 & 19.33 & 15.86 & 50.27 \\  Base & 8.21 & 17.32 & 10.25 & 46.15 \\ LMR & 8.01 & 18.87 & 15.66 & 49.44 \\ SMR & 8.40 & 18.25 & 15.24 & 48.57 \\   

Table 2: Ablation results on MSC with different learning strategies. “Base” denotes the model fine-tuned without SMR and LMR learning.

Figure 3: Fluency, coherence, and consistency in human evaluations: ours vs StreamingLLM.

importance of both strategies. The absence of SMR results in prominent declines in BLEU and ROUGE scores, indicating inadequate information aggregation in conv-attn sinks. Consequently, the model struggles to extract valuable information from conv-attn sinks during lengthy conversations, resulting in reduced text quality.

Similarly, without LMR, the model's performance declines significantly, indicating that relying solely on SMR leads to excessive guidance, limiting the model's ability in extended conversations. Thus, both SMR and LMR are crucial for enhancing information gathering and text extraction across conversations of long contexts.

### Performance on different context length

We evaluate the model performance across different conversation lengths in terms of perplexity and BLEU under varying context length (i.e., the number of utterances in the dialogue context) during inference, as shown in Figure 5. As dialogue length increases, StreamingDialogue exhibits greater superiority over StreamingLLM, with perplexity stabilizing and nearing convergence, and BLEU improving. Furthermore, StreamingDialogue maintains stable perplexity even with prolonged conversations over 25K tokens in inference (see Figure 6). This highlights our method's stability in handling long dialogues and emphasizes the importance of conv-attn sinks in enhancing long-term memory.

### Performance under the non-training setting

We validate the performance of StreamingLLM and our method under a non-training setting on the MSC test set using Llama-2-7B-Chat, Llama-3-8B-Instruct  and Mistral-7B . As the table

Figure 4: Average perplexity and BLEU for StreamingLLM and StreamingDialogue on the MSC test set across varying utterance counts. Figure 5: Per-token latency and memory usage by method on MSC for varying input lengths, with memory reported as total minus fixed.

   Model & Method & BLEU-avg & BLEU-1 & BLEU-2 & ROUGE-1 & ROUGE-2 & ROUGE-L \\   & StreamingLLM & 20.16 & 51.18 & 29.99 & 15.90 & 1.92 & 14.26 \\  & Ours & 20.19 & 51.55 & 30.03 & 16.46 & 2.11 & 15.00 \\   & StreamingLLM & 16.48 & 39.68 & 24.63 & 16.88 & 1.93 & 15.47 \\  & Ours & 16.77 & 40.10 & 24.88 & 17.11 & 2.01 & 15.85 \\   & StreamingLLM & 12.75 & 42.86 & 19.99 & 12.58 & 1.83 & 11.73 \\  & Ours & 13.33 & 44.08 & 20.65 & 13.40 & 1.98 & 12.58 \\   

Table 3: Results under the non-training setting on the MSC test set.

3 shows, our StreamingDialogue, thanks to the conv-attn sinks, retains more complete historical information. Consequently, it consistently outperforms StreamingLLM. This demonstrates that applying conv-attn sinks for modeling long contexts remains effective even without any training.

### Information preservation

To assess our method's information compression loss, we use SMR-trained models to reconstruct dialogue content from the MSC test set, leveraging only the conv-attn sink of each utterance. Randomly selecting 6,000 utterances from the MSC test set, we present the average results in Table 4. Our method achieves a BLEU-1 score of 89.19%, signifying effective compression of dialogue information with minimal losses.

### Speedup for inference

Figure 5 depicts the average per-token latency and memory usage during dialogue generation with NVIDIA A100 GPU using various methods. As input lengths increase, StreamingDialogue shows minimal growth in memory usage for caching conv-attn sinks, with latency exhibiting linear growth. This suggests that as dialogue length increases, StreamingDialogue's advantage becomes more promising. At an input length of 2,048, our method demonstrates a 6 \(\) improvement in memory usage compared to dense attention and an 18 \(\) improvement compared to dense attention with re-computation. In terms of per-token latency, our method shows a 4 \(\) improvement compared to dense attention with re-computation. Moreover, our method maintains similar latency and memory usage as StreamingLLM as context length varies.

### Impacts of SMR & LMR learning

Since the motivation of SMR and LMR learning is to improve conv-attn sinks aggregation capability, we examine the attention maps after SMR and LMR co-training, comparing them with the base model to confirm enhancement. Results are illustrated in Figure 7. Guided by SMR and LMR, the model's attention patterns transform into maps that sharply concentrate on conv-attn sinks, showcasing our effective enhancement of their information aggregation ability.

   BLEU-avg & BLEU-1 & BLEU-2 & ROUGE-1 & ROUGE-L \\ 
68.02 & 89.19 & 76.83 & 76.79 & 72.94 \\   

Table 4: Dialogue reconstruction performance.

Figure 8: The generated dialogues by StreamingLLM and StreamingDialogue for the same input dialogue history from an MSC episode, with an average utterance length of \(L=32\) tokens. Bold italic indicates key information in the dialogue.

Figure 7: Comparison of attention maps before and after learning. “Base” denotes Llama-2-7B, while “SMR & LMR” represents the model obtained post co-training with SMR and LMR on Llama-2-7B. The “</s>” positions in the encoded sentences are: 3, 6, 13, and 21.

### Case study: memory capacity

To validate StreamingDialogue's effectiveness in enhancing long-term memory, we conduct a case study comparing it with StreamingLLM. Figure 8 illustrates content generated by both methods. StreamingLLM responds solely based on recent utterances, lacking connection to distant context and coherence, thus reaffirming its unsuitability for open-domain dialogue. In contrast, StreamingDialogue effectively recalls distant historical information (e.g., 44 utterances ago), demonstrating the model's enhanced ability to remember long conversations through SMR and LMR.

### Generality of conv-attn sinks

To establish the generality of the conv-attn sink phenomenon, where separators attract more attention than other tokens within dialogues, we conduct both qualitative and quantitative analyses. Qualitatively, we visualize attention patterns across different training methods, attention mechanisms, and types of dataset constructions, demonstrating that this phenomenon persists regardless of these variables. Quantitative measures further support these findings, with a set threshold indicating significantly higher attention on separators than on other tokens. Detailed results, including visualizations and statistical data, are provided in Appendix E. This comprehensive analysis confirms the robustness of the conv-attn sink phenomenon across different settings and models.

### Analysis of EoU tokens' information aggregation capability

To assess the effectiveness of EoU tokens in capturing dialogue information, we conduct experiments using an untrained Llama-2-7B-Chat model. The model focuses solely on EoU tokens and the last complete utterance. In a case study with the conversation: "Did you have a caramel macchiato today?c/s>Yes!</s>What kind of coffee did you have today?</s>," the model successfully identifies the key information, responding with, "I had a delicious caramel macchiato this morning."

We replicate this experiment across 10 different prompt formats, each containing 20 samples. The results indicate that 68% of the model's responses accurately include essential information. Further details on these formats are available in Appendix G.

### Hyper-parameter sensitives

We investigate the impact of two hyper-parameters in our method: the number of utterances in SMR samples (\(s\)) and the number of query-response pairs in LMR samples (\(l\)), both ranging from \(\{8,12,16,20,24,28,32\}\). We maintain a constant total number of utterances in the training set, with examples like \(s=8=24,000\) and \(s=12=16,000\). Results shown in Figure 9 reveal that StreamingDialogue performs best with higher values of \(s\) and \(l\), optimally at \(s\{28,32\}\) and \(l\{20,24,28\}\). This suggests that longer texts enhance the model's learning.

## 5 Conclusion

Generating high-quality open-domain dialogues with prolonged contexts is quite challenging. Existing solutions, like dense attention, have efficiency issues. While StreamingLLM supports efficient language modeling, it struggles to preserve historical information, leading to low-quality generation in prolonged conversations. In this paper, we introduce StreamingDialogue, a framework capable of facilitating efficient and prolonged dialogue. By identifying separator tokens EoU as "conv-attn sinks" and compressing dialogue information into them with minimal losses, StreamingDialogue conserves memory, enhances efficiency, and augments long-term memory capabilities. Additionally, we propose two learning strategies to enhance conv-attn sink aggregation and memory reactivation. Our method shows better performance compared to strong baselines. In the future, we will explore extending StreamingDialogue towards never-ending dialogue in the context of lifelong learning.

Figure 9: Normalized performance scores (PPL, B-avg, R-L, and D-3) on MSC for various \(l\) with \(s\) fixed at 28 and various \(s\) with \(l\) fixed at 24.