# Boosting with Tempered Exponential Measures

Richard Nock

Google Research

richardnock@google.com &Ehsan Amid

Google DeepMind

eamid@google.com &Manfred K. Warmuth

Google Research

manfred@google.com

###### Abstract

One of the most popular ML algorithms, AdaBoost, can be derived from the dual of a relative entropy minimization problem subject to the fact that the positive weights on the examples sum to one. Essentially, harder examples receive higher probabilities. We generalize this setup to the recently introduced _tempered exponential measures_ (tems) where normalization is enforced on a specific power of the measure and not the measure itself. tems are indexed by a parameter \(t\) and generalize exponential families (\(t=1\)). Our algorithm, \(t\)-AdaBoost, recovers AdaBoost as a special case (\(t=1\)). We show that \(t\)-AdaBoost retains AdaBoost's celebrated exponential convergence rate on margins when \(t[0,1)\) while allowing a slight improvement of the rate's hidden constant compared to \(t=1\). \(t\)-AdaBoost partially computes on a generalization of classical arithmetic over the reals and brings notable properties like guaranteed bounded leveraging coefficients for \(t[0,1)\). From the loss that \(t\)-AdaBoost minimizes (a generalization of the exponential loss), we show how to derive a new family of _tempered_ losses for the induction of domain-partitioning classifiers like decision trees. Crucially, strict properness is ensured for all while their boosting rates span the full known spectrum of boosting rates. Experiments using \(t\)-AdaBoost+trees display that significant leverage can be achieved by tuning \(t\).

## 1 Introduction

AdaBoost is one of the most popular ML algorithms [8; 30]. It efficiently aggregates weak hypotheses into a highly accurate linear combination . The common motivations of boosting algorithms focus on choosing good linear weights (the leveraging coefficients) for combining the weak hypotheses. A dual view of boosting highlights the dual parameters, which are the weights on the examples. These weights define a distribution, and AdaBoost can be viewed as minimizing a relative entropy to the last distribution subject to a linear constraint introduced by the current hypothesis . For this reason (more in SS 2), AdaBoost's weights define an exponential family.

**In this paper**, we go beyond weighing the examples with a discrete exponential family distribution, relaxing the constraint that the total mass be unit but instead requiring it for the measure's \(1/(2-t)\)'th power, where \(t\) is a temperature parameter. Such measures, called _tempered exponential measures_ (tems), have been recently introduced . Here we apply the discrete version of these tems for deriving a novel boosting algorithm called \(t\)-AdaBoost. Again the measures are solutions to a relative entropy minimization problem, but the relative entropy is built from Tsallis entropy and "tempered" by a parameter \(t\). As \(t 1\)tems become standard exponential family distributions and our new algorithm merges into AdaBoost. As much as AdaBoost minimizes the exponential loss, \(t\)-AdaBoost minimizes a generalization of this loss we denote as the _tempered exponential loss_.

tems were introduced in the context of clustering, where they were shown to improve the robustness to outliers of clustering's population minimizers . They have also been shown to bring low-level sparsity features to optimal transport . Boosting is a high-precision machinery: AdaBoost is known to achieve near-optimal boosting rates under the weak learning assumption , but it haslong been known that numerical issues can derail it, in particular, because of the unbounded weight update rule . So the question of what the tem setting can bring for boosting is of primordial importance. As we show, \(t\)-AdaBoost can suffer no rate setback as boosting's exponential rate of convergence on _margins_ can be preserved for all \(t[0,1)\). Several interesting features emerge: the weight update becomes bounded, margin optimization can be _tuned_ with \(t\) to focus on examples with very low margin and besides linear separators, the algorithm can also learn _progressively_ clipped models1. Finally, the weight update makes appear a new regime whereby weights can "switch off and on": an example's weight can become zero if too well classified by the current linear separator, and later on revert to non-zero if badly classified by a next iterate. \(t\)-AdaBoost makes use of a generalization of classical arithmetic over the reals introduced decades ago .

Boosting algorithms for linear models like AdaBoost bring more than just learning good linear separators: it is known that (ada)boosting linear models can be used to emulate the training of _decision trees_ (DT) , which are models known to lead to some of the best of-the-shelf classifiers when linearly combined . Unsurprisingly, the algorithm obtained emulates the classical top-down induction of a tree found in major packages like CART  and C4.5 . The _loss_ equivalently minimized, which is, _e.g._, Matusita's loss for AdaBoost[30, Section 4.1], is a lot more consequential. Contrary to losses for real-valued classification, losses to train DTs rely on the estimates of the posterior learned by the model; they are usually called _losses for Class Probability Estimation_ (CPE ). The CPE loss is crucial to elicit because (i) it is possible to check whether it is "good" from the standpoint of properness (Bayes rule is optimal for the loss ), and (ii) it conditions boosting rates, only a handful of them being known, for the most popular CPE losses [11; 22; 31].

**In this paper**, we show that this emulation scheme on \(t\)-AdaBoost provides a new family of CPE losses with remarkable constancy with respect to properness: losses are _strictly_ proper (Bayes rule is the _sole_ optimum) for any \(t(-,2)\) and proper for \(t=-\). Furthermore, over the range \(t[-,1]\), the range of boosting rates spans the full spectrum of known boosting rates .

We provide experiments displaying the boosting ability of \(t\)-AdaBoost over a range of \(t\) encompassing potentially more than the set of values covered by our theory, and highlight the potential of using \(t\) as a parameter for efficient tuning the loss [25, Section 8]. Due to a lack of space, proofs are relegated to the appendix (APP). A primer on tems is also given in APP., Section I.

## 2 Related work

Boosting refers to the ability of an algorithm to combine the outputs of moderately accurate, "weak" hypotheses into a highly accurate, "strong" ensemble. Originally, boosting was introduced in the context of Valiant's PAC learning model as a way to circumvent the then-existing amount of related negative results [10; 34]. After the first formal proof that boosting is indeed achievable , AdaBoost became the first practical and proof-checked boosting algorithm [8; 30]. Boosting was thus born in a machine learning context, but later on, it also emerged in statistics as a way to learn from class residuals computed using the gradient of the loss [9; 21], resulting this time in a flurry of computationally efficient algorithms, still called boosting algorithms, but for which the connection with the original weak/strong learning framework is in general not known.

Our paper draws its boosting connections with AdaBoost's formal lineage. AdaBoost has spurred a long line of work alongside different directions, including statistical consistency , noise handling [15; 16], low-resource optimization , _etc_. The starting point of our work is a fascinating result in convex optimization establishing a duality between the algorithm and its memory of past iteration's performances, a probability distribution of so-called _weights_ over examples . From this standpoint, AdaBoost solves the dual of the optimization of a Bregman divergence (constructed from the negative Shannon entropy as the generator) between weights subject to zero correlation with the last weak classifier's performance. As a consequence, weights define an exponential family. Indeed, whenever a relative entropy is minimized subject to linear constraints, then the solution is a member of an exponential family of distributions (see _e.g._[2, Section 2.8.1] for an axiomatization of exponential families). AdaBoost's distribution on the examples is a member of a discrete exponential family where the training examples are the finite support of the distribution, sufficient statistics are defined from the weak learners, and the leveraging coefficients are the natural parameters. In summary, there is an intimate relationship between boosting a-la-AdaBoost, exponential families, and Bregman divergences [7; 12; 20] and our work "elevates" these methods above exponential families.

## 3 Definitions

We define the \(t\)-logarithm and \(t\)-exponential [17; Chapter 7],

\[_{t}(z)(z^{1-t}-1),_{t}(z )[1+(1-t)z]_{+}^{1/(1-t)}([z]_{+}\{0,z\}), \]

where the case \(t=1\) is supposed to be the extension by continuity to the \(\) and \(\) functions, respectively. To preserve the concavity of \(_{t}\) and the convexity of \(_{t}\), we need \(t 0\). In the general case, we also note the asymmetry of the composition: while \(_{t}_{t}(z)=z, t\), we have \(_{t}_{t}(z)=z\) for \(t=1\) (\( z\)), but

\[_{t}_{t}(z)=\{-,z\}(t<1)_{t}_{t}(z)=\{,z\}(t>1).\]

Comparisons between vectors and real-valued functions written on vectors are assumed component-wise. We assume \(t 2\) and define notation \(t\)* \( 1/(2-t)\). We now define the key set in which we model our weights (boldfaces denote vector notation).

**Definition 3.1**.: _The co-simplex of \(^{m}\), \(_{m}\) is defined as \(_{m}\{^{m}:^ {}^{1/t}=1\}\)._

The letters \(\) will be used to denote tems in \(_{m}\) while \(\) denote the co-density \(^{}}\) or any element of the probability simplex. We define the general tempered relative entropy as

\[D_{t}(^{}\|) = _{i[m]}q^{}_{i}(_{t}q^{}_{i}- _{t}q_{i})-_{t-1}q^{}_{i}+_{t-1}q_{i}, \]

where \([m]\{1,...,m\}\). The tempered relative entropy is a Bregman divergence with convex generator \(_{t}(z) z_{t}z-_{t-1}(z)\) (for \(t\)) and \(_{t}(z)^{}=_{t}(x)\). As \(t 1\), \(D_{t}(,^{})\) becomes the relative entropy with generator \(_{1}(x)=x(x)-x\).

## 4 Tempered boosting as tempered entropy projection

We start with a fixed sample \(=\{(_{i},y_{i}):i[m]\}\) where observations \(_{i}\) lie in some domain \(\) and labels \(y_{i}\) are \( 1\). AdaBoost maintains a distribution \(\) over the sample. At the current iteration, this distribution is updated based on a current _weak hypothesis_\(h^{}\) using an exponential update:

\[p^{}_{i}=(- u_{i})}{_{k}p_{k}(- u _{k})},\;\;u_{i} y_{i}h(_{i}),.\]

In  this update is motivated as minimizing a relative entropy subject to the constraint that \(^{}\) is a distribution summing to 1 and \(^{}=0\). Following this blueprint, we create a boosting algorithm maintaining a discrete tem over the sample which is motivated as a constrained minimization of the tempered relative entropy, with a normalization constraint on the co-simplex of \(^{m}\):

\[^{}  _{}_{m}} D_{t}(}\|),^{m}.\] \[}^{}=0\]

We now show that the solution \(^{}\) is a tempered generalization of AdaBoost's exponential update.

**Theorem 1**.: _For all \(t\{2\}\), all solutions to (3) have the form_

\[q^{}_{i}=(_{t}q_{i}- u_{i})}{Z_{t}}(- _{t}_{t}(- u_{i})}{Z_{t}},\;\;a_{t}b [a^{1-t}+b^{1-t}-1]_{+}^{}), \]

_where \(Z_{t}\) ensures co-simplex normalization of the co-density. Furthermore, the unknown \(\) satisfies_

\[-_{t}(Z_{t}())(= Z_{t}()), \]or equivalently is a solution to the nonlinear equation_

\[^{}()^{} = 0. \]

_Finally, if either (i) \(t_{>0}\{2\}\) or (ii) \(t=0\) and \(\) is not collinear to \(\), then \(Z_{t}()\) is strictly convex: the solution to (3) is thus unique, and can be found from expression (4) by finding the unique minimizer of (5) or (equivalently) the unique solution to (6)._

(Proof in APP., Section II.1) The \(t\)-product \(_{t}\), which satisfies \(_{t}(a+b)=_{t}(a)_{t}_{t}(b)\), was introduced in . Collinearity never happens in our ML setting because \(\) contains the edges of a weak classifier: \(>0\) and collinearity would imply that \(\) the weak classifier performs perfect classification, and thus defeats the purpose of training an ensemble. \( t\{2\}\), we have the simplified expression for the normalization coefficient of the tem and the co-density \(^{}\) of \(^{}\):

\[Z_{t}=\|_{t}(_{t}-)\|_{1/t }\ ;\ \ p^{}_{i}=_{t}_{t}(- }{t^{}})}{Z^{}_{t}}\ \ \ \ Z^{}_{t} Z^{1/t}_{t}. \]

## 5 Tempered boosting for linear classifiers and clipped linear classifiers

ModelsA model (or classifier) is an element of \(^{}\). For any model \(H\), its empirical risk over \(\) is \(F_{}{{1}}}(H,)(1/m)_{i}[y_{i}(H(_{i}))]\) where \([.]\), Iverson's bracket , is the Boolean value of the inner predicate. We learn linear separators and _clipped_ linear separators. Let \((v_{j})_{j 1}\) be the terms of a series and \( 0\). The clipped sum of the series is:

\[}{{(-)}}\!\!\!\! _{j[J]}v_{j}&=&\{,\{-,v_{J}+ }{{(-)}}\!\!\!\!_{j[J-1]}v_{j}\\ \}\}&([-,]),\ \ J>1,\]

and we define the base case \(J=1\) by replacing the inner clipped sum with 0. Note that clipped summation is non-commutative, and so is different from clipping in \([-,]\) the whole sum itself2. Given a set of so-called weak hypotheses \(h_{j}^{}\) and leveraging coefficients \(_{j}\) (for \(j[J]\)), the corresponding linear separators and clipped linear separators are

\[H_{J}()_{j[J]}_{j}h_{j}(); H_{J}^{( )}()}{{(- )}}\!\!\!\!_{j[J]}_{j}h_{j}(). \]

Tampered boosting and its general convergenceOur algorithm, \(t\)-AdaBoost, is presented in Algorithm 1, using presentation conventions from . Before analyzing its convergence, several properties are to be noted for \(t\)-AdaBoost: first, it keeps the appealing property, introduced by AdaBoost, that examples receiving the wrong class by the current weak classifier are reweightedhigher (if \(_{j}>0\)). Second, the leveraging coefficients for weak classifiers in the final classifier (\(_{j}\)s) are not the same as the ones used to update the weights (\(_{j}\)s), unless \(t=1\). Third and last, because of the definition of \(_{t}\) (1), if \(t<1\), tempered weights can switch off and on, _i.e._, become 0 if an example is "too well classified" and then revert back to being \(>0\) if the example becomes wrongly classified by the current weak classifier (if \(_{j}>0\)). To take into account those zeroing weights, we denote \([m]_{j}^{}\{i:q_{ji}=0\}\) and \(m_{j}^{}([m]_{j}^{})\) (\( j[J]\) and \(\) denotes the cardinal). Let \(R_{j}_{i[m]_{j}^{}}|y_{i}h_{j}(_{i})|/q_{ ji}^{1-t}\) and \(q_{j}^{}_{i[m]_{j}^{}}|y_{i}h_{j}(_{i}) |^{1/(1-t)}/R_{j}^{1/(1-t)}\). It is worth noting that \(q_{j}^{}\) is homogeneous to a tempered weight.

**Theorem 2**.: _At iteration \(j\), define the weight function \(q_{ji}^{}=q_{ji}\) if \(i[m]_{j}^{}\) and \(q_{j}^{}\) otherwise; set_

\[_{j}  ^{}}{q_{j}^{}}^{2-t})R_{j}} _{i[m]}q_{ji}^{}y_{i}h_{j}(_{i})([-1,1]). \]

_In algorithm \(t\)-AdaBoost, consider the choices (with the convention \(_{k=1}^{0}v_{k} 1\))_

\[_{j}-}_{t}(}{M_{1-t}(1- _{j},1+_{j})}),_{j}}^{*}( _{k=1}^{j-1}Z_{k})^{1-t}_{j}, \]

_where \(M_{q}(a,b)((a^{q}+b^{q})/2)^{1/q}\) is the \(q\)-power mean. Then for any \(H\{H_{J},H_{J}^{(}{{1-t}})}\}\), its empirical risk is upperbounded as:_

\[F_{}{{1}}}(H,)_{j=1}^{J}Z_{tj}^{2-t} _{j=1}^{J}(1+{m_{j}^{}}{q_{j}^{}}^{2-t}) K_{t}( _{j})(K_{t}(z)}{M_{1-t}(1-z,1+z)}). \]

(Proof in APP., Section II.2) We jointly comment \(t\)-AdaBoost and Theorem 2 in two parts.

**Case \(t 1^{-}\):**\(t\)-AdaBoost converges to AdaBoost and Theorem 2 to its convergence analysis: \(t\)-AdaBoost converges to AdaBoost as presented in [30, Figure 1]: the tempered simplex becomes the probability simplex, \(_{t}\) converges to regular multiplication, weight update (8) becomes AdaBoost's, \(_{j}_{j}\) in (11) and finally the expression of \(_{j}\) converges to AdaBoost's leveraging coefficient in  (\(_{t 1}M_{1-t}(a,b)=\)). Even guarantee (12) converges to AdaBoost's popular guarantee of [30, Corollary 1] (\(_{t 1}K_{t}(z)=}\), \(m_{j}^{}=0\)). Also, in this case, we learn only the unclipped classifier since \(_{t 1^{-}}H_{J}^{(}{{1-t}})}=H_{J}\).

**Case \(t<1\):** Let us first comment on the convergence rate. The proof of Theorem 2 shows that \(K_{t}(z)(-z^{2}/(2t^{*}))\). Suppose there is no weight switching, so \(m_{j}^{}=0, j\) (see Section 7) and, as in the boosting model, suppose there exists \(>0\) such that \(|_{j}|, j\). Then \(t\)-AdaBoost is guaranteed to attain empirical risk below some \(>0\) after a number of iterations equal to \(J=(2t^{*}/^{2})(1/)\). \(t^{*}\) being an increasing function of \(t\), we see that \(t\)-AdaBoost is able to slightly improve upon AdaBoost's celebrated rate . However, \(t^{*}=1/2\) for \(t=0\) so the improvement is just on the hidden constant. This analysis is suited for small values of \(|_{j}|\) and does not reveal an interesting phenomenon for better weak hypotheses. Figure 1 compares \(K_{t}(z)\) curves (\(K_{1}(z)_{t 1}K_{t}(z)=}\) for AdaBoost, see [30, Corollary 1]), showing the case \(t<1\) can be substantially better, especially when weak hypotheses are not "too weak". If \(m_{j}^{}>0\), switching weights can impede our convergence _analysis_, though exponential convergence is always possible if \({m_{j}^{}}{q_{j}^{}}^{2-t}\) is small enough; also, when it is not, we may in fact have converged to a good model (see APP., Remark 1). A good criterion to train weak hypotheses is then the optimization of the edge \(_{j}\), thus using \(_{j}^{}\) normalized in the simplex. Other key features of \(t\)-AdaBoost are as follows. First, the weight update and leveraging coefficients of weak classifiers are bounded because \(|_{j}|<1/(R_{j}(1-t))\) (APP., Lemma H) (this is not the case for \(t 1^{-}\)). This guarantees that new weights are bounded before normalization (unlike for \(t 1^{-}\)). Second, we remark that \(_{j}_{j}\) if \(t 1\). Factor \(m^{1-t^{*}}\) is added for convergence analysis purposes; we can discard it to train the unclipped classifier: it does not change its empirical risk. This is, however, different for factor \(_{k=1}^{j-1}Z_{k}\): from (12), we conclude that this is an indication of how well the past ensemble performs.

phenomenon that does not occur in boosting, where an excellent weak hypothesis on the current weights can have a leveraging coefficient so large that it wipes out the classification of the past ones. This can be useful to control numerical instabilities.

Extension to marginsA key property of boosting algorithms like AdaBoost is to be able to boost not just the empirical risk but more generally _margins_, where a margin integrates both the accuracy of label prediction but also the confidence in prediction (say \(|H|\)). We generalize the margin notion of  to the tempered arithmetic and let \(_{t}((,y),H)_{t}(yH()/2)\) denote the margin of \(H\) on example \((,y)\), where \(_{t}(z)(1-_{t}(-2z))/(1+_{t}(-2z))([-1,1])\) is the tempered hyperbolic tangent. The objective of minimizing the empirical risk is generalized to minimizing the margin risk, \(F_{t,}(H,)(1/m)_{i}[_{t}((_{i},y_{i}),H)]\), where \((-1,1)\). Guarantees on the empirical risk are guarantees on the margin risk for \(=0\) only. In just a few steps, we can generalize Theorem 2 to _all_\((-1,1)\). For space reason, we state the core part of the generalization, from which extending it to a generalization of Theorem 2 is simple.

**Theorem 3**.: _For any \((-1,1)\) and \(t\), the guarantee of algorithm \(t\)-AdaBoost in Theorem 2 extends to the margin risk, with notations from Theorem 2, via:_

\[F_{t,}(H,)  ()^{2-t}_{j=1}^{J}Z_{tj }^{2-t}. \]

(Proof in APP., Section II.3) At a high level, \(t\)-AdaBoost brings similar margin maximization properties as AdaBoost. Digging a bit in (13) reveals an interesting phenomenon for \(t 1\) on how margins are optimized compared to \(t=1\). Pick \(<0\), so we focus on those examples for which the classifier \(H\) has high confidence in its _wrong_ classification. In this case, factor \(((1+)/(1-))^{2-t}\) is increasing as a function of \(t\) (and this pattern is reversed for \(>0\)). In words, the smaller we pick \(t\) and the better is the bound in (13), suggesting increased "focus" of \(t\)-AdaBoost on increasing the margins of examples _with low negative margin_ (_e.g._ the most difficult ones) compared to the case \(t=1\).

The tempered exponential lossIn the same way as AdaBoost introduced the now famous exponential loss, (12) recommends to minimize the normalization coefficient, following (7),

\[Z_{tj}^{2-t}() = \|_{t}(_{t}_{j}-_{j}) \|_{1/t^{}}^{1/t^{}}(u_{ji} y_{i}h_{j}( _{i})). \]

We cannot easily unravel the normalization coefficient to make appear an equivalent generalization of the exponential loss, unless we make several assumptions, one being \(_{i}|h_{j}(_{i})|\) is small enough for any \(j[J]\). In this case, we end up with an equivalent criterion to minimize which looks like

\[F_{t}(H,) = _{i}_{t}^{2-t}(-y_{i}H(_{i}) ), \]

Figure 1: Plot of \(K_{t}(z)\) in (12), \(t\) (the smaller, the better for convergence).

where we have absorbed in \(H\) the factor \(m^{1-t^{}}\) appearing in the \(_{t}\) (scaling \(H\) by a positive value does not change its empirical risk). This defines a generalization of the exponential loss which we call the _tempered exponential loss_. Notice that one can choose to minimize \(F_{t}(H,)\) disregarding any constraint on \(|H|\).

## 6 A broad family of boosting-compliant proper losses for decision trees

Losses for class probability estimationWhen it comes to tabular data, it has long been known that some of the best models to linearly combine with boosting are decision trees (DT, ). Decision trees, like other domain-partitioning classifiers, are not trained by minimizing a _surrogate loss_ defined over real-valued predictions, but defined over _class probability estimation_ (CPE, ), those estimators being posterior estimation computed at the leaves. Let us introduce a few definitions for those. A CPE loss \(:\{-1,1\}\) is

\[(y,u)   y=1_{1}(u)+ y=-1 _{-1}(u). \]

Functions \(_{1},_{-1}\) are called _partial_ losses. The pointwise conditional risk of local guess \(u\) with respect to a ground truth \(v\) is:

\[(u,v)  v_{1}(u)+(1-v)_{-1}(u). \]

A loss is _proper_ iff for any ground truth \(v\), \((v,v)=_{u}(u,v)\), and strictly proper iff \(u=v\) is the sole minimizer . The (pointwise) _Bayes_ risk is \((v)_{u}(u,v)\). The log/cross-entropy-loss, square-loss, Matusita loss are examples of CPE losses. One then trains a DT minimizing the expectation of this loss over leaves' posteriors, \(_{}[(p_{})]\), \(p_{}\) being the local proportion of positive examples at leaf \(\) - or equivalently, the local posterior.

Deriving CPE losses from (ada)boostingRecently, it was shown how to derive in a general way a CPE loss to train a DT from the minimization of a surrogate loss with a boosting algorithm . In our case, the surrogate would be \(_{tj}\) (14) and the boosting algorithm, \(t\)-AdaBoost. The principle is simple and fits in four steps: (i) show that a DT can equivalently perform simple linear classifications, (ii) use a weak learner that designs splits and the boosting algorithm to fit the leveraging coefficient and compute those in closed form, (iii) simplify the expression of the loss using those, (iv) show that the expression simplified is, in fact, a CPE loss. To get (i), we remark that a DT contains a tree (graph). One can associate to each node a real value. To classify an observation, we sum all reals from the root to a leaf and decide on the class based on the sign of the prediction, just like for any real-valued predictor. Suppose we are at a leaf. What kind of weak hypotheses can create splits "in disguise"? Those can be of the form

\[h_{j}()   x_{k} a_{j} b_{j}, a_{j},b_{j },\]

where the observation variable \(x_{k}\) is assumed real valued for simplicity and the test \( x_{k} a_{j}\) splits the leaf's domain in two non-empty subsets. This creates half of the split. \(_{j}() x_{k}<a_{j}-b_{j}\) creates the other half of the split. Remarkably \(h_{j}\) satisfies the weak learning assumption iff \(_{j}\) does . So we get the split design part of (ii). We compute the leveraging coefficients at the new leaves from the surrogate's minimization / boosting algorithm, end up with new real predictions at the new leaves (instead of the original \(b_{j},-b_{j}\)), push those predictions in the surrogate loss for (iii), simplify it and, quite remarkably end up with a loss of the form \(_{}[(p_{})]\), where L turns out to be the pointwise Bayes risk \(\) of a proper loss  (notation from ).

In the case of , it is, in fact, granted that we end up with such a "nice" CPE loss because of the choice of the surrogates at the start. In our case, however, nothing grants this _a priori_ if we start from the tempered exponential loss \(_{tj}\) (14) so it is legitimate to wonder whether such a chain of derivations (summarized) can happen to reverse engineer an interesting CPE loss:

\[_{tj}}{{}} }{{}}^{(t)} }{{}}_{1}^{(t)};_{-1}^{(t)}( \]

When such a complete derivation happens until the partial losses \(_{1};_{-1}\) and their properties, we shall write that minimizing \(_{tj}\)_elicits_ the corresponding loss and partial losses.

**Theorem 4**.: _Minimizing \(_{tj}\) elicits the CPE loss we define as the **tempered loss**, with partial losses:_

\[_{1}^{(t)}(u)((u,1-u)})^{2-t}, _{-1}^{(t)}(u)_{1}^{(t)}(1-u),(t[-,2]). \]

_The tempered loss is symmetric, differentiable, strictly proper for \(t(-,2)\) and proper for \(t=-\)._Differentiability means the partial losses are differentiable, and symmetry follows from the relationship between partial losses  (the proof, in APP., Section II.4, derives the infinite case, \(_{1}^{(-)}(u)=2[u 1/2]\)) Let us explicit the Bayes risk of the tempered loss and a key property.

**Lemma 1**.: _The Bayes risk of the tempered loss is (\(M_{q}\) defined in Theorem 2):_

\[^{(t)}(u) = (u,1-u)}, \]

_and it satisfies \( u, z[2\{u,1-u\},1]\), \(2t[-,2]\) such that \(^{(t)}(u)=z\)._

Lemma 1, whose proof is trivial, allows to show a key boosting result: \(t=1\) retrieves Matusita's loss, for which a near-optimal boosting rate is known  while \(t=-\) retrieves the empirical risk, which yields the worst possible guarantee . In between, we have, for example, CART's Gini criterion for \(t=0\), which yields an intermediate boosting guarantee. Continuity with respect to \(t\) of the Bayes risks in between the empirical risk and Matusita's loss means the boosting ranges of the tempered loss cover _the full known spectrum of boosting rates_ for \(t[-,1]\). We know of no (differentiable and) proper CPE loss with such coverage. Note that (i) this is a non-constructive result as we do not associate a specific \(t\) for a specific rate, and (ii) the state-of-the-art boosting rates for DT induction do not seem to cover the case \(t(1,2)\), thus left as an open question.

## 7 Experiments

We have performed experiments on a testbed of 10 UCI domains, whose details are given in APP. (Section A3). Experiments were carried out using a 10-fold stratified cross-validation procedure.

    &  &  &  &  \\   &  &  &  &  &  \\ 

Table 1: Experiments on \(t\)-AdaBoost comparing with AdaBoost (\(t=1\), bullets) on three domains (rows), displaying from left to right the estimated true error of non-clipped and clipped models, and the min and max codensity weights. These domains were chosen to give an example of three different situations: small values of \(t\) perform well (abalone), the best performance is achieved by the largest \(t\) (_e.g._ AdaBoost, qsar), and the worst performance is achieved by the largest \(t\) (adult). Topmost row is without noise (\(=0\)) while the others are with \(10\%\) training noise; \(t\) scale displayed with varying color and width (colormap indicated on each plot). Averages shown for readability: see Table 2 for exhaustive statistical tests.

To compare \(t\)-AdaBoost with AdaBoost, we ran \(t\)-AdaBoost with a first range of values of \(t\{0.0,0.2,0.4,0.6,0.8,0.9\}\). This is in the range of values covered by our convergence result for linear separators in Theorem 2. Our results on decision tree induction cover a much wider range, in particular for \(t(1,2)\). To assess whether this can be an interesting range to study, we added \(t=1.1\) to the set of tested \(t\) values. When \(t>1\), some extra care is to be put into computations because the weight update becomes unbounded, in a way that is worse than AdaBoost. Indeed, as can be seen from (8), if \(_{j}y_{i}h_{j}(_{i})-1/(t-1)\) (the example is badly classified by the current weak hypothesis, assuming wlog \(_{j}>0\)), the weight becomes infinity before renormalization. In our experiments, picking a value of \(t\) close to \(2\) clearly shows this problem, so to be able to still explore whether \(t>1\) can be useful, we picked a value close to \(1\), namely \(t=1.1\), and checked that in our experiments this produced no such numerical issue. We also considered training clipped and not clipped models.

All boosting models were trained for a number of \(J=20\) decision trees (The appendix provides experiments on training bigger sets). Each decision tree is induced using the tempered loss with the corresponding value of \(t\) (see Theorem 4) following the classical top-down template, which consists in growing the current heaviest leaf in the tree and picking the best split for the leaf chosen. We implemented \(t\)-AdaBoost exactly as in Section 5, including computing leveraging coefficients as suggested. Thus, we do not scale models. More details are provided in the appendix. In our experiments, we also included experiments on a phenomenon highlighted more than a decade ago  and fine-tuned more recently , the fact that a convex booster's model is the weakest link when it has to deal with noise in training data. This is an important task because while the tempered exponential loss is convex, it does not fit into the blueprint loss of [15, Definition 1] because it is not \(C^{1}\) if \(t 1\). One might thus wonder how \(t\)-AdaBoost behaves when training data is affected by noise. Letting \(\) denote the proportion of noisy data in the training sample, we tried \(\{0.0,0.1\}\) (The appendix provides experiments on more noise levels). We follow the noise model of  and thus independently flip the true label of each example with probability \(\).

For each run, we recorded the average test error and the average maximum and minimum co-density weight. Table 1 presents a subset of the results obtained on three domains. Table 2 presents a more synthetic view in terms of statistical significance of the results for \(t 1\) vs. \(t=1\) (AdaBoost). The table reports only results for \(t 0.6\) for synthesis purposes. Values \(t<0.6\) performed on average slightly worse than the others _but_ on some domains, as the example of abalone suggests in Table 2 (the plots include all values of \(t\) tested in \([0,1.1]\)), we clearly got above-par results for such small values of \(t\), both in terms of final test error but also fast early convergence to low test error. This comment can be generalized to all values of \(t\).

The weights reveal interesting patterns as well. First, perhaps surprisingly, we never encountered the case where weights switch off, regardless of the value of \(t\). The average minimum weight curves of Table 1 generalize to all our tests (see the appendix). This does not rule out the fact that boosting for a much longer number of iterations might lead to weights switching off/on, but the fact that this does not happen at least early during boosting probably comes from the fact that the leveraging coefficients for weights (\(\).) are bounded. Furthermore, their maximal absolute value is all the smaller as \(t\) decreases to 0. Second, there is a pattern that also repeats on the maximum weights, not on all domains but on a large majority of them and can be seen in abalone and adult in Table 1: the maximum weight of AdaBoost tends to increase much more rapidly compared to \(t\)-AdaBoost with \(t<1\). In the latter case, we almost systematically observe that the maximum weight tends to be upperbounded, which is not the case for AdaBoost (the growth of the maximal weight looks almost linear). Having bounded weights could be of help to handle numerical issues of (ada)boosting .

Our experiments certainly confirm the boosting nature of \(t\)-AdaBoost if we compare its convergence to that of AdaBoost: more often than not, it is in fact comparable to that of AdaBoost. While this applies broadly for \(t 0.6\), we observed examples where much smaller values (even \(t=0.0\)) could yield such fast convergence. Importantly, this applies to clipped models as well and it is important to notice because it means attaining a low "boosted" error does not come at the price of learning models with large range. This is an interesting property: for \(t=0.0\), we would be guaranteed that the computation of the clipped prediction is always in \([-1,1]\). Generalizing our comment on small values of \(t\) above, we observed that an efficient tuning algorithm for \(t\) could be able to get very substantial leverage over AdaBoost. Table 2 was crafted for a standard limit \(p\)-val of 0.1 and "blurs" the best results that can be obtained. On several domains (winered, abalone, eeg, creditcard, adult), applicable \(p\)-values for which we would conclude that some \(t 1\) performs better than \(t=1\) drop in between \(7E-4\) and \(0.05\). Unsurprisingly, AdaBoost also manages to beat significantly alternative values of \(t\) in several cases. Our experiments with training noise (\(=0.1\)) go in the same direction. Looking at Table 1, one could eventually be tempted to conclude that \(t\) slightly smaller than 1.0 may be a better choice than adaboosting (\(t=1\)), as suggested by our results for \(t=0.9\), but we do not think this produces a general "rule-of-thumb". There is also no apparent "noise-dependent" pattern that would obviously separate the cases \(t<1\) from \(t=1\) even when the tempered exponential loss does not fit to 's theory. Finally, looking at the results for \(t>1\) also yields the same basic conclusions, which suggests that boosting can be attainable outside the range covered by our theory (in particular Theorem 2).

All this brings us to the experimental conclusion that the question does not reside on opposing the case \(t 1\) to the case \(t=1\). Rather, our experiments suggest - pretty much like our theory does - that the actual question resides in how to efficiently _learn_\(t\) on a domain-dependent basis. Our experiments indeed demonstrate that substantial gains could be obtained, to handle overfitting or noise.

## 8 Discussion, limitations and conclusion

AdaBoost is one of the original and simplest Boosting algorithms. In this paper we generalized AdaBoost to maintaining a tempered measure over the examples by minimizing a tempered relative entropy. We kept the setup as simple as possible and therefore focused on generalizing AdaBoost. However more advanced boosting algorithms have been designed based on relative entropy minimization subject to linear constraints. There are versions that constrain the edges of all past hypotheses to be zero . Also, when the maximum margin of the game is larger than zero, then AdaBoost cycles over non-optimal solutions . Later Boosting algorithms provably optimize the margin of the solution by adjusting the constraint value on the dual edge away from zero (see e.g. ). Finally, the ELRP-Boost algorithm optimizes a trade off between relative entropy and the edge . We conjecture that all of these orthogonal direction have generalizations to the tempered case as well and are worth exploring.

These are theoretical directions that, if successful, would contribute to bring more tools to the design of rigorous boosting algorithms. This is important because boosting suffers several impediments, not all of which we have mentioned: for example, to get statistical consistency for AdaBoost, it is known that early stopping is mandatory . More generally, non-Lipschitz losses like the exponential loss seem to be harder to handle compared to Lipschitz losses  (but they yield in general better convergence rates). The validity of the weak learning assumption of boosting can also be discussed, in particular regarding the negative result of  which advocates, beyond just better (ada)boosting, for boosting for _more_ classes of models / architectures . Alongside this direction, we feel that our experiments on noise handling give a preliminary account of the fact that there is no "one \(t\) fits all" case, but a much more in depth analysis is required to elicit / tune a "good" \(t\). This is a crucial issue for noise handling , but as we explain in Section 7, this could bring benefits in much wider contexts as well.

  \(\) &  &  \\ \(t\) & \(0.6\) & \(0.8\) & \(0.9\) & \(1.1\) &  &  &  &  \\ \([\)clipped\(]\) & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\   \(\#\)better & 2 & 3 & 1 & 2 & 1 & 3 & & & 1 & 1 & 1 & 2 & 2 & 1 & & & \\  \(\#\)equivalent & 5 & 5 & 6 & 6 & 7 & 7 & 6 & 7 & 4 & 8 & 8 & 7 & 8 & 9 & 8 & 10 \\  \(\#\)worse & 3 & 2 & 3 & 2 & 2 & & 4 & 3 & 5 & 1 & 1 & 1 & & & 2 & & \\   

Table 2: Outcomes of student paired \(t\)-tests over 10 UCI domains, with training noise \(\{0.0,0.1\}\), for \(t\{0.6,0.8,0.9,1.0,1.1\}\) and with / without clipped models. For each triple (\(\), \(t\), \([\)clipped\(]\)), we give the number of domains for which the corresponding setting of \(t\)-AdaBoost is statistically better than AdaBoost(\(\#\)better), the number for which it is statistically worse (\(\#\)worse) and the number for which we cannot reject the assumption of identical performances. Threshold \(p-\)val = 0.1.