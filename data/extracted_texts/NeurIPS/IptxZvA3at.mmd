# GEO-Bench:

Toward Foundation Models for Earth Monitoring

 Alexandre Lacoste\({}^{*}{}^{1}\)

Hannah Kemer\({}^{4}\)

Bjorn Lutgens\({}^{5}\)

Jeremy Irvin\({}^{3}\)

David Dao\({}^{6}\)

Hamed Alemohammad\({}^{7}\)

Alexandre Drouin\({}^{1,8}\)

Mehmet Gunturkun\({}^{1}\)

Gabriel Huang\({}^{1,9}\)

David Vazquez\({}^{1}\)

Dava Newman\({}^{5}\)

Yoshua Bengio\({}^{8,9}\)

Stefano Ermon\({}^{3}\)

Xiao Xiang Zhu\({}^{2}\)

\({}^{1}\) ServiceNow Research \({}^{2}\) Technical University of Munich \({}^{3}\) Stanford University \({}^{4}\) Arizona State University \({}^{5}\) MIT \({}^{6}\) ETH Zurich \({}^{7}\) Clark University \({}^{8}\) Mila-Quebec \({}^{9}\) University of Montreal

###### Abstract

Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined _foundation models_, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.

## 1 Introduction

Earth monitoring with machine learning-based methods plays an increasing role in climate change mitigation and adaptation as well as climate science . Related applications include methane source detection , forest carbon quantification , extreme weather prediction , and crop monitoring . Across many of these applications, pre-trained models (e.g., a ResNet trained on ImageNet) have been used to increase generalisation performance. Improvement of the pre-trained models has been shown to reduce the need for large labelled datasets in some contexts  and can improve model generalisation outside of the training distribution . Recent studies exploring the scaling of such pre-trained models found that increasing the size of an unsupervised (or weakly supervised) dataset as well as properly scaling the model led to an even greater increase in performance under various metrics .

While the training of such large-scale models is usually reserved for industrial research groups with very large computer clusters, the publication of pre-trained models creates vast opportunities for the entire research and technology community (including communities of domain experts outside of machine learning). These large pre-trained models were recently coined as _foundation models_ as they might serve as foundations for sub-fields of machine learning. Specifically, the publication of large pre-trained models like BERT  and GPT-3  led to a paradigm shift in the field of natural language processing (NLP). This inspired a similar shift in the field of computer vision with the release of models like CLIP  and DINO . While CLIP performs well on various types of vision tasks, it still under-performson Earth monitoring tasks . This is not surprising as it is trained mainly on RGB images taken from a ground perspective at a single point in time.

While there are many similarities between Earth observation datasets and typical ML image datasets, there are also many important differences to consider when designing effective ML models. Earth observation images are taken from an overhead rather than ground perspective, usually from a fixed distance from the Earth's surface (defined by a satellite's orbit). The satellite revisits provide a temporal axis that is sometimes irregular (e.g., a few times per year) or regular (e.g., every five days) with cloud coverage causing spurious occlusions. Images are acquired with sensors containing multiple spectral bands (e.g., thirteen for Sentinel-2), or even with different kinds of sensors, e.g., synthetic aperture radar (SAR), which can penetrate cloud coverage. Moreover, the GPS coordinates and timestamp of each acquisition offer the opportunity to combine data from multiple sources, e.g., weather data, semantic maps, and elevation. This leads to a rich multi-modal signal with potentially missing information that can be inferred from other elements of the signal. There are currently petabytes of accessible satellite datasets containing images of the Earth under various modalities from the present day to as far back as the 1960s. Distilling this large amount of information into pre-trained models of various sizes offers the opportunity to redistribute this information and make it accessible to various labs for increasing the performances on a large range of downstream tasks.

The fundamental goal of these large pre-trained models is to improve generalization performance on downstream tasks. Hence, to support the machine learning community in producing better pre-trained models, it is crucial to provide a benchmark with a wide variety of downstream tasks, covering a range of modalities and dataset shapes that are likely to be encountered in practice. At the moment, existing works on pre-training models from earth observations e.g., [13; 46; 69], evaluate on different sets of downstream tasks, making it impossible to directly compare performance. Moreover, the set of tasks is often narrow in terms of diversity and the statistical methodologies do not adequately report the uncertainties in the evaluation.

The present work aims to fill this void by providing a wide range of tasks across various countries with various modalities of sensors. Also, the transformed versions of the datasets are smaller than their original form, and all results can be replicated on single GPUs. This increases accessibility to research labs with limited resources and reduces overall energy consumption. Our proposed benchmark, GEO-Bench1, is composed of six image classification and six semantic segmentation tasks, which were curated by domain experts to ensure their diversity and relevance toward sustainable development. We expect this contribution to:

* Stimulate and facilitate the development of foundation models for Earth monitoring
* Provide a systematic way of measuring the quality of models for better scientific progress
* Provide insights into which pre-trained models work best
* Potentially reduces negative impacts of foundation models through an open evaluation procedure.

In what follows, we start by discussing sources of data that can serve to train foundation models for earth monitoring (Sec. 2). We then present the details of GEO-Bench (Sec. 3) and how it can be used for the evaluation of foundation models (Sec. 4). Further, we review existing benchmark datasets for earth monitoring and discuss why GEO-Bench is complementary (Sec. 5). Finally, we present an extensive set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down reference points and to gain valuable information on existing pre-trained models (Sec. 6).

## 2 Remote sensing data for self-supervision

The development of foundation models does not typically rely on a specific dataset for the pre-training phase. The choice of data source is part of the design of the model, e.g., a very large corpus of text from the internet  or pairs of text associated with images from the web . As such, we do not provide data for training foundation models with this benchmark. However, for completeness, we outline potential sources of Earth observation data that could be used for pre-training foundation models.

Multispectral images with revisits Satellite data sources such as Sentinel-2 [20; 23] and Landsat 8  provide images in multiple spectral bands with periodic revisits. This yields a four-dimensional array of structured data (longitude, latitude, wavelength, time) which can be used to perform various forms of self-supervision, e.g., predicting adjacent tiles  or contrasting the different seasons for the same region .

Other sensors Synthetic Aperture Radar (SAR) and terrain elevation are also frequently available and can be matched to other sources of data through geolocalisation . Such data are complementary to optical spectral bands and may encourage the model to learn higher-level semantic representations.

Semantic data Through georeferencing, text-based data such as Wikipedia articles can be linked to satellite images . It is also possible to join content from non-image data layers like OpenStreetMap . By predicting or contrasting information from these sources, the model may learn useful and transferable semantic representations.

## 3 GEO-Bench

GEO-Bench is composed of 6 classification tasks and 6 segmentation tasks. Detailed characteristics are presented in Table 1, examples are depicted in Figure 2 and 3, and the spatial coverage on the world map is presented in Figure 8 (supplementary material). In what follows, we describe the procedure for collecting and transforming the datasets.

### Design Principles

GEO-Bench was established by modifying and gathering geospatial datasets, adhering to principles that secure accessibility, usability, and effective model performance assessment across tasks.

Ease of Use A fundamental goal was to create an accessible, simple-to-use benchmark, and a compact dataset assortment with code for loading the data in a consistent schema. A key aim was to harmonize data to reduce the engineering work needed to tailor pre-trained architectures, while maintaining sensor type and resolution diversity.

Sector Experts and Steering Committee To align GEO-Bench with practical use-cases, we assembled a team of six sector experts from fields such as forestry and climate science. A steering committee of respected scientists guides high-level benchmark decisions, assuring relevance and impact.

Diversity of Modalities The objective is to evaluate model adaptability to varied geospatial sensors. Thus, the benchmark encompasses multispectral, SAR, hyperspectral, elevation, and cloud probability modalities, with spatial resolutions from 0.1 to 30 m/pixel.

Diversity of Tasks We ventured beyond image classification, incorporating object detection and semantic segmentation. To maintain _ease of use_, detection and counting tasks were transformed into semantic segmentation. This led to two task sets: six image classification tasks, and six semantic segmentation tasks [25; 38].

Original Train, Validation, and Test Splits Original dataset splits were preserved when available; otherwise, we generated validation and test sets from the train set while ensuring no spatial overlap.

Permissive License Most datasets needed to be adapted from their original form to satisfy the above criteria and be included in the benchmark. Hence, we include only datasets with permissive licenses.

Figure 1: Foundation models encapsulate multimodal data streams through self-supervised training. The trained models can then be fine-tuned for a variety of climate-related remote sensing tasks. Image sources: quantification , detection , generation , counting , segmentation , and multi-class classification .

### Dataset Transformations

To produce a benchmark that complies with the design choices of Section 3.1, we applied the following transformations to each dataset. The procedure that was used to download and transform each dataset is fully documented and open-sourced in the GEO-Bench GitHub repository2.

    &  & \# Classes & Train & Val & Test & \# Bands & RGB res & Sensors & Cite & License \\  m-biggerather & 120 x 120 & 43 & 20000 & 1000 & 1000 & 12 & 10.0 & Sentinel-2 &  & CDLA-P-1.0 \\ m-sq2sat & 32 x 32 & 17 & 19992 & 986 & 986 & 18 & 10.0 & Sentinel-2 &  & CC-BY-4.0 \\ m-brick-klin & 64 x 64 & 2 & 15063 & 999 & 999 & 13 & 10.0 & Sentinel-2 &  & CC-BY-S4.0 \\ m-foresnet & 332 x 332 & 12 & 6464 & 989 & 993 & 6 & 15.0 & Landsat-8 &  & CC-BY-4.0 \\ m-cursat & 64 x 64 & 10 & 2000 & 1000 & 1000 & 13 & 10.0 & Sentinel-2 &  & MIT \\ m-pq4ger & 320 x 320 & 2 & 11814 & 999 & 999 & 3 & 0.1 & RGB &  & MIT \\   &  & & & & & & & & & & \\  Name & Image Size & \# Classes & Train & Val & Test & \# Bands & RGB res & Sensors & Cite & License \\  m-pq4-seq & 320 x 320 & 2 & 3000 & 403 & 403 & 3 & 0.1 & RGB &  & MIT \\ m-chesupake-landcover & 256 x 256 & 7 & 3000 & 1000 & 1000 & 4 & 1.0 & RGB &  & CDLA-P-1.0 \\ m-cashve-plantation & 256 x 256 & 7 & 1150 & 400 & 50 & 13 & 10.0 & Sentinel-2 &  & CC-BY-4.0 \\ m-S\(\)-crop-type & 256 x 256 & 10 & 3000 & 1000 & 1000 & 13 & 10.0 & Sentinel-2 &  & CC-BY-4.0 \\ m-m-cattle & 500 x 500 & 2 & 524 & 66 & 65 & 3 & 0.1 & RGB &  & CC-BY-4.0 \\ m-NeonTree & 400 x 400 & 2 & 270 & 94 & 93 & 5 & 0.1 & RGB &  & CC 1.0 \\  & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\   

Table 1: **GEO-Bench: Characteristics of datasets in the benchmark. Since datasets are _modified_, we prepend their name with “m-” to distinguish them from the original dataset.**

Figure 3: Representative samples of the **segmentation benchmark**.

Figure 2: Representative samples of the **classification benchmark**.

**Subsampling Large Datasets** To be more representative of typical downstream tasks, where data is usually scarce, datasets larger than \(20000\) samples were randomly subsampled. Avoiding large downstream tasks also comes with other benefits:

* In Appendix A, we show that larger downstream datasets can decrease the ability to discriminate between two models that are similar in performance.
* Downstream tasks with very large training sets will not usually benefit from pre-training3. Hence they are less useful for our evaluation purpose. * A smaller benchmark is faster to download, yields results quicker and requires less energy for computation.
* We can increase the variety of experiments and the number of seeds to improve the knowledge gained from experiments.

**Removing Class Imbalance** We randomly subsampled large classes to have near-uniform class sizes across datasets. This was done to prevent users of the benchmark from increasing their score by using clever class imbalance techniques instead of making progress on better pre-trained models. While good performance on highly imbalanced (long tail of classes) datasets would be a desired property of a pre-trained model, we have not found a good dataset containing a large number of classes.

## 4 Using The Benchmark

**Fine Tuning** In the self-supervised learning literature, it is common to use the pre-trained model to encode a fixed representation of each image in the dataset and learn to classify images based on this representation . While this works relatively well, this method highly depends on the pre-training task as it may not learn to encode information that is important for the downstream task [65; 53]. In practice, fine-tuning the pre-trained model often mitigates this issue and is known to frequently yield a much higher generalization performance than a model trained from random weights [46; 11]. Since this is more representative of practical usage, we encourage users of the benchmark to report the results of fine-tuned models. On the other hand, we do not discourage users from also reporting results with fixed backbones (pre-trained weights) as this can provide valuable information about the pre-trained model. In all cases, we ask users to report their fine-tuning methodology with enough details for reproducibility.

**Hyperparameter Tuning** Deep learning algorithms often require the adjustment of hyperparameters, especially when an architecture is fine-tuned on a small dataset. For this reason, we recommend adjusting hyperparameters, but within a maximum budget of 16 trials per task4. Early stopping based on validation metrics is also recommended.

**Data Augmentation** Data augmentation plays a crucial role in the training of deep learning models, especially with small training datasets. Hence, we consider it to be part of the fine-tuning process. As a guideline, we propose limiting the augmentations to \(90^{}\) rotations and vertical and horizontal flips5. On the other hand, we also encourage users to study what are the best data augmentations for remote sensing as this could lead to useful findings for practitioners and the benchmark is well-suited for evaluating such findings.

**Toolbox** To facilitate the usage of the benchmark, we provide a collection of tools for various parts of the experimental pipeline as part of the open-sourced codebase6. This includes tools for loading datasets and visualising results. We also provide tools based on PyTorch-Lightning  to facilitate model training.

### Reporting Results

For reliable and comparable results across different publications, we recommend that users follow this procedure to report results. The aim is to report results on individual tasks as well as aggregated across all tasks, with reliable confidence intervals (inspired by ). Code is provided to generate figures based on raw results.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_EMPTY:7]

### Protocol

For each model, we replaced the last layer with a randomly initialised layer of the appropriate shape for the task at hand. We use different learning rates for the last layer (which starts from random weights) and for the backbone (which starts from pre-trained weights). The best learning rates were selected using the highest accuracy or Intersection over Union (IoU) on the validation set over 16 trials 8. After choosing the hyperparameters, we repeated the training for 10 seeds. To minimize overfitting, we selected the best time step using accuracy (or IoU) on the validation set and we reported the test metrics at the chosen time step. We use AdamW  to train convolution architectures and SGD to train transformer architectures.

### Classification

#### 6.2.1 Baselines Naming Schema

Each baseline name starts with the corresponding architecture: **ResNet18 and ResNet50:** standard ResNet architectures ; **ConvNeXt-B:** the base architecture of ConvNeXt ; **ViT-T and ViT-S:** ViT architectures  of size tiny and small respectively; **SwinV2-T:** a SwinV2-tiny architecture ;

Then, keywords provide details about the training procedure: **SeCo:** a ResNet50 model trained on Sentinel 2 data with temporal contrastive loss across seasons ; **MoCo-S2 and DINO-S2:** model trained with self-supervision on Sentinel data  (RGB and Multispectral pre-trained weights); **Rnd:** weights are randomly initialised; **timm:** pre-trained weights are obtained from the timm library, usually from training on ImageNet; **+R-Multi:** we manually augment an RGB architecture by randomly initialising the weights of the missing channels in the 1st layer; **multi:** the pre-trained model has multispectral channels.

#### 6.2.2 Comparing Baselines on RGB only

In Figure 4, we report bootstrapped IQM of the normalized accuracy (Sec 4.1) for the six datasets of the classification benchmark, as well as aggregated results9. In this first experiment, all models can only see the RGB channels.

These results offer valuable information across 10 common baselines in the literature. We denote the outstanding performance of ConvNext and SwinV2 compared to other models. It is by a large margin the best models in aggregated results and almost systematically outperforms all models on all datasets. We can also observe the large difference between Scratch ResNet18 and ResNet18 on all datasets. This highlights the importance of using a pre-trained model. Also, perhaps disappointingly, the existing model pre-trained on remote sensing data does not exhibit any improvement compared to their timm pre-trained weights, i.e., ResNet18-MoCo-S2, ResNet50-MoCo-S2, and ResNet50-SeCo-S2 are all comparable to ResNet18 on the aggregated performance. On the other hand, in Section 6.2.4, we see that ResNet50-MoCo-S2-multi can leverage multispectral data to slightly surpass ResNet50-timm.

Another insight that can be gained from these results is how useful a dataset is at discriminating baselines, i.e., a dataset where most baselines perform equally would have limited utility in our benchmark. To this end, we had to discard GeoLiteClef 2022  as all models were performing equally badly10. m-eurosat also offers limited discriminativity as most models obtain very high accuracy (see Figure 9). To make this dataset harder, we subsample down to 2000 training samples. We can now see that smaller models tend to perform better on this dataset, but the discriminativity remains fairly low.

#### 6.2.3 Accuracy vs training set size

As part of the benchmark, we also provide official subsets of the training sets with train ratios of (0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1) 11.

Figure 5 depicts a different perspective on the models. First, we can observe the noise due to the hyperparameter selection process that is not accounted for by repeating 10 seeds with fixed hyperparameters. Also, we see that ConvNeXt often becomes better than SwinV2 as the training set decreases. This coincides with the common observations that transformer architectures tend to be more data-hungry, but also tend to outperform convolution architectures in the high data regime . We note also, that ConvNeXt-B-timm only requires 2% of the training set to obtain aggregated performances comparable to that of ResNet18-Rnd. This impressive factor of 50x on data efficiency highlights the importance of developing new architectures and new pre-training methods. Finally, we can observe an increase in the discriminativity of the datasets as the training set decreases, specifically for m-eurosat, when the task becomes more difficult, the strong baselines stand out even more. The discriminativity of datasets is further studied in Section A.5.

#### 6.2.4 Leveraging Multispectral Information

We now study the effect of leveraging multispectral information during the pre-training phase and during the fine-tuning phase. We do so by fixing the backbone to either ResNet50 (Fig. 6) or ViT-S (Fig. 7) and exploring various weight initialisation schema. Since we could only find pre-trained models for Sentinel-2, we limit this experiment to the four datasets satisfying this criterion.

We found that using a model pre-trained on RGB-only (timm pre-trained) and augmenting the architecture by randomly initialising the weights of the missing channels in the first layer (+RMulti) does not lead to systematic improvement. Moreover, the fine-tuning time is largely extended since we have to wait until the newly initialised weights on the first layer fully converge. On the other hand, the ResNet50 pre-trained on Sentinel-2 using DINO or MoCo  leads to a modest performance increase on average. When looking at ViT-S (Fig. 7), incorporating multi-spectral only leads to a systematic performance decrease.

### Segmentation

We defer experiments on the Segmentation benchmark to Appendix A.3, where we provide experiments on six baselines (ResNet18, ResNet50, ResNet101) \(\) (U-Net, DeepLabV3) with pre-trained weights provided by the timm library. While ResNet101-DeepLabV3 performs best in aggregate, it still underperforms on some datasets.

### Resource Usage

See Appendix A.6 for detailed resource usage of each algorithm evaluated in this section. We report the number of parameters, memory usage, the time required for a forward pass, and the convergence time for fine-tuning on downstream tasks. While memory footprint can increase by a factor of 4x for a model like SwinV2 and ConvNeXt-B compared to ResNet50, their forward pass is only twice as slow.

## 7 Conclusion

We developed a new benchmark for evaluating pre-trained models on remote sensing downstream tasks. This involves adapting a variety of remote sensing datasets to a more conventional machine learning pipeline and providing code for fine-tuning and evaluating individual tasks. We expect that this benchmark will stimulate the development of new foundation models that could lead to better generalization on a variety of earth monitoring downstream tasks and could open up opportunities for new applications.

LimitationsOur benchmark does not extensively evaluate all desired features of a pre-trained model for earth monitoring. For example, it does not evaluate its ability to fine-tune temporal data nor perform fusion with other types of data such as text or weather. The spatial coverage of the benchmark covers most continents and improves coverage over individual datasets. However, the spatial coverage could still be largely improved to include a much wider range of countries and biomes. Finally, as pre-trained models become stronger, they will get closer to the theoretical limit of generalization performance, i.e. approaching the aleatoric uncertainty of the dataset. Under such a regime, we expect a bigger overlap between error bars when comparing 2 different models.