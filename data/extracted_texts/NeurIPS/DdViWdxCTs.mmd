# Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation

Fei Zhang\({}^{1}\)   Tianfei Zhou\({}^{3}\)   Boyang Li\({}^{4}\)   Hao He\({}^{5}\)   Chaofan Ma\({}^{1}\)   Tianjiao Zhang\({}^{1}\)   Jiangchao Yao\({}^{1,2}\)   Ya Zhang\({}^{1,2}\)   Yanfeng Wang\({}^{1,2}\)

\({}^{1}\)CMIC, Shanghai Jiao Tong University  \({}^{2}\)Shanghai AI Laboratory

\({}^{3}\)Beijing Institute of Technology  \({}^{4}\)National University of Defense Technology  \({}^{5}\)CUHK

ferenas@sjtu.edu.cn, ztfei.debug@gmail.com

{chaofanma, xiaoeyuztj, Sunarker, ya_zhang, wangyanfeng622@}@sjtu.edu.cn

###### Abstract

This paper studies the problem of _weakly open-vocabulary semantic segmentation_ (WOVSS), which learns to segment objects of arbitrary classes using mere image-text pairs. Existing works turn to enhance the vanilla vision transformer by introducing explicit grouping recognition, i.e., employing several group tokens/centroids to cluster the image tokens and perform the group-text alignment. Nevertheless, these methods suffer from a _granularity inconsistency_ regarding the usage of group tokens, which are aligned in the all-to-one _v.s._ one-to-one manners during the training and inference phases, respectively. We argue that this discrepancy arises from the lack of elaborate supervision for each group token. To bridge this granularity gap, this paper explores explicit supervision for the group tokens from the _prototypical knowledge_. To this end, this paper proposes the _non-learnable prototypical regularization_ (NPR) where non-learnable prototypes are estimated from source features to serve as supervision and enable contrastive matching of the group tokens. This regularization encourages the group tokens to segment objects with less redundancy and capture more comprehensive semantic regions, leading to increased _compactness_ and _richness_. Based on NPR, we propose the _prototypical guidance segmentation network_ (PGSeg) that incorporates multi-modal regularization by leveraging prototypical sources from both images and texts at different levels, progressively enhancing the segmentation capability with diverse prototypical patterns. Experimental results show that our proposed method achieves state-of-the-art performance on several benchmark datasets. The source code is available at https://github.com/Ferenas/PGSeg.

+
Footnote †: dagger\) denotes the corresponding author

## 1 Introduction

Recently, the remarkable success of _vision-language pre-training_ (VLP) methods  has invigorated the field of semantic segmentation, one of the prominent computer vision tasks. This advancement has led to the emergence of an intriguing task known as _open-vocabulary semantic segmentation_ (OVSS), which aims to segment object pixels belonging to arbitrary classes beyond pre-defined categories. To address this challenge, most works  have turned to a large quantity of image-text pairs equipped with precisely-annotated masks. To liberate OVSS from exhaustive pixel-level ground truth, we in this paper excavate _weakly open-vocabulary semantic segmentation_ (WOVSS), a more arduous setting that achieves OVSS with mere image-text pairs.

To learn from vast image-text data, _vision transformer_ (ViT)  has exhibited impressive advances in acquiring powerful visual representations from the text [39; 29]. However, the vanilla ViT lacks an explicit grouping component, rendering it inadequate for achieving comparable fine-grained segmentation with only text supervision. To improve ViT with the potential for segmenting ability, most WOVSS approaches [33; 50; 41; 51; 35] proposed to cluster the patch-level visual features into several learnable group tokens/centroids, and process the group-text alignment to generate the corresponding categories. Though effective, these methods inevitably are plagued with a _granularity inconsistency_ concerning the group tokens. During the training stage, these learnable group tokens are averaged to facilitate all-to-one group-text alignment, while a one-to-one alignment strategy is employed during zero-shot inference (please refer to Figure 2 for more details). This inconsistency arises due to the weak supervision inherent in WOVSS, otherwise, they could be regularized with, e.g. pixel-level ground truth, to perform promising segmentation results as normal OVSS methods do [9; 28]. To break this ill-regularized learning pattern, this paper aims to bridge the granularity gap by exploring explicit supervision for the group tokens, remedying the flawed supervision in WOVSS.

Before delving into the proper guidance for group tokens, let us revisit an interesting question: _What constitutes a good cluster?_ Such a question drives us to put forward two properties that a reliable group centroid should possess. 1) _Compactness_ ensures that a cluster centroid and its clustered items are closely located in the feature space, forming a compact cluster with minimal noise and redundancy [31; 24; 30]. 2) _Richness_ refers to the capability of a centroid to capture diverse and accurate patterns, thereby enhancing zero-shot generalization capability [3; 4; 56]. These two properties motivate us to find the supervision by exploiting the _prototypical knowledge_[4; 31; 30] from an _expectation-maximization_ (EM) estimated data density. To this end, we propose the _non-learnable prototypical regularization_ (NPR) that adopts the _Gaussian mixture models_ (GMM) , one of the soft clustering models, to generate the supervision from the source features for each group token. Specifically, we treat the learned Gaussian distributions from the source features as prototypes, which are then used to align with the group tokens in a contrastive manner. Notably, each non-learnable prototype (uninvolved in gradient backpropagation) is able to regularize the corresponding group token, enabling it to segment compactly and richly. As shown in Figure 1, the group tokens could benefit from this prototypical supervision to segment the objects with less redundancy and more accurate semantic patterns, effectively alleviating the under- and over-segmentation problems.

To instantiate the prototypical patterns in NPR, this paper introduces a novel investigation into using multi-modal information as the source features. The low-level image features, with detailed texture information, could be an intuitive choice to refine the segmentation results [44; 49; 1]. Beyond such simple uni-modality, we further mine out the prototypes from the text to regularize the group token with textual information. Hence we propose two strategies, namely _image-level_ NPR (I-NPR) and _text-level_ NPR (T-NPR), to provide multi-modality regularization to the group tokens. Based on this, we propose the _prototypical guidance segmentation network_ (PGSeg), a hierarchical segmentation model that incorporates I-NPR and T-NPR into the group tokens at different levels, progressively improving the segmenting ability of the group tokens. Overall, we make the following contributions:

Figure 1: Illustration of our motivation. Our method explores the _prototypical knowledge_ to provide explicit supervision for the group tokens, which improves the segmentation results with the increased _richness_ and _compactness_. The former improves the feature representation of group tokens, resulting in enlarged semantic regions, and the latter reduces cluster redundancy and noise.

* We propose NPR that explores and exploits the _prototypical knowledge_ to serve as a valid supervision of the group tokens in segmenting objects. This explicit regularization is encouraged to bring compact and rich feature representation to the group tokens.
* We propose PGSeg, a simple yet effective segmentation architecture that extracts the _prototypical knowledge_ from both the image and text to regularize the group tokens at different levels, progressively guiding the group tokens to segment in an explicit way.
* Extensive results on several benchmarks demonstrate the superiority and effectiveness of our method. Particularly, our method yields new state-of-the-art performance by reaching **53.2**% mIoU and **28.7**% mIoU on PASCAL VOC12  and COCO , respectively. It is worth highlighting that our PGSeg model, trained solely on the CC12M dataset , surpasses some state-of-the-art methods utilizing large foundation models such as CLIP  and BERT  by up to **14.5%** and **5.2%** in terms of mIoU on PASCAL VOC12 and COCO, respectively.

## 2 Related Work

**Weakly Open-Vocabulary Semantic Segmentation.** Most existing works addressing WOVSS can be categorized into two groups based on whether CLIP  or Diffusion Models  is employed as the fundamental model. The first category focuses on extracting coarse localization features from CLIP or Stable Diffusion Models and subsequently refining them to achieve fine-grained segmentation results [55; 6; 36]. The second category of approaches, distinct from those focused on CLIP, centers around enhancing the plain ViT by incorporating grouping recognition, resulting in a foundational segmentation model [33; 50; 41; 51]. In these methods, several learnable group tokens/centroids are introduced to extract visual concepts from the image features.  proposed GroupViT that assigns these tokens to input patch tokens, enabling a learnable clustering process during training.  also presented a grouping-based approach, and introduced a reconstruction loss and a superpixel-based regularization loss to improve the inner clustering results. Our work aligns with the second category of approaches. Note that the setting of WOVSS is extremely similar to _weakly supervised semantic segmentation_ (WSSS), where a segmentation model is obtained with simply image-level labels. Most works addressing WSSS aim to use the low-level of the image information to iteratively refine the segmentation mask [44; 1; 54], requiring massive additional training or inference stages on the target dataset. Therefore, this paper explores an end-to-end mechanism that efficiently incorporates low-level information on the segmentation mask.

**Prototypes for Deep Representation Learning.** The prototypes typically refer to the centroids from conventional clustering methods . Based on the _expectation-maximization_ (EM) algorithm, prototypes are learned by estimating the data features through a mixture of prior distributions. As a result, prototypes are often regarded as "non-learnable" since they deviate from the usual gradient-based learning in deep neural networks [31; 56]. The inclusion of prototypical patterns has been extensively explored to enhance feature representation learning in _contrastive learning_ (CL) [3; 4; 5; 31; 57], which aims to match the feature embeddings of a pair of aligned samples. The success of these approaches highlights two important benefits that prototypes bring to feature alignment. The first goes to the _compactness_[31; 30; 24], where they find that the prototypes could reformulate features into a more compact representation, reducing redundancy and noise in feature alignment. This leads to more reliable feature representations. Another benefit is to enhance the _richness_ of the feature representation by capturing more learning patterns. CL often suffers from the _dimensional collapse_, where embedding vectors occupy a lower-dimensional subspace than their original dimension, resulting in limited diversity in feature representation. To address it, a line of work has leveraged the prototypes to serve as a constraint on the feature alignment, which is validated to effectively enrich the feature representation [3; 4; 5; 53]. This work explores the use of _prototypical knowledge_ with the expectation of providing the aforementioned advantages to segmentation clusters.

## 3 Rethinking the Semantic Grouping Mechanism in WOVSS

To effectively tackle WOVSS, recent works [50; 41; 35; 51] have placed significant emphasis on incorporating explicit grouping recognition into the plain model. To this end, these methods developed a _semantic grouping mechanism_ (SGM) based on ViT . Formally, given \(m\) input patch tokens \(=[_{1},_{2},..,_{m}]^{m d}\) and extra \(q\) learnable group tokens\([_{1},_{2},...,_{q}]^{q d}\), where \(d\) is the dimension of data and \(q<m\). SGM clusters \(\) and outputs new clustered tokens \(}^{q d}\) by \(}=(()()^{ })()+\), where \(:^{q d}^{q d},():^{m d}^{m  d}\) represent the _Query_, _Key_ (_Value_) mapping function. Figure 2 clearly demonstrates this cross-attention-based clustering process. Here each patch token is assigned to a corresponding group token by the Straight-Through Gumbel-Softmax, making this process end-to-end trainable. We formulate this patch-group assignment as \(=()()^{}^{q m}\). By plugging the SGM into ViT, the vanilla image encoder could be empowered with potential segmenting ability. However, it could be observed that this mechanism presents a _granularity inconsistency_ for the group tokens between the training and inference stages (as shown in Figure 2). More specifically, during the training phase, all group tokens are globally averaged to match the corresponding text embedding for the final group-text alignment, while during the inference phase, each group token necessitates a separate comparison with each class embedding in order to acquire the semantic label, which is then used to annotate the corresponding image regions based on the patch-group assignment. Consequently, the group tokens used in the one-to-one group-text alignment do not receive explicit supervision as they are subjected to all-to-one text-based regularization during the training stage. This discrepancy in supervision may contribute to the performance gap observed between OVSS and WOVSS. In OVSS, each learnable group token can be treated as a query embedding, generating a dense embedding for object mask prediction. This dense embedding can be further regularized by the patterns extracted from pixel-level ground truth annotations [9; 28; 58]. Therefore, such granularity discrepancy essentially is derived from the weak supervision of WOVSS. Despite the challenges posed, we are motivated to explore customized regularization techniques for each group token, aiming to compensate for the absence of pixel-level annotations. By explicitly addressing the granularity gap, we aim to enhance the segmentation performance in WOVSS.

## 4 Methods

### Exploring Prototypical Knowledge for Explicit Supervision

To find explicit and reliable supervision, we turn to the _prototypical knowledge_ to form a regularized basis that can bring certain benefits to the group token in segmentation. _Gaussian mixture models_ (GMM)  has been experimentally validated to form a basis that could reduce the redundancy of the features [31; 24]. Inspired by this, we propose the _non-learnable prototypical regularization_ (NPR) that uses GMM to extract the prototypes from the prototypical source (like a way of data mining), and then aligns such prototypes with the group centroids in a contrastive way.

**Prototype Generation.** The first stage of NPR is to generate the supervision by GMM. GMM is based on a mixture of Gaussian distributions, where each component of the mixture corresponds to a cluster in the data. Formally, given the prototypical source features \(=[_{1},...,_{m}]^{m d}\), and extra \(q\) randomly-initialized prototypes \(=[_{1},...,_{q}]^{q d}\), where \(m\) and \(d\) represents the number and the dimension of the prototypical source. In this way, the distribution of \(\) could be expressed as \(p()=_{i=1}^{q}_{i}(|_{i},_{i})\), where \(_{i}^{1}\), \(_{i}^{d}\) and the \(_{i}^{d d}\) are the weight, mean and covariance of the \(i\)-th Gaussian component. Here we take the means as the prototypes. To work out (\(,,\)), the log-likelihood of \(p()\) is maximized through the _expectation-maximization_ (EM) algorithm, which optimizes the model, until convergency is reached, by alternating between an _expectation_ (E) step and a _maximization_ (M) step. In the E step, the probability of \(j\)-th source feature

Figure 2: The _granularity inconsistency_ of SGM.

\(_{j}\) belonging to the \(i\)-th Gaussian prototype could be expressed as

\[y_{ij}=(_{j}|_{i})}{_{i=1}^{q}( _{j}|_{i})}=_{i}_{j}^{})}{_{i=1}^{q}( _{i}_{j}^{})},i\{0,...,q\},j\{0,...,m\},\] (1)

where \(:^{1 d}^{1}\) denotes a kernel function. For simplicity, we set \(\) as the identity matrix \(\) and leave out \(\). We experimentally observe that the different choices of \(\) negligible effects on the final results, so we simplify the Gaussian Kernel \((-\|-\|^{2}/2^{2})\) to the exponential inner dot \((^{})\). Based on the estimated \(y_{ij}\), the prototypes \(\) in the M step could be updated as

\[_{i}=^{m}y_{ij}_{j}}{_{j=1}^{m}y_{ij}}.\] (2)

After alternatively repeating the E step and \(\) step, a bunch of compact prototypes representing the prototypical information could be obtained to supervise the group tokens. Note that we could reconstruct the prototypical source \(\) by \(^{}=^{}^{m d},}=[y_{ij}]_{q m}\).

**Prototype Supervision.** The second stage is to regularize \(\) with the updated \(\). Based on the cosine similarity between \(\) and \(\), we first perform Hungarian matching  to ensure that each centroid \(\) has a corresponding target prototype \(\). Denote the matched prototypes as \(^{}=[^{}_{1},...,^{}_{q}] ^{q d}\). Then, we combine the matched pair of \((,^{})\) as the positive samples, and propose the _Prototypical Guidance_ (PG) loss \(_{}\) to regularize the group centroids in a contrastive way:

\[_{}(,^{})=-_{i=1}^ {q}((_{i},^{}_{i})/)}{ _{j=1}^{q}((_{i},^{}_{j})/)}+ (^{}_{i},_{i})/)}{_{j= 1}^{q}((^{}_{i},_{j})/)}),\] (3)

where \(=0.1\) is the temperature hyper-parameter, and \((,)=^{}}{\|\|\|\|}\) calculates the cosine similarity between two vectors. Based on the PG loss, we introduce a simple _hard rejecting strategy_ (HRS) that only considers the positive pairs whose similarity is beyond a fixed threshold \(\). We claim that one group centroid could be wrongly guided by the matched prototype once they have a significant difference, which will be discussed in Section 5.3. Besides, we here assume that the number of prototypes and group centroids is the same, and we will also show the case where the number of them is different in Section 5.3 (only the matched pairs are considered to calculate \(_{}\)).

**Compactness & Richness.** The essence of NPR is to regulate each group centroid with a normalized prototype from a prior distribution, yielding two essential benefits as discussed 2. The first one is the _compactness_, which helps refine the clustered results by reducing noise and redundancy . The second one goes to the _richness_ that empowers the group tokens with rich feature representation by relieving the _dimensional collapse_ through the application of normalized regularization , capturing more accurate patterns as possible. In all, we believe that NPR could augment the segmenting ability of the group tokens with these two benefits, which will be validated in Section 5.

**Complexity Analysis.** The EM algorithm is the key part in NRP (Prototype Generation in Algorithm 1). It is crucial to carefully consider its time complexity regarding iterative learning. However, through a simplified implementation, we demonstrate that the time complexity of prototype generation in NPR is \((q m d)\) for a single sample. This complexity is reasonably acceptable for implementation purposes. The actual computational performance will be demonstrated in Section 5.3.

### Exploiting Prototypical Guidance for WOVSS

In this section, we introduce our proposed _prototypical guidance segmentation network_ (PGSeg) in detail, which incorporates the proposed NPR into SGM to address WOVSS.

**Main Architecture.** As shown in Figure 3, the overall framework of PGSeg is mainly comprised of a text encoder and an image encoder. The transformer-based text encoder is used to output the text embedding, denoted as \(^{t}^{n c}\), where \(n\) denotes the sample batch size and \(c\) denotes the feature dimension. For the image encoder, we adopt a ViT-based architecture as the image encoder. To instill the image encoder with the segmenting ability, we propose PG Unit, which is a plug-and-play module to perform grouping based on SGM. Intuitively, a multitude of PG Units, along with several transformer layers, can be sequentially connected to perform hierarchical grouping during the forward learning process. The image embedding \(^{i}^{n c}\), as the output of the image encoder, is generated by average-pooling and mapping the output tokens in the final PG Unit. Based on the architecture of PGSeg, we assume that the image encoder could be split into \(L\) levels if \(L\) PG Units are inserted. Formally, we denote the input tokens in the \(l\)-th level as \(_{l}^{n m_{l} d}\), and \(q_{l}\) learnable group tokens as \(_{l}^{n q_{l} d}\), where \(m_{l}\) (\(q_{l}\)) denotes the number of input patch (group) tokens in level \(l\). Likewise, denote the output tokens in the \(l\)-th level as \(}_{l}^{n q_{l} d}\). Intuitively, we hold that \(_{l+1}=}_{l},l\{1,...,L\}\) due to the sequential connection among the PG Units, namely the output tokens in the previous level serve as the input patch tokens in the next level.

**PG Unit.** To instantiate NPR, we propose PG Unit that mines out the multi-modal prototypical source from image-text representations. In this way, we propose two NPR-based strategies based on the type of prototypical pattern, namely _image-level NPR_ (I-NPR) and _text-level NPR_ (T-NPR). For I-NPR, we adopt the input tokens placed before the transformer layers as the image-level prototypical pattern, which are denoted as \(_{l}^{n m_{l} d}\). Based on extra non-learnable prototypes \(_{l}^{n q_{l} d}\), we follow the Algorithm 1 and regularize \(_{l}\) with \(_{l}\) and \(_{l}\). Besides, we further reform the input tokens by \(_{l}=_{l}+_{l}^{}\), where \(_{l}^{}^{n m_{l} d}\) are the reconstructed image-level source, as the input tokens of SGM to enhance the robustness of the model learning .

For T-NPR, we turn to the text embedding \(^{t}^{n c 1}\) as the text-level prototypical pattern to improve the group tokens \(_{l}\) in capturing semantic information. Specifically, we introduce additional text prototypes \(_{l}^{n q_{l} 1}\) and update them with \(^{t}\). Subsequently, we regular \((_{l})\) with the updated \(_{l}\), where \(:^{n q_{l} d}^{n q _{l} 1}\) averages the \(_{l}\) along dimension \(d\). Essentially, T-NPR aligns the score of each group token with the center value clustered from dimension \(d\) in \(^{l}\).

**Training Loss.** Based on the proposed I-NPR and T-NPR in PG Unit, the overall training loss is

\[_{}=_{}(^{t},^{i})+ _{l=1}^{L}_{j=1}^{n}_{} (_{l}^{j},_{l}^{j})+_{}( (_{l}^{j}),_{l}^{j}),\] (4)

where \(_{}\) is the symmetric image-text contrastive loss in , \(\) and \(\) are the hyper-parameters to balance the loss. Here we empirically set \(=0.1\) and \(=0.01\).

Figure 3: The overall framework of our proposed PGSeg. PGSeg consists of an image encoder and a text encoder. Through sequential connections facilitated by the PG Unit and several transformer layers, the image encoder is organized into multiple hierarchical levels (highlighted in orange), enabling the progressive grouping of the input patch tokens. Best viewed in color.

**Momentum update of Prototype.** It is critical to set appropriate initialization of \(\) to guarantee robust convergence . To this end, we leverage the _exponential moving average_ (EMA) strategy to globally update the initial prototypes after each training round: \(_{}=_{}+(1-)_{i=1 }^{n}_{T}^{i}\), where \(_{T}^{i}\) denotes the updated prototypes at the final \(T\)-th iteration in NPR, and \(_{}_{}\) is the initial prototypes for the next/current training round. Empirically we set \(=0.9\).

## 5 Experiments

### Implementation Details

**Model Architecture.** Following conventions in [50; 41; 35], we adopt ViT-S  as the visual backbone, which consists of 12 transformer layers with a hidden dimension of 384. The input image size is 224\(\)224 and the patch size is 16\(\)16. Based on the experimental performance in [50; 41], we set \(L=2\) for PGSeg, which means two individual PG Units are incorporated into the ViT module at different places. The number of learnable group tokens is 64 in level 1 and 8 in level 2, namely \(q_{1}=64,q_{2}=8\). Two PG Units are inserted after the \(6^{th}\) and \(9^{th}\) transformer layers, respectively. The transformer-based text encoder is the same as , with the hidden dimension of \(256\).

**Datasets and Evaluation Metric.** Following [50; 41; 35; 51], we use CC12M  and RedCaps  as the training sets, and each of them contains 12 million image-text pairs. For the downstream evaluation datasets, we select 5 benchmarks: PASCAL VOC12 (20 foreground classes) , PASCAL Context (59 foreground classes) , COCO Object (80 foreground classes) , ImageNet-S (919 foreground classes)  and LVIS  (1203 foreground classes). All of them contain 1 extra background class. We use the mean Intersection-over-Union (mIoU) as the evaluation metric.

**Training and Inference Settings.** During the training stage, we use Adam  optimizer with a weight decay of 0.05. We set the batch size as 4096, and use the cosine learning strategy with an initial learning rate of 1.6e\({}^{-3}\). We train the PGSeg for 40 epochs with 5 epochs of linear warm-up. As the generated features are unreliable in early epochs, we set \(==0\) at the first 30 epochs. For the selecting threshold \(\) of HRS in NPR, we experimentally set it to 0.1. The whole training process is implemented on 4 A100 GPUs, each with 80 GB of memory. During the inference, where no additional training is involved, we obtain the patch-group assignment with the cross-attention maps in two PG Units: \(_{}=_{1}_{2}^{m q_{2}}\). We then generate the foreground semantic mask, and set a fixed threshold as the background score to evaluate the segmentation mask. Note that the templates of the text prompt could significantly impact the segmentation results . To ensure fair comparisons, we follow the same prompt template [50; 39; 41] (_a photo of..._) to generate the text embedding with the foreground class name for the evaluated benchmark datasets.

### Zero-shot Segmentation Performance

**Comparison with Zero-shot Baselines.** Similar to [41; 50], we compare our methods with seven baselines. Four of them train a plain ViT and a text encoder simply by \(_{}\), and generate the segmentation mask by using different pixel-grouping methods on the image features.  find that increased [CLS] tokens in ViT serve as meaningful centers for perceptual grouping. Inspired by this, we additionally build a baseline VIT-8S to serve as a more potent parametric baseline. It trains a plain ViT-S but with increasing the \([]\) token amount into 8, and then aligns the averaged embedding of the final 8 \([]\) tokens with the text embedding in a contrastive way. ViT-8S generates the assignment by summing the cross-attention maps (\([]\) to patch) in each transformer layer. Table 1 shows the performance of these methods on the validation set of PASCAL VOC12, note that all methods here are trained simply with CC12M. It is intuitive that PGSeg outperforms all the compared baselines. Notably, PGSeg adopts the same backbone as GroupViT  and ViewCo , while achieving a **5.8%** and **3.3%** performance improvement over them.

   Architecture & Pixel-Grouping Methods & mIoU (\%) \\  ViT-S & pixel-wise & 20.1 \\ ViT-S & K-means & 25.0 \\ ViT-S & Mean-shift & 20.7 \\ ViT-S & Spectral clustering & 19.7 \\  ViT-8S & ✗ & 38.1 \\ GroupViT  & ✗ & 43.2 \\ ViewCo  & ✗ & 45.7 \\ PGSeg & ✗ & **49.0** \\   

Table 1: Comparison with zero-shot baselines.

**Comparison with SOTA.** Table 3 lists the mIoU of recent state-of-the-art (SOTA) methods on the validation splits of PASCAL VOC12, PASCAL Context, and COCO datasets. Since there is a huge gap among these methods in terms of the datasets and pre-trained model, we particularly report the total data volume and the specific pre-trained model that each method used during the whole training process, with the expectation for a clear and sound fair comparison. As shown in Table 3, our PGSeg achieves the SOTA performance among all the methods with the same data volume. Particularly, our method, with comparably small data volume, even achieves a more stunning performance than the methods with huge foundation models, validating the superiority and effectiveness of our PGSeg.

**Challenging Segmentation.** Here we introduce two challenging benchmarks to explore the potential of PGSeg in real-world segmentation. ImageNet-S, distilled from the ImageNet , contains 919 classes of object-centric images with human-annotated mask labels. LVIS contains 1203 classes that are incorporated with various real-world objects. As shown in Table 2, both PGSeg and GroupViT (CC12M + RedCaps12M) are weak in segmenting these two datasets. Such frustrating performance might be due to inadequate image-text alignment of new vocabularies. It is observed that both their segmentation performance could be boosted through the _True Label Guidance_, considering only the true labels of the evaluating samples. It is also found that PGSeg performs better than GroupViT with the TLG strategy. Therefore, we believe that the segmenting performance of PGSeg could be further enhanced by including more image-text pairs.

### Ablation Studies

In this section, we use the version of PGSeg with CC12M+RedCaps12M to implement all ablation studies on PASCAL VOC12 in detail, which contain the effectiveness of the modules in the PG Unit, some analysis of the prototypes, and the computational performance of PGSeg.

   Methods & ImageNet-S (- / +TLG) & LVIS (- / +TLG) \\  GroupViT & 19.6 / 32.5 & 3.2 / 6.9 \\ PGSeg & 19.7 / **33.8** & 3.4 / **7.2** \\   

Table 2: Challenging segmentation results. TLG refers to the _True Label Guidance_ strategy.

Figure 4: Qualitative results on PASCAL VOC12. Compared with GroupViT, the group tokens in PGSeg could capture the object (marked with a white circle) in a more complete and delicate way.

   Methods & Training Data (volume) & Pre-trained Models & VOC12 & Context & COCO \\  RECO  & CC400M  + ImageNetM (401M) & CLIP  + MOCO  & 25.1 & 19.9 & 15.7 \\ MaskCLIP  & CC400M  (400M) & CLIP  & 29.3 & 21.1 & 15.5 \\ ViL-Seg  & CC12M  (12M) & ✘ & 34.4 & 16.3 & 16.4 \\ MaskCLIP  & CC400M  + _ST_ (400M) & CLIP  & 38.8 & **23.6** & 20.6 \\ GroupViT  & CC12M  (12M) & ✘ & 41.1 & 18.2 & 18.4 \\ OVSegmentor  & CC12M  + ImageNet1M  (13M) & BERT  + DINO  & 44.5 & 18.3 & 19.0 \\  PGSeg & CC12M  (12M) & ✘ & ✘ & **49.0** & 20.6 & **22.9** \\   GroupViT  & CC12M  + RedCaps12M  (24M) & ✘ & 50.8 & 23.6 & 27.5 \\ SegCLIP  & CC403M  + COCO400k  (403-4M) & CLIP  & 52.6 & **24.7** & 26.5 \\ GroupViT  & CC12M  + YFCC14M  (26M) & ✘ & 52.3 & 22.4 & 20.9 \\ ViewC0  & CC12M  + YFCC14M  (26M) & ✘ & 52.4 & 23.0 & 23.5 \\  PGSeg & CC12M  + RedCaps12M  (24M) & ✘ & **53.2** & 23.8 & **28.7** \\   

Table 3: Comparison with SOTA in terms of mIoU(%). All the image encoders here are built on ViT-S . _ST_ means that  uses the _self-training_ strategy on the evaluated datasets to refine the mask. The best results are highlighted in **bold** (underline marks the cases under the same volume).

**Effectiveness of PG Unit.** Table 4 shows the effectiveness of each designed module in the PG Unit. Recall that in Section 4.2 we propose two different NPR-based strategies in the PG Unit, namely I-NPR and T-NPR. As shown in Table 4, both these strategies are effective in enhancing the baseline, delivering **1.75**% and **0.58**% improvements, respectively. We also propose the HRS to further improve the performance of NPR by filtering the group-prototype pairs with a fixed selecting threshold \(\). Consequently, it is observed that proper threshold could lead to the boosting of PGSeg (**+0.41**%), which finally achieves **53.24**% performance together with T-NPR.

**The number of Prototypes.** Recall that in Section 4.1 we mention that the number of prototypes could be different from the group tokens. Table 5 reports the performance of PGSeg with different numbers of prototypes. Note that at the \(1^{st}\) (\(2^{nd}\)) level, the number of group tokens remains constant at 64 (8). Here we exclude the HRS to ensure a comprehensive consideration of all prototypes. We only consider the matched group-prototype pairs as the positive samples, and all other extra groups/prototypes would be considered to form the negative samples. In other words, if the number of group tokens is smaller (larger) than the number of prototypes, the symmetric \(_{}\) would simply calculate the left (right) part accordingly. As depicted in Table 5, it has been observed that the optimal performance is attained when the number of prototypes is equal to the number of group tokens. Moreover, any increase or decrease in the number of prototypes beyond this optimal value tends to negatively impact the segmentation performance to some extent. This reveals that the number of negative samples or positive samples is vital to the performance of prototypical alignment. Clearly, the number of positive sample pairs would decrease if the number of prototypes is less than 64/8, otherwise, the number of negative sample pairs would increase. Therefore, our experimental results on the number of prototypes reach a consistent conclusion with .

**Two benefits of NPR.** Here we aim to investigate the nature of the two benefits, i.e., _compactness_ and _richness_ (mentioned in Section 4.1), to understand the effectiveness of NPR better. To validate the first one, we use SUSE to visualize the clustered 5 input patch tokens of SGM among 50 samples. As shown in Figure 6, it is intuitively found that the patch tokens in PGSeg are more tightly clustered than GroupViT, where most input tokens are comparably scattered. With the help of the compact basis in NPR, the input patch tokens become less noisy in the feature space, which is also supported by the visualized results in Figure 5 and Figure 1. The second benefit aims to enrich the feature representation to capture more accurate semantic patterns by relieving the _dimensional collapse_. As shown in Figure 6, we calculate the mean and variance of 384 dimensions for each \(1^{st}\) level group token (64 in total) based on 300 samples. It is evident that although the means are nearly identical, the dimensional variance of PGSeg is significantly larger than that of GroupViT, indicating a better dimensional representation for each group token with prototypical regularization. Overall, these two explicit benefits are validated to contribute to the enhanced segmenting ability of the group tokens.

Computational Performance.Here we present the floating-point operations (FLOPs) and model parameters for three methods in Table 6 as well. The FLOPs are calculated based on an image size of \(448 448\). It is observed that PGSeg introduces a 3.2G increase in FLOPs compared to GroupViT, but still maintains a lower FLOP count (-3.1G) than ViT-S. Therefore, the computational complexity of PGSeg remains manageable, indicating a reasonable level of computational efficiency.

## 6 Discussion with SAM

Recently, the Segment Anything Model (SAM) , an impressive model for interactive segmentation, has demonstrated significant progress in image segmentation tasks. SAM supports segmenting everything in an arbitrary image, which is a powerful foundation model to address OVSS. SAM is trained on a massive dataset comprising **11 billion** images. In contrast, PGSeg is trained on a smaller dataset consisting of only **24 million** image-text pairs. Intuitively, the data volume of SAM is approximately **460 times** that of PGSeg. Despite their vast data amount, SAM also incorporates over 1 billion accurately annotated masks. Therefore, SAM is clearly better than PGSeg. Though a huge performance gap between our PGSeg and SAM, we would like to present some comparisons between these two models to present the research value of WOVSS. As illustrated in Figure 7, the segmenting groups in SAM provide comprehensive coverage of objects in an extremely fine-grained manner. In comparison, our learnable groups effectively capture entire objects without requiring instance-level recognition. For example, in the \(1^{st}\) column of the image, our yellow group can represent the overall forest background, while SAM can differentiate between individual trees within the forest background. However, it is important to note that our PGSeg model achieves comparable segmentation capabilities for certain intuitive objects, such as umbrellas, ships, babies, etc., with significantly fewer image-text pairs compared to SAM. Although vast attention has been paid to investigating the huge foundation model with vast data collection, given this impressive performance, we believe that WOVSS is a fascinating research topic that merits future investigation.

## 7 Conclusion

The majority of efforts in _weakly open-vocabulary semantic segmentation_ (WOVSS) have focused on implementing explicit grouping recognition while overlooking the customized supervision needed for the inherent clustering of group tokens. This oversight has led to a _granularity inconsistency_ between the training and inference stages. To address this issue, our paper proposed to leverage _prototypical knowledge_ to mine out explicit supervision, which encourages the generation of compact and rich feature representations for the group tokens. Additionally, we introduced multi-modal prototypes derived from image-text information to instantiate this knowledge, resulting in diverse patterns that enhance the segmenting ability of the group tokens. Through quantitative and qualitative experiments, we have demonstrated the effectiveness and superiority of this straightforward concept. In all, our bottom-up concept further validates the potential of prototypes, which exhibit _compactness_ and _richness_, as promising elements for the top-down segmentation methods. Therefore, we believe that it is worth further exploring the full potential of prototypes in more weakly supervised tasks.

   Methods & FLOPs & Params \\  ViT-S & 16.7G & 21.6M \\ GroupViT & 10.4G & 28.7M \\ PGSeg & 13.6G & 35.1M \\   

Table 6: Computational performance.

Figure 7: Class-agnostic segmentation comparison between SAM and PGSeg on LVIS.

Acknowledgement

This work is supported by the National Key R&D Program of China (No. 2022ZD0160703), STCSM (No. 22511106101, No. 22511105700, No. 21DZ1100100), 111 plan (No. BP0719010) and National Natural Science Foundation of China (No. 62306178).