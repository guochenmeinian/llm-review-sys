# Mechanism Design for Collaborative Normal Mean Estimation

Yiding Chen

UW-Madison

ychen695@wisc.edu &Xiaojin Zhu

UW-Madison

jerryzhu@cs.wisc.edu &Kirthevasan Kandasamy

UW-Madison

kandasamy@cs.wisc.edu

###### Abstract

We study collaborative normal mean estimation, where \(m\) strategic agents collect i.i.d samples from a normal distribution \((,^{2})\) at a cost. They all wish to estimate the mean \(\). By sharing data with each other, agents can obtain better estimates while keeping the cost of data collection small. To facilitate this collaboration, we wish to design mechanisms that encourage agents to collect a sufficient amount of data and share it truthfully, so that they are all better off than working alone. In naive mechanisms, such as simply pooling and sharing all the data, an individual agent might find it beneficial to under-collect and/or fabricate data, which can lead to poor social outcomes. We design a novel mechanism that overcomes these challenges via two key techniques: first, when sharing the others' data with an agent, the mechanism corrupts this dataset proportional to how much the data reported by the agent differs from the others; second, we design minimax optimal estimators for the corrupted dataset. Our mechanism, which is Nash incentive compatible and individually rational, achieves a social penalty (sum of all agents' estimation errors and data collection costs) that is at most a factor 2 of the global minimum. When applied to high dimensional (non-Gaussian) distributions with bounded variance, this mechanism retains these three properties, but with slightly weaker results. Finally, in two special cases where we restrict the strategy space of the agents, we design mechanisms that essentially achieve the global minimum.

## 1 Introduction

With the rise in popularity of machine learning, data is becoming an increasingly valuable resource for businesses, scientific organizations, and government institutions. However, data collection is often costly. For instance, to collect data, businesses may need to carry out market research, scientists may need to conduct experiments, and government institutions may need to perform surveys on public services. However, once data has been generated, it can be freely replicated and used by many organizations . Hence, instead of simply collecting and learning from their own data, by sharing data with each other, organizations can mutually reduce their own data collection costs and improve the utility they derive from data . In fact, there are already several platforms to facilitate data sharing among businesses , scientific organizations , and public institutions .

However, simply pooling everyone's data and sharing with each other can lead to free-riding . For instance, if an agent (e.g an organization) sees that other agents are already contributing a large amount of data, then, the cost she incurs to collect her own dataset may not offset the marginal improvement in _her own_ learned model due to diminishing returns of increasing dataset sizes (we describe this rigorously in SS2). Hence, while she benefits from others' data, she has no incentive to collect and contribute data to the pool. A seemingly simple fix to this free-riding problem is to only return the datasets of the others if an agent submits a large enough dataset herself. However, this can be easily manipulated by a strategic agent who submits a large fabricated (fake) dataset without incurring any cost, receives the others' data, and then discards her fabricated dataset when learning. While the agent has benefited by this bad behavior, other agents who may use this fabricated datasetare worse off. Moreover, a naive test by the mechanism to check if the agent has fabricated data can be sidestepped by agents who collect only a small dataset and fabricate a larger dataset using this small dataset (e.g by fitting a model to the small dataset and then sampling from this fitted model).

In this work, we study these challenges in data sharing in one of the most foundational statistical problems, normal mean estimation, where the goal is to estimate the mean \(\) of a normal distribution \((,^{2})\) with known variance \(^{2}\). We wish to design _mechanisms_ for data sharing that satisfy the three fundamental desiderata of mechanism design; _Nash incentive compatibility (NIC):_ agents have incentive to collect a sufficiently large amount of data and share it truthfully provided that all other agents are doing so; _individual rationality (IR):_ agents are better off participating in the mechanism than working on their own; and _efficiency:_ the mechanism leads to outcomes with small estimation error and data collection costs for all agents.

**Contributions:**_(i)_ In SS2, we formalize collaborative normal mean estimation in the presence of strategic agents. _(ii)_ In SS3, we design an NIC and IR mechanism for this problem to prevent free-riding and data fabrication and show that its social penalty, i.e sum of all agents' estimation errors and data collection costs, is at most twice that of the global minimum. _(iii)_ In Appendix E, we study the same mechanism in high dimensional settings and relax the Gaussian assumption to distributions with bounded variance. We show that the mechanism retains its properties, with only a slight weakening of the NIC and efficiency guarantees. _(iv)_ In SS4, we consider two special cases where we impose natural restrictions on the agents' strategy space. We show that it is possible to design mechanisms which essentially achieve the global minimum social penalty in both settings. Next, we will summarize our primary mechanism and the associated theorem in SS3.

### Summary of main results

_Formalism:_ We assume that all agents have a fixed cost for collecting one sample, and define an agent's penalty (negative utility) as the sum of her estimation error and the cost she incurred to collect data. To make the problem well-defined, for the estimation error, we find it necessary to consider the _maximum risk_, i.e maximum expected error over all \(\). A mechanism asks agents to collect data, and then shares the data among the agents in an appropriate manner to achieve the three desiderata. An agent's strategy space consists of three components: how much data she wishes to collect, what she chooses to submit after collecting the data, and how she estimates the mean \(\) using the dataset she collected, the dataset she submitted, and the information she received from the mechanism.

_Mechanism and theoretical result:_ In our mechanism, which we call C3D (Cross-Check and Corrupt based on Difference), each agent \(i\) collects a dataset \(X_{i}\) and submits a possibly fabricated or altered version \(Y_{i}\) to the mechanism. The mechanism then determines agent \(i\)'s allocation in the following manner. It pools the data from the other agents and splits them into two subsets \(Z_{i},Z^{}_{i}\). Then, \(Z_{i}\) is returned as is, while \(Z^{}_{i}\) is corrupted by adding noise that is proportional to the difference between \(Y_{i}\) and \(Z_{i}\). If an agent collects less or fabricates, she risks looking different to the others, and will receive a dataset \(Z^{}_{i}\) of poorer quality. We show that this mechanism has a Nash equilibrium where all agents collect a sufficiently large amount of data, submit it truthfully, and use a carefully weighted average of the three datasets \(X_{i},Z_{i},\) and \(Z^{}_{i}\) as their estimate for \(\). The weighting uses some additional side information that the mechanism provides to each agent. Below, we state an informal version of the main theoretical result of this paper, which summarizes the properties of our mechanism.

_Theorem 1 (informal):_ The above mechanism is Nash incentive compatible, individually rational, and achieves a social penalty that is at most twice the globally minimum social penalty._

Corruption is the first of two ingredients to achieving NIC. The second is the design of the weighted average estimator which is (minimax) optimal after corruption. To illustrate why this is important, say that the mechanism had assumed that the agents will use any other sub-optimal estimator (e.g a simple average). Then it will need to lower the amount of corruption to ensure IR and efficiency. However, a strategic agent will realize that she can achieve a lower maximum risk with a better estimator (instead of collecting more data herself and/or receiving less corrupt data from the mechanism). She can leverage this insight to collect less data and lower her overall penalty.

_Proof techniques:_ The most challenging part of our analysis is to show NIC, First, to show minimax optimality of our estimator, we construct a sequence of normal priors for \(\) and show that the minimum Bayes' risk converges to the maximum risk of the weighted average estimator. However, when compared to typical minimax proofs, we face more significant challenges. The first of these is that the combined dataset \(X_{i} Z_{i} Z_{i}^{}\) is neither independent nor identically distributed as the corruption is data-dependent. The second is that the agent's submission \(Y_{i}\) also determines the degree of corruption, so we cannot look at the estimator in isolation when computing the minimum Bayes' risk; we should also consider the space of functions an agent may use to determine \(Y_{i}\) from \(X_{i}\). The third is that the expressions for the minimum Bayes' risk do not have closed form solutions and require non-trivial algebraic manipulations. To complete the NIC proof, we show that due to the carefully chosen amount of corruption, the agent should collect a sufficient amount of data to avoid excessive corruption, but not too much so as to increase her data collection costs.

### Related Work

Mechanism design is one of the core areas of research in game theory [13; 18; 36]. Our work here is more related to mechanism design without payments, which has seen applications in fair division , matching markets , and kidney exchange  to name a few. There is a long history of work in the intersection of machine learning and mechanism design, although the overwhelming majority apply learning techniques when there is incomplete information about the mechanism or agent preferences, (e.g [6; 8; 22; 28; 30]). On the flip side, some work have designed data marketplaces, where customers may purchase data from contributors [4; 5; 19; 38]. These differ from our focus where we wish to incentivize agents to collaborate without payments.

Due to the popularity of shared data platforms [1; 2; 16; 34] and federated learning , there has been a recent interest in designing mechanisms for data sharing. Sim et al.  and Xu et al.  study fairness in collaborative data sharing, where the goal is to reward agents according to the amount of data they contribute. However, their mechanisms do not apply when strategic agents may try to manipulate a mechanism. Blum et al.  and Karimireddy et al.  study collaboration in federated learning. However, the strategy space of an agent is restricted to how much data they collect and their mechanism rewards each agent according to the quantity of the data she submitted. The above four works recognize that free-riding can be detrimental to data sharing, but assume that agents will not fabricate data. As discussed above, if this assumption is not true, agents can easily manipulate such mechanisms. Fraboni et al.  and Lin et al.  study federated learning settings where free-riders may send in fabricated gradients without incurring the computational cost of computing the gradients. However, their focus is on designing gradient descent algorithms that are robust to such attacks and not on incentivizing agents to perform the gradient computations. Some work have designed mechanisms for federated learning so as to elicit private information (such as data collection costs), but their focus is not on preventing free-riding or fabrication [15; 26]. Miller et al.  uses scoring systems to develop mechanisms that prevent signal fabrication. However, the agents in their settings can only choose to report either their true signal or something else but can not freely choose how much data to collect. Cai et al.  study mechanism design where a learner incentivizes agents to collect data via payments. Their mechanism, which also cross-checks the data submitted by the agents, has connections to our setting in SS4.2 where we consider a restricted strategy space for the agents.

Our approach of using corruption to engender good behaviour draws inspiration from the robust estimation literature, which design estimators that are robust to data from malicious agents [12; 14; 27]. However, to the best of our knowledge, the specific form of corruption and the subsequent design of the minimax optimal estimator are new in this work, and require novel analysis techniques.

## 2 Problem Setup

We will now formally define our problem. We have \(m\) agents, who are each able to collect i.i.d samples from a normal distribution \((,^{2})\), where \(^{2}\) is known. They wish to estimate the mean \(\) of this distribution. To collect one sample, the agent has to incur a cost \(c\). We will assume that \(^{2}\), \(c\), and \(m\) are public information. However, \(\) is unknown, and no agent has auxiliary information, such as a prior, about \(\). An agent wishes to minimize her estimation error, while simultaneously keeping the cost of data collection low. While an agent may collect data on her own to manage this trade-off, by sharing data with other agents, she can reduce costs while simultaneously improving her estimate. We wish to design mechanisms to facilitate such sharing of data.

**Mechanism:** A mechanism receives a dataset from each agent, and in turn returns an _allocation_\(A_{i}\) to each agent. An agent will use her allocation to estimate \(\). This allocation could be, for instance, a larger dataset obtained with other agents' datasets. The mechanism designer is free to choose a space of allocations \(\) to achieve the desired goals. Formally, we define a mechanism as a tuple \(M=(,b)\) where \(\) denotes the space of allocations, and \(b\) is a procedure to map the datasets collected from the \(m\) agents to \(m\) allocations. Denoting the universal set by \(\), we write the space of mechanisms \(\) as

\[=M=(,\,b):\ \ ,\ \ b:(_{n  0}^{n})^{m}^{m}}.\] (1)

As is customary, we will assume that the mechanism designer will publish the space of allocations \(\) and the mapping \(b\) (the procedure used to obtain the allocations) ahead of time, so that agents can determine their strategies. However, specific values computed/realized during the execution of the mechanism are not revealed, unless the mechanism chooses to do so via the allocation \(A_{i}\).

**Agents' strategy space:** Once the mechanism is published, the agent will choose a strategy. In our setting, this will be the tuple \((n_{i},f_{i},h_{i})\), which determines how much data she wishes to collect, what she chooses to submit, and how she wishes to estimate the mean \(\). First, the agent samples \(n_{i}\) points to collect her initial dataset \(X_{i}=\{x_{i,j}\}_{j=1}^{n_{i}}\), where \(x_{i,j}(,^{2})\), incurring \(cn_{i}\) cost. She then submits \(Y_{i}=\{y_{i,j}\}_{j}=f_{i}(X_{i})\) to the mechanism. Here \(f_{i}\) is a function which maps the collected dataset to a possibly fabricated or falsified dataset of a potentially different size. In particular, this fabrication can depend on the data she has collected. For instance, the agent could collect only a small dataset, fit a Gaussian, and then sample from it.

Finally, the mechanism returns the agent's allocation \(A_{i}\), and the agent computes an estimate \(h_{i}(X_{i},Y_{i},A_{i})\) for \(\) using her initial dataset \(X_{i}\), the dataset she submitted \(Y_{i}\), and the allocation she received \(A_{i}\). We include \(Y_{i}\) as part of the estimate since an agent's submission may affect the allocation she receives. Consequently, agents could try to elicit additional information about \(\) via a carefully chosen \(Y_{i}\). We can write the strategy space of an agent as \(=\), where \(\) is the space of functions mapping the dataset collected to the dataset submitted, and \(\) is the space of all estimators using all the information she has. We have:

\[=f:_{n 0}^{n}_{n  0}^{n}}, 28.452756pt=h: _{n 0}^{n}\ \ _{n 0}^{n}\ \ \ }.\] (2)

One element of interest in \(\) is the identity \(\) which maps a dataset to itself. A mechanism designer would like an agent to use \(f_{i}=\), i.e to submit the data that she collected as is, so that other agents can benefit from her data.

Going forward, when \(s=\{s_{i}\}_{i}^{m}\) denotes the strategies of all agents, we will use \(s_{-i}=\{s_{j}\}_{j i}\) to denote the strategies of all agents except \(i\). Without loss of generality, we will assume that agent strategies are deterministic. If they are stochastic, our results will carry through for every realization of any external source of randomness that the agent uses to determine \((n_{i},f_{i},h_{i})\).

**Agent penalty:** The agent's _penalty_\(p_{i}\) (i.e negative utility) is the sum of her squared estimation error and the cost \(cn_{i}\) incurred to collect her dataset \(X_{i}\) of \(n_{i}\) points. The agent's penalty depends on the mechanism \(M\) and the strategies \(s=\{s_{j}\}_{j}\) of all the agents. Making this explicit, \(p_{i}\) is defined as:

\[p_{i}(M,s)=_{}[(h_{i}(X_{i},Y_{i},A_{i })-)^{2}]\ +\ cn_{i}\] (3)

The term inside the expectation is the squared difference between the agent's estimate and the true mean (conditioned on the true mean \(\)). The expectation is with respect to the randomness of all agents' data and possibly any randomness in the mechanism. We consider the _maximum risk_, i.e supremum over \(\), since the true mean \(\) is unknown to the agent a priori, and their strategy should yield good estimates, and hence small penalty, over all possible values \(\). To illustrate this further, note that when the value of true mean \(\) is \(^{}\), the optimal strategy for an agent will always be to not collect any data and choose the estimator \(h_{i}(,,)=^{}\) leading to \(0\) penalty. However, this strategy can be meaningfully realized by an agent only if she knew that \(=^{}\) a priori which renders the problem meaningless1. Considering the maximum risk accounts for the fact that \(\) is unknown and makes the problem well-defined.

**Recommended strategies:** In addition to publishing the mechanism, the mechanism designer will recommend strategies \(s^{}=\{s^{}_{i}\}_{i}^{m}\) for the agents so as to incentivize collaboration and induce optimal social outcomes.

**Desiderata:** We can now define the three desiderata for a mechanism:

1. _Nash Incentive compatibility (NIC):_ A mechanism \(M=(,b)\) is said to be NIC at the recommended strategy profile \(s^{}\) if, for each agent \(i\), and for every other alternative strategy \(s_{i}\) for that agent, we have \(p_{i}(M,s^{}) p_{i}(M,(s_{i},s^{}_{-i}))\). That is, \(s^{}\) is a Nash equilibrium so no agent has incentive to deviate if all other agents are following \(s^{}\).
2. _Individual rationality (IR):_ We say that a mechanism \(M\) is IR at \(s^{}\) if no agent suffers from a higher penalty by participating in the mechanism than the lowest possible penalty she could achieve on her own when all other agents are following \(s^{}\). If an agent does not participate, she does not submit nor receive any data from the mechanism; she will simply choose how much data to collect and design the best possible estimator. Formally, we say that a mechanism \(M\) is IR if the following is true for each agent \(i\): \[p_{i}(M,s^{})_{n_{i},\,h_{i}}\,\{ _{}\,[(h_{i}(X_{i},, )-)^{2}\,|\,]\ +\ cn_{i}\}.\] (4)
3. _Efficiency:_ The _social penalty_\(P(M,s)\) of a mechanism \(M\) when agents follow strategies \(s\), is the sum of agent penalties (defined below). We define \((M,s^{})\) to be the ratio between the social penalty of a mechanism at the recommended strategies \(s^{}\), and the lowest possible social penalty among all possible mechanisms and strategies (_without_ NIC or IR constraints). We have: \[P(M,s)=_{i[m]}p_{i}(M,s),(M,s^{})=)}{_{M^{},\,s^{m}}P(M^{ },s)}\] (5) Note that \( 1\). We say that a mechanism is efficient if \((M,s^{})=1\) and that it is approximately efficient if \((M,s^{})\) is bounded by some constant that does not depend on \(m\). If \(s^{}\) is a Nash equilibrium, then \((M,s^{})\) can be viewed as an upper bound on the price of stability .

For what follows, we will discuss optimal strategies for agents working on her own and present a simple mechanism which minimizes the social penalty, but has a poor Nash equilibrium.

**Optimal strategies for an agent working on her own:** Recall that, given \(n\) samples \(\{x_{i}\}_{i=1}^{n}\) from \((,^{2})\), the sample mean is a minimax optimal estimator ; i.e among all possible estimators \(h\), the sample mean minimizes the maximum risk \(_{}[(-h(\{x_{i}\}_{i=1}^{n},, ))^{2}\,|\,]\) (note that the agent only has the dataset she collected). Moreover, its mean squared error is \(^{2}/n\) for all \(\). Hence, an agent acting on her own will choose the sample mean and collect \(n_{i}=/\) samples so as to minimize their penalty; as long as the amount of data is less than \(/\), an agent has incentive to collect more data since the cost of collecting one more point is offset by the marginal decrease in estimation error. This can be seen via the following simple calculation:

\[_{n_{i}\\ h_{i}}_{}[(h_{i}(X_{i },,)-)^{2}\ \,]\ +\ cn_{i}=_{n_{i}}}{n_{i}}+ cn_{i}=2\ \ }{{=}}\ p_{}^{}\,.\] (6)

Let \(p_{}^{}=2\) denote the lowest achievable penalty by an agent working on her own. If all \(m\) agents work independently, then the total social penalty is \(mp_{}^{}=2 m\). Next, we will look at a simple mechanism and an associated set of strategies which achieve the global minimum penalty. This will show that it is possible for all agents to achieve a significantly lower penalty via collaboration.

**A globally optimal mechanism _without_ strategic considerations:** The following simple mechanism \(M_{}\), pools all the data from the other agents and gives it back to an agent. Precisely, it chooses the space of allocation \(=_{n 0}^{n}\) to be datasets of arbitrary length, and sets agent \(i\)'s allocation to be \(A_{i}=_{j i}Y_{i}\). The recommended strategies \(s^{}=\{(n_{i}^{},f_{i}^{},h_{i}^{ })\}_{i}\) asks each agent to collect \(n_{i}^{}=/\) points2, submit it as is \(f_{i}^{}=\), and use the sample mean of all pointsas her estimate \(h_{i}^{}}(X_{i},X_{i},A_{i})= A_{i}|}_{z X _{i} A_{i}}z\). It is straightforward to show that this minimizes the social penalty if all agents follow \(s^{}}\). After each agent has collected their datasets \(\{X_{i}\}_{i}\), the social penalty is minimized if all agents have access to each other's datasets and they all use a minimax optimal estimator: this justifies using \(M_{}}\) with \(f_{i}^{}}=\) and setting \(h_{i}^{}}\) to be the sample mean. The following simple calculation justifies the choice of \(_{i}n_{i}^{}}\):

\[_{s^{m}}_{i=1}^{m}(_{}[(h_{i }(X_{i},f_{i},A_{i})-)^{2}\ \,]+cn_{i})=_{\{n_{i}\}_{i}}( }{_{i}n_{i}}+c_{i}n_{i})=2.\]

However, \(s^{}}\) is not a Nash equilibrium of this mechanism, as an agent will find it beneficial to free-ride. If all other agents are submitting \(/\) points, by collecting no points, an agent's penalty is \(/(m-1)\), as she does not incur any data collection cost. This is strictly smaller than \(2\) when \(m 3\). In fact, it is not hard to show that \(M_{}}\) is at a Nash equilibrium only when the total amount of data is \(/\); for additional points, the marginal reduction in the estimation error for an individual agent does not offset her data collection costs. The social penalty at these equilibria is \((m+1)\) which is significantly larger than the global minimum when there are many agents.

A seemingly simple way to fix this mechanism is to only return the datasets of the other agents if an agent submits at least \(/\) points. However, as we will see in SS4.1, such a mechanism can also be manipulated by an agent who submits a fabricated dataset of \(/\) points without actually collecting any data and incurring any cost and then discarding the fabricated dataset when estimating. Any naive test to check for the quality of the data can also be sidestepped by agents who sample only a few points, and use that to fabricate a larger dataset (e.g by sampling a large number of points from a Gaussian fitted to the small sample). Next, we will present our mechanism for this problem which satisfies all three desiderata.

## 3 Method and Results

We have outlined our mechanism \(M_{}}\), and its interaction with the agents in Algorithm 1 in the natural order of events. We will first describe it procedurally, and then motivate our design choices. Our mechanism uses the following allocation space, \(=_{n 0}^{n}_{n 0}^{n} _{+}\). An allocation \(A_{i}=(Z_{i},Z_{i}^{},_{i}^{2})\) consists of an uncorrupted dataset \(Z_{i}\), a corrupted dataset \(Z_{i}^{}\), and the variance \(_{i}^{2}\) of the noise added to \(Z_{i}^{}\) for corruption. Once the mechanism and the allocation space are published, agent \(i\) chooses her strategy \(s=(n_{i},f_{i},h_{i})\). She collects a dataset \(X_{i}=\{x_{i,j}\}_{j=1}^{n_{i}}\), where \(x_{i,j}(,^{2})\), and submits \(Y_{i}=f_{i}(X_{i})\) to the mechanism.

Our mechanism determines agent \(i\)'s allocation as follows. Let \(Y_{-i}\) be the union of all datasets submitted by the other agents. If there are at most four agents, we simply return all of the other agents' data without corruption by setting \(A_{i}(Y_{-i},,0)\). If there are more agents, the mechanism first chooses a random subset of size \(\{|Y_{-i}|,\ /\}\) from \(Y_{-i}\); denote this \(Z_{i}\). In line 13, the mechanism individually adds Gaussian noise to the remaining points \(Y_{-i}|Z_{i}\) to obtain \(Z_{i}^{}\) (line 14). The variance \(_{i}^{2}\) of the noise depends on the difference between the sample means of the subset \(Z_{i}\) and the agent's submission \(Y_{i}\). It is modulated by a value \(\), which is a function of \(c\), \(m\), and \(^{2}\). Precisely, \(\) is the smallest number larger than \((cm)^{-1/4}\) which satisfies \(G()=0\), where:

\[G():=\!(\!}{/}-1\! )\!(m/c)^{1/4}}-(\!(4(m+1)}{}-1\!)\!}\!\!( }{8^{2}})\!}\! (\!(m/c)^{1/4}}{2})\!)\] (7)

Finally, the mechanism returns the allocation \(A_{i}=(Z_{i},Z_{i}^{},_{i}^{2})\) to agent \(i\) and the agent estimates \(\).

_Recommended strategies:_ The recommended strategy \(s_{i}^{}=(n_{i}^{},f_{i}^{},h_{i}^{})\) for agent \(i\) is given in (8). The agent should collect \(n_{i}^{}=/(m)\) samples if there are at most four agents, and \(n_{i}^{}=/\) samples otherwise. She should submit it without fabrication or alteration \(f_{i}=\), and then use a weighted average of the datasets \((X_{i},Z_{i},Z_{i}^{})\) to estimate \(\). The weighting is proportional to the inverse variance of the data. For \(X_{i}\) and \(Z_{i}\) this is simply \(^{2}\), but for \(Z_{i}^{}\), the variance is \(^{2}+_{i}^{2}\) since the mechanism adds Gaussian noise with variance \(_{i}^{2}\). We have:

\[n_{i}^{}=}&m  4\\ }&m>4, 56.905512ptf_{i}^{}= ,\] \[h_{i}^{}(X_{i},Y_{i},(Z_{i},Z_{i}^{},_{i}^{2}))= }_{u X_{i} Z_{i}}u++ _{i}^{2}}_{u Z_{i}^{}}u}{}|X_{i} Z_{i}^ {}|++_{i}^{2}}|Z_{i}^{}|}\] (8)

_Design choices:_ Next, we will describe our design choices and highlight some key challenges. When \(m 4\), it is straightforward to show that the mechanism satisfies all our desired properties (see beginning of SS3.1), so we will focus on the case \(m>4\). First, recall that the mechanism needs to incentivize agents to collect a sufficient amount of samples. However, simply counting the number of samples can be easily manipulated by an agent who simply submits a fabricated dataset of a large number of points. Instead, Algorithm 1 attempts to infer the quality of the data submitted by the agents using how well an agent's submission \(Y_{i}\) approximates \(\). Ideally, we would set the variance \(_{i}^{2}\) of this corruption to be proportional to the difference \((|}_{y Y_{i}}y-)^{2}\), so that the more data she submits, the less the variance of \(Z_{i}^{}\), which in turn yields a more accurate estimate for \(\). However, since \(\) is unknown, we use a subset \(Z_{i}\) obtained from other agents' data as a proxy for \(\), and set \(_{i}^{2}\) proportional to \((|}_{y Y_{i}}y-|}_{z Z_{i}}z )^{2}\). If all agents are following \(s^{}\), then \(|Y_{i}|=|Z_{i}|=/=n_{i}^{}\); it is sufficient to use only \(n_{i}^{}\) points for validating \(Y_{i}\) since both \(|}_{y Y_{i}}y\) and \(|}_{z Z_{i}}z\) will have the same order of error in approximating \(\).

The second main challenge is the design of the recommended estimator \(h_{i}^{}\). In SS3.1 we show how splitting \(Y_{-i}\) into a clean and corrupted parts \(Z_{i},Z_{i}^{}\) allows us to design a minimax optimal estimator. A minimax optimal estimator is crucial to achieving NIC. To explain this, say that the mechanism assumes that agents will use a sub-optimal estimator, e.g sample mean of \(X_{i} Z_{i} Z_{i}^{}\). Then, to account for the larger estimation error, it will need to choose a lower level of corruption \(_{i}^{2}\) to minimize the social penalty. However, a smart agent will realize that she can achieve a lower maximum risk by using a better estimator, such as the weighted average, instead of collecting more data in order to reduce the amount of corruption used by the mechanism. She can leverage this insight to collect less data and reduce her overall penalty.

This concludes the description of our mechanism. The following theorem, which is the main theoretical result of this paper, states that \(M_{}\) achieves the three desiderata outlined in SS2.

**Theorem 1**.: _Let \(m>1\), \(\) be as defined in (7), and \(s_{i}^{}\) be as defined in (8). Then, the following statements are true about the mechanism \(M_{}\) in Algorithm 1. (i) The strategy profile \(s^{}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{}\). (iii) The mechanism is approximately efficient, with \(}(M_{},s^{}) 2\)._

The mechanism is NIC as, provided that others are following \(s_{i}^{}\), there is no reason for any one agent to deviate. Moreover, we achieve low social penalty at \(s_{i}^{}\). Other than \(s^{}\), there is also a set of similar Nash equilibria with the same social penalty: the agents can each add a same constant to the data points they collect and subtract the same value from the final estimate. Before we proceed,the expression for \(\) in (7) warrants explanation. If we treat \(\) is a variable, we find that different choices of \(\) can lead to other Nash equilibria with corresponding bounds on PR. This specific choice of \(\) leads to a Nash equilibrium where agents collect \(/\) points, and a small bound on \(\). Throughout this manuscript, we will treat \(\) as the specific value obtained by solving (7), and _not_ as a variable.

High dimensional non-Gaussian distributions:In Appendix E, we study \(M_{}\) when applied to \(d\)-dimensional distributions. In Theorem 7, we show that under bounded variance assumptions, \(s^{}\) is an \(_{m}\)-approximate Nash equilibrium and that \((M_{},s^{}) 2+_{m}\) where \(_{m}(1/m)\).

### Proof sketch of Theorem 1

_When \(m 4\):_ First, consider the (easy) case \(m 4\). At \(s_{i}^{}\), the total amount of data collected is \(/\) (see \(n_{i}^{}\) in (8)), and as there is no corrupted dataset, \(h_{i}^{}\) simply reduces to the sample mean of \(X_{} Y_{-i}\). The mechanism is IR since an agent's penalty will be \((1+1/m)\) which is smaller than \(p_{}^{}\) (6). It is approximately efficient since the social penalty is \((m+1)\) which is at most twice the global minimum \(2\) when \(m 4\). Finally, NIC is guaranteed by the same argument used in (6); as long as the total amount of data is less than \(/\), the cost of collecting one more point is offset by the marginal decrease in the estimation error; hence, the agent is incentivized to collect more data. Moreover, as \(A_{i}\) does not depend on \(f_{i}\) under these conditions, there is no incentive to fabricate or falsify data.

_When \(m>4\):_ We will divide this proof into four parts. We first show that \(G()=0\) in line (6) has a solution \(\) larger than \(^{}}=(cm)^{-1/4}\). This will also be useful when analyzing the efficiency.

**1. Equation** (7) **has a solution.** We derive an asymptotic expansion of \(()\) using integration by parts to analyze the solution to (7). When \(m 5\), we show that \(G^{}} G^{}}(1+8/ )<0\). By continuity of \(G\), there exists \(_{m}^{}},^{}}(1+8/) \) s.t. \(G(_{m})=0\). For \(m\) large enough such that the residual in the asymptotic expansion is negligible, we show \(_{m}^{}},^{}}(1+ m/m)\) via an identical technique.

**2. The strategies \(s^{}\) in (8) is a Nash equilibrium:** We show this via the following two steps. First (**2.1**), We show that fixing any \(n_{i}\), the maximum risk and thus the penalty \(p_{i}\) is minimized when agent \(i\) submits the raw data and uses the weighted average as specified in (8), i.e for all \(n_{i}\),

\[p_{i}(M_{},((n_{i},f_{i}^{},h_{i}^{}),s_{-i}^{}))  p_{i}(M_{},((n_{i},f_{i},h_{i}),s_{-i}^{})),\ \ (n_{i},f_{i},h_{i}).\] (9)

Second (**2.2**), we show that \(p_{i}\) is minimized when agent \(i\) collects \(n_{i}^{}\) samples under \((f_{i}^{},h_{i}^{})\), i.e.

\[p_{i}(M_{},((n_{i}^{},f_{i}^{},h_{i}^{}),s_{-i}^{ })) p_{i}(M_{},((n_{i},f_{i}^{},h_{i}^{}),s_{-i} ^{})),\,n_{i}.\] (10)

_2.1: Proof of_ (9). As the data collection cost does not change for fixed \(n_{i}\), it is sufficient to show that \((f_{i}^{},h_{i}^{})\) minimizes the maximum risk. Our proof is inspired by the following well-known recipe for proving minimax optimality of an estimator : design a sequence of priors \(\{_{}\}_{}\), compute the minimum Bayes' risk \(\{R_{}\}_{}\) for any estimator, and then show that \(R_{}\) converges to the maximum risk of the proposed estimator as \(\).

To apply this recipe, we use a sequence of normal priors \(_{}=(0,^{2})\) for \(\). However, before we proceed, we need to handle two issues. The first of these concerns the posterior for \(\) when conditioned on \((X_{i},Z_{i},Z_{i}^{})\). Since the corruption terms \(_{z,i}\) added to \(Z_{i}^{}\) depend on \(X_{i}\) and \(Z_{i}\), this dataset is not independent. Moreover, as the variance \(_{i}^{2}\) is the difference between two normal random variables, \(Z_{i}^{}\) is not normal. Despite these, we are able to show that the posterior \(|(X_{i},Z_{i},Z_{i}^{})\) is normal. The second challenge is that the submission \(f_{i}\) also affects the estimation error as it determines the amount of noise \(_{i}^{2}\). We handle this by viewing \(\) as a rich class of estimators and derive the optimal Bayes' estimator \((f_{i,}^{},h_{i,}^{})\) under the prior \(_{}\). We then show that the minimum Bayes' risk converges to the maximum risk when using \((f_{i}^{},h_{i}^{})\).

Next, under the prior \(_{}=(0,^{2})\), we can minimize the Bayes' risk with respect to \(h_{i}\) by setting \(h_{i,}^{}\) to be the posterior mean. Then, the minimum Bayes' risk \(R_{}\) can be written as,

\[R_{}=_{f_{i}}[(|Z_{i}^{}| ^{2}+^{2}|}_{y Y_{i}}y-|}_{z Z_{i}}z^{2})^{-1}+|+|Z_{i}|}{^{2}} +})^{-1}]\]

Note that \(Y_{i}=f_{i}(X_{i})\) depends on \(f_{i}\). Via the Hardy-Littlewood inequality , we can show that the above quantity is minimized when \(f_{i,}^{}\) is chosen to be a shrunk version of the agent's initial dataset \(X_{i}\), i.e \(f_{i,}^{}(X_{i})=\{(1+^{2}/(|X|^{2})) ^{-1}x,\ \,x X_{i}\}\). This gives us an expression for the minimum Bayes' risk \(R_{}\) under prior \(_{}\). To conclude the proof, we note that the minimum Bayes' risk under any prior is a lower bound on the maximum risk, and show that \(R_{}\) approaches the maximum risk of \((f_{i}^{},h_{i}^{})\) from below. Hence, \((f_{i}^{},h_{i}^{})\) is minimax optimal for any \(n_{i}\). (Above, it is worth noting that \(f_{i,}^{} f_{i}^{}=\) as \(\). In the Appendix, we also find that \(h_{i,}^{} h_{i}^{}\). )

_2.2: Proof of (10)_. We can now write \(p_{i}(M_{},((n_{i},f_{i}^{},h_{i}^{}),s_{-i}^{}))= R_{}+cn_{i}\), where \(R_{}\) is the maximum risk of \((f_{i}^{},h_{i}^{})\) (and equivalently, the limit of the minimum Bayes' risk):

\[R_{}:=_{x(0,1)}(m-2)n_{i}^{ }^{2}+^{2}^{2}/n_{i}+^{2}/n_{i}^{ }x^{2}^{-1}+(n_{i}+n_{i}^{})^{-2}^{-1} \]

The term inside the expectation is convex in \(n_{i}\) for each fixed \(x\). As expectation preserves convexity, we can conclude that \(p_{i}\) is a convex function of \(n_{i}\). The choice of \(\) in (7) ensures that the derivative is \(0\) at \(n^{}\) which implies that \(n^{}\) is a minimum of this function.

**3. \(M_{}\) is individually rational at \(s^{}\):** This is a direct consequence of step 2 as we can show that an agent 'working on her own' is a valid strategy in \(M_{}\).

**4. \(M_{}\) is approximately efficient at \(s^{}\):** By observing that the global minimum penalty is \(2\), we use a series of nontrivial algebraic manipulations to show \((M_{},s^{})=/ n_{i}^{}-1}{4(m+1)^{2}/(mn_{i}^{})-1}+1\). As \(>^{}}\), some simple algebra leads to \((M_{},s^{})<2\).

## 4 Special Cases: Restricting the Agents' Strategy Space

In this section, we study two special cases motivated by some natural use cases, where we restrict the agents' strategy space. In addition to providing better guarantees on the efficiency, this will also help us better illustrate the challenges in our original setting.

### Agents cannot fabricate or falsify data

First, we study a setting where agents are not allowed to fabricate data or falsify data. Specifically, in (2), \(\) is restricted to functions which map a dataset to any subset. This is applicable when there are regulations preventing such behavior (e.g government institutions, hospitals)

_Mechanism:_ The discussion at the end of SS2 motivates the following modification to the pooling mechanism. We set the allocation space to be \(=_{n 0}^{n}\), i.e the space of all datasets. If an agent \(i\) submits at least \(/\) points, then give her all the other agents' datasets, i.e \(A_{i}=_{j i}Y_{j}\); otherwise, set \(A_{i}=\). The recommended strategy \(s_{i}^{}=(n_{i}^{},f_{i}^{},h_{i}^{})\) of each agent is to collect \(/\) points, submit it as is \(f_{i}^{}=\), and then use the sample mean of \(Z_{i} A_{i}\) to estimate \(\). The theorem below, whose proof is straightforward, states the main properties of this mechanism.

**Theorem 2**.: _The following statements about the mechanism and strategy profile \(s^{}\) in the paragraph above are true when \(\) is restricted to functions which map a dataset to any subset: (i) \(s^{}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{}\). (iii) At \(s^{}\), the mechanism is efficient._

It is not hard to see that this mechanism can be easily manipulated by the agent if there are no restrictions on \(\). As the mechanism only checks for the amount of data submitted, the agent can submit a fabricated dataset of \(/\) points, and then discard this dataset when computing the estimate, which results in detrimental free-riding.

### Agents accept an estimated value from the mechanism

Our next setting is motivated by use cases where the mechanism may directly deploy the estimated value for \(\) in some downstream application for the agent, i.e the agents are forced to use this value. This is motivated by federated learning, where agents collect and send data to a server (mechanism), which deploys a model (estimate) directly on the agent's device . This requires modifying the agent's strategy space to \(=\). Now, an agent can only choose \((n_{i},f_{i})\), how much data she wishes to collect, and how to fabricate or falsify the dataset. A mechanism is defined as a procedure \(b:_{n 0}^{n}^{m}^{m}\), which maps \(m\) datasets to \(m\) estimated mean values.

Algorithm 3 (see Appendix D) outlines a family of mechanisms parametrized by \(>0\) for this setting. As we will see shortly, with parameter \(\), the mechanism can achieve a \(\) of \((1+)\). This mechanism computes agent \(i\)'s estimate for \(\) as follows. First, let \(Y_{-i}\) be the union of all datasets submitted by the other agents. Similar to Algorithm 1, the algorithm individually adds Gaussian to each \(Y_{-i}\) to obtain \(Z_{i}\) (line 10). Unlike before, this noise is added to the entire dataset and the variance \(_{i}^{2}\) of this noise depends on the difference between the sample means of the agent's submission \(Y_{i}\) and all of the other agents' submissions \(Y_{-i}\). It also depends on two \(\)-dependent parameters defined in line 6. Finally, the mechanism deploys the sample mean of \(Y_{i} Z_{i}\) as the estimate for \(\). The recommended strategies \(s^{*}_{i}=(n^{*}_{i},f^{*}_{i})\) for the agents is to simply collect \(n^{*}_{i}=/\) points and submit it as is \(f^{*}_{i}=\). The following theorem states the main properties of the mechanism.

**Theorem 3**.: _Let \(>0\). The following statements about Algorithm 3 and the strategy profile \(s^{*}\) given in the paragraph above are true: (i) \(s^{*}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{*}\). (iii) At \(s^{*}\), the mechanism is approximately efficient with \((M,s^{*}) 1+\)._

The above theorem states that it is possible to obtain a social penalty that is arbitrarily close to the global minimum under the given restriction of the strategy space. However, this mechanism is not NIC if agents are allowed to design their own estimator. For instance, if the mechanism returns \(A_{i}=Z_{i}\) (line 10), then using a weighted average of the data in \(X_{i}\) and \(Z_{i}\) yields a lower estimation error than simple average used by the mechanism (see Appendix D). An agent can leverage this insight to collect and submit less data and obtain a lower overall penalty at the expense of other agents. Cai et al.  study a setting where agents are incentivized to collect data and submit it truthfully via payments. Interestingly, their corruption method can be viewed as a special case of Algorithm 3 with \(k_{}=1\) and only achieves a \(1.5\) factor of the global minimum social penalty. Moreover, when applied to the more general strategy space, it shares the same shortcomings as the mechanism in Theorem 3.

## 5 Conclusion

We studied collaborative normal mean estimation in the presence of strategic agents. Naive mechanisms which only look at the quantity of the dataset submitted, can be manipulated by agents who under-collect and/or fabricate data, leaving all agents worse off. To address this issue, when sharing the others' data with an agent, our mechanism \(M_{}\) corrupts this dataset proportional to how much the data reported by the agent differs from the other agents. We design minimax optimal estimators for this corrupted dataset to achieve a socially desirable Nash equilibrium.

**Future directions:** We believe that designing mechanisms for other collaborative learning settings may require relaxing the _exact_ NIC guarantees to make the analysis tractable. For many learning problems, it is difficult to design exactly optimal estimators, and it is common to settle for rate-optimal (i.e up to constants) estimators . For instance, even simply relaxing to high dimensional distributions with bounded variance, \(M_{}\) can only provide an approximate NIC guarantee.