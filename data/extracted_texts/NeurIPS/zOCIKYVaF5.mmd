# Residual Alignment:

Uncovering the Mechanisms of Residual Networks

Jianing Li

University of Toronto

jrobert.li@mail.utoronto.ca&Vardan Papyan

University of Toronto

vardan.papyan@utoronto.ca

###### Abstract

The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements (code) reveal a process called Residual Alignment (RA) characterized by four properties:

1. [label=()]
2. intermediate representations of a given input are _equispaced_ on a _line_, embedded in high dimensional space, as observed by Gai and Zhang (2021);
3. top left and right singular vectors of Residual Jacobians align with each other and across different depths;
4. Residual Jacobians are at most rank \(C\) for fully-connected ResNets, where \(C\) is the number of classes; and
5. top singular values of Residual Jacobians scale inversely with depth.

RA consistently occurs in models that generalize well, in both fully-connected and convolutional architectures, across various depths and widths, for varying numbers of classes, on all tested benchmark datasets, but ceases to occur once the skip connections are removed. It also provably occurs in a novel mathematical model we propose. This phenomenon reveals a strong alignment between residual branches of a ResNet (RA2+4), imparting a highly rigid geometric structure to the intermediate representations as they progress _linearly_ through the network (RA1) up to the final layer, where they undergo Neural Collapse.

## 1 Introduction

### Background

The Residual Network (ResNet) architecture (He et al., 2016), a special case of Highway Networks (Srivastava et al., 2015), has taken the field of deep learning by storm, since its proposal in 2015. The incredibly simple architectural modification of adding a skip connection, spanning a set of layers, has become a go-to architectural choice for deep learning researchers and practitioners alike. Initially applied in computer vision, it has since been integrated as a crucial component in biomedical imaging and generative models via the U-Net (Ronneberger et al., 2015), natural language processing through the Transformer (Vaswani et al., 2017), and reinforcement learning as seen in the case of AlphaGo Zero (Silver et al., 2017).

The ResNet architecture first passes an input \(x\) through initial layers \(\) (comprising a sequence of convolution, batch normalization, and ReLU operations), depending on trainable parameters \(_{0}\), togenerate an initial representation

\[h_{1}=(x;_{0})^{D}.\]

This is then processed through a sequence of residual blocks

\[h_{i+1}=(h_{i}+(h_{i};_{i})), 1 i L,\]

each refining the previous layer's representation, \(h_{i}^{d}\), through a simple residual branch

\[(h_{i};_{i}):^{D}^{D},\]

which depends on trainable parameters \(_{i}\) and an element-wise non-linearity, \(\), which is simply an identity mapping, in the case of pre-activation ResNets (He et al., 2016). The final block's output, \(h_{L+1}\), is fed to a classifier,

\[(h_{L+1};_{L+1}):^{D}^{C},\]

depending on trainable parameters \(_{L+1}\).

In the original ResNet architecture, \(\) and \(\) are compositions of linear transformations, element-wise nonlinearities, and normalization layers. The WideResNet architecture, by Zagoruyko and Komodakis (2016), further incorporates dropout layers into \(\). Yet more changes are made in ResNet (Xie et al., 2017), where \(\) is computed from the summation of parallel computational branches.

Training a ResNet for classification involves minimizing a loss on a training dataset, \(\{x_{n},y_{n}\}_{n=1}^{N}\), consisting of inputs \(x_{n}\) and labels \(y_{n}\) plus a weight decay term

\[*{minimize}_{\{_{i}\}_{i=1}^{L+1}}\,_ {n=1}^{N}(f(x_{n};),y_{n})+\|\|_{2}^{2},\] (1)

where \(f(x_{n};)\) are the outputs of the classifier \(\), also called logits, and \(\) are all the parameters.

### Problem Statement

The significant improvement in performance, achieved through the simple addition of a skip connection, has generated interest in understanding the underlying mechanisms of ResNets. Despite this, to this date, no definitive theory or explanation has been widely accepted by the deep learning community for the success of ResNets.

Figure 1: **Visualization of Residual Alignment.** Intermediate representations of a ResNet341, trained on CIFAR10, are projected onto two random vectors. Representations of each individual image are color-coded based on its true label and connected to form a trajectory, so as to showcase their progression throughout the network. Notice the _linear_ arrangement of intermediate representations along with _equidistant_ spacing between representations corresponding to consecutive layers (RA1). Our work shows, this phenomenon results from the _alignment_ of top singular vectors of Residual Jacobians (RA2) and the _inverse scaling_ of top singular values with depth (RA4). It is also noteworthy that the magnitudes of class means significantly increase with depth compared to the within-class variability, indicating the representations undergo layer-wise Neural Collapse (Papyan, 2020; Galanti et al., 2022; He and Su, 2022; Li et al., 2023).

### Method Overview

In this paper, we conduct a thorough empirical study of the ResNet architecture and its constituent residual blocks with the aim of investigating the characteristics of an individual residual block and the relationship between any pair of them. As the residual blocks are nonlinear functions of their inputs, we examine their linearizations through the _Residual Jacobian matrices_2:

\[^{}(h_{i}+(h_{i};_{i}))(h_{i};_{i})}{ h_{i}}^{D D}.\]

Following Equation (1.1), these correspond to the derivative of the residual block with respect to its input, _excluding_ the contribution from the skip connection, \(^{}(h_{i}+(h_{i};_{i}))^{D  D}\). In the case of a pre-activation ResNet, these are simply equal to the derivative of the residual branch with respect to its input, i.e., \((h_{i};_{i})/ h_{i}\). Since the Residual Jacobians are high-dimensional matrices, and likely contain meaningful information only in some of their subspaces, we measure their singular value decomposition (SVD), given by \(J_{i}=U_{i}S_{i}V_{i}^{}\), where \(U_{i}\) and \(V_{i}\) are the respective left and right singular vectors, and \(S_{i}\) is the singular value matrix.

### Contributions

We discover a phenomenon called _Residual Alignment (RA)_, consistently occurring in ResNet models that generalize well, characterized by four properties3:

* Intermediate representations of a given input are _equispaced_ on a _line_, embedded in high dimensional space, as shown in Figure 1 and observed by Gai and Zhang (2021);
* Top left and right singular vectors of Residual Jacobians align with each other and across different depths, as observed in Figure 2;
* Residual Jacobians are at most rank \(C\) for fully-connected ResNets, where \(C\) is the number of classes, as illustrated in Figures 3 and 4; and
* Top singular values of Residual Jacobians scale inversely proportional with depth, as depicted in Figure 4.

The properties are interrelated and, in fact, (RA1) can be logically derived from the other properties, as demonstrated in Section 3.

As a further contribution, in section B of the Appendix we prove theoretically the emergence of RA under the setting of binary classification with cross-entropy loss. Our proof relies on a novel mathematical abstraction called the _Unconstrained Jacobians Model_, in which the Residual Jacobians are optimized directly, so as to minimize the loss, and are not constrained by the architecture and parameters of the residual branches. This mathematical abstraction is motivated by recent theoretical works on Neural Collapse, as discussed in Section 5.

### Results Summary

Our empirical investigation, presented in the main text and the Appendix, consistently identifies RA across an extensive range of:

**Architectures:**: standard ResNets (convolutional layers with progressively increasing channels, interspersed with downsampling layers) as well as simpler designs (fully-connected layers);
**Datasets:**: MNIST, FashionMNIST, CIFAR10, CIFAR100, and ImageNette [Howard]; and
**Hyperparameters:**: network depth and width.

In addition to our main findings, we include an experiment showing the co-occurrence of (RA) and Neural Collapse in Figure 7. We also performed three counterfactual experiments that show:1. If the number of classes in the dataset is increased, the singular vector alignment occurs in a higher dimensional subspace (Figure 3);
2. If stochastic depth  is incorporated, the singular vector alignment is amplified (Figure 5); and
3. If the skip connections are removed, RA does not occur (Figure 6);

Figure 3: **(RA3) : Singular vector alignment occurs in subspace of rank \(\) C.** The figure presents a sequence of subplots that illustrate the matrix \(U_{16,10}^{}J_{9}V_{16,10}\). Here, \(J_{9}\) represents the \(9\)-th Residual Jacobian, while \(U_{16,10}\) and \(V_{16,10}\) correspond to the leading \(10\) left and right singular vectors, respectively, of the \(16\)-th Residual Jacobian, \(J_{16}\). These calculations are based on ResNet34 models (Type 1 model in Section 2.1). These models have been trained on specific subsets of the CIFAR10 dataset, comprising of 4, 6, and 8 classes, as well as the complete CIFAR10 and CIFAR100 datasets. Each result is presented in the corresponding Subfigures 3a, 3b, 3c, 3d, and 3e. As the number of classes increases, the alignment of singular vectors occurs in an increasingly higher-dimensional subspace.

Figure 2: **(RA2) : Top singular vectors of Residual Jacobians align.** Subfigure 2a and Subfigure 2b present the alignment of the first 8 blocks and the last 7 blocks, respectively, for a ResNet34 trained on CIFAR100 (Type 3 model in Section 2.1) forwarding a single randomly sampled input. Each subplot \((i,j)\) illustrates the matrix \(U_{j,30}^{}J_{i}V_{j,30}\), where \(U_{j,30}\) and \(V_{j,30}\) denote the top-30 left and right singular vectors of the Residual Jacobian \(j_{j}\), respectively, and \(i,j\) are the indices of the residual blocks, i.e., their depth. A distinct diagonal line of intense pixels is apparent in almost every subplot, signifying that the top singular vectors of \(J_{j}\) diagonalize \(J_{i}\). In similar terms, this means that the top singular vectors of \(J_{i}\) and \(J_{j}\) align and (RA2) holds. This pattern persists when \(V_{j,30}^{}J_{i}U_{j,30}\) is plotted, instead of \(U_{j,30}^{}J_{i}V_{j,30}\), further confirming that the top left and right singular vectors align in accordance with (RA2). Additional visualizations of both matrices, across various models and datasets, are available in subsections C.2 and C.3 of the Appendix. It is crucial to highlight that no alignment exists between the Jacobians at initialization, and the alignment emerges during training.

   Model & MNIST & FashionMNIST & CIFAR10 & CIFAR100 & ImageNette \\  Type 1 & 98.9 & 90.9 & 58.3 & 30.4 & 42.6 \\ Type 2 & 99.5 & 92.0 & 88.5 & 54.6 & 67.9 \\ Type 3 & 99.6 & 92.8 & 87.3 & 62.6 & 64.2 \\   

Table 1: (**RA3) occurs in models that generalize well.** The table displays the test accuracies of models trained to study (RA). Our reported accuracies closely align with those presented in Table 1 of Papyan et al. (2020), indicating that (RA) is observed in extensively trained models that exhibit strong performance in terms of test accuracy.

Figure 4: Depiction of Residual Jacobian singular values for ResNet34 trained on CIFAR10 (Type 1 model in Section 2.1). Subfigure 3(a) shows the top \(20\) singular values of Residual Jacobians, while Subfigure 3(b) illustrates the inverse scaling of the top \(1\) values. More singular value plots, from diverse models and datasets, are available in subsection C.4 of the Appendix.

Figure 5: **Stochastic depth amplifies singular vector alignment.** A comparison of (RA2) for two Type 3 models trained on CIFAR10 over \(50\) epochs: one employing the stochastic depth technique (with a drop probability of 0.3 for skipping residual blocks during training) and the other without it.

## 2 Methods

### Networks

We train 5 types of models:

**Type 1**: models consist of 16 basic residual blocks, each containing two fully-connected layers of dimension \(D{=}512\);

Figure 6: **Skip connections cause Residual Alignment. The experiment depicted in Figure 2 is replicated using two additional models. The first is a ResNet18 trained on CIFAR10 (Type 4a model in Section 2.1), with results showcased in Subfigure 5(a). The second is a ResNet18 _without skip connections_ (Type 4b model in Section 2.1), with results displayed in Subfigure 5(b). When the skip connections are removed, the top singular vectors no longer align. Alignment is visible in the diagonal subplots of Subfigure 5(b), as each Residual Jacobian is diagonalized by its own singular vectors.**

Figure 7: **Co-occurrence of Residual Alignment and Neural Collapse. The sub-figures display the progression of Neural Collapse metrics for a Type 4a model throughout 350 training epochs on the MNIST dataset as well as the emergence of Residual Alignment at the end of the training process.**

[MISSING_PAGE_FAIL:7]

### (Ra2+3+4) Imply (Ra1)

As mentioned in the introduction, the properties of (RA) are interconnected, and this relationship is demonstrated through the following theorem:

**Theorem 3.1**.: _For binary classification, in a pre-activation ResNet, assuming the Jacobian linearizations are exact and satisfy (RA2+3+4), then (RA1) holds for the intermediate representations._

The proof of this Theorem is deferred to section A of the Appendix.

### Unconstrained Jacobians Model Leads to RA

We propose the following abstraction of the optimization problem in Equation (1).

**Definition 3.2** (Unconstrained Jacobians Model).: Given a fixed input \(_{x}^{D}\) and its label \(y\{+1,-1\}\), find matrices \(J_{i}^{D D}\), \(1 i L\), and vector \(w^{D}\) that

\[*{minimize}_{w,\{J_{i}\}_{i=1}^{L}}(w^{} _{i=1}^{L}(I+J_{i})_{x},y)+_{i=1}^{L}\|J_{i}\|_{F}^ {2}+\|w\|_{2}^{2}.\]

In the problem described above, \(J_{i}\) again represents the Residual Jacobian of the \(i\)-th residual branch and \(w\) represents the classifier Jacobian,

\[(h_{L+1};_{L+1})}{ h_{L+1}}.\]

It is referred to as the "Unconstrained Jacobians Model" because the Jacobians are not restricted to any specific form and are not required to be realizable by a set of layers. In the Unconstrained Jacobians Model, the Jacobians are regularized through simple functions. However, in reality, weight decay, dropout, normalization layers, and parallel branches regularize the Jacobians in intricate ways that are hard to capture mathematically.

We prove in section B the Appendix the following theorem:

**Theorem 3.3**.: _For binary classification, there exists a global optimum of the Unconstrained Jacobians Model where the top Jacobian singular vectors are aligned (RA2), all Jacobians are rank one, analogous to (RA3), and the top Jacobian singular values are equal, analogous to (RA4)._

Here, the top singular values are equal and not decaying as predicted by (RA4), because the Jacobians are evaluated on the classification boundary instead of on training examples. Therefore, the intermediate representations are not equispaced on a line, as predicted by (RA1) but are rather exponentially-spaced on a line.

## 4 Discussion

In this section, we pose research questions that emerge from the discovery of RA.

GeneralizationThe discovery of RA offers a vivid analogy for comprehending generalization in deep learning. According to RA, we can envision a ResNet as a system of conveyor belts, where each conveyor carries representations of a specific class in a unified direction at a constant speed. Misclassification occurs when the representation mistakenly steps onto the wrong conveyor belt in the initial layers of the network. This perspective leads us to hypothesize that the first few layers of the network play a vital role in generalization, surpassing the significance of subsequent layers. Consequently, more research should be dedicated towards studying the dynamics of the initial layers.

Neural CollapseAs mentioned in Figure 1, there is an intriguing pattern where models exhibit RA concurrently with layer-wise Neural Collapse (Papyan, 2020; Galanti et al., 2022; Li et al., 2023). The concurrent manifestation of these two distinct events could possibly be more than mere coincidence. It would be interesting to explore if RA could shed light on phenomena related to layer-wise Neural Collapse such as the "Law of Data Separation" (He and Su, 2022).

RA in TransformersWe have empirically substantiated the occurrence of RA in ResNets. However, our study has not yet extended to Transformers, which also incorporate residual connections. An intriguing line of inquiry for future work would be to probe whether RA, or a phenomenon akin to it, manifests within these architectures.

Recurrent Architectures Exhibiting RARecurrent neural networks, Neural ODEs (Chen et al., 2018), and deep equilibrium models (Bai et al., 2019) iteratively apply a layer within a deep neural network. This leads to the following questions:

_Do the intermediate representations of these models exhibit RA? If not, can we propose a novel architecture that recurrently applies a computation but does exhibit RA?_

Model CompressionIn our work, we demonstrate that the network can converge to a model that iteratively applies a computation, even without imposing explicit constraints, like the architectures in the previous subsection. This leads us to yet another question:

_Can we replicate the original network's performance by distilling the aligning layers into a single layer and iteratively applying it?_

Regularization TechniquesExisting regularization techniques, including layer permutation (Liaw et al., 2021) and structured dropout (Fan et al., 2019), should strengthen RA. A question arises:

_Could RA explain the success of these methods and do they indeed amplify RA?_

## 5 Related Work

Linearization of Residual JacobiansPrior to our work, Rothauge et al. (2019) proposed the linearization of residual Jacobians and investigated the distribution of their singular values. Our work, however, concerns the discovery of the alignment between the Residual Jacobians, as well as its theoretical understanding.

GeneralizationA thorough empirical investigation by Novak et al. (2018) found that the Frobenius norm of the input-output Jacobian of a network correlates well with generalization. Our research complements theirs by conducting both empirical and theoretical investigations on the _Residual Jacobians_, which form the input-output Jacobian of a ResNet.

ResNets at InitializationPrevious works have contributed significantly to the understanding of the properties of intermediate representations of randomly initialized ResNets, with studies such as Hayou (2022) exploring the infinite-width and finite-width regime and Li et al. (2021, 2022) investigating the infinite-depth and infinite-width regime. Our current research diverges from these previous works by specifically focusing on studying the Residual Jacobians of fully trained ResNets.

Optimization LandscapeLi et al. (2016) analyzed the Hessian of the loss function for a ResNet initialized with zero parameters. Additionally, Lu et al. (2020) use a mean-field analysis of ResNets to demonstrate convergence to a global minimum. Their analysis builds upon the observation that a ResNet is similar to a shallow network ensemble, first noted by Veit et al. (2016).

Rather than focusing on the optimization convergence properties of SGD, our goal is to examine the properties of the intermediate representations and Residual Jacobians that emerge during training.

Neural CollapseRecent theoretical work by Mixon et al. (2020), Lu and Steinerberger (2020), E and Wojtowytsch (2020), Poggio and Liao (2020), Zhu et al. (2021), Han et al. (2021), Tirer and Bruna (2022), Wang et al., Kothapalli et al. (2022) analyzed the Neural Collapse phenomenon, discovered by Papyan, Han, and Donoho (2020), through the unconstrained features model and the layer-pealed model. In these, the assumption is made that the last-layer representations, which are fed to a classifier, have the freedom to move independently and are not constrained to be the output of a deep network. Our Unconstrained Jacobians Model takes inspiration from these mathematical models by abstracting away the complex Residual Jacobians and assuming they can be optimized directly.

Unrolled Iterative Algorithm PerspectiveGreff et al. (2016); Papyan et al. (2017), and Ebski et al. (2018) recognized ResNets as unrolled iterative algorithms performing iterative inference. We build upon this understanding by delving deeper into the empirical and theoretical relationships between the Residual Jacobians.

Neural ODEChen et al. (2018) introduced the Neural ODE: a continuous-depth, tied-weights ResNet. Initially, it may seem unlikely that the iterations or Jacobians of a traditional ResNet would possess any characteristics associated with Neural ODEs. However, Sander et al. (2022) proved that assuming the initial loss is small and the initial parameters are smooth with depth, _linear_ ResNets converge to a Neural ODE as the number of layers increases. Still, it is unclear if these findings extend to _nonlinear_ ResNets trained on _real_ data.

Our work, however, demonstrates that even in such cases the Residual Jacobians align in their top subspaces and, as a result, simple ODEs emerge within these subspaces. We also provide theoretical justification for this claim through the Unconstrained Jacobians Model.

Optimal TransportGai and Zhang (2021) view ResNets as aiming to transport an input distribution to a label distribution, through a geodesic curve in the Wasserstein space, induced by the optimal transport map. They provide empirical evidence to support their claim, showing that intermediate representations are equidistant on a straight line induced by the optimal transport map, and comment: _"Though ResNet approximates the geodesic curve better than plain network, it may not be a perfect implement in high-dimensional space due to its layer-wise heterogeneity."_

Our research demonstrates that intermediate representations lie on a line as a result of the Jacobian singular vectors aligning (RA2) and the Jacobian singular values scaling inversely with depth (RA4) and, in fact, due to the _absence_ of purported "layer-wise heterogeneity."

Analysis of Deep Linear NetworksMulayoff and Michaeli (2020) proved that training a deep _linear network,_ with a _quadratic loss_, and _no weight decay_, necessarily converges to the flattest of all minima. At this optimum point, the spectral norm of the input-feature Jacobian increases exponentially with depth, and the singular vectors of consecutive weight matrices align.

Our theoretical study considers _ResNets_, trained with a _binary cross-entropy loss_, and _weight decay_. Instead of assuming that training has reached a flat minimum, we analyze the Unconstrained Jacobians Model, on the classification boundary in the input space, and prove phenomena that we have observed through experiments on _nonlinear_ ResNets.

Analysis of WeightsCohen et al. (2021) studied the scaling behavior of trained _weights_ in deep residual networks. Our study complements theirs by focusing on examining the _Residual Jacobians_ of ResNet architectures.

## 6 Conclusion

In this paper, we offer a detailed empirical examination of the ResNet architecture, a model that has seen extensive application across a broad spectrum of deep learning domains. Our primary aim was to demystify the mechanisms that contribute to ResNet's remarkable success, a topic that, to date, remains an enigma within the deep learning community.

Our investigation has led to the discovery of a consistent phenomenon, which we have termed RA. The characteristics of RA were observed across a wide array of benchmark datasets, canonical architectures, and hyperparameters, demonstrating its general applicability. Moreover, we conducted counterfactual studies that underscored the critical role of skip connections in the emergence of RA and the effect of the number of classes. In an attempt to theoretically ground the emergence of RA, we proposed the use of an innovative mathematical abstraction - the Unconstrained Jacobians Model, specifically in the context of binary classification with cross-entropy loss.

Our exploration has not only shed light on the intricate mechanisms driving ResNets' performance but also points to connections with the recent phenomenon of layer-wise Neural Collapse. Furthermore, our findings pave the way for future research in the understanding of existing regularization methods, the design of novel architectures, the development of model compression techniques, and the theoretical investigation of generalization.