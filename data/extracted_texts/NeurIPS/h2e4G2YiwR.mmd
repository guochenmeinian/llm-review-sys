# Action Imitation in Common Action Space for Customized Action Image Synthesis

Wang Lin\({}^{1,*}\), Jingyuan Chen\({}^{1,}\), Jiaxin Shi\({}^{2,*}\), Zirun Guo\({}^{1}\), Yichen Zhu\({}^{1}\), Zehan Wang\({}^{1}\),

**Tao Jin\({}^{1}\), Zhou Zhao\({}^{1}\), Fei Wu\({}^{1}\), Shuicheng YAN\({}^{3}\), Hanwang Zhang\({}^{4}\)**

Zhejiang University\({}^{1}\), Xmax.AI\({}^{2}\), Skywork AI Singapore\({}^{3}\), Nanyang Technological University\({}^{4}\)

{linwanglw, jingyuanchen, gzr, yc_zhu, wangzehan01,

jint_zju, zhaozhou, wufei}@zju.edu.cn

shijx12@gmail.com, shuicheng.yan@kunlun-inc.com,

hanwangzhang@ntu.edu.sg

Equal Contribution.Corresponding Author.

###### Abstract

We propose a novel method, **TwinAct**, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (_e.g._, the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors. Project page: [https://twinact-official.github.io/TwinAct/](https://twinact-official.github.io/TwinAct/).

## 1 Introduction

We are interested in customizing Text-Guided Diffusion Models (TGDMs) (_e.g._, Stable Diffusion ) to generate customized actions specified by a few user-provided images. For example, given a few images of _Black Widow_'s signature three-point landing action, we can represent this action as a unique token \(V^{*}\) and generate imaginary creation by the prompt like _"Leonardo \(V^{*}\) "_ (the third row in Figure 1). One might question why we do not condition TGDMs on precise textual descriptions  or sketch images  to generate the desired action. The reason is that actions are often unutterable, and even with finely detailed descriptions (SD column in Figure 1) or skeleton images (ControlNet column in Figure 1), TGDMs cannot accurately follow these instructions.

Further, can we use existing few-shot customized TGDMs [4; 7; 8; 10; 14; 22; 27] to customize actions? Unfortunately, the answer is still no. This is because these methods fine-tune the textual embeddings of \(V^{*}\) in prompts like "_Black Widow \(V^{*}\)_" by reconstructing the corresponding image. But, except for actions, there are many other semantics in the exemplar images that are coupled in \(V^{*}\), like the characteristics of the specific actors. Thus, the few-shot action images in those methodslack an effective inductive bias to achieve decoupled action tokens \(V^{*}\) from actors, which results in action-irrelevant information like _Black Widow_'s red curly hair being confounded in the generated images as shown in Figure 1. Although some methods mitigate the effects of missing inductive bias through contrastive learning on textual embeddings or visual images, they still struggle to exclude all action-irrelevant semantics. For example,  uses prepositions as positive samples and randomly selects negative samples from a limited set of POS words (_i.e._, nouns, adjectives). However, this approach can not guarantee the exclusion of all action-irrelevant information from exemplar images for \(V^{*}\), resulting in the inclusion of irrelevant semantics like _manicure_ and _red curl hair_ (the first and third rows in Figure 1) confounding the customized tokens. On the other hand,  and  utilize contrastive training data involving the same subject with and without specific customized concepts (_e.g,_ melted and closed eyes, etc.) to decouple the subject and customized concepts. However, the creation of extensive contextual training data is costly, particularly when dealing with customized actions that involve multiple subjects.

By observing the real-world process of a director instructing an actor to perform a specific action, such as the signature action of the character _Black Widow_ which involves a set of action phrases including "left hand on the ground", "right hand raised behind the back", "left leg on the ground", and "right leg straight out", we find that these phrases concentrate solely on actions without providing any specific details about the actor involved. Moreover, these phrases can be modified or combined to create new actions, as illustrated in Figure 2. Inspired by these observations, we propose a method called **TwinAct**, which aims to decouple the action from the actor by constructing an effective _Common Action Space_. This space is exclusively for actions, eliminating any confusion with actor-specific characteristics and enabling the generation of diverse and creative actions that can be applied across various actors and scenarios.

Our approach involves three steps. **1)** We collect \(832\) action phrases that cover a range of movements from finger gestures to full body poses, and extract the corresponding textual embeddings using

Figure 1: **Qualitative comparisons of TwinAct with other methods. TwinAct preserves the identity consistency of actors while allowing customized actions to be accurately generalized across different actors by effectively decoupling actions and actors.**

the tokenizer of CLIP . These textual embeddings are used to build a common action space (Section 3.2) through Principal Component Analysis (PCA ); **2**) We employ a Multilayer Perceptron (MLP) to modulate the PCA coefficients to imitate the customized actions within the action space (Section 3.3); and **3**) We introduce an action similarity loss to provide semantic similarity between the exemplar images and generated images, resulting in synthesizing highly adaptable customized action images in diverse contexts (Section 3.3). Extensive experiments demonstrate the superiority of our method over the existing method. Finally, the main contributions of this paper are summarized as:

* We propose **TwinAct** to generate customized action images, which can synthesize novel renditions of user-specific action in different contexts including animals, humans, and even customized actors.
* We introduce a common action space that focuses solely on the action itself, without any actor-specific details. By combining action bases in this space for customized action imitation, we are able to effectively decouple action from actor, allowing for greater flexibility in customization.
* Through extensive experiments, we demonstrate that TwinAct outperforms previous methods in maintaining the fidelity of customized actions while also preserving the consistency of actor identities.

## 2 Related Work

### Image Generation and Customization

The text-guided diffusion models (TGDMs) [15; 19; 20; 21; 23] have emerged as a powerful paradigm in the domain of image generation, enabling the creation of image variations that are aligned with specific textual descriptions. Recently, the customized TGDMs [4; 5; 8; 10; 13; 11; 22] aim to encoder the customized concept within the textual embedding, often represented by a unique token and decode the token into pixels by the cross-attention in the U-net decoder . Textual Inversion  optimizes textual embedding and synthesizes personalized images by integrating the concept token with the target prompt. DreamBooth  extends this concept by proposing a framework that optimizes all parameters of the denoising U-Net architecture, based on a specific token and the class category of the subject. Several other works [5; 10] have focused on optimizing subsets of weights or introducing additional adapters to achieve more efficient optimization and better conditioning of the generated images. For instance, Custom Diffusion  fine-tunes only the cross-attention layers within the U-Net, while P+  expands the textual-conditioning space with per-layer tokens to allow for greater disentanglement and control over the generation process. Despite these methods achieving commendable results in customized actors, they struggle with generating customized action images, as shown in Figure 1, due to the lack of an effective inductive bias with few exemplar images.

### Customized Action Image generation

In contrast to the rapid progress in customized actors, customized action has received less attention in the community. Since the complex details such as specific angles, body positions, and motion trajectories, describing a specific action is more challenging than describing a specific actor. Even with finely detailed descriptions (see Appendix A.1) TGDMs still cannot accurately follow these textual descriptions. A straightforward approach to generate a specific action image is ControlNet  which is conditional on a given skeleton image to generate images. However, they only provide a rough approximation of action consistency (see Appendix A.3). Particularly, when it comes to capturing fine details such as fingers, skeleton images may lack the necessary detail to accurately reproduce these actions, leading to noticeable visual defects in the generated images. In addition, when conditioned on detailed images (see Appendix A.3), they suffer from limited diversity and flexibility. Recent advancements for customized TGDMs preliminary explore action-based customization. Reversion  proposes relation-steering contrastive learning that aims to guide the relation prompt towards relation-dense regions within the text embedding space, thereby disentangling the learned relation from other concepts like appearances. Lego  and ADI  construct contexts involving the same subject with and without a specific concept, to decouple the subject and specific concepts. However, these methods by employing contrastive learning on textual embeddings or visual images can not effectively decouple actions from actors. This is because they cannot exclude all irrelevant concepts with a limited number of negative samples.

## 3 Methods

Imagine the following scenario: When the character _Black Widow_ performs her signature three-point landing action, the director may give the following instruction "_Bend knees and lower body down into a squatting position; Place right hand forward, and right arm should be fully extended_". We can see that when people are asked to perform a specific action, it is often accompanied by instructions that involve the composition of various body movements and the adjustment of angles and amplitudes as shown in the right part of Figure 2. Inspired by the way in which humans perform specific actions, we propose **TwinAct** which applies similar principles to action customization. Specifically, we first collect a set of basic actions and encode them using a pre-trained text encoder to build a common action space represented by a set of action bases. Subsequently, we imitate the customized action by decomposing it into a combination of action bases with corresponding weights. Finally, we utilize both the reconstruction loss and action similarity loss to fine-tune the customized token embedding in the text encoder and the LoRA  layer in the text-to-image decoder to generate high-fidelity images of the customized action.

### Preliminaries

Our study is based on the text-guided diffusion models (TGDMs) which consist of a textual encoder \(\) and a text-to-image decoder \(\). Given a few sample action images \(\) and a textual prompt \(c\) with a customized token \(V^{*}\), such as "a photo of \(V^{*}\)", we aim to fine-tune the embedding of customized token \(V^{*}\) in \(\) and the LoRA layer in \(\) to realize action customization. The textual encoder \(\) first tokenizes the prompt \(c\) into a set of token embeddings \(\), which are then used to generate a textual condition \(_{}(c)\) with the text transformer in \(\). Subsequently, the textual condition \(_{}(c)\) is utilized by the conditional denoising diffusion model \(_{}()\) in \(\) to generate images. The commonly used training objective is:

\[_{rec}=_{,t,_{0},c}[\|- _{}(_{t},t,_{}(c))\|^{2}] \]

where \(t\) is the timestep, \(_{0}\) is the original image and \(_{t}\) is a noisy image constructed by adding noise \((0,1)\) to \(_{0}\). After training, any textual condition \(_{}(c)\) with \(V^{*}\) can generate customized action in a new context defined by prompt \(c\).

### Step-1: Building Common Action Space

The initial step involves identifying the set of action bases that defines the common action space within the text-to-image diffusion model.

First, we task GPT-4  with generating a range of common action phrases based on different body parts, including head, fingers, hands, and full body (see Appendix B.2). Then, we carefully sift through the generated action phrases to eliminate duplicates. Subsequently, we build a manual filter based on the pre-trained text-to-image diffusion model by constructing prompts and synthetic images

Figure 2: **The construction of Common Action Space.**of each action phrase to remove those action phrases that are unknown to the text-to-image diffusion model, _e.g._, "Reverse Plank" as shown in Figure 2.

After the filtering process, we identify a total of \(m=832\) action phrases. Each phrase \(a_{i}\), where \(i\{1,...,m\}\), is tokenized and encoded into an action embedding group \(A_{i}=[a_{1}^{i},...,a_{k}^{i}]\). It is important to note that the length \(k\) of each action embedding group \(A_{i}\) varies, as each action may consist of a different number of words. Through empirical observation, it was found that action phrases with three words or fewer are the most common, prompting the designation of each \(A_{i}\) as containing three embeddings (_i.e._, \(k=3\) for all \(m\) actions). For simplicity, we denote the three embeddings of all \(A_{i}\) as \(Q_{1}\), \(Q_{2}\), and \(Q_{3}\) respectively, where \(Q_{k}=[a_{k}^{i},...,a_{k}^{n}]\).

Inspiring by prior research  employing PCA to transform high-dimensional facial data into a lower-dimensional space, we calculate the PCA transformation matrix \(B_{k}\) for each set \(VQ_{k}\) using the function PCA\((Q_{k},p)\). This transformation plays a crucial role in reducing redundant information within the data and enhancing the efficiency of model optimization. Specifically, PCA\((Q_{k},p)\) reduce dimensionality of \(Q_{k}^{m d}\) and generate \(p\) principal components as \(B_{k}=[b_{k}^{1},,b_{k}^{p}]^{p p}\), which captures the essential features of data in a more compact form. Our experimental findings indicate that optimal results are achieved when \(p=512\) (see Section 4.3). Finally, we obtain a set of action bases as \([B_{1},B_{2},B_{3}]\), which define an action space that does not contain any action-irrelevant information thus providing a strong inductive bias to achieve decoupled action token embeddings.

### Step-2: Imitating Action via Action Bases

This step involves searching for the optimal parameters to combine the action bases for imitating customized actions.

Our experiments (see Figure 1) have shown that existing methods, which aim to identify optimal personalized token embeddings within the vast text space of pre-trained text encoders, struggle with the inclusion of non-action information. And even with contrastive learning, they are still unable to exclude all irrelevant information. Fortunately, action bases offer an alternative approach for optimizing the customized token embedding. By combining action bases \([B_{1},B_{2},B_{3}]\) with corresponding coefficients \([W_{1},W_{2},W_{3}]\), where \(W_{k}=[w_{k}^{1},,w_{k}^{p}]\), new actions can be imitated as shown in Figure 2. To achieve this, we first utilize a Multi-Layered Perceptron (MLP) to combine the action bases. While backpropagation is a common method for determining optimal coefficients, our experiments indicate its limited effectiveness due to the scarcity of user-provided images (see Section 4.3). Therefore, we use CLIP as an action encoder to extract semantic features as a prior. Specifically, given an action sample image \(\), we first extract the action features \(_{p}\) with action encoder \(()\), then MLP is used to map \(_{p}\) into the modulating coefficients \([W_{1},W_{2},W_{3}]\) as:

\[W_{k}=(_{p}) \]

Finally, for a customized action, we can combine the action bases to imitate the action as:

\[=[_{1},_{2},_{3}],_{k}=_{j=1}^{p}w_{k}^ {j}b_{k}^{j} \]

Figure 3: **The overview of the proposed TwinAct.** We optimize the coefficients of the action bases to avoid encoding the action-irrelevant features. After training, we combine the learned coefficients and shared action base to generate images with the customized action.

where \(\) is the embedding of the customized tokens \(V^{*}\). In this way, \(\) is a linear combination of the action bases. Consequently, the vector interpolated in the action space would not contain any action-irrelevant features, allowing us to achieve the objective of decoupling the action and the actor.

### Step-3: Generating Customized Action

The final step involves optimizing the embeddings of the customized token \(V^{*}\), which is derived from the action base through linear combinations and synthesis customized action image.

**Training.** Initially, a few user-provided images and corresponding textual prompts containing \(V^{*}\) are used to apply the pixel reconstruction-based loss \(_{rec}\), similar to previous customized TGDMs (in Eq 1). However, the traditional \(_{rec}\) in existing TGDMs aims to faithfully reconstruct the sample image appearance, focusing on low-level pixel features. This narrow focus limits the model's ability to recognize and invert high-level action features in the images, as shown in Figure 7(b).

To address this limitation, we introduce an action similarity loss to enhance the model's grasp of high-level action semantics. Beginning with an input image \(\) and a corresponding prompt \(c\), we randomly initialize a latent variable \(_{T}\). This variable is progressively denoised until it reaches a randomly selected timestep \(t\), at which point the denoised image \(^{prd}\) is predicted directly from \(_{t}\). The goal is to evaluate the action similarity between these two images. Specifically, by leveraging action encoder \(()\), the encoded action features for both the reference action \(_{p}\) and the generated action \(_{p}^{prd}\) are extracted. We then calculate the cosine similarity between these action features to measure action consistency during the generation process. This similarity is formally defined as:

\[_{act\_sim}=_{c p(c)}_{x p (x|c)}[1-(_{p},_{p}^{prd})] \]

By integrating the action similarity loss, the model's focus shifts from detailed pixel information to high-level action semantics. The final training loss for TwinAct can be expressed as:

\[=_{rec}+_{act\_sim} \]

**Testing.** After training, the three groups of coefficients \([W_{1},W_{2},W_{3}]\) and the LoRA parameters in the text-to-image decoder are saved. Users can generate images depicting customized actions in different contexts by providing prompts such as _"Leonardo \(V^{*}\)"_.

## 4 Experiment

### Experiment Setup

**Dataset.** We tackle the challenge of decoupling specific actions from the actors in user-provided images. Since there are no publicly available customized action datasets, we introduce a novel benchmark, consisting of \(12\) actions involving multiple body parts, such as fingers, arms, legs, and full-body motions. Each action contains approximately \(10\) sample images. In addition to actions performed by a single actor, we incorporate more complex actions that involve multiple actors.

**Metrics.** To comprehensively validate the efficacy of TwinAct, we incorporate objective metrics to assess the quality of the generated images. We calculate the consistency between the action in the sample image and the generated image through the CLIP score, which is denoted as "\(S_{Action}\)". Additionally, ensuring identity consistency is a crucial aspect of our task, so we evaluate the actor's identity similarity using a pre-trained face recognition encoder , which is denoted as "\(S_{actor}\)". In addition, we also conduct user studies including "\(U_{Action}\)" and "\(U_{Actor}\)", representing user preferences for action fidelity and actor identity consistency in the generated images.

### Comparison with State-of-the-art Methods

**Qualitative Comparison.** We illustrate the advancements of our method compared to previous approaches in handling various forms of actions, including hand, body, single-actor, and multi-actor actions, as shown in Figure 1. Initially, we evaluate two methods for generating customized action images without fine-tuning, as shown in the column of SD and ControlNet in Figure 1. Images generated from detailed textual prompts (SD) frequently exhibit misalignment with the actions in the sample images. On the other hand, Controlled Generative Models (ControlNet), which depend on Figure 4: **The results of customized action generation with TwinAct.** TwinAct generates images of different actors performing customized actions such as celebrities and animals, and maintains the consistency of the action and identity of the subject.

skeleton images, tend to only imitate the basic structures of the sample images and lack a profound understanding of the contextual dependencies required for action customization. This deficiency leads to significant body deformations and flaws, particularly when animals are involved in the action. For example, in the fifth and sixth rows of Figure 1, both the human and the gorilla mistakenly raise their hands instead of their feet.

Subsequently, we conduct comparisons with state-of-the-art baselines such as Textual Inversion , DreamBooth , Reversion , Custom Diffusion , P+ , and ADI . Textual Inversion and DreamBooth tend to capture minimal action-related information, primarily because of their limited tunable parameters. On the other hand, Custom Diffusion, which fine-tunes cross-attention in U-net, and P+, which utilizes per-layer token embeddings, manage to retain more information from the sample images. However, they uncontrollably invert action-irrelevant details in the sample images and bring these details into the generated image, such as the human hand and the manicure in the first and second rows shown in Figure 1. In addition, Reversion and ADI try to exclude action-irrelevant information from the images by contrastive learning but face challenges in comparing all negative samples. Therefore, the images they generate still contain some confounded details, such as the red curly hair of the _Black Widow_ in the third row and the yoga pants in the sixth row. In contrast, TwinAct shows the best customized action generation results. Through the acquisition of a customized action token embedding, which is derived from a linear combination of action bases within the common action space, TwinAct accurately capture the action information in the sample images while excluding irrelevant details with good generalization.

**Quantitative Comparison.** In addition to the visual quality, we also present a numerical comparison between TwinAct and the baselines in Table 1. As can be seen, our approach outperforms in terms of both action and actor similarity, demonstrating its effectiveness in imitating customized actions and filtering out irrelevant information that could potentially confuse the portrayal of the new actor. Furthermore, the performance across four representative types of action customization tasks (_i.e._, Single-Actor, Two-Actor, With-Human, With-Animal) is illustrated in Figure 6. Our method consistently shows superior performance and the least variability across all types. Notably, the method labeled as P+ experiences the most significant performance drop when involving animals, and ADI exhibits a considerable performance drop when handling actions involving two actors.

Additionally, following the methodology outlined in , we conduct a quantitative comparison involving human evaluators to further validate the effectiveness of TwinAct. We invite \(100\) users to rate each pair of actor-action images generated by TwinAct and baselines on a scale ranging from 1 (worst) to 5 (best), focusing on both action and actor consistency. The collective feedback, as presented in Table 1, clearly indicates a strong preference for TwinAct among the users.

    & Methods & Text Inversion & DreamBooth & Reversion & Custom Diffusion & P+ & ADI & Ours \\  Objective & \(S_{Action}\) & 9.12 & 12.23 & 18.73. & 26.83 & 33.95 & 45.32 & **69.47** \\  & \(S_{Actor}\) & 33.85 & 38.53 & 45.58 & 41.58 & 46.12 & 48.76 & **73.34** \\  User & \(U_{Action}\) & 0.22 & 0.19 & 0.89 & 1.16 & 1.30 & 2.25 & **3.86** \\ Study & \(U_{Actor}\) & 0.53 & 0.70 & 1.05 & 1.31 & 1.23 & 2.08 & **4.12** \\   

Table 1: **Quantitative comparisons.** We evaluate TwinAct based on both objective metrics and user study. The results indicate that TwinAct surpasses all baseline methods.

Figure 5: The results of **customized actors** performing **customized actions** generated by TwinAct.

[MISSING_PAGE_FAIL:9]

Conclusion

In this paper, we introduce a novel approach, TwinAct, for customized action image generation, which aims to preserve the fidelity of the action and the consistency of the actor's identity. TwinAct consists of a common action space and can create new actions by adjusting or combining the action bases in the space. By imitating action via action bases, TwinAct effectively decouples action from the actor. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions across various subjects, including humans, animals, and customized actors. Furthermore, TwinAct's ability to preserve the high fidelity and identity consistency in generated images highlights its robustness and adaptability. The potential applications of TwinAct in areas such as animation, gaming, and visual effects are vast, offering new opportunities for creative and personalized content generation.

## 6 Acknowledgements

This work was supported by the National Natural Science Foundation of China (No.62307032, No.62037001) and the Key Research and Development Program of Zhejiang Province (No. 2023C03192).