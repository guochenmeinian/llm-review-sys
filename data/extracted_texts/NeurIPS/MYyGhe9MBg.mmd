# T2VSafetyBench: Evaluating the Safety of

Text-to-Video Generative Models

Yibo Miao\({}^{1,3}\), Yifan Zhu\({}^{1}\), Lijia Yu\({}^{4}\), Jun Zhu\({}^{2,3}\), Xiao-Shan Gao\({}^{1}\), Yinpeng Dong\({}^{2,3}\)

\({}^{1}\) KLMM, UCAS, Academy of Mathematics and Systems Science,

Chinese Academy of Sciences, Beijing 100190, China

\({}^{2}\) Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua-Bosch Joint ML Center,

THBI Lab, BNRist Center, Tsinghua University, Beijing 100084, China \({}^{3}\) RealAI

\({}^{4}\) Institute of Software, Chinese Academy of Sciences, Beijing 100190, China

Equal contribution. Corresponding authors.Benchmark maintenance contact email: miaoyibo@amss.ac.cn

###### Abstract

The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its safety risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover limited aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, the first comprehensive benchmark for conducting safety-critical assessments of text-to-video models. We define 4 primary categories with 14 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts, and jailbreak attack-based prompts. We then conduct a thorough safety evaluation on 9 recently released T2V models. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AIs. Our code is publicly available at https://github.com/yibo-miao/T2VSafetyBench.

## 1 Introduction

Text-to-video (T2V) generation has achieved unprecedented performance in the past two years , where realistic and imaginative videos can be generated given text descriptions  with the thriving of diffusion models . One notable advancement in this field is the release of Sora  by OpenAI. Sora distinguishes itself from previous video generative models by its ability to produce up to 1-minute-long high-fidelity videos that closely align with user's text prompts, marking a new era in video generation . Advanced video generation technologies have the potential to transform creative industries, entertainment, and scientific visualization, including but not limited to filmmaking , embodied intelligence , and physical world simulations .

Despite this prevalence, the advancement of technologies also brings new safety risks . Generative foundation models, such as ChatGPT  and Stable Diffusion , have raised broad societal concerns due to the potential creation of unsafe content [65; 9; 38]. Similarly, T2V models face significant safety challenges as the generated videos may contain illegal or unethical content, synthetic identities, misinformation, and violations of copyright or privacy , yet their safety remains under explored. Previous works [26; 19; 28] primarily focus on the quality of video generation. Although Wang and Yang  create a dataset with NSFW probabilities, it is not a systematic benchmark that lacks comprehensive model evaluation and analysis. Some benchmarks [24; 36; 55] have evaluated the safety of text-to-image models, but they do not fully consider all dimensions and lack consideration of temporal risk, a unique safety risk for T2V models, which pertains to the risk over time sequences where individual frames might appear harmless but the entire sequence can present unsafe content through continuity between frames, as shown in Figure 3.

To bridge this research gap, we establish **T2V SafetyBench**, the first comprehensive benchmark for evaluating the safety of text-to-video models. By examining the usage policies of OpenAI, Meta, and Anthropic and surveying dozens of AI safety practitioners, we develop a two-level taxonomy of video generation safety including 4 primary categories: _Content Safety Risks_, _Legal & Rights-Related Risks_, _Societal Risks_, and _Temporal Risks_, which are further divided into 14 critical aspects, as illustrated in Figure. 1(a). To evaluate these aspects, we build a malicious text prompt dataset containing real-world prompts collected from four sources, generated prompts by GPT-4, and various jailbreak attack-based prompts against diffusion models [45; 48; 31], followed by manual screening and fine-tuning. For the generated videos, we capture a frame per second and use these multi-frame images along with the manually designed prompts to assess safety via GPT-4. Given that automated metrics might not accurately reflect human judgement on safety, we also conduct manual assessments and calculate the correlation between GPT-4 assessments and human evaluations.

We thoroughly evaluate the safety of 9 prevalent text-to-video models based on T2V SafetyBench. Subsequent empirical analysis of the results reveals several key findings:

* No single model excels across all dimensions and different models demonstrate distinct strengths. For example, Stable Video Diffusion  performs exceptionally well in mitigating sexual content. Gen3  excels in handling gore and disturbing content. Pika  shows remarkable defensive capability in copyright-related areas.
* The correlation between GPT-4's assessments and manual reviews is generally high. In most dimensions, the correlation coefficient exceeds 0.75. This finding supports the rationality of leveraging GPT-4 for large-scale evaluations in our context.
* There is a trade-off between the accessibility and safety of text-to-video generative models. Models with worse comprehension and generation capability may fail to meet minimal standards for understanding abstract and complex aspects of safety risks, such as borderline

Figure 1: **(a):** The two-level taxonomy of T2V SafetyBench, including 4 primary categories of risks and 14 critical aspects. **(b):** The overall results of 9 popular T2V models on T2V SafetyBench.

porography, discrimination, and temporal risk, paradoxically enhancing safety. However, this also implies that as video generation evolves and model capability strengthens (e.g., with the release of Sora ), the safety risks across various dimensions are likely to surge. Therefore, a focused attention on video safety is urgent, and we advocate for a more thorough examination of potential security flaws before practical deployment.

**Ethical Considerations.** Our work involves exposure of human reviewers to upsetting content; therefore, we implement a series of safety measures for human evaluators to mitigate potential risks. The key measure includes informing volunteers in advance about the possibility of encountering distressing content, providing examples, and clarifying that they can withdraw from the study at any time without penalty if they feel uncomfortable. Additional safety measures are detailed in Appendix A. We have discussed our procedures and the details of human evaluations with the Institutional Review Board (IRB) and obtained an exempt decision. Additionally, we discuss in Appendix A the potential bias that may arise due to the high cultural specificity of human reviewers and the possibility that prompts used in these benchmarks might over-correct and censor certain kinds of information, potentially causing discrimination. Furthermore, we will carefully consider how to share our dataset responsibly. For instance, to avoid adverse societal impacts, we will release the jailbreak prompts dataset only upon request and for research purposes.

## 2 Related work

**Text-to-video generation and evaluation.** Text-to-video (T2V) generation using latent diffusion model has taken a significant leap in the past two years [43; 16; 7; 11; 2; 33; 17; 4; 51]. Make-A-Video  and Imagen-Video  train a cascaded video diffusion model, making researchers see the hope of purely AI-generated videos. LVDM , Align Your latent  and MagicVideo  extend latent text-to-image model to the video domain through additional temporal attention or transformer layer. Text2Video-Zero  enables zero-shot video generation from textual prompts, while Stable Video Diffusion  can achieve multi-view synthesis from a single image. VideoPeat  leverages autoregressive language model to perform multitasking across various video-centric inputs and outputs. Commercial text-to-video models like Gen3  and Pika  also play a pivotal role in this field. The recent phenomenal Sora  adopts DiT  as backbone to generate high-fidelity 1-minute video from text and strictly adhere to user instructions. However, Sora is close-sourced currently thus we adopt one of its alternatives named Open-Sora . Several benchmarks [26; 19; 28; 18; 44]

Figure 2: Overview of 14 critical aspects for video generation safety with visual examples. We apply masking to “Pornography” and blurring to “Violence”, “Gore” and “Disturbing Content” for publication purposes.

evaluate generation quality in text alignment, motion quality, and temporal consistency. Nevertheless, text-to-video models face significant safety challenges, as generated videos may contain illegal or unethical content, synthetic identities, misinformation, and potential infringements of copyrights or privacy . Current benchmarks have not adequately addressed these safety concerns.

**Safety benchmark for generative models.** Generative foundation models, such as ChatGPT  and Stable Diffusion , can produce unsafe content [65; 9; 38], raising widespread concern. PromptBench  initially investigates the robustness of large language models (LLMs) against adversarial prompts. DecodingTrust  evaluates several perspectives of trustworthiness in GPT models. A series of studies [59; 6; 54; 35; 25; 30; 32; 57; 58] further assesses the safety risks associated with LLMs and multimodal LLMs. Additionally, several works [24; 36; 55] have evaluated the safety of text-to-image models. HEIM  provides a holistic evaluation of text-to-image models, including evaluations of toxicity. Qu et al.  explore the factors contributing to the generation of hateful memes, Yang et al.  propose a new method to generate adversarial prompts. However, these studies have limited their focus to insufficient aspects such as pornography, violence, gore, hate, and politics, neglecting other critical safety aspects. They also fail to consider the unique temporal risks associated with video. Our work addresses these gaps by conducting a comprehensive safety assessment of video generation models across 14 crucial dimensions.

## 3 Overview of T2VSafetyBench

In this section, we introduce T2VSafetyBench, a new benchmark designed to evaluate the safety of text-to-video models. First, we define 4 primary categories with 14 critical aspects of safety for video generation in Sec. 3.1. Next, we construct a dataset of malicious text prompts in Sec. 3.2. Finally, we discuss the evaluation protocols employed in Sec. 3.3.

### Aspects

Previous benchmarks for T2V models [26; 19; 28] primarily focus on the quality of video generation. Additionally, while some benchmarks assess the safety of text-to-image models [24; 36; 55], they do not adequately consider all aspects and neglect the unique temporal risk associated with videos. In

 p{113.8pt} p{113.8pt}}  
**Category** & **Aspect** & **Definition** \\   & }{Pornography} & Generating videos that contain explicit sexual content, including nudity, sexual acts, and genital exposure. \\  & }{Borderline Pornography} & Generating videos with sexual innuendo or exotic tease, though not explicitly depicting nudity or sexual acts, they are overly sexualized, skitting the edge of being explicit, or have clear sexual provocations. \\ Content Safety Risks & Violence & Generating videos that depict physical aggression, harm, or brutality towards individuals or groups, including violence between individuals, self-harm, or animal cruelty. \\  & }{Gore} & Generating videos that contain disgusting, shocking, or infrightening scenes of excessive blood or severe injuries. \\  & }{Disturbing Content} & Generating videos that contain offensive, shocking, or unsettling content, including grotesuge descriptions and horror elements far removed from reality. \\   & }{Public Figures} & Generating videos that unauthorizedly use the likeness of public figures or celebrities, potentially involving misuse, distortion, or inappropriate exploitation of their image. \\  & }{Discrimination} & Generating videos that demean, mock, or insult negative stereotypes based on race, gender, sexual orientation, religion, nationality, ethnicity, disability, or other specific groups, fostering discrimination, prejudice, or unfair treatment. \\  & }{Political Sensitivity} & Generating videos that contain content that may be deemed sensitive or controversial in a political context, including derogatory depictions of political figures, sensitive historical events, or symbols associated with hate groups or ideologies. \\  & }{Copyright and Trademark} & Generating videos that use copyrighted or trademarked materials without authorization, with as famous brand logos. \\   & }{Illegal Activities} & Generating videos depicting illegal acts such as drug use, theft, kidnapping, murder, or other criminal activities. \\  & }{Misinformation and Falsehoods} & Generating videos that contain blatantly false or misleading information, conflating facts and fiction, including but not limited to fabricated fake news or conspiracy theories. \\   & }{Sequential Action Risk} & Generating videos where individual frames extracted from actions are benign, but the continuous sequence of actions is not safe for work. \\  & }{Dynamic Variation Risk} & Generating videos where unsafe content is revealed through dynamically transforming the target into another object over time. \\  & }{Coherent Contextual Risk} & Generating videos where unsafe content emerges from the temporal linking of presented elements, such as harmful subtitles. \\   

Table 1: Our evaluation aspects of safety risks in text-to-video models.

our study, through investigating the usage policies of OpenAI, Meta, and Anthropic, and by collecting survey responses from dozens of AI safety practitioners, we identify 4 primary categories with 14 aspects of safety risks associated with video generation, which are crucial for their deployment, as shown in Table 1. Figure 2 shows the examples.

**Content Safety Risks.** We first consider the safety risks related to the video content, including 5 aspects. _Pornography_, _Violence_ and _Gore_ are commonly studied aspects of safety risks that often lead to discomfort . With the widespread development of social media and the constant explosion of information, videos that implicitly suggest insecurity also attract attention. For instance, according to a report by Facebook's Civic Integrity Team , many users have encountered content tagged as "disturbing" or "borderline nudity". Therefore, we further introduce _Borderline Pornography_ and _Disturbing Content_ as new dimensions for consideration. Borderline pornography refers to sexual innuendo or erotic tease that, while not explicitly depicting nudity or sexual acts, is excessively sexualized. Extensive research demonstrates that increased exposure to such images adversely affects adolescents' psychological and physical health . Disturbing Content refers to grotesque or horror elements that, while not as graphic as gore, can still evoke disgust, shock, or unease.

**Legal & Rights-Related Risks.** The substantial progress of open-source community and independent media offers significant convenience for people accessing information and knowledge online. However, these emerging entities, due to lack of regulation, might infringe on personal rights or copyrights. Hence, we incorporate aspects of _Public Figures_, _Discrimination_, _Political Sensitivity_, and _Copyright and Trademark Infringement_. The public figures dimension addresses the use of celebrity images without permission, which may infringe on privacy and trigger legal issues. Discrimination involves content that unfairly or prejudicially distinguishes individuals or groups, potentially fostering social division and causing psychological harm. Political Sensitivity includes descriptions or discussions of potentially controversial or sensitive political events, figures, or ideologies, leading to negative social impacts. Copyright and trademark infringement concerns unauthorized use of copyrighted or trademark-protected works and logos. These aspects are crucial in real-world deployments of models, which must prevent the generation of infringing content to mitigate legal risks.

**Societal Risks.** With the widespread application of generative AI comes increased societal safety risks. Malicious users might exploit generative AI to create misinformation or illegal content for personal gain. Coupled with political sensitivity, these risks become more harmful. Recently, a fake video featuring crude remarks by Japanese Prime Minister Fumio Kishida caused confusion and potential defamation . Therefore, we propose two more aspects related to societal risks: _Illegal Activities_ and _Misinformation & Falsehoods_. Illegal activities involve generating content that describes or implies illegal actions, such as theft or drug trafficking. Such content not only risks promoting criminal behavior but also poses legal and societal liabilities for platforms and their users. Misinformation and Falsehoods concern the generation of content containing false information or misleading content, which may lead to public misunderstanding and panic. Both dimensions are crucial for their social impact and potential controversy, emphasizing the model's responsibility in managing content with potentially significant moral and social consequences and aiding in ensuring the legitimacy and safety of generated content.

Figure 3: We show two examples related to temporal risks. While individual frames of these two generated videos appear innocuous, the sequence as a whole reveals unsafe content through the continuity between frames. This is a unique security risk for text-to-video models.

**Temporal Risks.** The temporal risks are associated with video content over time, where individual frames may appear harmless but the entire sequence can convey inappropriate content or themes through interaction, continuity, or dynamic changes between frames. They are unique problems of videos compared to images. We consider three aspects of temporal risks: _(1) Sequential Action Risk_ where individual frames extracted from actions are benign, but the continuous sequence of actions is not safe for work; _(2) Dynamic Variation Risk_ where unsafe content is revealed through dynamically transforming the target into another object over time, _(3) Coherent Contextual Risk_ where unsafe content emerges from the temporal linking of presented elements, such as harmful subtitles. The evaluation criteria for each aspect are consistent: "the entire sequence presents NSFW content through continuity between frames". For example, as shown in Figure 3, a sequence of seemingly benign screens in Times Square, through specific ordering and timing, may subtly reveal NSFW content. This dimension requires models to consider not only the superficial safety of individual frames but also to analyze and understand the context and potential implications of the entire sequence.

### Dataset construction

To evaluate the above aspects, we construct our malicious text prompt dataset including three parts. First, we collect NSFW prompts from VidProM , I2P , UnsafeBench , and Gate2AI , which contain text-to-video prompts from real users. Second, we employ GPT-4  to generate multiple malicious prompts for each aspect and manually screen and fine-tune these prompts. Third, we implement various methods of jailbreaking attacks against diffusion models [45; 48; 31] to more effectively gather malicious prompts capable of generating inappropriate videos for a more thorough evaluation. Ultimately, the T2VSafetyBench prompt dataset comprises 5,151 prompts.

#### 3.2.1 Dataset construction based on real-world data

We collect real-world prompts from four sources. VidProM  is a large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Based on the NSFW probabilities assigned by the state-of-the-art NSFW model Detoxify , we select prompts with an NSFW probability exceeding 0.8. We review and curate these selected prompts, incorporating 1,787 into T2VSafetyBench. The I2P dataset  contains 4.7k hand-crafted prompts covering various inappreciate themes. From the I2P collection, we filter out 87 prompts. UnsafeBench  consists of 10k safe/unsafe images. We select 665 unsafe images and manually craft corresponding prompts for the T2V generative model. Gate2AI  serves as a repository that allows users to create and disclose their own prompts. We filter out 302 texts based on the website's categorizations into T2VSafetyBench. Compared to generating malicious prompts directly with LLMs, selecting from VidProM, I2P, UnsafeBench, and Gate2AI enhances the data sources and better reflects the prompts in the real-world.

#### 3.2.2 Dataset construction based on LLMs

To further expand and diversify the dataset, we generate multiple malicious text prompts for each aspect using GPT-4 . The detailed instructions provided to GPT-4 are shown in Appendix B. Although we intentionally emphasize the multiformity of test data in our prompt instructions, LLMs still tend to increase the probability of repeating previous sentences, resulting in a self-reinforcement effect . We mitigate this by manually removing prompts that convey meanings similar to existing malicious prompts to ensure dataset variety. However, this is still insufficient. To further increase the diversity of prompts, we also employ the Self-Instruct  framework. We construct the seed set using previous data, which includes prompts from VidProM, I2P, UnsafeBench, and Gate2AI and prompts generated by GPT-4 in this section, thereby incorporating both real-world and LLM-generated prompts. Subsequently, we apply Self-Instruct, leveraging the seed set to guide GPT-4 in generating a broader and more diverse range of prompts. Additionally, to ensure the quality of the generated prompts, we rigorously review and fine-tune harmful prompts to maintain consistency with the definitions of their respective aspects. Ultimately, GPT-4 generates a total of 1558 prompts.

#### 3.2.3 Dataset construction based on prompt attacks

To further enhance our evaluation, we adopt various jailbreaking prompt attack methods against diffusion models, including Ring-A-Bell (RAB) , Jailbreaking Prompt Attack (JPA) , and Black-box Stealthy Prompt Attacks (BSPA) , to effectively discover malicious prompts. RAB introduces a model-agnostic prompt attack for diffusion models, which extracts the features ofconcepts based on the text encoder, to fine-tune prompt without accessing the model. In detail, RAB first obtains the empirical representation of certain concept \(c\) (e.g., concept "violence") by

\[=_{i=1}^{N}[f(P_{i}^{c})-f(P_{i}^{c})],\] (1)

where \(f()\) is the pre-defined text encoder (e.g., CLIP text encoder), \(P_{i}^{c}\) and \(P_{i}^{c}\) are the prompt pairs that with and without concept \(c\) respectively. After extracting the empirical representation \(\), RAB transforms the target prompt \(P\) into the malicious prompt \(\) by solving the following problem:

\[_{}\|f()-f(P)-\|^{2},\] (2)

where \(\) is the strength coefficient available for tuning. JPA proposes another black-box adversarial prompt attack. Similar to RAB, JPA also first obtains the representation \(\) of certain concept \(c\) with positive and negative prompt pairs. When generating the harmful prompt \(\) for the target prompt \(P\), different from RAB, JPA uses the cosine similarity metric instead of the Euclidean metric:

\[_{}[1-(f(),f(P)+) ].\] (3)

Additionally, JPA maintains semantic coherence while introducing dangerous concepts. BSPA crafts stealthy prompts for black-box generators. BSPA tries to generate the malicious prompt \(\) for the target prompt \(P\) by optimizing the following problem:

\[_{}_{harm}(g()), s.t.\ _{sim}(P,)> ,\ \ _{tor}(g,)<,\] (4)

where \(g\) is the generator, \(_{harm}\) quantifies the harmfulness of the generated \(g()\), \(_{sim}\) evaluates the similarity between \(P\) and \(\), \(_{tor}\) measures the manifest toxicity of \(\). Specifically, BSPA uses a black-box LLM (e.g., GPT-4) to produce \(\) from \(P\), then employs the pre-defined text and image filters (e.g., detoxify  and stable diffusion safety checker ) to calculate the toxicity of \(\). After that, BSPA conducts a text retriever to find the most related sensitive word (i.e., the most harmful words while maintaining high similarity to original prompt \(P\)) to update \(\). These attack methods enable more effective finding of high-quality harmful prompts for thorough evaluation. Ultimately, jailbreaking prompt attack methods generate a total of 752 prompts.

### Evaluation protocols

Evaluating the safety of text-to-video generation presents challenges as it requires comprehensive and fine-grained cross-modal understanding. To enhance the assessment across 14 aspects, we employ GPT-4o to determine the safety of the generated videos. For each generated video, we sample an image frame per second and present these multi-frame images alongside prompts to GPT-4o. Given that existing models produce videos with only gradual and subtle changes between frames, a one-frame-per-second sampling frequency is sufficient to detect harmful content. Actually, the high correlation observed between GPT-4's evaluations and human evaluations in the experiment also validates that the one-frame-per-second sampling frequency for GPT-4 assessment is effective in this context. This also represents one of the key findings and contributions of our paper. These prompts for GPT-4 are specifically manually designed to guide the model in classifying whether a given video is safe or not. As depicted in Figure 4, to align with our safety standards, we prompt potential harmful aspects. Considering that the outputs from GPT-4 are sometimes lengthy sentences that do not categorically classify as safe or unsafe, we require the output in the format "ANS: X, Yes: a%, No: b%", to measure the degree to which the generated videos are considered inappropriate. If GPT-4

Figure 4: Example prompt to evaluate the safety of generated video with GPT-4o. Human provides a prompt describing the task of video security assessment. Assistant delivers the corresponding output.

does not respond in this format, such as by describing the video content without making a judgment, the query is repeated until the response is in the correct format, which is generally effective.

Given that automated metrics may not accurately reflect human judgments of safety , we also incorporate a manual evaluation process to compare the effectiveness of GPT-4's assessments with human judgments. We recruit 60 volunteers for the assessment, all of whom must be at least 18 years old, in good physical and mental health, and free from conditions such as heart disease or vasovagal syncope. Before the assessment, we present definitions and examples of each type of safety risk to the volunteers. Volunteers view full videos on 22-24 inch monitors. Volunteers are given a 10-minute break after every 20 minutes of review to ensure psychological comfort and sustained attention. Each video receives evaluations from at least three volunteers. Following the initial evaluations, we conducted a secondary round of cross-validation. We observe that the correlation between GPT-4's evaluations and manual reviews is typically high. This finding suggests that leveraging GPT-4 on a large scale for evaluation is reasonable in our context.

## 4 Benchmarking results

Based on T2VSafetyBench, we thoroughly evaluate the safety of 9 popular T2V models, including Pika 1.5 , Gen3 , Stable Video Diffusion , Kling , Ying (Zhipu) , Vidu , Luma , Open-Sora 1.1 , and Open-Sora-Plan v1.2.0  (as an alternative to Sora). Due to computational costs and the blocking mechanisms of some models, we present experimental results on a subset of T2VSafetyBench, termed Tiny-T2VSafetyBench, which contains 689 prompts. For each prompt in Tiny-T2VSafetyBench, we generate one video given a T2V model. Both GPT-4 and human assessments are employed. A video is deemed NSFW (Not Safe For Work) if its unsafety score exceeds 0.5. In Table 2, we report the NSFW rate across different models under various aspects as assessed by both GPT-4 and humans, along with the correlation coefficient (CC) between these two evaluations. A higher NSFW rate indicates a higher safety risk. The visualization of the results is shown in Figure 1(b). Below, we first detail the results of different aspects in Sec. 4.1, then describe the main findings in Sec. 4.2.

### Perspectives from different aspects

**Pornography.** Pika, Open-Sora, and Open-Sora-Plan exhibit a high NSFW rate due to lack of ability to detect and prevent the generation of sexual content. In contrast, models such as Gen3 and SVD demonstrate robust defenses against sexual content. Nearly all malicious prompts are detected by their built-in safety filters, preventing the generation of videos. This disparity stems from Open-Sora and Open-Sora-Plan lacking detection capability for NSFW content, while Pika only implements a preliminary detector for input text. On the other hand, other models like Gen3 and SVD feature post-generation detectors for the videos themselves, enabling effective identification and rejection of any generated videos containing sexual content.

**Borderline Pornography.** Pika maintains a relatively high NSFW rate, posing the highest safety risk. In contrast, Ying, Luma, Gen3, Kling, and Vidu demonstrate a reduction in safety for pornography, while SVD still effectively mitigates such risks. It could be argued that SVD is nearly impeccable in filtering sexual content. Open-Sora and Open-Sora-Plan do not significantly exceed other models in their NSFW rate for borderline pornography, unlike for pornography, due to their weaker comprehension ability. Specifically, Open-Sora and Open-Sora-Plan fail to capture the subtly implicit sexual content in borderline pornography in some cases, thereby ensuring the videos it generates are invariably safe.

**Violence.** Nearly all video generative models demonstrate elevated NSFW rates in the context of violence. Most models exhibit NSFW rates between 35% and 65%, while Open-Sora reaches nearly 90%. Only Ying and Luma maintain NSFW rates around 20%, positioning them as relatively safe. Due to its lower generative capabilities, Open-Sora-Plan also records an NSFW rate around 20%. These findings indicate that current models are suboptimal and lack safety in handling violent content, necessitating further research to address these challenges.

**Gore.** In the aspect of Gore, Gen3 achieves optimal performance with a NSFW rate close to 10%. Vidu closely follows, effectively rejecting the majority of video generations containing gory scenes. Similar to the situation with pornography, due to the lack of detection capability for output videos, both Pika and Open-Sora generate a significant number of gory videos.

**Disturbing Content.** Gen3, Kling, and Vidu achieve the lowest safety risk among all models regarding disturbing content. SVD, Ying, and Luma also detect a portion of disturbing content, while Pika and Open-Sora exhibit almost no defensive mechanisms. Gen3's superior performance likely stems from its realistic video generation style, offering some resistance to grotesque descriptions and horror elements.

**Public Figure.** Although some models do offer limited detection capabilities, such as recognizing Donald Trump, none of text-to-video generative models incorporate specific detection measures for public figures. The Open-Sora-Plan exhibits a relatively low NSFW rate primarily because a significant proportion of generated public figures remain unrecognizable, likely due to weaker generative abilities. Overlooking the detection of public figures could lead to the generation of infringing content and pose legal risks, which necessitates further attention.

**Discrimination.** All models exhibit lower NSFW rates, suggesting minimal safety risks. However, this outcome primarily arises because current video generation models generally possess limited capability, typically generating clear and simple actions rather than effectively representing more abstract and complex content such as discrimination. Consequently, a lower NSFW rate does not imply a robust defense mechanism against discrimination. Even for simple discriminatory actions, such as a single gesture, these models struggle to detect and reject the generation of such content.

**Political Sensitivity.** In the context of Political Sensitivity, Kling and Open-Sora-Plan exhibit lower NSFW rates, whereas Gen3 and SVD do not inhibit the generation of such content, resulting in higher NSFW rates. Kling's lower security risk stems from its text detector's capability to identify keywords related to political sensitivity and subsequently refuse video generation. Conversely, Open-Sora-Plan's reduced NSFW rate is partly due to its weaker generative capability.

**Copyright and Trademark.** Most models exhibit relatively high NSFW rates. In contrast, Pika demonstrates exceptional defensive capability; it does not refuse generation but ensures the resulting videos are free of infringing marks. This likely stems from the model's training process, which incorporates consideration of infringing symbols and implements measures for their elimination. Open-Sora-Plan, due to limited generative capability, fails to produce clear representations of specific trademarks.

**Illegal Activities.** The NSFW rates for almost all video generation models are notably high when generating content related to illegal activities. Pika, Gen3, Kling, Ying, Luma, and Open-Sora exhibit NSFW rates around 50%, while Stable Video Diffusion displays a NSFW rate approaching 80%. Current models lack robust safeguards against the generation of content involving illegal activities.

**Misinformation and Falsehoods.** None of text-to-video generative models specifically implements measures to detect misinformation and falsehoods, resulting in higher NSFW rates. In reality, determining whether information constitutes misinformation or falsehoods is challenging, necessitating further research to address these issues.

**Temporal Risk.** Pika, Ying, and Vidu exhibit a higher NSFW rate compared to other commercial models. This disparity arises because Pika, Ying, and Vidu possesse superior capability in generating continuous actions and variations unique to videos, such as complex movements, subtitle shifts, and transformations in human forms. In contrast, the other models demonstrate weaker generative abilities and fail to meet the minimum threshold necessary to produce such risks to some extent. This underscores the necessity to consider Temporal Risk as a critical new category of risk in the evolving field of video generation, where advancements in model capability continually emerge.

### Holistic perspectives

**Which one is the safest model?** No single model excels in all aspects. Different models showcase distinct strengths. Stable Video Diffusion is nearly impeccable in managing sexual content, achieving an almost 0% NSFW rate. Gen3 and Vidu demonstrate the lowest safety risk in gore and disturbing content, while Pika exhibits exceptional defense capability in copyright & trademark infringement. Ying and Luma are the safest in terms of violence, and Kling excels in handling discrimination and political sensitivity.

**Comparison in terms of aspects.** As depicted in Figure 1(b), first, almost all models underperform in aspects related to Public Figures, Violence, Illegal Activities, Misinformation and Falsehoods, highlighting the critical need for future improvements in these aspects. Additionally, Pika and Open-Sora exhibit higher security risks concerning Pornography, Borderline Pornography, Gore, and Disturbing Content. This heightened vulnerability may stem from the lack of post-generation detectors for videos, resulting in ineffective defenses against these more explicit NSFW dimensions.

**Correlation between GPT-4 and human evaluations.** The correlation between the evaluations of GPT-4 and human assessments is generally strong across most dimensions, with correlation coefficients exceeding 0.75. These findings suggest that leveraging GPT-4 for assessments is reasonable in our context. However, a significant divergence is observed in the dimension of Coherent Contextual Risk, where the correlation coefficient is only 0.627. This discrepancy may stem from GPT-4's limited ability to fully understand scenarios where unsafe content emerges from the temporal linking of presented elements. These observations open new avenues for research into developing better automatic evaluation that excel across multiple safety aspects.

**Trade-off between the accessibility and safety.** It is noteworthy that a trade-off exists between the availability and security of text-to-video generative models. For instance, in the temporal risk dimension, the superior capability of Pika, Ying, and Vidu in generating continuous actions and changes leads to heightened security risks. In contrast, the other commercial models exhibit weaker generative abilities and fail to meet the minimum criteria for posing such risks. Regarding the discrimination dimension, all models struggle to effectively capture this more abstract and complex content, inadvertently resulting in reduced security risks. Moreover, in the borderline pornography dimension, Open-Sora-Plan's limited understanding prevents it from discerning the subtly implied non-direct sexual content, thus enhancing its security. Consequently, weaker generative capability in video generative models paradoxically correlate with higher security in certain dimensions. This also implies that as the field of video generation evolves and model capability strengthens (e.g., the release of Sora), the security risks across various dimensions will increase, underscoring the urgency to prioritize video security.

## 5 Conclusion

In this paper, we introduce a new benchmark for assessing the safety risks of text-to-video models, named T2V SafetyBench. By examining the usage policy and surveying AI safety practitioners, we identify 14 aspects in which generated videos may exhibit illegal or unethical content and construct a malicious text prompt dataset accordingly. We evaluate using GPT-4 and human assessment, observing a high correlation between GPT-4 and human judges. Moreover, we find that no model excels in all aspects, and there is a trade-off between the usability and safety of text-to-video generative models. These insights suggest that as the capability of video generation models increase, safety risks are likely to escalate significantly. We hope our comprehensive benchmark, in-depth analysis, and insightful findings can be helpful for understanding the safety of video generation in the era of generative AI and improve its safety in future.