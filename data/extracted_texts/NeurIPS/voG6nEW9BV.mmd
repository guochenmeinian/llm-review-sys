# Conditional score-based diffusion models for

Bayesian inference in infinite dimensions

 Lorenzo Baldassari

University of Basel

lorenzo.baldassari@unibas.ch

&Ali Siahkoohi

Rice University

alisk@rice.edu

&Josselin Garnier

Ecole Polytechnique, IP Paris

josselin.garnier@polytechnique.edu

&Knut Solna

University of California Irvine

ksolna@math.uci.edu

&Maarten V. de Hoop

Rice University

mvd2@rice.edu

###### Abstract

Since their initial introduction, score-based diffusion models (SDMs) have been successfully applied to solve a variety of linear inverse problems in finite-dimensional vector spaces due to their ability to efficiently approximate the posterior distribution. However, using SDMs for inverse problems in infinite-dimensional function spaces has only been addressed recently, primarily through methods that learn the unconditional score. While this approach is advantageous for some inverse problems, it is mostly heuristic and involves numerous computationally costly forward operator evaluations during posterior sampling. To address these limitations, we propose a theoretically grounded method for sampling from the posterior of infinite-dimensional Bayesian linear inverse problems based on amortized conditional SDMs. In particular, we prove that one of the most successful approaches for estimating the conditional score in finite dimensions--the conditional denoising estimator--can also be applied in infinite dimensions. A significant part of our analysis is dedicated to demonstrating that extending infinite-dimensional SDMs to the conditional setting requires careful consideration, as the conditional score typically blows up for small times, contrarily to the unconditional score. We conclude by presenting stylized and large-scale numerical examples that validate our approach, offer additional insights, and demonstrate that our method enables large-scale, discretization-invariant Bayesian inference.

## 1 Introduction

Inverse problems seek to estimate unknown parameters using noisy observations or measurements. One of the main challenges is that they are often ill-posed. A problem is ill-posed if there are no solutions, or there are many (two or more) solutions, or the solution is unstable in relation to small errors in the observations . A common approach to transform the original ill-posed problem into a well-posed one is to formulate it as a least-squares optimization problem that minimizes the difference between observed and predicted data. However, minimization of the data misfit alone negatively impacts the quality of the obtained solution due to the presence of noise in the data and the inherent nullspace of the forward operator . Casting the inverse problem into a Bayesian probabilistic framework allows, instead, for a full characterization of all the possible solutions . The Bayesian approach consists of putting a prior probability distribution describing uncertainty in the parameters of interest, and finding the posterior distribution over these parameters . The prior must be chosen appropriately in order to mitigate the ill-posedness of the problem and facilitate computation of the posterior. By adopting the Bayesian formulation, rather than finding one single solution to the inverseproblem (e.g., the maximum a posteriori estimator ), a distribution of solutions--the posterior--is finally obtained, whose samples are consistent with the observed data. The posterior distribution can then be sampled to extract statistical information that allows for uncertainty quantification .

Over the past few years, deep learning-based methods have been successfully applied to analyze linear inverse problems in a Bayesian fashion. In particular, recently introduced score-based diffusion models (SDMs)  have become increasingly popular, due to their ability of producing approximating samples from the posterior distribution . An SDM consists of a diffusion process, which gradually perturbs the data distribution toward a tractable distribution according to a prescribed stochastic differential equation (SDE) by progressively injecting Gaussian noise, and a generative model, which entails a denoising process defined by approximating the time-reversal of the diffusion. Crucially, the denoising stage is also a diffusion process  whose drift depends on the logarithmic gradients of the noised data densities--the scores--which are estimated by Song et al.  using a neural network. Among the advantages of SDMs over other deep generative models is that they produce high-quality samples, matching the performance of generative adversarial networks , without suffering from training instabilities and mode-collapse . Additionally, SDMs are not restricted to invertible architectures like normalizing flows , which often limits the complexity of the distributions that can be learned. Finally, and most importantly to the scope of this work, SDMs have demonstrated superior performance in a variety of inverse problems, such as image inpainting , image colorization , compressing sensing, and medical imaging .

In the aforementioned cases, SDMs have been applied by assuming that the data distribution of interest is supported on a finite-dimensional vector space. However, in many inverse problems, especially those governed by partial differential equations (PDEs), the unknown parameters to be estimated are functions (e.g., coefficient functions, boundary and initial conditions, or source functions) that exist in a suitable function space, typically an infinite-dimensional Hilbert space. The inverse heat equation or the elliptic inverse source problem presented in  are typical examples of ill-posed inverse problems that are naturally formulated in infinite-dimensional Hilbert spaces. In addition to these PDE-based examples, other interesting cases that are not PDE-based include geometric inverse problems (e.g., determining the Riemann metric from geodesic information or the background velocity map from travel time information in geophysics ) and inverse problems involving singular integral operators . A potential solution for all of these problems could be to discretize the input and output functions into finite-dimensional vectors and apply SDMs to sample from the posterior. However, theoretical studies of current diffusion models suggest that performance guarantees do not generalize well on increasing dimensions . This is precisely why Andrew Stuart's guiding principle to study a Bayesian inverse problem for functions--"avoid discretization until the last possible moment" --is critical to the use of SDMs.

Motivated by Stuart's principle, in this work we _define a conditional score in the infinite-dimensional setting_, a critical step for studying Bayesian inverse problems directly in function spaces through SDMs. In particular, we show that using this newly defined score as a reverse drift of the diffusion process yields a generative stage that samples, under specified conditions, from the correct target conditional distribution. We carry out the analysis by focusing on two cases: the case of a Gaussian prior measure and the case of a general class of priors given as a density with respect to a Gaussian measure. Studying the model for a Gaussian prior measure provides illuminating insight, not only because it yields an analytic formula of the score, but also because it gives a full characterization of SDMs in the infinite-dimensional setting, showing under which conditions we are sampling from the correct target conditional distribution and how fast the reverse SDE converges to it. It also serves as a guide for the analysis in the case of a general class of prior measures. Finally, we conclude this work by presenting, in Section 6, stylized and large-scale numerical examples that demonstrate the applicability of our SDM. Specifically, we show that our SDM model (i) is able to _approximate non-Gaussian multi-modal distributions_, a challenging task that poses difficulties for many generative models ; (ii) is _discretization-invariant_, a property that is a consequence of our theoretical and computational framework being built on the infinite-dimensional formulation proposed by Stuart ; and (iii) _applicable to solve large-scale Bayesian inverse problems_, which we demonstrate by applying it to a large-scale problem in geophysics, i.e., the linearized wave-equation-based imaging via the Born approximation that involves estimating a \(256\!\!256\)-dimensional unknown parameter.

Related worksOur work is primarily motivated by Andrew Stuart's comprehensive mathematical theory for studying PDE-governed inverse problems in a Bayesian fashion . In particular, we are interested in the infinite-dimensional analysis [7; 26], which emphasizes the importance of analyzing PDE-governed inverse problems directly in function space before discretization.

Our paper builds upon a rich and ever expanding body of theoretical and applied works dedicated to SDMs. Song et al.  defined SDMs integrating both score-based (Hyvarinen ; Song and Ermon ) and diffusion (Sohl-Dickstein et al. ; Ho et al. ) models into a single continuous-time framework based on stochastic differential equations. The generative stage in SDMs is based on a result from Anderson  proving that the denoising process is also a diffusion process whose drift depends on the scores. This result holds only in _vector spaces_, which explains the difficulties to extend SDMs to more general function spaces. Initially, there have been attempts to project the input functions into a finite-dimensional feature space and then apply SDMs (Dupont et al. ; Phillips et al. ). However, these approaches _are not discretization-invariant_. It is only very recently that SDMs have been directly studied in function spaces, specifically infinite-dimensional Hilbert spaces. Kerrigan et al.  generalized diffusion models to operate directly in function spaces, but they did not consider the time-continuous limit based on SDEs (Song et al. ). Dutordoir et al.  proposed a denoising diffusion generative model for performing Bayesian inference of functions. Lim et al.  generalized score matching for trace-class noise corruptions that live in the Hilbert space of the data. However, as Kerrigan et al.  and Dutordoir et al. , they did not investigate the connection to the forward and backward SDEs as Song et al.  did in finite dimensions. Three recent works, Pidstrigach et al. , Franzese et al.  and Lim et al. , finally established such connection for the _unconditional setting_. In particular, Franzese et al.  used results from infinite-dimensional SDEs theory (Follmer and Wakolbinger ; Millet et al. ) close to Anderson .

Among the mentioned works, Pidstrigach et al.  is the closest to ours. We adopt their formalism to establish theoretical guarantees for sampling from the conditional distribution. Another crucial contribution comes from Batzolis et al. , as we build upon their proof to show that the score can be estimated by using a denoising score matching objective conditioned on the observed data [17; 40]. A key element in Pidstrigach et al. , emphasized also in our analysis, is obtaining an estimate on the expected square norm of the score that needs to be _uniform in time_. We explicitly compute the expected square norm of the conditional score in the case of a Gaussian prior measure, which shows that a uniform in time estimate is _not always possible in the conditional setting_. This is not surprising, given that the singularity in the conditional score as noise vanishes is a well-known phenomenon in finite dimensions and has been investigated in many works, both from a theoretical and a practical standpoint [41; 42]. In our paper, we provide a set of concrete conditions to be satisfied to ensure a uniform estimate in time for a general class of prior measures in infinite dimensions.

Pidstrigach et al.  have also proposed a method for performing conditional sampling, building upon the approach introduced by Song et al.  in a finite-dimensional setting. Like our approach, their method can be viewed as a contribution to the literature on likelihood-free, simulation-based inference [43; 44]. Specifically, the algorithm proposed by Pidstrigach et al.  relies on a projection-type approach that incorporates the observed data into the unconditional sampling process via a proximal optimization step to generate intermediate samples consistent with the measuring acquisition process. This allows Pidstrigach et al. _to avoid defining the conditional score1_. While their method has been shown to work well with specific inverse problems, such as medical imaging , it is primarily heuristic, and its computational efficiency varies depending on the specific inverse problem at hand. Notably, their algorithm may require numerous computationally costly forward operator evaluations during posterior sampling. Furthermore, their implementation does not fully exploit the discretization-invariance property achieved by studying the problem in infinite dimensions since they employ a UNet to parametrize their score, limiting the evaluation of their score function to the training interval. The novelty of our work is then twofold. First, we provide theoretically grounded guarantees for an approach that is not heuristic and can be implemented such that _it is not constrained to the grid on which we trained our network_. As a result, we show that we effectively take advantage of the discretization-invariance property achieved by adopting the infinite-dimensionalformulation proposed by Stuart . Second, we perform discretization-invariant Bayesian inference by learning an _amortized version of the conditional score_. This is done by making the score function depending on the observations. As a result, provided that we have access to high-quality training data, during sampling we can input any new observation that we wish to condition on directly during simulation of the reverse SDE. In this sense, our method is _data-driven_, as the information about the forward model is implicitly encoded in the data pairs used to learn the conditional score. This addresses a critical gap in the existing literature, as the other approach using infinite-dimensional SDM resorts to projections onto the measurement subspace for sampling from the posterior--a method that not only lacks theoretical interpretation but may also yield unsatisfactory performance due to costly forward operator computations. There are well-documented instances in the literature where amortized methods can be a preferred option in Bayesian inverse problems [45; 46; 47; 48; 49; 50], as they reduce inference computational costs by incurring an offline initial training cost for a deep neural network that is capable of approximating the posterior for unseen observed data, provided that one has access to a set of data pairs that adequately represent the underlying joint distribution.

Main contributionsThe main contribution of this work is the _analysis of conditional SDMs in infinite-dimensional Hilbert spaces_. More specifically,

* We introduce the conditional score in an infinite-dimensional setting (Section 3).
* We provide a comprehensive analysis of the forward-reverse conditional SDE framework in the case of a _Gaussian prior measure_. We explicitly compute the expected square norm of the conditional score, which shows that a uniform in time estimate _is not always possible for the conditional score_. We prove that as long as we start from the invariant distribution of the diffusion process, the reverse SDE converges to the target distribution exponentially fast (Section 4).
* We provide a set of conditions to be satisfied to ensure a uniform in time estimate for a general class of prior measures that _are given as a density with respect to a Gaussian measure_. Under these conditions, the conditional score--used as a reverse drift of the diffusion process in SDMs--yields a generative stage that samples from the target conditional distribution (Section 5).
* We prove that the conditional score can be estimated via a conditional denoising score matching objective in infinite dimensions (Section 5).
* We present examples that validate our approach, offer additional insights, and demonstrate that our method enables _large-scale_, _discretization-invariant_ Bayesian inference (Section 6).

## 2 Background

Here, we review the definition of unconditional score-based diffusion models (SDMs) in infinite-dimensional Hilbert spaces proposed by Pidstrigach et al. , as we will adopt the same formalism to define SDMs for conditional settings. We refer to Appendix A for a brief introduction to key tools of probability theory in function spaces.

Let \(_{}\) be the target measure, supported on a separable Hilbert space \((H,,)\). Consider a forward infinite-dimensional diffusion process \((X_{t})_{t[0,T]}\) for continuous time variable \(t[0,T]\), where \(X_{0}\) is the starting variable and \(X_{t}\) its perturbation at time \(t\). The diffusion process is defined by the following SDE:

\[dX_{t}=-X_{t}dt+dW_{t},\] (1)

where \(C:H H\) is a fixed trace class, positive-definite, symmetric covariance operator and \(W_{t}\) is a Wiener process on \(H\). Here and throughout the paper, the initial conditions and the driving Wiener processes in (1) are assumed independent.

The forward SDE evolves \(X_{0}_{0}\) towards the Gaussian measure \((0,C)\) as \(t\). The goal of score-based diffusion models is to convert the SDE in (1) to a generative model by first sampling \(X_{T}(0,C)\), and then running the correspondent reverse-time SDE. In the finite-dimensional case, Song et al.  show that the reverse-time SDE requires the knowledge of the score function \( p_{t}(X_{t})\), where \(p_{t}(X_{t})\) is the density of the marginal distribution of \(X_{t}\) (from now on denoted \(_{t}\)) with respect to the Lebesgue measure. In infinite-dimensional Hilbert spaces, there is no natural analogue of the Lebesgue measure (for additional details, see ) and the density is thus no longer well defined. However, Pidstrigach et al. [24, Lemma 1] notice that, in the finite-dimensional setting where \(H=^{D}\), the score can be expressed as follows:

\[C_{x} p_{t}(x)=-(1-e^{-t})^{-1}(x-e^{-t/2}[X_{0}|X_{t}=x] ),\] (2)

for \(t>0\). Since the right-hand side of the expression above is also well-defined in infinite dimensions, Pidstrigach et al.  formally define the score as follows:

**Definition 1**.: _In the infinite-dimensional setting, the score or reverse drift is defined by_

\[S(t,x):=-(1-e^{-t})^{-1}(x-e^{-t/2}[X_{0}|X_{t}=x]).\] (3)

Assuming that the expected square norm of the score is uniformly bounded in time, Pidstrigach et al. [24, Theorem 1] shows that the following SDE

\[dZ_{t}=Z_{t}dt+S(T-t,Z_{t})t+dW_{t}, Z_{0} _{T},\] (4)

is the time-reversal of (1) and the distribution of \(Z_{T}\) is thus equal to \(_{0}\), proving that the forward-reverse SDE framework of Song et al.  generalizes to the infinite-dimensional setting. The reverse SDE requires the knowledge of this newly defined score, and one approach for estimating it is, similarly to , by using the denoising score matching loss 

\[[\|(t,X_{t})+(1-e^{-t})^{-1}(X_{t}-e^{-t/2} X_{0})\|^{2}],\] (5)

where \((t,X_{t})\) is typically approximated by training a neural network.

## 3 The conditional score in infinite dimensions

Analogous to the score function relative to the unconditional SDM in infinite dimensions, we now define the score corresponding to the reverse drift of an SDE when conditioned on observations. We consider a setting where \(X_{0}\) is an \(H\)-valued random variable and \(H\) is an infinite-dimensional Hilbert space. Denote by

\[Y=AX_{0}+B,\] (6)

a noisy observation given by \(n\) linear measurements, where the measurement acquisition process is represented by a linear operator \(A:H^{n}\), and \(B(0,C_{B})\) represents the noise, with \(C_{B}\) a \(n n\) nonnegative matrix. Within a Bayesian probabilistic framework, solving (6) amounts to putting an appropriately chosen prior probability distribution \(_{0}\) on \(X_{0}\), and sampling from the conditional distribution of \(X_{0}\) given \(Y=y\).

To the best of our knowledge, the only existing algorithm which performs conditional sampling using infinite-dimensional diffusion models on Hilbert spaces is based on the work of Song et al. . The idea, adapted to infinite dimensions by Pidstrigach et al. , is to incorporate the observations into the unconditional sampling process of the SDM via a proximal optimization step to generate intermediate samples that are consistent with the measuring acquisition process. Our method relies instead on _utilizing the score of infinite-dimensional SDMs conditioned on observed data_, which we introduce in this work. We begin by defining the conditional score, by first noticing that, in finite dimensions, we have the following lemma:

**Lemma 1**.: _In the finite-dimensional setting where \(H=^{D}\), we can express the conditional score function for \(t>0\) as_

\[C_{x} p_{t}(x|y)=-(1-e^{-t})^{-1}(x-e^{-t/2}[X_{ 0}|Y=y,X_{t}=x]).\] (7)

Since the right-hand side of (7) is well-defined in infinite dimensions, by following the same line of thought of Pidstrigach et al.  we formally define the score as follows:

**Definition 2**.: _In the infinite-dimensional setting, the conditional score is defined by_

\[S(t,x,y):=-(1-e^{-t})^{-1}(x-e^{-t/2}[X_{0}|Y=y,X_{t}=x ]).\] (8)

**Remark 1**.: _It is possible to define the conditional score in infinite-dimensional Hilbert spaces by resorting to the results of , see Appendix C.1._For Definition 2 to make sense, we need to show that if we use (8) as the drift of the time-reversal of the SDE in (1) conditioned on \(y\), then it will sample the correct conditional distribution of \(X_{0}\) given \(Y=y\) in infinite dimensions. In the next sections, we will carry out the analysis by focusing on two cases: the case of a Gaussian prior measure \((0,C_{})\), and the case where the prior of \(X_{0}\) is given as a density with respect to a Gaussian measure, i.e.,

\[X_{0}_{0},}{d}(x_{0})=)}}{ _{}[e^{-(X_{0})}]},=(0,C_{}),\] (9)

where \(C_{}\) is positive and trace class and \(\) is bounded with \(_{}[\|C_{}_{H}(X_{0})\|^{2}]<+\).

## 4 Forward-reverse conditional SDE framework for a Gaussian prior measure

We begin our analysis of the forward-reverse conditional SDE framework by examining the case where the prior of \(X_{0}\) is a Gaussian measure. This case provides illuminating insight, not only because it is possible to get an analytic formula of the score, but also because it offers a full characterization of SDMs in the infinite-dimensional setting, showing under which conditions we are sampling from the correct target conditional distribution and how fast the reverse SDE converges to it. We also show that the conditional score can have a singular behavior at small times when the observations are noiseless, in contrast with the unconditional score under similar hypotheses.

We assume that \(=0\) in (9). All distributions in play are Gaussian:

\[X_{0} 0,C_{},\] (10) \[X_{t}|X_{0} e^{-t/2}X_{0},(1-e^{-t})C,\] (11) \[X_{0}|Y M_{o}Y,C_{o},\] (12) \[X_{t}|Y e^{-t/2}M_{o}Y,e^{-t}C_{o}+(1-e^{-t})C,\] (13)

where \(M_{o}=C_{}A^{*}(AC_{}A^{*}+C_{B})^{-1}\) and \(C_{o}=C_{}-C_{}A^{*}(AC_{}A^{*}+C_{B})^{-1}AC_{}\). By Mercer theorem , there exist \((_{j})\) in \([0,+)\) and an orthonormal basis \((v_{j})\) in \(H\) such that \(C_{}v_{j}=_{j}v_{j}\)\( j\). We consider the infinite-dimensional case with \(_{j}>0\)\( j\). We assume that \(C_{}\) is trace class so that \(_{j}_{j}<+\). We assume that the functions \(v_{j}\) are eigenfunctions of \(C\) and we denote by \(_{j}\) the corresponding eigenvalues.

We assume an observational model corresponding to observing a finite-dimensional subspace of \(H\) spanned by \(v_{(1)},,v_{(n)}\) corresponding to \(g_{k}=v_{(k)},\ k=1,,n\), where \(g_{j} H\) is such that \((Af)_{j}= g_{j},f\). We denote \(^{(n)}=\{(1),,(n)\}\). We assume moreover \(C_{B}=_{B}^{2}I_{n}\). Let \(Z_{t}\) be the solution of reverse-time SDE:

\[dZ_{t}=Z_{t}dt+S(T-t,Z_{t},y)t+dW_{t}, Z_{0}  X_{T}|Y=y.\] (14)

We want to show that the reverse SDE we have just formulated in (14) indeed constitutes a reversal of the stochastic dynamics from the forward SDE in (1) conditioned on \(y\). To this aim, we will need the following lemma:

**Lemma 2**.: _We define \(Z^{(j)}= v_{j},Z\), \(p^{(j)}=_{j}/_{j}\) for all \(j\). We also define \(y^{(j)}=y_{(j)}\) for \(j^{(n)}\) and \(y^{(j)}=0\) otherwise, and \(q^{(j)}=_{j}/_{B}^{2}\) for \(j^{(n)}\) and \(q^{(j)}=0\) otherwise. Then we can write for all \(j\)_

\[dZ_{t}^{(j)}=^{(x,j)}(T-t)Z_{t}^{(j)}dt+^{(y,j)}(T-t)y^{(j)}dt+}dW^{(j)},\] (15)

_with \(W^{(j)}\) independent and identically distributed standard Brownian motions,_

\[^{(x,j)}(t)=-p^{(j)}(1+q^{(j)})}{1+(e^{t}-1)p^{(j)}(1 +q^{(j)})},^{(y,j)}(t)=p^{(j)}q^{(j)}}{1+(e^{t}-1)p^{(j)} (1+q^{(j)})}.\] (16)

Proof.: The proof is a Gaussian calculation. It relies on computing \( S,v_{j}\), which yields an analytic formula. See Appendix B. 

Lemma 2 enables us to discuss when we are sampling from the correct target conditional distribution \(X_{0}|YM_{o}Y,C_{o}\). We can make a few remarks:* In the limit \(T\), we get \(^{(x,j)}(T-t)-1/2\) and \(^{(y,j)}(T-t) 0\).
* If \(j^{(n)}\) then we have the same mode dynamics as in the unconditional case. Thus we sample from the correct target distribution if \(T\) is large or if we start from \(Z_{0}^{(j)}(0,_{0}^{(j)})\) for \(_{0}^{(j)}=_{j}e^{-T}+_{j}(1-e^{-T})\), which is the distribution of \(X_{T}^{(j)}= X_{T},v_{j}\) given \(Y=y\).
* If \(j^{(n)}\) and we start from \(Z_{0}^{(j)}(_{0}^{(j)},_{0}^{(j)})\), then we find \(Z_{T}^{(j)}_{T}^{(j)},_{T}^{(j)})\) with \[_{T}^{(j)}=_{0}^{(j)}(}{(1+(e^{T}-1)p^{(j)}(1+q^{ (j)}))^{2}})+}{1+q^{(j)}}(1--1)p^{(j )}(1+q^{(j)})}),\] (17) \[_{T}^{(j)}=_{0}^{(j)}e^{T/2}}{1+(e^{T}-1)p^{(j)}(1+q^{(j) })}+q^{(j)}}{1+q^{(j)}}(1--1)p^{(j)}(1+q^{ (j)})}).\] (18) The distribution of \(X_{0}^{(j)}= X_{0},v_{j}\) given \(Y=y\) is \((y^{(j)}q^{(j)}/(1+q^{(j)}),_{j}/(1+q^{(j)})\). As \(_{T}^{(j)} y^{(j)}q^{(j)}/(1+q^{(j)})\) and \(_{T}^{(j)}_{j}/(1+q^{(j)})\) as \(T+\), this shows that we sample from the exact target distribution (the one of \(X_{0}\) given \(Y=y\)) for \(T\) large.
* If we start the reverse-time SDE from the correct model \[_{0}^{(j)}=y^{(j)}q^{(j)}}{1+q^{(j)}},_{0}^{ (j)}=e^{-T}}{1+q^{(j)}}+_{j}(1-e^{-T}),\] (19) then indeed \(Z_{T}^{(j)}(y^{(j)}q^{(j)}/(1+q^{(j)}),_{j}/(1+q^{(j)}))\). This shows that, for any \(T\), \(Z_{T}\) has the same distribution as \(X_{0}\) given \(Y=y\), which is the exact target distribution. We can show similarly that \(Z_{T-t}\) has the same distribution as \(X_{t}\) given \(Y=y\) for any \(t[0,T]\).
* In the case that \(_{B}=0\) so that we observe the mode values perfectly for \(j^{(n)}\), then \[^{(x,j)}(t)=-}{e^{t}-1},^{(y,j)}(t)=}{e^{t}-1},\] (20) and indeed \(_{t T}Z_{t}^{(j)}=y^{(j)}\) a.s. Indeed the \(t^{-1}\) singularity at the origin drives the process to the origin like in the Brownian bridge.

Our analysis shows that, as long as we start from the invariant distribution of the diffusion process, we are able to sample from the correct target conditional distribution and that happens exponentially fast. This proves that the score of Definition 2 is the reverse drift of the SDE in (14). Additionally, the analysis shows that the score is uniformly bounded, except when there is no noise in the observations, blowing up near \(t=0\).

**Remark 2**.: _Note that, for \(q^{(j)}=0\), we obtain the unconditional model:_

\[dZ_{t}^{(j)}=^{(j)}(T-t)Z_{t}^{(j)}dt+}dW^{(j)},^{(j)}(t)=-p^{(j)}}{1+(e^{t}-1)p^{(j)}}.\] (21)

_If \(C=C_{}\), the square expectation of the norm and the Lipschitz constant of the score are uniformly bounded in time: \(_{j,t[0,T]}|^{(j)}(t)|=1/2\)._

**Proposition 1**.: _The score is \(S(t,x,y)=_{j}S_{G}^{(j)}(t, x,v_{j},y^{(j)})v_{j}\), \(S_{G}^{(j)}(t,x^{(j)},y^{(j)})=(^{(x,j)}(T-t)-1/2)x^{(j)}+^{ (y,j)}(T-t)y^{(j)}\) and it satisfies_

\[[\|S(t,X_{t},y)\|_{H}^{2}|Y=y]=_{j}(1+q^{(j)})}{1+(e^ {t}-1)p^{(j)}(1+q^{(j)})}^{2}}{_{j}}.\] (22)

Proof.: The proof is a Gaussian calculation given in Appendix B. 

In the unconditional setting, we have \([\|S(t,X_{t})\|_{H}^{2}]=_{j}}{1+(e^{t}-1)p^{(j)}} ^{2}}{_{j}}\) which is equal to \(_{j}_{j}\) when \(C=C_{}\). It is indeed uniformly bounded in time.

In the conditional and noiseless setting (\(_{B}=0\)), we have \([\|S(t,X_{t},y)\|_{H}^{2}|Y=y]=_{j^{(n)}} {e^{t}}{1+(e^{t}-1)p^{(j)}}^{2}}{_{j}}+_{j ^{(n)}}}{1-e^{-t}}\), which blows up as \(1/t\) as \(t 0\). This result shows that the extension of the score-based diffusion models to the conditional setting is not trivial.

Well-posedness for the reverse SDE for a general class of prior measures

We are now ready to consider the case of a general class of prior measures given as a density with respect to a Gaussian measure. The analysis of this case resembles the one of Pidstrigach et al.  for the unconditional setting. The main challenge is the singularity of the score for small times, an event that in the Gaussian case was observed in the noiseless setting. In this section we will provide a set of conditions to be satisfied by \(\) in (9), so that the conditional score is bounded uniformly in time. The existence of this bound is needed to make sense of the forward-reverse conditional SDE, and to prove the accuracy and stability of the conditional sampling.

We start the analysis by recalling that, in the infinite-dimensional case, the conditional score is (8). It is easy to get a first estimate:

\[[\|S(t,X_{t},y)\|_{H}^{2}|Y=y](1-e^{-t})^{-1}(C).\] (23)

The proof follows from Jensen inequality and the law of total expectation, see Appendix C. Note that (23) is indeed an upper bound of (22) since \((C)=_{j}_{j}\).

Note that the bound (23) is also valid for the unconditional score \(S(t,x)=-(1-e^{-t})^{-1}x-e^{-t/2}[X_{0}|X_{t}=x]\). We can observe that the upper bound (23) blows up in the limit of small times.

We can make a few comments:

* The bound (23) is convenient for positive times, but the use of Jensen's inequality results in a very crude bound for small times. As shown in the previous section, we know that there exists a bound (21) for the unconditional score in the Gaussian case that is uniform in time.
* The singular behavior as \(1/t\) at small time \(t\) is, however, not artificial. Such a behavior is needed in order to drive the state to the deterministic initial condition when there are exact observations. This behavior has been exhibited by (20) and (22) in the Gaussian case when \(_{B}=0\). This indicates that the following assumption (24) is not trivial in the conditional setting.

For Definition 2 to make sense in the more general case where the prior of \(X_{0}\) is given as a density with respect to a Gaussian measure, we will need to make the following assumption.

**Assumption 1**.: _For any \(y^{n}\), we have_

\[_{t[0,T]}\|S(t,X_{t},y)\|_{H}^{2}|Y=y<.\] (24)

We are now ready to state the analogous result to Pidstrigach et al. [24, Theorem 1].

**Proposition 2**.: _Under Assumption 1, the solution of the reverse-time SDE_

\[dZ_{t}=Z_{t}dt+S(T-t,Z_{t},y)dt+dW_{t}, Z_{0} X_ {T}|Y=y,\] (25)

_satisfies \(Z_{T} X_{0}|Y=y\)._

Proof.: Given Assumption 1, the proof follows the same steps as the one given in  for the unconditional score. See Appendix C for the full proof. 

Assumption 1 is satisfied under some appropriate conditions. In the following proposition, we provide a set of conditions that ensure the satisfaction of this assumption. It shows that it is possible to get an upper bound in (23) that is uniform in time provided some additional conditions are fulfilled.

**Proposition 3**.: _We assume that \(C_{}\) in (9) and \(C\) in (1) have the same basis of eigenfunctions \((v_{j})\) and we define \(X_{t}^{(j)}= X_{t},v_{j}\) and \(S^{(j)}(t,x,y)= S(t,x,y),v_{j}\) so that in (1) \(S(t,x,y)=_{j}S^{(j)}(t,x,y)v_{j}\). We assume an observational model as described in Section 4 and that the \(p^{(j)}(1+q^{(j)})\) are uniformly bounded with respect to \(j\) and that \(C\) is of trace class. We make a modified version of assumption in (9) as follows. We assume that 1) the conditional distribution of \(X_{0}\) given \(Y=y\) is absolutely continuous with respect to the Gaussian measure \(\) with a Radon-Nikodym derivative proportional to \((-(x_{0},y))\); 2) we have \((x_{0},y)=_{j}^{(j)}(x_{0}^{(j)},y)\), \(x_{0}^{(j)}= x_{0},v_{j}\); 3) for \(^{(j)}(x^{(j)},y)=(-^{(j)}(x^{(j)},y))\) we have_

\[|^{(j)}(x^{(j)},y)| K,|^{(j)}(x^{(j)},y)- ^{(j)}{({x^{(j)}}^{},y)}| L{|{x^{(j)}}^{}}-x^{(j)}|,\] (26)

_where \(K\) and \(L\) do not depend on \(j\). Then Assumption 1 holds true._Proof.: The proof is given in Appendix C. 

To use the new score function of Definition 2 for sampling from the posterior, we need to define a way to estimate it. In other words, we need to define a loss function over which the difference between the true score and a neural network \(s_{}(t,x_{t},y)\) is minimized in \(\). A natural choice for the loss function is

\[_{t U(0,T),x_{t},y(X_{t},Y)}\|S(t,x_{t},y)- s_{}(t,x_{t},y)\|_{H}^{2},\] (27)

however it cannot be minimized directly since we do not have access to the ground truth conditional score \(S(t,x_{t},y)\). Therefore, in practice, a different objective has to be used. Batzolis et al.  proved that, in finite dimensions, a denoising score matching loss can be used:

\[_{t U(0,T),x_{0},y(X_{0},Y),x_{t}( X_{t}|X_{0}=x_{0})}\|C_{x_{t}} p(x_{t}|x_{0})-s_{}(t,x_{t},y) \|^{2}.\] (28)

This expression involves only \(_{x_{t}} p(x_{t}|x_{0})\) which can be computed analytically from the transition kernel of the forward diffusion process, also in infinite dimensions. In the following proposition, we build on the arguments of Batzolis et al.  and provide a proof that the conditional denoising estimator is a consistent estimator of the conditional score in infinite dimensions.

**Proposition 4**.: _Under Assumption 1, the minimizer in \(\) of_

\[_{x_{0},y(X_{0},Y),x_{t}(X_{t}|X_{0}= x_{0})}\|-(1-e^{-t})^{-1}(x_{t}-e^{-t/2}x_{0})-s_{}(t,x_{t},y)\|_{H}^ {2}\] (29)

_is the same as the minimizer of_

\[_{x_{t},y(X_{t},Y)}\|S(t,x_{t},y)-s_{}(t,x_{t},y)\|_{H}^{2}.\] (30)

_The same result holds if we add \(t(0,T)\) in the expectations._

Proof.: The proof combines some of the arguments of Batzolis et al.  and steps of the proof of Lemma 2 in , see Appendix C. 

**Remark 3**.: _A statement of robustness can be written as in [24, Theorem 2]._

## 6 Numerical experiments

To put the presented theoretical results into practice, we provide two examples. The first stylized example aims at showcasing (i) the ability of our method in capturing nontrivial conditional distributions; and (ii) the discretization-invariance property of the learned conditional SDM. In the second example, we sample from the posterior distribution of a linearized seismic imaging problem in order to demonstrate the applicability of our method to large-scale problems. In both examples, in order to enable learning in function spaces, we parameterize the conditional score using Fourier neural operators . Details regarding our experiment and implementation2 are presented at Appendix D.

Stylized exampleInspired by Phillips et al. , we define the target density via the relation \(x_{0}=ay^{2}+\) with \((1,2)\), \(a\{-1,1\}\), and \(y[-3,3]\). Figure 0(a) illustrates the samples \(x_{0}\) evaluated on a fine \(y\) grid. After training (details in Appendix D), we sample the conditional distribution on uniformly sampled grids between \([-3,3]\), each having 20 to 40 grid points. Figures 0(b) and 0(c) show the predicted samples for grid sizes of \(25\) and \(35\), respectively. The marginal conditionals associated with \(y=-1.0,0.0,0.5\) are shown in Figures 0(d)-0(f), respectively. The gray shaded density in the bottom row of Figure 1 indicates the ground truth density, and colored estimated densities correspond to different discretizations of the horizontal axis. The visual inspection of samples and estimated densities indicates that our approach is indeed discretization-invariant.

Linearized seismic imaging exampleIn this experiment, we address the problem of estimating the short-wavelength component of the Earth's subsurface squared-slowness model (i.e., seismic image; cf. Figure 1(a)) given surface measurements and a long-wavelength, smooth squared-slowness model (cf. Figure 1(b)). Following Orozco et al. , in order to reduce the high dimensionality of surface measurements, we apply the adjoint of the forward operator, the Born scattering operator, to the measurements and use the outcome (cf. Figure 1(c)) instead of measured data to condition the SDM. After training, given previously unseen observed data, we use the SDM to sample \(10^{3}\) posterior samples to estimate the conditional mean (cf. Figure 1(d)), which corresponds to the minimum-variance estimate , and the pointwise standard deviation (cf. Figure 1(e)), which we use to quantify the uncertainty. As expected, the pointwise standard deviation highlights areas of high uncertainty, particularly in regions with complex geological structures--such as near intricate reflectors and areas with limited illumination (deep and close to boundaries). We also observe a strong correlation between the pointwise standard deviation and the error in the conditional mean estimate (Figure 1(f)), confirming the accuracy of our Bayesian inference method.

## 7 Conclusions

We introduced a theoretically-grounded method that _is able to perform conditional sampling in infinite-dimensional Hilbert (function) spaces using score-based diffusion models._ This is a foundational step in using diffusion models to perform Bayesian inference. To achieve this, we learned the infinite-dimensional score function, as defined by Pidstrigach et al. , conditioned on the observed data. Under mild assumptions on the prior, this newly defined score--used as the reverse drift of the diffusion process--yields a generative model that samples from the posterior of a linear inverse problem. In particular, the well-known singularity in the conditional score for small times can be avoided. Building on these results, we presented stylized and large-scale examples that showcase the validity of our method and its discretization-invariance, a property that is a consequence of our theoretical and computational framework being built on infinite-dimensional spaces.

Figure 1: A depiction of the method’s discretization invariance. Top row displays ground truth (a) and predicted samples (functions) on a uniformly sampled grid with (b) 25 and (c) 35 grid points. Bottom row shows conditional distribution marginals for (d) \(y=-1.0\), (e) \(y=0.0\), and (f) \(y=0.5\).

Figure 2: Seismic imaging and uncertainty quantification. (a) Ground-truth seismic image. (b) Background squared-slowness. (c) Data after applying the adjoint Born operator. (d) Conditional (posterior) mean. (e) Pointwise standard deviation. (f) Absolute error between Figures 1(a) and 1(d).