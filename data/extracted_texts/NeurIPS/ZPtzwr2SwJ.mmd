# Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback

Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback

 Canzhe Zhao

Shanghai Jiao Tong University

canzhezhao@sjtu.edu.cn

&Ruofeng Yang

Shanghai Jiao Tong University

wanshuiyin@sjtu.edu.cn

&Baoxiang Wang

The Chinese University of Hong Kong, Shenzhen

bxiangwang@cuhk.edu.cn

&Xuezhou Zhang

Boston University

xuezhouz@bu.edu

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

Corresponding author.

###### Abstract

In this work, we study the low-rank MDPs with adversarially changed losses in the full-information feedback setting. In particular, the unknown transition probability kernel admits a low-rank matrix decomposition (Uehara et al., 2022), and the loss functions may change adversarially but are revealed to the learner at the end of each episode. We propose a policy optimization-based algorithm POLO, and we prove that it attains the \((K^{}{{6}}}A^{}}{{2}}}d(1+M)/(1- )^{2})\) regret guarantee, where \(d\) is rank of the transition kernel (and hence the dimension of the unknown representations), \(A\) is the cardinality of the action space, \(M\) is the cardinality of the model class that contains all the plausible representations, and \(\) is the discounted factor. Notably, our algorithm is oracle-efficient and has a regret guarantee with no dependence on the size of potentially arbitrarily large state space. Furthermore, we also prove an \((}{1-})\) regret lower bound for this problem, showing that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting. To the best of our knowledge, we present the first algorithm that interleaves representation learning, exploration, and exploitation to achieve the sublinear regret guarantee for RL with nonlinear function approximation and adversarial losses.

## 1 Introduction

In reinforcement learning (RL), the goal is to learn a (near) optimal policy through the interactions between the learner and the environment, which is typically modeled as the Markov decision processes (MDPs) (Feinberg, 1996). When the state and action spaces are finite, many works have established the minimax (near) optimal regret guarantees for MDPs with finite horizon (Azar et al., 2017) and MDPs with infinite horizon (Tossou et al., 2019, He et al., 2021). In real applications of RL, however, the state and action spaces may be arbitrarily large and even infinite, which may lead to the curse of dimensionality. To tackle this issue, a common approach is _function approximation_, which approximates the value functions of given policies with the leverage of feature mappings. Assuming that the feature mapping which embeds the state-action pairs to a low dimensional embedding space is known, RL with linear function approximation has been well studied recently. In particular, linear mixture MDPs (Ayoub et al., 2020) and linear MDPs (Jin et al., 2020b) are the two models of RL with linear function approximation that have been extensively studied. Notably, their (near) optimal regret guarantees are established by Zhou et al. (2021) and He et al. (2023) respectively. Nevertheless, in scenarios with complex and large-scale data, attaining the true underlying feature mappings might be unrealistic, and thus representation learning is needed. Many empirical works have shown that representation learning can accelerate the sample and computation efficiency of RL (Silver et al., 2018; Laskin et al., 2020; Yang and Nachum, 2021; Stooke et al., 2021; Schwarzer et al., 2021; Xie et al., 2022). However, representation learning is provably more difficult in RL and other sequential decision-making problems than in non-sequential and non-interactive problems (_e.g._, supervised learning) (Du et al., 2020; Wang et al., 2021; Weisz et al., 2021; Uehara et al., 2022). To pursue sample-efficient RL in the presence of representation learning, recent works have made initial attempts to study the theoretical guarantees of representation learning in RL under the fixed or stochastic loss functions (Uehara et al., 2022; Zhang et al., 2022).

In practice, however, it might be stringent to assume that the loss functions are stochastic. To tackle this issue, Even-Dar et al. (2009) and Yu et al. (2009) propose the first algorithms with provably theoretical guarantees that can handle adversarial MDPs, where the loss functions may change adversarially in each episode. Subsequently, most of the works in this line of research focus on learning tabular MDPs with adversarial loss functions (Neu et al., 2010, 2012; Arora et al., 2012; Zimin and Neu, 2013; Dekel and Hazan, 2013; Dick et al., 2014; Rosenberg and Mansour, 2019, 2019; Jin and Luo, 2020; Jin et al., 2020; Shani et al., 2020; Chen et al., 2021; Ghasemi et al., 2021; Rosenberg and Mansour, 2021; Jin et al., 2021; Dai et al., 2022; Chen et al., 2022). To learn adversarial MDPs with large state and action spaces, some recent works study RL with adversarial loss functions and linear function approximation (Cai et al., 2020; Neu and Olkhovskaya, 2021; Luo et al., 2021, 2021; He et al., 2022; Zhao et al., 2023; Kong et al., 2023). However, all these existing works assume that the state-action feature representations are known. As aforementioned, in complex and high-dimensional environments, the application of these algorithms may be still hindered due to the potential difficulty of knowing the true feature mappings a priori. Therefore, the following question naturally remains open:

_Can we devise an algorithm to simultaneously tackle the representation learning and adversarially changed loss functions in reinforcement learning?_

In this work, we give an affirmative answer to the above question in the setting of adversarial low-rank MDPs with full-information feedback. Specifically, in this problem, the unknown transition probability kernel admits a low-rank matrix decomposition but the true representations regarding the transitions are not known a priori. Meanwhile, the loss functions are arbitrarily chosen by an adversary across episodes and are revealed to the learner after each episode.

To solve this problem, we propose a policy optimization-based algorithm, which we call **P**olicy **O**ptimization for **LO**w-rank MDPs (POLO). Specifically, our POLO algorithm obtains an \((K^{}{{6}}}A^{}{{2}}}d(1+M)/(1- )^{2})\) regret guarantee for adversarial low-rank MDPs in the full-information feedback setting and is oracle-efficient. Algorithmically, POLO follows similar ideas of optimistic policy optimization methods in that it first constructs optimistic value function estimates and then runs online mirror descent (OMD) over the optimistic value estimates to deal with the adversarially changed loss functions (Shani et al., 2020, Cai et al., 2020; He et al., 2022; Chen et al., 2022). However, in the presence of representation learning, the exploration and exploitation needed to learn the adversarial MDPs are more difficult than them in the tabular case (Shani et al., 2020, Chen et al., 2022) and in the linear case (Cai et al., 2020; He et al., 2022), where the exploration and exploitation can be essentially well balanced by directly modifying the algorithms studying stochastic MDPs in the tabular and linear cases. Concretely, to learn stochastic low-rank MDPs, previous works perform maximum likelihood estimation (MLE) over the experienced transitions (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). Though the balance of representation learning, exploration, and exploitation can be simultaneously handled by the algorithms in these works, these algorithms intrinsically have no regret guarantees but only sample complexity guarantees even in the stochastic case, since these algorithms need to take actions uniformly at certain stepsin each episode (_cf._, Lemma 9 of Uehara et al. (2022)).2 Hence, a straightforward adaption of their methods from the stochastic setting to the adversarial setting will fail to learn adversarial low-rank MDPs. To cope with this issue, we carefully devise an algorithm with a _doubled exploration and exploitation_ scheme, which interleaves (a) the exploration over transitions required in representation learning; and (b) the exploration and exploitation suggested by the policy optimization. To this end, our algorithm adopts a mixed roll-out policy, which consists of a uniformly explorative policy and a policy optimized by OMD. Through carefully tuning the hyper-parameter of the mixing coefficient used in our mixed policy, we can avoid pulling actions uniformly at random to conduct exploration in each episode and only conduct uniform exploration at a certain fraction of all the episodes (see Section 3.1 for details). Besides, unlike tabular and linear (mixture) MDPs, it is in general hard to achieve the point-wise optimism for each state-action pair. Therefore, depart from previous methods (Shani et al., 2020; Cai et al., 2020; He et al., 2022) conducting policy optimization in the _true_ model (_i.e._, the transition kernel characterized by the true representation), our algorithm conducts policy optimization in the fixed _learned_ model with the epoch-based model update, which enables a new analysis scheme that only requires a _near optimism_ at the initial state \(s_{0}\) (see Section 3.2 for

   Algorithm & Model & Feedback & Regret & Unknown \\  & & & & Features \\  OPPO & Linear Mixture & Full- & \((d/(1-)^{2})\) & ✗ \\
[Cai et al., 2020] & MDPs & information & & \\  POWERS & Linear Mixture & Full- & \((d/(1-)^{3/2})\) & ✗ \\
[He et al., 2022] & MDPs & information & & \\  LSUOB-REPS & Linear Mixture & Bandit & \((dS^{2}+})\) & ✗ \\ Zhao et al. (2023) & MDPs & Feedback & & \\  Luo et al. (2021) & Linear MDPs & Bandit & \((d^{2}K^{14/15}/(1-)^{4})\) & ✗ \\  & & Feedback & & \\  Dai et al. (2023) & Linear MDPs & Bandit & \((a^{2/3}}{(1-)^{2/9}})\) & ✗ \\  & & Feedback & & \\  PO-LSBE & Linear MDPs & Bandit & \((}{(1-)^{2}}+K^{5/7}}{(1- )^{4}})\) & ✗ \\ Sherman et al. (2023) & & Feedback & & \\  GLAP & Linear MDPs & Bandit & \((d^{7/5}H^{12/5}K^{4/5})\) & ✗ \\ Kong et al. (2023) & & Feedback & & \\  OPPO+ & Linear MDPs & Full- & \((K^{3/4}+d^{5/2}}{(1-)^{2}})\) & ✗ \\ Zhong and Zhang (2023) & & information & & \\  POLO & Low-rank & Full- & \((A^{1/2}d(1+M)}{(1-)^{2}})\) & ✗ \\
**(Ours)** & MDPs & Information & \((}{1-})\) & \\   

Table 1: Comparisons of regret bounds with most related works studying adversarial RL with function approximation under unknown transitions. \(K\) is the number of episodes, \(d\) is the ambient dimension of the feature mapping, \(\) is the discounted factor for infinite-horizon MDPs, and \(S\), \(A\), and \(M\) are the cardinality of the state space, action space, and model class, respectively. Note that the dependence on \(\) is not strictly comparable since some works originally studying finite-horizon MDPs and these results are translated into results for infinite-horizon MDPs by substituting horizon length \(H\) with \((1/(1-))\). The column of “unknown features” indicates whether the algorithm can work in the case when no true feature mappings are known a priori.

details). Also, we prove a regret lower bound of order \((}{1-})\) for low-rank MDPs with fixed loss functions, which thus also serves as a regret lower bound for our problem and indicates that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting. To the best of our knowledge, this work makes the first step to establish an algorithm with a sublinear regret guarantee for adversarial low-rank MDPs, which permits RL with both nonlinear function approximation and adversarial loss functions. The concrete comparisons between the results of this work and those of previous works are summarized in Table 1.

### Additional Related Works

RL with Function ApproximationSignificant advances have emerged in RL with function approximation to cope with the curse of dimensionality in arbitrarily large state space or action space. In general, these results fall into two categories. The first category studies RL with linear function approximation, including linear MDPs (Yang and Wang, 2019; Jin et al., 2020; Du et al., 2020; Zanette et al., 2020; Wang et al., 2020, 2021; He et al., 2021; Hu et al., 2022; He et al., 2022) and linear mixture MDPs (Ayoub et al., 2020; Zhang et al., 2021; Zhou et al., 2021; He et al., 2021; Zhou and Gu, 2022; Wu et al., 2022; Min et al., 2022; Zhao et al., 2023). Remarkably, He et al. (2023) and Zhou et al. (2021) obtain the nearly minimax optimal regret \((dK})\) in linear MDPs and linear mixture MDPs respectively when the loss functions are fixed or stochastic. The other category studies RL with general function approximation. Amongst these works, Jiang et al. (2017), Dann et al. (2018), Sun et al. (2019), Du et al. (2019), and Jin et al. (2021) study the MDPs satisfying the low Bellman-rank assumption, which assumes the Bellman error matrix has a low-rank factorization. Also, Du et al. (2021) consider a similar but slightly more general assumption termed as bounded bilinear rank. Besides, Russo and Roy (2013), Wang et al. (2020), Jin et al. (2021), and Ishfaq et al. (2021) study low Eluder dimension assumption, which is originally proposed to characterize the complexity of function classes for bandit problems.

Representation learning in RL arises when the feature mapping that embeds the state-action pairs in RL with linear function approximation is no longer known a priori. Such a problem is typically studied in the setting of low-rank MDPs, which does not assume the feature mapping of state-action pairs is known. Consequently, the setting of low-rank MDPs strictly generalizes the setting of linear MDPs, but at the cost of being more difficult to learn due to potential nonlinear function approximation induced by representation learning. In this line of research, algorithms with provably sample complexity guarantees have been developed in both model-based methods (Agarwal et al., 2020; Ren et al., 2022; Uehara et al., 2022) and model-free methods (Modi et al., 2021; Zhang et al., 2022), respectively. The model-based algorithms of Agarwal et al. (2020), Ren et al. (2022) and Uehara et al. (2022) learn the representation from a given model class of transition probability kernels. In contrast, the model-free methods do not require model learning but may bear some limitations. In particular, Modi et al. (2021) assume the MDPs satisfying the minimal reachability assumption, and the sample complexity of the algorithm of Zhang et al. (2022) only holds for a special class of low-rank MDPs called block MDPs. Besides, representation learning in Markov games has also been investigated recently (Ni et al., 2022).

RL with Adversarial LossRecent years have witnessed significant advances in learning RL with adversarial losses in the tabular case (Neu et al., 2010; 2012; 2012; Arora et al., 2012; Zimin and Neu, 2013; Dekel and Hazan, 2013; Dick et al., 2014; Rosenberg and Mansour, 2019; 2019; Jin and Luo, 2020; Jin et al., 2020; Shani et al., 2020; Chen et al., 2021; Ghasemi et al., 2021; Rosenberg and Mansour, 2021; Jin et al., 2021; Dai et al., 2022; Chen et al., 2022). When it comes to the setting of linear function approximation, various policy optimization-based methods have been established to solve adversarial linear mixture MDPs (Cai et al., 2020; He et al., 2022) and adversarial linear MDPs (Luo et al., 2021; 2012; 2013; Dai et al., 2023; Sherman et al., 2023; Zhong and Zhang, 2023). The other line of works studies RL with linear function approximation and adversarial losses using occupancy measure-based methods (Neu and Olkhovskaya, 2021; Zhao et al., 2023; Kong et al., 2023). To the best of our knowledge, however, there are no works in existing literature studying RL with both nonlinear function approximation and adversarial loss functions.

## 2 Preliminaries

We consider episodic infinite horizon low-rank MDPs with adversarial loss functions, the preliminaries of which are introduced as follows.

Episodic Infinite-horizon Adversarial MDPsAn episodic infinite horizon adversarial MDP is denoted by a tuple \((,,P^{},\{_{k}\}_{k=1}^{K},,d_{0})\),3 where \(\) is the state space (with potentially infinitely many states), \(\) is the finite action space with cardinality \(||=A\), \(P^{}:\) is the transition probability kernel such that \(P^{}(s^{} s,a)\) is the probability of transferring to state \(s^{}\) from state \(s\) after executing action \(a\), \([0,1)\) is the discount factor, \(d_{0}()\) is the initial distribution over the state space, and \(_{k}:\) is the loss function of episode \(k\) chosen by the adversary. For the ease of exposition, we assume \(d_{0}\) is known.

In this work, we consider a special class of MDPs called _low-rank MDPs_(Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). Specifically, instead of assuming a known true feature mapping, low-rank MDPs only assume that the transition probability kernel \(P^{}\) admits a low-rank decomposition, with the formal definition given as follows.

**Definition 2.1** (Low-rank MDPs).: _An MDP is a low-rank MDP if there exist two feature embedding functions \(^{}:^{d}\) and \(^{}:^{d}\) such that for any \((s,a,s^{})\), \(P^{}(s^{} s,a)=^{}(s^{})^{ }^{}(s,a)\), where \(\|^{}(s,a)\|_{2} 1\) and for any function \(g:,\|^{}(s)g(s)(s) \|_{2}\)._

Note that the regularity assumption imposed over \(^{}\) and \(^{}\) is only for the purpose of normalization.

Function ApproximationWhen the state space is arbitrarily large, function approximation is usually considered to permit sample-efficient learning for MDPs. Since the true feature mapping of state-action pairs is not known a priori in the low-rank MDPs, to make this problem tractable, we assume the access to a _realizable_ model class as previous works (Agarwal et al., 2020; Uehara et al., 2022), detailed in the following.

**Assumption 2.1**.: _There exists a known model class \(=\{(,):,\}\) satisfying (a) \(\|(s,a)\|_{2} 1\) and \(^{}(s^{})(s,a)(s^{} )=1\) for any \((s,a,s^{})\), \(\), \(\); and (b) \(\|(s)g(s)(s)\|_{2}\) for any function \(g:\). Moreover, it holds that \(^{}\) and \(^{}\)._

Throughout this paper, for the sake of brevity, we assume that the cardinality of \(\) and \(\) are finite, meaning that \(\) also has bounded cardinality \(M=||\). However, note that extending the analyses to the function classes with infinite cardinality but bounded statistical complexity (_e.g._, VC dimension) is not technically difficult.

Interaction ProtocolWe now introduce the interaction protocol between the learner and the environment. To begin with, denote by \(d_{P}^{}(s,a)=(1-)_{=0}^{}^{}d_{P,}^{} (s,a)\) the state-action occupancy distribution, where \(d_{P,}^{}(s,a)\) is the probability of visiting \((s,a)\) at step \(\) under policy \(\) and transition \(P\). With a slight abuse of notation, let \(d_{P}^{}(s)=_{a}d_{P}^{}(s,a)\) be the state occupancy distribution, denoting the probability of visiting state \(s\) under \(\) and \(P\).

Ahead of time, an MDP is decided by the environment, and only the state space \(\) and the action space \(\) are revealed to the learner. Meanwhile, the adversary secretly chooses \(K\) loss functions \(\{_{k}\}_{k=1}^{K}\), each of which will be used in one episode. The interaction will proceed in \(K\) episodes. At the beginning of episode \(k\), the learner chooses a stochastic policy \(_{k}:\), where \(_{k}(a s)\) is probability of taking \(a\) at state \(s\). Starting from an initial state \(s_{0} d_{0}\), the learner repeatedly executes policy \(_{k}\) until reaching the termination. After episode \(k\) is terminated, the learner observes a trajectory \(\{(s_{k,},a_{k,})\}_{}\) as well as the loss function \(_{k}\).

For each episode \(k\) and each state-action pair \((s,a)\), the state-action value \(Q_{k}^{}(s,a)\) is defined as \(Q_{k}^{}(s,a)=[_{=0}^{}^{}_{k}(s_{ k,},a_{k,})|,P^{},(s_{k,0},a_{k,0})=(s,a)]\). Also define \(V_{k}^{}(s)=_{a( s)}[Q_{k}^{}(s,a)]\) and \(V_{k}^{}=_{s_{0} d_{0}}[V_{k}^{}(s_{0})]\). The learning objective is to minimize the _pseudo regret_ with respect to \(^{}\), defined as

\[_{K}=[_{k=1}^{K}(V_{k}^{_{k}}-V_{k}^{^{ }})],\]where the expectation is taken over the potential randomness of the algorithm, \(^{}_{}_{k=1}^{K}V_{k}^{}\) is the fixed optimal policy in hindsight and \(\) is the set of stochastic policies.

## 3 Algorithm

This section presents our POLO algorithm with the pseudocode illustrated in Algorithm 1. At a high level, POLO leverages a mixed roll-out policy to conduct doubled exploration and exploitation, _i.e._, (a) the exploration over transitions required by representation learning; and (b) the exploration and exploitation over adversarially changed loss functions required by the policy optimization (Section 3.1). To deal with the issue that only the _near optimism_ at the initial state \(s_{0}\) is available in low-rank MDPs, POLO conducts policy optimization in fixed _learned_ models with the epoch-based model update, which enables a new analysis scheme (Section 3.2).

### Doubled Exploration and Exploitation

Let \(_{k}\) be the policy of episode \(k\) computed by policy optimization (_cf._, Eq. (3)). At the beginning of episode \(k\), our algorithm first collects a state \(s_{k} d_{P^{}}^{_{k}}\). (Line 6) by invoking a geometric sampling _roll-in_ procedure (Kakade and Langford, 2002; Agarwal et al., 2021; Uehara et al., 2022). Starting from an initial state \(s_{0} d_{0}\), at each step \(\), this roll-in procedure will terminate and return state \(s_{}\) with probability \(1-\), and otherwise will take action \(a_{}_{k}( s_{})\) and transfer to the next state \(s_{} P^{}( s_{},a_{})\).

Then our algorithm will further interact with the environment in successive two steps after collecting \(s_{k} d_{P^{}}^{_{k}}\) (Line 7 - Line 11) like previous works studying low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022). One of the main differences between previous algorithms and ours lies in how to deal with exploration and exploitation when interacting with the environment. In the case of stochastic low-rank MDPs, the key in the analysis is to ensure the (near) optimism of the optimal policy \(^{}\) in the learned model, which essentially requires to bound the performance gap \(_{k}^{^{}}-V_{k}^{^{}}\) by relating it with the model error regarding \(^{}\), _i.e._, \(_{(s,a) d_{P^{}}^{^{}}}[\|_{k}( s,a )-P^{}( s,a)\|_{1}]\), where \(_{k}\) is the MLE solution defined in Eq. (1). However, this model error is not directly controllable, as the algorithm does not know the optimal policy \(^{}\) and can not collect data following \(^{}\) to bound the model error. Fortunately, by empirical process theory (Geer, 2000; Zhang, 2006), the model error \(_{(s,a)_{k}}[\|_{k}( s,a)-P^{}(  s,a)\|_{1}]\) regarding the executed policies \(\{_{i}\}_{i=1}^{k}\) is bounded, where \(_{k}(s,a)=_{i=1}^{k}d_{P^{}}^{_{i}}(s,a)\). Therefore, previous works (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022) ensure the optimism of the optimal policy \(^{}\) by applying importance weighting to change the measure from \(d_{P^{}}^{^{}}\) to \(_{k}\) when bounding the model error. The complication is that to control the ratio \(^{}(a s)/_{i}(a s)\) for any \((s,a)\) and \(i[K]\) in importance weighting, the algorithms in previous works take actions from the uniform distribution \(U()\) over action space \(\), which intuitively can be seen as conducting exploration over transitions required by representation learning. Consequently, though these algorithms enjoy excellent sample complexities, they intrinsically do not have regret guarantees due to the uniform exploration over action space, even in the stochastic setting. Moreover, to learn adversarial low-rank MDPs, it is required to take actions adaptively according to the observed loss functions in previous episodes instead of uniformly taking actions. To address this "conflict" so as to learn adversarial low-rank MDPs, we propose to use a mixed roll-out policy to interleave (a) the exploration over transitions required by representation learning; and (b) the exploration and exploitation over the adversarial loss functions by policy optimization, which we call _doubled exploration and exploitation_ and is pivotal to achieving our regret bound as we will shortly see. Formally, our algorithm will conduct the exploration over the transitions with probability \(\) and execute policy \(_{k}\) optimized by OMD with probability \(1-\) respectively, as shown in Line 7 - Line 11.

After interacting with the environment, the newly collected data in these two steps will be used to update the datasets (Line 14), and the empirical transition \(_{k}\) will be updated by performing MLE over the updated datasets by solving (Line 16)

\[(_{k},_{k})=*{arg\,max}_{( ,)}_{_{k}_{k}^{ }}[^{}(s^{})(s,a)]\,,\] (1)

where we denote \(_{}[f(s,a,s^{})]=|}_{(s,a,s^{})}f(s,a,s^{})\).

```
1:Input: Mixing coefficient \(\), epoch length \(L\), regularization coefficients \(\{_{k}\}_{k=1}^{K}\), bonus coefficients \(\{_{k}\}_{k=1}^{K}\), model class \(\), number of episodes \(K\), learning rate \(\).
2:Initialization: Set \(_{0}=\), \(_{0}^{}=\).
3:for\(i=1,2,, K/L\)do
4: Set \(k_{i}=(i-1)L+1\) and \(_{k_{i}}( s)\) to be uniform for any \(s\).
5:for\(k=k_{i},k_{i}+1,,k_{i}+L-1\)do
6: Sample \(s_{k}\) from \(_{_{s}}^{_{k}}\).
7: Sample \(c_{k}(1-)\).
8:if\(c_{k}=1\)then
9: Sample \(a_{k}_{k}( s_{k}),s^{}_{k} P^{}(  s_{k},a_{k}),a^{}_{k}_{k}( s^{}_{k}),s^{}_{k} P^{}( s^{}_{k},a^{}_{k})\).
10:else
11: Sample \(a_{k} U(),s^{}_{k} P^{}( s_{k},a_{k}),a ^{}_{k} U(),s^{}_{k} P^{}( s ^{}_{k},a^{}_{k})\).
12:endif
13: Observe the loss function \(_{k}\).
14: Update datasets \(_{k}=_{k-1}\{(s_{k},a_{k},s^{}_{k})\}\), \(^{}_{k}=^{}_{k-1}\{(s^{}_{k},a^{ }_{k},s^{}_{k})\}\).
15:if\(k=k_{i}\)then
16: Set the empirical transition \(_{k}(s^{} s,a)=_{k}(s^{})^{} _{k}(s,a)\), \((s,a,s^{})\), via solving Eq. (1).
17: Update the empirical covariance matrix \(_{k}=_{(s,a)_{k}}_{k}(s,a) _{k}(s,a)^{}+_{k}I\).
18: Set the bonus function \(_{k}(s,a):=(_{k}\|_{k}(s,a)\|_{_{k}^{-1}},2)/(1-)\), \((s,a)\).
19:else
20: Set the empirical transition \(_{k}=_{k_{i}}\) and bonus function \(_{k}=_{k_{i}}\).
21:endif
22: Compute \(_{k}^{_{k}}(,)=Evaluation}( _{k},_{k}-_{k},_{k})\).
23: Update policy \(_{k+1}()_{k}() (-_{k}^{_{k}}(,))\).
24:endfor
25:endfor ```

**Algorithm 1** Policy Optimization for Low-rank MDPs (POLO)

### Policy Optimization in Fixed Learned Models

It remains to compute the policy \(_{k+1}\) to be used in the next episode. To this end, we resort to the canonical OMD framework to perform policy optimization like previous methods (Shani et al., 2020; Cai et al., 2020; He et al., 2022). However, previous OMD-based policy optimization methods for tabular and linear (mixture) MDPs (Shani et al., 2020; Cai et al., 2020; He et al., 2022) critically depend on the point-wise optimism for each state-action pair, _i.e._, \(_{k}^{_{k}}(s,a)_{k}(s,a)+[P^{} _{k}^{_{k}}](s,a)\), to enable the decomposition (_cf._, Lemma 1 by Shani et al. (2020))

\[_{k}^{_{k}}(s_{0})-V_{k}^{^{}}(s_{0}) =[_{=0}^{}^{} _{k}( s_{})-^{}( s_{}), {Q}_{k}^{_{k}}(s_{},)\,^{},P ^{},s_{0}]\] \[+[_{=0}^{}^{}( {Q}_{k}^{_{k}}(s_{},a_{})-_{k}(s_{},a_{})- [P^{}_{k}^{_{k}}](s_{},a_{} ))\,^{},P^{},s_{0}]\,,\]

where \(_{k}^{_{k}}\) and \(_{k}^{_{k}}\) are the state value and state-action value functions of \(_{k}\) on \((_{k},_{k}-_{k})\) with \(_{k}\) as some bonus function and the expectation is taken over the randomness of sampling \(a_{}^{}( s_{})\) and \(s_{+1} P^{}( s_{},a_{})\). The summation of the first term in the above display is contributed by competing with the optimal policy \(^{}\) in the _true_ model \(P^{}\) and can be bounded by common OMD analysis, which thus can be regarded as conducting policy optimization in the _true_ model. The point-wise optimism guarantees that the second term is less than or equal to \(0\).

Nevertheless, in low-rank MDPs, due to the unknown representation, it is generally hard to obtain the above point-wise optimism, which leaves the second optimism term unbounded. To cope with this issue, we instead consider the following decomposition:

\[_{k}^{_{k}}(s_{0})-V_{k}^{^{}}(s_{0})\]\[= _{k}^{_{k}}(s_{0})-_{k}^{^{*}}(s_{0})+ _{k}^{^{*}}(s_{0})-V_{k}^{^{*}}(s_{0})\] \[= [_{=0}^{}^{} _{k}( s_{k,})-^{*}( s_{k,}), _{k}^{_{k}}(s_{k,},)|^{*},_{k},s_{ 0}]+_{k}^{^{*}}(s_{0})-V_{k}^{^{*}}(s_{0})\,,\] (2)

where the first term is contributed by competing against the optimal policy \(^{}\) in the _learned_ model \(_{k}\) and can be seen as conducting policy optimization in _learned_ models. This decomposition will be amenable as long as we can achieve a _near optimism_ at the initial state \(s_{0}\), _i.e._, \(_{k}^{^{*}}(s_{0})-V_{k}^{^{*}}(s_{0}) 0\), which turns out to be feasible for low-rank MDPs (Uehara et al., 2022). However, there remains one more caveat. The first term in Eq. (2) is now no longer directly bounded by OMD analysis, due to the local update nature of OMD-based policy optimization at each state and the state occupancy distribution \(d_{_{k}}^{^{*}}\) now varies across different episodes. To address this issue, Algorithm 1 adopts an epoch-based transition update, in which one epoch has \(L\) episodes and the model is only updated at the first episode in one epoch (Line 15 - Line 20).4 Concretely, Algorithm 1 sets \(_{k}=_{k_{i}}\) and \(_{k}=_{k_{i}}\), where \(k_{i}\) is the first episode of the epoch to which the episode \(k\) belongs. In this manner, the learned model is _fixed_ in one epoch, and thus the regret of dealing with the adversarial loss functions by competing against the optimal policy \(^{}\) can be bounded in one epoch. Subsequently, at the end of episode \(k\), our algorithm first computes the optimistic value estimate \(_{k}^{_{k}}\) for current policy \(_{k}\) under \(_{k}\) together with the bonus-enhanced loss functions \(_{k}-_{k}\) by policy evaluation (Line 22). Note that this boils down to planning in the setting of linear MDPs for given features in the learned model and this can be done computationally efficiently (Jin et al., 2020b). Then the policy is updated by solving

\[_{k+1}( s)*{arg\,min}_{( s) ()}( s),_{k}^{ _{k}}(s,)+D_{F}(( s),_{k}( s ))\,,\] (3)

where \(>0\) is the learning rate to be tuned later and \(D_{F}(x,y)=F(x)-F(y)- x-y, F(y)\) is the Bregman divergence induced by the regularizer \(F\). With \(F(( s))=_{a}(a s)(a s)\) as the negative entropy, the closed-form solution to the above display is shown in Line 23, which can be regarded as a kind of soft policy improvement.

## 4 Analysis

### Regret Upper Bound

The regret upper bound of our POLO algorithm for learning adversarial low-rank MDPs is guaranteed by the following theorem.

**Theorem 4.1**.: _Suppose \(K>d^{6}A^{3}/(1-)^{6}\). For any adversarial low-rank MDP satisfying Definition 2.1, by setting the epoch length \(L=K^{}{{2}}}A^{}{{2}}}d^{-1}(1-)\), learning rate \(=(1-)\), bonus coefficient \(_{k}=O()(Mk/)})\), regularization coefficient \(_{k}=O(d(Mk/))\), mixing coefficient \(=K^{}{{6}}}A^{}{{2}}}d/(1-)\), and \(=1/K\), then the regret of Algorithm 1 is upper bounded by_

\[_{K}=O(}A^{}d(1+AMK^{2} )}{(1-)^{2}})\,.\]

**Remark 4.1**.: _Ignoring the dependence on all logarithmic factors but \(M\), the regret upper bound can be simplified as \((K^{}{{6}}}A^{}{{2}}}d(1+M)/(1- )^{2})\). Comparing with the regret lower bound \((}{1-})\) in Theorem 4.2, the regret upper bound in Theorem 4.1 matches in \(A\) up to a logarithmic factor but looses in factors of \(K\) and \(d\). Also, note that when \(K\) is large enough such that \(\) and \(L\) can be chosen as \(=K^{}{{6}}}A^{}{{3}}}d^{}{{3}}}/(1- )\) and \(L=K^{}{{2}}}A^{}{{2}}}d^{-2}(1-)=K^{ }{{3}}}A^{}{{3}}}d^{}{{3}}} 1\), meaning that \(K d^{4}A^{2}\), the regret upper bound can be further optimized to \((K^{}{{6}}}A^{}{{3}}}d^{} {{3}}}(1+M)/(1-)^{2})\). Note that this does not conflict with the regret lower bound in Section 4.3 since the magnitude of this upper bound is still larger than that of the regret lower bound as long as \(K A^{}{{2}}}d^{}{{2}}}^{6}(1-)^{3}\)._

### Proof of Regret Upper Bound

We now present the proof of Theorem 4.1. To begin with, recall that in each episode \(k\), after state \(s_{k}\) is sampled from \(d_{P^{*}}^{_{k}}\), the actual roll-out policy will be \(_{k}( s)= U()+(1-)_{k}(  s)\). Therefore, it holds that

\[_{K} =[_{k=1}^{K}(V_{k}^{_{k}}-V_{k}^{^{ *}})]\] \[=[_{k=1}^{K}\{c_{k}=1\}(V_{k}^ {_{k}}-V_{k}^{^{*}})+\{c_{k}=0\}(V_{k}^{_{k}}-V_ {k}^{^{*}})]\] \[[_{k=1}^{K}(V_{k}^{_{k}}-V_ {k}^{^{*}})]+\,,\] (4)

where the inequality is due to that \(_{=0}^{}^{}_{k}(s_{},a_{})[0,1/(1-)]\) holds for any episode \(k\) and any trajectory \(\{(s_{},a_{})\}_{=0}^{}\). We now turn to bound the first term in Eq. (4) by decomposing it into the following three terms

\[[_{k=1}^{K}(V_{k}^{_{k}}-V_{k}^{^{*}} )]=[^{K}(V_{k}^{_{k}}-_{k}^{_{k}})}_{}+^{K}(_{k}^{_{k}}- _{k}^{^{*}})}_{}+^{K} (_{k}^{^{*}}-V_{k}^{^{*}})}_{}]\,.\] (5)

Bounding OMD Regret TermThe OMD Regret Term is contributed by competing against \(^{*}\) using \(_{k}\) with \(_{k}^{_{k}}\) as loss function in the learned model \(_{k_{i}}\). This term is thus bounded by standard OMD analysis, detailed in the following lemma.

**Lemma 4.1**.: _By setting \(=(1-)\), the OMD Regret Term is bounded as \([_{k=1}^{K}(_{k}^{_{k}}- {V}_{k}^{^{*}})]}{}}\)._

Bounding Optimism TermThe Optimism Term is controlled by choosing appropriate bonus coefficient \(_{k}\). Note that different from tabular and linear cases, the bonus functions and coefficients here are not devised to control the optimism for each state-action pair. Instead, they are devised to provide a (near) optimism only at the initial state \(s_{0}\).

**Lemma 4.2**.: _By setting \(_{k}=O()(Mk/)})\) and \(_{k}=O(d(Mk/))\), with probability \(1-\), the Optimism Term is bounded as \(_{k=1}^{K}(_{k}^{^{*}}-V_{k}^{^{*}})(L+ )}}\)._

Bounding Estimation Bias TermIt remains to bound the Estimation Bias Term, which comes from the difference between the values of running the same policy \(_{k}\) in the true model (_i.e._, \(P^{*}\) and \(_{k}\)) and the learned empirical model (_i.e._, \(_{k}\) and \(_{k}-_{k}\)), respectively. This term can be translated into the error between the true model and the learned model using the common simulation lemma, which is thus bounded by the summation of bonus functions. Note that since the empirical features used to construct our bonus functions vary in each episode, we first relate the bonus functions with the fixed true feature \(^{*}\) using the one-step trick [Uehara et al., 2022, Zhang et al., 2022], and finally bound this term with the leverage of the canonical elliptical potential lemma. The result is shown in the following lemma.

**Lemma 4.3**.: _By setting \(_{k}=O()(Mk/)})\) and \(_{k}=O(d(Mk/))\), with probability \(1-\), the Estimation Bias Term is bounded as \(_{k=1}^{K}(V_{k}^{_{k}}-_{k}^{_{k}} ) O(}{(1-)^{3}})\)._

We refer the readers to Appendix A for the proof of the above lemmas. The proof of Theorem 4.1 is now concluded by first combining Eq. (4), Eq. (5), Lemma 4.1, 4.2, and 4.3 and then choosing \(L=K^{}{{2}}}A^{-}{{2}}}d^{-1}(1-)\), \(=K^{-}{{6}}}A^{}{{2}}}d/(1-)\), and \(=1/K\).

Intuitively, the epoch length \(L\) illustrates a trade-off between dealing with the adversarial losses and the representation learning over the unknown transitions. When \(L\) is large, there will be fewer restarts in the running of OMD and thus the learner will suffer less regret contributed by dealing with the adversarial losses as shown by Lemma 4.1. In contrast, a smaller \(L\) enables more frequent model updates, which leads to more accurate model estimation and less regret contributed by the representation learning as shown by Lemma 4.2 and 4.3.

### Regret Lower Bound

This section presents the regret lower bound for learning adversarial low-rank MDPs with fixed loss functions in Theorem 4.2, which thus also serves as a regret lower bound for learning adversarial low-rank MDPs with full-information feedback.

**Theorem 4.2**.: _Suppose \(d 8\), \(S d+1\), \(A d-3\), and \(K 2(d-4)A\). Then for any algorithm \(\), there exists an episodic infinite-horizon low-rank MDP \(_{}\) with fixed loss function such that the regret for this MDP is lower bounded by \((}{1-})\)._

Proof Sketch.: At a high level, we construct \(dA\) hard-to-learn low-rank MDP instances, which are difficult to distinguish in KL divergence but have very different optimal policies. In particular, all the constructed low-rank MDP instances have three levels of states, in which the only state in the first level is a fixed initial state and the states in the third level are absorbing states. Moreover, only one unique absorbing state in the third level has the lowest loss, which is termed as the "good state". In the constructed low-rank MDP instance \(_{(i^{},a^{})}\), the learner can only take specific action to transfer to state \(s_{2,i^{}}\) in the second level and then take the other specific action to transfer to the unique good state. Due to the unknown representations of state-action pairs, the learner needs to distinguish all these \(dA\) low-rank MDP instances, which is essentially equivalent to dealing with a bandit problem with \(dA\) "arms". The detailed proof of Theorem 4.2 is postponed to Appendix B. 

**Remark 4.2**.: _Theorem 4.2, to the best of our knowledge, provides the first regret lower bound for learning low-rank MDPs with fixed loss functions. We note that this regret lower bound can hold when \(d S\) and \(d A\), which thus means that this lower bound is non-trivial. Besides, the regret upper bound in our Theorem 4.1 matches the regret lower bound in \(A\) up to a logarithmic factor but looses a factor of \((K^{}{{3}}}d^{}{{2}}}/((1-) ^{2}))\). Importantly, compared with the regret upper bound \((d})\) of linear MDPs  (the finite horizon \(H\) is substituted by the effective horizon \((1/(1-))\) in our infinite-horizon setting for a fair comparison), the dependence on \(A\) in the regret lower bound of low-rank MDP shows a clear separation between low-rank MDPs and linear MDPs, which demonstrates that low-rank MDPs are statistically more difficult to learn than linear MDPs in the regret minimization setting._

## 5 Conclusions

In this work, we study learning adversarial low-rank MDPs with unknown transition and full-information feedback. We prove that our proposed algorithm POLO achieves the \((K^{}{{6}}}A^{}{{2}}}d(1+M)/(1- )^{2})\) regret, which is the first sublinear regret guarantee for this challenging problem. The design of our proposed algorithm features (a) a doubled exploration and exploitation scheme to simultaneously learn the transitions and adversarial loss functions; and (b) policy optimization in the fixed learned models with epoch-based model update to enable a new analysis scheme that only requires the near optimism at the initial state instead of the point-wise optimism. Also, we prove an \((}{1-})\) regret lower bound for this problem, serving as the first regret lower bound for learning low-rank MDPs in the regret minimization setting. Besides, there also remain several interesting future directions to be explored. One natural question is whether it is possible to further optimize the dependence of our regret guarantee on the number of episodes \(K\). The other question is how to also learn adversarial low-rank MDPs with only the bandit feedback available. This is also challenging since the current occupancy measure-based methods and policy optimization-based methods tackling adversarial MDPs with bandit feedback both depend on the point-wise optimism provided by the true feature mapping, which seems not feasible in low-rank MDPs. We hope our results may shed light on better understandings of RL with both nonlinear function approximation and adversarial losses and we leave the above extensions as our future works.

## Limitations

We note that in general our algorithm is oracle-efficient (given access to the MLE computation oracle in Eq. (1)) but may not be computationally efficient as previous works studying low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Zhang et al., 2022; Ni et al., 2022). However, we also remark that in practice, these algorithms including ours are computationally feasible since the computation of MLE is only a standard supervised learning problem and can be implemented using gradient descent methods. The other limitation is that throughout this paper, we assume model class \(\) with bounded cardinality \(M\) and the regret upper bound of our algorithm has a logarithmic dependence on \(M\). We remark that similar assumptions and dependence have also appeared in previous theoretical works studying RL with general function approximation (Jiang et al., 2017; Sun et al., 2019). Also, extending the analyses to an infinite hypothesis class is possible if the hypothesis class has bounded statistical complexity (Agarwal et al., 2020).