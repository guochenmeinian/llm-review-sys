# Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees

Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees

 Taiki Miyagawa*

Independent Researcher

chopin.grande.valse.brillatnte@gmail.com

&Takeru Yokota*

Interdisciplinary Theoretical and Mathematical Sciences Program (iTHEMS), RIKEN

Center for Quantum Computing, RIKEN

takeru.yokota@riken.jp

These authors contributed equally to this work.

###### Abstract

We propose the first learning scheme for functional differential equations (FDEs). FDEs play a fundamental role in physics, mathematics, and optimal control. However, the numerical analysis of FDEs has faced challenges due to its unrealistic computational costs and has been a long standing problem over decades. Thus, numerical approximations of FDEs have been developed, but they often oversimplify the solutions. To tackle these two issues, we propose a hybrid approach combining physics-informed neural networks (PINNs) with the _cylindrical approximation_. The cylindrical approximation expands functions and functional derivatives with an orthonormal basis and transforms FDEs into high-dimensional PDEs. To validate the reliability of the cylindrical approximation for FDE applications, we prove the convergence theorems of approximated functional derivatives and solutions. Then, the derived high-dimensional PDEs are numerically solved with PINNs. Through the capabilities of PINNs, our approach can handle a broader class of functional derivatives more efficiently than conventional discretization-based methods, improving the scalability of the cylindrical approximation. As a proof of concept, we conduct experiments on two FDEs and demonstrate that our model can successfully achieve typical \(L^{1}\) relative error orders of PINNs \( 10^{-3}\). Overall, our work provides a strong backbone for physicists, mathematicians, and machine learning experts to analyze previously challenging FDEs, thereby democratizing their numerical analysis, which has received limited attention. Code is available at https://github.com/TaikiMiyagawa/FunctionalPINN.

## 1 Introduction

Functional differential equations (FDEs) appear in a wide variety of research areas [91; 92; 79]. FDEs are partial differential equations (PDEs) involving functional derivatives, where a functional \(F\) is a function of an input function \((x)\) to a real number, i.e., \(F: F([])\), and a functional derivative is defined as the derivative of functional w.r.t. the input function at \(x\), denoted by \( F([])/(x)\). FDEs play a fundamental role in Fokker-Planck systems , turbulence theory , quantum field theory , mean-field games , mean-field optimal control [18; 81], and unnormalized optimal transport . Major examples of FDEs include the Hopf functional equation in fluid mechanics, the Fokker-Planck functional equation in the theory of stochastic processes, and the functional Hamilton-Jacobi equation in optimal control problems in density spaces.

Despite their wide applicability, numerical analyses of FDEs are known to suffer from significant computational complexity; therefore, numerical approximation methods have been developed over decades. They include the functional power series expansion , the Reynolds number expansion , finite difference methods , finite element methods , tensor decomposition methods , and the cylindrical approximation .

However, they tend to oversimplify the solution of FDEs, prioritizing the reduction of computational costs. The functional power series expansion is applicable only to input functions close to the expansion center. Moreover, it has no convergence guarantees in general . The Reynolds number expansion requires the Reynolds number to be close to zero, severely restricting its applicability, because the Reynolds number for turbulent flow can be \( 1000\). Discretization-based methods such as the finite difference and element methods restrict spacetime resolution and/or the class of input functions and functional derivatives . Existing methods relying on the cylindrical approximation, akin to the spectral method for PDEs, include tensor decomposition  to reduce computational costs; however, they tend to significantly simplify the class of input functions and functional derivatives. For instance, their expressivity is limited to polynomials or Fourier series of a few degrees.

To address the notorious computational complexity and limited approximation ability, we propose a hybrid approach combining physics-informed neural networks (PINNs) and the cylindrical approximation (Fig. 1). In the first stage, we expand the input function with orthonormal basis functions, thereby transforming a given FDE into a high-dimensional PDE of the expansion coefficients. This approximation is referred to as the _cylindrical approximation_. We prove the convergence of the approximated functional derivatives and FDE solutions, validating the reliability of the cylindrical approximation for FDE applications, which is our main theoretical contribution. In the second stage, the derived high-dimensional PDE is numerically solved with a PINN, which is known to be a universal approximator tailored to solve high-dimensional PDEs efficiently in a mesh-free manner.

A notable advantage of our approach is that, with the help of PINNs, it reduces the computational complexity by orders of magnitude, compared with previous discretization-based methods. In fact, it requires only \((m^{r})\), where \(m\) represents the "class size" of input functions and functional derivatives (e.g., the degree of polynomials), and \(r\) (\( 1\)) denotes the order of the functional derivative included in the target FDE (typically \(1\) or \(2\)). This is a notable reduction from the state-of-the-art cylindrical approximation algorithm , which requires as large as \((m^{6})\). Consequently, our approach substantially extends the class of input functions and functional derivatives that can be represented by the cylindrical approximation. For instance, our approach extends the degrees of polynomials or Fourier series used for the approximation from \(6\) to \(1000\), showing unprecedented expressivity.

As a proof of concept, we conduct experiments on two FDEs: the functional transport equation and the Burgers-Hopf equation. The results show that our model accurately approximates not only the solutions but also their functional derivatives, successfully achieving typical \(L^{1}\) relative error orders of PINNs \( 10^{-3}\).

Our contribution is threefold. (1) We propose the first learning scheme for FDEs to address the significant computational complexity and limited approximation ability. Our model exponentially extends the class of input functions and functional derivatives that can be handled accurately and efficiently. (2) We prove the convergence of approximated functional derivatives and FDE solutions, ensuring the cylindrical approximation to be safely applied to FDEs. (3) Our experimental results show that our model accurately approximates not only the FDE solutions but also their functional derivatives.

Figure 1: **Overall architecture. An FDE is simplified to a high-dimensional PDE via the cylindrical approximation. The PDE is solved with a PINN. The approximated functional derivative can be efficiently computed with automatic differentiation.**

## 2 Related Work

FDEs are prevalent across numerous research fields [27; 24; 32; 16; 18; 81; 67; 71]. Research on FDEs has mainly focused on their theoretical aspects and formal solutions, with very few algorithms available to numerically solve general FDEs [18; 79; 91; 104]. In , a numerical method specialized for the Hamilton-Jacobi functional equation for optimal control problems in density space is proposed, based on spacetime discretization. Similarly,  employs spacetime discretization with tensor decomposition. The state-of-the-art algorithm proposed in , the CP-ALS (Canonical Polyadic tensor expansion & Alternating Least Squares) algorithm, uses the cylindrical approximation along with the finite difference method and tensor decomposition, requiring \((m^{6})\) (see App. F.2 for the derivation), whereas our model requires only \((m^{r})\) (\(r\) is \(1\) or \(2\) in most FDEs). Furthermore, our model does not require such discretization, making it mesh-free. See App. B for an additional introduction to FDEs and their approximations.

The cylindrical approximation originates from the theory of stochastic processes [34; 88]. It is reminiscent of the spectral method for PDEs  and is a generalization to FDEs. Convergence theorems of the cylindrical approximation are summarized in a recent seminal paper . Note that the cylindrical approximation in this paper (Eq. (2)) is different from the one in , tailored for practical use. Consequently, our convergence theorems also differ from those in . See App. C.4.2 for technical details. See App. A for more comparisons with other studies.

## 3 Proposed Approach

### Step 1: Cylindrical Approximation

We first introduce the _cylindrical approximation of functionals, functional derivatives, and FDEs_, beginning with the expansion of input functions and culminating in the transformation of FDEs into high-dimensional PDEs. Additionally, we prove the convergence theorems for this approximation. The rigorous mathematical background is reviewed in App. C for interested readers.

Firstly, we define the _cylindrical approximation of functionals_[7; 29; 34; 88]. Any function \(\) in a real separable Hilbert space \(H\) can be represented uniquely in terms of an orthonormal basis \(\{_{k}\}_{k=0}^{}\) as \((x)=_{k=0}^{}a_{k}_{k}(x)\), where \(a_{k}:=(,_{k})_{H}\) are the _coefficients_ (or spectrum) of \(\) in terms of \(\{_{k}\}_{k 0}\), and \((,)_{H}\) denotes the inner product of \(H\). Substituting this expansion to functional \(F([])\), we can define a multivariable function \(f(\{a_{k}\}_{k=0}^{}):=F([_{k=0}^{}a_{k}_{k}])\) for any functional \(F:H\). Truncating \(k\) at \(m-1_{ 0}\) gives the cylindrical approximation of functionals:

\[f(\{a_{k}\}_{k=0}^{m-1}):=F([P_{m}]),\] (1)

where \(P_{m}\) is the projection operator s.t. \(P_{m}(x):=_{k=0}^{m-1}a_{k}_{k}(x)\), and \(m\) is referred to as the _degree_ of approximation. See Thm. C.19 and Thm. C.20 for the uniform convergence and convergence rate of this approximation, originally given by [75; 92].

Secondly, we define the _cylindrical approximation of functional derivatives_. The functional derivative of \(F\) w.r.t. \(\) at \(x\) is defined as \(:=_{ 0} \), where \((x)\) denotes the Dirac delta function. This definition is impractical to simulate on computers with spacetime discretization; thus, we employ the expansion \(=_{k=0}^{}(,_{k})_{H}_{k}(x)\). The expansion is possible because \(\) itself is a function of \(x\) in \(H\) and thus can be represented as an orthonormal basis expansion. Note that the expansion coefficients \((,_{k})_{H}\) are known to be equal to \(}\) (see App. C.4.2 for the proof). Hence, truncating \(\) at \(m-1\) gives the cylindrical approximation of functional derivatives:

\[P_{m}])}{(x)}=_{k=0}^{m-1}}_{k}(x)\,.\] (2)

Note that Eq. (2) is different from the cylindrical approximation adopted in [91; 92]. They do not apply \(P_{m}\) to \( F([P_{m}])/(x)\), and the emerging "tail term" \(_{k=m}^{}( F([])/,_{k})_{k}(x)\) is simply ignored without any rationale.

The first main theoretical contribution of our work is the following convergence theorem of Eq. (2).

**Theorem 3.1** (Pointwise convergence of approximated functional derivatives (informal)).: _For arbitrary \( H\) and orthonormal basis \(\{_{0},_{1},\}\), Eq. (2) converges to \(\) as \(m\)._

The formal statement and proof are given in App. D.1. The convergence rate is the same as \(\|-P_{m}\|\) if \( F([])/(x)\{_{0},,_{m-1}\}\). A technical discussion when this is not the case is provided in App. D.

Finally, we define the _cylindrical approximation of FDEs_. In this paper, we consider the _abstract evolution equation_, a class of FDEs having the following form:

\[ F([],t)/ t=([])F([],t)\] (3)

with \(F([],0)=F_{0}([])\), where \(([])\) is a linear functional operator, and \(F_{0}\) is a given initial condition. The abstract evolution equation is a crucial class of FDEs in physics, mathematics, and engineering, including the Hopf functional equation, Fokker-Planck functional equation, and functional Hamilton-Jacobi equation. The cylindrical approximation of the abstract evolution equation is given by

\[ f(,t)/ t=_{m}([])f(,t)\] (4)

with \(f(,0)=f_{0}()\), where \(:=(a_{0},,a_{m-1})^{}\), \(f(,t):=F([P_{m}],t)\), and \(f_{0}():=F_{0}([P_{m}])\). The operator \(_{m}([])\) is the cylindrical approximation of \(([])\). Examples are given in Secs. 3.1.1 & 3.1.2.

The second main theoretical contribution of our work is the following convergence theorem of solutions:

**Theorem 3.2** (Convergence of approximated solutions (informal)).: _Under the cylindrical approximation (Eq. (2)), if the FDE depends on functional derivatives only in the form of the inner product (\(v,)_{H}\) (\(v H\)), then, the solution of the approximated abstract evolution equation (\(F([P_{m}],t)\)) converges to the solution of the original one (\(F([],t)\)) as \(m\)._

The proof is given in App. E. The convergence is visualized in Fig. 2. Similar theorems for the FDEs with the second or higher-order functional derivatives can be shown in a similar way. The inner-product assumption in Thm. 3.2 is satisfied by major FDEs, such as the Hopf functional equation, Fokker-Planck functional equation, and functional Hamilton-Jacobi equation.

In the following, we apply the cylindrical approximation to two FDEs: the functional transport equation (FTE) and the Burgers-Hopf equation (BHE).

#### 3.1.1 Application 1: Functional Transport Equation

We first construct a simple FDE, the _functional transport equation_ (FTE), which is a generalization of the transport equation (the continuity equation) . The FTE is provided by

\[F([],t)=- dx\,u(x),\] (5)

with the initial condition \(F([],0)=F_{0}([])\), where \(x[-1,1]\), and \(u(x)\) is a given function. Specifically, we use the initial condition \(F([],0)=_{0} dx\,u(x)(x)\) with \(_{0}\) a constant. The exact solution is \(F([],t)=F_{0}([-ut])=_{0} dx\,u(x)((x)-u(x)t)\), as can be seen by substituting this into Eq. (5). More details and motivations of the FTE are provided in App. E.1.

The cylindrical approximation of the FTE is given by

\[f(,t)=-_{k=0}^{m-1}u_{k}}f(,t)\] (6)

Figure 2: **Cylindrical approximation of FDE’s solution. The \(L^{1}\) relative error, defined as \(_{i=1}^{N}|F([_{i}])-F([P_{m}_{i}])|/|F([_{i }])|\), diminishes with increasing \(m\). The Burgers-Hopf equation with the delta initial condition (Sec. 3.1.2) is considered. Note that PINN’s training is not included.**with the initial condition \(f(,0)=f_{0}()\), where \((x)=_{k=0}^{m-1}a_{k}_{k}(x)\), and \(f_{0}()=F_{0}([P_{m}])=_{0}_{k=0}^{m-1}u_{k}a_{k}\) with \(u_{k}:=(u,_{k})_{L^{2}([-1,1])}= dxu(x)_{k}(x)\). We use the Legendre polynomials as the orthonormal basis \(\{_{k}\}_{k 0}\). The exact solution of Eq. (6) is \(f(,t)=_{0}_{k=0}^{m-1}u_{k}(a_{k}-u_{k}t)\).

In our experiments in Sec. 4, we consider two types of FTEs: (i) \(u_{k}:=_{0}\) for \(k=1\) (0 otherwise) and (ii) \(u_{k}:=_{0}\) for \(k 14\) (0 otherwise), where \(_{0}\) is a constant. For convenience, we call them the _linear_ and _nonlinear initial conditions_, respectively. The high-dimensional PDE thus obtained (Eq. (6)) is solved with PINNs (Sec. 3.2).

#### 3.1.2 Application 2: Burgers-Hopf Equation

The second FDE is the _Burgers-Hopf equation_ (BHE), a crucial equation in turbulence theory:

\[= dx(x)}{ x^{2}}\] (7)

with the initial condition \(F([],0)=F_{0}([])\), where \(x[-1/2,1/2]\). Specifically, we use the Gaussian initial condition \(F_{0}([])=- dx(x)+ dx dx^{ }C(x,x^{})(x)(x^{})\), where \(\) is a constant, and \(C(x,x^{})\) is the infinite-dimensional covariance matrix. The exact solution is \(F([],t)=F_{0}([])\), where \(([],x,t):=}_{-}^{}dx^{ }e^{-(t-x^{})^{2}}(x^{})\). The derivation is provided in App. D.2.1. Strictly speaking, Eq. (7) is a modification of the original BHE. The modification includes making the BHE dimensionless and neglecting the advection term. For more technical details, see App. E.2.

The cylindrical approximation of the BHE is given by

\[f(,t)=_{k=0}^{m-1}_{l=0}^{m-1} dx (x)}{ x^{2}}_{l}(x)a_{k}}f(,t)\] (8)

with the initial condition \(f(,0)=f_{0}()\), where \((x)=_{k=0}^{m-1}a_{k}_{k}(x)\), \(f_{0}()=F_{0}([P_{m}])=-a_{0}+_{k,l=0}^ {m-1}_{kl}a_{k}a_{l}\) with \(_{kl}:= dx dx^{}C(x,x^{})_{k}(x)_{l}(x^ {})\). We use the Fourier series as the orthonormal basis: \(\{_{k}(x)\}_{k 0}=\{1,( kx),( kx)\}_{k 1}\). Then, the exact solution under the cylindrical approximation is given by

\[f(,t)=-a_{0}+_{k,l=0}^{M-1}(e^{-4^{ 2}(k^{2}+l^{2})t}_{2k,2l}a_{2k}a_{2l}\] \[+e^{-4^{2}(k^{2}+(l+1)^{2})t}_{2k,2l+1}a_{2k}a_{2l+1} +e^{-4^{2}((k+1)^{2}+l^{2})t}_{2k+1,2l}a_{2k+1}a_{2l}\] \[+e^{-4^{2}((k+1)^{2}+(l+1)^{2})t}_{2k+1,2l+1}a_{2k+1} a_{2l+1}/2\,,\] (9)

where \(m=2M\) (\(M\)). The derivation is given in App. D.2.2.

In our experiments in Sec. 4, we adopt two types of the covariance matrices: (i) \(_{ij}=^{2}\) for all \(i=j 0\) (0 otherwise) and (ii) \(_{ij}=^{2}\) for \(i=j=0\) (0 otherwise), where \(^{2}\) is a constant. Substituting (i) and (ii) into \(f_{0}\), we have two types of initial conditions, which we call the _delta and constant initial conditions_, respectively. Again, the high-dimensional PDE thus obtained (Eq. (8)) is solved with PINNs (Sec. 3.2).

### Step 2: Solving Approximated FDEs with PINNs

We briefly introduce the foundation of PINNs . PINNs are universal approximators and can solve PDEs. Let us consider a PDE \( f(t,x)/ t=[f]\) with an initial condition \([f]|_{[-0}=0\), where \(t\), \(x[-1,1]\). \(\) and \(\) are operators defining the PDE and the initial condition, respectively. The PINN aims to approximate the solution \(f(t,x)\). Thus, the inputs to the PINN are \(t\) and \(x\), randomly sampled from \(\) and \([-1,1]\), respectively. Note that \((t=0,x)\) with \(x[-1,1]\) are also input

Figure 3: **PINN’s architecture.**

to the PINN to compute the boundary loss. The inputs are transformed through linear layers and activation functions. The final output of the PINN is an approximation of \(f(t,x)\), denoted by \((t,x)\). The loss function is the weighted sum of the residual loss \(\|(t,x)/ t-[]\|\) and the boundary loss \(\|B[]\|_{t=0}\|\), where \(\|\|\) is a certain norm. The partial derivatives in the loss function can be computed via automatic differentiation of the PINN's output \(\) w.r.t. the inputs \((t,x)\). Finally, the weight parameters of the PINN are minimized through backpropagation.

Next, we explain how to apply PINNs to our high-dimensional PDEs: see Fig. 3. For concreteness, consider the FTE (Eq. (6)) with the linear initial condition. The inputs to the PINN are \((t,a_{0},a_{1},,a_{m-1})\) and \((0,a_{0},a_{1},,a_{m-1})\), randomly sampled from finite intervals, and the outputs are \((,t)\) and \((,0)\), respectively. Then, \((,t)/ t\) and \((,t)/\) are computed via automatic differentiation, and we obtain the residual loss \(\|(,t)/ t+_{k=0}^{m-1}u_{k}( ,t)/ a_{k}\|\) and the boundary loss \(\|_{0}()-_{0}_{k=0}^{m-1}u_{k}a_{k}\|\), where \(_{0}():=(,0)\). These losses are minimized via mini-batch optimization.

Computational ComplexityThe total computational complexity w.r.t. \(m\) up to the computation of functional derivatives is given by \((m)+(m^{r})=(m^{r})\), where \(r 1\) is the order of the functional derivative included in the target FDE (typically \(1\) or \(2\)). The first term \((m)\) comes from the input layer of the PINN. The second term \((m^{r})\) comes from the computation of functional derivatives under the cylindrical approximation (Eq. (2)). See App. F.4 for more detailed discussions on computational complexity.

This is a notable reduction from discretization-based methods such as finite difference and element methods, which typically require exponentially large computational complexity w.r.t. the dimension of PDE \(m\). Also, \((m^{r})\) is significantly smaller than the state-of-the-art cylindrical approximation algorithm, the CP-ALS , which requires \((m^{6})\) (the derivation is given in App. F.2). Consequently, given that \(m\) represents the "class size" of input functions and functional derivatives (Eqs. (1) & (2)), our approach significantly extends the range of input functions and functional derivatives that can be represented via the cylindrical approximation. In fact, our approach extends the degrees of polynomials or Fourier series used for the approximation from \(6\) to \(1000\) (Sec. 4).

Finally, we note that the selection of basis functions influences computational efficiency. The choice depends on the specific FDE, boundary conditions, symmetry, function spaces, and numerical stability. For further discussions, see App. F.3).

In summary, our proposed approach transforms an FDE into a high-dimensional PDE using cylindrical approximation and then solves it with a PINN, which serves as a universal approximator of the solution (Figs. 1 & 3). It is important to note that our model employs the basic PINN framework, allowing for seamless integration with any techniques developed within the PINN community.

## 4 Experiment

   Degree &  \\ 
4 & \((1.26820 0.31421) 10^{-3}\) \\
20 & \((2.01716 0.21742) 10^{-3}\) \\
100 & \((6.24740 0.33492) 10^{-3}\) \\  Degree &  \\ 
4 & \((1.32203 0.44061) 10^{-3}\) \\
20 & \((2.29632 0.16459) 10^{-3}\) \\
100 & \((1.23312 0.18931) 10^{-3}\) \\  Degree &  \\ 
4 & \((1.79295 0.28535) 10^{-3}\) \\
100 & \((7.63769 0.90872) 10^{-3}\) \\
1000 & \((8.27096 1.19378) 10^{-3}\) \\  Degree &  \\ 
4 & \((2.37627 0.15278) 10^{-4}\) \\
100 & \((1.84506 0.15765) 10^{-3}\) \\
1000 & \((1.76470 0.36885) 10^{-3}\) \\ 
1000 & \((1.76470 0.36885) 10^{-3}\) \\   

Table 1: **Mean relative and absolute errors of models trained on FTEs under linear (top two) and nonlinear (bottom two) initial conditions.** I.C. is short for “initial condition”. The error bars are the standard deviation over 10 training runs with different random seeds. Note that this table is not for the assessment of the theoretical convergence of the cylindrical approximation (see Fig. 2, App. H.6, and the footnote in Sec. 4.1 instead).

[MISSING_PAGE_EMPTY:7]

Figure 5: **Absolute error of first-order functional derivative of FTE with degree 100 (top) and 1000 (bottom) under linear (top) and nonlinear (bottom) initial conditions.** The error bars represent the standard deviation over 10 runs with different random seeds.

Figure 6: **Relative error of first-order functional derivative of BHE with degree 100 under delta initial condition.** The error bars represent the standard deviation over 10 runs. The top/bottom four panels show the results with/without the loss function \(\|W(,t)\|\), respectively. With this loss function, the error reduces by \(10^{-1}\).

### Result : Functional Transport Equation

**Tab. 1** shows the main result.2 Our model achieves typical relative error orders of PINNs \( 10^{-3}\), even when the degree is as large as \(1000\), which means our model's capability of representing \(\) and \( F([],t)/(x)\) as polynomials of degree \(1000\). This is a notable improvement from the state-of-the-art algorithm , which can handle \(m 6\).

**Fig. 5** shows the absolute error of the first-order functional derivative estimated at \(t=1\) and \(=0\). Again, \(=0\) is not included in the training set. The errors in the top four panels increase at the edges of intervals (\(x= 1\)) due to Runge's phenomenon .

### Result: Burgers-Hopf Equation

**Tab. 2** shows the main result. Again, our model successfully achieves \( 10^{-3}\), the typical order of relative error of PINNs. See Fig. 2, App. H.6, and the footnote in Sec. 4.1 for the assessment of the theoretical convergence of the cylindrical approximation.

**Fig. 6** shows the relative error of first-order functional derivatives at \(=0\). Note again that some of the collocation points used for this figure are not included in the training dataset, highlighting the model's ability to extrapolate beyond its training dataset (\(a_{k} 0\)). Additionally, the error is reduced by a factor of \(10^{-1}\) by incorporating a loss term corresponding to the identity \(W(,t)=0\) (bottom four panels).

**Fig. 7** visualizes the analytic solution, prediction, and absolute error of a model trained on the BHE with degree \(20\) under the delta initial condition. The absolute error w.r.t. \(a_{0}\) is \(10^{-4}\) times smaller than the scale of the solution; i.e., the model learns \(\) well in the direction of \(a_{0}\), which dominates the analytic solution. Conversely, the absolute error w.r.t. \(a_{19}\) is on par with the scale of the solution. This result is anticipated because the dependence of the solution on \(a_{19}\) is much smaller than \(a_{0}\). This relationship is evident from Eq. (9), which indicates that the higher degree terms decay exponentially in terms of \(k\), \(l\), and \(t\), and the solution is dominated by \(a_{k}\) with \(k 1\). Therefore, optimizing the model in the direction of \(a_{19}\) has only a negligible effect on minimizing the loss function.

Finally, many additional experimental results are provided in App. H, including a comparison with the CP-ALS algorithm.

Figure 7: **Analytic solution (top four panels), prediction (second four panels), and absolute error (last four panels) of BHE with degree 20 under delta initial condition. The horizontal axes are \(a_{k}\) (\(k=0,1,2,\) or \(19\)). The other coefficients are kept 0. Our model successfully learns the BHE.**

## 5 Limitations

One limitation of our work is that the spacetime dimension is limited to \(1+1\) (\(t\) and \(x\)) in our experiments. However, generalization to \(1+d\) dimensions is feasible, albeit with additional computational costs. For \(d>1\) dimensional spaces, we have several options for expansion bases .

Another limitation is that the orders of functional derivatives in FDEs in our experiments are limited to \(r=1\). However, extending to \(r 2\) is straightforward. For instance, the cylindrical approximation of the second-order functional derivative is expressed as \(^{2}F([],t)/(x)(y)_{k,l=0}^{ m-1}^{2}f(,t)/ a_{k} a_{l}_{k}(x)_{l}(y)\), which can be computed via backpropagation twice.

Furthermore, this paper focuses exclusively on the abstract evolution equation. While this includes important FDEs (see Sec. 3), it does not cover certain equations, such as the Schwinger-Dyson equation or the Wetterich equation. Nonetheless, the mathematical foundations regarding the existence and uniqueness of these FDEs remain unestablished, which is beyond the scope of our paper. Once these foundations are defined, applying our model to these equations would be straightforward. More discussions on limitations, including technical ones, are provided in App. F.1.