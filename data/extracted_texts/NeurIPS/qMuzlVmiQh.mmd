# Topic-Conversation Relevance (TCR)

Dataset and Benchmarks

 Yaran Fan

Microsoft

yaran.fan@microsoft.com &Jamie Pool

Microsoft

jamie.pool@microsoft.com &Senja Filipi

Microsoft

sefilipi@microsoft.com &Ross Cutler

Microsoft

ross.cutler@microsoft.com

###### Abstract

Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective . To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-41 to evaluate the model accuracy in understanding transcription-topic relevance.

## 1 Introduction

Since the 2020 COVID-19 pandemic, an increasing share of meetings have shifted from in-person to online. The Gartner 2021 Digital Worker Experience Survey reports that the number of in-person meetings dropped from 63% in 2019 to 33% in 2021 . The same survey predicted that in 2024, only 25% of the meetings will happen in person.

Together with the growing number of online meetings, there are ongoing complaints about ineffective meetings due to a lack of focused discussions or focused tasks . Having a meeting facilitator to keep the discussions focused is one of the meeting design characteristics enabling more effective meetings .

Measuring how relevant a conversation transcript is to an intended topic is crucial to quantifying how focused the communication is, and to creating tools that behave as a virtual meeting moderator by keeping the discussion on-track. A very low rating on the relevance of the conversation to the topic meant for discussion would be an indicator of a non-focused discussion. In practice this translates to the problem of keeping discussions focused on a predefined meeting agenda.

While there is existing work about the importance of topics serving as input to text summarizing models , we could not find references about work studying the relevance of a topic to a particular body of text it didn't originate from. One of our intuitions for why this field has had little exploration is because of technological limitations before the recent Generative AI advancements.

With the current advancement in the field of Generative AI, deep understanding of language and relationships between bodies of text has reached new levels of accuracy, and has gotten very close to human performance [10; 11; 12; 13; 14].

To begin investing in measuring the relevance of a conversation to a predefined agenda topic, a comprehensive dataset of conversations associated with the topic of each conversation section is needed. Ideally, the topics should be defined before the conversation starts in a pre-meeting agenda style. There are several public datasets built from real human conversations that serve as the base for our Topic-Conversation Relevance (TCR) dataset; however, most of the topics from these datasets are post-meeting minutes that summarize what happens instead of what is planned.

The contributions of the TCR dataset are (1) We create a large topic-conversation dataset covering multiple domains of meetings. This dataset consists of the newly collected meetings and aggregated public data sources. (2) We use GPT-4 to rewrite long and detailed meeting minutes into a pre-meeting agenda topic style. (3) We provide a design of an extensible schema that allows users to create variations of meetings where topics can be flexibly added and removed. (4) We open source scripts for data augmentation and synthetic meeting creation on top of the TCR dataset.

We review the related works in Section 2. We present the datasets and the schema in Section 3, and elaborate to discuss the new SIM data collection and public data sources. In Section 4 we go over the benchmark results generated by GPT-4 on the Topic-Conversation Relevance task, and share insights from running such prompts across datasets. In Section 5, we describe limits and future work in this direction.

## 2 Related Work

In this paper, we refer to "topics" as the key points to be discussed during a meeting and such topics would have been put in the meeting agenda by the organizer before the meeting starts. To the best of the authors knowledge there is no research related to the task of measuring conversation relevance to pre-meeting agenda topics. However, the related topic of meeting summarization, or minuting has been well studied.

Two challenges (AutoMin) in the field of meeting summarization have been held where teams participated in order to progress the field [15; 16]. The first challenge had teams using BART-based models achieving the best performance [17; 18]. With the improvements in generative AI and the growing adoption of Large Language Models (LLMs), a second challenge was done recently. In this challenge, the participants [19; 20; 21] achieved good results with different large models, such as, Llama-based Vicuna , Dolly , and GPT-3's text-davinci-003 . The challenge organizers used GPT-4 for benchmarking as well and it demonstrated good performance for the meeting summarization task. The organizers also used GPT-4 to evaluate submissions along with human evaluation results, but found that it was unreliable for this task. The challenge organizers also called out the need to answer research questions related to transcript summary relevance, to better understand content and coverage from different annotators.

There are multiple datasets for the task of benchmarking the meeting summarization task. Most of such individual datasets often contains only one type of meeting. The AMI dataset  is a collection of meetings transcripts and summaries that cover the topic of product design in an scenario setting and a small amount of non-scenario meetings. The topic annotation is very brief and limited. The ICSI  Corpus contains 75 project meetings and discussions in an academic environment. It has high-level human-annotated topics that are very brief. MeetingBank  is a dataset of 1,250 city council meetings from multiple US counties. Detailed meeting minutes for each meeting subsection are documented in this dataset. The QMSum  dataset aggregates three public data sources (ICSI, AMI, and Parliament meetings from Welsh and Canada) and generates minutes for the text summarization tasks. The paper further shows that for a BART model that training a model on data from one of the datasets and testing it on the other one leads to poor performance. By training on all datasets they were able to build a more robust model. To further expand on data for the automatic minute task Nedoluzhko  put together the ELITR data corpus. This data consists of meetings in both Czech and English, with transcripts and meeting minuting being taken by different annotators. In order to align the transcripts with the minutes the tool ALIGNMEET  was used.

## 3 Topic-Conversation Relevance (TCR) Dataset

We create the TCR Dataset that covers a variety of meeting topics and styles. The dataset consists of both meeting data collected by the author team and existing publicly available datasets.

Overall, the TCR dataset contains 1,506 unique meetings, 22 million words in transcripts and more than 15,000 meeting topics. Table 1 provides high-level statistics of the dataset.

The pre-selected MeetingBank data is large comparing with other data sources. To balance the meeting styles and create representative benchmark results, we also create a subset of 30 randomly selected meetings denoted as MeetingBank_rnd30. The subset is available separately from the complete MeetingBank data in the TCR dataset. A summary of the balanced subsets is presented in Table 2.

We also provide exploratory analysis of per meeting metrics in Table 3. The full exploratory analysis with standard deviations is provided in the Appendix 5. The dataset and related scripts are available in the topic_conversation GitHub repository2.

### Data Schema

All data files in the TCR dataset are in JSON format. An example schema is presented in Figure 1.

Data from different sources are split into separate files. In each file, data is grouped by meeting. For each meeting, we provide meeting level metadata and detailed topic level information. The topics

    &  & Number of & Number of &  &   } \\  & & Meetings & Topics & & (Hours) \\   & SIM & 84 & 84 & 529,012 & 48.6 \\  & SIM\_syn100 & 100 & 348 & 500,825 & 45.7 \\  & ICSI & 75 & 489 & 767,437 & 70.4 \\  & MeetingBank & 1,100 & 6,595 & 19,626,469 & 2,493.8 \\  & NCPC & 20 & 160 & 423,305 & 47.2 \\  & QMSum\_AMI & 96 & 510 & 489,961 & 54.4 \\  & QMSum\_Parliament & 20 & 158 & 276,620 & 30.7 \\  & ELITR & 11 & 94 & 56,521 & 6.3 \\   & **Sub Total** & **1,506** & **8,438** & **22,670,150** & **2,797** \\ 
**Different** & QMSum\_ICSI & 52 & 288 & 527,206 & 48.8 \\
**Annotations** & MeetingBank ReAnnotated & 1,100 & 6,585 & 19,626,469 & 2,493.8 \\   

Table 1: Topic-Conversation Relevance (TCR) Dataset Statistics

    & Number of & Number of &  & Duration \\  & Meetings & Topics & & (Hours) \\  SIM & 84 & 84 & 529,012 & 48.6 \\ SIM\_syn100 & 100 & 348 & 500,825 & 45.7 \\ ICSI & 75 & 489 & 767,437 & 70.4 \\ QMSum\_ICSI & - & 288 & - & - \\ QMSum\_AMI & 96 & 510 & 489,961 & 54.4 \\ QMSum\_Parliament & 20 & 158 & 276,620 & 30.7 \\ MeetingBank\_rnd30 & 30 & 189 & 461,155 & 58.0 \\ MeetingBank\_ReAnnotated\_rnd30 & - & 189 & - & - \\ NCPC & 20 & 160 & 423,305 & 47.2 \\ ELITR & 11 & 94 & 56,521 & 6.3 \\ 
**Total** & **436** & **2,509** & **3,504,836** & **361** \\   

Table 2: Balanced Topic-Conversation Relevance (TCR) Dataset Statisticsare ordered by start time. For each topic, the corresponding transcripts are presented in a list. Each transcript line contains the raw contents in text, speaker ID, time information, line and word counts. If the original data source does not have timestamps, the time information is estimated based on word counts at a fixed 150 words per minute rate for each transcript line. In such cases, the metadata marks "timestamp_source" as "estimated" for the entire meeting.

We provide two scripts (_script_create_synthetic_meetings_SIM.py_, _script_augment_data.py_) in the project repo to create more synthetics meetings or generate augmented version of the existing datasets. Outputs from those scripts follow the same data schema and can be easily combined into the existing TCR dataset.

### New Data Collection: Speech Interruption Meetings (SIM)

In a previous study done by the team regarding speech interruptions and meeting inclusiveness , we collect multi-party online meetings in which participants actively interact with each other to debate a topic. This Speech Interruption Meetings (SIM) dataset is released for the first time as part of the TCR dataset. We also create 100 synthetic meetings on top of these raw meetings. Both the original meetings and synthetics meetings are included in the TCR dataset.

#### 3.2.1 Raw Data

In total, we include 84 raw meetings (48.6 hours) in the TCR dataset. The meetings cover 14 different topics and there are about 530,000 words in the transcripts. In total, 149 unique speakers3 participated in this batch of data collection. Speaker distribution details can be found in Appendix Table 6.

The SIM data captures natural online meeting dynamics. To collect the data, we invite 4 participants to join a remote conference call on Microsoft Teams. Each meeting has a single dedicated topic that can elicit debate. The participants discuss the topic for about 30 minutes. Natural interactions between participants are strongly encouraged. We collect separate audio channels and machine-generated transcripts for each meeting. In the transcripts, the participants are marked as _speaker_1,2,3,4_ randomly within each meeting. We only include transcripts data in the TCR dataset at this stage as audio is not directly related to the the benchmark task.

#### 3.2.2 Synthetic Meetings

Given the raw meetings from the SIM dataset has only one dedicated topic per meeting, we also generate 100 synthetic meetings with multiple topics by randomly combining meetings snippets from different topics together.

The workflow to generate such synthetic meetings involves the following steps. First, we remove the first and last 5 minutes of the transcripts, to eliminate potential meeting setup contents, greetings, and icebreaker talk. These trimmed meetings are the candidate meetings. Second, for each new synthetic

    & **Date** & **Meeting** & **N Speakers** & **N Topics** & **Topic** & **Topic Text** \\  & **Source** & **Duration** & **per** & **per** & **Duration** & **Length** \\  & **(minutes)** & **Meeting** & **Meeting** & **(minutes)** & **(words)** \\  SIM & 34.24 & 4.00 & 1.00 & 34.24 & 11.32 \\ SIM\_syn100 & 27.24 & 4.00 & 3.48 & 7.83 & 10.72 \\ ICSI & 45.19 & 6.20 & 6.52 & 6.93 & 2.85 \\ QMSum\_ICSI & 44.88 & 6.31 & 4.27 & 10.40 & 4.15 \\ QMSum\_AMI & 34.03 & 4.00 & 3.94 & 8.45 & 6.76 \\ QMSum\_Parliament & 92.21 & 23.80 & 6.45 & 13.90 & 8.43 \\ MeetingBank & 109.86 & 8.54 & 5.98 & 18.36 & 59.47 \\ MeetingBank\_ReAnnotated & 109.86 & 8.54 & 5.97 & 18.39 & 10.03 \\ NCPC & 141.69 & 25.60 & 8.00 & 17.71 & 6.27 \\ ELITR & 34.26 & 5.45 & 7.64 & 4.01 & 6.95 \\   

Table 3: Exploratory Analysis of Mean Metrics per Meeting by Data Sourcemeeting, we randomly decide how many unique topics (2 to 5) to include. Then, for each unique topic, we randomly select 5 to 11 minutes consecutive transcripts from candidate meetings with that topic. Based on the setup, the generated 100 synthetic meetings have an average meeting length of 28 minutes. We refer to this set as SIM_syn100.

The scripts that we use to generate the SIM_syn100 data is shared in the project repo. With the configurable parameters, users can create an arbitrary number of synthetic meetings with the desired number of topics and duration splits.

### Public Data Sources

To make the TCR dataset cover a wide range of meeting styles and domains, we integrate another 5 publicly available data sources. In this section, we describe the pre-processing procedures we apply to each of them.

#### 3.3.1 ICSI Corpus

We use all 75 meetings from the the ICSI Corpus . Starting from the word-level transcripts from the original corpus, we exclude the tags for non-verbal events and keep only the transcribed words. This is because for real-time machine-generated transcripts, such events are not marked as tags, but either transcribed as part of the contents, or omitted. For long utterances from the same speaker, we assign a line break in the transcript either when an end-of-sentence tag occurs, or there is a gap that is at least 0.5 second long between two words. We assign timestamps for each sentence based on the original word-level timestamps from the data source.

We make minor adjustments to the topic annotations if there is an identify-mismatch problem between the topics and the speaker IDs. The speaker IDs for each meeting are assigned as speaker_A,

Figure 1: Topic-Conversation Relevance (TCR) Dataset Schema

speaker_B, etc., however, the topic annotations refer to speakers by either their metadata ID (e.g., me001) or their first names. To align the different identify systems, we refer to the metadata and convert the speaker reference style in the topic annotations to align with the transcripts by using speaker IDs. This guarantees the data consistency within annotations.

#### 3.3.2 Selected QMSum Dataset

The QMSum  data has 3 different input data sources and we treat them separately. For QMSum_ICSI data, we use the pre-processed transcripts and timestamps from the original ICSI corpus. We use the QMSum annotations as the new topics. Given the topic styles and the section breaks are very different between QMSum_ICSI and the original ICSI Corpus, we decide to keep both sets of meetings and create benchmark results for them separately. For QMSum_AMI and QMSum_Parliament data, we remove non-verbal tags from the transcripts. As timestamps are not available in QMSum, we create estimated timestamps by the fixed 150 words per minute rate for these two data sources.

The annotations are done as meeting minutes in the QMSum dataset. In cases where the transcripts are not included in the minuting, we fill the empty values by the following logic. If the missing topic is at the very beginning of the meeting, we assign a topic of "Beginning_no_topic"; if the missing topic is at the very end of the meeting, we assign a topic of "Ending_no_topic". If the lack of annotation happens between two topics, we assume the previous topic continues and fill the empty value by taking the previous topic. In the QMSum annotation, it is also possible that one line of transcripts belong to multiple topics. We use the first annotation based on the timestamps. In any given meeting, if more than 15% of the transcripts have missing annotations or overlapping annotations issues, we exclude the meeting due to undesirable annotation quality. Overall, we keep 168 out of the original 232 meetings.

#### 3.3.3 Selected MeetingBank Dataset

We use the timestamps in the metadata from the original data source to exclude meetings that are shorter than 15 minutes. In total, 1,100 out of the 1,250 MeetingBank  meetings are included in our dataset. We remove unicode from both the transcripts and annotations. Though some of the original timestamps do not start from 0, we keep the original timestamps as it is necessary to locate the corresponding audio contents if needed. In the TCR data, it is very easy to align the beginning to 0 by removing the start timestamp documented in the meeting metadata.

In the TCR dataset, we provide two sets of annotations for the MeetingBank data:

Original AnnotationsWe take the "summary" field from the MeetingBank metadata as the topic annotations. These annotations are in meeting minutes styles and often are long and very detailed. If in the original data source one sentence belong to multiple summaries, we keep only the first occurrence.

Re-annotated TopicsThe original meeting summaries contain not only the topic for a section but often the outcomes. To have pre-meeting style topics, we need to remove outcomes that would not have been known before the meeting happens. Additionally some of the meeting minutes are excerpts from the transcripts, so modifying the annotations would give a more accurate representation of the topic-conversation relevance benchmark. In order to rewrite a summary to a pre-meeting agenda type of topic, a GPT-4 prompt is developed. An example of the input and output is shown below:

* Original summary: _A bill for an ordinance changing the zoning classification for 5611 East Iowa Avenue in Virginia Village. Approves an official map amendment to rezone property located at 5611 East Iowa Avenue from S-SU-D to S-RH-2.5 (suburban, single-unit to suburban, rowhouse) in Council District 6. The Committee approved filing this item at its meeting on 7-10-18._
* Re-annotated topic: _Zoning Change for 5611 East Iowa Avenue in Virginia Village._

To guarantee high quality of the re-annotated topics, we randomly selected 100 samples and collected Mean Opinion Score (MOS) scores of 1-5 from 3 project members. A score of 5 means that the re-annotated topic is in a proper pre-meeting agenda style and it captures the key information from the original annotation. A score of 1 means that neither is true. Across the 300 votes, the average MOS is 4.6 and the median is 5.

The full MeetingBank data with re-annotated topics is referred to as MeetingBank_ReAnnotated and the subset with the same 30 meetings is referred to as MeetingBank_ReAnnotated_rnd30.

#### 3.3.4 Selected NCPC Meetings

The National Capital Planning Commission (NCPC)  is a government agency that meets once a month to discuss projects for in the united states capitol region. The meeting agendas and transcripts are publicly available. To the best of our knowledge, the TCR dataset is the first work to add this data source to a structured dataset. We randomly select 20 NCPC meetings where agenda is available. Both meeting transcripts and agenda topics are documented in the same PDF file for each meeting. In order to convert the data to a structured format, the PDFs are converted to text files, and the body of the text is extracted, along with the topic titles. As the PDFs do not share the same structure, additional manual adjustments are applied to guarantee a high conversion accuracy. The original transcripts do not have time information, hence the timestamps are estimated with the fixed rate of 150 words per minute.

#### 3.3.5 Selected ELITR Dataset

The ELITR  data is a corpus of meetings in Czech and English containing transcripts along with minutes written by multiple annotators. As our work focuses on English only at this stage, we keep just the English meetings. Among the English meetings, 49 have meeting minutes that can be aligned with the corresponding transcripts. We further reduce the size of this dataset to address the following challenges: First, with multiple annotations available from up to 11 different annotators per meeting, we need to keep only one annotation per meeting. Second, the meeting minutes can contain too many detailed items that are not suitable to be considered as topics. Third, the annotations do not necessarily point to a consecutive chunk of transcripts, but jump back and forth. To account for these issues, we keep only meetings with an annotation of at most 10 topics, and the annotations are not interspersed. With all the filters, we include 11 meetings into the TCR dataset. If there is no annotation for some parts of the transcripts, we follow the same logic described in Section 3.3.2 to fill the empty values. The original transcripts do not have timestamps, so we estimate the time information with the fixed rate of 150 words per minute.

Figure 2: Metadata Schema for Augmented Meetings

### Data Augmentation

All data sources described above provide ground truth topics for subsections of transcripts. However, the list of topics in the annotation only reflect the topic that are discussed. Real meetings do not always follow the planned agenda. Participants sometimes go off topic and have to skip some pre-arranged topics due to time limits. The TCR dataset schema is designed to test and evaluate such scenarios by incorporating the "variations" section. To reflect such scenarios, we also provide a script to either (1) add topics that are not discussed to a meeting as a planned topic or (2) remove topics and corresponding contents from a meeting. This can help enrich the TCR dataset to include a varied range of meeting styles. Implementation details can be found in the project repository.

For the augmented meetings, we keep the type of variation and the changed topic list in the "variations" field in the metadata for each meeting. Figure 2 shows an example of the change in metadata for an augmented meeting. If a topic is planned, but not discussed in a meeting, the topic is added to the "variation_addTopics" and the corresponding empty contents are also added to the "topic" section. Users can easily extend this by adding topics with non-empty contents to expand the simulation further. If we want to remove a topic and its corresponding contents together from a meeting, the changes are reflected in the "variation_removeTopics" list as well as the "topic" contents. The timestamps of the remaining contents are also changed accordingly. With this structure, we can test the relevance between the transcripts and topics that could have been planned but are not part of the actual conversation.

## 4 Topic-Conversation Relevance Benchmarks

We generate Topic-Conversation Relevance benchmarks on a selected subset4 of the TCR dataset. Given the significant difference in meeting styles and structures, the benchmarks are reported for each data source separately.

### Methodology

We use GPT-4 to create the benchmark results. For each meeting, we cut the transcripts into snippets with equal length based on timestamps. We conduct the experiments in duration length of 5 minutes, 10 minutes and 15 minutes. Then the prompt takes the snippet of transcripts and the full topic list from the meeting as inputs, and asks for an evaluation of the transcript's relevance to each topic in the list. The relevance score is represented by 4 levels: 0 means Not Relevant, 1 means Somewhat Relevant, 2 means Mostly Relevant, and 3 means Very Relevant. The detailed definitions of the relevance levels are given as a multiple-choice question in the prompt. In the development stage, we try different output requests, such as floats, integers, binaries and multiple choices. We present the final benchmark results all in the multiple choices style as it has been giving the most robust results across all data sources.

In the evaluation stage, we treat the Topic-Conversation Relevance problem as a binary classification. If based on the ground truth label, a topic is discussed for more than 30 seconds in the transcripts, then we mark it as "Discussed", otherwise "Not Discussed". For the GPT-4 responses, we treat "0 Not Relevant" as "Not Discussed", and everything else as "Discussed". In the results presented in Section4.2, we specifically focus on scenarios where the discussion is off-topic, so the "Not Discussed" topics are treated as positive cases. We use Precision ("LLM detects a topic is not being discussed and it is true") and Recall ("A topic is not being discussed and LLM detects it") as the main metrics. The full results treating "Discussed" and "Not Discussed" as positive cases respectively are shown in the Appendix A.3.

### Results

The benchmark results focusing on the "Not Discussed" category are shown in Table 4. We split the results by data source and transcripts length.

For the highly structured meetings (MeetingBank, NCPC), the benchmark results show very high precision and recall. Most of these meetings follow the pre-defined agenda topics strictly and often state the topic to-be-discussed at the beginning of the section. Different annotations do not impact the results much. The other less structured meetings, such as project meetings (ICSI, ELITR) and brainstorming discussions (SIM), are more challenging. Most of these meetings do not have clear statements separating different topics and related sub-topics are often discussed back and forth. Different topic annotations can impact the results significantly.

We also notice that if there are multiple topics included in the same snippet of transcripts (8), it is even more challenging to correctly predict the relevance comparing with single-topic transcript (9). This could be due to the fact that transitions between topics are not always clear in the less structured meetings. Results split by topic counts are also included in Appendix A.3.

## 5 Future Work

The dataset can be further improved by including more types of meetings in different domains. However, it is particularly hard to obtain real day-to-day meetings in a working environment as most of such meetings consist sensitive business information. Hence the project team is working on inviting domain experts (e.g., legal, healthcare, finance, etc.) to create meeting agendas for different types of meetings in their industry, and conducting domain-specific meetings based on the agendas. We are currently in the data collection stage using the same method as the SIM dataset, with additional requirements on participants' professional experience.

In addition to English, we are also working on integrating other languages into the dataset. One of the efforts is to translate the current data sources into other languages with reliable translation services and test the performance on the same tasks.

A challenge of evaluating topic-conversation relevance is the blurred boundaries between topics. At a meeting structure level, a certain chunk of transcripts can be marked as belonging to a topic, but it is very likely that some parts of the conversation are actually not directly related to the topic or

  
**Data** & **Transcripts** & **N** & **Prompt*Topic** & **F1** & **Precision** & **Recall** \\
**Source** & **Length** & **Prompts** & **Pairs** & & & \\   & 5 min & 509 & 2,031 & 0.9587 & 0.9620 & 0.9554 \\  & 10 min & 231 & 930 & 0.9272 & 0.8994 & 0.9568 \\  & 15 min & 137 & 562 & 0.9175 & 0.8669 & 0.9744 \\   & 5 min & 790 & 5,175 & 0.8663 & 0.9615 & 0.7882 \\  & 10 min & 382 & 2,502 & 0.8582 & 0.9462 & 0.7851 \\  & 15 min & 244 & 1,594 & 0.8488 & 0.9243 & 0.7847 \\   & 5 min & 550 & 3,079 & 0.7506 & 0.9373 & 0.6259 \\  & 10 min & 266 & 1,492 & 0.7242 & 0.9441 & 0.5874 \\  & 15 min & 170 & 955 & 0.7222 & 0.9381 & 0.5871 \\   & 5 min & 594 & 4,236 & 0.9804 & 0.9891 & 0.9720 \\  & 10 min & 301 & 2,168 & 0.9767 & 0.9843 & 0.9691 \\  & 15 min & 204 & 1,479 & 0.9688 & 0.9671 & 0.9705 \\   & 5 min & 594 & 4,236 & 0.9817 & 0.9913 & 0.9723 \\  & 10 min & 301 & 2,168 & 0.9810 & 0.9895 & 0.9726 \\   & 15 min & 204 & 1,479 & 0.9755 & 0.9824 & 0.9687 \\   & 5 min & 562 & 4,585 & 0.9702 & 0.9855 & 0.9553 \\   & 10 min & 277 & 2,261 & 0.9631 & 0.9800 & 0.9468 \\   & 15 min & 181 & 1,478 & 0.9614 & 0.9664 & 0.9565 \\   & 5 min & 70 & 584 & 0.8429 & 0.9390 & 0.7646 \\   & 10 min & 31 & 261 & 0.8182 & 0.9184 & 0.7377 \\   & 15 min & 20 & 166 & 0.8043 & 0.8706 & 0.7475 \\   

Table 4: Topic-Conversation Relevance Benchmark Resultseven belong to another listed topic. It would be desirable to create sub-labels at sentence or group of sentences level to capture relevance scores at a lower granularity.

Additionally, we believe it would be beneficial to include audio data in the TCR dataset along with transcripts. We will work on aggregating audio data for multiple data sources (SIM data and other public data) into the dataset.

We acknowledge Naglakshmi Chande, Quchen Fu, Xavier Gitiaux and Thierry Tremblay from the Microsoft Teams team for all the brainstorming, analysis reviews and infrastructure work. We acknowledge the data donation workers for the SIM dataset who were compensated for their time and effort. No competing interests are associated with this work.