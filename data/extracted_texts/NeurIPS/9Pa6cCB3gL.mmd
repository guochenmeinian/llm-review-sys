# UMB: Understanding Model Behavior for

Open-World Object Detection

 Xing Xi &Yangyang Huang &Zhijie Zhong &Ronghua Luo

School of Computer Science and Engineering

South China University of Technology

GuangZhou, China 510006

Corresponding author: rhluo@scut.edu.cn.

###### Abstract

Open-World Object Detection (OWOD) is a challenging task that requires the detector to identify unlabeled objects and continuously demands the detector to learn new knowledge based on existing ones. Existing methods primarily focus on recalling unknown objects, neglecting to explore the reasons behind them. This paper aims to understand the model's behavior in predicting the unknown category. First, we model the text attribute and the positive sample probability, obtaining their empirical probability, which can be seen as the detector's estimation of the likelihood of the target with certain known attributes being predicted as the foreground. Then, we jointly decide whether the current object should be categorized in the unknown category based on the empirical, the in-distribution, and the out-of-distribution probability. Finally, based on the decision-making process, we can infer the similarity of an unknown object to known classes and identify the attribute with the most significant impact on the decision-making process. This additional information can help us understand the behavior of the model's prediction in the unknown class. The evaluation results on the Real-World Object Detection (RWD) benchmark, which consists of five real-world application datasets, show that we surpassed the previous state-of-the-art (SOTA) with an absolute gain of 5.3 mAP for unknown classes, reaching 20.5 mAP. Our code is available at [https://github.com/xxyzll/UMB](https://github.com/xxyzll/UMB).

## 1 Introduction

As a fundamental task in computer vision, object detection has always been the focus of extensive attention. Traditional object detection methods are trained on closed datasets, assuming all detected objects have already been annotated in the training set. However, the real-world environment's complexity means it is impossible to annotate all objects. As a result, the application of traditional detection methods is limited. Open World Object Detection (OWOD) has been introduced to address the issue. OWOD can be divided into two subtasks: mining potential objects and incremental learning. The former requires the model to detect categories in the test set that have not been annotated in the training set. These newly discovered objects are then handed over to annotators, who select the categories of interest. Subsequently, the model is required to fine-tune its existing knowledge to detect these newly added categories (incremental learning).

Existing works primarily focus on generating pseudo-labels for potential objects in the training set, treating these pseudo-labels as annotations for unknown categories. For instance, ORE labels samples with high objectness predicted as background as potential objects. CAT and RE-OWOD utilize selective search to provide annotations for unknown categories. OW-DETR proposes an attention-driven pseudo-label strategy to mine potential positive samples. However, despite theseheuristic methods being able to recall potential objects, they share a common flaw. As shown in Figure 1 (left), existing methods can only detect unknown objects and then provide these labels to annotators. However, the reason the model would predict these objects remains unknown to the annotators. Therefore, this paper attempts to understand the model's prediction of potential objects, establish connections between unknown and known categories, and then provide this additional information to annotators.

To achieve this, we propose a novel model (UMB) that uses textual attributes to mine potential unknown objects. Specifically, we first define targets that share similar attributes with known categories but are predicted as background as potential objects. Then, to find these potential objects, we build a distribution that associates attribute similarity with the probability of positive samples, which can be seen as the empirical probability of an object possessing a particular attribute being classified as a positive sample. If a sample predicted as background has a high empirical probability and attribute similarity, we regard it as an unknown object. Finally, based on the decision-making process, we infer the most similar known classes with the unknown object and calculate the most significant impact attribute. As shown in Figure 1 (right), our method can identify the unknown and provide information about their connections with known categories and the influence of attributes on decision-making.

We evaluated our method on the Real-World Object Detection (RWD) benchmark composed of datasets from five practical applications, and our method achieved significant improvements. We improved almost all datasets, surpassing the OVC (GT) that uses real class names. Significantly, in the Surgery dataset, we achieved the **213%** performance in unknown category. The main contributions of this paper are as follows:

* To the best of our knowledge, we are the first to notice the limitations of models on unknown predictions and attempt to understand the predictive behaviour of models.
* To achieve this, we propose a new model framework (UMB) that can detect unknown categories and understand model behavior utilizing the textual description of known categories.
* We model the textual attributes and the probability of positive samples to obtain the empirical probability. By combining the empirical probability, the in-distribution probability, and the out-of-distribution probability, we are able to discover unknown categories.
* The evaluation results on the RWD benchmark show that our method achieved significant performance improvements, establishing a new state-of-the-art (SOTA) with 5.3 mAP advantages in both known and unknown category performance.

## 2 Related Works

### Open Vocabulary Object Detection

Open Vocabulary Object Detection (OVD), as a subset of open-world perception, was initially introduced by OVR-CNN. OVD employs the text encoder to transform classes needing detection

Figure 1: An illustration of our UMB and other methods. Previous OWOD methods only detected unknown objects (left), while our method further understands the model’s behaviour (right).

[MISSING_PAGE_FAIL:3]

categories to linearly combine similarities. Therefore, given a visual embedding \(e_{vis_{i}}\), the probability of its corresponding known category \(j\) is:

\[p(C_{j}|e_{vis_{i}}) =Sigmoid(w_{j,1} sim(e_{vis_{i}},e_{att_{1}}) +...+w_{j,n} sim(e_{vis_{i}},e_{att_{n}})) \] \[=Sigmoid(_{k=1}^{n}w_{j,k} sim(e_{vis_{i}},e_{att_{k}})),\]

where \(Sigmoid\) is the Sigmoid activation function, and \(sim\) denotes the Cosine Similarity. The pseudocode for known class prediction and attribute generation can be found in Algorithm 1.

### Text Attribute Modeling (TAM)

#### 3.2.1 Attribute Modeling

We model the attribute similarities in the training set with category confidence as the positive sample probability to build an empirical probability distribution. However, as shown in eqn. 3, the score is the linear combination of all attribute similarities, so it is influenced by all attributes simultaneously. Thus, we weigh confidence with linear combination weights (\(W^{m n}\)) to balance the contributions of different attributes. Specifically, given visual embedding \(e_{vis_{k}}\), positive sample probability of attribute \(i\) for category \(j\) can be represented as:

\[(e_{att_{i}},C_{j}|e_{vis_{k}})=w_{j,i}^{1-} p (C_{j}|e_{vis_{k}})^{},\;\;w_{j,i}=W[j,i] \]

where \(\) is a hyperparameter used to balance the contributions of weights and scores. For simplicity, we use the geometric weighted average. We incorporate all similarities in the training set and their corresponding probabilities of positive samples, establishing a mapping \(f_{i,j}:sim(e_{att_{i}},e_{vis_{k}})(e_{att_{i}},C_{j} |e_{vis_{k}})\). However, during training, the model cannot utilize the annotations of any unknown classes. Therefore, we define the positive sample probability of attribute \(i\) for the unknown class as the maximum of its probability to all known classes. Specifically, the probability of attribute \(i\) for the unknown class \(C_{u}\) can be represented as:

\[(e_{att_{i}},C_{u}|e_{vis_{k}}) =max((e_{att_{i}},C_{1}|e_{vis_{k}}),...,(e_{att_{i}},C_{m}|e_{vis_{k}})) \] \[=argmax_{j[1,m]}((e_{att_{i}},C_{j} |e_{vis_{k}}))\]

Figure 2: Overall structure of our UMB. It begins by populating prompt template with known class names and employing large language model (LLM) to generate attributes (Sec. 3.1). These attributes are then filled into template and encoded by text encoder to generate attribute embeddings (\(E_{att}\)). We model the attributes and their corresponding positive sample probabilities to build empirical probability (Sec. 3.2). We utilize the empirical, in-distribution and out-of-distribution probability to ascertain whether an object pertains to an unknown category (Sec. 3.3).

#### 3.2.2 Distribution Optimization and Fitting

To establish a continuous probability distribution, we need to optimize and fit the original distribution. First, contrary to the OWOD benchmark that heavily relies on extensive COCO and VOC data, RWD pays more attention to real-world application and is specifically designed for the few-shot setting. This results in the model not having sufficient samples to establish the probability distribution. Consequently, there are some undefined points in the mapping function \(f_{i,j}\). To mitigate this, we employ the linear interpolation to estimate the values of these missing points \(\):

\[f_{i,j}()=k(-_{l})+f_{i,j}(_{l}),\;\;k=(f_{i,j}(_{r})-f_{i,j}(_{l}))/( _{r}-_{l}), \]

where, \(_{l}\) and \(_{r}\) respectively represent the points to the left and right of \(\) that are closest in the mapping \(f_{i,j}\).

Then, we utilize the sliding window to filter the noise present in the distribution. With a predetermined window size, we calculate the maximum positive sample probability across the entire window to substitute the current value:

\[f_{i,j}(sim(e_{vis_{k}},e_{att_{i}}))=-1]}{argmax}f_{i,j}(sim(e_{vis_{k}},e_{att_{i}})+a), \]

where \(W_{sz}\) denotes the window size. As depicted in Figure 3, the employment of linear interpolation and the sliding window ensures the original shape of the probability distribution remains intact, concurrently minimizing the noise inherent in the distribution.

Finally, as shown in Figure 3 (middle), the optimized probability distribution \(f_{i,u}\) demonstrates the multi-peak characteristic. Consequently, we postulate that the original distribution is composed of multiple basic probability distributions (e.g., Gaussian Distribution). As a result, we employ the mixture probability distribution to fit the initial distribution. Specifically, we construct the model using the linear combination of multiple Gaussian distributions:

\[f_{i,u}(sim(e_{vis_{k}},e_{att_{i}}))= _{a=1}^{A}Gm(sim(e_{vis_{k}},e_{att_{i}})|w_{a},_{a}, _{a}), \] \[Gm(x|w,,)= w}e^{}{2 ^{2}}},\]

where \(A\) is the number of the Gaussian distribution. Additionally, we observed that certain attributes demonstrate the skewed distribution, suggesting that fitting with the Gaussian model may not be the optimal choice. Consequently, we utilize the asymmetric Weibull distribution as a substitute for the Gaussian distribution:

\[Wb(x|w,,k)=w()^{(k-1) }e^{-()^{k}}. \]

In order to ascertain the parameters of these distributions, we designate them as learnable parameters and employ Mean Squared Error (MSE) as the loss function for optimization. The pseudocode for text attribute modeling can be found in Algorithm 2.

Figure 3: An illustration of the Probability Mixture Model. To establish a continuous probability distribution, we use linear interpolation on the original distribution (left) to estimate missing points and employ the sliding window to eliminate noise within the distribution (middle). Finally, we use the probabilistic mixture model to fit the optimized distribution (right).

### Unknown Inference

Following FOMO, we calculate the weighted mean of all attribute embeddings as the embedding for the unknown class:

\[e_{att_{u}}=_{j=1}^{m}(_{i=1}^{n}e_{att_{i}} w_{j,i })^{d}. \]

Following this, we utilize the pre-trained \(scale\) layer (\(^{d}^{1}\)) and \(shift\) layer (\(^{d}^{1}\)) for the purpose of scaling the similarity:

\[T(sim(e_{vis_{k}},e_{att_{u}}))=(sim(e_{vis_{k}},e_{att_{u}})+shift(e_{vis_{k} })) scale(e_{vis_{k}}). \]

Finally, we adjust the similarity of the average embedding. This adjustment is segmented into three components: empirical probability, in-distribution probability, and out-of-distribution probability.

**Empirical Probability (Empirical Prob)**. For known categories, each attribute contributes unevenly to the category score (eqn. 6). Hence, for the unknown class, merely using the summation of the empirical probability to ascertain category confidence is suboptimal. We utilize the maximum weight from the known class to balance the contributions from various attributes of the unknown class. Specifically, for the visual embedding \(e_{vis_{k}}\), its corresponding empirical probability is:

\[_{u}(e_{vis_{k}})=_{i=1}^{n}f_{i,u}(sim(e_{vis_{k}},e_{att_{i}})) _{i},\;_{i}=\;w_{j,i}. \]

Herein, \(f_{i,u}\) denotes the positive sample probability of attribute \(i\) towards the unknown class, as established earlier.

**In-Distribution Probability (ID Prob)**. We aspire for the model to observe the known attribute of the current object. Consequently, we incorporate the weighted sum of the scaled attribute similarities:

\[f_{}(e_{vis_{k}})=_{i=1}^{n}Sigmoid(T(e_{vis_{k}},e_{att_{i}})) _{i}. \]

**Out-of-Distribution Probability (OOD Prob)**. Both empirical probability and in-distribution probability are based on the model's prediction on known classes. Therefore, inevitably, the model predicts high empirical probabilities and in-distribution probabilities for known categories. To counteract this, we employ out-of-distribution probability to offset their influence:

\[f_{OOD}(e_{vis_{k}})=(1-(T(sim(e_{vis_{k}},e_{att_{i}})) w_{j,i})). \]

Ultimately, given the visual embedding \(e_{vis_{k}}\), the corresponding confidence for the unknown class is denoted as:

\[p(C_{u}|e_{vis_{k}})=Sigmoid((_{u}(e_{vis_{k} })(1-)}_{Empirical\;Prob}+}(e_{vis_{k}}) }_{ID\;Prob})(e_{vis_{k}})}_{OOD\;Prob}) \] \[ Sigmoid(},e_{att_{u}}))}_{Average \;Similarity}),\]

where \(\) is used to balance the contribution form in-distribution and empirical probability.

### Additional Information

**Similarity between known and unknown classes**. Predictions for both unknown and known classes are determined by attribute similarity. Hence, we can compute its similarity with known classes based on the visual embedding of objects classified as unknown. Similar to unknown inference, we take into account the empirical probability of the current object and its confidence in being predicted as a known class. Specifically, for the visual embedding \(e_{vis_{k}}\) of objects predicted as unknown, the corresponding similarity to known classes is:

\[S_{u}(e_{vis_{k}})=(_{i=1}^{n}f_{i,j}( sim(e_{vis_{k}},e_{att_{i}}))+p(C_{j}|e_{vis_{k}})) \]

**Maximal attribute contribution**. Attributes are used to compute the similarity with visual embeddings, and then the model makes predictions based on this similarity. Therefore, the contribution of each similarity can be calculated to determine the impact of a particular attribute in the decision-making process. For a visual embedding \(e_{vis_{k}}\) that is predicted as an unknown class, the influence of attribute \(i\) on the current decision is denoted as:

\[Ctr(e_{att_{i}})=_{i}(Sigmoid(T(e_{vis_{k}},e_{att_{i}})) +f_{i,u}(sim(e_{vis_{k}},e_{att_{i}}))(1-)) \]

## 4 Experiments

### More Details and Experiments

In our supplemental material, we provide detailed information about our experiments, including: comprehensive descriptions of the datasets (sec A.2), definition of OWOD (sec A.1), evaluation metrics (sec A.3), details (sec A.4), more extensive ablation studies (sec A.5), analysis and visualization of PMM training (sec A.6), similarity evaluation(sec A.7), attribute study (sec A.8), discussion of the limitations (sec A.10) and broad impact(sec A.9).

### Datasets

The OWOD benchmark is established on the VOC and COCO datasets. In the era of foundation models, the zero-shot capability of detectors on such datasets has reached its limit, for instance, OWL-ViT unknown recall is 79.0. Therefore, following FOMO, we have shifted the benchmark for evaluating detector performance to the more practically applicable RWD benchmark.

### Comparison with Other State-of-the-art Models

Table 1 presents the comparison of our UMB method and previous SOTA methods established on the RWD benchmark. Overall, our method achieved comprehensive leadership, surpassing previous methods with the unknown performance advantage of **4.4** mAP (Wb-B) and **5.3** mAP (Gm-L), demonstrating the effectiveness of our method. In addition, although Base+GT uses the name of unknown categories, it performs poorly in the Aerial, Game, Medical, and Surgery datasets. Our method does not rely on unknown class names and significantly outperforms Base+GT (e.g., Surgery: 2.4 (Base+GT-L) vs 25.6 (UMB-Wb-L)). When compared with Base-FS, which received the same

    &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  &  &  \\  & U & K & PK & CK & U & K & PK & CK & U & K & PK & CK & U & K & PK & CK & U & K & PK & CK & U & K & PK & CK \\  Base+GTB & 29.8 & 45.0 & 45.0 & 36.7 & 1.3 & 5.7 & 5.7 & 1.4 & 15.0 & 0.4 & 0.4 & 0.1 & 0.5 & 0.0 & 0.0 & 0.1 & 5.6 & 1.5 & 1.4 & 0.3 & 10.4 & 10.5 & 10.5 & 7.7 \\ Base-FS-B & 7.1 & 41.1 & 41.1 & 31.9 & 1.2 & 10.4 & 10.1 & 4.0 & 16.0 & 4.6 & 4.8 & 3.9 & 0.6 & 6.1 & 6.1 & 3.3 & 1.3 & 11.9 & 11.3 & 10.9 & 5.2 & 14.8 & 14.7 & 10.8 \\ FOMO-B & 3.5 & 43.8 & 44.1 & 40.8 & 0.9 & 12.0 & 12.6 & 5.4 & 13.3 & 3.8 & 4.4 & 4.1 & 2.1 & 6.4 & 5.5 & 11.5 & 6.1 & 12.7 & 12.9 & 11.0 & 5.2 & 15.7 & 15.9 & 14.6 \\  Base+GTL & 34.8 & 36.0 & 36.0 & 42.3 & 1.0 & 7.9 & 7.2 & 0.8 & 12.4 & 0.9 & 0.8 & 0.3 & 2.4 & 0.2 & 0.2 & 0.3 & 2.4 & 0.2 & 2.6 & 1.3 & 10.6 & 9.0 & 9.4 & 9.0 \\ Base-FS-L & 2.4 & 43.6 & 42.9 & 42.8 & 9.2 & 23.7 & 21.9 & 13.0 & 8.2 & 10.4 & 10.2 & 13.4 & 1.1 & 23.2 & 21.7 & 24.2 & 3.6 & 26.0 & 25.0 & 7.4 & 5.0 & 25.4 & 24.3 & 20.2 \\ FOMO-L & 1.82 & 50.1 & 48.1 & 47.1 & 6.0 & 25.3 & 23.7 & 16.0 & 30.4 & 10.7 & 9.9 & 11.2 & 9.4 & 21.8 & 19.9 & 34.6 & 12.0 & 29.0 & 28.9 & 8.5 & 15.2 & 27.4 & 26.1 & 23.5 \\   \\  UMB-Gm-B & 13.3 & 43.8 & 43.0 & 39.7 & 1.5 & 18.8 & 19.0 & 5.9 & 15.2 & 4.1 & 4.7 & 4.3 & **2.3** & 5.4 & 3.5 & 11.8 & 10.1 & 13.9 & 14.3 & 11.1 & 8.5 & 17.2 & 16.9 & 14.6 \\ UMB-Wb-B & **13.5** & 43.8 & 43.0 & 39.7 & 1.4 & 18.8 & 19.0 & 5.9 & **16.3** & 4.1 & 4.7 & 4.3 & **2.3** & 5.4 & 3.5 & 11.8 & **14.5** & 13.9 & 14.3 & 11.1 & **9.6** & 17.2 & 16.9 & 14.6 \\ UMB-Gm-L & **18.6** & 50.7 & 50.5 & 50.4 & **11.2** & 42.7 & 40.4 & 22.6 & **38.1** & 11.1 & 10.7 & 10.5 & **13.2** & 22.2 & 19.1 & 34.5 & 24.5 & 36.6 & 39.0 & 17.4 & **20.5** & 32.7 & 31.9 & 27.1 \\ UMB-Wb-L & **18.6** & 50.8 & 50.5 & 50.4 & 11.1 & 42.8 & 40.4 & 22.5 & 32.7 & 11.1 & 10.7 & 10.5 & 8.6 & 22.3 & 17.3 & 33.2 & **25.6** & 36.6 & 39.0 & 17.4 & 19.3 & 32.7 & 31.6 & 26.8 \\   

Table 1: Comparison with previous SOTA methods on the RWD benchmark. Base+GT represents the standard OVC setting using all class names including unknown label. Base-FS indicates the baseline of fine-tuning the benchmark model with the same supervision received. B and L respectively represent two different sizes of the OWL-ViT model, B/14 and L/14. U, K, PK, and CK respectively represent unknown categories, known categories, previously known categories, and currently introduced categories. Overall indicates the average performance of the model on 5 datasets. Wb and Gm respectively represent use of Weibull and Gaussian distribution during the fitting stage.

supervision, FOMO did not achieve comprehensive leadership and even lagged behind by 3.6 mAP in Aquatic. Our method leads whether compared with Base-FS or FOMO, and in the Surgery dataset, we **doubled** the performance of FOMO (12.0 (FOMO-L) vs 25.6 (UMB-Wb-L)). Gm and Wb exhibit different strengths in various OWL-ViT models. UMB-Wb shows an advantage on the B/16 model (+1.1 mAP), while the trend is reversed on the L/14 base (-1.2 mAP). Therefore, we provide two different types of probability distributions (Gm and Wb) as interchangeable options.

### Ablation Study

Table 2 provides the incremental results of our UMB. The initial performance uses the average category embedding (eqn. 10) and out-of-distribution probability (eqn. 14). When the in-distribution probability is introduced, which is used to capture the known attributes of the current target, the performance improves by 5.8 mAP. However, in the Aerial dataset, U\({}_{AP}\) only reaches 3.8 mAP, reducing by 1 mAP, replaced by a significant increase in recall rate (+44.1), which means that the detector erroneously treats many background samples as unknown objects. Such a result also proves the limitations of the unknown recall as the detection metric previously used in the OWOD benchmark. Finally, by adding the empirical distribution, our UMB achieves a comprehensive lead (Gm +5.0, Wb +3.8). In addition, the effect of balance parameter \(\) on the recall rate is not obvious. In fact, in the Aquatic and Surgery datasets, the variance of the recall rate distribution reaches 0, which means that alpha correctly suppresses the background samples erroneously predicted by the detector. Overall, our UMB can provide a higher unknown recall rate (UMB-Gm 73.4, UMB-Wb 73.8) while ensuring detection accuracy.

### Visualization

In figure 4, we provide the qualitative analysis of FOMO and our UMB. The visual analysis is divided into three parts: the recall ability of the unknown category, recall precision, and analysis of additional information. UMB shows superior performance in recalling unknown objects. UMB successfully recalled tools in the Surgery dataset (fifth row, Wb ID 3) and accurately recalled playgrounds and roofs in the Aerial dataset (second row, Wb ID 1, 2), while FOMO failed to recall these objects. Regarding recall precision, FOMO predicts multiple results to an object and incorrectly classifies unknown classified objects as known classes, such as the hero characters in the Game dataset (third row) and the misclassification of four objects in the Aquatic dataset (first row). In contrast, our UMB shows higher precision. Regarding additional information, the misclassification of FOMO in the Aquatic dataset reflects the defects of OWL-ViT in classifying these objects. With the help of formula n, UMB can infer the category most similar to the current object, and these categories correspond to the categories misclassified by FOMO. In addition, for the same object (ID 3,4), UMB identifies that the attribute with the greatest impact on the entire decision is consistent. These results prove the accuracy of our method in inferring the connection between unknown and known and discovering the attributes that have the greatest impact on decision-making.

## 5 Conclusion

This paper attempts to understand the detector's behaviour in predicting unknown objects. To achieve this, we propose a novel detection framework, UMB, which employs class-agnostic textual attributes to unearth potential objects in the background. Given that the model's detection process for known

    &  &  &  &  &  &  \\   & U\({}_{AP}\) & U\({}_{RE}\) & AvgSol & U\({}_{AP}\) & U\({}_{RE}\) & AvgSol & U\({}_{AP}\) & U\({}_{RE}\) & AvgSol & U\({}_{AP}\) & U\({}_{RE}\) & AvgSol & U\({}_{AP}\) & U\({}_{RE}\) & AvgSol \\  Mean Embedding+OOD & 4.2 & 76.8 & - & 4.8 & 16.9 & - & 22.4 & 80.6 & - & 0.3 & 2.8 & - & 17.0 & 95.5 & - & 9.7 & 54.5 & - \\ +ID Probability & 17.7 & 93.3 & - & 3.8 & 61.0 & - & 32.2 & 90.9 & - & 7.4 & 47.4 & - & 16.3 & 96.3 & - & 15.5 & 77.8 & - \\ +Gaussian (UMB-Gm) & 18.6 & 93.3 & 93.0 & 11.2 & 40.2 & 55.6/6.5 & 35.1 & 90.7 & 98.0/2 & 13.2 & 47.4 & 42.8/9.2 & 24.5 & 96.3 & 96.3/0 & 20.5 & 73.4 & 75.6/3.2 \\  /Weibull (UMB-Wb) & 18.6 & 93.3 & 93.0(1) & 11.1 & 41.3 & 55.9/6.1 & 32.7 & 90.7 & 90.8/0.2 & 8.6 & 47.4 & 42.2/10.2 & 25.6 & 96.3 & 96.3/0 & 19.3 & 73.8 & 75.7/3.3 \\   

Table 2: Ablation study of UMB on RWD. We provide incremental results of model performance. U\({}_{AP}\) and U\({}_{RE}\) represent the model’s mean Average Precision (mAP) and corresponding recall rate on unknown category. Avg and Std denote the mean and variance of the recall rate distribution for unknown classes under different \(\) settings (eqn. 15). In A.5, we provide more analysis.

and unknown classes hinges on textual attributes, our UMB can use the textual attributes of unknown objects to infer the most similar known category. In addition, we can calculate the attributes that have the most significant impact on the entire decision-making process. This supplementary information aids annotators in understanding the model's behaviour in predicting unknowns. We hope that UMB can promote the application of Open-World Object Detection in real-world scenarios.

Figure 4: Qualitative Analysis. Each row, from left to right, represents: FOMO, UMB-Wb, and UMB-Gm, respectively. From top to bottom, the results are given for Aquatic, Aerial, Game, Medical, and Surgery. For fairness and clarity, we only display the TOP-K unknown predictions with a confidence level greater than 0.5. Unknown predictions are marked in Red, while known classes are marked in yellow. Each table provides the most similar known category (Category) for each unknown prediction, and the attribute (Attribute Text) that has the greatest impact on the decision-making process. In section 7, we provide an evaluation of the accuracy rate of similarity prediction.