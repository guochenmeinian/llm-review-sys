# A False Sense of Privacy:

Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage

Rui Xin\({}^{1}\) Niloofar Miresghballah\({}^{1}\) Stella Li\({}^{1}\) Michael Duan\({}^{1}\) Hyunwoo Kim\({}^{2}\)

Yejin Choi\({}^{1}\) Yulia Tsvetkov\({}^{1}\) Sewoong Oh\({}^{1}\) Pang Wei Koh\({}^{1}\)

\({}^{1}\)University of Washington \({}^{2}\)Allen Institute for Artificial Intelligence

rx31@cs.washington.edu niloofar@cs.washington.edu

Equal Contribution

###### Abstract

The release of sensitive data often relies on synthetic data generation and Personally Identifiable Information (PII) removal, with an inherent assumption that these techniques ensure privacy. However, the effectiveness of sanitization methods for text datasets has not been thoroughly evaluated. To address this critical gap, we propose the first privacy evaluation framework for the release of sanitized textual datasets. In our framework, a sparse retriever initially links sanitized records with target individuals based on known auxiliary information. Subsequently, semantic matching quantifies the extent of additional information that can be inferred about these individuals from the matched records. We apply our framework to two datasets: MedQA, containing medical records, and WildChat, comprising individual conversations with ChatGPT. Our results demonstrate that seemingly innocuous auxiliary information, such as specific speech patterns, can be used to deduce personal attributes like age or substance use history from the synthesized dataset. We show that private information can persist in sanitized records at a semantic level, even in synthetic data. Our findings highlight that _current data sanitization methods create a false sense of privacy_ by making only surface-level textual manipulations. This underscores the urgent need for more robust protection methods that address semantic-level information leakage.

## 1 Introduction

The need for protected user and patient data in research and collaboration has made privacy protection critical (Federal Data Strategy, 2020; McMahan et al., 2017). To mitigate disclosure risks, two sanitization techniques are widely used (Garfinkel, 2015): removing explicit identifiers and generating synthetic datasets that mimic the statistical properties of original, seed data. This latter approach has gained significant traction, especially in medical domains (Giuffre & Shung, 2023), where it has been hailed as a silver-bullet solution for privacy-preserving data publishing, as the generated information is considered not to contain real units from the original data (Stadler et al., 2022; Rankin et al., 2020). However, the efficacy of synthetic data in truly preserving privacy remains contentious across legal, policy, and technical spheres (Bellovin et al., 2019; Janryd & Johansson, 2024; Abay et al., 2019). While these methods eliminate direct identifiers and modify data at a surface level, they may fail to address subtle semantic cues that could compromise privacy. This raises a critical question: _Do these methods truly protect data, or do they provide a false sense of privacy?_

Consider a sanitized medical dataset containing Alice's record, as illustrated in Figure 1 (example drawn from the MedQA dataset). Conventional sanitization methods often rely on lexical matching and removal of direct identifiers like names, deeming data safe when no matches are found (Pilan et al., 2022). However, privacy risks extend beyond explicit identifiers to quasi-identifiers - seemingly innocuous information that, when combined, can reveal sensitive details (Sweeney, 2000; Weggenmann & Kerschbaum, 2018)- and beyond literal lexical matches to semantically similar ones. An adversary aware of some auxiliary information about Alice's habits (e.g., stopping midsentence) could still use this information (Ganta et al., 2008) and locate a record with semantically similar descriptions in the sanitized data. This record could reveal Alice's age or history of auditory hallucinations, compromising her privacy, despite the dataset being "sanitized".

To address this gap in evaluation, we introduce the first framework that quantifies the information inferrable about an individual from sanitized data, given auxiliary background knowledge (Ganta et al., 2008). Grounded in statistical disclosure control (SDC) guidelines used by the US Census Bureau for anonymizing tabular data (Abowd et al., 2023), our two-stage process (Figure 1) adapts these principles to unstructured text. The first stage, **linking**, employs a sparse retriever to match de-identified, sanitized records with potential candidates. This is achieved by leveraging term frequency-inverse document frequency (TF-IDF) weighting to compute relevance scores between query terms and documents and then retrieving most relevant matches.

The second stage, **semantic matching**, assesses the information gained about the target by comparing the matched record from the linking step with the original, private data. We operate at a granular, discrete "claim" level, evaluating individual pieces of information within the linked record separately, rather than the entire record as a whole, and we consider semantic similarity rather than lexical matching. This allows for a more nuanced assessment of privacy risks. For example, consider Alice's case again (Figure 1). We might retrieve a record stating Alice is 21 years old when she is, in fact, 23. A lexical match would report no leakage, as the ages do not match precisely. Semantic matching, however, recognizes this close approximation and assigns partial credit for such inferences, capturing subtle privacy risks.

We evaluate various state-of-the-art sanitization methods on two real-world datasets: MedQA (Jin et al., 2021), containing diverse medical notes, and a subset of WildChat (Zhao et al., 2024), featuring AI-human dialogues with personal details (Mireshballah et al., 2024). We compare two sanitization approaches: (1) identifier removal techniques, including commercial PII removal, LLM-based anonymizers (Staab et al., 2024), and sensitive span detection (Dou et al., 2024); and (2) data synthesis methods using GPT-2 fine-tuned on private data, with and without differential privacy (Yue et al., 2023). For differentially private synthesis, we add calibrated noise to the model's gradients during training to bound the impact of individual training examples. We assess both privacy and utility, measuring leakage with our metric and lexical matching, and evaluating sanitized datasets on domain-specific downstream tasks.

_Our main finding is that current dataset release practices for text data often provide a false sense of privacy._ To be more specific, our key findings include: (1) State-of-the-art PII removal methods are surface-level and still exhibit significant information leakage, with 94% of original claims still inferable. (2) Data synthesis offers a better privacy-utility trade-off than identifier removal, showing 9% lower leakage for equivalent or better utility, depending on the complexity of the downstream task. (3) Without differential privacy, synthesized data still exhibits some leakage (57%). (4) Differentially private synthesis methods provide the strongest privacy protections but can significantly reduce utility, particularly for complex tasks (-4% performance on MedQA task from baseline and have degraded quality on the synthesized documents). We also conduct comprehensive ablations, including using different semantic matching techniques and changing the auxiliary attributes used for

Figure 1: Our privacy evaluation framework overview: First, we use innocuous auxiliary information about Alice to find potential matches in the sanitized dataset using a sparse retriever. Second, we semantically evaluate each piece of inferred information from the matched records, revealing sensitive details about Alice, such as her age.

de-identification, providing a thorough analysis of our framework's performance across various text dataset release scenarios. Our results highlight the necessity to develop privacy guard trails that go beyond surface-level protections and obvious identifiers, ensuring a more comprehensive approach to data privacy in text-based domains.

## 2 Privacy Metric

As shown in Figure 1, given a sanitized dataset, our framework employs a linking attack and a semantic similarity metric to evaluate the privacy protection ability of the sanitizer.

### Problem Statement

Let \(_{}=\{x^{(i)}\}_{i=1}^{N}\) denote the original dataset and \(_{}=S(_{})=\{y^{(i)}\}_{i=1} ^{M}\) the sanitized dataset for the given data sanitization method of interest \(S\). Our goal is to evaluate the privacy of \(_{}\) under a re-identification attack by an adversary which has access to \(_{}\) as well as auxiliary information \(^{(i)}=A(x^{(i)}) x^{(i)}\) for entries in \(_{}\). The access function \(A\) depends on the threat model; in our experiments, \(A(x)\) randomly selects three claims from \(x\) (see SS2.2 below).

To assess potential privacy breaches that could result from the public release of a sanitized dataset, we define \(L(^{(i)},_{})^{(i)}\) as a linking method that takes some auxiliary information \(^{(i)}\) and the sanitized dataset \(_{}\) as inputs and produces a linked record \(^{(i)}_{}\). Let \((x^{(i)},^{(i)})\) be a similarity metric quantifying the similarity between the original record \(x^{(i)}\) and the linked record \(^{(i)}\). Given these components, we define our privacy metric as:

\[(_{},_{}) =_{x^{(i)}_{}}[(x^{(i)},L( ^{(i)},_{}))].\] (1)

### Atomizing Documents

Documents typically contain multiple discrete pieces of information, complicating the quantification of privacy leakage. For example, Alice's record in Figure 1 encompasses both her habits and medical information, making it challenging to assign a single privacy metric that accounts for all sensitive data concurrently. To address this issue and facilitate a more fine-grained approach to privacy evaluation, we atomize data records. Adopting the core concept introduced by Min et al. (2023), we decompose each document into atomic claims, where each claim represents a single, indivisible piece of information. In our framework, we partition each data record \(x^{(i)}\) into a set of atomized claims \(x^{(i)}_{j}\).

### Linking Method \(L\)

We employ a sparse information retrieval technique \(L_{}\), specifically the BM25 retriever (Lin et al., 2021), to link auxiliary information with sanitized documents. Our approach concatenates the auxiliary information \(^{(i)}\) into a single text chunk, which serves as the query for searching a datastore of sanitized documents. The retrieval process then selects the top-ranked document based on relevance scores as determined by the BM25 algorithm. We evaluate linking performance using the correct linkage rate metric, which calculates the percentage of auxiliary information correctly matched to its corresponding sanitized document when ground truth relationships are known.

### Similarity Metric \(\)

Upon linking auxiliary information to a sanitized document, we quantify the amount of information gain using a similarity metric \(_{}\). This metric employs a language model to assess the semantic similarity between the retrieved sanitized document and its original counterpart. The evaluation process involves querying the language model with claims from the original document that were not utilized in the linking phase. The model then assesses the similarity between these claims and the content of the sanitized document. We employ a three-point scale for this assessment: a score of 1 indicates identical information, while a score of 3 signifies that the claim is unsupported by the sanitized document. In this scoring scheme, a higher value of \(\) corresponds to a greater degree of privacy preservation, as it indicates reduced similarity between the original and sanitized documents. All scores are normalized to the range . The specific prompt used for this evaluation can be found in Appendix C.4.

### Baseline

To validate our approach, we establish a baseline using established text similarity metrics, defining complementary functions \(L_{}\) and \(_{}\). Both functions are implemented using ROUGE-L (Lin, 2004). Specifically, the baseline linking method \(L_{}\) processes auxiliary information \(^{(i)}\) by concatenating it into a single text chunk, following the approach described in Section 2.3, and identifies the sanitized document with the maximum ROUGE-L score. To compute the baseline privacy metric \(_{}\), we calculate one minus the ROUGE-L score between the original document \(x^{(i)}\) and its linked sanitized version. This formulation ensures that higher values indicate stronger privacy protection.

### Datasets and Utility Metrics

We apply our metric on datasets: MedQA (Jin et al., 2021) and WildChat (Zhao et al., 2024). Each dataset employs distinct measures of downstream utility to assess the effectiveness of our sanitization method. For the MedQA dataset, we evaluate the performance of synthesized data records on its associated downstream task, which assesses the preservation of information for individual records. Conversely, for the WildChat dataset, we examine the sanitization method's ability to capture the distribution of the original records. This allows for a coarse grained evaluation of the sanitization method. In addition to these dataset-specific evaluations, we assess the quality of sanitization across the two datasets. Detailed descriptions of the datasets and sanitization methods are presented in Appendix A.1, while the prompts used in our study are provided in Appendix C.

## 3 Experimental Results

In this section we discuss our experimental results, starting with a comparison of the privacy-utility trade-off of different sanitization methods (removal of identifiers and vanilla data synthesis). Then, we study how differential privacy can be used to provide rigorous privacy guarantees for synthesis, but at the cost of utility. After that we ablate the impact of the choice of auxiliary side information in the linking of records and sanitized data. Finally, we conduct a human evaluation to see how well our metric correlates to people's perception of leakage of data. We provide a few qualitative examples of matched documents in Table 5 in the Appendix.

### Privacy-Utility Trade-off: Comparing Different Sanitization Methods

We present a analysis of the privacy-utility trade-off across various data sanitization methods in Table 1. The lexical distance utilizes ROUGE-L as the similarity matching function \(L_{}\), with the corresponding privacy metric \(_{}\) calculated as one minus the ROUGE-L score, as introduced in SS2.5. Semantic distance is obtained using our prompt-based method \(_{}\) after linking the auxiliary information to the sanitized document with \(L_{}\), which evaluates whether the retrieved information semantically supports the original data, as discussed in SS2.4. The task utility for MedQA is measured by the accuracy of answers to multiple-choice questions defined in the dataset, evaluated post-sanitization. Notably, the remove all information baseline achieves an accuracy of 0.44. For WildChat, utility is determined by a normalized chi-squared distance related to the classification of documents, as described in SSA.1.1. Text coherence, as introduced in SSA.1.2, is a text quality metric ranging from 1 to 5. The higher the score, the better quality outputting generation is.

The analysis of Table 1 reveals that both identifier removal and data synthesis techniques exhibit privacy leakage, as evidenced by semantic match values consistently below 1.0 (perfect privacy). Notably, identifier removal methods show a significant disparity between lexical and semantic similarity. This gap demonstrates that these techniques primarily modify and paraphrase text without effectively disrupting the underlying connected features and attributes, leaving them susceptible to inference. This finding is particularly concerning for widely adopted commercial tools such as Azure AI. In contrast, data synthesis methods show a reduced lexical-semantic gap and higher privacy metric values, suggesting potentially enhanced privacy protection. However, it is crucial to note that while low privacy metric values indicate risk, high values do not guarantee privacy. Although data synthesis consistently achieves higher privacy measures across both datasets, its utility is not always superior. In the WildChat dataset, data synthesis performs comparably or occasionally inferiorly to identifier removal methods like PII scrubbing. Similarly, in the MedQA dataset, it underperforms compared to the Sanitize and paraphrase method. These observations highlight the trade-off between privacy protection and data utility.

### Privacy-Utility Trade-off: Data Synthesis with Differential Privacy

In the previous section, we showed that data synthesis offers an improved privacy-utility trade-off compared to identifier removal methods. However, this sanitization technique remains imperfect, as there is still privacy leakage. To address this, researchers often integrate data synthesis with differential privacy (DP) to establish formal bounds on potential data leakage (Yue et al., 2023). The bounding of the leakage in DP is governed by the privacy budget, denoted as \(\). A higher \(\) value

    & &  &  \\ 
**Dataset** & **Method** &  **Lexical** \\ **Distance** \\  &  **Semantic** \\ **Distance** \\  &  **Task** \\ **Utility** \\  & 
 **Text** \\ **Coherence** \\  \\   & No Sanitization & 0.08 & 0.04 & 0.69 & 3.79 \\  & Remove All Info & - & - & 0.44 & - \\   & Sanitize \& Paraphrase & 0.66 & 0.31 & 0.65 & 3.60 \\  & Azure AI PII tool & 0.20 & 0.06 & 0.67 & 3.29 \\  & Dou et al. (2024) & 0.61 & 0.34 & 0.61 & 2.84 \\  & Staab et al. (2024) & 0.53 & 0.33 & 0.62 & 3.07 \\   & Data Synthesis & 0.46 & 0.43 & 0.62 & 3.44 \\   & No Sanitization & 0.04 & 0.19 & 0.99 & 4.06 \\   & Sanitize \& Paraphrase & 0.73 & 0.44 & 0.62 & 3.76 \\   & Azure AI PII tool & 0.17 & 0.21 & 0.99 & 3.68 \\   & Dou et al. (2024) & 0.27 & 0.22 & 0.99 & 2.97 \\   & Staab et al. (2024) & 0.49 & 0.40 & 0.98 & 3.49 \\    & Data Synthesis & 0.86 & 0.83 & 0.93 & 3.28 \\   

Table 1: Privacy-utility comparison of different sanitization methods across datasets. Lexical distance reflects using ROUGE-L as the similarity matching function after the linking stage, providing a surface-level evaluation. Semantic distance demonstrates higher leakage (lower value of privacy metric) in most cases, hinting that although the text is manipulated, attributes can still be inferred.

    & &  &  \\ 
**Dataset** &  **Privacy** \\ **Budget** \\  &  **Lexical** \\ **Distance** \\  &  **Semantic** \\ **Distance** \\  &  **Task** \\ **Utility** \\  & 
 **Text** \\ **Coherence** \\  \\   & \(=\) & 0.46 & 0.43 & 0.62 & 3.44 \\  & \(=1024\) & 0.79 & 0.92 & 0.40 & 2.25 \\  & \(=64\) & 0.79 & 0.92 & 0.41 & 2.14 \\  & \(=3\) & 0.79 & 0.93 & 0.40 & 2.04 \\   & \(=\) & 0.86 & 0.83 & 0.93 & 3.28 \\  & \(=1024\) & 0.88 & 0.87 & 0.88 & 1.83 \\   & \(=64\) & 0.88 & 0.88 & 0.81 & 1.84 \\   & \(=3\) & 0.89 & 0.89 & 0.70 & 1.64 \\   

Table 2: Privacy-utility comparison of data synthesis using differential privacy with different levels of \(\), across datasets. Lower \(\) means more private. Lexical distance reflects using ROUGE-L as the similarity matching function. Even high values of \(\) provide low leakage, albeit at the cost of utility and quality.

corresponds to reduced privacy. Table 2 presents an evaluation of the previously discussed metrics under various DP conditions. The row where \(=\) is equivalent to not applying differential privacy, i.e. the vanilla data synthesis row from Table 1.

Our analysis reveals that implementing DP, even with relaxed guarantees such as \(=1024\), significantly enhances privacy protection. The lexical privacy metric increases from \(0.46\) to \(0.79\), and the semantic privacy metric from \(0.43\) to \(0.92\). However, this enhanced privacy comes at the cost of task utility. For MedQA, utility drops from \(0.62\) to \(0.40\), falling below the baseline of not using private data (\(0.44\)). Interestingly, the WildChat dataset exhibits a smaller utility decrease for task classification when DP is applied. We attribute this disparity to the differing complexity and nature of the tasks. Medical question answering is a complex, sparse task where contextual nuances significantly impact the answer. Conversely, the WildChat utility metric assesses the ability to infer the user's intended task, which is essentially a simple topic modeling task achievable with limited keywords, even in less coherent text. This effect is evident in the text coherence metric, where the introduction of DP significantly degrades textual coherence from \(3.28\) to \(1.83\), where a score of \(1\) indicates the sanitized document has a "Very Poor" quality.

A final observation from this experiment reveals that, unlike in the previous section, certain \(\) values yield privacy metrics via lexical overlaps that are much lower than semantic similarity. Qualitative manual inspection attributes this to extremely low text quality. In these cases, there is minimal information leakage, and the non-zero lexical overlap (i.e., privacy metric not reaching \(1.0\)) stems from matches in propositions, articles, and modifiers (e.g., "a", "the") with the original text, indicating false leakage. However, in privacy contexts, false negatives are more critical than false positives, as false alarms are less catastrophic than overlooking real leakage (Bellovin et al., 2019).

### Analysis: Changing the Available Auxiliary Information

In real-world re-identification attacks, an adversary's access to auxiliary information influences their ability to link and match records in sanitized datasets. Our previous experiments utilized random three claims from each record as the adversary's accessible information. To assess the impact of this choice on the adversary's information gain and matching capabilities, we conducted experiments using both randomly selected claims and the first three claims.

Table 3 presents the results of these experiments, focusing on the correct linkage rate (defined in SS2.3) for sample-level, identifier removal methods. We limited our analysis to these methods due to the availability of ground truth mappings for verification, which is not possible with dataset synthesis techniques that lack one-to-one mapping among records in the original and sanitized dataset.

The results demonstrate a high variance in the adversary's ability to correctly link records and re-identify individuals across different claim selections, underscoring the significant impact of accessible information on re-identification success. Notably, for the MedQA dataset, methods relying

    &  & **First Three** & **Random Three** & **Last Three** \\  & & **Claims** & **Claims** & **Claims** \\   & No Sanitization & 0.99 & 0.99 & 0.99 \\   & Sanitize \& Paraphrase & 0.58 & 0.66 & 0.78 \\  & Scrubbing & 0.81 & 0.91 & 0.94 \\  & Dou et al. (2024) & 0.70 & 0.67 & 0.69 \\  & Staab et al. (2024) & 0.58 & 0.69 & 0.78 \\   & No Sanitization & 0.98 & 0.98 & 0.98 \\   & Sanitize \& Paraphrase & 0.59 & 0.62 & 0.56 \\   & Scrubbing & 0.89 & 0.88 & 0.82 \\   & Dou et al. (2024) & 0.88 & 0.88 & 0.83 \\   & Staab et al. (2024) & 0.66 & 0.69 & 0.68 \\   

Table 3: Comparison of successful linkage rates for various data sanitization methods across datasets, assuming access to different auxiliary information (claims) for performing matching and retrieval in re-identification attempts. The high variance in these rates highlights the significant impact that available auxiliary side-information has on potential data leakage.

on Large Language Models (LLMs), such as sanitize & paraphrase and the approach proposed by Staab et al. (2024), exhibit the highest variance. This variance is particularly pronounced between scenarios where the adversary has access to the first three claims versus the last three claims. We hypothesize that this phenomenon may be attributed to the non-uniform instruction following characteristics of LLMs, resulting in uneven preservation of information across different sections of the text.

### Human Evaluation of the Similarity Metric

We conducted a small-scale human study to assess the efficacy of our language model in reflecting human preferences for the similarity metric \(\), as defined in Section 2.4. Three of the authors provided annotations for 580 claims. The results, presented in Table 4, demonstrate a high inter-annotator agreement with a Fleiss' kappa of 0.87. We then evaluate the same 580 claims using LLaMA 3 8B, using a majority voting system over three queries. This method achieved a Spearman correlation coefficient of 0.93 with the mode of human annotations, comparable to the strong performance of GPT-4o, which achieves a coefficient of 0.96. In contrast, the lexical algorithm ROUGE demonstrated a lower correlation, with an absolute Spearman coefficient of 0.81.

## 4 Related Work

**Privacy evaluations of dataset disclosure.** Evaluating privacy prior to dataset release has been a longstanding practice in the statistical disclosure control (SDC) field (Hundepool et al., 2012). This practice spans various fields, including legal, technical, and medical domains (Bellovin et al., 2019; Garfinkel, 2015; Giuffre and Shung, 2023). Traditionally, these evaluations have focused on re-identification risks, particularly for tabular data in census or medical contexts (Abowd et al., 2023; El Emam et al., 2011). While there have been attempts to create text anonymization benchmarks (Pilan et al., 2022), these primarily concentrate on span detection and anonymization rather than comprehensive re-identification and focus on scrubbing methods rather than data synthesis, contrary to our work. Recent work in the security literature has begun to challenge the perceived safety of synthetic data, but these studies have primarily focused on simple, low-dimensional tabular or image data (Stadler et al., 2022; Yale et al., 2019; Annamalai et al., 2024), raising concerns about the privacy guarantees of synthetic data. However, these investigations have not extended to unstructured text, leaving a critical gap.

**Data sanitization through removal of identifiers.** Traditional approaches to data sanitization have centered on the detection and removal of Personally Identifiable Information (PII) (Mendels et al., 2018; Montani et al., 2022) relying on named entity recognition (NER) systems and masking. Recently, LLMs have been employed for this task: Staab et al. (2024) developed an iterative prompting method using GPT-4 to achieve implicit attribute removal, moving beyond simple token replacement. Similarly, Dou et al. (2024) proposed a two-step approach, combining a self-disclosure detection model with an abstraction technique to reduce privacy risks in text data. Morris et al. (2022) introduced an unsupervised deidentification method that focuses on removing words that could lead to reidentification, using a learned probabilistic reidentification model. Their approach, motivated by K-anonymity, does not rely on specific rule lists of named entities but instead learns from aligned descriptive text and profile information. However, their method requires a dataset of aligned text and profiles, which may not always be available in real-world scenarios. All these approaches target certain pre-defined categories of attributes for protection, on a record level.

  
**Metric/Model** & **Measure** & **Value** & **P-value** \\  Human Agreement & Fleiss’ Kappa & 0.8748 & - \\  LLaMA 3 8B & Spearman Correlation & 0.9252 & 2.37e-245 \\  GPT-4o & Spearman Correlation & 0.9567 & 5.37e-312 \\  ROUGE-L recall & Spearman Correlation & -0.8057 & 1.48e-133 \\   

Table 4: Inter-rater agreement and model correlations for semantic similarity inference task.

**Data sanitization through synthesis.** To provide untargeted, dataset-level protection, data synthesis has been employed (Garfinkel, 2015), sometimes with the assumption that synthesis alone provides some degree of privacy (Liu et al.). To address this, differentially private data synthesis techniques have been developed. Xie et al. (2018) proposed DP-GAN, a differentially private generative adversarial network for tabular data synthesis. Torkzadehmahani et al. (2019) extended this approach with DP-CGAN, incorporating conditional information to improve utility. For textual data, Weggenmann et al. (2022); Igamberdiev and Habernal (2023); Bo et al. (2021); Igamberdiev et al. (2022) proposed and benchmarked differentially private VAE, BART, and autoencoder with embedding rewards, to sanitize text. Yue et al. (2023); Mattern et al. (2022); Mireshballah et al. (2022); Kurakin et al. (2023) introduce differentially private fine-tuning approachs for large language models to generate synthetic text. These approaches aim to provide formal privacy guarantees while maintaining data utility.

## 5 Conclusion

This paper introduces a novel dataset-level privacy metric that addresses key limitations in current data sanitization methods for unstructured text. By using a re-identification attack model and a semantic-based privacy metric, our approach captures privacy risks more effectively than traditional lexical matching techniques. Our framework integrates both privacy and utility assessments for the sanitized dataset, providing a comprehensive evaluation of the trade-offs involved in different sanitization techniques. Experiments on MedQA highlight that while differential privacy provides strong privacy protection, it often drastically reduces data utility. Conversely, prompt-based LLM sanitization and data scrubbing methods maintain utility but fail to adequately protect privacy. Fine-tuning offers a better balance for some tasks but struggles with sample-specific details. Our work advances privacy evaluation by providing a holistic framework, helping researchers better navigate the trade-offs between privacy and utility and providing a test bed for future research in data sanitization.

## Limitations and Future Works

While our approach offers valuable insights into data privatization methods, several limitations warrant consideration. Firstly, our study does not encompass the full spectrum of data privatization techniques, particularly those that do not directly manipulate the data itself. Secondly, although we have conducted preliminary investigations into the efficacy of our approach at various stages of the pipeline, further rigorous studies are necessary to fully validate its accuracy, especially concerning the computations of privacy metric. Additionally, our analysis was confined to a single dataset within the medical domain, which limits the generalizability of our findings. Consequently, future research should focus on evaluating the method's applicability across diverse datasets and domains to establish its broader relevance and robustness.

Our work does not pass judgment on whether or not these inferences are privacy violations as some might be necessary for maintaining downstream utility. Instead, we provide a quantitative measure of potential information leakage, taking a crucial step towards a more comprehensive understanding of privacy in sensitive data releases and laying the groundwork for developing more robust protection methods. Ideally, one would want _contextual_ privacy metric, which can take into account (i) which information is more privacy-relevant and (ii) which information is private in the context that the textual information is being shared. These are extremely challenging questions that we believe are beyond the scope of this paper. Nevertheless, they represent exciting research directions to pursue, particularly given recent advances in LLMs.

## Ethics Statement

Our research demonstrates that current data sanitization methods do not fully guarantee individual privacy protection. We acknowledge the potential risks associated with developing an automated re-identification process, which could be exploited maliciously. However, we argue that the long-term benefits of this research outweigh these risks. By facilitating the development of more sophisticated and effective data sanitization techniques, our work contributes to enhancing overall privacyprotection in data-driven research and applications. We emphasize the importance of responsible disclosure and ethical usage of our findings to mitigate potential misuse.

This study utilizes two primary datasets: WildChat and MedQA. WildChat (Zhao et al., 2024) comprises user interactions with GPT-3.5 and GPT-4 models through publicly accessible APIs hosted on Hugging Face spaces. Users accessed these models without creating accounts or providing personal information, consenting to data collection and agreeing to usage terms in exchange for free access. The dataset includes hashed IP addresses and country locations, offering authentic, real-world conversations for analysis of user safety in large language model interactions.

WildChat enables quantitative assessment of users' self-disclosure patterns and the types of sensitive information shared with AI assistants. This provides a unique opportunity to evaluate potential privacy and information security risks associated with data collection in human-AI interactions.

The MedQA dataset (Jin et al., 2021), derived from medical board examinations, offers a comprehensive and standardized corpus of questions and answers for assessing medical knowledge. Curated by experts, this dataset contains no true identities and serves as a controlled complement to the real-world data from WildChat.