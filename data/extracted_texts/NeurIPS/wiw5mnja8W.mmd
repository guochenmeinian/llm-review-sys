# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_FAIL:2]

**Why are such simulators important for ML research?** The successes in other subareas in machine learning are driven largely by the existence of capable and qualitative simulators [10; 11; 12; 13]. With an easy-to-use interface and easily customisable environments, simulators allow researchers to focus on model development rather than creating their own (often conflicting) evaluation protocols. With AllSim, we hope to drive innovation for resource allocation in multi-user problems in healthcare, engineering, economics, etc. The aforementioned simulators are great examples of systematic evaluation across entire research communities, however, they do not: learn realistic and unbiased simulation objects from data, allow for multi-user simulation, or model dynamic non-steady-state scenarios.

**Desiderata.** From Figure 1 we identify three important desiderata: (1) A simulation should extract unbiased components from historical data which was tainted by existing policies; (2) The simulation should infer unbiased outcomes despite having access to only these biased data, which includes long-term impact on system-wide utilities since present allocations influence future allocations, requiring counterfactual inference (to determine outcomes under different allocations). (3) Using the extracted components from (1), a user must be able to perturb and change the components to fit their specific needs to evaluate different policies and settings before being deployed in the real world.

**Contributions** In this work, we present AllSim (Allocation Simulator), a general-purpose open-source framework for performing data-driven simulation of scarce resource allocation policies for _pre-deployment_ evaluation. We use modular environment mechanisms to capture a range of environment conditions (e.g. varying arrival rates, sudden shocks, etc.), and provide for comprehensive parameters to be learned from historical data, as well as allowing users to further configure parameters for stress testing and sensitivity analysis. Potential outcomes are evaluated using unbiased causal effects methods: Upon interaction with a policy, AllSim outputs a batch dataset detailing all of the simulated outcomes, allowing users to draw their own conclusions over the effectiveness of a policy. Compared to existing work, we believe this simulation framework takes a step towards more methodical evaluation of scarce resource allocation policies.

In Appendix B we compare against other strategies used to evaluate allocation policies. AllSim's itself is built using ideas from various fields in machine learning which we also review in Appendix B. Furthermore, in Appendix B.1 we review some medical simulations which _seem_ related, but are not.

## 2 AllSim

Let \(X^{d}\) denote the feature vector of a _user_, and let \((t)\) denote the arrival process of users. At each time \(t\), let \((t)\{X_{i}\}_{i=0}^{N(t)}\!\!(t)\) give the arrival set of (time-varying) size \(N(t)\). Likewise, let \(R^{e}\) be the feature vector of a _resource_, and let \((t)\) be the arrival process of resources. At each time \(t\), let \((t)\{R_{j}\}_{j=0}^{M(t)}\!\!(t)\) give the arrival set of (time-varying) size \(M(t)\).

While we make no assumptions on how users are modelled, we assume that resources are immediately _perishable_--that is, each incoming resource cannot be kept idle, and must be consumed by some user in the same time step. In organ transplantation, for instance, the time between harvesting an organ and transplanting it ("cold ischemia time") must be minimized to prevent degradation [14; 15; 16].

Let \(Y_{+}\) be the outcome of a _matched_ user, drawn from the distribution \((X,R)\) induced by assigning a resource \(R\) to a user \(X\). At each time \(t\), let \(_{+}(t)\{Y_{+}(X_{R},R):R(t)\}\) give the set of outcomes that result from matching each incoming \(R(t)\) with its assigned \(X_{R}\). Likewise, let \(Y_{-}\) be the outcome of an _un-matched_ user, drawn from the distribution \((X,)\). At each time \(t\), let the set of outcomes for users who are never assigned a resource be given by

  
**Problem setting** & **Users** & **Resources** & **Allocation policy** & **Utility** \\  _Headhunting_ & Openings & Applicants & Assignment & Hires (\& retention) \\ _Project staffing_ & Projects & Workers & Staffing & Project success \\ _Organ transplantation_ & Patients & Donors & Matching & Post/Pre TX survival \\ _Mechanical ventilation_ & Patients & Vertilators & Triaging & ICU Discharge \\ _Bicycle sharing_ & Docks & Bicycles & Redistribution & Idle times \\   

Table 1: **Example situations. We list a few example situations that follow the general problem formalism introduced in this paper and the repeated allocation policy used to solve them.**\(_{-}(t)\{Y_{-}(X,):X(t), ( t^{} t)(R(t^{}),X=X_{R})\}\). (Note that we focus on discrete-time settings (e.g. hours or days), and leave continuous time for future work). Then we have:

**Definition 1** (Repeated Resource Allocation): Denote an _environment_ with the tuple \((,,)\). The _repeated resource allocation_ problem is to decide which users to assign each incoming resource to--that is, to come up with a _repeated allocation policy_\(:^{e}(^{d})^{d}\), perhaps to optimize some utility defined on the basis of (un-)matched outcomes. For instance, if \(Y\) is a patient's post-transplantation survival time, we might wish to maximize the average survival time.

With the necessary notation, and a formal definition of a policy's input and output in Definition 1, we are equipped to introduce each component of AllSim as illustrated in Figure 1. In Sect. 2.4 we also discuss how AllSim's output can be used to evaluate a new policy (or any other modification from the decision maker). Details regarding the simulation life-cycle can be found in Appendix A.

### Arrival of users and resources

There are two necessary ingredients that comprise the arrival of new users, and new resources: the amount (\(N(t)\) and \(M(t)\), respectively), and the description (\(X_{i}\) and \(R_{j}\), respectively). Each is modelled differently. Before we sample the user and resource description, we first sample the amount of each arriving at time \(t\) from an associated arrival process-- i.e., in this subsection we will focus on \(N(t)\) and \(M(t)\). We first introduce the structure of the arrival processes, and explain how their parameters can be learned from data and modified by a decision maker to setup the environment.

First, we stress that \(N(t)\) and \(M(t)\) are not necessarily sampled from _constant_ arrival processes. Instead, we want the user and resource arrivals to change over time either completely or per user/resource type, which we will explain in more detail below. To accommodate this, we split each arrival process into a product of separate arrival processes which we combine into \((t)\) and \((t)\) as:

\[}(t,_{x}) =}_{1}(t,_{1,x})}_{K}(t,_{K,x}),\] (1) \[}(t,_{r}) =}_{1}(t,_{1,r})}_{L}(t,_{L,r}),\] (2)

where each individual arrival process in \(}_{k}\) and \(}_{l}\) is parameterised with (learnable) parameters \(_{k,x}\) and \(_{l,r}\) with \(k[K]\) and \(l[L]\), respectively. Each factor corresponds with some (learned or predefined) user-type (Equation (1)) and resource-type (Equation (2)). Having these factors allows us to model increasing numbers of, for example, older/younger patients entering a transplant wait-list.

In order for \((t)\) and \((t)\) to change over time, we let their parameterisation, \(_{x}\) and \(_{r}\), change in \(t\). As an example, we can set the arrival processes to Poisson processes (we refer to Appendix E for other examples) with arrival rates \(_{x}=_{k}(t)\) and \(_{r}=_{l}(t)\), which we can both model over time as,

\[_{k}(t) =_{k}_{k}(0)g_{k}(t),\] (3) \[_{l}(t) =_{l}_{l}(0)g_{l}(t),\] (4)

with \(_{k}(t),_{l}(t)_{+}\), and \(_{k},_{l}_{+}\) as a normalising constant such that the sum of all \(_{k}\) equal some overall arrival rate \(a_{x}\), and similarly, the sum of all \(_{l}\) equal some overall arrival rate \(a_{r}\). Lastly, \(g_{k}\) and \(g_{l}\) are continuous functions that simulate a user-specified drift. Note that these \(g\) can also be a combination of multiple drift scenarios, or can be shared across different \(k,l\). Having \(g\), allows practitioners to very accurately describe the non-stationarity they wish to test for. Optionally, \(_{k}\) and \(_{l}\) can be kept fixed throughout the simulation such that \(a_{x}\) and \(a_{r}\) vary as does \(g_{k,l}(t)\), or it can be recomputed for every step \(t\), such that \(a_{x}\) and \(a_{r}\) are kept fixed throughout the simulation.

As such, we have a set of arrival rates, \(_{x}=[_{1},...,_{K}]\), with \(_{k}_{k}=_{x}\) with \(_{x}_{+}\) as the total arrival rate of recipients. The advantage of splitting \(_{x}\) into multiple \(_{k}\), is that we can finetune the arrival of certain recipient types, yet allow comparison between \(_{x}\) and \(_{r}\) (the total arrival rate for resources). For example, the \(k^{}\) recipient type may be completely absent when a policy is launched, but over time it gradually enters the system, increasing \(_{x}\) as a whole. Naturally, we also model the arrival of resources as we have for recipients, but left it out of discussion for clarity.

Learning \(_{x,r}\) naturally depends on the choice of arrival process. In our setups below we use a Poisson process and have either: (i) learned the dynamic parameters \(_{k,l}(t)\) as in Equations (3) and (4) using polynomial regression over a time-windowed average of incoming users and resources-- over all data to compute a correct \(_{k,l}\), as well as per predefined condition; or (ii) have predefined an arrival function and drift functions, \(g\), to illustrate a scenario where one wishes to test a prespecified scenario.

### New users and resources \(p()\) and \(p()\)

From \(}(t)\) and \(}(t)\) we sample \(N(t)\) and \(M(t)\), respectively. Of course, we need to provide the tested policies with more than just an _amount_ of users and resources arriving at time \(t\). Furthermore, when working with user and resource types (using the decomposition in Equations (1) and (2)), we have \(N(t)=_{k}N_{k}(t)\) and \(M(t)=_{l}M_{l}(t)\), where each \(N_{k}(t)\) and \(M_{l}(t)\) represents an amount of users and resources _per type_. As such, we need these types to sample detailed descriptions of each.

When a recipient or a resource arrives, we sample them from a distribution denoted \(p_{_{s}}(X)\) for the recipients, and \(p_{_{s}}(R)\) for the resources. These distributions are either learnt from data, or shared as an open-source (but privatised) distribution. For the user-distributions we learn from \(_{t}_{t}\), and similarly, for the resource-distributions we learn from \(_{t}(t)\). Since both remain independent from the past policy (no policy determines which users and resources arrive in the system), we can use any (conditional) generative model to learn these distributions as we are not required to de-bias these data.

Of course, we need to be able to sample specific user and resource _types_. For this we require _conditional_ generative models, where the condition corresponds with a type: \(p_{_{s}}(X)\) becomes \(p_{_{s}}(X|k)\), and similarly \(p_{_{s}}(R)\) becomes \(p_{_{s}}(R|l)\). In case we wish to use an unconditional generative model, we can simply learn multiple: \(p_{_{k,x}}(X)\) for each \(k[K]\), and similarly for \(p_{_{l,x}}(R)\) for each \(l[L]\).

Interestingly, we do not need to account for any variability (learned nor specified) over time, since this is completely modelled through the arrival processes in Sect. 2.1. In particular, whenever we wish one type, \(k\) to dominate others, we simply increase \(g_{k}(t)\) in Equations (3) and (4). In case we only want one type to appear after \(t\) in the simulation, we set \(g_{k}=0\) for \(t^{}<t\) and increase it for \(t^{}>t\).

### Utility

The final component in AllSim, as per Figure 1, are the utilities: functions of the policy (\(\)), the users and resources (\(X\) and \(R\)), and crucially, the allocation outcomes (\(Y\)). Given the previous sections, all that remains are the outcomes and how we can combine each element into a new dataset, \(^{}\), with _counterfactual_ outcomes, \(Y^{}\).

As the outcome is a function of the resource and its recipient, inference is a hard problem as allocations suggested by the tested policy deviate from historical data which was collected under a different policy (i.e., they are counterfactual). Consequentially, some combinations are less observed in the original data, illustrated in Figure 2. In Figure 2 we illustrate two policies, \(\) and \(^{}\) which result in different datasets \(\) and \(^{}\). The latter (\(^{}\)) is what we wish to provide with AllSim, using only data from the former (\(\)).

**Counterfactual inference.** AllSim handles this difficult problem by using a counterfactual estimator. Counterfactual methods correct for allocation bias explicitly . In particular, these methods aim to make an unbiased prediction of _the potential outcome_, associated with some treatment (or resource). We are interested in counterfactual methods that model the potential outcomes for the recipients when they are (not) allocated a resource. A counterfactual estimator then "completes" the dataset (\(^{}\)) as,

\[(t)=[}((t))|_{}(t )],\] (5)

where \(}((t))\) is the estimated potential outcome, using methodology known in the potential outcomes literature [17; 18; 19; 20; 21; 22], and \(_{}(t)\) are the recipients selected by a policy \(\) at time \(t\). The potential outcome is a random variable depicting the (possibly alternative) outcome when the user receives the resource, \(R(t)\). Note that this is not the same as simply conditioning the outcome

Figure 2: **Allocation policies bias data. Above illustration depicts two policies: \(\) and \(^{}\). Each policy is tasked with assigning resources to users as per Definition 1. Besides users and resources, we observe an outcome, \(Y\). Despite observing the same users and resources, a different policy results in _completely_ different outcomes, \(Y^{}\), and data, \(^{}\).**

variables on the users, for the reasons outlined above: conditioning using only biased data will lead to biased estimates for the outcome variable. Hence, literature on counterfactual inference introduced the potential outcomes notation in Equation (5) to differentiate between \(Y(R(t))\) and \(Y|R(t)\). We provide a comprehensive overview of counterfactual methods and literature in Appendix H.

### Putting it all together

We have now discussed each component in the middle section of Figure 1. What remains are the decision maker's perturbations, and finally, combining each component into a new dataset, \(^{}\).

**Perturbations.** From Figure 1 we learn that a decision maker can make three types of perturbations: (i) they can replace the original policy, \(\), with a new (alternative) policy, \(^{}\); (ii) they can change the utility function, \(u\), which takes as argument a dataset comprised of users, resources, outcomes, and a policy; and lastly, (iii) they can change the types, as well as the amount, of users and resources entering the system. Given these perturbations, the policy is allowed to act in a different environment.

Changing the policy in (i) is done simply by implementing the new policy according to the simulation interface (discussed in the next section). We stress once more that this paper does not provide guidance for allocation policies nor does it propose a new policy of any kind. In fact, the policies used in the following section are tried and tested policies, currently in use in practice. Changing the utility function for (ii) is easily done in AllSim as running the simulation does not depend at all on the chosen utility function! As AllSim provides a completely counterfactual dataset, \(^{}\), the utility is computed _post-hoc_ which allows us to always fall back on the generated dataset. Finally, perturbing arrivals (iii) is already discussed in Sect. 2.1; the arrival processes are perturbed through \(g\).

**Sampling data.** Equations (1) to (4) provides us with \((t)\) and \((t)\). Equation (5) provides us with an estimated potential outcome given \(X(t)\), \(R(t)\) and their allocations using \(^{}\). AllSim then carefully indicates a timestamp for each combination and then presents the decision maker with a new dataset:

\[^{}\{(X,R,(R),t)_{i}:i=1,,N\}.\]

Having a new (counterfactual) dataset based on \(^{}\), \(^{}\), allows to easily calculate various performance utilities of interest, which the decision maker can use to evaluate the allocation policy, _pre-deployment_:

**Definition 2** (Pre-Deployment Evaluation): Let \(f:_{k}^{k}...\) denote an _evaluation metric_ mapping a sequence of outcomes \(\{(t)\}_{t=1...}\) to some space of _evaluation outcome_\(\) (e.g. for the average survival time, this would simply be \(\)), where \((t)_{+}(t)_{-}(t)\). Given a problem \(\) and policy \(\), the _pre-deployment evaluation_ problem is to compute statistics of the distribution \(_{,}\) of evaluation outcomes \(f(\{(t)\}_{t=1,...})\); commonly, this would be the mean \(_{,}[f(\{(t)\}_{t=1,...})]\).

Note that we have defined \(f\) in terms of the _sequence_ of per-period outcomes such that it gives maximum flexibility: Depending on how individual outcomes \(Y\) are defined, we can measure point estimates (e.g. the mean survival), compare subpopulations (e.g. whether some types of recipients systematically receive more favourable outcomes), examine trends (e.g. whether outcomes degrade as the types of recipients arriving change), or potentially investigate more complex hypotheses.

## 3 AllSim Interface & Examples

Given the formal definition of AllSim presented in Sect. 2 (and further in Appendix A), we now introduce AllSim's programming interface and use it directly to provide some experimental results. We split this section in two major parts: first, we show the type of analysis AllSim can do for us, as well as how to tailor AllSim to the decision maker's needs; and then, we show how realistic the AllSim simulations are, compared to the factual data; we show that AllSim models realistic systems.

### Example analysis and decision-maker specifications

As a first example, let us showcase an analysis to illustrate the possible impact AllSim may have in practice. Throughout this section, we will use the open-access United Network for Organ Sharing (UNOS) dataset which comprises 25 years of liver-to-patient allocation. Importantly, we had to make zero adjustments to our framework to fully capture these data, showcasing the generality of the AllSim framework. We only use UNOS data until 2019 which, interestingly, predates the COVID-19 global pandemic. As such, it is impossible to evaluate policies using only these data: we need AllSim to model a counterfactual scenario that mimics what we saw during the pandemic to test a policy.

In Figure 3 we ran the MELD-Na policy in two hypothetical scenarios: one where COVID-19 happens (which resulted in a 50% drop in the donor liver arrival rates ), and one where it doesn't. With AllSim we can model each scenario confidently. For this particular example, we fix the seed of AllSim and only change the supply of organs by giving two different resource arrival processes:

```
1defcovid(t):
2ift<600:
3return.5else:
5return.25
6
7defno_covid(t):
8return.5 ```

Figure 4: **Specifying a simulation using AllSim. In the above, a decision maker defines a set of donor arrival rates, based on age (\(_{}\) and \(_{}\)). Using these very simple, but custom, arrival rates, we see a direct influence in the user and resource distributions (\(p()\) and \(p()\)). These perturbations constitute as perturbations of type (iii) as per Sect. 2.4. Finally, the decision maker tries out three different policies: MELD, MELD-na, and FIFO, which constitute as perturbation type (i). The result of these policies is shown on the right. The reported averages are windowed over 300 samples.**

Figure 3: **Two hypothetical scenarios. We require AllSim to evaluate a policy (e.g. MELD-Na) in hypothetical (counterfactual) scenarios. The x-axis is time, and the y-axis indicates survival time.**

### AllSim's realism

In this section, we will learn an AllSim configuration purely from the UNOS data (i.e. without a decision maker's input), such that we can compare AllSim's output side-by-side with what actually happened in UNOS. If they match up, we confirm that AllSim can output realistic scenarios (as UNOS is a real dataset). However, before we do so, we first show _how_ we use AllSim from a programming perspective and configure appropriate arrival processes and densities for this particular use case.

First we determine how many users and resources we need to sample, once we know the amount we sample them from a generative model. The former is modeled through a Poisson arrival rate that changes over time, and the second is sampled from some learnt density. Of course, a user can implement their own arrival process by inheriting from the abstract ArrivalProcess class.

Importantly, we need to be able to condition the density on some pre-specified characteristic of the object of interest. For example, one may be interested in modelling the arrival of harvested organs of older patients distinctly from younger patients. An example of this is provided in Figure 5, where we show the changing resources coming in the system, alongside the code that generated the result.

Object densities.Before discussing a temporal arrival rate, we first discuss modelling the object's densities. Consider lns 3-12 in the righthand side of Figure 5. Using this code, we first define what we want to condition on, using a Condition object: in this case we formulate age brackets. With the KDEDensity class, which is a subclass of the abstract Density class, we can automatically model a density, conditioned on these age brackets. Each Density object implements a fit and sample function, which is used to sample new objects by the System, which we discuss next.

Arrival processes.Using a Density, we move on to lns 14-26, where we first build a system of multiple arrival processes, one for each discrete condition as in Equations (1) and (2). In particular, we define a PoisonProcess for each condition (or age bracket), which is then provided to a PoissonSystem. Using the PoissonSystem, we can sample the arriving objects for each \(t\) in

Figure 5: **AllSim (easily) simulates realistic environments. Using real-world data on donor organs, we let AllSim model 3 years of organ arrivals and compare it with the actual arrival as reported in the data. In Figures 4(a) and 4(b) we show AllSim’s output (in donor age and BMI), given the code on the right. With minimal code, a simple condition (4 age brackets), and conservative models (polynomial regression to fit the arrival rates, and a Gaussian kernel density to model the organ densities), we find that AllSim accurately models the actual (real-world) arrival of organs as reported in the UNOS data.**In 25. Note that we also model alpha, returning the overall arrival rate, such that the system can calculate an appropriate \(\). Figure 4(a) shows that AllSim accurately models the arriving objects.

With the arrival processes coded above together with a counterfactual Inference object, we compose a Simulation object-- the main interaction interface. In particular, one defines a set of arrival rates (such as in Figure 4(b)) for both users and resources (cfr. ln 13-17) to create a simulation:

```
1simulation=asim.init(resource_process,patient_process,inference,columns) ```

With that simulation, a practitioner can instantiate a Policy, which implements the add and select methods. For example, we have implemented the MELD policy , which is a widely known and used ScRAP for liver allocation. Using the simulation, we can generate a simulated dataset:

```
1df=simulation.evaluate(policy=meld_policy,T=T) ```

Where df is a Pandas DataFrame. Naturally, df contains an enormous amount of information w.r.t. the policy's allocations in our environment. As such, we have included only a subset of the potential results in Figure 4. Additional results and details can be found in Appendices C and H. Ultimately, the practitioner determines appropriate analysis, settings, and performance metrics.

### Beyond Organs

AllSim is a general purpose simulator which evaluates scarce resource allocation policies. While we have mainly focused on organ-transplantation so far, AllSim is also applicable in other settings. To illustrate, we show how one can implement a vaccine distribution policy evaluation system in AllSim. This use-case illustrates the few adjustments one has to make compared to the organ-allocation problem. Specifically, in vaccine distribution, each resource is the same and arrives in batches. Furthermore, the type of patient-in-need is also much broader (in fact, they cover the entire population). Yet, AllSim is perfectly capable of modelling this scenario given the following:

* Batch arrival requires a multiplier: if the Poisson process samples a value of 2 on one day, we could simply interpret this as two batches of 1000 doses, i.e. multiply by batch content.
* We no longer require a resource density as vaccines are not unique, contrasting organ allocation. This is implemented as a dummy-density that always returns 1 (or the vaccine amount).
* The broader patient-type is achieved by retraining the recipient-density on the entire population.

These implementation details are relatively simple to implement using AllSim's modular API. While not necessarily a problem in vaccine distribution, recipient arrival in the ICU in a setting of infectious disease (such as COVID-19), is definitely different compared to the organ-allocation setting. With organ-allocation, we can safely assume a Poisson process for recipient arrival as recipients enter the system independently. This is not true for infectious diseases: one recipient arriving may indicate higher infection rates. As such, recipients _do not_ arrive independently, motivating AllSim's design.

It is clear that above scenario can no longer rely on a Poisson arrival process for new recipients entering the system. Instead, accurately modelling a situation of infectious disease could be done using a Hawkes process. To illustrate, we include some code below showing exactly how one may go about including such a Hawkes process in AllSim (replacing the Poisson processes used earlier).

```
1classHawkesProcess(PoissonProcess):
2def_init_(self,lam:float=.1,update_lam:Callable[[int],float]=lambdat:t,delta:float=.1,a:float=.2):asserta>=0,"ashouldbellargerthanorequalto0"assertdelta>0,"deltashouldbellargerthan0"super()._init_(lam,update_lam)
1self.a,self.delta,self._samples=a,delta,[]defget_lam_unnormalized(self,t:int)->float:
* returnself_baseline_lam+np.sum( self.a*self.beta*np.exp(-beta*(t-self_samples[ self._samples<t])))
* defprogress(self,t:int,neu:float=1)->int:
* self.lam=neu*self.get_lam_unnormalized(t)#eqs.(5,6) sample=np.random.poisson(lam=self.lam)
* self._samples.append(sample) returnsmple

**Allocation policies from machine learning and OR.** It seems that both the ML [1; 29; 3; 30; 31; 32; 33; 34] and OR [35; 36; 37; 38; 39; 40; 41] community is focused more and more on this important class of problems- which is fantastic! But it also warrants careful evaluation. Furthermore, if we find that the evaluation strategies in medicine (which generally propose linear combinations of features [42; 26] or simple CoxPH models [43; 44; 45; 46]) have shortcomings, then this is certainly the case for much more complicated strategies introduced in ML or OR. In fact, a recent survey confirmed exactly this concern: [47; cfr. Limitations of ML in transplant medicine]. It is in these extended scenarios where AllSim could help.

Naturally, problems solved by the OR community concern a _variant_ of the general problem presented in this paper. For example, Balseiro et al.  are concerned with distributing a _fixed_ set of resources, to a varying set of incoming users. While different, such problems can still be modelled in AllSim. In the specific case of Balseiro et al. , resources are not unique (they represent an amount) and require much less machinery than what we require to model the varying resource scenario. Specifically, one can model the remaining amount of resources as an attribute in our Policy class.

## 4 Conclusion

AllSim provides the means to perform standardised evaluation of repeated resource allocation policies in non-steady-state environments. While our experiments focus on organ-transplantation for the sake of exhibition, Appendix G illustrates AllSim for COVID-19 vaccine distribution, an example outside organ-transplantation. We believe that AllSim's generality and modularity allows for _sensible_ adoption in a wide range of application areas. Furthermore, having standardised evaluation will encourage research in this very important and impactful domain spanning many application areas.

Conducting research in repeated resource allocation requires consideration of a policy's societal impact. While we believe AllSim will _aid_ (rather than negatively impact) in this respect (by offering more than simple aggregate statistics), in Appendix F we provide a section dedicated to this topic.

**Ethical research.** We envisage AllSim as a tool to _help_ accurate and standardised evaluation of repeated resource allocation policies, however emphasise that any finding would need to be further verified by a human expert or in some cases by a clinical trial. Ultimately, the decision on whether or not to trust a decision making tool is up to the acting decision-maker and ethics board. We hope that AllSim can help in any way to facilitate that decision, but stress that suggestions or evaluation always require critical assessment, as is the case for any research. We also refer the reader to Appendix F for a more thorough discussion on the potential societal impact of systems such as AllSim.

**Reproducibility.** To encourage reproducibility, we have included all our code to reproduce the presented results (as well as those in Appendix C). It should be clear from this paper, that reproducibility is actually one of the main reasons for doing this type of research in the first place. Furthermore, we have included a detailed discussion on how to use our simulation in Appendices D and E.