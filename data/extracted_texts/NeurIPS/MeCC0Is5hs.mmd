# A Modular Conditional Diffusion Framework for Image Reconstruction

Magauiya Zhussip

MTS AI

m.zhussip@mts.ai

Equal contribution

Iaroslav Koshelev

AI Foundation and Algorithm Lab

ys.koshelev@gmail.com

&Stamatis Lefkimmiatis

MTS AI

s.lefkimmiatis@mts.ai

Equal contributionWork performed while at AI Foundation and Algorithm Lab

###### Abstract

Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality. However, the task-specific nature of existing solutions and the excessive computational costs related to their training, make such models impractical and challenging to use for different IR tasks than those that were initially trained for. This hinders their wider adoption, especially by those who lack access to powerful computational resources and vast amount of training data. In this work we aim to address the above issues and enable the successful adoption of DPMs in practical IR-related applications. Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a relatively small module (0.7M params) related to the particular IR task of interest. Moreover, the architecture of the proposed framework allows for a sampling strategy that leads to at least four times reduction of neural function evaluations without suffering any performance loss, while it can also be combined with existing acceleration techniques such as DDIM. We evaluate our model on four benchmarks for the tasks of burst JDD-SR, dynamic scene deblurring, and super-resolution. Our method outperforms existing approaches in terms of perceptual quality while it retains a competitive performance with respect to fidelity metrics.

## 1 Introduction

With the advent of deep learning we have witnessed outstanding results in a wide range of computer vision tasks , including many challenging blind image restoration (IR) problems  such as burst imaging , super-resolution (SR) , deconvolution , _etc_. The standard approach for supervised learning in a blind IR setting involves training a feed-forward network that should estimate the latent image based on the available low-quality measurements. Such models are usually trained to maximize _fidelity_ metrics like PSNR or SSIM, but the visual quality of the resulting images is sub-optimal . The inclusion of perceptual losses  to the objective can improve the visual results, but fails to convincingly address the problem.

A promising direction towards IR results of high visual quality is to consider such problems within a generative framework. Several generative models have been recently proposed including Variational Autoencoders (VAEs) , Generative Adversarial Neural Networks (GANs) , Normalizing Flows (NFs)  and Diffusion Probabilistic Models (DPMs) . Due to their impressive results in image generation, they have been further utilized to perform _conditional sampling_ of high-quality images, with their low-quality or distorted counterparts playing the role of the conditionalinput [21; 38; 46; 39]. To date, DPMs appear to be the most promising framework and lead to the best results among all existing generative approaches.

Nevertheless, there are certain limitations that existing DPMs face, which hinder their wider adoption in IR tasks. In particular, inference of such models involves a sampling process that requires a large number (in the order of hundreds) of neural function evaluations (NFEs), which can be computationally very expensive, especially when considering images of high resolution. Another important limitation is that an efficient conditioning on the image measurements has yet to be proposed for DPMs in order to make them applicable to a wider range of blind IR problems. Indeed, all of the existing methods aim to learn the parameters of a single input-conditioned network for a specific blind IR task. As a result, the trained model overfits on the distribution of the condition space, and the whole model has to be retrained if we need to employ it to a different reconstruction task than the one that was initially trained for. Considering the huge amount of data and computational resources required for training a single DPM (see appendix A), such re-training becomes infeasible if at least one of the previous requirements is not satisfied.

In this work we aim to address the above issues by proposing a novel conditional diffusion network coupled with an accelerated sampling process. Specifically, our network adopts an improved conditioning strategy and is built on the foundation of existing off-the-shelf IR networks paired with a denoising module, which is applicable to a variety of reconstruction problems without requiring any re-training. Additionally, we introduce an accelerated sampling procedure that is enabled by our proposed network architecture and allows the merging of a large number of sampling steps in a single one, computed with a single NFE. Our proposed acceleration can work in tandem with accelerated sampling schemes such as DDIM . To assess the performance of our network, we validate it on three challenging blind IR tasks, namely, burst joint demosaicking, denoising and super-resolution (JDD-SR), dynamic scene deblurring, and \(4\) single image super-resolution (SISR). In all of the tested scenarios, our approach demonstrates the best perception-distortion trade-off among the state-of-the-art (SOTA) methods, while compared to other DPM-based solutions it requires a smaller number of sampling steps.

## 2 Related Works

Burst Image Restoration.One of the pioneering works in multi-frame IR was introduced in , where a frequency-domain-based solution was proposed. Then, several MAP models with various regularization terms have been designed to cope with visual artifacts caused by operating in the frequency domain [3; 18; 63]. Using the same MAP framework, a JDD-SR method robust to noise and outliers was developed in . Meanwhile, the block matching alignment algorithm of  was extended by  to obtain a robust motion model with the aid of an adaptive kernel interpolation method merging sparse pixel samples.

Advancements in deep learning have led to high-performing methods such as those in [17; 37; 4; 5; 49; 41]. The DBSR approach  aligns multiple input frames in the feature space utilizing an optical flow estimator (_e.g_. PWCNet ) and employs an attention-based fusion mechanism to aggregate features. In  a differentiable image registration module has been introduced, which exploits the aliasing effects appearing in bursts of low-resolution (LR) images. In  KBNet estimates blur kernels for a burst sequence to incorporate them with LR features so as to generate a better super-resolved image, while in  BIPNet attempts to fuse complementary information from the burst sequence with the help of generated pseudo-burst features. Another line of work effectively employs deformable convolutions for the inter-frame alignment task [49; 74; 17] and achieves SOTA results in various tasks, including burst SR.

Single Image Restoration.Among the recent IR methods, the most successful ones are those that adopt an end-to-end supervised formulation, where a deep neural network is trained to directly map a low-quality and degraded image to a point estimate of the latent high-quality image [88; 45; 69; 83]. Consequently, in the pursuit of further improving the reconstructions and achieving a better pixel-level result, more advanced network architectures have been proposed [82; 42; 10; 34], at the cost of being more computationally heavy. While this formulation leads to SOTA fidelity (PSNR, SSIM), the produced output is an average/median of all plausible predictions, which typically lacks high-frequency information (texture).

Generative adversarial networks (GAN)  have been adopted by several IR methods such as SISR [38; 73; 75] and dynamic scene deblurring [35; 36; 85] to produce more natural and perceptually pleasing results. Although this adversarial non-reference formulation aims to push the predictions towards the manifold of natural images, it is also prone to introducing unrealistic texture and hallucinations in the output . Moreover, the adversarial training process requires extra supervision as it can easily fall into a mode collapse or may diverge [2; 62].

Likelihood-based deep generative models such as NFs [47; 46], auto-regressive models , and VAEs  have also been applied to IR tasks, where one can obtain a diverse set of predictions from a learned posterior . Conditioned on LR inputs, flow-based methods attempt to map high-resolution (HR) images to the latent flow-space. Although such techniques circumvent the training instability met in GANs, strong architectural constraints (network invertibility) still remain an issue.

Recently another class of methods based on a stochastic diffusion process has been introduced and demonstrated outstanding performance on various tasks that range from unconditional image generation [26; 55; 60] to image-to-image translation/restoration [39; 61; 78; 20; 60; 15; 76; 59]. DvSR proposed in  employs a "predict-and-refine" conditional diffusion method specifically tailored for the image deblurring task, while SRDiff  utilizes features of a pretrained SR model for conditional super-resolved image generation. Further, recent works in [76; 80] have considered several IR tasks (inpainting, super-resolution, colorization, etc.). In conclusion, their ability to capture complex statistics of the visual world, makes DPMs a very attractive solution that is worth being further investigated.

## 3 Proposed Conditional Diffusion Model

### Background

Denoising Diffusion Probabilistic Models (DDPMs) [26; 65] are special cases of Hierarchical Markovian Variational Autoencoders where the dimension of the latent variables matches the dimension of the data. Starting with a sample \(_{0}^{N}\), the encoding sequence \(\{_{t}\}_{t=0}^{T}\) traverses the latent space with a _diffusion process_ defined by a Gaussian transition probability:

\[q(_{t}|_{t-1})(_{t}; {1-_{t}}_{t-1},_{t}_{N}).\] (1)

The sequence \(0<_{1},_{2},,_{T}<1\) that appears in eq. (1) defines the noise scheduling for the forward process in such a way so that the latent variable at the final timestep \(T\) approximates the standard Gaussian: \(_{T}(_{T};,_{N})\). Based on this diffusion process, it is possible to express the transition probability directly from \(_{0}\) to \(_{t}\) in closed form as:

\[q(_{t}|_{0})=(_{t};_{t}}_{0},(1-_{t})_{N}),\] (2)

where \(_{t} 1-_{t}\) and \(_{t}_{s=1}^{t}_{s}\).

The _reverse process_ is enabled by the posterior distribution which is represented in the form:

\[p(_{t-1}|_{t},_{0})=(_{t-1 };_{t}(_{t},_{0}),_{t}^{2}_{N} ),\] (3)where \(_{t}(_{t},_{0})-_{ t}}}{1-_{t}}_{0}+(1-_{t-1})}}{1- _{t}}_{t}\) and \(_{t}^{2}_{t-1}}{1-_{t}}_{t}\). DDPMs aim to approximate its mean by the quantity \(}(_{t},t)\), which is learned from training data, and then utilize eq.3 to perform sampling. There are different possible parameterizations of \(}(_{t},_{0})\), which accordingly lead to different interpretations for the transition mean . In this work, we pursue the one based on the score function \( p(_{t})\), which reads as:

\[_{t}(_{t},_{0})=_{t}+(1- _{t}) p(_{t})}{}}.\] (4)

In this case, the reverse process defined in eq.3 can be considered as sampling via Annealed Langevin Dynamics, in which the score function is approximated by the quantity \(}(_{t},t)\) learned via denoising score matching [28; 72].

### Conditional Score Matching

The diffusion models described above do not take into account the dependence of the sampled data on their degraded observations \(\!\!^{M}\), when we are dealing with IR problems. Fortunately, the score-based models can be extended to accommodate conditional sampling by replacing the score function in eq.4 with a conditional score function \(_{_{t}} p(_{t}|)\). For non-blind IR problems, a popular approach is to decompose the conditional score function into a score function \( p(_{t})\) and a log-likelihood gradient term \(_{_{t}} p(|_{t})\)[54; 67]. This last term is directly dependent on the image formation model, which unfortunately is unknown for blind IR tasks. Therefore, most of the existing works [39; 59; 60] aim instead to learn the primal conditional score function \(_{_{t}} p(_{t}|)\) via _ad-hoc_ conditional denoising score matching. In this work, we also utilize the primal conditional score function, but we rely on its explicit form as given in the following lemma, whose proof is provided in the appendixB.

**Lemma 3.1**.: _Let \(\!\!^{M}\), \(_{0}\!\!^{N} p(_{0}|)\), and \(_{t}\!\!^{N}\), \(_{t}\!\!\) are defined as in eq.2. Then, the conditional score function is computed as:_

\[_{_{t}} p(_{t}|)=_{t}}}[_{0}|,_{t}] -_{t}}{1-_{t}}.\] (5)

The above result implies that the conditional score function can be approximated by utilizing a trained joint reconstruction and denoising model. Specifically, if the augmented variable \(_{t}=[^{}_{t}^{}]^{ }\) represents the union of the degraded data \(\) and the noisy data \(_{t}\), then the conditional expected value \(}[_{0}|_{t}]\) corresponds to the reconstructed underlying image \(_{0}\) from the measurements \(_{t}\). A joint reconstruction model \(_{}(,_{t},t)\) can be trained by the minimization of the empirical expected pixel mean-squared error (MSE) across the samples from the training dataset \((_{0},,_{t},t)\):

\[_{}}_{_{0},_{t}, }}(,_{t},t)-_{0}_{2}^{2}=_{}_{i}} (^{i},_{t}^{i},t)-_{0}^{i}_{2}^{2}.\] (6)

The optimal solution is the conditional expectation \(_{}^{}(,_{t},t)=}[_{0}|,_{t}]\), and, thus, such a trained model can be substituted in eq.5. This amounts to approximating the conditional score function \(_{_{t}} p(_{t}|)\) with \(_{}^{c}(,_{t},t)_{t}}_{}(,_{t},t)-_{t}}{1- _{t}}\).

### Proposed Network Architecture

Our objective is to parameterize the function \(_{}(,_{t},t)\) in a form of a neural network (CNN) and design a specific architecture of this network. The absence of explicit knowledge about the formation model \(_{0}\) requires the network to learn it implicitly from training data. Such an approach generally results in over-fitting, meaning that the trained model can only be employed for the task it was originally trained for [39; 78]. To overcome this problem, we initially build on the hypothesis that the conditional expectation \(}[_{0}|,_{t}]\) related to the conditional score function in eq.5, can be approximated by a function of two easier to compute conditional expectations, that is

\[}[_{0}|,_{t}] f (}[_{0}|],}[_{0}|_{t}]).\] (7)

Based on such an approximation, it is now possible to learn a single unconditional generative denoising model that can be applied in different reconstruction problems. Further, we note that despite the absence of a good approximation of the likelihood term, \(}[_{0}|]\), various task-specific networks trained with a fidelity objective are readily available in the literature. Indeed, using a similar reasoning as the one provided for eq. (6), such reconstruction networks can output a good approximation of the quantity \([_{0}|]\). This finally motivates us to express the joint reconstruction and denoising network \(_{}(,_{t},t)\) into three components (see Figure 1). Specifically, our network can be described as \(_{_{F}}^{F}(_{_{I_{R}}^{IR}}^{IR} (,_{_{D}}^{D}(}_{t},_{t}),t)\), where \(}_{t}_{t}}{_{t}}}(_{0},_{t}^{2}_{N})\) is the noisy version of \(_{0}\) with noise variance \(_{t}^{2}_{t}}{_{t}}\) according to eq. (2), and the sub-modules \(_{_{D}}^{D},_{_{I_{R}}}^{IR},_{_{F}}^{F}\) are defined next.

**IR network \(_{_{IR}}^{IR}()\)**, which is learned in a supervised manner to predict \([_{0}|]\). Specifically, we employ the BST-Small  for burst JDD-SR, FFTformer  for dynamic scene deblurring, and SwinIR  for SISR. We do not train these networks but use their publicly available trained weights.

**Denoising network \(_{_{D}}^{D}(}_{t},_{t})\)**, which is learned in a supervised manner to predict \([_{0}|_{t}]\) by denoising \(}_{t}\). Specifically, we employ a smaller version of MIRNet , which we call MIRNet-S. It is obtained by reducing the amount of RRG and MRB blocks from the original architecture to three and one, respectively. We refer to the original paper  for the detailed description of the blocks structure, as we use them without any modifications. Once trained, this network is reused for all considered reconstruction problems. We note that we our motivation for utilizing a smaller version of MIRNet as a Denoising module, is to approximately match the number of parameters and the computational complexity of the networks used in our framework with those of the alternative methods under study. This way we can ensure a fair evaluation and comparison among competing methods. Such strategy has allowed us to achieve direct performance comparisons under similar conditions.

**Fusion network \(_{_{F}}^{F}(_{0}^{IR},_{0}^{D},t)\)**, which predicts the conditional expectation \([_{0}|,_{t}]\). This module refines and combines the predictions of the previous two networks and is the only one that needs to be trained for each specific IR task. The fusion network accepts as inputs the image estimates \(_{0}^{IR},_{0}^{D}\) and a timestep \(t\). Its architecture consists of two branches. The first one involves a convolution layer with \(n_{f}\) output channels followed by a single dense block  without batch normalization. Its purpose is to independently encode both input images into the corresponding features \(_{1},_{2}\) with \(n_{f}\) channels each. The second branch encodes the timestep \(t\) into a vector of weights \((0,1)^{n_{f}}\) using the sinusoidal positional encoding , followed by a two layer perceptron and a sigmoid function as the final activation. The features \(_{1},_{2}\) and the weights \(\) are then passed to the Convex Combination Channel Attention (3CA) layer, which performs the per-channel aggregation of input features as a convex combination of the form: \(_{1}+(-)_{2}\). The output of this layer is decoded by two consequent dense blocks with \(n_{f}\) channels each, followed by a convolution layer which produces the final output \([_{0}|,_{t}]\). This proposed architecture results in a significantly smaller network size than those of the Denoising and IR modules. Thus we can train the fusion network fast and by using only a small amount of problem-specific training data. While we explored several basic fusion architectures, we did not delve into extensive research to ascertain the optimal design. Our proposed fusion module serves as a proof of concept, validating our framework and demonstrating its potential for performance enhancement. A comprehensive investigation into optimal fusion architectures remains a promising area for future research.

Such a modular overall architecture allows us to capitalize on the existing SOTA non-blind denoising and blind IR networks, while it also allows us to easily replace any of these networks when better ones become available in the future. As we describe next, another important advantage of our proposed pipeline, is that it allows us to achieve a significant acceleration for the sampling process without incurring any loss of reconstruction quality.

### Proposed Accelerated Sampling

According to eq. (3), our conditional denoiser should be evaluated for all timesteps \(t=\), which leads to a total of \(T\) NFEs. We note that by construction, for the forward process it holds that \(_{T}(,_{N})\). This means that in the beginning of sampling, the latent variable \(_{T}\) does not contain any information about \(_{0}\). It is also reasonable to expect that a similar lack of information about \(_{0}\) exists for a number of steps prior to \(T\). Specifically, for those steps we expect that the quantity \([_{0}|,_{t}]\) is heavily influenced by \([_{0}|]\), while the contribution of \([_{0}|_{t}]\) is not significant enough. A theoretical justification for this argument is provided in appendix C. Based on the above reasoning, we select a timestep \(\) such that for the first \(T-\) reverse steps we use the following approximation: \([_{0}|,_{t}][_{0}|]=_{_{I_{R}}}^{IR}()\). This is achieved by disabling the lower branch of our proposed conditional score matching network, namely the Denoising \(_{_{D}}^{D}(}_{t},_{t})\) and Fusion \(_{_{F}}^{F}(_{0}^{IR},_{0}^{D})\) modules (Figure 1). Our strategy can be further supported by the recent study in , where it has been demonstrated that the image sampling via DPMs could be divided into stages depending on the reverse process timesteps. In this spirit we activate the Denoising and Fusion modules at a timestep \(\) that is selected experimentally for the particular IR task of interest. Our results clearly indicate that the reconstruction result is going to be exactly the same whether we utilize the multi-step reverse diffusion process or the proposed one-step process that is described in Lemma 3.2. Indeed, since the quantity \([_{0}|]\) is predicted by the IR network, which does not depend on the reverse diffusion parameters, \(_{_{IR}}^{IR}()\) needs to be evaluated only once and its output can be re-used throughout the whole iterative sampling procedure. Expanding more on this idea, we show in Lemma 3.2 that it is possible to omit entirely the first \(T-\) reverse diffusion steps and instead perform a single step directly from \(T\) to \(\) with a procedure very similar to the one obtained for the diffusion process in eq. (2) from eq. (1). We provide the derivation in appendix D.

**Lemma 3.2**.: _The transition probability defined in eq. (3) for a single reverse step, can be extended to \(k\) reverse steps starting from \(_{t}\) as:_

\[p(_{t-k}|_{t},_{0})=(_{t-k};_{ t,k}(_{t},_{0}),_{t,k}^{2}_{N}),\] (8)

_where_

\[_{t,k}(_{t},_{0})=_{i=0}^{k-1}_{t-i- 1}^{t-k+1}_{t-i}_{0}+_{t}^{t-k+1}_{t}\;\;\; \;_{t,k}^{2}=_{i=0}^{k-1}(_{t-i-1}^{t-k+1})^{2} _{t-i}^{2}.\] (9)

_In the above equations we make use of the following notation:_

\[_{t}=_{t-1}}_{t}}{1-_{t}}\;\; \;\;_{i}^{j}^{i}_{n }}^{1-_{j-1}}&i j\\ 1&i<j\] (10)

Since for the first \(T-\) steps \(_{0}\) is approximated by the quantity \([_{0}|]\), which is independent of the timestep \(t\), we utilize eq. (8) to directly sample \(_{}\) as

\[_{} p(_{}|_{T},[_{0}| ])=(_{};_{T,T-}( _{T},[_{0}|]),_{T,T-}^{ 2}).\] (11)

This allows us to reduce the NFEs from \(T+1\) to \(+1\), meaning that the required evaluations of Denoising + Fusion networks is reduced from \(T\) to \(\). In both cases, we additionally count a single evaluation of the IR network. We depict our acceleration strategy with a red arrow in Figure 2. Finally, in practice we use \(=\) for burst JDD-SR and dynamic scene deblurring, and \(=\) for SISR, effectively reducing the NFEs by two orders of magnitude and a factor of four, respectively.

The proposed acceleration procedure can be also interpreted as starting the sampling from step \(\) of the latent space using a non-standard Gaussian distribution as defined in eq. (11), instead of starting from step \(T\) and using a standard Gaussian sample \(_{T}(,_{N})\). We note, that a similar idea was explored in [13; 52], where it was proposed to start the sampling from an observation that has been passed through a predefined number of forward diffusion steps. In our case the starting point for sampling is obtained via the approximated reverse process, which as a consequence of Lemma 3.2 does not alter the final reconstruction result. In another words, if we approximate \(_{0}\) with \([_{0}|]\), then the reconstruction result will be the same both for the multi-step reverse diffusion process and

   Methods & PSNR\({}^{+}\) & SSIM\({}^{+}\) & LPIPS\({}_{-}\) & TOPHQ\({}_{-}\) & NEE \({}_{-}\) & Params\({}^{+}\) \\  Target & \(\) & 1 & 0 & 0 & N/A & N/A \\  DBSR & 31.98 & 0.891 & 0.198 & 0.10 & N/A & 13.0M \\ DeepRep & 34.66 & 0.927 & 0.136 & 0.07 & N/A & 12.1M \\ EBSR & 36.05 & 0.940 & 0.111 & 0.15 & N/A & 26.0M \\ BPNet & 34.86 & 0.934 & 0.112 & 0.03 & N/A & 6.7M \\ BSRT-Small & 35.91 & 0.940 & 0.109 & 0.12 & N/A & 4.9M \\ BSRT-Large & 36.98 & 0.947 & 0.095 & 0.16 & N/A & 20.7M \\  Ours & 35.53 & 0.933 & 0.084 & 0.02 & 6 & 21.6M \\   

Table 1: Performance evaluation on the task of Burst JDD-SR. We highlight the overall \(}\) for each metric.

Figure 2: Forward and reverse diffusion process. Blue solid arrows: transitions at the forward pass with sampling distribution from eq. (1). Dashed arrow: cumulative transition probability from eq. (2). Black solid arrows: transitions at the backward pass with the sampling distribution from eq. (3). Red solid arrow: closed-form cumulative transition probability from eq. (8) representing our accelerated sampling.

the proposed one-step process. Moreover, it is easy to show that our strategy generalizes the one proposed in [13; 52], as it leads to the same starting point if we make the following specific choices: \(_{T}(_{T}}[_{0} |],(1-_{T})\,)\) and \(_{t}^{2}=_{t-1}}{1-_{t}}_{t}\). In practice \(_{T} 0\), so the first condition holds almost exactly. The second condition represents the particular choice of the noise variance used in the reverse process, with several existing parametrizations [26; 55]. Our method is compatible with all of them and leads to different distributions for the starting point. In all our experiments we use the parametrization \(_{t}^{2}=_{t}\) from . More detailed conceptual and technical differences along with experimental results are provided in appendix E. Furthermore, our acceleration strategy is complimentary to other sampling acceleration techniques [66; 31]. To demonstrate this, we utilize the DDIM  sampling to further reduce the NFEs by a factor of five for the SISR task. As a result, the reverse diffusion process used for this problem requires \(+1\) NFEs in total.

## 4 Experimental Results

We evaluate our method on four public datasets across a range of tasks, namely burst JDD-SR, dynamic scene deblurring, and SISR. Below we describe our specific architecture and design choices related to all utilized modules.

Training.Our training procedure consists of two stages. We first employ a diverse, yet small DF2K (combination of DIV2K  and Flickr2K ) dataset to train a Denoising Module for Gaussian denoising in the sRGB domain with input noise levels ranging in \([0,244.3]\). These noise levels corresponds to timesteps in the range of \(\) for the diffusion process with \(T=1000\). We use the original training procedure of MIRNet  to learn the parameters of our MIRNet-S architecture. At the second stage we train our Fusion Modules with \(n_{f}=64\) for each IR task and the corresponding pre-trained off-the-shelf IR network. It is worth noting, that at this stage the parameters of Denoising and IR modules are kept frozen and only the Fusion Module is trained. Specifically, we train it for \(300k\) iterations with a learning rate of \(10^{-4} 0.99\) it/1000, batch size of 128, and crop size of \(256 256\). To train our Fusion networks we use datasets that are common among our main competitors, specifically the ZurichRAW2RGB  dataset for burst JDD-SR, GoPro  for dynamic scene deblurring and DIV2K  for \(4\) SISR. For burst JDD-SR the Fusion network is trained in the sRGB domain. All the networks are trained using the Ascend 910 AI accelerators . To make our results reproducible, we provide a full description of the training procedure in appendix H.

Inference.For each IR task we use the procedure described in section 3.4 to obtain the reconstructed images with \(T=1000\). To demonstrate the effectiveness of our approach, for each problem of interest except for burst JDD-SR we need half of the NFEs compared to the diffusion-based competitor that uses the least number of sampling steps. Specifically, for dynamic scene deblurring we use \(=5\), resulting in \(200\) acceleration achieved solely by our proposed sampling strategy. This amounts to \(6\) NFEs when counting the additional IR network evaluation performed to skip the first \(T-=995\) steps using eq. (8). For SISR we select \(=250\), which results in \(4\) acceleration using our sampling procedure. In order to demonstrate how it can be complemented by other proposed acceleration strategies, for the final \(=250\) steps we achieve \(5\) step reduction by employing DDIM sampling . The combination of both acceleration strategies results in \(20\) step reduction and \(51\) NFEs overall. Applying the DDIM acceleration technique on top of our proposed one-step strategy leads to an insignificant quantitative/qualitative difference (see appendix G) compared to our original scheme. Since for the burst JDD-SR problem no diffusion-based methods have yet been proposed, we use the same setting as for the dynamic scene deblurring problem, as it requires the smallest NFEs. In all our experiments we use the linear scheduling of the diffusion process variances \(_{t}[2 10^{-2},10^{-4}]\) defined in eq. (1).

Evaluation.For the burst JDD-SR evaluation we use the SyntheticBurst test set , consisting of 300 synthetically pre-generated raw burst sequences. Each sequence contains 14 noisy raw LR images with handshake motion, whose corresponding targets have a resolution of \(320 320\). Since our networks are trained on sRGB images, the outputs of all methods are converted to the sRGB space prior to comparison. For dynamic scene deblurring we evaluate on the GoPro test  and HIDE  benchmarks, which contain 1111 and 2025 images of 720p resolution, respectively. For \(4\) SISR we use the DIV2K validation dataset  consisting of 100 images of 2K resolution.

For the quantitative evaluation of the reconstruction quality we rely on the widely used fidelity metrics PSNR and SSIM , and the reference-based perceptual metric LPIPS . Moreover, we also utilize the non-reference image quality assessment (NR-IQA) metric TOPIQ  and report the absolute distance between the output and the target scores, which we indicate as TOPIQ\({}_{}\).

### Results

Burst JDD-SR.In Table 1 we compare our proposed pipeline with existing methods, namely DBSR , DeepRep , and the current SOTA methods, namely BIPNet , BSRT-Small , BSRT-Large , and EBSR . Our method demonstrates SOTA performance across the perceptual metrics while maintaining competitive PSNR and SSIM scores compared to existing methods. Thus, our method reconstructs images that are closer to the target based on the human perception while maintaining a high level of fidelity. We refer to figures in the appendix M for a qualitative visual assessment. Furthermore, we notice an improvement in terms of visual quality and perceptual metrics compared to BSRT-Small, which we use as the IR module of choice in our framework. This indicates that our approach preserves the fidelity of the IR model outputs, while enhancing their perceptual quality by running few reverse diffusion steps. It is worth noting that for this case, where a burst of raw images serves as input, we use the exact same denoising network that was trained on sRGB images and which we later deploy to all considered single-input IR tasks. This highlights the generalization ability of our approach not only to different IR problems but also to different input formats. Note that AFCNet , LKR , and Burstormer  are not included in our comparisons due to the absence of a publicly available implementation (or trained network parameters). Finally, the comparison with SOTA EBSR and BSRT shows that our DP-IR reconstructions compares favorably in terms of visual quality, while not lacking significantly in terms of fidelity.

Dynamic Scene Deblurring.Table 2 show quantitative results on the GoPro  and HIDE  datasets, respectively. We compare our approach with the SOTA reconstruction-based methods: NAFNet , FFTFormer  and diffusion-based methods: DvSR , InDI , and icDPM . Our framework outperforms all competing methods across the perceptual metrics and demonstrates the best perception-distortion (P-D) trade-off among all perceptual-based methods. Moreover, our DP-IR uses twice less number of reverse steps (NFE=5) compared to the state-of-the-art InDI and still achieves better perceptual quality (e.g. LPIPS) and is more consistent with the ground-truth (+2.22dB

   &  &  \\  & PSNR & SSIM & LPIPS & LPIPS & LPIPS & PSNR & SSIM & LPIPS & LPIPS & LPIPS & LPIPS \\  Target & \(\) & 1 & 0 & 0 & \(\) & 1 & 0 & 0 & N/A & N/A \\  HINet & 32.77 & 0.960 & 0.088 & 0.033 & 30.33 & 0.932 & 0.120 & 0.044 & N/A & 88.6M \\ MPRNet & 32.66 & 0.959 & 0.089 & 0.027 & 39.06 & 0.939 & 0.114 & 0.059 & N/A & 20.1M \\ MIMO-UNet+ & 32.44 & 0.957 & 0.091 & 0.034 & 29.99 & 0.930 & 0.124 & 0.028 & N/A & 16.1M \\ NAFNet & 33.71 & 0.967 & 0.078 & 0.017 & 31.32 & 0.943 & 0.103 & 0.024 & N/A & 67.9M \\ Restormer & 32.90 & 0.961 & 0.084 & 0.018 & 31.20 & 0.942 & 0.109 & 0.048 & N/A & 26.1M \\ FFTFormer & 34.21 & 0.969 & 0.071 & 0.012 & 31.62 & 0.946 & 0.096 & 0.006 & N/A & 16.6M \\   \\  DeblurGAN2 & 29.08 & 0.918 & 0.117 & 0.025 & 27.51 & 0.885 & 0.159 & 0.065 & N/A & 60.9M \\ DvSR\({}^{}\) & 31.66 & 0.948 & 0.059 & - & 29.77 & 0.922 & 0.089 & - & 500 & 26.1M \\ icDPM\({}^{}\) & 31.19 & 0.943 & 0.057 & - & 29.14 & 0.910 & 0.088 & - & 500 & 52.0M \\ InDi\({}^{}\) & 31.49 & 0.946 & 0.058 & - & - & - & - & 10 & 27.7M \\ Ours & 33.72 & 0.963 & 0.053 & 0.011 & 31.32 & 0.937 & 0.087 & 0.002 & 6 & 33.2M \\  

Table 2: Performance evaluation on the GoPro and HIDE test sets for dynamic scene deblurring. \({}^{}\) indicates that public implementation is unavailable and the scores are copied from the authorsâ€™ paper. We highlight the overall best for each metric, and the best among perceptual-oriented methods.

Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.

in PSNR). Despite the fact that our fusion module is trained only on the GoPro dataset, the gains in perceptual quality do transfer over the HIDE test images, providing SOTA scores for the perceptual metrics. Also, among perceptual-oriented methods, DP-IR outperforms the closest competitor DvSR by 1.55dB in terms of PSNR. Visual comparisons of our method and the SOTA deblurring models: NAFNet, FFTFormer, DvSR, and InDI is depicted in Figure 3. From these results we observe that our model shows a noticeable improvement in perceptual quality. Moreover, we have performed a computational cost analysis for the diffusion-based models and significantly outperformed existing methods from \(\)2 to 100 times (see appendix F).

Super-Resolution.We compare our method with reconstruction-based [38; 73; 42; 11; 10], GAN-based [73; 87; 42], NF-based  models, and DPMs [60; 39; 15; 20]. Table 3 summarizes the quantitative results on the DIV2K validation set. Our solution produces the best fidelity scores among all six perceptual-based methods and the best TOPIQ perceptual metric among all competing methods. The visual comparison in Figure 4 reveals that our framework produces super-resolved images that exhibit more refined structures and fine-grained details. Additional examples for all reported IR tasks are provided in appendix M.

## 5 Ablation Studies

Modular Approach.One of the main benefits of our framework is its ability to capitalize on the performance of existing restoration networks at a relatively low additional computational cost. To showcase this, we conduct an experiment on the task of \(4\) SISR using two different denoising architectures, namely UDP  and MIRNet-S , and two IR architectures, namely RRDB  and SwinIR . UDP is trained with the same settings as MIRNet-S and the fusion module is retrained for each of the three cases. From Table 4, we observe that the same IR network combined with a better denoising module, leads to better fidelity (see PSNR, SSIM). A same trend, but for perceptual metrics is observed if one upgrades the IR module from RRDB to the SwinIR and keeps the same denoising module. This clearly indicates that one can achieve better results by employing either a

  } &  &  &  &  \\  Target & inf & 1 & 0 & 0 \\  UDP & RRDB & 27.93 & 0.777 & 0.149 & 0.006 \\ MIRNet-S RRDB & 28.12 & 0.795 & 0.150 & 0.014 \\ MIRNet-S SwinIR & 28.12 & 0.793 & 0.140 & 0.002 \\  

Table 4: Ablation on various fusion networks on DIV2K validation for \(4\) SR task

  } &  &  &  &  &  &  \\  Target & \(\) & 1 & 0 & 0 & 0 & N/A \\  SRResNet & 20.70 & 0.824 & 0.266 & 0.046 & NA & 1.5M \\ RRDB & 29.48 & 0.834 & 0.254 & 0.038 & NA & 1.76M \\ SwinIR & 29.63 & 0.837 & 0.248 & 0.030 & NA & 11.9M \\ LILF & 29.30 & 0.830 & 0.258 & 0.046 & NA & 22.3M \\ HAT & 29.75 & 0.840 & 0.245 & 0.035 & NA & 20.6M \\  } &  \\  ESRGAN & 26.64 & 0.758 & 0.115 & 0.014 & NA & 16.7M \\ HCFlow & 27.02 & 0.766 & 0.124 & 0.021 & NA & 23.2M \\ SwinIR-GAN & 24.88 & 0.734 & 0.222 & 0.115 & NA & 11.9M \\ LDM & 23.30 & 0.697 & 0.218 & 0.019 & 106 & 169.0M \\ SRDiff & 27.14 & 0.773 & 0.129 & 0.008 & 100 & 23.6M \\ InDI & 26.45 & 0.741 & 0.136 & 0.009 & 100 & 62.3M \\ IDM & 27.35 & 0.782 & 0.147 & 0.008 & 2000 & 116.6M \\ Ours & 28.12 & 0.793 & 0.140 & 0.002 & 51 & 28.5M \\  

Table 3: Performance evaluation on the DIV2K validation set for \(4\) SISR. We highlight the overall best for each metric, and the best among perceptual-oriented methods.

  } &  & 100 & 150 & 200 & 250 & 300 & 350 \\  PSNR \(\) & 28.77 & 28.46 & 28.24 & 28.06 & 27.93 & 27.81 & 27.71 \\ LIPPS\(\) & 0.170 & 0.161 & 0.155 & 0.152 & 0.149 & 0.147 & 0.146 \\  

Table 5: Ablation on various fusion networks on DIV2K validation for \(4\) SR task

Figure 4: Visual comparisons on the DIV2K validation set for the task of \(4\) bicubic super-resolution (best viewed by zooming in). Every output image is accompanied by its LPIPS value.

more powerful denoiser or IR module, without the need to fully retrain the entire score estimator as is the common practice followed in most of the existing methods (_e.g_. LDM, SRDiff, DvSR, _etc_. ).

Fusion Strategies.In this study we use the UDP denoiser and the RRDB network from Table 4 and study several different fusion approaches, namely the Time-dependent Weighted Averaging (TDWA), Learnable Discrete Wavelet Transform (L-DWT), our proposed Fusion network, and U-Net with time embeddings . TDWA consists of sinusoidal positional encoding followed by a three-layer MLP, which predicts the weights for the timestep \(t\). Those weights are then passed to the 3CA layer (section 3.3) together with the outputs from the denoiser and IR modules to perform the fusion in the image space. In contrast, L-DWT directly learns weights for each scale and channel of a 3-level Haar DWT . L-DWT has only 30 trainable parameters, which needs significantly less training time and data. Table 5 indicates that, as expected, a more powerful fusion module leads to better perceptual and reconstruction quality. Overall, we see that the proposed Fusion network shows a better balance between the reconstruction, visual quality and the computational cost.

Perception-Distortion Trade-off.By varying the timestep \(\), when the denoiser and fusion modules are activated, one can favor the perceptual quality over the reconstruction fidelity (see Table 6). Here, we use the same UDP denoiser and RRDB as the IR module and experiment on the DIV2K  validation for the task of \(4\) SISR. Table 6 shows that the denoiser can operate on \(>250\), which corresponds to a wider noise range than the one the denoiser is initially trained for. Furthermore, we observe the same perception-distortion trade-off for dynamic scene deblurring task (see Appendix Table 13)

LimitationsOur empirical findings highlight that the optimal selection of \(\) is intrinsically linked to the nature of the reconstruction problem, particularly the output quality of the IR Network. While we have experimentally identified optimal parameters for each test dataset in this study, we posit that a more refined approach would involve tailoring the acceleration parameters on an individual sample basis. However, the absence of a dependable methodology for assessing the quality of the IR and Denoising Networks' outputs at specific diffusion process timesteps - especially in the absence of ground truth data - constitutes a considerable challenge. This underscores a compelling avenue for future inquiry into adaptive optimization of acceleration parameters.

Furthermore, a notable constraint of our approach is its reliance on the efficacy of the employed Denoising and IR modules. As such, for novel image restoration tasks where a pre-trained IR network is unavailable, our framework might be inapplicable. Additionally, for imaging modalities (e.g. medical imaging) lacking a trained score-matching network (denoising module), it is imperative to either fine-tune an existing module or undertake comprehensive re-training with appropriate image datasets.

## 6 Conclusion

We present a modular conditional diffusion probabilistic framework for IR problems along with a sampling acceleration strategy that achieves a significant speed-up during the inference stage. Our framework achieves SOTA results both quantitatively and visually on the tasks of burst JDD-SR, dynamic scene deblurring, and \(4\) SISR without the need for re-training on a large pool of data and significant computational cost. This is mainly accomplished by utilizing pretrained models and only training a relatively small fusion module. While in this work we have not exhaustively considered all blind IR problems, we hope that our results can serve as a positive indication that the perceptual quality of the reconstructed outputs can improve significantly at the cost of only several additional NFEs, making possible a wider adoption of DPMs for IR applications even when there are tight requirements on computational complexity. Our ablation studies indicate that a variety of pretrained networks can be used with our method and further improvements on the results can be achieved by utilizing better denoising, IR, and fusion modules.