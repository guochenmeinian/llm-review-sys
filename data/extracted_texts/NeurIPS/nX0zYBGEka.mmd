# FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning

Jinyuan Jia

The Pennsylvania State University

jingyuan@psu.edu

&Zhuowen Yuan

UIUC

zhuowen3@illinois.edu

&Dinuka Sahabandu

University of Washington

sdinuka@uw.edu

&Luyao Niu

University of Washington

luyaoniu@uw.edu

&Arezoo Rajabi

University of Washington

rajabia@uw.edu

&Bhaskar Ramasubramanian

Western Washington University

ramasub@wwu.edu

&Bo Li

UIUC

lbo@illinois.edu

&Radha Poovendran

University of Washington

rp3@uw.edu

Equal contribution.

###### Abstract

Federated learning (FL) provides a distributed training paradigm where multiple clients can jointly train a global model without sharing their local data. However, recent studies have shown that FL offers an additional surface for backdoor attacks. For instance, an attacker can compromise a subset of clients and thus corrupt the global model to misclassify an input with a backdoor trigger as the adversarial target. Existing defenses for FL against backdoor attacks usually detect and exclude the corrupted information from the compromised clients based on a _static_ attacker model. However, such defenses are inadequate against _dynamic_ attackers who strategically adapt their attack strategies. To bridge this gap, we model the strategic interactions between the defender and dynamic attackers as a minimax game. Based on the analysis of the game, we design an interactive defense mechanism FedGame. We prove that under mild assumptions, the global model trained with FedGame under backdoor attacks is close to that trained without attacks. Empirically, we compare FedGame with multiple _state-of-the-art_ baselines on several benchmark datasets under various attacks. We show that FedGame can effectively defend against strategic attackers and achieves significantly higher robustness than baselines. Our code is available at: https://github.com/AI-secure/FedGame.

## 1 Introduction

Federated learning (FL) [21; 28] aims to train a global model over training data that are distributed across multiple clients (e.g., mobile phones, IoT devices) in an iterative manner. In each communication round, a cloud server shares its global model with selected clients. Then, each selected client uses the global model to initialize its local model, utilizes its local training dataset to train the local model, and sends the local model update to the server. Finally, the server uses an aggregation rule to integrate local model updates from clients to update its global model.

Due to the distributed nature of FL, many studies [3; 4; 5; 21; 44] have shown that FL is vulnerable to backdoor attacks [11; 18; 26; 27; 30; 33; 37; 40; 41; 55; 59]. For instance, an attacker can compromisea subset of clients and manipulate their local training datasets to corrupt the global model such that it predicts an attacker-chosen target class for any inputs embedded with a backdoor trigger . To defend against such backdoor attacks, many defenses [7; 32; 34; 36; 39; 51] have been proposed (see Section 2 for details). However, all those defenses consider a _static_ attack model where an attacker sticks to a fixed strategy and does not adapt its attack strategies. As a result, they are less effective under adaptive attacks, e.g., Wang et al.  showed that defenses in [6; 39] can be bypassed by appropriately designed attacks. While the vulnerability of the FL against dynamic or adaptive attack is known, dynamic defense has not been well-studied yet.

In this work, we propose FedGame, a game-theoretic defense against backdoor attacks in FL. Specifically, we formulate FedGame as a minimax game between the server (defender) and attacker, enabling them to adapt their defense and attack strategies strategically. While the server has no prior knowledge about which client is compromised, our key idea is that the server can compute a _genuine score_ for each client, which should be large (or small) if the client is benign (or compromised) in each communication round. The genuine score serves as a weight for the local model update of the client when used to update the global model. The goal of the defender is to minimize the genuine scores for compromised clients and maximize them for benign ones. To solve the resulting minimax game for the defender, we follow a three-step process consisting of 1) building an auxiliary global model, 2) exploiting the auxiliary global model to reverse engineer a backdoor trigger and target class, and 3) testing whether the local model of a client will predict an input embedded with the reverse engineered backdoor trigger as the target class to compute a genuine score for the client. Based on the deployed defense, the goal of the attacker is to optimize its attack strategy by maximizing the effectiveness of the backdoor attack while remaining stealthy. Our key observation is that the attack effectiveness is determined by two factors: the genuine score and the local model of the client. We optimize the attack strategy with respect to those two factors to maximize the effectiveness of backdoor attacks against dynamic defense.

We perform both theoretical analysis and empirical evaluations for FedGame. Theoretically, we prove that the global model trained with our defense under backdoor attacks is close to that trained without attacks (measured by Euclidean distance of model parameters). Empirically, we perform comprehensive evaluations. In particular, we evaluate FedGame on benchmark datasets to demonstrate its effectiveness under _state-of-the-art_ backdoor attacks [3; 54; 59]. Moreover, we compare it with six _state-of-the-art_ baselines [6; 7; 36; 39; 56]. Our results indicate that FedGame outperforms them by a significant margin (e.g., the attack success rate for FedGame is less than 13% while it is greater than 95% for those baselines on CIFAR10 under Scaling attack ). We also perform comprehensive ablation studies and evaluate FedGame against adaptive attacks. Our results show FedGame is consistently effective. Our key contributions are as follows:

* We propose FedGame, the first game-theoretic defense against dynamic backdoor attacks to FL.
* We provide theoretical guarantees of FedGame. We show that the global model trained with FedGame under backdoor attacks is close to that without attacks.
* We perform a systematic evaluation of FedGame on benchmark datasets and demonstrate that FedGame significantly outperforms state-of-the-art baselines.

## 2 Related Work

Backdoor Attacks in FL.In backdoor attacks against FL [3; 4; 5; 31; 44; 50; 59], an attacker aims to make a global model predict a target class for any input embedded with a backdoor trigger via compromised clients. For instance, Bagdasaryan et al.  proposed the scaling attack in which an attacker first uses a mix of backdoored and clean training examples to train its local model and then scales the local model update by a factor before sending it to the server. In our work, we will leverage _state-of-the-art_ attacks [3; 54; 59] to perform strategic backdoor attacks against our defense.

Defenses against Backdoor Attacks in FL.Many defenses [39; 7; 34; 51; 36; 32; 57; 9] were proposed to mitigate backdoor attacks in FL. For instance, Sun et al.  proposed norm-clipping, which clips the norm of the local model update of a client to a certain threshold. Some work extended differential privacy [15; 1; 29] to mitigate backdoor attacks to federated learning. The idea is to clip the local model update and add Gaussian noise. Cao et al.  proposed FLTrust, which leveraged the similarity of the local model update of a client with that computed by the server itself on its clean dataset. DeepSight  performs clustering on local models and then filters out those from compromised clients. By design, DeepSight is ineffective when the fraction of compromised clients is larger than 50%. Other defenses include Byzantine-robust FL methods such as Krum , Trimmed Mean , and Median . However, all of those defenses only consider a static attacker model. As a result, they become less effective against dynamic attackers who strategically adapt their attack strategies.

Another line of research focuses on detecting compromised clients [24; 58]. As those defenses need to collect many local model updates from clients to make confident detection, the global model may already be backdoored before those clients are detected. Some recent studies [8; 53; 57; 10] proposed certified defenses against compromised clients. However, they can only tolerate a moderate fraction of compromised clients (e.g., less than 10%) as shown in their results. In contrast, FedGame can tolerate a much higher fraction (e.g., 80%) of compromised clients.

In this work, we aim to prevent backdoor attacks to federated learning, i.e., our goal is to train a robust global model under compromised clients. The server could also utilize post-training defenses [43; 51] to remove the backdoor in a backdoored global model. Prevention-based defense could be combined with those post-training defenses to form a defense-in-depth. We note that the defense proposed in  requires the server to own some clean samples that have the same distribution as the local training data of clients. The defense proposed in  prunes filters based on pruning sequences collected from clients and thus is less effective when the fraction of compromised clients is large.

## 3 Background and Threat Model

Federated Learning.Let \(\) denote the set of clients and \(_{i}\) denote the local training dataset of client \(i\). In the \(t^{th}\) communication round, the server first sends the current global model \(^{t}\) to the selected clients. Then, each selected client \(i\) trains a local model (denoted as \(^{t}_{i}\)) by fine-tuning the global model \(^{t}\) using its local training dataset \(_{i}\). For simplicity, we use \(=(,y)\) to denote a training example in \(_{i}\), where \(\) is the training input (e.g., an image) and \(y\) is its ground truth label. Given \(_{i}\) and the global model \(^{t}\), the local objective is defined as \((_{i};^{t})=_{i}|}_{ _{i}}(;^{t})\) where \(\) is a loss function (e.g., cross-entropy loss). The client can use gradient descent to update its local model based on the global model and its local training dataset, i.e., \(^{t}_{i}=^{t}-_{l}(_{i}; ^{t})}{^{t}}\), where \(_{l}\) is the local learning rate. Note that stochastic gradient descent can also be used in practice. After that, the client sends \(g^{t}_{i}=^{t}_{i}-^{t}\) (called _local model update_) to the server. Note that it is equivalent for the client to send a local model or local model update to the server as \(^{t}_{i}=^{t}+g^{t}_{i}\). After receiving the local model updates from all clients, the server can aggregate them based on an aggregation rule \(\) (e.g., FedAvg ) to update its global model, i.e., we have \(^{t+1}=^{t}+(g^{t}_{1},g^{t}_{2},,g^{t}_{| |})\), where \(||\) represents the number of clients and \(\) is the learning rate of the global model.

Threat Model.We consider attacks proposed in previous work [3; 54; 59]. We assume an attacker can compromise a set of clients (denoted as \(_{a}\)). To perform backdoor attacks, the attacker first selects an arbitrary backdoor trigger \(\) and a target class \(y^{tc}\). For each client \(i_{a}\) in the \(t^{th}\) (\(t=1,2,\)) communication round, the attacker can choose an arbitrary fraction (denoted as \(r^{t}_{i}\)) of training examples from the local training dataset of the client, embed the backdoor trigger \(\) to training inputs, and relabel them as the target class \(y^{tc}\). In _state-of-the-art_ backdoor attacks [3; 54; 59], the attacker leverages those trigger-embedded training examples to inject the backdoor into the local model of a compromised client to attack the global model.

Following , we consider that the server itself has a small clean training dataset (denoted as \(_{s}\)), which could be collected from the same or different domains of the local training datasets of clients. Note that the server does not have 1) information on compromised clients, and 2) the poisoning ratio \(r^{t}_{i}\) (\(i_{a}\)), backdoor trigger \(\), and target class \(y^{tc}\) selected by the attacker. In our threat model, we do not make any other assumptions for the server compared with standard federated learning .

## 4 Methodology

Overview.We formulate FedGame as a minimax game between the defender and attacker, which enables them to optimize their strategies respectively. In particular, the defender computes a genuine score for each client in each communication round. The goal of the defender is to maximize the genuine score for a benign client and minimize it for a compromised one. Given the genuine score for each client, we use a weighted average over all the local model updates to update the global model, i.e., we have

\[^{t+1}=^{t}+}p_{i}^{ t}}_{i}p_{i}^{t}g_{i}^{t},\] (1)

where \(p_{i}^{t}\) is the genuine score for client \(i\) in the \(t^{th}\) communication round and \(\) is the learning rate of the global model. We compute a weighted average of local models because we aim to achieve robustness against attacks while making minimal changes to FedAvg , which has been shown to achieve high utility in diverse settings. In our experiments, we empirically show that the genuine scores are almost constantly 0 for malicious clients after a few communication rounds, which indicates that the malicious model updates will not be aggregated at all, reducing Equation 1 to a robust FedAvg against attacks. (See Figure 2(c)(d) in the Appendix for more details.) Based on Equation 1, the effectiveness of the attack is determined by two components: genuine scores and local models of compromised clients. In our framework, the attacker optimizes the tradeoff between these two components to maximize the effectiveness of backdoor attacks.

### Game Formulation

Computing Genuine Scores.The key challenge for the server to compute genuine scores for clients is that the server can only access their local model updates, i.e., the server can only leverage local models of clients to compute genuine scores. To tackle this issue, we observe that the local model of a compromised client is more likely to predict a trigger-embedded input as the target class. Therefore, the server can first reverse engineer the backdoor trigger \(_{re}\) and target class \(y_{re}^{tc}\) (which we will discuss more in the next subsection) and then use them to compute the genuine score for each client. Since the client \(i\) sends its local model update \(g_{i}^{t}\) to the server, the server can compute the local model of the client \(i\) as \(_{i}^{t}=^{t}+g_{i}^{t}\). With \(_{i}^{t}\), the server can compute \(p_{i}^{t}\) as follows:

\[p_{i}^{t}=1-_{s}|}_{ _{s}}(G(_{re};_{i}^{t})=y_{ re}^{tc}),\] (2)

where \(\) is an indicator function, \(_{s}\) is the clean training dataset of the server, \(_{re}\) is a trigger-embedded input, and \(G(_{re};_{i}^{t})\) represents the label of \(_{re}\) predicted by \(_{i}^{t}\). Intuitively, the genuine score for client \(i\) is small if a large fraction of inputs embedded with the reverse-engineered backdoor trigger is predicted as the target class by its local model. We note that the defender only needs to query local models to compute their genuine scores. Moreover, our defense is still applicable when differential privacy [49; 14] is used to protect the local models.

Optimization Problem of the Defender.The server aims to reverse engineer the backdoor trigger \(_{re}\) and target class \(y_{re}^{tc}\) such that the genuine scores for compromised clients are minimized while those for benign clients are maximized. Formally, we have the following optimization problem:

\[_{_{re},y_{re}^{tc}}(_{i_{a}}p_{i}^{ t}-_{j_{a}}p_{j}^{t}).\] (3)

Note that \(_{a}\) is the set of malicious clients that is unknown to the server. In the next subsection, we will discuss how to address this challenge to solve the optimization problem.

Optimization Problem of the Attack.The goal of an attacker is to maximize its attack effectiveness. Based on Equation 1, the attacker needs to: 1) maximize the genuine scores for compromised clients while minimizing them for benign ones, i.e., \((_{i_{a}}p_{i}^{t}-_{j_{a}}p_{j}^{t})\), and 2) make the local models of compromised clients predict an input embedded with the attacker-chosen backdoor trigger \(\) as the target class \(y^{tc}\). To perform the backdoor attack in the \(t^{}\) communication round, the attacker embeds the backdoor to a certain fraction (denoted as \(r_{i}^{t}\)) of training examples in the local training dataset of the client and uses them as data augmentation. Intuitively, a larger \(r_{i}^{t}\) encourages a higher attack success rate but can potentially result in a lower genuine score. In other words, \(r_{i}^{t}\) measures a tradeoff between these two factors. Formally, the attacker can solve the following optimization problem to find the desired tradeoff:

\[_{R^{t},}(_{i_{a}}p_{i}^{t}-_{j _{a}}p_{j}^{t}+_{i_{a}}r_{i}^{t}),\] (4)

where \(R^{t}=\{r_{i}^{t} i_{a}\}\), \(\) is the trigger, and \(\) is a hyperparameter to balance the two terms.

Minimax Game.Given the optimization problems of the defender and attacker, we have the following minimax game:

\[_{_{res},y_{res}^{tc}}_{R^{t},}(_{i_{a}}p _{i}^{t}-_{j_{a}}p_{j}^{t}+_ {i_{a}}r_{i}^{t}).\] (5)

Note that \(r_{i}^{t}\) (\(i_{a}\)) and \(\) are chosen by the attacker. Thus, we can add them to the objective function in Equation 3 without influencing its solution given the local model updates of clients. In Section A of Appendix, we provide an intuitive interpretation regarding how to connect our above objective to the ultimate goal of the defender and attacker (the defender aims to obtain a clean model, while the attacker wants the global model to be backdoored).

### Solving the Minimax Game by the Defender

The key challenge for the server to solve Equation 5 is that it does not know \(_{a}\) (set of compromised clients). To address the challenge, our idea is to construct an _auxiliary global model_ based on local models of all clients. Suppose \(g_{i}^{t}\) is the local model update sent by each client \(i\) to the server. Our auxiliary global model is constructed as follows: \(_{a}^{t}=^{t}+|}_{i}g_{i}^{t}\), where \(^{t}\) is the global model. Our intuition is that such a naively aggregated auxiliary global model is inclined to predict a trigger-embedded input as the target class under backdoor attacks. As a result, given the auxiliary global model, we can use an arbitrary existing method [42; 46; 38; 48] to reverse engineer the backdoor trigger and target class based on it, which enables us to compute genuine scores for clients based on Equation 2. With those genuine scores, we can use Equation 1 to update the global model to protect it from backdoor attacks in every communication round. The complete algorithm of our FedGame is shown in Algorithm 1 of Appendix.

Our framework is compatible with any trigger reverse engineering methods, allowing off-the-shelf incorporation of techniques developed for centralized supervised learning into federated learning. Note that developing a new reverse engineering method is not the focus of this work. Instead, our goal is to formulate the attack and defense against backdoor attacks on federated learning as a minimax game, which enables us to defend against dynamic attacks.

### Solving the Minimax Game by the Attacker

The goal of the attacker is to find \(r_{i}^{t}\) for each client \(i_{a}\) such that the loss function in Equation 5 is maximized. As the attacker does not know the genuine scores of benign clients, the attacker can find \(r_{i}^{t}\) to maximize \(p_{i}^{t}+ r_{i}^{t}\) for client \(i_{a}\) to approximately solve the optimization problem in Equation 5. However, the key challenge is that the attacker does not know the reverse engineered backdoor trigger \(_{re}\) and the target class \(y_{re}^{tc}\) of the defender to compute the genuine score for the client \(i\). In response, the attacker can use the backdoor trigger \(\) and target class \(y_{c}^{tc}\) chosen by itself. Moreover, the attacker reserves a certain fraction (e.g., 10%) of training data from its local training dataset \(_{i}\) as the validation dataset (denoted as \(_{i}^{rev}\)) to find the best \(r_{i}^{t}\).

Estimating the Genuine Score Given \(r_{i}^{t}\).For a given \(r_{i}^{t}\), the client \(i\) can embed the backdoor to \(r_{i}^{t}\) fraction of training examples in \(_{i}_{i}^{rev}\) and then use those backdoored training examples to augment \(_{i}_{i}^{rev}\) to train a local model (denoted as \(_{i}^{t}\)). Then, the genuine score can be estimated as \(_{i}^{t}=1-_{i}^{rev}|}_{ _{i}^{rev}}(G(;_{i}^ {t})=y^{tc})\), where \(G(;_{i}^{t})\) is the predicted label by the global model \(_{i}^{t}\) for the trigger-embedded input \(\).

Finding the Optimal \(r_{i}^{t}\).The client can use grid search to find \(r_{i}^{t}\) that achieves the largest \(_{i}^{t}+ r_{i}^{t}\). After estimating the optimal \(r_{i}^{t}\), client \(i\) can embed the backdoor to \(r_{i}^{t}\) fraction of training examples and utilize them to perform backdoor attacks based on state-of-the-art methods [3; 54; 59].

Trigger Optimization.The attacker can choose an arbitrary static trigger to launch backdoor attacks, or dynamically optimize it to make the attack more effective. Given the fixed location and bounding box of the backdoor trigger, the attacker can optimize the trigger pattern with gradient descent such that it is more likely for a backdoored input to be predicted as the target class. For instance, the attacker can optimize the trigger pattern to minimize the cumulative loss on all training data of malicious clients, i.e., \(^{t}=*{argmin}_{^{*}}_{_{i _{0}}_{i}}(^{t}(^{*}),y ^{tc})\), where \(^{t}(^{*})\) is the output of the model which is a probability simplex of all possible classes and \(\) is a standard loss function (e.g., cross-entropy loss). The attacker can then solve Equation 5 with the optimized trigger. Recall that the attacker aims to find the largest \(_{i}^{t}+ r_{i}^{t}\). Therefore, if the resulting \(_{i}^{t}+ r_{i}^{t}\) given \(^{t}\) is lower than that given the previous trigger \(^{t-1}\), we let \(^{t}=^{t-1}\).

The complete algorithm for the compromised clients is shown in Algorithm 2 in Appendix C.

## 5 Theoretical Analysis

This section provides a theoretical analysis of FedGame under backdoor attacks. Suppose the global model parameters are in a bounded space. We derive an upper bound for the \(L_{2}\)-norm of the difference between the parameters of the global models with and without attacks. To analyze the robustness of FedGame, we make the following assumptions, which are commonly used in the analysis of previous studies  on federated learning.

**Assumption 5.1**.: The loss function is \(\)-strongly convex with \(L\)-Lipschitz continuous gradient. Formally, we have the following for arbitrary \(\) and \(^{}\):

\[(_{}(;)-_{^{} }(;^{}))^{T}(-^{})\| -^{}\|_{2}^{2},\] (6) \[\|_{}(;)-_{^{ }}(;^{})\|_{2} L\|- ^{}\|_{2},\] (7)

where \(\) is an arbitrary training example.

**Assumption 5.2**.: We assume the gradient \(_{}(;)\) is bounded with respect to \(L_{2}\)-norm for arbitrary \(\) and \(\), i.e., there exists some \(M 0\) such that

\[\|_{}(;)\|_{2} M.\] (8)

Suppose \(_{c}^{t}\) is the global model trained by FedGame without any attacks in the \(t\)th communication round, i.e., each client \(i\) uses its clean local training dataset \(_{i}\) to train a local model. Moreover, we assume gradient descent is used by each client to train its local model. Suppose \(q_{i}^{t}\) is the genuine score for client \(i\) without attacks. Moreover, we denote \(_{i}^{t}=^{t}}{_{i}q_{i}^{t}}\) as the normalized genuine score for client \(i\). To perform the backdoor attack, we assume a compromised client \(i\) can embed the backdoor trigger to \(r_{i}^{t}\) fraction of training examples in the local training dataset of the client and relabel them as the target class. Those backdoored training examples are used to augment the local training dataset of the client. Suppose \(^{t}\) is the global model under the backdoor attack in the \(t\)th communication round with our defense. We denote \(_{i}^{t}=^{t}}{_{i}p_{i}^{t}}\) as the normalized genuine score for client \(i\) with attacks in the \(t\)th communication round. Formally, we have:

**Lemma 5.3** (Robustness Guarantee for One Communication Round).: _Suppose Assumptions 5.1 and 5.2 hold. Moreover, we assume \((1-r^{t})_{i}^{t}_{i}^{t}(1+r^{t})_{i}^{t}\), where \(i\) and \(r^{t}=_{j_{a}}r_{j}^{t}\). Then, we have:_

\[\|^{t+1}-_{c}^{t+1}\|_{2}\] (9) \[ +^{2}L^{2}+2^{2}L^{t} }\|^{t}-_{c}^{t}\|_{2}+(1+ L+ 2^{t})}+2 r^{t}M,\] (10)

_where \(\) is the learning rate of the global model, \(L\) and \(\) are defined in Assumption 5.1, \(^{t}=_{i_{a}}_{i}^{t}r_{i}^{t}M\), and \(M\) is defined in Assumption 5.2._

Proof sketch.: Our idea is to decompose \(\|^{t+1}-_{c}^{t+1}\|_{2}\) into two terms. Then, we derive an upper bound for each term based on the change of the local model updates of clients under backdoor attacks and the properties of the loss function. As a result, our derived upper bound relies on \(r_{i}^{t}\) for each client \(i_{a}\), parameters \(\), \(L\), and \(M\) in our assumptions, as well as the parameter difference of the global models in the previous iteration, i.e., \(\|^{t}-_{c}^{t}\|_{2}\). Our complete proof is in Appendix B.1.

In the above lemma, we derive an upper bound of \(\|^{t+1}-_{c}^{t+1}\|_{2}\) with respect to \(\|^{t}-_{c}^{t}\|_{2}\) for one communication round. In the next theorem, we derive an upper bound of \(\|^{t}-_{c}^{t}\|_{2}\) as \(t\). We iterative apply Lemma 5.3 for successive values of \(t\) and have the following theorem:

**Theorem 5.4** (Robustness Guarantee).: _Suppose Assumptions 5.1 and 5.2 hold. Moreover, we assume \((1-r^{t})_{i}^{t}_{i}^{t}(1+r^{t})_{i}^{t}\) for \(i\), \(^{t}\) and \(r^{t} r\) hold for all communication round \(t\), and \(>2\), where \(r^{t}=_{j_{a}}r_{j}^{t}\) and \(^{t}=_{i_{a}}_{i}^{t}r_{i}^{t}M\). Let the global model learning rate by chosen as \(0<<+2L}\). Then, we have:_

\[\|^{t}-_{c}^{t}\|_{2}+2 rM}{1-L^{2}+2 ^{2}L}}\] (11)

_holds as \(t\)._

Proof sketch.: Given the conditions that \(^{t}\) and \(r^{t} r\) as well as the fact that the right-hand side of Equation 10 is monotonic with respect to \(^{t}\) and \(r^{t}\), we can replace \(^{t}\) and \(r^{t}\) in Equation 10 with \(\) and \(r\). Then, we iterative apply the equation for successive values of \(t\). When \(0<<+2L}\), we have \(0<1-+2+^{2}L^{2}+2^{2}L<1\). By letting \(t\), we can reach the conclusion. The complete proof can be found in Appendix B.2. 

Our theorem implies that the global model parameters under our defense against adaptive attacks do not deviate too much from those of the global model without attack when the fraction of backdoored training examples \(r^{t}\) is bounded.

## 6 Experiments

In order to thoroughly evaluate the effectiveness of FedGame, we conduct comprehensive experiments on 1) evaluation against three _state-of-the-art_ backdoor attacks: Scaling attack , DBA , and Neurotoxin , 2) comparison with six existing baselines including Krum, Median, Norm-Clipping, Differential Privacy, DeepSight, FLTrust, 3) evaluation against strong adaptive attacks, and 4) comprehensive ablation studies.

### Experimental Setup

Datasets and Models.We use two benchmark datasets: MNIST  and CIFAR10  for FL tasks. MNIST has 60,000 training and 10,000 testing images, each of which has a size of 28 \(\) 28 belonging to one of 10 classes. CIFAR10 consists of 50,000 training and 10,000 testing images with a size of 32 \(\) 32. Each image is categorized into one of 10 classes. For each dataset, we randomly sample 90% of training data for clients, and the remaining 10% of training data is reserved to evaluate our defense when the clean training dataset of the server is from the same domain as those of clients. We use a CNN with two convolution layers (detailed architecture can be found in Table 2 in Appendix) and ResNet-18  which is pre-trained on ImageNet  as the global models for MNIST and CIFAR10.

FL Settings.We consider two settings: local training data are independently and identically distributed (IID) among clients, and non-IID. We follow the previous work  to distribute training data to clients by using a parameter \(q\) to control the degree of non-IID, which models the probability that training images from one category are distributed to a particular client (or a set of clients). We set \(q=0.5\) by following . Moreover, we train a global model based on 10 clients for 200 iterations with a global learning rate \(=1.0\). In each communication round, we use SGD to train the local model of each client for two epochs with a local learning rate of 0.01.

Attack Settings.We consider three _state-of-the-art_ backdoor attacks on federated learning, i.e., Scaling attack , DBA , and Neurotoxin . For Scaling attack, we set the scaling parameter to be #total clients/(\(\)#compromised clients) by following . For Neurotoxin, we set the ratio of masked gradients to be 1%, following the choice in . We use the same backdoor trigger and target class as used in those works. By default, we assume 60% of clients are compromised by an attacker. When the attacker solves the minimax game in Equation 5, we set the default \(=1\). We explore the impact of \(\) in our experiments. We randomly sample 10% of the local data of each compromised client as validation data to search for an optimal \(r_{i}^{t}\). Moreover, we set the granularity of grid search to 0.1 when searching for \(r_{i}^{t}\).

Baselines.We compare our defense with the following methods: FedAvg , Krum , Median , Norm-Clipping , Differential Privacy (DP) , DeepSight , and FLTrust . Please see Appendix D.2 for parameter settings for those baselines.

Evaluation Metrics.We use _testing accuracy (TA)_ and _attack success rate (ASR)_ as evaluation metrics. Concretely, TA is the fraction of clean testing inputs that are correctly predicted, and ASR refers to the fraction of backdoored testing inputs that are predicted as the target class.

Defense Settings.We consider two settings: in-domain and out-of-domain. For the in-domain setting, we consider the clean training dataset of the server is from the same domain as the local training datasets of clients. We use the reserved data as the clean training dataset of the server for each dataset. For the out-of-domain setting, we consider the server has a clean training dataset that is from a different domain than the FL task. In particular, we randomly sample 6,000 images from FashionMNIST  for MNIST and sample 5,000 images from GTSRB  for CIFAR10 as the clean training dataset of the server. Unless otherwise mentioned, we adopt Neural Cleanse  to reverse engineer the backdoor trigger and target class.

### Experimental Results

We show the results of FedGame compared with existing defenses under IID and non-IID settings in Table 1. We defer the results against Neurotoxin to Appendix D.3 due to space limitations. We have the following observations from the experimental results. First, FedGame outperforms all existing defenses in terms of ASR. In particular, FedGame can reduce ASR to random guessing (i.e., ASR of FedAvg under no attacks) in both IID and non-IID settings for clients as well as both in-domain and out-of-domain settings for the server. Intrinsically, FedGame performs better because our game-theoretic defense enables the defender to optimize its strategy against dynamic, adaptive attacks. We

    & Client &  &  \\   & Data &  &  &  &  &  &  &  &  &  &  &  \\   & & & & & & & & & & & & & In Out-of \\   &  &  & TA (\%) & 99.04 & 98.77 & 98.78 & 99.17 & 95.48 & 92.97 & 97.69 & 97.93 & 98.53 & 98.56 \\   & & ASR (\%) & 9.69 & 99.99 & 99.99 & 99.97 & 98.54 & 99.45 & 20.03 & 16.01 & **9.72** & **9.68** \\   &  &  & TA (\%) & 81.08 & 80.51 & 76.44 & 80.17 & 80.38 & 43.22 & 76.79 & 75.71 & 74.81 & 74.65 \\   & & & ASR (\%) & 8.39 & 99.80 & 99.94 & 99.82 & 99.87 & 99.58 & 98.58 & 99.46 & **8.92** & **9.24** \\   &  &  & TA (\%) & 98.98 & 99.15 & 96.88 & 99.12 & 94.54 & 91.52 & 97.39 & 97.68 & 98.28 & 98.34 \\   & & ASR (\%) & 9.73 & 99.99 & 85.03 & 99.98 & 98.16 & 99.54 & 20.03 & 19.61 & **10.42** & **10.88** \\   &  &  & TA (\%) & 80.25 & 75.35 & 67.66 & 79.54 & 70.18 & 50.79 & 77.76 & 75.08 & 73.88 & 73.57 \\   & & ASR (\%) & 9.67 & 99.92 & 99.92 & 99.99 & 99.63 & 95.01 & 99.03 & 99.82 & **11.76** & **12.03** \\   &  &  & TA (\%) & 99.04 & 99.03 & 98.87 & 98.98 & 98.99 & 98.96 & 97.98 & 97.84 & 98.05 \\   & & ASR (\%) & 9.69 & 100.00 & 10.06 & 99.81 & 99.75 & 99.73 & 15.02 & 10.02 & **9.56** & **9.68** \\   &  &  & TA (\%) & 81.08 & 80.90 & 76.09 & 80.00 & 80.21 & 41.36 & 72.13 & 75.17 & 73.18 & 72.93 \\   & & ASR (\%) & 8.39 & 93.44 & 94.97 & 91.60 & 91.90 & 86.96 & 83.26 & 66.58 & **8.81** & **9.00** \\   &  &  & TA (\%) & 98.98 & 98.98 & 98.58 & 99.13 & 93.98 & 88.91 & 96.65 & 97.62 & 98.58 & 98.59 \\   & & ASR (\%) & 9.73 & 100.00 & 10.52 & 99.85 & 55.97 & 99.66 & 13.21 & 10.19 & **9.97** & **9.83** \\   &  &  & TA (\%) & 80.25 & 80.15 & 74.31 & 79.78 & 78.78 & 38.17 & 73.83 & 74.57 & 73.52 & 73.21 \\   & & ASR (\%) & 9.67 & 95.03 & 60.06 & 95.00 & 92.51 & 99.51 & 80.75 & 74.35 & **10.62** & **10.67** \\   

Table 1: Comparison of FedGame with existing defenses under Scaling attack and DBA. The total number of clients is 10, where 60% are compromised. The best results for each setting among FedGame and existing defenses are bold.

note that FLTrust outperforms other defenses (except FedGame) in most cases since it exploits a clean training dataset from the same domain as local training datasets of clients. However, FLTrust is not applicable when the server only holds an out-of-domain clean training dataset, while FedGame can relax such an assumption and will still be applicable. Moreover, our experimental results indicate that FedGame achieves comparable performance even if the server holds an out-of-domain clean training dataset.

To further understand our results,we visualize the average genuine (or trust) scores computed by FedGame (or FLTrust) for compromised and benign clients in Appendix D.4. In particular, we find that the genuine scores produced by FedGame are much lower than those produced by FLTrust for compromised clients, which explains why FedGame outperforms FLTrust. Second, FedGame achieves comparable TA with existing defenses, indicating that FedGame preserves the utility of global models.

Furthermore, we show the comparison results of FedGame with existing defenses against the scaling attack when the total number of clients is 30 in Table 4 in Appendix D.5. Our observations are similar, which indicates that FedGame consistently outperforms existing defenses under different numbers of clients and backdoor attacks.

Impact of \(\).\(\) is a hyperparameter used by an attacker (see Eqn. 4) when searching for the optimal \(r_{i}^{t}\) for each compromised client \(i\) in each communication round \(t\). Figure 1(a) shows the impact of \(\) on ASR of FedGame. The results show that FedGame is insensitive to different \(\)'s. The reason is that the genuine score for a compromised client is small when \(\) is large, and the local model of a compromised client is less likely to predict a trigger-embedded input as the target class when \(\) is small. As a result, backdoor attacks with different \(\) are ineffective under FedGame.

Impact of the Fraction of Compromised Clients.Figure 1(b) shows the impact of the fraction of compromised clients on ASR. As the results show, FedGame is effective for a different fraction of compromised clients in both in-domain and out-of-domain settings. In contrast, FLTrust is ineffective when the fraction of compromised clients is large. For instance, FedGame can achieve 9.84% (in-domain) and 10.12% (out-of-domain) ASR even if 80% of clients are compromised on MNIST. Under the same setting, the ASR of FLTrust is 99.95%, indicating that the defense fails.

Impact of Client Selection.By default, we consider that all clients are selected in each communication round. We also consider only a subset of clients are selected in each communication round by the server. Figure 1(c) shows our experimental results. Our results suggest that our defense is effective and consistently outperforms FLTrust.

Computation Cost.FedGame computes a genuine score for each client in each communication round. Here we demonstrate its computational efficiency. On average, it takes 0.148s to compute a genuine score for each client in each communication round on a single NVIDIA 2080 Ti GPU. We note that the server could from a resourceful tech company (e.g., Google, Meta, Apple), which would have enough computation resources to compute it for millions of clients. Moreover, those local models can be evaluated in parallel.

Figure 1: Comparing FedGame and FLTrust under different variations of attack. (a) Different \(\). (b) Different fractions of malicious clients. (c) Different numbers of clients selected in each communication round. (d) Different trigger sizes.

Performance under Static Attacks.In our evaluation, we consider an attacker optimizing the fraction of backdoored training examples. We also evaluate FedGame under existing attacks where the attacker does not optimize it. Under the default setting, FedGame can achieve an ASR of 9.75%, indicating that our defense is effective under static attack.

Furthermore, we discuss the impact of trigger reverse engineering methods, the total number of clients, and the size of clean data of the server in Appendix D.6.

### Adaptive Attacks

In this subsection, we discuss some potential adaptive strategies that may be leveraged by the attacker to evade our defense.

Data Replacing Attack.By default, we consider an attacker optimizing the fraction of backdoored training examples _added_ to the local training dataset of a compromised client to maximize backdoor effectiveness. We also consider an attacker who _replaces_ a fraction of the local training data with backdoored samples and optimizes such fraction. Under our default setting, the ASR of FedGame is 9.71% and 10.69% on MNIST and CIFAR10, respectively, indicating that our defense is still effective under data replacing attacks.

Variation in Triggers.We demonstrate that our defense is robust to variation in triggers. In particular. we try triggers with sizes \(2 2\), \(4 4\), and \(6 6\) under the default setting. Figure 1(d) compares FedGame with FLTrust. The results demonstrate that FedGame is consistently effective under triggers with different sizes.

We note that although an attacker can slightly manipulate the parameters of local models such that they are more similar to those of benign clients, FedGame does not rely on model parameters for detection. Instead, FedGame leverages the model behaviors, i.e., whether the model predicts inputs with our reverse-engineered trigger as the target class. As a result, our defense would still be effective even if the change in the model parameters is small as long as the model has backdoor behavior (this is required to make the attack effective). This is also the reason why our defense is better than existing methods such as FLTrust which leverages model parameters for defense.

## 7 Conclusion and Future Work

In this work, we propose FedGame, a general game-theory based defense against adaptive backdoor attacks in federated learning. Our formulated minimax game enables the defender and attacker to dynamically optimize their strategies. Moreover, we respectively design solutions for both of them to solve the minimax game. We perform theoretical analysis and empirical evaluations for our framework. Our results demonstrate the effectiveness of FedGame under strategic backdoor attackers. Moreover, FedGame achieves significantly higher robustness than baselines in different settings.

We consider that an attacker could optimize the poisoning ratio and trigger pattern in our work. We believe it is an interesting future work to consider other factors for the attacker (e.g., trigger size, consideration of long-term goal, design of new loss functions , poisoned neurons selection ). Moreover, we consider a zero-sum Stackelberg game in this work. Another interesting future work is to consider other game formulations, e.g., Bayesian games.