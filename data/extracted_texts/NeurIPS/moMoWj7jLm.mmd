# FormulaReasoning: A Dataset for

Formula-Based Numerical Reasoning

 Xiao Li  Bolin Zhu  Sichen Liu  Yin Zhu  Yiwei Liu  Gong Cheng

State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

{xiaoli.nju,bolinzhu,sichenliu,yinzhu,ywliu}@smail.nju.edu.cn

gcheng@nju.edu.cn

Corresponding author

###### Abstract

The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we construct a dataset for formula-based numerical reasoning called FormulaReasoning, which consists of 5,420 reasoning-based questions. We employ it to conduct evaluations of LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thought methods, and we further explore using retrieval-augmented LLMs provided with an external formula database associated with our dataset. We also experiment with supervised methods where we divide the reasoning process into formula generation, parameter extraction, and numerical calculation, and perform data augmentation. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaReasoning.

## 1 Introduction

Numerical reasoning constitutes one of the significant forms within natural language reasoning (Frieder et al., 2023). The study of numerical reasoning has seen substantial progress in recent years, largely driven by the development of LLMs (OpenAI, 2023; Touvron et al., 2023; Li et al., 2023c) and specialized datasets (Wang et al., 2017; Dua et al., 2019; Amini et al., 2019; Cobbe et al., 2021a). Current datasets for numerical reasoning typically include simple, commonsense numerical questions that do not reflect the complexity of real-world problems. These datasets have not fully addressed the interpretability issue in numerical reasoning, as they often rely on implicit commonsense knowledge without explicit guidance knowledge during the reasoning process. This issue becomes particularly evident when LLMs meet hallucination (Frieder et al., 2023; Bang et al., 2023). Consequently, one might naturally ask _"What knowledge could I use to guide numerical reasoning process?"_. Formulas exactly represent such knowledge that has been largely overlooked in research but is frequently utilized in real-life applications.

Take a question from the GSM8K (Cobbe et al., 2021a) as an example: "A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?". This example only requires the use of implicit _commonsense mathematical knowledge_ to solve without domain-specific formula. However, in our FormulaReasoning dataset, we require _specific formulas_ to guide the numerical reasoning process, such as the formula used to calculate the heat absorption of an object.

Recently, Liu et al., 2023 constructed two formula-based datasets, Math23K-F and MAWPS-F. However, the formulas in these datasets primarily consist of commonsense formulas (such as total_amount = unit_amount \(\) total_number), and only 33.5% and 38.4% of the questions in these datasets, respectively, require the use of formulas.

To address this gap, we constructed a dataset for numerical reasoning that requires the use of formulas called FormulaReasoning. We annotated formulas for each question in FormulaReasoning. An example of FormulaReasoning is shown in Figure 1.2 The formula-based feature makes FormulaReasoning a more challenging dataset for developing systems that can tackle real-world numerical reasoning problems. Indeed, in fields such as mathematics and physics, formulas serve as an important vessel for representing domain knowledge. However, existing datasets scarcely consider explicit incorporation of formulas into numerical reasoning.

We collected questions requiring formula-based numerical reasoning from Chinese junior high school physics examinations. With the _combined efforts of manual annotation and assistance from LLMs_, we annotated each question with an explanation text, a final answer, and a set of relevant formulas (including formula structures, parameter names, symbols, numerical values, and units) and built a _formula database_. The formula database functions as an external knowledge base, which can be used to evaluate retrieval-based/augmented systems. In Table 1, we compare FormulaReasoning with two existing formula-based datasets and the well-known GSM8K. In comparison to Math23K-F and MAWPS-F, FormulaReasoning contains _a larger number of formulas_ (272), whereas the other two datasets contain 51 and 18 formulas. Additionally, all questions in FormulaReasoning require

   Dataset & Math23K-F & MAWPS-F & GSM8K & FormulaReasoning \\  \# questions & 23,162 & 2,373 & 8,792 & 5,420 \\ \# formulas (and variants) & 51 (131) & 18 (46) & 0 (0) & 272 (824) \\ \# questions requiring formula (proportion) & 7,750 (33.46\%) & 911 (38.39\%) & N/A & 5,420 (100\%) \\ Avg. \# reasoning steps & 1.16 & 1.01 & 3.59 & 2.37 \\   

Table 1: Statistics of Math23-F, MAWPS-F, GSM8K and our FormulaReasoning.

Figure 1: An example taken from FormulaReasoning. Numerical values (including units) given in the question and obtained from intermediate steps are highlighted in red and purple, respectively. Formulas and their elements are in blue.

the use of formulas. The _higher average number of reasoning steps_ (2.37 vs. 1.16/1.01) implies that FormulaReasoning is more challenging and better suited for evaluating existing models as a multi-step formula-based reasoning task.

We used FormulaReasoning to evaluate LLMs ranging from 7B to >100B parameters, as well as fine-tuned models such as Qwen-1.8B (Bai et al., 2023) and ChatGLM-6B (Zeng et al., 2022) with a proposed Chain-of-Thought supervised fine-tuned method and a data augmentation method. We also trained an encoder for formula retrieval and experimented with retrieval-augmented generative models. Our empirical findings show that the best existing models only achieve an accuracy of around 74%, lagging behind an accuracy 92% of humans, indicating that there is still significant room for exploration in formula-based numerical reasoning.

Our contributions are summarized as follows:

* We construct a formula-based numerical reasoning dataset FormulaReasoning, with fine-grained annotations for each question. As a formular knowledge-guided numerical reasoning dataset, it can be applied to tasks involving trustworthy and verifiable reasoning.
* We conduct evaluations on LLMs of various sizes, supervised fine-tuned models, and retrieval-augmented generative models. The experimental results establish a strong baseline for future research and also indicate that the task remains unresolved.

The dataset is available on https://zenodo.org/doi/10.5281/zenodo.11408109 under the CC BY 4.0 License and our code is available on https://github.com/nju-websoft/FormulaReasoning under the Apache License 2.0.

## 2 Related Work

### Numerical Reasoning Datasets

Numerical reasoning is one of the fundamental capabilities of natural language reasoning. The study of numerical reasoning in natural language has existed for several years. Numerous datasets, such as DROP (Dua et al., 2019), GSM8K (Cobbe et al., 2021b), TSQA (Li et al., 2021) and MATH (Hendrycks et al., 2021), have introduced natural language numerical reasoning. Another line of research focusing on numerical reasoning in natural language is math word problem (MWP). MWP tasks typically provide a short passage (i.e., a question) and require the generation of an arithmetic expression that can compute an answer. Representative datasets include MAWPS (Koncel-Kedziorski et al., 2016), Math23K (Wang et al., 2017), MathQA (Amini et al., 2019), etc.

The recently introduced datasets (Liu et al., 2023) Math23K-F and MAWPS-F require formulas for only 33.5% and 38.4% of the questions, respectively, and the formulas within these datasets are all simple commonsense formulas (e.g., total_cost = unit_cost \(\) total_number). By contrast, our FormulaReasoning dataset collects questions from junior high school physics examinations, with every question accompanied by formulas. In addition, we also annotated a _formula database_ for FormulaReasoning that can serve as an external knowledge base, used to assess retrieval-augmented systems.

### Numerical Reasoning Methods

The methods for solving numerical reasoning have evolved from statistical approaches (Hosseini et al., 2014; Kushman et al., 2014) to those based on rules and templates (Shi et al., 2015; Wang et al., 2019) and further to methods based on deep learning models (Gupta et al., 2019; Chen et al., 2022; Kim et al., 2022; Li et al., 2023a). In the past two years, with the rapid development of LLMs, LLMs have demonstrated strong capabilities in resolving numerical reasoning questions. Consequently, several methods aimed at enhancing the reasoning abilities of LLMs have been proposed, including the notable Chain of Thoughts (CoTs) method (Wei et al., 2022), along with many subsequent variant approaches (Kojima et al., 2022; Wang et al., 2022; Zhou et al., 2022; Li et al., 2023b).

We established representative existing methods as baselines for FormulaReasoning, including zero/few-shot CoTs prompting methods to LLMs ranging from 7B to over 100B parameters. We trained a specialized formula retriever for retrieving formulas and explored retrieval-enhanced numerical reasoning. We also divided the reasoning process into formula generation, parameter extraction, and calculation, and used data augmentation to enhance fine-tuned models with fewer than 7B parameters.

## 3 Dataset Construction

We collected raw questions from Chinese junior high school physics examinations from 2015 to the present. We had a total of five postgraduate volunteer students, and they all hold a bachelor's degree in science and engineering. We then annotated the reasoning steps and corresponding formulas for each question. This process involved a combination of manual annotation and the assistance of LLMs to improve the efficiency of annotation. Each question is associated with an explanation of the reasoning steps in natural language with a symbolic representation of the reasoning steps using formulas, including the values and units for all the parameters within the formulas. Finally, we compiled all the formulas we merged those expressing the same meaning to create a formula database. We describe this process to construct FormulaReasoning in detail below.

### Preprocessing

We crawled 18,433 junior high school physics examination questions in China from 2015 to the present from public sources, including only those with free-text answers and excluding multiple-choice and true/false questions. Each raw question contains a _question text_ and an _explanation text that includes the reasoning steps_. We eliminated questions requiring diagrams.

Subsequently, we filtered the questions by assessing the presence of numerical values within the explanation and confirming that the final answer was numerical. Utilizing a regular expression-based approach, we extracted the _final numeric answer_, including its unit, from the explanation. We found that for 487 questions, the regular expressions did not return results, so we manually annotated the positions of their answers in the text explanations. Following the preprocessing phase, we compiled an initial dataset comprising 6,306 questions.

**Original explanation.**

The change in water temperature is 60 - 20 = 40 \({}^{}\)C. Therefore, the heat absorbed by the water is Q_{absorbed}=50 kg \(\) 4.2 \( 10^{3}\) J/(kg.\({}^{}\)C) \(\) 40 \({}^{}\)C = 8.4 \( 10^{6}\) J. Given that the total electrical energy consumed in the heating process is \(1 10^{7}\) J, the thermal efficiency of the water heater can be calculated using the formula for the efficiency of a heat engine: \(\) = Q_{absorbed} )/W_{total}\( 100\)% = (\(8.4 10^{6}\) J)/(\(1.0 10^{7}\) J)\( 100\)% = 84%. Answer: If it is known that the total electrical energy consumed during the heating process is \(1 10^{7}\), the thermal efficiency of the water heater is \(84\)%.

**Explanation with normalized formulas.**

1. Calculating the temperature increase in water: [Degree of water temperature increase] = [Final temperature] - [Initial temperature] = 60 \({}^{}\)C - 20 \({}^{}\)C = 40 \({}^{}\)C. The degree of water temperature increase = 40 \({}^{}\)C.

2. Calculating the heat absorbed by water: [Heat absorbed by water] = [Mass of water] \(\) [Specific heat capacity of water] \(\) [Degree of water temperature increase] = 50 kg \(\) 4.2 \(\) 10\({}^{3}\) J/(kg.\({}^{}\)C) \(\) 40 \({}^{}\)C = 8400000 J. The heat absorbed by water = 8400000 J.

3. The thermal efficiency of the water heater can be obtained from: [Thermal efficiency of the water heater] = [Heat absorbed by water] / [Total electrical energy consumed] \(\) 100% = 8400000 J / (\(1 10^{7}\) J) * 100% = 84%. The thermal efficiency of the water heater = 84%.

Answer = 84%.

### Formula Normalization

We found that the reasoning steps (i.e. the explanation) in the obtained raw dataset lacked a normalized format and were expressed quite casually. Some formulas mixed parameter names (e.g., "mass of

|} 
**Original explanation.** \\ The change in water temperature is 60 - 20 = 40 \({}^{}\)C. Therefore, the heat absorbed by the water is Q_{absorbed}=50 kg \(\) 4.2 \( 10^{3}\) J/(kg.\({}^{}\)C) \(\) 40 \({}^{}\)C = 8.4 \( 10^{6}\) J. Given that the total electrical energy consumed in the heating process is \(1 10^{7}\) J, the thermal efficiency of the water heater can be calculated using the formula for the efficiency of a heat engine: \(\) = Q_{absorbed} )/W_{total}\( 100\)% = (\(8.4 10^{6}\) J)/(\(1.0 10^{7}\) J)\( 100\)% = 84%. Answer: If it is known that the total electrical energy consumed during the heating process is \(1 10^{7}\), the thermal efficiency of the water heater is \(84\)%. \\
**Explanation with normalized formulas.** \\
1. Calculating the temperature increase in water: [Degree of water temperature increase] = [Final temperature] - [Initial temperature] = 60 \({}^{}\)C - 20 \({}^{}\)C = 40 \({}^{}\)C. The degree of water temperature increase = 40 \({}^{}\)C.

2. Calculating the heat absorbed by water: [Heat absorbed by water] = [Mass of water] \(\) [Specific heat capacity of water] \(\) [Degree of water temperature increase] = 50 kg \(\) 4.2 \(\) 10\({}^{3}\) J/(kg.\({}^{}\)C) \(\) 40 \({}^{}\)C = 8400000 J. The heat absorbed by water = 8400000 J.

3. The thermal efficiency of the water heater can be obtained from: [Thermal efficiency of the water heater] = [Heat absorbed by water] / [Total electrical energy consumed] \(\) 100% = 8400000 J / (\(1 10^{7}\) J) * 100% = 84%. The thermal efficiency of the water heater = 84%.

Answer = 84%.

Table 2: Original explanation and explanation with normalized formulas (highlighted in blue).

water") and symbols (e.g., "\(m_{water}\)"), while others simply provided calculations in numerical form without parameter names or symbols. In order to ensure that all explanations adopted a normalized form of formulas, we normalized the formula annotations in the explanations. An example can be found in Table 2. In this process, we need to _identify the formulas used within the original explanations_ and to _correct any formatting issues_. Manually undertaking such tasks would require significant effort. However, since the process is not open-ended, but rather structured and verifiable, we could automatically, e.g., _using a LLM_, extract formulas from the explanations, calculate each step, and compare the result with the given answer to ensure the accuracy of this normalization process.

Specifically, to enhance the efficiency of the annotation, we adopted a coarse-to-fine annotation approach with the help of a LLM3. We first prompted the LLM in a few-shot manner to generate accurate explanations of the reasoning process. Then, we used few-shot prompts to guide the LLM in correcting minor errors within the normalized explanations, including formatting errors in formula annotations and inaccuracies in the parameters used during computations. Both prompts can be found in Appendix C.1.1. Next, we will provide a detailed description of this process.

Initially, we introduced the question along with its original explanation and the corresponding answer to guide the LLM through few-shot prompting to revise the original explanation. We observed that the ability of the LLM to revise explanations towards normalized explanations remained satisfactory. To assess the correctness of the revised explanations, we extracted formulas from these explanations and then computed the answer using the numbat tool4. In addition to providing explanations, we also required the LLM to present the values, symbols, and units of each parameter in the formulas in the form of a table. An example is shown in Figure 1.

At this stage, we checked the correctness of the formula format in the explanations by automatic rules, including whether there were omissions in parameter names, parameter symbols, or corresponding units, and these issues were all correctable. Therefore, if our program detected that the LLM had not successfully generated an accurate normalized explanation, we used few-shot prompting to identify and correct these specific errors. More details can be found in Appendix C.1.1. We observed that the questions which remained incorrect despite multiple attempts by the LLM were of notably poor quality, including missing important reasoning steps, unclear question formulation, and so on. Some examples of these questions can be found in Appendix C.1.2. These questions were removed from our dataset. Following this step, our dataset contains a remaining total of 5,420 questions.

### Formula Database Construction

Our next step was to _construct a unified formula database for the entire dataset_. Given that parameters in the same formula can be expressed differently across various problem contexts, for instance, the two formulas "[weight of water] = [mass of water] * [gravitational acceleration]" and "[weight] = [mass] * [gravitational acceleration]" both calculate the weight of an object, we need to merge these formulas into a single representation.

We divided the construction process of the formula database into three steps: 1) Merge the formulas through symbolic rules. 2) Merge the formulas through semantic-based method. 3) Manual review and error correction. In Table 3, we present the initial number of formulas and the remaining number of formulas after each step.

  Step & \# Formulas \\  Before merging & 12,906 \\ After symbolic rules based merging & 1,163 \\ After semantic-based merging & 439 \\ After manual review and error correction & 272 \\  

Table 3: Changes in the number of formulas after each merging step.

Symbolic rules based merging.In this step, we merged formulas through symbolic rules. Specifically, this was achieved by _comparing the structure of the formulas and the symbols_.Take the following as an example of judging whether two formulas have the same structure: the formulas "\(f_{1}:\ a_{1}\)=(\(b_{1}\)+\(c_{1}\))/\(d_{1}\)", "\(f_{2}:\ a_{2}\)=(\(b_{2}\)+\(c_{2}\))/\(d_{2}\)" and "\(f_{3}:\ b_{1}\)=\(a_{1}\)*\(d_{1}\)-\(c_{1}\)" have the same structure because \(f_{2}\) can be derived from \(f_{1}\) by renaming parameters, and \(f_{3}\) can be obtained from \(f_{1}\) by transformation. Moreover, in physics, certain physical quantities are conventionally represented by specific symbols. For example, the mass of an object is often denoted by "\(m\)" and the density of an object is frequently represented by the symbol "\(\)". Subscripts are then used to distinguish which specific object a physical quantity refers to, such as "\(_{water}\)" for the density of water. For any two formulas, we first computed all the transformations of each formula to obtain a set of all its variants. Then, we compared the formula structures in the two sets to determine if two formulas were structurally equivalent. If they shared the same structure, we then compared whether their symbols, with subscripts removed, were identical. If they were, we considered these two formulas to be mergeable. When merging, we retained the parameter with the shorter length from the two. After merging based on symbolic rules, we reduced the number of formulas in the formula database from 12,906 to 1,163.

Semantic-based merging.In the symbolic rules based merging process, the semantic information of the parameter names was neglected. This led us to _perform merges grounded on the semantics of the parameter names_. For instance, two formulas that were not merged during the symbolic fusion stage, "[density] = [mass] / [volume]" and "[density of water] = [mass of water] / [volume of water]", can actually be merged. We would carry out the merging of these two formulas based on the semantic information of the parameter names (for example, "density" and "density of water" are semantically similar). Specifically, for formulas with identical structures, we tokenized each pair of corresponding parameters to create two sets of words5. When the two sets overlapped, the parameters were considered to have semantic connection, and the formulas became candidates for merging. Utilizing this approach, we identified a set of pairs of potentially mergeable formulas and then consulted the LLM for a thorough evaluation of each pair. The prompts can be found in Appendix C.1.3. After this step, the number of formulas in the formula database was reduced to 439.

Manual review and error correction.Upon completing the aforementioned merging process, we manually inspected the correctness of the results, rectified instances where errors occurred during merging, and manually merged formulas that were overlooked by the LLM. In this process, there were two human volunteers cross-validating the results of manual review and annotation. Finally, we obtained a formula database consisting of 272 formulas.

## 4 Experiments Setup

In this section, we explore several methods for handling the questions within FormulaReasoning, including prompting LLMs using zero-shot and few-shot chain-of-thought (CoT, Wei et al., 2022; Kojima et al., 2022), and training a formula retriever to retrieve formulas to be incorporated into LLM prompts. Additionally, we employed two approaches to enhancing the reasoning abilities of fine-tuned models with fewer than 7B parameters. The first approach involved dividing the reasoning process into distinct steps: formula generation, parameter extraction, and numerical calculation. The second approach leveraged data augmentation to improve the models' reasoning ability.

### Dataset Split

We divided FormulaReasoning into into subsets for training, _id_ (in-distribution) test, and _ood_ (out-of-distribution) test, comprising 4,608, 421 and 391 questions, respectively. We required that all formulas in the id test must appear in the training set, whereas in the odd test, each question involves at least one formula that has not been seen in the training set. This division is designed to evaluate the generalizability of fine-tuned models on formulas that they have not previously encountered.

### Evaluation

#### 4.2.1 Human Performance

We recruited 108 students from a high school, with each student being assigned 7-8 questions. Each student was given 40 minutes to complete these questions. These questions were used as part of their in-class exercises, and at the end, each student received a gift. The final statistics were collected to evaluate human performance, which was consented by all the students.

#### 4.2.2 LLMs

Following Kojima et al., 2022, we incorporated the phrase "Let's think step by step" into the zero-shot prompt to guide LLMs in generating the reasoning steps. For the few-shot setting, we randomly sampled five questions from the training set to serve as examples for in-context learning. Each example includes the question text and the reasoning steps (i.e., the explanation). Examples of the prompts can be found in Appendix C.4.1.

We conducted experiments on GPT-4-turbo, GPT-3.5-turbo, GLM4, and Qwen-max, with each of these models having over 100 billion parameters. We also evaluated on Llama2-7B (Touvron et al., 2023), Llama3-8B (Meta, 2024), Qwen-7B/14B (Bai et al., 2023), InternLM2-7B/20B (Team, 2023), ChatGLM3-6B (Zeng et al., 2022), including the base and chat versions of these models. We followed the common practice that few-shot experiments were performed on the base versions, while zero-shot experiments were conducted on the chat or instruct versions.

#### 4.2.3 Formula Retrieved

We trained a formula retriever on the training set. Specifically, we encoded each question using the Chinese-BERT-wvm-base (Devlin et al., 2019; Cui et al., 2021) model to obtain the CLS vector of the question. Each formula in the formula database was represented by a randomly initialized vector. During training, we calculated the cosine score between the question vector and the formula vector. The retriever was then trained with in-batch negatives and contrastive learning loss (Gao et al., 2021). Subsequently, for each question in the id test, we retrieved the top five formulas with the highest scores and included them in the prompt to observe the change in the performance of the LLM when provided with relevant formulas. More details can be found in Appendix C.4.2.

#### 4.2.4 Supervised Fine-tuned Models

We found that directly prompting models possessing fewer than 7B parameters failed to produce satisfactory outcomes (for example, ChatGLM3-6B attained merely 8.99 points in a zero-shot setting). Therefore, we conducted supervised fine-tuning of models with fewer than 7B parameters, yet discerned that, dissimilar to larger models (such as GPT-4-turbo), smaller models did not exhibit proficient performance in numerical extraction and calculation. In order to augment the reasoning capabilities of smaller models, we explored two approaches for improvement.

Chain-of-Thought Supervised Fine-Tuning (CoT-SFT)We decomposed the reasoning process into several steps. First, we instructed the model to generate the formulas required to solve the question. Subsequently, the parameter names within the formulas were extracted, allowing the model to retrieve the corresponding values and units from the context. Next, the formulas and the associated parameter values were provided to a calculator to obtain the final result. This approach relieved the model of the numerical calculation, allowing it to concentrate on the reasoning aspect.

Data Augmentation (DA)We augmented the training dataset with the assistance of larger models. Firstly, we utilized a few-shot approach to prompt the LLM (Qwen-max) to generate new question-answer pairs. The correctness of the computation process generated by the LLM was meticulously verified using a calculator. Subsequently, the formulas generated by the model were extracted and normalized. More details could be found in Appendix C.3.1.

### Metric

We utilized numbat to evaluate the predictions generated by the model against the gold-standard answers. A prediction is deemed correct if the relative error (prediction - gold) / gold is less than 1%. We employed _accuracy_, which is the proportion of questions answered correctly, as our metric.

## 5 Experiments Results

In this section, we presented the experimental results and analysis. Due to space constraints, the error analysis can be found in Appendix C.2 and the implementation details can be found in Appendix C.4.

### Human Performance

In FormulaReasoning, humans achieved impressive performance, with a score of 93.49 on the id test, 90.47 on the odd test, and an average score of 92.03.

### Results of LLMs

The evaluation results on LLMs are shown in Table 4. _GPT-4-turbo exhibited the best performance in both zero-shot and few-shot settings_, surpassing the second-ranked GLM4 by an average of 6.16 points in zero-shot setting and 10.22 in few-shot setting. Among models with size not exceeding 20B, Qwen-14B demonstrated commendable performance in both zero-shot and few-shot settings. The subpar performance of Llama2 might be due to its pre-training data being primarily in English. We also conducted few-shot testing on the chat version of LLMs with size not exceeding 20B, and the results can be found in Appendix C.4.3. After incorporating few-shot examples, GPT-4-turbo, GPT-3.5-turbo and Qwen-max demonstrated performance improvements, ranging from 0.24 to 6.14. However, similar performance changes were not observed on GLM4, possibly due to its supervised fine-tuning and alignment with human preferences which enhanced GLM4's understanding of instructions but probably also compromised its in-context learning ability.

Human performance surpassed the performance of few-shot GTP-4-turbo on the id and ood tests by margins of 21.99 and 13.25 points, respectively. Such results demonstrated that there remained a substantial gap between the current capabilities of state-of-the-art LLMs and human performance. This was even more pronounced when considering smaller-scale models. These findings underscored _the challenging nature of FormulaReasoning as an unresolved dataset_, and that there was significant room for improvement in LLMs as they struggled to match human levels of reasoning.

### Results of LLMs with Formula Retrieve

    &  &  &  \\   & & id test & ood test & Avg. & id test & ood test & Avg. \\  GPT-4-turbo & unknown & **70.07** & **72.89** & **71.43** & **71.50** & **77.49** & **74.38** \\ GPT-3.5-turbo & unknown & 26.13 & 25.58 & 25.87 & 32.07 & 29.92 & 31.03 \\ GLM4 & >100B & 65.32 & 65.22 & 65.27 & 62.47 & 65.98 & 64.16 \\ Qwen-max & >100B & 58.67 & 57.80 & 58.25 & 58.91 & 63.94 & 61.33 \\ InternalM\({}^{*}\) & 20B & 5.70 & 4.60 & 5.17 & 18.29 & 11.25 & 14.90 \\ Qwen\({}^{*}\) & 14B & 32.07 & 37.60 & 34.73 & 44.89 & 36.83 & 41.01 \\ Llama3\({}^{*}\) & 8B & 26.66 & 17.98 & 20.41 & 12.81 & 8.87 & 10.91 \\ Llama2\({}^{*}\) & 7B & 0.00 & 0.26 & 0.13 & 1.43 & 0.26 & 0.87 \\ Qwen\({}^{*}\) & 7B & 7.36 & 8.70 & 8.01 & 21.14 & 18.16 & 19.71 \\ InterLM\({}^{*}\) & 7B & 7.84 & 7.67 & 7.76 & 9.50 & 8.18 & 8.86 \\ ChatGLM3\({}^{*}\) & 6B & 9.36 & 8.62 & 8.99 & 23.89 & 19.95 & 21.92 \\  Human & - & 93.49 & 90.47 & 92.03 & 93.49 & 90.47 & 92.03 \\   

Table 4: Results of LLMs with zero-shot and few-shot prompting. \({}^{*}\) indicates that the chat or instruct version of the model was used in the zero-shot setting, while the base version of the model was used in the few-shot setting.

The results of LLMs utilizing the formula retriever are shown in Table 5. We found that the impact on performance varied among different LLMs when incorporating retrieved formulas into prompts. We observed a positive enhancement on GLM4, with score increments of 4.99 and 3.33 with zero-shot and few-shot, respectively, on the id test. However, we observed a performance decline with GPT-4-turbo. Specifically, we found that the top 5 retrieved formulas often included irrelevant ones, as the number of formulas required varies for each problem. The presence of these extraneous formulas affected the model's performance, indicating that there is considerable room for further research in utilizing a formula database.

### Results of Supervised Fine-tuned Models

Table 6 shows the results for the supervised fine-tuned models, with and without CoT-SFT and DA, which were detailed in Section 4.2.4. In most settings, both models achieved higher scores on the id test than the odd test, yet they still exhibited considerable performance on the odd test. This indicates that _1) the odd formulas indeed impacted model performance_ and _2) the models still demonstrate generalizability._ We hope that the division of id test and odd test will be helpful for assessing the generalization ability of fine-tuned models in future works.

It was noteworthy that with CoT-SFT, Qwen-1.8B and ChatGLM3-6B, with a mere parameter count of 1.8B and 6B, respectively, achieved performance comparable to GPT-4-turbo (though such a comparison may not be entirely fair). This indicated that the incorporation of CoT-SFT and the use of calculators could significantly enhance the reasoning capabilities of small models. Our findings revealed that _focusing on reasoning with CoT while delegating numerical calculation to a calculator could enhance the performance of small models_, given their limited calculating capability. _The assistance of LLMs for data augmentation could also enhance smaller models' reasoning capability_. This discovery provides valuable insights for future deployment of numerical reasoning systems on small models.

## 6 Conclusion and Limitations

We introduced FormulaReasoning, a dataset for formula-based numerical reasoning. We annotated the reasoning steps with formulas for each question with both manual and LLM-assisted efforts. Furthermore, we constructed a formula database after merging formulas with similar meanings, serving as an external knowledge base for subsequent retrieval-based/augmented approaches. We evaluated FormulaReasoning across various sizes of LLMs, supervised fine-tuned models, and retrieval-augmented LLMs, demonstrating its challenging nature as an unresolved task. Our findings indicate substantial room for improvement of existing models on formula-based numerical reasoning, thus motivating future research efforts.

We have also translated the dataset into English unitizing LLMs. However, we have not yet accurately assessed the quality of the translated dataset. At present, we have not released the English version of the dataset, but we will do so later after ensuring the quality of the English dataset. Additionally, our dataset is limited to the domain of physics. Although junior high school physics is not overly complex and can be understood by most people, it is still possible to explore formula-based question answering data in other domains.

   Model & Size & id test & ood test & Avg. \\  Qwen-1.8B & & 55.91 & 44.58 & 50.25 \\ + DA & 1.8B & 56.16 & 45.32 & 50.74 \\ + CoT-SFT & & 73.65 & 74.38 & 74.00 \\  ChatGLM-6B & & 52.95 & 40.64 & 47.02 \\ + DA & 6B & 53.44 & 45.32 & 49.53 \\ + CoT-SFT & & 74.63 & 73.89 & 74.23 \\   

Table 6: Results of supervised fine-tuned models on FormulaReasoning.

   Model & zero-shot & few-shot \\  GLM4 & 65.32 & 62.47 \\ + formula retriever & 70.31 & 65.80 \\  GPT-4-turbo & 70.07 & 71.50 \\ + formula retriever & 68.17 & 67.00 \\   

Table 5: Results of LLMs with Formula Retriever on the id test.