# Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension

Kedar Karhadkar\({}^{1}\), Michael Murray\({}^{1}\), Guido Montufar\({}^{1,2,3}\)

\({}^{1}\)Department of Mathematics, UCLA

\({}^{2}\)Department of Statistics & Data Science, UCLA

\({}^{3}\)Max Planck Institute MiS

###### Abstract

Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension \(d_{0}\) scales at least logarithmically in the number of samples \(n\). In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when \(d_{0}\) is held constant versus \(n\). We prove our results through a novel application of the hemisphere transform.

## 1 Introduction

A popular approach for studying the optimization dynamics of neural networks is analyzing the neural tangent kernel (NTK), which corresponds to the Gram matrix obtained from the Jacobian of the network parametrization map (Jacot et al., 2018). When the network parameters are adjusted by gradient descent, the network function follows a kernel gradient descent in function space with respect to the NTK. By bounding the smallest eigenvalue of the NTK away from zero it is possible to obtain global convergence guarantees for gradient descent parameter optimization (Du et al., 2019; Oymak and Soltanolkotabi, 2020) as well as results on generalization (Arora et al., 2019; Montanari and Zhong, 2022) and data memorization capacity (Montanari and Zhong, 2022; Nguyen et al., 2021; Bombari et al., 2022). These key advances highlight the importance of deriving tight, quantitative bounds for the smallest eigenvalue of the NTK at initialization.

While initial breakthroughs on the convergence of gradient optimization in neural networks (Li and Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019) required unrealistic conditions on the width of the layers, subsequent and substantive efforts have reduced the level of overparametrization required to ensure that the NTK is well conditioned at initialization (Zou and Gu, 2019; Oymak and Soltanolkotabi, 2020). In particular, Nguyen (2021); Nguyen et al. (2021); Banerjee et al. (2023) showed that layer width scaling linearly in the number of training samples \(n\) suffices to bound the smallest eigenvalue and Montanari and Zhong (2022); Bombari et al. (2022) obtained results for networks with sub-linear layer width and the minimum possible number of parameters \((n)\) up to logarithmic factors. However, and as discussed in Section 2, the bounds provided in prior works require that the data is drawn from a distribution satisfying a Lipschitz concentration property, and only hold with high probability if the input dimension \(d_{0}\) scales as \(\)(Bombari et al., 2022) or \((n)\)(Nguyen et al., 2021). These existing results therefore require that the dimension of the data grows unbounded as the number of training samples \(n\) increases and as such there is a gap in our understanding of cases where the data is sampled from a fixed, or lower-dimensional space.

In this work we present new lower and upper bounds on the smallest eigenvalue of a randomly initialized, fully connected ReLU network: compared with prior work, our results hold for arbitrarydata on a sphere of arbitrary dimension. Our techniques are novel and rely on the hemisphere transform as well as the addition formula for spherical harmonics.

We study neural networks denoted as functions \(f:^{d_{0}}\), where \(\) is an inner product space. To be clear, \(f(;)\) denotes the output of the network for a given input \(^{d_{0}}\) and parameter choice \(\). For brevity we occasionally write \(f()\) in place of \(f(;)\) if the context is clear. We use \(n\) to denote the size of the training sample, \(d_{0}\) the dimension of the input features, \(L\) the network depth, \(d_{l}\) the width of the \(l\)th layer and \(:\) the ReLU activation function. Given \(n\) input data points \(_{1},,_{n}^{d_{0}}\) we write \(=[_{1},,_{n}]^{d_{0} n}\) and define \(F:^{n}\) to be the evaluation of the network on these \(n\) data points as a function of the parameter \(\),

\[F()=[f(_{1};),,f(_{n};)]^{T}.\]

We define the neural tangent kernel (NTK) of \(F\) as

\[()=(_{}F())^{*}(_{}F())^{n n},\] (1)

where the gradient \(\) and adjoint \(*\) are taken with respect to the inner product on \(\) and the Euclidean inner product on \(^{n}\). More explicitly \([()]_{ik}=_{}f(_{i};),_{}f(_{k};)\). For convenience we write \(\) in place of \(()\). We are concerned with the minimum eigenvalue \(_{}()\), which depends both on the input data \(\) and the parameter \(\). We say the dataset \(_{1},,_{n}\) is \(\)_-separated_ for \((0,]\) if \(_{i k}(\|_{i}-_{k}\|,\|_{i}+_{k}\|)\), which is a measure of distance in direction.

Main contributions.Our results are for data that lies on a sphere and is \(\)-separated for some \((0,]\). Unlike prior work we do not make any assumptions on the distribution from which the data is sampled, e.g., uniform on the sphere or Lipschitz concentrated, and we do not require the input dimension \(d_{0}\) to scale with the number of samples \(n\).

* In Theorem 1 we consider shallow ReLU networks with input dimension \(d_{0}\) and hidden width \(d_{1}\) and prove that if \(d_{1}=(\|\|^{2}d_{0}^{3}^{-2})\) then with high probability \(_{}()=(d_{0}^{-3}^{2})\). Furthermore, defining \(^{}=_{i k}\|_{i}-_{k}\|\), we have \(_{}()=O(^{})\).
* In Theorem 8 we illustrate how our results for shallow networks can be extended to cover depth-\(L\) networks. In particular, if the layer widths satisfy a pyramidal condition, meaning \(d_{l} d_{l+1}\) for \(l\{1,,L-1\}\), \(d_{L-1} 2^{L}(nL/)\) and \(d_{1}=(nd_{0}^{3}^{-4})\), then \(_{}()=(d_{0}^{-3}^{4})\) and \(_{}()=O(L)\) with high probability.
* Our results allow us to analyze the smallest eigenvalue of the NTK for data drawn from any distribution for which one can establish \(\)-separation with high probability in terms of \(d_{0}\) and \(n\). For example, for shallow networks with data drawn uniformly from a sphere, in Corollary 2 we show that if \(d_{0}d_{1}=(n^{1+4/(d_{0}-1)})\), then with high probability \(_{}()=(n^{-2/(d_{0}-1)})\) and \(_{}()=(n^{-4/(d_{0}-1)})\). Moreover, this bound is tight up to logarithmic factors for \(d_{0}=((n))\) matching prior findings for this regime.

The rest of this paper is structured as follows: in Section 2 we provide a summary of related works and compare and contrast our results with the existing state of the art; in Section 3 we present our results for shallow networks; finally in Section 4 we extend our shallow results to the deep case.

Notations.With regard to general points on notation we let \([n]=\{1,2,,n\}\) denote the set of the first \(n\) positive integers. If \(^{d}\) then we let \([]_{i}\) denote the \(i\)th entry of \(\). If \(f\) and \(g\) are real-valued functions, we write \(f g\) or \(f=O(g)\) when there exists an absolute constant \(C\) such that \(f(x) Cg(x)\) for all \(x\). Similarly, we write \(f g\) or \(f=(g)\) when there exists a constant \(c\) such that \(f(x) cg(x)\) for all \(x\). We write \(f g\) when \(f g\) and \(f g\) both hold. The notation \(\) hides logarithmic factors. Logarithms are generally considered to be in base \(e\), though in most settings the particular choice of base can be absorbed by a constant.

## 2 Related work

Prior work on the NTK.Jacot et al. (2018) highlight that the optimization dynamics of neural networks are controlled by the Gram matrix of the Jacobian of the network function, an object referred to as the NTK Gram matrix, or, as we refer to it here, simply the NTK. That work also shows that in the infinite-width limit the NTK converges in probability to a deterministic kernel. Of particular interest is the observation that in the infinite-width setting the network behaves like a linear model (Lee et al., 2019). Further, if a network is polynomially wide in the number of samples then the smallest eigenvalue of the NTK can be lower bounded in terms of the smallest eigenvalue of its infinite-width analog. As a result, assuming the latter is positive, global convergence guarantees for gradient descent can be obtained (Du et al., 2019, 2019, 2019, 2019, 2020, 2020, 2021, 2020, 2021, 2022, 2023). The positive definiteness of the NTK is equivalent to the Jacobian having full rank, which can also be used to study the loss landscape (Liu et al., 2020, 2022, 2023). Beyond the smallest eigenvalue, there is interest in characterizing the full spectrum of the NTK (Basri et al., 2019, 2020, 2021, 2023), which has implications on the dynamics of the empirical risk (Arora et al., 2019, 2021) as well as the generalization error (Cao et al., 2021, 2020, 2021, 2022, 2022, 2023). Finally, although a powerful and successful tool for analyzing neural networks it must be noted that the NTK has limitations, most notably perhaps that it struggles to explain the rich feature learning commonly observed in practice (Lee et al., 2020, 2021, 2020).

Prior work on the smallest eigenvalue of the NTK.Many of the prior works discussed so far assume or prove that \(_{}()\) is positive, but do not provide a quantitative lower bound. Here we discuss works seeking to address this issue and to which we view our work as complementary. For shallow ReLU networks and data drawn uniformly from the sphere, Xie et al. (2017, Theorem 3) and Montanari and Zhong (2022, Theorem 3.2) provide lower bounds on the smallest singular and eigenvalue value of the Jacobian and NTK respectively. In addition to requiring the data to be drawn uniform from the sphere both of these results are high dimensional in the sense that for Xie et al. (2017, Theorem 3) to be non-vacuous it is necessary that \(d_{0}=(d_{1}n^{2})\), while Montanari and Zhong (2022, Theorem 3.2) requires, as per their Assumption 3.1, that \(d_{0}=()\).

Nguyen et al. (2021, Theorem 4.1) derives lower and upper bounds for the smallest eigenvalue of the NTK for deep ReLU networks under standard initialization conditions assuming the data is drawn from a distribution satisfying a Lipschitz concentration property. They show that the NTK is well conditioned if the network has a layer of width of order equal to the number of data points \(n\) up to logarithmic factors. Concretely, if at least one layer has width linear in \(n\) (ignoring logarithmic factors) and the others are at least poly-logarithmic in \(n\), then \(_{}()=(_{r}^{2}()d_{0})\) (or \((_{r}^{2}())\) with normalized data), where \(_{r}()\) denotes the \(r\)th Hermite coefficient of \(\) with any even integer \(r 2\). However, in their result the bound holds with high probability only if \(d_{0}\) scales as \((n)\).

Bombari et al. (2022, Theorem 1) derive lower and upper bounds for the smallest eigenvalue of the NTK under similar conditions as Nguyen et al. (2021, Theorem 4.1) aside from the following: they consider smooth rather than ReLU activation functions, the widths follow a loose pyramidal topology, meaning \(d_{l}=O(d_{l-1})\) for all \(l[L-1]\), \(d_{L-1}d_{L-2}\) scales linearly in \(n\) (ignoring logarithmic factors), and there exists a \(>0\) such that \(n^{}=O(d_{L-1})\). Under these conditions they show that \(_{}()=(d_{L-1}d_{L-2})\) with high probability as both \(d_{L-1}\) and \(n\) grow. This result illustrates that for the NTK to be well conditioned it suffices that the number of neurons grows as \(()\). The loose pyramidal condition on the widths implies \(d_{L-1}d_{L-2}=O(d_{0}^{2})\) and as they also assume that \(n=o(d_{L-1}d_{L-2})\) then \(n=o(d_{0}^{2})\) which in turn implies \(d_{0}=()\).

The rough strategy used by both Bombari et al. (2022) and Nguyen et al. (2021), as well as in our own results, can be described in terms of two main steps. In the first step, one bounds the smallest eigenvalue of a shallow network. The results for the shallow case can then be extended to the deep case, e.g., via a layerwise decomposition of the NTK matrix. This second step is architecture-dependent and its proof depends on the bounds derived in the first step. Our results focus on improving the first step which imply corresponding improvements for the second step.

## 3 Shallow networks

Here we study the smallest eigenvalue of the NTK of a shallow neural network. The parameter space \(\) of this network is \(^{d_{1} d_{0}}^{d_{1}}\) and it is equipped with the inner product

\[(,),(^{},^{})=( ^{T}^{})+^{T}^{}.\]For convenience we sometimes write \(d=d_{0}\). The neural network \(f:^{d_{0}}\) is defined as

\[f(;,)=}}_{j=1}^{d_{1}}v_{j}(_{j}^{T}),\] (2)

where \(=[_{1},,_{d_{1}}]^{T}^{d_{1} d_{0}}\) are the inner layer weights, \(=[v_{1},,v_{d_{1}}]^{T}^{d_{1}}\) the outer layer weights, and \(=(,)\). We consider the ReLU activation function applied entrywise with \((z)=\{0,z\}\). The derivative \(\) satisfies \((z)=1\) for \(z>0\) and \((z)=0\) for \(z<0\). Although \(\) is not differentiable at 0, we take \((0)=0\) by convention. Unless otherwise stated we assume that the entries of \(\) and \(\) are drawn mutually iid from a standard Gaussian distribution \((0,1)\). Our main result for shallow networks is the following theorem.

**Theorem 1**.: _Let \(d 3\), \((0,1)\), and \(,^{}(0,)\). Suppose that \(_{1},,_{n}^{d-1}\) are \(\)-separated and \(_{i k}\|_{i}-_{k}\|^{}\). Define_

\[=(1+)^{-3}^{2}.\]

_If \(d_{1}\|^{2}}{}\), then with probability at least \(1-\),_

\[_{}()^{}.\]

A proof of Theorem 1 is provided in Appendix C.7. Suppressing logarithmic factors, Theorem 1 implies that \(d_{1}=(\|\|^{2}d_{0}^{3}^{-2})\) suffices to ensure that \(_{}()=(d_{0}^{-3}^{2})\) and \(_{}()=O(^{})\) with high probability (note the trivial bound \(\|\|^{2}\|\|_{F}^{2} n\)). We emphasize that unlike existing results i) we make no distributional assumptions on the data, instead only assuming a milder \(\)-separated condition, and ii) our bounds hold with high probability even if \(d_{0}\) is held constant.

A few further remarks are in order. First, the condition \(d_{0} 3\) is necessary because our technique relies on the addition formula for spherical harmonics (Efthimiou & Frye, 2014, Theorem 4.11); the bound we derive based on this formula (Lemma 15 in Appendix A.2) becomes vacuous for \(d_{0}<3\). However, for \(d_{0}=2\) analogous bounds could be derived using more elementary tools while the case \(d_{0}=1\) is of little interest as only a trivial dataset is possible. Moreover, data in \(^{1}\) could be embedded in \(^{2}\) since we do not impose any distributional assumptions.

Second, one can use Theorem 1 to bound the smallest eigenvalue of the NTK for data drawn from the uniform distribution on the sphere by bounding \(\) with high probability in terms of \(n\) and \(d\). We use that \(=(n^{-2/d_{0}})\) and \(^{}=O(n^{-2/d_{0}})\) with high probability. We direct the interested reader to Appendix C.8 for further details.

**Corollary 2**.: _Let \(d 3\), \(n 2\), \((0,1)\), \(_{1},,_{n} U(^{d-1})\) be mutually iid. Define_

\[=(1+)^{-3}(}{n^{4}})^{1/(d-1)}.\]

_If \(d_{1}(1+) \), then with probability at least \(1-\) over the data and network parameters,_

\[_{}()(})^{1/(d-1)}.\]

The above corollary implies that if \(d_{0}d_{1}=(n^{1+4/(d_{0}-1)})\), then with high probability \(_{}()=(n^{-4/(d_{0}-1)})\) and \(_{}()=(n^{-2/(d_{0}-1)})\). In particular, for data sampled uniformly from a sphere, the scaling \(d_{0}=( n)\) is both necessary and sufficient for \(_{}()\) to be \((1)\). In particular the bounds are sharp in this case.

### Proof outline for Theorem 1

Recall the definitions of \(F()\) and \(\) in (1). For the choice of \(f\) given in (2), a straightforward decomposition of the NTK with respect to the inner and outer weights gives

\[=_{1}+_{2},\] (3)where \(_{1}=_{}F()^{*}_{}F()\) and \(_{2}=_{}F()^{*}_{}F()=}()^{T}()\). As both \(_{1}\) and \(_{2}\) are positive semi-definite,

\[_{}()_{}(_{1})+_{}(_{2});\] (4)

see, e.g., Horn & Johnson (2012, Theorem 4.3.1). Our proof now follows the highlighted steps below.

1) Bound the smallest eigenvalue in terms of the infinite-width limit.We proceed to bound both \(_{}(_{1})\) and \(_{}(_{2})\) in terms of the smallest eigenvalues of their infinite-width counterparts, see Lemmas 3 and 4 below, which act as good approximations for sufficiently wide networks.

**Lemma 3**.: _Suppose that \(_{1},,_{n}^{d-1}\). Let_

\[_{1}=_{}(_{ U(^{d-1})} [(^{T})(^{T} )]).\]

_If \(_{1}>0\) and \(d_{1}_{1}^{-1}\|\|^{2}\), then with probability at least \(1-\)_

\[_{}(_{1})_{1}.\]

**Lemma 4**.: _Suppose that \(_{1},,_{n}^{d-1}\). Let_

\[_{2}=d_{}(_{ U(^{d-1})} [(^{T})(^{T})]).\]

_If \(_{2}>0\) and \(d_{1}}(}) ()\), then with probability at least \(1-\)_

\[_{}(_{2})_{2}.\]

We prove Lemmas 3 and 4 in Appendices C.1 and C.2 respectively. Observe that while the parameters of the model are initialized as Gaussian, the expectations above are taken with respect to the uniform measure on the sphere. The motivation for using the uniform measure on the sphere is that it enables us to work with spherical harmonics, for which there is the highly useful _addition formula_ (see, e.g., Efthimiou & Frye, 2014, Theorem 4.11). The exchange of measures is possible in the case of Lemma 3 due to the scale invariance of \(\), while for Lemma 4 it is possible because \(\) is homogeneous.

2) Interpret the infinite-width kernel in terms of a hemisphere transform.Next, for a given \(\) and \(\{,\}\) we define the limiting NTK \(_{}^{}^{n n}\) as

\[_{}^{}=_{ U(^{d-1})}[ (^{T})(^{T})].\] (5)

Consider a fixed vector \(^{n-1}\) and interpret the Euclidean inner product \((^{T}),\) as a function of \(^{d-1}\). It will prove useful to think of this map as an integral transform. To this end let \((^{d-1})\) denote the vector space of signed Radon measures on \(^{d-1}\) and fix \(\{,\}\). For a signed Radon measure \((^{d-1})\) we introduce the integral transform \(T_{}:^{d-1}\), defined as

\[(T_{})()=_{^{d-1}}(,)d ().\] (6)

Note for \(\{,\}\) this is a _hemisphere transform_(Rubin, 1999) as the integrand \((,)\) is supported on a hemisphere normal to \(\). We provide background material on the hemisphere transform in Appendix B. Let \(_{}\) denote the space of signed Radon measures supported on the data set \(\{_{1},,_{n}\}\). For each measure \(_{}\) there exists a vector \(^{n}\) such that \(=_{i=1}^{n}z_{i}_{_{i}}\), where \(_{}\) is the Dirac measure supported on \(\). We write \(=_{}\) to indicate this correspondence. The following lemma relates the smallest eigenvalue of \(_{}^{}\) to the norm of the hemisphere transform of a measure supported on the data; a proof is provided in Appendix C.3.

**Lemma 5**.: _Fix \(^{d n}\) and \(\{,\}\). For all \(^{n}\), \(_{}^{},=\|T_{}_{}\|^{2}\). Moreover,_

\[_{}(_{}^{})=_{\|\|=1}\|T_{}_{} \|^{2}.\]3) Bound the hemisphere transform norm via spherical harmonics.We proceed to lower bound \(\|T_{}_{}\|^{2}\) for all \(^{d}\). Let \(L^{2}(^{d-1})\) denote the Hilbert space of real-valued, square-integrable functions with respect to the uniform probability measure on \(^{d-1}\), and let \((^{d-1}) L^{2}(^{d-1})\) denote the subspace of continuous functions. For \((^{d-1})\) and \(g(^{d-1})\) we define

\[,g:=_{^{d-1}}g()d().\]

If \(g_{1},,g_{N} L^{2}(^{d-1})\) are orthonormal, in particular consider \(g_{r}\) as spherical harmonics, then via a Bessel inequality

\[\|T_{}_{}\|^{2}_{a=1}^{N}| T_{}_{},g_ {a}|^{2}=_{a=1}^{N}|_{},T_{}g_{a}|^{2}= _{a=1}^{N}|_{i=1}^{n}(T_{}g_{a})(_{i})z_{i}|^{2}.\]

Importantly, \(T_{}\) is self-adjoint (see Lemma 17 in Appendix B for details) and the spherical harmonics are eigenfunctions of \(T_{}\), i.e., \(T_{}g_{a}=_{a}g_{a}\). A summary of the key properties of spherical harmonics needed for our results are provided in Appendix A.2. Therefore

\[\|T_{}_{}\|^{2}_{a=1}^{N}|_{i=1}^{n}(T_{}g_{ a})(_{i})z_{i}|^{2}=_{a=1}^{N}_{a}^{2}|_{i=1}^{n}g_ {a}(_{i})z_{i}|^{2}_{a}_{a}^{2}\|\|_{2}^ {2},\]

where \(^{N n}\) is a matrix with entries \([]_{ai}=g_{a}(_{i})\). As a result

\[_{}(_{}^{})_{a}_{a}^{2}_{} ^{2}().\]

4) Bound the hemisphere transform and spherical harmonics on the data.The following result shows that if we let the functions \((g_{a})_{a[N]}\) be spherical harmonics and allow \(N\) to be sufficiently large, then we can bound the minimum singular value of \(\). In what follows let \(_{r}^{d}\) denote the vector space of degree-\(r\) harmonic homogeneous polynomials on \(d\) variables.

**Lemma 6**.: _Suppose \(_{1},,_{n}^{d-1}\) are \(\)-separated. Suppose that \(\{0,1\}\) and that \(R_{ 0}\) are such that \(N:=_{r=0}^{R}(_{2r+}^{d})\) satisfies \(N C(}{2})^{-(d-2)/2}\) where \(C>0\) is a universal constant. Let \(g_{1},,g_{N}\) be spherical harmonics which form an orthonormal basis of \(_{r=0}^{R}_{2r+}^{d}\). If \(^{N n}\) is defined as \(_{ai}=g_{a}(_{i})\) then \(_{}()}\)._

A proof of Lemma 6 can be found in Appendix C.4. By carefully choosing values for \(R\) and \(N\) in Lemma 6 and performing some asymptotics on the resulting expressions, we arrive at the following bound on the hemisphere transform of a measure.

**Lemma 7**.: _Let \(d 3\) and suppose that \(_{1},,_{n}^{d-1}\) are \(\)-separated. For all \(^{n}\) with \(\|\| 1\) then_

\[\|T_{}_{}\|^{2}(1+ { d})^{-3}^{2}&=\\ (1+)^{-3}^{4}&= .\]

A proof of Lemma 7 is provided in Appendix C.5. The lower bound of Theorem 1 follows by bounding \(_{1}\), as defined in Lemma 3, using Lemma 7.

Before proceeding to the upper bound, we pause to remark on the generality of this argument for handling other activation functions. First, we use the positive homogeneity of the activation function in order to write \(_{}(_{}^{})\) as the \(L^{2}(^{d-1})\) norm of a function on the sphere. This is beneficial as it allows us to work with the spherical harmonics and use the associated addition formula. The ReLU activation and its derivative are also convenient with regard to computing the eigenvalues of the hemisphere transform (or more generally the eigenvalues of the integral operator). In particular, this requires evaluating integrals against Gegenbauer polynomials for which analytic expressions are available. For polynomial or piecewise polynomial activations similar results could be obtained. However, for other activations, e.g., \(\) or sigmoid, such quantities appear challenging to compute.

5) Upper bound.The upper bound of Theorem 1 is simpler than the lower bound and hinges on the following calculation. Let \(_{i},_{k}\) be two data points. Then

\[_{}()(_{i}-_{k})^{T}(_{i }-_{k})=\|_{}f(_{i})-_{}f(_{k})\|^{2}.\]

Therefore it suffices to upper bound the norm of \(_{}f(_{i})-_{}f(_{k})\). We choose \(i,k[n]\) such that \(_{i},_{k}\) are the two closest points in the dataset. We then translate this into a statement about the gradients. If \(\|_{i}-_{k}\|\), then with high probability over the network parameters, \(\|_{}f(_{i})-_{}f(_{k})\|^{2} \) (see Lemma 29), and we arrive at the desired upper bound in Theorem 1.

## 4 From shallow to deep neural networks

Our goal here is to detail just one approach as how the results of Section 3 can be extended to deep networks. To be clear, here we consider a fully connected network with input dimension \(d_{0}\) and \(L\) layers, where each layer has width \(d_{1},,d_{L}\) respectively and \(d_{L}=1\). The parameter space \(\) is a product space of matrices \(_{l=1}^{L}^{d_{l} d_{l-1}}\), equipped with the inner product

\[(_{1},,_{L}),(_{1}^{},,_{L} ^{})=_{l=1}^{L}(_{l}^{T}_{l}^{ }).\]

The feature maps \(f_{l}:^{d_{0}}^{d_{l}}\) of the neural network are given by

\[f_{l}(;)=&l=0\\ (_{l}f_{l-1}(;))&l[L-1]\\ _{l}f_{l-1}(;)&l=L,\]

where \(_{l}^{d_{l} d_{l-1}}\) for all \(l[L]\), \(=(_{1},,_{L})\) and \(\) is the ReLU function \(x(0,x)\) applied elementwise. We define the network map \(f\) to be the final feature map multiplied by a normalizing constant:

\[f=(_{l=1}^{L-1}}})f_{L}.\] (7)

Given \(n\) data points \(_{1},,_{n}\), we bound the smallest eigenvalue of the NTK (1) associated with this particular choice of \(f\).

**Theorem 8**.: _Suppose \((0,1/3)\), \((0,]\), \(d_{0} 3\), the data \(_{1},_{2},,_{n}^{d_{0}-1}\) is \(\)-separated and define_

\[=(1+(1/)}{ d_{0}})^{-3}^{4}.\]

_With regard to the network architecture, let \(L 3\), \(d_{l} d_{l+1}\) for all \(l[L-1]\), \(d_{L-1} 2^{L}()\) and \(d_{1}()( )\). Then with probability at least \(1-\) over the network parameters_

\[_{}() L.\]

We emphasize that these bounds make no distributional assumptions on the data other than lying on the sphere and being \(\)-separated; in particular, they hold even for constant \(d_{0}\). Indeed, if we consider \(d_{0}\) as some constant then Theorem 8 implies that if the first layer is sufficiently wide, \(d_{1}=(n^{-4})\), then with high probability over the parameters \(_{}()=(^{4})\) and \(_{}()=O(1)\).

A few remarks are in order. First, the pyramidal condition on the network widths could be relaxed by more directly borrowing techniques from Nguyen et al. (2021). We adopt this condition as it has the advantage of making the dependence of our bounds on the network depth \(L\) clearer. Second, compared with Theorem 1 and ignoring log factors, we observe the lower bound differs by a factor of \(^{2}\). This arises as a result of the smallest eigenvalue of the feature Gram matrix \(_{1}^{T}_{1}\) being equivalent to the Jacobian of a shallow network with respect to the second layer weights, not the inner layer weights, which has a different lower bound as per Lemma 7. For reasons apparent in the proof outline below the lower bound on \(_{}()\) lacks a dependency on \(L\), however we hypothesize it should also grow linearly with \(L\) thereby matching the dependency of the upper bound. Finally, the upper bound itself follows a similar approach as used by Nguyen et al. (2021) and is weak in the sense that we cannot take advantage of the dataset separation for gradients deeper into the network. We remark that this is also a common problem in the prior work of Nguyen et al. (2021) and Bombari et al. (2022), we refer the reader to the proof outline below for further details.

### Proof outline for Theorem 8

The proof of the deep case is structured around the decomposition of the NTK provided in Lemma 9 below. To state this decomposition we introduce the following quantities. For \(l[L-1]\) we define the feature matrices \(_{l}^{d_{l} n}\) by

\[_{l}=[f_{l}(_{1}),,f_{l}(_{n})].\]

For \(l[L-1]\) and \(^{d}\) we define the activation patterns \(_{l}()\{0,1\}^{d_{l} d_{l}}\) to be the diagonal matrices

\[_{l}()=((_{l}f_{l-1}())).\]

Finally, we let \(_{n}\) denote the vector of all ones in \(^{n}\).

**Lemma 9**.: _Let \(_{1},,_{n}^{d}\) be nonzero. There exists an open set \(\) of full Lebesgue measure such that \(f(_{i};)\) is continuously differentiable on \(\) for all \(i[n]\). Moreover, for all \(\) the NTK Gram matrix \(\) defined in (1) with network function (7) satisfies_

\[(_{l=1}^{L-1}}{2})\!=_{l=0}^{L-1}(_ {l}^{T}_{l})(_{l+1}_{l+1}^{T}),\]

_where the \(i\)th row of \(_{l}^{n n_{l}}\) is defined as_

\[[_{l}]_{i,:}=_{l}(_{i})(_{k=l+1} ^{L-1}_{k}^{T}_{k}(_{i}))_{L}^{T},&l[L-1 ],\\ _{n},&l=L.\]

For completeness we prove Lemma 9 in Appendix D.1. Observe each matrix summand in Lemma 9 is positive semi-definite (PSD) and recall for any two PSD matrices \(\) and \(\) one has \(_{}(+)_{}()+_{}()\) (see e.g. Horn & Johnson, 2012, Theorem 4.3.1) and \(_{}()_{}()_{i[n]}[]_{ii}\)(Schur, 1911). Therefore

\[(_{l=1}^{L-1}}{2})_{}()_{ l=0}^{L-1}_{}((_{l}^{T}_{l})(_{l+1} _{l+1}^{T}))_{}(_{1}^{T}_{1})_ {i[n]}\|[_{2}]_{i,:}\|^{2}.\]

In order to upper bound the smallest eigenvalue we follow Nguyen et al. (2021) and analyze the Raleigh quotient \(R()=^{T}}{\|\|^{2}}\). In particular, for any nonzero \(^{n}\) we have \(_{}() R()\) and therefore \(_{}() R(_{i})=[]_{ii}\) for all \(i[n]\). As a result

\[(_{l=1}^{L-1}}{2})_{}()[ _{l=0}^{L-1}(_{l}^{T}_{l})(_{l+1}_{l+1}^{T}) ]_{ii}=_{l=0}^{L-1}\|f_{l}(_{i})\|^{2}\|[_{l+1}]_{i,:}\| ^{2}.\]

Combining the upper and lower bounds we have

\[_{}(_{1}^{T}_{1})_{i[n]}\|[_{2 }]_{i,:}\|^{2}_{}()(_{l=1}^{L-1}}{2} )_{l=0}^{L-1}\|f_{l}(_{i})\|^{2}\|[_{l+1}]_{i,:}\|^{ 2},\] (8)

where the right hand side holds for any \(i[n]\). Based on (8), we proceed first by bounding the norm of the network features. We achieve this via an inductive argument, bounding the norm of the features at one layer with high probability, and then conditioning on this event to bound the norm of the features at the next layer with high probability.

**Lemma 10**.: _Let \(^{d_{0}-1}\), \(L 2\) and \(l[L-1]\). If \(d_{k} l^{2}(l/)\) for all \(k[l]\), then_

\[e^{-1}(_{h=1}^{l}}{2})\|f_{l}()\|^{2} e (_{h=1}^{l}}{2})\]

_holds with probability at least \(1-\) over the network parameters._A proof of Lemma 10 is provided in Appendix D.2. Next we derive upper and lower bounds on the backpropagation terms \([_{l}]_{i,:}\). Our strategy for this is as follows: for \(l[L-2]\), let \(_{l}()=_{l}()(_{k=l+1}^{L-1}_{k}^{T} _{k}())\) and observe

\[[_{l}]_{i,:}=_{l}(_{i})_{L}^{T}.\]

Since \(_{i}^{d_{0}-1}\), it is sufficient to lower bound \(\|_{l}()_{L}^{T}\|_{2}^{2}\) for an arbitrary \(^{d_{0}-1}\). As the vector \(_{L}^{T}^{d_{L-1}}\) is distributed as \(_{L}^{T}(_{d_{L-1}},I_{d_{L-1}})\), following Vershynin (2018, Theorem 6.3.2) we have that for any \(^{d_{l} d_{L-1}}\) and \(t 0\)

\[(\|_{L}^{T}\|-\|\|_{F} t) 2(- }{\|\|^{2}})\]

for some constant \(C>0\). As a result, with \(t=\|\|_{F}^{2}\) then

\[(\|\|_{F}^{2}\|_{L}^{T}\|^{2} \|\|_{F}^{2}) 1-(-C\|_{F} ^{2}}{\|\|^{2}}).\]

In order to lower bound \(\|_{l}()_{L}^{T}\|^{2}\) with high probability over the parameters it therefore suffices to condition on appropriate bounds for \(\|_{l}()\|_{F}^{2}\) and \(\|_{l}()\|_{2}^{2}\). These bounds are provided in Lemmas 34 and 35 in Appendices D.3 and D.4 respectively. With these two lemmas in place we can bound \(\|_{l}(_{i})_{L}^{T}\|^{2}\).

**Lemma 11**.: _Let \(^{d_{0}-1}\), suppose \(L 3\), \(d_{k} d_{k+1}\) for all \(k[L-1]\) and \(d_{L-1} 2^{L}()\). Then, for any \(l[L-1]\), with probability at least \(1-\) over the network parameters_

\[\|_{l}()_{L}^{T}\|^{2} 2^{-L+l+1}_{k=l}^{L-1}d_{k}.\]

By combining Lemma 11 with a union bound we arrive at the following corollary, relevant for the lower bound of (8).

**Corollary 12**.: _Let \(_{i}^{d_{0}-1}\) for all \(i[n]\), \(L 3\), \(d_{l} d_{l+1}\) for all \(l[L-1]\) and \(d_{L-1} 2^{L}()\). Then, for any \(l[L-1]\), with probability at least \(1-\) over the network parameters_

\[_{i[n]}\|[_{2}]_{i,:}\|^{2} 2^{-L}_{k=2}^{L-1}d_{k}.\]

The first-layer feature Gram matrix \(_{1}^{T}_{1}\) in the deep case is identically distributed to \(_{2}\) in the two-layer case; see (3) and the related definitions. Therefore we can apply Lemma 4 to lower bound the smallest eigenvalue of \(_{1}^{T}_{1}\). This, in combination with Corollary 12, yields the lower bound of Theorem 8. The upper bound follows by combining the bound on the feature norms provided by Lemma 10 with the bound on the backpropagation terms given in Lemma 11. A detailed proof of Theorem 8 is provided in Appendix D.6.

## 5 Conclusion

Summary and implications.Quantitative bounds on the smallest eigenvalue of the NTK are a critical ingredient for many current analyses of network optimization. Prior works provide bounds which are only applicable for data drawn from particular distributions and for which the input dimension \(d_{0}\) scales appropriately with the number of data samples \(n\). This work plugs an important gap in the existing literature by providing bounds for arbitrary datasets on the sphere (including those drawn from any distribution on the sphere) in terms of a measure of distance between data points. Furthermore, these bounds are applicable for any \(d_{0}\), in particular even \(d_{0}\) held constant with respect to \(n\).

Limitations.Our bounds currently only hold for the ReLU activation function. Another limitation, also present in prior work, is that our upper bound on the smallest eigenvalue of the NTK for deep networks in Theorem 8 does not capture the data separation. Finally, a mild limitation of this work is that we require the data to be normalized so as to lie on the sphere.

Future work.The proof techniques developed here could be applied to analyze the NTK in the context of other homogeneous activation functions. One could potentially relax the homogeneity condition on the activation function, or the condition of unit norm data, by considering an integral transform on the space \(L^{2}(^{d},)\) rather than \(L^{2}(^{d-1})\), where \(\) denotes the standard Gaussian measure (since the weights are drawn from a Gaussian distribution). Beyond fully connected networks, conducting comparable analyses in the context of other architectures, e.g., CNNs, GNNs, or transformers, would be valuable future work.

#### Acknowledgment

This project has been supported by NSF CAREER 2145630, NSF 2212520, DFG 464109215 within SPP 2298 Theoretical Foundations of Deep Learning, and BMBF in DAAD project 57616814.