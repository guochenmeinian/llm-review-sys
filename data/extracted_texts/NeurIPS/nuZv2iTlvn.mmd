# Non-Euclidean Mixture Model for Social Network Embedding

Roshni G. Iyer

Computer Science Department

University of California, Los Angeles

Los Angeles, California, USA

roshnigiyer@cs.ucla.edu

&Yewen Wang

Computer Science Department

University of California, Los Angeles

Los Angeles, California, USA

wyw10804@gmail.com

&Wei Wang

Computer Science Department

University of California, Los Angeles

Los Angeles, California, USA

weiwang@cs.ucla.edu

&Yizhou Sun

Computer Science Department

University of California, Los Angeles

Los Angeles, California, USA

yzsun@cs.ucla.edu

###### Abstract

It is largely agreed that social network links are formed due to either homophily or social influence. Inspired by this, we aim at understanding the generation of links via providing a novel embedding-based graph formation model. Different from existing graph representation learning, where link generation probabilities are defined as a simple function of the corresponding node embeddings, we model the link generation as a mixture model of the two factors. In addition, we model the homophily factor in spherical space and the influence factor in hyperbolic space to accommodate the fact that (1) homophily results in cycles and (2) influence results in hierarchies in networks. We also design a special projection to align these two spaces. We call this model Non-Euclidean Mixture Model, i.e., \(\). We further integrate \(\) with our non-Euclidean graph variational autoencoder (VAE) framework, \(\)-\(\). \(\)-\(\) learns embeddings through a unified framework which uses non-Euclidean GNN encoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novel space unification loss component to unify distinct non-Euclidean geometric spaces. Experiments on public datasets show \(\)-\(\) significantly outperforms state-of-the-art baselines on social network generation and classification tasks, demonstrating its ability to better _explain_ how the social network is formed.

## 1 Introduction

Social networks are omnipresent because they are used for modeling interactions among users on social platforms. Social network analysis plays a key role in several applications, including detecting underlying communities among users , classifying people into meaningful social classes , and predicting user connectivity . Most existing embedding models are designed based on the _homophily_ aspect of social networks [4; 5]. They utilize the intuition that associated nodes in a social network imply feature similarity, and an edge is usually generated between similar nodes. Prior works have used shallow embedding models to represent homophily, like matrix factorization and random-walk (Section 2), which are parameter intensive and do not employ message passing. As an improvement, graph neural network (GNN) models (Section 2) have been proposed to more effectively capture homophily by representing a node through its local neighborhood context.

However, research of **RaRE** and work of  show homophily is insufficient, and _social influence_ is also critical in forming connections. This is due to popular nodes having direct influence in forming links . For example, in Twitter network, users tend to follow celebrities _in addition to_ users who share similar interests . Though **RaRE** jointly models both factors, it has limitations in modeling capabilities. Specifically, it is parameter intensive as each node embedding is fully parameterized through a Bayesian framework. Further, **RaRE** assumes graphs are transductive, limiting its performance in the practical inductive setting where new links not seen during training must be predicted. Moreover, nearly all works embedding social networks utilize a single zero-curvature Euclidean space, when in reality, network factors may create different topologies. Specifically, edges generated by homophily tend to form cycles , while edges generated by social influence tend to form tree structures [11; 12]. From Riemannian geometry, people have found that networks with cycles are best represented by spherical space embeddings , while tree structured networks are best represented by hyperbolic space embeddings . Thus, an end-to-end model to bridge social network embeddings of distinct non-Euclidean geometric spaces is a promising direction.

Our motivation is two-fold: (1) We aim to _understand_ how the social network is generated e.g., which factors affect node connectivity and what topological patterns emerge in the network as a result. (2) Using our learning from (1), we aim to design a more realistic deep learning model to _explain_ how the network is generated (inferring new connections). We summarize our contributions as follows:

* We propose Graph-based Non-Euclidean Mixture Model (**NMM**) to explain social network generation. **NMM** represents nodes via joint influence by homophily (modeled in spherical space) and social influence (modeled in hyperbolic space), while seamlessly unifying embeddings via our space unification loss.
* To our knowledge, we are also the first to couple **NMM** with a graph-based VAE learning framework, **NMM-GNN**. Specifically, we introduce a novel non-Euclidean VAE framework where node embeddings are learned with a powerful encoder of GNNs using spherical and hyperbolic spaces, non-Euclidean Gaussian priors, and unified non-Euclidean optimization.
* Extensive experiments on several real-world datasets on large-scale social networks, Wikipedia networks, and attributed graphs demonstrate effectiveness of **NMM-GNN** in social network generation and classification, which outperforms state-of-the-art (SOTA) network embedding models.

## 2 Preliminary and Related Work

We provide an overview of social network embedding models and discuss advancements in non-Euclidean graph learning.

### Social Network Embedding

Several works that embed social networks merely model homophily , capturing node-node similarity, without also considering a node's social influence or node popularity e.g., a celebrity. Homophily-based models include shallow embedding models and GNN embedding models. Most shallow embedding models are either based on matrix factorization  or random-walk . Though GNN models  effectively learn on large networks and in inductive settings, they still fail to model the social influence factor in the network. Further, even the model that captures both homophily and social influence, **RaRE**, has limitations in that its fully-parameterized node embeddings require large parameter size to be learned, and it models all nodes in Euclidean space, an approach not effective in capturing different topologies (e.g., cycles and hierarchy) in the social network.

### Non-Euclidean Geometry for Graphs

Non-Euclidean geometric spaces, commonly used to model surfaces in mathematics and physics, are curved geometries that include spherical spaces with positive curvature, and hyperbolic spaces with negative curvature . Works have recently found Euclidean space modeling to be insufficient for non-Euclidean graph-structured data . Namely, spherical spaces have been shown to effectively embed graphs with cyclic structure due to their positive curvature [21; 13], while hyperbolic spaces have been shown to effectively embed graphs with hierarchical structure due to their negative curvature and exponential or "tree-like" growth of the space [22; 23]. HGCN  and  are critical works exploring GNNs in non-Euclidean spaces. Both these hyperbolic GNN methods have shown significant improvement on benchmark datasets by preserving hierarchical structure of graphs. In knowledge graphs,  has achieved notable performance by modeling graphs in non-Euclidean spaces of various curvatures, using both hyperbolic and spherical space.

## 3 Methodology

A social network \(G=(V,A)\) consists of a set of vertices \(V=\{v_{i}\}_{i=1}^{N}\), and associated adjacency matrix \(e_{ij} A\), where \(e_{ij}\) is an edge from \(v_{i}\) to \(v_{j}\). We aim to design a model to jointly learn both node homophily and social influence representation, denoted \(_{i}^{S}\) and \(_{i}^{H}\) respectively, that can best explain the social network in terms of link reconstruction. In this section, we describe our architecture. First, we introduce a novel non-Euclidean mixture model, called **NMM**, to model the probability of a new link. Second, we add a non-Euclidean GNN encoder to enrich **NMM**, called **NMM-GNN**, which is connected to the variational autoencoder framework. Source code is in the Appendix. For details on future directions, the reader is also referred to the Limitations section in the Appendix.

### Overview

To model both factors of homophily and social influence that affect graph connectivity, we model each node \(v_{i}\) with homophily regulated representation, \(_{i}^{S}\), and social influence regulated representation, \(_{i}^{H}\)1. A unified framework is designed such that \(_{i}^{S}\) and \(_{i}^{H}\) influence each other bi-directionally and are seamlessly merged through **NMM**. **NMM** utilizes non-Euclidean geometric spaces to better represent homophily and social influence components which produce curved structures like cycles and trees. To improve **NMM**, we enrich its encoder by GNNs, \(q_{v}(_{i}^{H}|G)\) and \(q_{}(_{i}^{S}|G)\). To ensure generated embeddings are in spherical and hyperbolic spaces respectively, non-Euclidean GNNs are adopted. The enhanced model is **NMM-GNN**, which has a clear connection to the VAE framework.

Figure 1 illustrates the architecture overview of **NMM-GNN**. Specifically, the encoder component maps nodes into homophily based embedding, \(^{S}\), in spherical space (for homophily generated cycles) and social influence based embedding, \(^{H}\), in hyperbolic space (for social influence generated trees), which follow non-Euclidean prior distributions. The embeddings are passed into our mixture model decoder, which models the probability of a link as a mixture of a homophily based distribution component and a social influence based distribution component. The objective is to maximize the likelihood to observe the links, or equivalently to minimize the link reconstruction loss. In addition, the two geometric spaces are ensured to be aligned together via a space unification regularization term, to make sure the two embeddings of the same node are corresponding to each other.

### Modeling via Non-Euclidean Mixture Model (NMM)

Geometrically, the space of **NMM** is a spherical surface inside a unit Poincare ball. Each node \(v_{i}\) corresponds to (1) a point in the Poincare ball, \(_{i}^{H}\), and (2) a point on the spherical surface,

Figure 1: Model Architecture. (a) Architecture overview of **NMM-GNN**, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of **NMM-GNN**\(_{i}^{H}\) from the hyperbolic space is projected to its corresponding point in the spherical space, \(_{i}^{S^{}}\), such that its geodesic distance is ensured to be close to \(v_{i}\)â€™s existing spherical space representation, \(_{i}^{S}\).

\(_{i}^{S}\), where both spaces are embedded inside a Euclidean space. These two points are aligned by enforcing projection of \(_{i}^{H}\) onto the spherical surface to be close to \(_{i}^{S}\), as projecting a star onto earth's atmosphere, shown in Fig. 1(b). We note that the space alignment does not mean embeddings are enforced to be the same in two spaces2. Rather, we require the projection of \(_{i}^{H}\) is close to \(i\)'s embedding in spherical space \(_{i}^{S}\). In this case, the two distances of spherical and hyperbolic spaces are also different from each other (norm space difference vs. spherical geodesic distance). Without the space alignment, \(_{i}^{H}\) has too much degree of freedom, which can move freely as long its norm is kept the same. Lastly, the probability of generating a link is a mixture of the probability of generating the link in each non-Euclidean space.

#### 3.2.1 Modeling for homophily

Representation of homophily regulated nodes.We embed homophily based representation of node \(v_{i}\), or \(_{i}^{S}\), on the surface of the spherical ball in spherical space, \(^{d}\). Formally, \(^{d}=\{_{i}^{S}^{d}_{i}^{S}\|=w^{S}\}\) is the \(d\)-dimensional \(w^{S}\)-norm ball, where \(\|\|\) is the Euclidean norm and \(w^{S}[0,1)\) is a constant to ensure the spherical surface is inside the unit Poincare ball. To better capture homophily distribution, \(p(_{i}^{S})\), we use spherical Gaussian distribution \(G_{S}()\) as spherical prior, described in Table 1. \(\) is the axis or _direction_ of the lobe controlling where the lobe is located on the sphere, and points towards the center of the lobe; \(\) is the _sharpness_ of the lobe such that as this value increases, the lobe will become narrower in width; and \(a\) is the amplitude or _intensity_ of the lobe, corresponding to the height of the lobe at its peak.

Link prediction using homophily based distribution.The probability of link \(e_{ij}=1\) between nodes \(v_{i}\) and \(v_{j}\), also described in Table 1, is determined by the geodesic distance between \(_{i}^{S}\) and \(_{j}^{S}\). \(_{s}(_{i}^{S},_{j}^{S})=((_{i}^{S},_{j}^{S}))\) is the geodesic distance between \(_{i}^{S}\) and \(_{j}^{S}\); \(J>0\) and \(B 0\) are model parameters; and \(,\) denotes the inner product. The probabilistic model is designed so nodes greatly dissimilar (exhibiting low homophily), or \(_{s}(_{i}^{S},_{j}^{S})+\), have low probability of a link being generated, or \(p_{}(e_{ij}=1) 0\). In the case of nodes exhibiting high homophily, they either (1) may be connected due to highly similar characteristics, or (2) may not be connected simply because the individuals do not know each other. Our distribution models both scenarios. Note when \(_{s}(_{i}^{S},_{j}^{S}) 0\), \(p_{}(e_{ij}=1)}\), which can be interpreted as a factor to control sparsity of the network.

#### 3.2.2 Modeling for social influence

Representation of social influence regulated nodes.We embed social influence based representations of node \(v_{i}\), or \(_{i}^{H}\), on the Poincare ball from hyperbolic space, \(^{d+1}\), to better capture resulting hierarchical structures. Social influence regulated nodes are represented as points, \(_{i}^{H}\), belonging inside the Poincare (open) ball in \(^{d+1}\). Formally, \(^{d+1}=\{_{i}^{H}^{d+1}\|_{i}^{H} \|=w_{z_{i}}^{H}\},w_{z_{i}}^{H}[0,1)\), is \((d+1)\)-dimensional \(w_{z_{i}}^{H}\)-norm ball, where \(\|\|\) is Euclidean norm. We assume center of the Poincare ball is aligned with center of the sphere, and is one dimension larger than the sphere to ensure the spherical surface is inside the Poincare ball. To better capture a social influence regulated distribution, \(p(_{i}^{H})\), we use Hyperbolic Gaussian distribution \(G_{H}()\) as non-Euclidean Gaussian prior, described in Table 2. \(_{i}^{H}}\) is the origin of \((r,)\) for radius \(r\) and angle \(\) in polar coordinates. \(_{i}^{H}}\) is the center of mass and \(>0\) is the dispersion parameter, where the dispersion dependent normalization constant \(Z()\) accounts for the underlying non-Euclidean geometry. \(Z()\) is partitioned into angular \(\) and radial \(r\) components. \(()\) is Euler's gamma function, and \(_{H}()\) is the hyperbolic distance between two hyperbolic space node embeddings, \(\) and \(\), where \(\|\|\) denotes Euclidean norm.

Table 1: Homophily regulated nodes: Distribution Prior and Link Generation ProbabilityLink prediction using social influence based distribution.We model existence of an edge, \(e_{ij}=1\) between nodes \(v_{i}\) and \(v_{j}\), also described in Table 2, as a function of norm space difference, \(}()\), between two hyperbolic space node embeddings, \(_{i}^{H}\) and \(_{j}^{H}\). We utilize norm space difference as opposed to hyperbolic distance, since nodes of similar social influence status may have large distance in the Poincare (open) ball due to nodes possibly being placed towards the ball's boundary. This is because, at the boundary of the ball, nodes become infinitely distanced apart. Thus, to allow for numerical stability and to capture social influence difference (in which the higher the social influence of nodes indicated by large in-degree and smaller out-degree, the closer they are embedded towards the center of the ball), we utilize norm space as indicator. Consistent with the notation of , a node with higher social influence is associated with a smaller norm value. \(C\) and \(D\) are learned model parameters and norm space of a vector is \(()=\|\|\). \(}(_{i}^{H},_{j}^{H})=|(_{i}^{H} )-(_{j}^{H})|\) is the norm difference between \(_{i}^{H}\) and \(_{j}^{H}\); \(C>0\) and \(D 0\) are model parameters; and \(()\) denotes the L1 normalization function. The probabilistic model is designed such that nodes largely different in popularity (exhibiting high social influence), or \(}(_{i}^{H},_{j}^{H})+\), have high probability of a link being generated, or \(p_{}(e_{ij}=1) 1\). In the case both nodes exhibit low social influence (such as having similar social rank), they either (1) may be connected due to highly similar characteristics, or (2) may not be connected simply because individuals do not know each other. Our distribution models both scenarios. When \(}(_{i}^{H},_{j}^{H}) 0\), \(p_{}(e_{ij}=1)}{1+e^{D}}\), which can be interpreted as another factor to control sparsity of the network.

#### 3.2.3 Non-Euclidean Mixture Model

Link Prediction Using Mixed Space Distribution.Since both homophily and social influence affect the connectivity structure of social networks, we model existence of a new link between nodes, \(p_{}(e_{ij}=1)\), as a weighted combination distribution of these factors. Specifically, our non-Euclidean mixture model is a weighted combination of homophily based distribution, \(p_{}(e_{ij}=1)\), and social influence based distribution, \(p_{}(e_{ij}=1)\), with learned weight \(\):

\[p_{}(e_{ij}=1) =p_{}(e_{ij}=1|_{i}^{S},_{j}^{S},_{i}^{H},_{j}^{H})\] \[= p_{}(e_{ij}=1)+(1-) p_{ }(e_{ij}=1)\] (6)

where \(p_{}(e_{ij}=1)\) is modeled in positively curved spherical space since homophily based links may form cycles due to similarity connections between node clusters. \(p_{}(e_{ij}=1)\) is modeled in negatively curved hyperbolic space since social influence based links may form tree-like structures due to popularity-based social hierarchy between node clusters. We would like to highlight that the link between nodes \(i\) and \(j\) is a mixture model because each link is a weighted combination of influence from both spherical and hyperbolic spaces (not one or the other) as evidenced in Equation 6. As shown in Figure 0(b), the same node has two representations - one in the spherical space and one in the hyperbolic space, and because they represent the same underlying node, they need to be aligned. Therefore, these two network factors do not contradict each other, but rather _work together_ to explain how links are formed between users.

Table 2: Social influence regulated nodes: Distribution Prior and Link Generation Probability

### Modeling via Non-Euclidean VAE on Graphs

We enrich the encoder of **NMM** to generate better embeddings. To do so, we explore GNN methods which have been shown to be more effective than shallow embedding methods (see Experiments and Ablation Studies). We refer to this enriched **NMM** model as **NMM-GNN**. **NMM-GNN** uses non-Euclidean VAE as its learning framework. The framework integrates a mixture of different non-Euclidean geometric spaces e.g., hyperbolic and spherical spaces, for learning of encoder, decoder, and node prior distributions, and ensures geometric spaces are unified during training.

Encoder model.The encoder learns two corresponding embedding representations per node \(v_{i}\) to produce spherical embedding \(_{i}^{S}\) and hyperbolic embedding \(_{i}^{H}\). For homophily regulated nodes in spherical space, \(^{d}\), any spherical space GNN (SGNN) can be applied, and for social influence regulated nodes in hyperbolic space, \(^{d+1}\), any hyperbolic space GNN (HGNN) can be applied. The general framework for SGNN and HGNN are shown in Tables 3 and 4. \(_{i}^{S^{(l)}}^{d^{(l)}}\) and \(_{i}^{H^{(l)}}^{d+1^{(l)}}\) are spherical and hyperbolic feature representations of node \(v_{i}\) at layer \(l\), with dimensionality \(d\) and \((d+1)\) respectively. \(f\) is a message-specific neural network function of incoming messages to \(v_{i}\) from neighborhood context \(N_{i}\), and activation function \(()\), typically \(()\) for all layers but the last one being \(()\).

Example Non-Euclidean Encoder models.We illustrate Non-Euclidean Graph Convolutional Neural Network (Non-Euclidean GCN) as an example GNN. Tables 3 and 4 describe the model architectures for both Spherical GCN and Hyperbolic GCN respectively. Node features are initialized by sampling each embedding dimension from a distribution uniformly at random for all nodes where \(_{i}^{S^{(0)}}^{d^{(l)}}([0,1))^{d}\) and \(_{i}^{H^{(0)}}^{d+1^{(l)}}([0,1))^{ d+1}\) respectively. The retraction operator, \(()\), involves mapping between spaces. For non-Euclidean spaces, retraction is performed between non-Euclidean space and approximate tangent Euclidean space using logarithmic and exponential map functions. Specifically, \(_{0}^{H}(_{i}^{H})=^{-1}(\|_{i}^{H}\|) _{i}^{H}}{\|_{i}^{H}\|}\) is a logarithmic map at center \(\) from hyperbolic space to Euclidean tangent space, and \(_{0}^{H}(_{i}^{H})=(\|_{i}^{H}\|)_{i}^{H^{}}}{\|_{i}^{H}\|}\) is an exponential map at center \(\) from Euclidean tangent space to hyperbolic space. \(_{0}^{S}(_{i}^{S})=^{-1}(\|_{i}^{S}\|)_{i}^{S }}{\|_{i}^{H}\|}\) is a logarithmic map at center \(\) from spherical space to Euclidean tangent space and \(_{0}^{S}(_{i}^{S})=(\|_{i}^{S}\|)_{i}^{S}}{ \|_{i}^{H}\|}\) is an exponential map at center \(\) from Euclidean tangent space to spherical space. where \(_{i}^{S^{(0)}}\), \(_{i}^{H^{(l)}}\) are embeddings of node \(v_{i}\) at layer \(l[0,L)\), \(L=2\); \(_{l}\) is a layer-specific learnable weight matrix; \(N_{i}\) is the set of nodes in the neighborhood context of \(v_{i}\); \(e_{j,i}\) is the edge-weight between nodes \(v_{j} v_{i}\), with default edge weight being \(1.0\) if an edge exists. \(m_{i}\),\(m_{j}\) are entries of the degree matrix, with \(m_{i}=1+_{j N_{i}}e_{j,i}\).

Decoder model.**NMM** can be considered as a probabilistic decoder for link generation, which maps embeddings of two nodes into the probablity to generate a link between them.

Table 4: General Framework and Example Model for Hyperbolic Graph Neural Network.

Table 3: General Framework and Example Model for Spherical Graph Neural Network.

Joint loss function.The training loss involves components of reconstruction loss (to ensure the generated graph is consistent with the original graph), KL divergence loss (to ensure predicted embeddings \(_{i}^{S}\) and \(_{i}^{H}\) closely match their non-Euclidean Gaussian distributions), and space unification loss (to ensure \(_{i}^{S}\) and \(_{i}^{H}\) map to the same node \(v_{i}\)).

Reconstruction Loss.Table 5 shows reconstruction loss, which minimizes the upper bound on the negative log-likelihood. \(_{A}\) is a hyperparameter; \(A^{}=XAX^{T}\), given \(X 0,1^{k d}\) where \(X_{a,i}=1\) only if node \(a\) is assigned to \(i G\) and \(X_{a,i}=0\) otherwise, where \(\) is the predicted graph.

KL Divergence Loss.The KL divergence loss is formed by minimizing the equations described in Table 6. Minimizing the KL divergence loss ensures that the homophily regulated nodes and social influence regulated nodes closely align to their underlying non-Euclidean distribution priors. As described in Section 3, these distributions are designed to appropriately capture the distinct topologies that emerge as a result of the respective social network factors.

Space Unification Loss.The space unification loss is formed by minimizing the equations described in Table 6. Minimizing the space unification loss ensures that the hyperbolic space representation of node \(v_{i}\) in spherical space, \(_{S}(_{i}^{H})\), is close to the corresponding learned representation of node \(v_{i}\) in the spherical space, \(_{i}^{S}\). Note that using a normalized hyperbolic disk is not a substitute for the projection operator from the social influence hyperbolic space onto the homophily spherical space. The projection operation _solely_ projects out-of-sphere nodes onto the \(w^{S}\) norm space, or the norm at the surface of the spherical ball. A normalization operator would instead change the embedding values of all nodes. More importantly, the space unification loss component ensures minimal spherical geodesic distance between \(_{i}^{H^{}}\)'s representation on the spherical ball and \(_{i}^{S}\), illustrated in Figure 1(b).

Total Loss.The overall loss function for homophily regulated and social influence regulated nodes respectively is then a summation of the above loss components given by:

\[L^{S}=L^{S}_{}(,;G)+L^{S}_{}(;G)+L_{ }(G)\] (15)

\[L^{H}=L^{S}_{}(,;G)+L^{H}_{}(;G)+L_{ }(G)\] (16)

### Training

This section details **NMM-GNN**'s training framework, using non-Euclidean VAE, for representing social networks. We describe the optimization method for variables from Sections 3.2 and 3.3.

Embedding Initialization.We randomly initialize all embeddings of \(_{i}^{S}\) and \(_{i}^{H}\). For homophily regulated nodes, we choose a value for norm \(w^{S}\), that is sampled from uniform distribution: \(w^{S}:w^{S}[0,1)([0,1))\) for all nodes, and for social influence regulated nodes, we choose a value for norm \(w_{i,}^{H}\), assigned uniformly at random per node. We set curvature values of spherical and hyperbolic spaces as \(K_{S}=1\) and \(K_{H}=-1\) respectively. We leave the non-trivial problem of learning optimal curvatures as future work.

Training procedure for homophily regulated nodes.Parameter optimization for learning node embeddings is performed using Riemannian stochastic gradient descent (RSGD) for the spherical space as shown in Table 7. To ensure the updated node embeddings remain in norm-\(w^{S}\) space, we perform a rescaling operation, \(_{S}\), to project out-of-boundary embeddings back to the surface of

Table 5: Description for Reconstruction Loss.

[MISSING_PAGE_FAIL:8]

nodes). In the extreme case (Case 1) where only social influence is at play, e.g., weight of homophily representation is learned close to 0, the hyperbolic space will be used. On the other hand if only homophily is at play (Case 2), e.g., weight of homophily representation is learned close to 0, the spherical space will be used. In the normal case of both factors at play (Case 3), then both spaces will be used and can be jointly aligned with our space alignment mechanism. When using product space, Cases 1, 2, and 3 will all not be distinguished from each other as all cases will be modeled by one complex non-Euclidean geometric space as a Cartesian product of spherical and hyperbolic spaces.

Time Complexity of NMM-GNN.Regarding our model, **NMM** is highly efficient, with time complexity \(O(ed+nd)\), where \(n\) is number of nodes, \(e\) is number of edges, and \(d\) is dimension size. In comparison, the time complexity analyses for the remaining baseline models are as follows: the mixture model of **RaRE** is \(O(ed+nd)\) which is comparable to the **NMM** mixture model, and the GNN embedding models of **GCN**, **GAT** (with one-head attention), and \(\)**-GCN** (for \(=0\)) are \(O(ed+nd^{2})\). \(\)**-GCN** (for \( 0\)) and **HGCN**'s time complexities are \(O(ed+a nd^{2})\), where \(a\) is the filter length, and **NMM-GNN** is \(O(ed+nd^{2})\) which is comparable to GNN embedding models. As our work focuses on improving accuracy of learned embeddings, we further use GraphVAE training with **NMM** to achieve SOTA performance. We would like to point out that GraphVAE (of **NMM-GNN**) training is also designed to be highly parallelizable, which allows for scalability. Moreover, our model is capable of learning on real-world, highly large-scale graphs on the order of millions of nodes and billion of edges, e.g., Friendster, while achieving the best performance, which attests to its practical value to the network science community.

### Evaluation

We detail our evaluation procedure for multi-label classification and link prediction. For all experiments, for fairness of comparison to baselines, we utilize the experiment procedure of . Specifically, 90% of links are randomly sampled as training data. We do not perform cross-validation, since it may cause overfitting to occur as our framework uses learnable parameters e.g., \(^{S}\), \(^{H}\), \(J\), \(B\), \(C\), \(D\), \(\), \(\), \(\), \(\), \(_{l}\), and \(\) which is a function of \(^{H}\) equivalently interpreted as mean square error. Per dataset, we choose hyperparameter values for \(_{A}\) in reconstruction loss: {0, 1, 2, 4, 8, 16, 32, 64}, step sizes \(_{t}\): {0.005, 0.001, 0.01, 0.05, 0.1}, and experiments are performed on AWS cluster (8 Nvidia GPUs).

#### 4.3.1 Classification

Evaluation results are in Table 10. We observe that mixture models (homophily and social influence), achieve better performance on all datasets for all metrics. Specifically, it improves over structural embedding models (**GraphWave**), GNNs (**GAT**, **HGCN**), and homophily-based models (**GELTOR**, **NRP**). We also see learning embeddings in non-Euclidean geometric spaces helps better represent structures in social networks (**HGCN** vs. **GAT**, **RaRE** vs. **NMM**). We further observe using GNN-based encoders with GraphVAE learning yields additional improvement (**NMM** vs. **NMM-GNN**).

#### 4.3.2 Link Prediction

As from , we measure quality of link prediction by sorting probability scores of every pair of nodes per model and evaluating them using area under the ROC curve (AUC) score. Specifically, 10% of existing edges and non-existing edges are hidden from training set, and probabilities are examined by

   Category & Description \\  Structural & **GraRep**, shallow embedding integrating global structural information \\ Embedding & **RoIX**, unsupervised learning approach using structural role based similarity \\ Models & **GraphWave**, shallow embedding model using spectral graph wavelet diffusion patterns \\  GNN Embedding & **GraphSAGE**, inductive framework using node features and neighbor aggregation \\ Models & **GCN**, semi-supervised learning model via graph convolution on local neighborhoods \\ Models & **GAT**, graph attention model using mask self-attention layers on local neighborhoods \\ (Euclidean space) & **GIN**, graph embedding model based on the Weisfeiler-Lehman (WL) graph isomorphism test \\  & **GELTOR**, graph contrastive learning framework for unsupervised graph data \\  Homophily-based & **GELTOR**, embedding method using learning-to-rank with AdaSim* similarity metric \\ Embedding Models & **NRP**, embedding model using pairwise personalized PageRank on the global graph \\  GNN Embedding Models & **HGCN**, hyperbolic GCN model utilizing Riemannian geometry and hyperboloid model \\ (non-Euclidean space) & \(\)-GCN**, GCN model using product space e.g., product of constant curvature spaces \\  Mixture Models & **RaRE**, Bayesian probabilistic modeffector node proximity/popularity via posterior estimation \\ (homophily and social influence) & **NMM**, our non-Euclidean mixture model (see Eqn. 6), without use of GraphVAE framework \\   

Table 9: Category and description of baseline models.

[MISSING_PAGE_FAIL:10]

## 6 Acknowledgments and Disclosure of Funding

This work was partially supported by DARPA HR0011-24-9-0370; NSF 1937599, 2106859, 2119643, 2200274, 2211557, 2303037, 2312501; NIH U54HG012517, U24DK097771; Optum AI; NASA; SRC JUMP 2.0 Center; Amazon Research Awards; and Snapchat Gifts.