# ST\({}_{k}\): A Scalable Module for Solving Top-k Problems+

Footnote †: Weidong Liu and Xiaojun Mao are the co-corresponding authors. Xiaojun Mao’s research is supported by NSFC Grant No. 12422111 and 12371273, the Shanghai Rising-Star Program 23QA1404600 and Young Elite Scientists Sponsorship Program by CAST (2023QNRC001).

Hanchen Xia\({}^{{},}\), Weidong Liu\({}^{{},{}}\), Xiaojun Mao\({}^{{},@sectionsign}\)

\(\{\{\)School of Mathematical Sciences,

\({}^{}\)Ministry of Education Key Lab of Artificial Intelligence,

\({}^{@sectionsign}\)Ministry of Education Key Laboratory of Scientific and Engineering Computing\(\}\)

Shanghai Jiao Tong University, Shanghai, China

\(\)RoyalFlush AI Research Institute, Hangzhou, China

{x_nc_2000, weidongl, maoxj}@sjtu.edu.cn

###### Abstract

The cost of ranking becomes significant in the new stage of deep learning. We propose ST\({}_{k}\), a fully differentiable module with a single trainable parameter, designed to solve the Top-k problem without requiring additional time or GPU memory. Due to its fully differentiable nature, ST\({}_{k}\) can be embedded end-to-end into neural networks and optimize the Top-k problems within a unified computational graph. We apply ST\({}_{k}\) to the Average Top-k Loss (AT\({}_{k}\)), which inherently faces a Top-k problem. The proposed ST\({}_{k}\) Loss outperforms AT\({}_{k}\) Loss and achieves the best average performance on multiple benchmarks, with the lowest standard deviation. With the assistance of ST\({}_{k}\) Loss, we surpass the state-of-the-art (SOTA) on both CIFAR-100-LT and Places-LT leaderboards.

## 1 Introduction

The ranking problem is quite common in the field of AI. For imbalanced datasets, the Average Top-k (AT\({}_{k}\)) Loss is more suitable than the conventional Average Loss . In the context of ambiguous classification tasks, Top-k Learning allows the ground truth to fall within the largest \(k\) probabilities, enhancing the model's generalizability . For language models, the Top-k sampling method helps the models select the top \(k\) most probable words during text generation, producing more fluent and coherent sentences. Distributed learning systems employ Top-k sparsification with error compensation (Top-k SGD) to reduce communication traffic without noticeably impacting model accuracy . However, as deep learning models continue to grow in size, **the cost of the ranking process becomes increasingly significant**. For example, on a single NVIDIA H800 GPU, tuning a Llama-8B model using the LoRA  method with a batch size of 4096 and 1000 iterations takes 990.14 seconds. In particular, performing QuickSort on the individual losses consumes 86.33 seconds.

In this work, we propose ST\({}_{k}\) (Smoothed Top-k), a scalable module for Top-k problems. By adding only a single trainable parameter, ST\({}_{k}\) is able to solve the Top-k problem in \((n+k)\) steps. Due to the fully differentiable nature of ST\({}_{k}\), it can be embedded end-to-end as a layer. Experiments show that we can even add \(\) to the computational graph for unified optimization using Stochastic Gradient Descent (SGD), which means that we do not need to consider the time cost of solving the Top-k problems within the computational graph. However, we can still enjoy the performance improvements that Top-k optimization provides. The contributions of this work are summarized as follows:* We propose a uniformly convergent approximation of the ReLU function.
* We propose an efficient and robust Smoothed Top-k module, \(_{k}\).
* We apply \(_{k}\) Module to smooth \(_{k}\) Loss, resulting in performance improvement and refreshing the state-of-the-art methods on two long-tailed learning leaderboards.
* We design an imbalanced classification dataset with a theoretical decision boundary.

For experiments, by applying the \(_{k}\) Module, we smooth the \(_{k}\) Loss into \(_{k}\) Loss. The computation time of \(_{k}\) Loss is almost identical to that of the average aggregating method, significantly faster than the sorting-required \(_{k}\) Loss, and it exhibits the best performance. Experiments on synthetic datasets demonstrate that models trained with \(_{k}\) Loss most closely approximate the theoretical decision boundary. On benchmarks of imbalanced binary classification, models trained by \(_{k}\) Loss exhibit the lowest average misclassification rate and the lowest standard deviation. On regression datasets, models trained with \(_{k}\) Loss exhibit the lowest (RMSE). Experiments on large real-world datasets demonstrate that \(_{k}\) Loss, as an aggregating trick for individual losses, is a scalable technique that improves accuracy on long-tailed benchmarks. With the help of \(_{k}\) Loss, we surpass the state-of-the-art (SOTA) on both the CIFAR-100-LT and Places-LT leaderboards.

### Related Work

In current research, although there are many studies on individual loss, the general characteristics of aggregate loss are often overlooked. In the existing machine learning literature, a related line of work is the data subset selection problem (Wei et al., 2015), which aims to select a subset from a large training dataset for model training while minimizing average loss. Curriculum learning (Bengio et al., 2009) and self-paced (Kumar et al., 2010) learning are two recent learning schemes. They organized the training process into several iterative stages, gradually including training data from easy to difficult to learn, where the difficulty level is measured by individual loss. Therefore, each training session in these methods corresponds to the average aggregate loss in the selected subset. The difficulties encountered by the Average Loss when dealing with imbalanced data, as discussed in (Shalev-Shwartz and Wexler, 2016; Huang et al., 2020), prompted the exploration of more robust aggregate losses. Among these, Lyu et al. (2020) introduced the \(_{k}\) loss, which averages the \(k\) largest individual losses, and exhibits advanced performance on imbalanced datasets.

Many classification tasks in the real world have inherent label confusion, as mentioned in Berrada et al. (2018). This confusion may arise from various factors, such as incorrect labels, incomplete annotations, or some fundamental ambiguities that even confuse the true labels for human experts. Therefore, some works proposed the concept of Top-k Learning in the field of image classification to address the issues of multiple semantics and semantic confusion in images (Lapin et al., 2017).

Berrada et al. (2018) proposed a method to partially smooth the Top-k Learning loss function, but did not completely solve the sorting problem in the loss function and introduced a sorting computation of \(C_{n}^{k}\). Petersen et al. (2022) proposed a Split Selection Network (SSN) based on sorting networks, which made the Top-k process differentiable and achieved the state of the art on ImageNet-1K at that time. However, the computation required by this method is cumbersome and multilayered. Sorting networks are similar to algorithms like QuickSort with time complexities of \((n n)\).

### Notational Conventions

Let \(^{n}\) denotes the set \(\{1,...,n\}\) and \(_{\{a\}}\) denotes the indicator function (which is 1 when the proposition \(a\) is true and 0 otherwise). Thus, the sign function can be defined as: \(sign(x)=_{\{x>0\}}-_{\{x<0\}}\). The Hinge function can be defined as: \([x]_{+}=\{0,x\}\). We use \(\|x\|_{1}\), \(\|x\|_{2}\), and \(\|x\|_{}\) to represent the \(_{1}\), \(_{2}\), and \(_{}\) norms of \(x\), respectively. For the set \(L=_{1},_{2},...,_{n}\), \(_{[k]}\) denotes the \(k\)-th largest element, so we have \(_{}_{}..._{[n]}\). In supervised learning problems, our training set typically contains an input set and a target set, the input set coming from the input domain \(\), and the target set from the target domain \(\), and we use their joint domain \(=\) to represent the range of the dataset. The training set \(S=z_{1},z_{2},...,z_{n}\) is a subset of \(\), where \(z_{i}=(x_{i},y_{i})\). Our task is to find a predictor \(f:\) from the function family \(\) that can predict the corresponding target \(y\) based on the new input \(x\). To evaluate the effect of the predictor, we need to introduce an individual loss function \(:^{+}\), where \(=(f(x),y)\) usually reflects some distance between the prediction \(=f(x)\) and the true value \(y\). The training process of the predictor can be described as using the gradient descent algorithm to optimize an objective function (usually minimizing the loss function). The objective function can generally be written as \((f,S)+(f)\), where \((f,S)\) is the aggregated individual loss function, and \((f)\) is a regularization term (\(_{1}\) or \(_{2}\) regularization term). The loss function \((f,S)\) is usually Average Loss, that is, \(_{avg}(f,S)=_{i=1}^{n}(f(x_{i}),y_{i})\). However, recent works, have shown some drawbacks of the average loss in adapting to imbalanced data distributions (Shalev-Shwartz and Wexler, 2016), and explored choices other than the average loss for the aggregate loss formed from individual losses, e.g., the maximum (aggregate) loss, \(_{max}(f,S)=_{i_{n}}(f(x_{i}),y_{i})\).

## 2 St\({}_{k}\) Architecture

Suppose we have a set of elements \(\{e_{i}\}_{i=1}^{n}\), then the Top-k problem can be describe as:

1. find the \(k\)-th largest element \(e_{[k]}\);
2. find the sum of top-k largest elements \(_{i=1}^{k}e_{[k]}\).

This process can certainly be achieved through conventional sorting and summation. However, in the worst-case scenario, the cost of solving the Top-k problem can reach \((n^{2})\). Ogryczak and Tamir (2003) proposed an equivalent optimization form to solve the Top-k problem and proved its linear convergence.

\[_{i=1}^{k}e_{[k]}=_{ 0}\{_{i=1}^{n}[e_{i}- ]_{+}+k\},\] (1)

However, this surrogate objective function suffers from a non-differentiable point at \(e_{i}=\), which makes it challenging to optimize. The key to solving this problem lies in designing a function approximating \([]_{+}\), which can be regarded as a rectified linear unit (ReLU) function. Here, we introduce the Smoothed ReLU (SReLU).

\[=[x+(}{^{ 2}}+1}-1)],\] (2)

where \(\) is a hyperparameter (usually we set \(=0.01\)). It can be observed from Figure 2 that as \(\) decreases, SReLU increasingly approximates the ReLU function. In fact, SReLU converges uniformly to ReLU as \( 0^{+}\); a detailed proof of this uniform convergence is provided in Proposition 1 of Appendix A.1. With the help of SReLU, the objective function (1) can be smoothed as:

\[_{i=1}^{k}e_{[k]}_{ 0}_{ i=1}^{n}[(e_{i}-)+(-)^{2}}{ ^{2}}+1}-1)]+k},\] (3)

the optimal \(^{*}=e_{[k]}\) which is the \(k\)-th largest element. In the following section, we will introduce an application scenario for ST\({}_{k}\).

Figure 1: ST\({}_{k}\) Architecture. For any layer of neurons in a neural network, to solve the Top-k problem for its weights, insert an ST\({}_{k}\) Module. The trainable parameter \(\) will gradually approximate the \(k\)-th largest element during the optimization process. And this \(\) can be used to filter neurons.

## 3 From AT\({}_{k}\) Loss to ST\({}_{k}\) Loss

In the training process of a neural network, we first choose a form of individual loss (e.g., logistic loss, hinge loss, mean square loss, or cross-entropy loss). Then, we aggregate all individual losses to calculate their average, which is the most common aggregate loss function: Average Loss. The Average Loss is widely used in a myriad of deep learning tasks. This widespread application stems from the robust theoretical foundations (Bartlett et al., 2006; De Vito et al., 2005). However, Average Loss tends to overfit the training data, especially on imbalanced datasets (Shalev-Shwartz and Wexler, 2016; Huang et al., 2020). This has inspired the motivation to find other forms of aggregate loss, such as the maximum value among individual losses (referred to as Maximum Loss). The Average Top-k (AT\({}_{k}\)) Loss was introduced by Lyu et al. (2020):

\[_{at-k}=_{i=1}^{k}_{[k]},\] (4)

which represents the average of the largest \(k\) individual losses. According to the derivation in (Ogryczak and Tamir, 2003), this ranking loss can be written in the following equivalent form:

\[_{mat-k}=_{i=1}^{n}[_{i}-]_{+ }+.\] (5)

With the help of the ST\({}_{k}\) Module, \(_{mat-k}(f,S)\) can be reconstructed as

\[_{st-k}=_{i=1}^{n}(_{i}-)+ (-)^{2}}{^{2}}+1}-1) +,\] (6)

where \(_{i}=(f_{w_{}}(x_{i}),y_{i})\) is the individual loss of sample \(i\), and \(w_{}\) represents the set of parameters of the predictor \(f\). It is easy to verify that \(_{ 0^{+}}_{st-k}=_{mat-k}\), given Proposition

Figure 2: ReLU and SReLU with various smoothing coefficients \(\).

I in Appendix A.1.

\[[+_{i=1}^{n}[_{i}-]_ {+}]-[+_{i=1}^{n}[( _{i}-)+(-)^{2}}{^{2}}+ 1}-1)]]\] \[=_{i=1}^{n}[-)^{2}}- -)^{2}+^{2}}+]\] \[=_{i=1}^{n}[-}{-)^{2}}+-)^{2}+^{2}}}]\] \[=_{i=1}^{n}[(1--)^{2}}{^{2}}}+-)^{2} }{^{2}}+1}})]\] \[<\]

The approximation error between the smoothed loss function and the original loss function can be uniformly bounded by \(/2,\).

When \(\) is convex, Equation (6) exhibits joint convexity with respect to the parameters \((w_{},)\), making the problem a special case of the non-linear multiple choice knapsack problem (Zemel, 1984), which has at most \(q=2\) roots. These roots can be found in constant time, allowing the problem to be solved in \((n q)=(n)\) time when \(q\) is fixed (Megiddo, 1984). Therefore, it can be iteratively updated using relatively simple algorithms. For example, in the case of batch learning, the block coordinate descent (BCD) method (Nocedal and Wright, 1999) can be employed, where \(w_{}\) and \(\) are updated alternately after initialization.

BCD-ST\({}_{k}\):

\[^{(t+1)} *{argmin}_{}_{st-k};\] \[w^{(t+1)} *{argmin}_{w}_{st-k}.\] (7)

The convergence of the above coordinate descent algorithms can be found in Luo and Tseng (1992); Saha and Tewari (2013); Tseng (2001).

Furthermore, empirical evidence suggests that we do not need to spend extra time optimizing \(\) separately, incorporating \(\) into the computational graph of \(w_{}\) for unified optimization using Stochastic Gradient Descent (SGD) (Bottou and Bousquet, 2008; Shamir, 2011; Srebro and Tewari, 2010), performance improvements can still be achieved.

SGD-ST\({}_{k}\):

\[^{(t+1)} ^{(t)}-_{}_{ st-k};\] \[w^{(t+1)}  w^{(t)}-_{w}_{st-k}.\]

where \(_{t}\) is the size of the update step, and when \(_{t}}}\), the stochastic gradient descent method can ensure convergence to a local minimum of Equation (6) (Shamir, 2011; Srebro and Tewari, 2010). By eliminating points where the gradients are discontinuous, the training process becomes more stable, and converges faster, as experimentally demonstrated by the standard deviations reported in Tables 3, 4, and 9, and the time cost reported in Table 2.

## 4 Synthetic Experiments

### Time Cost

We first perform experiments to compare the time costs of two standard sorting algorithms, AT\({}_{k}\), and ST\({}_{k}\), in calculating the ranking average. The experimental setup involves finding the Top-k (k=5) sum from 10,000 standard normally distributed samples. For both AT\({}_{k}\) and ST\({}_{k}\), we iterate until the error is less than \(10^{-2}\). For each algorithm, we run 50 experiments and record the average time taken.

As shown in Table 1, \(_{k}\) indeed demonstrates linear time complexity while also exhibiting a stable optimization process.

### Gaussian Distributed Dataset

To illustrate the capability of \(_{k}\) Loss in approximating the ideal decision boundary, we design a Gaussian-distributed dataset. The dataset is generated by a pre-set covariance matrix \(^{d d}\), and mean vectors \(_{1},_{2}^{d}\) corresponding to two categories. We set \(d=200\), \(_{jk}=0.25^{0.5+|j-k|}\), \(_{1}=_{d}\) and \(_{2}=(1,1,...,1,0,0,...,0)^{}\), which has 10 ones. For the predictor, we use a Logistic Regression (LR) model \(f()=(^{}+b)\). The detailed derivation process of the decision boundary can be found in the Appendix A.2. Here, we present the conclusion directly. For two normal populations with a given covariance matrix \(\), and means \(_{1}\) and \(_{2}\), the LR model has a theoretical decision boundary:

\[^{*}=(_{1}-_{0})^{}^{-1};\ \ b^{*}=_{0}^{}^{-1}_{0}+ _{1}^{}^{-1}_{1}+.\] (8)

Figure 3 provide an example on 2D plane, we set \(_{(j,k)}=0.8^{|j-k|},_{1}=^{},_{2}=^{}\), thus according to our derivation above, \(^{*}=[1.111,1.111]^{},b^{*}=-0.836\), where the black dashed line is the theoretical boundary.

We use \(_{1}\)(Tian and Gu, 2017) to measure the overlap of estimated supports and true supports:

\[_{1}=2}{+},\]

where \(=(})( ^{*})|}{|(})|}\), \(=(})( ^{*})|}{|(^{*})|}\), where \(}\) is the estimated value of the parameters obtained by the predictor, and \(^{*}\) is the theoretical value of the parameters mentioned above. The operator \(||\) is used to find the cardinality of the set, which is the number of elements, and \(()\) is the support set of the vector, which refers to the set of indices of its non-zero elements.

Other basic settings are as follows. The loss function we use is the binary cross-entropy loss. To obtain a sparse solution, we add an \(_{1}\) regularization term. The number of samples is 10,000 for the training set and 2,500 each for the validation set and the test set. We implement early stopping of the iteration based on the accuracy curve in the validation set; that is, we break when the increase in validation accuracy within 200 steps is less than \(10^{-4}\). For each meta-experiment, we repeat it 50 times and take the average. Now we conduct two groups of experiments as follows.

**Aggregate Losses and ReLU Variants.** In these experiments, we set the ratio of positive to negative samples at 8:2 for the training set, while the validation set and the test set remain 1:1. We also compare SReLU with other variants of ReLU. The smoothing coefficient for SReLU is set to 0.01, and the settings for the other ReLU variants remain at their defaults. We use Adam as our optimizer,

  Algorithm & Complexity & Average Time(s) \\  BubbleSort & \((n^{2})\) & \(20.42196 3.7015\) \\ HeapSort & \((n(n))\) & \(0.1243 0.0446\) \\ \(_{k}\) & \((n+k)\) & \(0.2167 0.1528\) \\ \(_{k}\) (Ours) & \((n+k)\) & \(\) \\  

Table 1: Performance comparison of different sorting algorithms and our method.

Figure 3: A Synthetic Example on 2D-Plain.

setting the batch size to 512, while keeping the other hyperparameters as default. According to Equation (8), we could compute the theoretical value of \(^{*}\), which is a vector with only the first 10 elements non-zero. **Sensitive Analysis.** In these experiments, we only adjust the positive-negative ratio and retain the rest of the settings from the previous experiment.

Table 2 presents the Accuracy and \(_{1}\) score achieved when different aggregate loss functions are combined with the LR model and the cross-entropy loss, trained to convergence. With the help of SReLU, \(_{k}\) Loss achieves performance that surpasses that of all other aggregate losses. Due to its property of being a real uniformly convergent approximation (not just similar in shape), SReLU outperforms other ReLU variants in this scenario. Figure 4 shows the relationship between accuracy, \(_{1}\)-score, and negative sample ratio (negative ratio). \(_{k}\) is less sensitive to the imbalance ratio,

## 5 Real World Applications

### Binary Classification

We select binary classification datasets from the KEEL2 and UCI3 databases; see Table 9.

Next, we tested the performance of different forms of aggregate loss on binary classification benchmarks. The individual loss function can generally be chosen as Logistic or Hinge, which are defined

    &  &  & _{k}\)} & _{k}\)} & _{k}\)} \\   & & & & & ELU & SoftPlus & Leaky-ReLU & SReLU (Ours) \\  Accuracy (\%) & 72.864 & 65.448 & 73.013 & 73.180 & 72.988 & 70.968 & 73.864 & **76.104** \\ \(_{1}\)-Score & 0.1904 & 0.0909 & 0.1986 & 0.2063 & 0.1925 & 0.1592 & 0.2441 & **0.3446** \\ Time (s) & 16.525 & 6.145 & 19.271 & 15.774 & 15.572 & 16.233 & 15.525 & 15.436 \\   

Table 2: Accuracy and \(_{1}\)-Score on the synthetic dataset.

Figure 4: Accuracy and \(_{1}\)-Score vs Negative Sample Ratio.

   Dataset & Average & Maximum & AT\({}_{k}\) & MAT\({}_{k}\) & ST\({}_{k}\)(Ours) \\  appendicitis & 13.778\(\)6.601 & 32.815\(\)11.306 & 13.630\(\)6.400 & 14.667\(\)7.114 & **13.406\(\)5.491** \\ australian & 13.549\(\)1.025 & 46.89\(\)8.779 & 14.428\(\)1.056 & 14.324\(\)1.058 & **13.538\(\)0.982** \\ german & 26.328\(\)2.069 & 45.832\(\)5.465 & 26.808\(\)2.347 & 26.392\(\)2.431 & **25.561\(\)1.988** \\ phoneme & 20.823\(\)2.801 & 45.83\(\)10.819 & 21.420\(\)2.732 & 17.828\(\)2.709 & **16.427\(\)2.494** \\ spambase & 6.972\(\)1.519 & 45.777\(\)19.418 & 6.955\(\)1.535 & 6.687\(\)1.577 & **6.610\(\)1.333** \\ titanic & 22.613\(\)1.226 & 48.065\(\)11.338 & 22.468\(\)1.441 & 22.211\(\)1.095 & **21.801\(\)0.911** \\ wisconsin & 3.275\(\)0.814 & 34.468\(\)9.306 & 3.205\(\)0.773 & 3.046\(\)0.570 & **2.936\(\)0.665** \\   

Table 3: Misclassification Rate(%) and Standard Derivation of Various Aggregate Losses Combined with Individual Logistic Loss.

as follows:

\[:&(f(x),y)=(1+(- yf(x)));\\ :&(f(x),y)=[1-yf(x)]_{+}.\]

The prediction model is a two-layer MLP with 10 nodes in the hidden layer, activated by the ReLU function between the two fully connected layers. To increase the stability of the training process, we added an \(_{2}\) regularization term to the loss function \((w)=\|w\|_{2}^{2}\). We divided the datasets into training, validation, and test sets in a 0.5 : 0.25 : 0.25 ratio. Nowadays, SGD is generally replaced by the mini-batch method instead of stochastic gradient descent with a single sample point, which is faster and more robust. To accommodate datasets of varying sizes in the article, we set the batch size to 16.

The hyperparameters in the experiment include \(k\) in MAT\({}_{k}\) and AT\({}_{k}\), the coefficient of the regularization term \(C\), the initial learning rate \(\), and the smoothing coefficient \(\). These hyperparameters will be selected based on their convergence accuracy performance on the validation set (for each combination of hyperparameters, we repeat the experiment fifty times and take the average of their prediction accuracy on the validation set as the basis for parameter selection). The search spaces for several hyperparameters are as follows: \(k\{1\}[0.1:0.1:1]\); \(C\{10^{0},10^{1},10^{2},10^{3},10^{4},10^{5}\}\); \(\{0.1,0.05,0,01,0.005,0.001\}\); \(\{0.1,0.01,0.001,0.0001\}\).

Since traditional gradient descent is too sensitive to the choice of step size, which is not conducive to our pure comparison of the convergence speed and accuracy of the loss function, we use the AdaGrad algorithm to iteratively update the learning rate. During the learning process, to avoid overfitting the model, we record the accuracy of the MLP predictor on the validation set after each iteration, and perform an early stop when the accuracy does not increase (the increase in the accuracy of the predictor on the validation set is less than \(10^{-6}\) after 50 steps) and roll back the model to the checkpoint with the highest accuracy on the validation set during the entire training process; otherwise, we continue training until convergence. Tables 3 and 4, respectively, show the average probability of misclassification (%) of the models trained to convergence in 50 experiments in the test set under individual Logistic and Hinge loss (the standard deviation of the 50 experiments is listed in parentheses).

In the results shown in Tables 3 and 4. Almost all the lowest misclassification rates appear in the ST\({}_{k}\) trained models. Furthermore, models trained with ST\({}_{k}\) Loss exhibit the lowest standard deviation, which to some extent demonstrates the robustness of the training process.

### Long-Tailed Classification

The interest in long-tailed classification tasks has increased with the advent of large vision-language models such as contrastive language-image pre-training (CLIP) Radford et al. (2021). Long-tailed versions of recognized datasets were built by the community. Using the Pareto distribution (\(=6\)), the ImageNet-1K and Places datasets can be sampled to create ImageNet-LT and Places-LT datasets, as described in (Liu et al., 2019). Consider two types of imbalance (Cui et al., 2019; Buda et al., 2018), Cao et al. (2019) built CIFAR-10-LT (\(=10\)) and CIFAR-100-LT (\(=100\)), where \(=_{i}\{n_{i}\}/_{i}\{n_{i}\}\) represents the imbalance ratio, defined as the ratio between the sample sizes of the most frequent and least frequent class. Due to varying word frequencies, machine translation, or

   Dataset & Average & Maximum & AT\({}_{k}\) & MAT\({}_{k}\) & ST\({}_{k}\)(Ours) \\  appendicitisitis & 15.852\(\)8.082 & 28.000\(\)12.443 & 16.148\(\)7.436 & 15.481\(\)5.577 & **14.667\(\)5.736** \\ wisconsin & 3.240\(\)1.014 & 12.000\(\)10.760 & 3.158\(\)1.085 & 2.936\(\)1.212 & **2.889\(\)1.069** \\ australian & **13.225\(\)2.351** & 36.347\(\)9.576 & 13.896\(\)2.671 & 14.243\(\)2.343 & 13.746\(\)1.966 \\ german & 26.912\(\)2.694 & 43.864\(\)12.432 & 26.776\(\)3.010 & 26.232\(\)2.190 & **25.928\(\)2.973** \\ titanic & 22.152\(\)1.862 & 47.691\(\)18.858 & 22.316\(\)1.490 & 22.116\(\)1.567 & **21.966\(\)1.289** \\ phoneme & 21.543\(\)1.358 & 44.656\(\)16.805 & 20.733\(\)1.093 & 17.335\(\)1.125 & **17.152\(\)0.955** \\ spambase & 7.463\(\)0.742 & 33.986\(\)8.955 & 7.197\(\)0.695 & 6.645\(\)0.677 & **6.342\(\)0.589** \\   

Table 4: Misclassification Rate (%) and Standard Derivation of Various Aggregate Loss Combined with Individual Hinge Loss.

more generally, text generating, can inherently be considered as long-tailed classification tasks. We choose the WMT20174 and IWSLT20145 datasets for the experiments.

SettingsFor visual classification tasks, we use Parameter-Efficient Long-Tailed (PEL) Recognition , which primarily leverages information from a text encoder to adjust classification probabilities. The text encoder in PEL is derived from the pre-trained CLIP model, and the backbone is a Vision Transformer (ViT-Base)  pre-trained in ImageNet-21K. We freeze the backbone and train the branch model of Parameter Efficient Fine-Tuning (PEFT) . We simply replaced the Average aggregation of individual losses in the Logit Adjust (LA)  loss with AT\({}_{k}\) or ST\({}_{k}\). For \(k\), we set it at \(0.9\) batch size, and \(=0.01\). Then we report the accuracy of the balanced test sets. Visual classification experiments can be implemented on a single L20 GPU, with varying durations ranging from 20 to 200 minutes.

For translation tasks, we use the training workflows provided by OpenNMT6 and FAIR-Seq7, which are very easy to reproduce. Models are essentially different sizes of Transformers (see the footnotes). In the training process of language models, the actual batch size, calculated as the sequence length multiplied by the number of sequences, is often dynamic. Consequently, we set \(k/\)batch size \(=0.96\). We simply replaced the Average aggregation of individual losses in the Cross Entropy (CE) loss with AT\({}_{k}\) or ST\({}_{k}\). We calculate the bilingual evaluation understudy (BLEU) scores for the models. The BLEU for IWSLT2014-trained models are derived from their test set, while for the WMT2017-trained models, we evaluate their BLEU on the Newstest2016 test set. Training these Transformer-based models would take about 10 hours on a single RTX 4090.

Overall ResultsST\({}_{k}\) Loss, as an aggregating trick for individual losses, is a scalable technique that improves accuracy on long-tailed benchmarks. The models trained by ST\({}_{k}\) Loss surpass SOTA on the CIFAR-100-LT and Places-LT leaderboards.

Figure 5: ImageNet-LT.

### Regression Tasks

For descriptions of the four regression datasets, see Table 9. In regression tasks, the individual loss is generally the distance (\(_{1}\) or \(_{2}\) distance) between the predicted value and the ground truth. Next, we present the specific settings of the experiment. For the Housing, Abalone, and Cpusmall datasets, we normalize the output to between \(\); for the Sinc dataset, we first randomly sample 1000 points \((x_{i},y_{i})\) from the function, where \(x_{i}[-10,10]\), then we use the Radial Basis Function (RBF) kernel to map \(x_{i}\) into the kernel space. We select 10 RBF kernels, resulting in a 10-dimensional input \(x=[k(x,c_{1}),...,k(x,c_{10})]\), where \(k(x,c_{j})=(-(x-c_{j})^{2})\). Furthermore, we add Gaussian noise \((0,0.2^{2})\) to the target output \(y\).

For individual losses, we choose the absolute value loss (\(_{1}\)) and the squared loss (\(_{2}\)). The prediction model is the same as above, a two-layer MLP with 10 nodes in the hidden layer, activated by the ReLU function between the two fully connected layers. As in binary classification experiments, we randomly divide the dataset into training, validation, and test sets, with hyperparameters chosen based on the performance in the validation set, repeated 50 times, and statistics on the test set are reported.

Tables 6 show the average RMSE and standard deviation of 50 experiments in regression datasets with different aggregate losses combined with absolute individual loss and squared individual loss (given the experience of the previous experiment, we no longer compare with Maximum Loss here), and ST\({}_{k}\) shows further performance improvement after smoothing.

## 6 Conclusion and Limitation

The proposed ST\({}_{k}\) Module can effectively solve the Top-k problem within neural networks, with no additional GPU memory or ranking time. Due to its fully differentiable nature, the training process are stable. By applying ST\({}_{k}\) Module to the Average Top-k Loss, we achieve significant improvements across numerous benchmarks.

The limitation of this study is that we have not yet evaluated the ST\({}_{k}\) Module for additional application scenarios. The effectiveness of the ST\({}_{k}\) Module is only demonstrated in smoothing AT\({}_{k}\) loss. We hope that future research will further explore the potential utility of the ST\({}_{k}\) Module.