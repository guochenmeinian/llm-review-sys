# VRA: Variational Rectified Activation for

Out-of-distribution Detection

Mingyu Xu\({}^{1,2}\), Zheng Lian\({}^{1}\)1, Bin Liu\({}^{1,2}\), Jianhua Tao\({}^{3,4}\)

\({}^{1}\)The State Key Laboratory of Multimodal Artificial Intelligence Systems,

Institute of Automation, Chinese Academy of Sciences

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)Department of Automation, Tsinghua University

\({}^{4}\)Beijing National Research Center for Information Science and Technology, Tsinghua University

{xumingyu2021, lianzheng2016}@ia.ac.cn

###### Abstract

Out-of-distribution (OOD) detection is critical to building reliable machine learning systems in the open world. Researchers have proposed various strategies to reduce model overconfidence on OOD data. Among them, ReAct is a typical and effective technique to deal with model overconfidence, which truncates high activations to increase the gap between in-distribution and OOD. Despite its promising results, is this technique the best choice? To answer this question, we leverage the variational method to find the optimal operation and verify the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection, rather than focusing only on high activations like ReAct. This motivates us to propose a novel technique called "Variational Rectified Activation (VRA)", which simulates these suppression and amplification operations using piecewise functions. Experimental results on multiple benchmark datasets demonstrate that our method outperforms existing post-hoc strategies. Meanwhile, VRA is compatible with different scoring functions and network architectures. Our code is available at [https://github.com/zeroQiaoba/VRA](https://github.com/zeroQiaoba/VRA).

## 1 Introduction

Systems deployed in the real world often encounter out-of-distribution (OOD) data, i.e., samples from an irrelevant distribution whose label set has no interaction with the training data. Most of the existing systems tend to generate overconfident estimations for OOD data, seriously affecting their reliability . Therefore, researchers propose the OOD detection task, which aims to determine whether a sample comes from in-distribution (ID) or OOD. This task allows the model to reject recognition when faced with unfamiliar samples. Considering its importance, OOD detection has attracted increasing attention from researchers and has been applied to many fields with high-safety requirements such as autonomous driving  and medical diagnosis .

In OOD detection, existing methods can be roughly divided into two categories: methods requiring training and post-hoc strategies. The first category identifies OOD data by training-time regularization  or external OOD samples . But they require more computational resources and are inconvenient in practical applications. To this end, researchers propose post-hoc strategies that directly use pretrained models for OOD detection. Due to their ease of implementation, these methods have attracted increasing attention in recent years. Among them, React  is a typical post-hoc strategy that truncates abnormally high activations to increase the gap between ID and OOD, thereby improving detection performance. But is this operation the best choice for widening the gap?To answer this question, we use the variational method to solve for the optimal operation. Based on this operation, we reveal the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection. Then, we propose a simple yet effective strategy called "Variational Rectified Activation (VRA)", which mimics suppression and amplification operations using piecewise functions. To verify its effectiveness, we conduct experiments on multiple benchmark datasets, including CIFAR-10, CIFAR-100, and the more challenging ImageNet. Experimental results demonstrate that our method outperforms existing post-hoc strategies, setting new state-of-the-art records. The main contributions of this paper can be summarized as follows:

* **(Theory)** From the perspective of the variational method, we find the best operation for maximizing the gap between ID and OOD. This operation verifies the necessity of suppressing abnormally low and high activations and amplifying intermediate activations.
* **(Method)** We propose a simple yet effective post-hoc strategy called VRA. Our method is compatible with various scoring functions and network architectures.
* **(Performance)** Experimental results on benchmark datasets demonstrate the effectiveness of our method. VRA is superior to existing post-hoc strategies in OOD detection.

## 2 Methodology

### Problem Definition

Let \(\) be the input space and \(\) be the label space with \(c\) distinct categories. Consider a supervised classification task on a dataset containing \(N\) labeled samples \(\{,y\}\), where \(y\) is the ground-truth label for the sample \(\). Ideally, all test samples come from the same distribution as the training data. But in practice, the test sample may come from an unknown distribution, such as an irrelevant distribution whose label set has no intersection with \(\). In this paper, we use \(p_{}\) to represent the marginal distribution of \(\) and \(p_{}\) to represent the distribution of OOD data. In this paper, we aim to determine whether a sample comes from ID or OOD.

### Motivation

Among all methods, ReAct is a typical and effective post-hoc strategy . Suppose \(h()=\{z_{i}\}_{i=1}^{m}\) is the feature vector of the penultimate layer and \(m\) denotes the feature dimension. For convenience, we use \(z\) as shorthand for \(z_{i}\). ReAct truncates activations above a threshold \(c\) for each \(z\):

\[g(z)=(z,c), \]

where \(c=\) is equivalent to the model without truncation. ReAct has demonstrated that this truncation operation can increase the gap between ID and OOD :

\[_{}[g(z)]-_{}[g(z)]_{ {in}}[z]-_{}[z]. \]

Despite its promising results, is this strategy the best option for widening the gap between ID and OOD? In this paper, we attempt to answer this question with the help of the variational method.

### VRA Framework

To find the best operation, we should optimize the following objectives:

* Maximize the gap between ID and OOD.
* Minimize the modification brought by the operation to maximally preserve the input.

The final objective function is calculated as follows:

\[_{g}(g)=_{}[(g(z)-z)^{2}]-2( _{}[g(z)]-_{}[g(z)]), \]

where \(\) controls the trade-off between two losses. To solve for Eq. 3, we first make a mild assumption to ensure the function space \(\) is sufficiently complex.

**Assumption 1**: _We assume \(_{}[|z|]\), \(_{}[|z|]\), \(_{}[z^{2}]\), and \(_{}[z^{2}]\) exist. Let \(\) be a Hilbert space:_

\[=\{g(z)|_{}[|g(z)|],_{}[|g(z) |],_{}[g(z)^{2}],_{}[g(z)^{2}]<\}. \]

_This space is sufficiently complex containing most functions, such as identity functions, constant functions, and all bounded continuous functions. Then, we define the inner product of \(\) as follows:_

\[<g_{a}(z),g_{b}(z)>= g_{a}(z)g_{b}(z)p_{}(z)dz. \]

Combining this assumption, the equivalent equation of Eq. 3 is:

\[_{g}(g)=(g(z)-z)^{2}p_{} (z)-2 g(z)(p_{}(z)-p_{}(z))dz. \]

Then, we leverage the variational method to solve for the functional extreme value. We mark \(g^{*}()\) as the optimal solution. \( f()\) and \(\), we then have:

\[(g^{*})(g^{*}+ f). \]

It can be converted to:

\[(g^{*}(z)-z)^{2}p_{}(z)-2 g^{*}(z) (p_{}(z)-p_{}(z))dz \] \[(g^{*}(z)+ f(z)-z)^{2}p_{}(z )-2(g^{*}(z)+ f(z))(p_{}(z)-p_{}(z))dz. \]

Then, we have:

\[^{2} f^{2}(z)p_{}(z)dz+2 f(z)(g^{*}(z)-z -(1-}(z)}{p_{}(z)}))p_{ }(z)dz 0. \]

Combining Assumption 1 and the arbitrariness of \(\), we can get:

\[ f(z)(g^{*}(z)-z-(1-}(z)}{p_{}(z)}))p_{}(z)dz=0. \]

Considering Assumption 1 and the arbitrariness of \(f(z)\), we have:

\[g^{*}(z)-z-(1-}(z)}{p_{}(z)})=0. \]

Therefore, the optimal activation function is:

\[g^{*}(z)=z+(1-}(z)}{p_{}(z)}). \]

To verify its effectiveness, we compare the optimal function \(g^{*}()\) with the unrectified function \(g(z)=z\). Since \(g^{*}()\) is the optimal solution, it should get a smaller value in Eq. 3:

\[_{}[(g^{*}(z)-z)^{2}]-2(_{}[ g^{*}(z)]-_{}[g^{*}(z)])_{}[(z-z) ^{2}]-2(_{}[z]-_{}[z]). \]

The equivalent equation of Eq. 14 is:

\[(_{}[g^{*}(z)]-_{}[g^{*}(z)]) -(_{}[z]-_{}[z]) {2}_{}[(g^{*}(z)-z)^{2}]. \]

It shows that \(g^{*}()\) enlarges the gap between ID and OOD by at least \(_{}[(g^{*}(z)-z)^{2}] 0\).

### Practical Implementations

Through theoretical analysis, we have found the optimal operation \(g^{*}()\) that can maximize the gap between ID and OOD. But in practice, this operation depends on the specific expressions of \(p_{}\) and \(p_{}\). Estimating these expressions is a challenging task given that OOD data comes from unknown distributions . This drives us to look for more practical implementations.

For this purpose, we treat ImageNet as ID data and select multiple OOD datasets. We first use histograms to approximate the probability density functions of \(p_{}\) and \(p_{}\). Then, we compute \(g^{*}()\) and compare it with ReAct, whose threshold is set to the 90\({}^{th}\) percentile of activations estimated on ID data, consistent with the original paper . Experimental results are shown in Figure 1. Compared with the model without truncation, we observe that ReAct suppresses high activations (see Figure 1(d)\(\)1(f)). Unlike ReAct, the optimal operation \(g^{*}()\) further demonstrates the necessity of suppressing abnormally low activations in OOD detection. To mimic such operations, we design a piecewise function called VRA:

\[(z)=0,z<\\ z, z\\ ,z>,\]

where \(\) and \(\) are two thresholds for determining low and high activations. Obviously, \(=0\) and \(=\) represent models without activation truncation; \(=0\) and \(>0\) represent models equivalent to ReAct. Therefore, our method is a more general operation. Since different features have distinct distributions, we propose an adaptively adjusted strategy to determine \(\) and \(\). Specifically, we predefine \(_{}\) and \(_{}\) satisfying \(_{}<_{}\). Then, we treat the \(_{}\)-quantile (or \(_{}\)-quantile) of activations estimated on ID data as \(\) (or \(\)). Meanwhile, we observe that \(g^{*}()\) amplifies intermediate activations in Figure 1(d)\(\)1(f). Therefore, we propose another variant of VRA called VRA+, which further introduces a hyper-parameter \(\) to control the degree of amplification:

\[(z)=0,z<\\ z+, z\\ ,z>.\]

## 3 Experiments

### Experimental Setup

Corpus DescriptionIn line with previous works, we consider different OOD datasets for distinct ID datasets [8; 10]. For CIFAR benchmarks  as ID data, we use the official train/test splits for ID data and select six datasets as OOD data: Textures , SVHN , Places365 , LSUN-Crop , LSUN-Resize , and iSUN ; for ImageNet  as ID data, it is more challenging than

Figure 1: Empirical PDFs for \(p_{}()\) and \(p_{}()\), and visualization of different activation functions. We treat ImageNet as ID data and select multiple OOD datasets for visualization.

CIFAR benchmarks due to larger label space and higher resolution images. To ensure non-overlapped categories between ID and OOD, we select a subset from four datasets as OOD data, in line with previous works [8; 10]: iNaturalist , SUN , Places , and Textures .

BaselinesTo verify the effectiveness of our method, we implement the following state-of-the-art post-hoc strategies as baselines: 1) MSP  is the most basic method that directly leverages the maximum softmax probability to identify OOD data; 2) ODIN  uses temperature scaling and input perturbation to increase the gap between ID and OOD; 3) Mahalanobis  calculates the distance from the nearest class center as the indicator; 4) Energy  replaces the maximum softmax probability with the theoretically guaranteed energy score; 5) ReAct  applies activation truncation to remove abnormally high activations; 6) KNN  exploits non-parametric nearest-neighbor distance for OOD detection; 7) DICE  leverages sparsification to select the most salient weights; 8) SHE  uses the energy function defined in the modern Hopfield network . 9) ASH  removed a large portion of a sample's activation at a late layer.

Implementation DetailsOur method contains three user-specific parameters: the thresholds \(_{}\) and \(_{}\), and the degree of amplification \(\). We select \(_{}\) from \(\{0.5,0.6,0.65,0.7\}\), \(_{}\) from \(\{0.8,0.85,0.9,0.95,0.99\}\), and \(\) from \(\{0.2,0.3,0.4,0.5,0.6,0.7\}\). Consistent with previous works , we use Gaussian noise images as the validation set for hyperparameter tuning. By default, we use DenseNet-101  for CIFAR and ResNet-50  for ImageNet. All experiments are implemented with PyTorch  and carried out with NVIDIA Tesla V100 GPU. To compare the performance of different methods, we exploit two widely used OOD detection metrics: FPR95 and AUROC. Among them, FPR95 measures the false positive rate of OOD data when the true positive rate of ID data is 95%; AUROC measures the area under the receiver operating characteristic curve.

### Experimental Results and Discussion

Main ResultsTo verify the effectiveness of our method, we compare VRA-based methods with competitive post-hoc strategies. Experimental results are shown in Table 1 and Table 2. We observe that our method generally achieves Top3 performance on different datasets and performs the best overall. Different from these baselines, we attempt to maximize the gap between ID and OOD by suppressing abnormally low and high activations and amplifying intermediate activations. These

   &  &  &  &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  \\   \\  MSP  & 47.27 & 93.48 & 33.57 & 95.54 & 42.10 & 94.51 & 42.31 & 94.52 & 64.15 & 88.15 & 63.02 & 88.57 & 48.74 & 92.46 \\ ODIN  & 25.29 & 94.57 & 04.70 & 98.86 & **0.89** & 90.2 & **0.39** & 98.90 & 57.50 & 82.38 & 52.85 & 88.55 & 24.57 & 93.71 \\ Mahalanobis  & **06.42** & **98.31** & 56.55 & 86.96 & 09.14 & 97.09 & 90.78 & 97.25 & **21.51** & 92.15 & 85.14 & 63.15 & 31.42 & 89.15 \\ Energy  & 40.61 & 93.99 & 03.81 & 99.15 & 09.28 & 98.12 & 10.07 & 98.07 & 56.12 & 86.43 & **39.40** & 91.64 & 26.55 & 94.57 \\ ReAct  & 41.64 & 93.87 & 05.96 & 98.84 & 11.46 & 97.87 & 12.72 & 97.72 & 43.58 & 92.47 & 43.31 & 91.03 & 26.45 & 95.30 \\ KNN  & 13.51 & 96.68 & 30.95 & 93.82 & 11.37 & 97.72 & 10.79 & 97.91 & 24.50 & **95.19** & 63.88 & 85.00 & 25.83 & 94.39 \\ DICE  & 25.99 & 95.90 & **00.26** & **99.92** & 03.91 & **99.20** & 04.36 & **99.14** & 41.90 & 88.18 & 48.59 & 89.13 & 20.84 & 95.25 \\ SHE  & 28.12 & 94.72 & **07.6** & 98.84 & 07.93 & 98.15 & 10.99 & 97.55 & 51.98 & 83.07 & 59.35 & 84.16 & 26.82 & 92.98 \\ ASH & 30.14 & 95.29 & 2.82 & 99.34 & 7.97 & 98.33 & 8.46 & 98.29 & **50.85** & 88.29 & 40.46 & 91.76 & 23.45 & 95.22 \\
**VRA** & 18.75 & 96.68 & 01.32 & **99.63** & 05.80 & 98.69 & 05.70 & 98.69 & 34.89 & 93.38 & 91.69 & 17.74 & 96.47 \\
**VRA+** & 13.54 & 97.45 & 02.03 & 99.56 & 06.37 & 98.72 & 06.15 & 98.71 & 27.07 & 95.03 & 39.97 & **91.96** & **15.85** & **96.91** \\   \\  MSP  & 81.70 & 75.40 & 60.49 & 85.60 & 85.24 & 69.18 & 85.99 & 70.17 & 84.79 & 71.48 & 82.55 & 74.31 & 80.13 & 74.36 \\ ODIN  & 41.35 & 92.65 & 10.54 & 97.93 & 65.22 & 84.22 & 67.05 & 83.84 & 82.34 & 71.48 & 82.32 & 76.84 & 58.14 & 84.49 \\ Mahalanobis  & **22.44** & **95.67** & 68.90 & 86.30 & **23.07** & **94.20** & 31.38 & 89.28 & 62.39 & 79.39 & 92.66 & 61.39 & 50.14 & 84.37 \\ Energy  & 87.46 & 81.85 & 14.72 & 97.43 & 70.65 & 80.14 & 74.54 & 78.95 & 84.15 & 71.03 & 79.20 & 77.22 & 68.45 & 81.19 \\ ReAct  & 83.81 & 81.41 & 25.55 & 94.92 & 60.08 & 87.88 & 65.27 & 78.65 & 77.78 & 95.85 & 62.56 & 74.04 & 65.86 & 83.96 \\ KNN  & 23.96 & 93.99 & 70.98 & 73.37 & 76.34 & 76.69 & 70.88 & 78.58 & **37.75** & 87.48 & 95.20 & 59.70 & 62.52 & 78.30 \\ DICE  & 54.65 & 88.84 & **00.93** & **99.74** & 49.40 & 91.04 & 48.27 & 90.08 & 65.04 & 76.42 & 79.58 & 72.66 & 49.72 & 87.83 \\ SHE  & 41.89 & 90.61 & 01.06 & 99.68 & 71.88 & 73.97 & 72.73 & 76.14 & 61.49 & 76.57 & 83.33 & 70.53 & 56.78 & 81.25 \\ ASH & 81.86 & 83.86 & 11.60 & 97.89 & 67.56 & 81.67 & 70.90 & 80.81 & 78.24 & 74.09 & 77.03 & 77.94 & 64.53 & 82.71 \\
**VRA** & 70.91 & 87.46 & 10.73 & 98.04 & 38.52 & 93.49 & 38.53 & 93.42 & 47.64 & **90.17** & **76.39** & **78.66** & 47.12 & 90.21 \\
**VRA+** & 62.64 & 88.70 & 19.82 & 96.33 & 28.44 & **95.47** & **28.72** & **95.18** & 40.62 & **91.57** & 79.78 & 76.42 & **43.34** & **90.61** \\  

Table 1: **Main results on CIFAR benchmarks.** In this table, we compare detection performance with competitive post-hoc strategies. All methods are pretrained on ID data. We report the results for each dataset, as well as the average results across all datasets. “FR.” and “AU.” are abbreviations of FPR95 and AUROC. Top3 results are marked in red, and darker colors indicate better performance.

[MISSING_PAGE_FAIL:6]

Performance Upper Bound AnalysisWe propose VRA and VRA+ to approximate the optimal operation for OOD detection. But is it necessary to design other functions to get a better approximation? To answer this question, we need to reveal whether \(g^{*}()\) can reach the upper-bound performance. The core of estimating \(g^{*}()\) is to estimate the probability density functions of \(p_{}\) and \(p_{}\). To this end, we consider two ideal cases: _VRA-True_ and _VRA-Fake-True_. In the first case, we assume that all ID and OOD data are known in advance; in the second case, we randomly select 1% of ID and OOD data from the entire dataset. Both cases leverage histograms to estimate \(p_{}\) and \(p_{}\) and use Eq. 13 to calculate \(g^{*}()\). Considering that histograms provide a piecewise form of \(g^{*}()\), we directly use the piecewise function to represent \(g^{*}()\). In Table 4, we observe that both ideal cases can achieve near-perfect results. Therefore, \(g^{*}()\) that increases the gap between ID and OOD can generate more discriminative features for OOD detection. In the future, we will explore other functions that can better describe the optimal operation for better performance.

Parameter Sensitivity AnalysisVRA uses two hyper-parameters (\(_{}\) and \(_{}\)) to adaptively adjust thresholds for low and high activations. In this section, we conduct parameter sensitivity analysis and reveal their impact on OOD detection. In Figure 2, we observe that our method does not perform well when \(_{}\) and \(_{}\) are inappropriate. A large \(_{}\) suppresses too many low activations, while a large \(_{}\) suppresses too few high activations. Therefore, it is necessary to choose proper \(_{}\) and \(_{}\) for VRA.

Role of Adaptively Adjusted StrategyIn this paper, we adopt an adaptive strategy to automatically determine \(\) and \(\). To verify its effectiveness, we compare this adaptive strategy with another strategy that uses fixed \(\) and \(\) for different features. To determine these hyper-parameters, we use Gaussian

    &  &  &  &  \\  & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) \\  CIFAR-10 & 26.55 & 94.57 & 17.74 & 96.47 & 13.27 & 97.75 & **00.96** & **99.81** \\ CIFAR-100 & 68.45 & 81.19 & 47.12 & 90.21 & 23.62 & 94.20 & **01.58** & **99.69** \\ ImageNet & 58.41 & 86.17 & 25.49 & 94.57 & 13.09 & 96.89 & **03.50** & **99.31** \\   

Table 4: **Performance upper bound analysis**. For each ID dataset, we report the average results over multiple OOD datasets. We use DenseNet-101  for CIFAR and ResNet-50  for ImageNet.

Figure 2: **Parameter sensitivity analysis. For each ID dataset, we report the average results over multiple OOD datasets. We use DenseNet-101  for CIFAR and ResNet-50  for ImageNet.**

noise images as the validation set, in line with previous works . Experimental results in Table 5 demonstrate that our adaptive strategy outperforms this fixed strategy. The reason lies in that different features have distinct statistical distributions. Using fixed thresholds for different features will limit the performance of OOD detection.

Compatibility with BackbonesIn this section, we further verify the compatibility of our method with different backbones. For a fair comparison, all methods are pretrained on ImageNet, and we report the average results on four OOD datasets of ImageNet. Compared with competitive post-hoc strategies, experimental results in Table 6 demonstrate that our method can achieve the best performance under different network architectures. These results validate the effectiveness and compatibility of our method. Meanwhile, we observe some interesting phenomena in Table 6. ReAct  points out that mismatched BatchNorm  statistics between ID and OOD lead to model overconfidence on OOD data. In Table 6, VGG-16 and VGG-16-BN refer to models without and with BatchNorm, respectively. We observe that no matter with or without BatchNorm, ReAct cannot achieve better performance than Energy, consistent with previous findings . Therefore, BatchNorm may not be the only reason for model overconfidence, and the network architecture also matters. Furthermore, Energy  generally outperforms MSP  with the exception of EfficientNetV2, which also reveals its limitation in compatibility. In the future, we will conduct an in-depth analysis to reveal the reasons behind these phenomena.

## 4 Further Analysis

Combining features with logit outputs can achieve better performance in OOD detection . Therefore, we design another variant of VRA called VRA++, whose scoring function is defined as:

\[_{v}_{i=1}^{m}g(z_{i})+_{i=1}^{c}e^{l_{i}}, \]

where \(z_{i},i[1,m]\) represents the \(i\)-th feature and \(l_{i},i[1,c]\) represents the \(i\)-th logit output. This scoring function consists of two items: (1) Since we have maximized the gap between ID and OOD \(_{}[g(z_{i})]-_{}[g(z_{i})]\), we directly use the sum of all rectified features \(_{i=1}^{m}g(z_{i})\) as the indicator; (2) We also calculate the energy score on logit outputs for OOD detection. These items are combined using a balancing factor \(_{v}\). Unlike VRA using piecewise functions, we further test the performance of the quadratic function \(g(z)=-z^{2}+_{v}z\). By choosing a proper \(_{v}\), this quadratic function can

    &  &  &  &  \\  & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) & FPR95 \(\) & AUROC \(\) \\  ResNet-18  & 69.70 & 80.61 & 58.59 & 80.40 & 36.36 & 92.17 & **34.87** & **92.58** \\ ResNet-34  & 68.84 & 81.19 & 57.20 & 86.84 & 32.23 & 93.08 & **30.63** & **93.46** \\ ResNet-50  & 66.95 & 81.99 & 58.40 & 86.17 & 31.43 & 92.95 & **25.49** & **94.57** \\ ResNet-101  & 64.70 & 82.47 & 54.84 & 87.29 & 31.68 & 93.03 & **25.80** & **94.36** \\ ResNet-152  & 61.35 & 83.74 & 50.39 & 88.61 & 26.57 & 94.22 & **22.21** & **95.20** \\ VGG-16  & 67.94 & 81.60 & 54.33 & 88.17 & 67.81 & 83.68 & **32.99** & **92.59** \\ VGG-16-BN  & 65.92 & 82.00 & 50.49 & 89.03 & 59.02 & 86.34 & **35.12** & **92.05** \\ EfficientNetV2  & 57.57 & 83.96 & 75.29 & 71.10 & 48.28 & 88.01 & **43.81** & **89.76** \\ RegNet  & 65.37 & 82.85 & 59.46 & 85.51 & 34.65 & 92.53 & **26.18** & **94.55** \\ MobileNetV3  & 67.99 & 82.14 & 60.49 & 87.80 & 60.72 & 87.82 & **56.65** & **89.30** \\   

Table 6: **Compatibility with different backbones.** All methods are pretrained on ImageNet.

    &  &  &  \\  & & \(\) & \(\) & \(_{}\) & \(_{}\) & FPR95 \(\) & AUROC \(\) \\   & assign \(\), \(\) & 0.50 & 1.50 & – & – & 19.44 & 96.34 \\  & assign \(_{}\), \(_{}\) & – & – & 0.60 & 0.95 & **17.74** & **96.47** \\   & assign \(\), \(\) & 0.50 & 1.50 & – & – & 56.35 & 86.09 \\  & assign \(_{}\), \(_{}\) & – & – & 0.60 & 0.95 & **47.12** & **90.21** \\   

Table 5: **Role of adaptively adjusted strategy.** We use DenseNet-101  for CIFAR.

also simulate suppression and amplification operations. Finally, our scoring function is defined as:

\[-_{v}_{i=1}^{m}(z_{i}^{2}-_{v}z_{i})+_{i=1}^{c}e^{l_{i}}. \]

Among all methods, ViM  is a powerful strategy that combines features and logit outputs. For a fair comparison with ViM, we use the same ID data (ImageNet), OOD data (OpenImage-O , Texture , iNaturalist , and ImageNet-O ), and network architecture (BiT ). Experimental results in Table 7 demonstrate that VRA++ achieves better performance than ViM, verifying the scalability and high potential of our method. Meanwhile, VRA++ generally achieves the best performance among all variants (see Table 8). These results further demonstrate the necessity of combining features and logit outputs in OOD detection.

## 5 Related Work

Post-hoc MethodPost-hoc strategies are an important branch of OOD detection. Due to their ease of implementation, they have attracted increasing attention from researchers. Among them, MSP  was the most basic post-hoc strategy, which directly leveraged the maximum value of the posterior distribution as the indicator. Since then, researchers have proposed various post-hoc approaches. For example, ODIN  used temperature scaling and input perturbations to improve the separability of ID and OOD data. Energy  replaced the softmax confidence score in MSP  with the theoretically guaranteed energy score. Mahalanobis  used the minimum distance from the class centers to identify OOD data. KNN  was a nonparametric method that explored K-nearest neighbors. More recently, researchers have found that the reason behind model overconfidence in OOD data lies in abnormally high activations of a small number of neurons. To address this, Dice  used weight sparsification, while ReAct  exploited activation truncation. Different from these works, we further demonstrate that abnormally low activations also affect OOD detection performance. This motivates us to propose VRA to rectify the activation function.

Activation FunctionActivation functions are an important part of neural networks [40; 41]. Previously, researchers found that neural networks with the ReLU activation function produced abnormally high activations for inputs far from the training data, harming the reliability of deployed systems . To address this problem, ReAct used a truncation operation to rectify activation functions. In this paper, we propose a more powerful rectified activation function for OOD detection. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our method.

Variational MethodThe variational method is often used to solve for the functional extreme value. Its most famous application in neural networks is the variational autoencoder , which solves for the functional extreme value by trading off reconstruction loss and Kullback-Leibler divergence. It

    & OpenImage-O & Texture &  & ImageNet-O &  \\  & FR. \(\) & AU. \(\) & FR. \(\) & AU. \(\) & FR. \(\) & AU. \(\) & FR. \(\) & AU. \(\) & FR. \(\) & AU. \(\) \\  MSP  & 73.72 & 84.16 & 76.65 & 79.80 & 64.09 & 87.92 & 96.85 & 57.12 & 77.83 & 77.25has also been applied to other complex scenarios  and multimodal tasks . In this paper, we use the variational method to find the operation that can maximize the gap between ID and OOD.

## 6 Limitation

Distributions of ID and OOD data impact the performance of VRA. In the future, we will conduct a theoretical analysis to explain the reason behind this phenomenon. Meanwhile, analogous to previous works such as ReAct and ASH, this paper mainly focuses on the pre-trained classifiers with ReLU-based activations. Although we have explored some other architectures in the appendix, future experiments in more structures are also needed.

In this paper, we treat \(_{g}_{}[g(z)]-_{}[g(z)]\) as the core objective function derived from ReAct and \(_{g}_{}[(g(z)-z)^{2}]\) as the regularization term. However, there may be better regularization terms that can not only guarantee the existence of the optimal solution but also ensure that the expression of the optimal solution is easy to implement and has good interpretability. Therefore, we will explore other regularization terms for OOD detection. Meanwhile, this paper uses simple piecewise functions to approximate the complex optimal operation. In the future, we will explore other functional forms that can better describe the optimal operation.

## 7 Conclusion

This paper proposes a post-hoc OOD detection strategy called VRA. From the perspective of the variational method, we find the theoretically optimal operation for maximizing the gap between ID and OOD. This operation reveals the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection. Therefore, we propose VRA to mimic these suppression and amplification operations. Experimental results show that our method outperforms existing post-hoc strategies and is compatible with different scoring functions and network architectures. In the ideal case of knowing a small fraction of OOD samples, we can achieve near-perfect performance, demonstrating the strong potential of our method. Meanwhile, we verify the effectiveness of our adaptively adjusted strategy and reveal the impact of different hyper-parameters.

## 8 Acknowledge

This work is supported by the National Natural Science Foundation of China (No.62201572, No.61831022, No.62276259, No.U21B2010, No.62271083), Beijing Municipal Science & Technology Commission, Administrative Commission of Zhongguancun Science Park (No.Z211100004821013), Open Research Projects of Zhejiang Lab (NO. 2021KH0AB06), CCF-Baidu Open Fund (No.OF2022025).