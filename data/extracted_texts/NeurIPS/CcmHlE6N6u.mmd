# LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes

Zefan Qu  Ke Xu\({}^{}\)   Gerhard Petrus Hancke  Rynson W.H. Lau\({}^{}\)

Department of Computer Science

City University of Hong Kong

zefanqu2-c@my.cityu.edu.hk, kkangwing@gmail.com,

{gp.hancke, Rynson.Lau}@cityu.edu.hk

###### Abstract

Neural Radiance Fields (NeRFs) have shown remarkable performances in producing novel-view images from high-quality scene images. However, hand-held low-light photography challenges NeRFs as the captured images may simultaneously suffer from low visibility, noise, and camera shakes. While existing NeRF methods may handle either low light or motion, directly combining them or incorporating additional image-based enhancement methods does not work as these degradation factors are highly coupled. We observe that noise in low-light images is always sharp regardless of camera shakes, which implies an implicit order of these degradation factors within the image formation process. This inspires us to explore such an order to decouple and remove these degradation factors while training the NeRF. To this end, we propose in this paper a novel model, named LuSh-NeRF, which can reconstruct a clean and sharp NeRF from a group of hand-held low-light images. The key idea of LuSh-NeRF is to sequentially model noise and blur in the images via multi-view feature consistency and frequency information of NeRF, respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise Decomposition (SND) module for decoupling the noise from the scene representation and a novel Camera Trajectory Prediction (CTP) module for the estimation of camera motions based on low-frequency scene information. To facilitate training and evaluations, we construct a new dataset containing both synthetic and real images. Experiments show that LuSh-NeRF outperforms existing approaches. Our code and dataset can be found here: https://github.com/quzefan/LuSh-NeRF.

## 1 Introduction

Neural Radiance Fields (NeRFs)  have achieved notable success in modeling 3D scene information via implicit functions learned from a set of 2D images with known camera poses. Since optimizing NeRFs by measuring the colorimetric errors of training views essentially requires bright and sharp training images, hand-held low-light photography, which is prevalent in our daily life (_e.g._, in nighttime scenes), cannot be used directly for training NeRFs to produce visually pleasing novel view images (Fig. 1(b)), due to the co-existence of low visibility, noise, and camera motion blur in the captured images.

A straightforward solution is to incorporate existing low-light enhancement/deblurring methods (_e.g._, ) to preprocess the captured images before using them for training the NeRFs. However, it raises two problems. First, as these methods are typically image-based, they do not consider the multi-view consistency. Second, These enhancement methods may introduceadditional artifacts (_e.g._, overexposure and unnatural color contrast). We note that there are some NeRF methods proposed for handling low-light scenes [47; 6] and scene motions [26; 48; 19; 7]. However, while the former assumes no camera motions occur during the capture, the latter cannot handle low-light scenes. Directly applying them to our problem does not work. A visual example is shown in Fig. 1((c) to (e)), where existing methods struggle to render the desired results.

We observe that in the captured low-light images, noise always appears sharp regardless of the camera shakes, due to the independent sensor noise generation within the collection and transformation of photons into electronic signals in the camera Image Signal Processor (ISP). This implies an implicit order of low visibility, sensor noise, and blur, which inspires us to model such an implicit order to decouple and remove those degradation factors for NeRF's training in an unsupervised manner.

In this paper, we propose a novel method, called LuSh-NeRF, to Light up and Sharpen NeRF by sequentially modeling the degradation factors. Specifically, the brightness of training images is first scaled up to provide more contextual information (which simultaneously amplifies the noise in the images). We then propose two novel modules, _i.e._, a Scene-Noise Decomposition (SND) module and a Camera Trajectory Prediction (CTP) module, to handle the noise and camera shake problems. The SND module learns to decouple noise from the implicit scene representation by explicitly learning a noise field through leveraging the multi-view feature consistency of NeRF. The CTP module then estimates the camera trajectories for sharpening image details based on the low-frequency information of denoised scene images rendered by SND. The two modules are optimized in an iterative manner such that the denoised scene representation of the SND module provides more information for predicting trajectories in the CTP module, while the results with sharp details from the CTP module in turn facilitate the denoising process in the SND module. To facilitate model training and evaluations, we construct a new dataset consisting of five synthesized scenes and five real scenes. As shown in Fig. 1(f), our LuSh-NeRF can render bright and sharp novel-view results.

In summary, this work has the following main contributions:

* We propose the first method (LuSh-NeRF) to reconstruct a NeRF from hand-held low-light photographs, by decoupling and removing degradation factors through modeling their implicit orders.
* Our LuSh-NeRF contains two novel modules, a novel SND module for noise removal from the implicit scene representation and a novel CTP module for handling camera motions.
* We construct the first dataset for training and evaluations. Experiments show that LuSh-NeRF outperforms existing methods.

Figure 1: Given a hand-held captured low-light scene (a), while (a combination of) existing low-light enhancement/NeRF methods may not produce visually pleasing novel-view images ((b)-(e)), our LuSh-NeRF can produce bright and sharp results (f).

Related Works

**Neural Radiance Field (NeRF).** NeRF  has fundamentally changed the way of modeling 3D scenes by learning coordinate-based implicit neural representations from a set of 2D observations. has gained widespread popularity in computer vision and graphics tasks pertaining to neural rendering, leveraging coordinate-based implicit neural representations (INR). It has gained significant popularity in computer vision and graphics tasks, with many NeRF variants proposed for, _e.g._, accelerating the training and rendering of NeRF [14; 18; 40; 41], handling dynamic scenes [21; 34; 39; 44; 61] and digital humans body [1; 34; 36; 37; 10] or human head [53; 68; 16] modeling, and the manipulation [4; 38; 28; 23] or generation [8; 15; 25; 33] of scene contents. Many variants of NeRF are also gradually expanding into multiple research areas. For example, [14; 18; 40; 41] are proposed to accelerate the NeRF training procedure, [21; 34; 39; 44; 61] are applied to render dynamic scenarios, [4; 38; 28; 23] are focused on the NeRF relighting methods, [1; 34; 36; 37; 53] are expanded to the non-rigid object rendering, [8; 15; 25; 33] are used for the generation models. These methods typically require high-quality (_i.e._, bright and sharp) 2D images for optimizing NeRFs.

**NeRF from Low-light Images.** Recently, there are some methods [47; 30; 6] proposed to relax such constraints by learning to reconstruct NeRFs from low-light images. Wang _et al._ reconstructs a normally illuminated scene with multiple low-light images in an unsupervised manner. Cui _et al._ can change the luminance of the scene by extending the transmittance function in NeRF. However, these methods do not consider the camera shakes that often occur in hand-held low-light imaging.

**NeRF from Blurry Images.** Rendering scenes with multiple blurry images is frequent and challenging in the real world. Ma _et al._ firstly propose the deblurring problem in NeRF and simulate the blurring process with a deformable kernel. Most works [19; 48; 20; 17] model the camera motion trajectory in 3D space to handle the blur in normal-light scenes. Peng _et al._ accelerate the deblurring process with an efficient rendering scheme. Huang _et al._ utilize a blur generator to model the camera imaging process for the all-in-focus photos. However, these methods cannot handle hand-held low-light photographs as they do not jointly model noise and camera motions under low-light conditions.

**Low-light Image Enhancement.** Deep learning-based methods have been shown to be more effective in this task. Some Retinex-based methods [52; 65; 64; 54] and end-to-end methods [9; 11; 27; 32; 50; 56; 66; 58; 46; 22] are capable of achieving good enhanced results. Dong _et al._ abandon the bayer-filter to recover the low light color from raw camera data. Dudhane _et al._ merge sequential images taken in the same scene to enhance the image quality. Xu _et al._ take an SNR prior to guide the feature learning and formulate the network with a new self-attention model. However, there are few methods that can address the blur present in practical low-light images owing to the long exposure time. Recently, Zhou _et al._ propose a large dataset for the low-light deblurring task, and some works [67; 62] propose solutions to enhance low-light blurred images.

**Low-light Deblurring.** There are few methods that can address the blur present in practical low-light images owing to the long exposure time. Recently, Zhou _et al._ propose a large dataset for the low-light deblurring task, and some works [67; 62] propose solutions to enhance low-light blurred images. Zhou _et al._ propose a highly coupled encoder-decoder architecture to solve the joint artifact. Zhang _et al._ delves into the multiple degradations and proposes an all-in-one fashion restoration network. While our work addresses a similar problem, we aim for NeRF reconstruction, which is more challenging.

## 3 Proposed Method

In this section, we will introduce the details of the proposed method LuSh-NeRF, which can reconstruct a normally illuminated, sharp, and clean scenario from a set of hand-held captured low-light 2D images. Specifically, we first propose the SND module to separate noise from the scene information in the pre-processed image (Sec. 3.2), and then use the CTP module to make an accurate prediction of the kernel from the low-frequency information in the image (Sec. 3.3). Our network is supervised by multiple loss functions (Sec. 3.4). The overall structure of the network is shown in Fig. 2.

### Problem Statement

**Preliminary of Neural Radiance Field.** NeRF utilizes volume rendering  to synthesize 3D scenes by simulating multiple camera rays \((t)=+t\) emitted into the scene, where \(\) represents the ray origin, \(t\) is the sample distance, and \(\) denotes the ray direction. The values at various sample points along these rays are encoded within an implicit neural network. Given a 3D coordinate \(=(x,y,z)\) and a querying view direction \(=(,)\), NeRF estimates the function \(F:(,)(,)\) through an MLP network, where \(\) and \(\) represent the RGB color value and the volume density, respectively. After obtaining the information of each sample point on a ray, the pixel value \(()\) of ray \(\) can be computed as:

\[()=_{i=1}^{N}w_{i}_{i}=_{i=1}^{N}T_{i} (1-(-_{i}_{i}))_{i}, T_{i}=( -_{j=1}^{i-1}_{i}_{i}),\] (1)

where \(\) is the distance between two subsequent sample points on a ray. \(T_{i}\) is the accumulated transmittance value that denotes the radiance decay rate of sampled points.

**Problem Modeling.** For images taken in practical low-light scenarios, degradation can be modeled into 3 categories, as shown in the example in Fig. 3: **(1) Low value intensity.** The pixel value intensities in the image are generally low, which leads to increased difficulty in NeRF training , resulting in poor results as shown in Fig. 1(b); **(2) Noise.** Due to the sensor noise generated within the collection and transformation of photons, the image pixel values are distorted randomly and unpredictably; and **(3) Camera motion blur.** Long exposure times inevitably lead to blur in the photo due to the camera motion. When trying to address one of the three defects, the others interfere significantly. Therefore, how to reconstruct a properly illuminated, clear, and sharp NeRF scene from this poor-quality image is a very challenging problem.

Figure 3: Different degradations in the real low light images. (a) Low intensity (b) Noise (c) Blur.

Figure 2: The pipeline of our proposed LuSh-NeRF. It contains two novel modules: (a) SND module: Decompose the noise in each view from the origin training image with a Noise NeRF architecture, and utilize the multi-view consistency characteristic in 3D scenario to separate the scene information and noise better; (b) CTP module: To minimize the interference of noise in low-light images on blur kernel predictions, the high frequency domain of the low light regions which are severely affected by noise are abandoned. In the rendering stage, we discard the Noise Estimator and Blur Kernel, and only use the Scenario-NeRF to render the enhanced scene.

In this work, we attempt to optimize the NeRF by decoupling and removing the above defects in the training stage. Modeling and analyzing noise and blur are difficult at low pixel intensity values. Thus, the low visibility of the image should first be raised. Considering that noise in a low-light photo is mainly generated by the imaging process and would not be affected by the camera motions, the implicit order of the defect decomposition should be low visibility, noise, and blur.

Specifically, given multiple training images \(I_{ll,noisy,blurry}\) from different views, NeRF aims to render a scenario with \(I_{hl,clear,sharp}\). We first scale up the brightness of \(I_{ll,noisy,blurry}\) and get \(I_{hl,noisy,blurry}\) for more contextual information, but this process simultaneously amplifies the noise in the image. LuSh-NeRF then decomposes the image into scene and noise information with 3D multi-view consistency, removing the noise from the image to obtain \(I_{hl,clear,blurry}\). Finally, the image without noise interference is utilized to model the blur phenomenon and restore \(I_{hl,clear,sharp}\). The enhancement process for LuSh-NeRF is as follows:

\[I_{hl,clear,sharp}=()=((I_{ll,noisy,blurry})-}_{I_{hl,clear,blurry}}),\] (2)

where Noise is the noise of the different views predicted by the SND module. \(()\) denotes the CTP module to ease the camera motion blur phenomenon, and \(()\) is the NeRF rendering function.

During training, due to the lack of ground truth, \(I_{hl,noisy,blurry}\) (_i.e._, \((I_{ll,noisy,blurry})\)) is used as the supervised signal. The inverse of Eq. 2 should be taken to simulate the low-quality image as:

\[_{hl,noisy,blurry}=^{-1}((LuSh-NeRF))+,\] (3)

After training, the correct-light (denoted as _hl_), sharp and clear NeRF model can be obtained directly by discarding the \(^{-1}\) function and Noise. In the following subsections, we introduce how we estimate the Noise under each view with the SND module and how we accurately predict the \(^{-1}\) function with the CTP module.

### Scenario-Noise Decomposition

Directly training NeRF with \(I_{hl,noisy,blurry}\) would still produce defective rendering results, as scaling up the low-light images can amplify noise and distort colors significantly, causing more severe multi-view inconsistency.

We note that while the noise is generally sharp and stochastic and has different values in different views, scenario information across views tends to be consistent. This makes it possible to decouple noise and scenario information. To co-optimize with the base network Scenario-NeRF (S-NeRF), we propose a new network, named Noise-Estimator (N-Estimator), which has the same network structure as S-NeRF, to compute the noise of each rendering coordinate. Since the noise is not consistent, N-Estimator discards the volume rendering calculation and directly takes the values of the intermediate sampling points of the input rays as the output. The formula for obtaining the noisy pixel \(C_{noisy}(r)\) is:

\[ C_{noisy}(r)=(C_{S-NeRF}(r))+C_{N-Estimator }(r)=(_{i=1}^{N}w_{i}_{i})+n_{},\\ where n_{}=MLP_{N-Estimator}(P_{mid},d),\] (4)

where \(P_{mid}\) is the intermediate sampling points coordinate, and \(d\) is the view direction. The \(()\) function refers to the Camera Trajectory Prediction module, \(N\) is the number of sampled points on a ray, and \(n_{}\) is the noise value rendered by N-Estimator. The volume rendering computation of S-NeRF gradually forces S-NeRF to learn the consistency scenario information, and the noise that does not share this feature is decomposed from the scene representation by the N-Estimator during the optimization process to fit the final noisy training images.

To more accurately decouple the scene information and noise, LuSh-NeRF utilizes the Rays Alignment supervision in the SND module. Specifically, An image matching method  is applied to the images rendered by S-NeRF after sharpening in each viewpoint to obtain a dense pixel matching matrix \(M\) and a certainty matrix \(C\), which represent the coordinate pairs of matching pixels in different views and the confidence level of each matching. The current input ray is then taken as \(anchor\_ray\), and the coordinates of its rendered pixels are used to find the matching pixels under other viewpoints in \(M\). The confidence of the matching pairs in \(C\) should be higher than the preset threshold \(\) to ensure that accurate matches can be obtained.

With these matching pixel coordinates, we can get the corresponding aligned rays \(align\_ray_{i}\) under different viewpoints \(view_{i}\). Ideally, the RGB colors computed by \(anchor\_ray\) and \(align\_ray_{i}\) should be close to identical owing to the scenario consistency. So a consistency loss \(L_{consistency}\) is proposed to reduce the distance within these rendering pixels in S-NeRF to force the decoupling of scene information and noise, as:

\[L_{consistency}=_{i=1}^{K}|C_{S-NeRF}(r_{i})-_{j=1}^{K}C_{S-NeRF}(r_{j})|,\] (5)

where \(K\) is the number of training views. Constrained by \(L_{consistency}\), the information extracted by S-NeRF has a higher viewpoint consistency, leading to a better decomposition of the scene information and noise.

### Camera Trajectory Prediction

Motivated by , the CTP module employs the same thoughts to predict the rigid camera motions with all rays in each view. However, unlike the properly lighted blurry data used in , the practical low light conditions present substantial interferences, which could cause notable errors in the CTP network predictions, as demonstrated in the results of Subsec 4.1.

When the SND module is not fully converged, noise still exists in the SND-processed scenario. The kernel prediction using rays with strong noise interference may interrupt the CTP network, and directly lead to the failure of the whole NeRF training. Hence, it is necessary to filter the rays for CTP module optimization. Since the high-frequency and low-intensity regions in the low-light images are more disturbed by noise, corrupting the information in the original raw image , the CTP module utilizes Discrete Fourier Transform (DFT) to obtain the image frequency map as:

\[Y(u,v)=(x(m,n))=_{m=0}^{M-1}_{n=0}^{N-1}x(m,n) e^{-j2 (+)}.\] (6)

The noisy high-frequency regions of the image are filtered by the low-pass filter \(H\) of radius \(r\):

\[Y_{}(u,v)=H(u,v) Y(u,v),\,H(u,v)= 1,&+v^{2}} r\\ 0,&+v^{2}}>r.\] (7)

The retained low-frequency image regions are then converted back to the image via the DFT\({}^{-1}\) function to obtain the image informative region mask \(Mask_{}(m,n)\) as:

\[Mask_{}(m,n)=(^{-1}[Y_{} (u,v)],T),\] (8)

where Bi(\(,T\)) is a binarization function with threshold \(T\). \(Mask_{}(m,n)\) classifies the rays into \(ray_{clear}\) and \(ray_{noisy}\), which are used to render the clear and noise-dominated pixels in the training images separately. The gradient of \(ray_{noisy}\) to the rigid motion prediction network is detached during NeRF optimization to reduce the impact of noise yet to be removed by the SND module on blur kernel prediction as:

\[rays=[ray_{clear},(ray_{noisy})],\] (9)

where the Detach(\(\)) function is used to detach the gradient from the variables. The role of the low-pass frequency filter is shown in Fig. 7, where the CTP Mask reduces the interference of noisy regions on the blur kernel prediction compared to the mask obtained by directly using the RGB intensity as the threshold.

### Training & Rendering

After employing the SND and CTP modules as in Eq. 3, we have the simulated RGB values \(_{hl,noisy,blurry}\) in the image \(_{hl,noisy,blurry}\). We can then optimize the network with the MSE loss \(L_{construction}\) as:

\[L_{construction}=_{i}\|_{hl,noisy,blurry}(r_{i})-C_{hl,noisy, blurry}(r_{i})\|_{2}.\] (10)All the modules in the network are trained in an end-to-end manner with the following training losses:

\[L_{LuSh-NeRF}= L_{construction}+ L_{consistency},\] (11)

where \(\) and \(\) are the hyper-parameters to balance the impact of two losses. When rendering a scenario, only the S-NeRF network in LuSh-NeRF is utilized with the volume rendering method to produce images of different views.

## 4 Experiments

**Our Dataset.** Since we are the first to reconstruct NeRF with hand-held low-light photographs, we build a new dataset based on the low-light image deblur dataset  for training and evaluation. Specifically, our dataset consists of 5 synthetic and 5 real scenes, for the quantitative and generalization evaluations. We use the COLMAP  method to estimate the camera pose of each image in the scenarios. Each scenario contains 20-25 images, at 1120\(\) 640 resolution. The brightness of images in each scenario is extremely low, where the intensities of most pixels are below 50. 80% of the images contain camera shake problems, which pose a significant challenge for NeRF reconstruction. Refer to Appendix A.3 for more details of our dataset.

**Implementation Details.** We have implemented our LuSh-NeRF based on the official code of Deblur-NeRF , and with the same Rigid Blurring Kernel network in . The S-NeRF shares the same structure as NeRF , while the the depth and width of N-Estimator are set to half of those of the S-NeRF. The number of camera motions \(k\) and the frequency filter radius in the CTP module are set to 4 and 30. The number of aligned rays \(K\) and certainty threshold \(\) in the SND module are set to 20 and 0.8. Before training, the input images are up-scaled by gamma adjustment and histogram equalization. The batch size is set to 1,024 rays, with 64 fine and coarse sampled points. \(\) and \(\) are set to 1 and 0 during the first 60K iterations for better rendering results, to avoid the inaccuracy matching matrix \(M\) interfering with the SND module. The two hyper-parameters are then changed to

Figure 4: Qualitative results of different methods on our real scenes. Our LuSh-NeRF can render cleaner and sharper results for real low-light scenes with camera motions.

1 and \(1 10^{-2}\) in the last 40K iterations. All the experiments in this paper are performed on a PC with an i9-13900K CPU and a single NVIDIA RTX3090 GPU.

### Main Results

**Evaluation Methods and Metrics.** Since we are the first to tackle the NeRF reconstruction from low-light images captured with camera motions, we design several baseline methods by combining existing state-of-the-art image-based and NeRF-based methods: (1) Low-light image enhancement [54; 49] + Image deblurring method [60; 59] + NeRF ; (2) Joint low-light image enhancement and deblurring method  + NeRF ; (3) Low-light image enhancement [13; 54] + deblurring NeRF ; and (4) Image deblurring [59; 60] + low-light NeRF enhancement . All the selected models have been shown to achieve SOTA performances on their individual tasks. PSNR, SSIM, and LPIPS  metrics are used to evaluate the performance difference of the rendered images between LuSh-NeRF and the combination of other state-of-the-art methods. Due to the significant variations in color contrast ratio and image brightness of the restored low light images by different methods, we consider LPIPS (Learned Perceptual Image Patch Similarity), which can reasonably assess perceptual image quality and visual differences rather than the pixel-level difference, as the key metric.

**Visual Comparisons.** Fig. 4 presents qualitative comparison of the rendering results from the above three combined methods with those of our LuSh-NeRF in our real low-light scenarios. We can see that the results from LEDNet  are still dark, which significantly limits the learning capability of the NeRF. While PairLIE  combined with DP-NeRF  can reconstruct clear rendering results, they are rather unnatural. The color contrast is severely shifted, resulting in unrealistic images with some lost details and incomplete blur removal. In addition, we can also observe that first deblurring on low-light images and then applying the LLE method, as in Restormer  + LLNeRF , is less effective, and there is still substantial blurring in the results. In contrast, our approach can recover detailed scenes with natural colors, resulting in a sharp and clean novel view visualization.

Fig. 5 presents qualitative comparison between existing methods and our LuSh-NeRF on our synthetic scenes. All of the above methods suffer serious performance degradation when the image luminance is low and the camera shake is severe (_e.g._, the bottom of the first example in Fig. 5), where LuSh-NeRF can still reconstruct a satisfactory scene by decoupling the defects of the scene. As we can see from

Figure 5: Qualitative results of different methods on our synthetic scenes. Our method yields the most natural restoration results while sharpening the image.

the given visualizations on realistic and synthesized data, higher PSNR and SSIM metrics of the methods (_e.g._, LEDNet + NeRF) do not entirely represent better performances on the NeRF results in our proposed task. This demonstrates that the perceptual metric, LPIPS, is better at assessing the merits of the reconstructed scenario. Refer to Appendix A.5 for more visual results.

**Quantitative Comparison.** We conduct a quantitative comparison of our method against various combinations of SOTA approaches on our synthesized data in Tab. 1. Note that our synthesized data originates from the training and testing sets of LOL-Blur dataset , while the weight of LEDNet  is also specifically trained on the synthesized dataset of LOL-Blur. This condition does not constitute a fair comparison. Nonetheless, we report the results of using "LEDNet  + NeRF " as a reference.

Our method achieves better performances than multiple combinations of existing methods on all 5 synthesized data. In terms of the LPIPS metric on all the scenarios, our method outperforms the rendering results of LEDNet  + NeRF  in an unsupervised optimization manner, which proves that the SND and CTP modules in LuSh-NeRF fully utilize the 3D information of the scene to aid in the recovery of the irreversible quality degradation in the single image.

**Ablation Study.** Fig. 6 demonstrates the effect of the various components of LuSh-NeRF on a realistic scenario. After the images are preprocessed to enhance the luminance, the noise in the images is significantly amplified (as shown in the white box). Directly using blur kernel prediction  on these images is heavily interfered by noise, which prevents recovery from camera shakes (Fig. 6(b)). In addition, incorrect kernel predictions have a negative impact as they make it difficult to align the multi-view images rendered by S-NeRF and the SND module cannot obtain accurate viewpoint consistency supervision (Fig. 6(c)). When the CTP module is utilized, the blur of the image is alleviated, but the noise problem is still serious (Fig. 6(d)). First sharpening the image with CTP

    &  &  &  \\   & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\  NeRF  & 6.02 & 0.0307 & 0.8030 & 112.5 & 0.5195 & 0.4061 & 5.53 & 0.0716 & 0.8418 \\ LLFormer  + MPNet  + NeRF  & 19.01 & **0.5585** & 0.4571 & 14.67 & 0.5756 & 0.2785 & 18.90 & 0.5285 & 0.5290 \\ URetienNet  + Restomer  + NeRF  & 17.47 & 0.5420 & 0.4820 & 12.38 & 0.3483 & 0.6347 & 17.06 & 0.5205 & 0.5256 \\ LEDNet  + NeRF  & 18.14 & 0.6025 & 0.3550 & 21.09 & 0.7140 & 0.3009 & 15.21 & 0.5869 & 0.4137 \\ Restomer  + LLNet  & 17.85 & 0.5541 & 0.5000 & 14.03 & 0.5462 & 0.4182 & 14.94 & 0.520 & 0.5260 \\ MPNetNet  + LLNet  & 17.27 & 0.5493 & 0.4942 & 10.88 & 0.4744 & 0.4334 & 13.47 & **0.5420** & 0.5146 \\ PairLIE  + DP-NeRF  & 14.08 & 0.4574 & 0.3651 & 13.34 & 0.4482 & 0.2995 & 13.11 & 0.5048 & 0.4316 \\ URetienNet  + DP-NeRF  & 16.95 & 0.4884 & 0.3762 & 14.12 & 0.4199 & 0.5829 & 16.43 & 0.5211 & 0.4472 \\ 
**LaSnRF (Ours)** & **19.06** & 0.5354 & **0.5182** & **0.5182** & **0.6325** & **0.6265** & **19.34** & **0.5275** & **0.8832** \\    
    &  \\   & Method & &  &  \\   & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\  NeRF  & 7.54 & 0.0553 & 0.7186 & 8.19 & 0.1679 & 0.5048 & 7.71 & 0.1683 & 0.6548 \\ LLFormer  + MPNetNet  + NeRF  & 16.04 & 0.5204 & 0.4192 & **2.022** & **0.7153** & 0.3026 & 18.14 & 0.5728 & 0.3973 \\ URetienNet  + Restomer  + NeRF  & 16.29 & 0.5152 & 0.4347 & 0.39 & 0.7045 & 0.2834 & 16.72 & 0.5225 & 0.4721 \\ LEDNet  + NeRF  & 18.24 & 0.6127 & 0.2772 & 19.33 & 0.7279 & 0.2570 & 18.40 & 0.6488 & 0.3204 \\ Restomer  + LLNet  & 15.22 & 0.5016 & 0.4199 & 20.04 & 0.7126 & 0.3452 & 16.42 & 0.5679 & 0.4419 \\ MPNetNet  + LLNet  & 14.25 & 0.5155 & 0.3923 & 14.16 & 0.5213 & 0.4958 & 14.01 & 0.5205 & 0.4661 \\ PairLIE  + DP-NeRF  & 13.86 & 0.5137 & 0.3157 & 19.65 & 0.6055 & 0.2600 & 14.81 & 0.5059 & 0.3344 \\ URetienNet  + DP-NeRF  & 16.38 & 0.5286 & 0.3550 & 20.21 & 0.6328 & 0.2616 & 16.74 & 0.5182 & 0.4046 \\ LuShNet (Ours) & **18.94** & **0.5884** & **0.2562** & **21.09** & **0.6421** & **0.2400** & **19.31** & **0.5853** & **0.2914** \\   

Table 1: Quantitative comparison of different methods on our synthetic scenes. The **best** and the second performances of each scenario are marked in the table.

Figure 6: Ablation study of LuSh-NeRF on a real scenario.

and then removing noise with SND can somewhat suppress the noise problem, but the noise may be reconstructed to the various viewpoints and is not completely removed (Fig. 6(e)). LuSh-NeRF can render sharp and clean scenario images in novel views after the training process (Fig. 6(f)).

We show in the Fig. 7 the difference between the two masks obtained by directly performing an RGB intensity threshold and performing a frequency filtering before taking a threshold. (1) The RGB intensities in many noise-dominant regions are also high (e.g., the sky and the grass), which are harmful to the blur estimation but not excluded in the mask produced by thresholding. (2) The CTP module uses a frequency filter to identify the low-frequency-dominated regions of the image, and then obtains the desired mask based on the RGB intensity values. The gradients of rays in high-frequency and dark regions (regions more severely affected by noise) are detached during the Blur Kernel optimization process, which ensures better blur modeling.

## 5 Conclusion and Limitation

In this paper, we have proposed the NeRF reconstruction task from low-light images with camera motions, and a novel unsupervised reconstruction network LuSh-NeRF. LuSh-NeRF models the NeRF restoration process based on the characteristics of three kinds of defects in the scene, _i.e._, low-light, noise, and camera blur, and successfully reconstructs a normal-light, clean, and sharp NeRF network from poor-quality low-light images, by exploiting multi-view consistency and frequency-domain information. To facilitate the training and evaluation of the new proposed task, we construct a new dataset which contains 5 synthetic and 5 real scenes low light images with hand-held camera motions. Extensive experiments on our proposed dataset demonstrate that our method is capable of achieving satisfactory NeRF reconstruction results, despite the defects in the input images.

The LuSh-NeRF network also has some limitations. One limitation is that it needs to optimize two NeRF networks at the same time and render multiple rays for the blur problem when sharpening images during training, which results in a slower optimization process, even though this would not interfere with the rendering speed. Besides, noise that is relatively similar across views may be difficult to remove due to the reliance on viewpoint consistency. (Refer to Appendix A.8 for details.) As a future work, we would like to address these issues.

Figure 7: The RGB intensity mask and CTP mask comparison. Two masks are generated with the same threshold \(T\). White points represent 1 and black points represent 0 in each mask.